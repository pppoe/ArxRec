<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20250916.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian\n  Splatting and Bag of Embeddings", "author": "Abdalla Arafa and Didier Stricker", "abstract": "  Novel view synthesis has seen significant advancements with 3D Gaussian\nSplatting (3DGS), enabling real-time photorealistic rendering. However, the\ninherent fuzziness of Gaussian Splatting presents challenges for 3D scene\nunderstanding, restricting its broader applications in AR/VR and robotics.\nWhile recent works attempt to learn semantics via 2D foundation model\ndistillation, they inherit fundamental limitations: alpha blending averages\nsemantics across objects, making 3D-level understanding impossible. We propose\na paradigm-shifting alternative that bypasses differentiable rendering for\nsemantics entirely. Our key insight is to leverage predecomposed object-level\nGaussians and represent each object through multiview CLIP feature aggregation,\ncreating comprehensive \"bags of embeddings\" that holistically describe objects.\nThis allows: (1) accurate open-vocabulary object retrieval by comparing text\nqueries to object-level (not Gaussian-level) embeddings, and (2) seamless task\nadaptation: propagating object IDs to pixels for 2D segmentation or to\nGaussians for 3D extraction. Experiments demonstrate that our method\neffectively overcomes the challenges of 3D open-vocabulary object extraction\nwhile remaining comparable to state-of-the-art performance in 2D\nopen-vocabulary segmentation, ensuring minimal compromise.\n", "link": "http://arxiv.org/abs/2509.12938v1", "date": "2025-09-16", "relevancy": 3.34, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.684}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6812}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Averages%3A%20Open-Vocabulary%203D%20Scene%20Understanding%20with%20Gaussian%0A%20%20Splatting%20and%20Bag%20of%20Embeddings&body=Title%3A%20Beyond%20Averages%3A%20Open-Vocabulary%203D%20Scene%20Understanding%20with%20Gaussian%0A%20%20Splatting%20and%20Bag%20of%20Embeddings%0AAuthor%3A%20Abdalla%20Arafa%20and%20Didier%20Stricker%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20has%20seen%20significant%20advancements%20with%203D%20Gaussian%0ASplatting%20%283DGS%29%2C%20enabling%20real-time%20photorealistic%20rendering.%20However%2C%20the%0Ainherent%20fuzziness%20of%20Gaussian%20Splatting%20presents%20challenges%20for%203D%20scene%0Aunderstanding%2C%20restricting%20its%20broader%20applications%20in%20AR/VR%20and%20robotics.%0AWhile%20recent%20works%20attempt%20to%20learn%20semantics%20via%202D%20foundation%20model%0Adistillation%2C%20they%20inherit%20fundamental%20limitations%3A%20alpha%20blending%20averages%0Asemantics%20across%20objects%2C%20making%203D-level%20understanding%20impossible.%20We%20propose%0Aa%20paradigm-shifting%20alternative%20that%20bypasses%20differentiable%20rendering%20for%0Asemantics%20entirely.%20Our%20key%20insight%20is%20to%20leverage%20predecomposed%20object-level%0AGaussians%20and%20represent%20each%20object%20through%20multiview%20CLIP%20feature%20aggregation%2C%0Acreating%20comprehensive%20%22bags%20of%20embeddings%22%20that%20holistically%20describe%20objects.%0AThis%20allows%3A%20%281%29%20accurate%20open-vocabulary%20object%20retrieval%20by%20comparing%20text%0Aqueries%20to%20object-level%20%28not%20Gaussian-level%29%20embeddings%2C%20and%20%282%29%20seamless%20task%0Aadaptation%3A%20propagating%20object%20IDs%20to%20pixels%20for%202D%20segmentation%20or%20to%0AGaussians%20for%203D%20extraction.%20Experiments%20demonstrate%20that%20our%20method%0Aeffectively%20overcomes%20the%20challenges%20of%203D%20open-vocabulary%20object%20extraction%0Awhile%20remaining%20comparable%20to%20state-of-the-art%20performance%20in%202D%0Aopen-vocabulary%20segmentation%2C%20ensuring%20minimal%20compromise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12938v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Averages%253A%2520Open-Vocabulary%25203D%2520Scene%2520Understanding%2520with%2520Gaussian%250A%2520%2520Splatting%2520and%2520Bag%2520of%2520Embeddings%26entry.906535625%3DAbdalla%2520Arafa%2520and%2520Didier%2520Stricker%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520has%2520seen%2520significant%2520advancements%2520with%25203D%2520Gaussian%250ASplatting%2520%25283DGS%2529%252C%2520enabling%2520real-time%2520photorealistic%2520rendering.%2520However%252C%2520the%250Ainherent%2520fuzziness%2520of%2520Gaussian%2520Splatting%2520presents%2520challenges%2520for%25203D%2520scene%250Aunderstanding%252C%2520restricting%2520its%2520broader%2520applications%2520in%2520AR/VR%2520and%2520robotics.%250AWhile%2520recent%2520works%2520attempt%2520to%2520learn%2520semantics%2520via%25202D%2520foundation%2520model%250Adistillation%252C%2520they%2520inherit%2520fundamental%2520limitations%253A%2520alpha%2520blending%2520averages%250Asemantics%2520across%2520objects%252C%2520making%25203D-level%2520understanding%2520impossible.%2520We%2520propose%250Aa%2520paradigm-shifting%2520alternative%2520that%2520bypasses%2520differentiable%2520rendering%2520for%250Asemantics%2520entirely.%2520Our%2520key%2520insight%2520is%2520to%2520leverage%2520predecomposed%2520object-level%250AGaussians%2520and%2520represent%2520each%2520object%2520through%2520multiview%2520CLIP%2520feature%2520aggregation%252C%250Acreating%2520comprehensive%2520%2522bags%2520of%2520embeddings%2522%2520that%2520holistically%2520describe%2520objects.%250AThis%2520allows%253A%2520%25281%2529%2520accurate%2520open-vocabulary%2520object%2520retrieval%2520by%2520comparing%2520text%250Aqueries%2520to%2520object-level%2520%2528not%2520Gaussian-level%2529%2520embeddings%252C%2520and%2520%25282%2529%2520seamless%2520task%250Aadaptation%253A%2520propagating%2520object%2520IDs%2520to%2520pixels%2520for%25202D%2520segmentation%2520or%2520to%250AGaussians%2520for%25203D%2520extraction.%2520Experiments%2520demonstrate%2520that%2520our%2520method%250Aeffectively%2520overcomes%2520the%2520challenges%2520of%25203D%2520open-vocabulary%2520object%2520extraction%250Awhile%2520remaining%2520comparable%2520to%2520state-of-the-art%2520performance%2520in%25202D%250Aopen-vocabulary%2520segmentation%252C%2520ensuring%2520minimal%2520compromise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12938v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Averages%3A%20Open-Vocabulary%203D%20Scene%20Understanding%20with%20Gaussian%0A%20%20Splatting%20and%20Bag%20of%20Embeddings&entry.906535625=Abdalla%20Arafa%20and%20Didier%20Stricker&entry.1292438233=%20%20Novel%20view%20synthesis%20has%20seen%20significant%20advancements%20with%203D%20Gaussian%0ASplatting%20%283DGS%29%2C%20enabling%20real-time%20photorealistic%20rendering.%20However%2C%20the%0Ainherent%20fuzziness%20of%20Gaussian%20Splatting%20presents%20challenges%20for%203D%20scene%0Aunderstanding%2C%20restricting%20its%20broader%20applications%20in%20AR/VR%20and%20robotics.%0AWhile%20recent%20works%20attempt%20to%20learn%20semantics%20via%202D%20foundation%20model%0Adistillation%2C%20they%20inherit%20fundamental%20limitations%3A%20alpha%20blending%20averages%0Asemantics%20across%20objects%2C%20making%203D-level%20understanding%20impossible.%20We%20propose%0Aa%20paradigm-shifting%20alternative%20that%20bypasses%20differentiable%20rendering%20for%0Asemantics%20entirely.%20Our%20key%20insight%20is%20to%20leverage%20predecomposed%20object-level%0AGaussians%20and%20represent%20each%20object%20through%20multiview%20CLIP%20feature%20aggregation%2C%0Acreating%20comprehensive%20%22bags%20of%20embeddings%22%20that%20holistically%20describe%20objects.%0AThis%20allows%3A%20%281%29%20accurate%20open-vocabulary%20object%20retrieval%20by%20comparing%20text%0Aqueries%20to%20object-level%20%28not%20Gaussian-level%29%20embeddings%2C%20and%20%282%29%20seamless%20task%0Aadaptation%3A%20propagating%20object%20IDs%20to%20pixels%20for%202D%20segmentation%20or%20to%0AGaussians%20for%203D%20extraction.%20Experiments%20demonstrate%20that%20our%20method%0Aeffectively%20overcomes%20the%20challenges%20of%203D%20open-vocabulary%20object%20extraction%0Awhile%20remaining%20comparable%20to%20state-of-the-art%20performance%20in%202D%0Aopen-vocabulary%20segmentation%2C%20ensuring%20minimal%20compromise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12938v1&entry.124074799=Read"},
{"title": "Dream3DAvatar: Text-Controlled 3D Avatar Reconstruction from a Single\n  Image", "author": "Gaofeng Liu and Hengsen Li and Ruoyu Gao and Xuetong Li and Zhiyuan Ma and Tao Fang", "abstract": "  With the rapid advancement of 3D representation techniques and generative\nmodels, substantial progress has been made in reconstructing full-body 3D\navatars from a single image. However, this task remains fundamentally\nill-posedness due to the limited information available from monocular input,\nmaking it difficult to control the geometry and texture of occluded regions\nduring generation. To address these challenges, we redesign the reconstruction\npipeline and propose Dream3DAvatar, an efficient and text-controllable\ntwo-stage framework for 3D avatar generation. In the first stage, we develop a\nlightweight, adapter-enhanced multi-view generation model. Specifically, we\nintroduce the Pose-Adapter to inject SMPL-X renderings and skeletal information\ninto SDXL, enforcing geometric and pose consistency across views. To preserve\nfacial identity, we incorporate ID-Adapter-G, which injects high-resolution\nfacial features into the generation process. Additionally, we leverage BLIP2 to\ngenerate high-quality textual descriptions of the multi-view images, enhancing\ntext-driven controllability in occluded regions. In the second stage, we design\na feedforward Transformer model equipped with a multi-view feature fusion\nmodule to reconstruct high-fidelity 3D Gaussian Splat representations (3DGS)\nfrom the generated images. Furthermore, we introduce ID-Adapter-R, which\nutilizes a gating mechanism to effectively fuse facial features into the\nreconstruction process, improving high-frequency detail recovery. Extensive\nexperiments demonstrate that our method can generate realistic, animation-ready\n3D avatars without any post-processing and consistently outperforms existing\nbaselines across multiple evaluation metrics.\n", "link": "http://arxiv.org/abs/2509.13013v1", "date": "2025-09-16", "relevancy": 3.2953, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6596}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6596}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6579}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dream3DAvatar%3A%20Text-Controlled%203D%20Avatar%20Reconstruction%20from%20a%20Single%0A%20%20Image&body=Title%3A%20Dream3DAvatar%3A%20Text-Controlled%203D%20Avatar%20Reconstruction%20from%20a%20Single%0A%20%20Image%0AAuthor%3A%20Gaofeng%20Liu%20and%20Hengsen%20Li%20and%20Ruoyu%20Gao%20and%20Xuetong%20Li%20and%20Zhiyuan%20Ma%20and%20Tao%20Fang%0AAbstract%3A%20%20%20With%20the%20rapid%20advancement%20of%203D%20representation%20techniques%20and%20generative%0Amodels%2C%20substantial%20progress%20has%20been%20made%20in%20reconstructing%20full-body%203D%0Aavatars%20from%20a%20single%20image.%20However%2C%20this%20task%20remains%20fundamentally%0Aill-posedness%20due%20to%20the%20limited%20information%20available%20from%20monocular%20input%2C%0Amaking%20it%20difficult%20to%20control%20the%20geometry%20and%20texture%20of%20occluded%20regions%0Aduring%20generation.%20To%20address%20these%20challenges%2C%20we%20redesign%20the%20reconstruction%0Apipeline%20and%20propose%20Dream3DAvatar%2C%20an%20efficient%20and%20text-controllable%0Atwo-stage%20framework%20for%203D%20avatar%20generation.%20In%20the%20first%20stage%2C%20we%20develop%20a%0Alightweight%2C%20adapter-enhanced%20multi-view%20generation%20model.%20Specifically%2C%20we%0Aintroduce%20the%20Pose-Adapter%20to%20inject%20SMPL-X%20renderings%20and%20skeletal%20information%0Ainto%20SDXL%2C%20enforcing%20geometric%20and%20pose%20consistency%20across%20views.%20To%20preserve%0Afacial%20identity%2C%20we%20incorporate%20ID-Adapter-G%2C%20which%20injects%20high-resolution%0Afacial%20features%20into%20the%20generation%20process.%20Additionally%2C%20we%20leverage%20BLIP2%20to%0Agenerate%20high-quality%20textual%20descriptions%20of%20the%20multi-view%20images%2C%20enhancing%0Atext-driven%20controllability%20in%20occluded%20regions.%20In%20the%20second%20stage%2C%20we%20design%0Aa%20feedforward%20Transformer%20model%20equipped%20with%20a%20multi-view%20feature%20fusion%0Amodule%20to%20reconstruct%20high-fidelity%203D%20Gaussian%20Splat%20representations%20%283DGS%29%0Afrom%20the%20generated%20images.%20Furthermore%2C%20we%20introduce%20ID-Adapter-R%2C%20which%0Autilizes%20a%20gating%20mechanism%20to%20effectively%20fuse%20facial%20features%20into%20the%0Areconstruction%20process%2C%20improving%20high-frequency%20detail%20recovery.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20can%20generate%20realistic%2C%20animation-ready%0A3D%20avatars%20without%20any%20post-processing%20and%20consistently%20outperforms%20existing%0Abaselines%20across%20multiple%20evaluation%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13013v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDream3DAvatar%253A%2520Text-Controlled%25203D%2520Avatar%2520Reconstruction%2520from%2520a%2520Single%250A%2520%2520Image%26entry.906535625%3DGaofeng%2520Liu%2520and%2520Hengsen%2520Li%2520and%2520Ruoyu%2520Gao%2520and%2520Xuetong%2520Li%2520and%2520Zhiyuan%2520Ma%2520and%2520Tao%2520Fang%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520advancement%2520of%25203D%2520representation%2520techniques%2520and%2520generative%250Amodels%252C%2520substantial%2520progress%2520has%2520been%2520made%2520in%2520reconstructing%2520full-body%25203D%250Aavatars%2520from%2520a%2520single%2520image.%2520However%252C%2520this%2520task%2520remains%2520fundamentally%250Aill-posedness%2520due%2520to%2520the%2520limited%2520information%2520available%2520from%2520monocular%2520input%252C%250Amaking%2520it%2520difficult%2520to%2520control%2520the%2520geometry%2520and%2520texture%2520of%2520occluded%2520regions%250Aduring%2520generation.%2520To%2520address%2520these%2520challenges%252C%2520we%2520redesign%2520the%2520reconstruction%250Apipeline%2520and%2520propose%2520Dream3DAvatar%252C%2520an%2520efficient%2520and%2520text-controllable%250Atwo-stage%2520framework%2520for%25203D%2520avatar%2520generation.%2520In%2520the%2520first%2520stage%252C%2520we%2520develop%2520a%250Alightweight%252C%2520adapter-enhanced%2520multi-view%2520generation%2520model.%2520Specifically%252C%2520we%250Aintroduce%2520the%2520Pose-Adapter%2520to%2520inject%2520SMPL-X%2520renderings%2520and%2520skeletal%2520information%250Ainto%2520SDXL%252C%2520enforcing%2520geometric%2520and%2520pose%2520consistency%2520across%2520views.%2520To%2520preserve%250Afacial%2520identity%252C%2520we%2520incorporate%2520ID-Adapter-G%252C%2520which%2520injects%2520high-resolution%250Afacial%2520features%2520into%2520the%2520generation%2520process.%2520Additionally%252C%2520we%2520leverage%2520BLIP2%2520to%250Agenerate%2520high-quality%2520textual%2520descriptions%2520of%2520the%2520multi-view%2520images%252C%2520enhancing%250Atext-driven%2520controllability%2520in%2520occluded%2520regions.%2520In%2520the%2520second%2520stage%252C%2520we%2520design%250Aa%2520feedforward%2520Transformer%2520model%2520equipped%2520with%2520a%2520multi-view%2520feature%2520fusion%250Amodule%2520to%2520reconstruct%2520high-fidelity%25203D%2520Gaussian%2520Splat%2520representations%2520%25283DGS%2529%250Afrom%2520the%2520generated%2520images.%2520Furthermore%252C%2520we%2520introduce%2520ID-Adapter-R%252C%2520which%250Autilizes%2520a%2520gating%2520mechanism%2520to%2520effectively%2520fuse%2520facial%2520features%2520into%2520the%250Areconstruction%2520process%252C%2520improving%2520high-frequency%2520detail%2520recovery.%2520Extensive%250Aexperiments%2520demonstrate%2520that%2520our%2520method%2520can%2520generate%2520realistic%252C%2520animation-ready%250A3D%2520avatars%2520without%2520any%2520post-processing%2520and%2520consistently%2520outperforms%2520existing%250Abaselines%2520across%2520multiple%2520evaluation%2520metrics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13013v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dream3DAvatar%3A%20Text-Controlled%203D%20Avatar%20Reconstruction%20from%20a%20Single%0A%20%20Image&entry.906535625=Gaofeng%20Liu%20and%20Hengsen%20Li%20and%20Ruoyu%20Gao%20and%20Xuetong%20Li%20and%20Zhiyuan%20Ma%20and%20Tao%20Fang&entry.1292438233=%20%20With%20the%20rapid%20advancement%20of%203D%20representation%20techniques%20and%20generative%0Amodels%2C%20substantial%20progress%20has%20been%20made%20in%20reconstructing%20full-body%203D%0Aavatars%20from%20a%20single%20image.%20However%2C%20this%20task%20remains%20fundamentally%0Aill-posedness%20due%20to%20the%20limited%20information%20available%20from%20monocular%20input%2C%0Amaking%20it%20difficult%20to%20control%20the%20geometry%20and%20texture%20of%20occluded%20regions%0Aduring%20generation.%20To%20address%20these%20challenges%2C%20we%20redesign%20the%20reconstruction%0Apipeline%20and%20propose%20Dream3DAvatar%2C%20an%20efficient%20and%20text-controllable%0Atwo-stage%20framework%20for%203D%20avatar%20generation.%20In%20the%20first%20stage%2C%20we%20develop%20a%0Alightweight%2C%20adapter-enhanced%20multi-view%20generation%20model.%20Specifically%2C%20we%0Aintroduce%20the%20Pose-Adapter%20to%20inject%20SMPL-X%20renderings%20and%20skeletal%20information%0Ainto%20SDXL%2C%20enforcing%20geometric%20and%20pose%20consistency%20across%20views.%20To%20preserve%0Afacial%20identity%2C%20we%20incorporate%20ID-Adapter-G%2C%20which%20injects%20high-resolution%0Afacial%20features%20into%20the%20generation%20process.%20Additionally%2C%20we%20leverage%20BLIP2%20to%0Agenerate%20high-quality%20textual%20descriptions%20of%20the%20multi-view%20images%2C%20enhancing%0Atext-driven%20controllability%20in%20occluded%20regions.%20In%20the%20second%20stage%2C%20we%20design%0Aa%20feedforward%20Transformer%20model%20equipped%20with%20a%20multi-view%20feature%20fusion%0Amodule%20to%20reconstruct%20high-fidelity%203D%20Gaussian%20Splat%20representations%20%283DGS%29%0Afrom%20the%20generated%20images.%20Furthermore%2C%20we%20introduce%20ID-Adapter-R%2C%20which%0Autilizes%20a%20gating%20mechanism%20to%20effectively%20fuse%20facial%20features%20into%20the%0Areconstruction%20process%2C%20improving%20high-frequency%20detail%20recovery.%20Extensive%0Aexperiments%20demonstrate%20that%20our%20method%20can%20generate%20realistic%2C%20animation-ready%0A3D%20avatars%20without%20any%20post-processing%20and%20consistently%20outperforms%20existing%0Abaselines%20across%20multiple%20evaluation%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13013v1&entry.124074799=Read"},
{"title": "3D Aware Region Prompted Vision Language Model", "author": "An-Chieh Cheng and Yang Fu and Yukang Chen and Zhijian Liu and Xiaolong Li and Subhashree Radhakrishnan and Song Han and Yao Lu and Jan Kautz and Pavlo Molchanov and Hongxu Yin and Xiaolong Wang and Sifei Liu", "abstract": "  We present Spatial Region 3D (SR-3D) aware vision-language model that\nconnects single-view 2D images and multi-view 3D data through a shared visual\ntoken space. SR-3D supports flexible region prompting, allowing users to\nannotate regions with bounding boxes, segmentation masks on any frame, or\ndirectly in 3D, without the need for exhaustive multi-frame labeling. We\nachieve this by enriching 2D visual features with 3D positional embeddings,\nwhich allows the 3D model to draw upon strong 2D priors for more accurate\nspatial reasoning across frames, even when objects of interest do not co-occur\nwithin the same view. Extensive experiments on both general 2D vision language\nand specialized 3D spatial benchmarks demonstrate that SR-3D achieves\nstate-of-the-art performance, underscoring its effectiveness for unifying 2D\nand 3D representation space on scene understanding. Moreover, we observe\napplicability to in-the-wild videos without sensory 3D inputs or ground-truth\n3D annotations, where SR-3D accurately infers spatial relationships and metric\nmeasurements.\n", "link": "http://arxiv.org/abs/2509.13317v1", "date": "2025-09-16", "relevancy": 3.2891, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6803}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6803}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6129}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Aware%20Region%20Prompted%20Vision%20Language%20Model&body=Title%3A%203D%20Aware%20Region%20Prompted%20Vision%20Language%20Model%0AAuthor%3A%20An-Chieh%20Cheng%20and%20Yang%20Fu%20and%20Yukang%20Chen%20and%20Zhijian%20Liu%20and%20Xiaolong%20Li%20and%20Subhashree%20Radhakrishnan%20and%20Song%20Han%20and%20Yao%20Lu%20and%20Jan%20Kautz%20and%20Pavlo%20Molchanov%20and%20Hongxu%20Yin%20and%20Xiaolong%20Wang%20and%20Sifei%20Liu%0AAbstract%3A%20%20%20We%20present%20Spatial%20Region%203D%20%28SR-3D%29%20aware%20vision-language%20model%20that%0Aconnects%20single-view%202D%20images%20and%20multi-view%203D%20data%20through%20a%20shared%20visual%0Atoken%20space.%20SR-3D%20supports%20flexible%20region%20prompting%2C%20allowing%20users%20to%0Aannotate%20regions%20with%20bounding%20boxes%2C%20segmentation%20masks%20on%20any%20frame%2C%20or%0Adirectly%20in%203D%2C%20without%20the%20need%20for%20exhaustive%20multi-frame%20labeling.%20We%0Aachieve%20this%20by%20enriching%202D%20visual%20features%20with%203D%20positional%20embeddings%2C%0Awhich%20allows%20the%203D%20model%20to%20draw%20upon%20strong%202D%20priors%20for%20more%20accurate%0Aspatial%20reasoning%20across%20frames%2C%20even%20when%20objects%20of%20interest%20do%20not%20co-occur%0Awithin%20the%20same%20view.%20Extensive%20experiments%20on%20both%20general%202D%20vision%20language%0Aand%20specialized%203D%20spatial%20benchmarks%20demonstrate%20that%20SR-3D%20achieves%0Astate-of-the-art%20performance%2C%20underscoring%20its%20effectiveness%20for%20unifying%202D%0Aand%203D%20representation%20space%20on%20scene%20understanding.%20Moreover%2C%20we%20observe%0Aapplicability%20to%20in-the-wild%20videos%20without%20sensory%203D%20inputs%20or%20ground-truth%0A3D%20annotations%2C%20where%20SR-3D%20accurately%20infers%20spatial%20relationships%20and%20metric%0Ameasurements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Aware%2520Region%2520Prompted%2520Vision%2520Language%2520Model%26entry.906535625%3DAn-Chieh%2520Cheng%2520and%2520Yang%2520Fu%2520and%2520Yukang%2520Chen%2520and%2520Zhijian%2520Liu%2520and%2520Xiaolong%2520Li%2520and%2520Subhashree%2520Radhakrishnan%2520and%2520Song%2520Han%2520and%2520Yao%2520Lu%2520and%2520Jan%2520Kautz%2520and%2520Pavlo%2520Molchanov%2520and%2520Hongxu%2520Yin%2520and%2520Xiaolong%2520Wang%2520and%2520Sifei%2520Liu%26entry.1292438233%3D%2520%2520We%2520present%2520Spatial%2520Region%25203D%2520%2528SR-3D%2529%2520aware%2520vision-language%2520model%2520that%250Aconnects%2520single-view%25202D%2520images%2520and%2520multi-view%25203D%2520data%2520through%2520a%2520shared%2520visual%250Atoken%2520space.%2520SR-3D%2520supports%2520flexible%2520region%2520prompting%252C%2520allowing%2520users%2520to%250Aannotate%2520regions%2520with%2520bounding%2520boxes%252C%2520segmentation%2520masks%2520on%2520any%2520frame%252C%2520or%250Adirectly%2520in%25203D%252C%2520without%2520the%2520need%2520for%2520exhaustive%2520multi-frame%2520labeling.%2520We%250Aachieve%2520this%2520by%2520enriching%25202D%2520visual%2520features%2520with%25203D%2520positional%2520embeddings%252C%250Awhich%2520allows%2520the%25203D%2520model%2520to%2520draw%2520upon%2520strong%25202D%2520priors%2520for%2520more%2520accurate%250Aspatial%2520reasoning%2520across%2520frames%252C%2520even%2520when%2520objects%2520of%2520interest%2520do%2520not%2520co-occur%250Awithin%2520the%2520same%2520view.%2520Extensive%2520experiments%2520on%2520both%2520general%25202D%2520vision%2520language%250Aand%2520specialized%25203D%2520spatial%2520benchmarks%2520demonstrate%2520that%2520SR-3D%2520achieves%250Astate-of-the-art%2520performance%252C%2520underscoring%2520its%2520effectiveness%2520for%2520unifying%25202D%250Aand%25203D%2520representation%2520space%2520on%2520scene%2520understanding.%2520Moreover%252C%2520we%2520observe%250Aapplicability%2520to%2520in-the-wild%2520videos%2520without%2520sensory%25203D%2520inputs%2520or%2520ground-truth%250A3D%2520annotations%252C%2520where%2520SR-3D%2520accurately%2520infers%2520spatial%2520relationships%2520and%2520metric%250Ameasurements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Aware%20Region%20Prompted%20Vision%20Language%20Model&entry.906535625=An-Chieh%20Cheng%20and%20Yang%20Fu%20and%20Yukang%20Chen%20and%20Zhijian%20Liu%20and%20Xiaolong%20Li%20and%20Subhashree%20Radhakrishnan%20and%20Song%20Han%20and%20Yao%20Lu%20and%20Jan%20Kautz%20and%20Pavlo%20Molchanov%20and%20Hongxu%20Yin%20and%20Xiaolong%20Wang%20and%20Sifei%20Liu&entry.1292438233=%20%20We%20present%20Spatial%20Region%203D%20%28SR-3D%29%20aware%20vision-language%20model%20that%0Aconnects%20single-view%202D%20images%20and%20multi-view%203D%20data%20through%20a%20shared%20visual%0Atoken%20space.%20SR-3D%20supports%20flexible%20region%20prompting%2C%20allowing%20users%20to%0Aannotate%20regions%20with%20bounding%20boxes%2C%20segmentation%20masks%20on%20any%20frame%2C%20or%0Adirectly%20in%203D%2C%20without%20the%20need%20for%20exhaustive%20multi-frame%20labeling.%20We%0Aachieve%20this%20by%20enriching%202D%20visual%20features%20with%203D%20positional%20embeddings%2C%0Awhich%20allows%20the%203D%20model%20to%20draw%20upon%20strong%202D%20priors%20for%20more%20accurate%0Aspatial%20reasoning%20across%20frames%2C%20even%20when%20objects%20of%20interest%20do%20not%20co-occur%0Awithin%20the%20same%20view.%20Extensive%20experiments%20on%20both%20general%202D%20vision%20language%0Aand%20specialized%203D%20spatial%20benchmarks%20demonstrate%20that%20SR-3D%20achieves%0Astate-of-the-art%20performance%2C%20underscoring%20its%20effectiveness%20for%20unifying%202D%0Aand%203D%20representation%20space%20on%20scene%20understanding.%20Moreover%2C%20we%20observe%0Aapplicability%20to%20in-the-wild%20videos%20without%20sensory%203D%20inputs%20or%20ground-truth%0A3D%20annotations%2C%20where%20SR-3D%20accurately%20infers%20spatial%20relationships%20and%20metric%0Ameasurements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13317v1&entry.124074799=Read"},
{"title": "Talking to DINO: Bridging Self-Supervised Vision Backbones with Language\n  for Open-Vocabulary Segmentation", "author": "Luca Barsellotti and Lorenzo Bianchi and Nicola Messina and Fabio Carrara and Marcella Cornia and Lorenzo Baraldi and Fabrizio Falchi and Rita Cucchiara", "abstract": "  Open-Vocabulary Segmentation (OVS) aims at segmenting images from free-form\ntextual concepts without predefined training classes. While existing\nvision-language models such as CLIP can generate segmentation masks by\nleveraging coarse spatial information from Vision Transformers, they face\nchallenges in spatial localization due to their global alignment of image and\ntext features. Conversely, self-supervised visual models like DINO excel in\nfine-grained visual encoding but lack integration with language. To bridge this\ngap, we present Talk2DINO, a novel hybrid approach that combines the spatial\naccuracy of DINOv2 with the language understanding of CLIP. Our approach aligns\nthe textual embeddings of CLIP to the patch-level features of DINOv2 through a\nlearned mapping function without the need to fine-tune the underlying\nbackbones. At training time, we exploit the attention maps of DINOv2 to\nselectively align local visual patches with textual embeddings. We show that\nthe powerful semantic and localization abilities of Talk2DINO can enhance the\nsegmentation process, resulting in more natural and less noisy segmentations,\nand that our approach can also effectively distinguish foreground objects from\nthe background. Experimental results demonstrate that Talk2DINO achieves\nstate-of-the-art performance across several unsupervised OVS benchmarks. Source\ncode and models are publicly available at:\nhttps://lorebianchi98.github.io/Talk2DINO/.\n", "link": "http://arxiv.org/abs/2411.19331v3", "date": "2025-09-16", "relevancy": 3.0484, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6208}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6208}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Talking%20to%20DINO%3A%20Bridging%20Self-Supervised%20Vision%20Backbones%20with%20Language%0A%20%20for%20Open-Vocabulary%20Segmentation&body=Title%3A%20Talking%20to%20DINO%3A%20Bridging%20Self-Supervised%20Vision%20Backbones%20with%20Language%0A%20%20for%20Open-Vocabulary%20Segmentation%0AAuthor%3A%20Luca%20Barsellotti%20and%20Lorenzo%20Bianchi%20and%20Nicola%20Messina%20and%20Fabio%20Carrara%20and%20Marcella%20Cornia%20and%20Lorenzo%20Baraldi%20and%20Fabrizio%20Falchi%20and%20Rita%20Cucchiara%0AAbstract%3A%20%20%20Open-Vocabulary%20Segmentation%20%28OVS%29%20aims%20at%20segmenting%20images%20from%20free-form%0Atextual%20concepts%20without%20predefined%20training%20classes.%20While%20existing%0Avision-language%20models%20such%20as%20CLIP%20can%20generate%20segmentation%20masks%20by%0Aleveraging%20coarse%20spatial%20information%20from%20Vision%20Transformers%2C%20they%20face%0Achallenges%20in%20spatial%20localization%20due%20to%20their%20global%20alignment%20of%20image%20and%0Atext%20features.%20Conversely%2C%20self-supervised%20visual%20models%20like%20DINO%20excel%20in%0Afine-grained%20visual%20encoding%20but%20lack%20integration%20with%20language.%20To%20bridge%20this%0Agap%2C%20we%20present%20Talk2DINO%2C%20a%20novel%20hybrid%20approach%20that%20combines%20the%20spatial%0Aaccuracy%20of%20DINOv2%20with%20the%20language%20understanding%20of%20CLIP.%20Our%20approach%20aligns%0Athe%20textual%20embeddings%20of%20CLIP%20to%20the%20patch-level%20features%20of%20DINOv2%20through%20a%0Alearned%20mapping%20function%20without%20the%20need%20to%20fine-tune%20the%20underlying%0Abackbones.%20At%20training%20time%2C%20we%20exploit%20the%20attention%20maps%20of%20DINOv2%20to%0Aselectively%20align%20local%20visual%20patches%20with%20textual%20embeddings.%20We%20show%20that%0Athe%20powerful%20semantic%20and%20localization%20abilities%20of%20Talk2DINO%20can%20enhance%20the%0Asegmentation%20process%2C%20resulting%20in%20more%20natural%20and%20less%20noisy%20segmentations%2C%0Aand%20that%20our%20approach%20can%20also%20effectively%20distinguish%20foreground%20objects%20from%0Athe%20background.%20Experimental%20results%20demonstrate%20that%20Talk2DINO%20achieves%0Astate-of-the-art%20performance%20across%20several%20unsupervised%20OVS%20benchmarks.%20Source%0Acode%20and%20models%20are%20publicly%20available%20at%3A%0Ahttps%3A//lorebianchi98.github.io/Talk2DINO/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.19331v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTalking%2520to%2520DINO%253A%2520Bridging%2520Self-Supervised%2520Vision%2520Backbones%2520with%2520Language%250A%2520%2520for%2520Open-Vocabulary%2520Segmentation%26entry.906535625%3DLuca%2520Barsellotti%2520and%2520Lorenzo%2520Bianchi%2520and%2520Nicola%2520Messina%2520and%2520Fabio%2520Carrara%2520and%2520Marcella%2520Cornia%2520and%2520Lorenzo%2520Baraldi%2520and%2520Fabrizio%2520Falchi%2520and%2520Rita%2520Cucchiara%26entry.1292438233%3D%2520%2520Open-Vocabulary%2520Segmentation%2520%2528OVS%2529%2520aims%2520at%2520segmenting%2520images%2520from%2520free-form%250Atextual%2520concepts%2520without%2520predefined%2520training%2520classes.%2520While%2520existing%250Avision-language%2520models%2520such%2520as%2520CLIP%2520can%2520generate%2520segmentation%2520masks%2520by%250Aleveraging%2520coarse%2520spatial%2520information%2520from%2520Vision%2520Transformers%252C%2520they%2520face%250Achallenges%2520in%2520spatial%2520localization%2520due%2520to%2520their%2520global%2520alignment%2520of%2520image%2520and%250Atext%2520features.%2520Conversely%252C%2520self-supervised%2520visual%2520models%2520like%2520DINO%2520excel%2520in%250Afine-grained%2520visual%2520encoding%2520but%2520lack%2520integration%2520with%2520language.%2520To%2520bridge%2520this%250Agap%252C%2520we%2520present%2520Talk2DINO%252C%2520a%2520novel%2520hybrid%2520approach%2520that%2520combines%2520the%2520spatial%250Aaccuracy%2520of%2520DINOv2%2520with%2520the%2520language%2520understanding%2520of%2520CLIP.%2520Our%2520approach%2520aligns%250Athe%2520textual%2520embeddings%2520of%2520CLIP%2520to%2520the%2520patch-level%2520features%2520of%2520DINOv2%2520through%2520a%250Alearned%2520mapping%2520function%2520without%2520the%2520need%2520to%2520fine-tune%2520the%2520underlying%250Abackbones.%2520At%2520training%2520time%252C%2520we%2520exploit%2520the%2520attention%2520maps%2520of%2520DINOv2%2520to%250Aselectively%2520align%2520local%2520visual%2520patches%2520with%2520textual%2520embeddings.%2520We%2520show%2520that%250Athe%2520powerful%2520semantic%2520and%2520localization%2520abilities%2520of%2520Talk2DINO%2520can%2520enhance%2520the%250Asegmentation%2520process%252C%2520resulting%2520in%2520more%2520natural%2520and%2520less%2520noisy%2520segmentations%252C%250Aand%2520that%2520our%2520approach%2520can%2520also%2520effectively%2520distinguish%2520foreground%2520objects%2520from%250Athe%2520background.%2520Experimental%2520results%2520demonstrate%2520that%2520Talk2DINO%2520achieves%250Astate-of-the-art%2520performance%2520across%2520several%2520unsupervised%2520OVS%2520benchmarks.%2520Source%250Acode%2520and%2520models%2520are%2520publicly%2520available%2520at%253A%250Ahttps%253A//lorebianchi98.github.io/Talk2DINO/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.19331v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Talking%20to%20DINO%3A%20Bridging%20Self-Supervised%20Vision%20Backbones%20with%20Language%0A%20%20for%20Open-Vocabulary%20Segmentation&entry.906535625=Luca%20Barsellotti%20and%20Lorenzo%20Bianchi%20and%20Nicola%20Messina%20and%20Fabio%20Carrara%20and%20Marcella%20Cornia%20and%20Lorenzo%20Baraldi%20and%20Fabrizio%20Falchi%20and%20Rita%20Cucchiara&entry.1292438233=%20%20Open-Vocabulary%20Segmentation%20%28OVS%29%20aims%20at%20segmenting%20images%20from%20free-form%0Atextual%20concepts%20without%20predefined%20training%20classes.%20While%20existing%0Avision-language%20models%20such%20as%20CLIP%20can%20generate%20segmentation%20masks%20by%0Aleveraging%20coarse%20spatial%20information%20from%20Vision%20Transformers%2C%20they%20face%0Achallenges%20in%20spatial%20localization%20due%20to%20their%20global%20alignment%20of%20image%20and%0Atext%20features.%20Conversely%2C%20self-supervised%20visual%20models%20like%20DINO%20excel%20in%0Afine-grained%20visual%20encoding%20but%20lack%20integration%20with%20language.%20To%20bridge%20this%0Agap%2C%20we%20present%20Talk2DINO%2C%20a%20novel%20hybrid%20approach%20that%20combines%20the%20spatial%0Aaccuracy%20of%20DINOv2%20with%20the%20language%20understanding%20of%20CLIP.%20Our%20approach%20aligns%0Athe%20textual%20embeddings%20of%20CLIP%20to%20the%20patch-level%20features%20of%20DINOv2%20through%20a%0Alearned%20mapping%20function%20without%20the%20need%20to%20fine-tune%20the%20underlying%0Abackbones.%20At%20training%20time%2C%20we%20exploit%20the%20attention%20maps%20of%20DINOv2%20to%0Aselectively%20align%20local%20visual%20patches%20with%20textual%20embeddings.%20We%20show%20that%0Athe%20powerful%20semantic%20and%20localization%20abilities%20of%20Talk2DINO%20can%20enhance%20the%0Asegmentation%20process%2C%20resulting%20in%20more%20natural%20and%20less%20noisy%20segmentations%2C%0Aand%20that%20our%20approach%20can%20also%20effectively%20distinguish%20foreground%20objects%20from%0Athe%20background.%20Experimental%20results%20demonstrate%20that%20Talk2DINO%20achieves%0Astate-of-the-art%20performance%20across%20several%20unsupervised%20OVS%20benchmarks.%20Source%0Acode%20and%20models%20are%20publicly%20available%20at%3A%0Ahttps%3A//lorebianchi98.github.io/Talk2DINO/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.19331v3&entry.124074799=Read"},
{"title": "HERO: Rethinking Visual Token Early Dropping in High-Resolution Large\n  Vision-Language Models", "author": "Xu Li and Yuxuan Liang and Xiaolei Chen and Yi Zheng and Haotian Chen and Bin Li and Xiangyang Xue", "abstract": "  By cropping high-resolution images into local tiles and encoding them\nindependently, High-Resolution Large Vision-Language Models (HR-LVLMs) have\ndemonstrated remarkable fine-grained visual understanding capabilities.\nHowever, this divide-and-conquer paradigm significantly increases the number of\nvisual tokens, resulting in substantial computational and memory overhead. To\nbetter understand and address this challenge, we empirically investigate visual\ntoken utilization in HR-LVLMs and uncover three key findings: (1) the local\ntiles have varying importance, jointly determined by visual saliency and task\nrelevance; (2) the CLS token in CLIP-based vision encoders exhibits a two-stage\nattention pattern across layers, with each stage attending to different types\nof visual tokens; (3) the visual tokens emphasized at different stages encode\ninformation at varying levels of granularity, playing complementary roles\nwithin LVLMs. Building on these insights, we propose HERO, a High-resolution\nvisual token early dropping framework that integrates content-adaptive token\nbudget allocation with function-aware token selection. By accurately estimating\ntile-level importance and selectively retaining visual tokens with\ncomplementary roles, HERO achieves superior efficiency-accuracy trade-offs\nacross diverse benchmarks and model scales, all in a training-free manner. This\nstudy provides both empirical insights and practical solutions toward efficient\ninference in HR-LVLMs.\n", "link": "http://arxiv.org/abs/2509.13067v1", "date": "2025-09-16", "relevancy": 2.9414, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6205}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5722}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5722}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HERO%3A%20Rethinking%20Visual%20Token%20Early%20Dropping%20in%20High-Resolution%20Large%0A%20%20Vision-Language%20Models&body=Title%3A%20HERO%3A%20Rethinking%20Visual%20Token%20Early%20Dropping%20in%20High-Resolution%20Large%0A%20%20Vision-Language%20Models%0AAuthor%3A%20Xu%20Li%20and%20Yuxuan%20Liang%20and%20Xiaolei%20Chen%20and%20Yi%20Zheng%20and%20Haotian%20Chen%20and%20Bin%20Li%20and%20Xiangyang%20Xue%0AAbstract%3A%20%20%20By%20cropping%20high-resolution%20images%20into%20local%20tiles%20and%20encoding%20them%0Aindependently%2C%20High-Resolution%20Large%20Vision-Language%20Models%20%28HR-LVLMs%29%20have%0Ademonstrated%20remarkable%20fine-grained%20visual%20understanding%20capabilities.%0AHowever%2C%20this%20divide-and-conquer%20paradigm%20significantly%20increases%20the%20number%20of%0Avisual%20tokens%2C%20resulting%20in%20substantial%20computational%20and%20memory%20overhead.%20To%0Abetter%20understand%20and%20address%20this%20challenge%2C%20we%20empirically%20investigate%20visual%0Atoken%20utilization%20in%20HR-LVLMs%20and%20uncover%20three%20key%20findings%3A%20%281%29%20the%20local%0Atiles%20have%20varying%20importance%2C%20jointly%20determined%20by%20visual%20saliency%20and%20task%0Arelevance%3B%20%282%29%20the%20CLS%20token%20in%20CLIP-based%20vision%20encoders%20exhibits%20a%20two-stage%0Aattention%20pattern%20across%20layers%2C%20with%20each%20stage%20attending%20to%20different%20types%0Aof%20visual%20tokens%3B%20%283%29%20the%20visual%20tokens%20emphasized%20at%20different%20stages%20encode%0Ainformation%20at%20varying%20levels%20of%20granularity%2C%20playing%20complementary%20roles%0Awithin%20LVLMs.%20Building%20on%20these%20insights%2C%20we%20propose%20HERO%2C%20a%20High-resolution%0Avisual%20token%20early%20dropping%20framework%20that%20integrates%20content-adaptive%20token%0Abudget%20allocation%20with%20function-aware%20token%20selection.%20By%20accurately%20estimating%0Atile-level%20importance%20and%20selectively%20retaining%20visual%20tokens%20with%0Acomplementary%20roles%2C%20HERO%20achieves%20superior%20efficiency-accuracy%20trade-offs%0Aacross%20diverse%20benchmarks%20and%20model%20scales%2C%20all%20in%20a%20training-free%20manner.%20This%0Astudy%20provides%20both%20empirical%20insights%20and%20practical%20solutions%20toward%20efficient%0Ainference%20in%20HR-LVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13067v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHERO%253A%2520Rethinking%2520Visual%2520Token%2520Early%2520Dropping%2520in%2520High-Resolution%2520Large%250A%2520%2520Vision-Language%2520Models%26entry.906535625%3DXu%2520Li%2520and%2520Yuxuan%2520Liang%2520and%2520Xiaolei%2520Chen%2520and%2520Yi%2520Zheng%2520and%2520Haotian%2520Chen%2520and%2520Bin%2520Li%2520and%2520Xiangyang%2520Xue%26entry.1292438233%3D%2520%2520By%2520cropping%2520high-resolution%2520images%2520into%2520local%2520tiles%2520and%2520encoding%2520them%250Aindependently%252C%2520High-Resolution%2520Large%2520Vision-Language%2520Models%2520%2528HR-LVLMs%2529%2520have%250Ademonstrated%2520remarkable%2520fine-grained%2520visual%2520understanding%2520capabilities.%250AHowever%252C%2520this%2520divide-and-conquer%2520paradigm%2520significantly%2520increases%2520the%2520number%2520of%250Avisual%2520tokens%252C%2520resulting%2520in%2520substantial%2520computational%2520and%2520memory%2520overhead.%2520To%250Abetter%2520understand%2520and%2520address%2520this%2520challenge%252C%2520we%2520empirically%2520investigate%2520visual%250Atoken%2520utilization%2520in%2520HR-LVLMs%2520and%2520uncover%2520three%2520key%2520findings%253A%2520%25281%2529%2520the%2520local%250Atiles%2520have%2520varying%2520importance%252C%2520jointly%2520determined%2520by%2520visual%2520saliency%2520and%2520task%250Arelevance%253B%2520%25282%2529%2520the%2520CLS%2520token%2520in%2520CLIP-based%2520vision%2520encoders%2520exhibits%2520a%2520two-stage%250Aattention%2520pattern%2520across%2520layers%252C%2520with%2520each%2520stage%2520attending%2520to%2520different%2520types%250Aof%2520visual%2520tokens%253B%2520%25283%2529%2520the%2520visual%2520tokens%2520emphasized%2520at%2520different%2520stages%2520encode%250Ainformation%2520at%2520varying%2520levels%2520of%2520granularity%252C%2520playing%2520complementary%2520roles%250Awithin%2520LVLMs.%2520Building%2520on%2520these%2520insights%252C%2520we%2520propose%2520HERO%252C%2520a%2520High-resolution%250Avisual%2520token%2520early%2520dropping%2520framework%2520that%2520integrates%2520content-adaptive%2520token%250Abudget%2520allocation%2520with%2520function-aware%2520token%2520selection.%2520By%2520accurately%2520estimating%250Atile-level%2520importance%2520and%2520selectively%2520retaining%2520visual%2520tokens%2520with%250Acomplementary%2520roles%252C%2520HERO%2520achieves%2520superior%2520efficiency-accuracy%2520trade-offs%250Aacross%2520diverse%2520benchmarks%2520and%2520model%2520scales%252C%2520all%2520in%2520a%2520training-free%2520manner.%2520This%250Astudy%2520provides%2520both%2520empirical%2520insights%2520and%2520practical%2520solutions%2520toward%2520efficient%250Ainference%2520in%2520HR-LVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13067v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HERO%3A%20Rethinking%20Visual%20Token%20Early%20Dropping%20in%20High-Resolution%20Large%0A%20%20Vision-Language%20Models&entry.906535625=Xu%20Li%20and%20Yuxuan%20Liang%20and%20Xiaolei%20Chen%20and%20Yi%20Zheng%20and%20Haotian%20Chen%20and%20Bin%20Li%20and%20Xiangyang%20Xue&entry.1292438233=%20%20By%20cropping%20high-resolution%20images%20into%20local%20tiles%20and%20encoding%20them%0Aindependently%2C%20High-Resolution%20Large%20Vision-Language%20Models%20%28HR-LVLMs%29%20have%0Ademonstrated%20remarkable%20fine-grained%20visual%20understanding%20capabilities.%0AHowever%2C%20this%20divide-and-conquer%20paradigm%20significantly%20increases%20the%20number%20of%0Avisual%20tokens%2C%20resulting%20in%20substantial%20computational%20and%20memory%20overhead.%20To%0Abetter%20understand%20and%20address%20this%20challenge%2C%20we%20empirically%20investigate%20visual%0Atoken%20utilization%20in%20HR-LVLMs%20and%20uncover%20three%20key%20findings%3A%20%281%29%20the%20local%0Atiles%20have%20varying%20importance%2C%20jointly%20determined%20by%20visual%20saliency%20and%20task%0Arelevance%3B%20%282%29%20the%20CLS%20token%20in%20CLIP-based%20vision%20encoders%20exhibits%20a%20two-stage%0Aattention%20pattern%20across%20layers%2C%20with%20each%20stage%20attending%20to%20different%20types%0Aof%20visual%20tokens%3B%20%283%29%20the%20visual%20tokens%20emphasized%20at%20different%20stages%20encode%0Ainformation%20at%20varying%20levels%20of%20granularity%2C%20playing%20complementary%20roles%0Awithin%20LVLMs.%20Building%20on%20these%20insights%2C%20we%20propose%20HERO%2C%20a%20High-resolution%0Avisual%20token%20early%20dropping%20framework%20that%20integrates%20content-adaptive%20token%0Abudget%20allocation%20with%20function-aware%20token%20selection.%20By%20accurately%20estimating%0Atile-level%20importance%20and%20selectively%20retaining%20visual%20tokens%20with%0Acomplementary%20roles%2C%20HERO%20achieves%20superior%20efficiency-accuracy%20trade-offs%0Aacross%20diverse%20benchmarks%20and%20model%20scales%2C%20all%20in%20a%20training-free%20manner.%20This%0Astudy%20provides%20both%20empirical%20insights%20and%20practical%20solutions%20toward%20efficient%0Ainference%20in%20HR-LVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13067v1&entry.124074799=Read"},
{"title": "Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction\n  From Single Images", "author": "Danling Cao", "abstract": "  Recovering 3D face models from 2D in-the-wild images has gained considerable\nattention in the computer vision community due to its wide range of potential\napplications. However, the lack of ground-truth labeled datasets and the\ncomplexity of real-world environments remain significant challenges. In this\nchapter, we propose a convolutional neural network-based approach, the\nHierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face\nmodels from single in-the-wild images. Our model predicts detailed facial\ngeometry, texture, pose, and illumination parameters from a single image.\nSpecifically, we employ a pre-trained hierarchical backbone network and\nintroduce multi-level attention mechanisms at different stages of 2D face image\nfeature extraction. A semi-supervised training strategy is employed,\nincorporating 3D Morphable Model (3DMM) parameters from publicly available\ndatasets along with a differentiable renderer, enabling an end-to-end training\nprocess. Extensive experiments, including both comparative and ablation\nstudies, were conducted on two benchmark datasets, AFLW2000-3D and MICC\nFlorence, focusing on 3D face reconstruction and 3D face alignment tasks. The\neffectiveness of the proposed method was evaluated both quantitatively and\nqualitatively.\n", "link": "http://arxiv.org/abs/2509.10024v2", "date": "2025-09-16", "relevancy": 2.9283, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6026}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5968}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20MLANet%3A%20Multi-level%20Attention%20for%203D%20Face%20Reconstruction%0A%20%20From%20Single%20Images&body=Title%3A%20Hierarchical%20MLANet%3A%20Multi-level%20Attention%20for%203D%20Face%20Reconstruction%0A%20%20From%20Single%20Images%0AAuthor%3A%20Danling%20Cao%0AAbstract%3A%20%20%20Recovering%203D%20face%20models%20from%202D%20in-the-wild%20images%20has%20gained%20considerable%0Aattention%20in%20the%20computer%20vision%20community%20due%20to%20its%20wide%20range%20of%20potential%0Aapplications.%20However%2C%20the%20lack%20of%20ground-truth%20labeled%20datasets%20and%20the%0Acomplexity%20of%20real-world%20environments%20remain%20significant%20challenges.%20In%20this%0Achapter%2C%20we%20propose%20a%20convolutional%20neural%20network-based%20approach%2C%20the%0AHierarchical%20Multi-Level%20Attention%20Network%20%28MLANet%29%2C%20for%20reconstructing%203D%20face%0Amodels%20from%20single%20in-the-wild%20images.%20Our%20model%20predicts%20detailed%20facial%0Ageometry%2C%20texture%2C%20pose%2C%20and%20illumination%20parameters%20from%20a%20single%20image.%0ASpecifically%2C%20we%20employ%20a%20pre-trained%20hierarchical%20backbone%20network%20and%0Aintroduce%20multi-level%20attention%20mechanisms%20at%20different%20stages%20of%202D%20face%20image%0Afeature%20extraction.%20A%20semi-supervised%20training%20strategy%20is%20employed%2C%0Aincorporating%203D%20Morphable%20Model%20%283DMM%29%20parameters%20from%20publicly%20available%0Adatasets%20along%20with%20a%20differentiable%20renderer%2C%20enabling%20an%20end-to-end%20training%0Aprocess.%20Extensive%20experiments%2C%20including%20both%20comparative%20and%20ablation%0Astudies%2C%20were%20conducted%20on%20two%20benchmark%20datasets%2C%20AFLW2000-3D%20and%20MICC%0AFlorence%2C%20focusing%20on%203D%20face%20reconstruction%20and%203D%20face%20alignment%20tasks.%20The%0Aeffectiveness%20of%20the%20proposed%20method%20was%20evaluated%20both%20quantitatively%20and%0Aqualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.10024v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520MLANet%253A%2520Multi-level%2520Attention%2520for%25203D%2520Face%2520Reconstruction%250A%2520%2520From%2520Single%2520Images%26entry.906535625%3DDanling%2520Cao%26entry.1292438233%3D%2520%2520Recovering%25203D%2520face%2520models%2520from%25202D%2520in-the-wild%2520images%2520has%2520gained%2520considerable%250Aattention%2520in%2520the%2520computer%2520vision%2520community%2520due%2520to%2520its%2520wide%2520range%2520of%2520potential%250Aapplications.%2520However%252C%2520the%2520lack%2520of%2520ground-truth%2520labeled%2520datasets%2520and%2520the%250Acomplexity%2520of%2520real-world%2520environments%2520remain%2520significant%2520challenges.%2520In%2520this%250Achapter%252C%2520we%2520propose%2520a%2520convolutional%2520neural%2520network-based%2520approach%252C%2520the%250AHierarchical%2520Multi-Level%2520Attention%2520Network%2520%2528MLANet%2529%252C%2520for%2520reconstructing%25203D%2520face%250Amodels%2520from%2520single%2520in-the-wild%2520images.%2520Our%2520model%2520predicts%2520detailed%2520facial%250Ageometry%252C%2520texture%252C%2520pose%252C%2520and%2520illumination%2520parameters%2520from%2520a%2520single%2520image.%250ASpecifically%252C%2520we%2520employ%2520a%2520pre-trained%2520hierarchical%2520backbone%2520network%2520and%250Aintroduce%2520multi-level%2520attention%2520mechanisms%2520at%2520different%2520stages%2520of%25202D%2520face%2520image%250Afeature%2520extraction.%2520A%2520semi-supervised%2520training%2520strategy%2520is%2520employed%252C%250Aincorporating%25203D%2520Morphable%2520Model%2520%25283DMM%2529%2520parameters%2520from%2520publicly%2520available%250Adatasets%2520along%2520with%2520a%2520differentiable%2520renderer%252C%2520enabling%2520an%2520end-to-end%2520training%250Aprocess.%2520Extensive%2520experiments%252C%2520including%2520both%2520comparative%2520and%2520ablation%250Astudies%252C%2520were%2520conducted%2520on%2520two%2520benchmark%2520datasets%252C%2520AFLW2000-3D%2520and%2520MICC%250AFlorence%252C%2520focusing%2520on%25203D%2520face%2520reconstruction%2520and%25203D%2520face%2520alignment%2520tasks.%2520The%250Aeffectiveness%2520of%2520the%2520proposed%2520method%2520was%2520evaluated%2520both%2520quantitatively%2520and%250Aqualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10024v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20MLANet%3A%20Multi-level%20Attention%20for%203D%20Face%20Reconstruction%0A%20%20From%20Single%20Images&entry.906535625=Danling%20Cao&entry.1292438233=%20%20Recovering%203D%20face%20models%20from%202D%20in-the-wild%20images%20has%20gained%20considerable%0Aattention%20in%20the%20computer%20vision%20community%20due%20to%20its%20wide%20range%20of%20potential%0Aapplications.%20However%2C%20the%20lack%20of%20ground-truth%20labeled%20datasets%20and%20the%0Acomplexity%20of%20real-world%20environments%20remain%20significant%20challenges.%20In%20this%0Achapter%2C%20we%20propose%20a%20convolutional%20neural%20network-based%20approach%2C%20the%0AHierarchical%20Multi-Level%20Attention%20Network%20%28MLANet%29%2C%20for%20reconstructing%203D%20face%0Amodels%20from%20single%20in-the-wild%20images.%20Our%20model%20predicts%20detailed%20facial%0Ageometry%2C%20texture%2C%20pose%2C%20and%20illumination%20parameters%20from%20a%20single%20image.%0ASpecifically%2C%20we%20employ%20a%20pre-trained%20hierarchical%20backbone%20network%20and%0Aintroduce%20multi-level%20attention%20mechanisms%20at%20different%20stages%20of%202D%20face%20image%0Afeature%20extraction.%20A%20semi-supervised%20training%20strategy%20is%20employed%2C%0Aincorporating%203D%20Morphable%20Model%20%283DMM%29%20parameters%20from%20publicly%20available%0Adatasets%20along%20with%20a%20differentiable%20renderer%2C%20enabling%20an%20end-to-end%20training%0Aprocess.%20Extensive%20experiments%2C%20including%20both%20comparative%20and%20ablation%0Astudies%2C%20were%20conducted%20on%20two%20benchmark%20datasets%2C%20AFLW2000-3D%20and%20MICC%0AFlorence%2C%20focusing%20on%203D%20face%20reconstruction%20and%203D%20face%20alignment%20tasks.%20The%0Aeffectiveness%20of%20the%20proposed%20method%20was%20evaluated%20both%20quantitatively%20and%0Aqualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.10024v2&entry.124074799=Read"},
{"title": "More performant and scalable: Rethinking contrastive vision-language\n  pre-training of radiology in the LLM era", "author": "Yingtai Li and Haoran Lai and Xiaoqian Zhou and Shuai Ming and Wenxin Ma and Wei Wei and Shaohua Kevin Zhou", "abstract": "  The emergence of Large Language Models (LLMs) presents unprecedented\nopportunities to revolutionize medical contrastive vision-language\npre-training. In this paper, we show how LLMs can facilitate large-scale\nsupervised pre-training, thereby advancing vision-language alignment. We begin\nby demonstrate that modern LLMs can automatically extract diagnostic labels\nfrom radiology reports with remarkable precision (>96\\% AUC in our experiments)\nwithout complex prompt engineering, enabling the creation of large-scale\n\"silver-standard\" datasets at a minimal cost (~\\$3 for 50k CT image-report\npairs). Further, we find that vision encoder trained on this \"silver-standard\"\ndataset achieves performance comparable to those trained on labels extracted by\nspecialized BERT-based models, thereby democratizing the access to large-scale\nsupervised pre-training. Building on this foundation, we proceed to reveal that\nsupervised pre-training fundamentally improves contrastive vision-language\nalignment. Our approach achieves state-of-the-art performance using only a 3D\nResNet-18 with vanilla CLIP training, including 83.8\\% AUC for zero-shot\ndiagnosis on CT-RATE, 77.3\\% AUC on RAD-ChestCT, and substantial improvements\nin cross-modal retrieval (MAP@50=53.7\\% for image-image, Recall@100=52.2\\% for\nreport-image). These results demonstrate the potential of utilizing LLMs to\nfacilitate {\\bf more performant and scalable} medical AI systems. Our code is\navaiable at https://github.com/SadVoxel/More-performant-and-scalable.\n", "link": "http://arxiv.org/abs/2509.13175v1", "date": "2025-09-16", "relevancy": 2.9235, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5859}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5859}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20More%20performant%20and%20scalable%3A%20Rethinking%20contrastive%20vision-language%0A%20%20pre-training%20of%20radiology%20in%20the%20LLM%20era&body=Title%3A%20More%20performant%20and%20scalable%3A%20Rethinking%20contrastive%20vision-language%0A%20%20pre-training%20of%20radiology%20in%20the%20LLM%20era%0AAuthor%3A%20Yingtai%20Li%20and%20Haoran%20Lai%20and%20Xiaoqian%20Zhou%20and%20Shuai%20Ming%20and%20Wenxin%20Ma%20and%20Wei%20Wei%20and%20Shaohua%20Kevin%20Zhou%0AAbstract%3A%20%20%20The%20emergence%20of%20Large%20Language%20Models%20%28LLMs%29%20presents%20unprecedented%0Aopportunities%20to%20revolutionize%20medical%20contrastive%20vision-language%0Apre-training.%20In%20this%20paper%2C%20we%20show%20how%20LLMs%20can%20facilitate%20large-scale%0Asupervised%20pre-training%2C%20thereby%20advancing%20vision-language%20alignment.%20We%20begin%0Aby%20demonstrate%20that%20modern%20LLMs%20can%20automatically%20extract%20diagnostic%20labels%0Afrom%20radiology%20reports%20with%20remarkable%20precision%20%28%3E96%5C%25%20AUC%20in%20our%20experiments%29%0Awithout%20complex%20prompt%20engineering%2C%20enabling%20the%20creation%20of%20large-scale%0A%22silver-standard%22%20datasets%20at%20a%20minimal%20cost%20%28~%5C%243%20for%2050k%20CT%20image-report%0Apairs%29.%20Further%2C%20we%20find%20that%20vision%20encoder%20trained%20on%20this%20%22silver-standard%22%0Adataset%20achieves%20performance%20comparable%20to%20those%20trained%20on%20labels%20extracted%20by%0Aspecialized%20BERT-based%20models%2C%20thereby%20democratizing%20the%20access%20to%20large-scale%0Asupervised%20pre-training.%20Building%20on%20this%20foundation%2C%20we%20proceed%20to%20reveal%20that%0Asupervised%20pre-training%20fundamentally%20improves%20contrastive%20vision-language%0Aalignment.%20Our%20approach%20achieves%20state-of-the-art%20performance%20using%20only%20a%203D%0AResNet-18%20with%20vanilla%20CLIP%20training%2C%20including%2083.8%5C%25%20AUC%20for%20zero-shot%0Adiagnosis%20on%20CT-RATE%2C%2077.3%5C%25%20AUC%20on%20RAD-ChestCT%2C%20and%20substantial%20improvements%0Ain%20cross-modal%20retrieval%20%28MAP%4050%3D53.7%5C%25%20for%20image-image%2C%20Recall%40100%3D52.2%5C%25%20for%0Areport-image%29.%20These%20results%20demonstrate%20the%20potential%20of%20utilizing%20LLMs%20to%0Afacilitate%20%7B%5Cbf%20more%20performant%20and%20scalable%7D%20medical%20AI%20systems.%20Our%20code%20is%0Aavaiable%20at%20https%3A//github.com/SadVoxel/More-performant-and-scalable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13175v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMore%2520performant%2520and%2520scalable%253A%2520Rethinking%2520contrastive%2520vision-language%250A%2520%2520pre-training%2520of%2520radiology%2520in%2520the%2520LLM%2520era%26entry.906535625%3DYingtai%2520Li%2520and%2520Haoran%2520Lai%2520and%2520Xiaoqian%2520Zhou%2520and%2520Shuai%2520Ming%2520and%2520Wenxin%2520Ma%2520and%2520Wei%2520Wei%2520and%2520Shaohua%2520Kevin%2520Zhou%26entry.1292438233%3D%2520%2520The%2520emergence%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520presents%2520unprecedented%250Aopportunities%2520to%2520revolutionize%2520medical%2520contrastive%2520vision-language%250Apre-training.%2520In%2520this%2520paper%252C%2520we%2520show%2520how%2520LLMs%2520can%2520facilitate%2520large-scale%250Asupervised%2520pre-training%252C%2520thereby%2520advancing%2520vision-language%2520alignment.%2520We%2520begin%250Aby%2520demonstrate%2520that%2520modern%2520LLMs%2520can%2520automatically%2520extract%2520diagnostic%2520labels%250Afrom%2520radiology%2520reports%2520with%2520remarkable%2520precision%2520%2528%253E96%255C%2525%2520AUC%2520in%2520our%2520experiments%2529%250Awithout%2520complex%2520prompt%2520engineering%252C%2520enabling%2520the%2520creation%2520of%2520large-scale%250A%2522silver-standard%2522%2520datasets%2520at%2520a%2520minimal%2520cost%2520%2528~%255C%25243%2520for%252050k%2520CT%2520image-report%250Apairs%2529.%2520Further%252C%2520we%2520find%2520that%2520vision%2520encoder%2520trained%2520on%2520this%2520%2522silver-standard%2522%250Adataset%2520achieves%2520performance%2520comparable%2520to%2520those%2520trained%2520on%2520labels%2520extracted%2520by%250Aspecialized%2520BERT-based%2520models%252C%2520thereby%2520democratizing%2520the%2520access%2520to%2520large-scale%250Asupervised%2520pre-training.%2520Building%2520on%2520this%2520foundation%252C%2520we%2520proceed%2520to%2520reveal%2520that%250Asupervised%2520pre-training%2520fundamentally%2520improves%2520contrastive%2520vision-language%250Aalignment.%2520Our%2520approach%2520achieves%2520state-of-the-art%2520performance%2520using%2520only%2520a%25203D%250AResNet-18%2520with%2520vanilla%2520CLIP%2520training%252C%2520including%252083.8%255C%2525%2520AUC%2520for%2520zero-shot%250Adiagnosis%2520on%2520CT-RATE%252C%252077.3%255C%2525%2520AUC%2520on%2520RAD-ChestCT%252C%2520and%2520substantial%2520improvements%250Ain%2520cross-modal%2520retrieval%2520%2528MAP%254050%253D53.7%255C%2525%2520for%2520image-image%252C%2520Recall%2540100%253D52.2%255C%2525%2520for%250Areport-image%2529.%2520These%2520results%2520demonstrate%2520the%2520potential%2520of%2520utilizing%2520LLMs%2520to%250Afacilitate%2520%257B%255Cbf%2520more%2520performant%2520and%2520scalable%257D%2520medical%2520AI%2520systems.%2520Our%2520code%2520is%250Aavaiable%2520at%2520https%253A//github.com/SadVoxel/More-performant-and-scalable.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13175v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=More%20performant%20and%20scalable%3A%20Rethinking%20contrastive%20vision-language%0A%20%20pre-training%20of%20radiology%20in%20the%20LLM%20era&entry.906535625=Yingtai%20Li%20and%20Haoran%20Lai%20and%20Xiaoqian%20Zhou%20and%20Shuai%20Ming%20and%20Wenxin%20Ma%20and%20Wei%20Wei%20and%20Shaohua%20Kevin%20Zhou&entry.1292438233=%20%20The%20emergence%20of%20Large%20Language%20Models%20%28LLMs%29%20presents%20unprecedented%0Aopportunities%20to%20revolutionize%20medical%20contrastive%20vision-language%0Apre-training.%20In%20this%20paper%2C%20we%20show%20how%20LLMs%20can%20facilitate%20large-scale%0Asupervised%20pre-training%2C%20thereby%20advancing%20vision-language%20alignment.%20We%20begin%0Aby%20demonstrate%20that%20modern%20LLMs%20can%20automatically%20extract%20diagnostic%20labels%0Afrom%20radiology%20reports%20with%20remarkable%20precision%20%28%3E96%5C%25%20AUC%20in%20our%20experiments%29%0Awithout%20complex%20prompt%20engineering%2C%20enabling%20the%20creation%20of%20large-scale%0A%22silver-standard%22%20datasets%20at%20a%20minimal%20cost%20%28~%5C%243%20for%2050k%20CT%20image-report%0Apairs%29.%20Further%2C%20we%20find%20that%20vision%20encoder%20trained%20on%20this%20%22silver-standard%22%0Adataset%20achieves%20performance%20comparable%20to%20those%20trained%20on%20labels%20extracted%20by%0Aspecialized%20BERT-based%20models%2C%20thereby%20democratizing%20the%20access%20to%20large-scale%0Asupervised%20pre-training.%20Building%20on%20this%20foundation%2C%20we%20proceed%20to%20reveal%20that%0Asupervised%20pre-training%20fundamentally%20improves%20contrastive%20vision-language%0Aalignment.%20Our%20approach%20achieves%20state-of-the-art%20performance%20using%20only%20a%203D%0AResNet-18%20with%20vanilla%20CLIP%20training%2C%20including%2083.8%5C%25%20AUC%20for%20zero-shot%0Adiagnosis%20on%20CT-RATE%2C%2077.3%5C%25%20AUC%20on%20RAD-ChestCT%2C%20and%20substantial%20improvements%0Ain%20cross-modal%20retrieval%20%28MAP%4050%3D53.7%5C%25%20for%20image-image%2C%20Recall%40100%3D52.2%5C%25%20for%0Areport-image%29.%20These%20results%20demonstrate%20the%20potential%20of%20utilizing%20LLMs%20to%0Afacilitate%20%7B%5Cbf%20more%20performant%20and%20scalable%7D%20medical%20AI%20systems.%20Our%20code%20is%0Aavaiable%20at%20https%3A//github.com/SadVoxel/More-performant-and-scalable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13175v1&entry.124074799=Read"},
{"title": "MSMA: Multi-Scale Feature Fusion For Multi-Attribute 3D Face\n  Reconstruction From Unconstrained Images", "author": "Danling Cao", "abstract": "  Reconstructing 3D face from a single unconstrained image remains a\nchallenging problem due to diverse conditions in unconstrained environments.\nRecently, learning-based methods have achieved notable results by effectively\ncapturing complex facial structures and details across varying conditions.\nConsequently, many existing approaches employ projection-based losses between\ngenerated and input images to constrain model training. However, learning-based\nmethods for 3D face reconstruction typically require substantial amounts of 3D\nfacial data, which is difficult and costly to obtain. Consequently, to reduce\nreliance on labeled 3D face datasets, many existing approaches employ\nprojection-based losses between generated and input images to constrain model\ntraining. Nonetheless, despite these advancements, existing approaches\nfrequently struggle to capture detailed and multi-scale features under diverse\nfacial attributes and conditions, leading to incomplete or less accurate\nreconstructions. In this paper, we propose a Multi-Scale Feature Fusion with\nMulti-Attribute (MSMA) framework for 3D face reconstruction from unconstrained\nimages. Our method integrates multi-scale feature fusion with a focus on\nmulti-attribute learning and leverages a large-kernel attention module to\nenhance the precision of feature extraction across scales, enabling accurate 3D\nfacial parameter estimation from a single 2D image. Comprehensive experiments\non the MICC Florence, Facewarehouse and custom-collect datasets demonstrate\nthat our approach achieves results on par with current state-of-the-art\nmethods, and in some instances, surpasses SOTA performance across challenging\nconditions.\n", "link": "http://arxiv.org/abs/2509.11763v2", "date": "2025-09-16", "relevancy": 2.9213, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5965}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5781}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5781}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSMA%3A%20Multi-Scale%20Feature%20Fusion%20For%20Multi-Attribute%203D%20Face%0A%20%20Reconstruction%20From%20Unconstrained%20Images&body=Title%3A%20MSMA%3A%20Multi-Scale%20Feature%20Fusion%20For%20Multi-Attribute%203D%20Face%0A%20%20Reconstruction%20From%20Unconstrained%20Images%0AAuthor%3A%20Danling%20Cao%0AAbstract%3A%20%20%20Reconstructing%203D%20face%20from%20a%20single%20unconstrained%20image%20remains%20a%0Achallenging%20problem%20due%20to%20diverse%20conditions%20in%20unconstrained%20environments.%0ARecently%2C%20learning-based%20methods%20have%20achieved%20notable%20results%20by%20effectively%0Acapturing%20complex%20facial%20structures%20and%20details%20across%20varying%20conditions.%0AConsequently%2C%20many%20existing%20approaches%20employ%20projection-based%20losses%20between%0Agenerated%20and%20input%20images%20to%20constrain%20model%20training.%20However%2C%20learning-based%0Amethods%20for%203D%20face%20reconstruction%20typically%20require%20substantial%20amounts%20of%203D%0Afacial%20data%2C%20which%20is%20difficult%20and%20costly%20to%20obtain.%20Consequently%2C%20to%20reduce%0Areliance%20on%20labeled%203D%20face%20datasets%2C%20many%20existing%20approaches%20employ%0Aprojection-based%20losses%20between%20generated%20and%20input%20images%20to%20constrain%20model%0Atraining.%20Nonetheless%2C%20despite%20these%20advancements%2C%20existing%20approaches%0Afrequently%20struggle%20to%20capture%20detailed%20and%20multi-scale%20features%20under%20diverse%0Afacial%20attributes%20and%20conditions%2C%20leading%20to%20incomplete%20or%20less%20accurate%0Areconstructions.%20In%20this%20paper%2C%20we%20propose%20a%20Multi-Scale%20Feature%20Fusion%20with%0AMulti-Attribute%20%28MSMA%29%20framework%20for%203D%20face%20reconstruction%20from%20unconstrained%0Aimages.%20Our%20method%20integrates%20multi-scale%20feature%20fusion%20with%20a%20focus%20on%0Amulti-attribute%20learning%20and%20leverages%20a%20large-kernel%20attention%20module%20to%0Aenhance%20the%20precision%20of%20feature%20extraction%20across%20scales%2C%20enabling%20accurate%203D%0Afacial%20parameter%20estimation%20from%20a%20single%202D%20image.%20Comprehensive%20experiments%0Aon%20the%20MICC%20Florence%2C%20Facewarehouse%20and%20custom-collect%20datasets%20demonstrate%0Athat%20our%20approach%20achieves%20results%20on%20par%20with%20current%20state-of-the-art%0Amethods%2C%20and%20in%20some%20instances%2C%20surpasses%20SOTA%20performance%20across%20challenging%0Aconditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11763v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSMA%253A%2520Multi-Scale%2520Feature%2520Fusion%2520For%2520Multi-Attribute%25203D%2520Face%250A%2520%2520Reconstruction%2520From%2520Unconstrained%2520Images%26entry.906535625%3DDanling%2520Cao%26entry.1292438233%3D%2520%2520Reconstructing%25203D%2520face%2520from%2520a%2520single%2520unconstrained%2520image%2520remains%2520a%250Achallenging%2520problem%2520due%2520to%2520diverse%2520conditions%2520in%2520unconstrained%2520environments.%250ARecently%252C%2520learning-based%2520methods%2520have%2520achieved%2520notable%2520results%2520by%2520effectively%250Acapturing%2520complex%2520facial%2520structures%2520and%2520details%2520across%2520varying%2520conditions.%250AConsequently%252C%2520many%2520existing%2520approaches%2520employ%2520projection-based%2520losses%2520between%250Agenerated%2520and%2520input%2520images%2520to%2520constrain%2520model%2520training.%2520However%252C%2520learning-based%250Amethods%2520for%25203D%2520face%2520reconstruction%2520typically%2520require%2520substantial%2520amounts%2520of%25203D%250Afacial%2520data%252C%2520which%2520is%2520difficult%2520and%2520costly%2520to%2520obtain.%2520Consequently%252C%2520to%2520reduce%250Areliance%2520on%2520labeled%25203D%2520face%2520datasets%252C%2520many%2520existing%2520approaches%2520employ%250Aprojection-based%2520losses%2520between%2520generated%2520and%2520input%2520images%2520to%2520constrain%2520model%250Atraining.%2520Nonetheless%252C%2520despite%2520these%2520advancements%252C%2520existing%2520approaches%250Afrequently%2520struggle%2520to%2520capture%2520detailed%2520and%2520multi-scale%2520features%2520under%2520diverse%250Afacial%2520attributes%2520and%2520conditions%252C%2520leading%2520to%2520incomplete%2520or%2520less%2520accurate%250Areconstructions.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520Multi-Scale%2520Feature%2520Fusion%2520with%250AMulti-Attribute%2520%2528MSMA%2529%2520framework%2520for%25203D%2520face%2520reconstruction%2520from%2520unconstrained%250Aimages.%2520Our%2520method%2520integrates%2520multi-scale%2520feature%2520fusion%2520with%2520a%2520focus%2520on%250Amulti-attribute%2520learning%2520and%2520leverages%2520a%2520large-kernel%2520attention%2520module%2520to%250Aenhance%2520the%2520precision%2520of%2520feature%2520extraction%2520across%2520scales%252C%2520enabling%2520accurate%25203D%250Afacial%2520parameter%2520estimation%2520from%2520a%2520single%25202D%2520image.%2520Comprehensive%2520experiments%250Aon%2520the%2520MICC%2520Florence%252C%2520Facewarehouse%2520and%2520custom-collect%2520datasets%2520demonstrate%250Athat%2520our%2520approach%2520achieves%2520results%2520on%2520par%2520with%2520current%2520state-of-the-art%250Amethods%252C%2520and%2520in%2520some%2520instances%252C%2520surpasses%2520SOTA%2520performance%2520across%2520challenging%250Aconditions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11763v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSMA%3A%20Multi-Scale%20Feature%20Fusion%20For%20Multi-Attribute%203D%20Face%0A%20%20Reconstruction%20From%20Unconstrained%20Images&entry.906535625=Danling%20Cao&entry.1292438233=%20%20Reconstructing%203D%20face%20from%20a%20single%20unconstrained%20image%20remains%20a%0Achallenging%20problem%20due%20to%20diverse%20conditions%20in%20unconstrained%20environments.%0ARecently%2C%20learning-based%20methods%20have%20achieved%20notable%20results%20by%20effectively%0Acapturing%20complex%20facial%20structures%20and%20details%20across%20varying%20conditions.%0AConsequently%2C%20many%20existing%20approaches%20employ%20projection-based%20losses%20between%0Agenerated%20and%20input%20images%20to%20constrain%20model%20training.%20However%2C%20learning-based%0Amethods%20for%203D%20face%20reconstruction%20typically%20require%20substantial%20amounts%20of%203D%0Afacial%20data%2C%20which%20is%20difficult%20and%20costly%20to%20obtain.%20Consequently%2C%20to%20reduce%0Areliance%20on%20labeled%203D%20face%20datasets%2C%20many%20existing%20approaches%20employ%0Aprojection-based%20losses%20between%20generated%20and%20input%20images%20to%20constrain%20model%0Atraining.%20Nonetheless%2C%20despite%20these%20advancements%2C%20existing%20approaches%0Afrequently%20struggle%20to%20capture%20detailed%20and%20multi-scale%20features%20under%20diverse%0Afacial%20attributes%20and%20conditions%2C%20leading%20to%20incomplete%20or%20less%20accurate%0Areconstructions.%20In%20this%20paper%2C%20we%20propose%20a%20Multi-Scale%20Feature%20Fusion%20with%0AMulti-Attribute%20%28MSMA%29%20framework%20for%203D%20face%20reconstruction%20from%20unconstrained%0Aimages.%20Our%20method%20integrates%20multi-scale%20feature%20fusion%20with%20a%20focus%20on%0Amulti-attribute%20learning%20and%20leverages%20a%20large-kernel%20attention%20module%20to%0Aenhance%20the%20precision%20of%20feature%20extraction%20across%20scales%2C%20enabling%20accurate%203D%0Afacial%20parameter%20estimation%20from%20a%20single%202D%20image.%20Comprehensive%20experiments%0Aon%20the%20MICC%20Florence%2C%20Facewarehouse%20and%20custom-collect%20datasets%20demonstrate%0Athat%20our%20approach%20achieves%20results%20on%20par%20with%20current%20state-of-the-art%0Amethods%2C%20and%20in%20some%20instances%2C%20surpasses%20SOTA%20performance%20across%20challenging%0Aconditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11763v2&entry.124074799=Read"},
{"title": "Cross-Layer Vision Smoothing: Enhancing Visual Understanding via\n  Sustained Focus on Key Objects in Large Vision-Language Models", "author": "Jianfei Zhao and Feng Zhang and Xin Sun and Lingxing Kong and Zhixing Tan and Chong Feng", "abstract": "  Large Vision-Language Models (LVLMs) can accurately locate key objects in\nimages, yet their attention to these objects tends to be very brief. Motivated\nby the hypothesis that sustained focus on key objects can improve LVLMs' visual\ncapabilities, we propose Cross-Layer Vision Smoothing (CLVS). The core idea of\nCLVS is to incorporate a vision memory that smooths the attention distribution\nacross layers. Specifically, we initialize this vision memory with\nposition-unbiased visual attention in the first layer. In subsequent layers,\nthe model's visual attention jointly considers the vision memory from previous\nlayers, while the memory is updated iteratively, thereby maintaining smooth\nattention on key objects. Given that visual understanding primarily occurs in\nthe early and middle layers of the model, we use uncertainty as an indicator of\ncompleted visual understanding and terminate the smoothing process accordingly.\nExperiments on four benchmarks across three LVLMs confirm the effectiveness and\ngeneralizability of our method. CLVS achieves state-of-the-art performance on a\nvariety of visual understanding tasks, with particularly significant\nimprovements in relation and attribute understanding.\n", "link": "http://arxiv.org/abs/2509.12897v1", "date": "2025-09-16", "relevancy": 2.9091, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5902}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5651}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Layer%20Vision%20Smoothing%3A%20Enhancing%20Visual%20Understanding%20via%0A%20%20Sustained%20Focus%20on%20Key%20Objects%20in%20Large%20Vision-Language%20Models&body=Title%3A%20Cross-Layer%20Vision%20Smoothing%3A%20Enhancing%20Visual%20Understanding%20via%0A%20%20Sustained%20Focus%20on%20Key%20Objects%20in%20Large%20Vision-Language%20Models%0AAuthor%3A%20Jianfei%20Zhao%20and%20Feng%20Zhang%20and%20Xin%20Sun%20and%20Lingxing%20Kong%20and%20Zhixing%20Tan%20and%20Chong%20Feng%0AAbstract%3A%20%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20can%20accurately%20locate%20key%20objects%20in%0Aimages%2C%20yet%20their%20attention%20to%20these%20objects%20tends%20to%20be%20very%20brief.%20Motivated%0Aby%20the%20hypothesis%20that%20sustained%20focus%20on%20key%20objects%20can%20improve%20LVLMs%27%20visual%0Acapabilities%2C%20we%20propose%20Cross-Layer%20Vision%20Smoothing%20%28CLVS%29.%20The%20core%20idea%20of%0ACLVS%20is%20to%20incorporate%20a%20vision%20memory%20that%20smooths%20the%20attention%20distribution%0Aacross%20layers.%20Specifically%2C%20we%20initialize%20this%20vision%20memory%20with%0Aposition-unbiased%20visual%20attention%20in%20the%20first%20layer.%20In%20subsequent%20layers%2C%0Athe%20model%27s%20visual%20attention%20jointly%20considers%20the%20vision%20memory%20from%20previous%0Alayers%2C%20while%20the%20memory%20is%20updated%20iteratively%2C%20thereby%20maintaining%20smooth%0Aattention%20on%20key%20objects.%20Given%20that%20visual%20understanding%20primarily%20occurs%20in%0Athe%20early%20and%20middle%20layers%20of%20the%20model%2C%20we%20use%20uncertainty%20as%20an%20indicator%20of%0Acompleted%20visual%20understanding%20and%20terminate%20the%20smoothing%20process%20accordingly.%0AExperiments%20on%20four%20benchmarks%20across%20three%20LVLMs%20confirm%20the%20effectiveness%20and%0Ageneralizability%20of%20our%20method.%20CLVS%20achieves%20state-of-the-art%20performance%20on%20a%0Avariety%20of%20visual%20understanding%20tasks%2C%20with%20particularly%20significant%0Aimprovements%20in%20relation%20and%20attribute%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12897v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Layer%2520Vision%2520Smoothing%253A%2520Enhancing%2520Visual%2520Understanding%2520via%250A%2520%2520Sustained%2520Focus%2520on%2520Key%2520Objects%2520in%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DJianfei%2520Zhao%2520and%2520Feng%2520Zhang%2520and%2520Xin%2520Sun%2520and%2520Lingxing%2520Kong%2520and%2520Zhixing%2520Tan%2520and%2520Chong%2520Feng%26entry.1292438233%3D%2520%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520can%2520accurately%2520locate%2520key%2520objects%2520in%250Aimages%252C%2520yet%2520their%2520attention%2520to%2520these%2520objects%2520tends%2520to%2520be%2520very%2520brief.%2520Motivated%250Aby%2520the%2520hypothesis%2520that%2520sustained%2520focus%2520on%2520key%2520objects%2520can%2520improve%2520LVLMs%2527%2520visual%250Acapabilities%252C%2520we%2520propose%2520Cross-Layer%2520Vision%2520Smoothing%2520%2528CLVS%2529.%2520The%2520core%2520idea%2520of%250ACLVS%2520is%2520to%2520incorporate%2520a%2520vision%2520memory%2520that%2520smooths%2520the%2520attention%2520distribution%250Aacross%2520layers.%2520Specifically%252C%2520we%2520initialize%2520this%2520vision%2520memory%2520with%250Aposition-unbiased%2520visual%2520attention%2520in%2520the%2520first%2520layer.%2520In%2520subsequent%2520layers%252C%250Athe%2520model%2527s%2520visual%2520attention%2520jointly%2520considers%2520the%2520vision%2520memory%2520from%2520previous%250Alayers%252C%2520while%2520the%2520memory%2520is%2520updated%2520iteratively%252C%2520thereby%2520maintaining%2520smooth%250Aattention%2520on%2520key%2520objects.%2520Given%2520that%2520visual%2520understanding%2520primarily%2520occurs%2520in%250Athe%2520early%2520and%2520middle%2520layers%2520of%2520the%2520model%252C%2520we%2520use%2520uncertainty%2520as%2520an%2520indicator%2520of%250Acompleted%2520visual%2520understanding%2520and%2520terminate%2520the%2520smoothing%2520process%2520accordingly.%250AExperiments%2520on%2520four%2520benchmarks%2520across%2520three%2520LVLMs%2520confirm%2520the%2520effectiveness%2520and%250Ageneralizability%2520of%2520our%2520method.%2520CLVS%2520achieves%2520state-of-the-art%2520performance%2520on%2520a%250Avariety%2520of%2520visual%2520understanding%2520tasks%252C%2520with%2520particularly%2520significant%250Aimprovements%2520in%2520relation%2520and%2520attribute%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12897v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Layer%20Vision%20Smoothing%3A%20Enhancing%20Visual%20Understanding%20via%0A%20%20Sustained%20Focus%20on%20Key%20Objects%20in%20Large%20Vision-Language%20Models&entry.906535625=Jianfei%20Zhao%20and%20Feng%20Zhang%20and%20Xin%20Sun%20and%20Lingxing%20Kong%20and%20Zhixing%20Tan%20and%20Chong%20Feng&entry.1292438233=%20%20Large%20Vision-Language%20Models%20%28LVLMs%29%20can%20accurately%20locate%20key%20objects%20in%0Aimages%2C%20yet%20their%20attention%20to%20these%20objects%20tends%20to%20be%20very%20brief.%20Motivated%0Aby%20the%20hypothesis%20that%20sustained%20focus%20on%20key%20objects%20can%20improve%20LVLMs%27%20visual%0Acapabilities%2C%20we%20propose%20Cross-Layer%20Vision%20Smoothing%20%28CLVS%29.%20The%20core%20idea%20of%0ACLVS%20is%20to%20incorporate%20a%20vision%20memory%20that%20smooths%20the%20attention%20distribution%0Aacross%20layers.%20Specifically%2C%20we%20initialize%20this%20vision%20memory%20with%0Aposition-unbiased%20visual%20attention%20in%20the%20first%20layer.%20In%20subsequent%20layers%2C%0Athe%20model%27s%20visual%20attention%20jointly%20considers%20the%20vision%20memory%20from%20previous%0Alayers%2C%20while%20the%20memory%20is%20updated%20iteratively%2C%20thereby%20maintaining%20smooth%0Aattention%20on%20key%20objects.%20Given%20that%20visual%20understanding%20primarily%20occurs%20in%0Athe%20early%20and%20middle%20layers%20of%20the%20model%2C%20we%20use%20uncertainty%20as%20an%20indicator%20of%0Acompleted%20visual%20understanding%20and%20terminate%20the%20smoothing%20process%20accordingly.%0AExperiments%20on%20four%20benchmarks%20across%20three%20LVLMs%20confirm%20the%20effectiveness%20and%0Ageneralizability%20of%20our%20method.%20CLVS%20achieves%20state-of-the-art%20performance%20on%20a%0Avariety%20of%20visual%20understanding%20tasks%2C%20with%20particularly%20significant%0Aimprovements%20in%20relation%20and%20attribute%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12897v1&entry.124074799=Read"},
{"title": "CryoSplat: Gaussian Splatting for Cryo-EM Homogeneous Reconstruction", "author": "Suyi Chen and Haibin Ling", "abstract": "  As a critical modality for structural biology, cryogenic electron microscopy\n(cryo-EM) facilitates the determination of macromolecular structures at\nnear-atomic resolution. The core computational task in single-particle cryo-EM\nis to reconstruct the 3D electrostatic potential of a molecule from a large\ncollection of noisy 2D projections acquired at unknown orientations. Gaussian\nmixture models (GMMs) provide a continuous, compact, and physically\ninterpretable representation for molecular density and have recently gained\ninterest in cryo-EM reconstruction. However, existing methods rely on external\nconsensus maps or atomic models for initialization, limiting their use in\nself-contained pipelines. Addressing this issue, we introduce cryoGS, a\nGMM-based method that integrates Gaussian splatting with the physics of cryo-EM\nimage formation. In particular, we develop an orthogonal projection-aware\nGaussian splatting, with adaptations such as a normalization term and\nFFT-aligned coordinate system tailored for cryo-EM imaging. All these\ninnovations enable stable and efficient homogeneous reconstruction directly\nfrom raw cryo-EM particle images using random initialization. Experimental\nresults on real datasets validate the effectiveness and robustness of cryoGS\nover representative baselines. The code will be released upon publication.\n", "link": "http://arxiv.org/abs/2508.04929v2", "date": "2025-09-16", "relevancy": 2.8922, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6067}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.578}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CryoSplat%3A%20Gaussian%20Splatting%20for%20Cryo-EM%20Homogeneous%20Reconstruction&body=Title%3A%20CryoSplat%3A%20Gaussian%20Splatting%20for%20Cryo-EM%20Homogeneous%20Reconstruction%0AAuthor%3A%20Suyi%20Chen%20and%20Haibin%20Ling%0AAbstract%3A%20%20%20As%20a%20critical%20modality%20for%20structural%20biology%2C%20cryogenic%20electron%20microscopy%0A%28cryo-EM%29%20facilitates%20the%20determination%20of%20macromolecular%20structures%20at%0Anear-atomic%20resolution.%20The%20core%20computational%20task%20in%20single-particle%20cryo-EM%0Ais%20to%20reconstruct%20the%203D%20electrostatic%20potential%20of%20a%20molecule%20from%20a%20large%0Acollection%20of%20noisy%202D%20projections%20acquired%20at%20unknown%20orientations.%20Gaussian%0Amixture%20models%20%28GMMs%29%20provide%20a%20continuous%2C%20compact%2C%20and%20physically%0Ainterpretable%20representation%20for%20molecular%20density%20and%20have%20recently%20gained%0Ainterest%20in%20cryo-EM%20reconstruction.%20However%2C%20existing%20methods%20rely%20on%20external%0Aconsensus%20maps%20or%20atomic%20models%20for%20initialization%2C%20limiting%20their%20use%20in%0Aself-contained%20pipelines.%20Addressing%20this%20issue%2C%20we%20introduce%20cryoGS%2C%20a%0AGMM-based%20method%20that%20integrates%20Gaussian%20splatting%20with%20the%20physics%20of%20cryo-EM%0Aimage%20formation.%20In%20particular%2C%20we%20develop%20an%20orthogonal%20projection-aware%0AGaussian%20splatting%2C%20with%20adaptations%20such%20as%20a%20normalization%20term%20and%0AFFT-aligned%20coordinate%20system%20tailored%20for%20cryo-EM%20imaging.%20All%20these%0Ainnovations%20enable%20stable%20and%20efficient%20homogeneous%20reconstruction%20directly%0Afrom%20raw%20cryo-EM%20particle%20images%20using%20random%20initialization.%20Experimental%0Aresults%20on%20real%20datasets%20validate%20the%20effectiveness%20and%20robustness%20of%20cryoGS%0Aover%20representative%20baselines.%20The%20code%20will%20be%20released%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.04929v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCryoSplat%253A%2520Gaussian%2520Splatting%2520for%2520Cryo-EM%2520Homogeneous%2520Reconstruction%26entry.906535625%3DSuyi%2520Chen%2520and%2520Haibin%2520Ling%26entry.1292438233%3D%2520%2520As%2520a%2520critical%2520modality%2520for%2520structural%2520biology%252C%2520cryogenic%2520electron%2520microscopy%250A%2528cryo-EM%2529%2520facilitates%2520the%2520determination%2520of%2520macromolecular%2520structures%2520at%250Anear-atomic%2520resolution.%2520The%2520core%2520computational%2520task%2520in%2520single-particle%2520cryo-EM%250Ais%2520to%2520reconstruct%2520the%25203D%2520electrostatic%2520potential%2520of%2520a%2520molecule%2520from%2520a%2520large%250Acollection%2520of%2520noisy%25202D%2520projections%2520acquired%2520at%2520unknown%2520orientations.%2520Gaussian%250Amixture%2520models%2520%2528GMMs%2529%2520provide%2520a%2520continuous%252C%2520compact%252C%2520and%2520physically%250Ainterpretable%2520representation%2520for%2520molecular%2520density%2520and%2520have%2520recently%2520gained%250Ainterest%2520in%2520cryo-EM%2520reconstruction.%2520However%252C%2520existing%2520methods%2520rely%2520on%2520external%250Aconsensus%2520maps%2520or%2520atomic%2520models%2520for%2520initialization%252C%2520limiting%2520their%2520use%2520in%250Aself-contained%2520pipelines.%2520Addressing%2520this%2520issue%252C%2520we%2520introduce%2520cryoGS%252C%2520a%250AGMM-based%2520method%2520that%2520integrates%2520Gaussian%2520splatting%2520with%2520the%2520physics%2520of%2520cryo-EM%250Aimage%2520formation.%2520In%2520particular%252C%2520we%2520develop%2520an%2520orthogonal%2520projection-aware%250AGaussian%2520splatting%252C%2520with%2520adaptations%2520such%2520as%2520a%2520normalization%2520term%2520and%250AFFT-aligned%2520coordinate%2520system%2520tailored%2520for%2520cryo-EM%2520imaging.%2520All%2520these%250Ainnovations%2520enable%2520stable%2520and%2520efficient%2520homogeneous%2520reconstruction%2520directly%250Afrom%2520raw%2520cryo-EM%2520particle%2520images%2520using%2520random%2520initialization.%2520Experimental%250Aresults%2520on%2520real%2520datasets%2520validate%2520the%2520effectiveness%2520and%2520robustness%2520of%2520cryoGS%250Aover%2520representative%2520baselines.%2520The%2520code%2520will%2520be%2520released%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.04929v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CryoSplat%3A%20Gaussian%20Splatting%20for%20Cryo-EM%20Homogeneous%20Reconstruction&entry.906535625=Suyi%20Chen%20and%20Haibin%20Ling&entry.1292438233=%20%20As%20a%20critical%20modality%20for%20structural%20biology%2C%20cryogenic%20electron%20microscopy%0A%28cryo-EM%29%20facilitates%20the%20determination%20of%20macromolecular%20structures%20at%0Anear-atomic%20resolution.%20The%20core%20computational%20task%20in%20single-particle%20cryo-EM%0Ais%20to%20reconstruct%20the%203D%20electrostatic%20potential%20of%20a%20molecule%20from%20a%20large%0Acollection%20of%20noisy%202D%20projections%20acquired%20at%20unknown%20orientations.%20Gaussian%0Amixture%20models%20%28GMMs%29%20provide%20a%20continuous%2C%20compact%2C%20and%20physically%0Ainterpretable%20representation%20for%20molecular%20density%20and%20have%20recently%20gained%0Ainterest%20in%20cryo-EM%20reconstruction.%20However%2C%20existing%20methods%20rely%20on%20external%0Aconsensus%20maps%20or%20atomic%20models%20for%20initialization%2C%20limiting%20their%20use%20in%0Aself-contained%20pipelines.%20Addressing%20this%20issue%2C%20we%20introduce%20cryoGS%2C%20a%0AGMM-based%20method%20that%20integrates%20Gaussian%20splatting%20with%20the%20physics%20of%20cryo-EM%0Aimage%20formation.%20In%20particular%2C%20we%20develop%20an%20orthogonal%20projection-aware%0AGaussian%20splatting%2C%20with%20adaptations%20such%20as%20a%20normalization%20term%20and%0AFFT-aligned%20coordinate%20system%20tailored%20for%20cryo-EM%20imaging.%20All%20these%0Ainnovations%20enable%20stable%20and%20efficient%20homogeneous%20reconstruction%20directly%0Afrom%20raw%20cryo-EM%20particle%20images%20using%20random%20initialization.%20Experimental%0Aresults%20on%20real%20datasets%20validate%20the%20effectiveness%20and%20robustness%20of%20cryoGS%0Aover%20representative%20baselines.%20The%20code%20will%20be%20released%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.04929v2&entry.124074799=Read"},
{"title": "TFANet: Three-Stage Image-Text Feature Alignment Network for Robust\n  Referring Image Segmentation", "author": "Qianqi Lu and Yuxiang Xie and Jing Zhang and Shiwei Zou and Yan Chen and Xidao Luan", "abstract": "  Referring Image Segmentation (RIS) is a task that segments image regions\nbased on language expressions, requiring fine-grained alignment between two\nmodalities. However, existing methods often struggle with multimodal\nmisalignment and language semantic loss, especially in complex scenes\ncontaining multiple visually similar objects, where uniquely described targets\nare frequently mislocalized or incompletely segmented. To tackle these\nchallenges, this paper proposes TFANet, a Three-stage Image-Text Feature\nAlignment Network that systematically enhances multimodal alignment through a\nhierarchical framework comprising three stages: Knowledge Plus Stage (KPS),\nKnowledge Fusion Stage (KFS), and Knowledge Intensification Stage (KIS). In the\nfirst stage, we design the Multiscale Linear Cross-Attention Module (MLAM),\nwhich facilitates bidirectional semantic exchange between visual features and\ntextual representations across multiple scales. This establishes rich and\nefficient alignment between image regions and different granularities of\nlinguistic descriptions. Subsequently, the KFS further strengthens feature\nalignment through the Cross-modal Feature Scanning Module (CFSM), which applies\nmultimodal selective scanning to capture long-range dependencies and construct\na unified multimodal representation. This is essential for modeling long-range\ncross-modal dependencies and enhancing alignment accuracy in complex scenes.\nFinally, in the KIS, we propose the Word-level Linguistic Feature-guided\nSemantic Deepening Module (WFDM) to compensate for semantic degradation\nintroduced in earlier stages.\n", "link": "http://arxiv.org/abs/2509.13070v1", "date": "2025-09-16", "relevancy": 2.8398, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6071}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5579}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TFANet%3A%20Three-Stage%20Image-Text%20Feature%20Alignment%20Network%20for%20Robust%0A%20%20Referring%20Image%20Segmentation&body=Title%3A%20TFANet%3A%20Three-Stage%20Image-Text%20Feature%20Alignment%20Network%20for%20Robust%0A%20%20Referring%20Image%20Segmentation%0AAuthor%3A%20Qianqi%20Lu%20and%20Yuxiang%20Xie%20and%20Jing%20Zhang%20and%20Shiwei%20Zou%20and%20Yan%20Chen%20and%20Xidao%20Luan%0AAbstract%3A%20%20%20Referring%20Image%20Segmentation%20%28RIS%29%20is%20a%20task%20that%20segments%20image%20regions%0Abased%20on%20language%20expressions%2C%20requiring%20fine-grained%20alignment%20between%20two%0Amodalities.%20However%2C%20existing%20methods%20often%20struggle%20with%20multimodal%0Amisalignment%20and%20language%20semantic%20loss%2C%20especially%20in%20complex%20scenes%0Acontaining%20multiple%20visually%20similar%20objects%2C%20where%20uniquely%20described%20targets%0Aare%20frequently%20mislocalized%20or%20incompletely%20segmented.%20To%20tackle%20these%0Achallenges%2C%20this%20paper%20proposes%20TFANet%2C%20a%20Three-stage%20Image-Text%20Feature%0AAlignment%20Network%20that%20systematically%20enhances%20multimodal%20alignment%20through%20a%0Ahierarchical%20framework%20comprising%20three%20stages%3A%20Knowledge%20Plus%20Stage%20%28KPS%29%2C%0AKnowledge%20Fusion%20Stage%20%28KFS%29%2C%20and%20Knowledge%20Intensification%20Stage%20%28KIS%29.%20In%20the%0Afirst%20stage%2C%20we%20design%20the%20Multiscale%20Linear%20Cross-Attention%20Module%20%28MLAM%29%2C%0Awhich%20facilitates%20bidirectional%20semantic%20exchange%20between%20visual%20features%20and%0Atextual%20representations%20across%20multiple%20scales.%20This%20establishes%20rich%20and%0Aefficient%20alignment%20between%20image%20regions%20and%20different%20granularities%20of%0Alinguistic%20descriptions.%20Subsequently%2C%20the%20KFS%20further%20strengthens%20feature%0Aalignment%20through%20the%20Cross-modal%20Feature%20Scanning%20Module%20%28CFSM%29%2C%20which%20applies%0Amultimodal%20selective%20scanning%20to%20capture%20long-range%20dependencies%20and%20construct%0Aa%20unified%20multimodal%20representation.%20This%20is%20essential%20for%20modeling%20long-range%0Across-modal%20dependencies%20and%20enhancing%20alignment%20accuracy%20in%20complex%20scenes.%0AFinally%2C%20in%20the%20KIS%2C%20we%20propose%20the%20Word-level%20Linguistic%20Feature-guided%0ASemantic%20Deepening%20Module%20%28WFDM%29%20to%20compensate%20for%20semantic%20degradation%0Aintroduced%20in%20earlier%20stages.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13070v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTFANet%253A%2520Three-Stage%2520Image-Text%2520Feature%2520Alignment%2520Network%2520for%2520Robust%250A%2520%2520Referring%2520Image%2520Segmentation%26entry.906535625%3DQianqi%2520Lu%2520and%2520Yuxiang%2520Xie%2520and%2520Jing%2520Zhang%2520and%2520Shiwei%2520Zou%2520and%2520Yan%2520Chen%2520and%2520Xidao%2520Luan%26entry.1292438233%3D%2520%2520Referring%2520Image%2520Segmentation%2520%2528RIS%2529%2520is%2520a%2520task%2520that%2520segments%2520image%2520regions%250Abased%2520on%2520language%2520expressions%252C%2520requiring%2520fine-grained%2520alignment%2520between%2520two%250Amodalities.%2520However%252C%2520existing%2520methods%2520often%2520struggle%2520with%2520multimodal%250Amisalignment%2520and%2520language%2520semantic%2520loss%252C%2520especially%2520in%2520complex%2520scenes%250Acontaining%2520multiple%2520visually%2520similar%2520objects%252C%2520where%2520uniquely%2520described%2520targets%250Aare%2520frequently%2520mislocalized%2520or%2520incompletely%2520segmented.%2520To%2520tackle%2520these%250Achallenges%252C%2520this%2520paper%2520proposes%2520TFANet%252C%2520a%2520Three-stage%2520Image-Text%2520Feature%250AAlignment%2520Network%2520that%2520systematically%2520enhances%2520multimodal%2520alignment%2520through%2520a%250Ahierarchical%2520framework%2520comprising%2520three%2520stages%253A%2520Knowledge%2520Plus%2520Stage%2520%2528KPS%2529%252C%250AKnowledge%2520Fusion%2520Stage%2520%2528KFS%2529%252C%2520and%2520Knowledge%2520Intensification%2520Stage%2520%2528KIS%2529.%2520In%2520the%250Afirst%2520stage%252C%2520we%2520design%2520the%2520Multiscale%2520Linear%2520Cross-Attention%2520Module%2520%2528MLAM%2529%252C%250Awhich%2520facilitates%2520bidirectional%2520semantic%2520exchange%2520between%2520visual%2520features%2520and%250Atextual%2520representations%2520across%2520multiple%2520scales.%2520This%2520establishes%2520rich%2520and%250Aefficient%2520alignment%2520between%2520image%2520regions%2520and%2520different%2520granularities%2520of%250Alinguistic%2520descriptions.%2520Subsequently%252C%2520the%2520KFS%2520further%2520strengthens%2520feature%250Aalignment%2520through%2520the%2520Cross-modal%2520Feature%2520Scanning%2520Module%2520%2528CFSM%2529%252C%2520which%2520applies%250Amultimodal%2520selective%2520scanning%2520to%2520capture%2520long-range%2520dependencies%2520and%2520construct%250Aa%2520unified%2520multimodal%2520representation.%2520This%2520is%2520essential%2520for%2520modeling%2520long-range%250Across-modal%2520dependencies%2520and%2520enhancing%2520alignment%2520accuracy%2520in%2520complex%2520scenes.%250AFinally%252C%2520in%2520the%2520KIS%252C%2520we%2520propose%2520the%2520Word-level%2520Linguistic%2520Feature-guided%250ASemantic%2520Deepening%2520Module%2520%2528WFDM%2529%2520to%2520compensate%2520for%2520semantic%2520degradation%250Aintroduced%2520in%2520earlier%2520stages.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13070v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TFANet%3A%20Three-Stage%20Image-Text%20Feature%20Alignment%20Network%20for%20Robust%0A%20%20Referring%20Image%20Segmentation&entry.906535625=Qianqi%20Lu%20and%20Yuxiang%20Xie%20and%20Jing%20Zhang%20and%20Shiwei%20Zou%20and%20Yan%20Chen%20and%20Xidao%20Luan&entry.1292438233=%20%20Referring%20Image%20Segmentation%20%28RIS%29%20is%20a%20task%20that%20segments%20image%20regions%0Abased%20on%20language%20expressions%2C%20requiring%20fine-grained%20alignment%20between%20two%0Amodalities.%20However%2C%20existing%20methods%20often%20struggle%20with%20multimodal%0Amisalignment%20and%20language%20semantic%20loss%2C%20especially%20in%20complex%20scenes%0Acontaining%20multiple%20visually%20similar%20objects%2C%20where%20uniquely%20described%20targets%0Aare%20frequently%20mislocalized%20or%20incompletely%20segmented.%20To%20tackle%20these%0Achallenges%2C%20this%20paper%20proposes%20TFANet%2C%20a%20Three-stage%20Image-Text%20Feature%0AAlignment%20Network%20that%20systematically%20enhances%20multimodal%20alignment%20through%20a%0Ahierarchical%20framework%20comprising%20three%20stages%3A%20Knowledge%20Plus%20Stage%20%28KPS%29%2C%0AKnowledge%20Fusion%20Stage%20%28KFS%29%2C%20and%20Knowledge%20Intensification%20Stage%20%28KIS%29.%20In%20the%0Afirst%20stage%2C%20we%20design%20the%20Multiscale%20Linear%20Cross-Attention%20Module%20%28MLAM%29%2C%0Awhich%20facilitates%20bidirectional%20semantic%20exchange%20between%20visual%20features%20and%0Atextual%20representations%20across%20multiple%20scales.%20This%20establishes%20rich%20and%0Aefficient%20alignment%20between%20image%20regions%20and%20different%20granularities%20of%0Alinguistic%20descriptions.%20Subsequently%2C%20the%20KFS%20further%20strengthens%20feature%0Aalignment%20through%20the%20Cross-modal%20Feature%20Scanning%20Module%20%28CFSM%29%2C%20which%20applies%0Amultimodal%20selective%20scanning%20to%20capture%20long-range%20dependencies%20and%20construct%0Aa%20unified%20multimodal%20representation.%20This%20is%20essential%20for%20modeling%20long-range%0Across-modal%20dependencies%20and%20enhancing%20alignment%20accuracy%20in%20complex%20scenes.%0AFinally%2C%20in%20the%20KIS%2C%20we%20propose%20the%20Word-level%20Linguistic%20Feature-guided%0ASemantic%20Deepening%20Module%20%28WFDM%29%20to%20compensate%20for%20semantic%20degradation%0Aintroduced%20in%20earlier%20stages.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13070v1&entry.124074799=Read"},
{"title": "MSGFusion: Multimodal Scene Graph-Guided Infrared and Visible Image\n  Fusion", "author": "Guihui Li and Bowei Dong and Kaizhi Dong and Jiayi Li and Haiyong Zheng", "abstract": "  Infrared and visible image fusion has garnered considerable attention owing\nto the strong complementarity of these two modalities in complex, harsh\nenvironments. While deep learning-based fusion methods have made remarkable\nadvances in feature extraction, alignment, fusion, and reconstruction, they\nstill depend largely on low-level visual cues, such as texture and contrast,\nand struggle to capture the high-level semantic information embedded in images.\nRecent attempts to incorporate text as a source of semantic guidance have\nrelied on unstructured descriptions that neither explicitly model entities,\nattributes, and relationships nor provide spatial localization, thereby\nlimiting fine-grained fusion performance. To overcome these challenges, we\nintroduce MSGFusion, a multimodal scene graph-guided fusion framework for\ninfrared and visible imagery. By deeply coupling structured scene graphs\nderived from text and vision, MSGFusion explicitly represents entities,\nattributes, and spatial relations, and then synchronously refines high-level\nsemantics and low-level details through successive modules for scene graph\nrepresentation, hierarchical aggregation, and graph-driven fusion. Extensive\nexperiments on multiple public benchmarks show that MSGFusion significantly\noutperforms state-of-the-art approaches, particularly in detail preservation\nand structural clarity, and delivers superior semantic consistency and\ngeneralizability in downstream tasks such as low-light object detection,\nsemantic segmentation, and medical image fusion.\n", "link": "http://arxiv.org/abs/2509.12901v1", "date": "2025-09-16", "relevancy": 2.8261, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5743}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5743}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSGFusion%3A%20Multimodal%20Scene%20Graph-Guided%20Infrared%20and%20Visible%20Image%0A%20%20Fusion&body=Title%3A%20MSGFusion%3A%20Multimodal%20Scene%20Graph-Guided%20Infrared%20and%20Visible%20Image%0A%20%20Fusion%0AAuthor%3A%20Guihui%20Li%20and%20Bowei%20Dong%20and%20Kaizhi%20Dong%20and%20Jiayi%20Li%20and%20Haiyong%20Zheng%0AAbstract%3A%20%20%20Infrared%20and%20visible%20image%20fusion%20has%20garnered%20considerable%20attention%20owing%0Ato%20the%20strong%20complementarity%20of%20these%20two%20modalities%20in%20complex%2C%20harsh%0Aenvironments.%20While%20deep%20learning-based%20fusion%20methods%20have%20made%20remarkable%0Aadvances%20in%20feature%20extraction%2C%20alignment%2C%20fusion%2C%20and%20reconstruction%2C%20they%0Astill%20depend%20largely%20on%20low-level%20visual%20cues%2C%20such%20as%20texture%20and%20contrast%2C%0Aand%20struggle%20to%20capture%20the%20high-level%20semantic%20information%20embedded%20in%20images.%0ARecent%20attempts%20to%20incorporate%20text%20as%20a%20source%20of%20semantic%20guidance%20have%0Arelied%20on%20unstructured%20descriptions%20that%20neither%20explicitly%20model%20entities%2C%0Aattributes%2C%20and%20relationships%20nor%20provide%20spatial%20localization%2C%20thereby%0Alimiting%20fine-grained%20fusion%20performance.%20To%20overcome%20these%20challenges%2C%20we%0Aintroduce%20MSGFusion%2C%20a%20multimodal%20scene%20graph-guided%20fusion%20framework%20for%0Ainfrared%20and%20visible%20imagery.%20By%20deeply%20coupling%20structured%20scene%20graphs%0Aderived%20from%20text%20and%20vision%2C%20MSGFusion%20explicitly%20represents%20entities%2C%0Aattributes%2C%20and%20spatial%20relations%2C%20and%20then%20synchronously%20refines%20high-level%0Asemantics%20and%20low-level%20details%20through%20successive%20modules%20for%20scene%20graph%0Arepresentation%2C%20hierarchical%20aggregation%2C%20and%20graph-driven%20fusion.%20Extensive%0Aexperiments%20on%20multiple%20public%20benchmarks%20show%20that%20MSGFusion%20significantly%0Aoutperforms%20state-of-the-art%20approaches%2C%20particularly%20in%20detail%20preservation%0Aand%20structural%20clarity%2C%20and%20delivers%20superior%20semantic%20consistency%20and%0Ageneralizability%20in%20downstream%20tasks%20such%20as%20low-light%20object%20detection%2C%0Asemantic%20segmentation%2C%20and%20medical%20image%20fusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12901v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSGFusion%253A%2520Multimodal%2520Scene%2520Graph-Guided%2520Infrared%2520and%2520Visible%2520Image%250A%2520%2520Fusion%26entry.906535625%3DGuihui%2520Li%2520and%2520Bowei%2520Dong%2520and%2520Kaizhi%2520Dong%2520and%2520Jiayi%2520Li%2520and%2520Haiyong%2520Zheng%26entry.1292438233%3D%2520%2520Infrared%2520and%2520visible%2520image%2520fusion%2520has%2520garnered%2520considerable%2520attention%2520owing%250Ato%2520the%2520strong%2520complementarity%2520of%2520these%2520two%2520modalities%2520in%2520complex%252C%2520harsh%250Aenvironments.%2520While%2520deep%2520learning-based%2520fusion%2520methods%2520have%2520made%2520remarkable%250Aadvances%2520in%2520feature%2520extraction%252C%2520alignment%252C%2520fusion%252C%2520and%2520reconstruction%252C%2520they%250Astill%2520depend%2520largely%2520on%2520low-level%2520visual%2520cues%252C%2520such%2520as%2520texture%2520and%2520contrast%252C%250Aand%2520struggle%2520to%2520capture%2520the%2520high-level%2520semantic%2520information%2520embedded%2520in%2520images.%250ARecent%2520attempts%2520to%2520incorporate%2520text%2520as%2520a%2520source%2520of%2520semantic%2520guidance%2520have%250Arelied%2520on%2520unstructured%2520descriptions%2520that%2520neither%2520explicitly%2520model%2520entities%252C%250Aattributes%252C%2520and%2520relationships%2520nor%2520provide%2520spatial%2520localization%252C%2520thereby%250Alimiting%2520fine-grained%2520fusion%2520performance.%2520To%2520overcome%2520these%2520challenges%252C%2520we%250Aintroduce%2520MSGFusion%252C%2520a%2520multimodal%2520scene%2520graph-guided%2520fusion%2520framework%2520for%250Ainfrared%2520and%2520visible%2520imagery.%2520By%2520deeply%2520coupling%2520structured%2520scene%2520graphs%250Aderived%2520from%2520text%2520and%2520vision%252C%2520MSGFusion%2520explicitly%2520represents%2520entities%252C%250Aattributes%252C%2520and%2520spatial%2520relations%252C%2520and%2520then%2520synchronously%2520refines%2520high-level%250Asemantics%2520and%2520low-level%2520details%2520through%2520successive%2520modules%2520for%2520scene%2520graph%250Arepresentation%252C%2520hierarchical%2520aggregation%252C%2520and%2520graph-driven%2520fusion.%2520Extensive%250Aexperiments%2520on%2520multiple%2520public%2520benchmarks%2520show%2520that%2520MSGFusion%2520significantly%250Aoutperforms%2520state-of-the-art%2520approaches%252C%2520particularly%2520in%2520detail%2520preservation%250Aand%2520structural%2520clarity%252C%2520and%2520delivers%2520superior%2520semantic%2520consistency%2520and%250Ageneralizability%2520in%2520downstream%2520tasks%2520such%2520as%2520low-light%2520object%2520detection%252C%250Asemantic%2520segmentation%252C%2520and%2520medical%2520image%2520fusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12901v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSGFusion%3A%20Multimodal%20Scene%20Graph-Guided%20Infrared%20and%20Visible%20Image%0A%20%20Fusion&entry.906535625=Guihui%20Li%20and%20Bowei%20Dong%20and%20Kaizhi%20Dong%20and%20Jiayi%20Li%20and%20Haiyong%20Zheng&entry.1292438233=%20%20Infrared%20and%20visible%20image%20fusion%20has%20garnered%20considerable%20attention%20owing%0Ato%20the%20strong%20complementarity%20of%20these%20two%20modalities%20in%20complex%2C%20harsh%0Aenvironments.%20While%20deep%20learning-based%20fusion%20methods%20have%20made%20remarkable%0Aadvances%20in%20feature%20extraction%2C%20alignment%2C%20fusion%2C%20and%20reconstruction%2C%20they%0Astill%20depend%20largely%20on%20low-level%20visual%20cues%2C%20such%20as%20texture%20and%20contrast%2C%0Aand%20struggle%20to%20capture%20the%20high-level%20semantic%20information%20embedded%20in%20images.%0ARecent%20attempts%20to%20incorporate%20text%20as%20a%20source%20of%20semantic%20guidance%20have%0Arelied%20on%20unstructured%20descriptions%20that%20neither%20explicitly%20model%20entities%2C%0Aattributes%2C%20and%20relationships%20nor%20provide%20spatial%20localization%2C%20thereby%0Alimiting%20fine-grained%20fusion%20performance.%20To%20overcome%20these%20challenges%2C%20we%0Aintroduce%20MSGFusion%2C%20a%20multimodal%20scene%20graph-guided%20fusion%20framework%20for%0Ainfrared%20and%20visible%20imagery.%20By%20deeply%20coupling%20structured%20scene%20graphs%0Aderived%20from%20text%20and%20vision%2C%20MSGFusion%20explicitly%20represents%20entities%2C%0Aattributes%2C%20and%20spatial%20relations%2C%20and%20then%20synchronously%20refines%20high-level%0Asemantics%20and%20low-level%20details%20through%20successive%20modules%20for%20scene%20graph%0Arepresentation%2C%20hierarchical%20aggregation%2C%20and%20graph-driven%20fusion.%20Extensive%0Aexperiments%20on%20multiple%20public%20benchmarks%20show%20that%20MSGFusion%20significantly%0Aoutperforms%20state-of-the-art%20approaches%2C%20particularly%20in%20detail%20preservation%0Aand%20structural%20clarity%2C%20and%20delivers%20superior%20semantic%20consistency%20and%0Ageneralizability%20in%20downstream%20tasks%20such%20as%20low-light%20object%20detection%2C%0Asemantic%20segmentation%2C%20and%20medical%20image%20fusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12901v1&entry.124074799=Read"},
{"title": "PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era", "author": "Xu Zheng and Chenfei Liao and Ziqiao Weng and Kaiyu Lei and Zihao Dongfang and Haocong He and Yuanhuiyi Lyu and Lutao Jiang and Lu Qi and Li Chen and Danda Pani Paudel and Kailun Yang and Linfeng Zhang and Luc Van Gool and Xuming Hu", "abstract": "  Omnidirectional vision, using 360-degree vision to understand the\nenvironment, has become increasingly critical across domains like robotics,\nindustrial inspection, and environmental monitoring. Compared to traditional\npinhole vision, omnidirectional vision provides holistic environmental\nawareness, significantly enhancing the completeness of scene perception and the\nreliability of decision-making. However, foundational research in this area has\nhistorically lagged behind traditional pinhole vision. This talk presents an\nemerging trend in the embodied AI era: the rapid development of omnidirectional\nvision, driven by growing industrial demand and academic interest. We highlight\nrecent breakthroughs in omnidirectional generation, omnidirectional perception,\nomnidirectional understanding, and related datasets. Drawing on insights from\nboth academia and industry, we propose an ideal panoramic system architecture\nin the embodied AI era, PANORAMA, which consists of four key subsystems.\nMoreover, we offer in-depth opinions related to emerging trends and\ncross-community impacts at the intersection of panoramic vision and embodied\nAI, along with the future roadmap and open challenges. This overview\nsynthesizes state-of-the-art advancements and outlines challenges and\nopportunities for future research in building robust, general-purpose\nomnidirectional AI systems in the embodied AI era.\n", "link": "http://arxiv.org/abs/2509.12989v1", "date": "2025-09-16", "relevancy": 2.7897, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5744}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PANORAMA%3A%20The%20Rise%20of%20Omnidirectional%20Vision%20in%20the%20Embodied%20AI%20Era&body=Title%3A%20PANORAMA%3A%20The%20Rise%20of%20Omnidirectional%20Vision%20in%20the%20Embodied%20AI%20Era%0AAuthor%3A%20Xu%20Zheng%20and%20Chenfei%20Liao%20and%20Ziqiao%20Weng%20and%20Kaiyu%20Lei%20and%20Zihao%20Dongfang%20and%20Haocong%20He%20and%20Yuanhuiyi%20Lyu%20and%20Lutao%20Jiang%20and%20Lu%20Qi%20and%20Li%20Chen%20and%20Danda%20Pani%20Paudel%20and%20Kailun%20Yang%20and%20Linfeng%20Zhang%20and%20Luc%20Van%20Gool%20and%20Xuming%20Hu%0AAbstract%3A%20%20%20Omnidirectional%20vision%2C%20using%20360-degree%20vision%20to%20understand%20the%0Aenvironment%2C%20has%20become%20increasingly%20critical%20across%20domains%20like%20robotics%2C%0Aindustrial%20inspection%2C%20and%20environmental%20monitoring.%20Compared%20to%20traditional%0Apinhole%20vision%2C%20omnidirectional%20vision%20provides%20holistic%20environmental%0Aawareness%2C%20significantly%20enhancing%20the%20completeness%20of%20scene%20perception%20and%20the%0Areliability%20of%20decision-making.%20However%2C%20foundational%20research%20in%20this%20area%20has%0Ahistorically%20lagged%20behind%20traditional%20pinhole%20vision.%20This%20talk%20presents%20an%0Aemerging%20trend%20in%20the%20embodied%20AI%20era%3A%20the%20rapid%20development%20of%20omnidirectional%0Avision%2C%20driven%20by%20growing%20industrial%20demand%20and%20academic%20interest.%20We%20highlight%0Arecent%20breakthroughs%20in%20omnidirectional%20generation%2C%20omnidirectional%20perception%2C%0Aomnidirectional%20understanding%2C%20and%20related%20datasets.%20Drawing%20on%20insights%20from%0Aboth%20academia%20and%20industry%2C%20we%20propose%20an%20ideal%20panoramic%20system%20architecture%0Ain%20the%20embodied%20AI%20era%2C%20PANORAMA%2C%20which%20consists%20of%20four%20key%20subsystems.%0AMoreover%2C%20we%20offer%20in-depth%20opinions%20related%20to%20emerging%20trends%20and%0Across-community%20impacts%20at%20the%20intersection%20of%20panoramic%20vision%20and%20embodied%0AAI%2C%20along%20with%20the%20future%20roadmap%20and%20open%20challenges.%20This%20overview%0Asynthesizes%20state-of-the-art%20advancements%20and%20outlines%20challenges%20and%0Aopportunities%20for%20future%20research%20in%20building%20robust%2C%20general-purpose%0Aomnidirectional%20AI%20systems%20in%20the%20embodied%20AI%20era.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPANORAMA%253A%2520The%2520Rise%2520of%2520Omnidirectional%2520Vision%2520in%2520the%2520Embodied%2520AI%2520Era%26entry.906535625%3DXu%2520Zheng%2520and%2520Chenfei%2520Liao%2520and%2520Ziqiao%2520Weng%2520and%2520Kaiyu%2520Lei%2520and%2520Zihao%2520Dongfang%2520and%2520Haocong%2520He%2520and%2520Yuanhuiyi%2520Lyu%2520and%2520Lutao%2520Jiang%2520and%2520Lu%2520Qi%2520and%2520Li%2520Chen%2520and%2520Danda%2520Pani%2520Paudel%2520and%2520Kailun%2520Yang%2520and%2520Linfeng%2520Zhang%2520and%2520Luc%2520Van%2520Gool%2520and%2520Xuming%2520Hu%26entry.1292438233%3D%2520%2520Omnidirectional%2520vision%252C%2520using%2520360-degree%2520vision%2520to%2520understand%2520the%250Aenvironment%252C%2520has%2520become%2520increasingly%2520critical%2520across%2520domains%2520like%2520robotics%252C%250Aindustrial%2520inspection%252C%2520and%2520environmental%2520monitoring.%2520Compared%2520to%2520traditional%250Apinhole%2520vision%252C%2520omnidirectional%2520vision%2520provides%2520holistic%2520environmental%250Aawareness%252C%2520significantly%2520enhancing%2520the%2520completeness%2520of%2520scene%2520perception%2520and%2520the%250Areliability%2520of%2520decision-making.%2520However%252C%2520foundational%2520research%2520in%2520this%2520area%2520has%250Ahistorically%2520lagged%2520behind%2520traditional%2520pinhole%2520vision.%2520This%2520talk%2520presents%2520an%250Aemerging%2520trend%2520in%2520the%2520embodied%2520AI%2520era%253A%2520the%2520rapid%2520development%2520of%2520omnidirectional%250Avision%252C%2520driven%2520by%2520growing%2520industrial%2520demand%2520and%2520academic%2520interest.%2520We%2520highlight%250Arecent%2520breakthroughs%2520in%2520omnidirectional%2520generation%252C%2520omnidirectional%2520perception%252C%250Aomnidirectional%2520understanding%252C%2520and%2520related%2520datasets.%2520Drawing%2520on%2520insights%2520from%250Aboth%2520academia%2520and%2520industry%252C%2520we%2520propose%2520an%2520ideal%2520panoramic%2520system%2520architecture%250Ain%2520the%2520embodied%2520AI%2520era%252C%2520PANORAMA%252C%2520which%2520consists%2520of%2520four%2520key%2520subsystems.%250AMoreover%252C%2520we%2520offer%2520in-depth%2520opinions%2520related%2520to%2520emerging%2520trends%2520and%250Across-community%2520impacts%2520at%2520the%2520intersection%2520of%2520panoramic%2520vision%2520and%2520embodied%250AAI%252C%2520along%2520with%2520the%2520future%2520roadmap%2520and%2520open%2520challenges.%2520This%2520overview%250Asynthesizes%2520state-of-the-art%2520advancements%2520and%2520outlines%2520challenges%2520and%250Aopportunities%2520for%2520future%2520research%2520in%2520building%2520robust%252C%2520general-purpose%250Aomnidirectional%2520AI%2520systems%2520in%2520the%2520embodied%2520AI%2520era.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PANORAMA%3A%20The%20Rise%20of%20Omnidirectional%20Vision%20in%20the%20Embodied%20AI%20Era&entry.906535625=Xu%20Zheng%20and%20Chenfei%20Liao%20and%20Ziqiao%20Weng%20and%20Kaiyu%20Lei%20and%20Zihao%20Dongfang%20and%20Haocong%20He%20and%20Yuanhuiyi%20Lyu%20and%20Lutao%20Jiang%20and%20Lu%20Qi%20and%20Li%20Chen%20and%20Danda%20Pani%20Paudel%20and%20Kailun%20Yang%20and%20Linfeng%20Zhang%20and%20Luc%20Van%20Gool%20and%20Xuming%20Hu&entry.1292438233=%20%20Omnidirectional%20vision%2C%20using%20360-degree%20vision%20to%20understand%20the%0Aenvironment%2C%20has%20become%20increasingly%20critical%20across%20domains%20like%20robotics%2C%0Aindustrial%20inspection%2C%20and%20environmental%20monitoring.%20Compared%20to%20traditional%0Apinhole%20vision%2C%20omnidirectional%20vision%20provides%20holistic%20environmental%0Aawareness%2C%20significantly%20enhancing%20the%20completeness%20of%20scene%20perception%20and%20the%0Areliability%20of%20decision-making.%20However%2C%20foundational%20research%20in%20this%20area%20has%0Ahistorically%20lagged%20behind%20traditional%20pinhole%20vision.%20This%20talk%20presents%20an%0Aemerging%20trend%20in%20the%20embodied%20AI%20era%3A%20the%20rapid%20development%20of%20omnidirectional%0Avision%2C%20driven%20by%20growing%20industrial%20demand%20and%20academic%20interest.%20We%20highlight%0Arecent%20breakthroughs%20in%20omnidirectional%20generation%2C%20omnidirectional%20perception%2C%0Aomnidirectional%20understanding%2C%20and%20related%20datasets.%20Drawing%20on%20insights%20from%0Aboth%20academia%20and%20industry%2C%20we%20propose%20an%20ideal%20panoramic%20system%20architecture%0Ain%20the%20embodied%20AI%20era%2C%20PANORAMA%2C%20which%20consists%20of%20four%20key%20subsystems.%0AMoreover%2C%20we%20offer%20in-depth%20opinions%20related%20to%20emerging%20trends%20and%0Across-community%20impacts%20at%20the%20intersection%20of%20panoramic%20vision%20and%20embodied%0AAI%2C%20along%20with%20the%20future%20roadmap%20and%20open%20challenges.%20This%20overview%0Asynthesizes%20state-of-the-art%20advancements%20and%20outlines%20challenges%20and%0Aopportunities%20for%20future%20research%20in%20building%20robust%2C%20general-purpose%0Aomnidirectional%20AI%20systems%20in%20the%20embodied%20AI%20era.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12989v1&entry.124074799=Read"},
{"title": "Taming Anomalies with Down-Up Sampling Networks: Group Center Preserving\n  Reconstruction for 3D Anomaly Detection", "author": "Hanzhe Liang and Jie Zhang and Tao Dai and Linlin Shen and Jinbao Wang and Can Gao", "abstract": "  Reconstruction-based methods have demonstrated very promising results for 3D\nanomaly detection. However, these methods face great challenges in handling\nhigh-precision point clouds due to the large scale and complex structure. In\nthis study, a Down-Up Sampling Network (DUS-Net) is proposed to reconstruct\nhigh-precision point clouds for 3D anomaly detection by preserving the group\ncenter geometric structure. The DUS-Net first introduces a Noise Generation\nmodule to generate noisy patches, which facilitates the diversity of training\ndata and strengthens the feature representation for reconstruction. Then, a\nDown-sampling Network (Down-Net) is developed to learn an anomaly-free center\npoint cloud from patches with noise injection. Subsequently, an Up-sampling\nNetwork (Up-Net) is designed to reconstruct high-precision point clouds by\nfusing multi-scale up-sampling features. Our method leverages group centers for\nconstruction, enabling the preservation of geometric structure and providing a\nmore precise point cloud. Extensive experiments demonstrate the effectiveness\nof our proposed method, achieving state-of-the-art (SOTA) performance with an\nObject-level AUROC of 79.9% and 79.5%, and a Point-level AUROC of 71.2% and\n84.7% on the Real3D-AD and Anomaly-ShapeNet datasets, respectively.\n", "link": "http://arxiv.org/abs/2507.03903v2", "date": "2025-09-16", "relevancy": 2.7857, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.579}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5489}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5435}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Taming%20Anomalies%20with%20Down-Up%20Sampling%20Networks%3A%20Group%20Center%20Preserving%0A%20%20Reconstruction%20for%203D%20Anomaly%20Detection&body=Title%3A%20Taming%20Anomalies%20with%20Down-Up%20Sampling%20Networks%3A%20Group%20Center%20Preserving%0A%20%20Reconstruction%20for%203D%20Anomaly%20Detection%0AAuthor%3A%20Hanzhe%20Liang%20and%20Jie%20Zhang%20and%20Tao%20Dai%20and%20Linlin%20Shen%20and%20Jinbao%20Wang%20and%20Can%20Gao%0AAbstract%3A%20%20%20Reconstruction-based%20methods%20have%20demonstrated%20very%20promising%20results%20for%203D%0Aanomaly%20detection.%20However%2C%20these%20methods%20face%20great%20challenges%20in%20handling%0Ahigh-precision%20point%20clouds%20due%20to%20the%20large%20scale%20and%20complex%20structure.%20In%0Athis%20study%2C%20a%20Down-Up%20Sampling%20Network%20%28DUS-Net%29%20is%20proposed%20to%20reconstruct%0Ahigh-precision%20point%20clouds%20for%203D%20anomaly%20detection%20by%20preserving%20the%20group%0Acenter%20geometric%20structure.%20The%20DUS-Net%20first%20introduces%20a%20Noise%20Generation%0Amodule%20to%20generate%20noisy%20patches%2C%20which%20facilitates%20the%20diversity%20of%20training%0Adata%20and%20strengthens%20the%20feature%20representation%20for%20reconstruction.%20Then%2C%20a%0ADown-sampling%20Network%20%28Down-Net%29%20is%20developed%20to%20learn%20an%20anomaly-free%20center%0Apoint%20cloud%20from%20patches%20with%20noise%20injection.%20Subsequently%2C%20an%20Up-sampling%0ANetwork%20%28Up-Net%29%20is%20designed%20to%20reconstruct%20high-precision%20point%20clouds%20by%0Afusing%20multi-scale%20up-sampling%20features.%20Our%20method%20leverages%20group%20centers%20for%0Aconstruction%2C%20enabling%20the%20preservation%20of%20geometric%20structure%20and%20providing%20a%0Amore%20precise%20point%20cloud.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%0Aof%20our%20proposed%20method%2C%20achieving%20state-of-the-art%20%28SOTA%29%20performance%20with%20an%0AObject-level%20AUROC%20of%2079.9%25%20and%2079.5%25%2C%20and%20a%20Point-level%20AUROC%20of%2071.2%25%20and%0A84.7%25%20on%20the%20Real3D-AD%20and%20Anomaly-ShapeNet%20datasets%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.03903v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTaming%2520Anomalies%2520with%2520Down-Up%2520Sampling%2520Networks%253A%2520Group%2520Center%2520Preserving%250A%2520%2520Reconstruction%2520for%25203D%2520Anomaly%2520Detection%26entry.906535625%3DHanzhe%2520Liang%2520and%2520Jie%2520Zhang%2520and%2520Tao%2520Dai%2520and%2520Linlin%2520Shen%2520and%2520Jinbao%2520Wang%2520and%2520Can%2520Gao%26entry.1292438233%3D%2520%2520Reconstruction-based%2520methods%2520have%2520demonstrated%2520very%2520promising%2520results%2520for%25203D%250Aanomaly%2520detection.%2520However%252C%2520these%2520methods%2520face%2520great%2520challenges%2520in%2520handling%250Ahigh-precision%2520point%2520clouds%2520due%2520to%2520the%2520large%2520scale%2520and%2520complex%2520structure.%2520In%250Athis%2520study%252C%2520a%2520Down-Up%2520Sampling%2520Network%2520%2528DUS-Net%2529%2520is%2520proposed%2520to%2520reconstruct%250Ahigh-precision%2520point%2520clouds%2520for%25203D%2520anomaly%2520detection%2520by%2520preserving%2520the%2520group%250Acenter%2520geometric%2520structure.%2520The%2520DUS-Net%2520first%2520introduces%2520a%2520Noise%2520Generation%250Amodule%2520to%2520generate%2520noisy%2520patches%252C%2520which%2520facilitates%2520the%2520diversity%2520of%2520training%250Adata%2520and%2520strengthens%2520the%2520feature%2520representation%2520for%2520reconstruction.%2520Then%252C%2520a%250ADown-sampling%2520Network%2520%2528Down-Net%2529%2520is%2520developed%2520to%2520learn%2520an%2520anomaly-free%2520center%250Apoint%2520cloud%2520from%2520patches%2520with%2520noise%2520injection.%2520Subsequently%252C%2520an%2520Up-sampling%250ANetwork%2520%2528Up-Net%2529%2520is%2520designed%2520to%2520reconstruct%2520high-precision%2520point%2520clouds%2520by%250Afusing%2520multi-scale%2520up-sampling%2520features.%2520Our%2520method%2520leverages%2520group%2520centers%2520for%250Aconstruction%252C%2520enabling%2520the%2520preservation%2520of%2520geometric%2520structure%2520and%2520providing%2520a%250Amore%2520precise%2520point%2520cloud.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%250Aof%2520our%2520proposed%2520method%252C%2520achieving%2520state-of-the-art%2520%2528SOTA%2529%2520performance%2520with%2520an%250AObject-level%2520AUROC%2520of%252079.9%2525%2520and%252079.5%2525%252C%2520and%2520a%2520Point-level%2520AUROC%2520of%252071.2%2525%2520and%250A84.7%2525%2520on%2520the%2520Real3D-AD%2520and%2520Anomaly-ShapeNet%2520datasets%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.03903v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Taming%20Anomalies%20with%20Down-Up%20Sampling%20Networks%3A%20Group%20Center%20Preserving%0A%20%20Reconstruction%20for%203D%20Anomaly%20Detection&entry.906535625=Hanzhe%20Liang%20and%20Jie%20Zhang%20and%20Tao%20Dai%20and%20Linlin%20Shen%20and%20Jinbao%20Wang%20and%20Can%20Gao&entry.1292438233=%20%20Reconstruction-based%20methods%20have%20demonstrated%20very%20promising%20results%20for%203D%0Aanomaly%20detection.%20However%2C%20these%20methods%20face%20great%20challenges%20in%20handling%0Ahigh-precision%20point%20clouds%20due%20to%20the%20large%20scale%20and%20complex%20structure.%20In%0Athis%20study%2C%20a%20Down-Up%20Sampling%20Network%20%28DUS-Net%29%20is%20proposed%20to%20reconstruct%0Ahigh-precision%20point%20clouds%20for%203D%20anomaly%20detection%20by%20preserving%20the%20group%0Acenter%20geometric%20structure.%20The%20DUS-Net%20first%20introduces%20a%20Noise%20Generation%0Amodule%20to%20generate%20noisy%20patches%2C%20which%20facilitates%20the%20diversity%20of%20training%0Adata%20and%20strengthens%20the%20feature%20representation%20for%20reconstruction.%20Then%2C%20a%0ADown-sampling%20Network%20%28Down-Net%29%20is%20developed%20to%20learn%20an%20anomaly-free%20center%0Apoint%20cloud%20from%20patches%20with%20noise%20injection.%20Subsequently%2C%20an%20Up-sampling%0ANetwork%20%28Up-Net%29%20is%20designed%20to%20reconstruct%20high-precision%20point%20clouds%20by%0Afusing%20multi-scale%20up-sampling%20features.%20Our%20method%20leverages%20group%20centers%20for%0Aconstruction%2C%20enabling%20the%20preservation%20of%20geometric%20structure%20and%20providing%20a%0Amore%20precise%20point%20cloud.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%0Aof%20our%20proposed%20method%2C%20achieving%20state-of-the-art%20%28SOTA%29%20performance%20with%20an%0AObject-level%20AUROC%20of%2079.9%25%20and%2079.5%25%2C%20and%20a%20Point-level%20AUROC%20of%2071.2%25%20and%0A84.7%25%20on%20the%20Real3D-AD%20and%20Anomaly-ShapeNet%20datasets%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.03903v2&entry.124074799=Read"},
{"title": "HAM: Hierarchical Adapter Merging for Scalable Continual Learning", "author": "Eric Nuertey Coleman and Luigi Quarantiello and Samrat Mukherjee and Julio Hurtado and Vincenzo Lomonaco", "abstract": "  Continual learning is an essential capability of human cognition, yet it\nposes significant challenges for current deep learning models. The primary\nissue is that new knowledge can interfere with previously learned information,\ncausing the model to forget earlier knowledge in favor of the new, a phenomenon\nknown as catastrophic forgetting. Although large pre-trained models can\npartially mitigate forgetting by leveraging their existing knowledge and\nover-parameterization, they often struggle when confronted with novel data\ndistributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,\nenable efficient adaptation to new knowledge. However, they still face\nchallenges in scaling to dynamic learning scenarios and long sequences of\ntasks, as maintaining one adapter per task introduces complexity and increases\nthe potential for interference. In this paper, we introduce Hierarchical\nAdapters Merging (HAM), a novel framework that dynamically combines adapters\nfrom different tasks during training. This approach enables HAM to scale\neffectively, allowing it to manage more tasks than competing baselines with\nimproved efficiency. To achieve this, HAM maintains a fixed set of groups that\nhierarchically consolidate new adapters. For each task, HAM trains a low-rank\nadapter along with an importance scalar, then dynamically groups tasks based on\nadapter similarity. Within each group, adapters are pruned, scaled and merge,\nfacilitating transfer learning between related tasks. Extensive experiments on\nthree vision benchmarks show that HAM significantly outperforms\nstate-of-the-art methods, particularly as the number of tasks increases.\n", "link": "http://arxiv.org/abs/2509.13211v1", "date": "2025-09-16", "relevancy": 2.7801, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6381}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5199}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.51}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HAM%3A%20Hierarchical%20Adapter%20Merging%20for%20Scalable%20Continual%20Learning&body=Title%3A%20HAM%3A%20Hierarchical%20Adapter%20Merging%20for%20Scalable%20Continual%20Learning%0AAuthor%3A%20Eric%20Nuertey%20Coleman%20and%20Luigi%20Quarantiello%20and%20Samrat%20Mukherjee%20and%20Julio%20Hurtado%20and%20Vincenzo%20Lomonaco%0AAbstract%3A%20%20%20Continual%20learning%20is%20an%20essential%20capability%20of%20human%20cognition%2C%20yet%20it%0Aposes%20significant%20challenges%20for%20current%20deep%20learning%20models.%20The%20primary%0Aissue%20is%20that%20new%20knowledge%20can%20interfere%20with%20previously%20learned%20information%2C%0Acausing%20the%20model%20to%20forget%20earlier%20knowledge%20in%20favor%20of%20the%20new%2C%20a%20phenomenon%0Aknown%20as%20catastrophic%20forgetting.%20Although%20large%20pre-trained%20models%20can%0Apartially%20mitigate%20forgetting%20by%20leveraging%20their%20existing%20knowledge%20and%0Aover-parameterization%2C%20they%20often%20struggle%20when%20confronted%20with%20novel%20data%0Adistributions.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%2C%20such%20as%20LoRA%2C%0Aenable%20efficient%20adaptation%20to%20new%20knowledge.%20However%2C%20they%20still%20face%0Achallenges%20in%20scaling%20to%20dynamic%20learning%20scenarios%20and%20long%20sequences%20of%0Atasks%2C%20as%20maintaining%20one%20adapter%20per%20task%20introduces%20complexity%20and%20increases%0Athe%20potential%20for%20interference.%20In%20this%20paper%2C%20we%20introduce%20Hierarchical%0AAdapters%20Merging%20%28HAM%29%2C%20a%20novel%20framework%20that%20dynamically%20combines%20adapters%0Afrom%20different%20tasks%20during%20training.%20This%20approach%20enables%20HAM%20to%20scale%0Aeffectively%2C%20allowing%20it%20to%20manage%20more%20tasks%20than%20competing%20baselines%20with%0Aimproved%20efficiency.%20To%20achieve%20this%2C%20HAM%20maintains%20a%20fixed%20set%20of%20groups%20that%0Ahierarchically%20consolidate%20new%20adapters.%20For%20each%20task%2C%20HAM%20trains%20a%20low-rank%0Aadapter%20along%20with%20an%20importance%20scalar%2C%20then%20dynamically%20groups%20tasks%20based%20on%0Aadapter%20similarity.%20Within%20each%20group%2C%20adapters%20are%20pruned%2C%20scaled%20and%20merge%2C%0Afacilitating%20transfer%20learning%20between%20related%20tasks.%20Extensive%20experiments%20on%0Athree%20vision%20benchmarks%20show%20that%20HAM%20significantly%20outperforms%0Astate-of-the-art%20methods%2C%20particularly%20as%20the%20number%20of%20tasks%20increases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13211v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHAM%253A%2520Hierarchical%2520Adapter%2520Merging%2520for%2520Scalable%2520Continual%2520Learning%26entry.906535625%3DEric%2520Nuertey%2520Coleman%2520and%2520Luigi%2520Quarantiello%2520and%2520Samrat%2520Mukherjee%2520and%2520Julio%2520Hurtado%2520and%2520Vincenzo%2520Lomonaco%26entry.1292438233%3D%2520%2520Continual%2520learning%2520is%2520an%2520essential%2520capability%2520of%2520human%2520cognition%252C%2520yet%2520it%250Aposes%2520significant%2520challenges%2520for%2520current%2520deep%2520learning%2520models.%2520The%2520primary%250Aissue%2520is%2520that%2520new%2520knowledge%2520can%2520interfere%2520with%2520previously%2520learned%2520information%252C%250Acausing%2520the%2520model%2520to%2520forget%2520earlier%2520knowledge%2520in%2520favor%2520of%2520the%2520new%252C%2520a%2520phenomenon%250Aknown%2520as%2520catastrophic%2520forgetting.%2520Although%2520large%2520pre-trained%2520models%2520can%250Apartially%2520mitigate%2520forgetting%2520by%2520leveraging%2520their%2520existing%2520knowledge%2520and%250Aover-parameterization%252C%2520they%2520often%2520struggle%2520when%2520confronted%2520with%2520novel%2520data%250Adistributions.%2520Parameter-Efficient%2520Fine-Tuning%2520%2528PEFT%2529%2520methods%252C%2520such%2520as%2520LoRA%252C%250Aenable%2520efficient%2520adaptation%2520to%2520new%2520knowledge.%2520However%252C%2520they%2520still%2520face%250Achallenges%2520in%2520scaling%2520to%2520dynamic%2520learning%2520scenarios%2520and%2520long%2520sequences%2520of%250Atasks%252C%2520as%2520maintaining%2520one%2520adapter%2520per%2520task%2520introduces%2520complexity%2520and%2520increases%250Athe%2520potential%2520for%2520interference.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520Hierarchical%250AAdapters%2520Merging%2520%2528HAM%2529%252C%2520a%2520novel%2520framework%2520that%2520dynamically%2520combines%2520adapters%250Afrom%2520different%2520tasks%2520during%2520training.%2520This%2520approach%2520enables%2520HAM%2520to%2520scale%250Aeffectively%252C%2520allowing%2520it%2520to%2520manage%2520more%2520tasks%2520than%2520competing%2520baselines%2520with%250Aimproved%2520efficiency.%2520To%2520achieve%2520this%252C%2520HAM%2520maintains%2520a%2520fixed%2520set%2520of%2520groups%2520that%250Ahierarchically%2520consolidate%2520new%2520adapters.%2520For%2520each%2520task%252C%2520HAM%2520trains%2520a%2520low-rank%250Aadapter%2520along%2520with%2520an%2520importance%2520scalar%252C%2520then%2520dynamically%2520groups%2520tasks%2520based%2520on%250Aadapter%2520similarity.%2520Within%2520each%2520group%252C%2520adapters%2520are%2520pruned%252C%2520scaled%2520and%2520merge%252C%250Afacilitating%2520transfer%2520learning%2520between%2520related%2520tasks.%2520Extensive%2520experiments%2520on%250Athree%2520vision%2520benchmarks%2520show%2520that%2520HAM%2520significantly%2520outperforms%250Astate-of-the-art%2520methods%252C%2520particularly%2520as%2520the%2520number%2520of%2520tasks%2520increases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13211v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HAM%3A%20Hierarchical%20Adapter%20Merging%20for%20Scalable%20Continual%20Learning&entry.906535625=Eric%20Nuertey%20Coleman%20and%20Luigi%20Quarantiello%20and%20Samrat%20Mukherjee%20and%20Julio%20Hurtado%20and%20Vincenzo%20Lomonaco&entry.1292438233=%20%20Continual%20learning%20is%20an%20essential%20capability%20of%20human%20cognition%2C%20yet%20it%0Aposes%20significant%20challenges%20for%20current%20deep%20learning%20models.%20The%20primary%0Aissue%20is%20that%20new%20knowledge%20can%20interfere%20with%20previously%20learned%20information%2C%0Acausing%20the%20model%20to%20forget%20earlier%20knowledge%20in%20favor%20of%20the%20new%2C%20a%20phenomenon%0Aknown%20as%20catastrophic%20forgetting.%20Although%20large%20pre-trained%20models%20can%0Apartially%20mitigate%20forgetting%20by%20leveraging%20their%20existing%20knowledge%20and%0Aover-parameterization%2C%20they%20often%20struggle%20when%20confronted%20with%20novel%20data%0Adistributions.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%2C%20such%20as%20LoRA%2C%0Aenable%20efficient%20adaptation%20to%20new%20knowledge.%20However%2C%20they%20still%20face%0Achallenges%20in%20scaling%20to%20dynamic%20learning%20scenarios%20and%20long%20sequences%20of%0Atasks%2C%20as%20maintaining%20one%20adapter%20per%20task%20introduces%20complexity%20and%20increases%0Athe%20potential%20for%20interference.%20In%20this%20paper%2C%20we%20introduce%20Hierarchical%0AAdapters%20Merging%20%28HAM%29%2C%20a%20novel%20framework%20that%20dynamically%20combines%20adapters%0Afrom%20different%20tasks%20during%20training.%20This%20approach%20enables%20HAM%20to%20scale%0Aeffectively%2C%20allowing%20it%20to%20manage%20more%20tasks%20than%20competing%20baselines%20with%0Aimproved%20efficiency.%20To%20achieve%20this%2C%20HAM%20maintains%20a%20fixed%20set%20of%20groups%20that%0Ahierarchically%20consolidate%20new%20adapters.%20For%20each%20task%2C%20HAM%20trains%20a%20low-rank%0Aadapter%20along%20with%20an%20importance%20scalar%2C%20then%20dynamically%20groups%20tasks%20based%20on%0Aadapter%20similarity.%20Within%20each%20group%2C%20adapters%20are%20pruned%2C%20scaled%20and%20merge%2C%0Afacilitating%20transfer%20learning%20between%20related%20tasks.%20Extensive%20experiments%20on%0Athree%20vision%20benchmarks%20show%20that%20HAM%20significantly%20outperforms%0Astate-of-the-art%20methods%2C%20particularly%20as%20the%20number%20of%20tasks%20increases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13211v1&entry.124074799=Read"},
{"title": "A Synthetic Data Pipeline for Supporting Manufacturing SMEs in Visual\n  Assembly Control", "author": "Jonas Werheid and Shengjie He and Aymen Gannouni and Anas Abdelrazeq and Robert H. Schmitt", "abstract": "  Quality control of assembly processes is essential in manufacturing to ensure\nnot only the quality of individual components but also their proper integration\ninto the final product. To assist in this matter, automated assembly control\nusing computer vision methods has been widely implemented. However, the costs\nassociated with image acquisition, annotation, and training of computer vision\nalgorithms pose challenges for integration, especially for small- and\nmedium-sized enterprises (SMEs), which often lack the resources for extensive\ntraining, data collection, and manual image annotation. Synthetic data offers\nthe potential to reduce manual data collection and labeling. Nevertheless, its\npractical application in the context of assembly quality remains limited. In\nthis work, we present a novel approach for easily integrable and data-efficient\nvisual assembly control. Our approach leverages simulated scene generation\nbased on computer-aided design (CAD) data and object detection algorithms. The\nresults demonstrate a time-saving pipeline for generating image data in\nmanufacturing environments, achieving a mean Average Precision (mAP@0.5:0.95)\nup to 99,5% for correctly identifying instances of synthetic planetary gear\nsystem components within our simulated training data, and up to 93% when\ntransferred to real-world camera-captured testing data. This research\nhighlights the effectiveness of synthetic data generation within an adaptable\npipeline and underscores its potential to support SMEs in implementing\nresource-efficient visual assembly control solutions.\n", "link": "http://arxiv.org/abs/2509.13089v1", "date": "2025-09-16", "relevancy": 2.7425, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5488}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5488}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Synthetic%20Data%20Pipeline%20for%20Supporting%20Manufacturing%20SMEs%20in%20Visual%0A%20%20Assembly%20Control&body=Title%3A%20A%20Synthetic%20Data%20Pipeline%20for%20Supporting%20Manufacturing%20SMEs%20in%20Visual%0A%20%20Assembly%20Control%0AAuthor%3A%20Jonas%20Werheid%20and%20Shengjie%20He%20and%20Aymen%20Gannouni%20and%20Anas%20Abdelrazeq%20and%20Robert%20H.%20Schmitt%0AAbstract%3A%20%20%20Quality%20control%20of%20assembly%20processes%20is%20essential%20in%20manufacturing%20to%20ensure%0Anot%20only%20the%20quality%20of%20individual%20components%20but%20also%20their%20proper%20integration%0Ainto%20the%20final%20product.%20To%20assist%20in%20this%20matter%2C%20automated%20assembly%20control%0Ausing%20computer%20vision%20methods%20has%20been%20widely%20implemented.%20However%2C%20the%20costs%0Aassociated%20with%20image%20acquisition%2C%20annotation%2C%20and%20training%20of%20computer%20vision%0Aalgorithms%20pose%20challenges%20for%20integration%2C%20especially%20for%20small-%20and%0Amedium-sized%20enterprises%20%28SMEs%29%2C%20which%20often%20lack%20the%20resources%20for%20extensive%0Atraining%2C%20data%20collection%2C%20and%20manual%20image%20annotation.%20Synthetic%20data%20offers%0Athe%20potential%20to%20reduce%20manual%20data%20collection%20and%20labeling.%20Nevertheless%2C%20its%0Apractical%20application%20in%20the%20context%20of%20assembly%20quality%20remains%20limited.%20In%0Athis%20work%2C%20we%20present%20a%20novel%20approach%20for%20easily%20integrable%20and%20data-efficient%0Avisual%20assembly%20control.%20Our%20approach%20leverages%20simulated%20scene%20generation%0Abased%20on%20computer-aided%20design%20%28CAD%29%20data%20and%20object%20detection%20algorithms.%20The%0Aresults%20demonstrate%20a%20time-saving%20pipeline%20for%20generating%20image%20data%20in%0Amanufacturing%20environments%2C%20achieving%20a%20mean%20Average%20Precision%20%28mAP%400.5%3A0.95%29%0Aup%20to%2099%2C5%25%20for%20correctly%20identifying%20instances%20of%20synthetic%20planetary%20gear%0Asystem%20components%20within%20our%20simulated%20training%20data%2C%20and%20up%20to%2093%25%20when%0Atransferred%20to%20real-world%20camera-captured%20testing%20data.%20This%20research%0Ahighlights%20the%20effectiveness%20of%20synthetic%20data%20generation%20within%20an%20adaptable%0Apipeline%20and%20underscores%20its%20potential%20to%20support%20SMEs%20in%20implementing%0Aresource-efficient%20visual%20assembly%20control%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13089v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Synthetic%2520Data%2520Pipeline%2520for%2520Supporting%2520Manufacturing%2520SMEs%2520in%2520Visual%250A%2520%2520Assembly%2520Control%26entry.906535625%3DJonas%2520Werheid%2520and%2520Shengjie%2520He%2520and%2520Aymen%2520Gannouni%2520and%2520Anas%2520Abdelrazeq%2520and%2520Robert%2520H.%2520Schmitt%26entry.1292438233%3D%2520%2520Quality%2520control%2520of%2520assembly%2520processes%2520is%2520essential%2520in%2520manufacturing%2520to%2520ensure%250Anot%2520only%2520the%2520quality%2520of%2520individual%2520components%2520but%2520also%2520their%2520proper%2520integration%250Ainto%2520the%2520final%2520product.%2520To%2520assist%2520in%2520this%2520matter%252C%2520automated%2520assembly%2520control%250Ausing%2520computer%2520vision%2520methods%2520has%2520been%2520widely%2520implemented.%2520However%252C%2520the%2520costs%250Aassociated%2520with%2520image%2520acquisition%252C%2520annotation%252C%2520and%2520training%2520of%2520computer%2520vision%250Aalgorithms%2520pose%2520challenges%2520for%2520integration%252C%2520especially%2520for%2520small-%2520and%250Amedium-sized%2520enterprises%2520%2528SMEs%2529%252C%2520which%2520often%2520lack%2520the%2520resources%2520for%2520extensive%250Atraining%252C%2520data%2520collection%252C%2520and%2520manual%2520image%2520annotation.%2520Synthetic%2520data%2520offers%250Athe%2520potential%2520to%2520reduce%2520manual%2520data%2520collection%2520and%2520labeling.%2520Nevertheless%252C%2520its%250Apractical%2520application%2520in%2520the%2520context%2520of%2520assembly%2520quality%2520remains%2520limited.%2520In%250Athis%2520work%252C%2520we%2520present%2520a%2520novel%2520approach%2520for%2520easily%2520integrable%2520and%2520data-efficient%250Avisual%2520assembly%2520control.%2520Our%2520approach%2520leverages%2520simulated%2520scene%2520generation%250Abased%2520on%2520computer-aided%2520design%2520%2528CAD%2529%2520data%2520and%2520object%2520detection%2520algorithms.%2520The%250Aresults%2520demonstrate%2520a%2520time-saving%2520pipeline%2520for%2520generating%2520image%2520data%2520in%250Amanufacturing%2520environments%252C%2520achieving%2520a%2520mean%2520Average%2520Precision%2520%2528mAP%25400.5%253A0.95%2529%250Aup%2520to%252099%252C5%2525%2520for%2520correctly%2520identifying%2520instances%2520of%2520synthetic%2520planetary%2520gear%250Asystem%2520components%2520within%2520our%2520simulated%2520training%2520data%252C%2520and%2520up%2520to%252093%2525%2520when%250Atransferred%2520to%2520real-world%2520camera-captured%2520testing%2520data.%2520This%2520research%250Ahighlights%2520the%2520effectiveness%2520of%2520synthetic%2520data%2520generation%2520within%2520an%2520adaptable%250Apipeline%2520and%2520underscores%2520its%2520potential%2520to%2520support%2520SMEs%2520in%2520implementing%250Aresource-efficient%2520visual%2520assembly%2520control%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13089v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Synthetic%20Data%20Pipeline%20for%20Supporting%20Manufacturing%20SMEs%20in%20Visual%0A%20%20Assembly%20Control&entry.906535625=Jonas%20Werheid%20and%20Shengjie%20He%20and%20Aymen%20Gannouni%20and%20Anas%20Abdelrazeq%20and%20Robert%20H.%20Schmitt&entry.1292438233=%20%20Quality%20control%20of%20assembly%20processes%20is%20essential%20in%20manufacturing%20to%20ensure%0Anot%20only%20the%20quality%20of%20individual%20components%20but%20also%20their%20proper%20integration%0Ainto%20the%20final%20product.%20To%20assist%20in%20this%20matter%2C%20automated%20assembly%20control%0Ausing%20computer%20vision%20methods%20has%20been%20widely%20implemented.%20However%2C%20the%20costs%0Aassociated%20with%20image%20acquisition%2C%20annotation%2C%20and%20training%20of%20computer%20vision%0Aalgorithms%20pose%20challenges%20for%20integration%2C%20especially%20for%20small-%20and%0Amedium-sized%20enterprises%20%28SMEs%29%2C%20which%20often%20lack%20the%20resources%20for%20extensive%0Atraining%2C%20data%20collection%2C%20and%20manual%20image%20annotation.%20Synthetic%20data%20offers%0Athe%20potential%20to%20reduce%20manual%20data%20collection%20and%20labeling.%20Nevertheless%2C%20its%0Apractical%20application%20in%20the%20context%20of%20assembly%20quality%20remains%20limited.%20In%0Athis%20work%2C%20we%20present%20a%20novel%20approach%20for%20easily%20integrable%20and%20data-efficient%0Avisual%20assembly%20control.%20Our%20approach%20leverages%20simulated%20scene%20generation%0Abased%20on%20computer-aided%20design%20%28CAD%29%20data%20and%20object%20detection%20algorithms.%20The%0Aresults%20demonstrate%20a%20time-saving%20pipeline%20for%20generating%20image%20data%20in%0Amanufacturing%20environments%2C%20achieving%20a%20mean%20Average%20Precision%20%28mAP%400.5%3A0.95%29%0Aup%20to%2099%2C5%25%20for%20correctly%20identifying%20instances%20of%20synthetic%20planetary%20gear%0Asystem%20components%20within%20our%20simulated%20training%20data%2C%20and%20up%20to%2093%25%20when%0Atransferred%20to%20real-world%20camera-captured%20testing%20data.%20This%20research%0Ahighlights%20the%20effectiveness%20of%20synthetic%20data%20generation%20within%20an%20adaptable%0Apipeline%20and%20underscores%20its%20potential%20to%20support%20SMEs%20in%20implementing%0Aresource-efficient%20visual%20assembly%20control%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13089v1&entry.124074799=Read"},
{"title": "ByDeWay: Boost Your multimodal LLM with DEpth prompting in a\n  Training-Free Way", "author": "Rajarshi Roy and Devleena Das and Ankesh Banerjee and Arjya Bhattacharjee and Kousik Dasgupta and Subarna Tripathi", "abstract": "  We introduce ByDeWay, a training-free framework designed to enhance the\nperformance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel\nprompting strategy called Layered-Depth-Based Prompting (LDP), which improves\nspatial reasoning and grounding without modifying any model parameters. It\nsegments the scene into closest, mid-range, and farthest layers using monocular\ndepth estimation, then generates region-specific captions with a grounded\nvision-language model. These structured, depth-aware captions are appended to\nthe image-question prompt, enriching it with spatial context. This guides MLLMs\nto produce more grounded and less hallucinated responses. Our method is\nlightweight, modular, and compatible with black-box MLLMs. Experiments on\nhallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show\nconsistent improvements across multiple MLLMs, validating the effectiveness of\ndepth-aware prompting in a zero-training setting.\n", "link": "http://arxiv.org/abs/2507.08679v2", "date": "2025-09-16", "relevancy": 2.7295, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5569}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ByDeWay%3A%20Boost%20Your%20multimodal%20LLM%20with%20DEpth%20prompting%20in%20a%0A%20%20Training-Free%20Way&body=Title%3A%20ByDeWay%3A%20Boost%20Your%20multimodal%20LLM%20with%20DEpth%20prompting%20in%20a%0A%20%20Training-Free%20Way%0AAuthor%3A%20Rajarshi%20Roy%20and%20Devleena%20Das%20and%20Ankesh%20Banerjee%20and%20Arjya%20Bhattacharjee%20and%20Kousik%20Dasgupta%20and%20Subarna%20Tripathi%0AAbstract%3A%20%20%20We%20introduce%20ByDeWay%2C%20a%20training-free%20framework%20designed%20to%20enhance%20the%0Aperformance%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20ByDeWay%20uses%20a%20novel%0Aprompting%20strategy%20called%20Layered-Depth-Based%20Prompting%20%28LDP%29%2C%20which%20improves%0Aspatial%20reasoning%20and%20grounding%20without%20modifying%20any%20model%20parameters.%20It%0Asegments%20the%20scene%20into%20closest%2C%20mid-range%2C%20and%20farthest%20layers%20using%20monocular%0Adepth%20estimation%2C%20then%20generates%20region-specific%20captions%20with%20a%20grounded%0Avision-language%20model.%20These%20structured%2C%20depth-aware%20captions%20are%20appended%20to%0Athe%20image-question%20prompt%2C%20enriching%20it%20with%20spatial%20context.%20This%20guides%20MLLMs%0Ato%20produce%20more%20grounded%20and%20less%20hallucinated%20responses.%20Our%20method%20is%0Alightweight%2C%20modular%2C%20and%20compatible%20with%20black-box%20MLLMs.%20Experiments%20on%0Ahallucination-sensitive%20%28POPE%29%20and%20reasoning-intensive%20%28GQA%29%20benchmarks%20show%0Aconsistent%20improvements%20across%20multiple%20MLLMs%2C%20validating%20the%20effectiveness%20of%0Adepth-aware%20prompting%20in%20a%20zero-training%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.08679v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DByDeWay%253A%2520Boost%2520Your%2520multimodal%2520LLM%2520with%2520DEpth%2520prompting%2520in%2520a%250A%2520%2520Training-Free%2520Way%26entry.906535625%3DRajarshi%2520Roy%2520and%2520Devleena%2520Das%2520and%2520Ankesh%2520Banerjee%2520and%2520Arjya%2520Bhattacharjee%2520and%2520Kousik%2520Dasgupta%2520and%2520Subarna%2520Tripathi%26entry.1292438233%3D%2520%2520We%2520introduce%2520ByDeWay%252C%2520a%2520training-free%2520framework%2520designed%2520to%2520enhance%2520the%250Aperformance%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529.%2520ByDeWay%2520uses%2520a%2520novel%250Aprompting%2520strategy%2520called%2520Layered-Depth-Based%2520Prompting%2520%2528LDP%2529%252C%2520which%2520improves%250Aspatial%2520reasoning%2520and%2520grounding%2520without%2520modifying%2520any%2520model%2520parameters.%2520It%250Asegments%2520the%2520scene%2520into%2520closest%252C%2520mid-range%252C%2520and%2520farthest%2520layers%2520using%2520monocular%250Adepth%2520estimation%252C%2520then%2520generates%2520region-specific%2520captions%2520with%2520a%2520grounded%250Avision-language%2520model.%2520These%2520structured%252C%2520depth-aware%2520captions%2520are%2520appended%2520to%250Athe%2520image-question%2520prompt%252C%2520enriching%2520it%2520with%2520spatial%2520context.%2520This%2520guides%2520MLLMs%250Ato%2520produce%2520more%2520grounded%2520and%2520less%2520hallucinated%2520responses.%2520Our%2520method%2520is%250Alightweight%252C%2520modular%252C%2520and%2520compatible%2520with%2520black-box%2520MLLMs.%2520Experiments%2520on%250Ahallucination-sensitive%2520%2528POPE%2529%2520and%2520reasoning-intensive%2520%2528GQA%2529%2520benchmarks%2520show%250Aconsistent%2520improvements%2520across%2520multiple%2520MLLMs%252C%2520validating%2520the%2520effectiveness%2520of%250Adepth-aware%2520prompting%2520in%2520a%2520zero-training%2520setting.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.08679v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ByDeWay%3A%20Boost%20Your%20multimodal%20LLM%20with%20DEpth%20prompting%20in%20a%0A%20%20Training-Free%20Way&entry.906535625=Rajarshi%20Roy%20and%20Devleena%20Das%20and%20Ankesh%20Banerjee%20and%20Arjya%20Bhattacharjee%20and%20Kousik%20Dasgupta%20and%20Subarna%20Tripathi&entry.1292438233=%20%20We%20introduce%20ByDeWay%2C%20a%20training-free%20framework%20designed%20to%20enhance%20the%0Aperformance%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%20ByDeWay%20uses%20a%20novel%0Aprompting%20strategy%20called%20Layered-Depth-Based%20Prompting%20%28LDP%29%2C%20which%20improves%0Aspatial%20reasoning%20and%20grounding%20without%20modifying%20any%20model%20parameters.%20It%0Asegments%20the%20scene%20into%20closest%2C%20mid-range%2C%20and%20farthest%20layers%20using%20monocular%0Adepth%20estimation%2C%20then%20generates%20region-specific%20captions%20with%20a%20grounded%0Avision-language%20model.%20These%20structured%2C%20depth-aware%20captions%20are%20appended%20to%0Athe%20image-question%20prompt%2C%20enriching%20it%20with%20spatial%20context.%20This%20guides%20MLLMs%0Ato%20produce%20more%20grounded%20and%20less%20hallucinated%20responses.%20Our%20method%20is%0Alightweight%2C%20modular%2C%20and%20compatible%20with%20black-box%20MLLMs.%20Experiments%20on%0Ahallucination-sensitive%20%28POPE%29%20and%20reasoning-intensive%20%28GQA%29%20benchmarks%20show%0Aconsistent%20improvements%20across%20multiple%20MLLMs%2C%20validating%20the%20effectiveness%20of%0Adepth-aware%20prompting%20in%20a%20zero-training%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.08679v2&entry.124074799=Read"},
{"title": "RingMo-Aerial: An Aerial Remote Sensing Foundation Model With Affine\n  Transformation Contrastive Learning", "author": "Wenhui Diao and Haichen Yu and Kaiyue Kang and Tong Ling and Di Liu and Yingchao Feng and Hanbo Bi and Libo Ren and Xuexue Li and Yongqiang Mao and Xian Sun", "abstract": "  Aerial Remote Sensing (ARS) vision tasks present significant challenges due\nto the unique viewing angle characteristics. Existing research has primarily\nfocused on algorithms for specific tasks, which have limited applicability in a\nbroad range of ARS vision applications. This paper proposes RingMo-Aerial,\naiming to fill the gap in foundation model research in the field of ARS vision.\nA Frequency-Enhanced Multi-Head Self-Attention (FE-MSA) mechanism is introduced\nto strengthen the model's capacity for small-object representation.\nComplementarily, an affine transformation-based contrastive learning method\nimproves its adaptability to the tilted viewing angles inherent in ARS tasks.\nFurthermore, the ARS-Adapter, an efficient parameter fine-tuning method, is\nproposed to improve the model's adaptability and performance in various ARS\nvision tasks. Experimental results demonstrate that RingMo-Aerial achieves SOTA\nperformance on multiple downstream tasks. This indicates the practicality and\nefficacy of RingMo-Aerial in enhancing the performance of ARS vision tasks.\n", "link": "http://arxiv.org/abs/2409.13366v4", "date": "2025-09-16", "relevancy": 2.7151, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5524}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5383}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5383}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RingMo-Aerial%3A%20An%20Aerial%20Remote%20Sensing%20Foundation%20Model%20With%20Affine%0A%20%20Transformation%20Contrastive%20Learning&body=Title%3A%20RingMo-Aerial%3A%20An%20Aerial%20Remote%20Sensing%20Foundation%20Model%20With%20Affine%0A%20%20Transformation%20Contrastive%20Learning%0AAuthor%3A%20Wenhui%20Diao%20and%20Haichen%20Yu%20and%20Kaiyue%20Kang%20and%20Tong%20Ling%20and%20Di%20Liu%20and%20Yingchao%20Feng%20and%20Hanbo%20Bi%20and%20Libo%20Ren%20and%20Xuexue%20Li%20and%20Yongqiang%20Mao%20and%20Xian%20Sun%0AAbstract%3A%20%20%20Aerial%20Remote%20Sensing%20%28ARS%29%20vision%20tasks%20present%20significant%20challenges%20due%0Ato%20the%20unique%20viewing%20angle%20characteristics.%20Existing%20research%20has%20primarily%0Afocused%20on%20algorithms%20for%20specific%20tasks%2C%20which%20have%20limited%20applicability%20in%20a%0Abroad%20range%20of%20ARS%20vision%20applications.%20This%20paper%20proposes%20RingMo-Aerial%2C%0Aaiming%20to%20fill%20the%20gap%20in%20foundation%20model%20research%20in%20the%20field%20of%20ARS%20vision.%0AA%20Frequency-Enhanced%20Multi-Head%20Self-Attention%20%28FE-MSA%29%20mechanism%20is%20introduced%0Ato%20strengthen%20the%20model%27s%20capacity%20for%20small-object%20representation.%0AComplementarily%2C%20an%20affine%20transformation-based%20contrastive%20learning%20method%0Aimproves%20its%20adaptability%20to%20the%20tilted%20viewing%20angles%20inherent%20in%20ARS%20tasks.%0AFurthermore%2C%20the%20ARS-Adapter%2C%20an%20efficient%20parameter%20fine-tuning%20method%2C%20is%0Aproposed%20to%20improve%20the%20model%27s%20adaptability%20and%20performance%20in%20various%20ARS%0Avision%20tasks.%20Experimental%20results%20demonstrate%20that%20RingMo-Aerial%20achieves%20SOTA%0Aperformance%20on%20multiple%20downstream%20tasks.%20This%20indicates%20the%20practicality%20and%0Aefficacy%20of%20RingMo-Aerial%20in%20enhancing%20the%20performance%20of%20ARS%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13366v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRingMo-Aerial%253A%2520An%2520Aerial%2520Remote%2520Sensing%2520Foundation%2520Model%2520With%2520Affine%250A%2520%2520Transformation%2520Contrastive%2520Learning%26entry.906535625%3DWenhui%2520Diao%2520and%2520Haichen%2520Yu%2520and%2520Kaiyue%2520Kang%2520and%2520Tong%2520Ling%2520and%2520Di%2520Liu%2520and%2520Yingchao%2520Feng%2520and%2520Hanbo%2520Bi%2520and%2520Libo%2520Ren%2520and%2520Xuexue%2520Li%2520and%2520Yongqiang%2520Mao%2520and%2520Xian%2520Sun%26entry.1292438233%3D%2520%2520Aerial%2520Remote%2520Sensing%2520%2528ARS%2529%2520vision%2520tasks%2520present%2520significant%2520challenges%2520due%250Ato%2520the%2520unique%2520viewing%2520angle%2520characteristics.%2520Existing%2520research%2520has%2520primarily%250Afocused%2520on%2520algorithms%2520for%2520specific%2520tasks%252C%2520which%2520have%2520limited%2520applicability%2520in%2520a%250Abroad%2520range%2520of%2520ARS%2520vision%2520applications.%2520This%2520paper%2520proposes%2520RingMo-Aerial%252C%250Aaiming%2520to%2520fill%2520the%2520gap%2520in%2520foundation%2520model%2520research%2520in%2520the%2520field%2520of%2520ARS%2520vision.%250AA%2520Frequency-Enhanced%2520Multi-Head%2520Self-Attention%2520%2528FE-MSA%2529%2520mechanism%2520is%2520introduced%250Ato%2520strengthen%2520the%2520model%2527s%2520capacity%2520for%2520small-object%2520representation.%250AComplementarily%252C%2520an%2520affine%2520transformation-based%2520contrastive%2520learning%2520method%250Aimproves%2520its%2520adaptability%2520to%2520the%2520tilted%2520viewing%2520angles%2520inherent%2520in%2520ARS%2520tasks.%250AFurthermore%252C%2520the%2520ARS-Adapter%252C%2520an%2520efficient%2520parameter%2520fine-tuning%2520method%252C%2520is%250Aproposed%2520to%2520improve%2520the%2520model%2527s%2520adaptability%2520and%2520performance%2520in%2520various%2520ARS%250Avision%2520tasks.%2520Experimental%2520results%2520demonstrate%2520that%2520RingMo-Aerial%2520achieves%2520SOTA%250Aperformance%2520on%2520multiple%2520downstream%2520tasks.%2520This%2520indicates%2520the%2520practicality%2520and%250Aefficacy%2520of%2520RingMo-Aerial%2520in%2520enhancing%2520the%2520performance%2520of%2520ARS%2520vision%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13366v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RingMo-Aerial%3A%20An%20Aerial%20Remote%20Sensing%20Foundation%20Model%20With%20Affine%0A%20%20Transformation%20Contrastive%20Learning&entry.906535625=Wenhui%20Diao%20and%20Haichen%20Yu%20and%20Kaiyue%20Kang%20and%20Tong%20Ling%20and%20Di%20Liu%20and%20Yingchao%20Feng%20and%20Hanbo%20Bi%20and%20Libo%20Ren%20and%20Xuexue%20Li%20and%20Yongqiang%20Mao%20and%20Xian%20Sun&entry.1292438233=%20%20Aerial%20Remote%20Sensing%20%28ARS%29%20vision%20tasks%20present%20significant%20challenges%20due%0Ato%20the%20unique%20viewing%20angle%20characteristics.%20Existing%20research%20has%20primarily%0Afocused%20on%20algorithms%20for%20specific%20tasks%2C%20which%20have%20limited%20applicability%20in%20a%0Abroad%20range%20of%20ARS%20vision%20applications.%20This%20paper%20proposes%20RingMo-Aerial%2C%0Aaiming%20to%20fill%20the%20gap%20in%20foundation%20model%20research%20in%20the%20field%20of%20ARS%20vision.%0AA%20Frequency-Enhanced%20Multi-Head%20Self-Attention%20%28FE-MSA%29%20mechanism%20is%20introduced%0Ato%20strengthen%20the%20model%27s%20capacity%20for%20small-object%20representation.%0AComplementarily%2C%20an%20affine%20transformation-based%20contrastive%20learning%20method%0Aimproves%20its%20adaptability%20to%20the%20tilted%20viewing%20angles%20inherent%20in%20ARS%20tasks.%0AFurthermore%2C%20the%20ARS-Adapter%2C%20an%20efficient%20parameter%20fine-tuning%20method%2C%20is%0Aproposed%20to%20improve%20the%20model%27s%20adaptability%20and%20performance%20in%20various%20ARS%0Avision%20tasks.%20Experimental%20results%20demonstrate%20that%20RingMo-Aerial%20achieves%20SOTA%0Aperformance%20on%20multiple%20downstream%20tasks.%20This%20indicates%20the%20practicality%20and%0Aefficacy%20of%20RingMo-Aerial%20in%20enhancing%20the%20performance%20of%20ARS%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13366v4&entry.124074799=Read"},
{"title": "Compressed Video Quality Enhancement: Classifying and Benchmarking over\n  Standards", "author": "Xiem HoangVan and Dang BuiDinh and Sang NguyenQuang and Wen-Hsiao Peng", "abstract": "  Compressed video quality enhancement (CVQE) is crucial for improving user\nexperience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.\nWhile deep learning based CVQE has driven significant progress, existing\nsurveys still suffer from limitations: lack of systematic classification\nlinking methods to specific standards and artifacts, insufficient comparative\nanalysis of architectural paradigms across coding types, and underdeveloped\nbenchmarking practices. To address these gaps, this paper presents three key\ncontributions. First, it introduces a novel taxonomy classifying CVQE methods\nacross architectural paradigms, coding standards, and compressed-domain feature\nutilization. Second, it proposes a unified benchmarking framework integrating\nmodern compression protocols and standard test sequences for fair\nmulti-criteria evaluation. Third, it provides a systematic analysis of the\ncritical trade-offs between reconstruction performance and computational\ncomplexity observed in state-of-the-art methods and highlighting promising\ndirections for future research. This comprehensive review aims to establish a\nfoundation for consistent assessment and informed model selection in CVQE\nresearch and deployment.\n", "link": "http://arxiv.org/abs/2509.10407v2", "date": "2025-09-16", "relevancy": 2.7065, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5457}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Compressed%20Video%20Quality%20Enhancement%3A%20Classifying%20and%20Benchmarking%20over%0A%20%20Standards&body=Title%3A%20Compressed%20Video%20Quality%20Enhancement%3A%20Classifying%20and%20Benchmarking%20over%0A%20%20Standards%0AAuthor%3A%20Xiem%20HoangVan%20and%20Dang%20BuiDinh%20and%20Sang%20NguyenQuang%20and%20Wen-Hsiao%20Peng%0AAbstract%3A%20%20%20Compressed%20video%20quality%20enhancement%20%28CVQE%29%20is%20crucial%20for%20improving%20user%0Aexperience%20with%20lossy%20video%20codecs%20like%20H.264/AVC%2C%20H.265/HEVC%2C%20and%20H.266/VVC.%0AWhile%20deep%20learning%20based%20CVQE%20has%20driven%20significant%20progress%2C%20existing%0Asurveys%20still%20suffer%20from%20limitations%3A%20lack%20of%20systematic%20classification%0Alinking%20methods%20to%20specific%20standards%20and%20artifacts%2C%20insufficient%20comparative%0Aanalysis%20of%20architectural%20paradigms%20across%20coding%20types%2C%20and%20underdeveloped%0Abenchmarking%20practices.%20To%20address%20these%20gaps%2C%20this%20paper%20presents%20three%20key%0Acontributions.%20First%2C%20it%20introduces%20a%20novel%20taxonomy%20classifying%20CVQE%20methods%0Aacross%20architectural%20paradigms%2C%20coding%20standards%2C%20and%20compressed-domain%20feature%0Autilization.%20Second%2C%20it%20proposes%20a%20unified%20benchmarking%20framework%20integrating%0Amodern%20compression%20protocols%20and%20standard%20test%20sequences%20for%20fair%0Amulti-criteria%20evaluation.%20Third%2C%20it%20provides%20a%20systematic%20analysis%20of%20the%0Acritical%20trade-offs%20between%20reconstruction%20performance%20and%20computational%0Acomplexity%20observed%20in%20state-of-the-art%20methods%20and%20highlighting%20promising%0Adirections%20for%20future%20research.%20This%20comprehensive%20review%20aims%20to%20establish%20a%0Afoundation%20for%20consistent%20assessment%20and%20informed%20model%20selection%20in%20CVQE%0Aresearch%20and%20deployment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.10407v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCompressed%2520Video%2520Quality%2520Enhancement%253A%2520Classifying%2520and%2520Benchmarking%2520over%250A%2520%2520Standards%26entry.906535625%3DXiem%2520HoangVan%2520and%2520Dang%2520BuiDinh%2520and%2520Sang%2520NguyenQuang%2520and%2520Wen-Hsiao%2520Peng%26entry.1292438233%3D%2520%2520Compressed%2520video%2520quality%2520enhancement%2520%2528CVQE%2529%2520is%2520crucial%2520for%2520improving%2520user%250Aexperience%2520with%2520lossy%2520video%2520codecs%2520like%2520H.264/AVC%252C%2520H.265/HEVC%252C%2520and%2520H.266/VVC.%250AWhile%2520deep%2520learning%2520based%2520CVQE%2520has%2520driven%2520significant%2520progress%252C%2520existing%250Asurveys%2520still%2520suffer%2520from%2520limitations%253A%2520lack%2520of%2520systematic%2520classification%250Alinking%2520methods%2520to%2520specific%2520standards%2520and%2520artifacts%252C%2520insufficient%2520comparative%250Aanalysis%2520of%2520architectural%2520paradigms%2520across%2520coding%2520types%252C%2520and%2520underdeveloped%250Abenchmarking%2520practices.%2520To%2520address%2520these%2520gaps%252C%2520this%2520paper%2520presents%2520three%2520key%250Acontributions.%2520First%252C%2520it%2520introduces%2520a%2520novel%2520taxonomy%2520classifying%2520CVQE%2520methods%250Aacross%2520architectural%2520paradigms%252C%2520coding%2520standards%252C%2520and%2520compressed-domain%2520feature%250Autilization.%2520Second%252C%2520it%2520proposes%2520a%2520unified%2520benchmarking%2520framework%2520integrating%250Amodern%2520compression%2520protocols%2520and%2520standard%2520test%2520sequences%2520for%2520fair%250Amulti-criteria%2520evaluation.%2520Third%252C%2520it%2520provides%2520a%2520systematic%2520analysis%2520of%2520the%250Acritical%2520trade-offs%2520between%2520reconstruction%2520performance%2520and%2520computational%250Acomplexity%2520observed%2520in%2520state-of-the-art%2520methods%2520and%2520highlighting%2520promising%250Adirections%2520for%2520future%2520research.%2520This%2520comprehensive%2520review%2520aims%2520to%2520establish%2520a%250Afoundation%2520for%2520consistent%2520assessment%2520and%2520informed%2520model%2520selection%2520in%2520CVQE%250Aresearch%2520and%2520deployment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.10407v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compressed%20Video%20Quality%20Enhancement%3A%20Classifying%20and%20Benchmarking%20over%0A%20%20Standards&entry.906535625=Xiem%20HoangVan%20and%20Dang%20BuiDinh%20and%20Sang%20NguyenQuang%20and%20Wen-Hsiao%20Peng&entry.1292438233=%20%20Compressed%20video%20quality%20enhancement%20%28CVQE%29%20is%20crucial%20for%20improving%20user%0Aexperience%20with%20lossy%20video%20codecs%20like%20H.264/AVC%2C%20H.265/HEVC%2C%20and%20H.266/VVC.%0AWhile%20deep%20learning%20based%20CVQE%20has%20driven%20significant%20progress%2C%20existing%0Asurveys%20still%20suffer%20from%20limitations%3A%20lack%20of%20systematic%20classification%0Alinking%20methods%20to%20specific%20standards%20and%20artifacts%2C%20insufficient%20comparative%0Aanalysis%20of%20architectural%20paradigms%20across%20coding%20types%2C%20and%20underdeveloped%0Abenchmarking%20practices.%20To%20address%20these%20gaps%2C%20this%20paper%20presents%20three%20key%0Acontributions.%20First%2C%20it%20introduces%20a%20novel%20taxonomy%20classifying%20CVQE%20methods%0Aacross%20architectural%20paradigms%2C%20coding%20standards%2C%20and%20compressed-domain%20feature%0Autilization.%20Second%2C%20it%20proposes%20a%20unified%20benchmarking%20framework%20integrating%0Amodern%20compression%20protocols%20and%20standard%20test%20sequences%20for%20fair%0Amulti-criteria%20evaluation.%20Third%2C%20it%20provides%20a%20systematic%20analysis%20of%20the%0Acritical%20trade-offs%20between%20reconstruction%20performance%20and%20computational%0Acomplexity%20observed%20in%20state-of-the-art%20methods%20and%20highlighting%20promising%0Adirections%20for%20future%20research.%20This%20comprehensive%20review%20aims%20to%20establish%20a%0Afoundation%20for%20consistent%20assessment%20and%20informed%20model%20selection%20in%20CVQE%0Aresearch%20and%20deployment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.10407v2&entry.124074799=Read"},
{"title": "Layout-Aware OCR for Black Digital Archives with Unsupervised Evaluation", "author": "Fitsum Sileshi Beyene and Christopher L. Dancy", "abstract": "  Despite their cultural and historical significance, Black digital archives\ncontinue to be a structurally underrepresented area in AI research and\ninfrastructure. This is especially evident in efforts to digitize historical\nBlack newspapers, where inconsistent typography, visual degradation, and\nlimited annotated layout data hinder accurate transcription, despite the\navailability of various systems that claim to handle optical character\nrecognition (OCR) well. In this short paper, we present a layout-aware OCR\npipeline tailored for Black newspaper archives and introduce an unsupervised\nevaluation framework suited to low-resource archival contexts. Our approach\nintegrates synthetic layout generation, model pretraining on augmented data,\nand a fusion of state-of-the-art You Only Look Once (YOLO) detectors. We used\nthree annotation-free evaluation metrics, the Semantic Coherence Score (SCS),\nRegion Entropy (RE), and Textual Redundancy Score (TRS), which quantify\nlinguistic fluency, informational diversity, and redundancy across OCR regions.\nOur evaluation on a 400-page dataset from ten Black newspaper titles\ndemonstrates that layout-aware OCR improves structural diversity and reduces\nredundancy compared to full-page baselines, with modest trade-offs in\ncoherence. Our results highlight the importance of respecting cultural layout\nlogic in AI-driven document understanding and lay the foundation for future\ncommunity-driven and ethically grounded archival AI systems.\n", "link": "http://arxiv.org/abs/2509.13236v1", "date": "2025-09-16", "relevancy": 2.6977, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6379}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4916}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.489}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Layout-Aware%20OCR%20for%20Black%20Digital%20Archives%20with%20Unsupervised%20Evaluation&body=Title%3A%20Layout-Aware%20OCR%20for%20Black%20Digital%20Archives%20with%20Unsupervised%20Evaluation%0AAuthor%3A%20Fitsum%20Sileshi%20Beyene%20and%20Christopher%20L.%20Dancy%0AAbstract%3A%20%20%20Despite%20their%20cultural%20and%20historical%20significance%2C%20Black%20digital%20archives%0Acontinue%20to%20be%20a%20structurally%20underrepresented%20area%20in%20AI%20research%20and%0Ainfrastructure.%20This%20is%20especially%20evident%20in%20efforts%20to%20digitize%20historical%0ABlack%20newspapers%2C%20where%20inconsistent%20typography%2C%20visual%20degradation%2C%20and%0Alimited%20annotated%20layout%20data%20hinder%20accurate%20transcription%2C%20despite%20the%0Aavailability%20of%20various%20systems%20that%20claim%20to%20handle%20optical%20character%0Arecognition%20%28OCR%29%20well.%20In%20this%20short%20paper%2C%20we%20present%20a%20layout-aware%20OCR%0Apipeline%20tailored%20for%20Black%20newspaper%20archives%20and%20introduce%20an%20unsupervised%0Aevaluation%20framework%20suited%20to%20low-resource%20archival%20contexts.%20Our%20approach%0Aintegrates%20synthetic%20layout%20generation%2C%20model%20pretraining%20on%20augmented%20data%2C%0Aand%20a%20fusion%20of%20state-of-the-art%20You%20Only%20Look%20Once%20%28YOLO%29%20detectors.%20We%20used%0Athree%20annotation-free%20evaluation%20metrics%2C%20the%20Semantic%20Coherence%20Score%20%28SCS%29%2C%0ARegion%20Entropy%20%28RE%29%2C%20and%20Textual%20Redundancy%20Score%20%28TRS%29%2C%20which%20quantify%0Alinguistic%20fluency%2C%20informational%20diversity%2C%20and%20redundancy%20across%20OCR%20regions.%0AOur%20evaluation%20on%20a%20400-page%20dataset%20from%20ten%20Black%20newspaper%20titles%0Ademonstrates%20that%20layout-aware%20OCR%20improves%20structural%20diversity%20and%20reduces%0Aredundancy%20compared%20to%20full-page%20baselines%2C%20with%20modest%20trade-offs%20in%0Acoherence.%20Our%20results%20highlight%20the%20importance%20of%20respecting%20cultural%20layout%0Alogic%20in%20AI-driven%20document%20understanding%20and%20lay%20the%20foundation%20for%20future%0Acommunity-driven%20and%20ethically%20grounded%20archival%20AI%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13236v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLayout-Aware%2520OCR%2520for%2520Black%2520Digital%2520Archives%2520with%2520Unsupervised%2520Evaluation%26entry.906535625%3DFitsum%2520Sileshi%2520Beyene%2520and%2520Christopher%2520L.%2520Dancy%26entry.1292438233%3D%2520%2520Despite%2520their%2520cultural%2520and%2520historical%2520significance%252C%2520Black%2520digital%2520archives%250Acontinue%2520to%2520be%2520a%2520structurally%2520underrepresented%2520area%2520in%2520AI%2520research%2520and%250Ainfrastructure.%2520This%2520is%2520especially%2520evident%2520in%2520efforts%2520to%2520digitize%2520historical%250ABlack%2520newspapers%252C%2520where%2520inconsistent%2520typography%252C%2520visual%2520degradation%252C%2520and%250Alimited%2520annotated%2520layout%2520data%2520hinder%2520accurate%2520transcription%252C%2520despite%2520the%250Aavailability%2520of%2520various%2520systems%2520that%2520claim%2520to%2520handle%2520optical%2520character%250Arecognition%2520%2528OCR%2529%2520well.%2520In%2520this%2520short%2520paper%252C%2520we%2520present%2520a%2520layout-aware%2520OCR%250Apipeline%2520tailored%2520for%2520Black%2520newspaper%2520archives%2520and%2520introduce%2520an%2520unsupervised%250Aevaluation%2520framework%2520suited%2520to%2520low-resource%2520archival%2520contexts.%2520Our%2520approach%250Aintegrates%2520synthetic%2520layout%2520generation%252C%2520model%2520pretraining%2520on%2520augmented%2520data%252C%250Aand%2520a%2520fusion%2520of%2520state-of-the-art%2520You%2520Only%2520Look%2520Once%2520%2528YOLO%2529%2520detectors.%2520We%2520used%250Athree%2520annotation-free%2520evaluation%2520metrics%252C%2520the%2520Semantic%2520Coherence%2520Score%2520%2528SCS%2529%252C%250ARegion%2520Entropy%2520%2528RE%2529%252C%2520and%2520Textual%2520Redundancy%2520Score%2520%2528TRS%2529%252C%2520which%2520quantify%250Alinguistic%2520fluency%252C%2520informational%2520diversity%252C%2520and%2520redundancy%2520across%2520OCR%2520regions.%250AOur%2520evaluation%2520on%2520a%2520400-page%2520dataset%2520from%2520ten%2520Black%2520newspaper%2520titles%250Ademonstrates%2520that%2520layout-aware%2520OCR%2520improves%2520structural%2520diversity%2520and%2520reduces%250Aredundancy%2520compared%2520to%2520full-page%2520baselines%252C%2520with%2520modest%2520trade-offs%2520in%250Acoherence.%2520Our%2520results%2520highlight%2520the%2520importance%2520of%2520respecting%2520cultural%2520layout%250Alogic%2520in%2520AI-driven%2520document%2520understanding%2520and%2520lay%2520the%2520foundation%2520for%2520future%250Acommunity-driven%2520and%2520ethically%2520grounded%2520archival%2520AI%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13236v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Layout-Aware%20OCR%20for%20Black%20Digital%20Archives%20with%20Unsupervised%20Evaluation&entry.906535625=Fitsum%20Sileshi%20Beyene%20and%20Christopher%20L.%20Dancy&entry.1292438233=%20%20Despite%20their%20cultural%20and%20historical%20significance%2C%20Black%20digital%20archives%0Acontinue%20to%20be%20a%20structurally%20underrepresented%20area%20in%20AI%20research%20and%0Ainfrastructure.%20This%20is%20especially%20evident%20in%20efforts%20to%20digitize%20historical%0ABlack%20newspapers%2C%20where%20inconsistent%20typography%2C%20visual%20degradation%2C%20and%0Alimited%20annotated%20layout%20data%20hinder%20accurate%20transcription%2C%20despite%20the%0Aavailability%20of%20various%20systems%20that%20claim%20to%20handle%20optical%20character%0Arecognition%20%28OCR%29%20well.%20In%20this%20short%20paper%2C%20we%20present%20a%20layout-aware%20OCR%0Apipeline%20tailored%20for%20Black%20newspaper%20archives%20and%20introduce%20an%20unsupervised%0Aevaluation%20framework%20suited%20to%20low-resource%20archival%20contexts.%20Our%20approach%0Aintegrates%20synthetic%20layout%20generation%2C%20model%20pretraining%20on%20augmented%20data%2C%0Aand%20a%20fusion%20of%20state-of-the-art%20You%20Only%20Look%20Once%20%28YOLO%29%20detectors.%20We%20used%0Athree%20annotation-free%20evaluation%20metrics%2C%20the%20Semantic%20Coherence%20Score%20%28SCS%29%2C%0ARegion%20Entropy%20%28RE%29%2C%20and%20Textual%20Redundancy%20Score%20%28TRS%29%2C%20which%20quantify%0Alinguistic%20fluency%2C%20informational%20diversity%2C%20and%20redundancy%20across%20OCR%20regions.%0AOur%20evaluation%20on%20a%20400-page%20dataset%20from%20ten%20Black%20newspaper%20titles%0Ademonstrates%20that%20layout-aware%20OCR%20improves%20structural%20diversity%20and%20reduces%0Aredundancy%20compared%20to%20full-page%20baselines%2C%20with%20modest%20trade-offs%20in%0Acoherence.%20Our%20results%20highlight%20the%20importance%20of%20respecting%20cultural%20layout%0Alogic%20in%20AI-driven%20document%20understanding%20and%20lay%20the%20foundation%20for%20future%0Acommunity-driven%20and%20ethically%20grounded%20archival%20AI%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13236v1&entry.124074799=Read"},
{"title": "Optimal Transport Based Unsupervised Restoration Learning Exploiting\n  Degradation Sparsity", "author": "Fei Wen and Wei Wang and Zeyu Yan and Wenbin Jiang", "abstract": "  Optimal transport (OT) has recently been shown as a promising criterion for\nunsupervised restoration when no explicit prior model is available. Despite its\ntheoretical appeal, OT still significantly falls short of supervised methods on\nchallenging tasks such as super-resolution, deraining, and dehazing. In this\npaper, we propose a \\emph{sparsity-aware optimal transport} (SOT) framework to\nbridge this gap by leveraging a key observation: the degradations in these\ntasks exhibit distinct sparsity in the frequency domain. Incorporating this\nsparsity prior into OT can significantly reduce the ambiguity of the inverse\nmapping for restoration and substantially boost performance. We provide\nanalysis to show exploiting degradation sparsity benefits unsupervised\nrestoration learning. Extensive experiments on real-world super-resolution,\nderaining, and dehazing demonstrate that SOT offers notable performance gains\nover standard OT, while achieving superior perceptual quality compared to\nexisting supervised and unsupervised methods. In particular, SOT consistently\noutperforms existing unsupervised methods across all three tasks and narrows\nthe performance gap to supervised counterparts.\n", "link": "http://arxiv.org/abs/2305.00273v2", "date": "2025-09-16", "relevancy": 2.6797, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5517}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5434}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5128}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Transport%20Based%20Unsupervised%20Restoration%20Learning%20Exploiting%0A%20%20Degradation%20Sparsity&body=Title%3A%20Optimal%20Transport%20Based%20Unsupervised%20Restoration%20Learning%20Exploiting%0A%20%20Degradation%20Sparsity%0AAuthor%3A%20Fei%20Wen%20and%20Wei%20Wang%20and%20Zeyu%20Yan%20and%20Wenbin%20Jiang%0AAbstract%3A%20%20%20Optimal%20transport%20%28OT%29%20has%20recently%20been%20shown%20as%20a%20promising%20criterion%20for%0Aunsupervised%20restoration%20when%20no%20explicit%20prior%20model%20is%20available.%20Despite%20its%0Atheoretical%20appeal%2C%20OT%20still%20significantly%20falls%20short%20of%20supervised%20methods%20on%0Achallenging%20tasks%20such%20as%20super-resolution%2C%20deraining%2C%20and%20dehazing.%20In%20this%0Apaper%2C%20we%20propose%20a%20%5Cemph%7Bsparsity-aware%20optimal%20transport%7D%20%28SOT%29%20framework%20to%0Abridge%20this%20gap%20by%20leveraging%20a%20key%20observation%3A%20the%20degradations%20in%20these%0Atasks%20exhibit%20distinct%20sparsity%20in%20the%20frequency%20domain.%20Incorporating%20this%0Asparsity%20prior%20into%20OT%20can%20significantly%20reduce%20the%20ambiguity%20of%20the%20inverse%0Amapping%20for%20restoration%20and%20substantially%20boost%20performance.%20We%20provide%0Aanalysis%20to%20show%20exploiting%20degradation%20sparsity%20benefits%20unsupervised%0Arestoration%20learning.%20Extensive%20experiments%20on%20real-world%20super-resolution%2C%0Aderaining%2C%20and%20dehazing%20demonstrate%20that%20SOT%20offers%20notable%20performance%20gains%0Aover%20standard%20OT%2C%20while%20achieving%20superior%20perceptual%20quality%20compared%20to%0Aexisting%20supervised%20and%20unsupervised%20methods.%20In%20particular%2C%20SOT%20consistently%0Aoutperforms%20existing%20unsupervised%20methods%20across%20all%20three%20tasks%20and%20narrows%0Athe%20performance%20gap%20to%20supervised%20counterparts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.00273v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Transport%2520Based%2520Unsupervised%2520Restoration%2520Learning%2520Exploiting%250A%2520%2520Degradation%2520Sparsity%26entry.906535625%3DFei%2520Wen%2520and%2520Wei%2520Wang%2520and%2520Zeyu%2520Yan%2520and%2520Wenbin%2520Jiang%26entry.1292438233%3D%2520%2520Optimal%2520transport%2520%2528OT%2529%2520has%2520recently%2520been%2520shown%2520as%2520a%2520promising%2520criterion%2520for%250Aunsupervised%2520restoration%2520when%2520no%2520explicit%2520prior%2520model%2520is%2520available.%2520Despite%2520its%250Atheoretical%2520appeal%252C%2520OT%2520still%2520significantly%2520falls%2520short%2520of%2520supervised%2520methods%2520on%250Achallenging%2520tasks%2520such%2520as%2520super-resolution%252C%2520deraining%252C%2520and%2520dehazing.%2520In%2520this%250Apaper%252C%2520we%2520propose%2520a%2520%255Cemph%257Bsparsity-aware%2520optimal%2520transport%257D%2520%2528SOT%2529%2520framework%2520to%250Abridge%2520this%2520gap%2520by%2520leveraging%2520a%2520key%2520observation%253A%2520the%2520degradations%2520in%2520these%250Atasks%2520exhibit%2520distinct%2520sparsity%2520in%2520the%2520frequency%2520domain.%2520Incorporating%2520this%250Asparsity%2520prior%2520into%2520OT%2520can%2520significantly%2520reduce%2520the%2520ambiguity%2520of%2520the%2520inverse%250Amapping%2520for%2520restoration%2520and%2520substantially%2520boost%2520performance.%2520We%2520provide%250Aanalysis%2520to%2520show%2520exploiting%2520degradation%2520sparsity%2520benefits%2520unsupervised%250Arestoration%2520learning.%2520Extensive%2520experiments%2520on%2520real-world%2520super-resolution%252C%250Aderaining%252C%2520and%2520dehazing%2520demonstrate%2520that%2520SOT%2520offers%2520notable%2520performance%2520gains%250Aover%2520standard%2520OT%252C%2520while%2520achieving%2520superior%2520perceptual%2520quality%2520compared%2520to%250Aexisting%2520supervised%2520and%2520unsupervised%2520methods.%2520In%2520particular%252C%2520SOT%2520consistently%250Aoutperforms%2520existing%2520unsupervised%2520methods%2520across%2520all%2520three%2520tasks%2520and%2520narrows%250Athe%2520performance%2520gap%2520to%2520supervised%2520counterparts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.00273v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Transport%20Based%20Unsupervised%20Restoration%20Learning%20Exploiting%0A%20%20Degradation%20Sparsity&entry.906535625=Fei%20Wen%20and%20Wei%20Wang%20and%20Zeyu%20Yan%20and%20Wenbin%20Jiang&entry.1292438233=%20%20Optimal%20transport%20%28OT%29%20has%20recently%20been%20shown%20as%20a%20promising%20criterion%20for%0Aunsupervised%20restoration%20when%20no%20explicit%20prior%20model%20is%20available.%20Despite%20its%0Atheoretical%20appeal%2C%20OT%20still%20significantly%20falls%20short%20of%20supervised%20methods%20on%0Achallenging%20tasks%20such%20as%20super-resolution%2C%20deraining%2C%20and%20dehazing.%20In%20this%0Apaper%2C%20we%20propose%20a%20%5Cemph%7Bsparsity-aware%20optimal%20transport%7D%20%28SOT%29%20framework%20to%0Abridge%20this%20gap%20by%20leveraging%20a%20key%20observation%3A%20the%20degradations%20in%20these%0Atasks%20exhibit%20distinct%20sparsity%20in%20the%20frequency%20domain.%20Incorporating%20this%0Asparsity%20prior%20into%20OT%20can%20significantly%20reduce%20the%20ambiguity%20of%20the%20inverse%0Amapping%20for%20restoration%20and%20substantially%20boost%20performance.%20We%20provide%0Aanalysis%20to%20show%20exploiting%20degradation%20sparsity%20benefits%20unsupervised%0Arestoration%20learning.%20Extensive%20experiments%20on%20real-world%20super-resolution%2C%0Aderaining%2C%20and%20dehazing%20demonstrate%20that%20SOT%20offers%20notable%20performance%20gains%0Aover%20standard%20OT%2C%20while%20achieving%20superior%20perceptual%20quality%20compared%20to%0Aexisting%20supervised%20and%20unsupervised%20methods.%20In%20particular%2C%20SOT%20consistently%0Aoutperforms%20existing%20unsupervised%20methods%20across%20all%20three%20tasks%20and%20narrows%0Athe%20performance%20gap%20to%20supervised%20counterparts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.00273v2&entry.124074799=Read"},
{"title": "Advancing Real-World Parking Slot Detection with Large-Scale Dataset and\n  Semi-Supervised Baseline", "author": "Zhihao Zhang and Chunyu Lin and Lang Nie and Jiyuan Wang and Yao Zhao", "abstract": "  As automatic parking systems evolve, the accurate detection of parking slots\nhas become increasingly critical. This study focuses on parking slot detection\nusing surround-view cameras, which offer a comprehensive bird's-eye view of the\nparking environment. However, the current datasets are limited in scale, and\nthe scenes they contain are seldom disrupted by real-world noise (e.g., light,\nocclusion, etc.). Moreover, manual data annotation is prone to errors and\nomissions due to the complexity of real-world conditions, significantly\nincreasing the cost of annotating large-scale datasets. To address these\nissues, we first construct a large-scale parking slot detection dataset (named\nCRPS-D), which includes various lighting distributions, diverse weather\nconditions, and challenging parking slot variants. Compared with existing\ndatasets, the proposed dataset boasts the largest data scale and consists of a\nhigher density of parking slots, particularly featuring more slanted parking\nslots. Additionally, we develop a semi-supervised baseline for parking slot\ndetection, termed SS-PSD, to further improve performance by exploiting\nunlabeled data. To our knowledge, this is the first semi-supervised approach in\nparking slot detection, which is built on the teacher-student model with\nconfidence-guided mask consistency and adaptive feature perturbation.\nExperimental results demonstrate the superiority of SS-PSD over the existing\nstate-of-the-art (SoTA) solutions on both the proposed dataset and the existing\ndataset. Particularly, the more unlabeled data there is, the more significant\nthe gains brought by our semi-supervised scheme. The relevant source codes and\nthe dataset have been made publicly available at\nhttps://github.com/zzh362/CRPS-D.\n", "link": "http://arxiv.org/abs/2509.13133v1", "date": "2025-09-16", "relevancy": 2.6666, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5389}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.531}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Advancing%20Real-World%20Parking%20Slot%20Detection%20with%20Large-Scale%20Dataset%20and%0A%20%20Semi-Supervised%20Baseline&body=Title%3A%20Advancing%20Real-World%20Parking%20Slot%20Detection%20with%20Large-Scale%20Dataset%20and%0A%20%20Semi-Supervised%20Baseline%0AAuthor%3A%20Zhihao%20Zhang%20and%20Chunyu%20Lin%20and%20Lang%20Nie%20and%20Jiyuan%20Wang%20and%20Yao%20Zhao%0AAbstract%3A%20%20%20As%20automatic%20parking%20systems%20evolve%2C%20the%20accurate%20detection%20of%20parking%20slots%0Ahas%20become%20increasingly%20critical.%20This%20study%20focuses%20on%20parking%20slot%20detection%0Ausing%20surround-view%20cameras%2C%20which%20offer%20a%20comprehensive%20bird%27s-eye%20view%20of%20the%0Aparking%20environment.%20However%2C%20the%20current%20datasets%20are%20limited%20in%20scale%2C%20and%0Athe%20scenes%20they%20contain%20are%20seldom%20disrupted%20by%20real-world%20noise%20%28e.g.%2C%20light%2C%0Aocclusion%2C%20etc.%29.%20Moreover%2C%20manual%20data%20annotation%20is%20prone%20to%20errors%20and%0Aomissions%20due%20to%20the%20complexity%20of%20real-world%20conditions%2C%20significantly%0Aincreasing%20the%20cost%20of%20annotating%20large-scale%20datasets.%20To%20address%20these%0Aissues%2C%20we%20first%20construct%20a%20large-scale%20parking%20slot%20detection%20dataset%20%28named%0ACRPS-D%29%2C%20which%20includes%20various%20lighting%20distributions%2C%20diverse%20weather%0Aconditions%2C%20and%20challenging%20parking%20slot%20variants.%20Compared%20with%20existing%0Adatasets%2C%20the%20proposed%20dataset%20boasts%20the%20largest%20data%20scale%20and%20consists%20of%20a%0Ahigher%20density%20of%20parking%20slots%2C%20particularly%20featuring%20more%20slanted%20parking%0Aslots.%20Additionally%2C%20we%20develop%20a%20semi-supervised%20baseline%20for%20parking%20slot%0Adetection%2C%20termed%20SS-PSD%2C%20to%20further%20improve%20performance%20by%20exploiting%0Aunlabeled%20data.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20semi-supervised%20approach%20in%0Aparking%20slot%20detection%2C%20which%20is%20built%20on%20the%20teacher-student%20model%20with%0Aconfidence-guided%20mask%20consistency%20and%20adaptive%20feature%20perturbation.%0AExperimental%20results%20demonstrate%20the%20superiority%20of%20SS-PSD%20over%20the%20existing%0Astate-of-the-art%20%28SoTA%29%20solutions%20on%20both%20the%20proposed%20dataset%20and%20the%20existing%0Adataset.%20Particularly%2C%20the%20more%20unlabeled%20data%20there%20is%2C%20the%20more%20significant%0Athe%20gains%20brought%20by%20our%20semi-supervised%20scheme.%20The%20relevant%20source%20codes%20and%0Athe%20dataset%20have%20been%20made%20publicly%20available%20at%0Ahttps%3A//github.com/zzh362/CRPS-D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13133v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdvancing%2520Real-World%2520Parking%2520Slot%2520Detection%2520with%2520Large-Scale%2520Dataset%2520and%250A%2520%2520Semi-Supervised%2520Baseline%26entry.906535625%3DZhihao%2520Zhang%2520and%2520Chunyu%2520Lin%2520and%2520Lang%2520Nie%2520and%2520Jiyuan%2520Wang%2520and%2520Yao%2520Zhao%26entry.1292438233%3D%2520%2520As%2520automatic%2520parking%2520systems%2520evolve%252C%2520the%2520accurate%2520detection%2520of%2520parking%2520slots%250Ahas%2520become%2520increasingly%2520critical.%2520This%2520study%2520focuses%2520on%2520parking%2520slot%2520detection%250Ausing%2520surround-view%2520cameras%252C%2520which%2520offer%2520a%2520comprehensive%2520bird%2527s-eye%2520view%2520of%2520the%250Aparking%2520environment.%2520However%252C%2520the%2520current%2520datasets%2520are%2520limited%2520in%2520scale%252C%2520and%250Athe%2520scenes%2520they%2520contain%2520are%2520seldom%2520disrupted%2520by%2520real-world%2520noise%2520%2528e.g.%252C%2520light%252C%250Aocclusion%252C%2520etc.%2529.%2520Moreover%252C%2520manual%2520data%2520annotation%2520is%2520prone%2520to%2520errors%2520and%250Aomissions%2520due%2520to%2520the%2520complexity%2520of%2520real-world%2520conditions%252C%2520significantly%250Aincreasing%2520the%2520cost%2520of%2520annotating%2520large-scale%2520datasets.%2520To%2520address%2520these%250Aissues%252C%2520we%2520first%2520construct%2520a%2520large-scale%2520parking%2520slot%2520detection%2520dataset%2520%2528named%250ACRPS-D%2529%252C%2520which%2520includes%2520various%2520lighting%2520distributions%252C%2520diverse%2520weather%250Aconditions%252C%2520and%2520challenging%2520parking%2520slot%2520variants.%2520Compared%2520with%2520existing%250Adatasets%252C%2520the%2520proposed%2520dataset%2520boasts%2520the%2520largest%2520data%2520scale%2520and%2520consists%2520of%2520a%250Ahigher%2520density%2520of%2520parking%2520slots%252C%2520particularly%2520featuring%2520more%2520slanted%2520parking%250Aslots.%2520Additionally%252C%2520we%2520develop%2520a%2520semi-supervised%2520baseline%2520for%2520parking%2520slot%250Adetection%252C%2520termed%2520SS-PSD%252C%2520to%2520further%2520improve%2520performance%2520by%2520exploiting%250Aunlabeled%2520data.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520semi-supervised%2520approach%2520in%250Aparking%2520slot%2520detection%252C%2520which%2520is%2520built%2520on%2520the%2520teacher-student%2520model%2520with%250Aconfidence-guided%2520mask%2520consistency%2520and%2520adaptive%2520feature%2520perturbation.%250AExperimental%2520results%2520demonstrate%2520the%2520superiority%2520of%2520SS-PSD%2520over%2520the%2520existing%250Astate-of-the-art%2520%2528SoTA%2529%2520solutions%2520on%2520both%2520the%2520proposed%2520dataset%2520and%2520the%2520existing%250Adataset.%2520Particularly%252C%2520the%2520more%2520unlabeled%2520data%2520there%2520is%252C%2520the%2520more%2520significant%250Athe%2520gains%2520brought%2520by%2520our%2520semi-supervised%2520scheme.%2520The%2520relevant%2520source%2520codes%2520and%250Athe%2520dataset%2520have%2520been%2520made%2520publicly%2520available%2520at%250Ahttps%253A//github.com/zzh362/CRPS-D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13133v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20Real-World%20Parking%20Slot%20Detection%20with%20Large-Scale%20Dataset%20and%0A%20%20Semi-Supervised%20Baseline&entry.906535625=Zhihao%20Zhang%20and%20Chunyu%20Lin%20and%20Lang%20Nie%20and%20Jiyuan%20Wang%20and%20Yao%20Zhao&entry.1292438233=%20%20As%20automatic%20parking%20systems%20evolve%2C%20the%20accurate%20detection%20of%20parking%20slots%0Ahas%20become%20increasingly%20critical.%20This%20study%20focuses%20on%20parking%20slot%20detection%0Ausing%20surround-view%20cameras%2C%20which%20offer%20a%20comprehensive%20bird%27s-eye%20view%20of%20the%0Aparking%20environment.%20However%2C%20the%20current%20datasets%20are%20limited%20in%20scale%2C%20and%0Athe%20scenes%20they%20contain%20are%20seldom%20disrupted%20by%20real-world%20noise%20%28e.g.%2C%20light%2C%0Aocclusion%2C%20etc.%29.%20Moreover%2C%20manual%20data%20annotation%20is%20prone%20to%20errors%20and%0Aomissions%20due%20to%20the%20complexity%20of%20real-world%20conditions%2C%20significantly%0Aincreasing%20the%20cost%20of%20annotating%20large-scale%20datasets.%20To%20address%20these%0Aissues%2C%20we%20first%20construct%20a%20large-scale%20parking%20slot%20detection%20dataset%20%28named%0ACRPS-D%29%2C%20which%20includes%20various%20lighting%20distributions%2C%20diverse%20weather%0Aconditions%2C%20and%20challenging%20parking%20slot%20variants.%20Compared%20with%20existing%0Adatasets%2C%20the%20proposed%20dataset%20boasts%20the%20largest%20data%20scale%20and%20consists%20of%20a%0Ahigher%20density%20of%20parking%20slots%2C%20particularly%20featuring%20more%20slanted%20parking%0Aslots.%20Additionally%2C%20we%20develop%20a%20semi-supervised%20baseline%20for%20parking%20slot%0Adetection%2C%20termed%20SS-PSD%2C%20to%20further%20improve%20performance%20by%20exploiting%0Aunlabeled%20data.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20semi-supervised%20approach%20in%0Aparking%20slot%20detection%2C%20which%20is%20built%20on%20the%20teacher-student%20model%20with%0Aconfidence-guided%20mask%20consistency%20and%20adaptive%20feature%20perturbation.%0AExperimental%20results%20demonstrate%20the%20superiority%20of%20SS-PSD%20over%20the%20existing%0Astate-of-the-art%20%28SoTA%29%20solutions%20on%20both%20the%20proposed%20dataset%20and%20the%20existing%0Adataset.%20Particularly%2C%20the%20more%20unlabeled%20data%20there%20is%2C%20the%20more%20significant%0Athe%20gains%20brought%20by%20our%20semi-supervised%20scheme.%20The%20relevant%20source%20codes%20and%0Athe%20dataset%20have%20been%20made%20publicly%20available%20at%0Ahttps%3A//github.com/zzh362/CRPS-D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13133v1&entry.124074799=Read"},
{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "author": "Manuel-Andreas Schneider and Lukas H\u00f6llein and Matthias Nie\u00dfner", "abstract": "  Generating 3D worlds from text is a highly anticipated goal in computer\nvision. Existing works are limited by the degree of exploration they allow\ninside of a scene, i.e., produce streched-out and noisy artifacts when moving\nbeyond central or panoramic perspectives. To this end, we propose\nWorldExplorer, a novel method based on autoregressive video trajectory\ngeneration, which builds fully navigable 3D scenes with consistent visual\nquality across a wide range of viewpoints. We initialize our scenes by creating\nmulti-view consistent images corresponding to a 360 degree panorama. Then, we\nexpand it by leveraging video diffusion models in an iterative scene generation\npipeline. Concretely, we generate multiple videos along short, pre-defined\ntrajectories, that explore the scene in depth, including motion around objects.\nOur novel scene memory conditions each video on the most relevant prior views,\nwhile a collision-detection mechanism prevents degenerate results, like moving\ninto objects. Finally, we fuse all generated views into a unified 3D\nrepresentation via 3D Gaussian Splatting optimization. Compared to prior\napproaches, WorldExplorer produces high-quality scenes that remain stable under\nlarge camera motion, enabling for the first time realistic and unrestricted\nexploration. We believe this marks a significant step toward generating\nimmersive and truly explorable virtual 3D environments.\n", "link": "http://arxiv.org/abs/2506.01799v2", "date": "2025-09-16", "relevancy": 2.6596, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6845}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.661}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WorldExplorer%3A%20Towards%20Generating%20Fully%20Navigable%203D%20Scenes&body=Title%3A%20WorldExplorer%3A%20Towards%20Generating%20Fully%20Navigable%203D%20Scenes%0AAuthor%3A%20Manuel-Andreas%20Schneider%20and%20Lukas%20H%C3%B6llein%20and%20Matthias%20Nie%C3%9Fner%0AAbstract%3A%20%20%20Generating%203D%20worlds%20from%20text%20is%20a%20highly%20anticipated%20goal%20in%20computer%0Avision.%20Existing%20works%20are%20limited%20by%20the%20degree%20of%20exploration%20they%20allow%0Ainside%20of%20a%20scene%2C%20i.e.%2C%20produce%20streched-out%20and%20noisy%20artifacts%20when%20moving%0Abeyond%20central%20or%20panoramic%20perspectives.%20To%20this%20end%2C%20we%20propose%0AWorldExplorer%2C%20a%20novel%20method%20based%20on%20autoregressive%20video%20trajectory%0Ageneration%2C%20which%20builds%20fully%20navigable%203D%20scenes%20with%20consistent%20visual%0Aquality%20across%20a%20wide%20range%20of%20viewpoints.%20We%20initialize%20our%20scenes%20by%20creating%0Amulti-view%20consistent%20images%20corresponding%20to%20a%20360%20degree%20panorama.%20Then%2C%20we%0Aexpand%20it%20by%20leveraging%20video%20diffusion%20models%20in%20an%20iterative%20scene%20generation%0Apipeline.%20Concretely%2C%20we%20generate%20multiple%20videos%20along%20short%2C%20pre-defined%0Atrajectories%2C%20that%20explore%20the%20scene%20in%20depth%2C%20including%20motion%20around%20objects.%0AOur%20novel%20scene%20memory%20conditions%20each%20video%20on%20the%20most%20relevant%20prior%20views%2C%0Awhile%20a%20collision-detection%20mechanism%20prevents%20degenerate%20results%2C%20like%20moving%0Ainto%20objects.%20Finally%2C%20we%20fuse%20all%20generated%20views%20into%20a%20unified%203D%0Arepresentation%20via%203D%20Gaussian%20Splatting%20optimization.%20Compared%20to%20prior%0Aapproaches%2C%20WorldExplorer%20produces%20high-quality%20scenes%20that%20remain%20stable%20under%0Alarge%20camera%20motion%2C%20enabling%20for%20the%20first%20time%20realistic%20and%20unrestricted%0Aexploration.%20We%20believe%20this%20marks%20a%20significant%20step%20toward%20generating%0Aimmersive%20and%20truly%20explorable%20virtual%203D%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.01799v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorldExplorer%253A%2520Towards%2520Generating%2520Fully%2520Navigable%25203D%2520Scenes%26entry.906535625%3DManuel-Andreas%2520Schneider%2520and%2520Lukas%2520H%25C3%25B6llein%2520and%2520Matthias%2520Nie%25C3%259Fner%26entry.1292438233%3D%2520%2520Generating%25203D%2520worlds%2520from%2520text%2520is%2520a%2520highly%2520anticipated%2520goal%2520in%2520computer%250Avision.%2520Existing%2520works%2520are%2520limited%2520by%2520the%2520degree%2520of%2520exploration%2520they%2520allow%250Ainside%2520of%2520a%2520scene%252C%2520i.e.%252C%2520produce%2520streched-out%2520and%2520noisy%2520artifacts%2520when%2520moving%250Abeyond%2520central%2520or%2520panoramic%2520perspectives.%2520To%2520this%2520end%252C%2520we%2520propose%250AWorldExplorer%252C%2520a%2520novel%2520method%2520based%2520on%2520autoregressive%2520video%2520trajectory%250Ageneration%252C%2520which%2520builds%2520fully%2520navigable%25203D%2520scenes%2520with%2520consistent%2520visual%250Aquality%2520across%2520a%2520wide%2520range%2520of%2520viewpoints.%2520We%2520initialize%2520our%2520scenes%2520by%2520creating%250Amulti-view%2520consistent%2520images%2520corresponding%2520to%2520a%2520360%2520degree%2520panorama.%2520Then%252C%2520we%250Aexpand%2520it%2520by%2520leveraging%2520video%2520diffusion%2520models%2520in%2520an%2520iterative%2520scene%2520generation%250Apipeline.%2520Concretely%252C%2520we%2520generate%2520multiple%2520videos%2520along%2520short%252C%2520pre-defined%250Atrajectories%252C%2520that%2520explore%2520the%2520scene%2520in%2520depth%252C%2520including%2520motion%2520around%2520objects.%250AOur%2520novel%2520scene%2520memory%2520conditions%2520each%2520video%2520on%2520the%2520most%2520relevant%2520prior%2520views%252C%250Awhile%2520a%2520collision-detection%2520mechanism%2520prevents%2520degenerate%2520results%252C%2520like%2520moving%250Ainto%2520objects.%2520Finally%252C%2520we%2520fuse%2520all%2520generated%2520views%2520into%2520a%2520unified%25203D%250Arepresentation%2520via%25203D%2520Gaussian%2520Splatting%2520optimization.%2520Compared%2520to%2520prior%250Aapproaches%252C%2520WorldExplorer%2520produces%2520high-quality%2520scenes%2520that%2520remain%2520stable%2520under%250Alarge%2520camera%2520motion%252C%2520enabling%2520for%2520the%2520first%2520time%2520realistic%2520and%2520unrestricted%250Aexploration.%2520We%2520believe%2520this%2520marks%2520a%2520significant%2520step%2520toward%2520generating%250Aimmersive%2520and%2520truly%2520explorable%2520virtual%25203D%2520environments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.01799v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WorldExplorer%3A%20Towards%20Generating%20Fully%20Navigable%203D%20Scenes&entry.906535625=Manuel-Andreas%20Schneider%20and%20Lukas%20H%C3%B6llein%20and%20Matthias%20Nie%C3%9Fner&entry.1292438233=%20%20Generating%203D%20worlds%20from%20text%20is%20a%20highly%20anticipated%20goal%20in%20computer%0Avision.%20Existing%20works%20are%20limited%20by%20the%20degree%20of%20exploration%20they%20allow%0Ainside%20of%20a%20scene%2C%20i.e.%2C%20produce%20streched-out%20and%20noisy%20artifacts%20when%20moving%0Abeyond%20central%20or%20panoramic%20perspectives.%20To%20this%20end%2C%20we%20propose%0AWorldExplorer%2C%20a%20novel%20method%20based%20on%20autoregressive%20video%20trajectory%0Ageneration%2C%20which%20builds%20fully%20navigable%203D%20scenes%20with%20consistent%20visual%0Aquality%20across%20a%20wide%20range%20of%20viewpoints.%20We%20initialize%20our%20scenes%20by%20creating%0Amulti-view%20consistent%20images%20corresponding%20to%20a%20360%20degree%20panorama.%20Then%2C%20we%0Aexpand%20it%20by%20leveraging%20video%20diffusion%20models%20in%20an%20iterative%20scene%20generation%0Apipeline.%20Concretely%2C%20we%20generate%20multiple%20videos%20along%20short%2C%20pre-defined%0Atrajectories%2C%20that%20explore%20the%20scene%20in%20depth%2C%20including%20motion%20around%20objects.%0AOur%20novel%20scene%20memory%20conditions%20each%20video%20on%20the%20most%20relevant%20prior%20views%2C%0Awhile%20a%20collision-detection%20mechanism%20prevents%20degenerate%20results%2C%20like%20moving%0Ainto%20objects.%20Finally%2C%20we%20fuse%20all%20generated%20views%20into%20a%20unified%203D%0Arepresentation%20via%203D%20Gaussian%20Splatting%20optimization.%20Compared%20to%20prior%0Aapproaches%2C%20WorldExplorer%20produces%20high-quality%20scenes%20that%20remain%20stable%20under%0Alarge%20camera%20motion%2C%20enabling%20for%20the%20first%20time%20realistic%20and%20unrestricted%0Aexploration.%20We%20believe%20this%20marks%20a%20significant%20step%20toward%20generating%0Aimmersive%20and%20truly%20explorable%20virtual%203D%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.01799v2&entry.124074799=Read"},
{"title": "Vi-SAFE: A Spatial-Temporal Framework for Efficient Violence Detection\n  in Public Surveillance", "author": "Ligang Chang and Shengkai Xu and Liangchang Shen and Binhan Xu and Junqiao Wang and Tianyu Shi and Yanhui Du", "abstract": "  Violence detection in public surveillance is critical for public safety. This\nstudy addresses challenges such as small-scale targets, complex environments,\nand real-time temporal analysis. We propose Vi-SAFE, a spatial-temporal\nframework that integrates an enhanced YOLOv8 with a Temporal Segment Network\n(TSN) for video surveillance. The YOLOv8 model is optimized with GhostNetV3 as\na lightweight backbone, an exponential moving average (EMA) attention\nmechanism, and pruning to reduce computational cost while maintaining accuracy.\nYOLOv8 and TSN are trained separately on pedestrian and violence datasets,\nwhere YOLOv8 extracts human regions and TSN performs binary classification of\nviolent behavior. Experiments on the RWF-2000 dataset show that Vi-SAFE\nachieves an accuracy of 0.88, surpassing TSN alone (0.77) and outperforming\nexisting methods in both accuracy and efficiency, demonstrating its\neffectiveness for public safety surveillance. Code is available at\nhttps://anonymous.4open.science/r/Vi-SAFE-3B42/README.md.\n", "link": "http://arxiv.org/abs/2509.13210v1", "date": "2025-09-16", "relevancy": 2.6083, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.55}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.513}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vi-SAFE%3A%20A%20Spatial-Temporal%20Framework%20for%20Efficient%20Violence%20Detection%0A%20%20in%20Public%20Surveillance&body=Title%3A%20Vi-SAFE%3A%20A%20Spatial-Temporal%20Framework%20for%20Efficient%20Violence%20Detection%0A%20%20in%20Public%20Surveillance%0AAuthor%3A%20Ligang%20Chang%20and%20Shengkai%20Xu%20and%20Liangchang%20Shen%20and%20Binhan%20Xu%20and%20Junqiao%20Wang%20and%20Tianyu%20Shi%20and%20Yanhui%20Du%0AAbstract%3A%20%20%20Violence%20detection%20in%20public%20surveillance%20is%20critical%20for%20public%20safety.%20This%0Astudy%20addresses%20challenges%20such%20as%20small-scale%20targets%2C%20complex%20environments%2C%0Aand%20real-time%20temporal%20analysis.%20We%20propose%20Vi-SAFE%2C%20a%20spatial-temporal%0Aframework%20that%20integrates%20an%20enhanced%20YOLOv8%20with%20a%20Temporal%20Segment%20Network%0A%28TSN%29%20for%20video%20surveillance.%20The%20YOLOv8%20model%20is%20optimized%20with%20GhostNetV3%20as%0Aa%20lightweight%20backbone%2C%20an%20exponential%20moving%20average%20%28EMA%29%20attention%0Amechanism%2C%20and%20pruning%20to%20reduce%20computational%20cost%20while%20maintaining%20accuracy.%0AYOLOv8%20and%20TSN%20are%20trained%20separately%20on%20pedestrian%20and%20violence%20datasets%2C%0Awhere%20YOLOv8%20extracts%20human%20regions%20and%20TSN%20performs%20binary%20classification%20of%0Aviolent%20behavior.%20Experiments%20on%20the%20RWF-2000%20dataset%20show%20that%20Vi-SAFE%0Aachieves%20an%20accuracy%20of%200.88%2C%20surpassing%20TSN%20alone%20%280.77%29%20and%20outperforming%0Aexisting%20methods%20in%20both%20accuracy%20and%20efficiency%2C%20demonstrating%20its%0Aeffectiveness%20for%20public%20safety%20surveillance.%20Code%20is%20available%20at%0Ahttps%3A//anonymous.4open.science/r/Vi-SAFE-3B42/README.md.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13210v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVi-SAFE%253A%2520A%2520Spatial-Temporal%2520Framework%2520for%2520Efficient%2520Violence%2520Detection%250A%2520%2520in%2520Public%2520Surveillance%26entry.906535625%3DLigang%2520Chang%2520and%2520Shengkai%2520Xu%2520and%2520Liangchang%2520Shen%2520and%2520Binhan%2520Xu%2520and%2520Junqiao%2520Wang%2520and%2520Tianyu%2520Shi%2520and%2520Yanhui%2520Du%26entry.1292438233%3D%2520%2520Violence%2520detection%2520in%2520public%2520surveillance%2520is%2520critical%2520for%2520public%2520safety.%2520This%250Astudy%2520addresses%2520challenges%2520such%2520as%2520small-scale%2520targets%252C%2520complex%2520environments%252C%250Aand%2520real-time%2520temporal%2520analysis.%2520We%2520propose%2520Vi-SAFE%252C%2520a%2520spatial-temporal%250Aframework%2520that%2520integrates%2520an%2520enhanced%2520YOLOv8%2520with%2520a%2520Temporal%2520Segment%2520Network%250A%2528TSN%2529%2520for%2520video%2520surveillance.%2520The%2520YOLOv8%2520model%2520is%2520optimized%2520with%2520GhostNetV3%2520as%250Aa%2520lightweight%2520backbone%252C%2520an%2520exponential%2520moving%2520average%2520%2528EMA%2529%2520attention%250Amechanism%252C%2520and%2520pruning%2520to%2520reduce%2520computational%2520cost%2520while%2520maintaining%2520accuracy.%250AYOLOv8%2520and%2520TSN%2520are%2520trained%2520separately%2520on%2520pedestrian%2520and%2520violence%2520datasets%252C%250Awhere%2520YOLOv8%2520extracts%2520human%2520regions%2520and%2520TSN%2520performs%2520binary%2520classification%2520of%250Aviolent%2520behavior.%2520Experiments%2520on%2520the%2520RWF-2000%2520dataset%2520show%2520that%2520Vi-SAFE%250Aachieves%2520an%2520accuracy%2520of%25200.88%252C%2520surpassing%2520TSN%2520alone%2520%25280.77%2529%2520and%2520outperforming%250Aexisting%2520methods%2520in%2520both%2520accuracy%2520and%2520efficiency%252C%2520demonstrating%2520its%250Aeffectiveness%2520for%2520public%2520safety%2520surveillance.%2520Code%2520is%2520available%2520at%250Ahttps%253A//anonymous.4open.science/r/Vi-SAFE-3B42/README.md.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13210v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vi-SAFE%3A%20A%20Spatial-Temporal%20Framework%20for%20Efficient%20Violence%20Detection%0A%20%20in%20Public%20Surveillance&entry.906535625=Ligang%20Chang%20and%20Shengkai%20Xu%20and%20Liangchang%20Shen%20and%20Binhan%20Xu%20and%20Junqiao%20Wang%20and%20Tianyu%20Shi%20and%20Yanhui%20Du&entry.1292438233=%20%20Violence%20detection%20in%20public%20surveillance%20is%20critical%20for%20public%20safety.%20This%0Astudy%20addresses%20challenges%20such%20as%20small-scale%20targets%2C%20complex%20environments%2C%0Aand%20real-time%20temporal%20analysis.%20We%20propose%20Vi-SAFE%2C%20a%20spatial-temporal%0Aframework%20that%20integrates%20an%20enhanced%20YOLOv8%20with%20a%20Temporal%20Segment%20Network%0A%28TSN%29%20for%20video%20surveillance.%20The%20YOLOv8%20model%20is%20optimized%20with%20GhostNetV3%20as%0Aa%20lightweight%20backbone%2C%20an%20exponential%20moving%20average%20%28EMA%29%20attention%0Amechanism%2C%20and%20pruning%20to%20reduce%20computational%20cost%20while%20maintaining%20accuracy.%0AYOLOv8%20and%20TSN%20are%20trained%20separately%20on%20pedestrian%20and%20violence%20datasets%2C%0Awhere%20YOLOv8%20extracts%20human%20regions%20and%20TSN%20performs%20binary%20classification%20of%0Aviolent%20behavior.%20Experiments%20on%20the%20RWF-2000%20dataset%20show%20that%20Vi-SAFE%0Aachieves%20an%20accuracy%20of%200.88%2C%20surpassing%20TSN%20alone%20%280.77%29%20and%20outperforming%0Aexisting%20methods%20in%20both%20accuracy%20and%20efficiency%2C%20demonstrating%20its%0Aeffectiveness%20for%20public%20safety%20surveillance.%20Code%20is%20available%20at%0Ahttps%3A//anonymous.4open.science/r/Vi-SAFE-3B42/README.md.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13210v1&entry.124074799=Read"},
{"title": "SuPreME: A Supervised Pre-training Framework for Multimodal ECG\n  Representation Learning", "author": "Mingsheng Cai and Jiuming Jiang and Wenhao Huang and Che Liu and Rossella Arcucci", "abstract": "  Cardiovascular diseases are a leading cause of death and disability\nworldwide. Electrocardiogram (ECG) is critical for diagnosing and monitoring\ncardiac health, but obtaining large-scale annotated ECG datasets is\nlabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)\nmethods mitigate this by learning features without extensive labels but fail to\ncapture fine-grained clinical semantics and require extensive task-specific\nfine-tuning. To address these challenges, we propose $\\textbf{SuPreME}$, a\n$\\textbf{Su}$pervised $\\textbf{Pre}$-training framework for\n$\\textbf{M}$ultimodal $\\textbf{E}$CG representation learning. SuPreME is\npre-trained using structured diagnostic labels derived from ECG report entities\nthrough a one-time offline extraction with Large Language Models (LLMs), which\nhelp denoise, standardize cardiac concepts, and improve clinical representation\nlearning. By fusing ECG signals with textual cardiac queries instead of fixed\nlabels, SuPreME enables zero-shot classification of unseen conditions without\nfurther fine-tuning. We evaluate SuPreME on six downstream datasets covering\n106 cardiac conditions, achieving superior zero-shot AUC performance of\n$77.20\\%$, surpassing state-of-the-art eSSLs by $4.98\\%$. Results demonstrate\nSuPreME's effectiveness in leveraging structured, clinically relevant knowledge\nfor high-quality ECG representations.\n", "link": "http://arxiv.org/abs/2502.19668v3", "date": "2025-09-16", "relevancy": 2.6078, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5408}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5153}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5085}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuPreME%3A%20A%20Supervised%20Pre-training%20Framework%20for%20Multimodal%20ECG%0A%20%20Representation%20Learning&body=Title%3A%20SuPreME%3A%20A%20Supervised%20Pre-training%20Framework%20for%20Multimodal%20ECG%0A%20%20Representation%20Learning%0AAuthor%3A%20Mingsheng%20Cai%20and%20Jiuming%20Jiang%20and%20Wenhao%20Huang%20and%20Che%20Liu%20and%20Rossella%20Arcucci%0AAbstract%3A%20%20%20Cardiovascular%20diseases%20are%20a%20leading%20cause%20of%20death%20and%20disability%0Aworldwide.%20Electrocardiogram%20%28ECG%29%20is%20critical%20for%20diagnosing%20and%20monitoring%0Acardiac%20health%2C%20but%20obtaining%20large-scale%20annotated%20ECG%20datasets%20is%0Alabor-intensive%20and%20time-consuming.%20Recent%20ECG%20Self-Supervised%20Learning%20%28eSSL%29%0Amethods%20mitigate%20this%20by%20learning%20features%20without%20extensive%20labels%20but%20fail%20to%0Acapture%20fine-grained%20clinical%20semantics%20and%20require%20extensive%20task-specific%0Afine-tuning.%20To%20address%20these%20challenges%2C%20we%20propose%20%24%5Ctextbf%7BSuPreME%7D%24%2C%20a%0A%24%5Ctextbf%7BSu%7D%24pervised%20%24%5Ctextbf%7BPre%7D%24-training%20framework%20for%0A%24%5Ctextbf%7BM%7D%24ultimodal%20%24%5Ctextbf%7BE%7D%24CG%20representation%20learning.%20SuPreME%20is%0Apre-trained%20using%20structured%20diagnostic%20labels%20derived%20from%20ECG%20report%20entities%0Athrough%20a%20one-time%20offline%20extraction%20with%20Large%20Language%20Models%20%28LLMs%29%2C%20which%0Ahelp%20denoise%2C%20standardize%20cardiac%20concepts%2C%20and%20improve%20clinical%20representation%0Alearning.%20By%20fusing%20ECG%20signals%20with%20textual%20cardiac%20queries%20instead%20of%20fixed%0Alabels%2C%20SuPreME%20enables%20zero-shot%20classification%20of%20unseen%20conditions%20without%0Afurther%20fine-tuning.%20We%20evaluate%20SuPreME%20on%20six%20downstream%20datasets%20covering%0A106%20cardiac%20conditions%2C%20achieving%20superior%20zero-shot%20AUC%20performance%20of%0A%2477.20%5C%25%24%2C%20surpassing%20state-of-the-art%20eSSLs%20by%20%244.98%5C%25%24.%20Results%20demonstrate%0ASuPreME%27s%20effectiveness%20in%20leveraging%20structured%2C%20clinically%20relevant%20knowledge%0Afor%20high-quality%20ECG%20representations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.19668v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuPreME%253A%2520A%2520Supervised%2520Pre-training%2520Framework%2520for%2520Multimodal%2520ECG%250A%2520%2520Representation%2520Learning%26entry.906535625%3DMingsheng%2520Cai%2520and%2520Jiuming%2520Jiang%2520and%2520Wenhao%2520Huang%2520and%2520Che%2520Liu%2520and%2520Rossella%2520Arcucci%26entry.1292438233%3D%2520%2520Cardiovascular%2520diseases%2520are%2520a%2520leading%2520cause%2520of%2520death%2520and%2520disability%250Aworldwide.%2520Electrocardiogram%2520%2528ECG%2529%2520is%2520critical%2520for%2520diagnosing%2520and%2520monitoring%250Acardiac%2520health%252C%2520but%2520obtaining%2520large-scale%2520annotated%2520ECG%2520datasets%2520is%250Alabor-intensive%2520and%2520time-consuming.%2520Recent%2520ECG%2520Self-Supervised%2520Learning%2520%2528eSSL%2529%250Amethods%2520mitigate%2520this%2520by%2520learning%2520features%2520without%2520extensive%2520labels%2520but%2520fail%2520to%250Acapture%2520fine-grained%2520clinical%2520semantics%2520and%2520require%2520extensive%2520task-specific%250Afine-tuning.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520%2524%255Ctextbf%257BSuPreME%257D%2524%252C%2520a%250A%2524%255Ctextbf%257BSu%257D%2524pervised%2520%2524%255Ctextbf%257BPre%257D%2524-training%2520framework%2520for%250A%2524%255Ctextbf%257BM%257D%2524ultimodal%2520%2524%255Ctextbf%257BE%257D%2524CG%2520representation%2520learning.%2520SuPreME%2520is%250Apre-trained%2520using%2520structured%2520diagnostic%2520labels%2520derived%2520from%2520ECG%2520report%2520entities%250Athrough%2520a%2520one-time%2520offline%2520extraction%2520with%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520which%250Ahelp%2520denoise%252C%2520standardize%2520cardiac%2520concepts%252C%2520and%2520improve%2520clinical%2520representation%250Alearning.%2520By%2520fusing%2520ECG%2520signals%2520with%2520textual%2520cardiac%2520queries%2520instead%2520of%2520fixed%250Alabels%252C%2520SuPreME%2520enables%2520zero-shot%2520classification%2520of%2520unseen%2520conditions%2520without%250Afurther%2520fine-tuning.%2520We%2520evaluate%2520SuPreME%2520on%2520six%2520downstream%2520datasets%2520covering%250A106%2520cardiac%2520conditions%252C%2520achieving%2520superior%2520zero-shot%2520AUC%2520performance%2520of%250A%252477.20%255C%2525%2524%252C%2520surpassing%2520state-of-the-art%2520eSSLs%2520by%2520%25244.98%255C%2525%2524.%2520Results%2520demonstrate%250ASuPreME%2527s%2520effectiveness%2520in%2520leveraging%2520structured%252C%2520clinically%2520relevant%2520knowledge%250Afor%2520high-quality%2520ECG%2520representations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.19668v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuPreME%3A%20A%20Supervised%20Pre-training%20Framework%20for%20Multimodal%20ECG%0A%20%20Representation%20Learning&entry.906535625=Mingsheng%20Cai%20and%20Jiuming%20Jiang%20and%20Wenhao%20Huang%20and%20Che%20Liu%20and%20Rossella%20Arcucci&entry.1292438233=%20%20Cardiovascular%20diseases%20are%20a%20leading%20cause%20of%20death%20and%20disability%0Aworldwide.%20Electrocardiogram%20%28ECG%29%20is%20critical%20for%20diagnosing%20and%20monitoring%0Acardiac%20health%2C%20but%20obtaining%20large-scale%20annotated%20ECG%20datasets%20is%0Alabor-intensive%20and%20time-consuming.%20Recent%20ECG%20Self-Supervised%20Learning%20%28eSSL%29%0Amethods%20mitigate%20this%20by%20learning%20features%20without%20extensive%20labels%20but%20fail%20to%0Acapture%20fine-grained%20clinical%20semantics%20and%20require%20extensive%20task-specific%0Afine-tuning.%20To%20address%20these%20challenges%2C%20we%20propose%20%24%5Ctextbf%7BSuPreME%7D%24%2C%20a%0A%24%5Ctextbf%7BSu%7D%24pervised%20%24%5Ctextbf%7BPre%7D%24-training%20framework%20for%0A%24%5Ctextbf%7BM%7D%24ultimodal%20%24%5Ctextbf%7BE%7D%24CG%20representation%20learning.%20SuPreME%20is%0Apre-trained%20using%20structured%20diagnostic%20labels%20derived%20from%20ECG%20report%20entities%0Athrough%20a%20one-time%20offline%20extraction%20with%20Large%20Language%20Models%20%28LLMs%29%2C%20which%0Ahelp%20denoise%2C%20standardize%20cardiac%20concepts%2C%20and%20improve%20clinical%20representation%0Alearning.%20By%20fusing%20ECG%20signals%20with%20textual%20cardiac%20queries%20instead%20of%20fixed%0Alabels%2C%20SuPreME%20enables%20zero-shot%20classification%20of%20unseen%20conditions%20without%0Afurther%20fine-tuning.%20We%20evaluate%20SuPreME%20on%20six%20downstream%20datasets%20covering%0A106%20cardiac%20conditions%2C%20achieving%20superior%20zero-shot%20AUC%20performance%20of%0A%2477.20%5C%25%24%2C%20surpassing%20state-of-the-art%20eSSLs%20by%20%244.98%5C%25%24.%20Results%20demonstrate%0ASuPreME%27s%20effectiveness%20in%20leveraging%20structured%2C%20clinically%20relevant%20knowledge%0Afor%20high-quality%20ECG%20representations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.19668v3&entry.124074799=Read"},
{"title": "Zero-shot Hierarchical Plant Segmentation via Foundation Segmentation\n  Models and Text-to-image Attention", "author": "Junhao Xing and Ryohei Miyakawa and Yang Yang and Xinpeng Liu and Risa Shinoda and Hiroaki Santo and Yosuke Toda and Fumio Okura", "abstract": "  Foundation segmentation models achieve reasonable leaf instance extraction\nfrom top-view crop images without training (i.e., zero-shot). However,\nsegmenting entire plant individuals with each consisting of multiple\noverlapping leaves remains challenging. This problem is referred to as a\nhierarchical segmentation task, typically requiring annotated training\ndatasets, which are often species-specific and require notable human labor. To\naddress this, we introduce ZeroPlantSeg, a zero-shot segmentation for\nrosette-shaped plant individuals from top-view images. We integrate a\nfoundation segmentation model, extracting leaf instances, and a vision-language\nmodel, reasoning about plants' structures to extract plant individuals without\nadditional training. Evaluations on datasets with multiple plant species,\ngrowth stages, and shooting environments demonstrate that our method surpasses\nexisting zero-shot methods and achieves better cross-domain performance than\nsupervised methods. Implementations are available at\nhttps://github.com/JunhaoXing/ZeroPlantSeg.\n", "link": "http://arxiv.org/abs/2509.09116v2", "date": "2025-09-16", "relevancy": 2.6027, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5238}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5238}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zero-shot%20Hierarchical%20Plant%20Segmentation%20via%20Foundation%20Segmentation%0A%20%20Models%20and%20Text-to-image%20Attention&body=Title%3A%20Zero-shot%20Hierarchical%20Plant%20Segmentation%20via%20Foundation%20Segmentation%0A%20%20Models%20and%20Text-to-image%20Attention%0AAuthor%3A%20Junhao%20Xing%20and%20Ryohei%20Miyakawa%20and%20Yang%20Yang%20and%20Xinpeng%20Liu%20and%20Risa%20Shinoda%20and%20Hiroaki%20Santo%20and%20Yosuke%20Toda%20and%20Fumio%20Okura%0AAbstract%3A%20%20%20Foundation%20segmentation%20models%20achieve%20reasonable%20leaf%20instance%20extraction%0Afrom%20top-view%20crop%20images%20without%20training%20%28i.e.%2C%20zero-shot%29.%20However%2C%0Asegmenting%20entire%20plant%20individuals%20with%20each%20consisting%20of%20multiple%0Aoverlapping%20leaves%20remains%20challenging.%20This%20problem%20is%20referred%20to%20as%20a%0Ahierarchical%20segmentation%20task%2C%20typically%20requiring%20annotated%20training%0Adatasets%2C%20which%20are%20often%20species-specific%20and%20require%20notable%20human%20labor.%20To%0Aaddress%20this%2C%20we%20introduce%20ZeroPlantSeg%2C%20a%20zero-shot%20segmentation%20for%0Arosette-shaped%20plant%20individuals%20from%20top-view%20images.%20We%20integrate%20a%0Afoundation%20segmentation%20model%2C%20extracting%20leaf%20instances%2C%20and%20a%20vision-language%0Amodel%2C%20reasoning%20about%20plants%27%20structures%20to%20extract%20plant%20individuals%20without%0Aadditional%20training.%20Evaluations%20on%20datasets%20with%20multiple%20plant%20species%2C%0Agrowth%20stages%2C%20and%20shooting%20environments%20demonstrate%20that%20our%20method%20surpasses%0Aexisting%20zero-shot%20methods%20and%20achieves%20better%20cross-domain%20performance%20than%0Asupervised%20methods.%20Implementations%20are%20available%20at%0Ahttps%3A//github.com/JunhaoXing/ZeroPlantSeg.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.09116v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZero-shot%2520Hierarchical%2520Plant%2520Segmentation%2520via%2520Foundation%2520Segmentation%250A%2520%2520Models%2520and%2520Text-to-image%2520Attention%26entry.906535625%3DJunhao%2520Xing%2520and%2520Ryohei%2520Miyakawa%2520and%2520Yang%2520Yang%2520and%2520Xinpeng%2520Liu%2520and%2520Risa%2520Shinoda%2520and%2520Hiroaki%2520Santo%2520and%2520Yosuke%2520Toda%2520and%2520Fumio%2520Okura%26entry.1292438233%3D%2520%2520Foundation%2520segmentation%2520models%2520achieve%2520reasonable%2520leaf%2520instance%2520extraction%250Afrom%2520top-view%2520crop%2520images%2520without%2520training%2520%2528i.e.%252C%2520zero-shot%2529.%2520However%252C%250Asegmenting%2520entire%2520plant%2520individuals%2520with%2520each%2520consisting%2520of%2520multiple%250Aoverlapping%2520leaves%2520remains%2520challenging.%2520This%2520problem%2520is%2520referred%2520to%2520as%2520a%250Ahierarchical%2520segmentation%2520task%252C%2520typically%2520requiring%2520annotated%2520training%250Adatasets%252C%2520which%2520are%2520often%2520species-specific%2520and%2520require%2520notable%2520human%2520labor.%2520To%250Aaddress%2520this%252C%2520we%2520introduce%2520ZeroPlantSeg%252C%2520a%2520zero-shot%2520segmentation%2520for%250Arosette-shaped%2520plant%2520individuals%2520from%2520top-view%2520images.%2520We%2520integrate%2520a%250Afoundation%2520segmentation%2520model%252C%2520extracting%2520leaf%2520instances%252C%2520and%2520a%2520vision-language%250Amodel%252C%2520reasoning%2520about%2520plants%2527%2520structures%2520to%2520extract%2520plant%2520individuals%2520without%250Aadditional%2520training.%2520Evaluations%2520on%2520datasets%2520with%2520multiple%2520plant%2520species%252C%250Agrowth%2520stages%252C%2520and%2520shooting%2520environments%2520demonstrate%2520that%2520our%2520method%2520surpasses%250Aexisting%2520zero-shot%2520methods%2520and%2520achieves%2520better%2520cross-domain%2520performance%2520than%250Asupervised%2520methods.%2520Implementations%2520are%2520available%2520at%250Ahttps%253A//github.com/JunhaoXing/ZeroPlantSeg.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.09116v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zero-shot%20Hierarchical%20Plant%20Segmentation%20via%20Foundation%20Segmentation%0A%20%20Models%20and%20Text-to-image%20Attention&entry.906535625=Junhao%20Xing%20and%20Ryohei%20Miyakawa%20and%20Yang%20Yang%20and%20Xinpeng%20Liu%20and%20Risa%20Shinoda%20and%20Hiroaki%20Santo%20and%20Yosuke%20Toda%20and%20Fumio%20Okura&entry.1292438233=%20%20Foundation%20segmentation%20models%20achieve%20reasonable%20leaf%20instance%20extraction%0Afrom%20top-view%20crop%20images%20without%20training%20%28i.e.%2C%20zero-shot%29.%20However%2C%0Asegmenting%20entire%20plant%20individuals%20with%20each%20consisting%20of%20multiple%0Aoverlapping%20leaves%20remains%20challenging.%20This%20problem%20is%20referred%20to%20as%20a%0Ahierarchical%20segmentation%20task%2C%20typically%20requiring%20annotated%20training%0Adatasets%2C%20which%20are%20often%20species-specific%20and%20require%20notable%20human%20labor.%20To%0Aaddress%20this%2C%20we%20introduce%20ZeroPlantSeg%2C%20a%20zero-shot%20segmentation%20for%0Arosette-shaped%20plant%20individuals%20from%20top-view%20images.%20We%20integrate%20a%0Afoundation%20segmentation%20model%2C%20extracting%20leaf%20instances%2C%20and%20a%20vision-language%0Amodel%2C%20reasoning%20about%20plants%27%20structures%20to%20extract%20plant%20individuals%20without%0Aadditional%20training.%20Evaluations%20on%20datasets%20with%20multiple%20plant%20species%2C%0Agrowth%20stages%2C%20and%20shooting%20environments%20demonstrate%20that%20our%20method%20surpasses%0Aexisting%20zero-shot%20methods%20and%20achieves%20better%20cross-domain%20performance%20than%0Asupervised%20methods.%20Implementations%20are%20available%20at%0Ahttps%3A//github.com/JunhaoXing/ZeroPlantSeg.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.09116v2&entry.124074799=Read"},
{"title": "Ensemble Visualization With Variational Autoencoder", "author": "Cenyang Wu and Qinhan Yu and Liang Zhou", "abstract": "  We present a new method to visualize data ensembles by constructing\nstructured probabilistic representations in latent spaces, i.e.,\nlower-dimensional representations of spatial data features. Our approach\ntransforms the spatial features of an ensemble into a latent space through\nfeature space conversion and unsupervised learning using a variational\nautoencoder (VAE). The resulting latent spaces follow multivariate standard\nGaussian distributions, enabling analytical computation of confidence intervals\nand density estimation of the probabilistic distribution that generates the\ndata ensemble. Preliminary results on a weather forecasting ensemble\ndemonstrate the effectiveness and versatility of our method.\n", "link": "http://arxiv.org/abs/2509.13000v1", "date": "2025-09-16", "relevancy": 2.5961, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.549}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5067}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.502}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ensemble%20Visualization%20With%20Variational%20Autoencoder&body=Title%3A%20Ensemble%20Visualization%20With%20Variational%20Autoencoder%0AAuthor%3A%20Cenyang%20Wu%20and%20Qinhan%20Yu%20and%20Liang%20Zhou%0AAbstract%3A%20%20%20We%20present%20a%20new%20method%20to%20visualize%20data%20ensembles%20by%20constructing%0Astructured%20probabilistic%20representations%20in%20latent%20spaces%2C%20i.e.%2C%0Alower-dimensional%20representations%20of%20spatial%20data%20features.%20Our%20approach%0Atransforms%20the%20spatial%20features%20of%20an%20ensemble%20into%20a%20latent%20space%20through%0Afeature%20space%20conversion%20and%20unsupervised%20learning%20using%20a%20variational%0Aautoencoder%20%28VAE%29.%20The%20resulting%20latent%20spaces%20follow%20multivariate%20standard%0AGaussian%20distributions%2C%20enabling%20analytical%20computation%20of%20confidence%20intervals%0Aand%20density%20estimation%20of%20the%20probabilistic%20distribution%20that%20generates%20the%0Adata%20ensemble.%20Preliminary%20results%20on%20a%20weather%20forecasting%20ensemble%0Ademonstrate%20the%20effectiveness%20and%20versatility%20of%20our%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnsemble%2520Visualization%2520With%2520Variational%2520Autoencoder%26entry.906535625%3DCenyang%2520Wu%2520and%2520Qinhan%2520Yu%2520and%2520Liang%2520Zhou%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520method%2520to%2520visualize%2520data%2520ensembles%2520by%2520constructing%250Astructured%2520probabilistic%2520representations%2520in%2520latent%2520spaces%252C%2520i.e.%252C%250Alower-dimensional%2520representations%2520of%2520spatial%2520data%2520features.%2520Our%2520approach%250Atransforms%2520the%2520spatial%2520features%2520of%2520an%2520ensemble%2520into%2520a%2520latent%2520space%2520through%250Afeature%2520space%2520conversion%2520and%2520unsupervised%2520learning%2520using%2520a%2520variational%250Aautoencoder%2520%2528VAE%2529.%2520The%2520resulting%2520latent%2520spaces%2520follow%2520multivariate%2520standard%250AGaussian%2520distributions%252C%2520enabling%2520analytical%2520computation%2520of%2520confidence%2520intervals%250Aand%2520density%2520estimation%2520of%2520the%2520probabilistic%2520distribution%2520that%2520generates%2520the%250Adata%2520ensemble.%2520Preliminary%2520results%2520on%2520a%2520weather%2520forecasting%2520ensemble%250Ademonstrate%2520the%2520effectiveness%2520and%2520versatility%2520of%2520our%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ensemble%20Visualization%20With%20Variational%20Autoencoder&entry.906535625=Cenyang%20Wu%20and%20Qinhan%20Yu%20and%20Liang%20Zhou&entry.1292438233=%20%20We%20present%20a%20new%20method%20to%20visualize%20data%20ensembles%20by%20constructing%0Astructured%20probabilistic%20representations%20in%20latent%20spaces%2C%20i.e.%2C%0Alower-dimensional%20representations%20of%20spatial%20data%20features.%20Our%20approach%0Atransforms%20the%20spatial%20features%20of%20an%20ensemble%20into%20a%20latent%20space%20through%0Afeature%20space%20conversion%20and%20unsupervised%20learning%20using%20a%20variational%0Aautoencoder%20%28VAE%29.%20The%20resulting%20latent%20spaces%20follow%20multivariate%20standard%0AGaussian%20distributions%2C%20enabling%20analytical%20computation%20of%20confidence%20intervals%0Aand%20density%20estimation%20of%20the%20probabilistic%20distribution%20that%20generates%20the%0Adata%20ensemble.%20Preliminary%20results%20on%20a%20weather%20forecasting%20ensemble%0Ademonstrate%20the%20effectiveness%20and%20versatility%20of%20our%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13000v1&entry.124074799=Read"},
{"title": "Population Estimation using Deep Learning over Gandhinagar Urban Area", "author": "Jai Singla and Peal Jotania and Keivalya Pandya", "abstract": "  Population estimation is crucial for various applications, from resource\nallocation to urban planning. Traditional methods such as surveys and censuses\nare expensive, time-consuming and also heavily dependent on human resources,\nrequiring significant manpower for data collection and processing. In this\nstudy a deep learning solution is proposed to estimate population using high\nresolution (0.3 m) satellite imagery, Digital Elevation Models (DEM) of 0.5m\nresolution and vector boundaries. Proposed method combines Convolution Neural\nNetwork (CNN) architecture for classification task to classify buildings as\nresidential and non-residential and Artificial Neural Network (ANN)\narchitecture to estimate the population. Approx. 48k building footprints over\nGandhinagar urban area are utilized containing both residential and\nnon-residential, with residential categories further used for building-level\npopulation estimation. Experimental results on a large-scale dataset\ndemonstrate the effectiveness of our model, achieving an impressive overall\nF1-score of 0.9936. The proposed system employs advanced geospatial analysis\nwith high spatial resolution to estimate Gandhinagar population at 278,954. By\nintegrating real-time data updates, standardized metrics, and infrastructure\nplanning capabilities, this automated approach addresses critical limitations\nof conventional census-based methodologies. The framework provides\nmunicipalities with a scalable and replicable tool for optimized resource\nmanagement in rapidly urbanizing cities, showcasing the efficiency of AI-driven\ngeospatial analytics in enhancing data-driven urban governance.\n", "link": "http://arxiv.org/abs/2509.12926v1", "date": "2025-09-16", "relevancy": 2.5723, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5186}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5179}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5068}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Population%20Estimation%20using%20Deep%20Learning%20over%20Gandhinagar%20Urban%20Area&body=Title%3A%20Population%20Estimation%20using%20Deep%20Learning%20over%20Gandhinagar%20Urban%20Area%0AAuthor%3A%20Jai%20Singla%20and%20Peal%20Jotania%20and%20Keivalya%20Pandya%0AAbstract%3A%20%20%20Population%20estimation%20is%20crucial%20for%20various%20applications%2C%20from%20resource%0Aallocation%20to%20urban%20planning.%20Traditional%20methods%20such%20as%20surveys%20and%20censuses%0Aare%20expensive%2C%20time-consuming%20and%20also%20heavily%20dependent%20on%20human%20resources%2C%0Arequiring%20significant%20manpower%20for%20data%20collection%20and%20processing.%20In%20this%0Astudy%20a%20deep%20learning%20solution%20is%20proposed%20to%20estimate%20population%20using%20high%0Aresolution%20%280.3%20m%29%20satellite%20imagery%2C%20Digital%20Elevation%20Models%20%28DEM%29%20of%200.5m%0Aresolution%20and%20vector%20boundaries.%20Proposed%20method%20combines%20Convolution%20Neural%0ANetwork%20%28CNN%29%20architecture%20for%20classification%20task%20to%20classify%20buildings%20as%0Aresidential%20and%20non-residential%20and%20Artificial%20Neural%20Network%20%28ANN%29%0Aarchitecture%20to%20estimate%20the%20population.%20Approx.%2048k%20building%20footprints%20over%0AGandhinagar%20urban%20area%20are%20utilized%20containing%20both%20residential%20and%0Anon-residential%2C%20with%20residential%20categories%20further%20used%20for%20building-level%0Apopulation%20estimation.%20Experimental%20results%20on%20a%20large-scale%20dataset%0Ademonstrate%20the%20effectiveness%20of%20our%20model%2C%20achieving%20an%20impressive%20overall%0AF1-score%20of%200.9936.%20The%20proposed%20system%20employs%20advanced%20geospatial%20analysis%0Awith%20high%20spatial%20resolution%20to%20estimate%20Gandhinagar%20population%20at%20278%2C954.%20By%0Aintegrating%20real-time%20data%20updates%2C%20standardized%20metrics%2C%20and%20infrastructure%0Aplanning%20capabilities%2C%20this%20automated%20approach%20addresses%20critical%20limitations%0Aof%20conventional%20census-based%20methodologies.%20The%20framework%20provides%0Amunicipalities%20with%20a%20scalable%20and%20replicable%20tool%20for%20optimized%20resource%0Amanagement%20in%20rapidly%20urbanizing%20cities%2C%20showcasing%20the%20efficiency%20of%20AI-driven%0Ageospatial%20analytics%20in%20enhancing%20data-driven%20urban%20governance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12926v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPopulation%2520Estimation%2520using%2520Deep%2520Learning%2520over%2520Gandhinagar%2520Urban%2520Area%26entry.906535625%3DJai%2520Singla%2520and%2520Peal%2520Jotania%2520and%2520Keivalya%2520Pandya%26entry.1292438233%3D%2520%2520Population%2520estimation%2520is%2520crucial%2520for%2520various%2520applications%252C%2520from%2520resource%250Aallocation%2520to%2520urban%2520planning.%2520Traditional%2520methods%2520such%2520as%2520surveys%2520and%2520censuses%250Aare%2520expensive%252C%2520time-consuming%2520and%2520also%2520heavily%2520dependent%2520on%2520human%2520resources%252C%250Arequiring%2520significant%2520manpower%2520for%2520data%2520collection%2520and%2520processing.%2520In%2520this%250Astudy%2520a%2520deep%2520learning%2520solution%2520is%2520proposed%2520to%2520estimate%2520population%2520using%2520high%250Aresolution%2520%25280.3%2520m%2529%2520satellite%2520imagery%252C%2520Digital%2520Elevation%2520Models%2520%2528DEM%2529%2520of%25200.5m%250Aresolution%2520and%2520vector%2520boundaries.%2520Proposed%2520method%2520combines%2520Convolution%2520Neural%250ANetwork%2520%2528CNN%2529%2520architecture%2520for%2520classification%2520task%2520to%2520classify%2520buildings%2520as%250Aresidential%2520and%2520non-residential%2520and%2520Artificial%2520Neural%2520Network%2520%2528ANN%2529%250Aarchitecture%2520to%2520estimate%2520the%2520population.%2520Approx.%252048k%2520building%2520footprints%2520over%250AGandhinagar%2520urban%2520area%2520are%2520utilized%2520containing%2520both%2520residential%2520and%250Anon-residential%252C%2520with%2520residential%2520categories%2520further%2520used%2520for%2520building-level%250Apopulation%2520estimation.%2520Experimental%2520results%2520on%2520a%2520large-scale%2520dataset%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520model%252C%2520achieving%2520an%2520impressive%2520overall%250AF1-score%2520of%25200.9936.%2520The%2520proposed%2520system%2520employs%2520advanced%2520geospatial%2520analysis%250Awith%2520high%2520spatial%2520resolution%2520to%2520estimate%2520Gandhinagar%2520population%2520at%2520278%252C954.%2520By%250Aintegrating%2520real-time%2520data%2520updates%252C%2520standardized%2520metrics%252C%2520and%2520infrastructure%250Aplanning%2520capabilities%252C%2520this%2520automated%2520approach%2520addresses%2520critical%2520limitations%250Aof%2520conventional%2520census-based%2520methodologies.%2520The%2520framework%2520provides%250Amunicipalities%2520with%2520a%2520scalable%2520and%2520replicable%2520tool%2520for%2520optimized%2520resource%250Amanagement%2520in%2520rapidly%2520urbanizing%2520cities%252C%2520showcasing%2520the%2520efficiency%2520of%2520AI-driven%250Ageospatial%2520analytics%2520in%2520enhancing%2520data-driven%2520urban%2520governance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12926v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Population%20Estimation%20using%20Deep%20Learning%20over%20Gandhinagar%20Urban%20Area&entry.906535625=Jai%20Singla%20and%20Peal%20Jotania%20and%20Keivalya%20Pandya&entry.1292438233=%20%20Population%20estimation%20is%20crucial%20for%20various%20applications%2C%20from%20resource%0Aallocation%20to%20urban%20planning.%20Traditional%20methods%20such%20as%20surveys%20and%20censuses%0Aare%20expensive%2C%20time-consuming%20and%20also%20heavily%20dependent%20on%20human%20resources%2C%0Arequiring%20significant%20manpower%20for%20data%20collection%20and%20processing.%20In%20this%0Astudy%20a%20deep%20learning%20solution%20is%20proposed%20to%20estimate%20population%20using%20high%0Aresolution%20%280.3%20m%29%20satellite%20imagery%2C%20Digital%20Elevation%20Models%20%28DEM%29%20of%200.5m%0Aresolution%20and%20vector%20boundaries.%20Proposed%20method%20combines%20Convolution%20Neural%0ANetwork%20%28CNN%29%20architecture%20for%20classification%20task%20to%20classify%20buildings%20as%0Aresidential%20and%20non-residential%20and%20Artificial%20Neural%20Network%20%28ANN%29%0Aarchitecture%20to%20estimate%20the%20population.%20Approx.%2048k%20building%20footprints%20over%0AGandhinagar%20urban%20area%20are%20utilized%20containing%20both%20residential%20and%0Anon-residential%2C%20with%20residential%20categories%20further%20used%20for%20building-level%0Apopulation%20estimation.%20Experimental%20results%20on%20a%20large-scale%20dataset%0Ademonstrate%20the%20effectiveness%20of%20our%20model%2C%20achieving%20an%20impressive%20overall%0AF1-score%20of%200.9936.%20The%20proposed%20system%20employs%20advanced%20geospatial%20analysis%0Awith%20high%20spatial%20resolution%20to%20estimate%20Gandhinagar%20population%20at%20278%2C954.%20By%0Aintegrating%20real-time%20data%20updates%2C%20standardized%20metrics%2C%20and%20infrastructure%0Aplanning%20capabilities%2C%20this%20automated%20approach%20addresses%20critical%20limitations%0Aof%20conventional%20census-based%20methodologies.%20The%20framework%20provides%0Amunicipalities%20with%20a%20scalable%20and%20replicable%20tool%20for%20optimized%20resource%0Amanagement%20in%20rapidly%20urbanizing%20cities%2C%20showcasing%20the%20efficiency%20of%20AI-driven%0Ageospatial%20analytics%20in%20enhancing%20data-driven%20urban%20governance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12926v1&entry.124074799=Read"},
{"title": "T2V-Turbo-v2: Enhancing Video Generation Model Post-Training through\n  Data, Reward, and Conditional Guidance Design", "author": "Jiachen Li and Qian Long and Jian Zheng and Xiaofeng Gao and Robinson Piramuthu and Wenhu Chen and William Yang Wang", "abstract": "  In this paper, we focus on enhancing a diffusion-based text-to-video (T2V)\nmodel during the post-training phase by distilling a highly capable consistency\nmodel from a pretrained T2V model. Our proposed method, T2V-Turbo-v2,\nintroduces a significant advancement by integrating various supervision\nsignals, including high-quality training data, reward model feedback, and\nconditional guidance, into the consistency distillation process. Through\ncomprehensive ablation studies, we highlight the crucial importance of\ntailoring datasets to specific learning objectives and the effectiveness of\nlearning from diverse reward models for enhancing both the visual quality and\ntext-video alignment. Additionally, we highlight the vast design space of\nconditional guidance strategies, which centers on designing an effective energy\nfunction to augment the teacher ODE solver. We demonstrate the potential of\nthis approach by extracting motion guidance from the training datasets and\nincorporating it into the ODE solver, showcasing its effectiveness in improving\nthe motion quality of the generated videos with the improved motion-related\nmetrics from VBench and T2V-CompBench. Empirically, our T2V-Turbo-v2\nestablishes a new state-of-the-art result on VBench, with a Total score of\n85.13, surpassing proprietary systems such as Gen-3 and Kling.\n", "link": "http://arxiv.org/abs/2410.05677v3", "date": "2025-09-16", "relevancy": 2.5717, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6801}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6377}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6333}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T2V-Turbo-v2%3A%20Enhancing%20Video%20Generation%20Model%20Post-Training%20through%0A%20%20Data%2C%20Reward%2C%20and%20Conditional%20Guidance%20Design&body=Title%3A%20T2V-Turbo-v2%3A%20Enhancing%20Video%20Generation%20Model%20Post-Training%20through%0A%20%20Data%2C%20Reward%2C%20and%20Conditional%20Guidance%20Design%0AAuthor%3A%20Jiachen%20Li%20and%20Qian%20Long%20and%20Jian%20Zheng%20and%20Xiaofeng%20Gao%20and%20Robinson%20Piramuthu%20and%20Wenhu%20Chen%20and%20William%20Yang%20Wang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20focus%20on%20enhancing%20a%20diffusion-based%20text-to-video%20%28T2V%29%0Amodel%20during%20the%20post-training%20phase%20by%20distilling%20a%20highly%20capable%20consistency%0Amodel%20from%20a%20pretrained%20T2V%20model.%20Our%20proposed%20method%2C%20T2V-Turbo-v2%2C%0Aintroduces%20a%20significant%20advancement%20by%20integrating%20various%20supervision%0Asignals%2C%20including%20high-quality%20training%20data%2C%20reward%20model%20feedback%2C%20and%0Aconditional%20guidance%2C%20into%20the%20consistency%20distillation%20process.%20Through%0Acomprehensive%20ablation%20studies%2C%20we%20highlight%20the%20crucial%20importance%20of%0Atailoring%20datasets%20to%20specific%20learning%20objectives%20and%20the%20effectiveness%20of%0Alearning%20from%20diverse%20reward%20models%20for%20enhancing%20both%20the%20visual%20quality%20and%0Atext-video%20alignment.%20Additionally%2C%20we%20highlight%20the%20vast%20design%20space%20of%0Aconditional%20guidance%20strategies%2C%20which%20centers%20on%20designing%20an%20effective%20energy%0Afunction%20to%20augment%20the%20teacher%20ODE%20solver.%20We%20demonstrate%20the%20potential%20of%0Athis%20approach%20by%20extracting%20motion%20guidance%20from%20the%20training%20datasets%20and%0Aincorporating%20it%20into%20the%20ODE%20solver%2C%20showcasing%20its%20effectiveness%20in%20improving%0Athe%20motion%20quality%20of%20the%20generated%20videos%20with%20the%20improved%20motion-related%0Ametrics%20from%20VBench%20and%20T2V-CompBench.%20Empirically%2C%20our%20T2V-Turbo-v2%0Aestablishes%20a%20new%20state-of-the-art%20result%20on%20VBench%2C%20with%20a%20Total%20score%20of%0A85.13%2C%20surpassing%20proprietary%20systems%20such%20as%20Gen-3%20and%20Kling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.05677v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT2V-Turbo-v2%253A%2520Enhancing%2520Video%2520Generation%2520Model%2520Post-Training%2520through%250A%2520%2520Data%252C%2520Reward%252C%2520and%2520Conditional%2520Guidance%2520Design%26entry.906535625%3DJiachen%2520Li%2520and%2520Qian%2520Long%2520and%2520Jian%2520Zheng%2520and%2520Xiaofeng%2520Gao%2520and%2520Robinson%2520Piramuthu%2520and%2520Wenhu%2520Chen%2520and%2520William%2520Yang%2520Wang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520enhancing%2520a%2520diffusion-based%2520text-to-video%2520%2528T2V%2529%250Amodel%2520during%2520the%2520post-training%2520phase%2520by%2520distilling%2520a%2520highly%2520capable%2520consistency%250Amodel%2520from%2520a%2520pretrained%2520T2V%2520model.%2520Our%2520proposed%2520method%252C%2520T2V-Turbo-v2%252C%250Aintroduces%2520a%2520significant%2520advancement%2520by%2520integrating%2520various%2520supervision%250Asignals%252C%2520including%2520high-quality%2520training%2520data%252C%2520reward%2520model%2520feedback%252C%2520and%250Aconditional%2520guidance%252C%2520into%2520the%2520consistency%2520distillation%2520process.%2520Through%250Acomprehensive%2520ablation%2520studies%252C%2520we%2520highlight%2520the%2520crucial%2520importance%2520of%250Atailoring%2520datasets%2520to%2520specific%2520learning%2520objectives%2520and%2520the%2520effectiveness%2520of%250Alearning%2520from%2520diverse%2520reward%2520models%2520for%2520enhancing%2520both%2520the%2520visual%2520quality%2520and%250Atext-video%2520alignment.%2520Additionally%252C%2520we%2520highlight%2520the%2520vast%2520design%2520space%2520of%250Aconditional%2520guidance%2520strategies%252C%2520which%2520centers%2520on%2520designing%2520an%2520effective%2520energy%250Afunction%2520to%2520augment%2520the%2520teacher%2520ODE%2520solver.%2520We%2520demonstrate%2520the%2520potential%2520of%250Athis%2520approach%2520by%2520extracting%2520motion%2520guidance%2520from%2520the%2520training%2520datasets%2520and%250Aincorporating%2520it%2520into%2520the%2520ODE%2520solver%252C%2520showcasing%2520its%2520effectiveness%2520in%2520improving%250Athe%2520motion%2520quality%2520of%2520the%2520generated%2520videos%2520with%2520the%2520improved%2520motion-related%250Ametrics%2520from%2520VBench%2520and%2520T2V-CompBench.%2520Empirically%252C%2520our%2520T2V-Turbo-v2%250Aestablishes%2520a%2520new%2520state-of-the-art%2520result%2520on%2520VBench%252C%2520with%2520a%2520Total%2520score%2520of%250A85.13%252C%2520surpassing%2520proprietary%2520systems%2520such%2520as%2520Gen-3%2520and%2520Kling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.05677v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T2V-Turbo-v2%3A%20Enhancing%20Video%20Generation%20Model%20Post-Training%20through%0A%20%20Data%2C%20Reward%2C%20and%20Conditional%20Guidance%20Design&entry.906535625=Jiachen%20Li%20and%20Qian%20Long%20and%20Jian%20Zheng%20and%20Xiaofeng%20Gao%20and%20Robinson%20Piramuthu%20and%20Wenhu%20Chen%20and%20William%20Yang%20Wang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20focus%20on%20enhancing%20a%20diffusion-based%20text-to-video%20%28T2V%29%0Amodel%20during%20the%20post-training%20phase%20by%20distilling%20a%20highly%20capable%20consistency%0Amodel%20from%20a%20pretrained%20T2V%20model.%20Our%20proposed%20method%2C%20T2V-Turbo-v2%2C%0Aintroduces%20a%20significant%20advancement%20by%20integrating%20various%20supervision%0Asignals%2C%20including%20high-quality%20training%20data%2C%20reward%20model%20feedback%2C%20and%0Aconditional%20guidance%2C%20into%20the%20consistency%20distillation%20process.%20Through%0Acomprehensive%20ablation%20studies%2C%20we%20highlight%20the%20crucial%20importance%20of%0Atailoring%20datasets%20to%20specific%20learning%20objectives%20and%20the%20effectiveness%20of%0Alearning%20from%20diverse%20reward%20models%20for%20enhancing%20both%20the%20visual%20quality%20and%0Atext-video%20alignment.%20Additionally%2C%20we%20highlight%20the%20vast%20design%20space%20of%0Aconditional%20guidance%20strategies%2C%20which%20centers%20on%20designing%20an%20effective%20energy%0Afunction%20to%20augment%20the%20teacher%20ODE%20solver.%20We%20demonstrate%20the%20potential%20of%0Athis%20approach%20by%20extracting%20motion%20guidance%20from%20the%20training%20datasets%20and%0Aincorporating%20it%20into%20the%20ODE%20solver%2C%20showcasing%20its%20effectiveness%20in%20improving%0Athe%20motion%20quality%20of%20the%20generated%20videos%20with%20the%20improved%20motion-related%0Ametrics%20from%20VBench%20and%20T2V-CompBench.%20Empirically%2C%20our%20T2V-Turbo-v2%0Aestablishes%20a%20new%20state-of-the-art%20result%20on%20VBench%2C%20with%20a%20Total%20score%20of%0A85.13%2C%20surpassing%20proprietary%20systems%20such%20as%20Gen-3%20and%20Kling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.05677v3&entry.124074799=Read"},
{"title": "4DRadar-GS: Self-Supervised Dynamic Driving Scene Reconstruction with 4D\n  Radar", "author": "Xiao Tang and Guirong Zhuo and Cong Wang and Boyuan Zheng and Minqing Huang and Lianqing Zheng and Long Chen and Shouyi Lu", "abstract": "  3D reconstruction and novel view synthesis are critical for validating\nautonomous driving systems and training advanced perception models. Recent\nself-supervised methods have gained significant attention due to their\ncost-effectiveness and enhanced generalization in scenarios where annotated\nbounding boxes are unavailable. However, existing approaches, which often rely\non frequency-domain decoupling or optical flow, struggle to accurately\nreconstruct dynamic objects due to imprecise motion estimation and weak\ntemporal consistency, resulting in incomplete or distorted representations of\ndynamic scene elements. To address these challenges, we propose 4DRadar-GS, a\n4D Radar-augmented self-supervised 3D reconstruction framework tailored for\ndynamic driving scenes. Specifically, we first present a 4D Radar-assisted\nGaussian initialization scheme that leverages 4D Radar's velocity and spatial\ninformation to segment dynamic objects and recover monocular depth scale,\ngenerating accurate Gaussian point representations. In addition, we propose a\nVelocity-guided PointTrack (VGPT) model, which is jointly trained with the\nreconstruction pipeline under scene flow supervision, to track fine-grained\ndynamic trajectories and construct temporally consistent representations.\nEvaluated on the OmniHD-Scenes dataset, 4DRadar-GS achieves state-of-the-art\nperformance in dynamic driving scene 3D reconstruction.\n", "link": "http://arxiv.org/abs/2509.12931v1", "date": "2025-09-16", "relevancy": 2.5394, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6595}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6256}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5964}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%204DRadar-GS%3A%20Self-Supervised%20Dynamic%20Driving%20Scene%20Reconstruction%20with%204D%0A%20%20Radar&body=Title%3A%204DRadar-GS%3A%20Self-Supervised%20Dynamic%20Driving%20Scene%20Reconstruction%20with%204D%0A%20%20Radar%0AAuthor%3A%20Xiao%20Tang%20and%20Guirong%20Zhuo%20and%20Cong%20Wang%20and%20Boyuan%20Zheng%20and%20Minqing%20Huang%20and%20Lianqing%20Zheng%20and%20Long%20Chen%20and%20Shouyi%20Lu%0AAbstract%3A%20%20%203D%20reconstruction%20and%20novel%20view%20synthesis%20are%20critical%20for%20validating%0Aautonomous%20driving%20systems%20and%20training%20advanced%20perception%20models.%20Recent%0Aself-supervised%20methods%20have%20gained%20significant%20attention%20due%20to%20their%0Acost-effectiveness%20and%20enhanced%20generalization%20in%20scenarios%20where%20annotated%0Abounding%20boxes%20are%20unavailable.%20However%2C%20existing%20approaches%2C%20which%20often%20rely%0Aon%20frequency-domain%20decoupling%20or%20optical%20flow%2C%20struggle%20to%20accurately%0Areconstruct%20dynamic%20objects%20due%20to%20imprecise%20motion%20estimation%20and%20weak%0Atemporal%20consistency%2C%20resulting%20in%20incomplete%20or%20distorted%20representations%20of%0Adynamic%20scene%20elements.%20To%20address%20these%20challenges%2C%20we%20propose%204DRadar-GS%2C%20a%0A4D%20Radar-augmented%20self-supervised%203D%20reconstruction%20framework%20tailored%20for%0Adynamic%20driving%20scenes.%20Specifically%2C%20we%20first%20present%20a%204D%20Radar-assisted%0AGaussian%20initialization%20scheme%20that%20leverages%204D%20Radar%27s%20velocity%20and%20spatial%0Ainformation%20to%20segment%20dynamic%20objects%20and%20recover%20monocular%20depth%20scale%2C%0Agenerating%20accurate%20Gaussian%20point%20representations.%20In%20addition%2C%20we%20propose%20a%0AVelocity-guided%20PointTrack%20%28VGPT%29%20model%2C%20which%20is%20jointly%20trained%20with%20the%0Areconstruction%20pipeline%20under%20scene%20flow%20supervision%2C%20to%20track%20fine-grained%0Adynamic%20trajectories%20and%20construct%20temporally%20consistent%20representations.%0AEvaluated%20on%20the%20OmniHD-Scenes%20dataset%2C%204DRadar-GS%20achieves%20state-of-the-art%0Aperformance%20in%20dynamic%20driving%20scene%203D%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12931v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D4DRadar-GS%253A%2520Self-Supervised%2520Dynamic%2520Driving%2520Scene%2520Reconstruction%2520with%25204D%250A%2520%2520Radar%26entry.906535625%3DXiao%2520Tang%2520and%2520Guirong%2520Zhuo%2520and%2520Cong%2520Wang%2520and%2520Boyuan%2520Zheng%2520and%2520Minqing%2520Huang%2520and%2520Lianqing%2520Zheng%2520and%2520Long%2520Chen%2520and%2520Shouyi%2520Lu%26entry.1292438233%3D%2520%25203D%2520reconstruction%2520and%2520novel%2520view%2520synthesis%2520are%2520critical%2520for%2520validating%250Aautonomous%2520driving%2520systems%2520and%2520training%2520advanced%2520perception%2520models.%2520Recent%250Aself-supervised%2520methods%2520have%2520gained%2520significant%2520attention%2520due%2520to%2520their%250Acost-effectiveness%2520and%2520enhanced%2520generalization%2520in%2520scenarios%2520where%2520annotated%250Abounding%2520boxes%2520are%2520unavailable.%2520However%252C%2520existing%2520approaches%252C%2520which%2520often%2520rely%250Aon%2520frequency-domain%2520decoupling%2520or%2520optical%2520flow%252C%2520struggle%2520to%2520accurately%250Areconstruct%2520dynamic%2520objects%2520due%2520to%2520imprecise%2520motion%2520estimation%2520and%2520weak%250Atemporal%2520consistency%252C%2520resulting%2520in%2520incomplete%2520or%2520distorted%2520representations%2520of%250Adynamic%2520scene%2520elements.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%25204DRadar-GS%252C%2520a%250A4D%2520Radar-augmented%2520self-supervised%25203D%2520reconstruction%2520framework%2520tailored%2520for%250Adynamic%2520driving%2520scenes.%2520Specifically%252C%2520we%2520first%2520present%2520a%25204D%2520Radar-assisted%250AGaussian%2520initialization%2520scheme%2520that%2520leverages%25204D%2520Radar%2527s%2520velocity%2520and%2520spatial%250Ainformation%2520to%2520segment%2520dynamic%2520objects%2520and%2520recover%2520monocular%2520depth%2520scale%252C%250Agenerating%2520accurate%2520Gaussian%2520point%2520representations.%2520In%2520addition%252C%2520we%2520propose%2520a%250AVelocity-guided%2520PointTrack%2520%2528VGPT%2529%2520model%252C%2520which%2520is%2520jointly%2520trained%2520with%2520the%250Areconstruction%2520pipeline%2520under%2520scene%2520flow%2520supervision%252C%2520to%2520track%2520fine-grained%250Adynamic%2520trajectories%2520and%2520construct%2520temporally%2520consistent%2520representations.%250AEvaluated%2520on%2520the%2520OmniHD-Scenes%2520dataset%252C%25204DRadar-GS%2520achieves%2520state-of-the-art%250Aperformance%2520in%2520dynamic%2520driving%2520scene%25203D%2520reconstruction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12931v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=4DRadar-GS%3A%20Self-Supervised%20Dynamic%20Driving%20Scene%20Reconstruction%20with%204D%0A%20%20Radar&entry.906535625=Xiao%20Tang%20and%20Guirong%20Zhuo%20and%20Cong%20Wang%20and%20Boyuan%20Zheng%20and%20Minqing%20Huang%20and%20Lianqing%20Zheng%20and%20Long%20Chen%20and%20Shouyi%20Lu&entry.1292438233=%20%203D%20reconstruction%20and%20novel%20view%20synthesis%20are%20critical%20for%20validating%0Aautonomous%20driving%20systems%20and%20training%20advanced%20perception%20models.%20Recent%0Aself-supervised%20methods%20have%20gained%20significant%20attention%20due%20to%20their%0Acost-effectiveness%20and%20enhanced%20generalization%20in%20scenarios%20where%20annotated%0Abounding%20boxes%20are%20unavailable.%20However%2C%20existing%20approaches%2C%20which%20often%20rely%0Aon%20frequency-domain%20decoupling%20or%20optical%20flow%2C%20struggle%20to%20accurately%0Areconstruct%20dynamic%20objects%20due%20to%20imprecise%20motion%20estimation%20and%20weak%0Atemporal%20consistency%2C%20resulting%20in%20incomplete%20or%20distorted%20representations%20of%0Adynamic%20scene%20elements.%20To%20address%20these%20challenges%2C%20we%20propose%204DRadar-GS%2C%20a%0A4D%20Radar-augmented%20self-supervised%203D%20reconstruction%20framework%20tailored%20for%0Adynamic%20driving%20scenes.%20Specifically%2C%20we%20first%20present%20a%204D%20Radar-assisted%0AGaussian%20initialization%20scheme%20that%20leverages%204D%20Radar%27s%20velocity%20and%20spatial%0Ainformation%20to%20segment%20dynamic%20objects%20and%20recover%20monocular%20depth%20scale%2C%0Agenerating%20accurate%20Gaussian%20point%20representations.%20In%20addition%2C%20we%20propose%20a%0AVelocity-guided%20PointTrack%20%28VGPT%29%20model%2C%20which%20is%20jointly%20trained%20with%20the%0Areconstruction%20pipeline%20under%20scene%20flow%20supervision%2C%20to%20track%20fine-grained%0Adynamic%20trajectories%20and%20construct%20temporally%20consistent%20representations.%0AEvaluated%20on%20the%20OmniHD-Scenes%20dataset%2C%204DRadar-GS%20achieves%20state-of-the-art%0Aperformance%20in%20dynamic%20driving%20scene%203D%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12931v1&entry.124074799=Read"},
{"title": "AREPAS: Anomaly Detection in Fine-Grained Anatomy with\n  Reconstruction-Based Semantic Patch-Scoring", "author": "Branko Mitic and Philipp Seeb\u00f6ck and Helmut Prosch and Georg Langs", "abstract": "  Early detection of newly emerging diseases, lesion severity assessment,\ndifferentiation of medical conditions and automated screening are examples for\nthe wide applicability and importance of anomaly detection (AD) and\nunsupervised segmentation in medicine. Normal fine-grained tissue variability\nsuch as present in pulmonary anatomy is a major challenge for existing\ngenerative AD methods. Here, we propose a novel generative AD approach\naddressing this issue. It consists of an image-to-image translation for\nanomaly-free reconstruction and a subsequent patch similarity scoring between\nobserved and generated image-pairs for precise anomaly localization. We\nvalidate the new method on chest computed tomography (CT) scans for the\ndetection and segmentation of infectious disease lesions. To assess\ngeneralizability, we evaluate the method on an ischemic stroke lesion\nsegmentation task in T1-weighted brain MRI. Results show improved pixel-level\nanomaly segmentation in both chest CTs and brain MRIs, with relative DICE score\nimprovements of +1.9% and +4.4%, respectively, compared to other\nstate-of-the-art reconstruction-based methods.\n", "link": "http://arxiv.org/abs/2509.12905v1", "date": "2025-09-16", "relevancy": 2.5275, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5073}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5052}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.504}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AREPAS%3A%20Anomaly%20Detection%20in%20Fine-Grained%20Anatomy%20with%0A%20%20Reconstruction-Based%20Semantic%20Patch-Scoring&body=Title%3A%20AREPAS%3A%20Anomaly%20Detection%20in%20Fine-Grained%20Anatomy%20with%0A%20%20Reconstruction-Based%20Semantic%20Patch-Scoring%0AAuthor%3A%20Branko%20Mitic%20and%20Philipp%20Seeb%C3%B6ck%20and%20Helmut%20Prosch%20and%20Georg%20Langs%0AAbstract%3A%20%20%20Early%20detection%20of%20newly%20emerging%20diseases%2C%20lesion%20severity%20assessment%2C%0Adifferentiation%20of%20medical%20conditions%20and%20automated%20screening%20are%20examples%20for%0Athe%20wide%20applicability%20and%20importance%20of%20anomaly%20detection%20%28AD%29%20and%0Aunsupervised%20segmentation%20in%20medicine.%20Normal%20fine-grained%20tissue%20variability%0Asuch%20as%20present%20in%20pulmonary%20anatomy%20is%20a%20major%20challenge%20for%20existing%0Agenerative%20AD%20methods.%20Here%2C%20we%20propose%20a%20novel%20generative%20AD%20approach%0Aaddressing%20this%20issue.%20It%20consists%20of%20an%20image-to-image%20translation%20for%0Aanomaly-free%20reconstruction%20and%20a%20subsequent%20patch%20similarity%20scoring%20between%0Aobserved%20and%20generated%20image-pairs%20for%20precise%20anomaly%20localization.%20We%0Avalidate%20the%20new%20method%20on%20chest%20computed%20tomography%20%28CT%29%20scans%20for%20the%0Adetection%20and%20segmentation%20of%20infectious%20disease%20lesions.%20To%20assess%0Ageneralizability%2C%20we%20evaluate%20the%20method%20on%20an%20ischemic%20stroke%20lesion%0Asegmentation%20task%20in%20T1-weighted%20brain%20MRI.%20Results%20show%20improved%20pixel-level%0Aanomaly%20segmentation%20in%20both%20chest%20CTs%20and%20brain%20MRIs%2C%20with%20relative%20DICE%20score%0Aimprovements%20of%20%2B1.9%25%20and%20%2B4.4%25%2C%20respectively%2C%20compared%20to%20other%0Astate-of-the-art%20reconstruction-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAREPAS%253A%2520Anomaly%2520Detection%2520in%2520Fine-Grained%2520Anatomy%2520with%250A%2520%2520Reconstruction-Based%2520Semantic%2520Patch-Scoring%26entry.906535625%3DBranko%2520Mitic%2520and%2520Philipp%2520Seeb%25C3%25B6ck%2520and%2520Helmut%2520Prosch%2520and%2520Georg%2520Langs%26entry.1292438233%3D%2520%2520Early%2520detection%2520of%2520newly%2520emerging%2520diseases%252C%2520lesion%2520severity%2520assessment%252C%250Adifferentiation%2520of%2520medical%2520conditions%2520and%2520automated%2520screening%2520are%2520examples%2520for%250Athe%2520wide%2520applicability%2520and%2520importance%2520of%2520anomaly%2520detection%2520%2528AD%2529%2520and%250Aunsupervised%2520segmentation%2520in%2520medicine.%2520Normal%2520fine-grained%2520tissue%2520variability%250Asuch%2520as%2520present%2520in%2520pulmonary%2520anatomy%2520is%2520a%2520major%2520challenge%2520for%2520existing%250Agenerative%2520AD%2520methods.%2520Here%252C%2520we%2520propose%2520a%2520novel%2520generative%2520AD%2520approach%250Aaddressing%2520this%2520issue.%2520It%2520consists%2520of%2520an%2520image-to-image%2520translation%2520for%250Aanomaly-free%2520reconstruction%2520and%2520a%2520subsequent%2520patch%2520similarity%2520scoring%2520between%250Aobserved%2520and%2520generated%2520image-pairs%2520for%2520precise%2520anomaly%2520localization.%2520We%250Avalidate%2520the%2520new%2520method%2520on%2520chest%2520computed%2520tomography%2520%2528CT%2529%2520scans%2520for%2520the%250Adetection%2520and%2520segmentation%2520of%2520infectious%2520disease%2520lesions.%2520To%2520assess%250Ageneralizability%252C%2520we%2520evaluate%2520the%2520method%2520on%2520an%2520ischemic%2520stroke%2520lesion%250Asegmentation%2520task%2520in%2520T1-weighted%2520brain%2520MRI.%2520Results%2520show%2520improved%2520pixel-level%250Aanomaly%2520segmentation%2520in%2520both%2520chest%2520CTs%2520and%2520brain%2520MRIs%252C%2520with%2520relative%2520DICE%2520score%250Aimprovements%2520of%2520%252B1.9%2525%2520and%2520%252B4.4%2525%252C%2520respectively%252C%2520compared%2520to%2520other%250Astate-of-the-art%2520reconstruction-based%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AREPAS%3A%20Anomaly%20Detection%20in%20Fine-Grained%20Anatomy%20with%0A%20%20Reconstruction-Based%20Semantic%20Patch-Scoring&entry.906535625=Branko%20Mitic%20and%20Philipp%20Seeb%C3%B6ck%20and%20Helmut%20Prosch%20and%20Georg%20Langs&entry.1292438233=%20%20Early%20detection%20of%20newly%20emerging%20diseases%2C%20lesion%20severity%20assessment%2C%0Adifferentiation%20of%20medical%20conditions%20and%20automated%20screening%20are%20examples%20for%0Athe%20wide%20applicability%20and%20importance%20of%20anomaly%20detection%20%28AD%29%20and%0Aunsupervised%20segmentation%20in%20medicine.%20Normal%20fine-grained%20tissue%20variability%0Asuch%20as%20present%20in%20pulmonary%20anatomy%20is%20a%20major%20challenge%20for%20existing%0Agenerative%20AD%20methods.%20Here%2C%20we%20propose%20a%20novel%20generative%20AD%20approach%0Aaddressing%20this%20issue.%20It%20consists%20of%20an%20image-to-image%20translation%20for%0Aanomaly-free%20reconstruction%20and%20a%20subsequent%20patch%20similarity%20scoring%20between%0Aobserved%20and%20generated%20image-pairs%20for%20precise%20anomaly%20localization.%20We%0Avalidate%20the%20new%20method%20on%20chest%20computed%20tomography%20%28CT%29%20scans%20for%20the%0Adetection%20and%20segmentation%20of%20infectious%20disease%20lesions.%20To%20assess%0Ageneralizability%2C%20we%20evaluate%20the%20method%20on%20an%20ischemic%20stroke%20lesion%0Asegmentation%20task%20in%20T1-weighted%20brain%20MRI.%20Results%20show%20improved%20pixel-level%0Aanomaly%20segmentation%20in%20both%20chest%20CTs%20and%20brain%20MRIs%2C%20with%20relative%20DICE%20score%0Aimprovements%20of%20%2B1.9%25%20and%20%2B4.4%25%2C%20respectively%2C%20compared%20to%20other%0Astate-of-the-art%20reconstruction-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12905v1&entry.124074799=Read"},
{"title": "GView: A Survey of Binary Forensics via Visual, Semantic, and\n  AI-Enhanced Analysis", "author": "Raul Zaharia and Drago\u015f Gavrilu\u0163 and Gheorghi\u0163\u0103 Mutu", "abstract": "  Cybersecurity threats continue to become more sophisticated and diverse in\ntheir artifacts, boosting both their volume and complexity. To overcome those\nchallenges, we present GView, an open-source forensic analysis framework with\nvisual and AI-enhanced reasoning. It started with focus on the practical\ncybersecurity industry. It has evolved significantly, incorporating large\nlanguage models (LLMs) to dynamically enhance reasoning and ease the forensic\nworkflows. This paper surveys both the current state of GView with its\npublished papers alongside those that are in the publishing process. It also\nincludes its innovative use of logical inference through predicates and\ninference rules for both the analyzed documents and the user's actions for\nbetter suggestions. We highlight the extensible architecture, showcasing its\npotential as a bridge between the practical forensics worlds with the academic\nresearch.\n", "link": "http://arxiv.org/abs/2509.13025v1", "date": "2025-09-16", "relevancy": 2.5219, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5148}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5148}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4835}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GView%3A%20A%20Survey%20of%20Binary%20Forensics%20via%20Visual%2C%20Semantic%2C%20and%0A%20%20AI-Enhanced%20Analysis&body=Title%3A%20GView%3A%20A%20Survey%20of%20Binary%20Forensics%20via%20Visual%2C%20Semantic%2C%20and%0A%20%20AI-Enhanced%20Analysis%0AAuthor%3A%20Raul%20Zaharia%20and%20Drago%C5%9F%20Gavrilu%C5%A3%20and%20Gheorghi%C5%A3%C4%83%20Mutu%0AAbstract%3A%20%20%20Cybersecurity%20threats%20continue%20to%20become%20more%20sophisticated%20and%20diverse%20in%0Atheir%20artifacts%2C%20boosting%20both%20their%20volume%20and%20complexity.%20To%20overcome%20those%0Achallenges%2C%20we%20present%20GView%2C%20an%20open-source%20forensic%20analysis%20framework%20with%0Avisual%20and%20AI-enhanced%20reasoning.%20It%20started%20with%20focus%20on%20the%20practical%0Acybersecurity%20industry.%20It%20has%20evolved%20significantly%2C%20incorporating%20large%0Alanguage%20models%20%28LLMs%29%20to%20dynamically%20enhance%20reasoning%20and%20ease%20the%20forensic%0Aworkflows.%20This%20paper%20surveys%20both%20the%20current%20state%20of%20GView%20with%20its%0Apublished%20papers%20alongside%20those%20that%20are%20in%20the%20publishing%20process.%20It%20also%0Aincludes%20its%20innovative%20use%20of%20logical%20inference%20through%20predicates%20and%0Ainference%20rules%20for%20both%20the%20analyzed%20documents%20and%20the%20user%27s%20actions%20for%0Abetter%20suggestions.%20We%20highlight%20the%20extensible%20architecture%2C%20showcasing%20its%0Apotential%20as%20a%20bridge%20between%20the%20practical%20forensics%20worlds%20with%20the%20academic%0Aresearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13025v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGView%253A%2520A%2520Survey%2520of%2520Binary%2520Forensics%2520via%2520Visual%252C%2520Semantic%252C%2520and%250A%2520%2520AI-Enhanced%2520Analysis%26entry.906535625%3DRaul%2520Zaharia%2520and%2520Drago%25C5%259F%2520Gavrilu%25C5%25A3%2520and%2520Gheorghi%25C5%25A3%25C4%2583%2520Mutu%26entry.1292438233%3D%2520%2520Cybersecurity%2520threats%2520continue%2520to%2520become%2520more%2520sophisticated%2520and%2520diverse%2520in%250Atheir%2520artifacts%252C%2520boosting%2520both%2520their%2520volume%2520and%2520complexity.%2520To%2520overcome%2520those%250Achallenges%252C%2520we%2520present%2520GView%252C%2520an%2520open-source%2520forensic%2520analysis%2520framework%2520with%250Avisual%2520and%2520AI-enhanced%2520reasoning.%2520It%2520started%2520with%2520focus%2520on%2520the%2520practical%250Acybersecurity%2520industry.%2520It%2520has%2520evolved%2520significantly%252C%2520incorporating%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520to%2520dynamically%2520enhance%2520reasoning%2520and%2520ease%2520the%2520forensic%250Aworkflows.%2520This%2520paper%2520surveys%2520both%2520the%2520current%2520state%2520of%2520GView%2520with%2520its%250Apublished%2520papers%2520alongside%2520those%2520that%2520are%2520in%2520the%2520publishing%2520process.%2520It%2520also%250Aincludes%2520its%2520innovative%2520use%2520of%2520logical%2520inference%2520through%2520predicates%2520and%250Ainference%2520rules%2520for%2520both%2520the%2520analyzed%2520documents%2520and%2520the%2520user%2527s%2520actions%2520for%250Abetter%2520suggestions.%2520We%2520highlight%2520the%2520extensible%2520architecture%252C%2520showcasing%2520its%250Apotential%2520as%2520a%2520bridge%2520between%2520the%2520practical%2520forensics%2520worlds%2520with%2520the%2520academic%250Aresearch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13025v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GView%3A%20A%20Survey%20of%20Binary%20Forensics%20via%20Visual%2C%20Semantic%2C%20and%0A%20%20AI-Enhanced%20Analysis&entry.906535625=Raul%20Zaharia%20and%20Drago%C5%9F%20Gavrilu%C5%A3%20and%20Gheorghi%C5%A3%C4%83%20Mutu&entry.1292438233=%20%20Cybersecurity%20threats%20continue%20to%20become%20more%20sophisticated%20and%20diverse%20in%0Atheir%20artifacts%2C%20boosting%20both%20their%20volume%20and%20complexity.%20To%20overcome%20those%0Achallenges%2C%20we%20present%20GView%2C%20an%20open-source%20forensic%20analysis%20framework%20with%0Avisual%20and%20AI-enhanced%20reasoning.%20It%20started%20with%20focus%20on%20the%20practical%0Acybersecurity%20industry.%20It%20has%20evolved%20significantly%2C%20incorporating%20large%0Alanguage%20models%20%28LLMs%29%20to%20dynamically%20enhance%20reasoning%20and%20ease%20the%20forensic%0Aworkflows.%20This%20paper%20surveys%20both%20the%20current%20state%20of%20GView%20with%20its%0Apublished%20papers%20alongside%20those%20that%20are%20in%20the%20publishing%20process.%20It%20also%0Aincludes%20its%20innovative%20use%20of%20logical%20inference%20through%20predicates%20and%0Ainference%20rules%20for%20both%20the%20analyzed%20documents%20and%20the%20user%27s%20actions%20for%0Abetter%20suggestions.%20We%20highlight%20the%20extensible%20architecture%2C%20showcasing%20its%0Apotential%20as%20a%20bridge%20between%20the%20practical%20forensics%20worlds%20with%20the%20academic%0Aresearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13025v1&entry.124074799=Read"},
{"title": "Shapes of Cognition for Computational Cognitive Modeling", "author": "Marjorie McShane and Sergei Nirenburg and Sanjay Oruganti and Jesse English", "abstract": "  Shapes of cognition is a new conceptual paradigm for the computational\ncognitive modeling of Language-Endowed Intelligent Agents (LEIAs). Shapes are\nremembered constellations of sensory, linguistic, conceptual, episodic, and\nprocedural knowledge that allow agents to cut through the complexity of real\nlife the same way as people do: by expecting things to be typical, recognizing\npatterns, acting by habit, reasoning by analogy, satisficing, and generally\nminimizing cognitive load to the degree situations permit. Atypical outcomes\nare treated using shapes-based recovery methods, such as learning on the fly,\nasking a human partner for help, or seeking an actionable, even if imperfect,\nsituational understanding. Although shapes is an umbrella term, it is not\nvague: shapes-based modeling involves particular objectives, hypotheses,\nmodeling strategies, knowledge bases, and actual models of wide-ranging\nphenomena, all implemented within a particular cognitive architecture. Such\nspecificity is needed both to vet our hypotheses and to achieve our practical\naims of building useful agent systems that are explainable, extensible, and\nworthy of our trust, even in critical domains. However, although the LEIA\nexample of shapes-based modeling is specific, the principles can be applied\nmore broadly, giving new life to knowledge-based and hybrid AI.\n", "link": "http://arxiv.org/abs/2509.13288v1", "date": "2025-09-16", "relevancy": 2.5078, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5107}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shapes%20of%20Cognition%20for%20Computational%20Cognitive%20Modeling&body=Title%3A%20Shapes%20of%20Cognition%20for%20Computational%20Cognitive%20Modeling%0AAuthor%3A%20Marjorie%20McShane%20and%20Sergei%20Nirenburg%20and%20Sanjay%20Oruganti%20and%20Jesse%20English%0AAbstract%3A%20%20%20Shapes%20of%20cognition%20is%20a%20new%20conceptual%20paradigm%20for%20the%20computational%0Acognitive%20modeling%20of%20Language-Endowed%20Intelligent%20Agents%20%28LEIAs%29.%20Shapes%20are%0Aremembered%20constellations%20of%20sensory%2C%20linguistic%2C%20conceptual%2C%20episodic%2C%20and%0Aprocedural%20knowledge%20that%20allow%20agents%20to%20cut%20through%20the%20complexity%20of%20real%0Alife%20the%20same%20way%20as%20people%20do%3A%20by%20expecting%20things%20to%20be%20typical%2C%20recognizing%0Apatterns%2C%20acting%20by%20habit%2C%20reasoning%20by%20analogy%2C%20satisficing%2C%20and%20generally%0Aminimizing%20cognitive%20load%20to%20the%20degree%20situations%20permit.%20Atypical%20outcomes%0Aare%20treated%20using%20shapes-based%20recovery%20methods%2C%20such%20as%20learning%20on%20the%20fly%2C%0Aasking%20a%20human%20partner%20for%20help%2C%20or%20seeking%20an%20actionable%2C%20even%20if%20imperfect%2C%0Asituational%20understanding.%20Although%20shapes%20is%20an%20umbrella%20term%2C%20it%20is%20not%0Avague%3A%20shapes-based%20modeling%20involves%20particular%20objectives%2C%20hypotheses%2C%0Amodeling%20strategies%2C%20knowledge%20bases%2C%20and%20actual%20models%20of%20wide-ranging%0Aphenomena%2C%20all%20implemented%20within%20a%20particular%20cognitive%20architecture.%20Such%0Aspecificity%20is%20needed%20both%20to%20vet%20our%20hypotheses%20and%20to%20achieve%20our%20practical%0Aaims%20of%20building%20useful%20agent%20systems%20that%20are%20explainable%2C%20extensible%2C%20and%0Aworthy%20of%20our%20trust%2C%20even%20in%20critical%20domains.%20However%2C%20although%20the%20LEIA%0Aexample%20of%20shapes-based%20modeling%20is%20specific%2C%20the%20principles%20can%20be%20applied%0Amore%20broadly%2C%20giving%20new%20life%20to%20knowledge-based%20and%20hybrid%20AI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13288v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShapes%2520of%2520Cognition%2520for%2520Computational%2520Cognitive%2520Modeling%26entry.906535625%3DMarjorie%2520McShane%2520and%2520Sergei%2520Nirenburg%2520and%2520Sanjay%2520Oruganti%2520and%2520Jesse%2520English%26entry.1292438233%3D%2520%2520Shapes%2520of%2520cognition%2520is%2520a%2520new%2520conceptual%2520paradigm%2520for%2520the%2520computational%250Acognitive%2520modeling%2520of%2520Language-Endowed%2520Intelligent%2520Agents%2520%2528LEIAs%2529.%2520Shapes%2520are%250Aremembered%2520constellations%2520of%2520sensory%252C%2520linguistic%252C%2520conceptual%252C%2520episodic%252C%2520and%250Aprocedural%2520knowledge%2520that%2520allow%2520agents%2520to%2520cut%2520through%2520the%2520complexity%2520of%2520real%250Alife%2520the%2520same%2520way%2520as%2520people%2520do%253A%2520by%2520expecting%2520things%2520to%2520be%2520typical%252C%2520recognizing%250Apatterns%252C%2520acting%2520by%2520habit%252C%2520reasoning%2520by%2520analogy%252C%2520satisficing%252C%2520and%2520generally%250Aminimizing%2520cognitive%2520load%2520to%2520the%2520degree%2520situations%2520permit.%2520Atypical%2520outcomes%250Aare%2520treated%2520using%2520shapes-based%2520recovery%2520methods%252C%2520such%2520as%2520learning%2520on%2520the%2520fly%252C%250Aasking%2520a%2520human%2520partner%2520for%2520help%252C%2520or%2520seeking%2520an%2520actionable%252C%2520even%2520if%2520imperfect%252C%250Asituational%2520understanding.%2520Although%2520shapes%2520is%2520an%2520umbrella%2520term%252C%2520it%2520is%2520not%250Avague%253A%2520shapes-based%2520modeling%2520involves%2520particular%2520objectives%252C%2520hypotheses%252C%250Amodeling%2520strategies%252C%2520knowledge%2520bases%252C%2520and%2520actual%2520models%2520of%2520wide-ranging%250Aphenomena%252C%2520all%2520implemented%2520within%2520a%2520particular%2520cognitive%2520architecture.%2520Such%250Aspecificity%2520is%2520needed%2520both%2520to%2520vet%2520our%2520hypotheses%2520and%2520to%2520achieve%2520our%2520practical%250Aaims%2520of%2520building%2520useful%2520agent%2520systems%2520that%2520are%2520explainable%252C%2520extensible%252C%2520and%250Aworthy%2520of%2520our%2520trust%252C%2520even%2520in%2520critical%2520domains.%2520However%252C%2520although%2520the%2520LEIA%250Aexample%2520of%2520shapes-based%2520modeling%2520is%2520specific%252C%2520the%2520principles%2520can%2520be%2520applied%250Amore%2520broadly%252C%2520giving%2520new%2520life%2520to%2520knowledge-based%2520and%2520hybrid%2520AI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13288v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shapes%20of%20Cognition%20for%20Computational%20Cognitive%20Modeling&entry.906535625=Marjorie%20McShane%20and%20Sergei%20Nirenburg%20and%20Sanjay%20Oruganti%20and%20Jesse%20English&entry.1292438233=%20%20Shapes%20of%20cognition%20is%20a%20new%20conceptual%20paradigm%20for%20the%20computational%0Acognitive%20modeling%20of%20Language-Endowed%20Intelligent%20Agents%20%28LEIAs%29.%20Shapes%20are%0Aremembered%20constellations%20of%20sensory%2C%20linguistic%2C%20conceptual%2C%20episodic%2C%20and%0Aprocedural%20knowledge%20that%20allow%20agents%20to%20cut%20through%20the%20complexity%20of%20real%0Alife%20the%20same%20way%20as%20people%20do%3A%20by%20expecting%20things%20to%20be%20typical%2C%20recognizing%0Apatterns%2C%20acting%20by%20habit%2C%20reasoning%20by%20analogy%2C%20satisficing%2C%20and%20generally%0Aminimizing%20cognitive%20load%20to%20the%20degree%20situations%20permit.%20Atypical%20outcomes%0Aare%20treated%20using%20shapes-based%20recovery%20methods%2C%20such%20as%20learning%20on%20the%20fly%2C%0Aasking%20a%20human%20partner%20for%20help%2C%20or%20seeking%20an%20actionable%2C%20even%20if%20imperfect%2C%0Asituational%20understanding.%20Although%20shapes%20is%20an%20umbrella%20term%2C%20it%20is%20not%0Avague%3A%20shapes-based%20modeling%20involves%20particular%20objectives%2C%20hypotheses%2C%0Amodeling%20strategies%2C%20knowledge%20bases%2C%20and%20actual%20models%20of%20wide-ranging%0Aphenomena%2C%20all%20implemented%20within%20a%20particular%20cognitive%20architecture.%20Such%0Aspecificity%20is%20needed%20both%20to%20vet%20our%20hypotheses%20and%20to%20achieve%20our%20practical%0Aaims%20of%20building%20useful%20agent%20systems%20that%20are%20explainable%2C%20extensible%2C%20and%0Aworthy%20of%20our%20trust%2C%20even%20in%20critical%20domains.%20However%2C%20although%20the%20LEIA%0Aexample%20of%20shapes-based%20modeling%20is%20specific%2C%20the%20principles%20can%20be%20applied%0Amore%20broadly%2C%20giving%20new%20life%20to%20knowledge-based%20and%20hybrid%20AI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13288v1&entry.124074799=Read"},
{"title": "ChartGaze: Enhancing Chart Understanding in LVLMs with Eye-Tracking\n  Guided Attention Refinement", "author": "Ali Salamatian and Amirhossein Abaskohi and Wan-Cyuan Fan and Mir Rayat Imtiaz Hossain and Leonid Sigal and Giuseppe Carenini", "abstract": "  Charts are a crucial visual medium for communicating and representing\ninformation. While Large Vision-Language Models (LVLMs) have made progress on\nchart question answering (CQA), the task remains challenging, particularly when\nmodels attend to irrelevant regions of the chart. In this work, we present\nChartGaze, a new eye-tracking dataset that captures human gaze patterns during\nchart reasoning tasks. Through a systematic comparison of human and model\nattention, we find that LVLMs often diverge from human gaze, leading to reduced\ninterpretability and accuracy. To address this, we propose a gaze-guided\nattention refinement that aligns image-text attention with human fixations. Our\napproach improves both answer accuracy and attention alignment, yielding gains\nof up to 2.56 percentage points across multiple models. These results\ndemonstrate the promise of incorporating human gaze to enhance both the\nreasoning quality and interpretability of chart-focused LVLMs.\n", "link": "http://arxiv.org/abs/2509.13282v1", "date": "2025-09-16", "relevancy": 2.5027, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5038}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4989}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4989}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChartGaze%3A%20Enhancing%20Chart%20Understanding%20in%20LVLMs%20with%20Eye-Tracking%0A%20%20Guided%20Attention%20Refinement&body=Title%3A%20ChartGaze%3A%20Enhancing%20Chart%20Understanding%20in%20LVLMs%20with%20Eye-Tracking%0A%20%20Guided%20Attention%20Refinement%0AAuthor%3A%20Ali%20Salamatian%20and%20Amirhossein%20Abaskohi%20and%20Wan-Cyuan%20Fan%20and%20Mir%20Rayat%20Imtiaz%20Hossain%20and%20Leonid%20Sigal%20and%20Giuseppe%20Carenini%0AAbstract%3A%20%20%20Charts%20are%20a%20crucial%20visual%20medium%20for%20communicating%20and%20representing%0Ainformation.%20While%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20made%20progress%20on%0Achart%20question%20answering%20%28CQA%29%2C%20the%20task%20remains%20challenging%2C%20particularly%20when%0Amodels%20attend%20to%20irrelevant%20regions%20of%20the%20chart.%20In%20this%20work%2C%20we%20present%0AChartGaze%2C%20a%20new%20eye-tracking%20dataset%20that%20captures%20human%20gaze%20patterns%20during%0Achart%20reasoning%20tasks.%20Through%20a%20systematic%20comparison%20of%20human%20and%20model%0Aattention%2C%20we%20find%20that%20LVLMs%20often%20diverge%20from%20human%20gaze%2C%20leading%20to%20reduced%0Ainterpretability%20and%20accuracy.%20To%20address%20this%2C%20we%20propose%20a%20gaze-guided%0Aattention%20refinement%20that%20aligns%20image-text%20attention%20with%20human%20fixations.%20Our%0Aapproach%20improves%20both%20answer%20accuracy%20and%20attention%20alignment%2C%20yielding%20gains%0Aof%20up%20to%202.56%20percentage%20points%20across%20multiple%20models.%20These%20results%0Ademonstrate%20the%20promise%20of%20incorporating%20human%20gaze%20to%20enhance%20both%20the%0Areasoning%20quality%20and%20interpretability%20of%20chart-focused%20LVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13282v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChartGaze%253A%2520Enhancing%2520Chart%2520Understanding%2520in%2520LVLMs%2520with%2520Eye-Tracking%250A%2520%2520Guided%2520Attention%2520Refinement%26entry.906535625%3DAli%2520Salamatian%2520and%2520Amirhossein%2520Abaskohi%2520and%2520Wan-Cyuan%2520Fan%2520and%2520Mir%2520Rayat%2520Imtiaz%2520Hossain%2520and%2520Leonid%2520Sigal%2520and%2520Giuseppe%2520Carenini%26entry.1292438233%3D%2520%2520Charts%2520are%2520a%2520crucial%2520visual%2520medium%2520for%2520communicating%2520and%2520representing%250Ainformation.%2520While%2520Large%2520Vision-Language%2520Models%2520%2528LVLMs%2529%2520have%2520made%2520progress%2520on%250Achart%2520question%2520answering%2520%2528CQA%2529%252C%2520the%2520task%2520remains%2520challenging%252C%2520particularly%2520when%250Amodels%2520attend%2520to%2520irrelevant%2520regions%2520of%2520the%2520chart.%2520In%2520this%2520work%252C%2520we%2520present%250AChartGaze%252C%2520a%2520new%2520eye-tracking%2520dataset%2520that%2520captures%2520human%2520gaze%2520patterns%2520during%250Achart%2520reasoning%2520tasks.%2520Through%2520a%2520systematic%2520comparison%2520of%2520human%2520and%2520model%250Aattention%252C%2520we%2520find%2520that%2520LVLMs%2520often%2520diverge%2520from%2520human%2520gaze%252C%2520leading%2520to%2520reduced%250Ainterpretability%2520and%2520accuracy.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520gaze-guided%250Aattention%2520refinement%2520that%2520aligns%2520image-text%2520attention%2520with%2520human%2520fixations.%2520Our%250Aapproach%2520improves%2520both%2520answer%2520accuracy%2520and%2520attention%2520alignment%252C%2520yielding%2520gains%250Aof%2520up%2520to%25202.56%2520percentage%2520points%2520across%2520multiple%2520models.%2520These%2520results%250Ademonstrate%2520the%2520promise%2520of%2520incorporating%2520human%2520gaze%2520to%2520enhance%2520both%2520the%250Areasoning%2520quality%2520and%2520interpretability%2520of%2520chart-focused%2520LVLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13282v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChartGaze%3A%20Enhancing%20Chart%20Understanding%20in%20LVLMs%20with%20Eye-Tracking%0A%20%20Guided%20Attention%20Refinement&entry.906535625=Ali%20Salamatian%20and%20Amirhossein%20Abaskohi%20and%20Wan-Cyuan%20Fan%20and%20Mir%20Rayat%20Imtiaz%20Hossain%20and%20Leonid%20Sigal%20and%20Giuseppe%20Carenini&entry.1292438233=%20%20Charts%20are%20a%20crucial%20visual%20medium%20for%20communicating%20and%20representing%0Ainformation.%20While%20Large%20Vision-Language%20Models%20%28LVLMs%29%20have%20made%20progress%20on%0Achart%20question%20answering%20%28CQA%29%2C%20the%20task%20remains%20challenging%2C%20particularly%20when%0Amodels%20attend%20to%20irrelevant%20regions%20of%20the%20chart.%20In%20this%20work%2C%20we%20present%0AChartGaze%2C%20a%20new%20eye-tracking%20dataset%20that%20captures%20human%20gaze%20patterns%20during%0Achart%20reasoning%20tasks.%20Through%20a%20systematic%20comparison%20of%20human%20and%20model%0Aattention%2C%20we%20find%20that%20LVLMs%20often%20diverge%20from%20human%20gaze%2C%20leading%20to%20reduced%0Ainterpretability%20and%20accuracy.%20To%20address%20this%2C%20we%20propose%20a%20gaze-guided%0Aattention%20refinement%20that%20aligns%20image-text%20attention%20with%20human%20fixations.%20Our%0Aapproach%20improves%20both%20answer%20accuracy%20and%20attention%20alignment%2C%20yielding%20gains%0Aof%20up%20to%202.56%20percentage%20points%20across%20multiple%20models.%20These%20results%0Ademonstrate%20the%20promise%20of%20incorporating%20human%20gaze%20to%20enhance%20both%20the%0Areasoning%20quality%20and%20interpretability%20of%20chart-focused%20LVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13282v1&entry.124074799=Read"},
{"title": "Bridging Performance Gaps for Foundation Models: A Post-Training\n  Strategy for ECGFounder", "author": "Ya Zhou and Yujie Yang and Xiaohan Fan and Wei Zhao", "abstract": "  ECG foundation models are increasingly popular due to their adaptability\nacross various tasks. However, their clinical applicability is often limited by\nperformance gaps compared to task-specific models, even after pre-training on\nlarge ECG datasets and fine-tuning on target data. This limitation is likely\ndue to the lack of an effective post-training strategy. In this paper, we\npropose a simple yet effective post-training approach to enhance ECGFounder, a\nstate-of-the-art ECG foundation model pre-trained on over 7 million ECG\nrecordings. Experiments on the PTB-XL benchmark show that our approach improves\nthe baseline fine-tuning strategy by 1.2%-3.3% in macro AUROC and 5.3%-20.9% in\nmacro AUPRC. Additionally, our method outperforms several recent\nstate-of-the-art approaches, including task-specific and advanced\narchitectures. Further evaluation reveals that our method is more stable and\nsample-efficient compared to the baseline, achieving a 9.1% improvement in\nmacro AUROC and a 34.9% improvement in macro AUPRC using just 10% of the\ntraining data. Ablation studies identify key components, such as stochastic\ndepth and preview linear probing, that contribute to the enhanced performance.\nThese findings underscore the potential of post-training strategies to improve\nECG foundation models, and we hope this work will contribute to the continued\ndevelopment of foundation models in the ECG domain.\n", "link": "http://arxiv.org/abs/2509.12991v1", "date": "2025-09-16", "relevancy": 2.4613, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bridging%20Performance%20Gaps%20for%20Foundation%20Models%3A%20A%20Post-Training%0A%20%20Strategy%20for%20ECGFounder&body=Title%3A%20Bridging%20Performance%20Gaps%20for%20Foundation%20Models%3A%20A%20Post-Training%0A%20%20Strategy%20for%20ECGFounder%0AAuthor%3A%20Ya%20Zhou%20and%20Yujie%20Yang%20and%20Xiaohan%20Fan%20and%20Wei%20Zhao%0AAbstract%3A%20%20%20ECG%20foundation%20models%20are%20increasingly%20popular%20due%20to%20their%20adaptability%0Aacross%20various%20tasks.%20However%2C%20their%20clinical%20applicability%20is%20often%20limited%20by%0Aperformance%20gaps%20compared%20to%20task-specific%20models%2C%20even%20after%20pre-training%20on%0Alarge%20ECG%20datasets%20and%20fine-tuning%20on%20target%20data.%20This%20limitation%20is%20likely%0Adue%20to%20the%20lack%20of%20an%20effective%20post-training%20strategy.%20In%20this%20paper%2C%20we%0Apropose%20a%20simple%20yet%20effective%20post-training%20approach%20to%20enhance%20ECGFounder%2C%20a%0Astate-of-the-art%20ECG%20foundation%20model%20pre-trained%20on%20over%207%20million%20ECG%0Arecordings.%20Experiments%20on%20the%20PTB-XL%20benchmark%20show%20that%20our%20approach%20improves%0Athe%20baseline%20fine-tuning%20strategy%20by%201.2%25-3.3%25%20in%20macro%20AUROC%20and%205.3%25-20.9%25%20in%0Amacro%20AUPRC.%20Additionally%2C%20our%20method%20outperforms%20several%20recent%0Astate-of-the-art%20approaches%2C%20including%20task-specific%20and%20advanced%0Aarchitectures.%20Further%20evaluation%20reveals%20that%20our%20method%20is%20more%20stable%20and%0Asample-efficient%20compared%20to%20the%20baseline%2C%20achieving%20a%209.1%25%20improvement%20in%0Amacro%20AUROC%20and%20a%2034.9%25%20improvement%20in%20macro%20AUPRC%20using%20just%2010%25%20of%20the%0Atraining%20data.%20Ablation%20studies%20identify%20key%20components%2C%20such%20as%20stochastic%0Adepth%20and%20preview%20linear%20probing%2C%20that%20contribute%20to%20the%20enhanced%20performance.%0AThese%20findings%20underscore%20the%20potential%20of%20post-training%20strategies%20to%20improve%0AECG%20foundation%20models%2C%20and%20we%20hope%20this%20work%20will%20contribute%20to%20the%20continued%0Adevelopment%20of%20foundation%20models%20in%20the%20ECG%20domain.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12991v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBridging%2520Performance%2520Gaps%2520for%2520Foundation%2520Models%253A%2520A%2520Post-Training%250A%2520%2520Strategy%2520for%2520ECGFounder%26entry.906535625%3DYa%2520Zhou%2520and%2520Yujie%2520Yang%2520and%2520Xiaohan%2520Fan%2520and%2520Wei%2520Zhao%26entry.1292438233%3D%2520%2520ECG%2520foundation%2520models%2520are%2520increasingly%2520popular%2520due%2520to%2520their%2520adaptability%250Aacross%2520various%2520tasks.%2520However%252C%2520their%2520clinical%2520applicability%2520is%2520often%2520limited%2520by%250Aperformance%2520gaps%2520compared%2520to%2520task-specific%2520models%252C%2520even%2520after%2520pre-training%2520on%250Alarge%2520ECG%2520datasets%2520and%2520fine-tuning%2520on%2520target%2520data.%2520This%2520limitation%2520is%2520likely%250Adue%2520to%2520the%2520lack%2520of%2520an%2520effective%2520post-training%2520strategy.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520simple%2520yet%2520effective%2520post-training%2520approach%2520to%2520enhance%2520ECGFounder%252C%2520a%250Astate-of-the-art%2520ECG%2520foundation%2520model%2520pre-trained%2520on%2520over%25207%2520million%2520ECG%250Arecordings.%2520Experiments%2520on%2520the%2520PTB-XL%2520benchmark%2520show%2520that%2520our%2520approach%2520improves%250Athe%2520baseline%2520fine-tuning%2520strategy%2520by%25201.2%2525-3.3%2525%2520in%2520macro%2520AUROC%2520and%25205.3%2525-20.9%2525%2520in%250Amacro%2520AUPRC.%2520Additionally%252C%2520our%2520method%2520outperforms%2520several%2520recent%250Astate-of-the-art%2520approaches%252C%2520including%2520task-specific%2520and%2520advanced%250Aarchitectures.%2520Further%2520evaluation%2520reveals%2520that%2520our%2520method%2520is%2520more%2520stable%2520and%250Asample-efficient%2520compared%2520to%2520the%2520baseline%252C%2520achieving%2520a%25209.1%2525%2520improvement%2520in%250Amacro%2520AUROC%2520and%2520a%252034.9%2525%2520improvement%2520in%2520macro%2520AUPRC%2520using%2520just%252010%2525%2520of%2520the%250Atraining%2520data.%2520Ablation%2520studies%2520identify%2520key%2520components%252C%2520such%2520as%2520stochastic%250Adepth%2520and%2520preview%2520linear%2520probing%252C%2520that%2520contribute%2520to%2520the%2520enhanced%2520performance.%250AThese%2520findings%2520underscore%2520the%2520potential%2520of%2520post-training%2520strategies%2520to%2520improve%250AECG%2520foundation%2520models%252C%2520and%2520we%2520hope%2520this%2520work%2520will%2520contribute%2520to%2520the%2520continued%250Adevelopment%2520of%2520foundation%2520models%2520in%2520the%2520ECG%2520domain.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12991v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bridging%20Performance%20Gaps%20for%20Foundation%20Models%3A%20A%20Post-Training%0A%20%20Strategy%20for%20ECGFounder&entry.906535625=Ya%20Zhou%20and%20Yujie%20Yang%20and%20Xiaohan%20Fan%20and%20Wei%20Zhao&entry.1292438233=%20%20ECG%20foundation%20models%20are%20increasingly%20popular%20due%20to%20their%20adaptability%0Aacross%20various%20tasks.%20However%2C%20their%20clinical%20applicability%20is%20often%20limited%20by%0Aperformance%20gaps%20compared%20to%20task-specific%20models%2C%20even%20after%20pre-training%20on%0Alarge%20ECG%20datasets%20and%20fine-tuning%20on%20target%20data.%20This%20limitation%20is%20likely%0Adue%20to%20the%20lack%20of%20an%20effective%20post-training%20strategy.%20In%20this%20paper%2C%20we%0Apropose%20a%20simple%20yet%20effective%20post-training%20approach%20to%20enhance%20ECGFounder%2C%20a%0Astate-of-the-art%20ECG%20foundation%20model%20pre-trained%20on%20over%207%20million%20ECG%0Arecordings.%20Experiments%20on%20the%20PTB-XL%20benchmark%20show%20that%20our%20approach%20improves%0Athe%20baseline%20fine-tuning%20strategy%20by%201.2%25-3.3%25%20in%20macro%20AUROC%20and%205.3%25-20.9%25%20in%0Amacro%20AUPRC.%20Additionally%2C%20our%20method%20outperforms%20several%20recent%0Astate-of-the-art%20approaches%2C%20including%20task-specific%20and%20advanced%0Aarchitectures.%20Further%20evaluation%20reveals%20that%20our%20method%20is%20more%20stable%20and%0Asample-efficient%20compared%20to%20the%20baseline%2C%20achieving%20a%209.1%25%20improvement%20in%0Amacro%20AUROC%20and%20a%2034.9%25%20improvement%20in%20macro%20AUPRC%20using%20just%2010%25%20of%20the%0Atraining%20data.%20Ablation%20studies%20identify%20key%20components%2C%20such%20as%20stochastic%0Adepth%20and%20preview%20linear%20probing%2C%20that%20contribute%20to%20the%20enhanced%20performance.%0AThese%20findings%20underscore%20the%20potential%20of%20post-training%20strategies%20to%20improve%0AECG%20foundation%20models%2C%20and%20we%20hope%20this%20work%20will%20contribute%20to%20the%20continued%0Adevelopment%20of%20foundation%20models%20in%20the%20ECG%20domain.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12991v1&entry.124074799=Read"},
{"title": "TokenSkip: Controllable Chain-of-Thought Compression in LLMs", "author": "Heming Xia and Chak Tou Leong and Wenjie Wang and Yongqi Li and Wenjie Li", "abstract": "  Chain-of-Thought (CoT) has been proven effective in enhancing the reasoning\ncapabilities of large language models (LLMs). Recent advancements, such as\nOpenAI's o1 and DeepSeek-R1, suggest that scaling up the length of CoT\nsequences during inference could further boost LLM reasoning performance.\nHowever, due to the autoregressive nature of LLM decoding, longer CoT outputs\nlead to a linear increase in inference latency, adversely affecting user\nexperience, particularly when the CoT exceeds 10,000 tokens. To address this\nlimitation, we analyze the semantic importance of tokens within CoT outputs and\nreveal that their contributions to reasoning vary. Building on this insight, we\npropose TokenSkip, a simple yet effective approach that enables LLMs to\nselectively skip less important tokens, allowing for controllable CoT\ncompression. Extensive experiments across various models and tasks demonstrate\nthe effectiveness of TokenSkip in reducing CoT token usage while preserving\nstrong reasoning performance. Notably, when applied to Qwen2.5-14B-Instruct,\nTokenSkip reduces reasoning tokens by 40% (from 313 to 181) on GSM8K, with less\nthan a 0.4% performance drop. We release our code and checkpoints in\nhttps://github.com/hemingkx/TokenSkip.\n", "link": "http://arxiv.org/abs/2502.12067v3", "date": "2025-09-16", "relevancy": 2.4607, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4648}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TokenSkip%3A%20Controllable%20Chain-of-Thought%20Compression%20in%20LLMs&body=Title%3A%20TokenSkip%3A%20Controllable%20Chain-of-Thought%20Compression%20in%20LLMs%0AAuthor%3A%20Heming%20Xia%20and%20Chak%20Tou%20Leong%20and%20Wenjie%20Wang%20and%20Yongqi%20Li%20and%20Wenjie%20Li%0AAbstract%3A%20%20%20Chain-of-Thought%20%28CoT%29%20has%20been%20proven%20effective%20in%20enhancing%20the%20reasoning%0Acapabilities%20of%20large%20language%20models%20%28LLMs%29.%20Recent%20advancements%2C%20such%20as%0AOpenAI%27s%20o1%20and%20DeepSeek-R1%2C%20suggest%20that%20scaling%20up%20the%20length%20of%20CoT%0Asequences%20during%20inference%20could%20further%20boost%20LLM%20reasoning%20performance.%0AHowever%2C%20due%20to%20the%20autoregressive%20nature%20of%20LLM%20decoding%2C%20longer%20CoT%20outputs%0Alead%20to%20a%20linear%20increase%20in%20inference%20latency%2C%20adversely%20affecting%20user%0Aexperience%2C%20particularly%20when%20the%20CoT%20exceeds%2010%2C000%20tokens.%20To%20address%20this%0Alimitation%2C%20we%20analyze%20the%20semantic%20importance%20of%20tokens%20within%20CoT%20outputs%20and%0Areveal%20that%20their%20contributions%20to%20reasoning%20vary.%20Building%20on%20this%20insight%2C%20we%0Apropose%20TokenSkip%2C%20a%20simple%20yet%20effective%20approach%20that%20enables%20LLMs%20to%0Aselectively%20skip%20less%20important%20tokens%2C%20allowing%20for%20controllable%20CoT%0Acompression.%20Extensive%20experiments%20across%20various%20models%20and%20tasks%20demonstrate%0Athe%20effectiveness%20of%20TokenSkip%20in%20reducing%20CoT%20token%20usage%20while%20preserving%0Astrong%20reasoning%20performance.%20Notably%2C%20when%20applied%20to%20Qwen2.5-14B-Instruct%2C%0ATokenSkip%20reduces%20reasoning%20tokens%20by%2040%25%20%28from%20313%20to%20181%29%20on%20GSM8K%2C%20with%20less%0Athan%20a%200.4%25%20performance%20drop.%20We%20release%20our%20code%20and%20checkpoints%20in%0Ahttps%3A//github.com/hemingkx/TokenSkip.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.12067v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTokenSkip%253A%2520Controllable%2520Chain-of-Thought%2520Compression%2520in%2520LLMs%26entry.906535625%3DHeming%2520Xia%2520and%2520Chak%2520Tou%2520Leong%2520and%2520Wenjie%2520Wang%2520and%2520Yongqi%2520Li%2520and%2520Wenjie%2520Li%26entry.1292438233%3D%2520%2520Chain-of-Thought%2520%2528CoT%2529%2520has%2520been%2520proven%2520effective%2520in%2520enhancing%2520the%2520reasoning%250Acapabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Recent%2520advancements%252C%2520such%2520as%250AOpenAI%2527s%2520o1%2520and%2520DeepSeek-R1%252C%2520suggest%2520that%2520scaling%2520up%2520the%2520length%2520of%2520CoT%250Asequences%2520during%2520inference%2520could%2520further%2520boost%2520LLM%2520reasoning%2520performance.%250AHowever%252C%2520due%2520to%2520the%2520autoregressive%2520nature%2520of%2520LLM%2520decoding%252C%2520longer%2520CoT%2520outputs%250Alead%2520to%2520a%2520linear%2520increase%2520in%2520inference%2520latency%252C%2520adversely%2520affecting%2520user%250Aexperience%252C%2520particularly%2520when%2520the%2520CoT%2520exceeds%252010%252C000%2520tokens.%2520To%2520address%2520this%250Alimitation%252C%2520we%2520analyze%2520the%2520semantic%2520importance%2520of%2520tokens%2520within%2520CoT%2520outputs%2520and%250Areveal%2520that%2520their%2520contributions%2520to%2520reasoning%2520vary.%2520Building%2520on%2520this%2520insight%252C%2520we%250Apropose%2520TokenSkip%252C%2520a%2520simple%2520yet%2520effective%2520approach%2520that%2520enables%2520LLMs%2520to%250Aselectively%2520skip%2520less%2520important%2520tokens%252C%2520allowing%2520for%2520controllable%2520CoT%250Acompression.%2520Extensive%2520experiments%2520across%2520various%2520models%2520and%2520tasks%2520demonstrate%250Athe%2520effectiveness%2520of%2520TokenSkip%2520in%2520reducing%2520CoT%2520token%2520usage%2520while%2520preserving%250Astrong%2520reasoning%2520performance.%2520Notably%252C%2520when%2520applied%2520to%2520Qwen2.5-14B-Instruct%252C%250ATokenSkip%2520reduces%2520reasoning%2520tokens%2520by%252040%2525%2520%2528from%2520313%2520to%2520181%2529%2520on%2520GSM8K%252C%2520with%2520less%250Athan%2520a%25200.4%2525%2520performance%2520drop.%2520We%2520release%2520our%2520code%2520and%2520checkpoints%2520in%250Ahttps%253A//github.com/hemingkx/TokenSkip.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12067v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TokenSkip%3A%20Controllable%20Chain-of-Thought%20Compression%20in%20LLMs&entry.906535625=Heming%20Xia%20and%20Chak%20Tou%20Leong%20and%20Wenjie%20Wang%20and%20Yongqi%20Li%20and%20Wenjie%20Li&entry.1292438233=%20%20Chain-of-Thought%20%28CoT%29%20has%20been%20proven%20effective%20in%20enhancing%20the%20reasoning%0Acapabilities%20of%20large%20language%20models%20%28LLMs%29.%20Recent%20advancements%2C%20such%20as%0AOpenAI%27s%20o1%20and%20DeepSeek-R1%2C%20suggest%20that%20scaling%20up%20the%20length%20of%20CoT%0Asequences%20during%20inference%20could%20further%20boost%20LLM%20reasoning%20performance.%0AHowever%2C%20due%20to%20the%20autoregressive%20nature%20of%20LLM%20decoding%2C%20longer%20CoT%20outputs%0Alead%20to%20a%20linear%20increase%20in%20inference%20latency%2C%20adversely%20affecting%20user%0Aexperience%2C%20particularly%20when%20the%20CoT%20exceeds%2010%2C000%20tokens.%20To%20address%20this%0Alimitation%2C%20we%20analyze%20the%20semantic%20importance%20of%20tokens%20within%20CoT%20outputs%20and%0Areveal%20that%20their%20contributions%20to%20reasoning%20vary.%20Building%20on%20this%20insight%2C%20we%0Apropose%20TokenSkip%2C%20a%20simple%20yet%20effective%20approach%20that%20enables%20LLMs%20to%0Aselectively%20skip%20less%20important%20tokens%2C%20allowing%20for%20controllable%20CoT%0Acompression.%20Extensive%20experiments%20across%20various%20models%20and%20tasks%20demonstrate%0Athe%20effectiveness%20of%20TokenSkip%20in%20reducing%20CoT%20token%20usage%20while%20preserving%0Astrong%20reasoning%20performance.%20Notably%2C%20when%20applied%20to%20Qwen2.5-14B-Instruct%2C%0ATokenSkip%20reduces%20reasoning%20tokens%20by%2040%25%20%28from%20313%20to%20181%29%20on%20GSM8K%2C%20with%20less%0Athan%20a%200.4%25%20performance%20drop.%20We%20release%20our%20code%20and%20checkpoints%20in%0Ahttps%3A//github.com/hemingkx/TokenSkip.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.12067v3&entry.124074799=Read"},
{"title": "Data-driven Methods of Extracting Text Structure and Information\n  Transfer", "author": "Shinichi Honna and Taichi Murayama and Akira Matsui", "abstract": "  The Anna Karenina Principle (AKP) holds that success requires satisfying a\nsmall set of essential conditions, whereas failure takes diverse forms. We test\nAKP, its reverse, and two further patterns described as ordered and noisy\nacross novels, online encyclopedias, research papers, and movies. Texts are\nrepresented as sequences of functional blocks, and convergence is assessed in\ntransition order and position. Results show that structural principles vary by\nmedium: novels follow reverse AKP in order, Wikipedia combines AKP with ordered\npatterns, academic papers display reverse AKP in order but remain noisy in\nposition, and movies diverge by genre. Success therefore depends on structural\nconstraints that are specific to each medium, while failure assumes different\nshapes across domains.\n", "link": "http://arxiv.org/abs/2509.12999v1", "date": "2025-09-16", "relevancy": 2.4576, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4925}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.491}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-driven%20Methods%20of%20Extracting%20Text%20Structure%20and%20Information%0A%20%20Transfer&body=Title%3A%20Data-driven%20Methods%20of%20Extracting%20Text%20Structure%20and%20Information%0A%20%20Transfer%0AAuthor%3A%20Shinichi%20Honna%20and%20Taichi%20Murayama%20and%20Akira%20Matsui%0AAbstract%3A%20%20%20The%20Anna%20Karenina%20Principle%20%28AKP%29%20holds%20that%20success%20requires%20satisfying%20a%0Asmall%20set%20of%20essential%20conditions%2C%20whereas%20failure%20takes%20diverse%20forms.%20We%20test%0AAKP%2C%20its%20reverse%2C%20and%20two%20further%20patterns%20described%20as%20ordered%20and%20noisy%0Aacross%20novels%2C%20online%20encyclopedias%2C%20research%20papers%2C%20and%20movies.%20Texts%20are%0Arepresented%20as%20sequences%20of%20functional%20blocks%2C%20and%20convergence%20is%20assessed%20in%0Atransition%20order%20and%20position.%20Results%20show%20that%20structural%20principles%20vary%20by%0Amedium%3A%20novels%20follow%20reverse%20AKP%20in%20order%2C%20Wikipedia%20combines%20AKP%20with%20ordered%0Apatterns%2C%20academic%20papers%20display%20reverse%20AKP%20in%20order%20but%20remain%20noisy%20in%0Aposition%2C%20and%20movies%20diverge%20by%20genre.%20Success%20therefore%20depends%20on%20structural%0Aconstraints%20that%20are%20specific%20to%20each%20medium%2C%20while%20failure%20assumes%20different%0Ashapes%20across%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-driven%2520Methods%2520of%2520Extracting%2520Text%2520Structure%2520and%2520Information%250A%2520%2520Transfer%26entry.906535625%3DShinichi%2520Honna%2520and%2520Taichi%2520Murayama%2520and%2520Akira%2520Matsui%26entry.1292438233%3D%2520%2520The%2520Anna%2520Karenina%2520Principle%2520%2528AKP%2529%2520holds%2520that%2520success%2520requires%2520satisfying%2520a%250Asmall%2520set%2520of%2520essential%2520conditions%252C%2520whereas%2520failure%2520takes%2520diverse%2520forms.%2520We%2520test%250AAKP%252C%2520its%2520reverse%252C%2520and%2520two%2520further%2520patterns%2520described%2520as%2520ordered%2520and%2520noisy%250Aacross%2520novels%252C%2520online%2520encyclopedias%252C%2520research%2520papers%252C%2520and%2520movies.%2520Texts%2520are%250Arepresented%2520as%2520sequences%2520of%2520functional%2520blocks%252C%2520and%2520convergence%2520is%2520assessed%2520in%250Atransition%2520order%2520and%2520position.%2520Results%2520show%2520that%2520structural%2520principles%2520vary%2520by%250Amedium%253A%2520novels%2520follow%2520reverse%2520AKP%2520in%2520order%252C%2520Wikipedia%2520combines%2520AKP%2520with%2520ordered%250Apatterns%252C%2520academic%2520papers%2520display%2520reverse%2520AKP%2520in%2520order%2520but%2520remain%2520noisy%2520in%250Aposition%252C%2520and%2520movies%2520diverge%2520by%2520genre.%2520Success%2520therefore%2520depends%2520on%2520structural%250Aconstraints%2520that%2520are%2520specific%2520to%2520each%2520medium%252C%2520while%2520failure%2520assumes%2520different%250Ashapes%2520across%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-driven%20Methods%20of%20Extracting%20Text%20Structure%20and%20Information%0A%20%20Transfer&entry.906535625=Shinichi%20Honna%20and%20Taichi%20Murayama%20and%20Akira%20Matsui&entry.1292438233=%20%20The%20Anna%20Karenina%20Principle%20%28AKP%29%20holds%20that%20success%20requires%20satisfying%20a%0Asmall%20set%20of%20essential%20conditions%2C%20whereas%20failure%20takes%20diverse%20forms.%20We%20test%0AAKP%2C%20its%20reverse%2C%20and%20two%20further%20patterns%20described%20as%20ordered%20and%20noisy%0Aacross%20novels%2C%20online%20encyclopedias%2C%20research%20papers%2C%20and%20movies.%20Texts%20are%0Arepresented%20as%20sequences%20of%20functional%20blocks%2C%20and%20convergence%20is%20assessed%20in%0Atransition%20order%20and%20position.%20Results%20show%20that%20structural%20principles%20vary%20by%0Amedium%3A%20novels%20follow%20reverse%20AKP%20in%20order%2C%20Wikipedia%20combines%20AKP%20with%20ordered%0Apatterns%2C%20academic%20papers%20display%20reverse%20AKP%20in%20order%20but%20remain%20noisy%20in%0Aposition%2C%20and%20movies%20diverge%20by%20genre.%20Success%20therefore%20depends%20on%20structural%0Aconstraints%20that%20are%20specific%20to%20each%20medium%2C%20while%20failure%20assumes%20different%0Ashapes%20across%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12999v1&entry.124074799=Read"},
{"title": "Single-stream Policy Optimization", "author": "Zhongwen Xu and Zihan Ding", "abstract": "  We revisit policy-gradient optimization for Large Language Models (LLMs) from\na single-stream perspective. Prevailing group-based methods like GRPO reduce\nvariance with on-the-fly baselines but suffer from critical flaws: frequent\ndegenerate groups erase learning signals, and synchronization barriers hinder\nscalability. We introduce Single-stream Policy Optimization (SPO), which\neliminates these issues by design. SPO replaces per-group baselines with a\npersistent, KL-adaptive value tracker and normalizes advantages globally across\nthe batch, providing a stable, low-variance learning signal for every sample.\nBeing group-free, SPO enables higher throughput and scales effectively in\nlong-horizon or tool-integrated settings where generation times vary.\nFurthermore, the persistent value tracker naturally enables an adaptive\ncurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO\nconverges more smoothly and attains higher accuracy than GRPO, while\neliminating computation wasted on degenerate groups. Ablation studies confirm\nthat SPO's gains stem from its principled approach to baseline estimation and\nadvantage normalization, offering a more robust and efficient path for LLM\nreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the\naverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial\nabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,\n+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain\nin pass@$k$ across the evaluated $k$ values. SPO's success challenges the\nprevailing trend of adding incidental complexity to RL algorithms, highlighting\na path where fundamental principles, not architectural workarounds, drive the\nnext wave of progress in LLM reasoning.\n", "link": "http://arxiv.org/abs/2509.13232v1", "date": "2025-09-16", "relevancy": 2.4336, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4895}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Single-stream%20Policy%20Optimization&body=Title%3A%20Single-stream%20Policy%20Optimization%0AAuthor%3A%20Zhongwen%20Xu%20and%20Zihan%20Ding%0AAbstract%3A%20%20%20We%20revisit%20policy-gradient%20optimization%20for%20Large%20Language%20Models%20%28LLMs%29%20from%0Aa%20single-stream%20perspective.%20Prevailing%20group-based%20methods%20like%20GRPO%20reduce%0Avariance%20with%20on-the-fly%20baselines%20but%20suffer%20from%20critical%20flaws%3A%20frequent%0Adegenerate%20groups%20erase%20learning%20signals%2C%20and%20synchronization%20barriers%20hinder%0Ascalability.%20We%20introduce%20Single-stream%20Policy%20Optimization%20%28SPO%29%2C%20which%0Aeliminates%20these%20issues%20by%20design.%20SPO%20replaces%20per-group%20baselines%20with%20a%0Apersistent%2C%20KL-adaptive%20value%20tracker%20and%20normalizes%20advantages%20globally%20across%0Athe%20batch%2C%20providing%20a%20stable%2C%20low-variance%20learning%20signal%20for%20every%20sample.%0ABeing%20group-free%2C%20SPO%20enables%20higher%20throughput%20and%20scales%20effectively%20in%0Along-horizon%20or%20tool-integrated%20settings%20where%20generation%20times%20vary.%0AFurthermore%2C%20the%20persistent%20value%20tracker%20naturally%20enables%20an%20adaptive%0Acurriculum%20via%20prioritized%20sampling.%20Experiments%20using%20Qwen3-8B%20show%20that%20SPO%0Aconverges%20more%20smoothly%20and%20attains%20higher%20accuracy%20than%20GRPO%2C%20while%0Aeliminating%20computation%20wasted%20on%20degenerate%20groups.%20Ablation%20studies%20confirm%0Athat%20SPO%27s%20gains%20stem%20from%20its%20principled%20approach%20to%20baseline%20estimation%20and%0Aadvantage%20normalization%2C%20offering%20a%20more%20robust%20and%20efficient%20path%20for%20LLM%0Areasoning.%20Across%20five%20hard%20math%20benchmarks%20with%20Qwen3%208B%2C%20SPO%20improves%20the%0Aaverage%20maj%4032%20by%20%2B3.4%20percentage%20points%20%28pp%29%20over%20GRPO%2C%20driven%20by%20substantial%0Aabsolute%20point%20gains%20on%20challenging%20datasets%2C%20including%20%2B7.3%20pp%20on%20BRUMO%2025%2C%0A%2B4.4%20pp%20on%20AIME%2025%2C%20%2B3.3%20pp%20on%20HMMT%2025%2C%20and%20achieves%20consistent%20relative%20gain%0Ain%20pass%40%24k%24%20across%20the%20evaluated%20%24k%24%20values.%20SPO%27s%20success%20challenges%20the%0Aprevailing%20trend%20of%20adding%20incidental%20complexity%20to%20RL%20algorithms%2C%20highlighting%0Aa%20path%20where%20fundamental%20principles%2C%20not%20architectural%20workarounds%2C%20drive%20the%0Anext%20wave%20of%20progress%20in%20LLM%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSingle-stream%2520Policy%2520Optimization%26entry.906535625%3DZhongwen%2520Xu%2520and%2520Zihan%2520Ding%26entry.1292438233%3D%2520%2520We%2520revisit%2520policy-gradient%2520optimization%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520from%250Aa%2520single-stream%2520perspective.%2520Prevailing%2520group-based%2520methods%2520like%2520GRPO%2520reduce%250Avariance%2520with%2520on-the-fly%2520baselines%2520but%2520suffer%2520from%2520critical%2520flaws%253A%2520frequent%250Adegenerate%2520groups%2520erase%2520learning%2520signals%252C%2520and%2520synchronization%2520barriers%2520hinder%250Ascalability.%2520We%2520introduce%2520Single-stream%2520Policy%2520Optimization%2520%2528SPO%2529%252C%2520which%250Aeliminates%2520these%2520issues%2520by%2520design.%2520SPO%2520replaces%2520per-group%2520baselines%2520with%2520a%250Apersistent%252C%2520KL-adaptive%2520value%2520tracker%2520and%2520normalizes%2520advantages%2520globally%2520across%250Athe%2520batch%252C%2520providing%2520a%2520stable%252C%2520low-variance%2520learning%2520signal%2520for%2520every%2520sample.%250ABeing%2520group-free%252C%2520SPO%2520enables%2520higher%2520throughput%2520and%2520scales%2520effectively%2520in%250Along-horizon%2520or%2520tool-integrated%2520settings%2520where%2520generation%2520times%2520vary.%250AFurthermore%252C%2520the%2520persistent%2520value%2520tracker%2520naturally%2520enables%2520an%2520adaptive%250Acurriculum%2520via%2520prioritized%2520sampling.%2520Experiments%2520using%2520Qwen3-8B%2520show%2520that%2520SPO%250Aconverges%2520more%2520smoothly%2520and%2520attains%2520higher%2520accuracy%2520than%2520GRPO%252C%2520while%250Aeliminating%2520computation%2520wasted%2520on%2520degenerate%2520groups.%2520Ablation%2520studies%2520confirm%250Athat%2520SPO%2527s%2520gains%2520stem%2520from%2520its%2520principled%2520approach%2520to%2520baseline%2520estimation%2520and%250Aadvantage%2520normalization%252C%2520offering%2520a%2520more%2520robust%2520and%2520efficient%2520path%2520for%2520LLM%250Areasoning.%2520Across%2520five%2520hard%2520math%2520benchmarks%2520with%2520Qwen3%25208B%252C%2520SPO%2520improves%2520the%250Aaverage%2520maj%254032%2520by%2520%252B3.4%2520percentage%2520points%2520%2528pp%2529%2520over%2520GRPO%252C%2520driven%2520by%2520substantial%250Aabsolute%2520point%2520gains%2520on%2520challenging%2520datasets%252C%2520including%2520%252B7.3%2520pp%2520on%2520BRUMO%252025%252C%250A%252B4.4%2520pp%2520on%2520AIME%252025%252C%2520%252B3.3%2520pp%2520on%2520HMMT%252025%252C%2520and%2520achieves%2520consistent%2520relative%2520gain%250Ain%2520pass%2540%2524k%2524%2520across%2520the%2520evaluated%2520%2524k%2524%2520values.%2520SPO%2527s%2520success%2520challenges%2520the%250Aprevailing%2520trend%2520of%2520adding%2520incidental%2520complexity%2520to%2520RL%2520algorithms%252C%2520highlighting%250Aa%2520path%2520where%2520fundamental%2520principles%252C%2520not%2520architectural%2520workarounds%252C%2520drive%2520the%250Anext%2520wave%2520of%2520progress%2520in%2520LLM%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single-stream%20Policy%20Optimization&entry.906535625=Zhongwen%20Xu%20and%20Zihan%20Ding&entry.1292438233=%20%20We%20revisit%20policy-gradient%20optimization%20for%20Large%20Language%20Models%20%28LLMs%29%20from%0Aa%20single-stream%20perspective.%20Prevailing%20group-based%20methods%20like%20GRPO%20reduce%0Avariance%20with%20on-the-fly%20baselines%20but%20suffer%20from%20critical%20flaws%3A%20frequent%0Adegenerate%20groups%20erase%20learning%20signals%2C%20and%20synchronization%20barriers%20hinder%0Ascalability.%20We%20introduce%20Single-stream%20Policy%20Optimization%20%28SPO%29%2C%20which%0Aeliminates%20these%20issues%20by%20design.%20SPO%20replaces%20per-group%20baselines%20with%20a%0Apersistent%2C%20KL-adaptive%20value%20tracker%20and%20normalizes%20advantages%20globally%20across%0Athe%20batch%2C%20providing%20a%20stable%2C%20low-variance%20learning%20signal%20for%20every%20sample.%0ABeing%20group-free%2C%20SPO%20enables%20higher%20throughput%20and%20scales%20effectively%20in%0Along-horizon%20or%20tool-integrated%20settings%20where%20generation%20times%20vary.%0AFurthermore%2C%20the%20persistent%20value%20tracker%20naturally%20enables%20an%20adaptive%0Acurriculum%20via%20prioritized%20sampling.%20Experiments%20using%20Qwen3-8B%20show%20that%20SPO%0Aconverges%20more%20smoothly%20and%20attains%20higher%20accuracy%20than%20GRPO%2C%20while%0Aeliminating%20computation%20wasted%20on%20degenerate%20groups.%20Ablation%20studies%20confirm%0Athat%20SPO%27s%20gains%20stem%20from%20its%20principled%20approach%20to%20baseline%20estimation%20and%0Aadvantage%20normalization%2C%20offering%20a%20more%20robust%20and%20efficient%20path%20for%20LLM%0Areasoning.%20Across%20five%20hard%20math%20benchmarks%20with%20Qwen3%208B%2C%20SPO%20improves%20the%0Aaverage%20maj%4032%20by%20%2B3.4%20percentage%20points%20%28pp%29%20over%20GRPO%2C%20driven%20by%20substantial%0Aabsolute%20point%20gains%20on%20challenging%20datasets%2C%20including%20%2B7.3%20pp%20on%20BRUMO%2025%2C%0A%2B4.4%20pp%20on%20AIME%2025%2C%20%2B3.3%20pp%20on%20HMMT%2025%2C%20and%20achieves%20consistent%20relative%20gain%0Ain%20pass%40%24k%24%20across%20the%20evaluated%20%24k%24%20values.%20SPO%27s%20success%20challenges%20the%0Aprevailing%20trend%20of%20adding%20incidental%20complexity%20to%20RL%20algorithms%2C%20highlighting%0Aa%20path%20where%20fundamental%20principles%2C%20not%20architectural%20workarounds%2C%20drive%20the%0Anext%20wave%20of%20progress%20in%20LLM%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13232v1&entry.124074799=Read"},
{"title": "Evaluating the Robustness of Open-Source Vision-Language Models to\n  Domain Shift in Object Captioning", "author": "Federico Tavella and Amber Drinkwater and Angelo Cangelosi", "abstract": "  Vision-Language Models (VLMs) have emerged as powerful tools for generating\ntextual descriptions from visual data. While these models excel on web-scale\ndatasets, their robustness to the domain shifts inherent in many real-world\napplications remains under-explored. This paper presents a systematic\nevaluation of VLM performance on a single-view object captioning task when\nfaced with a controlled, physical domain shift. We compare captioning accuracy\nacross two distinct object sets: a collection of multi-material, real-world\ntools and a set of single-material, 3D-printed items. The 3D-printed set\nintroduces a significant domain shift in texture and material properties,\nchallenging the models' generalization capabilities. Our quantitative results\ndemonstrate that all tested VLMs show a marked performance degradation when\ndescribing the 3D-printed objects compared to the real-world tools. This\nunderscores a critical limitation in the ability of current models to\ngeneralize beyond surface-level features and highlights the need for more\nrobust architectures for real-world signal processing applications.\n", "link": "http://arxiv.org/abs/2506.19579v2", "date": "2025-09-16", "relevancy": 2.4282, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6182}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6182}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20the%20Robustness%20of%20Open-Source%20Vision-Language%20Models%20to%0A%20%20Domain%20Shift%20in%20Object%20Captioning&body=Title%3A%20Evaluating%20the%20Robustness%20of%20Open-Source%20Vision-Language%20Models%20to%0A%20%20Domain%20Shift%20in%20Object%20Captioning%0AAuthor%3A%20Federico%20Tavella%20and%20Amber%20Drinkwater%20and%20Angelo%20Cangelosi%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20have%20emerged%20as%20powerful%20tools%20for%20generating%0Atextual%20descriptions%20from%20visual%20data.%20While%20these%20models%20excel%20on%20web-scale%0Adatasets%2C%20their%20robustness%20to%20the%20domain%20shifts%20inherent%20in%20many%20real-world%0Aapplications%20remains%20under-explored.%20This%20paper%20presents%20a%20systematic%0Aevaluation%20of%20VLM%20performance%20on%20a%20single-view%20object%20captioning%20task%20when%0Afaced%20with%20a%20controlled%2C%20physical%20domain%20shift.%20We%20compare%20captioning%20accuracy%0Aacross%20two%20distinct%20object%20sets%3A%20a%20collection%20of%20multi-material%2C%20real-world%0Atools%20and%20a%20set%20of%20single-material%2C%203D-printed%20items.%20The%203D-printed%20set%0Aintroduces%20a%20significant%20domain%20shift%20in%20texture%20and%20material%20properties%2C%0Achallenging%20the%20models%27%20generalization%20capabilities.%20Our%20quantitative%20results%0Ademonstrate%20that%20all%20tested%20VLMs%20show%20a%20marked%20performance%20degradation%20when%0Adescribing%20the%203D-printed%20objects%20compared%20to%20the%20real-world%20tools.%20This%0Aunderscores%20a%20critical%20limitation%20in%20the%20ability%20of%20current%20models%20to%0Ageneralize%20beyond%20surface-level%20features%20and%20highlights%20the%20need%20for%20more%0Arobust%20architectures%20for%20real-world%20signal%20processing%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.19579v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520the%2520Robustness%2520of%2520Open-Source%2520Vision-Language%2520Models%2520to%250A%2520%2520Domain%2520Shift%2520in%2520Object%2520Captioning%26entry.906535625%3DFederico%2520Tavella%2520and%2520Amber%2520Drinkwater%2520and%2520Angelo%2520Cangelosi%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520emerged%2520as%2520powerful%2520tools%2520for%2520generating%250Atextual%2520descriptions%2520from%2520visual%2520data.%2520While%2520these%2520models%2520excel%2520on%2520web-scale%250Adatasets%252C%2520their%2520robustness%2520to%2520the%2520domain%2520shifts%2520inherent%2520in%2520many%2520real-world%250Aapplications%2520remains%2520under-explored.%2520This%2520paper%2520presents%2520a%2520systematic%250Aevaluation%2520of%2520VLM%2520performance%2520on%2520a%2520single-view%2520object%2520captioning%2520task%2520when%250Afaced%2520with%2520a%2520controlled%252C%2520physical%2520domain%2520shift.%2520We%2520compare%2520captioning%2520accuracy%250Aacross%2520two%2520distinct%2520object%2520sets%253A%2520a%2520collection%2520of%2520multi-material%252C%2520real-world%250Atools%2520and%2520a%2520set%2520of%2520single-material%252C%25203D-printed%2520items.%2520The%25203D-printed%2520set%250Aintroduces%2520a%2520significant%2520domain%2520shift%2520in%2520texture%2520and%2520material%2520properties%252C%250Achallenging%2520the%2520models%2527%2520generalization%2520capabilities.%2520Our%2520quantitative%2520results%250Ademonstrate%2520that%2520all%2520tested%2520VLMs%2520show%2520a%2520marked%2520performance%2520degradation%2520when%250Adescribing%2520the%25203D-printed%2520objects%2520compared%2520to%2520the%2520real-world%2520tools.%2520This%250Aunderscores%2520a%2520critical%2520limitation%2520in%2520the%2520ability%2520of%2520current%2520models%2520to%250Ageneralize%2520beyond%2520surface-level%2520features%2520and%2520highlights%2520the%2520need%2520for%2520more%250Arobust%2520architectures%2520for%2520real-world%2520signal%2520processing%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19579v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20the%20Robustness%20of%20Open-Source%20Vision-Language%20Models%20to%0A%20%20Domain%20Shift%20in%20Object%20Captioning&entry.906535625=Federico%20Tavella%20and%20Amber%20Drinkwater%20and%20Angelo%20Cangelosi&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20have%20emerged%20as%20powerful%20tools%20for%20generating%0Atextual%20descriptions%20from%20visual%20data.%20While%20these%20models%20excel%20on%20web-scale%0Adatasets%2C%20their%20robustness%20to%20the%20domain%20shifts%20inherent%20in%20many%20real-world%0Aapplications%20remains%20under-explored.%20This%20paper%20presents%20a%20systematic%0Aevaluation%20of%20VLM%20performance%20on%20a%20single-view%20object%20captioning%20task%20when%0Afaced%20with%20a%20controlled%2C%20physical%20domain%20shift.%20We%20compare%20captioning%20accuracy%0Aacross%20two%20distinct%20object%20sets%3A%20a%20collection%20of%20multi-material%2C%20real-world%0Atools%20and%20a%20set%20of%20single-material%2C%203D-printed%20items.%20The%203D-printed%20set%0Aintroduces%20a%20significant%20domain%20shift%20in%20texture%20and%20material%20properties%2C%0Achallenging%20the%20models%27%20generalization%20capabilities.%20Our%20quantitative%20results%0Ademonstrate%20that%20all%20tested%20VLMs%20show%20a%20marked%20performance%20degradation%20when%0Adescribing%20the%203D-printed%20objects%20compared%20to%20the%20real-world%20tools.%20This%0Aunderscores%20a%20critical%20limitation%20in%20the%20ability%20of%20current%20models%20to%0Ageneralize%20beyond%20surface-level%20features%20and%20highlights%20the%20need%20for%20more%0Arobust%20architectures%20for%20real-world%20signal%20processing%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.19579v2&entry.124074799=Read"},
{"title": "TeraSim-World: Worldwide Safety-Critical Data Synthesis for End-to-End\n  Autonomous Driving", "author": "Jiawei Wang and Haowei Sun and Xintao Yan and Shuo Feng and Jun Gao and Henry X. Liu", "abstract": "  Safe and scalable deployment of end-to-end (E2E) autonomous driving requires\nextensive and diverse data, particularly safety-critical events. Existing data\nare mostly generated from simulators with a significant sim-to-real gap or\ncollected from on-road testing that is costly and unsafe. This paper presents\nTeraSim-World, an automated pipeline that synthesizes realistic and\ngeographically diverse safety-critical data for E2E autonomous driving at\nanywhere in the world. Starting from an arbitrary location, TeraSim-World\nretrieves real-world maps and traffic demand from geospatial data sources.\nThen, it simulates agent behaviors from naturalistic driving datasets, and\norchestrates diverse adversities to create corner cases. Informed by street\nviews of the same location, it achieves photorealistic, geographically grounded\nsensor rendering via the frontier video generation model Cosmos-Drive. By\nbridging agent and sensor simulations, TeraSim-World provides a scalable and\ncritical~data synthesis framework for training and evaluation of E2E autonomous\ndriving systems.\n", "link": "http://arxiv.org/abs/2509.13164v1", "date": "2025-09-16", "relevancy": 2.4255, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.486}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.486}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TeraSim-World%3A%20Worldwide%20Safety-Critical%20Data%20Synthesis%20for%20End-to-End%0A%20%20Autonomous%20Driving&body=Title%3A%20TeraSim-World%3A%20Worldwide%20Safety-Critical%20Data%20Synthesis%20for%20End-to-End%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Jiawei%20Wang%20and%20Haowei%20Sun%20and%20Xintao%20Yan%20and%20Shuo%20Feng%20and%20Jun%20Gao%20and%20Henry%20X.%20Liu%0AAbstract%3A%20%20%20Safe%20and%20scalable%20deployment%20of%20end-to-end%20%28E2E%29%20autonomous%20driving%20requires%0Aextensive%20and%20diverse%20data%2C%20particularly%20safety-critical%20events.%20Existing%20data%0Aare%20mostly%20generated%20from%20simulators%20with%20a%20significant%20sim-to-real%20gap%20or%0Acollected%20from%20on-road%20testing%20that%20is%20costly%20and%20unsafe.%20This%20paper%20presents%0ATeraSim-World%2C%20an%20automated%20pipeline%20that%20synthesizes%20realistic%20and%0Ageographically%20diverse%20safety-critical%20data%20for%20E2E%20autonomous%20driving%20at%0Aanywhere%20in%20the%20world.%20Starting%20from%20an%20arbitrary%20location%2C%20TeraSim-World%0Aretrieves%20real-world%20maps%20and%20traffic%20demand%20from%20geospatial%20data%20sources.%0AThen%2C%20it%20simulates%20agent%20behaviors%20from%20naturalistic%20driving%20datasets%2C%20and%0Aorchestrates%20diverse%20adversities%20to%20create%20corner%20cases.%20Informed%20by%20street%0Aviews%20of%20the%20same%20location%2C%20it%20achieves%20photorealistic%2C%20geographically%20grounded%0Asensor%20rendering%20via%20the%20frontier%20video%20generation%20model%20Cosmos-Drive.%20By%0Abridging%20agent%20and%20sensor%20simulations%2C%20TeraSim-World%20provides%20a%20scalable%20and%0Acritical~data%20synthesis%20framework%20for%20training%20and%20evaluation%20of%20E2E%20autonomous%0Adriving%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13164v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTeraSim-World%253A%2520Worldwide%2520Safety-Critical%2520Data%2520Synthesis%2520for%2520End-to-End%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DJiawei%2520Wang%2520and%2520Haowei%2520Sun%2520and%2520Xintao%2520Yan%2520and%2520Shuo%2520Feng%2520and%2520Jun%2520Gao%2520and%2520Henry%2520X.%2520Liu%26entry.1292438233%3D%2520%2520Safe%2520and%2520scalable%2520deployment%2520of%2520end-to-end%2520%2528E2E%2529%2520autonomous%2520driving%2520requires%250Aextensive%2520and%2520diverse%2520data%252C%2520particularly%2520safety-critical%2520events.%2520Existing%2520data%250Aare%2520mostly%2520generated%2520from%2520simulators%2520with%2520a%2520significant%2520sim-to-real%2520gap%2520or%250Acollected%2520from%2520on-road%2520testing%2520that%2520is%2520costly%2520and%2520unsafe.%2520This%2520paper%2520presents%250ATeraSim-World%252C%2520an%2520automated%2520pipeline%2520that%2520synthesizes%2520realistic%2520and%250Ageographically%2520diverse%2520safety-critical%2520data%2520for%2520E2E%2520autonomous%2520driving%2520at%250Aanywhere%2520in%2520the%2520world.%2520Starting%2520from%2520an%2520arbitrary%2520location%252C%2520TeraSim-World%250Aretrieves%2520real-world%2520maps%2520and%2520traffic%2520demand%2520from%2520geospatial%2520data%2520sources.%250AThen%252C%2520it%2520simulates%2520agent%2520behaviors%2520from%2520naturalistic%2520driving%2520datasets%252C%2520and%250Aorchestrates%2520diverse%2520adversities%2520to%2520create%2520corner%2520cases.%2520Informed%2520by%2520street%250Aviews%2520of%2520the%2520same%2520location%252C%2520it%2520achieves%2520photorealistic%252C%2520geographically%2520grounded%250Asensor%2520rendering%2520via%2520the%2520frontier%2520video%2520generation%2520model%2520Cosmos-Drive.%2520By%250Abridging%2520agent%2520and%2520sensor%2520simulations%252C%2520TeraSim-World%2520provides%2520a%2520scalable%2520and%250Acritical~data%2520synthesis%2520framework%2520for%2520training%2520and%2520evaluation%2520of%2520E2E%2520autonomous%250Adriving%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13164v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TeraSim-World%3A%20Worldwide%20Safety-Critical%20Data%20Synthesis%20for%20End-to-End%0A%20%20Autonomous%20Driving&entry.906535625=Jiawei%20Wang%20and%20Haowei%20Sun%20and%20Xintao%20Yan%20and%20Shuo%20Feng%20and%20Jun%20Gao%20and%20Henry%20X.%20Liu&entry.1292438233=%20%20Safe%20and%20scalable%20deployment%20of%20end-to-end%20%28E2E%29%20autonomous%20driving%20requires%0Aextensive%20and%20diverse%20data%2C%20particularly%20safety-critical%20events.%20Existing%20data%0Aare%20mostly%20generated%20from%20simulators%20with%20a%20significant%20sim-to-real%20gap%20or%0Acollected%20from%20on-road%20testing%20that%20is%20costly%20and%20unsafe.%20This%20paper%20presents%0ATeraSim-World%2C%20an%20automated%20pipeline%20that%20synthesizes%20realistic%20and%0Ageographically%20diverse%20safety-critical%20data%20for%20E2E%20autonomous%20driving%20at%0Aanywhere%20in%20the%20world.%20Starting%20from%20an%20arbitrary%20location%2C%20TeraSim-World%0Aretrieves%20real-world%20maps%20and%20traffic%20demand%20from%20geospatial%20data%20sources.%0AThen%2C%20it%20simulates%20agent%20behaviors%20from%20naturalistic%20driving%20datasets%2C%20and%0Aorchestrates%20diverse%20adversities%20to%20create%20corner%20cases.%20Informed%20by%20street%0Aviews%20of%20the%20same%20location%2C%20it%20achieves%20photorealistic%2C%20geographically%20grounded%0Asensor%20rendering%20via%20the%20frontier%20video%20generation%20model%20Cosmos-Drive.%20By%0Abridging%20agent%20and%20sensor%20simulations%2C%20TeraSim-World%20provides%20a%20scalable%20and%0Acritical~data%20synthesis%20framework%20for%20training%20and%20evaluation%20of%20E2E%20autonomous%0Adriving%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13164v1&entry.124074799=Read"},
{"title": "Learning from Heterophilic Graphs: A Spectral Theory Perspective on the\n  Impact of Self-Loops and Parallel Edges", "author": "Kushal Bose and Swagatam Das", "abstract": "  Graph heterophily poses a formidable challenge to the performance of\nMessage-passing Graph Neural Networks (MP-GNNs). The familiar low-pass filters\nlike Graph Convolutional Networks (GCNs) face performance degradation, which\ncan be attributed to the blending of the messages from dissimilar neighboring\nnodes. The performance of the low-pass filters on heterophilic graphs still\nrequires an in-depth analysis. In this context, we update the heterophilic\ngraphs by adding a number of self-loops and parallel edges. We observe that\neigenvalues of the graph Laplacian decrease and increase respectively by\nincreasing the number of self-loops and parallel edges. We conduct several\nstudies regarding the performance of GCN on various benchmark heterophilic\nnetworks by adding either self-loops or parallel edges. The studies reveal that\nthe GCN exhibited either increasing or decreasing performance trends on adding\nself-loops and parallel edges. In light of the studies, we established\nconnections between the graph spectra and the performance trends of the\nlow-pass filters on the heterophilic graphs. The graph spectra characterize the\nessential intrinsic properties of the input graph like the presence of\nconnected components, sparsity, average degree, cluster structures, etc. Our\nwork is adept at seamlessly evaluating graph spectrum and properties by\nobserving the performance trends of the low-pass filters without pursuing the\ncostly eigenvalue decomposition. The theoretical foundations are also discussed\nto validate the impact of adding self-loops and parallel edges on the graph\nspectrum.\n", "link": "http://arxiv.org/abs/2509.13139v1", "date": "2025-09-16", "relevancy": 2.3966, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4913}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4739}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20from%20Heterophilic%20Graphs%3A%20A%20Spectral%20Theory%20Perspective%20on%20the%0A%20%20Impact%20of%20Self-Loops%20and%20Parallel%20Edges&body=Title%3A%20Learning%20from%20Heterophilic%20Graphs%3A%20A%20Spectral%20Theory%20Perspective%20on%20the%0A%20%20Impact%20of%20Self-Loops%20and%20Parallel%20Edges%0AAuthor%3A%20Kushal%20Bose%20and%20Swagatam%20Das%0AAbstract%3A%20%20%20Graph%20heterophily%20poses%20a%20formidable%20challenge%20to%20the%20performance%20of%0AMessage-passing%20Graph%20Neural%20Networks%20%28MP-GNNs%29.%20The%20familiar%20low-pass%20filters%0Alike%20Graph%20Convolutional%20Networks%20%28GCNs%29%20face%20performance%20degradation%2C%20which%0Acan%20be%20attributed%20to%20the%20blending%20of%20the%20messages%20from%20dissimilar%20neighboring%0Anodes.%20The%20performance%20of%20the%20low-pass%20filters%20on%20heterophilic%20graphs%20still%0Arequires%20an%20in-depth%20analysis.%20In%20this%20context%2C%20we%20update%20the%20heterophilic%0Agraphs%20by%20adding%20a%20number%20of%20self-loops%20and%20parallel%20edges.%20We%20observe%20that%0Aeigenvalues%20of%20the%20graph%20Laplacian%20decrease%20and%20increase%20respectively%20by%0Aincreasing%20the%20number%20of%20self-loops%20and%20parallel%20edges.%20We%20conduct%20several%0Astudies%20regarding%20the%20performance%20of%20GCN%20on%20various%20benchmark%20heterophilic%0Anetworks%20by%20adding%20either%20self-loops%20or%20parallel%20edges.%20The%20studies%20reveal%20that%0Athe%20GCN%20exhibited%20either%20increasing%20or%20decreasing%20performance%20trends%20on%20adding%0Aself-loops%20and%20parallel%20edges.%20In%20light%20of%20the%20studies%2C%20we%20established%0Aconnections%20between%20the%20graph%20spectra%20and%20the%20performance%20trends%20of%20the%0Alow-pass%20filters%20on%20the%20heterophilic%20graphs.%20The%20graph%20spectra%20characterize%20the%0Aessential%20intrinsic%20properties%20of%20the%20input%20graph%20like%20the%20presence%20of%0Aconnected%20components%2C%20sparsity%2C%20average%20degree%2C%20cluster%20structures%2C%20etc.%20Our%0Awork%20is%20adept%20at%20seamlessly%20evaluating%20graph%20spectrum%20and%20properties%20by%0Aobserving%20the%20performance%20trends%20of%20the%20low-pass%20filters%20without%20pursuing%20the%0Acostly%20eigenvalue%20decomposition.%20The%20theoretical%20foundations%20are%20also%20discussed%0Ato%20validate%20the%20impact%20of%20adding%20self-loops%20and%20parallel%20edges%20on%20the%20graph%0Aspectrum.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13139v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520from%2520Heterophilic%2520Graphs%253A%2520A%2520Spectral%2520Theory%2520Perspective%2520on%2520the%250A%2520%2520Impact%2520of%2520Self-Loops%2520and%2520Parallel%2520Edges%26entry.906535625%3DKushal%2520Bose%2520and%2520Swagatam%2520Das%26entry.1292438233%3D%2520%2520Graph%2520heterophily%2520poses%2520a%2520formidable%2520challenge%2520to%2520the%2520performance%2520of%250AMessage-passing%2520Graph%2520Neural%2520Networks%2520%2528MP-GNNs%2529.%2520The%2520familiar%2520low-pass%2520filters%250Alike%2520Graph%2520Convolutional%2520Networks%2520%2528GCNs%2529%2520face%2520performance%2520degradation%252C%2520which%250Acan%2520be%2520attributed%2520to%2520the%2520blending%2520of%2520the%2520messages%2520from%2520dissimilar%2520neighboring%250Anodes.%2520The%2520performance%2520of%2520the%2520low-pass%2520filters%2520on%2520heterophilic%2520graphs%2520still%250Arequires%2520an%2520in-depth%2520analysis.%2520In%2520this%2520context%252C%2520we%2520update%2520the%2520heterophilic%250Agraphs%2520by%2520adding%2520a%2520number%2520of%2520self-loops%2520and%2520parallel%2520edges.%2520We%2520observe%2520that%250Aeigenvalues%2520of%2520the%2520graph%2520Laplacian%2520decrease%2520and%2520increase%2520respectively%2520by%250Aincreasing%2520the%2520number%2520of%2520self-loops%2520and%2520parallel%2520edges.%2520We%2520conduct%2520several%250Astudies%2520regarding%2520the%2520performance%2520of%2520GCN%2520on%2520various%2520benchmark%2520heterophilic%250Anetworks%2520by%2520adding%2520either%2520self-loops%2520or%2520parallel%2520edges.%2520The%2520studies%2520reveal%2520that%250Athe%2520GCN%2520exhibited%2520either%2520increasing%2520or%2520decreasing%2520performance%2520trends%2520on%2520adding%250Aself-loops%2520and%2520parallel%2520edges.%2520In%2520light%2520of%2520the%2520studies%252C%2520we%2520established%250Aconnections%2520between%2520the%2520graph%2520spectra%2520and%2520the%2520performance%2520trends%2520of%2520the%250Alow-pass%2520filters%2520on%2520the%2520heterophilic%2520graphs.%2520The%2520graph%2520spectra%2520characterize%2520the%250Aessential%2520intrinsic%2520properties%2520of%2520the%2520input%2520graph%2520like%2520the%2520presence%2520of%250Aconnected%2520components%252C%2520sparsity%252C%2520average%2520degree%252C%2520cluster%2520structures%252C%2520etc.%2520Our%250Awork%2520is%2520adept%2520at%2520seamlessly%2520evaluating%2520graph%2520spectrum%2520and%2520properties%2520by%250Aobserving%2520the%2520performance%2520trends%2520of%2520the%2520low-pass%2520filters%2520without%2520pursuing%2520the%250Acostly%2520eigenvalue%2520decomposition.%2520The%2520theoretical%2520foundations%2520are%2520also%2520discussed%250Ato%2520validate%2520the%2520impact%2520of%2520adding%2520self-loops%2520and%2520parallel%2520edges%2520on%2520the%2520graph%250Aspectrum.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13139v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20from%20Heterophilic%20Graphs%3A%20A%20Spectral%20Theory%20Perspective%20on%20the%0A%20%20Impact%20of%20Self-Loops%20and%20Parallel%20Edges&entry.906535625=Kushal%20Bose%20and%20Swagatam%20Das&entry.1292438233=%20%20Graph%20heterophily%20poses%20a%20formidable%20challenge%20to%20the%20performance%20of%0AMessage-passing%20Graph%20Neural%20Networks%20%28MP-GNNs%29.%20The%20familiar%20low-pass%20filters%0Alike%20Graph%20Convolutional%20Networks%20%28GCNs%29%20face%20performance%20degradation%2C%20which%0Acan%20be%20attributed%20to%20the%20blending%20of%20the%20messages%20from%20dissimilar%20neighboring%0Anodes.%20The%20performance%20of%20the%20low-pass%20filters%20on%20heterophilic%20graphs%20still%0Arequires%20an%20in-depth%20analysis.%20In%20this%20context%2C%20we%20update%20the%20heterophilic%0Agraphs%20by%20adding%20a%20number%20of%20self-loops%20and%20parallel%20edges.%20We%20observe%20that%0Aeigenvalues%20of%20the%20graph%20Laplacian%20decrease%20and%20increase%20respectively%20by%0Aincreasing%20the%20number%20of%20self-loops%20and%20parallel%20edges.%20We%20conduct%20several%0Astudies%20regarding%20the%20performance%20of%20GCN%20on%20various%20benchmark%20heterophilic%0Anetworks%20by%20adding%20either%20self-loops%20or%20parallel%20edges.%20The%20studies%20reveal%20that%0Athe%20GCN%20exhibited%20either%20increasing%20or%20decreasing%20performance%20trends%20on%20adding%0Aself-loops%20and%20parallel%20edges.%20In%20light%20of%20the%20studies%2C%20we%20established%0Aconnections%20between%20the%20graph%20spectra%20and%20the%20performance%20trends%20of%20the%0Alow-pass%20filters%20on%20the%20heterophilic%20graphs.%20The%20graph%20spectra%20characterize%20the%0Aessential%20intrinsic%20properties%20of%20the%20input%20graph%20like%20the%20presence%20of%0Aconnected%20components%2C%20sparsity%2C%20average%20degree%2C%20cluster%20structures%2C%20etc.%20Our%0Awork%20is%20adept%20at%20seamlessly%20evaluating%20graph%20spectrum%20and%20properties%20by%0Aobserving%20the%20performance%20trends%20of%20the%20low-pass%20filters%20without%20pursuing%20the%0Acostly%20eigenvalue%20decomposition.%20The%20theoretical%20foundations%20are%20also%20discussed%0Ato%20validate%20the%20impact%20of%20adding%20self-loops%20and%20parallel%20edges%20on%20the%20graph%0Aspectrum.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13139v1&entry.124074799=Read"},
{"title": "Search-TTA: A Multimodal Test-Time Adaptation Framework for Visual\n  Search in the Wild", "author": "Derek Ming Siang Tan and  Shailesh and Boyang Liu and Alok Raj and Qi Xuan Ang and Weiheng Dai and Tanishq Duhan and Jimmy Chiun and Yuhong Cao and Florian Shkurti and Guillaume Sartoretti", "abstract": "  To perform outdoor autonomous visual navigation and search, a robot may\nleverage satellite imagery as a prior map. This can help inform high-level\nsearch and exploration strategies, even when such images lack sufficient\nresolution to allow for visual recognition of targets. However, there are\nlimited training datasets of satellite images with annotated targets that are\nnot directly visible. Furthermore, approaches which leverage large Vision\nLanguage Models (VLMs) for generalization may yield inaccurate outputs due to\nhallucination, leading to inefficient search. To address these challenges, we\nintroduce Search-TTA, a multimodal test-time adaptation framework with a\nflexible plug-and-play interface compatible with various input modalities (e.g.\nimage, text, sound) and planning methods. First, we pretrain a satellite image\nencoder to align with CLIP's visual encoder to output probability distributions\nof target presence used for visual search. Second, our framework dynamically\nrefines CLIP's predictions during search using a test-time adaptation\nmechanism. Through a novel feedback loop inspired by Spatial Poisson Point\nProcesses, uncertainty-weighted gradient updates are used to correct\npotentially inaccurate predictions and improve search performance. To train and\nevaluate Search-TTA, we curate AVS-Bench, a visual search dataset based on\ninternet-scale ecological data that contains up to 380k training and 8k\nvalidation images (in- and out-domain). We find that Search-TTA improves\nplanner performance by up to 30.0%, particularly in cases with poor initial\nCLIP predictions due to limited training data. It also performs comparably with\nsignificantly larger VLMs, and achieves zero-shot generalization to unseen\nmodalities. Finally, we deploy Search-TTA on a real UAV via\nhardware-in-the-loop testing, by simulating its operation within a large-scale\nsimulation that provides onboard sensing.\n", "link": "http://arxiv.org/abs/2505.11350v3", "date": "2025-09-16", "relevancy": 2.3964, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6105}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5911}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5907}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Search-TTA%3A%20A%20Multimodal%20Test-Time%20Adaptation%20Framework%20for%20Visual%0A%20%20Search%20in%20the%20Wild&body=Title%3A%20Search-TTA%3A%20A%20Multimodal%20Test-Time%20Adaptation%20Framework%20for%20Visual%0A%20%20Search%20in%20the%20Wild%0AAuthor%3A%20Derek%20Ming%20Siang%20Tan%20and%20%20Shailesh%20and%20Boyang%20Liu%20and%20Alok%20Raj%20and%20Qi%20Xuan%20Ang%20and%20Weiheng%20Dai%20and%20Tanishq%20Duhan%20and%20Jimmy%20Chiun%20and%20Yuhong%20Cao%20and%20Florian%20Shkurti%20and%20Guillaume%20Sartoretti%0AAbstract%3A%20%20%20To%20perform%20outdoor%20autonomous%20visual%20navigation%20and%20search%2C%20a%20robot%20may%0Aleverage%20satellite%20imagery%20as%20a%20prior%20map.%20This%20can%20help%20inform%20high-level%0Asearch%20and%20exploration%20strategies%2C%20even%20when%20such%20images%20lack%20sufficient%0Aresolution%20to%20allow%20for%20visual%20recognition%20of%20targets.%20However%2C%20there%20are%0Alimited%20training%20datasets%20of%20satellite%20images%20with%20annotated%20targets%20that%20are%0Anot%20directly%20visible.%20Furthermore%2C%20approaches%20which%20leverage%20large%20Vision%0ALanguage%20Models%20%28VLMs%29%20for%20generalization%20may%20yield%20inaccurate%20outputs%20due%20to%0Ahallucination%2C%20leading%20to%20inefficient%20search.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20Search-TTA%2C%20a%20multimodal%20test-time%20adaptation%20framework%20with%20a%0Aflexible%20plug-and-play%20interface%20compatible%20with%20various%20input%20modalities%20%28e.g.%0Aimage%2C%20text%2C%20sound%29%20and%20planning%20methods.%20First%2C%20we%20pretrain%20a%20satellite%20image%0Aencoder%20to%20align%20with%20CLIP%27s%20visual%20encoder%20to%20output%20probability%20distributions%0Aof%20target%20presence%20used%20for%20visual%20search.%20Second%2C%20our%20framework%20dynamically%0Arefines%20CLIP%27s%20predictions%20during%20search%20using%20a%20test-time%20adaptation%0Amechanism.%20Through%20a%20novel%20feedback%20loop%20inspired%20by%20Spatial%20Poisson%20Point%0AProcesses%2C%20uncertainty-weighted%20gradient%20updates%20are%20used%20to%20correct%0Apotentially%20inaccurate%20predictions%20and%20improve%20search%20performance.%20To%20train%20and%0Aevaluate%20Search-TTA%2C%20we%20curate%20AVS-Bench%2C%20a%20visual%20search%20dataset%20based%20on%0Ainternet-scale%20ecological%20data%20that%20contains%20up%20to%20380k%20training%20and%208k%0Avalidation%20images%20%28in-%20and%20out-domain%29.%20We%20find%20that%20Search-TTA%20improves%0Aplanner%20performance%20by%20up%20to%2030.0%25%2C%20particularly%20in%20cases%20with%20poor%20initial%0ACLIP%20predictions%20due%20to%20limited%20training%20data.%20It%20also%20performs%20comparably%20with%0Asignificantly%20larger%20VLMs%2C%20and%20achieves%20zero-shot%20generalization%20to%20unseen%0Amodalities.%20Finally%2C%20we%20deploy%20Search-TTA%20on%20a%20real%20UAV%20via%0Ahardware-in-the-loop%20testing%2C%20by%20simulating%20its%20operation%20within%20a%20large-scale%0Asimulation%20that%20provides%20onboard%20sensing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.11350v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSearch-TTA%253A%2520A%2520Multimodal%2520Test-Time%2520Adaptation%2520Framework%2520for%2520Visual%250A%2520%2520Search%2520in%2520the%2520Wild%26entry.906535625%3DDerek%2520Ming%2520Siang%2520Tan%2520and%2520%2520Shailesh%2520and%2520Boyang%2520Liu%2520and%2520Alok%2520Raj%2520and%2520Qi%2520Xuan%2520Ang%2520and%2520Weiheng%2520Dai%2520and%2520Tanishq%2520Duhan%2520and%2520Jimmy%2520Chiun%2520and%2520Yuhong%2520Cao%2520and%2520Florian%2520Shkurti%2520and%2520Guillaume%2520Sartoretti%26entry.1292438233%3D%2520%2520To%2520perform%2520outdoor%2520autonomous%2520visual%2520navigation%2520and%2520search%252C%2520a%2520robot%2520may%250Aleverage%2520satellite%2520imagery%2520as%2520a%2520prior%2520map.%2520This%2520can%2520help%2520inform%2520high-level%250Asearch%2520and%2520exploration%2520strategies%252C%2520even%2520when%2520such%2520images%2520lack%2520sufficient%250Aresolution%2520to%2520allow%2520for%2520visual%2520recognition%2520of%2520targets.%2520However%252C%2520there%2520are%250Alimited%2520training%2520datasets%2520of%2520satellite%2520images%2520with%2520annotated%2520targets%2520that%2520are%250Anot%2520directly%2520visible.%2520Furthermore%252C%2520approaches%2520which%2520leverage%2520large%2520Vision%250ALanguage%2520Models%2520%2528VLMs%2529%2520for%2520generalization%2520may%2520yield%2520inaccurate%2520outputs%2520due%2520to%250Ahallucination%252C%2520leading%2520to%2520inefficient%2520search.%2520To%2520address%2520these%2520challenges%252C%2520we%250Aintroduce%2520Search-TTA%252C%2520a%2520multimodal%2520test-time%2520adaptation%2520framework%2520with%2520a%250Aflexible%2520plug-and-play%2520interface%2520compatible%2520with%2520various%2520input%2520modalities%2520%2528e.g.%250Aimage%252C%2520text%252C%2520sound%2529%2520and%2520planning%2520methods.%2520First%252C%2520we%2520pretrain%2520a%2520satellite%2520image%250Aencoder%2520to%2520align%2520with%2520CLIP%2527s%2520visual%2520encoder%2520to%2520output%2520probability%2520distributions%250Aof%2520target%2520presence%2520used%2520for%2520visual%2520search.%2520Second%252C%2520our%2520framework%2520dynamically%250Arefines%2520CLIP%2527s%2520predictions%2520during%2520search%2520using%2520a%2520test-time%2520adaptation%250Amechanism.%2520Through%2520a%2520novel%2520feedback%2520loop%2520inspired%2520by%2520Spatial%2520Poisson%2520Point%250AProcesses%252C%2520uncertainty-weighted%2520gradient%2520updates%2520are%2520used%2520to%2520correct%250Apotentially%2520inaccurate%2520predictions%2520and%2520improve%2520search%2520performance.%2520To%2520train%2520and%250Aevaluate%2520Search-TTA%252C%2520we%2520curate%2520AVS-Bench%252C%2520a%2520visual%2520search%2520dataset%2520based%2520on%250Ainternet-scale%2520ecological%2520data%2520that%2520contains%2520up%2520to%2520380k%2520training%2520and%25208k%250Avalidation%2520images%2520%2528in-%2520and%2520out-domain%2529.%2520We%2520find%2520that%2520Search-TTA%2520improves%250Aplanner%2520performance%2520by%2520up%2520to%252030.0%2525%252C%2520particularly%2520in%2520cases%2520with%2520poor%2520initial%250ACLIP%2520predictions%2520due%2520to%2520limited%2520training%2520data.%2520It%2520also%2520performs%2520comparably%2520with%250Asignificantly%2520larger%2520VLMs%252C%2520and%2520achieves%2520zero-shot%2520generalization%2520to%2520unseen%250Amodalities.%2520Finally%252C%2520we%2520deploy%2520Search-TTA%2520on%2520a%2520real%2520UAV%2520via%250Ahardware-in-the-loop%2520testing%252C%2520by%2520simulating%2520its%2520operation%2520within%2520a%2520large-scale%250Asimulation%2520that%2520provides%2520onboard%2520sensing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11350v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Search-TTA%3A%20A%20Multimodal%20Test-Time%20Adaptation%20Framework%20for%20Visual%0A%20%20Search%20in%20the%20Wild&entry.906535625=Derek%20Ming%20Siang%20Tan%20and%20%20Shailesh%20and%20Boyang%20Liu%20and%20Alok%20Raj%20and%20Qi%20Xuan%20Ang%20and%20Weiheng%20Dai%20and%20Tanishq%20Duhan%20and%20Jimmy%20Chiun%20and%20Yuhong%20Cao%20and%20Florian%20Shkurti%20and%20Guillaume%20Sartoretti&entry.1292438233=%20%20To%20perform%20outdoor%20autonomous%20visual%20navigation%20and%20search%2C%20a%20robot%20may%0Aleverage%20satellite%20imagery%20as%20a%20prior%20map.%20This%20can%20help%20inform%20high-level%0Asearch%20and%20exploration%20strategies%2C%20even%20when%20such%20images%20lack%20sufficient%0Aresolution%20to%20allow%20for%20visual%20recognition%20of%20targets.%20However%2C%20there%20are%0Alimited%20training%20datasets%20of%20satellite%20images%20with%20annotated%20targets%20that%20are%0Anot%20directly%20visible.%20Furthermore%2C%20approaches%20which%20leverage%20large%20Vision%0ALanguage%20Models%20%28VLMs%29%20for%20generalization%20may%20yield%20inaccurate%20outputs%20due%20to%0Ahallucination%2C%20leading%20to%20inefficient%20search.%20To%20address%20these%20challenges%2C%20we%0Aintroduce%20Search-TTA%2C%20a%20multimodal%20test-time%20adaptation%20framework%20with%20a%0Aflexible%20plug-and-play%20interface%20compatible%20with%20various%20input%20modalities%20%28e.g.%0Aimage%2C%20text%2C%20sound%29%20and%20planning%20methods.%20First%2C%20we%20pretrain%20a%20satellite%20image%0Aencoder%20to%20align%20with%20CLIP%27s%20visual%20encoder%20to%20output%20probability%20distributions%0Aof%20target%20presence%20used%20for%20visual%20search.%20Second%2C%20our%20framework%20dynamically%0Arefines%20CLIP%27s%20predictions%20during%20search%20using%20a%20test-time%20adaptation%0Amechanism.%20Through%20a%20novel%20feedback%20loop%20inspired%20by%20Spatial%20Poisson%20Point%0AProcesses%2C%20uncertainty-weighted%20gradient%20updates%20are%20used%20to%20correct%0Apotentially%20inaccurate%20predictions%20and%20improve%20search%20performance.%20To%20train%20and%0Aevaluate%20Search-TTA%2C%20we%20curate%20AVS-Bench%2C%20a%20visual%20search%20dataset%20based%20on%0Ainternet-scale%20ecological%20data%20that%20contains%20up%20to%20380k%20training%20and%208k%0Avalidation%20images%20%28in-%20and%20out-domain%29.%20We%20find%20that%20Search-TTA%20improves%0Aplanner%20performance%20by%20up%20to%2030.0%25%2C%20particularly%20in%20cases%20with%20poor%20initial%0ACLIP%20predictions%20due%20to%20limited%20training%20data.%20It%20also%20performs%20comparably%20with%0Asignificantly%20larger%20VLMs%2C%20and%20achieves%20zero-shot%20generalization%20to%20unseen%0Amodalities.%20Finally%2C%20we%20deploy%20Search-TTA%20on%20a%20real%20UAV%20via%0Ahardware-in-the-loop%20testing%2C%20by%20simulating%20its%20operation%20within%20a%20large-scale%0Asimulation%20that%20provides%20onboard%20sensing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.11350v3&entry.124074799=Read"},
{"title": "Weakly and Self-Supervised Class-Agnostic Motion Prediction for\n  Autonomous Driving", "author": "Ruibo Li and Hanyu Shi and Zhe Wang and Guosheng Lin", "abstract": "  Understanding motion in dynamic environments is critical for autonomous\ndriving, thereby motivating research on class-agnostic motion prediction. In\nthis work, we investigate weakly and self-supervised class-agnostic motion\nprediction from LiDAR point clouds. Outdoor scenes typically consist of mobile\nforegrounds and static backgrounds, allowing motion understanding to be\nassociated with scene parsing. Based on this observation, we propose a novel\nweakly supervised paradigm that replaces motion annotations with fully or\npartially annotated (1%, 0.1%) foreground/background masks for supervision. To\nthis end, we develop a weakly supervised approach utilizing\nforeground/background cues to guide the self-supervised learning of motion\nprediction models. Since foreground motion generally occurs in non-ground\nregions, non-ground/ground masks can serve as an alternative to\nforeground/background masks, further reducing annotation effort. Leveraging\nnon-ground/ground cues, we propose two additional approaches: a weakly\nsupervised method requiring fewer (0.01%) foreground/background annotations,\nand a self-supervised method without annotations. Furthermore, we design a\nRobust Consistency-aware Chamfer Distance loss that incorporates multi-frame\ninformation and robust penalty functions to suppress outliers in\nself-supervised learning. Experiments show that our weakly and self-supervised\nmodels outperform existing self-supervised counterparts, and our weakly\nsupervised models even rival some supervised ones. This demonstrates that our\napproaches effectively balance annotation effort and performance.\n", "link": "http://arxiv.org/abs/2509.13116v1", "date": "2025-09-16", "relevancy": 2.3941, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6134}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5998}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Weakly%20and%20Self-Supervised%20Class-Agnostic%20Motion%20Prediction%20for%0A%20%20Autonomous%20Driving&body=Title%3A%20Weakly%20and%20Self-Supervised%20Class-Agnostic%20Motion%20Prediction%20for%0A%20%20Autonomous%20Driving%0AAuthor%3A%20Ruibo%20Li%20and%20Hanyu%20Shi%20and%20Zhe%20Wang%20and%20Guosheng%20Lin%0AAbstract%3A%20%20%20Understanding%20motion%20in%20dynamic%20environments%20is%20critical%20for%20autonomous%0Adriving%2C%20thereby%20motivating%20research%20on%20class-agnostic%20motion%20prediction.%20In%0Athis%20work%2C%20we%20investigate%20weakly%20and%20self-supervised%20class-agnostic%20motion%0Aprediction%20from%20LiDAR%20point%20clouds.%20Outdoor%20scenes%20typically%20consist%20of%20mobile%0Aforegrounds%20and%20static%20backgrounds%2C%20allowing%20motion%20understanding%20to%20be%0Aassociated%20with%20scene%20parsing.%20Based%20on%20this%20observation%2C%20we%20propose%20a%20novel%0Aweakly%20supervised%20paradigm%20that%20replaces%20motion%20annotations%20with%20fully%20or%0Apartially%20annotated%20%281%25%2C%200.1%25%29%20foreground/background%20masks%20for%20supervision.%20To%0Athis%20end%2C%20we%20develop%20a%20weakly%20supervised%20approach%20utilizing%0Aforeground/background%20cues%20to%20guide%20the%20self-supervised%20learning%20of%20motion%0Aprediction%20models.%20Since%20foreground%20motion%20generally%20occurs%20in%20non-ground%0Aregions%2C%20non-ground/ground%20masks%20can%20serve%20as%20an%20alternative%20to%0Aforeground/background%20masks%2C%20further%20reducing%20annotation%20effort.%20Leveraging%0Anon-ground/ground%20cues%2C%20we%20propose%20two%20additional%20approaches%3A%20a%20weakly%0Asupervised%20method%20requiring%20fewer%20%280.01%25%29%20foreground/background%20annotations%2C%0Aand%20a%20self-supervised%20method%20without%20annotations.%20Furthermore%2C%20we%20design%20a%0ARobust%20Consistency-aware%20Chamfer%20Distance%20loss%20that%20incorporates%20multi-frame%0Ainformation%20and%20robust%20penalty%20functions%20to%20suppress%20outliers%20in%0Aself-supervised%20learning.%20Experiments%20show%20that%20our%20weakly%20and%20self-supervised%0Amodels%20outperform%20existing%20self-supervised%20counterparts%2C%20and%20our%20weakly%0Asupervised%20models%20even%20rival%20some%20supervised%20ones.%20This%20demonstrates%20that%20our%0Aapproaches%20effectively%20balance%20annotation%20effort%20and%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13116v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWeakly%2520and%2520Self-Supervised%2520Class-Agnostic%2520Motion%2520Prediction%2520for%250A%2520%2520Autonomous%2520Driving%26entry.906535625%3DRuibo%2520Li%2520and%2520Hanyu%2520Shi%2520and%2520Zhe%2520Wang%2520and%2520Guosheng%2520Lin%26entry.1292438233%3D%2520%2520Understanding%2520motion%2520in%2520dynamic%2520environments%2520is%2520critical%2520for%2520autonomous%250Adriving%252C%2520thereby%2520motivating%2520research%2520on%2520class-agnostic%2520motion%2520prediction.%2520In%250Athis%2520work%252C%2520we%2520investigate%2520weakly%2520and%2520self-supervised%2520class-agnostic%2520motion%250Aprediction%2520from%2520LiDAR%2520point%2520clouds.%2520Outdoor%2520scenes%2520typically%2520consist%2520of%2520mobile%250Aforegrounds%2520and%2520static%2520backgrounds%252C%2520allowing%2520motion%2520understanding%2520to%2520be%250Aassociated%2520with%2520scene%2520parsing.%2520Based%2520on%2520this%2520observation%252C%2520we%2520propose%2520a%2520novel%250Aweakly%2520supervised%2520paradigm%2520that%2520replaces%2520motion%2520annotations%2520with%2520fully%2520or%250Apartially%2520annotated%2520%25281%2525%252C%25200.1%2525%2529%2520foreground/background%2520masks%2520for%2520supervision.%2520To%250Athis%2520end%252C%2520we%2520develop%2520a%2520weakly%2520supervised%2520approach%2520utilizing%250Aforeground/background%2520cues%2520to%2520guide%2520the%2520self-supervised%2520learning%2520of%2520motion%250Aprediction%2520models.%2520Since%2520foreground%2520motion%2520generally%2520occurs%2520in%2520non-ground%250Aregions%252C%2520non-ground/ground%2520masks%2520can%2520serve%2520as%2520an%2520alternative%2520to%250Aforeground/background%2520masks%252C%2520further%2520reducing%2520annotation%2520effort.%2520Leveraging%250Anon-ground/ground%2520cues%252C%2520we%2520propose%2520two%2520additional%2520approaches%253A%2520a%2520weakly%250Asupervised%2520method%2520requiring%2520fewer%2520%25280.01%2525%2529%2520foreground/background%2520annotations%252C%250Aand%2520a%2520self-supervised%2520method%2520without%2520annotations.%2520Furthermore%252C%2520we%2520design%2520a%250ARobust%2520Consistency-aware%2520Chamfer%2520Distance%2520loss%2520that%2520incorporates%2520multi-frame%250Ainformation%2520and%2520robust%2520penalty%2520functions%2520to%2520suppress%2520outliers%2520in%250Aself-supervised%2520learning.%2520Experiments%2520show%2520that%2520our%2520weakly%2520and%2520self-supervised%250Amodels%2520outperform%2520existing%2520self-supervised%2520counterparts%252C%2520and%2520our%2520weakly%250Asupervised%2520models%2520even%2520rival%2520some%2520supervised%2520ones.%2520This%2520demonstrates%2520that%2520our%250Aapproaches%2520effectively%2520balance%2520annotation%2520effort%2520and%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13116v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Weakly%20and%20Self-Supervised%20Class-Agnostic%20Motion%20Prediction%20for%0A%20%20Autonomous%20Driving&entry.906535625=Ruibo%20Li%20and%20Hanyu%20Shi%20and%20Zhe%20Wang%20and%20Guosheng%20Lin&entry.1292438233=%20%20Understanding%20motion%20in%20dynamic%20environments%20is%20critical%20for%20autonomous%0Adriving%2C%20thereby%20motivating%20research%20on%20class-agnostic%20motion%20prediction.%20In%0Athis%20work%2C%20we%20investigate%20weakly%20and%20self-supervised%20class-agnostic%20motion%0Aprediction%20from%20LiDAR%20point%20clouds.%20Outdoor%20scenes%20typically%20consist%20of%20mobile%0Aforegrounds%20and%20static%20backgrounds%2C%20allowing%20motion%20understanding%20to%20be%0Aassociated%20with%20scene%20parsing.%20Based%20on%20this%20observation%2C%20we%20propose%20a%20novel%0Aweakly%20supervised%20paradigm%20that%20replaces%20motion%20annotations%20with%20fully%20or%0Apartially%20annotated%20%281%25%2C%200.1%25%29%20foreground/background%20masks%20for%20supervision.%20To%0Athis%20end%2C%20we%20develop%20a%20weakly%20supervised%20approach%20utilizing%0Aforeground/background%20cues%20to%20guide%20the%20self-supervised%20learning%20of%20motion%0Aprediction%20models.%20Since%20foreground%20motion%20generally%20occurs%20in%20non-ground%0Aregions%2C%20non-ground/ground%20masks%20can%20serve%20as%20an%20alternative%20to%0Aforeground/background%20masks%2C%20further%20reducing%20annotation%20effort.%20Leveraging%0Anon-ground/ground%20cues%2C%20we%20propose%20two%20additional%20approaches%3A%20a%20weakly%0Asupervised%20method%20requiring%20fewer%20%280.01%25%29%20foreground/background%20annotations%2C%0Aand%20a%20self-supervised%20method%20without%20annotations.%20Furthermore%2C%20we%20design%20a%0ARobust%20Consistency-aware%20Chamfer%20Distance%20loss%20that%20incorporates%20multi-frame%0Ainformation%20and%20robust%20penalty%20functions%20to%20suppress%20outliers%20in%0Aself-supervised%20learning.%20Experiments%20show%20that%20our%20weakly%20and%20self-supervised%0Amodels%20outperform%20existing%20self-supervised%20counterparts%2C%20and%20our%20weakly%0Asupervised%20models%20even%20rival%20some%20supervised%20ones.%20This%20demonstrates%20that%20our%0Aapproaches%20effectively%20balance%20annotation%20effort%20and%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13116v1&entry.124074799=Read"},
{"title": "Enhancing Video Large Language Models with Structured Multi-Video\n  Collaborative Reasoning (early version)", "author": "Zhihao He and Tianyao He and Tieyuan Chen and Yun Xu and Huabin Liu and Chaofan Gan and Gui Zou and Weiyao Lin", "abstract": "  Despite the prosperity of the video language model, the current pursuit of\ncomprehensive video reasoning is thwarted by the inherent spatio-temporal\nincompleteness within individual videos, resulting in hallucinations and\ninaccuracies. A promising solution is to augment the reasoning performance with\nmultiple related videos. However, video tokens are numerous and contain\nredundant information, so directly feeding the relevant video data into a large\nlanguage model to enhance responses could be counterproductive. To address this\nchallenge, we propose a multi-video collaborative framework for video language\nmodels. For efficient and flexible video representation, we establish a Video\nStructuring Module to represent the video's knowledge as a spatio-temporal\ngraph. Based on the structured video representation, we design the Graph Fusion\nModule to fuse the structured knowledge and valuable information from related\nvideos into the augmented graph node tokens. Finally, we construct an elaborate\nmulti-video structured prompt to integrate the graph, visual, and textual\ntokens as the input to the large language model. Extensive experiments\nsubstantiate the effectiveness of our framework, showcasing its potential as a\npromising avenue for advancing video language models.\n", "link": "http://arxiv.org/abs/2509.13161v1", "date": "2025-09-16", "relevancy": 2.3897, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6053}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6053}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Video%20Large%20Language%20Models%20with%20Structured%20Multi-Video%0A%20%20Collaborative%20Reasoning%20%28early%20version%29&body=Title%3A%20Enhancing%20Video%20Large%20Language%20Models%20with%20Structured%20Multi-Video%0A%20%20Collaborative%20Reasoning%20%28early%20version%29%0AAuthor%3A%20Zhihao%20He%20and%20Tianyao%20He%20and%20Tieyuan%20Chen%20and%20Yun%20Xu%20and%20Huabin%20Liu%20and%20Chaofan%20Gan%20and%20Gui%20Zou%20and%20Weiyao%20Lin%0AAbstract%3A%20%20%20Despite%20the%20prosperity%20of%20the%20video%20language%20model%2C%20the%20current%20pursuit%20of%0Acomprehensive%20video%20reasoning%20is%20thwarted%20by%20the%20inherent%20spatio-temporal%0Aincompleteness%20within%20individual%20videos%2C%20resulting%20in%20hallucinations%20and%0Ainaccuracies.%20A%20promising%20solution%20is%20to%20augment%20the%20reasoning%20performance%20with%0Amultiple%20related%20videos.%20However%2C%20video%20tokens%20are%20numerous%20and%20contain%0Aredundant%20information%2C%20so%20directly%20feeding%20the%20relevant%20video%20data%20into%20a%20large%0Alanguage%20model%20to%20enhance%20responses%20could%20be%20counterproductive.%20To%20address%20this%0Achallenge%2C%20we%20propose%20a%20multi-video%20collaborative%20framework%20for%20video%20language%0Amodels.%20For%20efficient%20and%20flexible%20video%20representation%2C%20we%20establish%20a%20Video%0AStructuring%20Module%20to%20represent%20the%20video%27s%20knowledge%20as%20a%20spatio-temporal%0Agraph.%20Based%20on%20the%20structured%20video%20representation%2C%20we%20design%20the%20Graph%20Fusion%0AModule%20to%20fuse%20the%20structured%20knowledge%20and%20valuable%20information%20from%20related%0Avideos%20into%20the%20augmented%20graph%20node%20tokens.%20Finally%2C%20we%20construct%20an%20elaborate%0Amulti-video%20structured%20prompt%20to%20integrate%20the%20graph%2C%20visual%2C%20and%20textual%0Atokens%20as%20the%20input%20to%20the%20large%20language%20model.%20Extensive%20experiments%0Asubstantiate%20the%20effectiveness%20of%20our%20framework%2C%20showcasing%20its%20potential%20as%20a%0Apromising%20avenue%20for%20advancing%20video%20language%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13161v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Video%2520Large%2520Language%2520Models%2520with%2520Structured%2520Multi-Video%250A%2520%2520Collaborative%2520Reasoning%2520%2528early%2520version%2529%26entry.906535625%3DZhihao%2520He%2520and%2520Tianyao%2520He%2520and%2520Tieyuan%2520Chen%2520and%2520Yun%2520Xu%2520and%2520Huabin%2520Liu%2520and%2520Chaofan%2520Gan%2520and%2520Gui%2520Zou%2520and%2520Weiyao%2520Lin%26entry.1292438233%3D%2520%2520Despite%2520the%2520prosperity%2520of%2520the%2520video%2520language%2520model%252C%2520the%2520current%2520pursuit%2520of%250Acomprehensive%2520video%2520reasoning%2520is%2520thwarted%2520by%2520the%2520inherent%2520spatio-temporal%250Aincompleteness%2520within%2520individual%2520videos%252C%2520resulting%2520in%2520hallucinations%2520and%250Ainaccuracies.%2520A%2520promising%2520solution%2520is%2520to%2520augment%2520the%2520reasoning%2520performance%2520with%250Amultiple%2520related%2520videos.%2520However%252C%2520video%2520tokens%2520are%2520numerous%2520and%2520contain%250Aredundant%2520information%252C%2520so%2520directly%2520feeding%2520the%2520relevant%2520video%2520data%2520into%2520a%2520large%250Alanguage%2520model%2520to%2520enhance%2520responses%2520could%2520be%2520counterproductive.%2520To%2520address%2520this%250Achallenge%252C%2520we%2520propose%2520a%2520multi-video%2520collaborative%2520framework%2520for%2520video%2520language%250Amodels.%2520For%2520efficient%2520and%2520flexible%2520video%2520representation%252C%2520we%2520establish%2520a%2520Video%250AStructuring%2520Module%2520to%2520represent%2520the%2520video%2527s%2520knowledge%2520as%2520a%2520spatio-temporal%250Agraph.%2520Based%2520on%2520the%2520structured%2520video%2520representation%252C%2520we%2520design%2520the%2520Graph%2520Fusion%250AModule%2520to%2520fuse%2520the%2520structured%2520knowledge%2520and%2520valuable%2520information%2520from%2520related%250Avideos%2520into%2520the%2520augmented%2520graph%2520node%2520tokens.%2520Finally%252C%2520we%2520construct%2520an%2520elaborate%250Amulti-video%2520structured%2520prompt%2520to%2520integrate%2520the%2520graph%252C%2520visual%252C%2520and%2520textual%250Atokens%2520as%2520the%2520input%2520to%2520the%2520large%2520language%2520model.%2520Extensive%2520experiments%250Asubstantiate%2520the%2520effectiveness%2520of%2520our%2520framework%252C%2520showcasing%2520its%2520potential%2520as%2520a%250Apromising%2520avenue%2520for%2520advancing%2520video%2520language%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13161v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Video%20Large%20Language%20Models%20with%20Structured%20Multi-Video%0A%20%20Collaborative%20Reasoning%20%28early%20version%29&entry.906535625=Zhihao%20He%20and%20Tianyao%20He%20and%20Tieyuan%20Chen%20and%20Yun%20Xu%20and%20Huabin%20Liu%20and%20Chaofan%20Gan%20and%20Gui%20Zou%20and%20Weiyao%20Lin&entry.1292438233=%20%20Despite%20the%20prosperity%20of%20the%20video%20language%20model%2C%20the%20current%20pursuit%20of%0Acomprehensive%20video%20reasoning%20is%20thwarted%20by%20the%20inherent%20spatio-temporal%0Aincompleteness%20within%20individual%20videos%2C%20resulting%20in%20hallucinations%20and%0Ainaccuracies.%20A%20promising%20solution%20is%20to%20augment%20the%20reasoning%20performance%20with%0Amultiple%20related%20videos.%20However%2C%20video%20tokens%20are%20numerous%20and%20contain%0Aredundant%20information%2C%20so%20directly%20feeding%20the%20relevant%20video%20data%20into%20a%20large%0Alanguage%20model%20to%20enhance%20responses%20could%20be%20counterproductive.%20To%20address%20this%0Achallenge%2C%20we%20propose%20a%20multi-video%20collaborative%20framework%20for%20video%20language%0Amodels.%20For%20efficient%20and%20flexible%20video%20representation%2C%20we%20establish%20a%20Video%0AStructuring%20Module%20to%20represent%20the%20video%27s%20knowledge%20as%20a%20spatio-temporal%0Agraph.%20Based%20on%20the%20structured%20video%20representation%2C%20we%20design%20the%20Graph%20Fusion%0AModule%20to%20fuse%20the%20structured%20knowledge%20and%20valuable%20information%20from%20related%0Avideos%20into%20the%20augmented%20graph%20node%20tokens.%20Finally%2C%20we%20construct%20an%20elaborate%0Amulti-video%20structured%20prompt%20to%20integrate%20the%20graph%2C%20visual%2C%20and%20textual%0Atokens%20as%20the%20input%20to%20the%20large%20language%20model.%20Extensive%20experiments%0Asubstantiate%20the%20effectiveness%20of%20our%20framework%2C%20showcasing%20its%20potential%20as%20a%0Apromising%20avenue%20for%20advancing%20video%20language%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13161v1&entry.124074799=Read"},
{"title": "Perception Before Reasoning: Two-Stage Reinforcement Learning for Visual\n  Reasoning in Vision-Language Models", "author": "Yan Chen and Long Li and Teng Xi and Long Zeng and Jingdong Wang", "abstract": "  Reinforcement learning (RL) has proven highly effective in eliciting the\nreasoning capabilities of large language models (LLMs). Inspired by this\nsuccess, recent studies have explored applying similar techniques to\nvision-language models (VLMs), aiming to enhance their reasoning performance.\nHowever, directly transplanting RL methods from LLMs to VLMs is suboptimal, as\nthe tasks faced by VLMs are inherently more complex. Specifically, VLMs must\nfirst accurately perceive and understand visual inputs before reasoning can be\neffectively performed. To address this challenge, we propose a two-stage\nreinforcement learning framework designed to jointly enhance both the\nperceptual and reasoning capabilities of VLMs. To mitigate the vanishing\nadvantage issue commonly observed in RL training, we first perform\ndataset-level sampling to selectively strengthen specific capabilities using\ndistinct data sources. During training, the first stage focuses on improving\nthe model's visual perception through coarse- and fine-grained visual\nunderstanding, while the second stage targets the enhancement of reasoning\nabilities. After the proposed two-stage reinforcement learning process, we\nobtain PeBR-R1, a vision-language model with significantly enhanced perceptual\nand reasoning capabilities. Experimental results on seven benchmark datasets\ndemonstrate the effectiveness of our approach and validate the superior\nperformance of PeBR-R1 across diverse visual reasoning tasks.\n", "link": "http://arxiv.org/abs/2509.13031v1", "date": "2025-09-16", "relevancy": 2.3888, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6098}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6098}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Perception%20Before%20Reasoning%3A%20Two-Stage%20Reinforcement%20Learning%20for%20Visual%0A%20%20Reasoning%20in%20Vision-Language%20Models&body=Title%3A%20Perception%20Before%20Reasoning%3A%20Two-Stage%20Reinforcement%20Learning%20for%20Visual%0A%20%20Reasoning%20in%20Vision-Language%20Models%0AAuthor%3A%20Yan%20Chen%20and%20Long%20Li%20and%20Teng%20Xi%20and%20Long%20Zeng%20and%20Jingdong%20Wang%0AAbstract%3A%20%20%20Reinforcement%20learning%20%28RL%29%20has%20proven%20highly%20effective%20in%20eliciting%20the%0Areasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29.%20Inspired%20by%20this%0Asuccess%2C%20recent%20studies%20have%20explored%20applying%20similar%20techniques%20to%0Avision-language%20models%20%28VLMs%29%2C%20aiming%20to%20enhance%20their%20reasoning%20performance.%0AHowever%2C%20directly%20transplanting%20RL%20methods%20from%20LLMs%20to%20VLMs%20is%20suboptimal%2C%20as%0Athe%20tasks%20faced%20by%20VLMs%20are%20inherently%20more%20complex.%20Specifically%2C%20VLMs%20must%0Afirst%20accurately%20perceive%20and%20understand%20visual%20inputs%20before%20reasoning%20can%20be%0Aeffectively%20performed.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20two-stage%0Areinforcement%20learning%20framework%20designed%20to%20jointly%20enhance%20both%20the%0Aperceptual%20and%20reasoning%20capabilities%20of%20VLMs.%20To%20mitigate%20the%20vanishing%0Aadvantage%20issue%20commonly%20observed%20in%20RL%20training%2C%20we%20first%20perform%0Adataset-level%20sampling%20to%20selectively%20strengthen%20specific%20capabilities%20using%0Adistinct%20data%20sources.%20During%20training%2C%20the%20first%20stage%20focuses%20on%20improving%0Athe%20model%27s%20visual%20perception%20through%20coarse-%20and%20fine-grained%20visual%0Aunderstanding%2C%20while%20the%20second%20stage%20targets%20the%20enhancement%20of%20reasoning%0Aabilities.%20After%20the%20proposed%20two-stage%20reinforcement%20learning%20process%2C%20we%0Aobtain%20PeBR-R1%2C%20a%20vision-language%20model%20with%20significantly%20enhanced%20perceptual%0Aand%20reasoning%20capabilities.%20Experimental%20results%20on%20seven%20benchmark%20datasets%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20and%20validate%20the%20superior%0Aperformance%20of%20PeBR-R1%20across%20diverse%20visual%20reasoning%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13031v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerception%2520Before%2520Reasoning%253A%2520Two-Stage%2520Reinforcement%2520Learning%2520for%2520Visual%250A%2520%2520Reasoning%2520in%2520Vision-Language%2520Models%26entry.906535625%3DYan%2520Chen%2520and%2520Long%2520Li%2520and%2520Teng%2520Xi%2520and%2520Long%2520Zeng%2520and%2520Jingdong%2520Wang%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520%2528RL%2529%2520has%2520proven%2520highly%2520effective%2520in%2520eliciting%2520the%250Areasoning%2520capabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Inspired%2520by%2520this%250Asuccess%252C%2520recent%2520studies%2520have%2520explored%2520applying%2520similar%2520techniques%2520to%250Avision-language%2520models%2520%2528VLMs%2529%252C%2520aiming%2520to%2520enhance%2520their%2520reasoning%2520performance.%250AHowever%252C%2520directly%2520transplanting%2520RL%2520methods%2520from%2520LLMs%2520to%2520VLMs%2520is%2520suboptimal%252C%2520as%250Athe%2520tasks%2520faced%2520by%2520VLMs%2520are%2520inherently%2520more%2520complex.%2520Specifically%252C%2520VLMs%2520must%250Afirst%2520accurately%2520perceive%2520and%2520understand%2520visual%2520inputs%2520before%2520reasoning%2520can%2520be%250Aeffectively%2520performed.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520two-stage%250Areinforcement%2520learning%2520framework%2520designed%2520to%2520jointly%2520enhance%2520both%2520the%250Aperceptual%2520and%2520reasoning%2520capabilities%2520of%2520VLMs.%2520To%2520mitigate%2520the%2520vanishing%250Aadvantage%2520issue%2520commonly%2520observed%2520in%2520RL%2520training%252C%2520we%2520first%2520perform%250Adataset-level%2520sampling%2520to%2520selectively%2520strengthen%2520specific%2520capabilities%2520using%250Adistinct%2520data%2520sources.%2520During%2520training%252C%2520the%2520first%2520stage%2520focuses%2520on%2520improving%250Athe%2520model%2527s%2520visual%2520perception%2520through%2520coarse-%2520and%2520fine-grained%2520visual%250Aunderstanding%252C%2520while%2520the%2520second%2520stage%2520targets%2520the%2520enhancement%2520of%2520reasoning%250Aabilities.%2520After%2520the%2520proposed%2520two-stage%2520reinforcement%2520learning%2520process%252C%2520we%250Aobtain%2520PeBR-R1%252C%2520a%2520vision-language%2520model%2520with%2520significantly%2520enhanced%2520perceptual%250Aand%2520reasoning%2520capabilities.%2520Experimental%2520results%2520on%2520seven%2520benchmark%2520datasets%250Ademonstrate%2520the%2520effectiveness%2520of%2520our%2520approach%2520and%2520validate%2520the%2520superior%250Aperformance%2520of%2520PeBR-R1%2520across%2520diverse%2520visual%2520reasoning%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13031v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception%20Before%20Reasoning%3A%20Two-Stage%20Reinforcement%20Learning%20for%20Visual%0A%20%20Reasoning%20in%20Vision-Language%20Models&entry.906535625=Yan%20Chen%20and%20Long%20Li%20and%20Teng%20Xi%20and%20Long%20Zeng%20and%20Jingdong%20Wang&entry.1292438233=%20%20Reinforcement%20learning%20%28RL%29%20has%20proven%20highly%20effective%20in%20eliciting%20the%0Areasoning%20capabilities%20of%20large%20language%20models%20%28LLMs%29.%20Inspired%20by%20this%0Asuccess%2C%20recent%20studies%20have%20explored%20applying%20similar%20techniques%20to%0Avision-language%20models%20%28VLMs%29%2C%20aiming%20to%20enhance%20their%20reasoning%20performance.%0AHowever%2C%20directly%20transplanting%20RL%20methods%20from%20LLMs%20to%20VLMs%20is%20suboptimal%2C%20as%0Athe%20tasks%20faced%20by%20VLMs%20are%20inherently%20more%20complex.%20Specifically%2C%20VLMs%20must%0Afirst%20accurately%20perceive%20and%20understand%20visual%20inputs%20before%20reasoning%20can%20be%0Aeffectively%20performed.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20two-stage%0Areinforcement%20learning%20framework%20designed%20to%20jointly%20enhance%20both%20the%0Aperceptual%20and%20reasoning%20capabilities%20of%20VLMs.%20To%20mitigate%20the%20vanishing%0Aadvantage%20issue%20commonly%20observed%20in%20RL%20training%2C%20we%20first%20perform%0Adataset-level%20sampling%20to%20selectively%20strengthen%20specific%20capabilities%20using%0Adistinct%20data%20sources.%20During%20training%2C%20the%20first%20stage%20focuses%20on%20improving%0Athe%20model%27s%20visual%20perception%20through%20coarse-%20and%20fine-grained%20visual%0Aunderstanding%2C%20while%20the%20second%20stage%20targets%20the%20enhancement%20of%20reasoning%0Aabilities.%20After%20the%20proposed%20two-stage%20reinforcement%20learning%20process%2C%20we%0Aobtain%20PeBR-R1%2C%20a%20vision-language%20model%20with%20significantly%20enhanced%20perceptual%0Aand%20reasoning%20capabilities.%20Experimental%20results%20on%20seven%20benchmark%20datasets%0Ademonstrate%20the%20effectiveness%20of%20our%20approach%20and%20validate%20the%20superior%0Aperformance%20of%20PeBR-R1%20across%20diverse%20visual%20reasoning%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13031v1&entry.124074799=Read"},
{"title": "ReTrack: Data Unlearning in Diffusion Models through Redirecting the\n  Denoising Trajectory", "author": "Qitan Shi and Cheng Jin and Jiawei Zhang and Yuantao Gu", "abstract": "  Diffusion models excel at generating high-quality, diverse images but suffer\nfrom training data memorization, raising critical privacy and safety concerns.\nData unlearning has emerged to mitigate this issue by removing the influence of\nspecific data without retraining from scratch. We propose ReTrack, a fast and\neffective data unlearning method for diffusion models. ReTrack employs\nimportance sampling to construct a more efficient fine-tuning loss, which we\napproximate by retaining only dominant terms. This yields an interpretable\nobjective that redirects denoising trajectories toward the $k$-nearest\nneighbors, enabling efficient unlearning while preserving generative quality.\nExperiments on MNIST T-Shirt, CelebA-HQ, CIFAR-10, and Stable Diffusion show\nthat ReTrack achieves state-of-the-art performance, striking the best trade-off\nbetween unlearning strength and generation quality preservation.\n", "link": "http://arxiv.org/abs/2509.13007v1", "date": "2025-09-16", "relevancy": 2.3878, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6286}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5774}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5731}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ReTrack%3A%20Data%20Unlearning%20in%20Diffusion%20Models%20through%20Redirecting%20the%0A%20%20Denoising%20Trajectory&body=Title%3A%20ReTrack%3A%20Data%20Unlearning%20in%20Diffusion%20Models%20through%20Redirecting%20the%0A%20%20Denoising%20Trajectory%0AAuthor%3A%20Qitan%20Shi%20and%20Cheng%20Jin%20and%20Jiawei%20Zhang%20and%20Yuantao%20Gu%0AAbstract%3A%20%20%20Diffusion%20models%20excel%20at%20generating%20high-quality%2C%20diverse%20images%20but%20suffer%0Afrom%20training%20data%20memorization%2C%20raising%20critical%20privacy%20and%20safety%20concerns.%0AData%20unlearning%20has%20emerged%20to%20mitigate%20this%20issue%20by%20removing%20the%20influence%20of%0Aspecific%20data%20without%20retraining%20from%20scratch.%20We%20propose%20ReTrack%2C%20a%20fast%20and%0Aeffective%20data%20unlearning%20method%20for%20diffusion%20models.%20ReTrack%20employs%0Aimportance%20sampling%20to%20construct%20a%20more%20efficient%20fine-tuning%20loss%2C%20which%20we%0Aapproximate%20by%20retaining%20only%20dominant%20terms.%20This%20yields%20an%20interpretable%0Aobjective%20that%20redirects%20denoising%20trajectories%20toward%20the%20%24k%24-nearest%0Aneighbors%2C%20enabling%20efficient%20unlearning%20while%20preserving%20generative%20quality.%0AExperiments%20on%20MNIST%20T-Shirt%2C%20CelebA-HQ%2C%20CIFAR-10%2C%20and%20Stable%20Diffusion%20show%0Athat%20ReTrack%20achieves%20state-of-the-art%20performance%2C%20striking%20the%20best%20trade-off%0Abetween%20unlearning%20strength%20and%20generation%20quality%20preservation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13007v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReTrack%253A%2520Data%2520Unlearning%2520in%2520Diffusion%2520Models%2520through%2520Redirecting%2520the%250A%2520%2520Denoising%2520Trajectory%26entry.906535625%3DQitan%2520Shi%2520and%2520Cheng%2520Jin%2520and%2520Jiawei%2520Zhang%2520and%2520Yuantao%2520Gu%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520excel%2520at%2520generating%2520high-quality%252C%2520diverse%2520images%2520but%2520suffer%250Afrom%2520training%2520data%2520memorization%252C%2520raising%2520critical%2520privacy%2520and%2520safety%2520concerns.%250AData%2520unlearning%2520has%2520emerged%2520to%2520mitigate%2520this%2520issue%2520by%2520removing%2520the%2520influence%2520of%250Aspecific%2520data%2520without%2520retraining%2520from%2520scratch.%2520We%2520propose%2520ReTrack%252C%2520a%2520fast%2520and%250Aeffective%2520data%2520unlearning%2520method%2520for%2520diffusion%2520models.%2520ReTrack%2520employs%250Aimportance%2520sampling%2520to%2520construct%2520a%2520more%2520efficient%2520fine-tuning%2520loss%252C%2520which%2520we%250Aapproximate%2520by%2520retaining%2520only%2520dominant%2520terms.%2520This%2520yields%2520an%2520interpretable%250Aobjective%2520that%2520redirects%2520denoising%2520trajectories%2520toward%2520the%2520%2524k%2524-nearest%250Aneighbors%252C%2520enabling%2520efficient%2520unlearning%2520while%2520preserving%2520generative%2520quality.%250AExperiments%2520on%2520MNIST%2520T-Shirt%252C%2520CelebA-HQ%252C%2520CIFAR-10%252C%2520and%2520Stable%2520Diffusion%2520show%250Athat%2520ReTrack%2520achieves%2520state-of-the-art%2520performance%252C%2520striking%2520the%2520best%2520trade-off%250Abetween%2520unlearning%2520strength%2520and%2520generation%2520quality%2520preservation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13007v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ReTrack%3A%20Data%20Unlearning%20in%20Diffusion%20Models%20through%20Redirecting%20the%0A%20%20Denoising%20Trajectory&entry.906535625=Qitan%20Shi%20and%20Cheng%20Jin%20and%20Jiawei%20Zhang%20and%20Yuantao%20Gu&entry.1292438233=%20%20Diffusion%20models%20excel%20at%20generating%20high-quality%2C%20diverse%20images%20but%20suffer%0Afrom%20training%20data%20memorization%2C%20raising%20critical%20privacy%20and%20safety%20concerns.%0AData%20unlearning%20has%20emerged%20to%20mitigate%20this%20issue%20by%20removing%20the%20influence%20of%0Aspecific%20data%20without%20retraining%20from%20scratch.%20We%20propose%20ReTrack%2C%20a%20fast%20and%0Aeffective%20data%20unlearning%20method%20for%20diffusion%20models.%20ReTrack%20employs%0Aimportance%20sampling%20to%20construct%20a%20more%20efficient%20fine-tuning%20loss%2C%20which%20we%0Aapproximate%20by%20retaining%20only%20dominant%20terms.%20This%20yields%20an%20interpretable%0Aobjective%20that%20redirects%20denoising%20trajectories%20toward%20the%20%24k%24-nearest%0Aneighbors%2C%20enabling%20efficient%20unlearning%20while%20preserving%20generative%20quality.%0AExperiments%20on%20MNIST%20T-Shirt%2C%20CelebA-HQ%2C%20CIFAR-10%2C%20and%20Stable%20Diffusion%20show%0Athat%20ReTrack%20achieves%20state-of-the-art%20performance%2C%20striking%20the%20best%20trade-off%0Abetween%20unlearning%20strength%20and%20generation%20quality%20preservation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13007v1&entry.124074799=Read"},
{"title": "ResidualViT for Efficient Temporally Dense Video Encoding", "author": "Mattia Soldan and Fabian Caba Heilbron and Bernard Ghanem and Josef Sivic and Bryan Russell", "abstract": "  Several video understanding tasks, such as natural language temporal video\ngrounding, temporal activity localization, and audio description generation,\nrequire \"temporally dense\" reasoning over frames sampled at high temporal\nresolution. However, computing frame-level features for these tasks is\ncomputationally expensive given the temporal resolution requirements. In this\npaper, we make three contributions to reduce the cost of computing features for\ntemporally dense tasks. First, we introduce a vision transformer (ViT)\narchitecture, dubbed ResidualViT, that leverages the large temporal redundancy\nin videos to efficiently compute temporally dense frame-level features. Our\narchitecture incorporates (i) learnable residual connections that ensure\ntemporal consistency across consecutive frames and (ii) a token reduction\nmodule that enhances processing speed by selectively discarding temporally\nredundant information while reusing weights of a pretrained foundation model.\nSecond, we propose a lightweight distillation strategy to approximate the\nframe-level features of the original foundation model. Finally, we evaluate our\napproach across four tasks and five datasets, in both zero-shot and fully\nsupervised settings, demonstrating significant reductions in computational cost\n(up to 60%) and improvements in inference speed (up to 2.5x faster), all while\nclosely approximating the accuracy of the original foundation model.\n", "link": "http://arxiv.org/abs/2509.13255v1", "date": "2025-09-16", "relevancy": 2.3749, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5945}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5872}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ResidualViT%20for%20Efficient%20Temporally%20Dense%20Video%20Encoding&body=Title%3A%20ResidualViT%20for%20Efficient%20Temporally%20Dense%20Video%20Encoding%0AAuthor%3A%20Mattia%20Soldan%20and%20Fabian%20Caba%20Heilbron%20and%20Bernard%20Ghanem%20and%20Josef%20Sivic%20and%20Bryan%20Russell%0AAbstract%3A%20%20%20Several%20video%20understanding%20tasks%2C%20such%20as%20natural%20language%20temporal%20video%0Agrounding%2C%20temporal%20activity%20localization%2C%20and%20audio%20description%20generation%2C%0Arequire%20%22temporally%20dense%22%20reasoning%20over%20frames%20sampled%20at%20high%20temporal%0Aresolution.%20However%2C%20computing%20frame-level%20features%20for%20these%20tasks%20is%0Acomputationally%20expensive%20given%20the%20temporal%20resolution%20requirements.%20In%20this%0Apaper%2C%20we%20make%20three%20contributions%20to%20reduce%20the%20cost%20of%20computing%20features%20for%0Atemporally%20dense%20tasks.%20First%2C%20we%20introduce%20a%20vision%20transformer%20%28ViT%29%0Aarchitecture%2C%20dubbed%20ResidualViT%2C%20that%20leverages%20the%20large%20temporal%20redundancy%0Ain%20videos%20to%20efficiently%20compute%20temporally%20dense%20frame-level%20features.%20Our%0Aarchitecture%20incorporates%20%28i%29%20learnable%20residual%20connections%20that%20ensure%0Atemporal%20consistency%20across%20consecutive%20frames%20and%20%28ii%29%20a%20token%20reduction%0Amodule%20that%20enhances%20processing%20speed%20by%20selectively%20discarding%20temporally%0Aredundant%20information%20while%20reusing%20weights%20of%20a%20pretrained%20foundation%20model.%0ASecond%2C%20we%20propose%20a%20lightweight%20distillation%20strategy%20to%20approximate%20the%0Aframe-level%20features%20of%20the%20original%20foundation%20model.%20Finally%2C%20we%20evaluate%20our%0Aapproach%20across%20four%20tasks%20and%20five%20datasets%2C%20in%20both%20zero-shot%20and%20fully%0Asupervised%20settings%2C%20demonstrating%20significant%20reductions%20in%20computational%20cost%0A%28up%20to%2060%25%29%20and%20improvements%20in%20inference%20speed%20%28up%20to%202.5x%20faster%29%2C%20all%20while%0Aclosely%20approximating%20the%20accuracy%20of%20the%20original%20foundation%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DResidualViT%2520for%2520Efficient%2520Temporally%2520Dense%2520Video%2520Encoding%26entry.906535625%3DMattia%2520Soldan%2520and%2520Fabian%2520Caba%2520Heilbron%2520and%2520Bernard%2520Ghanem%2520and%2520Josef%2520Sivic%2520and%2520Bryan%2520Russell%26entry.1292438233%3D%2520%2520Several%2520video%2520understanding%2520tasks%252C%2520such%2520as%2520natural%2520language%2520temporal%2520video%250Agrounding%252C%2520temporal%2520activity%2520localization%252C%2520and%2520audio%2520description%2520generation%252C%250Arequire%2520%2522temporally%2520dense%2522%2520reasoning%2520over%2520frames%2520sampled%2520at%2520high%2520temporal%250Aresolution.%2520However%252C%2520computing%2520frame-level%2520features%2520for%2520these%2520tasks%2520is%250Acomputationally%2520expensive%2520given%2520the%2520temporal%2520resolution%2520requirements.%2520In%2520this%250Apaper%252C%2520we%2520make%2520three%2520contributions%2520to%2520reduce%2520the%2520cost%2520of%2520computing%2520features%2520for%250Atemporally%2520dense%2520tasks.%2520First%252C%2520we%2520introduce%2520a%2520vision%2520transformer%2520%2528ViT%2529%250Aarchitecture%252C%2520dubbed%2520ResidualViT%252C%2520that%2520leverages%2520the%2520large%2520temporal%2520redundancy%250Ain%2520videos%2520to%2520efficiently%2520compute%2520temporally%2520dense%2520frame-level%2520features.%2520Our%250Aarchitecture%2520incorporates%2520%2528i%2529%2520learnable%2520residual%2520connections%2520that%2520ensure%250Atemporal%2520consistency%2520across%2520consecutive%2520frames%2520and%2520%2528ii%2529%2520a%2520token%2520reduction%250Amodule%2520that%2520enhances%2520processing%2520speed%2520by%2520selectively%2520discarding%2520temporally%250Aredundant%2520information%2520while%2520reusing%2520weights%2520of%2520a%2520pretrained%2520foundation%2520model.%250ASecond%252C%2520we%2520propose%2520a%2520lightweight%2520distillation%2520strategy%2520to%2520approximate%2520the%250Aframe-level%2520features%2520of%2520the%2520original%2520foundation%2520model.%2520Finally%252C%2520we%2520evaluate%2520our%250Aapproach%2520across%2520four%2520tasks%2520and%2520five%2520datasets%252C%2520in%2520both%2520zero-shot%2520and%2520fully%250Asupervised%2520settings%252C%2520demonstrating%2520significant%2520reductions%2520in%2520computational%2520cost%250A%2528up%2520to%252060%2525%2529%2520and%2520improvements%2520in%2520inference%2520speed%2520%2528up%2520to%25202.5x%2520faster%2529%252C%2520all%2520while%250Aclosely%2520approximating%2520the%2520accuracy%2520of%2520the%2520original%2520foundation%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ResidualViT%20for%20Efficient%20Temporally%20Dense%20Video%20Encoding&entry.906535625=Mattia%20Soldan%20and%20Fabian%20Caba%20Heilbron%20and%20Bernard%20Ghanem%20and%20Josef%20Sivic%20and%20Bryan%20Russell&entry.1292438233=%20%20Several%20video%20understanding%20tasks%2C%20such%20as%20natural%20language%20temporal%20video%0Agrounding%2C%20temporal%20activity%20localization%2C%20and%20audio%20description%20generation%2C%0Arequire%20%22temporally%20dense%22%20reasoning%20over%20frames%20sampled%20at%20high%20temporal%0Aresolution.%20However%2C%20computing%20frame-level%20features%20for%20these%20tasks%20is%0Acomputationally%20expensive%20given%20the%20temporal%20resolution%20requirements.%20In%20this%0Apaper%2C%20we%20make%20three%20contributions%20to%20reduce%20the%20cost%20of%20computing%20features%20for%0Atemporally%20dense%20tasks.%20First%2C%20we%20introduce%20a%20vision%20transformer%20%28ViT%29%0Aarchitecture%2C%20dubbed%20ResidualViT%2C%20that%20leverages%20the%20large%20temporal%20redundancy%0Ain%20videos%20to%20efficiently%20compute%20temporally%20dense%20frame-level%20features.%20Our%0Aarchitecture%20incorporates%20%28i%29%20learnable%20residual%20connections%20that%20ensure%0Atemporal%20consistency%20across%20consecutive%20frames%20and%20%28ii%29%20a%20token%20reduction%0Amodule%20that%20enhances%20processing%20speed%20by%20selectively%20discarding%20temporally%0Aredundant%20information%20while%20reusing%20weights%20of%20a%20pretrained%20foundation%20model.%0ASecond%2C%20we%20propose%20a%20lightweight%20distillation%20strategy%20to%20approximate%20the%0Aframe-level%20features%20of%20the%20original%20foundation%20model.%20Finally%2C%20we%20evaluate%20our%0Aapproach%20across%20four%20tasks%20and%20five%20datasets%2C%20in%20both%20zero-shot%20and%20fully%0Asupervised%20settings%2C%20demonstrating%20significant%20reductions%20in%20computational%20cost%0A%28up%20to%2060%25%29%20and%20improvements%20in%20inference%20speed%20%28up%20to%202.5x%20faster%29%2C%20all%20while%0Aclosely%20approximating%20the%20accuracy%20of%20the%20original%20foundation%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13255v1&entry.124074799=Read"},
{"title": "RadGame: An AI-Powered Platform for Radiology Education", "author": "Mohammed Baharoon and Siavash Raissi and John S. Jun and Thibault Heintz and Mahmoud Alabbad and Ali Alburkani and Sung Eun Kim and Kent Kleinschmidt and Abdulrahman O. Alhumaydhi and Mohannad Mohammed G. Alghamdi and Jeremy Francis Palacio and Mohammed Bukhaytan and Noah Michael Prudlo and Rithvik Akula and Brady Chrisler and Benjamin Galligos and Mohammed O. Almutairi and Mazeen Mohammed Alanazi and Nasser M. Alrashdi and Joel Jihwan Hwang and Sri Sai Dinesh Jaliparthi and Luke David Nelson and Nathaniel Nguyen and Sathvik Suryadevara and Steven Kim and Mohammed F. Mohammed and Yevgeniy R. Semenov and Kun-Hsing Yu and Abdulrhman Aljouie and Hassan AlOmaish and Adam Rodman and Pranav Rajpurkar", "abstract": "  We introduce RadGame, an AI-powered gamified platform for radiology education\nthat targets two core skills: localizing findings and generating reports.\nTraditional radiology training is based on passive exposure to cases or active\npractice with real-time input from supervising radiologists, limiting\nopportunities for immediate and scalable feedback. RadGame addresses this gap\nby combining gamification with large-scale public datasets and automated,\nAI-driven feedback that provides clear, structured guidance to human learners.\nIn RadGame Localize, players draw bounding boxes around abnormalities, which\nare automatically compared to radiologist-drawn annotations from public\ndatasets, and visual explanations are generated by vision-language models for\nuser missed findings. In RadGame Report, players compose findings given a chest\nX-ray, patient age and indication, and receive structured AI feedback based on\nradiology report generation metrics, highlighting errors and omissions compared\nto a radiologist's written ground truth report from public datasets, producing\na final performance and style score. In a prospective evaluation, participants\nusing RadGame achieved a 68% improvement in localization accuracy compared to\n17% with traditional passive methods and a 31% improvement in report-writing\naccuracy compared to 4% with traditional methods after seeing the same cases.\nRadGame highlights the potential of AI-driven gamification to deliver scalable,\nfeedback-rich radiology training and reimagines the application of medical AI\nresources in education.\n", "link": "http://arxiv.org/abs/2509.13270v1", "date": "2025-09-16", "relevancy": 2.3587, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4798}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.477}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RadGame%3A%20An%20AI-Powered%20Platform%20for%20Radiology%20Education&body=Title%3A%20RadGame%3A%20An%20AI-Powered%20Platform%20for%20Radiology%20Education%0AAuthor%3A%20Mohammed%20Baharoon%20and%20Siavash%20Raissi%20and%20John%20S.%20Jun%20and%20Thibault%20Heintz%20and%20Mahmoud%20Alabbad%20and%20Ali%20Alburkani%20and%20Sung%20Eun%20Kim%20and%20Kent%20Kleinschmidt%20and%20Abdulrahman%20O.%20Alhumaydhi%20and%20Mohannad%20Mohammed%20G.%20Alghamdi%20and%20Jeremy%20Francis%20Palacio%20and%20Mohammed%20Bukhaytan%20and%20Noah%20Michael%20Prudlo%20and%20Rithvik%20Akula%20and%20Brady%20Chrisler%20and%20Benjamin%20Galligos%20and%20Mohammed%20O.%20Almutairi%20and%20Mazeen%20Mohammed%20Alanazi%20and%20Nasser%20M.%20Alrashdi%20and%20Joel%20Jihwan%20Hwang%20and%20Sri%20Sai%20Dinesh%20Jaliparthi%20and%20Luke%20David%20Nelson%20and%20Nathaniel%20Nguyen%20and%20Sathvik%20Suryadevara%20and%20Steven%20Kim%20and%20Mohammed%20F.%20Mohammed%20and%20Yevgeniy%20R.%20Semenov%20and%20Kun-Hsing%20Yu%20and%20Abdulrhman%20Aljouie%20and%20Hassan%20AlOmaish%20and%20Adam%20Rodman%20and%20Pranav%20Rajpurkar%0AAbstract%3A%20%20%20We%20introduce%20RadGame%2C%20an%20AI-powered%20gamified%20platform%20for%20radiology%20education%0Athat%20targets%20two%20core%20skills%3A%20localizing%20findings%20and%20generating%20reports.%0ATraditional%20radiology%20training%20is%20based%20on%20passive%20exposure%20to%20cases%20or%20active%0Apractice%20with%20real-time%20input%20from%20supervising%20radiologists%2C%20limiting%0Aopportunities%20for%20immediate%20and%20scalable%20feedback.%20RadGame%20addresses%20this%20gap%0Aby%20combining%20gamification%20with%20large-scale%20public%20datasets%20and%20automated%2C%0AAI-driven%20feedback%20that%20provides%20clear%2C%20structured%20guidance%20to%20human%20learners.%0AIn%20RadGame%20Localize%2C%20players%20draw%20bounding%20boxes%20around%20abnormalities%2C%20which%0Aare%20automatically%20compared%20to%20radiologist-drawn%20annotations%20from%20public%0Adatasets%2C%20and%20visual%20explanations%20are%20generated%20by%20vision-language%20models%20for%0Auser%20missed%20findings.%20In%20RadGame%20Report%2C%20players%20compose%20findings%20given%20a%20chest%0AX-ray%2C%20patient%20age%20and%20indication%2C%20and%20receive%20structured%20AI%20feedback%20based%20on%0Aradiology%20report%20generation%20metrics%2C%20highlighting%20errors%20and%20omissions%20compared%0Ato%20a%20radiologist%27s%20written%20ground%20truth%20report%20from%20public%20datasets%2C%20producing%0Aa%20final%20performance%20and%20style%20score.%20In%20a%20prospective%20evaluation%2C%20participants%0Ausing%20RadGame%20achieved%20a%2068%25%20improvement%20in%20localization%20accuracy%20compared%20to%0A17%25%20with%20traditional%20passive%20methods%20and%20a%2031%25%20improvement%20in%20report-writing%0Aaccuracy%20compared%20to%204%25%20with%20traditional%20methods%20after%20seeing%20the%20same%20cases.%0ARadGame%20highlights%20the%20potential%20of%20AI-driven%20gamification%20to%20deliver%20scalable%2C%0Afeedback-rich%20radiology%20training%20and%20reimagines%20the%20application%20of%20medical%20AI%0Aresources%20in%20education.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13270v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRadGame%253A%2520An%2520AI-Powered%2520Platform%2520for%2520Radiology%2520Education%26entry.906535625%3DMohammed%2520Baharoon%2520and%2520Siavash%2520Raissi%2520and%2520John%2520S.%2520Jun%2520and%2520Thibault%2520Heintz%2520and%2520Mahmoud%2520Alabbad%2520and%2520Ali%2520Alburkani%2520and%2520Sung%2520Eun%2520Kim%2520and%2520Kent%2520Kleinschmidt%2520and%2520Abdulrahman%2520O.%2520Alhumaydhi%2520and%2520Mohannad%2520Mohammed%2520G.%2520Alghamdi%2520and%2520Jeremy%2520Francis%2520Palacio%2520and%2520Mohammed%2520Bukhaytan%2520and%2520Noah%2520Michael%2520Prudlo%2520and%2520Rithvik%2520Akula%2520and%2520Brady%2520Chrisler%2520and%2520Benjamin%2520Galligos%2520and%2520Mohammed%2520O.%2520Almutairi%2520and%2520Mazeen%2520Mohammed%2520Alanazi%2520and%2520Nasser%2520M.%2520Alrashdi%2520and%2520Joel%2520Jihwan%2520Hwang%2520and%2520Sri%2520Sai%2520Dinesh%2520Jaliparthi%2520and%2520Luke%2520David%2520Nelson%2520and%2520Nathaniel%2520Nguyen%2520and%2520Sathvik%2520Suryadevara%2520and%2520Steven%2520Kim%2520and%2520Mohammed%2520F.%2520Mohammed%2520and%2520Yevgeniy%2520R.%2520Semenov%2520and%2520Kun-Hsing%2520Yu%2520and%2520Abdulrhman%2520Aljouie%2520and%2520Hassan%2520AlOmaish%2520and%2520Adam%2520Rodman%2520and%2520Pranav%2520Rajpurkar%26entry.1292438233%3D%2520%2520We%2520introduce%2520RadGame%252C%2520an%2520AI-powered%2520gamified%2520platform%2520for%2520radiology%2520education%250Athat%2520targets%2520two%2520core%2520skills%253A%2520localizing%2520findings%2520and%2520generating%2520reports.%250ATraditional%2520radiology%2520training%2520is%2520based%2520on%2520passive%2520exposure%2520to%2520cases%2520or%2520active%250Apractice%2520with%2520real-time%2520input%2520from%2520supervising%2520radiologists%252C%2520limiting%250Aopportunities%2520for%2520immediate%2520and%2520scalable%2520feedback.%2520RadGame%2520addresses%2520this%2520gap%250Aby%2520combining%2520gamification%2520with%2520large-scale%2520public%2520datasets%2520and%2520automated%252C%250AAI-driven%2520feedback%2520that%2520provides%2520clear%252C%2520structured%2520guidance%2520to%2520human%2520learners.%250AIn%2520RadGame%2520Localize%252C%2520players%2520draw%2520bounding%2520boxes%2520around%2520abnormalities%252C%2520which%250Aare%2520automatically%2520compared%2520to%2520radiologist-drawn%2520annotations%2520from%2520public%250Adatasets%252C%2520and%2520visual%2520explanations%2520are%2520generated%2520by%2520vision-language%2520models%2520for%250Auser%2520missed%2520findings.%2520In%2520RadGame%2520Report%252C%2520players%2520compose%2520findings%2520given%2520a%2520chest%250AX-ray%252C%2520patient%2520age%2520and%2520indication%252C%2520and%2520receive%2520structured%2520AI%2520feedback%2520based%2520on%250Aradiology%2520report%2520generation%2520metrics%252C%2520highlighting%2520errors%2520and%2520omissions%2520compared%250Ato%2520a%2520radiologist%2527s%2520written%2520ground%2520truth%2520report%2520from%2520public%2520datasets%252C%2520producing%250Aa%2520final%2520performance%2520and%2520style%2520score.%2520In%2520a%2520prospective%2520evaluation%252C%2520participants%250Ausing%2520RadGame%2520achieved%2520a%252068%2525%2520improvement%2520in%2520localization%2520accuracy%2520compared%2520to%250A17%2525%2520with%2520traditional%2520passive%2520methods%2520and%2520a%252031%2525%2520improvement%2520in%2520report-writing%250Aaccuracy%2520compared%2520to%25204%2525%2520with%2520traditional%2520methods%2520after%2520seeing%2520the%2520same%2520cases.%250ARadGame%2520highlights%2520the%2520potential%2520of%2520AI-driven%2520gamification%2520to%2520deliver%2520scalable%252C%250Afeedback-rich%2520radiology%2520training%2520and%2520reimagines%2520the%2520application%2520of%2520medical%2520AI%250Aresources%2520in%2520education.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13270v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RadGame%3A%20An%20AI-Powered%20Platform%20for%20Radiology%20Education&entry.906535625=Mohammed%20Baharoon%20and%20Siavash%20Raissi%20and%20John%20S.%20Jun%20and%20Thibault%20Heintz%20and%20Mahmoud%20Alabbad%20and%20Ali%20Alburkani%20and%20Sung%20Eun%20Kim%20and%20Kent%20Kleinschmidt%20and%20Abdulrahman%20O.%20Alhumaydhi%20and%20Mohannad%20Mohammed%20G.%20Alghamdi%20and%20Jeremy%20Francis%20Palacio%20and%20Mohammed%20Bukhaytan%20and%20Noah%20Michael%20Prudlo%20and%20Rithvik%20Akula%20and%20Brady%20Chrisler%20and%20Benjamin%20Galligos%20and%20Mohammed%20O.%20Almutairi%20and%20Mazeen%20Mohammed%20Alanazi%20and%20Nasser%20M.%20Alrashdi%20and%20Joel%20Jihwan%20Hwang%20and%20Sri%20Sai%20Dinesh%20Jaliparthi%20and%20Luke%20David%20Nelson%20and%20Nathaniel%20Nguyen%20and%20Sathvik%20Suryadevara%20and%20Steven%20Kim%20and%20Mohammed%20F.%20Mohammed%20and%20Yevgeniy%20R.%20Semenov%20and%20Kun-Hsing%20Yu%20and%20Abdulrhman%20Aljouie%20and%20Hassan%20AlOmaish%20and%20Adam%20Rodman%20and%20Pranav%20Rajpurkar&entry.1292438233=%20%20We%20introduce%20RadGame%2C%20an%20AI-powered%20gamified%20platform%20for%20radiology%20education%0Athat%20targets%20two%20core%20skills%3A%20localizing%20findings%20and%20generating%20reports.%0ATraditional%20radiology%20training%20is%20based%20on%20passive%20exposure%20to%20cases%20or%20active%0Apractice%20with%20real-time%20input%20from%20supervising%20radiologists%2C%20limiting%0Aopportunities%20for%20immediate%20and%20scalable%20feedback.%20RadGame%20addresses%20this%20gap%0Aby%20combining%20gamification%20with%20large-scale%20public%20datasets%20and%20automated%2C%0AAI-driven%20feedback%20that%20provides%20clear%2C%20structured%20guidance%20to%20human%20learners.%0AIn%20RadGame%20Localize%2C%20players%20draw%20bounding%20boxes%20around%20abnormalities%2C%20which%0Aare%20automatically%20compared%20to%20radiologist-drawn%20annotations%20from%20public%0Adatasets%2C%20and%20visual%20explanations%20are%20generated%20by%20vision-language%20models%20for%0Auser%20missed%20findings.%20In%20RadGame%20Report%2C%20players%20compose%20findings%20given%20a%20chest%0AX-ray%2C%20patient%20age%20and%20indication%2C%20and%20receive%20structured%20AI%20feedback%20based%20on%0Aradiology%20report%20generation%20metrics%2C%20highlighting%20errors%20and%20omissions%20compared%0Ato%20a%20radiologist%27s%20written%20ground%20truth%20report%20from%20public%20datasets%2C%20producing%0Aa%20final%20performance%20and%20style%20score.%20In%20a%20prospective%20evaluation%2C%20participants%0Ausing%20RadGame%20achieved%20a%2068%25%20improvement%20in%20localization%20accuracy%20compared%20to%0A17%25%20with%20traditional%20passive%20methods%20and%20a%2031%25%20improvement%20in%20report-writing%0Aaccuracy%20compared%20to%204%25%20with%20traditional%20methods%20after%20seeing%20the%20same%20cases.%0ARadGame%20highlights%20the%20potential%20of%20AI-driven%20gamification%20to%20deliver%20scalable%2C%0Afeedback-rich%20radiology%20training%20and%20reimagines%20the%20application%20of%20medical%20AI%0Aresources%20in%20education.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13270v1&entry.124074799=Read"},
{"title": "BAPFL: Exploring Backdoor Attacks Against Prototype-based Federated\n  Learning", "author": "Honghong Zeng and Jiong Lou and Zhe Wang and Hefeng Zhou and Chentao Wu and Wei Zhao and Jie Li", "abstract": "  Prototype-based federated learning (PFL) has emerged as a promising paradigm\nto address data heterogeneity problems in federated learning, as it leverages\nmean feature vectors as prototypes to enhance model generalization. However,\nits robustness against backdoor attacks remains largely unexplored. In this\npaper, we identify that PFL is inherently resistant to existing backdoor\nattacks due to its unique prototype learning mechanism and local data\nheterogeneity. To further explore the security of PFL, we propose BAPFL, the\nfirst backdoor attack method specifically designed for PFL frameworks. BAPFL\nintegrates a prototype poisoning strategy with a trigger optimization\nmechanism. The prototype poisoning strategy manipulates the trajectories of\nglobal prototypes to mislead the prototype training of benign clients, pushing\ntheir local prototypes of clean samples away from the prototypes of\ntrigger-embedded samples. Meanwhile, the trigger optimization mechanism learns\na unique and stealthy trigger for each potential target label, and guides the\nprototypes of trigger-embedded samples to align closely with the global\nprototype of the target label. Experimental results across multiple datasets\nand PFL variants demonstrate that BAPFL achieves a 35\\%-75\\% improvement in\nattack success rate compared to traditional backdoor attacks, while preserving\nmain task accuracy. These results highlight the effectiveness, stealthiness,\nand adaptability of BAPFL in PFL.\n", "link": "http://arxiv.org/abs/2509.12964v1", "date": "2025-09-16", "relevancy": 2.3288, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.494}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4699}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BAPFL%3A%20Exploring%20Backdoor%20Attacks%20Against%20Prototype-based%20Federated%0A%20%20Learning&body=Title%3A%20BAPFL%3A%20Exploring%20Backdoor%20Attacks%20Against%20Prototype-based%20Federated%0A%20%20Learning%0AAuthor%3A%20Honghong%20Zeng%20and%20Jiong%20Lou%20and%20Zhe%20Wang%20and%20Hefeng%20Zhou%20and%20Chentao%20Wu%20and%20Wei%20Zhao%20and%20Jie%20Li%0AAbstract%3A%20%20%20Prototype-based%20federated%20learning%20%28PFL%29%20has%20emerged%20as%20a%20promising%20paradigm%0Ato%20address%20data%20heterogeneity%20problems%20in%20federated%20learning%2C%20as%20it%20leverages%0Amean%20feature%20vectors%20as%20prototypes%20to%20enhance%20model%20generalization.%20However%2C%0Aits%20robustness%20against%20backdoor%20attacks%20remains%20largely%20unexplored.%20In%20this%0Apaper%2C%20we%20identify%20that%20PFL%20is%20inherently%20resistant%20to%20existing%20backdoor%0Aattacks%20due%20to%20its%20unique%20prototype%20learning%20mechanism%20and%20local%20data%0Aheterogeneity.%20To%20further%20explore%20the%20security%20of%20PFL%2C%20we%20propose%20BAPFL%2C%20the%0Afirst%20backdoor%20attack%20method%20specifically%20designed%20for%20PFL%20frameworks.%20BAPFL%0Aintegrates%20a%20prototype%20poisoning%20strategy%20with%20a%20trigger%20optimization%0Amechanism.%20The%20prototype%20poisoning%20strategy%20manipulates%20the%20trajectories%20of%0Aglobal%20prototypes%20to%20mislead%20the%20prototype%20training%20of%20benign%20clients%2C%20pushing%0Atheir%20local%20prototypes%20of%20clean%20samples%20away%20from%20the%20prototypes%20of%0Atrigger-embedded%20samples.%20Meanwhile%2C%20the%20trigger%20optimization%20mechanism%20learns%0Aa%20unique%20and%20stealthy%20trigger%20for%20each%20potential%20target%20label%2C%20and%20guides%20the%0Aprototypes%20of%20trigger-embedded%20samples%20to%20align%20closely%20with%20the%20global%0Aprototype%20of%20the%20target%20label.%20Experimental%20results%20across%20multiple%20datasets%0Aand%20PFL%20variants%20demonstrate%20that%20BAPFL%20achieves%20a%2035%5C%25-75%5C%25%20improvement%20in%0Aattack%20success%20rate%20compared%20to%20traditional%20backdoor%20attacks%2C%20while%20preserving%0Amain%20task%20accuracy.%20These%20results%20highlight%20the%20effectiveness%2C%20stealthiness%2C%0Aand%20adaptability%20of%20BAPFL%20in%20PFL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBAPFL%253A%2520Exploring%2520Backdoor%2520Attacks%2520Against%2520Prototype-based%2520Federated%250A%2520%2520Learning%26entry.906535625%3DHonghong%2520Zeng%2520and%2520Jiong%2520Lou%2520and%2520Zhe%2520Wang%2520and%2520Hefeng%2520Zhou%2520and%2520Chentao%2520Wu%2520and%2520Wei%2520Zhao%2520and%2520Jie%2520Li%26entry.1292438233%3D%2520%2520Prototype-based%2520federated%2520learning%2520%2528PFL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%250Ato%2520address%2520data%2520heterogeneity%2520problems%2520in%2520federated%2520learning%252C%2520as%2520it%2520leverages%250Amean%2520feature%2520vectors%2520as%2520prototypes%2520to%2520enhance%2520model%2520generalization.%2520However%252C%250Aits%2520robustness%2520against%2520backdoor%2520attacks%2520remains%2520largely%2520unexplored.%2520In%2520this%250Apaper%252C%2520we%2520identify%2520that%2520PFL%2520is%2520inherently%2520resistant%2520to%2520existing%2520backdoor%250Aattacks%2520due%2520to%2520its%2520unique%2520prototype%2520learning%2520mechanism%2520and%2520local%2520data%250Aheterogeneity.%2520To%2520further%2520explore%2520the%2520security%2520of%2520PFL%252C%2520we%2520propose%2520BAPFL%252C%2520the%250Afirst%2520backdoor%2520attack%2520method%2520specifically%2520designed%2520for%2520PFL%2520frameworks.%2520BAPFL%250Aintegrates%2520a%2520prototype%2520poisoning%2520strategy%2520with%2520a%2520trigger%2520optimization%250Amechanism.%2520The%2520prototype%2520poisoning%2520strategy%2520manipulates%2520the%2520trajectories%2520of%250Aglobal%2520prototypes%2520to%2520mislead%2520the%2520prototype%2520training%2520of%2520benign%2520clients%252C%2520pushing%250Atheir%2520local%2520prototypes%2520of%2520clean%2520samples%2520away%2520from%2520the%2520prototypes%2520of%250Atrigger-embedded%2520samples.%2520Meanwhile%252C%2520the%2520trigger%2520optimization%2520mechanism%2520learns%250Aa%2520unique%2520and%2520stealthy%2520trigger%2520for%2520each%2520potential%2520target%2520label%252C%2520and%2520guides%2520the%250Aprototypes%2520of%2520trigger-embedded%2520samples%2520to%2520align%2520closely%2520with%2520the%2520global%250Aprototype%2520of%2520the%2520target%2520label.%2520Experimental%2520results%2520across%2520multiple%2520datasets%250Aand%2520PFL%2520variants%2520demonstrate%2520that%2520BAPFL%2520achieves%2520a%252035%255C%2525-75%255C%2525%2520improvement%2520in%250Aattack%2520success%2520rate%2520compared%2520to%2520traditional%2520backdoor%2520attacks%252C%2520while%2520preserving%250Amain%2520task%2520accuracy.%2520These%2520results%2520highlight%2520the%2520effectiveness%252C%2520stealthiness%252C%250Aand%2520adaptability%2520of%2520BAPFL%2520in%2520PFL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BAPFL%3A%20Exploring%20Backdoor%20Attacks%20Against%20Prototype-based%20Federated%0A%20%20Learning&entry.906535625=Honghong%20Zeng%20and%20Jiong%20Lou%20and%20Zhe%20Wang%20and%20Hefeng%20Zhou%20and%20Chentao%20Wu%20and%20Wei%20Zhao%20and%20Jie%20Li&entry.1292438233=%20%20Prototype-based%20federated%20learning%20%28PFL%29%20has%20emerged%20as%20a%20promising%20paradigm%0Ato%20address%20data%20heterogeneity%20problems%20in%20federated%20learning%2C%20as%20it%20leverages%0Amean%20feature%20vectors%20as%20prototypes%20to%20enhance%20model%20generalization.%20However%2C%0Aits%20robustness%20against%20backdoor%20attacks%20remains%20largely%20unexplored.%20In%20this%0Apaper%2C%20we%20identify%20that%20PFL%20is%20inherently%20resistant%20to%20existing%20backdoor%0Aattacks%20due%20to%20its%20unique%20prototype%20learning%20mechanism%20and%20local%20data%0Aheterogeneity.%20To%20further%20explore%20the%20security%20of%20PFL%2C%20we%20propose%20BAPFL%2C%20the%0Afirst%20backdoor%20attack%20method%20specifically%20designed%20for%20PFL%20frameworks.%20BAPFL%0Aintegrates%20a%20prototype%20poisoning%20strategy%20with%20a%20trigger%20optimization%0Amechanism.%20The%20prototype%20poisoning%20strategy%20manipulates%20the%20trajectories%20of%0Aglobal%20prototypes%20to%20mislead%20the%20prototype%20training%20of%20benign%20clients%2C%20pushing%0Atheir%20local%20prototypes%20of%20clean%20samples%20away%20from%20the%20prototypes%20of%0Atrigger-embedded%20samples.%20Meanwhile%2C%20the%20trigger%20optimization%20mechanism%20learns%0Aa%20unique%20and%20stealthy%20trigger%20for%20each%20potential%20target%20label%2C%20and%20guides%20the%0Aprototypes%20of%20trigger-embedded%20samples%20to%20align%20closely%20with%20the%20global%0Aprototype%20of%20the%20target%20label.%20Experimental%20results%20across%20multiple%20datasets%0Aand%20PFL%20variants%20demonstrate%20that%20BAPFL%20achieves%20a%2035%5C%25-75%5C%25%20improvement%20in%0Aattack%20success%20rate%20compared%20to%20traditional%20backdoor%20attacks%2C%20while%20preserving%0Amain%20task%20accuracy.%20These%20results%20highlight%20the%20effectiveness%2C%20stealthiness%2C%0Aand%20adaptability%20of%20BAPFL%20in%20PFL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12964v1&entry.124074799=Read"},
{"title": "Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation\n  with Uncertainty-Guided Pseudo-Labeling", "author": "Yunyao Lu and Yihang Wu and Ahmad Chaddad and Tareef Daqqaq and Reem Kateb", "abstract": "  Despite the remarkable performance of supervised medical image segmentation\nmodels, relying on a large amount of labeled data is impractical in real-world\nsituations. Semi-supervised learning approaches aim to alleviate this challenge\nusing unlabeled data through pseudo-label generation. Yet, existing\nsemi-supervised segmentation methods still suffer from noisy pseudo-labels and\ninsufficient supervision within the feature space. To solve these challenges,\nthis paper proposes a novel semi-supervised 3D medical image segmentation\nframework based on a dual-network architecture. Specifically, we investigate a\nCross Consistency Enhancement module using both cross pseudo and\nentropy-filtered supervision to reduce the noisy pseudo-labels, while we design\na dynamic weighting strategy to adjust the contributions of pseudo-labels using\nan uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). In\naddition, we use a self-supervised contrastive learning mechanism to align\nuncertain voxel features with reliable class prototypes by effectively\ndifferentiating between trustworthy and uncertain predictions, thus reducing\nprediction uncertainty. Extensive experiments are conducted on three 3D\nsegmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposed\napproach consistently exhibits superior performance across various settings\n(e.g., 89.95\\% Dice score on left Atrial with 10\\% labeled data) compared to\nthe state-of-the-art methods. Furthermore, the usefulness of the proposed\nmodules is further validated via ablation experiments.\n", "link": "http://arxiv.org/abs/2509.13084v1", "date": "2025-09-16", "relevancy": 2.3277, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6773}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.578}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Dual%20Network%20Based%20Semi-Supervised%20Medical%20Image%20Segmentation%0A%20%20with%20Uncertainty-Guided%20Pseudo-Labeling&body=Title%3A%20Enhancing%20Dual%20Network%20Based%20Semi-Supervised%20Medical%20Image%20Segmentation%0A%20%20with%20Uncertainty-Guided%20Pseudo-Labeling%0AAuthor%3A%20Yunyao%20Lu%20and%20Yihang%20Wu%20and%20Ahmad%20Chaddad%20and%20Tareef%20Daqqaq%20and%20Reem%20Kateb%0AAbstract%3A%20%20%20Despite%20the%20remarkable%20performance%20of%20supervised%20medical%20image%20segmentation%0Amodels%2C%20relying%20on%20a%20large%20amount%20of%20labeled%20data%20is%20impractical%20in%20real-world%0Asituations.%20Semi-supervised%20learning%20approaches%20aim%20to%20alleviate%20this%20challenge%0Ausing%20unlabeled%20data%20through%20pseudo-label%20generation.%20Yet%2C%20existing%0Asemi-supervised%20segmentation%20methods%20still%20suffer%20from%20noisy%20pseudo-labels%20and%0Ainsufficient%20supervision%20within%20the%20feature%20space.%20To%20solve%20these%20challenges%2C%0Athis%20paper%20proposes%20a%20novel%20semi-supervised%203D%20medical%20image%20segmentation%0Aframework%20based%20on%20a%20dual-network%20architecture.%20Specifically%2C%20we%20investigate%20a%0ACross%20Consistency%20Enhancement%20module%20using%20both%20cross%20pseudo%20and%0Aentropy-filtered%20supervision%20to%20reduce%20the%20noisy%20pseudo-labels%2C%20while%20we%20design%0Aa%20dynamic%20weighting%20strategy%20to%20adjust%20the%20contributions%20of%20pseudo-labels%20using%0Aan%20uncertainty-aware%20mechanism%20%28i.e.%2C%20Kullback-Leibler%20divergence%29.%20In%0Aaddition%2C%20we%20use%20a%20self-supervised%20contrastive%20learning%20mechanism%20to%20align%0Auncertain%20voxel%20features%20with%20reliable%20class%20prototypes%20by%20effectively%0Adifferentiating%20between%20trustworthy%20and%20uncertain%20predictions%2C%20thus%20reducing%0Aprediction%20uncertainty.%20Extensive%20experiments%20are%20conducted%20on%20three%203D%0Asegmentation%20datasets%2C%20Left%20Atrial%2C%20NIH%20Pancreas%20and%20BraTS-2019.%20The%20proposed%0Aapproach%20consistently%20exhibits%20superior%20performance%20across%20various%20settings%0A%28e.g.%2C%2089.95%5C%25%20Dice%20score%20on%20left%20Atrial%20with%2010%5C%25%20labeled%20data%29%20compared%20to%0Athe%20state-of-the-art%20methods.%20Furthermore%2C%20the%20usefulness%20of%20the%20proposed%0Amodules%20is%20further%20validated%20via%20ablation%20experiments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Dual%2520Network%2520Based%2520Semi-Supervised%2520Medical%2520Image%2520Segmentation%250A%2520%2520with%2520Uncertainty-Guided%2520Pseudo-Labeling%26entry.906535625%3DYunyao%2520Lu%2520and%2520Yihang%2520Wu%2520and%2520Ahmad%2520Chaddad%2520and%2520Tareef%2520Daqqaq%2520and%2520Reem%2520Kateb%26entry.1292438233%3D%2520%2520Despite%2520the%2520remarkable%2520performance%2520of%2520supervised%2520medical%2520image%2520segmentation%250Amodels%252C%2520relying%2520on%2520a%2520large%2520amount%2520of%2520labeled%2520data%2520is%2520impractical%2520in%2520real-world%250Asituations.%2520Semi-supervised%2520learning%2520approaches%2520aim%2520to%2520alleviate%2520this%2520challenge%250Ausing%2520unlabeled%2520data%2520through%2520pseudo-label%2520generation.%2520Yet%252C%2520existing%250Asemi-supervised%2520segmentation%2520methods%2520still%2520suffer%2520from%2520noisy%2520pseudo-labels%2520and%250Ainsufficient%2520supervision%2520within%2520the%2520feature%2520space.%2520To%2520solve%2520these%2520challenges%252C%250Athis%2520paper%2520proposes%2520a%2520novel%2520semi-supervised%25203D%2520medical%2520image%2520segmentation%250Aframework%2520based%2520on%2520a%2520dual-network%2520architecture.%2520Specifically%252C%2520we%2520investigate%2520a%250ACross%2520Consistency%2520Enhancement%2520module%2520using%2520both%2520cross%2520pseudo%2520and%250Aentropy-filtered%2520supervision%2520to%2520reduce%2520the%2520noisy%2520pseudo-labels%252C%2520while%2520we%2520design%250Aa%2520dynamic%2520weighting%2520strategy%2520to%2520adjust%2520the%2520contributions%2520of%2520pseudo-labels%2520using%250Aan%2520uncertainty-aware%2520mechanism%2520%2528i.e.%252C%2520Kullback-Leibler%2520divergence%2529.%2520In%250Aaddition%252C%2520we%2520use%2520a%2520self-supervised%2520contrastive%2520learning%2520mechanism%2520to%2520align%250Auncertain%2520voxel%2520features%2520with%2520reliable%2520class%2520prototypes%2520by%2520effectively%250Adifferentiating%2520between%2520trustworthy%2520and%2520uncertain%2520predictions%252C%2520thus%2520reducing%250Aprediction%2520uncertainty.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520three%25203D%250Asegmentation%2520datasets%252C%2520Left%2520Atrial%252C%2520NIH%2520Pancreas%2520and%2520BraTS-2019.%2520The%2520proposed%250Aapproach%2520consistently%2520exhibits%2520superior%2520performance%2520across%2520various%2520settings%250A%2528e.g.%252C%252089.95%255C%2525%2520Dice%2520score%2520on%2520left%2520Atrial%2520with%252010%255C%2525%2520labeled%2520data%2529%2520compared%2520to%250Athe%2520state-of-the-art%2520methods.%2520Furthermore%252C%2520the%2520usefulness%2520of%2520the%2520proposed%250Amodules%2520is%2520further%2520validated%2520via%2520ablation%2520experiments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Dual%20Network%20Based%20Semi-Supervised%20Medical%20Image%20Segmentation%0A%20%20with%20Uncertainty-Guided%20Pseudo-Labeling&entry.906535625=Yunyao%20Lu%20and%20Yihang%20Wu%20and%20Ahmad%20Chaddad%20and%20Tareef%20Daqqaq%20and%20Reem%20Kateb&entry.1292438233=%20%20Despite%20the%20remarkable%20performance%20of%20supervised%20medical%20image%20segmentation%0Amodels%2C%20relying%20on%20a%20large%20amount%20of%20labeled%20data%20is%20impractical%20in%20real-world%0Asituations.%20Semi-supervised%20learning%20approaches%20aim%20to%20alleviate%20this%20challenge%0Ausing%20unlabeled%20data%20through%20pseudo-label%20generation.%20Yet%2C%20existing%0Asemi-supervised%20segmentation%20methods%20still%20suffer%20from%20noisy%20pseudo-labels%20and%0Ainsufficient%20supervision%20within%20the%20feature%20space.%20To%20solve%20these%20challenges%2C%0Athis%20paper%20proposes%20a%20novel%20semi-supervised%203D%20medical%20image%20segmentation%0Aframework%20based%20on%20a%20dual-network%20architecture.%20Specifically%2C%20we%20investigate%20a%0ACross%20Consistency%20Enhancement%20module%20using%20both%20cross%20pseudo%20and%0Aentropy-filtered%20supervision%20to%20reduce%20the%20noisy%20pseudo-labels%2C%20while%20we%20design%0Aa%20dynamic%20weighting%20strategy%20to%20adjust%20the%20contributions%20of%20pseudo-labels%20using%0Aan%20uncertainty-aware%20mechanism%20%28i.e.%2C%20Kullback-Leibler%20divergence%29.%20In%0Aaddition%2C%20we%20use%20a%20self-supervised%20contrastive%20learning%20mechanism%20to%20align%0Auncertain%20voxel%20features%20with%20reliable%20class%20prototypes%20by%20effectively%0Adifferentiating%20between%20trustworthy%20and%20uncertain%20predictions%2C%20thus%20reducing%0Aprediction%20uncertainty.%20Extensive%20experiments%20are%20conducted%20on%20three%203D%0Asegmentation%20datasets%2C%20Left%20Atrial%2C%20NIH%20Pancreas%20and%20BraTS-2019.%20The%20proposed%0Aapproach%20consistently%20exhibits%20superior%20performance%20across%20various%20settings%0A%28e.g.%2C%2089.95%5C%25%20Dice%20score%20on%20left%20Atrial%20with%2010%5C%25%20labeled%20data%29%20compared%20to%0Athe%20state-of-the-art%20methods.%20Furthermore%2C%20the%20usefulness%20of%20the%20proposed%0Amodules%20is%20further%20validated%20via%20ablation%20experiments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13084v1&entry.124074799=Read"},
{"title": "Context-Aware Membership Inference Attacks against Pre-trained Large\n  Language Models", "author": "Hongyan Chang and Ali Shahin Shamsabadi and Kleomenis Katevas and Hamed Haddadi and Reza Shokri", "abstract": "  Membership Inference Attacks (MIAs) on pre-trained Large Language Models\n(LLMs) aim at determining if a data point was part of the model's training set.\nPrior MIAs that are built for classification models fail at LLMs, due to\nignoring the generative nature of LLMs across token sequences. In this paper,\nwe present a novel attack on pre-trained LLMs that adapts MIA statistical tests\nto the perplexity dynamics of subsequences within a data point. Our method\nsignificantly outperforms prior approaches, revealing context-dependent\nmemorization patterns in pre-trained LLMs.\n", "link": "http://arxiv.org/abs/2409.13745v2", "date": "2025-09-16", "relevancy": 2.32, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4707}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4606}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Aware%20Membership%20Inference%20Attacks%20against%20Pre-trained%20Large%0A%20%20Language%20Models&body=Title%3A%20Context-Aware%20Membership%20Inference%20Attacks%20against%20Pre-trained%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Hongyan%20Chang%20and%20Ali%20Shahin%20Shamsabadi%20and%20Kleomenis%20Katevas%20and%20Hamed%20Haddadi%20and%20Reza%20Shokri%0AAbstract%3A%20%20%20Membership%20Inference%20Attacks%20%28MIAs%29%20on%20pre-trained%20Large%20Language%20Models%0A%28LLMs%29%20aim%20at%20determining%20if%20a%20data%20point%20was%20part%20of%20the%20model%27s%20training%20set.%0APrior%20MIAs%20that%20are%20built%20for%20classification%20models%20fail%20at%20LLMs%2C%20due%20to%0Aignoring%20the%20generative%20nature%20of%20LLMs%20across%20token%20sequences.%20In%20this%20paper%2C%0Awe%20present%20a%20novel%20attack%20on%20pre-trained%20LLMs%20that%20adapts%20MIA%20statistical%20tests%0Ato%20the%20perplexity%20dynamics%20of%20subsequences%20within%20a%20data%20point.%20Our%20method%0Asignificantly%20outperforms%20prior%20approaches%2C%20revealing%20context-dependent%0Amemorization%20patterns%20in%20pre-trained%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.13745v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Aware%2520Membership%2520Inference%2520Attacks%2520against%2520Pre-trained%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DHongyan%2520Chang%2520and%2520Ali%2520Shahin%2520Shamsabadi%2520and%2520Kleomenis%2520Katevas%2520and%2520Hamed%2520Haddadi%2520and%2520Reza%2520Shokri%26entry.1292438233%3D%2520%2520Membership%2520Inference%2520Attacks%2520%2528MIAs%2529%2520on%2520pre-trained%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520aim%2520at%2520determining%2520if%2520a%2520data%2520point%2520was%2520part%2520of%2520the%2520model%2527s%2520training%2520set.%250APrior%2520MIAs%2520that%2520are%2520built%2520for%2520classification%2520models%2520fail%2520at%2520LLMs%252C%2520due%2520to%250Aignoring%2520the%2520generative%2520nature%2520of%2520LLMs%2520across%2520token%2520sequences.%2520In%2520this%2520paper%252C%250Awe%2520present%2520a%2520novel%2520attack%2520on%2520pre-trained%2520LLMs%2520that%2520adapts%2520MIA%2520statistical%2520tests%250Ato%2520the%2520perplexity%2520dynamics%2520of%2520subsequences%2520within%2520a%2520data%2520point.%2520Our%2520method%250Asignificantly%2520outperforms%2520prior%2520approaches%252C%2520revealing%2520context-dependent%250Amemorization%2520patterns%2520in%2520pre-trained%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.13745v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Aware%20Membership%20Inference%20Attacks%20against%20Pre-trained%20Large%0A%20%20Language%20Models&entry.906535625=Hongyan%20Chang%20and%20Ali%20Shahin%20Shamsabadi%20and%20Kleomenis%20Katevas%20and%20Hamed%20Haddadi%20and%20Reza%20Shokri&entry.1292438233=%20%20Membership%20Inference%20Attacks%20%28MIAs%29%20on%20pre-trained%20Large%20Language%20Models%0A%28LLMs%29%20aim%20at%20determining%20if%20a%20data%20point%20was%20part%20of%20the%20model%27s%20training%20set.%0APrior%20MIAs%20that%20are%20built%20for%20classification%20models%20fail%20at%20LLMs%2C%20due%20to%0Aignoring%20the%20generative%20nature%20of%20LLMs%20across%20token%20sequences.%20In%20this%20paper%2C%0Awe%20present%20a%20novel%20attack%20on%20pre-trained%20LLMs%20that%20adapts%20MIA%20statistical%20tests%0Ato%20the%20perplexity%20dynamics%20of%20subsequences%20within%20a%20data%20point.%20Our%20method%0Asignificantly%20outperforms%20prior%20approaches%2C%20revealing%20context-dependent%0Amemorization%20patterns%20in%20pre-trained%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.13745v2&entry.124074799=Read"},
{"title": "DVDP: An End-to-End Policy for Mobile Robot Visual Docking with RGB-D\n  Perception", "author": "Haohan Min and Zhoujian Li and Yu Yang and Jinyu Chen and Shenghai Yuan", "abstract": "  Automatic docking has long been a significant challenge in the field of\nmobile robotics. Compared to other automatic docking methods, visual docking\nmethods offer higher precision and lower deployment costs, making them an\nefficient and promising choice for this task. However, visual docking methods\nimpose strict requirements on the robot's initial position at the start of the\ndocking process. To overcome the limitations of current vision-based methods,\nwe propose an innovative end-to-end visual docking method named DVDP(direct\nvisual docking policy). This approach requires only a binocular RGB-D camera\ninstalled on the mobile robot to directly output the robot's docking path,\nachieving end-to-end automatic docking. Furthermore, we have collected a\nlarge-scale dataset of mobile robot visual automatic docking dataset through a\ncombination of virtual and real environments using the Unity 3D platform and\nactual mobile robot setups. We developed a series of evaluation metrics to\nquantify the performance of the end-to-end visual docking method. Extensive\nexperiments, including benchmarks against leading perception backbones adapted\ninto our framework, demonstrate that our method achieves superior performance.\nFinally, real-world deployment on the SCOUT Mini confirmed DVDP's efficacy,\nwith our model generating smooth, feasible docking trajectories that meet\nphysical constraints and reach the target pose.\n", "link": "http://arxiv.org/abs/2509.13024v1", "date": "2025-09-16", "relevancy": 2.3038, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5773}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5772}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5741}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DVDP%3A%20An%20End-to-End%20Policy%20for%20Mobile%20Robot%20Visual%20Docking%20with%20RGB-D%0A%20%20Perception&body=Title%3A%20DVDP%3A%20An%20End-to-End%20Policy%20for%20Mobile%20Robot%20Visual%20Docking%20with%20RGB-D%0A%20%20Perception%0AAuthor%3A%20Haohan%20Min%20and%20Zhoujian%20Li%20and%20Yu%20Yang%20and%20Jinyu%20Chen%20and%20Shenghai%20Yuan%0AAbstract%3A%20%20%20Automatic%20docking%20has%20long%20been%20a%20significant%20challenge%20in%20the%20field%20of%0Amobile%20robotics.%20Compared%20to%20other%20automatic%20docking%20methods%2C%20visual%20docking%0Amethods%20offer%20higher%20precision%20and%20lower%20deployment%20costs%2C%20making%20them%20an%0Aefficient%20and%20promising%20choice%20for%20this%20task.%20However%2C%20visual%20docking%20methods%0Aimpose%20strict%20requirements%20on%20the%20robot%27s%20initial%20position%20at%20the%20start%20of%20the%0Adocking%20process.%20To%20overcome%20the%20limitations%20of%20current%20vision-based%20methods%2C%0Awe%20propose%20an%20innovative%20end-to-end%20visual%20docking%20method%20named%20DVDP%28direct%0Avisual%20docking%20policy%29.%20This%20approach%20requires%20only%20a%20binocular%20RGB-D%20camera%0Ainstalled%20on%20the%20mobile%20robot%20to%20directly%20output%20the%20robot%27s%20docking%20path%2C%0Aachieving%20end-to-end%20automatic%20docking.%20Furthermore%2C%20we%20have%20collected%20a%0Alarge-scale%20dataset%20of%20mobile%20robot%20visual%20automatic%20docking%20dataset%20through%20a%0Acombination%20of%20virtual%20and%20real%20environments%20using%20the%20Unity%203D%20platform%20and%0Aactual%20mobile%20robot%20setups.%20We%20developed%20a%20series%20of%20evaluation%20metrics%20to%0Aquantify%20the%20performance%20of%20the%20end-to-end%20visual%20docking%20method.%20Extensive%0Aexperiments%2C%20including%20benchmarks%20against%20leading%20perception%20backbones%20adapted%0Ainto%20our%20framework%2C%20demonstrate%20that%20our%20method%20achieves%20superior%20performance.%0AFinally%2C%20real-world%20deployment%20on%20the%20SCOUT%20Mini%20confirmed%20DVDP%27s%20efficacy%2C%0Awith%20our%20model%20generating%20smooth%2C%20feasible%20docking%20trajectories%20that%20meet%0Aphysical%20constraints%20and%20reach%20the%20target%20pose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13024v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDVDP%253A%2520An%2520End-to-End%2520Policy%2520for%2520Mobile%2520Robot%2520Visual%2520Docking%2520with%2520RGB-D%250A%2520%2520Perception%26entry.906535625%3DHaohan%2520Min%2520and%2520Zhoujian%2520Li%2520and%2520Yu%2520Yang%2520and%2520Jinyu%2520Chen%2520and%2520Shenghai%2520Yuan%26entry.1292438233%3D%2520%2520Automatic%2520docking%2520has%2520long%2520been%2520a%2520significant%2520challenge%2520in%2520the%2520field%2520of%250Amobile%2520robotics.%2520Compared%2520to%2520other%2520automatic%2520docking%2520methods%252C%2520visual%2520docking%250Amethods%2520offer%2520higher%2520precision%2520and%2520lower%2520deployment%2520costs%252C%2520making%2520them%2520an%250Aefficient%2520and%2520promising%2520choice%2520for%2520this%2520task.%2520However%252C%2520visual%2520docking%2520methods%250Aimpose%2520strict%2520requirements%2520on%2520the%2520robot%2527s%2520initial%2520position%2520at%2520the%2520start%2520of%2520the%250Adocking%2520process.%2520To%2520overcome%2520the%2520limitations%2520of%2520current%2520vision-based%2520methods%252C%250Awe%2520propose%2520an%2520innovative%2520end-to-end%2520visual%2520docking%2520method%2520named%2520DVDP%2528direct%250Avisual%2520docking%2520policy%2529.%2520This%2520approach%2520requires%2520only%2520a%2520binocular%2520RGB-D%2520camera%250Ainstalled%2520on%2520the%2520mobile%2520robot%2520to%2520directly%2520output%2520the%2520robot%2527s%2520docking%2520path%252C%250Aachieving%2520end-to-end%2520automatic%2520docking.%2520Furthermore%252C%2520we%2520have%2520collected%2520a%250Alarge-scale%2520dataset%2520of%2520mobile%2520robot%2520visual%2520automatic%2520docking%2520dataset%2520through%2520a%250Acombination%2520of%2520virtual%2520and%2520real%2520environments%2520using%2520the%2520Unity%25203D%2520platform%2520and%250Aactual%2520mobile%2520robot%2520setups.%2520We%2520developed%2520a%2520series%2520of%2520evaluation%2520metrics%2520to%250Aquantify%2520the%2520performance%2520of%2520the%2520end-to-end%2520visual%2520docking%2520method.%2520Extensive%250Aexperiments%252C%2520including%2520benchmarks%2520against%2520leading%2520perception%2520backbones%2520adapted%250Ainto%2520our%2520framework%252C%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%2520performance.%250AFinally%252C%2520real-world%2520deployment%2520on%2520the%2520SCOUT%2520Mini%2520confirmed%2520DVDP%2527s%2520efficacy%252C%250Awith%2520our%2520model%2520generating%2520smooth%252C%2520feasible%2520docking%2520trajectories%2520that%2520meet%250Aphysical%2520constraints%2520and%2520reach%2520the%2520target%2520pose.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13024v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DVDP%3A%20An%20End-to-End%20Policy%20for%20Mobile%20Robot%20Visual%20Docking%20with%20RGB-D%0A%20%20Perception&entry.906535625=Haohan%20Min%20and%20Zhoujian%20Li%20and%20Yu%20Yang%20and%20Jinyu%20Chen%20and%20Shenghai%20Yuan&entry.1292438233=%20%20Automatic%20docking%20has%20long%20been%20a%20significant%20challenge%20in%20the%20field%20of%0Amobile%20robotics.%20Compared%20to%20other%20automatic%20docking%20methods%2C%20visual%20docking%0Amethods%20offer%20higher%20precision%20and%20lower%20deployment%20costs%2C%20making%20them%20an%0Aefficient%20and%20promising%20choice%20for%20this%20task.%20However%2C%20visual%20docking%20methods%0Aimpose%20strict%20requirements%20on%20the%20robot%27s%20initial%20position%20at%20the%20start%20of%20the%0Adocking%20process.%20To%20overcome%20the%20limitations%20of%20current%20vision-based%20methods%2C%0Awe%20propose%20an%20innovative%20end-to-end%20visual%20docking%20method%20named%20DVDP%28direct%0Avisual%20docking%20policy%29.%20This%20approach%20requires%20only%20a%20binocular%20RGB-D%20camera%0Ainstalled%20on%20the%20mobile%20robot%20to%20directly%20output%20the%20robot%27s%20docking%20path%2C%0Aachieving%20end-to-end%20automatic%20docking.%20Furthermore%2C%20we%20have%20collected%20a%0Alarge-scale%20dataset%20of%20mobile%20robot%20visual%20automatic%20docking%20dataset%20through%20a%0Acombination%20of%20virtual%20and%20real%20environments%20using%20the%20Unity%203D%20platform%20and%0Aactual%20mobile%20robot%20setups.%20We%20developed%20a%20series%20of%20evaluation%20metrics%20to%0Aquantify%20the%20performance%20of%20the%20end-to-end%20visual%20docking%20method.%20Extensive%0Aexperiments%2C%20including%20benchmarks%20against%20leading%20perception%20backbones%20adapted%0Ainto%20our%20framework%2C%20demonstrate%20that%20our%20method%20achieves%20superior%20performance.%0AFinally%2C%20real-world%20deployment%20on%20the%20SCOUT%20Mini%20confirmed%20DVDP%27s%20efficacy%2C%0Awith%20our%20model%20generating%20smooth%2C%20feasible%20docking%20trajectories%20that%20meet%0Aphysical%20constraints%20and%20reach%20the%20target%20pose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13024v1&entry.124074799=Read"},
{"title": "MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation", "author": "Minqing Huang and Shouyi Lu and Boyuan Zheng and Ziyao Li and Xiao Tang and Guirong Zhuo", "abstract": "  4D radar super-resolution, which aims to reconstruct sparse and noisy point\nclouds into dense and geometrically consistent representations, is a\nfoundational problem in autonomous perception. However, existing methods often\nsuffer from high training cost or rely on complex diffusion-based sampling,\nresulting in high inference latency and poor generalization, making it\ndifficult to balance accuracy and efficiency. To address these limitations, we\npropose MSDNet, a multi-stage distillation framework that efficiently transfers\ndense LiDAR priors to 4D radar features to achieve both high reconstruction\nquality and computational efficiency. The first stage performs\nreconstruction-guided feature distillation, aligning and densifying the\nstudent's features through feature reconstruction. In the second stage, we\npropose diffusion-guided feature distillation, which treats the stage-one\ndistilled features as a noisy version of the teacher's representations and\nrefines them via a lightweight diffusion network. Furthermore, we introduce a\nnoise adapter that adaptively aligns the noise level of the feature with a\npredefined diffusion timestep, enabling a more precise denoising. Extensive\nexperiments on the VoD and in-house datasets demonstrate that MSDNet achieves\nboth high-fidelity reconstruction and low-latency inference in the task of 4D\nradar point cloud super-resolution, and consistently improves performance on\ndownstream tasks. The code will be publicly available upon publication.\n", "link": "http://arxiv.org/abs/2509.13149v1", "date": "2025-09-16", "relevancy": 2.302, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6146}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5857}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MSDNet%3A%20Efficient%204D%20Radar%20Super-Resolution%20via%20Multi-Stage%20Distillation&body=Title%3A%20MSDNet%3A%20Efficient%204D%20Radar%20Super-Resolution%20via%20Multi-Stage%20Distillation%0AAuthor%3A%20Minqing%20Huang%20and%20Shouyi%20Lu%20and%20Boyuan%20Zheng%20and%20Ziyao%20Li%20and%20Xiao%20Tang%20and%20Guirong%20Zhuo%0AAbstract%3A%20%20%204D%20radar%20super-resolution%2C%20which%20aims%20to%20reconstruct%20sparse%20and%20noisy%20point%0Aclouds%20into%20dense%20and%20geometrically%20consistent%20representations%2C%20is%20a%0Afoundational%20problem%20in%20autonomous%20perception.%20However%2C%20existing%20methods%20often%0Asuffer%20from%20high%20training%20cost%20or%20rely%20on%20complex%20diffusion-based%20sampling%2C%0Aresulting%20in%20high%20inference%20latency%20and%20poor%20generalization%2C%20making%20it%0Adifficult%20to%20balance%20accuracy%20and%20efficiency.%20To%20address%20these%20limitations%2C%20we%0Apropose%20MSDNet%2C%20a%20multi-stage%20distillation%20framework%20that%20efficiently%20transfers%0Adense%20LiDAR%20priors%20to%204D%20radar%20features%20to%20achieve%20both%20high%20reconstruction%0Aquality%20and%20computational%20efficiency.%20The%20first%20stage%20performs%0Areconstruction-guided%20feature%20distillation%2C%20aligning%20and%20densifying%20the%0Astudent%27s%20features%20through%20feature%20reconstruction.%20In%20the%20second%20stage%2C%20we%0Apropose%20diffusion-guided%20feature%20distillation%2C%20which%20treats%20the%20stage-one%0Adistilled%20features%20as%20a%20noisy%20version%20of%20the%20teacher%27s%20representations%20and%0Arefines%20them%20via%20a%20lightweight%20diffusion%20network.%20Furthermore%2C%20we%20introduce%20a%0Anoise%20adapter%20that%20adaptively%20aligns%20the%20noise%20level%20of%20the%20feature%20with%20a%0Apredefined%20diffusion%20timestep%2C%20enabling%20a%20more%20precise%20denoising.%20Extensive%0Aexperiments%20on%20the%20VoD%20and%20in-house%20datasets%20demonstrate%20that%20MSDNet%20achieves%0Aboth%20high-fidelity%20reconstruction%20and%20low-latency%20inference%20in%20the%20task%20of%204D%0Aradar%20point%20cloud%20super-resolution%2C%20and%20consistently%20improves%20performance%20on%0Adownstream%20tasks.%20The%20code%20will%20be%20publicly%20available%20upon%20publication.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13149v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMSDNet%253A%2520Efficient%25204D%2520Radar%2520Super-Resolution%2520via%2520Multi-Stage%2520Distillation%26entry.906535625%3DMinqing%2520Huang%2520and%2520Shouyi%2520Lu%2520and%2520Boyuan%2520Zheng%2520and%2520Ziyao%2520Li%2520and%2520Xiao%2520Tang%2520and%2520Guirong%2520Zhuo%26entry.1292438233%3D%2520%25204D%2520radar%2520super-resolution%252C%2520which%2520aims%2520to%2520reconstruct%2520sparse%2520and%2520noisy%2520point%250Aclouds%2520into%2520dense%2520and%2520geometrically%2520consistent%2520representations%252C%2520is%2520a%250Afoundational%2520problem%2520in%2520autonomous%2520perception.%2520However%252C%2520existing%2520methods%2520often%250Asuffer%2520from%2520high%2520training%2520cost%2520or%2520rely%2520on%2520complex%2520diffusion-based%2520sampling%252C%250Aresulting%2520in%2520high%2520inference%2520latency%2520and%2520poor%2520generalization%252C%2520making%2520it%250Adifficult%2520to%2520balance%2520accuracy%2520and%2520efficiency.%2520To%2520address%2520these%2520limitations%252C%2520we%250Apropose%2520MSDNet%252C%2520a%2520multi-stage%2520distillation%2520framework%2520that%2520efficiently%2520transfers%250Adense%2520LiDAR%2520priors%2520to%25204D%2520radar%2520features%2520to%2520achieve%2520both%2520high%2520reconstruction%250Aquality%2520and%2520computational%2520efficiency.%2520The%2520first%2520stage%2520performs%250Areconstruction-guided%2520feature%2520distillation%252C%2520aligning%2520and%2520densifying%2520the%250Astudent%2527s%2520features%2520through%2520feature%2520reconstruction.%2520In%2520the%2520second%2520stage%252C%2520we%250Apropose%2520diffusion-guided%2520feature%2520distillation%252C%2520which%2520treats%2520the%2520stage-one%250Adistilled%2520features%2520as%2520a%2520noisy%2520version%2520of%2520the%2520teacher%2527s%2520representations%2520and%250Arefines%2520them%2520via%2520a%2520lightweight%2520diffusion%2520network.%2520Furthermore%252C%2520we%2520introduce%2520a%250Anoise%2520adapter%2520that%2520adaptively%2520aligns%2520the%2520noise%2520level%2520of%2520the%2520feature%2520with%2520a%250Apredefined%2520diffusion%2520timestep%252C%2520enabling%2520a%2520more%2520precise%2520denoising.%2520Extensive%250Aexperiments%2520on%2520the%2520VoD%2520and%2520in-house%2520datasets%2520demonstrate%2520that%2520MSDNet%2520achieves%250Aboth%2520high-fidelity%2520reconstruction%2520and%2520low-latency%2520inference%2520in%2520the%2520task%2520of%25204D%250Aradar%2520point%2520cloud%2520super-resolution%252C%2520and%2520consistently%2520improves%2520performance%2520on%250Adownstream%2520tasks.%2520The%2520code%2520will%2520be%2520publicly%2520available%2520upon%2520publication.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13149v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MSDNet%3A%20Efficient%204D%20Radar%20Super-Resolution%20via%20Multi-Stage%20Distillation&entry.906535625=Minqing%20Huang%20and%20Shouyi%20Lu%20and%20Boyuan%20Zheng%20and%20Ziyao%20Li%20and%20Xiao%20Tang%20and%20Guirong%20Zhuo&entry.1292438233=%20%204D%20radar%20super-resolution%2C%20which%20aims%20to%20reconstruct%20sparse%20and%20noisy%20point%0Aclouds%20into%20dense%20and%20geometrically%20consistent%20representations%2C%20is%20a%0Afoundational%20problem%20in%20autonomous%20perception.%20However%2C%20existing%20methods%20often%0Asuffer%20from%20high%20training%20cost%20or%20rely%20on%20complex%20diffusion-based%20sampling%2C%0Aresulting%20in%20high%20inference%20latency%20and%20poor%20generalization%2C%20making%20it%0Adifficult%20to%20balance%20accuracy%20and%20efficiency.%20To%20address%20these%20limitations%2C%20we%0Apropose%20MSDNet%2C%20a%20multi-stage%20distillation%20framework%20that%20efficiently%20transfers%0Adense%20LiDAR%20priors%20to%204D%20radar%20features%20to%20achieve%20both%20high%20reconstruction%0Aquality%20and%20computational%20efficiency.%20The%20first%20stage%20performs%0Areconstruction-guided%20feature%20distillation%2C%20aligning%20and%20densifying%20the%0Astudent%27s%20features%20through%20feature%20reconstruction.%20In%20the%20second%20stage%2C%20we%0Apropose%20diffusion-guided%20feature%20distillation%2C%20which%20treats%20the%20stage-one%0Adistilled%20features%20as%20a%20noisy%20version%20of%20the%20teacher%27s%20representations%20and%0Arefines%20them%20via%20a%20lightweight%20diffusion%20network.%20Furthermore%2C%20we%20introduce%20a%0Anoise%20adapter%20that%20adaptively%20aligns%20the%20noise%20level%20of%20the%20feature%20with%20a%0Apredefined%20diffusion%20timestep%2C%20enabling%20a%20more%20precise%20denoising.%20Extensive%0Aexperiments%20on%20the%20VoD%20and%20in-house%20datasets%20demonstrate%20that%20MSDNet%20achieves%0Aboth%20high-fidelity%20reconstruction%20and%20low-latency%20inference%20in%20the%20task%20of%204D%0Aradar%20point%20cloud%20super-resolution%2C%20and%20consistently%20improves%20performance%20on%0Adownstream%20tasks.%20The%20code%20will%20be%20publicly%20available%20upon%20publication.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13149v1&entry.124074799=Read"},
{"title": "StyleSculptor: Zero-Shot Style-Controllable 3D Asset Generation with\n  Texture-Geometry Dual Guidance", "author": "Zefan Qu and Zhenwei Wang and Haoyuan Wang and Ke Xu and Gerhard Hancke and Rynson W. H. Lau", "abstract": "  Creating 3D assets that follow the texture and geometry style of existing\nones is often desirable or even inevitable in practical applications like video\ngaming and virtual reality. While impressive progress has been made in\ngenerating 3D objects from text or images, creating style-controllable 3D\nassets remains a complex and challenging problem. In this work, we propose\nStyleSculptor, a novel training-free approach for generating style-guided 3D\nassets from a content image and one or more style images. Unlike previous\nworks, StyleSculptor achieves style-guided 3D generation in a zero-shot manner,\nenabling fine-grained 3D style control that captures the texture, geometry, or\nboth styles of user-provided style images. At the core of StyleSculptor is a\nnovel Style Disentangled Attention (SD-Attn) module, which establishes a\ndynamic interaction between the input content image and style image for\nstyle-guided 3D asset generation via a cross-3D attention mechanism, enabling\nstable feature fusion and effective style-guided generation. To alleviate\nsemantic content leakage, we also introduce a style-disentangled feature\nselection strategy within the SD-Attn module, which leverages the variance of\n3D feature patches to disentangle style- and content-significant channels,\nallowing selective feature injection within the attention framework. With\nSD-Attn, the network can dynamically compute texture-, geometry-, or\nboth-guided features to steer the 3D generation process. Built upon this, we\nfurther propose the Style Guided Control (SGC) mechanism, which enables\nexclusive geometry- or texture-only stylization, as well as adjustable style\nintensity control. Extensive experiments demonstrate that StyleSculptor\noutperforms existing baseline methods in producing high-fidelity 3D assets.\n", "link": "http://arxiv.org/abs/2509.13301v1", "date": "2025-09-16", "relevancy": 2.2967, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5795}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5707}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StyleSculptor%3A%20Zero-Shot%20Style-Controllable%203D%20Asset%20Generation%20with%0A%20%20Texture-Geometry%20Dual%20Guidance&body=Title%3A%20StyleSculptor%3A%20Zero-Shot%20Style-Controllable%203D%20Asset%20Generation%20with%0A%20%20Texture-Geometry%20Dual%20Guidance%0AAuthor%3A%20Zefan%20Qu%20and%20Zhenwei%20Wang%20and%20Haoyuan%20Wang%20and%20Ke%20Xu%20and%20Gerhard%20Hancke%20and%20Rynson%20W.%20H.%20Lau%0AAbstract%3A%20%20%20Creating%203D%20assets%20that%20follow%20the%20texture%20and%20geometry%20style%20of%20existing%0Aones%20is%20often%20desirable%20or%20even%20inevitable%20in%20practical%20applications%20like%20video%0Agaming%20and%20virtual%20reality.%20While%20impressive%20progress%20has%20been%20made%20in%0Agenerating%203D%20objects%20from%20text%20or%20images%2C%20creating%20style-controllable%203D%0Aassets%20remains%20a%20complex%20and%20challenging%20problem.%20In%20this%20work%2C%20we%20propose%0AStyleSculptor%2C%20a%20novel%20training-free%20approach%20for%20generating%20style-guided%203D%0Aassets%20from%20a%20content%20image%20and%20one%20or%20more%20style%20images.%20Unlike%20previous%0Aworks%2C%20StyleSculptor%20achieves%20style-guided%203D%20generation%20in%20a%20zero-shot%20manner%2C%0Aenabling%20fine-grained%203D%20style%20control%20that%20captures%20the%20texture%2C%20geometry%2C%20or%0Aboth%20styles%20of%20user-provided%20style%20images.%20At%20the%20core%20of%20StyleSculptor%20is%20a%0Anovel%20Style%20Disentangled%20Attention%20%28SD-Attn%29%20module%2C%20which%20establishes%20a%0Adynamic%20interaction%20between%20the%20input%20content%20image%20and%20style%20image%20for%0Astyle-guided%203D%20asset%20generation%20via%20a%20cross-3D%20attention%20mechanism%2C%20enabling%0Astable%20feature%20fusion%20and%20effective%20style-guided%20generation.%20To%20alleviate%0Asemantic%20content%20leakage%2C%20we%20also%20introduce%20a%20style-disentangled%20feature%0Aselection%20strategy%20within%20the%20SD-Attn%20module%2C%20which%20leverages%20the%20variance%20of%0A3D%20feature%20patches%20to%20disentangle%20style-%20and%20content-significant%20channels%2C%0Aallowing%20selective%20feature%20injection%20within%20the%20attention%20framework.%20With%0ASD-Attn%2C%20the%20network%20can%20dynamically%20compute%20texture-%2C%20geometry-%2C%20or%0Aboth-guided%20features%20to%20steer%20the%203D%20generation%20process.%20Built%20upon%20this%2C%20we%0Afurther%20propose%20the%20Style%20Guided%20Control%20%28SGC%29%20mechanism%2C%20which%20enables%0Aexclusive%20geometry-%20or%20texture-only%20stylization%2C%20as%20well%20as%20adjustable%20style%0Aintensity%20control.%20Extensive%20experiments%20demonstrate%20that%20StyleSculptor%0Aoutperforms%20existing%20baseline%20methods%20in%20producing%20high-fidelity%203D%20assets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13301v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStyleSculptor%253A%2520Zero-Shot%2520Style-Controllable%25203D%2520Asset%2520Generation%2520with%250A%2520%2520Texture-Geometry%2520Dual%2520Guidance%26entry.906535625%3DZefan%2520Qu%2520and%2520Zhenwei%2520Wang%2520and%2520Haoyuan%2520Wang%2520and%2520Ke%2520Xu%2520and%2520Gerhard%2520Hancke%2520and%2520Rynson%2520W.%2520H.%2520Lau%26entry.1292438233%3D%2520%2520Creating%25203D%2520assets%2520that%2520follow%2520the%2520texture%2520and%2520geometry%2520style%2520of%2520existing%250Aones%2520is%2520often%2520desirable%2520or%2520even%2520inevitable%2520in%2520practical%2520applications%2520like%2520video%250Agaming%2520and%2520virtual%2520reality.%2520While%2520impressive%2520progress%2520has%2520been%2520made%2520in%250Agenerating%25203D%2520objects%2520from%2520text%2520or%2520images%252C%2520creating%2520style-controllable%25203D%250Aassets%2520remains%2520a%2520complex%2520and%2520challenging%2520problem.%2520In%2520this%2520work%252C%2520we%2520propose%250AStyleSculptor%252C%2520a%2520novel%2520training-free%2520approach%2520for%2520generating%2520style-guided%25203D%250Aassets%2520from%2520a%2520content%2520image%2520and%2520one%2520or%2520more%2520style%2520images.%2520Unlike%2520previous%250Aworks%252C%2520StyleSculptor%2520achieves%2520style-guided%25203D%2520generation%2520in%2520a%2520zero-shot%2520manner%252C%250Aenabling%2520fine-grained%25203D%2520style%2520control%2520that%2520captures%2520the%2520texture%252C%2520geometry%252C%2520or%250Aboth%2520styles%2520of%2520user-provided%2520style%2520images.%2520At%2520the%2520core%2520of%2520StyleSculptor%2520is%2520a%250Anovel%2520Style%2520Disentangled%2520Attention%2520%2528SD-Attn%2529%2520module%252C%2520which%2520establishes%2520a%250Adynamic%2520interaction%2520between%2520the%2520input%2520content%2520image%2520and%2520style%2520image%2520for%250Astyle-guided%25203D%2520asset%2520generation%2520via%2520a%2520cross-3D%2520attention%2520mechanism%252C%2520enabling%250Astable%2520feature%2520fusion%2520and%2520effective%2520style-guided%2520generation.%2520To%2520alleviate%250Asemantic%2520content%2520leakage%252C%2520we%2520also%2520introduce%2520a%2520style-disentangled%2520feature%250Aselection%2520strategy%2520within%2520the%2520SD-Attn%2520module%252C%2520which%2520leverages%2520the%2520variance%2520of%250A3D%2520feature%2520patches%2520to%2520disentangle%2520style-%2520and%2520content-significant%2520channels%252C%250Aallowing%2520selective%2520feature%2520injection%2520within%2520the%2520attention%2520framework.%2520With%250ASD-Attn%252C%2520the%2520network%2520can%2520dynamically%2520compute%2520texture-%252C%2520geometry-%252C%2520or%250Aboth-guided%2520features%2520to%2520steer%2520the%25203D%2520generation%2520process.%2520Built%2520upon%2520this%252C%2520we%250Afurther%2520propose%2520the%2520Style%2520Guided%2520Control%2520%2528SGC%2529%2520mechanism%252C%2520which%2520enables%250Aexclusive%2520geometry-%2520or%2520texture-only%2520stylization%252C%2520as%2520well%2520as%2520adjustable%2520style%250Aintensity%2520control.%2520Extensive%2520experiments%2520demonstrate%2520that%2520StyleSculptor%250Aoutperforms%2520existing%2520baseline%2520methods%2520in%2520producing%2520high-fidelity%25203D%2520assets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13301v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StyleSculptor%3A%20Zero-Shot%20Style-Controllable%203D%20Asset%20Generation%20with%0A%20%20Texture-Geometry%20Dual%20Guidance&entry.906535625=Zefan%20Qu%20and%20Zhenwei%20Wang%20and%20Haoyuan%20Wang%20and%20Ke%20Xu%20and%20Gerhard%20Hancke%20and%20Rynson%20W.%20H.%20Lau&entry.1292438233=%20%20Creating%203D%20assets%20that%20follow%20the%20texture%20and%20geometry%20style%20of%20existing%0Aones%20is%20often%20desirable%20or%20even%20inevitable%20in%20practical%20applications%20like%20video%0Agaming%20and%20virtual%20reality.%20While%20impressive%20progress%20has%20been%20made%20in%0Agenerating%203D%20objects%20from%20text%20or%20images%2C%20creating%20style-controllable%203D%0Aassets%20remains%20a%20complex%20and%20challenging%20problem.%20In%20this%20work%2C%20we%20propose%0AStyleSculptor%2C%20a%20novel%20training-free%20approach%20for%20generating%20style-guided%203D%0Aassets%20from%20a%20content%20image%20and%20one%20or%20more%20style%20images.%20Unlike%20previous%0Aworks%2C%20StyleSculptor%20achieves%20style-guided%203D%20generation%20in%20a%20zero-shot%20manner%2C%0Aenabling%20fine-grained%203D%20style%20control%20that%20captures%20the%20texture%2C%20geometry%2C%20or%0Aboth%20styles%20of%20user-provided%20style%20images.%20At%20the%20core%20of%20StyleSculptor%20is%20a%0Anovel%20Style%20Disentangled%20Attention%20%28SD-Attn%29%20module%2C%20which%20establishes%20a%0Adynamic%20interaction%20between%20the%20input%20content%20image%20and%20style%20image%20for%0Astyle-guided%203D%20asset%20generation%20via%20a%20cross-3D%20attention%20mechanism%2C%20enabling%0Astable%20feature%20fusion%20and%20effective%20style-guided%20generation.%20To%20alleviate%0Asemantic%20content%20leakage%2C%20we%20also%20introduce%20a%20style-disentangled%20feature%0Aselection%20strategy%20within%20the%20SD-Attn%20module%2C%20which%20leverages%20the%20variance%20of%0A3D%20feature%20patches%20to%20disentangle%20style-%20and%20content-significant%20channels%2C%0Aallowing%20selective%20feature%20injection%20within%20the%20attention%20framework.%20With%0ASD-Attn%2C%20the%20network%20can%20dynamically%20compute%20texture-%2C%20geometry-%2C%20or%0Aboth-guided%20features%20to%20steer%20the%203D%20generation%20process.%20Built%20upon%20this%2C%20we%0Afurther%20propose%20the%20Style%20Guided%20Control%20%28SGC%29%20mechanism%2C%20which%20enables%0Aexclusive%20geometry-%20or%20texture-only%20stylization%2C%20as%20well%20as%20adjustable%20style%0Aintensity%20control.%20Extensive%20experiments%20demonstrate%20that%20StyleSculptor%0Aoutperforms%20existing%20baseline%20methods%20in%20producing%20high-fidelity%203D%20assets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13301v1&entry.124074799=Read"},
{"title": "Spiking Vocos: An Energy-Efficient Neural Vocoder", "author": "Yukun Chen and Zhaoxi Mu and Andong Li and Peilin Li and Xinyu Yang", "abstract": "  Despite the remarkable progress in the synthesis speed and fidelity of neural\nvocoders, their high energy consumption remains a critical barrier to practical\ndeployment on computationally restricted edge devices. Spiking Neural Networks\n(SNNs), widely recognized for their high energy efficiency due to their\nevent-driven nature, offer a promising solution for low-resource scenarios. In\nthis paper, we propose Spiking Vocos, a novel spiking neural vocoder with\nultra-low energy consumption, built upon the efficient Vocos framework. To\nmitigate the inherent information bottleneck in SNNs, we design a Spiking\nConvNeXt module to reduce Multiply-Accumulate (MAC) operations and incorporate\nan amplitude shortcut path to preserve crucial signal dynamics. Furthermore, to\nbridge the performance gap with its Artificial Neural Network (ANN)\ncounterpart, we introduce a self-architectural distillation strategy to\neffectively transfer knowledge. A lightweight Temporal Shift Module is also\nintegrated to enhance the model's ability to fuse information across the\ntemporal dimension with negligible computational overhead. Experiments\ndemonstrate that our model achieves performance comparable to its ANN\ncounterpart, with UTMOS and PESQ scores of 3.74 and 3.45 respectively, while\nconsuming only 14.7% of the energy. The source code is available at\nhttps://github.com/pymaster17/Spiking-Vocos.\n", "link": "http://arxiv.org/abs/2509.13049v1", "date": "2025-09-16", "relevancy": 2.2944, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4716}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4533}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spiking%20Vocos%3A%20An%20Energy-Efficient%20Neural%20Vocoder&body=Title%3A%20Spiking%20Vocos%3A%20An%20Energy-Efficient%20Neural%20Vocoder%0AAuthor%3A%20Yukun%20Chen%20and%20Zhaoxi%20Mu%20and%20Andong%20Li%20and%20Peilin%20Li%20and%20Xinyu%20Yang%0AAbstract%3A%20%20%20Despite%20the%20remarkable%20progress%20in%20the%20synthesis%20speed%20and%20fidelity%20of%20neural%0Avocoders%2C%20their%20high%20energy%20consumption%20remains%20a%20critical%20barrier%20to%20practical%0Adeployment%20on%20computationally%20restricted%20edge%20devices.%20Spiking%20Neural%20Networks%0A%28SNNs%29%2C%20widely%20recognized%20for%20their%20high%20energy%20efficiency%20due%20to%20their%0Aevent-driven%20nature%2C%20offer%20a%20promising%20solution%20for%20low-resource%20scenarios.%20In%0Athis%20paper%2C%20we%20propose%20Spiking%20Vocos%2C%20a%20novel%20spiking%20neural%20vocoder%20with%0Aultra-low%20energy%20consumption%2C%20built%20upon%20the%20efficient%20Vocos%20framework.%20To%0Amitigate%20the%20inherent%20information%20bottleneck%20in%20SNNs%2C%20we%20design%20a%20Spiking%0AConvNeXt%20module%20to%20reduce%20Multiply-Accumulate%20%28MAC%29%20operations%20and%20incorporate%0Aan%20amplitude%20shortcut%20path%20to%20preserve%20crucial%20signal%20dynamics.%20Furthermore%2C%20to%0Abridge%20the%20performance%20gap%20with%20its%20Artificial%20Neural%20Network%20%28ANN%29%0Acounterpart%2C%20we%20introduce%20a%20self-architectural%20distillation%20strategy%20to%0Aeffectively%20transfer%20knowledge.%20A%20lightweight%20Temporal%20Shift%20Module%20is%20also%0Aintegrated%20to%20enhance%20the%20model%27s%20ability%20to%20fuse%20information%20across%20the%0Atemporal%20dimension%20with%20negligible%20computational%20overhead.%20Experiments%0Ademonstrate%20that%20our%20model%20achieves%20performance%20comparable%20to%20its%20ANN%0Acounterpart%2C%20with%20UTMOS%20and%20PESQ%20scores%20of%203.74%20and%203.45%20respectively%2C%20while%0Aconsuming%20only%2014.7%25%20of%20the%20energy.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/pymaster17/Spiking-Vocos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13049v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpiking%2520Vocos%253A%2520An%2520Energy-Efficient%2520Neural%2520Vocoder%26entry.906535625%3DYukun%2520Chen%2520and%2520Zhaoxi%2520Mu%2520and%2520Andong%2520Li%2520and%2520Peilin%2520Li%2520and%2520Xinyu%2520Yang%26entry.1292438233%3D%2520%2520Despite%2520the%2520remarkable%2520progress%2520in%2520the%2520synthesis%2520speed%2520and%2520fidelity%2520of%2520neural%250Avocoders%252C%2520their%2520high%2520energy%2520consumption%2520remains%2520a%2520critical%2520barrier%2520to%2520practical%250Adeployment%2520on%2520computationally%2520restricted%2520edge%2520devices.%2520Spiking%2520Neural%2520Networks%250A%2528SNNs%2529%252C%2520widely%2520recognized%2520for%2520their%2520high%2520energy%2520efficiency%2520due%2520to%2520their%250Aevent-driven%2520nature%252C%2520offer%2520a%2520promising%2520solution%2520for%2520low-resource%2520scenarios.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520Spiking%2520Vocos%252C%2520a%2520novel%2520spiking%2520neural%2520vocoder%2520with%250Aultra-low%2520energy%2520consumption%252C%2520built%2520upon%2520the%2520efficient%2520Vocos%2520framework.%2520To%250Amitigate%2520the%2520inherent%2520information%2520bottleneck%2520in%2520SNNs%252C%2520we%2520design%2520a%2520Spiking%250AConvNeXt%2520module%2520to%2520reduce%2520Multiply-Accumulate%2520%2528MAC%2529%2520operations%2520and%2520incorporate%250Aan%2520amplitude%2520shortcut%2520path%2520to%2520preserve%2520crucial%2520signal%2520dynamics.%2520Furthermore%252C%2520to%250Abridge%2520the%2520performance%2520gap%2520with%2520its%2520Artificial%2520Neural%2520Network%2520%2528ANN%2529%250Acounterpart%252C%2520we%2520introduce%2520a%2520self-architectural%2520distillation%2520strategy%2520to%250Aeffectively%2520transfer%2520knowledge.%2520A%2520lightweight%2520Temporal%2520Shift%2520Module%2520is%2520also%250Aintegrated%2520to%2520enhance%2520the%2520model%2527s%2520ability%2520to%2520fuse%2520information%2520across%2520the%250Atemporal%2520dimension%2520with%2520negligible%2520computational%2520overhead.%2520Experiments%250Ademonstrate%2520that%2520our%2520model%2520achieves%2520performance%2520comparable%2520to%2520its%2520ANN%250Acounterpart%252C%2520with%2520UTMOS%2520and%2520PESQ%2520scores%2520of%25203.74%2520and%25203.45%2520respectively%252C%2520while%250Aconsuming%2520only%252014.7%2525%2520of%2520the%2520energy.%2520The%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/pymaster17/Spiking-Vocos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13049v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spiking%20Vocos%3A%20An%20Energy-Efficient%20Neural%20Vocoder&entry.906535625=Yukun%20Chen%20and%20Zhaoxi%20Mu%20and%20Andong%20Li%20and%20Peilin%20Li%20and%20Xinyu%20Yang&entry.1292438233=%20%20Despite%20the%20remarkable%20progress%20in%20the%20synthesis%20speed%20and%20fidelity%20of%20neural%0Avocoders%2C%20their%20high%20energy%20consumption%20remains%20a%20critical%20barrier%20to%20practical%0Adeployment%20on%20computationally%20restricted%20edge%20devices.%20Spiking%20Neural%20Networks%0A%28SNNs%29%2C%20widely%20recognized%20for%20their%20high%20energy%20efficiency%20due%20to%20their%0Aevent-driven%20nature%2C%20offer%20a%20promising%20solution%20for%20low-resource%20scenarios.%20In%0Athis%20paper%2C%20we%20propose%20Spiking%20Vocos%2C%20a%20novel%20spiking%20neural%20vocoder%20with%0Aultra-low%20energy%20consumption%2C%20built%20upon%20the%20efficient%20Vocos%20framework.%20To%0Amitigate%20the%20inherent%20information%20bottleneck%20in%20SNNs%2C%20we%20design%20a%20Spiking%0AConvNeXt%20module%20to%20reduce%20Multiply-Accumulate%20%28MAC%29%20operations%20and%20incorporate%0Aan%20amplitude%20shortcut%20path%20to%20preserve%20crucial%20signal%20dynamics.%20Furthermore%2C%20to%0Abridge%20the%20performance%20gap%20with%20its%20Artificial%20Neural%20Network%20%28ANN%29%0Acounterpart%2C%20we%20introduce%20a%20self-architectural%20distillation%20strategy%20to%0Aeffectively%20transfer%20knowledge.%20A%20lightweight%20Temporal%20Shift%20Module%20is%20also%0Aintegrated%20to%20enhance%20the%20model%27s%20ability%20to%20fuse%20information%20across%20the%0Atemporal%20dimension%20with%20negligible%20computational%20overhead.%20Experiments%0Ademonstrate%20that%20our%20model%20achieves%20performance%20comparable%20to%20its%20ANN%0Acounterpart%2C%20with%20UTMOS%20and%20PESQ%20scores%20of%203.74%20and%203.45%20respectively%2C%20while%0Aconsuming%20only%2014.7%25%20of%20the%20energy.%20The%20source%20code%20is%20available%20at%0Ahttps%3A//github.com/pymaster17/Spiking-Vocos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13049v1&entry.124074799=Read"},
{"title": "Contrastive timbre representations for musical instrument and\n  synthesizer retrieval", "author": "Gwendal Le Vaillant and Yannick Molle", "abstract": "  Efficiently retrieving specific instrument timbres from audio mixtures\nremains a challenge in digital music production. This paper introduces a\ncontrastive learning framework for musical instrument retrieval, enabling\ndirect querying of instrument databases using a single model for both single-\nand multi-instrument sounds. We propose techniques to generate realistic\npositive/negative pairs of sounds for virtual musical instruments, such as\nsamplers and synthesizers, addressing limitations in common audio data\naugmentation methods.\n  The first experiment focuses on instrument retrieval from a dataset of 3,884\ninstruments, using single-instrument audio as input. Contrastive approaches are\ncompetitive with previous works based on classification pre-training. The\nsecond experiment considers multi-instrument retrieval with a mixture of\ninstruments as audio input. In this case, the proposed contrastive framework\noutperforms related works, achieving 81.7\\% top-1 and 95.7\\% top-5 accuracies\nfor three-instrument mixtures.\n", "link": "http://arxiv.org/abs/2509.13285v1", "date": "2025-09-16", "relevancy": 2.2926, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4689}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4533}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4533}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contrastive%20timbre%20representations%20for%20musical%20instrument%20and%0A%20%20synthesizer%20retrieval&body=Title%3A%20Contrastive%20timbre%20representations%20for%20musical%20instrument%20and%0A%20%20synthesizer%20retrieval%0AAuthor%3A%20Gwendal%20Le%20Vaillant%20and%20Yannick%20Molle%0AAbstract%3A%20%20%20Efficiently%20retrieving%20specific%20instrument%20timbres%20from%20audio%20mixtures%0Aremains%20a%20challenge%20in%20digital%20music%20production.%20This%20paper%20introduces%20a%0Acontrastive%20learning%20framework%20for%20musical%20instrument%20retrieval%2C%20enabling%0Adirect%20querying%20of%20instrument%20databases%20using%20a%20single%20model%20for%20both%20single-%0Aand%20multi-instrument%20sounds.%20We%20propose%20techniques%20to%20generate%20realistic%0Apositive/negative%20pairs%20of%20sounds%20for%20virtual%20musical%20instruments%2C%20such%20as%0Asamplers%20and%20synthesizers%2C%20addressing%20limitations%20in%20common%20audio%20data%0Aaugmentation%20methods.%0A%20%20The%20first%20experiment%20focuses%20on%20instrument%20retrieval%20from%20a%20dataset%20of%203%2C884%0Ainstruments%2C%20using%20single-instrument%20audio%20as%20input.%20Contrastive%20approaches%20are%0Acompetitive%20with%20previous%20works%20based%20on%20classification%20pre-training.%20The%0Asecond%20experiment%20considers%20multi-instrument%20retrieval%20with%20a%20mixture%20of%0Ainstruments%20as%20audio%20input.%20In%20this%20case%2C%20the%20proposed%20contrastive%20framework%0Aoutperforms%20related%20works%2C%20achieving%2081.7%5C%25%20top-1%20and%2095.7%5C%25%20top-5%20accuracies%0Afor%20three-instrument%20mixtures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContrastive%2520timbre%2520representations%2520for%2520musical%2520instrument%2520and%250A%2520%2520synthesizer%2520retrieval%26entry.906535625%3DGwendal%2520Le%2520Vaillant%2520and%2520Yannick%2520Molle%26entry.1292438233%3D%2520%2520Efficiently%2520retrieving%2520specific%2520instrument%2520timbres%2520from%2520audio%2520mixtures%250Aremains%2520a%2520challenge%2520in%2520digital%2520music%2520production.%2520This%2520paper%2520introduces%2520a%250Acontrastive%2520learning%2520framework%2520for%2520musical%2520instrument%2520retrieval%252C%2520enabling%250Adirect%2520querying%2520of%2520instrument%2520databases%2520using%2520a%2520single%2520model%2520for%2520both%2520single-%250Aand%2520multi-instrument%2520sounds.%2520We%2520propose%2520techniques%2520to%2520generate%2520realistic%250Apositive/negative%2520pairs%2520of%2520sounds%2520for%2520virtual%2520musical%2520instruments%252C%2520such%2520as%250Asamplers%2520and%2520synthesizers%252C%2520addressing%2520limitations%2520in%2520common%2520audio%2520data%250Aaugmentation%2520methods.%250A%2520%2520The%2520first%2520experiment%2520focuses%2520on%2520instrument%2520retrieval%2520from%2520a%2520dataset%2520of%25203%252C884%250Ainstruments%252C%2520using%2520single-instrument%2520audio%2520as%2520input.%2520Contrastive%2520approaches%2520are%250Acompetitive%2520with%2520previous%2520works%2520based%2520on%2520classification%2520pre-training.%2520The%250Asecond%2520experiment%2520considers%2520multi-instrument%2520retrieval%2520with%2520a%2520mixture%2520of%250Ainstruments%2520as%2520audio%2520input.%2520In%2520this%2520case%252C%2520the%2520proposed%2520contrastive%2520framework%250Aoutperforms%2520related%2520works%252C%2520achieving%252081.7%255C%2525%2520top-1%2520and%252095.7%255C%2525%2520top-5%2520accuracies%250Afor%2520three-instrument%2520mixtures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contrastive%20timbre%20representations%20for%20musical%20instrument%20and%0A%20%20synthesizer%20retrieval&entry.906535625=Gwendal%20Le%20Vaillant%20and%20Yannick%20Molle&entry.1292438233=%20%20Efficiently%20retrieving%20specific%20instrument%20timbres%20from%20audio%20mixtures%0Aremains%20a%20challenge%20in%20digital%20music%20production.%20This%20paper%20introduces%20a%0Acontrastive%20learning%20framework%20for%20musical%20instrument%20retrieval%2C%20enabling%0Adirect%20querying%20of%20instrument%20databases%20using%20a%20single%20model%20for%20both%20single-%0Aand%20multi-instrument%20sounds.%20We%20propose%20techniques%20to%20generate%20realistic%0Apositive/negative%20pairs%20of%20sounds%20for%20virtual%20musical%20instruments%2C%20such%20as%0Asamplers%20and%20synthesizers%2C%20addressing%20limitations%20in%20common%20audio%20data%0Aaugmentation%20methods.%0A%20%20The%20first%20experiment%20focuses%20on%20instrument%20retrieval%20from%20a%20dataset%20of%203%2C884%0Ainstruments%2C%20using%20single-instrument%20audio%20as%20input.%20Contrastive%20approaches%20are%0Acompetitive%20with%20previous%20works%20based%20on%20classification%20pre-training.%20The%0Asecond%20experiment%20considers%20multi-instrument%20retrieval%20with%20a%20mixture%20of%0Ainstruments%20as%20audio%20input.%20In%20this%20case%2C%20the%20proposed%20contrastive%20framework%0Aoutperforms%20related%20works%2C%20achieving%2081.7%5C%25%20top-1%20and%2095.7%5C%25%20top-5%20accuracies%0Afor%20three-instrument%20mixtures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13285v1&entry.124074799=Read"},
{"title": "Robust Adaptation of Large Multimodal Models for Retrieval Augmented\n  Hateful Meme Detection", "author": "Jingbiao Mei and Jinghong Chen and Guangyu Yang and Weizhe Lin and Bill Byrne", "abstract": "  Hateful memes have become a significant concern on the Internet,\nnecessitating robust automated detection systems. While Large Multimodal Models\n(LMMs) have shown promise in hateful meme detection, they face notable\nchallenges like sub-optimal performance and limited out-of-domain\ngeneralization capabilities. Recent studies further reveal the limitations of\nboth supervised fine-tuning (SFT) and in-context learning when applied to LMMs\nin this setting. To address these issues, we propose a robust adaptation\nframework for hateful meme detection that enhances in-domain accuracy and\ncross-domain generalization while preserving the general vision-language\ncapabilities of LMMs. Analysis reveals that our approach achieves improved\nrobustness under adversarial attacks compared to SFT models. Experiments on six\nmeme classification datasets show that our approach achieves state-of-the-art\nperformance, outperforming larger agentic systems. Moreover, our method\ngenerates higher-quality rationales for explaining hateful content compared to\nstandard SFT, enhancing model interpretability. Code available at\nhttps://github.com/JingbiaoMei/RGCL\n", "link": "http://arxiv.org/abs/2502.13061v4", "date": "2025-09-16", "relevancy": 2.2876, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5839}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5751}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5341}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Adaptation%20of%20Large%20Multimodal%20Models%20for%20Retrieval%20Augmented%0A%20%20Hateful%20Meme%20Detection&body=Title%3A%20Robust%20Adaptation%20of%20Large%20Multimodal%20Models%20for%20Retrieval%20Augmented%0A%20%20Hateful%20Meme%20Detection%0AAuthor%3A%20Jingbiao%20Mei%20and%20Jinghong%20Chen%20and%20Guangyu%20Yang%20and%20Weizhe%20Lin%20and%20Bill%20Byrne%0AAbstract%3A%20%20%20Hateful%20memes%20have%20become%20a%20significant%20concern%20on%20the%20Internet%2C%0Anecessitating%20robust%20automated%20detection%20systems.%20While%20Large%20Multimodal%20Models%0A%28LMMs%29%20have%20shown%20promise%20in%20hateful%20meme%20detection%2C%20they%20face%20notable%0Achallenges%20like%20sub-optimal%20performance%20and%20limited%20out-of-domain%0Ageneralization%20capabilities.%20Recent%20studies%20further%20reveal%20the%20limitations%20of%0Aboth%20supervised%20fine-tuning%20%28SFT%29%20and%20in-context%20learning%20when%20applied%20to%20LMMs%0Ain%20this%20setting.%20To%20address%20these%20issues%2C%20we%20propose%20a%20robust%20adaptation%0Aframework%20for%20hateful%20meme%20detection%20that%20enhances%20in-domain%20accuracy%20and%0Across-domain%20generalization%20while%20preserving%20the%20general%20vision-language%0Acapabilities%20of%20LMMs.%20Analysis%20reveals%20that%20our%20approach%20achieves%20improved%0Arobustness%20under%20adversarial%20attacks%20compared%20to%20SFT%20models.%20Experiments%20on%20six%0Ameme%20classification%20datasets%20show%20that%20our%20approach%20achieves%20state-of-the-art%0Aperformance%2C%20outperforming%20larger%20agentic%20systems.%20Moreover%2C%20our%20method%0Agenerates%20higher-quality%20rationales%20for%20explaining%20hateful%20content%20compared%20to%0Astandard%20SFT%2C%20enhancing%20model%20interpretability.%20Code%20available%20at%0Ahttps%3A//github.com/JingbiaoMei/RGCL%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.13061v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Adaptation%2520of%2520Large%2520Multimodal%2520Models%2520for%2520Retrieval%2520Augmented%250A%2520%2520Hateful%2520Meme%2520Detection%26entry.906535625%3DJingbiao%2520Mei%2520and%2520Jinghong%2520Chen%2520and%2520Guangyu%2520Yang%2520and%2520Weizhe%2520Lin%2520and%2520Bill%2520Byrne%26entry.1292438233%3D%2520%2520Hateful%2520memes%2520have%2520become%2520a%2520significant%2520concern%2520on%2520the%2520Internet%252C%250Anecessitating%2520robust%2520automated%2520detection%2520systems.%2520While%2520Large%2520Multimodal%2520Models%250A%2528LMMs%2529%2520have%2520shown%2520promise%2520in%2520hateful%2520meme%2520detection%252C%2520they%2520face%2520notable%250Achallenges%2520like%2520sub-optimal%2520performance%2520and%2520limited%2520out-of-domain%250Ageneralization%2520capabilities.%2520Recent%2520studies%2520further%2520reveal%2520the%2520limitations%2520of%250Aboth%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520and%2520in-context%2520learning%2520when%2520applied%2520to%2520LMMs%250Ain%2520this%2520setting.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520robust%2520adaptation%250Aframework%2520for%2520hateful%2520meme%2520detection%2520that%2520enhances%2520in-domain%2520accuracy%2520and%250Across-domain%2520generalization%2520while%2520preserving%2520the%2520general%2520vision-language%250Acapabilities%2520of%2520LMMs.%2520Analysis%2520reveals%2520that%2520our%2520approach%2520achieves%2520improved%250Arobustness%2520under%2520adversarial%2520attacks%2520compared%2520to%2520SFT%2520models.%2520Experiments%2520on%2520six%250Ameme%2520classification%2520datasets%2520show%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%250Aperformance%252C%2520outperforming%2520larger%2520agentic%2520systems.%2520Moreover%252C%2520our%2520method%250Agenerates%2520higher-quality%2520rationales%2520for%2520explaining%2520hateful%2520content%2520compared%2520to%250Astandard%2520SFT%252C%2520enhancing%2520model%2520interpretability.%2520Code%2520available%2520at%250Ahttps%253A//github.com/JingbiaoMei/RGCL%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.13061v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Adaptation%20of%20Large%20Multimodal%20Models%20for%20Retrieval%20Augmented%0A%20%20Hateful%20Meme%20Detection&entry.906535625=Jingbiao%20Mei%20and%20Jinghong%20Chen%20and%20Guangyu%20Yang%20and%20Weizhe%20Lin%20and%20Bill%20Byrne&entry.1292438233=%20%20Hateful%20memes%20have%20become%20a%20significant%20concern%20on%20the%20Internet%2C%0Anecessitating%20robust%20automated%20detection%20systems.%20While%20Large%20Multimodal%20Models%0A%28LMMs%29%20have%20shown%20promise%20in%20hateful%20meme%20detection%2C%20they%20face%20notable%0Achallenges%20like%20sub-optimal%20performance%20and%20limited%20out-of-domain%0Ageneralization%20capabilities.%20Recent%20studies%20further%20reveal%20the%20limitations%20of%0Aboth%20supervised%20fine-tuning%20%28SFT%29%20and%20in-context%20learning%20when%20applied%20to%20LMMs%0Ain%20this%20setting.%20To%20address%20these%20issues%2C%20we%20propose%20a%20robust%20adaptation%0Aframework%20for%20hateful%20meme%20detection%20that%20enhances%20in-domain%20accuracy%20and%0Across-domain%20generalization%20while%20preserving%20the%20general%20vision-language%0Acapabilities%20of%20LMMs.%20Analysis%20reveals%20that%20our%20approach%20achieves%20improved%0Arobustness%20under%20adversarial%20attacks%20compared%20to%20SFT%20models.%20Experiments%20on%20six%0Ameme%20classification%20datasets%20show%20that%20our%20approach%20achieves%20state-of-the-art%0Aperformance%2C%20outperforming%20larger%20agentic%20systems.%20Moreover%2C%20our%20method%0Agenerates%20higher-quality%20rationales%20for%20explaining%20hateful%20content%20compared%20to%0Astandard%20SFT%2C%20enhancing%20model%20interpretability.%20Code%20available%20at%0Ahttps%3A//github.com/JingbiaoMei/RGCL%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.13061v4&entry.124074799=Read"},
{"title": "FastDriveVLA: Efficient End-to-End Driving via Plug-and-Play\n  Reconstruction-based Token Pruning", "author": "Jiajun Cao and Qizhe Zhang and Peidong Jia and Xuhui Zhao and Bo Lan and Xiaoan Zhang and Zhuo Li and Xiaobao Wei and Sixiang Chen and Liyun Li and Xianming Liu and Ming Lu and Yang Wang and Shanghang Zhang", "abstract": "  Vision-Language-Action (VLA) models have demonstrated significant potential\nin complex scene understanding and action reasoning, leading to their\nincreasing adoption in end-to-end autonomous driving systems. However, the long\nvisual tokens of VLA models greatly increase computational costs. Current\nvisual token pruning methods in Vision-Language Models (VLM) rely on either\nvisual token similarity or visual-text attention, but both have shown poor\nperformance in autonomous driving scenarios. Given that human drivers\nconcentrate on relevant foreground areas while driving, we assert that\nretaining visual tokens containing this foreground information is essential for\neffective decision-making. Inspired by this, we propose FastDriveVLA, a novel\nreconstruction-based vision token pruning framework designed specifically for\nautonomous driving. FastDriveVLA includes a plug-and-play visual token pruner\ncalled ReconPruner, which prioritizes foreground information through MAE-style\npixel reconstruction. A novel adversarial foreground-background reconstruction\nstrategy is designed to train ReconPruner for the visual encoder of VLA models.\nOnce trained, ReconPruner can be seamlessly applied to different VLA models\nwith the same visual encoder without retraining. To train ReconPruner, we also\nintroduce a large-scale dataset called nuScenes-FG, consisting of 241K\nimage-mask pairs with annotated foreground regions. Our approach achieves\nstate-of-the-art results on the nuScenes open-loop planning benchmark across\ndifferent pruning ratios.\n", "link": "http://arxiv.org/abs/2507.23318v3", "date": "2025-09-16", "relevancy": 2.285, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5797}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5797}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastDriveVLA%3A%20Efficient%20End-to-End%20Driving%20via%20Plug-and-Play%0A%20%20Reconstruction-based%20Token%20Pruning&body=Title%3A%20FastDriveVLA%3A%20Efficient%20End-to-End%20Driving%20via%20Plug-and-Play%0A%20%20Reconstruction-based%20Token%20Pruning%0AAuthor%3A%20Jiajun%20Cao%20and%20Qizhe%20Zhang%20and%20Peidong%20Jia%20and%20Xuhui%20Zhao%20and%20Bo%20Lan%20and%20Xiaoan%20Zhang%20and%20Zhuo%20Li%20and%20Xiaobao%20Wei%20and%20Sixiang%20Chen%20and%20Liyun%20Li%20and%20Xianming%20Liu%20and%20Ming%20Lu%20and%20Yang%20Wang%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Vision-Language-Action%20%28VLA%29%20models%20have%20demonstrated%20significant%20potential%0Ain%20complex%20scene%20understanding%20and%20action%20reasoning%2C%20leading%20to%20their%0Aincreasing%20adoption%20in%20end-to-end%20autonomous%20driving%20systems.%20However%2C%20the%20long%0Avisual%20tokens%20of%20VLA%20models%20greatly%20increase%20computational%20costs.%20Current%0Avisual%20token%20pruning%20methods%20in%20Vision-Language%20Models%20%28VLM%29%20rely%20on%20either%0Avisual%20token%20similarity%20or%20visual-text%20attention%2C%20but%20both%20have%20shown%20poor%0Aperformance%20in%20autonomous%20driving%20scenarios.%20Given%20that%20human%20drivers%0Aconcentrate%20on%20relevant%20foreground%20areas%20while%20driving%2C%20we%20assert%20that%0Aretaining%20visual%20tokens%20containing%20this%20foreground%20information%20is%20essential%20for%0Aeffective%20decision-making.%20Inspired%20by%20this%2C%20we%20propose%20FastDriveVLA%2C%20a%20novel%0Areconstruction-based%20vision%20token%20pruning%20framework%20designed%20specifically%20for%0Aautonomous%20driving.%20FastDriveVLA%20includes%20a%20plug-and-play%20visual%20token%20pruner%0Acalled%20ReconPruner%2C%20which%20prioritizes%20foreground%20information%20through%20MAE-style%0Apixel%20reconstruction.%20A%20novel%20adversarial%20foreground-background%20reconstruction%0Astrategy%20is%20designed%20to%20train%20ReconPruner%20for%20the%20visual%20encoder%20of%20VLA%20models.%0AOnce%20trained%2C%20ReconPruner%20can%20be%20seamlessly%20applied%20to%20different%20VLA%20models%0Awith%20the%20same%20visual%20encoder%20without%20retraining.%20To%20train%20ReconPruner%2C%20we%20also%0Aintroduce%20a%20large-scale%20dataset%20called%20nuScenes-FG%2C%20consisting%20of%20241K%0Aimage-mask%20pairs%20with%20annotated%20foreground%20regions.%20Our%20approach%20achieves%0Astate-of-the-art%20results%20on%20the%20nuScenes%20open-loop%20planning%20benchmark%20across%0Adifferent%20pruning%20ratios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.23318v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastDriveVLA%253A%2520Efficient%2520End-to-End%2520Driving%2520via%2520Plug-and-Play%250A%2520%2520Reconstruction-based%2520Token%2520Pruning%26entry.906535625%3DJiajun%2520Cao%2520and%2520Qizhe%2520Zhang%2520and%2520Peidong%2520Jia%2520and%2520Xuhui%2520Zhao%2520and%2520Bo%2520Lan%2520and%2520Xiaoan%2520Zhang%2520and%2520Zhuo%2520Li%2520and%2520Xiaobao%2520Wei%2520and%2520Sixiang%2520Chen%2520and%2520Liyun%2520Li%2520and%2520Xianming%2520Liu%2520and%2520Ming%2520Lu%2520and%2520Yang%2520Wang%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520demonstrated%2520significant%2520potential%250Ain%2520complex%2520scene%2520understanding%2520and%2520action%2520reasoning%252C%2520leading%2520to%2520their%250Aincreasing%2520adoption%2520in%2520end-to-end%2520autonomous%2520driving%2520systems.%2520However%252C%2520the%2520long%250Avisual%2520tokens%2520of%2520VLA%2520models%2520greatly%2520increase%2520computational%2520costs.%2520Current%250Avisual%2520token%2520pruning%2520methods%2520in%2520Vision-Language%2520Models%2520%2528VLM%2529%2520rely%2520on%2520either%250Avisual%2520token%2520similarity%2520or%2520visual-text%2520attention%252C%2520but%2520both%2520have%2520shown%2520poor%250Aperformance%2520in%2520autonomous%2520driving%2520scenarios.%2520Given%2520that%2520human%2520drivers%250Aconcentrate%2520on%2520relevant%2520foreground%2520areas%2520while%2520driving%252C%2520we%2520assert%2520that%250Aretaining%2520visual%2520tokens%2520containing%2520this%2520foreground%2520information%2520is%2520essential%2520for%250Aeffective%2520decision-making.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520FastDriveVLA%252C%2520a%2520novel%250Areconstruction-based%2520vision%2520token%2520pruning%2520framework%2520designed%2520specifically%2520for%250Aautonomous%2520driving.%2520FastDriveVLA%2520includes%2520a%2520plug-and-play%2520visual%2520token%2520pruner%250Acalled%2520ReconPruner%252C%2520which%2520prioritizes%2520foreground%2520information%2520through%2520MAE-style%250Apixel%2520reconstruction.%2520A%2520novel%2520adversarial%2520foreground-background%2520reconstruction%250Astrategy%2520is%2520designed%2520to%2520train%2520ReconPruner%2520for%2520the%2520visual%2520encoder%2520of%2520VLA%2520models.%250AOnce%2520trained%252C%2520ReconPruner%2520can%2520be%2520seamlessly%2520applied%2520to%2520different%2520VLA%2520models%250Awith%2520the%2520same%2520visual%2520encoder%2520without%2520retraining.%2520To%2520train%2520ReconPruner%252C%2520we%2520also%250Aintroduce%2520a%2520large-scale%2520dataset%2520called%2520nuScenes-FG%252C%2520consisting%2520of%2520241K%250Aimage-mask%2520pairs%2520with%2520annotated%2520foreground%2520regions.%2520Our%2520approach%2520achieves%250Astate-of-the-art%2520results%2520on%2520the%2520nuScenes%2520open-loop%2520planning%2520benchmark%2520across%250Adifferent%2520pruning%2520ratios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.23318v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastDriveVLA%3A%20Efficient%20End-to-End%20Driving%20via%20Plug-and-Play%0A%20%20Reconstruction-based%20Token%20Pruning&entry.906535625=Jiajun%20Cao%20and%20Qizhe%20Zhang%20and%20Peidong%20Jia%20and%20Xuhui%20Zhao%20and%20Bo%20Lan%20and%20Xiaoan%20Zhang%20and%20Zhuo%20Li%20and%20Xiaobao%20Wei%20and%20Sixiang%20Chen%20and%20Liyun%20Li%20and%20Xianming%20Liu%20and%20Ming%20Lu%20and%20Yang%20Wang%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Vision-Language-Action%20%28VLA%29%20models%20have%20demonstrated%20significant%20potential%0Ain%20complex%20scene%20understanding%20and%20action%20reasoning%2C%20leading%20to%20their%0Aincreasing%20adoption%20in%20end-to-end%20autonomous%20driving%20systems.%20However%2C%20the%20long%0Avisual%20tokens%20of%20VLA%20models%20greatly%20increase%20computational%20costs.%20Current%0Avisual%20token%20pruning%20methods%20in%20Vision-Language%20Models%20%28VLM%29%20rely%20on%20either%0Avisual%20token%20similarity%20or%20visual-text%20attention%2C%20but%20both%20have%20shown%20poor%0Aperformance%20in%20autonomous%20driving%20scenarios.%20Given%20that%20human%20drivers%0Aconcentrate%20on%20relevant%20foreground%20areas%20while%20driving%2C%20we%20assert%20that%0Aretaining%20visual%20tokens%20containing%20this%20foreground%20information%20is%20essential%20for%0Aeffective%20decision-making.%20Inspired%20by%20this%2C%20we%20propose%20FastDriveVLA%2C%20a%20novel%0Areconstruction-based%20vision%20token%20pruning%20framework%20designed%20specifically%20for%0Aautonomous%20driving.%20FastDriveVLA%20includes%20a%20plug-and-play%20visual%20token%20pruner%0Acalled%20ReconPruner%2C%20which%20prioritizes%20foreground%20information%20through%20MAE-style%0Apixel%20reconstruction.%20A%20novel%20adversarial%20foreground-background%20reconstruction%0Astrategy%20is%20designed%20to%20train%20ReconPruner%20for%20the%20visual%20encoder%20of%20VLA%20models.%0AOnce%20trained%2C%20ReconPruner%20can%20be%20seamlessly%20applied%20to%20different%20VLA%20models%0Awith%20the%20same%20visual%20encoder%20without%20retraining.%20To%20train%20ReconPruner%2C%20we%20also%0Aintroduce%20a%20large-scale%20dataset%20called%20nuScenes-FG%2C%20consisting%20of%20241K%0Aimage-mask%20pairs%20with%20annotated%20foreground%20regions.%20Our%20approach%20achieves%0Astate-of-the-art%20results%20on%20the%20nuScenes%20open-loop%20planning%20benchmark%20across%0Adifferent%20pruning%20ratios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.23318v3&entry.124074799=Read"},
{"title": "Learn from Global Correlations: Enhancing Evolutionary Algorithm via\n  Spectral GNN", "author": "Kaichen Ouyang and Zong Ke and Shengwei Fu and Lingjie Liu and Puning Zhao and Dayu Hu", "abstract": "  Evolutionary algorithms (EAs) simulate natural selection but have two main\nlimitations: (1) they rarely update individuals based on global correlations,\nlimiting comprehensive learning; (2) they struggle with balancing exploration\nand exploitation, where excessive exploitation causes premature convergence,\nand excessive exploration slows down the search. Moreover, EAs often depend on\nmanual parameter settings, which can disrupt the exploration-exploitation\nbalance. To address these issues, we propose Graph Neural Evolution (GNE), a\nnovel EA framework. GNE represents the population as a graph, where nodes\nrepresent individuals, and edges capture their relationships, enabling global\ninformation usage. GNE utilizes spectral graph neural networks (GNNs) to\ndecompose evolutionary signals into frequency components, applying a filtering\nfunction to fuse these components. High-frequency components capture diverse\nglobal information, while low-frequency ones capture more consistent\ninformation. This explicit frequency filtering strategy directly controls\nglobal-scale features through frequency components, overcoming the limitations\nof manual parameter settings and making the exploration-exploitation control\nmore interpretable and manageable. Tests on nine benchmark functions (e.g.,\nSphere, Rastrigin, Rosenbrock) show that GNE outperforms classical (GA, DE,\nCMA-ES) and advanced algorithms (SDAES, RL-SHADE) under various conditions,\nincluding noise-corrupted and optimal solution deviation scenarios. GNE\nachieves solutions several orders of magnitude better (e.g., 3.07e-20 mean on\nSphere vs. 1.51e-07).\n", "link": "http://arxiv.org/abs/2412.17629v4", "date": "2025-09-16", "relevancy": 2.2758, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4778}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4495}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learn%20from%20Global%20Correlations%3A%20Enhancing%20Evolutionary%20Algorithm%20via%0A%20%20Spectral%20GNN&body=Title%3A%20Learn%20from%20Global%20Correlations%3A%20Enhancing%20Evolutionary%20Algorithm%20via%0A%20%20Spectral%20GNN%0AAuthor%3A%20Kaichen%20Ouyang%20and%20Zong%20Ke%20and%20Shengwei%20Fu%20and%20Lingjie%20Liu%20and%20Puning%20Zhao%20and%20Dayu%20Hu%0AAbstract%3A%20%20%20Evolutionary%20algorithms%20%28EAs%29%20simulate%20natural%20selection%20but%20have%20two%20main%0Alimitations%3A%20%281%29%20they%20rarely%20update%20individuals%20based%20on%20global%20correlations%2C%0Alimiting%20comprehensive%20learning%3B%20%282%29%20they%20struggle%20with%20balancing%20exploration%0Aand%20exploitation%2C%20where%20excessive%20exploitation%20causes%20premature%20convergence%2C%0Aand%20excessive%20exploration%20slows%20down%20the%20search.%20Moreover%2C%20EAs%20often%20depend%20on%0Amanual%20parameter%20settings%2C%20which%20can%20disrupt%20the%20exploration-exploitation%0Abalance.%20To%20address%20these%20issues%2C%20we%20propose%20Graph%20Neural%20Evolution%20%28GNE%29%2C%20a%0Anovel%20EA%20framework.%20GNE%20represents%20the%20population%20as%20a%20graph%2C%20where%20nodes%0Arepresent%20individuals%2C%20and%20edges%20capture%20their%20relationships%2C%20enabling%20global%0Ainformation%20usage.%20GNE%20utilizes%20spectral%20graph%20neural%20networks%20%28GNNs%29%20to%0Adecompose%20evolutionary%20signals%20into%20frequency%20components%2C%20applying%20a%20filtering%0Afunction%20to%20fuse%20these%20components.%20High-frequency%20components%20capture%20diverse%0Aglobal%20information%2C%20while%20low-frequency%20ones%20capture%20more%20consistent%0Ainformation.%20This%20explicit%20frequency%20filtering%20strategy%20directly%20controls%0Aglobal-scale%20features%20through%20frequency%20components%2C%20overcoming%20the%20limitations%0Aof%20manual%20parameter%20settings%20and%20making%20the%20exploration-exploitation%20control%0Amore%20interpretable%20and%20manageable.%20Tests%20on%20nine%20benchmark%20functions%20%28e.g.%2C%0ASphere%2C%20Rastrigin%2C%20Rosenbrock%29%20show%20that%20GNE%20outperforms%20classical%20%28GA%2C%20DE%2C%0ACMA-ES%29%20and%20advanced%20algorithms%20%28SDAES%2C%20RL-SHADE%29%20under%20various%20conditions%2C%0Aincluding%20noise-corrupted%20and%20optimal%20solution%20deviation%20scenarios.%20GNE%0Aachieves%20solutions%20several%20orders%20of%20magnitude%20better%20%28e.g.%2C%203.07e-20%20mean%20on%0ASphere%20vs.%201.51e-07%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.17629v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearn%2520from%2520Global%2520Correlations%253A%2520Enhancing%2520Evolutionary%2520Algorithm%2520via%250A%2520%2520Spectral%2520GNN%26entry.906535625%3DKaichen%2520Ouyang%2520and%2520Zong%2520Ke%2520and%2520Shengwei%2520Fu%2520and%2520Lingjie%2520Liu%2520and%2520Puning%2520Zhao%2520and%2520Dayu%2520Hu%26entry.1292438233%3D%2520%2520Evolutionary%2520algorithms%2520%2528EAs%2529%2520simulate%2520natural%2520selection%2520but%2520have%2520two%2520main%250Alimitations%253A%2520%25281%2529%2520they%2520rarely%2520update%2520individuals%2520based%2520on%2520global%2520correlations%252C%250Alimiting%2520comprehensive%2520learning%253B%2520%25282%2529%2520they%2520struggle%2520with%2520balancing%2520exploration%250Aand%2520exploitation%252C%2520where%2520excessive%2520exploitation%2520causes%2520premature%2520convergence%252C%250Aand%2520excessive%2520exploration%2520slows%2520down%2520the%2520search.%2520Moreover%252C%2520EAs%2520often%2520depend%2520on%250Amanual%2520parameter%2520settings%252C%2520which%2520can%2520disrupt%2520the%2520exploration-exploitation%250Abalance.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520Graph%2520Neural%2520Evolution%2520%2528GNE%2529%252C%2520a%250Anovel%2520EA%2520framework.%2520GNE%2520represents%2520the%2520population%2520as%2520a%2520graph%252C%2520where%2520nodes%250Arepresent%2520individuals%252C%2520and%2520edges%2520capture%2520their%2520relationships%252C%2520enabling%2520global%250Ainformation%2520usage.%2520GNE%2520utilizes%2520spectral%2520graph%2520neural%2520networks%2520%2528GNNs%2529%2520to%250Adecompose%2520evolutionary%2520signals%2520into%2520frequency%2520components%252C%2520applying%2520a%2520filtering%250Afunction%2520to%2520fuse%2520these%2520components.%2520High-frequency%2520components%2520capture%2520diverse%250Aglobal%2520information%252C%2520while%2520low-frequency%2520ones%2520capture%2520more%2520consistent%250Ainformation.%2520This%2520explicit%2520frequency%2520filtering%2520strategy%2520directly%2520controls%250Aglobal-scale%2520features%2520through%2520frequency%2520components%252C%2520overcoming%2520the%2520limitations%250Aof%2520manual%2520parameter%2520settings%2520and%2520making%2520the%2520exploration-exploitation%2520control%250Amore%2520interpretable%2520and%2520manageable.%2520Tests%2520on%2520nine%2520benchmark%2520functions%2520%2528e.g.%252C%250ASphere%252C%2520Rastrigin%252C%2520Rosenbrock%2529%2520show%2520that%2520GNE%2520outperforms%2520classical%2520%2528GA%252C%2520DE%252C%250ACMA-ES%2529%2520and%2520advanced%2520algorithms%2520%2528SDAES%252C%2520RL-SHADE%2529%2520under%2520various%2520conditions%252C%250Aincluding%2520noise-corrupted%2520and%2520optimal%2520solution%2520deviation%2520scenarios.%2520GNE%250Aachieves%2520solutions%2520several%2520orders%2520of%2520magnitude%2520better%2520%2528e.g.%252C%25203.07e-20%2520mean%2520on%250ASphere%2520vs.%25201.51e-07%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17629v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learn%20from%20Global%20Correlations%3A%20Enhancing%20Evolutionary%20Algorithm%20via%0A%20%20Spectral%20GNN&entry.906535625=Kaichen%20Ouyang%20and%20Zong%20Ke%20and%20Shengwei%20Fu%20and%20Lingjie%20Liu%20and%20Puning%20Zhao%20and%20Dayu%20Hu&entry.1292438233=%20%20Evolutionary%20algorithms%20%28EAs%29%20simulate%20natural%20selection%20but%20have%20two%20main%0Alimitations%3A%20%281%29%20they%20rarely%20update%20individuals%20based%20on%20global%20correlations%2C%0Alimiting%20comprehensive%20learning%3B%20%282%29%20they%20struggle%20with%20balancing%20exploration%0Aand%20exploitation%2C%20where%20excessive%20exploitation%20causes%20premature%20convergence%2C%0Aand%20excessive%20exploration%20slows%20down%20the%20search.%20Moreover%2C%20EAs%20often%20depend%20on%0Amanual%20parameter%20settings%2C%20which%20can%20disrupt%20the%20exploration-exploitation%0Abalance.%20To%20address%20these%20issues%2C%20we%20propose%20Graph%20Neural%20Evolution%20%28GNE%29%2C%20a%0Anovel%20EA%20framework.%20GNE%20represents%20the%20population%20as%20a%20graph%2C%20where%20nodes%0Arepresent%20individuals%2C%20and%20edges%20capture%20their%20relationships%2C%20enabling%20global%0Ainformation%20usage.%20GNE%20utilizes%20spectral%20graph%20neural%20networks%20%28GNNs%29%20to%0Adecompose%20evolutionary%20signals%20into%20frequency%20components%2C%20applying%20a%20filtering%0Afunction%20to%20fuse%20these%20components.%20High-frequency%20components%20capture%20diverse%0Aglobal%20information%2C%20while%20low-frequency%20ones%20capture%20more%20consistent%0Ainformation.%20This%20explicit%20frequency%20filtering%20strategy%20directly%20controls%0Aglobal-scale%20features%20through%20frequency%20components%2C%20overcoming%20the%20limitations%0Aof%20manual%20parameter%20settings%20and%20making%20the%20exploration-exploitation%20control%0Amore%20interpretable%20and%20manageable.%20Tests%20on%20nine%20benchmark%20functions%20%28e.g.%2C%0ASphere%2C%20Rastrigin%2C%20Rosenbrock%29%20show%20that%20GNE%20outperforms%20classical%20%28GA%2C%20DE%2C%0ACMA-ES%29%20and%20advanced%20algorithms%20%28SDAES%2C%20RL-SHADE%29%20under%20various%20conditions%2C%0Aincluding%20noise-corrupted%20and%20optimal%20solution%20deviation%20scenarios.%20GNE%0Aachieves%20solutions%20several%20orders%20of%20magnitude%20better%20%28e.g.%2C%203.07e-20%20mean%20on%0ASphere%20vs.%201.51e-07%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.17629v4&entry.124074799=Read"},
{"title": "Curriculum Multi-Task Self-Supervision Improves Lightweight\n  Architectures for Onboard Satellite Hyperspectral Image Segmentation", "author": "Hugo Carlesso and Josiane Mothe and Radu Tudor Ionescu", "abstract": "  Hyperspectral imaging (HSI) captures detailed spectral signatures across\nhundreds of contiguous bands per pixel, being indispensable for remote sensing\napplications such as land-cover classification, change detection, and\nenvironmental monitoring. Due to the high dimensionality of HSI data and the\nslow rate of data transfer in satellite-based systems, compact and efficient\nmodels are required to support onboard processing and minimize the transmission\nof redundant or low-value data, e.g. cloud-covered areas. To this end, we\nintroduce a novel curriculum multi-task self-supervised learning (CMTSSL)\nframework designed for lightweight architectures for HSI analysis. CMTSSL\nintegrates masked image modeling with decoupled spatial and spectral jigsaw\npuzzle solving, guided by a curriculum learning strategy that progressively\nincreases data complexity during self-supervision. This enables the encoder to\njointly capture fine-grained spectral continuity, spatial structure, and global\nsemantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneously\naddresses spatial and spectral reasoning within a unified and computationally\nefficient design, being particularly suitable for training lightweight models\nfor onboard satellite deployment. We validate our approach on four public\nbenchmark datasets, demonstrating consistent gains in downstream segmentation\ntasks, using architectures that are over 16,000x lighter than some\nstate-of-the-art models. These results highlight the potential of CMTSSL in\ngeneralizable representation learning with lightweight architectures for\nreal-world HSI applications. Our code is publicly available at\nhttps://github.com/hugocarlesso/CMTSSL.\n", "link": "http://arxiv.org/abs/2509.13229v1", "date": "2025-09-16", "relevancy": 2.2752, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5795}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.566}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curriculum%20Multi-Task%20Self-Supervision%20Improves%20Lightweight%0A%20%20Architectures%20for%20Onboard%20Satellite%20Hyperspectral%20Image%20Segmentation&body=Title%3A%20Curriculum%20Multi-Task%20Self-Supervision%20Improves%20Lightweight%0A%20%20Architectures%20for%20Onboard%20Satellite%20Hyperspectral%20Image%20Segmentation%0AAuthor%3A%20Hugo%20Carlesso%20and%20Josiane%20Mothe%20and%20Radu%20Tudor%20Ionescu%0AAbstract%3A%20%20%20Hyperspectral%20imaging%20%28HSI%29%20captures%20detailed%20spectral%20signatures%20across%0Ahundreds%20of%20contiguous%20bands%20per%20pixel%2C%20being%20indispensable%20for%20remote%20sensing%0Aapplications%20such%20as%20land-cover%20classification%2C%20change%20detection%2C%20and%0Aenvironmental%20monitoring.%20Due%20to%20the%20high%20dimensionality%20of%20HSI%20data%20and%20the%0Aslow%20rate%20of%20data%20transfer%20in%20satellite-based%20systems%2C%20compact%20and%20efficient%0Amodels%20are%20required%20to%20support%20onboard%20processing%20and%20minimize%20the%20transmission%0Aof%20redundant%20or%20low-value%20data%2C%20e.g.%20cloud-covered%20areas.%20To%20this%20end%2C%20we%0Aintroduce%20a%20novel%20curriculum%20multi-task%20self-supervised%20learning%20%28CMTSSL%29%0Aframework%20designed%20for%20lightweight%20architectures%20for%20HSI%20analysis.%20CMTSSL%0Aintegrates%20masked%20image%20modeling%20with%20decoupled%20spatial%20and%20spectral%20jigsaw%0Apuzzle%20solving%2C%20guided%20by%20a%20curriculum%20learning%20strategy%20that%20progressively%0Aincreases%20data%20complexity%20during%20self-supervision.%20This%20enables%20the%20encoder%20to%0Ajointly%20capture%20fine-grained%20spectral%20continuity%2C%20spatial%20structure%2C%20and%20global%0Asemantic%20features.%20Unlike%20prior%20dual-task%20SSL%20methods%2C%20CMTSSL%20simultaneously%0Aaddresses%20spatial%20and%20spectral%20reasoning%20within%20a%20unified%20and%20computationally%0Aefficient%20design%2C%20being%20particularly%20suitable%20for%20training%20lightweight%20models%0Afor%20onboard%20satellite%20deployment.%20We%20validate%20our%20approach%20on%20four%20public%0Abenchmark%20datasets%2C%20demonstrating%20consistent%20gains%20in%20downstream%20segmentation%0Atasks%2C%20using%20architectures%20that%20are%20over%2016%2C000x%20lighter%20than%20some%0Astate-of-the-art%20models.%20These%20results%20highlight%20the%20potential%20of%20CMTSSL%20in%0Ageneralizable%20representation%20learning%20with%20lightweight%20architectures%20for%0Areal-world%20HSI%20applications.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/hugocarlesso/CMTSSL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13229v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurriculum%2520Multi-Task%2520Self-Supervision%2520Improves%2520Lightweight%250A%2520%2520Architectures%2520for%2520Onboard%2520Satellite%2520Hyperspectral%2520Image%2520Segmentation%26entry.906535625%3DHugo%2520Carlesso%2520and%2520Josiane%2520Mothe%2520and%2520Radu%2520Tudor%2520Ionescu%26entry.1292438233%3D%2520%2520Hyperspectral%2520imaging%2520%2528HSI%2529%2520captures%2520detailed%2520spectral%2520signatures%2520across%250Ahundreds%2520of%2520contiguous%2520bands%2520per%2520pixel%252C%2520being%2520indispensable%2520for%2520remote%2520sensing%250Aapplications%2520such%2520as%2520land-cover%2520classification%252C%2520change%2520detection%252C%2520and%250Aenvironmental%2520monitoring.%2520Due%2520to%2520the%2520high%2520dimensionality%2520of%2520HSI%2520data%2520and%2520the%250Aslow%2520rate%2520of%2520data%2520transfer%2520in%2520satellite-based%2520systems%252C%2520compact%2520and%2520efficient%250Amodels%2520are%2520required%2520to%2520support%2520onboard%2520processing%2520and%2520minimize%2520the%2520transmission%250Aof%2520redundant%2520or%2520low-value%2520data%252C%2520e.g.%2520cloud-covered%2520areas.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520a%2520novel%2520curriculum%2520multi-task%2520self-supervised%2520learning%2520%2528CMTSSL%2529%250Aframework%2520designed%2520for%2520lightweight%2520architectures%2520for%2520HSI%2520analysis.%2520CMTSSL%250Aintegrates%2520masked%2520image%2520modeling%2520with%2520decoupled%2520spatial%2520and%2520spectral%2520jigsaw%250Apuzzle%2520solving%252C%2520guided%2520by%2520a%2520curriculum%2520learning%2520strategy%2520that%2520progressively%250Aincreases%2520data%2520complexity%2520during%2520self-supervision.%2520This%2520enables%2520the%2520encoder%2520to%250Ajointly%2520capture%2520fine-grained%2520spectral%2520continuity%252C%2520spatial%2520structure%252C%2520and%2520global%250Asemantic%2520features.%2520Unlike%2520prior%2520dual-task%2520SSL%2520methods%252C%2520CMTSSL%2520simultaneously%250Aaddresses%2520spatial%2520and%2520spectral%2520reasoning%2520within%2520a%2520unified%2520and%2520computationally%250Aefficient%2520design%252C%2520being%2520particularly%2520suitable%2520for%2520training%2520lightweight%2520models%250Afor%2520onboard%2520satellite%2520deployment.%2520We%2520validate%2520our%2520approach%2520on%2520four%2520public%250Abenchmark%2520datasets%252C%2520demonstrating%2520consistent%2520gains%2520in%2520downstream%2520segmentation%250Atasks%252C%2520using%2520architectures%2520that%2520are%2520over%252016%252C000x%2520lighter%2520than%2520some%250Astate-of-the-art%2520models.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520CMTSSL%2520in%250Ageneralizable%2520representation%2520learning%2520with%2520lightweight%2520architectures%2520for%250Areal-world%2520HSI%2520applications.%2520Our%2520code%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/hugocarlesso/CMTSSL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13229v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curriculum%20Multi-Task%20Self-Supervision%20Improves%20Lightweight%0A%20%20Architectures%20for%20Onboard%20Satellite%20Hyperspectral%20Image%20Segmentation&entry.906535625=Hugo%20Carlesso%20and%20Josiane%20Mothe%20and%20Radu%20Tudor%20Ionescu&entry.1292438233=%20%20Hyperspectral%20imaging%20%28HSI%29%20captures%20detailed%20spectral%20signatures%20across%0Ahundreds%20of%20contiguous%20bands%20per%20pixel%2C%20being%20indispensable%20for%20remote%20sensing%0Aapplications%20such%20as%20land-cover%20classification%2C%20change%20detection%2C%20and%0Aenvironmental%20monitoring.%20Due%20to%20the%20high%20dimensionality%20of%20HSI%20data%20and%20the%0Aslow%20rate%20of%20data%20transfer%20in%20satellite-based%20systems%2C%20compact%20and%20efficient%0Amodels%20are%20required%20to%20support%20onboard%20processing%20and%20minimize%20the%20transmission%0Aof%20redundant%20or%20low-value%20data%2C%20e.g.%20cloud-covered%20areas.%20To%20this%20end%2C%20we%0Aintroduce%20a%20novel%20curriculum%20multi-task%20self-supervised%20learning%20%28CMTSSL%29%0Aframework%20designed%20for%20lightweight%20architectures%20for%20HSI%20analysis.%20CMTSSL%0Aintegrates%20masked%20image%20modeling%20with%20decoupled%20spatial%20and%20spectral%20jigsaw%0Apuzzle%20solving%2C%20guided%20by%20a%20curriculum%20learning%20strategy%20that%20progressively%0Aincreases%20data%20complexity%20during%20self-supervision.%20This%20enables%20the%20encoder%20to%0Ajointly%20capture%20fine-grained%20spectral%20continuity%2C%20spatial%20structure%2C%20and%20global%0Asemantic%20features.%20Unlike%20prior%20dual-task%20SSL%20methods%2C%20CMTSSL%20simultaneously%0Aaddresses%20spatial%20and%20spectral%20reasoning%20within%20a%20unified%20and%20computationally%0Aefficient%20design%2C%20being%20particularly%20suitable%20for%20training%20lightweight%20models%0Afor%20onboard%20satellite%20deployment.%20We%20validate%20our%20approach%20on%20four%20public%0Abenchmark%20datasets%2C%20demonstrating%20consistent%20gains%20in%20downstream%20segmentation%0Atasks%2C%20using%20architectures%20that%20are%20over%2016%2C000x%20lighter%20than%20some%0Astate-of-the-art%20models.%20These%20results%20highlight%20the%20potential%20of%20CMTSSL%20in%0Ageneralizable%20representation%20learning%20with%20lightweight%20architectures%20for%0Areal-world%20HSI%20applications.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/hugocarlesso/CMTSSL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13229v1&entry.124074799=Read"},
{"title": "ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic\n  Medical Datasets Generation", "author": "Salvatore Esposito and Mat\u00edas Mattamala and Daniel Rebain and Francis Xiatian Zhang and Kevin Dhaliwal and Mohsen Khadem and Subramanian Ramamoorthy", "abstract": "  Continuum robots are advancing bronchoscopy procedures by accessing complex\nlung airways and enabling targeted interventions. However, their development is\nlimited by the lack of realistic training and test environments: Real data is\ndifficult to collect due to ethical constraints and patient safety concerns,\nand developing autonomy algorithms requires realistic imaging and physical\nfeedback. We present ROOM (Realistic Optical Observation in Medicine), a\ncomprehensive simulation framework designed for generating photorealistic\nbronchoscopy training data. By leveraging patient CT scans, our pipeline\nrenders multi-modal sensor data including RGB images with realistic noise and\nlight specularities, metric depth maps, surface normals, optical flow and point\nclouds at medically relevant scales. We validate the data generated by ROOM in\ntwo canonical tasks for medical robotics -- multi-view pose estimation and\nmonocular depth estimation, demonstrating diverse challenges that\nstate-of-the-art methods must overcome to transfer to these medical settings.\nFurthermore, we show that the data produced by ROOM can be used to fine-tune\nexisting depth estimation models to overcome these challenges, also enabling\nother downstream applications such as navigation. We expect that ROOM will\nenable large-scale data generation across diverse patient anatomies and\nprocedural scenarios that are challenging to capture in clinical settings. Code\nand data: https://github.com/iamsalvatore/room.\n", "link": "http://arxiv.org/abs/2509.13177v1", "date": "2025-09-16", "relevancy": 2.2689, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5678}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5672}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.567}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ROOM%3A%20A%20Physics-Based%20Continuum%20Robot%20Simulator%20for%20Photorealistic%0A%20%20Medical%20Datasets%20Generation&body=Title%3A%20ROOM%3A%20A%20Physics-Based%20Continuum%20Robot%20Simulator%20for%20Photorealistic%0A%20%20Medical%20Datasets%20Generation%0AAuthor%3A%20Salvatore%20Esposito%20and%20Mat%C3%ADas%20Mattamala%20and%20Daniel%20Rebain%20and%20Francis%20Xiatian%20Zhang%20and%20Kevin%20Dhaliwal%20and%20Mohsen%20Khadem%20and%20Subramanian%20Ramamoorthy%0AAbstract%3A%20%20%20Continuum%20robots%20are%20advancing%20bronchoscopy%20procedures%20by%20accessing%20complex%0Alung%20airways%20and%20enabling%20targeted%20interventions.%20However%2C%20their%20development%20is%0Alimited%20by%20the%20lack%20of%20realistic%20training%20and%20test%20environments%3A%20Real%20data%20is%0Adifficult%20to%20collect%20due%20to%20ethical%20constraints%20and%20patient%20safety%20concerns%2C%0Aand%20developing%20autonomy%20algorithms%20requires%20realistic%20imaging%20and%20physical%0Afeedback.%20We%20present%20ROOM%20%28Realistic%20Optical%20Observation%20in%20Medicine%29%2C%20a%0Acomprehensive%20simulation%20framework%20designed%20for%20generating%20photorealistic%0Abronchoscopy%20training%20data.%20By%20leveraging%20patient%20CT%20scans%2C%20our%20pipeline%0Arenders%20multi-modal%20sensor%20data%20including%20RGB%20images%20with%20realistic%20noise%20and%0Alight%20specularities%2C%20metric%20depth%20maps%2C%20surface%20normals%2C%20optical%20flow%20and%20point%0Aclouds%20at%20medically%20relevant%20scales.%20We%20validate%20the%20data%20generated%20by%20ROOM%20in%0Atwo%20canonical%20tasks%20for%20medical%20robotics%20--%20multi-view%20pose%20estimation%20and%0Amonocular%20depth%20estimation%2C%20demonstrating%20diverse%20challenges%20that%0Astate-of-the-art%20methods%20must%20overcome%20to%20transfer%20to%20these%20medical%20settings.%0AFurthermore%2C%20we%20show%20that%20the%20data%20produced%20by%20ROOM%20can%20be%20used%20to%20fine-tune%0Aexisting%20depth%20estimation%20models%20to%20overcome%20these%20challenges%2C%20also%20enabling%0Aother%20downstream%20applications%20such%20as%20navigation.%20We%20expect%20that%20ROOM%20will%0Aenable%20large-scale%20data%20generation%20across%20diverse%20patient%20anatomies%20and%0Aprocedural%20scenarios%20that%20are%20challenging%20to%20capture%20in%20clinical%20settings.%20Code%0Aand%20data%3A%20https%3A//github.com/iamsalvatore/room.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13177v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DROOM%253A%2520A%2520Physics-Based%2520Continuum%2520Robot%2520Simulator%2520for%2520Photorealistic%250A%2520%2520Medical%2520Datasets%2520Generation%26entry.906535625%3DSalvatore%2520Esposito%2520and%2520Mat%25C3%25ADas%2520Mattamala%2520and%2520Daniel%2520Rebain%2520and%2520Francis%2520Xiatian%2520Zhang%2520and%2520Kevin%2520Dhaliwal%2520and%2520Mohsen%2520Khadem%2520and%2520Subramanian%2520Ramamoorthy%26entry.1292438233%3D%2520%2520Continuum%2520robots%2520are%2520advancing%2520bronchoscopy%2520procedures%2520by%2520accessing%2520complex%250Alung%2520airways%2520and%2520enabling%2520targeted%2520interventions.%2520However%252C%2520their%2520development%2520is%250Alimited%2520by%2520the%2520lack%2520of%2520realistic%2520training%2520and%2520test%2520environments%253A%2520Real%2520data%2520is%250Adifficult%2520to%2520collect%2520due%2520to%2520ethical%2520constraints%2520and%2520patient%2520safety%2520concerns%252C%250Aand%2520developing%2520autonomy%2520algorithms%2520requires%2520realistic%2520imaging%2520and%2520physical%250Afeedback.%2520We%2520present%2520ROOM%2520%2528Realistic%2520Optical%2520Observation%2520in%2520Medicine%2529%252C%2520a%250Acomprehensive%2520simulation%2520framework%2520designed%2520for%2520generating%2520photorealistic%250Abronchoscopy%2520training%2520data.%2520By%2520leveraging%2520patient%2520CT%2520scans%252C%2520our%2520pipeline%250Arenders%2520multi-modal%2520sensor%2520data%2520including%2520RGB%2520images%2520with%2520realistic%2520noise%2520and%250Alight%2520specularities%252C%2520metric%2520depth%2520maps%252C%2520surface%2520normals%252C%2520optical%2520flow%2520and%2520point%250Aclouds%2520at%2520medically%2520relevant%2520scales.%2520We%2520validate%2520the%2520data%2520generated%2520by%2520ROOM%2520in%250Atwo%2520canonical%2520tasks%2520for%2520medical%2520robotics%2520--%2520multi-view%2520pose%2520estimation%2520and%250Amonocular%2520depth%2520estimation%252C%2520demonstrating%2520diverse%2520challenges%2520that%250Astate-of-the-art%2520methods%2520must%2520overcome%2520to%2520transfer%2520to%2520these%2520medical%2520settings.%250AFurthermore%252C%2520we%2520show%2520that%2520the%2520data%2520produced%2520by%2520ROOM%2520can%2520be%2520used%2520to%2520fine-tune%250Aexisting%2520depth%2520estimation%2520models%2520to%2520overcome%2520these%2520challenges%252C%2520also%2520enabling%250Aother%2520downstream%2520applications%2520such%2520as%2520navigation.%2520We%2520expect%2520that%2520ROOM%2520will%250Aenable%2520large-scale%2520data%2520generation%2520across%2520diverse%2520patient%2520anatomies%2520and%250Aprocedural%2520scenarios%2520that%2520are%2520challenging%2520to%2520capture%2520in%2520clinical%2520settings.%2520Code%250Aand%2520data%253A%2520https%253A//github.com/iamsalvatore/room.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13177v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROOM%3A%20A%20Physics-Based%20Continuum%20Robot%20Simulator%20for%20Photorealistic%0A%20%20Medical%20Datasets%20Generation&entry.906535625=Salvatore%20Esposito%20and%20Mat%C3%ADas%20Mattamala%20and%20Daniel%20Rebain%20and%20Francis%20Xiatian%20Zhang%20and%20Kevin%20Dhaliwal%20and%20Mohsen%20Khadem%20and%20Subramanian%20Ramamoorthy&entry.1292438233=%20%20Continuum%20robots%20are%20advancing%20bronchoscopy%20procedures%20by%20accessing%20complex%0Alung%20airways%20and%20enabling%20targeted%20interventions.%20However%2C%20their%20development%20is%0Alimited%20by%20the%20lack%20of%20realistic%20training%20and%20test%20environments%3A%20Real%20data%20is%0Adifficult%20to%20collect%20due%20to%20ethical%20constraints%20and%20patient%20safety%20concerns%2C%0Aand%20developing%20autonomy%20algorithms%20requires%20realistic%20imaging%20and%20physical%0Afeedback.%20We%20present%20ROOM%20%28Realistic%20Optical%20Observation%20in%20Medicine%29%2C%20a%0Acomprehensive%20simulation%20framework%20designed%20for%20generating%20photorealistic%0Abronchoscopy%20training%20data.%20By%20leveraging%20patient%20CT%20scans%2C%20our%20pipeline%0Arenders%20multi-modal%20sensor%20data%20including%20RGB%20images%20with%20realistic%20noise%20and%0Alight%20specularities%2C%20metric%20depth%20maps%2C%20surface%20normals%2C%20optical%20flow%20and%20point%0Aclouds%20at%20medically%20relevant%20scales.%20We%20validate%20the%20data%20generated%20by%20ROOM%20in%0Atwo%20canonical%20tasks%20for%20medical%20robotics%20--%20multi-view%20pose%20estimation%20and%0Amonocular%20depth%20estimation%2C%20demonstrating%20diverse%20challenges%20that%0Astate-of-the-art%20methods%20must%20overcome%20to%20transfer%20to%20these%20medical%20settings.%0AFurthermore%2C%20we%20show%20that%20the%20data%20produced%20by%20ROOM%20can%20be%20used%20to%20fine-tune%0Aexisting%20depth%20estimation%20models%20to%20overcome%20these%20challenges%2C%20also%20enabling%0Aother%20downstream%20applications%20such%20as%20navigation.%20We%20expect%20that%20ROOM%20will%0Aenable%20large-scale%20data%20generation%20across%20diverse%20patient%20anatomies%20and%0Aprocedural%20scenarios%20that%20are%20challenging%20to%20capture%20in%20clinical%20settings.%20Code%0Aand%20data%3A%20https%3A//github.com/iamsalvatore/room.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13177v1&entry.124074799=Read"},
{"title": "Beyond Anthropomorphism: Enhancing Grasping and Eliminating a Degree of\n  Freedom by Fusing the Abduction of Digits Four and Five", "author": "Simon Fritsch and Liam Achenbach and Riccardo Bianco and Nicola Irmiger and Gawain Marti and Samuel Visca and Chenyu Yang and Davide Liconti and Barnabas Gavin Cangan and Robert Jomar Malate and Ronan J. Hinchet and Robert K. Katzschmann", "abstract": "  This paper presents the SABD hand, a 16-degree-of-freedom (DoF) robotic hand\nthat departs from purely anthropomorphic designs to achieve an expanded grasp\nenvelope, enable manipulation poses beyond human capability, and reduce the\nrequired number of actuators. This is achieved by combining the\nadduction/abduction (Add/Abd) joint of digits four and five into a single joint\nwith a large range of motion. The combined joint increases the workspace of the\ndigits by 400\\% and reduces the required DoFs while retaining dexterity.\nExperimental results demonstrate that the combined Add/Abd joint enables the\nhand to grasp objects with a side distance of up to 200 mm. Reinforcement\nlearning-based investigations show that the design enables grasping policies\nthat are effective not only for handling larger objects but also for achieving\nenhanced grasp stability. In teleoperated trials, the hand successfully\nperformed 86\\% of attempted grasps on suitable YCB objects, including\nchallenging non-anthropomorphic configurations. These findings validate the\ndesign's ability to enhance grasp stability, flexibility, and dexterous\nmanipulation without added complexity, making it well-suited for a wide range\nof applications.\n", "link": "http://arxiv.org/abs/2509.13074v1", "date": "2025-09-16", "relevancy": 2.2659, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6093}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.562}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Anthropomorphism%3A%20Enhancing%20Grasping%20and%20Eliminating%20a%20Degree%20of%0A%20%20Freedom%20by%20Fusing%20the%20Abduction%20of%20Digits%20Four%20and%20Five&body=Title%3A%20Beyond%20Anthropomorphism%3A%20Enhancing%20Grasping%20and%20Eliminating%20a%20Degree%20of%0A%20%20Freedom%20by%20Fusing%20the%20Abduction%20of%20Digits%20Four%20and%20Five%0AAuthor%3A%20Simon%20Fritsch%20and%20Liam%20Achenbach%20and%20Riccardo%20Bianco%20and%20Nicola%20Irmiger%20and%20Gawain%20Marti%20and%20Samuel%20Visca%20and%20Chenyu%20Yang%20and%20Davide%20Liconti%20and%20Barnabas%20Gavin%20Cangan%20and%20Robert%20Jomar%20Malate%20and%20Ronan%20J.%20Hinchet%20and%20Robert%20K.%20Katzschmann%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20SABD%20hand%2C%20a%2016-degree-of-freedom%20%28DoF%29%20robotic%20hand%0Athat%20departs%20from%20purely%20anthropomorphic%20designs%20to%20achieve%20an%20expanded%20grasp%0Aenvelope%2C%20enable%20manipulation%20poses%20beyond%20human%20capability%2C%20and%20reduce%20the%0Arequired%20number%20of%20actuators.%20This%20is%20achieved%20by%20combining%20the%0Aadduction/abduction%20%28Add/Abd%29%20joint%20of%20digits%20four%20and%20five%20into%20a%20single%20joint%0Awith%20a%20large%20range%20of%20motion.%20The%20combined%20joint%20increases%20the%20workspace%20of%20the%0Adigits%20by%20400%5C%25%20and%20reduces%20the%20required%20DoFs%20while%20retaining%20dexterity.%0AExperimental%20results%20demonstrate%20that%20the%20combined%20Add/Abd%20joint%20enables%20the%0Ahand%20to%20grasp%20objects%20with%20a%20side%20distance%20of%20up%20to%20200%20mm.%20Reinforcement%0Alearning-based%20investigations%20show%20that%20the%20design%20enables%20grasping%20policies%0Athat%20are%20effective%20not%20only%20for%20handling%20larger%20objects%20but%20also%20for%20achieving%0Aenhanced%20grasp%20stability.%20In%20teleoperated%20trials%2C%20the%20hand%20successfully%0Aperformed%2086%5C%25%20of%20attempted%20grasps%20on%20suitable%20YCB%20objects%2C%20including%0Achallenging%20non-anthropomorphic%20configurations.%20These%20findings%20validate%20the%0Adesign%27s%20ability%20to%20enhance%20grasp%20stability%2C%20flexibility%2C%20and%20dexterous%0Amanipulation%20without%20added%20complexity%2C%20making%20it%20well-suited%20for%20a%20wide%20range%0Aof%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Anthropomorphism%253A%2520Enhancing%2520Grasping%2520and%2520Eliminating%2520a%2520Degree%2520of%250A%2520%2520Freedom%2520by%2520Fusing%2520the%2520Abduction%2520of%2520Digits%2520Four%2520and%2520Five%26entry.906535625%3DSimon%2520Fritsch%2520and%2520Liam%2520Achenbach%2520and%2520Riccardo%2520Bianco%2520and%2520Nicola%2520Irmiger%2520and%2520Gawain%2520Marti%2520and%2520Samuel%2520Visca%2520and%2520Chenyu%2520Yang%2520and%2520Davide%2520Liconti%2520and%2520Barnabas%2520Gavin%2520Cangan%2520and%2520Robert%2520Jomar%2520Malate%2520and%2520Ronan%2520J.%2520Hinchet%2520and%2520Robert%2520K.%2520Katzschmann%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520the%2520SABD%2520hand%252C%2520a%252016-degree-of-freedom%2520%2528DoF%2529%2520robotic%2520hand%250Athat%2520departs%2520from%2520purely%2520anthropomorphic%2520designs%2520to%2520achieve%2520an%2520expanded%2520grasp%250Aenvelope%252C%2520enable%2520manipulation%2520poses%2520beyond%2520human%2520capability%252C%2520and%2520reduce%2520the%250Arequired%2520number%2520of%2520actuators.%2520This%2520is%2520achieved%2520by%2520combining%2520the%250Aadduction/abduction%2520%2528Add/Abd%2529%2520joint%2520of%2520digits%2520four%2520and%2520five%2520into%2520a%2520single%2520joint%250Awith%2520a%2520large%2520range%2520of%2520motion.%2520The%2520combined%2520joint%2520increases%2520the%2520workspace%2520of%2520the%250Adigits%2520by%2520400%255C%2525%2520and%2520reduces%2520the%2520required%2520DoFs%2520while%2520retaining%2520dexterity.%250AExperimental%2520results%2520demonstrate%2520that%2520the%2520combined%2520Add/Abd%2520joint%2520enables%2520the%250Ahand%2520to%2520grasp%2520objects%2520with%2520a%2520side%2520distance%2520of%2520up%2520to%2520200%2520mm.%2520Reinforcement%250Alearning-based%2520investigations%2520show%2520that%2520the%2520design%2520enables%2520grasping%2520policies%250Athat%2520are%2520effective%2520not%2520only%2520for%2520handling%2520larger%2520objects%2520but%2520also%2520for%2520achieving%250Aenhanced%2520grasp%2520stability.%2520In%2520teleoperated%2520trials%252C%2520the%2520hand%2520successfully%250Aperformed%252086%255C%2525%2520of%2520attempted%2520grasps%2520on%2520suitable%2520YCB%2520objects%252C%2520including%250Achallenging%2520non-anthropomorphic%2520configurations.%2520These%2520findings%2520validate%2520the%250Adesign%2527s%2520ability%2520to%2520enhance%2520grasp%2520stability%252C%2520flexibility%252C%2520and%2520dexterous%250Amanipulation%2520without%2520added%2520complexity%252C%2520making%2520it%2520well-suited%2520for%2520a%2520wide%2520range%250Aof%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Anthropomorphism%3A%20Enhancing%20Grasping%20and%20Eliminating%20a%20Degree%20of%0A%20%20Freedom%20by%20Fusing%20the%20Abduction%20of%20Digits%20Four%20and%20Five&entry.906535625=Simon%20Fritsch%20and%20Liam%20Achenbach%20and%20Riccardo%20Bianco%20and%20Nicola%20Irmiger%20and%20Gawain%20Marti%20and%20Samuel%20Visca%20and%20Chenyu%20Yang%20and%20Davide%20Liconti%20and%20Barnabas%20Gavin%20Cangan%20and%20Robert%20Jomar%20Malate%20and%20Ronan%20J.%20Hinchet%20and%20Robert%20K.%20Katzschmann&entry.1292438233=%20%20This%20paper%20presents%20the%20SABD%20hand%2C%20a%2016-degree-of-freedom%20%28DoF%29%20robotic%20hand%0Athat%20departs%20from%20purely%20anthropomorphic%20designs%20to%20achieve%20an%20expanded%20grasp%0Aenvelope%2C%20enable%20manipulation%20poses%20beyond%20human%20capability%2C%20and%20reduce%20the%0Arequired%20number%20of%20actuators.%20This%20is%20achieved%20by%20combining%20the%0Aadduction/abduction%20%28Add/Abd%29%20joint%20of%20digits%20four%20and%20five%20into%20a%20single%20joint%0Awith%20a%20large%20range%20of%20motion.%20The%20combined%20joint%20increases%20the%20workspace%20of%20the%0Adigits%20by%20400%5C%25%20and%20reduces%20the%20required%20DoFs%20while%20retaining%20dexterity.%0AExperimental%20results%20demonstrate%20that%20the%20combined%20Add/Abd%20joint%20enables%20the%0Ahand%20to%20grasp%20objects%20with%20a%20side%20distance%20of%20up%20to%20200%20mm.%20Reinforcement%0Alearning-based%20investigations%20show%20that%20the%20design%20enables%20grasping%20policies%0Athat%20are%20effective%20not%20only%20for%20handling%20larger%20objects%20but%20also%20for%20achieving%0Aenhanced%20grasp%20stability.%20In%20teleoperated%20trials%2C%20the%20hand%20successfully%0Aperformed%2086%5C%25%20of%20attempted%20grasps%20on%20suitable%20YCB%20objects%2C%20including%0Achallenging%20non-anthropomorphic%20configurations.%20These%20findings%20validate%20the%0Adesign%27s%20ability%20to%20enhance%20grasp%20stability%2C%20flexibility%2C%20and%20dexterous%0Amanipulation%20without%20added%20complexity%2C%20making%20it%20well-suited%20for%20a%20wide%20range%0Aof%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13074v1&entry.124074799=Read"},
{"title": "Efficient Cold-Start Recommendation via BPE Token-Level Embedding\n  Initialization with LLM", "author": "Yushang Zhao and Xinyue Han and Qian Leng and Qianyi Sun and Haotian Lyu and Chengrui Zhou", "abstract": "  The cold-start issue is the challenge when we talk about recommender systems,\nespecially in the case when we do not have the past interaction data of new\nusers or new items. Content-based features or hybrid solutions are common as\nconventional solutions, but they can only work in a sparse metadata environment\nwith shallow patterns. In this paper, the efficient cold-start recommendation\nstrategy is presented, which is based on the sub word-level representations by\napplying Byte Pair Encoding (BPE) tokenization and pre-trained Large Language\nModel (LLM) embedding in the initialization procedure. We obtain fine-grained\ntoken-level vectors that are aligned with the BPE vocabulary as opposed to\nusing coarse-grained sentence embeddings. Together, these token embeddings can\nbe used as dense semantic priors on unseen entities, making immediate\nrecommendation performance possible without user-item interaction history. Our\nmechanism can be compared to collaborative filtering systems and tested over\nbenchmark datasets with stringent cold-start assumptions. Experimental findings\nshow that the given BPE-LLM method achieves higher Recall@k, NDCG@k, and Hit\nRate measurements compared to the standard baseline and displays the same\ncapability of sufficient computational performance. Furthermore, we demonstrate\nthat using subword-aware embeddings yields better generalizability and is more\ninterpretable, especially within a multilingual and sparse input setting. The\npractical application of token-level semantic initialization as a lightweight,\nbut nevertheless effective extension to modern recommender systems in the\nzero-shot setting is indicated within this work.\n", "link": "http://arxiv.org/abs/2509.13179v1", "date": "2025-09-16", "relevancy": 2.2642, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4648}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Cold-Start%20Recommendation%20via%20BPE%20Token-Level%20Embedding%0A%20%20Initialization%20with%20LLM&body=Title%3A%20Efficient%20Cold-Start%20Recommendation%20via%20BPE%20Token-Level%20Embedding%0A%20%20Initialization%20with%20LLM%0AAuthor%3A%20Yushang%20Zhao%20and%20Xinyue%20Han%20and%20Qian%20Leng%20and%20Qianyi%20Sun%20and%20Haotian%20Lyu%20and%20Chengrui%20Zhou%0AAbstract%3A%20%20%20The%20cold-start%20issue%20is%20the%20challenge%20when%20we%20talk%20about%20recommender%20systems%2C%0Aespecially%20in%20the%20case%20when%20we%20do%20not%20have%20the%20past%20interaction%20data%20of%20new%0Ausers%20or%20new%20items.%20Content-based%20features%20or%20hybrid%20solutions%20are%20common%20as%0Aconventional%20solutions%2C%20but%20they%20can%20only%20work%20in%20a%20sparse%20metadata%20environment%0Awith%20shallow%20patterns.%20In%20this%20paper%2C%20the%20efficient%20cold-start%20recommendation%0Astrategy%20is%20presented%2C%20which%20is%20based%20on%20the%20sub%20word-level%20representations%20by%0Aapplying%20Byte%20Pair%20Encoding%20%28BPE%29%20tokenization%20and%20pre-trained%20Large%20Language%0AModel%20%28LLM%29%20embedding%20in%20the%20initialization%20procedure.%20We%20obtain%20fine-grained%0Atoken-level%20vectors%20that%20are%20aligned%20with%20the%20BPE%20vocabulary%20as%20opposed%20to%0Ausing%20coarse-grained%20sentence%20embeddings.%20Together%2C%20these%20token%20embeddings%20can%0Abe%20used%20as%20dense%20semantic%20priors%20on%20unseen%20entities%2C%20making%20immediate%0Arecommendation%20performance%20possible%20without%20user-item%20interaction%20history.%20Our%0Amechanism%20can%20be%20compared%20to%20collaborative%20filtering%20systems%20and%20tested%20over%0Abenchmark%20datasets%20with%20stringent%20cold-start%20assumptions.%20Experimental%20findings%0Ashow%20that%20the%20given%20BPE-LLM%20method%20achieves%20higher%20Recall%40k%2C%20NDCG%40k%2C%20and%20Hit%0ARate%20measurements%20compared%20to%20the%20standard%20baseline%20and%20displays%20the%20same%0Acapability%20of%20sufficient%20computational%20performance.%20Furthermore%2C%20we%20demonstrate%0Athat%20using%20subword-aware%20embeddings%20yields%20better%20generalizability%20and%20is%20more%0Ainterpretable%2C%20especially%20within%20a%20multilingual%20and%20sparse%20input%20setting.%20The%0Apractical%20application%20of%20token-level%20semantic%20initialization%20as%20a%20lightweight%2C%0Abut%20nevertheless%20effective%20extension%20to%20modern%20recommender%20systems%20in%20the%0Azero-shot%20setting%20is%20indicated%20within%20this%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Cold-Start%2520Recommendation%2520via%2520BPE%2520Token-Level%2520Embedding%250A%2520%2520Initialization%2520with%2520LLM%26entry.906535625%3DYushang%2520Zhao%2520and%2520Xinyue%2520Han%2520and%2520Qian%2520Leng%2520and%2520Qianyi%2520Sun%2520and%2520Haotian%2520Lyu%2520and%2520Chengrui%2520Zhou%26entry.1292438233%3D%2520%2520The%2520cold-start%2520issue%2520is%2520the%2520challenge%2520when%2520we%2520talk%2520about%2520recommender%2520systems%252C%250Aespecially%2520in%2520the%2520case%2520when%2520we%2520do%2520not%2520have%2520the%2520past%2520interaction%2520data%2520of%2520new%250Ausers%2520or%2520new%2520items.%2520Content-based%2520features%2520or%2520hybrid%2520solutions%2520are%2520common%2520as%250Aconventional%2520solutions%252C%2520but%2520they%2520can%2520only%2520work%2520in%2520a%2520sparse%2520metadata%2520environment%250Awith%2520shallow%2520patterns.%2520In%2520this%2520paper%252C%2520the%2520efficient%2520cold-start%2520recommendation%250Astrategy%2520is%2520presented%252C%2520which%2520is%2520based%2520on%2520the%2520sub%2520word-level%2520representations%2520by%250Aapplying%2520Byte%2520Pair%2520Encoding%2520%2528BPE%2529%2520tokenization%2520and%2520pre-trained%2520Large%2520Language%250AModel%2520%2528LLM%2529%2520embedding%2520in%2520the%2520initialization%2520procedure.%2520We%2520obtain%2520fine-grained%250Atoken-level%2520vectors%2520that%2520are%2520aligned%2520with%2520the%2520BPE%2520vocabulary%2520as%2520opposed%2520to%250Ausing%2520coarse-grained%2520sentence%2520embeddings.%2520Together%252C%2520these%2520token%2520embeddings%2520can%250Abe%2520used%2520as%2520dense%2520semantic%2520priors%2520on%2520unseen%2520entities%252C%2520making%2520immediate%250Arecommendation%2520performance%2520possible%2520without%2520user-item%2520interaction%2520history.%2520Our%250Amechanism%2520can%2520be%2520compared%2520to%2520collaborative%2520filtering%2520systems%2520and%2520tested%2520over%250Abenchmark%2520datasets%2520with%2520stringent%2520cold-start%2520assumptions.%2520Experimental%2520findings%250Ashow%2520that%2520the%2520given%2520BPE-LLM%2520method%2520achieves%2520higher%2520Recall%2540k%252C%2520NDCG%2540k%252C%2520and%2520Hit%250ARate%2520measurements%2520compared%2520to%2520the%2520standard%2520baseline%2520and%2520displays%2520the%2520same%250Acapability%2520of%2520sufficient%2520computational%2520performance.%2520Furthermore%252C%2520we%2520demonstrate%250Athat%2520using%2520subword-aware%2520embeddings%2520yields%2520better%2520generalizability%2520and%2520is%2520more%250Ainterpretable%252C%2520especially%2520within%2520a%2520multilingual%2520and%2520sparse%2520input%2520setting.%2520The%250Apractical%2520application%2520of%2520token-level%2520semantic%2520initialization%2520as%2520a%2520lightweight%252C%250Abut%2520nevertheless%2520effective%2520extension%2520to%2520modern%2520recommender%2520systems%2520in%2520the%250Azero-shot%2520setting%2520is%2520indicated%2520within%2520this%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Cold-Start%20Recommendation%20via%20BPE%20Token-Level%20Embedding%0A%20%20Initialization%20with%20LLM&entry.906535625=Yushang%20Zhao%20and%20Xinyue%20Han%20and%20Qian%20Leng%20and%20Qianyi%20Sun%20and%20Haotian%20Lyu%20and%20Chengrui%20Zhou&entry.1292438233=%20%20The%20cold-start%20issue%20is%20the%20challenge%20when%20we%20talk%20about%20recommender%20systems%2C%0Aespecially%20in%20the%20case%20when%20we%20do%20not%20have%20the%20past%20interaction%20data%20of%20new%0Ausers%20or%20new%20items.%20Content-based%20features%20or%20hybrid%20solutions%20are%20common%20as%0Aconventional%20solutions%2C%20but%20they%20can%20only%20work%20in%20a%20sparse%20metadata%20environment%0Awith%20shallow%20patterns.%20In%20this%20paper%2C%20the%20efficient%20cold-start%20recommendation%0Astrategy%20is%20presented%2C%20which%20is%20based%20on%20the%20sub%20word-level%20representations%20by%0Aapplying%20Byte%20Pair%20Encoding%20%28BPE%29%20tokenization%20and%20pre-trained%20Large%20Language%0AModel%20%28LLM%29%20embedding%20in%20the%20initialization%20procedure.%20We%20obtain%20fine-grained%0Atoken-level%20vectors%20that%20are%20aligned%20with%20the%20BPE%20vocabulary%20as%20opposed%20to%0Ausing%20coarse-grained%20sentence%20embeddings.%20Together%2C%20these%20token%20embeddings%20can%0Abe%20used%20as%20dense%20semantic%20priors%20on%20unseen%20entities%2C%20making%20immediate%0Arecommendation%20performance%20possible%20without%20user-item%20interaction%20history.%20Our%0Amechanism%20can%20be%20compared%20to%20collaborative%20filtering%20systems%20and%20tested%20over%0Abenchmark%20datasets%20with%20stringent%20cold-start%20assumptions.%20Experimental%20findings%0Ashow%20that%20the%20given%20BPE-LLM%20method%20achieves%20higher%20Recall%40k%2C%20NDCG%40k%2C%20and%20Hit%0ARate%20measurements%20compared%20to%20the%20standard%20baseline%20and%20displays%20the%20same%0Acapability%20of%20sufficient%20computational%20performance.%20Furthermore%2C%20we%20demonstrate%0Athat%20using%20subword-aware%20embeddings%20yields%20better%20generalizability%20and%20is%20more%0Ainterpretable%2C%20especially%20within%20a%20multilingual%20and%20sparse%20input%20setting.%20The%0Apractical%20application%20of%20token-level%20semantic%20initialization%20as%20a%20lightweight%2C%0Abut%20nevertheless%20effective%20extension%20to%20modern%20recommender%20systems%20in%20the%0Azero-shot%20setting%20is%20indicated%20within%20this%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13179v1&entry.124074799=Read"},
{"title": "MATTER: Multiscale Attention for Registration Error Regression", "author": "Shipeng Liu and Ziliang Xiong and Khac-Hoang Ngo and Per-Erik Forss\u00e9n", "abstract": "  Point cloud registration (PCR) is crucial for many downstream tasks, such as\nsimultaneous localization and mapping (SLAM) and object tracking. This makes\ndetecting and quantifying registration misalignment, i.e.,~{\\it PCR quality\nvalidation}, an important task. All existing methods treat validation as a\nclassification task, aiming to assign the PCR quality to a few classes. In this\nwork, we instead use regression for PCR validation, allowing for a more\nfine-grained quantification of the registration quality. We also extend\npreviously used misalignment-related features by using multiscale extraction\nand attention-based aggregation. This leads to accurate and robust registration\nerror estimation on diverse datasets, especially for point clouds with\nheterogeneous spatial densities. Furthermore, when used to guide a mapping\ndownstream task, our method significantly improves the mapping quality for a\ngiven amount of re-registered frames, compared to the state-of-the-art\nclassification-based method.\n", "link": "http://arxiv.org/abs/2509.12924v1", "date": "2025-09-16", "relevancy": 2.2623, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5822}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5654}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5244}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MATTER%3A%20Multiscale%20Attention%20for%20Registration%20Error%20Regression&body=Title%3A%20MATTER%3A%20Multiscale%20Attention%20for%20Registration%20Error%20Regression%0AAuthor%3A%20Shipeng%20Liu%20and%20Ziliang%20Xiong%20and%20Khac-Hoang%20Ngo%20and%20Per-Erik%20Forss%C3%A9n%0AAbstract%3A%20%20%20Point%20cloud%20registration%20%28PCR%29%20is%20crucial%20for%20many%20downstream%20tasks%2C%20such%20as%0Asimultaneous%20localization%20and%20mapping%20%28SLAM%29%20and%20object%20tracking.%20This%20makes%0Adetecting%20and%20quantifying%20registration%20misalignment%2C%20i.e.%2C~%7B%5Cit%20PCR%20quality%0Avalidation%7D%2C%20an%20important%20task.%20All%20existing%20methods%20treat%20validation%20as%20a%0Aclassification%20task%2C%20aiming%20to%20assign%20the%20PCR%20quality%20to%20a%20few%20classes.%20In%20this%0Awork%2C%20we%20instead%20use%20regression%20for%20PCR%20validation%2C%20allowing%20for%20a%20more%0Afine-grained%20quantification%20of%20the%20registration%20quality.%20We%20also%20extend%0Apreviously%20used%20misalignment-related%20features%20by%20using%20multiscale%20extraction%0Aand%20attention-based%20aggregation.%20This%20leads%20to%20accurate%20and%20robust%20registration%0Aerror%20estimation%20on%20diverse%20datasets%2C%20especially%20for%20point%20clouds%20with%0Aheterogeneous%20spatial%20densities.%20Furthermore%2C%20when%20used%20to%20guide%20a%20mapping%0Adownstream%20task%2C%20our%20method%20significantly%20improves%20the%20mapping%20quality%20for%20a%0Agiven%20amount%20of%20re-registered%20frames%2C%20compared%20to%20the%20state-of-the-art%0Aclassification-based%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMATTER%253A%2520Multiscale%2520Attention%2520for%2520Registration%2520Error%2520Regression%26entry.906535625%3DShipeng%2520Liu%2520and%2520Ziliang%2520Xiong%2520and%2520Khac-Hoang%2520Ngo%2520and%2520Per-Erik%2520Forss%25C3%25A9n%26entry.1292438233%3D%2520%2520Point%2520cloud%2520registration%2520%2528PCR%2529%2520is%2520crucial%2520for%2520many%2520downstream%2520tasks%252C%2520such%2520as%250Asimultaneous%2520localization%2520and%2520mapping%2520%2528SLAM%2529%2520and%2520object%2520tracking.%2520This%2520makes%250Adetecting%2520and%2520quantifying%2520registration%2520misalignment%252C%2520i.e.%252C~%257B%255Cit%2520PCR%2520quality%250Avalidation%257D%252C%2520an%2520important%2520task.%2520All%2520existing%2520methods%2520treat%2520validation%2520as%2520a%250Aclassification%2520task%252C%2520aiming%2520to%2520assign%2520the%2520PCR%2520quality%2520to%2520a%2520few%2520classes.%2520In%2520this%250Awork%252C%2520we%2520instead%2520use%2520regression%2520for%2520PCR%2520validation%252C%2520allowing%2520for%2520a%2520more%250Afine-grained%2520quantification%2520of%2520the%2520registration%2520quality.%2520We%2520also%2520extend%250Apreviously%2520used%2520misalignment-related%2520features%2520by%2520using%2520multiscale%2520extraction%250Aand%2520attention-based%2520aggregation.%2520This%2520leads%2520to%2520accurate%2520and%2520robust%2520registration%250Aerror%2520estimation%2520on%2520diverse%2520datasets%252C%2520especially%2520for%2520point%2520clouds%2520with%250Aheterogeneous%2520spatial%2520densities.%2520Furthermore%252C%2520when%2520used%2520to%2520guide%2520a%2520mapping%250Adownstream%2520task%252C%2520our%2520method%2520significantly%2520improves%2520the%2520mapping%2520quality%2520for%2520a%250Agiven%2520amount%2520of%2520re-registered%2520frames%252C%2520compared%2520to%2520the%2520state-of-the-art%250Aclassification-based%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MATTER%3A%20Multiscale%20Attention%20for%20Registration%20Error%20Regression&entry.906535625=Shipeng%20Liu%20and%20Ziliang%20Xiong%20and%20Khac-Hoang%20Ngo%20and%20Per-Erik%20Forss%C3%A9n&entry.1292438233=%20%20Point%20cloud%20registration%20%28PCR%29%20is%20crucial%20for%20many%20downstream%20tasks%2C%20such%20as%0Asimultaneous%20localization%20and%20mapping%20%28SLAM%29%20and%20object%20tracking.%20This%20makes%0Adetecting%20and%20quantifying%20registration%20misalignment%2C%20i.e.%2C~%7B%5Cit%20PCR%20quality%0Avalidation%7D%2C%20an%20important%20task.%20All%20existing%20methods%20treat%20validation%20as%20a%0Aclassification%20task%2C%20aiming%20to%20assign%20the%20PCR%20quality%20to%20a%20few%20classes.%20In%20this%0Awork%2C%20we%20instead%20use%20regression%20for%20PCR%20validation%2C%20allowing%20for%20a%20more%0Afine-grained%20quantification%20of%20the%20registration%20quality.%20We%20also%20extend%0Apreviously%20used%20misalignment-related%20features%20by%20using%20multiscale%20extraction%0Aand%20attention-based%20aggregation.%20This%20leads%20to%20accurate%20and%20robust%20registration%0Aerror%20estimation%20on%20diverse%20datasets%2C%20especially%20for%20point%20clouds%20with%0Aheterogeneous%20spatial%20densities.%20Furthermore%2C%20when%20used%20to%20guide%20a%20mapping%0Adownstream%20task%2C%20our%20method%20significantly%20improves%20the%20mapping%20quality%20for%20a%0Agiven%20amount%20of%20re-registered%20frames%2C%20compared%20to%20the%20state-of-the-art%0Aclassification-based%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12924v1&entry.124074799=Read"},
{"title": "A Visualized Framework for Event Cooperation with Generative Agents", "author": "Yuyang Tian and Shunqiang Mao and Wenchang Gao and Lanlan Qiu and Tianxing He", "abstract": "  Large Language Models (LLMs) have revolutionized the simulation of agent\nsocieties, enabling autonomous planning, memory formation, and social\ninteractions. However, existing frameworks often overlook systematic\nevaluations for event organization and lack visualized integration with\nphysically grounded environments, limiting agents' ability to navigate spaces\nand interact with items realistically. We develop MiniAgentPro, a visualization\nplatform featuring an intuitive map editor for customizing environments and a\nsimulation player with smooth animations. Based on this tool, we introduce a\ncomprehensive test set comprising eight diverse event scenarios with basic and\nhard variants to assess agents' ability. Evaluations using GPT-4o demonstrate\nstrong performance in basic settings but highlight coordination challenges in\nhard variants.\n", "link": "http://arxiv.org/abs/2509.13011v1", "date": "2025-09-16", "relevancy": 2.2385, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.579}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5656}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Visualized%20Framework%20for%20Event%20Cooperation%20with%20Generative%20Agents&body=Title%3A%20A%20Visualized%20Framework%20for%20Event%20Cooperation%20with%20Generative%20Agents%0AAuthor%3A%20Yuyang%20Tian%20and%20Shunqiang%20Mao%20and%20Wenchang%20Gao%20and%20Lanlan%20Qiu%20and%20Tianxing%20He%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20the%20simulation%20of%20agent%0Asocieties%2C%20enabling%20autonomous%20planning%2C%20memory%20formation%2C%20and%20social%0Ainteractions.%20However%2C%20existing%20frameworks%20often%20overlook%20systematic%0Aevaluations%20for%20event%20organization%20and%20lack%20visualized%20integration%20with%0Aphysically%20grounded%20environments%2C%20limiting%20agents%27%20ability%20to%20navigate%20spaces%0Aand%20interact%20with%20items%20realistically.%20We%20develop%20MiniAgentPro%2C%20a%20visualization%0Aplatform%20featuring%20an%20intuitive%20map%20editor%20for%20customizing%20environments%20and%20a%0Asimulation%20player%20with%20smooth%20animations.%20Based%20on%20this%20tool%2C%20we%20introduce%20a%0Acomprehensive%20test%20set%20comprising%20eight%20diverse%20event%20scenarios%20with%20basic%20and%0Ahard%20variants%20to%20assess%20agents%27%20ability.%20Evaluations%20using%20GPT-4o%20demonstrate%0Astrong%20performance%20in%20basic%20settings%20but%20highlight%20coordination%20challenges%20in%0Ahard%20variants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13011v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Visualized%2520Framework%2520for%2520Event%2520Cooperation%2520with%2520Generative%2520Agents%26entry.906535625%3DYuyang%2520Tian%2520and%2520Shunqiang%2520Mao%2520and%2520Wenchang%2520Gao%2520and%2520Lanlan%2520Qiu%2520and%2520Tianxing%2520He%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520revolutionized%2520the%2520simulation%2520of%2520agent%250Asocieties%252C%2520enabling%2520autonomous%2520planning%252C%2520memory%2520formation%252C%2520and%2520social%250Ainteractions.%2520However%252C%2520existing%2520frameworks%2520often%2520overlook%2520systematic%250Aevaluations%2520for%2520event%2520organization%2520and%2520lack%2520visualized%2520integration%2520with%250Aphysically%2520grounded%2520environments%252C%2520limiting%2520agents%2527%2520ability%2520to%2520navigate%2520spaces%250Aand%2520interact%2520with%2520items%2520realistically.%2520We%2520develop%2520MiniAgentPro%252C%2520a%2520visualization%250Aplatform%2520featuring%2520an%2520intuitive%2520map%2520editor%2520for%2520customizing%2520environments%2520and%2520a%250Asimulation%2520player%2520with%2520smooth%2520animations.%2520Based%2520on%2520this%2520tool%252C%2520we%2520introduce%2520a%250Acomprehensive%2520test%2520set%2520comprising%2520eight%2520diverse%2520event%2520scenarios%2520with%2520basic%2520and%250Ahard%2520variants%2520to%2520assess%2520agents%2527%2520ability.%2520Evaluations%2520using%2520GPT-4o%2520demonstrate%250Astrong%2520performance%2520in%2520basic%2520settings%2520but%2520highlight%2520coordination%2520challenges%2520in%250Ahard%2520variants.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13011v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Visualized%20Framework%20for%20Event%20Cooperation%20with%20Generative%20Agents&entry.906535625=Yuyang%20Tian%20and%20Shunqiang%20Mao%20and%20Wenchang%20Gao%20and%20Lanlan%20Qiu%20and%20Tianxing%20He&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20the%20simulation%20of%20agent%0Asocieties%2C%20enabling%20autonomous%20planning%2C%20memory%20formation%2C%20and%20social%0Ainteractions.%20However%2C%20existing%20frameworks%20often%20overlook%20systematic%0Aevaluations%20for%20event%20organization%20and%20lack%20visualized%20integration%20with%0Aphysically%20grounded%20environments%2C%20limiting%20agents%27%20ability%20to%20navigate%20spaces%0Aand%20interact%20with%20items%20realistically.%20We%20develop%20MiniAgentPro%2C%20a%20visualization%0Aplatform%20featuring%20an%20intuitive%20map%20editor%20for%20customizing%20environments%20and%20a%0Asimulation%20player%20with%20smooth%20animations.%20Based%20on%20this%20tool%2C%20we%20introduce%20a%0Acomprehensive%20test%20set%20comprising%20eight%20diverse%20event%20scenarios%20with%20basic%20and%0Ahard%20variants%20to%20assess%20agents%27%20ability.%20Evaluations%20using%20GPT-4o%20demonstrate%0Astrong%20performance%20in%20basic%20settings%20but%20highlight%20coordination%20challenges%20in%0Ahard%20variants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13011v1&entry.124074799=Read"},
{"title": "Time-step Mixup for Efficient Spiking Knowledge Transfer from Appearance\n  to Event Domain", "author": "Yuqi Xie and Shuhan Ye and Chong Wang and Jiazhen Xu and Le Shen and Yuanbin Qian and Jiangbo Qian", "abstract": "  The integration of event cameras and spiking neural networks holds great\npromise for energy-efficient visual processing. However, the limited\navailability of event data and the sparse nature of DVS outputs pose challenges\nfor effective training. Although some prior work has attempted to transfer\nsemantic knowledge from RGB datasets to DVS, they often overlook the\nsignificant distribution gap between the two modalities. In this paper, we\npropose Time-step Mixup knowledge transfer (TMKT), a novel fine-grained mixing\nstrategy that exploits the asynchronous nature of SNNs by interpolating RGB and\nDVS inputs at various time-steps. To enable label mixing in cross-modal\nscenarios, we further introduce modality-aware auxiliary learning objectives.\nThese objectives support the time-step mixup process and enhance the model's\nability to discriminate effectively across different modalities. Our approach\nenables smoother knowledge transfer, alleviates modality shift during training,\nand achieves superior performance in spiking image classification tasks.\nExtensive experiments demonstrate the effectiveness of our method across\nmultiple datasets. The code will be released after the double-blind review\nprocess.\n", "link": "http://arxiv.org/abs/2509.12959v1", "date": "2025-09-16", "relevancy": 2.2264, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.59}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5572}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5426}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Time-step%20Mixup%20for%20Efficient%20Spiking%20Knowledge%20Transfer%20from%20Appearance%0A%20%20to%20Event%20Domain&body=Title%3A%20Time-step%20Mixup%20for%20Efficient%20Spiking%20Knowledge%20Transfer%20from%20Appearance%0A%20%20to%20Event%20Domain%0AAuthor%3A%20Yuqi%20Xie%20and%20Shuhan%20Ye%20and%20Chong%20Wang%20and%20Jiazhen%20Xu%20and%20Le%20Shen%20and%20Yuanbin%20Qian%20and%20Jiangbo%20Qian%0AAbstract%3A%20%20%20The%20integration%20of%20event%20cameras%20and%20spiking%20neural%20networks%20holds%20great%0Apromise%20for%20energy-efficient%20visual%20processing.%20However%2C%20the%20limited%0Aavailability%20of%20event%20data%20and%20the%20sparse%20nature%20of%20DVS%20outputs%20pose%20challenges%0Afor%20effective%20training.%20Although%20some%20prior%20work%20has%20attempted%20to%20transfer%0Asemantic%20knowledge%20from%20RGB%20datasets%20to%20DVS%2C%20they%20often%20overlook%20the%0Asignificant%20distribution%20gap%20between%20the%20two%20modalities.%20In%20this%20paper%2C%20we%0Apropose%20Time-step%20Mixup%20knowledge%20transfer%20%28TMKT%29%2C%20a%20novel%20fine-grained%20mixing%0Astrategy%20that%20exploits%20the%20asynchronous%20nature%20of%20SNNs%20by%20interpolating%20RGB%20and%0ADVS%20inputs%20at%20various%20time-steps.%20To%20enable%20label%20mixing%20in%20cross-modal%0Ascenarios%2C%20we%20further%20introduce%20modality-aware%20auxiliary%20learning%20objectives.%0AThese%20objectives%20support%20the%20time-step%20mixup%20process%20and%20enhance%20the%20model%27s%0Aability%20to%20discriminate%20effectively%20across%20different%20modalities.%20Our%20approach%0Aenables%20smoother%20knowledge%20transfer%2C%20alleviates%20modality%20shift%20during%20training%2C%0Aand%20achieves%20superior%20performance%20in%20spiking%20image%20classification%20tasks.%0AExtensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20method%20across%0Amultiple%20datasets.%20The%20code%20will%20be%20released%20after%20the%20double-blind%20review%0Aprocess.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTime-step%2520Mixup%2520for%2520Efficient%2520Spiking%2520Knowledge%2520Transfer%2520from%2520Appearance%250A%2520%2520to%2520Event%2520Domain%26entry.906535625%3DYuqi%2520Xie%2520and%2520Shuhan%2520Ye%2520and%2520Chong%2520Wang%2520and%2520Jiazhen%2520Xu%2520and%2520Le%2520Shen%2520and%2520Yuanbin%2520Qian%2520and%2520Jiangbo%2520Qian%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520event%2520cameras%2520and%2520spiking%2520neural%2520networks%2520holds%2520great%250Apromise%2520for%2520energy-efficient%2520visual%2520processing.%2520However%252C%2520the%2520limited%250Aavailability%2520of%2520event%2520data%2520and%2520the%2520sparse%2520nature%2520of%2520DVS%2520outputs%2520pose%2520challenges%250Afor%2520effective%2520training.%2520Although%2520some%2520prior%2520work%2520has%2520attempted%2520to%2520transfer%250Asemantic%2520knowledge%2520from%2520RGB%2520datasets%2520to%2520DVS%252C%2520they%2520often%2520overlook%2520the%250Asignificant%2520distribution%2520gap%2520between%2520the%2520two%2520modalities.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520Time-step%2520Mixup%2520knowledge%2520transfer%2520%2528TMKT%2529%252C%2520a%2520novel%2520fine-grained%2520mixing%250Astrategy%2520that%2520exploits%2520the%2520asynchronous%2520nature%2520of%2520SNNs%2520by%2520interpolating%2520RGB%2520and%250ADVS%2520inputs%2520at%2520various%2520time-steps.%2520To%2520enable%2520label%2520mixing%2520in%2520cross-modal%250Ascenarios%252C%2520we%2520further%2520introduce%2520modality-aware%2520auxiliary%2520learning%2520objectives.%250AThese%2520objectives%2520support%2520the%2520time-step%2520mixup%2520process%2520and%2520enhance%2520the%2520model%2527s%250Aability%2520to%2520discriminate%2520effectively%2520across%2520different%2520modalities.%2520Our%2520approach%250Aenables%2520smoother%2520knowledge%2520transfer%252C%2520alleviates%2520modality%2520shift%2520during%2520training%252C%250Aand%2520achieves%2520superior%2520performance%2520in%2520spiking%2520image%2520classification%2520tasks.%250AExtensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520across%250Amultiple%2520datasets.%2520The%2520code%2520will%2520be%2520released%2520after%2520the%2520double-blind%2520review%250Aprocess.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time-step%20Mixup%20for%20Efficient%20Spiking%20Knowledge%20Transfer%20from%20Appearance%0A%20%20to%20Event%20Domain&entry.906535625=Yuqi%20Xie%20and%20Shuhan%20Ye%20and%20Chong%20Wang%20and%20Jiazhen%20Xu%20and%20Le%20Shen%20and%20Yuanbin%20Qian%20and%20Jiangbo%20Qian&entry.1292438233=%20%20The%20integration%20of%20event%20cameras%20and%20spiking%20neural%20networks%20holds%20great%0Apromise%20for%20energy-efficient%20visual%20processing.%20However%2C%20the%20limited%0Aavailability%20of%20event%20data%20and%20the%20sparse%20nature%20of%20DVS%20outputs%20pose%20challenges%0Afor%20effective%20training.%20Although%20some%20prior%20work%20has%20attempted%20to%20transfer%0Asemantic%20knowledge%20from%20RGB%20datasets%20to%20DVS%2C%20they%20often%20overlook%20the%0Asignificant%20distribution%20gap%20between%20the%20two%20modalities.%20In%20this%20paper%2C%20we%0Apropose%20Time-step%20Mixup%20knowledge%20transfer%20%28TMKT%29%2C%20a%20novel%20fine-grained%20mixing%0Astrategy%20that%20exploits%20the%20asynchronous%20nature%20of%20SNNs%20by%20interpolating%20RGB%20and%0ADVS%20inputs%20at%20various%20time-steps.%20To%20enable%20label%20mixing%20in%20cross-modal%0Ascenarios%2C%20we%20further%20introduce%20modality-aware%20auxiliary%20learning%20objectives.%0AThese%20objectives%20support%20the%20time-step%20mixup%20process%20and%20enhance%20the%20model%27s%0Aability%20to%20discriminate%20effectively%20across%20different%20modalities.%20Our%20approach%0Aenables%20smoother%20knowledge%20transfer%2C%20alleviates%20modality%20shift%20during%20training%2C%0Aand%20achieves%20superior%20performance%20in%20spiking%20image%20classification%20tasks.%0AExtensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20method%20across%0Amultiple%20datasets.%20The%20code%20will%20be%20released%20after%20the%20double-blind%20review%0Aprocess.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12959v1&entry.124074799=Read"},
{"title": "Do Natural Language Descriptions of Model Activations Convey Privileged\n  Information?", "author": "Millicent Li and Alberto Mario Ceballos Arroyo and Giordano Rogers and Naomi Saphra and Byron C. Wallace", "abstract": "  Recent interpretability methods have proposed to translate LLM internal\nrepresentations into natural language descriptions using a second verbalizer\nLLM. This is intended to illuminate how the target model represents and\noperates on inputs. But do such activation verbalization approaches actually\nprovide privileged knowledge about the internal workings of the target model,\nor do they merely convey information about its inputs? We critically evaluate\npopular verbalization methods across datasets used in prior work and find that\nthey succeed at benchmarks without any access to target model internals,\nsuggesting that these datasets are not ideal for evaluating verbalization\nmethods. We then run controlled experiments which reveal that verbalizations\noften reflect the parametric knowledge of the verbalizer LLM which generated\nthem, rather than the activations of the target LLM being decoded. Taken\ntogether, our results indicate a need for targeted benchmarks and experimental\ncontrols to rigorously assess whether verbalization methods provide meaningful\ninsights into the operations of LLMs.\n", "link": "http://arxiv.org/abs/2509.13316v1", "date": "2025-09-16", "relevancy": 2.2009, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5657}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Natural%20Language%20Descriptions%20of%20Model%20Activations%20Convey%20Privileged%0A%20%20Information%3F&body=Title%3A%20Do%20Natural%20Language%20Descriptions%20of%20Model%20Activations%20Convey%20Privileged%0A%20%20Information%3F%0AAuthor%3A%20Millicent%20Li%20and%20Alberto%20Mario%20Ceballos%20Arroyo%20and%20Giordano%20Rogers%20and%20Naomi%20Saphra%20and%20Byron%20C.%20Wallace%0AAbstract%3A%20%20%20Recent%20interpretability%20methods%20have%20proposed%20to%20translate%20LLM%20internal%0Arepresentations%20into%20natural%20language%20descriptions%20using%20a%20second%20verbalizer%0ALLM.%20This%20is%20intended%20to%20illuminate%20how%20the%20target%20model%20represents%20and%0Aoperates%20on%20inputs.%20But%20do%20such%20activation%20verbalization%20approaches%20actually%0Aprovide%20privileged%20knowledge%20about%20the%20internal%20workings%20of%20the%20target%20model%2C%0Aor%20do%20they%20merely%20convey%20information%20about%20its%20inputs%3F%20We%20critically%20evaluate%0Apopular%20verbalization%20methods%20across%20datasets%20used%20in%20prior%20work%20and%20find%20that%0Athey%20succeed%20at%20benchmarks%20without%20any%20access%20to%20target%20model%20internals%2C%0Asuggesting%20that%20these%20datasets%20are%20not%20ideal%20for%20evaluating%20verbalization%0Amethods.%20We%20then%20run%20controlled%20experiments%20which%20reveal%20that%20verbalizations%0Aoften%20reflect%20the%20parametric%20knowledge%20of%20the%20verbalizer%20LLM%20which%20generated%0Athem%2C%20rather%20than%20the%20activations%20of%20the%20target%20LLM%20being%20decoded.%20Taken%0Atogether%2C%20our%20results%20indicate%20a%20need%20for%20targeted%20benchmarks%20and%20experimental%0Acontrols%20to%20rigorously%20assess%20whether%20verbalization%20methods%20provide%20meaningful%0Ainsights%20into%20the%20operations%20of%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Natural%2520Language%2520Descriptions%2520of%2520Model%2520Activations%2520Convey%2520Privileged%250A%2520%2520Information%253F%26entry.906535625%3DMillicent%2520Li%2520and%2520Alberto%2520Mario%2520Ceballos%2520Arroyo%2520and%2520Giordano%2520Rogers%2520and%2520Naomi%2520Saphra%2520and%2520Byron%2520C.%2520Wallace%26entry.1292438233%3D%2520%2520Recent%2520interpretability%2520methods%2520have%2520proposed%2520to%2520translate%2520LLM%2520internal%250Arepresentations%2520into%2520natural%2520language%2520descriptions%2520using%2520a%2520second%2520verbalizer%250ALLM.%2520This%2520is%2520intended%2520to%2520illuminate%2520how%2520the%2520target%2520model%2520represents%2520and%250Aoperates%2520on%2520inputs.%2520But%2520do%2520such%2520activation%2520verbalization%2520approaches%2520actually%250Aprovide%2520privileged%2520knowledge%2520about%2520the%2520internal%2520workings%2520of%2520the%2520target%2520model%252C%250Aor%2520do%2520they%2520merely%2520convey%2520information%2520about%2520its%2520inputs%253F%2520We%2520critically%2520evaluate%250Apopular%2520verbalization%2520methods%2520across%2520datasets%2520used%2520in%2520prior%2520work%2520and%2520find%2520that%250Athey%2520succeed%2520at%2520benchmarks%2520without%2520any%2520access%2520to%2520target%2520model%2520internals%252C%250Asuggesting%2520that%2520these%2520datasets%2520are%2520not%2520ideal%2520for%2520evaluating%2520verbalization%250Amethods.%2520We%2520then%2520run%2520controlled%2520experiments%2520which%2520reveal%2520that%2520verbalizations%250Aoften%2520reflect%2520the%2520parametric%2520knowledge%2520of%2520the%2520verbalizer%2520LLM%2520which%2520generated%250Athem%252C%2520rather%2520than%2520the%2520activations%2520of%2520the%2520target%2520LLM%2520being%2520decoded.%2520Taken%250Atogether%252C%2520our%2520results%2520indicate%2520a%2520need%2520for%2520targeted%2520benchmarks%2520and%2520experimental%250Acontrols%2520to%2520rigorously%2520assess%2520whether%2520verbalization%2520methods%2520provide%2520meaningful%250Ainsights%2520into%2520the%2520operations%2520of%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Natural%20Language%20Descriptions%20of%20Model%20Activations%20Convey%20Privileged%0A%20%20Information%3F&entry.906535625=Millicent%20Li%20and%20Alberto%20Mario%20Ceballos%20Arroyo%20and%20Giordano%20Rogers%20and%20Naomi%20Saphra%20and%20Byron%20C.%20Wallace&entry.1292438233=%20%20Recent%20interpretability%20methods%20have%20proposed%20to%20translate%20LLM%20internal%0Arepresentations%20into%20natural%20language%20descriptions%20using%20a%20second%20verbalizer%0ALLM.%20This%20is%20intended%20to%20illuminate%20how%20the%20target%20model%20represents%20and%0Aoperates%20on%20inputs.%20But%20do%20such%20activation%20verbalization%20approaches%20actually%0Aprovide%20privileged%20knowledge%20about%20the%20internal%20workings%20of%20the%20target%20model%2C%0Aor%20do%20they%20merely%20convey%20information%20about%20its%20inputs%3F%20We%20critically%20evaluate%0Apopular%20verbalization%20methods%20across%20datasets%20used%20in%20prior%20work%20and%20find%20that%0Athey%20succeed%20at%20benchmarks%20without%20any%20access%20to%20target%20model%20internals%2C%0Asuggesting%20that%20these%20datasets%20are%20not%20ideal%20for%20evaluating%20verbalization%0Amethods.%20We%20then%20run%20controlled%20experiments%20which%20reveal%20that%20verbalizations%0Aoften%20reflect%20the%20parametric%20knowledge%20of%20the%20verbalizer%20LLM%20which%20generated%0Athem%2C%20rather%20than%20the%20activations%20of%20the%20target%20LLM%20being%20decoded.%20Taken%0Atogether%2C%20our%20results%20indicate%20a%20need%20for%20targeted%20benchmarks%20and%20experimental%0Acontrols%20to%20rigorously%20assess%20whether%20verbalization%20methods%20provide%20meaningful%0Ainsights%20into%20the%20operations%20of%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13316v1&entry.124074799=Read"},
{"title": "Spatiotemporal graph neural process for reconstruction, extrapolation,\n  and classification of cardiac trajectories", "author": "Jaume Banus and Augustin C. Ogier and Roger Hullin and Philippe Meyer and Ruud B. van Heeswijk and Jonas Richiardi", "abstract": "  We present a probabilistic framework for modeling structured spatiotemporal\ndynamics from sparse observations, focusing on cardiac motion. Our approach\nintegrates neural ordinary differential equations (NODEs), graph neural\nnetworks (GNNs), and neural processes into a unified model that captures\nuncertainty, temporal continuity, and anatomical structure. We represent\ndynamic systems as spatiotemporal multiplex graphs and model their latent\ntrajectories using a GNN-parameterized vector field. Given the sparse context\nobservations at node and edge levels, the model infers a distribution over\nlatent initial states and control variables, enabling both interpolation and\nextrapolation of trajectories. We validate the method on three synthetic\ndynamical systems (coupled pendulum, Lorenz attractor, and Kuramoto\noscillators) and two real-world cardiac imaging datasets - ACDC (N=150) and UK\nBiobank (N=526) - demonstrating accurate reconstruction, extrapolation, and\ndisease classification capabilities. The model accurately reconstructs\ntrajectories and extrapolates future cardiac cycles from a single observed\ncycle. It achieves state-of-the-art results on the ACDC classification task (up\nto 99% accuracy), and detects atrial fibrillation in UK Biobank subjects with\ncompetitive performance (up to 67% accuracy). This work introduces a flexible\napproach for analyzing cardiac motion and offers a foundation for graph-based\nlearning in structured biomedical spatiotemporal time-series data.\n", "link": "http://arxiv.org/abs/2509.12953v1", "date": "2025-09-16", "relevancy": 2.1875, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.553}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5477}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5294}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatiotemporal%20graph%20neural%20process%20for%20reconstruction%2C%20extrapolation%2C%0A%20%20and%20classification%20of%20cardiac%20trajectories&body=Title%3A%20Spatiotemporal%20graph%20neural%20process%20for%20reconstruction%2C%20extrapolation%2C%0A%20%20and%20classification%20of%20cardiac%20trajectories%0AAuthor%3A%20Jaume%20Banus%20and%20Augustin%20C.%20Ogier%20and%20Roger%20Hullin%20and%20Philippe%20Meyer%20and%20Ruud%20B.%20van%20Heeswijk%20and%20Jonas%20Richiardi%0AAbstract%3A%20%20%20We%20present%20a%20probabilistic%20framework%20for%20modeling%20structured%20spatiotemporal%0Adynamics%20from%20sparse%20observations%2C%20focusing%20on%20cardiac%20motion.%20Our%20approach%0Aintegrates%20neural%20ordinary%20differential%20equations%20%28NODEs%29%2C%20graph%20neural%0Anetworks%20%28GNNs%29%2C%20and%20neural%20processes%20into%20a%20unified%20model%20that%20captures%0Auncertainty%2C%20temporal%20continuity%2C%20and%20anatomical%20structure.%20We%20represent%0Adynamic%20systems%20as%20spatiotemporal%20multiplex%20graphs%20and%20model%20their%20latent%0Atrajectories%20using%20a%20GNN-parameterized%20vector%20field.%20Given%20the%20sparse%20context%0Aobservations%20at%20node%20and%20edge%20levels%2C%20the%20model%20infers%20a%20distribution%20over%0Alatent%20initial%20states%20and%20control%20variables%2C%20enabling%20both%20interpolation%20and%0Aextrapolation%20of%20trajectories.%20We%20validate%20the%20method%20on%20three%20synthetic%0Adynamical%20systems%20%28coupled%20pendulum%2C%20Lorenz%20attractor%2C%20and%20Kuramoto%0Aoscillators%29%20and%20two%20real-world%20cardiac%20imaging%20datasets%20-%20ACDC%20%28N%3D150%29%20and%20UK%0ABiobank%20%28N%3D526%29%20-%20demonstrating%20accurate%20reconstruction%2C%20extrapolation%2C%20and%0Adisease%20classification%20capabilities.%20The%20model%20accurately%20reconstructs%0Atrajectories%20and%20extrapolates%20future%20cardiac%20cycles%20from%20a%20single%20observed%0Acycle.%20It%20achieves%20state-of-the-art%20results%20on%20the%20ACDC%20classification%20task%20%28up%0Ato%2099%25%20accuracy%29%2C%20and%20detects%20atrial%20fibrillation%20in%20UK%20Biobank%20subjects%20with%0Acompetitive%20performance%20%28up%20to%2067%25%20accuracy%29.%20This%20work%20introduces%20a%20flexible%0Aapproach%20for%20analyzing%20cardiac%20motion%20and%20offers%20a%20foundation%20for%20graph-based%0Alearning%20in%20structured%20biomedical%20spatiotemporal%20time-series%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12953v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatiotemporal%2520graph%2520neural%2520process%2520for%2520reconstruction%252C%2520extrapolation%252C%250A%2520%2520and%2520classification%2520of%2520cardiac%2520trajectories%26entry.906535625%3DJaume%2520Banus%2520and%2520Augustin%2520C.%2520Ogier%2520and%2520Roger%2520Hullin%2520and%2520Philippe%2520Meyer%2520and%2520Ruud%2520B.%2520van%2520Heeswijk%2520and%2520Jonas%2520Richiardi%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520probabilistic%2520framework%2520for%2520modeling%2520structured%2520spatiotemporal%250Adynamics%2520from%2520sparse%2520observations%252C%2520focusing%2520on%2520cardiac%2520motion.%2520Our%2520approach%250Aintegrates%2520neural%2520ordinary%2520differential%2520equations%2520%2528NODEs%2529%252C%2520graph%2520neural%250Anetworks%2520%2528GNNs%2529%252C%2520and%2520neural%2520processes%2520into%2520a%2520unified%2520model%2520that%2520captures%250Auncertainty%252C%2520temporal%2520continuity%252C%2520and%2520anatomical%2520structure.%2520We%2520represent%250Adynamic%2520systems%2520as%2520spatiotemporal%2520multiplex%2520graphs%2520and%2520model%2520their%2520latent%250Atrajectories%2520using%2520a%2520GNN-parameterized%2520vector%2520field.%2520Given%2520the%2520sparse%2520context%250Aobservations%2520at%2520node%2520and%2520edge%2520levels%252C%2520the%2520model%2520infers%2520a%2520distribution%2520over%250Alatent%2520initial%2520states%2520and%2520control%2520variables%252C%2520enabling%2520both%2520interpolation%2520and%250Aextrapolation%2520of%2520trajectories.%2520We%2520validate%2520the%2520method%2520on%2520three%2520synthetic%250Adynamical%2520systems%2520%2528coupled%2520pendulum%252C%2520Lorenz%2520attractor%252C%2520and%2520Kuramoto%250Aoscillators%2529%2520and%2520two%2520real-world%2520cardiac%2520imaging%2520datasets%2520-%2520ACDC%2520%2528N%253D150%2529%2520and%2520UK%250ABiobank%2520%2528N%253D526%2529%2520-%2520demonstrating%2520accurate%2520reconstruction%252C%2520extrapolation%252C%2520and%250Adisease%2520classification%2520capabilities.%2520The%2520model%2520accurately%2520reconstructs%250Atrajectories%2520and%2520extrapolates%2520future%2520cardiac%2520cycles%2520from%2520a%2520single%2520observed%250Acycle.%2520It%2520achieves%2520state-of-the-art%2520results%2520on%2520the%2520ACDC%2520classification%2520task%2520%2528up%250Ato%252099%2525%2520accuracy%2529%252C%2520and%2520detects%2520atrial%2520fibrillation%2520in%2520UK%2520Biobank%2520subjects%2520with%250Acompetitive%2520performance%2520%2528up%2520to%252067%2525%2520accuracy%2529.%2520This%2520work%2520introduces%2520a%2520flexible%250Aapproach%2520for%2520analyzing%2520cardiac%2520motion%2520and%2520offers%2520a%2520foundation%2520for%2520graph-based%250Alearning%2520in%2520structured%2520biomedical%2520spatiotemporal%2520time-series%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12953v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatiotemporal%20graph%20neural%20process%20for%20reconstruction%2C%20extrapolation%2C%0A%20%20and%20classification%20of%20cardiac%20trajectories&entry.906535625=Jaume%20Banus%20and%20Augustin%20C.%20Ogier%20and%20Roger%20Hullin%20and%20Philippe%20Meyer%20and%20Ruud%20B.%20van%20Heeswijk%20and%20Jonas%20Richiardi&entry.1292438233=%20%20We%20present%20a%20probabilistic%20framework%20for%20modeling%20structured%20spatiotemporal%0Adynamics%20from%20sparse%20observations%2C%20focusing%20on%20cardiac%20motion.%20Our%20approach%0Aintegrates%20neural%20ordinary%20differential%20equations%20%28NODEs%29%2C%20graph%20neural%0Anetworks%20%28GNNs%29%2C%20and%20neural%20processes%20into%20a%20unified%20model%20that%20captures%0Auncertainty%2C%20temporal%20continuity%2C%20and%20anatomical%20structure.%20We%20represent%0Adynamic%20systems%20as%20spatiotemporal%20multiplex%20graphs%20and%20model%20their%20latent%0Atrajectories%20using%20a%20GNN-parameterized%20vector%20field.%20Given%20the%20sparse%20context%0Aobservations%20at%20node%20and%20edge%20levels%2C%20the%20model%20infers%20a%20distribution%20over%0Alatent%20initial%20states%20and%20control%20variables%2C%20enabling%20both%20interpolation%20and%0Aextrapolation%20of%20trajectories.%20We%20validate%20the%20method%20on%20three%20synthetic%0Adynamical%20systems%20%28coupled%20pendulum%2C%20Lorenz%20attractor%2C%20and%20Kuramoto%0Aoscillators%29%20and%20two%20real-world%20cardiac%20imaging%20datasets%20-%20ACDC%20%28N%3D150%29%20and%20UK%0ABiobank%20%28N%3D526%29%20-%20demonstrating%20accurate%20reconstruction%2C%20extrapolation%2C%20and%0Adisease%20classification%20capabilities.%20The%20model%20accurately%20reconstructs%0Atrajectories%20and%20extrapolates%20future%20cardiac%20cycles%20from%20a%20single%20observed%0Acycle.%20It%20achieves%20state-of-the-art%20results%20on%20the%20ACDC%20classification%20task%20%28up%0Ato%2099%25%20accuracy%29%2C%20and%20detects%20atrial%20fibrillation%20in%20UK%20Biobank%20subjects%20with%0Acompetitive%20performance%20%28up%20to%2067%25%20accuracy%29.%20This%20work%20introduces%20a%20flexible%0Aapproach%20for%20analyzing%20cardiac%20motion%20and%20offers%20a%20foundation%20for%20graph-based%0Alearning%20in%20structured%20biomedical%20spatiotemporal%20time-series%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12953v1&entry.124074799=Read"},
{"title": "TSPC: A Two-Stage Phoneme-Centric Architecture for code-switching\n  Vietnamese-English Speech Recognition", "author": "Minh N. H. Nguyen and Anh Nguyen Tran and Dung Truong Dinh and Nam Van Vo", "abstract": "  Code-switching (CS) presents a significant challenge for general Auto-Speech\nRecognition (ASR) systems. Existing methods often fail to capture the subtle\nphonological shifts inherent in CS scenarios. The challenge is particularly\ndifficult for language pairs like Vietnamese and English, where both distinct\nphonological features and the ambiguity arising from similar sound recognition\nare present. In this paper, we propose a novel architecture for\nVietnamese-English CS ASR, a Two-Stage Phoneme-Centric model (TSPC). The TSPC\nemploys a phoneme-centric approach, built upon an extended Vietnamese phoneme\nset as an intermediate representation to facilitate mixed-lingual modeling.\nExperimental results demonstrate that TSPC consistently outperforms existing\nbaselines, including PhoWhisper-base, in Vietnamese-English CS ASR, achieving a\nsignificantly lower word error rate of 20.8\\% with reduced training resources.\nFurthermore, the phonetic-based two-stage architecture enables phoneme\nadaptation and language conversion to enhance ASR performance in complex CS\nVietnamese-English ASR scenarios.\n", "link": "http://arxiv.org/abs/2509.05983v2", "date": "2025-09-16", "relevancy": 2.1808, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4503}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4358}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4224}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TSPC%3A%20A%20Two-Stage%20Phoneme-Centric%20Architecture%20for%20code-switching%0A%20%20Vietnamese-English%20Speech%20Recognition&body=Title%3A%20TSPC%3A%20A%20Two-Stage%20Phoneme-Centric%20Architecture%20for%20code-switching%0A%20%20Vietnamese-English%20Speech%20Recognition%0AAuthor%3A%20Minh%20N.%20H.%20Nguyen%20and%20Anh%20Nguyen%20Tran%20and%20Dung%20Truong%20Dinh%20and%20Nam%20Van%20Vo%0AAbstract%3A%20%20%20Code-switching%20%28CS%29%20presents%20a%20significant%20challenge%20for%20general%20Auto-Speech%0ARecognition%20%28ASR%29%20systems.%20Existing%20methods%20often%20fail%20to%20capture%20the%20subtle%0Aphonological%20shifts%20inherent%20in%20CS%20scenarios.%20The%20challenge%20is%20particularly%0Adifficult%20for%20language%20pairs%20like%20Vietnamese%20and%20English%2C%20where%20both%20distinct%0Aphonological%20features%20and%20the%20ambiguity%20arising%20from%20similar%20sound%20recognition%0Aare%20present.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20architecture%20for%0AVietnamese-English%20CS%20ASR%2C%20a%20Two-Stage%20Phoneme-Centric%20model%20%28TSPC%29.%20The%20TSPC%0Aemploys%20a%20phoneme-centric%20approach%2C%20built%20upon%20an%20extended%20Vietnamese%20phoneme%0Aset%20as%20an%20intermediate%20representation%20to%20facilitate%20mixed-lingual%20modeling.%0AExperimental%20results%20demonstrate%20that%20TSPC%20consistently%20outperforms%20existing%0Abaselines%2C%20including%20PhoWhisper-base%2C%20in%20Vietnamese-English%20CS%20ASR%2C%20achieving%20a%0Asignificantly%20lower%20word%20error%20rate%20of%2020.8%5C%25%20with%20reduced%20training%20resources.%0AFurthermore%2C%20the%20phonetic-based%20two-stage%20architecture%20enables%20phoneme%0Aadaptation%20and%20language%20conversion%20to%20enhance%20ASR%20performance%20in%20complex%20CS%0AVietnamese-English%20ASR%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05983v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTSPC%253A%2520A%2520Two-Stage%2520Phoneme-Centric%2520Architecture%2520for%2520code-switching%250A%2520%2520Vietnamese-English%2520Speech%2520Recognition%26entry.906535625%3DMinh%2520N.%2520H.%2520Nguyen%2520and%2520Anh%2520Nguyen%2520Tran%2520and%2520Dung%2520Truong%2520Dinh%2520and%2520Nam%2520Van%2520Vo%26entry.1292438233%3D%2520%2520Code-switching%2520%2528CS%2529%2520presents%2520a%2520significant%2520challenge%2520for%2520general%2520Auto-Speech%250ARecognition%2520%2528ASR%2529%2520systems.%2520Existing%2520methods%2520often%2520fail%2520to%2520capture%2520the%2520subtle%250Aphonological%2520shifts%2520inherent%2520in%2520CS%2520scenarios.%2520The%2520challenge%2520is%2520particularly%250Adifficult%2520for%2520language%2520pairs%2520like%2520Vietnamese%2520and%2520English%252C%2520where%2520both%2520distinct%250Aphonological%2520features%2520and%2520the%2520ambiguity%2520arising%2520from%2520similar%2520sound%2520recognition%250Aare%2520present.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520architecture%2520for%250AVietnamese-English%2520CS%2520ASR%252C%2520a%2520Two-Stage%2520Phoneme-Centric%2520model%2520%2528TSPC%2529.%2520The%2520TSPC%250Aemploys%2520a%2520phoneme-centric%2520approach%252C%2520built%2520upon%2520an%2520extended%2520Vietnamese%2520phoneme%250Aset%2520as%2520an%2520intermediate%2520representation%2520to%2520facilitate%2520mixed-lingual%2520modeling.%250AExperimental%2520results%2520demonstrate%2520that%2520TSPC%2520consistently%2520outperforms%2520existing%250Abaselines%252C%2520including%2520PhoWhisper-base%252C%2520in%2520Vietnamese-English%2520CS%2520ASR%252C%2520achieving%2520a%250Asignificantly%2520lower%2520word%2520error%2520rate%2520of%252020.8%255C%2525%2520with%2520reduced%2520training%2520resources.%250AFurthermore%252C%2520the%2520phonetic-based%2520two-stage%2520architecture%2520enables%2520phoneme%250Aadaptation%2520and%2520language%2520conversion%2520to%2520enhance%2520ASR%2520performance%2520in%2520complex%2520CS%250AVietnamese-English%2520ASR%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05983v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TSPC%3A%20A%20Two-Stage%20Phoneme-Centric%20Architecture%20for%20code-switching%0A%20%20Vietnamese-English%20Speech%20Recognition&entry.906535625=Minh%20N.%20H.%20Nguyen%20and%20Anh%20Nguyen%20Tran%20and%20Dung%20Truong%20Dinh%20and%20Nam%20Van%20Vo&entry.1292438233=%20%20Code-switching%20%28CS%29%20presents%20a%20significant%20challenge%20for%20general%20Auto-Speech%0ARecognition%20%28ASR%29%20systems.%20Existing%20methods%20often%20fail%20to%20capture%20the%20subtle%0Aphonological%20shifts%20inherent%20in%20CS%20scenarios.%20The%20challenge%20is%20particularly%0Adifficult%20for%20language%20pairs%20like%20Vietnamese%20and%20English%2C%20where%20both%20distinct%0Aphonological%20features%20and%20the%20ambiguity%20arising%20from%20similar%20sound%20recognition%0Aare%20present.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20architecture%20for%0AVietnamese-English%20CS%20ASR%2C%20a%20Two-Stage%20Phoneme-Centric%20model%20%28TSPC%29.%20The%20TSPC%0Aemploys%20a%20phoneme-centric%20approach%2C%20built%20upon%20an%20extended%20Vietnamese%20phoneme%0Aset%20as%20an%20intermediate%20representation%20to%20facilitate%20mixed-lingual%20modeling.%0AExperimental%20results%20demonstrate%20that%20TSPC%20consistently%20outperforms%20existing%0Abaselines%2C%20including%20PhoWhisper-base%2C%20in%20Vietnamese-English%20CS%20ASR%2C%20achieving%20a%0Asignificantly%20lower%20word%20error%20rate%20of%2020.8%5C%25%20with%20reduced%20training%20resources.%0AFurthermore%2C%20the%20phonetic-based%20two-stage%20architecture%20enables%20phoneme%0Aadaptation%20and%20language%20conversion%20to%20enhance%20ASR%20performance%20in%20complex%20CS%0AVietnamese-English%20ASR%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05983v2&entry.124074799=Read"},
{"title": "Break the Checkbox: Challenging Closed-Style Evaluations of Cultural\n  Alignment in LLMs", "author": "Mohsinul Kabir and Ajwad Abrar and Sophia Ananiadou", "abstract": "  A large number of studies rely on closed-style multiple-choice surveys to\nevaluate cultural alignment in Large Language Models (LLMs). In this work, we\nchallenge this constrained evaluation paradigm and explore more realistic,\nunconstrained approaches. Using the World Values Survey (WVS) and Hofstede\nCultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger\ncultural alignment in less constrained settings, where responses are not\nforced. Additionally, we show that even minor changes, such as reordering\nsurvey choices, lead to inconsistent outputs, exposing the limitations of\nclosed-style evaluations. Our findings advocate for more robust and flexible\nevaluation frameworks that focus on specific cultural proxies, encouraging more\nnuanced and accurate assessments of cultural alignment in LLMs.\n", "link": "http://arxiv.org/abs/2502.08045v3", "date": "2025-09-16", "relevancy": 2.1704, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4421}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4421}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4179}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Break%20the%20Checkbox%3A%20Challenging%20Closed-Style%20Evaluations%20of%20Cultural%0A%20%20Alignment%20in%20LLMs&body=Title%3A%20Break%20the%20Checkbox%3A%20Challenging%20Closed-Style%20Evaluations%20of%20Cultural%0A%20%20Alignment%20in%20LLMs%0AAuthor%3A%20Mohsinul%20Kabir%20and%20Ajwad%20Abrar%20and%20Sophia%20Ananiadou%0AAbstract%3A%20%20%20A%20large%20number%20of%20studies%20rely%20on%20closed-style%20multiple-choice%20surveys%20to%0Aevaluate%20cultural%20alignment%20in%20Large%20Language%20Models%20%28LLMs%29.%20In%20this%20work%2C%20we%0Achallenge%20this%20constrained%20evaluation%20paradigm%20and%20explore%20more%20realistic%2C%0Aunconstrained%20approaches.%20Using%20the%20World%20Values%20Survey%20%28WVS%29%20and%20Hofstede%0ACultural%20Dimensions%20as%20case%20studies%2C%20we%20demonstrate%20that%20LLMs%20exhibit%20stronger%0Acultural%20alignment%20in%20less%20constrained%20settings%2C%20where%20responses%20are%20not%0Aforced.%20Additionally%2C%20we%20show%20that%20even%20minor%20changes%2C%20such%20as%20reordering%0Asurvey%20choices%2C%20lead%20to%20inconsistent%20outputs%2C%20exposing%20the%20limitations%20of%0Aclosed-style%20evaluations.%20Our%20findings%20advocate%20for%20more%20robust%20and%20flexible%0Aevaluation%20frameworks%20that%20focus%20on%20specific%20cultural%20proxies%2C%20encouraging%20more%0Anuanced%20and%20accurate%20assessments%20of%20cultural%20alignment%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.08045v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreak%2520the%2520Checkbox%253A%2520Challenging%2520Closed-Style%2520Evaluations%2520of%2520Cultural%250A%2520%2520Alignment%2520in%2520LLMs%26entry.906535625%3DMohsinul%2520Kabir%2520and%2520Ajwad%2520Abrar%2520and%2520Sophia%2520Ananiadou%26entry.1292438233%3D%2520%2520A%2520large%2520number%2520of%2520studies%2520rely%2520on%2520closed-style%2520multiple-choice%2520surveys%2520to%250Aevaluate%2520cultural%2520alignment%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520In%2520this%2520work%252C%2520we%250Achallenge%2520this%2520constrained%2520evaluation%2520paradigm%2520and%2520explore%2520more%2520realistic%252C%250Aunconstrained%2520approaches.%2520Using%2520the%2520World%2520Values%2520Survey%2520%2528WVS%2529%2520and%2520Hofstede%250ACultural%2520Dimensions%2520as%2520case%2520studies%252C%2520we%2520demonstrate%2520that%2520LLMs%2520exhibit%2520stronger%250Acultural%2520alignment%2520in%2520less%2520constrained%2520settings%252C%2520where%2520responses%2520are%2520not%250Aforced.%2520Additionally%252C%2520we%2520show%2520that%2520even%2520minor%2520changes%252C%2520such%2520as%2520reordering%250Asurvey%2520choices%252C%2520lead%2520to%2520inconsistent%2520outputs%252C%2520exposing%2520the%2520limitations%2520of%250Aclosed-style%2520evaluations.%2520Our%2520findings%2520advocate%2520for%2520more%2520robust%2520and%2520flexible%250Aevaluation%2520frameworks%2520that%2520focus%2520on%2520specific%2520cultural%2520proxies%252C%2520encouraging%2520more%250Anuanced%2520and%2520accurate%2520assessments%2520of%2520cultural%2520alignment%2520in%2520LLMs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.08045v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Break%20the%20Checkbox%3A%20Challenging%20Closed-Style%20Evaluations%20of%20Cultural%0A%20%20Alignment%20in%20LLMs&entry.906535625=Mohsinul%20Kabir%20and%20Ajwad%20Abrar%20and%20Sophia%20Ananiadou&entry.1292438233=%20%20A%20large%20number%20of%20studies%20rely%20on%20closed-style%20multiple-choice%20surveys%20to%0Aevaluate%20cultural%20alignment%20in%20Large%20Language%20Models%20%28LLMs%29.%20In%20this%20work%2C%20we%0Achallenge%20this%20constrained%20evaluation%20paradigm%20and%20explore%20more%20realistic%2C%0Aunconstrained%20approaches.%20Using%20the%20World%20Values%20Survey%20%28WVS%29%20and%20Hofstede%0ACultural%20Dimensions%20as%20case%20studies%2C%20we%20demonstrate%20that%20LLMs%20exhibit%20stronger%0Acultural%20alignment%20in%20less%20constrained%20settings%2C%20where%20responses%20are%20not%0Aforced.%20Additionally%2C%20we%20show%20that%20even%20minor%20changes%2C%20such%20as%20reordering%0Asurvey%20choices%2C%20lead%20to%20inconsistent%20outputs%2C%20exposing%20the%20limitations%20of%0Aclosed-style%20evaluations.%20Our%20findings%20advocate%20for%20more%20robust%20and%20flexible%0Aevaluation%20frameworks%20that%20focus%20on%20specific%20cultural%20proxies%2C%20encouraging%20more%0Anuanced%20and%20accurate%20assessments%20of%20cultural%20alignment%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.08045v3&entry.124074799=Read"},
{"title": "TexTAR : Textual Attribute Recognition in Multi-domain and Multi-lingual\n  Document Images", "author": "Rohan Kumar and Jyothi Swaroopa Jinka and Ravi Kiran Sarvadevabhatla", "abstract": "  Recognizing textual attributes such as bold, italic, underline and strikeout\nis essential for understanding text semantics, structure, and visual\npresentation. These attributes highlight key information, making them crucial\nfor document analysis. Existing methods struggle with computational efficiency\nor adaptability in noisy, multilingual settings. To address this, we introduce\nTexTAR, a multi-task, context-aware Transformer for Textual Attribute\nRecognition (TAR). Our novel data selection pipeline enhances context\nawareness, and our architecture employs a 2D RoPE (Rotary Positional\nEmbedding)-style mechanism to incorporate input context for more accurate\nattribute predictions. We also introduce MMTAD, a diverse, multilingual,\nmulti-domain dataset annotated with text attributes across real-world documents\nsuch as legal records, notices, and textbooks. Extensive evaluations show\nTexTAR outperforms existing methods, demonstrating that contextual awareness\ncontributes to state-of-the-art TAR performance.\n", "link": "http://arxiv.org/abs/2509.13151v1", "date": "2025-09-16", "relevancy": 2.1702, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.597}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5079}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4932}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TexTAR%20%3A%20Textual%20Attribute%20Recognition%20in%20Multi-domain%20and%20Multi-lingual%0A%20%20Document%20Images&body=Title%3A%20TexTAR%20%3A%20Textual%20Attribute%20Recognition%20in%20Multi-domain%20and%20Multi-lingual%0A%20%20Document%20Images%0AAuthor%3A%20Rohan%20Kumar%20and%20Jyothi%20Swaroopa%20Jinka%20and%20Ravi%20Kiran%20Sarvadevabhatla%0AAbstract%3A%20%20%20Recognizing%20textual%20attributes%20such%20as%20bold%2C%20italic%2C%20underline%20and%20strikeout%0Ais%20essential%20for%20understanding%20text%20semantics%2C%20structure%2C%20and%20visual%0Apresentation.%20These%20attributes%20highlight%20key%20information%2C%20making%20them%20crucial%0Afor%20document%20analysis.%20Existing%20methods%20struggle%20with%20computational%20efficiency%0Aor%20adaptability%20in%20noisy%2C%20multilingual%20settings.%20To%20address%20this%2C%20we%20introduce%0ATexTAR%2C%20a%20multi-task%2C%20context-aware%20Transformer%20for%20Textual%20Attribute%0ARecognition%20%28TAR%29.%20Our%20novel%20data%20selection%20pipeline%20enhances%20context%0Aawareness%2C%20and%20our%20architecture%20employs%20a%202D%20RoPE%20%28Rotary%20Positional%0AEmbedding%29-style%20mechanism%20to%20incorporate%20input%20context%20for%20more%20accurate%0Aattribute%20predictions.%20We%20also%20introduce%20MMTAD%2C%20a%20diverse%2C%20multilingual%2C%0Amulti-domain%20dataset%20annotated%20with%20text%20attributes%20across%20real-world%20documents%0Asuch%20as%20legal%20records%2C%20notices%2C%20and%20textbooks.%20Extensive%20evaluations%20show%0ATexTAR%20outperforms%20existing%20methods%2C%20demonstrating%20that%20contextual%20awareness%0Acontributes%20to%20state-of-the-art%20TAR%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13151v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTexTAR%2520%253A%2520Textual%2520Attribute%2520Recognition%2520in%2520Multi-domain%2520and%2520Multi-lingual%250A%2520%2520Document%2520Images%26entry.906535625%3DRohan%2520Kumar%2520and%2520Jyothi%2520Swaroopa%2520Jinka%2520and%2520Ravi%2520Kiran%2520Sarvadevabhatla%26entry.1292438233%3D%2520%2520Recognizing%2520textual%2520attributes%2520such%2520as%2520bold%252C%2520italic%252C%2520underline%2520and%2520strikeout%250Ais%2520essential%2520for%2520understanding%2520text%2520semantics%252C%2520structure%252C%2520and%2520visual%250Apresentation.%2520These%2520attributes%2520highlight%2520key%2520information%252C%2520making%2520them%2520crucial%250Afor%2520document%2520analysis.%2520Existing%2520methods%2520struggle%2520with%2520computational%2520efficiency%250Aor%2520adaptability%2520in%2520noisy%252C%2520multilingual%2520settings.%2520To%2520address%2520this%252C%2520we%2520introduce%250ATexTAR%252C%2520a%2520multi-task%252C%2520context-aware%2520Transformer%2520for%2520Textual%2520Attribute%250ARecognition%2520%2528TAR%2529.%2520Our%2520novel%2520data%2520selection%2520pipeline%2520enhances%2520context%250Aawareness%252C%2520and%2520our%2520architecture%2520employs%2520a%25202D%2520RoPE%2520%2528Rotary%2520Positional%250AEmbedding%2529-style%2520mechanism%2520to%2520incorporate%2520input%2520context%2520for%2520more%2520accurate%250Aattribute%2520predictions.%2520We%2520also%2520introduce%2520MMTAD%252C%2520a%2520diverse%252C%2520multilingual%252C%250Amulti-domain%2520dataset%2520annotated%2520with%2520text%2520attributes%2520across%2520real-world%2520documents%250Asuch%2520as%2520legal%2520records%252C%2520notices%252C%2520and%2520textbooks.%2520Extensive%2520evaluations%2520show%250ATexTAR%2520outperforms%2520existing%2520methods%252C%2520demonstrating%2520that%2520contextual%2520awareness%250Acontributes%2520to%2520state-of-the-art%2520TAR%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13151v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TexTAR%20%3A%20Textual%20Attribute%20Recognition%20in%20Multi-domain%20and%20Multi-lingual%0A%20%20Document%20Images&entry.906535625=Rohan%20Kumar%20and%20Jyothi%20Swaroopa%20Jinka%20and%20Ravi%20Kiran%20Sarvadevabhatla&entry.1292438233=%20%20Recognizing%20textual%20attributes%20such%20as%20bold%2C%20italic%2C%20underline%20and%20strikeout%0Ais%20essential%20for%20understanding%20text%20semantics%2C%20structure%2C%20and%20visual%0Apresentation.%20These%20attributes%20highlight%20key%20information%2C%20making%20them%20crucial%0Afor%20document%20analysis.%20Existing%20methods%20struggle%20with%20computational%20efficiency%0Aor%20adaptability%20in%20noisy%2C%20multilingual%20settings.%20To%20address%20this%2C%20we%20introduce%0ATexTAR%2C%20a%20multi-task%2C%20context-aware%20Transformer%20for%20Textual%20Attribute%0ARecognition%20%28TAR%29.%20Our%20novel%20data%20selection%20pipeline%20enhances%20context%0Aawareness%2C%20and%20our%20architecture%20employs%20a%202D%20RoPE%20%28Rotary%20Positional%0AEmbedding%29-style%20mechanism%20to%20incorporate%20input%20context%20for%20more%20accurate%0Aattribute%20predictions.%20We%20also%20introduce%20MMTAD%2C%20a%20diverse%2C%20multilingual%2C%0Amulti-domain%20dataset%20annotated%20with%20text%20attributes%20across%20real-world%20documents%0Asuch%20as%20legal%20records%2C%20notices%2C%20and%20textbooks.%20Extensive%20evaluations%20show%0ATexTAR%20outperforms%20existing%20methods%2C%20demonstrating%20that%20contextual%20awareness%0Acontributes%20to%20state-of-the-art%20TAR%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13151v1&entry.124074799=Read"},
{"title": "T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and\n  Efficient UAV Tracking", "author": "Hojat Ardi and Amir Jahanshahi and Ali Diba", "abstract": "  Aerial object tracking remains a challenging task due to scale variations,\ndynamic backgrounds, clutter, and frequent occlusions. While most existing\ntrackers emphasize spatial cues, they often overlook temporal dependencies,\nresulting in limited robustness in long-term tracking and under occlusion.\nFurthermore, correlation-based Siamese trackers are inherently constrained by\nthe linear nature of correlation operations, making them ineffective against\ncomplex, non-linear appearance changes. To address these limitations, we\nintroduce T-SiamTPN, a temporal-aware Siamese tracking framework that extends\nthe SiamTPN architecture with explicit temporal modeling. Our approach\nincorporates temporal feature fusion and attention-based interactions,\nstrengthening temporal consistency and enabling richer feature representations.\nThese enhancements yield significant improvements over the baseline and achieve\nperformance competitive with state-of-the-art trackers. Crucially, despite the\nadded temporal modules, T-SiamTPN preserves computational efficiency. Deployed\non the resource-constrained Jetson Nano, the tracker runs in real time at 7.1\nFPS, demonstrating its suitability for real-world embedded applications without\nnotable runtime overhead. Experimental results highlight substantial gains:\ncompared to the baseline, T-SiamTPN improves success rate by 13.7% and\nprecision by 14.7%. These findings underscore the importance of temporal\nmodeling in Siamese tracking frameworks and establish T-SiamTPN as a strong and\nefficient solution for aerial object tracking. Code is available at:\nhttps://github.com/to/be/released\n", "link": "http://arxiv.org/abs/2509.12913v1", "date": "2025-09-16", "relevancy": 2.1615, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5469}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5405}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20T-SiamTPN%3A%20Temporal%20Siamese%20Transformer%20Pyramid%20Networks%20for%20Robust%20and%0A%20%20Efficient%20UAV%20Tracking&body=Title%3A%20T-SiamTPN%3A%20Temporal%20Siamese%20Transformer%20Pyramid%20Networks%20for%20Robust%20and%0A%20%20Efficient%20UAV%20Tracking%0AAuthor%3A%20Hojat%20Ardi%20and%20Amir%20Jahanshahi%20and%20Ali%20Diba%0AAbstract%3A%20%20%20Aerial%20object%20tracking%20remains%20a%20challenging%20task%20due%20to%20scale%20variations%2C%0Adynamic%20backgrounds%2C%20clutter%2C%20and%20frequent%20occlusions.%20While%20most%20existing%0Atrackers%20emphasize%20spatial%20cues%2C%20they%20often%20overlook%20temporal%20dependencies%2C%0Aresulting%20in%20limited%20robustness%20in%20long-term%20tracking%20and%20under%20occlusion.%0AFurthermore%2C%20correlation-based%20Siamese%20trackers%20are%20inherently%20constrained%20by%0Athe%20linear%20nature%20of%20correlation%20operations%2C%20making%20them%20ineffective%20against%0Acomplex%2C%20non-linear%20appearance%20changes.%20To%20address%20these%20limitations%2C%20we%0Aintroduce%20T-SiamTPN%2C%20a%20temporal-aware%20Siamese%20tracking%20framework%20that%20extends%0Athe%20SiamTPN%20architecture%20with%20explicit%20temporal%20modeling.%20Our%20approach%0Aincorporates%20temporal%20feature%20fusion%20and%20attention-based%20interactions%2C%0Astrengthening%20temporal%20consistency%20and%20enabling%20richer%20feature%20representations.%0AThese%20enhancements%20yield%20significant%20improvements%20over%20the%20baseline%20and%20achieve%0Aperformance%20competitive%20with%20state-of-the-art%20trackers.%20Crucially%2C%20despite%20the%0Aadded%20temporal%20modules%2C%20T-SiamTPN%20preserves%20computational%20efficiency.%20Deployed%0Aon%20the%20resource-constrained%20Jetson%20Nano%2C%20the%20tracker%20runs%20in%20real%20time%20at%207.1%0AFPS%2C%20demonstrating%20its%20suitability%20for%20real-world%20embedded%20applications%20without%0Anotable%20runtime%20overhead.%20Experimental%20results%20highlight%20substantial%20gains%3A%0Acompared%20to%20the%20baseline%2C%20T-SiamTPN%20improves%20success%20rate%20by%2013.7%25%20and%0Aprecision%20by%2014.7%25.%20These%20findings%20underscore%20the%20importance%20of%20temporal%0Amodeling%20in%20Siamese%20tracking%20frameworks%20and%20establish%20T-SiamTPN%20as%20a%20strong%20and%0Aefficient%20solution%20for%20aerial%20object%20tracking.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/to/be/released%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DT-SiamTPN%253A%2520Temporal%2520Siamese%2520Transformer%2520Pyramid%2520Networks%2520for%2520Robust%2520and%250A%2520%2520Efficient%2520UAV%2520Tracking%26entry.906535625%3DHojat%2520Ardi%2520and%2520Amir%2520Jahanshahi%2520and%2520Ali%2520Diba%26entry.1292438233%3D%2520%2520Aerial%2520object%2520tracking%2520remains%2520a%2520challenging%2520task%2520due%2520to%2520scale%2520variations%252C%250Adynamic%2520backgrounds%252C%2520clutter%252C%2520and%2520frequent%2520occlusions.%2520While%2520most%2520existing%250Atrackers%2520emphasize%2520spatial%2520cues%252C%2520they%2520often%2520overlook%2520temporal%2520dependencies%252C%250Aresulting%2520in%2520limited%2520robustness%2520in%2520long-term%2520tracking%2520and%2520under%2520occlusion.%250AFurthermore%252C%2520correlation-based%2520Siamese%2520trackers%2520are%2520inherently%2520constrained%2520by%250Athe%2520linear%2520nature%2520of%2520correlation%2520operations%252C%2520making%2520them%2520ineffective%2520against%250Acomplex%252C%2520non-linear%2520appearance%2520changes.%2520To%2520address%2520these%2520limitations%252C%2520we%250Aintroduce%2520T-SiamTPN%252C%2520a%2520temporal-aware%2520Siamese%2520tracking%2520framework%2520that%2520extends%250Athe%2520SiamTPN%2520architecture%2520with%2520explicit%2520temporal%2520modeling.%2520Our%2520approach%250Aincorporates%2520temporal%2520feature%2520fusion%2520and%2520attention-based%2520interactions%252C%250Astrengthening%2520temporal%2520consistency%2520and%2520enabling%2520richer%2520feature%2520representations.%250AThese%2520enhancements%2520yield%2520significant%2520improvements%2520over%2520the%2520baseline%2520and%2520achieve%250Aperformance%2520competitive%2520with%2520state-of-the-art%2520trackers.%2520Crucially%252C%2520despite%2520the%250Aadded%2520temporal%2520modules%252C%2520T-SiamTPN%2520preserves%2520computational%2520efficiency.%2520Deployed%250Aon%2520the%2520resource-constrained%2520Jetson%2520Nano%252C%2520the%2520tracker%2520runs%2520in%2520real%2520time%2520at%25207.1%250AFPS%252C%2520demonstrating%2520its%2520suitability%2520for%2520real-world%2520embedded%2520applications%2520without%250Anotable%2520runtime%2520overhead.%2520Experimental%2520results%2520highlight%2520substantial%2520gains%253A%250Acompared%2520to%2520the%2520baseline%252C%2520T-SiamTPN%2520improves%2520success%2520rate%2520by%252013.7%2525%2520and%250Aprecision%2520by%252014.7%2525.%2520These%2520findings%2520underscore%2520the%2520importance%2520of%2520temporal%250Amodeling%2520in%2520Siamese%2520tracking%2520frameworks%2520and%2520establish%2520T-SiamTPN%2520as%2520a%2520strong%2520and%250Aefficient%2520solution%2520for%2520aerial%2520object%2520tracking.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/to/be/released%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=T-SiamTPN%3A%20Temporal%20Siamese%20Transformer%20Pyramid%20Networks%20for%20Robust%20and%0A%20%20Efficient%20UAV%20Tracking&entry.906535625=Hojat%20Ardi%20and%20Amir%20Jahanshahi%20and%20Ali%20Diba&entry.1292438233=%20%20Aerial%20object%20tracking%20remains%20a%20challenging%20task%20due%20to%20scale%20variations%2C%0Adynamic%20backgrounds%2C%20clutter%2C%20and%20frequent%20occlusions.%20While%20most%20existing%0Atrackers%20emphasize%20spatial%20cues%2C%20they%20often%20overlook%20temporal%20dependencies%2C%0Aresulting%20in%20limited%20robustness%20in%20long-term%20tracking%20and%20under%20occlusion.%0AFurthermore%2C%20correlation-based%20Siamese%20trackers%20are%20inherently%20constrained%20by%0Athe%20linear%20nature%20of%20correlation%20operations%2C%20making%20them%20ineffective%20against%0Acomplex%2C%20non-linear%20appearance%20changes.%20To%20address%20these%20limitations%2C%20we%0Aintroduce%20T-SiamTPN%2C%20a%20temporal-aware%20Siamese%20tracking%20framework%20that%20extends%0Athe%20SiamTPN%20architecture%20with%20explicit%20temporal%20modeling.%20Our%20approach%0Aincorporates%20temporal%20feature%20fusion%20and%20attention-based%20interactions%2C%0Astrengthening%20temporal%20consistency%20and%20enabling%20richer%20feature%20representations.%0AThese%20enhancements%20yield%20significant%20improvements%20over%20the%20baseline%20and%20achieve%0Aperformance%20competitive%20with%20state-of-the-art%20trackers.%20Crucially%2C%20despite%20the%0Aadded%20temporal%20modules%2C%20T-SiamTPN%20preserves%20computational%20efficiency.%20Deployed%0Aon%20the%20resource-constrained%20Jetson%20Nano%2C%20the%20tracker%20runs%20in%20real%20time%20at%207.1%0AFPS%2C%20demonstrating%20its%20suitability%20for%20real-world%20embedded%20applications%20without%0Anotable%20runtime%20overhead.%20Experimental%20results%20highlight%20substantial%20gains%3A%0Acompared%20to%20the%20baseline%2C%20T-SiamTPN%20improves%20success%20rate%20by%2013.7%25%20and%0Aprecision%20by%2014.7%25.%20These%20findings%20underscore%20the%20importance%20of%20temporal%0Amodeling%20in%20Siamese%20tracking%20frameworks%20and%20establish%20T-SiamTPN%20as%20a%20strong%20and%0Aefficient%20solution%20for%20aerial%20object%20tracking.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/to/be/released%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12913v1&entry.124074799=Read"},
{"title": "ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich\n  Manipulation", "author": "Jiawen Yu and Hairuo Liu and Qiaojun Yu and Jieji Ren and Ce Hao and Haitong Ding and Guangyu Huang and Guofan Huang and Yan Song and Panpan Cai and Cewu Lu and Wenqiang Zhang", "abstract": "  Vision-Language-Action (VLA) models have advanced general-purpose robotic\nmanipulation by leveraging pretrained visual and linguistic representations.\nHowever, they struggle with contact-rich tasks that require fine-grained\ncontrol involving force, especially under visual occlusion or dynamic\nuncertainty. To address these limitations, we propose ForceVLA, a novel\nend-to-end manipulation framework that treats external force sensing as a\nfirst-class modality within VLA systems. ForceVLA introduces FVLMoE, a\nforce-aware Mixture-of-Experts fusion module that dynamically integrates\npretrained visual-language embeddings with real-time 6-axis force feedback\nduring action decoding. This enables context-aware routing across\nmodality-specific experts, enhancing the robot's ability to adapt to subtle\ncontact dynamics. We also introduce \\textbf{ForceVLA-Data}, a new dataset\ncomprising synchronized vision, proprioception, and force-torque signals across\nfive contact-rich manipulation tasks. ForceVLA improves average task success by\n23.2% over strong pi_0-based baselines, achieving up to 80% success in tasks\nsuch as plug insertion. Our approach highlights the importance of multimodal\nintegration for dexterous manipulation and sets a new benchmark for physically\nintelligent robotic control. Code and data will be released at\nhttps://sites.google.com/view/forcevla2025.\n", "link": "http://arxiv.org/abs/2505.22159v2", "date": "2025-09-16", "relevancy": 2.1609, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5704}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5359}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5325}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ForceVLA%3A%20Enhancing%20VLA%20Models%20with%20a%20Force-aware%20MoE%20for%20Contact-rich%0A%20%20Manipulation&body=Title%3A%20ForceVLA%3A%20Enhancing%20VLA%20Models%20with%20a%20Force-aware%20MoE%20for%20Contact-rich%0A%20%20Manipulation%0AAuthor%3A%20Jiawen%20Yu%20and%20Hairuo%20Liu%20and%20Qiaojun%20Yu%20and%20Jieji%20Ren%20and%20Ce%20Hao%20and%20Haitong%20Ding%20and%20Guangyu%20Huang%20and%20Guofan%20Huang%20and%20Yan%20Song%20and%20Panpan%20Cai%20and%20Cewu%20Lu%20and%20Wenqiang%20Zhang%0AAbstract%3A%20%20%20Vision-Language-Action%20%28VLA%29%20models%20have%20advanced%20general-purpose%20robotic%0Amanipulation%20by%20leveraging%20pretrained%20visual%20and%20linguistic%20representations.%0AHowever%2C%20they%20struggle%20with%20contact-rich%20tasks%20that%20require%20fine-grained%0Acontrol%20involving%20force%2C%20especially%20under%20visual%20occlusion%20or%20dynamic%0Auncertainty.%20To%20address%20these%20limitations%2C%20we%20propose%20ForceVLA%2C%20a%20novel%0Aend-to-end%20manipulation%20framework%20that%20treats%20external%20force%20sensing%20as%20a%0Afirst-class%20modality%20within%20VLA%20systems.%20ForceVLA%20introduces%20FVLMoE%2C%20a%0Aforce-aware%20Mixture-of-Experts%20fusion%20module%20that%20dynamically%20integrates%0Apretrained%20visual-language%20embeddings%20with%20real-time%206-axis%20force%20feedback%0Aduring%20action%20decoding.%20This%20enables%20context-aware%20routing%20across%0Amodality-specific%20experts%2C%20enhancing%20the%20robot%27s%20ability%20to%20adapt%20to%20subtle%0Acontact%20dynamics.%20We%20also%20introduce%20%5Ctextbf%7BForceVLA-Data%7D%2C%20a%20new%20dataset%0Acomprising%20synchronized%20vision%2C%20proprioception%2C%20and%20force-torque%20signals%20across%0Afive%20contact-rich%20manipulation%20tasks.%20ForceVLA%20improves%20average%20task%20success%20by%0A23.2%25%20over%20strong%20pi_0-based%20baselines%2C%20achieving%20up%20to%2080%25%20success%20in%20tasks%0Asuch%20as%20plug%20insertion.%20Our%20approach%20highlights%20the%20importance%20of%20multimodal%0Aintegration%20for%20dexterous%20manipulation%20and%20sets%20a%20new%20benchmark%20for%20physically%0Aintelligent%20robotic%20control.%20Code%20and%20data%20will%20be%20released%20at%0Ahttps%3A//sites.google.com/view/forcevla2025.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.22159v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForceVLA%253A%2520Enhancing%2520VLA%2520Models%2520with%2520a%2520Force-aware%2520MoE%2520for%2520Contact-rich%250A%2520%2520Manipulation%26entry.906535625%3DJiawen%2520Yu%2520and%2520Hairuo%2520Liu%2520and%2520Qiaojun%2520Yu%2520and%2520Jieji%2520Ren%2520and%2520Ce%2520Hao%2520and%2520Haitong%2520Ding%2520and%2520Guangyu%2520Huang%2520and%2520Guofan%2520Huang%2520and%2520Yan%2520Song%2520and%2520Panpan%2520Cai%2520and%2520Cewu%2520Lu%2520and%2520Wenqiang%2520Zhang%26entry.1292438233%3D%2520%2520Vision-Language-Action%2520%2528VLA%2529%2520models%2520have%2520advanced%2520general-purpose%2520robotic%250Amanipulation%2520by%2520leveraging%2520pretrained%2520visual%2520and%2520linguistic%2520representations.%250AHowever%252C%2520they%2520struggle%2520with%2520contact-rich%2520tasks%2520that%2520require%2520fine-grained%250Acontrol%2520involving%2520force%252C%2520especially%2520under%2520visual%2520occlusion%2520or%2520dynamic%250Auncertainty.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520ForceVLA%252C%2520a%2520novel%250Aend-to-end%2520manipulation%2520framework%2520that%2520treats%2520external%2520force%2520sensing%2520as%2520a%250Afirst-class%2520modality%2520within%2520VLA%2520systems.%2520ForceVLA%2520introduces%2520FVLMoE%252C%2520a%250Aforce-aware%2520Mixture-of-Experts%2520fusion%2520module%2520that%2520dynamically%2520integrates%250Apretrained%2520visual-language%2520embeddings%2520with%2520real-time%25206-axis%2520force%2520feedback%250Aduring%2520action%2520decoding.%2520This%2520enables%2520context-aware%2520routing%2520across%250Amodality-specific%2520experts%252C%2520enhancing%2520the%2520robot%2527s%2520ability%2520to%2520adapt%2520to%2520subtle%250Acontact%2520dynamics.%2520We%2520also%2520introduce%2520%255Ctextbf%257BForceVLA-Data%257D%252C%2520a%2520new%2520dataset%250Acomprising%2520synchronized%2520vision%252C%2520proprioception%252C%2520and%2520force-torque%2520signals%2520across%250Afive%2520contact-rich%2520manipulation%2520tasks.%2520ForceVLA%2520improves%2520average%2520task%2520success%2520by%250A23.2%2525%2520over%2520strong%2520pi_0-based%2520baselines%252C%2520achieving%2520up%2520to%252080%2525%2520success%2520in%2520tasks%250Asuch%2520as%2520plug%2520insertion.%2520Our%2520approach%2520highlights%2520the%2520importance%2520of%2520multimodal%250Aintegration%2520for%2520dexterous%2520manipulation%2520and%2520sets%2520a%2520new%2520benchmark%2520for%2520physically%250Aintelligent%2520robotic%2520control.%2520Code%2520and%2520data%2520will%2520be%2520released%2520at%250Ahttps%253A//sites.google.com/view/forcevla2025.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.22159v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ForceVLA%3A%20Enhancing%20VLA%20Models%20with%20a%20Force-aware%20MoE%20for%20Contact-rich%0A%20%20Manipulation&entry.906535625=Jiawen%20Yu%20and%20Hairuo%20Liu%20and%20Qiaojun%20Yu%20and%20Jieji%20Ren%20and%20Ce%20Hao%20and%20Haitong%20Ding%20and%20Guangyu%20Huang%20and%20Guofan%20Huang%20and%20Yan%20Song%20and%20Panpan%20Cai%20and%20Cewu%20Lu%20and%20Wenqiang%20Zhang&entry.1292438233=%20%20Vision-Language-Action%20%28VLA%29%20models%20have%20advanced%20general-purpose%20robotic%0Amanipulation%20by%20leveraging%20pretrained%20visual%20and%20linguistic%20representations.%0AHowever%2C%20they%20struggle%20with%20contact-rich%20tasks%20that%20require%20fine-grained%0Acontrol%20involving%20force%2C%20especially%20under%20visual%20occlusion%20or%20dynamic%0Auncertainty.%20To%20address%20these%20limitations%2C%20we%20propose%20ForceVLA%2C%20a%20novel%0Aend-to-end%20manipulation%20framework%20that%20treats%20external%20force%20sensing%20as%20a%0Afirst-class%20modality%20within%20VLA%20systems.%20ForceVLA%20introduces%20FVLMoE%2C%20a%0Aforce-aware%20Mixture-of-Experts%20fusion%20module%20that%20dynamically%20integrates%0Apretrained%20visual-language%20embeddings%20with%20real-time%206-axis%20force%20feedback%0Aduring%20action%20decoding.%20This%20enables%20context-aware%20routing%20across%0Amodality-specific%20experts%2C%20enhancing%20the%20robot%27s%20ability%20to%20adapt%20to%20subtle%0Acontact%20dynamics.%20We%20also%20introduce%20%5Ctextbf%7BForceVLA-Data%7D%2C%20a%20new%20dataset%0Acomprising%20synchronized%20vision%2C%20proprioception%2C%20and%20force-torque%20signals%20across%0Afive%20contact-rich%20manipulation%20tasks.%20ForceVLA%20improves%20average%20task%20success%20by%0A23.2%25%20over%20strong%20pi_0-based%20baselines%2C%20achieving%20up%20to%2080%25%20success%20in%20tasks%0Asuch%20as%20plug%20insertion.%20Our%20approach%20highlights%20the%20importance%20of%20multimodal%0Aintegration%20for%20dexterous%20manipulation%20and%20sets%20a%20new%20benchmark%20for%20physically%0Aintelligent%20robotic%20control.%20Code%20and%20data%20will%20be%20released%20at%0Ahttps%3A//sites.google.com/view/forcevla2025.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.22159v2&entry.124074799=Read"},
{"title": "Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation", "author": "Irene Iele and Francesco Di Feola and Valerio Guarrasi and Paolo Soda", "abstract": "  Image-to-image translation has emerged as a powerful technique in medical\nimaging, enabling tasks such as image denoising and cross-modality conversion.\nHowever, it suffers from limitations in handling out-of-distribution samples\nwithout causing performance degradation. To address this limitation, we propose\na novel Test-Time Adaptation (TTA) framework that dynamically adjusts the\ntranslation process based on the characteristics of each test sample. Our\nmethod introduces a Reconstruction Module to quantify the domain shift and a\nDynamic Adaptation Block that selectively modifies the internal features of a\npretrained translation model to mitigate the shift without compromising the\nperformance on in-distribution samples that do not require adaptation. We\nevaluate our approach on two medical image-to-image translation tasks: low-dose\nCT denoising and T1 to T2 MRI translation, showing consistent improvements over\nboth the baseline translation model without TTA and prior TTA methods. Our\nanalysis highlights the limitations of the state-of-the-art that uniformly\napply the adaptation to both out-of-distribution and in-distribution samples,\ndemonstrating that dynamic, sample-specific adjustment offers a promising path\nto improve model resilience in real-world scenarios. The code is available at:\nhttps://github.com/Sample-Aware-TTA/Code.\n", "link": "http://arxiv.org/abs/2508.00766v2", "date": "2025-09-16", "relevancy": 2.1539, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5497}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5334}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5293}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sample-Aware%20Test-Time%20Adaptation%20for%20Medical%20Image-to-Image%20Translation&body=Title%3A%20Sample-Aware%20Test-Time%20Adaptation%20for%20Medical%20Image-to-Image%20Translation%0AAuthor%3A%20Irene%20Iele%20and%20Francesco%20Di%20Feola%20and%20Valerio%20Guarrasi%20and%20Paolo%20Soda%0AAbstract%3A%20%20%20Image-to-image%20translation%20has%20emerged%20as%20a%20powerful%20technique%20in%20medical%0Aimaging%2C%20enabling%20tasks%20such%20as%20image%20denoising%20and%20cross-modality%20conversion.%0AHowever%2C%20it%20suffers%20from%20limitations%20in%20handling%20out-of-distribution%20samples%0Awithout%20causing%20performance%20degradation.%20To%20address%20this%20limitation%2C%20we%20propose%0Aa%20novel%20Test-Time%20Adaptation%20%28TTA%29%20framework%20that%20dynamically%20adjusts%20the%0Atranslation%20process%20based%20on%20the%20characteristics%20of%20each%20test%20sample.%20Our%0Amethod%20introduces%20a%20Reconstruction%20Module%20to%20quantify%20the%20domain%20shift%20and%20a%0ADynamic%20Adaptation%20Block%20that%20selectively%20modifies%20the%20internal%20features%20of%20a%0Apretrained%20translation%20model%20to%20mitigate%20the%20shift%20without%20compromising%20the%0Aperformance%20on%20in-distribution%20samples%20that%20do%20not%20require%20adaptation.%20We%0Aevaluate%20our%20approach%20on%20two%20medical%20image-to-image%20translation%20tasks%3A%20low-dose%0ACT%20denoising%20and%20T1%20to%20T2%20MRI%20translation%2C%20showing%20consistent%20improvements%20over%0Aboth%20the%20baseline%20translation%20model%20without%20TTA%20and%20prior%20TTA%20methods.%20Our%0Aanalysis%20highlights%20the%20limitations%20of%20the%20state-of-the-art%20that%20uniformly%0Aapply%20the%20adaptation%20to%20both%20out-of-distribution%20and%20in-distribution%20samples%2C%0Ademonstrating%20that%20dynamic%2C%20sample-specific%20adjustment%20offers%20a%20promising%20path%0Ato%20improve%20model%20resilience%20in%20real-world%20scenarios.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/Sample-Aware-TTA/Code.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.00766v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSample-Aware%2520Test-Time%2520Adaptation%2520for%2520Medical%2520Image-to-Image%2520Translation%26entry.906535625%3DIrene%2520Iele%2520and%2520Francesco%2520Di%2520Feola%2520and%2520Valerio%2520Guarrasi%2520and%2520Paolo%2520Soda%26entry.1292438233%3D%2520%2520Image-to-image%2520translation%2520has%2520emerged%2520as%2520a%2520powerful%2520technique%2520in%2520medical%250Aimaging%252C%2520enabling%2520tasks%2520such%2520as%2520image%2520denoising%2520and%2520cross-modality%2520conversion.%250AHowever%252C%2520it%2520suffers%2520from%2520limitations%2520in%2520handling%2520out-of-distribution%2520samples%250Awithout%2520causing%2520performance%2520degradation.%2520To%2520address%2520this%2520limitation%252C%2520we%2520propose%250Aa%2520novel%2520Test-Time%2520Adaptation%2520%2528TTA%2529%2520framework%2520that%2520dynamically%2520adjusts%2520the%250Atranslation%2520process%2520based%2520on%2520the%2520characteristics%2520of%2520each%2520test%2520sample.%2520Our%250Amethod%2520introduces%2520a%2520Reconstruction%2520Module%2520to%2520quantify%2520the%2520domain%2520shift%2520and%2520a%250ADynamic%2520Adaptation%2520Block%2520that%2520selectively%2520modifies%2520the%2520internal%2520features%2520of%2520a%250Apretrained%2520translation%2520model%2520to%2520mitigate%2520the%2520shift%2520without%2520compromising%2520the%250Aperformance%2520on%2520in-distribution%2520samples%2520that%2520do%2520not%2520require%2520adaptation.%2520We%250Aevaluate%2520our%2520approach%2520on%2520two%2520medical%2520image-to-image%2520translation%2520tasks%253A%2520low-dose%250ACT%2520denoising%2520and%2520T1%2520to%2520T2%2520MRI%2520translation%252C%2520showing%2520consistent%2520improvements%2520over%250Aboth%2520the%2520baseline%2520translation%2520model%2520without%2520TTA%2520and%2520prior%2520TTA%2520methods.%2520Our%250Aanalysis%2520highlights%2520the%2520limitations%2520of%2520the%2520state-of-the-art%2520that%2520uniformly%250Aapply%2520the%2520adaptation%2520to%2520both%2520out-of-distribution%2520and%2520in-distribution%2520samples%252C%250Ademonstrating%2520that%2520dynamic%252C%2520sample-specific%2520adjustment%2520offers%2520a%2520promising%2520path%250Ato%2520improve%2520model%2520resilience%2520in%2520real-world%2520scenarios.%2520The%2520code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/Sample-Aware-TTA/Code.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.00766v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sample-Aware%20Test-Time%20Adaptation%20for%20Medical%20Image-to-Image%20Translation&entry.906535625=Irene%20Iele%20and%20Francesco%20Di%20Feola%20and%20Valerio%20Guarrasi%20and%20Paolo%20Soda&entry.1292438233=%20%20Image-to-image%20translation%20has%20emerged%20as%20a%20powerful%20technique%20in%20medical%0Aimaging%2C%20enabling%20tasks%20such%20as%20image%20denoising%20and%20cross-modality%20conversion.%0AHowever%2C%20it%20suffers%20from%20limitations%20in%20handling%20out-of-distribution%20samples%0Awithout%20causing%20performance%20degradation.%20To%20address%20this%20limitation%2C%20we%20propose%0Aa%20novel%20Test-Time%20Adaptation%20%28TTA%29%20framework%20that%20dynamically%20adjusts%20the%0Atranslation%20process%20based%20on%20the%20characteristics%20of%20each%20test%20sample.%20Our%0Amethod%20introduces%20a%20Reconstruction%20Module%20to%20quantify%20the%20domain%20shift%20and%20a%0ADynamic%20Adaptation%20Block%20that%20selectively%20modifies%20the%20internal%20features%20of%20a%0Apretrained%20translation%20model%20to%20mitigate%20the%20shift%20without%20compromising%20the%0Aperformance%20on%20in-distribution%20samples%20that%20do%20not%20require%20adaptation.%20We%0Aevaluate%20our%20approach%20on%20two%20medical%20image-to-image%20translation%20tasks%3A%20low-dose%0ACT%20denoising%20and%20T1%20to%20T2%20MRI%20translation%2C%20showing%20consistent%20improvements%20over%0Aboth%20the%20baseline%20translation%20model%20without%20TTA%20and%20prior%20TTA%20methods.%20Our%0Aanalysis%20highlights%20the%20limitations%20of%20the%20state-of-the-art%20that%20uniformly%0Aapply%20the%20adaptation%20to%20both%20out-of-distribution%20and%20in-distribution%20samples%2C%0Ademonstrating%20that%20dynamic%2C%20sample-specific%20adjustment%20offers%20a%20promising%20path%0Ato%20improve%20model%20resilience%20in%20real-world%20scenarios.%20The%20code%20is%20available%20at%3A%0Ahttps%3A//github.com/Sample-Aware-TTA/Code.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.00766v2&entry.124074799=Read"},
{"title": "Spiking Neural Networks for Continuous Control via End-to-End\n  Model-Based Learning", "author": "Justus Huebotter and Pablo Lanillos and Marcel van Gerven and Serge Thill", "abstract": "  Despite recent progress in training spiking neural networks (SNNs) for\nclassification, their application to continuous motor control remains limited.\nHere, we demonstrate that fully spiking architectures can be trained end-to-end\nto control robotic arms with multiple degrees of freedom in continuous\nenvironments. Our predictive-control framework combines Leaky\nIntegrate-and-Fire dynamics with surrogate gradients, jointly optimizing a\nforward model for dynamics prediction and a policy network for goal-directed\naction. We evaluate this approach on both a planar 2D reaching task and a\nsimulated 6-DOF Franka Emika Panda robot. Results show that SNNs can achieve\nstable training and accurate torque control, establishing their viability for\nhigh-dimensional motor tasks. An extensive ablation study highlights the role\nof initialization, learnable time constants, and regularization in shaping\ntraining dynamics. We conclude that while stable and effective control can be\nachieved, recurrent spiking networks remain highly sensitive to hyperparameter\nsettings, underscoring the importance of principled design choices.\n", "link": "http://arxiv.org/abs/2509.05356v2", "date": "2025-09-16", "relevancy": 2.1511, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5499}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5341}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5271}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spiking%20Neural%20Networks%20for%20Continuous%20Control%20via%20End-to-End%0A%20%20Model-Based%20Learning&body=Title%3A%20Spiking%20Neural%20Networks%20for%20Continuous%20Control%20via%20End-to-End%0A%20%20Model-Based%20Learning%0AAuthor%3A%20Justus%20Huebotter%20and%20Pablo%20Lanillos%20and%20Marcel%20van%20Gerven%20and%20Serge%20Thill%0AAbstract%3A%20%20%20Despite%20recent%20progress%20in%20training%20spiking%20neural%20networks%20%28SNNs%29%20for%0Aclassification%2C%20their%20application%20to%20continuous%20motor%20control%20remains%20limited.%0AHere%2C%20we%20demonstrate%20that%20fully%20spiking%20architectures%20can%20be%20trained%20end-to-end%0Ato%20control%20robotic%20arms%20with%20multiple%20degrees%20of%20freedom%20in%20continuous%0Aenvironments.%20Our%20predictive-control%20framework%20combines%20Leaky%0AIntegrate-and-Fire%20dynamics%20with%20surrogate%20gradients%2C%20jointly%20optimizing%20a%0Aforward%20model%20for%20dynamics%20prediction%20and%20a%20policy%20network%20for%20goal-directed%0Aaction.%20We%20evaluate%20this%20approach%20on%20both%20a%20planar%202D%20reaching%20task%20and%20a%0Asimulated%206-DOF%20Franka%20Emika%20Panda%20robot.%20Results%20show%20that%20SNNs%20can%20achieve%0Astable%20training%20and%20accurate%20torque%20control%2C%20establishing%20their%20viability%20for%0Ahigh-dimensional%20motor%20tasks.%20An%20extensive%20ablation%20study%20highlights%20the%20role%0Aof%20initialization%2C%20learnable%20time%20constants%2C%20and%20regularization%20in%20shaping%0Atraining%20dynamics.%20We%20conclude%20that%20while%20stable%20and%20effective%20control%20can%20be%0Aachieved%2C%20recurrent%20spiking%20networks%20remain%20highly%20sensitive%20to%20hyperparameter%0Asettings%2C%20underscoring%20the%20importance%20of%20principled%20design%20choices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.05356v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpiking%2520Neural%2520Networks%2520for%2520Continuous%2520Control%2520via%2520End-to-End%250A%2520%2520Model-Based%2520Learning%26entry.906535625%3DJustus%2520Huebotter%2520and%2520Pablo%2520Lanillos%2520and%2520Marcel%2520van%2520Gerven%2520and%2520Serge%2520Thill%26entry.1292438233%3D%2520%2520Despite%2520recent%2520progress%2520in%2520training%2520spiking%2520neural%2520networks%2520%2528SNNs%2529%2520for%250Aclassification%252C%2520their%2520application%2520to%2520continuous%2520motor%2520control%2520remains%2520limited.%250AHere%252C%2520we%2520demonstrate%2520that%2520fully%2520spiking%2520architectures%2520can%2520be%2520trained%2520end-to-end%250Ato%2520control%2520robotic%2520arms%2520with%2520multiple%2520degrees%2520of%2520freedom%2520in%2520continuous%250Aenvironments.%2520Our%2520predictive-control%2520framework%2520combines%2520Leaky%250AIntegrate-and-Fire%2520dynamics%2520with%2520surrogate%2520gradients%252C%2520jointly%2520optimizing%2520a%250Aforward%2520model%2520for%2520dynamics%2520prediction%2520and%2520a%2520policy%2520network%2520for%2520goal-directed%250Aaction.%2520We%2520evaluate%2520this%2520approach%2520on%2520both%2520a%2520planar%25202D%2520reaching%2520task%2520and%2520a%250Asimulated%25206-DOF%2520Franka%2520Emika%2520Panda%2520robot.%2520Results%2520show%2520that%2520SNNs%2520can%2520achieve%250Astable%2520training%2520and%2520accurate%2520torque%2520control%252C%2520establishing%2520their%2520viability%2520for%250Ahigh-dimensional%2520motor%2520tasks.%2520An%2520extensive%2520ablation%2520study%2520highlights%2520the%2520role%250Aof%2520initialization%252C%2520learnable%2520time%2520constants%252C%2520and%2520regularization%2520in%2520shaping%250Atraining%2520dynamics.%2520We%2520conclude%2520that%2520while%2520stable%2520and%2520effective%2520control%2520can%2520be%250Aachieved%252C%2520recurrent%2520spiking%2520networks%2520remain%2520highly%2520sensitive%2520to%2520hyperparameter%250Asettings%252C%2520underscoring%2520the%2520importance%2520of%2520principled%2520design%2520choices.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.05356v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spiking%20Neural%20Networks%20for%20Continuous%20Control%20via%20End-to-End%0A%20%20Model-Based%20Learning&entry.906535625=Justus%20Huebotter%20and%20Pablo%20Lanillos%20and%20Marcel%20van%20Gerven%20and%20Serge%20Thill&entry.1292438233=%20%20Despite%20recent%20progress%20in%20training%20spiking%20neural%20networks%20%28SNNs%29%20for%0Aclassification%2C%20their%20application%20to%20continuous%20motor%20control%20remains%20limited.%0AHere%2C%20we%20demonstrate%20that%20fully%20spiking%20architectures%20can%20be%20trained%20end-to-end%0Ato%20control%20robotic%20arms%20with%20multiple%20degrees%20of%20freedom%20in%20continuous%0Aenvironments.%20Our%20predictive-control%20framework%20combines%20Leaky%0AIntegrate-and-Fire%20dynamics%20with%20surrogate%20gradients%2C%20jointly%20optimizing%20a%0Aforward%20model%20for%20dynamics%20prediction%20and%20a%20policy%20network%20for%20goal-directed%0Aaction.%20We%20evaluate%20this%20approach%20on%20both%20a%20planar%202D%20reaching%20task%20and%20a%0Asimulated%206-DOF%20Franka%20Emika%20Panda%20robot.%20Results%20show%20that%20SNNs%20can%20achieve%0Astable%20training%20and%20accurate%20torque%20control%2C%20establishing%20their%20viability%20for%0Ahigh-dimensional%20motor%20tasks.%20An%20extensive%20ablation%20study%20highlights%20the%20role%0Aof%20initialization%2C%20learnable%20time%20constants%2C%20and%20regularization%20in%20shaping%0Atraining%20dynamics.%20We%20conclude%20that%20while%20stable%20and%20effective%20control%20can%20be%0Aachieved%2C%20recurrent%20spiking%20networks%20remain%20highly%20sensitive%20to%20hyperparameter%0Asettings%2C%20underscoring%20the%20importance%20of%20principled%20design%20choices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.05356v2&entry.124074799=Read"},
{"title": "BranchGRPO: Stable and Efficient GRPO with Structured Branching in\n  Diffusion Models", "author": "Yuming Li and Yikai Wang and Yuying Zhu and Zhongyu Zhao and Ming Lu and Qi She and Shanghang Zhang", "abstract": "  Recent progress in aligning image and video generative models with Group\nRelative Policy Optimization (GRPO) has improved human preference alignment,\nbut existing variants remain inefficient due to sequential rollouts and large\nnumbers of sampling steps, unreliable credit assignment: sparse terminal\nrewards are uniformly propagated across timesteps, failing to capture the\nvarying criticality of decisions during denoising. In this paper, we present\nBranchGRPO, a method that restructures the rollout process into a branching\ntree, where shared prefixes amortize computation and pruning removes low-value\npaths and redundant depths. BranchGRPO introduces three contributions: (1) a\nbranching scheme that amortizes rollout cost through shared prefixes while\npreserving exploration diversity; (2) a reward fusion and depth-wise advantage\nestimator that transforms sparse terminal rewards into dense step-level\nsignals; and (3) pruning strategies that cut gradient computation but leave\nforward rollouts and exploration unaffected. On HPDv2.1 image alignment,\nBranchGRPO improves alignment scores by up to \\textbf{16\\%} over DanceGRPO,\nwhile reducing per-iteration training time by nearly \\textbf{55\\%}. A hybrid\nvariant, BranchGRPO-Mix, further accelerates training to 4.7x faster than\nDanceGRPO without degrading alignment. On WanX video generation, it further\nachieves higher Video-Align scores with sharper and temporally consistent\nframes compared to DanceGRPO. Codes are available at\n\\href{https://fredreic1849.github.io/BranchGRPO-Webpage/}{BranchGRPO}.\n", "link": "http://arxiv.org/abs/2509.06040v4", "date": "2025-09-16", "relevancy": 2.1486, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5632}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5373}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5266}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BranchGRPO%3A%20Stable%20and%20Efficient%20GRPO%20with%20Structured%20Branching%20in%0A%20%20Diffusion%20Models&body=Title%3A%20BranchGRPO%3A%20Stable%20and%20Efficient%20GRPO%20with%20Structured%20Branching%20in%0A%20%20Diffusion%20Models%0AAuthor%3A%20Yuming%20Li%20and%20Yikai%20Wang%20and%20Yuying%20Zhu%20and%20Zhongyu%20Zhao%20and%20Ming%20Lu%20and%20Qi%20She%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Recent%20progress%20in%20aligning%20image%20and%20video%20generative%20models%20with%20Group%0ARelative%20Policy%20Optimization%20%28GRPO%29%20has%20improved%20human%20preference%20alignment%2C%0Abut%20existing%20variants%20remain%20inefficient%20due%20to%20sequential%20rollouts%20and%20large%0Anumbers%20of%20sampling%20steps%2C%20unreliable%20credit%20assignment%3A%20sparse%20terminal%0Arewards%20are%20uniformly%20propagated%20across%20timesteps%2C%20failing%20to%20capture%20the%0Avarying%20criticality%20of%20decisions%20during%20denoising.%20In%20this%20paper%2C%20we%20present%0ABranchGRPO%2C%20a%20method%20that%20restructures%20the%20rollout%20process%20into%20a%20branching%0Atree%2C%20where%20shared%20prefixes%20amortize%20computation%20and%20pruning%20removes%20low-value%0Apaths%20and%20redundant%20depths.%20BranchGRPO%20introduces%20three%20contributions%3A%20%281%29%20a%0Abranching%20scheme%20that%20amortizes%20rollout%20cost%20through%20shared%20prefixes%20while%0Apreserving%20exploration%20diversity%3B%20%282%29%20a%20reward%20fusion%20and%20depth-wise%20advantage%0Aestimator%20that%20transforms%20sparse%20terminal%20rewards%20into%20dense%20step-level%0Asignals%3B%20and%20%283%29%20pruning%20strategies%20that%20cut%20gradient%20computation%20but%20leave%0Aforward%20rollouts%20and%20exploration%20unaffected.%20On%20HPDv2.1%20image%20alignment%2C%0ABranchGRPO%20improves%20alignment%20scores%20by%20up%20to%20%5Ctextbf%7B16%5C%25%7D%20over%20DanceGRPO%2C%0Awhile%20reducing%20per-iteration%20training%20time%20by%20nearly%20%5Ctextbf%7B55%5C%25%7D.%20A%20hybrid%0Avariant%2C%20BranchGRPO-Mix%2C%20further%20accelerates%20training%20to%204.7x%20faster%20than%0ADanceGRPO%20without%20degrading%20alignment.%20On%20WanX%20video%20generation%2C%20it%20further%0Aachieves%20higher%20Video-Align%20scores%20with%20sharper%20and%20temporally%20consistent%0Aframes%20compared%20to%20DanceGRPO.%20Codes%20are%20available%20at%0A%5Chref%7Bhttps%3A//fredreic1849.github.io/BranchGRPO-Webpage/%7D%7BBranchGRPO%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06040v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBranchGRPO%253A%2520Stable%2520and%2520Efficient%2520GRPO%2520with%2520Structured%2520Branching%2520in%250A%2520%2520Diffusion%2520Models%26entry.906535625%3DYuming%2520Li%2520and%2520Yikai%2520Wang%2520and%2520Yuying%2520Zhu%2520and%2520Zhongyu%2520Zhao%2520and%2520Ming%2520Lu%2520and%2520Qi%2520She%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520aligning%2520image%2520and%2520video%2520generative%2520models%2520with%2520Group%250ARelative%2520Policy%2520Optimization%2520%2528GRPO%2529%2520has%2520improved%2520human%2520preference%2520alignment%252C%250Abut%2520existing%2520variants%2520remain%2520inefficient%2520due%2520to%2520sequential%2520rollouts%2520and%2520large%250Anumbers%2520of%2520sampling%2520steps%252C%2520unreliable%2520credit%2520assignment%253A%2520sparse%2520terminal%250Arewards%2520are%2520uniformly%2520propagated%2520across%2520timesteps%252C%2520failing%2520to%2520capture%2520the%250Avarying%2520criticality%2520of%2520decisions%2520during%2520denoising.%2520In%2520this%2520paper%252C%2520we%2520present%250ABranchGRPO%252C%2520a%2520method%2520that%2520restructures%2520the%2520rollout%2520process%2520into%2520a%2520branching%250Atree%252C%2520where%2520shared%2520prefixes%2520amortize%2520computation%2520and%2520pruning%2520removes%2520low-value%250Apaths%2520and%2520redundant%2520depths.%2520BranchGRPO%2520introduces%2520three%2520contributions%253A%2520%25281%2529%2520a%250Abranching%2520scheme%2520that%2520amortizes%2520rollout%2520cost%2520through%2520shared%2520prefixes%2520while%250Apreserving%2520exploration%2520diversity%253B%2520%25282%2529%2520a%2520reward%2520fusion%2520and%2520depth-wise%2520advantage%250Aestimator%2520that%2520transforms%2520sparse%2520terminal%2520rewards%2520into%2520dense%2520step-level%250Asignals%253B%2520and%2520%25283%2529%2520pruning%2520strategies%2520that%2520cut%2520gradient%2520computation%2520but%2520leave%250Aforward%2520rollouts%2520and%2520exploration%2520unaffected.%2520On%2520HPDv2.1%2520image%2520alignment%252C%250ABranchGRPO%2520improves%2520alignment%2520scores%2520by%2520up%2520to%2520%255Ctextbf%257B16%255C%2525%257D%2520over%2520DanceGRPO%252C%250Awhile%2520reducing%2520per-iteration%2520training%2520time%2520by%2520nearly%2520%255Ctextbf%257B55%255C%2525%257D.%2520A%2520hybrid%250Avariant%252C%2520BranchGRPO-Mix%252C%2520further%2520accelerates%2520training%2520to%25204.7x%2520faster%2520than%250ADanceGRPO%2520without%2520degrading%2520alignment.%2520On%2520WanX%2520video%2520generation%252C%2520it%2520further%250Aachieves%2520higher%2520Video-Align%2520scores%2520with%2520sharper%2520and%2520temporally%2520consistent%250Aframes%2520compared%2520to%2520DanceGRPO.%2520Codes%2520are%2520available%2520at%250A%255Chref%257Bhttps%253A//fredreic1849.github.io/BranchGRPO-Webpage/%257D%257BBranchGRPO%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06040v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BranchGRPO%3A%20Stable%20and%20Efficient%20GRPO%20with%20Structured%20Branching%20in%0A%20%20Diffusion%20Models&entry.906535625=Yuming%20Li%20and%20Yikai%20Wang%20and%20Yuying%20Zhu%20and%20Zhongyu%20Zhao%20and%20Ming%20Lu%20and%20Qi%20She%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Recent%20progress%20in%20aligning%20image%20and%20video%20generative%20models%20with%20Group%0ARelative%20Policy%20Optimization%20%28GRPO%29%20has%20improved%20human%20preference%20alignment%2C%0Abut%20existing%20variants%20remain%20inefficient%20due%20to%20sequential%20rollouts%20and%20large%0Anumbers%20of%20sampling%20steps%2C%20unreliable%20credit%20assignment%3A%20sparse%20terminal%0Arewards%20are%20uniformly%20propagated%20across%20timesteps%2C%20failing%20to%20capture%20the%0Avarying%20criticality%20of%20decisions%20during%20denoising.%20In%20this%20paper%2C%20we%20present%0ABranchGRPO%2C%20a%20method%20that%20restructures%20the%20rollout%20process%20into%20a%20branching%0Atree%2C%20where%20shared%20prefixes%20amortize%20computation%20and%20pruning%20removes%20low-value%0Apaths%20and%20redundant%20depths.%20BranchGRPO%20introduces%20three%20contributions%3A%20%281%29%20a%0Abranching%20scheme%20that%20amortizes%20rollout%20cost%20through%20shared%20prefixes%20while%0Apreserving%20exploration%20diversity%3B%20%282%29%20a%20reward%20fusion%20and%20depth-wise%20advantage%0Aestimator%20that%20transforms%20sparse%20terminal%20rewards%20into%20dense%20step-level%0Asignals%3B%20and%20%283%29%20pruning%20strategies%20that%20cut%20gradient%20computation%20but%20leave%0Aforward%20rollouts%20and%20exploration%20unaffected.%20On%20HPDv2.1%20image%20alignment%2C%0ABranchGRPO%20improves%20alignment%20scores%20by%20up%20to%20%5Ctextbf%7B16%5C%25%7D%20over%20DanceGRPO%2C%0Awhile%20reducing%20per-iteration%20training%20time%20by%20nearly%20%5Ctextbf%7B55%5C%25%7D.%20A%20hybrid%0Avariant%2C%20BranchGRPO-Mix%2C%20further%20accelerates%20training%20to%204.7x%20faster%20than%0ADanceGRPO%20without%20degrading%20alignment.%20On%20WanX%20video%20generation%2C%20it%20further%0Aachieves%20higher%20Video-Align%20scores%20with%20sharper%20and%20temporally%20consistent%0Aframes%20compared%20to%20DanceGRPO.%20Codes%20are%20available%20at%0A%5Chref%7Bhttps%3A//fredreic1849.github.io/BranchGRPO-Webpage/%7D%7BBranchGRPO%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06040v4&entry.124074799=Read"},
{"title": "Traces Propagation: Memory-Efficient and Scalable Forward-Only Learning\n  in Spiking Neural Networks", "author": "Lorenzo Pes and Bojian Yin and Sander Stuijk and Federico Corradi", "abstract": "  Spiking Neural Networks (SNNs) provide an efficient framework for processing\ndynamic spatio-temporal signals and for investigating the learning principles\nunderlying biological neural systems. A key challenge in training SNNs is to\nsolve both spatial and temporal credit assignment. The dominant approach for\ntraining SNNs is Backpropagation Through Time (BPTT) with surrogate gradients.\nHowever, BPTT is in stark contrast with the spatial and temporal locality\nobserved in biological neural systems and leads to high computational and\nmemory demands, limiting efficient training strategies and on-device learning.\nAlthough existing local learning rules achieve local temporal credit assignment\nby leveraging eligibility traces, they fail to address the spatial credit\nassignment without resorting to auxiliary layer-wise matrices, which increase\nmemory overhead and hinder scalability, especially on embedded devices. In this\nwork, we propose Traces Propagation (TP), a forward-only, memory-efficient,\nscalable, and fully local learning rule that combines eligibility traces with a\nlayer-wise contrastive loss without requiring auxiliary layer-wise matrices. TP\noutperforms other fully local learning rules on NMNIST and SHD datasets. On\nmore complex datasets such as DVS-GESTURE and DVS-CIFAR10, TP showcases\ncompetitive performance and scales effectively to deeper SNN architectures such\nas VGG-9, while providing favorable memory scaling compared to prior fully\nlocal scalable rules, for datasets with a significant number of classes.\nFinally, we show that TP is well suited for practical fine-tuning tasks, such\nas keyword spotting on the Google Speech Commands dataset, thus paving the way\nfor efficient learning at the edge.\n", "link": "http://arxiv.org/abs/2509.13053v1", "date": "2025-09-16", "relevancy": 2.1183, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5352}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5281}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5245}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Traces%20Propagation%3A%20Memory-Efficient%20and%20Scalable%20Forward-Only%20Learning%0A%20%20in%20Spiking%20Neural%20Networks&body=Title%3A%20Traces%20Propagation%3A%20Memory-Efficient%20and%20Scalable%20Forward-Only%20Learning%0A%20%20in%20Spiking%20Neural%20Networks%0AAuthor%3A%20Lorenzo%20Pes%20and%20Bojian%20Yin%20and%20Sander%20Stuijk%20and%20Federico%20Corradi%0AAbstract%3A%20%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20provide%20an%20efficient%20framework%20for%20processing%0Adynamic%20spatio-temporal%20signals%20and%20for%20investigating%20the%20learning%20principles%0Aunderlying%20biological%20neural%20systems.%20A%20key%20challenge%20in%20training%20SNNs%20is%20to%0Asolve%20both%20spatial%20and%20temporal%20credit%20assignment.%20The%20dominant%20approach%20for%0Atraining%20SNNs%20is%20Backpropagation%20Through%20Time%20%28BPTT%29%20with%20surrogate%20gradients.%0AHowever%2C%20BPTT%20is%20in%20stark%20contrast%20with%20the%20spatial%20and%20temporal%20locality%0Aobserved%20in%20biological%20neural%20systems%20and%20leads%20to%20high%20computational%20and%0Amemory%20demands%2C%20limiting%20efficient%20training%20strategies%20and%20on-device%20learning.%0AAlthough%20existing%20local%20learning%20rules%20achieve%20local%20temporal%20credit%20assignment%0Aby%20leveraging%20eligibility%20traces%2C%20they%20fail%20to%20address%20the%20spatial%20credit%0Aassignment%20without%20resorting%20to%20auxiliary%20layer-wise%20matrices%2C%20which%20increase%0Amemory%20overhead%20and%20hinder%20scalability%2C%20especially%20on%20embedded%20devices.%20In%20this%0Awork%2C%20we%20propose%20Traces%20Propagation%20%28TP%29%2C%20a%20forward-only%2C%20memory-efficient%2C%0Ascalable%2C%20and%20fully%20local%20learning%20rule%20that%20combines%20eligibility%20traces%20with%20a%0Alayer-wise%20contrastive%20loss%20without%20requiring%20auxiliary%20layer-wise%20matrices.%20TP%0Aoutperforms%20other%20fully%20local%20learning%20rules%20on%20NMNIST%20and%20SHD%20datasets.%20On%0Amore%20complex%20datasets%20such%20as%20DVS-GESTURE%20and%20DVS-CIFAR10%2C%20TP%20showcases%0Acompetitive%20performance%20and%20scales%20effectively%20to%20deeper%20SNN%20architectures%20such%0Aas%20VGG-9%2C%20while%20providing%20favorable%20memory%20scaling%20compared%20to%20prior%20fully%0Alocal%20scalable%20rules%2C%20for%20datasets%20with%20a%20significant%20number%20of%20classes.%0AFinally%2C%20we%20show%20that%20TP%20is%20well%20suited%20for%20practical%20fine-tuning%20tasks%2C%20such%0Aas%20keyword%20spotting%20on%20the%20Google%20Speech%20Commands%20dataset%2C%20thus%20paving%20the%20way%0Afor%20efficient%20learning%20at%20the%20edge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraces%2520Propagation%253A%2520Memory-Efficient%2520and%2520Scalable%2520Forward-Only%2520Learning%250A%2520%2520in%2520Spiking%2520Neural%2520Networks%26entry.906535625%3DLorenzo%2520Pes%2520and%2520Bojian%2520Yin%2520and%2520Sander%2520Stuijk%2520and%2520Federico%2520Corradi%26entry.1292438233%3D%2520%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520provide%2520an%2520efficient%2520framework%2520for%2520processing%250Adynamic%2520spatio-temporal%2520signals%2520and%2520for%2520investigating%2520the%2520learning%2520principles%250Aunderlying%2520biological%2520neural%2520systems.%2520A%2520key%2520challenge%2520in%2520training%2520SNNs%2520is%2520to%250Asolve%2520both%2520spatial%2520and%2520temporal%2520credit%2520assignment.%2520The%2520dominant%2520approach%2520for%250Atraining%2520SNNs%2520is%2520Backpropagation%2520Through%2520Time%2520%2528BPTT%2529%2520with%2520surrogate%2520gradients.%250AHowever%252C%2520BPTT%2520is%2520in%2520stark%2520contrast%2520with%2520the%2520spatial%2520and%2520temporal%2520locality%250Aobserved%2520in%2520biological%2520neural%2520systems%2520and%2520leads%2520to%2520high%2520computational%2520and%250Amemory%2520demands%252C%2520limiting%2520efficient%2520training%2520strategies%2520and%2520on-device%2520learning.%250AAlthough%2520existing%2520local%2520learning%2520rules%2520achieve%2520local%2520temporal%2520credit%2520assignment%250Aby%2520leveraging%2520eligibility%2520traces%252C%2520they%2520fail%2520to%2520address%2520the%2520spatial%2520credit%250Aassignment%2520without%2520resorting%2520to%2520auxiliary%2520layer-wise%2520matrices%252C%2520which%2520increase%250Amemory%2520overhead%2520and%2520hinder%2520scalability%252C%2520especially%2520on%2520embedded%2520devices.%2520In%2520this%250Awork%252C%2520we%2520propose%2520Traces%2520Propagation%2520%2528TP%2529%252C%2520a%2520forward-only%252C%2520memory-efficient%252C%250Ascalable%252C%2520and%2520fully%2520local%2520learning%2520rule%2520that%2520combines%2520eligibility%2520traces%2520with%2520a%250Alayer-wise%2520contrastive%2520loss%2520without%2520requiring%2520auxiliary%2520layer-wise%2520matrices.%2520TP%250Aoutperforms%2520other%2520fully%2520local%2520learning%2520rules%2520on%2520NMNIST%2520and%2520SHD%2520datasets.%2520On%250Amore%2520complex%2520datasets%2520such%2520as%2520DVS-GESTURE%2520and%2520DVS-CIFAR10%252C%2520TP%2520showcases%250Acompetitive%2520performance%2520and%2520scales%2520effectively%2520to%2520deeper%2520SNN%2520architectures%2520such%250Aas%2520VGG-9%252C%2520while%2520providing%2520favorable%2520memory%2520scaling%2520compared%2520to%2520prior%2520fully%250Alocal%2520scalable%2520rules%252C%2520for%2520datasets%2520with%2520a%2520significant%2520number%2520of%2520classes.%250AFinally%252C%2520we%2520show%2520that%2520TP%2520is%2520well%2520suited%2520for%2520practical%2520fine-tuning%2520tasks%252C%2520such%250Aas%2520keyword%2520spotting%2520on%2520the%2520Google%2520Speech%2520Commands%2520dataset%252C%2520thus%2520paving%2520the%2520way%250Afor%2520efficient%2520learning%2520at%2520the%2520edge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Traces%20Propagation%3A%20Memory-Efficient%20and%20Scalable%20Forward-Only%20Learning%0A%20%20in%20Spiking%20Neural%20Networks&entry.906535625=Lorenzo%20Pes%20and%20Bojian%20Yin%20and%20Sander%20Stuijk%20and%20Federico%20Corradi&entry.1292438233=%20%20Spiking%20Neural%20Networks%20%28SNNs%29%20provide%20an%20efficient%20framework%20for%20processing%0Adynamic%20spatio-temporal%20signals%20and%20for%20investigating%20the%20learning%20principles%0Aunderlying%20biological%20neural%20systems.%20A%20key%20challenge%20in%20training%20SNNs%20is%20to%0Asolve%20both%20spatial%20and%20temporal%20credit%20assignment.%20The%20dominant%20approach%20for%0Atraining%20SNNs%20is%20Backpropagation%20Through%20Time%20%28BPTT%29%20with%20surrogate%20gradients.%0AHowever%2C%20BPTT%20is%20in%20stark%20contrast%20with%20the%20spatial%20and%20temporal%20locality%0Aobserved%20in%20biological%20neural%20systems%20and%20leads%20to%20high%20computational%20and%0Amemory%20demands%2C%20limiting%20efficient%20training%20strategies%20and%20on-device%20learning.%0AAlthough%20existing%20local%20learning%20rules%20achieve%20local%20temporal%20credit%20assignment%0Aby%20leveraging%20eligibility%20traces%2C%20they%20fail%20to%20address%20the%20spatial%20credit%0Aassignment%20without%20resorting%20to%20auxiliary%20layer-wise%20matrices%2C%20which%20increase%0Amemory%20overhead%20and%20hinder%20scalability%2C%20especially%20on%20embedded%20devices.%20In%20this%0Awork%2C%20we%20propose%20Traces%20Propagation%20%28TP%29%2C%20a%20forward-only%2C%20memory-efficient%2C%0Ascalable%2C%20and%20fully%20local%20learning%20rule%20that%20combines%20eligibility%20traces%20with%20a%0Alayer-wise%20contrastive%20loss%20without%20requiring%20auxiliary%20layer-wise%20matrices.%20TP%0Aoutperforms%20other%20fully%20local%20learning%20rules%20on%20NMNIST%20and%20SHD%20datasets.%20On%0Amore%20complex%20datasets%20such%20as%20DVS-GESTURE%20and%20DVS-CIFAR10%2C%20TP%20showcases%0Acompetitive%20performance%20and%20scales%20effectively%20to%20deeper%20SNN%20architectures%20such%0Aas%20VGG-9%2C%20while%20providing%20favorable%20memory%20scaling%20compared%20to%20prior%20fully%0Alocal%20scalable%20rules%2C%20for%20datasets%20with%20a%20significant%20number%20of%20classes.%0AFinally%2C%20we%20show%20that%20TP%20is%20well%20suited%20for%20practical%20fine-tuning%20tasks%2C%20such%0Aas%20keyword%20spotting%20on%20the%20Google%20Speech%20Commands%20dataset%2C%20thus%20paving%20the%20way%0Afor%20efficient%20learning%20at%20the%20edge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13053v1&entry.124074799=Read"},
{"title": "Using KL-Divergence to Focus Frequency Information in Low-Light Image\n  Enhancement", "author": "Yan Xingyang and Huang Xiaohong and Zhang Zhao and You Tian and Xu Ziheng", "abstract": "  In the Fourier domain, luminance information is primarily encoded in the\namplitude spectrum, while spatial structures are captured in the phase\ncomponents. The traditional Fourier Frequency information fitting employs\npixel-wise loss functions, which tend to focus excessively on local information\nand may lead to global information loss. In this paper, we present LLFDisc, a\nU-shaped deep enhancement network that integrates cross-attention and gating\nmechanisms tailored for frequency-aware enhancement. We propose a novel\ndistribution-aware loss that directly fits the Fourier-domain information and\nminimizes their divergence using a closed-form KL-Divergence objective. This\nenables the model to align Fourier-domain information more robustly than with\nconventional MSE-based losses. Furthermore, we enhance the perceptual loss\nbased on VGG by embedding KL-Divergence on extracted deep features, enabling\nbetter structural fidelity. Extensive experiments across multiple benchmarks\ndemonstrate that LLFDisc achieves state-of-the-art performance in both\nqualitative and quantitative evaluations. Our code will be released at:\nhttps://github.com/YanXY000/LLFDisc\n", "link": "http://arxiv.org/abs/2509.13083v1", "date": "2025-09-16", "relevancy": 2.1072, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5357}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5287}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20KL-Divergence%20to%20Focus%20Frequency%20Information%20in%20Low-Light%20Image%0A%20%20Enhancement&body=Title%3A%20Using%20KL-Divergence%20to%20Focus%20Frequency%20Information%20in%20Low-Light%20Image%0A%20%20Enhancement%0AAuthor%3A%20Yan%20Xingyang%20and%20Huang%20Xiaohong%20and%20Zhang%20Zhao%20and%20You%20Tian%20and%20Xu%20Ziheng%0AAbstract%3A%20%20%20In%20the%20Fourier%20domain%2C%20luminance%20information%20is%20primarily%20encoded%20in%20the%0Aamplitude%20spectrum%2C%20while%20spatial%20structures%20are%20captured%20in%20the%20phase%0Acomponents.%20The%20traditional%20Fourier%20Frequency%20information%20fitting%20employs%0Apixel-wise%20loss%20functions%2C%20which%20tend%20to%20focus%20excessively%20on%20local%20information%0Aand%20may%20lead%20to%20global%20information%20loss.%20In%20this%20paper%2C%20we%20present%20LLFDisc%2C%20a%0AU-shaped%20deep%20enhancement%20network%20that%20integrates%20cross-attention%20and%20gating%0Amechanisms%20tailored%20for%20frequency-aware%20enhancement.%20We%20propose%20a%20novel%0Adistribution-aware%20loss%20that%20directly%20fits%20the%20Fourier-domain%20information%20and%0Aminimizes%20their%20divergence%20using%20a%20closed-form%20KL-Divergence%20objective.%20This%0Aenables%20the%20model%20to%20align%20Fourier-domain%20information%20more%20robustly%20than%20with%0Aconventional%20MSE-based%20losses.%20Furthermore%2C%20we%20enhance%20the%20perceptual%20loss%0Abased%20on%20VGG%20by%20embedding%20KL-Divergence%20on%20extracted%20deep%20features%2C%20enabling%0Abetter%20structural%20fidelity.%20Extensive%20experiments%20across%20multiple%20benchmarks%0Ademonstrate%20that%20LLFDisc%20achieves%20state-of-the-art%20performance%20in%20both%0Aqualitative%20and%20quantitative%20evaluations.%20Our%20code%20will%20be%20released%20at%3A%0Ahttps%3A//github.com/YanXY000/LLFDisc%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13083v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520KL-Divergence%2520to%2520Focus%2520Frequency%2520Information%2520in%2520Low-Light%2520Image%250A%2520%2520Enhancement%26entry.906535625%3DYan%2520Xingyang%2520and%2520Huang%2520Xiaohong%2520and%2520Zhang%2520Zhao%2520and%2520You%2520Tian%2520and%2520Xu%2520Ziheng%26entry.1292438233%3D%2520%2520In%2520the%2520Fourier%2520domain%252C%2520luminance%2520information%2520is%2520primarily%2520encoded%2520in%2520the%250Aamplitude%2520spectrum%252C%2520while%2520spatial%2520structures%2520are%2520captured%2520in%2520the%2520phase%250Acomponents.%2520The%2520traditional%2520Fourier%2520Frequency%2520information%2520fitting%2520employs%250Apixel-wise%2520loss%2520functions%252C%2520which%2520tend%2520to%2520focus%2520excessively%2520on%2520local%2520information%250Aand%2520may%2520lead%2520to%2520global%2520information%2520loss.%2520In%2520this%2520paper%252C%2520we%2520present%2520LLFDisc%252C%2520a%250AU-shaped%2520deep%2520enhancement%2520network%2520that%2520integrates%2520cross-attention%2520and%2520gating%250Amechanisms%2520tailored%2520for%2520frequency-aware%2520enhancement.%2520We%2520propose%2520a%2520novel%250Adistribution-aware%2520loss%2520that%2520directly%2520fits%2520the%2520Fourier-domain%2520information%2520and%250Aminimizes%2520their%2520divergence%2520using%2520a%2520closed-form%2520KL-Divergence%2520objective.%2520This%250Aenables%2520the%2520model%2520to%2520align%2520Fourier-domain%2520information%2520more%2520robustly%2520than%2520with%250Aconventional%2520MSE-based%2520losses.%2520Furthermore%252C%2520we%2520enhance%2520the%2520perceptual%2520loss%250Abased%2520on%2520VGG%2520by%2520embedding%2520KL-Divergence%2520on%2520extracted%2520deep%2520features%252C%2520enabling%250Abetter%2520structural%2520fidelity.%2520Extensive%2520experiments%2520across%2520multiple%2520benchmarks%250Ademonstrate%2520that%2520LLFDisc%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%250Aqualitative%2520and%2520quantitative%2520evaluations.%2520Our%2520code%2520will%2520be%2520released%2520at%253A%250Ahttps%253A//github.com/YanXY000/LLFDisc%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13083v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20KL-Divergence%20to%20Focus%20Frequency%20Information%20in%20Low-Light%20Image%0A%20%20Enhancement&entry.906535625=Yan%20Xingyang%20and%20Huang%20Xiaohong%20and%20Zhang%20Zhao%20and%20You%20Tian%20and%20Xu%20Ziheng&entry.1292438233=%20%20In%20the%20Fourier%20domain%2C%20luminance%20information%20is%20primarily%20encoded%20in%20the%0Aamplitude%20spectrum%2C%20while%20spatial%20structures%20are%20captured%20in%20the%20phase%0Acomponents.%20The%20traditional%20Fourier%20Frequency%20information%20fitting%20employs%0Apixel-wise%20loss%20functions%2C%20which%20tend%20to%20focus%20excessively%20on%20local%20information%0Aand%20may%20lead%20to%20global%20information%20loss.%20In%20this%20paper%2C%20we%20present%20LLFDisc%2C%20a%0AU-shaped%20deep%20enhancement%20network%20that%20integrates%20cross-attention%20and%20gating%0Amechanisms%20tailored%20for%20frequency-aware%20enhancement.%20We%20propose%20a%20novel%0Adistribution-aware%20loss%20that%20directly%20fits%20the%20Fourier-domain%20information%20and%0Aminimizes%20their%20divergence%20using%20a%20closed-form%20KL-Divergence%20objective.%20This%0Aenables%20the%20model%20to%20align%20Fourier-domain%20information%20more%20robustly%20than%20with%0Aconventional%20MSE-based%20losses.%20Furthermore%2C%20we%20enhance%20the%20perceptual%20loss%0Abased%20on%20VGG%20by%20embedding%20KL-Divergence%20on%20extracted%20deep%20features%2C%20enabling%0Abetter%20structural%20fidelity.%20Extensive%20experiments%20across%20multiple%20benchmarks%0Ademonstrate%20that%20LLFDisc%20achieves%20state-of-the-art%20performance%20in%20both%0Aqualitative%20and%20quantitative%20evaluations.%20Our%20code%20will%20be%20released%20at%3A%0Ahttps%3A//github.com/YanXY000/LLFDisc%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13083v1&entry.124074799=Read"},
{"title": "Hierarchical Deep Fusion Framework for Multi-dimensional Facial Forgery\n  Detection -- The 2024 Global Deepfake Image Detection Challenge", "author": "Kohou Wang and Huan Hu and Xiang Liu and Zezhou Chen and Ping Chen and Zhaoxiang Liu and Shiguo Lian", "abstract": "  The proliferation of sophisticated deepfake technology poses significant\nchallenges to digital security and authenticity. Detecting these forgeries,\nespecially across a wide spectrum of manipulation techniques, requires robust\nand generalized models. This paper introduces the Hierarchical Deep Fusion\nFramework (HDFF), an ensemble-based deep learning architecture designed for\nhigh-performance facial forgery detection. Our framework integrates four\ndiverse pre-trained sub-models, Swin-MLP, CoAtNet, EfficientNetV2, and DaViT,\nwhich are meticulously fine-tuned through a multi-stage process on the\nMultiFFDI dataset. By concatenating the feature representations from these\nspecialized models and training a final classifier layer, HDFF effectively\nleverages their collective strengths. This approach achieved a final score of\n0.96852 on the competition's private leaderboard, securing the 20th position\nout of 184 teams, demonstrating the efficacy of hierarchical fusion for complex\nimage classification tasks.\n", "link": "http://arxiv.org/abs/2509.13107v1", "date": "2025-09-16", "relevancy": 2.1066, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5373}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5205}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Deep%20Fusion%20Framework%20for%20Multi-dimensional%20Facial%20Forgery%0A%20%20Detection%20--%20The%202024%20Global%20Deepfake%20Image%20Detection%20Challenge&body=Title%3A%20Hierarchical%20Deep%20Fusion%20Framework%20for%20Multi-dimensional%20Facial%20Forgery%0A%20%20Detection%20--%20The%202024%20Global%20Deepfake%20Image%20Detection%20Challenge%0AAuthor%3A%20Kohou%20Wang%20and%20Huan%20Hu%20and%20Xiang%20Liu%20and%20Zezhou%20Chen%20and%20Ping%20Chen%20and%20Zhaoxiang%20Liu%20and%20Shiguo%20Lian%0AAbstract%3A%20%20%20The%20proliferation%20of%20sophisticated%20deepfake%20technology%20poses%20significant%0Achallenges%20to%20digital%20security%20and%20authenticity.%20Detecting%20these%20forgeries%2C%0Aespecially%20across%20a%20wide%20spectrum%20of%20manipulation%20techniques%2C%20requires%20robust%0Aand%20generalized%20models.%20This%20paper%20introduces%20the%20Hierarchical%20Deep%20Fusion%0AFramework%20%28HDFF%29%2C%20an%20ensemble-based%20deep%20learning%20architecture%20designed%20for%0Ahigh-performance%20facial%20forgery%20detection.%20Our%20framework%20integrates%20four%0Adiverse%20pre-trained%20sub-models%2C%20Swin-MLP%2C%20CoAtNet%2C%20EfficientNetV2%2C%20and%20DaViT%2C%0Awhich%20are%20meticulously%20fine-tuned%20through%20a%20multi-stage%20process%20on%20the%0AMultiFFDI%20dataset.%20By%20concatenating%20the%20feature%20representations%20from%20these%0Aspecialized%20models%20and%20training%20a%20final%20classifier%20layer%2C%20HDFF%20effectively%0Aleverages%20their%20collective%20strengths.%20This%20approach%20achieved%20a%20final%20score%20of%0A0.96852%20on%20the%20competition%27s%20private%20leaderboard%2C%20securing%20the%2020th%20position%0Aout%20of%20184%20teams%2C%20demonstrating%20the%20efficacy%20of%20hierarchical%20fusion%20for%20complex%0Aimage%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13107v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Deep%2520Fusion%2520Framework%2520for%2520Multi-dimensional%2520Facial%2520Forgery%250A%2520%2520Detection%2520--%2520The%25202024%2520Global%2520Deepfake%2520Image%2520Detection%2520Challenge%26entry.906535625%3DKohou%2520Wang%2520and%2520Huan%2520Hu%2520and%2520Xiang%2520Liu%2520and%2520Zezhou%2520Chen%2520and%2520Ping%2520Chen%2520and%2520Zhaoxiang%2520Liu%2520and%2520Shiguo%2520Lian%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520sophisticated%2520deepfake%2520technology%2520poses%2520significant%250Achallenges%2520to%2520digital%2520security%2520and%2520authenticity.%2520Detecting%2520these%2520forgeries%252C%250Aespecially%2520across%2520a%2520wide%2520spectrum%2520of%2520manipulation%2520techniques%252C%2520requires%2520robust%250Aand%2520generalized%2520models.%2520This%2520paper%2520introduces%2520the%2520Hierarchical%2520Deep%2520Fusion%250AFramework%2520%2528HDFF%2529%252C%2520an%2520ensemble-based%2520deep%2520learning%2520architecture%2520designed%2520for%250Ahigh-performance%2520facial%2520forgery%2520detection.%2520Our%2520framework%2520integrates%2520four%250Adiverse%2520pre-trained%2520sub-models%252C%2520Swin-MLP%252C%2520CoAtNet%252C%2520EfficientNetV2%252C%2520and%2520DaViT%252C%250Awhich%2520are%2520meticulously%2520fine-tuned%2520through%2520a%2520multi-stage%2520process%2520on%2520the%250AMultiFFDI%2520dataset.%2520By%2520concatenating%2520the%2520feature%2520representations%2520from%2520these%250Aspecialized%2520models%2520and%2520training%2520a%2520final%2520classifier%2520layer%252C%2520HDFF%2520effectively%250Aleverages%2520their%2520collective%2520strengths.%2520This%2520approach%2520achieved%2520a%2520final%2520score%2520of%250A0.96852%2520on%2520the%2520competition%2527s%2520private%2520leaderboard%252C%2520securing%2520the%252020th%2520position%250Aout%2520of%2520184%2520teams%252C%2520demonstrating%2520the%2520efficacy%2520of%2520hierarchical%2520fusion%2520for%2520complex%250Aimage%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13107v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Deep%20Fusion%20Framework%20for%20Multi-dimensional%20Facial%20Forgery%0A%20%20Detection%20--%20The%202024%20Global%20Deepfake%20Image%20Detection%20Challenge&entry.906535625=Kohou%20Wang%20and%20Huan%20Hu%20and%20Xiang%20Liu%20and%20Zezhou%20Chen%20and%20Ping%20Chen%20and%20Zhaoxiang%20Liu%20and%20Shiguo%20Lian&entry.1292438233=%20%20The%20proliferation%20of%20sophisticated%20deepfake%20technology%20poses%20significant%0Achallenges%20to%20digital%20security%20and%20authenticity.%20Detecting%20these%20forgeries%2C%0Aespecially%20across%20a%20wide%20spectrum%20of%20manipulation%20techniques%2C%20requires%20robust%0Aand%20generalized%20models.%20This%20paper%20introduces%20the%20Hierarchical%20Deep%20Fusion%0AFramework%20%28HDFF%29%2C%20an%20ensemble-based%20deep%20learning%20architecture%20designed%20for%0Ahigh-performance%20facial%20forgery%20detection.%20Our%20framework%20integrates%20four%0Adiverse%20pre-trained%20sub-models%2C%20Swin-MLP%2C%20CoAtNet%2C%20EfficientNetV2%2C%20and%20DaViT%2C%0Awhich%20are%20meticulously%20fine-tuned%20through%20a%20multi-stage%20process%20on%20the%0AMultiFFDI%20dataset.%20By%20concatenating%20the%20feature%20representations%20from%20these%0Aspecialized%20models%20and%20training%20a%20final%20classifier%20layer%2C%20HDFF%20effectively%0Aleverages%20their%20collective%20strengths.%20This%20approach%20achieved%20a%20final%20score%20of%0A0.96852%20on%20the%20competition%27s%20private%20leaderboard%2C%20securing%20the%2020th%20position%0Aout%20of%20184%20teams%2C%20demonstrating%20the%20efficacy%20of%20hierarchical%20fusion%20for%20complex%0Aimage%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13107v1&entry.124074799=Read"},
{"title": "Multi-Model Synthetic Training for Mission-Critical Small Language\n  Models", "author": "Nolan Platt and Pragyansmita Nayak", "abstract": "  Large Language Models (LLMs) have demonstrated remarkable capabilities across\nmany domains, yet their application to specialized fields remains constrained\nby the scarcity and complexity of domain-specific training data. We present a\nnovel approach that achieves a 261x cost reduction for maritime intelligence by\nusing LLMs as one-time teachers rather than using them directly for inference.\nOur method transforms 3.2 billion Automatic Identification System (AIS) vessel\ntracking records into 21,543 synthetic question and answer pairs through\nmulti-model generation (GPT-4o and o3-mini), preventing overfitting and\nensuring accurate reasoning. The resulting fine-tuned Qwen2.5-7B model achieves\n75% accuracy on maritime tasks, while being substantially cheaper than using a\nlarger model for inference. We show that smaller, cheaper models -- when fine\ntuned properly -- can provide similar accuracy compared to larger models that\nare prohibitively expensive. Our work contributes to the growing field of\nsynthetic dataset generation for specialized AI applications and presents a\nhighly reproducible framework for domains where manual annotation is\ninfeasible. Beyond expanding research in the growing field of specialized small\nlanguage models, our approach has immediate applications in maritime safety,\nsecurity operations, and vessel traffic management systems in various\nindustries.\n", "link": "http://arxiv.org/abs/2509.13047v1", "date": "2025-09-16", "relevancy": 2.1058, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5356}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5214}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Model%20Synthetic%20Training%20for%20Mission-Critical%20Small%20Language%0A%20%20Models&body=Title%3A%20Multi-Model%20Synthetic%20Training%20for%20Mission-Critical%20Small%20Language%0A%20%20Models%0AAuthor%3A%20Nolan%20Platt%20and%20Pragyansmita%20Nayak%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Amany%20domains%2C%20yet%20their%20application%20to%20specialized%20fields%20remains%20constrained%0Aby%20the%20scarcity%20and%20complexity%20of%20domain-specific%20training%20data.%20We%20present%20a%0Anovel%20approach%20that%20achieves%20a%20261x%20cost%20reduction%20for%20maritime%20intelligence%20by%0Ausing%20LLMs%20as%20one-time%20teachers%20rather%20than%20using%20them%20directly%20for%20inference.%0AOur%20method%20transforms%203.2%20billion%20Automatic%20Identification%20System%20%28AIS%29%20vessel%0Atracking%20records%20into%2021%2C543%20synthetic%20question%20and%20answer%20pairs%20through%0Amulti-model%20generation%20%28GPT-4o%20and%20o3-mini%29%2C%20preventing%20overfitting%20and%0Aensuring%20accurate%20reasoning.%20The%20resulting%20fine-tuned%20Qwen2.5-7B%20model%20achieves%0A75%25%20accuracy%20on%20maritime%20tasks%2C%20while%20being%20substantially%20cheaper%20than%20using%20a%0Alarger%20model%20for%20inference.%20We%20show%20that%20smaller%2C%20cheaper%20models%20--%20when%20fine%0Atuned%20properly%20--%20can%20provide%20similar%20accuracy%20compared%20to%20larger%20models%20that%0Aare%20prohibitively%20expensive.%20Our%20work%20contributes%20to%20the%20growing%20field%20of%0Asynthetic%20dataset%20generation%20for%20specialized%20AI%20applications%20and%20presents%20a%0Ahighly%20reproducible%20framework%20for%20domains%20where%20manual%20annotation%20is%0Ainfeasible.%20Beyond%20expanding%20research%20in%20the%20growing%20field%20of%20specialized%20small%0Alanguage%20models%2C%20our%20approach%20has%20immediate%20applications%20in%20maritime%20safety%2C%0Asecurity%20operations%2C%20and%20vessel%20traffic%20management%20systems%20in%20various%0Aindustries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13047v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Model%2520Synthetic%2520Training%2520for%2520Mission-Critical%2520Small%2520Language%250A%2520%2520Models%26entry.906535625%3DNolan%2520Platt%2520and%2520Pragyansmita%2520Nayak%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520across%250Amany%2520domains%252C%2520yet%2520their%2520application%2520to%2520specialized%2520fields%2520remains%2520constrained%250Aby%2520the%2520scarcity%2520and%2520complexity%2520of%2520domain-specific%2520training%2520data.%2520We%2520present%2520a%250Anovel%2520approach%2520that%2520achieves%2520a%2520261x%2520cost%2520reduction%2520for%2520maritime%2520intelligence%2520by%250Ausing%2520LLMs%2520as%2520one-time%2520teachers%2520rather%2520than%2520using%2520them%2520directly%2520for%2520inference.%250AOur%2520method%2520transforms%25203.2%2520billion%2520Automatic%2520Identification%2520System%2520%2528AIS%2529%2520vessel%250Atracking%2520records%2520into%252021%252C543%2520synthetic%2520question%2520and%2520answer%2520pairs%2520through%250Amulti-model%2520generation%2520%2528GPT-4o%2520and%2520o3-mini%2529%252C%2520preventing%2520overfitting%2520and%250Aensuring%2520accurate%2520reasoning.%2520The%2520resulting%2520fine-tuned%2520Qwen2.5-7B%2520model%2520achieves%250A75%2525%2520accuracy%2520on%2520maritime%2520tasks%252C%2520while%2520being%2520substantially%2520cheaper%2520than%2520using%2520a%250Alarger%2520model%2520for%2520inference.%2520We%2520show%2520that%2520smaller%252C%2520cheaper%2520models%2520--%2520when%2520fine%250Atuned%2520properly%2520--%2520can%2520provide%2520similar%2520accuracy%2520compared%2520to%2520larger%2520models%2520that%250Aare%2520prohibitively%2520expensive.%2520Our%2520work%2520contributes%2520to%2520the%2520growing%2520field%2520of%250Asynthetic%2520dataset%2520generation%2520for%2520specialized%2520AI%2520applications%2520and%2520presents%2520a%250Ahighly%2520reproducible%2520framework%2520for%2520domains%2520where%2520manual%2520annotation%2520is%250Ainfeasible.%2520Beyond%2520expanding%2520research%2520in%2520the%2520growing%2520field%2520of%2520specialized%2520small%250Alanguage%2520models%252C%2520our%2520approach%2520has%2520immediate%2520applications%2520in%2520maritime%2520safety%252C%250Asecurity%2520operations%252C%2520and%2520vessel%2520traffic%2520management%2520systems%2520in%2520various%250Aindustries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13047v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Model%20Synthetic%20Training%20for%20Mission-Critical%20Small%20Language%0A%20%20Models&entry.906535625=Nolan%20Platt%20and%20Pragyansmita%20Nayak&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Amany%20domains%2C%20yet%20their%20application%20to%20specialized%20fields%20remains%20constrained%0Aby%20the%20scarcity%20and%20complexity%20of%20domain-specific%20training%20data.%20We%20present%20a%0Anovel%20approach%20that%20achieves%20a%20261x%20cost%20reduction%20for%20maritime%20intelligence%20by%0Ausing%20LLMs%20as%20one-time%20teachers%20rather%20than%20using%20them%20directly%20for%20inference.%0AOur%20method%20transforms%203.2%20billion%20Automatic%20Identification%20System%20%28AIS%29%20vessel%0Atracking%20records%20into%2021%2C543%20synthetic%20question%20and%20answer%20pairs%20through%0Amulti-model%20generation%20%28GPT-4o%20and%20o3-mini%29%2C%20preventing%20overfitting%20and%0Aensuring%20accurate%20reasoning.%20The%20resulting%20fine-tuned%20Qwen2.5-7B%20model%20achieves%0A75%25%20accuracy%20on%20maritime%20tasks%2C%20while%20being%20substantially%20cheaper%20than%20using%20a%0Alarger%20model%20for%20inference.%20We%20show%20that%20smaller%2C%20cheaper%20models%20--%20when%20fine%0Atuned%20properly%20--%20can%20provide%20similar%20accuracy%20compared%20to%20larger%20models%20that%0Aare%20prohibitively%20expensive.%20Our%20work%20contributes%20to%20the%20growing%20field%20of%0Asynthetic%20dataset%20generation%20for%20specialized%20AI%20applications%20and%20presents%20a%0Ahighly%20reproducible%20framework%20for%20domains%20where%20manual%20annotation%20is%0Ainfeasible.%20Beyond%20expanding%20research%20in%20the%20growing%20field%20of%20specialized%20small%0Alanguage%20models%2C%20our%20approach%20has%20immediate%20applications%20in%20maritime%20safety%2C%0Asecurity%20operations%2C%20and%20vessel%20traffic%20management%20systems%20in%20various%0Aindustries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13047v1&entry.124074799=Read"},
{"title": "Dual-Stage Reweighted MoE for Long-Tailed Egocentric Mistake Detection", "author": "Boyu Han and Qianqian Xu and Shilong Bao and Zhiyong Yang and Sicong Li and Qingming Huang", "abstract": "  In this report, we address the problem of determining whether a user performs\nan action incorrectly from egocentric video data. To handle the challenges\nposed by subtle and infrequent mistakes, we propose a Dual-Stage Reweighted\nMixture-of-Experts (DR-MoE) framework. In the first stage, features are\nextracted using a frozen ViViT model and a LoRA-tuned ViViT model, which are\ncombined through a feature-level expert module. In the second stage, three\nclassifiers are trained with different objectives: reweighted cross-entropy to\nmitigate class imbalance, AUC loss to improve ranking under skewed\ndistributions, and label-aware loss with sharpness-aware minimization to\nenhance calibration and generalization. Their predictions are fused using a\nclassification-level expert module. The proposed method achieves strong\nperformance, particularly in identifying rare and ambiguous mistake instances.\nThe code is available at https://github.com/boyuh/DR-MoE.\n", "link": "http://arxiv.org/abs/2509.12990v1", "date": "2025-09-16", "relevancy": 2.1032, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5665}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5382}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4971}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Stage%20Reweighted%20MoE%20for%20Long-Tailed%20Egocentric%20Mistake%20Detection&body=Title%3A%20Dual-Stage%20Reweighted%20MoE%20for%20Long-Tailed%20Egocentric%20Mistake%20Detection%0AAuthor%3A%20Boyu%20Han%20and%20Qianqian%20Xu%20and%20Shilong%20Bao%20and%20Zhiyong%20Yang%20and%20Sicong%20Li%20and%20Qingming%20Huang%0AAbstract%3A%20%20%20In%20this%20report%2C%20we%20address%20the%20problem%20of%20determining%20whether%20a%20user%20performs%0Aan%20action%20incorrectly%20from%20egocentric%20video%20data.%20To%20handle%20the%20challenges%0Aposed%20by%20subtle%20and%20infrequent%20mistakes%2C%20we%20propose%20a%20Dual-Stage%20Reweighted%0AMixture-of-Experts%20%28DR-MoE%29%20framework.%20In%20the%20first%20stage%2C%20features%20are%0Aextracted%20using%20a%20frozen%20ViViT%20model%20and%20a%20LoRA-tuned%20ViViT%20model%2C%20which%20are%0Acombined%20through%20a%20feature-level%20expert%20module.%20In%20the%20second%20stage%2C%20three%0Aclassifiers%20are%20trained%20with%20different%20objectives%3A%20reweighted%20cross-entropy%20to%0Amitigate%20class%20imbalance%2C%20AUC%20loss%20to%20improve%20ranking%20under%20skewed%0Adistributions%2C%20and%20label-aware%20loss%20with%20sharpness-aware%20minimization%20to%0Aenhance%20calibration%20and%20generalization.%20Their%20predictions%20are%20fused%20using%20a%0Aclassification-level%20expert%20module.%20The%20proposed%20method%20achieves%20strong%0Aperformance%2C%20particularly%20in%20identifying%20rare%20and%20ambiguous%20mistake%20instances.%0AThe%20code%20is%20available%20at%20https%3A//github.com/boyuh/DR-MoE.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Stage%2520Reweighted%2520MoE%2520for%2520Long-Tailed%2520Egocentric%2520Mistake%2520Detection%26entry.906535625%3DBoyu%2520Han%2520and%2520Qianqian%2520Xu%2520and%2520Shilong%2520Bao%2520and%2520Zhiyong%2520Yang%2520and%2520Sicong%2520Li%2520and%2520Qingming%2520Huang%26entry.1292438233%3D%2520%2520In%2520this%2520report%252C%2520we%2520address%2520the%2520problem%2520of%2520determining%2520whether%2520a%2520user%2520performs%250Aan%2520action%2520incorrectly%2520from%2520egocentric%2520video%2520data.%2520To%2520handle%2520the%2520challenges%250Aposed%2520by%2520subtle%2520and%2520infrequent%2520mistakes%252C%2520we%2520propose%2520a%2520Dual-Stage%2520Reweighted%250AMixture-of-Experts%2520%2528DR-MoE%2529%2520framework.%2520In%2520the%2520first%2520stage%252C%2520features%2520are%250Aextracted%2520using%2520a%2520frozen%2520ViViT%2520model%2520and%2520a%2520LoRA-tuned%2520ViViT%2520model%252C%2520which%2520are%250Acombined%2520through%2520a%2520feature-level%2520expert%2520module.%2520In%2520the%2520second%2520stage%252C%2520three%250Aclassifiers%2520are%2520trained%2520with%2520different%2520objectives%253A%2520reweighted%2520cross-entropy%2520to%250Amitigate%2520class%2520imbalance%252C%2520AUC%2520loss%2520to%2520improve%2520ranking%2520under%2520skewed%250Adistributions%252C%2520and%2520label-aware%2520loss%2520with%2520sharpness-aware%2520minimization%2520to%250Aenhance%2520calibration%2520and%2520generalization.%2520Their%2520predictions%2520are%2520fused%2520using%2520a%250Aclassification-level%2520expert%2520module.%2520The%2520proposed%2520method%2520achieves%2520strong%250Aperformance%252C%2520particularly%2520in%2520identifying%2520rare%2520and%2520ambiguous%2520mistake%2520instances.%250AThe%2520code%2520is%2520available%2520at%2520https%253A//github.com/boyuh/DR-MoE.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Stage%20Reweighted%20MoE%20for%20Long-Tailed%20Egocentric%20Mistake%20Detection&entry.906535625=Boyu%20Han%20and%20Qianqian%20Xu%20and%20Shilong%20Bao%20and%20Zhiyong%20Yang%20and%20Sicong%20Li%20and%20Qingming%20Huang&entry.1292438233=%20%20In%20this%20report%2C%20we%20address%20the%20problem%20of%20determining%20whether%20a%20user%20performs%0Aan%20action%20incorrectly%20from%20egocentric%20video%20data.%20To%20handle%20the%20challenges%0Aposed%20by%20subtle%20and%20infrequent%20mistakes%2C%20we%20propose%20a%20Dual-Stage%20Reweighted%0AMixture-of-Experts%20%28DR-MoE%29%20framework.%20In%20the%20first%20stage%2C%20features%20are%0Aextracted%20using%20a%20frozen%20ViViT%20model%20and%20a%20LoRA-tuned%20ViViT%20model%2C%20which%20are%0Acombined%20through%20a%20feature-level%20expert%20module.%20In%20the%20second%20stage%2C%20three%0Aclassifiers%20are%20trained%20with%20different%20objectives%3A%20reweighted%20cross-entropy%20to%0Amitigate%20class%20imbalance%2C%20AUC%20loss%20to%20improve%20ranking%20under%20skewed%0Adistributions%2C%20and%20label-aware%20loss%20with%20sharpness-aware%20minimization%20to%0Aenhance%20calibration%20and%20generalization.%20Their%20predictions%20are%20fused%20using%20a%0Aclassification-level%20expert%20module.%20The%20proposed%20method%20achieves%20strong%0Aperformance%2C%20particularly%20in%20identifying%20rare%20and%20ambiguous%20mistake%20instances.%0AThe%20code%20is%20available%20at%20https%3A//github.com/boyuh/DR-MoE.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12990v1&entry.124074799=Read"},
{"title": "B-TGAT: A Bi-directional Temporal Graph Attention Transformer for\n  Clustering Multivariate Spatiotemporal Data", "author": "Francis Ndikum Nji and Vandana Janaja and Jianwu Wang", "abstract": "  Clustering high-dimensional multivariate spatiotemporal climate data is\nchallenging due to complex temporal dependencies, evolving spatial\ninteractions, and non-stationary dynamics. Conventional clustering methods,\nincluding recurrent and convolutional models, often struggle to capture both\nlocal and global temporal relationships while preserving spatial context. We\npresent a time-distributed hybrid U-Net autoencoder that integrates a\nBi-directional Temporal Graph Attention Transformer (B-TGAT) to guide efficient\ntemporal clustering of multidimensional spatiotemporal climate datasets. The\nencoder and decoder are equipped with ConvLSTM2D modules that extract joint\nspatial--temporal features by modeling localized dynamics and spatial\ncorrelations over time, and skip connections that preserve multiscale spatial\ndetails during feature compression and reconstruction. At the bottleneck,\nB-TGAT integrates graph-based spatial modeling with attention-driven temporal\nencoding, enabling adaptive weighting of temporal neighbors and capturing both\nshort and long-range dependencies across regions. This architecture produces\ndiscriminative latent embeddings optimized for clustering. Experiments on three\ndistinct spatiotemporal climate datasets demonstrate superior cluster\nseparability, temporal stability, and alignment with known climate transitions\ncompared to state-of-the-art baselines. The integration of ConvLSTM2D, U-Net\nskip connections, and B-TGAT enhances temporal clustering performance while\nproviding interpretable insights into complex spatiotemporal variability,\nadvancing both methodological development and climate science applications.\n", "link": "http://arxiv.org/abs/2509.13202v1", "date": "2025-09-16", "relevancy": 2.0955, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5512}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.511}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20B-TGAT%3A%20A%20Bi-directional%20Temporal%20Graph%20Attention%20Transformer%20for%0A%20%20Clustering%20Multivariate%20Spatiotemporal%20Data&body=Title%3A%20B-TGAT%3A%20A%20Bi-directional%20Temporal%20Graph%20Attention%20Transformer%20for%0A%20%20Clustering%20Multivariate%20Spatiotemporal%20Data%0AAuthor%3A%20Francis%20Ndikum%20Nji%20and%20Vandana%20Janaja%20and%20Jianwu%20Wang%0AAbstract%3A%20%20%20Clustering%20high-dimensional%20multivariate%20spatiotemporal%20climate%20data%20is%0Achallenging%20due%20to%20complex%20temporal%20dependencies%2C%20evolving%20spatial%0Ainteractions%2C%20and%20non-stationary%20dynamics.%20Conventional%20clustering%20methods%2C%0Aincluding%20recurrent%20and%20convolutional%20models%2C%20often%20struggle%20to%20capture%20both%0Alocal%20and%20global%20temporal%20relationships%20while%20preserving%20spatial%20context.%20We%0Apresent%20a%20time-distributed%20hybrid%20U-Net%20autoencoder%20that%20integrates%20a%0ABi-directional%20Temporal%20Graph%20Attention%20Transformer%20%28B-TGAT%29%20to%20guide%20efficient%0Atemporal%20clustering%20of%20multidimensional%20spatiotemporal%20climate%20datasets.%20The%0Aencoder%20and%20decoder%20are%20equipped%20with%20ConvLSTM2D%20modules%20that%20extract%20joint%0Aspatial--temporal%20features%20by%20modeling%20localized%20dynamics%20and%20spatial%0Acorrelations%20over%20time%2C%20and%20skip%20connections%20that%20preserve%20multiscale%20spatial%0Adetails%20during%20feature%20compression%20and%20reconstruction.%20At%20the%20bottleneck%2C%0AB-TGAT%20integrates%20graph-based%20spatial%20modeling%20with%20attention-driven%20temporal%0Aencoding%2C%20enabling%20adaptive%20weighting%20of%20temporal%20neighbors%20and%20capturing%20both%0Ashort%20and%20long-range%20dependencies%20across%20regions.%20This%20architecture%20produces%0Adiscriminative%20latent%20embeddings%20optimized%20for%20clustering.%20Experiments%20on%20three%0Adistinct%20spatiotemporal%20climate%20datasets%20demonstrate%20superior%20cluster%0Aseparability%2C%20temporal%20stability%2C%20and%20alignment%20with%20known%20climate%20transitions%0Acompared%20to%20state-of-the-art%20baselines.%20The%20integration%20of%20ConvLSTM2D%2C%20U-Net%0Askip%20connections%2C%20and%20B-TGAT%20enhances%20temporal%20clustering%20performance%20while%0Aproviding%20interpretable%20insights%20into%20complex%20spatiotemporal%20variability%2C%0Aadvancing%20both%20methodological%20development%20and%20climate%20science%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13202v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DB-TGAT%253A%2520A%2520Bi-directional%2520Temporal%2520Graph%2520Attention%2520Transformer%2520for%250A%2520%2520Clustering%2520Multivariate%2520Spatiotemporal%2520Data%26entry.906535625%3DFrancis%2520Ndikum%2520Nji%2520and%2520Vandana%2520Janaja%2520and%2520Jianwu%2520Wang%26entry.1292438233%3D%2520%2520Clustering%2520high-dimensional%2520multivariate%2520spatiotemporal%2520climate%2520data%2520is%250Achallenging%2520due%2520to%2520complex%2520temporal%2520dependencies%252C%2520evolving%2520spatial%250Ainteractions%252C%2520and%2520non-stationary%2520dynamics.%2520Conventional%2520clustering%2520methods%252C%250Aincluding%2520recurrent%2520and%2520convolutional%2520models%252C%2520often%2520struggle%2520to%2520capture%2520both%250Alocal%2520and%2520global%2520temporal%2520relationships%2520while%2520preserving%2520spatial%2520context.%2520We%250Apresent%2520a%2520time-distributed%2520hybrid%2520U-Net%2520autoencoder%2520that%2520integrates%2520a%250ABi-directional%2520Temporal%2520Graph%2520Attention%2520Transformer%2520%2528B-TGAT%2529%2520to%2520guide%2520efficient%250Atemporal%2520clustering%2520of%2520multidimensional%2520spatiotemporal%2520climate%2520datasets.%2520The%250Aencoder%2520and%2520decoder%2520are%2520equipped%2520with%2520ConvLSTM2D%2520modules%2520that%2520extract%2520joint%250Aspatial--temporal%2520features%2520by%2520modeling%2520localized%2520dynamics%2520and%2520spatial%250Acorrelations%2520over%2520time%252C%2520and%2520skip%2520connections%2520that%2520preserve%2520multiscale%2520spatial%250Adetails%2520during%2520feature%2520compression%2520and%2520reconstruction.%2520At%2520the%2520bottleneck%252C%250AB-TGAT%2520integrates%2520graph-based%2520spatial%2520modeling%2520with%2520attention-driven%2520temporal%250Aencoding%252C%2520enabling%2520adaptive%2520weighting%2520of%2520temporal%2520neighbors%2520and%2520capturing%2520both%250Ashort%2520and%2520long-range%2520dependencies%2520across%2520regions.%2520This%2520architecture%2520produces%250Adiscriminative%2520latent%2520embeddings%2520optimized%2520for%2520clustering.%2520Experiments%2520on%2520three%250Adistinct%2520spatiotemporal%2520climate%2520datasets%2520demonstrate%2520superior%2520cluster%250Aseparability%252C%2520temporal%2520stability%252C%2520and%2520alignment%2520with%2520known%2520climate%2520transitions%250Acompared%2520to%2520state-of-the-art%2520baselines.%2520The%2520integration%2520of%2520ConvLSTM2D%252C%2520U-Net%250Askip%2520connections%252C%2520and%2520B-TGAT%2520enhances%2520temporal%2520clustering%2520performance%2520while%250Aproviding%2520interpretable%2520insights%2520into%2520complex%2520spatiotemporal%2520variability%252C%250Aadvancing%2520both%2520methodological%2520development%2520and%2520climate%2520science%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13202v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=B-TGAT%3A%20A%20Bi-directional%20Temporal%20Graph%20Attention%20Transformer%20for%0A%20%20Clustering%20Multivariate%20Spatiotemporal%20Data&entry.906535625=Francis%20Ndikum%20Nji%20and%20Vandana%20Janaja%20and%20Jianwu%20Wang&entry.1292438233=%20%20Clustering%20high-dimensional%20multivariate%20spatiotemporal%20climate%20data%20is%0Achallenging%20due%20to%20complex%20temporal%20dependencies%2C%20evolving%20spatial%0Ainteractions%2C%20and%20non-stationary%20dynamics.%20Conventional%20clustering%20methods%2C%0Aincluding%20recurrent%20and%20convolutional%20models%2C%20often%20struggle%20to%20capture%20both%0Alocal%20and%20global%20temporal%20relationships%20while%20preserving%20spatial%20context.%20We%0Apresent%20a%20time-distributed%20hybrid%20U-Net%20autoencoder%20that%20integrates%20a%0ABi-directional%20Temporal%20Graph%20Attention%20Transformer%20%28B-TGAT%29%20to%20guide%20efficient%0Atemporal%20clustering%20of%20multidimensional%20spatiotemporal%20climate%20datasets.%20The%0Aencoder%20and%20decoder%20are%20equipped%20with%20ConvLSTM2D%20modules%20that%20extract%20joint%0Aspatial--temporal%20features%20by%20modeling%20localized%20dynamics%20and%20spatial%0Acorrelations%20over%20time%2C%20and%20skip%20connections%20that%20preserve%20multiscale%20spatial%0Adetails%20during%20feature%20compression%20and%20reconstruction.%20At%20the%20bottleneck%2C%0AB-TGAT%20integrates%20graph-based%20spatial%20modeling%20with%20attention-driven%20temporal%0Aencoding%2C%20enabling%20adaptive%20weighting%20of%20temporal%20neighbors%20and%20capturing%20both%0Ashort%20and%20long-range%20dependencies%20across%20regions.%20This%20architecture%20produces%0Adiscriminative%20latent%20embeddings%20optimized%20for%20clustering.%20Experiments%20on%20three%0Adistinct%20spatiotemporal%20climate%20datasets%20demonstrate%20superior%20cluster%0Aseparability%2C%20temporal%20stability%2C%20and%20alignment%20with%20known%20climate%20transitions%0Acompared%20to%20state-of-the-art%20baselines.%20The%20integration%20of%20ConvLSTM2D%2C%20U-Net%0Askip%20connections%2C%20and%20B-TGAT%20enhances%20temporal%20clustering%20performance%20while%0Aproviding%20interpretable%20insights%20into%20complex%20spatiotemporal%20variability%2C%0Aadvancing%20both%20methodological%20development%20and%20climate%20science%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13202v1&entry.124074799=Read"},
{"title": "HoloDx: Knowledge- and Data-Driven Multimodal Diagnosis of Alzheimer's\n  Disease", "author": "Qiuhui Chen and Jintao Wang and Gang Wang and Yi Hong", "abstract": "  Accurate diagnosis of Alzheimer's disease (AD) requires effectively\nintegrating multimodal data and clinical expertise. However, existing methods\noften struggle to fully utilize multimodal information and lack structured\nmechanisms to incorporate dynamic domain knowledge. To address these\nlimitations, we propose HoloDx, a knowledge- and data-driven framework that\nenhances AD diagnosis by aligning domain knowledge with multimodal clinical\ndata. HoloDx incorporates a knowledge injection module with a knowledge-aware\ngated cross-attention, allowing the model to dynamically integrate\ndomain-specific insights from both large language models (LLMs) and clinical\nexpertise. Also, a memory injection module with a designed prototypical memory\nattention enables the model to retain and retrieve subject-specific\ninformation, ensuring consistency in decision-making. By jointly leveraging\nthese mechanisms, HoloDx enhances interpretability, improves robustness, and\neffectively aligns prior knowledge with current subject data. Evaluations on\nfive AD datasets demonstrate that HoloDx outperforms state-of-the-art methods,\nachieving superior diagnostic accuracy and strong generalization across diverse\ncohorts. The source code will be released upon publication acceptance.\n", "link": "http://arxiv.org/abs/2504.19075v2", "date": "2025-09-16", "relevancy": 2.085, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5378}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5181}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5059}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoloDx%3A%20Knowledge-%20and%20Data-Driven%20Multimodal%20Diagnosis%20of%20Alzheimer%27s%0A%20%20Disease&body=Title%3A%20HoloDx%3A%20Knowledge-%20and%20Data-Driven%20Multimodal%20Diagnosis%20of%20Alzheimer%27s%0A%20%20Disease%0AAuthor%3A%20Qiuhui%20Chen%20and%20Jintao%20Wang%20and%20Gang%20Wang%20and%20Yi%20Hong%0AAbstract%3A%20%20%20Accurate%20diagnosis%20of%20Alzheimer%27s%20disease%20%28AD%29%20requires%20effectively%0Aintegrating%20multimodal%20data%20and%20clinical%20expertise.%20However%2C%20existing%20methods%0Aoften%20struggle%20to%20fully%20utilize%20multimodal%20information%20and%20lack%20structured%0Amechanisms%20to%20incorporate%20dynamic%20domain%20knowledge.%20To%20address%20these%0Alimitations%2C%20we%20propose%20HoloDx%2C%20a%20knowledge-%20and%20data-driven%20framework%20that%0Aenhances%20AD%20diagnosis%20by%20aligning%20domain%20knowledge%20with%20multimodal%20clinical%0Adata.%20HoloDx%20incorporates%20a%20knowledge%20injection%20module%20with%20a%20knowledge-aware%0Agated%20cross-attention%2C%20allowing%20the%20model%20to%20dynamically%20integrate%0Adomain-specific%20insights%20from%20both%20large%20language%20models%20%28LLMs%29%20and%20clinical%0Aexpertise.%20Also%2C%20a%20memory%20injection%20module%20with%20a%20designed%20prototypical%20memory%0Aattention%20enables%20the%20model%20to%20retain%20and%20retrieve%20subject-specific%0Ainformation%2C%20ensuring%20consistency%20in%20decision-making.%20By%20jointly%20leveraging%0Athese%20mechanisms%2C%20HoloDx%20enhances%20interpretability%2C%20improves%20robustness%2C%20and%0Aeffectively%20aligns%20prior%20knowledge%20with%20current%20subject%20data.%20Evaluations%20on%0Afive%20AD%20datasets%20demonstrate%20that%20HoloDx%20outperforms%20state-of-the-art%20methods%2C%0Aachieving%20superior%20diagnostic%20accuracy%20and%20strong%20generalization%20across%20diverse%0Acohorts.%20The%20source%20code%20will%20be%20released%20upon%20publication%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.19075v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoloDx%253A%2520Knowledge-%2520and%2520Data-Driven%2520Multimodal%2520Diagnosis%2520of%2520Alzheimer%2527s%250A%2520%2520Disease%26entry.906535625%3DQiuhui%2520Chen%2520and%2520Jintao%2520Wang%2520and%2520Gang%2520Wang%2520and%2520Yi%2520Hong%26entry.1292438233%3D%2520%2520Accurate%2520diagnosis%2520of%2520Alzheimer%2527s%2520disease%2520%2528AD%2529%2520requires%2520effectively%250Aintegrating%2520multimodal%2520data%2520and%2520clinical%2520expertise.%2520However%252C%2520existing%2520methods%250Aoften%2520struggle%2520to%2520fully%2520utilize%2520multimodal%2520information%2520and%2520lack%2520structured%250Amechanisms%2520to%2520incorporate%2520dynamic%2520domain%2520knowledge.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520HoloDx%252C%2520a%2520knowledge-%2520and%2520data-driven%2520framework%2520that%250Aenhances%2520AD%2520diagnosis%2520by%2520aligning%2520domain%2520knowledge%2520with%2520multimodal%2520clinical%250Adata.%2520HoloDx%2520incorporates%2520a%2520knowledge%2520injection%2520module%2520with%2520a%2520knowledge-aware%250Agated%2520cross-attention%252C%2520allowing%2520the%2520model%2520to%2520dynamically%2520integrate%250Adomain-specific%2520insights%2520from%2520both%2520large%2520language%2520models%2520%2528LLMs%2529%2520and%2520clinical%250Aexpertise.%2520Also%252C%2520a%2520memory%2520injection%2520module%2520with%2520a%2520designed%2520prototypical%2520memory%250Aattention%2520enables%2520the%2520model%2520to%2520retain%2520and%2520retrieve%2520subject-specific%250Ainformation%252C%2520ensuring%2520consistency%2520in%2520decision-making.%2520By%2520jointly%2520leveraging%250Athese%2520mechanisms%252C%2520HoloDx%2520enhances%2520interpretability%252C%2520improves%2520robustness%252C%2520and%250Aeffectively%2520aligns%2520prior%2520knowledge%2520with%2520current%2520subject%2520data.%2520Evaluations%2520on%250Afive%2520AD%2520datasets%2520demonstrate%2520that%2520HoloDx%2520outperforms%2520state-of-the-art%2520methods%252C%250Aachieving%2520superior%2520diagnostic%2520accuracy%2520and%2520strong%2520generalization%2520across%2520diverse%250Acohorts.%2520The%2520source%2520code%2520will%2520be%2520released%2520upon%2520publication%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.19075v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoloDx%3A%20Knowledge-%20and%20Data-Driven%20Multimodal%20Diagnosis%20of%20Alzheimer%27s%0A%20%20Disease&entry.906535625=Qiuhui%20Chen%20and%20Jintao%20Wang%20and%20Gang%20Wang%20and%20Yi%20Hong&entry.1292438233=%20%20Accurate%20diagnosis%20of%20Alzheimer%27s%20disease%20%28AD%29%20requires%20effectively%0Aintegrating%20multimodal%20data%20and%20clinical%20expertise.%20However%2C%20existing%20methods%0Aoften%20struggle%20to%20fully%20utilize%20multimodal%20information%20and%20lack%20structured%0Amechanisms%20to%20incorporate%20dynamic%20domain%20knowledge.%20To%20address%20these%0Alimitations%2C%20we%20propose%20HoloDx%2C%20a%20knowledge-%20and%20data-driven%20framework%20that%0Aenhances%20AD%20diagnosis%20by%20aligning%20domain%20knowledge%20with%20multimodal%20clinical%0Adata.%20HoloDx%20incorporates%20a%20knowledge%20injection%20module%20with%20a%20knowledge-aware%0Agated%20cross-attention%2C%20allowing%20the%20model%20to%20dynamically%20integrate%0Adomain-specific%20insights%20from%20both%20large%20language%20models%20%28LLMs%29%20and%20clinical%0Aexpertise.%20Also%2C%20a%20memory%20injection%20module%20with%20a%20designed%20prototypical%20memory%0Aattention%20enables%20the%20model%20to%20retain%20and%20retrieve%20subject-specific%0Ainformation%2C%20ensuring%20consistency%20in%20decision-making.%20By%20jointly%20leveraging%0Athese%20mechanisms%2C%20HoloDx%20enhances%20interpretability%2C%20improves%20robustness%2C%20and%0Aeffectively%20aligns%20prior%20knowledge%20with%20current%20subject%20data.%20Evaluations%20on%0Afive%20AD%20datasets%20demonstrate%20that%20HoloDx%20outperforms%20state-of-the-art%20methods%2C%0Aachieving%20superior%20diagnostic%20accuracy%20and%20strong%20generalization%20across%20diverse%0Acohorts.%20The%20source%20code%20will%20be%20released%20upon%20publication%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.19075v2&entry.124074799=Read"},
{"title": "AI Governance in Higher Education: A course design exploring regulatory,\n  ethical and practical considerations", "author": "Rapha\u00ebl Weuts and Johannes Bleher and Hannah Bleher and Rozanne Tuesday Flores and Guo Xuanyang and Pawe\u0142 Pujszo and Zsolt Alm\u00e1si", "abstract": "  As artificial intelligence (AI) systems permeate critical sectors, the need\nfor professionals who can address ethical, legal and governance challenges has\nbecome urgent. Current AI ethics education remains fragmented, often siloed by\ndiscipline and disconnected from practice. This paper synthesizes literature\nand regulatory developments to propose a modular, interdisciplinary curriculum\nthat integrates technical foundations with ethics, law and policy. We highlight\nrecurring operational failures in AI - bias, misspecified objectives,\ngeneralization errors, misuse and governance breakdowns - and link them to\npedagogical strategies for teaching AI governance. Drawing on perspectives from\nthe EU, China and international frameworks, we outline a semester plan that\nemphasizes integrated ethics, stakeholder engagement and experiential learning.\nThe curriculum aims to prepare students to diagnose risks, navigate regulation\nand engage diverse stakeholders, fostering adaptive and ethically grounded\nprofessionals for responsible AI governance.\n", "link": "http://arxiv.org/abs/2509.06176v2", "date": "2025-09-16", "relevancy": 2.0846, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4296}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4117}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI%20Governance%20in%20Higher%20Education%3A%20A%20course%20design%20exploring%20regulatory%2C%0A%20%20ethical%20and%20practical%20considerations&body=Title%3A%20AI%20Governance%20in%20Higher%20Education%3A%20A%20course%20design%20exploring%20regulatory%2C%0A%20%20ethical%20and%20practical%20considerations%0AAuthor%3A%20Rapha%C3%ABl%20Weuts%20and%20Johannes%20Bleher%20and%20Hannah%20Bleher%20and%20Rozanne%20Tuesday%20Flores%20and%20Guo%20Xuanyang%20and%20Pawe%C5%82%20Pujszo%20and%20Zsolt%20Alm%C3%A1si%0AAbstract%3A%20%20%20As%20artificial%20intelligence%20%28AI%29%20systems%20permeate%20critical%20sectors%2C%20the%20need%0Afor%20professionals%20who%20can%20address%20ethical%2C%20legal%20and%20governance%20challenges%20has%0Abecome%20urgent.%20Current%20AI%20ethics%20education%20remains%20fragmented%2C%20often%20siloed%20by%0Adiscipline%20and%20disconnected%20from%20practice.%20This%20paper%20synthesizes%20literature%0Aand%20regulatory%20developments%20to%20propose%20a%20modular%2C%20interdisciplinary%20curriculum%0Athat%20integrates%20technical%20foundations%20with%20ethics%2C%20law%20and%20policy.%20We%20highlight%0Arecurring%20operational%20failures%20in%20AI%20-%20bias%2C%20misspecified%20objectives%2C%0Ageneralization%20errors%2C%20misuse%20and%20governance%20breakdowns%20-%20and%20link%20them%20to%0Apedagogical%20strategies%20for%20teaching%20AI%20governance.%20Drawing%20on%20perspectives%20from%0Athe%20EU%2C%20China%20and%20international%20frameworks%2C%20we%20outline%20a%20semester%20plan%20that%0Aemphasizes%20integrated%20ethics%2C%20stakeholder%20engagement%20and%20experiential%20learning.%0AThe%20curriculum%20aims%20to%20prepare%20students%20to%20diagnose%20risks%2C%20navigate%20regulation%0Aand%20engage%20diverse%20stakeholders%2C%20fostering%20adaptive%20and%20ethically%20grounded%0Aprofessionals%20for%20responsible%20AI%20governance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.06176v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI%2520Governance%2520in%2520Higher%2520Education%253A%2520A%2520course%2520design%2520exploring%2520regulatory%252C%250A%2520%2520ethical%2520and%2520practical%2520considerations%26entry.906535625%3DRapha%25C3%25ABl%2520Weuts%2520and%2520Johannes%2520Bleher%2520and%2520Hannah%2520Bleher%2520and%2520Rozanne%2520Tuesday%2520Flores%2520and%2520Guo%2520Xuanyang%2520and%2520Pawe%25C5%2582%2520Pujszo%2520and%2520Zsolt%2520Alm%25C3%25A1si%26entry.1292438233%3D%2520%2520As%2520artificial%2520intelligence%2520%2528AI%2529%2520systems%2520permeate%2520critical%2520sectors%252C%2520the%2520need%250Afor%2520professionals%2520who%2520can%2520address%2520ethical%252C%2520legal%2520and%2520governance%2520challenges%2520has%250Abecome%2520urgent.%2520Current%2520AI%2520ethics%2520education%2520remains%2520fragmented%252C%2520often%2520siloed%2520by%250Adiscipline%2520and%2520disconnected%2520from%2520practice.%2520This%2520paper%2520synthesizes%2520literature%250Aand%2520regulatory%2520developments%2520to%2520propose%2520a%2520modular%252C%2520interdisciplinary%2520curriculum%250Athat%2520integrates%2520technical%2520foundations%2520with%2520ethics%252C%2520law%2520and%2520policy.%2520We%2520highlight%250Arecurring%2520operational%2520failures%2520in%2520AI%2520-%2520bias%252C%2520misspecified%2520objectives%252C%250Ageneralization%2520errors%252C%2520misuse%2520and%2520governance%2520breakdowns%2520-%2520and%2520link%2520them%2520to%250Apedagogical%2520strategies%2520for%2520teaching%2520AI%2520governance.%2520Drawing%2520on%2520perspectives%2520from%250Athe%2520EU%252C%2520China%2520and%2520international%2520frameworks%252C%2520we%2520outline%2520a%2520semester%2520plan%2520that%250Aemphasizes%2520integrated%2520ethics%252C%2520stakeholder%2520engagement%2520and%2520experiential%2520learning.%250AThe%2520curriculum%2520aims%2520to%2520prepare%2520students%2520to%2520diagnose%2520risks%252C%2520navigate%2520regulation%250Aand%2520engage%2520diverse%2520stakeholders%252C%2520fostering%2520adaptive%2520and%2520ethically%2520grounded%250Aprofessionals%2520for%2520responsible%2520AI%2520governance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.06176v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI%20Governance%20in%20Higher%20Education%3A%20A%20course%20design%20exploring%20regulatory%2C%0A%20%20ethical%20and%20practical%20considerations&entry.906535625=Rapha%C3%ABl%20Weuts%20and%20Johannes%20Bleher%20and%20Hannah%20Bleher%20and%20Rozanne%20Tuesday%20Flores%20and%20Guo%20Xuanyang%20and%20Pawe%C5%82%20Pujszo%20and%20Zsolt%20Alm%C3%A1si&entry.1292438233=%20%20As%20artificial%20intelligence%20%28AI%29%20systems%20permeate%20critical%20sectors%2C%20the%20need%0Afor%20professionals%20who%20can%20address%20ethical%2C%20legal%20and%20governance%20challenges%20has%0Abecome%20urgent.%20Current%20AI%20ethics%20education%20remains%20fragmented%2C%20often%20siloed%20by%0Adiscipline%20and%20disconnected%20from%20practice.%20This%20paper%20synthesizes%20literature%0Aand%20regulatory%20developments%20to%20propose%20a%20modular%2C%20interdisciplinary%20curriculum%0Athat%20integrates%20technical%20foundations%20with%20ethics%2C%20law%20and%20policy.%20We%20highlight%0Arecurring%20operational%20failures%20in%20AI%20-%20bias%2C%20misspecified%20objectives%2C%0Ageneralization%20errors%2C%20misuse%20and%20governance%20breakdowns%20-%20and%20link%20them%20to%0Apedagogical%20strategies%20for%20teaching%20AI%20governance.%20Drawing%20on%20perspectives%20from%0Athe%20EU%2C%20China%20and%20international%20frameworks%2C%20we%20outline%20a%20semester%20plan%20that%0Aemphasizes%20integrated%20ethics%2C%20stakeholder%20engagement%20and%20experiential%20learning.%0AThe%20curriculum%20aims%20to%20prepare%20students%20to%20diagnose%20risks%2C%20navigate%20regulation%0Aand%20engage%20diverse%20stakeholders%2C%20fostering%20adaptive%20and%20ethically%20grounded%0Aprofessionals%20for%20responsible%20AI%20governance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.06176v2&entry.124074799=Read"},
{"title": "Black-box Model Merging for Language-Model-as-a-Service with Massive\n  Model Repositories", "author": "Shilian Chen and Jie Zhou and Tianyu Huai and Yujiang Lu and Junsong Li and Bihao Zhan and Qianjun Pan and Yutao Yang and Xin Li and Qin Chen and Hang Yan and Liang He", "abstract": "  Model merging refers to the process of integrating multiple distinct models\ninto a unified model that preserves and combines the strengths and capabilities\nof the individual models. Most existing approaches rely on task vectors to\ncombine models, typically under the assumption that model parameters are\naccessible. However, for extremely large language models (LLMs) such as GPT-4,\nwhich are often provided solely as black-box services through API interfaces\n(Language-Model-as-a-Service), model weights are not available to end users.\nThis presents a significant challenge, which we refer to as black-box model\nmerging (BMM) with massive LLMs. To address this challenge, we propose a\nderivative-free optimization framework based on the evolutionary algorithm\n(Evo-Merging) that enables effective model merging using only inference-time\nAPI queries. Our method consists of two key components: (1) sparsity-based\ndenoising, designed to identify and filter out irrelevant or redundant\ninformation across models, and (2) sign-aware scaling, which dynamically\ncomputes optimal combination weights for the relevant models based on their\nperformance. We also provide a formal justification, along with a theoretical\nanalysis, for our asymmetric sparsification. Extensive experimental evaluations\ndemonstrate that our approach achieves state-of-the-art results on a range of\ntasks, significantly outperforming existing strong baselines.\n", "link": "http://arxiv.org/abs/2509.12951v1", "date": "2025-09-16", "relevancy": 2.0753, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5207}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5207}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Black-box%20Model%20Merging%20for%20Language-Model-as-a-Service%20with%20Massive%0A%20%20Model%20Repositories&body=Title%3A%20Black-box%20Model%20Merging%20for%20Language-Model-as-a-Service%20with%20Massive%0A%20%20Model%20Repositories%0AAuthor%3A%20Shilian%20Chen%20and%20Jie%20Zhou%20and%20Tianyu%20Huai%20and%20Yujiang%20Lu%20and%20Junsong%20Li%20and%20Bihao%20Zhan%20and%20Qianjun%20Pan%20and%20Yutao%20Yang%20and%20Xin%20Li%20and%20Qin%20Chen%20and%20Hang%20Yan%20and%20Liang%20He%0AAbstract%3A%20%20%20Model%20merging%20refers%20to%20the%20process%20of%20integrating%20multiple%20distinct%20models%0Ainto%20a%20unified%20model%20that%20preserves%20and%20combines%20the%20strengths%20and%20capabilities%0Aof%20the%20individual%20models.%20Most%20existing%20approaches%20rely%20on%20task%20vectors%20to%0Acombine%20models%2C%20typically%20under%20the%20assumption%20that%20model%20parameters%20are%0Aaccessible.%20However%2C%20for%20extremely%20large%20language%20models%20%28LLMs%29%20such%20as%20GPT-4%2C%0Awhich%20are%20often%20provided%20solely%20as%20black-box%20services%20through%20API%20interfaces%0A%28Language-Model-as-a-Service%29%2C%20model%20weights%20are%20not%20available%20to%20end%20users.%0AThis%20presents%20a%20significant%20challenge%2C%20which%20we%20refer%20to%20as%20black-box%20model%0Amerging%20%28BMM%29%20with%20massive%20LLMs.%20To%20address%20this%20challenge%2C%20we%20propose%20a%0Aderivative-free%20optimization%20framework%20based%20on%20the%20evolutionary%20algorithm%0A%28Evo-Merging%29%20that%20enables%20effective%20model%20merging%20using%20only%20inference-time%0AAPI%20queries.%20Our%20method%20consists%20of%20two%20key%20components%3A%20%281%29%20sparsity-based%0Adenoising%2C%20designed%20to%20identify%20and%20filter%20out%20irrelevant%20or%20redundant%0Ainformation%20across%20models%2C%20and%20%282%29%20sign-aware%20scaling%2C%20which%20dynamically%0Acomputes%20optimal%20combination%20weights%20for%20the%20relevant%20models%20based%20on%20their%0Aperformance.%20We%20also%20provide%20a%20formal%20justification%2C%20along%20with%20a%20theoretical%0Aanalysis%2C%20for%20our%20asymmetric%20sparsification.%20Extensive%20experimental%20evaluations%0Ademonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20results%20on%20a%20range%20of%0Atasks%2C%20significantly%20outperforming%20existing%20strong%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlack-box%2520Model%2520Merging%2520for%2520Language-Model-as-a-Service%2520with%2520Massive%250A%2520%2520Model%2520Repositories%26entry.906535625%3DShilian%2520Chen%2520and%2520Jie%2520Zhou%2520and%2520Tianyu%2520Huai%2520and%2520Yujiang%2520Lu%2520and%2520Junsong%2520Li%2520and%2520Bihao%2520Zhan%2520and%2520Qianjun%2520Pan%2520and%2520Yutao%2520Yang%2520and%2520Xin%2520Li%2520and%2520Qin%2520Chen%2520and%2520Hang%2520Yan%2520and%2520Liang%2520He%26entry.1292438233%3D%2520%2520Model%2520merging%2520refers%2520to%2520the%2520process%2520of%2520integrating%2520multiple%2520distinct%2520models%250Ainto%2520a%2520unified%2520model%2520that%2520preserves%2520and%2520combines%2520the%2520strengths%2520and%2520capabilities%250Aof%2520the%2520individual%2520models.%2520Most%2520existing%2520approaches%2520rely%2520on%2520task%2520vectors%2520to%250Acombine%2520models%252C%2520typically%2520under%2520the%2520assumption%2520that%2520model%2520parameters%2520are%250Aaccessible.%2520However%252C%2520for%2520extremely%2520large%2520language%2520models%2520%2528LLMs%2529%2520such%2520as%2520GPT-4%252C%250Awhich%2520are%2520often%2520provided%2520solely%2520as%2520black-box%2520services%2520through%2520API%2520interfaces%250A%2528Language-Model-as-a-Service%2529%252C%2520model%2520weights%2520are%2520not%2520available%2520to%2520end%2520users.%250AThis%2520presents%2520a%2520significant%2520challenge%252C%2520which%2520we%2520refer%2520to%2520as%2520black-box%2520model%250Amerging%2520%2528BMM%2529%2520with%2520massive%2520LLMs.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%250Aderivative-free%2520optimization%2520framework%2520based%2520on%2520the%2520evolutionary%2520algorithm%250A%2528Evo-Merging%2529%2520that%2520enables%2520effective%2520model%2520merging%2520using%2520only%2520inference-time%250AAPI%2520queries.%2520Our%2520method%2520consists%2520of%2520two%2520key%2520components%253A%2520%25281%2529%2520sparsity-based%250Adenoising%252C%2520designed%2520to%2520identify%2520and%2520filter%2520out%2520irrelevant%2520or%2520redundant%250Ainformation%2520across%2520models%252C%2520and%2520%25282%2529%2520sign-aware%2520scaling%252C%2520which%2520dynamically%250Acomputes%2520optimal%2520combination%2520weights%2520for%2520the%2520relevant%2520models%2520based%2520on%2520their%250Aperformance.%2520We%2520also%2520provide%2520a%2520formal%2520justification%252C%2520along%2520with%2520a%2520theoretical%250Aanalysis%252C%2520for%2520our%2520asymmetric%2520sparsification.%2520Extensive%2520experimental%2520evaluations%250Ademonstrate%2520that%2520our%2520approach%2520achieves%2520state-of-the-art%2520results%2520on%2520a%2520range%2520of%250Atasks%252C%2520significantly%2520outperforming%2520existing%2520strong%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Black-box%20Model%20Merging%20for%20Language-Model-as-a-Service%20with%20Massive%0A%20%20Model%20Repositories&entry.906535625=Shilian%20Chen%20and%20Jie%20Zhou%20and%20Tianyu%20Huai%20and%20Yujiang%20Lu%20and%20Junsong%20Li%20and%20Bihao%20Zhan%20and%20Qianjun%20Pan%20and%20Yutao%20Yang%20and%20Xin%20Li%20and%20Qin%20Chen%20and%20Hang%20Yan%20and%20Liang%20He&entry.1292438233=%20%20Model%20merging%20refers%20to%20the%20process%20of%20integrating%20multiple%20distinct%20models%0Ainto%20a%20unified%20model%20that%20preserves%20and%20combines%20the%20strengths%20and%20capabilities%0Aof%20the%20individual%20models.%20Most%20existing%20approaches%20rely%20on%20task%20vectors%20to%0Acombine%20models%2C%20typically%20under%20the%20assumption%20that%20model%20parameters%20are%0Aaccessible.%20However%2C%20for%20extremely%20large%20language%20models%20%28LLMs%29%20such%20as%20GPT-4%2C%0Awhich%20are%20often%20provided%20solely%20as%20black-box%20services%20through%20API%20interfaces%0A%28Language-Model-as-a-Service%29%2C%20model%20weights%20are%20not%20available%20to%20end%20users.%0AThis%20presents%20a%20significant%20challenge%2C%20which%20we%20refer%20to%20as%20black-box%20model%0Amerging%20%28BMM%29%20with%20massive%20LLMs.%20To%20address%20this%20challenge%2C%20we%20propose%20a%0Aderivative-free%20optimization%20framework%20based%20on%20the%20evolutionary%20algorithm%0A%28Evo-Merging%29%20that%20enables%20effective%20model%20merging%20using%20only%20inference-time%0AAPI%20queries.%20Our%20method%20consists%20of%20two%20key%20components%3A%20%281%29%20sparsity-based%0Adenoising%2C%20designed%20to%20identify%20and%20filter%20out%20irrelevant%20or%20redundant%0Ainformation%20across%20models%2C%20and%20%282%29%20sign-aware%20scaling%2C%20which%20dynamically%0Acomputes%20optimal%20combination%20weights%20for%20the%20relevant%20models%20based%20on%20their%0Aperformance.%20We%20also%20provide%20a%20formal%20justification%2C%20along%20with%20a%20theoretical%0Aanalysis%2C%20for%20our%20asymmetric%20sparsification.%20Extensive%20experimental%20evaluations%0Ademonstrate%20that%20our%20approach%20achieves%20state-of-the-art%20results%20on%20a%20range%20of%0Atasks%2C%20significantly%20outperforming%20existing%20strong%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12951v1&entry.124074799=Read"},
{"title": "Shaping Explanations: Semantic Reward Modeling with Encoder-Only\n  Transformers for GRPO", "author": "Francesco Pappone and Ruggero Marino Lazzaroni and Federico Califano and Niccol\u00f2 Gentile and Roberto Marras", "abstract": "  While Large Language Models (LLMs) excel at generating human-like text,\naligning their outputs with complex, qualitative goals like pedagogical\nsoundness remains a significant challenge. Standard reinforcement learning\ntechniques often rely on slow and expensive LLM-as-a-judge evaluations or on\nbrittle, keyword-based metrics like ROUGE, which fail to capture the semantic\nessence of a high-quality explanation. In this work, we introduce a novel\napproach to reward shaping within the Group Relative Policy Optimisation (GRPO)\nframework. Our central contribution is the use of a small, efficient\nencoder-only transformer as a semantic reward model. This model provides a\ndense, semantically rich reward signal based on the cosine similarity between a\ngenerated explanation and a ground-truth reference, guiding the policy towards\nexplanations that are not just factually correct but also structurally and\nconceptually aligned with expert reasoning. We apply this method to the task of\ntraining a model for the Italian medical-school entrance examinations,\nfollowing standard domain-adaptive continued pre-training (CPT) and supervised\nfine-tuning (SFT). Our results demonstrate that GRPO with our proposed semantic\nreward significantly improves explanation faithfulness and clarity over a\nstrong SFT baseline, showcasing the power of using lightweight encoder models\nfor nuanced reward shaping in complex generation tasks\n", "link": "http://arxiv.org/abs/2509.13081v1", "date": "2025-09-16", "relevancy": 2.0729, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5187}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shaping%20Explanations%3A%20Semantic%20Reward%20Modeling%20with%20Encoder-Only%0A%20%20Transformers%20for%20GRPO&body=Title%3A%20Shaping%20Explanations%3A%20Semantic%20Reward%20Modeling%20with%20Encoder-Only%0A%20%20Transformers%20for%20GRPO%0AAuthor%3A%20Francesco%20Pappone%20and%20Ruggero%20Marino%20Lazzaroni%20and%20Federico%20Califano%20and%20Niccol%C3%B2%20Gentile%20and%20Roberto%20Marras%0AAbstract%3A%20%20%20While%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20generating%20human-like%20text%2C%0Aaligning%20their%20outputs%20with%20complex%2C%20qualitative%20goals%20like%20pedagogical%0Asoundness%20remains%20a%20significant%20challenge.%20Standard%20reinforcement%20learning%0Atechniques%20often%20rely%20on%20slow%20and%20expensive%20LLM-as-a-judge%20evaluations%20or%20on%0Abrittle%2C%20keyword-based%20metrics%20like%20ROUGE%2C%20which%20fail%20to%20capture%20the%20semantic%0Aessence%20of%20a%20high-quality%20explanation.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0Aapproach%20to%20reward%20shaping%20within%20the%20Group%20Relative%20Policy%20Optimisation%20%28GRPO%29%0Aframework.%20Our%20central%20contribution%20is%20the%20use%20of%20a%20small%2C%20efficient%0Aencoder-only%20transformer%20as%20a%20semantic%20reward%20model.%20This%20model%20provides%20a%0Adense%2C%20semantically%20rich%20reward%20signal%20based%20on%20the%20cosine%20similarity%20between%20a%0Agenerated%20explanation%20and%20a%20ground-truth%20reference%2C%20guiding%20the%20policy%20towards%0Aexplanations%20that%20are%20not%20just%20factually%20correct%20but%20also%20structurally%20and%0Aconceptually%20aligned%20with%20expert%20reasoning.%20We%20apply%20this%20method%20to%20the%20task%20of%0Atraining%20a%20model%20for%20the%20Italian%20medical-school%20entrance%20examinations%2C%0Afollowing%20standard%20domain-adaptive%20continued%20pre-training%20%28CPT%29%20and%20supervised%0Afine-tuning%20%28SFT%29.%20Our%20results%20demonstrate%20that%20GRPO%20with%20our%20proposed%20semantic%0Areward%20significantly%20improves%20explanation%20faithfulness%20and%20clarity%20over%20a%0Astrong%20SFT%20baseline%2C%20showcasing%20the%20power%20of%20using%20lightweight%20encoder%20models%0Afor%20nuanced%20reward%20shaping%20in%20complex%20generation%20tasks%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13081v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShaping%2520Explanations%253A%2520Semantic%2520Reward%2520Modeling%2520with%2520Encoder-Only%250A%2520%2520Transformers%2520for%2520GRPO%26entry.906535625%3DFrancesco%2520Pappone%2520and%2520Ruggero%2520Marino%2520Lazzaroni%2520and%2520Federico%2520Califano%2520and%2520Niccol%25C3%25B2%2520Gentile%2520and%2520Roberto%2520Marras%26entry.1292438233%3D%2520%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520at%2520generating%2520human-like%2520text%252C%250Aaligning%2520their%2520outputs%2520with%2520complex%252C%2520qualitative%2520goals%2520like%2520pedagogical%250Asoundness%2520remains%2520a%2520significant%2520challenge.%2520Standard%2520reinforcement%2520learning%250Atechniques%2520often%2520rely%2520on%2520slow%2520and%2520expensive%2520LLM-as-a-judge%2520evaluations%2520or%2520on%250Abrittle%252C%2520keyword-based%2520metrics%2520like%2520ROUGE%252C%2520which%2520fail%2520to%2520capture%2520the%2520semantic%250Aessence%2520of%2520a%2520high-quality%2520explanation.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520novel%250Aapproach%2520to%2520reward%2520shaping%2520within%2520the%2520Group%2520Relative%2520Policy%2520Optimisation%2520%2528GRPO%2529%250Aframework.%2520Our%2520central%2520contribution%2520is%2520the%2520use%2520of%2520a%2520small%252C%2520efficient%250Aencoder-only%2520transformer%2520as%2520a%2520semantic%2520reward%2520model.%2520This%2520model%2520provides%2520a%250Adense%252C%2520semantically%2520rich%2520reward%2520signal%2520based%2520on%2520the%2520cosine%2520similarity%2520between%2520a%250Agenerated%2520explanation%2520and%2520a%2520ground-truth%2520reference%252C%2520guiding%2520the%2520policy%2520towards%250Aexplanations%2520that%2520are%2520not%2520just%2520factually%2520correct%2520but%2520also%2520structurally%2520and%250Aconceptually%2520aligned%2520with%2520expert%2520reasoning.%2520We%2520apply%2520this%2520method%2520to%2520the%2520task%2520of%250Atraining%2520a%2520model%2520for%2520the%2520Italian%2520medical-school%2520entrance%2520examinations%252C%250Afollowing%2520standard%2520domain-adaptive%2520continued%2520pre-training%2520%2528CPT%2529%2520and%2520supervised%250Afine-tuning%2520%2528SFT%2529.%2520Our%2520results%2520demonstrate%2520that%2520GRPO%2520with%2520our%2520proposed%2520semantic%250Areward%2520significantly%2520improves%2520explanation%2520faithfulness%2520and%2520clarity%2520over%2520a%250Astrong%2520SFT%2520baseline%252C%2520showcasing%2520the%2520power%2520of%2520using%2520lightweight%2520encoder%2520models%250Afor%2520nuanced%2520reward%2520shaping%2520in%2520complex%2520generation%2520tasks%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13081v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shaping%20Explanations%3A%20Semantic%20Reward%20Modeling%20with%20Encoder-Only%0A%20%20Transformers%20for%20GRPO&entry.906535625=Francesco%20Pappone%20and%20Ruggero%20Marino%20Lazzaroni%20and%20Federico%20Califano%20and%20Niccol%C3%B2%20Gentile%20and%20Roberto%20Marras&entry.1292438233=%20%20While%20Large%20Language%20Models%20%28LLMs%29%20excel%20at%20generating%20human-like%20text%2C%0Aaligning%20their%20outputs%20with%20complex%2C%20qualitative%20goals%20like%20pedagogical%0Asoundness%20remains%20a%20significant%20challenge.%20Standard%20reinforcement%20learning%0Atechniques%20often%20rely%20on%20slow%20and%20expensive%20LLM-as-a-judge%20evaluations%20or%20on%0Abrittle%2C%20keyword-based%20metrics%20like%20ROUGE%2C%20which%20fail%20to%20capture%20the%20semantic%0Aessence%20of%20a%20high-quality%20explanation.%20In%20this%20work%2C%20we%20introduce%20a%20novel%0Aapproach%20to%20reward%20shaping%20within%20the%20Group%20Relative%20Policy%20Optimisation%20%28GRPO%29%0Aframework.%20Our%20central%20contribution%20is%20the%20use%20of%20a%20small%2C%20efficient%0Aencoder-only%20transformer%20as%20a%20semantic%20reward%20model.%20This%20model%20provides%20a%0Adense%2C%20semantically%20rich%20reward%20signal%20based%20on%20the%20cosine%20similarity%20between%20a%0Agenerated%20explanation%20and%20a%20ground-truth%20reference%2C%20guiding%20the%20policy%20towards%0Aexplanations%20that%20are%20not%20just%20factually%20correct%20but%20also%20structurally%20and%0Aconceptually%20aligned%20with%20expert%20reasoning.%20We%20apply%20this%20method%20to%20the%20task%20of%0Atraining%20a%20model%20for%20the%20Italian%20medical-school%20entrance%20examinations%2C%0Afollowing%20standard%20domain-adaptive%20continued%20pre-training%20%28CPT%29%20and%20supervised%0Afine-tuning%20%28SFT%29.%20Our%20results%20demonstrate%20that%20GRPO%20with%20our%20proposed%20semantic%0Areward%20significantly%20improves%20explanation%20faithfulness%20and%20clarity%20over%20a%0Astrong%20SFT%20baseline%2C%20showcasing%20the%20power%20of%20using%20lightweight%20encoder%20models%0Afor%20nuanced%20reward%20shaping%20in%20complex%20generation%20tasks%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13081v1&entry.124074799=Read"},
{"title": "Comprehend, Divide, and Conquer: Feature Subspace Exploration via\n  Multi-Agent Hierarchical Reinforcement Learning", "author": "Weiliang Zhang and Xiaohan Huang and Yi Du and Ziyue Qiao and Qingqing Long and Zhen Meng and Yuanchun Zhou and Meng Xiao", "abstract": "  Feature selection aims to preprocess the target dataset, find an optimal and\nmost streamlined feature subset, and enhance the downstream machine learning\ntask. Among filter, wrapper, and embedded-based approaches, the reinforcement\nlearning (RL)-based subspace exploration strategy provides a novel objective\noptimization-directed perspective and promising performance. Nevertheless, even\nwith improved performance, current reinforcement learning approaches face\nchallenges similar to conventional methods when dealing with complex datasets.\nThese challenges stem from the inefficient paradigm of using one agent per\nfeature and the inherent complexities present in the datasets. This observation\nmotivates us to investigate and address the above issue and propose a novel\napproach, namely HRLFS. Our methodology initially employs a Large Language\nModel (LLM)-based hybrid state extractor to capture each feature's mathematical\nand semantic characteristics. Based on this information, features are\nclustered, facilitating the construction of hierarchical agents for each\ncluster and sub-cluster. Extensive experiments demonstrate the efficiency,\nscalability, and robustness of our approach. Compared to contemporary or the\none-feature-one-agent RL-based approaches, HRLFS improves the downstream ML\nperformance with iterative feature subspace exploration while accelerating\ntotal run time by reducing the number of agents involved.\n", "link": "http://arxiv.org/abs/2504.17356v2", "date": "2025-09-16", "relevancy": 2.0727, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5446}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5163}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5095}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comprehend%2C%20Divide%2C%20and%20Conquer%3A%20Feature%20Subspace%20Exploration%20via%0A%20%20Multi-Agent%20Hierarchical%20Reinforcement%20Learning&body=Title%3A%20Comprehend%2C%20Divide%2C%20and%20Conquer%3A%20Feature%20Subspace%20Exploration%20via%0A%20%20Multi-Agent%20Hierarchical%20Reinforcement%20Learning%0AAuthor%3A%20Weiliang%20Zhang%20and%20Xiaohan%20Huang%20and%20Yi%20Du%20and%20Ziyue%20Qiao%20and%20Qingqing%20Long%20and%20Zhen%20Meng%20and%20Yuanchun%20Zhou%20and%20Meng%20Xiao%0AAbstract%3A%20%20%20Feature%20selection%20aims%20to%20preprocess%20the%20target%20dataset%2C%20find%20an%20optimal%20and%0Amost%20streamlined%20feature%20subset%2C%20and%20enhance%20the%20downstream%20machine%20learning%0Atask.%20Among%20filter%2C%20wrapper%2C%20and%20embedded-based%20approaches%2C%20the%20reinforcement%0Alearning%20%28RL%29-based%20subspace%20exploration%20strategy%20provides%20a%20novel%20objective%0Aoptimization-directed%20perspective%20and%20promising%20performance.%20Nevertheless%2C%20even%0Awith%20improved%20performance%2C%20current%20reinforcement%20learning%20approaches%20face%0Achallenges%20similar%20to%20conventional%20methods%20when%20dealing%20with%20complex%20datasets.%0AThese%20challenges%20stem%20from%20the%20inefficient%20paradigm%20of%20using%20one%20agent%20per%0Afeature%20and%20the%20inherent%20complexities%20present%20in%20the%20datasets.%20This%20observation%0Amotivates%20us%20to%20investigate%20and%20address%20the%20above%20issue%20and%20propose%20a%20novel%0Aapproach%2C%20namely%20HRLFS.%20Our%20methodology%20initially%20employs%20a%20Large%20Language%0AModel%20%28LLM%29-based%20hybrid%20state%20extractor%20to%20capture%20each%20feature%27s%20mathematical%0Aand%20semantic%20characteristics.%20Based%20on%20this%20information%2C%20features%20are%0Aclustered%2C%20facilitating%20the%20construction%20of%20hierarchical%20agents%20for%20each%0Acluster%20and%20sub-cluster.%20Extensive%20experiments%20demonstrate%20the%20efficiency%2C%0Ascalability%2C%20and%20robustness%20of%20our%20approach.%20Compared%20to%20contemporary%20or%20the%0Aone-feature-one-agent%20RL-based%20approaches%2C%20HRLFS%20improves%20the%20downstream%20ML%0Aperformance%20with%20iterative%20feature%20subspace%20exploration%20while%20accelerating%0Atotal%20run%20time%20by%20reducing%20the%20number%20of%20agents%20involved.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.17356v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComprehend%252C%2520Divide%252C%2520and%2520Conquer%253A%2520Feature%2520Subspace%2520Exploration%2520via%250A%2520%2520Multi-Agent%2520Hierarchical%2520Reinforcement%2520Learning%26entry.906535625%3DWeiliang%2520Zhang%2520and%2520Xiaohan%2520Huang%2520and%2520Yi%2520Du%2520and%2520Ziyue%2520Qiao%2520and%2520Qingqing%2520Long%2520and%2520Zhen%2520Meng%2520and%2520Yuanchun%2520Zhou%2520and%2520Meng%2520Xiao%26entry.1292438233%3D%2520%2520Feature%2520selection%2520aims%2520to%2520preprocess%2520the%2520target%2520dataset%252C%2520find%2520an%2520optimal%2520and%250Amost%2520streamlined%2520feature%2520subset%252C%2520and%2520enhance%2520the%2520downstream%2520machine%2520learning%250Atask.%2520Among%2520filter%252C%2520wrapper%252C%2520and%2520embedded-based%2520approaches%252C%2520the%2520reinforcement%250Alearning%2520%2528RL%2529-based%2520subspace%2520exploration%2520strategy%2520provides%2520a%2520novel%2520objective%250Aoptimization-directed%2520perspective%2520and%2520promising%2520performance.%2520Nevertheless%252C%2520even%250Awith%2520improved%2520performance%252C%2520current%2520reinforcement%2520learning%2520approaches%2520face%250Achallenges%2520similar%2520to%2520conventional%2520methods%2520when%2520dealing%2520with%2520complex%2520datasets.%250AThese%2520challenges%2520stem%2520from%2520the%2520inefficient%2520paradigm%2520of%2520using%2520one%2520agent%2520per%250Afeature%2520and%2520the%2520inherent%2520complexities%2520present%2520in%2520the%2520datasets.%2520This%2520observation%250Amotivates%2520us%2520to%2520investigate%2520and%2520address%2520the%2520above%2520issue%2520and%2520propose%2520a%2520novel%250Aapproach%252C%2520namely%2520HRLFS.%2520Our%2520methodology%2520initially%2520employs%2520a%2520Large%2520Language%250AModel%2520%2528LLM%2529-based%2520hybrid%2520state%2520extractor%2520to%2520capture%2520each%2520feature%2527s%2520mathematical%250Aand%2520semantic%2520characteristics.%2520Based%2520on%2520this%2520information%252C%2520features%2520are%250Aclustered%252C%2520facilitating%2520the%2520construction%2520of%2520hierarchical%2520agents%2520for%2520each%250Acluster%2520and%2520sub-cluster.%2520Extensive%2520experiments%2520demonstrate%2520the%2520efficiency%252C%250Ascalability%252C%2520and%2520robustness%2520of%2520our%2520approach.%2520Compared%2520to%2520contemporary%2520or%2520the%250Aone-feature-one-agent%2520RL-based%2520approaches%252C%2520HRLFS%2520improves%2520the%2520downstream%2520ML%250Aperformance%2520with%2520iterative%2520feature%2520subspace%2520exploration%2520while%2520accelerating%250Atotal%2520run%2520time%2520by%2520reducing%2520the%2520number%2520of%2520agents%2520involved.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.17356v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comprehend%2C%20Divide%2C%20and%20Conquer%3A%20Feature%20Subspace%20Exploration%20via%0A%20%20Multi-Agent%20Hierarchical%20Reinforcement%20Learning&entry.906535625=Weiliang%20Zhang%20and%20Xiaohan%20Huang%20and%20Yi%20Du%20and%20Ziyue%20Qiao%20and%20Qingqing%20Long%20and%20Zhen%20Meng%20and%20Yuanchun%20Zhou%20and%20Meng%20Xiao&entry.1292438233=%20%20Feature%20selection%20aims%20to%20preprocess%20the%20target%20dataset%2C%20find%20an%20optimal%20and%0Amost%20streamlined%20feature%20subset%2C%20and%20enhance%20the%20downstream%20machine%20learning%0Atask.%20Among%20filter%2C%20wrapper%2C%20and%20embedded-based%20approaches%2C%20the%20reinforcement%0Alearning%20%28RL%29-based%20subspace%20exploration%20strategy%20provides%20a%20novel%20objective%0Aoptimization-directed%20perspective%20and%20promising%20performance.%20Nevertheless%2C%20even%0Awith%20improved%20performance%2C%20current%20reinforcement%20learning%20approaches%20face%0Achallenges%20similar%20to%20conventional%20methods%20when%20dealing%20with%20complex%20datasets.%0AThese%20challenges%20stem%20from%20the%20inefficient%20paradigm%20of%20using%20one%20agent%20per%0Afeature%20and%20the%20inherent%20complexities%20present%20in%20the%20datasets.%20This%20observation%0Amotivates%20us%20to%20investigate%20and%20address%20the%20above%20issue%20and%20propose%20a%20novel%0Aapproach%2C%20namely%20HRLFS.%20Our%20methodology%20initially%20employs%20a%20Large%20Language%0AModel%20%28LLM%29-based%20hybrid%20state%20extractor%20to%20capture%20each%20feature%27s%20mathematical%0Aand%20semantic%20characteristics.%20Based%20on%20this%20information%2C%20features%20are%0Aclustered%2C%20facilitating%20the%20construction%20of%20hierarchical%20agents%20for%20each%0Acluster%20and%20sub-cluster.%20Extensive%20experiments%20demonstrate%20the%20efficiency%2C%0Ascalability%2C%20and%20robustness%20of%20our%20approach.%20Compared%20to%20contemporary%20or%20the%0Aone-feature-one-agent%20RL-based%20approaches%2C%20HRLFS%20improves%20the%20downstream%20ML%0Aperformance%20with%20iterative%20feature%20subspace%20exploration%20while%20accelerating%0Atotal%20run%20time%20by%20reducing%20the%20number%20of%20agents%20involved.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.17356v2&entry.124074799=Read"},
{"title": "MEIL-NeRF: Memory-Efficient Incremental Learning of Neural Radiance\n  Fields", "author": "Jaeyoung Chung and Kanggeon Lee and Sungyong Baik and Kyoung Mu Lee", "abstract": "  Hinged on the representation power of neural networks, neural radiance fields\n(NeRF) have recently emerged as one of the promising and widely applicable\nmethods for 3D object and scene representation. However, NeRF faces challenges\nin practical applications, such as large-scale scenes and edge devices with a\nlimited amount of memory, where data needs to be processed sequentially. Under\nsuch incremental learning scenarios, neural networks are known to suffer\ncatastrophic forgetting: easily forgetting previously seen data after training\nwith new data. We observe that previous incremental learning algorithms are\nlimited by either low performance or memory scalability issues. As such, we\ndevelop a Memory-Efficient Incremental Learning algorithm for NeRF (MEIL-NeRF).\nMEIL-NeRF takes inspiration from NeRF itself in that a neural network can serve\nas a memory that provides the pixel RGB values, given rays as queries. Upon the\nmotivation, our framework learns which rays to query NeRF to extract previous\npixel values. The extracted pixel values are then used to train NeRF in a\nself-distillation manner to prevent catastrophic forgetting. As a result,\nMEIL-NeRF demonstrates constant memory consumption and competitive performance.\n", "link": "http://arxiv.org/abs/2212.08328v3", "date": "2025-09-16", "relevancy": 2.0718, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5299}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5227}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEIL-NeRF%3A%20Memory-Efficient%20Incremental%20Learning%20of%20Neural%20Radiance%0A%20%20Fields&body=Title%3A%20MEIL-NeRF%3A%20Memory-Efficient%20Incremental%20Learning%20of%20Neural%20Radiance%0A%20%20Fields%0AAuthor%3A%20Jaeyoung%20Chung%20and%20Kanggeon%20Lee%20and%20Sungyong%20Baik%20and%20Kyoung%20Mu%20Lee%0AAbstract%3A%20%20%20Hinged%20on%20the%20representation%20power%20of%20neural%20networks%2C%20neural%20radiance%20fields%0A%28NeRF%29%20have%20recently%20emerged%20as%20one%20of%20the%20promising%20and%20widely%20applicable%0Amethods%20for%203D%20object%20and%20scene%20representation.%20However%2C%20NeRF%20faces%20challenges%0Ain%20practical%20applications%2C%20such%20as%20large-scale%20scenes%20and%20edge%20devices%20with%20a%0Alimited%20amount%20of%20memory%2C%20where%20data%20needs%20to%20be%20processed%20sequentially.%20Under%0Asuch%20incremental%20learning%20scenarios%2C%20neural%20networks%20are%20known%20to%20suffer%0Acatastrophic%20forgetting%3A%20easily%20forgetting%20previously%20seen%20data%20after%20training%0Awith%20new%20data.%20We%20observe%20that%20previous%20incremental%20learning%20algorithms%20are%0Alimited%20by%20either%20low%20performance%20or%20memory%20scalability%20issues.%20As%20such%2C%20we%0Adevelop%20a%20Memory-Efficient%20Incremental%20Learning%20algorithm%20for%20NeRF%20%28MEIL-NeRF%29.%0AMEIL-NeRF%20takes%20inspiration%20from%20NeRF%20itself%20in%20that%20a%20neural%20network%20can%20serve%0Aas%20a%20memory%20that%20provides%20the%20pixel%20RGB%20values%2C%20given%20rays%20as%20queries.%20Upon%20the%0Amotivation%2C%20our%20framework%20learns%20which%20rays%20to%20query%20NeRF%20to%20extract%20previous%0Apixel%20values.%20The%20extracted%20pixel%20values%20are%20then%20used%20to%20train%20NeRF%20in%20a%0Aself-distillation%20manner%20to%20prevent%20catastrophic%20forgetting.%20As%20a%20result%2C%0AMEIL-NeRF%20demonstrates%20constant%20memory%20consumption%20and%20competitive%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.08328v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEIL-NeRF%253A%2520Memory-Efficient%2520Incremental%2520Learning%2520of%2520Neural%2520Radiance%250A%2520%2520Fields%26entry.906535625%3DJaeyoung%2520Chung%2520and%2520Kanggeon%2520Lee%2520and%2520Sungyong%2520Baik%2520and%2520Kyoung%2520Mu%2520Lee%26entry.1292438233%3D%2520%2520Hinged%2520on%2520the%2520representation%2520power%2520of%2520neural%2520networks%252C%2520neural%2520radiance%2520fields%250A%2528NeRF%2529%2520have%2520recently%2520emerged%2520as%2520one%2520of%2520the%2520promising%2520and%2520widely%2520applicable%250Amethods%2520for%25203D%2520object%2520and%2520scene%2520representation.%2520However%252C%2520NeRF%2520faces%2520challenges%250Ain%2520practical%2520applications%252C%2520such%2520as%2520large-scale%2520scenes%2520and%2520edge%2520devices%2520with%2520a%250Alimited%2520amount%2520of%2520memory%252C%2520where%2520data%2520needs%2520to%2520be%2520processed%2520sequentially.%2520Under%250Asuch%2520incremental%2520learning%2520scenarios%252C%2520neural%2520networks%2520are%2520known%2520to%2520suffer%250Acatastrophic%2520forgetting%253A%2520easily%2520forgetting%2520previously%2520seen%2520data%2520after%2520training%250Awith%2520new%2520data.%2520We%2520observe%2520that%2520previous%2520incremental%2520learning%2520algorithms%2520are%250Alimited%2520by%2520either%2520low%2520performance%2520or%2520memory%2520scalability%2520issues.%2520As%2520such%252C%2520we%250Adevelop%2520a%2520Memory-Efficient%2520Incremental%2520Learning%2520algorithm%2520for%2520NeRF%2520%2528MEIL-NeRF%2529.%250AMEIL-NeRF%2520takes%2520inspiration%2520from%2520NeRF%2520itself%2520in%2520that%2520a%2520neural%2520network%2520can%2520serve%250Aas%2520a%2520memory%2520that%2520provides%2520the%2520pixel%2520RGB%2520values%252C%2520given%2520rays%2520as%2520queries.%2520Upon%2520the%250Amotivation%252C%2520our%2520framework%2520learns%2520which%2520rays%2520to%2520query%2520NeRF%2520to%2520extract%2520previous%250Apixel%2520values.%2520The%2520extracted%2520pixel%2520values%2520are%2520then%2520used%2520to%2520train%2520NeRF%2520in%2520a%250Aself-distillation%2520manner%2520to%2520prevent%2520catastrophic%2520forgetting.%2520As%2520a%2520result%252C%250AMEIL-NeRF%2520demonstrates%2520constant%2520memory%2520consumption%2520and%2520competitive%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2212.08328v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEIL-NeRF%3A%20Memory-Efficient%20Incremental%20Learning%20of%20Neural%20Radiance%0A%20%20Fields&entry.906535625=Jaeyoung%20Chung%20and%20Kanggeon%20Lee%20and%20Sungyong%20Baik%20and%20Kyoung%20Mu%20Lee&entry.1292438233=%20%20Hinged%20on%20the%20representation%20power%20of%20neural%20networks%2C%20neural%20radiance%20fields%0A%28NeRF%29%20have%20recently%20emerged%20as%20one%20of%20the%20promising%20and%20widely%20applicable%0Amethods%20for%203D%20object%20and%20scene%20representation.%20However%2C%20NeRF%20faces%20challenges%0Ain%20practical%20applications%2C%20such%20as%20large-scale%20scenes%20and%20edge%20devices%20with%20a%0Alimited%20amount%20of%20memory%2C%20where%20data%20needs%20to%20be%20processed%20sequentially.%20Under%0Asuch%20incremental%20learning%20scenarios%2C%20neural%20networks%20are%20known%20to%20suffer%0Acatastrophic%20forgetting%3A%20easily%20forgetting%20previously%20seen%20data%20after%20training%0Awith%20new%20data.%20We%20observe%20that%20previous%20incremental%20learning%20algorithms%20are%0Alimited%20by%20either%20low%20performance%20or%20memory%20scalability%20issues.%20As%20such%2C%20we%0Adevelop%20a%20Memory-Efficient%20Incremental%20Learning%20algorithm%20for%20NeRF%20%28MEIL-NeRF%29.%0AMEIL-NeRF%20takes%20inspiration%20from%20NeRF%20itself%20in%20that%20a%20neural%20network%20can%20serve%0Aas%20a%20memory%20that%20provides%20the%20pixel%20RGB%20values%2C%20given%20rays%20as%20queries.%20Upon%20the%0Amotivation%2C%20our%20framework%20learns%20which%20rays%20to%20query%20NeRF%20to%20extract%20previous%0Apixel%20values.%20The%20extracted%20pixel%20values%20are%20then%20used%20to%20train%20NeRF%20in%20a%0Aself-distillation%20manner%20to%20prevent%20catastrophic%20forgetting.%20As%20a%20result%2C%0AMEIL-NeRF%20demonstrates%20constant%20memory%20consumption%20and%20competitive%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.08328v3&entry.124074799=Read"},
{"title": "Reasoning with Preference Constraints: A Benchmark for Language Models\n  in Many-to-One Matching Markets", "author": "Marylou Fauchard and Florian Carichon and Margarida Carvalho and Golnoosh Farnadi", "abstract": "  Recent advances in reasoning with large language models (LLMs) have\ndemonstrated strong performance on complex mathematical tasks, including\ncombinatorial optimization. Techniques such as Chain-of-Thought and In-Context\nLearning have further enhanced this capability, making LLMs both powerful and\naccessible tools for a wide range of users, including non-experts. However,\napplying LLMs to matching problems, which require reasoning under preferential\nand structural constraints, remains underexplored. To address this gap, we\nintroduce a novel benchmark of 369 instances of the College Admission Problem,\na canonical example of a matching problem with preferences, to evaluate LLMs\nacross key dimensions: feasibility, stability, and optimality. We employ this\nbenchmark to assess the performance of several open-weight LLMs. Our results\nfirst reveal that while LLMs can satisfy certain constraints, they struggle to\nmeet all evaluation criteria consistently. They also show that reasoning LLMs,\nlike QwQ and GPT-oss, significantly outperform traditional models such as\nLlama, Qwen or Mistral, defined here as models used without any dedicated\nreasoning mechanisms. Moreover, we observed that LLMs reacted differently to\nthe various prompting strategies tested, which include Chain-of-Thought,\nIn-Context Learning and role-based prompting, with no prompt consistently\noffering the best performance. Finally, we report the performances from\niterative prompting with auto-generated feedback and show that they are not\nmonotonic; they can peak early and then significantly decline in later\nattempts. Overall, this work offers a new perspective on model reasoning\nperformance and the effectiveness of prompting strategies in combinatorial\noptimization problems with preferential constraints.\n", "link": "http://arxiv.org/abs/2509.13131v1", "date": "2025-09-16", "relevancy": 2.0691, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.523}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reasoning%20with%20Preference%20Constraints%3A%20A%20Benchmark%20for%20Language%20Models%0A%20%20in%20Many-to-One%20Matching%20Markets&body=Title%3A%20Reasoning%20with%20Preference%20Constraints%3A%20A%20Benchmark%20for%20Language%20Models%0A%20%20in%20Many-to-One%20Matching%20Markets%0AAuthor%3A%20Marylou%20Fauchard%20and%20Florian%20Carichon%20and%20Margarida%20Carvalho%20and%20Golnoosh%20Farnadi%0AAbstract%3A%20%20%20Recent%20advances%20in%20reasoning%20with%20large%20language%20models%20%28LLMs%29%20have%0Ademonstrated%20strong%20performance%20on%20complex%20mathematical%20tasks%2C%20including%0Acombinatorial%20optimization.%20Techniques%20such%20as%20Chain-of-Thought%20and%20In-Context%0ALearning%20have%20further%20enhanced%20this%20capability%2C%20making%20LLMs%20both%20powerful%20and%0Aaccessible%20tools%20for%20a%20wide%20range%20of%20users%2C%20including%20non-experts.%20However%2C%0Aapplying%20LLMs%20to%20matching%20problems%2C%20which%20require%20reasoning%20under%20preferential%0Aand%20structural%20constraints%2C%20remains%20underexplored.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20a%20novel%20benchmark%20of%20369%20instances%20of%20the%20College%20Admission%20Problem%2C%0Aa%20canonical%20example%20of%20a%20matching%20problem%20with%20preferences%2C%20to%20evaluate%20LLMs%0Aacross%20key%20dimensions%3A%20feasibility%2C%20stability%2C%20and%20optimality.%20We%20employ%20this%0Abenchmark%20to%20assess%20the%20performance%20of%20several%20open-weight%20LLMs.%20Our%20results%0Afirst%20reveal%20that%20while%20LLMs%20can%20satisfy%20certain%20constraints%2C%20they%20struggle%20to%0Ameet%20all%20evaluation%20criteria%20consistently.%20They%20also%20show%20that%20reasoning%20LLMs%2C%0Alike%20QwQ%20and%20GPT-oss%2C%20significantly%20outperform%20traditional%20models%20such%20as%0ALlama%2C%20Qwen%20or%20Mistral%2C%20defined%20here%20as%20models%20used%20without%20any%20dedicated%0Areasoning%20mechanisms.%20Moreover%2C%20we%20observed%20that%20LLMs%20reacted%20differently%20to%0Athe%20various%20prompting%20strategies%20tested%2C%20which%20include%20Chain-of-Thought%2C%0AIn-Context%20Learning%20and%20role-based%20prompting%2C%20with%20no%20prompt%20consistently%0Aoffering%20the%20best%20performance.%20Finally%2C%20we%20report%20the%20performances%20from%0Aiterative%20prompting%20with%20auto-generated%20feedback%20and%20show%20that%20they%20are%20not%0Amonotonic%3B%20they%20can%20peak%20early%20and%20then%20significantly%20decline%20in%20later%0Aattempts.%20Overall%2C%20this%20work%20offers%20a%20new%20perspective%20on%20model%20reasoning%0Aperformance%20and%20the%20effectiveness%20of%20prompting%20strategies%20in%20combinatorial%0Aoptimization%20problems%20with%20preferential%20constraints.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13131v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReasoning%2520with%2520Preference%2520Constraints%253A%2520A%2520Benchmark%2520for%2520Language%2520Models%250A%2520%2520in%2520Many-to-One%2520Matching%2520Markets%26entry.906535625%3DMarylou%2520Fauchard%2520and%2520Florian%2520Carichon%2520and%2520Margarida%2520Carvalho%2520and%2520Golnoosh%2520Farnadi%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520reasoning%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%250Ademonstrated%2520strong%2520performance%2520on%2520complex%2520mathematical%2520tasks%252C%2520including%250Acombinatorial%2520optimization.%2520Techniques%2520such%2520as%2520Chain-of-Thought%2520and%2520In-Context%250ALearning%2520have%2520further%2520enhanced%2520this%2520capability%252C%2520making%2520LLMs%2520both%2520powerful%2520and%250Aaccessible%2520tools%2520for%2520a%2520wide%2520range%2520of%2520users%252C%2520including%2520non-experts.%2520However%252C%250Aapplying%2520LLMs%2520to%2520matching%2520problems%252C%2520which%2520require%2520reasoning%2520under%2520preferential%250Aand%2520structural%2520constraints%252C%2520remains%2520underexplored.%2520To%2520address%2520this%2520gap%252C%2520we%250Aintroduce%2520a%2520novel%2520benchmark%2520of%2520369%2520instances%2520of%2520the%2520College%2520Admission%2520Problem%252C%250Aa%2520canonical%2520example%2520of%2520a%2520matching%2520problem%2520with%2520preferences%252C%2520to%2520evaluate%2520LLMs%250Aacross%2520key%2520dimensions%253A%2520feasibility%252C%2520stability%252C%2520and%2520optimality.%2520We%2520employ%2520this%250Abenchmark%2520to%2520assess%2520the%2520performance%2520of%2520several%2520open-weight%2520LLMs.%2520Our%2520results%250Afirst%2520reveal%2520that%2520while%2520LLMs%2520can%2520satisfy%2520certain%2520constraints%252C%2520they%2520struggle%2520to%250Ameet%2520all%2520evaluation%2520criteria%2520consistently.%2520They%2520also%2520show%2520that%2520reasoning%2520LLMs%252C%250Alike%2520QwQ%2520and%2520GPT-oss%252C%2520significantly%2520outperform%2520traditional%2520models%2520such%2520as%250ALlama%252C%2520Qwen%2520or%2520Mistral%252C%2520defined%2520here%2520as%2520models%2520used%2520without%2520any%2520dedicated%250Areasoning%2520mechanisms.%2520Moreover%252C%2520we%2520observed%2520that%2520LLMs%2520reacted%2520differently%2520to%250Athe%2520various%2520prompting%2520strategies%2520tested%252C%2520which%2520include%2520Chain-of-Thought%252C%250AIn-Context%2520Learning%2520and%2520role-based%2520prompting%252C%2520with%2520no%2520prompt%2520consistently%250Aoffering%2520the%2520best%2520performance.%2520Finally%252C%2520we%2520report%2520the%2520performances%2520from%250Aiterative%2520prompting%2520with%2520auto-generated%2520feedback%2520and%2520show%2520that%2520they%2520are%2520not%250Amonotonic%253B%2520they%2520can%2520peak%2520early%2520and%2520then%2520significantly%2520decline%2520in%2520later%250Aattempts.%2520Overall%252C%2520this%2520work%2520offers%2520a%2520new%2520perspective%2520on%2520model%2520reasoning%250Aperformance%2520and%2520the%2520effectiveness%2520of%2520prompting%2520strategies%2520in%2520combinatorial%250Aoptimization%2520problems%2520with%2520preferential%2520constraints.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13131v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reasoning%20with%20Preference%20Constraints%3A%20A%20Benchmark%20for%20Language%20Models%0A%20%20in%20Many-to-One%20Matching%20Markets&entry.906535625=Marylou%20Fauchard%20and%20Florian%20Carichon%20and%20Margarida%20Carvalho%20and%20Golnoosh%20Farnadi&entry.1292438233=%20%20Recent%20advances%20in%20reasoning%20with%20large%20language%20models%20%28LLMs%29%20have%0Ademonstrated%20strong%20performance%20on%20complex%20mathematical%20tasks%2C%20including%0Acombinatorial%20optimization.%20Techniques%20such%20as%20Chain-of-Thought%20and%20In-Context%0ALearning%20have%20further%20enhanced%20this%20capability%2C%20making%20LLMs%20both%20powerful%20and%0Aaccessible%20tools%20for%20a%20wide%20range%20of%20users%2C%20including%20non-experts.%20However%2C%0Aapplying%20LLMs%20to%20matching%20problems%2C%20which%20require%20reasoning%20under%20preferential%0Aand%20structural%20constraints%2C%20remains%20underexplored.%20To%20address%20this%20gap%2C%20we%0Aintroduce%20a%20novel%20benchmark%20of%20369%20instances%20of%20the%20College%20Admission%20Problem%2C%0Aa%20canonical%20example%20of%20a%20matching%20problem%20with%20preferences%2C%20to%20evaluate%20LLMs%0Aacross%20key%20dimensions%3A%20feasibility%2C%20stability%2C%20and%20optimality.%20We%20employ%20this%0Abenchmark%20to%20assess%20the%20performance%20of%20several%20open-weight%20LLMs.%20Our%20results%0Afirst%20reveal%20that%20while%20LLMs%20can%20satisfy%20certain%20constraints%2C%20they%20struggle%20to%0Ameet%20all%20evaluation%20criteria%20consistently.%20They%20also%20show%20that%20reasoning%20LLMs%2C%0Alike%20QwQ%20and%20GPT-oss%2C%20significantly%20outperform%20traditional%20models%20such%20as%0ALlama%2C%20Qwen%20or%20Mistral%2C%20defined%20here%20as%20models%20used%20without%20any%20dedicated%0Areasoning%20mechanisms.%20Moreover%2C%20we%20observed%20that%20LLMs%20reacted%20differently%20to%0Athe%20various%20prompting%20strategies%20tested%2C%20which%20include%20Chain-of-Thought%2C%0AIn-Context%20Learning%20and%20role-based%20prompting%2C%20with%20no%20prompt%20consistently%0Aoffering%20the%20best%20performance.%20Finally%2C%20we%20report%20the%20performances%20from%0Aiterative%20prompting%20with%20auto-generated%20feedback%20and%20show%20that%20they%20are%20not%0Amonotonic%3B%20they%20can%20peak%20early%20and%20then%20significantly%20decline%20in%20later%0Aattempts.%20Overall%2C%20this%20work%20offers%20a%20new%20perspective%20on%20model%20reasoning%0Aperformance%20and%20the%20effectiveness%20of%20prompting%20strategies%20in%20combinatorial%0Aoptimization%20problems%20with%20preferential%20constraints.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13131v1&entry.124074799=Read"},
{"title": "CredID: Credible Multi-Bit Watermark for Large Language Models\n  Identification", "author": "Haoyu Jiang and Xuhong Wang and Ping Yi and Shanzhe Lei and Yilun Lin", "abstract": "  Large Language Models (LLMs) are widely used in complex natural language\nprocessing tasks but raise privacy and security concerns due to the lack of\nidentity recognition. This paper proposes a multi-party credible watermarking\nframework (CredID) involving a trusted third party (TTP) and multiple LLM\nvendors to address these issues. In the watermark embedding stage, vendors\nrequest a seed from the TTP to generate watermarked text without sending the\nuser's prompt. In the extraction stage, the TTP coordinates each vendor to\nextract and verify the watermark from the text. This provides a credible\nwatermarking scheme while preserving vendor privacy. Furthermore, current\nwatermarking algorithms struggle with text quality, information capacity, and\nrobustness, making it challenging to meet the diverse identification needs of\nLLMs. Thus, we propose a novel multi-bit watermarking algorithm and an\nopen-source toolkit to facilitate research. Experiments show our CredID\nenhances watermark credibility and efficiency without compromising text\nquality. Additionally, we successfully utilized this framework to achieve\nhighly accurate identification among multiple LLM vendors.\n", "link": "http://arxiv.org/abs/2412.03107v2", "date": "2025-09-16", "relevancy": 2.0603, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5453}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5365}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CredID%3A%20Credible%20Multi-Bit%20Watermark%20for%20Large%20Language%20Models%0A%20%20Identification&body=Title%3A%20CredID%3A%20Credible%20Multi-Bit%20Watermark%20for%20Large%20Language%20Models%0A%20%20Identification%0AAuthor%3A%20Haoyu%20Jiang%20and%20Xuhong%20Wang%20and%20Ping%20Yi%20and%20Shanzhe%20Lei%20and%20Yilun%20Lin%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20widely%20used%20in%20complex%20natural%20language%0Aprocessing%20tasks%20but%20raise%20privacy%20and%20security%20concerns%20due%20to%20the%20lack%20of%0Aidentity%20recognition.%20This%20paper%20proposes%20a%20multi-party%20credible%20watermarking%0Aframework%20%28CredID%29%20involving%20a%20trusted%20third%20party%20%28TTP%29%20and%20multiple%20LLM%0Avendors%20to%20address%20these%20issues.%20In%20the%20watermark%20embedding%20stage%2C%20vendors%0Arequest%20a%20seed%20from%20the%20TTP%20to%20generate%20watermarked%20text%20without%20sending%20the%0Auser%27s%20prompt.%20In%20the%20extraction%20stage%2C%20the%20TTP%20coordinates%20each%20vendor%20to%0Aextract%20and%20verify%20the%20watermark%20from%20the%20text.%20This%20provides%20a%20credible%0Awatermarking%20scheme%20while%20preserving%20vendor%20privacy.%20Furthermore%2C%20current%0Awatermarking%20algorithms%20struggle%20with%20text%20quality%2C%20information%20capacity%2C%20and%0Arobustness%2C%20making%20it%20challenging%20to%20meet%20the%20diverse%20identification%20needs%20of%0ALLMs.%20Thus%2C%20we%20propose%20a%20novel%20multi-bit%20watermarking%20algorithm%20and%20an%0Aopen-source%20toolkit%20to%20facilitate%20research.%20Experiments%20show%20our%20CredID%0Aenhances%20watermark%20credibility%20and%20efficiency%20without%20compromising%20text%0Aquality.%20Additionally%2C%20we%20successfully%20utilized%20this%20framework%20to%20achieve%0Ahighly%20accurate%20identification%20among%20multiple%20LLM%20vendors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2412.03107v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCredID%253A%2520Credible%2520Multi-Bit%2520Watermark%2520for%2520Large%2520Language%2520Models%250A%2520%2520Identification%26entry.906535625%3DHaoyu%2520Jiang%2520and%2520Xuhong%2520Wang%2520and%2520Ping%2520Yi%2520and%2520Shanzhe%2520Lei%2520and%2520Yilun%2520Lin%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520widely%2520used%2520in%2520complex%2520natural%2520language%250Aprocessing%2520tasks%2520but%2520raise%2520privacy%2520and%2520security%2520concerns%2520due%2520to%2520the%2520lack%2520of%250Aidentity%2520recognition.%2520This%2520paper%2520proposes%2520a%2520multi-party%2520credible%2520watermarking%250Aframework%2520%2528CredID%2529%2520involving%2520a%2520trusted%2520third%2520party%2520%2528TTP%2529%2520and%2520multiple%2520LLM%250Avendors%2520to%2520address%2520these%2520issues.%2520In%2520the%2520watermark%2520embedding%2520stage%252C%2520vendors%250Arequest%2520a%2520seed%2520from%2520the%2520TTP%2520to%2520generate%2520watermarked%2520text%2520without%2520sending%2520the%250Auser%2527s%2520prompt.%2520In%2520the%2520extraction%2520stage%252C%2520the%2520TTP%2520coordinates%2520each%2520vendor%2520to%250Aextract%2520and%2520verify%2520the%2520watermark%2520from%2520the%2520text.%2520This%2520provides%2520a%2520credible%250Awatermarking%2520scheme%2520while%2520preserving%2520vendor%2520privacy.%2520Furthermore%252C%2520current%250Awatermarking%2520algorithms%2520struggle%2520with%2520text%2520quality%252C%2520information%2520capacity%252C%2520and%250Arobustness%252C%2520making%2520it%2520challenging%2520to%2520meet%2520the%2520diverse%2520identification%2520needs%2520of%250ALLMs.%2520Thus%252C%2520we%2520propose%2520a%2520novel%2520multi-bit%2520watermarking%2520algorithm%2520and%2520an%250Aopen-source%2520toolkit%2520to%2520facilitate%2520research.%2520Experiments%2520show%2520our%2520CredID%250Aenhances%2520watermark%2520credibility%2520and%2520efficiency%2520without%2520compromising%2520text%250Aquality.%2520Additionally%252C%2520we%2520successfully%2520utilized%2520this%2520framework%2520to%2520achieve%250Ahighly%2520accurate%2520identification%2520among%2520multiple%2520LLM%2520vendors.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.03107v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CredID%3A%20Credible%20Multi-Bit%20Watermark%20for%20Large%20Language%20Models%0A%20%20Identification&entry.906535625=Haoyu%20Jiang%20and%20Xuhong%20Wang%20and%20Ping%20Yi%20and%20Shanzhe%20Lei%20and%20Yilun%20Lin&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20widely%20used%20in%20complex%20natural%20language%0Aprocessing%20tasks%20but%20raise%20privacy%20and%20security%20concerns%20due%20to%20the%20lack%20of%0Aidentity%20recognition.%20This%20paper%20proposes%20a%20multi-party%20credible%20watermarking%0Aframework%20%28CredID%29%20involving%20a%20trusted%20third%20party%20%28TTP%29%20and%20multiple%20LLM%0Avendors%20to%20address%20these%20issues.%20In%20the%20watermark%20embedding%20stage%2C%20vendors%0Arequest%20a%20seed%20from%20the%20TTP%20to%20generate%20watermarked%20text%20without%20sending%20the%0Auser%27s%20prompt.%20In%20the%20extraction%20stage%2C%20the%20TTP%20coordinates%20each%20vendor%20to%0Aextract%20and%20verify%20the%20watermark%20from%20the%20text.%20This%20provides%20a%20credible%0Awatermarking%20scheme%20while%20preserving%20vendor%20privacy.%20Furthermore%2C%20current%0Awatermarking%20algorithms%20struggle%20with%20text%20quality%2C%20information%20capacity%2C%20and%0Arobustness%2C%20making%20it%20challenging%20to%20meet%20the%20diverse%20identification%20needs%20of%0ALLMs.%20Thus%2C%20we%20propose%20a%20novel%20multi-bit%20watermarking%20algorithm%20and%20an%0Aopen-source%20toolkit%20to%20facilitate%20research.%20Experiments%20show%20our%20CredID%0Aenhances%20watermark%20credibility%20and%20efficiency%20without%20compromising%20text%0Aquality.%20Additionally%2C%20we%20successfully%20utilized%20this%20framework%20to%20achieve%0Ahighly%20accurate%20identification%20among%20multiple%20LLM%20vendors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2412.03107v2&entry.124074799=Read"},
{"title": "Analysis of Fourier Neural Operators via Effective Field Theory", "author": "Taeyoung Kim", "abstract": "  Fourier Neural Operators (FNOs) have emerged as leading surrogates for solver\noperators for various functional problems, yet their stability, generalization\nand frequency behavior lack a principled explanation. We present a systematic\neffective field theory analysis of FNOs in an infinite dimensional function\nspace, deriving closed recursion relations for the layer kernel and four point\nvertex and then examining three practically important settings-analytic\nactivations, scale invariant cases and architectures with residual connections.\nThe theory shows that nonlinear activations inevitably couple frequency inputs\nto high frequency modes that are otherwise discarded by spectral truncation,\nand experiments confirm this frequency transfer. For wide networks, we derive\nexplicit criticality conditions on the weight initialization ensemble that\nensure small input perturbations maintain a uniform scale across depth, and we\nconfirm experimentally that the theoretically predicted ratio of kernel\nperturbations matches the measurements. Taken together, our results quantify\nhow nonlinearity enables neural operators to capture non-trivial features,\nsupply criteria for hyperparameter selection via criticality analysis, and\nexplain why scale invariant activations and residual connections enhance\nfeature learning in FNOs.\n", "link": "http://arxiv.org/abs/2507.21833v2", "date": "2025-09-16", "relevancy": 1.6944, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4334}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4265}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4126}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Analysis%20of%20Fourier%20Neural%20Operators%20via%20Effective%20Field%20Theory&body=Title%3A%20Analysis%20of%20Fourier%20Neural%20Operators%20via%20Effective%20Field%20Theory%0AAuthor%3A%20Taeyoung%20Kim%0AAbstract%3A%20%20%20Fourier%20Neural%20Operators%20%28FNOs%29%20have%20emerged%20as%20leading%20surrogates%20for%20solver%0Aoperators%20for%20various%20functional%20problems%2C%20yet%20their%20stability%2C%20generalization%0Aand%20frequency%20behavior%20lack%20a%20principled%20explanation.%20We%20present%20a%20systematic%0Aeffective%20field%20theory%20analysis%20of%20FNOs%20in%20an%20infinite%20dimensional%20function%0Aspace%2C%20deriving%20closed%20recursion%20relations%20for%20the%20layer%20kernel%20and%20four%20point%0Avertex%20and%20then%20examining%20three%20practically%20important%20settings-analytic%0Aactivations%2C%20scale%20invariant%20cases%20and%20architectures%20with%20residual%20connections.%0AThe%20theory%20shows%20that%20nonlinear%20activations%20inevitably%20couple%20frequency%20inputs%0Ato%20high%20frequency%20modes%20that%20are%20otherwise%20discarded%20by%20spectral%20truncation%2C%0Aand%20experiments%20confirm%20this%20frequency%20transfer.%20For%20wide%20networks%2C%20we%20derive%0Aexplicit%20criticality%20conditions%20on%20the%20weight%20initialization%20ensemble%20that%0Aensure%20small%20input%20perturbations%20maintain%20a%20uniform%20scale%20across%20depth%2C%20and%20we%0Aconfirm%20experimentally%20that%20the%20theoretically%20predicted%20ratio%20of%20kernel%0Aperturbations%20matches%20the%20measurements.%20Taken%20together%2C%20our%20results%20quantify%0Ahow%20nonlinearity%20enables%20neural%20operators%20to%20capture%20non-trivial%20features%2C%0Asupply%20criteria%20for%20hyperparameter%20selection%20via%20criticality%20analysis%2C%20and%0Aexplain%20why%20scale%20invariant%20activations%20and%20residual%20connections%20enhance%0Afeature%20learning%20in%20FNOs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2507.21833v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnalysis%2520of%2520Fourier%2520Neural%2520Operators%2520via%2520Effective%2520Field%2520Theory%26entry.906535625%3DTaeyoung%2520Kim%26entry.1292438233%3D%2520%2520Fourier%2520Neural%2520Operators%2520%2528FNOs%2529%2520have%2520emerged%2520as%2520leading%2520surrogates%2520for%2520solver%250Aoperators%2520for%2520various%2520functional%2520problems%252C%2520yet%2520their%2520stability%252C%2520generalization%250Aand%2520frequency%2520behavior%2520lack%2520a%2520principled%2520explanation.%2520We%2520present%2520a%2520systematic%250Aeffective%2520field%2520theory%2520analysis%2520of%2520FNOs%2520in%2520an%2520infinite%2520dimensional%2520function%250Aspace%252C%2520deriving%2520closed%2520recursion%2520relations%2520for%2520the%2520layer%2520kernel%2520and%2520four%2520point%250Avertex%2520and%2520then%2520examining%2520three%2520practically%2520important%2520settings-analytic%250Aactivations%252C%2520scale%2520invariant%2520cases%2520and%2520architectures%2520with%2520residual%2520connections.%250AThe%2520theory%2520shows%2520that%2520nonlinear%2520activations%2520inevitably%2520couple%2520frequency%2520inputs%250Ato%2520high%2520frequency%2520modes%2520that%2520are%2520otherwise%2520discarded%2520by%2520spectral%2520truncation%252C%250Aand%2520experiments%2520confirm%2520this%2520frequency%2520transfer.%2520For%2520wide%2520networks%252C%2520we%2520derive%250Aexplicit%2520criticality%2520conditions%2520on%2520the%2520weight%2520initialization%2520ensemble%2520that%250Aensure%2520small%2520input%2520perturbations%2520maintain%2520a%2520uniform%2520scale%2520across%2520depth%252C%2520and%2520we%250Aconfirm%2520experimentally%2520that%2520the%2520theoretically%2520predicted%2520ratio%2520of%2520kernel%250Aperturbations%2520matches%2520the%2520measurements.%2520Taken%2520together%252C%2520our%2520results%2520quantify%250Ahow%2520nonlinearity%2520enables%2520neural%2520operators%2520to%2520capture%2520non-trivial%2520features%252C%250Asupply%2520criteria%2520for%2520hyperparameter%2520selection%2520via%2520criticality%2520analysis%252C%2520and%250Aexplain%2520why%2520scale%2520invariant%2520activations%2520and%2520residual%2520connections%2520enhance%250Afeature%2520learning%2520in%2520FNOs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21833v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Analysis%20of%20Fourier%20Neural%20Operators%20via%20Effective%20Field%20Theory&entry.906535625=Taeyoung%20Kim&entry.1292438233=%20%20Fourier%20Neural%20Operators%20%28FNOs%29%20have%20emerged%20as%20leading%20surrogates%20for%20solver%0Aoperators%20for%20various%20functional%20problems%2C%20yet%20their%20stability%2C%20generalization%0Aand%20frequency%20behavior%20lack%20a%20principled%20explanation.%20We%20present%20a%20systematic%0Aeffective%20field%20theory%20analysis%20of%20FNOs%20in%20an%20infinite%20dimensional%20function%0Aspace%2C%20deriving%20closed%20recursion%20relations%20for%20the%20layer%20kernel%20and%20four%20point%0Avertex%20and%20then%20examining%20three%20practically%20important%20settings-analytic%0Aactivations%2C%20scale%20invariant%20cases%20and%20architectures%20with%20residual%20connections.%0AThe%20theory%20shows%20that%20nonlinear%20activations%20inevitably%20couple%20frequency%20inputs%0Ato%20high%20frequency%20modes%20that%20are%20otherwise%20discarded%20by%20spectral%20truncation%2C%0Aand%20experiments%20confirm%20this%20frequency%20transfer.%20For%20wide%20networks%2C%20we%20derive%0Aexplicit%20criticality%20conditions%20on%20the%20weight%20initialization%20ensemble%20that%0Aensure%20small%20input%20perturbations%20maintain%20a%20uniform%20scale%20across%20depth%2C%20and%20we%0Aconfirm%20experimentally%20that%20the%20theoretically%20predicted%20ratio%20of%20kernel%0Aperturbations%20matches%20the%20measurements.%20Taken%20together%2C%20our%20results%20quantify%0Ahow%20nonlinearity%20enables%20neural%20operators%20to%20capture%20non-trivial%20features%2C%0Asupply%20criteria%20for%20hyperparameter%20selection%20via%20criticality%20analysis%2C%20and%0Aexplain%20why%20scale%20invariant%20activations%20and%20residual%20connections%20enhance%0Afeature%20learning%20in%20FNOs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2507.21833v2&entry.124074799=Read"},
{"title": "Sublinear-Time Algorithms for Diagonally Dominant Systems and\n  Applications to the Friedkin-Johnsen Model", "author": "Weiming Feng and Zelin Li and Pan Peng", "abstract": "  We study sublinear-time algorithms for solving linear systems $Sz = b$, where\n$S$ is a diagonally dominant matrix, i.e., $|S_{ii}| \\geq \\delta + \\sum_{j \\ne\ni} |S_{ij}|$ for all $i \\in [n]$, for some $\\delta \\geq 0$. We present\nrandomized algorithms that, for any $u \\in [n]$, return an estimate $z_u$ of\n$z^*_u$ with additive error $\\varepsilon$ or $\\varepsilon \\lVert\nz^*\\rVert_\\infty$, where $z^*$ is some solution to $Sz^* = b$, and the\nalgorithm only needs to read a small portion of the input $S$ and $b$. For\nexample, when the additive error is $\\varepsilon$ and assuming $\\delta>0$, we\ngive an algorithm that runs in time $O\\left( \\frac{\\|b\\|_\\infty^2\nS_{\\max}}{\\delta^3 \\varepsilon^2} \\log \\frac{\\| b \\|_\\infty}{\\delta\n\\varepsilon} \\right)$, where $S_{\\max} = \\max_{i \\in [n]} |S_{ii}|$. We also\nprove a matching lower bound, showing that the linear dependence on $S_{\\max}$\nis optimal. Unlike previous sublinear-time algorithms, which apply only to\nsymmetric diagonally dominant matrices with non-negative diagonal entries, our\nalgorithm works for general strictly diagonally dominant matrices ($\\delta >\n0$) and a broader class of non-strictly diagonally dominant matrices $(\\delta =\n0)$. Our approach is based on analyzing a simple probabilistic recurrence\nsatisfied by the solution. As an application, we obtain an improved\nsublinear-time algorithm for opinion estimation in the Friedkin--Johnsen model.\n", "link": "http://arxiv.org/abs/2509.13112v1", "date": "2025-09-16", "relevancy": 1.6835, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4757}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4125}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sublinear-Time%20Algorithms%20for%20Diagonally%20Dominant%20Systems%20and%0A%20%20Applications%20to%20the%20Friedkin-Johnsen%20Model&body=Title%3A%20Sublinear-Time%20Algorithms%20for%20Diagonally%20Dominant%20Systems%20and%0A%20%20Applications%20to%20the%20Friedkin-Johnsen%20Model%0AAuthor%3A%20Weiming%20Feng%20and%20Zelin%20Li%20and%20Pan%20Peng%0AAbstract%3A%20%20%20We%20study%20sublinear-time%20algorithms%20for%20solving%20linear%20systems%20%24Sz%20%3D%20b%24%2C%20where%0A%24S%24%20is%20a%20diagonally%20dominant%20matrix%2C%20i.e.%2C%20%24%7CS_%7Bii%7D%7C%20%5Cgeq%20%5Cdelta%20%2B%20%5Csum_%7Bj%20%5Cne%0Ai%7D%20%7CS_%7Bij%7D%7C%24%20for%20all%20%24i%20%5Cin%20%5Bn%5D%24%2C%20for%20some%20%24%5Cdelta%20%5Cgeq%200%24.%20We%20present%0Arandomized%20algorithms%20that%2C%20for%20any%20%24u%20%5Cin%20%5Bn%5D%24%2C%20return%20an%20estimate%20%24z_u%24%20of%0A%24z%5E%2A_u%24%20with%20additive%20error%20%24%5Cvarepsilon%24%20or%20%24%5Cvarepsilon%20%5ClVert%0Az%5E%2A%5CrVert_%5Cinfty%24%2C%20where%20%24z%5E%2A%24%20is%20some%20solution%20to%20%24Sz%5E%2A%20%3D%20b%24%2C%20and%20the%0Aalgorithm%20only%20needs%20to%20read%20a%20small%20portion%20of%20the%20input%20%24S%24%20and%20%24b%24.%20For%0Aexample%2C%20when%20the%20additive%20error%20is%20%24%5Cvarepsilon%24%20and%20assuming%20%24%5Cdelta%3E0%24%2C%20we%0Agive%20an%20algorithm%20that%20runs%20in%20time%20%24O%5Cleft%28%20%5Cfrac%7B%5C%7Cb%5C%7C_%5Cinfty%5E2%0AS_%7B%5Cmax%7D%7D%7B%5Cdelta%5E3%20%5Cvarepsilon%5E2%7D%20%5Clog%20%5Cfrac%7B%5C%7C%20b%20%5C%7C_%5Cinfty%7D%7B%5Cdelta%0A%5Cvarepsilon%7D%20%5Cright%29%24%2C%20where%20%24S_%7B%5Cmax%7D%20%3D%20%5Cmax_%7Bi%20%5Cin%20%5Bn%5D%7D%20%7CS_%7Bii%7D%7C%24.%20We%20also%0Aprove%20a%20matching%20lower%20bound%2C%20showing%20that%20the%20linear%20dependence%20on%20%24S_%7B%5Cmax%7D%24%0Ais%20optimal.%20Unlike%20previous%20sublinear-time%20algorithms%2C%20which%20apply%20only%20to%0Asymmetric%20diagonally%20dominant%20matrices%20with%20non-negative%20diagonal%20entries%2C%20our%0Aalgorithm%20works%20for%20general%20strictly%20diagonally%20dominant%20matrices%20%28%24%5Cdelta%20%3E%0A0%24%29%20and%20a%20broader%20class%20of%20non-strictly%20diagonally%20dominant%20matrices%20%24%28%5Cdelta%20%3D%0A0%29%24.%20Our%20approach%20is%20based%20on%20analyzing%20a%20simple%20probabilistic%20recurrence%0Asatisfied%20by%20the%20solution.%20As%20an%20application%2C%20we%20obtain%20an%20improved%0Asublinear-time%20algorithm%20for%20opinion%20estimation%20in%20the%20Friedkin--Johnsen%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSublinear-Time%2520Algorithms%2520for%2520Diagonally%2520Dominant%2520Systems%2520and%250A%2520%2520Applications%2520to%2520the%2520Friedkin-Johnsen%2520Model%26entry.906535625%3DWeiming%2520Feng%2520and%2520Zelin%2520Li%2520and%2520Pan%2520Peng%26entry.1292438233%3D%2520%2520We%2520study%2520sublinear-time%2520algorithms%2520for%2520solving%2520linear%2520systems%2520%2524Sz%2520%253D%2520b%2524%252C%2520where%250A%2524S%2524%2520is%2520a%2520diagonally%2520dominant%2520matrix%252C%2520i.e.%252C%2520%2524%257CS_%257Bii%257D%257C%2520%255Cgeq%2520%255Cdelta%2520%252B%2520%255Csum_%257Bj%2520%255Cne%250Ai%257D%2520%257CS_%257Bij%257D%257C%2524%2520for%2520all%2520%2524i%2520%255Cin%2520%255Bn%255D%2524%252C%2520for%2520some%2520%2524%255Cdelta%2520%255Cgeq%25200%2524.%2520We%2520present%250Arandomized%2520algorithms%2520that%252C%2520for%2520any%2520%2524u%2520%255Cin%2520%255Bn%255D%2524%252C%2520return%2520an%2520estimate%2520%2524z_u%2524%2520of%250A%2524z%255E%252A_u%2524%2520with%2520additive%2520error%2520%2524%255Cvarepsilon%2524%2520or%2520%2524%255Cvarepsilon%2520%255ClVert%250Az%255E%252A%255CrVert_%255Cinfty%2524%252C%2520where%2520%2524z%255E%252A%2524%2520is%2520some%2520solution%2520to%2520%2524Sz%255E%252A%2520%253D%2520b%2524%252C%2520and%2520the%250Aalgorithm%2520only%2520needs%2520to%2520read%2520a%2520small%2520portion%2520of%2520the%2520input%2520%2524S%2524%2520and%2520%2524b%2524.%2520For%250Aexample%252C%2520when%2520the%2520additive%2520error%2520is%2520%2524%255Cvarepsilon%2524%2520and%2520assuming%2520%2524%255Cdelta%253E0%2524%252C%2520we%250Agive%2520an%2520algorithm%2520that%2520runs%2520in%2520time%2520%2524O%255Cleft%2528%2520%255Cfrac%257B%255C%257Cb%255C%257C_%255Cinfty%255E2%250AS_%257B%255Cmax%257D%257D%257B%255Cdelta%255E3%2520%255Cvarepsilon%255E2%257D%2520%255Clog%2520%255Cfrac%257B%255C%257C%2520b%2520%255C%257C_%255Cinfty%257D%257B%255Cdelta%250A%255Cvarepsilon%257D%2520%255Cright%2529%2524%252C%2520where%2520%2524S_%257B%255Cmax%257D%2520%253D%2520%255Cmax_%257Bi%2520%255Cin%2520%255Bn%255D%257D%2520%257CS_%257Bii%257D%257C%2524.%2520We%2520also%250Aprove%2520a%2520matching%2520lower%2520bound%252C%2520showing%2520that%2520the%2520linear%2520dependence%2520on%2520%2524S_%257B%255Cmax%257D%2524%250Ais%2520optimal.%2520Unlike%2520previous%2520sublinear-time%2520algorithms%252C%2520which%2520apply%2520only%2520to%250Asymmetric%2520diagonally%2520dominant%2520matrices%2520with%2520non-negative%2520diagonal%2520entries%252C%2520our%250Aalgorithm%2520works%2520for%2520general%2520strictly%2520diagonally%2520dominant%2520matrices%2520%2528%2524%255Cdelta%2520%253E%250A0%2524%2529%2520and%2520a%2520broader%2520class%2520of%2520non-strictly%2520diagonally%2520dominant%2520matrices%2520%2524%2528%255Cdelta%2520%253D%250A0%2529%2524.%2520Our%2520approach%2520is%2520based%2520on%2520analyzing%2520a%2520simple%2520probabilistic%2520recurrence%250Asatisfied%2520by%2520the%2520solution.%2520As%2520an%2520application%252C%2520we%2520obtain%2520an%2520improved%250Asublinear-time%2520algorithm%2520for%2520opinion%2520estimation%2520in%2520the%2520Friedkin--Johnsen%2520model.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sublinear-Time%20Algorithms%20for%20Diagonally%20Dominant%20Systems%20and%0A%20%20Applications%20to%20the%20Friedkin-Johnsen%20Model&entry.906535625=Weiming%20Feng%20and%20Zelin%20Li%20and%20Pan%20Peng&entry.1292438233=%20%20We%20study%20sublinear-time%20algorithms%20for%20solving%20linear%20systems%20%24Sz%20%3D%20b%24%2C%20where%0A%24S%24%20is%20a%20diagonally%20dominant%20matrix%2C%20i.e.%2C%20%24%7CS_%7Bii%7D%7C%20%5Cgeq%20%5Cdelta%20%2B%20%5Csum_%7Bj%20%5Cne%0Ai%7D%20%7CS_%7Bij%7D%7C%24%20for%20all%20%24i%20%5Cin%20%5Bn%5D%24%2C%20for%20some%20%24%5Cdelta%20%5Cgeq%200%24.%20We%20present%0Arandomized%20algorithms%20that%2C%20for%20any%20%24u%20%5Cin%20%5Bn%5D%24%2C%20return%20an%20estimate%20%24z_u%24%20of%0A%24z%5E%2A_u%24%20with%20additive%20error%20%24%5Cvarepsilon%24%20or%20%24%5Cvarepsilon%20%5ClVert%0Az%5E%2A%5CrVert_%5Cinfty%24%2C%20where%20%24z%5E%2A%24%20is%20some%20solution%20to%20%24Sz%5E%2A%20%3D%20b%24%2C%20and%20the%0Aalgorithm%20only%20needs%20to%20read%20a%20small%20portion%20of%20the%20input%20%24S%24%20and%20%24b%24.%20For%0Aexample%2C%20when%20the%20additive%20error%20is%20%24%5Cvarepsilon%24%20and%20assuming%20%24%5Cdelta%3E0%24%2C%20we%0Agive%20an%20algorithm%20that%20runs%20in%20time%20%24O%5Cleft%28%20%5Cfrac%7B%5C%7Cb%5C%7C_%5Cinfty%5E2%0AS_%7B%5Cmax%7D%7D%7B%5Cdelta%5E3%20%5Cvarepsilon%5E2%7D%20%5Clog%20%5Cfrac%7B%5C%7C%20b%20%5C%7C_%5Cinfty%7D%7B%5Cdelta%0A%5Cvarepsilon%7D%20%5Cright%29%24%2C%20where%20%24S_%7B%5Cmax%7D%20%3D%20%5Cmax_%7Bi%20%5Cin%20%5Bn%5D%7D%20%7CS_%7Bii%7D%7C%24.%20We%20also%0Aprove%20a%20matching%20lower%20bound%2C%20showing%20that%20the%20linear%20dependence%20on%20%24S_%7B%5Cmax%7D%24%0Ais%20optimal.%20Unlike%20previous%20sublinear-time%20algorithms%2C%20which%20apply%20only%20to%0Asymmetric%20diagonally%20dominant%20matrices%20with%20non-negative%20diagonal%20entries%2C%20our%0Aalgorithm%20works%20for%20general%20strictly%20diagonally%20dominant%20matrices%20%28%24%5Cdelta%20%3E%0A0%24%29%20and%20a%20broader%20class%20of%20non-strictly%20diagonally%20dominant%20matrices%20%24%28%5Cdelta%20%3D%0A0%29%24.%20Our%20approach%20is%20based%20on%20analyzing%20a%20simple%20probabilistic%20recurrence%0Asatisfied%20by%20the%20solution.%20As%20an%20application%2C%20we%20obtain%20an%20improved%0Asublinear-time%20algorithm%20for%20opinion%20estimation%20in%20the%20Friedkin--Johnsen%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13112v1&entry.124074799=Read"},
{"title": "Causal Discovery via Quantile Partial Effect", "author": "Yikang Chen and Xingzhe Sun and Dehui Du", "abstract": "  Quantile Partial Effect (QPE) is a statistic associated with conditional\nquantile regression, measuring the effect of covariates at different levels.\nOur theory demonstrates that when the QPE of cause on effect is assumed to lie\nin a finite linear span, cause and effect are identifiable from their\nobservational distribution. This generalizes previous identifiability results\nbased on Functional Causal Models (FCMs) with additive, heteroscedastic noise,\netc. Meanwhile, since QPE resides entirely at the observational level, this\nparametric assumption does not require considering mechanisms, noise, or even\nthe Markov assumption, but rather directly utilizes the asymmetry of shape\ncharacteristics in the observational distribution. By performing basis function\ntests on the estimated QPE, causal directions can be distinguished, which is\nempirically shown to be effective in experiments on a large number of bivariate\ncausal discovery datasets. For multivariate causal discovery, leveraging the\nclose connection between QPE and score functions, we find that Fisher\nInformation is sufficient as a statistical measure to determine causal order\nwhen assumptions are made about the second moment of QPE. We validate the\nfeasibility of using Fisher Information to identify causal order on multiple\nsynthetic and real-world multivariate causal discovery datasets.\n", "link": "http://arxiv.org/abs/2509.12981v1", "date": "2025-09-16", "relevancy": 1.5658, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4169}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3946}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.3782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Causal%20Discovery%20via%20Quantile%20Partial%20Effect&body=Title%3A%20Causal%20Discovery%20via%20Quantile%20Partial%20Effect%0AAuthor%3A%20Yikang%20Chen%20and%20Xingzhe%20Sun%20and%20Dehui%20Du%0AAbstract%3A%20%20%20Quantile%20Partial%20Effect%20%28QPE%29%20is%20a%20statistic%20associated%20with%20conditional%0Aquantile%20regression%2C%20measuring%20the%20effect%20of%20covariates%20at%20different%20levels.%0AOur%20theory%20demonstrates%20that%20when%20the%20QPE%20of%20cause%20on%20effect%20is%20assumed%20to%20lie%0Ain%20a%20finite%20linear%20span%2C%20cause%20and%20effect%20are%20identifiable%20from%20their%0Aobservational%20distribution.%20This%20generalizes%20previous%20identifiability%20results%0Abased%20on%20Functional%20Causal%20Models%20%28FCMs%29%20with%20additive%2C%20heteroscedastic%20noise%2C%0Aetc.%20Meanwhile%2C%20since%20QPE%20resides%20entirely%20at%20the%20observational%20level%2C%20this%0Aparametric%20assumption%20does%20not%20require%20considering%20mechanisms%2C%20noise%2C%20or%20even%0Athe%20Markov%20assumption%2C%20but%20rather%20directly%20utilizes%20the%20asymmetry%20of%20shape%0Acharacteristics%20in%20the%20observational%20distribution.%20By%20performing%20basis%20function%0Atests%20on%20the%20estimated%20QPE%2C%20causal%20directions%20can%20be%20distinguished%2C%20which%20is%0Aempirically%20shown%20to%20be%20effective%20in%20experiments%20on%20a%20large%20number%20of%20bivariate%0Acausal%20discovery%20datasets.%20For%20multivariate%20causal%20discovery%2C%20leveraging%20the%0Aclose%20connection%20between%20QPE%20and%20score%20functions%2C%20we%20find%20that%20Fisher%0AInformation%20is%20sufficient%20as%20a%20statistical%20measure%20to%20determine%20causal%20order%0Awhen%20assumptions%20are%20made%20about%20the%20second%20moment%20of%20QPE.%20We%20validate%20the%0Afeasibility%20of%20using%20Fisher%20Information%20to%20identify%20causal%20order%20on%20multiple%0Asynthetic%20and%20real-world%20multivariate%20causal%20discovery%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.12981v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCausal%2520Discovery%2520via%2520Quantile%2520Partial%2520Effect%26entry.906535625%3DYikang%2520Chen%2520and%2520Xingzhe%2520Sun%2520and%2520Dehui%2520Du%26entry.1292438233%3D%2520%2520Quantile%2520Partial%2520Effect%2520%2528QPE%2529%2520is%2520a%2520statistic%2520associated%2520with%2520conditional%250Aquantile%2520regression%252C%2520measuring%2520the%2520effect%2520of%2520covariates%2520at%2520different%2520levels.%250AOur%2520theory%2520demonstrates%2520that%2520when%2520the%2520QPE%2520of%2520cause%2520on%2520effect%2520is%2520assumed%2520to%2520lie%250Ain%2520a%2520finite%2520linear%2520span%252C%2520cause%2520and%2520effect%2520are%2520identifiable%2520from%2520their%250Aobservational%2520distribution.%2520This%2520generalizes%2520previous%2520identifiability%2520results%250Abased%2520on%2520Functional%2520Causal%2520Models%2520%2528FCMs%2529%2520with%2520additive%252C%2520heteroscedastic%2520noise%252C%250Aetc.%2520Meanwhile%252C%2520since%2520QPE%2520resides%2520entirely%2520at%2520the%2520observational%2520level%252C%2520this%250Aparametric%2520assumption%2520does%2520not%2520require%2520considering%2520mechanisms%252C%2520noise%252C%2520or%2520even%250Athe%2520Markov%2520assumption%252C%2520but%2520rather%2520directly%2520utilizes%2520the%2520asymmetry%2520of%2520shape%250Acharacteristics%2520in%2520the%2520observational%2520distribution.%2520By%2520performing%2520basis%2520function%250Atests%2520on%2520the%2520estimated%2520QPE%252C%2520causal%2520directions%2520can%2520be%2520distinguished%252C%2520which%2520is%250Aempirically%2520shown%2520to%2520be%2520effective%2520in%2520experiments%2520on%2520a%2520large%2520number%2520of%2520bivariate%250Acausal%2520discovery%2520datasets.%2520For%2520multivariate%2520causal%2520discovery%252C%2520leveraging%2520the%250Aclose%2520connection%2520between%2520QPE%2520and%2520score%2520functions%252C%2520we%2520find%2520that%2520Fisher%250AInformation%2520is%2520sufficient%2520as%2520a%2520statistical%2520measure%2520to%2520determine%2520causal%2520order%250Awhen%2520assumptions%2520are%2520made%2520about%2520the%2520second%2520moment%2520of%2520QPE.%2520We%2520validate%2520the%250Afeasibility%2520of%2520using%2520Fisher%2520Information%2520to%2520identify%2520causal%2520order%2520on%2520multiple%250Asynthetic%2520and%2520real-world%2520multivariate%2520causal%2520discovery%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.12981v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Causal%20Discovery%20via%20Quantile%20Partial%20Effect&entry.906535625=Yikang%20Chen%20and%20Xingzhe%20Sun%20and%20Dehui%20Du&entry.1292438233=%20%20Quantile%20Partial%20Effect%20%28QPE%29%20is%20a%20statistic%20associated%20with%20conditional%0Aquantile%20regression%2C%20measuring%20the%20effect%20of%20covariates%20at%20different%20levels.%0AOur%20theory%20demonstrates%20that%20when%20the%20QPE%20of%20cause%20on%20effect%20is%20assumed%20to%20lie%0Ain%20a%20finite%20linear%20span%2C%20cause%20and%20effect%20are%20identifiable%20from%20their%0Aobservational%20distribution.%20This%20generalizes%20previous%20identifiability%20results%0Abased%20on%20Functional%20Causal%20Models%20%28FCMs%29%20with%20additive%2C%20heteroscedastic%20noise%2C%0Aetc.%20Meanwhile%2C%20since%20QPE%20resides%20entirely%20at%20the%20observational%20level%2C%20this%0Aparametric%20assumption%20does%20not%20require%20considering%20mechanisms%2C%20noise%2C%20or%20even%0Athe%20Markov%20assumption%2C%20but%20rather%20directly%20utilizes%20the%20asymmetry%20of%20shape%0Acharacteristics%20in%20the%20observational%20distribution.%20By%20performing%20basis%20function%0Atests%20on%20the%20estimated%20QPE%2C%20causal%20directions%20can%20be%20distinguished%2C%20which%20is%0Aempirically%20shown%20to%20be%20effective%20in%20experiments%20on%20a%20large%20number%20of%20bivariate%0Acausal%20discovery%20datasets.%20For%20multivariate%20causal%20discovery%2C%20leveraging%20the%0Aclose%20connection%20between%20QPE%20and%20score%20functions%2C%20we%20find%20that%20Fisher%0AInformation%20is%20sufficient%20as%20a%20statistical%20measure%20to%20determine%20causal%20order%0Awhen%20assumptions%20are%20made%20about%20the%20second%20moment%20of%20QPE.%20We%20validate%20the%0Afeasibility%20of%20using%20Fisher%20Information%20to%20identify%20causal%20order%20on%20multiple%0Asynthetic%20and%20real-world%20multivariate%20causal%20discovery%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.12981v1&entry.124074799=Read"},
{"title": "Image Realness Assessment and Localization with Multimodal Features", "author": "Lovish Kaushik and Agnij Biswas and Somdyuti Paul", "abstract": "  A reliable method of quantifying the perceptual realness of AI-generated\nimages and identifying visually inconsistent regions is crucial for practical\nuse of AI-generated images and for improving photorealism of generative AI via\nrealness feedback during training. This paper introduces a framework that\naccomplishes both overall objective realness assessment and local inconsistency\nidentification of AI-generated images using textual descriptions of visual\ninconsistencies generated by vision-language models trained on large datasets\nthat serve as reliable substitutes for human annotations. Our results\ndemonstrate that the proposed multimodal approach improves objective realness\nprediction performance and produces dense realness maps that effectively\ndistinguish between realistic and unrealistic spatial regions.\n", "link": "http://arxiv.org/abs/2509.13289v1", "date": "2025-09-16", "relevancy": 1.759, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5889}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5859}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Realness%20Assessment%20and%20Localization%20with%20Multimodal%20Features&body=Title%3A%20Image%20Realness%20Assessment%20and%20Localization%20with%20Multimodal%20Features%0AAuthor%3A%20Lovish%20Kaushik%20and%20Agnij%20Biswas%20and%20Somdyuti%20Paul%0AAbstract%3A%20%20%20A%20reliable%20method%20of%20quantifying%20the%20perceptual%20realness%20of%20AI-generated%0Aimages%20and%20identifying%20visually%20inconsistent%20regions%20is%20crucial%20for%20practical%0Ause%20of%20AI-generated%20images%20and%20for%20improving%20photorealism%20of%20generative%20AI%20via%0Arealness%20feedback%20during%20training.%20This%20paper%20introduces%20a%20framework%20that%0Aaccomplishes%20both%20overall%20objective%20realness%20assessment%20and%20local%20inconsistency%0Aidentification%20of%20AI-generated%20images%20using%20textual%20descriptions%20of%20visual%0Ainconsistencies%20generated%20by%20vision-language%20models%20trained%20on%20large%20datasets%0Athat%20serve%20as%20reliable%20substitutes%20for%20human%20annotations.%20Our%20results%0Ademonstrate%20that%20the%20proposed%20multimodal%20approach%20improves%20objective%20realness%0Aprediction%20performance%20and%20produces%20dense%20realness%20maps%20that%20effectively%0Adistinguish%20between%20realistic%20and%20unrealistic%20spatial%20regions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Realness%2520Assessment%2520and%2520Localization%2520with%2520Multimodal%2520Features%26entry.906535625%3DLovish%2520Kaushik%2520and%2520Agnij%2520Biswas%2520and%2520Somdyuti%2520Paul%26entry.1292438233%3D%2520%2520A%2520reliable%2520method%2520of%2520quantifying%2520the%2520perceptual%2520realness%2520of%2520AI-generated%250Aimages%2520and%2520identifying%2520visually%2520inconsistent%2520regions%2520is%2520crucial%2520for%2520practical%250Ause%2520of%2520AI-generated%2520images%2520and%2520for%2520improving%2520photorealism%2520of%2520generative%2520AI%2520via%250Arealness%2520feedback%2520during%2520training.%2520This%2520paper%2520introduces%2520a%2520framework%2520that%250Aaccomplishes%2520both%2520overall%2520objective%2520realness%2520assessment%2520and%2520local%2520inconsistency%250Aidentification%2520of%2520AI-generated%2520images%2520using%2520textual%2520descriptions%2520of%2520visual%250Ainconsistencies%2520generated%2520by%2520vision-language%2520models%2520trained%2520on%2520large%2520datasets%250Athat%2520serve%2520as%2520reliable%2520substitutes%2520for%2520human%2520annotations.%2520Our%2520results%250Ademonstrate%2520that%2520the%2520proposed%2520multimodal%2520approach%2520improves%2520objective%2520realness%250Aprediction%2520performance%2520and%2520produces%2520dense%2520realness%2520maps%2520that%2520effectively%250Adistinguish%2520between%2520realistic%2520and%2520unrealistic%2520spatial%2520regions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Realness%20Assessment%20and%20Localization%20with%20Multimodal%20Features&entry.906535625=Lovish%20Kaushik%20and%20Agnij%20Biswas%20and%20Somdyuti%20Paul&entry.1292438233=%20%20A%20reliable%20method%20of%20quantifying%20the%20perceptual%20realness%20of%20AI-generated%0Aimages%20and%20identifying%20visually%20inconsistent%20regions%20is%20crucial%20for%20practical%0Ause%20of%20AI-generated%20images%20and%20for%20improving%20photorealism%20of%20generative%20AI%20via%0Arealness%20feedback%20during%20training.%20This%20paper%20introduces%20a%20framework%20that%0Aaccomplishes%20both%20overall%20objective%20realness%20assessment%20and%20local%20inconsistency%0Aidentification%20of%20AI-generated%20images%20using%20textual%20descriptions%20of%20visual%0Ainconsistencies%20generated%20by%20vision-language%20models%20trained%20on%20large%20datasets%0Athat%20serve%20as%20reliable%20substitutes%20for%20human%20annotations.%20Our%20results%0Ademonstrate%20that%20the%20proposed%20multimodal%20approach%20improves%20objective%20realness%0Aprediction%20performance%20and%20produces%20dense%20realness%20maps%20that%20effectively%0Adistinguish%20between%20realistic%20and%20unrealistic%20spatial%20regions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13289v1&entry.124074799=Read"},
{"title": "Counterfactual Simulatability of LLM Explanations for Generation Tasks", "author": "Marvin Limpijankit and Yanda Chen and Melanie Subbiah and Nicholas Deas and Kathleen McKeown", "abstract": "  LLMs can be unpredictable, as even slight alterations to the prompt can cause\nthe output to change in unexpected ways. Thus, the ability of models to\naccurately explain their behavior is critical, especially in high-stakes\nsettings. One approach for evaluating explanations is counterfactual\nsimulatability, how well an explanation allows users to infer the model's\noutput on related counterfactuals. Counterfactual simulatability has been\npreviously studied for yes/no question answering tasks. We provide a general\nframework for extending this method to generation tasks, using news\nsummarization and medical suggestion as example use cases. We find that while\nLLM explanations do enable users to better predict LLM outputs on\ncounterfactuals in the summarization setting, there is significant room for\nimprovement for medical suggestion. Furthermore, our results suggest that the\nevaluation for counterfactual simulatability may be more appropriate for\nskill-based tasks as opposed to knowledge-based tasks.\n", "link": "http://arxiv.org/abs/2505.21740v2", "date": "2025-09-16", "relevancy": 1.4049, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4705}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4665}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4647}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Counterfactual%20Simulatability%20of%20LLM%20Explanations%20for%20Generation%20Tasks&body=Title%3A%20Counterfactual%20Simulatability%20of%20LLM%20Explanations%20for%20Generation%20Tasks%0AAuthor%3A%20Marvin%20Limpijankit%20and%20Yanda%20Chen%20and%20Melanie%20Subbiah%20and%20Nicholas%20Deas%20and%20Kathleen%20McKeown%0AAbstract%3A%20%20%20LLMs%20can%20be%20unpredictable%2C%20as%20even%20slight%20alterations%20to%20the%20prompt%20can%20cause%0Athe%20output%20to%20change%20in%20unexpected%20ways.%20Thus%2C%20the%20ability%20of%20models%20to%0Aaccurately%20explain%20their%20behavior%20is%20critical%2C%20especially%20in%20high-stakes%0Asettings.%20One%20approach%20for%20evaluating%20explanations%20is%20counterfactual%0Asimulatability%2C%20how%20well%20an%20explanation%20allows%20users%20to%20infer%20the%20model%27s%0Aoutput%20on%20related%20counterfactuals.%20Counterfactual%20simulatability%20has%20been%0Apreviously%20studied%20for%20yes/no%20question%20answering%20tasks.%20We%20provide%20a%20general%0Aframework%20for%20extending%20this%20method%20to%20generation%20tasks%2C%20using%20news%0Asummarization%20and%20medical%20suggestion%20as%20example%20use%20cases.%20We%20find%20that%20while%0ALLM%20explanations%20do%20enable%20users%20to%20better%20predict%20LLM%20outputs%20on%0Acounterfactuals%20in%20the%20summarization%20setting%2C%20there%20is%20significant%20room%20for%0Aimprovement%20for%20medical%20suggestion.%20Furthermore%2C%20our%20results%20suggest%20that%20the%0Aevaluation%20for%20counterfactual%20simulatability%20may%20be%20more%20appropriate%20for%0Askill-based%20tasks%20as%20opposed%20to%20knowledge-based%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21740v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounterfactual%2520Simulatability%2520of%2520LLM%2520Explanations%2520for%2520Generation%2520Tasks%26entry.906535625%3DMarvin%2520Limpijankit%2520and%2520Yanda%2520Chen%2520and%2520Melanie%2520Subbiah%2520and%2520Nicholas%2520Deas%2520and%2520Kathleen%2520McKeown%26entry.1292438233%3D%2520%2520LLMs%2520can%2520be%2520unpredictable%252C%2520as%2520even%2520slight%2520alterations%2520to%2520the%2520prompt%2520can%2520cause%250Athe%2520output%2520to%2520change%2520in%2520unexpected%2520ways.%2520Thus%252C%2520the%2520ability%2520of%2520models%2520to%250Aaccurately%2520explain%2520their%2520behavior%2520is%2520critical%252C%2520especially%2520in%2520high-stakes%250Asettings.%2520One%2520approach%2520for%2520evaluating%2520explanations%2520is%2520counterfactual%250Asimulatability%252C%2520how%2520well%2520an%2520explanation%2520allows%2520users%2520to%2520infer%2520the%2520model%2527s%250Aoutput%2520on%2520related%2520counterfactuals.%2520Counterfactual%2520simulatability%2520has%2520been%250Apreviously%2520studied%2520for%2520yes/no%2520question%2520answering%2520tasks.%2520We%2520provide%2520a%2520general%250Aframework%2520for%2520extending%2520this%2520method%2520to%2520generation%2520tasks%252C%2520using%2520news%250Asummarization%2520and%2520medical%2520suggestion%2520as%2520example%2520use%2520cases.%2520We%2520find%2520that%2520while%250ALLM%2520explanations%2520do%2520enable%2520users%2520to%2520better%2520predict%2520LLM%2520outputs%2520on%250Acounterfactuals%2520in%2520the%2520summarization%2520setting%252C%2520there%2520is%2520significant%2520room%2520for%250Aimprovement%2520for%2520medical%2520suggestion.%2520Furthermore%252C%2520our%2520results%2520suggest%2520that%2520the%250Aevaluation%2520for%2520counterfactual%2520simulatability%2520may%2520be%2520more%2520appropriate%2520for%250Askill-based%2520tasks%2520as%2520opposed%2520to%2520knowledge-based%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21740v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counterfactual%20Simulatability%20of%20LLM%20Explanations%20for%20Generation%20Tasks&entry.906535625=Marvin%20Limpijankit%20and%20Yanda%20Chen%20and%20Melanie%20Subbiah%20and%20Nicholas%20Deas%20and%20Kathleen%20McKeown&entry.1292438233=%20%20LLMs%20can%20be%20unpredictable%2C%20as%20even%20slight%20alterations%20to%20the%20prompt%20can%20cause%0Athe%20output%20to%20change%20in%20unexpected%20ways.%20Thus%2C%20the%20ability%20of%20models%20to%0Aaccurately%20explain%20their%20behavior%20is%20critical%2C%20especially%20in%20high-stakes%0Asettings.%20One%20approach%20for%20evaluating%20explanations%20is%20counterfactual%0Asimulatability%2C%20how%20well%20an%20explanation%20allows%20users%20to%20infer%20the%20model%27s%0Aoutput%20on%20related%20counterfactuals.%20Counterfactual%20simulatability%20has%20been%0Apreviously%20studied%20for%20yes/no%20question%20answering%20tasks.%20We%20provide%20a%20general%0Aframework%20for%20extending%20this%20method%20to%20generation%20tasks%2C%20using%20news%0Asummarization%20and%20medical%20suggestion%20as%20example%20use%20cases.%20We%20find%20that%20while%0ALLM%20explanations%20do%20enable%20users%20to%20better%20predict%20LLM%20outputs%20on%0Acounterfactuals%20in%20the%20summarization%20setting%2C%20there%20is%20significant%20room%20for%0Aimprovement%20for%20medical%20suggestion.%20Furthermore%2C%20our%20results%20suggest%20that%20the%0Aevaluation%20for%20counterfactual%20simulatability%20may%20be%20more%20appropriate%20for%0Askill-based%20tasks%20as%20opposed%20to%20knowledge-based%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21740v2&entry.124074799=Read"},
{"title": "Intelligent Vacuum Thermoforming Process", "author": "Andi Kuswoyo and Christos Margadji and Sebastian W. Pattinson", "abstract": "  Ensuring consistent quality in vacuum thermoforming presents challenges due\nto variations in material properties and tooling configurations. This research\nintroduces a vision-based quality control system to predict and optimise\nprocess parameters, thereby enhancing part quality with minimal data\nrequirements. A comprehensive dataset was developed using visual data from\nvacuum-formed samples subjected to various process parameters, supplemented by\nimage augmentation techniques to improve model training. A k-Nearest Neighbour\nalgorithm was subsequently employed to identify adjustments needed in process\nparameters by mapping low-quality parts to their high-quality counterparts. The\nmodel exhibited strong performance in adjusting heating power, heating time,\nand vacuum time to reduce defects and improve production efficiency.\n", "link": "http://arxiv.org/abs/2509.13250v1", "date": "2025-09-16", "relevancy": 1.9034, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4825}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.4792}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4679}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Intelligent%20Vacuum%20Thermoforming%20Process&body=Title%3A%20Intelligent%20Vacuum%20Thermoforming%20Process%0AAuthor%3A%20Andi%20Kuswoyo%20and%20Christos%20Margadji%20and%20Sebastian%20W.%20Pattinson%0AAbstract%3A%20%20%20Ensuring%20consistent%20quality%20in%20vacuum%20thermoforming%20presents%20challenges%20due%0Ato%20variations%20in%20material%20properties%20and%20tooling%20configurations.%20This%20research%0Aintroduces%20a%20vision-based%20quality%20control%20system%20to%20predict%20and%20optimise%0Aprocess%20parameters%2C%20thereby%20enhancing%20part%20quality%20with%20minimal%20data%0Arequirements.%20A%20comprehensive%20dataset%20was%20developed%20using%20visual%20data%20from%0Avacuum-formed%20samples%20subjected%20to%20various%20process%20parameters%2C%20supplemented%20by%0Aimage%20augmentation%20techniques%20to%20improve%20model%20training.%20A%20k-Nearest%20Neighbour%0Aalgorithm%20was%20subsequently%20employed%20to%20identify%20adjustments%20needed%20in%20process%0Aparameters%20by%20mapping%20low-quality%20parts%20to%20their%20high-quality%20counterparts.%20The%0Amodel%20exhibited%20strong%20performance%20in%20adjusting%20heating%20power%2C%20heating%20time%2C%0Aand%20vacuum%20time%20to%20reduce%20defects%20and%20improve%20production%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13250v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntelligent%2520Vacuum%2520Thermoforming%2520Process%26entry.906535625%3DAndi%2520Kuswoyo%2520and%2520Christos%2520Margadji%2520and%2520Sebastian%2520W.%2520Pattinson%26entry.1292438233%3D%2520%2520Ensuring%2520consistent%2520quality%2520in%2520vacuum%2520thermoforming%2520presents%2520challenges%2520due%250Ato%2520variations%2520in%2520material%2520properties%2520and%2520tooling%2520configurations.%2520This%2520research%250Aintroduces%2520a%2520vision-based%2520quality%2520control%2520system%2520to%2520predict%2520and%2520optimise%250Aprocess%2520parameters%252C%2520thereby%2520enhancing%2520part%2520quality%2520with%2520minimal%2520data%250Arequirements.%2520A%2520comprehensive%2520dataset%2520was%2520developed%2520using%2520visual%2520data%2520from%250Avacuum-formed%2520samples%2520subjected%2520to%2520various%2520process%2520parameters%252C%2520supplemented%2520by%250Aimage%2520augmentation%2520techniques%2520to%2520improve%2520model%2520training.%2520A%2520k-Nearest%2520Neighbour%250Aalgorithm%2520was%2520subsequently%2520employed%2520to%2520identify%2520adjustments%2520needed%2520in%2520process%250Aparameters%2520by%2520mapping%2520low-quality%2520parts%2520to%2520their%2520high-quality%2520counterparts.%2520The%250Amodel%2520exhibited%2520strong%2520performance%2520in%2520adjusting%2520heating%2520power%252C%2520heating%2520time%252C%250Aand%2520vacuum%2520time%2520to%2520reduce%2520defects%2520and%2520improve%2520production%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13250v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Intelligent%20Vacuum%20Thermoforming%20Process&entry.906535625=Andi%20Kuswoyo%20and%20Christos%20Margadji%20and%20Sebastian%20W.%20Pattinson&entry.1292438233=%20%20Ensuring%20consistent%20quality%20in%20vacuum%20thermoforming%20presents%20challenges%20due%0Ato%20variations%20in%20material%20properties%20and%20tooling%20configurations.%20This%20research%0Aintroduces%20a%20vision-based%20quality%20control%20system%20to%20predict%20and%20optimise%0Aprocess%20parameters%2C%20thereby%20enhancing%20part%20quality%20with%20minimal%20data%0Arequirements.%20A%20comprehensive%20dataset%20was%20developed%20using%20visual%20data%20from%0Avacuum-formed%20samples%20subjected%20to%20various%20process%20parameters%2C%20supplemented%20by%0Aimage%20augmentation%20techniques%20to%20improve%20model%20training.%20A%20k-Nearest%20Neighbour%0Aalgorithm%20was%20subsequently%20employed%20to%20identify%20adjustments%20needed%20in%20process%0Aparameters%20by%20mapping%20low-quality%20parts%20to%20their%20high-quality%20counterparts.%20The%0Amodel%20exhibited%20strong%20performance%20in%20adjusting%20heating%20power%2C%20heating%20time%2C%0Aand%20vacuum%20time%20to%20reduce%20defects%20and%20improve%20production%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13250v1&entry.124074799=Read"},
{"title": "Collaborative Loco-Manipulation for Pick-and-Place Tasks with Dynamic\n  Reward Curriculum", "author": "Tianxu An and Flavio De Vincenti and Yuntao Ma and Marco Hutter and Stelian Coros", "abstract": "  We present a hierarchical RL pipeline for training one-armed legged robots to\nperform pick-and-place (P&P) tasks end-to-end -- from approaching the payload\nto releasing it at a target area -- in both single-robot and cooperative\ndual-robot settings. We introduce a novel dynamic reward curriculum that\nenables a single policy to efficiently learn long-horizon P&P operations by\nprogressively guiding the agents through payload-centered sub-objectives.\nCompared to state-of-the-art approaches for long-horizon RL tasks, our method\nimproves training efficiency by 55% and reduces execution time by 18.6% in\nsimulation experiments. In the dual-robot case, we show that our policy enables\neach robot to attend to different components of its observation space at\ndistinct task stages, promoting effective coordination via autonomous attention\nshifts. We validate our method through real-world experiments using ANYmal D\nplatforms in both single- and dual-robot scenarios. To our knowledge, this is\nthe first RL pipeline that tackles the full scope of collaborative P&P with two\nlegged manipulators.\n", "link": "http://arxiv.org/abs/2509.13239v1", "date": "2025-09-16", "relevancy": 1.697, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5881}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5845}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Loco-Manipulation%20for%20Pick-and-Place%20Tasks%20with%20Dynamic%0A%20%20Reward%20Curriculum&body=Title%3A%20Collaborative%20Loco-Manipulation%20for%20Pick-and-Place%20Tasks%20with%20Dynamic%0A%20%20Reward%20Curriculum%0AAuthor%3A%20Tianxu%20An%20and%20Flavio%20De%20Vincenti%20and%20Yuntao%20Ma%20and%20Marco%20Hutter%20and%20Stelian%20Coros%0AAbstract%3A%20%20%20We%20present%20a%20hierarchical%20RL%20pipeline%20for%20training%20one-armed%20legged%20robots%20to%0Aperform%20pick-and-place%20%28P%26P%29%20tasks%20end-to-end%20--%20from%20approaching%20the%20payload%0Ato%20releasing%20it%20at%20a%20target%20area%20--%20in%20both%20single-robot%20and%20cooperative%0Adual-robot%20settings.%20We%20introduce%20a%20novel%20dynamic%20reward%20curriculum%20that%0Aenables%20a%20single%20policy%20to%20efficiently%20learn%20long-horizon%20P%26P%20operations%20by%0Aprogressively%20guiding%20the%20agents%20through%20payload-centered%20sub-objectives.%0ACompared%20to%20state-of-the-art%20approaches%20for%20long-horizon%20RL%20tasks%2C%20our%20method%0Aimproves%20training%20efficiency%20by%2055%25%20and%20reduces%20execution%20time%20by%2018.6%25%20in%0Asimulation%20experiments.%20In%20the%20dual-robot%20case%2C%20we%20show%20that%20our%20policy%20enables%0Aeach%20robot%20to%20attend%20to%20different%20components%20of%20its%20observation%20space%20at%0Adistinct%20task%20stages%2C%20promoting%20effective%20coordination%20via%20autonomous%20attention%0Ashifts.%20We%20validate%20our%20method%20through%20real-world%20experiments%20using%20ANYmal%20D%0Aplatforms%20in%20both%20single-%20and%20dual-robot%20scenarios.%20To%20our%20knowledge%2C%20this%20is%0Athe%20first%20RL%20pipeline%20that%20tackles%20the%20full%20scope%20of%20collaborative%20P%26P%20with%20two%0Alegged%20manipulators.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Loco-Manipulation%2520for%2520Pick-and-Place%2520Tasks%2520with%2520Dynamic%250A%2520%2520Reward%2520Curriculum%26entry.906535625%3DTianxu%2520An%2520and%2520Flavio%2520De%2520Vincenti%2520and%2520Yuntao%2520Ma%2520and%2520Marco%2520Hutter%2520and%2520Stelian%2520Coros%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520hierarchical%2520RL%2520pipeline%2520for%2520training%2520one-armed%2520legged%2520robots%2520to%250Aperform%2520pick-and-place%2520%2528P%2526P%2529%2520tasks%2520end-to-end%2520--%2520from%2520approaching%2520the%2520payload%250Ato%2520releasing%2520it%2520at%2520a%2520target%2520area%2520--%2520in%2520both%2520single-robot%2520and%2520cooperative%250Adual-robot%2520settings.%2520We%2520introduce%2520a%2520novel%2520dynamic%2520reward%2520curriculum%2520that%250Aenables%2520a%2520single%2520policy%2520to%2520efficiently%2520learn%2520long-horizon%2520P%2526P%2520operations%2520by%250Aprogressively%2520guiding%2520the%2520agents%2520through%2520payload-centered%2520sub-objectives.%250ACompared%2520to%2520state-of-the-art%2520approaches%2520for%2520long-horizon%2520RL%2520tasks%252C%2520our%2520method%250Aimproves%2520training%2520efficiency%2520by%252055%2525%2520and%2520reduces%2520execution%2520time%2520by%252018.6%2525%2520in%250Asimulation%2520experiments.%2520In%2520the%2520dual-robot%2520case%252C%2520we%2520show%2520that%2520our%2520policy%2520enables%250Aeach%2520robot%2520to%2520attend%2520to%2520different%2520components%2520of%2520its%2520observation%2520space%2520at%250Adistinct%2520task%2520stages%252C%2520promoting%2520effective%2520coordination%2520via%2520autonomous%2520attention%250Ashifts.%2520We%2520validate%2520our%2520method%2520through%2520real-world%2520experiments%2520using%2520ANYmal%2520D%250Aplatforms%2520in%2520both%2520single-%2520and%2520dual-robot%2520scenarios.%2520To%2520our%2520knowledge%252C%2520this%2520is%250Athe%2520first%2520RL%2520pipeline%2520that%2520tackles%2520the%2520full%2520scope%2520of%2520collaborative%2520P%2526P%2520with%2520two%250Alegged%2520manipulators.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Loco-Manipulation%20for%20Pick-and-Place%20Tasks%20with%20Dynamic%0A%20%20Reward%20Curriculum&entry.906535625=Tianxu%20An%20and%20Flavio%20De%20Vincenti%20and%20Yuntao%20Ma%20and%20Marco%20Hutter%20and%20Stelian%20Coros&entry.1292438233=%20%20We%20present%20a%20hierarchical%20RL%20pipeline%20for%20training%20one-armed%20legged%20robots%20to%0Aperform%20pick-and-place%20%28P%26P%29%20tasks%20end-to-end%20--%20from%20approaching%20the%20payload%0Ato%20releasing%20it%20at%20a%20target%20area%20--%20in%20both%20single-robot%20and%20cooperative%0Adual-robot%20settings.%20We%20introduce%20a%20novel%20dynamic%20reward%20curriculum%20that%0Aenables%20a%20single%20policy%20to%20efficiently%20learn%20long-horizon%20P%26P%20operations%20by%0Aprogressively%20guiding%20the%20agents%20through%20payload-centered%20sub-objectives.%0ACompared%20to%20state-of-the-art%20approaches%20for%20long-horizon%20RL%20tasks%2C%20our%20method%0Aimproves%20training%20efficiency%20by%2055%25%20and%20reduces%20execution%20time%20by%2018.6%25%20in%0Asimulation%20experiments.%20In%20the%20dual-robot%20case%2C%20we%20show%20that%20our%20policy%20enables%0Aeach%20robot%20to%20attend%20to%20different%20components%20of%20its%20observation%20space%20at%0Adistinct%20task%20stages%2C%20promoting%20effective%20coordination%20via%20autonomous%20attention%0Ashifts.%20We%20validate%20our%20method%20through%20real-world%20experiments%20using%20ANYmal%20D%0Aplatforms%20in%20both%20single-%20and%20dual-robot%20scenarios.%20To%20our%20knowledge%2C%20this%20is%0Athe%20first%20RL%20pipeline%20that%20tackles%20the%20full%20scope%20of%20collaborative%20P%26P%20with%20two%0Alegged%20manipulators.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13239v1&entry.124074799=Read"},
{"title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic\n  Data and Scalable Reinforcement Learning", "author": "Kuan Li and Zhongwang Zhang and Huifeng Yin and Rui Ye and Yida Zhao and Liwen Zhang and Litu Ou and Dingchu Zhang and Xixi Wu and Jialong Wu and Xinyu Wang and Zile Qiao and Zhen Zhang and Yong Jiang and Pengjun Xie and Fei Huang and Jingren Zhou", "abstract": "  Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all open-source agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.\n", "link": "http://arxiv.org/abs/2509.13305v1", "date": "2025-09-16", "relevancy": 1.5672, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.529}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5211}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5192}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WebSailor-V2%3A%20Bridging%20the%20Chasm%20to%20Proprietary%20Agents%20via%20Synthetic%0A%20%20Data%20and%20Scalable%20Reinforcement%20Learning&body=Title%3A%20WebSailor-V2%3A%20Bridging%20the%20Chasm%20to%20Proprietary%20Agents%20via%20Synthetic%0A%20%20Data%20and%20Scalable%20Reinforcement%20Learning%0AAuthor%3A%20Kuan%20Li%20and%20Zhongwang%20Zhang%20and%20Huifeng%20Yin%20and%20Rui%20Ye%20and%20Yida%20Zhao%20and%20Liwen%20Zhang%20and%20Litu%20Ou%20and%20Dingchu%20Zhang%20and%20Xixi%20Wu%20and%20Jialong%20Wu%20and%20Xinyu%20Wang%20and%20Zile%20Qiao%20and%20Zhen%20Zhang%20and%20Yong%20Jiang%20and%20Pengjun%20Xie%20and%20Fei%20Huang%20and%20Jingren%20Zhou%0AAbstract%3A%20%20%20Transcending%20human%20cognitive%20limitations%20represents%20a%20critical%20frontier%20in%0ALLM%20training.%20Proprietary%20agentic%20systems%20like%20DeepResearch%20have%20demonstrated%0Asuperhuman%20capabilities%20on%20extremely%20complex%20information-seeking%20benchmarks%0Asuch%20as%20BrowseComp%2C%20a%20feat%20previously%20unattainable.%20We%20posit%20that%20their%20success%0Ahinges%20on%20a%20sophisticated%20reasoning%20pattern%20absent%20in%20open-source%20models%3A%20the%0Aability%20to%20systematically%20reduce%20extreme%20uncertainty%20when%20navigating%20vast%0Ainformation%20landscapes.%20Based%20on%20this%20insight%2C%20we%20introduce%20WebSailor%2C%20a%0Acomplete%20post-training%20methodology%20designed%20to%20instill%20this%20crucial%20capability.%0AOur%20approach%20involves%20generating%20novel%2C%20high-uncertainty%20tasks%20through%0Astructured%20sampling%20and%20information%20obfuscation%2C%20RFT%20cold%20start%2C%20and%20an%0Aefficient%20agentic%20RL%20training%20algorithm%2C%20Duplicating%20Sampling%20Policy%0AOptimization%20%28DUPO%29.%20With%20this%20integrated%20pipeline%2C%20WebSailor%20significantly%0Aoutperforms%20all%20open-source%20agents%20in%20complex%20information-seeking%20tasks%2C%0Amatching%20proprietary%20agents%27%20performance%20and%20closing%20the%20capability%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.13305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWebSailor-V2%253A%2520Bridging%2520the%2520Chasm%2520to%2520Proprietary%2520Agents%2520via%2520Synthetic%250A%2520%2520Data%2520and%2520Scalable%2520Reinforcement%2520Learning%26entry.906535625%3DKuan%2520Li%2520and%2520Zhongwang%2520Zhang%2520and%2520Huifeng%2520Yin%2520and%2520Rui%2520Ye%2520and%2520Yida%2520Zhao%2520and%2520Liwen%2520Zhang%2520and%2520Litu%2520Ou%2520and%2520Dingchu%2520Zhang%2520and%2520Xixi%2520Wu%2520and%2520Jialong%2520Wu%2520and%2520Xinyu%2520Wang%2520and%2520Zile%2520Qiao%2520and%2520Zhen%2520Zhang%2520and%2520Yong%2520Jiang%2520and%2520Pengjun%2520Xie%2520and%2520Fei%2520Huang%2520and%2520Jingren%2520Zhou%26entry.1292438233%3D%2520%2520Transcending%2520human%2520cognitive%2520limitations%2520represents%2520a%2520critical%2520frontier%2520in%250ALLM%2520training.%2520Proprietary%2520agentic%2520systems%2520like%2520DeepResearch%2520have%2520demonstrated%250Asuperhuman%2520capabilities%2520on%2520extremely%2520complex%2520information-seeking%2520benchmarks%250Asuch%2520as%2520BrowseComp%252C%2520a%2520feat%2520previously%2520unattainable.%2520We%2520posit%2520that%2520their%2520success%250Ahinges%2520on%2520a%2520sophisticated%2520reasoning%2520pattern%2520absent%2520in%2520open-source%2520models%253A%2520the%250Aability%2520to%2520systematically%2520reduce%2520extreme%2520uncertainty%2520when%2520navigating%2520vast%250Ainformation%2520landscapes.%2520Based%2520on%2520this%2520insight%252C%2520we%2520introduce%2520WebSailor%252C%2520a%250Acomplete%2520post-training%2520methodology%2520designed%2520to%2520instill%2520this%2520crucial%2520capability.%250AOur%2520approach%2520involves%2520generating%2520novel%252C%2520high-uncertainty%2520tasks%2520through%250Astructured%2520sampling%2520and%2520information%2520obfuscation%252C%2520RFT%2520cold%2520start%252C%2520and%2520an%250Aefficient%2520agentic%2520RL%2520training%2520algorithm%252C%2520Duplicating%2520Sampling%2520Policy%250AOptimization%2520%2528DUPO%2529.%2520With%2520this%2520integrated%2520pipeline%252C%2520WebSailor%2520significantly%250Aoutperforms%2520all%2520open-source%2520agents%2520in%2520complex%2520information-seeking%2520tasks%252C%250Amatching%2520proprietary%2520agents%2527%2520performance%2520and%2520closing%2520the%2520capability%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WebSailor-V2%3A%20Bridging%20the%20Chasm%20to%20Proprietary%20Agents%20via%20Synthetic%0A%20%20Data%20and%20Scalable%20Reinforcement%20Learning&entry.906535625=Kuan%20Li%20and%20Zhongwang%20Zhang%20and%20Huifeng%20Yin%20and%20Rui%20Ye%20and%20Yida%20Zhao%20and%20Liwen%20Zhang%20and%20Litu%20Ou%20and%20Dingchu%20Zhang%20and%20Xixi%20Wu%20and%20Jialong%20Wu%20and%20Xinyu%20Wang%20and%20Zile%20Qiao%20and%20Zhen%20Zhang%20and%20Yong%20Jiang%20and%20Pengjun%20Xie%20and%20Fei%20Huang%20and%20Jingren%20Zhou&entry.1292438233=%20%20Transcending%20human%20cognitive%20limitations%20represents%20a%20critical%20frontier%20in%0ALLM%20training.%20Proprietary%20agentic%20systems%20like%20DeepResearch%20have%20demonstrated%0Asuperhuman%20capabilities%20on%20extremely%20complex%20information-seeking%20benchmarks%0Asuch%20as%20BrowseComp%2C%20a%20feat%20previously%20unattainable.%20We%20posit%20that%20their%20success%0Ahinges%20on%20a%20sophisticated%20reasoning%20pattern%20absent%20in%20open-source%20models%3A%20the%0Aability%20to%20systematically%20reduce%20extreme%20uncertainty%20when%20navigating%20vast%0Ainformation%20landscapes.%20Based%20on%20this%20insight%2C%20we%20introduce%20WebSailor%2C%20a%0Acomplete%20post-training%20methodology%20designed%20to%20instill%20this%20crucial%20capability.%0AOur%20approach%20involves%20generating%20novel%2C%20high-uncertainty%20tasks%20through%0Astructured%20sampling%20and%20information%20obfuscation%2C%20RFT%20cold%20start%2C%20and%20an%0Aefficient%20agentic%20RL%20training%20algorithm%2C%20Duplicating%20Sampling%20Policy%0AOptimization%20%28DUPO%29.%20With%20this%20integrated%20pipeline%2C%20WebSailor%20significantly%0Aoutperforms%20all%20open-source%20agents%20in%20complex%20information-seeking%20tasks%2C%0Amatching%20proprietary%20agents%27%20performance%20and%20closing%20the%20capability%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.13305v1&entry.124074799=Read"},
{"title": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with\n  Hierarchical Chunking", "author": "Wensheng Lu and Keyu Chen and Ruizhi Qiao and Xing Sun", "abstract": "  Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems.\n", "link": "http://arxiv.org/abs/2509.11552v2", "date": "2025-09-16", "relevancy": 1.7863, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4564}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4461}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HiChunk%3A%20Evaluating%20and%20Enhancing%20Retrieval-Augmented%20Generation%20with%0A%20%20Hierarchical%20Chunking&body=Title%3A%20HiChunk%3A%20Evaluating%20and%20Enhancing%20Retrieval-Augmented%20Generation%20with%0A%20%20Hierarchical%20Chunking%0AAuthor%3A%20Wensheng%20Lu%20and%20Keyu%20Chen%20and%20Ruizhi%20Qiao%20and%20Xing%20Sun%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20enhances%20the%20response%20capabilities%20of%0Alanguage%20models%20by%20integrating%20external%20knowledge%20sources.%20However%2C%20document%0Achunking%20as%20an%20important%20part%20of%20RAG%20system%20often%20lacks%20effective%20evaluation%0Atools.%20This%20paper%20first%20analyzes%20why%20existing%20RAG%20evaluation%20benchmarks%20are%0Ainadequate%20for%20assessing%20document%20chunking%20quality%2C%20specifically%20due%20to%0Aevidence%20sparsity.%20Based%20on%20this%20conclusion%2C%20we%20propose%20HiCBench%2C%20which%0Aincludes%20manually%20annotated%20multi-level%20document%20chunking%20points%2C%20synthesized%0Aevidence-dense%20quetion%20answer%28QA%29%20pairs%2C%20and%20their%20corresponding%20evidence%0Asources.%20Additionally%2C%20we%20introduce%20the%20HiChunk%20framework%2C%20a%20multi-level%0Adocument%20structuring%20framework%20based%20on%20fine-tuned%20LLMs%2C%20combined%20with%20the%0AAuto-Merge%20retrieval%20algorithm%20to%20improve%20retrieval%20quality.%20Experiments%0Ademonstrate%20that%20HiCBench%20effectively%20evaluates%20the%20impact%20of%20different%0Achunking%20methods%20across%20the%20entire%20RAG%20pipeline.%20Moreover%2C%20HiChunk%20achieves%0Abetter%20chunking%20quality%20within%20reasonable%20time%20consumption%2C%20thereby%20enhancing%0Athe%20overall%20performance%20of%20RAG%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.11552v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHiChunk%253A%2520Evaluating%2520and%2520Enhancing%2520Retrieval-Augmented%2520Generation%2520with%250A%2520%2520Hierarchical%2520Chunking%26entry.906535625%3DWensheng%2520Lu%2520and%2520Keyu%2520Chen%2520and%2520Ruizhi%2520Qiao%2520and%2520Xing%2520Sun%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520enhances%2520the%2520response%2520capabilities%2520of%250Alanguage%2520models%2520by%2520integrating%2520external%2520knowledge%2520sources.%2520However%252C%2520document%250Achunking%2520as%2520an%2520important%2520part%2520of%2520RAG%2520system%2520often%2520lacks%2520effective%2520evaluation%250Atools.%2520This%2520paper%2520first%2520analyzes%2520why%2520existing%2520RAG%2520evaluation%2520benchmarks%2520are%250Ainadequate%2520for%2520assessing%2520document%2520chunking%2520quality%252C%2520specifically%2520due%2520to%250Aevidence%2520sparsity.%2520Based%2520on%2520this%2520conclusion%252C%2520we%2520propose%2520HiCBench%252C%2520which%250Aincludes%2520manually%2520annotated%2520multi-level%2520document%2520chunking%2520points%252C%2520synthesized%250Aevidence-dense%2520quetion%2520answer%2528QA%2529%2520pairs%252C%2520and%2520their%2520corresponding%2520evidence%250Asources.%2520Additionally%252C%2520we%2520introduce%2520the%2520HiChunk%2520framework%252C%2520a%2520multi-level%250Adocument%2520structuring%2520framework%2520based%2520on%2520fine-tuned%2520LLMs%252C%2520combined%2520with%2520the%250AAuto-Merge%2520retrieval%2520algorithm%2520to%2520improve%2520retrieval%2520quality.%2520Experiments%250Ademonstrate%2520that%2520HiCBench%2520effectively%2520evaluates%2520the%2520impact%2520of%2520different%250Achunking%2520methods%2520across%2520the%2520entire%2520RAG%2520pipeline.%2520Moreover%252C%2520HiChunk%2520achieves%250Abetter%2520chunking%2520quality%2520within%2520reasonable%2520time%2520consumption%252C%2520thereby%2520enhancing%250Athe%2520overall%2520performance%2520of%2520RAG%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.11552v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HiChunk%3A%20Evaluating%20and%20Enhancing%20Retrieval-Augmented%20Generation%20with%0A%20%20Hierarchical%20Chunking&entry.906535625=Wensheng%20Lu%20and%20Keyu%20Chen%20and%20Ruizhi%20Qiao%20and%20Xing%20Sun&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20enhances%20the%20response%20capabilities%20of%0Alanguage%20models%20by%20integrating%20external%20knowledge%20sources.%20However%2C%20document%0Achunking%20as%20an%20important%20part%20of%20RAG%20system%20often%20lacks%20effective%20evaluation%0Atools.%20This%20paper%20first%20analyzes%20why%20existing%20RAG%20evaluation%20benchmarks%20are%0Ainadequate%20for%20assessing%20document%20chunking%20quality%2C%20specifically%20due%20to%0Aevidence%20sparsity.%20Based%20on%20this%20conclusion%2C%20we%20propose%20HiCBench%2C%20which%0Aincludes%20manually%20annotated%20multi-level%20document%20chunking%20points%2C%20synthesized%0Aevidence-dense%20quetion%20answer%28QA%29%20pairs%2C%20and%20their%20corresponding%20evidence%0Asources.%20Additionally%2C%20we%20introduce%20the%20HiChunk%20framework%2C%20a%20multi-level%0Adocument%20structuring%20framework%20based%20on%20fine-tuned%20LLMs%2C%20combined%20with%20the%0AAuto-Merge%20retrieval%20algorithm%20to%20improve%20retrieval%20quality.%20Experiments%0Ademonstrate%20that%20HiCBench%20effectively%20evaluates%20the%20impact%20of%20different%0Achunking%20methods%20across%20the%20entire%20RAG%20pipeline.%20Moreover%2C%20HiChunk%20achieves%0Abetter%20chunking%20quality%20within%20reasonable%20time%20consumption%2C%20thereby%20enhancing%0Athe%20overall%20performance%20of%20RAG%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.11552v2&entry.124074799=Read"},
{"title": "Hybrid Two-Stage Reconstruction of Multiscale Subsurface Flow with\n  Physics-informed Residual Connected Neural Operator", "author": "Peiqi Li and Jie Chen", "abstract": "  The novel neural networks show great potential in solving partial\ndifferential equations. For single-phase flow problems in subsurface porous\nmedia with high-contrast coefficients, the key is to develop neural operators\nwith accurate reconstruction capability and strict adherence to physical laws.\nIn this study, we proposed a hybrid two-stage framework that uses multiscale\nbasis functions and physics-guided deep learning to solve the Darcy flow\nproblem in high-contrast fractured porous media. In the first stage, a\ndata-driven model is used to reconstruct the multiscale basis function based on\nthe permeability field to achieve effective dimensionality reduction while\npreserving the necessary multiscale features. In the second stage, the\nphysics-informed neural network, together with Transformer-based global\ninformation extractor is used to reconstruct the pressure field by integrating\nthe physical constraints derived from the Darcy equation, ensuring consistency\nwith the physical laws of the real world. The model was evaluated on datasets\nwith different combinations of permeability and basis functions and performed\nwell in terms of reconstruction accuracy. Specifically, the framework achieves\nR2 values above 0.9 in terms of basis function fitting and pressure\nreconstruction, and the residual indicator is on the order of $1\\times\n10^{-4}$. These results validate the ability of the proposed framework to\nachieve accurate reconstruction while maintaining physical consistency.\n", "link": "http://arxiv.org/abs/2501.13271v2", "date": "2025-09-16", "relevancy": 1.5994, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5923}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5197}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5074}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Two-Stage%20Reconstruction%20of%20Multiscale%20Subsurface%20Flow%20with%0A%20%20Physics-informed%20Residual%20Connected%20Neural%20Operator&body=Title%3A%20Hybrid%20Two-Stage%20Reconstruction%20of%20Multiscale%20Subsurface%20Flow%20with%0A%20%20Physics-informed%20Residual%20Connected%20Neural%20Operator%0AAuthor%3A%20Peiqi%20Li%20and%20Jie%20Chen%0AAbstract%3A%20%20%20The%20novel%20neural%20networks%20show%20great%20potential%20in%20solving%20partial%0Adifferential%20equations.%20For%20single-phase%20flow%20problems%20in%20subsurface%20porous%0Amedia%20with%20high-contrast%20coefficients%2C%20the%20key%20is%20to%20develop%20neural%20operators%0Awith%20accurate%20reconstruction%20capability%20and%20strict%20adherence%20to%20physical%20laws.%0AIn%20this%20study%2C%20we%20proposed%20a%20hybrid%20two-stage%20framework%20that%20uses%20multiscale%0Abasis%20functions%20and%20physics-guided%20deep%20learning%20to%20solve%20the%20Darcy%20flow%0Aproblem%20in%20high-contrast%20fractured%20porous%20media.%20In%20the%20first%20stage%2C%20a%0Adata-driven%20model%20is%20used%20to%20reconstruct%20the%20multiscale%20basis%20function%20based%20on%0Athe%20permeability%20field%20to%20achieve%20effective%20dimensionality%20reduction%20while%0Apreserving%20the%20necessary%20multiscale%20features.%20In%20the%20second%20stage%2C%20the%0Aphysics-informed%20neural%20network%2C%20together%20with%20Transformer-based%20global%0Ainformation%20extractor%20is%20used%20to%20reconstruct%20the%20pressure%20field%20by%20integrating%0Athe%20physical%20constraints%20derived%20from%20the%20Darcy%20equation%2C%20ensuring%20consistency%0Awith%20the%20physical%20laws%20of%20the%20real%20world.%20The%20model%20was%20evaluated%20on%20datasets%0Awith%20different%20combinations%20of%20permeability%20and%20basis%20functions%20and%20performed%0Awell%20in%20terms%20of%20reconstruction%20accuracy.%20Specifically%2C%20the%20framework%20achieves%0AR2%20values%20above%200.9%20in%20terms%20of%20basis%20function%20fitting%20and%20pressure%0Areconstruction%2C%20and%20the%20residual%20indicator%20is%20on%20the%20order%20of%20%241%5Ctimes%0A10%5E%7B-4%7D%24.%20These%20results%20validate%20the%20ability%20of%20the%20proposed%20framework%20to%0Aachieve%20accurate%20reconstruction%20while%20maintaining%20physical%20consistency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2501.13271v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Two-Stage%2520Reconstruction%2520of%2520Multiscale%2520Subsurface%2520Flow%2520with%250A%2520%2520Physics-informed%2520Residual%2520Connected%2520Neural%2520Operator%26entry.906535625%3DPeiqi%2520Li%2520and%2520Jie%2520Chen%26entry.1292438233%3D%2520%2520The%2520novel%2520neural%2520networks%2520show%2520great%2520potential%2520in%2520solving%2520partial%250Adifferential%2520equations.%2520For%2520single-phase%2520flow%2520problems%2520in%2520subsurface%2520porous%250Amedia%2520with%2520high-contrast%2520coefficients%252C%2520the%2520key%2520is%2520to%2520develop%2520neural%2520operators%250Awith%2520accurate%2520reconstruction%2520capability%2520and%2520strict%2520adherence%2520to%2520physical%2520laws.%250AIn%2520this%2520study%252C%2520we%2520proposed%2520a%2520hybrid%2520two-stage%2520framework%2520that%2520uses%2520multiscale%250Abasis%2520functions%2520and%2520physics-guided%2520deep%2520learning%2520to%2520solve%2520the%2520Darcy%2520flow%250Aproblem%2520in%2520high-contrast%2520fractured%2520porous%2520media.%2520In%2520the%2520first%2520stage%252C%2520a%250Adata-driven%2520model%2520is%2520used%2520to%2520reconstruct%2520the%2520multiscale%2520basis%2520function%2520based%2520on%250Athe%2520permeability%2520field%2520to%2520achieve%2520effective%2520dimensionality%2520reduction%2520while%250Apreserving%2520the%2520necessary%2520multiscale%2520features.%2520In%2520the%2520second%2520stage%252C%2520the%250Aphysics-informed%2520neural%2520network%252C%2520together%2520with%2520Transformer-based%2520global%250Ainformation%2520extractor%2520is%2520used%2520to%2520reconstruct%2520the%2520pressure%2520field%2520by%2520integrating%250Athe%2520physical%2520constraints%2520derived%2520from%2520the%2520Darcy%2520equation%252C%2520ensuring%2520consistency%250Awith%2520the%2520physical%2520laws%2520of%2520the%2520real%2520world.%2520The%2520model%2520was%2520evaluated%2520on%2520datasets%250Awith%2520different%2520combinations%2520of%2520permeability%2520and%2520basis%2520functions%2520and%2520performed%250Awell%2520in%2520terms%2520of%2520reconstruction%2520accuracy.%2520Specifically%252C%2520the%2520framework%2520achieves%250AR2%2520values%2520above%25200.9%2520in%2520terms%2520of%2520basis%2520function%2520fitting%2520and%2520pressure%250Areconstruction%252C%2520and%2520the%2520residual%2520indicator%2520is%2520on%2520the%2520order%2520of%2520%25241%255Ctimes%250A10%255E%257B-4%257D%2524.%2520These%2520results%2520validate%2520the%2520ability%2520of%2520the%2520proposed%2520framework%2520to%250Aachieve%2520accurate%2520reconstruction%2520while%2520maintaining%2520physical%2520consistency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.13271v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Two-Stage%20Reconstruction%20of%20Multiscale%20Subsurface%20Flow%20with%0A%20%20Physics-informed%20Residual%20Connected%20Neural%20Operator&entry.906535625=Peiqi%20Li%20and%20Jie%20Chen&entry.1292438233=%20%20The%20novel%20neural%20networks%20show%20great%20potential%20in%20solving%20partial%0Adifferential%20equations.%20For%20single-phase%20flow%20problems%20in%20subsurface%20porous%0Amedia%20with%20high-contrast%20coefficients%2C%20the%20key%20is%20to%20develop%20neural%20operators%0Awith%20accurate%20reconstruction%20capability%20and%20strict%20adherence%20to%20physical%20laws.%0AIn%20this%20study%2C%20we%20proposed%20a%20hybrid%20two-stage%20framework%20that%20uses%20multiscale%0Abasis%20functions%20and%20physics-guided%20deep%20learning%20to%20solve%20the%20Darcy%20flow%0Aproblem%20in%20high-contrast%20fractured%20porous%20media.%20In%20the%20first%20stage%2C%20a%0Adata-driven%20model%20is%20used%20to%20reconstruct%20the%20multiscale%20basis%20function%20based%20on%0Athe%20permeability%20field%20to%20achieve%20effective%20dimensionality%20reduction%20while%0Apreserving%20the%20necessary%20multiscale%20features.%20In%20the%20second%20stage%2C%20the%0Aphysics-informed%20neural%20network%2C%20together%20with%20Transformer-based%20global%0Ainformation%20extractor%20is%20used%20to%20reconstruct%20the%20pressure%20field%20by%20integrating%0Athe%20physical%20constraints%20derived%20from%20the%20Darcy%20equation%2C%20ensuring%20consistency%0Awith%20the%20physical%20laws%20of%20the%20real%20world.%20The%20model%20was%20evaluated%20on%20datasets%0Awith%20different%20combinations%20of%20permeability%20and%20basis%20functions%20and%20performed%0Awell%20in%20terms%20of%20reconstruction%20accuracy.%20Specifically%2C%20the%20framework%20achieves%0AR2%20values%20above%200.9%20in%20terms%20of%20basis%20function%20fitting%20and%20pressure%0Areconstruction%2C%20and%20the%20residual%20indicator%20is%20on%20the%20order%20of%20%241%5Ctimes%0A10%5E%7B-4%7D%24.%20These%20results%20validate%20the%20ability%20of%20the%20proposed%20framework%20to%0Aachieve%20accurate%20reconstruction%20while%20maintaining%20physical%20consistency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2501.13271v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


