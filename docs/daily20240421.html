<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240418.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "NeRF-MAE: Masked AutoEncoders for Self-Supervised 3D Representation\n  Learning for Neural Radiance Fields", "author": "Muhammad Zubair Irshad and Sergey Zakahrov and Vitor Guizilini and Adrien Gaidon and Zsolt Kira and Rares Ambrus", "abstract": "  Neural fields excel in computer vision and robotics due to their ability to\nunderstand the 3D visual world such as inferring semantics, geometry, and\ndynamics. Given the capabilities of neural fields in densely representing a 3D\nscene from 2D images, we ask the question: Can we scale their self-supervised\npretraining, specifically using masked autoencoders, to generate effective 3D\nrepresentations from posed RGB images. Owing to the astounding success of\nextending transformers to novel data modalities, we employ standard 3D Vision\nTransformers to suit the unique formulation of NeRFs. We leverage NeRF's\nvolumetric grid as a dense input to the transformer, contrasting it with other\n3D representations such as pointclouds where the information density can be\nuneven, and the representation is irregular. Due to the difficulty of applying\nmasked autoencoders to an implicit representation, such as NeRF, we opt for\nextracting an explicit representation that canonicalizes scenes across domains\nby employing the camera trajectory for sampling. Our goal is made possible by\nmasking random patches from NeRF's radiance and density grid and employing a\nstandard 3D Swin Transformer to reconstruct the masked patches. In doing so,\nthe model can learn the semantic and spatial structure of complete scenes. We\npretrain this representation at scale on our proposed curated posed-RGB data,\ntotaling over 1.6 million images. Once pretrained, the encoder is used for\neffective 3D transfer learning. Our novel self-supervised pretraining for\nNeRFs, NeRF-MAE, scales remarkably well and improves performance on various\nchallenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,\nNeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF\nscene understanding baselines on Front3D and ScanNet datasets with an absolute\nperformance improvement of over 20% AP50 and 8% AP25 for 3D object detection.\n", "link": "http://arxiv.org/abs/2404.01300v2", "date": "2024-04-18", "relevancy": 2.9468, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6267}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5724}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.569}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NeRF-MAE%3A%20Masked%20AutoEncoders%20for%20Self-Supervised%203D%20Representation%0A%20%20Learning%20for%20Neural%20Radiance%20Fields&body=Title%3A%20NeRF-MAE%3A%20Masked%20AutoEncoders%20for%20Self-Supervised%203D%20Representation%0A%20%20Learning%20for%20Neural%20Radiance%20Fields%0AAuthor%3A%20Muhammad%20Zubair%20Irshad%20and%20Sergey%20Zakahrov%20and%20Vitor%20Guizilini%20and%20Adrien%20Gaidon%20and%20Zsolt%20Kira%20and%20Rares%20Ambrus%0AAbstract%3A%20%20%20Neural%20fields%20excel%20in%20computer%20vision%20and%20robotics%20due%20to%20their%20ability%20to%0Aunderstand%20the%203D%20visual%20world%20such%20as%20inferring%20semantics%2C%20geometry%2C%20and%0Adynamics.%20Given%20the%20capabilities%20of%20neural%20fields%20in%20densely%20representing%20a%203D%0Ascene%20from%202D%20images%2C%20we%20ask%20the%20question%3A%20Can%20we%20scale%20their%20self-supervised%0Apretraining%2C%20specifically%20using%20masked%20autoencoders%2C%20to%20generate%20effective%203D%0Arepresentations%20from%20posed%20RGB%20images.%20Owing%20to%20the%20astounding%20success%20of%0Aextending%20transformers%20to%20novel%20data%20modalities%2C%20we%20employ%20standard%203D%20Vision%0ATransformers%20to%20suit%20the%20unique%20formulation%20of%20NeRFs.%20We%20leverage%20NeRF%27s%0Avolumetric%20grid%20as%20a%20dense%20input%20to%20the%20transformer%2C%20contrasting%20it%20with%20other%0A3D%20representations%20such%20as%20pointclouds%20where%20the%20information%20density%20can%20be%0Auneven%2C%20and%20the%20representation%20is%20irregular.%20Due%20to%20the%20difficulty%20of%20applying%0Amasked%20autoencoders%20to%20an%20implicit%20representation%2C%20such%20as%20NeRF%2C%20we%20opt%20for%0Aextracting%20an%20explicit%20representation%20that%20canonicalizes%20scenes%20across%20domains%0Aby%20employing%20the%20camera%20trajectory%20for%20sampling.%20Our%20goal%20is%20made%20possible%20by%0Amasking%20random%20patches%20from%20NeRF%27s%20radiance%20and%20density%20grid%20and%20employing%20a%0Astandard%203D%20Swin%20Transformer%20to%20reconstruct%20the%20masked%20patches.%20In%20doing%20so%2C%0Athe%20model%20can%20learn%20the%20semantic%20and%20spatial%20structure%20of%20complete%20scenes.%20We%0Apretrain%20this%20representation%20at%20scale%20on%20our%20proposed%20curated%20posed-RGB%20data%2C%0Atotaling%20over%201.6%20million%20images.%20Once%20pretrained%2C%20the%20encoder%20is%20used%20for%0Aeffective%203D%20transfer%20learning.%20Our%20novel%20self-supervised%20pretraining%20for%0ANeRFs%2C%20NeRF-MAE%2C%20scales%20remarkably%20well%20and%20improves%20performance%20on%20various%0Achallenging%203D%20tasks.%20Utilizing%20unlabeled%20posed%202D%20data%20for%20pretraining%2C%0ANeRF-MAE%20significantly%20outperforms%20self-supervised%203D%20pretraining%20and%20NeRF%0Ascene%20understanding%20baselines%20on%20Front3D%20and%20ScanNet%20datasets%20with%20an%20absolute%0Aperformance%20improvement%20of%20over%2020%25%20AP50%20and%208%25%20AP25%20for%203D%20object%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01300v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRF-MAE%3A%20Masked%20AutoEncoders%20for%20Self-Supervised%203D%20Representation%0A%20%20Learning%20for%20Neural%20Radiance%20Fields&entry.906535625=Muhammad%20Zubair%20Irshad%20and%20Sergey%20Zakahrov%20and%20Vitor%20Guizilini%20and%20Adrien%20Gaidon%20and%20Zsolt%20Kira%20and%20Rares%20Ambrus&entry.1292438233=%20%20Neural%20fields%20excel%20in%20computer%20vision%20and%20robotics%20due%20to%20their%20ability%20to%0Aunderstand%20the%203D%20visual%20world%20such%20as%20inferring%20semantics%2C%20geometry%2C%20and%0Adynamics.%20Given%20the%20capabilities%20of%20neural%20fields%20in%20densely%20representing%20a%203D%0Ascene%20from%202D%20images%2C%20we%20ask%20the%20question%3A%20Can%20we%20scale%20their%20self-supervised%0Apretraining%2C%20specifically%20using%20masked%20autoencoders%2C%20to%20generate%20effective%203D%0Arepresentations%20from%20posed%20RGB%20images.%20Owing%20to%20the%20astounding%20success%20of%0Aextending%20transformers%20to%20novel%20data%20modalities%2C%20we%20employ%20standard%203D%20Vision%0ATransformers%20to%20suit%20the%20unique%20formulation%20of%20NeRFs.%20We%20leverage%20NeRF%27s%0Avolumetric%20grid%20as%20a%20dense%20input%20to%20the%20transformer%2C%20contrasting%20it%20with%20other%0A3D%20representations%20such%20as%20pointclouds%20where%20the%20information%20density%20can%20be%0Auneven%2C%20and%20the%20representation%20is%20irregular.%20Due%20to%20the%20difficulty%20of%20applying%0Amasked%20autoencoders%20to%20an%20implicit%20representation%2C%20such%20as%20NeRF%2C%20we%20opt%20for%0Aextracting%20an%20explicit%20representation%20that%20canonicalizes%20scenes%20across%20domains%0Aby%20employing%20the%20camera%20trajectory%20for%20sampling.%20Our%20goal%20is%20made%20possible%20by%0Amasking%20random%20patches%20from%20NeRF%27s%20radiance%20and%20density%20grid%20and%20employing%20a%0Astandard%203D%20Swin%20Transformer%20to%20reconstruct%20the%20masked%20patches.%20In%20doing%20so%2C%0Athe%20model%20can%20learn%20the%20semantic%20and%20spatial%20structure%20of%20complete%20scenes.%20We%0Apretrain%20this%20representation%20at%20scale%20on%20our%20proposed%20curated%20posed-RGB%20data%2C%0Atotaling%20over%201.6%20million%20images.%20Once%20pretrained%2C%20the%20encoder%20is%20used%20for%0Aeffective%203D%20transfer%20learning.%20Our%20novel%20self-supervised%20pretraining%20for%0ANeRFs%2C%20NeRF-MAE%2C%20scales%20remarkably%20well%20and%20improves%20performance%20on%20various%0Achallenging%203D%20tasks.%20Utilizing%20unlabeled%20posed%202D%20data%20for%20pretraining%2C%0ANeRF-MAE%20significantly%20outperforms%20self-supervised%203D%20pretraining%20and%20NeRF%0Ascene%20understanding%20baselines%20on%20Front3D%20and%20ScanNet%20datasets%20with%20an%20absolute%0Aperformance%20improvement%20of%20over%2020%25%20AP50%20and%208%25%20AP25%20for%203D%20object%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01300v2&entry.124074799=Read"},
{"title": "Point-In-Context: Understanding Point Cloud via In-Context Learning", "author": "Mengyuan Liu and Zhongbin Fang and Xia Li and Joachim M. Buhmann and Xiangtai Li and Chen Change Loy", "abstract": "  With the emergence of large-scale models trained on diverse datasets,\nin-context learning has emerged as a promising paradigm for multitasking,\nnotably in natural language processing and image processing. However, its\napplication in 3D point cloud tasks remains largely unexplored. In this work,\nwe introduce Point-In-Context (PIC), a novel framework for 3D point cloud\nunderstanding via in-context learning. We address the technical challenge of\neffectively extending masked point modeling to 3D point clouds by introducing a\nJoint Sampling module and proposing a vanilla version of PIC called\nPoint-In-Context-Generalist (PIC-G). PIC-G is designed as a generalist model\nfor various 3D point cloud tasks, with inputs and outputs modeled as\ncoordinates. In this paradigm, the challenging segmentation task is achieved by\nassigning label points with XYZ coordinates for each category; the final\nprediction is then chosen based on the label point closest to the predictions.\nTo break the limitation by the fixed label-coordinate assignment, which has\npoor generalization upon novel classes, we propose two novel training\nstrategies, In-Context Labeling and In-Context Enhancing, forming an extended\nversion of PIC named Point-In-Context-Segmenter (PIC-S), targeting improving\ndynamic context labeling and model training. By utilizing dynamic in-context\nlabels and extra in-context pairs, PIC-S achieves enhanced performance and\ngeneralization capability in and across part segmentation datasets. PIC is a\ngeneral framework so that other tasks or datasets can be seamlessly introduced\ninto our PIC through a unified data format. We conduct extensive experiments to\nvalidate the versatility and adaptability of our proposed methods in handling a\nwide range of tasks and segmenting multi-datasets. Our PIC-S is capable of\ngeneralizing unseen datasets and performing novel part segmentation by\ncustomizing prompts.\n", "link": "http://arxiv.org/abs/2404.12352v1", "date": "2024-04-18", "relevancy": 2.89, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6016}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5731}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5593}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Point-In-Context%3A%20Understanding%20Point%20Cloud%20via%20In-Context%20Learning&body=Title%3A%20Point-In-Context%3A%20Understanding%20Point%20Cloud%20via%20In-Context%20Learning%0AAuthor%3A%20Mengyuan%20Liu%20and%20Zhongbin%20Fang%20and%20Xia%20Li%20and%20Joachim%20M.%20Buhmann%20and%20Xiangtai%20Li%20and%20Chen%20Change%20Loy%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20large-scale%20models%20trained%20on%20diverse%20datasets%2C%0Ain-context%20learning%20has%20emerged%20as%20a%20promising%20paradigm%20for%20multitasking%2C%0Anotably%20in%20natural%20language%20processing%20and%20image%20processing.%20However%2C%20its%0Aapplication%20in%203D%20point%20cloud%20tasks%20remains%20largely%20unexplored.%20In%20this%20work%2C%0Awe%20introduce%20Point-In-Context%20%28PIC%29%2C%20a%20novel%20framework%20for%203D%20point%20cloud%0Aunderstanding%20via%20in-context%20learning.%20We%20address%20the%20technical%20challenge%20of%0Aeffectively%20extending%20masked%20point%20modeling%20to%203D%20point%20clouds%20by%20introducing%20a%0AJoint%20Sampling%20module%20and%20proposing%20a%20vanilla%20version%20of%20PIC%20called%0APoint-In-Context-Generalist%20%28PIC-G%29.%20PIC-G%20is%20designed%20as%20a%20generalist%20model%0Afor%20various%203D%20point%20cloud%20tasks%2C%20with%20inputs%20and%20outputs%20modeled%20as%0Acoordinates.%20In%20this%20paradigm%2C%20the%20challenging%20segmentation%20task%20is%20achieved%20by%0Aassigning%20label%20points%20with%20XYZ%20coordinates%20for%20each%20category%3B%20the%20final%0Aprediction%20is%20then%20chosen%20based%20on%20the%20label%20point%20closest%20to%20the%20predictions.%0ATo%20break%20the%20limitation%20by%20the%20fixed%20label-coordinate%20assignment%2C%20which%20has%0Apoor%20generalization%20upon%20novel%20classes%2C%20we%20propose%20two%20novel%20training%0Astrategies%2C%20In-Context%20Labeling%20and%20In-Context%20Enhancing%2C%20forming%20an%20extended%0Aversion%20of%20PIC%20named%20Point-In-Context-Segmenter%20%28PIC-S%29%2C%20targeting%20improving%0Adynamic%20context%20labeling%20and%20model%20training.%20By%20utilizing%20dynamic%20in-context%0Alabels%20and%20extra%20in-context%20pairs%2C%20PIC-S%20achieves%20enhanced%20performance%20and%0Ageneralization%20capability%20in%20and%20across%20part%20segmentation%20datasets.%20PIC%20is%20a%0Ageneral%20framework%20so%20that%20other%20tasks%20or%20datasets%20can%20be%20seamlessly%20introduced%0Ainto%20our%20PIC%20through%20a%20unified%20data%20format.%20We%20conduct%20extensive%20experiments%20to%0Avalidate%20the%20versatility%20and%20adaptability%20of%20our%20proposed%20methods%20in%20handling%20a%0Awide%20range%20of%20tasks%20and%20segmenting%20multi-datasets.%20Our%20PIC-S%20is%20capable%20of%0Ageneralizing%20unseen%20datasets%20and%20performing%20novel%20part%20segmentation%20by%0Acustomizing%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12352v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Point-In-Context%3A%20Understanding%20Point%20Cloud%20via%20In-Context%20Learning&entry.906535625=Mengyuan%20Liu%20and%20Zhongbin%20Fang%20and%20Xia%20Li%20and%20Joachim%20M.%20Buhmann%20and%20Xiangtai%20Li%20and%20Chen%20Change%20Loy&entry.1292438233=%20%20With%20the%20emergence%20of%20large-scale%20models%20trained%20on%20diverse%20datasets%2C%0Ain-context%20learning%20has%20emerged%20as%20a%20promising%20paradigm%20for%20multitasking%2C%0Anotably%20in%20natural%20language%20processing%20and%20image%20processing.%20However%2C%20its%0Aapplication%20in%203D%20point%20cloud%20tasks%20remains%20largely%20unexplored.%20In%20this%20work%2C%0Awe%20introduce%20Point-In-Context%20%28PIC%29%2C%20a%20novel%20framework%20for%203D%20point%20cloud%0Aunderstanding%20via%20in-context%20learning.%20We%20address%20the%20technical%20challenge%20of%0Aeffectively%20extending%20masked%20point%20modeling%20to%203D%20point%20clouds%20by%20introducing%20a%0AJoint%20Sampling%20module%20and%20proposing%20a%20vanilla%20version%20of%20PIC%20called%0APoint-In-Context-Generalist%20%28PIC-G%29.%20PIC-G%20is%20designed%20as%20a%20generalist%20model%0Afor%20various%203D%20point%20cloud%20tasks%2C%20with%20inputs%20and%20outputs%20modeled%20as%0Acoordinates.%20In%20this%20paradigm%2C%20the%20challenging%20segmentation%20task%20is%20achieved%20by%0Aassigning%20label%20points%20with%20XYZ%20coordinates%20for%20each%20category%3B%20the%20final%0Aprediction%20is%20then%20chosen%20based%20on%20the%20label%20point%20closest%20to%20the%20predictions.%0ATo%20break%20the%20limitation%20by%20the%20fixed%20label-coordinate%20assignment%2C%20which%20has%0Apoor%20generalization%20upon%20novel%20classes%2C%20we%20propose%20two%20novel%20training%0Astrategies%2C%20In-Context%20Labeling%20and%20In-Context%20Enhancing%2C%20forming%20an%20extended%0Aversion%20of%20PIC%20named%20Point-In-Context-Segmenter%20%28PIC-S%29%2C%20targeting%20improving%0Adynamic%20context%20labeling%20and%20model%20training.%20By%20utilizing%20dynamic%20in-context%0Alabels%20and%20extra%20in-context%20pairs%2C%20PIC-S%20achieves%20enhanced%20performance%20and%0Ageneralization%20capability%20in%20and%20across%20part%20segmentation%20datasets.%20PIC%20is%20a%0Ageneral%20framework%20so%20that%20other%20tasks%20or%20datasets%20can%20be%20seamlessly%20introduced%0Ainto%20our%20PIC%20through%20a%20unified%20data%20format.%20We%20conduct%20extensive%20experiments%20to%0Avalidate%20the%20versatility%20and%20adaptability%20of%20our%20proposed%20methods%20in%20handling%20a%0Awide%20range%20of%20tasks%20and%20segmenting%20multi-datasets.%20Our%20PIC-S%20is%20capable%20of%0Ageneralizing%20unseen%20datasets%20and%20performing%20novel%20part%20segmentation%20by%0Acustomizing%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12352v1&entry.124074799=Read"},
{"title": "DeepLocalization: Using change point detection for Temporal Action\n  Localization", "author": "Mohammed Shaiqur Rahman and Ibne Farabi Shihab and Lynna Chu and Anuj Sharma", "abstract": "  In this study, we introduce DeepLocalization, an innovative framework devised\nfor the real-time localization of actions tailored explicitly for monitoring\ndriver behavior. Utilizing the power of advanced deep learning methodologies,\nour objective is to tackle the critical issue of distracted driving-a\nsignificant factor contributing to road accidents. Our strategy employs a dual\napproach: leveraging Graph-Based Change-Point Detection for pinpointing actions\nin time alongside a Video Large Language Model (Video-LLM) for precisely\ncategorizing activities. Through careful prompt engineering, we customize the\nVideo-LLM to adeptly handle driving activities' nuances, ensuring its\nclassification efficacy even with sparse data. Engineered to be lightweight,\nour framework is optimized for consumer-grade GPUs, making it vastly applicable\nin practical scenarios. We subjected our method to rigorous testing on the\nSynDD2 dataset, a complex benchmark for distracted driving behaviors, where it\ndemonstrated commendable performance-achieving 57.5% accuracy in event\nclassification and 51% in event detection. These outcomes underscore the\nsubstantial promise of DeepLocalization in accurately identifying diverse\ndriver behaviors and their temporal occurrences, all within the bounds of\nlimited computational resources.\n", "link": "http://arxiv.org/abs/2404.12258v1", "date": "2024-04-18", "relevancy": 2.8492, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5873}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5692}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.553}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DeepLocalization%3A%20Using%20change%20point%20detection%20for%20Temporal%20Action%0A%20%20Localization&body=Title%3A%20DeepLocalization%3A%20Using%20change%20point%20detection%20for%20Temporal%20Action%0A%20%20Localization%0AAuthor%3A%20Mohammed%20Shaiqur%20Rahman%20and%20Ibne%20Farabi%20Shihab%20and%20Lynna%20Chu%20and%20Anuj%20Sharma%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20introduce%20DeepLocalization%2C%20an%20innovative%20framework%20devised%0Afor%20the%20real-time%20localization%20of%20actions%20tailored%20explicitly%20for%20monitoring%0Adriver%20behavior.%20Utilizing%20the%20power%20of%20advanced%20deep%20learning%20methodologies%2C%0Aour%20objective%20is%20to%20tackle%20the%20critical%20issue%20of%20distracted%20driving-a%0Asignificant%20factor%20contributing%20to%20road%20accidents.%20Our%20strategy%20employs%20a%20dual%0Aapproach%3A%20leveraging%20Graph-Based%20Change-Point%20Detection%20for%20pinpointing%20actions%0Ain%20time%20alongside%20a%20Video%20Large%20Language%20Model%20%28Video-LLM%29%20for%20precisely%0Acategorizing%20activities.%20Through%20careful%20prompt%20engineering%2C%20we%20customize%20the%0AVideo-LLM%20to%20adeptly%20handle%20driving%20activities%27%20nuances%2C%20ensuring%20its%0Aclassification%20efficacy%20even%20with%20sparse%20data.%20Engineered%20to%20be%20lightweight%2C%0Aour%20framework%20is%20optimized%20for%20consumer-grade%20GPUs%2C%20making%20it%20vastly%20applicable%0Ain%20practical%20scenarios.%20We%20subjected%20our%20method%20to%20rigorous%20testing%20on%20the%0ASynDD2%20dataset%2C%20a%20complex%20benchmark%20for%20distracted%20driving%20behaviors%2C%20where%20it%0Ademonstrated%20commendable%20performance-achieving%2057.5%25%20accuracy%20in%20event%0Aclassification%20and%2051%25%20in%20event%20detection.%20These%20outcomes%20underscore%20the%0Asubstantial%20promise%20of%20DeepLocalization%20in%20accurately%20identifying%20diverse%0Adriver%20behaviors%20and%20their%20temporal%20occurrences%2C%20all%20within%20the%20bounds%20of%0Alimited%20computational%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12258v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeepLocalization%3A%20Using%20change%20point%20detection%20for%20Temporal%20Action%0A%20%20Localization&entry.906535625=Mohammed%20Shaiqur%20Rahman%20and%20Ibne%20Farabi%20Shihab%20and%20Lynna%20Chu%20and%20Anuj%20Sharma&entry.1292438233=%20%20In%20this%20study%2C%20we%20introduce%20DeepLocalization%2C%20an%20innovative%20framework%20devised%0Afor%20the%20real-time%20localization%20of%20actions%20tailored%20explicitly%20for%20monitoring%0Adriver%20behavior.%20Utilizing%20the%20power%20of%20advanced%20deep%20learning%20methodologies%2C%0Aour%20objective%20is%20to%20tackle%20the%20critical%20issue%20of%20distracted%20driving-a%0Asignificant%20factor%20contributing%20to%20road%20accidents.%20Our%20strategy%20employs%20a%20dual%0Aapproach%3A%20leveraging%20Graph-Based%20Change-Point%20Detection%20for%20pinpointing%20actions%0Ain%20time%20alongside%20a%20Video%20Large%20Language%20Model%20%28Video-LLM%29%20for%20precisely%0Acategorizing%20activities.%20Through%20careful%20prompt%20engineering%2C%20we%20customize%20the%0AVideo-LLM%20to%20adeptly%20handle%20driving%20activities%27%20nuances%2C%20ensuring%20its%0Aclassification%20efficacy%20even%20with%20sparse%20data.%20Engineered%20to%20be%20lightweight%2C%0Aour%20framework%20is%20optimized%20for%20consumer-grade%20GPUs%2C%20making%20it%20vastly%20applicable%0Ain%20practical%20scenarios.%20We%20subjected%20our%20method%20to%20rigorous%20testing%20on%20the%0ASynDD2%20dataset%2C%20a%20complex%20benchmark%20for%20distracted%20driving%20behaviors%2C%20where%20it%0Ademonstrated%20commendable%20performance-achieving%2057.5%25%20accuracy%20in%20event%0Aclassification%20and%2051%25%20in%20event%20detection.%20These%20outcomes%20underscore%20the%0Asubstantial%20promise%20of%20DeepLocalization%20in%20accurately%20identifying%20diverse%0Adriver%20behaviors%20and%20their%20temporal%20occurrences%2C%20all%20within%20the%20bounds%20of%0Alimited%20computational%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12258v1&entry.124074799=Read"},
{"title": "Inverse Neural Rendering for Explainable Multi-Object Tracking", "author": "Julian Ost and Tanushree Banerjee and Mario Bijelic and Felix Heide", "abstract": "  Today, most methods for image understanding tasks rely on feed-forward neural\nnetworks. While this approach has allowed for empirical accuracy, efficiency,\nand task adaptation via fine-tuning, it also comes with fundamental\ndisadvantages. Existing networks often struggle to generalize across different\ndatasets, even on the same task. By design, these networks ultimately reason\nabout high-dimensional scene features, which are challenging to analyze. This\nis true especially when attempting to predict 3D information based on 2D\nimages. We propose to recast 3D multi-object tracking from RGB cameras as an\n\\emph{Inverse Rendering (IR)} problem, by optimizing via a differentiable\nrendering pipeline over the latent space of pre-trained 3D object\nrepresentations and retrieve the latents that best represent object instances\nin a given input image. To this end, we optimize an image loss over generative\nlatent spaces that inherently disentangle shape and appearance properties. We\ninvestigate not only an alternate take on tracking but our method also enables\nexamining the generated objects, reasoning about failure situations, and\nresolving ambiguous cases. We validate the generalization and scaling\ncapabilities of our method by learning the generative prior exclusively from\nsynthetic data and assessing camera-based 3D tracking on the nuScenes and Waymo\ndatasets. Both these datasets are completely unseen to our method and do not\nrequire fine-tuning. Videos and code are available at\nhttps://light.princeton.edu/inverse-rendering-tracking/.\n", "link": "http://arxiv.org/abs/2404.12359v1", "date": "2024-04-18", "relevancy": 2.7726, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5745}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5517}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5373}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Inverse%20Neural%20Rendering%20for%20Explainable%20Multi-Object%20Tracking&body=Title%3A%20Inverse%20Neural%20Rendering%20for%20Explainable%20Multi-Object%20Tracking%0AAuthor%3A%20Julian%20Ost%20and%20Tanushree%20Banerjee%20and%20Mario%20Bijelic%20and%20Felix%20Heide%0AAbstract%3A%20%20%20Today%2C%20most%20methods%20for%20image%20understanding%20tasks%20rely%20on%20feed-forward%20neural%0Anetworks.%20While%20this%20approach%20has%20allowed%20for%20empirical%20accuracy%2C%20efficiency%2C%0Aand%20task%20adaptation%20via%20fine-tuning%2C%20it%20also%20comes%20with%20fundamental%0Adisadvantages.%20Existing%20networks%20often%20struggle%20to%20generalize%20across%20different%0Adatasets%2C%20even%20on%20the%20same%20task.%20By%20design%2C%20these%20networks%20ultimately%20reason%0Aabout%20high-dimensional%20scene%20features%2C%20which%20are%20challenging%20to%20analyze.%20This%0Ais%20true%20especially%20when%20attempting%20to%20predict%203D%20information%20based%20on%202D%0Aimages.%20We%20propose%20to%20recast%203D%20multi-object%20tracking%20from%20RGB%20cameras%20as%20an%0A%5Cemph%7BInverse%20Rendering%20%28IR%29%7D%20problem%2C%20by%20optimizing%20via%20a%20differentiable%0Arendering%20pipeline%20over%20the%20latent%20space%20of%20pre-trained%203D%20object%0Arepresentations%20and%20retrieve%20the%20latents%20that%20best%20represent%20object%20instances%0Ain%20a%20given%20input%20image.%20To%20this%20end%2C%20we%20optimize%20an%20image%20loss%20over%20generative%0Alatent%20spaces%20that%20inherently%20disentangle%20shape%20and%20appearance%20properties.%20We%0Ainvestigate%20not%20only%20an%20alternate%20take%20on%20tracking%20but%20our%20method%20also%20enables%0Aexamining%20the%20generated%20objects%2C%20reasoning%20about%20failure%20situations%2C%20and%0Aresolving%20ambiguous%20cases.%20We%20validate%20the%20generalization%20and%20scaling%0Acapabilities%20of%20our%20method%20by%20learning%20the%20generative%20prior%20exclusively%20from%0Asynthetic%20data%20and%20assessing%20camera-based%203D%20tracking%20on%20the%20nuScenes%20and%20Waymo%0Adatasets.%20Both%20these%20datasets%20are%20completely%20unseen%20to%20our%20method%20and%20do%20not%0Arequire%20fine-tuning.%20Videos%20and%20code%20are%20available%20at%0Ahttps%3A//light.princeton.edu/inverse-rendering-tracking/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12359v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20Neural%20Rendering%20for%20Explainable%20Multi-Object%20Tracking&entry.906535625=Julian%20Ost%20and%20Tanushree%20Banerjee%20and%20Mario%20Bijelic%20and%20Felix%20Heide&entry.1292438233=%20%20Today%2C%20most%20methods%20for%20image%20understanding%20tasks%20rely%20on%20feed-forward%20neural%0Anetworks.%20While%20this%20approach%20has%20allowed%20for%20empirical%20accuracy%2C%20efficiency%2C%0Aand%20task%20adaptation%20via%20fine-tuning%2C%20it%20also%20comes%20with%20fundamental%0Adisadvantages.%20Existing%20networks%20often%20struggle%20to%20generalize%20across%20different%0Adatasets%2C%20even%20on%20the%20same%20task.%20By%20design%2C%20these%20networks%20ultimately%20reason%0Aabout%20high-dimensional%20scene%20features%2C%20which%20are%20challenging%20to%20analyze.%20This%0Ais%20true%20especially%20when%20attempting%20to%20predict%203D%20information%20based%20on%202D%0Aimages.%20We%20propose%20to%20recast%203D%20multi-object%20tracking%20from%20RGB%20cameras%20as%20an%0A%5Cemph%7BInverse%20Rendering%20%28IR%29%7D%20problem%2C%20by%20optimizing%20via%20a%20differentiable%0Arendering%20pipeline%20over%20the%20latent%20space%20of%20pre-trained%203D%20object%0Arepresentations%20and%20retrieve%20the%20latents%20that%20best%20represent%20object%20instances%0Ain%20a%20given%20input%20image.%20To%20this%20end%2C%20we%20optimize%20an%20image%20loss%20over%20generative%0Alatent%20spaces%20that%20inherently%20disentangle%20shape%20and%20appearance%20properties.%20We%0Ainvestigate%20not%20only%20an%20alternate%20take%20on%20tracking%20but%20our%20method%20also%20enables%0Aexamining%20the%20generated%20objects%2C%20reasoning%20about%20failure%20situations%2C%20and%0Aresolving%20ambiguous%20cases.%20We%20validate%20the%20generalization%20and%20scaling%0Acapabilities%20of%20our%20method%20by%20learning%20the%20generative%20prior%20exclusively%20from%0Asynthetic%20data%20and%20assessing%20camera-based%203D%20tracking%20on%20the%20nuScenes%20and%20Waymo%0Adatasets.%20Both%20these%20datasets%20are%20completely%20unseen%20to%20our%20method%20and%20do%20not%0Arequire%20fine-tuning.%20Videos%20and%20code%20are%20available%20at%0Ahttps%3A//light.princeton.edu/inverse-rendering-tracking/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12359v1&entry.124074799=Read"},
{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "author": "Isabella Liu and Hao Su and Xiaolong Wang", "abstract": "  Modern 3D engines and graphics pipelines require mesh as a memory-efficient\nrepresentation, which allows efficient rendering, geometry processing, texture\nediting, and many other downstream operations. However, it is still highly\ndifficult to obtain high-quality mesh in terms of structure and detail from\nmonocular visual observations. The problem becomes even more challenging for\ndynamic scenes and objects. To this end, we introduce Dynamic Gaussians Mesh\n(DG-Mesh), a framework to reconstruct a high-fidelity and time-consistent mesh\ngiven a single monocular video. Our work leverages the recent advancement in 3D\nGaussian Splatting to construct the mesh sequence with temporal consistency\nfrom a video. Building on top of this representation, DG-Mesh recovers\nhigh-quality meshes from the Gaussian points and can track the mesh vertices\nover time, which enables applications such as texture editing on dynamic\nobjects. We introduce the Gaussian-Mesh Anchoring, which encourages evenly\ndistributed Gaussians, resulting better mesh reconstruction through mesh-guided\ndensification and pruning on the deformed Gaussians. By applying\ncycle-consistent deformation between the canonical and the deformed space, we\ncan project the anchored Gaussian back to the canonical space and optimize\nGaussians across all time frames. During the evaluation on different datasets,\nDG-Mesh provides significantly better mesh reconstruction and rendering than\nbaselines.\n", "link": "http://arxiv.org/abs/2404.12379v1", "date": "2024-04-18", "relevancy": 2.6005, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5252}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5248}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5103}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Gaussians%20Mesh%3A%20Consistent%20Mesh%20Reconstruction%20from%20Monocular%0A%20%20Videos&body=Title%3A%20Dynamic%20Gaussians%20Mesh%3A%20Consistent%20Mesh%20Reconstruction%20from%20Monocular%0A%20%20Videos%0AAuthor%3A%20Isabella%20Liu%20and%20Hao%20Su%20and%20Xiaolong%20Wang%0AAbstract%3A%20%20%20Modern%203D%20engines%20and%20graphics%20pipelines%20require%20mesh%20as%20a%20memory-efficient%0Arepresentation%2C%20which%20allows%20efficient%20rendering%2C%20geometry%20processing%2C%20texture%0Aediting%2C%20and%20many%20other%20downstream%20operations.%20However%2C%20it%20is%20still%20highly%0Adifficult%20to%20obtain%20high-quality%20mesh%20in%20terms%20of%20structure%20and%20detail%20from%0Amonocular%20visual%20observations.%20The%20problem%20becomes%20even%20more%20challenging%20for%0Adynamic%20scenes%20and%20objects.%20To%20this%20end%2C%20we%20introduce%20Dynamic%20Gaussians%20Mesh%0A%28DG-Mesh%29%2C%20a%20framework%20to%20reconstruct%20a%20high-fidelity%20and%20time-consistent%20mesh%0Agiven%20a%20single%20monocular%20video.%20Our%20work%20leverages%20the%20recent%20advancement%20in%203D%0AGaussian%20Splatting%20to%20construct%20the%20mesh%20sequence%20with%20temporal%20consistency%0Afrom%20a%20video.%20Building%20on%20top%20of%20this%20representation%2C%20DG-Mesh%20recovers%0Ahigh-quality%20meshes%20from%20the%20Gaussian%20points%20and%20can%20track%20the%20mesh%20vertices%0Aover%20time%2C%20which%20enables%20applications%20such%20as%20texture%20editing%20on%20dynamic%0Aobjects.%20We%20introduce%20the%20Gaussian-Mesh%20Anchoring%2C%20which%20encourages%20evenly%0Adistributed%20Gaussians%2C%20resulting%20better%20mesh%20reconstruction%20through%20mesh-guided%0Adensification%20and%20pruning%20on%20the%20deformed%20Gaussians.%20By%20applying%0Acycle-consistent%20deformation%20between%20the%20canonical%20and%20the%20deformed%20space%2C%20we%0Acan%20project%20the%20anchored%20Gaussian%20back%20to%20the%20canonical%20space%20and%20optimize%0AGaussians%20across%20all%20time%20frames.%20During%20the%20evaluation%20on%20different%20datasets%2C%0ADG-Mesh%20provides%20significantly%20better%20mesh%20reconstruction%20and%20rendering%20than%0Abaselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12379v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Gaussians%20Mesh%3A%20Consistent%20Mesh%20Reconstruction%20from%20Monocular%0A%20%20Videos&entry.906535625=Isabella%20Liu%20and%20Hao%20Su%20and%20Xiaolong%20Wang&entry.1292438233=%20%20Modern%203D%20engines%20and%20graphics%20pipelines%20require%20mesh%20as%20a%20memory-efficient%0Arepresentation%2C%20which%20allows%20efficient%20rendering%2C%20geometry%20processing%2C%20texture%0Aediting%2C%20and%20many%20other%20downstream%20operations.%20However%2C%20it%20is%20still%20highly%0Adifficult%20to%20obtain%20high-quality%20mesh%20in%20terms%20of%20structure%20and%20detail%20from%0Amonocular%20visual%20observations.%20The%20problem%20becomes%20even%20more%20challenging%20for%0Adynamic%20scenes%20and%20objects.%20To%20this%20end%2C%20we%20introduce%20Dynamic%20Gaussians%20Mesh%0A%28DG-Mesh%29%2C%20a%20framework%20to%20reconstruct%20a%20high-fidelity%20and%20time-consistent%20mesh%0Agiven%20a%20single%20monocular%20video.%20Our%20work%20leverages%20the%20recent%20advancement%20in%203D%0AGaussian%20Splatting%20to%20construct%20the%20mesh%20sequence%20with%20temporal%20consistency%0Afrom%20a%20video.%20Building%20on%20top%20of%20this%20representation%2C%20DG-Mesh%20recovers%0Ahigh-quality%20meshes%20from%20the%20Gaussian%20points%20and%20can%20track%20the%20mesh%20vertices%0Aover%20time%2C%20which%20enables%20applications%20such%20as%20texture%20editing%20on%20dynamic%0Aobjects.%20We%20introduce%20the%20Gaussian-Mesh%20Anchoring%2C%20which%20encourages%20evenly%0Adistributed%20Gaussians%2C%20resulting%20better%20mesh%20reconstruction%20through%20mesh-guided%0Adensification%20and%20pruning%20on%20the%20deformed%20Gaussians.%20By%20applying%0Acycle-consistent%20deformation%20between%20the%20canonical%20and%20the%20deformed%20space%2C%20we%0Acan%20project%20the%20anchored%20Gaussian%20back%20to%20the%20canonical%20space%20and%20optimize%0AGaussians%20across%20all%20time%20frames.%20During%20the%20evaluation%20on%20different%20datasets%2C%0ADG-Mesh%20provides%20significantly%20better%20mesh%20reconstruction%20and%20rendering%20than%0Abaselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12379v1&entry.124074799=Read"},
{"title": "Alleviating Catastrophic Forgetting in Facial Expression Recognition\n  with Emotion-Centered Models", "author": "Israel A. Laurensi and Alceu de Souza Britto Jr. and Jean Paul Barddal and Alessandro Lameiras Koerich", "abstract": "  Facial expression recognition is a pivotal component in machine learning,\nfacilitating various applications. However, convolutional neural networks\n(CNNs) are often plagued by catastrophic forgetting, impeding their\nadaptability. The proposed method, emotion-centered generative replay (ECgr),\ntackles this challenge by integrating synthetic images from generative\nadversarial networks. Moreover, ECgr incorporates a quality assurance algorithm\nto ensure the fidelity of generated images. This dual approach enables CNNs to\nretain past knowledge while learning new tasks, enhancing their performance in\nemotion recognition. The experimental results on four diverse facial expression\ndatasets demonstrate that incorporating images generated by our\npseudo-rehearsal method enhances training on the targeted dataset and the\nsource dataset while making the CNN retain previously learned knowledge.\n", "link": "http://arxiv.org/abs/2404.12260v1", "date": "2024-04-18", "relevancy": 2.5895, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5243}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5151}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5142}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Alleviating%20Catastrophic%20Forgetting%20in%20Facial%20Expression%20Recognition%0A%20%20with%20Emotion-Centered%20Models&body=Title%3A%20Alleviating%20Catastrophic%20Forgetting%20in%20Facial%20Expression%20Recognition%0A%20%20with%20Emotion-Centered%20Models%0AAuthor%3A%20Israel%20A.%20Laurensi%20and%20Alceu%20de%20Souza%20Britto%20Jr.%20and%20Jean%20Paul%20Barddal%20and%20Alessandro%20Lameiras%20Koerich%0AAbstract%3A%20%20%20Facial%20expression%20recognition%20is%20a%20pivotal%20component%20in%20machine%20learning%2C%0Afacilitating%20various%20applications.%20However%2C%20convolutional%20neural%20networks%0A%28CNNs%29%20are%20often%20plagued%20by%20catastrophic%20forgetting%2C%20impeding%20their%0Aadaptability.%20The%20proposed%20method%2C%20emotion-centered%20generative%20replay%20%28ECgr%29%2C%0Atackles%20this%20challenge%20by%20integrating%20synthetic%20images%20from%20generative%0Aadversarial%20networks.%20Moreover%2C%20ECgr%20incorporates%20a%20quality%20assurance%20algorithm%0Ato%20ensure%20the%20fidelity%20of%20generated%20images.%20This%20dual%20approach%20enables%20CNNs%20to%0Aretain%20past%20knowledge%20while%20learning%20new%20tasks%2C%20enhancing%20their%20performance%20in%0Aemotion%20recognition.%20The%20experimental%20results%20on%20four%20diverse%20facial%20expression%0Adatasets%20demonstrate%20that%20incorporating%20images%20generated%20by%20our%0Apseudo-rehearsal%20method%20enhances%20training%20on%20the%20targeted%20dataset%20and%20the%0Asource%20dataset%20while%20making%20the%20CNN%20retain%20previously%20learned%20knowledge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12260v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alleviating%20Catastrophic%20Forgetting%20in%20Facial%20Expression%20Recognition%0A%20%20with%20Emotion-Centered%20Models&entry.906535625=Israel%20A.%20Laurensi%20and%20Alceu%20de%20Souza%20Britto%20Jr.%20and%20Jean%20Paul%20Barddal%20and%20Alessandro%20Lameiras%20Koerich&entry.1292438233=%20%20Facial%20expression%20recognition%20is%20a%20pivotal%20component%20in%20machine%20learning%2C%0Afacilitating%20various%20applications.%20However%2C%20convolutional%20neural%20networks%0A%28CNNs%29%20are%20often%20plagued%20by%20catastrophic%20forgetting%2C%20impeding%20their%0Aadaptability.%20The%20proposed%20method%2C%20emotion-centered%20generative%20replay%20%28ECgr%29%2C%0Atackles%20this%20challenge%20by%20integrating%20synthetic%20images%20from%20generative%0Aadversarial%20networks.%20Moreover%2C%20ECgr%20incorporates%20a%20quality%20assurance%20algorithm%0Ato%20ensure%20the%20fidelity%20of%20generated%20images.%20This%20dual%20approach%20enables%20CNNs%20to%0Aretain%20past%20knowledge%20while%20learning%20new%20tasks%2C%20enhancing%20their%20performance%20in%0Aemotion%20recognition.%20The%20experimental%20results%20on%20four%20diverse%20facial%20expression%0Adatasets%20demonstrate%20that%20incorporating%20images%20generated%20by%20our%0Apseudo-rehearsal%20method%20enhances%20training%20on%20the%20targeted%20dataset%20and%20the%0Asource%20dataset%20while%20making%20the%20CNN%20retain%20previously%20learned%20knowledge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12260v1&entry.124074799=Read"},
{"title": "Reducing Bias in Pre-trained Models by Tuning while Penalizing Change", "author": "Niklas Penzel and Gideon Stein and Joachim Denzler", "abstract": "  Deep models trained on large amounts of data often incorporate implicit\nbiases present during training time. If later such a bias is discovered during\ninference or deployment, it is often necessary to acquire new data and retrain\nthe model. This behavior is especially problematic in critical areas such as\nautonomous driving or medical decision-making. In these scenarios, new data is\noften expensive and hard to come by. In this work, we present a method based on\nchange penalization that takes a pre-trained model and adapts the weights to\nmitigate a previously detected bias. We achieve this by tuning a\nzero-initialized copy of a frozen pre-trained network. Our method needs very\nfew, in extreme cases only a single, examples that contradict the bias to\nincrease performance. Additionally, we propose an early stopping criterion to\nmodify baselines and reduce overfitting. We evaluate our approach on a\nwell-known bias in skin lesion classification and three other datasets from the\ndomain shift literature. We find that our approach works especially well with\nvery few images. Simple fine-tuning combined with our early stopping also leads\nto performance benefits for a larger number of tuning samples.\n", "link": "http://arxiv.org/abs/2404.12292v1", "date": "2024-04-18", "relevancy": 2.5739, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5273}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5172}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4998}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Reducing%20Bias%20in%20Pre-trained%20Models%20by%20Tuning%20while%20Penalizing%20Change&body=Title%3A%20Reducing%20Bias%20in%20Pre-trained%20Models%20by%20Tuning%20while%20Penalizing%20Change%0AAuthor%3A%20Niklas%20Penzel%20and%20Gideon%20Stein%20and%20Joachim%20Denzler%0AAbstract%3A%20%20%20Deep%20models%20trained%20on%20large%20amounts%20of%20data%20often%20incorporate%20implicit%0Abiases%20present%20during%20training%20time.%20If%20later%20such%20a%20bias%20is%20discovered%20during%0Ainference%20or%20deployment%2C%20it%20is%20often%20necessary%20to%20acquire%20new%20data%20and%20retrain%0Athe%20model.%20This%20behavior%20is%20especially%20problematic%20in%20critical%20areas%20such%20as%0Aautonomous%20driving%20or%20medical%20decision-making.%20In%20these%20scenarios%2C%20new%20data%20is%0Aoften%20expensive%20and%20hard%20to%20come%20by.%20In%20this%20work%2C%20we%20present%20a%20method%20based%20on%0Achange%20penalization%20that%20takes%20a%20pre-trained%20model%20and%20adapts%20the%20weights%20to%0Amitigate%20a%20previously%20detected%20bias.%20We%20achieve%20this%20by%20tuning%20a%0Azero-initialized%20copy%20of%20a%20frozen%20pre-trained%20network.%20Our%20method%20needs%20very%0Afew%2C%20in%20extreme%20cases%20only%20a%20single%2C%20examples%20that%20contradict%20the%20bias%20to%0Aincrease%20performance.%20Additionally%2C%20we%20propose%20an%20early%20stopping%20criterion%20to%0Amodify%20baselines%20and%20reduce%20overfitting.%20We%20evaluate%20our%20approach%20on%20a%0Awell-known%20bias%20in%20skin%20lesion%20classification%20and%20three%20other%20datasets%20from%20the%0Adomain%20shift%20literature.%20We%20find%20that%20our%20approach%20works%20especially%20well%20with%0Avery%20few%20images.%20Simple%20fine-tuning%20combined%20with%20our%20early%20stopping%20also%20leads%0Ato%20performance%20benefits%20for%20a%20larger%20number%20of%20tuning%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12292v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reducing%20Bias%20in%20Pre-trained%20Models%20by%20Tuning%20while%20Penalizing%20Change&entry.906535625=Niklas%20Penzel%20and%20Gideon%20Stein%20and%20Joachim%20Denzler&entry.1292438233=%20%20Deep%20models%20trained%20on%20large%20amounts%20of%20data%20often%20incorporate%20implicit%0Abiases%20present%20during%20training%20time.%20If%20later%20such%20a%20bias%20is%20discovered%20during%0Ainference%20or%20deployment%2C%20it%20is%20often%20necessary%20to%20acquire%20new%20data%20and%20retrain%0Athe%20model.%20This%20behavior%20is%20especially%20problematic%20in%20critical%20areas%20such%20as%0Aautonomous%20driving%20or%20medical%20decision-making.%20In%20these%20scenarios%2C%20new%20data%20is%0Aoften%20expensive%20and%20hard%20to%20come%20by.%20In%20this%20work%2C%20we%20present%20a%20method%20based%20on%0Achange%20penalization%20that%20takes%20a%20pre-trained%20model%20and%20adapts%20the%20weights%20to%0Amitigate%20a%20previously%20detected%20bias.%20We%20achieve%20this%20by%20tuning%20a%0Azero-initialized%20copy%20of%20a%20frozen%20pre-trained%20network.%20Our%20method%20needs%20very%0Afew%2C%20in%20extreme%20cases%20only%20a%20single%2C%20examples%20that%20contradict%20the%20bias%20to%0Aincrease%20performance.%20Additionally%2C%20we%20propose%20an%20early%20stopping%20criterion%20to%0Amodify%20baselines%20and%20reduce%20overfitting.%20We%20evaluate%20our%20approach%20on%20a%0Awell-known%20bias%20in%20skin%20lesion%20classification%20and%20three%20other%20datasets%20from%20the%0Adomain%20shift%20literature.%20We%20find%20that%20our%20approach%20works%20especially%20well%20with%0Avery%20few%20images.%20Simple%20fine-tuning%20combined%20with%20our%20early%20stopping%20also%20leads%0Ato%20performance%20benefits%20for%20a%20larger%20number%20of%20tuning%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12292v1&entry.124074799=Read"},
{"title": "Beyond Known Clusters: Probe New Prototypes for Efficient Generalized\n  Class Discovery", "author": "Ye Wang and Yaxiong Wang and Yujiao Wu and Bingchen Zhao and Xueming Qian", "abstract": "  Generalized Class Discovery (GCD) aims to dynamically assign labels to\nunlabelled data partially based on knowledge learned from labelled data, where\nthe unlabelled data may come from known or novel classes. The prevailing\napproach generally involves clustering across all data and learning conceptions\nby prototypical contrastive learning. However, existing methods largely hinge\non the performance of clustering algorithms and are thus subject to their\ninherent limitations. Firstly, the estimated cluster number is often smaller\nthan the ground truth, making the existing methods suffer from the lack of\nprototypes for comprehensive conception learning. To address this issue, we\npropose an adaptive probing mechanism that introduces learnable potential\nprototypes to expand cluster prototypes (centers). As there is no ground truth\nfor the potential prototype, we develop a self-supervised prototype learning\nframework to optimize the potential prototype in an end-to-end fashion.\nSecondly, clustering is computationally intensive, and the conventional\nstrategy of clustering both labelled and unlabelled instances exacerbates this\nissue. To counteract this inefficiency, we opt to cluster only the unlabelled\ninstances and subsequently expand the cluster prototypes with our introduced\npotential prototypes to fast explore novel classes. Despite the simplicity of\nour proposed method, extensive empirical analysis on a wide range of datasets\nconfirms that our method consistently delivers state-of-the-art results.\nSpecifically, our method surpasses the nearest competitor by a significant\nmargin of \\textbf{9.7}$\\%$ within the Stanford Cars dataset and\n\\textbf{12$\\times$} clustering efficiency within the Herbarium 19 dataset. We\nwill make the code and checkpoints publicly available at\n\\url{https://github.com/xjtuYW/PNP.git}.\n", "link": "http://arxiv.org/abs/2404.08995v2", "date": "2024-04-18", "relevancy": 2.5569, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.524}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5068}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5034}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Beyond%20Known%20Clusters%3A%20Probe%20New%20Prototypes%20for%20Efficient%20Generalized%0A%20%20Class%20Discovery&body=Title%3A%20Beyond%20Known%20Clusters%3A%20Probe%20New%20Prototypes%20for%20Efficient%20Generalized%0A%20%20Class%20Discovery%0AAuthor%3A%20Ye%20Wang%20and%20Yaxiong%20Wang%20and%20Yujiao%20Wu%20and%20Bingchen%20Zhao%20and%20Xueming%20Qian%0AAbstract%3A%20%20%20Generalized%20Class%20Discovery%20%28GCD%29%20aims%20to%20dynamically%20assign%20labels%20to%0Aunlabelled%20data%20partially%20based%20on%20knowledge%20learned%20from%20labelled%20data%2C%20where%0Athe%20unlabelled%20data%20may%20come%20from%20known%20or%20novel%20classes.%20The%20prevailing%0Aapproach%20generally%20involves%20clustering%20across%20all%20data%20and%20learning%20conceptions%0Aby%20prototypical%20contrastive%20learning.%20However%2C%20existing%20methods%20largely%20hinge%0Aon%20the%20performance%20of%20clustering%20algorithms%20and%20are%20thus%20subject%20to%20their%0Ainherent%20limitations.%20Firstly%2C%20the%20estimated%20cluster%20number%20is%20often%20smaller%0Athan%20the%20ground%20truth%2C%20making%20the%20existing%20methods%20suffer%20from%20the%20lack%20of%0Aprototypes%20for%20comprehensive%20conception%20learning.%20To%20address%20this%20issue%2C%20we%0Apropose%20an%20adaptive%20probing%20mechanism%20that%20introduces%20learnable%20potential%0Aprototypes%20to%20expand%20cluster%20prototypes%20%28centers%29.%20As%20there%20is%20no%20ground%20truth%0Afor%20the%20potential%20prototype%2C%20we%20develop%20a%20self-supervised%20prototype%20learning%0Aframework%20to%20optimize%20the%20potential%20prototype%20in%20an%20end-to-end%20fashion.%0ASecondly%2C%20clustering%20is%20computationally%20intensive%2C%20and%20the%20conventional%0Astrategy%20of%20clustering%20both%20labelled%20and%20unlabelled%20instances%20exacerbates%20this%0Aissue.%20To%20counteract%20this%20inefficiency%2C%20we%20opt%20to%20cluster%20only%20the%20unlabelled%0Ainstances%20and%20subsequently%20expand%20the%20cluster%20prototypes%20with%20our%20introduced%0Apotential%20prototypes%20to%20fast%20explore%20novel%20classes.%20Despite%20the%20simplicity%20of%0Aour%20proposed%20method%2C%20extensive%20empirical%20analysis%20on%20a%20wide%20range%20of%20datasets%0Aconfirms%20that%20our%20method%20consistently%20delivers%20state-of-the-art%20results.%0ASpecifically%2C%20our%20method%20surpasses%20the%20nearest%20competitor%20by%20a%20significant%0Amargin%20of%20%5Ctextbf%7B9.7%7D%24%5C%25%24%20within%20the%20Stanford%20Cars%20dataset%20and%0A%5Ctextbf%7B12%24%5Ctimes%24%7D%20clustering%20efficiency%20within%20the%20Herbarium%2019%20dataset.%20We%0Awill%20make%20the%20code%20and%20checkpoints%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//github.com/xjtuYW/PNP.git%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08995v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Known%20Clusters%3A%20Probe%20New%20Prototypes%20for%20Efficient%20Generalized%0A%20%20Class%20Discovery&entry.906535625=Ye%20Wang%20and%20Yaxiong%20Wang%20and%20Yujiao%20Wu%20and%20Bingchen%20Zhao%20and%20Xueming%20Qian&entry.1292438233=%20%20Generalized%20Class%20Discovery%20%28GCD%29%20aims%20to%20dynamically%20assign%20labels%20to%0Aunlabelled%20data%20partially%20based%20on%20knowledge%20learned%20from%20labelled%20data%2C%20where%0Athe%20unlabelled%20data%20may%20come%20from%20known%20or%20novel%20classes.%20The%20prevailing%0Aapproach%20generally%20involves%20clustering%20across%20all%20data%20and%20learning%20conceptions%0Aby%20prototypical%20contrastive%20learning.%20However%2C%20existing%20methods%20largely%20hinge%0Aon%20the%20performance%20of%20clustering%20algorithms%20and%20are%20thus%20subject%20to%20their%0Ainherent%20limitations.%20Firstly%2C%20the%20estimated%20cluster%20number%20is%20often%20smaller%0Athan%20the%20ground%20truth%2C%20making%20the%20existing%20methods%20suffer%20from%20the%20lack%20of%0Aprototypes%20for%20comprehensive%20conception%20learning.%20To%20address%20this%20issue%2C%20we%0Apropose%20an%20adaptive%20probing%20mechanism%20that%20introduces%20learnable%20potential%0Aprototypes%20to%20expand%20cluster%20prototypes%20%28centers%29.%20As%20there%20is%20no%20ground%20truth%0Afor%20the%20potential%20prototype%2C%20we%20develop%20a%20self-supervised%20prototype%20learning%0Aframework%20to%20optimize%20the%20potential%20prototype%20in%20an%20end-to-end%20fashion.%0ASecondly%2C%20clustering%20is%20computationally%20intensive%2C%20and%20the%20conventional%0Astrategy%20of%20clustering%20both%20labelled%20and%20unlabelled%20instances%20exacerbates%20this%0Aissue.%20To%20counteract%20this%20inefficiency%2C%20we%20opt%20to%20cluster%20only%20the%20unlabelled%0Ainstances%20and%20subsequently%20expand%20the%20cluster%20prototypes%20with%20our%20introduced%0Apotential%20prototypes%20to%20fast%20explore%20novel%20classes.%20Despite%20the%20simplicity%20of%0Aour%20proposed%20method%2C%20extensive%20empirical%20analysis%20on%20a%20wide%20range%20of%20datasets%0Aconfirms%20that%20our%20method%20consistently%20delivers%20state-of-the-art%20results.%0ASpecifically%2C%20our%20method%20surpasses%20the%20nearest%20competitor%20by%20a%20significant%0Amargin%20of%20%5Ctextbf%7B9.7%7D%24%5C%25%24%20within%20the%20Stanford%20Cars%20dataset%20and%0A%5Ctextbf%7B12%24%5Ctimes%24%7D%20clustering%20efficiency%20within%20the%20Herbarium%2019%20dataset.%20We%0Awill%20make%20the%20code%20and%20checkpoints%20publicly%20available%20at%0A%5Curl%7Bhttps%3A//github.com/xjtuYW/PNP.git%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08995v2&entry.124074799=Read"},
{"title": "Reciprocal Attention Mixing Transformer for Lightweight Image\n  Restoration", "author": "Haram Choi and Cheolwoong Na and Jihyeon Oh and Seungjae Lee and Jinseop Kim and Subeen Choe and Jeongmin Lee and Taehoon Kim and Jihoon Yang", "abstract": "  Although many recent works have made advancements in the image restoration\n(IR) field, they often suffer from an excessive number of parameters. Another\nissue is that most Transformer-based IR methods focus only on either local or\nglobal features, leading to limited receptive fields or deficient parameter\nissues. To address these problems, we propose a lightweight IR network,\nReciprocal Attention Mixing Transformer (RAMiT). It employs our proposed\ndimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which\ncompute bi-dimensional (spatial and channel) self-attentions in parallel with\ndifferent numbers of multi-heads. The bi-dimensional attentions help each other\nto complement their counterpart's drawbacks and are then mixed. Additionally,\nwe introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that\ncompensates for pixel-level information losses and utilizes semantic\ninformation while maintaining an efficient hierarchical structure. Furthermore,\nwe revisit and modify MobileNet V1 and V2 to attach efficient convolutions to\nour proposed components. The experimental results demonstrate that RAMiT\nachieves state-of-the-art performance on multiple lightweight IR tasks,\nincluding super-resolution, color denoising, grayscale denoising, low-light\nenhancement, and deraining. Codes are available at\nhttps://github.com/rami0205/RAMiT.\n", "link": "http://arxiv.org/abs/2305.11474v4", "date": "2024-04-18", "relevancy": 2.3525, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6133}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5957}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5705}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Reciprocal%20Attention%20Mixing%20Transformer%20for%20Lightweight%20Image%0A%20%20Restoration&body=Title%3A%20Reciprocal%20Attention%20Mixing%20Transformer%20for%20Lightweight%20Image%0A%20%20Restoration%0AAuthor%3A%20Haram%20Choi%20and%20Cheolwoong%20Na%20and%20Jihyeon%20Oh%20and%20Seungjae%20Lee%20and%20Jinseop%20Kim%20and%20Subeen%20Choe%20and%20Jeongmin%20Lee%20and%20Taehoon%20Kim%20and%20Jihoon%20Yang%0AAbstract%3A%20%20%20Although%20many%20recent%20works%20have%20made%20advancements%20in%20the%20image%20restoration%0A%28IR%29%20field%2C%20they%20often%20suffer%20from%20an%20excessive%20number%20of%20parameters.%20Another%0Aissue%20is%20that%20most%20Transformer-based%20IR%20methods%20focus%20only%20on%20either%20local%20or%0Aglobal%20features%2C%20leading%20to%20limited%20receptive%20fields%20or%20deficient%20parameter%0Aissues.%20To%20address%20these%20problems%2C%20we%20propose%20a%20lightweight%20IR%20network%2C%0AReciprocal%20Attention%20Mixing%20Transformer%20%28RAMiT%29.%20It%20employs%20our%20proposed%0Adimensional%20reciprocal%20attention%20mixing%20Transformer%20%28D-RAMiT%29%20blocks%2C%20which%0Acompute%20bi-dimensional%20%28spatial%20and%20channel%29%20self-attentions%20in%20parallel%20with%0Adifferent%20numbers%20of%20multi-heads.%20The%20bi-dimensional%20attentions%20help%20each%20other%0Ato%20complement%20their%20counterpart%27s%20drawbacks%20and%20are%20then%20mixed.%20Additionally%2C%0Awe%20introduce%20a%20hierarchical%20reciprocal%20attention%20mixing%20%28H-RAMi%29%20layer%20that%0Acompensates%20for%20pixel-level%20information%20losses%20and%20utilizes%20semantic%0Ainformation%20while%20maintaining%20an%20efficient%20hierarchical%20structure.%20Furthermore%2C%0Awe%20revisit%20and%20modify%20MobileNet%20V1%20and%20V2%20to%20attach%20efficient%20convolutions%20to%0Aour%20proposed%20components.%20The%20experimental%20results%20demonstrate%20that%20RAMiT%0Aachieves%20state-of-the-art%20performance%20on%20multiple%20lightweight%20IR%20tasks%2C%0Aincluding%20super-resolution%2C%20color%20denoising%2C%20grayscale%20denoising%2C%20low-light%0Aenhancement%2C%20and%20deraining.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/rami0205/RAMiT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.11474v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reciprocal%20Attention%20Mixing%20Transformer%20for%20Lightweight%20Image%0A%20%20Restoration&entry.906535625=Haram%20Choi%20and%20Cheolwoong%20Na%20and%20Jihyeon%20Oh%20and%20Seungjae%20Lee%20and%20Jinseop%20Kim%20and%20Subeen%20Choe%20and%20Jeongmin%20Lee%20and%20Taehoon%20Kim%20and%20Jihoon%20Yang&entry.1292438233=%20%20Although%20many%20recent%20works%20have%20made%20advancements%20in%20the%20image%20restoration%0A%28IR%29%20field%2C%20they%20often%20suffer%20from%20an%20excessive%20number%20of%20parameters.%20Another%0Aissue%20is%20that%20most%20Transformer-based%20IR%20methods%20focus%20only%20on%20either%20local%20or%0Aglobal%20features%2C%20leading%20to%20limited%20receptive%20fields%20or%20deficient%20parameter%0Aissues.%20To%20address%20these%20problems%2C%20we%20propose%20a%20lightweight%20IR%20network%2C%0AReciprocal%20Attention%20Mixing%20Transformer%20%28RAMiT%29.%20It%20employs%20our%20proposed%0Adimensional%20reciprocal%20attention%20mixing%20Transformer%20%28D-RAMiT%29%20blocks%2C%20which%0Acompute%20bi-dimensional%20%28spatial%20and%20channel%29%20self-attentions%20in%20parallel%20with%0Adifferent%20numbers%20of%20multi-heads.%20The%20bi-dimensional%20attentions%20help%20each%20other%0Ato%20complement%20their%20counterpart%27s%20drawbacks%20and%20are%20then%20mixed.%20Additionally%2C%0Awe%20introduce%20a%20hierarchical%20reciprocal%20attention%20mixing%20%28H-RAMi%29%20layer%20that%0Acompensates%20for%20pixel-level%20information%20losses%20and%20utilizes%20semantic%0Ainformation%20while%20maintaining%20an%20efficient%20hierarchical%20structure.%20Furthermore%2C%0Awe%20revisit%20and%20modify%20MobileNet%20V1%20and%20V2%20to%20attach%20efficient%20convolutions%20to%0Aour%20proposed%20components.%20The%20experimental%20results%20demonstrate%20that%20RAMiT%0Aachieves%20state-of-the-art%20performance%20on%20multiple%20lightweight%20IR%20tasks%2C%0Aincluding%20super-resolution%2C%20color%20denoising%2C%20grayscale%20denoising%2C%20low-light%0Aenhancement%2C%20and%20deraining.%20Codes%20are%20available%20at%0Ahttps%3A//github.com/rami0205/RAMiT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.11474v4&entry.124074799=Read"},
{"title": "G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and\n  Grasp Synthesis", "author": "Yufei Ye and Abhinav Gupta and Kris Kitani and Shubham Tulsiani", "abstract": "  We propose G-HOP, a denoising diffusion based generative prior for\nhand-object interactions that allows modeling both the 3D object and a human\nhand, conditioned on the object category. To learn a 3D spatial diffusion model\nthat can capture this joint distribution, we represent the human hand via a\nskeletal distance field to obtain a representation aligned with the (latent)\nsigned distance field for the object. We show that this hand-object prior can\nthen serve as generic guidance to facilitate other tasks like reconstruction\nfrom interaction clip and human grasp synthesis. We believe that our model,\ntrained by aggregating seven diverse real-world interaction datasets spanning\nacross 155 categories, represents a first approach that allows jointly\ngenerating both hand and object. Our empirical evaluations demonstrate the\nbenefit of this joint prior in video-based reconstruction and human grasp\nsynthesis, outperforming current task-specific baselines.\n  Project website: https://judyye.github.io/ghop-www\n", "link": "http://arxiv.org/abs/2404.12383v1", "date": "2024-04-18", "relevancy": 2.3385, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6664}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5529}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5155}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20G-HOP%3A%20Generative%20Hand-Object%20Prior%20for%20Interaction%20Reconstruction%20and%0A%20%20Grasp%20Synthesis&body=Title%3A%20G-HOP%3A%20Generative%20Hand-Object%20Prior%20for%20Interaction%20Reconstruction%20and%0A%20%20Grasp%20Synthesis%0AAuthor%3A%20Yufei%20Ye%20and%20Abhinav%20Gupta%20and%20Kris%20Kitani%20and%20Shubham%20Tulsiani%0AAbstract%3A%20%20%20We%20propose%20G-HOP%2C%20a%20denoising%20diffusion%20based%20generative%20prior%20for%0Ahand-object%20interactions%20that%20allows%20modeling%20both%20the%203D%20object%20and%20a%20human%0Ahand%2C%20conditioned%20on%20the%20object%20category.%20To%20learn%20a%203D%20spatial%20diffusion%20model%0Athat%20can%20capture%20this%20joint%20distribution%2C%20we%20represent%20the%20human%20hand%20via%20a%0Askeletal%20distance%20field%20to%20obtain%20a%20representation%20aligned%20with%20the%20%28latent%29%0Asigned%20distance%20field%20for%20the%20object.%20We%20show%20that%20this%20hand-object%20prior%20can%0Athen%20serve%20as%20generic%20guidance%20to%20facilitate%20other%20tasks%20like%20reconstruction%0Afrom%20interaction%20clip%20and%20human%20grasp%20synthesis.%20We%20believe%20that%20our%20model%2C%0Atrained%20by%20aggregating%20seven%20diverse%20real-world%20interaction%20datasets%20spanning%0Aacross%20155%20categories%2C%20represents%20a%20first%20approach%20that%20allows%20jointly%0Agenerating%20both%20hand%20and%20object.%20Our%20empirical%20evaluations%20demonstrate%20the%0Abenefit%20of%20this%20joint%20prior%20in%20video-based%20reconstruction%20and%20human%20grasp%0Asynthesis%2C%20outperforming%20current%20task-specific%20baselines.%0A%20%20Project%20website%3A%20https%3A//judyye.github.io/ghop-www%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12383v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G-HOP%3A%20Generative%20Hand-Object%20Prior%20for%20Interaction%20Reconstruction%20and%0A%20%20Grasp%20Synthesis&entry.906535625=Yufei%20Ye%20and%20Abhinav%20Gupta%20and%20Kris%20Kitani%20and%20Shubham%20Tulsiani&entry.1292438233=%20%20We%20propose%20G-HOP%2C%20a%20denoising%20diffusion%20based%20generative%20prior%20for%0Ahand-object%20interactions%20that%20allows%20modeling%20both%20the%203D%20object%20and%20a%20human%0Ahand%2C%20conditioned%20on%20the%20object%20category.%20To%20learn%20a%203D%20spatial%20diffusion%20model%0Athat%20can%20capture%20this%20joint%20distribution%2C%20we%20represent%20the%20human%20hand%20via%20a%0Askeletal%20distance%20field%20to%20obtain%20a%20representation%20aligned%20with%20the%20%28latent%29%0Asigned%20distance%20field%20for%20the%20object.%20We%20show%20that%20this%20hand-object%20prior%20can%0Athen%20serve%20as%20generic%20guidance%20to%20facilitate%20other%20tasks%20like%20reconstruction%0Afrom%20interaction%20clip%20and%20human%20grasp%20synthesis.%20We%20believe%20that%20our%20model%2C%0Atrained%20by%20aggregating%20seven%20diverse%20real-world%20interaction%20datasets%20spanning%0Aacross%20155%20categories%2C%20represents%20a%20first%20approach%20that%20allows%20jointly%0Agenerating%20both%20hand%20and%20object.%20Our%20empirical%20evaluations%20demonstrate%20the%0Abenefit%20of%20this%20joint%20prior%20in%20video-based%20reconstruction%20and%20human%20grasp%0Asynthesis%2C%20outperforming%20current%20task-specific%20baselines.%0A%20%20Project%20website%3A%20https%3A//judyye.github.io/ghop-www%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12383v1&entry.124074799=Read"},
{"title": "On the Content Bias in Fr\u00e9chet Video Distance", "author": "Songwei Ge and Aniruddha Mahapatra and Gaurav Parmar and Jun-Yan Zhu and Jia-Bin Huang", "abstract": "  Fr\\'echet Video Distance (FVD), a prominent metric for evaluating video\ngeneration models, is known to conflict with human perception occasionally. In\nthis paper, we aim to explore the extent of FVD's bias toward per-frame quality\nover temporal realism and identify its sources. We first quantify the FVD's\nsensitivity to the temporal axis by decoupling the frame and motion quality and\nfind that the FVD increases only slightly with large temporal corruption. We\nthen analyze the generated videos and show that via careful sampling from a\nlarge set of generated videos that do not contain motions, one can drastically\ndecrease FVD without improving the temporal quality. Both studies suggest FVD's\nbias towards the quality of individual frames. We further observe that the bias\ncan be attributed to the features extracted from a supervised video classifier\ntrained on the content-biased dataset. We show that FVD with features extracted\nfrom the recent large-scale self-supervised video models is less biased toward\nimage quality. Finally, we revisit a few real-world examples to validate our\nhypothesis.\n", "link": "http://arxiv.org/abs/2404.12391v1", "date": "2024-04-18", "relevancy": 2.3291, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6057}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5707}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5528}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20On%20the%20Content%20Bias%20in%20Fr%C3%A9chet%20Video%20Distance&body=Title%3A%20On%20the%20Content%20Bias%20in%20Fr%C3%A9chet%20Video%20Distance%0AAuthor%3A%20Songwei%20Ge%20and%20Aniruddha%20Mahapatra%20and%20Gaurav%20Parmar%20and%20Jun-Yan%20Zhu%20and%20Jia-Bin%20Huang%0AAbstract%3A%20%20%20Fr%5C%27echet%20Video%20Distance%20%28FVD%29%2C%20a%20prominent%20metric%20for%20evaluating%20video%0Ageneration%20models%2C%20is%20known%20to%20conflict%20with%20human%20perception%20occasionally.%20In%0Athis%20paper%2C%20we%20aim%20to%20explore%20the%20extent%20of%20FVD%27s%20bias%20toward%20per-frame%20quality%0Aover%20temporal%20realism%20and%20identify%20its%20sources.%20We%20first%20quantify%20the%20FVD%27s%0Asensitivity%20to%20the%20temporal%20axis%20by%20decoupling%20the%20frame%20and%20motion%20quality%20and%0Afind%20that%20the%20FVD%20increases%20only%20slightly%20with%20large%20temporal%20corruption.%20We%0Athen%20analyze%20the%20generated%20videos%20and%20show%20that%20via%20careful%20sampling%20from%20a%0Alarge%20set%20of%20generated%20videos%20that%20do%20not%20contain%20motions%2C%20one%20can%20drastically%0Adecrease%20FVD%20without%20improving%20the%20temporal%20quality.%20Both%20studies%20suggest%20FVD%27s%0Abias%20towards%20the%20quality%20of%20individual%20frames.%20We%20further%20observe%20that%20the%20bias%0Acan%20be%20attributed%20to%20the%20features%20extracted%20from%20a%20supervised%20video%20classifier%0Atrained%20on%20the%20content-biased%20dataset.%20We%20show%20that%20FVD%20with%20features%20extracted%0Afrom%20the%20recent%20large-scale%20self-supervised%20video%20models%20is%20less%20biased%20toward%0Aimage%20quality.%20Finally%2C%20we%20revisit%20a%20few%20real-world%20examples%20to%20validate%20our%0Ahypothesis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12391v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Content%20Bias%20in%20Fr%C3%A9chet%20Video%20Distance&entry.906535625=Songwei%20Ge%20and%20Aniruddha%20Mahapatra%20and%20Gaurav%20Parmar%20and%20Jun-Yan%20Zhu%20and%20Jia-Bin%20Huang&entry.1292438233=%20%20Fr%5C%27echet%20Video%20Distance%20%28FVD%29%2C%20a%20prominent%20metric%20for%20evaluating%20video%0Ageneration%20models%2C%20is%20known%20to%20conflict%20with%20human%20perception%20occasionally.%20In%0Athis%20paper%2C%20we%20aim%20to%20explore%20the%20extent%20of%20FVD%27s%20bias%20toward%20per-frame%20quality%0Aover%20temporal%20realism%20and%20identify%20its%20sources.%20We%20first%20quantify%20the%20FVD%27s%0Asensitivity%20to%20the%20temporal%20axis%20by%20decoupling%20the%20frame%20and%20motion%20quality%20and%0Afind%20that%20the%20FVD%20increases%20only%20slightly%20with%20large%20temporal%20corruption.%20We%0Athen%20analyze%20the%20generated%20videos%20and%20show%20that%20via%20careful%20sampling%20from%20a%0Alarge%20set%20of%20generated%20videos%20that%20do%20not%20contain%20motions%2C%20one%20can%20drastically%0Adecrease%20FVD%20without%20improving%20the%20temporal%20quality.%20Both%20studies%20suggest%20FVD%27s%0Abias%20towards%20the%20quality%20of%20individual%20frames.%20We%20further%20observe%20that%20the%20bias%0Acan%20be%20attributed%20to%20the%20features%20extracted%20from%20a%20supervised%20video%20classifier%0Atrained%20on%20the%20content-biased%20dataset.%20We%20show%20that%20FVD%20with%20features%20extracted%0Afrom%20the%20recent%20large-scale%20self-supervised%20video%20models%20is%20less%20biased%20toward%0Aimage%20quality.%20Finally%2C%20we%20revisit%20a%20few%20real-world%20examples%20to%20validate%20our%0Ahypothesis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12391v1&entry.124074799=Read"},
{"title": "AniClipart: Clipart Animation with Text-to-Video Priors", "author": "Ronghuan Wu and Wanchao Su and Kede Ma and Jing Liao", "abstract": "  Clipart, a pre-made graphic art form, offers a convenient and efficient way\nof illustrating visual content. Traditional workflows to convert static clipart\nimages into motion sequences are laborious and time-consuming, involving\nnumerous intricate steps like rigging, key animation and in-betweening. Recent\nadvancements in text-to-video generation hold great potential in resolving this\nproblem. Nevertheless, direct application of text-to-video generation models\noften struggles to retain the visual identity of clipart images or generate\ncartoon-style motions, resulting in unsatisfactory animation outcomes. In this\npaper, we introduce AniClipart, a system that transforms static clipart images\ninto high-quality motion sequences guided by text-to-video priors. To generate\ncartoon-style and smooth motion, we first define B\\'{e}zier curves over\nkeypoints of the clipart image as a form of motion regularization. We then\nalign the motion trajectories of the keypoints with the provided text prompt by\noptimizing the Video Score Distillation Sampling (VSDS) loss, which encodes\nadequate knowledge of natural motion within a pretrained text-to-video\ndiffusion model. With a differentiable As-Rigid-As-Possible shape deformation\nalgorithm, our method can be end-to-end optimized while maintaining deformation\nrigidity. Experimental results show that the proposed AniClipart consistently\noutperforms existing image-to-video generation models, in terms of text-video\nalignment, visual identity preservation, and motion consistency. Furthermore,\nwe showcase the versatility of AniClipart by adapting it to generate a broader\narray of animation formats, such as layered animation, which allows topological\nchanges.\n", "link": "http://arxiv.org/abs/2404.12347v1", "date": "2024-04-18", "relevancy": 2.2836, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6431}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5761}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5369}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20AniClipart%3A%20Clipart%20Animation%20with%20Text-to-Video%20Priors&body=Title%3A%20AniClipart%3A%20Clipart%20Animation%20with%20Text-to-Video%20Priors%0AAuthor%3A%20Ronghuan%20Wu%20and%20Wanchao%20Su%20and%20Kede%20Ma%20and%20Jing%20Liao%0AAbstract%3A%20%20%20Clipart%2C%20a%20pre-made%20graphic%20art%20form%2C%20offers%20a%20convenient%20and%20efficient%20way%0Aof%20illustrating%20visual%20content.%20Traditional%20workflows%20to%20convert%20static%20clipart%0Aimages%20into%20motion%20sequences%20are%20laborious%20and%20time-consuming%2C%20involving%0Anumerous%20intricate%20steps%20like%20rigging%2C%20key%20animation%20and%20in-betweening.%20Recent%0Aadvancements%20in%20text-to-video%20generation%20hold%20great%20potential%20in%20resolving%20this%0Aproblem.%20Nevertheless%2C%20direct%20application%20of%20text-to-video%20generation%20models%0Aoften%20struggles%20to%20retain%20the%20visual%20identity%20of%20clipart%20images%20or%20generate%0Acartoon-style%20motions%2C%20resulting%20in%20unsatisfactory%20animation%20outcomes.%20In%20this%0Apaper%2C%20we%20introduce%20AniClipart%2C%20a%20system%20that%20transforms%20static%20clipart%20images%0Ainto%20high-quality%20motion%20sequences%20guided%20by%20text-to-video%20priors.%20To%20generate%0Acartoon-style%20and%20smooth%20motion%2C%20we%20first%20define%20B%5C%27%7Be%7Dzier%20curves%20over%0Akeypoints%20of%20the%20clipart%20image%20as%20a%20form%20of%20motion%20regularization.%20We%20then%0Aalign%20the%20motion%20trajectories%20of%20the%20keypoints%20with%20the%20provided%20text%20prompt%20by%0Aoptimizing%20the%20Video%20Score%20Distillation%20Sampling%20%28VSDS%29%20loss%2C%20which%20encodes%0Aadequate%20knowledge%20of%20natural%20motion%20within%20a%20pretrained%20text-to-video%0Adiffusion%20model.%20With%20a%20differentiable%20As-Rigid-As-Possible%20shape%20deformation%0Aalgorithm%2C%20our%20method%20can%20be%20end-to-end%20optimized%20while%20maintaining%20deformation%0Arigidity.%20Experimental%20results%20show%20that%20the%20proposed%20AniClipart%20consistently%0Aoutperforms%20existing%20image-to-video%20generation%20models%2C%20in%20terms%20of%20text-video%0Aalignment%2C%20visual%20identity%20preservation%2C%20and%20motion%20consistency.%20Furthermore%2C%0Awe%20showcase%20the%20versatility%20of%20AniClipart%20by%20adapting%20it%20to%20generate%20a%20broader%0Aarray%20of%20animation%20formats%2C%20such%20as%20layered%20animation%2C%20which%20allows%20topological%0Achanges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12347v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AniClipart%3A%20Clipart%20Animation%20with%20Text-to-Video%20Priors&entry.906535625=Ronghuan%20Wu%20and%20Wanchao%20Su%20and%20Kede%20Ma%20and%20Jing%20Liao&entry.1292438233=%20%20Clipart%2C%20a%20pre-made%20graphic%20art%20form%2C%20offers%20a%20convenient%20and%20efficient%20way%0Aof%20illustrating%20visual%20content.%20Traditional%20workflows%20to%20convert%20static%20clipart%0Aimages%20into%20motion%20sequences%20are%20laborious%20and%20time-consuming%2C%20involving%0Anumerous%20intricate%20steps%20like%20rigging%2C%20key%20animation%20and%20in-betweening.%20Recent%0Aadvancements%20in%20text-to-video%20generation%20hold%20great%20potential%20in%20resolving%20this%0Aproblem.%20Nevertheless%2C%20direct%20application%20of%20text-to-video%20generation%20models%0Aoften%20struggles%20to%20retain%20the%20visual%20identity%20of%20clipart%20images%20or%20generate%0Acartoon-style%20motions%2C%20resulting%20in%20unsatisfactory%20animation%20outcomes.%20In%20this%0Apaper%2C%20we%20introduce%20AniClipart%2C%20a%20system%20that%20transforms%20static%20clipart%20images%0Ainto%20high-quality%20motion%20sequences%20guided%20by%20text-to-video%20priors.%20To%20generate%0Acartoon-style%20and%20smooth%20motion%2C%20we%20first%20define%20B%5C%27%7Be%7Dzier%20curves%20over%0Akeypoints%20of%20the%20clipart%20image%20as%20a%20form%20of%20motion%20regularization.%20We%20then%0Aalign%20the%20motion%20trajectories%20of%20the%20keypoints%20with%20the%20provided%20text%20prompt%20by%0Aoptimizing%20the%20Video%20Score%20Distillation%20Sampling%20%28VSDS%29%20loss%2C%20which%20encodes%0Aadequate%20knowledge%20of%20natural%20motion%20within%20a%20pretrained%20text-to-video%0Adiffusion%20model.%20With%20a%20differentiable%20As-Rigid-As-Possible%20shape%20deformation%0Aalgorithm%2C%20our%20method%20can%20be%20end-to-end%20optimized%20while%20maintaining%20deformation%0Arigidity.%20Experimental%20results%20show%20that%20the%20proposed%20AniClipart%20consistently%0Aoutperforms%20existing%20image-to-video%20generation%20models%2C%20in%20terms%20of%20text-video%0Aalignment%2C%20visual%20identity%20preservation%2C%20and%20motion%20consistency.%20Furthermore%2C%0Awe%20showcase%20the%20versatility%20of%20AniClipart%20by%20adapting%20it%20to%20generate%20a%20broader%0Aarray%20of%20animation%20formats%2C%20such%20as%20layered%20animation%2C%20which%20allows%20topological%0Achanges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12347v1&entry.124074799=Read"},
{"title": "Measuring Feature Dependency of Neural Networks by Collapsing Feature\n  Dimensions in the Data Manifold", "author": "Yinzhu Jin and Matthew B. Dwyer and P. Thomas Fletcher", "abstract": "  This paper introduces a new technique to measure the feature dependency of\nneural network models. The motivation is to better understand a model by\nquerying whether it is using information from human-understandable features,\ne.g., anatomical shape, volume, or image texture. Our method is based on the\nprinciple that if a model is dependent on a feature, then removal of that\nfeature should significantly harm its performance. A targeted feature is\n\"removed\" by collapsing the dimension in the data distribution that corresponds\nto that feature. We perform this by moving data points along the feature\ndimension to a baseline feature value while staying on the data manifold, as\nestimated by a deep generative model. Then we observe how the model's\nperformance changes on the modified test data set, with the target feature\ndimension removed. We test our method on deep neural network models trained on\nsynthetic image data with known ground truth, an Alzheimer's disease prediction\ntask using MRI and hippocampus segmentations from the OASIS-3 dataset, and a\ncell nuclei classification task using the Lizard dataset.\n", "link": "http://arxiv.org/abs/2404.12341v1", "date": "2024-04-18", "relevancy": 2.2775, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4616}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.454}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4509}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Measuring%20Feature%20Dependency%20of%20Neural%20Networks%20by%20Collapsing%20Feature%0A%20%20Dimensions%20in%20the%20Data%20Manifold&body=Title%3A%20Measuring%20Feature%20Dependency%20of%20Neural%20Networks%20by%20Collapsing%20Feature%0A%20%20Dimensions%20in%20the%20Data%20Manifold%0AAuthor%3A%20Yinzhu%20Jin%20and%20Matthew%20B.%20Dwyer%20and%20P.%20Thomas%20Fletcher%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20new%20technique%20to%20measure%20the%20feature%20dependency%20of%0Aneural%20network%20models.%20The%20motivation%20is%20to%20better%20understand%20a%20model%20by%0Aquerying%20whether%20it%20is%20using%20information%20from%20human-understandable%20features%2C%0Ae.g.%2C%20anatomical%20shape%2C%20volume%2C%20or%20image%20texture.%20Our%20method%20is%20based%20on%20the%0Aprinciple%20that%20if%20a%20model%20is%20dependent%20on%20a%20feature%2C%20then%20removal%20of%20that%0Afeature%20should%20significantly%20harm%20its%20performance.%20A%20targeted%20feature%20is%0A%22removed%22%20by%20collapsing%20the%20dimension%20in%20the%20data%20distribution%20that%20corresponds%0Ato%20that%20feature.%20We%20perform%20this%20by%20moving%20data%20points%20along%20the%20feature%0Adimension%20to%20a%20baseline%20feature%20value%20while%20staying%20on%20the%20data%20manifold%2C%20as%0Aestimated%20by%20a%20deep%20generative%20model.%20Then%20we%20observe%20how%20the%20model%27s%0Aperformance%20changes%20on%20the%20modified%20test%20data%20set%2C%20with%20the%20target%20feature%0Adimension%20removed.%20We%20test%20our%20method%20on%20deep%20neural%20network%20models%20trained%20on%0Asynthetic%20image%20data%20with%20known%20ground%20truth%2C%20an%20Alzheimer%27s%20disease%20prediction%0Atask%20using%20MRI%20and%20hippocampus%20segmentations%20from%20the%20OASIS-3%20dataset%2C%20and%20a%0Acell%20nuclei%20classification%20task%20using%20the%20Lizard%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12341v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20Feature%20Dependency%20of%20Neural%20Networks%20by%20Collapsing%20Feature%0A%20%20Dimensions%20in%20the%20Data%20Manifold&entry.906535625=Yinzhu%20Jin%20and%20Matthew%20B.%20Dwyer%20and%20P.%20Thomas%20Fletcher&entry.1292438233=%20%20This%20paper%20introduces%20a%20new%20technique%20to%20measure%20the%20feature%20dependency%20of%0Aneural%20network%20models.%20The%20motivation%20is%20to%20better%20understand%20a%20model%20by%0Aquerying%20whether%20it%20is%20using%20information%20from%20human-understandable%20features%2C%0Ae.g.%2C%20anatomical%20shape%2C%20volume%2C%20or%20image%20texture.%20Our%20method%20is%20based%20on%20the%0Aprinciple%20that%20if%20a%20model%20is%20dependent%20on%20a%20feature%2C%20then%20removal%20of%20that%0Afeature%20should%20significantly%20harm%20its%20performance.%20A%20targeted%20feature%20is%0A%22removed%22%20by%20collapsing%20the%20dimension%20in%20the%20data%20distribution%20that%20corresponds%0Ato%20that%20feature.%20We%20perform%20this%20by%20moving%20data%20points%20along%20the%20feature%0Adimension%20to%20a%20baseline%20feature%20value%20while%20staying%20on%20the%20data%20manifold%2C%20as%0Aestimated%20by%20a%20deep%20generative%20model.%20Then%20we%20observe%20how%20the%20model%27s%0Aperformance%20changes%20on%20the%20modified%20test%20data%20set%2C%20with%20the%20target%20feature%0Adimension%20removed.%20We%20test%20our%20method%20on%20deep%20neural%20network%20models%20trained%20on%0Asynthetic%20image%20data%20with%20known%20ground%20truth%2C%20an%20Alzheimer%27s%20disease%20prediction%0Atask%20using%20MRI%20and%20hippocampus%20segmentations%20from%20the%20OASIS-3%20dataset%2C%20and%20a%0Acell%20nuclei%20classification%20task%20using%20the%20Lizard%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12341v1&entry.124074799=Read"},
{"title": "VideoGigaGAN: Towards Detail-rich Video Super-Resolution", "author": "Yiran Xu and Taesung Park and Richard Zhang and Yang Zhou and Eli Shechtman and Feng Liu and Jia-Bin Huang and Difan Liu", "abstract": "  Video super-resolution (VSR) approaches have shown impressive temporal\nconsistency in upsampled videos. However, these approaches tend to generate\nblurrier results than their image counterparts as they are limited in their\ngenerative capability. This raises a fundamental question: can we extend the\nsuccess of a generative image upsampler to the VSR task while preserving the\ntemporal consistency? We introduce VideoGigaGAN, a new generative VSR model\nthat can produce videos with high-frequency details and temporal consistency.\nVideoGigaGAN builds upon a large-scale image upsampler -- GigaGAN. Simply\ninflating GigaGAN to a video model by adding temporal modules produces severe\ntemporal flickering. We identify several key issues and propose techniques that\nsignificantly improve the temporal consistency of upsampled videos. Our\nexperiments show that, unlike previous VSR methods, VideoGigaGAN generates\ntemporally consistent videos with more fine-grained appearance details. We\nvalidate the effectiveness of VideoGigaGAN by comparing it with\nstate-of-the-art VSR models on public datasets and showcasing video results\nwith $8\\times$ super-resolution.\n", "link": "http://arxiv.org/abs/2404.12388v1", "date": "2024-04-18", "relevancy": 2.2644, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5852}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.559}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5363}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VideoGigaGAN%3A%20Towards%20Detail-rich%20Video%20Super-Resolution&body=Title%3A%20VideoGigaGAN%3A%20Towards%20Detail-rich%20Video%20Super-Resolution%0AAuthor%3A%20Yiran%20Xu%20and%20Taesung%20Park%20and%20Richard%20Zhang%20and%20Yang%20Zhou%20and%20Eli%20Shechtman%20and%20Feng%20Liu%20and%20Jia-Bin%20Huang%20and%20Difan%20Liu%0AAbstract%3A%20%20%20Video%20super-resolution%20%28VSR%29%20approaches%20have%20shown%20impressive%20temporal%0Aconsistency%20in%20upsampled%20videos.%20However%2C%20these%20approaches%20tend%20to%20generate%0Ablurrier%20results%20than%20their%20image%20counterparts%20as%20they%20are%20limited%20in%20their%0Agenerative%20capability.%20This%20raises%20a%20fundamental%20question%3A%20can%20we%20extend%20the%0Asuccess%20of%20a%20generative%20image%20upsampler%20to%20the%20VSR%20task%20while%20preserving%20the%0Atemporal%20consistency%3F%20We%20introduce%20VideoGigaGAN%2C%20a%20new%20generative%20VSR%20model%0Athat%20can%20produce%20videos%20with%20high-frequency%20details%20and%20temporal%20consistency.%0AVideoGigaGAN%20builds%20upon%20a%20large-scale%20image%20upsampler%20--%20GigaGAN.%20Simply%0Ainflating%20GigaGAN%20to%20a%20video%20model%20by%20adding%20temporal%20modules%20produces%20severe%0Atemporal%20flickering.%20We%20identify%20several%20key%20issues%20and%20propose%20techniques%20that%0Asignificantly%20improve%20the%20temporal%20consistency%20of%20upsampled%20videos.%20Our%0Aexperiments%20show%20that%2C%20unlike%20previous%20VSR%20methods%2C%20VideoGigaGAN%20generates%0Atemporally%20consistent%20videos%20with%20more%20fine-grained%20appearance%20details.%20We%0Avalidate%20the%20effectiveness%20of%20VideoGigaGAN%20by%20comparing%20it%20with%0Astate-of-the-art%20VSR%20models%20on%20public%20datasets%20and%20showcasing%20video%20results%0Awith%20%248%5Ctimes%24%20super-resolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12388v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoGigaGAN%3A%20Towards%20Detail-rich%20Video%20Super-Resolution&entry.906535625=Yiran%20Xu%20and%20Taesung%20Park%20and%20Richard%20Zhang%20and%20Yang%20Zhou%20and%20Eli%20Shechtman%20and%20Feng%20Liu%20and%20Jia-Bin%20Huang%20and%20Difan%20Liu&entry.1292438233=%20%20Video%20super-resolution%20%28VSR%29%20approaches%20have%20shown%20impressive%20temporal%0Aconsistency%20in%20upsampled%20videos.%20However%2C%20these%20approaches%20tend%20to%20generate%0Ablurrier%20results%20than%20their%20image%20counterparts%20as%20they%20are%20limited%20in%20their%0Agenerative%20capability.%20This%20raises%20a%20fundamental%20question%3A%20can%20we%20extend%20the%0Asuccess%20of%20a%20generative%20image%20upsampler%20to%20the%20VSR%20task%20while%20preserving%20the%0Atemporal%20consistency%3F%20We%20introduce%20VideoGigaGAN%2C%20a%20new%20generative%20VSR%20model%0Athat%20can%20produce%20videos%20with%20high-frequency%20details%20and%20temporal%20consistency.%0AVideoGigaGAN%20builds%20upon%20a%20large-scale%20image%20upsampler%20--%20GigaGAN.%20Simply%0Ainflating%20GigaGAN%20to%20a%20video%20model%20by%20adding%20temporal%20modules%20produces%20severe%0Atemporal%20flickering.%20We%20identify%20several%20key%20issues%20and%20propose%20techniques%20that%0Asignificantly%20improve%20the%20temporal%20consistency%20of%20upsampled%20videos.%20Our%0Aexperiments%20show%20that%2C%20unlike%20previous%20VSR%20methods%2C%20VideoGigaGAN%20generates%0Atemporally%20consistent%20videos%20with%20more%20fine-grained%20appearance%20details.%20We%0Avalidate%20the%20effectiveness%20of%20VideoGigaGAN%20by%20comparing%20it%20with%0Astate-of-the-art%20VSR%20models%20on%20public%20datasets%20and%20showcasing%20video%20results%0Awith%20%248%5Ctimes%24%20super-resolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12388v1&entry.124074799=Read"},
{"title": "V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt\n  Instruction Tuning", "author": "Hang Hua and Yunlong Tang and Chenliang Xu and Jiebo Luo", "abstract": "  Video summarization aims to create short, accurate, and cohesive summaries of\nlonger videos. Despite the existence of various video summarization datasets, a\nnotable limitation is their limited amount of source videos, which hampers the\neffective fine-tuning of advanced large vision-language models (VLMs).\nAdditionally, most existing datasets are created for video-to-video\nsummarization, overlooking the contemporary need for multimodal video content\nsummarization. Recent efforts have been made to expand from unimodal to\nmultimodal video summarization, categorizing the task into three sub-tasks\nbased on the summary's modality: video-to-video (V2V), video-to-text (V2T), and\na combination of video and text summarization (V2VT). However, the textual\nsummaries in previous multimodal datasets are inadequate. To address these\nissues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset\nfeaturing 30,000 diverse videos sourced from YouTube, with lengths ranging from\n40 to 940 seconds and an average summarization ratio of 16.39\\%. Each video\nsummary in Instruct-V2Xum is paired with a textual summary that references\nspecific frame indexes, facilitating the generation of aligned video and\ntextual summaries. In addition, we propose a new video summarization framework\nnamed V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the\nfirst framework that unifies different video summarization tasks into one large\nlanguage model's (LLM) text decoder and achieves task-controllable video\nsummarization with temporal prompts and task instructions. Experiments show\nthat V2Xum-LLaMA outperforms strong baseline models on multiple video\nsummarization tasks. Furthermore, we propose an enhanced evaluation metric for\nV2V and V2VT summarization tasks.\n", "link": "http://arxiv.org/abs/2404.12353v1", "date": "2024-04-18", "relevancy": 2.2475, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5834}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5767}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5344}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20V2Xum-LLM%3A%20Cross-Modal%20Video%20Summarization%20with%20Temporal%20Prompt%0A%20%20Instruction%20Tuning&body=Title%3A%20V2Xum-LLM%3A%20Cross-Modal%20Video%20Summarization%20with%20Temporal%20Prompt%0A%20%20Instruction%20Tuning%0AAuthor%3A%20Hang%20Hua%20and%20Yunlong%20Tang%20and%20Chenliang%20Xu%20and%20Jiebo%20Luo%0AAbstract%3A%20%20%20Video%20summarization%20aims%20to%20create%20short%2C%20accurate%2C%20and%20cohesive%20summaries%20of%0Alonger%20videos.%20Despite%20the%20existence%20of%20various%20video%20summarization%20datasets%2C%20a%0Anotable%20limitation%20is%20their%20limited%20amount%20of%20source%20videos%2C%20which%20hampers%20the%0Aeffective%20fine-tuning%20of%20advanced%20large%20vision-language%20models%20%28VLMs%29.%0AAdditionally%2C%20most%20existing%20datasets%20are%20created%20for%20video-to-video%0Asummarization%2C%20overlooking%20the%20contemporary%20need%20for%20multimodal%20video%20content%0Asummarization.%20Recent%20efforts%20have%20been%20made%20to%20expand%20from%20unimodal%20to%0Amultimodal%20video%20summarization%2C%20categorizing%20the%20task%20into%20three%20sub-tasks%0Abased%20on%20the%20summary%27s%20modality%3A%20video-to-video%20%28V2V%29%2C%20video-to-text%20%28V2T%29%2C%20and%0Aa%20combination%20of%20video%20and%20text%20summarization%20%28V2VT%29.%20However%2C%20the%20textual%0Asummaries%20in%20previous%20multimodal%20datasets%20are%20inadequate.%20To%20address%20these%0Aissues%2C%20we%20introduce%20Instruct-V2Xum%2C%20a%20cross-modal%20video%20summarization%20dataset%0Afeaturing%2030%2C000%20diverse%20videos%20sourced%20from%20YouTube%2C%20with%20lengths%20ranging%20from%0A40%20to%20940%20seconds%20and%20an%20average%20summarization%20ratio%20of%2016.39%5C%25.%20Each%20video%0Asummary%20in%20Instruct-V2Xum%20is%20paired%20with%20a%20textual%20summary%20that%20references%0Aspecific%20frame%20indexes%2C%20facilitating%20the%20generation%20of%20aligned%20video%20and%0Atextual%20summaries.%20In%20addition%2C%20we%20propose%20a%20new%20video%20summarization%20framework%0Anamed%20V2Xum-LLM.%20V2Xum-LLM%2C%20specifically%20V2Xum-LLaMA%20in%20this%20study%2C%20is%20the%0Afirst%20framework%20that%20unifies%20different%20video%20summarization%20tasks%20into%20one%20large%0Alanguage%20model%27s%20%28LLM%29%20text%20decoder%20and%20achieves%20task-controllable%20video%0Asummarization%20with%20temporal%20prompts%20and%20task%20instructions.%20Experiments%20show%0Athat%20V2Xum-LLaMA%20outperforms%20strong%20baseline%20models%20on%20multiple%20video%0Asummarization%20tasks.%20Furthermore%2C%20we%20propose%20an%20enhanced%20evaluation%20metric%20for%0AV2V%20and%20V2VT%20summarization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12353v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V2Xum-LLM%3A%20Cross-Modal%20Video%20Summarization%20with%20Temporal%20Prompt%0A%20%20Instruction%20Tuning&entry.906535625=Hang%20Hua%20and%20Yunlong%20Tang%20and%20Chenliang%20Xu%20and%20Jiebo%20Luo&entry.1292438233=%20%20Video%20summarization%20aims%20to%20create%20short%2C%20accurate%2C%20and%20cohesive%20summaries%20of%0Alonger%20videos.%20Despite%20the%20existence%20of%20various%20video%20summarization%20datasets%2C%20a%0Anotable%20limitation%20is%20their%20limited%20amount%20of%20source%20videos%2C%20which%20hampers%20the%0Aeffective%20fine-tuning%20of%20advanced%20large%20vision-language%20models%20%28VLMs%29.%0AAdditionally%2C%20most%20existing%20datasets%20are%20created%20for%20video-to-video%0Asummarization%2C%20overlooking%20the%20contemporary%20need%20for%20multimodal%20video%20content%0Asummarization.%20Recent%20efforts%20have%20been%20made%20to%20expand%20from%20unimodal%20to%0Amultimodal%20video%20summarization%2C%20categorizing%20the%20task%20into%20three%20sub-tasks%0Abased%20on%20the%20summary%27s%20modality%3A%20video-to-video%20%28V2V%29%2C%20video-to-text%20%28V2T%29%2C%20and%0Aa%20combination%20of%20video%20and%20text%20summarization%20%28V2VT%29.%20However%2C%20the%20textual%0Asummaries%20in%20previous%20multimodal%20datasets%20are%20inadequate.%20To%20address%20these%0Aissues%2C%20we%20introduce%20Instruct-V2Xum%2C%20a%20cross-modal%20video%20summarization%20dataset%0Afeaturing%2030%2C000%20diverse%20videos%20sourced%20from%20YouTube%2C%20with%20lengths%20ranging%20from%0A40%20to%20940%20seconds%20and%20an%20average%20summarization%20ratio%20of%2016.39%5C%25.%20Each%20video%0Asummary%20in%20Instruct-V2Xum%20is%20paired%20with%20a%20textual%20summary%20that%20references%0Aspecific%20frame%20indexes%2C%20facilitating%20the%20generation%20of%20aligned%20video%20and%0Atextual%20summaries.%20In%20addition%2C%20we%20propose%20a%20new%20video%20summarization%20framework%0Anamed%20V2Xum-LLM.%20V2Xum-LLM%2C%20specifically%20V2Xum-LLaMA%20in%20this%20study%2C%20is%20the%0Afirst%20framework%20that%20unifies%20different%20video%20summarization%20tasks%20into%20one%20large%0Alanguage%20model%27s%20%28LLM%29%20text%20decoder%20and%20achieves%20task-controllable%20video%0Asummarization%20with%20temporal%20prompts%20and%20task%20instructions.%20Experiments%20show%0Athat%20V2Xum-LLaMA%20outperforms%20strong%20baseline%20models%20on%20multiple%20video%0Asummarization%20tasks.%20Furthermore%2C%20we%20propose%20an%20enhanced%20evaluation%20metric%20for%0AV2V%20and%20V2VT%20summarization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12353v1&entry.124074799=Read"},
{"title": "6Img-to-3D: Few-Image Large-Scale Outdoor Driving Scene Reconstruction", "author": "Th\u00e9o Gieruc and Marius K\u00e4stingsch\u00e4fer and Sebastian Bernhard and Mathieu Salzmann", "abstract": "  Current 3D reconstruction techniques struggle to infer unbounded scenes from\na few images faithfully. Specifically, existing methods have high computational\ndemands, require detailed pose information, and cannot reconstruct occluded\nregions reliably. We introduce 6Img-to-3D, an efficient, scalable\ntransformer-based encoder-renderer method for single-shot image to 3D\nreconstruction. Our method outputs a 3D-consistent parameterized triplane from\nonly six outward-facing input images for large-scale, unbounded outdoor driving\nscenarios. We take a step towards resolving existing shortcomings by combining\ncontracted custom cross- and self-attention mechanisms for triplane\nparameterization, differentiable volume rendering, scene contraction, and image\nfeature projection. We showcase that six surround-view vehicle images from a\nsingle timestamp without global pose information are enough to reconstruct\n360$^{\\circ}$ scenes during inference time, taking 395 ms. Our method allows,\nfor example, rendering third-person images and birds-eye views. Our code is\navailable at https://github.com/continental/6Img-to-3D, and more examples can\nbe found at our website here https://6Img-to-3D.GitHub.io/.\n", "link": "http://arxiv.org/abs/2404.12378v1", "date": "2024-04-18", "relevancy": 2.2308, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5721}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5556}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5442}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%206Img-to-3D%3A%20Few-Image%20Large-Scale%20Outdoor%20Driving%20Scene%20Reconstruction&body=Title%3A%206Img-to-3D%3A%20Few-Image%20Large-Scale%20Outdoor%20Driving%20Scene%20Reconstruction%0AAuthor%3A%20Th%C3%A9o%20Gieruc%20and%20Marius%20K%C3%A4stingsch%C3%A4fer%20and%20Sebastian%20Bernhard%20and%20Mathieu%20Salzmann%0AAbstract%3A%20%20%20Current%203D%20reconstruction%20techniques%20struggle%20to%20infer%20unbounded%20scenes%20from%0Aa%20few%20images%20faithfully.%20Specifically%2C%20existing%20methods%20have%20high%20computational%0Ademands%2C%20require%20detailed%20pose%20information%2C%20and%20cannot%20reconstruct%20occluded%0Aregions%20reliably.%20We%20introduce%206Img-to-3D%2C%20an%20efficient%2C%20scalable%0Atransformer-based%20encoder-renderer%20method%20for%20single-shot%20image%20to%203D%0Areconstruction.%20Our%20method%20outputs%20a%203D-consistent%20parameterized%20triplane%20from%0Aonly%20six%20outward-facing%20input%20images%20for%20large-scale%2C%20unbounded%20outdoor%20driving%0Ascenarios.%20We%20take%20a%20step%20towards%20resolving%20existing%20shortcomings%20by%20combining%0Acontracted%20custom%20cross-%20and%20self-attention%20mechanisms%20for%20triplane%0Aparameterization%2C%20differentiable%20volume%20rendering%2C%20scene%20contraction%2C%20and%20image%0Afeature%20projection.%20We%20showcase%20that%20six%20surround-view%20vehicle%20images%20from%20a%0Asingle%20timestamp%20without%20global%20pose%20information%20are%20enough%20to%20reconstruct%0A360%24%5E%7B%5Ccirc%7D%24%20scenes%20during%20inference%20time%2C%20taking%20395%20ms.%20Our%20method%20allows%2C%0Afor%20example%2C%20rendering%20third-person%20images%20and%20birds-eye%20views.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/continental/6Img-to-3D%2C%20and%20more%20examples%20can%0Abe%20found%20at%20our%20website%20here%20https%3A//6Img-to-3D.GitHub.io/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12378v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=6Img-to-3D%3A%20Few-Image%20Large-Scale%20Outdoor%20Driving%20Scene%20Reconstruction&entry.906535625=Th%C3%A9o%20Gieruc%20and%20Marius%20K%C3%A4stingsch%C3%A4fer%20and%20Sebastian%20Bernhard%20and%20Mathieu%20Salzmann&entry.1292438233=%20%20Current%203D%20reconstruction%20techniques%20struggle%20to%20infer%20unbounded%20scenes%20from%0Aa%20few%20images%20faithfully.%20Specifically%2C%20existing%20methods%20have%20high%20computational%0Ademands%2C%20require%20detailed%20pose%20information%2C%20and%20cannot%20reconstruct%20occluded%0Aregions%20reliably.%20We%20introduce%206Img-to-3D%2C%20an%20efficient%2C%20scalable%0Atransformer-based%20encoder-renderer%20method%20for%20single-shot%20image%20to%203D%0Areconstruction.%20Our%20method%20outputs%20a%203D-consistent%20parameterized%20triplane%20from%0Aonly%20six%20outward-facing%20input%20images%20for%20large-scale%2C%20unbounded%20outdoor%20driving%0Ascenarios.%20We%20take%20a%20step%20towards%20resolving%20existing%20shortcomings%20by%20combining%0Acontracted%20custom%20cross-%20and%20self-attention%20mechanisms%20for%20triplane%0Aparameterization%2C%20differentiable%20volume%20rendering%2C%20scene%20contraction%2C%20and%20image%0Afeature%20projection.%20We%20showcase%20that%20six%20surround-view%20vehicle%20images%20from%20a%0Asingle%20timestamp%20without%20global%20pose%20information%20are%20enough%20to%20reconstruct%0A360%24%5E%7B%5Ccirc%7D%24%20scenes%20during%20inference%20time%2C%20taking%20395%20ms.%20Our%20method%20allows%2C%0Afor%20example%2C%20rendering%20third-person%20images%20and%20birds-eye%20views.%20Our%20code%20is%0Aavailable%20at%20https%3A//github.com/continental/6Img-to-3D%2C%20and%20more%20examples%20can%0Abe%20found%20at%20our%20website%20here%20https%3A//6Img-to-3D.GitHub.io/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12378v1&entry.124074799=Read"},
{"title": "SPOT: Point Cloud Based Stereo Visual Place Recognition for Similar and\n  Opposing Viewpoints", "author": "Spencer Carmichael and Rahul Agrawal and Ram Vasudevan and Katherine A. Skinner", "abstract": "  Recognizing places from an opposing viewpoint during a return trip is a\ncommon experience for human drivers. However, the analogous robotics\ncapability, visual place recognition (VPR) with limited field of view cameras\nunder 180 degree rotations, has proven to be challenging to achieve. To address\nthis problem, this paper presents Same Place Opposing Trajectory (SPOT), a\ntechnique for opposing viewpoint VPR that relies exclusively on structure\nestimated through stereo visual odometry (VO). The method extends recent\nadvances in lidar descriptors and utilizes a novel double (similar and\nopposing) distance matrix sequence matching method. We evaluate SPOT on a\npublicly available dataset with 6.7-7.6 km routes driven in similar and\nopposing directions under various lighting conditions. The proposed algorithm\ndemonstrates remarkable improvement over the state-of-the-art, achieving up to\n91.7% recall at 100% precision in opposing viewpoint cases, while requiring\nless storage than all baselines tested and running faster than all but one.\nMoreover, the proposed method assumes no a priori knowledge of whether the\nviewpoint is similar or opposing, and also demonstrates competitive performance\nin similar viewpoint cases.\n", "link": "http://arxiv.org/abs/2404.12339v1", "date": "2024-04-18", "relevancy": 2.1919, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5572}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5439}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5349}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SPOT%3A%20Point%20Cloud%20Based%20Stereo%20Visual%20Place%20Recognition%20for%20Similar%20and%0A%20%20Opposing%20Viewpoints&body=Title%3A%20SPOT%3A%20Point%20Cloud%20Based%20Stereo%20Visual%20Place%20Recognition%20for%20Similar%20and%0A%20%20Opposing%20Viewpoints%0AAuthor%3A%20Spencer%20Carmichael%20and%20Rahul%20Agrawal%20and%20Ram%20Vasudevan%20and%20Katherine%20A.%20Skinner%0AAbstract%3A%20%20%20Recognizing%20places%20from%20an%20opposing%20viewpoint%20during%20a%20return%20trip%20is%20a%0Acommon%20experience%20for%20human%20drivers.%20However%2C%20the%20analogous%20robotics%0Acapability%2C%20visual%20place%20recognition%20%28VPR%29%20with%20limited%20field%20of%20view%20cameras%0Aunder%20180%20degree%20rotations%2C%20has%20proven%20to%20be%20challenging%20to%20achieve.%20To%20address%0Athis%20problem%2C%20this%20paper%20presents%20Same%20Place%20Opposing%20Trajectory%20%28SPOT%29%2C%20a%0Atechnique%20for%20opposing%20viewpoint%20VPR%20that%20relies%20exclusively%20on%20structure%0Aestimated%20through%20stereo%20visual%20odometry%20%28VO%29.%20The%20method%20extends%20recent%0Aadvances%20in%20lidar%20descriptors%20and%20utilizes%20a%20novel%20double%20%28similar%20and%0Aopposing%29%20distance%20matrix%20sequence%20matching%20method.%20We%20evaluate%20SPOT%20on%20a%0Apublicly%20available%20dataset%20with%206.7-7.6%20km%20routes%20driven%20in%20similar%20and%0Aopposing%20directions%20under%20various%20lighting%20conditions.%20The%20proposed%20algorithm%0Ademonstrates%20remarkable%20improvement%20over%20the%20state-of-the-art%2C%20achieving%20up%20to%0A91.7%25%20recall%20at%20100%25%20precision%20in%20opposing%20viewpoint%20cases%2C%20while%20requiring%0Aless%20storage%20than%20all%20baselines%20tested%20and%20running%20faster%20than%20all%20but%20one.%0AMoreover%2C%20the%20proposed%20method%20assumes%20no%20a%20priori%20knowledge%20of%20whether%20the%0Aviewpoint%20is%20similar%20or%20opposing%2C%20and%20also%20demonstrates%20competitive%20performance%0Ain%20similar%20viewpoint%20cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12339v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPOT%3A%20Point%20Cloud%20Based%20Stereo%20Visual%20Place%20Recognition%20for%20Similar%20and%0A%20%20Opposing%20Viewpoints&entry.906535625=Spencer%20Carmichael%20and%20Rahul%20Agrawal%20and%20Ram%20Vasudevan%20and%20Katherine%20A.%20Skinner&entry.1292438233=%20%20Recognizing%20places%20from%20an%20opposing%20viewpoint%20during%20a%20return%20trip%20is%20a%0Acommon%20experience%20for%20human%20drivers.%20However%2C%20the%20analogous%20robotics%0Acapability%2C%20visual%20place%20recognition%20%28VPR%29%20with%20limited%20field%20of%20view%20cameras%0Aunder%20180%20degree%20rotations%2C%20has%20proven%20to%20be%20challenging%20to%20achieve.%20To%20address%0Athis%20problem%2C%20this%20paper%20presents%20Same%20Place%20Opposing%20Trajectory%20%28SPOT%29%2C%20a%0Atechnique%20for%20opposing%20viewpoint%20VPR%20that%20relies%20exclusively%20on%20structure%0Aestimated%20through%20stereo%20visual%20odometry%20%28VO%29.%20The%20method%20extends%20recent%0Aadvances%20in%20lidar%20descriptors%20and%20utilizes%20a%20novel%20double%20%28similar%20and%0Aopposing%29%20distance%20matrix%20sequence%20matching%20method.%20We%20evaluate%20SPOT%20on%20a%0Apublicly%20available%20dataset%20with%206.7-7.6%20km%20routes%20driven%20in%20similar%20and%0Aopposing%20directions%20under%20various%20lighting%20conditions.%20The%20proposed%20algorithm%0Ademonstrates%20remarkable%20improvement%20over%20the%20state-of-the-art%2C%20achieving%20up%20to%0A91.7%25%20recall%20at%20100%25%20precision%20in%20opposing%20viewpoint%20cases%2C%20while%20requiring%0Aless%20storage%20than%20all%20baselines%20tested%20and%20running%20faster%20than%20all%20but%20one.%0AMoreover%2C%20the%20proposed%20method%20assumes%20no%20a%20priori%20knowledge%20of%20whether%20the%0Aviewpoint%20is%20similar%20or%20opposing%2C%20and%20also%20demonstrates%20competitive%20performance%0Ain%20similar%20viewpoint%20cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12339v1&entry.124074799=Read"},
{"title": "iRAG: An Incremental Retrieval Augmented Generation System for Videos", "author": "Md Adnan Arefeen and Biplob Debnath and Md Yusuf Sarwar Uddin and Srimat Chakradhar", "abstract": "  Retrieval augmented generation (RAG) systems combine the strengths of\nlanguage generation and information retrieval to power many real-world\napplications like chatbots. Use of RAG for combined understanding of multimodal\ndata such as text, images and videos is appealing but two critical limitations\nexist: one-time, upfront capture of all content in large multimodal data as\ntext descriptions entails high processing times, and not all information in the\nrich multimodal data is typically in the text descriptions. Since the user\nqueries are not known apriori, developing a system for multimodal to text\nconversion and interactive querying of multimodal data is challenging.\n  To address these limitations, we propose iRAG, which augments RAG with a\nnovel incremental workflow to enable interactive querying of large corpus of\nmultimodal data. Unlike traditional RAG, iRAG quickly indexes large\nrepositories of multimodal data, and in the incremental workflow, it uses the\nindex to opportunistically extract more details from select portions of the\nmultimodal data to retrieve context relevant to an interactive user query. Such\nan incremental workflow avoids long multimodal to text conversion times,\novercomes information loss issues by doing on-demand query-specific extraction\nof details in multimodal data, and ensures high quality of responses to\ninteractive user queries that are often not known apriori. To the best of our\nknowledge, iRAG is the first system to augment RAG with an incremental workflow\nto support efficient interactive querying of large, real-world multimodal data.\nExperimental results on real-world long videos demonstrate 23x to 25x faster\nvideo to text ingestion, while ensuring that quality of responses to\ninteractive user queries is comparable to responses from a traditional RAG\nwhere all video data is converted to text upfront before any querying.\n", "link": "http://arxiv.org/abs/2404.12309v1", "date": "2024-04-18", "relevancy": 2.1765, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5531}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5392}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5372}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20iRAG%3A%20An%20Incremental%20Retrieval%20Augmented%20Generation%20System%20for%20Videos&body=Title%3A%20iRAG%3A%20An%20Incremental%20Retrieval%20Augmented%20Generation%20System%20for%20Videos%0AAuthor%3A%20Md%20Adnan%20Arefeen%20and%20Biplob%20Debnath%20and%20Md%20Yusuf%20Sarwar%20Uddin%20and%20Srimat%20Chakradhar%0AAbstract%3A%20%20%20Retrieval%20augmented%20generation%20%28RAG%29%20systems%20combine%20the%20strengths%20of%0Alanguage%20generation%20and%20information%20retrieval%20to%20power%20many%20real-world%0Aapplications%20like%20chatbots.%20Use%20of%20RAG%20for%20combined%20understanding%20of%20multimodal%0Adata%20such%20as%20text%2C%20images%20and%20videos%20is%20appealing%20but%20two%20critical%20limitations%0Aexist%3A%20one-time%2C%20upfront%20capture%20of%20all%20content%20in%20large%20multimodal%20data%20as%0Atext%20descriptions%20entails%20high%20processing%20times%2C%20and%20not%20all%20information%20in%20the%0Arich%20multimodal%20data%20is%20typically%20in%20the%20text%20descriptions.%20Since%20the%20user%0Aqueries%20are%20not%20known%20apriori%2C%20developing%20a%20system%20for%20multimodal%20to%20text%0Aconversion%20and%20interactive%20querying%20of%20multimodal%20data%20is%20challenging.%0A%20%20To%20address%20these%20limitations%2C%20we%20propose%20iRAG%2C%20which%20augments%20RAG%20with%20a%0Anovel%20incremental%20workflow%20to%20enable%20interactive%20querying%20of%20large%20corpus%20of%0Amultimodal%20data.%20Unlike%20traditional%20RAG%2C%20iRAG%20quickly%20indexes%20large%0Arepositories%20of%20multimodal%20data%2C%20and%20in%20the%20incremental%20workflow%2C%20it%20uses%20the%0Aindex%20to%20opportunistically%20extract%20more%20details%20from%20select%20portions%20of%20the%0Amultimodal%20data%20to%20retrieve%20context%20relevant%20to%20an%20interactive%20user%20query.%20Such%0Aan%20incremental%20workflow%20avoids%20long%20multimodal%20to%20text%20conversion%20times%2C%0Aovercomes%20information%20loss%20issues%20by%20doing%20on-demand%20query-specific%20extraction%0Aof%20details%20in%20multimodal%20data%2C%20and%20ensures%20high%20quality%20of%20responses%20to%0Ainteractive%20user%20queries%20that%20are%20often%20not%20known%20apriori.%20To%20the%20best%20of%20our%0Aknowledge%2C%20iRAG%20is%20the%20first%20system%20to%20augment%20RAG%20with%20an%20incremental%20workflow%0Ato%20support%20efficient%20interactive%20querying%20of%20large%2C%20real-world%20multimodal%20data.%0AExperimental%20results%20on%20real-world%20long%20videos%20demonstrate%2023x%20to%2025x%20faster%0Avideo%20to%20text%20ingestion%2C%20while%20ensuring%20that%20quality%20of%20responses%20to%0Ainteractive%20user%20queries%20is%20comparable%20to%20responses%20from%20a%20traditional%20RAG%0Awhere%20all%20video%20data%20is%20converted%20to%20text%20upfront%20before%20any%20querying.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12309v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=iRAG%3A%20An%20Incremental%20Retrieval%20Augmented%20Generation%20System%20for%20Videos&entry.906535625=Md%20Adnan%20Arefeen%20and%20Biplob%20Debnath%20and%20Md%20Yusuf%20Sarwar%20Uddin%20and%20Srimat%20Chakradhar&entry.1292438233=%20%20Retrieval%20augmented%20generation%20%28RAG%29%20systems%20combine%20the%20strengths%20of%0Alanguage%20generation%20and%20information%20retrieval%20to%20power%20many%20real-world%0Aapplications%20like%20chatbots.%20Use%20of%20RAG%20for%20combined%20understanding%20of%20multimodal%0Adata%20such%20as%20text%2C%20images%20and%20videos%20is%20appealing%20but%20two%20critical%20limitations%0Aexist%3A%20one-time%2C%20upfront%20capture%20of%20all%20content%20in%20large%20multimodal%20data%20as%0Atext%20descriptions%20entails%20high%20processing%20times%2C%20and%20not%20all%20information%20in%20the%0Arich%20multimodal%20data%20is%20typically%20in%20the%20text%20descriptions.%20Since%20the%20user%0Aqueries%20are%20not%20known%20apriori%2C%20developing%20a%20system%20for%20multimodal%20to%20text%0Aconversion%20and%20interactive%20querying%20of%20multimodal%20data%20is%20challenging.%0A%20%20To%20address%20these%20limitations%2C%20we%20propose%20iRAG%2C%20which%20augments%20RAG%20with%20a%0Anovel%20incremental%20workflow%20to%20enable%20interactive%20querying%20of%20large%20corpus%20of%0Amultimodal%20data.%20Unlike%20traditional%20RAG%2C%20iRAG%20quickly%20indexes%20large%0Arepositories%20of%20multimodal%20data%2C%20and%20in%20the%20incremental%20workflow%2C%20it%20uses%20the%0Aindex%20to%20opportunistically%20extract%20more%20details%20from%20select%20portions%20of%20the%0Amultimodal%20data%20to%20retrieve%20context%20relevant%20to%20an%20interactive%20user%20query.%20Such%0Aan%20incremental%20workflow%20avoids%20long%20multimodal%20to%20text%20conversion%20times%2C%0Aovercomes%20information%20loss%20issues%20by%20doing%20on-demand%20query-specific%20extraction%0Aof%20details%20in%20multimodal%20data%2C%20and%20ensures%20high%20quality%20of%20responses%20to%0Ainteractive%20user%20queries%20that%20are%20often%20not%20known%20apriori.%20To%20the%20best%20of%20our%0Aknowledge%2C%20iRAG%20is%20the%20first%20system%20to%20augment%20RAG%20with%20an%20incremental%20workflow%0Ato%20support%20efficient%20interactive%20querying%20of%20large%2C%20real-world%20multimodal%20data.%0AExperimental%20results%20on%20real-world%20long%20videos%20demonstrate%2023x%20to%2025x%20faster%0Avideo%20to%20text%20ingestion%2C%20while%20ensuring%20that%20quality%20of%20responses%20to%0Ainteractive%20user%20queries%20is%20comparable%20to%20responses%20from%20a%20traditional%20RAG%0Awhere%20all%20video%20data%20is%20converted%20to%20text%20upfront%20before%20any%20querying.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12309v1&entry.124074799=Read"},
{"title": "When Medical Imaging Met Self-Attention: A Love Story That Didn't Quite\n  Work Out", "author": "Tristan Piater and Niklas Penzel and Gideon Stein and Joachim Denzler", "abstract": "  A substantial body of research has focused on developing systems that assist\nmedical professionals during labor-intensive early screening processes, many\nbased on convolutional deep-learning architectures. Recently, multiple studies\nexplored the application of so-called self-attention mechanisms in the vision\ndomain. These studies often report empirical improvements over fully\nconvolutional approaches on various datasets and tasks. To evaluate this trend\nfor medical imaging, we extend two widely adopted convolutional architectures\nwith different self-attention variants on two different medical datasets. With\nthis, we aim to specifically evaluate the possible advantages of additional\nself-attention. We compare our models with similarly sized convolutional and\nattention-based baselines and evaluate performance gains statistically.\nAdditionally, we investigate how including such layers changes the features\nlearned by these models during the training. Following a hyperparameter search,\nand contrary to our expectations, we observe no significant improvement in\nbalanced accuracy over fully convolutional models. We also find that important\nfeatures, such as dermoscopic structures in skin lesion images, are still not\nlearned by employing self-attention. Finally, analyzing local explanations, we\nconfirm biased feature usage. We conclude that merely incorporating attention\nis insufficient to surpass the performance of existing fully convolutional\nmethods.\n", "link": "http://arxiv.org/abs/2404.12295v1", "date": "2024-04-18", "relevancy": 2.1727, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5559}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5453}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5062}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20When%20Medical%20Imaging%20Met%20Self-Attention%3A%20A%20Love%20Story%20That%20Didn%27t%20Quite%0A%20%20Work%20Out&body=Title%3A%20When%20Medical%20Imaging%20Met%20Self-Attention%3A%20A%20Love%20Story%20That%20Didn%27t%20Quite%0A%20%20Work%20Out%0AAuthor%3A%20Tristan%20Piater%20and%20Niklas%20Penzel%20and%20Gideon%20Stein%20and%20Joachim%20Denzler%0AAbstract%3A%20%20%20A%20substantial%20body%20of%20research%20has%20focused%20on%20developing%20systems%20that%20assist%0Amedical%20professionals%20during%20labor-intensive%20early%20screening%20processes%2C%20many%0Abased%20on%20convolutional%20deep-learning%20architectures.%20Recently%2C%20multiple%20studies%0Aexplored%20the%20application%20of%20so-called%20self-attention%20mechanisms%20in%20the%20vision%0Adomain.%20These%20studies%20often%20report%20empirical%20improvements%20over%20fully%0Aconvolutional%20approaches%20on%20various%20datasets%20and%20tasks.%20To%20evaluate%20this%20trend%0Afor%20medical%20imaging%2C%20we%20extend%20two%20widely%20adopted%20convolutional%20architectures%0Awith%20different%20self-attention%20variants%20on%20two%20different%20medical%20datasets.%20With%0Athis%2C%20we%20aim%20to%20specifically%20evaluate%20the%20possible%20advantages%20of%20additional%0Aself-attention.%20We%20compare%20our%20models%20with%20similarly%20sized%20convolutional%20and%0Aattention-based%20baselines%20and%20evaluate%20performance%20gains%20statistically.%0AAdditionally%2C%20we%20investigate%20how%20including%20such%20layers%20changes%20the%20features%0Alearned%20by%20these%20models%20during%20the%20training.%20Following%20a%20hyperparameter%20search%2C%0Aand%20contrary%20to%20our%20expectations%2C%20we%20observe%20no%20significant%20improvement%20in%0Abalanced%20accuracy%20over%20fully%20convolutional%20models.%20We%20also%20find%20that%20important%0Afeatures%2C%20such%20as%20dermoscopic%20structures%20in%20skin%20lesion%20images%2C%20are%20still%20not%0Alearned%20by%20employing%20self-attention.%20Finally%2C%20analyzing%20local%20explanations%2C%20we%0Aconfirm%20biased%20feature%20usage.%20We%20conclude%20that%20merely%20incorporating%20attention%0Ais%20insufficient%20to%20surpass%20the%20performance%20of%20existing%20fully%20convolutional%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12295v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Medical%20Imaging%20Met%20Self-Attention%3A%20A%20Love%20Story%20That%20Didn%27t%20Quite%0A%20%20Work%20Out&entry.906535625=Tristan%20Piater%20and%20Niklas%20Penzel%20and%20Gideon%20Stein%20and%20Joachim%20Denzler&entry.1292438233=%20%20A%20substantial%20body%20of%20research%20has%20focused%20on%20developing%20systems%20that%20assist%0Amedical%20professionals%20during%20labor-intensive%20early%20screening%20processes%2C%20many%0Abased%20on%20convolutional%20deep-learning%20architectures.%20Recently%2C%20multiple%20studies%0Aexplored%20the%20application%20of%20so-called%20self-attention%20mechanisms%20in%20the%20vision%0Adomain.%20These%20studies%20often%20report%20empirical%20improvements%20over%20fully%0Aconvolutional%20approaches%20on%20various%20datasets%20and%20tasks.%20To%20evaluate%20this%20trend%0Afor%20medical%20imaging%2C%20we%20extend%20two%20widely%20adopted%20convolutional%20architectures%0Awith%20different%20self-attention%20variants%20on%20two%20different%20medical%20datasets.%20With%0Athis%2C%20we%20aim%20to%20specifically%20evaluate%20the%20possible%20advantages%20of%20additional%0Aself-attention.%20We%20compare%20our%20models%20with%20similarly%20sized%20convolutional%20and%0Aattention-based%20baselines%20and%20evaluate%20performance%20gains%20statistically.%0AAdditionally%2C%20we%20investigate%20how%20including%20such%20layers%20changes%20the%20features%0Alearned%20by%20these%20models%20during%20the%20training.%20Following%20a%20hyperparameter%20search%2C%0Aand%20contrary%20to%20our%20expectations%2C%20we%20observe%20no%20significant%20improvement%20in%0Abalanced%20accuracy%20over%20fully%20convolutional%20models.%20We%20also%20find%20that%20important%0Afeatures%2C%20such%20as%20dermoscopic%20structures%20in%20skin%20lesion%20images%2C%20are%20still%20not%0Alearned%20by%20employing%20self-attention.%20Finally%2C%20analyzing%20local%20explanations%2C%20we%0Aconfirm%20biased%20feature%20usage.%20We%20conclude%20that%20merely%20incorporating%20attention%0Ais%20insufficient%20to%20surpass%20the%20performance%20of%20existing%20fully%20convolutional%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12295v1&entry.124074799=Read"},
{"title": "Generalizable Face Landmarking Guided by Conditional Face Warping", "author": "Jiayi Liang and Haotian Liu and Hongteng Xu and Dixin Luo", "abstract": "  As a significant step for human face modeling, editing, and generation, face\nlandmarking aims at extracting facial keypoints from images. A generalizable\nface landmarker is required in practice because real-world facial images, e.g.,\nthe avatars in animations and games, are often stylized in various ways.\nHowever, achieving generalizable face landmarking is challenging due to the\ndiversity of facial styles and the scarcity of labeled stylized faces. In this\nstudy, we propose a simple but effective paradigm to learn a generalizable face\nlandmarker based on labeled real human faces and unlabeled stylized faces. Our\nmethod learns the face landmarker as the key module of a conditional face\nwarper. Given a pair of real and stylized facial images, the conditional face\nwarper predicts a warping field from the real face to the stylized one, in\nwhich the face landmarker predicts the ending points of the warping field and\nprovides us with high-quality pseudo landmarks for the corresponding stylized\nfacial images. Applying an alternating optimization strategy, we learn the face\nlandmarker to minimize $i)$ the discrepancy between the stylized faces and the\nwarped real ones and $ii)$ the prediction errors of both real and pseudo\nlandmarks. Experiments on various datasets show that our method outperforms\nexisting state-of-the-art domain adaptation methods in face landmarking tasks,\nleading to a face landmarker with better generalizability. Code is available at\nhttps://plustwo0.github.io/project-face-landmarker}{https://plustwo0.github.io/project-face-landmarker.\n", "link": "http://arxiv.org/abs/2404.12322v1", "date": "2024-04-18", "relevancy": 2.1678, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5476}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5438}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5232}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generalizable%20Face%20Landmarking%20Guided%20by%20Conditional%20Face%20Warping&body=Title%3A%20Generalizable%20Face%20Landmarking%20Guided%20by%20Conditional%20Face%20Warping%0AAuthor%3A%20Jiayi%20Liang%20and%20Haotian%20Liu%20and%20Hongteng%20Xu%20and%20Dixin%20Luo%0AAbstract%3A%20%20%20As%20a%20significant%20step%20for%20human%20face%20modeling%2C%20editing%2C%20and%20generation%2C%20face%0Alandmarking%20aims%20at%20extracting%20facial%20keypoints%20from%20images.%20A%20generalizable%0Aface%20landmarker%20is%20required%20in%20practice%20because%20real-world%20facial%20images%2C%20e.g.%2C%0Athe%20avatars%20in%20animations%20and%20games%2C%20are%20often%20stylized%20in%20various%20ways.%0AHowever%2C%20achieving%20generalizable%20face%20landmarking%20is%20challenging%20due%20to%20the%0Adiversity%20of%20facial%20styles%20and%20the%20scarcity%20of%20labeled%20stylized%20faces.%20In%20this%0Astudy%2C%20we%20propose%20a%20simple%20but%20effective%20paradigm%20to%20learn%20a%20generalizable%20face%0Alandmarker%20based%20on%20labeled%20real%20human%20faces%20and%20unlabeled%20stylized%20faces.%20Our%0Amethod%20learns%20the%20face%20landmarker%20as%20the%20key%20module%20of%20a%20conditional%20face%0Awarper.%20Given%20a%20pair%20of%20real%20and%20stylized%20facial%20images%2C%20the%20conditional%20face%0Awarper%20predicts%20a%20warping%20field%20from%20the%20real%20face%20to%20the%20stylized%20one%2C%20in%0Awhich%20the%20face%20landmarker%20predicts%20the%20ending%20points%20of%20the%20warping%20field%20and%0Aprovides%20us%20with%20high-quality%20pseudo%20landmarks%20for%20the%20corresponding%20stylized%0Afacial%20images.%20Applying%20an%20alternating%20optimization%20strategy%2C%20we%20learn%20the%20face%0Alandmarker%20to%20minimize%20%24i%29%24%20the%20discrepancy%20between%20the%20stylized%20faces%20and%20the%0Awarped%20real%20ones%20and%20%24ii%29%24%20the%20prediction%20errors%20of%20both%20real%20and%20pseudo%0Alandmarks.%20Experiments%20on%20various%20datasets%20show%20that%20our%20method%20outperforms%0Aexisting%20state-of-the-art%20domain%20adaptation%20methods%20in%20face%20landmarking%20tasks%2C%0Aleading%20to%20a%20face%20landmarker%20with%20better%20generalizability.%20Code%20is%20available%20at%0Ahttps%3A//plustwo0.github.io/project-face-landmarker%7D%7Bhttps%3A//plustwo0.github.io/project-face-landmarker.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12322v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20Face%20Landmarking%20Guided%20by%20Conditional%20Face%20Warping&entry.906535625=Jiayi%20Liang%20and%20Haotian%20Liu%20and%20Hongteng%20Xu%20and%20Dixin%20Luo&entry.1292438233=%20%20As%20a%20significant%20step%20for%20human%20face%20modeling%2C%20editing%2C%20and%20generation%2C%20face%0Alandmarking%20aims%20at%20extracting%20facial%20keypoints%20from%20images.%20A%20generalizable%0Aface%20landmarker%20is%20required%20in%20practice%20because%20real-world%20facial%20images%2C%20e.g.%2C%0Athe%20avatars%20in%20animations%20and%20games%2C%20are%20often%20stylized%20in%20various%20ways.%0AHowever%2C%20achieving%20generalizable%20face%20landmarking%20is%20challenging%20due%20to%20the%0Adiversity%20of%20facial%20styles%20and%20the%20scarcity%20of%20labeled%20stylized%20faces.%20In%20this%0Astudy%2C%20we%20propose%20a%20simple%20but%20effective%20paradigm%20to%20learn%20a%20generalizable%20face%0Alandmarker%20based%20on%20labeled%20real%20human%20faces%20and%20unlabeled%20stylized%20faces.%20Our%0Amethod%20learns%20the%20face%20landmarker%20as%20the%20key%20module%20of%20a%20conditional%20face%0Awarper.%20Given%20a%20pair%20of%20real%20and%20stylized%20facial%20images%2C%20the%20conditional%20face%0Awarper%20predicts%20a%20warping%20field%20from%20the%20real%20face%20to%20the%20stylized%20one%2C%20in%0Awhich%20the%20face%20landmarker%20predicts%20the%20ending%20points%20of%20the%20warping%20field%20and%0Aprovides%20us%20with%20high-quality%20pseudo%20landmarks%20for%20the%20corresponding%20stylized%0Afacial%20images.%20Applying%20an%20alternating%20optimization%20strategy%2C%20we%20learn%20the%20face%0Alandmarker%20to%20minimize%20%24i%29%24%20the%20discrepancy%20between%20the%20stylized%20faces%20and%20the%0Awarped%20real%20ones%20and%20%24ii%29%24%20the%20prediction%20errors%20of%20both%20real%20and%20pseudo%0Alandmarks.%20Experiments%20on%20various%20datasets%20show%20that%20our%20method%20outperforms%0Aexisting%20state-of-the-art%20domain%20adaptation%20methods%20in%20face%20landmarking%20tasks%2C%0Aleading%20to%20a%20face%20landmarker%20with%20better%20generalizability.%20Code%20is%20available%20at%0Ahttps%3A//plustwo0.github.io/project-face-landmarker%7D%7Bhttps%3A//plustwo0.github.io/project-face-landmarker.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12322v1&entry.124074799=Read"},
{"title": "Exposing Image Splicing Traces in Scientific Publications via\n  Uncertainty-guided Refinement", "author": "Xun Lin and Wenzhong Tang and Haoran Wang and Yizhong Liu and Yakun Ju and Shuai Wang and Zitong Yu", "abstract": "  Recently, a surge in scientific publications suspected of image manipulation\nhas led to numerous retractions, bringing the issue of image integrity into\nsharp focus. Although research on forensic detectors for image plagiarism and\nimage synthesis exists, the detection of image splicing traces in scientific\npublications remains unexplored. Compared to image duplication and synthesis,\nimage splicing detection is more challenging due to the lack of reference\nimages and the typically small tampered areas. Furthermore, disruptive factors\nin scientific images, such as artifacts from digital compression, abnormal\npatterns, and noise from physical operations, present misleading features like\nsplicing traces, significantly increasing the difficulty of this task.\nMoreover, the scarcity of high-quality datasets of spliced scientific images\nlimits potential advancements. In this work, we propose an Uncertainty-guided\nRefinement Network (URN) to mitigate the impact of these disruptive factors.\nOur URN can explicitly suppress the propagation of unreliable information flow\ncaused by disruptive factors between regions, thus obtaining robust splicing\nfeatures. Additionally, the URN is designed to concentrate improvements in\nuncertain prediction areas during the decoding phase. We also construct a\ndataset for image splicing detection (SciSp) containing 1,290 spliced images.\nCompared to existing datasets, SciSp includes the largest number of spliced\nimages and the most diverse sources. Comprehensive experiments conducted on\nthree benchmark datasets demonstrate the superiority of our approach. We also\nvalidate the URN's generalisability in resisting cross-dataset domain shifts\nand its robustness against various post-processing techniques, including\nadvanced deep-learning-based inpainting.\n", "link": "http://arxiv.org/abs/2309.16388v2", "date": "2024-04-18", "relevancy": 2.1398, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5595}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5328}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5273}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exposing%20Image%20Splicing%20Traces%20in%20Scientific%20Publications%20via%0A%20%20Uncertainty-guided%20Refinement&body=Title%3A%20Exposing%20Image%20Splicing%20Traces%20in%20Scientific%20Publications%20via%0A%20%20Uncertainty-guided%20Refinement%0AAuthor%3A%20Xun%20Lin%20and%20Wenzhong%20Tang%20and%20Haoran%20Wang%20and%20Yizhong%20Liu%20and%20Yakun%20Ju%20and%20Shuai%20Wang%20and%20Zitong%20Yu%0AAbstract%3A%20%20%20Recently%2C%20a%20surge%20in%20scientific%20publications%20suspected%20of%20image%20manipulation%0Ahas%20led%20to%20numerous%20retractions%2C%20bringing%20the%20issue%20of%20image%20integrity%20into%0Asharp%20focus.%20Although%20research%20on%20forensic%20detectors%20for%20image%20plagiarism%20and%0Aimage%20synthesis%20exists%2C%20the%20detection%20of%20image%20splicing%20traces%20in%20scientific%0Apublications%20remains%20unexplored.%20Compared%20to%20image%20duplication%20and%20synthesis%2C%0Aimage%20splicing%20detection%20is%20more%20challenging%20due%20to%20the%20lack%20of%20reference%0Aimages%20and%20the%20typically%20small%20tampered%20areas.%20Furthermore%2C%20disruptive%20factors%0Ain%20scientific%20images%2C%20such%20as%20artifacts%20from%20digital%20compression%2C%20abnormal%0Apatterns%2C%20and%20noise%20from%20physical%20operations%2C%20present%20misleading%20features%20like%0Asplicing%20traces%2C%20significantly%20increasing%20the%20difficulty%20of%20this%20task.%0AMoreover%2C%20the%20scarcity%20of%20high-quality%20datasets%20of%20spliced%20scientific%20images%0Alimits%20potential%20advancements.%20In%20this%20work%2C%20we%20propose%20an%20Uncertainty-guided%0ARefinement%20Network%20%28URN%29%20to%20mitigate%20the%20impact%20of%20these%20disruptive%20factors.%0AOur%20URN%20can%20explicitly%20suppress%20the%20propagation%20of%20unreliable%20information%20flow%0Acaused%20by%20disruptive%20factors%20between%20regions%2C%20thus%20obtaining%20robust%20splicing%0Afeatures.%20Additionally%2C%20the%20URN%20is%20designed%20to%20concentrate%20improvements%20in%0Auncertain%20prediction%20areas%20during%20the%20decoding%20phase.%20We%20also%20construct%20a%0Adataset%20for%20image%20splicing%20detection%20%28SciSp%29%20containing%201%2C290%20spliced%20images.%0ACompared%20to%20existing%20datasets%2C%20SciSp%20includes%20the%20largest%20number%20of%20spliced%0Aimages%20and%20the%20most%20diverse%20sources.%20Comprehensive%20experiments%20conducted%20on%0Athree%20benchmark%20datasets%20demonstrate%20the%20superiority%20of%20our%20approach.%20We%20also%0Avalidate%20the%20URN%27s%20generalisability%20in%20resisting%20cross-dataset%20domain%20shifts%0Aand%20its%20robustness%20against%20various%20post-processing%20techniques%2C%20including%0Aadvanced%20deep-learning-based%20inpainting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16388v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exposing%20Image%20Splicing%20Traces%20in%20Scientific%20Publications%20via%0A%20%20Uncertainty-guided%20Refinement&entry.906535625=Xun%20Lin%20and%20Wenzhong%20Tang%20and%20Haoran%20Wang%20and%20Yizhong%20Liu%20and%20Yakun%20Ju%20and%20Shuai%20Wang%20and%20Zitong%20Yu&entry.1292438233=%20%20Recently%2C%20a%20surge%20in%20scientific%20publications%20suspected%20of%20image%20manipulation%0Ahas%20led%20to%20numerous%20retractions%2C%20bringing%20the%20issue%20of%20image%20integrity%20into%0Asharp%20focus.%20Although%20research%20on%20forensic%20detectors%20for%20image%20plagiarism%20and%0Aimage%20synthesis%20exists%2C%20the%20detection%20of%20image%20splicing%20traces%20in%20scientific%0Apublications%20remains%20unexplored.%20Compared%20to%20image%20duplication%20and%20synthesis%2C%0Aimage%20splicing%20detection%20is%20more%20challenging%20due%20to%20the%20lack%20of%20reference%0Aimages%20and%20the%20typically%20small%20tampered%20areas.%20Furthermore%2C%20disruptive%20factors%0Ain%20scientific%20images%2C%20such%20as%20artifacts%20from%20digital%20compression%2C%20abnormal%0Apatterns%2C%20and%20noise%20from%20physical%20operations%2C%20present%20misleading%20features%20like%0Asplicing%20traces%2C%20significantly%20increasing%20the%20difficulty%20of%20this%20task.%0AMoreover%2C%20the%20scarcity%20of%20high-quality%20datasets%20of%20spliced%20scientific%20images%0Alimits%20potential%20advancements.%20In%20this%20work%2C%20we%20propose%20an%20Uncertainty-guided%0ARefinement%20Network%20%28URN%29%20to%20mitigate%20the%20impact%20of%20these%20disruptive%20factors.%0AOur%20URN%20can%20explicitly%20suppress%20the%20propagation%20of%20unreliable%20information%20flow%0Acaused%20by%20disruptive%20factors%20between%20regions%2C%20thus%20obtaining%20robust%20splicing%0Afeatures.%20Additionally%2C%20the%20URN%20is%20designed%20to%20concentrate%20improvements%20in%0Auncertain%20prediction%20areas%20during%20the%20decoding%20phase.%20We%20also%20construct%20a%0Adataset%20for%20image%20splicing%20detection%20%28SciSp%29%20containing%201%2C290%20spliced%20images.%0ACompared%20to%20existing%20datasets%2C%20SciSp%20includes%20the%20largest%20number%20of%20spliced%0Aimages%20and%20the%20most%20diverse%20sources.%20Comprehensive%20experiments%20conducted%20on%0Athree%20benchmark%20datasets%20demonstrate%20the%20superiority%20of%20our%20approach.%20We%20also%0Avalidate%20the%20URN%27s%20generalisability%20in%20resisting%20cross-dataset%20domain%20shifts%0Aand%20its%20robustness%20against%20various%20post-processing%20techniques%2C%20including%0Aadvanced%20deep-learning-based%20inpainting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16388v2&entry.124074799=Read"},
{"title": "Gradient-Regularized Out-of-Distribution Detection", "author": "Sina Sharifi and Taha Entesari and Bardia Safaei and Vishal M. Patel and Mahyar Fazlyab", "abstract": "  One of the challenges for neural networks in real-life applications is the\noverconfident errors these models make when the data is not from the original\ntraining distribution.\n  Addressing this issue is known as Out-of-Distribution (OOD) detection.\n  Many state-of-the-art OOD methods employ an auxiliary dataset as a surrogate\nfor OOD data during training to achieve improved performance.\n  However, these methods fail to fully exploit the local information embedded\nin the auxiliary dataset.\n  In this work, we propose the idea of leveraging the information embedded in\nthe gradient of the loss function during training to enable the network to not\nonly learn a desired OOD score for each sample but also to exhibit similar\nbehavior in a local neighborhood around each sample.\n  We also develop a novel energy-based sampling method to allow the network to\nbe exposed to more informative OOD samples during the training phase. This is\nespecially important when the auxiliary dataset is large. We demonstrate the\neffectiveness of our method through extensive experiments on several OOD\nbenchmarks, improving the existing state-of-the-art FPR95 by 4% on our ImageNet\nexperiment.\n  We further provide a theoretical analysis through the lens of certified\nrobustness and Lipschitz analysis to showcase the theoretical foundation of our\nwork. We will publicly release our code after the review process.\n", "link": "http://arxiv.org/abs/2404.12368v1", "date": "2024-04-18", "relevancy": 2.1317, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5724}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5048}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5047}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gradient-Regularized%20Out-of-Distribution%20Detection&body=Title%3A%20Gradient-Regularized%20Out-of-Distribution%20Detection%0AAuthor%3A%20Sina%20Sharifi%20and%20Taha%20Entesari%20and%20Bardia%20Safaei%20and%20Vishal%20M.%20Patel%20and%20Mahyar%20Fazlyab%0AAbstract%3A%20%20%20One%20of%20the%20challenges%20for%20neural%20networks%20in%20real-life%20applications%20is%20the%0Aoverconfident%20errors%20these%20models%20make%20when%20the%20data%20is%20not%20from%20the%20original%0Atraining%20distribution.%0A%20%20Addressing%20this%20issue%20is%20known%20as%20Out-of-Distribution%20%28OOD%29%20detection.%0A%20%20Many%20state-of-the-art%20OOD%20methods%20employ%20an%20auxiliary%20dataset%20as%20a%20surrogate%0Afor%20OOD%20data%20during%20training%20to%20achieve%20improved%20performance.%0A%20%20However%2C%20these%20methods%20fail%20to%20fully%20exploit%20the%20local%20information%20embedded%0Ain%20the%20auxiliary%20dataset.%0A%20%20In%20this%20work%2C%20we%20propose%20the%20idea%20of%20leveraging%20the%20information%20embedded%20in%0Athe%20gradient%20of%20the%20loss%20function%20during%20training%20to%20enable%20the%20network%20to%20not%0Aonly%20learn%20a%20desired%20OOD%20score%20for%20each%20sample%20but%20also%20to%20exhibit%20similar%0Abehavior%20in%20a%20local%20neighborhood%20around%20each%20sample.%0A%20%20We%20also%20develop%20a%20novel%20energy-based%20sampling%20method%20to%20allow%20the%20network%20to%0Abe%20exposed%20to%20more%20informative%20OOD%20samples%20during%20the%20training%20phase.%20This%20is%0Aespecially%20important%20when%20the%20auxiliary%20dataset%20is%20large.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20through%20extensive%20experiments%20on%20several%20OOD%0Abenchmarks%2C%20improving%20the%20existing%20state-of-the-art%20FPR95%20by%204%25%20on%20our%20ImageNet%0Aexperiment.%0A%20%20We%20further%20provide%20a%20theoretical%20analysis%20through%20the%20lens%20of%20certified%0Arobustness%20and%20Lipschitz%20analysis%20to%20showcase%20the%20theoretical%20foundation%20of%20our%0Awork.%20We%20will%20publicly%20release%20our%20code%20after%20the%20review%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12368v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gradient-Regularized%20Out-of-Distribution%20Detection&entry.906535625=Sina%20Sharifi%20and%20Taha%20Entesari%20and%20Bardia%20Safaei%20and%20Vishal%20M.%20Patel%20and%20Mahyar%20Fazlyab&entry.1292438233=%20%20One%20of%20the%20challenges%20for%20neural%20networks%20in%20real-life%20applications%20is%20the%0Aoverconfident%20errors%20these%20models%20make%20when%20the%20data%20is%20not%20from%20the%20original%0Atraining%20distribution.%0A%20%20Addressing%20this%20issue%20is%20known%20as%20Out-of-Distribution%20%28OOD%29%20detection.%0A%20%20Many%20state-of-the-art%20OOD%20methods%20employ%20an%20auxiliary%20dataset%20as%20a%20surrogate%0Afor%20OOD%20data%20during%20training%20to%20achieve%20improved%20performance.%0A%20%20However%2C%20these%20methods%20fail%20to%20fully%20exploit%20the%20local%20information%20embedded%0Ain%20the%20auxiliary%20dataset.%0A%20%20In%20this%20work%2C%20we%20propose%20the%20idea%20of%20leveraging%20the%20information%20embedded%20in%0Athe%20gradient%20of%20the%20loss%20function%20during%20training%20to%20enable%20the%20network%20to%20not%0Aonly%20learn%20a%20desired%20OOD%20score%20for%20each%20sample%20but%20also%20to%20exhibit%20similar%0Abehavior%20in%20a%20local%20neighborhood%20around%20each%20sample.%0A%20%20We%20also%20develop%20a%20novel%20energy-based%20sampling%20method%20to%20allow%20the%20network%20to%0Abe%20exposed%20to%20more%20informative%20OOD%20samples%20during%20the%20training%20phase.%20This%20is%0Aespecially%20important%20when%20the%20auxiliary%20dataset%20is%20large.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20through%20extensive%20experiments%20on%20several%20OOD%0Abenchmarks%2C%20improving%20the%20existing%20state-of-the-art%20FPR95%20by%204%25%20on%20our%20ImageNet%0Aexperiment.%0A%20%20We%20further%20provide%20a%20theoretical%20analysis%20through%20the%20lens%20of%20certified%0Arobustness%20and%20Lipschitz%20analysis%20to%20showcase%20the%20theoretical%20foundation%20of%20our%0Awork.%20We%20will%20publicly%20release%20our%20code%20after%20the%20review%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12368v1&entry.124074799=Read"},
{"title": "Blind Localization and Clustering of Anomalies in Textures", "author": "Andrei-Timotei Ardelean and Tim Weyrich", "abstract": "  Anomaly detection and localization in images is a growing field in computer\nvision. In this area, a seemingly understudied problem is anomaly clustering,\ni.e., identifying and grouping different types of anomalies in a fully\nunsupervised manner. In this work, we propose a novel method for clustering\nanomalies in largely stationary images (textures) in a blind setting. That is,\nthe input consists of normal and anomalous images without distinction and\nwithout labels. What contributes to the difficulty of the task is that\nanomalous regions are often small and may present only subtle changes in\nappearance, which can be easily overshadowed by the genuine variance in the\ntexture. Moreover, each anomaly type may have a complex appearance\ndistribution. We introduce a novel scheme for solving this task using a\ncombination of blind anomaly localization and contrastive learning. By\nidentifying the anomalous regions with high fidelity, we can restrict our focus\nto those regions of interest; then, contrastive learning is employed to\nincrease the separability of different anomaly types and reduce the intra-class\nvariation. Our experiments show that the proposed solution yields significantly\nbetter results compared to prior work, setting a new state of the art. Project\npage: https://reality.tf.fau.de/pub/ardelean2024blind.html.\n", "link": "http://arxiv.org/abs/2404.12246v1", "date": "2024-04-18", "relevancy": 2.1042, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5407}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5181}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5093}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Blind%20Localization%20and%20Clustering%20of%20Anomalies%20in%20Textures&body=Title%3A%20Blind%20Localization%20and%20Clustering%20of%20Anomalies%20in%20Textures%0AAuthor%3A%20Andrei-Timotei%20Ardelean%20and%20Tim%20Weyrich%0AAbstract%3A%20%20%20Anomaly%20detection%20and%20localization%20in%20images%20is%20a%20growing%20field%20in%20computer%0Avision.%20In%20this%20area%2C%20a%20seemingly%20understudied%20problem%20is%20anomaly%20clustering%2C%0Ai.e.%2C%20identifying%20and%20grouping%20different%20types%20of%20anomalies%20in%20a%20fully%0Aunsupervised%20manner.%20In%20this%20work%2C%20we%20propose%20a%20novel%20method%20for%20clustering%0Aanomalies%20in%20largely%20stationary%20images%20%28textures%29%20in%20a%20blind%20setting.%20That%20is%2C%0Athe%20input%20consists%20of%20normal%20and%20anomalous%20images%20without%20distinction%20and%0Awithout%20labels.%20What%20contributes%20to%20the%20difficulty%20of%20the%20task%20is%20that%0Aanomalous%20regions%20are%20often%20small%20and%20may%20present%20only%20subtle%20changes%20in%0Aappearance%2C%20which%20can%20be%20easily%20overshadowed%20by%20the%20genuine%20variance%20in%20the%0Atexture.%20Moreover%2C%20each%20anomaly%20type%20may%20have%20a%20complex%20appearance%0Adistribution.%20We%20introduce%20a%20novel%20scheme%20for%20solving%20this%20task%20using%20a%0Acombination%20of%20blind%20anomaly%20localization%20and%20contrastive%20learning.%20By%0Aidentifying%20the%20anomalous%20regions%20with%20high%20fidelity%2C%20we%20can%20restrict%20our%20focus%0Ato%20those%20regions%20of%20interest%3B%20then%2C%20contrastive%20learning%20is%20employed%20to%0Aincrease%20the%20separability%20of%20different%20anomaly%20types%20and%20reduce%20the%20intra-class%0Avariation.%20Our%20experiments%20show%20that%20the%20proposed%20solution%20yields%20significantly%0Abetter%20results%20compared%20to%20prior%20work%2C%20setting%20a%20new%20state%20of%20the%20art.%20Project%0Apage%3A%20https%3A//reality.tf.fau.de/pub/ardelean2024blind.html.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12246v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blind%20Localization%20and%20Clustering%20of%20Anomalies%20in%20Textures&entry.906535625=Andrei-Timotei%20Ardelean%20and%20Tim%20Weyrich&entry.1292438233=%20%20Anomaly%20detection%20and%20localization%20in%20images%20is%20a%20growing%20field%20in%20computer%0Avision.%20In%20this%20area%2C%20a%20seemingly%20understudied%20problem%20is%20anomaly%20clustering%2C%0Ai.e.%2C%20identifying%20and%20grouping%20different%20types%20of%20anomalies%20in%20a%20fully%0Aunsupervised%20manner.%20In%20this%20work%2C%20we%20propose%20a%20novel%20method%20for%20clustering%0Aanomalies%20in%20largely%20stationary%20images%20%28textures%29%20in%20a%20blind%20setting.%20That%20is%2C%0Athe%20input%20consists%20of%20normal%20and%20anomalous%20images%20without%20distinction%20and%0Awithout%20labels.%20What%20contributes%20to%20the%20difficulty%20of%20the%20task%20is%20that%0Aanomalous%20regions%20are%20often%20small%20and%20may%20present%20only%20subtle%20changes%20in%0Aappearance%2C%20which%20can%20be%20easily%20overshadowed%20by%20the%20genuine%20variance%20in%20the%0Atexture.%20Moreover%2C%20each%20anomaly%20type%20may%20have%20a%20complex%20appearance%0Adistribution.%20We%20introduce%20a%20novel%20scheme%20for%20solving%20this%20task%20using%20a%0Acombination%20of%20blind%20anomaly%20localization%20and%20contrastive%20learning.%20By%0Aidentifying%20the%20anomalous%20regions%20with%20high%20fidelity%2C%20we%20can%20restrict%20our%20focus%0Ato%20those%20regions%20of%20interest%3B%20then%2C%20contrastive%20learning%20is%20employed%20to%0Aincrease%20the%20separability%20of%20different%20anomaly%20types%20and%20reduce%20the%20intra-class%0Avariation.%20Our%20experiments%20show%20that%20the%20proposed%20solution%20yields%20significantly%0Abetter%20results%20compared%20to%20prior%20work%2C%20setting%20a%20new%20state%20of%20the%20art.%20Project%0Apage%3A%20https%3A//reality.tf.fau.de/pub/ardelean2024blind.html.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12246v1&entry.124074799=Read"},
{"title": "An Online Spatial-Temporal Graph Trajectory Planner for Autonomous\n  Vehicles", "author": "Jilan Samiuddin and Benoit Boulet and Di Wu", "abstract": "  The autonomous driving industry is expected to grow by over 20 times in the\ncoming decade and, thus, motivate researchers to delve into it. The primary\nfocus of their research is to ensure safety, comfort, and efficiency. An\nautonomous vehicle has several modules responsible for one or more of the\naforementioned items. Among these modules, the trajectory planner plays a\npivotal role in the safety of the vehicle and the comfort of its passengers.\nThe module is also responsible for respecting kinematic constraints and any\napplicable road constraints. In this paper, a novel online spatial-temporal\ngraph trajectory planner is introduced to generate safe and comfortable\ntrajectories. First, a spatial-temporal graph is constructed using the\nautonomous vehicle, its surrounding vehicles, and virtual nodes along the road\nwith respect to the vehicle itself. Next, the graph is forwarded into a\nsequential network to obtain the desired states. To support the planner, a\nsimple behavioral layer is also presented that determines kinematic constraints\nfor the planner. Furthermore, a novel potential function is also proposed to\ntrain the network. Finally, the proposed planner is tested on three different\ncomplex driving tasks, and the performance is compared with two frequently used\nmethods. The results show that the proposed planner generates safe and feasible\ntrajectories while achieving similar or longer distances in the forward\ndirection and comparable comfort ride.\n", "link": "http://arxiv.org/abs/2404.12256v1", "date": "2024-04-18", "relevancy": 2.0809, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5515}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5148}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4911}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20Online%20Spatial-Temporal%20Graph%20Trajectory%20Planner%20for%20Autonomous%0A%20%20Vehicles&body=Title%3A%20An%20Online%20Spatial-Temporal%20Graph%20Trajectory%20Planner%20for%20Autonomous%0A%20%20Vehicles%0AAuthor%3A%20Jilan%20Samiuddin%20and%20Benoit%20Boulet%20and%20Di%20Wu%0AAbstract%3A%20%20%20The%20autonomous%20driving%20industry%20is%20expected%20to%20grow%20by%20over%2020%20times%20in%20the%0Acoming%20decade%20and%2C%20thus%2C%20motivate%20researchers%20to%20delve%20into%20it.%20The%20primary%0Afocus%20of%20their%20research%20is%20to%20ensure%20safety%2C%20comfort%2C%20and%20efficiency.%20An%0Aautonomous%20vehicle%20has%20several%20modules%20responsible%20for%20one%20or%20more%20of%20the%0Aaforementioned%20items.%20Among%20these%20modules%2C%20the%20trajectory%20planner%20plays%20a%0Apivotal%20role%20in%20the%20safety%20of%20the%20vehicle%20and%20the%20comfort%20of%20its%20passengers.%0AThe%20module%20is%20also%20responsible%20for%20respecting%20kinematic%20constraints%20and%20any%0Aapplicable%20road%20constraints.%20In%20this%20paper%2C%20a%20novel%20online%20spatial-temporal%0Agraph%20trajectory%20planner%20is%20introduced%20to%20generate%20safe%20and%20comfortable%0Atrajectories.%20First%2C%20a%20spatial-temporal%20graph%20is%20constructed%20using%20the%0Aautonomous%20vehicle%2C%20its%20surrounding%20vehicles%2C%20and%20virtual%20nodes%20along%20the%20road%0Awith%20respect%20to%20the%20vehicle%20itself.%20Next%2C%20the%20graph%20is%20forwarded%20into%20a%0Asequential%20network%20to%20obtain%20the%20desired%20states.%20To%20support%20the%20planner%2C%20a%0Asimple%20behavioral%20layer%20is%20also%20presented%20that%20determines%20kinematic%20constraints%0Afor%20the%20planner.%20Furthermore%2C%20a%20novel%20potential%20function%20is%20also%20proposed%20to%0Atrain%20the%20network.%20Finally%2C%20the%20proposed%20planner%20is%20tested%20on%20three%20different%0Acomplex%20driving%20tasks%2C%20and%20the%20performance%20is%20compared%20with%20two%20frequently%20used%0Amethods.%20The%20results%20show%20that%20the%20proposed%20planner%20generates%20safe%20and%20feasible%0Atrajectories%20while%20achieving%20similar%20or%20longer%20distances%20in%20the%20forward%0Adirection%20and%20comparable%20comfort%20ride.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12256v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Online%20Spatial-Temporal%20Graph%20Trajectory%20Planner%20for%20Autonomous%0A%20%20Vehicles&entry.906535625=Jilan%20Samiuddin%20and%20Benoit%20Boulet%20and%20Di%20Wu&entry.1292438233=%20%20The%20autonomous%20driving%20industry%20is%20expected%20to%20grow%20by%20over%2020%20times%20in%20the%0Acoming%20decade%20and%2C%20thus%2C%20motivate%20researchers%20to%20delve%20into%20it.%20The%20primary%0Afocus%20of%20their%20research%20is%20to%20ensure%20safety%2C%20comfort%2C%20and%20efficiency.%20An%0Aautonomous%20vehicle%20has%20several%20modules%20responsible%20for%20one%20or%20more%20of%20the%0Aaforementioned%20items.%20Among%20these%20modules%2C%20the%20trajectory%20planner%20plays%20a%0Apivotal%20role%20in%20the%20safety%20of%20the%20vehicle%20and%20the%20comfort%20of%20its%20passengers.%0AThe%20module%20is%20also%20responsible%20for%20respecting%20kinematic%20constraints%20and%20any%0Aapplicable%20road%20constraints.%20In%20this%20paper%2C%20a%20novel%20online%20spatial-temporal%0Agraph%20trajectory%20planner%20is%20introduced%20to%20generate%20safe%20and%20comfortable%0Atrajectories.%20First%2C%20a%20spatial-temporal%20graph%20is%20constructed%20using%20the%0Aautonomous%20vehicle%2C%20its%20surrounding%20vehicles%2C%20and%20virtual%20nodes%20along%20the%20road%0Awith%20respect%20to%20the%20vehicle%20itself.%20Next%2C%20the%20graph%20is%20forwarded%20into%20a%0Asequential%20network%20to%20obtain%20the%20desired%20states.%20To%20support%20the%20planner%2C%20a%0Asimple%20behavioral%20layer%20is%20also%20presented%20that%20determines%20kinematic%20constraints%0Afor%20the%20planner.%20Furthermore%2C%20a%20novel%20potential%20function%20is%20also%20proposed%20to%0Atrain%20the%20network.%20Finally%2C%20the%20proposed%20planner%20is%20tested%20on%20three%20different%0Acomplex%20driving%20tasks%2C%20and%20the%20performance%20is%20compared%20with%20two%20frequently%20used%0Amethods.%20The%20results%20show%20that%20the%20proposed%20planner%20generates%20safe%20and%20feasible%0Atrajectories%20while%20achieving%20similar%20or%20longer%20distances%20in%20the%20forward%0Adirection%20and%20comparable%20comfort%20ride.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12256v1&entry.124074799=Read"},
{"title": "SOHES: Self-supervised Open-world Hierarchical Entity Segmentation", "author": "Shengcao Cao and Jiuxiang Gu and Jason Kuen and Hao Tan and Ruiyi Zhang and Handong Zhao and Ani Nenkova and Liang-Yan Gui and Tong Sun and Yu-Xiong Wang", "abstract": "  Open-world entity segmentation, as an emerging computer vision task, aims at\nsegmenting entities in images without being restricted by pre-defined classes,\noffering impressive generalization capabilities on unseen images and concepts.\nDespite its promise, existing entity segmentation methods like Segment Anything\nModel (SAM) rely heavily on costly expert annotators. This work presents\nSelf-supervised Open-world Hierarchical Entity Segmentation (SOHES), a novel\napproach that eliminates the need for human annotations. SOHES operates in\nthree phases: self-exploration, self-instruction, and self-correction. Given a\npre-trained self-supervised representation, we produce abundant high-quality\npseudo-labels through visual feature clustering. Then, we train a segmentation\nmodel on the pseudo-labels, and rectify the noises in pseudo-labels via a\nteacher-student mutual-learning procedure. Beyond segmenting entities, SOHES\nalso captures their constituent parts, providing a hierarchical understanding\nof visual entities. Using raw images as the sole training data, our method\nachieves unprecedented performance in self-supervised open-world segmentation,\nmarking a significant milestone towards high-quality open-world entity\nsegmentation in the absence of human-annotated masks. Project page:\nhttps://SOHES.github.io.\n", "link": "http://arxiv.org/abs/2404.12386v1", "date": "2024-04-18", "relevancy": 2.0477, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5186}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5109}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5056}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SOHES%3A%20Self-supervised%20Open-world%20Hierarchical%20Entity%20Segmentation&body=Title%3A%20SOHES%3A%20Self-supervised%20Open-world%20Hierarchical%20Entity%20Segmentation%0AAuthor%3A%20Shengcao%20Cao%20and%20Jiuxiang%20Gu%20and%20Jason%20Kuen%20and%20Hao%20Tan%20and%20Ruiyi%20Zhang%20and%20Handong%20Zhao%20and%20Ani%20Nenkova%20and%20Liang-Yan%20Gui%20and%20Tong%20Sun%20and%20Yu-Xiong%20Wang%0AAbstract%3A%20%20%20Open-world%20entity%20segmentation%2C%20as%20an%20emerging%20computer%20vision%20task%2C%20aims%20at%0Asegmenting%20entities%20in%20images%20without%20being%20restricted%20by%20pre-defined%20classes%2C%0Aoffering%20impressive%20generalization%20capabilities%20on%20unseen%20images%20and%20concepts.%0ADespite%20its%20promise%2C%20existing%20entity%20segmentation%20methods%20like%20Segment%20Anything%0AModel%20%28SAM%29%20rely%20heavily%20on%20costly%20expert%20annotators.%20This%20work%20presents%0ASelf-supervised%20Open-world%20Hierarchical%20Entity%20Segmentation%20%28SOHES%29%2C%20a%20novel%0Aapproach%20that%20eliminates%20the%20need%20for%20human%20annotations.%20SOHES%20operates%20in%0Athree%20phases%3A%20self-exploration%2C%20self-instruction%2C%20and%20self-correction.%20Given%20a%0Apre-trained%20self-supervised%20representation%2C%20we%20produce%20abundant%20high-quality%0Apseudo-labels%20through%20visual%20feature%20clustering.%20Then%2C%20we%20train%20a%20segmentation%0Amodel%20on%20the%20pseudo-labels%2C%20and%20rectify%20the%20noises%20in%20pseudo-labels%20via%20a%0Ateacher-student%20mutual-learning%20procedure.%20Beyond%20segmenting%20entities%2C%20SOHES%0Aalso%20captures%20their%20constituent%20parts%2C%20providing%20a%20hierarchical%20understanding%0Aof%20visual%20entities.%20Using%20raw%20images%20as%20the%20sole%20training%20data%2C%20our%20method%0Aachieves%20unprecedented%20performance%20in%20self-supervised%20open-world%20segmentation%2C%0Amarking%20a%20significant%20milestone%20towards%20high-quality%20open-world%20entity%0Asegmentation%20in%20the%20absence%20of%20human-annotated%20masks.%20Project%20page%3A%0Ahttps%3A//SOHES.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12386v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SOHES%3A%20Self-supervised%20Open-world%20Hierarchical%20Entity%20Segmentation&entry.906535625=Shengcao%20Cao%20and%20Jiuxiang%20Gu%20and%20Jason%20Kuen%20and%20Hao%20Tan%20and%20Ruiyi%20Zhang%20and%20Handong%20Zhao%20and%20Ani%20Nenkova%20and%20Liang-Yan%20Gui%20and%20Tong%20Sun%20and%20Yu-Xiong%20Wang&entry.1292438233=%20%20Open-world%20entity%20segmentation%2C%20as%20an%20emerging%20computer%20vision%20task%2C%20aims%20at%0Asegmenting%20entities%20in%20images%20without%20being%20restricted%20by%20pre-defined%20classes%2C%0Aoffering%20impressive%20generalization%20capabilities%20on%20unseen%20images%20and%20concepts.%0ADespite%20its%20promise%2C%20existing%20entity%20segmentation%20methods%20like%20Segment%20Anything%0AModel%20%28SAM%29%20rely%20heavily%20on%20costly%20expert%20annotators.%20This%20work%20presents%0ASelf-supervised%20Open-world%20Hierarchical%20Entity%20Segmentation%20%28SOHES%29%2C%20a%20novel%0Aapproach%20that%20eliminates%20the%20need%20for%20human%20annotations.%20SOHES%20operates%20in%0Athree%20phases%3A%20self-exploration%2C%20self-instruction%2C%20and%20self-correction.%20Given%20a%0Apre-trained%20self-supervised%20representation%2C%20we%20produce%20abundant%20high-quality%0Apseudo-labels%20through%20visual%20feature%20clustering.%20Then%2C%20we%20train%20a%20segmentation%0Amodel%20on%20the%20pseudo-labels%2C%20and%20rectify%20the%20noises%20in%20pseudo-labels%20via%20a%0Ateacher-student%20mutual-learning%20procedure.%20Beyond%20segmenting%20entities%2C%20SOHES%0Aalso%20captures%20their%20constituent%20parts%2C%20providing%20a%20hierarchical%20understanding%0Aof%20visual%20entities.%20Using%20raw%20images%20as%20the%20sole%20training%20data%2C%20our%20method%0Aachieves%20unprecedented%20performance%20in%20self-supervised%20open-world%20segmentation%2C%0Amarking%20a%20significant%20milestone%20towards%20high-quality%20open-world%20entity%0Asegmentation%20in%20the%20absence%20of%20human-annotated%20masks.%20Project%20page%3A%0Ahttps%3A//SOHES.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12386v1&entry.124074799=Read"},
{"title": "HardVis: Visual Analytics to Handle Instance Hardness Using\n  Undersampling and Oversampling Techniques", "author": "Angelos Chatzimparmpas and Fernando V. Paulovich and Andreas Kerren", "abstract": "  Despite the tremendous advances in machine learning (ML), training with\nimbalanced data still poses challenges in many real-world applications. Among a\nseries of diverse techniques to solve this problem, sampling algorithms are\nregarded as an efficient solution. However, the problem is more fundamental,\nwith many works emphasizing the importance of instance hardness. This issue\nrefers to the significance of managing unsafe or potentially noisy instances\nthat are more likely to be misclassified and serve as the root cause of poor\nclassification performance. This paper introduces HardVis, a visual analytics\nsystem designed to handle instance hardness mainly in imbalanced classification\nscenarios. Our proposed system assists users in visually comparing different\ndistributions of data types, selecting types of instances based on local\ncharacteristics that will later be affected by the active sampling method, and\nvalidating which suggestions from undersampling or oversampling techniques are\nbeneficial for the ML model. Additionally, rather than uniformly\nundersampling/oversampling a specific class, we allow users to find and sample\neasy and difficult to classify training instances from all classes. Users can\nexplore subsets of data from different perspectives to decide all those\nparameters, while HardVis keeps track of their steps and evaluates the model's\npredictive performance in a test set separately. The end result is a\nwell-balanced data set that boosts the predictive power of the ML model. The\nefficacy and effectiveness of HardVis are demonstrated with a hypothetical\nusage scenario and a use case. Finally, we also look at how useful our system\nis based on feedback we received from ML experts.\n", "link": "http://arxiv.org/abs/2203.15753v4", "date": "2024-04-18", "relevancy": 2.0252, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5369}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4847}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4838}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HardVis%3A%20Visual%20Analytics%20to%20Handle%20Instance%20Hardness%20Using%0A%20%20Undersampling%20and%20Oversampling%20Techniques&body=Title%3A%20HardVis%3A%20Visual%20Analytics%20to%20Handle%20Instance%20Hardness%20Using%0A%20%20Undersampling%20and%20Oversampling%20Techniques%0AAuthor%3A%20Angelos%20Chatzimparmpas%20and%20Fernando%20V.%20Paulovich%20and%20Andreas%20Kerren%0AAbstract%3A%20%20%20Despite%20the%20tremendous%20advances%20in%20machine%20learning%20%28ML%29%2C%20training%20with%0Aimbalanced%20data%20still%20poses%20challenges%20in%20many%20real-world%20applications.%20Among%20a%0Aseries%20of%20diverse%20techniques%20to%20solve%20this%20problem%2C%20sampling%20algorithms%20are%0Aregarded%20as%20an%20efficient%20solution.%20However%2C%20the%20problem%20is%20more%20fundamental%2C%0Awith%20many%20works%20emphasizing%20the%20importance%20of%20instance%20hardness.%20This%20issue%0Arefers%20to%20the%20significance%20of%20managing%20unsafe%20or%20potentially%20noisy%20instances%0Athat%20are%20more%20likely%20to%20be%20misclassified%20and%20serve%20as%20the%20root%20cause%20of%20poor%0Aclassification%20performance.%20This%20paper%20introduces%20HardVis%2C%20a%20visual%20analytics%0Asystem%20designed%20to%20handle%20instance%20hardness%20mainly%20in%20imbalanced%20classification%0Ascenarios.%20Our%20proposed%20system%20assists%20users%20in%20visually%20comparing%20different%0Adistributions%20of%20data%20types%2C%20selecting%20types%20of%20instances%20based%20on%20local%0Acharacteristics%20that%20will%20later%20be%20affected%20by%20the%20active%20sampling%20method%2C%20and%0Avalidating%20which%20suggestions%20from%20undersampling%20or%20oversampling%20techniques%20are%0Abeneficial%20for%20the%20ML%20model.%20Additionally%2C%20rather%20than%20uniformly%0Aundersampling/oversampling%20a%20specific%20class%2C%20we%20allow%20users%20to%20find%20and%20sample%0Aeasy%20and%20difficult%20to%20classify%20training%20instances%20from%20all%20classes.%20Users%20can%0Aexplore%20subsets%20of%20data%20from%20different%20perspectives%20to%20decide%20all%20those%0Aparameters%2C%20while%20HardVis%20keeps%20track%20of%20their%20steps%20and%20evaluates%20the%20model%27s%0Apredictive%20performance%20in%20a%20test%20set%20separately.%20The%20end%20result%20is%20a%0Awell-balanced%20data%20set%20that%20boosts%20the%20predictive%20power%20of%20the%20ML%20model.%20The%0Aefficacy%20and%20effectiveness%20of%20HardVis%20are%20demonstrated%20with%20a%20hypothetical%0Ausage%20scenario%20and%20a%20use%20case.%20Finally%2C%20we%20also%20look%20at%20how%20useful%20our%20system%0Ais%20based%20on%20feedback%20we%20received%20from%20ML%20experts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2203.15753v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HardVis%3A%20Visual%20Analytics%20to%20Handle%20Instance%20Hardness%20Using%0A%20%20Undersampling%20and%20Oversampling%20Techniques&entry.906535625=Angelos%20Chatzimparmpas%20and%20Fernando%20V.%20Paulovich%20and%20Andreas%20Kerren&entry.1292438233=%20%20Despite%20the%20tremendous%20advances%20in%20machine%20learning%20%28ML%29%2C%20training%20with%0Aimbalanced%20data%20still%20poses%20challenges%20in%20many%20real-world%20applications.%20Among%20a%0Aseries%20of%20diverse%20techniques%20to%20solve%20this%20problem%2C%20sampling%20algorithms%20are%0Aregarded%20as%20an%20efficient%20solution.%20However%2C%20the%20problem%20is%20more%20fundamental%2C%0Awith%20many%20works%20emphasizing%20the%20importance%20of%20instance%20hardness.%20This%20issue%0Arefers%20to%20the%20significance%20of%20managing%20unsafe%20or%20potentially%20noisy%20instances%0Athat%20are%20more%20likely%20to%20be%20misclassified%20and%20serve%20as%20the%20root%20cause%20of%20poor%0Aclassification%20performance.%20This%20paper%20introduces%20HardVis%2C%20a%20visual%20analytics%0Asystem%20designed%20to%20handle%20instance%20hardness%20mainly%20in%20imbalanced%20classification%0Ascenarios.%20Our%20proposed%20system%20assists%20users%20in%20visually%20comparing%20different%0Adistributions%20of%20data%20types%2C%20selecting%20types%20of%20instances%20based%20on%20local%0Acharacteristics%20that%20will%20later%20be%20affected%20by%20the%20active%20sampling%20method%2C%20and%0Avalidating%20which%20suggestions%20from%20undersampling%20or%20oversampling%20techniques%20are%0Abeneficial%20for%20the%20ML%20model.%20Additionally%2C%20rather%20than%20uniformly%0Aundersampling/oversampling%20a%20specific%20class%2C%20we%20allow%20users%20to%20find%20and%20sample%0Aeasy%20and%20difficult%20to%20classify%20training%20instances%20from%20all%20classes.%20Users%20can%0Aexplore%20subsets%20of%20data%20from%20different%20perspectives%20to%20decide%20all%20those%0Aparameters%2C%20while%20HardVis%20keeps%20track%20of%20their%20steps%20and%20evaluates%20the%20model%27s%0Apredictive%20performance%20in%20a%20test%20set%20separately.%20The%20end%20result%20is%20a%0Awell-balanced%20data%20set%20that%20boosts%20the%20predictive%20power%20of%20the%20ML%20model.%20The%0Aefficacy%20and%20effectiveness%20of%20HardVis%20are%20demonstrated%20with%20a%20hypothetical%0Ausage%20scenario%20and%20a%20use%20case.%20Finally%2C%20we%20also%20look%20at%20how%20useful%20our%20system%0Ais%20based%20on%20feedback%20we%20received%20from%20ML%20experts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2203.15753v4&entry.124074799=Read"},
{"title": "Toward Self-Improvement of LLMs via Imagination, Searching, and\n  Criticizing", "author": "Ye Tian and Baolin Peng and Linfeng Song and Lifeng Jin and Dian Yu and Haitao Mi and Dong Yu", "abstract": "  Despite the impressive capabilities of Large Language Models (LLMs) on\nvarious tasks, they still struggle with scenarios that involves complex\nreasoning and planning. Recent work proposed advanced prompting techniques and\nthe necessity of fine-tuning with high-quality data to augment LLMs' reasoning\nabilities. However, these approaches are inherently constrained by data\navailability and quality. In light of this, self-correction and self-learning\nemerge as viable solutions, employing strategies that allow LLMs to refine\ntheir outputs and learn from self-assessed rewards. Yet, the efficacy of LLMs\nin self-refining its response, particularly in complex reasoning and planning\ntask, remains dubious. In this paper, we introduce AlphaLLM for the\nself-improvements of LLMs, which integrates Monte Carlo Tree Search (MCTS) with\nLLMs to establish a self-improving loop, thereby enhancing the capabilities of\nLLMs without additional annotations. Drawing inspiration from the success of\nAlphaGo, AlphaLLM addresses the unique challenges of combining MCTS with LLM\nfor self-improvement, including data scarcity, the vastness search spaces of\nlanguage tasks, and the subjective nature of feedback in language tasks.\nAlphaLLM is comprised of prompt synthesis component, an efficient MCTS approach\ntailored for language tasks, and a trio of critic models for precise feedback.\nOur experimental results in mathematical reasoning tasks demonstrate that\nAlphaLLM significantly enhances the performance of LLMs without additional\nannotations, showing the potential for self-improvement in LLMs.\n", "link": "http://arxiv.org/abs/2404.12253v1", "date": "2024-04-18", "relevancy": 2.023, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5079}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5064}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5033}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Toward%20Self-Improvement%20of%20LLMs%20via%20Imagination%2C%20Searching%2C%20and%0A%20%20Criticizing&body=Title%3A%20Toward%20Self-Improvement%20of%20LLMs%20via%20Imagination%2C%20Searching%2C%20and%0A%20%20Criticizing%0AAuthor%3A%20Ye%20Tian%20and%20Baolin%20Peng%20and%20Linfeng%20Song%20and%20Lifeng%20Jin%20and%20Dian%20Yu%20and%20Haitao%20Mi%20and%20Dong%20Yu%0AAbstract%3A%20%20%20Despite%20the%20impressive%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20on%0Avarious%20tasks%2C%20they%20still%20struggle%20with%20scenarios%20that%20involves%20complex%0Areasoning%20and%20planning.%20Recent%20work%20proposed%20advanced%20prompting%20techniques%20and%0Athe%20necessity%20of%20fine-tuning%20with%20high-quality%20data%20to%20augment%20LLMs%27%20reasoning%0Aabilities.%20However%2C%20these%20approaches%20are%20inherently%20constrained%20by%20data%0Aavailability%20and%20quality.%20In%20light%20of%20this%2C%20self-correction%20and%20self-learning%0Aemerge%20as%20viable%20solutions%2C%20employing%20strategies%20that%20allow%20LLMs%20to%20refine%0Atheir%20outputs%20and%20learn%20from%20self-assessed%20rewards.%20Yet%2C%20the%20efficacy%20of%20LLMs%0Ain%20self-refining%20its%20response%2C%20particularly%20in%20complex%20reasoning%20and%20planning%0Atask%2C%20remains%20dubious.%20In%20this%20paper%2C%20we%20introduce%20AlphaLLM%20for%20the%0Aself-improvements%20of%20LLMs%2C%20which%20integrates%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29%20with%0ALLMs%20to%20establish%20a%20self-improving%20loop%2C%20thereby%20enhancing%20the%20capabilities%20of%0ALLMs%20without%20additional%20annotations.%20Drawing%20inspiration%20from%20the%20success%20of%0AAlphaGo%2C%20AlphaLLM%20addresses%20the%20unique%20challenges%20of%20combining%20MCTS%20with%20LLM%0Afor%20self-improvement%2C%20including%20data%20scarcity%2C%20the%20vastness%20search%20spaces%20of%0Alanguage%20tasks%2C%20and%20the%20subjective%20nature%20of%20feedback%20in%20language%20tasks.%0AAlphaLLM%20is%20comprised%20of%20prompt%20synthesis%20component%2C%20an%20efficient%20MCTS%20approach%0Atailored%20for%20language%20tasks%2C%20and%20a%20trio%20of%20critic%20models%20for%20precise%20feedback.%0AOur%20experimental%20results%20in%20mathematical%20reasoning%20tasks%20demonstrate%20that%0AAlphaLLM%20significantly%20enhances%20the%20performance%20of%20LLMs%20without%20additional%0Aannotations%2C%20showing%20the%20potential%20for%20self-improvement%20in%20LLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12253v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Self-Improvement%20of%20LLMs%20via%20Imagination%2C%20Searching%2C%20and%0A%20%20Criticizing&entry.906535625=Ye%20Tian%20and%20Baolin%20Peng%20and%20Linfeng%20Song%20and%20Lifeng%20Jin%20and%20Dian%20Yu%20and%20Haitao%20Mi%20and%20Dong%20Yu&entry.1292438233=%20%20Despite%20the%20impressive%20capabilities%20of%20Large%20Language%20Models%20%28LLMs%29%20on%0Avarious%20tasks%2C%20they%20still%20struggle%20with%20scenarios%20that%20involves%20complex%0Areasoning%20and%20planning.%20Recent%20work%20proposed%20advanced%20prompting%20techniques%20and%0Athe%20necessity%20of%20fine-tuning%20with%20high-quality%20data%20to%20augment%20LLMs%27%20reasoning%0Aabilities.%20However%2C%20these%20approaches%20are%20inherently%20constrained%20by%20data%0Aavailability%20and%20quality.%20In%20light%20of%20this%2C%20self-correction%20and%20self-learning%0Aemerge%20as%20viable%20solutions%2C%20employing%20strategies%20that%20allow%20LLMs%20to%20refine%0Atheir%20outputs%20and%20learn%20from%20self-assessed%20rewards.%20Yet%2C%20the%20efficacy%20of%20LLMs%0Ain%20self-refining%20its%20response%2C%20particularly%20in%20complex%20reasoning%20and%20planning%0Atask%2C%20remains%20dubious.%20In%20this%20paper%2C%20we%20introduce%20AlphaLLM%20for%20the%0Aself-improvements%20of%20LLMs%2C%20which%20integrates%20Monte%20Carlo%20Tree%20Search%20%28MCTS%29%20with%0ALLMs%20to%20establish%20a%20self-improving%20loop%2C%20thereby%20enhancing%20the%20capabilities%20of%0ALLMs%20without%20additional%20annotations.%20Drawing%20inspiration%20from%20the%20success%20of%0AAlphaGo%2C%20AlphaLLM%20addresses%20the%20unique%20challenges%20of%20combining%20MCTS%20with%20LLM%0Afor%20self-improvement%2C%20including%20data%20scarcity%2C%20the%20vastness%20search%20spaces%20of%0Alanguage%20tasks%2C%20and%20the%20subjective%20nature%20of%20feedback%20in%20language%20tasks.%0AAlphaLLM%20is%20comprised%20of%20prompt%20synthesis%20component%2C%20an%20efficient%20MCTS%20approach%0Atailored%20for%20language%20tasks%2C%20and%20a%20trio%20of%20critic%20models%20for%20precise%20feedback.%0AOur%20experimental%20results%20in%20mathematical%20reasoning%20tasks%20demonstrate%20that%0AAlphaLLM%20significantly%20enhances%20the%20performance%20of%20LLMs%20without%20additional%0Aannotations%2C%20showing%20the%20potential%20for%20self-improvement%20in%20LLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12253v1&entry.124074799=Read"},
{"title": "StackGenVis: Alignment of Data, Algorithms, and Models for Stacking\n  Ensemble Learning Using Performance Metrics", "author": "Angelos Chatzimparmpas and Rafael M. Martins and Kostiantyn Kucher and Andreas Kerren", "abstract": "  In machine learning (ML), ensemble methods such as bagging, boosting, and\nstacking are widely-established approaches that regularly achieve top-notch\npredictive performance. Stacking (also called \"stacked generalization\") is an\nensemble method that combines heterogeneous base models, arranged in at least\none layer, and then employs another metamodel to summarize the predictions of\nthose models. Although it may be a highly-effective approach for increasing the\npredictive performance of ML, generating a stack of models from scratch can be\na cumbersome trial-and-error process. This challenge stems from the enormous\nspace of available solutions, with different sets of data instances and\nfeatures that could be used for training, several algorithms to choose from,\nand instantiations of these algorithms using diverse parameters (i.e., models)\nthat perform differently according to various metrics. In this work, we present\na knowledge generation model, which supports ensemble learning with the use of\nvisualization, and a visual analytics system for stacked generalization. Our\nsystem, StackGenVis, assists users in dynamically adapting performance metrics,\nmanaging data instances, selecting the most important features for a given data\nset, choosing a set of top-performant and diverse algorithms, and measuring the\npredictive performance. In consequence, our proposed tool helps users to decide\nbetween distinct models and to reduce the complexity of the resulting stack by\nremoving overpromising and underperforming models. The applicability and\neffectiveness of StackGenVis are demonstrated with two use cases: a real-world\nhealthcare data set and a collection of data related to sentiment/stance\ndetection in texts. Finally, the tool has been evaluated through interviews\nwith three ML experts.\n", "link": "http://arxiv.org/abs/2005.01575v9", "date": "2024-04-18", "relevancy": 2.0009, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5075}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4975}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4889}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20StackGenVis%3A%20Alignment%20of%20Data%2C%20Algorithms%2C%20and%20Models%20for%20Stacking%0A%20%20Ensemble%20Learning%20Using%20Performance%20Metrics&body=Title%3A%20StackGenVis%3A%20Alignment%20of%20Data%2C%20Algorithms%2C%20and%20Models%20for%20Stacking%0A%20%20Ensemble%20Learning%20Using%20Performance%20Metrics%0AAuthor%3A%20Angelos%20Chatzimparmpas%20and%20Rafael%20M.%20Martins%20and%20Kostiantyn%20Kucher%20and%20Andreas%20Kerren%0AAbstract%3A%20%20%20In%20machine%20learning%20%28ML%29%2C%20ensemble%20methods%20such%20as%20bagging%2C%20boosting%2C%20and%0Astacking%20are%20widely-established%20approaches%20that%20regularly%20achieve%20top-notch%0Apredictive%20performance.%20Stacking%20%28also%20called%20%22stacked%20generalization%22%29%20is%20an%0Aensemble%20method%20that%20combines%20heterogeneous%20base%20models%2C%20arranged%20in%20at%20least%0Aone%20layer%2C%20and%20then%20employs%20another%20metamodel%20to%20summarize%20the%20predictions%20of%0Athose%20models.%20Although%20it%20may%20be%20a%20highly-effective%20approach%20for%20increasing%20the%0Apredictive%20performance%20of%20ML%2C%20generating%20a%20stack%20of%20models%20from%20scratch%20can%20be%0Aa%20cumbersome%20trial-and-error%20process.%20This%20challenge%20stems%20from%20the%20enormous%0Aspace%20of%20available%20solutions%2C%20with%20different%20sets%20of%20data%20instances%20and%0Afeatures%20that%20could%20be%20used%20for%20training%2C%20several%20algorithms%20to%20choose%20from%2C%0Aand%20instantiations%20of%20these%20algorithms%20using%20diverse%20parameters%20%28i.e.%2C%20models%29%0Athat%20perform%20differently%20according%20to%20various%20metrics.%20In%20this%20work%2C%20we%20present%0Aa%20knowledge%20generation%20model%2C%20which%20supports%20ensemble%20learning%20with%20the%20use%20of%0Avisualization%2C%20and%20a%20visual%20analytics%20system%20for%20stacked%20generalization.%20Our%0Asystem%2C%20StackGenVis%2C%20assists%20users%20in%20dynamically%20adapting%20performance%20metrics%2C%0Amanaging%20data%20instances%2C%20selecting%20the%20most%20important%20features%20for%20a%20given%20data%0Aset%2C%20choosing%20a%20set%20of%20top-performant%20and%20diverse%20algorithms%2C%20and%20measuring%20the%0Apredictive%20performance.%20In%20consequence%2C%20our%20proposed%20tool%20helps%20users%20to%20decide%0Abetween%20distinct%20models%20and%20to%20reduce%20the%20complexity%20of%20the%20resulting%20stack%20by%0Aremoving%20overpromising%20and%20underperforming%20models.%20The%20applicability%20and%0Aeffectiveness%20of%20StackGenVis%20are%20demonstrated%20with%20two%20use%20cases%3A%20a%20real-world%0Ahealthcare%20data%20set%20and%20a%20collection%20of%20data%20related%20to%20sentiment/stance%0Adetection%20in%20texts.%20Finally%2C%20the%20tool%20has%20been%20evaluated%20through%20interviews%0Awith%20three%20ML%20experts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2005.01575v9", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StackGenVis%3A%20Alignment%20of%20Data%2C%20Algorithms%2C%20and%20Models%20for%20Stacking%0A%20%20Ensemble%20Learning%20Using%20Performance%20Metrics&entry.906535625=Angelos%20Chatzimparmpas%20and%20Rafael%20M.%20Martins%20and%20Kostiantyn%20Kucher%20and%20Andreas%20Kerren&entry.1292438233=%20%20In%20machine%20learning%20%28ML%29%2C%20ensemble%20methods%20such%20as%20bagging%2C%20boosting%2C%20and%0Astacking%20are%20widely-established%20approaches%20that%20regularly%20achieve%20top-notch%0Apredictive%20performance.%20Stacking%20%28also%20called%20%22stacked%20generalization%22%29%20is%20an%0Aensemble%20method%20that%20combines%20heterogeneous%20base%20models%2C%20arranged%20in%20at%20least%0Aone%20layer%2C%20and%20then%20employs%20another%20metamodel%20to%20summarize%20the%20predictions%20of%0Athose%20models.%20Although%20it%20may%20be%20a%20highly-effective%20approach%20for%20increasing%20the%0Apredictive%20performance%20of%20ML%2C%20generating%20a%20stack%20of%20models%20from%20scratch%20can%20be%0Aa%20cumbersome%20trial-and-error%20process.%20This%20challenge%20stems%20from%20the%20enormous%0Aspace%20of%20available%20solutions%2C%20with%20different%20sets%20of%20data%20instances%20and%0Afeatures%20that%20could%20be%20used%20for%20training%2C%20several%20algorithms%20to%20choose%20from%2C%0Aand%20instantiations%20of%20these%20algorithms%20using%20diverse%20parameters%20%28i.e.%2C%20models%29%0Athat%20perform%20differently%20according%20to%20various%20metrics.%20In%20this%20work%2C%20we%20present%0Aa%20knowledge%20generation%20model%2C%20which%20supports%20ensemble%20learning%20with%20the%20use%20of%0Avisualization%2C%20and%20a%20visual%20analytics%20system%20for%20stacked%20generalization.%20Our%0Asystem%2C%20StackGenVis%2C%20assists%20users%20in%20dynamically%20adapting%20performance%20metrics%2C%0Amanaging%20data%20instances%2C%20selecting%20the%20most%20important%20features%20for%20a%20given%20data%0Aset%2C%20choosing%20a%20set%20of%20top-performant%20and%20diverse%20algorithms%2C%20and%20measuring%20the%0Apredictive%20performance.%20In%20consequence%2C%20our%20proposed%20tool%20helps%20users%20to%20decide%0Abetween%20distinct%20models%20and%20to%20reduce%20the%20complexity%20of%20the%20resulting%20stack%20by%0Aremoving%20overpromising%20and%20underperforming%20models.%20The%20applicability%20and%0Aeffectiveness%20of%20StackGenVis%20are%20demonstrated%20with%20two%20use%20cases%3A%20a%20real-world%0Ahealthcare%20data%20set%20and%20a%20collection%20of%20data%20related%20to%20sentiment/stance%0Adetection%20in%20texts.%20Finally%2C%20the%20tool%20has%20been%20evaluated%20through%20interviews%0Awith%20three%20ML%20experts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2005.01575v9&entry.124074799=Read"},
{"title": "Investigating Guiding Information for Adaptive Collocation Point\n  Sampling in PINNs", "author": "Jose Florido and He Wang and Amirul Khan and Peter K. Jimack", "abstract": "  Physics-informed neural networks (PINNs) provide a means of obtaining\napproximate solutions of partial differential equations and systems through the\nminimisation of an objective function which includes the evaluation of a\nresidual function at a set of collocation points within the domain. The quality\nof a PINNs solution depends upon numerous parameters, including the number and\ndistribution of these collocation points. In this paper we consider a number of\nstrategies for selecting these points and investigate their impact on the\noverall accuracy of the method. In particular, we suggest that no single\napproach is likely to be ``optimal'' but we show how a number of important\nmetrics can have an impact in improving the quality of the results obtained\nwhen using a fixed number of residual evaluations. We illustrate these\napproaches through the use of two benchmark test problems: Burgers' equation\nand the Allen-Cahn equation.\n", "link": "http://arxiv.org/abs/2404.12282v1", "date": "2024-04-18", "relevancy": 1.9865, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5205}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4801}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4781}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Investigating%20Guiding%20Information%20for%20Adaptive%20Collocation%20Point%0A%20%20Sampling%20in%20PINNs&body=Title%3A%20Investigating%20Guiding%20Information%20for%20Adaptive%20Collocation%20Point%0A%20%20Sampling%20in%20PINNs%0AAuthor%3A%20Jose%20Florido%20and%20He%20Wang%20and%20Amirul%20Khan%20and%20Peter%20K.%20Jimack%0AAbstract%3A%20%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20provide%20a%20means%20of%20obtaining%0Aapproximate%20solutions%20of%20partial%20differential%20equations%20and%20systems%20through%20the%0Aminimisation%20of%20an%20objective%20function%20which%20includes%20the%20evaluation%20of%20a%0Aresidual%20function%20at%20a%20set%20of%20collocation%20points%20within%20the%20domain.%20The%20quality%0Aof%20a%20PINNs%20solution%20depends%20upon%20numerous%20parameters%2C%20including%20the%20number%20and%0Adistribution%20of%20these%20collocation%20points.%20In%20this%20paper%20we%20consider%20a%20number%20of%0Astrategies%20for%20selecting%20these%20points%20and%20investigate%20their%20impact%20on%20the%0Aoverall%20accuracy%20of%20the%20method.%20In%20particular%2C%20we%20suggest%20that%20no%20single%0Aapproach%20is%20likely%20to%20be%20%60%60optimal%27%27%20but%20we%20show%20how%20a%20number%20of%20important%0Ametrics%20can%20have%20an%20impact%20in%20improving%20the%20quality%20of%20the%20results%20obtained%0Awhen%20using%20a%20fixed%20number%20of%20residual%20evaluations.%20We%20illustrate%20these%0Aapproaches%20through%20the%20use%20of%20two%20benchmark%20test%20problems%3A%20Burgers%27%20equation%0Aand%20the%20Allen-Cahn%20equation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12282v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Investigating%20Guiding%20Information%20for%20Adaptive%20Collocation%20Point%0A%20%20Sampling%20in%20PINNs&entry.906535625=Jose%20Florido%20and%20He%20Wang%20and%20Amirul%20Khan%20and%20Peter%20K.%20Jimack&entry.1292438233=%20%20Physics-informed%20neural%20networks%20%28PINNs%29%20provide%20a%20means%20of%20obtaining%0Aapproximate%20solutions%20of%20partial%20differential%20equations%20and%20systems%20through%20the%0Aminimisation%20of%20an%20objective%20function%20which%20includes%20the%20evaluation%20of%20a%0Aresidual%20function%20at%20a%20set%20of%20collocation%20points%20within%20the%20domain.%20The%20quality%0Aof%20a%20PINNs%20solution%20depends%20upon%20numerous%20parameters%2C%20including%20the%20number%20and%0Adistribution%20of%20these%20collocation%20points.%20In%20this%20paper%20we%20consider%20a%20number%20of%0Astrategies%20for%20selecting%20these%20points%20and%20investigate%20their%20impact%20on%20the%0Aoverall%20accuracy%20of%20the%20method.%20In%20particular%2C%20we%20suggest%20that%20no%20single%0Aapproach%20is%20likely%20to%20be%20%60%60optimal%27%27%20but%20we%20show%20how%20a%20number%20of%20important%0Ametrics%20can%20have%20an%20impact%20in%20improving%20the%20quality%20of%20the%20results%20obtained%0Awhen%20using%20a%20fixed%20number%20of%20residual%20evaluations.%20We%20illustrate%20these%0Aapproaches%20through%20the%20use%20of%20two%20benchmark%20test%20problems%3A%20Burgers%27%20equation%0Aand%20the%20Allen-Cahn%20equation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12282v1&entry.124074799=Read"},
{"title": "t-viSNE: Interactive Assessment and Interpretation of t-SNE Projections", "author": "Angelos Chatzimparmpas and Rafael M. Martins and Andreas Kerren", "abstract": "  t-Distributed Stochastic Neighbor Embedding (t-SNE) for the visualization of\nmultidimensional data has proven to be a popular approach, with successful\napplications in a wide range of domains. Despite their usefulness, t-SNE\nprojections can be hard to interpret or even misleading, which hurts the\ntrustworthiness of the results. Understanding the details of t-SNE itself and\nthe reasons behind specific patterns in its output may be a daunting task,\nespecially for non-experts in dimensionality reduction. In this work, we\npresent t-viSNE, an interactive tool for the visual exploration of t-SNE\nprojections that enables analysts to inspect different aspects of their\naccuracy and meaning, such as the effects of hyper-parameters, distance and\nneighborhood preservation, densities and costs of specific neighborhoods, and\nthe correlations between dimensions and visual patterns. We propose a coherent,\naccessible, and well-integrated collection of different views for the\nvisualization of t-SNE projections. The applicability and usability of t-viSNE\nare demonstrated through hypothetical usage scenarios with real data sets.\nFinally, we present the results of a user study where the tool's effectiveness\nwas evaluated. By bringing to light information that would normally be lost\nafter running t-SNE, we hope to support analysts in using t-SNE and making its\nresults better understandable.\n", "link": "http://arxiv.org/abs/2002.06910v5", "date": "2024-04-18", "relevancy": 1.972, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5129}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4791}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4782}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20t-viSNE%3A%20Interactive%20Assessment%20and%20Interpretation%20of%20t-SNE%20Projections&body=Title%3A%20t-viSNE%3A%20Interactive%20Assessment%20and%20Interpretation%20of%20t-SNE%20Projections%0AAuthor%3A%20Angelos%20Chatzimparmpas%20and%20Rafael%20M.%20Martins%20and%20Andreas%20Kerren%0AAbstract%3A%20%20%20t-Distributed%20Stochastic%20Neighbor%20Embedding%20%28t-SNE%29%20for%20the%20visualization%20of%0Amultidimensional%20data%20has%20proven%20to%20be%20a%20popular%20approach%2C%20with%20successful%0Aapplications%20in%20a%20wide%20range%20of%20domains.%20Despite%20their%20usefulness%2C%20t-SNE%0Aprojections%20can%20be%20hard%20to%20interpret%20or%20even%20misleading%2C%20which%20hurts%20the%0Atrustworthiness%20of%20the%20results.%20Understanding%20the%20details%20of%20t-SNE%20itself%20and%0Athe%20reasons%20behind%20specific%20patterns%20in%20its%20output%20may%20be%20a%20daunting%20task%2C%0Aespecially%20for%20non-experts%20in%20dimensionality%20reduction.%20In%20this%20work%2C%20we%0Apresent%20t-viSNE%2C%20an%20interactive%20tool%20for%20the%20visual%20exploration%20of%20t-SNE%0Aprojections%20that%20enables%20analysts%20to%20inspect%20different%20aspects%20of%20their%0Aaccuracy%20and%20meaning%2C%20such%20as%20the%20effects%20of%20hyper-parameters%2C%20distance%20and%0Aneighborhood%20preservation%2C%20densities%20and%20costs%20of%20specific%20neighborhoods%2C%20and%0Athe%20correlations%20between%20dimensions%20and%20visual%20patterns.%20We%20propose%20a%20coherent%2C%0Aaccessible%2C%20and%20well-integrated%20collection%20of%20different%20views%20for%20the%0Avisualization%20of%20t-SNE%20projections.%20The%20applicability%20and%20usability%20of%20t-viSNE%0Aare%20demonstrated%20through%20hypothetical%20usage%20scenarios%20with%20real%20data%20sets.%0AFinally%2C%20we%20present%20the%20results%20of%20a%20user%20study%20where%20the%20tool%27s%20effectiveness%0Awas%20evaluated.%20By%20bringing%20to%20light%20information%20that%20would%20normally%20be%20lost%0Aafter%20running%20t-SNE%2C%20we%20hope%20to%20support%20analysts%20in%20using%20t-SNE%20and%20making%20its%0Aresults%20better%20understandable.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2002.06910v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=t-viSNE%3A%20Interactive%20Assessment%20and%20Interpretation%20of%20t-SNE%20Projections&entry.906535625=Angelos%20Chatzimparmpas%20and%20Rafael%20M.%20Martins%20and%20Andreas%20Kerren&entry.1292438233=%20%20t-Distributed%20Stochastic%20Neighbor%20Embedding%20%28t-SNE%29%20for%20the%20visualization%20of%0Amultidimensional%20data%20has%20proven%20to%20be%20a%20popular%20approach%2C%20with%20successful%0Aapplications%20in%20a%20wide%20range%20of%20domains.%20Despite%20their%20usefulness%2C%20t-SNE%0Aprojections%20can%20be%20hard%20to%20interpret%20or%20even%20misleading%2C%20which%20hurts%20the%0Atrustworthiness%20of%20the%20results.%20Understanding%20the%20details%20of%20t-SNE%20itself%20and%0Athe%20reasons%20behind%20specific%20patterns%20in%20its%20output%20may%20be%20a%20daunting%20task%2C%0Aespecially%20for%20non-experts%20in%20dimensionality%20reduction.%20In%20this%20work%2C%20we%0Apresent%20t-viSNE%2C%20an%20interactive%20tool%20for%20the%20visual%20exploration%20of%20t-SNE%0Aprojections%20that%20enables%20analysts%20to%20inspect%20different%20aspects%20of%20their%0Aaccuracy%20and%20meaning%2C%20such%20as%20the%20effects%20of%20hyper-parameters%2C%20distance%20and%0Aneighborhood%20preservation%2C%20densities%20and%20costs%20of%20specific%20neighborhoods%2C%20and%0Athe%20correlations%20between%20dimensions%20and%20visual%20patterns.%20We%20propose%20a%20coherent%2C%0Aaccessible%2C%20and%20well-integrated%20collection%20of%20different%20views%20for%20the%0Avisualization%20of%20t-SNE%20projections.%20The%20applicability%20and%20usability%20of%20t-viSNE%0Aare%20demonstrated%20through%20hypothetical%20usage%20scenarios%20with%20real%20data%20sets.%0AFinally%2C%20we%20present%20the%20results%20of%20a%20user%20study%20where%20the%20tool%27s%20effectiveness%0Awas%20evaluated.%20By%20bringing%20to%20light%20information%20that%20would%20normally%20be%20lost%0Aafter%20running%20t-SNE%2C%20we%20hope%20to%20support%20analysts%20in%20using%20t-SNE%20and%20making%20its%0Aresults%20better%20understandable.%0A&entry.1838667208=http%3A//arxiv.org/abs/2002.06910v5&entry.124074799=Read"},
{"title": "Improving the interpretability of GNN predictions through\n  conformal-based graph sparsification", "author": "Pablo Sanchez-Martin and Kinaan Aamir Khan and Isabel Valera", "abstract": "  Graph Neural Networks (GNNs) have achieved state-of-the-art performance in\nsolving graph classification tasks. However, most GNN architectures aggregate\ninformation from all nodes and edges in a graph, regardless of their relevance\nto the task at hand, thus hindering the interpretability of their predictions.\nIn contrast to prior work, in this paper we propose a GNN \\emph{training}\napproach that jointly i) finds the most predictive subgraph by removing edges\nand/or nodes -- -\\emph{without making assumptions about the subgraph structure}\n-- while ii) optimizing the performance of the graph classification task. To\nthat end, we rely on reinforcement learning to solve the resulting bi-level\noptimization with a reward function based on conformal predictions to account\nfor the current in-training uncertainty of the classifier. Our empirical\nresults on nine different graph classification datasets show that our method\ncompetes in performance with baselines while relying on significantly sparser\nsubgraphs, leading to more interpretable GNN-based predictions.\n", "link": "http://arxiv.org/abs/2404.12356v1", "date": "2024-04-18", "relevancy": 1.9655, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5051}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4917}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4562}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Improving%20the%20interpretability%20of%20GNN%20predictions%20through%0A%20%20conformal-based%20graph%20sparsification&body=Title%3A%20Improving%20the%20interpretability%20of%20GNN%20predictions%20through%0A%20%20conformal-based%20graph%20sparsification%0AAuthor%3A%20Pablo%20Sanchez-Martin%20and%20Kinaan%20Aamir%20Khan%20and%20Isabel%20Valera%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20state-of-the-art%20performance%20in%0Asolving%20graph%20classification%20tasks.%20However%2C%20most%20GNN%20architectures%20aggregate%0Ainformation%20from%20all%20nodes%20and%20edges%20in%20a%20graph%2C%20regardless%20of%20their%20relevance%0Ato%20the%20task%20at%20hand%2C%20thus%20hindering%20the%20interpretability%20of%20their%20predictions.%0AIn%20contrast%20to%20prior%20work%2C%20in%20this%20paper%20we%20propose%20a%20GNN%20%5Cemph%7Btraining%7D%0Aapproach%20that%20jointly%20i%29%20finds%20the%20most%20predictive%20subgraph%20by%20removing%20edges%0Aand/or%20nodes%20--%20-%5Cemph%7Bwithout%20making%20assumptions%20about%20the%20subgraph%20structure%7D%0A--%20while%20ii%29%20optimizing%20the%20performance%20of%20the%20graph%20classification%20task.%20To%0Athat%20end%2C%20we%20rely%20on%20reinforcement%20learning%20to%20solve%20the%20resulting%20bi-level%0Aoptimization%20with%20a%20reward%20function%20based%20on%20conformal%20predictions%20to%20account%0Afor%20the%20current%20in-training%20uncertainty%20of%20the%20classifier.%20Our%20empirical%0Aresults%20on%20nine%20different%20graph%20classification%20datasets%20show%20that%20our%20method%0Acompetes%20in%20performance%20with%20baselines%20while%20relying%20on%20significantly%20sparser%0Asubgraphs%2C%20leading%20to%20more%20interpretable%20GNN-based%20predictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12356v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20the%20interpretability%20of%20GNN%20predictions%20through%0A%20%20conformal-based%20graph%20sparsification&entry.906535625=Pablo%20Sanchez-Martin%20and%20Kinaan%20Aamir%20Khan%20and%20Isabel%20Valera&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20have%20achieved%20state-of-the-art%20performance%20in%0Asolving%20graph%20classification%20tasks.%20However%2C%20most%20GNN%20architectures%20aggregate%0Ainformation%20from%20all%20nodes%20and%20edges%20in%20a%20graph%2C%20regardless%20of%20their%20relevance%0Ato%20the%20task%20at%20hand%2C%20thus%20hindering%20the%20interpretability%20of%20their%20predictions.%0AIn%20contrast%20to%20prior%20work%2C%20in%20this%20paper%20we%20propose%20a%20GNN%20%5Cemph%7Btraining%7D%0Aapproach%20that%20jointly%20i%29%20finds%20the%20most%20predictive%20subgraph%20by%20removing%20edges%0Aand/or%20nodes%20--%20-%5Cemph%7Bwithout%20making%20assumptions%20about%20the%20subgraph%20structure%7D%0A--%20while%20ii%29%20optimizing%20the%20performance%20of%20the%20graph%20classification%20task.%20To%0Athat%20end%2C%20we%20rely%20on%20reinforcement%20learning%20to%20solve%20the%20resulting%20bi-level%0Aoptimization%20with%20a%20reward%20function%20based%20on%20conformal%20predictions%20to%20account%0Afor%20the%20current%20in-training%20uncertainty%20of%20the%20classifier.%20Our%20empirical%0Aresults%20on%20nine%20different%20graph%20classification%20datasets%20show%20that%20our%20method%0Acompetes%20in%20performance%20with%20baselines%20while%20relying%20on%20significantly%20sparser%0Asubgraphs%2C%20leading%20to%20more%20interpretable%20GNN-based%20predictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12356v1&entry.124074799=Read"},
{"title": "Customizing Text-to-Image Diffusion with Camera Viewpoint Control", "author": "Nupur Kumari and Grace Su and Richard Zhang and Taesung Park and Eli Shechtman and Jun-Yan Zhu", "abstract": "  Model customization introduces new concepts to existing text-to-image models,\nenabling the generation of the new concept in novel contexts. However, such\nmethods lack accurate camera view control w.r.t the object, and users must\nresort to prompt engineering (e.g., adding \"top-view\") to achieve coarse view\ncontrol. In this work, we introduce a new task -- enabling explicit control of\ncamera viewpoint for model customization. This allows us to modify object\nproperties amongst various background scenes via text prompts, all while\nincorporating the target camera pose as additional control. This new task\npresents significant challenges in merging a 3D representation from the\nmulti-view images of the new concept with a general, 2D text-to-image model. To\nbridge this gap, we propose to condition the 2D diffusion process on rendered,\nview-dependent features of the new object. During training, we jointly adapt\nthe 2D diffusion modules and 3D feature predictions to reconstruct the object's\nappearance and geometry while reducing overfitting to the input multi-view\nimages. Our method outperforms existing image editing and model personalization\nbaselines in preserving the custom object's identity while following the input\ntext prompt and the object's camera pose.\n", "link": "http://arxiv.org/abs/2404.12333v1", "date": "2024-04-18", "relevancy": 1.9462, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.7293}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6532}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6147}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Customizing%20Text-to-Image%20Diffusion%20with%20Camera%20Viewpoint%20Control&body=Title%3A%20Customizing%20Text-to-Image%20Diffusion%20with%20Camera%20Viewpoint%20Control%0AAuthor%3A%20Nupur%20Kumari%20and%20Grace%20Su%20and%20Richard%20Zhang%20and%20Taesung%20Park%20and%20Eli%20Shechtman%20and%20Jun-Yan%20Zhu%0AAbstract%3A%20%20%20Model%20customization%20introduces%20new%20concepts%20to%20existing%20text-to-image%20models%2C%0Aenabling%20the%20generation%20of%20the%20new%20concept%20in%20novel%20contexts.%20However%2C%20such%0Amethods%20lack%20accurate%20camera%20view%20control%20w.r.t%20the%20object%2C%20and%20users%20must%0Aresort%20to%20prompt%20engineering%20%28e.g.%2C%20adding%20%22top-view%22%29%20to%20achieve%20coarse%20view%0Acontrol.%20In%20this%20work%2C%20we%20introduce%20a%20new%20task%20--%20enabling%20explicit%20control%20of%0Acamera%20viewpoint%20for%20model%20customization.%20This%20allows%20us%20to%20modify%20object%0Aproperties%20amongst%20various%20background%20scenes%20via%20text%20prompts%2C%20all%20while%0Aincorporating%20the%20target%20camera%20pose%20as%20additional%20control.%20This%20new%20task%0Apresents%20significant%20challenges%20in%20merging%20a%203D%20representation%20from%20the%0Amulti-view%20images%20of%20the%20new%20concept%20with%20a%20general%2C%202D%20text-to-image%20model.%20To%0Abridge%20this%20gap%2C%20we%20propose%20to%20condition%20the%202D%20diffusion%20process%20on%20rendered%2C%0Aview-dependent%20features%20of%20the%20new%20object.%20During%20training%2C%20we%20jointly%20adapt%0Athe%202D%20diffusion%20modules%20and%203D%20feature%20predictions%20to%20reconstruct%20the%20object%27s%0Aappearance%20and%20geometry%20while%20reducing%20overfitting%20to%20the%20input%20multi-view%0Aimages.%20Our%20method%20outperforms%20existing%20image%20editing%20and%20model%20personalization%0Abaselines%20in%20preserving%20the%20custom%20object%27s%20identity%20while%20following%20the%20input%0Atext%20prompt%20and%20the%20object%27s%20camera%20pose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12333v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Customizing%20Text-to-Image%20Diffusion%20with%20Camera%20Viewpoint%20Control&entry.906535625=Nupur%20Kumari%20and%20Grace%20Su%20and%20Richard%20Zhang%20and%20Taesung%20Park%20and%20Eli%20Shechtman%20and%20Jun-Yan%20Zhu&entry.1292438233=%20%20Model%20customization%20introduces%20new%20concepts%20to%20existing%20text-to-image%20models%2C%0Aenabling%20the%20generation%20of%20the%20new%20concept%20in%20novel%20contexts.%20However%2C%20such%0Amethods%20lack%20accurate%20camera%20view%20control%20w.r.t%20the%20object%2C%20and%20users%20must%0Aresort%20to%20prompt%20engineering%20%28e.g.%2C%20adding%20%22top-view%22%29%20to%20achieve%20coarse%20view%0Acontrol.%20In%20this%20work%2C%20we%20introduce%20a%20new%20task%20--%20enabling%20explicit%20control%20of%0Acamera%20viewpoint%20for%20model%20customization.%20This%20allows%20us%20to%20modify%20object%0Aproperties%20amongst%20various%20background%20scenes%20via%20text%20prompts%2C%20all%20while%0Aincorporating%20the%20target%20camera%20pose%20as%20additional%20control.%20This%20new%20task%0Apresents%20significant%20challenges%20in%20merging%20a%203D%20representation%20from%20the%0Amulti-view%20images%20of%20the%20new%20concept%20with%20a%20general%2C%202D%20text-to-image%20model.%20To%0Abridge%20this%20gap%2C%20we%20propose%20to%20condition%20the%202D%20diffusion%20process%20on%20rendered%2C%0Aview-dependent%20features%20of%20the%20new%20object.%20During%20training%2C%20we%20jointly%20adapt%0Athe%202D%20diffusion%20modules%20and%203D%20feature%20predictions%20to%20reconstruct%20the%20object%27s%0Aappearance%20and%20geometry%20while%20reducing%20overfitting%20to%20the%20input%20multi-view%0Aimages.%20Our%20method%20outperforms%20existing%20image%20editing%20and%20model%20personalization%0Abaselines%20in%20preserving%20the%20custom%20object%27s%20identity%20while%20following%20the%20input%0Atext%20prompt%20and%20the%20object%27s%20camera%20pose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12333v1&entry.124074799=Read"},
{"title": "FedEval-LLM: Federated Evaluation of Large Language Models on Downstream\n  Tasks with Collective Wisdom", "author": "Yuanqin He and Yan Kang and Lixin Fan and Qiang Yang", "abstract": "  Federated Learning (FL) has emerged as a promising solution for collaborative\ntraining of large language models (LLMs). However, the integration of LLMs into\nFL introduces new challenges, particularly concerning the evaluation of LLMs.\nTraditional evaluation methods that rely on labeled test sets and\nsimilarity-based metrics cover only a subset of the acceptable answers, thereby\nfailing to accurately reflect the performance of LLMs on generative tasks.\nMeanwhile, although automatic evaluation methods that leverage advanced LLMs\npresent potential, they face critical risks of data leakage due to the need to\ntransmit data to external servers and suboptimal performance on downstream\ntasks due to the lack of domain knowledge. To address these issues, we propose\na Federated Evaluation framework of Large Language Models, named FedEval-LLM,\nthat provides reliable performance measurements of LLMs on downstream tasks\nwithout the reliance on labeled test sets and external tools, thus ensuring\nstrong privacy-preserving capability. FedEval-LLM leverages a consortium of\npersonalized LLMs from participants as referees to provide domain knowledge and\ncollective evaluation capability, thus aligning to the respective downstream\ntasks and mitigating uncertainties and biases associated with a single referee.\nExperimental results demonstrate a significant improvement in the evaluation\ncapability of personalized evaluation models on downstream tasks. When applied\nto FL, these evaluation models exhibit strong agreement with human preference\nand RougeL-score on meticulously curated test sets. FedEval-LLM effectively\novercomes the limitations of traditional metrics and the reliance on external\nservices, making it a promising framework for the evaluation of LLMs within\ncollaborative training scenarios.\n", "link": "http://arxiv.org/abs/2404.12273v1", "date": "2024-04-18", "relevancy": 1.9198, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5458}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4696}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4639}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FedEval-LLM%3A%20Federated%20Evaluation%20of%20Large%20Language%20Models%20on%20Downstream%0A%20%20Tasks%20with%20Collective%20Wisdom&body=Title%3A%20FedEval-LLM%3A%20Federated%20Evaluation%20of%20Large%20Language%20Models%20on%20Downstream%0A%20%20Tasks%20with%20Collective%20Wisdom%0AAuthor%3A%20Yuanqin%20He%20and%20Yan%20Kang%20and%20Lixin%20Fan%20and%20Qiang%20Yang%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20has%20emerged%20as%20a%20promising%20solution%20for%20collaborative%0Atraining%20of%20large%20language%20models%20%28LLMs%29.%20However%2C%20the%20integration%20of%20LLMs%20into%0AFL%20introduces%20new%20challenges%2C%20particularly%20concerning%20the%20evaluation%20of%20LLMs.%0ATraditional%20evaluation%20methods%20that%20rely%20on%20labeled%20test%20sets%20and%0Asimilarity-based%20metrics%20cover%20only%20a%20subset%20of%20the%20acceptable%20answers%2C%20thereby%0Afailing%20to%20accurately%20reflect%20the%20performance%20of%20LLMs%20on%20generative%20tasks.%0AMeanwhile%2C%20although%20automatic%20evaluation%20methods%20that%20leverage%20advanced%20LLMs%0Apresent%20potential%2C%20they%20face%20critical%20risks%20of%20data%20leakage%20due%20to%20the%20need%20to%0Atransmit%20data%20to%20external%20servers%20and%20suboptimal%20performance%20on%20downstream%0Atasks%20due%20to%20the%20lack%20of%20domain%20knowledge.%20To%20address%20these%20issues%2C%20we%20propose%0Aa%20Federated%20Evaluation%20framework%20of%20Large%20Language%20Models%2C%20named%20FedEval-LLM%2C%0Athat%20provides%20reliable%20performance%20measurements%20of%20LLMs%20on%20downstream%20tasks%0Awithout%20the%20reliance%20on%20labeled%20test%20sets%20and%20external%20tools%2C%20thus%20ensuring%0Astrong%20privacy-preserving%20capability.%20FedEval-LLM%20leverages%20a%20consortium%20of%0Apersonalized%20LLMs%20from%20participants%20as%20referees%20to%20provide%20domain%20knowledge%20and%0Acollective%20evaluation%20capability%2C%20thus%20aligning%20to%20the%20respective%20downstream%0Atasks%20and%20mitigating%20uncertainties%20and%20biases%20associated%20with%20a%20single%20referee.%0AExperimental%20results%20demonstrate%20a%20significant%20improvement%20in%20the%20evaluation%0Acapability%20of%20personalized%20evaluation%20models%20on%20downstream%20tasks.%20When%20applied%0Ato%20FL%2C%20these%20evaluation%20models%20exhibit%20strong%20agreement%20with%20human%20preference%0Aand%20RougeL-score%20on%20meticulously%20curated%20test%20sets.%20FedEval-LLM%20effectively%0Aovercomes%20the%20limitations%20of%20traditional%20metrics%20and%20the%20reliance%20on%20external%0Aservices%2C%20making%20it%20a%20promising%20framework%20for%20the%20evaluation%20of%20LLMs%20within%0Acollaborative%20training%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12273v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedEval-LLM%3A%20Federated%20Evaluation%20of%20Large%20Language%20Models%20on%20Downstream%0A%20%20Tasks%20with%20Collective%20Wisdom&entry.906535625=Yuanqin%20He%20and%20Yan%20Kang%20and%20Lixin%20Fan%20and%20Qiang%20Yang&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20has%20emerged%20as%20a%20promising%20solution%20for%20collaborative%0Atraining%20of%20large%20language%20models%20%28LLMs%29.%20However%2C%20the%20integration%20of%20LLMs%20into%0AFL%20introduces%20new%20challenges%2C%20particularly%20concerning%20the%20evaluation%20of%20LLMs.%0ATraditional%20evaluation%20methods%20that%20rely%20on%20labeled%20test%20sets%20and%0Asimilarity-based%20metrics%20cover%20only%20a%20subset%20of%20the%20acceptable%20answers%2C%20thereby%0Afailing%20to%20accurately%20reflect%20the%20performance%20of%20LLMs%20on%20generative%20tasks.%0AMeanwhile%2C%20although%20automatic%20evaluation%20methods%20that%20leverage%20advanced%20LLMs%0Apresent%20potential%2C%20they%20face%20critical%20risks%20of%20data%20leakage%20due%20to%20the%20need%20to%0Atransmit%20data%20to%20external%20servers%20and%20suboptimal%20performance%20on%20downstream%0Atasks%20due%20to%20the%20lack%20of%20domain%20knowledge.%20To%20address%20these%20issues%2C%20we%20propose%0Aa%20Federated%20Evaluation%20framework%20of%20Large%20Language%20Models%2C%20named%20FedEval-LLM%2C%0Athat%20provides%20reliable%20performance%20measurements%20of%20LLMs%20on%20downstream%20tasks%0Awithout%20the%20reliance%20on%20labeled%20test%20sets%20and%20external%20tools%2C%20thus%20ensuring%0Astrong%20privacy-preserving%20capability.%20FedEval-LLM%20leverages%20a%20consortium%20of%0Apersonalized%20LLMs%20from%20participants%20as%20referees%20to%20provide%20domain%20knowledge%20and%0Acollective%20evaluation%20capability%2C%20thus%20aligning%20to%20the%20respective%20downstream%0Atasks%20and%20mitigating%20uncertainties%20and%20biases%20associated%20with%20a%20single%20referee.%0AExperimental%20results%20demonstrate%20a%20significant%20improvement%20in%20the%20evaluation%0Acapability%20of%20personalized%20evaluation%20models%20on%20downstream%20tasks.%20When%20applied%0Ato%20FL%2C%20these%20evaluation%20models%20exhibit%20strong%20agreement%20with%20human%20preference%0Aand%20RougeL-score%20on%20meticulously%20curated%20test%20sets.%20FedEval-LLM%20effectively%0Aovercomes%20the%20limitations%20of%20traditional%20metrics%20and%20the%20reliance%20on%20external%0Aservices%2C%20making%20it%20a%20promising%20framework%20for%20the%20evaluation%20of%20LLMs%20within%0Acollaborative%20training%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12273v1&entry.124074799=Read"},
{"title": "From $r$ to $Q^*$: Your Language Model is Secretly a Q-Function", "author": "Rafael Rafailov and Joey Hejna and Ryan Park and Chelsea Finn", "abstract": "  Reinforcement Learning From Human Feedback (RLHF) has been a critical to the\nsuccess of the latest generation of generative AI models. In response to the\ncomplex nature of the classical RLHF pipeline, direct alignment algorithms such\nas Direct Preference Optimization (DPO) have emerged as an alternative\napproach. Although DPO solves the same objective as the standard RLHF setup,\nthere is a mismatch between the two approaches. Standard RLHF deploys\nreinforcement learning in a specific token-level MDP, while DPO is derived as a\nbandit problem in which the whole response of the model is treated as a single\narm. In this work we rectify this difference, first we theoretically show that\nwe can derive DPO in the token-level MDP as a general inverse Q-learning\nalgorithm, which satisfies the Bellman equation. Using our theoretical results,\nwe provide three concrete empirical insights. First, we show that because of\nits token level interpretation, DPO is able to perform some type of credit\nassignment. Next, we prove that under the token level formulation, classical\nsearch-based algorithms, such as MCTS, which have recently been applied to the\nlanguage generation space, are equivalent to likelihood-based search on a DPO\npolicy. Empirically we show that a simple beam search yields meaningful\nimprovement over the base DPO policy. Finally, we show how the choice of\nreference policy causes implicit rewards to decline during training. We\nconclude by discussing applications of our work, including information\nelicitation in multi-tun dialogue, reasoning, agentic applications and\nend-to-end training of multi-model systems.\n", "link": "http://arxiv.org/abs/2404.12358v1", "date": "2024-04-18", "relevancy": 1.9165, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4806}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4796}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.478}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20From%20%24r%24%20to%20%24Q%5E%2A%24%3A%20Your%20Language%20Model%20is%20Secretly%20a%20Q-Function&body=Title%3A%20From%20%24r%24%20to%20%24Q%5E%2A%24%3A%20Your%20Language%20Model%20is%20Secretly%20a%20Q-Function%0AAuthor%3A%20Rafael%20Rafailov%20and%20Joey%20Hejna%20and%20Ryan%20Park%20and%20Chelsea%20Finn%0AAbstract%3A%20%20%20Reinforcement%20Learning%20From%20Human%20Feedback%20%28RLHF%29%20has%20been%20a%20critical%20to%20the%0Asuccess%20of%20the%20latest%20generation%20of%20generative%20AI%20models.%20In%20response%20to%20the%0Acomplex%20nature%20of%20the%20classical%20RLHF%20pipeline%2C%20direct%20alignment%20algorithms%20such%0Aas%20Direct%20Preference%20Optimization%20%28DPO%29%20have%20emerged%20as%20an%20alternative%0Aapproach.%20Although%20DPO%20solves%20the%20same%20objective%20as%20the%20standard%20RLHF%20setup%2C%0Athere%20is%20a%20mismatch%20between%20the%20two%20approaches.%20Standard%20RLHF%20deploys%0Areinforcement%20learning%20in%20a%20specific%20token-level%20MDP%2C%20while%20DPO%20is%20derived%20as%20a%0Abandit%20problem%20in%20which%20the%20whole%20response%20of%20the%20model%20is%20treated%20as%20a%20single%0Aarm.%20In%20this%20work%20we%20rectify%20this%20difference%2C%20first%20we%20theoretically%20show%20that%0Awe%20can%20derive%20DPO%20in%20the%20token-level%20MDP%20as%20a%20general%20inverse%20Q-learning%0Aalgorithm%2C%20which%20satisfies%20the%20Bellman%20equation.%20Using%20our%20theoretical%20results%2C%0Awe%20provide%20three%20concrete%20empirical%20insights.%20First%2C%20we%20show%20that%20because%20of%0Aits%20token%20level%20interpretation%2C%20DPO%20is%20able%20to%20perform%20some%20type%20of%20credit%0Aassignment.%20Next%2C%20we%20prove%20that%20under%20the%20token%20level%20formulation%2C%20classical%0Asearch-based%20algorithms%2C%20such%20as%20MCTS%2C%20which%20have%20recently%20been%20applied%20to%20the%0Alanguage%20generation%20space%2C%20are%20equivalent%20to%20likelihood-based%20search%20on%20a%20DPO%0Apolicy.%20Empirically%20we%20show%20that%20a%20simple%20beam%20search%20yields%20meaningful%0Aimprovement%20over%20the%20base%20DPO%20policy.%20Finally%2C%20we%20show%20how%20the%20choice%20of%0Areference%20policy%20causes%20implicit%20rewards%20to%20decline%20during%20training.%20We%0Aconclude%20by%20discussing%20applications%20of%20our%20work%2C%20including%20information%0Aelicitation%20in%20multi-tun%20dialogue%2C%20reasoning%2C%20agentic%20applications%20and%0Aend-to-end%20training%20of%20multi-model%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12358v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20%24r%24%20to%20%24Q%5E%2A%24%3A%20Your%20Language%20Model%20is%20Secretly%20a%20Q-Function&entry.906535625=Rafael%20Rafailov%20and%20Joey%20Hejna%20and%20Ryan%20Park%20and%20Chelsea%20Finn&entry.1292438233=%20%20Reinforcement%20Learning%20From%20Human%20Feedback%20%28RLHF%29%20has%20been%20a%20critical%20to%20the%0Asuccess%20of%20the%20latest%20generation%20of%20generative%20AI%20models.%20In%20response%20to%20the%0Acomplex%20nature%20of%20the%20classical%20RLHF%20pipeline%2C%20direct%20alignment%20algorithms%20such%0Aas%20Direct%20Preference%20Optimization%20%28DPO%29%20have%20emerged%20as%20an%20alternative%0Aapproach.%20Although%20DPO%20solves%20the%20same%20objective%20as%20the%20standard%20RLHF%20setup%2C%0Athere%20is%20a%20mismatch%20between%20the%20two%20approaches.%20Standard%20RLHF%20deploys%0Areinforcement%20learning%20in%20a%20specific%20token-level%20MDP%2C%20while%20DPO%20is%20derived%20as%20a%0Abandit%20problem%20in%20which%20the%20whole%20response%20of%20the%20model%20is%20treated%20as%20a%20single%0Aarm.%20In%20this%20work%20we%20rectify%20this%20difference%2C%20first%20we%20theoretically%20show%20that%0Awe%20can%20derive%20DPO%20in%20the%20token-level%20MDP%20as%20a%20general%20inverse%20Q-learning%0Aalgorithm%2C%20which%20satisfies%20the%20Bellman%20equation.%20Using%20our%20theoretical%20results%2C%0Awe%20provide%20three%20concrete%20empirical%20insights.%20First%2C%20we%20show%20that%20because%20of%0Aits%20token%20level%20interpretation%2C%20DPO%20is%20able%20to%20perform%20some%20type%20of%20credit%0Aassignment.%20Next%2C%20we%20prove%20that%20under%20the%20token%20level%20formulation%2C%20classical%0Asearch-based%20algorithms%2C%20such%20as%20MCTS%2C%20which%20have%20recently%20been%20applied%20to%20the%0Alanguage%20generation%20space%2C%20are%20equivalent%20to%20likelihood-based%20search%20on%20a%20DPO%0Apolicy.%20Empirically%20we%20show%20that%20a%20simple%20beam%20search%20yields%20meaningful%0Aimprovement%20over%20the%20base%20DPO%20policy.%20Finally%2C%20we%20show%20how%20the%20choice%20of%0Areference%20policy%20causes%20implicit%20rewards%20to%20decline%20during%20training.%20We%0Aconclude%20by%20discussing%20applications%20of%20our%20work%2C%20including%20information%0Aelicitation%20in%20multi-tun%20dialogue%2C%20reasoning%2C%20agentic%20applications%20and%0Aend-to-end%20training%20of%20multi-model%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12358v1&entry.124074799=Read"},
{"title": "Lazy Diffusion Transformer for Interactive Image Editing", "author": "Yotam Nitzan and Zongze Wu and Richard Zhang and Eli Shechtman and Daniel Cohen-Or and Taesung Park and Micha\u00ebl Gharbi", "abstract": "  We introduce a novel diffusion transformer, LazyDiffusion, that generates\npartial image updates efficiently. Our approach targets interactive image\nediting applications in which, starting from a blank canvas or an image, a user\nspecifies a sequence of localized image modifications using binary masks and\ntext prompts. Our generator operates in two phases. First, a context encoder\nprocesses the current canvas and user mask to produce a compact global context\ntailored to the region to generate. Second, conditioned on this context, a\ndiffusion-based transformer decoder synthesizes the masked pixels in a \"lazy\"\nfashion, i.e., it only generates the masked region. This contrasts with\nprevious works that either regenerate the full canvas, wasting time and\ncomputation, or confine processing to a tight rectangular crop around the mask,\nignoring the global image context altogether. Our decoder's runtime scales with\nthe mask size, which is typically small, while our encoder introduces\nnegligible overhead. We demonstrate that our approach is competitive with\nstate-of-the-art inpainting methods in terms of quality and fidelity while\nproviding a 10x speedup for typical user interactions, where the editing mask\nrepresents 10% of the image.\n", "link": "http://arxiv.org/abs/2404.12382v1", "date": "2024-04-18", "relevancy": 1.9091, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6591}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6469}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6231}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Lazy%20Diffusion%20Transformer%20for%20Interactive%20Image%20Editing&body=Title%3A%20Lazy%20Diffusion%20Transformer%20for%20Interactive%20Image%20Editing%0AAuthor%3A%20Yotam%20Nitzan%20and%20Zongze%20Wu%20and%20Richard%20Zhang%20and%20Eli%20Shechtman%20and%20Daniel%20Cohen-Or%20and%20Taesung%20Park%20and%20Micha%C3%ABl%20Gharbi%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20diffusion%20transformer%2C%20LazyDiffusion%2C%20that%20generates%0Apartial%20image%20updates%20efficiently.%20Our%20approach%20targets%20interactive%20image%0Aediting%20applications%20in%20which%2C%20starting%20from%20a%20blank%20canvas%20or%20an%20image%2C%20a%20user%0Aspecifies%20a%20sequence%20of%20localized%20image%20modifications%20using%20binary%20masks%20and%0Atext%20prompts.%20Our%20generator%20operates%20in%20two%20phases.%20First%2C%20a%20context%20encoder%0Aprocesses%20the%20current%20canvas%20and%20user%20mask%20to%20produce%20a%20compact%20global%20context%0Atailored%20to%20the%20region%20to%20generate.%20Second%2C%20conditioned%20on%20this%20context%2C%20a%0Adiffusion-based%20transformer%20decoder%20synthesizes%20the%20masked%20pixels%20in%20a%20%22lazy%22%0Afashion%2C%20i.e.%2C%20it%20only%20generates%20the%20masked%20region.%20This%20contrasts%20with%0Aprevious%20works%20that%20either%20regenerate%20the%20full%20canvas%2C%20wasting%20time%20and%0Acomputation%2C%20or%20confine%20processing%20to%20a%20tight%20rectangular%20crop%20around%20the%20mask%2C%0Aignoring%20the%20global%20image%20context%20altogether.%20Our%20decoder%27s%20runtime%20scales%20with%0Athe%20mask%20size%2C%20which%20is%20typically%20small%2C%20while%20our%20encoder%20introduces%0Anegligible%20overhead.%20We%20demonstrate%20that%20our%20approach%20is%20competitive%20with%0Astate-of-the-art%20inpainting%20methods%20in%20terms%20of%20quality%20and%20fidelity%20while%0Aproviding%20a%2010x%20speedup%20for%20typical%20user%20interactions%2C%20where%20the%20editing%20mask%0Arepresents%2010%25%20of%20the%20image.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12382v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lazy%20Diffusion%20Transformer%20for%20Interactive%20Image%20Editing&entry.906535625=Yotam%20Nitzan%20and%20Zongze%20Wu%20and%20Richard%20Zhang%20and%20Eli%20Shechtman%20and%20Daniel%20Cohen-Or%20and%20Taesung%20Park%20and%20Micha%C3%ABl%20Gharbi&entry.1292438233=%20%20We%20introduce%20a%20novel%20diffusion%20transformer%2C%20LazyDiffusion%2C%20that%20generates%0Apartial%20image%20updates%20efficiently.%20Our%20approach%20targets%20interactive%20image%0Aediting%20applications%20in%20which%2C%20starting%20from%20a%20blank%20canvas%20or%20an%20image%2C%20a%20user%0Aspecifies%20a%20sequence%20of%20localized%20image%20modifications%20using%20binary%20masks%20and%0Atext%20prompts.%20Our%20generator%20operates%20in%20two%20phases.%20First%2C%20a%20context%20encoder%0Aprocesses%20the%20current%20canvas%20and%20user%20mask%20to%20produce%20a%20compact%20global%20context%0Atailored%20to%20the%20region%20to%20generate.%20Second%2C%20conditioned%20on%20this%20context%2C%20a%0Adiffusion-based%20transformer%20decoder%20synthesizes%20the%20masked%20pixels%20in%20a%20%22lazy%22%0Afashion%2C%20i.e.%2C%20it%20only%20generates%20the%20masked%20region.%20This%20contrasts%20with%0Aprevious%20works%20that%20either%20regenerate%20the%20full%20canvas%2C%20wasting%20time%20and%0Acomputation%2C%20or%20confine%20processing%20to%20a%20tight%20rectangular%20crop%20around%20the%20mask%2C%0Aignoring%20the%20global%20image%20context%20altogether.%20Our%20decoder%27s%20runtime%20scales%20with%0Athe%20mask%20size%2C%20which%20is%20typically%20small%2C%20while%20our%20encoder%20introduces%0Anegligible%20overhead.%20We%20demonstrate%20that%20our%20approach%20is%20competitive%20with%0Astate-of-the-art%20inpainting%20methods%20in%20terms%20of%20quality%20and%20fidelity%20while%0Aproviding%20a%2010x%20speedup%20for%20typical%20user%20interactions%2C%20where%20the%20editing%20mask%0Arepresents%2010%25%20of%20the%20image.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12382v1&entry.124074799=Read"},
{"title": "A new dataset for measuring the performance of blood vessel segmentation\n  methods under distribution shifts", "author": "Matheus Viana da Silva and Nat\u00e1lia de Carvalho Santos and Julie Ouellette and Baptiste Lacoste and Cesar Henrique Comin", "abstract": "  Creating a dataset for training supervised machine learning algorithms can be\na demanding task. This is especially true for medical image segmentation since\none or more specialists are usually required for image annotation, and creating\nground truth labels for just a single image can take up to several hours. In\naddition, it is paramount that the annotated samples represent well the\ndifferent conditions that might affect the imaged tissues as well as possible\nchanges in the image acquisition process. This can only be achieved by\nconsidering samples that are typical in the dataset as well as atypical, or\neven outlier, samples. We introduce VessMAP, a heterogeneous blood vessel\nsegmentation dataset acquired by carefully sampling relevant images from a\nlarger non-annotated dataset. A methodology was developed to select both\nprototypical and atypical samples from the base dataset, thus defining an\nassorted set of images that can be used for measuring the performance of\nsegmentation algorithms on samples that are highly distinct from each other. To\ndemonstrate the potential of the new dataset, we show that the validation\nperformance of a neural network changes significantly depending on the splits\nused for training the network.\n", "link": "http://arxiv.org/abs/2301.04517v4", "date": "2024-04-18", "relevancy": 1.9051, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4798}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4744}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4722}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20new%20dataset%20for%20measuring%20the%20performance%20of%20blood%20vessel%20segmentation%0A%20%20methods%20under%20distribution%20shifts&body=Title%3A%20A%20new%20dataset%20for%20measuring%20the%20performance%20of%20blood%20vessel%20segmentation%0A%20%20methods%20under%20distribution%20shifts%0AAuthor%3A%20Matheus%20Viana%20da%20Silva%20and%20Nat%C3%A1lia%20de%20Carvalho%20Santos%20and%20Julie%20Ouellette%20and%20Baptiste%20Lacoste%20and%20Cesar%20Henrique%20Comin%0AAbstract%3A%20%20%20Creating%20a%20dataset%20for%20training%20supervised%20machine%20learning%20algorithms%20can%20be%0Aa%20demanding%20task.%20This%20is%20especially%20true%20for%20medical%20image%20segmentation%20since%0Aone%20or%20more%20specialists%20are%20usually%20required%20for%20image%20annotation%2C%20and%20creating%0Aground%20truth%20labels%20for%20just%20a%20single%20image%20can%20take%20up%20to%20several%20hours.%20In%0Aaddition%2C%20it%20is%20paramount%20that%20the%20annotated%20samples%20represent%20well%20the%0Adifferent%20conditions%20that%20might%20affect%20the%20imaged%20tissues%20as%20well%20as%20possible%0Achanges%20in%20the%20image%20acquisition%20process.%20This%20can%20only%20be%20achieved%20by%0Aconsidering%20samples%20that%20are%20typical%20in%20the%20dataset%20as%20well%20as%20atypical%2C%20or%0Aeven%20outlier%2C%20samples.%20We%20introduce%20VessMAP%2C%20a%20heterogeneous%20blood%20vessel%0Asegmentation%20dataset%20acquired%20by%20carefully%20sampling%20relevant%20images%20from%20a%0Alarger%20non-annotated%20dataset.%20A%20methodology%20was%20developed%20to%20select%20both%0Aprototypical%20and%20atypical%20samples%20from%20the%20base%20dataset%2C%20thus%20defining%20an%0Aassorted%20set%20of%20images%20that%20can%20be%20used%20for%20measuring%20the%20performance%20of%0Asegmentation%20algorithms%20on%20samples%20that%20are%20highly%20distinct%20from%20each%20other.%20To%0Ademonstrate%20the%20potential%20of%20the%20new%20dataset%2C%20we%20show%20that%20the%20validation%0Aperformance%20of%20a%20neural%20network%20changes%20significantly%20depending%20on%20the%20splits%0Aused%20for%20training%20the%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.04517v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20new%20dataset%20for%20measuring%20the%20performance%20of%20blood%20vessel%20segmentation%0A%20%20methods%20under%20distribution%20shifts&entry.906535625=Matheus%20Viana%20da%20Silva%20and%20Nat%C3%A1lia%20de%20Carvalho%20Santos%20and%20Julie%20Ouellette%20and%20Baptiste%20Lacoste%20and%20Cesar%20Henrique%20Comin&entry.1292438233=%20%20Creating%20a%20dataset%20for%20training%20supervised%20machine%20learning%20algorithms%20can%20be%0Aa%20demanding%20task.%20This%20is%20especially%20true%20for%20medical%20image%20segmentation%20since%0Aone%20or%20more%20specialists%20are%20usually%20required%20for%20image%20annotation%2C%20and%20creating%0Aground%20truth%20labels%20for%20just%20a%20single%20image%20can%20take%20up%20to%20several%20hours.%20In%0Aaddition%2C%20it%20is%20paramount%20that%20the%20annotated%20samples%20represent%20well%20the%0Adifferent%20conditions%20that%20might%20affect%20the%20imaged%20tissues%20as%20well%20as%20possible%0Achanges%20in%20the%20image%20acquisition%20process.%20This%20can%20only%20be%20achieved%20by%0Aconsidering%20samples%20that%20are%20typical%20in%20the%20dataset%20as%20well%20as%20atypical%2C%20or%0Aeven%20outlier%2C%20samples.%20We%20introduce%20VessMAP%2C%20a%20heterogeneous%20blood%20vessel%0Asegmentation%20dataset%20acquired%20by%20carefully%20sampling%20relevant%20images%20from%20a%0Alarger%20non-annotated%20dataset.%20A%20methodology%20was%20developed%20to%20select%20both%0Aprototypical%20and%20atypical%20samples%20from%20the%20base%20dataset%2C%20thus%20defining%20an%0Aassorted%20set%20of%20images%20that%20can%20be%20used%20for%20measuring%20the%20performance%20of%0Asegmentation%20algorithms%20on%20samples%20that%20are%20highly%20distinct%20from%20each%20other.%20To%0Ademonstrate%20the%20potential%20of%20the%20new%20dataset%2C%20we%20show%20that%20the%20validation%0Aperformance%20of%20a%20neural%20network%20changes%20significantly%20depending%20on%20the%20splits%0Aused%20for%20training%20the%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.04517v4&entry.124074799=Read"},
{"title": "KDk: A Defense Mechanism Against Label Inference Attacks in Vertical\n  Federated Learning", "author": "Marco Arazzi and Serena Nicolazzo and Antonino Nocera", "abstract": "  Vertical Federated Learning (VFL) is a category of Federated Learning in\nwhich models are trained collaboratively among parties with vertically\npartitioned data. Typically, in a VFL scenario, the labels of the samples are\nkept private from all the parties except for the aggregating server, that is\nthe label owner. Nevertheless, recent works discovered that by exploiting\ngradient information returned by the server to bottom models, with the\nknowledge of only a small set of auxiliary labels on a very limited subset of\ntraining data points, an adversary can infer the private labels. These attacks\nare known as label inference attacks in VFL. In our work, we propose a novel\nframework called KDk, that combines Knowledge Distillation and k-anonymity to\nprovide a defense mechanism against potential label inference attacks in a VFL\nscenario. Through an exhaustive experimental campaign we demonstrate that by\napplying our approach, the performance of the analyzed label inference attacks\ndecreases consistently, even by more than 60%, maintaining the accuracy of the\nwhole VFL almost unaltered.\n", "link": "http://arxiv.org/abs/2404.12369v1", "date": "2024-04-18", "relevancy": 1.8992, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5271}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4646}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4641}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20KDk%3A%20A%20Defense%20Mechanism%20Against%20Label%20Inference%20Attacks%20in%20Vertical%0A%20%20Federated%20Learning&body=Title%3A%20KDk%3A%20A%20Defense%20Mechanism%20Against%20Label%20Inference%20Attacks%20in%20Vertical%0A%20%20Federated%20Learning%0AAuthor%3A%20Marco%20Arazzi%20and%20Serena%20Nicolazzo%20and%20Antonino%20Nocera%0AAbstract%3A%20%20%20Vertical%20Federated%20Learning%20%28VFL%29%20is%20a%20category%20of%20Federated%20Learning%20in%0Awhich%20models%20are%20trained%20collaboratively%20among%20parties%20with%20vertically%0Apartitioned%20data.%20Typically%2C%20in%20a%20VFL%20scenario%2C%20the%20labels%20of%20the%20samples%20are%0Akept%20private%20from%20all%20the%20parties%20except%20for%20the%20aggregating%20server%2C%20that%20is%0Athe%20label%20owner.%20Nevertheless%2C%20recent%20works%20discovered%20that%20by%20exploiting%0Agradient%20information%20returned%20by%20the%20server%20to%20bottom%20models%2C%20with%20the%0Aknowledge%20of%20only%20a%20small%20set%20of%20auxiliary%20labels%20on%20a%20very%20limited%20subset%20of%0Atraining%20data%20points%2C%20an%20adversary%20can%20infer%20the%20private%20labels.%20These%20attacks%0Aare%20known%20as%20label%20inference%20attacks%20in%20VFL.%20In%20our%20work%2C%20we%20propose%20a%20novel%0Aframework%20called%20KDk%2C%20that%20combines%20Knowledge%20Distillation%20and%20k-anonymity%20to%0Aprovide%20a%20defense%20mechanism%20against%20potential%20label%20inference%20attacks%20in%20a%20VFL%0Ascenario.%20Through%20an%20exhaustive%20experimental%20campaign%20we%20demonstrate%20that%20by%0Aapplying%20our%20approach%2C%20the%20performance%20of%20the%20analyzed%20label%20inference%20attacks%0Adecreases%20consistently%2C%20even%20by%20more%20than%2060%25%2C%20maintaining%20the%20accuracy%20of%20the%0Awhole%20VFL%20almost%20unaltered.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12369v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KDk%3A%20A%20Defense%20Mechanism%20Against%20Label%20Inference%20Attacks%20in%20Vertical%0A%20%20Federated%20Learning&entry.906535625=Marco%20Arazzi%20and%20Serena%20Nicolazzo%20and%20Antonino%20Nocera&entry.1292438233=%20%20Vertical%20Federated%20Learning%20%28VFL%29%20is%20a%20category%20of%20Federated%20Learning%20in%0Awhich%20models%20are%20trained%20collaboratively%20among%20parties%20with%20vertically%0Apartitioned%20data.%20Typically%2C%20in%20a%20VFL%20scenario%2C%20the%20labels%20of%20the%20samples%20are%0Akept%20private%20from%20all%20the%20parties%20except%20for%20the%20aggregating%20server%2C%20that%20is%0Athe%20label%20owner.%20Nevertheless%2C%20recent%20works%20discovered%20that%20by%20exploiting%0Agradient%20information%20returned%20by%20the%20server%20to%20bottom%20models%2C%20with%20the%0Aknowledge%20of%20only%20a%20small%20set%20of%20auxiliary%20labels%20on%20a%20very%20limited%20subset%20of%0Atraining%20data%20points%2C%20an%20adversary%20can%20infer%20the%20private%20labels.%20These%20attacks%0Aare%20known%20as%20label%20inference%20attacks%20in%20VFL.%20In%20our%20work%2C%20we%20propose%20a%20novel%0Aframework%20called%20KDk%2C%20that%20combines%20Knowledge%20Distillation%20and%20k-anonymity%20to%0Aprovide%20a%20defense%20mechanism%20against%20potential%20label%20inference%20attacks%20in%20a%20VFL%0Ascenario.%20Through%20an%20exhaustive%20experimental%20campaign%20we%20demonstrate%20that%20by%0Aapplying%20our%20approach%2C%20the%20performance%20of%20the%20analyzed%20label%20inference%20attacks%0Adecreases%20consistently%2C%20even%20by%20more%20than%2060%25%2C%20maintaining%20the%20accuracy%20of%20the%0Awhole%20VFL%20almost%20unaltered.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12369v1&entry.124074799=Read"},
{"title": "A Survey on the Densest Subgraph Problem and Its Variants", "author": "Tommaso Lanciano and Atsushi Miyauchi and Adriano Fazzone and Francesco Bonchi", "abstract": "  The Densest Subgraph Problem requires to find, in a given graph, a subset of\nvertices whose induced subgraph maximizes a measure of density. The problem has\nreceived a great deal of attention in the algorithmic literature since the\nearly 1970s, with many variants proposed and many applications built on top of\nthis basic definition. Recent years have witnessed a revival of research\ninterest in this problem with several important contributions, including some\ngroundbreaking results, published in 2022 and 2023. This survey provides a deep\noverview of the fundamental results and an exhaustive coverage of the many\nvariants proposed in the literature, with a special attention to the most\nrecent results. The survey also presents a comprehensive overview of\napplications and discusses some interesting open problems for this evergreen\nresearch topic.\n", "link": "http://arxiv.org/abs/2303.14467v2", "date": "2024-04-18", "relevancy": 1.8988, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.3936}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.3837}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.362}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20the%20Densest%20Subgraph%20Problem%20and%20Its%20Variants&body=Title%3A%20A%20Survey%20on%20the%20Densest%20Subgraph%20Problem%20and%20Its%20Variants%0AAuthor%3A%20Tommaso%20Lanciano%20and%20Atsushi%20Miyauchi%20and%20Adriano%20Fazzone%20and%20Francesco%20Bonchi%0AAbstract%3A%20%20%20The%20Densest%20Subgraph%20Problem%20requires%20to%20find%2C%20in%20a%20given%20graph%2C%20a%20subset%20of%0Avertices%20whose%20induced%20subgraph%20maximizes%20a%20measure%20of%20density.%20The%20problem%20has%0Areceived%20a%20great%20deal%20of%20attention%20in%20the%20algorithmic%20literature%20since%20the%0Aearly%201970s%2C%20with%20many%20variants%20proposed%20and%20many%20applications%20built%20on%20top%20of%0Athis%20basic%20definition.%20Recent%20years%20have%20witnessed%20a%20revival%20of%20research%0Ainterest%20in%20this%20problem%20with%20several%20important%20contributions%2C%20including%20some%0Agroundbreaking%20results%2C%20published%20in%202022%20and%202023.%20This%20survey%20provides%20a%20deep%0Aoverview%20of%20the%20fundamental%20results%20and%20an%20exhaustive%20coverage%20of%20the%20many%0Avariants%20proposed%20in%20the%20literature%2C%20with%20a%20special%20attention%20to%20the%20most%0Arecent%20results.%20The%20survey%20also%20presents%20a%20comprehensive%20overview%20of%0Aapplications%20and%20discusses%20some%20interesting%20open%20problems%20for%20this%20evergreen%0Aresearch%20topic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.14467v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20the%20Densest%20Subgraph%20Problem%20and%20Its%20Variants&entry.906535625=Tommaso%20Lanciano%20and%20Atsushi%20Miyauchi%20and%20Adriano%20Fazzone%20and%20Francesco%20Bonchi&entry.1292438233=%20%20The%20Densest%20Subgraph%20Problem%20requires%20to%20find%2C%20in%20a%20given%20graph%2C%20a%20subset%20of%0Avertices%20whose%20induced%20subgraph%20maximizes%20a%20measure%20of%20density.%20The%20problem%20has%0Areceived%20a%20great%20deal%20of%20attention%20in%20the%20algorithmic%20literature%20since%20the%0Aearly%201970s%2C%20with%20many%20variants%20proposed%20and%20many%20applications%20built%20on%20top%20of%0Athis%20basic%20definition.%20Recent%20years%20have%20witnessed%20a%20revival%20of%20research%0Ainterest%20in%20this%20problem%20with%20several%20important%20contributions%2C%20including%20some%0Agroundbreaking%20results%2C%20published%20in%202022%20and%202023.%20This%20survey%20provides%20a%20deep%0Aoverview%20of%20the%20fundamental%20results%20and%20an%20exhaustive%20coverage%20of%20the%20many%0Avariants%20proposed%20in%20the%20literature%2C%20with%20a%20special%20attention%20to%20the%20most%0Arecent%20results.%20The%20survey%20also%20presents%20a%20comprehensive%20overview%20of%0Aapplications%20and%20discusses%20some%20interesting%20open%20problems%20for%20this%20evergreen%0Aresearch%20topic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.14467v2&entry.124074799=Read"},
{"title": "A Time-Inhomogeneous Markov Model for Resource Availability under Sparse\n  Observations", "author": "Lukas Rottkamp and Matthias Schubert", "abstract": "  Accurate spatio-temporal information about the current situation is crucial\nfor smart city applications such as modern routing algorithms. Often, this\ninformation describes the state of stationary resources, e.g. the availability\nof parking bays, charging stations or the amount of people waiting for a\nvehicle to pick them up near a given location. To exploit this kind of\ninformation, predicting future states of the monitored resources is often\nmandatory because a resource might change its state within the time until it is\nneeded. To train an accurate predictive model, it is often not possible to\nobtain a continuous time series on the state of the resource. For example, the\ninformation might be collected from traveling agents visiting the resource with\nan irregular frequency. Thus, it is necessary to develop methods which work on\nsparse observations for training and prediction. In this paper, we propose\ntime-inhomogeneous discrete Markov models to allow accurate prediction even\nwhen the frequency of observation is very rare. Our new model is able to blend\nrecent observations with historic data and also provide useful probabilistic\nestimates for future states. Since resources availability in a city is\ntypically time-dependent, our Markov model is time-inhomogeneous and cyclic\nwithin a predefined time interval. To train our model, we propose a modified\nBaum-Welch algorithm. Evaluations on real-world datasets of parking bay\navailability show that our new method indeed yields good results compared to\nmethods being trained on complete data and non-cyclic variants.\n", "link": "http://arxiv.org/abs/2404.12240v1", "date": "2024-04-18", "relevancy": 1.8945, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5233}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4669}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4605}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Time-Inhomogeneous%20Markov%20Model%20for%20Resource%20Availability%20under%20Sparse%0A%20%20Observations&body=Title%3A%20A%20Time-Inhomogeneous%20Markov%20Model%20for%20Resource%20Availability%20under%20Sparse%0A%20%20Observations%0AAuthor%3A%20Lukas%20Rottkamp%20and%20Matthias%20Schubert%0AAbstract%3A%20%20%20Accurate%20spatio-temporal%20information%20about%20the%20current%20situation%20is%20crucial%0Afor%20smart%20city%20applications%20such%20as%20modern%20routing%20algorithms.%20Often%2C%20this%0Ainformation%20describes%20the%20state%20of%20stationary%20resources%2C%20e.g.%20the%20availability%0Aof%20parking%20bays%2C%20charging%20stations%20or%20the%20amount%20of%20people%20waiting%20for%20a%0Avehicle%20to%20pick%20them%20up%20near%20a%20given%20location.%20To%20exploit%20this%20kind%20of%0Ainformation%2C%20predicting%20future%20states%20of%20the%20monitored%20resources%20is%20often%0Amandatory%20because%20a%20resource%20might%20change%20its%20state%20within%20the%20time%20until%20it%20is%0Aneeded.%20To%20train%20an%20accurate%20predictive%20model%2C%20it%20is%20often%20not%20possible%20to%0Aobtain%20a%20continuous%20time%20series%20on%20the%20state%20of%20the%20resource.%20For%20example%2C%20the%0Ainformation%20might%20be%20collected%20from%20traveling%20agents%20visiting%20the%20resource%20with%0Aan%20irregular%20frequency.%20Thus%2C%20it%20is%20necessary%20to%20develop%20methods%20which%20work%20on%0Asparse%20observations%20for%20training%20and%20prediction.%20In%20this%20paper%2C%20we%20propose%0Atime-inhomogeneous%20discrete%20Markov%20models%20to%20allow%20accurate%20prediction%20even%0Awhen%20the%20frequency%20of%20observation%20is%20very%20rare.%20Our%20new%20model%20is%20able%20to%20blend%0Arecent%20observations%20with%20historic%20data%20and%20also%20provide%20useful%20probabilistic%0Aestimates%20for%20future%20states.%20Since%20resources%20availability%20in%20a%20city%20is%0Atypically%20time-dependent%2C%20our%20Markov%20model%20is%20time-inhomogeneous%20and%20cyclic%0Awithin%20a%20predefined%20time%20interval.%20To%20train%20our%20model%2C%20we%20propose%20a%20modified%0ABaum-Welch%20algorithm.%20Evaluations%20on%20real-world%20datasets%20of%20parking%20bay%0Aavailability%20show%20that%20our%20new%20method%20indeed%20yields%20good%20results%20compared%20to%0Amethods%20being%20trained%20on%20complete%20data%20and%20non-cyclic%20variants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12240v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Time-Inhomogeneous%20Markov%20Model%20for%20Resource%20Availability%20under%20Sparse%0A%20%20Observations&entry.906535625=Lukas%20Rottkamp%20and%20Matthias%20Schubert&entry.1292438233=%20%20Accurate%20spatio-temporal%20information%20about%20the%20current%20situation%20is%20crucial%0Afor%20smart%20city%20applications%20such%20as%20modern%20routing%20algorithms.%20Often%2C%20this%0Ainformation%20describes%20the%20state%20of%20stationary%20resources%2C%20e.g.%20the%20availability%0Aof%20parking%20bays%2C%20charging%20stations%20or%20the%20amount%20of%20people%20waiting%20for%20a%0Avehicle%20to%20pick%20them%20up%20near%20a%20given%20location.%20To%20exploit%20this%20kind%20of%0Ainformation%2C%20predicting%20future%20states%20of%20the%20monitored%20resources%20is%20often%0Amandatory%20because%20a%20resource%20might%20change%20its%20state%20within%20the%20time%20until%20it%20is%0Aneeded.%20To%20train%20an%20accurate%20predictive%20model%2C%20it%20is%20often%20not%20possible%20to%0Aobtain%20a%20continuous%20time%20series%20on%20the%20state%20of%20the%20resource.%20For%20example%2C%20the%0Ainformation%20might%20be%20collected%20from%20traveling%20agents%20visiting%20the%20resource%20with%0Aan%20irregular%20frequency.%20Thus%2C%20it%20is%20necessary%20to%20develop%20methods%20which%20work%20on%0Asparse%20observations%20for%20training%20and%20prediction.%20In%20this%20paper%2C%20we%20propose%0Atime-inhomogeneous%20discrete%20Markov%20models%20to%20allow%20accurate%20prediction%20even%0Awhen%20the%20frequency%20of%20observation%20is%20very%20rare.%20Our%20new%20model%20is%20able%20to%20blend%0Arecent%20observations%20with%20historic%20data%20and%20also%20provide%20useful%20probabilistic%0Aestimates%20for%20future%20states.%20Since%20resources%20availability%20in%20a%20city%20is%0Atypically%20time-dependent%2C%20our%20Markov%20model%20is%20time-inhomogeneous%20and%20cyclic%0Awithin%20a%20predefined%20time%20interval.%20To%20train%20our%20model%2C%20we%20propose%20a%20modified%0ABaum-Welch%20algorithm.%20Evaluations%20on%20real-world%20datasets%20of%20parking%20bay%0Aavailability%20show%20that%20our%20new%20method%20indeed%20yields%20good%20results%20compared%20to%0Amethods%20being%20trained%20on%20complete%20data%20and%20non-cyclic%20variants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12240v1&entry.124074799=Read"},
{"title": "Transformer tricks: Removing weights for skipless transformers", "author": "Nils Graef", "abstract": "  He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). See\narXiv:2402.13388 and https://github.com/OpenMachine-ai/transformer-tricks for\ncode and more transformer tricks.\n", "link": "http://arxiv.org/abs/2404.12362v1", "date": "2024-04-18", "relevancy": 1.8724, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5008}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4915}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4316}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Transformer%20tricks%3A%20Removing%20weights%20for%20skipless%20transformers&body=Title%3A%20Transformer%20tricks%3A%20Removing%20weights%20for%20skipless%20transformers%0AAuthor%3A%20Nils%20Graef%0AAbstract%3A%20%20%20He%20and%20Hofmann%20%28arXiv%3A2311.01906%29%20detailed%20a%20skipless%20transformer%20without%20the%0AV%20and%20P%20%28post-attention%20projection%29%20linear%20layers%2C%20which%20reduces%20the%20total%0Anumber%20of%20weights.%20However%2C%20this%20scheme%20is%20only%20applicable%20to%20MHA%20%28multi-head%0Aattention%29%2C%20but%20not%20for%20MQA%20%28multi-query%20attention%29%20and%20GQA%20%28grouped-query%0Aattention%29.%20The%20latter%20schemes%20are%20used%20by%20many%20popular%20LLMs%20such%20as%20Llama%202%2C%0AMistral%2C%20Mixtral%2C%20PaLM%2C%20and%20Gemma.%20Therefore%2C%20this%20micro-paper%20proposes%0Amathematically%20equivalent%20versions%20that%20are%20suitable%20for%20MQA%20and%20GQA.%20For%0Aexample%2C%20removing%20Q%20and%20P%20from%20a%20skipless%20version%20of%20Mistral-7B%20would%20remove%0A15%25%20of%20its%20weights%20%28and%20thus%20reduce%20its%20compute%20and%20memory%20complexity%29.%20See%0AarXiv%3A2402.13388%20and%20https%3A//github.com/OpenMachine-ai/transformer-tricks%20for%0Acode%20and%20more%20transformer%20tricks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12362v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformer%20tricks%3A%20Removing%20weights%20for%20skipless%20transformers&entry.906535625=Nils%20Graef&entry.1292438233=%20%20He%20and%20Hofmann%20%28arXiv%3A2311.01906%29%20detailed%20a%20skipless%20transformer%20without%20the%0AV%20and%20P%20%28post-attention%20projection%29%20linear%20layers%2C%20which%20reduces%20the%20total%0Anumber%20of%20weights.%20However%2C%20this%20scheme%20is%20only%20applicable%20to%20MHA%20%28multi-head%0Aattention%29%2C%20but%20not%20for%20MQA%20%28multi-query%20attention%29%20and%20GQA%20%28grouped-query%0Aattention%29.%20The%20latter%20schemes%20are%20used%20by%20many%20popular%20LLMs%20such%20as%20Llama%202%2C%0AMistral%2C%20Mixtral%2C%20PaLM%2C%20and%20Gemma.%20Therefore%2C%20this%20micro-paper%20proposes%0Amathematically%20equivalent%20versions%20that%20are%20suitable%20for%20MQA%20and%20GQA.%20For%0Aexample%2C%20removing%20Q%20and%20P%20from%20a%20skipless%20version%20of%20Mistral-7B%20would%20remove%0A15%25%20of%20its%20weights%20%28and%20thus%20reduce%20its%20compute%20and%20memory%20complexity%29.%20See%0AarXiv%3A2402.13388%20and%20https%3A//github.com/OpenMachine-ai/transformer-tricks%20for%0Acode%20and%20more%20transformer%20tricks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12362v1&entry.124074799=Read"},
{"title": "RoboDreamer: Learning Compositional World Models for Robot Imagination", "author": "Siyuan Zhou and Yilun Du and Jiaben Chen and Yandong Li and Dit-Yan Yeung and Chuang Gan", "abstract": "  Text-to-video models have demonstrated substantial potential in robotic\ndecision-making, enabling the imagination of realistic plans of future actions\nas well as accurate environment simulation. However, one major issue in such\nmodels is generalization -- models are limited to synthesizing videos subject\nto language instructions similar to those seen at training time. This is\nheavily limiting in decision-making, where we seek a powerful world model to\nsynthesize plans of unseen combinations of objects and actions in order to\nsolve previously unseen tasks in new environments. To resolve this issue, we\nintroduce RoboDreamer, an innovative approach for learning a compositional\nworld model by factorizing the video generation. We leverage the natural\ncompositionality of language to parse instructions into a set of lower-level\nprimitives, which we condition a set of models on to generate videos. We\nillustrate how this factorization naturally enables compositional\ngeneralization, by allowing us to formulate a new natural language instruction\nas a combination of previously seen components. We further show how such a\nfactorization enables us to add additional multimodal goals, allowing us to\nspecify a video we wish to generate given both natural language instructions\nand a goal image. Our approach can successfully synthesize video plans on\nunseen goals in the RT-X, enables successful robot execution in simulation, and\nsubstantially outperforms monolithic baseline approaches to video generation.\n", "link": "http://arxiv.org/abs/2404.12377v1", "date": "2024-04-18", "relevancy": 1.8608, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6421}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6295}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5564}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RoboDreamer%3A%20Learning%20Compositional%20World%20Models%20for%20Robot%20Imagination&body=Title%3A%20RoboDreamer%3A%20Learning%20Compositional%20World%20Models%20for%20Robot%20Imagination%0AAuthor%3A%20Siyuan%20Zhou%20and%20Yilun%20Du%20and%20Jiaben%20Chen%20and%20Yandong%20Li%20and%20Dit-Yan%20Yeung%20and%20Chuang%20Gan%0AAbstract%3A%20%20%20Text-to-video%20models%20have%20demonstrated%20substantial%20potential%20in%20robotic%0Adecision-making%2C%20enabling%20the%20imagination%20of%20realistic%20plans%20of%20future%20actions%0Aas%20well%20as%20accurate%20environment%20simulation.%20However%2C%20one%20major%20issue%20in%20such%0Amodels%20is%20generalization%20--%20models%20are%20limited%20to%20synthesizing%20videos%20subject%0Ato%20language%20instructions%20similar%20to%20those%20seen%20at%20training%20time.%20This%20is%0Aheavily%20limiting%20in%20decision-making%2C%20where%20we%20seek%20a%20powerful%20world%20model%20to%0Asynthesize%20plans%20of%20unseen%20combinations%20of%20objects%20and%20actions%20in%20order%20to%0Asolve%20previously%20unseen%20tasks%20in%20new%20environments.%20To%20resolve%20this%20issue%2C%20we%0Aintroduce%20RoboDreamer%2C%20an%20innovative%20approach%20for%20learning%20a%20compositional%0Aworld%20model%20by%20factorizing%20the%20video%20generation.%20We%20leverage%20the%20natural%0Acompositionality%20of%20language%20to%20parse%20instructions%20into%20a%20set%20of%20lower-level%0Aprimitives%2C%20which%20we%20condition%20a%20set%20of%20models%20on%20to%20generate%20videos.%20We%0Aillustrate%20how%20this%20factorization%20naturally%20enables%20compositional%0Ageneralization%2C%20by%20allowing%20us%20to%20formulate%20a%20new%20natural%20language%20instruction%0Aas%20a%20combination%20of%20previously%20seen%20components.%20We%20further%20show%20how%20such%20a%0Afactorization%20enables%20us%20to%20add%20additional%20multimodal%20goals%2C%20allowing%20us%20to%0Aspecify%20a%20video%20we%20wish%20to%20generate%20given%20both%20natural%20language%20instructions%0Aand%20a%20goal%20image.%20Our%20approach%20can%20successfully%20synthesize%20video%20plans%20on%0Aunseen%20goals%20in%20the%20RT-X%2C%20enables%20successful%20robot%20execution%20in%20simulation%2C%20and%0Asubstantially%20outperforms%20monolithic%20baseline%20approaches%20to%20video%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12377v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RoboDreamer%3A%20Learning%20Compositional%20World%20Models%20for%20Robot%20Imagination&entry.906535625=Siyuan%20Zhou%20and%20Yilun%20Du%20and%20Jiaben%20Chen%20and%20Yandong%20Li%20and%20Dit-Yan%20Yeung%20and%20Chuang%20Gan&entry.1292438233=%20%20Text-to-video%20models%20have%20demonstrated%20substantial%20potential%20in%20robotic%0Adecision-making%2C%20enabling%20the%20imagination%20of%20realistic%20plans%20of%20future%20actions%0Aas%20well%20as%20accurate%20environment%20simulation.%20However%2C%20one%20major%20issue%20in%20such%0Amodels%20is%20generalization%20--%20models%20are%20limited%20to%20synthesizing%20videos%20subject%0Ato%20language%20instructions%20similar%20to%20those%20seen%20at%20training%20time.%20This%20is%0Aheavily%20limiting%20in%20decision-making%2C%20where%20we%20seek%20a%20powerful%20world%20model%20to%0Asynthesize%20plans%20of%20unseen%20combinations%20of%20objects%20and%20actions%20in%20order%20to%0Asolve%20previously%20unseen%20tasks%20in%20new%20environments.%20To%20resolve%20this%20issue%2C%20we%0Aintroduce%20RoboDreamer%2C%20an%20innovative%20approach%20for%20learning%20a%20compositional%0Aworld%20model%20by%20factorizing%20the%20video%20generation.%20We%20leverage%20the%20natural%0Acompositionality%20of%20language%20to%20parse%20instructions%20into%20a%20set%20of%20lower-level%0Aprimitives%2C%20which%20we%20condition%20a%20set%20of%20models%20on%20to%20generate%20videos.%20We%0Aillustrate%20how%20this%20factorization%20naturally%20enables%20compositional%0Ageneralization%2C%20by%20allowing%20us%20to%20formulate%20a%20new%20natural%20language%20instruction%0Aas%20a%20combination%20of%20previously%20seen%20components.%20We%20further%20show%20how%20such%20a%0Afactorization%20enables%20us%20to%20add%20additional%20multimodal%20goals%2C%20allowing%20us%20to%0Aspecify%20a%20video%20we%20wish%20to%20generate%20given%20both%20natural%20language%20instructions%0Aand%20a%20goal%20image.%20Our%20approach%20can%20successfully%20synthesize%20video%20plans%20on%0Aunseen%20goals%20in%20the%20RT-X%2C%20enables%20successful%20robot%20execution%20in%20simulation%2C%20and%0Asubstantially%20outperforms%20monolithic%20baseline%20approaches%20to%20video%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12377v1&entry.124074799=Read"},
{"title": "Visualization for Trust in Machine Learning Revisited: The State of the\n  Field in 2023", "author": "Angelos Chatzimparmpas and Kostiantyn Kucher and Andreas Kerren", "abstract": "  Visualization for explainable and trustworthy machine learning remains one of\nthe most important and heavily researched fields within information\nvisualization and visual analytics with various application domains, such as\nmedicine, finance, and bioinformatics. After our 2020 state-of-the-art report\ncomprising 200 techniques, we have persistently collected peer-reviewed\narticles describing visualization techniques, categorized them based on the\npreviously established categorization schema consisting of 119 categories, and\nprovided the resulting collection of 542 techniques in an online survey\nbrowser. In this survey article, we present the updated findings of new\nanalyses of this dataset as of fall 2023 and discuss trends, insights, and\neight open challenges for using visualizations in machine learning. Our results\ncorroborate the rapidly growing trend of visualization techniques for\nincreasing trust in machine learning models in the past three years, with\nvisualization found to help improve popular model explainability methods and\ncheck new deep learning architectures, for instance.\n", "link": "http://arxiv.org/abs/2403.12005v2", "date": "2024-04-18", "relevancy": 1.8576, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4949}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4695}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4471}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Visualization%20for%20Trust%20in%20Machine%20Learning%20Revisited%3A%20The%20State%20of%20the%0A%20%20Field%20in%202023&body=Title%3A%20Visualization%20for%20Trust%20in%20Machine%20Learning%20Revisited%3A%20The%20State%20of%20the%0A%20%20Field%20in%202023%0AAuthor%3A%20Angelos%20Chatzimparmpas%20and%20Kostiantyn%20Kucher%20and%20Andreas%20Kerren%0AAbstract%3A%20%20%20Visualization%20for%20explainable%20and%20trustworthy%20machine%20learning%20remains%20one%20of%0Athe%20most%20important%20and%20heavily%20researched%20fields%20within%20information%0Avisualization%20and%20visual%20analytics%20with%20various%20application%20domains%2C%20such%20as%0Amedicine%2C%20finance%2C%20and%20bioinformatics.%20After%20our%202020%20state-of-the-art%20report%0Acomprising%20200%20techniques%2C%20we%20have%20persistently%20collected%20peer-reviewed%0Aarticles%20describing%20visualization%20techniques%2C%20categorized%20them%20based%20on%20the%0Apreviously%20established%20categorization%20schema%20consisting%20of%20119%20categories%2C%20and%0Aprovided%20the%20resulting%20collection%20of%20542%20techniques%20in%20an%20online%20survey%0Abrowser.%20In%20this%20survey%20article%2C%20we%20present%20the%20updated%20findings%20of%20new%0Aanalyses%20of%20this%20dataset%20as%20of%20fall%202023%20and%20discuss%20trends%2C%20insights%2C%20and%0Aeight%20open%20challenges%20for%20using%20visualizations%20in%20machine%20learning.%20Our%20results%0Acorroborate%20the%20rapidly%20growing%20trend%20of%20visualization%20techniques%20for%0Aincreasing%20trust%20in%20machine%20learning%20models%20in%20the%20past%20three%20years%2C%20with%0Avisualization%20found%20to%20help%20improve%20popular%20model%20explainability%20methods%20and%0Acheck%20new%20deep%20learning%20architectures%2C%20for%20instance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12005v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visualization%20for%20Trust%20in%20Machine%20Learning%20Revisited%3A%20The%20State%20of%20the%0A%20%20Field%20in%202023&entry.906535625=Angelos%20Chatzimparmpas%20and%20Kostiantyn%20Kucher%20and%20Andreas%20Kerren&entry.1292438233=%20%20Visualization%20for%20explainable%20and%20trustworthy%20machine%20learning%20remains%20one%20of%0Athe%20most%20important%20and%20heavily%20researched%20fields%20within%20information%0Avisualization%20and%20visual%20analytics%20with%20various%20application%20domains%2C%20such%20as%0Amedicine%2C%20finance%2C%20and%20bioinformatics.%20After%20our%202020%20state-of-the-art%20report%0Acomprising%20200%20techniques%2C%20we%20have%20persistently%20collected%20peer-reviewed%0Aarticles%20describing%20visualization%20techniques%2C%20categorized%20them%20based%20on%20the%0Apreviously%20established%20categorization%20schema%20consisting%20of%20119%20categories%2C%20and%0Aprovided%20the%20resulting%20collection%20of%20542%20techniques%20in%20an%20online%20survey%0Abrowser.%20In%20this%20survey%20article%2C%20we%20present%20the%20updated%20findings%20of%20new%0Aanalyses%20of%20this%20dataset%20as%20of%20fall%202023%20and%20discuss%20trends%2C%20insights%2C%20and%0Aeight%20open%20challenges%20for%20using%20visualizations%20in%20machine%20learning.%20Our%20results%0Acorroborate%20the%20rapidly%20growing%20trend%20of%20visualization%20techniques%20for%0Aincreasing%20trust%20in%20machine%20learning%20models%20in%20the%20past%20three%20years%2C%20with%0Avisualization%20found%20to%20help%20improve%20popular%20model%20explainability%20methods%20and%0Acheck%20new%20deep%20learning%20architectures%2C%20for%20instance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12005v2&entry.124074799=Read"},
{"title": "Transferability Ranking of Adversarial Examples", "author": "Mosh Levy and Guy Amit and Yuval Elovici and Yisroel Mirsky", "abstract": "  Adversarial transferability in black-box scenarios presents a unique\nchallenge: while attackers can employ surrogate models to craft adversarial\nexamples, they lack assurance on whether these examples will successfully\ncompromise the target model. Until now, the prevalent method to ascertain\nsuccess has been trial and error-testing crafted samples directly on the victim\nmodel. This approach, however, risks detection with every attempt, forcing\nattackers to either perfect their first try or face exposure. Our paper\nintroduces a ranking strategy that refines the transfer attack process,\nenabling the attacker to estimate the likelihood of success without repeated\ntrials on the victim's system. By leveraging a set of diverse surrogate models,\nour method can predict transferability of adversarial examples. This strategy\ncan be used to either select the best sample to use in an attack or the best\nperturbation to apply to a specific sample. Using our strategy, we were able to\nraise the transferability of adversarial examples from a mere 20% - akin to\nrandom selection-up to near upper-bound levels, with some scenarios even\nwitnessing a 100% success rate. This substantial improvement not only sheds\nlight on the shared susceptibilities across diverse architectures but also\ndemonstrates that attackers can forego the detectable trial-and-error tactics\nraising increasing the threat of surrogate-based attacks.\n", "link": "http://arxiv.org/abs/2208.10878v2", "date": "2024-04-18", "relevancy": 1.8538, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4865}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4496}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4405}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Transferability%20Ranking%20of%20Adversarial%20Examples&body=Title%3A%20Transferability%20Ranking%20of%20Adversarial%20Examples%0AAuthor%3A%20Mosh%20Levy%20and%20Guy%20Amit%20and%20Yuval%20Elovici%20and%20Yisroel%20Mirsky%0AAbstract%3A%20%20%20Adversarial%20transferability%20in%20black-box%20scenarios%20presents%20a%20unique%0Achallenge%3A%20while%20attackers%20can%20employ%20surrogate%20models%20to%20craft%20adversarial%0Aexamples%2C%20they%20lack%20assurance%20on%20whether%20these%20examples%20will%20successfully%0Acompromise%20the%20target%20model.%20Until%20now%2C%20the%20prevalent%20method%20to%20ascertain%0Asuccess%20has%20been%20trial%20and%20error-testing%20crafted%20samples%20directly%20on%20the%20victim%0Amodel.%20This%20approach%2C%20however%2C%20risks%20detection%20with%20every%20attempt%2C%20forcing%0Aattackers%20to%20either%20perfect%20their%20first%20try%20or%20face%20exposure.%20Our%20paper%0Aintroduces%20a%20ranking%20strategy%20that%20refines%20the%20transfer%20attack%20process%2C%0Aenabling%20the%20attacker%20to%20estimate%20the%20likelihood%20of%20success%20without%20repeated%0Atrials%20on%20the%20victim%27s%20system.%20By%20leveraging%20a%20set%20of%20diverse%20surrogate%20models%2C%0Aour%20method%20can%20predict%20transferability%20of%20adversarial%20examples.%20This%20strategy%0Acan%20be%20used%20to%20either%20select%20the%20best%20sample%20to%20use%20in%20an%20attack%20or%20the%20best%0Aperturbation%20to%20apply%20to%20a%20specific%20sample.%20Using%20our%20strategy%2C%20we%20were%20able%20to%0Araise%20the%20transferability%20of%20adversarial%20examples%20from%20a%20mere%2020%25%20-%20akin%20to%0Arandom%20selection-up%20to%20near%20upper-bound%20levels%2C%20with%20some%20scenarios%20even%0Awitnessing%20a%20100%25%20success%20rate.%20This%20substantial%20improvement%20not%20only%20sheds%0Alight%20on%20the%20shared%20susceptibilities%20across%20diverse%20architectures%20but%20also%0Ademonstrates%20that%20attackers%20can%20forego%20the%20detectable%20trial-and-error%20tactics%0Araising%20increasing%20the%20threat%20of%20surrogate-based%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.10878v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transferability%20Ranking%20of%20Adversarial%20Examples&entry.906535625=Mosh%20Levy%20and%20Guy%20Amit%20and%20Yuval%20Elovici%20and%20Yisroel%20Mirsky&entry.1292438233=%20%20Adversarial%20transferability%20in%20black-box%20scenarios%20presents%20a%20unique%0Achallenge%3A%20while%20attackers%20can%20employ%20surrogate%20models%20to%20craft%20adversarial%0Aexamples%2C%20they%20lack%20assurance%20on%20whether%20these%20examples%20will%20successfully%0Acompromise%20the%20target%20model.%20Until%20now%2C%20the%20prevalent%20method%20to%20ascertain%0Asuccess%20has%20been%20trial%20and%20error-testing%20crafted%20samples%20directly%20on%20the%20victim%0Amodel.%20This%20approach%2C%20however%2C%20risks%20detection%20with%20every%20attempt%2C%20forcing%0Aattackers%20to%20either%20perfect%20their%20first%20try%20or%20face%20exposure.%20Our%20paper%0Aintroduces%20a%20ranking%20strategy%20that%20refines%20the%20transfer%20attack%20process%2C%0Aenabling%20the%20attacker%20to%20estimate%20the%20likelihood%20of%20success%20without%20repeated%0Atrials%20on%20the%20victim%27s%20system.%20By%20leveraging%20a%20set%20of%20diverse%20surrogate%20models%2C%0Aour%20method%20can%20predict%20transferability%20of%20adversarial%20examples.%20This%20strategy%0Acan%20be%20used%20to%20either%20select%20the%20best%20sample%20to%20use%20in%20an%20attack%20or%20the%20best%0Aperturbation%20to%20apply%20to%20a%20specific%20sample.%20Using%20our%20strategy%2C%20we%20were%20able%20to%0Araise%20the%20transferability%20of%20adversarial%20examples%20from%20a%20mere%2020%25%20-%20akin%20to%0Arandom%20selection-up%20to%20near%20upper-bound%20levels%2C%20with%20some%20scenarios%20even%0Awitnessing%20a%20100%25%20success%20rate.%20This%20substantial%20improvement%20not%20only%20sheds%0Alight%20on%20the%20shared%20susceptibilities%20across%20diverse%20architectures%20but%20also%0Ademonstrates%20that%20attackers%20can%20forego%20the%20detectable%20trial-and-error%20tactics%0Araising%20increasing%20the%20threat%20of%20surrogate-based%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.10878v2&entry.124074799=Read"},
{"title": "Low-rank tensor completion via tensor joint rank with logarithmic\n  composite norm", "author": "Hongbing Zhang", "abstract": "  Low-rank tensor completion (LRTC) aims to recover a complete low-rank tensor\nfrom incomplete observed tensor, attracting extensive attention in various\npractical applications such as image processing and computer vision. However,\ncurrent methods often perform well only when there is a sufficient of observed\ninformation, and they perform poorly or may fail when the observed information\nis less than 5\\%. In order to improve the utilization of observed information,\na new method called the tensor joint rank with logarithmic composite norm\n(TJLC) method is proposed. This method simultaneously exploits two types of\ntensor low-rank structures, namely tensor Tucker rank and tubal rank, thereby\nenhancing the inherent correlations between known and missing elements. To\naddress the challenge of applying two tensor ranks with significantly different\ndirectly to LRTC, a new tensor Logarithmic composite norm is further proposed.\nSubsequently, the TJLC model and algorithm for the LRTC problem are proposed.\nAdditionally, theoretical convergence guarantees for the TJLC method are\nprovided. Experiments on various real datasets demonstrate that the proposed\nmethod outperforms state-of-the-art methods significantly. Particularly, the\nproposed method achieves satisfactory recovery even when the observed\ninformation is as low as 1\\%, and the recovery performance improves\nsignificantly as the observed information increases.\n", "link": "http://arxiv.org/abs/2309.16208v2", "date": "2024-04-18", "relevancy": 1.8474, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4656}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4632}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.459}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Low-rank%20tensor%20completion%20via%20tensor%20joint%20rank%20with%20logarithmic%0A%20%20composite%20norm&body=Title%3A%20Low-rank%20tensor%20completion%20via%20tensor%20joint%20rank%20with%20logarithmic%0A%20%20composite%20norm%0AAuthor%3A%20Hongbing%20Zhang%0AAbstract%3A%20%20%20Low-rank%20tensor%20completion%20%28LRTC%29%20aims%20to%20recover%20a%20complete%20low-rank%20tensor%0Afrom%20incomplete%20observed%20tensor%2C%20attracting%20extensive%20attention%20in%20various%0Apractical%20applications%20such%20as%20image%20processing%20and%20computer%20vision.%20However%2C%0Acurrent%20methods%20often%20perform%20well%20only%20when%20there%20is%20a%20sufficient%20of%20observed%0Ainformation%2C%20and%20they%20perform%20poorly%20or%20may%20fail%20when%20the%20observed%20information%0Ais%20less%20than%205%5C%25.%20In%20order%20to%20improve%20the%20utilization%20of%20observed%20information%2C%0Aa%20new%20method%20called%20the%20tensor%20joint%20rank%20with%20logarithmic%20composite%20norm%0A%28TJLC%29%20method%20is%20proposed.%20This%20method%20simultaneously%20exploits%20two%20types%20of%0Atensor%20low-rank%20structures%2C%20namely%20tensor%20Tucker%20rank%20and%20tubal%20rank%2C%20thereby%0Aenhancing%20the%20inherent%20correlations%20between%20known%20and%20missing%20elements.%20To%0Aaddress%20the%20challenge%20of%20applying%20two%20tensor%20ranks%20with%20significantly%20different%0Adirectly%20to%20LRTC%2C%20a%20new%20tensor%20Logarithmic%20composite%20norm%20is%20further%20proposed.%0ASubsequently%2C%20the%20TJLC%20model%20and%20algorithm%20for%20the%20LRTC%20problem%20are%20proposed.%0AAdditionally%2C%20theoretical%20convergence%20guarantees%20for%20the%20TJLC%20method%20are%0Aprovided.%20Experiments%20on%20various%20real%20datasets%20demonstrate%20that%20the%20proposed%0Amethod%20outperforms%20state-of-the-art%20methods%20significantly.%20Particularly%2C%20the%0Aproposed%20method%20achieves%20satisfactory%20recovery%20even%20when%20the%20observed%0Ainformation%20is%20as%20low%20as%201%5C%25%2C%20and%20the%20recovery%20performance%20improves%0Asignificantly%20as%20the%20observed%20information%20increases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.16208v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-rank%20tensor%20completion%20via%20tensor%20joint%20rank%20with%20logarithmic%0A%20%20composite%20norm&entry.906535625=Hongbing%20Zhang&entry.1292438233=%20%20Low-rank%20tensor%20completion%20%28LRTC%29%20aims%20to%20recover%20a%20complete%20low-rank%20tensor%0Afrom%20incomplete%20observed%20tensor%2C%20attracting%20extensive%20attention%20in%20various%0Apractical%20applications%20such%20as%20image%20processing%20and%20computer%20vision.%20However%2C%0Acurrent%20methods%20often%20perform%20well%20only%20when%20there%20is%20a%20sufficient%20of%20observed%0Ainformation%2C%20and%20they%20perform%20poorly%20or%20may%20fail%20when%20the%20observed%20information%0Ais%20less%20than%205%5C%25.%20In%20order%20to%20improve%20the%20utilization%20of%20observed%20information%2C%0Aa%20new%20method%20called%20the%20tensor%20joint%20rank%20with%20logarithmic%20composite%20norm%0A%28TJLC%29%20method%20is%20proposed.%20This%20method%20simultaneously%20exploits%20two%20types%20of%0Atensor%20low-rank%20structures%2C%20namely%20tensor%20Tucker%20rank%20and%20tubal%20rank%2C%20thereby%0Aenhancing%20the%20inherent%20correlations%20between%20known%20and%20missing%20elements.%20To%0Aaddress%20the%20challenge%20of%20applying%20two%20tensor%20ranks%20with%20significantly%20different%0Adirectly%20to%20LRTC%2C%20a%20new%20tensor%20Logarithmic%20composite%20norm%20is%20further%20proposed.%0ASubsequently%2C%20the%20TJLC%20model%20and%20algorithm%20for%20the%20LRTC%20problem%20are%20proposed.%0AAdditionally%2C%20theoretical%20convergence%20guarantees%20for%20the%20TJLC%20method%20are%0Aprovided.%20Experiments%20on%20various%20real%20datasets%20demonstrate%20that%20the%20proposed%0Amethod%20outperforms%20state-of-the-art%20methods%20significantly.%20Particularly%2C%20the%0Aproposed%20method%20achieves%20satisfactory%20recovery%20even%20when%20the%20observed%0Ainformation%20is%20as%20low%20as%201%5C%25%2C%20and%20the%20recovery%20performance%20improves%0Asignificantly%20as%20the%20observed%20information%20increases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.16208v2&entry.124074799=Read"},
{"title": "Singular-limit analysis of gradient descent with noise injection", "author": "Anna Shalova and Andr\u00e9 Schlichting and Mark Peletier", "abstract": "  We study the limiting dynamics of a large class of noisy gradient descent\nsystems in the overparameterized regime. In this regime the set of global\nminimizers of the loss is large, and when initialized in a neighbourhood of\nthis zero-loss set a noisy gradient descent algorithm slowly evolves along this\nset. In some cases this slow evolution has been related to better\ngeneralisation properties. We characterize this evolution for the broad class\nof noisy gradient descent systems in the limit of small step size. Our results\nshow that the structure of the noise affects not just the form of the limiting\nprocess, but also the time scale at which the evolution takes place. We apply\nthe theory to Dropout, label noise and classical SGD (minibatching) noise, and\nshow that these evolve on different two time scales. Classical SGD even yields\na trivial evolution on both time scales, implying that additional noise is\nrequired for regularization. The results are inspired by the training of neural\nnetworks, but the theorems apply to noisy gradient descent of any loss that has\na non-trivial zero-loss set.\n", "link": "http://arxiv.org/abs/2404.12293v1", "date": "2024-04-18", "relevancy": 1.8303, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4615}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4599}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4527}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Singular-limit%20analysis%20of%20gradient%20descent%20with%20noise%20injection&body=Title%3A%20Singular-limit%20analysis%20of%20gradient%20descent%20with%20noise%20injection%0AAuthor%3A%20Anna%20Shalova%20and%20Andr%C3%A9%20Schlichting%20and%20Mark%20Peletier%0AAbstract%3A%20%20%20We%20study%20the%20limiting%20dynamics%20of%20a%20large%20class%20of%20noisy%20gradient%20descent%0Asystems%20in%20the%20overparameterized%20regime.%20In%20this%20regime%20the%20set%20of%20global%0Aminimizers%20of%20the%20loss%20is%20large%2C%20and%20when%20initialized%20in%20a%20neighbourhood%20of%0Athis%20zero-loss%20set%20a%20noisy%20gradient%20descent%20algorithm%20slowly%20evolves%20along%20this%0Aset.%20In%20some%20cases%20this%20slow%20evolution%20has%20been%20related%20to%20better%0Ageneralisation%20properties.%20We%20characterize%20this%20evolution%20for%20the%20broad%20class%0Aof%20noisy%20gradient%20descent%20systems%20in%20the%20limit%20of%20small%20step%20size.%20Our%20results%0Ashow%20that%20the%20structure%20of%20the%20noise%20affects%20not%20just%20the%20form%20of%20the%20limiting%0Aprocess%2C%20but%20also%20the%20time%20scale%20at%20which%20the%20evolution%20takes%20place.%20We%20apply%0Athe%20theory%20to%20Dropout%2C%20label%20noise%20and%20classical%20SGD%20%28minibatching%29%20noise%2C%20and%0Ashow%20that%20these%20evolve%20on%20different%20two%20time%20scales.%20Classical%20SGD%20even%20yields%0Aa%20trivial%20evolution%20on%20both%20time%20scales%2C%20implying%20that%20additional%20noise%20is%0Arequired%20for%20regularization.%20The%20results%20are%20inspired%20by%20the%20training%20of%20neural%0Anetworks%2C%20but%20the%20theorems%20apply%20to%20noisy%20gradient%20descent%20of%20any%20loss%20that%20has%0Aa%20non-trivial%20zero-loss%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12293v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Singular-limit%20analysis%20of%20gradient%20descent%20with%20noise%20injection&entry.906535625=Anna%20Shalova%20and%20Andr%C3%A9%20Schlichting%20and%20Mark%20Peletier&entry.1292438233=%20%20We%20study%20the%20limiting%20dynamics%20of%20a%20large%20class%20of%20noisy%20gradient%20descent%0Asystems%20in%20the%20overparameterized%20regime.%20In%20this%20regime%20the%20set%20of%20global%0Aminimizers%20of%20the%20loss%20is%20large%2C%20and%20when%20initialized%20in%20a%20neighbourhood%20of%0Athis%20zero-loss%20set%20a%20noisy%20gradient%20descent%20algorithm%20slowly%20evolves%20along%20this%0Aset.%20In%20some%20cases%20this%20slow%20evolution%20has%20been%20related%20to%20better%0Ageneralisation%20properties.%20We%20characterize%20this%20evolution%20for%20the%20broad%20class%0Aof%20noisy%20gradient%20descent%20systems%20in%20the%20limit%20of%20small%20step%20size.%20Our%20results%0Ashow%20that%20the%20structure%20of%20the%20noise%20affects%20not%20just%20the%20form%20of%20the%20limiting%0Aprocess%2C%20but%20also%20the%20time%20scale%20at%20which%20the%20evolution%20takes%20place.%20We%20apply%0Athe%20theory%20to%20Dropout%2C%20label%20noise%20and%20classical%20SGD%20%28minibatching%29%20noise%2C%20and%0Ashow%20that%20these%20evolve%20on%20different%20two%20time%20scales.%20Classical%20SGD%20even%20yields%0Aa%20trivial%20evolution%20on%20both%20time%20scales%2C%20implying%20that%20additional%20noise%20is%0Arequired%20for%20regularization.%20The%20results%20are%20inspired%20by%20the%20training%20of%20neural%0Anetworks%2C%20but%20the%20theorems%20apply%20to%20noisy%20gradient%20descent%20of%20any%20loss%20that%20has%0Aa%20non-trivial%20zero-loss%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12293v1&entry.124074799=Read"},
{"title": "Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM\n  Outputs with Human Preferences", "author": "Shreya Shankar and J. D. Zamfirescu-Pereira and Bj\u00f6rn Hartmann and Aditya G. Parameswaran and Ian Arawjo", "abstract": "  Due to the cumbersome nature of human evaluation and limitations of\ncode-based evaluation, Large Language Models (LLMs) are increasingly being used\nto assist humans in evaluating LLM outputs. Yet LLM-generated evaluators simply\ninherit all the problems of the LLMs they evaluate, requiring further human\nvalidation. We present a mixed-initiative approach to ``validate the\nvalidators'' -- aligning LLM-generated evaluation functions (be it prompts or\ncode) with human requirements. Our interface, EvalGen, provides automated\nassistance to users in generating evaluation criteria and implementing\nassertions. While generating candidate implementations (Python functions, LLM\ngrader prompts), EvalGen asks humans to grade a subset of LLM outputs; this\nfeedback is used to select implementations that better align with user grades.\nA qualitative study finds overall support for EvalGen but underscores the\nsubjectivity and iterative process of alignment. In particular, we identify a\nphenomenon we dub \\emph{criteria drift}: users need criteria to grade outputs,\nbut grading outputs helps users define criteria. What is more, some criteria\nappears \\emph{dependent} on the specific LLM outputs observed (rather than\nindependent criteria that can be defined \\emph{a priori}), raising serious\nquestions for approaches that assume the independence of evaluation from\nobservation of model outputs. We present our interface and implementation\ndetails, a comparison of our algorithm with a baseline approach, and\nimplications for the design of future LLM evaluation assistants.\n", "link": "http://arxiv.org/abs/2404.12272v1", "date": "2024-04-18", "relevancy": 1.8185, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5074}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4605}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4277}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Who%20Validates%20the%20Validators%3F%20Aligning%20LLM-Assisted%20Evaluation%20of%20LLM%0A%20%20Outputs%20with%20Human%20Preferences&body=Title%3A%20Who%20Validates%20the%20Validators%3F%20Aligning%20LLM-Assisted%20Evaluation%20of%20LLM%0A%20%20Outputs%20with%20Human%20Preferences%0AAuthor%3A%20Shreya%20Shankar%20and%20J.%20D.%20Zamfirescu-Pereira%20and%20Bj%C3%B6rn%20Hartmann%20and%20Aditya%20G.%20Parameswaran%20and%20Ian%20Arawjo%0AAbstract%3A%20%20%20Due%20to%20the%20cumbersome%20nature%20of%20human%20evaluation%20and%20limitations%20of%0Acode-based%20evaluation%2C%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20being%20used%0Ato%20assist%20humans%20in%20evaluating%20LLM%20outputs.%20Yet%20LLM-generated%20evaluators%20simply%0Ainherit%20all%20the%20problems%20of%20the%20LLMs%20they%20evaluate%2C%20requiring%20further%20human%0Avalidation.%20We%20present%20a%20mixed-initiative%20approach%20to%20%60%60validate%20the%0Avalidators%27%27%20--%20aligning%20LLM-generated%20evaluation%20functions%20%28be%20it%20prompts%20or%0Acode%29%20with%20human%20requirements.%20Our%20interface%2C%20EvalGen%2C%20provides%20automated%0Aassistance%20to%20users%20in%20generating%20evaluation%20criteria%20and%20implementing%0Aassertions.%20While%20generating%20candidate%20implementations%20%28Python%20functions%2C%20LLM%0Agrader%20prompts%29%2C%20EvalGen%20asks%20humans%20to%20grade%20a%20subset%20of%20LLM%20outputs%3B%20this%0Afeedback%20is%20used%20to%20select%20implementations%20that%20better%20align%20with%20user%20grades.%0AA%20qualitative%20study%20finds%20overall%20support%20for%20EvalGen%20but%20underscores%20the%0Asubjectivity%20and%20iterative%20process%20of%20alignment.%20In%20particular%2C%20we%20identify%20a%0Aphenomenon%20we%20dub%20%5Cemph%7Bcriteria%20drift%7D%3A%20users%20need%20criteria%20to%20grade%20outputs%2C%0Abut%20grading%20outputs%20helps%20users%20define%20criteria.%20What%20is%20more%2C%20some%20criteria%0Aappears%20%5Cemph%7Bdependent%7D%20on%20the%20specific%20LLM%20outputs%20observed%20%28rather%20than%0Aindependent%20criteria%20that%20can%20be%20defined%20%5Cemph%7Ba%20priori%7D%29%2C%20raising%20serious%0Aquestions%20for%20approaches%20that%20assume%20the%20independence%20of%20evaluation%20from%0Aobservation%20of%20model%20outputs.%20We%20present%20our%20interface%20and%20implementation%0Adetails%2C%20a%20comparison%20of%20our%20algorithm%20with%20a%20baseline%20approach%2C%20and%0Aimplications%20for%20the%20design%20of%20future%20LLM%20evaluation%20assistants.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12272v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Who%20Validates%20the%20Validators%3F%20Aligning%20LLM-Assisted%20Evaluation%20of%20LLM%0A%20%20Outputs%20with%20Human%20Preferences&entry.906535625=Shreya%20Shankar%20and%20J.%20D.%20Zamfirescu-Pereira%20and%20Bj%C3%B6rn%20Hartmann%20and%20Aditya%20G.%20Parameswaran%20and%20Ian%20Arawjo&entry.1292438233=%20%20Due%20to%20the%20cumbersome%20nature%20of%20human%20evaluation%20and%20limitations%20of%0Acode-based%20evaluation%2C%20Large%20Language%20Models%20%28LLMs%29%20are%20increasingly%20being%20used%0Ato%20assist%20humans%20in%20evaluating%20LLM%20outputs.%20Yet%20LLM-generated%20evaluators%20simply%0Ainherit%20all%20the%20problems%20of%20the%20LLMs%20they%20evaluate%2C%20requiring%20further%20human%0Avalidation.%20We%20present%20a%20mixed-initiative%20approach%20to%20%60%60validate%20the%0Avalidators%27%27%20--%20aligning%20LLM-generated%20evaluation%20functions%20%28be%20it%20prompts%20or%0Acode%29%20with%20human%20requirements.%20Our%20interface%2C%20EvalGen%2C%20provides%20automated%0Aassistance%20to%20users%20in%20generating%20evaluation%20criteria%20and%20implementing%0Aassertions.%20While%20generating%20candidate%20implementations%20%28Python%20functions%2C%20LLM%0Agrader%20prompts%29%2C%20EvalGen%20asks%20humans%20to%20grade%20a%20subset%20of%20LLM%20outputs%3B%20this%0Afeedback%20is%20used%20to%20select%20implementations%20that%20better%20align%20with%20user%20grades.%0AA%20qualitative%20study%20finds%20overall%20support%20for%20EvalGen%20but%20underscores%20the%0Asubjectivity%20and%20iterative%20process%20of%20alignment.%20In%20particular%2C%20we%20identify%20a%0Aphenomenon%20we%20dub%20%5Cemph%7Bcriteria%20drift%7D%3A%20users%20need%20criteria%20to%20grade%20outputs%2C%0Abut%20grading%20outputs%20helps%20users%20define%20criteria.%20What%20is%20more%2C%20some%20criteria%0Aappears%20%5Cemph%7Bdependent%7D%20on%20the%20specific%20LLM%20outputs%20observed%20%28rather%20than%0Aindependent%20criteria%20that%20can%20be%20defined%20%5Cemph%7Ba%20priori%7D%29%2C%20raising%20serious%0Aquestions%20for%20approaches%20that%20assume%20the%20independence%20of%20evaluation%20from%0Aobservation%20of%20model%20outputs.%20We%20present%20our%20interface%20and%20implementation%0Adetails%2C%20a%20comparison%20of%20our%20algorithm%20with%20a%20baseline%20approach%2C%20and%0Aimplications%20for%20the%20design%20of%20future%20LLM%20evaluation%20assistants.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12272v1&entry.124074799=Read"},
{"title": "Low Frequency Sampling in Model Predictive Path Integral Control", "author": "Bogdan Vlahov and Jason Gibson and David D. Fan and Patrick Spieler and Ali-akbar Agha-mohammadi and Evangelos A. Theodorou", "abstract": "  Sampling-based model-predictive controllers have become a powerful\noptimization tool for planning and control problems in various challenging\nenvironments. In this paper, we show how the default choice of uncorrelated\nGaussian distributions can be improved upon with the use of a colored noise\ndistribution. Our choice of distribution allows for the emphasis on low\nfrequency control signals, which can result in smoother and more exploratory\nsamples. We use this frequency-based sampling distribution with Model\nPredictive Path Integral (MPPI) in both hardware and simulation experiments to\nshow better or equal performance on systems with various speeds of input\nresponse.\n", "link": "http://arxiv.org/abs/2404.03094v2", "date": "2024-04-18", "relevancy": 1.8076, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4885}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4467}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4424}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Low%20Frequency%20Sampling%20in%20Model%20Predictive%20Path%20Integral%20Control&body=Title%3A%20Low%20Frequency%20Sampling%20in%20Model%20Predictive%20Path%20Integral%20Control%0AAuthor%3A%20Bogdan%20Vlahov%20and%20Jason%20Gibson%20and%20David%20D.%20Fan%20and%20Patrick%20Spieler%20and%20Ali-akbar%20Agha-mohammadi%20and%20Evangelos%20A.%20Theodorou%0AAbstract%3A%20%20%20Sampling-based%20model-predictive%20controllers%20have%20become%20a%20powerful%0Aoptimization%20tool%20for%20planning%20and%20control%20problems%20in%20various%20challenging%0Aenvironments.%20In%20this%20paper%2C%20we%20show%20how%20the%20default%20choice%20of%20uncorrelated%0AGaussian%20distributions%20can%20be%20improved%20upon%20with%20the%20use%20of%20a%20colored%20noise%0Adistribution.%20Our%20choice%20of%20distribution%20allows%20for%20the%20emphasis%20on%20low%0Afrequency%20control%20signals%2C%20which%20can%20result%20in%20smoother%20and%20more%20exploratory%0Asamples.%20We%20use%20this%20frequency-based%20sampling%20distribution%20with%20Model%0APredictive%20Path%20Integral%20%28MPPI%29%20in%20both%20hardware%20and%20simulation%20experiments%20to%0Ashow%20better%20or%20equal%20performance%20on%20systems%20with%20various%20speeds%20of%20input%0Aresponse.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03094v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low%20Frequency%20Sampling%20in%20Model%20Predictive%20Path%20Integral%20Control&entry.906535625=Bogdan%20Vlahov%20and%20Jason%20Gibson%20and%20David%20D.%20Fan%20and%20Patrick%20Spieler%20and%20Ali-akbar%20Agha-mohammadi%20and%20Evangelos%20A.%20Theodorou&entry.1292438233=%20%20Sampling-based%20model-predictive%20controllers%20have%20become%20a%20powerful%0Aoptimization%20tool%20for%20planning%20and%20control%20problems%20in%20various%20challenging%0Aenvironments.%20In%20this%20paper%2C%20we%20show%20how%20the%20default%20choice%20of%20uncorrelated%0AGaussian%20distributions%20can%20be%20improved%20upon%20with%20the%20use%20of%20a%20colored%20noise%0Adistribution.%20Our%20choice%20of%20distribution%20allows%20for%20the%20emphasis%20on%20low%0Afrequency%20control%20signals%2C%20which%20can%20result%20in%20smoother%20and%20more%20exploratory%0Asamples.%20We%20use%20this%20frequency-based%20sampling%20distribution%20with%20Model%0APredictive%20Path%20Integral%20%28MPPI%29%20in%20both%20hardware%20and%20simulation%20experiments%20to%0Ashow%20better%20or%20equal%20performance%20on%20systems%20with%20various%20speeds%20of%20input%0Aresponse.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03094v2&entry.124074799=Read"},
{"title": "ASID: Active Exploration for System Identification in Robotic\n  Manipulation", "author": "Marius Memmel and Andrew Wagenmaker and Chuning Zhu and Patrick Yin and Dieter Fox and Abhishek Gupta", "abstract": "  Model-free control strategies such as reinforcement learning have shown the\nability to learn control strategies without requiring an accurate model or\nsimulator of the world. While this is appealing due to the lack of modeling\nrequirements, such methods can be sample inefficient, making them impractical\nin many real-world domains. On the other hand, model-based control techniques\nleveraging accurate simulators can circumvent these challenges and use a large\namount of cheap simulation data to learn controllers that can effectively\ntransfer to the real world. The challenge with such model-based techniques is\nthe requirement for an extremely accurate simulation, requiring both the\nspecification of appropriate simulation assets and physical parameters. This\nrequires considerable human effort to design for every environment being\nconsidered. In this work, we propose a learning system that can leverage a\nsmall amount of real-world data to autonomously refine a simulation model and\nthen plan an accurate control strategy that can be deployed in the real world.\nOur approach critically relies on utilizing an initial (possibly inaccurate)\nsimulator to design effective exploration policies that, when deployed in the\nreal world, collect high-quality data. We demonstrate the efficacy of this\nparadigm in identifying articulation, mass, and other physical parameters in\nseveral challenging robotic manipulation tasks, and illustrate that only a\nsmall amount of real-world data can allow for effective sim-to-real transfer.\nProject website at https://weirdlabuw.github.io/asid\n", "link": "http://arxiv.org/abs/2404.12308v1", "date": "2024-04-18", "relevancy": 1.7904, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6645}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6168}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5617}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ASID%3A%20Active%20Exploration%20for%20System%20Identification%20in%20Robotic%0A%20%20Manipulation&body=Title%3A%20ASID%3A%20Active%20Exploration%20for%20System%20Identification%20in%20Robotic%0A%20%20Manipulation%0AAuthor%3A%20Marius%20Memmel%20and%20Andrew%20Wagenmaker%20and%20Chuning%20Zhu%20and%20Patrick%20Yin%20and%20Dieter%20Fox%20and%20Abhishek%20Gupta%0AAbstract%3A%20%20%20Model-free%20control%20strategies%20such%20as%20reinforcement%20learning%20have%20shown%20the%0Aability%20to%20learn%20control%20strategies%20without%20requiring%20an%20accurate%20model%20or%0Asimulator%20of%20the%20world.%20While%20this%20is%20appealing%20due%20to%20the%20lack%20of%20modeling%0Arequirements%2C%20such%20methods%20can%20be%20sample%20inefficient%2C%20making%20them%20impractical%0Ain%20many%20real-world%20domains.%20On%20the%20other%20hand%2C%20model-based%20control%20techniques%0Aleveraging%20accurate%20simulators%20can%20circumvent%20these%20challenges%20and%20use%20a%20large%0Aamount%20of%20cheap%20simulation%20data%20to%20learn%20controllers%20that%20can%20effectively%0Atransfer%20to%20the%20real%20world.%20The%20challenge%20with%20such%20model-based%20techniques%20is%0Athe%20requirement%20for%20an%20extremely%20accurate%20simulation%2C%20requiring%20both%20the%0Aspecification%20of%20appropriate%20simulation%20assets%20and%20physical%20parameters.%20This%0Arequires%20considerable%20human%20effort%20to%20design%20for%20every%20environment%20being%0Aconsidered.%20In%20this%20work%2C%20we%20propose%20a%20learning%20system%20that%20can%20leverage%20a%0Asmall%20amount%20of%20real-world%20data%20to%20autonomously%20refine%20a%20simulation%20model%20and%0Athen%20plan%20an%20accurate%20control%20strategy%20that%20can%20be%20deployed%20in%20the%20real%20world.%0AOur%20approach%20critically%20relies%20on%20utilizing%20an%20initial%20%28possibly%20inaccurate%29%0Asimulator%20to%20design%20effective%20exploration%20policies%20that%2C%20when%20deployed%20in%20the%0Areal%20world%2C%20collect%20high-quality%20data.%20We%20demonstrate%20the%20efficacy%20of%20this%0Aparadigm%20in%20identifying%20articulation%2C%20mass%2C%20and%20other%20physical%20parameters%20in%0Aseveral%20challenging%20robotic%20manipulation%20tasks%2C%20and%20illustrate%20that%20only%20a%0Asmall%20amount%20of%20real-world%20data%20can%20allow%20for%20effective%20sim-to-real%20transfer.%0AProject%20website%20at%20https%3A//weirdlabuw.github.io/asid%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12308v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASID%3A%20Active%20Exploration%20for%20System%20Identification%20in%20Robotic%0A%20%20Manipulation&entry.906535625=Marius%20Memmel%20and%20Andrew%20Wagenmaker%20and%20Chuning%20Zhu%20and%20Patrick%20Yin%20and%20Dieter%20Fox%20and%20Abhishek%20Gupta&entry.1292438233=%20%20Model-free%20control%20strategies%20such%20as%20reinforcement%20learning%20have%20shown%20the%0Aability%20to%20learn%20control%20strategies%20without%20requiring%20an%20accurate%20model%20or%0Asimulator%20of%20the%20world.%20While%20this%20is%20appealing%20due%20to%20the%20lack%20of%20modeling%0Arequirements%2C%20such%20methods%20can%20be%20sample%20inefficient%2C%20making%20them%20impractical%0Ain%20many%20real-world%20domains.%20On%20the%20other%20hand%2C%20model-based%20control%20techniques%0Aleveraging%20accurate%20simulators%20can%20circumvent%20these%20challenges%20and%20use%20a%20large%0Aamount%20of%20cheap%20simulation%20data%20to%20learn%20controllers%20that%20can%20effectively%0Atransfer%20to%20the%20real%20world.%20The%20challenge%20with%20such%20model-based%20techniques%20is%0Athe%20requirement%20for%20an%20extremely%20accurate%20simulation%2C%20requiring%20both%20the%0Aspecification%20of%20appropriate%20simulation%20assets%20and%20physical%20parameters.%20This%0Arequires%20considerable%20human%20effort%20to%20design%20for%20every%20environment%20being%0Aconsidered.%20In%20this%20work%2C%20we%20propose%20a%20learning%20system%20that%20can%20leverage%20a%0Asmall%20amount%20of%20real-world%20data%20to%20autonomously%20refine%20a%20simulation%20model%20and%0Athen%20plan%20an%20accurate%20control%20strategy%20that%20can%20be%20deployed%20in%20the%20real%20world.%0AOur%20approach%20critically%20relies%20on%20utilizing%20an%20initial%20%28possibly%20inaccurate%29%0Asimulator%20to%20design%20effective%20exploration%20policies%20that%2C%20when%20deployed%20in%20the%0Areal%20world%2C%20collect%20high-quality%20data.%20We%20demonstrate%20the%20efficacy%20of%20this%0Aparadigm%20in%20identifying%20articulation%2C%20mass%2C%20and%20other%20physical%20parameters%20in%0Aseveral%20challenging%20robotic%20manipulation%20tasks%2C%20and%20illustrate%20that%20only%20a%0Asmall%20amount%20of%20real-world%20data%20can%20allow%20for%20effective%20sim-to-real%20transfer.%0AProject%20website%20at%20https%3A//weirdlabuw.github.io/asid%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12308v1&entry.124074799=Read"},
{"title": "Concept Induction: Analyzing Unstructured Text with High-Level Concepts\n  Using LLooM", "author": "Michelle S. Lam and Janice Teoh and James Landay and Jeffrey Heer and Michael S. Bernstein", "abstract": "  Data analysts have long sought to turn unstructured text data into meaningful\nconcepts. Though common, topic modeling and clustering focus on lower-level\nkeywords and require significant interpretative work. We introduce concept\ninduction, a computational process that instead produces high-level concepts,\ndefined by explicit inclusion criteria, from unstructured text. For a dataset\nof toxic online comments, where a state-of-the-art BERTopic model outputs\n\"women, power, female,\" concept induction produces high-level concepts such as\n\"Criticism of traditional gender roles\" and \"Dismissal of women's concerns.\" We\npresent LLooM, a concept induction algorithm that leverages large language\nmodels to iteratively synthesize sampled text and propose human-interpretable\nconcepts of increasing generality. We then instantiate LLooM in a\nmixed-initiative text analysis tool, enabling analysts to shift their attention\nfrom interpreting topics to engaging in theory-driven analysis. Through\ntechnical evaluations and four analysis scenarios ranging from literature\nreview to content moderation, we find that LLooM's concepts improve upon the\nprior art of topic models in terms of quality and data coverage. In expert case\nstudies, LLooM helped researchers to uncover new insights even from familiar\ndatasets, for example by suggesting a previously unnoticed concept of attacks\non out-party stances in a political social media dataset.\n", "link": "http://arxiv.org/abs/2404.12259v1", "date": "2024-04-18", "relevancy": 1.7899, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4622}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4442}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.434}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Concept%20Induction%3A%20Analyzing%20Unstructured%20Text%20with%20High-Level%20Concepts%0A%20%20Using%20LLooM&body=Title%3A%20Concept%20Induction%3A%20Analyzing%20Unstructured%20Text%20with%20High-Level%20Concepts%0A%20%20Using%20LLooM%0AAuthor%3A%20Michelle%20S.%20Lam%20and%20Janice%20Teoh%20and%20James%20Landay%20and%20Jeffrey%20Heer%20and%20Michael%20S.%20Bernstein%0AAbstract%3A%20%20%20Data%20analysts%20have%20long%20sought%20to%20turn%20unstructured%20text%20data%20into%20meaningful%0Aconcepts.%20Though%20common%2C%20topic%20modeling%20and%20clustering%20focus%20on%20lower-level%0Akeywords%20and%20require%20significant%20interpretative%20work.%20We%20introduce%20concept%0Ainduction%2C%20a%20computational%20process%20that%20instead%20produces%20high-level%20concepts%2C%0Adefined%20by%20explicit%20inclusion%20criteria%2C%20from%20unstructured%20text.%20For%20a%20dataset%0Aof%20toxic%20online%20comments%2C%20where%20a%20state-of-the-art%20BERTopic%20model%20outputs%0A%22women%2C%20power%2C%20female%2C%22%20concept%20induction%20produces%20high-level%20concepts%20such%20as%0A%22Criticism%20of%20traditional%20gender%20roles%22%20and%20%22Dismissal%20of%20women%27s%20concerns.%22%20We%0Apresent%20LLooM%2C%20a%20concept%20induction%20algorithm%20that%20leverages%20large%20language%0Amodels%20to%20iteratively%20synthesize%20sampled%20text%20and%20propose%20human-interpretable%0Aconcepts%20of%20increasing%20generality.%20We%20then%20instantiate%20LLooM%20in%20a%0Amixed-initiative%20text%20analysis%20tool%2C%20enabling%20analysts%20to%20shift%20their%20attention%0Afrom%20interpreting%20topics%20to%20engaging%20in%20theory-driven%20analysis.%20Through%0Atechnical%20evaluations%20and%20four%20analysis%20scenarios%20ranging%20from%20literature%0Areview%20to%20content%20moderation%2C%20we%20find%20that%20LLooM%27s%20concepts%20improve%20upon%20the%0Aprior%20art%20of%20topic%20models%20in%20terms%20of%20quality%20and%20data%20coverage.%20In%20expert%20case%0Astudies%2C%20LLooM%20helped%20researchers%20to%20uncover%20new%20insights%20even%20from%20familiar%0Adatasets%2C%20for%20example%20by%20suggesting%20a%20previously%20unnoticed%20concept%20of%20attacks%0Aon%20out-party%20stances%20in%20a%20political%20social%20media%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12259v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Concept%20Induction%3A%20Analyzing%20Unstructured%20Text%20with%20High-Level%20Concepts%0A%20%20Using%20LLooM&entry.906535625=Michelle%20S.%20Lam%20and%20Janice%20Teoh%20and%20James%20Landay%20and%20Jeffrey%20Heer%20and%20Michael%20S.%20Bernstein&entry.1292438233=%20%20Data%20analysts%20have%20long%20sought%20to%20turn%20unstructured%20text%20data%20into%20meaningful%0Aconcepts.%20Though%20common%2C%20topic%20modeling%20and%20clustering%20focus%20on%20lower-level%0Akeywords%20and%20require%20significant%20interpretative%20work.%20We%20introduce%20concept%0Ainduction%2C%20a%20computational%20process%20that%20instead%20produces%20high-level%20concepts%2C%0Adefined%20by%20explicit%20inclusion%20criteria%2C%20from%20unstructured%20text.%20For%20a%20dataset%0Aof%20toxic%20online%20comments%2C%20where%20a%20state-of-the-art%20BERTopic%20model%20outputs%0A%22women%2C%20power%2C%20female%2C%22%20concept%20induction%20produces%20high-level%20concepts%20such%20as%0A%22Criticism%20of%20traditional%20gender%20roles%22%20and%20%22Dismissal%20of%20women%27s%20concerns.%22%20We%0Apresent%20LLooM%2C%20a%20concept%20induction%20algorithm%20that%20leverages%20large%20language%0Amodels%20to%20iteratively%20synthesize%20sampled%20text%20and%20propose%20human-interpretable%0Aconcepts%20of%20increasing%20generality.%20We%20then%20instantiate%20LLooM%20in%20a%0Amixed-initiative%20text%20analysis%20tool%2C%20enabling%20analysts%20to%20shift%20their%20attention%0Afrom%20interpreting%20topics%20to%20engaging%20in%20theory-driven%20analysis.%20Through%0Atechnical%20evaluations%20and%20four%20analysis%20scenarios%20ranging%20from%20literature%0Areview%20to%20content%20moderation%2C%20we%20find%20that%20LLooM%27s%20concepts%20improve%20upon%20the%0Aprior%20art%20of%20topic%20models%20in%20terms%20of%20quality%20and%20data%20coverage.%20In%20expert%20case%0Astudies%2C%20LLooM%20helped%20researchers%20to%20uncover%20new%20insights%20even%20from%20familiar%0Adatasets%2C%20for%20example%20by%20suggesting%20a%20previously%20unnoticed%20concept%20of%20attacks%0Aon%20out-party%20stances%20in%20a%20political%20social%20media%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12259v1&entry.124074799=Read"},
{"title": "One-shot Empirical Privacy Estimation for Federated Learning", "author": "Galen Andrew and Peter Kairouz and Sewoong Oh and Alina Oprea and H. Brendan McMahan and Vinith M. Suriyakumar", "abstract": "  Privacy estimation techniques for differentially private (DP) algorithms are\nuseful for comparing against analytical bounds, or to empirically measure\nprivacy loss in settings where known analytical bounds are not tight. However,\nexisting privacy auditing techniques usually make strong assumptions on the\nadversary (e.g., knowledge of intermediate model iterates or the training data\ndistribution), are tailored to specific tasks, model architectures, or DP\nalgorithm, and/or require retraining the model many times (typically on the\norder of thousands). These shortcomings make deploying such techniques at scale\ndifficult in practice, especially in federated settings where model training\ncan take days or weeks. In this work, we present a novel \"one-shot\" approach\nthat can systematically address these challenges, allowing efficient auditing\nor estimation of the privacy loss of a model during the same, single training\nrun used to fit model parameters, and without requiring any a priori knowledge\nabout the model architecture, task, or DP training algorithm. We show that our\nmethod provides provably correct estimates for the privacy loss under the\nGaussian mechanism, and we demonstrate its performance on well-established FL\nbenchmark datasets under several adversarial threat models.\n", "link": "http://arxiv.org/abs/2302.03098v5", "date": "2024-04-18", "relevancy": 1.7889, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4751}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4505}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4328}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20One-shot%20Empirical%20Privacy%20Estimation%20for%20Federated%20Learning&body=Title%3A%20One-shot%20Empirical%20Privacy%20Estimation%20for%20Federated%20Learning%0AAuthor%3A%20Galen%20Andrew%20and%20Peter%20Kairouz%20and%20Sewoong%20Oh%20and%20Alina%20Oprea%20and%20H.%20Brendan%20McMahan%20and%20Vinith%20M.%20Suriyakumar%0AAbstract%3A%20%20%20Privacy%20estimation%20techniques%20for%20differentially%20private%20%28DP%29%20algorithms%20are%0Auseful%20for%20comparing%20against%20analytical%20bounds%2C%20or%20to%20empirically%20measure%0Aprivacy%20loss%20in%20settings%20where%20known%20analytical%20bounds%20are%20not%20tight.%20However%2C%0Aexisting%20privacy%20auditing%20techniques%20usually%20make%20strong%20assumptions%20on%20the%0Aadversary%20%28e.g.%2C%20knowledge%20of%20intermediate%20model%20iterates%20or%20the%20training%20data%0Adistribution%29%2C%20are%20tailored%20to%20specific%20tasks%2C%20model%20architectures%2C%20or%20DP%0Aalgorithm%2C%20and/or%20require%20retraining%20the%20model%20many%20times%20%28typically%20on%20the%0Aorder%20of%20thousands%29.%20These%20shortcomings%20make%20deploying%20such%20techniques%20at%20scale%0Adifficult%20in%20practice%2C%20especially%20in%20federated%20settings%20where%20model%20training%0Acan%20take%20days%20or%20weeks.%20In%20this%20work%2C%20we%20present%20a%20novel%20%22one-shot%22%20approach%0Athat%20can%20systematically%20address%20these%20challenges%2C%20allowing%20efficient%20auditing%0Aor%20estimation%20of%20the%20privacy%20loss%20of%20a%20model%20during%20the%20same%2C%20single%20training%0Arun%20used%20to%20fit%20model%20parameters%2C%20and%20without%20requiring%20any%20a%20priori%20knowledge%0Aabout%20the%20model%20architecture%2C%20task%2C%20or%20DP%20training%20algorithm.%20We%20show%20that%20our%0Amethod%20provides%20provably%20correct%20estimates%20for%20the%20privacy%20loss%20under%20the%0AGaussian%20mechanism%2C%20and%20we%20demonstrate%20its%20performance%20on%20well-established%20FL%0Abenchmark%20datasets%20under%20several%20adversarial%20threat%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.03098v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One-shot%20Empirical%20Privacy%20Estimation%20for%20Federated%20Learning&entry.906535625=Galen%20Andrew%20and%20Peter%20Kairouz%20and%20Sewoong%20Oh%20and%20Alina%20Oprea%20and%20H.%20Brendan%20McMahan%20and%20Vinith%20M.%20Suriyakumar&entry.1292438233=%20%20Privacy%20estimation%20techniques%20for%20differentially%20private%20%28DP%29%20algorithms%20are%0Auseful%20for%20comparing%20against%20analytical%20bounds%2C%20or%20to%20empirically%20measure%0Aprivacy%20loss%20in%20settings%20where%20known%20analytical%20bounds%20are%20not%20tight.%20However%2C%0Aexisting%20privacy%20auditing%20techniques%20usually%20make%20strong%20assumptions%20on%20the%0Aadversary%20%28e.g.%2C%20knowledge%20of%20intermediate%20model%20iterates%20or%20the%20training%20data%0Adistribution%29%2C%20are%20tailored%20to%20specific%20tasks%2C%20model%20architectures%2C%20or%20DP%0Aalgorithm%2C%20and/or%20require%20retraining%20the%20model%20many%20times%20%28typically%20on%20the%0Aorder%20of%20thousands%29.%20These%20shortcomings%20make%20deploying%20such%20techniques%20at%20scale%0Adifficult%20in%20practice%2C%20especially%20in%20federated%20settings%20where%20model%20training%0Acan%20take%20days%20or%20weeks.%20In%20this%20work%2C%20we%20present%20a%20novel%20%22one-shot%22%20approach%0Athat%20can%20systematically%20address%20these%20challenges%2C%20allowing%20efficient%20auditing%0Aor%20estimation%20of%20the%20privacy%20loss%20of%20a%20model%20during%20the%20same%2C%20single%20training%0Arun%20used%20to%20fit%20model%20parameters%2C%20and%20without%20requiring%20any%20a%20priori%20knowledge%0Aabout%20the%20model%20architecture%2C%20task%2C%20or%20DP%20training%20algorithm.%20We%20show%20that%20our%0Amethod%20provides%20provably%20correct%20estimates%20for%20the%20privacy%20loss%20under%20the%0AGaussian%20mechanism%2C%20and%20we%20demonstrate%20its%20performance%20on%20well-established%20FL%0Abenchmark%20datasets%20under%20several%20adversarial%20threat%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.03098v5&entry.124074799=Read"},
{"title": "When LLMs are Unfit Use FastFit: Fast and Effective Text Classification\n  with Many Classes", "author": "Asaf Yehudai and Elron Bendel", "abstract": "  We present FastFit, a method, and a Python package design to provide fast and\naccurate few-shot classification, especially for scenarios with many\nsemantically similar classes. FastFit utilizes a novel approach integrating\nbatch contrastive learning and token-level similarity score. Compared to\nexisting few-shot learning packages, such as SetFit, Transformers, or few-shot\nprompting of large language models via API calls, FastFit significantly\nimproves multiclass classification performance in speed and accuracy across\nFewMany, our newly curated English benchmark, and Multilingual datasets.\nFastFit demonstrates a 3-20x improvement in training speed, completing training\nin just a few seconds. The FastFit package is now available on GitHub and PyPi,\npresenting a user-friendly solution for NLP practitioners.\n", "link": "http://arxiv.org/abs/2404.12365v1", "date": "2024-04-18", "relevancy": 1.774, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4544}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.442}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4198}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20When%20LLMs%20are%20Unfit%20Use%20FastFit%3A%20Fast%20and%20Effective%20Text%20Classification%0A%20%20with%20Many%20Classes&body=Title%3A%20When%20LLMs%20are%20Unfit%20Use%20FastFit%3A%20Fast%20and%20Effective%20Text%20Classification%0A%20%20with%20Many%20Classes%0AAuthor%3A%20Asaf%20Yehudai%20and%20Elron%20Bendel%0AAbstract%3A%20%20%20We%20present%20FastFit%2C%20a%20method%2C%20and%20a%20Python%20package%20design%20to%20provide%20fast%20and%0Aaccurate%20few-shot%20classification%2C%20especially%20for%20scenarios%20with%20many%0Asemantically%20similar%20classes.%20FastFit%20utilizes%20a%20novel%20approach%20integrating%0Abatch%20contrastive%20learning%20and%20token-level%20similarity%20score.%20Compared%20to%0Aexisting%20few-shot%20learning%20packages%2C%20such%20as%20SetFit%2C%20Transformers%2C%20or%20few-shot%0Aprompting%20of%20large%20language%20models%20via%20API%20calls%2C%20FastFit%20significantly%0Aimproves%20multiclass%20classification%20performance%20in%20speed%20and%20accuracy%20across%0AFewMany%2C%20our%20newly%20curated%20English%20benchmark%2C%20and%20Multilingual%20datasets.%0AFastFit%20demonstrates%20a%203-20x%20improvement%20in%20training%20speed%2C%20completing%20training%0Ain%20just%20a%20few%20seconds.%20The%20FastFit%20package%20is%20now%20available%20on%20GitHub%20and%20PyPi%2C%0Apresenting%20a%20user-friendly%20solution%20for%20NLP%20practitioners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12365v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20LLMs%20are%20Unfit%20Use%20FastFit%3A%20Fast%20and%20Effective%20Text%20Classification%0A%20%20with%20Many%20Classes&entry.906535625=Asaf%20Yehudai%20and%20Elron%20Bendel&entry.1292438233=%20%20We%20present%20FastFit%2C%20a%20method%2C%20and%20a%20Python%20package%20design%20to%20provide%20fast%20and%0Aaccurate%20few-shot%20classification%2C%20especially%20for%20scenarios%20with%20many%0Asemantically%20similar%20classes.%20FastFit%20utilizes%20a%20novel%20approach%20integrating%0Abatch%20contrastive%20learning%20and%20token-level%20similarity%20score.%20Compared%20to%0Aexisting%20few-shot%20learning%20packages%2C%20such%20as%20SetFit%2C%20Transformers%2C%20or%20few-shot%0Aprompting%20of%20large%20language%20models%20via%20API%20calls%2C%20FastFit%20significantly%0Aimproves%20multiclass%20classification%20performance%20in%20speed%20and%20accuracy%20across%0AFewMany%2C%20our%20newly%20curated%20English%20benchmark%2C%20and%20Multilingual%20datasets.%0AFastFit%20demonstrates%20a%203-20x%20improvement%20in%20training%20speed%2C%20completing%20training%0Ain%20just%20a%20few%20seconds.%20The%20FastFit%20package%20is%20now%20available%20on%20GitHub%20and%20PyPi%2C%0Apresenting%20a%20user-friendly%20solution%20for%20NLP%20practitioners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12365v1&entry.124074799=Read"},
{"title": "Adaptive Gait Modeling and Optimization for Principally Kinematic\n  Systems", "author": "Siming Deng and Noah J. Cowan and Brian A. Bittner", "abstract": "  Robotic adaptation to unanticipated operating conditions is crucial to\nachieving persistence and robustness in complex real world settings. For a wide\nrange of cutting-edge robotic systems, such as micro- and nano-scale robots,\nsoft robots, medical robots, and bio-hybrid robots, it is infeasible to\nanticipate the operating environment a priori due to complexities that arise\nfrom numerous factors including imprecision in manufacturing, chemo-mechanical\nforces, and poorly understood contact mechanics. Drawing inspiration from\ndata-driven modeling, geometric mechanics (or gauge theory), and adaptive\ncontrol, we employ an adaptive system identification framework and demonstrate\nits efficacy in enhancing the performance of principally kinematic locomotors\n(those governed by Rayleigh dissipation or zero momentum conservation). We\nshowcase the capability of the adaptive model to efficiently accommodate\nvarying terrains and iteratively modified behaviors within a behavior\noptimization framework. This provides both the ability to improve fundamental\nbehaviors and perform motion tracking to precision. Notably, we are capable of\noptimizing the gaits of the Purcell swimmer using approximately 10 cycles per\nlink, which for the nine-link Purcell swimmer provides a factor of ten\nimprovement in optimization speed over the state of the art. Beyond simply a\ncomputational speed up, this ten-fold improvement may enable this method to be\nsuccessfully deployed for in-situ behavior refinement, injury recovery, and\nterrain adaptation, particularly in domains where simulations provide poor\nguides for the real world.\n", "link": "http://arxiv.org/abs/2310.02141v2", "date": "2024-04-18", "relevancy": 1.7541, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6295}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5738}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5712}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Gait%20Modeling%20and%20Optimization%20for%20Principally%20Kinematic%0A%20%20Systems&body=Title%3A%20Adaptive%20Gait%20Modeling%20and%20Optimization%20for%20Principally%20Kinematic%0A%20%20Systems%0AAuthor%3A%20Siming%20Deng%20and%20Noah%20J.%20Cowan%20and%20Brian%20A.%20Bittner%0AAbstract%3A%20%20%20Robotic%20adaptation%20to%20unanticipated%20operating%20conditions%20is%20crucial%20to%0Aachieving%20persistence%20and%20robustness%20in%20complex%20real%20world%20settings.%20For%20a%20wide%0Arange%20of%20cutting-edge%20robotic%20systems%2C%20such%20as%20micro-%20and%20nano-scale%20robots%2C%0Asoft%20robots%2C%20medical%20robots%2C%20and%20bio-hybrid%20robots%2C%20it%20is%20infeasible%20to%0Aanticipate%20the%20operating%20environment%20a%20priori%20due%20to%20complexities%20that%20arise%0Afrom%20numerous%20factors%20including%20imprecision%20in%20manufacturing%2C%20chemo-mechanical%0Aforces%2C%20and%20poorly%20understood%20contact%20mechanics.%20Drawing%20inspiration%20from%0Adata-driven%20modeling%2C%20geometric%20mechanics%20%28or%20gauge%20theory%29%2C%20and%20adaptive%0Acontrol%2C%20we%20employ%20an%20adaptive%20system%20identification%20framework%20and%20demonstrate%0Aits%20efficacy%20in%20enhancing%20the%20performance%20of%20principally%20kinematic%20locomotors%0A%28those%20governed%20by%20Rayleigh%20dissipation%20or%20zero%20momentum%20conservation%29.%20We%0Ashowcase%20the%20capability%20of%20the%20adaptive%20model%20to%20efficiently%20accommodate%0Avarying%20terrains%20and%20iteratively%20modified%20behaviors%20within%20a%20behavior%0Aoptimization%20framework.%20This%20provides%20both%20the%20ability%20to%20improve%20fundamental%0Abehaviors%20and%20perform%20motion%20tracking%20to%20precision.%20Notably%2C%20we%20are%20capable%20of%0Aoptimizing%20the%20gaits%20of%20the%20Purcell%20swimmer%20using%20approximately%2010%20cycles%20per%0Alink%2C%20which%20for%20the%20nine-link%20Purcell%20swimmer%20provides%20a%20factor%20of%20ten%0Aimprovement%20in%20optimization%20speed%20over%20the%20state%20of%20the%20art.%20Beyond%20simply%20a%0Acomputational%20speed%20up%2C%20this%20ten-fold%20improvement%20may%20enable%20this%20method%20to%20be%0Asuccessfully%20deployed%20for%20in-situ%20behavior%20refinement%2C%20injury%20recovery%2C%20and%0Aterrain%20adaptation%2C%20particularly%20in%20domains%20where%20simulations%20provide%20poor%0Aguides%20for%20the%20real%20world.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.02141v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Gait%20Modeling%20and%20Optimization%20for%20Principally%20Kinematic%0A%20%20Systems&entry.906535625=Siming%20Deng%20and%20Noah%20J.%20Cowan%20and%20Brian%20A.%20Bittner&entry.1292438233=%20%20Robotic%20adaptation%20to%20unanticipated%20operating%20conditions%20is%20crucial%20to%0Aachieving%20persistence%20and%20robustness%20in%20complex%20real%20world%20settings.%20For%20a%20wide%0Arange%20of%20cutting-edge%20robotic%20systems%2C%20such%20as%20micro-%20and%20nano-scale%20robots%2C%0Asoft%20robots%2C%20medical%20robots%2C%20and%20bio-hybrid%20robots%2C%20it%20is%20infeasible%20to%0Aanticipate%20the%20operating%20environment%20a%20priori%20due%20to%20complexities%20that%20arise%0Afrom%20numerous%20factors%20including%20imprecision%20in%20manufacturing%2C%20chemo-mechanical%0Aforces%2C%20and%20poorly%20understood%20contact%20mechanics.%20Drawing%20inspiration%20from%0Adata-driven%20modeling%2C%20geometric%20mechanics%20%28or%20gauge%20theory%29%2C%20and%20adaptive%0Acontrol%2C%20we%20employ%20an%20adaptive%20system%20identification%20framework%20and%20demonstrate%0Aits%20efficacy%20in%20enhancing%20the%20performance%20of%20principally%20kinematic%20locomotors%0A%28those%20governed%20by%20Rayleigh%20dissipation%20or%20zero%20momentum%20conservation%29.%20We%0Ashowcase%20the%20capability%20of%20the%20adaptive%20model%20to%20efficiently%20accommodate%0Avarying%20terrains%20and%20iteratively%20modified%20behaviors%20within%20a%20behavior%0Aoptimization%20framework.%20This%20provides%20both%20the%20ability%20to%20improve%20fundamental%0Abehaviors%20and%20perform%20motion%20tracking%20to%20precision.%20Notably%2C%20we%20are%20capable%20of%0Aoptimizing%20the%20gaits%20of%20the%20Purcell%20swimmer%20using%20approximately%2010%20cycles%20per%0Alink%2C%20which%20for%20the%20nine-link%20Purcell%20swimmer%20provides%20a%20factor%20of%20ten%0Aimprovement%20in%20optimization%20speed%20over%20the%20state%20of%20the%20art.%20Beyond%20simply%20a%0Acomputational%20speed%20up%2C%20this%20ten-fold%20improvement%20may%20enable%20this%20method%20to%20be%0Asuccessfully%20deployed%20for%20in-situ%20behavior%20refinement%2C%20injury%20recovery%2C%20and%0Aterrain%20adaptation%2C%20particularly%20in%20domains%20where%20simulations%20provide%20poor%0Aguides%20for%20the%20real%20world.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.02141v2&entry.124074799=Read"},
{"title": "InstructIE: A Bilingual Instruction-based Information Extraction Dataset", "author": "Honghao Gui and Shuofei Qiao and Jintian Zhang and Hongbin Ye and Mengshu Sun and Lei Liang and Jeff Z. Pan and Huajun Chen and Ningyu Zhang", "abstract": "  Large language models can perform well on general natural language tasks, but\ntheir effectiveness is still not optimal for information extraction. Recent\nworks indicate that the main reason lies in the lack of extensive data on\ninformation extraction instructions. Note that the existing datasets on\ninformation extraction instructions not only have limited coverage but also\ninvolve high construction costs. To address this issue, we introduce\nInstructIE, a bilingual instruction-based information extraction dataset, which\ncovers 12 diverse domains. Specifically, we propose KG2Instruction, a framework\nspecifically for the automatic generation of such datasets. Experimental\nresults demonstrate that large language models trained with InstructIE can not\nonly obtain better information extraction capabilities but also enhance\nzero-shot performance compared with baselines.\n", "link": "http://arxiv.org/abs/2305.11527v3", "date": "2024-04-18", "relevancy": 1.7416, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4393}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4353}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4315}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20InstructIE%3A%20A%20Bilingual%20Instruction-based%20Information%20Extraction%20Dataset&body=Title%3A%20InstructIE%3A%20A%20Bilingual%20Instruction-based%20Information%20Extraction%20Dataset%0AAuthor%3A%20Honghao%20Gui%20and%20Shuofei%20Qiao%20and%20Jintian%20Zhang%20and%20Hongbin%20Ye%20and%20Mengshu%20Sun%20and%20Lei%20Liang%20and%20Jeff%20Z.%20Pan%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang%0AAbstract%3A%20%20%20Large%20language%20models%20can%20perform%20well%20on%20general%20natural%20language%20tasks%2C%20but%0Atheir%20effectiveness%20is%20still%20not%20optimal%20for%20information%20extraction.%20Recent%0Aworks%20indicate%20that%20the%20main%20reason%20lies%20in%20the%20lack%20of%20extensive%20data%20on%0Ainformation%20extraction%20instructions.%20Note%20that%20the%20existing%20datasets%20on%0Ainformation%20extraction%20instructions%20not%20only%20have%20limited%20coverage%20but%20also%0Ainvolve%20high%20construction%20costs.%20To%20address%20this%20issue%2C%20we%20introduce%0AInstructIE%2C%20a%20bilingual%20instruction-based%20information%20extraction%20dataset%2C%20which%0Acovers%2012%20diverse%20domains.%20Specifically%2C%20we%20propose%20KG2Instruction%2C%20a%20framework%0Aspecifically%20for%20the%20automatic%20generation%20of%20such%20datasets.%20Experimental%0Aresults%20demonstrate%20that%20large%20language%20models%20trained%20with%20InstructIE%20can%20not%0Aonly%20obtain%20better%20information%20extraction%20capabilities%20but%20also%20enhance%0Azero-shot%20performance%20compared%20with%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.11527v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InstructIE%3A%20A%20Bilingual%20Instruction-based%20Information%20Extraction%20Dataset&entry.906535625=Honghao%20Gui%20and%20Shuofei%20Qiao%20and%20Jintian%20Zhang%20and%20Hongbin%20Ye%20and%20Mengshu%20Sun%20and%20Lei%20Liang%20and%20Jeff%20Z.%20Pan%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang&entry.1292438233=%20%20Large%20language%20models%20can%20perform%20well%20on%20general%20natural%20language%20tasks%2C%20but%0Atheir%20effectiveness%20is%20still%20not%20optimal%20for%20information%20extraction.%20Recent%0Aworks%20indicate%20that%20the%20main%20reason%20lies%20in%20the%20lack%20of%20extensive%20data%20on%0Ainformation%20extraction%20instructions.%20Note%20that%20the%20existing%20datasets%20on%0Ainformation%20extraction%20instructions%20not%20only%20have%20limited%20coverage%20but%20also%0Ainvolve%20high%20construction%20costs.%20To%20address%20this%20issue%2C%20we%20introduce%0AInstructIE%2C%20a%20bilingual%20instruction-based%20information%20extraction%20dataset%2C%20which%0Acovers%2012%20diverse%20domains.%20Specifically%2C%20we%20propose%20KG2Instruction%2C%20a%20framework%0Aspecifically%20for%20the%20automatic%20generation%20of%20such%20datasets.%20Experimental%0Aresults%20demonstrate%20that%20large%20language%20models%20trained%20with%20InstructIE%20can%20not%0Aonly%20obtain%20better%20information%20extraction%20capabilities%20but%20also%20enhance%0Azero-shot%20performance%20compared%20with%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.11527v3&entry.124074799=Read"},
{"title": "Debiased Distribution Compression", "author": "Lingxiao Li and Raaz Dwivedi and Lester Mackey", "abstract": "  Modern compression methods can summarize a target distribution $\\mathbb{P}$\nmore succinctly than i.i.d. sampling but require access to a low-bias input\nsequence like a Markov chain converging quickly to $\\mathbb{P}$. We introduce a\nnew suite of compression methods suitable for compression with biased input\nsequences. Given $n$ points targeting the wrong distribution and quadratic\ntime, Stein Kernel Thinning (SKT) returns $\\sqrt{n}$ equal-weighted points with\n$\\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\\mathbb {P}$. For\nlarger-scale compression tasks, Low-rank SKT achieves the same feat in\nsub-quadratic time using an adaptive low-rank debiasing procedure that may be\nof independent interest. For downstream tasks that support simplex or\nconstant-preserving weights, Stein Recombination and Stein Cholesky achieve\neven greater parsimony, matching the guarantees of SKT with as few as\n$\\operatorname{poly-log}(n)$ weighted points. Underlying these advances are new\nguarantees for the quality of simplex-weighted coresets, the spectral decay of\nkernel matrices, and the covering numbers of Stein kernel Hilbert spaces. In\nour experiments, our techniques provide succinct and accurate posterior\nsummaries while overcoming biases due to burn-in, approximate Markov chain\nMonte Carlo, and tempering.\n", "link": "http://arxiv.org/abs/2404.12290v1", "date": "2024-04-18", "relevancy": 1.7342, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4459}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.445}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4166}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Debiased%20Distribution%20Compression&body=Title%3A%20Debiased%20Distribution%20Compression%0AAuthor%3A%20Lingxiao%20Li%20and%20Raaz%20Dwivedi%20and%20Lester%20Mackey%0AAbstract%3A%20%20%20Modern%20compression%20methods%20can%20summarize%20a%20target%20distribution%20%24%5Cmathbb%7BP%7D%24%0Amore%20succinctly%20than%20i.i.d.%20sampling%20but%20require%20access%20to%20a%20low-bias%20input%0Asequence%20like%20a%20Markov%20chain%20converging%20quickly%20to%20%24%5Cmathbb%7BP%7D%24.%20We%20introduce%20a%0Anew%20suite%20of%20compression%20methods%20suitable%20for%20compression%20with%20biased%20input%0Asequences.%20Given%20%24n%24%20points%20targeting%20the%20wrong%20distribution%20and%20quadratic%0Atime%2C%20Stein%20Kernel%20Thinning%20%28SKT%29%20returns%20%24%5Csqrt%7Bn%7D%24%20equal-weighted%20points%20with%0A%24%5Cwidetilde%7BO%7D%28n%5E%7B-1/2%7D%29%24%20maximum%20mean%20discrepancy%20%28MMD%29%20to%20%24%5Cmathbb%20%7BP%7D%24.%20For%0Alarger-scale%20compression%20tasks%2C%20Low-rank%20SKT%20achieves%20the%20same%20feat%20in%0Asub-quadratic%20time%20using%20an%20adaptive%20low-rank%20debiasing%20procedure%20that%20may%20be%0Aof%20independent%20interest.%20For%20downstream%20tasks%20that%20support%20simplex%20or%0Aconstant-preserving%20weights%2C%20Stein%20Recombination%20and%20Stein%20Cholesky%20achieve%0Aeven%20greater%20parsimony%2C%20matching%20the%20guarantees%20of%20SKT%20with%20as%20few%20as%0A%24%5Coperatorname%7Bpoly-log%7D%28n%29%24%20weighted%20points.%20Underlying%20these%20advances%20are%20new%0Aguarantees%20for%20the%20quality%20of%20simplex-weighted%20coresets%2C%20the%20spectral%20decay%20of%0Akernel%20matrices%2C%20and%20the%20covering%20numbers%20of%20Stein%20kernel%20Hilbert%20spaces.%20In%0Aour%20experiments%2C%20our%20techniques%20provide%20succinct%20and%20accurate%20posterior%0Asummaries%20while%20overcoming%20biases%20due%20to%20burn-in%2C%20approximate%20Markov%20chain%0AMonte%20Carlo%2C%20and%20tempering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12290v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Debiased%20Distribution%20Compression&entry.906535625=Lingxiao%20Li%20and%20Raaz%20Dwivedi%20and%20Lester%20Mackey&entry.1292438233=%20%20Modern%20compression%20methods%20can%20summarize%20a%20target%20distribution%20%24%5Cmathbb%7BP%7D%24%0Amore%20succinctly%20than%20i.i.d.%20sampling%20but%20require%20access%20to%20a%20low-bias%20input%0Asequence%20like%20a%20Markov%20chain%20converging%20quickly%20to%20%24%5Cmathbb%7BP%7D%24.%20We%20introduce%20a%0Anew%20suite%20of%20compression%20methods%20suitable%20for%20compression%20with%20biased%20input%0Asequences.%20Given%20%24n%24%20points%20targeting%20the%20wrong%20distribution%20and%20quadratic%0Atime%2C%20Stein%20Kernel%20Thinning%20%28SKT%29%20returns%20%24%5Csqrt%7Bn%7D%24%20equal-weighted%20points%20with%0A%24%5Cwidetilde%7BO%7D%28n%5E%7B-1/2%7D%29%24%20maximum%20mean%20discrepancy%20%28MMD%29%20to%20%24%5Cmathbb%20%7BP%7D%24.%20For%0Alarger-scale%20compression%20tasks%2C%20Low-rank%20SKT%20achieves%20the%20same%20feat%20in%0Asub-quadratic%20time%20using%20an%20adaptive%20low-rank%20debiasing%20procedure%20that%20may%20be%0Aof%20independent%20interest.%20For%20downstream%20tasks%20that%20support%20simplex%20or%0Aconstant-preserving%20weights%2C%20Stein%20Recombination%20and%20Stein%20Cholesky%20achieve%0Aeven%20greater%20parsimony%2C%20matching%20the%20guarantees%20of%20SKT%20with%20as%20few%20as%0A%24%5Coperatorname%7Bpoly-log%7D%28n%29%24%20weighted%20points.%20Underlying%20these%20advances%20are%20new%0Aguarantees%20for%20the%20quality%20of%20simplex-weighted%20coresets%2C%20the%20spectral%20decay%20of%0Akernel%20matrices%2C%20and%20the%20covering%20numbers%20of%20Stein%20kernel%20Hilbert%20spaces.%20In%0Aour%20experiments%2C%20our%20techniques%20provide%20succinct%20and%20accurate%20posterior%0Asummaries%20while%20overcoming%20biases%20due%20to%20burn-in%2C%20approximate%20Markov%20chain%0AMonte%20Carlo%2C%20and%20tempering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12290v1&entry.124074799=Read"},
{"title": "Design And Flight Testing Of LQRi Attitude Control For Quadcopter UAV", "author": "Astik Srivastava and S. Indu and Richa Sharma", "abstract": "  This paper presents the design, implementation, and flight test results of\nlinear quadratic integral regulator (LQRi) based attitude control for a\nquadcopter UAV. We present the derivation of the mathematical model for the\nkinematics and dynamics of the UAV, along with the linearized state space\nrepresentation of the system about hover conditions. LQR and LQRi controllers\nare then designed to stabilize the UAV in hover conditions and to track desired\nattitude commands. The controllers are then implemented onboard the Pixhawk\nflight controller and flight test results are discussed. Finally, the code\nrelated to this paper has been published open-source for replication and\nfurther research\n", "link": "http://arxiv.org/abs/2404.12261v1", "date": "2024-04-18", "relevancy": 1.7299, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.449}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.421}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4205}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Design%20And%20Flight%20Testing%20Of%20LQRi%20Attitude%20Control%20For%20Quadcopter%20UAV&body=Title%3A%20Design%20And%20Flight%20Testing%20Of%20LQRi%20Attitude%20Control%20For%20Quadcopter%20UAV%0AAuthor%3A%20Astik%20Srivastava%20and%20S.%20Indu%20and%20Richa%20Sharma%0AAbstract%3A%20%20%20This%20paper%20presents%20the%20design%2C%20implementation%2C%20and%20flight%20test%20results%20of%0Alinear%20quadratic%20integral%20regulator%20%28LQRi%29%20based%20attitude%20control%20for%20a%0Aquadcopter%20UAV.%20We%20present%20the%20derivation%20of%20the%20mathematical%20model%20for%20the%0Akinematics%20and%20dynamics%20of%20the%20UAV%2C%20along%20with%20the%20linearized%20state%20space%0Arepresentation%20of%20the%20system%20about%20hover%20conditions.%20LQR%20and%20LQRi%20controllers%0Aare%20then%20designed%20to%20stabilize%20the%20UAV%20in%20hover%20conditions%20and%20to%20track%20desired%0Aattitude%20commands.%20The%20controllers%20are%20then%20implemented%20onboard%20the%20Pixhawk%0Aflight%20controller%20and%20flight%20test%20results%20are%20discussed.%20Finally%2C%20the%20code%0Arelated%20to%20this%20paper%20has%20been%20published%20open-source%20for%20replication%20and%0Afurther%20research%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12261v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Design%20And%20Flight%20Testing%20Of%20LQRi%20Attitude%20Control%20For%20Quadcopter%20UAV&entry.906535625=Astik%20Srivastava%20and%20S.%20Indu%20and%20Richa%20Sharma&entry.1292438233=%20%20This%20paper%20presents%20the%20design%2C%20implementation%2C%20and%20flight%20test%20results%20of%0Alinear%20quadratic%20integral%20regulator%20%28LQRi%29%20based%20attitude%20control%20for%20a%0Aquadcopter%20UAV.%20We%20present%20the%20derivation%20of%20the%20mathematical%20model%20for%20the%0Akinematics%20and%20dynamics%20of%20the%20UAV%2C%20along%20with%20the%20linearized%20state%20space%0Arepresentation%20of%20the%20system%20about%20hover%20conditions.%20LQR%20and%20LQRi%20controllers%0Aare%20then%20designed%20to%20stabilize%20the%20UAV%20in%20hover%20conditions%20and%20to%20track%20desired%0Aattitude%20commands.%20The%20controllers%20are%20then%20implemented%20onboard%20the%20Pixhawk%0Aflight%20controller%20and%20flight%20test%20results%20are%20discussed.%20Finally%2C%20the%20code%0Arelated%20to%20this%20paper%20has%20been%20published%20open-source%20for%20replication%20and%0Afurther%20research%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12261v1&entry.124074799=Read"},
{"title": "RISE: 3D Perception Makes Real-World Robot Imitation Simple and\n  Effective", "author": "Chenxi Wang and Hongjie Fang and Hao-Shu Fang and Cewu Lu", "abstract": "  Precise robot manipulations require rich spatial information in imitation\nlearning. Image-based policies model object positions from fixed cameras, which\nare sensitive to camera view changes. Policies utilizing 3D point clouds\nusually predict keyframes rather than continuous actions, posing difficulty in\ndynamic and contact-rich scenarios. To utilize 3D perception efficiently, we\npresent RISE, an end-to-end baseline for real-world imitation learning, which\npredicts continuous actions directly from single-view point clouds. It\ncompresses the point cloud to tokens with a sparse 3D encoder. After adding\nsparse positional encoding, the tokens are featurized using a transformer.\nFinally, the features are decoded into robot actions by a diffusion head.\nTrained with 50 demonstrations for each real-world task, RISE surpasses\ncurrently representative 2D and 3D policies by a large margin, showcasing\nsignificant advantages in both accuracy and efficiency. Experiments also\ndemonstrate that RISE is more general and robust to environmental change\ncompared with previous baselines. Project website: rise-policy.github.io.\n", "link": "http://arxiv.org/abs/2404.12281v1", "date": "2024-04-18", "relevancy": 1.7242, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6068}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5966}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5532}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20RISE%3A%203D%20Perception%20Makes%20Real-World%20Robot%20Imitation%20Simple%20and%0A%20%20Effective&body=Title%3A%20RISE%3A%203D%20Perception%20Makes%20Real-World%20Robot%20Imitation%20Simple%20and%0A%20%20Effective%0AAuthor%3A%20Chenxi%20Wang%20and%20Hongjie%20Fang%20and%20Hao-Shu%20Fang%20and%20Cewu%20Lu%0AAbstract%3A%20%20%20Precise%20robot%20manipulations%20require%20rich%20spatial%20information%20in%20imitation%0Alearning.%20Image-based%20policies%20model%20object%20positions%20from%20fixed%20cameras%2C%20which%0Aare%20sensitive%20to%20camera%20view%20changes.%20Policies%20utilizing%203D%20point%20clouds%0Ausually%20predict%20keyframes%20rather%20than%20continuous%20actions%2C%20posing%20difficulty%20in%0Adynamic%20and%20contact-rich%20scenarios.%20To%20utilize%203D%20perception%20efficiently%2C%20we%0Apresent%20RISE%2C%20an%20end-to-end%20baseline%20for%20real-world%20imitation%20learning%2C%20which%0Apredicts%20continuous%20actions%20directly%20from%20single-view%20point%20clouds.%20It%0Acompresses%20the%20point%20cloud%20to%20tokens%20with%20a%20sparse%203D%20encoder.%20After%20adding%0Asparse%20positional%20encoding%2C%20the%20tokens%20are%20featurized%20using%20a%20transformer.%0AFinally%2C%20the%20features%20are%20decoded%20into%20robot%20actions%20by%20a%20diffusion%20head.%0ATrained%20with%2050%20demonstrations%20for%20each%20real-world%20task%2C%20RISE%20surpasses%0Acurrently%20representative%202D%20and%203D%20policies%20by%20a%20large%20margin%2C%20showcasing%0Asignificant%20advantages%20in%20both%20accuracy%20and%20efficiency.%20Experiments%20also%0Ademonstrate%20that%20RISE%20is%20more%20general%20and%20robust%20to%20environmental%20change%0Acompared%20with%20previous%20baselines.%20Project%20website%3A%20rise-policy.github.io.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12281v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RISE%3A%203D%20Perception%20Makes%20Real-World%20Robot%20Imitation%20Simple%20and%0A%20%20Effective&entry.906535625=Chenxi%20Wang%20and%20Hongjie%20Fang%20and%20Hao-Shu%20Fang%20and%20Cewu%20Lu&entry.1292438233=%20%20Precise%20robot%20manipulations%20require%20rich%20spatial%20information%20in%20imitation%0Alearning.%20Image-based%20policies%20model%20object%20positions%20from%20fixed%20cameras%2C%20which%0Aare%20sensitive%20to%20camera%20view%20changes.%20Policies%20utilizing%203D%20point%20clouds%0Ausually%20predict%20keyframes%20rather%20than%20continuous%20actions%2C%20posing%20difficulty%20in%0Adynamic%20and%20contact-rich%20scenarios.%20To%20utilize%203D%20perception%20efficiently%2C%20we%0Apresent%20RISE%2C%20an%20end-to-end%20baseline%20for%20real-world%20imitation%20learning%2C%20which%0Apredicts%20continuous%20actions%20directly%20from%20single-view%20point%20clouds.%20It%0Acompresses%20the%20point%20cloud%20to%20tokens%20with%20a%20sparse%203D%20encoder.%20After%20adding%0Asparse%20positional%20encoding%2C%20the%20tokens%20are%20featurized%20using%20a%20transformer.%0AFinally%2C%20the%20features%20are%20decoded%20into%20robot%20actions%20by%20a%20diffusion%20head.%0ATrained%20with%2050%20demonstrations%20for%20each%20real-world%20task%2C%20RISE%20surpasses%0Acurrently%20representative%202D%20and%203D%20policies%20by%20a%20large%20margin%2C%20showcasing%0Asignificant%20advantages%20in%20both%20accuracy%20and%20efficiency.%20Experiments%20also%0Ademonstrate%20that%20RISE%20is%20more%20general%20and%20robust%20to%20environmental%20change%0Acompared%20with%20previous%20baselines.%20Project%20website%3A%20rise-policy.github.io.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12281v1&entry.124074799=Read"},
{"title": "Struggle with Adversarial Defense? Try Diffusion", "author": "Yujie Li and Yanbin Wang and Haitao Xu and Bin Liu and Jianguo Sun and Zhenhao Guo and Wenrui Ma", "abstract": "  Adversarial attacks induce misclassification by introducing subtle\nperturbations. Recently, diffusion models are applied to the image classifiers\nto improve adversarial robustness through adversarial training or by purifying\nadversarial noise. However, diffusion-based adversarial training often\nencounters convergence challenges and high computational expenses.\nAdditionally, diffusion-based purification inevitably causes data shift and is\ndeemed susceptible to stronger adaptive attacks. To tackle these issues, we\npropose the Truth Maximization Diffusion Classifier (TMDC), a generative\nBayesian classifier that builds upon pre-trained diffusion models and the\nBayesian theorem. Unlike data-driven classifiers, TMDC, guided by Bayesian\nprinciples, utilizes the conditional likelihood from diffusion models to\ndetermine the class probabilities of input images, thereby insulating against\nthe influences of data shift and the limitations of adversarial training.\nMoreover, to enhance TMDC's resilience against more potent adversarial attacks,\nwe propose an optimization strategy for diffusion classifiers. This strategy\ninvolves post-training the diffusion model on perturbed datasets with\nground-truth labels as conditions, guiding the diffusion model to learn the\ndata distribution and maximizing the likelihood under the ground-truth labels.\nThe proposed method achieves state-of-the-art performance on the CIFAR10\ndataset against heavy white-box attacks and strong adaptive attacks.\nSpecifically, TMDC achieves robust accuracies of 82.81% against $l_{\\infty}$\nnorm-bounded perturbations and 86.05% against $l_{2}$ norm-bounded\nperturbations, respectively, with $\\epsilon=0.05$.\n", "link": "http://arxiv.org/abs/2404.08273v2", "date": "2024-04-18", "relevancy": 1.6788, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6169}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.544}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5412}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Struggle%20with%20Adversarial%20Defense%3F%20Try%20Diffusion&body=Title%3A%20Struggle%20with%20Adversarial%20Defense%3F%20Try%20Diffusion%0AAuthor%3A%20Yujie%20Li%20and%20Yanbin%20Wang%20and%20Haitao%20Xu%20and%20Bin%20Liu%20and%20Jianguo%20Sun%20and%20Zhenhao%20Guo%20and%20Wenrui%20Ma%0AAbstract%3A%20%20%20Adversarial%20attacks%20induce%20misclassification%20by%20introducing%20subtle%0Aperturbations.%20Recently%2C%20diffusion%20models%20are%20applied%20to%20the%20image%20classifiers%0Ato%20improve%20adversarial%20robustness%20through%20adversarial%20training%20or%20by%20purifying%0Aadversarial%20noise.%20However%2C%20diffusion-based%20adversarial%20training%20often%0Aencounters%20convergence%20challenges%20and%20high%20computational%20expenses.%0AAdditionally%2C%20diffusion-based%20purification%20inevitably%20causes%20data%20shift%20and%20is%0Adeemed%20susceptible%20to%20stronger%20adaptive%20attacks.%20To%20tackle%20these%20issues%2C%20we%0Apropose%20the%20Truth%20Maximization%20Diffusion%20Classifier%20%28TMDC%29%2C%20a%20generative%0ABayesian%20classifier%20that%20builds%20upon%20pre-trained%20diffusion%20models%20and%20the%0ABayesian%20theorem.%20Unlike%20data-driven%20classifiers%2C%20TMDC%2C%20guided%20by%20Bayesian%0Aprinciples%2C%20utilizes%20the%20conditional%20likelihood%20from%20diffusion%20models%20to%0Adetermine%20the%20class%20probabilities%20of%20input%20images%2C%20thereby%20insulating%20against%0Athe%20influences%20of%20data%20shift%20and%20the%20limitations%20of%20adversarial%20training.%0AMoreover%2C%20to%20enhance%20TMDC%27s%20resilience%20against%20more%20potent%20adversarial%20attacks%2C%0Awe%20propose%20an%20optimization%20strategy%20for%20diffusion%20classifiers.%20This%20strategy%0Ainvolves%20post-training%20the%20diffusion%20model%20on%20perturbed%20datasets%20with%0Aground-truth%20labels%20as%20conditions%2C%20guiding%20the%20diffusion%20model%20to%20learn%20the%0Adata%20distribution%20and%20maximizing%20the%20likelihood%20under%20the%20ground-truth%20labels.%0AThe%20proposed%20method%20achieves%20state-of-the-art%20performance%20on%20the%20CIFAR10%0Adataset%20against%20heavy%20white-box%20attacks%20and%20strong%20adaptive%20attacks.%0ASpecifically%2C%20TMDC%20achieves%20robust%20accuracies%20of%2082.81%25%20against%20%24l_%7B%5Cinfty%7D%24%0Anorm-bounded%20perturbations%20and%2086.05%25%20against%20%24l_%7B2%7D%24%20norm-bounded%0Aperturbations%2C%20respectively%2C%20with%20%24%5Cepsilon%3D0.05%24.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08273v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Struggle%20with%20Adversarial%20Defense%3F%20Try%20Diffusion&entry.906535625=Yujie%20Li%20and%20Yanbin%20Wang%20and%20Haitao%20Xu%20and%20Bin%20Liu%20and%20Jianguo%20Sun%20and%20Zhenhao%20Guo%20and%20Wenrui%20Ma&entry.1292438233=%20%20Adversarial%20attacks%20induce%20misclassification%20by%20introducing%20subtle%0Aperturbations.%20Recently%2C%20diffusion%20models%20are%20applied%20to%20the%20image%20classifiers%0Ato%20improve%20adversarial%20robustness%20through%20adversarial%20training%20or%20by%20purifying%0Aadversarial%20noise.%20However%2C%20diffusion-based%20adversarial%20training%20often%0Aencounters%20convergence%20challenges%20and%20high%20computational%20expenses.%0AAdditionally%2C%20diffusion-based%20purification%20inevitably%20causes%20data%20shift%20and%20is%0Adeemed%20susceptible%20to%20stronger%20adaptive%20attacks.%20To%20tackle%20these%20issues%2C%20we%0Apropose%20the%20Truth%20Maximization%20Diffusion%20Classifier%20%28TMDC%29%2C%20a%20generative%0ABayesian%20classifier%20that%20builds%20upon%20pre-trained%20diffusion%20models%20and%20the%0ABayesian%20theorem.%20Unlike%20data-driven%20classifiers%2C%20TMDC%2C%20guided%20by%20Bayesian%0Aprinciples%2C%20utilizes%20the%20conditional%20likelihood%20from%20diffusion%20models%20to%0Adetermine%20the%20class%20probabilities%20of%20input%20images%2C%20thereby%20insulating%20against%0Athe%20influences%20of%20data%20shift%20and%20the%20limitations%20of%20adversarial%20training.%0AMoreover%2C%20to%20enhance%20TMDC%27s%20resilience%20against%20more%20potent%20adversarial%20attacks%2C%0Awe%20propose%20an%20optimization%20strategy%20for%20diffusion%20classifiers.%20This%20strategy%0Ainvolves%20post-training%20the%20diffusion%20model%20on%20perturbed%20datasets%20with%0Aground-truth%20labels%20as%20conditions%2C%20guiding%20the%20diffusion%20model%20to%20learn%20the%0Adata%20distribution%20and%20maximizing%20the%20likelihood%20under%20the%20ground-truth%20labels.%0AThe%20proposed%20method%20achieves%20state-of-the-art%20performance%20on%20the%20CIFAR10%0Adataset%20against%20heavy%20white-box%20attacks%20and%20strong%20adaptive%20attacks.%0ASpecifically%2C%20TMDC%20achieves%20robust%20accuracies%20of%2082.81%25%20against%20%24l_%7B%5Cinfty%7D%24%0Anorm-bounded%20perturbations%20and%2086.05%25%20against%20%24l_%7B2%7D%24%20norm-bounded%0Aperturbations%2C%20respectively%2C%20with%20%24%5Cepsilon%3D0.05%24.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08273v2&entry.124074799=Read"},
{"title": "Physics-integrated generative modeling using attentive planar\n  normalizing flow based variational autoencoder", "author": "Sheikh Waqas Akhtar", "abstract": "  Physics-integrated generative modeling is a class of hybrid or grey-box\nmodeling in which we augment the the data-driven model with the physics\nknowledge governing the data distribution. The use of physics knowledge allows\nthe generative model to produce output in a controlled way, so that the output,\nby construction, complies with the physical laws. It imparts improved\ngeneralization ability to extrapolate beyond the training distribution as well\nas improved interpretability because the model is partly grounded in firm\ndomain knowledge. In this work, we aim to improve the fidelity of\nreconstruction and robustness to noise in the physics integrated generative\nmodel. To this end, we use variational-autoencoder as a generative model. To\nimprove the reconstruction results of the decoder, we propose to learn the\nlatent posterior distribution of both the physics as well as the trainable\ndata-driven components using planar normalizng flow. Normalizng flow based\nposterior distribution harnesses the inherent dynamical structure of the data\ndistribution, hence the learned model gets closer to the true underlying data\ndistribution. To improve the robustness of generative model against noise\ninjected in the model, we propose a modification in the encoder part of the\nnormalizing flow based VAE. We designed the encoder to incorporate scaled dot\nproduct attention based contextual information in the noisy latent vector which\nwill mitigate the adverse effect of noise in the latent vector and make the\nmodel more robust. We empirically evaluated our models on human locomotion\ndataset [33] and the results validate the efficacy of our proposed models in\nterms of improvement in reconstruction quality as well as robustness against\nnoise injected in the model.\n", "link": "http://arxiv.org/abs/2404.12267v1", "date": "2024-04-18", "relevancy": 1.6635, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5809}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5599}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5418}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Physics-integrated%20generative%20modeling%20using%20attentive%20planar%0A%20%20normalizing%20flow%20based%20variational%20autoencoder&body=Title%3A%20Physics-integrated%20generative%20modeling%20using%20attentive%20planar%0A%20%20normalizing%20flow%20based%20variational%20autoencoder%0AAuthor%3A%20Sheikh%20Waqas%20Akhtar%0AAbstract%3A%20%20%20Physics-integrated%20generative%20modeling%20is%20a%20class%20of%20hybrid%20or%20grey-box%0Amodeling%20in%20which%20we%20augment%20the%20the%20data-driven%20model%20with%20the%20physics%0Aknowledge%20governing%20the%20data%20distribution.%20The%20use%20of%20physics%20knowledge%20allows%0Athe%20generative%20model%20to%20produce%20output%20in%20a%20controlled%20way%2C%20so%20that%20the%20output%2C%0Aby%20construction%2C%20complies%20with%20the%20physical%20laws.%20It%20imparts%20improved%0Ageneralization%20ability%20to%20extrapolate%20beyond%20the%20training%20distribution%20as%20well%0Aas%20improved%20interpretability%20because%20the%20model%20is%20partly%20grounded%20in%20firm%0Adomain%20knowledge.%20In%20this%20work%2C%20we%20aim%20to%20improve%20the%20fidelity%20of%0Areconstruction%20and%20robustness%20to%20noise%20in%20the%20physics%20integrated%20generative%0Amodel.%20To%20this%20end%2C%20we%20use%20variational-autoencoder%20as%20a%20generative%20model.%20To%0Aimprove%20the%20reconstruction%20results%20of%20the%20decoder%2C%20we%20propose%20to%20learn%20the%0Alatent%20posterior%20distribution%20of%20both%20the%20physics%20as%20well%20as%20the%20trainable%0Adata-driven%20components%20using%20planar%20normalizng%20flow.%20Normalizng%20flow%20based%0Aposterior%20distribution%20harnesses%20the%20inherent%20dynamical%20structure%20of%20the%20data%0Adistribution%2C%20hence%20the%20learned%20model%20gets%20closer%20to%20the%20true%20underlying%20data%0Adistribution.%20To%20improve%20the%20robustness%20of%20generative%20model%20against%20noise%0Ainjected%20in%20the%20model%2C%20we%20propose%20a%20modification%20in%20the%20encoder%20part%20of%20the%0Anormalizing%20flow%20based%20VAE.%20We%20designed%20the%20encoder%20to%20incorporate%20scaled%20dot%0Aproduct%20attention%20based%20contextual%20information%20in%20the%20noisy%20latent%20vector%20which%0Awill%20mitigate%20the%20adverse%20effect%20of%20noise%20in%20the%20latent%20vector%20and%20make%20the%0Amodel%20more%20robust.%20We%20empirically%20evaluated%20our%20models%20on%20human%20locomotion%0Adataset%20%5B33%5D%20and%20the%20results%20validate%20the%20efficacy%20of%20our%20proposed%20models%20in%0Aterms%20of%20improvement%20in%20reconstruction%20quality%20as%20well%20as%20robustness%20against%0Anoise%20injected%20in%20the%20model.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12267v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Physics-integrated%20generative%20modeling%20using%20attentive%20planar%0A%20%20normalizing%20flow%20based%20variational%20autoencoder&entry.906535625=Sheikh%20Waqas%20Akhtar&entry.1292438233=%20%20Physics-integrated%20generative%20modeling%20is%20a%20class%20of%20hybrid%20or%20grey-box%0Amodeling%20in%20which%20we%20augment%20the%20the%20data-driven%20model%20with%20the%20physics%0Aknowledge%20governing%20the%20data%20distribution.%20The%20use%20of%20physics%20knowledge%20allows%0Athe%20generative%20model%20to%20produce%20output%20in%20a%20controlled%20way%2C%20so%20that%20the%20output%2C%0Aby%20construction%2C%20complies%20with%20the%20physical%20laws.%20It%20imparts%20improved%0Ageneralization%20ability%20to%20extrapolate%20beyond%20the%20training%20distribution%20as%20well%0Aas%20improved%20interpretability%20because%20the%20model%20is%20partly%20grounded%20in%20firm%0Adomain%20knowledge.%20In%20this%20work%2C%20we%20aim%20to%20improve%20the%20fidelity%20of%0Areconstruction%20and%20robustness%20to%20noise%20in%20the%20physics%20integrated%20generative%0Amodel.%20To%20this%20end%2C%20we%20use%20variational-autoencoder%20as%20a%20generative%20model.%20To%0Aimprove%20the%20reconstruction%20results%20of%20the%20decoder%2C%20we%20propose%20to%20learn%20the%0Alatent%20posterior%20distribution%20of%20both%20the%20physics%20as%20well%20as%20the%20trainable%0Adata-driven%20components%20using%20planar%20normalizng%20flow.%20Normalizng%20flow%20based%0Aposterior%20distribution%20harnesses%20the%20inherent%20dynamical%20structure%20of%20the%20data%0Adistribution%2C%20hence%20the%20learned%20model%20gets%20closer%20to%20the%20true%20underlying%20data%0Adistribution.%20To%20improve%20the%20robustness%20of%20generative%20model%20against%20noise%0Ainjected%20in%20the%20model%2C%20we%20propose%20a%20modification%20in%20the%20encoder%20part%20of%20the%0Anormalizing%20flow%20based%20VAE.%20We%20designed%20the%20encoder%20to%20incorporate%20scaled%20dot%0Aproduct%20attention%20based%20contextual%20information%20in%20the%20noisy%20latent%20vector%20which%0Awill%20mitigate%20the%20adverse%20effect%20of%20noise%20in%20the%20latent%20vector%20and%20make%20the%0Amodel%20more%20robust.%20We%20empirically%20evaluated%20our%20models%20on%20human%20locomotion%0Adataset%20%5B33%5D%20and%20the%20results%20validate%20the%20efficacy%20of%20our%20proposed%20models%20in%0Aterms%20of%20improvement%20in%20reconstruction%20quality%20as%20well%20as%20robustness%20against%0Anoise%20injected%20in%20the%20model.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12267v1&entry.124074799=Read"},
{"title": "Dynamic Modality and View Selection for Multimodal Emotion Recognition\n  with Missing Modalities", "author": "Luciana Trinkaus Menon and Luiz Carlos Ribeiro Neduziak and Jean Paul Barddal and Alessandro Lameiras Koerich and Alceu de Souza Britto Jr", "abstract": "  The study of human emotions, traditionally a cornerstone in fields like\npsychology and neuroscience, has been profoundly impacted by the advent of\nartificial intelligence (AI). Multiple channels, such as speech (voice) and\nfacial expressions (image), are crucial in understanding human emotions.\nHowever, AI's journey in multimodal emotion recognition (MER) is marked by\nsubstantial technical challenges. One significant hurdle is how AI models\nmanage the absence of a particular modality - a frequent occurrence in\nreal-world situations. This study's central focus is assessing the performance\nand resilience of two strategies when confronted with the lack of one modality:\na novel multimodal dynamic modality and view selection and a cross-attention\nmechanism. Results on the RECOLA dataset show that dynamic selection-based\nmethods are a promising approach for MER. In the missing modalities scenarios,\nall dynamic selection-based methods outperformed the baseline. The study\nconcludes by emphasizing the intricate interplay between audio and video\nmodalities in emotion prediction, showcasing the adaptability of dynamic\nselection methods in handling missing modalities.\n", "link": "http://arxiv.org/abs/2404.12251v1", "date": "2024-04-18", "relevancy": 1.6273, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5541}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5427}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5377}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Modality%20and%20View%20Selection%20for%20Multimodal%20Emotion%20Recognition%0A%20%20with%20Missing%20Modalities&body=Title%3A%20Dynamic%20Modality%20and%20View%20Selection%20for%20Multimodal%20Emotion%20Recognition%0A%20%20with%20Missing%20Modalities%0AAuthor%3A%20Luciana%20Trinkaus%20Menon%20and%20Luiz%20Carlos%20Ribeiro%20Neduziak%20and%20Jean%20Paul%20Barddal%20and%20Alessandro%20Lameiras%20Koerich%20and%20Alceu%20de%20Souza%20Britto%20Jr%0AAbstract%3A%20%20%20The%20study%20of%20human%20emotions%2C%20traditionally%20a%20cornerstone%20in%20fields%20like%0Apsychology%20and%20neuroscience%2C%20has%20been%20profoundly%20impacted%20by%20the%20advent%20of%0Aartificial%20intelligence%20%28AI%29.%20Multiple%20channels%2C%20such%20as%20speech%20%28voice%29%20and%0Afacial%20expressions%20%28image%29%2C%20are%20crucial%20in%20understanding%20human%20emotions.%0AHowever%2C%20AI%27s%20journey%20in%20multimodal%20emotion%20recognition%20%28MER%29%20is%20marked%20by%0Asubstantial%20technical%20challenges.%20One%20significant%20hurdle%20is%20how%20AI%20models%0Amanage%20the%20absence%20of%20a%20particular%20modality%20-%20a%20frequent%20occurrence%20in%0Areal-world%20situations.%20This%20study%27s%20central%20focus%20is%20assessing%20the%20performance%0Aand%20resilience%20of%20two%20strategies%20when%20confronted%20with%20the%20lack%20of%20one%20modality%3A%0Aa%20novel%20multimodal%20dynamic%20modality%20and%20view%20selection%20and%20a%20cross-attention%0Amechanism.%20Results%20on%20the%20RECOLA%20dataset%20show%20that%20dynamic%20selection-based%0Amethods%20are%20a%20promising%20approach%20for%20MER.%20In%20the%20missing%20modalities%20scenarios%2C%0Aall%20dynamic%20selection-based%20methods%20outperformed%20the%20baseline.%20The%20study%0Aconcludes%20by%20emphasizing%20the%20intricate%20interplay%20between%20audio%20and%20video%0Amodalities%20in%20emotion%20prediction%2C%20showcasing%20the%20adaptability%20of%20dynamic%0Aselection%20methods%20in%20handling%20missing%20modalities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12251v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Modality%20and%20View%20Selection%20for%20Multimodal%20Emotion%20Recognition%0A%20%20with%20Missing%20Modalities&entry.906535625=Luciana%20Trinkaus%20Menon%20and%20Luiz%20Carlos%20Ribeiro%20Neduziak%20and%20Jean%20Paul%20Barddal%20and%20Alessandro%20Lameiras%20Koerich%20and%20Alceu%20de%20Souza%20Britto%20Jr&entry.1292438233=%20%20The%20study%20of%20human%20emotions%2C%20traditionally%20a%20cornerstone%20in%20fields%20like%0Apsychology%20and%20neuroscience%2C%20has%20been%20profoundly%20impacted%20by%20the%20advent%20of%0Aartificial%20intelligence%20%28AI%29.%20Multiple%20channels%2C%20such%20as%20speech%20%28voice%29%20and%0Afacial%20expressions%20%28image%29%2C%20are%20crucial%20in%20understanding%20human%20emotions.%0AHowever%2C%20AI%27s%20journey%20in%20multimodal%20emotion%20recognition%20%28MER%29%20is%20marked%20by%0Asubstantial%20technical%20challenges.%20One%20significant%20hurdle%20is%20how%20AI%20models%0Amanage%20the%20absence%20of%20a%20particular%20modality%20-%20a%20frequent%20occurrence%20in%0Areal-world%20situations.%20This%20study%27s%20central%20focus%20is%20assessing%20the%20performance%0Aand%20resilience%20of%20two%20strategies%20when%20confronted%20with%20the%20lack%20of%20one%20modality%3A%0Aa%20novel%20multimodal%20dynamic%20modality%20and%20view%20selection%20and%20a%20cross-attention%0Amechanism.%20Results%20on%20the%20RECOLA%20dataset%20show%20that%20dynamic%20selection-based%0Amethods%20are%20a%20promising%20approach%20for%20MER.%20In%20the%20missing%20modalities%20scenarios%2C%0Aall%20dynamic%20selection-based%20methods%20outperformed%20the%20baseline.%20The%20study%0Aconcludes%20by%20emphasizing%20the%20intricate%20interplay%20between%20audio%20and%20video%0Amodalities%20in%20emotion%20prediction%2C%20showcasing%20the%20adaptability%20of%20dynamic%0Aselection%20methods%20in%20handling%20missing%20modalities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12251v1&entry.124074799=Read"},
{"title": "MedThink: Explaining Medical Visual Question Answering via Multimodal\n  Decision-Making Rationale", "author": "Xiaotang Gai and Chenyi Zhou and Jiaxiang Liu and Yang Feng and Jian Wu and Zuozhu Liu", "abstract": "  Medical Visual Question Answering (MedVQA), which offers language responses\nto image-based medical inquiries, represents a challenging task and significant\nadvancement in healthcare. It assists medical experts to swiftly interpret\nmedical images, thereby enabling faster and more accurate diagnoses. However,\nthe model interpretability and transparency of existing MedVQA solutions are\noften limited, posing challenges in understanding their decision-making\nprocesses. To address this issue, we devise a semi-automated annotation process\nto streamlining data preparation and build new benchmark MedVQA datasets R-RAD\nand R-SLAKE. The R-RAD and R-SLAKE datasets provide intermediate medical\ndecision-making rationales generated by multimodal large language models and\nhuman annotations for question-answering pairs in existing MedVQA datasets,\ni.e., VQA-RAD and SLAKE. Moreover, we design a novel framework which finetunes\nlightweight pretrained generative models by incorporating medical\ndecision-making rationales into the training process. The framework includes\nthree distinct strategies to generate decision outcomes and corresponding\nrationales, thereby clearly showcasing the medical decision-making process\nduring reasoning. Extensive experiments demonstrate that our method can achieve\nan accuracy of 83.5% on R-RAD and 86.3% on R-SLAKE, significantly outperforming\nexisting state-of-the-art baselines. Dataset and code will be released.\n", "link": "http://arxiv.org/abs/2404.12372v1", "date": "2024-04-18", "relevancy": 1.6105, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5903}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5238}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.516}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MedThink%3A%20Explaining%20Medical%20Visual%20Question%20Answering%20via%20Multimodal%0A%20%20Decision-Making%20Rationale&body=Title%3A%20MedThink%3A%20Explaining%20Medical%20Visual%20Question%20Answering%20via%20Multimodal%0A%20%20Decision-Making%20Rationale%0AAuthor%3A%20Xiaotang%20Gai%20and%20Chenyi%20Zhou%20and%20Jiaxiang%20Liu%20and%20Yang%20Feng%20and%20Jian%20Wu%20and%20Zuozhu%20Liu%0AAbstract%3A%20%20%20Medical%20Visual%20Question%20Answering%20%28MedVQA%29%2C%20which%20offers%20language%20responses%0Ato%20image-based%20medical%20inquiries%2C%20represents%20a%20challenging%20task%20and%20significant%0Aadvancement%20in%20healthcare.%20It%20assists%20medical%20experts%20to%20swiftly%20interpret%0Amedical%20images%2C%20thereby%20enabling%20faster%20and%20more%20accurate%20diagnoses.%20However%2C%0Athe%20model%20interpretability%20and%20transparency%20of%20existing%20MedVQA%20solutions%20are%0Aoften%20limited%2C%20posing%20challenges%20in%20understanding%20their%20decision-making%0Aprocesses.%20To%20address%20this%20issue%2C%20we%20devise%20a%20semi-automated%20annotation%20process%0Ato%20streamlining%20data%20preparation%20and%20build%20new%20benchmark%20MedVQA%20datasets%20R-RAD%0Aand%20R-SLAKE.%20The%20R-RAD%20and%20R-SLAKE%20datasets%20provide%20intermediate%20medical%0Adecision-making%20rationales%20generated%20by%20multimodal%20large%20language%20models%20and%0Ahuman%20annotations%20for%20question-answering%20pairs%20in%20existing%20MedVQA%20datasets%2C%0Ai.e.%2C%20VQA-RAD%20and%20SLAKE.%20Moreover%2C%20we%20design%20a%20novel%20framework%20which%20finetunes%0Alightweight%20pretrained%20generative%20models%20by%20incorporating%20medical%0Adecision-making%20rationales%20into%20the%20training%20process.%20The%20framework%20includes%0Athree%20distinct%20strategies%20to%20generate%20decision%20outcomes%20and%20corresponding%0Arationales%2C%20thereby%20clearly%20showcasing%20the%20medical%20decision-making%20process%0Aduring%20reasoning.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20can%20achieve%0Aan%20accuracy%20of%2083.5%25%20on%20R-RAD%20and%2086.3%25%20on%20R-SLAKE%2C%20significantly%20outperforming%0Aexisting%20state-of-the-art%20baselines.%20Dataset%20and%20code%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12372v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedThink%3A%20Explaining%20Medical%20Visual%20Question%20Answering%20via%20Multimodal%0A%20%20Decision-Making%20Rationale&entry.906535625=Xiaotang%20Gai%20and%20Chenyi%20Zhou%20and%20Jiaxiang%20Liu%20and%20Yang%20Feng%20and%20Jian%20Wu%20and%20Zuozhu%20Liu&entry.1292438233=%20%20Medical%20Visual%20Question%20Answering%20%28MedVQA%29%2C%20which%20offers%20language%20responses%0Ato%20image-based%20medical%20inquiries%2C%20represents%20a%20challenging%20task%20and%20significant%0Aadvancement%20in%20healthcare.%20It%20assists%20medical%20experts%20to%20swiftly%20interpret%0Amedical%20images%2C%20thereby%20enabling%20faster%20and%20more%20accurate%20diagnoses.%20However%2C%0Athe%20model%20interpretability%20and%20transparency%20of%20existing%20MedVQA%20solutions%20are%0Aoften%20limited%2C%20posing%20challenges%20in%20understanding%20their%20decision-making%0Aprocesses.%20To%20address%20this%20issue%2C%20we%20devise%20a%20semi-automated%20annotation%20process%0Ato%20streamlining%20data%20preparation%20and%20build%20new%20benchmark%20MedVQA%20datasets%20R-RAD%0Aand%20R-SLAKE.%20The%20R-RAD%20and%20R-SLAKE%20datasets%20provide%20intermediate%20medical%0Adecision-making%20rationales%20generated%20by%20multimodal%20large%20language%20models%20and%0Ahuman%20annotations%20for%20question-answering%20pairs%20in%20existing%20MedVQA%20datasets%2C%0Ai.e.%2C%20VQA-RAD%20and%20SLAKE.%20Moreover%2C%20we%20design%20a%20novel%20framework%20which%20finetunes%0Alightweight%20pretrained%20generative%20models%20by%20incorporating%20medical%0Adecision-making%20rationales%20into%20the%20training%20process.%20The%20framework%20includes%0Athree%20distinct%20strategies%20to%20generate%20decision%20outcomes%20and%20corresponding%0Arationales%2C%20thereby%20clearly%20showcasing%20the%20medical%20decision-making%20process%0Aduring%20reasoning.%20Extensive%20experiments%20demonstrate%20that%20our%20method%20can%20achieve%0Aan%20accuracy%20of%2083.5%25%20on%20R-RAD%20and%2086.3%25%20on%20R-SLAKE%2C%20significantly%20outperforming%0Aexisting%20state-of-the-art%20baselines.%20Dataset%20and%20code%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12372v1&entry.124074799=Read"},
{"title": "DF-DM: A foundational process model for multimodal data fusion in the\n  artificial intelligence era", "author": "David Restrepo and Chenwei Wu and Constanza V\u00e1squez-Venegas and Luis Filipe Nakayama and Leo Anthony Celi and Diego M L\u00f3pez", "abstract": "  In the big data era, integrating diverse data modalities poses significant\nchallenges, particularly in complex fields like healthcare. This paper\nintroduces a new process model for multimodal Data Fusion for Data Mining,\nintegrating embeddings and the Cross-Industry Standard Process for Data Mining\nwith the existing Data Fusion Information Group model. Our model aims to\ndecrease computational costs, complexity, and bias while improving efficiency\nand reliability. We also propose \"disentangled dense fusion\", a novel embedding\nfusion method designed to optimize mutual information and facilitate dense\ninter-modality feature interaction, thereby minimizing redundant information.\n  We demonstrate the model's efficacy through three use cases: predicting\ndiabetic retinopathy using retinal images and patient metadata, domestic\nviolence prediction employing satellite imagery, internet, and census data, and\nidentifying clinical and demographic features from radiography images and\nclinical notes. The model achieved a Macro F1 score of 0.92 in diabetic\nretinopathy prediction, an R-squared of 0.854 and sMAPE of 24.868 in domestic\nviolence prediction, and a macro AUC of 0.92 and 0.99 for disease prediction\nand sex classification, respectively, in radiological analysis.\n  These results underscore the Data Fusion for Data Mining model's potential to\nsignificantly impact multimodal data processing, promoting its adoption in\ndiverse, resource-constrained settings.\n", "link": "http://arxiv.org/abs/2404.12278v1", "date": "2024-04-18", "relevancy": 1.5986, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5672}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5347}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5184}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DF-DM%3A%20A%20foundational%20process%20model%20for%20multimodal%20data%20fusion%20in%20the%0A%20%20artificial%20intelligence%20era&body=Title%3A%20DF-DM%3A%20A%20foundational%20process%20model%20for%20multimodal%20data%20fusion%20in%20the%0A%20%20artificial%20intelligence%20era%0AAuthor%3A%20David%20Restrepo%20and%20Chenwei%20Wu%20and%20Constanza%20V%C3%A1squez-Venegas%20and%20Luis%20Filipe%20Nakayama%20and%20Leo%20Anthony%20Celi%20and%20Diego%20M%20L%C3%B3pez%0AAbstract%3A%20%20%20In%20the%20big%20data%20era%2C%20integrating%20diverse%20data%20modalities%20poses%20significant%0Achallenges%2C%20particularly%20in%20complex%20fields%20like%20healthcare.%20This%20paper%0Aintroduces%20a%20new%20process%20model%20for%20multimodal%20Data%20Fusion%20for%20Data%20Mining%2C%0Aintegrating%20embeddings%20and%20the%20Cross-Industry%20Standard%20Process%20for%20Data%20Mining%0Awith%20the%20existing%20Data%20Fusion%20Information%20Group%20model.%20Our%20model%20aims%20to%0Adecrease%20computational%20costs%2C%20complexity%2C%20and%20bias%20while%20improving%20efficiency%0Aand%20reliability.%20We%20also%20propose%20%22disentangled%20dense%20fusion%22%2C%20a%20novel%20embedding%0Afusion%20method%20designed%20to%20optimize%20mutual%20information%20and%20facilitate%20dense%0Ainter-modality%20feature%20interaction%2C%20thereby%20minimizing%20redundant%20information.%0A%20%20We%20demonstrate%20the%20model%27s%20efficacy%20through%20three%20use%20cases%3A%20predicting%0Adiabetic%20retinopathy%20using%20retinal%20images%20and%20patient%20metadata%2C%20domestic%0Aviolence%20prediction%20employing%20satellite%20imagery%2C%20internet%2C%20and%20census%20data%2C%20and%0Aidentifying%20clinical%20and%20demographic%20features%20from%20radiography%20images%20and%0Aclinical%20notes.%20The%20model%20achieved%20a%20Macro%20F1%20score%20of%200.92%20in%20diabetic%0Aretinopathy%20prediction%2C%20an%20R-squared%20of%200.854%20and%20sMAPE%20of%2024.868%20in%20domestic%0Aviolence%20prediction%2C%20and%20a%20macro%20AUC%20of%200.92%20and%200.99%20for%20disease%20prediction%0Aand%20sex%20classification%2C%20respectively%2C%20in%20radiological%20analysis.%0A%20%20These%20results%20underscore%20the%20Data%20Fusion%20for%20Data%20Mining%20model%27s%20potential%20to%0Asignificantly%20impact%20multimodal%20data%20processing%2C%20promoting%20its%20adoption%20in%0Adiverse%2C%20resource-constrained%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12278v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DF-DM%3A%20A%20foundational%20process%20model%20for%20multimodal%20data%20fusion%20in%20the%0A%20%20artificial%20intelligence%20era&entry.906535625=David%20Restrepo%20and%20Chenwei%20Wu%20and%20Constanza%20V%C3%A1squez-Venegas%20and%20Luis%20Filipe%20Nakayama%20and%20Leo%20Anthony%20Celi%20and%20Diego%20M%20L%C3%B3pez&entry.1292438233=%20%20In%20the%20big%20data%20era%2C%20integrating%20diverse%20data%20modalities%20poses%20significant%0Achallenges%2C%20particularly%20in%20complex%20fields%20like%20healthcare.%20This%20paper%0Aintroduces%20a%20new%20process%20model%20for%20multimodal%20Data%20Fusion%20for%20Data%20Mining%2C%0Aintegrating%20embeddings%20and%20the%20Cross-Industry%20Standard%20Process%20for%20Data%20Mining%0Awith%20the%20existing%20Data%20Fusion%20Information%20Group%20model.%20Our%20model%20aims%20to%0Adecrease%20computational%20costs%2C%20complexity%2C%20and%20bias%20while%20improving%20efficiency%0Aand%20reliability.%20We%20also%20propose%20%22disentangled%20dense%20fusion%22%2C%20a%20novel%20embedding%0Afusion%20method%20designed%20to%20optimize%20mutual%20information%20and%20facilitate%20dense%0Ainter-modality%20feature%20interaction%2C%20thereby%20minimizing%20redundant%20information.%0A%20%20We%20demonstrate%20the%20model%27s%20efficacy%20through%20three%20use%20cases%3A%20predicting%0Adiabetic%20retinopathy%20using%20retinal%20images%20and%20patient%20metadata%2C%20domestic%0Aviolence%20prediction%20employing%20satellite%20imagery%2C%20internet%2C%20and%20census%20data%2C%20and%0Aidentifying%20clinical%20and%20demographic%20features%20from%20radiography%20images%20and%0Aclinical%20notes.%20The%20model%20achieved%20a%20Macro%20F1%20score%20of%200.92%20in%20diabetic%0Aretinopathy%20prediction%2C%20an%20R-squared%20of%200.854%20and%20sMAPE%20of%2024.868%20in%20domestic%0Aviolence%20prediction%2C%20and%20a%20macro%20AUC%20of%200.92%20and%200.99%20for%20disease%20prediction%0Aand%20sex%20classification%2C%20respectively%2C%20in%20radiological%20analysis.%0A%20%20These%20results%20underscore%20the%20Data%20Fusion%20for%20Data%20Mining%20model%27s%20potential%20to%0Asignificantly%20impact%20multimodal%20data%20processing%2C%20promoting%20its%20adoption%20in%0Adiverse%2C%20resource-constrained%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12278v1&entry.124074799=Read"},
{"title": "Learning the Domain Specific Inverse NUFFT for Accelerated Spiral MRI\n  using Diffusion Models", "author": "Trevor J. Chan and Chamith S. Rajapakse", "abstract": "  Deep learning methods for accelerated MRI achieve state-of-the-art results\nbut largely ignore additional speedups possible with noncartesian sampling\ntrajectories. To address this gap, we created a generative diffusion\nmodel-based reconstruction algorithm for multi-coil highly undersampled spiral\nMRI. This model uses conditioning during training as well as frequency-based\nguidance to ensure consistency between images and measurements. Evaluated on\nretrospective data, we show high quality (structural similarity > 0.87) in\nreconstructed images with ultrafast scan times (0.02 seconds for a 2D image).\nWe use this algorithm to identify a set of optimal variable-density spiral\ntrajectories and show large improvements in image quality compared to\nconventional reconstruction using the non-uniform fast Fourier transform. By\ncombining efficient spiral sampling trajectories, multicoil imaging, and deep\nlearning reconstruction, these methods could enable the extremely high\nacceleration factors needed for real-time 3D imaging.\n", "link": "http://arxiv.org/abs/2404.12361v1", "date": "2024-04-18", "relevancy": 1.5984, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5714}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5527}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5094}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Learning%20the%20Domain%20Specific%20Inverse%20NUFFT%20for%20Accelerated%20Spiral%20MRI%0A%20%20using%20Diffusion%20Models&body=Title%3A%20Learning%20the%20Domain%20Specific%20Inverse%20NUFFT%20for%20Accelerated%20Spiral%20MRI%0A%20%20using%20Diffusion%20Models%0AAuthor%3A%20Trevor%20J.%20Chan%20and%20Chamith%20S.%20Rajapakse%0AAbstract%3A%20%20%20Deep%20learning%20methods%20for%20accelerated%20MRI%20achieve%20state-of-the-art%20results%0Abut%20largely%20ignore%20additional%20speedups%20possible%20with%20noncartesian%20sampling%0Atrajectories.%20To%20address%20this%20gap%2C%20we%20created%20a%20generative%20diffusion%0Amodel-based%20reconstruction%20algorithm%20for%20multi-coil%20highly%20undersampled%20spiral%0AMRI.%20This%20model%20uses%20conditioning%20during%20training%20as%20well%20as%20frequency-based%0Aguidance%20to%20ensure%20consistency%20between%20images%20and%20measurements.%20Evaluated%20on%0Aretrospective%20data%2C%20we%20show%20high%20quality%20%28structural%20similarity%20%3E%200.87%29%20in%0Areconstructed%20images%20with%20ultrafast%20scan%20times%20%280.02%20seconds%20for%20a%202D%20image%29.%0AWe%20use%20this%20algorithm%20to%20identify%20a%20set%20of%20optimal%20variable-density%20spiral%0Atrajectories%20and%20show%20large%20improvements%20in%20image%20quality%20compared%20to%0Aconventional%20reconstruction%20using%20the%20non-uniform%20fast%20Fourier%20transform.%20By%0Acombining%20efficient%20spiral%20sampling%20trajectories%2C%20multicoil%20imaging%2C%20and%20deep%0Alearning%20reconstruction%2C%20these%20methods%20could%20enable%20the%20extremely%20high%0Aacceleration%20factors%20needed%20for%20real-time%203D%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12361v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20the%20Domain%20Specific%20Inverse%20NUFFT%20for%20Accelerated%20Spiral%20MRI%0A%20%20using%20Diffusion%20Models&entry.906535625=Trevor%20J.%20Chan%20and%20Chamith%20S.%20Rajapakse&entry.1292438233=%20%20Deep%20learning%20methods%20for%20accelerated%20MRI%20achieve%20state-of-the-art%20results%0Abut%20largely%20ignore%20additional%20speedups%20possible%20with%20noncartesian%20sampling%0Atrajectories.%20To%20address%20this%20gap%2C%20we%20created%20a%20generative%20diffusion%0Amodel-based%20reconstruction%20algorithm%20for%20multi-coil%20highly%20undersampled%20spiral%0AMRI.%20This%20model%20uses%20conditioning%20during%20training%20as%20well%20as%20frequency-based%0Aguidance%20to%20ensure%20consistency%20between%20images%20and%20measurements.%20Evaluated%20on%0Aretrospective%20data%2C%20we%20show%20high%20quality%20%28structural%20similarity%20%3E%200.87%29%20in%0Areconstructed%20images%20with%20ultrafast%20scan%20times%20%280.02%20seconds%20for%20a%202D%20image%29.%0AWe%20use%20this%20algorithm%20to%20identify%20a%20set%20of%20optimal%20variable-density%20spiral%0Atrajectories%20and%20show%20large%20improvements%20in%20image%20quality%20compared%20to%0Aconventional%20reconstruction%20using%20the%20non-uniform%20fast%20Fourier%20transform.%20By%0Acombining%20efficient%20spiral%20sampling%20trajectories%2C%20multicoil%20imaging%2C%20and%20deep%0Alearning%20reconstruction%2C%20these%20methods%20could%20enable%20the%20extremely%20high%0Aacceleration%20factors%20needed%20for%20real-time%203D%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12361v1&entry.124074799=Read"},
{"title": "Guided Discrete Diffusion for Electronic Health Record Generation", "author": "Zixiang Chen and Jun Han and Yongqian Li and Yiwen Kou and Eran Halperin and Robert E. Tillman and Quanquan Gu", "abstract": "  Electronic health records (EHRs) are a pivotal data source that enables\nnumerous applications in computational medicine, e.g., disease progression\nprediction, clinical trial design, and health economics and outcomes research.\nDespite wide usability, their sensitive nature raises privacy and\nconfidentially concerns, which limit potential use cases. To tackle these\nchallenges, we explore the use of generative models to synthesize artificial,\nyet realistic EHRs. While diffusion-based methods have recently demonstrated\nstate-of-the-art performance in generating other data modalities and overcome\nthe training instability and mode collapse issues that plague previous\nGAN-based approaches, their applications in EHR generation remain\nunderexplored. The discrete nature of tabular medical code data in EHRs poses\nchallenges for high-quality data generation, especially for continuous\ndiffusion models. To this end, we introduce a novel tabular EHR generation\nmethod, EHR-D3PM, which enables both unconditional and conditional generation\nusing the discrete diffusion model. Our experiments demonstrate that EHR-D3PM\nsignificantly outperforms existing generative baselines on comprehensive\nfidelity and utility metrics while maintaining less membership vulnerability\nrisks. Furthermore, we show EHR-D3PM is effective as a data augmentation method\nand enhances performance on downstream tasks when combined with real data.\n", "link": "http://arxiv.org/abs/2404.12314v1", "date": "2024-04-18", "relevancy": 1.5804, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5291}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5285}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5194}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Guided%20Discrete%20Diffusion%20for%20Electronic%20Health%20Record%20Generation&body=Title%3A%20Guided%20Discrete%20Diffusion%20for%20Electronic%20Health%20Record%20Generation%0AAuthor%3A%20Zixiang%20Chen%20and%20Jun%20Han%20and%20Yongqian%20Li%20and%20Yiwen%20Kou%20and%20Eran%20Halperin%20and%20Robert%20E.%20Tillman%20and%20Quanquan%20Gu%0AAbstract%3A%20%20%20Electronic%20health%20records%20%28EHRs%29%20are%20a%20pivotal%20data%20source%20that%20enables%0Anumerous%20applications%20in%20computational%20medicine%2C%20e.g.%2C%20disease%20progression%0Aprediction%2C%20clinical%20trial%20design%2C%20and%20health%20economics%20and%20outcomes%20research.%0ADespite%20wide%20usability%2C%20their%20sensitive%20nature%20raises%20privacy%20and%0Aconfidentially%20concerns%2C%20which%20limit%20potential%20use%20cases.%20To%20tackle%20these%0Achallenges%2C%20we%20explore%20the%20use%20of%20generative%20models%20to%20synthesize%20artificial%2C%0Ayet%20realistic%20EHRs.%20While%20diffusion-based%20methods%20have%20recently%20demonstrated%0Astate-of-the-art%20performance%20in%20generating%20other%20data%20modalities%20and%20overcome%0Athe%20training%20instability%20and%20mode%20collapse%20issues%20that%20plague%20previous%0AGAN-based%20approaches%2C%20their%20applications%20in%20EHR%20generation%20remain%0Aunderexplored.%20The%20discrete%20nature%20of%20tabular%20medical%20code%20data%20in%20EHRs%20poses%0Achallenges%20for%20high-quality%20data%20generation%2C%20especially%20for%20continuous%0Adiffusion%20models.%20To%20this%20end%2C%20we%20introduce%20a%20novel%20tabular%20EHR%20generation%0Amethod%2C%20EHR-D3PM%2C%20which%20enables%20both%20unconditional%20and%20conditional%20generation%0Ausing%20the%20discrete%20diffusion%20model.%20Our%20experiments%20demonstrate%20that%20EHR-D3PM%0Asignificantly%20outperforms%20existing%20generative%20baselines%20on%20comprehensive%0Afidelity%20and%20utility%20metrics%20while%20maintaining%20less%20membership%20vulnerability%0Arisks.%20Furthermore%2C%20we%20show%20EHR-D3PM%20is%20effective%20as%20a%20data%20augmentation%20method%0Aand%20enhances%20performance%20on%20downstream%20tasks%20when%20combined%20with%20real%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12314v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guided%20Discrete%20Diffusion%20for%20Electronic%20Health%20Record%20Generation&entry.906535625=Zixiang%20Chen%20and%20Jun%20Han%20and%20Yongqian%20Li%20and%20Yiwen%20Kou%20and%20Eran%20Halperin%20and%20Robert%20E.%20Tillman%20and%20Quanquan%20Gu&entry.1292438233=%20%20Electronic%20health%20records%20%28EHRs%29%20are%20a%20pivotal%20data%20source%20that%20enables%0Anumerous%20applications%20in%20computational%20medicine%2C%20e.g.%2C%20disease%20progression%0Aprediction%2C%20clinical%20trial%20design%2C%20and%20health%20economics%20and%20outcomes%20research.%0ADespite%20wide%20usability%2C%20their%20sensitive%20nature%20raises%20privacy%20and%0Aconfidentially%20concerns%2C%20which%20limit%20potential%20use%20cases.%20To%20tackle%20these%0Achallenges%2C%20we%20explore%20the%20use%20of%20generative%20models%20to%20synthesize%20artificial%2C%0Ayet%20realistic%20EHRs.%20While%20diffusion-based%20methods%20have%20recently%20demonstrated%0Astate-of-the-art%20performance%20in%20generating%20other%20data%20modalities%20and%20overcome%0Athe%20training%20instability%20and%20mode%20collapse%20issues%20that%20plague%20previous%0AGAN-based%20approaches%2C%20their%20applications%20in%20EHR%20generation%20remain%0Aunderexplored.%20The%20discrete%20nature%20of%20tabular%20medical%20code%20data%20in%20EHRs%20poses%0Achallenges%20for%20high-quality%20data%20generation%2C%20especially%20for%20continuous%0Adiffusion%20models.%20To%20this%20end%2C%20we%20introduce%20a%20novel%20tabular%20EHR%20generation%0Amethod%2C%20EHR-D3PM%2C%20which%20enables%20both%20unconditional%20and%20conditional%20generation%0Ausing%20the%20discrete%20diffusion%20model.%20Our%20experiments%20demonstrate%20that%20EHR-D3PM%0Asignificantly%20outperforms%20existing%20generative%20baselines%20on%20comprehensive%0Afidelity%20and%20utility%20metrics%20while%20maintaining%20less%20membership%20vulnerability%0Arisks.%20Furthermore%2C%20we%20show%20EHR-D3PM%20is%20effective%20as%20a%20data%20augmentation%20method%0Aand%20enhances%20performance%20on%20downstream%20tasks%20when%20combined%20with%20real%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12314v1&entry.124074799=Read"},
{"title": "Moving Object Segmentation: All You Need Is SAM (and Flow)", "author": "Junyu Xie and Charig Yang and Weidi Xie and Andrew Zisserman", "abstract": "  The objective of this paper is motion segmentation -- discovering and\nsegmenting the moving objects in a video. This is a much studied area with\nnumerous careful,and sometimes complex, approaches and training schemes\nincluding: self-supervised learning, learning from synthetic datasets,\nobject-centric representations, amodal representations, and many more. Our\ninterest in this paper is to determine if the Segment Anything model (SAM) can\ncontribute to this task. We investigate two models for combining SAM with\noptical flow that harness the segmentation power of SAM with the ability of\nflow to discover and group moving objects. In the first model, we adapt SAM to\ntake optical flow, rather than RGB, as an input. In the second, SAM takes RGB\nas an input, and flow is used as a segmentation prompt. These surprisingly\nsimple methods, without any further modifications, outperform all previous\napproaches by a considerable margin in both single and multi-object benchmarks.\nWe also extend these frame-level segmentations to sequence-level segmentations\nthat maintain object identity. Again, this simple model outperforms previous\nmethods on multiple video object segmentation benchmarks.\n", "link": "http://arxiv.org/abs/2404.12389v1", "date": "2024-04-18", "relevancy": 1.5702, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.529}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5171}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5157}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Moving%20Object%20Segmentation%3A%20All%20You%20Need%20Is%20SAM%20%28and%20Flow%29&body=Title%3A%20Moving%20Object%20Segmentation%3A%20All%20You%20Need%20Is%20SAM%20%28and%20Flow%29%0AAuthor%3A%20Junyu%20Xie%20and%20Charig%20Yang%20and%20Weidi%20Xie%20and%20Andrew%20Zisserman%0AAbstract%3A%20%20%20The%20objective%20of%20this%20paper%20is%20motion%20segmentation%20--%20discovering%20and%0Asegmenting%20the%20moving%20objects%20in%20a%20video.%20This%20is%20a%20much%20studied%20area%20with%0Anumerous%20careful%2Cand%20sometimes%20complex%2C%20approaches%20and%20training%20schemes%0Aincluding%3A%20self-supervised%20learning%2C%20learning%20from%20synthetic%20datasets%2C%0Aobject-centric%20representations%2C%20amodal%20representations%2C%20and%20many%20more.%20Our%0Ainterest%20in%20this%20paper%20is%20to%20determine%20if%20the%20Segment%20Anything%20model%20%28SAM%29%20can%0Acontribute%20to%20this%20task.%20We%20investigate%20two%20models%20for%20combining%20SAM%20with%0Aoptical%20flow%20that%20harness%20the%20segmentation%20power%20of%20SAM%20with%20the%20ability%20of%0Aflow%20to%20discover%20and%20group%20moving%20objects.%20In%20the%20first%20model%2C%20we%20adapt%20SAM%20to%0Atake%20optical%20flow%2C%20rather%20than%20RGB%2C%20as%20an%20input.%20In%20the%20second%2C%20SAM%20takes%20RGB%0Aas%20an%20input%2C%20and%20flow%20is%20used%20as%20a%20segmentation%20prompt.%20These%20surprisingly%0Asimple%20methods%2C%20without%20any%20further%20modifications%2C%20outperform%20all%20previous%0Aapproaches%20by%20a%20considerable%20margin%20in%20both%20single%20and%20multi-object%20benchmarks.%0AWe%20also%20extend%20these%20frame-level%20segmentations%20to%20sequence-level%20segmentations%0Athat%20maintain%20object%20identity.%20Again%2C%20this%20simple%20model%20outperforms%20previous%0Amethods%20on%20multiple%20video%20object%20segmentation%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12389v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Moving%20Object%20Segmentation%3A%20All%20You%20Need%20Is%20SAM%20%28and%20Flow%29&entry.906535625=Junyu%20Xie%20and%20Charig%20Yang%20and%20Weidi%20Xie%20and%20Andrew%20Zisserman&entry.1292438233=%20%20The%20objective%20of%20this%20paper%20is%20motion%20segmentation%20--%20discovering%20and%0Asegmenting%20the%20moving%20objects%20in%20a%20video.%20This%20is%20a%20much%20studied%20area%20with%0Anumerous%20careful%2Cand%20sometimes%20complex%2C%20approaches%20and%20training%20schemes%0Aincluding%3A%20self-supervised%20learning%2C%20learning%20from%20synthetic%20datasets%2C%0Aobject-centric%20representations%2C%20amodal%20representations%2C%20and%20many%20more.%20Our%0Ainterest%20in%20this%20paper%20is%20to%20determine%20if%20the%20Segment%20Anything%20model%20%28SAM%29%20can%0Acontribute%20to%20this%20task.%20We%20investigate%20two%20models%20for%20combining%20SAM%20with%0Aoptical%20flow%20that%20harness%20the%20segmentation%20power%20of%20SAM%20with%20the%20ability%20of%0Aflow%20to%20discover%20and%20group%20moving%20objects.%20In%20the%20first%20model%2C%20we%20adapt%20SAM%20to%0Atake%20optical%20flow%2C%20rather%20than%20RGB%2C%20as%20an%20input.%20In%20the%20second%2C%20SAM%20takes%20RGB%0Aas%20an%20input%2C%20and%20flow%20is%20used%20as%20a%20segmentation%20prompt.%20These%20surprisingly%0Asimple%20methods%2C%20without%20any%20further%20modifications%2C%20outperform%20all%20previous%0Aapproaches%20by%20a%20considerable%20margin%20in%20both%20single%20and%20multi-object%20benchmarks.%0AWe%20also%20extend%20these%20frame-level%20segmentations%20to%20sequence-level%20segmentations%0Athat%20maintain%20object%20identity.%20Again%2C%20this%20simple%20model%20outperforms%20previous%0Amethods%20on%20multiple%20video%20object%20segmentation%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12389v1&entry.124074799=Read"},
{"title": "Towards a Foundation Model for Partial Differential Equation:\n  Multi-Operator Learning and Extrapolation", "author": "Jingmin Sun and Yuxuan Liu and Zecheng Zhang and Hayden Schaeffer", "abstract": "  Foundation models, such as large language models, have demonstrated success\nin addressing various language and image processing tasks. In this work, we\nintroduce a multi-modal foundation model for scientific problems, named\nPROSE-PDE. Our model, designed for bi-modality to bi-modality learning, is a\nmulti-operator learning approach which can predict future states of\nspatiotemporal systems while concurrently learning the underlying governing\nequations of the physical system. Specifically, we focus on multi-operator\nlearning by training distinct one-dimensional time-dependent nonlinear constant\ncoefficient partial differential equations, with potential applications to many\nphysical applications including physics, geology, and biology. More\nimportantly, we provide three extrapolation studies to demonstrate that\nPROSE-PDE can generalize physical features through the robust training of\nmultiple operators and that the proposed model can extrapolate to predict PDE\nsolutions whose models or data were unseen during the training. Furthermore, we\nshow through systematic numerical experiments that the utilization of the\nsymbolic modality in our model effectively resolves the well-posedness problems\nwith training multiple operators and thus enhances our model's predictive\ncapabilities.\n", "link": "http://arxiv.org/abs/2404.12355v1", "date": "2024-04-18", "relevancy": 1.5599, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5549}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5116}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5059}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Foundation%20Model%20for%20Partial%20Differential%20Equation%3A%0A%20%20Multi-Operator%20Learning%20and%20Extrapolation&body=Title%3A%20Towards%20a%20Foundation%20Model%20for%20Partial%20Differential%20Equation%3A%0A%20%20Multi-Operator%20Learning%20and%20Extrapolation%0AAuthor%3A%20Jingmin%20Sun%20and%20Yuxuan%20Liu%20and%20Zecheng%20Zhang%20and%20Hayden%20Schaeffer%0AAbstract%3A%20%20%20Foundation%20models%2C%20such%20as%20large%20language%20models%2C%20have%20demonstrated%20success%0Ain%20addressing%20various%20language%20and%20image%20processing%20tasks.%20In%20this%20work%2C%20we%0Aintroduce%20a%20multi-modal%20foundation%20model%20for%20scientific%20problems%2C%20named%0APROSE-PDE.%20Our%20model%2C%20designed%20for%20bi-modality%20to%20bi-modality%20learning%2C%20is%20a%0Amulti-operator%20learning%20approach%20which%20can%20predict%20future%20states%20of%0Aspatiotemporal%20systems%20while%20concurrently%20learning%20the%20underlying%20governing%0Aequations%20of%20the%20physical%20system.%20Specifically%2C%20we%20focus%20on%20multi-operator%0Alearning%20by%20training%20distinct%20one-dimensional%20time-dependent%20nonlinear%20constant%0Acoefficient%20partial%20differential%20equations%2C%20with%20potential%20applications%20to%20many%0Aphysical%20applications%20including%20physics%2C%20geology%2C%20and%20biology.%20More%0Aimportantly%2C%20we%20provide%20three%20extrapolation%20studies%20to%20demonstrate%20that%0APROSE-PDE%20can%20generalize%20physical%20features%20through%20the%20robust%20training%20of%0Amultiple%20operators%20and%20that%20the%20proposed%20model%20can%20extrapolate%20to%20predict%20PDE%0Asolutions%20whose%20models%20or%20data%20were%20unseen%20during%20the%20training.%20Furthermore%2C%20we%0Ashow%20through%20systematic%20numerical%20experiments%20that%20the%20utilization%20of%20the%0Asymbolic%20modality%20in%20our%20model%20effectively%20resolves%20the%20well-posedness%20problems%0Awith%20training%20multiple%20operators%20and%20thus%20enhances%20our%20model%27s%20predictive%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12355v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Foundation%20Model%20for%20Partial%20Differential%20Equation%3A%0A%20%20Multi-Operator%20Learning%20and%20Extrapolation&entry.906535625=Jingmin%20Sun%20and%20Yuxuan%20Liu%20and%20Zecheng%20Zhang%20and%20Hayden%20Schaeffer&entry.1292438233=%20%20Foundation%20models%2C%20such%20as%20large%20language%20models%2C%20have%20demonstrated%20success%0Ain%20addressing%20various%20language%20and%20image%20processing%20tasks.%20In%20this%20work%2C%20we%0Aintroduce%20a%20multi-modal%20foundation%20model%20for%20scientific%20problems%2C%20named%0APROSE-PDE.%20Our%20model%2C%20designed%20for%20bi-modality%20to%20bi-modality%20learning%2C%20is%20a%0Amulti-operator%20learning%20approach%20which%20can%20predict%20future%20states%20of%0Aspatiotemporal%20systems%20while%20concurrently%20learning%20the%20underlying%20governing%0Aequations%20of%20the%20physical%20system.%20Specifically%2C%20we%20focus%20on%20multi-operator%0Alearning%20by%20training%20distinct%20one-dimensional%20time-dependent%20nonlinear%20constant%0Acoefficient%20partial%20differential%20equations%2C%20with%20potential%20applications%20to%20many%0Aphysical%20applications%20including%20physics%2C%20geology%2C%20and%20biology.%20More%0Aimportantly%2C%20we%20provide%20three%20extrapolation%20studies%20to%20demonstrate%20that%0APROSE-PDE%20can%20generalize%20physical%20features%20through%20the%20robust%20training%20of%0Amultiple%20operators%20and%20that%20the%20proposed%20model%20can%20extrapolate%20to%20predict%20PDE%0Asolutions%20whose%20models%20or%20data%20were%20unseen%20during%20the%20training.%20Furthermore%2C%20we%0Ashow%20through%20systematic%20numerical%20experiments%20that%20the%20utilization%20of%20the%0Asymbolic%20modality%20in%20our%20model%20effectively%20resolves%20the%20well-posedness%20problems%0Awith%20training%20multiple%20operators%20and%20thus%20enhances%20our%20model%27s%20predictive%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12355v1&entry.124074799=Read"},
{"title": "Deep Gaussian mixture model for unsupervised image segmentation", "author": "Matthias Schwab and Agnes Mayr and Markus Haltmeier", "abstract": "  The recent emergence of deep learning has led to a great deal of work on\ndesigning supervised deep semantic segmentation algorithms. As in many tasks\nsufficient pixel-level labels are very difficult to obtain, we propose a method\nwhich combines a Gaussian mixture model (GMM) with unsupervised deep learning\ntechniques. In the standard GMM the pixel values with each sub-region are\nmodelled by a Gaussian distribution. In order to identify the different\nregions, the parameter vector that minimizes the negative log-likelihood (NLL)\nfunction regarding the GMM has to be approximated. For this task, usually\niterative optimization methods such as the expectation-maximization (EM)\nalgorithm are used. In this paper, we propose to estimate these parameters\ndirectly from the image using a convolutional neural network (CNN). We thus\nchange the iterative procedure in the EM algorithm replacing the\nexpectation-step by a gradient-step with regard to the networks parameters.\nThis means that the network is trained to minimize the NLL function of the GMM\nwhich comes with at least two advantages. As once trained, the network is able\nto predict label probabilities very quickly compared with time consuming\niterative optimization methods. Secondly, due to the deep image prior our\nmethod is able to partially overcome one of the main disadvantages of GMM,\nwhich is not taking into account correlation between neighboring pixels, as it\nassumes independence between them. We demonstrate the advantages of our method\nin various experiments on the example of myocardial infarct segmentation on\nmulti-sequence MRI images.\n", "link": "http://arxiv.org/abs/2404.12252v1", "date": "2024-04-18", "relevancy": 1.5549, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.523}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5195}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.516}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Deep%20Gaussian%20mixture%20model%20for%20unsupervised%20image%20segmentation&body=Title%3A%20Deep%20Gaussian%20mixture%20model%20for%20unsupervised%20image%20segmentation%0AAuthor%3A%20Matthias%20Schwab%20and%20Agnes%20Mayr%20and%20Markus%20Haltmeier%0AAbstract%3A%20%20%20The%20recent%20emergence%20of%20deep%20learning%20has%20led%20to%20a%20great%20deal%20of%20work%20on%0Adesigning%20supervised%20deep%20semantic%20segmentation%20algorithms.%20As%20in%20many%20tasks%0Asufficient%20pixel-level%20labels%20are%20very%20difficult%20to%20obtain%2C%20we%20propose%20a%20method%0Awhich%20combines%20a%20Gaussian%20mixture%20model%20%28GMM%29%20with%20unsupervised%20deep%20learning%0Atechniques.%20In%20the%20standard%20GMM%20the%20pixel%20values%20with%20each%20sub-region%20are%0Amodelled%20by%20a%20Gaussian%20distribution.%20In%20order%20to%20identify%20the%20different%0Aregions%2C%20the%20parameter%20vector%20that%20minimizes%20the%20negative%20log-likelihood%20%28NLL%29%0Afunction%20regarding%20the%20GMM%20has%20to%20be%20approximated.%20For%20this%20task%2C%20usually%0Aiterative%20optimization%20methods%20such%20as%20the%20expectation-maximization%20%28EM%29%0Aalgorithm%20are%20used.%20In%20this%20paper%2C%20we%20propose%20to%20estimate%20these%20parameters%0Adirectly%20from%20the%20image%20using%20a%20convolutional%20neural%20network%20%28CNN%29.%20We%20thus%0Achange%20the%20iterative%20procedure%20in%20the%20EM%20algorithm%20replacing%20the%0Aexpectation-step%20by%20a%20gradient-step%20with%20regard%20to%20the%20networks%20parameters.%0AThis%20means%20that%20the%20network%20is%20trained%20to%20minimize%20the%20NLL%20function%20of%20the%20GMM%0Awhich%20comes%20with%20at%20least%20two%20advantages.%20As%20once%20trained%2C%20the%20network%20is%20able%0Ato%20predict%20label%20probabilities%20very%20quickly%20compared%20with%20time%20consuming%0Aiterative%20optimization%20methods.%20Secondly%2C%20due%20to%20the%20deep%20image%20prior%20our%0Amethod%20is%20able%20to%20partially%20overcome%20one%20of%20the%20main%20disadvantages%20of%20GMM%2C%0Awhich%20is%20not%20taking%20into%20account%20correlation%20between%20neighboring%20pixels%2C%20as%20it%0Aassumes%20independence%20between%20them.%20We%20demonstrate%20the%20advantages%20of%20our%20method%0Ain%20various%20experiments%20on%20the%20example%20of%20myocardial%20infarct%20segmentation%20on%0Amulti-sequence%20MRI%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12252v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Gaussian%20mixture%20model%20for%20unsupervised%20image%20segmentation&entry.906535625=Matthias%20Schwab%20and%20Agnes%20Mayr%20and%20Markus%20Haltmeier&entry.1292438233=%20%20The%20recent%20emergence%20of%20deep%20learning%20has%20led%20to%20a%20great%20deal%20of%20work%20on%0Adesigning%20supervised%20deep%20semantic%20segmentation%20algorithms.%20As%20in%20many%20tasks%0Asufficient%20pixel-level%20labels%20are%20very%20difficult%20to%20obtain%2C%20we%20propose%20a%20method%0Awhich%20combines%20a%20Gaussian%20mixture%20model%20%28GMM%29%20with%20unsupervised%20deep%20learning%0Atechniques.%20In%20the%20standard%20GMM%20the%20pixel%20values%20with%20each%20sub-region%20are%0Amodelled%20by%20a%20Gaussian%20distribution.%20In%20order%20to%20identify%20the%20different%0Aregions%2C%20the%20parameter%20vector%20that%20minimizes%20the%20negative%20log-likelihood%20%28NLL%29%0Afunction%20regarding%20the%20GMM%20has%20to%20be%20approximated.%20For%20this%20task%2C%20usually%0Aiterative%20optimization%20methods%20such%20as%20the%20expectation-maximization%20%28EM%29%0Aalgorithm%20are%20used.%20In%20this%20paper%2C%20we%20propose%20to%20estimate%20these%20parameters%0Adirectly%20from%20the%20image%20using%20a%20convolutional%20neural%20network%20%28CNN%29.%20We%20thus%0Achange%20the%20iterative%20procedure%20in%20the%20EM%20algorithm%20replacing%20the%0Aexpectation-step%20by%20a%20gradient-step%20with%20regard%20to%20the%20networks%20parameters.%0AThis%20means%20that%20the%20network%20is%20trained%20to%20minimize%20the%20NLL%20function%20of%20the%20GMM%0Awhich%20comes%20with%20at%20least%20two%20advantages.%20As%20once%20trained%2C%20the%20network%20is%20able%0Ato%20predict%20label%20probabilities%20very%20quickly%20compared%20with%20time%20consuming%0Aiterative%20optimization%20methods.%20Secondly%2C%20due%20to%20the%20deep%20image%20prior%20our%0Amethod%20is%20able%20to%20partially%20overcome%20one%20of%20the%20main%20disadvantages%20of%20GMM%2C%0Awhich%20is%20not%20taking%20into%20account%20correlation%20between%20neighboring%20pixels%2C%20as%20it%0Aassumes%20independence%20between%20them.%20We%20demonstrate%20the%20advantages%20of%20our%20method%0Ain%20various%20experiments%20on%20the%20example%20of%20myocardial%20infarct%20segmentation%20on%0Amulti-sequence%20MRI%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12252v1&entry.124074799=Read"},
{"title": "Performance Evaluation of Segment Anything Model with Variational\n  Prompting for Application to Non-Visible Spectrum Imagery", "author": "Yona Falinie A. Gaus and Neelanjan Bhowmik and Brian K. S. Isaac-Medina and Toby P. Breckon", "abstract": "  The Segment Anything Model (SAM) is a deep neural network foundational model\ndesigned to perform instance segmentation which has gained significant\npopularity given its zero-shot segmentation ability. SAM operates by generating\nmasks based on various input prompts such as text, bounding boxes, points, or\nmasks, introducing a novel methodology to overcome the constraints posed by\ndataset-specific scarcity. While SAM is trained on an extensive dataset,\ncomprising ~11M images, it mostly consists of natural photographic images with\nonly very limited images from other modalities. Whilst the rapid progress in\nvisual infrared surveillance and X-ray security screening imaging technologies,\ndriven forward by advances in deep learning, has significantly enhanced the\nability to detect, classify and segment objects with high accuracy, it is not\nevident if the SAM zero-shot capabilities can be transferred to such\nmodalities. This work assesses SAM capabilities in segmenting objects of\ninterest in the X-ray/infrared modalities. Our approach reuses the pre-trained\nSAM with three different prompts: bounding box, centroid and random points. We\npresent quantitative/qualitative results to showcase the performance on\nselected datasets. Our results show that SAM can segment objects in the X-ray\nmodality when given a box prompt, but its performance varies for point prompts.\nSpecifically, SAM performs poorly in segmenting slender objects and organic\nmaterials, such as plastic bottles. We find that infrared objects are also\nchallenging to segment with point prompts given the low-contrast nature of this\nmodality. This study shows that while SAM demonstrates outstanding zero-shot\ncapabilities with box prompts, its performance ranges from moderate to poor for\npoint prompts, indicating that special consideration on the cross-modal\ngeneralisation of SAM is needed when considering use on X-ray/infrared imagery.\n", "link": "http://arxiv.org/abs/2404.12285v1", "date": "2024-04-18", "relevancy": 1.5534, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5344}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5164}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5045}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Performance%20Evaluation%20of%20Segment%20Anything%20Model%20with%20Variational%0A%20%20Prompting%20for%20Application%20to%20Non-Visible%20Spectrum%20Imagery&body=Title%3A%20Performance%20Evaluation%20of%20Segment%20Anything%20Model%20with%20Variational%0A%20%20Prompting%20for%20Application%20to%20Non-Visible%20Spectrum%20Imagery%0AAuthor%3A%20Yona%20Falinie%20A.%20Gaus%20and%20Neelanjan%20Bhowmik%20and%20Brian%20K.%20S.%20Isaac-Medina%20and%20Toby%20P.%20Breckon%0AAbstract%3A%20%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20is%20a%20deep%20neural%20network%20foundational%20model%0Adesigned%20to%20perform%20instance%20segmentation%20which%20has%20gained%20significant%0Apopularity%20given%20its%20zero-shot%20segmentation%20ability.%20SAM%20operates%20by%20generating%0Amasks%20based%20on%20various%20input%20prompts%20such%20as%20text%2C%20bounding%20boxes%2C%20points%2C%20or%0Amasks%2C%20introducing%20a%20novel%20methodology%20to%20overcome%20the%20constraints%20posed%20by%0Adataset-specific%20scarcity.%20While%20SAM%20is%20trained%20on%20an%20extensive%20dataset%2C%0Acomprising%20~11M%20images%2C%20it%20mostly%20consists%20of%20natural%20photographic%20images%20with%0Aonly%20very%20limited%20images%20from%20other%20modalities.%20Whilst%20the%20rapid%20progress%20in%0Avisual%20infrared%20surveillance%20and%20X-ray%20security%20screening%20imaging%20technologies%2C%0Adriven%20forward%20by%20advances%20in%20deep%20learning%2C%20has%20significantly%20enhanced%20the%0Aability%20to%20detect%2C%20classify%20and%20segment%20objects%20with%20high%20accuracy%2C%20it%20is%20not%0Aevident%20if%20the%20SAM%20zero-shot%20capabilities%20can%20be%20transferred%20to%20such%0Amodalities.%20This%20work%20assesses%20SAM%20capabilities%20in%20segmenting%20objects%20of%0Ainterest%20in%20the%20X-ray/infrared%20modalities.%20Our%20approach%20reuses%20the%20pre-trained%0ASAM%20with%20three%20different%20prompts%3A%20bounding%20box%2C%20centroid%20and%20random%20points.%20We%0Apresent%20quantitative/qualitative%20results%20to%20showcase%20the%20performance%20on%0Aselected%20datasets.%20Our%20results%20show%20that%20SAM%20can%20segment%20objects%20in%20the%20X-ray%0Amodality%20when%20given%20a%20box%20prompt%2C%20but%20its%20performance%20varies%20for%20point%20prompts.%0ASpecifically%2C%20SAM%20performs%20poorly%20in%20segmenting%20slender%20objects%20and%20organic%0Amaterials%2C%20such%20as%20plastic%20bottles.%20We%20find%20that%20infrared%20objects%20are%20also%0Achallenging%20to%20segment%20with%20point%20prompts%20given%20the%20low-contrast%20nature%20of%20this%0Amodality.%20This%20study%20shows%20that%20while%20SAM%20demonstrates%20outstanding%20zero-shot%0Acapabilities%20with%20box%20prompts%2C%20its%20performance%20ranges%20from%20moderate%20to%20poor%20for%0Apoint%20prompts%2C%20indicating%20that%20special%20consideration%20on%20the%20cross-modal%0Ageneralisation%20of%20SAM%20is%20needed%20when%20considering%20use%20on%20X-ray/infrared%20imagery.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12285v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Performance%20Evaluation%20of%20Segment%20Anything%20Model%20with%20Variational%0A%20%20Prompting%20for%20Application%20to%20Non-Visible%20Spectrum%20Imagery&entry.906535625=Yona%20Falinie%20A.%20Gaus%20and%20Neelanjan%20Bhowmik%20and%20Brian%20K.%20S.%20Isaac-Medina%20and%20Toby%20P.%20Breckon&entry.1292438233=%20%20The%20Segment%20Anything%20Model%20%28SAM%29%20is%20a%20deep%20neural%20network%20foundational%20model%0Adesigned%20to%20perform%20instance%20segmentation%20which%20has%20gained%20significant%0Apopularity%20given%20its%20zero-shot%20segmentation%20ability.%20SAM%20operates%20by%20generating%0Amasks%20based%20on%20various%20input%20prompts%20such%20as%20text%2C%20bounding%20boxes%2C%20points%2C%20or%0Amasks%2C%20introducing%20a%20novel%20methodology%20to%20overcome%20the%20constraints%20posed%20by%0Adataset-specific%20scarcity.%20While%20SAM%20is%20trained%20on%20an%20extensive%20dataset%2C%0Acomprising%20~11M%20images%2C%20it%20mostly%20consists%20of%20natural%20photographic%20images%20with%0Aonly%20very%20limited%20images%20from%20other%20modalities.%20Whilst%20the%20rapid%20progress%20in%0Avisual%20infrared%20surveillance%20and%20X-ray%20security%20screening%20imaging%20technologies%2C%0Adriven%20forward%20by%20advances%20in%20deep%20learning%2C%20has%20significantly%20enhanced%20the%0Aability%20to%20detect%2C%20classify%20and%20segment%20objects%20with%20high%20accuracy%2C%20it%20is%20not%0Aevident%20if%20the%20SAM%20zero-shot%20capabilities%20can%20be%20transferred%20to%20such%0Amodalities.%20This%20work%20assesses%20SAM%20capabilities%20in%20segmenting%20objects%20of%0Ainterest%20in%20the%20X-ray/infrared%20modalities.%20Our%20approach%20reuses%20the%20pre-trained%0ASAM%20with%20three%20different%20prompts%3A%20bounding%20box%2C%20centroid%20and%20random%20points.%20We%0Apresent%20quantitative/qualitative%20results%20to%20showcase%20the%20performance%20on%0Aselected%20datasets.%20Our%20results%20show%20that%20SAM%20can%20segment%20objects%20in%20the%20X-ray%0Amodality%20when%20given%20a%20box%20prompt%2C%20but%20its%20performance%20varies%20for%20point%20prompts.%0ASpecifically%2C%20SAM%20performs%20poorly%20in%20segmenting%20slender%20objects%20and%20organic%0Amaterials%2C%20such%20as%20plastic%20bottles.%20We%20find%20that%20infrared%20objects%20are%20also%0Achallenging%20to%20segment%20with%20point%20prompts%20given%20the%20low-contrast%20nature%20of%20this%0Amodality.%20This%20study%20shows%20that%20while%20SAM%20demonstrates%20outstanding%20zero-shot%0Acapabilities%20with%20box%20prompts%2C%20its%20performance%20ranges%20from%20moderate%20to%20poor%20for%0Apoint%20prompts%2C%20indicating%20that%20special%20consideration%20on%20the%20cross-modal%0Ageneralisation%20of%20SAM%20is%20needed%20when%20considering%20use%20on%20X-ray/infrared%20imagery.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12285v1&entry.124074799=Read"},
{"title": "Advancing the Robustness of Large Language Models through Self-Denoised\n  Smoothing", "author": "Jiabao Ji and Bairu Hou and Zhen Zhang and Guanhua Zhang and Wenqi Fan and Qing Li and Yang Zhang and Gaowen Liu and Sijia Liu and Shiyu Chang", "abstract": "  Although large language models (LLMs) have achieved significant success,\ntheir vulnerability to adversarial perturbations, including recent jailbreak\nattacks, has raised considerable concerns. However, the increasing size of\nthese models and their limited access make improving their robustness a\nchallenging task. Among various defense strategies, randomized smoothing has\nshown great potential for LLMs, as it does not require full access to the\nmodel's parameters or fine-tuning via adversarial training. However, randomized\nsmoothing involves adding noise to the input before model prediction, and the\nfinal model's robustness largely depends on the model's performance on these\nnoise corrupted data. Its effectiveness is often limited by the model's\nsub-optimal performance on noisy data. To address this issue, we propose to\nleverage the multitasking nature of LLMs to first denoise the noisy inputs and\nthen to make predictions based on these denoised versions. We call this\nprocedure self-denoised smoothing. Unlike previous denoised smoothing\ntechniques in computer vision, which require training a separate model to\nenhance the robustness of LLMs, our method offers significantly better\nefficiency and flexibility. Our experimental results indicate that our method\nsurpasses existing methods in both empirical and certified robustness in\ndefending against adversarial attacks for both downstream tasks and human\nalignments (i.e., jailbreak attacks). Our code is publicly available at\nhttps://github.com/UCSB-NLP-Chang/SelfDenoise\n", "link": "http://arxiv.org/abs/2404.12274v1", "date": "2024-04-18", "relevancy": 1.5528, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.535}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5137}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5102}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Advancing%20the%20Robustness%20of%20Large%20Language%20Models%20through%20Self-Denoised%0A%20%20Smoothing&body=Title%3A%20Advancing%20the%20Robustness%20of%20Large%20Language%20Models%20through%20Self-Denoised%0A%20%20Smoothing%0AAuthor%3A%20Jiabao%20Ji%20and%20Bairu%20Hou%20and%20Zhen%20Zhang%20and%20Guanhua%20Zhang%20and%20Wenqi%20Fan%20and%20Qing%20Li%20and%20Yang%20Zhang%20and%20Gaowen%20Liu%20and%20Sijia%20Liu%20and%20Shiyu%20Chang%0AAbstract%3A%20%20%20Although%20large%20language%20models%20%28LLMs%29%20have%20achieved%20significant%20success%2C%0Atheir%20vulnerability%20to%20adversarial%20perturbations%2C%20including%20recent%20jailbreak%0Aattacks%2C%20has%20raised%20considerable%20concerns.%20However%2C%20the%20increasing%20size%20of%0Athese%20models%20and%20their%20limited%20access%20make%20improving%20their%20robustness%20a%0Achallenging%20task.%20Among%20various%20defense%20strategies%2C%20randomized%20smoothing%20has%0Ashown%20great%20potential%20for%20LLMs%2C%20as%20it%20does%20not%20require%20full%20access%20to%20the%0Amodel%27s%20parameters%20or%20fine-tuning%20via%20adversarial%20training.%20However%2C%20randomized%0Asmoothing%20involves%20adding%20noise%20to%20the%20input%20before%20model%20prediction%2C%20and%20the%0Afinal%20model%27s%20robustness%20largely%20depends%20on%20the%20model%27s%20performance%20on%20these%0Anoise%20corrupted%20data.%20Its%20effectiveness%20is%20often%20limited%20by%20the%20model%27s%0Asub-optimal%20performance%20on%20noisy%20data.%20To%20address%20this%20issue%2C%20we%20propose%20to%0Aleverage%20the%20multitasking%20nature%20of%20LLMs%20to%20first%20denoise%20the%20noisy%20inputs%20and%0Athen%20to%20make%20predictions%20based%20on%20these%20denoised%20versions.%20We%20call%20this%0Aprocedure%20self-denoised%20smoothing.%20Unlike%20previous%20denoised%20smoothing%0Atechniques%20in%20computer%20vision%2C%20which%20require%20training%20a%20separate%20model%20to%0Aenhance%20the%20robustness%20of%20LLMs%2C%20our%20method%20offers%20significantly%20better%0Aefficiency%20and%20flexibility.%20Our%20experimental%20results%20indicate%20that%20our%20method%0Asurpasses%20existing%20methods%20in%20both%20empirical%20and%20certified%20robustness%20in%0Adefending%20against%20adversarial%20attacks%20for%20both%20downstream%20tasks%20and%20human%0Aalignments%20%28i.e.%2C%20jailbreak%20attacks%29.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/UCSB-NLP-Chang/SelfDenoise%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12274v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Advancing%20the%20Robustness%20of%20Large%20Language%20Models%20through%20Self-Denoised%0A%20%20Smoothing&entry.906535625=Jiabao%20Ji%20and%20Bairu%20Hou%20and%20Zhen%20Zhang%20and%20Guanhua%20Zhang%20and%20Wenqi%20Fan%20and%20Qing%20Li%20and%20Yang%20Zhang%20and%20Gaowen%20Liu%20and%20Sijia%20Liu%20and%20Shiyu%20Chang&entry.1292438233=%20%20Although%20large%20language%20models%20%28LLMs%29%20have%20achieved%20significant%20success%2C%0Atheir%20vulnerability%20to%20adversarial%20perturbations%2C%20including%20recent%20jailbreak%0Aattacks%2C%20has%20raised%20considerable%20concerns.%20However%2C%20the%20increasing%20size%20of%0Athese%20models%20and%20their%20limited%20access%20make%20improving%20their%20robustness%20a%0Achallenging%20task.%20Among%20various%20defense%20strategies%2C%20randomized%20smoothing%20has%0Ashown%20great%20potential%20for%20LLMs%2C%20as%20it%20does%20not%20require%20full%20access%20to%20the%0Amodel%27s%20parameters%20or%20fine-tuning%20via%20adversarial%20training.%20However%2C%20randomized%0Asmoothing%20involves%20adding%20noise%20to%20the%20input%20before%20model%20prediction%2C%20and%20the%0Afinal%20model%27s%20robustness%20largely%20depends%20on%20the%20model%27s%20performance%20on%20these%0Anoise%20corrupted%20data.%20Its%20effectiveness%20is%20often%20limited%20by%20the%20model%27s%0Asub-optimal%20performance%20on%20noisy%20data.%20To%20address%20this%20issue%2C%20we%20propose%20to%0Aleverage%20the%20multitasking%20nature%20of%20LLMs%20to%20first%20denoise%20the%20noisy%20inputs%20and%0Athen%20to%20make%20predictions%20based%20on%20these%20denoised%20versions.%20We%20call%20this%0Aprocedure%20self-denoised%20smoothing.%20Unlike%20previous%20denoised%20smoothing%0Atechniques%20in%20computer%20vision%2C%20which%20require%20training%20a%20separate%20model%20to%0Aenhance%20the%20robustness%20of%20LLMs%2C%20our%20method%20offers%20significantly%20better%0Aefficiency%20and%20flexibility.%20Our%20experimental%20results%20indicate%20that%20our%20method%0Asurpasses%20existing%20methods%20in%20both%20empirical%20and%20certified%20robustness%20in%0Adefending%20against%20adversarial%20attacks%20for%20both%20downstream%20tasks%20and%20human%0Aalignments%20%28i.e.%2C%20jailbreak%20attacks%29.%20Our%20code%20is%20publicly%20available%20at%0Ahttps%3A//github.com/UCSB-NLP-Chang/SelfDenoise%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12274v1&entry.124074799=Read"},
{"title": "BLINK: Multimodal Large Language Models Can See but Not Perceive", "author": "Xingyu Fu and Yushi Hu and Bangzheng Li and Yu Feng and Haoyu Wang and Xudong Lin and Dan Roth and Noah A. Smith and Wei-Chiu Ma and Ranjay Krishna", "abstract": "  We introduce Blink, a new benchmark for multimodal language models (LLMs)\nthat focuses on core visual perception abilities not found in other\nevaluations. Most of the Blink tasks can be solved by humans \"within a blink\"\n(e.g., relative depth estimation, visual correspondence, forensics detection,\nand multi-view reasoning). However, we find these perception-demanding tasks\ncast significant challenges for current multimodal LLMs because they resist\nmediation through natural language. Blink reformats 14 classic computer vision\ntasks into 3,807 multiple-choice questions, paired with single or multiple\nimages and visual prompting. While humans get 95.70% accuracy on average, Blink\nis surprisingly challenging for existing multimodal LLMs: even the\nbest-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only\n13.17% and 7.63% higher than random guessing, indicating that such perception\nabilities have not \"emerged\" yet in recent multimodal LLMs. Our analysis also\nhighlights that specialist CV models could solve these problems much better,\nsuggesting potential pathways for future improvements. We believe Blink will\nstimulate the community to help multimodal LLMs catch up with human-level\nvisual perception.\n", "link": "http://arxiv.org/abs/2404.12390v1", "date": "2024-04-18", "relevancy": 1.5503, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5271}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5092}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4984}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20BLINK%3A%20Multimodal%20Large%20Language%20Models%20Can%20See%20but%20Not%20Perceive&body=Title%3A%20BLINK%3A%20Multimodal%20Large%20Language%20Models%20Can%20See%20but%20Not%20Perceive%0AAuthor%3A%20Xingyu%20Fu%20and%20Yushi%20Hu%20and%20Bangzheng%20Li%20and%20Yu%20Feng%20and%20Haoyu%20Wang%20and%20Xudong%20Lin%20and%20Dan%20Roth%20and%20Noah%20A.%20Smith%20and%20Wei-Chiu%20Ma%20and%20Ranjay%20Krishna%0AAbstract%3A%20%20%20We%20introduce%20Blink%2C%20a%20new%20benchmark%20for%20multimodal%20language%20models%20%28LLMs%29%0Athat%20focuses%20on%20core%20visual%20perception%20abilities%20not%20found%20in%20other%0Aevaluations.%20Most%20of%20the%20Blink%20tasks%20can%20be%20solved%20by%20humans%20%22within%20a%20blink%22%0A%28e.g.%2C%20relative%20depth%20estimation%2C%20visual%20correspondence%2C%20forensics%20detection%2C%0Aand%20multi-view%20reasoning%29.%20However%2C%20we%20find%20these%20perception-demanding%20tasks%0Acast%20significant%20challenges%20for%20current%20multimodal%20LLMs%20because%20they%20resist%0Amediation%20through%20natural%20language.%20Blink%20reformats%2014%20classic%20computer%20vision%0Atasks%20into%203%2C807%20multiple-choice%20questions%2C%20paired%20with%20single%20or%20multiple%0Aimages%20and%20visual%20prompting.%20While%20humans%20get%2095.70%25%20accuracy%20on%20average%2C%20Blink%0Ais%20surprisingly%20challenging%20for%20existing%20multimodal%20LLMs%3A%20even%20the%0Abest-performing%20GPT-4V%20and%20Gemini%20achieve%20accuracies%20of%2051.26%25%20and%2045.72%25%2C%20only%0A13.17%25%20and%207.63%25%20higher%20than%20random%20guessing%2C%20indicating%20that%20such%20perception%0Aabilities%20have%20not%20%22emerged%22%20yet%20in%20recent%20multimodal%20LLMs.%20Our%20analysis%20also%0Ahighlights%20that%20specialist%20CV%20models%20could%20solve%20these%20problems%20much%20better%2C%0Asuggesting%20potential%20pathways%20for%20future%20improvements.%20We%20believe%20Blink%20will%0Astimulate%20the%20community%20to%20help%20multimodal%20LLMs%20catch%20up%20with%20human-level%0Avisual%20perception.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12390v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLINK%3A%20Multimodal%20Large%20Language%20Models%20Can%20See%20but%20Not%20Perceive&entry.906535625=Xingyu%20Fu%20and%20Yushi%20Hu%20and%20Bangzheng%20Li%20and%20Yu%20Feng%20and%20Haoyu%20Wang%20and%20Xudong%20Lin%20and%20Dan%20Roth%20and%20Noah%20A.%20Smith%20and%20Wei-Chiu%20Ma%20and%20Ranjay%20Krishna&entry.1292438233=%20%20We%20introduce%20Blink%2C%20a%20new%20benchmark%20for%20multimodal%20language%20models%20%28LLMs%29%0Athat%20focuses%20on%20core%20visual%20perception%20abilities%20not%20found%20in%20other%0Aevaluations.%20Most%20of%20the%20Blink%20tasks%20can%20be%20solved%20by%20humans%20%22within%20a%20blink%22%0A%28e.g.%2C%20relative%20depth%20estimation%2C%20visual%20correspondence%2C%20forensics%20detection%2C%0Aand%20multi-view%20reasoning%29.%20However%2C%20we%20find%20these%20perception-demanding%20tasks%0Acast%20significant%20challenges%20for%20current%20multimodal%20LLMs%20because%20they%20resist%0Amediation%20through%20natural%20language.%20Blink%20reformats%2014%20classic%20computer%20vision%0Atasks%20into%203%2C807%20multiple-choice%20questions%2C%20paired%20with%20single%20or%20multiple%0Aimages%20and%20visual%20prompting.%20While%20humans%20get%2095.70%25%20accuracy%20on%20average%2C%20Blink%0Ais%20surprisingly%20challenging%20for%20existing%20multimodal%20LLMs%3A%20even%20the%0Abest-performing%20GPT-4V%20and%20Gemini%20achieve%20accuracies%20of%2051.26%25%20and%2045.72%25%2C%20only%0A13.17%25%20and%207.63%25%20higher%20than%20random%20guessing%2C%20indicating%20that%20such%20perception%0Aabilities%20have%20not%20%22emerged%22%20yet%20in%20recent%20multimodal%20LLMs.%20Our%20analysis%20also%0Ahighlights%20that%20specialist%20CV%20models%20could%20solve%20these%20problems%20much%20better%2C%0Asuggesting%20potential%20pathways%20for%20future%20improvements.%20We%20believe%20Blink%20will%0Astimulate%20the%20community%20to%20help%20multimodal%20LLMs%20catch%20up%20with%20human-level%0Avisual%20perception.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12390v1&entry.124074799=Read"},
{"title": "Large Language Models for Synthetic Participatory Planning of Shared\n  Automated Electric Mobility Systems", "author": "Jiangbo Yu", "abstract": "  Unleashing the synergies of rapidly evolving mobility technologies in a\nmulti-stakeholder landscape presents unique challenges and opportunities for\naddressing urban transportation problems. This paper introduces a novel\nsynthetic participatory method, critically leveraging large language models\n(LLMs) to create digital avatars representing diverse stakeholders to plan\nshared automated electric mobility systems (SAEMS). These calibratable agents\ncollaboratively identify objectives, envision and evaluate SAEMS alternatives,\nand strategize implementation under risks and constraints. The results of a\nMontreal case study indicate that a structured and parameterized workflow\nprovides outputs with high controllability and comprehensiveness on an SAEMS\nplan than generated using a single LLM-enabled expert agent. Consequently, the\napproach provides a promising avenue for cost-efficiently improving the\ninclusivity and interpretability of multi-objective transportation planning,\nsuggesting a paradigm shift in how we envision and strategize for sustainable\nand equitable transportation systems.\n", "link": "http://arxiv.org/abs/2404.12317v1", "date": "2024-04-18", "relevancy": 1.5472, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5323}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5239}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5059}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20for%20Synthetic%20Participatory%20Planning%20of%20Shared%0A%20%20Automated%20Electric%20Mobility%20Systems&body=Title%3A%20Large%20Language%20Models%20for%20Synthetic%20Participatory%20Planning%20of%20Shared%0A%20%20Automated%20Electric%20Mobility%20Systems%0AAuthor%3A%20Jiangbo%20Yu%0AAbstract%3A%20%20%20Unleashing%20the%20synergies%20of%20rapidly%20evolving%20mobility%20technologies%20in%20a%0Amulti-stakeholder%20landscape%20presents%20unique%20challenges%20and%20opportunities%20for%0Aaddressing%20urban%20transportation%20problems.%20This%20paper%20introduces%20a%20novel%0Asynthetic%20participatory%20method%2C%20critically%20leveraging%20large%20language%20models%0A%28LLMs%29%20to%20create%20digital%20avatars%20representing%20diverse%20stakeholders%20to%20plan%0Ashared%20automated%20electric%20mobility%20systems%20%28SAEMS%29.%20These%20calibratable%20agents%0Acollaboratively%20identify%20objectives%2C%20envision%20and%20evaluate%20SAEMS%20alternatives%2C%0Aand%20strategize%20implementation%20under%20risks%20and%20constraints.%20The%20results%20of%20a%0AMontreal%20case%20study%20indicate%20that%20a%20structured%20and%20parameterized%20workflow%0Aprovides%20outputs%20with%20high%20controllability%20and%20comprehensiveness%20on%20an%20SAEMS%0Aplan%20than%20generated%20using%20a%20single%20LLM-enabled%20expert%20agent.%20Consequently%2C%20the%0Aapproach%20provides%20a%20promising%20avenue%20for%20cost-efficiently%20improving%20the%0Ainclusivity%20and%20interpretability%20of%20multi-objective%20transportation%20planning%2C%0Asuggesting%20a%20paradigm%20shift%20in%20how%20we%20envision%20and%20strategize%20for%20sustainable%0Aand%20equitable%20transportation%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12317v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20for%20Synthetic%20Participatory%20Planning%20of%20Shared%0A%20%20Automated%20Electric%20Mobility%20Systems&entry.906535625=Jiangbo%20Yu&entry.1292438233=%20%20Unleashing%20the%20synergies%20of%20rapidly%20evolving%20mobility%20technologies%20in%20a%0Amulti-stakeholder%20landscape%20presents%20unique%20challenges%20and%20opportunities%20for%0Aaddressing%20urban%20transportation%20problems.%20This%20paper%20introduces%20a%20novel%0Asynthetic%20participatory%20method%2C%20critically%20leveraging%20large%20language%20models%0A%28LLMs%29%20to%20create%20digital%20avatars%20representing%20diverse%20stakeholders%20to%20plan%0Ashared%20automated%20electric%20mobility%20systems%20%28SAEMS%29.%20These%20calibratable%20agents%0Acollaboratively%20identify%20objectives%2C%20envision%20and%20evaluate%20SAEMS%20alternatives%2C%0Aand%20strategize%20implementation%20under%20risks%20and%20constraints.%20The%20results%20of%20a%0AMontreal%20case%20study%20indicate%20that%20a%20structured%20and%20parameterized%20workflow%0Aprovides%20outputs%20with%20high%20controllability%20and%20comprehensiveness%20on%20an%20SAEMS%0Aplan%20than%20generated%20using%20a%20single%20LLM-enabled%20expert%20agent.%20Consequently%2C%20the%0Aapproach%20provides%20a%20promising%20avenue%20for%20cost-efficiently%20improving%20the%0Ainclusivity%20and%20interpretability%20of%20multi-objective%20transportation%20planning%2C%0Asuggesting%20a%20paradigm%20shift%20in%20how%20we%20envision%20and%20strategize%20for%20sustainable%0Aand%20equitable%20transportation%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12317v1&entry.124074799=Read"},
{"title": "A Perspective on Deep Vision Performance with Standard Image and Video\n  Codecs", "author": "Christoph Reich and Oliver Hahn and Daniel Cremers and Stefan Roth and Biplob Debnath", "abstract": "  Resource-constrained hardware, such as edge devices or cell phones, often\nrely on cloud servers to provide the required computational resources for\ninference in deep vision models. However, transferring image and video data\nfrom an edge or mobile device to a cloud server requires coding to deal with\nnetwork constraints. The use of standardized codecs, such as JPEG or H.264, is\nprevalent and required to ensure interoperability. This paper aims to examine\nthe implications of employing standardized codecs within deep vision pipelines.\nWe find that using JPEG and H.264 coding significantly deteriorates the\naccuracy across a broad range of vision tasks and models. For instance, strong\ncompression rates reduce semantic segmentation accuracy by more than 80% in\nmIoU. In contrast to previous findings, our analysis extends beyond image and\naction classification to localization and dense prediction tasks, thus\nproviding a more comprehensive perspective.\n", "link": "http://arxiv.org/abs/2404.12330v1", "date": "2024-04-18", "relevancy": 1.5373, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5452}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5057}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4966}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Perspective%20on%20Deep%20Vision%20Performance%20with%20Standard%20Image%20and%20Video%0A%20%20Codecs&body=Title%3A%20A%20Perspective%20on%20Deep%20Vision%20Performance%20with%20Standard%20Image%20and%20Video%0A%20%20Codecs%0AAuthor%3A%20Christoph%20Reich%20and%20Oliver%20Hahn%20and%20Daniel%20Cremers%20and%20Stefan%20Roth%20and%20Biplob%20Debnath%0AAbstract%3A%20%20%20Resource-constrained%20hardware%2C%20such%20as%20edge%20devices%20or%20cell%20phones%2C%20often%0Arely%20on%20cloud%20servers%20to%20provide%20the%20required%20computational%20resources%20for%0Ainference%20in%20deep%20vision%20models.%20However%2C%20transferring%20image%20and%20video%20data%0Afrom%20an%20edge%20or%20mobile%20device%20to%20a%20cloud%20server%20requires%20coding%20to%20deal%20with%0Anetwork%20constraints.%20The%20use%20of%20standardized%20codecs%2C%20such%20as%20JPEG%20or%20H.264%2C%20is%0Aprevalent%20and%20required%20to%20ensure%20interoperability.%20This%20paper%20aims%20to%20examine%0Athe%20implications%20of%20employing%20standardized%20codecs%20within%20deep%20vision%20pipelines.%0AWe%20find%20that%20using%20JPEG%20and%20H.264%20coding%20significantly%20deteriorates%20the%0Aaccuracy%20across%20a%20broad%20range%20of%20vision%20tasks%20and%20models.%20For%20instance%2C%20strong%0Acompression%20rates%20reduce%20semantic%20segmentation%20accuracy%20by%20more%20than%2080%25%20in%0AmIoU.%20In%20contrast%20to%20previous%20findings%2C%20our%20analysis%20extends%20beyond%20image%20and%0Aaction%20classification%20to%20localization%20and%20dense%20prediction%20tasks%2C%20thus%0Aproviding%20a%20more%20comprehensive%20perspective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12330v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Perspective%20on%20Deep%20Vision%20Performance%20with%20Standard%20Image%20and%20Video%0A%20%20Codecs&entry.906535625=Christoph%20Reich%20and%20Oliver%20Hahn%20and%20Daniel%20Cremers%20and%20Stefan%20Roth%20and%20Biplob%20Debnath&entry.1292438233=%20%20Resource-constrained%20hardware%2C%20such%20as%20edge%20devices%20or%20cell%20phones%2C%20often%0Arely%20on%20cloud%20servers%20to%20provide%20the%20required%20computational%20resources%20for%0Ainference%20in%20deep%20vision%20models.%20However%2C%20transferring%20image%20and%20video%20data%0Afrom%20an%20edge%20or%20mobile%20device%20to%20a%20cloud%20server%20requires%20coding%20to%20deal%20with%0Anetwork%20constraints.%20The%20use%20of%20standardized%20codecs%2C%20such%20as%20JPEG%20or%20H.264%2C%20is%0Aprevalent%20and%20required%20to%20ensure%20interoperability.%20This%20paper%20aims%20to%20examine%0Athe%20implications%20of%20employing%20standardized%20codecs%20within%20deep%20vision%20pipelines.%0AWe%20find%20that%20using%20JPEG%20and%20H.264%20coding%20significantly%20deteriorates%20the%0Aaccuracy%20across%20a%20broad%20range%20of%20vision%20tasks%20and%20models.%20For%20instance%2C%20strong%0Acompression%20rates%20reduce%20semantic%20segmentation%20accuracy%20by%20more%20than%2080%25%20in%0AmIoU.%20In%20contrast%20to%20previous%20findings%2C%20our%20analysis%20extends%20beyond%20image%20and%0Aaction%20classification%20to%20localization%20and%20dense%20prediction%20tasks%2C%20thus%0Aproviding%20a%20more%20comprehensive%20perspective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12330v1&entry.124074799=Read"},
{"title": "Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language\n  Models", "author": "Aitor Ormazabal and Che Zheng and Cyprien de Masson d'Autume and Dani Yogatama and Deyu Fu and Donovan Ong and Eric Chen and Eugenie Lamprecht and Hai Pham and Isaac Ong and Kaloyan Aleksiev and Lei Li and Matthew Henderson and Max Bain and Mikel Artetxe and Nishant Relan and Piotr Padlewski and Qi Liu and Ren Chen and Samuel Phua and Yazheng Yang and Yi Tay and Yuqi Wang and Zhongkai Zhu and Zhihui Xie", "abstract": "  We introduce Reka Core, Flash, and Edge, a series of powerful multimodal\nlanguage models trained from scratch by Reka. Reka models are able to process\nand reason with text, images, video, and audio inputs. This technical report\ndiscusses details of training some of these models and provides comprehensive\nevaluation results. We show that Reka Edge and Reka Flash are not only\nstate-of-the-art but also outperform many much larger models, delivering\noutsized values for their respective compute class. Meanwhile, our most capable\nand largest model, Reka Core, approaches the best frontier models on both\nautomatic evaluations and blind human evaluations. On image question answering\nbenchmarks (e.g. MMMU, VQAv2), Core performs competitively to GPT4-V.\nMeanwhile, on multimodal chat, Core ranks as the second most preferred model\nunder a blind third-party human evaluation setup, outperforming other models\nsuch as Claude 3 Opus. On text benchmarks, Core not only performs competitively\nto other frontier models on a set of well-established benchmarks (e.g. MMLU,\nGSM8K) but also outperforms GPT4-0613 on human evaluation. On video question\nanswering (Perception-Test), Core outperforms Gemini Ultra. Models are shipped\nin production at http://chat.reka.ai . A showcase of non cherry picked\nqualitative examples can also be found at http://showcase.reka.ai .\n", "link": "http://arxiv.org/abs/2404.12387v1", "date": "2024-04-18", "relevancy": 1.5337, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5204}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5013}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4983}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Reka%20Core%2C%20Flash%2C%20and%20Edge%3A%20A%20Series%20of%20Powerful%20Multimodal%20Language%0A%20%20Models&body=Title%3A%20Reka%20Core%2C%20Flash%2C%20and%20Edge%3A%20A%20Series%20of%20Powerful%20Multimodal%20Language%0A%20%20Models%0AAuthor%3A%20Aitor%20Ormazabal%20and%20Che%20Zheng%20and%20Cyprien%20de%20Masson%20d%27Autume%20and%20Dani%20Yogatama%20and%20Deyu%20Fu%20and%20Donovan%20Ong%20and%20Eric%20Chen%20and%20Eugenie%20Lamprecht%20and%20Hai%20Pham%20and%20Isaac%20Ong%20and%20Kaloyan%20Aleksiev%20and%20Lei%20Li%20and%20Matthew%20Henderson%20and%20Max%20Bain%20and%20Mikel%20Artetxe%20and%20Nishant%20Relan%20and%20Piotr%20Padlewski%20and%20Qi%20Liu%20and%20Ren%20Chen%20and%20Samuel%20Phua%20and%20Yazheng%20Yang%20and%20Yi%20Tay%20and%20Yuqi%20Wang%20and%20Zhongkai%20Zhu%20and%20Zhihui%20Xie%0AAbstract%3A%20%20%20We%20introduce%20Reka%20Core%2C%20Flash%2C%20and%20Edge%2C%20a%20series%20of%20powerful%20multimodal%0Alanguage%20models%20trained%20from%20scratch%20by%20Reka.%20Reka%20models%20are%20able%20to%20process%0Aand%20reason%20with%20text%2C%20images%2C%20video%2C%20and%20audio%20inputs.%20This%20technical%20report%0Adiscusses%20details%20of%20training%20some%20of%20these%20models%20and%20provides%20comprehensive%0Aevaluation%20results.%20We%20show%20that%20Reka%20Edge%20and%20Reka%20Flash%20are%20not%20only%0Astate-of-the-art%20but%20also%20outperform%20many%20much%20larger%20models%2C%20delivering%0Aoutsized%20values%20for%20their%20respective%20compute%20class.%20Meanwhile%2C%20our%20most%20capable%0Aand%20largest%20model%2C%20Reka%20Core%2C%20approaches%20the%20best%20frontier%20models%20on%20both%0Aautomatic%20evaluations%20and%20blind%20human%20evaluations.%20On%20image%20question%20answering%0Abenchmarks%20%28e.g.%20MMMU%2C%20VQAv2%29%2C%20Core%20performs%20competitively%20to%20GPT4-V.%0AMeanwhile%2C%20on%20multimodal%20chat%2C%20Core%20ranks%20as%20the%20second%20most%20preferred%20model%0Aunder%20a%20blind%20third-party%20human%20evaluation%20setup%2C%20outperforming%20other%20models%0Asuch%20as%20Claude%203%20Opus.%20On%20text%20benchmarks%2C%20Core%20not%20only%20performs%20competitively%0Ato%20other%20frontier%20models%20on%20a%20set%20of%20well-established%20benchmarks%20%28e.g.%20MMLU%2C%0AGSM8K%29%20but%20also%20outperforms%20GPT4-0613%20on%20human%20evaluation.%20On%20video%20question%0Aanswering%20%28Perception-Test%29%2C%20Core%20outperforms%20Gemini%20Ultra.%20Models%20are%20shipped%0Ain%20production%20at%20http%3A//chat.reka.ai%20.%20A%20showcase%20of%20non%20cherry%20picked%0Aqualitative%20examples%20can%20also%20be%20found%20at%20http%3A//showcase.reka.ai%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12387v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reka%20Core%2C%20Flash%2C%20and%20Edge%3A%20A%20Series%20of%20Powerful%20Multimodal%20Language%0A%20%20Models&entry.906535625=Aitor%20Ormazabal%20and%20Che%20Zheng%20and%20Cyprien%20de%20Masson%20d%27Autume%20and%20Dani%20Yogatama%20and%20Deyu%20Fu%20and%20Donovan%20Ong%20and%20Eric%20Chen%20and%20Eugenie%20Lamprecht%20and%20Hai%20Pham%20and%20Isaac%20Ong%20and%20Kaloyan%20Aleksiev%20and%20Lei%20Li%20and%20Matthew%20Henderson%20and%20Max%20Bain%20and%20Mikel%20Artetxe%20and%20Nishant%20Relan%20and%20Piotr%20Padlewski%20and%20Qi%20Liu%20and%20Ren%20Chen%20and%20Samuel%20Phua%20and%20Yazheng%20Yang%20and%20Yi%20Tay%20and%20Yuqi%20Wang%20and%20Zhongkai%20Zhu%20and%20Zhihui%20Xie&entry.1292438233=%20%20We%20introduce%20Reka%20Core%2C%20Flash%2C%20and%20Edge%2C%20a%20series%20of%20powerful%20multimodal%0Alanguage%20models%20trained%20from%20scratch%20by%20Reka.%20Reka%20models%20are%20able%20to%20process%0Aand%20reason%20with%20text%2C%20images%2C%20video%2C%20and%20audio%20inputs.%20This%20technical%20report%0Adiscusses%20details%20of%20training%20some%20of%20these%20models%20and%20provides%20comprehensive%0Aevaluation%20results.%20We%20show%20that%20Reka%20Edge%20and%20Reka%20Flash%20are%20not%20only%0Astate-of-the-art%20but%20also%20outperform%20many%20much%20larger%20models%2C%20delivering%0Aoutsized%20values%20for%20their%20respective%20compute%20class.%20Meanwhile%2C%20our%20most%20capable%0Aand%20largest%20model%2C%20Reka%20Core%2C%20approaches%20the%20best%20frontier%20models%20on%20both%0Aautomatic%20evaluations%20and%20blind%20human%20evaluations.%20On%20image%20question%20answering%0Abenchmarks%20%28e.g.%20MMMU%2C%20VQAv2%29%2C%20Core%20performs%20competitively%20to%20GPT4-V.%0AMeanwhile%2C%20on%20multimodal%20chat%2C%20Core%20ranks%20as%20the%20second%20most%20preferred%20model%0Aunder%20a%20blind%20third-party%20human%20evaluation%20setup%2C%20outperforming%20other%20models%0Asuch%20as%20Claude%203%20Opus.%20On%20text%20benchmarks%2C%20Core%20not%20only%20performs%20competitively%0Ato%20other%20frontier%20models%20on%20a%20set%20of%20well-established%20benchmarks%20%28e.g.%20MMLU%2C%0AGSM8K%29%20but%20also%20outperforms%20GPT4-0613%20on%20human%20evaluation.%20On%20video%20question%0Aanswering%20%28Perception-Test%29%2C%20Core%20outperforms%20Gemini%20Ultra.%20Models%20are%20shipped%0Ain%20production%20at%20http%3A//chat.reka.ai%20.%20A%20showcase%20of%20non%20cherry%20picked%0Aqualitative%20examples%20can%20also%20be%20found%20at%20http%3A//showcase.reka.ai%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12387v1&entry.124074799=Read"},
{"title": "A Mean-Field Analysis of Neural Gradient Descent-Ascent: Applications to\n  Functional Conditional Moment Equations", "author": "Yuchen Zhu and Yufeng Zhang and Zhaoran Wang and Zhuoran Yang and Xiaohong Chen", "abstract": "  We study minimax optimization problems defined over infinite-dimensional\nfunction classes. In particular, we restrict the functions to the class of\noverparameterized two-layer neural networks and study (i) the convergence of\nthe gradient descent-ascent algorithm and (ii) the representation learning of\nthe neural network. As an initial step, we consider the minimax optimization\nproblem stemming from estimating a functional equation defined by conditional\nexpectations via adversarial estimation, where the objective function is\nquadratic in the functional space. For this problem, we establish convergence\nunder the mean-field regime by considering the continuous-time and\ninfinite-width limit of the optimization dynamics. Under this regime, gradient\ndescent-ascent corresponds to a Wasserstein gradient flow over the space of\nprobability measures defined over the space of neural network parameters. We\nprove that the Wasserstein gradient flow converges globally to a stationary\npoint of the minimax objective at a $\\mathcal{O}(T^{-1} + \\alpha^{-1} ) $\nsublinear rate, and additionally finds the solution to the functional equation\nwhen the regularizer of the minimax objective is strongly convex. Here $T$\ndenotes the time and $\\alpha$ is a scaling parameter of the neural network. In\nterms of representation learning, our results show that the feature\nrepresentation induced by the neural networks is allowed to deviate from the\ninitial one by the magnitude of $\\mathcal{O}(\\alpha^{-1})$, measured in terms\nof the Wasserstein distance. Finally, we apply our general results to concrete\nexamples including policy evaluation, nonparametric instrumental variable\nregression, and asset pricing.\n", "link": "http://arxiv.org/abs/2404.12312v1", "date": "2024-04-18", "relevancy": 1.5269, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5224}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5098}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4747}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Mean-Field%20Analysis%20of%20Neural%20Gradient%20Descent-Ascent%3A%20Applications%20to%0A%20%20Functional%20Conditional%20Moment%20Equations&body=Title%3A%20A%20Mean-Field%20Analysis%20of%20Neural%20Gradient%20Descent-Ascent%3A%20Applications%20to%0A%20%20Functional%20Conditional%20Moment%20Equations%0AAuthor%3A%20Yuchen%20Zhu%20and%20Yufeng%20Zhang%20and%20Zhaoran%20Wang%20and%20Zhuoran%20Yang%20and%20Xiaohong%20Chen%0AAbstract%3A%20%20%20We%20study%20minimax%20optimization%20problems%20defined%20over%20infinite-dimensional%0Afunction%20classes.%20In%20particular%2C%20we%20restrict%20the%20functions%20to%20the%20class%20of%0Aoverparameterized%20two-layer%20neural%20networks%20and%20study%20%28i%29%20the%20convergence%20of%0Athe%20gradient%20descent-ascent%20algorithm%20and%20%28ii%29%20the%20representation%20learning%20of%0Athe%20neural%20network.%20As%20an%20initial%20step%2C%20we%20consider%20the%20minimax%20optimization%0Aproblem%20stemming%20from%20estimating%20a%20functional%20equation%20defined%20by%20conditional%0Aexpectations%20via%20adversarial%20estimation%2C%20where%20the%20objective%20function%20is%0Aquadratic%20in%20the%20functional%20space.%20For%20this%20problem%2C%20we%20establish%20convergence%0Aunder%20the%20mean-field%20regime%20by%20considering%20the%20continuous-time%20and%0Ainfinite-width%20limit%20of%20the%20optimization%20dynamics.%20Under%20this%20regime%2C%20gradient%0Adescent-ascent%20corresponds%20to%20a%20Wasserstein%20gradient%20flow%20over%20the%20space%20of%0Aprobability%20measures%20defined%20over%20the%20space%20of%20neural%20network%20parameters.%20We%0Aprove%20that%20the%20Wasserstein%20gradient%20flow%20converges%20globally%20to%20a%20stationary%0Apoint%20of%20the%20minimax%20objective%20at%20a%20%24%5Cmathcal%7BO%7D%28T%5E%7B-1%7D%20%2B%20%5Calpha%5E%7B-1%7D%20%29%20%24%0Asublinear%20rate%2C%20and%20additionally%20finds%20the%20solution%20to%20the%20functional%20equation%0Awhen%20the%20regularizer%20of%20the%20minimax%20objective%20is%20strongly%20convex.%20Here%20%24T%24%0Adenotes%20the%20time%20and%20%24%5Calpha%24%20is%20a%20scaling%20parameter%20of%20the%20neural%20network.%20In%0Aterms%20of%20representation%20learning%2C%20our%20results%20show%20that%20the%20feature%0Arepresentation%20induced%20by%20the%20neural%20networks%20is%20allowed%20to%20deviate%20from%20the%0Ainitial%20one%20by%20the%20magnitude%20of%20%24%5Cmathcal%7BO%7D%28%5Calpha%5E%7B-1%7D%29%24%2C%20measured%20in%20terms%0Aof%20the%20Wasserstein%20distance.%20Finally%2C%20we%20apply%20our%20general%20results%20to%20concrete%0Aexamples%20including%20policy%20evaluation%2C%20nonparametric%20instrumental%20variable%0Aregression%2C%20and%20asset%20pricing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12312v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Mean-Field%20Analysis%20of%20Neural%20Gradient%20Descent-Ascent%3A%20Applications%20to%0A%20%20Functional%20Conditional%20Moment%20Equations&entry.906535625=Yuchen%20Zhu%20and%20Yufeng%20Zhang%20and%20Zhaoran%20Wang%20and%20Zhuoran%20Yang%20and%20Xiaohong%20Chen&entry.1292438233=%20%20We%20study%20minimax%20optimization%20problems%20defined%20over%20infinite-dimensional%0Afunction%20classes.%20In%20particular%2C%20we%20restrict%20the%20functions%20to%20the%20class%20of%0Aoverparameterized%20two-layer%20neural%20networks%20and%20study%20%28i%29%20the%20convergence%20of%0Athe%20gradient%20descent-ascent%20algorithm%20and%20%28ii%29%20the%20representation%20learning%20of%0Athe%20neural%20network.%20As%20an%20initial%20step%2C%20we%20consider%20the%20minimax%20optimization%0Aproblem%20stemming%20from%20estimating%20a%20functional%20equation%20defined%20by%20conditional%0Aexpectations%20via%20adversarial%20estimation%2C%20where%20the%20objective%20function%20is%0Aquadratic%20in%20the%20functional%20space.%20For%20this%20problem%2C%20we%20establish%20convergence%0Aunder%20the%20mean-field%20regime%20by%20considering%20the%20continuous-time%20and%0Ainfinite-width%20limit%20of%20the%20optimization%20dynamics.%20Under%20this%20regime%2C%20gradient%0Adescent-ascent%20corresponds%20to%20a%20Wasserstein%20gradient%20flow%20over%20the%20space%20of%0Aprobability%20measures%20defined%20over%20the%20space%20of%20neural%20network%20parameters.%20We%0Aprove%20that%20the%20Wasserstein%20gradient%20flow%20converges%20globally%20to%20a%20stationary%0Apoint%20of%20the%20minimax%20objective%20at%20a%20%24%5Cmathcal%7BO%7D%28T%5E%7B-1%7D%20%2B%20%5Calpha%5E%7B-1%7D%20%29%20%24%0Asublinear%20rate%2C%20and%20additionally%20finds%20the%20solution%20to%20the%20functional%20equation%0Awhen%20the%20regularizer%20of%20the%20minimax%20objective%20is%20strongly%20convex.%20Here%20%24T%24%0Adenotes%20the%20time%20and%20%24%5Calpha%24%20is%20a%20scaling%20parameter%20of%20the%20neural%20network.%20In%0Aterms%20of%20representation%20learning%2C%20our%20results%20show%20that%20the%20feature%0Arepresentation%20induced%20by%20the%20neural%20networks%20is%20allowed%20to%20deviate%20from%20the%0Ainitial%20one%20by%20the%20magnitude%20of%20%24%5Cmathcal%7BO%7D%28%5Calpha%5E%7B-1%7D%29%24%2C%20measured%20in%20terms%0Aof%20the%20Wasserstein%20distance.%20Finally%2C%20we%20apply%20our%20general%20results%20to%20concrete%0Aexamples%20including%20policy%20evaluation%2C%20nonparametric%20instrumental%20variable%0Aregression%2C%20and%20asset%20pricing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12312v1&entry.124074799=Read"},
{"title": "TensAIR: Real-Time Training of Neural Networks from Data-streams", "author": "Mauro D. L. Tosi and Vinu E. Venugopal and Martin Theobald", "abstract": "  Online learning (OL) from data streams is an emerging area of research that\nencompasses numerous challenges from stream processing, machine learning, and\nnetworking. Stream-processing platforms, such as Apache Kafka and Flink, have\nbasic extensions for the training of Artificial Neural Networks (ANNs) in a\nstream-processing pipeline. However, these extensions were not designed to\ntrain ANNs in real-time, and they suffer from performance and scalability\nissues when doing so. This paper presents TensAIR, the first OL system for\ntraining ANNs in real time. TensAIR achieves remarkable performance and\nscalability by using a decentralized and asynchronous architecture to train ANN\nmodels (either freshly initialized or pre-trained) via DASGD (decentralized and\nasynchronous stochastic gradient descent). We empirically demonstrate that\nTensAIR achieves a nearly linear scale-out performance in terms of (1) the\nnumber of worker nodes deployed in the network, and (2) the throughput at which\nthe data batches arrive at the dataflow operators. We depict the versatility of\nTensAIR by investigating both sparse (word embedding) and dense (image\nclassification) use cases, for which TensAIR achieved from 6 to 116 times\nhigher sustainable throughput rates than state-of-the-art systems for training\nANN in a stream-processing pipeline.\n", "link": "http://arxiv.org/abs/2211.10280v2", "date": "2024-04-18", "relevancy": 1.526, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5149}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5094}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4924}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TensAIR%3A%20Real-Time%20Training%20of%20Neural%20Networks%20from%20Data-streams&body=Title%3A%20TensAIR%3A%20Real-Time%20Training%20of%20Neural%20Networks%20from%20Data-streams%0AAuthor%3A%20Mauro%20D.%20L.%20Tosi%20and%20Vinu%20E.%20Venugopal%20and%20Martin%20Theobald%0AAbstract%3A%20%20%20Online%20learning%20%28OL%29%20from%20data%20streams%20is%20an%20emerging%20area%20of%20research%20that%0Aencompasses%20numerous%20challenges%20from%20stream%20processing%2C%20machine%20learning%2C%20and%0Anetworking.%20Stream-processing%20platforms%2C%20such%20as%20Apache%20Kafka%20and%20Flink%2C%20have%0Abasic%20extensions%20for%20the%20training%20of%20Artificial%20Neural%20Networks%20%28ANNs%29%20in%20a%0Astream-processing%20pipeline.%20However%2C%20these%20extensions%20were%20not%20designed%20to%0Atrain%20ANNs%20in%20real-time%2C%20and%20they%20suffer%20from%20performance%20and%20scalability%0Aissues%20when%20doing%20so.%20This%20paper%20presents%20TensAIR%2C%20the%20first%20OL%20system%20for%0Atraining%20ANNs%20in%20real%20time.%20TensAIR%20achieves%20remarkable%20performance%20and%0Ascalability%20by%20using%20a%20decentralized%20and%20asynchronous%20architecture%20to%20train%20ANN%0Amodels%20%28either%20freshly%20initialized%20or%20pre-trained%29%20via%20DASGD%20%28decentralized%20and%0Aasynchronous%20stochastic%20gradient%20descent%29.%20We%20empirically%20demonstrate%20that%0ATensAIR%20achieves%20a%20nearly%20linear%20scale-out%20performance%20in%20terms%20of%20%281%29%20the%0Anumber%20of%20worker%20nodes%20deployed%20in%20the%20network%2C%20and%20%282%29%20the%20throughput%20at%20which%0Athe%20data%20batches%20arrive%20at%20the%20dataflow%20operators.%20We%20depict%20the%20versatility%20of%0ATensAIR%20by%20investigating%20both%20sparse%20%28word%20embedding%29%20and%20dense%20%28image%0Aclassification%29%20use%20cases%2C%20for%20which%20TensAIR%20achieved%20from%206%20to%20116%20times%0Ahigher%20sustainable%20throughput%20rates%20than%20state-of-the-art%20systems%20for%20training%0AANN%20in%20a%20stream-processing%20pipeline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.10280v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TensAIR%3A%20Real-Time%20Training%20of%20Neural%20Networks%20from%20Data-streams&entry.906535625=Mauro%20D.%20L.%20Tosi%20and%20Vinu%20E.%20Venugopal%20and%20Martin%20Theobald&entry.1292438233=%20%20Online%20learning%20%28OL%29%20from%20data%20streams%20is%20an%20emerging%20area%20of%20research%20that%0Aencompasses%20numerous%20challenges%20from%20stream%20processing%2C%20machine%20learning%2C%20and%0Anetworking.%20Stream-processing%20platforms%2C%20such%20as%20Apache%20Kafka%20and%20Flink%2C%20have%0Abasic%20extensions%20for%20the%20training%20of%20Artificial%20Neural%20Networks%20%28ANNs%29%20in%20a%0Astream-processing%20pipeline.%20However%2C%20these%20extensions%20were%20not%20designed%20to%0Atrain%20ANNs%20in%20real-time%2C%20and%20they%20suffer%20from%20performance%20and%20scalability%0Aissues%20when%20doing%20so.%20This%20paper%20presents%20TensAIR%2C%20the%20first%20OL%20system%20for%0Atraining%20ANNs%20in%20real%20time.%20TensAIR%20achieves%20remarkable%20performance%20and%0Ascalability%20by%20using%20a%20decentralized%20and%20asynchronous%20architecture%20to%20train%20ANN%0Amodels%20%28either%20freshly%20initialized%20or%20pre-trained%29%20via%20DASGD%20%28decentralized%20and%0Aasynchronous%20stochastic%20gradient%20descent%29.%20We%20empirically%20demonstrate%20that%0ATensAIR%20achieves%20a%20nearly%20linear%20scale-out%20performance%20in%20terms%20of%20%281%29%20the%0Anumber%20of%20worker%20nodes%20deployed%20in%20the%20network%2C%20and%20%282%29%20the%20throughput%20at%20which%0Athe%20data%20batches%20arrive%20at%20the%20dataflow%20operators.%20We%20depict%20the%20versatility%20of%0ATensAIR%20by%20investigating%20both%20sparse%20%28word%20embedding%29%20and%20dense%20%28image%0Aclassification%29%20use%20cases%2C%20for%20which%20TensAIR%20achieved%20from%206%20to%20116%20times%0Ahigher%20sustainable%20throughput%20rates%20than%20state-of-the-art%20systems%20for%20training%0AANN%20in%20a%20stream-processing%20pipeline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.10280v2&entry.124074799=Read"},
{"title": "Can We Edit Multimodal Large Language Models?", "author": "Siyuan Cheng and Bozhong Tian and Qingbin Liu and Xi Chen and Yongheng Wang and Huajun Chen and Ningyu Zhang", "abstract": "  In this paper, we focus on editing Multimodal Large Language Models (MLLMs).\nCompared to editing single-modal LLMs, multimodal model editing is more\nchallenging, which demands a higher level of scrutiny and careful consideration\nin the editing process. To facilitate research in this area, we construct a new\nbenchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite\nof innovative metrics for evaluation. We conduct comprehensive experiments\ninvolving various model editing baselines and analyze the impact of editing\ndifferent components for multimodal LLMs. Empirically, we notice that previous\nbaselines can implement editing multimodal LLMs to some extent, but the effect\nis still barely satisfactory, indicating the potential difficulty of this task.\nWe hope that our work can provide the NLP community with insights. Code and\ndataset are available in https://github.com/zjunlp/EasyEdit.\n", "link": "http://arxiv.org/abs/2310.08475v5", "date": "2024-04-18", "relevancy": 1.5146, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5837}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4835}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4819}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Can%20We%20Edit%20Multimodal%20Large%20Language%20Models%3F&body=Title%3A%20Can%20We%20Edit%20Multimodal%20Large%20Language%20Models%3F%0AAuthor%3A%20Siyuan%20Cheng%20and%20Bozhong%20Tian%20and%20Qingbin%20Liu%20and%20Xi%20Chen%20and%20Yongheng%20Wang%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20focus%20on%20editing%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%0ACompared%20to%20editing%20single-modal%20LLMs%2C%20multimodal%20model%20editing%20is%20more%0Achallenging%2C%20which%20demands%20a%20higher%20level%20of%20scrutiny%20and%20careful%20consideration%0Ain%20the%20editing%20process.%20To%20facilitate%20research%20in%20this%20area%2C%20we%20construct%20a%20new%0Abenchmark%2C%20dubbed%20MMEdit%2C%20for%20editing%20multimodal%20LLMs%20and%20establishing%20a%20suite%0Aof%20innovative%20metrics%20for%20evaluation.%20We%20conduct%20comprehensive%20experiments%0Ainvolving%20various%20model%20editing%20baselines%20and%20analyze%20the%20impact%20of%20editing%0Adifferent%20components%20for%20multimodal%20LLMs.%20Empirically%2C%20we%20notice%20that%20previous%0Abaselines%20can%20implement%20editing%20multimodal%20LLMs%20to%20some%20extent%2C%20but%20the%20effect%0Ais%20still%20barely%20satisfactory%2C%20indicating%20the%20potential%20difficulty%20of%20this%20task.%0AWe%20hope%20that%20our%20work%20can%20provide%20the%20NLP%20community%20with%20insights.%20Code%20and%0Adataset%20are%20available%20in%20https%3A//github.com/zjunlp/EasyEdit.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08475v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20We%20Edit%20Multimodal%20Large%20Language%20Models%3F&entry.906535625=Siyuan%20Cheng%20and%20Bozhong%20Tian%20and%20Qingbin%20Liu%20and%20Xi%20Chen%20and%20Yongheng%20Wang%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20focus%20on%20editing%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29.%0ACompared%20to%20editing%20single-modal%20LLMs%2C%20multimodal%20model%20editing%20is%20more%0Achallenging%2C%20which%20demands%20a%20higher%20level%20of%20scrutiny%20and%20careful%20consideration%0Ain%20the%20editing%20process.%20To%20facilitate%20research%20in%20this%20area%2C%20we%20construct%20a%20new%0Abenchmark%2C%20dubbed%20MMEdit%2C%20for%20editing%20multimodal%20LLMs%20and%20establishing%20a%20suite%0Aof%20innovative%20metrics%20for%20evaluation.%20We%20conduct%20comprehensive%20experiments%0Ainvolving%20various%20model%20editing%20baselines%20and%20analyze%20the%20impact%20of%20editing%0Adifferent%20components%20for%20multimodal%20LLMs.%20Empirically%2C%20we%20notice%20that%20previous%0Abaselines%20can%20implement%20editing%20multimodal%20LLMs%20to%20some%20extent%2C%20but%20the%20effect%0Ais%20still%20barely%20satisfactory%2C%20indicating%20the%20potential%20difficulty%20of%20this%20task.%0AWe%20hope%20that%20our%20work%20can%20provide%20the%20NLP%20community%20with%20insights.%20Code%20and%0Adataset%20are%20available%20in%20https%3A//github.com/zjunlp/EasyEdit.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08475v5&entry.124074799=Read"},
{"title": "Chimera: A Lossless Decoding Method for Accelerating Large Language\n  Models Inference by Fusing all Tokens", "author": "Ziqian Zeng and Jiahong Yu and Qianshi Pang and Zihao Wang and Huiping Zhuang and Hongen Shao and Xiaofeng Zou", "abstract": "  Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks. However, their widespread application is hindered by the\nresource-intensive decoding process. To address this challenge, current\napproaches have incorporated additional decoding heads to enable parallel\nprediction of multiple subsequent tokens, thereby achieving inference\nacceleration. Nevertheless, the accuracy of these decoding heads falls short of\nthe auto-regressive decoding approach.\n  In light of these limitations, we propose Chimera, a novel framework\nspecifically designed for speculative sampling. Within this framework, we\nintroduce a lightweight draft model that effectively utilizes previously\ngenerated tokens to predict subsequent words. To ensure both accuracy and\nefficiency, we present two strategies within the lightweight draft model.\nFirstly, we focus on capturing short-range dependencies at the bottom layer.\nSecondly, we leverage the readily available representations from the original\nLLM.Through empirical evaluation on the Vicuna and LlaMA-2 series, Chimera\ndemonstrates impressive results, achieving an average latency speedup ratio of\n2.7x compared to the vanilla auto-regressive decoding approach. This highlights\nthe potential of our proposed framework in significantly improving the\nefficiency of large language models during the decoding process.\n", "link": "http://arxiv.org/abs/2402.15758v2", "date": "2024-04-18", "relevancy": 1.5109, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5332}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.507}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4904}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Chimera%3A%20A%20Lossless%20Decoding%20Method%20for%20Accelerating%20Large%20Language%0A%20%20Models%20Inference%20by%20Fusing%20all%20Tokens&body=Title%3A%20Chimera%3A%20A%20Lossless%20Decoding%20Method%20for%20Accelerating%20Large%20Language%0A%20%20Models%20Inference%20by%20Fusing%20all%20Tokens%0AAuthor%3A%20Ziqian%20Zeng%20and%20Jiahong%20Yu%20and%20Qianshi%20Pang%20and%20Zihao%20Wang%20and%20Huiping%20Zhuang%20and%20Hongen%20Shao%20and%20Xiaofeng%20Zou%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Avarious%20tasks.%20However%2C%20their%20widespread%20application%20is%20hindered%20by%20the%0Aresource-intensive%20decoding%20process.%20To%20address%20this%20challenge%2C%20current%0Aapproaches%20have%20incorporated%20additional%20decoding%20heads%20to%20enable%20parallel%0Aprediction%20of%20multiple%20subsequent%20tokens%2C%20thereby%20achieving%20inference%0Aacceleration.%20Nevertheless%2C%20the%20accuracy%20of%20these%20decoding%20heads%20falls%20short%20of%0Athe%20auto-regressive%20decoding%20approach.%0A%20%20In%20light%20of%20these%20limitations%2C%20we%20propose%20Chimera%2C%20a%20novel%20framework%0Aspecifically%20designed%20for%20speculative%20sampling.%20Within%20this%20framework%2C%20we%0Aintroduce%20a%20lightweight%20draft%20model%20that%20effectively%20utilizes%20previously%0Agenerated%20tokens%20to%20predict%20subsequent%20words.%20To%20ensure%20both%20accuracy%20and%0Aefficiency%2C%20we%20present%20two%20strategies%20within%20the%20lightweight%20draft%20model.%0AFirstly%2C%20we%20focus%20on%20capturing%20short-range%20dependencies%20at%20the%20bottom%20layer.%0ASecondly%2C%20we%20leverage%20the%20readily%20available%20representations%20from%20the%20original%0ALLM.Through%20empirical%20evaluation%20on%20the%20Vicuna%20and%20LlaMA-2%20series%2C%20Chimera%0Ademonstrates%20impressive%20results%2C%20achieving%20an%20average%20latency%20speedup%20ratio%20of%0A2.7x%20compared%20to%20the%20vanilla%20auto-regressive%20decoding%20approach.%20This%20highlights%0Athe%20potential%20of%20our%20proposed%20framework%20in%20significantly%20improving%20the%0Aefficiency%20of%20large%20language%20models%20during%20the%20decoding%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15758v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chimera%3A%20A%20Lossless%20Decoding%20Method%20for%20Accelerating%20Large%20Language%0A%20%20Models%20Inference%20by%20Fusing%20all%20Tokens&entry.906535625=Ziqian%20Zeng%20and%20Jiahong%20Yu%20and%20Qianshi%20Pang%20and%20Zihao%20Wang%20and%20Huiping%20Zhuang%20and%20Hongen%20Shao%20and%20Xiaofeng%20Zou&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20across%0Avarious%20tasks.%20However%2C%20their%20widespread%20application%20is%20hindered%20by%20the%0Aresource-intensive%20decoding%20process.%20To%20address%20this%20challenge%2C%20current%0Aapproaches%20have%20incorporated%20additional%20decoding%20heads%20to%20enable%20parallel%0Aprediction%20of%20multiple%20subsequent%20tokens%2C%20thereby%20achieving%20inference%0Aacceleration.%20Nevertheless%2C%20the%20accuracy%20of%20these%20decoding%20heads%20falls%20short%20of%0Athe%20auto-regressive%20decoding%20approach.%0A%20%20In%20light%20of%20these%20limitations%2C%20we%20propose%20Chimera%2C%20a%20novel%20framework%0Aspecifically%20designed%20for%20speculative%20sampling.%20Within%20this%20framework%2C%20we%0Aintroduce%20a%20lightweight%20draft%20model%20that%20effectively%20utilizes%20previously%0Agenerated%20tokens%20to%20predict%20subsequent%20words.%20To%20ensure%20both%20accuracy%20and%0Aefficiency%2C%20we%20present%20two%20strategies%20within%20the%20lightweight%20draft%20model.%0AFirstly%2C%20we%20focus%20on%20capturing%20short-range%20dependencies%20at%20the%20bottom%20layer.%0ASecondly%2C%20we%20leverage%20the%20readily%20available%20representations%20from%20the%20original%0ALLM.Through%20empirical%20evaluation%20on%20the%20Vicuna%20and%20LlaMA-2%20series%2C%20Chimera%0Ademonstrates%20impressive%20results%2C%20achieving%20an%20average%20latency%20speedup%20ratio%20of%0A2.7x%20compared%20to%20the%20vanilla%20auto-regressive%20decoding%20approach.%20This%20highlights%0Athe%20potential%20of%20our%20proposed%20framework%20in%20significantly%20improving%20the%0Aefficiency%20of%20large%20language%20models%20during%20the%20decoding%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15758v2&entry.124074799=Read"},
{"title": "MeshLRM: Large Reconstruction Model for High-Quality Mesh", "author": "Xinyue Wei and Kai Zhang and Sai Bi and Hao Tan and Fujun Luan and Valentin Deschaintre and Kalyan Sunkavalli and Hao Su and Zexiang Xu", "abstract": "  We propose MeshLRM, a novel LRM-based approach that can reconstruct a\nhigh-quality mesh from merely four input images in less than one second.\nDifferent from previous large reconstruction models (LRMs) that focus on\nNeRF-based reconstruction, MeshLRM incorporates differentiable mesh extraction\nand rendering within the LRM framework. This allows for end-to-end mesh\nreconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering.\nMoreover, we improve the LRM architecture by simplifying several complex\ndesigns in previous LRMs. MeshLRM's NeRF initialization is sequentially trained\nwith low- and high-resolution images; this new LRM training strategy enables\nsignificantly faster convergence and thereby leads to better quality with less\ncompute. Our approach achieves state-of-the-art mesh reconstruction from\nsparse-view inputs and also allows for many downstream applications, including\ntext-to-3D and single-image-to-3D generation. Project page:\nhttps://sarahweiii.github.io/meshlrm/\n", "link": "http://arxiv.org/abs/2404.12385v1", "date": "2024-04-18", "relevancy": 1.5085, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5138}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5022}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4933}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MeshLRM%3A%20Large%20Reconstruction%20Model%20for%20High-Quality%20Mesh&body=Title%3A%20MeshLRM%3A%20Large%20Reconstruction%20Model%20for%20High-Quality%20Mesh%0AAuthor%3A%20Xinyue%20Wei%20and%20Kai%20Zhang%20and%20Sai%20Bi%20and%20Hao%20Tan%20and%20Fujun%20Luan%20and%20Valentin%20Deschaintre%20and%20Kalyan%20Sunkavalli%20and%20Hao%20Su%20and%20Zexiang%20Xu%0AAbstract%3A%20%20%20We%20propose%20MeshLRM%2C%20a%20novel%20LRM-based%20approach%20that%20can%20reconstruct%20a%0Ahigh-quality%20mesh%20from%20merely%20four%20input%20images%20in%20less%20than%20one%20second.%0ADifferent%20from%20previous%20large%20reconstruction%20models%20%28LRMs%29%20that%20focus%20on%0ANeRF-based%20reconstruction%2C%20MeshLRM%20incorporates%20differentiable%20mesh%20extraction%0Aand%20rendering%20within%20the%20LRM%20framework.%20This%20allows%20for%20end-to-end%20mesh%0Areconstruction%20by%20fine-tuning%20a%20pre-trained%20NeRF%20LRM%20with%20mesh%20rendering.%0AMoreover%2C%20we%20improve%20the%20LRM%20architecture%20by%20simplifying%20several%20complex%0Adesigns%20in%20previous%20LRMs.%20MeshLRM%27s%20NeRF%20initialization%20is%20sequentially%20trained%0Awith%20low-%20and%20high-resolution%20images%3B%20this%20new%20LRM%20training%20strategy%20enables%0Asignificantly%20faster%20convergence%20and%20thereby%20leads%20to%20better%20quality%20with%20less%0Acompute.%20Our%20approach%20achieves%20state-of-the-art%20mesh%20reconstruction%20from%0Asparse-view%20inputs%20and%20also%20allows%20for%20many%20downstream%20applications%2C%20including%0Atext-to-3D%20and%20single-image-to-3D%20generation.%20Project%20page%3A%0Ahttps%3A//sarahweiii.github.io/meshlrm/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12385v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MeshLRM%3A%20Large%20Reconstruction%20Model%20for%20High-Quality%20Mesh&entry.906535625=Xinyue%20Wei%20and%20Kai%20Zhang%20and%20Sai%20Bi%20and%20Hao%20Tan%20and%20Fujun%20Luan%20and%20Valentin%20Deschaintre%20and%20Kalyan%20Sunkavalli%20and%20Hao%20Su%20and%20Zexiang%20Xu&entry.1292438233=%20%20We%20propose%20MeshLRM%2C%20a%20novel%20LRM-based%20approach%20that%20can%20reconstruct%20a%0Ahigh-quality%20mesh%20from%20merely%20four%20input%20images%20in%20less%20than%20one%20second.%0ADifferent%20from%20previous%20large%20reconstruction%20models%20%28LRMs%29%20that%20focus%20on%0ANeRF-based%20reconstruction%2C%20MeshLRM%20incorporates%20differentiable%20mesh%20extraction%0Aand%20rendering%20within%20the%20LRM%20framework.%20This%20allows%20for%20end-to-end%20mesh%0Areconstruction%20by%20fine-tuning%20a%20pre-trained%20NeRF%20LRM%20with%20mesh%20rendering.%0AMoreover%2C%20we%20improve%20the%20LRM%20architecture%20by%20simplifying%20several%20complex%0Adesigns%20in%20previous%20LRMs.%20MeshLRM%27s%20NeRF%20initialization%20is%20sequentially%20trained%0Awith%20low-%20and%20high-resolution%20images%3B%20this%20new%20LRM%20training%20strategy%20enables%0Asignificantly%20faster%20convergence%20and%20thereby%20leads%20to%20better%20quality%20with%20less%0Acompute.%20Our%20approach%20achieves%20state-of-the-art%20mesh%20reconstruction%20from%0Asparse-view%20inputs%20and%20also%20allows%20for%20many%20downstream%20applications%2C%20including%0Atext-to-3D%20and%20single-image-to-3D%20generation.%20Project%20page%3A%0Ahttps%3A//sarahweiii.github.io/meshlrm/%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12385v1&entry.124074799=Read"},
{"title": "Automatic Speech Recognition using Advanced Deep Learning Approaches: A\n  survey", "author": "Hamza Kheddar and Mustapha Hemis and Yassine Himeur", "abstract": "  Recent advancements in deep learning (DL) have posed a significant challenge\nfor automatic speech recognition (ASR). ASR relies on extensive training\ndatasets, including confidential ones, and demands substantial computational\nand storage resources. Enabling adaptive systems improves ASR performance in\ndynamic environments. DL techniques assume training and testing data originate\nfrom the same domain, which is not always true. Advanced DL techniques like\ndeep transfer learning (DTL), federated learning (FL), and reinforcement\nlearning (RL) address these issues. DTL allows high-performance models using\nsmall yet related datasets, FL enables training on confidential data without\ndataset possession, and RL optimizes decision-making in dynamic environments,\nreducing computation costs. This survey offers a comprehensive review of DTL,\nFL, and RL-based ASR frameworks, aiming to provide insights into the latest\ndevelopments and aid researchers and professionals in understanding the current\nchallenges. Additionally, transformers, which are advanced DL techniques\nheavily used in proposed ASR frameworks, are considered in this survey for\ntheir ability to capture extensive dependencies in the input ASR sequence. The\npaper starts by presenting the background of DTL, FL, RL, and Transformers and\nthen adopts a well-designed taxonomy to outline the state-of-the-art\napproaches. Subsequently, a critical analysis is conducted to identify the\nstrengths and weaknesses of each framework. Additionally, a comparative study\nis presented to highlight the existing challenges, paving the way for future\nresearch opportunities.\n", "link": "http://arxiv.org/abs/2403.01255v2", "date": "2024-04-18", "relevancy": 1.4892, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5261}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4924}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4861}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Automatic%20Speech%20Recognition%20using%20Advanced%20Deep%20Learning%20Approaches%3A%20A%0A%20%20survey&body=Title%3A%20Automatic%20Speech%20Recognition%20using%20Advanced%20Deep%20Learning%20Approaches%3A%20A%0A%20%20survey%0AAuthor%3A%20Hamza%20Kheddar%20and%20Mustapha%20Hemis%20and%20Yassine%20Himeur%0AAbstract%3A%20%20%20Recent%20advancements%20in%20deep%20learning%20%28DL%29%20have%20posed%20a%20significant%20challenge%0Afor%20automatic%20speech%20recognition%20%28ASR%29.%20ASR%20relies%20on%20extensive%20training%0Adatasets%2C%20including%20confidential%20ones%2C%20and%20demands%20substantial%20computational%0Aand%20storage%20resources.%20Enabling%20adaptive%20systems%20improves%20ASR%20performance%20in%0Adynamic%20environments.%20DL%20techniques%20assume%20training%20and%20testing%20data%20originate%0Afrom%20the%20same%20domain%2C%20which%20is%20not%20always%20true.%20Advanced%20DL%20techniques%20like%0Adeep%20transfer%20learning%20%28DTL%29%2C%20federated%20learning%20%28FL%29%2C%20and%20reinforcement%0Alearning%20%28RL%29%20address%20these%20issues.%20DTL%20allows%20high-performance%20models%20using%0Asmall%20yet%20related%20datasets%2C%20FL%20enables%20training%20on%20confidential%20data%20without%0Adataset%20possession%2C%20and%20RL%20optimizes%20decision-making%20in%20dynamic%20environments%2C%0Areducing%20computation%20costs.%20This%20survey%20offers%20a%20comprehensive%20review%20of%20DTL%2C%0AFL%2C%20and%20RL-based%20ASR%20frameworks%2C%20aiming%20to%20provide%20insights%20into%20the%20latest%0Adevelopments%20and%20aid%20researchers%20and%20professionals%20in%20understanding%20the%20current%0Achallenges.%20Additionally%2C%20transformers%2C%20which%20are%20advanced%20DL%20techniques%0Aheavily%20used%20in%20proposed%20ASR%20frameworks%2C%20are%20considered%20in%20this%20survey%20for%0Atheir%20ability%20to%20capture%20extensive%20dependencies%20in%20the%20input%20ASR%20sequence.%20The%0Apaper%20starts%20by%20presenting%20the%20background%20of%20DTL%2C%20FL%2C%20RL%2C%20and%20Transformers%20and%0Athen%20adopts%20a%20well-designed%20taxonomy%20to%20outline%20the%20state-of-the-art%0Aapproaches.%20Subsequently%2C%20a%20critical%20analysis%20is%20conducted%20to%20identify%20the%0Astrengths%20and%20weaknesses%20of%20each%20framework.%20Additionally%2C%20a%20comparative%20study%0Ais%20presented%20to%20highlight%20the%20existing%20challenges%2C%20paving%20the%20way%20for%20future%0Aresearch%20opportunities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01255v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Speech%20Recognition%20using%20Advanced%20Deep%20Learning%20Approaches%3A%20A%0A%20%20survey&entry.906535625=Hamza%20Kheddar%20and%20Mustapha%20Hemis%20and%20Yassine%20Himeur&entry.1292438233=%20%20Recent%20advancements%20in%20deep%20learning%20%28DL%29%20have%20posed%20a%20significant%20challenge%0Afor%20automatic%20speech%20recognition%20%28ASR%29.%20ASR%20relies%20on%20extensive%20training%0Adatasets%2C%20including%20confidential%20ones%2C%20and%20demands%20substantial%20computational%0Aand%20storage%20resources.%20Enabling%20adaptive%20systems%20improves%20ASR%20performance%20in%0Adynamic%20environments.%20DL%20techniques%20assume%20training%20and%20testing%20data%20originate%0Afrom%20the%20same%20domain%2C%20which%20is%20not%20always%20true.%20Advanced%20DL%20techniques%20like%0Adeep%20transfer%20learning%20%28DTL%29%2C%20federated%20learning%20%28FL%29%2C%20and%20reinforcement%0Alearning%20%28RL%29%20address%20these%20issues.%20DTL%20allows%20high-performance%20models%20using%0Asmall%20yet%20related%20datasets%2C%20FL%20enables%20training%20on%20confidential%20data%20without%0Adataset%20possession%2C%20and%20RL%20optimizes%20decision-making%20in%20dynamic%20environments%2C%0Areducing%20computation%20costs.%20This%20survey%20offers%20a%20comprehensive%20review%20of%20DTL%2C%0AFL%2C%20and%20RL-based%20ASR%20frameworks%2C%20aiming%20to%20provide%20insights%20into%20the%20latest%0Adevelopments%20and%20aid%20researchers%20and%20professionals%20in%20understanding%20the%20current%0Achallenges.%20Additionally%2C%20transformers%2C%20which%20are%20advanced%20DL%20techniques%0Aheavily%20used%20in%20proposed%20ASR%20frameworks%2C%20are%20considered%20in%20this%20survey%20for%0Atheir%20ability%20to%20capture%20extensive%20dependencies%20in%20the%20input%20ASR%20sequence.%20The%0Apaper%20starts%20by%20presenting%20the%20background%20of%20DTL%2C%20FL%2C%20RL%2C%20and%20Transformers%20and%0Athen%20adopts%20a%20well-designed%20taxonomy%20to%20outline%20the%20state-of-the-art%0Aapproaches.%20Subsequently%2C%20a%20critical%20analysis%20is%20conducted%20to%20identify%20the%0Astrengths%20and%20weaknesses%20of%20each%20framework.%20Additionally%2C%20a%20comparative%20study%0Ais%20presented%20to%20highlight%20the%20existing%20challenges%2C%20paving%20the%20way%20for%20future%0Aresearch%20opportunities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01255v2&entry.124074799=Read"},
{"title": "DeforestVis: Behavior Analysis of Machine Learning Models with Surrogate\n  Decision Stumps", "author": "Angelos Chatzimparmpas and Rafael M. Martins and Alexandru C. Telea and Andreas Kerren", "abstract": "  As the complexity of machine learning (ML) models increases and their\napplication in different (and critical) domains grows, there is a strong demand\nfor more interpretable and trustworthy ML. A direct, model-agnostic, way to\ninterpret such models is to train surrogate models-such as rule sets and\ndecision trees-that sufficiently approximate the original ones while being\nsimpler and easier-to-explain. Yet, rule sets can become very lengthy, with\nmany if-else statements, and decision tree depth grows rapidly when accurately\nemulating complex ML models. In such cases, both approaches can fail to meet\ntheir core goal-providing users with model interpretability. To tackle this, we\npropose DeforestVis, a visual analytics tool that offers summarization of the\nbehaviour of complex ML models by providing surrogate decision stumps\n(one-level decision trees) generated with the Adaptive Boosting (AdaBoost)\ntechnique. DeforestVis helps users to explore the complexity versus fidelity\ntrade-off by incrementally generating more stumps, creating attribute-based\nexplanations with weighted stumps to justify decision making, and analysing the\nimpact of rule overriding on training instance allocation between one or more\nstumps. An independent test set allows users to monitor the effectiveness of\nmanual rule changes and form hypotheses based on case-by-case analyses. We show\nthe applicability and usefulness of DeforestVis with two use cases and expert\ninterviews with data analysts and model developers.\n", "link": "http://arxiv.org/abs/2304.00133v5", "date": "2024-04-18", "relevancy": 1.4469, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4986}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4802}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4766}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20DeforestVis%3A%20Behavior%20Analysis%20of%20Machine%20Learning%20Models%20with%20Surrogate%0A%20%20Decision%20Stumps&body=Title%3A%20DeforestVis%3A%20Behavior%20Analysis%20of%20Machine%20Learning%20Models%20with%20Surrogate%0A%20%20Decision%20Stumps%0AAuthor%3A%20Angelos%20Chatzimparmpas%20and%20Rafael%20M.%20Martins%20and%20Alexandru%20C.%20Telea%20and%20Andreas%20Kerren%0AAbstract%3A%20%20%20As%20the%20complexity%20of%20machine%20learning%20%28ML%29%20models%20increases%20and%20their%0Aapplication%20in%20different%20%28and%20critical%29%20domains%20grows%2C%20there%20is%20a%20strong%20demand%0Afor%20more%20interpretable%20and%20trustworthy%20ML.%20A%20direct%2C%20model-agnostic%2C%20way%20to%0Ainterpret%20such%20models%20is%20to%20train%20surrogate%20models-such%20as%20rule%20sets%20and%0Adecision%20trees-that%20sufficiently%20approximate%20the%20original%20ones%20while%20being%0Asimpler%20and%20easier-to-explain.%20Yet%2C%20rule%20sets%20can%20become%20very%20lengthy%2C%20with%0Amany%20if-else%20statements%2C%20and%20decision%20tree%20depth%20grows%20rapidly%20when%20accurately%0Aemulating%20complex%20ML%20models.%20In%20such%20cases%2C%20both%20approaches%20can%20fail%20to%20meet%0Atheir%20core%20goal-providing%20users%20with%20model%20interpretability.%20To%20tackle%20this%2C%20we%0Apropose%20DeforestVis%2C%20a%20visual%20analytics%20tool%20that%20offers%20summarization%20of%20the%0Abehaviour%20of%20complex%20ML%20models%20by%20providing%20surrogate%20decision%20stumps%0A%28one-level%20decision%20trees%29%20generated%20with%20the%20Adaptive%20Boosting%20%28AdaBoost%29%0Atechnique.%20DeforestVis%20helps%20users%20to%20explore%20the%20complexity%20versus%20fidelity%0Atrade-off%20by%20incrementally%20generating%20more%20stumps%2C%20creating%20attribute-based%0Aexplanations%20with%20weighted%20stumps%20to%20justify%20decision%20making%2C%20and%20analysing%20the%0Aimpact%20of%20rule%20overriding%20on%20training%20instance%20allocation%20between%20one%20or%20more%0Astumps.%20An%20independent%20test%20set%20allows%20users%20to%20monitor%20the%20effectiveness%20of%0Amanual%20rule%20changes%20and%20form%20hypotheses%20based%20on%20case-by-case%20analyses.%20We%20show%0Athe%20applicability%20and%20usefulness%20of%20DeforestVis%20with%20two%20use%20cases%20and%20expert%0Ainterviews%20with%20data%20analysts%20and%20model%20developers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2304.00133v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeforestVis%3A%20Behavior%20Analysis%20of%20Machine%20Learning%20Models%20with%20Surrogate%0A%20%20Decision%20Stumps&entry.906535625=Angelos%20Chatzimparmpas%20and%20Rafael%20M.%20Martins%20and%20Alexandru%20C.%20Telea%20and%20Andreas%20Kerren&entry.1292438233=%20%20As%20the%20complexity%20of%20machine%20learning%20%28ML%29%20models%20increases%20and%20their%0Aapplication%20in%20different%20%28and%20critical%29%20domains%20grows%2C%20there%20is%20a%20strong%20demand%0Afor%20more%20interpretable%20and%20trustworthy%20ML.%20A%20direct%2C%20model-agnostic%2C%20way%20to%0Ainterpret%20such%20models%20is%20to%20train%20surrogate%20models-such%20as%20rule%20sets%20and%0Adecision%20trees-that%20sufficiently%20approximate%20the%20original%20ones%20while%20being%0Asimpler%20and%20easier-to-explain.%20Yet%2C%20rule%20sets%20can%20become%20very%20lengthy%2C%20with%0Amany%20if-else%20statements%2C%20and%20decision%20tree%20depth%20grows%20rapidly%20when%20accurately%0Aemulating%20complex%20ML%20models.%20In%20such%20cases%2C%20both%20approaches%20can%20fail%20to%20meet%0Atheir%20core%20goal-providing%20users%20with%20model%20interpretability.%20To%20tackle%20this%2C%20we%0Apropose%20DeforestVis%2C%20a%20visual%20analytics%20tool%20that%20offers%20summarization%20of%20the%0Abehaviour%20of%20complex%20ML%20models%20by%20providing%20surrogate%20decision%20stumps%0A%28one-level%20decision%20trees%29%20generated%20with%20the%20Adaptive%20Boosting%20%28AdaBoost%29%0Atechnique.%20DeforestVis%20helps%20users%20to%20explore%20the%20complexity%20versus%20fidelity%0Atrade-off%20by%20incrementally%20generating%20more%20stumps%2C%20creating%20attribute-based%0Aexplanations%20with%20weighted%20stumps%20to%20justify%20decision%20making%2C%20and%20analysing%20the%0Aimpact%20of%20rule%20overriding%20on%20training%20instance%20allocation%20between%20one%20or%20more%0Astumps.%20An%20independent%20test%20set%20allows%20users%20to%20monitor%20the%20effectiveness%20of%0Amanual%20rule%20changes%20and%20form%20hypotheses%20based%20on%20case-by-case%20analyses.%20We%20show%0Athe%20applicability%20and%20usefulness%20of%20DeforestVis%20with%20two%20use%20cases%20and%20expert%0Ainterviews%20with%20data%20analysts%20and%20model%20developers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2304.00133v5&entry.124074799=Read"},
{"title": "VisRuler: Visual Analytics for Extracting Decision Rules from Bagged and\n  Boosted Decision Trees", "author": "Angelos Chatzimparmpas and Rafael M. Martins and Andreas Kerren", "abstract": "  Bagging and boosting are two popular ensemble methods in machine learning\n(ML) that produce many individual decision trees. Due to the inherent ensemble\ncharacteristic of these methods, they typically outperform single decision\ntrees or other ML models in predictive performance. However, numerous decision\npaths are generated for each decision tree, increasing the overall complexity\nof the model and hindering its use in domains that require trustworthy and\nexplainable decisions, such as finance, social care, and health care. Thus, the\ninterpretability of bagging and boosting algorithms, such as random forest and\nadaptive boosting, reduces as the number of decisions rises. In this paper, we\npropose a visual analytics tool that aims to assist users in extracting\ndecisions from such ML models via a thorough visual inspection workflow that\nincludes selecting a set of robust and diverse models (originating from\ndifferent ensemble learning algorithms), choosing important features according\nto their global contribution, and deciding which decisions are essential for\nglobal explanation (or locally, for specific cases). The outcome is a final\ndecision based on the class agreement of several models and the explored manual\ndecisions exported by users. We evaluated the applicability and effectiveness\nof VisRuler via a use case, a usage scenario, and a user study. The evaluation\nrevealed that most users managed to successfully use our system to explore\ndecision rules visually, performing the proposed tasks and answering the given\nquestions in a satisfying way.\n", "link": "http://arxiv.org/abs/2112.00334v5", "date": "2024-04-18", "relevancy": 1.4342, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4985}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4558}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4492}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VisRuler%3A%20Visual%20Analytics%20for%20Extracting%20Decision%20Rules%20from%20Bagged%20and%0A%20%20Boosted%20Decision%20Trees&body=Title%3A%20VisRuler%3A%20Visual%20Analytics%20for%20Extracting%20Decision%20Rules%20from%20Bagged%20and%0A%20%20Boosted%20Decision%20Trees%0AAuthor%3A%20Angelos%20Chatzimparmpas%20and%20Rafael%20M.%20Martins%20and%20Andreas%20Kerren%0AAbstract%3A%20%20%20Bagging%20and%20boosting%20are%20two%20popular%20ensemble%20methods%20in%20machine%20learning%0A%28ML%29%20that%20produce%20many%20individual%20decision%20trees.%20Due%20to%20the%20inherent%20ensemble%0Acharacteristic%20of%20these%20methods%2C%20they%20typically%20outperform%20single%20decision%0Atrees%20or%20other%20ML%20models%20in%20predictive%20performance.%20However%2C%20numerous%20decision%0Apaths%20are%20generated%20for%20each%20decision%20tree%2C%20increasing%20the%20overall%20complexity%0Aof%20the%20model%20and%20hindering%20its%20use%20in%20domains%20that%20require%20trustworthy%20and%0Aexplainable%20decisions%2C%20such%20as%20finance%2C%20social%20care%2C%20and%20health%20care.%20Thus%2C%20the%0Ainterpretability%20of%20bagging%20and%20boosting%20algorithms%2C%20such%20as%20random%20forest%20and%0Aadaptive%20boosting%2C%20reduces%20as%20the%20number%20of%20decisions%20rises.%20In%20this%20paper%2C%20we%0Apropose%20a%20visual%20analytics%20tool%20that%20aims%20to%20assist%20users%20in%20extracting%0Adecisions%20from%20such%20ML%20models%20via%20a%20thorough%20visual%20inspection%20workflow%20that%0Aincludes%20selecting%20a%20set%20of%20robust%20and%20diverse%20models%20%28originating%20from%0Adifferent%20ensemble%20learning%20algorithms%29%2C%20choosing%20important%20features%20according%0Ato%20their%20global%20contribution%2C%20and%20deciding%20which%20decisions%20are%20essential%20for%0Aglobal%20explanation%20%28or%20locally%2C%20for%20specific%20cases%29.%20The%20outcome%20is%20a%20final%0Adecision%20based%20on%20the%20class%20agreement%20of%20several%20models%20and%20the%20explored%20manual%0Adecisions%20exported%20by%20users.%20We%20evaluated%20the%20applicability%20and%20effectiveness%0Aof%20VisRuler%20via%20a%20use%20case%2C%20a%20usage%20scenario%2C%20and%20a%20user%20study.%20The%20evaluation%0Arevealed%20that%20most%20users%20managed%20to%20successfully%20use%20our%20system%20to%20explore%0Adecision%20rules%20visually%2C%20performing%20the%20proposed%20tasks%20and%20answering%20the%20given%0Aquestions%20in%20a%20satisfying%20way.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2112.00334v5", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisRuler%3A%20Visual%20Analytics%20for%20Extracting%20Decision%20Rules%20from%20Bagged%20and%0A%20%20Boosted%20Decision%20Trees&entry.906535625=Angelos%20Chatzimparmpas%20and%20Rafael%20M.%20Martins%20and%20Andreas%20Kerren&entry.1292438233=%20%20Bagging%20and%20boosting%20are%20two%20popular%20ensemble%20methods%20in%20machine%20learning%0A%28ML%29%20that%20produce%20many%20individual%20decision%20trees.%20Due%20to%20the%20inherent%20ensemble%0Acharacteristic%20of%20these%20methods%2C%20they%20typically%20outperform%20single%20decision%0Atrees%20or%20other%20ML%20models%20in%20predictive%20performance.%20However%2C%20numerous%20decision%0Apaths%20are%20generated%20for%20each%20decision%20tree%2C%20increasing%20the%20overall%20complexity%0Aof%20the%20model%20and%20hindering%20its%20use%20in%20domains%20that%20require%20trustworthy%20and%0Aexplainable%20decisions%2C%20such%20as%20finance%2C%20social%20care%2C%20and%20health%20care.%20Thus%2C%20the%0Ainterpretability%20of%20bagging%20and%20boosting%20algorithms%2C%20such%20as%20random%20forest%20and%0Aadaptive%20boosting%2C%20reduces%20as%20the%20number%20of%20decisions%20rises.%20In%20this%20paper%2C%20we%0Apropose%20a%20visual%20analytics%20tool%20that%20aims%20to%20assist%20users%20in%20extracting%0Adecisions%20from%20such%20ML%20models%20via%20a%20thorough%20visual%20inspection%20workflow%20that%0Aincludes%20selecting%20a%20set%20of%20robust%20and%20diverse%20models%20%28originating%20from%0Adifferent%20ensemble%20learning%20algorithms%29%2C%20choosing%20important%20features%20according%0Ato%20their%20global%20contribution%2C%20and%20deciding%20which%20decisions%20are%20essential%20for%0Aglobal%20explanation%20%28or%20locally%2C%20for%20specific%20cases%29.%20The%20outcome%20is%20a%20final%0Adecision%20based%20on%20the%20class%20agreement%20of%20several%20models%20and%20the%20explored%20manual%0Adecisions%20exported%20by%20users.%20We%20evaluated%20the%20applicability%20and%20effectiveness%0Aof%20VisRuler%20via%20a%20use%20case%2C%20a%20usage%20scenario%2C%20and%20a%20user%20study.%20The%20evaluation%0Arevealed%20that%20most%20users%20managed%20to%20successfully%20use%20our%20system%20to%20explore%0Adecision%20rules%20visually%2C%20performing%20the%20proposed%20tasks%20and%20answering%20the%20given%0Aquestions%20in%20a%20satisfying%20way.%0A&entry.1838667208=http%3A//arxiv.org/abs/2112.00334v5&entry.124074799=Read"},
{"title": "FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise\n  Selection and Semi-Automatic Extraction Approaches", "author": "Angelos Chatzimparmpas and Rafael M. Martins and Kostiantyn Kucher and Andreas Kerren", "abstract": "  The machine learning (ML) life cycle involves a series of iterative steps,\nfrom the effective gathering and preparation of the data, including complex\nfeature engineering processes, to the presentation and improvement of results,\nwith various algorithms to choose from in every step. Feature engineering in\nparticular can be very beneficial for ML, leading to numerous improvements such\nas boosting the predictive results, decreasing computational times, reducing\nexcessive noise, and increasing the transparency behind the decisions taken\nduring the training. Despite that, while several visual analytics tools exist\nto monitor and control the different stages of the ML life cycle (especially\nthose related to data and algorithms), feature engineering support remains\ninadequate. In this paper, we present FeatureEnVi, a visual analytics system\nspecifically designed to assist with the feature engineering process. Our\nproposed system helps users to choose the most important feature, to transform\nthe original features into powerful alternatives, and to experiment with\ndifferent feature generation combinations. Additionally, data space slicing\nallows users to explore the impact of features on both local and global scales.\nFeatureEnVi utilizes multiple automatic feature selection techniques;\nfurthermore, it visually guides users with statistical evidence about the\ninfluence of each feature (or subsets of features). The final outcome is the\nextraction of heavily engineered features, evaluated by multiple validation\nmetrics. The usefulness and applicability of FeatureEnVi are demonstrated with\ntwo use cases and a case study. We also report feedback from interviews with\ntwo ML experts and a visualization researcher who assessed the effectiveness of\nour system.\n", "link": "http://arxiv.org/abs/2103.14539v4", "date": "2024-04-18", "relevancy": 1.4307, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4944}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.482}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4679}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FeatureEnVi%3A%20Visual%20Analytics%20for%20Feature%20Engineering%20Using%20Stepwise%0A%20%20Selection%20and%20Semi-Automatic%20Extraction%20Approaches&body=Title%3A%20FeatureEnVi%3A%20Visual%20Analytics%20for%20Feature%20Engineering%20Using%20Stepwise%0A%20%20Selection%20and%20Semi-Automatic%20Extraction%20Approaches%0AAuthor%3A%20Angelos%20Chatzimparmpas%20and%20Rafael%20M.%20Martins%20and%20Kostiantyn%20Kucher%20and%20Andreas%20Kerren%0AAbstract%3A%20%20%20The%20machine%20learning%20%28ML%29%20life%20cycle%20involves%20a%20series%20of%20iterative%20steps%2C%0Afrom%20the%20effective%20gathering%20and%20preparation%20of%20the%20data%2C%20including%20complex%0Afeature%20engineering%20processes%2C%20to%20the%20presentation%20and%20improvement%20of%20results%2C%0Awith%20various%20algorithms%20to%20choose%20from%20in%20every%20step.%20Feature%20engineering%20in%0Aparticular%20can%20be%20very%20beneficial%20for%20ML%2C%20leading%20to%20numerous%20improvements%20such%0Aas%20boosting%20the%20predictive%20results%2C%20decreasing%20computational%20times%2C%20reducing%0Aexcessive%20noise%2C%20and%20increasing%20the%20transparency%20behind%20the%20decisions%20taken%0Aduring%20the%20training.%20Despite%20that%2C%20while%20several%20visual%20analytics%20tools%20exist%0Ato%20monitor%20and%20control%20the%20different%20stages%20of%20the%20ML%20life%20cycle%20%28especially%0Athose%20related%20to%20data%20and%20algorithms%29%2C%20feature%20engineering%20support%20remains%0Ainadequate.%20In%20this%20paper%2C%20we%20present%20FeatureEnVi%2C%20a%20visual%20analytics%20system%0Aspecifically%20designed%20to%20assist%20with%20the%20feature%20engineering%20process.%20Our%0Aproposed%20system%20helps%20users%20to%20choose%20the%20most%20important%20feature%2C%20to%20transform%0Athe%20original%20features%20into%20powerful%20alternatives%2C%20and%20to%20experiment%20with%0Adifferent%20feature%20generation%20combinations.%20Additionally%2C%20data%20space%20slicing%0Aallows%20users%20to%20explore%20the%20impact%20of%20features%20on%20both%20local%20and%20global%20scales.%0AFeatureEnVi%20utilizes%20multiple%20automatic%20feature%20selection%20techniques%3B%0Afurthermore%2C%20it%20visually%20guides%20users%20with%20statistical%20evidence%20about%20the%0Ainfluence%20of%20each%20feature%20%28or%20subsets%20of%20features%29.%20The%20final%20outcome%20is%20the%0Aextraction%20of%20heavily%20engineered%20features%2C%20evaluated%20by%20multiple%20validation%0Ametrics.%20The%20usefulness%20and%20applicability%20of%20FeatureEnVi%20are%20demonstrated%20with%0Atwo%20use%20cases%20and%20a%20case%20study.%20We%20also%20report%20feedback%20from%20interviews%20with%0Atwo%20ML%20experts%20and%20a%20visualization%20researcher%20who%20assessed%20the%20effectiveness%20of%0Aour%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2103.14539v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FeatureEnVi%3A%20Visual%20Analytics%20for%20Feature%20Engineering%20Using%20Stepwise%0A%20%20Selection%20and%20Semi-Automatic%20Extraction%20Approaches&entry.906535625=Angelos%20Chatzimparmpas%20and%20Rafael%20M.%20Martins%20and%20Kostiantyn%20Kucher%20and%20Andreas%20Kerren&entry.1292438233=%20%20The%20machine%20learning%20%28ML%29%20life%20cycle%20involves%20a%20series%20of%20iterative%20steps%2C%0Afrom%20the%20effective%20gathering%20and%20preparation%20of%20the%20data%2C%20including%20complex%0Afeature%20engineering%20processes%2C%20to%20the%20presentation%20and%20improvement%20of%20results%2C%0Awith%20various%20algorithms%20to%20choose%20from%20in%20every%20step.%20Feature%20engineering%20in%0Aparticular%20can%20be%20very%20beneficial%20for%20ML%2C%20leading%20to%20numerous%20improvements%20such%0Aas%20boosting%20the%20predictive%20results%2C%20decreasing%20computational%20times%2C%20reducing%0Aexcessive%20noise%2C%20and%20increasing%20the%20transparency%20behind%20the%20decisions%20taken%0Aduring%20the%20training.%20Despite%20that%2C%20while%20several%20visual%20analytics%20tools%20exist%0Ato%20monitor%20and%20control%20the%20different%20stages%20of%20the%20ML%20life%20cycle%20%28especially%0Athose%20related%20to%20data%20and%20algorithms%29%2C%20feature%20engineering%20support%20remains%0Ainadequate.%20In%20this%20paper%2C%20we%20present%20FeatureEnVi%2C%20a%20visual%20analytics%20system%0Aspecifically%20designed%20to%20assist%20with%20the%20feature%20engineering%20process.%20Our%0Aproposed%20system%20helps%20users%20to%20choose%20the%20most%20important%20feature%2C%20to%20transform%0Athe%20original%20features%20into%20powerful%20alternatives%2C%20and%20to%20experiment%20with%0Adifferent%20feature%20generation%20combinations.%20Additionally%2C%20data%20space%20slicing%0Aallows%20users%20to%20explore%20the%20impact%20of%20features%20on%20both%20local%20and%20global%20scales.%0AFeatureEnVi%20utilizes%20multiple%20automatic%20feature%20selection%20techniques%3B%0Afurthermore%2C%20it%20visually%20guides%20users%20with%20statistical%20evidence%20about%20the%0Ainfluence%20of%20each%20feature%20%28or%20subsets%20of%20features%29.%20The%20final%20outcome%20is%20the%0Aextraction%20of%20heavily%20engineered%20features%2C%20evaluated%20by%20multiple%20validation%0Ametrics.%20The%20usefulness%20and%20applicability%20of%20FeatureEnVi%20are%20demonstrated%20with%0Atwo%20use%20cases%20and%20a%20case%20study.%20We%20also%20report%20feedback%20from%20interviews%20with%0Atwo%20ML%20experts%20and%20a%20visualization%20researcher%20who%20assessed%20the%20effectiveness%20of%0Aour%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2103.14539v4&entry.124074799=Read"},
{"title": "The State of the Art in Enhancing Trust in Machine Learning Models with\n  the Use of Visualizations", "author": "A. Chatzimparmpas and R. Martins and I. Jusufi and K. Kucher and Fabrice Rossi and A. Kerren", "abstract": "  Machine learning (ML) models are nowadays used in complex applications in\nvarious domains, such as medicine, bioinformatics, and other sciences. Due to\ntheir black box nature, however, it may sometimes be hard to understand and\ntrust the results they provide. This has increased the demand for reliable\nvisualization tools related to enhancing trust in ML models, which has become a\nprominent topic of research in the visualization community over the past\ndecades. To provide an overview and present the frontiers of current research\non the topic, we present a State-of-the-Art Report (STAR) on enhancing trust in\nML models with the use of interactive visualization. We define and describe the\nbackground of the topic, introduce a categorization for visualization\ntechniques that aim to accomplish this goal, and discuss insights and\nopportunities for future research directions. Among our contributions is a\ncategorization of trust against different facets of interactive ML, expanded\nand improved from previous research. Our results are investigated from\ndifferent analytical perspectives: (a) providing a statistical overview, (b)\nsummarizing key findings, (c) performing topic analyses, and (d) exploring the\ndata sets used in the individual papers, all with the support of an interactive\nweb-based survey browser. We intend this survey to be beneficial for\nvisualization researchers whose interests involve making ML models more\ntrustworthy, as well as researchers and practitioners from other disciplines in\ntheir search for effective visualization techniques suitable for solving their\ntasks with confidence and conveying meaning to their data.\n", "link": "http://arxiv.org/abs/2212.11737v2", "date": "2024-04-18", "relevancy": 1.4267, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4923}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4807}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4669}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20State%20of%20the%20Art%20in%20Enhancing%20Trust%20in%20Machine%20Learning%20Models%20with%0A%20%20the%20Use%20of%20Visualizations&body=Title%3A%20The%20State%20of%20the%20Art%20in%20Enhancing%20Trust%20in%20Machine%20Learning%20Models%20with%0A%20%20the%20Use%20of%20Visualizations%0AAuthor%3A%20A.%20Chatzimparmpas%20and%20R.%20Martins%20and%20I.%20Jusufi%20and%20K.%20Kucher%20and%20Fabrice%20Rossi%20and%20A.%20Kerren%0AAbstract%3A%20%20%20Machine%20learning%20%28ML%29%20models%20are%20nowadays%20used%20in%20complex%20applications%20in%0Avarious%20domains%2C%20such%20as%20medicine%2C%20bioinformatics%2C%20and%20other%20sciences.%20Due%20to%0Atheir%20black%20box%20nature%2C%20however%2C%20it%20may%20sometimes%20be%20hard%20to%20understand%20and%0Atrust%20the%20results%20they%20provide.%20This%20has%20increased%20the%20demand%20for%20reliable%0Avisualization%20tools%20related%20to%20enhancing%20trust%20in%20ML%20models%2C%20which%20has%20become%20a%0Aprominent%20topic%20of%20research%20in%20the%20visualization%20community%20over%20the%20past%0Adecades.%20To%20provide%20an%20overview%20and%20present%20the%20frontiers%20of%20current%20research%0Aon%20the%20topic%2C%20we%20present%20a%20State-of-the-Art%20Report%20%28STAR%29%20on%20enhancing%20trust%20in%0AML%20models%20with%20the%20use%20of%20interactive%20visualization.%20We%20define%20and%20describe%20the%0Abackground%20of%20the%20topic%2C%20introduce%20a%20categorization%20for%20visualization%0Atechniques%20that%20aim%20to%20accomplish%20this%20goal%2C%20and%20discuss%20insights%20and%0Aopportunities%20for%20future%20research%20directions.%20Among%20our%20contributions%20is%20a%0Acategorization%20of%20trust%20against%20different%20facets%20of%20interactive%20ML%2C%20expanded%0Aand%20improved%20from%20previous%20research.%20Our%20results%20are%20investigated%20from%0Adifferent%20analytical%20perspectives%3A%20%28a%29%20providing%20a%20statistical%20overview%2C%20%28b%29%0Asummarizing%20key%20findings%2C%20%28c%29%20performing%20topic%20analyses%2C%20and%20%28d%29%20exploring%20the%0Adata%20sets%20used%20in%20the%20individual%20papers%2C%20all%20with%20the%20support%20of%20an%20interactive%0Aweb-based%20survey%20browser.%20We%20intend%20this%20survey%20to%20be%20beneficial%20for%0Avisualization%20researchers%20whose%20interests%20involve%20making%20ML%20models%20more%0Atrustworthy%2C%20as%20well%20as%20researchers%20and%20practitioners%20from%20other%20disciplines%20in%0Atheir%20search%20for%20effective%20visualization%20techniques%20suitable%20for%20solving%20their%0Atasks%20with%20confidence%20and%20conveying%20meaning%20to%20their%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.11737v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20State%20of%20the%20Art%20in%20Enhancing%20Trust%20in%20Machine%20Learning%20Models%20with%0A%20%20the%20Use%20of%20Visualizations&entry.906535625=A.%20Chatzimparmpas%20and%20R.%20Martins%20and%20I.%20Jusufi%20and%20K.%20Kucher%20and%20Fabrice%20Rossi%20and%20A.%20Kerren&entry.1292438233=%20%20Machine%20learning%20%28ML%29%20models%20are%20nowadays%20used%20in%20complex%20applications%20in%0Avarious%20domains%2C%20such%20as%20medicine%2C%20bioinformatics%2C%20and%20other%20sciences.%20Due%20to%0Atheir%20black%20box%20nature%2C%20however%2C%20it%20may%20sometimes%20be%20hard%20to%20understand%20and%0Atrust%20the%20results%20they%20provide.%20This%20has%20increased%20the%20demand%20for%20reliable%0Avisualization%20tools%20related%20to%20enhancing%20trust%20in%20ML%20models%2C%20which%20has%20become%20a%0Aprominent%20topic%20of%20research%20in%20the%20visualization%20community%20over%20the%20past%0Adecades.%20To%20provide%20an%20overview%20and%20present%20the%20frontiers%20of%20current%20research%0Aon%20the%20topic%2C%20we%20present%20a%20State-of-the-Art%20Report%20%28STAR%29%20on%20enhancing%20trust%20in%0AML%20models%20with%20the%20use%20of%20interactive%20visualization.%20We%20define%20and%20describe%20the%0Abackground%20of%20the%20topic%2C%20introduce%20a%20categorization%20for%20visualization%0Atechniques%20that%20aim%20to%20accomplish%20this%20goal%2C%20and%20discuss%20insights%20and%0Aopportunities%20for%20future%20research%20directions.%20Among%20our%20contributions%20is%20a%0Acategorization%20of%20trust%20against%20different%20facets%20of%20interactive%20ML%2C%20expanded%0Aand%20improved%20from%20previous%20research.%20Our%20results%20are%20investigated%20from%0Adifferent%20analytical%20perspectives%3A%20%28a%29%20providing%20a%20statistical%20overview%2C%20%28b%29%0Asummarizing%20key%20findings%2C%20%28c%29%20performing%20topic%20analyses%2C%20and%20%28d%29%20exploring%20the%0Adata%20sets%20used%20in%20the%20individual%20papers%2C%20all%20with%20the%20support%20of%20an%20interactive%0Aweb-based%20survey%20browser.%20We%20intend%20this%20survey%20to%20be%20beneficial%20for%0Avisualization%20researchers%20whose%20interests%20involve%20making%20ML%20models%20more%0Atrustworthy%2C%20as%20well%20as%20researchers%20and%20practitioners%20from%20other%20disciplines%20in%0Atheir%20search%20for%20effective%20visualization%20techniques%20suitable%20for%20solving%20their%0Atasks%20with%20confidence%20and%20conveying%20meaning%20to%20their%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.11737v2&entry.124074799=Read"},
{"title": "JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal\n  Large Language Models against Jailbreak Attacks", "author": "Weidi Luo and Siyuan Ma and Xiaogeng Liu and Xiaoyu Guo and Chaowei Xiao", "abstract": "  With the rapid advancements in Multimodal Large Language Models (MLLMs),\nsecuring these models against malicious inputs while aligning them with human\nvalues has emerged as a critical challenge. In this paper, we investigate an\nimportant and unexplored question of whether techniques that successfully\njailbreak Large Language Models (LLMs) can be equally effective in jailbreaking\nMLLMs. To explore this issue, we introduce JailBreakV-28K, a pioneering\nbenchmark designed to assess the transferability of LLM jailbreak techniques to\nMLLMs, thereby evaluating the robustness of MLLMs against diverse jailbreak\nattacks. Utilizing a dataset of 2, 000 malicious queries that is also proposed\nin this paper, we generate 20, 000 text-based jailbreak prompts using advanced\njailbreak attacks on LLMs, alongside 8, 000 image-based jailbreak inputs from\nrecent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 test\ncases across a spectrum of adversarial scenarios. Our evaluation of 10\nopen-source MLLMs reveals a notably high Attack Success Rate (ASR) for attacks\ntransferred from LLMs, highlighting a critical vulnerability in MLLMs that\nstems from their text-processing capabilities. Our findings underscore the\nurgent need for future research to address alignment vulnerabilities in MLLMs\nfrom both textual and visual inputs.\n", "link": "http://arxiv.org/abs/2404.03027v2", "date": "2024-04-18", "relevancy": 1.4147, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4772}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4752}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4539}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20JailBreakV-28K%3A%20A%20Benchmark%20for%20Assessing%20the%20Robustness%20of%20MultiModal%0A%20%20Large%20Language%20Models%20against%20Jailbreak%20Attacks&body=Title%3A%20JailBreakV-28K%3A%20A%20Benchmark%20for%20Assessing%20the%20Robustness%20of%20MultiModal%0A%20%20Large%20Language%20Models%20against%20Jailbreak%20Attacks%0AAuthor%3A%20Weidi%20Luo%20and%20Siyuan%20Ma%20and%20Xiaogeng%20Liu%20and%20Xiaoyu%20Guo%20and%20Chaowei%20Xiao%0AAbstract%3A%20%20%20With%20the%20rapid%20advancements%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%0Asecuring%20these%20models%20against%20malicious%20inputs%20while%20aligning%20them%20with%20human%0Avalues%20has%20emerged%20as%20a%20critical%20challenge.%20In%20this%20paper%2C%20we%20investigate%20an%0Aimportant%20and%20unexplored%20question%20of%20whether%20techniques%20that%20successfully%0Ajailbreak%20Large%20Language%20Models%20%28LLMs%29%20can%20be%20equally%20effective%20in%20jailbreaking%0AMLLMs.%20To%20explore%20this%20issue%2C%20we%20introduce%20JailBreakV-28K%2C%20a%20pioneering%0Abenchmark%20designed%20to%20assess%20the%20transferability%20of%20LLM%20jailbreak%20techniques%20to%0AMLLMs%2C%20thereby%20evaluating%20the%20robustness%20of%20MLLMs%20against%20diverse%20jailbreak%0Aattacks.%20Utilizing%20a%20dataset%20of%202%2C%20000%20malicious%20queries%20that%20is%20also%20proposed%0Ain%20this%20paper%2C%20we%20generate%2020%2C%20000%20text-based%20jailbreak%20prompts%20using%20advanced%0Ajailbreak%20attacks%20on%20LLMs%2C%20alongside%208%2C%20000%20image-based%20jailbreak%20inputs%20from%0Arecent%20MLLMs%20jailbreak%20attacks%2C%20our%20comprehensive%20dataset%20includes%2028%2C%20000%20test%0Acases%20across%20a%20spectrum%20of%20adversarial%20scenarios.%20Our%20evaluation%20of%2010%0Aopen-source%20MLLMs%20reveals%20a%20notably%20high%20Attack%20Success%20Rate%20%28ASR%29%20for%20attacks%0Atransferred%20from%20LLMs%2C%20highlighting%20a%20critical%20vulnerability%20in%20MLLMs%20that%0Astems%20from%20their%20text-processing%20capabilities.%20Our%20findings%20underscore%20the%0Aurgent%20need%20for%20future%20research%20to%20address%20alignment%20vulnerabilities%20in%20MLLMs%0Afrom%20both%20textual%20and%20visual%20inputs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03027v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JailBreakV-28K%3A%20A%20Benchmark%20for%20Assessing%20the%20Robustness%20of%20MultiModal%0A%20%20Large%20Language%20Models%20against%20Jailbreak%20Attacks&entry.906535625=Weidi%20Luo%20and%20Siyuan%20Ma%20and%20Xiaogeng%20Liu%20and%20Xiaoyu%20Guo%20and%20Chaowei%20Xiao&entry.1292438233=%20%20With%20the%20rapid%20advancements%20in%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%0Asecuring%20these%20models%20against%20malicious%20inputs%20while%20aligning%20them%20with%20human%0Avalues%20has%20emerged%20as%20a%20critical%20challenge.%20In%20this%20paper%2C%20we%20investigate%20an%0Aimportant%20and%20unexplored%20question%20of%20whether%20techniques%20that%20successfully%0Ajailbreak%20Large%20Language%20Models%20%28LLMs%29%20can%20be%20equally%20effective%20in%20jailbreaking%0AMLLMs.%20To%20explore%20this%20issue%2C%20we%20introduce%20JailBreakV-28K%2C%20a%20pioneering%0Abenchmark%20designed%20to%20assess%20the%20transferability%20of%20LLM%20jailbreak%20techniques%20to%0AMLLMs%2C%20thereby%20evaluating%20the%20robustness%20of%20MLLMs%20against%20diverse%20jailbreak%0Aattacks.%20Utilizing%20a%20dataset%20of%202%2C%20000%20malicious%20queries%20that%20is%20also%20proposed%0Ain%20this%20paper%2C%20we%20generate%2020%2C%20000%20text-based%20jailbreak%20prompts%20using%20advanced%0Ajailbreak%20attacks%20on%20LLMs%2C%20alongside%208%2C%20000%20image-based%20jailbreak%20inputs%20from%0Arecent%20MLLMs%20jailbreak%20attacks%2C%20our%20comprehensive%20dataset%20includes%2028%2C%20000%20test%0Acases%20across%20a%20spectrum%20of%20adversarial%20scenarios.%20Our%20evaluation%20of%2010%0Aopen-source%20MLLMs%20reveals%20a%20notably%20high%20Attack%20Success%20Rate%20%28ASR%29%20for%20attacks%0Atransferred%20from%20LLMs%2C%20highlighting%20a%20critical%20vulnerability%20in%20MLLMs%20that%0Astems%20from%20their%20text-processing%20capabilities.%20Our%20findings%20underscore%20the%0Aurgent%20need%20for%20future%20research%20to%20address%20alignment%20vulnerabilities%20in%20MLLMs%0Afrom%20both%20textual%20and%20visual%20inputs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03027v2&entry.124074799=Read"},
{"title": "Neural Networks with Causal Graph Constraints: A New Approach for\n  Treatment Effects Estimation", "author": "Roger Pros and Jordi Vitri\u00e0", "abstract": "  In recent years, there has been a growing interest in using machine learning\ntechniques for the estimation of treatment effects. Most of the best-performing\nmethods rely on representation learning strategies that encourage shared\nbehavior among potential outcomes to increase the precision of treatment effect\nestimates. In this paper we discuss and classify these models in terms of their\nalgorithmic inductive biases and present a new model, NN-CGC, that considers\nadditional information from the causal graph. NN-CGC tackles bias resulting\nfrom spurious variable interactions by implementing novel constraints on\nmodels, and it can be integrated with other representation learning methods. We\ntest the effectiveness of our method using three different base models on\ncommon benchmarks. Our results indicate that our model constraints lead to\nsignificant improvements, achieving new state-of-the-art results in treatment\neffects estimation. We also show that our method is robust to imperfect causal\ngraphs and that using partial causal information is preferable to ignoring it.\n", "link": "http://arxiv.org/abs/2404.12238v1", "date": "2024-04-18", "relevancy": 1.4041, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4874}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4505}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4372}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Neural%20Networks%20with%20Causal%20Graph%20Constraints%3A%20A%20New%20Approach%20for%0A%20%20Treatment%20Effects%20Estimation&body=Title%3A%20Neural%20Networks%20with%20Causal%20Graph%20Constraints%3A%20A%20New%20Approach%20for%0A%20%20Treatment%20Effects%20Estimation%0AAuthor%3A%20Roger%20Pros%20and%20Jordi%20Vitri%C3%A0%0AAbstract%3A%20%20%20In%20recent%20years%2C%20there%20has%20been%20a%20growing%20interest%20in%20using%20machine%20learning%0Atechniques%20for%20the%20estimation%20of%20treatment%20effects.%20Most%20of%20the%20best-performing%0Amethods%20rely%20on%20representation%20learning%20strategies%20that%20encourage%20shared%0Abehavior%20among%20potential%20outcomes%20to%20increase%20the%20precision%20of%20treatment%20effect%0Aestimates.%20In%20this%20paper%20we%20discuss%20and%20classify%20these%20models%20in%20terms%20of%20their%0Aalgorithmic%20inductive%20biases%20and%20present%20a%20new%20model%2C%20NN-CGC%2C%20that%20considers%0Aadditional%20information%20from%20the%20causal%20graph.%20NN-CGC%20tackles%20bias%20resulting%0Afrom%20spurious%20variable%20interactions%20by%20implementing%20novel%20constraints%20on%0Amodels%2C%20and%20it%20can%20be%20integrated%20with%20other%20representation%20learning%20methods.%20We%0Atest%20the%20effectiveness%20of%20our%20method%20using%20three%20different%20base%20models%20on%0Acommon%20benchmarks.%20Our%20results%20indicate%20that%20our%20model%20constraints%20lead%20to%0Asignificant%20improvements%2C%20achieving%20new%20state-of-the-art%20results%20in%20treatment%0Aeffects%20estimation.%20We%20also%20show%20that%20our%20method%20is%20robust%20to%20imperfect%20causal%0Agraphs%20and%20that%20using%20partial%20causal%20information%20is%20preferable%20to%20ignoring%20it.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12238v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Neural%20Networks%20with%20Causal%20Graph%20Constraints%3A%20A%20New%20Approach%20for%0A%20%20Treatment%20Effects%20Estimation&entry.906535625=Roger%20Pros%20and%20Jordi%20Vitri%C3%A0&entry.1292438233=%20%20In%20recent%20years%2C%20there%20has%20been%20a%20growing%20interest%20in%20using%20machine%20learning%0Atechniques%20for%20the%20estimation%20of%20treatment%20effects.%20Most%20of%20the%20best-performing%0Amethods%20rely%20on%20representation%20learning%20strategies%20that%20encourage%20shared%0Abehavior%20among%20potential%20outcomes%20to%20increase%20the%20precision%20of%20treatment%20effect%0Aestimates.%20In%20this%20paper%20we%20discuss%20and%20classify%20these%20models%20in%20terms%20of%20their%0Aalgorithmic%20inductive%20biases%20and%20present%20a%20new%20model%2C%20NN-CGC%2C%20that%20considers%0Aadditional%20information%20from%20the%20causal%20graph.%20NN-CGC%20tackles%20bias%20resulting%0Afrom%20spurious%20variable%20interactions%20by%20implementing%20novel%20constraints%20on%0Amodels%2C%20and%20it%20can%20be%20integrated%20with%20other%20representation%20learning%20methods.%20We%0Atest%20the%20effectiveness%20of%20our%20method%20using%20three%20different%20base%20models%20on%0Acommon%20benchmarks.%20Our%20results%20indicate%20that%20our%20model%20constraints%20lead%20to%0Asignificant%20improvements%2C%20achieving%20new%20state-of-the-art%20results%20in%20treatment%0Aeffects%20estimation.%20We%20also%20show%20that%20our%20method%20is%20robust%20to%20imperfect%20causal%0Agraphs%20and%20that%20using%20partial%20causal%20information%20is%20preferable%20to%20ignoring%20it.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12238v1&entry.124074799=Read"},
{"title": "SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in\n  Clinical Summarization", "author": "Prakamya Mishra and Zonghai Yao and Parth Vashisht and Feiyun Ouyang and Beining Wang and Vidhi Dhaval Mody and Hong Yu", "abstract": "  Large Language Models (LLMs) such as GPT & Llama have demonstrated\nsignificant achievements in summarization tasks but struggle with factual\ninaccuracies, a critical issue in clinical NLP applications where errors could\nlead to serious consequences. To counter the high costs and limited\navailability of expert-annotated data for factual alignment, this study\nintroduces an innovative pipeline that utilizes >100B parameter GPT variants\nlike GPT-3.5 & GPT-4 to act as synthetic experts to generate high-quality\nsynthetics feedback aimed at enhancing factual consistency in clinical note\nsummarization. Our research primarily focuses on edit feedback generated by\nthese synthetic feedback experts without additional human annotations,\nmirroring and optimizing the practical scenario in which medical professionals\nrefine AI system outputs. Although such 100B+ parameter GPT variants have\nproven to demonstrate expertise in various clinical NLP tasks, such as the\nMedical Licensing Examination, there is scant research on their capacity to act\nas synthetic feedback experts and deliver expert-level edit feedback for\nimproving the generation quality of weaker (<10B parameter) LLMs like GPT-2\n(1.5B) & Llama 2 (7B) in clinical domain. So in this work, we leverage 100B+\nGPT variants to act as synthetic feedback experts offering expert-level edit\nfeedback, that is used to reduce hallucinations and align weaker (<10B\nparameter) LLMs with medical facts using two distinct alignment algorithms (DPO\n& SALT), endeavoring to narrow the divide between AI-generated content and\nfactual accuracy. This highlights the substantial potential of LLM-based\nsynthetic edits in enhancing the alignment of clinical factuality.\n", "link": "http://arxiv.org/abs/2402.13919v3", "date": "2024-04-18", "relevancy": 1.3827, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.505}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4487}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4474}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20SYNFAC-EDIT%3A%20Synthetic%20Imitation%20Edit%20Feedback%20for%20Factual%20Alignment%20in%0A%20%20Clinical%20Summarization&body=Title%3A%20SYNFAC-EDIT%3A%20Synthetic%20Imitation%20Edit%20Feedback%20for%20Factual%20Alignment%20in%0A%20%20Clinical%20Summarization%0AAuthor%3A%20Prakamya%20Mishra%20and%20Zonghai%20Yao%20and%20Parth%20Vashisht%20and%20Feiyun%20Ouyang%20and%20Beining%20Wang%20and%20Vidhi%20Dhaval%20Mody%20and%20Hong%20Yu%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20such%20as%20GPT%20%26%20Llama%20have%20demonstrated%0Asignificant%20achievements%20in%20summarization%20tasks%20but%20struggle%20with%20factual%0Ainaccuracies%2C%20a%20critical%20issue%20in%20clinical%20NLP%20applications%20where%20errors%20could%0Alead%20to%20serious%20consequences.%20To%20counter%20the%20high%20costs%20and%20limited%0Aavailability%20of%20expert-annotated%20data%20for%20factual%20alignment%2C%20this%20study%0Aintroduces%20an%20innovative%20pipeline%20that%20utilizes%20%3E100B%20parameter%20GPT%20variants%0Alike%20GPT-3.5%20%26%20GPT-4%20to%20act%20as%20synthetic%20experts%20to%20generate%20high-quality%0Asynthetics%20feedback%20aimed%20at%20enhancing%20factual%20consistency%20in%20clinical%20note%0Asummarization.%20Our%20research%20primarily%20focuses%20on%20edit%20feedback%20generated%20by%0Athese%20synthetic%20feedback%20experts%20without%20additional%20human%20annotations%2C%0Amirroring%20and%20optimizing%20the%20practical%20scenario%20in%20which%20medical%20professionals%0Arefine%20AI%20system%20outputs.%20Although%20such%20100B%2B%20parameter%20GPT%20variants%20have%0Aproven%20to%20demonstrate%20expertise%20in%20various%20clinical%20NLP%20tasks%2C%20such%20as%20the%0AMedical%20Licensing%20Examination%2C%20there%20is%20scant%20research%20on%20their%20capacity%20to%20act%0Aas%20synthetic%20feedback%20experts%20and%20deliver%20expert-level%20edit%20feedback%20for%0Aimproving%20the%20generation%20quality%20of%20weaker%20%28%3C10B%20parameter%29%20LLMs%20like%20GPT-2%0A%281.5B%29%20%26%20Llama%202%20%287B%29%20in%20clinical%20domain.%20So%20in%20this%20work%2C%20we%20leverage%20100B%2B%0AGPT%20variants%20to%20act%20as%20synthetic%20feedback%20experts%20offering%20expert-level%20edit%0Afeedback%2C%20that%20is%20used%20to%20reduce%20hallucinations%20and%20align%20weaker%20%28%3C10B%0Aparameter%29%20LLMs%20with%20medical%20facts%20using%20two%20distinct%20alignment%20algorithms%20%28DPO%0A%26%20SALT%29%2C%20endeavoring%20to%20narrow%20the%20divide%20between%20AI-generated%20content%20and%0Afactual%20accuracy.%20This%20highlights%20the%20substantial%20potential%20of%20LLM-based%0Asynthetic%20edits%20in%20enhancing%20the%20alignment%20of%20clinical%20factuality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13919v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SYNFAC-EDIT%3A%20Synthetic%20Imitation%20Edit%20Feedback%20for%20Factual%20Alignment%20in%0A%20%20Clinical%20Summarization&entry.906535625=Prakamya%20Mishra%20and%20Zonghai%20Yao%20and%20Parth%20Vashisht%20and%20Feiyun%20Ouyang%20and%20Beining%20Wang%20and%20Vidhi%20Dhaval%20Mody%20and%20Hong%20Yu&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20such%20as%20GPT%20%26%20Llama%20have%20demonstrated%0Asignificant%20achievements%20in%20summarization%20tasks%20but%20struggle%20with%20factual%0Ainaccuracies%2C%20a%20critical%20issue%20in%20clinical%20NLP%20applications%20where%20errors%20could%0Alead%20to%20serious%20consequences.%20To%20counter%20the%20high%20costs%20and%20limited%0Aavailability%20of%20expert-annotated%20data%20for%20factual%20alignment%2C%20this%20study%0Aintroduces%20an%20innovative%20pipeline%20that%20utilizes%20%3E100B%20parameter%20GPT%20variants%0Alike%20GPT-3.5%20%26%20GPT-4%20to%20act%20as%20synthetic%20experts%20to%20generate%20high-quality%0Asynthetics%20feedback%20aimed%20at%20enhancing%20factual%20consistency%20in%20clinical%20note%0Asummarization.%20Our%20research%20primarily%20focuses%20on%20edit%20feedback%20generated%20by%0Athese%20synthetic%20feedback%20experts%20without%20additional%20human%20annotations%2C%0Amirroring%20and%20optimizing%20the%20practical%20scenario%20in%20which%20medical%20professionals%0Arefine%20AI%20system%20outputs.%20Although%20such%20100B%2B%20parameter%20GPT%20variants%20have%0Aproven%20to%20demonstrate%20expertise%20in%20various%20clinical%20NLP%20tasks%2C%20such%20as%20the%0AMedical%20Licensing%20Examination%2C%20there%20is%20scant%20research%20on%20their%20capacity%20to%20act%0Aas%20synthetic%20feedback%20experts%20and%20deliver%20expert-level%20edit%20feedback%20for%0Aimproving%20the%20generation%20quality%20of%20weaker%20%28%3C10B%20parameter%29%20LLMs%20like%20GPT-2%0A%281.5B%29%20%26%20Llama%202%20%287B%29%20in%20clinical%20domain.%20So%20in%20this%20work%2C%20we%20leverage%20100B%2B%0AGPT%20variants%20to%20act%20as%20synthetic%20feedback%20experts%20offering%20expert-level%20edit%0Afeedback%2C%20that%20is%20used%20to%20reduce%20hallucinations%20and%20align%20weaker%20%28%3C10B%0Aparameter%29%20LLMs%20with%20medical%20facts%20using%20two%20distinct%20alignment%20algorithms%20%28DPO%0A%26%20SALT%29%2C%20endeavoring%20to%20narrow%20the%20divide%20between%20AI-generated%20content%20and%0Afactual%20accuracy.%20This%20highlights%20the%20substantial%20potential%20of%20LLM-based%0Asynthetic%20edits%20in%20enhancing%20the%20alignment%20of%20clinical%20factuality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13919v3&entry.124074799=Read"},
{"title": "Matching the Statistical Query Lower Bound for k-sparse Parity Problems\n  with Stochastic Gradient Descent", "author": "Yiwen Kou and Zixiang Chen and Quanquan Gu and Sham M. Kakade", "abstract": "  The $k$-parity problem is a classical problem in computational complexity and\nalgorithmic theory, serving as a key benchmark for understanding computational\nclasses. In this paper, we solve the $k$-parity problem with stochastic\ngradient descent (SGD) on two-layer fully-connected neural networks. We\ndemonstrate that SGD can efficiently solve the $k$-sparse parity problem on a\n$d$-dimensional hypercube ($k\\le O(\\sqrt{d})$) with a sample complexity of\n$\\tilde{O}(d^{k-1})$ using $2^{\\Theta(k)}$ neurons, thus matching the\nestablished $\\Omega(d^{k})$ lower bounds of Statistical Query (SQ) models. Our\ntheoretical analysis begins by constructing a good neural network capable of\ncorrectly solving the $k$-parity problem. We then demonstrate how a trained\nneural network with SGD can effectively approximate this good network, solving\nthe $k$-parity problem with small statistical errors. Our theoretical results\nand findings are supported by empirical evidence, showcasing the efficiency and\nefficacy of our approach.\n", "link": "http://arxiv.org/abs/2404.12376v1", "date": "2024-04-18", "relevancy": 1.3819, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4673}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4531}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4516}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Matching%20the%20Statistical%20Query%20Lower%20Bound%20for%20k-sparse%20Parity%20Problems%0A%20%20with%20Stochastic%20Gradient%20Descent&body=Title%3A%20Matching%20the%20Statistical%20Query%20Lower%20Bound%20for%20k-sparse%20Parity%20Problems%0A%20%20with%20Stochastic%20Gradient%20Descent%0AAuthor%3A%20Yiwen%20Kou%20and%20Zixiang%20Chen%20and%20Quanquan%20Gu%20and%20Sham%20M.%20Kakade%0AAbstract%3A%20%20%20The%20%24k%24-parity%20problem%20is%20a%20classical%20problem%20in%20computational%20complexity%20and%0Aalgorithmic%20theory%2C%20serving%20as%20a%20key%20benchmark%20for%20understanding%20computational%0Aclasses.%20In%20this%20paper%2C%20we%20solve%20the%20%24k%24-parity%20problem%20with%20stochastic%0Agradient%20descent%20%28SGD%29%20on%20two-layer%20fully-connected%20neural%20networks.%20We%0Ademonstrate%20that%20SGD%20can%20efficiently%20solve%20the%20%24k%24-sparse%20parity%20problem%20on%20a%0A%24d%24-dimensional%20hypercube%20%28%24k%5Cle%20O%28%5Csqrt%7Bd%7D%29%24%29%20with%20a%20sample%20complexity%20of%0A%24%5Ctilde%7BO%7D%28d%5E%7Bk-1%7D%29%24%20using%20%242%5E%7B%5CTheta%28k%29%7D%24%20neurons%2C%20thus%20matching%20the%0Aestablished%20%24%5COmega%28d%5E%7Bk%7D%29%24%20lower%20bounds%20of%20Statistical%20Query%20%28SQ%29%20models.%20Our%0Atheoretical%20analysis%20begins%20by%20constructing%20a%20good%20neural%20network%20capable%20of%0Acorrectly%20solving%20the%20%24k%24-parity%20problem.%20We%20then%20demonstrate%20how%20a%20trained%0Aneural%20network%20with%20SGD%20can%20effectively%20approximate%20this%20good%20network%2C%20solving%0Athe%20%24k%24-parity%20problem%20with%20small%20statistical%20errors.%20Our%20theoretical%20results%0Aand%20findings%20are%20supported%20by%20empirical%20evidence%2C%20showcasing%20the%20efficiency%20and%0Aefficacy%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12376v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matching%20the%20Statistical%20Query%20Lower%20Bound%20for%20k-sparse%20Parity%20Problems%0A%20%20with%20Stochastic%20Gradient%20Descent&entry.906535625=Yiwen%20Kou%20and%20Zixiang%20Chen%20and%20Quanquan%20Gu%20and%20Sham%20M.%20Kakade&entry.1292438233=%20%20The%20%24k%24-parity%20problem%20is%20a%20classical%20problem%20in%20computational%20complexity%20and%0Aalgorithmic%20theory%2C%20serving%20as%20a%20key%20benchmark%20for%20understanding%20computational%0Aclasses.%20In%20this%20paper%2C%20we%20solve%20the%20%24k%24-parity%20problem%20with%20stochastic%0Agradient%20descent%20%28SGD%29%20on%20two-layer%20fully-connected%20neural%20networks.%20We%0Ademonstrate%20that%20SGD%20can%20efficiently%20solve%20the%20%24k%24-sparse%20parity%20problem%20on%20a%0A%24d%24-dimensional%20hypercube%20%28%24k%5Cle%20O%28%5Csqrt%7Bd%7D%29%24%29%20with%20a%20sample%20complexity%20of%0A%24%5Ctilde%7BO%7D%28d%5E%7Bk-1%7D%29%24%20using%20%242%5E%7B%5CTheta%28k%29%7D%24%20neurons%2C%20thus%20matching%20the%0Aestablished%20%24%5COmega%28d%5E%7Bk%7D%29%24%20lower%20bounds%20of%20Statistical%20Query%20%28SQ%29%20models.%20Our%0Atheoretical%20analysis%20begins%20by%20constructing%20a%20good%20neural%20network%20capable%20of%0Acorrectly%20solving%20the%20%24k%24-parity%20problem.%20We%20then%20demonstrate%20how%20a%20trained%0Aneural%20network%20with%20SGD%20can%20effectively%20approximate%20this%20good%20network%2C%20solving%0Athe%20%24k%24-parity%20problem%20with%20small%20statistical%20errors.%20Our%20theoretical%20results%0Aand%20findings%20are%20supported%20by%20empirical%20evidence%2C%20showcasing%20the%20efficiency%20and%0Aefficacy%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12376v1&entry.124074799=Read"},
{"title": "VisEvol: Visual Analytics to Support Hyperparameter Search through\n  Evolutionary Optimization", "author": "Angelos Chatzimparmpas and Rafael M. Martins and Kostiantyn Kucher and Andreas Kerren", "abstract": "  During the training phase of machine learning (ML) models, it is usually\nnecessary to configure several hyperparameters. This process is computationally\nintensive and requires an extensive search to infer the best hyperparameter set\nfor the given problem. The challenge is exacerbated by the fact that most ML\nmodels are complex internally, and training involves trial-and-error processes\nthat could remarkably affect the predictive result. Moreover, each\nhyperparameter of an ML algorithm is potentially intertwined with the others,\nand changing it might result in unforeseeable impacts on the remaining\nhyperparameters. Evolutionary optimization is a promising method to try and\naddress those issues. According to this method, performant models are stored,\nwhile the remainder are improved through crossover and mutation processes\ninspired by genetic algorithms. We present VisEvol, a visual analytics tool\nthat supports interactive exploration of hyperparameters and intervention in\nthis evolutionary procedure. In summary, our proposed tool helps the user to\ngenerate new models through evolution and eventually explore powerful\nhyperparameter combinations in diverse regions of the extensive hyperparameter\nspace. The outcome is a voting ensemble (with equal rights) that boosts the\nfinal predictive performance. The utility and applicability of VisEvol are\ndemonstrated with two use cases and interviews with ML experts who evaluated\nthe effectiveness of the tool.\n", "link": "http://arxiv.org/abs/2012.01205v4", "date": "2024-04-18", "relevancy": 1.3649, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4649}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.459}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4494}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20VisEvol%3A%20Visual%20Analytics%20to%20Support%20Hyperparameter%20Search%20through%0A%20%20Evolutionary%20Optimization&body=Title%3A%20VisEvol%3A%20Visual%20Analytics%20to%20Support%20Hyperparameter%20Search%20through%0A%20%20Evolutionary%20Optimization%0AAuthor%3A%20Angelos%20Chatzimparmpas%20and%20Rafael%20M.%20Martins%20and%20Kostiantyn%20Kucher%20and%20Andreas%20Kerren%0AAbstract%3A%20%20%20During%20the%20training%20phase%20of%20machine%20learning%20%28ML%29%20models%2C%20it%20is%20usually%0Anecessary%20to%20configure%20several%20hyperparameters.%20This%20process%20is%20computationally%0Aintensive%20and%20requires%20an%20extensive%20search%20to%20infer%20the%20best%20hyperparameter%20set%0Afor%20the%20given%20problem.%20The%20challenge%20is%20exacerbated%20by%20the%20fact%20that%20most%20ML%0Amodels%20are%20complex%20internally%2C%20and%20training%20involves%20trial-and-error%20processes%0Athat%20could%20remarkably%20affect%20the%20predictive%20result.%20Moreover%2C%20each%0Ahyperparameter%20of%20an%20ML%20algorithm%20is%20potentially%20intertwined%20with%20the%20others%2C%0Aand%20changing%20it%20might%20result%20in%20unforeseeable%20impacts%20on%20the%20remaining%0Ahyperparameters.%20Evolutionary%20optimization%20is%20a%20promising%20method%20to%20try%20and%0Aaddress%20those%20issues.%20According%20to%20this%20method%2C%20performant%20models%20are%20stored%2C%0Awhile%20the%20remainder%20are%20improved%20through%20crossover%20and%20mutation%20processes%0Ainspired%20by%20genetic%20algorithms.%20We%20present%20VisEvol%2C%20a%20visual%20analytics%20tool%0Athat%20supports%20interactive%20exploration%20of%20hyperparameters%20and%20intervention%20in%0Athis%20evolutionary%20procedure.%20In%20summary%2C%20our%20proposed%20tool%20helps%20the%20user%20to%0Agenerate%20new%20models%20through%20evolution%20and%20eventually%20explore%20powerful%0Ahyperparameter%20combinations%20in%20diverse%20regions%20of%20the%20extensive%20hyperparameter%0Aspace.%20The%20outcome%20is%20a%20voting%20ensemble%20%28with%20equal%20rights%29%20that%20boosts%20the%0Afinal%20predictive%20performance.%20The%20utility%20and%20applicability%20of%20VisEvol%20are%0Ademonstrated%20with%20two%20use%20cases%20and%20interviews%20with%20ML%20experts%20who%20evaluated%0Athe%20effectiveness%20of%20the%20tool.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2012.01205v4", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VisEvol%3A%20Visual%20Analytics%20to%20Support%20Hyperparameter%20Search%20through%0A%20%20Evolutionary%20Optimization&entry.906535625=Angelos%20Chatzimparmpas%20and%20Rafael%20M.%20Martins%20and%20Kostiantyn%20Kucher%20and%20Andreas%20Kerren&entry.1292438233=%20%20During%20the%20training%20phase%20of%20machine%20learning%20%28ML%29%20models%2C%20it%20is%20usually%0Anecessary%20to%20configure%20several%20hyperparameters.%20This%20process%20is%20computationally%0Aintensive%20and%20requires%20an%20extensive%20search%20to%20infer%20the%20best%20hyperparameter%20set%0Afor%20the%20given%20problem.%20The%20challenge%20is%20exacerbated%20by%20the%20fact%20that%20most%20ML%0Amodels%20are%20complex%20internally%2C%20and%20training%20involves%20trial-and-error%20processes%0Athat%20could%20remarkably%20affect%20the%20predictive%20result.%20Moreover%2C%20each%0Ahyperparameter%20of%20an%20ML%20algorithm%20is%20potentially%20intertwined%20with%20the%20others%2C%0Aand%20changing%20it%20might%20result%20in%20unforeseeable%20impacts%20on%20the%20remaining%0Ahyperparameters.%20Evolutionary%20optimization%20is%20a%20promising%20method%20to%20try%20and%0Aaddress%20those%20issues.%20According%20to%20this%20method%2C%20performant%20models%20are%20stored%2C%0Awhile%20the%20remainder%20are%20improved%20through%20crossover%20and%20mutation%20processes%0Ainspired%20by%20genetic%20algorithms.%20We%20present%20VisEvol%2C%20a%20visual%20analytics%20tool%0Athat%20supports%20interactive%20exploration%20of%20hyperparameters%20and%20intervention%20in%0Athis%20evolutionary%20procedure.%20In%20summary%2C%20our%20proposed%20tool%20helps%20the%20user%20to%0Agenerate%20new%20models%20through%20evolution%20and%20eventually%20explore%20powerful%0Ahyperparameter%20combinations%20in%20diverse%20regions%20of%20the%20extensive%20hyperparameter%0Aspace.%20The%20outcome%20is%20a%20voting%20ensemble%20%28with%20equal%20rights%29%20that%20boosts%20the%0Afinal%20predictive%20performance.%20The%20utility%20and%20applicability%20of%20VisEvol%20are%0Ademonstrated%20with%20two%20use%20cases%20and%20interviews%20with%20ML%20experts%20who%20evaluated%0Athe%20effectiveness%20of%20the%20tool.%0A&entry.1838667208=http%3A//arxiv.org/abs/2012.01205v4&entry.124074799=Read"},
{"title": "Information theory unifies atomistic machine learning, uncertainty\n  quantification, and materials thermodynamics", "author": "Daniel Schwalbe-Koda and Sebastien Hamel and Babak Sadigh and Fei Zhou and Vincenzo Lordi", "abstract": "  An accurate description of information is relevant for a range of problems in\natomistic modeling, such as sampling methods, detecting rare events, analyzing\ndatasets, or performing uncertainty quantification (UQ) in machine learning\n(ML)-driven simulations. Although individual methods have been proposed for\neach of these tasks, they lack a common theoretical background integrating\ntheir solutions. Here, we introduce an information theoretical framework that\nunifies predictions of phase transformations, kinetic events, dataset\noptimality, and model-free UQ from atomistic simulations, thus bridging\nmaterials modeling, ML, and statistical mechanics. We first demonstrate that,\nfor a proposed representation, the information entropy of a distribution of\natom-centered environments is a surrogate value for thermodynamic entropy.\nUsing molecular dynamics (MD) simulations, we show that information entropy\ndifferences from trajectories can be used to build phase diagrams, identify\nrare events, and recover classical theories of nucleation. Building on these\nresults, we use this general concept of entropy to quantify information in\ndatasets for ML interatomic potentials (IPs), informing compression, explaining\ntrends in testing errors, and evaluating the efficiency of active learning\nstrategies. Finally, we propose a model-free UQ method for MLIPs using\ninformation entropy, showing it reliably detects extrapolation regimes, scales\nto millions of atoms, and goes beyond model errors. This method is made\navailable as the package QUESTS: Quick Uncertainty and Entropy via STructural\nSimilarity, providing a new unifying theory for data-driven atomistic modeling\nand combining efforts in ML, first-principles thermodynamics, and simulations.\n", "link": "http://arxiv.org/abs/2404.12367v1", "date": "2024-04-18", "relevancy": 1.3624, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4718}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4492}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.449}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Information%20theory%20unifies%20atomistic%20machine%20learning%2C%20uncertainty%0A%20%20quantification%2C%20and%20materials%20thermodynamics&body=Title%3A%20Information%20theory%20unifies%20atomistic%20machine%20learning%2C%20uncertainty%0A%20%20quantification%2C%20and%20materials%20thermodynamics%0AAuthor%3A%20Daniel%20Schwalbe-Koda%20and%20Sebastien%20Hamel%20and%20Babak%20Sadigh%20and%20Fei%20Zhou%20and%20Vincenzo%20Lordi%0AAbstract%3A%20%20%20An%20accurate%20description%20of%20information%20is%20relevant%20for%20a%20range%20of%20problems%20in%0Aatomistic%20modeling%2C%20such%20as%20sampling%20methods%2C%20detecting%20rare%20events%2C%20analyzing%0Adatasets%2C%20or%20performing%20uncertainty%20quantification%20%28UQ%29%20in%20machine%20learning%0A%28ML%29-driven%20simulations.%20Although%20individual%20methods%20have%20been%20proposed%20for%0Aeach%20of%20these%20tasks%2C%20they%20lack%20a%20common%20theoretical%20background%20integrating%0Atheir%20solutions.%20Here%2C%20we%20introduce%20an%20information%20theoretical%20framework%20that%0Aunifies%20predictions%20of%20phase%20transformations%2C%20kinetic%20events%2C%20dataset%0Aoptimality%2C%20and%20model-free%20UQ%20from%20atomistic%20simulations%2C%20thus%20bridging%0Amaterials%20modeling%2C%20ML%2C%20and%20statistical%20mechanics.%20We%20first%20demonstrate%20that%2C%0Afor%20a%20proposed%20representation%2C%20the%20information%20entropy%20of%20a%20distribution%20of%0Aatom-centered%20environments%20is%20a%20surrogate%20value%20for%20thermodynamic%20entropy.%0AUsing%20molecular%20dynamics%20%28MD%29%20simulations%2C%20we%20show%20that%20information%20entropy%0Adifferences%20from%20trajectories%20can%20be%20used%20to%20build%20phase%20diagrams%2C%20identify%0Arare%20events%2C%20and%20recover%20classical%20theories%20of%20nucleation.%20Building%20on%20these%0Aresults%2C%20we%20use%20this%20general%20concept%20of%20entropy%20to%20quantify%20information%20in%0Adatasets%20for%20ML%20interatomic%20potentials%20%28IPs%29%2C%20informing%20compression%2C%20explaining%0Atrends%20in%20testing%20errors%2C%20and%20evaluating%20the%20efficiency%20of%20active%20learning%0Astrategies.%20Finally%2C%20we%20propose%20a%20model-free%20UQ%20method%20for%20MLIPs%20using%0Ainformation%20entropy%2C%20showing%20it%20reliably%20detects%20extrapolation%20regimes%2C%20scales%0Ato%20millions%20of%20atoms%2C%20and%20goes%20beyond%20model%20errors.%20This%20method%20is%20made%0Aavailable%20as%20the%20package%20QUESTS%3A%20Quick%20Uncertainty%20and%20Entropy%20via%20STructural%0ASimilarity%2C%20providing%20a%20new%20unifying%20theory%20for%20data-driven%20atomistic%20modeling%0Aand%20combining%20efforts%20in%20ML%2C%20first-principles%20thermodynamics%2C%20and%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12367v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Information%20theory%20unifies%20atomistic%20machine%20learning%2C%20uncertainty%0A%20%20quantification%2C%20and%20materials%20thermodynamics&entry.906535625=Daniel%20Schwalbe-Koda%20and%20Sebastien%20Hamel%20and%20Babak%20Sadigh%20and%20Fei%20Zhou%20and%20Vincenzo%20Lordi&entry.1292438233=%20%20An%20accurate%20description%20of%20information%20is%20relevant%20for%20a%20range%20of%20problems%20in%0Aatomistic%20modeling%2C%20such%20as%20sampling%20methods%2C%20detecting%20rare%20events%2C%20analyzing%0Adatasets%2C%20or%20performing%20uncertainty%20quantification%20%28UQ%29%20in%20machine%20learning%0A%28ML%29-driven%20simulations.%20Although%20individual%20methods%20have%20been%20proposed%20for%0Aeach%20of%20these%20tasks%2C%20they%20lack%20a%20common%20theoretical%20background%20integrating%0Atheir%20solutions.%20Here%2C%20we%20introduce%20an%20information%20theoretical%20framework%20that%0Aunifies%20predictions%20of%20phase%20transformations%2C%20kinetic%20events%2C%20dataset%0Aoptimality%2C%20and%20model-free%20UQ%20from%20atomistic%20simulations%2C%20thus%20bridging%0Amaterials%20modeling%2C%20ML%2C%20and%20statistical%20mechanics.%20We%20first%20demonstrate%20that%2C%0Afor%20a%20proposed%20representation%2C%20the%20information%20entropy%20of%20a%20distribution%20of%0Aatom-centered%20environments%20is%20a%20surrogate%20value%20for%20thermodynamic%20entropy.%0AUsing%20molecular%20dynamics%20%28MD%29%20simulations%2C%20we%20show%20that%20information%20entropy%0Adifferences%20from%20trajectories%20can%20be%20used%20to%20build%20phase%20diagrams%2C%20identify%0Arare%20events%2C%20and%20recover%20classical%20theories%20of%20nucleation.%20Building%20on%20these%0Aresults%2C%20we%20use%20this%20general%20concept%20of%20entropy%20to%20quantify%20information%20in%0Adatasets%20for%20ML%20interatomic%20potentials%20%28IPs%29%2C%20informing%20compression%2C%20explaining%0Atrends%20in%20testing%20errors%2C%20and%20evaluating%20the%20efficiency%20of%20active%20learning%0Astrategies.%20Finally%2C%20we%20propose%20a%20model-free%20UQ%20method%20for%20MLIPs%20using%0Ainformation%20entropy%2C%20showing%20it%20reliably%20detects%20extrapolation%20regimes%2C%20scales%0Ato%20millions%20of%20atoms%2C%20and%20goes%20beyond%20model%20errors.%20This%20method%20is%20made%0Aavailable%20as%20the%20package%20QUESTS%3A%20Quick%20Uncertainty%20and%20Entropy%20via%20STructural%0ASimilarity%2C%20providing%20a%20new%20unifying%20theory%20for%20data-driven%20atomistic%20modeling%0Aand%20combining%20efforts%20in%20ML%2C%20first-principles%20thermodynamics%2C%20and%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12367v1&entry.124074799=Read"},
{"title": "floZ: Evidence estimation from posterior samples with normalizing flows", "author": "Rahul Srinivasan and Marco Crisostomi and Roberto Trotta and Enrico Barausse and Matteo Breschi", "abstract": "  We propose a novel method (floZ), based on normalizing flows, for estimating\nthe Bayesian evidence (and its numerical uncertainty) from a set of samples\ndrawn from the unnormalized posterior distribution. We validate it on\ndistributions whose evidence is known analytically, up to 15 parameter space\ndimensions, and compare with two state-of-the-art techniques for estimating the\nevidence: nested sampling (which computes the evidence as its main target) and\na k-nearest-neighbors technique that produces evidence estimates from posterior\nsamples. Provided representative samples from the target posterior are\navailable, our method is more robust to posterior distributions with sharp\nfeatures, especially in higher dimensions. It has wide applicability, e.g., to\nestimate the evidence from variational inference, Markov-chain Monte Carlo\nsamples, or any other method that delivers samples from the unnormalized\nposterior density.\n", "link": "http://arxiv.org/abs/2404.12294v1", "date": "2024-04-18", "relevancy": 1.3595, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4629}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4511}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4485}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20floZ%3A%20Evidence%20estimation%20from%20posterior%20samples%20with%20normalizing%20flows&body=Title%3A%20floZ%3A%20Evidence%20estimation%20from%20posterior%20samples%20with%20normalizing%20flows%0AAuthor%3A%20Rahul%20Srinivasan%20and%20Marco%20Crisostomi%20and%20Roberto%20Trotta%20and%20Enrico%20Barausse%20and%20Matteo%20Breschi%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20method%20%28floZ%29%2C%20based%20on%20normalizing%20flows%2C%20for%20estimating%0Athe%20Bayesian%20evidence%20%28and%20its%20numerical%20uncertainty%29%20from%20a%20set%20of%20samples%0Adrawn%20from%20the%20unnormalized%20posterior%20distribution.%20We%20validate%20it%20on%0Adistributions%20whose%20evidence%20is%20known%20analytically%2C%20up%20to%2015%20parameter%20space%0Adimensions%2C%20and%20compare%20with%20two%20state-of-the-art%20techniques%20for%20estimating%20the%0Aevidence%3A%20nested%20sampling%20%28which%20computes%20the%20evidence%20as%20its%20main%20target%29%20and%0Aa%20k-nearest-neighbors%20technique%20that%20produces%20evidence%20estimates%20from%20posterior%0Asamples.%20Provided%20representative%20samples%20from%20the%20target%20posterior%20are%0Aavailable%2C%20our%20method%20is%20more%20robust%20to%20posterior%20distributions%20with%20sharp%0Afeatures%2C%20especially%20in%20higher%20dimensions.%20It%20has%20wide%20applicability%2C%20e.g.%2C%20to%0Aestimate%20the%20evidence%20from%20variational%20inference%2C%20Markov-chain%20Monte%20Carlo%0Asamples%2C%20or%20any%20other%20method%20that%20delivers%20samples%20from%20the%20unnormalized%0Aposterior%20density.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12294v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=floZ%3A%20Evidence%20estimation%20from%20posterior%20samples%20with%20normalizing%20flows&entry.906535625=Rahul%20Srinivasan%20and%20Marco%20Crisostomi%20and%20Roberto%20Trotta%20and%20Enrico%20Barausse%20and%20Matteo%20Breschi&entry.1292438233=%20%20We%20propose%20a%20novel%20method%20%28floZ%29%2C%20based%20on%20normalizing%20flows%2C%20for%20estimating%0Athe%20Bayesian%20evidence%20%28and%20its%20numerical%20uncertainty%29%20from%20a%20set%20of%20samples%0Adrawn%20from%20the%20unnormalized%20posterior%20distribution.%20We%20validate%20it%20on%0Adistributions%20whose%20evidence%20is%20known%20analytically%2C%20up%20to%2015%20parameter%20space%0Adimensions%2C%20and%20compare%20with%20two%20state-of-the-art%20techniques%20for%20estimating%20the%0Aevidence%3A%20nested%20sampling%20%28which%20computes%20the%20evidence%20as%20its%20main%20target%29%20and%0Aa%20k-nearest-neighbors%20technique%20that%20produces%20evidence%20estimates%20from%20posterior%0Asamples.%20Provided%20representative%20samples%20from%20the%20target%20posterior%20are%0Aavailable%2C%20our%20method%20is%20more%20robust%20to%20posterior%20distributions%20with%20sharp%0Afeatures%2C%20especially%20in%20higher%20dimensions.%20It%20has%20wide%20applicability%2C%20e.g.%2C%20to%0Aestimate%20the%20evidence%20from%20variational%20inference%2C%20Markov-chain%20Monte%20Carlo%0Asamples%2C%20or%20any%20other%20method%20that%20delivers%20samples%20from%20the%20unnormalized%0Aposterior%20density.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12294v1&entry.124074799=Read"},
{"title": "Online Advertisements with LLMs: Opportunities and Challenges", "author": "Soheil Feizi and MohammadTaghi Hajiaghayi and Keivan Rezaei and Suho Shin", "abstract": "  This paper explores the potential for leveraging Large Language Models (LLM)\nin the realm of online advertising systems. We delve into essential\nrequirements including privacy, latency, reliability as well as the\nsatisfaction of users and advertisers that such a system must fulfill. We\nfurther introduce a general framework for LLM advertisement, consisting of\nmodification, bidding, prediction, and auction modules. Different design\nconsiderations for each module are presented. Fundamental questions regarding\npracticality, efficiency, and implementation challenges of these designs are\nraised for future research. Finally, we explore the prospect of LLM-based\ndynamic creative optimization as a means to significantly enhance the appeal of\nadvertisements to users and discuss its additional challenges.\n", "link": "http://arxiv.org/abs/2311.07601v3", "date": "2024-04-18", "relevancy": 1.3562, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.455}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4522}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4488}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Online%20Advertisements%20with%20LLMs%3A%20Opportunities%20and%20Challenges&body=Title%3A%20Online%20Advertisements%20with%20LLMs%3A%20Opportunities%20and%20Challenges%0AAuthor%3A%20Soheil%20Feizi%20and%20MohammadTaghi%20Hajiaghayi%20and%20Keivan%20Rezaei%20and%20Suho%20Shin%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20potential%20for%20leveraging%20Large%20Language%20Models%20%28LLM%29%0Ain%20the%20realm%20of%20online%20advertising%20systems.%20We%20delve%20into%20essential%0Arequirements%20including%20privacy%2C%20latency%2C%20reliability%20as%20well%20as%20the%0Asatisfaction%20of%20users%20and%20advertisers%20that%20such%20a%20system%20must%20fulfill.%20We%0Afurther%20introduce%20a%20general%20framework%20for%20LLM%20advertisement%2C%20consisting%20of%0Amodification%2C%20bidding%2C%20prediction%2C%20and%20auction%20modules.%20Different%20design%0Aconsiderations%20for%20each%20module%20are%20presented.%20Fundamental%20questions%20regarding%0Apracticality%2C%20efficiency%2C%20and%20implementation%20challenges%20of%20these%20designs%20are%0Araised%20for%20future%20research.%20Finally%2C%20we%20explore%20the%20prospect%20of%20LLM-based%0Adynamic%20creative%20optimization%20as%20a%20means%20to%20significantly%20enhance%20the%20appeal%20of%0Aadvertisements%20to%20users%20and%20discuss%20its%20additional%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.07601v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Advertisements%20with%20LLMs%3A%20Opportunities%20and%20Challenges&entry.906535625=Soheil%20Feizi%20and%20MohammadTaghi%20Hajiaghayi%20and%20Keivan%20Rezaei%20and%20Suho%20Shin&entry.1292438233=%20%20This%20paper%20explores%20the%20potential%20for%20leveraging%20Large%20Language%20Models%20%28LLM%29%0Ain%20the%20realm%20of%20online%20advertising%20systems.%20We%20delve%20into%20essential%0Arequirements%20including%20privacy%2C%20latency%2C%20reliability%20as%20well%20as%20the%0Asatisfaction%20of%20users%20and%20advertisers%20that%20such%20a%20system%20must%20fulfill.%20We%0Afurther%20introduce%20a%20general%20framework%20for%20LLM%20advertisement%2C%20consisting%20of%0Amodification%2C%20bidding%2C%20prediction%2C%20and%20auction%20modules.%20Different%20design%0Aconsiderations%20for%20each%20module%20are%20presented.%20Fundamental%20questions%20regarding%0Apracticality%2C%20efficiency%2C%20and%20implementation%20challenges%20of%20these%20designs%20are%0Araised%20for%20future%20research.%20Finally%2C%20we%20explore%20the%20prospect%20of%20LLM-based%0Adynamic%20creative%20optimization%20as%20a%20means%20to%20significantly%20enhance%20the%20appeal%20of%0Aadvertisements%20to%20users%20and%20discuss%20its%20additional%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.07601v3&entry.124074799=Read"},
{"title": "Accounting for AI and Users Shaping One Another: The Role of\n  Mathematical Models", "author": "Sarah Dean and Evan Dong and Meena Jagadeesan and Liu Leqi", "abstract": "  As AI systems enter into a growing number of societal domains, these systems\nincreasingly shape and are shaped by user preferences, opinions, and behaviors.\nHowever, the design of AI systems rarely accounts for how AI and users shape\none another. In this position paper, we argue for the development of formal\ninteraction models which mathematically specify how AI and users shape one\nanother. Formal interaction models can be leveraged to (1) specify interactions\nfor implementation, (2) monitor interactions through empirical analysis, (3)\nanticipate societal impacts via counterfactual analysis, and (4) control\nsocietal impacts via interventions. The design space of formal interaction\nmodels is vast, and model design requires careful consideration of factors such\nas style, granularity, mathematical complexity, and measurability. Using\ncontent recommender systems as a case study, we critically examine the nascent\nliterature of formal interaction models with respect to these use-cases and\ndesign axes. More broadly, we call for the community to leverage formal\ninteraction models when designing, evaluating, or auditing any AI system which\ninteracts with users.\n", "link": "http://arxiv.org/abs/2404.12366v1", "date": "2024-04-18", "relevancy": 0.8645, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.442}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4325}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4223}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Accounting%20for%20AI%20and%20Users%20Shaping%20One%20Another%3A%20The%20Role%20of%0A%20%20Mathematical%20Models&body=Title%3A%20Accounting%20for%20AI%20and%20Users%20Shaping%20One%20Another%3A%20The%20Role%20of%0A%20%20Mathematical%20Models%0AAuthor%3A%20Sarah%20Dean%20and%20Evan%20Dong%20and%20Meena%20Jagadeesan%20and%20Liu%20Leqi%0AAbstract%3A%20%20%20As%20AI%20systems%20enter%20into%20a%20growing%20number%20of%20societal%20domains%2C%20these%20systems%0Aincreasingly%20shape%20and%20are%20shaped%20by%20user%20preferences%2C%20opinions%2C%20and%20behaviors.%0AHowever%2C%20the%20design%20of%20AI%20systems%20rarely%20accounts%20for%20how%20AI%20and%20users%20shape%0Aone%20another.%20In%20this%20position%20paper%2C%20we%20argue%20for%20the%20development%20of%20formal%0Ainteraction%20models%20which%20mathematically%20specify%20how%20AI%20and%20users%20shape%20one%0Aanother.%20Formal%20interaction%20models%20can%20be%20leveraged%20to%20%281%29%20specify%20interactions%0Afor%20implementation%2C%20%282%29%20monitor%20interactions%20through%20empirical%20analysis%2C%20%283%29%0Aanticipate%20societal%20impacts%20via%20counterfactual%20analysis%2C%20and%20%284%29%20control%0Asocietal%20impacts%20via%20interventions.%20The%20design%20space%20of%20formal%20interaction%0Amodels%20is%20vast%2C%20and%20model%20design%20requires%20careful%20consideration%20of%20factors%20such%0Aas%20style%2C%20granularity%2C%20mathematical%20complexity%2C%20and%20measurability.%20Using%0Acontent%20recommender%20systems%20as%20a%20case%20study%2C%20we%20critically%20examine%20the%20nascent%0Aliterature%20of%20formal%20interaction%20models%20with%20respect%20to%20these%20use-cases%20and%0Adesign%20axes.%20More%20broadly%2C%20we%20call%20for%20the%20community%20to%20leverage%20formal%0Ainteraction%20models%20when%20designing%2C%20evaluating%2C%20or%20auditing%20any%20AI%20system%20which%0Ainteracts%20with%20users.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12366v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accounting%20for%20AI%20and%20Users%20Shaping%20One%20Another%3A%20The%20Role%20of%0A%20%20Mathematical%20Models&entry.906535625=Sarah%20Dean%20and%20Evan%20Dong%20and%20Meena%20Jagadeesan%20and%20Liu%20Leqi&entry.1292438233=%20%20As%20AI%20systems%20enter%20into%20a%20growing%20number%20of%20societal%20domains%2C%20these%20systems%0Aincreasingly%20shape%20and%20are%20shaped%20by%20user%20preferences%2C%20opinions%2C%20and%20behaviors.%0AHowever%2C%20the%20design%20of%20AI%20systems%20rarely%20accounts%20for%20how%20AI%20and%20users%20shape%0Aone%20another.%20In%20this%20position%20paper%2C%20we%20argue%20for%20the%20development%20of%20formal%0Ainteraction%20models%20which%20mathematically%20specify%20how%20AI%20and%20users%20shape%20one%0Aanother.%20Formal%20interaction%20models%20can%20be%20leveraged%20to%20%281%29%20specify%20interactions%0Afor%20implementation%2C%20%282%29%20monitor%20interactions%20through%20empirical%20analysis%2C%20%283%29%0Aanticipate%20societal%20impacts%20via%20counterfactual%20analysis%2C%20and%20%284%29%20control%0Asocietal%20impacts%20via%20interventions.%20The%20design%20space%20of%20formal%20interaction%0Amodels%20is%20vast%2C%20and%20model%20design%20requires%20careful%20consideration%20of%20factors%20such%0Aas%20style%2C%20granularity%2C%20mathematical%20complexity%2C%20and%20measurability.%20Using%0Acontent%20recommender%20systems%20as%20a%20case%20study%2C%20we%20critically%20examine%20the%20nascent%0Aliterature%20of%20formal%20interaction%20models%20with%20respect%20to%20these%20use-cases%20and%0Adesign%20axes.%20More%20broadly%2C%20we%20call%20for%20the%20community%20to%20leverage%20formal%0Ainteraction%20models%20when%20designing%2C%20evaluating%2C%20or%20auditing%20any%20AI%20system%20which%0Ainteracts%20with%20users.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12366v1&entry.124074799=Read"},
{"title": "Evaluating AI for Law: Bridging the Gap with Open-Source Solutions", "author": "Rohan Bhambhoria and Samuel Dahan and Jonathan Li and Xiaodan Zhu", "abstract": "  This study evaluates the performance of general-purpose AI, like ChatGPT, in\nlegal question-answering tasks, highlighting significant risks to legal\nprofessionals and clients. It suggests leveraging foundational models enhanced\nby domain-specific knowledge to overcome these issues. The paper advocates for\ncreating open-source legal AI systems to improve accuracy, transparency, and\nnarrative diversity, addressing general AI's shortcomings in legal contexts.\n", "link": "http://arxiv.org/abs/2404.12349v1", "date": "2024-04-18", "relevancy": 1.2973, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4349}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4346}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4239}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Evaluating%20AI%20for%20Law%3A%20Bridging%20the%20Gap%20with%20Open-Source%20Solutions&body=Title%3A%20Evaluating%20AI%20for%20Law%3A%20Bridging%20the%20Gap%20with%20Open-Source%20Solutions%0AAuthor%3A%20Rohan%20Bhambhoria%20and%20Samuel%20Dahan%20and%20Jonathan%20Li%20and%20Xiaodan%20Zhu%0AAbstract%3A%20%20%20This%20study%20evaluates%20the%20performance%20of%20general-purpose%20AI%2C%20like%20ChatGPT%2C%20in%0Alegal%20question-answering%20tasks%2C%20highlighting%20significant%20risks%20to%20legal%0Aprofessionals%20and%20clients.%20It%20suggests%20leveraging%20foundational%20models%20enhanced%0Aby%20domain-specific%20knowledge%20to%20overcome%20these%20issues.%20The%20paper%20advocates%20for%0Acreating%20open-source%20legal%20AI%20systems%20to%20improve%20accuracy%2C%20transparency%2C%20and%0Anarrative%20diversity%2C%20addressing%20general%20AI%27s%20shortcomings%20in%20legal%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12349v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20AI%20for%20Law%3A%20Bridging%20the%20Gap%20with%20Open-Source%20Solutions&entry.906535625=Rohan%20Bhambhoria%20and%20Samuel%20Dahan%20and%20Jonathan%20Li%20and%20Xiaodan%20Zhu&entry.1292438233=%20%20This%20study%20evaluates%20the%20performance%20of%20general-purpose%20AI%2C%20like%20ChatGPT%2C%20in%0Alegal%20question-answering%20tasks%2C%20highlighting%20significant%20risks%20to%20legal%0Aprofessionals%20and%20clients.%20It%20suggests%20leveraging%20foundational%20models%20enhanced%0Aby%20domain-specific%20knowledge%20to%20overcome%20these%20issues.%20The%20paper%20advocates%20for%0Acreating%20open-source%20legal%20AI%20systems%20to%20improve%20accuracy%2C%20transparency%2C%20and%0Anarrative%20diversity%2C%20addressing%20general%20AI%27s%20shortcomings%20in%20legal%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12349v1&entry.124074799=Read"},
{"title": "MetaStackVis: Visually-Assisted Performance Evaluation of Metamodels", "author": "Ilya Ploshchik and Angelos Chatzimparmpas and Andreas Kerren", "abstract": "  Stacking (or stacked generalization) is an ensemble learning method with one\nmain distinctiveness from the rest: even though several base models are trained\non the original data set, their predictions are further used as input data for\none or more metamodels arranged in at least one extra layer. Composing a stack\nof models can produce high-performance outcomes, but it usually involves a\ntrial-and-error process. Therefore, our previously developed visual analytics\nsystem, StackGenVis, was mainly designed to assist users in choosing a set of\ntop-performing and diverse models by measuring their predictive performance.\nHowever, it only employs a single logistic regression metamodel. In this paper,\nwe investigate the impact of alternative metamodels on the performance of\nstacking ensembles using a novel visualization tool, called MetaStackVis. Our\ninteractive tool helps users to visually explore different singular and pairs\nof metamodels according to their predictive probabilities and multiple\nvalidation metrics, as well as their ability to predict specific problematic\ndata instances. MetaStackVis was evaluated with a usage scenario based on a\nmedical data set and via expert interviews.\n", "link": "http://arxiv.org/abs/2212.03539v3", "date": "2024-04-18", "relevancy": 1.3049, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4381}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4354}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4269}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MetaStackVis%3A%20Visually-Assisted%20Performance%20Evaluation%20of%20Metamodels&body=Title%3A%20MetaStackVis%3A%20Visually-Assisted%20Performance%20Evaluation%20of%20Metamodels%0AAuthor%3A%20Ilya%20Ploshchik%20and%20Angelos%20Chatzimparmpas%20and%20Andreas%20Kerren%0AAbstract%3A%20%20%20Stacking%20%28or%20stacked%20generalization%29%20is%20an%20ensemble%20learning%20method%20with%20one%0Amain%20distinctiveness%20from%20the%20rest%3A%20even%20though%20several%20base%20models%20are%20trained%0Aon%20the%20original%20data%20set%2C%20their%20predictions%20are%20further%20used%20as%20input%20data%20for%0Aone%20or%20more%20metamodels%20arranged%20in%20at%20least%20one%20extra%20layer.%20Composing%20a%20stack%0Aof%20models%20can%20produce%20high-performance%20outcomes%2C%20but%20it%20usually%20involves%20a%0Atrial-and-error%20process.%20Therefore%2C%20our%20previously%20developed%20visual%20analytics%0Asystem%2C%20StackGenVis%2C%20was%20mainly%20designed%20to%20assist%20users%20in%20choosing%20a%20set%20of%0Atop-performing%20and%20diverse%20models%20by%20measuring%20their%20predictive%20performance.%0AHowever%2C%20it%20only%20employs%20a%20single%20logistic%20regression%20metamodel.%20In%20this%20paper%2C%0Awe%20investigate%20the%20impact%20of%20alternative%20metamodels%20on%20the%20performance%20of%0Astacking%20ensembles%20using%20a%20novel%20visualization%20tool%2C%20called%20MetaStackVis.%20Our%0Ainteractive%20tool%20helps%20users%20to%20visually%20explore%20different%20singular%20and%20pairs%0Aof%20metamodels%20according%20to%20their%20predictive%20probabilities%20and%20multiple%0Avalidation%20metrics%2C%20as%20well%20as%20their%20ability%20to%20predict%20specific%20problematic%0Adata%20instances.%20MetaStackVis%20was%20evaluated%20with%20a%20usage%20scenario%20based%20on%20a%0Amedical%20data%20set%20and%20via%20expert%20interviews.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2212.03539v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaStackVis%3A%20Visually-Assisted%20Performance%20Evaluation%20of%20Metamodels&entry.906535625=Ilya%20Ploshchik%20and%20Angelos%20Chatzimparmpas%20and%20Andreas%20Kerren&entry.1292438233=%20%20Stacking%20%28or%20stacked%20generalization%29%20is%20an%20ensemble%20learning%20method%20with%20one%0Amain%20distinctiveness%20from%20the%20rest%3A%20even%20though%20several%20base%20models%20are%20trained%0Aon%20the%20original%20data%20set%2C%20their%20predictions%20are%20further%20used%20as%20input%20data%20for%0Aone%20or%20more%20metamodels%20arranged%20in%20at%20least%20one%20extra%20layer.%20Composing%20a%20stack%0Aof%20models%20can%20produce%20high-performance%20outcomes%2C%20but%20it%20usually%20involves%20a%0Atrial-and-error%20process.%20Therefore%2C%20our%20previously%20developed%20visual%20analytics%0Asystem%2C%20StackGenVis%2C%20was%20mainly%20designed%20to%20assist%20users%20in%20choosing%20a%20set%20of%0Atop-performing%20and%20diverse%20models%20by%20measuring%20their%20predictive%20performance.%0AHowever%2C%20it%20only%20employs%20a%20single%20logistic%20regression%20metamodel.%20In%20this%20paper%2C%0Awe%20investigate%20the%20impact%20of%20alternative%20metamodels%20on%20the%20performance%20of%0Astacking%20ensembles%20using%20a%20novel%20visualization%20tool%2C%20called%20MetaStackVis.%20Our%0Ainteractive%20tool%20helps%20users%20to%20visually%20explore%20different%20singular%20and%20pairs%0Aof%20metamodels%20according%20to%20their%20predictive%20probabilities%20and%20multiple%0Avalidation%20metrics%2C%20as%20well%20as%20their%20ability%20to%20predict%20specific%20problematic%0Adata%20instances.%20MetaStackVis%20was%20evaluated%20with%20a%20usage%20scenario%20based%20on%20a%0Amedical%20data%20set%20and%20via%20expert%20interviews.%0A&entry.1838667208=http%3A//arxiv.org/abs/2212.03539v3&entry.124074799=Read"},
{"title": "Adjoint Sensitivities of Chaotic Flows without Adjoint Solvers: A\n  Data-Driven Approach", "author": "Defne E. Ozan and Luca Magri", "abstract": "  In one calculation, adjoint sensitivity analysis provides the gradient of a\nquantity of interest with respect to all system's parameters. Conventionally,\nadjoint solvers need to be implemented by differentiating computational models,\nwhich can be a cumbersome task and is code-specific. To propose an adjoint\nsolver that is not code-specific, we develop a data-driven strategy. We\ndemonstrate its application on the computation of gradients of long-time\naverages of chaotic flows. First, we deploy a parameter-aware echo state\nnetwork (ESN) to accurately forecast and simulate the dynamics of a dynamical\nsystem for a range of system's parameters. Second, we derive the adjoint of the\nparameter-aware ESN. Finally, we combine the parameter-aware ESN with its\nadjoint version to compute the sensitivities to the system parameters. We\nshowcase the method on a prototypical chaotic system. Because adjoint\nsensitivities in chaotic regimes diverge for long integration times, we analyse\nthe application of ensemble adjoint method to the ESN. We find that the adjoint\nsensitivities obtained from the ESN match closely with the original system.\nThis work opens possibilities for sensitivity analysis without code-specific\nadjoint solvers.\n", "link": "http://arxiv.org/abs/2404.12315v1", "date": "2024-04-18", "relevancy": 1.3109, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4473}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4368}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4329}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Adjoint%20Sensitivities%20of%20Chaotic%20Flows%20without%20Adjoint%20Solvers%3A%20A%0A%20%20Data-Driven%20Approach&body=Title%3A%20Adjoint%20Sensitivities%20of%20Chaotic%20Flows%20without%20Adjoint%20Solvers%3A%20A%0A%20%20Data-Driven%20Approach%0AAuthor%3A%20Defne%20E.%20Ozan%20and%20Luca%20Magri%0AAbstract%3A%20%20%20In%20one%20calculation%2C%20adjoint%20sensitivity%20analysis%20provides%20the%20gradient%20of%20a%0Aquantity%20of%20interest%20with%20respect%20to%20all%20system%27s%20parameters.%20Conventionally%2C%0Aadjoint%20solvers%20need%20to%20be%20implemented%20by%20differentiating%20computational%20models%2C%0Awhich%20can%20be%20a%20cumbersome%20task%20and%20is%20code-specific.%20To%20propose%20an%20adjoint%0Asolver%20that%20is%20not%20code-specific%2C%20we%20develop%20a%20data-driven%20strategy.%20We%0Ademonstrate%20its%20application%20on%20the%20computation%20of%20gradients%20of%20long-time%0Aaverages%20of%20chaotic%20flows.%20First%2C%20we%20deploy%20a%20parameter-aware%20echo%20state%0Anetwork%20%28ESN%29%20to%20accurately%20forecast%20and%20simulate%20the%20dynamics%20of%20a%20dynamical%0Asystem%20for%20a%20range%20of%20system%27s%20parameters.%20Second%2C%20we%20derive%20the%20adjoint%20of%20the%0Aparameter-aware%20ESN.%20Finally%2C%20we%20combine%20the%20parameter-aware%20ESN%20with%20its%0Aadjoint%20version%20to%20compute%20the%20sensitivities%20to%20the%20system%20parameters.%20We%0Ashowcase%20the%20method%20on%20a%20prototypical%20chaotic%20system.%20Because%20adjoint%0Asensitivities%20in%20chaotic%20regimes%20diverge%20for%20long%20integration%20times%2C%20we%20analyse%0Athe%20application%20of%20ensemble%20adjoint%20method%20to%20the%20ESN.%20We%20find%20that%20the%20adjoint%0Asensitivities%20obtained%20from%20the%20ESN%20match%20closely%20with%20the%20original%20system.%0AThis%20work%20opens%20possibilities%20for%20sensitivity%20analysis%20without%20code-specific%0Aadjoint%20solvers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12315v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adjoint%20Sensitivities%20of%20Chaotic%20Flows%20without%20Adjoint%20Solvers%3A%20A%0A%20%20Data-Driven%20Approach&entry.906535625=Defne%20E.%20Ozan%20and%20Luca%20Magri&entry.1292438233=%20%20In%20one%20calculation%2C%20adjoint%20sensitivity%20analysis%20provides%20the%20gradient%20of%20a%0Aquantity%20of%20interest%20with%20respect%20to%20all%20system%27s%20parameters.%20Conventionally%2C%0Aadjoint%20solvers%20need%20to%20be%20implemented%20by%20differentiating%20computational%20models%2C%0Awhich%20can%20be%20a%20cumbersome%20task%20and%20is%20code-specific.%20To%20propose%20an%20adjoint%0Asolver%20that%20is%20not%20code-specific%2C%20we%20develop%20a%20data-driven%20strategy.%20We%0Ademonstrate%20its%20application%20on%20the%20computation%20of%20gradients%20of%20long-time%0Aaverages%20of%20chaotic%20flows.%20First%2C%20we%20deploy%20a%20parameter-aware%20echo%20state%0Anetwork%20%28ESN%29%20to%20accurately%20forecast%20and%20simulate%20the%20dynamics%20of%20a%20dynamical%0Asystem%20for%20a%20range%20of%20system%27s%20parameters.%20Second%2C%20we%20derive%20the%20adjoint%20of%20the%0Aparameter-aware%20ESN.%20Finally%2C%20we%20combine%20the%20parameter-aware%20ESN%20with%20its%0Aadjoint%20version%20to%20compute%20the%20sensitivities%20to%20the%20system%20parameters.%20We%0Ashowcase%20the%20method%20on%20a%20prototypical%20chaotic%20system.%20Because%20adjoint%0Asensitivities%20in%20chaotic%20regimes%20diverge%20for%20long%20integration%20times%2C%20we%20analyse%0Athe%20application%20of%20ensemble%20adjoint%20method%20to%20the%20ESN.%20We%20find%20that%20the%20adjoint%0Asensitivities%20obtained%20from%20the%20ESN%20match%20closely%20with%20the%20original%20system.%0AThis%20work%20opens%20possibilities%20for%20sensitivity%20analysis%20without%20code-specific%0Aadjoint%20solvers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12315v1&entry.124074799=Read"},
{"title": "Augmenting emotion features in irony detection with Large language\n  modeling", "author": "Yucheng Lin and Yuhan Xia and Yunfei Long", "abstract": "  This study introduces a novel method for irony detection, applying Large\nLanguage Models (LLMs) with prompt-based learning to facilitate emotion-centric\ntext augmentation. Traditional irony detection techniques typically fall short\ndue to their reliance on static linguistic features and predefined knowledge\nbases, often overlooking the nuanced emotional dimensions integral to irony. In\ncontrast, our methodology augments the detection process by integrating subtle\nemotional cues, augmented through LLMs, into three benchmark pre-trained NLP\nmodels - BERT, T5, and GPT-2 - which are widely recognized as foundational in\nirony detection. We assessed our method using the SemEval-2018 Task 3 dataset\nand observed substantial enhancements in irony detection capabilities.\n", "link": "http://arxiv.org/abs/2404.12291v1", "date": "2024-04-18", "relevancy": 1.2901, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4573}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4302}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.419}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Augmenting%20emotion%20features%20in%20irony%20detection%20with%20Large%20language%0A%20%20modeling&body=Title%3A%20Augmenting%20emotion%20features%20in%20irony%20detection%20with%20Large%20language%0A%20%20modeling%0AAuthor%3A%20Yucheng%20Lin%20and%20Yuhan%20Xia%20and%20Yunfei%20Long%0AAbstract%3A%20%20%20This%20study%20introduces%20a%20novel%20method%20for%20irony%20detection%2C%20applying%20Large%0ALanguage%20Models%20%28LLMs%29%20with%20prompt-based%20learning%20to%20facilitate%20emotion-centric%0Atext%20augmentation.%20Traditional%20irony%20detection%20techniques%20typically%20fall%20short%0Adue%20to%20their%20reliance%20on%20static%20linguistic%20features%20and%20predefined%20knowledge%0Abases%2C%20often%20overlooking%20the%20nuanced%20emotional%20dimensions%20integral%20to%20irony.%20In%0Acontrast%2C%20our%20methodology%20augments%20the%20detection%20process%20by%20integrating%20subtle%0Aemotional%20cues%2C%20augmented%20through%20LLMs%2C%20into%20three%20benchmark%20pre-trained%20NLP%0Amodels%20-%20BERT%2C%20T5%2C%20and%20GPT-2%20-%20which%20are%20widely%20recognized%20as%20foundational%20in%0Airony%20detection.%20We%20assessed%20our%20method%20using%20the%20SemEval-2018%20Task%203%20dataset%0Aand%20observed%20substantial%20enhancements%20in%20irony%20detection%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12291v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Augmenting%20emotion%20features%20in%20irony%20detection%20with%20Large%20language%0A%20%20modeling&entry.906535625=Yucheng%20Lin%20and%20Yuhan%20Xia%20and%20Yunfei%20Long&entry.1292438233=%20%20This%20study%20introduces%20a%20novel%20method%20for%20irony%20detection%2C%20applying%20Large%0ALanguage%20Models%20%28LLMs%29%20with%20prompt-based%20learning%20to%20facilitate%20emotion-centric%0Atext%20augmentation.%20Traditional%20irony%20detection%20techniques%20typically%20fall%20short%0Adue%20to%20their%20reliance%20on%20static%20linguistic%20features%20and%20predefined%20knowledge%0Abases%2C%20often%20overlooking%20the%20nuanced%20emotional%20dimensions%20integral%20to%20irony.%20In%0Acontrast%2C%20our%20methodology%20augments%20the%20detection%20process%20by%20integrating%20subtle%0Aemotional%20cues%2C%20augmented%20through%20LLMs%2C%20into%20three%20benchmark%20pre-trained%20NLP%0Amodels%20-%20BERT%2C%20T5%2C%20and%20GPT-2%20-%20which%20are%20widely%20recognized%20as%20foundational%20in%0Airony%20detection.%20We%20assessed%20our%20method%20using%20the%20SemEval-2018%20Task%203%20dataset%0Aand%20observed%20substantial%20enhancements%20in%20irony%20detection%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12291v1&entry.124074799=Read"},
{"title": "Private graphon estimation via sum-of-squares", "author": "Hongjie Chen and Jingqiu Ding and Tommaso d'Orsi and Yiding Hua and Chih-Hung Liu and David Steurer", "abstract": "  We develop the first pure node-differentially-private algorithms for learning\nstochastic block models and for graphon estimation with polynomial running time\nfor any constant number of blocks. The statistical utility guarantees match\nthose of the previous best information-theoretic (exponential-time)\nnode-private mechanisms for these problems. The algorithm is based on an\nexponential mechanism for a score function defined in terms of a sum-of-squares\nrelaxation whose level depends on the number of blocks. The key ingredients of\nour results are (1) a characterization of the distance between the block\ngraphons in terms of a quadratic optimization over the polytope of doubly\nstochastic matrices, (2) a general sum-of-squares convergence result for\npolynomial optimization over arbitrary polytopes, and (3) a general approach to\nperform Lipschitz extensions of score functions as part of the sum-of-squares\nalgorithmic paradigm.\n", "link": "http://arxiv.org/abs/2403.12213v2", "date": "2024-04-18", "relevancy": 1.2674, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4276}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4217}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4105}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Private%20graphon%20estimation%20via%20sum-of-squares&body=Title%3A%20Private%20graphon%20estimation%20via%20sum-of-squares%0AAuthor%3A%20Hongjie%20Chen%20and%20Jingqiu%20Ding%20and%20Tommaso%20d%27Orsi%20and%20Yiding%20Hua%20and%20Chih-Hung%20Liu%20and%20David%20Steurer%0AAbstract%3A%20%20%20We%20develop%20the%20first%20pure%20node-differentially-private%20algorithms%20for%20learning%0Astochastic%20block%20models%20and%20for%20graphon%20estimation%20with%20polynomial%20running%20time%0Afor%20any%20constant%20number%20of%20blocks.%20The%20statistical%20utility%20guarantees%20match%0Athose%20of%20the%20previous%20best%20information-theoretic%20%28exponential-time%29%0Anode-private%20mechanisms%20for%20these%20problems.%20The%20algorithm%20is%20based%20on%20an%0Aexponential%20mechanism%20for%20a%20score%20function%20defined%20in%20terms%20of%20a%20sum-of-squares%0Arelaxation%20whose%20level%20depends%20on%20the%20number%20of%20blocks.%20The%20key%20ingredients%20of%0Aour%20results%20are%20%281%29%20a%20characterization%20of%20the%20distance%20between%20the%20block%0Agraphons%20in%20terms%20of%20a%20quadratic%20optimization%20over%20the%20polytope%20of%20doubly%0Astochastic%20matrices%2C%20%282%29%20a%20general%20sum-of-squares%20convergence%20result%20for%0Apolynomial%20optimization%20over%20arbitrary%20polytopes%2C%20and%20%283%29%20a%20general%20approach%20to%0Aperform%20Lipschitz%20extensions%20of%20score%20functions%20as%20part%20of%20the%20sum-of-squares%0Aalgorithmic%20paradigm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12213v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Private%20graphon%20estimation%20via%20sum-of-squares&entry.906535625=Hongjie%20Chen%20and%20Jingqiu%20Ding%20and%20Tommaso%20d%27Orsi%20and%20Yiding%20Hua%20and%20Chih-Hung%20Liu%20and%20David%20Steurer&entry.1292438233=%20%20We%20develop%20the%20first%20pure%20node-differentially-private%20algorithms%20for%20learning%0Astochastic%20block%20models%20and%20for%20graphon%20estimation%20with%20polynomial%20running%20time%0Afor%20any%20constant%20number%20of%20blocks.%20The%20statistical%20utility%20guarantees%20match%0Athose%20of%20the%20previous%20best%20information-theoretic%20%28exponential-time%29%0Anode-private%20mechanisms%20for%20these%20problems.%20The%20algorithm%20is%20based%20on%20an%0Aexponential%20mechanism%20for%20a%20score%20function%20defined%20in%20terms%20of%20a%20sum-of-squares%0Arelaxation%20whose%20level%20depends%20on%20the%20number%20of%20blocks.%20The%20key%20ingredients%20of%0Aour%20results%20are%20%281%29%20a%20characterization%20of%20the%20distance%20between%20the%20block%0Agraphons%20in%20terms%20of%20a%20quadratic%20optimization%20over%20the%20polytope%20of%20doubly%0Astochastic%20matrices%2C%20%282%29%20a%20general%20sum-of-squares%20convergence%20result%20for%0Apolynomial%20optimization%20over%20arbitrary%20polytopes%2C%20and%20%283%29%20a%20general%20approach%20to%0Aperform%20Lipschitz%20extensions%20of%20score%20functions%20as%20part%20of%20the%20sum-of-squares%0Aalgorithmic%20paradigm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12213v2&entry.124074799=Read"},
{"title": "Food Portion Estimation via 3D Object Scaling", "author": "Gautham Vinod and Jiangpeng He and Zeman Shao and Fengqing Zhu", "abstract": "  Image-based methods to analyze food images have alleviated the user burden\nand biases associated with traditional methods. However, accurate portion\nestimation remains a major challenge due to the loss of 3D information in the\n2D representation of foods captured by smartphone cameras or wearable devices.\nIn this paper, we propose a new framework to estimate both food volume and\nenergy from 2D images by leveraging the power of 3D food models and physical\nreference in the eating scene. Our method estimates the pose of the camera and\nthe food object in the input image and recreates the eating occasion by\nrendering an image of a 3D model of the food with the estimated poses. We also\nintroduce a new dataset, SimpleFood45, which contains 2D images of 45 food\nitems and associated annotations including food volume, weight, and energy. Our\nmethod achieves an average error of 31.10 kCal (17.67%) on this dataset,\noutperforming existing portion estimation methods.\n", "link": "http://arxiv.org/abs/2404.12257v1", "date": "2024-04-18", "relevancy": 1.0011, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.519}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5001}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4826}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Food%20Portion%20Estimation%20via%203D%20Object%20Scaling&body=Title%3A%20Food%20Portion%20Estimation%20via%203D%20Object%20Scaling%0AAuthor%3A%20Gautham%20Vinod%20and%20Jiangpeng%20He%20and%20Zeman%20Shao%20and%20Fengqing%20Zhu%0AAbstract%3A%20%20%20Image-based%20methods%20to%20analyze%20food%20images%20have%20alleviated%20the%20user%20burden%0Aand%20biases%20associated%20with%20traditional%20methods.%20However%2C%20accurate%20portion%0Aestimation%20remains%20a%20major%20challenge%20due%20to%20the%20loss%20of%203D%20information%20in%20the%0A2D%20representation%20of%20foods%20captured%20by%20smartphone%20cameras%20or%20wearable%20devices.%0AIn%20this%20paper%2C%20we%20propose%20a%20new%20framework%20to%20estimate%20both%20food%20volume%20and%0Aenergy%20from%202D%20images%20by%20leveraging%20the%20power%20of%203D%20food%20models%20and%20physical%0Areference%20in%20the%20eating%20scene.%20Our%20method%20estimates%20the%20pose%20of%20the%20camera%20and%0Athe%20food%20object%20in%20the%20input%20image%20and%20recreates%20the%20eating%20occasion%20by%0Arendering%20an%20image%20of%20a%203D%20model%20of%20the%20food%20with%20the%20estimated%20poses.%20We%20also%0Aintroduce%20a%20new%20dataset%2C%20SimpleFood45%2C%20which%20contains%202D%20images%20of%2045%20food%0Aitems%20and%20associated%20annotations%20including%20food%20volume%2C%20weight%2C%20and%20energy.%20Our%0Amethod%20achieves%20an%20average%20error%20of%2031.10%20kCal%20%2817.67%25%29%20on%20this%20dataset%2C%0Aoutperforming%20existing%20portion%20estimation%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12257v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Food%20Portion%20Estimation%20via%203D%20Object%20Scaling&entry.906535625=Gautham%20Vinod%20and%20Jiangpeng%20He%20and%20Zeman%20Shao%20and%20Fengqing%20Zhu&entry.1292438233=%20%20Image-based%20methods%20to%20analyze%20food%20images%20have%20alleviated%20the%20user%20burden%0Aand%20biases%20associated%20with%20traditional%20methods.%20However%2C%20accurate%20portion%0Aestimation%20remains%20a%20major%20challenge%20due%20to%20the%20loss%20of%203D%20information%20in%20the%0A2D%20representation%20of%20foods%20captured%20by%20smartphone%20cameras%20or%20wearable%20devices.%0AIn%20this%20paper%2C%20we%20propose%20a%20new%20framework%20to%20estimate%20both%20food%20volume%20and%0Aenergy%20from%202D%20images%20by%20leveraging%20the%20power%20of%203D%20food%20models%20and%20physical%0Areference%20in%20the%20eating%20scene.%20Our%20method%20estimates%20the%20pose%20of%20the%20camera%20and%0Athe%20food%20object%20in%20the%20input%20image%20and%20recreates%20the%20eating%20occasion%20by%0Arendering%20an%20image%20of%20a%203D%20model%20of%20the%20food%20with%20the%20estimated%20poses.%20We%20also%0Aintroduce%20a%20new%20dataset%2C%20SimpleFood45%2C%20which%20contains%202D%20images%20of%2045%20food%0Aitems%20and%20associated%20annotations%20including%20food%20volume%2C%20weight%2C%20and%20energy.%20Our%0Amethod%20achieves%20an%20average%20error%20of%2031.10%20kCal%20%2817.67%25%29%20on%20this%20dataset%2C%0Aoutperforming%20existing%20portion%20estimation%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12257v1&entry.124074799=Read"},
{"title": "State Space Models for Event Cameras", "author": "Nikola Zubi\u0107 and Mathias Gehrig and Davide Scaramuzza", "abstract": "  Today, state-of-the-art deep neural networks that process event-camera data\nfirst convert a temporal window of events into dense, grid-like input\nrepresentations. As such, they exhibit poor generalizability when deployed at\nhigher inference frequencies (i.e., smaller temporal windows) than the ones\nthey were trained on. We address this challenge by introducing state-space\nmodels (SSMs) with learnable timescale parameters to event-based vision. This\ndesign adapts to varying frequencies without the need to retrain the network at\ndifferent frequencies. Additionally, we investigate two strategies to\ncounteract aliasing effects when deploying the model at higher frequencies. We\ncomprehensively evaluate our approach against existing methods based on RNN and\nTransformer architectures across various benchmarks, including Gen1 and 1 Mpx\nevent camera datasets. Our results demonstrate that SSM-based models train 33%\nfaster and also exhibit minimal performance degradation when tested at higher\nfrequencies than the training input. Traditional RNN and Transformer models\nexhibit performance drops of more than 20 mAP, with SSMs having a drop of 3.76\nmAP, highlighting the effectiveness of SSMs in event-based vision tasks.\n", "link": "http://arxiv.org/abs/2402.15584v3", "date": "2024-04-18", "relevancy": 1.0776, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5916}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5162}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5086}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20State%20Space%20Models%20for%20Event%20Cameras&body=Title%3A%20State%20Space%20Models%20for%20Event%20Cameras%0AAuthor%3A%20Nikola%20Zubi%C4%87%20and%20Mathias%20Gehrig%20and%20Davide%20Scaramuzza%0AAbstract%3A%20%20%20Today%2C%20state-of-the-art%20deep%20neural%20networks%20that%20process%20event-camera%20data%0Afirst%20convert%20a%20temporal%20window%20of%20events%20into%20dense%2C%20grid-like%20input%0Arepresentations.%20As%20such%2C%20they%20exhibit%20poor%20generalizability%20when%20deployed%20at%0Ahigher%20inference%20frequencies%20%28i.e.%2C%20smaller%20temporal%20windows%29%20than%20the%20ones%0Athey%20were%20trained%20on.%20We%20address%20this%20challenge%20by%20introducing%20state-space%0Amodels%20%28SSMs%29%20with%20learnable%20timescale%20parameters%20to%20event-based%20vision.%20This%0Adesign%20adapts%20to%20varying%20frequencies%20without%20the%20need%20to%20retrain%20the%20network%20at%0Adifferent%20frequencies.%20Additionally%2C%20we%20investigate%20two%20strategies%20to%0Acounteract%20aliasing%20effects%20when%20deploying%20the%20model%20at%20higher%20frequencies.%20We%0Acomprehensively%20evaluate%20our%20approach%20against%20existing%20methods%20based%20on%20RNN%20and%0ATransformer%20architectures%20across%20various%20benchmarks%2C%20including%20Gen1%20and%201%20Mpx%0Aevent%20camera%20datasets.%20Our%20results%20demonstrate%20that%20SSM-based%20models%20train%2033%25%0Afaster%20and%20also%20exhibit%20minimal%20performance%20degradation%20when%20tested%20at%20higher%0Afrequencies%20than%20the%20training%20input.%20Traditional%20RNN%20and%20Transformer%20models%0Aexhibit%20performance%20drops%20of%20more%20than%2020%20mAP%2C%20with%20SSMs%20having%20a%20drop%20of%203.76%0AmAP%2C%20highlighting%20the%20effectiveness%20of%20SSMs%20in%20event-based%20vision%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15584v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=State%20Space%20Models%20for%20Event%20Cameras&entry.906535625=Nikola%20Zubi%C4%87%20and%20Mathias%20Gehrig%20and%20Davide%20Scaramuzza&entry.1292438233=%20%20Today%2C%20state-of-the-art%20deep%20neural%20networks%20that%20process%20event-camera%20data%0Afirst%20convert%20a%20temporal%20window%20of%20events%20into%20dense%2C%20grid-like%20input%0Arepresentations.%20As%20such%2C%20they%20exhibit%20poor%20generalizability%20when%20deployed%20at%0Ahigher%20inference%20frequencies%20%28i.e.%2C%20smaller%20temporal%20windows%29%20than%20the%20ones%0Athey%20were%20trained%20on.%20We%20address%20this%20challenge%20by%20introducing%20state-space%0Amodels%20%28SSMs%29%20with%20learnable%20timescale%20parameters%20to%20event-based%20vision.%20This%0Adesign%20adapts%20to%20varying%20frequencies%20without%20the%20need%20to%20retrain%20the%20network%20at%0Adifferent%20frequencies.%20Additionally%2C%20we%20investigate%20two%20strategies%20to%0Acounteract%20aliasing%20effects%20when%20deploying%20the%20model%20at%20higher%20frequencies.%20We%0Acomprehensively%20evaluate%20our%20approach%20against%20existing%20methods%20based%20on%20RNN%20and%0ATransformer%20architectures%20across%20various%20benchmarks%2C%20including%20Gen1%20and%201%20Mpx%0Aevent%20camera%20datasets.%20Our%20results%20demonstrate%20that%20SSM-based%20models%20train%2033%25%0Afaster%20and%20also%20exhibit%20minimal%20performance%20degradation%20when%20tested%20at%20higher%0Afrequencies%20than%20the%20training%20input.%20Traditional%20RNN%20and%20Transformer%20models%0Aexhibit%20performance%20drops%20of%20more%20than%2020%20mAP%2C%20with%20SSMs%20having%20a%20drop%20of%203.76%0AmAP%2C%20highlighting%20the%20effectiveness%20of%20SSMs%20in%20event-based%20vision%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15584v3&entry.124074799=Read"},
{"title": "Introducing v0.5 of the AI Safety Benchmark from MLCommons", "author": "Bertie Vidgen and Adarsh Agrawal and Ahmed M. Ahmed and Victor Akinwande and Namir Al-Nuaimi and Najla Alfaraj and Elie Alhajjar and Lora Aroyo and Trupti Bavalatti and Borhane Blili-Hamelin and Kurt Bollacker and Rishi Bomassani and Marisa Ferrara Boston and Sim\u00e9on Campos and Kal Chakra and Canyu Chen and Cody Coleman and Zacharie Delpierre Coudert and Leon Derczynski and Debojyoti Dutta and Ian Eisenberg and James Ezick and Heather Frase and Brian Fuller and Ram Gandikota and Agasthya Gangavarapu and Ananya Gangavarapu and James Gealy and Rajat Ghosh and James Goel and Usman Gohar and Sujata Goswami and Scott A. Hale and Wiebke Hutiri and Joseph Marvin Imperial and Surgan Jandial and Nick Judd and Felix Juefei-Xu and Foutse Khomh and Bhavya Kailkhura and Hannah Rose Kirk and Kevin Klyman and Chris Knotz and Michael Kuchnik and Shachi H. Kumar and Chris Lengerich and Bo Li and Zeyi Liao and Eileen Peters Long and Victor Lu and Yifan Mai and Priyanka Mary Mammen and Kelvin Manyeki and Sean McGregor and Virendra Mehta and Shafee Mohammed and Emanuel Moss and Lama Nachman and Dinesh Jinenhally Naganna and Amin Nikanjam and Besmira Nushi and Luis Oala and Iftach Orr and Alicia Parrish and Cigdem Patlak and William Pietri and Forough Poursabzi-Sangdeh and Eleonora Presani and Fabrizio Puletti and Paul R\u00f6ttger and Saurav Sahay and Tim Santos and Nino Scherrer and Alice Schoenauer Sebag and Patrick Schramowski and Abolfazl Shahbazi and Vin Sharma and Xudong Shen and Vamsi Sistla and Leonard Tang and Davide Testuggine and Vithursan Thangarasa and Elizabeth Anne Watkins and Rebecca Weiss and Chris Welty and Tyler Wilbers and Adina Williams and Carole-Jean Wu and Poonam Yadav and Xianjun Yang and Yi Zeng and Wenhui Zhang and Fedor Zhdanov and Jiacheng Zhu and Percy Liang and Peter Mattson and Joaquin Vanschoren", "abstract": "  This paper introduces v0.5 of the AI Safety Benchmark, which has been created\nby the MLCommons AI Safety Working Group. The AI Safety Benchmark has been\ndesigned to assess the safety risks of AI systems that use chat-tuned language\nmodels. We introduce a principled approach to specifying and constructing the\nbenchmark, which for v0.5 covers only a single use case (an adult chatting to a\ngeneral-purpose assistant in English), and a limited set of personas (i.e.,\ntypical users, malicious users, and vulnerable users). We created a new\ntaxonomy of 13 hazard categories, of which 7 have tests in the v0.5 benchmark.\nWe plan to release version 1.0 of the AI Safety Benchmark by the end of 2024.\nThe v1.0 benchmark will provide meaningful insights into the safety of AI\nsystems. However, the v0.5 benchmark should not be used to assess the safety of\nAI systems. We have sought to fully document the limitations, flaws, and\nchallenges of v0.5. This release of v0.5 of the AI Safety Benchmark includes\n(1) a principled approach to specifying and constructing the benchmark, which\ncomprises use cases, types of systems under test (SUTs), language and context,\npersonas, tests, and test items; (2) a taxonomy of 13 hazard categories with\ndefinitions and subcategories; (3) tests for seven of the hazard categories,\neach comprising a unique set of test items, i.e., prompts. There are 43,090\ntest items in total, which we created with templates; (4) a grading system for\nAI systems against the benchmark; (5) an openly available platform, and\ndownloadable tool, called ModelBench that can be used to evaluate the safety of\nAI systems on the benchmark; (6) an example evaluation report which benchmarks\nthe performance of over a dozen openly available chat-tuned language models;\n(7) a test specification for the benchmark.\n", "link": "http://arxiv.org/abs/2404.12241v1", "date": "2024-04-18", "relevancy": 0.856, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4585}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4152}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4103}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Introducing%20v0.5%20of%20the%20AI%20Safety%20Benchmark%20from%20MLCommons&body=Title%3A%20Introducing%20v0.5%20of%20the%20AI%20Safety%20Benchmark%20from%20MLCommons%0AAuthor%3A%20Bertie%20Vidgen%20and%20Adarsh%20Agrawal%20and%20Ahmed%20M.%20Ahmed%20and%20Victor%20Akinwande%20and%20Namir%20Al-Nuaimi%20and%20Najla%20Alfaraj%20and%20Elie%20Alhajjar%20and%20Lora%20Aroyo%20and%20Trupti%20Bavalatti%20and%20Borhane%20Blili-Hamelin%20and%20Kurt%20Bollacker%20and%20Rishi%20Bomassani%20and%20Marisa%20Ferrara%20Boston%20and%20Sim%C3%A9on%20Campos%20and%20Kal%20Chakra%20and%20Canyu%20Chen%20and%20Cody%20Coleman%20and%20Zacharie%20Delpierre%20Coudert%20and%20Leon%20Derczynski%20and%20Debojyoti%20Dutta%20and%20Ian%20Eisenberg%20and%20James%20Ezick%20and%20Heather%20Frase%20and%20Brian%20Fuller%20and%20Ram%20Gandikota%20and%20Agasthya%20Gangavarapu%20and%20Ananya%20Gangavarapu%20and%20James%20Gealy%20and%20Rajat%20Ghosh%20and%20James%20Goel%20and%20Usman%20Gohar%20and%20Sujata%20Goswami%20and%20Scott%20A.%20Hale%20and%20Wiebke%20Hutiri%20and%20Joseph%20Marvin%20Imperial%20and%20Surgan%20Jandial%20and%20Nick%20Judd%20and%20Felix%20Juefei-Xu%20and%20Foutse%20Khomh%20and%20Bhavya%20Kailkhura%20and%20Hannah%20Rose%20Kirk%20and%20Kevin%20Klyman%20and%20Chris%20Knotz%20and%20Michael%20Kuchnik%20and%20Shachi%20H.%20Kumar%20and%20Chris%20Lengerich%20and%20Bo%20Li%20and%20Zeyi%20Liao%20and%20Eileen%20Peters%20Long%20and%20Victor%20Lu%20and%20Yifan%20Mai%20and%20Priyanka%20Mary%20Mammen%20and%20Kelvin%20Manyeki%20and%20Sean%20McGregor%20and%20Virendra%20Mehta%20and%20Shafee%20Mohammed%20and%20Emanuel%20Moss%20and%20Lama%20Nachman%20and%20Dinesh%20Jinenhally%20Naganna%20and%20Amin%20Nikanjam%20and%20Besmira%20Nushi%20and%20Luis%20Oala%20and%20Iftach%20Orr%20and%20Alicia%20Parrish%20and%20Cigdem%20Patlak%20and%20William%20Pietri%20and%20Forough%20Poursabzi-Sangdeh%20and%20Eleonora%20Presani%20and%20Fabrizio%20Puletti%20and%20Paul%20R%C3%B6ttger%20and%20Saurav%20Sahay%20and%20Tim%20Santos%20and%20Nino%20Scherrer%20and%20Alice%20Schoenauer%20Sebag%20and%20Patrick%20Schramowski%20and%20Abolfazl%20Shahbazi%20and%20Vin%20Sharma%20and%20Xudong%20Shen%20and%20Vamsi%20Sistla%20and%20Leonard%20Tang%20and%20Davide%20Testuggine%20and%20Vithursan%20Thangarasa%20and%20Elizabeth%20Anne%20Watkins%20and%20Rebecca%20Weiss%20and%20Chris%20Welty%20and%20Tyler%20Wilbers%20and%20Adina%20Williams%20and%20Carole-Jean%20Wu%20and%20Poonam%20Yadav%20and%20Xianjun%20Yang%20and%20Yi%20Zeng%20and%20Wenhui%20Zhang%20and%20Fedor%20Zhdanov%20and%20Jiacheng%20Zhu%20and%20Percy%20Liang%20and%20Peter%20Mattson%20and%20Joaquin%20Vanschoren%0AAbstract%3A%20%20%20This%20paper%20introduces%20v0.5%20of%20the%20AI%20Safety%20Benchmark%2C%20which%20has%20been%20created%0Aby%20the%20MLCommons%20AI%20Safety%20Working%20Group.%20The%20AI%20Safety%20Benchmark%20has%20been%0Adesigned%20to%20assess%20the%20safety%20risks%20of%20AI%20systems%20that%20use%20chat-tuned%20language%0Amodels.%20We%20introduce%20a%20principled%20approach%20to%20specifying%20and%20constructing%20the%0Abenchmark%2C%20which%20for%20v0.5%20covers%20only%20a%20single%20use%20case%20%28an%20adult%20chatting%20to%20a%0Ageneral-purpose%20assistant%20in%20English%29%2C%20and%20a%20limited%20set%20of%20personas%20%28i.e.%2C%0Atypical%20users%2C%20malicious%20users%2C%20and%20vulnerable%20users%29.%20We%20created%20a%20new%0Ataxonomy%20of%2013%20hazard%20categories%2C%20of%20which%207%20have%20tests%20in%20the%20v0.5%20benchmark.%0AWe%20plan%20to%20release%20version%201.0%20of%20the%20AI%20Safety%20Benchmark%20by%20the%20end%20of%202024.%0AThe%20v1.0%20benchmark%20will%20provide%20meaningful%20insights%20into%20the%20safety%20of%20AI%0Asystems.%20However%2C%20the%20v0.5%20benchmark%20should%20not%20be%20used%20to%20assess%20the%20safety%20of%0AAI%20systems.%20We%20have%20sought%20to%20fully%20document%20the%20limitations%2C%20flaws%2C%20and%0Achallenges%20of%20v0.5.%20This%20release%20of%20v0.5%20of%20the%20AI%20Safety%20Benchmark%20includes%0A%281%29%20a%20principled%20approach%20to%20specifying%20and%20constructing%20the%20benchmark%2C%20which%0Acomprises%20use%20cases%2C%20types%20of%20systems%20under%20test%20%28SUTs%29%2C%20language%20and%20context%2C%0Apersonas%2C%20tests%2C%20and%20test%20items%3B%20%282%29%20a%20taxonomy%20of%2013%20hazard%20categories%20with%0Adefinitions%20and%20subcategories%3B%20%283%29%20tests%20for%20seven%20of%20the%20hazard%20categories%2C%0Aeach%20comprising%20a%20unique%20set%20of%20test%20items%2C%20i.e.%2C%20prompts.%20There%20are%2043%2C090%0Atest%20items%20in%20total%2C%20which%20we%20created%20with%20templates%3B%20%284%29%20a%20grading%20system%20for%0AAI%20systems%20against%20the%20benchmark%3B%20%285%29%20an%20openly%20available%20platform%2C%20and%0Adownloadable%20tool%2C%20called%20ModelBench%20that%20can%20be%20used%20to%20evaluate%20the%20safety%20of%0AAI%20systems%20on%20the%20benchmark%3B%20%286%29%20an%20example%20evaluation%20report%20which%20benchmarks%0Athe%20performance%20of%20over%20a%20dozen%20openly%20available%20chat-tuned%20language%20models%3B%0A%287%29%20a%20test%20specification%20for%20the%20benchmark.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12241v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Introducing%20v0.5%20of%20the%20AI%20Safety%20Benchmark%20from%20MLCommons&entry.906535625=Bertie%20Vidgen%20and%20Adarsh%20Agrawal%20and%20Ahmed%20M.%20Ahmed%20and%20Victor%20Akinwande%20and%20Namir%20Al-Nuaimi%20and%20Najla%20Alfaraj%20and%20Elie%20Alhajjar%20and%20Lora%20Aroyo%20and%20Trupti%20Bavalatti%20and%20Borhane%20Blili-Hamelin%20and%20Kurt%20Bollacker%20and%20Rishi%20Bomassani%20and%20Marisa%20Ferrara%20Boston%20and%20Sim%C3%A9on%20Campos%20and%20Kal%20Chakra%20and%20Canyu%20Chen%20and%20Cody%20Coleman%20and%20Zacharie%20Delpierre%20Coudert%20and%20Leon%20Derczynski%20and%20Debojyoti%20Dutta%20and%20Ian%20Eisenberg%20and%20James%20Ezick%20and%20Heather%20Frase%20and%20Brian%20Fuller%20and%20Ram%20Gandikota%20and%20Agasthya%20Gangavarapu%20and%20Ananya%20Gangavarapu%20and%20James%20Gealy%20and%20Rajat%20Ghosh%20and%20James%20Goel%20and%20Usman%20Gohar%20and%20Sujata%20Goswami%20and%20Scott%20A.%20Hale%20and%20Wiebke%20Hutiri%20and%20Joseph%20Marvin%20Imperial%20and%20Surgan%20Jandial%20and%20Nick%20Judd%20and%20Felix%20Juefei-Xu%20and%20Foutse%20Khomh%20and%20Bhavya%20Kailkhura%20and%20Hannah%20Rose%20Kirk%20and%20Kevin%20Klyman%20and%20Chris%20Knotz%20and%20Michael%20Kuchnik%20and%20Shachi%20H.%20Kumar%20and%20Chris%20Lengerich%20and%20Bo%20Li%20and%20Zeyi%20Liao%20and%20Eileen%20Peters%20Long%20and%20Victor%20Lu%20and%20Yifan%20Mai%20and%20Priyanka%20Mary%20Mammen%20and%20Kelvin%20Manyeki%20and%20Sean%20McGregor%20and%20Virendra%20Mehta%20and%20Shafee%20Mohammed%20and%20Emanuel%20Moss%20and%20Lama%20Nachman%20and%20Dinesh%20Jinenhally%20Naganna%20and%20Amin%20Nikanjam%20and%20Besmira%20Nushi%20and%20Luis%20Oala%20and%20Iftach%20Orr%20and%20Alicia%20Parrish%20and%20Cigdem%20Patlak%20and%20William%20Pietri%20and%20Forough%20Poursabzi-Sangdeh%20and%20Eleonora%20Presani%20and%20Fabrizio%20Puletti%20and%20Paul%20R%C3%B6ttger%20and%20Saurav%20Sahay%20and%20Tim%20Santos%20and%20Nino%20Scherrer%20and%20Alice%20Schoenauer%20Sebag%20and%20Patrick%20Schramowski%20and%20Abolfazl%20Shahbazi%20and%20Vin%20Sharma%20and%20Xudong%20Shen%20and%20Vamsi%20Sistla%20and%20Leonard%20Tang%20and%20Davide%20Testuggine%20and%20Vithursan%20Thangarasa%20and%20Elizabeth%20Anne%20Watkins%20and%20Rebecca%20Weiss%20and%20Chris%20Welty%20and%20Tyler%20Wilbers%20and%20Adina%20Williams%20and%20Carole-Jean%20Wu%20and%20Poonam%20Yadav%20and%20Xianjun%20Yang%20and%20Yi%20Zeng%20and%20Wenhui%20Zhang%20and%20Fedor%20Zhdanov%20and%20Jiacheng%20Zhu%20and%20Percy%20Liang%20and%20Peter%20Mattson%20and%20Joaquin%20Vanschoren&entry.1292438233=%20%20This%20paper%20introduces%20v0.5%20of%20the%20AI%20Safety%20Benchmark%2C%20which%20has%20been%20created%0Aby%20the%20MLCommons%20AI%20Safety%20Working%20Group.%20The%20AI%20Safety%20Benchmark%20has%20been%0Adesigned%20to%20assess%20the%20safety%20risks%20of%20AI%20systems%20that%20use%20chat-tuned%20language%0Amodels.%20We%20introduce%20a%20principled%20approach%20to%20specifying%20and%20constructing%20the%0Abenchmark%2C%20which%20for%20v0.5%20covers%20only%20a%20single%20use%20case%20%28an%20adult%20chatting%20to%20a%0Ageneral-purpose%20assistant%20in%20English%29%2C%20and%20a%20limited%20set%20of%20personas%20%28i.e.%2C%0Atypical%20users%2C%20malicious%20users%2C%20and%20vulnerable%20users%29.%20We%20created%20a%20new%0Ataxonomy%20of%2013%20hazard%20categories%2C%20of%20which%207%20have%20tests%20in%20the%20v0.5%20benchmark.%0AWe%20plan%20to%20release%20version%201.0%20of%20the%20AI%20Safety%20Benchmark%20by%20the%20end%20of%202024.%0AThe%20v1.0%20benchmark%20will%20provide%20meaningful%20insights%20into%20the%20safety%20of%20AI%0Asystems.%20However%2C%20the%20v0.5%20benchmark%20should%20not%20be%20used%20to%20assess%20the%20safety%20of%0AAI%20systems.%20We%20have%20sought%20to%20fully%20document%20the%20limitations%2C%20flaws%2C%20and%0Achallenges%20of%20v0.5.%20This%20release%20of%20v0.5%20of%20the%20AI%20Safety%20Benchmark%20includes%0A%281%29%20a%20principled%20approach%20to%20specifying%20and%20constructing%20the%20benchmark%2C%20which%0Acomprises%20use%20cases%2C%20types%20of%20systems%20under%20test%20%28SUTs%29%2C%20language%20and%20context%2C%0Apersonas%2C%20tests%2C%20and%20test%20items%3B%20%282%29%20a%20taxonomy%20of%2013%20hazard%20categories%20with%0Adefinitions%20and%20subcategories%3B%20%283%29%20tests%20for%20seven%20of%20the%20hazard%20categories%2C%0Aeach%20comprising%20a%20unique%20set%20of%20test%20items%2C%20i.e.%2C%20prompts.%20There%20are%2043%2C090%0Atest%20items%20in%20total%2C%20which%20we%20created%20with%20templates%3B%20%284%29%20a%20grading%20system%20for%0AAI%20systems%20against%20the%20benchmark%3B%20%285%29%20an%20openly%20available%20platform%2C%20and%0Adownloadable%20tool%2C%20called%20ModelBench%20that%20can%20be%20used%20to%20evaluate%20the%20safety%20of%0AAI%20systems%20on%20the%20benchmark%3B%20%286%29%20an%20example%20evaluation%20report%20which%20benchmarks%0Athe%20performance%20of%20over%20a%20dozen%20openly%20available%20chat-tuned%20language%20models%3B%0A%287%29%20a%20test%20specification%20for%20the%20benchmark.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12241v1&entry.124074799=Read"},
{"title": "Simultaneous Interpretation Corpus Construction by Large Language Models\n  in Distant Language Pair", "author": "Yusuke Sakai and Mana Makinae and Hidetaka Kamigaito and Taro Watanabe", "abstract": "  In Simultaneous Machine Translation (SiMT) systems, training with a\nsimultaneous interpretation (SI) corpus is an effective method for achieving\nhigh-quality yet low-latency systems. However, it is very challenging to curate\nsuch a corpus due to limitations in the abilities of annotators, and hence,\nexisting SI corpora are limited. Therefore, we propose a method to convert\nexisting speech translation corpora into interpretation-style data, maintaining\nthe original word order and preserving the entire source content using Large\nLanguage Models (LLM-SI-Corpus). We demonstrate that fine-tuning SiMT models in\ntext-to-text and speech-to-text settings with the LLM-SI-Corpus reduces\nlatencies while maintaining the same level of quality as the models trained\nwith offline datasets. The LLM-SI-Corpus is available at\n\\url{https://github.com/yusuke1997/LLM-SI-Corpus}.\n", "link": "http://arxiv.org/abs/2404.12299v1", "date": "2024-04-18", "relevancy": 1.2917, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4426}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4296}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4211}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Simultaneous%20Interpretation%20Corpus%20Construction%20by%20Large%20Language%20Models%0A%20%20in%20Distant%20Language%20Pair&body=Title%3A%20Simultaneous%20Interpretation%20Corpus%20Construction%20by%20Large%20Language%20Models%0A%20%20in%20Distant%20Language%20Pair%0AAuthor%3A%20Yusuke%20Sakai%20and%20Mana%20Makinae%20and%20Hidetaka%20Kamigaito%20and%20Taro%20Watanabe%0AAbstract%3A%20%20%20In%20Simultaneous%20Machine%20Translation%20%28SiMT%29%20systems%2C%20training%20with%20a%0Asimultaneous%20interpretation%20%28SI%29%20corpus%20is%20an%20effective%20method%20for%20achieving%0Ahigh-quality%20yet%20low-latency%20systems.%20However%2C%20it%20is%20very%20challenging%20to%20curate%0Asuch%20a%20corpus%20due%20to%20limitations%20in%20the%20abilities%20of%20annotators%2C%20and%20hence%2C%0Aexisting%20SI%20corpora%20are%20limited.%20Therefore%2C%20we%20propose%20a%20method%20to%20convert%0Aexisting%20speech%20translation%20corpora%20into%20interpretation-style%20data%2C%20maintaining%0Athe%20original%20word%20order%20and%20preserving%20the%20entire%20source%20content%20using%20Large%0ALanguage%20Models%20%28LLM-SI-Corpus%29.%20We%20demonstrate%20that%20fine-tuning%20SiMT%20models%20in%0Atext-to-text%20and%20speech-to-text%20settings%20with%20the%20LLM-SI-Corpus%20reduces%0Alatencies%20while%20maintaining%20the%20same%20level%20of%20quality%20as%20the%20models%20trained%0Awith%20offline%20datasets.%20The%20LLM-SI-Corpus%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/yusuke1997/LLM-SI-Corpus%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12299v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simultaneous%20Interpretation%20Corpus%20Construction%20by%20Large%20Language%20Models%0A%20%20in%20Distant%20Language%20Pair&entry.906535625=Yusuke%20Sakai%20and%20Mana%20Makinae%20and%20Hidetaka%20Kamigaito%20and%20Taro%20Watanabe&entry.1292438233=%20%20In%20Simultaneous%20Machine%20Translation%20%28SiMT%29%20systems%2C%20training%20with%20a%0Asimultaneous%20interpretation%20%28SI%29%20corpus%20is%20an%20effective%20method%20for%20achieving%0Ahigh-quality%20yet%20low-latency%20systems.%20However%2C%20it%20is%20very%20challenging%20to%20curate%0Asuch%20a%20corpus%20due%20to%20limitations%20in%20the%20abilities%20of%20annotators%2C%20and%20hence%2C%0Aexisting%20SI%20corpora%20are%20limited.%20Therefore%2C%20we%20propose%20a%20method%20to%20convert%0Aexisting%20speech%20translation%20corpora%20into%20interpretation-style%20data%2C%20maintaining%0Athe%20original%20word%20order%20and%20preserving%20the%20entire%20source%20content%20using%20Large%0ALanguage%20Models%20%28LLM-SI-Corpus%29.%20We%20demonstrate%20that%20fine-tuning%20SiMT%20models%20in%0Atext-to-text%20and%20speech-to-text%20settings%20with%20the%20LLM-SI-Corpus%20reduces%0Alatencies%20while%20maintaining%20the%20same%20level%20of%20quality%20as%20the%20models%20trained%0Awith%20offline%20datasets.%20The%20LLM-SI-Corpus%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/yusuke1997/LLM-SI-Corpus%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12299v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


