<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240502.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "author": "Zhexi Peng and Tianjia Shao and Yong Liu and Jingke Zhou and Yin Yang and Jingdong Wang and Kun Zhou", "abstract": "  We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction\nsystem with an RGBD camera for large-scale environments using Gaussian\nsplatting. The system features a compact Gaussian representation and a highly\nefficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be\neither opaque or nearly transparent, with the opaque ones fitting the surface\nand dominant colors, and transparent ones fitting residual colors. By rendering\ndepth in a different way from color rendering, we let a single opaque Gaussian\nwell fit a local surface region without the need of multiple overlapping\nGaussians, hence largely reducing the memory and computation cost. For\non-the-fly Gaussian optimization, we explicitly add Gaussians for three types\nof pixels per frame: newly observed, with large color errors, and with large\ndepth errors. We also categorize all Gaussians into stable and unstable ones,\nwhere the stable Gaussians are expected to well fit previously observed RGBD\nimages and otherwise unstable. We only optimize the unstable Gaussians and only\nrender the pixels occupied by unstable Gaussians. In this way, both the number\nof Gaussians to be optimized and pixels to be rendered are largely reduced, and\nthe optimization can be done in real time. We show real-time reconstructions of\na variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD\nSLAM, our system achieves comparable high-quality reconstruction but with\naround twice the speed and half the memory cost, and shows superior performance\nin the realism of novel view synthesis and camera tracking accuracy.\n", "link": "http://arxiv.org/abs/2404.19706v2", "date": "2024-05-01", "relevancy": 3.6624, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.9948}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6628}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5399}], "mailto": "mailto:daeR=997470421.yrtne&2v60791.4042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.ycarucca02%gnikcart02%aremac02%dna02%sisehtnys02%weiv02%levon02%fo02%msilaer02%eht02%niA0%ecnamrofrep02%roirepus02%swohs02%dna02%C2%tsoc02%yromem02%eht02%flah02%dna02%deeps02%eht02%eciwt02%dnuoraA0%htiw02%tub02%noitcurtsnocer02%ytilauq-hgih02%elbarapmoc02%seveihca02%metsys02%ruo02%C2%MALSA0%DBGR02%desab-FReN02%tra-eht-fo-etats02%eht02%htiw02%derapmoC02%.senecs02%egral02%fo02%yteirav02%aA0%fo02%snoitcurtsnocer02%emit-laer02%wohs02%eW02%.emit02%laer02%ni02%enod02%eb02%nac02%noitazimitpo02%ehtA0%dna02%C2%decuder02%ylegral02%era02%deredner02%eb02%ot02%slexip02%dna02%dezimitpo02%eb02%ot02%snaissuaG02%foA0%rebmun02%eht02%htob02%C2%yaw02%siht02%nI02%.snaissuaG02%elbatsnu02%yb02%deipucco02%slexip02%eht02%rednerA0%ylno02%dna02%snaissuaG02%elbatsnu02%eht02%ezimitpo02%ylno02%eW02%.elbatsnu02%esiwrehto02%dna02%segamiA0%DBGR02%devresbo02%ylsuoiverp02%tif02%llew02%ot02%detcepxe02%era02%snaissuaG02%elbats02%eht02%erehwA0%C2%seno02%elbatsnu02%dna02%elbats02%otni02%snaissuaG02%lla02%ezirogetac02%osla02%eW02%.srorre02%htpedA0%egral02%htiw02%dna02%C2%srorre02%roloc02%egral02%htiw02%C2%devresbo02%ylwen02%A3%emarf02%rep02%slexip02%foA0%sepyt02%eerht02%rof02%snaissuaG02%dda02%ylticilpxe02%ew02%C2%noitazimitpo02%naissuaG02%ylf-eht-noA0%roF02%.tsoc02%noitatupmoc02%dna02%yromem02%eht02%gnicuder02%ylegral02%ecneh02%C2%snaissuaGA0%gnippalrevo02%elpitlum02%fo02%deen02%eht02%tuohtiw02%noiger02%ecafrus02%lacol02%a02%tif02%llewA0%naissuaG02%euqapo02%elgnis02%a02%tel02%ew02%C2%gniredner02%roloc02%morf02%yaw02%tnereffid02%a02%ni02%htpedA0%gniredner02%yB02%.sroloc02%laudiser02%gnittif02%seno02%tnerapsnart02%dna02%C2%sroloc02%tnanimod02%dnaA0%ecafrus02%eht02%gnittif02%seno02%euqapo02%eht02%htiw02%C2%tnerapsnart02%ylraen02%ro02%euqapo02%rehtieA0%eb02%ot02%naissuaG02%hcae02%ecrof02%eW02%.emehcs02%noitazimitpo02%naissuaG02%ylf-eht-no02%tneiciffeA0%ylhgih02%a02%dna02%noitatneserper02%naissuaG02%tcapmoc02%a02%serutaef02%metsys02%ehT02%.gnittalpsA0%naissuaG02%gnisu02%stnemnorivne02%elacs-egral02%rof02%aremac02%DBGR02%na02%htiw02%metsysA0%noitcurtsnocer02%D302%emit-laer02%a02%C2%92%MALS-GTR82%02%MALS02%naissuaG02%emit-laeR02%tneserp02%eW02%02%=3328342921.yrtne&uohZ02%nuK02%dna02%gnaW02%gnodgniJ02%dna02%gnaY02%niY02%dna02%uohZ02%ekgniJ02%dna02%uiL02%gnoY02%dna02%oahS02%aijnaiT02%dna02%gneP02%ixehZ=526535609.yrtne&gnittalpS02%naissuaG02%gnisu02%elacS02%ta02%noitcurtsnoceR02%D302%emit-laeR02%A3%MALS-GTR=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20RTG-SLAM%3A%20Real-time%203D%20Reconstruction%20at%20Scale%20using%20Gaussian%20Splatting&body=Title%3A%20RTG-SLAM%3A%20Real-time%203D%20Reconstruction%20at%20Scale%20using%20Gaussian%20Splatting%0AAuthor%3A%20Zhexi%20Peng%20and%20Tianjia%20Shao%20and%20Yong%20Liu%20and%20Jingke%20Zhou%20and%20Yin%20Yang%20and%20Jingdong%20Wang%20and%20Kun%20Zhou%0AAbstract%3A%20%20%20We%20present%20Real-time%20Gaussian%20SLAM%20%28RTG-SLAM%29%2C%20a%20real-time%203D%20reconstruction%0Asystem%20with%20an%20RGBD%20camera%20for%20large-scale%20environments%20using%20Gaussian%0Asplatting.%20The%20system%20features%20a%20compact%20Gaussian%20representation%20and%20a%20highly%0Aefficient%20on-the-fly%20Gaussian%20optimization%20scheme.%20We%20force%20each%20Gaussian%20to%20be%0Aeither%20opaque%20or%20nearly%20transparent%2C%20with%20the%20opaque%20ones%20fitting%20the%20surface%0Aand%20dominant%20colors%2C%20and%20transparent%20ones%20fitting%20residual%20colors.%20By%20rendering%0Adepth%20in%20a%20different%20way%20from%20color%20rendering%2C%20we%20let%20a%20single%20opaque%20Gaussian%0Awell%20fit%20a%20local%20surface%20region%20without%20the%20need%20of%20multiple%20overlapping%0AGaussians%2C%20hence%20largely%20reducing%20the%20memory%20and%20computation%20cost.%20For%0Aon-the-fly%20Gaussian%20optimization%2C%20we%20explicitly%20add%20Gaussians%20for%20three%20types%0Aof%20pixels%20per%20frame%3A%20newly%20observed%2C%20with%20large%20color%20errors%2C%20and%20with%20large%0Adepth%20errors.%20We%20also%20categorize%20all%20Gaussians%20into%20stable%20and%20unstable%20ones%2C%0Awhere%20the%20stable%20Gaussians%20are%20expected%20to%20well%20fit%20previously%20observed%20RGBD%0Aimages%20and%20otherwise%20unstable.%20We%20only%20optimize%20the%20unstable%20Gaussians%20and%20only%0Arender%20the%20pixels%20occupied%20by%20unstable%20Gaussians.%20In%20this%20way%2C%20both%20the%20number%0Aof%20Gaussians%20to%20be%20optimized%20and%20pixels%20to%20be%20rendered%20are%20largely%20reduced%2C%20and%0Athe%20optimization%20can%20be%20done%20in%20real%20time.%20We%20show%20real-time%20reconstructions%20of%0Aa%20variety%20of%20large%20scenes.%20Compared%20with%20the%20state-of-the-art%20NeRF-based%20RGBD%0ASLAM%2C%20our%20system%20achieves%20comparable%20high-quality%20reconstruction%20but%20with%0Aaround%20twice%20the%20speed%20and%20half%20the%20memory%20cost%2C%20and%20shows%20superior%20performance%0Ain%20the%20realism%20of%20novel%20view%20synthesis%20and%20camera%20tracking%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19706v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RTG-SLAM%3A%20Real-time%203D%20Reconstruction%20at%20Scale%20using%20Gaussian%20Splatting&entry.906535625=Zhexi%20Peng%20and%20Tianjia%20Shao%20and%20Yong%20Liu%20and%20Jingke%20Zhou%20and%20Yin%20Yang%20and%20Jingdong%20Wang%20and%20Kun%20Zhou&entry.1292438233=%20%20We%20present%20Real-time%20Gaussian%20SLAM%20%28RTG-SLAM%29%2C%20a%20real-time%203D%20reconstruction%0Asystem%20with%20an%20RGBD%20camera%20for%20large-scale%20environments%20using%20Gaussian%0Asplatting.%20The%20system%20features%20a%20compact%20Gaussian%20representation%20and%20a%20highly%0Aefficient%20on-the-fly%20Gaussian%20optimization%20scheme.%20We%20force%20each%20Gaussian%20to%20be%0Aeither%20opaque%20or%20nearly%20transparent%2C%20with%20the%20opaque%20ones%20fitting%20the%20surface%0Aand%20dominant%20colors%2C%20and%20transparent%20ones%20fitting%20residual%20colors.%20By%20rendering%0Adepth%20in%20a%20different%20way%20from%20color%20rendering%2C%20we%20let%20a%20single%20opaque%20Gaussian%0Awell%20fit%20a%20local%20surface%20region%20without%20the%20need%20of%20multiple%20overlapping%0AGaussians%2C%20hence%20largely%20reducing%20the%20memory%20and%20computation%20cost.%20For%0Aon-the-fly%20Gaussian%20optimization%2C%20we%20explicitly%20add%20Gaussians%20for%20three%20types%0Aof%20pixels%20per%20frame%3A%20newly%20observed%2C%20with%20large%20color%20errors%2C%20and%20with%20large%0Adepth%20errors.%20We%20also%20categorize%20all%20Gaussians%20into%20stable%20and%20unstable%20ones%2C%0Awhere%20the%20stable%20Gaussians%20are%20expected%20to%20well%20fit%20previously%20observed%20RGBD%0Aimages%20and%20otherwise%20unstable.%20We%20only%20optimize%20the%20unstable%20Gaussians%20and%20only%0Arender%20the%20pixels%20occupied%20by%20unstable%20Gaussians.%20In%20this%20way%2C%20both%20the%20number%0Aof%20Gaussians%20to%20be%20optimized%20and%20pixels%20to%20be%20rendered%20are%20largely%20reduced%2C%20and%0Athe%20optimization%20can%20be%20done%20in%20real%20time.%20We%20show%20real-time%20reconstructions%20of%0Aa%20variety%20of%20large%20scenes.%20Compared%20with%20the%20state-of-the-art%20NeRF-based%20RGBD%0ASLAM%2C%20our%20system%20achieves%20comparable%20high-quality%20reconstruction%20but%20with%0Aaround%20twice%20the%20speed%20and%20half%20the%20memory%20cost%2C%20and%20shows%20superior%20performance%0Ain%20the%20realism%20of%20novel%20view%20synthesis%20and%20camera%20tracking%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19706v2&entry.124074799=Read"},
{"title": "Morphing Tokens Draw Strong Masked Image Models", "author": "Taekyung Kim and Byeongho Heo and Dongyoon Han", "abstract": "  Masked image modeling (MIM) is a promising option for training Vision\nTransformers among various self-supervised learning (SSL) methods. The essence\nof MIM lies in token-wise masked token predictions, with targets tokenized from\nimages or generated by pre-trained models such as vision-language models. While\ntokenizers or pre-trained models are plausible MIM targets, they often offer\nspatially inconsistent targets even for neighboring tokens, complicating models\nto learn unified discriminative representations. Our pilot study confirms that\naddressing spatial inconsistencies has the potential to enhance representation\nquality. Motivated by the findings, we introduce a novel self-supervision\nsignal called Dynamic Token Morphing (DTM), which dynamically aggregates\ncontextually related tokens to yield contextualized targets. DTM is compatible\nwith various SSL frameworks; we showcase an improved MIM by employing DTM,\nbarely introducing extra training costs. Our experiments on ImageNet-1K and\nADE20K demonstrate the superiority of our methods compared with\nstate-of-the-art, complex MIM methods. Furthermore, the comparative evaluation\nof the iNaturalists and fine-grained visual classification datasets further\nvalidates the transferability of our method on various downstream tasks. Code\nis available at https://github.com/naver-ai/dtm\n", "link": "http://arxiv.org/abs/2401.00254v2", "date": "2024-05-02", "relevancy": 2.9794, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6003}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5974}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.59}], "mailto": "mailto:daeR=997470421.yrtne&2v45200.1042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%mtd/ia-revan/moc.buhtig//A3%sptth02%ta02%elbaliava02%siA0%edoC02%.sksat02%maertsnwod02%suoirav02%no02%dohtem02%ruo02%fo02%ytilibarefsnart02%eht02%setadilavA0%rehtruf02%stesatad02%noitacifissalc02%lausiv02%deniarg-enif02%dna02%stsilarutaNi02%eht02%foA0%noitaulave02%evitarapmoc02%eht02%C2%eromrehtruF02%.sdohtem02%MIM02%xelpmoc02%C2%tra-eht-fo-etatsA0%htiw02%derapmoc02%sdohtem02%ruo02%fo02%ytiroirepus02%eht02%etartsnomed02%K02EDAA0%dna02%K1-teNegamI02%no02%stnemirepxe02%ruO02%.stsoc02%gniniart02%artxe02%gnicudortni02%ylerabA0%C2%MTD02%gniyolpme02%yb02%MIM02%devorpmi02%na02%esacwohs02%ew02%B3%skrowemarf02%LSS02%suoirav02%htiwA0%elbitapmoc02%si02%MTD02%.stegrat02%dezilautxetnoc02%dleiy02%ot02%snekot02%detaler02%yllautxetnocA0%setagergga02%yllacimanyd02%hcihw02%C2%92%MTD82%02%gnihproM02%nekoT02%cimanyD02%dellac02%langisA0%noisivrepus-fles02%levon02%a02%ecudortni02%ew02%C2%sgnidnif02%eht02%yb02%detavitoM02%.ytilauqA0%noitatneserper02%ecnahne02%ot02%laitnetop02%eht02%sah02%seicnetsisnocni02%laitaps02%gnisserddaA0%taht02%smrifnoc02%yduts02%tolip02%ruO02%.snoitatneserper02%evitanimircsid02%deifinu02%nrael02%otA0%sledom02%gnitacilpmoc02%C2%snekot02%gnirobhgien02%rof02%neve02%stegrat02%tnetsisnocni02%yllaitapsA0%reffo02%netfo02%yeht02%C2%stegrat02%MIM02%elbisualp02%era02%sledom02%deniart-erp02%ro02%srezinekotA0%elihW02%.sledom02%egaugnal-noisiv02%sa02%hcus02%sledom02%deniart-erp02%yb02%detareneg02%ro02%segamiA0%morf02%dezinekot02%stegrat02%htiw02%C2%snoitciderp02%nekot02%deksam02%esiw-nekot02%ni02%seil02%MIM02%foA0%ecnesse02%ehT02%.sdohtem02%92%LSS82%02%gninrael02%desivrepus-fles02%suoirav02%gnoma02%sremrofsnarTA0%noisiV02%gniniart02%rof02%noitpo02%gnisimorp02%a02%si02%92%MIM82%02%gniledom02%egami02%deksaM02%02%=3328342921.yrtne&naH02%nooygnoD02%dna02%oeH02%ohgnoeyB02%dna02%miK02%gnuykeaT=526535609.yrtne&sledoM02%egamI02%deksaM02%gnortS02%warD02%snekoT02%gnihproM=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Morphing%20Tokens%20Draw%20Strong%20Masked%20Image%20Models&body=Title%3A%20Morphing%20Tokens%20Draw%20Strong%20Masked%20Image%20Models%0AAuthor%3A%20Taekyung%20Kim%20and%20Byeongho%20Heo%20and%20Dongyoon%20Han%0AAbstract%3A%20%20%20Masked%20image%20modeling%20%28MIM%29%20is%20a%20promising%20option%20for%20training%20Vision%0ATransformers%20among%20various%20self-supervised%20learning%20%28SSL%29%20methods.%20The%20essence%0Aof%20MIM%20lies%20in%20token-wise%20masked%20token%20predictions%2C%20with%20targets%20tokenized%20from%0Aimages%20or%20generated%20by%20pre-trained%20models%20such%20as%20vision-language%20models.%20While%0Atokenizers%20or%20pre-trained%20models%20are%20plausible%20MIM%20targets%2C%20they%20often%20offer%0Aspatially%20inconsistent%20targets%20even%20for%20neighboring%20tokens%2C%20complicating%20models%0Ato%20learn%20unified%20discriminative%20representations.%20Our%20pilot%20study%20confirms%20that%0Aaddressing%20spatial%20inconsistencies%20has%20the%20potential%20to%20enhance%20representation%0Aquality.%20Motivated%20by%20the%20findings%2C%20we%20introduce%20a%20novel%20self-supervision%0Asignal%20called%20Dynamic%20Token%20Morphing%20%28DTM%29%2C%20which%20dynamically%20aggregates%0Acontextually%20related%20tokens%20to%20yield%20contextualized%20targets.%20DTM%20is%20compatible%0Awith%20various%20SSL%20frameworks%3B%20we%20showcase%20an%20improved%20MIM%20by%20employing%20DTM%2C%0Abarely%20introducing%20extra%20training%20costs.%20Our%20experiments%20on%20ImageNet-1K%20and%0AADE20K%20demonstrate%20the%20superiority%20of%20our%20methods%20compared%20with%0Astate-of-the-art%2C%20complex%20MIM%20methods.%20Furthermore%2C%20the%20comparative%20evaluation%0Aof%20the%20iNaturalists%20and%20fine-grained%20visual%20classification%20datasets%20further%0Avalidates%20the%20transferability%20of%20our%20method%20on%20various%20downstream%20tasks.%20Code%0Ais%20available%20at%20https%3A//github.com/naver-ai/dtm%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.00254v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Morphing%20Tokens%20Draw%20Strong%20Masked%20Image%20Models&entry.906535625=Taekyung%20Kim%20and%20Byeongho%20Heo%20and%20Dongyoon%20Han&entry.1292438233=%20%20Masked%20image%20modeling%20%28MIM%29%20is%20a%20promising%20option%20for%20training%20Vision%0ATransformers%20among%20various%20self-supervised%20learning%20%28SSL%29%20methods.%20The%20essence%0Aof%20MIM%20lies%20in%20token-wise%20masked%20token%20predictions%2C%20with%20targets%20tokenized%20from%0Aimages%20or%20generated%20by%20pre-trained%20models%20such%20as%20vision-language%20models.%20While%0Atokenizers%20or%20pre-trained%20models%20are%20plausible%20MIM%20targets%2C%20they%20often%20offer%0Aspatially%20inconsistent%20targets%20even%20for%20neighboring%20tokens%2C%20complicating%20models%0Ato%20learn%20unified%20discriminative%20representations.%20Our%20pilot%20study%20confirms%20that%0Aaddressing%20spatial%20inconsistencies%20has%20the%20potential%20to%20enhance%20representation%0Aquality.%20Motivated%20by%20the%20findings%2C%20we%20introduce%20a%20novel%20self-supervision%0Asignal%20called%20Dynamic%20Token%20Morphing%20%28DTM%29%2C%20which%20dynamically%20aggregates%0Acontextually%20related%20tokens%20to%20yield%20contextualized%20targets.%20DTM%20is%20compatible%0Awith%20various%20SSL%20frameworks%3B%20we%20showcase%20an%20improved%20MIM%20by%20employing%20DTM%2C%0Abarely%20introducing%20extra%20training%20costs.%20Our%20experiments%20on%20ImageNet-1K%20and%0AADE20K%20demonstrate%20the%20superiority%20of%20our%20methods%20compared%20with%0Astate-of-the-art%2C%20complex%20MIM%20methods.%20Furthermore%2C%20the%20comparative%20evaluation%0Aof%20the%20iNaturalists%20and%20fine-grained%20visual%20classification%20datasets%20further%0Avalidates%20the%20transferability%20of%20our%20method%20on%20various%20downstream%20tasks.%20Code%0Ais%20available%20at%20https%3A//github.com/naver-ai/dtm%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.00254v2&entry.124074799=Read"},
{"title": "NeRF-Guided Unsupervised Learning of RGB-D Registration", "author": "Zhinan Yu and Zheng Qin and Yijie Tang and Yongjun Wang and Renjiao Yi and Chenyang Zhu and Kai Xu", "abstract": "  This paper focuses on training a robust RGB-D registration model without\nground-truth pose supervision. Existing methods usually adopt a pairwise\ntraining strategy based on differentiable rendering, which enforces the\nphotometric and the geometric consistency between the two registered frames as\nsupervision. However, this frame-to-frame framework suffers from poor\nmulti-view consistency due to factors such as lighting changes, geometry\nocclusion and reflective materials. In this paper, we present NeRF-UR, a novel\nframe-to-model optimization framework for unsupervised RGB-D registration.\nInstead of frame-to-frame consistency, we leverage the neural radiance field\n(NeRF) as a global model of the scene and use the consistency between the input\nand the NeRF-rerendered frames for pose optimization. This design can\nsignificantly improve the robustness in scenarios with poor multi-view\nconsistency and provides better learning signal for the registration model.\nFurthermore, to bootstrap the NeRF optimization, we create a synthetic dataset,\nSim-RGBD, through a photo-realistic simulator to warm up the registration\nmodel. By first training the registration model on Sim-RGBD and later\nunsupervisedly fine-tuning on real data, our framework enables distilling the\ncapability of feature extraction and registration from simulation to reality.\nOur method outperforms the state-of-the-art counterparts on two popular indoor\nRGB-D datasets, ScanNet and 3DMatch. Code and models will be released for paper\nreproduction.\n", "link": "http://arxiv.org/abs/2405.00507v1", "date": "2024-05-01", "relevancy": 2.9745, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6318}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5978}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5551}], "mailto": "mailto:daeR=997470421.yrtne&1v70500.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.noitcudorperA0%repap02%rof02%desaeler02%eb02%lliw02%sledom02%dna02%edoC02%.hctaMD302%dna02%teNnacS02%C2%stesatad02%D-BGRA0%roodni02%ralupop02%owt02%no02%strapretnuoc02%tra-eht-fo-etats02%eht02%smrofreptuo02%dohtem02%ruOA0%.ytilaer02%ot02%noitalumis02%morf02%noitartsiger02%dna02%noitcartxe02%erutaef02%fo02%ytilibapacA0%eht02%gnillitsid02%selbane02%krowemarf02%ruo02%C2%atad02%laer02%no02%gninut-enif02%yldesivrepusnuA0%retal02%dna02%DBGR-miS02%no02%ledom02%noitartsiger02%eht02%gniniart02%tsrif02%yB02%.ledomA0%noitartsiger02%eht02%pu02%mraw02%ot02%rotalumis02%citsilaer-otohp02%a02%hguorht02%C2%DBGR-miSA0%C2%tesatad02%citehtnys02%a02%etaerc02%ew02%C2%noitazimitpo02%FReN02%eht02%partstoob02%ot02%C2%eromrehtruFA0%.ledom02%noitartsiger02%eht02%rof02%langis02%gninrael02%retteb02%sedivorp02%dna02%ycnetsisnocA0%weiv-itlum02%roop02%htiw02%soiranecs02%ni02%ssentsubor02%eht02%evorpmi02%yltnacifingisA0%nac02%ngised02%sihT02%.noitazimitpo02%esop02%rof02%semarf02%derednerer-FReN02%eht02%dnaA0%tupni02%eht02%neewteb02%ycnetsisnoc02%eht02%esu02%dna02%enecs02%eht02%fo02%ledom02%labolg02%a02%sa02%92%FReN82%A0%dleif02%ecnaidar02%laruen02%eht02%egarevel02%ew02%C2%ycnetsisnoc02%emarf-ot-emarf02%fo02%daetsnIA0%.noitartsiger02%D-BGR02%desivrepusnu02%rof02%krowemarf02%noitazimitpo02%ledom-ot-emarfA0%levon02%a02%C2%RU-FReN02%tneserp02%ew02%C2%repap02%siht02%nI02%.slairetam02%evitcelfer02%dna02%noisulccoA0%yrtemoeg02%C2%segnahc02%gnithgil02%sa02%hcus02%srotcaf02%ot02%eud02%ycnetsisnoc02%weiv-itlumA0%roop02%morf02%sreffus02%krowemarf02%emarf-ot-emarf02%siht02%C2%revewoH02%.noisivrepusA0%sa02%semarf02%deretsiger02%owt02%eht02%neewteb02%ycnetsisnoc02%cirtemoeg02%eht02%dna02%cirtemotohpA0%eht02%secrofne02%hcihw02%C2%gniredner02%elbaitnereffid02%no02%desab02%ygetarts02%gniniartA0%esiwriap02%a02%tpoda02%yllausu02%sdohtem02%gnitsixE02%.noisivrepus02%esop02%hturt-dnuorgA0%tuohtiw02%ledom02%noitartsiger02%D-BGR02%tsubor02%a02%gniniart02%no02%sesucof02%repap02%sihT02%02%=3328342921.yrtne&uX02%iaK02%dna02%uhZ02%gnaynehC02%dna02%iY02%oaijneR02%dna02%gnaW02%nujgnoY02%dna02%gnaT02%eijiY02%dna02%niQ02%gnehZ02%dna02%uY02%nanihZ=526535609.yrtne&noitartsigeR02%D-BGR02%fo02%gninraeL02%desivrepusnU02%dediuG-FReN=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20NeRF-Guided%20Unsupervised%20Learning%20of%20RGB-D%20Registration&body=Title%3A%20NeRF-Guided%20Unsupervised%20Learning%20of%20RGB-D%20Registration%0AAuthor%3A%20Zhinan%20Yu%20and%20Zheng%20Qin%20and%20Yijie%20Tang%20and%20Yongjun%20Wang%20and%20Renjiao%20Yi%20and%20Chenyang%20Zhu%20and%20Kai%20Xu%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20training%20a%20robust%20RGB-D%20registration%20model%20without%0Aground-truth%20pose%20supervision.%20Existing%20methods%20usually%20adopt%20a%20pairwise%0Atraining%20strategy%20based%20on%20differentiable%20rendering%2C%20which%20enforces%20the%0Aphotometric%20and%20the%20geometric%20consistency%20between%20the%20two%20registered%20frames%20as%0Asupervision.%20However%2C%20this%20frame-to-frame%20framework%20suffers%20from%20poor%0Amulti-view%20consistency%20due%20to%20factors%20such%20as%20lighting%20changes%2C%20geometry%0Aocclusion%20and%20reflective%20materials.%20In%20this%20paper%2C%20we%20present%20NeRF-UR%2C%20a%20novel%0Aframe-to-model%20optimization%20framework%20for%20unsupervised%20RGB-D%20registration.%0AInstead%20of%20frame-to-frame%20consistency%2C%20we%20leverage%20the%20neural%20radiance%20field%0A%28NeRF%29%20as%20a%20global%20model%20of%20the%20scene%20and%20use%20the%20consistency%20between%20the%20input%0Aand%20the%20NeRF-rerendered%20frames%20for%20pose%20optimization.%20This%20design%20can%0Asignificantly%20improve%20the%20robustness%20in%20scenarios%20with%20poor%20multi-view%0Aconsistency%20and%20provides%20better%20learning%20signal%20for%20the%20registration%20model.%0AFurthermore%2C%20to%20bootstrap%20the%20NeRF%20optimization%2C%20we%20create%20a%20synthetic%20dataset%2C%0ASim-RGBD%2C%20through%20a%20photo-realistic%20simulator%20to%20warm%20up%20the%20registration%0Amodel.%20By%20first%20training%20the%20registration%20model%20on%20Sim-RGBD%20and%20later%0Aunsupervisedly%20fine-tuning%20on%20real%20data%2C%20our%20framework%20enables%20distilling%20the%0Acapability%20of%20feature%20extraction%20and%20registration%20from%20simulation%20to%20reality.%0AOur%20method%20outperforms%20the%20state-of-the-art%20counterparts%20on%20two%20popular%20indoor%0ARGB-D%20datasets%2C%20ScanNet%20and%203DMatch.%20Code%20and%20models%20will%20be%20released%20for%20paper%0Areproduction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00507v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeRF-Guided%20Unsupervised%20Learning%20of%20RGB-D%20Registration&entry.906535625=Zhinan%20Yu%20and%20Zheng%20Qin%20and%20Yijie%20Tang%20and%20Yongjun%20Wang%20and%20Renjiao%20Yi%20and%20Chenyang%20Zhu%20and%20Kai%20Xu&entry.1292438233=%20%20This%20paper%20focuses%20on%20training%20a%20robust%20RGB-D%20registration%20model%20without%0Aground-truth%20pose%20supervision.%20Existing%20methods%20usually%20adopt%20a%20pairwise%0Atraining%20strategy%20based%20on%20differentiable%20rendering%2C%20which%20enforces%20the%0Aphotometric%20and%20the%20geometric%20consistency%20between%20the%20two%20registered%20frames%20as%0Asupervision.%20However%2C%20this%20frame-to-frame%20framework%20suffers%20from%20poor%0Amulti-view%20consistency%20due%20to%20factors%20such%20as%20lighting%20changes%2C%20geometry%0Aocclusion%20and%20reflective%20materials.%20In%20this%20paper%2C%20we%20present%20NeRF-UR%2C%20a%20novel%0Aframe-to-model%20optimization%20framework%20for%20unsupervised%20RGB-D%20registration.%0AInstead%20of%20frame-to-frame%20consistency%2C%20we%20leverage%20the%20neural%20radiance%20field%0A%28NeRF%29%20as%20a%20global%20model%20of%20the%20scene%20and%20use%20the%20consistency%20between%20the%20input%0Aand%20the%20NeRF-rerendered%20frames%20for%20pose%20optimization.%20This%20design%20can%0Asignificantly%20improve%20the%20robustness%20in%20scenarios%20with%20poor%20multi-view%0Aconsistency%20and%20provides%20better%20learning%20signal%20for%20the%20registration%20model.%0AFurthermore%2C%20to%20bootstrap%20the%20NeRF%20optimization%2C%20we%20create%20a%20synthetic%20dataset%2C%0ASim-RGBD%2C%20through%20a%20photo-realistic%20simulator%20to%20warm%20up%20the%20registration%0Amodel.%20By%20first%20training%20the%20registration%20model%20on%20Sim-RGBD%20and%20later%0Aunsupervisedly%20fine-tuning%20on%20real%20data%2C%20our%20framework%20enables%20distilling%20the%0Acapability%20of%20feature%20extraction%20and%20registration%20from%20simulation%20to%20reality.%0AOur%20method%20outperforms%20the%20state-of-the-art%20counterparts%20on%20two%20popular%20indoor%0ARGB-D%20datasets%2C%20ScanNet%20and%203DMatch.%20Code%20and%20models%20will%20be%20released%20for%20paper%0Areproduction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00507v1&entry.124074799=Read"},
{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "author": "Kai He and Kaixin Yao and Qixuan Zhang and Lingjie Liu and Jingyi Yu and Lan Xu", "abstract": "  Apparel's significant role in human appearance underscores the importance of\ngarment digitalization for digital human creation. Recent advances in 3D\ncontent creation are pivotal for digital human creation. Nonetheless, garment\ngeneration from text guidance is still nascent. We introduce a text-driven 3D\ngarment generation framework, DressCode, which aims to democratize design for\nnovices and offer immense potential in fashion design, virtual try-on, and\ndigital human creation. We first introduce SewingGPT, a GPT-based architecture\nintegrating cross-attention with text-conditioned embedding to generate sewing\npatterns with text guidance. We then tailor a pre-trained Stable Diffusion to\ngenerate tile-based Physically-based Rendering (PBR) textures for the garments.\nBy leveraging a large language model, our framework generates CG-friendly\ngarments through natural language interaction. It also facilitates pattern\ncompletion and texture editing, streamlining the design process through\nuser-friendly interaction. This framework fosters innovation by allowing\ncreators to freely experiment with designs and incorporate unique elements into\ntheir work. With comprehensive evaluations and comparisons with other\nstate-of-the-art methods, our method showcases superior quality and alignment\nwith input prompts. User studies further validate our high-quality rendering\nresults, highlighting its practical utility and potential in production\nsettings. Our project page is https://IHe-KaiI.github.io/DressCode/.\n", "link": "http://arxiv.org/abs/2401.16465v3", "date": "2024-05-01", "relevancy": 2.9689, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 1.0}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6046}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5395}], "mailto": "mailto:daeR=997470421.yrtne&3v56461.1042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%./edoCsserD/oi.buhtig.IiaK-eHI//A3%sptth02%si02%egap02%tcejorp02%ruO02%.sgnittesA0%noitcudorp02%ni02%laitnetop02%dna02%ytilitu02%lacitcarp02%sti02%gnithgilhgih02%C2%stluserA0%gniredner02%ytilauq-hgih02%ruo02%etadilav02%rehtruf02%seiduts02%resU02%.stpmorp02%tupni02%htiwA0%tnemngila02%dna02%ytilauq02%roirepus02%sesacwohs02%dohtem02%ruo02%C2%sdohtem02%tra-eht-fo-etatsA0%rehto02%htiw02%snosirapmoc02%dna02%snoitaulave02%evisneherpmoc02%htiW02%.krow02%riehtA0%otni02%stnemele02%euqinu02%etaroprocni02%dna02%sngised02%htiw02%tnemirepxe02%yleerf02%ot02%srotaercA0%gniwolla02%yb02%noitavonni02%sretsof02%krowemarf02%sihT02%.noitcaretni02%yldneirf-resuA0%hguorht02%ssecorp02%ngised02%eht02%gninilmaerts02%C2%gnitide02%erutxet02%dna02%noitelpmocA0%nrettap02%setatilicaf02%osla02%tI02%.noitcaretni02%egaugnal02%larutan02%hguorht02%stnemragA0%yldneirf-GC02%setareneg02%krowemarf02%ruo02%C2%ledom02%egaugnal02%egral02%a02%gnigarevel02%yBA0%.stnemrag02%eht02%rof02%serutxet02%92%RBP82%02%gniredneR02%desab-yllacisyhP02%desab-elit02%etarenegA0%ot02%noisuffiD02%elbatS02%deniart-erp02%a02%roliat02%neht02%eW02%.ecnadiug02%txet02%htiw02%snrettapA0%gniwes02%etareneg02%ot02%gniddebme02%denoitidnoc-txet02%htiw02%noitnetta-ssorc02%gnitargetniA0%erutcetihcra02%desab-TPG02%a02%C2%TPGgniweS02%ecudortni02%tsrif02%eW02%.noitaerc02%namuh02%latigidA0%dna02%C2%no-yrt02%lautriv02%C2%ngised02%noihsaf02%ni02%laitnetop02%esnemmi02%reffo02%dna02%secivonA0%rof02%ngised02%ezitarcomed02%ot02%smia02%hcihw02%C2%edoCsserD02%C2%krowemarf02%noitareneg02%tnemragA0%D302%nevird-txet02%a02%ecudortni02%eW02%.tnecsan02%llits02%si02%ecnadiug02%txet02%morf02%noitarenegA0%tnemrag02%C2%sselehtenoN02%.noitaerc02%namuh02%latigid02%rof02%latovip02%era02%noitaerc02%tnetnocA0%D302%ni02%secnavda02%tneceR02%.noitaerc02%namuh02%latigid02%rof02%noitazilatigid02%tnemragA0%fo02%ecnatropmi02%eht02%serocsrednu02%ecnaraeppa02%namuh02%ni02%elor02%tnacifingis02%s72%lerappA02%02%=3328342921.yrtne&uX02%naL02%dna02%uY02%iygniJ02%dna02%uiL02%eijgniL02%dna02%gnahZ02%nauxiQ02%dna02%oaY02%nixiaK02%dna02%eH02%iaK=526535609.yrtne&ecnadiuG02%02%A0%txeT02%morf02%stnemraG02%gnitareneG02%dna02%gniweS02%ylevissergerotuA02%A3%edoCsserD=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20DressCode%3A%20Autoregressively%20Sewing%20and%20Generating%20Garments%20from%20Text%0A%20%20Guidance&body=Title%3A%20DressCode%3A%20Autoregressively%20Sewing%20and%20Generating%20Garments%20from%20Text%0A%20%20Guidance%0AAuthor%3A%20Kai%20He%20and%20Kaixin%20Yao%20and%20Qixuan%20Zhang%20and%20Lingjie%20Liu%20and%20Jingyi%20Yu%20and%20Lan%20Xu%0AAbstract%3A%20%20%20Apparel%27s%20significant%20role%20in%20human%20appearance%20underscores%20the%20importance%20of%0Agarment%20digitalization%20for%20digital%20human%20creation.%20Recent%20advances%20in%203D%0Acontent%20creation%20are%20pivotal%20for%20digital%20human%20creation.%20Nonetheless%2C%20garment%0Ageneration%20from%20text%20guidance%20is%20still%20nascent.%20We%20introduce%20a%20text-driven%203D%0Agarment%20generation%20framework%2C%20DressCode%2C%20which%20aims%20to%20democratize%20design%20for%0Anovices%20and%20offer%20immense%20potential%20in%20fashion%20design%2C%20virtual%20try-on%2C%20and%0Adigital%20human%20creation.%20We%20first%20introduce%20SewingGPT%2C%20a%20GPT-based%20architecture%0Aintegrating%20cross-attention%20with%20text-conditioned%20embedding%20to%20generate%20sewing%0Apatterns%20with%20text%20guidance.%20We%20then%20tailor%20a%20pre-trained%20Stable%20Diffusion%20to%0Agenerate%20tile-based%20Physically-based%20Rendering%20%28PBR%29%20textures%20for%20the%20garments.%0ABy%20leveraging%20a%20large%20language%20model%2C%20our%20framework%20generates%20CG-friendly%0Agarments%20through%20natural%20language%20interaction.%20It%20also%20facilitates%20pattern%0Acompletion%20and%20texture%20editing%2C%20streamlining%20the%20design%20process%20through%0Auser-friendly%20interaction.%20This%20framework%20fosters%20innovation%20by%20allowing%0Acreators%20to%20freely%20experiment%20with%20designs%20and%20incorporate%20unique%20elements%20into%0Atheir%20work.%20With%20comprehensive%20evaluations%20and%20comparisons%20with%20other%0Astate-of-the-art%20methods%2C%20our%20method%20showcases%20superior%20quality%20and%20alignment%0Awith%20input%20prompts.%20User%20studies%20further%20validate%20our%20high-quality%20rendering%0Aresults%2C%20highlighting%20its%20practical%20utility%20and%20potential%20in%20production%0Asettings.%20Our%20project%20page%20is%20https%3A//IHe-KaiI.github.io/DressCode/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.16465v3%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DressCode%3A%20Autoregressively%20Sewing%20and%20Generating%20Garments%20from%20Text%0A%20%20Guidance&entry.906535625=Kai%20He%20and%20Kaixin%20Yao%20and%20Qixuan%20Zhang%20and%20Lingjie%20Liu%20and%20Jingyi%20Yu%20and%20Lan%20Xu&entry.1292438233=%20%20Apparel%27s%20significant%20role%20in%20human%20appearance%20underscores%20the%20importance%20of%0Agarment%20digitalization%20for%20digital%20human%20creation.%20Recent%20advances%20in%203D%0Acontent%20creation%20are%20pivotal%20for%20digital%20human%20creation.%20Nonetheless%2C%20garment%0Ageneration%20from%20text%20guidance%20is%20still%20nascent.%20We%20introduce%20a%20text-driven%203D%0Agarment%20generation%20framework%2C%20DressCode%2C%20which%20aims%20to%20democratize%20design%20for%0Anovices%20and%20offer%20immense%20potential%20in%20fashion%20design%2C%20virtual%20try-on%2C%20and%0Adigital%20human%20creation.%20We%20first%20introduce%20SewingGPT%2C%20a%20GPT-based%20architecture%0Aintegrating%20cross-attention%20with%20text-conditioned%20embedding%20to%20generate%20sewing%0Apatterns%20with%20text%20guidance.%20We%20then%20tailor%20a%20pre-trained%20Stable%20Diffusion%20to%0Agenerate%20tile-based%20Physically-based%20Rendering%20%28PBR%29%20textures%20for%20the%20garments.%0ABy%20leveraging%20a%20large%20language%20model%2C%20our%20framework%20generates%20CG-friendly%0Agarments%20through%20natural%20language%20interaction.%20It%20also%20facilitates%20pattern%0Acompletion%20and%20texture%20editing%2C%20streamlining%20the%20design%20process%20through%0Auser-friendly%20interaction.%20This%20framework%20fosters%20innovation%20by%20allowing%0Acreators%20to%20freely%20experiment%20with%20designs%20and%20incorporate%20unique%20elements%20into%0Atheir%20work.%20With%20comprehensive%20evaluations%20and%20comparisons%20with%20other%0Astate-of-the-art%20methods%2C%20our%20method%20showcases%20superior%20quality%20and%20alignment%0Awith%20input%20prompts.%20User%20studies%20further%20validate%20our%20high-quality%20rendering%0Aresults%2C%20highlighting%20its%20practical%20utility%20and%20potential%20in%20production%0Asettings.%20Our%20project%20page%20is%20https%3A//IHe-KaiI.github.io/DressCode/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.16465v3&entry.124074799=Read"},
{"title": "Coherent 3D Portrait Video Reconstruction via Triplane Fusion", "author": "Shengze Wang and Xueting Li and Chao Liu and Matthew Chan and Michael Stengel and Josef Spjut and Henry Fuchs and Shalini De Mello and Koki Nagano", "abstract": "  Recent breakthroughs in single-image 3D portrait reconstruction have enabled\ntelepresence systems to stream 3D portrait videos from a single camera in\nreal-time, potentially democratizing telepresence. However, per-frame 3D\nreconstruction exhibits temporal inconsistency and forgets the user's\nappearance. On the other hand, self-reenactment methods can render coherent 3D\nportraits by driving a personalized 3D prior, but fail to faithfully\nreconstruct the user's per-frame appearance (e.g., facial expressions and\nlighting). In this work, we recognize the need to maintain both coherent\nidentity and dynamic per-frame appearance to enable the best possible realism.\nTo this end, we propose a new fusion-based method that fuses a personalized 3D\nsubject prior with per-frame information, producing temporally stable 3D videos\nwith faithful reconstruction of the user's per-frame appearances. Trained only\nusing synthetic data produced by an expression-conditioned 3D GAN, our\nencoder-based method achieves both state-of-the-art 3D reconstruction accuracy\nand temporal consistency on in-studio and in-the-wild datasets.\n", "link": "http://arxiv.org/abs/2405.00794v1", "date": "2024-05-01", "relevancy": 2.9428, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6056}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5831}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.577}], "mailto": "mailto:daeR=997470421.yrtne&1v49700.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.stesatad02%dliw-eht-ni02%dna02%oiduts-ni02%no02%ycnetsisnoc02%laropmet02%dnaA0%ycarucca02%noitcurtsnocer02%D302%tra-eht-fo-etats02%htob02%seveihca02%dohtem02%desab-redocneA0%ruo02%C2%NAG02%D302%denoitidnoc-noisserpxe02%na02%yb02%decudorp02%atad02%citehtnys02%gnisuA0%ylno02%deniarT02%.secnaraeppa02%emarf-rep02%s72%resu02%eht02%fo02%noitcurtsnocer02%lufhtiaf02%htiwA0%soediv02%D302%elbats02%yllaropmet02%gnicudorp02%C2%noitamrofni02%emarf-rep02%htiw02%roirp02%tcejbusA0%D302%dezilanosrep02%a02%sesuf02%taht02%dohtem02%desab-noisuf02%wen02%a02%esoporp02%ew02%C2%dne02%siht02%oTA0%.msilaer02%elbissop02%tseb02%eht02%elbane02%ot02%ecnaraeppa02%emarf-rep02%cimanyd02%dna02%ytitnediA0%tnerehoc02%htob02%niatniam02%ot02%deen02%eht02%ezingocer02%ew02%C2%krow02%siht02%nI02%.92%gnithgilA0%dna02%snoisserpxe02%laicaf02%C2%.g.e82%02%ecnaraeppa02%emarf-rep02%s72%resu02%eht02%tcurtsnocerA0%yllufhtiaf02%ot02%liaf02%tub02%C2%roirp02%D302%dezilanosrep02%a02%gnivird02%yb02%stiartropA0%D302%tnerehoc02%redner02%nac02%sdohtem02%tnemtcaneer-fles02%C2%dnah02%rehto02%eht02%nO02%.ecnaraeppaA0%s72%resu02%eht02%stegrof02%dna02%ycnetsisnocni02%laropmet02%stibihxe02%noitcurtsnocerA0%D302%emarf-rep02%C2%revewoH02%.ecneserpelet02%gnizitarcomed02%yllaitnetop02%C2%emit-laerA0%ni02%aremac02%elgnis02%a02%morf02%soediv02%tiartrop02%D302%maerts02%ot02%smetsys02%ecneserpeletA0%delbane02%evah02%noitcurtsnocer02%tiartrop02%D302%egami-elgnis02%ni02%shguorhtkaerb02%tneceR02%02%=3328342921.yrtne&onagaN02%ikoK02%dna02%olleM02%eD02%inilahS02%dna02%shcuF02%yrneH02%dna02%tujpS02%fesoJ02%dna02%legnetS02%leahciM02%dna02%nahC02%wehttaM02%dna02%uiL02%oahC02%dna02%iL02%gniteuX02%dna02%gnaW02%ezgnehS=526535609.yrtne&noisuF02%enalpirT02%aiv02%noitcurtsnoceR02%oediV02%tiartroP02%D302%tnerehoC=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Coherent%203D%20Portrait%20Video%20Reconstruction%20via%20Triplane%20Fusion&body=Title%3A%20Coherent%203D%20Portrait%20Video%20Reconstruction%20via%20Triplane%20Fusion%0AAuthor%3A%20Shengze%20Wang%20and%20Xueting%20Li%20and%20Chao%20Liu%20and%20Matthew%20Chan%20and%20Michael%20Stengel%20and%20Josef%20Spjut%20and%20Henry%20Fuchs%20and%20Shalini%20De%20Mello%20and%20Koki%20Nagano%0AAbstract%3A%20%20%20Recent%20breakthroughs%20in%20single-image%203D%20portrait%20reconstruction%20have%20enabled%0Atelepresence%20systems%20to%20stream%203D%20portrait%20videos%20from%20a%20single%20camera%20in%0Areal-time%2C%20potentially%20democratizing%20telepresence.%20However%2C%20per-frame%203D%0Areconstruction%20exhibits%20temporal%20inconsistency%20and%20forgets%20the%20user%27s%0Aappearance.%20On%20the%20other%20hand%2C%20self-reenactment%20methods%20can%20render%20coherent%203D%0Aportraits%20by%20driving%20a%20personalized%203D%20prior%2C%20but%20fail%20to%20faithfully%0Areconstruct%20the%20user%27s%20per-frame%20appearance%20%28e.g.%2C%20facial%20expressions%20and%0Alighting%29.%20In%20this%20work%2C%20we%20recognize%20the%20need%20to%20maintain%20both%20coherent%0Aidentity%20and%20dynamic%20per-frame%20appearance%20to%20enable%20the%20best%20possible%20realism.%0ATo%20this%20end%2C%20we%20propose%20a%20new%20fusion-based%20method%20that%20fuses%20a%20personalized%203D%0Asubject%20prior%20with%20per-frame%20information%2C%20producing%20temporally%20stable%203D%20videos%0Awith%20faithful%20reconstruction%20of%20the%20user%27s%20per-frame%20appearances.%20Trained%20only%0Ausing%20synthetic%20data%20produced%20by%20an%20expression-conditioned%203D%20GAN%2C%20our%0Aencoder-based%20method%20achieves%20both%20state-of-the-art%203D%20reconstruction%20accuracy%0Aand%20temporal%20consistency%20on%20in-studio%20and%20in-the-wild%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00794v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coherent%203D%20Portrait%20Video%20Reconstruction%20via%20Triplane%20Fusion&entry.906535625=Shengze%20Wang%20and%20Xueting%20Li%20and%20Chao%20Liu%20and%20Matthew%20Chan%20and%20Michael%20Stengel%20and%20Josef%20Spjut%20and%20Henry%20Fuchs%20and%20Shalini%20De%20Mello%20and%20Koki%20Nagano&entry.1292438233=%20%20Recent%20breakthroughs%20in%20single-image%203D%20portrait%20reconstruction%20have%20enabled%0Atelepresence%20systems%20to%20stream%203D%20portrait%20videos%20from%20a%20single%20camera%20in%0Areal-time%2C%20potentially%20democratizing%20telepresence.%20However%2C%20per-frame%203D%0Areconstruction%20exhibits%20temporal%20inconsistency%20and%20forgets%20the%20user%27s%0Aappearance.%20On%20the%20other%20hand%2C%20self-reenactment%20methods%20can%20render%20coherent%203D%0Aportraits%20by%20driving%20a%20personalized%203D%20prior%2C%20but%20fail%20to%20faithfully%0Areconstruct%20the%20user%27s%20per-frame%20appearance%20%28e.g.%2C%20facial%20expressions%20and%0Alighting%29.%20In%20this%20work%2C%20we%20recognize%20the%20need%20to%20maintain%20both%20coherent%0Aidentity%20and%20dynamic%20per-frame%20appearance%20to%20enable%20the%20best%20possible%20realism.%0ATo%20this%20end%2C%20we%20propose%20a%20new%20fusion-based%20method%20that%20fuses%20a%20personalized%203D%0Asubject%20prior%20with%20per-frame%20information%2C%20producing%20temporally%20stable%203D%20videos%0Awith%20faithful%20reconstruction%20of%20the%20user%27s%20per-frame%20appearances.%20Trained%20only%0Ausing%20synthetic%20data%20produced%20by%20an%20expression-conditioned%203D%20GAN%2C%20our%0Aencoder-based%20method%20achieves%20both%20state-of-the-art%203D%20reconstruction%20accuracy%0Aand%20temporal%20consistency%20on%20in-studio%20and%20in-the-wild%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00794v1&entry.124074799=Read"},
{"title": "Addressing Diverging Training Costs using Local Restoration for Precise\n  Bird's Eye View Map Construction", "author": "Minsu Kim and Giseop Kim and Sunwook Choi", "abstract": "  Recent advancements in Bird's Eye View (BEV) fusion for map construction have\ndemonstrated remarkable mapping of urban environments. However, their deep and\nbulky architecture incurs substantial amounts of backpropagation memory and\ncomputing latency. Consequently, the problem poses an unavoidable bottleneck in\nconstructing high-resolution (HR) BEV maps, as their large-sized features cause\nsignificant increases in costs including GPU memory consumption and computing\nlatency, named diverging training costs issue. Affected by the problem, most\nexisting methods adopt low-resolution (LR) BEV and struggle to estimate the\nprecise locations of urban scene components like road lanes, and sidewalks. As\nthe imprecision leads to risky self-driving, the diverging training costs issue\nhas to be resolved. In this paper, we address the issue with our novel Trumpet\nNeural Network (TNN) mechanism. The framework utilizes LR BEV space and outputs\nan up-sampled semantic BEV map to create a memory-efficient pipeline. To this\nend, we introduce Local Restoration of BEV representation. Specifically, the\nup-sampled BEV representation has severely aliased, blocky signals, and thick\nsemantic labels. Our proposed Local Restoration restores the signals and thins\n(or narrows down) the width of the labels. Our extensive experiments show that\nthe TNN mechanism provides a plug-and-play memory-efficient pipeline, thereby\nenabling the effective estimation of real-sized (or precise) semantic labels\nfor BEV map construction.\n", "link": "http://arxiv.org/abs/2405.01016v1", "date": "2024-05-02", "relevancy": 2.8583, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5891}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5638}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5621}], "mailto": "mailto:daeR=997470421.yrtne&1v61010.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.noitcurtsnoc02%pam02%VEB02%rofA0%slebal02%citnames02%92%esicerp02%ro82%02%dezis-laer02%fo02%noitamitse02%evitceffe02%eht02%gnilbaneA0%ybereht02%C2%enilepip02%tneiciffe-yromem02%yalp-dna-gulp02%a02%sedivorp02%msinahcem02%NNT02%ehtA0%taht02%wohs02%stnemirepxe02%evisnetxe02%ruO02%.slebal02%eht02%fo02%htdiw02%eht02%92%nwod02%sworran02%ro82%A0%sniht02%dna02%slangis02%eht02%serotser02%noitarotseR02%lacoL02%desoporp02%ruO02%.slebal02%citnamesA0%kciht02%dna02%C2%slangis02%ykcolb02%C2%desaila02%ylereves02%sah02%noitatneserper02%VEB02%delpmas-puA0%eht02%C2%yllacificepS02%.noitatneserper02%VEB02%fo02%noitarotseR02%lacoL02%ecudortni02%ew02%C2%dneA0%siht02%oT02%.enilepip02%tneiciffe-yromem02%a02%etaerc02%ot02%pam02%VEB02%citnames02%delpmas-pu02%naA0%stuptuo02%dna02%ecaps02%VEB02%RL02%sezilitu02%krowemarf02%ehT02%.msinahcem02%92%NNT82%02%krowteN02%larueNA0%tepmurT02%levon02%ruo02%htiw02%eussi02%eht02%sserdda02%ew02%C2%repap02%siht02%nI02%.devloser02%eb02%ot02%sahA0%eussi02%stsoc02%gniniart02%gnigrevid02%eht02%C2%gnivird-fles02%yksir02%ot02%sdael02%noisicerpmi02%ehtA0%sA02%.sklawedis02%dna02%C2%senal02%daor02%ekil02%stnenopmoc02%enecs02%nabru02%fo02%snoitacol02%esicerpA0%eht02%etamitse02%ot02%elggurts02%dna02%VEB02%92%RL82%02%noituloser-wol02%tpoda02%sdohtem02%gnitsixeA0%tsom02%C2%melborp02%eht02%yb02%detceffA02%.eussi02%stsoc02%gniniart02%gnigrevid02%deman02%C2%ycnetalA0%gnitupmoc02%dna02%noitpmusnoc02%yromem02%UPG02%gnidulcni02%stsoc02%ni02%sesaercni02%tnacifingisA0%esuac02%serutaef02%dezis-egral02%rieht02%sa02%C2%spam02%VEB02%92%RH82%02%noituloser-hgih02%gnitcurtsnocA0%ni02%kcenelttob02%elbadiovanu02%na02%sesop02%melborp02%eht02%C2%yltneuqesnoC02%.ycnetal02%gnitupmocA0%dna02%yromem02%noitagaporpkcab02%fo02%stnuoma02%laitnatsbus02%srucni02%erutcetihcra02%yklubA0%dna02%peed02%rieht02%C2%revewoH02%.stnemnorivne02%nabru02%fo02%gnippam02%elbakramer02%detartsnomedA0%evah02%noitcurtsnoc02%pam02%rof02%noisuf02%92%VEB82%02%weiV02%eyE02%s72%driB02%ni02%stnemecnavda02%tneceR02%02%=3328342921.yrtne&iohC02%koownuS02%dna02%miK02%poesiG02%dna02%miK02%usniM=526535609.yrtne&noitcurtsnoC02%paM02%weiV02%eyE02%s72%driB02%02%A0%esicerP02%rof02%noitarotseR02%lacoL02%gnisu02%stsoC02%gniniarT02%gnigreviD02%gnisserddA=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Addressing%20Diverging%20Training%20Costs%20using%20Local%20Restoration%20for%20Precise%0A%20%20Bird%27s%20Eye%20View%20Map%20Construction&body=Title%3A%20Addressing%20Diverging%20Training%20Costs%20using%20Local%20Restoration%20for%20Precise%0A%20%20Bird%27s%20Eye%20View%20Map%20Construction%0AAuthor%3A%20Minsu%20Kim%20and%20Giseop%20Kim%20and%20Sunwook%20Choi%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Bird%27s%20Eye%20View%20%28BEV%29%20fusion%20for%20map%20construction%20have%0Ademonstrated%20remarkable%20mapping%20of%20urban%20environments.%20However%2C%20their%20deep%20and%0Abulky%20architecture%20incurs%20substantial%20amounts%20of%20backpropagation%20memory%20and%0Acomputing%20latency.%20Consequently%2C%20the%20problem%20poses%20an%20unavoidable%20bottleneck%20in%0Aconstructing%20high-resolution%20%28HR%29%20BEV%20maps%2C%20as%20their%20large-sized%20features%20cause%0Asignificant%20increases%20in%20costs%20including%20GPU%20memory%20consumption%20and%20computing%0Alatency%2C%20named%20diverging%20training%20costs%20issue.%20Affected%20by%20the%20problem%2C%20most%0Aexisting%20methods%20adopt%20low-resolution%20%28LR%29%20BEV%20and%20struggle%20to%20estimate%20the%0Aprecise%20locations%20of%20urban%20scene%20components%20like%20road%20lanes%2C%20and%20sidewalks.%20As%0Athe%20imprecision%20leads%20to%20risky%20self-driving%2C%20the%20diverging%20training%20costs%20issue%0Ahas%20to%20be%20resolved.%20In%20this%20paper%2C%20we%20address%20the%20issue%20with%20our%20novel%20Trumpet%0ANeural%20Network%20%28TNN%29%20mechanism.%20The%20framework%20utilizes%20LR%20BEV%20space%20and%20outputs%0Aan%20up-sampled%20semantic%20BEV%20map%20to%20create%20a%20memory-efficient%20pipeline.%20To%20this%0Aend%2C%20we%20introduce%20Local%20Restoration%20of%20BEV%20representation.%20Specifically%2C%20the%0Aup-sampled%20BEV%20representation%20has%20severely%20aliased%2C%20blocky%20signals%2C%20and%20thick%0Asemantic%20labels.%20Our%20proposed%20Local%20Restoration%20restores%20the%20signals%20and%20thins%0A%28or%20narrows%20down%29%20the%20width%20of%20the%20labels.%20Our%20extensive%20experiments%20show%20that%0Athe%20TNN%20mechanism%20provides%20a%20plug-and-play%20memory-efficient%20pipeline%2C%20thereby%0Aenabling%20the%20effective%20estimation%20of%20real-sized%20%28or%20precise%29%20semantic%20labels%0Afor%20BEV%20map%20construction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01016v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Addressing%20Diverging%20Training%20Costs%20using%20Local%20Restoration%20for%20Precise%0A%20%20Bird%27s%20Eye%20View%20Map%20Construction&entry.906535625=Minsu%20Kim%20and%20Giseop%20Kim%20and%20Sunwook%20Choi&entry.1292438233=%20%20Recent%20advancements%20in%20Bird%27s%20Eye%20View%20%28BEV%29%20fusion%20for%20map%20construction%20have%0Ademonstrated%20remarkable%20mapping%20of%20urban%20environments.%20However%2C%20their%20deep%20and%0Abulky%20architecture%20incurs%20substantial%20amounts%20of%20backpropagation%20memory%20and%0Acomputing%20latency.%20Consequently%2C%20the%20problem%20poses%20an%20unavoidable%20bottleneck%20in%0Aconstructing%20high-resolution%20%28HR%29%20BEV%20maps%2C%20as%20their%20large-sized%20features%20cause%0Asignificant%20increases%20in%20costs%20including%20GPU%20memory%20consumption%20and%20computing%0Alatency%2C%20named%20diverging%20training%20costs%20issue.%20Affected%20by%20the%20problem%2C%20most%0Aexisting%20methods%20adopt%20low-resolution%20%28LR%29%20BEV%20and%20struggle%20to%20estimate%20the%0Aprecise%20locations%20of%20urban%20scene%20components%20like%20road%20lanes%2C%20and%20sidewalks.%20As%0Athe%20imprecision%20leads%20to%20risky%20self-driving%2C%20the%20diverging%20training%20costs%20issue%0Ahas%20to%20be%20resolved.%20In%20this%20paper%2C%20we%20address%20the%20issue%20with%20our%20novel%20Trumpet%0ANeural%20Network%20%28TNN%29%20mechanism.%20The%20framework%20utilizes%20LR%20BEV%20space%20and%20outputs%0Aan%20up-sampled%20semantic%20BEV%20map%20to%20create%20a%20memory-efficient%20pipeline.%20To%20this%0Aend%2C%20we%20introduce%20Local%20Restoration%20of%20BEV%20representation.%20Specifically%2C%20the%0Aup-sampled%20BEV%20representation%20has%20severely%20aliased%2C%20blocky%20signals%2C%20and%20thick%0Asemantic%20labels.%20Our%20proposed%20Local%20Restoration%20restores%20the%20signals%20and%20thins%0A%28or%20narrows%20down%29%20the%20width%20of%20the%20labels.%20Our%20extensive%20experiments%20show%20that%0Athe%20TNN%20mechanism%20provides%20a%20plug-and-play%20memory-efficient%20pipeline%2C%20thereby%0Aenabling%20the%20effective%20estimation%20of%20real-sized%20%28or%20precise%29%20semantic%20labels%0Afor%20BEV%20map%20construction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01016v1&entry.124074799=Read"},
{"title": "Generative manufacturing systems using diffusion models and ChatGPT", "author": "Xingyu Li and Fei Tao and Wei Ye and Aydin Nassehi and John W. Sutherland", "abstract": "  In this study, we introduce Generative Manufacturing Systems (GMS) as a novel\napproach to effectively manage and coordinate autonomous manufacturing assets,\nthereby enhancing their responsiveness and flexibility to address a wide array\nof production objectives and human preferences. Deviating from traditional\nexplicit modeling, GMS employs generative AI, including diffusion models and\nChatGPT, for implicit learning from envisioned futures, marking a shift from a\nmodel-optimum to a training-sampling decision-making. Through the integration\nof generative AI, GMS enables complex decision-making through interactive\ndialogue with humans, allowing manufacturing assets to generate multiple\nhigh-quality global decisions that can be iteratively refined based on human\nfeedback. Empirical findings showcase GMS's substantial improvement in system\nresilience and responsiveness to uncertainties, with decision times reduced\nfrom seconds to milliseconds. The study underscores the inherent creativity and\ndiversity in the generated solutions, facilitating human-centric\ndecision-making through seamless and continuous human-machine interactions.\n", "link": "http://arxiv.org/abs/2405.00958v1", "date": "2024-05-02", "relevancy": 2.8365, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5789}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5653}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5577}], "mailto": "mailto:daeR=997470421.yrtne&1v85900.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.snoitcaretni02%enihcam-namuh02%suounitnoc02%dna02%sselmaes02%hguorht02%gnikam-noisicedA0%cirtnec-namuh02%gnitatilicaf02%C2%snoitulos02%detareneg02%eht02%ni02%ytisrevidA0%dna02%ytivitaerc02%tnerehni02%eht02%serocsrednu02%yduts02%ehT02%.sdnocesillim02%ot02%sdnoces02%morfA0%decuder02%semit02%noisiced02%htiw02%C2%seitniatrecnu02%ot02%ssenevisnopser02%dna02%ecneiliserA0%metsys02%ni02%tnemevorpmi02%laitnatsbus02%s72%SMG02%esacwohs02%sgnidnif02%laciripmE02%.kcabdeefA0%namuh02%no02%desab02%denifer02%ylevitareti02%eb02%nac02%taht02%snoisiced02%labolg02%ytilauq-hgihA0%elpitlum02%etareneg02%ot02%stessa02%gnirutcafunam02%gniwolla02%C2%snamuh02%htiw02%eugolaidA0%evitcaretni02%hguorht02%gnikam-noisiced02%xelpmoc02%selbane02%SMG02%C2%IA02%evitareneg02%foA0%noitargetni02%eht02%hguorhT02%.gnikam-noisiced02%gnilpmas-gniniart02%a02%ot02%mumitpo-ledomA0%a02%morf02%tfihs02%a02%gnikram02%C2%serutuf02%denoisivne02%morf02%gninrael02%ticilpmi02%rof02%C2%TPGtahCA0%dna02%sledom02%noisuffid02%gnidulcni02%C2%IA02%evitareneg02%syolpme02%SMG02%C2%gniledom02%ticilpxeA0%lanoitidart02%morf02%gnitaiveD02%.secnereferp02%namuh02%dna02%sevitcejbo02%noitcudorp02%foA0%yarra02%ediw02%a02%sserdda02%ot02%ytilibixelf02%dna02%ssenevisnopser02%rieht02%gnicnahne02%yberehtA0%C2%stessa02%gnirutcafunam02%suomonotua02%etanidrooc02%dna02%eganam02%ylevitceffe02%ot02%hcaorppaA0%levon02%a02%sa02%92%SMG82%02%smetsyS02%gnirutcafunaM02%evitareneG02%ecudortni02%ew02%C2%yduts02%siht02%nI02%02%=3328342921.yrtne&dnalrehtuS02%.W02%nhoJ02%dna02%ihessaN02%nidyA02%dna02%eY02%ieW02%dna02%oaT02%ieF02%dna02%iL02%uygniX=526535609.yrtne&TPGtahC02%dna02%sledom02%noisuffid02%gnisu02%smetsys02%gnirutcafunam02%evitareneG=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Generative%20manufacturing%20systems%20using%20diffusion%20models%20and%20ChatGPT&body=Title%3A%20Generative%20manufacturing%20systems%20using%20diffusion%20models%20and%20ChatGPT%0AAuthor%3A%20Xingyu%20Li%20and%20Fei%20Tao%20and%20Wei%20Ye%20and%20Aydin%20Nassehi%20and%20John%20W.%20Sutherland%0AAbstract%3A%20%20%20In%20this%20study%2C%20we%20introduce%20Generative%20Manufacturing%20Systems%20%28GMS%29%20as%20a%20novel%0Aapproach%20to%20effectively%20manage%20and%20coordinate%20autonomous%20manufacturing%20assets%2C%0Athereby%20enhancing%20their%20responsiveness%20and%20flexibility%20to%20address%20a%20wide%20array%0Aof%20production%20objectives%20and%20human%20preferences.%20Deviating%20from%20traditional%0Aexplicit%20modeling%2C%20GMS%20employs%20generative%20AI%2C%20including%20diffusion%20models%20and%0AChatGPT%2C%20for%20implicit%20learning%20from%20envisioned%20futures%2C%20marking%20a%20shift%20from%20a%0Amodel-optimum%20to%20a%20training-sampling%20decision-making.%20Through%20the%20integration%0Aof%20generative%20AI%2C%20GMS%20enables%20complex%20decision-making%20through%20interactive%0Adialogue%20with%20humans%2C%20allowing%20manufacturing%20assets%20to%20generate%20multiple%0Ahigh-quality%20global%20decisions%20that%20can%20be%20iteratively%20refined%20based%20on%20human%0Afeedback.%20Empirical%20findings%20showcase%20GMS%27s%20substantial%20improvement%20in%20system%0Aresilience%20and%20responsiveness%20to%20uncertainties%2C%20with%20decision%20times%20reduced%0Afrom%20seconds%20to%20milliseconds.%20The%20study%20underscores%20the%20inherent%20creativity%20and%0Adiversity%20in%20the%20generated%20solutions%2C%20facilitating%20human-centric%0Adecision-making%20through%20seamless%20and%20continuous%20human-machine%20interactions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00958v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20manufacturing%20systems%20using%20diffusion%20models%20and%20ChatGPT&entry.906535625=Xingyu%20Li%20and%20Fei%20Tao%20and%20Wei%20Ye%20and%20Aydin%20Nassehi%20and%20John%20W.%20Sutherland&entry.1292438233=%20%20In%20this%20study%2C%20we%20introduce%20Generative%20Manufacturing%20Systems%20%28GMS%29%20as%20a%20novel%0Aapproach%20to%20effectively%20manage%20and%20coordinate%20autonomous%20manufacturing%20assets%2C%0Athereby%20enhancing%20their%20responsiveness%20and%20flexibility%20to%20address%20a%20wide%20array%0Aof%20production%20objectives%20and%20human%20preferences.%20Deviating%20from%20traditional%0Aexplicit%20modeling%2C%20GMS%20employs%20generative%20AI%2C%20including%20diffusion%20models%20and%0AChatGPT%2C%20for%20implicit%20learning%20from%20envisioned%20futures%2C%20marking%20a%20shift%20from%20a%0Amodel-optimum%20to%20a%20training-sampling%20decision-making.%20Through%20the%20integration%0Aof%20generative%20AI%2C%20GMS%20enables%20complex%20decision-making%20through%20interactive%0Adialogue%20with%20humans%2C%20allowing%20manufacturing%20assets%20to%20generate%20multiple%0Ahigh-quality%20global%20decisions%20that%20can%20be%20iteratively%20refined%20based%20on%20human%0Afeedback.%20Empirical%20findings%20showcase%20GMS%27s%20substantial%20improvement%20in%20system%0Aresilience%20and%20responsiveness%20to%20uncertainties%2C%20with%20decision%20times%20reduced%0Afrom%20seconds%20to%20milliseconds.%20The%20study%20underscores%20the%20inherent%20creativity%20and%0Adiversity%20in%20the%20generated%20solutions%2C%20facilitating%20human-centric%0Adecision-making%20through%20seamless%20and%20continuous%20human-machine%20interactions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00958v1&entry.124074799=Read"},
{"title": "VideoGigaGAN: Towards Detail-rich Video Super-Resolution", "author": "Yiran Xu and Taesung Park and Richard Zhang and Yang Zhou and Eli Shechtman and Feng Liu and Jia-Bin Huang and Difan Liu", "abstract": "  Video super-resolution (VSR) approaches have shown impressive temporal\nconsistency in upsampled videos. However, these approaches tend to generate\nblurrier results than their image counterparts as they are limited in their\ngenerative capability. This raises a fundamental question: can we extend the\nsuccess of a generative image upsampler to the VSR task while preserving the\ntemporal consistency? We introduce VideoGigaGAN, a new generative VSR model\nthat can produce videos with high-frequency details and temporal consistency.\nVideoGigaGAN builds upon a large-scale image upsampler -- GigaGAN. Simply\ninflating GigaGAN to a video model by adding temporal modules produces severe\ntemporal flickering. We identify several key issues and propose techniques that\nsignificantly improve the temporal consistency of upsampled videos. Our\nexperiments show that, unlike previous VSR methods, VideoGigaGAN generates\ntemporally consistent videos with more fine-grained appearance details. We\nvalidate the effectiveness of VideoGigaGAN by comparing it with\nstate-of-the-art VSR models on public datasets and showcasing video results\nwith $8\\times$ super-resolution.\n", "link": "http://arxiv.org/abs/2404.12388v2", "date": "2024-05-01", "relevancy": 2.8262, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5852}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.559}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5516}], "mailto": "mailto:daeR=997470421.yrtne&2v88321.4042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.noituloser-repus02%42%semitC5%842%02%htiwA0%stluser02%oediv02%gnisacwohs02%dna02%stesatad02%cilbup02%no02%sledom02%RSV02%tra-eht-fo-etatsA0%htiw02%ti02%gnirapmoc02%yb02%NAGagiGoediV02%fo02%ssenevitceffe02%eht02%etadilavA0%eW02%.sliated02%ecnaraeppa02%deniarg-enif02%erom02%htiw02%soediv02%tnetsisnoc02%yllaropmetA0%setareneg02%NAGagiGoediV02%C2%sdohtem02%RSV02%suoiverp02%ekilnu02%C2%taht02%wohs02%stnemirepxeA0%ruO02%.soediv02%delpmaspu02%fo02%ycnetsisnoc02%laropmet02%eht02%evorpmi02%yltnacifingisA0%taht02%seuqinhcet02%esoporp02%dna02%seussi02%yek02%lareves02%yfitnedi02%eW02%.gnirekcilf02%laropmetA0%ereves02%secudorp02%seludom02%laropmet02%gnidda02%yb02%ledom02%oediv02%a02%ot02%NAGagiG02%gnitalfniA0%ylpmiS02%.NAGagiG02%--02%relpmaspu02%egami02%elacs-egral02%a02%nopu02%sdliub02%NAGagiGoediVA0%.ycnetsisnoc02%laropmet02%dna02%sliated02%ycneuqerf-hgih02%htiw02%soediv02%ecudorp02%nac02%tahtA0%ledom02%RSV02%evitareneg02%wen02%a02%C2%NAGagiGoediV02%ecudortni02%eW02%F3%ycnetsisnoc02%laropmetA0%eht02%gnivreserp02%elihw02%ksat02%RSV02%eht02%ot02%relpmaspu02%egami02%evitareneg02%a02%fo02%sseccusA0%eht02%dnetxe02%ew02%nac02%A3%noitseuq02%latnemadnuf02%a02%sesiar02%sihT02%.ytilibapac02%evitarenegA0%rieht02%ni02%detimil02%era02%yeht02%sa02%strapretnuoc02%egami02%rieht02%naht02%stluser02%reirrulbA0%etareneg02%ot02%dnet02%sehcaorppa02%eseht02%C2%revewoH02%.soediv02%delpmaspu02%ni02%ycnetsisnocA0%laropmet02%evisserpmi02%nwohs02%evah02%sehcaorppa02%92%RSV82%02%noituloser-repus02%oediV02%02%=3328342921.yrtne&uiL02%nafiD02%dna02%gnauH02%niB-aiJ02%dna02%uiL02%gneF02%dna02%namthcehS02%ilE02%dna02%uohZ02%gnaY02%dna02%gnahZ02%drahciR02%dna02%kraP02%gnuseaT02%dna02%uX02%nariY=526535609.yrtne&noituloseR-repuS02%oediV02%hcir-liateD02%sdrawoT02%A3%NAGagiGoediV=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20VideoGigaGAN%3A%20Towards%20Detail-rich%20Video%20Super-Resolution&body=Title%3A%20VideoGigaGAN%3A%20Towards%20Detail-rich%20Video%20Super-Resolution%0AAuthor%3A%20Yiran%20Xu%20and%20Taesung%20Park%20and%20Richard%20Zhang%20and%20Yang%20Zhou%20and%20Eli%20Shechtman%20and%20Feng%20Liu%20and%20Jia-Bin%20Huang%20and%20Difan%20Liu%0AAbstract%3A%20%20%20Video%20super-resolution%20%28VSR%29%20approaches%20have%20shown%20impressive%20temporal%0Aconsistency%20in%20upsampled%20videos.%20However%2C%20these%20approaches%20tend%20to%20generate%0Ablurrier%20results%20than%20their%20image%20counterparts%20as%20they%20are%20limited%20in%20their%0Agenerative%20capability.%20This%20raises%20a%20fundamental%20question%3A%20can%20we%20extend%20the%0Asuccess%20of%20a%20generative%20image%20upsampler%20to%20the%20VSR%20task%20while%20preserving%20the%0Atemporal%20consistency%3F%20We%20introduce%20VideoGigaGAN%2C%20a%20new%20generative%20VSR%20model%0Athat%20can%20produce%20videos%20with%20high-frequency%20details%20and%20temporal%20consistency.%0AVideoGigaGAN%20builds%20upon%20a%20large-scale%20image%20upsampler%20--%20GigaGAN.%20Simply%0Ainflating%20GigaGAN%20to%20a%20video%20model%20by%20adding%20temporal%20modules%20produces%20severe%0Atemporal%20flickering.%20We%20identify%20several%20key%20issues%20and%20propose%20techniques%20that%0Asignificantly%20improve%20the%20temporal%20consistency%20of%20upsampled%20videos.%20Our%0Aexperiments%20show%20that%2C%20unlike%20previous%20VSR%20methods%2C%20VideoGigaGAN%20generates%0Atemporally%20consistent%20videos%20with%20more%20fine-grained%20appearance%20details.%20We%0Avalidate%20the%20effectiveness%20of%20VideoGigaGAN%20by%20comparing%20it%20with%0Astate-of-the-art%20VSR%20models%20on%20public%20datasets%20and%20showcasing%20video%20results%0Awith%20%248%5Ctimes%24%20super-resolution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12388v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoGigaGAN%3A%20Towards%20Detail-rich%20Video%20Super-Resolution&entry.906535625=Yiran%20Xu%20and%20Taesung%20Park%20and%20Richard%20Zhang%20and%20Yang%20Zhou%20and%20Eli%20Shechtman%20and%20Feng%20Liu%20and%20Jia-Bin%20Huang%20and%20Difan%20Liu&entry.1292438233=%20%20Video%20super-resolution%20%28VSR%29%20approaches%20have%20shown%20impressive%20temporal%0Aconsistency%20in%20upsampled%20videos.%20However%2C%20these%20approaches%20tend%20to%20generate%0Ablurrier%20results%20than%20their%20image%20counterparts%20as%20they%20are%20limited%20in%20their%0Agenerative%20capability.%20This%20raises%20a%20fundamental%20question%3A%20can%20we%20extend%20the%0Asuccess%20of%20a%20generative%20image%20upsampler%20to%20the%20VSR%20task%20while%20preserving%20the%0Atemporal%20consistency%3F%20We%20introduce%20VideoGigaGAN%2C%20a%20new%20generative%20VSR%20model%0Athat%20can%20produce%20videos%20with%20high-frequency%20details%20and%20temporal%20consistency.%0AVideoGigaGAN%20builds%20upon%20a%20large-scale%20image%20upsampler%20--%20GigaGAN.%20Simply%0Ainflating%20GigaGAN%20to%20a%20video%20model%20by%20adding%20temporal%20modules%20produces%20severe%0Atemporal%20flickering.%20We%20identify%20several%20key%20issues%20and%20propose%20techniques%20that%0Asignificantly%20improve%20the%20temporal%20consistency%20of%20upsampled%20videos.%20Our%0Aexperiments%20show%20that%2C%20unlike%20previous%20VSR%20methods%2C%20VideoGigaGAN%20generates%0Atemporally%20consistent%20videos%20with%20more%20fine-grained%20appearance%20details.%20We%0Avalidate%20the%20effectiveness%20of%20VideoGigaGAN%20by%20comparing%20it%20with%0Astate-of-the-art%20VSR%20models%20on%20public%20datasets%20and%20showcasing%20video%20results%0Awith%20%248%5Ctimes%24%20super-resolution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12388v2&entry.124074799=Read"},
{"title": "Self-Supervised Learning for Interventional Image Analytics: Towards\n  Robust Device Trackers", "author": "Saahil Islam and Venkatesh N. Murthy and Dominik Neumann and Badhan Kumar Das and Puneet Sharma and Andreas Maier and Dorin Comaniciu and Florin C. Ghesu", "abstract": "  An accurate detection and tracking of devices such as guiding catheters in\nlive X-ray image acquisitions is an essential prerequisite for endovascular\ncardiac interventions. This information is leveraged for procedural guidance,\ne.g., directing stent placements. To ensure procedural safety and efficacy,\nthere is a need for high robustness no failures during tracking. To achieve\nthat, one needs to efficiently tackle challenges, such as: device obscuration\nby contrast agent or other external devices or wires, changes in field-of-view\nor acquisition angle, as well as the continuous movement due to cardiac and\nrespiratory motion. To overcome the aforementioned challenges, we propose a\nnovel approach to learn spatio-temporal features from a very large data cohort\nof over 16 million interventional X-ray frames using self-supervision for image\nsequence data. Our approach is based on a masked image modeling technique that\nleverages frame interpolation based reconstruction to learn fine inter-frame\ntemporal correspondences. The features encoded in the resulting model are\nfine-tuned downstream. Our approach achieves state-of-the-art performance and\nin particular robustness compared to ultra optimized reference solutions (that\nuse multi-stage feature fusion, multi-task and flow regularization). The\nexperiments show that our method achieves 66.31% reduction in maximum tracking\nerror against reference solutions (23.20% when flow regularization is used);\nachieving a success score of 97.95% at a 3x faster inference speed of 42\nframes-per-second (on GPU). The results encourage the use of our approach in\nvarious other tasks within interventional image analytics that require\neffective understanding of spatio-temporal semantics.\n", "link": "http://arxiv.org/abs/2405.01156v1", "date": "2024-05-02", "relevancy": 2.8214, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5792}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5697}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.544}], "mailto": "mailto:daeR=997470421.yrtne&1v65110.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.scitnames02%laropmet-oitaps02%fo02%gnidnatsrednu02%evitceffeA0%eriuqer02%taht02%scitylana02%egami02%lanoitnevretni02%nihtiw02%sksat02%rehto02%suoiravA0%ni02%hcaorppa02%ruo02%fo02%esu02%eht02%egaruocne02%stluser02%ehT02%.92%UPG02%no82%02%dnoces-rep-semarfA0%2402%fo02%deeps02%ecnerefni02%retsaf02%x302%a02%ta02%52%59.7902%fo02%erocs02%sseccus02%a02%gniveihcaA0%B3%92%desu02%si02%noitaziraluger02%wolf02%nehw02%52%02.3282%02%snoitulos02%ecnerefer02%tsniaga02%rorreA0%gnikcart02%mumixam02%ni02%noitcuder02%52%13.6602%seveihca02%dohtem02%ruo02%taht02%wohs02%stnemirepxeA0%ehT02%.92%noitaziraluger02%wolf02%dna02%ksat-itlum02%C2%noisuf02%erutaef02%egats-itlum02%esuA0%taht82%02%snoitulos02%ecnerefer02%dezimitpo02%artlu02%ot02%derapmoc02%ssentsubor02%ralucitrap02%niA0%dna02%ecnamrofrep02%tra-eht-fo-etats02%seveihca02%hcaorppa02%ruO02%.maertsnwod02%denut-enifA0%era02%ledom02%gnitluser02%eht02%ni02%dedocne02%serutaef02%ehT02%.secnednopserroc02%laropmetA0%emarf-retni02%enif02%nrael02%ot02%noitcurtsnocer02%desab02%noitalopretni02%emarf02%segarevelA0%taht02%euqinhcet02%gniledom02%egami02%deksam02%a02%no02%desab02%si02%hcaorppa02%ruO02%.atad02%ecneuqesA0%egami02%rof02%noisivrepus-fles02%gnisu02%semarf02%yar-X02%lanoitnevretni02%noillim02%6102%revo02%foA0%trohoc02%atad02%egral02%yrev02%a02%morf02%serutaef02%laropmet-oitaps02%nrael02%ot02%hcaorppa02%levonA0%a02%esoporp02%ew02%C2%segnellahc02%denoitnemerofa02%eht02%emocrevo02%oT02%.noitom02%yrotaripserA0%dna02%caidrac02%ot02%eud02%tnemevom02%suounitnoc02%eht02%sa02%llew02%sa02%C2%elgna02%noitisiuqca02%roA0%weiv-fo-dleif02%ni02%segnahc02%C2%seriw02%ro02%secived02%lanretxe02%rehto02%ro02%tnega02%tsartnoc02%ybA0%noitarucsbo02%ecived02%A3%sa02%hcus02%C2%segnellahc02%elkcat02%yltneiciffe02%ot02%sdeen02%eno02%C2%tahtA0%eveihca02%oT02%.gnikcart02%gnirud02%seruliaf02%on02%ssentsubor02%hgih02%rof02%deen02%a02%si02%erehtA0%C2%ycaciffe02%dna02%ytefas02%larudecorp02%erusne02%oT02%.stnemecalp02%tnets02%gnitcerid02%C2%.g.eA0%C2%ecnadiug02%larudecorp02%rof02%degarevel02%si02%noitamrofni02%sihT02%.snoitnevretni02%caidracA0%ralucsavodne02%rof02%etisiuqererp02%laitnesse02%na02%si02%snoitisiuqca02%egami02%yar-X02%evilA0%ni02%sretehtac02%gnidiug02%sa02%hcus02%secived02%fo02%gnikcart02%dna02%noitceted02%etarucca02%nA02%02%=3328342921.yrtne&usehG02%.C02%nirolF02%dna02%uicinamoC02%niroD02%dna02%reiaM02%saerdnA02%dna02%amrahS02%teenuP02%dna02%saD02%ramuK02%nahdaB02%dna02%nnamueN02%kinimoD02%dna02%yhtruM02%.N02%hsetakneV02%dna02%malsI02%lihaaS=526535609.yrtne&srekcarT02%eciveD02%tsuboR02%02%A0%sdrawoT02%A3%scitylanA02%egamI02%lanoitnevretnI02%rof02%gninraeL02%desivrepuS-fleS=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Self-Supervised%20Learning%20for%20Interventional%20Image%20Analytics%3A%20Towards%0A%20%20Robust%20Device%20Trackers&body=Title%3A%20Self-Supervised%20Learning%20for%20Interventional%20Image%20Analytics%3A%20Towards%0A%20%20Robust%20Device%20Trackers%0AAuthor%3A%20Saahil%20Islam%20and%20Venkatesh%20N.%20Murthy%20and%20Dominik%20Neumann%20and%20Badhan%20Kumar%20Das%20and%20Puneet%20Sharma%20and%20Andreas%20Maier%20and%20Dorin%20Comaniciu%20and%20Florin%20C.%20Ghesu%0AAbstract%3A%20%20%20An%20accurate%20detection%20and%20tracking%20of%20devices%20such%20as%20guiding%20catheters%20in%0Alive%20X-ray%20image%20acquisitions%20is%20an%20essential%20prerequisite%20for%20endovascular%0Acardiac%20interventions.%20This%20information%20is%20leveraged%20for%20procedural%20guidance%2C%0Ae.g.%2C%20directing%20stent%20placements.%20To%20ensure%20procedural%20safety%20and%20efficacy%2C%0Athere%20is%20a%20need%20for%20high%20robustness%20no%20failures%20during%20tracking.%20To%20achieve%0Athat%2C%20one%20needs%20to%20efficiently%20tackle%20challenges%2C%20such%20as%3A%20device%20obscuration%0Aby%20contrast%20agent%20or%20other%20external%20devices%20or%20wires%2C%20changes%20in%20field-of-view%0Aor%20acquisition%20angle%2C%20as%20well%20as%20the%20continuous%20movement%20due%20to%20cardiac%20and%0Arespiratory%20motion.%20To%20overcome%20the%20aforementioned%20challenges%2C%20we%20propose%20a%0Anovel%20approach%20to%20learn%20spatio-temporal%20features%20from%20a%20very%20large%20data%20cohort%0Aof%20over%2016%20million%20interventional%20X-ray%20frames%20using%20self-supervision%20for%20image%0Asequence%20data.%20Our%20approach%20is%20based%20on%20a%20masked%20image%20modeling%20technique%20that%0Aleverages%20frame%20interpolation%20based%20reconstruction%20to%20learn%20fine%20inter-frame%0Atemporal%20correspondences.%20The%20features%20encoded%20in%20the%20resulting%20model%20are%0Afine-tuned%20downstream.%20Our%20approach%20achieves%20state-of-the-art%20performance%20and%0Ain%20particular%20robustness%20compared%20to%20ultra%20optimized%20reference%20solutions%20%28that%0Ause%20multi-stage%20feature%20fusion%2C%20multi-task%20and%20flow%20regularization%29.%20The%0Aexperiments%20show%20that%20our%20method%20achieves%2066.31%25%20reduction%20in%20maximum%20tracking%0Aerror%20against%20reference%20solutions%20%2823.20%25%20when%20flow%20regularization%20is%20used%29%3B%0Aachieving%20a%20success%20score%20of%2097.95%25%20at%20a%203x%20faster%20inference%20speed%20of%2042%0Aframes-per-second%20%28on%20GPU%29.%20The%20results%20encourage%20the%20use%20of%20our%20approach%20in%0Avarious%20other%20tasks%20within%20interventional%20image%20analytics%20that%20require%0Aeffective%20understanding%20of%20spatio-temporal%20semantics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01156v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Supervised%20Learning%20for%20Interventional%20Image%20Analytics%3A%20Towards%0A%20%20Robust%20Device%20Trackers&entry.906535625=Saahil%20Islam%20and%20Venkatesh%20N.%20Murthy%20and%20Dominik%20Neumann%20and%20Badhan%20Kumar%20Das%20and%20Puneet%20Sharma%20and%20Andreas%20Maier%20and%20Dorin%20Comaniciu%20and%20Florin%20C.%20Ghesu&entry.1292438233=%20%20An%20accurate%20detection%20and%20tracking%20of%20devices%20such%20as%20guiding%20catheters%20in%0Alive%20X-ray%20image%20acquisitions%20is%20an%20essential%20prerequisite%20for%20endovascular%0Acardiac%20interventions.%20This%20information%20is%20leveraged%20for%20procedural%20guidance%2C%0Ae.g.%2C%20directing%20stent%20placements.%20To%20ensure%20procedural%20safety%20and%20efficacy%2C%0Athere%20is%20a%20need%20for%20high%20robustness%20no%20failures%20during%20tracking.%20To%20achieve%0Athat%2C%20one%20needs%20to%20efficiently%20tackle%20challenges%2C%20such%20as%3A%20device%20obscuration%0Aby%20contrast%20agent%20or%20other%20external%20devices%20or%20wires%2C%20changes%20in%20field-of-view%0Aor%20acquisition%20angle%2C%20as%20well%20as%20the%20continuous%20movement%20due%20to%20cardiac%20and%0Arespiratory%20motion.%20To%20overcome%20the%20aforementioned%20challenges%2C%20we%20propose%20a%0Anovel%20approach%20to%20learn%20spatio-temporal%20features%20from%20a%20very%20large%20data%20cohort%0Aof%20over%2016%20million%20interventional%20X-ray%20frames%20using%20self-supervision%20for%20image%0Asequence%20data.%20Our%20approach%20is%20based%20on%20a%20masked%20image%20modeling%20technique%20that%0Aleverages%20frame%20interpolation%20based%20reconstruction%20to%20learn%20fine%20inter-frame%0Atemporal%20correspondences.%20The%20features%20encoded%20in%20the%20resulting%20model%20are%0Afine-tuned%20downstream.%20Our%20approach%20achieves%20state-of-the-art%20performance%20and%0Ain%20particular%20robustness%20compared%20to%20ultra%20optimized%20reference%20solutions%20%28that%0Ause%20multi-stage%20feature%20fusion%2C%20multi-task%20and%20flow%20regularization%29.%20The%0Aexperiments%20show%20that%20our%20method%20achieves%2066.31%25%20reduction%20in%20maximum%20tracking%0Aerror%20against%20reference%20solutions%20%2823.20%25%20when%20flow%20regularization%20is%20used%29%3B%0Aachieving%20a%20success%20score%20of%2097.95%25%20at%20a%203x%20faster%20inference%20speed%20of%2042%0Aframes-per-second%20%28on%20GPU%29.%20The%20results%20encourage%20the%20use%20of%20our%20approach%20in%0Avarious%20other%20tasks%20within%20interventional%20image%20analytics%20that%20require%0Aeffective%20understanding%20of%20spatio-temporal%20semantics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01156v1&entry.124074799=Read"},
{"title": "DiL-NeRF: Delving into Lidar for Neural Radiance Field on Street Scenes", "author": "Shanlin Sun and Bingbing Zhuang and Ziyu Jiang and Buyu Liu and Xiaohui Xie and Manmohan Chandraker", "abstract": "  Photorealistic simulation plays a crucial role in applications such as\nautonomous driving, where advances in neural radiance fields (NeRFs) may allow\nbetter scalability through the automatic creation of digital 3D assets.\nHowever, reconstruction quality suffers on street scenes due to largely\ncollinear camera motions and sparser samplings at higher speeds. On the other\nhand, the application often demands rendering from camera views that deviate\nfrom the inputs to accurately simulate behaviors like lane changes. In this\npaper, we propose several insights that allow a better utilization of Lidar\ndata to improve NeRF quality on street scenes. First, our framework learns a\ngeometric scene representation from Lidar, which is fused with the implicit\ngrid-based representation for radiance decoding, thereby supplying stronger\ngeometric information offered by explicit point cloud. Second, we put forth a\nrobust occlusion-aware depth supervision scheme, which allows utilizing\ndensified Lidar points by accumulation. Third, we generate augmented training\nviews from Lidar points for further improvement. Our insights translate to\nlargely improved novel view synthesis under real driving scenes.\n", "link": "http://arxiv.org/abs/2405.00900v1", "date": "2024-05-01", "relevancy": 2.8119, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5825}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5584}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5463}], "mailto": "mailto:daeR=997470421.yrtne&1v00900.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.senecs02%gnivird02%laer02%rednu02%sisehtnys02%weiv02%levon02%devorpmi02%ylegralA0%ot02%etalsnart02%sthgisni02%ruO02%.tnemevorpmi02%rehtruf02%rof02%stniop02%radiL02%morf02%sweivA0%gniniart02%detnemgua02%etareneg02%ew02%C2%drihT02%.noitalumucca02%yb02%stniop02%radiL02%deifisnedA0%gnizilitu02%swolla02%hcihw02%C2%emehcs02%noisivrepus02%htped02%erawa-noisulcco02%tsuborA0%a02%htrof02%tup02%ew02%C2%dnoceS02%.duolc02%tniop02%ticilpxe02%yb02%dereffo02%noitamrofni02%cirtemoegA0%regnorts02%gniylppus02%ybereht02%C2%gnidoced02%ecnaidar02%rof02%noitatneserper02%desab-dirgA0%ticilpmi02%eht02%htiw02%desuf02%si02%hcihw02%C2%radiL02%morf02%noitatneserper02%enecs02%cirtemoegA0%a02%snrael02%krowemarf02%ruo02%C2%tsriF02%.senecs02%teerts02%no02%ytilauq02%FReN02%evorpmi02%ot02%atadA0%radiL02%fo02%noitazilitu02%retteb02%a02%wolla02%taht02%sthgisni02%lareves02%esoporp02%ew02%C2%repapA0%siht02%nI02%.segnahc02%enal02%ekil02%sroivaheb02%etalumis02%yletarucca02%ot02%stupni02%eht02%morfA0%etaived02%taht02%sweiv02%aremac02%morf02%gniredner02%sdnamed02%netfo02%noitacilppa02%eht02%C2%dnahA0%rehto02%eht02%nO02%.sdeeps02%rehgih02%ta02%sgnilpmas02%resraps02%dna02%snoitom02%aremac02%raenillocA0%ylegral02%ot02%eud02%senecs02%teerts02%no02%sreffus02%ytilauq02%noitcurtsnocer02%C2%revewoHA0%.stessa02%D302%latigid02%fo02%noitaerc02%citamotua02%eht02%hguorht02%ytilibalacs02%rettebA0%wolla02%yam02%92%sFReN82%02%sdleif02%ecnaidar02%laruen02%ni02%secnavda02%erehw02%C2%gnivird02%suomonotuaA0%sa02%hcus02%snoitacilppa02%ni02%elor02%laicurc02%a02%syalp02%noitalumis02%citsilaerotohP02%02%=3328342921.yrtne&rekardnahC02%nahomnaM02%dna02%eiX02%iuhoaiX02%dna02%uiL02%uyuB02%dna02%gnaiJ02%uyiZ02%dna02%gnauhZ02%gnibgniB02%dna02%nuS02%nilnahS=526535609.yrtne&senecS02%teertS02%no02%dleiF02%ecnaidaR02%larueN02%rof02%radiL02%otni02%gnivleD02%A3%FReN-LiD=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20DiL-NeRF%3A%20Delving%20into%20Lidar%20for%20Neural%20Radiance%20Field%20on%20Street%20Scenes&body=Title%3A%20DiL-NeRF%3A%20Delving%20into%20Lidar%20for%20Neural%20Radiance%20Field%20on%20Street%20Scenes%0AAuthor%3A%20Shanlin%20Sun%20and%20Bingbing%20Zhuang%20and%20Ziyu%20Jiang%20and%20Buyu%20Liu%20and%20Xiaohui%20Xie%20and%20Manmohan%20Chandraker%0AAbstract%3A%20%20%20Photorealistic%20simulation%20plays%20a%20crucial%20role%20in%20applications%20such%20as%0Aautonomous%20driving%2C%20where%20advances%20in%20neural%20radiance%20fields%20%28NeRFs%29%20may%20allow%0Abetter%20scalability%20through%20the%20automatic%20creation%20of%20digital%203D%20assets.%0AHowever%2C%20reconstruction%20quality%20suffers%20on%20street%20scenes%20due%20to%20largely%0Acollinear%20camera%20motions%20and%20sparser%20samplings%20at%20higher%20speeds.%20On%20the%20other%0Ahand%2C%20the%20application%20often%20demands%20rendering%20from%20camera%20views%20that%20deviate%0Afrom%20the%20inputs%20to%20accurately%20simulate%20behaviors%20like%20lane%20changes.%20In%20this%0Apaper%2C%20we%20propose%20several%20insights%20that%20allow%20a%20better%20utilization%20of%20Lidar%0Adata%20to%20improve%20NeRF%20quality%20on%20street%20scenes.%20First%2C%20our%20framework%20learns%20a%0Ageometric%20scene%20representation%20from%20Lidar%2C%20which%20is%20fused%20with%20the%20implicit%0Agrid-based%20representation%20for%20radiance%20decoding%2C%20thereby%20supplying%20stronger%0Ageometric%20information%20offered%20by%20explicit%20point%20cloud.%20Second%2C%20we%20put%20forth%20a%0Arobust%20occlusion-aware%20depth%20supervision%20scheme%2C%20which%20allows%20utilizing%0Adensified%20Lidar%20points%20by%20accumulation.%20Third%2C%20we%20generate%20augmented%20training%0Aviews%20from%20Lidar%20points%20for%20further%20improvement.%20Our%20insights%20translate%20to%0Alargely%20improved%20novel%20view%20synthesis%20under%20real%20driving%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00900v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DiL-NeRF%3A%20Delving%20into%20Lidar%20for%20Neural%20Radiance%20Field%20on%20Street%20Scenes&entry.906535625=Shanlin%20Sun%20and%20Bingbing%20Zhuang%20and%20Ziyu%20Jiang%20and%20Buyu%20Liu%20and%20Xiaohui%20Xie%20and%20Manmohan%20Chandraker&entry.1292438233=%20%20Photorealistic%20simulation%20plays%20a%20crucial%20role%20in%20applications%20such%20as%0Aautonomous%20driving%2C%20where%20advances%20in%20neural%20radiance%20fields%20%28NeRFs%29%20may%20allow%0Abetter%20scalability%20through%20the%20automatic%20creation%20of%20digital%203D%20assets.%0AHowever%2C%20reconstruction%20quality%20suffers%20on%20street%20scenes%20due%20to%20largely%0Acollinear%20camera%20motions%20and%20sparser%20samplings%20at%20higher%20speeds.%20On%20the%20other%0Ahand%2C%20the%20application%20often%20demands%20rendering%20from%20camera%20views%20that%20deviate%0Afrom%20the%20inputs%20to%20accurately%20simulate%20behaviors%20like%20lane%20changes.%20In%20this%0Apaper%2C%20we%20propose%20several%20insights%20that%20allow%20a%20better%20utilization%20of%20Lidar%0Adata%20to%20improve%20NeRF%20quality%20on%20street%20scenes.%20First%2C%20our%20framework%20learns%20a%0Ageometric%20scene%20representation%20from%20Lidar%2C%20which%20is%20fused%20with%20the%20implicit%0Agrid-based%20representation%20for%20radiance%20decoding%2C%20thereby%20supplying%20stronger%0Ageometric%20information%20offered%20by%20explicit%20point%20cloud.%20Second%2C%20we%20put%20forth%20a%0Arobust%20occlusion-aware%20depth%20supervision%20scheme%2C%20which%20allows%20utilizing%0Adensified%20Lidar%20points%20by%20accumulation.%20Third%2C%20we%20generate%20augmented%20training%0Aviews%20from%20Lidar%20points%20for%20further%20improvement.%20Our%20insights%20translate%20to%0Alargely%20improved%20novel%20view%20synthesis%20under%20real%20driving%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00900v1&entry.124074799=Read"},
{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "author": "Shengjie Ma and Yanlin Weng and Tianjia Shao and Kun Zhou", "abstract": "  We introduce 3D Gaussian blendshapes for modeling photorealistic head\navatars. Taking a monocular video as input, we learn a base head model of\nneutral expression, along with a group of expression blendshapes, each of which\ncorresponds to a basis expression in classical parametric face models. Both the\nneutral model and expression blendshapes are represented as 3D Gaussians, which\ncontain a few properties to depict the avatar appearance. The avatar model of\nan arbitrary expression can be effectively generated by combining the neutral\nmodel and expression blendshapes through linear blending of Gaussians with the\nexpression coefficients. High-fidelity head avatar animations can be\nsynthesized in real time using Gaussian splatting. Compared to state-of-the-art\nmethods, our Gaussian blendshape representation better captures high-frequency\ndetails exhibited in input video, and achieves superior rendering performance.\n", "link": "http://arxiv.org/abs/2404.19398v2", "date": "2024-05-02", "relevancy": 2.7792, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5997}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5345}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5334}], "mailto": "mailto:daeR=997470421.yrtne&2v89391.4042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.ecnamrofrep02%gniredner02%roirepus02%seveihca02%dna02%C2%oediv02%tupni02%ni02%detibihxe02%sliatedA0%ycneuqerf-hgih02%serutpac02%retteb02%noitatneserper02%epahsdnelb02%naissuaG02%ruo02%C2%sdohtemA0%tra-eht-fo-etats02%ot02%derapmoC02%.gnittalps02%naissuaG02%gnisu02%emit02%laer02%ni02%dezisehtnysA0%eb02%nac02%snoitamina02%ratava02%daeh02%ytiledif-hgiH02%.stneiciffeoc02%noisserpxeA0%eht02%htiw02%snaissuaG02%fo02%gnidnelb02%raenil02%hguorht02%sepahsdnelb02%noisserpxe02%dna02%ledomA0%lartuen02%eht02%gninibmoc02%yb02%detareneg02%ylevitceffe02%eb02%nac02%noisserpxe02%yrartibra02%naA0%fo02%ledom02%ratava02%ehT02%.ecnaraeppa02%ratava02%eht02%tciped02%ot02%seitreporp02%wef02%a02%niatnocA0%hcihw02%C2%snaissuaG02%D302%sa02%detneserper02%era02%sepahsdnelb02%noisserpxe02%dna02%ledom02%lartuenA0%eht02%htoB02%.sledom02%ecaf02%cirtemarap02%lacissalc02%ni02%noisserpxe02%sisab02%a02%ot02%sdnopserrocA0%hcihw02%fo02%hcae02%C2%sepahsdnelb02%noisserpxe02%fo02%puorg02%a02%htiw02%gnola02%C2%noisserpxe02%lartuenA0%fo02%ledom02%daeh02%esab02%a02%nrael02%ew02%C2%tupni02%sa02%oediv02%raluconom02%a02%gnikaT02%.sratavaA0%daeh02%citsilaerotohp02%gniledom02%rof02%sepahsdnelb02%naissuaG02%D302%ecudortni02%eW02%02%=3328342921.yrtne&uohZ02%nuK02%dna02%oahS02%aijnaiT02%dna02%gneW02%nilnaY02%dna02%aM02%eijgnehS=526535609.yrtne&noitaminA02%ratavA02%daeH02%rof02%sepahsdnelB02%naissuaG02%D3=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%203D%20Gaussian%20Blendshapes%20for%20Head%20Avatar%20Animation&body=Title%3A%203D%20Gaussian%20Blendshapes%20for%20Head%20Avatar%20Animation%0AAuthor%3A%20Shengjie%20Ma%20and%20Yanlin%20Weng%20and%20Tianjia%20Shao%20and%20Kun%20Zhou%0AAbstract%3A%20%20%20We%20introduce%203D%20Gaussian%20blendshapes%20for%20modeling%20photorealistic%20head%0Aavatars.%20Taking%20a%20monocular%20video%20as%20input%2C%20we%20learn%20a%20base%20head%20model%20of%0Aneutral%20expression%2C%20along%20with%20a%20group%20of%20expression%20blendshapes%2C%20each%20of%20which%0Acorresponds%20to%20a%20basis%20expression%20in%20classical%20parametric%20face%20models.%20Both%20the%0Aneutral%20model%20and%20expression%20blendshapes%20are%20represented%20as%203D%20Gaussians%2C%20which%0Acontain%20a%20few%20properties%20to%20depict%20the%20avatar%20appearance.%20The%20avatar%20model%20of%0Aan%20arbitrary%20expression%20can%20be%20effectively%20generated%20by%20combining%20the%20neutral%0Amodel%20and%20expression%20blendshapes%20through%20linear%20blending%20of%20Gaussians%20with%20the%0Aexpression%20coefficients.%20High-fidelity%20head%20avatar%20animations%20can%20be%0Asynthesized%20in%20real%20time%20using%20Gaussian%20splatting.%20Compared%20to%20state-of-the-art%0Amethods%2C%20our%20Gaussian%20blendshape%20representation%20better%20captures%20high-frequency%0Adetails%20exhibited%20in%20input%20video%2C%20and%20achieves%20superior%20rendering%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19398v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Gaussian%20Blendshapes%20for%20Head%20Avatar%20Animation&entry.906535625=Shengjie%20Ma%20and%20Yanlin%20Weng%20and%20Tianjia%20Shao%20and%20Kun%20Zhou&entry.1292438233=%20%20We%20introduce%203D%20Gaussian%20blendshapes%20for%20modeling%20photorealistic%20head%0Aavatars.%20Taking%20a%20monocular%20video%20as%20input%2C%20we%20learn%20a%20base%20head%20model%20of%0Aneutral%20expression%2C%20along%20with%20a%20group%20of%20expression%20blendshapes%2C%20each%20of%20which%0Acorresponds%20to%20a%20basis%20expression%20in%20classical%20parametric%20face%20models.%20Both%20the%0Aneutral%20model%20and%20expression%20blendshapes%20are%20represented%20as%203D%20Gaussians%2C%20which%0Acontain%20a%20few%20properties%20to%20depict%20the%20avatar%20appearance.%20The%20avatar%20model%20of%0Aan%20arbitrary%20expression%20can%20be%20effectively%20generated%20by%20combining%20the%20neutral%0Amodel%20and%20expression%20blendshapes%20through%20linear%20blending%20of%20Gaussians%20with%20the%0Aexpression%20coefficients.%20High-fidelity%20head%20avatar%20animations%20can%20be%0Asynthesized%20in%20real%20time%20using%20Gaussian%20splatting.%20Compared%20to%20state-of-the-art%0Amethods%2C%20our%20Gaussian%20blendshape%20representation%20better%20captures%20high-frequency%0Adetails%20exhibited%20in%20input%20video%2C%20and%20achieves%20superior%20rendering%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19398v2&entry.124074799=Read"},
{"title": "FusionVision: A comprehensive approach of 3D object reconstruction and\n  segmentation from RGB-D cameras using YOLO and fast segment anything", "author": "Safouane El Ghazouali and Youssef Mhirit and Ali Oukhrid and Umberto Michelucci and Hichem Nouira", "abstract": "  In the realm of computer vision, the integration of advanced techniques into\nthe processing of RGB-D camera inputs poses a significant challenge, given the\ninherent complexities arising from diverse environmental conditions and varying\nobject appearances. Therefore, this paper introduces FusionVision, an\nexhaustive pipeline adapted for the robust 3D segmentation of objects in RGB-D\nimagery. Traditional computer vision systems face limitations in simultaneously\ncapturing precise object boundaries and achieving high-precision object\ndetection on depth map as they are mainly proposed for RGB cameras. To address\nthis challenge, FusionVision adopts an integrated approach by merging\nstate-of-the-art object detection techniques, with advanced instance\nsegmentation methods. The integration of these components enables a holistic\n(unified analysis of information obtained from both color \\textit{RGB} and\ndepth \\textit{D} channels) interpretation of RGB-D data, facilitating the\nextraction of comprehensive and accurate object information. The proposed\nFusionVision pipeline employs YOLO for identifying objects within the RGB image\ndomain. Subsequently, FastSAM, an innovative semantic segmentation model, is\napplied to delineate object boundaries, yielding refined segmentation masks.\nThe synergy between these components and their integration into 3D scene\nunderstanding ensures a cohesive fusion of object detection and segmentation,\nenhancing overall precision in 3D object segmentation. The code and pre-trained\nmodels are publicly available at https://github.com/safouaneelg/FusionVision/.\n", "link": "http://arxiv.org/abs/2403.00175v2", "date": "2024-05-01", "relevancy": 2.7589, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5875}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5365}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5313}], "mailto": "mailto:daeR=997470421.yrtne&2v57100.3042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%./noisiVnoisuF/gleenauofas/moc.buhtig//A3%sptth02%ta02%elbaliava02%ylcilbup02%era02%sledomA0%deniart-erp02%dna02%edoc02%ehT02%.noitatnemges02%tcejbo02%D302%ni02%noisicerp02%llarevo02%gnicnahneA0%C2%noitatnemges02%dna02%noitceted02%tcejbo02%fo02%noisuf02%evisehoc02%a02%serusne02%gnidnatsrednuA0%enecs02%D302%otni02%noitargetni02%rieht02%dna02%stnenopmoc02%eseht02%neewteb02%ygrenys02%ehTA0%.sksam02%noitatnemges02%denifer02%gnidleiy02%C2%seiradnuob02%tcejbo02%etaeniled02%ot02%deilppaA0%si02%C2%ledom02%noitatnemges02%citnames02%evitavonni02%na02%C2%MAStsaF02%C2%yltneuqesbuS02%.niamodA0%egami02%BGR02%eht02%nihtiw02%stcejbo02%gniyfitnedi02%rof02%OLOY02%syolpme02%enilepip02%noisiVnoisuFA0%desoporp02%ehT02%.noitamrofni02%tcejbo02%etarucca02%dna02%evisneherpmoc02%fo02%noitcartxeA0%eht02%gnitatilicaf02%C2%atad02%D-BGR02%fo02%noitaterpretni02%92%slennahc02%D7%DB7%titxetC5%02%htpedA0%dna02%D7%BGRB7%titxetC5%02%roloc02%htob02%morf02%deniatbo02%noitamrofni02%fo02%sisylana02%deifinu82%A0%citsiloh02%a02%selbane02%stnenopmoc02%eseht02%fo02%noitargetni02%ehT02%.sdohtem02%noitatnemgesA0%ecnatsni02%decnavda02%htiw02%C2%seuqinhcet02%noitceted02%tcejbo02%tra-eht-fo-etatsA0%gnigrem02%yb02%hcaorppa02%detargetni02%na02%stpoda02%noisiVnoisuF02%C2%egnellahc02%sihtA0%sserdda02%oT02%.saremac02%BGR02%rof02%desoporp02%ylniam02%era02%yeht02%sa02%pam02%htped02%no02%noitcetedA0%tcejbo02%noisicerp-hgih02%gniveihca02%dna02%seiradnuob02%tcejbo02%esicerp02%gnirutpacA0%ylsuoenatlumis02%ni02%snoitatimil02%ecaf02%smetsys02%noisiv02%retupmoc02%lanoitidarT02%.yregamiA0%D-BGR02%ni02%stcejbo02%fo02%noitatnemges02%D302%tsubor02%eht02%rof02%detpada02%enilepip02%evitsuahxeA0%na02%C2%noisiVnoisuF02%secudortni02%repap02%siht02%C2%eroferehT02%.secnaraeppa02%tcejboA0%gniyrav02%dna02%snoitidnoc02%latnemnorivne02%esrevid02%morf02%gnisira02%seitixelpmoc02%tnerehniA0%eht02%nevig02%C2%egnellahc02%tnacifingis02%a02%sesop02%stupni02%aremac02%D-BGR02%fo02%gnissecorp02%ehtA0%otni02%seuqinhcet02%decnavda02%fo02%noitargetni02%eht02%C2%noisiv02%retupmoc02%fo02%mlaer02%eht02%nI02%02%=3328342921.yrtne&ariuoN02%mehciH02%dna02%icculehciM02%otrebmU02%dna02%dirhkuO02%ilA02%dna02%tirihM02%fessuoY02%dna02%ilauozahG02%lE02%enauofaS=526535609.yrtne&gnihtyna02%tnemges02%tsaf02%dna02%OLOY02%gnisu02%saremac02%D-BGR02%morf02%noitatnemges02%02%A0%dna02%noitcurtsnocer02%tcejbo02%D302%fo02%hcaorppa02%evisneherpmoc02%A02%A3%noisiVnoisuF=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20FusionVision%3A%20A%20comprehensive%20approach%20of%203D%20object%20reconstruction%20and%0A%20%20segmentation%20from%20RGB-D%20cameras%20using%20YOLO%20and%20fast%20segment%20anything&body=Title%3A%20FusionVision%3A%20A%20comprehensive%20approach%20of%203D%20object%20reconstruction%20and%0A%20%20segmentation%20from%20RGB-D%20cameras%20using%20YOLO%20and%20fast%20segment%20anything%0AAuthor%3A%20Safouane%20El%20Ghazouali%20and%20Youssef%20Mhirit%20and%20Ali%20Oukhrid%20and%20Umberto%20Michelucci%20and%20Hichem%20Nouira%0AAbstract%3A%20%20%20In%20the%20realm%20of%20computer%20vision%2C%20the%20integration%20of%20advanced%20techniques%20into%0Athe%20processing%20of%20RGB-D%20camera%20inputs%20poses%20a%20significant%20challenge%2C%20given%20the%0Ainherent%20complexities%20arising%20from%20diverse%20environmental%20conditions%20and%20varying%0Aobject%20appearances.%20Therefore%2C%20this%20paper%20introduces%20FusionVision%2C%20an%0Aexhaustive%20pipeline%20adapted%20for%20the%20robust%203D%20segmentation%20of%20objects%20in%20RGB-D%0Aimagery.%20Traditional%20computer%20vision%20systems%20face%20limitations%20in%20simultaneously%0Acapturing%20precise%20object%20boundaries%20and%20achieving%20high-precision%20object%0Adetection%20on%20depth%20map%20as%20they%20are%20mainly%20proposed%20for%20RGB%20cameras.%20To%20address%0Athis%20challenge%2C%20FusionVision%20adopts%20an%20integrated%20approach%20by%20merging%0Astate-of-the-art%20object%20detection%20techniques%2C%20with%20advanced%20instance%0Asegmentation%20methods.%20The%20integration%20of%20these%20components%20enables%20a%20holistic%0A%28unified%20analysis%20of%20information%20obtained%20from%20both%20color%20%5Ctextit%7BRGB%7D%20and%0Adepth%20%5Ctextit%7BD%7D%20channels%29%20interpretation%20of%20RGB-D%20data%2C%20facilitating%20the%0Aextraction%20of%20comprehensive%20and%20accurate%20object%20information.%20The%20proposed%0AFusionVision%20pipeline%20employs%20YOLO%20for%20identifying%20objects%20within%20the%20RGB%20image%0Adomain.%20Subsequently%2C%20FastSAM%2C%20an%20innovative%20semantic%20segmentation%20model%2C%20is%0Aapplied%20to%20delineate%20object%20boundaries%2C%20yielding%20refined%20segmentation%20masks.%0AThe%20synergy%20between%20these%20components%20and%20their%20integration%20into%203D%20scene%0Aunderstanding%20ensures%20a%20cohesive%20fusion%20of%20object%20detection%20and%20segmentation%2C%0Aenhancing%20overall%20precision%20in%203D%20object%20segmentation.%20The%20code%20and%20pre-trained%0Amodels%20are%20publicly%20available%20at%20https%3A//github.com/safouaneelg/FusionVision/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00175v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FusionVision%3A%20A%20comprehensive%20approach%20of%203D%20object%20reconstruction%20and%0A%20%20segmentation%20from%20RGB-D%20cameras%20using%20YOLO%20and%20fast%20segment%20anything&entry.906535625=Safouane%20El%20Ghazouali%20and%20Youssef%20Mhirit%20and%20Ali%20Oukhrid%20and%20Umberto%20Michelucci%20and%20Hichem%20Nouira&entry.1292438233=%20%20In%20the%20realm%20of%20computer%20vision%2C%20the%20integration%20of%20advanced%20techniques%20into%0Athe%20processing%20of%20RGB-D%20camera%20inputs%20poses%20a%20significant%20challenge%2C%20given%20the%0Ainherent%20complexities%20arising%20from%20diverse%20environmental%20conditions%20and%20varying%0Aobject%20appearances.%20Therefore%2C%20this%20paper%20introduces%20FusionVision%2C%20an%0Aexhaustive%20pipeline%20adapted%20for%20the%20robust%203D%20segmentation%20of%20objects%20in%20RGB-D%0Aimagery.%20Traditional%20computer%20vision%20systems%20face%20limitations%20in%20simultaneously%0Acapturing%20precise%20object%20boundaries%20and%20achieving%20high-precision%20object%0Adetection%20on%20depth%20map%20as%20they%20are%20mainly%20proposed%20for%20RGB%20cameras.%20To%20address%0Athis%20challenge%2C%20FusionVision%20adopts%20an%20integrated%20approach%20by%20merging%0Astate-of-the-art%20object%20detection%20techniques%2C%20with%20advanced%20instance%0Asegmentation%20methods.%20The%20integration%20of%20these%20components%20enables%20a%20holistic%0A%28unified%20analysis%20of%20information%20obtained%20from%20both%20color%20%5Ctextit%7BRGB%7D%20and%0Adepth%20%5Ctextit%7BD%7D%20channels%29%20interpretation%20of%20RGB-D%20data%2C%20facilitating%20the%0Aextraction%20of%20comprehensive%20and%20accurate%20object%20information.%20The%20proposed%0AFusionVision%20pipeline%20employs%20YOLO%20for%20identifying%20objects%20within%20the%20RGB%20image%0Adomain.%20Subsequently%2C%20FastSAM%2C%20an%20innovative%20semantic%20segmentation%20model%2C%20is%0Aapplied%20to%20delineate%20object%20boundaries%2C%20yielding%20refined%20segmentation%20masks.%0AThe%20synergy%20between%20these%20components%20and%20their%20integration%20into%203D%20scene%0Aunderstanding%20ensures%20a%20cohesive%20fusion%20of%20object%20detection%20and%20segmentation%2C%0Aenhancing%20overall%20precision%20in%203D%20object%20segmentation.%20The%20code%20and%20pre-trained%0Amodels%20are%20publicly%20available%20at%20https%3A//github.com/safouaneelg/FusionVision/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00175v2&entry.124074799=Read"},
{"title": "Efficient Remote Sensing with Harmonized Transfer Learning and Modality\n  Alignment", "author": "Tengjun Huang", "abstract": "  With the rise of Visual and Language Pretraining (VLP), an increasing number\nof downstream tasks are adopting the paradigm of pretraining followed by\nfine-tuning. Although this paradigm has demonstrated potential in various\nmultimodal downstream tasks, its implementation in the remote sensing domain\nencounters some obstacles. Specifically, the tendency for same-modality\nembeddings to cluster together impedes efficient transfer learning. To tackle\nthis issue, we review the aim of multimodal transfer learning for downstream\ntasks from a unified perspective, and rethink the optimization process based on\nthree distinct objectives. We propose \"Harmonized Transfer Learning and\nModality Alignment (HarMA)\", a method that simultaneously satisfies task\nconstraints, modality alignment, and single-modality uniform alignment, while\nminimizing training overhead through parameter-efficient fine-tuning.\nRemarkably, without the need for external data for training, HarMA achieves\nstate-of-the-art performance in two popular multimodal retrieval tasks in the\nfield of remote sensing. Our experiments reveal that HarMA achieves competitive\nand even superior performance to fully fine-tuned models with only minimal\nadjustable parameters. Due to its simplicity, HarMA can be integrated into\nalmost all existing multimodal pretraining models. We hope this method can\nfacilitate the efficient application of large models to a wide range of\ndownstream tasks while significantly reducing the resource consumption. Code is\navailable at https://github.com/seekerhuang/HarMA.\n", "link": "http://arxiv.org/abs/2404.18253v3", "date": "2024-05-02", "relevancy": 2.7579, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6056}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5264}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5228}], "mailto": "mailto:daeR=997470421.yrtne&3v35281.4042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.AMraH/gnauhrekees/moc.buhtig//A3%sptth02%ta02%elbaliavaA0%si02%edoC02%.noitpmusnoc02%ecruoser02%eht02%gnicuder02%yltnacifingis02%elihw02%sksat02%maertsnwodA0%fo02%egnar02%ediw02%a02%ot02%sledom02%egral02%fo02%noitacilppa02%tneiciffe02%eht02%etatilicafA0%nac02%dohtem02%siht02%epoh02%eW02%.sledom02%gniniarterp02%ladomitlum02%gnitsixe02%lla02%tsomlaA0%otni02%detargetni02%eb02%nac02%AMraH02%C2%yticilpmis02%sti02%ot02%euD02%.sretemarap02%elbatsujdaA0%laminim02%ylno02%htiw02%sledom02%denut-enif02%ylluf02%ot02%ecnamrofrep02%roirepus02%neve02%dnaA0%evititepmoc02%seveihca02%AMraH02%taht02%laever02%stnemirepxe02%ruO02%.gnisnes02%etomer02%fo02%dleifA0%eht02%ni02%sksat02%laveirter02%ladomitlum02%ralupop02%owt02%ni02%ecnamrofrep02%tra-eht-fo-etatsA0%seveihca02%AMraH02%C2%gniniart02%rof02%atad02%lanretxe02%rof02%deen02%eht02%tuohtiw02%C2%ylbakrameRA0%.gninut-enif02%tneiciffe-retemarap02%hguorht02%daehrevo02%gniniart02%gniziminimA0%elihw02%C2%tnemngila02%mrofinu02%ytiladom-elgnis02%dna02%C2%tnemngila02%ytiladom02%C2%stniartsnocA0%ksat02%seifsitas02%ylsuoenatlumis02%taht02%dohtem02%a02%C2%22%92%AMraH82%02%tnemngilA02%ytiladoMA0%dna02%gninraeL02%refsnarT02%dezinomraH22%02%esoporp02%eW02%.sevitcejbo02%tcnitsid02%eerhtA0%no02%desab02%ssecorp02%noitazimitpo02%eht02%knihter02%dna02%C2%evitcepsrep02%deifinu02%a02%morf02%sksatA0%maertsnwod02%rof02%gninrael02%refsnart02%ladomitlum02%fo02%mia02%eht02%weiver02%ew02%C2%eussi02%sihtA0%elkcat02%oT02%.gninrael02%refsnart02%tneiciffe02%sedepmi02%rehtegot02%retsulc02%ot02%sgniddebmeA0%ytiladom-emas02%rof02%ycnednet02%eht02%C2%yllacificepS02%.selcatsbo02%emos02%sretnuocneA0%niamod02%gnisnes02%etomer02%eht02%ni02%noitatnemelpmi02%sti02%C2%sksat02%maertsnwod02%ladomitlumA0%suoirav02%ni02%laitnetop02%detartsnomed02%sah02%mgidarap02%siht02%hguohtlA02%.gninut-enifA0%yb02%dewollof02%gniniarterp02%fo02%mgidarap02%eht02%gnitpoda02%era02%sksat02%maertsnwod02%foA0%rebmun02%gnisaercni02%na02%C2%92%PLV82%02%gniniarterP02%egaugnaL02%dna02%lausiV02%fo02%esir02%eht02%htiW02%02%=3328342921.yrtne&gnauH02%nujgneT=526535609.yrtne&tnemngilA02%02%A0%ytiladoM02%dna02%gninraeL02%refsnarT02%dezinomraH02%htiw02%gnisneS02%etomeR02%tneiciffE=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Efficient%20Remote%20Sensing%20with%20Harmonized%20Transfer%20Learning%20and%20Modality%0A%20%20Alignment&body=Title%3A%20Efficient%20Remote%20Sensing%20with%20Harmonized%20Transfer%20Learning%20and%20Modality%0A%20%20Alignment%0AAuthor%3A%20Tengjun%20Huang%0AAbstract%3A%20%20%20With%20the%20rise%20of%20Visual%20and%20Language%20Pretraining%20%28VLP%29%2C%20an%20increasing%20number%0Aof%20downstream%20tasks%20are%20adopting%20the%20paradigm%20of%20pretraining%20followed%20by%0Afine-tuning.%20Although%20this%20paradigm%20has%20demonstrated%20potential%20in%20various%0Amultimodal%20downstream%20tasks%2C%20its%20implementation%20in%20the%20remote%20sensing%20domain%0Aencounters%20some%20obstacles.%20Specifically%2C%20the%20tendency%20for%20same-modality%0Aembeddings%20to%20cluster%20together%20impedes%20efficient%20transfer%20learning.%20To%20tackle%0Athis%20issue%2C%20we%20review%20the%20aim%20of%20multimodal%20transfer%20learning%20for%20downstream%0Atasks%20from%20a%20unified%20perspective%2C%20and%20rethink%20the%20optimization%20process%20based%20on%0Athree%20distinct%20objectives.%20We%20propose%20%22Harmonized%20Transfer%20Learning%20and%0AModality%20Alignment%20%28HarMA%29%22%2C%20a%20method%20that%20simultaneously%20satisfies%20task%0Aconstraints%2C%20modality%20alignment%2C%20and%20single-modality%20uniform%20alignment%2C%20while%0Aminimizing%20training%20overhead%20through%20parameter-efficient%20fine-tuning.%0ARemarkably%2C%20without%20the%20need%20for%20external%20data%20for%20training%2C%20HarMA%20achieves%0Astate-of-the-art%20performance%20in%20two%20popular%20multimodal%20retrieval%20tasks%20in%20the%0Afield%20of%20remote%20sensing.%20Our%20experiments%20reveal%20that%20HarMA%20achieves%20competitive%0Aand%20even%20superior%20performance%20to%20fully%20fine-tuned%20models%20with%20only%20minimal%0Aadjustable%20parameters.%20Due%20to%20its%20simplicity%2C%20HarMA%20can%20be%20integrated%20into%0Aalmost%20all%20existing%20multimodal%20pretraining%20models.%20We%20hope%20this%20method%20can%0Afacilitate%20the%20efficient%20application%20of%20large%20models%20to%20a%20wide%20range%20of%0Adownstream%20tasks%20while%20significantly%20reducing%20the%20resource%20consumption.%20Code%20is%0Aavailable%20at%20https%3A//github.com/seekerhuang/HarMA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18253v3%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Remote%20Sensing%20with%20Harmonized%20Transfer%20Learning%20and%20Modality%0A%20%20Alignment&entry.906535625=Tengjun%20Huang&entry.1292438233=%20%20With%20the%20rise%20of%20Visual%20and%20Language%20Pretraining%20%28VLP%29%2C%20an%20increasing%20number%0Aof%20downstream%20tasks%20are%20adopting%20the%20paradigm%20of%20pretraining%20followed%20by%0Afine-tuning.%20Although%20this%20paradigm%20has%20demonstrated%20potential%20in%20various%0Amultimodal%20downstream%20tasks%2C%20its%20implementation%20in%20the%20remote%20sensing%20domain%0Aencounters%20some%20obstacles.%20Specifically%2C%20the%20tendency%20for%20same-modality%0Aembeddings%20to%20cluster%20together%20impedes%20efficient%20transfer%20learning.%20To%20tackle%0Athis%20issue%2C%20we%20review%20the%20aim%20of%20multimodal%20transfer%20learning%20for%20downstream%0Atasks%20from%20a%20unified%20perspective%2C%20and%20rethink%20the%20optimization%20process%20based%20on%0Athree%20distinct%20objectives.%20We%20propose%20%22Harmonized%20Transfer%20Learning%20and%0AModality%20Alignment%20%28HarMA%29%22%2C%20a%20method%20that%20simultaneously%20satisfies%20task%0Aconstraints%2C%20modality%20alignment%2C%20and%20single-modality%20uniform%20alignment%2C%20while%0Aminimizing%20training%20overhead%20through%20parameter-efficient%20fine-tuning.%0ARemarkably%2C%20without%20the%20need%20for%20external%20data%20for%20training%2C%20HarMA%20achieves%0Astate-of-the-art%20performance%20in%20two%20popular%20multimodal%20retrieval%20tasks%20in%20the%0Afield%20of%20remote%20sensing.%20Our%20experiments%20reveal%20that%20HarMA%20achieves%20competitive%0Aand%20even%20superior%20performance%20to%20fully%20fine-tuned%20models%20with%20only%20minimal%0Aadjustable%20parameters.%20Due%20to%20its%20simplicity%2C%20HarMA%20can%20be%20integrated%20into%0Aalmost%20all%20existing%20multimodal%20pretraining%20models.%20We%20hope%20this%20method%20can%0Afacilitate%20the%20efficient%20application%20of%20large%20models%20to%20a%20wide%20range%20of%0Adownstream%20tasks%20while%20significantly%20reducing%20the%20resource%20consumption.%20Code%20is%0Aavailable%20at%20https%3A//github.com/seekerhuang/HarMA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18253v3&entry.124074799=Read"},
{"title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey", "author": "Penghao Zhao and Hailin Zhang and Qinhan Yu and Zhengren Wang and Yunteng Geng and Fangcheng Fu and Ling Yang and Wentao Zhang and Jie Jiang and Bin Cui", "abstract": "  Advancements in model algorithms, the growth of foundational models, and\naccess to high-quality datasets have propelled the evolution of Artificial\nIntelligence Generated Content (AIGC). Despite its notable successes, AIGC\nstill faces hurdles such as updating knowledge, handling long-tail data,\nmitigating data leakage, and managing high training and inference costs.\nRetrieval-Augmented Generation (RAG) has recently emerged as a paradigm to\naddress such challenges. In particular, RAG introduces the information\nretrieval process, which enhances the generation process by retrieving relevant\nobjects from available data stores, leading to higher accuracy and better\nrobustness. In this paper, we comprehensively review existing efforts that\nintegrate RAG technique into AIGC scenarios. We first classify RAG foundations\naccording to how the retriever augments the generator, distilling the\nfundamental abstractions of the augmentation methodologies for various\nretrievers and generators. This unified perspective encompasses all RAG\nscenarios, illuminating advancements and pivotal technologies that help with\npotential future progress. We also summarize additional enhancements methods\nfor RAG, facilitating effective engineering and implementation of RAG systems.\nThen from another view, we survey on practical applications of RAG across\ndifferent modalities and tasks, offering valuable references for researchers\nand practitioners. Furthermore, we introduce the benchmarks for RAG, discuss\nthe limitations of current RAG systems, and suggest potential directions for\nfuture research. Github: https://github.com/PKU-DAIR/RAG-Survey.\n", "link": "http://arxiv.org/abs/2402.19473v4", "date": "2024-05-02", "relevancy": 2.7387, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5726}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5517}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.519}], "mailto": "mailto:daeR=997470421.yrtne&4v37491.2042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.yevruS-GAR/RIAD-UKP/moc.buhtig//A3%sptth02%A3%buhtiG02%.hcraeser02%erutufA0%rof02%snoitcerid02%laitnetop02%tseggus02%dna02%C2%smetsys02%GAR02%tnerruc02%fo02%snoitatimil02%ehtA0%ssucsid02%C2%GAR02%rof02%skramhcneb02%eht02%ecudortni02%ew02%C2%eromrehtruF02%.srenoititcarp02%dnaA0%srehcraeser02%rof02%secnerefer02%elbaulav02%gnireffo02%C2%sksat02%dna02%seitiladom02%tnereffidA0%ssorca02%GAR02%fo02%snoitacilppa02%lacitcarp02%no02%yevrus02%ew02%C2%weiv02%rehtona02%morf02%nehTA0%.smetsys02%GAR02%fo02%noitatnemelpmi02%dna02%gnireenigne02%evitceffe02%gnitatilicaf02%C2%GAR02%rofA0%sdohtem02%stnemecnahne02%lanoitidda02%ezirammus02%osla02%eW02%.ssergorp02%erutuf02%laitnetopA0%htiw02%pleh02%taht02%seigolonhcet02%latovip02%dna02%stnemecnavda02%gnitanimulli02%C2%soiranecsA0%GAR02%lla02%sessapmocne02%evitcepsrep02%deifinu02%sihT02%.srotareneg02%dna02%sreveirterA0%suoirav02%rof02%seigolodohtem02%noitatnemgua02%eht02%fo02%snoitcartsba02%latnemadnufA0%eht02%gnillitsid02%C2%rotareneg02%eht02%stnemgua02%reveirter02%eht02%woh02%ot02%gnidroccaA0%snoitadnuof02%GAR02%yfissalc02%tsrif02%eW02%.soiranecs02%CGIA02%otni02%euqinhcet02%GAR02%etargetniA0%taht02%stroffe02%gnitsixe02%weiver02%ylevisneherpmoc02%ew02%C2%repap02%siht02%nI02%.ssentsuborA0%retteb02%dna02%ycarucca02%rehgih02%ot02%gnidael02%C2%serots02%atad02%elbaliava02%morf02%stcejboA0%tnaveler02%gniveirter02%yb02%ssecorp02%noitareneg02%eht02%secnahne02%hcihw02%C2%ssecorp02%laveirterA0%noitamrofni02%eht02%secudortni02%GAR02%C2%ralucitrap02%nI02%.segnellahc02%hcus02%sserddaA0%ot02%mgidarap02%a02%sa02%degreme02%yltnecer02%sah02%92%GAR82%02%noitareneG02%detnemguA-laveirteRA0%.stsoc02%ecnerefni02%dna02%gniniart02%hgih02%gniganam02%dna02%C2%egakael02%atad02%gnitagitimA0%C2%atad02%liat-gnol02%gnildnah02%C2%egdelwonk02%gnitadpu02%sa02%hcus02%seldruh02%secaf02%llitsA0%CGIA02%C2%sesseccus02%elbaton02%sti02%etipseD02%.92%CGIA82%02%tnetnoC02%detareneG02%ecnegilletnIA0%laicifitrA02%fo02%noitulove02%eht02%delleporp02%evah02%stesatad02%ytilauq-hgih02%ot02%sseccaA0%dna02%C2%sledom02%lanoitadnuof02%fo02%htworg02%eht02%C2%smhtirogla02%ledom02%ni02%stnemecnavdA02%02%=3328342921.yrtne&iuC02%niB02%dna02%gnaiJ02%eiJ02%dna02%gnahZ02%oatneW02%dna02%gnaY02%gniL02%dna02%uF02%gnehcgnaF02%dna02%gneG02%gnetnuY02%dna02%gnaW02%nergnehZ02%dna02%uY02%nahniQ02%dna02%gnahZ02%niliaH02%dna02%oahZ02%oahgneP=526535609.yrtne&yevruS02%A02%A3%tnetnoC02%detareneG-IA02%rof02%noitareneG02%detnemguA-laveirteR=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Retrieval-Augmented%20Generation%20for%20AI-Generated%20Content%3A%20A%20Survey&body=Title%3A%20Retrieval-Augmented%20Generation%20for%20AI-Generated%20Content%3A%20A%20Survey%0AAuthor%3A%20Penghao%20Zhao%20and%20Hailin%20Zhang%20and%20Qinhan%20Yu%20and%20Zhengren%20Wang%20and%20Yunteng%20Geng%20and%20Fangcheng%20Fu%20and%20Ling%20Yang%20and%20Wentao%20Zhang%20and%20Jie%20Jiang%20and%20Bin%20Cui%0AAbstract%3A%20%20%20Advancements%20in%20model%20algorithms%2C%20the%20growth%20of%20foundational%20models%2C%20and%0Aaccess%20to%20high-quality%20datasets%20have%20propelled%20the%20evolution%20of%20Artificial%0AIntelligence%20Generated%20Content%20%28AIGC%29.%20Despite%20its%20notable%20successes%2C%20AIGC%0Astill%20faces%20hurdles%20such%20as%20updating%20knowledge%2C%20handling%20long-tail%20data%2C%0Amitigating%20data%20leakage%2C%20and%20managing%20high%20training%20and%20inference%20costs.%0ARetrieval-Augmented%20Generation%20%28RAG%29%20has%20recently%20emerged%20as%20a%20paradigm%20to%0Aaddress%20such%20challenges.%20In%20particular%2C%20RAG%20introduces%20the%20information%0Aretrieval%20process%2C%20which%20enhances%20the%20generation%20process%20by%20retrieving%20relevant%0Aobjects%20from%20available%20data%20stores%2C%20leading%20to%20higher%20accuracy%20and%20better%0Arobustness.%20In%20this%20paper%2C%20we%20comprehensively%20review%20existing%20efforts%20that%0Aintegrate%20RAG%20technique%20into%20AIGC%20scenarios.%20We%20first%20classify%20RAG%20foundations%0Aaccording%20to%20how%20the%20retriever%20augments%20the%20generator%2C%20distilling%20the%0Afundamental%20abstractions%20of%20the%20augmentation%20methodologies%20for%20various%0Aretrievers%20and%20generators.%20This%20unified%20perspective%20encompasses%20all%20RAG%0Ascenarios%2C%20illuminating%20advancements%20and%20pivotal%20technologies%20that%20help%20with%0Apotential%20future%20progress.%20We%20also%20summarize%20additional%20enhancements%20methods%0Afor%20RAG%2C%20facilitating%20effective%20engineering%20and%20implementation%20of%20RAG%20systems.%0AThen%20from%20another%20view%2C%20we%20survey%20on%20practical%20applications%20of%20RAG%20across%0Adifferent%20modalities%20and%20tasks%2C%20offering%20valuable%20references%20for%20researchers%0Aand%20practitioners.%20Furthermore%2C%20we%20introduce%20the%20benchmarks%20for%20RAG%2C%20discuss%0Athe%20limitations%20of%20current%20RAG%20systems%2C%20and%20suggest%20potential%20directions%20for%0Afuture%20research.%20Github%3A%20https%3A//github.com/PKU-DAIR/RAG-Survey.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.19473v4%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Retrieval-Augmented%20Generation%20for%20AI-Generated%20Content%3A%20A%20Survey&entry.906535625=Penghao%20Zhao%20and%20Hailin%20Zhang%20and%20Qinhan%20Yu%20and%20Zhengren%20Wang%20and%20Yunteng%20Geng%20and%20Fangcheng%20Fu%20and%20Ling%20Yang%20and%20Wentao%20Zhang%20and%20Jie%20Jiang%20and%20Bin%20Cui&entry.1292438233=%20%20Advancements%20in%20model%20algorithms%2C%20the%20growth%20of%20foundational%20models%2C%20and%0Aaccess%20to%20high-quality%20datasets%20have%20propelled%20the%20evolution%20of%20Artificial%0AIntelligence%20Generated%20Content%20%28AIGC%29.%20Despite%20its%20notable%20successes%2C%20AIGC%0Astill%20faces%20hurdles%20such%20as%20updating%20knowledge%2C%20handling%20long-tail%20data%2C%0Amitigating%20data%20leakage%2C%20and%20managing%20high%20training%20and%20inference%20costs.%0ARetrieval-Augmented%20Generation%20%28RAG%29%20has%20recently%20emerged%20as%20a%20paradigm%20to%0Aaddress%20such%20challenges.%20In%20particular%2C%20RAG%20introduces%20the%20information%0Aretrieval%20process%2C%20which%20enhances%20the%20generation%20process%20by%20retrieving%20relevant%0Aobjects%20from%20available%20data%20stores%2C%20leading%20to%20higher%20accuracy%20and%20better%0Arobustness.%20In%20this%20paper%2C%20we%20comprehensively%20review%20existing%20efforts%20that%0Aintegrate%20RAG%20technique%20into%20AIGC%20scenarios.%20We%20first%20classify%20RAG%20foundations%0Aaccording%20to%20how%20the%20retriever%20augments%20the%20generator%2C%20distilling%20the%0Afundamental%20abstractions%20of%20the%20augmentation%20methodologies%20for%20various%0Aretrievers%20and%20generators.%20This%20unified%20perspective%20encompasses%20all%20RAG%0Ascenarios%2C%20illuminating%20advancements%20and%20pivotal%20technologies%20that%20help%20with%0Apotential%20future%20progress.%20We%20also%20summarize%20additional%20enhancements%20methods%0Afor%20RAG%2C%20facilitating%20effective%20engineering%20and%20implementation%20of%20RAG%20systems.%0AThen%20from%20another%20view%2C%20we%20survey%20on%20practical%20applications%20of%20RAG%20across%0Adifferent%20modalities%20and%20tasks%2C%20offering%20valuable%20references%20for%20researchers%0Aand%20practitioners.%20Furthermore%2C%20we%20introduce%20the%20benchmarks%20for%20RAG%2C%20discuss%0Athe%20limitations%20of%20current%20RAG%20systems%2C%20and%20suggest%20potential%20directions%20for%0Afuture%20research.%20Github%3A%20https%3A//github.com/PKU-DAIR/RAG-Survey.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.19473v4&entry.124074799=Read"},
{"title": "Spherical Linear Interpolation and Text-Anchoring for Zero-shot Composed\n  Image Retrieval", "author": "Young Kyun Jang and Dat Huynh and Ashish Shah and Wen-Kai Chen and Ser-Nam Lim", "abstract": "  Composed Image Retrieval (CIR) is a complex task that retrieves images using\na query, which is configured with an image and a caption that describes desired\nmodifications to that image. Supervised CIR approaches have shown strong\nperformance, but their reliance on expensive manually-annotated datasets\nrestricts their scalability and broader applicability. To address these issues,\nprevious studies have proposed pseudo-word token-based Zero-Shot CIR (ZS-CIR)\nmethods, which utilize a projection module to map images to word tokens.\nHowever, we conjecture that this approach has a downside: the projection module\ndistorts the original image representation and confines the resulting composed\nembeddings to the text-side. In order to resolve this, we introduce a novel\nZS-CIR method that uses Spherical Linear Interpolation (Slerp) to directly\nmerge image and text representations by identifying an intermediate embedding\nof both. Furthermore, we introduce Text-Anchored-Tuning (TAT), a method that\nfine-tunes the image encoder while keeping the text encoder fixed. TAT closes\nthe modality gap between images and text, making the Slerp process much more\neffective. Notably, the TAT method is not only efficient in terms of the scale\nof the training dataset and training time, but it also serves as an excellent\ninitial checkpoint for training supervised CIR models, thereby highlighting its\nwider potential. The integration of the Slerp-based ZS-CIR with a TAT-tuned\nmodel enables our approach to deliver state-of-the-art retrieval performance\nacross CIR benchmarks.\n", "link": "http://arxiv.org/abs/2405.00571v1", "date": "2024-05-01", "relevancy": 2.7369, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5722}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5444}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5256}], "mailto": "mailto:daeR=997470421.yrtne&1v17500.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.skramhcneb02%RIC02%ssorcaA0%ecnamrofrep02%laveirter02%tra-eht-fo-etats02%reviled02%ot02%hcaorppa02%ruo02%selbane02%ledomA0%denut-TAT02%a02%htiw02%RIC-SZ02%desab-prelS02%eht02%fo02%noitargetni02%ehT02%.laitnetop02%rediwA0%sti02%gnithgilhgih02%ybereht02%C2%sledom02%RIC02%desivrepus02%gniniart02%rof02%tniopkcehc02%laitiniA0%tnellecxe02%na02%sa02%sevres02%osla02%ti02%tub02%C2%emit02%gniniart02%dna02%tesatad02%gniniart02%eht02%foA0%elacs02%eht02%fo02%smret02%ni02%tneiciffe02%ylno02%ton02%si02%dohtem02%TAT02%eht02%C2%ylbatoN02%.evitceffeA0%erom02%hcum02%ssecorp02%prelS02%eht02%gnikam02%C2%txet02%dna02%segami02%neewteb02%pag02%ytiladom02%ehtA0%sesolc02%TAT02%.dexif02%redocne02%txet02%eht02%gnipeek02%elihw02%redocne02%egami02%eht02%senut-enifA0%taht02%dohtem02%a02%C2%92%TAT82%02%gninuT-derohcnA-txeT02%ecudortni02%ew02%C2%eromrehtruF02%.htob02%foA0%gniddebme02%etaidemretni02%na02%gniyfitnedi02%yb02%snoitatneserper02%txet02%dna02%egami02%egremA0%yltcerid02%ot02%92%prelS82%02%noitalopretnI02%raeniL02%lacirehpS02%sesu02%taht02%dohtem02%RIC-SZA0%levon02%a02%ecudortni02%ew02%C2%siht02%evloser02%ot02%redro02%nI02%.edis-txet02%eht02%ot02%sgniddebmeA0%desopmoc02%gnitluser02%eht02%senifnoc02%dna02%noitatneserper02%egami02%lanigiro02%eht02%strotsidA0%eludom02%noitcejorp02%eht02%A3%edisnwod02%a02%sah02%hcaorppa02%siht02%taht02%erutcejnoc02%ew02%C2%revewoHA0%.snekot02%drow02%ot02%segami02%pam02%ot02%eludom02%noitcejorp02%a02%ezilitu02%hcihw02%C2%sdohtemA0%92%RIC-SZ82%02%RIC02%tohS-oreZ02%desab-nekot02%drow-oduesp02%desoporp02%evah02%seiduts02%suoiverpA0%C2%seussi02%eseht02%sserdda02%oT02%.ytilibacilppa02%redaorb02%dna02%ytilibalacs02%rieht02%stcirtserA0%stesatad02%detatonna-yllaunam02%evisnepxe02%no02%ecnailer02%rieht02%tub02%C2%ecnamrofrepA0%gnorts02%nwohs02%evah02%sehcaorppa02%RIC02%desivrepuS02%.egami02%taht02%ot02%snoitacifidomA0%derised02%sebircsed02%taht02%noitpac02%a02%dna02%egami02%na02%htiw02%derugifnoc02%si02%hcihw02%C2%yreuq02%aA0%gnisu02%segami02%seveirter02%taht02%ksat02%xelpmoc02%a02%si02%92%RIC82%02%laveirteR02%egamI02%desopmoC02%02%=3328342921.yrtne&miL02%maN-reS02%dna02%nehC02%iaK-neW02%dna02%hahS02%hsihsA02%dna02%hnyuH02%taD02%dna02%gnaJ02%nuyK02%gnuoY=526535609.yrtne&laveirteR02%egamI02%02%A0%desopmoC02%tohs-oreZ02%rof02%gnirohcnA-txeT02%dna02%noitalopretnI02%raeniL02%lacirehpS=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Spherical%20Linear%20Interpolation%20and%20Text-Anchoring%20for%20Zero-shot%20Composed%0A%20%20Image%20Retrieval&body=Title%3A%20Spherical%20Linear%20Interpolation%20and%20Text-Anchoring%20for%20Zero-shot%20Composed%0A%20%20Image%20Retrieval%0AAuthor%3A%20Young%20Kyun%20Jang%20and%20Dat%20Huynh%20and%20Ashish%20Shah%20and%20Wen-Kai%20Chen%20and%20Ser-Nam%20Lim%0AAbstract%3A%20%20%20Composed%20Image%20Retrieval%20%28CIR%29%20is%20a%20complex%20task%20that%20retrieves%20images%20using%0Aa%20query%2C%20which%20is%20configured%20with%20an%20image%20and%20a%20caption%20that%20describes%20desired%0Amodifications%20to%20that%20image.%20Supervised%20CIR%20approaches%20have%20shown%20strong%0Aperformance%2C%20but%20their%20reliance%20on%20expensive%20manually-annotated%20datasets%0Arestricts%20their%20scalability%20and%20broader%20applicability.%20To%20address%20these%20issues%2C%0Aprevious%20studies%20have%20proposed%20pseudo-word%20token-based%20Zero-Shot%20CIR%20%28ZS-CIR%29%0Amethods%2C%20which%20utilize%20a%20projection%20module%20to%20map%20images%20to%20word%20tokens.%0AHowever%2C%20we%20conjecture%20that%20this%20approach%20has%20a%20downside%3A%20the%20projection%20module%0Adistorts%20the%20original%20image%20representation%20and%20confines%20the%20resulting%20composed%0Aembeddings%20to%20the%20text-side.%20In%20order%20to%20resolve%20this%2C%20we%20introduce%20a%20novel%0AZS-CIR%20method%20that%20uses%20Spherical%20Linear%20Interpolation%20%28Slerp%29%20to%20directly%0Amerge%20image%20and%20text%20representations%20by%20identifying%20an%20intermediate%20embedding%0Aof%20both.%20Furthermore%2C%20we%20introduce%20Text-Anchored-Tuning%20%28TAT%29%2C%20a%20method%20that%0Afine-tunes%20the%20image%20encoder%20while%20keeping%20the%20text%20encoder%20fixed.%20TAT%20closes%0Athe%20modality%20gap%20between%20images%20and%20text%2C%20making%20the%20Slerp%20process%20much%20more%0Aeffective.%20Notably%2C%20the%20TAT%20method%20is%20not%20only%20efficient%20in%20terms%20of%20the%20scale%0Aof%20the%20training%20dataset%20and%20training%20time%2C%20but%20it%20also%20serves%20as%20an%20excellent%0Ainitial%20checkpoint%20for%20training%20supervised%20CIR%20models%2C%20thereby%20highlighting%20its%0Awider%20potential.%20The%20integration%20of%20the%20Slerp-based%20ZS-CIR%20with%20a%20TAT-tuned%0Amodel%20enables%20our%20approach%20to%20deliver%20state-of-the-art%20retrieval%20performance%0Aacross%20CIR%20benchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00571v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spherical%20Linear%20Interpolation%20and%20Text-Anchoring%20for%20Zero-shot%20Composed%0A%20%20Image%20Retrieval&entry.906535625=Young%20Kyun%20Jang%20and%20Dat%20Huynh%20and%20Ashish%20Shah%20and%20Wen-Kai%20Chen%20and%20Ser-Nam%20Lim&entry.1292438233=%20%20Composed%20Image%20Retrieval%20%28CIR%29%20is%20a%20complex%20task%20that%20retrieves%20images%20using%0Aa%20query%2C%20which%20is%20configured%20with%20an%20image%20and%20a%20caption%20that%20describes%20desired%0Amodifications%20to%20that%20image.%20Supervised%20CIR%20approaches%20have%20shown%20strong%0Aperformance%2C%20but%20their%20reliance%20on%20expensive%20manually-annotated%20datasets%0Arestricts%20their%20scalability%20and%20broader%20applicability.%20To%20address%20these%20issues%2C%0Aprevious%20studies%20have%20proposed%20pseudo-word%20token-based%20Zero-Shot%20CIR%20%28ZS-CIR%29%0Amethods%2C%20which%20utilize%20a%20projection%20module%20to%20map%20images%20to%20word%20tokens.%0AHowever%2C%20we%20conjecture%20that%20this%20approach%20has%20a%20downside%3A%20the%20projection%20module%0Adistorts%20the%20original%20image%20representation%20and%20confines%20the%20resulting%20composed%0Aembeddings%20to%20the%20text-side.%20In%20order%20to%20resolve%20this%2C%20we%20introduce%20a%20novel%0AZS-CIR%20method%20that%20uses%20Spherical%20Linear%20Interpolation%20%28Slerp%29%20to%20directly%0Amerge%20image%20and%20text%20representations%20by%20identifying%20an%20intermediate%20embedding%0Aof%20both.%20Furthermore%2C%20we%20introduce%20Text-Anchored-Tuning%20%28TAT%29%2C%20a%20method%20that%0Afine-tunes%20the%20image%20encoder%20while%20keeping%20the%20text%20encoder%20fixed.%20TAT%20closes%0Athe%20modality%20gap%20between%20images%20and%20text%2C%20making%20the%20Slerp%20process%20much%20more%0Aeffective.%20Notably%2C%20the%20TAT%20method%20is%20not%20only%20efficient%20in%20terms%20of%20the%20scale%0Aof%20the%20training%20dataset%20and%20training%20time%2C%20but%20it%20also%20serves%20as%20an%20excellent%0Ainitial%20checkpoint%20for%20training%20supervised%20CIR%20models%2C%20thereby%20highlighting%20its%0Awider%20potential.%20The%20integration%20of%20the%20Slerp-based%20ZS-CIR%20with%20a%20TAT-tuned%0Amodel%20enables%20our%20approach%20to%20deliver%20state-of-the-art%20retrieval%20performance%0Aacross%20CIR%20benchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00571v1&entry.124074799=Read"},
{"title": "UWAFA-GAN: Ultra-Wide-Angle Fluorescein Angiography Transformation via\n  Multi-scale Generation and Registration Enhancement", "author": "Ruiquan Ge and Zhaojie Fang and Pengxue Wei and Zhanghao Chen and Hongyang Jiang and Ahmed Elazab and Wangting Li and Xiang Wan and Shaochong Zhang and Changmiao Wang", "abstract": "  Fundus photography, in combination with the ultra-wide-angle fundus (UWF)\ntechniques, becomes an indispensable diagnostic tool in clinical settings by\noffering a more comprehensive view of the retina. Nonetheless, UWF fluorescein\nangiography (UWF-FA) necessitates the administration of a fluorescent dye via\ninjection into the patient's hand or elbow unlike UWF scanning laser\nophthalmoscopy (UWF-SLO). To mitigate potential adverse effects associated with\ninjections, researchers have proposed the development of cross-modality medical\nimage generation algorithms capable of converting UWF-SLO images into their\nUWF-FA counterparts. Current image generation techniques applied to fundus\nphotography encounter difficulties in producing high-resolution retinal images,\nparticularly in capturing minute vascular lesions. To address these issues, we\nintroduce a novel conditional generative adversarial network (UWAFA-GAN) to\nsynthesize UWF-FA from UWF-SLO. This approach employs multi-scale generators\nand an attention transmit module to efficiently extract both global structures\nand local lesions. Additionally, to counteract the image blurriness issue that\narises from training with misaligned data, a registration module is integrated\nwithin this framework. Our method performs non-trivially on inception scores\nand details generation. Clinical user studies further indicate that the UWF-FA\nimages generated by UWAFA-GAN are clinically comparable to authentic images in\nterms of diagnostic reliability. Empirical evaluations on our proprietary UWF\nimage datasets elucidate that UWAFA-GAN outperforms extant methodologies. The\ncode is accessible at https://github.com/Tinysqua/UWAFA-GAN.\n", "link": "http://arxiv.org/abs/2405.00542v1", "date": "2024-05-01", "relevancy": 2.7231, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5574}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5383}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5382}], "mailto": "mailto:daeR=997470421.yrtne&1v24500.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.NAG-AFAWU/auqsyniT/moc.buhtig//A3%sptth02%ta02%elbissecca02%si02%edocA0%ehT02%.seigolodohtem02%tnatxe02%smrofreptuo02%NAG-AFAWU02%taht02%etadicule02%stesatad02%egamiA0%FWU02%yrateirporp02%ruo02%no02%snoitaulave02%laciripmE02%.ytilibailer02%citsongaid02%fo02%smretA0%ni02%segami02%citnehtua02%ot02%elbarapmoc02%yllacinilc02%era02%NAG-AFAWU02%yb02%detareneg02%segamiA0%AF-FWU02%eht02%taht02%etacidni02%rehtruf02%seiduts02%resu02%lacinilC02%.noitareneg02%sliated02%dnaA0%serocs02%noitpecni02%no02%yllaivirt-non02%smrofrep02%dohtem02%ruO02%.krowemarf02%siht02%nihtiwA0%detargetni02%si02%eludom02%noitartsiger02%a02%C2%atad02%dengilasim02%htiw02%gniniart02%morf02%sesiraA0%taht02%eussi02%ssenirrulb02%egami02%eht02%tcaretnuoc02%ot02%C2%yllanoitiddA02%.snoisel02%lacol02%dnaA0%serutcurts02%labolg02%htob02%tcartxe02%yltneiciffe02%ot02%eludom02%timsnart02%noitnetta02%na02%dnaA0%srotareneg02%elacs-itlum02%syolpme02%hcaorppa02%sihT02%.OLS-FWU02%morf02%AF-FWU02%ezisehtnysA0%ot02%92%NAG-AFAWU82%02%krowten02%lairasrevda02%evitareneg02%lanoitidnoc02%levon02%a02%ecudortniA0%ew02%C2%seussi02%eseht02%sserdda02%oT02%.snoisel02%ralucsav02%etunim02%gnirutpac02%ni02%ylralucitrapA0%C2%segami02%laniter02%noituloser-hgih02%gnicudorp02%ni02%seitluciffid02%retnuocne02%yhpargotohpA0%sudnuf02%ot02%deilppa02%seuqinhcet02%noitareneg02%egami02%tnerruC02%.strapretnuoc02%AF-FWUA0%rieht02%otni02%segami02%OLS-FWU02%gnitrevnoc02%fo02%elbapac02%smhtirogla02%noitareneg02%egamiA0%lacidem02%ytiladom-ssorc02%fo02%tnempoleved02%eht02%desoporp02%evah02%srehcraeser02%C2%snoitcejniA0%htiw02%detaicossa02%stceffe02%esrevda02%laitnetop02%etagitim02%oT02%.92%OLS-FWU82%02%ypocsomlahthpoA0%resal02%gninnacs02%FWU02%ekilnu02%woble02%ro02%dnah02%s72%tneitap02%eht02%otni02%noitcejniA0%aiv02%eyd02%tnecseroulf02%a02%fo02%noitartsinimda02%eht02%setatissecen02%92%AF-FWU82%02%yhpargoignaA0%niecseroulf02%FWU02%C2%sselehtenoN02%.aniter02%eht02%fo02%weiv02%evisneherpmoc02%erom02%a02%gnireffoA0%yb02%sgnittes02%lacinilc02%ni02%loot02%citsongaid02%elbasnepsidni02%na02%semoceb02%C2%seuqinhcetA0%92%FWU82%02%sudnuf02%elgna-ediw-artlu02%eht02%htiw02%noitanibmoc02%ni02%C2%yhpargotohp02%sudnuF02%02%=3328342921.yrtne&gnaW02%oaimgnahC02%dna02%gnahZ02%gnohcoahS02%dna02%naW02%gnaiX02%dna02%iL02%gnitgnaW02%dna02%bazalE02%demhA02%dna02%gnaiJ02%gnaygnoH02%dna02%nehC02%oahgnahZ02%dna02%ieW02%euxgneP02%dna02%gnaF02%eijoahZ02%dna02%eG02%nauqiuR=526535609.yrtne&tnemecnahnE02%noitartsigeR02%dna02%noitareneG02%elacs-itluM02%02%A0%aiv02%noitamrofsnarT02%yhpargoignA02%niecseroulF02%elgnA-ediW-artlU02%A3%NAG-AFAWU=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20UWAFA-GAN%3A%20Ultra-Wide-Angle%20Fluorescein%20Angiography%20Transformation%20via%0A%20%20Multi-scale%20Generation%20and%20Registration%20Enhancement&body=Title%3A%20UWAFA-GAN%3A%20Ultra-Wide-Angle%20Fluorescein%20Angiography%20Transformation%20via%0A%20%20Multi-scale%20Generation%20and%20Registration%20Enhancement%0AAuthor%3A%20Ruiquan%20Ge%20and%20Zhaojie%20Fang%20and%20Pengxue%20Wei%20and%20Zhanghao%20Chen%20and%20Hongyang%20Jiang%20and%20Ahmed%20Elazab%20and%20Wangting%20Li%20and%20Xiang%20Wan%20and%20Shaochong%20Zhang%20and%20Changmiao%20Wang%0AAbstract%3A%20%20%20Fundus%20photography%2C%20in%20combination%20with%20the%20ultra-wide-angle%20fundus%20%28UWF%29%0Atechniques%2C%20becomes%20an%20indispensable%20diagnostic%20tool%20in%20clinical%20settings%20by%0Aoffering%20a%20more%20comprehensive%20view%20of%20the%20retina.%20Nonetheless%2C%20UWF%20fluorescein%0Aangiography%20%28UWF-FA%29%20necessitates%20the%20administration%20of%20a%20fluorescent%20dye%20via%0Ainjection%20into%20the%20patient%27s%20hand%20or%20elbow%20unlike%20UWF%20scanning%20laser%0Aophthalmoscopy%20%28UWF-SLO%29.%20To%20mitigate%20potential%20adverse%20effects%20associated%20with%0Ainjections%2C%20researchers%20have%20proposed%20the%20development%20of%20cross-modality%20medical%0Aimage%20generation%20algorithms%20capable%20of%20converting%20UWF-SLO%20images%20into%20their%0AUWF-FA%20counterparts.%20Current%20image%20generation%20techniques%20applied%20to%20fundus%0Aphotography%20encounter%20difficulties%20in%20producing%20high-resolution%20retinal%20images%2C%0Aparticularly%20in%20capturing%20minute%20vascular%20lesions.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20novel%20conditional%20generative%20adversarial%20network%20%28UWAFA-GAN%29%20to%0Asynthesize%20UWF-FA%20from%20UWF-SLO.%20This%20approach%20employs%20multi-scale%20generators%0Aand%20an%20attention%20transmit%20module%20to%20efficiently%20extract%20both%20global%20structures%0Aand%20local%20lesions.%20Additionally%2C%20to%20counteract%20the%20image%20blurriness%20issue%20that%0Aarises%20from%20training%20with%20misaligned%20data%2C%20a%20registration%20module%20is%20integrated%0Awithin%20this%20framework.%20Our%20method%20performs%20non-trivially%20on%20inception%20scores%0Aand%20details%20generation.%20Clinical%20user%20studies%20further%20indicate%20that%20the%20UWF-FA%0Aimages%20generated%20by%20UWAFA-GAN%20are%20clinically%20comparable%20to%20authentic%20images%20in%0Aterms%20of%20diagnostic%20reliability.%20Empirical%20evaluations%20on%20our%20proprietary%20UWF%0Aimage%20datasets%20elucidate%20that%20UWAFA-GAN%20outperforms%20extant%20methodologies.%20The%0Acode%20is%20accessible%20at%20https%3A//github.com/Tinysqua/UWAFA-GAN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00542v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UWAFA-GAN%3A%20Ultra-Wide-Angle%20Fluorescein%20Angiography%20Transformation%20via%0A%20%20Multi-scale%20Generation%20and%20Registration%20Enhancement&entry.906535625=Ruiquan%20Ge%20and%20Zhaojie%20Fang%20and%20Pengxue%20Wei%20and%20Zhanghao%20Chen%20and%20Hongyang%20Jiang%20and%20Ahmed%20Elazab%20and%20Wangting%20Li%20and%20Xiang%20Wan%20and%20Shaochong%20Zhang%20and%20Changmiao%20Wang&entry.1292438233=%20%20Fundus%20photography%2C%20in%20combination%20with%20the%20ultra-wide-angle%20fundus%20%28UWF%29%0Atechniques%2C%20becomes%20an%20indispensable%20diagnostic%20tool%20in%20clinical%20settings%20by%0Aoffering%20a%20more%20comprehensive%20view%20of%20the%20retina.%20Nonetheless%2C%20UWF%20fluorescein%0Aangiography%20%28UWF-FA%29%20necessitates%20the%20administration%20of%20a%20fluorescent%20dye%20via%0Ainjection%20into%20the%20patient%27s%20hand%20or%20elbow%20unlike%20UWF%20scanning%20laser%0Aophthalmoscopy%20%28UWF-SLO%29.%20To%20mitigate%20potential%20adverse%20effects%20associated%20with%0Ainjections%2C%20researchers%20have%20proposed%20the%20development%20of%20cross-modality%20medical%0Aimage%20generation%20algorithms%20capable%20of%20converting%20UWF-SLO%20images%20into%20their%0AUWF-FA%20counterparts.%20Current%20image%20generation%20techniques%20applied%20to%20fundus%0Aphotography%20encounter%20difficulties%20in%20producing%20high-resolution%20retinal%20images%2C%0Aparticularly%20in%20capturing%20minute%20vascular%20lesions.%20To%20address%20these%20issues%2C%20we%0Aintroduce%20a%20novel%20conditional%20generative%20adversarial%20network%20%28UWAFA-GAN%29%20to%0Asynthesize%20UWF-FA%20from%20UWF-SLO.%20This%20approach%20employs%20multi-scale%20generators%0Aand%20an%20attention%20transmit%20module%20to%20efficiently%20extract%20both%20global%20structures%0Aand%20local%20lesions.%20Additionally%2C%20to%20counteract%20the%20image%20blurriness%20issue%20that%0Aarises%20from%20training%20with%20misaligned%20data%2C%20a%20registration%20module%20is%20integrated%0Awithin%20this%20framework.%20Our%20method%20performs%20non-trivially%20on%20inception%20scores%0Aand%20details%20generation.%20Clinical%20user%20studies%20further%20indicate%20that%20the%20UWF-FA%0Aimages%20generated%20by%20UWAFA-GAN%20are%20clinically%20comparable%20to%20authentic%20images%20in%0Aterms%20of%20diagnostic%20reliability.%20Empirical%20evaluations%20on%20our%20proprietary%20UWF%0Aimage%20datasets%20elucidate%20that%20UWAFA-GAN%20outperforms%20extant%20methodologies.%20The%0Acode%20is%20accessible%20at%20https%3A//github.com/Tinysqua/UWAFA-GAN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00542v1&entry.124074799=Read"},
{"title": "Learning to Compose: Improving Object Centric Learning by Injecting\n  Compositionality", "author": "Whie Jung and Jaehoon Yoo and Sungjin Ahn and Seunghoon Hong", "abstract": "  Learning compositional representation is a key aspect of object-centric\nlearning as it enables flexible systematic generalization and supports complex\nvisual reasoning. However, most of the existing approaches rely on\nauto-encoding objective, while the compositionality is implicitly imposed by\nthe architectural or algorithmic bias in the encoder. This misalignment between\nauto-encoding objective and learning compositionality often results in failure\nof capturing meaningful object representations. In this study, we propose a\nnovel objective that explicitly encourages compositionality of the\nrepresentations. Built upon the existing object-centric learning framework\n(e.g., slot attention), our method incorporates additional constraints that an\narbitrary mixture of object representations from two images should be valid by\nmaximizing the likelihood of the composite data. We demonstrate that\nincorporating our objective to the existing framework consistently improves the\nobjective-centric learning and enhances the robustness to the architectural\nchoices.\n", "link": "http://arxiv.org/abs/2405.00646v1", "date": "2024-05-01", "relevancy": 2.7141, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.6092}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5226}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4967}], "mailto": "mailto:daeR=997470421.yrtne&1v64600.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.seciohcA0%larutcetihcra02%eht02%ot02%ssentsubor02%eht02%secnahne02%dna02%gninrael02%cirtnec-evitcejboA0%eht02%sevorpmi02%yltnetsisnoc02%krowemarf02%gnitsixe02%eht02%ot02%evitcejbo02%ruo02%gnitaroprocniA0%taht02%etartsnomed02%eW02%.atad02%etisopmoc02%eht02%fo02%doohilekil02%eht02%gnizimixamA0%yb02%dilav02%eb02%dluohs02%segami02%owt02%morf02%snoitatneserper02%tcejbo02%fo02%erutxim02%yrartibraA0%na02%taht02%stniartsnoc02%lanoitidda02%setaroprocni02%dohtem02%ruo02%C2%92%noitnetta02%tols02%C2%.g.e82%A0%krowemarf02%gninrael02%cirtnec-tcejbo02%gnitsixe02%eht02%nopu02%tliuB02%.snoitatneserperA0%eht02%fo02%ytilanoitisopmoc02%segaruocne02%ylticilpxe02%taht02%evitcejbo02%levonA0%a02%esoporp02%ew02%C2%yduts02%siht02%nI02%.snoitatneserper02%tcejbo02%lufgninaem02%gnirutpac02%foA0%eruliaf02%ni02%stluser02%netfo02%ytilanoitisopmoc02%gninrael02%dna02%evitcejbo02%gnidocne-otuaA0%neewteb02%tnemngilasim02%sihT02%.redocne02%eht02%ni02%saib02%cimhtirogla02%ro02%larutcetihcra02%ehtA0%yb02%desopmi02%ylticilpmi02%si02%ytilanoitisopmoc02%eht02%elihw02%C2%evitcejbo02%gnidocne-otuaA0%no02%yler02%sehcaorppa02%gnitsixe02%eht02%fo02%tsom02%C2%revewoH02%.gninosaer02%lausivA0%xelpmoc02%stroppus02%dna02%noitazilareneg02%citametsys02%elbixelf02%selbane02%ti02%sa02%gninraelA0%cirtnec-tcejbo02%fo02%tcepsa02%yek02%a02%si02%noitatneserper02%lanoitisopmoc02%gninraeL02%02%=3328342921.yrtne&gnoH02%noohgnueS02%dna02%nhA02%nijgnuS02%dna02%ooY02%nooheaJ02%dna02%gnuJ02%eihW=526535609.yrtne&ytilanoitisopmoC02%02%A0%gnitcejnI02%yb02%gninraeL02%cirtneC02%tcejbO02%gnivorpmI02%A3%esopmoC02%ot02%gninraeL=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Compose%3A%20Improving%20Object%20Centric%20Learning%20by%20Injecting%0A%20%20Compositionality&body=Title%3A%20Learning%20to%20Compose%3A%20Improving%20Object%20Centric%20Learning%20by%20Injecting%0A%20%20Compositionality%0AAuthor%3A%20Whie%20Jung%20and%20Jaehoon%20Yoo%20and%20Sungjin%20Ahn%20and%20Seunghoon%20Hong%0AAbstract%3A%20%20%20Learning%20compositional%20representation%20is%20a%20key%20aspect%20of%20object-centric%0Alearning%20as%20it%20enables%20flexible%20systematic%20generalization%20and%20supports%20complex%0Avisual%20reasoning.%20However%2C%20most%20of%20the%20existing%20approaches%20rely%20on%0Aauto-encoding%20objective%2C%20while%20the%20compositionality%20is%20implicitly%20imposed%20by%0Athe%20architectural%20or%20algorithmic%20bias%20in%20the%20encoder.%20This%20misalignment%20between%0Aauto-encoding%20objective%20and%20learning%20compositionality%20often%20results%20in%20failure%0Aof%20capturing%20meaningful%20object%20representations.%20In%20this%20study%2C%20we%20propose%20a%0Anovel%20objective%20that%20explicitly%20encourages%20compositionality%20of%20the%0Arepresentations.%20Built%20upon%20the%20existing%20object-centric%20learning%20framework%0A%28e.g.%2C%20slot%20attention%29%2C%20our%20method%20incorporates%20additional%20constraints%20that%20an%0Aarbitrary%20mixture%20of%20object%20representations%20from%20two%20images%20should%20be%20valid%20by%0Amaximizing%20the%20likelihood%20of%20the%20composite%20data.%20We%20demonstrate%20that%0Aincorporating%20our%20objective%20to%20the%20existing%20framework%20consistently%20improves%20the%0Aobjective-centric%20learning%20and%20enhances%20the%20robustness%20to%20the%20architectural%0Achoices.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00646v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Compose%3A%20Improving%20Object%20Centric%20Learning%20by%20Injecting%0A%20%20Compositionality&entry.906535625=Whie%20Jung%20and%20Jaehoon%20Yoo%20and%20Sungjin%20Ahn%20and%20Seunghoon%20Hong&entry.1292438233=%20%20Learning%20compositional%20representation%20is%20a%20key%20aspect%20of%20object-centric%0Alearning%20as%20it%20enables%20flexible%20systematic%20generalization%20and%20supports%20complex%0Avisual%20reasoning.%20However%2C%20most%20of%20the%20existing%20approaches%20rely%20on%0Aauto-encoding%20objective%2C%20while%20the%20compositionality%20is%20implicitly%20imposed%20by%0Athe%20architectural%20or%20algorithmic%20bias%20in%20the%20encoder.%20This%20misalignment%20between%0Aauto-encoding%20objective%20and%20learning%20compositionality%20often%20results%20in%20failure%0Aof%20capturing%20meaningful%20object%20representations.%20In%20this%20study%2C%20we%20propose%20a%0Anovel%20objective%20that%20explicitly%20encourages%20compositionality%20of%20the%0Arepresentations.%20Built%20upon%20the%20existing%20object-centric%20learning%20framework%0A%28e.g.%2C%20slot%20attention%29%2C%20our%20method%20incorporates%20additional%20constraints%20that%20an%0Aarbitrary%20mixture%20of%20object%20representations%20from%20two%20images%20should%20be%20valid%20by%0Amaximizing%20the%20likelihood%20of%20the%20composite%20data.%20We%20demonstrate%20that%0Aincorporating%20our%20objective%20to%20the%20existing%20framework%20consistently%20improves%20the%0Aobjective-centric%20learning%20and%20enhances%20the%20robustness%20to%20the%20architectural%0Achoices.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00646v1&entry.124074799=Read"},
{"title": "HandSSCA: 3D Hand Mesh Reconstruction with State Space Channel Attention\n  from RGB images", "author": "Zixun Jiao and Xihan Wang and Quanli Gao", "abstract": "  Reconstructing a hand mesh from a single RGB image is a challenging task\nbecause hands are often occluded by objects. Most previous works attempted to\nintroduce more additional information and adopt attention mechanisms to improve\n3D reconstruction results, but it would increased computational complexity.\nThis observation prompts us to propose a new and concise architecture while\nimproving computational efficiency. In this work, we propose a simple and\neffective 3D hand mesh reconstruction network HandSSCA, which is the first to\nincorporate state space modeling into the field of hand pose estimation. In the\nnetwork, we have designed a novel state space channel attention module that\nextends the effective sensory field, extracts hand features in the spatial\ndimension, and enhances hand regional features in the channel dimension. This\ndesign helps to reconstruct a complete and detailed hand mesh. Extensive\nexperiments conducted on well-known datasets featuring challenging hand-object\nocclusions (such as FREIHAND, DEXYCB, and HO3D) demonstrate that our proposed\nHandSSCA achieves state-of-the-art performance while maintaining a minimal\nparameter count.\n", "link": "http://arxiv.org/abs/2405.01066v1", "date": "2024-05-02", "relevancy": 2.6906, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5547}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.54}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5197}], "mailto": "mailto:daeR=997470421.yrtne&1v66010.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.tnuoc02%retemarapA0%laminim02%a02%gniniatniam02%elihw02%ecnamrofrep02%tra-eht-fo-etats02%seveihca02%ACSSdnaHA0%desoporp02%ruo02%taht02%etartsnomed02%92%D3OH02%dna02%C2%BCYXED02%C2%DNAHIERF02%sa02%hcus82%02%snoisulccoA0%tcejbo-dnah02%gnignellahc02%gnirutaef02%stesatad02%nwonk-llew02%no02%detcudnoc02%stnemirepxeA0%evisnetxE02%.hsem02%dnah02%deliated02%dna02%etelpmoc02%a02%tcurtsnocer02%ot02%spleh02%ngisedA0%sihT02%.noisnemid02%lennahc02%eht02%ni02%serutaef02%lanoiger02%dnah02%secnahne02%dna02%C2%noisnemidA0%laitaps02%eht02%ni02%serutaef02%dnah02%stcartxe02%C2%dleif02%yrosnes02%evitceffe02%eht02%sdnetxeA0%taht02%eludom02%noitnetta02%lennahc02%ecaps02%etats02%levon02%a02%dengised02%evah02%ew02%C2%krowtenA0%eht02%nI02%.noitamitse02%esop02%dnah02%fo02%dleif02%eht02%otni02%gniledom02%ecaps02%etats02%etaroprocniA0%ot02%tsrif02%eht02%si02%hcihw02%C2%ACSSdnaH02%krowten02%noitcurtsnocer02%hsem02%dnah02%D302%evitceffeA0%dna02%elpmis02%a02%esoporp02%ew02%C2%krow02%siht02%nI02%.ycneiciffe02%lanoitatupmoc02%gnivorpmiA0%elihw02%erutcetihcra02%esicnoc02%dna02%wen02%a02%esoporp02%ot02%su02%stpmorp02%noitavresbo02%sihTA0%.ytixelpmoc02%lanoitatupmoc02%desaercni02%dluow02%ti02%tub02%C2%stluser02%noitcurtsnocer02%D3A0%evorpmi02%ot02%smsinahcem02%noitnetta02%tpoda02%dna02%noitamrofni02%lanoitidda02%erom02%ecudortniA0%ot02%detpmetta02%skrow02%suoiverp02%tsoM02%.stcejbo02%yb02%dedulcco02%netfo02%era02%sdnah02%esuacebA0%ksat02%gnignellahc02%a02%si02%egami02%BGR02%elgnis02%a02%morf02%hsem02%dnah02%a02%gnitcurtsnoceR02%02%=3328342921.yrtne&oaG02%ilnauQ02%dna02%gnaW02%nahiX02%dna02%oaiJ02%nuxiZ=526535609.yrtne&segami02%BGR02%morf02%02%A0%noitnettA02%lennahC02%ecapS02%etatS02%htiw02%noitcurtsnoceR02%hseM02%dnaH02%D302%A3%ACSSdnaH=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20HandSSCA%3A%203D%20Hand%20Mesh%20Reconstruction%20with%20State%20Space%20Channel%20Attention%0A%20%20from%20RGB%20images&body=Title%3A%20HandSSCA%3A%203D%20Hand%20Mesh%20Reconstruction%20with%20State%20Space%20Channel%20Attention%0A%20%20from%20RGB%20images%0AAuthor%3A%20Zixun%20Jiao%20and%20Xihan%20Wang%20and%20Quanli%20Gao%0AAbstract%3A%20%20%20Reconstructing%20a%20hand%20mesh%20from%20a%20single%20RGB%20image%20is%20a%20challenging%20task%0Abecause%20hands%20are%20often%20occluded%20by%20objects.%20Most%20previous%20works%20attempted%20to%0Aintroduce%20more%20additional%20information%20and%20adopt%20attention%20mechanisms%20to%20improve%0A3D%20reconstruction%20results%2C%20but%20it%20would%20increased%20computational%20complexity.%0AThis%20observation%20prompts%20us%20to%20propose%20a%20new%20and%20concise%20architecture%20while%0Aimproving%20computational%20efficiency.%20In%20this%20work%2C%20we%20propose%20a%20simple%20and%0Aeffective%203D%20hand%20mesh%20reconstruction%20network%20HandSSCA%2C%20which%20is%20the%20first%20to%0Aincorporate%20state%20space%20modeling%20into%20the%20field%20of%20hand%20pose%20estimation.%20In%20the%0Anetwork%2C%20we%20have%20designed%20a%20novel%20state%20space%20channel%20attention%20module%20that%0Aextends%20the%20effective%20sensory%20field%2C%20extracts%20hand%20features%20in%20the%20spatial%0Adimension%2C%20and%20enhances%20hand%20regional%20features%20in%20the%20channel%20dimension.%20This%0Adesign%20helps%20to%20reconstruct%20a%20complete%20and%20detailed%20hand%20mesh.%20Extensive%0Aexperiments%20conducted%20on%20well-known%20datasets%20featuring%20challenging%20hand-object%0Aocclusions%20%28such%20as%20FREIHAND%2C%20DEXYCB%2C%20and%20HO3D%29%20demonstrate%20that%20our%20proposed%0AHandSSCA%20achieves%20state-of-the-art%20performance%20while%20maintaining%20a%20minimal%0Aparameter%20count.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01066v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HandSSCA%3A%203D%20Hand%20Mesh%20Reconstruction%20with%20State%20Space%20Channel%20Attention%0A%20%20from%20RGB%20images&entry.906535625=Zixun%20Jiao%20and%20Xihan%20Wang%20and%20Quanli%20Gao&entry.1292438233=%20%20Reconstructing%20a%20hand%20mesh%20from%20a%20single%20RGB%20image%20is%20a%20challenging%20task%0Abecause%20hands%20are%20often%20occluded%20by%20objects.%20Most%20previous%20works%20attempted%20to%0Aintroduce%20more%20additional%20information%20and%20adopt%20attention%20mechanisms%20to%20improve%0A3D%20reconstruction%20results%2C%20but%20it%20would%20increased%20computational%20complexity.%0AThis%20observation%20prompts%20us%20to%20propose%20a%20new%20and%20concise%20architecture%20while%0Aimproving%20computational%20efficiency.%20In%20this%20work%2C%20we%20propose%20a%20simple%20and%0Aeffective%203D%20hand%20mesh%20reconstruction%20network%20HandSSCA%2C%20which%20is%20the%20first%20to%0Aincorporate%20state%20space%20modeling%20into%20the%20field%20of%20hand%20pose%20estimation.%20In%20the%0Anetwork%2C%20we%20have%20designed%20a%20novel%20state%20space%20channel%20attention%20module%20that%0Aextends%20the%20effective%20sensory%20field%2C%20extracts%20hand%20features%20in%20the%20spatial%0Adimension%2C%20and%20enhances%20hand%20regional%20features%20in%20the%20channel%20dimension.%20This%0Adesign%20helps%20to%20reconstruct%20a%20complete%20and%20detailed%20hand%20mesh.%20Extensive%0Aexperiments%20conducted%20on%20well-known%20datasets%20featuring%20challenging%20hand-object%0Aocclusions%20%28such%20as%20FREIHAND%2C%20DEXYCB%2C%20and%20HO3D%29%20demonstrate%20that%20our%20proposed%0AHandSSCA%20achieves%20state-of-the-art%20performance%20while%20maintaining%20a%20minimal%0Aparameter%20count.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01066v1&entry.124074799=Read"},
{"title": "Robust Decentralized Learning with Local Updates and Gradient Tracking", "author": "Sajjad Ghiasvand and Amirhossein Reisizadeh and Mahnoosh Alizadeh and Ramtin Pedarsani", "abstract": "  As distributed learning applications such as Federated Learning, the Internet\nof Things (IoT), and Edge Computing grow, it is critical to address the\nshortcomings of such technologies from a theoretical perspective. As an\nabstraction, we consider decentralized learning over a network of communicating\nclients or nodes and tackle two major challenges: data heterogeneity and\nadversarial robustness. We propose a decentralized minimax optimization method\nthat employs two important modules: local updates and gradient tracking.\nMinimax optimization is the key tool to enable adversarial training for\nensuring robustness. Having local updates is essential in Federated Learning\n(FL) applications to mitigate the communication bottleneck, and utilizing\ngradient tracking is essential to proving convergence in the case of data\nheterogeneity. We analyze the performance of the proposed algorithm,\nDec-FedTrack, in the case of nonconvex-strongly concave minimax optimization,\nand prove that it converges a stationary point. We also conduct numerical\nexperiments to support our theoretical findings.\n", "link": "http://arxiv.org/abs/2405.00965v1", "date": "2024-05-02", "relevancy": 2.6777, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5548}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5324}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5194}], "mailto": "mailto:daeR=997470421.yrtne&1v56900.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.sgnidnif02%laciteroeht02%ruo02%troppus02%ot02%stnemirepxeA0%laciremun02%tcudnoc02%osla02%eW02%.tniop02%yranoitats02%a02%segrevnoc02%ti02%taht02%evorp02%dnaA0%C2%noitazimitpo02%xaminim02%evacnoc02%ylgnorts-xevnocnon02%fo02%esac02%eht02%ni02%C2%kcarTdeF-ceDA0%C2%mhtirogla02%desoporp02%eht02%fo02%ecnamrofrep02%eht02%ezylana02%eW02%.ytienegoretehA0%atad02%fo02%esac02%eht02%ni02%ecnegrevnoc02%gnivorp02%ot02%laitnesse02%si02%gnikcart02%tneidargA0%gnizilitu02%dna02%C2%kcenelttob02%noitacinummoc02%eht02%etagitim02%ot02%snoitacilppa02%92%LF82%A0%gninraeL02%detaredeF02%ni02%laitnesse02%si02%setadpu02%lacol02%gnivaH02%.ssentsubor02%gnirusneA0%rof02%gniniart02%lairasrevda02%elbane02%ot02%loot02%yek02%eht02%si02%noitazimitpo02%xaminiMA0%.gnikcart02%tneidarg02%dna02%setadpu02%lacol02%A3%seludom02%tnatropmi02%owt02%syolpme02%tahtA0%dohtem02%noitazimitpo02%xaminim02%dezilartneced02%a02%esoporp02%eW02%.ssentsubor02%lairasrevdaA0%dna02%ytienegoreteh02%atad02%A3%segnellahc02%rojam02%owt02%elkcat02%dna02%sedon02%ro02%stneilcA0%gnitacinummoc02%fo02%krowten02%a02%revo02%gninrael02%dezilartneced02%redisnoc02%ew02%C2%noitcartsbaA0%na02%sA02%.evitcepsrep02%laciteroeht02%a02%morf02%seigolonhcet02%hcus02%fo02%sgnimoctrohsA0%eht02%sserdda02%ot02%lacitirc02%si02%ti02%C2%worg02%gnitupmoC02%egdE02%dna02%C2%92%ToI82%02%sgnihT02%foA0%tenretnI02%eht02%C2%gninraeL02%detaredeF02%sa02%hcus02%snoitacilppa02%gninrael02%detubirtsid02%sA02%02%=3328342921.yrtne&inasradeP02%nitmaR02%dna02%hedazilA02%hsoonhaM02%dna02%hedazisieR02%niessohrimA02%dna02%dnavsaihG02%dajjaS=526535609.yrtne&gnikcarT02%tneidarG02%dna02%setadpU02%lacoL02%htiw02%gninraeL02%dezilartneceD02%tsuboR=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Robust%20Decentralized%20Learning%20with%20Local%20Updates%20and%20Gradient%20Tracking&body=Title%3A%20Robust%20Decentralized%20Learning%20with%20Local%20Updates%20and%20Gradient%20Tracking%0AAuthor%3A%20Sajjad%20Ghiasvand%20and%20Amirhossein%20Reisizadeh%20and%20Mahnoosh%20Alizadeh%20and%20Ramtin%20Pedarsani%0AAbstract%3A%20%20%20As%20distributed%20learning%20applications%20such%20as%20Federated%20Learning%2C%20the%20Internet%0Aof%20Things%20%28IoT%29%2C%20and%20Edge%20Computing%20grow%2C%20it%20is%20critical%20to%20address%20the%0Ashortcomings%20of%20such%20technologies%20from%20a%20theoretical%20perspective.%20As%20an%0Aabstraction%2C%20we%20consider%20decentralized%20learning%20over%20a%20network%20of%20communicating%0Aclients%20or%20nodes%20and%20tackle%20two%20major%20challenges%3A%20data%20heterogeneity%20and%0Aadversarial%20robustness.%20We%20propose%20a%20decentralized%20minimax%20optimization%20method%0Athat%20employs%20two%20important%20modules%3A%20local%20updates%20and%20gradient%20tracking.%0AMinimax%20optimization%20is%20the%20key%20tool%20to%20enable%20adversarial%20training%20for%0Aensuring%20robustness.%20Having%20local%20updates%20is%20essential%20in%20Federated%20Learning%0A%28FL%29%20applications%20to%20mitigate%20the%20communication%20bottleneck%2C%20and%20utilizing%0Agradient%20tracking%20is%20essential%20to%20proving%20convergence%20in%20the%20case%20of%20data%0Aheterogeneity.%20We%20analyze%20the%20performance%20of%20the%20proposed%20algorithm%2C%0ADec-FedTrack%2C%20in%20the%20case%20of%20nonconvex-strongly%20concave%20minimax%20optimization%2C%0Aand%20prove%20that%20it%20converges%20a%20stationary%20point.%20We%20also%20conduct%20numerical%0Aexperiments%20to%20support%20our%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00965v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Decentralized%20Learning%20with%20Local%20Updates%20and%20Gradient%20Tracking&entry.906535625=Sajjad%20Ghiasvand%20and%20Amirhossein%20Reisizadeh%20and%20Mahnoosh%20Alizadeh%20and%20Ramtin%20Pedarsani&entry.1292438233=%20%20As%20distributed%20learning%20applications%20such%20as%20Federated%20Learning%2C%20the%20Internet%0Aof%20Things%20%28IoT%29%2C%20and%20Edge%20Computing%20grow%2C%20it%20is%20critical%20to%20address%20the%0Ashortcomings%20of%20such%20technologies%20from%20a%20theoretical%20perspective.%20As%20an%0Aabstraction%2C%20we%20consider%20decentralized%20learning%20over%20a%20network%20of%20communicating%0Aclients%20or%20nodes%20and%20tackle%20two%20major%20challenges%3A%20data%20heterogeneity%20and%0Aadversarial%20robustness.%20We%20propose%20a%20decentralized%20minimax%20optimization%20method%0Athat%20employs%20two%20important%20modules%3A%20local%20updates%20and%20gradient%20tracking.%0AMinimax%20optimization%20is%20the%20key%20tool%20to%20enable%20adversarial%20training%20for%0Aensuring%20robustness.%20Having%20local%20updates%20is%20essential%20in%20Federated%20Learning%0A%28FL%29%20applications%20to%20mitigate%20the%20communication%20bottleneck%2C%20and%20utilizing%0Agradient%20tracking%20is%20essential%20to%20proving%20convergence%20in%20the%20case%20of%20data%0Aheterogeneity.%20We%20analyze%20the%20performance%20of%20the%20proposed%20algorithm%2C%0ADec-FedTrack%2C%20in%20the%20case%20of%20nonconvex-strongly%20concave%20minimax%20optimization%2C%0Aand%20prove%20that%20it%20converges%20a%20stationary%20point.%20We%20also%20conduct%20numerical%0Aexperiments%20to%20support%20our%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00965v1&entry.124074799=Read"},
{"title": "CLIP4STR: A Simple Baseline for Scene Text Recognition with Pre-trained\n  Vision-Language Model", "author": "Shuai Zhao and Ruijie Quan and Linchao Zhu and Yi Yang", "abstract": "  Pre-trained vision-language models~(VLMs) are the de-facto foundation models\nfor various downstream tasks. However, scene text recognition methods still\nprefer backbones pre-trained on a single modality, namely, the visual modality,\ndespite the potential of VLMs to serve as powerful scene text readers. For\nexample, CLIP can robustly identify regular (horizontal) and irregular\n(rotated, curved, blurred, or occluded) text in images. With such merits, we\ntransform CLIP into a scene text reader and introduce CLIP4STR, a simple yet\neffective STR method built upon image and text encoders of CLIP. It has two\nencoder-decoder branches: a visual branch and a cross-modal branch. The visual\nbranch provides an initial prediction based on the visual feature, and the\ncross-modal branch refines this prediction by addressing the discrepancy\nbetween the visual feature and text semantics. To fully leverage the\ncapabilities of both branches, we design a dual predict-and-refine decoding\nscheme for inference. We scale CLIP4STR in terms of the model size,\npre-training data, and training data, achieving state-of-the-art performance on\n11 STR benchmarks. Additionally, a comprehensive empirical study is provided to\nenhance the understanding of the adaptation of CLIP to STR. We believe our\nmethod establishes a simple yet strong baseline for future STR research with\nVLMs.\n", "link": "http://arxiv.org/abs/2305.14014v3", "date": "2024-05-02", "relevancy": 2.6735, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.604}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5072}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4929}], "mailto": "mailto:daeR=997470421.yrtne&3v41041.5032/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.sMLVA0%htiw02%hcraeser02%RTS02%erutuf02%rof02%enilesab02%gnorts02%tey02%elpmis02%a02%sehsilbatse02%dohtemA0%ruo02%eveileb02%eW02%.RTS02%ot02%PILC02%fo02%noitatpada02%eht02%fo02%gnidnatsrednu02%eht02%ecnahneA0%ot02%dedivorp02%si02%yduts02%laciripme02%evisneherpmoc02%a02%C2%yllanoitiddA02%.skramhcneb02%RTS02%11A0%no02%ecnamrofrep02%tra-eht-fo-etats02%gniveihca02%C2%atad02%gniniart02%dna02%C2%atad02%gniniart-erpA0%C2%ezis02%ledom02%eht02%fo02%smret02%ni02%RTS4PILC02%elacs02%eW02%.ecnerefni02%rof02%emehcsA0%gnidoced02%enifer-dna-tciderp02%laud02%a02%ngised02%ew02%C2%sehcnarb02%htob02%fo02%seitilibapacA0%eht02%egarevel02%ylluf02%oT02%.scitnames02%txet02%dna02%erutaef02%lausiv02%eht02%neewtebA0%ycnapercsid02%eht02%gnisserdda02%yb02%noitciderp02%siht02%senifer02%hcnarb02%ladom-ssorcA0%eht02%dna02%C2%erutaef02%lausiv02%eht02%no02%desab02%noitciderp02%laitini02%na02%sedivorp02%hcnarbA0%lausiv02%ehT02%.hcnarb02%ladom-ssorc02%a02%dna02%hcnarb02%lausiv02%a02%A3%sehcnarb02%redoced-redocneA0%owt02%sah02%tI02%.PILC02%fo02%sredocne02%txet02%dna02%egami02%nopu02%tliub02%dohtem02%RTS02%evitceffeA0%tey02%elpmis02%a02%C2%RTS4PILC02%ecudortni02%dna02%redaer02%txet02%enecs02%a02%otni02%PILC02%mrofsnartA0%ew02%C2%stirem02%hcus02%htiW02%.segami02%ni02%txet02%92%dedulcco02%ro02%C2%derrulb02%C2%devruc02%C2%detator82%A0%ralugerri02%dna02%92%latnoziroh82%02%raluger02%yfitnedi02%yltsubor02%nac02%PILC02%C2%elpmaxeA0%roF02%.sredaer02%txet02%enecs02%lufrewop02%sa02%evres02%ot02%sMLV02%fo02%laitnetop02%eht02%etipsedA0%C2%ytiladom02%lausiv02%eht02%C2%yleman02%C2%ytiladom02%elgnis02%a02%no02%deniart-erp02%senobkcab02%referpA0%llits02%sdohtem02%noitingocer02%txet02%enecs02%C2%revewoH02%.sksat02%maertsnwod02%suoirav02%rofA0%sledom02%noitadnuof02%otcaf-ed02%eht02%era02%92%sMLV82%~sledom02%egaugnal-noisiv02%deniart-erP02%02%=3328342921.yrtne&gnaY02%iY02%dna02%uhZ02%oahcniL02%dna02%nauQ02%eijiuR02%dna02%oahZ02%iauhS=526535609.yrtne&ledoM02%egaugnaL-noisiV02%02%A0%deniart-erP02%htiw02%noitingoceR02%txeT02%enecS02%rof02%enilesaB02%elpmiS02%A02%A3%RTS4PILC=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20CLIP4STR%3A%20A%20Simple%20Baseline%20for%20Scene%20Text%20Recognition%20with%20Pre-trained%0A%20%20Vision-Language%20Model&body=Title%3A%20CLIP4STR%3A%20A%20Simple%20Baseline%20for%20Scene%20Text%20Recognition%20with%20Pre-trained%0A%20%20Vision-Language%20Model%0AAuthor%3A%20Shuai%20Zhao%20and%20Ruijie%20Quan%20and%20Linchao%20Zhu%20and%20Yi%20Yang%0AAbstract%3A%20%20%20Pre-trained%20vision-language%20models~%28VLMs%29%20are%20the%20de-facto%20foundation%20models%0Afor%20various%20downstream%20tasks.%20However%2C%20scene%20text%20recognition%20methods%20still%0Aprefer%20backbones%20pre-trained%20on%20a%20single%20modality%2C%20namely%2C%20the%20visual%20modality%2C%0Adespite%20the%20potential%20of%20VLMs%20to%20serve%20as%20powerful%20scene%20text%20readers.%20For%0Aexample%2C%20CLIP%20can%20robustly%20identify%20regular%20%28horizontal%29%20and%20irregular%0A%28rotated%2C%20curved%2C%20blurred%2C%20or%20occluded%29%20text%20in%20images.%20With%20such%20merits%2C%20we%0Atransform%20CLIP%20into%20a%20scene%20text%20reader%20and%20introduce%20CLIP4STR%2C%20a%20simple%20yet%0Aeffective%20STR%20method%20built%20upon%20image%20and%20text%20encoders%20of%20CLIP.%20It%20has%20two%0Aencoder-decoder%20branches%3A%20a%20visual%20branch%20and%20a%20cross-modal%20branch.%20The%20visual%0Abranch%20provides%20an%20initial%20prediction%20based%20on%20the%20visual%20feature%2C%20and%20the%0Across-modal%20branch%20refines%20this%20prediction%20by%20addressing%20the%20discrepancy%0Abetween%20the%20visual%20feature%20and%20text%20semantics.%20To%20fully%20leverage%20the%0Acapabilities%20of%20both%20branches%2C%20we%20design%20a%20dual%20predict-and-refine%20decoding%0Ascheme%20for%20inference.%20We%20scale%20CLIP4STR%20in%20terms%20of%20the%20model%20size%2C%0Apre-training%20data%2C%20and%20training%20data%2C%20achieving%20state-of-the-art%20performance%20on%0A11%20STR%20benchmarks.%20Additionally%2C%20a%20comprehensive%20empirical%20study%20is%20provided%20to%0Aenhance%20the%20understanding%20of%20the%20adaptation%20of%20CLIP%20to%20STR.%20We%20believe%20our%0Amethod%20establishes%20a%20simple%20yet%20strong%20baseline%20for%20future%20STR%20research%20with%0AVLMs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.14014v3%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP4STR%3A%20A%20Simple%20Baseline%20for%20Scene%20Text%20Recognition%20with%20Pre-trained%0A%20%20Vision-Language%20Model&entry.906535625=Shuai%20Zhao%20and%20Ruijie%20Quan%20and%20Linchao%20Zhu%20and%20Yi%20Yang&entry.1292438233=%20%20Pre-trained%20vision-language%20models~%28VLMs%29%20are%20the%20de-facto%20foundation%20models%0Afor%20various%20downstream%20tasks.%20However%2C%20scene%20text%20recognition%20methods%20still%0Aprefer%20backbones%20pre-trained%20on%20a%20single%20modality%2C%20namely%2C%20the%20visual%20modality%2C%0Adespite%20the%20potential%20of%20VLMs%20to%20serve%20as%20powerful%20scene%20text%20readers.%20For%0Aexample%2C%20CLIP%20can%20robustly%20identify%20regular%20%28horizontal%29%20and%20irregular%0A%28rotated%2C%20curved%2C%20blurred%2C%20or%20occluded%29%20text%20in%20images.%20With%20such%20merits%2C%20we%0Atransform%20CLIP%20into%20a%20scene%20text%20reader%20and%20introduce%20CLIP4STR%2C%20a%20simple%20yet%0Aeffective%20STR%20method%20built%20upon%20image%20and%20text%20encoders%20of%20CLIP.%20It%20has%20two%0Aencoder-decoder%20branches%3A%20a%20visual%20branch%20and%20a%20cross-modal%20branch.%20The%20visual%0Abranch%20provides%20an%20initial%20prediction%20based%20on%20the%20visual%20feature%2C%20and%20the%0Across-modal%20branch%20refines%20this%20prediction%20by%20addressing%20the%20discrepancy%0Abetween%20the%20visual%20feature%20and%20text%20semantics.%20To%20fully%20leverage%20the%0Acapabilities%20of%20both%20branches%2C%20we%20design%20a%20dual%20predict-and-refine%20decoding%0Ascheme%20for%20inference.%20We%20scale%20CLIP4STR%20in%20terms%20of%20the%20model%20size%2C%0Apre-training%20data%2C%20and%20training%20data%2C%20achieving%20state-of-the-art%20performance%20on%0A11%20STR%20benchmarks.%20Additionally%2C%20a%20comprehensive%20empirical%20study%20is%20provided%20to%0Aenhance%20the%20understanding%20of%20the%20adaptation%20of%20CLIP%20to%20STR.%20We%20believe%20our%0Amethod%20establishes%20a%20simple%20yet%20strong%20baseline%20for%20future%20STR%20research%20with%0AVLMs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.14014v3&entry.124074799=Read"},
{"title": "Single Image Super-Resolution Based on Global-Local Information Synergy", "author": "Nianzu Qiao and Lamei Di and Changyin Sun", "abstract": "  Although several image super-resolution solutions exist, they still face many\nchallenges. CNN-based algorithms, despite the reduction in computational\ncomplexity, still need to improve their accuracy. While Transformer-based\nalgorithms have higher accuracy, their ultra-high computational complexity\nmakes them difficult to be accepted in practical applications. To overcome the\nexisting challenges, a novel super-resolution reconstruction algorithm is\nproposed in this paper. The algorithm achieves a significant increase in\naccuracy through a unique design while maintaining a low complexity. The core\nof the algorithm lies in its cleverly designed Global-Local Information\nExtraction Module and Basic Block Module. By combining global and local\ninformation, the Global-Local Information Extraction Module aims to understand\nthe image content more comprehensively so as to recover the global structure\nand local details in the image more accurately, which provides rich information\nsupport for the subsequent reconstruction process. Experimental results show\nthat the comprehensive performance of the algorithm proposed in this paper is\noptimal, providing an efficient and practical new solution in the field of\nsuper-resolution reconstruction.\n", "link": "http://arxiv.org/abs/2405.01085v1", "date": "2024-05-02", "relevancy": 2.6616, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5386}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5316}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5267}], "mailto": "mailto:daeR=997470421.yrtne&1v58010.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.noitcurtsnocer02%noituloser-repusA0%fo02%dleif02%eht02%ni02%noitulos02%wen02%lacitcarp02%dna02%tneiciffe02%na02%gnidivorp02%C2%lamitpoA0%si02%repap02%siht02%ni02%desoporp02%mhtirogla02%eht02%fo02%ecnamrofrep02%evisneherpmoc02%eht02%tahtA0%wohs02%stluser02%latnemirepxE02%.ssecorp02%noitcurtsnocer02%tneuqesbus02%eht02%rof02%troppusA0%noitamrofni02%hcir02%sedivorp02%hcihw02%C2%yletarucca02%erom02%egami02%eht02%ni02%sliated02%lacol02%dnaA0%erutcurts02%labolg02%eht02%revocer02%ot02%sa02%os02%ylevisneherpmoc02%erom02%tnetnoc02%egami02%ehtA0%dnatsrednu02%ot02%smia02%eludoM02%noitcartxE02%noitamrofnI02%lacoL-labolG02%eht02%C2%noitamrofniA0%lacol02%dna02%labolg02%gninibmoc02%yB02%.eludoM02%kcolB02%cisaB02%dna02%eludoM02%noitcartxEA0%noitamrofnI02%lacoL-labolG02%dengised02%ylrevelc02%sti02%ni02%seil02%mhtirogla02%eht02%foA0%eroc02%ehT02%.ytixelpmoc02%wol02%a02%gniniatniam02%elihw02%ngised02%euqinu02%a02%hguorht02%ycaruccaA0%ni02%esaercni02%tnacifingis02%a02%seveihca02%mhtirogla02%ehT02%.repap02%siht02%ni02%desoporpA0%si02%mhtirogla02%noitcurtsnocer02%noituloser-repus02%levon02%a02%C2%segnellahc02%gnitsixeA0%eht02%emocrevo02%oT02%.snoitacilppa02%lacitcarp02%ni02%detpecca02%eb02%ot02%tluciffid02%meht02%sekamA0%ytixelpmoc02%lanoitatupmoc02%hgih-artlu02%rieht02%C2%ycarucca02%rehgih02%evah02%smhtiroglaA0%desab-remrofsnarT02%elihW02%.ycarucca02%rieht02%evorpmi02%ot02%deen02%llits02%C2%ytixelpmocA0%lanoitatupmoc02%ni02%noitcuder02%eht02%etipsed02%C2%smhtirogla02%desab-NNC02%.segnellahcA0%ynam02%ecaf02%llits02%yeht02%C2%tsixe02%snoitulos02%noituloser-repus02%egami02%lareves02%hguohtlA02%02%=3328342921.yrtne&nuS02%niygnahC02%dna02%iD02%iemaL02%dna02%oaiQ02%uznaiN=526535609.yrtne&ygrenyS02%noitamrofnI02%lacoL-labolG02%no02%desaB02%noituloseR-repuS02%egamI02%elgniS=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Single%20Image%20Super-Resolution%20Based%20on%20Global-Local%20Information%20Synergy&body=Title%3A%20Single%20Image%20Super-Resolution%20Based%20on%20Global-Local%20Information%20Synergy%0AAuthor%3A%20Nianzu%20Qiao%20and%20Lamei%20Di%20and%20Changyin%20Sun%0AAbstract%3A%20%20%20Although%20several%20image%20super-resolution%20solutions%20exist%2C%20they%20still%20face%20many%0Achallenges.%20CNN-based%20algorithms%2C%20despite%20the%20reduction%20in%20computational%0Acomplexity%2C%20still%20need%20to%20improve%20their%20accuracy.%20While%20Transformer-based%0Aalgorithms%20have%20higher%20accuracy%2C%20their%20ultra-high%20computational%20complexity%0Amakes%20them%20difficult%20to%20be%20accepted%20in%20practical%20applications.%20To%20overcome%20the%0Aexisting%20challenges%2C%20a%20novel%20super-resolution%20reconstruction%20algorithm%20is%0Aproposed%20in%20this%20paper.%20The%20algorithm%20achieves%20a%20significant%20increase%20in%0Aaccuracy%20through%20a%20unique%20design%20while%20maintaining%20a%20low%20complexity.%20The%20core%0Aof%20the%20algorithm%20lies%20in%20its%20cleverly%20designed%20Global-Local%20Information%0AExtraction%20Module%20and%20Basic%20Block%20Module.%20By%20combining%20global%20and%20local%0Ainformation%2C%20the%20Global-Local%20Information%20Extraction%20Module%20aims%20to%20understand%0Athe%20image%20content%20more%20comprehensively%20so%20as%20to%20recover%20the%20global%20structure%0Aand%20local%20details%20in%20the%20image%20more%20accurately%2C%20which%20provides%20rich%20information%0Asupport%20for%20the%20subsequent%20reconstruction%20process.%20Experimental%20results%20show%0Athat%20the%20comprehensive%20performance%20of%20the%20algorithm%20proposed%20in%20this%20paper%20is%0Aoptimal%2C%20providing%20an%20efficient%20and%20practical%20new%20solution%20in%20the%20field%20of%0Asuper-resolution%20reconstruction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01085v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Single%20Image%20Super-Resolution%20Based%20on%20Global-Local%20Information%20Synergy&entry.906535625=Nianzu%20Qiao%20and%20Lamei%20Di%20and%20Changyin%20Sun&entry.1292438233=%20%20Although%20several%20image%20super-resolution%20solutions%20exist%2C%20they%20still%20face%20many%0Achallenges.%20CNN-based%20algorithms%2C%20despite%20the%20reduction%20in%20computational%0Acomplexity%2C%20still%20need%20to%20improve%20their%20accuracy.%20While%20Transformer-based%0Aalgorithms%20have%20higher%20accuracy%2C%20their%20ultra-high%20computational%20complexity%0Amakes%20them%20difficult%20to%20be%20accepted%20in%20practical%20applications.%20To%20overcome%20the%0Aexisting%20challenges%2C%20a%20novel%20super-resolution%20reconstruction%20algorithm%20is%0Aproposed%20in%20this%20paper.%20The%20algorithm%20achieves%20a%20significant%20increase%20in%0Aaccuracy%20through%20a%20unique%20design%20while%20maintaining%20a%20low%20complexity.%20The%20core%0Aof%20the%20algorithm%20lies%20in%20its%20cleverly%20designed%20Global-Local%20Information%0AExtraction%20Module%20and%20Basic%20Block%20Module.%20By%20combining%20global%20and%20local%0Ainformation%2C%20the%20Global-Local%20Information%20Extraction%20Module%20aims%20to%20understand%0Athe%20image%20content%20more%20comprehensively%20so%20as%20to%20recover%20the%20global%20structure%0Aand%20local%20details%20in%20the%20image%20more%20accurately%2C%20which%20provides%20rich%20information%0Asupport%20for%20the%20subsequent%20reconstruction%20process.%20Experimental%20results%20show%0Athat%20the%20comprehensive%20performance%20of%20the%20algorithm%20proposed%20in%20this%20paper%20is%0Aoptimal%2C%20providing%20an%20efficient%20and%20practical%20new%20solution%20in%20the%20field%20of%0Asuper-resolution%20reconstruction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01085v1&entry.124074799=Read"},
{"title": "Controllable Text Generation in the Instruction-Tuning Era", "author": "Dhananjay Ashok and Barnabas Poczos", "abstract": "  While most research on controllable text generation has focused on steering\nbase Language Models, the emerging instruction-tuning and prompting paradigm\noffers an alternate approach to controllability. We compile and release\nConGenBench, a testbed of 17 different controllable generation tasks, using a\nsubset of it to benchmark the performance of 9 different baselines and methods\non Instruction-tuned Language Models. To our surprise, we find that\nprompting-based approaches outperform controllable text generation methods on\nmost datasets and tasks, highlighting a need for research on controllable text\ngeneration with Instruction-tuned Language Models in specific. Prompt-based\napproaches match human performance on most stylistic tasks while lagging on\nstructural tasks, foregrounding a need to study more varied constraints and\nmore challenging stylistic tasks. To facilitate such research, we provide an\nalgorithm that uses only a task dataset and a Large Language Model with\nin-context capabilities to automatically generate a constraint dataset. This\nmethod eliminates the fields dependence on pre-curated constraint datasets,\nhence vastly expanding the range of constraints that can be studied in the\nfuture.\n", "link": "http://arxiv.org/abs/2405.01490v1", "date": "2024-05-02", "relevancy": 2.6568, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5692}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5418}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4831}], "mailto": "mailto:daeR=997470421.yrtne&1v09410.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.erutufA0%eht02%ni02%deiduts02%eb02%nac02%taht02%stniartsnoc02%fo02%egnar02%eht02%gnidnapxe02%yltsav02%ecnehA0%C2%stesatad02%tniartsnoc02%detaruc-erp02%no02%ecnedneped02%sdleif02%eht02%setanimile02%dohtemA0%sihT02%.tesatad02%tniartsnoc02%a02%etareneg02%yllacitamotua02%ot02%seitilibapac02%txetnoc-niA0%htiw02%ledoM02%egaugnaL02%egraL02%a02%dna02%tesatad02%ksat02%a02%ylno02%sesu02%taht02%mhtiroglaA0%na02%edivorp02%ew02%C2%hcraeser02%hcus02%etatilicaf02%oT02%.sksat02%citsilyts02%gnignellahc02%eromA0%dna02%stniartsnoc02%deirav02%erom02%yduts02%ot02%deen02%a02%gnidnuorgerof02%C2%sksat02%larutcurtsA0%no02%gniggal02%elihw02%sksat02%citsilyts02%tsom02%no02%ecnamrofrep02%namuh02%hctam02%sehcaorppaA0%desab-tpmorP02%.cificeps02%ni02%sledoM02%egaugnaL02%denut-noitcurtsnI02%htiw02%noitarenegA0%txet02%elballortnoc02%no02%hcraeser02%rof02%deen02%a02%gnithgilhgih02%C2%sksat02%dna02%stesatad02%tsomA0%no02%sdohtem02%noitareneg02%txet02%elballortnoc02%mrofreptuo02%sehcaorppa02%desab-gnitpmorpA0%taht02%dnif02%ew02%C2%esirprus02%ruo02%oT02%.sledoM02%egaugnaL02%denut-noitcurtsnI02%noA0%sdohtem02%dna02%senilesab02%tnereffid02%902%fo02%ecnamrofrep02%eht02%kramhcneb02%ot02%ti02%fo02%tesbusA0%a02%gnisu02%C2%sksat02%noitareneg02%elballortnoc02%tnereffid02%7102%fo02%debtset02%a02%C2%hcneBneGnoCA0%esaeler02%dna02%elipmoc02%eW02%.ytiliballortnoc02%ot02%hcaorppa02%etanretla02%na02%sreffoA0%mgidarap02%gnitpmorp02%dna02%gninut-noitcurtsni02%gnigreme02%eht02%C2%sledoM02%egaugnaL02%esabA0%gnireets02%no02%desucof02%sah02%noitareneg02%txet02%elballortnoc02%no02%hcraeser02%tsom02%elihW02%02%=3328342921.yrtne&sozcoP02%sabanraB02%dna02%kohsA02%yajnanahD=526535609.yrtne&arE02%gninuT-noitcurtsnI02%eht02%ni02%noitareneG02%txeT02%elballortnoC=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Controllable%20Text%20Generation%20in%20the%20Instruction-Tuning%20Era&body=Title%3A%20Controllable%20Text%20Generation%20in%20the%20Instruction-Tuning%20Era%0AAuthor%3A%20Dhananjay%20Ashok%20and%20Barnabas%20Poczos%0AAbstract%3A%20%20%20While%20most%20research%20on%20controllable%20text%20generation%20has%20focused%20on%20steering%0Abase%20Language%20Models%2C%20the%20emerging%20instruction-tuning%20and%20prompting%20paradigm%0Aoffers%20an%20alternate%20approach%20to%20controllability.%20We%20compile%20and%20release%0AConGenBench%2C%20a%20testbed%20of%2017%20different%20controllable%20generation%20tasks%2C%20using%20a%0Asubset%20of%20it%20to%20benchmark%20the%20performance%20of%209%20different%20baselines%20and%20methods%0Aon%20Instruction-tuned%20Language%20Models.%20To%20our%20surprise%2C%20we%20find%20that%0Aprompting-based%20approaches%20outperform%20controllable%20text%20generation%20methods%20on%0Amost%20datasets%20and%20tasks%2C%20highlighting%20a%20need%20for%20research%20on%20controllable%20text%0Ageneration%20with%20Instruction-tuned%20Language%20Models%20in%20specific.%20Prompt-based%0Aapproaches%20match%20human%20performance%20on%20most%20stylistic%20tasks%20while%20lagging%20on%0Astructural%20tasks%2C%20foregrounding%20a%20need%20to%20study%20more%20varied%20constraints%20and%0Amore%20challenging%20stylistic%20tasks.%20To%20facilitate%20such%20research%2C%20we%20provide%20an%0Aalgorithm%20that%20uses%20only%20a%20task%20dataset%20and%20a%20Large%20Language%20Model%20with%0Ain-context%20capabilities%20to%20automatically%20generate%20a%20constraint%20dataset.%20This%0Amethod%20eliminates%20the%20fields%20dependence%20on%20pre-curated%20constraint%20datasets%2C%0Ahence%20vastly%20expanding%20the%20range%20of%20constraints%20that%20can%20be%20studied%20in%20the%0Afuture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01490v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Controllable%20Text%20Generation%20in%20the%20Instruction-Tuning%20Era&entry.906535625=Dhananjay%20Ashok%20and%20Barnabas%20Poczos&entry.1292438233=%20%20While%20most%20research%20on%20controllable%20text%20generation%20has%20focused%20on%20steering%0Abase%20Language%20Models%2C%20the%20emerging%20instruction-tuning%20and%20prompting%20paradigm%0Aoffers%20an%20alternate%20approach%20to%20controllability.%20We%20compile%20and%20release%0AConGenBench%2C%20a%20testbed%20of%2017%20different%20controllable%20generation%20tasks%2C%20using%20a%0Asubset%20of%20it%20to%20benchmark%20the%20performance%20of%209%20different%20baselines%20and%20methods%0Aon%20Instruction-tuned%20Language%20Models.%20To%20our%20surprise%2C%20we%20find%20that%0Aprompting-based%20approaches%20outperform%20controllable%20text%20generation%20methods%20on%0Amost%20datasets%20and%20tasks%2C%20highlighting%20a%20need%20for%20research%20on%20controllable%20text%0Ageneration%20with%20Instruction-tuned%20Language%20Models%20in%20specific.%20Prompt-based%0Aapproaches%20match%20human%20performance%20on%20most%20stylistic%20tasks%20while%20lagging%20on%0Astructural%20tasks%2C%20foregrounding%20a%20need%20to%20study%20more%20varied%20constraints%20and%0Amore%20challenging%20stylistic%20tasks.%20To%20facilitate%20such%20research%2C%20we%20provide%20an%0Aalgorithm%20that%20uses%20only%20a%20task%20dataset%20and%20a%20Large%20Language%20Model%20with%0Ain-context%20capabilities%20to%20automatically%20generate%20a%20constraint%20dataset.%20This%0Amethod%20eliminates%20the%20fields%20dependence%20on%20pre-curated%20constraint%20datasets%2C%0Ahence%20vastly%20expanding%20the%20range%20of%20constraints%20that%20can%20be%20studied%20in%20the%0Afuture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01490v1&entry.124074799=Read"},
{"title": "The Perception-Robustness Tradeoff in Deterministic Image Restoration", "author": "Guy Ohayon and Tomer Michaeli and Michael Elad", "abstract": "  We study the behavior of deterministic methods for solving inverse problems\nin imaging. These methods are commonly designed to achieve two goals: (1)\nattaining high perceptual quality, and (2) generating reconstructions that are\nconsistent with the measurements. We provide a rigorous proof that the better a\npredictor satisfies these two requirements, the larger its Lipschitz constant\nmust be, regardless of the nature of the degradation involved. In particular,\nto approach perfect perceptual quality and perfect consistency, the Lipschitz\nconstant of the model must grow to infinity. This implies that such methods are\nnecessarily more susceptible to adversarial attacks. We demonstrate our theory\non single image super-resolution algorithms, addressing both noisy and\nnoiseless settings. We also show how this undesired behavior can be leveraged\nto explore the posterior distribution, thereby allowing the deterministic model\nto imitate stochastic methods.\n", "link": "http://arxiv.org/abs/2311.09253v2", "date": "2024-05-02", "relevancy": 2.6568, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5357}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5346}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5237}], "mailto": "mailto:daeR=997470421.yrtne&2v35290.1132/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.sdohtem02%citsahcots02%etatimi02%otA0%ledom02%citsinimreted02%eht02%gniwolla02%ybereht02%C2%noitubirtsid02%roiretsop02%eht02%erolpxe02%otA0%degarevel02%eb02%nac02%roivaheb02%derisednu02%siht02%woh02%wohs02%osla02%eW02%.sgnittes02%sselesionA0%dna02%ysion02%htob02%gnisserdda02%C2%smhtirogla02%noituloser-repus02%egami02%elgnis02%noA0%yroeht02%ruo02%etartsnomed02%eW02%.skcatta02%lairasrevda02%ot02%elbitpecsus02%erom02%ylirassecenA0%era02%sdohtem02%hcus02%taht02%seilpmi02%sihT02%.ytinifni02%ot02%worg02%tsum02%ledom02%eht02%fo02%tnatsnocA0%ztihcspiL02%eht02%C2%ycnetsisnoc02%tcefrep02%dna02%ytilauq02%lautpecrep02%tcefrep02%hcaorppa02%otA0%C2%ralucitrap02%nI02%.devlovni02%noitadarged02%eht02%fo02%erutan02%eht02%fo02%sseldrager02%C2%eb02%tsumA0%tnatsnoc02%ztihcspiL02%sti02%regral02%eht02%C2%stnemeriuqer02%owt02%eseht02%seifsitas02%rotciderpA0%a02%retteb02%eht02%taht02%foorp02%suorogir02%a02%edivorp02%eW02%.stnemerusaem02%eht02%htiw02%tnetsisnocA0%era02%taht02%snoitcurtsnocer02%gnitareneg02%92%282%02%dna02%C2%ytilauq02%lautpecrep02%hgih02%gniniattaA0%92%182%02%A3%slaog02%owt02%eveihca02%ot02%dengised02%ylnommoc02%era02%sdohtem02%esehT02%.gnigami02%niA0%smelborp02%esrevni02%gnivlos02%rof02%sdohtem02%citsinimreted02%fo02%roivaheb02%eht02%yduts02%eW02%02%=3328342921.yrtne&dalE02%leahciM02%dna02%ileahciM02%remoT02%dna02%noyahO02%yuG=526535609.yrtne&noitarotseR02%egamI02%citsinimreteD02%ni02%ffoedarT02%ssentsuboR-noitpecreP02%ehT=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20The%20Perception-Robustness%20Tradeoff%20in%20Deterministic%20Image%20Restoration&body=Title%3A%20The%20Perception-Robustness%20Tradeoff%20in%20Deterministic%20Image%20Restoration%0AAuthor%3A%20Guy%20Ohayon%20and%20Tomer%20Michaeli%20and%20Michael%20Elad%0AAbstract%3A%20%20%20We%20study%20the%20behavior%20of%20deterministic%20methods%20for%20solving%20inverse%20problems%0Ain%20imaging.%20These%20methods%20are%20commonly%20designed%20to%20achieve%20two%20goals%3A%20%281%29%0Aattaining%20high%20perceptual%20quality%2C%20and%20%282%29%20generating%20reconstructions%20that%20are%0Aconsistent%20with%20the%20measurements.%20We%20provide%20a%20rigorous%20proof%20that%20the%20better%20a%0Apredictor%20satisfies%20these%20two%20requirements%2C%20the%20larger%20its%20Lipschitz%20constant%0Amust%20be%2C%20regardless%20of%20the%20nature%20of%20the%20degradation%20involved.%20In%20particular%2C%0Ato%20approach%20perfect%20perceptual%20quality%20and%20perfect%20consistency%2C%20the%20Lipschitz%0Aconstant%20of%20the%20model%20must%20grow%20to%20infinity.%20This%20implies%20that%20such%20methods%20are%0Anecessarily%20more%20susceptible%20to%20adversarial%20attacks.%20We%20demonstrate%20our%20theory%0Aon%20single%20image%20super-resolution%20algorithms%2C%20addressing%20both%20noisy%20and%0Anoiseless%20settings.%20We%20also%20show%20how%20this%20undesired%20behavior%20can%20be%20leveraged%0Ato%20explore%20the%20posterior%20distribution%2C%20thereby%20allowing%20the%20deterministic%20model%0Ato%20imitate%20stochastic%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.09253v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Perception-Robustness%20Tradeoff%20in%20Deterministic%20Image%20Restoration&entry.906535625=Guy%20Ohayon%20and%20Tomer%20Michaeli%20and%20Michael%20Elad&entry.1292438233=%20%20We%20study%20the%20behavior%20of%20deterministic%20methods%20for%20solving%20inverse%20problems%0Ain%20imaging.%20These%20methods%20are%20commonly%20designed%20to%20achieve%20two%20goals%3A%20%281%29%0Aattaining%20high%20perceptual%20quality%2C%20and%20%282%29%20generating%20reconstructions%20that%20are%0Aconsistent%20with%20the%20measurements.%20We%20provide%20a%20rigorous%20proof%20that%20the%20better%20a%0Apredictor%20satisfies%20these%20two%20requirements%2C%20the%20larger%20its%20Lipschitz%20constant%0Amust%20be%2C%20regardless%20of%20the%20nature%20of%20the%20degradation%20involved.%20In%20particular%2C%0Ato%20approach%20perfect%20perceptual%20quality%20and%20perfect%20consistency%2C%20the%20Lipschitz%0Aconstant%20of%20the%20model%20must%20grow%20to%20infinity.%20This%20implies%20that%20such%20methods%20are%0Anecessarily%20more%20susceptible%20to%20adversarial%20attacks.%20We%20demonstrate%20our%20theory%0Aon%20single%20image%20super-resolution%20algorithms%2C%20addressing%20both%20noisy%20and%0Anoiseless%20settings.%20We%20also%20show%20how%20this%20undesired%20behavior%20can%20be%20leveraged%0Ato%20explore%20the%20posterior%20distribution%2C%20thereby%20allowing%20the%20deterministic%20model%0Ato%20imitate%20stochastic%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.09253v2&entry.124074799=Read"},
{"title": "CromSS: Cross-modal pre-training with noisy labels for remote sensing\n  image segmentation", "author": "Chenying Liu and Conrad Albrecht and Yi Wang and Xiao Xiang Zhu", "abstract": "  We study the potential of noisy labels y to pretrain semantic segmentation\nmodels in a multi-modal learning framework for geospatial applications.\nSpecifically, we propose a novel Cross-modal Sample Selection method (CromSS)\nthat utilizes the class distributions P^{(d)}(x,c) over pixels x and classes c\nmodelled by multiple sensors/modalities d of a given geospatial scene.\nConsistency of predictions across sensors $d$ is jointly informed by the\nentropy of P^{(d)}(x,c). Noisy label sampling we determine by the confidence of\neach sensor d in the noisy class label, P^{(d)}(x,c=y(x)). To verify the\nperformance of our approach, we conduct experiments with Sentinel-1 (radar) and\nSentinel-2 (optical) satellite imagery from the globally-sampled SSL4EO-S12\ndataset. We pair those scenes with 9-class noisy labels sourced from the Google\nDynamic World project for pretraining. Transfer learning evaluations\n(downstream task) on the DFC2020 dataset confirm the effectiveness of the\nproposed method for remote sensing image segmentation.\n", "link": "http://arxiv.org/abs/2405.01217v1", "date": "2024-05-02", "relevancy": 2.6472, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.547}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5234}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.518}], "mailto": "mailto:daeR=997470421.yrtne&1v71210.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.noitatnemges02%egami02%gnisnes02%etomer02%rof02%dohtem02%desoporpA0%eht02%fo02%ssenevitceffe02%eht02%mrifnoc02%tesatad02%0202CFD02%eht02%no02%92%ksat02%maertsnwod82%A0%snoitaulave02%gninrael02%refsnarT02%.gniniarterp02%rof02%tcejorp02%dlroW02%cimanyDA0%elgooG02%eht02%morf02%decruos02%slebal02%ysion02%ssalc-902%htiw02%senecs02%esoht02%riap02%eW02%.tesatadA0%21S-OE4LSS02%delpmas-yllabolg02%eht02%morf02%yregami02%etilletas02%92%lacitpo82%02%2-lenitneSA0%dna02%92%radar82%02%1-lenitneS02%htiw02%stnemirepxe02%tcudnoc02%ew02%C2%hcaorppa02%ruo02%fo02%ecnamrofrepA0%eht02%yfirev02%oT02%.92%92%x82%yD3%cC2%x82%D7%92%d82%B7%E5%P02%C2%lebal02%ssalc02%ysion02%eht02%ni02%d02%rosnes02%hcaeA0%fo02%ecnedifnoc02%eht02%yb02%enimreted02%ew02%gnilpmas02%lebal02%ysioN02%.92%cC2%x82%D7%92%d82%B7%E5%P02%fo02%yportneA0%eht02%yb02%demrofni02%yltnioj02%si02%42%d42%02%srosnes02%ssorca02%snoitciderp02%fo02%ycnetsisnoCA0%.enecs02%laitapsoeg02%nevig02%a02%fo02%d02%seitiladom/srosnes02%elpitlum02%yb02%delledomA0%c02%sessalc02%dna02%x02%slexip02%revo02%92%cC2%x82%D7%92%d82%B7%E5%P02%snoitubirtsid02%ssalc02%eht02%sezilitu02%tahtA0%92%SSmorC82%02%dohtem02%noitceleS02%elpmaS02%ladom-ssorC02%levon02%a02%esoporp02%ew02%C2%yllacificepSA0%.snoitacilppa02%laitapsoeg02%rof02%krowemarf02%gninrael02%ladom-itlum02%a02%ni02%sledomA0%noitatnemges02%citnames02%niarterp02%ot02%y02%slebal02%ysion02%fo02%laitnetop02%eht02%yduts02%eW02%02%=3328342921.yrtne&uhZ02%gnaiX02%oaiX02%dna02%gnaW02%iY02%dna02%thcerblA02%darnoC02%dna02%uiL02%gniynehC=526535609.yrtne&noitatnemges02%egami02%02%A0%gnisnes02%etomer02%rof02%slebal02%ysion02%htiw02%gniniart-erp02%ladom-ssorC02%A3%SSmorC=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20CromSS%3A%20Cross-modal%20pre-training%20with%20noisy%20labels%20for%20remote%20sensing%0A%20%20image%20segmentation&body=Title%3A%20CromSS%3A%20Cross-modal%20pre-training%20with%20noisy%20labels%20for%20remote%20sensing%0A%20%20image%20segmentation%0AAuthor%3A%20Chenying%20Liu%20and%20Conrad%20Albrecht%20and%20Yi%20Wang%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20%20%20We%20study%20the%20potential%20of%20noisy%20labels%20y%20to%20pretrain%20semantic%20segmentation%0Amodels%20in%20a%20multi-modal%20learning%20framework%20for%20geospatial%20applications.%0ASpecifically%2C%20we%20propose%20a%20novel%20Cross-modal%20Sample%20Selection%20method%20%28CromSS%29%0Athat%20utilizes%20the%20class%20distributions%20P%5E%7B%28d%29%7D%28x%2Cc%29%20over%20pixels%20x%20and%20classes%20c%0Amodelled%20by%20multiple%20sensors/modalities%20d%20of%20a%20given%20geospatial%20scene.%0AConsistency%20of%20predictions%20across%20sensors%20%24d%24%20is%20jointly%20informed%20by%20the%0Aentropy%20of%20P%5E%7B%28d%29%7D%28x%2Cc%29.%20Noisy%20label%20sampling%20we%20determine%20by%20the%20confidence%20of%0Aeach%20sensor%20d%20in%20the%20noisy%20class%20label%2C%20P%5E%7B%28d%29%7D%28x%2Cc%3Dy%28x%29%29.%20To%20verify%20the%0Aperformance%20of%20our%20approach%2C%20we%20conduct%20experiments%20with%20Sentinel-1%20%28radar%29%20and%0ASentinel-2%20%28optical%29%20satellite%20imagery%20from%20the%20globally-sampled%20SSL4EO-S12%0Adataset.%20We%20pair%20those%20scenes%20with%209-class%20noisy%20labels%20sourced%20from%20the%20Google%0ADynamic%20World%20project%20for%20pretraining.%20Transfer%20learning%20evaluations%0A%28downstream%20task%29%20on%20the%20DFC2020%20dataset%20confirm%20the%20effectiveness%20of%20the%0Aproposed%20method%20for%20remote%20sensing%20image%20segmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01217v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CromSS%3A%20Cross-modal%20pre-training%20with%20noisy%20labels%20for%20remote%20sensing%0A%20%20image%20segmentation&entry.906535625=Chenying%20Liu%20and%20Conrad%20Albrecht%20and%20Yi%20Wang%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=%20%20We%20study%20the%20potential%20of%20noisy%20labels%20y%20to%20pretrain%20semantic%20segmentation%0Amodels%20in%20a%20multi-modal%20learning%20framework%20for%20geospatial%20applications.%0ASpecifically%2C%20we%20propose%20a%20novel%20Cross-modal%20Sample%20Selection%20method%20%28CromSS%29%0Athat%20utilizes%20the%20class%20distributions%20P%5E%7B%28d%29%7D%28x%2Cc%29%20over%20pixels%20x%20and%20classes%20c%0Amodelled%20by%20multiple%20sensors/modalities%20d%20of%20a%20given%20geospatial%20scene.%0AConsistency%20of%20predictions%20across%20sensors%20%24d%24%20is%20jointly%20informed%20by%20the%0Aentropy%20of%20P%5E%7B%28d%29%7D%28x%2Cc%29.%20Noisy%20label%20sampling%20we%20determine%20by%20the%20confidence%20of%0Aeach%20sensor%20d%20in%20the%20noisy%20class%20label%2C%20P%5E%7B%28d%29%7D%28x%2Cc%3Dy%28x%29%29.%20To%20verify%20the%0Aperformance%20of%20our%20approach%2C%20we%20conduct%20experiments%20with%20Sentinel-1%20%28radar%29%20and%0ASentinel-2%20%28optical%29%20satellite%20imagery%20from%20the%20globally-sampled%20SSL4EO-S12%0Adataset.%20We%20pair%20those%20scenes%20with%209-class%20noisy%20labels%20sourced%20from%20the%20Google%0ADynamic%20World%20project%20for%20pretraining.%20Transfer%20learning%20evaluations%0A%28downstream%20task%29%20on%20the%20DFC2020%20dataset%20confirm%20the%20effectiveness%20of%20the%0Aproposed%20method%20for%20remote%20sensing%20image%20segmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01217v1&entry.124074799=Read"},
{"title": "Compact 3D Scene Representation via Self-Organizing Gaussian Grids", "author": "Wieland Morgenstern and Florian Barthel and Anna Hilsmann and Peter Eisert", "abstract": "  3D Gaussian Splatting has recently emerged as a highly promising technique\nfor modeling of static 3D scenes. In contrast to Neural Radiance Fields, it\nutilizes efficient rasterization allowing for very fast rendering at\nhigh-quality. However, the storage size is significantly higher, which hinders\npractical deployment, e.g. on resource constrained devices. In this paper, we\nintroduce a compact scene representation organizing the parameters of 3D\nGaussian Splatting (3DGS) into a 2D grid with local homogeneity, ensuring a\ndrastic reduction in storage requirements without compromising visual quality\nduring rendering. Central to our idea is the explicit exploitation of\nperceptual redundancies present in natural scenes. In essence, the inherent\nnature of a scene allows for numerous permutations of Gaussian parameters to\nequivalently represent it. To this end, we propose a novel highly parallel\nalgorithm that regularly arranges the high-dimensional Gaussian parameters into\na 2D grid while preserving their neighborhood structure. During training, we\nfurther enforce local smoothness between the sorted parameters in the grid. The\nuncompressed Gaussians use the same structure as 3DGS, ensuring a seamless\nintegration with established renderers. Our method achieves a reduction factor\nof 17x to 42x in size for complex scenes with no increase in training time,\nmarking a substantial leap forward in the domain of 3D scene distribution and\nconsumption. Additional information can be found on our project page:\nhttps://fraunhoferhhi.github.io/Self-Organizing-Gaussians/\n", "link": "http://arxiv.org/abs/2312.13299v2", "date": "2024-05-02", "relevancy": 2.6342, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7224}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6589}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4979}], "mailto": "mailto:daeR=997470421.yrtne&2v99231.2132/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%/snaissuaG-gnizinagrO-fleS/oi.buhtig.ihhrefohnuarf//A3%sptthA0%A3%egap02%tcejorp02%ruo02%no02%dnuof02%eb02%nac02%noitamrofni02%lanoitiddA02%.noitpmusnocA0%dna02%noitubirtsid02%enecs02%D302%fo02%niamod02%eht02%ni02%drawrof02%pael02%laitnatsbus02%a02%gnikramA0%C2%emit02%gniniart02%ni02%esaercni02%on02%htiw02%senecs02%xelpmoc02%rof02%ezis02%ni02%x2402%ot02%x7102%foA0%rotcaf02%noitcuder02%a02%seveihca02%dohtem02%ruO02%.sreredner02%dehsilbatse02%htiw02%noitargetniA0%sselmaes02%a02%gnirusne02%C2%SGD302%sa02%erutcurts02%emas02%eht02%esu02%snaissuaG02%desserpmocnuA0%ehT02%.dirg02%eht02%ni02%sretemarap02%detros02%eht02%neewteb02%ssenhtooms02%lacol02%ecrofne02%rehtrufA0%ew02%C2%gniniart02%gniruD02%.erutcurts02%doohrobhgien02%rieht02%gnivreserp02%elihw02%dirg02%D202%aA0%otni02%sretemarap02%naissuaG02%lanoisnemid-hgih02%eht02%segnarra02%ylraluger02%taht02%mhtiroglaA0%lellarap02%ylhgih02%levon02%a02%esoporp02%ew02%C2%dne02%siht02%oT02%.ti02%tneserper02%yltnelaviuqeA0%ot02%sretemarap02%naissuaG02%fo02%snoitatumrep02%suoremun02%rof02%swolla02%enecs02%a02%fo02%erutanA0%tnerehni02%eht02%C2%ecnesse02%nI02%.senecs02%larutan02%ni02%tneserp02%seicnadnuder02%lautpecrepA0%fo02%noitatiolpxe02%ticilpxe02%eht02%si02%aedi02%ruo02%ot02%lartneC02%.gniredner02%gnirudA0%ytilauq02%lausiv02%gnisimorpmoc02%tuohtiw02%stnemeriuqer02%egarots02%ni02%noitcuder02%citsardA0%a02%gnirusne02%C2%ytienegomoh02%lacol02%htiw02%dirg02%D202%a02%otni02%92%SGD382%02%gnittalpS02%naissuaGA0%D302%fo02%sretemarap02%eht02%gnizinagro02%noitatneserper02%enecs02%tcapmoc02%a02%ecudortniA0%ew02%C2%repap02%siht02%nI02%.secived02%deniartsnoc02%ecruoser02%no02%.g.e02%C2%tnemyolped02%lacitcarpA0%srednih02%hcihw02%C2%rehgih02%yltnacifingis02%si02%ezis02%egarots02%eht02%C2%revewoH02%.ytilauq-hgihA0%ta02%gniredner02%tsaf02%yrev02%rof02%gniwolla02%noitaziretsar02%tneiciffe02%sezilituA0%ti02%C2%sdleiF02%ecnaidaR02%larueN02%ot02%tsartnoc02%nI02%.senecs02%D302%citats02%fo02%gniledom02%rofA0%euqinhcet02%gnisimorp02%ylhgih02%a02%sa02%degreme02%yltnecer02%sah02%gnittalpS02%naissuaG02%D302%02%=3328342921.yrtne&tresiE02%reteP02%dna02%nnamsliH02%annA02%dna02%lehtraB02%nairolF02%dna02%nretsnegroM02%dnaleiW=526535609.yrtne&sdirG02%naissuaG02%gnizinagrO-fleS02%aiv02%noitatneserpeR02%enecS02%D302%tcapmoC=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Compact%203D%20Scene%20Representation%20via%20Self-Organizing%20Gaussian%20Grids&body=Title%3A%20Compact%203D%20Scene%20Representation%20via%20Self-Organizing%20Gaussian%20Grids%0AAuthor%3A%20Wieland%20Morgenstern%20and%20Florian%20Barthel%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert%0AAbstract%3A%20%20%203D%20Gaussian%20Splatting%20has%20recently%20emerged%20as%20a%20highly%20promising%20technique%0Afor%20modeling%20of%20static%203D%20scenes.%20In%20contrast%20to%20Neural%20Radiance%20Fields%2C%20it%0Autilizes%20efficient%20rasterization%20allowing%20for%20very%20fast%20rendering%20at%0Ahigh-quality.%20However%2C%20the%20storage%20size%20is%20significantly%20higher%2C%20which%20hinders%0Apractical%20deployment%2C%20e.g.%20on%20resource%20constrained%20devices.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20compact%20scene%20representation%20organizing%20the%20parameters%20of%203D%0AGaussian%20Splatting%20%283DGS%29%20into%20a%202D%20grid%20with%20local%20homogeneity%2C%20ensuring%20a%0Adrastic%20reduction%20in%20storage%20requirements%20without%20compromising%20visual%20quality%0Aduring%20rendering.%20Central%20to%20our%20idea%20is%20the%20explicit%20exploitation%20of%0Aperceptual%20redundancies%20present%20in%20natural%20scenes.%20In%20essence%2C%20the%20inherent%0Anature%20of%20a%20scene%20allows%20for%20numerous%20permutations%20of%20Gaussian%20parameters%20to%0Aequivalently%20represent%20it.%20To%20this%20end%2C%20we%20propose%20a%20novel%20highly%20parallel%0Aalgorithm%20that%20regularly%20arranges%20the%20high-dimensional%20Gaussian%20parameters%20into%0Aa%202D%20grid%20while%20preserving%20their%20neighborhood%20structure.%20During%20training%2C%20we%0Afurther%20enforce%20local%20smoothness%20between%20the%20sorted%20parameters%20in%20the%20grid.%20The%0Auncompressed%20Gaussians%20use%20the%20same%20structure%20as%203DGS%2C%20ensuring%20a%20seamless%0Aintegration%20with%20established%20renderers.%20Our%20method%20achieves%20a%20reduction%20factor%0Aof%2017x%20to%2042x%20in%20size%20for%20complex%20scenes%20with%20no%20increase%20in%20training%20time%2C%0Amarking%20a%20substantial%20leap%20forward%20in%20the%20domain%20of%203D%20scene%20distribution%20and%0Aconsumption.%20Additional%20information%20can%20be%20found%20on%20our%20project%20page%3A%0Ahttps%3A//fraunhoferhhi.github.io/Self-Organizing-Gaussians/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13299v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compact%203D%20Scene%20Representation%20via%20Self-Organizing%20Gaussian%20Grids&entry.906535625=Wieland%20Morgenstern%20and%20Florian%20Barthel%20and%20Anna%20Hilsmann%20and%20Peter%20Eisert&entry.1292438233=%20%203D%20Gaussian%20Splatting%20has%20recently%20emerged%20as%20a%20highly%20promising%20technique%0Afor%20modeling%20of%20static%203D%20scenes.%20In%20contrast%20to%20Neural%20Radiance%20Fields%2C%20it%0Autilizes%20efficient%20rasterization%20allowing%20for%20very%20fast%20rendering%20at%0Ahigh-quality.%20However%2C%20the%20storage%20size%20is%20significantly%20higher%2C%20which%20hinders%0Apractical%20deployment%2C%20e.g.%20on%20resource%20constrained%20devices.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20compact%20scene%20representation%20organizing%20the%20parameters%20of%203D%0AGaussian%20Splatting%20%283DGS%29%20into%20a%202D%20grid%20with%20local%20homogeneity%2C%20ensuring%20a%0Adrastic%20reduction%20in%20storage%20requirements%20without%20compromising%20visual%20quality%0Aduring%20rendering.%20Central%20to%20our%20idea%20is%20the%20explicit%20exploitation%20of%0Aperceptual%20redundancies%20present%20in%20natural%20scenes.%20In%20essence%2C%20the%20inherent%0Anature%20of%20a%20scene%20allows%20for%20numerous%20permutations%20of%20Gaussian%20parameters%20to%0Aequivalently%20represent%20it.%20To%20this%20end%2C%20we%20propose%20a%20novel%20highly%20parallel%0Aalgorithm%20that%20regularly%20arranges%20the%20high-dimensional%20Gaussian%20parameters%20into%0Aa%202D%20grid%20while%20preserving%20their%20neighborhood%20structure.%20During%20training%2C%20we%0Afurther%20enforce%20local%20smoothness%20between%20the%20sorted%20parameters%20in%20the%20grid.%20The%0Auncompressed%20Gaussians%20use%20the%20same%20structure%20as%203DGS%2C%20ensuring%20a%20seamless%0Aintegration%20with%20established%20renderers.%20Our%20method%20achieves%20a%20reduction%20factor%0Aof%2017x%20to%2042x%20in%20size%20for%20complex%20scenes%20with%20no%20increase%20in%20training%20time%2C%0Amarking%20a%20substantial%20leap%20forward%20in%20the%20domain%20of%203D%20scene%20distribution%20and%0Aconsumption.%20Additional%20information%20can%20be%20found%20on%20our%20project%20page%3A%0Ahttps%3A//fraunhoferhhi.github.io/Self-Organizing-Gaussians/%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13299v2&entry.124074799=Read"},
{"title": "Pyramid Pixel Context Adaption Network for Medical Image Classification\n  with Supervised Contrastive Learning", "author": "Xiaoqing Zhang and Zunjie Xiao and Xiao Wu and Yanlin Chen and Jilu Zhao and Yan Hu and Jiang Liu", "abstract": "  Spatial attention mechanism has been widely incorporated into deep neural\nnetworks (DNNs), significantly lifting the performance in computer vision tasks\nvia long-range dependency modeling. However, it may perform poorly in medical\nimage analysis. Unfortunately, existing efforts are often unaware that\nlong-range dependency modeling has limitations in highlighting subtle lesion\nregions. To overcome this limitation, we propose a practical yet lightweight\narchitectural unit, Pyramid Pixel Context Adaption (PPCA) module, which\nexploits multi-scale pixel context information to recalibrate pixel position in\na pixel-independent manner dynamically. PPCA first applies a well-designed\ncross-channel pyramid pooling to aggregate multi-scale pixel context\ninformation, then eliminates the inconsistency among them by the well-designed\npixel normalization, and finally estimates per pixel attention weight via a\npixel context integration. By embedding PPCA into a DNN with negligible\noverhead, the PPCANet is developed for medical image classification. In\naddition, we introduce supervised contrastive learning to enhance feature\nrepresentation by exploiting the potential of label information via supervised\ncontrastive loss. The extensive experiments on six medical image datasets show\nthat PPCANet outperforms state-of-the-art attention-based networks and recent\ndeep neural networks. We also provide visual analysis and ablation study to\nexplain the behavior of PPCANet in the decision-making process.\n", "link": "http://arxiv.org/abs/2303.01917v3", "date": "2024-05-02", "relevancy": 2.6189, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5341}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5336}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5037}], "mailto": "mailto:daeR=997470421.yrtne&3v71910.3032/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.ssecorp02%gnikam-noisiced02%eht02%ni02%teNACPP02%fo02%roivaheb02%eht02%nialpxeA0%ot02%yduts02%noitalba02%dna02%sisylana02%lausiv02%edivorp02%osla02%eW02%.skrowten02%laruen02%peedA0%tnecer02%dna02%skrowten02%desab-noitnetta02%tra-eht-fo-etats02%smrofreptuo02%teNACPP02%tahtA0%wohs02%stesatad02%egami02%lacidem02%xis02%no02%stnemirepxe02%evisnetxe02%ehT02%.ssol02%evitsartnocA0%desivrepus02%aiv02%noitamrofni02%lebal02%fo02%laitnetop02%eht02%gnitiolpxe02%yb02%noitatneserperA0%erutaef02%ecnahne02%ot02%gninrael02%evitsartnoc02%desivrepus02%ecudortni02%ew02%C2%noitiddaA0%nI02%.noitacifissalc02%egami02%lacidem02%rof02%depoleved02%si02%teNACPP02%eht02%C2%daehrevoA0%elbigilgen02%htiw02%NND02%a02%otni02%ACPP02%gniddebme02%yB02%.noitargetni02%txetnoc02%lexipA0%a02%aiv02%thgiew02%noitnetta02%lexip02%rep02%setamitse02%yllanif02%dna02%C2%noitazilamron02%lexipA0%dengised-llew02%eht02%yb02%meht02%gnoma02%ycnetsisnocni02%eht02%setanimile02%neht02%C2%noitamrofniA0%txetnoc02%lexip02%elacs-itlum02%etagergga02%ot02%gniloop02%dimaryp02%lennahc-ssorcA0%dengised-llew02%a02%seilppa02%tsrif02%ACPP02%.yllacimanyd02%rennam02%tnednepedni-lexip02%aA0%ni02%noitisop02%lexip02%etarbilacer02%ot02%noitamrofni02%txetnoc02%lexip02%elacs-itlum02%stiolpxeA0%hcihw02%C2%eludom02%92%ACPP82%02%noitpadA02%txetnoC02%lexiP02%dimaryP02%C2%tinu02%larutcetihcraA0%thgiewthgil02%tey02%lacitcarp02%a02%esoporp02%ew02%C2%noitatimil02%siht02%emocrevo02%oT02%.snoigerA0%noisel02%eltbus02%gnithgilhgih02%ni02%snoitatimil02%sah02%gniledom02%ycnedneped02%egnar-gnolA0%taht02%erawanu02%netfo02%era02%stroffe02%gnitsixe02%C2%yletanutrofnU02%.sisylana02%egamiA0%lacidem02%ni02%ylroop02%mrofrep02%yam02%ti02%C2%revewoH02%.gniledom02%ycnedneped02%egnar-gnol02%aivA0%sksat02%noisiv02%retupmoc02%ni02%ecnamrofrep02%eht02%gnitfil02%yltnacifingis02%C2%92%sNND82%02%skrowtenA0%laruen02%peed02%otni02%detaroprocni02%ylediw02%neeb02%sah02%msinahcem02%noitnetta02%laitapS02%02%=3328342921.yrtne&uiL02%gnaiJ02%dna02%uH02%naY02%dna02%oahZ02%uliJ02%dna02%nehC02%nilnaY02%dna02%uW02%oaiX02%dna02%oaiX02%eijnuZ02%dna02%gnahZ02%gniqoaiX=526535609.yrtne&gninraeL02%evitsartnoC02%desivrepuS02%htiw02%02%A0%noitacifissalC02%egamI02%lacideM02%rof02%krowteN02%noitpadA02%txetnoC02%lexiP02%dimaryP=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Pyramid%20Pixel%20Context%20Adaption%20Network%20for%20Medical%20Image%20Classification%0A%20%20with%20Supervised%20Contrastive%20Learning&body=Title%3A%20Pyramid%20Pixel%20Context%20Adaption%20Network%20for%20Medical%20Image%20Classification%0A%20%20with%20Supervised%20Contrastive%20Learning%0AAuthor%3A%20Xiaoqing%20Zhang%20and%20Zunjie%20Xiao%20and%20Xiao%20Wu%20and%20Yanlin%20Chen%20and%20Jilu%20Zhao%20and%20Yan%20Hu%20and%20Jiang%20Liu%0AAbstract%3A%20%20%20Spatial%20attention%20mechanism%20has%20been%20widely%20incorporated%20into%20deep%20neural%0Anetworks%20%28DNNs%29%2C%20significantly%20lifting%20the%20performance%20in%20computer%20vision%20tasks%0Avia%20long-range%20dependency%20modeling.%20However%2C%20it%20may%20perform%20poorly%20in%20medical%0Aimage%20analysis.%20Unfortunately%2C%20existing%20efforts%20are%20often%20unaware%20that%0Along-range%20dependency%20modeling%20has%20limitations%20in%20highlighting%20subtle%20lesion%0Aregions.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20practical%20yet%20lightweight%0Aarchitectural%20unit%2C%20Pyramid%20Pixel%20Context%20Adaption%20%28PPCA%29%20module%2C%20which%0Aexploits%20multi-scale%20pixel%20context%20information%20to%20recalibrate%20pixel%20position%20in%0Aa%20pixel-independent%20manner%20dynamically.%20PPCA%20first%20applies%20a%20well-designed%0Across-channel%20pyramid%20pooling%20to%20aggregate%20multi-scale%20pixel%20context%0Ainformation%2C%20then%20eliminates%20the%20inconsistency%20among%20them%20by%20the%20well-designed%0Apixel%20normalization%2C%20and%20finally%20estimates%20per%20pixel%20attention%20weight%20via%20a%0Apixel%20context%20integration.%20By%20embedding%20PPCA%20into%20a%20DNN%20with%20negligible%0Aoverhead%2C%20the%20PPCANet%20is%20developed%20for%20medical%20image%20classification.%20In%0Aaddition%2C%20we%20introduce%20supervised%20contrastive%20learning%20to%20enhance%20feature%0Arepresentation%20by%20exploiting%20the%20potential%20of%20label%20information%20via%20supervised%0Acontrastive%20loss.%20The%20extensive%20experiments%20on%20six%20medical%20image%20datasets%20show%0Athat%20PPCANet%20outperforms%20state-of-the-art%20attention-based%20networks%20and%20recent%0Adeep%20neural%20networks.%20We%20also%20provide%20visual%20analysis%20and%20ablation%20study%20to%0Aexplain%20the%20behavior%20of%20PPCANet%20in%20the%20decision-making%20process.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.01917v3%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pyramid%20Pixel%20Context%20Adaption%20Network%20for%20Medical%20Image%20Classification%0A%20%20with%20Supervised%20Contrastive%20Learning&entry.906535625=Xiaoqing%20Zhang%20and%20Zunjie%20Xiao%20and%20Xiao%20Wu%20and%20Yanlin%20Chen%20and%20Jilu%20Zhao%20and%20Yan%20Hu%20and%20Jiang%20Liu&entry.1292438233=%20%20Spatial%20attention%20mechanism%20has%20been%20widely%20incorporated%20into%20deep%20neural%0Anetworks%20%28DNNs%29%2C%20significantly%20lifting%20the%20performance%20in%20computer%20vision%20tasks%0Avia%20long-range%20dependency%20modeling.%20However%2C%20it%20may%20perform%20poorly%20in%20medical%0Aimage%20analysis.%20Unfortunately%2C%20existing%20efforts%20are%20often%20unaware%20that%0Along-range%20dependency%20modeling%20has%20limitations%20in%20highlighting%20subtle%20lesion%0Aregions.%20To%20overcome%20this%20limitation%2C%20we%20propose%20a%20practical%20yet%20lightweight%0Aarchitectural%20unit%2C%20Pyramid%20Pixel%20Context%20Adaption%20%28PPCA%29%20module%2C%20which%0Aexploits%20multi-scale%20pixel%20context%20information%20to%20recalibrate%20pixel%20position%20in%0Aa%20pixel-independent%20manner%20dynamically.%20PPCA%20first%20applies%20a%20well-designed%0Across-channel%20pyramid%20pooling%20to%20aggregate%20multi-scale%20pixel%20context%0Ainformation%2C%20then%20eliminates%20the%20inconsistency%20among%20them%20by%20the%20well-designed%0Apixel%20normalization%2C%20and%20finally%20estimates%20per%20pixel%20attention%20weight%20via%20a%0Apixel%20context%20integration.%20By%20embedding%20PPCA%20into%20a%20DNN%20with%20negligible%0Aoverhead%2C%20the%20PPCANet%20is%20developed%20for%20medical%20image%20classification.%20In%0Aaddition%2C%20we%20introduce%20supervised%20contrastive%20learning%20to%20enhance%20feature%0Arepresentation%20by%20exploiting%20the%20potential%20of%20label%20information%20via%20supervised%0Acontrastive%20loss.%20The%20extensive%20experiments%20on%20six%20medical%20image%20datasets%20show%0Athat%20PPCANet%20outperforms%20state-of-the-art%20attention-based%20networks%20and%20recent%0Adeep%20neural%20networks.%20We%20also%20provide%20visual%20analysis%20and%20ablation%20study%20to%0Aexplain%20the%20behavior%20of%20PPCANet%20in%20the%20decision-making%20process.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.01917v3&entry.124074799=Read"},
{"title": "GRASP: A Rehearsal Policy for Efficient Online Continual Learning", "author": "Md Yousuf Harun and Jhair Gallardo and Junyu Chen and Christopher Kanan", "abstract": "  Continual learning (CL) in deep neural networks (DNNs) involves incrementally\naccumulating knowledge in a DNN from a growing data stream. A major challenge\nin CL is that non-stationary data streams cause catastrophic forgetting of\npreviously learned abilities. A popular solution is rehearsal: storing past\nobservations in a buffer and then sampling the buffer to update the DNN.\nUniform sampling in a class-balanced manner is highly effective, and better\nsample selection policies have been elusive. Here, we propose a new sample\nselection policy called GRASP that selects the most prototypical (easy) samples\nfirst and then gradually selects less prototypical (harder) examples. GRASP has\nlittle additional compute or memory overhead compared to uniform selection,\nenabling it to scale to large datasets. Compared to 17 other rehearsal\npolicies, GRASP achieves higher accuracy in CL experiments on ImageNet.\nCompared to uniform balanced sampling, GRASP achieves the same performance with\n40% fewer updates. We also show that GRASP is effective for CL on five text\nclassification datasets.\n", "link": "http://arxiv.org/abs/2308.13646v2", "date": "2024-05-01", "relevancy": 2.6186, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5407}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5317}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4987}], "mailto": "mailto:daeR=997470421.yrtne&2v64631.8032/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.stesatad02%noitacifissalcA0%txet02%evif02%no02%LC02%rof02%evitceffe02%si02%PSARG02%taht02%wohs02%osla02%eW02%.setadpu02%rewef02%52%04A0%htiw02%ecnamrofrep02%emas02%eht02%seveihca02%PSARG02%C2%gnilpmas02%decnalab02%mrofinu02%ot02%derapmoCA0%.teNegamI02%no02%stnemirepxe02%LC02%ni02%ycarucca02%rehgih02%seveihca02%PSARG02%C2%seicilopA0%lasraeher02%rehto02%7102%ot02%derapmoC02%.stesatad02%egral02%ot02%elacs02%ot02%ti02%gnilbaneA0%C2%noitceles02%mrofinu02%ot02%derapmoc02%daehrevo02%yromem02%ro02%etupmoc02%lanoitidda02%elttilA0%sah02%PSARG02%.selpmaxe02%92%redrah82%02%lacipytotorp02%ssel02%stceles02%yllaudarg02%neht02%dna02%tsrifA0%selpmas02%92%ysae82%02%lacipytotorp02%tsom02%eht02%stceles02%taht02%PSARG02%dellac02%ycilop02%noitcelesA0%elpmas02%wen02%a02%esoporp02%ew02%C2%ereH02%.evisule02%neeb02%evah02%seicilop02%noitceles02%elpmasA0%retteb02%dna02%C2%evitceffe02%ylhgih02%si02%rennam02%decnalab-ssalc02%a02%ni02%gnilpmas02%mrofinUA0%.NND02%eht02%etadpu02%ot02%reffub02%eht02%gnilpmas02%neht02%dna02%reffub02%a02%ni02%snoitavresboA0%tsap02%gnirots02%A3%lasraeher02%si02%noitulos02%ralupop02%A02%.seitiliba02%denrael02%ylsuoiverpA0%fo02%gnittegrof02%cihportsatac02%esuac02%smaerts02%atad02%yranoitats-non02%taht02%si02%LC02%niA0%egnellahc02%rojam02%A02%.maerts02%atad02%gniworg02%a02%morf02%NND02%a02%ni02%egdelwonk02%gnitalumuccaA0%yllatnemercni02%sevlovni02%92%sNND82%02%skrowten02%laruen02%peed02%ni02%92%LC82%02%gninrael02%launitnoC02%02%=3328342921.yrtne&nanaK02%rehpotsirhC02%dna02%nehC02%uynuJ02%dna02%odrallaG02%riahJ02%dna02%nuraH02%fusuoY02%dM=526535609.yrtne&gninraeL02%launitnoC02%enilnO02%tneiciffE02%rof02%yciloP02%lasraeheR02%A02%A3%PSARG=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20GRASP%3A%20A%20Rehearsal%20Policy%20for%20Efficient%20Online%20Continual%20Learning&body=Title%3A%20GRASP%3A%20A%20Rehearsal%20Policy%20for%20Efficient%20Online%20Continual%20Learning%0AAuthor%3A%20Md%20Yousuf%20Harun%20and%20Jhair%20Gallardo%20and%20Junyu%20Chen%20and%20Christopher%20Kanan%0AAbstract%3A%20%20%20Continual%20learning%20%28CL%29%20in%20deep%20neural%20networks%20%28DNNs%29%20involves%20incrementally%0Aaccumulating%20knowledge%20in%20a%20DNN%20from%20a%20growing%20data%20stream.%20A%20major%20challenge%0Ain%20CL%20is%20that%20non-stationary%20data%20streams%20cause%20catastrophic%20forgetting%20of%0Apreviously%20learned%20abilities.%20A%20popular%20solution%20is%20rehearsal%3A%20storing%20past%0Aobservations%20in%20a%20buffer%20and%20then%20sampling%20the%20buffer%20to%20update%20the%20DNN.%0AUniform%20sampling%20in%20a%20class-balanced%20manner%20is%20highly%20effective%2C%20and%20better%0Asample%20selection%20policies%20have%20been%20elusive.%20Here%2C%20we%20propose%20a%20new%20sample%0Aselection%20policy%20called%20GRASP%20that%20selects%20the%20most%20prototypical%20%28easy%29%20samples%0Afirst%20and%20then%20gradually%20selects%20less%20prototypical%20%28harder%29%20examples.%20GRASP%20has%0Alittle%20additional%20compute%20or%20memory%20overhead%20compared%20to%20uniform%20selection%2C%0Aenabling%20it%20to%20scale%20to%20large%20datasets.%20Compared%20to%2017%20other%20rehearsal%0Apolicies%2C%20GRASP%20achieves%20higher%20accuracy%20in%20CL%20experiments%20on%20ImageNet.%0ACompared%20to%20uniform%20balanced%20sampling%2C%20GRASP%20achieves%20the%20same%20performance%20with%0A40%25%20fewer%20updates.%20We%20also%20show%20that%20GRASP%20is%20effective%20for%20CL%20on%20five%20text%0Aclassification%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.13646v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRASP%3A%20A%20Rehearsal%20Policy%20for%20Efficient%20Online%20Continual%20Learning&entry.906535625=Md%20Yousuf%20Harun%20and%20Jhair%20Gallardo%20and%20Junyu%20Chen%20and%20Christopher%20Kanan&entry.1292438233=%20%20Continual%20learning%20%28CL%29%20in%20deep%20neural%20networks%20%28DNNs%29%20involves%20incrementally%0Aaccumulating%20knowledge%20in%20a%20DNN%20from%20a%20growing%20data%20stream.%20A%20major%20challenge%0Ain%20CL%20is%20that%20non-stationary%20data%20streams%20cause%20catastrophic%20forgetting%20of%0Apreviously%20learned%20abilities.%20A%20popular%20solution%20is%20rehearsal%3A%20storing%20past%0Aobservations%20in%20a%20buffer%20and%20then%20sampling%20the%20buffer%20to%20update%20the%20DNN.%0AUniform%20sampling%20in%20a%20class-balanced%20manner%20is%20highly%20effective%2C%20and%20better%0Asample%20selection%20policies%20have%20been%20elusive.%20Here%2C%20we%20propose%20a%20new%20sample%0Aselection%20policy%20called%20GRASP%20that%20selects%20the%20most%20prototypical%20%28easy%29%20samples%0Afirst%20and%20then%20gradually%20selects%20less%20prototypical%20%28harder%29%20examples.%20GRASP%20has%0Alittle%20additional%20compute%20or%20memory%20overhead%20compared%20to%20uniform%20selection%2C%0Aenabling%20it%20to%20scale%20to%20large%20datasets.%20Compared%20to%2017%20other%20rehearsal%0Apolicies%2C%20GRASP%20achieves%20higher%20accuracy%20in%20CL%20experiments%20on%20ImageNet.%0ACompared%20to%20uniform%20balanced%20sampling%2C%20GRASP%20achieves%20the%20same%20performance%20with%0A40%25%20fewer%20updates.%20We%20also%20show%20that%20GRASP%20is%20effective%20for%20CL%20on%20five%20text%0Aclassification%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.13646v2&entry.124074799=Read"},
{"title": "Locality Regularized Reconstruction: Structured Sparsity and Delaunay\n  Triangulations", "author": "Marshall Mueller and James M. Murphy and Abiy Tasissa", "abstract": "  Linear representation learning is widely studied due to its conceptual\nsimplicity and empirical utility in tasks such as compression, classification,\nand feature extraction. Given a set of points $[\\mathbf{x}_1, \\mathbf{x}_2,\n\\ldots, \\mathbf{x}_n] = \\mathbf{X} \\in \\mathbb{R}^{d \\times n}$ and a vector\n$\\mathbf{y} \\in \\mathbb{R}^d$, the goal is to find coefficients $\\mathbf{w} \\in\n\\mathbb{R}^n$ so that $\\mathbf{X} \\mathbf{w} \\approx \\mathbf{y}$, subject to\nsome desired structure on $\\mathbf{w}$. In this work we seek $\\mathbf{w}$ that\nforms a local reconstruction of $\\mathbf{y}$ by solving a regularized least\nsquares regression problem. We obtain local solutions through a locality\nfunction that promotes the use of columns of $\\mathbf{X}$ that are close to\n$\\mathbf{y}$ when used as a regularization term. We prove that, for all levels\nof regularization and under a mild condition that the columns of $\\mathbf{X}$\nhave a unique Delaunay triangulation, the optimal coefficients' number of\nnon-zero entries is upper bounded by $d+1$, thereby providing local sparse\nsolutions when $d \\ll n$. Under the same condition we also show that for any\n$\\mathbf{y}$ contained in the convex hull of $\\mathbf{X}$ there exists a regime\nof regularization parameter such that the optimal coefficients are supported on\nthe vertices of the Delaunay simplex containing $\\mathbf{y}$. This provides an\ninterpretation of the sparsity as having structure obtained implicitly from the\nDelaunay triangulation of $\\mathbf{X}$. We demonstrate that our locality\nregularized problem can be solved in comparable time to other methods that\nidentify the containing Delaunay simplex.\n", "link": "http://arxiv.org/abs/2405.00837v1", "date": "2024-05-01", "relevancy": 2.6013, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5363}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5189}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5055}], "mailto": "mailto:daeR=997470421.yrtne&1v73800.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.xelpmis02%yanualeD02%gniniatnoc02%eht02%yfitnediA0%taht02%sdohtem02%rehto02%ot02%emit02%elbarapmoc02%ni02%devlos02%eb02%nac02%melborp02%deziralugerA0%ytilacol02%ruo02%taht02%etartsnomed02%eW02%.42%D7%XB7%fbhtamC5%42%02%fo02%noitalugnairt02%yanualeDA0%eht02%morf02%ylticilpmi02%deniatbo02%erutcurts02%gnivah02%sa02%ytisraps02%eht02%fo02%noitaterpretniA0%na02%sedivorp02%sihT02%.42%D7%yB7%fbhtamC5%42%02%gniniatnoc02%xelpmis02%yanualeD02%eht02%fo02%secitrev02%ehtA0%no02%detroppus02%era02%stneiciffeoc02%lamitpo02%eht02%taht02%hcus02%retemarap02%noitaziraluger02%foA0%emiger02%a02%stsixe02%ereht02%42%D7%XB7%fbhtamC5%42%02%fo02%lluh02%xevnoc02%eht02%ni02%deniatnoc02%42%D7%yB7%fbhtamC5%42%A0%yna02%rof02%taht02%wohs02%osla02%ew02%noitidnoc02%emas02%eht02%rednU02%.42%n02%llC5%02%d42%02%nehw02%snoitulosA0%esraps02%lacol02%gnidivorp02%ybereht02%C2%42%1B2%d42%02%yb02%dednuob02%reppu02%si02%seirtne02%orez-nonA0%fo02%rebmun02%72%stneiciffeoc02%lamitpo02%eht02%C2%noitalugnairt02%yanualeD02%euqinu02%a02%evahA0%42%D7%XB7%fbhtamC5%42%02%fo02%snmuloc02%eht02%taht02%noitidnoc02%dlim02%a02%rednu02%dna02%noitaziraluger02%foA0%slevel02%lla02%rof02%C2%taht02%evorp02%eW02%.mret02%noitaziraluger02%a02%sa02%desu02%nehw02%42%D7%yB7%fbhtamC5%42%A0%ot02%esolc02%era02%taht02%42%D7%XB7%fbhtamC5%42%02%fo02%snmuloc02%fo02%esu02%eht02%setomorp02%taht02%noitcnufA0%ytilacol02%a02%hguorht02%snoitulos02%lacol02%niatbo02%eW02%.melborp02%noisserger02%serauqsA0%tsael02%deziraluger02%a02%gnivlos02%yb02%42%D7%yB7%fbhtamC5%42%02%fo02%noitcurtsnocer02%lacol02%a02%smrofA0%taht02%42%D7%wB7%fbhtamC5%42%02%kees02%ew02%krow02%siht02%nI02%.42%D7%wB7%fbhtamC5%42%02%no02%erutcurts02%derised02%emosA0%ot02%tcejbus02%C2%42%D7%yB7%fbhtamC5%02%xorppaC5%02%D7%wB7%fbhtamC5%02%D7%XB7%fbhtamC5%42%02%taht02%os02%42%nE5%D7%RB7%bbhtamC5%A0%niC5%02%D7%wB7%fbhtamC5%42%02%stneiciffeoc02%dnif02%ot02%si02%laog02%eht02%C2%42%dE5%D7%RB7%bbhtamC5%02%niC5%02%D7%yB7%fbhtamC5%42%A0%rotcev02%a02%dna02%42%D7%n02%semitC5%02%dB7%E5%D7%RB7%bbhtamC5%02%niC5%02%D7%XB7%fbhtamC5%02%D3%02%D5%n_D7%xB7%fbhtamC5%02%C2%stodlC5%A0%C2%2_D7%xB7%fbhtamC5%02%C2%1_D7%xB7%fbhtamC5%B5%42%02%stniop02%fo02%tes02%a02%neviG02%.noitcartxe02%erutaef02%dnaA0%C2%noitacifissalc02%C2%noisserpmoc02%sa02%hcus02%sksat02%ni02%ytilitu02%laciripme02%dna02%yticilpmisA0%lautpecnoc02%sti02%ot02%eud02%deiduts02%ylediw02%si02%gninrael02%noitatneserper02%raeniL02%02%=3328342921.yrtne&assisaT02%yibA02%dna02%yhpruM02%.M02%semaJ02%dna02%relleuM02%llahsraM=526535609.yrtne&snoitalugnairT02%02%A0%yanualeD02%dna02%ytisrapS02%derutcurtS02%A3%noitcurtsnoceR02%deziralugeR02%ytilacoL=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Locality%20Regularized%20Reconstruction%3A%20Structured%20Sparsity%20and%20Delaunay%0A%20%20Triangulations&body=Title%3A%20Locality%20Regularized%20Reconstruction%3A%20Structured%20Sparsity%20and%20Delaunay%0A%20%20Triangulations%0AAuthor%3A%20Marshall%20Mueller%20and%20James%20M.%20Murphy%20and%20Abiy%20Tasissa%0AAbstract%3A%20%20%20Linear%20representation%20learning%20is%20widely%20studied%20due%20to%20its%20conceptual%0Asimplicity%20and%20empirical%20utility%20in%20tasks%20such%20as%20compression%2C%20classification%2C%0Aand%20feature%20extraction.%20Given%20a%20set%20of%20points%20%24%5B%5Cmathbf%7Bx%7D_1%2C%20%5Cmathbf%7Bx%7D_2%2C%0A%5Cldots%2C%20%5Cmathbf%7Bx%7D_n%5D%20%3D%20%5Cmathbf%7BX%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd%20%5Ctimes%20n%7D%24%20and%20a%20vector%0A%24%5Cmathbf%7By%7D%20%5Cin%20%5Cmathbb%7BR%7D%5Ed%24%2C%20the%20goal%20is%20to%20find%20coefficients%20%24%5Cmathbf%7Bw%7D%20%5Cin%0A%5Cmathbb%7BR%7D%5En%24%20so%20that%20%24%5Cmathbf%7BX%7D%20%5Cmathbf%7Bw%7D%20%5Capprox%20%5Cmathbf%7By%7D%24%2C%20subject%20to%0Asome%20desired%20structure%20on%20%24%5Cmathbf%7Bw%7D%24.%20In%20this%20work%20we%20seek%20%24%5Cmathbf%7Bw%7D%24%20that%0Aforms%20a%20local%20reconstruction%20of%20%24%5Cmathbf%7By%7D%24%20by%20solving%20a%20regularized%20least%0Asquares%20regression%20problem.%20We%20obtain%20local%20solutions%20through%20a%20locality%0Afunction%20that%20promotes%20the%20use%20of%20columns%20of%20%24%5Cmathbf%7BX%7D%24%20that%20are%20close%20to%0A%24%5Cmathbf%7By%7D%24%20when%20used%20as%20a%20regularization%20term.%20We%20prove%20that%2C%20for%20all%20levels%0Aof%20regularization%20and%20under%20a%20mild%20condition%20that%20the%20columns%20of%20%24%5Cmathbf%7BX%7D%24%0Ahave%20a%20unique%20Delaunay%20triangulation%2C%20the%20optimal%20coefficients%27%20number%20of%0Anon-zero%20entries%20is%20upper%20bounded%20by%20%24d%2B1%24%2C%20thereby%20providing%20local%20sparse%0Asolutions%20when%20%24d%20%5Cll%20n%24.%20Under%20the%20same%20condition%20we%20also%20show%20that%20for%20any%0A%24%5Cmathbf%7By%7D%24%20contained%20in%20the%20convex%20hull%20of%20%24%5Cmathbf%7BX%7D%24%20there%20exists%20a%20regime%0Aof%20regularization%20parameter%20such%20that%20the%20optimal%20coefficients%20are%20supported%20on%0Athe%20vertices%20of%20the%20Delaunay%20simplex%20containing%20%24%5Cmathbf%7By%7D%24.%20This%20provides%20an%0Ainterpretation%20of%20the%20sparsity%20as%20having%20structure%20obtained%20implicitly%20from%20the%0ADelaunay%20triangulation%20of%20%24%5Cmathbf%7BX%7D%24.%20We%20demonstrate%20that%20our%20locality%0Aregularized%20problem%20can%20be%20solved%20in%20comparable%20time%20to%20other%20methods%20that%0Aidentify%20the%20containing%20Delaunay%20simplex.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00837v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locality%20Regularized%20Reconstruction%3A%20Structured%20Sparsity%20and%20Delaunay%0A%20%20Triangulations&entry.906535625=Marshall%20Mueller%20and%20James%20M.%20Murphy%20and%20Abiy%20Tasissa&entry.1292438233=%20%20Linear%20representation%20learning%20is%20widely%20studied%20due%20to%20its%20conceptual%0Asimplicity%20and%20empirical%20utility%20in%20tasks%20such%20as%20compression%2C%20classification%2C%0Aand%20feature%20extraction.%20Given%20a%20set%20of%20points%20%24%5B%5Cmathbf%7Bx%7D_1%2C%20%5Cmathbf%7Bx%7D_2%2C%0A%5Cldots%2C%20%5Cmathbf%7Bx%7D_n%5D%20%3D%20%5Cmathbf%7BX%7D%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bd%20%5Ctimes%20n%7D%24%20and%20a%20vector%0A%24%5Cmathbf%7By%7D%20%5Cin%20%5Cmathbb%7BR%7D%5Ed%24%2C%20the%20goal%20is%20to%20find%20coefficients%20%24%5Cmathbf%7Bw%7D%20%5Cin%0A%5Cmathbb%7BR%7D%5En%24%20so%20that%20%24%5Cmathbf%7BX%7D%20%5Cmathbf%7Bw%7D%20%5Capprox%20%5Cmathbf%7By%7D%24%2C%20subject%20to%0Asome%20desired%20structure%20on%20%24%5Cmathbf%7Bw%7D%24.%20In%20this%20work%20we%20seek%20%24%5Cmathbf%7Bw%7D%24%20that%0Aforms%20a%20local%20reconstruction%20of%20%24%5Cmathbf%7By%7D%24%20by%20solving%20a%20regularized%20least%0Asquares%20regression%20problem.%20We%20obtain%20local%20solutions%20through%20a%20locality%0Afunction%20that%20promotes%20the%20use%20of%20columns%20of%20%24%5Cmathbf%7BX%7D%24%20that%20are%20close%20to%0A%24%5Cmathbf%7By%7D%24%20when%20used%20as%20a%20regularization%20term.%20We%20prove%20that%2C%20for%20all%20levels%0Aof%20regularization%20and%20under%20a%20mild%20condition%20that%20the%20columns%20of%20%24%5Cmathbf%7BX%7D%24%0Ahave%20a%20unique%20Delaunay%20triangulation%2C%20the%20optimal%20coefficients%27%20number%20of%0Anon-zero%20entries%20is%20upper%20bounded%20by%20%24d%2B1%24%2C%20thereby%20providing%20local%20sparse%0Asolutions%20when%20%24d%20%5Cll%20n%24.%20Under%20the%20same%20condition%20we%20also%20show%20that%20for%20any%0A%24%5Cmathbf%7By%7D%24%20contained%20in%20the%20convex%20hull%20of%20%24%5Cmathbf%7BX%7D%24%20there%20exists%20a%20regime%0Aof%20regularization%20parameter%20such%20that%20the%20optimal%20coefficients%20are%20supported%20on%0Athe%20vertices%20of%20the%20Delaunay%20simplex%20containing%20%24%5Cmathbf%7By%7D%24.%20This%20provides%20an%0Ainterpretation%20of%20the%20sparsity%20as%20having%20structure%20obtained%20implicitly%20from%20the%0ADelaunay%20triangulation%20of%20%24%5Cmathbf%7BX%7D%24.%20We%20demonstrate%20that%20our%20locality%0Aregularized%20problem%20can%20be%20solved%20in%20comparable%20time%20to%20other%20methods%20that%0Aidentify%20the%20containing%20Delaunay%20simplex.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00837v1&entry.124074799=Read"},
{"title": "Perception and Localization of Macular Degeneration Applying\n  Convolutional Neural Network, ResNet and Grad-CAM", "author": "Tahmim Hossain and Sagor Chandro Bakchy", "abstract": "  A well-known retinal disease that sends blurry visions to the affected\npatients is Macular Degeneration. This research is based on classifying the\nhealthy and macular degeneration fundus by localizing the affected region of\nthe fundus. A CNN architecture and CNN with ResNet architecture (ResNet50,\nResNet50v2, ResNet101, ResNet101v2, ResNet152, ResNet152v2) as the backbone are\nused to classify the two types of fundus. The data are split into three\ncategories including (a) Training set is 90% and Testing set is 10% (b)\nTraining set is 80% and Testing set is 20%, (c) Training set is 50% and Testing\nset is 50%. After the training, the best model has been selected from the\nevaluation metrics. Among the models, CNN with a backbone of ResNet50 performs\nbest which gives the training accuracy of 98.7% for 90% train and 10% test data\nsplit. With this model, we have performed the Grad-CAM visualization to get the\nregion of the affected area of the fundus.\n", "link": "http://arxiv.org/abs/2404.15918v2", "date": "2024-05-02", "relevancy": 2.58, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.556}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5222}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4698}], "mailto": "mailto:daeR=997470421.yrtne&2v81951.4042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.sudnuf02%eht02%fo02%aera02%detceffa02%eht02%fo02%noigerA0%eht02%teg02%ot02%noitazilausiv02%MAC-darG02%eht02%demrofrep02%evah02%ew02%C2%ledom02%siht02%htiW02%.tilpsA0%atad02%tset02%52%0102%dna02%niart02%52%0902%rof02%52%7.8902%fo02%ycarucca02%gniniart02%eht02%sevig02%hcihw02%tsebA0%smrofrep02%05teNseR02%fo02%enobkcab02%a02%htiw02%NNC02%C2%sledom02%eht02%gnomA02%.scirtem02%noitaulaveA0%eht02%morf02%detceles02%neeb02%sah02%ledom02%tseb02%eht02%C2%gniniart02%eht02%retfA02%.52%0502%si02%tesA0%gnitseT02%dna02%52%0502%si02%tes02%gniniarT02%92%c82%02%C2%52%0202%si02%tes02%gnitseT02%dna02%52%0802%si02%tes02%gniniarTA0%92%b82%02%52%0102%si02%tes02%gnitseT02%dna02%52%0902%si02%tes02%gniniarT02%92%a82%02%gnidulcni02%seirogetacA0%eerht02%otni02%tilps02%era02%atad02%ehT02%.sudnuf02%fo02%sepyt02%owt02%eht02%yfissalc02%ot02%desuA0%era02%enobkcab02%eht02%sa02%92%2v251teNseR02%C2%251teNseR02%C2%2v101teNseR02%C2%101teNseR02%C2%2v05teNseRA0%C2%05teNseR82%02%erutcetihcra02%teNseR02%htiw02%NNC02%dna02%erutcetihcra02%NNC02%A02%.sudnuf02%ehtA0%fo02%noiger02%detceffa02%eht02%gnizilacol02%yb02%sudnuf02%noitareneged02%ralucam02%dna02%yhtlaehA0%eht02%gniyfissalc02%no02%desab02%si02%hcraeser02%sihT02%.noitarenegeD02%ralucaM02%si02%stneitapA0%detceffa02%eht02%ot02%snoisiv02%yrrulb02%sdnes02%taht02%esaesid02%laniter02%nwonk-llew02%A02%02%=3328342921.yrtne&yhckaB02%ordnahC02%rogaS02%dna02%niassoH02%mimhaT=526535609.yrtne&MAC-darG02%dna02%teNseR02%C2%krowteN02%larueN02%lanoitulovnoC02%02%A0%gniylppA02%noitarenegeD02%ralucaM02%fo02%noitazilacoL02%dna02%noitpecreP=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Perception%20and%20Localization%20of%20Macular%20Degeneration%20Applying%0A%20%20Convolutional%20Neural%20Network%2C%20ResNet%20and%20Grad-CAM&body=Title%3A%20Perception%20and%20Localization%20of%20Macular%20Degeneration%20Applying%0A%20%20Convolutional%20Neural%20Network%2C%20ResNet%20and%20Grad-CAM%0AAuthor%3A%20Tahmim%20Hossain%20and%20Sagor%20Chandro%20Bakchy%0AAbstract%3A%20%20%20A%20well-known%20retinal%20disease%20that%20sends%20blurry%20visions%20to%20the%20affected%0Apatients%20is%20Macular%20Degeneration.%20This%20research%20is%20based%20on%20classifying%20the%0Ahealthy%20and%20macular%20degeneration%20fundus%20by%20localizing%20the%20affected%20region%20of%0Athe%20fundus.%20A%20CNN%20architecture%20and%20CNN%20with%20ResNet%20architecture%20%28ResNet50%2C%0AResNet50v2%2C%20ResNet101%2C%20ResNet101v2%2C%20ResNet152%2C%20ResNet152v2%29%20as%20the%20backbone%20are%0Aused%20to%20classify%20the%20two%20types%20of%20fundus.%20The%20data%20are%20split%20into%20three%0Acategories%20including%20%28a%29%20Training%20set%20is%2090%25%20and%20Testing%20set%20is%2010%25%20%28b%29%0ATraining%20set%20is%2080%25%20and%20Testing%20set%20is%2020%25%2C%20%28c%29%20Training%20set%20is%2050%25%20and%20Testing%0Aset%20is%2050%25.%20After%20the%20training%2C%20the%20best%20model%20has%20been%20selected%20from%20the%0Aevaluation%20metrics.%20Among%20the%20models%2C%20CNN%20with%20a%20backbone%20of%20ResNet50%20performs%0Abest%20which%20gives%20the%20training%20accuracy%20of%2098.7%25%20for%2090%25%20train%20and%2010%25%20test%20data%0Asplit.%20With%20this%20model%2C%20we%20have%20performed%20the%20Grad-CAM%20visualization%20to%20get%20the%0Aregion%20of%20the%20affected%20area%20of%20the%20fundus.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15918v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Perception%20and%20Localization%20of%20Macular%20Degeneration%20Applying%0A%20%20Convolutional%20Neural%20Network%2C%20ResNet%20and%20Grad-CAM&entry.906535625=Tahmim%20Hossain%20and%20Sagor%20Chandro%20Bakchy&entry.1292438233=%20%20A%20well-known%20retinal%20disease%20that%20sends%20blurry%20visions%20to%20the%20affected%0Apatients%20is%20Macular%20Degeneration.%20This%20research%20is%20based%20on%20classifying%20the%0Ahealthy%20and%20macular%20degeneration%20fundus%20by%20localizing%20the%20affected%20region%20of%0Athe%20fundus.%20A%20CNN%20architecture%20and%20CNN%20with%20ResNet%20architecture%20%28ResNet50%2C%0AResNet50v2%2C%20ResNet101%2C%20ResNet101v2%2C%20ResNet152%2C%20ResNet152v2%29%20as%20the%20backbone%20are%0Aused%20to%20classify%20the%20two%20types%20of%20fundus.%20The%20data%20are%20split%20into%20three%0Acategories%20including%20%28a%29%20Training%20set%20is%2090%25%20and%20Testing%20set%20is%2010%25%20%28b%29%0ATraining%20set%20is%2080%25%20and%20Testing%20set%20is%2020%25%2C%20%28c%29%20Training%20set%20is%2050%25%20and%20Testing%0Aset%20is%2050%25.%20After%20the%20training%2C%20the%20best%20model%20has%20been%20selected%20from%20the%0Aevaluation%20metrics.%20Among%20the%20models%2C%20CNN%20with%20a%20backbone%20of%20ResNet50%20performs%0Abest%20which%20gives%20the%20training%20accuracy%20of%2098.7%25%20for%2090%25%20train%20and%2010%25%20test%20data%0Asplit.%20With%20this%20model%2C%20we%20have%20performed%20the%20Grad-CAM%20visualization%20to%20get%20the%0Aregion%20of%20the%20affected%20area%20of%20the%20fundus.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15918v2&entry.124074799=Read"},
{"title": "BetterV: Controlled Verilog Generation with Discriminative Guidance", "author": "Zehua Pei and Hui-Ling Zhen and Mingxuan Yuan and Yu Huang and Bei Yu", "abstract": "  Due to the growing complexity of modern Integrated Circuits (ICs), there is a\nneed for automated circuit design methods. Recent years have seen rising\nresearch in hardware design language generation to facilitate the design\nprocess. In this work, we propose a Verilog generation framework, BetterV,\nwhich fine-tunes the large language models (LLMs) on processed domain-specific\ndatasets and incorporates generative discriminators for guidance on particular\ndesign demands. The Verilog modules are collected, filtered and processed from\ninternet to form a clean and abundant dataset. Instruct-tuning methods are\nspecially designed to fine-tune the LLMs to understand the knowledge about\nVerilog. Furthermore, data are augmented to enrich the training set and also\nused to train a generative discriminator on particular downstream task, which\nleads a guidance for the LLMs to optimize the Verilog implementation. BetterV\nhas the ability to generate syntactically and functionally correct Verilog,\nwhich can outperform GPT-4 on the VerilogEval benchmark. With the help of\ntask-specific generative discriminator, BetterV can achieve remarkable\nimprovement on various electronic design automation (EDA) downstream tasks,\nincluding the netlist node reduction for synthesis and verification runtime\nreduction with Boolean Satisfiability (SAT) solving.\n", "link": "http://arxiv.org/abs/2402.03375v3", "date": "2024-05-02", "relevancy": 2.5706, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5188}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5123}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5113}], "mailto": "mailto:daeR=997470421.yrtne&3v57330.2042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.gnivlos02%92%TAS82%02%ytilibaifsitaS02%naelooB02%htiw02%noitcuderA0%emitnur02%noitacifirev02%dna02%sisehtnys02%rof02%noitcuder02%edon02%tsilten02%eht02%gnidulcniA0%C2%sksat02%maertsnwod02%92%ADE82%02%noitamotua02%ngised02%cinortcele02%suoirav02%no02%tnemevorpmiA0%elbakramer02%eveihca02%nac02%VretteB02%C2%rotanimircsid02%evitareneg02%cificeps-ksatA0%fo02%pleh02%eht02%htiW02%.kramhcneb02%lavEgolireV02%eht02%no02%4-TPG02%mrofreptuo02%nac02%hcihwA0%C2%golireV02%tcerroc02%yllanoitcnuf02%dna02%yllacitcatnys02%etareneg02%ot02%ytiliba02%eht02%sahA0%VretteB02%.noitatnemelpmi02%golireV02%eht02%ezimitpo02%ot02%sMLL02%eht02%rof02%ecnadiug02%a02%sdaelA0%hcihw02%C2%ksat02%maertsnwod02%ralucitrap02%no02%rotanimircsid02%evitareneg02%a02%niart02%ot02%desuA0%osla02%dna02%tes02%gniniart02%eht02%hcirne02%ot02%detnemgua02%era02%atad02%C2%eromrehtruF02%.golireVA0%tuoba02%egdelwonk02%eht02%dnatsrednu02%ot02%sMLL02%eht02%enut-enif02%ot02%dengised02%yllaicepsA0%era02%sdohtem02%gninut-tcurtsnI02%.tesatad02%tnadnuba02%dna02%naelc02%a02%mrof02%ot02%tenretniA0%morf02%dessecorp02%dna02%deretlif02%C2%detcelloc02%era02%seludom02%golireV02%ehT02%.sdnamed02%ngisedA0%ralucitrap02%no02%ecnadiug02%rof02%srotanimircsid02%evitareneg02%setaroprocni02%dna02%stesatadA0%cificeps-niamod02%dessecorp02%no02%92%sMLL82%02%sledom02%egaugnal02%egral02%eht02%senut-enif02%hcihwA0%C2%VretteB02%C2%krowemarf02%noitareneg02%golireV02%a02%esoporp02%ew02%C2%krow02%siht02%nI02%.ssecorpA0%ngised02%eht02%etatilicaf02%ot02%noitareneg02%egaugnal02%ngised02%erawdrah02%ni02%hcraeserA0%gnisir02%nees02%evah02%sraey02%tneceR02%.sdohtem02%ngised02%tiucric02%detamotua02%rof02%deenA0%a02%si02%ereht02%C2%92%sCI82%02%stiucriC02%detargetnI02%nredom02%fo02%ytixelpmoc02%gniworg02%eht02%ot02%euD02%02%=3328342921.yrtne&uY02%ieB02%dna02%gnauH02%uY02%dna02%nauY02%nauxgniM02%dna02%nehZ02%gniL-iuH02%dna02%ieP02%auheZ=526535609.yrtne&ecnadiuG02%evitanimircsiD02%htiw02%noitareneG02%golireV02%dellortnoC02%A3%VretteB=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20BetterV%3A%20Controlled%20Verilog%20Generation%20with%20Discriminative%20Guidance&body=Title%3A%20BetterV%3A%20Controlled%20Verilog%20Generation%20with%20Discriminative%20Guidance%0AAuthor%3A%20Zehua%20Pei%20and%20Hui-Ling%20Zhen%20and%20Mingxuan%20Yuan%20and%20Yu%20Huang%20and%20Bei%20Yu%0AAbstract%3A%20%20%20Due%20to%20the%20growing%20complexity%20of%20modern%20Integrated%20Circuits%20%28ICs%29%2C%20there%20is%20a%0Aneed%20for%20automated%20circuit%20design%20methods.%20Recent%20years%20have%20seen%20rising%0Aresearch%20in%20hardware%20design%20language%20generation%20to%20facilitate%20the%20design%0Aprocess.%20In%20this%20work%2C%20we%20propose%20a%20Verilog%20generation%20framework%2C%20BetterV%2C%0Awhich%20fine-tunes%20the%20large%20language%20models%20%28LLMs%29%20on%20processed%20domain-specific%0Adatasets%20and%20incorporates%20generative%20discriminators%20for%20guidance%20on%20particular%0Adesign%20demands.%20The%20Verilog%20modules%20are%20collected%2C%20filtered%20and%20processed%20from%0Ainternet%20to%20form%20a%20clean%20and%20abundant%20dataset.%20Instruct-tuning%20methods%20are%0Aspecially%20designed%20to%20fine-tune%20the%20LLMs%20to%20understand%20the%20knowledge%20about%0AVerilog.%20Furthermore%2C%20data%20are%20augmented%20to%20enrich%20the%20training%20set%20and%20also%0Aused%20to%20train%20a%20generative%20discriminator%20on%20particular%20downstream%20task%2C%20which%0Aleads%20a%20guidance%20for%20the%20LLMs%20to%20optimize%20the%20Verilog%20implementation.%20BetterV%0Ahas%20the%20ability%20to%20generate%20syntactically%20and%20functionally%20correct%20Verilog%2C%0Awhich%20can%20outperform%20GPT-4%20on%20the%20VerilogEval%20benchmark.%20With%20the%20help%20of%0Atask-specific%20generative%20discriminator%2C%20BetterV%20can%20achieve%20remarkable%0Aimprovement%20on%20various%20electronic%20design%20automation%20%28EDA%29%20downstream%20tasks%2C%0Aincluding%20the%20netlist%20node%20reduction%20for%20synthesis%20and%20verification%20runtime%0Areduction%20with%20Boolean%20Satisfiability%20%28SAT%29%20solving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.03375v3%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BetterV%3A%20Controlled%20Verilog%20Generation%20with%20Discriminative%20Guidance&entry.906535625=Zehua%20Pei%20and%20Hui-Ling%20Zhen%20and%20Mingxuan%20Yuan%20and%20Yu%20Huang%20and%20Bei%20Yu&entry.1292438233=%20%20Due%20to%20the%20growing%20complexity%20of%20modern%20Integrated%20Circuits%20%28ICs%29%2C%20there%20is%20a%0Aneed%20for%20automated%20circuit%20design%20methods.%20Recent%20years%20have%20seen%20rising%0Aresearch%20in%20hardware%20design%20language%20generation%20to%20facilitate%20the%20design%0Aprocess.%20In%20this%20work%2C%20we%20propose%20a%20Verilog%20generation%20framework%2C%20BetterV%2C%0Awhich%20fine-tunes%20the%20large%20language%20models%20%28LLMs%29%20on%20processed%20domain-specific%0Adatasets%20and%20incorporates%20generative%20discriminators%20for%20guidance%20on%20particular%0Adesign%20demands.%20The%20Verilog%20modules%20are%20collected%2C%20filtered%20and%20processed%20from%0Ainternet%20to%20form%20a%20clean%20and%20abundant%20dataset.%20Instruct-tuning%20methods%20are%0Aspecially%20designed%20to%20fine-tune%20the%20LLMs%20to%20understand%20the%20knowledge%20about%0AVerilog.%20Furthermore%2C%20data%20are%20augmented%20to%20enrich%20the%20training%20set%20and%20also%0Aused%20to%20train%20a%20generative%20discriminator%20on%20particular%20downstream%20task%2C%20which%0Aleads%20a%20guidance%20for%20the%20LLMs%20to%20optimize%20the%20Verilog%20implementation.%20BetterV%0Ahas%20the%20ability%20to%20generate%20syntactically%20and%20functionally%20correct%20Verilog%2C%0Awhich%20can%20outperform%20GPT-4%20on%20the%20VerilogEval%20benchmark.%20With%20the%20help%20of%0Atask-specific%20generative%20discriminator%2C%20BetterV%20can%20achieve%20remarkable%0Aimprovement%20on%20various%20electronic%20design%20automation%20%28EDA%29%20downstream%20tasks%2C%0Aincluding%20the%20netlist%20node%20reduction%20for%20synthesis%20and%20verification%20runtime%0Areduction%20with%20Boolean%20Satisfiability%20%28SAT%29%20solving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.03375v3&entry.124074799=Read"},
{"title": "Unconstrained Stochastic CCA: Unifying Multiview and Self-Supervised\n  Learning", "author": "James Chapman and Lennie Wells and Ana Lawry Aguila", "abstract": "  The Canonical Correlation Analysis (CCA) family of methods is foundational in\nmultiview learning. Regularised linear CCA methods can be seen to generalise\nPartial Least Squares (PLS) and be unified with a Generalized Eigenvalue\nProblem (GEP) framework. However, classical algorithms for these linear methods\nare computationally infeasible for large-scale data. Extensions to Deep CCA\nshow great promise, but current training procedures are slow and complicated.\nFirst we propose a novel unconstrained objective that characterizes the top\nsubspace of GEPs. Our core contribution is a family of fast algorithms for\nstochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying\nstochastic gradient descent (SGD) to the corresponding CCA objectives. Our\nalgorithms show far faster convergence and recover higher correlations than the\nprevious state-of-the-art on all standard CCA and Deep CCA benchmarks. These\nimprovements allow us to perform a first-of-its-kind PLS analysis of an\nextremely large biomedical dataset from the UK Biobank, with over 33,000\nindividuals and 500,000 features. Finally, we apply our algorithms to match the\nperformance of `CCA-family' Self-Supervised Learning (SSL) methods on CIFAR-10\nand CIFAR-100 with minimal hyper-parameter tuning, and also present theory to\nclarify the links between these methods and classical CCA, laying the\ngroundwork for future insights.\n", "link": "http://arxiv.org/abs/2310.01012v4", "date": "2024-05-01", "relevancy": 2.5516, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5358}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5003}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4949}], "mailto": "mailto:daeR=997470421.yrtne&4v21010.0132/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.sthgisni02%erutuf02%rof02%krowdnuorgA0%eht02%gniyal02%C2%ACC02%lacissalc02%dna02%sdohtem02%eseht02%neewteb02%sknil02%eht02%yfiralcA0%ot02%yroeht02%tneserp02%osla02%dna02%C2%gninut02%retemarap-repyh02%laminim02%htiw02%001-RAFIC02%dnaA0%01-RAFIC02%no02%sdohtem02%92%LSS82%02%gninraeL02%desivrepuS-fleS02%72%ylimaf-ACC06%02%fo02%ecnamrofrepA0%eht02%hctam02%ot02%smhtirogla02%ruo02%ylppa02%ew02%C2%yllaniF02%.serutaef02%000C2%00502%dna02%slaudividniA0%000C2%3302%revo02%htiw02%C2%knaboiB02%KU02%eht02%morf02%tesatad02%lacidemoib02%egral02%ylemertxeA0%na02%fo02%sisylana02%SLP02%dnik-sti-fo-tsrif02%a02%mrofrep02%ot02%su02%wolla02%stnemevorpmiA0%esehT02%.skramhcneb02%ACC02%peeD02%dna02%ACC02%dradnats02%lla02%no02%tra-eht-fo-etats02%suoiverpA0%eht02%naht02%snoitalerroc02%rehgih02%revocer02%dna02%ecnegrevnoc02%retsaf02%raf02%wohs02%smhtiroglaA0%ruO02%.sevitcejbo02%ACC02%gnidnopserroc02%eht02%ot02%92%DGS82%02%tnecsed02%tneidarg02%citsahcotsA0%gniylppa02%yb02%deniatbo02%ylpmis02%C2%ACC02%peeD02%dna02%C2%ACC02%citsahcots02%C2%SLP02%citsahcotsA0%rof02%smhtirogla02%tsaf02%fo02%ylimaf02%a02%si02%noitubirtnoc02%eroc02%ruO02%.sPEG02%fo02%ecapsbusA0%pot02%eht02%seziretcarahc02%taht02%evitcejbo02%deniartsnocnu02%levon02%a02%esoporp02%ew02%tsriFA0%.detacilpmoc02%dna02%wols02%era02%serudecorp02%gniniart02%tnerruc02%tub02%C2%esimorp02%taerg02%wohsA0%ACC02%peeD02%ot02%snoisnetxE02%.atad02%elacs-egral02%rof02%elbisaefni02%yllanoitatupmoc02%eraA0%sdohtem02%raenil02%eseht02%rof02%smhtirogla02%lacissalc02%C2%revewoH02%.krowemarf02%92%PEG82%02%melborPA0%eulavnegiE02%dezilareneG02%a02%htiw02%deifinu02%eb02%dna02%92%SLP82%02%serauqS02%tsaeL02%laitraPA0%esilareneg02%ot02%nees02%eb02%nac02%sdohtem02%ACC02%raenil02%desiralugeR02%.gninrael02%weivitlumA0%ni02%lanoitadnuof02%si02%sdohtem02%fo02%ylimaf02%92%ACC82%02%sisylanA02%noitalerroC02%lacinonaC02%ehT02%02%=3328342921.yrtne&aliugA02%yrwaL02%anA02%dna02%slleW02%einneL02%dna02%nampahC02%semaJ=526535609.yrtne&gninraeL02%02%A0%desivrepuS-fleS02%dna02%weivitluM02%gniyfinU02%A3%ACC02%citsahcotS02%deniartsnocnU=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Unconstrained%20Stochastic%20CCA%3A%20Unifying%20Multiview%20and%20Self-Supervised%0A%20%20Learning&body=Title%3A%20Unconstrained%20Stochastic%20CCA%3A%20Unifying%20Multiview%20and%20Self-Supervised%0A%20%20Learning%0AAuthor%3A%20James%20Chapman%20and%20Lennie%20Wells%20and%20Ana%20Lawry%20Aguila%0AAbstract%3A%20%20%20The%20Canonical%20Correlation%20Analysis%20%28CCA%29%20family%20of%20methods%20is%20foundational%20in%0Amultiview%20learning.%20Regularised%20linear%20CCA%20methods%20can%20be%20seen%20to%20generalise%0APartial%20Least%20Squares%20%28PLS%29%20and%20be%20unified%20with%20a%20Generalized%20Eigenvalue%0AProblem%20%28GEP%29%20framework.%20However%2C%20classical%20algorithms%20for%20these%20linear%20methods%0Aare%20computationally%20infeasible%20for%20large-scale%20data.%20Extensions%20to%20Deep%20CCA%0Ashow%20great%20promise%2C%20but%20current%20training%20procedures%20are%20slow%20and%20complicated.%0AFirst%20we%20propose%20a%20novel%20unconstrained%20objective%20that%20characterizes%20the%20top%0Asubspace%20of%20GEPs.%20Our%20core%20contribution%20is%20a%20family%20of%20fast%20algorithms%20for%0Astochastic%20PLS%2C%20stochastic%20CCA%2C%20and%20Deep%20CCA%2C%20simply%20obtained%20by%20applying%0Astochastic%20gradient%20descent%20%28SGD%29%20to%20the%20corresponding%20CCA%20objectives.%20Our%0Aalgorithms%20show%20far%20faster%20convergence%20and%20recover%20higher%20correlations%20than%20the%0Aprevious%20state-of-the-art%20on%20all%20standard%20CCA%20and%20Deep%20CCA%20benchmarks.%20These%0Aimprovements%20allow%20us%20to%20perform%20a%20first-of-its-kind%20PLS%20analysis%20of%20an%0Aextremely%20large%20biomedical%20dataset%20from%20the%20UK%20Biobank%2C%20with%20over%2033%2C000%0Aindividuals%20and%20500%2C000%20features.%20Finally%2C%20we%20apply%20our%20algorithms%20to%20match%20the%0Aperformance%20of%20%60CCA-family%27%20Self-Supervised%20Learning%20%28SSL%29%20methods%20on%20CIFAR-10%0Aand%20CIFAR-100%20with%20minimal%20hyper-parameter%20tuning%2C%20and%20also%20present%20theory%20to%0Aclarify%20the%20links%20between%20these%20methods%20and%20classical%20CCA%2C%20laying%20the%0Agroundwork%20for%20future%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01012v4%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unconstrained%20Stochastic%20CCA%3A%20Unifying%20Multiview%20and%20Self-Supervised%0A%20%20Learning&entry.906535625=James%20Chapman%20and%20Lennie%20Wells%20and%20Ana%20Lawry%20Aguila&entry.1292438233=%20%20The%20Canonical%20Correlation%20Analysis%20%28CCA%29%20family%20of%20methods%20is%20foundational%20in%0Amultiview%20learning.%20Regularised%20linear%20CCA%20methods%20can%20be%20seen%20to%20generalise%0APartial%20Least%20Squares%20%28PLS%29%20and%20be%20unified%20with%20a%20Generalized%20Eigenvalue%0AProblem%20%28GEP%29%20framework.%20However%2C%20classical%20algorithms%20for%20these%20linear%20methods%0Aare%20computationally%20infeasible%20for%20large-scale%20data.%20Extensions%20to%20Deep%20CCA%0Ashow%20great%20promise%2C%20but%20current%20training%20procedures%20are%20slow%20and%20complicated.%0AFirst%20we%20propose%20a%20novel%20unconstrained%20objective%20that%20characterizes%20the%20top%0Asubspace%20of%20GEPs.%20Our%20core%20contribution%20is%20a%20family%20of%20fast%20algorithms%20for%0Astochastic%20PLS%2C%20stochastic%20CCA%2C%20and%20Deep%20CCA%2C%20simply%20obtained%20by%20applying%0Astochastic%20gradient%20descent%20%28SGD%29%20to%20the%20corresponding%20CCA%20objectives.%20Our%0Aalgorithms%20show%20far%20faster%20convergence%20and%20recover%20higher%20correlations%20than%20the%0Aprevious%20state-of-the-art%20on%20all%20standard%20CCA%20and%20Deep%20CCA%20benchmarks.%20These%0Aimprovements%20allow%20us%20to%20perform%20a%20first-of-its-kind%20PLS%20analysis%20of%20an%0Aextremely%20large%20biomedical%20dataset%20from%20the%20UK%20Biobank%2C%20with%20over%2033%2C000%0Aindividuals%20and%20500%2C000%20features.%20Finally%2C%20we%20apply%20our%20algorithms%20to%20match%20the%0Aperformance%20of%20%60CCA-family%27%20Self-Supervised%20Learning%20%28SSL%29%20methods%20on%20CIFAR-10%0Aand%20CIFAR-100%20with%20minimal%20hyper-parameter%20tuning%2C%20and%20also%20present%20theory%20to%0Aclarify%20the%20links%20between%20these%20methods%20and%20classical%20CCA%2C%20laying%20the%0Agroundwork%20for%20future%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01012v4&entry.124074799=Read"},
{"title": "From Empirical Observations to Universality: Dynamics of Deep Learning\n  with Inputs Built on Gaussian mixture", "author": "Jaeyong Bae and Hawoong Jeong", "abstract": "  This study broadens the scope of theoretical frameworks in deep learning by\ndelving into the dynamics of neural networks with inputs that demonstrate the\nstructural characteristics to Gaussian Mixture (GM). We analyzed how the\ndynamics of neural networks under GM-structured inputs diverge from the\npredictions of conventional theories based on simple Gaussian structures. A\nrevelation of our work is the observed convergence of neural network dynamics\ntowards conventional theory even with standardized GM inputs, highlighting an\nunexpected universality. We found that standardization, especially in\nconjunction with certain nonlinear functions, plays a critical role in this\nphenomena. Consequently, despite the complex and varied nature of GM\ndistributions, we demonstrate that neural networks exhibit asymptotic behaviors\nin line with predictions under simple Gaussian frameworks.\n", "link": "http://arxiv.org/abs/2405.00642v1", "date": "2024-05-01", "relevancy": 2.5361, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5428}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4897}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4891}], "mailto": "mailto:daeR=997470421.yrtne&1v24600.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.skrowemarf02%naissuaG02%elpmis02%rednu02%snoitciderp02%htiw02%enil02%niA0%sroivaheb02%citotpmysa02%tibihxe02%skrowten02%laruen02%taht02%etartsnomed02%ew02%C2%snoitubirtsidA0%MG02%fo02%erutan02%deirav02%dna02%xelpmoc02%eht02%etipsed02%C2%yltneuqesnoC02%.anemonehpA0%siht02%ni02%elor02%lacitirc02%a02%syalp02%C2%snoitcnuf02%raenilnon02%niatrec02%htiw02%noitcnujnocA0%ni02%yllaicepse02%C2%noitazidradnats02%taht02%dnuof02%eW02%.ytilasrevinu02%detcepxenuA0%na02%gnithgilhgih02%C2%stupni02%MG02%dezidradnats02%htiw02%neve02%yroeht02%lanoitnevnoc02%sdrawotA0%scimanyd02%krowten02%laruen02%fo02%ecnegrevnoc02%devresbo02%eht02%si02%krow02%ruo02%fo02%noitaleverA0%A02%.serutcurts02%naissuaG02%elpmis02%no02%desab02%seiroeht02%lanoitnevnoc02%fo02%snoitciderpA0%eht02%morf02%egrevid02%stupni02%derutcurts-MG02%rednu02%skrowten02%laruen02%fo02%scimanydA0%eht02%woh02%dezylana02%eW02%.92%MG82%02%erutxiM02%naissuaG02%ot02%scitsiretcarahc02%larutcurtsA0%eht02%etartsnomed02%taht02%stupni02%htiw02%skrowten02%laruen02%fo02%scimanyd02%eht02%otni02%gnivledA0%yb02%gninrael02%peed02%ni02%skrowemarf02%laciteroeht02%fo02%epocs02%eht02%snedaorb02%yduts02%sihT02%02%=3328342921.yrtne&gnoeJ02%gnoowaH02%dna02%eaB02%gnoyeaJ=526535609.yrtne&erutxim02%naissuaG02%no02%tliuB02%stupnI02%htiw02%02%A0%gninraeL02%peeD02%fo02%scimanyD02%A3%ytilasrevinU02%ot02%snoitavresbO02%laciripmE02%morF=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20From%20Empirical%20Observations%20to%20Universality%3A%20Dynamics%20of%20Deep%20Learning%0A%20%20with%20Inputs%20Built%20on%20Gaussian%20mixture&body=Title%3A%20From%20Empirical%20Observations%20to%20Universality%3A%20Dynamics%20of%20Deep%20Learning%0A%20%20with%20Inputs%20Built%20on%20Gaussian%20mixture%0AAuthor%3A%20Jaeyong%20Bae%20and%20Hawoong%20Jeong%0AAbstract%3A%20%20%20This%20study%20broadens%20the%20scope%20of%20theoretical%20frameworks%20in%20deep%20learning%20by%0Adelving%20into%20the%20dynamics%20of%20neural%20networks%20with%20inputs%20that%20demonstrate%20the%0Astructural%20characteristics%20to%20Gaussian%20Mixture%20%28GM%29.%20We%20analyzed%20how%20the%0Adynamics%20of%20neural%20networks%20under%20GM-structured%20inputs%20diverge%20from%20the%0Apredictions%20of%20conventional%20theories%20based%20on%20simple%20Gaussian%20structures.%20A%0Arevelation%20of%20our%20work%20is%20the%20observed%20convergence%20of%20neural%20network%20dynamics%0Atowards%20conventional%20theory%20even%20with%20standardized%20GM%20inputs%2C%20highlighting%20an%0Aunexpected%20universality.%20We%20found%20that%20standardization%2C%20especially%20in%0Aconjunction%20with%20certain%20nonlinear%20functions%2C%20plays%20a%20critical%20role%20in%20this%0Aphenomena.%20Consequently%2C%20despite%20the%20complex%20and%20varied%20nature%20of%20GM%0Adistributions%2C%20we%20demonstrate%20that%20neural%20networks%20exhibit%20asymptotic%20behaviors%0Ain%20line%20with%20predictions%20under%20simple%20Gaussian%20frameworks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00642v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Empirical%20Observations%20to%20Universality%3A%20Dynamics%20of%20Deep%20Learning%0A%20%20with%20Inputs%20Built%20on%20Gaussian%20mixture&entry.906535625=Jaeyong%20Bae%20and%20Hawoong%20Jeong&entry.1292438233=%20%20This%20study%20broadens%20the%20scope%20of%20theoretical%20frameworks%20in%20deep%20learning%20by%0Adelving%20into%20the%20dynamics%20of%20neural%20networks%20with%20inputs%20that%20demonstrate%20the%0Astructural%20characteristics%20to%20Gaussian%20Mixture%20%28GM%29.%20We%20analyzed%20how%20the%0Adynamics%20of%20neural%20networks%20under%20GM-structured%20inputs%20diverge%20from%20the%0Apredictions%20of%20conventional%20theories%20based%20on%20simple%20Gaussian%20structures.%20A%0Arevelation%20of%20our%20work%20is%20the%20observed%20convergence%20of%20neural%20network%20dynamics%0Atowards%20conventional%20theory%20even%20with%20standardized%20GM%20inputs%2C%20highlighting%20an%0Aunexpected%20universality.%20We%20found%20that%20standardization%2C%20especially%20in%0Aconjunction%20with%20certain%20nonlinear%20functions%2C%20plays%20a%20critical%20role%20in%20this%0Aphenomena.%20Consequently%2C%20despite%20the%20complex%20and%20varied%20nature%20of%20GM%0Adistributions%2C%20we%20demonstrate%20that%20neural%20networks%20exhibit%20asymptotic%20behaviors%0Ain%20line%20with%20predictions%20under%20simple%20Gaussian%20frameworks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00642v1&entry.124074799=Read"},
{"title": "Detecting Generative Parroting through Overfitting Masked Autoencoders", "author": "Saeid Asgari Taghanaki and Joseph Lambourne", "abstract": "  The advent of generative AI models has revolutionized digital content\ncreation, yet it introduces challenges in maintaining copyright integrity due\nto generative parroting, where models mimic their training data too closely.\nOur research presents a novel approach to tackle this issue by employing an\noverfitted Masked Autoencoder (MAE) to detect such parroted samples\neffectively. We establish a detection threshold based on the mean loss across\nthe training dataset, allowing for the precise identification of parroted\ncontent in modified datasets. Preliminary evaluations demonstrate promising\nresults, suggesting our method's potential to ensure ethical use and enhance\nthe legal compliance of generative models.\n", "link": "http://arxiv.org/abs/2403.19050v2", "date": "2024-05-02", "relevancy": 2.5357, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5128}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5116}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.497}], "mailto": "mailto:daeR=997470421.yrtne&2v05091.3042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.sledom02%evitareneg02%fo02%ecnailpmoc02%lagel02%ehtA0%ecnahne02%dna02%esu02%lacihte02%erusne02%ot02%laitnetop02%s72%dohtem02%ruo02%gnitseggus02%C2%stluserA0%gnisimorp02%etartsnomed02%snoitaulave02%yranimilerP02%.stesatad02%deifidom02%ni02%tnetnocA0%detorrap02%fo02%noitacifitnedi02%esicerp02%eht02%rof02%gniwolla02%C2%tesatad02%gniniart02%ehtA0%ssorca02%ssol02%naem02%eht02%no02%desab02%dlohserht02%noitceted02%a02%hsilbatse02%eW02%.ylevitceffeA0%selpmas02%detorrap02%hcus02%tceted02%ot02%92%EAM82%02%redocneotuA02%deksaM02%dettifrevoA0%na02%gniyolpme02%yb02%eussi02%siht02%elkcat02%ot02%hcaorppa02%levon02%a02%stneserp02%hcraeser02%ruOA0%.ylesolc02%oot02%atad02%gniniart02%rieht02%cimim02%sledom02%erehw02%C2%gnitorrap02%evitareneg02%otA0%eud02%ytirgetni02%thgirypoc02%gniniatniam02%ni02%segnellahc02%secudortni02%ti02%tey02%C2%noitaercA0%tnetnoc02%latigid02%dezinoitulover02%sah02%sledom02%IA02%evitareneg02%fo02%tnevda02%ehT02%02%=3328342921.yrtne&enruobmaL02%hpesoJ02%dna02%ikanahgaT02%iragsA02%dieaS=526535609.yrtne&sredocneotuA02%deksaM02%gnittifrevO02%hguorht02%gnitorraP02%evitareneG02%gnitceteD=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Detecting%20Generative%20Parroting%20through%20Overfitting%20Masked%20Autoencoders&body=Title%3A%20Detecting%20Generative%20Parroting%20through%20Overfitting%20Masked%20Autoencoders%0AAuthor%3A%20Saeid%20Asgari%20Taghanaki%20and%20Joseph%20Lambourne%0AAbstract%3A%20%20%20The%20advent%20of%20generative%20AI%20models%20has%20revolutionized%20digital%20content%0Acreation%2C%20yet%20it%20introduces%20challenges%20in%20maintaining%20copyright%20integrity%20due%0Ato%20generative%20parroting%2C%20where%20models%20mimic%20their%20training%20data%20too%20closely.%0AOur%20research%20presents%20a%20novel%20approach%20to%20tackle%20this%20issue%20by%20employing%20an%0Aoverfitted%20Masked%20Autoencoder%20%28MAE%29%20to%20detect%20such%20parroted%20samples%0Aeffectively.%20We%20establish%20a%20detection%20threshold%20based%20on%20the%20mean%20loss%20across%0Athe%20training%20dataset%2C%20allowing%20for%20the%20precise%20identification%20of%20parroted%0Acontent%20in%20modified%20datasets.%20Preliminary%20evaluations%20demonstrate%20promising%0Aresults%2C%20suggesting%20our%20method%27s%20potential%20to%20ensure%20ethical%20use%20and%20enhance%0Athe%20legal%20compliance%20of%20generative%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19050v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Generative%20Parroting%20through%20Overfitting%20Masked%20Autoencoders&entry.906535625=Saeid%20Asgari%20Taghanaki%20and%20Joseph%20Lambourne&entry.1292438233=%20%20The%20advent%20of%20generative%20AI%20models%20has%20revolutionized%20digital%20content%0Acreation%2C%20yet%20it%20introduces%20challenges%20in%20maintaining%20copyright%20integrity%20due%0Ato%20generative%20parroting%2C%20where%20models%20mimic%20their%20training%20data%20too%20closely.%0AOur%20research%20presents%20a%20novel%20approach%20to%20tackle%20this%20issue%20by%20employing%20an%0Aoverfitted%20Masked%20Autoencoder%20%28MAE%29%20to%20detect%20such%20parroted%20samples%0Aeffectively.%20We%20establish%20a%20detection%20threshold%20based%20on%20the%20mean%20loss%20across%0Athe%20training%20dataset%2C%20allowing%20for%20the%20precise%20identification%20of%20parroted%0Acontent%20in%20modified%20datasets.%20Preliminary%20evaluations%20demonstrate%20promising%0Aresults%2C%20suggesting%20our%20method%27s%20potential%20to%20ensure%20ethical%20use%20and%20enhance%0Athe%20legal%20compliance%20of%20generative%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19050v2&entry.124074799=Read"},
{"title": "Goal-conditioned reinforcement learning for ultrasound navigation\n  guidance", "author": "Abdoul Aziz Amadou and Vivek Singh and Florin C. Ghesu and Young-Ho Kim and Laura Stanciulescu and Harshitha P. Sai and Puneet Sharma and Alistair Young and Ronak Rajani and Kawal Rhode", "abstract": "  Transesophageal echocardiography (TEE) plays a pivotal role in cardiology for\ndiagnostic and interventional procedures. However, using it effectively\nrequires extensive training due to the intricate nature of image acquisition\nand interpretation. To enhance the efficiency of novice sonographers and reduce\nvariability in scan acquisitions, we propose a novel ultrasound (US) navigation\nassistance method based on contrastive learning as goal-conditioned\nreinforcement learning (GCRL). We augment the previous framework using a novel\ncontrastive patient batching method (CPB) and a data-augmented contrastive\nloss, both of which we demonstrate are essential to ensure generalization to\nanatomical variations across patients. The proposed framework enables\nnavigation to both standard diagnostic as well as intricate interventional\nviews with a single model. Our method was developed with a large dataset of 789\npatients and obtained an average error of 6.56 mm in position and 9.36 degrees\nin angle on a testing dataset of 140 patients, which is competitive or superior\nto models trained on individual views. Furthermore, we quantitatively validate\nour method's ability to navigate to interventional views such as the Left\nAtrial Appendage (LAA) view used in LAA closure. Our approach holds promise in\nproviding valuable guidance during transesophageal ultrasound examinations,\ncontributing to the advancement of skill acquisition for cardiac ultrasound\npractitioners.\n", "link": "http://arxiv.org/abs/2405.01409v1", "date": "2024-05-02", "relevancy": 2.5106, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5062}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5014}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4988}], "mailto": "mailto:daeR=997470421.yrtne&1v90410.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.srenoititcarpA0%dnuosartlu02%caidrac02%rof02%noitisiuqca02%lliks02%fo02%tnemecnavda02%eht02%ot02%gnitubirtnocA0%C2%snoitanimaxe02%dnuosartlu02%laegahposesnart02%gnirud02%ecnadiug02%elbaulav02%gnidivorpA0%ni02%esimorp02%sdloh02%hcaorppa02%ruO02%.erusolc02%AAL02%ni02%desu02%weiv02%92%AAL82%02%egadneppA02%lairtAA0%tfeL02%eht02%sa02%hcus02%sweiv02%lanoitnevretni02%ot02%etagivan02%ot02%ytiliba02%s72%dohtem02%ruoA0%etadilav02%ylevitatitnauq02%ew02%C2%eromrehtruF02%.sweiv02%laudividni02%no02%deniart02%sledom02%otA0%roirepus02%ro02%evititepmoc02%si02%hcihw02%C2%stneitap02%04102%fo02%tesatad02%gnitset02%a02%no02%elgna02%niA0%seerged02%63.902%dna02%noitisop02%ni02%mm02%65.602%fo02%rorre02%egareva02%na02%deniatbo02%dna02%stneitapA0%98702%fo02%tesatad02%egral02%a02%htiw02%depoleved02%saw02%dohtem02%ruO02%.ledom02%elgnis02%a02%htiw02%sweivA0%lanoitnevretni02%etacirtni02%sa02%llew02%sa02%citsongaid02%dradnats02%htob02%ot02%noitagivanA0%selbane02%krowemarf02%desoporp02%ehT02%.stneitap02%ssorca02%snoitairav02%lacimotanaA0%ot02%noitazilareneg02%erusne02%ot02%laitnesse02%era02%etartsnomed02%ew02%hcihw02%fo02%htob02%C2%ssolA0%evitsartnoc02%detnemgua-atad02%a02%dna02%92%BPC82%02%dohtem02%gnihctab02%tneitap02%evitsartnocA0%levon02%a02%gnisu02%krowemarf02%suoiverp02%eht02%tnemgua02%eW02%.92%LRCG82%02%gninrael02%tnemecrofnierA0%denoitidnoc-laog02%sa02%gninrael02%evitsartnoc02%no02%desab02%dohtem02%ecnatsissaA0%noitagivan02%92%SU82%02%dnuosartlu02%levon02%a02%esoporp02%ew02%C2%snoitisiuqca02%nacs02%ni02%ytilibairavA0%ecuder02%dna02%srehpargonos02%ecivon02%fo02%ycneiciffe02%eht02%ecnahne02%oT02%.noitaterpretni02%dnaA0%noitisiuqca02%egami02%fo02%erutan02%etacirtni02%eht02%ot02%eud02%gniniart02%evisnetxe02%seriuqerA0%ylevitceffe02%ti02%gnisu02%C2%revewoH02%.serudecorp02%lanoitnevretni02%dna02%citsongaidA0%rof02%ygoloidrac02%ni02%elor02%latovip02%a02%syalp02%92%EET82%02%yhpargoidracohce02%laegahposesnarT02%02%=3328342921.yrtne&edohR02%lawaK02%dna02%inajaR02%kanoR02%dna02%gnuoY02%riatsilA02%dna02%amrahS02%teenuP02%dna02%iaS02%.P02%ahtihsraH02%dna02%ucseluicnatS02%aruaL02%dna02%miK02%oH-gnuoY02%dna02%usehG02%.C02%nirolF02%dna02%hgniS02%keviV02%dna02%uodamA02%zizA02%luodbA=526535609.yrtne&ecnadiug02%02%A0%noitagivan02%dnuosartlu02%rof02%gninrael02%tnemecrofnier02%denoitidnoc-laoG=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Goal-conditioned%20reinforcement%20learning%20for%20ultrasound%20navigation%0A%20%20guidance&body=Title%3A%20Goal-conditioned%20reinforcement%20learning%20for%20ultrasound%20navigation%0A%20%20guidance%0AAuthor%3A%20Abdoul%20Aziz%20Amadou%20and%20Vivek%20Singh%20and%20Florin%20C.%20Ghesu%20and%20Young-Ho%20Kim%20and%20Laura%20Stanciulescu%20and%20Harshitha%20P.%20Sai%20and%20Puneet%20Sharma%20and%20Alistair%20Young%20and%20Ronak%20Rajani%20and%20Kawal%20Rhode%0AAbstract%3A%20%20%20Transesophageal%20echocardiography%20%28TEE%29%20plays%20a%20pivotal%20role%20in%20cardiology%20for%0Adiagnostic%20and%20interventional%20procedures.%20However%2C%20using%20it%20effectively%0Arequires%20extensive%20training%20due%20to%20the%20intricate%20nature%20of%20image%20acquisition%0Aand%20interpretation.%20To%20enhance%20the%20efficiency%20of%20novice%20sonographers%20and%20reduce%0Avariability%20in%20scan%20acquisitions%2C%20we%20propose%20a%20novel%20ultrasound%20%28US%29%20navigation%0Aassistance%20method%20based%20on%20contrastive%20learning%20as%20goal-conditioned%0Areinforcement%20learning%20%28GCRL%29.%20We%20augment%20the%20previous%20framework%20using%20a%20novel%0Acontrastive%20patient%20batching%20method%20%28CPB%29%20and%20a%20data-augmented%20contrastive%0Aloss%2C%20both%20of%20which%20we%20demonstrate%20are%20essential%20to%20ensure%20generalization%20to%0Aanatomical%20variations%20across%20patients.%20The%20proposed%20framework%20enables%0Anavigation%20to%20both%20standard%20diagnostic%20as%20well%20as%20intricate%20interventional%0Aviews%20with%20a%20single%20model.%20Our%20method%20was%20developed%20with%20a%20large%20dataset%20of%20789%0Apatients%20and%20obtained%20an%20average%20error%20of%206.56%20mm%20in%20position%20and%209.36%20degrees%0Ain%20angle%20on%20a%20testing%20dataset%20of%20140%20patients%2C%20which%20is%20competitive%20or%20superior%0Ato%20models%20trained%20on%20individual%20views.%20Furthermore%2C%20we%20quantitatively%20validate%0Aour%20method%27s%20ability%20to%20navigate%20to%20interventional%20views%20such%20as%20the%20Left%0AAtrial%20Appendage%20%28LAA%29%20view%20used%20in%20LAA%20closure.%20Our%20approach%20holds%20promise%20in%0Aproviding%20valuable%20guidance%20during%20transesophageal%20ultrasound%20examinations%2C%0Acontributing%20to%20the%20advancement%20of%20skill%20acquisition%20for%20cardiac%20ultrasound%0Apractitioners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01409v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Goal-conditioned%20reinforcement%20learning%20for%20ultrasound%20navigation%0A%20%20guidance&entry.906535625=Abdoul%20Aziz%20Amadou%20and%20Vivek%20Singh%20and%20Florin%20C.%20Ghesu%20and%20Young-Ho%20Kim%20and%20Laura%20Stanciulescu%20and%20Harshitha%20P.%20Sai%20and%20Puneet%20Sharma%20and%20Alistair%20Young%20and%20Ronak%20Rajani%20and%20Kawal%20Rhode&entry.1292438233=%20%20Transesophageal%20echocardiography%20%28TEE%29%20plays%20a%20pivotal%20role%20in%20cardiology%20for%0Adiagnostic%20and%20interventional%20procedures.%20However%2C%20using%20it%20effectively%0Arequires%20extensive%20training%20due%20to%20the%20intricate%20nature%20of%20image%20acquisition%0Aand%20interpretation.%20To%20enhance%20the%20efficiency%20of%20novice%20sonographers%20and%20reduce%0Avariability%20in%20scan%20acquisitions%2C%20we%20propose%20a%20novel%20ultrasound%20%28US%29%20navigation%0Aassistance%20method%20based%20on%20contrastive%20learning%20as%20goal-conditioned%0Areinforcement%20learning%20%28GCRL%29.%20We%20augment%20the%20previous%20framework%20using%20a%20novel%0Acontrastive%20patient%20batching%20method%20%28CPB%29%20and%20a%20data-augmented%20contrastive%0Aloss%2C%20both%20of%20which%20we%20demonstrate%20are%20essential%20to%20ensure%20generalization%20to%0Aanatomical%20variations%20across%20patients.%20The%20proposed%20framework%20enables%0Anavigation%20to%20both%20standard%20diagnostic%20as%20well%20as%20intricate%20interventional%0Aviews%20with%20a%20single%20model.%20Our%20method%20was%20developed%20with%20a%20large%20dataset%20of%20789%0Apatients%20and%20obtained%20an%20average%20error%20of%206.56%20mm%20in%20position%20and%209.36%20degrees%0Ain%20angle%20on%20a%20testing%20dataset%20of%20140%20patients%2C%20which%20is%20competitive%20or%20superior%0Ato%20models%20trained%20on%20individual%20views.%20Furthermore%2C%20we%20quantitatively%20validate%0Aour%20method%27s%20ability%20to%20navigate%20to%20interventional%20views%20such%20as%20the%20Left%0AAtrial%20Appendage%20%28LAA%29%20view%20used%20in%20LAA%20closure.%20Our%20approach%20holds%20promise%20in%0Aproviding%20valuable%20guidance%20during%20transesophageal%20ultrasound%20examinations%2C%0Acontributing%20to%20the%20advancement%20of%20skill%20acquisition%20for%20cardiac%20ultrasound%0Apractitioners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01409v1&entry.124074799=Read"},
{"title": "Efficient Data-driven Scene Simulation using Robotic Surgery Videos via\n  Physics-embedded 3D Gaussians", "author": "Zhenya Yang and Kai Chen and Yonghao Long and Qi Dou", "abstract": "  Surgical scene simulation plays a crucial role in surgical education and\nsimulator-based robot learning. Traditional approaches for creating these\nenvironments with surgical scene involve a labor-intensive process where\ndesigners hand-craft tissues models with textures and geometries for soft body\nsimulations. This manual approach is not only time-consuming but also limited\nin the scalability and realism. In contrast, data-driven simulation offers a\ncompelling alternative. It has the potential to automatically reconstruct 3D\nsurgical scenes from real-world surgical video data, followed by the\napplication of soft body physics. This area, however, is relatively uncharted.\nIn our research, we introduce 3D Gaussian as a learnable representation for\nsurgical scene, which is learned from stereo endoscopic video. To prevent\nover-fitting and ensure the geometrical correctness of these scenes, we\nincorporate depth supervision and anisotropy regularization into the Gaussian\nlearning process. Furthermore, we apply the Material Point Method, which is\nintegrated with physical properties, to the 3D Gaussians to achieve realistic\nscene deformations. Our method was evaluated on our collected in-house and\npublic surgical videos datasets. Results show that it can reconstruct and\nsimulate surgical scenes from endoscopic videos efficiently-taking only a few\nminutes to reconstruct the surgical scene-and produce both visually and\nphysically plausible deformations at a speed approaching real-time. The results\ndemonstrate great potential of our proposed method to enhance the efficiency\nand variety of simulations available for surgical education and robot learning.\n", "link": "http://arxiv.org/abs/2405.00956v1", "date": "2024-05-02", "relevancy": 2.5059, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6449}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6254}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5833}], "mailto": "mailto:daeR=997470421.yrtne&1v65900.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.gninrael02%tobor02%dna02%noitacude02%lacigrus02%rof02%elbaliava02%snoitalumis02%fo02%yteirav02%dnaA0%ycneiciffe02%eht02%ecnahne02%ot02%dohtem02%desoporp02%ruo02%fo02%laitnetop02%taerg02%etartsnomedA0%stluser02%ehT02%.emit-laer02%gnihcaorppa02%deeps02%a02%ta02%snoitamrofed02%elbisualp02%yllacisyhpA0%dna02%yllausiv02%htob02%ecudorp02%dna-enecs02%lacigrus02%eht02%tcurtsnocer02%ot02%setunimA0%wef02%a02%ylno02%gnikat-yltneiciffe02%soediv02%cipocsodne02%morf02%senecs02%lacigrus02%etalumisA0%dna02%tcurtsnocer02%nac02%ti02%taht02%wohs02%stluseR02%.stesatad02%soediv02%lacigrus02%cilbupA0%dna02%esuoh-ni02%detcelloc02%ruo02%no02%detaulave02%saw02%dohtem02%ruO02%.snoitamrofed02%enecsA0%citsilaer02%eveihca02%ot02%snaissuaG02%D302%eht02%ot02%C2%seitreporp02%lacisyhp02%htiw02%detargetniA0%si02%hcihw02%C2%dohteM02%tnioP02%lairetaM02%eht02%ylppa02%ew02%C2%eromrehtruF02%.ssecorp02%gninraelA0%naissuaG02%eht02%otni02%noitaziraluger02%yportosina02%dna02%noisivrepus02%htped02%etaroprocniA0%ew02%C2%senecs02%eseht02%fo02%ssentcerroc02%lacirtemoeg02%eht02%erusne02%dna02%gnittif-revoA0%tneverp02%oT02%.oediv02%cipocsodne02%oerets02%morf02%denrael02%si02%hcihw02%C2%enecs02%lacigrusA0%rof02%noitatneserper02%elbanrael02%a02%sa02%naissuaG02%D302%ecudortni02%ew02%C2%hcraeser02%ruo02%nIA0%.detrahcnu02%ylevitaler02%si02%C2%revewoh02%C2%aera02%sihT02%.scisyhp02%ydob02%tfos02%fo02%noitacilppaA0%eht02%yb02%dewollof02%C2%atad02%oediv02%lacigrus02%dlrow-laer02%morf02%senecs02%lacigrusA0%D302%tcurtsnocer02%yllacitamotua02%ot02%laitnetop02%eht02%sah02%tI02%.evitanretla02%gnillepmocA0%a02%sreffo02%noitalumis02%nevird-atad02%C2%tsartnoc02%nI02%.msilaer02%dna02%ytilibalacs02%eht02%niA0%detimil02%osla02%tub02%gnimusnoc-emit02%ylno02%ton02%si02%hcaorppa02%launam02%sihT02%.snoitalumisA0%ydob02%tfos02%rof02%seirtemoeg02%dna02%serutxet02%htiw02%sledom02%seussit02%tfarc-dnah02%srengisedA0%erehw02%ssecorp02%evisnetni-robal02%a02%evlovni02%enecs02%lacigrus02%htiw02%stnemnorivneA0%eseht02%gnitaerc02%rof02%sehcaorppa02%lanoitidarT02%.gninrael02%tobor02%desab-rotalumisA0%dna02%noitacude02%lacigrus02%ni02%elor02%laicurc02%a02%syalp02%noitalumis02%enecs02%lacigruS02%02%=3328342921.yrtne&uoD02%iQ02%dna02%gnoL02%oahgnoY02%dna02%nehC02%iaK02%dna02%gnaY02%aynehZ=526535609.yrtne&snaissuaG02%D302%deddebme-scisyhP02%02%A0%aiv02%soediV02%yregruS02%citoboR02%gnisu02%noitalumiS02%enecS02%nevird-ataD02%tneiciffE=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Efficient%20Data-driven%20Scene%20Simulation%20using%20Robotic%20Surgery%20Videos%20via%0A%20%20Physics-embedded%203D%20Gaussians&body=Title%3A%20Efficient%20Data-driven%20Scene%20Simulation%20using%20Robotic%20Surgery%20Videos%20via%0A%20%20Physics-embedded%203D%20Gaussians%0AAuthor%3A%20Zhenya%20Yang%20and%20Kai%20Chen%20and%20Yonghao%20Long%20and%20Qi%20Dou%0AAbstract%3A%20%20%20Surgical%20scene%20simulation%20plays%20a%20crucial%20role%20in%20surgical%20education%20and%0Asimulator-based%20robot%20learning.%20Traditional%20approaches%20for%20creating%20these%0Aenvironments%20with%20surgical%20scene%20involve%20a%20labor-intensive%20process%20where%0Adesigners%20hand-craft%20tissues%20models%20with%20textures%20and%20geometries%20for%20soft%20body%0Asimulations.%20This%20manual%20approach%20is%20not%20only%20time-consuming%20but%20also%20limited%0Ain%20the%20scalability%20and%20realism.%20In%20contrast%2C%20data-driven%20simulation%20offers%20a%0Acompelling%20alternative.%20It%20has%20the%20potential%20to%20automatically%20reconstruct%203D%0Asurgical%20scenes%20from%20real-world%20surgical%20video%20data%2C%20followed%20by%20the%0Aapplication%20of%20soft%20body%20physics.%20This%20area%2C%20however%2C%20is%20relatively%20uncharted.%0AIn%20our%20research%2C%20we%20introduce%203D%20Gaussian%20as%20a%20learnable%20representation%20for%0Asurgical%20scene%2C%20which%20is%20learned%20from%20stereo%20endoscopic%20video.%20To%20prevent%0Aover-fitting%20and%20ensure%20the%20geometrical%20correctness%20of%20these%20scenes%2C%20we%0Aincorporate%20depth%20supervision%20and%20anisotropy%20regularization%20into%20the%20Gaussian%0Alearning%20process.%20Furthermore%2C%20we%20apply%20the%20Material%20Point%20Method%2C%20which%20is%0Aintegrated%20with%20physical%20properties%2C%20to%20the%203D%20Gaussians%20to%20achieve%20realistic%0Ascene%20deformations.%20Our%20method%20was%20evaluated%20on%20our%20collected%20in-house%20and%0Apublic%20surgical%20videos%20datasets.%20Results%20show%20that%20it%20can%20reconstruct%20and%0Asimulate%20surgical%20scenes%20from%20endoscopic%20videos%20efficiently-taking%20only%20a%20few%0Aminutes%20to%20reconstruct%20the%20surgical%20scene-and%20produce%20both%20visually%20and%0Aphysically%20plausible%20deformations%20at%20a%20speed%20approaching%20real-time.%20The%20results%0Ademonstrate%20great%20potential%20of%20our%20proposed%20method%20to%20enhance%20the%20efficiency%0Aand%20variety%20of%20simulations%20available%20for%20surgical%20education%20and%20robot%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00956v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Data-driven%20Scene%20Simulation%20using%20Robotic%20Surgery%20Videos%20via%0A%20%20Physics-embedded%203D%20Gaussians&entry.906535625=Zhenya%20Yang%20and%20Kai%20Chen%20and%20Yonghao%20Long%20and%20Qi%20Dou&entry.1292438233=%20%20Surgical%20scene%20simulation%20plays%20a%20crucial%20role%20in%20surgical%20education%20and%0Asimulator-based%20robot%20learning.%20Traditional%20approaches%20for%20creating%20these%0Aenvironments%20with%20surgical%20scene%20involve%20a%20labor-intensive%20process%20where%0Adesigners%20hand-craft%20tissues%20models%20with%20textures%20and%20geometries%20for%20soft%20body%0Asimulations.%20This%20manual%20approach%20is%20not%20only%20time-consuming%20but%20also%20limited%0Ain%20the%20scalability%20and%20realism.%20In%20contrast%2C%20data-driven%20simulation%20offers%20a%0Acompelling%20alternative.%20It%20has%20the%20potential%20to%20automatically%20reconstruct%203D%0Asurgical%20scenes%20from%20real-world%20surgical%20video%20data%2C%20followed%20by%20the%0Aapplication%20of%20soft%20body%20physics.%20This%20area%2C%20however%2C%20is%20relatively%20uncharted.%0AIn%20our%20research%2C%20we%20introduce%203D%20Gaussian%20as%20a%20learnable%20representation%20for%0Asurgical%20scene%2C%20which%20is%20learned%20from%20stereo%20endoscopic%20video.%20To%20prevent%0Aover-fitting%20and%20ensure%20the%20geometrical%20correctness%20of%20these%20scenes%2C%20we%0Aincorporate%20depth%20supervision%20and%20anisotropy%20regularization%20into%20the%20Gaussian%0Alearning%20process.%20Furthermore%2C%20we%20apply%20the%20Material%20Point%20Method%2C%20which%20is%0Aintegrated%20with%20physical%20properties%2C%20to%20the%203D%20Gaussians%20to%20achieve%20realistic%0Ascene%20deformations.%20Our%20method%20was%20evaluated%20on%20our%20collected%20in-house%20and%0Apublic%20surgical%20videos%20datasets.%20Results%20show%20that%20it%20can%20reconstruct%20and%0Asimulate%20surgical%20scenes%20from%20endoscopic%20videos%20efficiently-taking%20only%20a%20few%0Aminutes%20to%20reconstruct%20the%20surgical%20scene-and%20produce%20both%20visually%20and%0Aphysically%20plausible%20deformations%20at%20a%20speed%20approaching%20real-time.%20The%20results%0Ademonstrate%20great%20potential%20of%20our%20proposed%20method%20to%20enhance%20the%20efficiency%0Aand%20variety%20of%20simulations%20available%20for%20surgical%20education%20and%20robot%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00956v1&entry.124074799=Read"},
{"title": "Feature-Aware Noise Contrastive Learning For Unsupervised Red Panda\n  Re-Identification", "author": "Jincheng Zhang and Qijun Zhao and Tie Liu", "abstract": "  To facilitate the re-identification (Re-ID) of individual animals, existing\nmethods primarily focus on maximizing feature similarity within the same\nindividual and enhancing distinctiveness between different individuals.\nHowever, most of them still rely on supervised learning and require substantial\nlabeled data, which is challenging to obtain. To avoid this issue, we propose a\nFeature-Aware Noise Contrastive Learning (FANCL) method to explore an\nunsupervised learning solution, which is then validated on the task of red\npanda re-ID. FANCL employs a Feature-Aware Noise Addition module to produce\nnoised images that conceal critical features and designs two contrastive\nlearning modules to calculate the losses. Firstly, a feature consistency module\nis designed to bridge the gap between the original and noised features.\nSecondly, the neural networks are trained through a cluster contrastive\nlearning module. Through these more challenging learning tasks, FANCL can\nadaptively extract deeper representations of red pandas. The experimental\nresults on a set of red panda images collected in both indoor and outdoor\nenvironments prove that FANCL outperforms several related state-of-the-art\nunsupervised methods, achieving high performance comparable to supervised\nlearning methods.\n", "link": "http://arxiv.org/abs/2405.00468v1", "date": "2024-05-01", "relevancy": 2.5008, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5028}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5018}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4959}], "mailto": "mailto:daeR=997470421.yrtne&1v86400.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.sdohtem02%gninraelA0%desivrepus02%ot02%elbarapmoc02%ecnamrofrep02%hgih02%gniveihca02%C2%sdohtem02%desivrepusnuA0%tra-eht-fo-etats02%detaler02%lareves02%smrofreptuo02%LCNAF02%taht02%evorp02%stnemnorivneA0%roodtuo02%dna02%roodni02%htob02%ni02%detcelloc02%segami02%adnap02%der02%fo02%tes02%a02%no02%stluserA0%latnemirepxe02%ehT02%.sadnap02%der02%fo02%snoitatneserper02%repeed02%tcartxe02%ylevitpadaA0%nac02%LCNAF02%C2%sksat02%gninrael02%gnignellahc02%erom02%eseht02%hguorhT02%.eludom02%gninraelA0%evitsartnoc02%retsulc02%a02%hguorht02%deniart02%era02%skrowten02%laruen02%eht02%C2%yldnoceSA0%.serutaef02%desion02%dna02%lanigiro02%eht02%neewteb02%pag02%eht02%egdirb02%ot02%dengised02%siA0%eludom02%ycnetsisnoc02%erutaef02%a02%C2%yltsriF02%.sessol02%eht02%etaluclac02%ot02%seludom02%gninraelA0%evitsartnoc02%owt02%sngised02%dna02%serutaef02%lacitirc02%laecnoc02%taht02%segami02%desionA0%ecudorp02%ot02%eludom02%noitiddA02%esioN02%erawA-erutaeF02%a02%syolpme02%LCNAF02%.DI-er02%adnapA0%der02%fo02%ksat02%eht02%no02%detadilav02%neht02%si02%hcihw02%C2%noitulos02%gninrael02%desivrepusnuA0%na02%erolpxe02%ot02%dohtem02%92%LCNAF82%02%gninraeL02%evitsartnoC02%esioN02%erawA-erutaeFA0%a02%esoporp02%ew02%C2%eussi02%siht02%diova02%oT02%.niatbo02%ot02%gnignellahc02%si02%hcihw02%C2%atad02%delebalA0%laitnatsbus02%eriuqer02%dna02%gninrael02%desivrepus02%no02%yler02%llits02%meht02%fo02%tsom02%C2%revewoHA0%.slaudividni02%tnereffid02%neewteb02%ssenevitcnitsid02%gnicnahne02%dna02%laudividniA0%emas02%eht02%nihtiw02%ytiralimis02%erutaef02%gnizimixam02%no02%sucof02%yliramirp02%sdohtemA0%gnitsixe02%C2%slamina02%laudividni02%fo02%92%DI-eR82%02%noitacifitnedi-er02%eht02%etatilicaf02%oT02%02%=3328342921.yrtne&uiL02%eiT02%dna02%oahZ02%nujiQ02%dna02%gnahZ02%gnehcniJ=526535609.yrtne&noitacifitnedI-eR02%02%A0%adnaP02%deR02%desivrepusnU02%roF02%gninraeL02%evitsartnoC02%esioN02%erawA-erutaeF=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Feature-Aware%20Noise%20Contrastive%20Learning%20For%20Unsupervised%20Red%20Panda%0A%20%20Re-Identification&body=Title%3A%20Feature-Aware%20Noise%20Contrastive%20Learning%20For%20Unsupervised%20Red%20Panda%0A%20%20Re-Identification%0AAuthor%3A%20Jincheng%20Zhang%20and%20Qijun%20Zhao%20and%20Tie%20Liu%0AAbstract%3A%20%20%20To%20facilitate%20the%20re-identification%20%28Re-ID%29%20of%20individual%20animals%2C%20existing%0Amethods%20primarily%20focus%20on%20maximizing%20feature%20similarity%20within%20the%20same%0Aindividual%20and%20enhancing%20distinctiveness%20between%20different%20individuals.%0AHowever%2C%20most%20of%20them%20still%20rely%20on%20supervised%20learning%20and%20require%20substantial%0Alabeled%20data%2C%20which%20is%20challenging%20to%20obtain.%20To%20avoid%20this%20issue%2C%20we%20propose%20a%0AFeature-Aware%20Noise%20Contrastive%20Learning%20%28FANCL%29%20method%20to%20explore%20an%0Aunsupervised%20learning%20solution%2C%20which%20is%20then%20validated%20on%20the%20task%20of%20red%0Apanda%20re-ID.%20FANCL%20employs%20a%20Feature-Aware%20Noise%20Addition%20module%20to%20produce%0Anoised%20images%20that%20conceal%20critical%20features%20and%20designs%20two%20contrastive%0Alearning%20modules%20to%20calculate%20the%20losses.%20Firstly%2C%20a%20feature%20consistency%20module%0Ais%20designed%20to%20bridge%20the%20gap%20between%20the%20original%20and%20noised%20features.%0ASecondly%2C%20the%20neural%20networks%20are%20trained%20through%20a%20cluster%20contrastive%0Alearning%20module.%20Through%20these%20more%20challenging%20learning%20tasks%2C%20FANCL%20can%0Aadaptively%20extract%20deeper%20representations%20of%20red%20pandas.%20The%20experimental%0Aresults%20on%20a%20set%20of%20red%20panda%20images%20collected%20in%20both%20indoor%20and%20outdoor%0Aenvironments%20prove%20that%20FANCL%20outperforms%20several%20related%20state-of-the-art%0Aunsupervised%20methods%2C%20achieving%20high%20performance%20comparable%20to%20supervised%0Alearning%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00468v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Feature-Aware%20Noise%20Contrastive%20Learning%20For%20Unsupervised%20Red%20Panda%0A%20%20Re-Identification&entry.906535625=Jincheng%20Zhang%20and%20Qijun%20Zhao%20and%20Tie%20Liu&entry.1292438233=%20%20To%20facilitate%20the%20re-identification%20%28Re-ID%29%20of%20individual%20animals%2C%20existing%0Amethods%20primarily%20focus%20on%20maximizing%20feature%20similarity%20within%20the%20same%0Aindividual%20and%20enhancing%20distinctiveness%20between%20different%20individuals.%0AHowever%2C%20most%20of%20them%20still%20rely%20on%20supervised%20learning%20and%20require%20substantial%0Alabeled%20data%2C%20which%20is%20challenging%20to%20obtain.%20To%20avoid%20this%20issue%2C%20we%20propose%20a%0AFeature-Aware%20Noise%20Contrastive%20Learning%20%28FANCL%29%20method%20to%20explore%20an%0Aunsupervised%20learning%20solution%2C%20which%20is%20then%20validated%20on%20the%20task%20of%20red%0Apanda%20re-ID.%20FANCL%20employs%20a%20Feature-Aware%20Noise%20Addition%20module%20to%20produce%0Anoised%20images%20that%20conceal%20critical%20features%20and%20designs%20two%20contrastive%0Alearning%20modules%20to%20calculate%20the%20losses.%20Firstly%2C%20a%20feature%20consistency%20module%0Ais%20designed%20to%20bridge%20the%20gap%20between%20the%20original%20and%20noised%20features.%0ASecondly%2C%20the%20neural%20networks%20are%20trained%20through%20a%20cluster%20contrastive%0Alearning%20module.%20Through%20these%20more%20challenging%20learning%20tasks%2C%20FANCL%20can%0Aadaptively%20extract%20deeper%20representations%20of%20red%20pandas.%20The%20experimental%0Aresults%20on%20a%20set%20of%20red%20panda%20images%20collected%20in%20both%20indoor%20and%20outdoor%0Aenvironments%20prove%20that%20FANCL%20outperforms%20several%20related%20state-of-the-art%0Aunsupervised%20methods%2C%20achieving%20high%20performance%20comparable%20to%20supervised%0Alearning%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00468v1&entry.124074799=Read"},
{"title": "Sparse is Enough in Fine-tuning Pre-trained Large Language Models", "author": "Weixi Song and Zuchao Li and Lefei Zhang and Hai Zhao and Bo Du", "abstract": "  With the prevalence of pre-training-fine-tuning paradigm, how to efficiently\nadapt the pre-trained model to the downstream tasks has been an intriguing\nissue. Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed for\nlow-cost adaptation. Although PEFT has demonstrated effectiveness and been\nwidely applied, the underlying principles are still unclear. In this paper, we\nadopt the PAC-Bayesian generalization error bound, viewing pre-training as a\nshift of prior distribution which leads to a tighter bound for generalization\nerror. We validate this shift from the perspectives of oscillations in the loss\nlandscape and the quasi-sparsity in gradient distribution. Based on this, we\npropose a gradient-based sparse fine-tuning algorithm, named Sparse Increment\nFine-Tuning (SIFT), and validate its effectiveness on a range of tasks\nincluding the GLUE Benchmark and Instruction-tuning. The code is accessible at\nhttps://github.com/song-wx/SIFT/.\n", "link": "http://arxiv.org/abs/2312.11875v2", "date": "2024-05-02", "relevancy": 2.4931, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5168}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4945}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4845}], "mailto": "mailto:daeR=997470421.yrtne&2v57811.2132/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%./TFIS/xw-gnos/moc.buhtig//A3%sptthA0%ta02%elbissecca02%si02%edoc02%ehT02%.gninut-noitcurtsnI02%dna02%kramhcneB02%EULG02%eht02%gnidulcniA0%sksat02%fo02%egnar02%a02%no02%ssenevitceffe02%sti02%etadilav02%dna02%C2%92%TFIS82%02%gninuT-eniFA0%tnemercnI02%esrapS02%deman02%C2%mhtirogla02%gninut-enif02%esraps02%desab-tneidarg02%a02%esoporpA0%ew02%C2%siht02%no02%desaB02%.noitubirtsid02%tneidarg02%ni02%ytisraps-isauq02%eht02%dna02%epacsdnalA0%ssol02%eht02%ni02%snoitallicso02%fo02%sevitcepsrep02%eht02%morf02%tfihs02%siht02%etadilav02%eW02%.rorreA0%noitazilareneg02%rof02%dnuob02%rethgit02%a02%ot02%sdael02%hcihw02%noitubirtsid02%roirp02%fo02%tfihsA0%a02%sa02%gniniart-erp02%gniweiv02%C2%dnuob02%rorre02%noitazilareneg02%naiseyaB-CAP02%eht02%tpodaA0%ew02%C2%repap02%siht02%nI02%.raelcnu02%llits02%era02%selpicnirp02%gniylrednu02%eht02%C2%deilppa02%ylediwA0%neeb02%dna02%ssenevitceffe02%detartsnomed02%sah02%TFEP02%hguohtlA02%.noitatpada02%tsoc-wolA0%rof02%desoporp02%neeb02%evah02%sdohtem02%92%TFEP82%02%gninuT-eniF02%tneiciffE-retemaraP02%.eussiA0%gniugirtni02%na02%neeb02%sah02%sksat02%maertsnwod02%eht02%ot02%ledom02%deniart-erp02%eht02%tpadaA0%yltneiciffe02%ot02%woh02%C2%mgidarap02%gninut-enif-gniniart-erp02%fo02%ecnelaverp02%eht02%htiW02%02%=3328342921.yrtne&uD02%oB02%dna02%oahZ02%iaH02%dna02%gnahZ02%iefeL02%dna02%iL02%oahcuZ02%dna02%gnoS02%ixieW=526535609.yrtne&sledoM02%egaugnaL02%egraL02%deniart-erP02%gninut-eniF02%ni02%hguonE02%si02%esrapS=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Sparse%20is%20Enough%20in%20Fine-tuning%20Pre-trained%20Large%20Language%20Models&body=Title%3A%20Sparse%20is%20Enough%20in%20Fine-tuning%20Pre-trained%20Large%20Language%20Models%0AAuthor%3A%20Weixi%20Song%20and%20Zuchao%20Li%20and%20Lefei%20Zhang%20and%20Hai%20Zhao%20and%20Bo%20Du%0AAbstract%3A%20%20%20With%20the%20prevalence%20of%20pre-training-fine-tuning%20paradigm%2C%20how%20to%20efficiently%0Aadapt%20the%20pre-trained%20model%20to%20the%20downstream%20tasks%20has%20been%20an%20intriguing%0Aissue.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20have%20been%20proposed%20for%0Alow-cost%20adaptation.%20Although%20PEFT%20has%20demonstrated%20effectiveness%20and%20been%0Awidely%20applied%2C%20the%20underlying%20principles%20are%20still%20unclear.%20In%20this%20paper%2C%20we%0Aadopt%20the%20PAC-Bayesian%20generalization%20error%20bound%2C%20viewing%20pre-training%20as%20a%0Ashift%20of%20prior%20distribution%20which%20leads%20to%20a%20tighter%20bound%20for%20generalization%0Aerror.%20We%20validate%20this%20shift%20from%20the%20perspectives%20of%20oscillations%20in%20the%20loss%0Alandscape%20and%20the%20quasi-sparsity%20in%20gradient%20distribution.%20Based%20on%20this%2C%20we%0Apropose%20a%20gradient-based%20sparse%20fine-tuning%20algorithm%2C%20named%20Sparse%20Increment%0AFine-Tuning%20%28SIFT%29%2C%20and%20validate%20its%20effectiveness%20on%20a%20range%20of%20tasks%0Aincluding%20the%20GLUE%20Benchmark%20and%20Instruction-tuning.%20The%20code%20is%20accessible%20at%0Ahttps%3A//github.com/song-wx/SIFT/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11875v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sparse%20is%20Enough%20in%20Fine-tuning%20Pre-trained%20Large%20Language%20Models&entry.906535625=Weixi%20Song%20and%20Zuchao%20Li%20and%20Lefei%20Zhang%20and%20Hai%20Zhao%20and%20Bo%20Du&entry.1292438233=%20%20With%20the%20prevalence%20of%20pre-training-fine-tuning%20paradigm%2C%20how%20to%20efficiently%0Aadapt%20the%20pre-trained%20model%20to%20the%20downstream%20tasks%20has%20been%20an%20intriguing%0Aissue.%20Parameter-Efficient%20Fine-Tuning%20%28PEFT%29%20methods%20have%20been%20proposed%20for%0Alow-cost%20adaptation.%20Although%20PEFT%20has%20demonstrated%20effectiveness%20and%20been%0Awidely%20applied%2C%20the%20underlying%20principles%20are%20still%20unclear.%20In%20this%20paper%2C%20we%0Aadopt%20the%20PAC-Bayesian%20generalization%20error%20bound%2C%20viewing%20pre-training%20as%20a%0Ashift%20of%20prior%20distribution%20which%20leads%20to%20a%20tighter%20bound%20for%20generalization%0Aerror.%20We%20validate%20this%20shift%20from%20the%20perspectives%20of%20oscillations%20in%20the%20loss%0Alandscape%20and%20the%20quasi-sparsity%20in%20gradient%20distribution.%20Based%20on%20this%2C%20we%0Apropose%20a%20gradient-based%20sparse%20fine-tuning%20algorithm%2C%20named%20Sparse%20Increment%0AFine-Tuning%20%28SIFT%29%2C%20and%20validate%20its%20effectiveness%20on%20a%20range%20of%20tasks%0Aincluding%20the%20GLUE%20Benchmark%20and%20Instruction-tuning.%20The%20code%20is%20accessible%20at%0Ahttps%3A//github.com/song-wx/SIFT/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11875v2&entry.124074799=Read"},
{"title": "A Survey on Transferability of Adversarial Examples across Deep Neural\n  Networks", "author": "Jindong Gu and Xiaojun Jia and Pau de Jorge and Wenqain Yu and Xinwei Liu and Avery Ma and Yuan Xun and Anjun Hu and Ashkan Khakzar and Zhijiang Li and Xiaochun Cao and Philip Torr", "abstract": "  The emergence of Deep Neural Networks (DNNs) has revolutionized various\ndomains by enabling the resolution of complex tasks spanning image recognition,\nnatural language processing, and scientific problem-solving. However, this\nprogress has also brought to light a concerning vulnerability: adversarial\nexamples. These crafted inputs, imperceptible to humans, can manipulate machine\nlearning models into making erroneous predictions, raising concerns for\nsafety-critical applications. An intriguing property of this phenomenon is the\ntransferability of adversarial examples, where perturbations crafted for one\nmodel can deceive another, often with a different architecture. This intriguing\nproperty enables black-box attacks which circumvents the need for detailed\nknowledge of the target model. This survey explores the landscape of the\nadversarial transferability of adversarial examples. We categorize existing\nmethodologies to enhance adversarial transferability and discuss the\nfundamental principles guiding each approach. While the predominant body of\nresearch primarily concentrates on image classification, we also extend our\ndiscussion to encompass other vision tasks and beyond. Challenges and\nopportunities are discussed, highlighting the importance of fortifying DNNs\nagainst adversarial vulnerabilities in an evolving landscape.\n", "link": "http://arxiv.org/abs/2310.17626v2", "date": "2024-05-02", "relevancy": 2.4906, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5136}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4947}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4861}], "mailto": "mailto:daeR=997470421.yrtne&2v62671.0132/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.epacsdnal02%gnivlove02%na02%ni02%seitilibarenluv02%lairasrevda02%tsniagaA0%sNND02%gniyfitrof02%fo02%ecnatropmi02%eht02%gnithgilhgih02%C2%dessucsid02%era02%seitinutroppoA0%dna02%segnellahC02%.dnoyeb02%dna02%sksat02%noisiv02%rehto02%ssapmocne02%ot02%noissucsidA0%ruo02%dnetxe02%osla02%ew02%C2%noitacifissalc02%egami02%no02%setartnecnoc02%yliramirp02%hcraeserA0%fo02%ydob02%tnanimoderp02%eht02%elihW02%.hcaorppa02%hcae02%gnidiug02%selpicnirp02%latnemadnufA0%eht02%ssucsid02%dna02%ytilibarefsnart02%lairasrevda02%ecnahne02%ot02%seigolodohtemA0%gnitsixe02%ezirogetac02%eW02%.selpmaxe02%lairasrevda02%fo02%ytilibarefsnart02%lairasrevdaA0%eht02%fo02%epacsdnal02%eht02%serolpxe02%yevrus02%sihT02%.ledom02%tegrat02%eht02%fo02%egdelwonkA0%deliated02%rof02%deen02%eht02%stnevmucric02%hcihw02%skcatta02%xob-kcalb02%selbane02%ytreporpA0%gniugirtni02%sihT02%.erutcetihcra02%tnereffid02%a02%htiw02%netfo02%C2%rehtona02%evieced02%nac02%ledomA0%eno02%rof02%detfarc02%snoitabrutrep02%erehw02%C2%selpmaxe02%lairasrevda02%fo02%ytilibarefsnartA0%eht02%si02%nonemonehp02%siht02%fo02%ytreporp02%gniugirtni02%nA02%.snoitacilppa02%lacitirc-ytefasA0%rof02%snrecnoc02%gnisiar02%C2%snoitciderp02%suoenorre02%gnikam02%otni02%sledom02%gninraelA0%enihcam02%etalupinam02%nac02%C2%snamuh02%ot02%elbitpecrepmi02%C2%stupni02%detfarc02%esehT02%.selpmaxeA0%lairasrevda02%A3%ytilibarenluv02%gninrecnoc02%a02%thgil02%ot02%thguorb02%osla02%sah02%ssergorpA0%siht02%C2%revewoH02%.gnivlos-melborp02%cifitneics02%dna02%C2%gnissecorp02%egaugnal02%larutanA0%C2%noitingocer02%egami02%gninnaps02%sksat02%xelpmoc02%fo02%noituloser02%eht02%gnilbane02%yb02%sniamodA0%suoirav02%dezinoitulover02%sah02%92%sNND82%02%skrowteN02%larueN02%peeD02%fo02%ecnegreme02%ehT02%02%=3328342921.yrtne&rroT02%pilihP02%dna02%oaC02%nuhcoaiX02%dna02%iL02%gnaijihZ02%dna02%razkahK02%nakhsA02%dna02%uH02%nujnA02%dna02%nuX02%nauY02%dna02%aM02%yrevA02%dna02%uiL02%iewniX02%dna02%uY02%niaqneW02%dna02%egroJ02%ed02%uaP02%dna02%aiJ02%nujoaiX02%dna02%uG02%gnodniJ=526535609.yrtne&skrowteN02%02%A0%larueN02%peeD02%ssorca02%selpmaxE02%lairasrevdA02%fo02%ytilibarefsnarT02%no02%yevruS02%A=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20on%20Transferability%20of%20Adversarial%20Examples%20across%20Deep%20Neural%0A%20%20Networks&body=Title%3A%20A%20Survey%20on%20Transferability%20of%20Adversarial%20Examples%20across%20Deep%20Neural%0A%20%20Networks%0AAuthor%3A%20Jindong%20Gu%20and%20Xiaojun%20Jia%20and%20Pau%20de%20Jorge%20and%20Wenqain%20Yu%20and%20Xinwei%20Liu%20and%20Avery%20Ma%20and%20Yuan%20Xun%20and%20Anjun%20Hu%20and%20Ashkan%20Khakzar%20and%20Zhijiang%20Li%20and%20Xiaochun%20Cao%20and%20Philip%20Torr%0AAbstract%3A%20%20%20The%20emergence%20of%20Deep%20Neural%20Networks%20%28DNNs%29%20has%20revolutionized%20various%0Adomains%20by%20enabling%20the%20resolution%20of%20complex%20tasks%20spanning%20image%20recognition%2C%0Anatural%20language%20processing%2C%20and%20scientific%20problem-solving.%20However%2C%20this%0Aprogress%20has%20also%20brought%20to%20light%20a%20concerning%20vulnerability%3A%20adversarial%0Aexamples.%20These%20crafted%20inputs%2C%20imperceptible%20to%20humans%2C%20can%20manipulate%20machine%0Alearning%20models%20into%20making%20erroneous%20predictions%2C%20raising%20concerns%20for%0Asafety-critical%20applications.%20An%20intriguing%20property%20of%20this%20phenomenon%20is%20the%0Atransferability%20of%20adversarial%20examples%2C%20where%20perturbations%20crafted%20for%20one%0Amodel%20can%20deceive%20another%2C%20often%20with%20a%20different%20architecture.%20This%20intriguing%0Aproperty%20enables%20black-box%20attacks%20which%20circumvents%20the%20need%20for%20detailed%0Aknowledge%20of%20the%20target%20model.%20This%20survey%20explores%20the%20landscape%20of%20the%0Aadversarial%20transferability%20of%20adversarial%20examples.%20We%20categorize%20existing%0Amethodologies%20to%20enhance%20adversarial%20transferability%20and%20discuss%20the%0Afundamental%20principles%20guiding%20each%20approach.%20While%20the%20predominant%20body%20of%0Aresearch%20primarily%20concentrates%20on%20image%20classification%2C%20we%20also%20extend%20our%0Adiscussion%20to%20encompass%20other%20vision%20tasks%20and%20beyond.%20Challenges%20and%0Aopportunities%20are%20discussed%2C%20highlighting%20the%20importance%20of%20fortifying%20DNNs%0Aagainst%20adversarial%20vulnerabilities%20in%20an%20evolving%20landscape.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.17626v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20on%20Transferability%20of%20Adversarial%20Examples%20across%20Deep%20Neural%0A%20%20Networks&entry.906535625=Jindong%20Gu%20and%20Xiaojun%20Jia%20and%20Pau%20de%20Jorge%20and%20Wenqain%20Yu%20and%20Xinwei%20Liu%20and%20Avery%20Ma%20and%20Yuan%20Xun%20and%20Anjun%20Hu%20and%20Ashkan%20Khakzar%20and%20Zhijiang%20Li%20and%20Xiaochun%20Cao%20and%20Philip%20Torr&entry.1292438233=%20%20The%20emergence%20of%20Deep%20Neural%20Networks%20%28DNNs%29%20has%20revolutionized%20various%0Adomains%20by%20enabling%20the%20resolution%20of%20complex%20tasks%20spanning%20image%20recognition%2C%0Anatural%20language%20processing%2C%20and%20scientific%20problem-solving.%20However%2C%20this%0Aprogress%20has%20also%20brought%20to%20light%20a%20concerning%20vulnerability%3A%20adversarial%0Aexamples.%20These%20crafted%20inputs%2C%20imperceptible%20to%20humans%2C%20can%20manipulate%20machine%0Alearning%20models%20into%20making%20erroneous%20predictions%2C%20raising%20concerns%20for%0Asafety-critical%20applications.%20An%20intriguing%20property%20of%20this%20phenomenon%20is%20the%0Atransferability%20of%20adversarial%20examples%2C%20where%20perturbations%20crafted%20for%20one%0Amodel%20can%20deceive%20another%2C%20often%20with%20a%20different%20architecture.%20This%20intriguing%0Aproperty%20enables%20black-box%20attacks%20which%20circumvents%20the%20need%20for%20detailed%0Aknowledge%20of%20the%20target%20model.%20This%20survey%20explores%20the%20landscape%20of%20the%0Aadversarial%20transferability%20of%20adversarial%20examples.%20We%20categorize%20existing%0Amethodologies%20to%20enhance%20adversarial%20transferability%20and%20discuss%20the%0Afundamental%20principles%20guiding%20each%20approach.%20While%20the%20predominant%20body%20of%0Aresearch%20primarily%20concentrates%20on%20image%20classification%2C%20we%20also%20extend%20our%0Adiscussion%20to%20encompass%20other%20vision%20tasks%20and%20beyond.%20Challenges%20and%0Aopportunities%20are%20discussed%2C%20highlighting%20the%20importance%20of%20fortifying%20DNNs%0Aagainst%20adversarial%20vulnerabilities%20in%20an%20evolving%20landscape.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.17626v2&entry.124074799=Read"},
{"title": "Evaluation of Video-Based rPPG in Challenging Environments: Artifact\n  Mitigation and Network Resilience", "author": "Nhi Nguyen and Le Nguyen and Honghan Li and Miguel Bordallo L\u00f3pez and Constantino \u00c1lvarez Casado", "abstract": "  Video-based remote photoplethysmography (rPPG) has emerged as a promising\ntechnology for non-contact vital sign monitoring, especially under controlled\nconditions. However, the accurate measurement of vital signs in real-world\nscenarios faces several challenges, including artifacts induced by videocodecs,\nlow-light noise, degradation, low dynamic range, occlusions, and hardware and\nnetwork constraints. In this article, we systematically investigate\ncomprehensive investigate these issues, measuring their detrimental effects on\nthe quality of rPPG measurements. Additionally, we propose practical strategies\nfor mitigating these challenges to improve the dependability and resilience of\nvideo-based rPPG systems. We detail methods for effective biosignal recovery in\nthe presence of network limitations and present denoising and inpainting\ntechniques aimed at preserving video frame integrity. Through extensive\nevaluations and direct comparisons, we demonstrate the effectiveness of the\napproaches in enhancing rPPG measurements under challenging environments,\ncontributing to the development of more reliable and effective remote vital\nsign monitoring technologies.\n", "link": "http://arxiv.org/abs/2405.01230v1", "date": "2024-05-02", "relevancy": 2.486, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5216}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.487}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4829}], "mailto": "mailto:daeR=997470421.yrtne&1v03210.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.seigolonhcet02%gnirotinom02%ngisA0%lativ02%etomer02%evitceffe02%dna02%elbailer02%erom02%fo02%tnempoleved02%eht02%ot02%gnitubirtnocA0%C2%stnemnorivne02%gnignellahc02%rednu02%stnemerusaem02%GPPr02%gnicnahne02%ni02%sehcaorppaA0%eht02%fo02%ssenevitceffe02%eht02%etartsnomed02%ew02%C2%snosirapmoc02%tcerid02%dna02%snoitaulaveA0%evisnetxe02%hguorhT02%.ytirgetni02%emarf02%oediv02%gnivreserp02%ta02%demia02%seuqinhcetA0%gnitniapni02%dna02%gnisioned02%tneserp02%dna02%snoitatimil02%krowten02%fo02%ecneserp02%ehtA0%ni02%yrevocer02%langisoib02%evitceffe02%rof02%sdohtem02%liated02%eW02%.smetsys02%GPPr02%desab-oedivA0%fo02%ecneiliser02%dna02%ytilibadneped02%eht02%evorpmi02%ot02%segnellahc02%eseht02%gnitagitim02%rofA0%seigetarts02%lacitcarp02%esoporp02%ew02%C2%yllanoitiddA02%.stnemerusaem02%GPPr02%fo02%ytilauq02%ehtA0%no02%stceffe02%latnemirted02%rieht02%gnirusaem02%C2%seussi02%eseht02%etagitsevni02%evisneherpmocA0%etagitsevni02%yllacitametsys02%ew02%C2%elcitra02%siht02%nI02%.stniartsnoc02%krowtenA0%dna02%erawdrah02%dna02%C2%snoisulcco02%C2%egnar02%cimanyd02%wol02%C2%noitadarged02%C2%esion02%thgil-wolA0%C2%scedocoediv02%yb02%decudni02%stcafitra02%gnidulcni02%C2%segnellahc02%lareves02%secaf02%soiranecsA0%dlrow-laer02%ni02%sngis02%lativ02%fo02%tnemerusaem02%etarucca02%eht02%C2%revewoH02%.snoitidnocA0%dellortnoc02%rednu02%yllaicepse02%C2%gnirotinom02%ngis02%lativ02%tcatnoc-non02%rof02%ygolonhcetA0%gnisimorp02%a02%sa02%degreme02%sah02%92%GPPr82%02%yhpargomsyhtelpotohp02%etomer02%desab-oediV02%02%=3328342921.yrtne&odasaC02%zeravl18%3C%02%onitnatsnoC02%dna02%zep3B%3C%L02%olladroB02%leugiM02%dna02%iL02%nahgnoH02%dna02%neyugN02%eL02%dna02%neyugN02%ihN=526535609.yrtne&ecneiliseR02%krowteN02%dna02%noitagitiM02%02%A0%tcafitrA02%A3%stnemnorivnE02%gnignellahC02%ni02%GPPr02%desaB-oediV02%fo02%noitaulavE=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20Video-Based%20rPPG%20in%20Challenging%20Environments%3A%20Artifact%0A%20%20Mitigation%20and%20Network%20Resilience&body=Title%3A%20Evaluation%20of%20Video-Based%20rPPG%20in%20Challenging%20Environments%3A%20Artifact%0A%20%20Mitigation%20and%20Network%20Resilience%0AAuthor%3A%20Nhi%20Nguyen%20and%20Le%20Nguyen%20and%20Honghan%20Li%20and%20Miguel%20Bordallo%20L%C3%B3pez%20and%20Constantino%20%C3%81lvarez%20Casado%0AAbstract%3A%20%20%20Video-based%20remote%20photoplethysmography%20%28rPPG%29%20has%20emerged%20as%20a%20promising%0Atechnology%20for%20non-contact%20vital%20sign%20monitoring%2C%20especially%20under%20controlled%0Aconditions.%20However%2C%20the%20accurate%20measurement%20of%20vital%20signs%20in%20real-world%0Ascenarios%20faces%20several%20challenges%2C%20including%20artifacts%20induced%20by%20videocodecs%2C%0Alow-light%20noise%2C%20degradation%2C%20low%20dynamic%20range%2C%20occlusions%2C%20and%20hardware%20and%0Anetwork%20constraints.%20In%20this%20article%2C%20we%20systematically%20investigate%0Acomprehensive%20investigate%20these%20issues%2C%20measuring%20their%20detrimental%20effects%20on%0Athe%20quality%20of%20rPPG%20measurements.%20Additionally%2C%20we%20propose%20practical%20strategies%0Afor%20mitigating%20these%20challenges%20to%20improve%20the%20dependability%20and%20resilience%20of%0Avideo-based%20rPPG%20systems.%20We%20detail%20methods%20for%20effective%20biosignal%20recovery%20in%0Athe%20presence%20of%20network%20limitations%20and%20present%20denoising%20and%20inpainting%0Atechniques%20aimed%20at%20preserving%20video%20frame%20integrity.%20Through%20extensive%0Aevaluations%20and%20direct%20comparisons%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%0Aapproaches%20in%20enhancing%20rPPG%20measurements%20under%20challenging%20environments%2C%0Acontributing%20to%20the%20development%20of%20more%20reliable%20and%20effective%20remote%20vital%0Asign%20monitoring%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01230v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20Video-Based%20rPPG%20in%20Challenging%20Environments%3A%20Artifact%0A%20%20Mitigation%20and%20Network%20Resilience&entry.906535625=Nhi%20Nguyen%20and%20Le%20Nguyen%20and%20Honghan%20Li%20and%20Miguel%20Bordallo%20L%C3%B3pez%20and%20Constantino%20%C3%81lvarez%20Casado&entry.1292438233=%20%20Video-based%20remote%20photoplethysmography%20%28rPPG%29%20has%20emerged%20as%20a%20promising%0Atechnology%20for%20non-contact%20vital%20sign%20monitoring%2C%20especially%20under%20controlled%0Aconditions.%20However%2C%20the%20accurate%20measurement%20of%20vital%20signs%20in%20real-world%0Ascenarios%20faces%20several%20challenges%2C%20including%20artifacts%20induced%20by%20videocodecs%2C%0Alow-light%20noise%2C%20degradation%2C%20low%20dynamic%20range%2C%20occlusions%2C%20and%20hardware%20and%0Anetwork%20constraints.%20In%20this%20article%2C%20we%20systematically%20investigate%0Acomprehensive%20investigate%20these%20issues%2C%20measuring%20their%20detrimental%20effects%20on%0Athe%20quality%20of%20rPPG%20measurements.%20Additionally%2C%20we%20propose%20practical%20strategies%0Afor%20mitigating%20these%20challenges%20to%20improve%20the%20dependability%20and%20resilience%20of%0Avideo-based%20rPPG%20systems.%20We%20detail%20methods%20for%20effective%20biosignal%20recovery%20in%0Athe%20presence%20of%20network%20limitations%20and%20present%20denoising%20and%20inpainting%0Atechniques%20aimed%20at%20preserving%20video%20frame%20integrity.%20Through%20extensive%0Aevaluations%20and%20direct%20comparisons%2C%20we%20demonstrate%20the%20effectiveness%20of%20the%0Aapproaches%20in%20enhancing%20rPPG%20measurements%20under%20challenging%20environments%2C%0Acontributing%20to%20the%20development%20of%20more%20reliable%20and%20effective%20remote%20vital%0Asign%20monitoring%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01230v1&entry.124074799=Read"},
{"title": "APLA: Additional Perturbation for Latent Noise with Adversarial Training\n  Enables Consistency", "author": "Yupu Yao and Shangqi Deng and Zihan Cao and Harry Zhang and Liang-Jian Deng", "abstract": "  Diffusion models have exhibited promising progress in video generation.\nHowever, they often struggle to retain consistent details within local regions\nacross frames. One underlying cause is that traditional diffusion models\napproximate Gaussian noise distribution by utilizing predictive noise, without\nfully accounting for the impact of inherent information within the input\nitself. Additionally, these models emphasize the distinction between\npredictions and references, neglecting information intrinsic to the videos. To\naddress this limitation, inspired by the self-attention mechanism, we propose a\nnovel text-to-video (T2V) generation network structure based on diffusion\nmodels, dubbed Additional Perturbation for Latent noise with Adversarial\ntraining (APLA). Our approach only necessitates a single video as input and\nbuilds upon pre-trained stable diffusion networks. Notably, we introduce an\nadditional compact network, known as the Video Generation Transformer (VGT).\nThis auxiliary component is designed to extract perturbations from the inherent\ninformation contained within the input, thereby refining inconsistent pixels\nduring temporal predictions. We leverage a hybrid architecture of transformers\nand convolutions to compensate for temporal intricacies, enhancing consistency\nbetween different frames within the video. Experiments demonstrate a noticeable\nimprovement in the consistency of the generated videos both qualitatively and\nquantitatively.\n", "link": "http://arxiv.org/abs/2308.12605v2", "date": "2024-05-02", "relevancy": 2.471, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6588}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6174}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6017}], "mailto": "mailto:daeR=997470421.yrtne&2v50621.8032/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.ylevitatitnauqA0%dna02%ylevitatilauq02%htob02%soediv02%detareneg02%eht02%fo02%ycnetsisnoc02%eht02%ni02%tnemevorpmiA0%elbaeciton02%a02%etartsnomed02%stnemirepxE02%.oediv02%eht02%nihtiw02%semarf02%tnereffid02%neewtebA0%ycnetsisnoc02%gnicnahne02%C2%seicacirtni02%laropmet02%rof02%etasnepmoc02%ot02%snoitulovnoc02%dnaA0%sremrofsnart02%fo02%erutcetihcra02%dirbyh02%a02%egarevel02%eW02%.snoitciderp02%laropmet02%gnirudA0%slexip02%tnetsisnocni02%gninifer02%ybereht02%C2%tupni02%eht02%nihtiw02%deniatnoc02%noitamrofniA0%tnerehni02%eht02%morf02%snoitabrutrep02%tcartxe02%ot02%dengised02%si02%tnenopmoc02%yrailixua02%sihTA0%.92%TGV82%02%remrofsnarT02%noitareneG02%oediV02%eht02%sa02%nwonk02%C2%krowten02%tcapmoc02%lanoitiddaA0%na02%ecudortni02%ew02%C2%ylbatoN02%.skrowten02%noisuffid02%elbats02%deniart-erp02%nopu02%sdliubA0%dna02%tupni02%sa02%oediv02%elgnis02%a02%setatissecen02%ylno02%hcaorppa02%ruO02%.92%ALPA82%02%gniniartA0%lairasrevdA02%htiw02%esion02%tnetaL02%rof02%noitabrutreP02%lanoitiddA02%debbud02%C2%sledomA0%noisuffid02%no02%desab02%erutcurts02%krowten02%noitareneg02%92%V2T82%02%oediv-ot-txet02%levonA0%a02%esoporp02%ew02%C2%msinahcem02%noitnetta-fles02%eht02%yb02%deripsni02%C2%noitatimil02%siht02%sserddaA0%oT02%.soediv02%eht02%ot02%cisnirtni02%noitamrofni02%gnitcelgen02%C2%secnerefer02%dna02%snoitciderpA0%neewteb02%noitcnitsid02%eht02%ezisahpme02%sledom02%eseht02%C2%yllanoitiddA02%.flestiA0%tupni02%eht02%nihtiw02%noitamrofni02%tnerehni02%fo02%tcapmi02%eht02%rof02%gnitnuocca02%yllufA0%tuohtiw02%C2%esion02%evitciderp02%gnizilitu02%yb02%noitubirtsid02%esion02%naissuaG02%etamixorppaA0%sledom02%noisuffid02%lanoitidart02%taht02%si02%esuac02%gniylrednu02%enO02%.semarf02%ssorcaA0%snoiger02%lacol02%nihtiw02%sliated02%tnetsisnoc02%niater02%ot02%elggurts02%netfo02%yeht02%C2%revewoHA0%.noitareneg02%oediv02%ni02%ssergorp02%gnisimorp02%detibihxe02%evah02%sledom02%noisuffiD02%02%=3328342921.yrtne&gneD02%naiJ-gnaiL02%dna02%gnahZ02%yrraH02%dna02%oaC02%nahiZ02%dna02%gneD02%iqgnahS02%dna02%oaY02%upuY=526535609.yrtne&ycnetsisnoC02%selbanE02%02%A0%gniniarT02%lairasrevdA02%htiw02%esioN02%tnetaL02%rof02%noitabrutreP02%lanoitiddA02%A3%ALPA=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20APLA%3A%20Additional%20Perturbation%20for%20Latent%20Noise%20with%20Adversarial%20Training%0A%20%20Enables%20Consistency&body=Title%3A%20APLA%3A%20Additional%20Perturbation%20for%20Latent%20Noise%20with%20Adversarial%20Training%0A%20%20Enables%20Consistency%0AAuthor%3A%20Yupu%20Yao%20and%20Shangqi%20Deng%20and%20Zihan%20Cao%20and%20Harry%20Zhang%20and%20Liang-Jian%20Deng%0AAbstract%3A%20%20%20Diffusion%20models%20have%20exhibited%20promising%20progress%20in%20video%20generation.%0AHowever%2C%20they%20often%20struggle%20to%20retain%20consistent%20details%20within%20local%20regions%0Aacross%20frames.%20One%20underlying%20cause%20is%20that%20traditional%20diffusion%20models%0Aapproximate%20Gaussian%20noise%20distribution%20by%20utilizing%20predictive%20noise%2C%20without%0Afully%20accounting%20for%20the%20impact%20of%20inherent%20information%20within%20the%20input%0Aitself.%20Additionally%2C%20these%20models%20emphasize%20the%20distinction%20between%0Apredictions%20and%20references%2C%20neglecting%20information%20intrinsic%20to%20the%20videos.%20To%0Aaddress%20this%20limitation%2C%20inspired%20by%20the%20self-attention%20mechanism%2C%20we%20propose%20a%0Anovel%20text-to-video%20%28T2V%29%20generation%20network%20structure%20based%20on%20diffusion%0Amodels%2C%20dubbed%20Additional%20Perturbation%20for%20Latent%20noise%20with%20Adversarial%0Atraining%20%28APLA%29.%20Our%20approach%20only%20necessitates%20a%20single%20video%20as%20input%20and%0Abuilds%20upon%20pre-trained%20stable%20diffusion%20networks.%20Notably%2C%20we%20introduce%20an%0Aadditional%20compact%20network%2C%20known%20as%20the%20Video%20Generation%20Transformer%20%28VGT%29.%0AThis%20auxiliary%20component%20is%20designed%20to%20extract%20perturbations%20from%20the%20inherent%0Ainformation%20contained%20within%20the%20input%2C%20thereby%20refining%20inconsistent%20pixels%0Aduring%20temporal%20predictions.%20We%20leverage%20a%20hybrid%20architecture%20of%20transformers%0Aand%20convolutions%20to%20compensate%20for%20temporal%20intricacies%2C%20enhancing%20consistency%0Abetween%20different%20frames%20within%20the%20video.%20Experiments%20demonstrate%20a%20noticeable%0Aimprovement%20in%20the%20consistency%20of%20the%20generated%20videos%20both%20qualitatively%20and%0Aquantitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.12605v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=APLA%3A%20Additional%20Perturbation%20for%20Latent%20Noise%20with%20Adversarial%20Training%0A%20%20Enables%20Consistency&entry.906535625=Yupu%20Yao%20and%20Shangqi%20Deng%20and%20Zihan%20Cao%20and%20Harry%20Zhang%20and%20Liang-Jian%20Deng&entry.1292438233=%20%20Diffusion%20models%20have%20exhibited%20promising%20progress%20in%20video%20generation.%0AHowever%2C%20they%20often%20struggle%20to%20retain%20consistent%20details%20within%20local%20regions%0Aacross%20frames.%20One%20underlying%20cause%20is%20that%20traditional%20diffusion%20models%0Aapproximate%20Gaussian%20noise%20distribution%20by%20utilizing%20predictive%20noise%2C%20without%0Afully%20accounting%20for%20the%20impact%20of%20inherent%20information%20within%20the%20input%0Aitself.%20Additionally%2C%20these%20models%20emphasize%20the%20distinction%20between%0Apredictions%20and%20references%2C%20neglecting%20information%20intrinsic%20to%20the%20videos.%20To%0Aaddress%20this%20limitation%2C%20inspired%20by%20the%20self-attention%20mechanism%2C%20we%20propose%20a%0Anovel%20text-to-video%20%28T2V%29%20generation%20network%20structure%20based%20on%20diffusion%0Amodels%2C%20dubbed%20Additional%20Perturbation%20for%20Latent%20noise%20with%20Adversarial%0Atraining%20%28APLA%29.%20Our%20approach%20only%20necessitates%20a%20single%20video%20as%20input%20and%0Abuilds%20upon%20pre-trained%20stable%20diffusion%20networks.%20Notably%2C%20we%20introduce%20an%0Aadditional%20compact%20network%2C%20known%20as%20the%20Video%20Generation%20Transformer%20%28VGT%29.%0AThis%20auxiliary%20component%20is%20designed%20to%20extract%20perturbations%20from%20the%20inherent%0Ainformation%20contained%20within%20the%20input%2C%20thereby%20refining%20inconsistent%20pixels%0Aduring%20temporal%20predictions.%20We%20leverage%20a%20hybrid%20architecture%20of%20transformers%0Aand%20convolutions%20to%20compensate%20for%20temporal%20intricacies%2C%20enhancing%20consistency%0Abetween%20different%20frames%20within%20the%20video.%20Experiments%20demonstrate%20a%20noticeable%0Aimprovement%20in%20the%20consistency%20of%20the%20generated%20videos%20both%20qualitatively%20and%0Aquantitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.12605v2&entry.124074799=Read"},
{"title": "IntraMix: Intra-Class Mixup Generation for Accurate Labels and Neighbors", "author": "Shenghe Zheng and Hongzhi Wang and Xianglong Liu", "abstract": "  Graph Neural Networks (GNNs) demonstrate excellent performance on graphs,\nwith their core idea about aggregating neighborhood information and learning\nfrom labels. However, the prevailing challenges in most graph datasets are\ntwofold of Insufficient High-Quality Labels and Lack of Neighborhoods,\nresulting in weak GNNs. Existing data augmentation methods designed to address\nthese two issues often tackle only one. They may either require extensive\ntraining of generators, rely on overly simplistic strategies, or demand\nsubstantial prior knowledge, leading to suboptimal generalization abilities. To\nsimultaneously address both of these two challenges, we propose an elegant\nmethod called IntraMix. IntraMix innovatively employs Mixup among low-quality\nlabeled data of the same class, generating high-quality labeled data at minimal\ncost. Additionally, it establishes neighborhoods for the generated data by\nconnecting them with data from the same class with high confidence, thereby\nenriching the neighborhoods of graphs. IntraMix efficiently tackles both\nchallenges faced by graphs and challenges the prior notion of the limited\neffectiveness of Mixup in node classification. IntraMix serves as a universal\nframework that can be readily applied to all GNNs. Extensive experiments\ndemonstrate the effectiveness of IntraMix across various GNNs and datasets.\n", "link": "http://arxiv.org/abs/2405.00957v1", "date": "2024-05-02", "relevancy": 2.468, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5314}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4774}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.472}], "mailto": "mailto:daeR=997470421.yrtne&1v75900.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.stesatad02%dna02%sNNG02%suoirav02%ssorca02%xiMartnI02%fo02%ssenevitceffe02%eht02%etartsnomedA0%stnemirepxe02%evisnetxE02%.sNNG02%lla02%ot02%deilppa02%ylidaer02%eb02%nac02%taht02%krowemarfA0%lasrevinu02%a02%sa02%sevres02%xiMartnI02%.noitacifissalc02%edon02%ni02%puxiM02%fo02%ssenevitceffeA0%detimil02%eht02%fo02%noiton02%roirp02%eht02%segnellahc02%dna02%shparg02%yb02%decaf02%segnellahcA0%htob02%selkcat02%yltneiciffe02%xiMartnI02%.shparg02%fo02%sdoohrobhgien02%eht02%gnihcirneA0%ybereht02%C2%ecnedifnoc02%hgih02%htiw02%ssalc02%emas02%eht02%morf02%atad02%htiw02%meht02%gnitcennocA0%yb02%atad02%detareneg02%eht02%rof02%sdoohrobhgien02%sehsilbatse02%ti02%C2%yllanoitiddA02%.tsocA0%laminim02%ta02%atad02%delebal02%ytilauq-hgih02%gnitareneg02%C2%ssalc02%emas02%eht02%fo02%atad02%delebalA0%ytilauq-wol02%gnoma02%puxiM02%syolpme02%ylevitavonni02%xiMartnI02%.xiMartnI02%dellac02%dohtemA0%tnagele02%na02%esoporp02%ew02%C2%segnellahc02%owt02%eseht02%fo02%htob02%sserdda02%ylsuoenatlumisA0%oT02%.seitiliba02%noitazilareneg02%lamitpobus02%ot02%gnidael02%C2%egdelwonk02%roirp02%laitnatsbusA0%dnamed02%ro02%C2%seigetarts02%citsilpmis02%ylrevo02%no02%yler02%C2%srotareneg02%fo02%gniniartA0%evisnetxe02%eriuqer02%rehtie02%yam02%yehT02%.eno02%ylno02%elkcat02%netfo02%seussi02%owt02%esehtA0%sserdda02%ot02%dengised02%sdohtem02%noitatnemgua02%atad02%gnitsixE02%.sNNG02%kaew02%ni02%gnitluserA0%C2%sdoohrobhgieN02%fo02%kcaL02%dna02%slebaL02%ytilauQ-hgiH02%tneiciffusnI02%fo02%dlofowtA0%era02%stesatad02%hparg02%tsom02%ni02%segnellahc02%gniliaverp02%eht02%C2%revewoH02%.slebal02%morfA0%gninrael02%dna02%noitamrofni02%doohrobhgien02%gnitagergga02%tuoba02%aedi02%eroc02%rieht02%htiwA0%C2%shparg02%no02%ecnamrofrep02%tnellecxe02%etartsnomed02%92%sNNG82%02%skrowteN02%larueN02%hparG02%02%=3328342921.yrtne&uiL02%gnolgnaiX02%dna02%gnaW02%ihzgnoH02%dna02%gnehZ02%ehgnehS=526535609.yrtne&srobhgieN02%dna02%slebaL02%etaruccA02%rof02%noitareneG02%puxiM02%ssalC-artnI02%A3%xiMartnI=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20IntraMix%3A%20Intra-Class%20Mixup%20Generation%20for%20Accurate%20Labels%20and%20Neighbors&body=Title%3A%20IntraMix%3A%20Intra-Class%20Mixup%20Generation%20for%20Accurate%20Labels%20and%20Neighbors%0AAuthor%3A%20Shenghe%20Zheng%20and%20Hongzhi%20Wang%20and%20Xianglong%20Liu%0AAbstract%3A%20%20%20Graph%20Neural%20Networks%20%28GNNs%29%20demonstrate%20excellent%20performance%20on%20graphs%2C%0Awith%20their%20core%20idea%20about%20aggregating%20neighborhood%20information%20and%20learning%0Afrom%20labels.%20However%2C%20the%20prevailing%20challenges%20in%20most%20graph%20datasets%20are%0Atwofold%20of%20Insufficient%20High-Quality%20Labels%20and%20Lack%20of%20Neighborhoods%2C%0Aresulting%20in%20weak%20GNNs.%20Existing%20data%20augmentation%20methods%20designed%20to%20address%0Athese%20two%20issues%20often%20tackle%20only%20one.%20They%20may%20either%20require%20extensive%0Atraining%20of%20generators%2C%20rely%20on%20overly%20simplistic%20strategies%2C%20or%20demand%0Asubstantial%20prior%20knowledge%2C%20leading%20to%20suboptimal%20generalization%20abilities.%20To%0Asimultaneously%20address%20both%20of%20these%20two%20challenges%2C%20we%20propose%20an%20elegant%0Amethod%20called%20IntraMix.%20IntraMix%20innovatively%20employs%20Mixup%20among%20low-quality%0Alabeled%20data%20of%20the%20same%20class%2C%20generating%20high-quality%20labeled%20data%20at%20minimal%0Acost.%20Additionally%2C%20it%20establishes%20neighborhoods%20for%20the%20generated%20data%20by%0Aconnecting%20them%20with%20data%20from%20the%20same%20class%20with%20high%20confidence%2C%20thereby%0Aenriching%20the%20neighborhoods%20of%20graphs.%20IntraMix%20efficiently%20tackles%20both%0Achallenges%20faced%20by%20graphs%20and%20challenges%20the%20prior%20notion%20of%20the%20limited%0Aeffectiveness%20of%20Mixup%20in%20node%20classification.%20IntraMix%20serves%20as%20a%20universal%0Aframework%20that%20can%20be%20readily%20applied%20to%20all%20GNNs.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20IntraMix%20across%20various%20GNNs%20and%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00957v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IntraMix%3A%20Intra-Class%20Mixup%20Generation%20for%20Accurate%20Labels%20and%20Neighbors&entry.906535625=Shenghe%20Zheng%20and%20Hongzhi%20Wang%20and%20Xianglong%20Liu&entry.1292438233=%20%20Graph%20Neural%20Networks%20%28GNNs%29%20demonstrate%20excellent%20performance%20on%20graphs%2C%0Awith%20their%20core%20idea%20about%20aggregating%20neighborhood%20information%20and%20learning%0Afrom%20labels.%20However%2C%20the%20prevailing%20challenges%20in%20most%20graph%20datasets%20are%0Atwofold%20of%20Insufficient%20High-Quality%20Labels%20and%20Lack%20of%20Neighborhoods%2C%0Aresulting%20in%20weak%20GNNs.%20Existing%20data%20augmentation%20methods%20designed%20to%20address%0Athese%20two%20issues%20often%20tackle%20only%20one.%20They%20may%20either%20require%20extensive%0Atraining%20of%20generators%2C%20rely%20on%20overly%20simplistic%20strategies%2C%20or%20demand%0Asubstantial%20prior%20knowledge%2C%20leading%20to%20suboptimal%20generalization%20abilities.%20To%0Asimultaneously%20address%20both%20of%20these%20two%20challenges%2C%20we%20propose%20an%20elegant%0Amethod%20called%20IntraMix.%20IntraMix%20innovatively%20employs%20Mixup%20among%20low-quality%0Alabeled%20data%20of%20the%20same%20class%2C%20generating%20high-quality%20labeled%20data%20at%20minimal%0Acost.%20Additionally%2C%20it%20establishes%20neighborhoods%20for%20the%20generated%20data%20by%0Aconnecting%20them%20with%20data%20from%20the%20same%20class%20with%20high%20confidence%2C%20thereby%0Aenriching%20the%20neighborhoods%20of%20graphs.%20IntraMix%20efficiently%20tackles%20both%0Achallenges%20faced%20by%20graphs%20and%20challenges%20the%20prior%20notion%20of%20the%20limited%0Aeffectiveness%20of%20Mixup%20in%20node%20classification.%20IntraMix%20serves%20as%20a%20universal%0Aframework%20that%20can%20be%20readily%20applied%20to%20all%20GNNs.%20Extensive%20experiments%0Ademonstrate%20the%20effectiveness%20of%20IntraMix%20across%20various%20GNNs%20and%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00957v1&entry.124074799=Read"},
{"title": "NeuroLGP-SM: Scalable Surrogate-Assisted Neuroevolution for Deep Neural\n  Networks", "author": "Fergal Stapleton and Edgar Galv\u00e1n", "abstract": "  Evolutionary Algorithms (EAs) play a crucial role in the architectural\nconfiguration and training of Artificial Deep Neural Networks (DNNs), a process\nknown as neuroevolution. However, neuroevolution is hindered by its inherent\ncomputational expense, requiring multiple generations, a large population, and\nnumerous epochs. The most computationally intensive aspect lies in evaluating\nthe fitness function of a single candidate solution. To address this challenge,\nwe employ Surrogate-assisted EAs (SAEAs). While a few SAEAs approaches have\nbeen proposed in neuroevolution, none have been applied to truly large DNNs due\nto issues like intractable information usage. In this work, drawing inspiration\nfrom Genetic Programming semantics, we use phenotypic distance vectors,\noutputted from DNNs, alongside Kriging Partial Least Squares (KPLS), an\napproach that is effective in handling these large vectors, making them\nsuitable for search. Our proposed approach, named Neuro-Linear Genetic\nProgramming surrogate model (NeuroLGP-SM), efficiently and accurately estimates\nDNN fitness without the need for complete evaluations. NeuroLGP-SM demonstrates\ncompetitive or superior results compared to 12 other methods, including\nNeuroLGP without SM, convolutional neural networks, support vector machines,\nand autoencoders. Additionally, it is worth noting that NeuroLGP-SM is 25% more\nenergy-efficient than its NeuroLGP counterpart. This efficiency advantage adds\nto the overall appeal of our proposed NeuroLGP-SM in optimising the\nconfiguration of large DNNs.\n", "link": "http://arxiv.org/abs/2404.08786v3", "date": "2024-05-02", "relevancy": 2.4478, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4993}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4853}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4841}], "mailto": "mailto:daeR=997470421.yrtne&3v68780.4042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.sNND02%egral02%fo02%noitarugifnocA0%eht02%gnisimitpo02%ni02%MS-PGLorueN02%desoporp02%ruo02%fo02%laeppa02%llarevo02%eht02%otA0%sdda02%egatnavda02%ycneiciffe02%sihT02%.trapretnuoc02%PGLorueN02%sti02%naht02%tneiciffe-ygreneA0%erom02%52%5202%si02%MS-PGLorueN02%taht02%gniton02%htrow02%si02%ti02%C2%yllanoitiddA02%.sredocneotua02%dnaA0%C2%senihcam02%rotcev02%troppus02%C2%skrowten02%laruen02%lanoitulovnoc02%C2%MS02%tuohtiw02%PGLorueNA0%gnidulcni02%C2%sdohtem02%rehto02%2102%ot02%derapmoc02%stluser02%roirepus02%ro02%evititepmocA0%setartsnomed02%MS-PGLorueN02%.snoitaulave02%etelpmoc02%rof02%deen02%eht02%tuohtiw02%ssentif02%NNDA0%setamitse02%yletarucca02%dna02%yltneiciffe02%C2%92%MS-PGLorueN82%02%ledom02%etagorrus02%gnimmargorPA0%citeneG02%raeniL-orueN02%deman02%C2%hcaorppa02%desoporp02%ruO02%.hcraes02%rof02%elbatiusA0%meht02%gnikam02%C2%srotcev02%egral02%eseht02%gnildnah02%ni02%evitceffe02%si02%taht02%hcaorppaA0%na02%C2%92%SLPK82%02%serauqS02%tsaeL02%laitraP02%gnigirK02%edisgnola02%C2%sNND02%morf02%dettuptuoA0%C2%srotcev02%ecnatsid02%cipytonehp02%esu02%ew02%C2%scitnames02%gnimmargorP02%citeneG02%morfA0%noitaripsni02%gniward02%C2%krow02%siht02%nI02%.egasu02%noitamrofni02%elbatcartni02%ekil02%seussi02%otA0%eud02%sNND02%egral02%ylurt02%ot02%deilppa02%neeb02%evah02%enon02%C2%noituloveoruen02%ni02%desoporp02%neebA0%evah02%sehcaorppa02%sAEAS02%wef02%a02%elihW02%.92%sAEAS82%02%sAE02%detsissa-etagorruS02%yolpme02%ewA0%C2%egnellahc02%siht02%sserdda02%oT02%.noitulos02%etadidnac02%elgnis02%a02%fo02%noitcnuf02%ssentif02%ehtA0%gnitaulave02%ni02%seil02%tcepsa02%evisnetni02%yllanoitatupmoc02%tsom02%ehT02%.shcope02%suoremunA0%dna02%C2%noitalupop02%egral02%a02%C2%snoitareneg02%elpitlum02%gniriuqer02%C2%esnepxe02%lanoitatupmocA0%tnerehni02%sti02%yb02%derednih02%si02%noituloveoruen02%C2%revewoH02%.noituloveoruen02%sa02%nwonkA0%ssecorp02%a02%C2%92%sNND82%02%skrowteN02%larueN02%peeD02%laicifitrA02%fo02%gniniart02%dna02%noitarugifnocA0%larutcetihcra02%eht02%ni02%elor02%laicurc02%a02%yalp02%92%sAE82%02%smhtiroglA02%yranoitulovE02%02%=3328342921.yrtne&n1A%3C%vlaG02%ragdE02%dna02%notelpatS02%lagreF=526535609.yrtne&skrowteN02%02%A0%larueN02%peeD02%rof02%noituloveorueN02%detsissA-etagorruS02%elbalacS02%A3%MS-PGLorueN=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20NeuroLGP-SM%3A%20Scalable%20Surrogate-Assisted%20Neuroevolution%20for%20Deep%20Neural%0A%20%20Networks&body=Title%3A%20NeuroLGP-SM%3A%20Scalable%20Surrogate-Assisted%20Neuroevolution%20for%20Deep%20Neural%0A%20%20Networks%0AAuthor%3A%20Fergal%20Stapleton%20and%20Edgar%20Galv%C3%A1n%0AAbstract%3A%20%20%20Evolutionary%20Algorithms%20%28EAs%29%20play%20a%20crucial%20role%20in%20the%20architectural%0Aconfiguration%20and%20training%20of%20Artificial%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20a%20process%0Aknown%20as%20neuroevolution.%20However%2C%20neuroevolution%20is%20hindered%20by%20its%20inherent%0Acomputational%20expense%2C%20requiring%20multiple%20generations%2C%20a%20large%20population%2C%20and%0Anumerous%20epochs.%20The%20most%20computationally%20intensive%20aspect%20lies%20in%20evaluating%0Athe%20fitness%20function%20of%20a%20single%20candidate%20solution.%20To%20address%20this%20challenge%2C%0Awe%20employ%20Surrogate-assisted%20EAs%20%28SAEAs%29.%20While%20a%20few%20SAEAs%20approaches%20have%0Abeen%20proposed%20in%20neuroevolution%2C%20none%20have%20been%20applied%20to%20truly%20large%20DNNs%20due%0Ato%20issues%20like%20intractable%20information%20usage.%20In%20this%20work%2C%20drawing%20inspiration%0Afrom%20Genetic%20Programming%20semantics%2C%20we%20use%20phenotypic%20distance%20vectors%2C%0Aoutputted%20from%20DNNs%2C%20alongside%20Kriging%20Partial%20Least%20Squares%20%28KPLS%29%2C%20an%0Aapproach%20that%20is%20effective%20in%20handling%20these%20large%20vectors%2C%20making%20them%0Asuitable%20for%20search.%20Our%20proposed%20approach%2C%20named%20Neuro-Linear%20Genetic%0AProgramming%20surrogate%20model%20%28NeuroLGP-SM%29%2C%20efficiently%20and%20accurately%20estimates%0ADNN%20fitness%20without%20the%20need%20for%20complete%20evaluations.%20NeuroLGP-SM%20demonstrates%0Acompetitive%20or%20superior%20results%20compared%20to%2012%20other%20methods%2C%20including%0ANeuroLGP%20without%20SM%2C%20convolutional%20neural%20networks%2C%20support%20vector%20machines%2C%0Aand%20autoencoders.%20Additionally%2C%20it%20is%20worth%20noting%20that%20NeuroLGP-SM%20is%2025%25%20more%0Aenergy-efficient%20than%20its%20NeuroLGP%20counterpart.%20This%20efficiency%20advantage%20adds%0Ato%20the%20overall%20appeal%20of%20our%20proposed%20NeuroLGP-SM%20in%20optimising%20the%0Aconfiguration%20of%20large%20DNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08786v3%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuroLGP-SM%3A%20Scalable%20Surrogate-Assisted%20Neuroevolution%20for%20Deep%20Neural%0A%20%20Networks&entry.906535625=Fergal%20Stapleton%20and%20Edgar%20Galv%C3%A1n&entry.1292438233=%20%20Evolutionary%20Algorithms%20%28EAs%29%20play%20a%20crucial%20role%20in%20the%20architectural%0Aconfiguration%20and%20training%20of%20Artificial%20Deep%20Neural%20Networks%20%28DNNs%29%2C%20a%20process%0Aknown%20as%20neuroevolution.%20However%2C%20neuroevolution%20is%20hindered%20by%20its%20inherent%0Acomputational%20expense%2C%20requiring%20multiple%20generations%2C%20a%20large%20population%2C%20and%0Anumerous%20epochs.%20The%20most%20computationally%20intensive%20aspect%20lies%20in%20evaluating%0Athe%20fitness%20function%20of%20a%20single%20candidate%20solution.%20To%20address%20this%20challenge%2C%0Awe%20employ%20Surrogate-assisted%20EAs%20%28SAEAs%29.%20While%20a%20few%20SAEAs%20approaches%20have%0Abeen%20proposed%20in%20neuroevolution%2C%20none%20have%20been%20applied%20to%20truly%20large%20DNNs%20due%0Ato%20issues%20like%20intractable%20information%20usage.%20In%20this%20work%2C%20drawing%20inspiration%0Afrom%20Genetic%20Programming%20semantics%2C%20we%20use%20phenotypic%20distance%20vectors%2C%0Aoutputted%20from%20DNNs%2C%20alongside%20Kriging%20Partial%20Least%20Squares%20%28KPLS%29%2C%20an%0Aapproach%20that%20is%20effective%20in%20handling%20these%20large%20vectors%2C%20making%20them%0Asuitable%20for%20search.%20Our%20proposed%20approach%2C%20named%20Neuro-Linear%20Genetic%0AProgramming%20surrogate%20model%20%28NeuroLGP-SM%29%2C%20efficiently%20and%20accurately%20estimates%0ADNN%20fitness%20without%20the%20need%20for%20complete%20evaluations.%20NeuroLGP-SM%20demonstrates%0Acompetitive%20or%20superior%20results%20compared%20to%2012%20other%20methods%2C%20including%0ANeuroLGP%20without%20SM%2C%20convolutional%20neural%20networks%2C%20support%20vector%20machines%2C%0Aand%20autoencoders.%20Additionally%2C%20it%20is%20worth%20noting%20that%20NeuroLGP-SM%20is%2025%25%20more%0Aenergy-efficient%20than%20its%20NeuroLGP%20counterpart.%20This%20efficiency%20advantage%20adds%0Ato%20the%20overall%20appeal%20of%20our%20proposed%20NeuroLGP-SM%20in%20optimising%20the%0Aconfiguration%20of%20large%20DNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08786v3&entry.124074799=Read"},
{"title": "Evaluation and Optimization of Adaptive Cruise Control in Autonomous\n  Vehicles using the CARLA Simulator: A Study on Performance under Wet and Dry\n  Weather Conditions", "author": "Roza Al-Hindaw and Taqwa I. Alhadidi and Mohammad Adas", "abstract": "  Adaptive Cruise Control ACC can change the speed of the ego vehicle to\nmaintain a safe distance from the following vehicle automatically. The primary\npurpose of this research is to use cutting-edge computing approaches to locate\nand track vehicles in real time under various conditions to achieve a safe ACC.\nThe paper examines the extension of ACC employing depth cameras and radar\nsensors within Autonomous Vehicles AVs to respond in real time by changing\nweather conditions using the Car Learning to Act CARLA simulation platform at\nnoon. The ego vehicle controller's decision to accelerate or decelerate depends\non the speed of the leading ahead vehicle and the safe distance from that\nvehicle. Simulation results show that a Proportional Integral Derivative PID\ncontrol of autonomous vehicles using a depth camera and radar sensors reduces\nthe speed of the leading vehicle and the ego vehicle when it rains. In\naddition, longer travel time was observed for both vehicles in rainy conditions\nthan in dry conditions. Also, PID control prevents the leading vehicle from\nrear collisions\n", "link": "http://arxiv.org/abs/2405.01504v1", "date": "2024-05-02", "relevancy": 2.4474, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4976}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4709}], "mailto": "mailto:daeR=997470421.yrtne&1v40510.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%snoisilloc02%raerA0%morf02%elcihev02%gnidael02%eht02%stneverp02%lortnoc02%DIP02%C2%oslA02%.snoitidnoc02%yrd02%ni02%nahtA0%snoitidnoc02%yniar02%ni02%selcihev02%htob02%rof02%devresbo02%saw02%emit02%levart02%regnol02%C2%noitiddaA0%nI02%.sniar02%ti02%nehw02%elcihev02%oge02%eht02%dna02%elcihev02%gnidael02%eht02%fo02%deeps02%ehtA0%secuder02%srosnes02%radar02%dna02%aremac02%htped02%a02%gnisu02%selcihev02%suomonotua02%fo02%lortnocA0%DIP02%evitavireD02%largetnI02%lanoitroporP02%a02%taht02%wohs02%stluser02%noitalumiS02%.elcihevA0%taht02%morf02%ecnatsid02%efas02%eht02%dna02%elcihev02%daeha02%gnidael02%eht02%fo02%deeps02%eht02%noA0%sdneped02%etareleced02%ro02%etarelecca02%ot02%noisiced02%s72%rellortnoc02%elcihev02%oge02%ehT02%.noonA0%ta02%mroftalp02%noitalumis02%ALRAC02%tcA02%ot02%gninraeL02%raC02%eht02%gnisu02%snoitidnoc02%rehtaewA0%gnignahc02%yb02%emit02%laer02%ni02%dnopser02%ot02%sVA02%selciheV02%suomonotuA02%nihtiw02%srosnesA0%radar02%dna02%saremac02%htped02%gniyolpme02%CCA02%fo02%noisnetxe02%eht02%senimaxe02%repap02%ehTA0%.CCA02%efas02%a02%eveihca02%ot02%snoitidnoc02%suoirav02%rednu02%emit02%laer02%ni02%selcihev02%kcart02%dnaA0%etacol02%ot02%sehcaorppa02%gnitupmoc02%egde-gnittuc02%esu02%ot02%si02%hcraeser02%siht02%fo02%esoprupA0%yramirp02%ehT02%.yllacitamotua02%elcihev02%gniwollof02%eht02%morf02%ecnatsid02%efas02%a02%niatniamA0%ot02%elcihev02%oge02%eht02%fo02%deeps02%eht02%egnahc02%nac02%CCA02%lortnoC02%esiurC02%evitpadA02%02%=3328342921.yrtne&sadA02%dammahoM02%dna02%ididahlA02%.I02%awqaT02%dna02%wadniH-lA02%azoR=526535609.yrtne&snoitidnoC02%rehtaeW02%02%A0%yrD02%dna02%teW02%rednu02%ecnamrofreP02%no02%ydutS02%A02%A3%rotalumiS02%ALRAC02%eht02%gnisu02%selciheV02%02%A0%suomonotuA02%ni02%lortnoC02%esiurC02%evitpadA02%fo02%noitazimitpO02%dna02%noitaulavE=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Evaluation%20and%20Optimization%20of%20Adaptive%20Cruise%20Control%20in%20Autonomous%0A%20%20Vehicles%20using%20the%20CARLA%20Simulator%3A%20A%20Study%20on%20Performance%20under%20Wet%20and%20Dry%0A%20%20Weather%20Conditions&body=Title%3A%20Evaluation%20and%20Optimization%20of%20Adaptive%20Cruise%20Control%20in%20Autonomous%0A%20%20Vehicles%20using%20the%20CARLA%20Simulator%3A%20A%20Study%20on%20Performance%20under%20Wet%20and%20Dry%0A%20%20Weather%20Conditions%0AAuthor%3A%20Roza%20Al-Hindaw%20and%20Taqwa%20I.%20Alhadidi%20and%20Mohammad%20Adas%0AAbstract%3A%20%20%20Adaptive%20Cruise%20Control%20ACC%20can%20change%20the%20speed%20of%20the%20ego%20vehicle%20to%0Amaintain%20a%20safe%20distance%20from%20the%20following%20vehicle%20automatically.%20The%20primary%0Apurpose%20of%20this%20research%20is%20to%20use%20cutting-edge%20computing%20approaches%20to%20locate%0Aand%20track%20vehicles%20in%20real%20time%20under%20various%20conditions%20to%20achieve%20a%20safe%20ACC.%0AThe%20paper%20examines%20the%20extension%20of%20ACC%20employing%20depth%20cameras%20and%20radar%0Asensors%20within%20Autonomous%20Vehicles%20AVs%20to%20respond%20in%20real%20time%20by%20changing%0Aweather%20conditions%20using%20the%20Car%20Learning%20to%20Act%20CARLA%20simulation%20platform%20at%0Anoon.%20The%20ego%20vehicle%20controller%27s%20decision%20to%20accelerate%20or%20decelerate%20depends%0Aon%20the%20speed%20of%20the%20leading%20ahead%20vehicle%20and%20the%20safe%20distance%20from%20that%0Avehicle.%20Simulation%20results%20show%20that%20a%20Proportional%20Integral%20Derivative%20PID%0Acontrol%20of%20autonomous%20vehicles%20using%20a%20depth%20camera%20and%20radar%20sensors%20reduces%0Athe%20speed%20of%20the%20leading%20vehicle%20and%20the%20ego%20vehicle%20when%20it%20rains.%20In%0Aaddition%2C%20longer%20travel%20time%20was%20observed%20for%20both%20vehicles%20in%20rainy%20conditions%0Athan%20in%20dry%20conditions.%20Also%2C%20PID%20control%20prevents%20the%20leading%20vehicle%20from%0Arear%20collisions%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01504v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20and%20Optimization%20of%20Adaptive%20Cruise%20Control%20in%20Autonomous%0A%20%20Vehicles%20using%20the%20CARLA%20Simulator%3A%20A%20Study%20on%20Performance%20under%20Wet%20and%20Dry%0A%20%20Weather%20Conditions&entry.906535625=Roza%20Al-Hindaw%20and%20Taqwa%20I.%20Alhadidi%20and%20Mohammad%20Adas&entry.1292438233=%20%20Adaptive%20Cruise%20Control%20ACC%20can%20change%20the%20speed%20of%20the%20ego%20vehicle%20to%0Amaintain%20a%20safe%20distance%20from%20the%20following%20vehicle%20automatically.%20The%20primary%0Apurpose%20of%20this%20research%20is%20to%20use%20cutting-edge%20computing%20approaches%20to%20locate%0Aand%20track%20vehicles%20in%20real%20time%20under%20various%20conditions%20to%20achieve%20a%20safe%20ACC.%0AThe%20paper%20examines%20the%20extension%20of%20ACC%20employing%20depth%20cameras%20and%20radar%0Asensors%20within%20Autonomous%20Vehicles%20AVs%20to%20respond%20in%20real%20time%20by%20changing%0Aweather%20conditions%20using%20the%20Car%20Learning%20to%20Act%20CARLA%20simulation%20platform%20at%0Anoon.%20The%20ego%20vehicle%20controller%27s%20decision%20to%20accelerate%20or%20decelerate%20depends%0Aon%20the%20speed%20of%20the%20leading%20ahead%20vehicle%20and%20the%20safe%20distance%20from%20that%0Avehicle.%20Simulation%20results%20show%20that%20a%20Proportional%20Integral%20Derivative%20PID%0Acontrol%20of%20autonomous%20vehicles%20using%20a%20depth%20camera%20and%20radar%20sensors%20reduces%0Athe%20speed%20of%20the%20leading%20vehicle%20and%20the%20ego%20vehicle%20when%20it%20rains.%20In%0Aaddition%2C%20longer%20travel%20time%20was%20observed%20for%20both%20vehicles%20in%20rainy%20conditions%0Athan%20in%20dry%20conditions.%20Also%2C%20PID%20control%20prevents%20the%20leading%20vehicle%20from%0Arear%20collisions%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01504v1&entry.124074799=Read"},
{"title": "Adaptive aggregation of Monte Carlo augmented decomposed filters for\n  efficient group-equivariant convolutional neural network", "author": "Wenzhao Zhao and Barbara D. Wichtmann and Steffen Albert and Angelika Maurer and Frank G. Z\u00f6llner and Ulrike Attenberger and J\u00fcrgen Hesser", "abstract": "  Group-equivariant convolutional neural networks (G-CNN) heavily rely on\nparameter sharing to increase CNN's data efficiency and performance. However,\nthe parameter-sharing strategy greatly increases the computational burden for\neach added parameter, which hampers its application to deep neural network\nmodels. In this paper, we address these problems by proposing a\nnon-parameter-sharing approach for group equivariant neural networks. The\nproposed methods adaptively aggregate a diverse range of filters by a weighted\nsum of stochastically augmented decomposed filters. We give theoretical proof\nabout how the continuous group convolution can be approximated by our methods.\nOur method applies to both continuous and discrete groups, where the\naugmentation is implemented using Monte Carlo sampling and bootstrap\nresampling, respectively. We demonstrate that our methods serve as an efficient\nextension of standard CNN. Experiments on group equivariance tests show how our\nmethods can achieve superior performance to parameter-sharing group equivariant\nnetworks. Experiments on image classification and image denoising tasks show\nthat in certain scenarios, with a suitable set of filter bases, our method\nhelps improve the performance of standard CNNs and build efficient lightweight\nimage denoising networks. The code will be available at\nhttps://github.com/ZhaoWenzhao/MCG_CNN.\n", "link": "http://arxiv.org/abs/2305.10110v3", "date": "2024-05-01", "relevancy": 2.4382, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4968}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4857}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4804}], "mailto": "mailto:daeR=997470421.yrtne&3v01101.5032/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.NNC_GCM/oahzneWoahZ/moc.buhtig//A3%sptthA0%ta02%elbaliava02%eb02%lliw02%edoc02%ehT02%.skrowten02%gnisioned02%egamiA0%thgiewthgil02%tneiciffe02%dliub02%dna02%sNNC02%dradnats02%fo02%ecnamrofrep02%eht02%evorpmi02%splehA0%dohtem02%ruo02%C2%sesab02%retlif02%fo02%tes02%elbatius02%a02%htiw02%C2%soiranecs02%niatrec02%ni02%tahtA0%wohs02%sksat02%gnisioned02%egami02%dna02%noitacifissalc02%egami02%no02%stnemirepxE02%.skrowtenA0%tnairaviuqe02%puorg02%gnirahs-retemarap02%ot02%ecnamrofrep02%roirepus02%eveihca02%nac02%sdohtemA0%ruo02%woh02%wohs02%stset02%ecnairaviuqe02%puorg02%no02%stnemirepxE02%.NNC02%dradnats02%fo02%noisnetxeA0%tneiciffe02%na02%sa02%evres02%sdohtem02%ruo02%taht02%etartsnomed02%eW02%.ylevitcepser02%C2%gnilpmaserA0%partstoob02%dna02%gnilpmas02%olraC02%etnoM02%gnisu02%detnemelpmi02%si02%noitatnemguaA0%eht02%erehw02%C2%spuorg02%etercsid02%dna02%suounitnoc02%htob02%ot02%seilppa02%dohtem02%ruOA0%.sdohtem02%ruo02%yb02%detamixorppa02%eb02%nac02%noitulovnoc02%puorg02%suounitnoc02%eht02%woh02%tuobaA0%foorp02%laciteroeht02%evig02%eW02%.sretlif02%desopmoced02%detnemgua02%yllacitsahcots02%fo02%musA0%dethgiew02%a02%yb02%sretlif02%fo02%egnar02%esrevid02%a02%etagergga02%ylevitpada02%sdohtem02%desoporpA0%ehT02%.skrowten02%laruen02%tnairaviuqe02%puorg02%rof02%hcaorppa02%gnirahs-retemarap-nonA0%a02%gnisoporp02%yb02%smelborp02%eseht02%sserdda02%ew02%C2%repap02%siht02%nI02%.sledomA0%krowten02%laruen02%peed02%ot02%noitacilppa02%sti02%srepmah02%hcihw02%C2%retemarap02%dedda02%hcaeA0%rof02%nedrub02%lanoitatupmoc02%eht02%sesaercni02%yltaerg02%ygetarts02%gnirahs-retemarap02%ehtA0%C2%revewoH02%.ecnamrofrep02%dna02%ycneiciffe02%atad02%s72%NNC02%esaercni02%ot02%gnirahs02%retemarapA0%no02%yler02%ylivaeh02%92%NNC-G82%02%skrowten02%laruen02%lanoitulovnoc02%tnairaviuqe-puorG02%02%=3328342921.yrtne&resseH02%negrCB%3C%J02%dna02%regrebnettA02%ekirlU02%dna02%renll6B%3C%Z02%.G02%knarF02%dna02%reruaM02%akilegnA02%dna02%treblA02%neffetS02%dna02%nnamthciW02%.D02%arabraB02%dna02%oahZ02%oahzneW=526535609.yrtne&krowten02%laruen02%lanoitulovnoc02%tnairaviuqe-puorg02%tneiciffe02%02%A0%rof02%sretlif02%desopmoced02%detnemgua02%olraC02%etnoM02%fo02%noitagergga02%evitpadA=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Adaptive%20aggregation%20of%20Monte%20Carlo%20augmented%20decomposed%20filters%20for%0A%20%20efficient%20group-equivariant%20convolutional%20neural%20network&body=Title%3A%20Adaptive%20aggregation%20of%20Monte%20Carlo%20augmented%20decomposed%20filters%20for%0A%20%20efficient%20group-equivariant%20convolutional%20neural%20network%0AAuthor%3A%20Wenzhao%20Zhao%20and%20Barbara%20D.%20Wichtmann%20and%20Steffen%20Albert%20and%20Angelika%20Maurer%20and%20Frank%20G.%20Z%C3%B6llner%20and%20Ulrike%20Attenberger%20and%20J%C3%BCrgen%20Hesser%0AAbstract%3A%20%20%20Group-equivariant%20convolutional%20neural%20networks%20%28G-CNN%29%20heavily%20rely%20on%0Aparameter%20sharing%20to%20increase%20CNN%27s%20data%20efficiency%20and%20performance.%20However%2C%0Athe%20parameter-sharing%20strategy%20greatly%20increases%20the%20computational%20burden%20for%0Aeach%20added%20parameter%2C%20which%20hampers%20its%20application%20to%20deep%20neural%20network%0Amodels.%20In%20this%20paper%2C%20we%20address%20these%20problems%20by%20proposing%20a%0Anon-parameter-sharing%20approach%20for%20group%20equivariant%20neural%20networks.%20The%0Aproposed%20methods%20adaptively%20aggregate%20a%20diverse%20range%20of%20filters%20by%20a%20weighted%0Asum%20of%20stochastically%20augmented%20decomposed%20filters.%20We%20give%20theoretical%20proof%0Aabout%20how%20the%20continuous%20group%20convolution%20can%20be%20approximated%20by%20our%20methods.%0AOur%20method%20applies%20to%20both%20continuous%20and%20discrete%20groups%2C%20where%20the%0Aaugmentation%20is%20implemented%20using%20Monte%20Carlo%20sampling%20and%20bootstrap%0Aresampling%2C%20respectively.%20We%20demonstrate%20that%20our%20methods%20serve%20as%20an%20efficient%0Aextension%20of%20standard%20CNN.%20Experiments%20on%20group%20equivariance%20tests%20show%20how%20our%0Amethods%20can%20achieve%20superior%20performance%20to%20parameter-sharing%20group%20equivariant%0Anetworks.%20Experiments%20on%20image%20classification%20and%20image%20denoising%20tasks%20show%0Athat%20in%20certain%20scenarios%2C%20with%20a%20suitable%20set%20of%20filter%20bases%2C%20our%20method%0Ahelps%20improve%20the%20performance%20of%20standard%20CNNs%20and%20build%20efficient%20lightweight%0Aimage%20denoising%20networks.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/ZhaoWenzhao/MCG_CNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.10110v3%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20aggregation%20of%20Monte%20Carlo%20augmented%20decomposed%20filters%20for%0A%20%20efficient%20group-equivariant%20convolutional%20neural%20network&entry.906535625=Wenzhao%20Zhao%20and%20Barbara%20D.%20Wichtmann%20and%20Steffen%20Albert%20and%20Angelika%20Maurer%20and%20Frank%20G.%20Z%C3%B6llner%20and%20Ulrike%20Attenberger%20and%20J%C3%BCrgen%20Hesser&entry.1292438233=%20%20Group-equivariant%20convolutional%20neural%20networks%20%28G-CNN%29%20heavily%20rely%20on%0Aparameter%20sharing%20to%20increase%20CNN%27s%20data%20efficiency%20and%20performance.%20However%2C%0Athe%20parameter-sharing%20strategy%20greatly%20increases%20the%20computational%20burden%20for%0Aeach%20added%20parameter%2C%20which%20hampers%20its%20application%20to%20deep%20neural%20network%0Amodels.%20In%20this%20paper%2C%20we%20address%20these%20problems%20by%20proposing%20a%0Anon-parameter-sharing%20approach%20for%20group%20equivariant%20neural%20networks.%20The%0Aproposed%20methods%20adaptively%20aggregate%20a%20diverse%20range%20of%20filters%20by%20a%20weighted%0Asum%20of%20stochastically%20augmented%20decomposed%20filters.%20We%20give%20theoretical%20proof%0Aabout%20how%20the%20continuous%20group%20convolution%20can%20be%20approximated%20by%20our%20methods.%0AOur%20method%20applies%20to%20both%20continuous%20and%20discrete%20groups%2C%20where%20the%0Aaugmentation%20is%20implemented%20using%20Monte%20Carlo%20sampling%20and%20bootstrap%0Aresampling%2C%20respectively.%20We%20demonstrate%20that%20our%20methods%20serve%20as%20an%20efficient%0Aextension%20of%20standard%20CNN.%20Experiments%20on%20group%20equivariance%20tests%20show%20how%20our%0Amethods%20can%20achieve%20superior%20performance%20to%20parameter-sharing%20group%20equivariant%0Anetworks.%20Experiments%20on%20image%20classification%20and%20image%20denoising%20tasks%20show%0Athat%20in%20certain%20scenarios%2C%20with%20a%20suitable%20set%20of%20filter%20bases%2C%20our%20method%0Ahelps%20improve%20the%20performance%20of%20standard%20CNNs%20and%20build%20efficient%20lightweight%0Aimage%20denoising%20networks.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/ZhaoWenzhao/MCG_CNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.10110v3&entry.124074799=Read"},
{"title": "IntervenGen: Interventional Data Generation for Robust and\n  Data-Efficient Robot Imitation Learning", "author": "Ryan Hoque and Ajay Mandlekar and Caelan Garrett and Ken Goldberg and Dieter Fox", "abstract": "  Imitation learning is a promising paradigm for training robot control\npolicies, but these policies can suffer from distribution shift, where the\nconditions at evaluation time differ from those in the training data. A popular\napproach for increasing policy robustness to distribution shift is interactive\nimitation learning (i.e., DAgger and variants), where a human operator provides\ncorrective interventions during policy rollouts. However, collecting a\nsufficient amount of interventions to cover the distribution of policy mistakes\ncan be burdensome for human operators. We propose IntervenGen (I-Gen), a novel\ndata generation system that can autonomously produce a large set of corrective\ninterventions with rich coverage of the state space from a small number of\nhuman interventions. We apply I-Gen to 4 simulated environments and 1 physical\nenvironment with object pose estimation error and show that it can increase\npolicy robustness by up to 39x with only 10 human interventions. Videos and\nmore results are available at https://sites.google.com/view/intervengen2024.\n", "link": "http://arxiv.org/abs/2405.01472v1", "date": "2024-05-02", "relevancy": 2.4295, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.6525}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.588}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.57}], "mailto": "mailto:daeR=997470421.yrtne&1v27410.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.4202negnevretni/weiv/moc.elgoog.setis//A3%sptth02%ta02%elbaliava02%era02%stluser02%eromA0%dna02%soediV02%.snoitnevretni02%namuh02%0102%ylno02%htiw02%x9302%ot02%pu02%yb02%ssentsubor02%ycilopA0%esaercni02%nac02%ti02%taht02%wohs02%dna02%rorre02%noitamitse02%esop02%tcejbo02%htiw02%tnemnorivneA0%lacisyhp02%102%dna02%stnemnorivne02%detalumis02%402%ot02%neG-I02%ylppa02%eW02%.snoitnevretni02%namuhA0%fo02%rebmun02%llams02%a02%morf02%ecaps02%etats02%eht02%fo02%egarevoc02%hcir02%htiw02%snoitnevretniA0%evitcerroc02%fo02%tes02%egral02%a02%ecudorp02%ylsuomonotua02%nac02%taht02%metsys02%noitareneg02%atadA0%levon02%a02%C2%92%neG-I82%02%neGnevretnI02%esoporp02%eW02%.srotarepo02%namuh02%rof02%emosnedrub02%eb02%nacA0%sekatsim02%ycilop02%fo02%noitubirtsid02%eht02%revoc02%ot02%snoitnevretni02%fo02%tnuoma02%tneiciffusA0%a02%gnitcelloc02%C2%revewoH02%.stuollor02%ycilop02%gnirud02%snoitnevretni02%evitcerrocA0%sedivorp02%rotarepo02%namuh02%a02%erehw02%C2%92%stnairav02%dna02%reggAD02%C2%.e.i82%02%gninrael02%noitatimiA0%evitcaretni02%si02%tfihs02%noitubirtsid02%ot02%ssentsubor02%ycilop02%gnisaercni02%rof02%hcaorppaA0%ralupop02%A02%.atad02%gniniart02%eht02%ni02%esoht02%morf02%reffid02%emit02%noitaulave02%ta02%snoitidnocA0%eht02%erehw02%C2%tfihs02%noitubirtsid02%morf02%reffus02%nac02%seicilop02%eseht02%tub02%C2%seicilopA0%lortnoc02%tobor02%gniniart02%rof02%mgidarap02%gnisimorp02%a02%si02%gninrael02%noitatimI02%02%=3328342921.yrtne&xoF02%reteiD02%dna02%grebdloG02%neK02%dna02%tterraG02%naleaC02%dna02%rakeldnaM02%yajA02%dna02%euqoH02%nayR=526535609.yrtne&gninraeL02%noitatimI02%toboR02%tneiciffE-ataD02%02%A0%dna02%tsuboR02%rof02%noitareneG02%ataD02%lanoitnevretnI02%A3%neGnevretnI=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20IntervenGen%3A%20Interventional%20Data%20Generation%20for%20Robust%20and%0A%20%20Data-Efficient%20Robot%20Imitation%20Learning&body=Title%3A%20IntervenGen%3A%20Interventional%20Data%20Generation%20for%20Robust%20and%0A%20%20Data-Efficient%20Robot%20Imitation%20Learning%0AAuthor%3A%20Ryan%20Hoque%20and%20Ajay%20Mandlekar%20and%20Caelan%20Garrett%20and%20Ken%20Goldberg%20and%20Dieter%20Fox%0AAbstract%3A%20%20%20Imitation%20learning%20is%20a%20promising%20paradigm%20for%20training%20robot%20control%0Apolicies%2C%20but%20these%20policies%20can%20suffer%20from%20distribution%20shift%2C%20where%20the%0Aconditions%20at%20evaluation%20time%20differ%20from%20those%20in%20the%20training%20data.%20A%20popular%0Aapproach%20for%20increasing%20policy%20robustness%20to%20distribution%20shift%20is%20interactive%0Aimitation%20learning%20%28i.e.%2C%20DAgger%20and%20variants%29%2C%20where%20a%20human%20operator%20provides%0Acorrective%20interventions%20during%20policy%20rollouts.%20However%2C%20collecting%20a%0Asufficient%20amount%20of%20interventions%20to%20cover%20the%20distribution%20of%20policy%20mistakes%0Acan%20be%20burdensome%20for%20human%20operators.%20We%20propose%20IntervenGen%20%28I-Gen%29%2C%20a%20novel%0Adata%20generation%20system%20that%20can%20autonomously%20produce%20a%20large%20set%20of%20corrective%0Ainterventions%20with%20rich%20coverage%20of%20the%20state%20space%20from%20a%20small%20number%20of%0Ahuman%20interventions.%20We%20apply%20I-Gen%20to%204%20simulated%20environments%20and%201%20physical%0Aenvironment%20with%20object%20pose%20estimation%20error%20and%20show%20that%20it%20can%20increase%0Apolicy%20robustness%20by%20up%20to%2039x%20with%20only%2010%20human%20interventions.%20Videos%20and%0Amore%20results%20are%20available%20at%20https%3A//sites.google.com/view/intervengen2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01472v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IntervenGen%3A%20Interventional%20Data%20Generation%20for%20Robust%20and%0A%20%20Data-Efficient%20Robot%20Imitation%20Learning&entry.906535625=Ryan%20Hoque%20and%20Ajay%20Mandlekar%20and%20Caelan%20Garrett%20and%20Ken%20Goldberg%20and%20Dieter%20Fox&entry.1292438233=%20%20Imitation%20learning%20is%20a%20promising%20paradigm%20for%20training%20robot%20control%0Apolicies%2C%20but%20these%20policies%20can%20suffer%20from%20distribution%20shift%2C%20where%20the%0Aconditions%20at%20evaluation%20time%20differ%20from%20those%20in%20the%20training%20data.%20A%20popular%0Aapproach%20for%20increasing%20policy%20robustness%20to%20distribution%20shift%20is%20interactive%0Aimitation%20learning%20%28i.e.%2C%20DAgger%20and%20variants%29%2C%20where%20a%20human%20operator%20provides%0Acorrective%20interventions%20during%20policy%20rollouts.%20However%2C%20collecting%20a%0Asufficient%20amount%20of%20interventions%20to%20cover%20the%20distribution%20of%20policy%20mistakes%0Acan%20be%20burdensome%20for%20human%20operators.%20We%20propose%20IntervenGen%20%28I-Gen%29%2C%20a%20novel%0Adata%20generation%20system%20that%20can%20autonomously%20produce%20a%20large%20set%20of%20corrective%0Ainterventions%20with%20rich%20coverage%20of%20the%20state%20space%20from%20a%20small%20number%20of%0Ahuman%20interventions.%20We%20apply%20I-Gen%20to%204%20simulated%20environments%20and%201%20physical%0Aenvironment%20with%20object%20pose%20estimation%20error%20and%20show%20that%20it%20can%20increase%0Apolicy%20robustness%20by%20up%20to%2039x%20with%20only%2010%20human%20interventions.%20Videos%20and%0Amore%20results%20are%20available%20at%20https%3A//sites.google.com/view/intervengen2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01472v1&entry.124074799=Read"},
{"title": "Learning to Embed Time Series Patches Independently", "author": "Seunghan Lee and Taeyoung Park and Kibok Lee", "abstract": "  Masked time series modeling has recently gained much attention as a\nself-supervised representation learning strategy for time series. Inspired by\nmasked image modeling in computer vision, recent works first patchify and\npartially mask out time series, and then train Transformers to capture the\ndependencies between patches by predicting masked patches from unmasked\npatches. However, we argue that capturing such patch dependencies might not be\nan optimal strategy for time series representation learning; rather, learning\nto embed patches independently results in better time series representations.\nSpecifically, we propose to use 1) the simple patch reconstruction task, which\nautoencode each patch without looking at other patches, and 2) the simple\npatch-wise MLP that embeds each patch independently. In addition, we introduce\ncomplementary contrastive learning to hierarchically capture adjacent time\nseries information efficiently. Our proposed method improves time series\nforecasting and classification performance compared to state-of-the-art\nTransformer-based models, while it is more efficient in terms of the number of\nparameters and training/inference time. Code is available at this repository:\nhttps://github.com/seunghan96/pits.\n", "link": "http://arxiv.org/abs/2312.16427v4", "date": "2024-05-02", "relevancy": 2.4288, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5065}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4816}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4692}], "mailto": "mailto:daeR=997470421.yrtne&4v72461.2132/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.stip/69nahgnues/moc.buhtig//A3%sptthA0%A3%yrotisoper02%siht02%ta02%elbaliava02%si02%edoC02%.emit02%ecnerefni/gniniart02%dna02%sretemarapA0%fo02%rebmun02%eht02%fo02%smret02%ni02%tneiciffe02%erom02%si02%ti02%elihw02%C2%sledom02%desab-remrofsnarTA0%tra-eht-fo-etats02%ot02%derapmoc02%ecnamrofrep02%noitacifissalc02%dna02%gnitsacerofA0%seires02%emit02%sevorpmi02%dohtem02%desoporp02%ruO02%.yltneiciffe02%noitamrofni02%seiresA0%emit02%tnecajda02%erutpac02%yllacihcrareih02%ot02%gninrael02%evitsartnoc02%yratnemelpmocA0%ecudortni02%ew02%C2%noitidda02%nI02%.yltnednepedni02%hctap02%hcae02%sdebme02%taht02%PLM02%esiw-hctapA0%elpmis02%eht02%92%202%dna02%C2%sehctap02%rehto02%ta02%gnikool02%tuohtiw02%hctap02%hcae02%edocneotuaA0%hcihw02%C2%ksat02%noitcurtsnocer02%hctap02%elpmis02%eht02%92%102%esu02%ot02%esoporp02%ew02%C2%yllacificepSA0%.snoitatneserper02%seires02%emit02%retteb02%ni02%stluser02%yltnednepedni02%sehctap02%debme02%otA0%gninrael02%C2%rehtar02%B3%gninrael02%noitatneserper02%seires02%emit02%rof02%ygetarts02%lamitpo02%naA0%eb02%ton02%thgim02%seicnedneped02%hctap02%hcus02%gnirutpac02%taht02%eugra02%ew02%C2%revewoH02%.sehctapA0%deksamnu02%morf02%sehctap02%deksam02%gnitciderp02%yb02%sehctap02%neewteb02%seicnednepedA0%eht02%erutpac02%ot02%sremrofsnarT02%niart02%neht02%dna02%C2%seires02%emit02%tuo02%ksam02%yllaitrapA0%dna02%yfihctap02%tsrif02%skrow02%tnecer02%C2%noisiv02%retupmoc02%ni02%gniledom02%egami02%deksamA0%yb02%deripsnI02%.seires02%emit02%rof02%ygetarts02%gninrael02%noitatneserper02%desivrepus-flesA0%a02%sa02%noitnetta02%hcum02%deniag02%yltnecer02%sah02%gniledom02%seires02%emit02%deksaM02%02%=3328342921.yrtne&eeL02%kobiK02%dna02%kraP02%gnuoyeaT02%dna02%eeL02%nahgnueS=526535609.yrtne&yltnednepednI02%sehctaP02%seireS02%emiT02%debmE02%ot02%gninraeL=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Embed%20Time%20Series%20Patches%20Independently&body=Title%3A%20Learning%20to%20Embed%20Time%20Series%20Patches%20Independently%0AAuthor%3A%20Seunghan%20Lee%20and%20Taeyoung%20Park%20and%20Kibok%20Lee%0AAbstract%3A%20%20%20Masked%20time%20series%20modeling%20has%20recently%20gained%20much%20attention%20as%20a%0Aself-supervised%20representation%20learning%20strategy%20for%20time%20series.%20Inspired%20by%0Amasked%20image%20modeling%20in%20computer%20vision%2C%20recent%20works%20first%20patchify%20and%0Apartially%20mask%20out%20time%20series%2C%20and%20then%20train%20Transformers%20to%20capture%20the%0Adependencies%20between%20patches%20by%20predicting%20masked%20patches%20from%20unmasked%0Apatches.%20However%2C%20we%20argue%20that%20capturing%20such%20patch%20dependencies%20might%20not%20be%0Aan%20optimal%20strategy%20for%20time%20series%20representation%20learning%3B%20rather%2C%20learning%0Ato%20embed%20patches%20independently%20results%20in%20better%20time%20series%20representations.%0ASpecifically%2C%20we%20propose%20to%20use%201%29%20the%20simple%20patch%20reconstruction%20task%2C%20which%0Aautoencode%20each%20patch%20without%20looking%20at%20other%20patches%2C%20and%202%29%20the%20simple%0Apatch-wise%20MLP%20that%20embeds%20each%20patch%20independently.%20In%20addition%2C%20we%20introduce%0Acomplementary%20contrastive%20learning%20to%20hierarchically%20capture%20adjacent%20time%0Aseries%20information%20efficiently.%20Our%20proposed%20method%20improves%20time%20series%0Aforecasting%20and%20classification%20performance%20compared%20to%20state-of-the-art%0ATransformer-based%20models%2C%20while%20it%20is%20more%20efficient%20in%20terms%20of%20the%20number%20of%0Aparameters%20and%20training/inference%20time.%20Code%20is%20available%20at%20this%20repository%3A%0Ahttps%3A//github.com/seunghan96/pits.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16427v4%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Embed%20Time%20Series%20Patches%20Independently&entry.906535625=Seunghan%20Lee%20and%20Taeyoung%20Park%20and%20Kibok%20Lee&entry.1292438233=%20%20Masked%20time%20series%20modeling%20has%20recently%20gained%20much%20attention%20as%20a%0Aself-supervised%20representation%20learning%20strategy%20for%20time%20series.%20Inspired%20by%0Amasked%20image%20modeling%20in%20computer%20vision%2C%20recent%20works%20first%20patchify%20and%0Apartially%20mask%20out%20time%20series%2C%20and%20then%20train%20Transformers%20to%20capture%20the%0Adependencies%20between%20patches%20by%20predicting%20masked%20patches%20from%20unmasked%0Apatches.%20However%2C%20we%20argue%20that%20capturing%20such%20patch%20dependencies%20might%20not%20be%0Aan%20optimal%20strategy%20for%20time%20series%20representation%20learning%3B%20rather%2C%20learning%0Ato%20embed%20patches%20independently%20results%20in%20better%20time%20series%20representations.%0ASpecifically%2C%20we%20propose%20to%20use%201%29%20the%20simple%20patch%20reconstruction%20task%2C%20which%0Aautoencode%20each%20patch%20without%20looking%20at%20other%20patches%2C%20and%202%29%20the%20simple%0Apatch-wise%20MLP%20that%20embeds%20each%20patch%20independently.%20In%20addition%2C%20we%20introduce%0Acomplementary%20contrastive%20learning%20to%20hierarchically%20capture%20adjacent%20time%0Aseries%20information%20efficiently.%20Our%20proposed%20method%20improves%20time%20series%0Aforecasting%20and%20classification%20performance%20compared%20to%20state-of-the-art%0ATransformer-based%20models%2C%20while%20it%20is%20more%20efficient%20in%20terms%20of%20the%20number%20of%0Aparameters%20and%20training/inference%20time.%20Code%20is%20available%20at%20this%20repository%3A%0Ahttps%3A//github.com/seunghan96/pits.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16427v4&entry.124074799=Read"},
{"title": "Polynomial Chaos Expanded Gaussian Process", "author": "Dominik Polke and Tim K\u00f6sters and Elmar Ahle and Dirk S\u00f6ffker", "abstract": "  In complex and unknown processes, global models are initially generated over\nthe entire experimental space, but they often fail to provide accurate\npredictions in local areas. Recognizing this limitation, this study addresses\nthe need for models that effectively represent both global and local\nexperimental spaces. It introduces a novel machine learning (ML) approach:\nPolynomial Chaos Expanded Gaussian Process (PCEGP), leveraging polynomial chaos\nexpansion (PCE) to calculate input-dependent hyperparameters of the Gaussian\nprocess (GP). This approach provides a mathematically interpretable method that\nincorporates non-stationary covariance functions and heteroscedastic noise\nestimation to generate locally adapted models. The model performance is\ncompared to different algorithms in benchmark tests for regression tasks. The\nresults demonstrate low prediction errors of the PCEGP in these benchmark\napplications, highlighting model performance that is often competitive with or\nsuperior to previous methods. A key advantage of the presented model is the\ntransparency and traceability in the calculation of hyperparameters and model\npredictions.\n", "link": "http://arxiv.org/abs/2405.01052v1", "date": "2024-05-02", "relevancy": 2.4286, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4978}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4898}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4695}], "mailto": "mailto:daeR=997470421.yrtne&1v25010.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.snoitciderpA0%ledom02%dna02%sretemaraprepyh02%fo02%noitaluclac02%eht02%ni02%ytilibaecart02%dna02%ycnerapsnartA0%eht02%si02%ledom02%detneserp02%eht02%fo02%egatnavda02%yek02%A02%.sdohtem02%suoiverp02%ot02%roirepusA0%ro02%htiw02%evititepmoc02%netfo02%si02%taht02%ecnamrofrep02%ledom02%gnithgilhgih02%C2%snoitacilppaA0%kramhcneb02%eseht02%ni02%PGECP02%eht02%fo02%srorre02%noitciderp02%wol02%etartsnomed02%stluserA0%ehT02%.sksat02%noisserger02%rof02%stset02%kramhcneb02%ni02%smhtirogla02%tnereffid02%ot02%derapmocA0%si02%ecnamrofrep02%ledom02%ehT02%.sledom02%detpada02%yllacol02%etareneg02%ot02%noitamitseA0%esion02%citsadecsoreteh02%dna02%snoitcnuf02%ecnairavoc02%yranoitats-non02%setaroprocniA0%taht02%dohtem02%elbaterpretni02%yllacitamehtam02%a02%sedivorp02%hcaorppa02%sihT02%.92%PG82%02%ssecorpA0%naissuaG02%eht02%fo02%sretemaraprepyh02%tnedneped-tupni02%etaluclac02%ot02%92%ECP82%02%noisnapxeA0%soahc02%laimonylop02%gnigarevel02%C2%92%PGECP82%02%ssecorP02%naissuaG02%dednapxE02%soahC02%laimonyloPA0%A3%hcaorppa02%92%LM82%02%gninrael02%enihcam02%levon02%a02%secudortni02%tI02%.secaps02%latnemirepxeA0%lacol02%dna02%labolg02%htob02%tneserper02%ylevitceffe02%taht02%sledom02%rof02%deen02%ehtA0%sesserdda02%yduts02%siht02%C2%noitatimil02%siht02%gnizingoceR02%.saera02%lacol02%ni02%snoitciderpA0%etarucca02%edivorp02%ot02%liaf02%netfo02%yeht02%tub02%C2%ecaps02%latnemirepxe02%eritne02%ehtA0%revo02%detareneg02%yllaitini02%era02%sledom02%labolg02%C2%sessecorp02%nwonknu02%dna02%xelpmoc02%nI02%02%=3328342921.yrtne&rekff6B%3C%S02%kriD02%dna02%elhA02%ramlE02%dna02%srets6B%3C%K02%miT02%dna02%ekloP02%kinimoD=526535609.yrtne&ssecorP02%naissuaG02%dednapxE02%soahC02%laimonyloP=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Polynomial%20Chaos%20Expanded%20Gaussian%20Process&body=Title%3A%20Polynomial%20Chaos%20Expanded%20Gaussian%20Process%0AAuthor%3A%20Dominik%20Polke%20and%20Tim%20K%C3%B6sters%20and%20Elmar%20Ahle%20and%20Dirk%20S%C3%B6ffker%0AAbstract%3A%20%20%20In%20complex%20and%20unknown%20processes%2C%20global%20models%20are%20initially%20generated%20over%0Athe%20entire%20experimental%20space%2C%20but%20they%20often%20fail%20to%20provide%20accurate%0Apredictions%20in%20local%20areas.%20Recognizing%20this%20limitation%2C%20this%20study%20addresses%0Athe%20need%20for%20models%20that%20effectively%20represent%20both%20global%20and%20local%0Aexperimental%20spaces.%20It%20introduces%20a%20novel%20machine%20learning%20%28ML%29%20approach%3A%0APolynomial%20Chaos%20Expanded%20Gaussian%20Process%20%28PCEGP%29%2C%20leveraging%20polynomial%20chaos%0Aexpansion%20%28PCE%29%20to%20calculate%20input-dependent%20hyperparameters%20of%20the%20Gaussian%0Aprocess%20%28GP%29.%20This%20approach%20provides%20a%20mathematically%20interpretable%20method%20that%0Aincorporates%20non-stationary%20covariance%20functions%20and%20heteroscedastic%20noise%0Aestimation%20to%20generate%20locally%20adapted%20models.%20The%20model%20performance%20is%0Acompared%20to%20different%20algorithms%20in%20benchmark%20tests%20for%20regression%20tasks.%20The%0Aresults%20demonstrate%20low%20prediction%20errors%20of%20the%20PCEGP%20in%20these%20benchmark%0Aapplications%2C%20highlighting%20model%20performance%20that%20is%20often%20competitive%20with%20or%0Asuperior%20to%20previous%20methods.%20A%20key%20advantage%20of%20the%20presented%20model%20is%20the%0Atransparency%20and%20traceability%20in%20the%20calculation%20of%20hyperparameters%20and%20model%0Apredictions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01052v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Polynomial%20Chaos%20Expanded%20Gaussian%20Process&entry.906535625=Dominik%20Polke%20and%20Tim%20K%C3%B6sters%20and%20Elmar%20Ahle%20and%20Dirk%20S%C3%B6ffker&entry.1292438233=%20%20In%20complex%20and%20unknown%20processes%2C%20global%20models%20are%20initially%20generated%20over%0Athe%20entire%20experimental%20space%2C%20but%20they%20often%20fail%20to%20provide%20accurate%0Apredictions%20in%20local%20areas.%20Recognizing%20this%20limitation%2C%20this%20study%20addresses%0Athe%20need%20for%20models%20that%20effectively%20represent%20both%20global%20and%20local%0Aexperimental%20spaces.%20It%20introduces%20a%20novel%20machine%20learning%20%28ML%29%20approach%3A%0APolynomial%20Chaos%20Expanded%20Gaussian%20Process%20%28PCEGP%29%2C%20leveraging%20polynomial%20chaos%0Aexpansion%20%28PCE%29%20to%20calculate%20input-dependent%20hyperparameters%20of%20the%20Gaussian%0Aprocess%20%28GP%29.%20This%20approach%20provides%20a%20mathematically%20interpretable%20method%20that%0Aincorporates%20non-stationary%20covariance%20functions%20and%20heteroscedastic%20noise%0Aestimation%20to%20generate%20locally%20adapted%20models.%20The%20model%20performance%20is%0Acompared%20to%20different%20algorithms%20in%20benchmark%20tests%20for%20regression%20tasks.%20The%0Aresults%20demonstrate%20low%20prediction%20errors%20of%20the%20PCEGP%20in%20these%20benchmark%0Aapplications%2C%20highlighting%20model%20performance%20that%20is%20often%20competitive%20with%20or%0Asuperior%20to%20previous%20methods.%20A%20key%20advantage%20of%20the%20presented%20model%20is%20the%0Atransparency%20and%20traceability%20in%20the%20calculation%20of%20hyperparameters%20and%20model%0Apredictions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01052v1&entry.124074799=Read"},
{"title": "Lying Graph Convolution: Learning to Lie for Node Classification Tasks", "author": "Daniele Castellana", "abstract": "  In the context of machine learning for graphs, many researchers have\nempirically observed that Deep Graph Networks (DGNs) perform favourably on node\nclassification tasks when the graph structure is homophilic (\\ie adjacent nodes\nare similar). In this paper, we introduce Lying-GCN, a new DGN inspired by\nopinion dynamics that can adaptively work in both the heterophilic and the\nhomophilic setting. At each layer, each agent (node) shares its own opinions\n(node embeddings) with its neighbours. Instead of sharing its opinion directly\nas in GCN, we introduce a mechanism which allows agents to lie. Such a\nmechanism is adaptive, thus the agents learn how and when to lie according to\nthe task that should be solved. We provide a characterisation of our proposal\nin terms of dynamical systems, by studying the spectral property of the\ncoefficient matrix of the system. While the steady state of the system\ncollapses to zero, we believe the lying mechanism is still usable to solve node\nclassification tasks. We empirically prove our belief on both synthetic and\nreal-world datasets, by showing that the lying mechanism allows to increase the\nperformances in the heterophilic setting without harming the results in the\nhomophilic one.\n", "link": "http://arxiv.org/abs/2405.01247v1", "date": "2024-05-02", "relevancy": 2.4184, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5109}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.471}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4691}], "mailto": "mailto:daeR=997470421.yrtne&1v74210.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.eno02%cilihpomohA0%eht02%ni02%stluser02%eht02%gnimrah02%tuohtiw02%gnittes02%cilihporeteh02%eht02%ni02%secnamrofrepA0%eht02%esaercni02%ot02%swolla02%msinahcem02%gniyl02%eht02%taht02%gniwohs02%yb02%C2%stesatad02%dlrow-laerA0%dna02%citehtnys02%htob02%no02%feileb02%ruo02%evorp02%yllaciripme02%eW02%.sksat02%noitacifissalcA0%edon02%evlos02%ot02%elbasu02%llits02%si02%msinahcem02%gniyl02%eht02%eveileb02%ew02%C2%orez02%ot02%sespallocA0%metsys02%eht02%fo02%etats02%ydaets02%eht02%elihW02%.metsys02%eht02%fo02%xirtam02%tneiciffeocA0%eht02%fo02%ytreporp02%lartceps02%eht02%gniyduts02%yb02%C2%smetsys02%lacimanyd02%fo02%smret02%niA0%lasoporp02%ruo02%fo02%noitasiretcarahc02%a02%edivorp02%eW02%.devlos02%eb02%dluohs02%taht02%ksat02%ehtA0%ot02%gnidrocca02%eil02%ot02%nehw02%dna02%woh02%nrael02%stnega02%eht02%suht02%C2%evitpada02%si02%msinahcemA0%a02%hcuS02%.eil02%ot02%stnega02%swolla02%hcihw02%msinahcem02%a02%ecudortni02%ew02%C2%NCG02%ni02%saA0%yltcerid02%noinipo02%sti02%gnirahs02%fo02%daetsnI02%.sruobhgien02%sti02%htiw02%92%sgniddebme02%edon82%A0%snoinipo02%nwo02%sti02%serahs02%92%edon82%02%tnega02%hcae02%C2%reyal02%hcae02%tA02%.gnittes02%cilihpomohA0%eht02%dna02%cilihporeteh02%eht02%htob02%ni02%krow02%ylevitpada02%nac02%taht02%scimanyd02%noinipoA0%yb02%deripsni02%NGD02%wen02%a02%C2%NCG-gniyL02%ecudortni02%ew02%C2%repap02%siht02%nI02%.92%ralimis02%eraA0%sedon02%tnecajda02%eiC5%82%02%cilihpomoh02%si02%erutcurts02%hparg02%eht02%nehw02%sksat02%noitacifissalcA0%edon02%no02%ylbaruovaf02%mrofrep02%92%sNGD82%02%skrowteN02%hparG02%peeD02%taht02%devresbo02%yllaciripmeA0%evah02%srehcraeser02%ynam02%C2%shparg02%rof02%gninrael02%enihcam02%fo02%txetnoc02%eht02%nI02%02%=3328342921.yrtne&analletsaC02%eleinaD=526535609.yrtne&sksaT02%noitacifissalC02%edoN02%rof02%eiL02%ot02%gninraeL02%A3%noitulovnoC02%hparG02%gniyL=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Lying%20Graph%20Convolution%3A%20Learning%20to%20Lie%20for%20Node%20Classification%20Tasks&body=Title%3A%20Lying%20Graph%20Convolution%3A%20Learning%20to%20Lie%20for%20Node%20Classification%20Tasks%0AAuthor%3A%20Daniele%20Castellana%0AAbstract%3A%20%20%20In%20the%20context%20of%20machine%20learning%20for%20graphs%2C%20many%20researchers%20have%0Aempirically%20observed%20that%20Deep%20Graph%20Networks%20%28DGNs%29%20perform%20favourably%20on%20node%0Aclassification%20tasks%20when%20the%20graph%20structure%20is%20homophilic%20%28%5Cie%20adjacent%20nodes%0Aare%20similar%29.%20In%20this%20paper%2C%20we%20introduce%20Lying-GCN%2C%20a%20new%20DGN%20inspired%20by%0Aopinion%20dynamics%20that%20can%20adaptively%20work%20in%20both%20the%20heterophilic%20and%20the%0Ahomophilic%20setting.%20At%20each%20layer%2C%20each%20agent%20%28node%29%20shares%20its%20own%20opinions%0A%28node%20embeddings%29%20with%20its%20neighbours.%20Instead%20of%20sharing%20its%20opinion%20directly%0Aas%20in%20GCN%2C%20we%20introduce%20a%20mechanism%20which%20allows%20agents%20to%20lie.%20Such%20a%0Amechanism%20is%20adaptive%2C%20thus%20the%20agents%20learn%20how%20and%20when%20to%20lie%20according%20to%0Athe%20task%20that%20should%20be%20solved.%20We%20provide%20a%20characterisation%20of%20our%20proposal%0Ain%20terms%20of%20dynamical%20systems%2C%20by%20studying%20the%20spectral%20property%20of%20the%0Acoefficient%20matrix%20of%20the%20system.%20While%20the%20steady%20state%20of%20the%20system%0Acollapses%20to%20zero%2C%20we%20believe%20the%20lying%20mechanism%20is%20still%20usable%20to%20solve%20node%0Aclassification%20tasks.%20We%20empirically%20prove%20our%20belief%20on%20both%20synthetic%20and%0Areal-world%20datasets%2C%20by%20showing%20that%20the%20lying%20mechanism%20allows%20to%20increase%20the%0Aperformances%20in%20the%20heterophilic%20setting%20without%20harming%20the%20results%20in%20the%0Ahomophilic%20one.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01247v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lying%20Graph%20Convolution%3A%20Learning%20to%20Lie%20for%20Node%20Classification%20Tasks&entry.906535625=Daniele%20Castellana&entry.1292438233=%20%20In%20the%20context%20of%20machine%20learning%20for%20graphs%2C%20many%20researchers%20have%0Aempirically%20observed%20that%20Deep%20Graph%20Networks%20%28DGNs%29%20perform%20favourably%20on%20node%0Aclassification%20tasks%20when%20the%20graph%20structure%20is%20homophilic%20%28%5Cie%20adjacent%20nodes%0Aare%20similar%29.%20In%20this%20paper%2C%20we%20introduce%20Lying-GCN%2C%20a%20new%20DGN%20inspired%20by%0Aopinion%20dynamics%20that%20can%20adaptively%20work%20in%20both%20the%20heterophilic%20and%20the%0Ahomophilic%20setting.%20At%20each%20layer%2C%20each%20agent%20%28node%29%20shares%20its%20own%20opinions%0A%28node%20embeddings%29%20with%20its%20neighbours.%20Instead%20of%20sharing%20its%20opinion%20directly%0Aas%20in%20GCN%2C%20we%20introduce%20a%20mechanism%20which%20allows%20agents%20to%20lie.%20Such%20a%0Amechanism%20is%20adaptive%2C%20thus%20the%20agents%20learn%20how%20and%20when%20to%20lie%20according%20to%0Athe%20task%20that%20should%20be%20solved.%20We%20provide%20a%20characterisation%20of%20our%20proposal%0Ain%20terms%20of%20dynamical%20systems%2C%20by%20studying%20the%20spectral%20property%20of%20the%0Acoefficient%20matrix%20of%20the%20system.%20While%20the%20steady%20state%20of%20the%20system%0Acollapses%20to%20zero%2C%20we%20believe%20the%20lying%20mechanism%20is%20still%20usable%20to%20solve%20node%0Aclassification%20tasks.%20We%20empirically%20prove%20our%20belief%20on%20both%20synthetic%20and%0Areal-world%20datasets%2C%20by%20showing%20that%20the%20lying%20mechanism%20allows%20to%20increase%20the%0Aperformances%20in%20the%20heterophilic%20setting%20without%20harming%20the%20results%20in%20the%0Ahomophilic%20one.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01247v1&entry.124074799=Read"},
{"title": "Explicitly Modeling Generality into Self-Supervised Learning", "author": "Jingyao Wang and Wenwen Qiang and Changwen Zheng", "abstract": "  The goal of generality in machine learning is to achieve excellent\nperformance on various unseen tasks and domains. Recently, self-supervised\nlearning (SSL) has been regarded as an effective method to achieve this goal.\nIt can learn high-quality representations from unlabeled data and achieve\npromising empirical performance on multiple downstream tasks. Existing SSL\nmethods mainly constrain generality from two aspects: (i) large-scale training\ndata, and (ii) learning task-level shared knowledge. However, these methods\nlack explicit modeling of the SSL generality in the learning objective, and the\ntheoretical understanding of SSL's generality remains limited. This may cause\nSSL models to overfit in data-scarce situations and generalize poorly in the\nreal world, making it difficult to achieve true generality. To address these\nissues, we provide a theoretical definition of generality in SSL and define a\n$\\sigma$-measurement to help quantify it. Based on this insight, we explicitly\nmodel generality into self-supervised learning and further propose a novel SSL\nframework, called GeSSL. It introduces a self-motivated target based on\n$\\sigma$-measurement, which enables the model to find the optimal update\ndirection towards generality. Extensive theoretical and empirical evaluations\ndemonstrate the superior performance of the proposed GeSSL.\n", "link": "http://arxiv.org/abs/2405.01053v1", "date": "2024-05-02", "relevancy": 2.4173, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4994}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4772}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4737}], "mailto": "mailto:daeR=997470421.yrtne&1v35010.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.LSSeG02%desoporp02%eht02%fo02%ecnamrofrep02%roirepus02%eht02%etartsnomedA0%snoitaulave02%laciripme02%dna02%laciteroeht02%evisnetxE02%.ytilareneg02%sdrawot02%noitceridA0%etadpu02%lamitpo02%eht02%dnif02%ot02%ledom02%eht02%selbane02%hcihw02%C2%tnemerusaem-42%amgisC5%42%A0%no02%desab02%tegrat02%detavitom-fles02%a02%secudortni02%tI02%.LSSeG02%dellac02%C2%krowemarfA0%LSS02%levon02%a02%esoporp02%rehtruf02%dna02%gninrael02%desivrepus-fles02%otni02%ytilareneg02%ledomA0%ylticilpxe02%ew02%C2%thgisni02%siht02%no02%desaB02%.ti02%yfitnauq02%pleh02%ot02%tnemerusaem-42%amgisC5%42%A0%a02%enifed02%dna02%LSS02%ni02%ytilareneg02%fo02%noitinifed02%laciteroeht02%a02%edivorp02%ew02%C2%seussiA0%eseht02%sserdda02%oT02%.ytilareneg02%eurt02%eveihca02%ot02%tluciffid02%ti02%gnikam02%C2%dlrow02%laerA0%eht02%ni02%ylroop02%ezilareneg02%dna02%snoitautis02%ecracs-atad02%ni02%tifrevo02%ot02%sledom02%LSSA0%esuac02%yam02%sihT02%.detimil02%sniamer02%ytilareneg02%s72%LSS02%fo02%gnidnatsrednu02%laciteroehtA0%eht02%dna02%C2%evitcejbo02%gninrael02%eht02%ni02%ytilareneg02%LSS02%eht02%fo02%gniledom02%ticilpxe02%kcalA0%sdohtem02%eseht02%C2%revewoH02%.egdelwonk02%derahs02%level-ksat02%gninrael02%92%ii82%02%dna02%C2%atadA0%gniniart02%elacs-egral02%92%i82%02%A3%stcepsa02%owt02%morf02%ytilareneg02%niartsnoc02%ylniam02%sdohtemA0%LSS02%gnitsixE02%.sksat02%maertsnwod02%elpitlum02%no02%ecnamrofrep02%laciripme02%gnisimorpA0%eveihca02%dna02%atad02%delebalnu02%morf02%snoitatneserper02%ytilauq-hgih02%nrael02%nac02%tIA0%.laog02%siht02%eveihca02%ot02%dohtem02%evitceffe02%na02%sa02%dedrager02%neeb02%sah02%92%LSS82%02%gninraelA0%desivrepus-fles02%C2%yltneceR02%.sniamod02%dna02%sksat02%neesnu02%suoirav02%no02%ecnamrofrepA0%tnellecxe02%eveihca02%ot02%si02%gninrael02%enihcam02%ni02%ytilareneg02%fo02%laog02%ehT02%02%=3328342921.yrtne&gnehZ02%newgnahC02%dna02%gnaiQ02%newneW02%dna02%gnaW02%oaygniJ=526535609.yrtne&gninraeL02%desivrepuS-fleS02%otni02%ytilareneG02%gniledoM02%ylticilpxE=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Explicitly%20Modeling%20Generality%20into%20Self-Supervised%20Learning&body=Title%3A%20Explicitly%20Modeling%20Generality%20into%20Self-Supervised%20Learning%0AAuthor%3A%20Jingyao%20Wang%20and%20Wenwen%20Qiang%20and%20Changwen%20Zheng%0AAbstract%3A%20%20%20The%20goal%20of%20generality%20in%20machine%20learning%20is%20to%20achieve%20excellent%0Aperformance%20on%20various%20unseen%20tasks%20and%20domains.%20Recently%2C%20self-supervised%0Alearning%20%28SSL%29%20has%20been%20regarded%20as%20an%20effective%20method%20to%20achieve%20this%20goal.%0AIt%20can%20learn%20high-quality%20representations%20from%20unlabeled%20data%20and%20achieve%0Apromising%20empirical%20performance%20on%20multiple%20downstream%20tasks.%20Existing%20SSL%0Amethods%20mainly%20constrain%20generality%20from%20two%20aspects%3A%20%28i%29%20large-scale%20training%0Adata%2C%20and%20%28ii%29%20learning%20task-level%20shared%20knowledge.%20However%2C%20these%20methods%0Alack%20explicit%20modeling%20of%20the%20SSL%20generality%20in%20the%20learning%20objective%2C%20and%20the%0Atheoretical%20understanding%20of%20SSL%27s%20generality%20remains%20limited.%20This%20may%20cause%0ASSL%20models%20to%20overfit%20in%20data-scarce%20situations%20and%20generalize%20poorly%20in%20the%0Areal%20world%2C%20making%20it%20difficult%20to%20achieve%20true%20generality.%20To%20address%20these%0Aissues%2C%20we%20provide%20a%20theoretical%20definition%20of%20generality%20in%20SSL%20and%20define%20a%0A%24%5Csigma%24-measurement%20to%20help%20quantify%20it.%20Based%20on%20this%20insight%2C%20we%20explicitly%0Amodel%20generality%20into%20self-supervised%20learning%20and%20further%20propose%20a%20novel%20SSL%0Aframework%2C%20called%20GeSSL.%20It%20introduces%20a%20self-motivated%20target%20based%20on%0A%24%5Csigma%24-measurement%2C%20which%20enables%20the%20model%20to%20find%20the%20optimal%20update%0Adirection%20towards%20generality.%20Extensive%20theoretical%20and%20empirical%20evaluations%0Ademonstrate%20the%20superior%20performance%20of%20the%20proposed%20GeSSL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01053v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicitly%20Modeling%20Generality%20into%20Self-Supervised%20Learning&entry.906535625=Jingyao%20Wang%20and%20Wenwen%20Qiang%20and%20Changwen%20Zheng&entry.1292438233=%20%20The%20goal%20of%20generality%20in%20machine%20learning%20is%20to%20achieve%20excellent%0Aperformance%20on%20various%20unseen%20tasks%20and%20domains.%20Recently%2C%20self-supervised%0Alearning%20%28SSL%29%20has%20been%20regarded%20as%20an%20effective%20method%20to%20achieve%20this%20goal.%0AIt%20can%20learn%20high-quality%20representations%20from%20unlabeled%20data%20and%20achieve%0Apromising%20empirical%20performance%20on%20multiple%20downstream%20tasks.%20Existing%20SSL%0Amethods%20mainly%20constrain%20generality%20from%20two%20aspects%3A%20%28i%29%20large-scale%20training%0Adata%2C%20and%20%28ii%29%20learning%20task-level%20shared%20knowledge.%20However%2C%20these%20methods%0Alack%20explicit%20modeling%20of%20the%20SSL%20generality%20in%20the%20learning%20objective%2C%20and%20the%0Atheoretical%20understanding%20of%20SSL%27s%20generality%20remains%20limited.%20This%20may%20cause%0ASSL%20models%20to%20overfit%20in%20data-scarce%20situations%20and%20generalize%20poorly%20in%20the%0Areal%20world%2C%20making%20it%20difficult%20to%20achieve%20true%20generality.%20To%20address%20these%0Aissues%2C%20we%20provide%20a%20theoretical%20definition%20of%20generality%20in%20SSL%20and%20define%20a%0A%24%5Csigma%24-measurement%20to%20help%20quantify%20it.%20Based%20on%20this%20insight%2C%20we%20explicitly%0Amodel%20generality%20into%20self-supervised%20learning%20and%20further%20propose%20a%20novel%20SSL%0Aframework%2C%20called%20GeSSL.%20It%20introduces%20a%20self-motivated%20target%20based%20on%0A%24%5Csigma%24-measurement%2C%20which%20enables%20the%20model%20to%20find%20the%20optimal%20update%0Adirection%20towards%20generality.%20Extensive%20theoretical%20and%20empirical%20evaluations%0Ademonstrate%20the%20superior%20performance%20of%20the%20proposed%20GeSSL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01053v1&entry.124074799=Read"},
{"title": "WEST GCN-LSTM: Weighted Stacked Spatio-Temporal Graph Neural Networks\n  for Regional Traffic Forecasting", "author": "Theodoros Theodoropoulos and Angelos-Christos Maroudis and Antonios Makris and Konstantinos Tserpes", "abstract": "  Regional traffic forecasting is a critical challenge in urban mobility, with\napplications to various fields such as the Internet of Everything. In recent\nyears, spatio-temporal graph neural networks have achieved state-of-the-art\nresults in the context of numerous traffic forecasting challenges. This work\naims at expanding upon the conventional spatio-temporal graph neural network\narchitectures in a manner that may facilitate the inclusion of information\nregarding the examined regions, as well as the populations that traverse them,\nin order to establish a more efficient prediction model. The end-product of\nthis scientific endeavour is a novel spatio-temporal graph neural network\narchitecture that is referred to as WEST (WEighted STacked) GCN-LSTM.\nFurthermore, the inclusion of the aforementioned information is conducted via\nthe use of two novel dedicated algorithms that are referred to as the Shared\nBorders Policy and the Adjustable Hops Policy. Through information fusion and\ndistillation, the proposed solution manages to significantly outperform its\ncompetitors in the frame of an experimental evaluation that consists of 19\nforecasting models, across several datasets. Finally, an additional ablation\nstudy determined that each of the components of the proposed solution\ncontributes towards enhancing its overall performance.\n", "link": "http://arxiv.org/abs/2405.00570v1", "date": "2024-05-01", "relevancy": 2.4146, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4924}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4804}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.476}], "mailto": "mailto:daeR=997470421.yrtne&1v07500.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.ecnamrofrep02%llarevo02%sti02%gnicnahne02%sdrawot02%setubirtnocA0%noitulos02%desoporp02%eht02%fo02%stnenopmoc02%eht02%fo02%hcae02%taht02%denimreted02%ydutsA0%noitalba02%lanoitidda02%na02%C2%yllaniF02%.stesatad02%lareves02%ssorca02%C2%sledom02%gnitsacerofA0%9102%fo02%stsisnoc02%taht02%noitaulave02%latnemirepxe02%na02%fo02%emarf02%eht02%ni02%srotitepmocA0%sti02%mrofreptuo02%yltnacifingis02%ot02%seganam02%noitulos02%desoporp02%eht02%C2%noitallitsidA0%dna02%noisuf02%noitamrofni02%hguorhT02%.yciloP02%spoH02%elbatsujdA02%eht02%dna02%yciloP02%sredroBA0%derahS02%eht02%sa02%ot02%derrefer02%era02%taht02%smhtirogla02%detacided02%levon02%owt02%fo02%esu02%ehtA0%aiv02%detcudnoc02%si02%noitamrofni02%denoitnemerofa02%eht02%fo02%noisulcni02%eht02%C2%eromrehtruFA0%.MTSL-NCG02%92%dekcaTS02%dethgiEW82%02%TSEW02%sa02%ot02%derrefer02%si02%taht02%erutcetihcraA0%krowten02%laruen02%hparg02%laropmet-oitaps02%levon02%a02%si02%ruovaedne02%cifitneics02%sihtA0%fo02%tcudorp-dne02%ehT02%.ledom02%noitciderp02%tneiciffe02%erom02%a02%hsilbatse02%ot02%redro02%niA0%C2%meht02%esrevart02%taht02%snoitalupop02%eht02%sa02%llew02%sa02%C2%snoiger02%denimaxe02%eht02%gnidragerA0%noitamrofni02%fo02%noisulcni02%eht02%etatilicaf02%yam02%taht02%rennam02%a02%ni02%serutcetihcraA0%krowten02%laruen02%hparg02%laropmet-oitaps02%lanoitnevnoc02%eht02%nopu02%gnidnapxe02%ta02%smiaA0%krow02%sihT02%.segnellahc02%gnitsacerof02%ciffart02%suoremun02%fo02%txetnoc02%eht02%ni02%stluserA0%tra-eht-fo-etats02%deveihca02%evah02%skrowten02%laruen02%hparg02%laropmet-oitaps02%C2%sraeyA0%tnecer02%nI02%.gnihtyrevE02%fo02%tenretnI02%eht02%sa02%hcus02%sdleif02%suoirav02%ot02%snoitacilppaA0%htiw02%C2%ytilibom02%nabru02%ni02%egnellahc02%lacitirc02%a02%si02%gnitsacerof02%ciffart02%lanoigeR02%02%=3328342921.yrtne&sepresT02%sonitnatsnoK02%dna02%sirkaM02%soinotnA02%dna02%siduoraM02%sotsirhC-solegnA02%dna02%soluoporodoehT02%sorodoehT=526535609.yrtne&gnitsaceroF02%ciffarT02%lanoigeR02%rof02%02%A0%skrowteN02%larueN02%hparG02%laropmeT-oitapS02%dekcatS02%dethgieW02%A3%MTSL-NCG02%TSEW=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20WEST%20GCN-LSTM%3A%20Weighted%20Stacked%20Spatio-Temporal%20Graph%20Neural%20Networks%0A%20%20for%20Regional%20Traffic%20Forecasting&body=Title%3A%20WEST%20GCN-LSTM%3A%20Weighted%20Stacked%20Spatio-Temporal%20Graph%20Neural%20Networks%0A%20%20for%20Regional%20Traffic%20Forecasting%0AAuthor%3A%20Theodoros%20Theodoropoulos%20and%20Angelos-Christos%20Maroudis%20and%20Antonios%20Makris%20and%20Konstantinos%20Tserpes%0AAbstract%3A%20%20%20Regional%20traffic%20forecasting%20is%20a%20critical%20challenge%20in%20urban%20mobility%2C%20with%0Aapplications%20to%20various%20fields%20such%20as%20the%20Internet%20of%20Everything.%20In%20recent%0Ayears%2C%20spatio-temporal%20graph%20neural%20networks%20have%20achieved%20state-of-the-art%0Aresults%20in%20the%20context%20of%20numerous%20traffic%20forecasting%20challenges.%20This%20work%0Aaims%20at%20expanding%20upon%20the%20conventional%20spatio-temporal%20graph%20neural%20network%0Aarchitectures%20in%20a%20manner%20that%20may%20facilitate%20the%20inclusion%20of%20information%0Aregarding%20the%20examined%20regions%2C%20as%20well%20as%20the%20populations%20that%20traverse%20them%2C%0Ain%20order%20to%20establish%20a%20more%20efficient%20prediction%20model.%20The%20end-product%20of%0Athis%20scientific%20endeavour%20is%20a%20novel%20spatio-temporal%20graph%20neural%20network%0Aarchitecture%20that%20is%20referred%20to%20as%20WEST%20%28WEighted%20STacked%29%20GCN-LSTM.%0AFurthermore%2C%20the%20inclusion%20of%20the%20aforementioned%20information%20is%20conducted%20via%0Athe%20use%20of%20two%20novel%20dedicated%20algorithms%20that%20are%20referred%20to%20as%20the%20Shared%0ABorders%20Policy%20and%20the%20Adjustable%20Hops%20Policy.%20Through%20information%20fusion%20and%0Adistillation%2C%20the%20proposed%20solution%20manages%20to%20significantly%20outperform%20its%0Acompetitors%20in%20the%20frame%20of%20an%20experimental%20evaluation%20that%20consists%20of%2019%0Aforecasting%20models%2C%20across%20several%20datasets.%20Finally%2C%20an%20additional%20ablation%0Astudy%20determined%20that%20each%20of%20the%20components%20of%20the%20proposed%20solution%0Acontributes%20towards%20enhancing%20its%20overall%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00570v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WEST%20GCN-LSTM%3A%20Weighted%20Stacked%20Spatio-Temporal%20Graph%20Neural%20Networks%0A%20%20for%20Regional%20Traffic%20Forecasting&entry.906535625=Theodoros%20Theodoropoulos%20and%20Angelos-Christos%20Maroudis%20and%20Antonios%20Makris%20and%20Konstantinos%20Tserpes&entry.1292438233=%20%20Regional%20traffic%20forecasting%20is%20a%20critical%20challenge%20in%20urban%20mobility%2C%20with%0Aapplications%20to%20various%20fields%20such%20as%20the%20Internet%20of%20Everything.%20In%20recent%0Ayears%2C%20spatio-temporal%20graph%20neural%20networks%20have%20achieved%20state-of-the-art%0Aresults%20in%20the%20context%20of%20numerous%20traffic%20forecasting%20challenges.%20This%20work%0Aaims%20at%20expanding%20upon%20the%20conventional%20spatio-temporal%20graph%20neural%20network%0Aarchitectures%20in%20a%20manner%20that%20may%20facilitate%20the%20inclusion%20of%20information%0Aregarding%20the%20examined%20regions%2C%20as%20well%20as%20the%20populations%20that%20traverse%20them%2C%0Ain%20order%20to%20establish%20a%20more%20efficient%20prediction%20model.%20The%20end-product%20of%0Athis%20scientific%20endeavour%20is%20a%20novel%20spatio-temporal%20graph%20neural%20network%0Aarchitecture%20that%20is%20referred%20to%20as%20WEST%20%28WEighted%20STacked%29%20GCN-LSTM.%0AFurthermore%2C%20the%20inclusion%20of%20the%20aforementioned%20information%20is%20conducted%20via%0Athe%20use%20of%20two%20novel%20dedicated%20algorithms%20that%20are%20referred%20to%20as%20the%20Shared%0ABorders%20Policy%20and%20the%20Adjustable%20Hops%20Policy.%20Through%20information%20fusion%20and%0Adistillation%2C%20the%20proposed%20solution%20manages%20to%20significantly%20outperform%20its%0Acompetitors%20in%20the%20frame%20of%20an%20experimental%20evaluation%20that%20consists%20of%2019%0Aforecasting%20models%2C%20across%20several%20datasets.%20Finally%2C%20an%20additional%20ablation%0Astudy%20determined%20that%20each%20of%20the%20components%20of%20the%20proposed%20solution%0Acontributes%20towards%20enhancing%20its%20overall%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00570v1&entry.124074799=Read"},
{"title": "Spectrally Pruned Gaussian Fields with Neural Compensation", "author": "Runyi Yang and Zhenxin Zhu and Zhou Jiang and Baijun Ye and Xiaoxue Chen and Yifei Zhang and Yuantao Chen and Jian Zhao and Hao Zhao", "abstract": "  Recently, 3D Gaussian Splatting, as a novel 3D representation, has garnered\nattention for its fast rendering speed and high rendering quality. However,\nthis comes with high memory consumption, e.g., a well-trained Gaussian field\nmay utilize three million Gaussian primitives and over 700 MB of memory. We\ncredit this high memory footprint to the lack of consideration for the\nrelationship between primitives. In this paper, we propose a memory-efficient\nGaussian field named SUNDAE with spectral pruning and neural compensation. On\none hand, we construct a graph on the set of Gaussian primitives to model their\nrelationship and design a spectral down-sampling module to prune out primitives\nwhile preserving desired signals. On the other hand, to compensate for the\nquality loss of pruning Gaussians, we exploit a lightweight neural network head\nto mix splatted features, which effectively compensates for quality losses\nwhile capturing the relationship between primitives in its weights. We\ndemonstrate the performance of SUNDAE with extensive results. For example,\nSUNDAE can achieve 26.80 PSNR at 145 FPS using 104 MB memory while the vanilla\nGaussian splatting algorithm achieves 25.60 PSNR at 160 FPS using 523 MB\nmemory, on the Mip-NeRF360 dataset. Codes are publicly available at\nhttps://runyiyang.github.io/projects/SUNDAE/.\n", "link": "http://arxiv.org/abs/2405.00676v1", "date": "2024-05-01", "relevancy": 2.4114, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6464}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6036}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4919}], "mailto": "mailto:daeR=997470421.yrtne&1v67600.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%./EADNUS/stcejorp/oi.buhtig.gnayiynur//A3%sptthA0%ta02%elbaliava02%ylcilbup02%era02%sedoC02%.tesatad02%063FReN-piM02%eht02%no02%C2%yromemA0%BM02%32502%gnisu02%SPF02%06102%ta02%RNSP02%06.5202%seveihca02%mhtirogla02%gnittalps02%naissuaGA0%allinav02%eht02%elihw02%yromem02%BM02%40102%gnisu02%SPF02%54102%ta02%RNSP02%08.6202%eveihca02%nac02%EADNUSA0%C2%elpmaxe02%roF02%.stluser02%evisnetxe02%htiw02%EADNUS02%fo02%ecnamrofrep02%eht02%etartsnomedA0%eW02%.sthgiew02%sti02%ni02%sevitimirp02%neewteb02%pihsnoitaler02%eht02%gnirutpac02%elihwA0%sessol02%ytilauq02%rof02%setasnepmoc02%ylevitceffe02%hcihw02%C2%serutaef02%dettalps02%xim02%otA0%daeh02%krowten02%laruen02%thgiewthgil02%a02%tiolpxe02%ew02%C2%snaissuaG02%gninurp02%fo02%ssol02%ytilauqA0%eht02%rof02%etasnepmoc02%ot02%C2%dnah02%rehto02%eht02%nO02%.slangis02%derised02%gnivreserp02%elihwA0%sevitimirp02%tuo02%enurp02%ot02%eludom02%gnilpmas-nwod02%lartceps02%a02%ngised02%dna02%pihsnoitalerA0%rieht02%ledom02%ot02%sevitimirp02%naissuaG02%fo02%tes02%eht02%no02%hparg02%a02%tcurtsnoc02%ew02%C2%dnah02%enoA0%nO02%.noitasnepmoc02%laruen02%dna02%gninurp02%lartceps02%htiw02%EADNUS02%deman02%dleif02%naissuaGA0%tneiciffe-yromem02%a02%esoporp02%ew02%C2%repap02%siht02%nI02%.sevitimirp02%neewteb02%pihsnoitalerA0%eht02%rof02%noitaredisnoc02%fo02%kcal02%eht02%ot02%tnirptoof02%yromem02%hgih02%siht02%tidercA0%eW02%.yromem02%fo02%BM02%00702%revo02%dna02%sevitimirp02%naissuaG02%noillim02%eerht02%ezilitu02%yamA0%dleif02%naissuaG02%deniart-llew02%a02%C2%.g.e02%C2%noitpmusnoc02%yromem02%hgih02%htiw02%semoc02%sihtA0%C2%revewoH02%.ytilauq02%gniredner02%hgih02%dna02%deeps02%gniredner02%tsaf02%sti02%rof02%noitnettaA0%derenrag02%sah02%C2%noitatneserper02%D302%levon02%a02%sa02%C2%gnittalpS02%naissuaG02%D302%C2%yltneceR02%02%=3328342921.yrtne&oahZ02%oaH02%dna02%oahZ02%naiJ02%dna02%nehC02%oatnauY02%dna02%gnahZ02%iefiY02%dna02%nehC02%euxoaiX02%dna02%eY02%nujiaB02%dna02%gnaiJ02%uohZ02%dna02%uhZ02%nixnehZ02%dna02%gnaY02%iynuR=526535609.yrtne&noitasnepmoC02%larueN02%htiw02%sdleiF02%naissuaG02%denurP02%yllartcepS=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Spectrally%20Pruned%20Gaussian%20Fields%20with%20Neural%20Compensation&body=Title%3A%20Spectrally%20Pruned%20Gaussian%20Fields%20with%20Neural%20Compensation%0AAuthor%3A%20Runyi%20Yang%20and%20Zhenxin%20Zhu%20and%20Zhou%20Jiang%20and%20Baijun%20Ye%20and%20Xiaoxue%20Chen%20and%20Yifei%20Zhang%20and%20Yuantao%20Chen%20and%20Jian%20Zhao%20and%20Hao%20Zhao%0AAbstract%3A%20%20%20Recently%2C%203D%20Gaussian%20Splatting%2C%20as%20a%20novel%203D%20representation%2C%20has%20garnered%0Aattention%20for%20its%20fast%20rendering%20speed%20and%20high%20rendering%20quality.%20However%2C%0Athis%20comes%20with%20high%20memory%20consumption%2C%20e.g.%2C%20a%20well-trained%20Gaussian%20field%0Amay%20utilize%20three%20million%20Gaussian%20primitives%20and%20over%20700%20MB%20of%20memory.%20We%0Acredit%20this%20high%20memory%20footprint%20to%20the%20lack%20of%20consideration%20for%20the%0Arelationship%20between%20primitives.%20In%20this%20paper%2C%20we%20propose%20a%20memory-efficient%0AGaussian%20field%20named%20SUNDAE%20with%20spectral%20pruning%20and%20neural%20compensation.%20On%0Aone%20hand%2C%20we%20construct%20a%20graph%20on%20the%20set%20of%20Gaussian%20primitives%20to%20model%20their%0Arelationship%20and%20design%20a%20spectral%20down-sampling%20module%20to%20prune%20out%20primitives%0Awhile%20preserving%20desired%20signals.%20On%20the%20other%20hand%2C%20to%20compensate%20for%20the%0Aquality%20loss%20of%20pruning%20Gaussians%2C%20we%20exploit%20a%20lightweight%20neural%20network%20head%0Ato%20mix%20splatted%20features%2C%20which%20effectively%20compensates%20for%20quality%20losses%0Awhile%20capturing%20the%20relationship%20between%20primitives%20in%20its%20weights.%20We%0Ademonstrate%20the%20performance%20of%20SUNDAE%20with%20extensive%20results.%20For%20example%2C%0ASUNDAE%20can%20achieve%2026.80%20PSNR%20at%20145%20FPS%20using%20104%20MB%20memory%20while%20the%20vanilla%0AGaussian%20splatting%20algorithm%20achieves%2025.60%20PSNR%20at%20160%20FPS%20using%20523%20MB%0Amemory%2C%20on%20the%20Mip-NeRF360%20dataset.%20Codes%20are%20publicly%20available%20at%0Ahttps%3A//runyiyang.github.io/projects/SUNDAE/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00676v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectrally%20Pruned%20Gaussian%20Fields%20with%20Neural%20Compensation&entry.906535625=Runyi%20Yang%20and%20Zhenxin%20Zhu%20and%20Zhou%20Jiang%20and%20Baijun%20Ye%20and%20Xiaoxue%20Chen%20and%20Yifei%20Zhang%20and%20Yuantao%20Chen%20and%20Jian%20Zhao%20and%20Hao%20Zhao&entry.1292438233=%20%20Recently%2C%203D%20Gaussian%20Splatting%2C%20as%20a%20novel%203D%20representation%2C%20has%20garnered%0Aattention%20for%20its%20fast%20rendering%20speed%20and%20high%20rendering%20quality.%20However%2C%0Athis%20comes%20with%20high%20memory%20consumption%2C%20e.g.%2C%20a%20well-trained%20Gaussian%20field%0Amay%20utilize%20three%20million%20Gaussian%20primitives%20and%20over%20700%20MB%20of%20memory.%20We%0Acredit%20this%20high%20memory%20footprint%20to%20the%20lack%20of%20consideration%20for%20the%0Arelationship%20between%20primitives.%20In%20this%20paper%2C%20we%20propose%20a%20memory-efficient%0AGaussian%20field%20named%20SUNDAE%20with%20spectral%20pruning%20and%20neural%20compensation.%20On%0Aone%20hand%2C%20we%20construct%20a%20graph%20on%20the%20set%20of%20Gaussian%20primitives%20to%20model%20their%0Arelationship%20and%20design%20a%20spectral%20down-sampling%20module%20to%20prune%20out%20primitives%0Awhile%20preserving%20desired%20signals.%20On%20the%20other%20hand%2C%20to%20compensate%20for%20the%0Aquality%20loss%20of%20pruning%20Gaussians%2C%20we%20exploit%20a%20lightweight%20neural%20network%20head%0Ato%20mix%20splatted%20features%2C%20which%20effectively%20compensates%20for%20quality%20losses%0Awhile%20capturing%20the%20relationship%20between%20primitives%20in%20its%20weights.%20We%0Ademonstrate%20the%20performance%20of%20SUNDAE%20with%20extensive%20results.%20For%20example%2C%0ASUNDAE%20can%20achieve%2026.80%20PSNR%20at%20145%20FPS%20using%20104%20MB%20memory%20while%20the%20vanilla%0AGaussian%20splatting%20algorithm%20achieves%2025.60%20PSNR%20at%20160%20FPS%20using%20523%20MB%0Amemory%2C%20on%20the%20Mip-NeRF360%20dataset.%20Codes%20are%20publicly%20available%20at%0Ahttps%3A//runyiyang.github.io/projects/SUNDAE/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00676v1&entry.124074799=Read"},
{"title": "Learning-to-solve unit commitment based on few-shot physics-guided\n  spatial-temporal graph convolution network", "author": "Mei Yang and Gao Qiu andJunyong Liu and Kai Liu", "abstract": "  This letter proposes a few-shot physics-guided spatial temporal graph\nconvolutional network (FPG-STGCN) to fast solve unit commitment (UC). Firstly,\nSTGCN is tailored to parameterize UC. Then, few-shot physics-guided learning\nscheme is proposed. It exploits few typical UC solutions yielded via commercial\noptimizer to escape from local minimum, and leverages the augmented Lagrangian\nmethod for constraint satisfaction. To further enable both feasibility and\ncontinuous relaxation for integers in learning process, straight-through\nestimator for Tanh-Sign composition is proposed to fully differentiate the\nmixed integer solution space. Case study on the IEEE benchmark justifies that,\nour method bests mainstream learning ways on UC feasibility, and surpasses\ntraditional solver on efficiency.\n", "link": "http://arxiv.org/abs/2405.01200v1", "date": "2024-05-02", "relevancy": 2.4109, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4866}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4836}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4763}], "mailto": "mailto:daeR=997470421.yrtne&1v00210.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.ycneiciffe02%no02%revlos02%lanoitidartA0%sessaprus02%dna02%C2%ytilibisaef02%CU02%no02%syaw02%gninrael02%maertsniam02%stseb02%dohtem02%ruoA0%C2%taht02%seifitsuj02%kramhcneb02%EEEI02%eht02%no02%yduts02%esaC02%.ecaps02%noitulos02%regetni02%deximA0%eht02%etaitnereffid02%ylluf02%ot02%desoporp02%si02%noitisopmoc02%ngiS-hnaT02%rof02%rotamitseA0%hguorht-thgiarts02%C2%ssecorp02%gninrael02%ni02%sregetni02%rof02%noitaxaler02%suounitnocA0%dna02%ytilibisaef02%htob02%elbane02%rehtruf02%oT02%.noitcafsitas02%tniartsnoc02%rof02%dohtemA0%naignargaL02%detnemgua02%eht02%segarevel02%dna02%C2%muminim02%lacol02%morf02%epacse02%ot02%rezimitpoA0%laicremmoc02%aiv02%dedleiy02%snoitulos02%CU02%lacipyt02%wef02%stiolpxe02%tI02%.desoporp02%si02%emehcsA0%gninrael02%dediug-scisyhp02%tohs-wef02%C2%nehT02%.CU02%eziretemarap02%ot02%deroliat02%si02%NCGTSA0%C2%yltsriF02%.92%CU82%02%tnemtimmoc02%tinu02%evlos02%tsaf02%ot02%92%NCGTS-GPF82%02%krowten02%lanoitulovnocA0%hparg02%laropmet02%laitaps02%dediug-scisyhp02%tohs-wef02%a02%sesoporp02%rettel02%sihT02%02%=3328342921.yrtne&uiL02%iaK02%dna02%uiL02%gnoynuJdna02%uiQ02%oaG02%dna02%gnaY02%ieM=526535609.yrtne&krowten02%noitulovnoc02%hparg02%laropmet-laitaps02%02%A0%dediug-scisyhp02%tohs-wef02%no02%desab02%tnemtimmoc02%tinu02%evlos-ot-gninraeL=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Learning-to-solve%20unit%20commitment%20based%20on%20few-shot%20physics-guided%0A%20%20spatial-temporal%20graph%20convolution%20network&body=Title%3A%20Learning-to-solve%20unit%20commitment%20based%20on%20few-shot%20physics-guided%0A%20%20spatial-temporal%20graph%20convolution%20network%0AAuthor%3A%20Mei%20Yang%20and%20Gao%20Qiu%20andJunyong%20Liu%20and%20Kai%20Liu%0AAbstract%3A%20%20%20This%20letter%20proposes%20a%20few-shot%20physics-guided%20spatial%20temporal%20graph%0Aconvolutional%20network%20%28FPG-STGCN%29%20to%20fast%20solve%20unit%20commitment%20%28UC%29.%20Firstly%2C%0ASTGCN%20is%20tailored%20to%20parameterize%20UC.%20Then%2C%20few-shot%20physics-guided%20learning%0Ascheme%20is%20proposed.%20It%20exploits%20few%20typical%20UC%20solutions%20yielded%20via%20commercial%0Aoptimizer%20to%20escape%20from%20local%20minimum%2C%20and%20leverages%20the%20augmented%20Lagrangian%0Amethod%20for%20constraint%20satisfaction.%20To%20further%20enable%20both%20feasibility%20and%0Acontinuous%20relaxation%20for%20integers%20in%20learning%20process%2C%20straight-through%0Aestimator%20for%20Tanh-Sign%20composition%20is%20proposed%20to%20fully%20differentiate%20the%0Amixed%20integer%20solution%20space.%20Case%20study%20on%20the%20IEEE%20benchmark%20justifies%20that%2C%0Aour%20method%20bests%20mainstream%20learning%20ways%20on%20UC%20feasibility%2C%20and%20surpasses%0Atraditional%20solver%20on%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01200v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning-to-solve%20unit%20commitment%20based%20on%20few-shot%20physics-guided%0A%20%20spatial-temporal%20graph%20convolution%20network&entry.906535625=Mei%20Yang%20and%20Gao%20Qiu%20andJunyong%20Liu%20and%20Kai%20Liu&entry.1292438233=%20%20This%20letter%20proposes%20a%20few-shot%20physics-guided%20spatial%20temporal%20graph%0Aconvolutional%20network%20%28FPG-STGCN%29%20to%20fast%20solve%20unit%20commitment%20%28UC%29.%20Firstly%2C%0ASTGCN%20is%20tailored%20to%20parameterize%20UC.%20Then%2C%20few-shot%20physics-guided%20learning%0Ascheme%20is%20proposed.%20It%20exploits%20few%20typical%20UC%20solutions%20yielded%20via%20commercial%0Aoptimizer%20to%20escape%20from%20local%20minimum%2C%20and%20leverages%20the%20augmented%20Lagrangian%0Amethod%20for%20constraint%20satisfaction.%20To%20further%20enable%20both%20feasibility%20and%0Acontinuous%20relaxation%20for%20integers%20in%20learning%20process%2C%20straight-through%0Aestimator%20for%20Tanh-Sign%20composition%20is%20proposed%20to%20fully%20differentiate%20the%0Amixed%20integer%20solution%20space.%20Case%20study%20on%20the%20IEEE%20benchmark%20justifies%20that%2C%0Aour%20method%20bests%20mainstream%20learning%20ways%20on%20UC%20feasibility%2C%20and%20surpasses%0Atraditional%20solver%20on%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01200v1&entry.124074799=Read"},
{"title": "Improving Subgraph-GNNs via Edge-Level Ego-Network Encodings", "author": "Nurudin Alvarez-Gonzalez and Andreas Kaltenbrunner and Vicen\u00e7 G\u00f3mez", "abstract": "  We present a novel edge-level ego-network encoding for learning on graphs\nthat can boost Message Passing Graph Neural Networks (MP-GNNs) by providing\nadditional node and edge features or extending message-passing formats. The\nproposed encoding is sufficient to distinguish Strongly Regular Graphs, a\nfamily of challenging 3-WL equivalent graphs. We show theoretically that such\nencoding is more expressive than node-based sub-graph MP-GNNs. In an empirical\nevaluation on four benchmarks with 10 graph datasets, our results match or\nimprove previous baselines on expressivity, graph classification, graph\nregression, and proximity tasks -- while reducing memory usage by 18.1x in\ncertain real-world settings.\n", "link": "http://arxiv.org/abs/2312.05905v2", "date": "2024-05-02", "relevancy": 2.4074, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5203}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4745}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4495}], "mailto": "mailto:daeR=997470421.yrtne&2v50950.2132/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.sgnittes02%dlrow-laer02%niatrecA0%ni02%x1.8102%yb02%egasu02%yromem02%gnicuder02%elihw02%--02%sksat02%ytimixorp02%dna02%C2%noissergerA0%hparg02%C2%noitacifissalc02%hparg02%C2%ytivisserpxe02%no02%senilesab02%suoiverp02%evorpmiA0%ro02%hctam02%stluser02%ruo02%C2%stesatad02%hparg02%0102%htiw02%skramhcneb02%ruof02%no02%noitaulaveA0%laciripme02%na02%nI02%.sNNG-PM02%hparg-bus02%desab-edon02%naht02%evisserpxe02%erom02%si02%gnidocneA0%hcus02%taht02%yllaciteroeht02%wohs02%eW02%.shparg02%tnelaviuqe02%LW-302%gnignellahc02%fo02%ylimafA0%a02%C2%shparG02%ralugeR02%ylgnortS02%hsiugnitsid02%ot02%tneiciffus02%si02%gnidocne02%desoporpA0%ehT02%.stamrof02%gnissap-egassem02%gnidnetxe02%ro02%serutaef02%egde02%dna02%edon02%lanoitiddaA0%gnidivorp02%yb02%92%sNNG-PM82%02%skrowteN02%larueN02%hparG02%gnissaP02%egasseM02%tsoob02%nac02%tahtA0%shparg02%no02%gninrael02%rof02%gnidocne02%krowten-oge02%level-egde02%levon02%a02%tneserp02%eW02%02%=3328342921.yrtne&zem3B%3C%G02%7A%3C%neciV02%dna02%rennurbnetlaK02%saerdnA02%dna02%zelaznoG-zeravlA02%niduruN=526535609.yrtne&sgnidocnE02%krowteN-ogE02%leveL-egdE02%aiv02%sNNG-hpargbuS02%gnivorpmI=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Improving%20Subgraph-GNNs%20via%20Edge-Level%20Ego-Network%20Encodings&body=Title%3A%20Improving%20Subgraph-GNNs%20via%20Edge-Level%20Ego-Network%20Encodings%0AAuthor%3A%20Nurudin%20Alvarez-Gonzalez%20and%20Andreas%20Kaltenbrunner%20and%20Vicen%C3%A7%20G%C3%B3mez%0AAbstract%3A%20%20%20We%20present%20a%20novel%20edge-level%20ego-network%20encoding%20for%20learning%20on%20graphs%0Athat%20can%20boost%20Message%20Passing%20Graph%20Neural%20Networks%20%28MP-GNNs%29%20by%20providing%0Aadditional%20node%20and%20edge%20features%20or%20extending%20message-passing%20formats.%20The%0Aproposed%20encoding%20is%20sufficient%20to%20distinguish%20Strongly%20Regular%20Graphs%2C%20a%0Afamily%20of%20challenging%203-WL%20equivalent%20graphs.%20We%20show%20theoretically%20that%20such%0Aencoding%20is%20more%20expressive%20than%20node-based%20sub-graph%20MP-GNNs.%20In%20an%20empirical%0Aevaluation%20on%20four%20benchmarks%20with%2010%20graph%20datasets%2C%20our%20results%20match%20or%0Aimprove%20previous%20baselines%20on%20expressivity%2C%20graph%20classification%2C%20graph%0Aregression%2C%20and%20proximity%20tasks%20--%20while%20reducing%20memory%20usage%20by%2018.1x%20in%0Acertain%20real-world%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05905v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Subgraph-GNNs%20via%20Edge-Level%20Ego-Network%20Encodings&entry.906535625=Nurudin%20Alvarez-Gonzalez%20and%20Andreas%20Kaltenbrunner%20and%20Vicen%C3%A7%20G%C3%B3mez&entry.1292438233=%20%20We%20present%20a%20novel%20edge-level%20ego-network%20encoding%20for%20learning%20on%20graphs%0Athat%20can%20boost%20Message%20Passing%20Graph%20Neural%20Networks%20%28MP-GNNs%29%20by%20providing%0Aadditional%20node%20and%20edge%20features%20or%20extending%20message-passing%20formats.%20The%0Aproposed%20encoding%20is%20sufficient%20to%20distinguish%20Strongly%20Regular%20Graphs%2C%20a%0Afamily%20of%20challenging%203-WL%20equivalent%20graphs.%20We%20show%20theoretically%20that%20such%0Aencoding%20is%20more%20expressive%20than%20node-based%20sub-graph%20MP-GNNs.%20In%20an%20empirical%0Aevaluation%20on%20four%20benchmarks%20with%2010%20graph%20datasets%2C%20our%20results%20match%20or%0Aimprove%20previous%20baselines%20on%20expressivity%2C%20graph%20classification%2C%20graph%0Aregression%2C%20and%20proximity%20tasks%20--%20while%20reducing%20memory%20usage%20by%2018.1x%20in%0Acertain%20real-world%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05905v2&entry.124074799=Read"},
{"title": "X-Oscar: A Progressive Framework for High-quality Text-guided 3D\n  Animatable Avatar Generation", "author": "Yiwei Ma and Zhekai Lin and Jiayi Ji and Yijun Fan and Xiaoshuai Sun and Rongrong Ji", "abstract": "  Recent advancements in automatic 3D avatar generation guided by text have\nmade significant progress. However, existing methods have limitations such as\noversaturation and low-quality output. To address these challenges, we propose\nX-Oscar, a progressive framework for generating high-quality animatable avatars\nfrom text prompts. It follows a sequential Geometry->Texture->Animation\nparadigm, simplifying optimization through step-by-step generation. To tackle\noversaturation, we introduce Adaptive Variational Parameter (AVP), representing\navatars as an adaptive distribution during training. Additionally, we present\nAvatar-aware Score Distillation Sampling (ASDS), a novel technique that\nincorporates avatar-aware noise into rendered images for improved generation\nquality during optimization. Extensive evaluations confirm the superiority of\nX-Oscar over existing text-to-3D and text-to-avatar approaches. Our anonymous\nproject page: https://xmu-xiaoma666.github.io/Projects/X-Oscar/.\n", "link": "http://arxiv.org/abs/2405.00954v1", "date": "2024-05-02", "relevancy": 2.3969, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.6157}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5891}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5836}], "mailto": "mailto:daeR=997470421.yrtne&1v45900.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%./racsO-X/stcejorP/oi.buhtig.666amoaix-umx//A3%sptth02%A3%egap02%tcejorpA0%suomynona02%ruO02%.sehcaorppa02%ratava-ot-txet02%dna02%D3-ot-txet02%gnitsixe02%revo02%racsO-XA0%fo02%ytiroirepus02%eht02%mrifnoc02%snoitaulave02%evisnetxE02%.noitazimitpo02%gnirud02%ytilauqA0%noitareneg02%devorpmi02%rof02%segami02%deredner02%otni02%esion02%erawa-ratava02%setaroprocniA0%taht02%euqinhcet02%levon02%a02%C2%92%SDSA82%02%gnilpmaS02%noitallitsiD02%erocS02%erawa-ratavAA0%tneserp02%ew02%C2%yllanoitiddA02%.gniniart02%gnirud02%noitubirtsid02%evitpada02%na02%sa02%sratavaA0%gnitneserper02%C2%92%PVA82%02%retemaraP02%lanoitairaV02%evitpadA02%ecudortni02%ew02%C2%noitarutasrevoA0%elkcat02%oT02%.noitareneg02%pets-yb-pets02%hguorht02%noitazimitpo02%gniyfilpmis02%C2%mgidarapA0%noitaminAE3%-erutxeTE3%-yrtemoeG02%laitneuqes02%a02%swollof02%tI02%.stpmorp02%txet02%morfA0%sratava02%elbatamina02%ytilauq-hgih02%gnitareneg02%rof02%krowemarf02%evissergorp02%a02%C2%racsO-XA0%esoporp02%ew02%C2%segnellahc02%eseht02%sserdda02%oT02%.tuptuo02%ytilauq-wol02%dna02%noitarutasrevoA0%sa02%hcus02%snoitatimil02%evah02%sdohtem02%gnitsixe02%C2%revewoH02%.ssergorp02%tnacifingis02%edamA0%evah02%txet02%yb02%dediug02%noitareneg02%ratava02%D302%citamotua02%ni02%stnemecnavda02%tneceR02%02%=3328342921.yrtne&iJ02%gnorgnoR02%dna02%nuS02%iauhsoaiX02%dna02%naF02%nujiY02%dna02%iJ02%iyaiJ02%dna02%niL02%iakehZ02%dna02%aM02%iewiY=526535609.yrtne&noitareneG02%ratavA02%elbataminA02%02%A0%D302%dediug-txeT02%ytilauq-hgiH02%rof02%krowemarF02%evissergorP02%A02%A3%racsO-X=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20X-Oscar%3A%20A%20Progressive%20Framework%20for%20High-quality%20Text-guided%203D%0A%20%20Animatable%20Avatar%20Generation&body=Title%3A%20X-Oscar%3A%20A%20Progressive%20Framework%20for%20High-quality%20Text-guided%203D%0A%20%20Animatable%20Avatar%20Generation%0AAuthor%3A%20Yiwei%20Ma%20and%20Zhekai%20Lin%20and%20Jiayi%20Ji%20and%20Yijun%20Fan%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji%0AAbstract%3A%20%20%20Recent%20advancements%20in%20automatic%203D%20avatar%20generation%20guided%20by%20text%20have%0Amade%20significant%20progress.%20However%2C%20existing%20methods%20have%20limitations%20such%20as%0Aoversaturation%20and%20low-quality%20output.%20To%20address%20these%20challenges%2C%20we%20propose%0AX-Oscar%2C%20a%20progressive%20framework%20for%20generating%20high-quality%20animatable%20avatars%0Afrom%20text%20prompts.%20It%20follows%20a%20sequential%20Geometry-%3ETexture-%3EAnimation%0Aparadigm%2C%20simplifying%20optimization%20through%20step-by-step%20generation.%20To%20tackle%0Aoversaturation%2C%20we%20introduce%20Adaptive%20Variational%20Parameter%20%28AVP%29%2C%20representing%0Aavatars%20as%20an%20adaptive%20distribution%20during%20training.%20Additionally%2C%20we%20present%0AAvatar-aware%20Score%20Distillation%20Sampling%20%28ASDS%29%2C%20a%20novel%20technique%20that%0Aincorporates%20avatar-aware%20noise%20into%20rendered%20images%20for%20improved%20generation%0Aquality%20during%20optimization.%20Extensive%20evaluations%20confirm%20the%20superiority%20of%0AX-Oscar%20over%20existing%20text-to-3D%20and%20text-to-avatar%20approaches.%20Our%20anonymous%0Aproject%20page%3A%20https%3A//xmu-xiaoma666.github.io/Projects/X-Oscar/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00954v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-Oscar%3A%20A%20Progressive%20Framework%20for%20High-quality%20Text-guided%203D%0A%20%20Animatable%20Avatar%20Generation&entry.906535625=Yiwei%20Ma%20and%20Zhekai%20Lin%20and%20Jiayi%20Ji%20and%20Yijun%20Fan%20and%20Xiaoshuai%20Sun%20and%20Rongrong%20Ji&entry.1292438233=%20%20Recent%20advancements%20in%20automatic%203D%20avatar%20generation%20guided%20by%20text%20have%0Amade%20significant%20progress.%20However%2C%20existing%20methods%20have%20limitations%20such%20as%0Aoversaturation%20and%20low-quality%20output.%20To%20address%20these%20challenges%2C%20we%20propose%0AX-Oscar%2C%20a%20progressive%20framework%20for%20generating%20high-quality%20animatable%20avatars%0Afrom%20text%20prompts.%20It%20follows%20a%20sequential%20Geometry-%3ETexture-%3EAnimation%0Aparadigm%2C%20simplifying%20optimization%20through%20step-by-step%20generation.%20To%20tackle%0Aoversaturation%2C%20we%20introduce%20Adaptive%20Variational%20Parameter%20%28AVP%29%2C%20representing%0Aavatars%20as%20an%20adaptive%20distribution%20during%20training.%20Additionally%2C%20we%20present%0AAvatar-aware%20Score%20Distillation%20Sampling%20%28ASDS%29%2C%20a%20novel%20technique%20that%0Aincorporates%20avatar-aware%20noise%20into%20rendered%20images%20for%20improved%20generation%0Aquality%20during%20optimization.%20Extensive%20evaluations%20confirm%20the%20superiority%20of%0AX-Oscar%20over%20existing%20text-to-3D%20and%20text-to-avatar%20approaches.%20Our%20anonymous%0Aproject%20page%3A%20https%3A//xmu-xiaoma666.github.io/Projects/X-Oscar/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00954v1&entry.124074799=Read"},
{"title": "Brighteye: Glaucoma Screening with Color Fundus Photographs based on\n  Vision Transformer", "author": "Hui Lin and Charilaos Apostolidis and Aggelos K. Katsaggelos", "abstract": "  Differences in image quality, lighting conditions, and patient demographics\npose challenges to automated glaucoma detection from color fundus photography.\nBrighteye, a method based on Vision Transformer, is proposed for glaucoma\ndetection and glaucomatous feature classification. Brighteye learns long-range\nrelationships among pixels within large fundus images using a self-attention\nmechanism. Prior to being input into Brighteye, the optic disc is localized\nusing YOLOv8, and the region of interest (ROI) around the disc center is\ncropped to ensure alignment with clinical practice. Optic disc detection\nimproves the sensitivity at 95% specificity from 79.20% to 85.70% for glaucoma\ndetection and the Hamming distance from 0.2470 to 0.1250 for glaucomatous\nfeature classification. In the developmental stage of the Justified Referral in\nAI Glaucoma Screening (JustRAIGS) challenge, the overall outcome secured the\nfifth position out of 226 entries.\n", "link": "http://arxiv.org/abs/2405.00857v1", "date": "2024-05-01", "relevancy": 2.3897, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4858}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4844}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4637}], "mailto": "mailto:daeR=997470421.yrtne&1v75800.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.seirtne02%62202%fo02%tuo02%noitisop02%htfifA0%eht02%deruces02%emoctuo02%llarevo02%eht02%C2%egnellahc02%92%SGIARtsuJ82%02%gnineercS02%amocualG02%IAA0%ni02%larrefeR02%deifitsuJ02%eht02%fo02%egats02%latnempoleved02%eht02%nI02%.noitacifissalc02%erutaefA0%suotamocualg02%rof02%0521.002%ot02%0742.002%morf02%ecnatsid02%gnimmaH02%eht02%dna02%noitcetedA0%amocualg02%rof02%52%07.5802%ot02%52%02.9702%morf02%yticificeps02%52%5902%ta02%ytivitisnes02%eht02%sevorpmiA0%noitceted02%csid02%citpO02%.ecitcarp02%lacinilc02%htiw02%tnemngila02%erusne02%ot02%depporcA0%si02%retnec02%csid02%eht02%dnuora02%92%IOR82%02%tseretni02%fo02%noiger02%eht02%dna02%C2%8vOLOY02%gnisuA0%dezilacol02%si02%csid02%citpo02%eht02%C2%eyethgirB02%otni02%tupni02%gnieb02%ot02%roirP02%.msinahcemA0%noitnetta-fles02%a02%gnisu02%segami02%sudnuf02%egral02%nihtiw02%slexip02%gnoma02%spihsnoitalerA0%egnar-gnol02%snrael02%eyethgirB02%.noitacifissalc02%erutaef02%suotamocualg02%dna02%noitcetedA0%amocualg02%rof02%desoporp02%si02%C2%remrofsnarT02%noisiV02%no02%desab02%dohtem02%a02%C2%eyethgirBA0%.yhpargotohp02%sudnuf02%roloc02%morf02%noitceted02%amocualg02%detamotua02%ot02%segnellahc02%esopA0%scihpargomed02%tneitap02%dna02%C2%snoitidnoc02%gnithgil02%C2%ytilauq02%egami02%ni02%secnereffiD02%02%=3328342921.yrtne&soleggastaK02%.K02%soleggA02%dna02%sidilotsopA02%soalirahC02%dna02%niL02%iuH=526535609.yrtne&remrofsnarT02%noisiV02%02%A0%no02%desab02%shpargotohP02%sudnuF02%roloC02%htiw02%gnineercS02%amocualG02%A3%eyethgirB=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Brighteye%3A%20Glaucoma%20Screening%20with%20Color%20Fundus%20Photographs%20based%20on%0A%20%20Vision%20Transformer&body=Title%3A%20Brighteye%3A%20Glaucoma%20Screening%20with%20Color%20Fundus%20Photographs%20based%20on%0A%20%20Vision%20Transformer%0AAuthor%3A%20Hui%20Lin%20and%20Charilaos%20Apostolidis%20and%20Aggelos%20K.%20Katsaggelos%0AAbstract%3A%20%20%20Differences%20in%20image%20quality%2C%20lighting%20conditions%2C%20and%20patient%20demographics%0Apose%20challenges%20to%20automated%20glaucoma%20detection%20from%20color%20fundus%20photography.%0ABrighteye%2C%20a%20method%20based%20on%20Vision%20Transformer%2C%20is%20proposed%20for%20glaucoma%0Adetection%20and%20glaucomatous%20feature%20classification.%20Brighteye%20learns%20long-range%0Arelationships%20among%20pixels%20within%20large%20fundus%20images%20using%20a%20self-attention%0Amechanism.%20Prior%20to%20being%20input%20into%20Brighteye%2C%20the%20optic%20disc%20is%20localized%0Ausing%20YOLOv8%2C%20and%20the%20region%20of%20interest%20%28ROI%29%20around%20the%20disc%20center%20is%0Acropped%20to%20ensure%20alignment%20with%20clinical%20practice.%20Optic%20disc%20detection%0Aimproves%20the%20sensitivity%20at%2095%25%20specificity%20from%2079.20%25%20to%2085.70%25%20for%20glaucoma%0Adetection%20and%20the%20Hamming%20distance%20from%200.2470%20to%200.1250%20for%20glaucomatous%0Afeature%20classification.%20In%20the%20developmental%20stage%20of%20the%20Justified%20Referral%20in%0AAI%20Glaucoma%20Screening%20%28JustRAIGS%29%20challenge%2C%20the%20overall%20outcome%20secured%20the%0Afifth%20position%20out%20of%20226%20entries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00857v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Brighteye%3A%20Glaucoma%20Screening%20with%20Color%20Fundus%20Photographs%20based%20on%0A%20%20Vision%20Transformer&entry.906535625=Hui%20Lin%20and%20Charilaos%20Apostolidis%20and%20Aggelos%20K.%20Katsaggelos&entry.1292438233=%20%20Differences%20in%20image%20quality%2C%20lighting%20conditions%2C%20and%20patient%20demographics%0Apose%20challenges%20to%20automated%20glaucoma%20detection%20from%20color%20fundus%20photography.%0ABrighteye%2C%20a%20method%20based%20on%20Vision%20Transformer%2C%20is%20proposed%20for%20glaucoma%0Adetection%20and%20glaucomatous%20feature%20classification.%20Brighteye%20learns%20long-range%0Arelationships%20among%20pixels%20within%20large%20fundus%20images%20using%20a%20self-attention%0Amechanism.%20Prior%20to%20being%20input%20into%20Brighteye%2C%20the%20optic%20disc%20is%20localized%0Ausing%20YOLOv8%2C%20and%20the%20region%20of%20interest%20%28ROI%29%20around%20the%20disc%20center%20is%0Acropped%20to%20ensure%20alignment%20with%20clinical%20practice.%20Optic%20disc%20detection%0Aimproves%20the%20sensitivity%20at%2095%25%20specificity%20from%2079.20%25%20to%2085.70%25%20for%20glaucoma%0Adetection%20and%20the%20Hamming%20distance%20from%200.2470%20to%200.1250%20for%20glaucomatous%0Afeature%20classification.%20In%20the%20developmental%20stage%20of%20the%20Justified%20Referral%20in%0AAI%20Glaucoma%20Screening%20%28JustRAIGS%29%20challenge%2C%20the%20overall%20outcome%20secured%20the%0Afifth%20position%20out%20of%20226%20entries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00857v1&entry.124074799=Read"},
{"title": "ATOM: Attention Mixer for Efficient Dataset Distillation", "author": "Samir Khaki and Ahmad Sajedi and Kai Wang and Lucy Z. Liu and Yuri A. Lawryshyn and Konstantinos N. Plataniotis", "abstract": "  Recent works in dataset distillation seek to minimize training expenses by\ngenerating a condensed synthetic dataset that encapsulates the information\npresent in a larger real dataset. These approaches ultimately aim to attain\ntest accuracy levels akin to those achieved by models trained on the entirety\nof the original dataset. Previous studies in feature and distribution matching\nhave achieved significant results without incurring the costs of bi-level\noptimization in the distillation process. Despite their convincing efficiency,\nmany of these methods suffer from marginal downstream performance improvements,\nlimited distillation of contextual information, and subpar cross-architecture\ngeneralization. To address these challenges in dataset distillation, we propose\nthe ATtentiOn Mixer (ATOM) module to efficiently distill large datasets using a\nmixture of channel and spatial-wise attention in the feature matching process.\nSpatial-wise attention helps guide the learning process based on consistent\nlocalization of classes in their respective images, allowing for distillation\nfrom a broader receptive field. Meanwhile, channel-wise attention captures the\ncontextual information associated with the class itself, thus making the\nsynthetic image more informative for training. By integrating both types of\nattention, our ATOM module demonstrates superior performance across various\ncomputer vision datasets, including CIFAR10/100 and TinyImagenet. Notably, our\nmethod significantly improves performance in scenarios with a low number of\nimages per class, thereby enhancing its potential. Furthermore, we maintain the\nimprovement in cross-architectures and applications such as neural architecture\nsearch.\n", "link": "http://arxiv.org/abs/2405.01373v1", "date": "2024-05-02", "relevancy": 2.3872, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6546}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5723}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5488}], "mailto": "mailto:daeR=997470421.yrtne&1v37310.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.hcraesA0%erutcetihcra02%laruen02%sa02%hcus02%snoitacilppa02%dna02%serutcetihcra-ssorc02%ni02%tnemevorpmiA0%eht02%niatniam02%ew02%C2%eromrehtruF02%.laitnetop02%sti02%gnicnahne02%ybereht02%C2%ssalc02%rep02%segamiA0%fo02%rebmun02%wol02%a02%htiw02%soiranecs02%ni02%ecnamrofrep02%sevorpmi02%yltnacifingis02%dohtemA0%ruo02%C2%ylbatoN02%.tenegamIyniT02%dna02%001/01RAFIC02%gnidulcni02%C2%stesatad02%noisiv02%retupmocA0%suoirav02%ssorca02%ecnamrofrep02%roirepus02%setartsnomed02%eludom02%MOTA02%ruo02%C2%noitnettaA0%fo02%sepyt02%htob02%gnitargetni02%yB02%.gniniart02%rof02%evitamrofni02%erom02%egami02%citehtnysA0%eht02%gnikam02%suht02%C2%flesti02%ssalc02%eht02%htiw02%detaicossa02%noitamrofni02%lautxetnocA0%eht02%serutpac02%noitnetta02%esiw-lennahc02%C2%elihwnaeM02%.dleif02%evitpecer02%redaorb02%a02%morfA0%noitallitsid02%rof02%gniwolla02%C2%segami02%evitcepser02%rieht02%ni02%sessalc02%fo02%noitazilacolA0%tnetsisnoc02%no02%desab02%ssecorp02%gninrael02%eht02%ediug02%spleh02%noitnetta02%esiw-laitapSA0%.ssecorp02%gnihctam02%erutaef02%eht02%ni02%noitnetta02%esiw-laitaps02%dna02%lennahc02%fo02%erutximA0%a02%gnisu02%stesatad02%egral02%llitsid02%yltneiciffe02%ot02%eludom02%92%MOTA82%02%rexiM02%nOitnetTA02%ehtA0%esoporp02%ew02%C2%noitallitsid02%tesatad02%ni02%segnellahc02%eseht02%sserdda02%oT02%.noitazilarenegA0%erutcetihcra-ssorc02%rapbus02%dna02%C2%noitamrofni02%lautxetnoc02%fo02%noitallitsid02%detimilA0%C2%stnemevorpmi02%ecnamrofrep02%maertsnwod02%lanigram02%morf02%reffus02%sdohtem02%eseht02%fo02%ynamA0%C2%ycneiciffe02%gnicnivnoc02%rieht02%etipseD02%.ssecorp02%noitallitsid02%eht02%ni02%noitazimitpoA0%level-ib02%fo02%stsoc02%eht02%gnirrucni02%tuohtiw02%stluser02%tnacifingis02%deveihca02%evahA0%gnihctam02%noitubirtsid02%dna02%erutaef02%ni02%seiduts02%suoiverP02%.tesatad02%lanigiro02%eht02%foA0%yteritne02%eht02%no02%deniart02%sledom02%yb02%deveihca02%esoht02%ot02%nika02%slevel02%ycarucca02%tsetA0%niatta02%ot02%mia02%yletamitlu02%sehcaorppa02%esehT02%.tesatad02%laer02%regral02%a02%ni02%tneserpA0%noitamrofni02%eht02%setaluspacne02%taht02%tesatad02%citehtnys02%desnednoc02%a02%gnitarenegA0%yb02%sesnepxe02%gniniart02%eziminim02%ot02%kees02%noitallitsid02%tesatad02%ni02%skrow02%tneceR02%02%=3328342921.yrtne&sitoinatalP02%.N02%sonitnatsnoK02%dna02%nyhsyrwaL02%.A02%iruY02%dna02%uiL02%.Z02%ycuL02%dna02%gnaW02%iaK02%dna02%idejaS02%damhA02%dna02%ikahK02%rimaS=526535609.yrtne&noitallitsiD02%tesataD02%tneiciffE02%rof02%rexiM02%noitnettA02%A3%MOTA=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20ATOM%3A%20Attention%20Mixer%20for%20Efficient%20Dataset%20Distillation&body=Title%3A%20ATOM%3A%20Attention%20Mixer%20for%20Efficient%20Dataset%20Distillation%0AAuthor%3A%20Samir%20Khaki%20and%20Ahmad%20Sajedi%20and%20Kai%20Wang%20and%20Lucy%20Z.%20Liu%20and%20Yuri%20A.%20Lawryshyn%20and%20Konstantinos%20N.%20Plataniotis%0AAbstract%3A%20%20%20Recent%20works%20in%20dataset%20distillation%20seek%20to%20minimize%20training%20expenses%20by%0Agenerating%20a%20condensed%20synthetic%20dataset%20that%20encapsulates%20the%20information%0Apresent%20in%20a%20larger%20real%20dataset.%20These%20approaches%20ultimately%20aim%20to%20attain%0Atest%20accuracy%20levels%20akin%20to%20those%20achieved%20by%20models%20trained%20on%20the%20entirety%0Aof%20the%20original%20dataset.%20Previous%20studies%20in%20feature%20and%20distribution%20matching%0Ahave%20achieved%20significant%20results%20without%20incurring%20the%20costs%20of%20bi-level%0Aoptimization%20in%20the%20distillation%20process.%20Despite%20their%20convincing%20efficiency%2C%0Amany%20of%20these%20methods%20suffer%20from%20marginal%20downstream%20performance%20improvements%2C%0Alimited%20distillation%20of%20contextual%20information%2C%20and%20subpar%20cross-architecture%0Ageneralization.%20To%20address%20these%20challenges%20in%20dataset%20distillation%2C%20we%20propose%0Athe%20ATtentiOn%20Mixer%20%28ATOM%29%20module%20to%20efficiently%20distill%20large%20datasets%20using%20a%0Amixture%20of%20channel%20and%20spatial-wise%20attention%20in%20the%20feature%20matching%20process.%0ASpatial-wise%20attention%20helps%20guide%20the%20learning%20process%20based%20on%20consistent%0Alocalization%20of%20classes%20in%20their%20respective%20images%2C%20allowing%20for%20distillation%0Afrom%20a%20broader%20receptive%20field.%20Meanwhile%2C%20channel-wise%20attention%20captures%20the%0Acontextual%20information%20associated%20with%20the%20class%20itself%2C%20thus%20making%20the%0Asynthetic%20image%20more%20informative%20for%20training.%20By%20integrating%20both%20types%20of%0Aattention%2C%20our%20ATOM%20module%20demonstrates%20superior%20performance%20across%20various%0Acomputer%20vision%20datasets%2C%20including%20CIFAR10/100%20and%20TinyImagenet.%20Notably%2C%20our%0Amethod%20significantly%20improves%20performance%20in%20scenarios%20with%20a%20low%20number%20of%0Aimages%20per%20class%2C%20thereby%20enhancing%20its%20potential.%20Furthermore%2C%20we%20maintain%20the%0Aimprovement%20in%20cross-architectures%20and%20applications%20such%20as%20neural%20architecture%0Asearch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01373v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ATOM%3A%20Attention%20Mixer%20for%20Efficient%20Dataset%20Distillation&entry.906535625=Samir%20Khaki%20and%20Ahmad%20Sajedi%20and%20Kai%20Wang%20and%20Lucy%20Z.%20Liu%20and%20Yuri%20A.%20Lawryshyn%20and%20Konstantinos%20N.%20Plataniotis&entry.1292438233=%20%20Recent%20works%20in%20dataset%20distillation%20seek%20to%20minimize%20training%20expenses%20by%0Agenerating%20a%20condensed%20synthetic%20dataset%20that%20encapsulates%20the%20information%0Apresent%20in%20a%20larger%20real%20dataset.%20These%20approaches%20ultimately%20aim%20to%20attain%0Atest%20accuracy%20levels%20akin%20to%20those%20achieved%20by%20models%20trained%20on%20the%20entirety%0Aof%20the%20original%20dataset.%20Previous%20studies%20in%20feature%20and%20distribution%20matching%0Ahave%20achieved%20significant%20results%20without%20incurring%20the%20costs%20of%20bi-level%0Aoptimization%20in%20the%20distillation%20process.%20Despite%20their%20convincing%20efficiency%2C%0Amany%20of%20these%20methods%20suffer%20from%20marginal%20downstream%20performance%20improvements%2C%0Alimited%20distillation%20of%20contextual%20information%2C%20and%20subpar%20cross-architecture%0Ageneralization.%20To%20address%20these%20challenges%20in%20dataset%20distillation%2C%20we%20propose%0Athe%20ATtentiOn%20Mixer%20%28ATOM%29%20module%20to%20efficiently%20distill%20large%20datasets%20using%20a%0Amixture%20of%20channel%20and%20spatial-wise%20attention%20in%20the%20feature%20matching%20process.%0ASpatial-wise%20attention%20helps%20guide%20the%20learning%20process%20based%20on%20consistent%0Alocalization%20of%20classes%20in%20their%20respective%20images%2C%20allowing%20for%20distillation%0Afrom%20a%20broader%20receptive%20field.%20Meanwhile%2C%20channel-wise%20attention%20captures%20the%0Acontextual%20information%20associated%20with%20the%20class%20itself%2C%20thus%20making%20the%0Asynthetic%20image%20more%20informative%20for%20training.%20By%20integrating%20both%20types%20of%0Aattention%2C%20our%20ATOM%20module%20demonstrates%20superior%20performance%20across%20various%0Acomputer%20vision%20datasets%2C%20including%20CIFAR10/100%20and%20TinyImagenet.%20Notably%2C%20our%0Amethod%20significantly%20improves%20performance%20in%20scenarios%20with%20a%20low%20number%20of%0Aimages%20per%20class%2C%20thereby%20enhancing%20its%20potential.%20Furthermore%2C%20we%20maintain%20the%0Aimprovement%20in%20cross-architectures%20and%20applications%20such%20as%20neural%20architecture%0Asearch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01373v1&entry.124074799=Read"},
{"title": "Towards Consistent Object Detection via LiDAR-Camera Synergy", "author": "Kai Luo and Hao Wu and Kefu Yi and Kailun Yang and Wei Hao and Rongdong Hu", "abstract": "  As human-machine interaction continues to evolve, the capacity for\nenvironmental perception is becoming increasingly crucial. Integrating the two\nmost common types of sensory data, images, and point clouds, can enhance\ndetection accuracy. However, currently, no model exists that can simultaneously\ndetect an object's position in both point clouds and images and ascertain their\ncorresponding relationship. This information is invaluable for human-machine\ninteractions, offering new possibilities for their enhancement. In light of\nthis, this paper introduces an end-to-end Consistency Object Detection (COD)\nalgorithm framework that requires only a single forward inference to\nsimultaneously obtain an object's position in both point clouds and images and\nestablish their correlation. Furthermore, to assess the accuracy of the object\ncorrelation between point clouds and images, this paper proposes a new\nevaluation metric, Consistency Precision (CP). To verify the effectiveness of\nthe proposed framework, an extensive set of experiments has been conducted on\nthe KITTI and DAIR-V2X datasets. The study also explored how the proposed\nconsistency detection method performs on images when the calibration parameters\nbetween images and point clouds are disturbed, compared to existing\npost-processing methods. The experimental results demonstrate that the proposed\nmethod exhibits excellent detection performance and robustness, achieving\nend-to-end consistency detection. The source code will be made publicly\navailable at https://github.com/xifen523/COD.\n", "link": "http://arxiv.org/abs/2405.01258v1", "date": "2024-05-02", "relevancy": 2.384, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6142}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5855}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5766}], "mailto": "mailto:daeR=997470421.yrtne&1v85210.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.DOC/325nefix/moc.buhtig//A3%sptth02%ta02%elbaliavaA0%ylcilbup02%edam02%eb02%lliw02%edoc02%ecruos02%ehT02%.noitceted02%ycnetsisnoc02%dne-ot-dneA0%gniveihca02%C2%ssentsubor02%dna02%ecnamrofrep02%noitceted02%tnellecxe02%stibihxe02%dohtemA0%desoporp02%eht02%taht02%etartsnomed02%stluser02%latnemirepxe02%ehT02%.sdohtem02%gnissecorp-tsopA0%gnitsixe02%ot02%derapmoc02%C2%debrutsid02%era02%sduolc02%tniop02%dna02%segami02%neewtebA0%sretemarap02%noitarbilac02%eht02%nehw02%segami02%no02%smrofrep02%dohtem02%noitceted02%ycnetsisnocA0%desoporp02%eht02%woh02%derolpxe02%osla02%yduts02%ehT02%.stesatad02%X2V-RIAD02%dna02%ITTIK02%ehtA0%no02%detcudnoc02%neeb02%sah02%stnemirepxe02%fo02%tes02%evisnetxe02%na02%C2%krowemarf02%desoporp02%ehtA0%fo02%ssenevitceffe02%eht02%yfirev02%oT02%.92%PC82%02%noisicerP02%ycnetsisnoC02%C2%cirtem02%noitaulaveA0%wen02%a02%sesoporp02%repap02%siht02%C2%segami02%dna02%sduolc02%tniop02%neewteb02%noitalerrocA0%tcejbo02%eht02%fo02%ycarucca02%eht02%ssessa02%ot02%C2%eromrehtruF02%.noitalerroc02%rieht02%hsilbatseA0%dna02%segami02%dna02%sduolc02%tniop02%htob02%ni02%noitisop02%s72%tcejbo02%na02%niatbo02%ylsuoenatlumisA0%ot02%ecnerefni02%drawrof02%elgnis02%a02%ylno02%seriuqer02%taht02%krowemarf02%mhtiroglaA0%92%DOC82%02%noitceteD02%tcejbO02%ycnetsisnoC02%dne-ot-dne02%na02%secudortni02%repap02%siht02%C2%sihtA0%fo02%thgil02%nI02%.tnemecnahne02%rieht02%rof02%seitilibissop02%wen02%gnireffo02%C2%snoitcaretniA0%enihcam-namuh02%rof02%elbaulavni02%si02%noitamrofni02%sihT02%.pihsnoitaler02%gnidnopserrocA0%rieht02%niatrecsa02%dna02%segami02%dna02%sduolc02%tniop02%htob02%ni02%noitisop02%s72%tcejbo02%na02%tcetedA0%ylsuoenatlumis02%nac02%taht02%stsixe02%ledom02%on02%C2%yltnerruc02%C2%revewoH02%.ycarucca02%noitcetedA0%ecnahne02%nac02%C2%sduolc02%tniop02%dna02%C2%segami02%C2%atad02%yrosnes02%fo02%sepyt02%nommoc02%tsomA0%owt02%eht02%gnitargetnI02%.laicurc02%ylgnisaercni02%gnimoceb02%si02%noitpecrep02%latnemnorivneA0%rof02%yticapac02%eht02%C2%evlove02%ot02%seunitnoc02%noitcaretni02%enihcam-namuh02%sA02%02%=3328342921.yrtne&uH02%gnodgnoR02%dna02%oaH02%ieW02%dna02%gnaY02%nuliaK02%dna02%iY02%ufeK02%dna02%uW02%oaH02%dna02%ouL02%iaK=526535609.yrtne&ygrenyS02%aremaC-RADiL02%aiv02%noitceteD02%tcejbO02%tnetsisnoC02%sdrawoT=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Towards%20Consistent%20Object%20Detection%20via%20LiDAR-Camera%20Synergy&body=Title%3A%20Towards%20Consistent%20Object%20Detection%20via%20LiDAR-Camera%20Synergy%0AAuthor%3A%20Kai%20Luo%20and%20Hao%20Wu%20and%20Kefu%20Yi%20and%20Kailun%20Yang%20and%20Wei%20Hao%20and%20Rongdong%20Hu%0AAbstract%3A%20%20%20As%20human-machine%20interaction%20continues%20to%20evolve%2C%20the%20capacity%20for%0Aenvironmental%20perception%20is%20becoming%20increasingly%20crucial.%20Integrating%20the%20two%0Amost%20common%20types%20of%20sensory%20data%2C%20images%2C%20and%20point%20clouds%2C%20can%20enhance%0Adetection%20accuracy.%20However%2C%20currently%2C%20no%20model%20exists%20that%20can%20simultaneously%0Adetect%20an%20object%27s%20position%20in%20both%20point%20clouds%20and%20images%20and%20ascertain%20their%0Acorresponding%20relationship.%20This%20information%20is%20invaluable%20for%20human-machine%0Ainteractions%2C%20offering%20new%20possibilities%20for%20their%20enhancement.%20In%20light%20of%0Athis%2C%20this%20paper%20introduces%20an%20end-to-end%20Consistency%20Object%20Detection%20%28COD%29%0Aalgorithm%20framework%20that%20requires%20only%20a%20single%20forward%20inference%20to%0Asimultaneously%20obtain%20an%20object%27s%20position%20in%20both%20point%20clouds%20and%20images%20and%0Aestablish%20their%20correlation.%20Furthermore%2C%20to%20assess%20the%20accuracy%20of%20the%20object%0Acorrelation%20between%20point%20clouds%20and%20images%2C%20this%20paper%20proposes%20a%20new%0Aevaluation%20metric%2C%20Consistency%20Precision%20%28CP%29.%20To%20verify%20the%20effectiveness%20of%0Athe%20proposed%20framework%2C%20an%20extensive%20set%20of%20experiments%20has%20been%20conducted%20on%0Athe%20KITTI%20and%20DAIR-V2X%20datasets.%20The%20study%20also%20explored%20how%20the%20proposed%0Aconsistency%20detection%20method%20performs%20on%20images%20when%20the%20calibration%20parameters%0Abetween%20images%20and%20point%20clouds%20are%20disturbed%2C%20compared%20to%20existing%0Apost-processing%20methods.%20The%20experimental%20results%20demonstrate%20that%20the%20proposed%0Amethod%20exhibits%20excellent%20detection%20performance%20and%20robustness%2C%20achieving%0Aend-to-end%20consistency%20detection.%20The%20source%20code%20will%20be%20made%20publicly%0Aavailable%20at%20https%3A//github.com/xifen523/COD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01258v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Consistent%20Object%20Detection%20via%20LiDAR-Camera%20Synergy&entry.906535625=Kai%20Luo%20and%20Hao%20Wu%20and%20Kefu%20Yi%20and%20Kailun%20Yang%20and%20Wei%20Hao%20and%20Rongdong%20Hu&entry.1292438233=%20%20As%20human-machine%20interaction%20continues%20to%20evolve%2C%20the%20capacity%20for%0Aenvironmental%20perception%20is%20becoming%20increasingly%20crucial.%20Integrating%20the%20two%0Amost%20common%20types%20of%20sensory%20data%2C%20images%2C%20and%20point%20clouds%2C%20can%20enhance%0Adetection%20accuracy.%20However%2C%20currently%2C%20no%20model%20exists%20that%20can%20simultaneously%0Adetect%20an%20object%27s%20position%20in%20both%20point%20clouds%20and%20images%20and%20ascertain%20their%0Acorresponding%20relationship.%20This%20information%20is%20invaluable%20for%20human-machine%0Ainteractions%2C%20offering%20new%20possibilities%20for%20their%20enhancement.%20In%20light%20of%0Athis%2C%20this%20paper%20introduces%20an%20end-to-end%20Consistency%20Object%20Detection%20%28COD%29%0Aalgorithm%20framework%20that%20requires%20only%20a%20single%20forward%20inference%20to%0Asimultaneously%20obtain%20an%20object%27s%20position%20in%20both%20point%20clouds%20and%20images%20and%0Aestablish%20their%20correlation.%20Furthermore%2C%20to%20assess%20the%20accuracy%20of%20the%20object%0Acorrelation%20between%20point%20clouds%20and%20images%2C%20this%20paper%20proposes%20a%20new%0Aevaluation%20metric%2C%20Consistency%20Precision%20%28CP%29.%20To%20verify%20the%20effectiveness%20of%0Athe%20proposed%20framework%2C%20an%20extensive%20set%20of%20experiments%20has%20been%20conducted%20on%0Athe%20KITTI%20and%20DAIR-V2X%20datasets.%20The%20study%20also%20explored%20how%20the%20proposed%0Aconsistency%20detection%20method%20performs%20on%20images%20when%20the%20calibration%20parameters%0Abetween%20images%20and%20point%20clouds%20are%20disturbed%2C%20compared%20to%20existing%0Apost-processing%20methods.%20The%20experimental%20results%20demonstrate%20that%20the%20proposed%0Amethod%20exhibits%20excellent%20detection%20performance%20and%20robustness%2C%20achieving%0Aend-to-end%20consistency%20detection.%20The%20source%20code%20will%20be%20made%20publicly%0Aavailable%20at%20https%3A//github.com/xifen523/COD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01258v1&entry.124074799=Read"},
{"title": "Efficient and Adaptive Posterior Sampling Algorithms for Bandits", "author": "Bingshan Hu and Zhiming Huang and Tianyue H. Zhang and Mathias L\u00e9cuyer and Nidhi Hegde", "abstract": "  We study Thompson Sampling-based algorithms for stochastic bandits with\nbounded rewards. As the existing problem-dependent regret bound for Thompson\nSampling with Gaussian priors [Agrawal and Goyal, 2017] is vacuous when $T \\le\n288 e^{64}$, we derive a more practical bound that tightens the coefficient of\nthe leading term %from $288 e^{64}$ to $1270$. Additionally, motivated by\nlarge-scale real-world applications that require scalability, adaptive\ncomputational resource allocation, and a balance in utility and computation, we\npropose two parameterized Thompson Sampling-based algorithms: Thompson Sampling\nwith Model Aggregation (TS-MA-$\\alpha$) and Thompson Sampling with Timestamp\nDuelling (TS-TD-$\\alpha$), where $\\alpha \\in [0,1]$ controls the trade-off\nbetween utility and computation. Both algorithms achieve $O\n\\left(K\\ln^{\\alpha+1}(T)/\\Delta \\right)$ regret bound, where $K$ is the number\nof arms, $T$ is the finite learning horizon, and $\\Delta$ denotes the single\nround performance loss when pulling a sub-optimal arm.\n", "link": "http://arxiv.org/abs/2405.01010v1", "date": "2024-05-02", "relevancy": 2.375, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5376}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4438}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4436}], "mailto": "mailto:daeR=997470421.yrtne&1v01010.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.mra02%lamitpo-bus02%a02%gnillup02%nehw02%ssol02%ecnamrofrep02%dnuorA0%elgnis02%eht02%setoned02%42%atleDC5%42%02%dna02%C2%noziroh02%gninrael02%etinif02%eht02%si02%42%T42%02%C2%smra02%foA0%rebmun02%eht02%si02%42%K42%02%erehw02%C2%dnuob02%terger02%42%92%thgirC5%02%atleDC5%/92%T82%D7%1B2%ahplaC5%B7%E5%nlC5%K82%tfelC5%A0%O42%02%eveihca02%smhtirogla02%htoB02%.noitatupmoc02%dna02%ytilitu02%neewtebA0%ffo-edart02%eht02%slortnoc02%42%D5%1C2%0B5%02%niC5%02%ahplaC5%42%02%erehw02%C2%92%42%ahplaC5%42%-DT-ST82%02%gnilleuDA0%pmatsemiT02%htiw02%gnilpmaS02%nospmohT02%dna02%92%42%ahplaC5%42%-AM-ST82%02%noitagerggA02%ledoM02%htiwA0%gnilpmaS02%nospmohT02%A3%smhtirogla02%desab-gnilpmaS02%nospmohT02%deziretemarap02%owt02%esoporpA0%ew02%C2%noitatupmoc02%dna02%ytilitu02%ni02%ecnalab02%a02%dna02%C2%noitacolla02%ecruoser02%lanoitatupmocA0%evitpada02%C2%ytilibalacs02%eriuqer02%taht02%snoitacilppa02%dlrow-laer02%elacs-egralA0%yb02%detavitom02%C2%yllanoitiddA02%.42%072142%02%ot02%42%D7%46B7%E5%e02%88242%02%morf52%02%mret02%gnidael02%ehtA0%fo02%tneiciffeoc02%eht02%snethgit02%taht02%dnuob02%lacitcarp02%erom02%a02%evired02%ew02%C2%42%D7%46B7%E5%e02%882A0%elC5%02%T42%02%nehw02%suoucav02%si02%D5%710202%C2%layoG02%dna02%lawargAB5%02%sroirp02%naissuaG02%htiw02%gnilpmaSA0%nospmohT02%rof02%dnuob02%terger02%tnedneped-melborp02%gnitsixe02%eht02%sA02%.sdrawer02%dednuobA0%htiw02%stidnab02%citsahcots02%rof02%smhtirogla02%desab-gnilpmaS02%nospmohT02%yduts02%eW02%02%=3328342921.yrtne&edgeH02%ihdiN02%dna02%reyuc9A%3C%L02%saihtaM02%dna02%gnahZ02%.H02%euynaiT02%dna02%gnauH02%gnimihZ02%dna02%uH02%nahsgniB=526535609.yrtne&stidnaB02%rof02%smhtiroglA02%gnilpmaS02%roiretsoP02%evitpadA02%dna02%tneiciffE=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Adaptive%20Posterior%20Sampling%20Algorithms%20for%20Bandits&body=Title%3A%20Efficient%20and%20Adaptive%20Posterior%20Sampling%20Algorithms%20for%20Bandits%0AAuthor%3A%20Bingshan%20Hu%20and%20Zhiming%20Huang%20and%20Tianyue%20H.%20Zhang%20and%20Mathias%20L%C3%A9cuyer%20and%20Nidhi%20Hegde%0AAbstract%3A%20%20%20We%20study%20Thompson%20Sampling-based%20algorithms%20for%20stochastic%20bandits%20with%0Abounded%20rewards.%20As%20the%20existing%20problem-dependent%20regret%20bound%20for%20Thompson%0ASampling%20with%20Gaussian%20priors%20%5BAgrawal%20and%20Goyal%2C%202017%5D%20is%20vacuous%20when%20%24T%20%5Cle%0A288%20e%5E%7B64%7D%24%2C%20we%20derive%20a%20more%20practical%20bound%20that%20tightens%20the%20coefficient%20of%0Athe%20leading%20term%20%25from%20%24288%20e%5E%7B64%7D%24%20to%20%241270%24.%20Additionally%2C%20motivated%20by%0Alarge-scale%20real-world%20applications%20that%20require%20scalability%2C%20adaptive%0Acomputational%20resource%20allocation%2C%20and%20a%20balance%20in%20utility%20and%20computation%2C%20we%0Apropose%20two%20parameterized%20Thompson%20Sampling-based%20algorithms%3A%20Thompson%20Sampling%0Awith%20Model%20Aggregation%20%28TS-MA-%24%5Calpha%24%29%20and%20Thompson%20Sampling%20with%20Timestamp%0ADuelling%20%28TS-TD-%24%5Calpha%24%29%2C%20where%20%24%5Calpha%20%5Cin%20%5B0%2C1%5D%24%20controls%20the%20trade-off%0Abetween%20utility%20and%20computation.%20Both%20algorithms%20achieve%20%24O%0A%5Cleft%28K%5Cln%5E%7B%5Calpha%2B1%7D%28T%29/%5CDelta%20%5Cright%29%24%20regret%20bound%2C%20where%20%24K%24%20is%20the%20number%0Aof%20arms%2C%20%24T%24%20is%20the%20finite%20learning%20horizon%2C%20and%20%24%5CDelta%24%20denotes%20the%20single%0Around%20performance%20loss%20when%20pulling%20a%20sub-optimal%20arm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01010v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Adaptive%20Posterior%20Sampling%20Algorithms%20for%20Bandits&entry.906535625=Bingshan%20Hu%20and%20Zhiming%20Huang%20and%20Tianyue%20H.%20Zhang%20and%20Mathias%20L%C3%A9cuyer%20and%20Nidhi%20Hegde&entry.1292438233=%20%20We%20study%20Thompson%20Sampling-based%20algorithms%20for%20stochastic%20bandits%20with%0Abounded%20rewards.%20As%20the%20existing%20problem-dependent%20regret%20bound%20for%20Thompson%0ASampling%20with%20Gaussian%20priors%20%5BAgrawal%20and%20Goyal%2C%202017%5D%20is%20vacuous%20when%20%24T%20%5Cle%0A288%20e%5E%7B64%7D%24%2C%20we%20derive%20a%20more%20practical%20bound%20that%20tightens%20the%20coefficient%20of%0Athe%20leading%20term%20%25from%20%24288%20e%5E%7B64%7D%24%20to%20%241270%24.%20Additionally%2C%20motivated%20by%0Alarge-scale%20real-world%20applications%20that%20require%20scalability%2C%20adaptive%0Acomputational%20resource%20allocation%2C%20and%20a%20balance%20in%20utility%20and%20computation%2C%20we%0Apropose%20two%20parameterized%20Thompson%20Sampling-based%20algorithms%3A%20Thompson%20Sampling%0Awith%20Model%20Aggregation%20%28TS-MA-%24%5Calpha%24%29%20and%20Thompson%20Sampling%20with%20Timestamp%0ADuelling%20%28TS-TD-%24%5Calpha%24%29%2C%20where%20%24%5Calpha%20%5Cin%20%5B0%2C1%5D%24%20controls%20the%20trade-off%0Abetween%20utility%20and%20computation.%20Both%20algorithms%20achieve%20%24O%0A%5Cleft%28K%5Cln%5E%7B%5Calpha%2B1%7D%28T%29/%5CDelta%20%5Cright%29%24%20regret%20bound%2C%20where%20%24K%24%20is%20the%20number%0Aof%20arms%2C%20%24T%24%20is%20the%20finite%20learning%20horizon%2C%20and%20%24%5CDelta%24%20denotes%20the%20single%0Around%20performance%20loss%20when%20pulling%20a%20sub-optimal%20arm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01010v1&entry.124074799=Read"},
{"title": "Boosting Jailbreak Attack with Momentum", "author": "Yihao Zhang and Zeming Wei", "abstract": "  Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, yet they remain vulnerable to adversarial attacks, notably the\nwell-documented \\textit{jailbreak} attack. Recently, the Greedy Coordinate\nGradient (GCG) attack has demonstrated efficacy in exploiting this\nvulnerability by optimizing adversarial prompts through a combination of\ngradient heuristics and greedy search. However, the efficiency of this attack\nhas become a bottleneck in the attacking process. To mitigate this limitation,\nin this paper we rethink the generation of adversarial prompts through an\noptimization lens, aiming to stabilize the optimization process and harness\nmore heuristic insights from previous iterations. Specifically, we introduce\nthe \\textbf{M}omentum \\textbf{A}ccelerated G\\textbf{C}G (\\textbf{MAC}) attack,\nwhich incorporates a momentum term into the gradient heuristic. Experimental\nresults showcase the notable enhancement achieved by MAP in gradient-based\nattacks on aligned language models. Our code is available at\nhttps://github.com/weizeming/momentum-attack-llm.\n", "link": "http://arxiv.org/abs/2405.01229v1", "date": "2024-05-02", "relevancy": 2.3735, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.488}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4698}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4663}], "mailto": "mailto:daeR=997470421.yrtne&1v92210.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.mll-kcatta-mutnemom/gnimeziew/moc.buhtig//A3%sptthA0%ta02%elbaliava02%si02%edoc02%ruO02%.sledom02%egaugnal02%dengila02%no02%skcattaA0%desab-tneidarg02%ni02%PAM02%yb02%deveihca02%tnemecnahne02%elbaton02%eht02%esacwohs02%stluserA0%latnemirepxE02%.citsirueh02%tneidarg02%eht02%otni02%mret02%mutnemom02%a02%setaroprocni02%hcihwA0%C2%kcatta02%92%D7%CAMB7%fbtxetC5%82%02%GD7%CB7%fbtxetC5%G02%detareleccD7%AB7%fbtxetC5%02%mutnemoD7%MB7%fbtxetC5%02%ehtA0%ecudortni02%ew02%C2%yllacificepS02%.snoitareti02%suoiverp02%morf02%sthgisni02%citsirueh02%eromA0%ssenrah02%dna02%ssecorp02%noitazimitpo02%eht02%ezilibats02%ot02%gnimia02%C2%snel02%noitazimitpoA0%na02%hguorht02%stpmorp02%lairasrevda02%fo02%noitareneg02%eht02%knihter02%ew02%repap02%siht02%niA0%C2%noitatimil02%siht02%etagitim02%oT02%.ssecorp02%gnikcatta02%eht02%ni02%kcenelttob02%a02%emoceb02%sahA0%kcatta02%siht02%fo02%ycneiciffe02%eht02%C2%revewoH02%.hcraes02%ydeerg02%dna02%scitsirueh02%tneidargA0%fo02%noitanibmoc02%a02%hguorht02%stpmorp02%lairasrevda02%gnizimitpo02%yb02%ytilibarenluvA0%siht02%gnitiolpxe02%ni02%ycaciffe02%detartsnomed02%sah02%kcatta02%92%GCG82%02%tneidarGA0%etanidrooC02%ydeerG02%eht02%C2%yltneceR02%.kcatta02%D7%kaerbliajB7%titxetC5%02%detnemucod-llewA0%eht02%ylbaton02%C2%skcatta02%lairasrevda02%ot02%elbarenluv02%niamer02%yeht02%tey02%C2%sksatA0%esrevid02%ssorca02%sseccus02%elbakramer02%deveihca02%evah02%92%sMLL82%02%sledoM02%egaugnaL02%egraL02%02%=3328342921.yrtne&ieW02%gnimeZ02%dna02%gnahZ02%oahiY=526535609.yrtne&mutnemoM02%htiw02%kcattA02%kaerbliaJ02%gnitsooB=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Boosting%20Jailbreak%20Attack%20with%20Momentum&body=Title%3A%20Boosting%20Jailbreak%20Attack%20with%20Momentum%0AAuthor%3A%20Yihao%20Zhang%20and%20Zeming%20Wei%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20across%20diverse%0Atasks%2C%20yet%20they%20remain%20vulnerable%20to%20adversarial%20attacks%2C%20notably%20the%0Awell-documented%20%5Ctextit%7Bjailbreak%7D%20attack.%20Recently%2C%20the%20Greedy%20Coordinate%0AGradient%20%28GCG%29%20attack%20has%20demonstrated%20efficacy%20in%20exploiting%20this%0Avulnerability%20by%20optimizing%20adversarial%20prompts%20through%20a%20combination%20of%0Agradient%20heuristics%20and%20greedy%20search.%20However%2C%20the%20efficiency%20of%20this%20attack%0Ahas%20become%20a%20bottleneck%20in%20the%20attacking%20process.%20To%20mitigate%20this%20limitation%2C%0Ain%20this%20paper%20we%20rethink%20the%20generation%20of%20adversarial%20prompts%20through%20an%0Aoptimization%20lens%2C%20aiming%20to%20stabilize%20the%20optimization%20process%20and%20harness%0Amore%20heuristic%20insights%20from%20previous%20iterations.%20Specifically%2C%20we%20introduce%0Athe%20%5Ctextbf%7BM%7Domentum%20%5Ctextbf%7BA%7Dccelerated%20G%5Ctextbf%7BC%7DG%20%28%5Ctextbf%7BMAC%7D%29%20attack%2C%0Awhich%20incorporates%20a%20momentum%20term%20into%20the%20gradient%20heuristic.%20Experimental%0Aresults%20showcase%20the%20notable%20enhancement%20achieved%20by%20MAP%20in%20gradient-based%0Aattacks%20on%20aligned%20language%20models.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/weizeming/momentum-attack-llm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01229v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Jailbreak%20Attack%20with%20Momentum&entry.906535625=Yihao%20Zhang%20and%20Zeming%20Wei&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20achieved%20remarkable%20success%20across%20diverse%0Atasks%2C%20yet%20they%20remain%20vulnerable%20to%20adversarial%20attacks%2C%20notably%20the%0Awell-documented%20%5Ctextit%7Bjailbreak%7D%20attack.%20Recently%2C%20the%20Greedy%20Coordinate%0AGradient%20%28GCG%29%20attack%20has%20demonstrated%20efficacy%20in%20exploiting%20this%0Avulnerability%20by%20optimizing%20adversarial%20prompts%20through%20a%20combination%20of%0Agradient%20heuristics%20and%20greedy%20search.%20However%2C%20the%20efficiency%20of%20this%20attack%0Ahas%20become%20a%20bottleneck%20in%20the%20attacking%20process.%20To%20mitigate%20this%20limitation%2C%0Ain%20this%20paper%20we%20rethink%20the%20generation%20of%20adversarial%20prompts%20through%20an%0Aoptimization%20lens%2C%20aiming%20to%20stabilize%20the%20optimization%20process%20and%20harness%0Amore%20heuristic%20insights%20from%20previous%20iterations.%20Specifically%2C%20we%20introduce%0Athe%20%5Ctextbf%7BM%7Domentum%20%5Ctextbf%7BA%7Dccelerated%20G%5Ctextbf%7BC%7DG%20%28%5Ctextbf%7BMAC%7D%29%20attack%2C%0Awhich%20incorporates%20a%20momentum%20term%20into%20the%20gradient%20heuristic.%20Experimental%0Aresults%20showcase%20the%20notable%20enhancement%20achieved%20by%20MAP%20in%20gradient-based%0Aattacks%20on%20aligned%20language%20models.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/weizeming/momentum-attack-llm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01229v1&entry.124074799=Read"},
{"title": "USC: Uncompromising Spatial Constraints for Safety-Oriented 3D Object\n  Detectors in Autonomous Driving", "author": "Brian Hsuan-Cheng Liao and Chih-Hong Cheng and Hasan Esen and Alois Knoll", "abstract": "  We consider the safety-oriented performance of 3D object detectors in\nautonomous driving contexts. Specifically, despite impressive results shown by\nthe mass literature, developers often find it hard to ensure the safe\ndeployment of these learning-based perception models. Attributing the challenge\nto the lack of safety-oriented metrics, we hereby present uncompromising\nspatial constraints (USC), which characterize a simple yet important\nlocalization requirement demanding the predictions to fully cover the objects\nwhen seen from the autonomous vehicle. The constraints, as we formulate using\nthe perspective and bird's-eye views, can be naturally reflected by\nquantitative measures, such that having an object detector with a higher score\nimplies a lower risk of collision. Finally, beyond model evaluation, we\nincorporate the quantitative measures into common loss functions to enable\nsafety-oriented fine-tuning for existing models. With experiments using the\nnuScenes dataset and a closed-loop simulation, our work demonstrates such\nconsiderations of safety notions at the perception level not only improve model\nperformances beyond accuracy but also allow for a more direct linkage to actual\nsystem safety.\n", "link": "http://arxiv.org/abs/2209.10368v4", "date": "2024-05-02", "relevancy": 2.3647, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.629}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5862}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5553}], "mailto": "mailto:daeR=997470421.yrtne&4v86301.9022/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.ytefas02%metsysA0%lautca02%ot02%egaknil02%tcerid02%erom02%a02%rof02%wolla02%osla02%tub02%ycarucca02%dnoyeb02%secnamrofrepA0%ledom02%evorpmi02%ylno02%ton02%level02%noitpecrep02%eht02%ta02%snoiton02%ytefas02%fo02%snoitaredisnocA0%hcus02%setartsnomed02%krow02%ruo02%C2%noitalumis02%pool-desolc02%a02%dna02%tesatad02%senecSunA0%eht02%gnisu02%stnemirepxe02%htiW02%.sledom02%gnitsixe02%rof02%gninut-enif02%detneiro-ytefasA0%elbane02%ot02%snoitcnuf02%ssol02%nommoc02%otni02%serusaem02%evitatitnauq02%eht02%etaroprocniA0%ew02%C2%noitaulave02%ledom02%dnoyeb02%C2%yllaniF02%.noisilloc02%fo02%ksir02%rewol02%a02%seilpmiA0%erocs02%rehgih02%a02%htiw02%rotceted02%tcejbo02%na02%gnivah02%taht02%hcus02%C2%serusaem02%evitatitnauqA0%yb02%detcelfer02%yllarutan02%eb02%nac02%C2%sweiv02%eye-s72%drib02%dna02%evitcepsrep02%ehtA0%gnisu02%etalumrof02%ew02%sa02%C2%stniartsnoc02%ehT02%.elcihev02%suomonotua02%eht02%morf02%nees02%nehwA0%stcejbo02%eht02%revoc02%ylluf02%ot02%snoitciderp02%eht02%gnidnamed02%tnemeriuqer02%noitazilacolA0%tnatropmi02%tey02%elpmis02%a02%eziretcarahc02%hcihw02%C2%92%CSU82%02%stniartsnoc02%laitapsA0%gnisimorpmocnu02%tneserp02%ybereh02%ew02%C2%scirtem02%detneiro-ytefas02%fo02%kcal02%eht02%otA0%egnellahc02%eht02%gnitubirttA02%.sledom02%noitpecrep02%desab-gninrael02%eseht02%fo02%tnemyolpedA0%efas02%eht02%erusne02%ot02%drah02%ti02%dnif02%netfo02%srepoleved02%C2%erutaretil02%ssam02%ehtA0%yb02%nwohs02%stluser02%evisserpmi02%etipsed02%C2%yllacificepS02%.stxetnoc02%gnivird02%suomonotuaA0%ni02%srotceted02%tcejbo02%D302%fo02%ecnamrofrep02%detneiro-ytefas02%eht02%redisnoc02%eW02%02%=3328342921.yrtne&llonK02%siolA02%dna02%nesE02%nasaH02%dna02%gnehC02%gnoH-hihC02%dna02%oaiL02%gnehC-nausH02%nairB=526535609.yrtne&gnivirD02%suomonotuA02%ni02%srotceteD02%02%A0%tcejbO02%D302%detneirO-ytefaS02%rof02%stniartsnoC02%laitapS02%gnisimorpmocnU02%A3%CSU=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20USC%3A%20Uncompromising%20Spatial%20Constraints%20for%20Safety-Oriented%203D%20Object%0A%20%20Detectors%20in%20Autonomous%20Driving&body=Title%3A%20USC%3A%20Uncompromising%20Spatial%20Constraints%20for%20Safety-Oriented%203D%20Object%0A%20%20Detectors%20in%20Autonomous%20Driving%0AAuthor%3A%20Brian%20Hsuan-Cheng%20Liao%20and%20Chih-Hong%20Cheng%20and%20Hasan%20Esen%20and%20Alois%20Knoll%0AAbstract%3A%20%20%20We%20consider%20the%20safety-oriented%20performance%20of%203D%20object%20detectors%20in%0Aautonomous%20driving%20contexts.%20Specifically%2C%20despite%20impressive%20results%20shown%20by%0Athe%20mass%20literature%2C%20developers%20often%20find%20it%20hard%20to%20ensure%20the%20safe%0Adeployment%20of%20these%20learning-based%20perception%20models.%20Attributing%20the%20challenge%0Ato%20the%20lack%20of%20safety-oriented%20metrics%2C%20we%20hereby%20present%20uncompromising%0Aspatial%20constraints%20%28USC%29%2C%20which%20characterize%20a%20simple%20yet%20important%0Alocalization%20requirement%20demanding%20the%20predictions%20to%20fully%20cover%20the%20objects%0Awhen%20seen%20from%20the%20autonomous%20vehicle.%20The%20constraints%2C%20as%20we%20formulate%20using%0Athe%20perspective%20and%20bird%27s-eye%20views%2C%20can%20be%20naturally%20reflected%20by%0Aquantitative%20measures%2C%20such%20that%20having%20an%20object%20detector%20with%20a%20higher%20score%0Aimplies%20a%20lower%20risk%20of%20collision.%20Finally%2C%20beyond%20model%20evaluation%2C%20we%0Aincorporate%20the%20quantitative%20measures%20into%20common%20loss%20functions%20to%20enable%0Asafety-oriented%20fine-tuning%20for%20existing%20models.%20With%20experiments%20using%20the%0AnuScenes%20dataset%20and%20a%20closed-loop%20simulation%2C%20our%20work%20demonstrates%20such%0Aconsiderations%20of%20safety%20notions%20at%20the%20perception%20level%20not%20only%20improve%20model%0Aperformances%20beyond%20accuracy%20but%20also%20allow%20for%20a%20more%20direct%20linkage%20to%20actual%0Asystem%20safety.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.10368v4%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=USC%3A%20Uncompromising%20Spatial%20Constraints%20for%20Safety-Oriented%203D%20Object%0A%20%20Detectors%20in%20Autonomous%20Driving&entry.906535625=Brian%20Hsuan-Cheng%20Liao%20and%20Chih-Hong%20Cheng%20and%20Hasan%20Esen%20and%20Alois%20Knoll&entry.1292438233=%20%20We%20consider%20the%20safety-oriented%20performance%20of%203D%20object%20detectors%20in%0Aautonomous%20driving%20contexts.%20Specifically%2C%20despite%20impressive%20results%20shown%20by%0Athe%20mass%20literature%2C%20developers%20often%20find%20it%20hard%20to%20ensure%20the%20safe%0Adeployment%20of%20these%20learning-based%20perception%20models.%20Attributing%20the%20challenge%0Ato%20the%20lack%20of%20safety-oriented%20metrics%2C%20we%20hereby%20present%20uncompromising%0Aspatial%20constraints%20%28USC%29%2C%20which%20characterize%20a%20simple%20yet%20important%0Alocalization%20requirement%20demanding%20the%20predictions%20to%20fully%20cover%20the%20objects%0Awhen%20seen%20from%20the%20autonomous%20vehicle.%20The%20constraints%2C%20as%20we%20formulate%20using%0Athe%20perspective%20and%20bird%27s-eye%20views%2C%20can%20be%20naturally%20reflected%20by%0Aquantitative%20measures%2C%20such%20that%20having%20an%20object%20detector%20with%20a%20higher%20score%0Aimplies%20a%20lower%20risk%20of%20collision.%20Finally%2C%20beyond%20model%20evaluation%2C%20we%0Aincorporate%20the%20quantitative%20measures%20into%20common%20loss%20functions%20to%20enable%0Asafety-oriented%20fine-tuning%20for%20existing%20models.%20With%20experiments%20using%20the%0AnuScenes%20dataset%20and%20a%20closed-loop%20simulation%2C%20our%20work%20demonstrates%20such%0Aconsiderations%20of%20safety%20notions%20at%20the%20perception%20level%20not%20only%20improve%20model%0Aperformances%20beyond%20accuracy%20but%20also%20allow%20for%20a%20more%20direct%20linkage%20to%20actual%0Asystem%20safety.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.10368v4&entry.124074799=Read"},
{"title": "Low-resource speech recognition and dialect identification of Irish in a\n  multi-task framework", "author": "Liam Lonergan and Mengjie Qian and Neasa N\u00ed Chiar\u00e1in and Christer Gobl and Ailbhe N\u00ed Chasaide", "abstract": "  This paper explores the use of Hybrid CTC/Attention encoder-decoder models\ntrained with Intermediate CTC (InterCTC) for Irish (Gaelic) low-resource speech\nrecognition (ASR) and dialect identification (DID). Results are compared to the\ncurrent best performing models trained for ASR (TDNN-HMM) and DID (ECAPA-TDNN).\nAn optimal InterCTC setting is initially established using a Conformer encoder.\nThis setting is then used to train a model with an E-branchformer encoder and\nthe performance of both architectures are compared. A multi-task fine-tuning\napproach is adopted for language model (LM) shallow fusion. The experiments\nyielded an improvement in DID accuracy of 10.8% relative to a baseline\nECAPA-TDNN, and WER performance approaching the TDNN-HMM model. This multi-task\napproach emerges as a promising strategy for Irish low-resource ASR and DID.\n", "link": "http://arxiv.org/abs/2405.01293v1", "date": "2024-05-02", "relevancy": 2.3626, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4896}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4793}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4487}], "mailto": "mailto:daeR=997470421.yrtne&1v39210.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.DID02%dna02%RSA02%ecruoser-wol02%hsirI02%rof02%ygetarts02%gnisimorp02%a02%sa02%segreme02%hcaorppaA0%ksat-itlum02%sihT02%.ledom02%MMH-NNDT02%eht02%gnihcaorppa02%ecnamrofrep02%REW02%dna02%C2%NNDT-APACEA0%enilesab02%a02%ot02%evitaler02%52%8.0102%fo02%ycarucca02%DID02%ni02%tnemevorpmi02%na02%dedleiyA0%stnemirepxe02%ehT02%.noisuf02%wollahs02%92%ML82%02%ledom02%egaugnal02%rof02%detpoda02%si02%hcaorppaA0%gninut-enif02%ksat-itlum02%A02%.derapmoc02%era02%serutcetihcra02%htob02%fo02%ecnamrofrep02%ehtA0%dna02%redocne02%remrofhcnarb-E02%na02%htiw02%ledom02%a02%niart02%ot02%desu02%neht02%si02%gnittes02%sihTA0%.redocne02%remrofnoC02%a02%gnisu02%dehsilbatse02%yllaitini02%si02%gnittes02%CTCretnI02%lamitpo02%nAA0%.92%NNDT-APACE82%02%DID02%dna02%92%MMH-NNDT82%02%RSA02%rof02%deniart02%sledom02%gnimrofrep02%tseb02%tnerrucA0%eht02%ot02%derapmoc02%era02%stluseR02%.92%DID82%02%noitacifitnedi02%tcelaid02%dna02%92%RSA82%02%noitingocerA0%hceeps02%ecruoser-wol02%92%cileaG82%02%hsirI02%rof02%92%CTCretnI82%02%CTC02%etaidemretnI02%htiw02%deniartA0%sledom02%redoced-redocne02%noitnettA/CTC02%dirbyH02%fo02%esu02%eht02%serolpxe02%repap02%sihT02%02%=3328342921.yrtne&ediasahC02%DA%3C%N02%ehbliA02%dna02%lboG02%retsirhC02%dna02%ni1A%3C%raihC02%DA%3C%N02%asaeN02%dna02%naiQ02%eijgneM02%dna02%nagrenoL02%maiL=526535609.yrtne&krowemarf02%ksat-itlum02%02%A0%a02%ni02%hsirI02%fo02%noitacifitnedi02%tcelaid02%dna02%noitingocer02%hceeps02%ecruoser-woL=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Low-resource%20speech%20recognition%20and%20dialect%20identification%20of%20Irish%20in%20a%0A%20%20multi-task%20framework&body=Title%3A%20Low-resource%20speech%20recognition%20and%20dialect%20identification%20of%20Irish%20in%20a%0A%20%20multi-task%20framework%0AAuthor%3A%20Liam%20Lonergan%20and%20Mengjie%20Qian%20and%20Neasa%20N%C3%AD%20Chiar%C3%A1in%20and%20Christer%20Gobl%20and%20Ailbhe%20N%C3%AD%20Chasaide%0AAbstract%3A%20%20%20This%20paper%20explores%20the%20use%20of%20Hybrid%20CTC/Attention%20encoder-decoder%20models%0Atrained%20with%20Intermediate%20CTC%20%28InterCTC%29%20for%20Irish%20%28Gaelic%29%20low-resource%20speech%0Arecognition%20%28ASR%29%20and%20dialect%20identification%20%28DID%29.%20Results%20are%20compared%20to%20the%0Acurrent%20best%20performing%20models%20trained%20for%20ASR%20%28TDNN-HMM%29%20and%20DID%20%28ECAPA-TDNN%29.%0AAn%20optimal%20InterCTC%20setting%20is%20initially%20established%20using%20a%20Conformer%20encoder.%0AThis%20setting%20is%20then%20used%20to%20train%20a%20model%20with%20an%20E-branchformer%20encoder%20and%0Athe%20performance%20of%20both%20architectures%20are%20compared.%20A%20multi-task%20fine-tuning%0Aapproach%20is%20adopted%20for%20language%20model%20%28LM%29%20shallow%20fusion.%20The%20experiments%0Ayielded%20an%20improvement%20in%20DID%20accuracy%20of%2010.8%25%20relative%20to%20a%20baseline%0AECAPA-TDNN%2C%20and%20WER%20performance%20approaching%20the%20TDNN-HMM%20model.%20This%20multi-task%0Aapproach%20emerges%20as%20a%20promising%20strategy%20for%20Irish%20low-resource%20ASR%20and%20DID.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01293v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Low-resource%20speech%20recognition%20and%20dialect%20identification%20of%20Irish%20in%20a%0A%20%20multi-task%20framework&entry.906535625=Liam%20Lonergan%20and%20Mengjie%20Qian%20and%20Neasa%20N%C3%AD%20Chiar%C3%A1in%20and%20Christer%20Gobl%20and%20Ailbhe%20N%C3%AD%20Chasaide&entry.1292438233=%20%20This%20paper%20explores%20the%20use%20of%20Hybrid%20CTC/Attention%20encoder-decoder%20models%0Atrained%20with%20Intermediate%20CTC%20%28InterCTC%29%20for%20Irish%20%28Gaelic%29%20low-resource%20speech%0Arecognition%20%28ASR%29%20and%20dialect%20identification%20%28DID%29.%20Results%20are%20compared%20to%20the%0Acurrent%20best%20performing%20models%20trained%20for%20ASR%20%28TDNN-HMM%29%20and%20DID%20%28ECAPA-TDNN%29.%0AAn%20optimal%20InterCTC%20setting%20is%20initially%20established%20using%20a%20Conformer%20encoder.%0AThis%20setting%20is%20then%20used%20to%20train%20a%20model%20with%20an%20E-branchformer%20encoder%20and%0Athe%20performance%20of%20both%20architectures%20are%20compared.%20A%20multi-task%20fine-tuning%0Aapproach%20is%20adopted%20for%20language%20model%20%28LM%29%20shallow%20fusion.%20The%20experiments%0Ayielded%20an%20improvement%20in%20DID%20accuracy%20of%2010.8%25%20relative%20to%20a%20baseline%0AECAPA-TDNN%2C%20and%20WER%20performance%20approaching%20the%20TDNN-HMM%20model.%20This%20multi-task%0Aapproach%20emerges%20as%20a%20promising%20strategy%20for%20Irish%20low-resource%20ASR%20and%20DID.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01293v1&entry.124074799=Read"},
{"title": "Multi-Space Alignments Towards Universal LiDAR Segmentation", "author": "Youquan Liu and Lingdong Kong and Xiaoyang Wu and Runnan Chen and Xin Li and Liang Pan and Ziwei Liu and Yuexin Ma", "abstract": "  A unified and versatile LiDAR segmentation model with strong robustness and\ngeneralizability is desirable for safe autonomous driving perception. This work\npresents M3Net, a one-of-a-kind framework for fulfilling multi-task,\nmulti-dataset, multi-modality LiDAR segmentation in a universal manner using\njust a single set of parameters. To better exploit data volume and diversity,\nwe first combine large-scale driving datasets acquired by different types of\nsensors from diverse scenes and then conduct alignments in three spaces, namely\ndata, feature, and label spaces, during the training. As a result, M3Net is\ncapable of taming heterogeneous data for training state-of-the-art LiDAR\nsegmentation models. Extensive experiments on twelve LiDAR segmentation\ndatasets verify our effectiveness. Notably, using a shared set of parameters,\nM3Net achieves 75.1%, 83.1%, and 72.4% mIoU scores, respectively, on the\nofficial benchmarks of SemanticKITTI, nuScenes, and Waymo Open.\n", "link": "http://arxiv.org/abs/2405.01538v1", "date": "2024-05-02", "relevancy": 2.3607, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5946}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5912}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5766}], "mailto": "mailto:daeR=997470421.yrtne&1v83510.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.nepO02%omyaW02%dna02%C2%senecSun02%C2%ITTIKcitnameS02%fo02%skramhcneb02%laiciffoA0%eht02%no02%C2%ylevitcepser02%C2%serocs02%UoIm02%52%4.2702%dna02%C2%52%1.3802%C2%52%1.5702%seveihca02%teN3MA0%C2%sretemarap02%fo02%tes02%derahs02%a02%gnisu02%C2%ylbatoN02%.ssenevitceffe02%ruo02%yfirev02%stesatadA0%noitatnemges02%RADiL02%evlewt02%no02%stnemirepxe02%evisnetxE02%.sledom02%noitatnemgesA0%RADiL02%tra-eht-fo-etats02%gniniart02%rof02%atad02%suoenegoreteh02%gnimat02%fo02%elbapacA0%si02%teN3M02%C2%tluser02%a02%sA02%.gniniart02%eht02%gnirud02%C2%secaps02%lebal02%dna02%C2%erutaef02%C2%atadA0%yleman02%C2%secaps02%eerht02%ni02%stnemngila02%tcudnoc02%neht02%dna02%senecs02%esrevid02%morf02%srosnesA0%fo02%sepyt02%tnereffid02%yb02%deriuqca02%stesatad02%gnivird02%elacs-egral02%enibmoc02%tsrif02%ewA0%C2%ytisrevid02%dna02%emulov02%atad02%tiolpxe02%retteb02%oT02%.sretemarap02%fo02%tes02%elgnis02%a02%tsujA0%gnisu02%rennam02%lasrevinu02%a02%ni02%noitatnemges02%RADiL02%ytiladom-itlum02%C2%tesatad-itlumA0%C2%ksat-itlum02%gnillifluf02%rof02%krowemarf02%dnik-a-fo-eno02%a02%C2%teN3M02%stneserpA0%krow02%sihT02%.noitpecrep02%gnivird02%suomonotua02%efas02%rof02%elbarised02%si02%ytilibazilarenegA0%dna02%ssentsubor02%gnorts02%htiw02%ledom02%noitatnemges02%RADiL02%elitasrev02%dna02%deifinu02%A02%02%=3328342921.yrtne&aM02%nixeuY02%dna02%uiL02%iewiZ02%dna02%naP02%gnaiL02%dna02%iL02%niX02%dna02%nehC02%nannuR02%dna02%uW02%gnayoaiX02%dna02%gnoK02%gnodgniL02%dna02%uiL02%nauquoY=526535609.yrtne&noitatnemgeS02%RADiL02%lasrevinU02%sdrawoT02%stnemngilA02%ecapS-itluM=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Multi-Space%20Alignments%20Towards%20Universal%20LiDAR%20Segmentation&body=Title%3A%20Multi-Space%20Alignments%20Towards%20Universal%20LiDAR%20Segmentation%0AAuthor%3A%20Youquan%20Liu%20and%20Lingdong%20Kong%20and%20Xiaoyang%20Wu%20and%20Runnan%20Chen%20and%20Xin%20Li%20and%20Liang%20Pan%20and%20Ziwei%20Liu%20and%20Yuexin%20Ma%0AAbstract%3A%20%20%20A%20unified%20and%20versatile%20LiDAR%20segmentation%20model%20with%20strong%20robustness%20and%0Ageneralizability%20is%20desirable%20for%20safe%20autonomous%20driving%20perception.%20This%20work%0Apresents%20M3Net%2C%20a%20one-of-a-kind%20framework%20for%20fulfilling%20multi-task%2C%0Amulti-dataset%2C%20multi-modality%20LiDAR%20segmentation%20in%20a%20universal%20manner%20using%0Ajust%20a%20single%20set%20of%20parameters.%20To%20better%20exploit%20data%20volume%20and%20diversity%2C%0Awe%20first%20combine%20large-scale%20driving%20datasets%20acquired%20by%20different%20types%20of%0Asensors%20from%20diverse%20scenes%20and%20then%20conduct%20alignments%20in%20three%20spaces%2C%20namely%0Adata%2C%20feature%2C%20and%20label%20spaces%2C%20during%20the%20training.%20As%20a%20result%2C%20M3Net%20is%0Acapable%20of%20taming%20heterogeneous%20data%20for%20training%20state-of-the-art%20LiDAR%0Asegmentation%20models.%20Extensive%20experiments%20on%20twelve%20LiDAR%20segmentation%0Adatasets%20verify%20our%20effectiveness.%20Notably%2C%20using%20a%20shared%20set%20of%20parameters%2C%0AM3Net%20achieves%2075.1%25%2C%2083.1%25%2C%20and%2072.4%25%20mIoU%20scores%2C%20respectively%2C%20on%20the%0Aofficial%20benchmarks%20of%20SemanticKITTI%2C%20nuScenes%2C%20and%20Waymo%20Open.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01538v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Space%20Alignments%20Towards%20Universal%20LiDAR%20Segmentation&entry.906535625=Youquan%20Liu%20and%20Lingdong%20Kong%20and%20Xiaoyang%20Wu%20and%20Runnan%20Chen%20and%20Xin%20Li%20and%20Liang%20Pan%20and%20Ziwei%20Liu%20and%20Yuexin%20Ma&entry.1292438233=%20%20A%20unified%20and%20versatile%20LiDAR%20segmentation%20model%20with%20strong%20robustness%20and%0Ageneralizability%20is%20desirable%20for%20safe%20autonomous%20driving%20perception.%20This%20work%0Apresents%20M3Net%2C%20a%20one-of-a-kind%20framework%20for%20fulfilling%20multi-task%2C%0Amulti-dataset%2C%20multi-modality%20LiDAR%20segmentation%20in%20a%20universal%20manner%20using%0Ajust%20a%20single%20set%20of%20parameters.%20To%20better%20exploit%20data%20volume%20and%20diversity%2C%0Awe%20first%20combine%20large-scale%20driving%20datasets%20acquired%20by%20different%20types%20of%0Asensors%20from%20diverse%20scenes%20and%20then%20conduct%20alignments%20in%20three%20spaces%2C%20namely%0Adata%2C%20feature%2C%20and%20label%20spaces%2C%20during%20the%20training.%20As%20a%20result%2C%20M3Net%20is%0Acapable%20of%20taming%20heterogeneous%20data%20for%20training%20state-of-the-art%20LiDAR%0Asegmentation%20models.%20Extensive%20experiments%20on%20twelve%20LiDAR%20segmentation%0Adatasets%20verify%20our%20effectiveness.%20Notably%2C%20using%20a%20shared%20set%20of%20parameters%2C%0AM3Net%20achieves%2075.1%25%2C%2083.1%25%2C%20and%2072.4%25%20mIoU%20scores%2C%20respectively%2C%20on%20the%0Aofficial%20benchmarks%20of%20SemanticKITTI%2C%20nuScenes%2C%20and%20Waymo%20Open.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01538v1&entry.124074799=Read"},
{"title": "MiniGPT-3D: Efficiently Aligning 3D Point Clouds with Large Language\n  Models using 2D Priors", "author": "Yuan Tang and Xu Han and Xianzhi Li and Qiao Yu and Yixue Hao and Long Hu and Min Chen", "abstract": "  Large 2D vision-language models (2D-LLMs) have gained significant attention\nby bridging Large Language Models (LLMs) with images using a simple projector.\nInspired by their success, large 3D point cloud-language models (3D-LLMs) also\nintegrate point clouds into LLMs. However, directly aligning point clouds with\nLLM requires expensive training costs, typically in hundreds of GPU-hours on\nA100, which hinders the development of 3D-LLMs. In this paper, we introduce\nMiniGPT-3D, an efficient and powerful 3D-LLM that achieves multiple SOTA\nresults while training for only 27 hours on one RTX 3090. Specifically, we\npropose to align 3D point clouds with LLMs using 2D priors from 2D-LLMs, which\ncan leverage the similarity between 2D and 3D visual information. We introduce\na novel four-stage training strategy for modality alignment in a cascaded way,\nand a mixture of query experts module to adaptively aggregate features with\nhigh efficiency. Moreover, we utilize parameter-efficient fine-tuning methods\nLoRA and Norm fine-tuning, resulting in only 47.8M learnable parameters, which\nis up to 260x fewer than existing methods. Extensive experiments show that\nMiniGPT-3D achieves SOTA on 3D object classification and captioning tasks, with\nsignificantly cheaper training costs. Notably, MiniGPT-3D gains an 8.12\nincrease on GPT-4 evaluation score for the challenging object captioning task\ncompared to ShapeLLM-13B, while the latter costs 160 total GPU-hours on 8 A800.\nWe are the first to explore the efficient 3D-LLM, offering new insights to the\ncommunity. Code and weights are available at\nhttps://github.com/TangYuan96/MiniGPT-3D.\n", "link": "http://arxiv.org/abs/2405.01413v1", "date": "2024-05-02", "relevancy": 2.3549, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6034}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5864}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5576}], "mailto": "mailto:daeR=997470421.yrtne&1v31410.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.D3-TPGiniM/69nauYgnaT/moc.buhtig//A3%sptthA0%ta02%elbaliava02%era02%sthgiew02%dna02%edoC02%.ytinummocA0%eht02%ot02%sthgisni02%wen02%gnireffo02%C2%MLL-D302%tneiciffe02%eht02%erolpxe02%ot02%tsrif02%eht02%era02%eWA0%.008A02%802%no02%sruoh-UPG02%latot02%06102%stsoc02%rettal02%eht02%elihw02%C2%B31-MLLepahS02%ot02%derapmocA0%ksat02%gninoitpac02%tcejbo02%gnignellahc02%eht02%rof02%erocs02%noitaulave02%4-TPG02%no02%esaercniA0%21.802%na02%sniag02%D3-TPGiniM02%C2%ylbatoN02%.stsoc02%gniniart02%repaehc02%yltnacifingisA0%htiw02%C2%sksat02%gninoitpac02%dna02%noitacifissalc02%tcejbo02%D302%no02%ATOS02%seveihca02%D3-TPGiniMA0%taht02%wohs02%stnemirepxe02%evisnetxE02%.sdohtem02%gnitsixe02%naht02%rewef02%x06202%ot02%pu02%siA0%hcihw02%C2%sretemarap02%elbanrael02%M8.7402%ylno02%ni02%gnitluser02%C2%gninut-enif02%mroN02%dna02%ARoLA0%sdohtem02%gninut-enif02%tneiciffe-retemarap02%ezilitu02%ew02%C2%revoeroM02%.ycneiciffe02%hgihA0%htiw02%serutaef02%etagergga02%ylevitpada02%ot02%eludom02%strepxe02%yreuq02%fo02%erutxim02%a02%dnaA0%C2%yaw02%dedacsac02%a02%ni02%tnemngila02%ytiladom02%rof02%ygetarts02%gniniart02%egats-ruof02%levon02%aA0%ecudortni02%eW02%.noitamrofni02%lausiv02%D302%dna02%D202%neewteb02%ytiralimis02%eht02%egarevel02%nacA0%hcihw02%C2%sMLL-D202%morf02%sroirp02%D202%gnisu02%sMLL02%htiw02%sduolc02%tniop02%D302%ngila02%ot02%esoporpA0%ew02%C2%yllacificepS02%.090302%XTR02%eno02%no02%sruoh02%7202%ylno02%rof02%gniniart02%elihw02%stluserA0%ATOS02%elpitlum02%seveihca02%taht02%MLL-D302%lufrewop02%dna02%tneiciffe02%na02%C2%D3-TPGiniMA0%ecudortni02%ew02%C2%repap02%siht02%nI02%.sMLL-D302%fo02%tnempoleved02%eht02%srednih02%hcihw02%C2%001AA0%no02%sruoh-UPG02%fo02%sderdnuh02%ni02%yllacipyt02%C2%stsoc02%gniniart02%evisnepxe02%seriuqer02%MLLA0%htiw02%sduolc02%tniop02%gningila02%yltcerid02%C2%revewoH02%.sMLL02%otni02%sduolc02%tniop02%etargetniA0%osla02%92%sMLL-D382%02%sledom02%egaugnal-duolc02%tniop02%D302%egral02%C2%sseccus02%rieht02%yb02%deripsnIA0%.rotcejorp02%elpmis02%a02%gnisu02%segami02%htiw02%92%sMLL82%02%sledoM02%egaugnaL02%egraL02%gnigdirb02%ybA0%noitnetta02%tnacifingis02%deniag02%evah02%92%sMLL-D282%02%sledom02%egaugnal-noisiv02%D202%egraL02%02%=3328342921.yrtne&nehC02%niM02%dna02%uH02%gnoL02%dna02%oaH02%euxiY02%dna02%uY02%oaiQ02%dna02%iL02%ihznaiX02%dna02%naH02%uX02%dna02%gnaT02%nauY=526535609.yrtne&sroirP02%D202%gnisu02%sledoM02%02%A0%egaugnaL02%egraL02%htiw02%sduolC02%tnioP02%D302%gningilA02%yltneiciffE02%A3%D3-TPGiniM=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20MiniGPT-3D%3A%20Efficiently%20Aligning%203D%20Point%20Clouds%20with%20Large%20Language%0A%20%20Models%20using%202D%20Priors&body=Title%3A%20MiniGPT-3D%3A%20Efficiently%20Aligning%203D%20Point%20Clouds%20with%20Large%20Language%0A%20%20Models%20using%202D%20Priors%0AAuthor%3A%20Yuan%20Tang%20and%20Xu%20Han%20and%20Xianzhi%20Li%20and%20Qiao%20Yu%20and%20Yixue%20Hao%20and%20Long%20Hu%20and%20Min%20Chen%0AAbstract%3A%20%20%20Large%202D%20vision-language%20models%20%282D-LLMs%29%20have%20gained%20significant%20attention%0Aby%20bridging%20Large%20Language%20Models%20%28LLMs%29%20with%20images%20using%20a%20simple%20projector.%0AInspired%20by%20their%20success%2C%20large%203D%20point%20cloud-language%20models%20%283D-LLMs%29%20also%0Aintegrate%20point%20clouds%20into%20LLMs.%20However%2C%20directly%20aligning%20point%20clouds%20with%0ALLM%20requires%20expensive%20training%20costs%2C%20typically%20in%20hundreds%20of%20GPU-hours%20on%0AA100%2C%20which%20hinders%20the%20development%20of%203D-LLMs.%20In%20this%20paper%2C%20we%20introduce%0AMiniGPT-3D%2C%20an%20efficient%20and%20powerful%203D-LLM%20that%20achieves%20multiple%20SOTA%0Aresults%20while%20training%20for%20only%2027%20hours%20on%20one%20RTX%203090.%20Specifically%2C%20we%0Apropose%20to%20align%203D%20point%20clouds%20with%20LLMs%20using%202D%20priors%20from%202D-LLMs%2C%20which%0Acan%20leverage%20the%20similarity%20between%202D%20and%203D%20visual%20information.%20We%20introduce%0Aa%20novel%20four-stage%20training%20strategy%20for%20modality%20alignment%20in%20a%20cascaded%20way%2C%0Aand%20a%20mixture%20of%20query%20experts%20module%20to%20adaptively%20aggregate%20features%20with%0Ahigh%20efficiency.%20Moreover%2C%20we%20utilize%20parameter-efficient%20fine-tuning%20methods%0ALoRA%20and%20Norm%20fine-tuning%2C%20resulting%20in%20only%2047.8M%20learnable%20parameters%2C%20which%0Ais%20up%20to%20260x%20fewer%20than%20existing%20methods.%20Extensive%20experiments%20show%20that%0AMiniGPT-3D%20achieves%20SOTA%20on%203D%20object%20classification%20and%20captioning%20tasks%2C%20with%0Asignificantly%20cheaper%20training%20costs.%20Notably%2C%20MiniGPT-3D%20gains%20an%208.12%0Aincrease%20on%20GPT-4%20evaluation%20score%20for%20the%20challenging%20object%20captioning%20task%0Acompared%20to%20ShapeLLM-13B%2C%20while%20the%20latter%20costs%20160%20total%20GPU-hours%20on%208%20A800.%0AWe%20are%20the%20first%20to%20explore%20the%20efficient%203D-LLM%2C%20offering%20new%20insights%20to%20the%0Acommunity.%20Code%20and%20weights%20are%20available%20at%0Ahttps%3A//github.com/TangYuan96/MiniGPT-3D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01413v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiniGPT-3D%3A%20Efficiently%20Aligning%203D%20Point%20Clouds%20with%20Large%20Language%0A%20%20Models%20using%202D%20Priors&entry.906535625=Yuan%20Tang%20and%20Xu%20Han%20and%20Xianzhi%20Li%20and%20Qiao%20Yu%20and%20Yixue%20Hao%20and%20Long%20Hu%20and%20Min%20Chen&entry.1292438233=%20%20Large%202D%20vision-language%20models%20%282D-LLMs%29%20have%20gained%20significant%20attention%0Aby%20bridging%20Large%20Language%20Models%20%28LLMs%29%20with%20images%20using%20a%20simple%20projector.%0AInspired%20by%20their%20success%2C%20large%203D%20point%20cloud-language%20models%20%283D-LLMs%29%20also%0Aintegrate%20point%20clouds%20into%20LLMs.%20However%2C%20directly%20aligning%20point%20clouds%20with%0ALLM%20requires%20expensive%20training%20costs%2C%20typically%20in%20hundreds%20of%20GPU-hours%20on%0AA100%2C%20which%20hinders%20the%20development%20of%203D-LLMs.%20In%20this%20paper%2C%20we%20introduce%0AMiniGPT-3D%2C%20an%20efficient%20and%20powerful%203D-LLM%20that%20achieves%20multiple%20SOTA%0Aresults%20while%20training%20for%20only%2027%20hours%20on%20one%20RTX%203090.%20Specifically%2C%20we%0Apropose%20to%20align%203D%20point%20clouds%20with%20LLMs%20using%202D%20priors%20from%202D-LLMs%2C%20which%0Acan%20leverage%20the%20similarity%20between%202D%20and%203D%20visual%20information.%20We%20introduce%0Aa%20novel%20four-stage%20training%20strategy%20for%20modality%20alignment%20in%20a%20cascaded%20way%2C%0Aand%20a%20mixture%20of%20query%20experts%20module%20to%20adaptively%20aggregate%20features%20with%0Ahigh%20efficiency.%20Moreover%2C%20we%20utilize%20parameter-efficient%20fine-tuning%20methods%0ALoRA%20and%20Norm%20fine-tuning%2C%20resulting%20in%20only%2047.8M%20learnable%20parameters%2C%20which%0Ais%20up%20to%20260x%20fewer%20than%20existing%20methods.%20Extensive%20experiments%20show%20that%0AMiniGPT-3D%20achieves%20SOTA%20on%203D%20object%20classification%20and%20captioning%20tasks%2C%20with%0Asignificantly%20cheaper%20training%20costs.%20Notably%2C%20MiniGPT-3D%20gains%20an%208.12%0Aincrease%20on%20GPT-4%20evaluation%20score%20for%20the%20challenging%20object%20captioning%20task%0Acompared%20to%20ShapeLLM-13B%2C%20while%20the%20latter%20costs%20160%20total%20GPU-hours%20on%208%20A800.%0AWe%20are%20the%20first%20to%20explore%20the%20efficient%203D-LLM%2C%20offering%20new%20insights%20to%20the%0Acommunity.%20Code%20and%20weights%20are%20available%20at%0Ahttps%3A//github.com/TangYuan96/MiniGPT-3D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01413v1&entry.124074799=Read"},
{"title": "An Information-Theoretic Perspective on Variance-Invariance-Covariance\n  Regularization", "author": "Ravid Shwartz-Ziv and Randall Balestriero and Kenji Kawaguchi and Tim G. J. Rudner and Yann LeCun", "abstract": "  Variance-Invariance-Covariance Regularization (VICReg) is a self-supervised\nlearning (SSL) method that has shown promising results on a variety of tasks.\nHowever, the fundamental mechanisms underlying VICReg remain unexplored. In\nthis paper, we present an information-theoretic perspective on the VICReg\nobjective. We begin by deriving information-theoretic quantities for\ndeterministic networks as an alternative to unrealistic stochastic network\nassumptions. We then relate the optimization of the VICReg objective to mutual\ninformation optimization, highlighting underlying assumptions and facilitating\na constructive comparison with other SSL algorithms and derive a generalization\nbound for VICReg, revealing its inherent advantages for downstream tasks.\nBuilding on these results, we introduce a family of SSL methods derived from\ninformation-theoretic principles that outperform existing SSL techniques.\n", "link": "http://arxiv.org/abs/2303.00633v4", "date": "2024-05-02", "relevancy": 2.3494, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4817}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4718}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4562}], "mailto": "mailto:daeR=997470421.yrtne&4v33600.3032/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.seuqinhcet02%LSS02%gnitsixe02%mrofreptuo02%taht02%selpicnirp02%citeroeht-noitamrofniA0%morf02%devired02%sdohtem02%LSS02%fo02%ylimaf02%a02%ecudortni02%ew02%C2%stluser02%eseht02%no02%gnidliuBA0%.sksat02%maertsnwod02%rof02%segatnavda02%tnerehni02%sti02%gnilaever02%C2%geRCIV02%rof02%dnuobA0%noitazilareneg02%a02%evired02%dna02%smhtirogla02%LSS02%rehto02%htiw02%nosirapmoc02%evitcurtsnoc02%aA0%gnitatilicaf02%dna02%snoitpmussa02%gniylrednu02%gnithgilhgih02%C2%noitazimitpo02%noitamrofniA0%lautum02%ot02%evitcejbo02%geRCIV02%eht02%fo02%noitazimitpo02%eht02%etaler02%neht02%eW02%.snoitpmussaA0%krowten02%citsahcots02%citsilaernu02%ot02%evitanretla02%na02%sa02%skrowten02%citsinimretedA0%rof02%seititnauq02%citeroeht-noitamrofni02%gnivired02%yb02%nigeb02%eW02%.evitcejboA0%geRCIV02%eht02%no02%evitcepsrep02%citeroeht-noitamrofni02%na02%tneserp02%ew02%C2%repap02%sihtA0%nI02%.derolpxenu02%niamer02%geRCIV02%gniylrednu02%smsinahcem02%latnemadnuf02%eht02%C2%revewoHA0%.sksat02%fo02%yteirav02%a02%no02%stluser02%gnisimorp02%nwohs02%sah02%taht02%dohtem02%92%LSS82%02%gninraelA0%desivrepus-fles02%a02%si02%92%geRCIV82%02%noitaziralugeR02%ecnairavoC-ecnairavnI-ecnairaV02%02%=3328342921.yrtne&nuCeL02%nnaY02%dna02%renduR02%.J02%.G02%miT02%dna02%ihcugawaK02%ijneK02%dna02%oreirtselaB02%lladnaR02%dna02%viZ-ztrawhS02%divaR=526535609.yrtne&noitaziralugeR02%02%A0%ecnairavoC-ecnairavnI-ecnairaV02%no02%evitcepsreP02%citeroehT-noitamrofnI02%nA=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20An%20Information-Theoretic%20Perspective%20on%20Variance-Invariance-Covariance%0A%20%20Regularization&body=Title%3A%20An%20Information-Theoretic%20Perspective%20on%20Variance-Invariance-Covariance%0A%20%20Regularization%0AAuthor%3A%20Ravid%20Shwartz-Ziv%20and%20Randall%20Balestriero%20and%20Kenji%20Kawaguchi%20and%20Tim%20G.%20J.%20Rudner%20and%20Yann%20LeCun%0AAbstract%3A%20%20%20Variance-Invariance-Covariance%20Regularization%20%28VICReg%29%20is%20a%20self-supervised%0Alearning%20%28SSL%29%20method%20that%20has%20shown%20promising%20results%20on%20a%20variety%20of%20tasks.%0AHowever%2C%20the%20fundamental%20mechanisms%20underlying%20VICReg%20remain%20unexplored.%20In%0Athis%20paper%2C%20we%20present%20an%20information-theoretic%20perspective%20on%20the%20VICReg%0Aobjective.%20We%20begin%20by%20deriving%20information-theoretic%20quantities%20for%0Adeterministic%20networks%20as%20an%20alternative%20to%20unrealistic%20stochastic%20network%0Aassumptions.%20We%20then%20relate%20the%20optimization%20of%20the%20VICReg%20objective%20to%20mutual%0Ainformation%20optimization%2C%20highlighting%20underlying%20assumptions%20and%20facilitating%0Aa%20constructive%20comparison%20with%20other%20SSL%20algorithms%20and%20derive%20a%20generalization%0Abound%20for%20VICReg%2C%20revealing%20its%20inherent%20advantages%20for%20downstream%20tasks.%0ABuilding%20on%20these%20results%2C%20we%20introduce%20a%20family%20of%20SSL%20methods%20derived%20from%0Ainformation-theoretic%20principles%20that%20outperform%20existing%20SSL%20techniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.00633v4%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Information-Theoretic%20Perspective%20on%20Variance-Invariance-Covariance%0A%20%20Regularization&entry.906535625=Ravid%20Shwartz-Ziv%20and%20Randall%20Balestriero%20and%20Kenji%20Kawaguchi%20and%20Tim%20G.%20J.%20Rudner%20and%20Yann%20LeCun&entry.1292438233=%20%20Variance-Invariance-Covariance%20Regularization%20%28VICReg%29%20is%20a%20self-supervised%0Alearning%20%28SSL%29%20method%20that%20has%20shown%20promising%20results%20on%20a%20variety%20of%20tasks.%0AHowever%2C%20the%20fundamental%20mechanisms%20underlying%20VICReg%20remain%20unexplored.%20In%0Athis%20paper%2C%20we%20present%20an%20information-theoretic%20perspective%20on%20the%20VICReg%0Aobjective.%20We%20begin%20by%20deriving%20information-theoretic%20quantities%20for%0Adeterministic%20networks%20as%20an%20alternative%20to%20unrealistic%20stochastic%20network%0Aassumptions.%20We%20then%20relate%20the%20optimization%20of%20the%20VICReg%20objective%20to%20mutual%0Ainformation%20optimization%2C%20highlighting%20underlying%20assumptions%20and%20facilitating%0Aa%20constructive%20comparison%20with%20other%20SSL%20algorithms%20and%20derive%20a%20generalization%0Abound%20for%20VICReg%2C%20revealing%20its%20inherent%20advantages%20for%20downstream%20tasks.%0ABuilding%20on%20these%20results%2C%20we%20introduce%20a%20family%20of%20SSL%20methods%20derived%20from%0Ainformation-theoretic%20principles%20that%20outperform%20existing%20SSL%20techniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.00633v4&entry.124074799=Read"},
{"title": "Community-Invariant Graph Contrastive Learning", "author": "Shiyin Tan and Dongyuan Li and Renhe Jiang and Ying Zhang and Manabu Okumura", "abstract": "  Graph augmentation has received great attention in recent years for graph\ncontrastive learning (GCL) to learn well-generalized node/graph\nrepresentations. However, mainstream GCL methods often favor randomly\ndisrupting graphs for augmentation, which shows limited generalization and\ninevitably leads to the corruption of high-level graph information, i.e., the\ngraph community. Moreover, current knowledge-based graph augmentation methods\ncan only focus on either topology or node features, causing the model to lack\nrobustness against various types of noise. To address these limitations, this\nresearch investigated the role of the graph community in graph augmentation and\nfigured out its crucial advantage for learnable graph augmentation. Based on\nour observations, we propose a community-invariant GCL framework to maintain\ngraph community structure during learnable graph augmentation. By maximizing\nthe spectral changes, this framework unifies the constraints of both topology\nand feature augmentation, enhancing the model's robustness. Empirical evidence\non 21 benchmark datasets demonstrates the exclusive merits of our framework.\nCode is released on Github (https://github.com/ShiyinTan/CI-GCL.git).\n", "link": "http://arxiv.org/abs/2405.01350v1", "date": "2024-05-02", "relevancy": 2.3453, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4918}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4634}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.452}], "mailto": "mailto:daeR=997470421.yrtne&1v05310.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.92%tig.LCG-IC/naTniyihS/moc.buhtig//A3%sptth82%02%buhtiG02%no02%desaeler02%si02%edoCA0%.krowemarf02%ruo02%fo02%stirem02%evisulcxe02%eht02%setartsnomed02%stesatad02%kramhcneb02%1202%noA0%ecnedive02%laciripmE02%.ssentsubor02%s72%ledom02%eht02%gnicnahne02%C2%noitatnemgua02%erutaef02%dnaA0%ygolopot02%htob02%fo02%stniartsnoc02%eht02%seifinu02%krowemarf02%siht02%C2%segnahc02%lartceps02%ehtA0%gnizimixam02%yB02%.noitatnemgua02%hparg02%elbanrael02%gnirud02%erutcurts02%ytinummoc02%hpargA0%niatniam02%ot02%krowemarf02%LCG02%tnairavni-ytinummoc02%a02%esoporp02%ew02%C2%snoitavresbo02%ruoA0%no02%desaB02%.noitatnemgua02%hparg02%elbanrael02%rof02%egatnavda02%laicurc02%sti02%tuo02%derugifA0%dna02%noitatnemgua02%hparg02%ni02%ytinummoc02%hparg02%eht02%fo02%elor02%eht02%detagitsevni02%hcraeserA0%siht02%C2%snoitatimil02%eseht02%sserdda02%oT02%.esion02%fo02%sepyt02%suoirav02%tsniaga02%ssentsuborA0%kcal02%ot02%ledom02%eht02%gnisuac02%C2%serutaef02%edon02%ro02%ygolopot02%rehtie02%no02%sucof02%ylno02%nacA0%sdohtem02%noitatnemgua02%hparg02%desab-egdelwonk02%tnerruc02%C2%revoeroM02%.ytinummoc02%hpargA0%eht02%C2%.e.i02%C2%noitamrofni02%hparg02%level-hgih02%fo02%noitpurroc02%eht02%ot02%sdael02%ylbativeniA0%dna02%noitazilareneg02%detimil02%swohs02%hcihw02%C2%noitatnemgua02%rof02%shparg02%gnitpursidA0%ylmodnar02%rovaf02%netfo02%sdohtem02%LCG02%maertsniam02%C2%revewoH02%.snoitatneserperA0%hparg/edon02%dezilareneg-llew02%nrael02%ot02%92%LCG82%02%gninrael02%evitsartnocA0%hparg02%rof02%sraey02%tnecer02%ni02%noitnetta02%taerg02%deviecer02%sah02%noitatnemgua02%hparG02%02%=3328342921.yrtne&arumukO02%ubanaM02%dna02%gnahZ02%gniY02%dna02%gnaiJ02%ehneR02%dna02%iL02%nauygnoD02%dna02%naT02%niyihS=526535609.yrtne&gninraeL02%evitsartnoC02%hparG02%tnairavnI-ytinummoC=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Community-Invariant%20Graph%20Contrastive%20Learning&body=Title%3A%20Community-Invariant%20Graph%20Contrastive%20Learning%0AAuthor%3A%20Shiyin%20Tan%20and%20Dongyuan%20Li%20and%20Renhe%20Jiang%20and%20Ying%20Zhang%20and%20Manabu%20Okumura%0AAbstract%3A%20%20%20Graph%20augmentation%20has%20received%20great%20attention%20in%20recent%20years%20for%20graph%0Acontrastive%20learning%20%28GCL%29%20to%20learn%20well-generalized%20node/graph%0Arepresentations.%20However%2C%20mainstream%20GCL%20methods%20often%20favor%20randomly%0Adisrupting%20graphs%20for%20augmentation%2C%20which%20shows%20limited%20generalization%20and%0Ainevitably%20leads%20to%20the%20corruption%20of%20high-level%20graph%20information%2C%20i.e.%2C%20the%0Agraph%20community.%20Moreover%2C%20current%20knowledge-based%20graph%20augmentation%20methods%0Acan%20only%20focus%20on%20either%20topology%20or%20node%20features%2C%20causing%20the%20model%20to%20lack%0Arobustness%20against%20various%20types%20of%20noise.%20To%20address%20these%20limitations%2C%20this%0Aresearch%20investigated%20the%20role%20of%20the%20graph%20community%20in%20graph%20augmentation%20and%0Afigured%20out%20its%20crucial%20advantage%20for%20learnable%20graph%20augmentation.%20Based%20on%0Aour%20observations%2C%20we%20propose%20a%20community-invariant%20GCL%20framework%20to%20maintain%0Agraph%20community%20structure%20during%20learnable%20graph%20augmentation.%20By%20maximizing%0Athe%20spectral%20changes%2C%20this%20framework%20unifies%20the%20constraints%20of%20both%20topology%0Aand%20feature%20augmentation%2C%20enhancing%20the%20model%27s%20robustness.%20Empirical%20evidence%0Aon%2021%20benchmark%20datasets%20demonstrates%20the%20exclusive%20merits%20of%20our%20framework.%0ACode%20is%20released%20on%20Github%20%28https%3A//github.com/ShiyinTan/CI-GCL.git%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01350v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Community-Invariant%20Graph%20Contrastive%20Learning&entry.906535625=Shiyin%20Tan%20and%20Dongyuan%20Li%20and%20Renhe%20Jiang%20and%20Ying%20Zhang%20and%20Manabu%20Okumura&entry.1292438233=%20%20Graph%20augmentation%20has%20received%20great%20attention%20in%20recent%20years%20for%20graph%0Acontrastive%20learning%20%28GCL%29%20to%20learn%20well-generalized%20node/graph%0Arepresentations.%20However%2C%20mainstream%20GCL%20methods%20often%20favor%20randomly%0Adisrupting%20graphs%20for%20augmentation%2C%20which%20shows%20limited%20generalization%20and%0Ainevitably%20leads%20to%20the%20corruption%20of%20high-level%20graph%20information%2C%20i.e.%2C%20the%0Agraph%20community.%20Moreover%2C%20current%20knowledge-based%20graph%20augmentation%20methods%0Acan%20only%20focus%20on%20either%20topology%20or%20node%20features%2C%20causing%20the%20model%20to%20lack%0Arobustness%20against%20various%20types%20of%20noise.%20To%20address%20these%20limitations%2C%20this%0Aresearch%20investigated%20the%20role%20of%20the%20graph%20community%20in%20graph%20augmentation%20and%0Afigured%20out%20its%20crucial%20advantage%20for%20learnable%20graph%20augmentation.%20Based%20on%0Aour%20observations%2C%20we%20propose%20a%20community-invariant%20GCL%20framework%20to%20maintain%0Agraph%20community%20structure%20during%20learnable%20graph%20augmentation.%20By%20maximizing%0Athe%20spectral%20changes%2C%20this%20framework%20unifies%20the%20constraints%20of%20both%20topology%0Aand%20feature%20augmentation%2C%20enhancing%20the%20model%27s%20robustness.%20Empirical%20evidence%0Aon%2021%20benchmark%20datasets%20demonstrates%20the%20exclusive%20merits%20of%20our%20framework.%0ACode%20is%20released%20on%20Github%20%28https%3A//github.com/ShiyinTan/CI-GCL.git%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01350v1&entry.124074799=Read"},
{"title": "Efficient Algorithms for Learning Monophonic Halfspaces in Graphs", "author": "Marco Bressan and Emmanuel Esposito and Maximilian Thiessen", "abstract": "  We study the problem of learning a binary classifier on the vertices of a\ngraph. In particular, we consider classifiers given by monophonic halfspaces,\npartitions of the vertices that are convex in a certain abstract sense.\nMonophonic halfspaces, and related notions such as geodesic halfspaces,have\nrecently attracted interest, and several connections have been drawn between\ntheir properties(e.g., their VC dimension) and the structure of the underlying\ngraph $G$. We prove several novel results for learning monophonic halfspaces in\nthe supervised, online, and active settings. Our main result is that a\nmonophonic halfspace can be learned with near-optimal passive sample complexity\nin time polynomial in $n = |V(G)|$. This requires us to devise a\npolynomial-time algorithm for consistent hypothesis checking, based on several\nstructural insights on monophonic halfspaces and on a reduction to\n$2$-satisfiability. We prove similar results for the online and active\nsettings. We also show that the concept class can be enumerated with delay\n$\\operatorname{poly}(n)$, and that empirical risk minimization can be performed\nin time $2^{\\omega(G)}\\operatorname{poly}(n)$ where $\\omega(G)$ is the clique\nnumber of $G$. These results answer open questions from the literature\n(Gonz\\'alez et al., 2020), and show a contrast with geodesic halfspaces, for\nwhich some of the said problems are NP-hard (Seiffarth et al., 2023).\n", "link": "http://arxiv.org/abs/2405.00853v1", "date": "2024-05-01", "relevancy": 2.3304, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4801}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4687}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4495}], "mailto": "mailto:daeR=997470421.yrtne&1v35800.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.92%320202%C2%.la02%te02%htraffieS82%02%drah-PN02%era02%smelborp02%dias02%eht02%fo02%emos02%hcihwA0%rof02%C2%secapsflah02%cisedoeg02%htiw02%tsartnoc02%a02%wohs02%dna02%C2%92%020202%C2%.la02%te02%zela72%C5%znoG82%A0%erutaretil02%eht02%morf02%snoitseuq02%nepo02%rewsna02%stluser02%esehT02%.42%G42%02%fo02%rebmunA0%euqilc02%eht02%si02%42%92%G82%agemoC5%42%02%erehw02%42%92%n82%D7%ylopB7%emanrotarepoC5%D7%92%G82%agemoC5%B7%E5%242%02%emit02%niA0%demrofrep02%eb02%nac02%noitaziminim02%ksir02%laciripme02%taht02%dna02%C2%42%92%n82%D7%ylopB7%emanrotarepoC5%42%A0%yaled02%htiw02%detaremune02%eb02%nac02%ssalc02%tpecnoc02%eht02%taht02%wohs02%osla02%eW02%.sgnittesA0%evitca02%dna02%enilno02%eht02%rof02%stluser02%ralimis02%evorp02%eW02%.ytilibaifsitas-42%242%A0%ot02%noitcuder02%a02%no02%dna02%secapsflah02%cinohponom02%no02%sthgisni02%larutcurtsA0%lareves02%no02%desab02%C2%gnikcehc02%sisehtopyh02%tnetsisnoc02%rof02%mhtirogla02%emit-laimonylopA0%a02%esived02%ot02%su02%seriuqer02%sihT02%.42%C7%92%G82%VC7%02%D3%02%n42%02%ni02%laimonylop02%emit02%niA0%ytixelpmoc02%elpmas02%evissap02%lamitpo-raen02%htiw02%denrael02%eb02%nac02%ecapsflah02%cinohponomA0%a02%taht02%si02%tluser02%niam02%ruO02%.sgnittes02%evitca02%dna02%C2%enilno02%C2%desivrepus02%ehtA0%ni02%secapsflah02%cinohponom02%gninrael02%rof02%stluser02%levon02%lareves02%evorp02%eW02%.42%G42%02%hpargA0%gniylrednu02%eht02%fo02%erutcurts02%eht02%dna02%92%noisnemid02%CV02%rieht02%C2%.g.e82%seitreporp02%riehtA0%neewteb02%nward02%neeb02%evah02%snoitcennoc02%lareves02%dna02%C2%tseretni02%detcartta02%yltnecerA0%evahC2%secapsflah02%cisedoeg02%sa02%hcus02%snoiton02%detaler02%dna02%C2%secapsflah02%cinohponoMA0%.esnes02%tcartsba02%niatrec02%a02%ni02%xevnoc02%era02%taht02%secitrev02%eht02%fo02%snoititrapA0%C2%secapsflah02%cinohponom02%yb02%nevig02%sreifissalc02%redisnoc02%ew02%C2%ralucitrap02%nI02%.hpargA0%a02%fo02%secitrev02%eht02%no02%reifissalc02%yranib02%a02%gninrael02%fo02%melborp02%eht02%yduts02%eW02%02%=3328342921.yrtne&nesseihT02%nailimixaM02%dna02%otisopsE02%leunammE02%dna02%nasserB02%ocraM=526535609.yrtne&shparG02%ni02%secapsflaH02%cinohponoM02%gninraeL02%rof02%smhtiroglA02%tneiciffE=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Efficient%20Algorithms%20for%20Learning%20Monophonic%20Halfspaces%20in%20Graphs&body=Title%3A%20Efficient%20Algorithms%20for%20Learning%20Monophonic%20Halfspaces%20in%20Graphs%0AAuthor%3A%20Marco%20Bressan%20and%20Emmanuel%20Esposito%20and%20Maximilian%20Thiessen%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20learning%20a%20binary%20classifier%20on%20the%20vertices%20of%20a%0Agraph.%20In%20particular%2C%20we%20consider%20classifiers%20given%20by%20monophonic%20halfspaces%2C%0Apartitions%20of%20the%20vertices%20that%20are%20convex%20in%20a%20certain%20abstract%20sense.%0AMonophonic%20halfspaces%2C%20and%20related%20notions%20such%20as%20geodesic%20halfspaces%2Chave%0Arecently%20attracted%20interest%2C%20and%20several%20connections%20have%20been%20drawn%20between%0Atheir%20properties%28e.g.%2C%20their%20VC%20dimension%29%20and%20the%20structure%20of%20the%20underlying%0Agraph%20%24G%24.%20We%20prove%20several%20novel%20results%20for%20learning%20monophonic%20halfspaces%20in%0Athe%20supervised%2C%20online%2C%20and%20active%20settings.%20Our%20main%20result%20is%20that%20a%0Amonophonic%20halfspace%20can%20be%20learned%20with%20near-optimal%20passive%20sample%20complexity%0Ain%20time%20polynomial%20in%20%24n%20%3D%20%7CV%28G%29%7C%24.%20This%20requires%20us%20to%20devise%20a%0Apolynomial-time%20algorithm%20for%20consistent%20hypothesis%20checking%2C%20based%20on%20several%0Astructural%20insights%20on%20monophonic%20halfspaces%20and%20on%20a%20reduction%20to%0A%242%24-satisfiability.%20We%20prove%20similar%20results%20for%20the%20online%20and%20active%0Asettings.%20We%20also%20show%20that%20the%20concept%20class%20can%20be%20enumerated%20with%20delay%0A%24%5Coperatorname%7Bpoly%7D%28n%29%24%2C%20and%20that%20empirical%20risk%20minimization%20can%20be%20performed%0Ain%20time%20%242%5E%7B%5Comega%28G%29%7D%5Coperatorname%7Bpoly%7D%28n%29%24%20where%20%24%5Comega%28G%29%24%20is%20the%20clique%0Anumber%20of%20%24G%24.%20These%20results%20answer%20open%20questions%20from%20the%20literature%0A%28Gonz%5C%27alez%20et%20al.%2C%202020%29%2C%20and%20show%20a%20contrast%20with%20geodesic%20halfspaces%2C%20for%0Awhich%20some%20of%20the%20said%20problems%20are%20NP-hard%20%28Seiffarth%20et%20al.%2C%202023%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00853v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Algorithms%20for%20Learning%20Monophonic%20Halfspaces%20in%20Graphs&entry.906535625=Marco%20Bressan%20and%20Emmanuel%20Esposito%20and%20Maximilian%20Thiessen&entry.1292438233=%20%20We%20study%20the%20problem%20of%20learning%20a%20binary%20classifier%20on%20the%20vertices%20of%20a%0Agraph.%20In%20particular%2C%20we%20consider%20classifiers%20given%20by%20monophonic%20halfspaces%2C%0Apartitions%20of%20the%20vertices%20that%20are%20convex%20in%20a%20certain%20abstract%20sense.%0AMonophonic%20halfspaces%2C%20and%20related%20notions%20such%20as%20geodesic%20halfspaces%2Chave%0Arecently%20attracted%20interest%2C%20and%20several%20connections%20have%20been%20drawn%20between%0Atheir%20properties%28e.g.%2C%20their%20VC%20dimension%29%20and%20the%20structure%20of%20the%20underlying%0Agraph%20%24G%24.%20We%20prove%20several%20novel%20results%20for%20learning%20monophonic%20halfspaces%20in%0Athe%20supervised%2C%20online%2C%20and%20active%20settings.%20Our%20main%20result%20is%20that%20a%0Amonophonic%20halfspace%20can%20be%20learned%20with%20near-optimal%20passive%20sample%20complexity%0Ain%20time%20polynomial%20in%20%24n%20%3D%20%7CV%28G%29%7C%24.%20This%20requires%20us%20to%20devise%20a%0Apolynomial-time%20algorithm%20for%20consistent%20hypothesis%20checking%2C%20based%20on%20several%0Astructural%20insights%20on%20monophonic%20halfspaces%20and%20on%20a%20reduction%20to%0A%242%24-satisfiability.%20We%20prove%20similar%20results%20for%20the%20online%20and%20active%0Asettings.%20We%20also%20show%20that%20the%20concept%20class%20can%20be%20enumerated%20with%20delay%0A%24%5Coperatorname%7Bpoly%7D%28n%29%24%2C%20and%20that%20empirical%20risk%20minimization%20can%20be%20performed%0Ain%20time%20%242%5E%7B%5Comega%28G%29%7D%5Coperatorname%7Bpoly%7D%28n%29%24%20where%20%24%5Comega%28G%29%24%20is%20the%20clique%0Anumber%20of%20%24G%24.%20These%20results%20answer%20open%20questions%20from%20the%20literature%0A%28Gonz%5C%27alez%20et%20al.%2C%202020%29%2C%20and%20show%20a%20contrast%20with%20geodesic%20halfspaces%2C%20for%0Awhich%20some%20of%20the%20said%20problems%20are%20NP-hard%20%28Seiffarth%20et%20al.%2C%202023%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00853v1&entry.124074799=Read"},
{"title": "OmniDrive: A Holistic LLM-Agent Framework for Autonomous Driving with 3D\n  Perception, Reasoning and Planning", "author": "Shihao Wang and Zhiding Yu and Xiaohui Jiang and Shiyi Lan and Min Shi and Nadine Chang and Jan Kautz and Ying Li and Jose M. Alvarez", "abstract": "  The advances in multimodal large language models (MLLMs) have led to growing\ninterests in LLM-based autonomous driving agents to leverage their strong\nreasoning capabilities. However, capitalizing on MLLMs' strong reasoning\ncapabilities for improved planning behavior is challenging since planning\nrequires full 3D situational awareness beyond 2D reasoning. To address this\nchallenge, our work proposes a holistic framework for strong alignment between\nagent models and 3D driving tasks. Our framework starts with a novel 3D MLLM\narchitecture that uses sparse queries to lift and compress visual\nrepresentations into 3D before feeding them into an LLM. This query-based\nrepresentation allows us to jointly encode dynamic objects and static map\nelements (e.g., traffic lanes), providing a condensed world model for\nperception-action alignment in 3D. We further propose OmniDrive-nuScenes, a new\nvisual question-answering dataset challenging the true 3D situational awareness\nof a model with comprehensive visual question-answering (VQA) tasks, including\nscene description, traffic regulation, 3D grounding, counterfactual reasoning,\ndecision making and planning. Extensive studies show the effectiveness of the\nproposed architecture as well as the importance of the VQA tasks for reasoning\nand planning in complex 3D scenes.\n", "link": "http://arxiv.org/abs/2405.01533v1", "date": "2024-05-02", "relevancy": 2.3284, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5937}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5757}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5731}], "mailto": "mailto:daeR=997470421.yrtne&1v33510.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.senecs02%D302%xelpmoc02%ni02%gninnalp02%dnaA0%gninosaer02%rof02%sksat02%AQV02%eht02%fo02%ecnatropmi02%eht02%sa02%llew02%sa02%erutcetihcra02%desoporpA0%eht02%fo02%ssenevitceffe02%eht02%wohs02%seiduts02%evisnetxE02%.gninnalp02%dna02%gnikam02%noisicedA0%C2%gninosaer02%lautcafretnuoc02%C2%gnidnuorg02%D302%C2%noitaluger02%ciffart02%C2%noitpircsed02%enecsA0%gnidulcni02%C2%sksat02%92%AQV82%02%gnirewsna-noitseuq02%lausiv02%evisneherpmoc02%htiw02%ledom02%a02%foA0%ssenerawa02%lanoitautis02%D302%eurt02%eht02%gnignellahc02%tesatad02%gnirewsna-noitseuq02%lausivA0%wen02%a02%C2%senecSun-evirDinmO02%esoporp02%rehtruf02%eW02%.D302%ni02%tnemngila02%noitca-noitpecrepA0%rof02%ledom02%dlrow02%desnednoc02%a02%gnidivorp02%C2%92%senal02%ciffart02%C2%.g.e82%02%stnemeleA0%pam02%citats02%dna02%stcejbo02%cimanyd02%edocne02%yltnioj02%ot02%su02%swolla02%noitatneserperA0%desab-yreuq02%sihT02%.MLL02%na02%otni02%meht02%gnideef02%erofeb02%D302%otni02%snoitatneserperA0%lausiv02%sserpmoc02%dna02%tfil02%ot02%seireuq02%esraps02%sesu02%taht02%erutcetihcraA0%MLLM02%D302%levon02%a02%htiw02%strats02%krowemarf02%ruO02%.sksat02%gnivird02%D302%dna02%sledom02%tnegaA0%neewteb02%tnemngila02%gnorts02%rof02%krowemarf02%citsiloh02%a02%sesoporp02%krow02%ruo02%C2%egnellahcA0%siht02%sserdda02%oT02%.gninosaer02%D202%dnoyeb02%ssenerawa02%lanoitautis02%D302%lluf02%seriuqerA0%gninnalp02%ecnis02%gnignellahc02%si02%roivaheb02%gninnalp02%devorpmi02%rof02%seitilibapacA0%gninosaer02%gnorts02%72%sMLLM02%no02%gnizilatipac02%C2%revewoH02%.seitilibapac02%gninosaerA0%gnorts02%rieht02%egarevel02%ot02%stnega02%gnivird02%suomonotua02%desab-MLL02%ni02%stseretniA0%gniworg02%ot02%del02%evah02%92%sMLLM82%02%sledom02%egaugnal02%egral02%ladomitlum02%ni02%secnavda02%ehT02%02%=3328342921.yrtne&zeravlA02%.M02%esoJ02%dna02%iL02%gniY02%dna02%ztuaK02%naJ02%dna02%gnahC02%enidaN02%dna02%ihS02%niM02%dna02%naL02%iyihS02%dna02%gnaiJ02%iuhoaiX02%dna02%uY02%gnidihZ02%dna02%gnaW02%oahihS=526535609.yrtne&gninnalP02%dna02%gninosaeR02%C2%noitpecreP02%02%A0%D302%htiw02%gnivirD02%suomonotuA02%rof02%krowemarF02%tnegA-MLL02%citsiloH02%A02%A3%evirDinmO=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20OmniDrive%3A%20A%20Holistic%20LLM-Agent%20Framework%20for%20Autonomous%20Driving%20with%203D%0A%20%20Perception%2C%20Reasoning%20and%20Planning&body=Title%3A%20OmniDrive%3A%20A%20Holistic%20LLM-Agent%20Framework%20for%20Autonomous%20Driving%20with%203D%0A%20%20Perception%2C%20Reasoning%20and%20Planning%0AAuthor%3A%20Shihao%20Wang%20and%20Zhiding%20Yu%20and%20Xiaohui%20Jiang%20and%20Shiyi%20Lan%20and%20Min%20Shi%20and%20Nadine%20Chang%20and%20Jan%20Kautz%20and%20Ying%20Li%20and%20Jose%20M.%20Alvarez%0AAbstract%3A%20%20%20The%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20led%20to%20growing%0Ainterests%20in%20LLM-based%20autonomous%20driving%20agents%20to%20leverage%20their%20strong%0Areasoning%20capabilities.%20However%2C%20capitalizing%20on%20MLLMs%27%20strong%20reasoning%0Acapabilities%20for%20improved%20planning%20behavior%20is%20challenging%20since%20planning%0Arequires%20full%203D%20situational%20awareness%20beyond%202D%20reasoning.%20To%20address%20this%0Achallenge%2C%20our%20work%20proposes%20a%20holistic%20framework%20for%20strong%20alignment%20between%0Aagent%20models%20and%203D%20driving%20tasks.%20Our%20framework%20starts%20with%20a%20novel%203D%20MLLM%0Aarchitecture%20that%20uses%20sparse%20queries%20to%20lift%20and%20compress%20visual%0Arepresentations%20into%203D%20before%20feeding%20them%20into%20an%20LLM.%20This%20query-based%0Arepresentation%20allows%20us%20to%20jointly%20encode%20dynamic%20objects%20and%20static%20map%0Aelements%20%28e.g.%2C%20traffic%20lanes%29%2C%20providing%20a%20condensed%20world%20model%20for%0Aperception-action%20alignment%20in%203D.%20We%20further%20propose%20OmniDrive-nuScenes%2C%20a%20new%0Avisual%20question-answering%20dataset%20challenging%20the%20true%203D%20situational%20awareness%0Aof%20a%20model%20with%20comprehensive%20visual%20question-answering%20%28VQA%29%20tasks%2C%20including%0Ascene%20description%2C%20traffic%20regulation%2C%203D%20grounding%2C%20counterfactual%20reasoning%2C%0Adecision%20making%20and%20planning.%20Extensive%20studies%20show%20the%20effectiveness%20of%20the%0Aproposed%20architecture%20as%20well%20as%20the%20importance%20of%20the%20VQA%20tasks%20for%20reasoning%0Aand%20planning%20in%20complex%203D%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01533v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniDrive%3A%20A%20Holistic%20LLM-Agent%20Framework%20for%20Autonomous%20Driving%20with%203D%0A%20%20Perception%2C%20Reasoning%20and%20Planning&entry.906535625=Shihao%20Wang%20and%20Zhiding%20Yu%20and%20Xiaohui%20Jiang%20and%20Shiyi%20Lan%20and%20Min%20Shi%20and%20Nadine%20Chang%20and%20Jan%20Kautz%20and%20Ying%20Li%20and%20Jose%20M.%20Alvarez&entry.1292438233=%20%20The%20advances%20in%20multimodal%20large%20language%20models%20%28MLLMs%29%20have%20led%20to%20growing%0Ainterests%20in%20LLM-based%20autonomous%20driving%20agents%20to%20leverage%20their%20strong%0Areasoning%20capabilities.%20However%2C%20capitalizing%20on%20MLLMs%27%20strong%20reasoning%0Acapabilities%20for%20improved%20planning%20behavior%20is%20challenging%20since%20planning%0Arequires%20full%203D%20situational%20awareness%20beyond%202D%20reasoning.%20To%20address%20this%0Achallenge%2C%20our%20work%20proposes%20a%20holistic%20framework%20for%20strong%20alignment%20between%0Aagent%20models%20and%203D%20driving%20tasks.%20Our%20framework%20starts%20with%20a%20novel%203D%20MLLM%0Aarchitecture%20that%20uses%20sparse%20queries%20to%20lift%20and%20compress%20visual%0Arepresentations%20into%203D%20before%20feeding%20them%20into%20an%20LLM.%20This%20query-based%0Arepresentation%20allows%20us%20to%20jointly%20encode%20dynamic%20objects%20and%20static%20map%0Aelements%20%28e.g.%2C%20traffic%20lanes%29%2C%20providing%20a%20condensed%20world%20model%20for%0Aperception-action%20alignment%20in%203D.%20We%20further%20propose%20OmniDrive-nuScenes%2C%20a%20new%0Avisual%20question-answering%20dataset%20challenging%20the%20true%203D%20situational%20awareness%0Aof%20a%20model%20with%20comprehensive%20visual%20question-answering%20%28VQA%29%20tasks%2C%20including%0Ascene%20description%2C%20traffic%20regulation%2C%203D%20grounding%2C%20counterfactual%20reasoning%2C%0Adecision%20making%20and%20planning.%20Extensive%20studies%20show%20the%20effectiveness%20of%20the%0Aproposed%20architecture%20as%20well%20as%20the%20importance%20of%20the%20VQA%20tasks%20for%20reasoning%0Aand%20planning%20in%20complex%203D%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01533v1&entry.124074799=Read"},
{"title": "Scalable network reconstruction in subquadratic time", "author": "Tiago P. Peixoto", "abstract": "  Network reconstruction consists in determining the unobserved pairwise\ncouplings between $N$ nodes given only observational data on the resulting\nbehavior that is conditioned on those couplings -- typically a time-series or\nindependent samples from a graphical model. A major obstacle to the scalability\nof algorithms proposed for this problem is a seemingly unavoidable quadratic\ncomplexity of $\\Omega(N^2)$, corresponding to the requirement of each possible\npairwise coupling being contemplated at least once, despite the fact that most\nnetworks of interest are sparse, with a number of non-zero couplings that is\nonly $O(N)$. Here we present a general algorithm applicable to a broad range of\nreconstruction problems that significantly outperforms this quadratic baseline.\nOur algorithm relies on a stochastic second neighbor search (Dong et al., 2011)\nthat produces the best edge candidates with high probability, thus bypassing an\nexhaustive quadratic search. If we rely on the conjecture that the\nsecond-neighbor search finishes in log-linear time (Baron & Darling, 2020;\n2022), we demonstrate theoretically that our algorithm finishes in subquadratic\ntime, with a data-dependent complexity loosely upper bounded by $O(N^{3/2}\\log\nN)$, but with a more typical log-linear complexity of $O(N\\log^2N)$. In\npractice, we show that our algorithm achieves a performance that is many orders\nof magnitude faster than the quadratic baseline -- in a manner consistent with\nour theoretical analysis -- allows for easy parallelization, and thus enables\nthe reconstruction of networks with hundreds of thousands and even millions of\nnodes and edges.\n", "link": "http://arxiv.org/abs/2401.01404v4", "date": "2024-05-02", "relevancy": 2.3264, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4832}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4723}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4403}], "mailto": "mailto:daeR=997470421.yrtne&4v40410.1042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.segde02%dna02%sedonA0%fo02%snoillim02%neve02%dna02%sdnasuoht02%fo02%sderdnuh02%htiw02%skrowten02%fo02%noitcurtsnocer02%ehtA0%selbane02%suht02%dna02%C2%noitazilellarap02%ysae02%rof02%swolla02%--02%sisylana02%laciteroeht02%ruoA0%htiw02%tnetsisnoc02%rennam02%a02%ni02%--02%enilesab02%citardauq02%eht02%naht02%retsaf02%edutingam02%foA0%sredro02%ynam02%si02%taht02%ecnamrofrep02%a02%seveihca02%mhtirogla02%ruo02%taht02%wohs02%ew02%C2%ecitcarpA0%nI02%.42%92%N2E5%golC5%N82%O42%02%fo02%ytixelpmoc02%raenil-gol02%lacipyt02%erom02%a02%htiw02%tub02%C2%42%92%NA0%golC5%D7%2/3B7%E5%N82%O42%02%yb02%dednuob02%reppu02%ylesool02%ytixelpmoc02%tnedneped-atad02%a02%htiw02%C2%emitA0%citardauqbus02%ni02%sehsinif02%mhtirogla02%ruo02%taht02%yllaciteroeht02%etartsnomed02%ew02%C2%92%2202A0%B3%020202%C2%gnilraD02%62%02%noraB82%02%emit02%raenil-gol02%ni02%sehsinif02%hcraes02%robhgien-dnocesA0%eht02%taht02%erutcejnoc02%eht02%no02%yler02%ew02%fI02%.hcraes02%citardauq02%evitsuahxeA0%na02%gnissapyb02%suht02%C2%ytilibaborp02%hgih02%htiw02%setadidnac02%egde02%tseb02%eht02%secudorp02%tahtA0%92%110202%C2%.la02%te02%gnoD82%02%hcraes02%robhgien02%dnoces02%citsahcots02%a02%no02%seiler02%mhtirogla02%ruOA0%.enilesab02%citardauq02%siht02%smrofreptuo02%yltnacifingis02%taht02%smelborp02%noitcurtsnocerA0%fo02%egnar02%daorb02%a02%ot02%elbacilppa02%mhtirogla02%lareneg02%a02%tneserp02%ew02%ereH02%.42%92%N82%O42%02%ylnoA0%si02%taht02%sgnilpuoc02%orez-non02%fo02%rebmun02%a02%htiw02%C2%esraps02%era02%tseretni02%fo02%skrowtenA0%tsom02%taht02%tcaf02%eht02%etipsed02%C2%ecno02%tsael02%ta02%detalpmetnoc02%gnieb02%gnilpuoc02%esiwriapA0%elbissop02%hcae02%fo02%tnemeriuqer02%eht02%ot02%gnidnopserroc02%C2%42%92%2E5%N82%agemOC5%42%02%fo02%ytixelpmocA0%citardauq02%elbadiovanu02%ylgnimees02%a02%si02%melborp02%siht02%rof02%desoporp02%smhtirogla02%foA0%ytilibalacs02%eht02%ot02%elcatsbo02%rojam02%A02%.ledom02%lacihparg02%a02%morf02%selpmas02%tnednepedniA0%ro02%seires-emit02%a02%yllacipyt02%--02%sgnilpuoc02%esoht02%no02%denoitidnoc02%si02%taht02%roivahebA0%gnitluser02%eht02%no02%atad02%lanoitavresbo02%ylno02%nevig02%sedon02%42%N42%02%neewteb02%sgnilpuocA0%esiwriap02%devresbonu02%eht02%gninimreted02%ni02%stsisnoc02%noitcurtsnocer02%krowteN02%02%=3328342921.yrtne&otoxieP02%.P02%ogaiT=526535609.yrtne&emit02%citardauqbus02%ni02%noitcurtsnocer02%krowten02%elbalacS=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Scalable%20network%20reconstruction%20in%20subquadratic%20time&body=Title%3A%20Scalable%20network%20reconstruction%20in%20subquadratic%20time%0AAuthor%3A%20Tiago%20P.%20Peixoto%0AAbstract%3A%20%20%20Network%20reconstruction%20consists%20in%20determining%20the%20unobserved%20pairwise%0Acouplings%20between%20%24N%24%20nodes%20given%20only%20observational%20data%20on%20the%20resulting%0Abehavior%20that%20is%20conditioned%20on%20those%20couplings%20--%20typically%20a%20time-series%20or%0Aindependent%20samples%20from%20a%20graphical%20model.%20A%20major%20obstacle%20to%20the%20scalability%0Aof%20algorithms%20proposed%20for%20this%20problem%20is%20a%20seemingly%20unavoidable%20quadratic%0Acomplexity%20of%20%24%5COmega%28N%5E2%29%24%2C%20corresponding%20to%20the%20requirement%20of%20each%20possible%0Apairwise%20coupling%20being%20contemplated%20at%20least%20once%2C%20despite%20the%20fact%20that%20most%0Anetworks%20of%20interest%20are%20sparse%2C%20with%20a%20number%20of%20non-zero%20couplings%20that%20is%0Aonly%20%24O%28N%29%24.%20Here%20we%20present%20a%20general%20algorithm%20applicable%20to%20a%20broad%20range%20of%0Areconstruction%20problems%20that%20significantly%20outperforms%20this%20quadratic%20baseline.%0AOur%20algorithm%20relies%20on%20a%20stochastic%20second%20neighbor%20search%20%28Dong%20et%20al.%2C%202011%29%0Athat%20produces%20the%20best%20edge%20candidates%20with%20high%20probability%2C%20thus%20bypassing%20an%0Aexhaustive%20quadratic%20search.%20If%20we%20rely%20on%20the%20conjecture%20that%20the%0Asecond-neighbor%20search%20finishes%20in%20log-linear%20time%20%28Baron%20%26%20Darling%2C%202020%3B%0A2022%29%2C%20we%20demonstrate%20theoretically%20that%20our%20algorithm%20finishes%20in%20subquadratic%0Atime%2C%20with%20a%20data-dependent%20complexity%20loosely%20upper%20bounded%20by%20%24O%28N%5E%7B3/2%7D%5Clog%0AN%29%24%2C%20but%20with%20a%20more%20typical%20log-linear%20complexity%20of%20%24O%28N%5Clog%5E2N%29%24.%20In%0Apractice%2C%20we%20show%20that%20our%20algorithm%20achieves%20a%20performance%20that%20is%20many%20orders%0Aof%20magnitude%20faster%20than%20the%20quadratic%20baseline%20--%20in%20a%20manner%20consistent%20with%0Aour%20theoretical%20analysis%20--%20allows%20for%20easy%20parallelization%2C%20and%20thus%20enables%0Athe%20reconstruction%20of%20networks%20with%20hundreds%20of%20thousands%20and%20even%20millions%20of%0Anodes%20and%20edges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.01404v4%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scalable%20network%20reconstruction%20in%20subquadratic%20time&entry.906535625=Tiago%20P.%20Peixoto&entry.1292438233=%20%20Network%20reconstruction%20consists%20in%20determining%20the%20unobserved%20pairwise%0Acouplings%20between%20%24N%24%20nodes%20given%20only%20observational%20data%20on%20the%20resulting%0Abehavior%20that%20is%20conditioned%20on%20those%20couplings%20--%20typically%20a%20time-series%20or%0Aindependent%20samples%20from%20a%20graphical%20model.%20A%20major%20obstacle%20to%20the%20scalability%0Aof%20algorithms%20proposed%20for%20this%20problem%20is%20a%20seemingly%20unavoidable%20quadratic%0Acomplexity%20of%20%24%5COmega%28N%5E2%29%24%2C%20corresponding%20to%20the%20requirement%20of%20each%20possible%0Apairwise%20coupling%20being%20contemplated%20at%20least%20once%2C%20despite%20the%20fact%20that%20most%0Anetworks%20of%20interest%20are%20sparse%2C%20with%20a%20number%20of%20non-zero%20couplings%20that%20is%0Aonly%20%24O%28N%29%24.%20Here%20we%20present%20a%20general%20algorithm%20applicable%20to%20a%20broad%20range%20of%0Areconstruction%20problems%20that%20significantly%20outperforms%20this%20quadratic%20baseline.%0AOur%20algorithm%20relies%20on%20a%20stochastic%20second%20neighbor%20search%20%28Dong%20et%20al.%2C%202011%29%0Athat%20produces%20the%20best%20edge%20candidates%20with%20high%20probability%2C%20thus%20bypassing%20an%0Aexhaustive%20quadratic%20search.%20If%20we%20rely%20on%20the%20conjecture%20that%20the%0Asecond-neighbor%20search%20finishes%20in%20log-linear%20time%20%28Baron%20%26%20Darling%2C%202020%3B%0A2022%29%2C%20we%20demonstrate%20theoretically%20that%20our%20algorithm%20finishes%20in%20subquadratic%0Atime%2C%20with%20a%20data-dependent%20complexity%20loosely%20upper%20bounded%20by%20%24O%28N%5E%7B3/2%7D%5Clog%0AN%29%24%2C%20but%20with%20a%20more%20typical%20log-linear%20complexity%20of%20%24O%28N%5Clog%5E2N%29%24.%20In%0Apractice%2C%20we%20show%20that%20our%20algorithm%20achieves%20a%20performance%20that%20is%20many%20orders%0Aof%20magnitude%20faster%20than%20the%20quadratic%20baseline%20--%20in%20a%20manner%20consistent%20with%0Aour%20theoretical%20analysis%20--%20allows%20for%20easy%20parallelization%2C%20and%20thus%20enables%0Athe%20reconstruction%20of%20networks%20with%20hundreds%20of%20thousands%20and%20even%20millions%20of%0Anodes%20and%20edges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.01404v4&entry.124074799=Read"},
{"title": "Hypergraph $p$-Laplacian regularization on point clouds for data\n  interpolation", "author": "Kehan Shi and Martin Burger", "abstract": "  As a generalization of graphs, hypergraphs are widely used to model\nhigher-order relations in data. This paper explores the benefit of the\nhypergraph structure for the interpolation of point cloud data that contain no\nexplicit structural information. We define the $\\varepsilon_n$-ball hypergraph\nand the $k_n$-nearest neighbor hypergraph on a point cloud and study the\n$p$-Laplacian regularization on the hypergraphs. We prove the variational\nconsistency between the hypergraph $p$-Laplacian regularization and the\ncontinuum $p$-Laplacian regularization in a semisupervised setting when the\nnumber of points $n$ goes to infinity while the number of labeled points\nremains fixed. A key improvement compared to the graph case is that the results\nrely on weaker assumptions on the upper bound of $\\varepsilon_n$ and $k_n$. To\nsolve the convex but non-differentiable large-scale optimization problem, we\nutilize the stochastic primal-dual hybrid gradient algorithm. Numerical\nexperiments on data interpolation verify that the hypergraph $p$-Laplacian\nregularization outperforms the graph $p$-Laplacian regularization in preventing\nthe development of spikes at the labeled points.\n", "link": "http://arxiv.org/abs/2405.01109v1", "date": "2024-05-02", "relevancy": 2.326, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.468}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4669}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4606}], "mailto": "mailto:daeR=997470421.yrtne&1v90110.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.stniop02%delebal02%eht02%ta02%sekips02%fo02%tnempoleved02%ehtA0%gnitneverp02%ni02%noitaziraluger02%naicalpaL-42%p42%02%hparg02%eht02%smrofreptuo02%noitaziralugerA0%naicalpaL-42%p42%02%hpargrepyh02%eht02%taht02%yfirev02%noitalopretni02%atad02%no02%stnemirepxeA0%laciremuN02%.mhtirogla02%tneidarg02%dirbyh02%laud-lamirp02%citsahcots02%eht02%ezilituA0%ew02%C2%melborp02%noitazimitpo02%elacs-egral02%elbaitnereffid-non02%tub02%xevnoc02%eht02%evlosA0%oT02%.42%n_k42%02%dna02%42%n_nolisperavC5%42%02%fo02%dnuob02%reppu02%eht02%no02%snoitpmussa02%rekaew02%no02%ylerA0%stluser02%eht02%taht02%si02%esac02%hparg02%eht02%ot02%derapmoc02%tnemevorpmi02%yek02%A02%.dexif02%sniamerA0%stniop02%delebal02%fo02%rebmun02%eht02%elihw02%ytinifni02%ot02%seog02%42%n42%02%stniop02%fo02%rebmunA0%eht02%nehw02%gnittes02%desivrepusimes02%a02%ni02%noitaziraluger02%naicalpaL-42%p42%02%muunitnocA0%eht02%dna02%noitaziraluger02%naicalpaL-42%p42%02%hpargrepyh02%eht02%neewteb02%ycnetsisnocA0%lanoitairav02%eht02%evorp02%eW02%.shpargrepyh02%eht02%no02%noitaziraluger02%naicalpaL-42%p42%A0%eht02%yduts02%dna02%duolc02%tniop02%a02%no02%hpargrepyh02%robhgien02%tseraen-42%n_k42%02%eht02%dnaA0%hpargrepyh02%llab-42%n_nolisperavC5%42%02%eht02%enifed02%eW02%.noitamrofni02%larutcurts02%ticilpxeA0%on02%niatnoc02%taht02%atad02%duolc02%tniop02%fo02%noitalopretni02%eht02%rof02%erutcurts02%hpargrepyhA0%eht02%fo02%tifeneb02%eht02%serolpxe02%repap02%sihT02%.atad02%ni02%snoitaler02%redro-rehgihA0%ledom02%ot02%desu02%ylediw02%era02%shpargrepyh02%C2%shparg02%fo02%noitazilareneg02%a02%sA02%02%=3328342921.yrtne&regruB02%nitraM02%dna02%ihS02%naheK=526535609.yrtne&noitalopretni02%02%A0%atad02%rof02%sduolc02%tniop02%no02%noitaziraluger02%naicalpaL-42%p42%02%hpargrepyH=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Hypergraph%20%24p%24-Laplacian%20regularization%20on%20point%20clouds%20for%20data%0A%20%20interpolation&body=Title%3A%20Hypergraph%20%24p%24-Laplacian%20regularization%20on%20point%20clouds%20for%20data%0A%20%20interpolation%0AAuthor%3A%20Kehan%20Shi%20and%20Martin%20Burger%0AAbstract%3A%20%20%20As%20a%20generalization%20of%20graphs%2C%20hypergraphs%20are%20widely%20used%20to%20model%0Ahigher-order%20relations%20in%20data.%20This%20paper%20explores%20the%20benefit%20of%20the%0Ahypergraph%20structure%20for%20the%20interpolation%20of%20point%20cloud%20data%20that%20contain%20no%0Aexplicit%20structural%20information.%20We%20define%20the%20%24%5Cvarepsilon_n%24-ball%20hypergraph%0Aand%20the%20%24k_n%24-nearest%20neighbor%20hypergraph%20on%20a%20point%20cloud%20and%20study%20the%0A%24p%24-Laplacian%20regularization%20on%20the%20hypergraphs.%20We%20prove%20the%20variational%0Aconsistency%20between%20the%20hypergraph%20%24p%24-Laplacian%20regularization%20and%20the%0Acontinuum%20%24p%24-Laplacian%20regularization%20in%20a%20semisupervised%20setting%20when%20the%0Anumber%20of%20points%20%24n%24%20goes%20to%20infinity%20while%20the%20number%20of%20labeled%20points%0Aremains%20fixed.%20A%20key%20improvement%20compared%20to%20the%20graph%20case%20is%20that%20the%20results%0Arely%20on%20weaker%20assumptions%20on%20the%20upper%20bound%20of%20%24%5Cvarepsilon_n%24%20and%20%24k_n%24.%20To%0Asolve%20the%20convex%20but%20non-differentiable%20large-scale%20optimization%20problem%2C%20we%0Autilize%20the%20stochastic%20primal-dual%20hybrid%20gradient%20algorithm.%20Numerical%0Aexperiments%20on%20data%20interpolation%20verify%20that%20the%20hypergraph%20%24p%24-Laplacian%0Aregularization%20outperforms%20the%20graph%20%24p%24-Laplacian%20regularization%20in%20preventing%0Athe%20development%20of%20spikes%20at%20the%20labeled%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01109v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hypergraph%20%24p%24-Laplacian%20regularization%20on%20point%20clouds%20for%20data%0A%20%20interpolation&entry.906535625=Kehan%20Shi%20and%20Martin%20Burger&entry.1292438233=%20%20As%20a%20generalization%20of%20graphs%2C%20hypergraphs%20are%20widely%20used%20to%20model%0Ahigher-order%20relations%20in%20data.%20This%20paper%20explores%20the%20benefit%20of%20the%0Ahypergraph%20structure%20for%20the%20interpolation%20of%20point%20cloud%20data%20that%20contain%20no%0Aexplicit%20structural%20information.%20We%20define%20the%20%24%5Cvarepsilon_n%24-ball%20hypergraph%0Aand%20the%20%24k_n%24-nearest%20neighbor%20hypergraph%20on%20a%20point%20cloud%20and%20study%20the%0A%24p%24-Laplacian%20regularization%20on%20the%20hypergraphs.%20We%20prove%20the%20variational%0Aconsistency%20between%20the%20hypergraph%20%24p%24-Laplacian%20regularization%20and%20the%0Acontinuum%20%24p%24-Laplacian%20regularization%20in%20a%20semisupervised%20setting%20when%20the%0Anumber%20of%20points%20%24n%24%20goes%20to%20infinity%20while%20the%20number%20of%20labeled%20points%0Aremains%20fixed.%20A%20key%20improvement%20compared%20to%20the%20graph%20case%20is%20that%20the%20results%0Arely%20on%20weaker%20assumptions%20on%20the%20upper%20bound%20of%20%24%5Cvarepsilon_n%24%20and%20%24k_n%24.%20To%0Asolve%20the%20convex%20but%20non-differentiable%20large-scale%20optimization%20problem%2C%20we%0Autilize%20the%20stochastic%20primal-dual%20hybrid%20gradient%20algorithm.%20Numerical%0Aexperiments%20on%20data%20interpolation%20verify%20that%20the%20hypergraph%20%24p%24-Laplacian%0Aregularization%20outperforms%20the%20graph%20%24p%24-Laplacian%20regularization%20in%20preventing%0Athe%20development%20of%20spikes%20at%20the%20labeled%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01109v1&entry.124074799=Read"},
{"title": "Monotone, Bi-Lipschitz, and Polyak-Lojasiewicz Networks", "author": "Ruigang Wang and Krishnamurthy Dvijotham and Ian R. Manchester", "abstract": "  This paper presents a new \\emph{bi-Lipschitz} invertible neural network, the\nBiLipNet, which has the ability to control both its \\emph{Lipschitzness}\n(output sensitivity to input perturbations) and \\emph{inverse Lipschitzness}\n(input distinguishability from different outputs). The main contribution is a\nnovel invertible residual layer with certified strong monotonicity and\nLipschitzness, which we compose with orthogonal layers to build bi-Lipschitz\nnetworks. The certification is based on incremental quadratic constraints,\nwhich achieves much tighter bounds compared to spectral normalization.\nMoreover, we formulate the model inverse calculation as a three-operator\nsplitting problem, for which fast algorithms are known. Based on the proposed\nbi-Lipschitz network, we introduce a new scalar-output network, the PLNet,\nwhich satisfies the Polyak-\\L{}ojasiewicz condition. It can be applied to learn\nnon-convex surrogate losses with favourable properties, e.g., a unique and\nefficiently-computable global minimum.\n", "link": "http://arxiv.org/abs/2402.01344v3", "date": "2024-05-02", "relevancy": 2.3076, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4833}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4624}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4389}], "mailto": "mailto:daeR=997470421.yrtne&3v44310.2042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.muminim02%labolg02%elbatupmoc-yltneiciffeA0%dna02%euqinu02%a02%C2%.g.e02%C2%seitreporp02%elbaruovaf02%htiw02%sessol02%etagorrus02%xevnoc-nonA0%nrael02%ot02%deilppa02%eb02%nac02%tI02%.noitidnoc02%zciweisajoD7%B7%LC5%-kayloP02%eht02%seifsitas02%hcihwA0%C2%teNLP02%eht02%C2%krowten02%tuptuo-ralacs02%wen02%a02%ecudortni02%ew02%C2%krowten02%ztihcspiL-ibA0%desoporp02%eht02%no02%desaB02%.nwonk02%era02%smhtirogla02%tsaf02%hcihw02%rof02%C2%melborp02%gnittilpsA0%rotarepo-eerht02%a02%sa02%noitaluclac02%esrevni02%ledom02%eht02%etalumrof02%ew02%C2%revoeroMA0%.noitazilamron02%lartceps02%ot02%derapmoc02%sdnuob02%rethgit02%hcum02%seveihca02%hcihwA0%C2%stniartsnoc02%citardauq02%latnemercni02%no02%desab02%si02%noitacifitrec02%ehT02%.skrowtenA0%ztihcspiL-ib02%dliub02%ot02%sreyal02%lanogohtro02%htiw02%esopmoc02%ew02%hcihw02%C2%ssenztihcspiLA0%dna02%yticinotonom02%gnorts02%deifitrec02%htiw02%reyal02%laudiser02%elbitrevni02%levonA0%a02%si02%noitubirtnoc02%niam02%ehT02%.92%stuptuo02%tnereffid02%morf02%ytilibahsiugnitsid02%tupni82%A0%D7%ssenztihcspiL02%esrevniB7%hpmeC5%02%dna02%92%snoitabrutrep02%tupni02%ot02%ytivitisnes02%tuptuo82%A0%D7%ssenztihcspiLB7%hpmeC5%02%sti02%htob02%lortnoc02%ot02%ytiliba02%eht02%sah02%hcihw02%C2%teNpiLiBA0%eht02%C2%krowten02%laruen02%elbitrevni02%D7%ztihcspiL-ibB7%hpmeC5%02%wen02%a02%stneserp02%repap02%sihT02%02%=3328342921.yrtne&retsehcnaM02%.R02%naI02%dna02%mahtojivD02%yhtrumanhsirK02%dna02%gnaW02%gnagiuR=526535609.yrtne&skrowteN02%zciweisajoL-kayloP02%dna02%C2%ztihcspiL-iB02%C2%enotonoM=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Monotone%2C%20Bi-Lipschitz%2C%20and%20Polyak-Lojasiewicz%20Networks&body=Title%3A%20Monotone%2C%20Bi-Lipschitz%2C%20and%20Polyak-Lojasiewicz%20Networks%0AAuthor%3A%20Ruigang%20Wang%20and%20Krishnamurthy%20Dvijotham%20and%20Ian%20R.%20Manchester%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20new%20%5Cemph%7Bbi-Lipschitz%7D%20invertible%20neural%20network%2C%20the%0ABiLipNet%2C%20which%20has%20the%20ability%20to%20control%20both%20its%20%5Cemph%7BLipschitzness%7D%0A%28output%20sensitivity%20to%20input%20perturbations%29%20and%20%5Cemph%7Binverse%20Lipschitzness%7D%0A%28input%20distinguishability%20from%20different%20outputs%29.%20The%20main%20contribution%20is%20a%0Anovel%20invertible%20residual%20layer%20with%20certified%20strong%20monotonicity%20and%0ALipschitzness%2C%20which%20we%20compose%20with%20orthogonal%20layers%20to%20build%20bi-Lipschitz%0Anetworks.%20The%20certification%20is%20based%20on%20incremental%20quadratic%20constraints%2C%0Awhich%20achieves%20much%20tighter%20bounds%20compared%20to%20spectral%20normalization.%0AMoreover%2C%20we%20formulate%20the%20model%20inverse%20calculation%20as%20a%20three-operator%0Asplitting%20problem%2C%20for%20which%20fast%20algorithms%20are%20known.%20Based%20on%20the%20proposed%0Abi-Lipschitz%20network%2C%20we%20introduce%20a%20new%20scalar-output%20network%2C%20the%20PLNet%2C%0Awhich%20satisfies%20the%20Polyak-%5CL%7B%7Dojasiewicz%20condition.%20It%20can%20be%20applied%20to%20learn%0Anon-convex%20surrogate%20losses%20with%20favourable%20properties%2C%20e.g.%2C%20a%20unique%20and%0Aefficiently-computable%20global%20minimum.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.01344v3%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Monotone%2C%20Bi-Lipschitz%2C%20and%20Polyak-Lojasiewicz%20Networks&entry.906535625=Ruigang%20Wang%20and%20Krishnamurthy%20Dvijotham%20and%20Ian%20R.%20Manchester&entry.1292438233=%20%20This%20paper%20presents%20a%20new%20%5Cemph%7Bbi-Lipschitz%7D%20invertible%20neural%20network%2C%20the%0ABiLipNet%2C%20which%20has%20the%20ability%20to%20control%20both%20its%20%5Cemph%7BLipschitzness%7D%0A%28output%20sensitivity%20to%20input%20perturbations%29%20and%20%5Cemph%7Binverse%20Lipschitzness%7D%0A%28input%20distinguishability%20from%20different%20outputs%29.%20The%20main%20contribution%20is%20a%0Anovel%20invertible%20residual%20layer%20with%20certified%20strong%20monotonicity%20and%0ALipschitzness%2C%20which%20we%20compose%20with%20orthogonal%20layers%20to%20build%20bi-Lipschitz%0Anetworks.%20The%20certification%20is%20based%20on%20incremental%20quadratic%20constraints%2C%0Awhich%20achieves%20much%20tighter%20bounds%20compared%20to%20spectral%20normalization.%0AMoreover%2C%20we%20formulate%20the%20model%20inverse%20calculation%20as%20a%20three-operator%0Asplitting%20problem%2C%20for%20which%20fast%20algorithms%20are%20known.%20Based%20on%20the%20proposed%0Abi-Lipschitz%20network%2C%20we%20introduce%20a%20new%20scalar-output%20network%2C%20the%20PLNet%2C%0Awhich%20satisfies%20the%20Polyak-%5CL%7B%7Dojasiewicz%20condition.%20It%20can%20be%20applied%20to%20learn%0Anon-convex%20surrogate%20losses%20with%20favourable%20properties%2C%20e.g.%2C%20a%20unique%20and%0Aefficiently-computable%20global%20minimum.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.01344v3&entry.124074799=Read"},
{"title": "Radar-Based Localization For Autonomous Ground Vehicles In Suburban\n  Neighborhoods", "author": "Andrew J. Kramer and Christoffer Heckman", "abstract": "  For autonomous ground vehicles (AGVs) deployed in suburban neighborhoods and\nother human-centric environments the problem of localization remains a\nfundamental challenge. There are well established methods for localization with\nGPS, lidar, and cameras. But even in ideal conditions these have limitations.\nGPS is not always available and is often not accurate enough on its own, visual\nmethods have difficulty coping with appearance changes due to weather and other\nfactors, and lidar methods are prone to defective solutions due to ambiguous\nscene geometry. Radar on the other hand is not highly susceptible to these\nproblems, owing in part to its longer range. Further, radar is also robust to\nchallenging conditions that interfere with vision and lidar including fog,\nsmoke, rain, and darkness. We present a radar-based localization system that\nincludes a novel method for highly-accurate radar odometry for smooth,\nhigh-frequency relative pose estimation and a novel method for radar-based\nplace recognition and relocalization. We present experiments demonstrating our\nmethods' accuracy and reliability, which are comparable with \\new{other\nmethods' published results for radar localization and we find outperform a\nsimilar method as ours applied to lidar measurements}. Further, we show our\nmethods are lightweight enough to run on common low-power embedded hardware\nwith ample headroom for other autonomy functions.\n", "link": "http://arxiv.org/abs/2405.00600v1", "date": "2024-05-01", "relevancy": 2.2955, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6177}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.546}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5341}], "mailto": "mailto:daeR=997470421.yrtne&1v00600.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.snoitcnuf02%ymonotua02%rehto02%rof02%moordaeh02%elpma02%htiwA0%erawdrah02%deddebme02%rewop-wol02%nommoc02%no02%nur02%ot02%hguone02%thgiewthgil02%era02%sdohtemA0%ruo02%wohs02%ew02%C2%rehtruF02%.D7%stnemerusaem02%radil02%ot02%deilppa02%sruo02%sa02%dohtem02%ralimisA0%a02%mrofreptuo02%dnif02%ew02%dna02%noitazilacol02%radar02%rof02%stluser02%dehsilbup02%72%sdohtemA0%rehtoB7%wenC5%02%htiw02%elbarapmoc02%era02%hcihw02%C2%ytilibailer02%dna02%ycarucca02%72%sdohtemA0%ruo02%gnitartsnomed02%stnemirepxe02%tneserp02%eW02%.noitazilacoler02%dna02%noitingocer02%ecalpA0%desab-radar02%rof02%dohtem02%levon02%a02%dna02%noitamitse02%esop02%evitaler02%ycneuqerf-hgihA0%C2%htooms02%rof02%yrtemodo02%radar02%etarucca-ylhgih02%rof02%dohtem02%levon02%a02%sedulcniA0%taht02%metsys02%noitazilacol02%desab-radar02%a02%tneserp02%eW02%.ssenkrad02%dna02%C2%niar02%C2%ekomsA0%C2%gof02%gnidulcni02%radil02%dna02%noisiv02%htiw02%erefretni02%taht02%snoitidnoc02%gnignellahcA0%ot02%tsubor02%osla02%si02%radar02%C2%rehtruF02%.egnar02%regnol02%sti02%ot02%trap02%ni02%gniwo02%C2%smelborpA0%eseht02%ot02%elbitpecsus02%ylhgih02%ton02%si02%dnah02%rehto02%eht02%no02%radaR02%.yrtemoeg02%enecsA0%suougibma02%ot02%eud02%snoitulos02%evitcefed02%ot02%enorp02%era02%sdohtem02%radil02%dna02%C2%srotcafA0%rehto02%dna02%rehtaew02%ot02%eud02%segnahc02%ecnaraeppa02%htiw02%gnipoc02%ytluciffid02%evah02%sdohtemA0%lausiv02%C2%nwo02%sti02%no02%hguone02%etarucca02%ton02%netfo02%si02%dna02%elbaliava02%syawla02%ton02%si02%SPGA0%.snoitatimil02%evah02%eseht02%snoitidnoc02%laedi02%ni02%neve02%tuB02%.saremac02%dna02%C2%radil02%C2%SPGA0%htiw02%noitazilacol02%rof02%sdohtem02%dehsilbatse02%llew02%era02%erehT02%.egnellahc02%latnemadnufA0%a02%sniamer02%noitazilacol02%fo02%melborp02%eht02%stnemnorivne02%cirtnec-namuh02%rehtoA0%dna02%sdoohrobhgien02%nabrubus02%ni02%deyolped02%92%sVGA82%02%selcihev02%dnuorg02%suomonotua02%roF02%02%=3328342921.yrtne&namkceH02%reffotsirhC02%dna02%remarK02%.J02%werdnA=526535609.yrtne&sdoohrobhgieN02%02%A0%nabrubuS02%nI02%selciheV02%dnuorG02%suomonotuA02%roF02%noitazilacoL02%desaB-radaR=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Radar-Based%20Localization%20For%20Autonomous%20Ground%20Vehicles%20In%20Suburban%0A%20%20Neighborhoods&body=Title%3A%20Radar-Based%20Localization%20For%20Autonomous%20Ground%20Vehicles%20In%20Suburban%0A%20%20Neighborhoods%0AAuthor%3A%20Andrew%20J.%20Kramer%20and%20Christoffer%20Heckman%0AAbstract%3A%20%20%20For%20autonomous%20ground%20vehicles%20%28AGVs%29%20deployed%20in%20suburban%20neighborhoods%20and%0Aother%20human-centric%20environments%20the%20problem%20of%20localization%20remains%20a%0Afundamental%20challenge.%20There%20are%20well%20established%20methods%20for%20localization%20with%0AGPS%2C%20lidar%2C%20and%20cameras.%20But%20even%20in%20ideal%20conditions%20these%20have%20limitations.%0AGPS%20is%20not%20always%20available%20and%20is%20often%20not%20accurate%20enough%20on%20its%20own%2C%20visual%0Amethods%20have%20difficulty%20coping%20with%20appearance%20changes%20due%20to%20weather%20and%20other%0Afactors%2C%20and%20lidar%20methods%20are%20prone%20to%20defective%20solutions%20due%20to%20ambiguous%0Ascene%20geometry.%20Radar%20on%20the%20other%20hand%20is%20not%20highly%20susceptible%20to%20these%0Aproblems%2C%20owing%20in%20part%20to%20its%20longer%20range.%20Further%2C%20radar%20is%20also%20robust%20to%0Achallenging%20conditions%20that%20interfere%20with%20vision%20and%20lidar%20including%20fog%2C%0Asmoke%2C%20rain%2C%20and%20darkness.%20We%20present%20a%20radar-based%20localization%20system%20that%0Aincludes%20a%20novel%20method%20for%20highly-accurate%20radar%20odometry%20for%20smooth%2C%0Ahigh-frequency%20relative%20pose%20estimation%20and%20a%20novel%20method%20for%20radar-based%0Aplace%20recognition%20and%20relocalization.%20We%20present%20experiments%20demonstrating%20our%0Amethods%27%20accuracy%20and%20reliability%2C%20which%20are%20comparable%20with%20%5Cnew%7Bother%0Amethods%27%20published%20results%20for%20radar%20localization%20and%20we%20find%20outperform%20a%0Asimilar%20method%20as%20ours%20applied%20to%20lidar%20measurements%7D.%20Further%2C%20we%20show%20our%0Amethods%20are%20lightweight%20enough%20to%20run%20on%20common%20low-power%20embedded%20hardware%0Awith%20ample%20headroom%20for%20other%20autonomy%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00600v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Radar-Based%20Localization%20For%20Autonomous%20Ground%20Vehicles%20In%20Suburban%0A%20%20Neighborhoods&entry.906535625=Andrew%20J.%20Kramer%20and%20Christoffer%20Heckman&entry.1292438233=%20%20For%20autonomous%20ground%20vehicles%20%28AGVs%29%20deployed%20in%20suburban%20neighborhoods%20and%0Aother%20human-centric%20environments%20the%20problem%20of%20localization%20remains%20a%0Afundamental%20challenge.%20There%20are%20well%20established%20methods%20for%20localization%20with%0AGPS%2C%20lidar%2C%20and%20cameras.%20But%20even%20in%20ideal%20conditions%20these%20have%20limitations.%0AGPS%20is%20not%20always%20available%20and%20is%20often%20not%20accurate%20enough%20on%20its%20own%2C%20visual%0Amethods%20have%20difficulty%20coping%20with%20appearance%20changes%20due%20to%20weather%20and%20other%0Afactors%2C%20and%20lidar%20methods%20are%20prone%20to%20defective%20solutions%20due%20to%20ambiguous%0Ascene%20geometry.%20Radar%20on%20the%20other%20hand%20is%20not%20highly%20susceptible%20to%20these%0Aproblems%2C%20owing%20in%20part%20to%20its%20longer%20range.%20Further%2C%20radar%20is%20also%20robust%20to%0Achallenging%20conditions%20that%20interfere%20with%20vision%20and%20lidar%20including%20fog%2C%0Asmoke%2C%20rain%2C%20and%20darkness.%20We%20present%20a%20radar-based%20localization%20system%20that%0Aincludes%20a%20novel%20method%20for%20highly-accurate%20radar%20odometry%20for%20smooth%2C%0Ahigh-frequency%20relative%20pose%20estimation%20and%20a%20novel%20method%20for%20radar-based%0Aplace%20recognition%20and%20relocalization.%20We%20present%20experiments%20demonstrating%20our%0Amethods%27%20accuracy%20and%20reliability%2C%20which%20are%20comparable%20with%20%5Cnew%7Bother%0Amethods%27%20published%20results%20for%20radar%20localization%20and%20we%20find%20outperform%20a%0Asimilar%20method%20as%20ours%20applied%20to%20lidar%20measurements%7D.%20Further%2C%20we%20show%20our%0Amethods%20are%20lightweight%20enough%20to%20run%20on%20common%20low-power%20embedded%20hardware%0Awith%20ample%20headroom%20for%20other%20autonomy%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00600v1&entry.124074799=Read"},
{"title": "Improving Subject-Driven Image Synthesis with Subject-Agnostic Guidance", "author": "Kelvin C. K. Chan and Yang Zhao and Xuhui Jia and Ming-Hsuan Yang and Huisheng Wang", "abstract": "  In subject-driven text-to-image synthesis, the synthesis process tends to be\nheavily influenced by the reference images provided by users, often overlooking\ncrucial attributes detailed in the text prompt. In this work, we propose\nSubject-Agnostic Guidance (SAG), a simple yet effective solution to remedy the\nproblem. We show that through constructing a subject-agnostic condition and\napplying our proposed dual classifier-free guidance, one could obtain outputs\nconsistent with both the given subject and input text prompts. We validate the\nefficacy of our approach through both optimization-based and encoder-based\nmethods. Additionally, we demonstrate its applicability in second-order\ncustomization methods, where an encoder-based model is fine-tuned with\nDreamBooth. Our approach is conceptually simple and requires only minimal code\nmodifications, but leads to substantial quality improvements, as evidenced by\nour evaluations and user studies.\n", "link": "http://arxiv.org/abs/2405.01356v1", "date": "2024-05-02", "relevancy": 2.2778, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5779}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5646}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5629}], "mailto": "mailto:daeR=997470421.yrtne&1v65310.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.seiduts02%resu02%dna02%snoitaulave02%ruoA0%yb02%decnedive02%sa02%C2%stnemevorpmi02%ytilauq02%laitnatsbus02%ot02%sdael02%tub02%C2%snoitacifidomA0%edoc02%laminim02%ylno02%seriuqer02%dna02%elpmis02%yllautpecnoc02%si02%hcaorppa02%ruO02%.htooBmaerDA0%htiw02%denut-enif02%si02%ledom02%desab-redocne02%na02%erehw02%C2%sdohtem02%noitazimotsucA0%redro-dnoces02%ni02%ytilibacilppa02%sti02%etartsnomed02%ew02%C2%yllanoitiddA02%.sdohtemA0%desab-redocne02%dna02%desab-noitazimitpo02%htob02%hguorht02%hcaorppa02%ruo02%fo02%ycaciffeA0%eht02%etadilav02%eW02%.stpmorp02%txet02%tupni02%dna02%tcejbus02%nevig02%eht02%htob02%htiw02%tnetsisnocA0%stuptuo02%niatbo02%dluoc02%eno02%C2%ecnadiug02%eerf-reifissalc02%laud02%desoporp02%ruo02%gniylppaA0%dna02%noitidnoc02%citsonga-tcejbus02%a02%gnitcurtsnoc02%hguorht02%taht02%wohs02%eW02%.melborpA0%eht02%ydemer02%ot02%noitulos02%evitceffe02%tey02%elpmis02%a02%C2%92%GAS82%02%ecnadiuG02%citsongA-tcejbuSA0%esoporp02%ew02%C2%krow02%siht02%nI02%.tpmorp02%txet02%eht02%ni02%deliated02%setubirtta02%laicurcA0%gnikoolrevo02%netfo02%C2%sresu02%yb02%dedivorp02%segami02%ecnerefer02%eht02%yb02%decneulfni02%ylivaehA0%eb02%ot02%sdnet02%ssecorp02%sisehtnys02%eht02%C2%sisehtnys02%egami-ot-txet02%nevird-tcejbus02%nI02%02%=3328342921.yrtne&gnaW02%gnehsiuH02%dna02%gnaY02%nausH-gniM02%dna02%aiJ02%iuhuX02%dna02%oahZ02%gnaY02%dna02%nahC02%.K02%.C02%nivleK=526535609.yrtne&ecnadiuG02%citsongA-tcejbuS02%htiw02%sisehtnyS02%egamI02%nevirD-tcejbuS02%gnivorpmI=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Improving%20Subject-Driven%20Image%20Synthesis%20with%20Subject-Agnostic%20Guidance&body=Title%3A%20Improving%20Subject-Driven%20Image%20Synthesis%20with%20Subject-Agnostic%20Guidance%0AAuthor%3A%20Kelvin%20C.%20K.%20Chan%20and%20Yang%20Zhao%20and%20Xuhui%20Jia%20and%20Ming-Hsuan%20Yang%20and%20Huisheng%20Wang%0AAbstract%3A%20%20%20In%20subject-driven%20text-to-image%20synthesis%2C%20the%20synthesis%20process%20tends%20to%20be%0Aheavily%20influenced%20by%20the%20reference%20images%20provided%20by%20users%2C%20often%20overlooking%0Acrucial%20attributes%20detailed%20in%20the%20text%20prompt.%20In%20this%20work%2C%20we%20propose%0ASubject-Agnostic%20Guidance%20%28SAG%29%2C%20a%20simple%20yet%20effective%20solution%20to%20remedy%20the%0Aproblem.%20We%20show%20that%20through%20constructing%20a%20subject-agnostic%20condition%20and%0Aapplying%20our%20proposed%20dual%20classifier-free%20guidance%2C%20one%20could%20obtain%20outputs%0Aconsistent%20with%20both%20the%20given%20subject%20and%20input%20text%20prompts.%20We%20validate%20the%0Aefficacy%20of%20our%20approach%20through%20both%20optimization-based%20and%20encoder-based%0Amethods.%20Additionally%2C%20we%20demonstrate%20its%20applicability%20in%20second-order%0Acustomization%20methods%2C%20where%20an%20encoder-based%20model%20is%20fine-tuned%20with%0ADreamBooth.%20Our%20approach%20is%20conceptually%20simple%20and%20requires%20only%20minimal%20code%0Amodifications%2C%20but%20leads%20to%20substantial%20quality%20improvements%2C%20as%20evidenced%20by%0Aour%20evaluations%20and%20user%20studies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01356v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Subject-Driven%20Image%20Synthesis%20with%20Subject-Agnostic%20Guidance&entry.906535625=Kelvin%20C.%20K.%20Chan%20and%20Yang%20Zhao%20and%20Xuhui%20Jia%20and%20Ming-Hsuan%20Yang%20and%20Huisheng%20Wang&entry.1292438233=%20%20In%20subject-driven%20text-to-image%20synthesis%2C%20the%20synthesis%20process%20tends%20to%20be%0Aheavily%20influenced%20by%20the%20reference%20images%20provided%20by%20users%2C%20often%20overlooking%0Acrucial%20attributes%20detailed%20in%20the%20text%20prompt.%20In%20this%20work%2C%20we%20propose%0ASubject-Agnostic%20Guidance%20%28SAG%29%2C%20a%20simple%20yet%20effective%20solution%20to%20remedy%20the%0Aproblem.%20We%20show%20that%20through%20constructing%20a%20subject-agnostic%20condition%20and%0Aapplying%20our%20proposed%20dual%20classifier-free%20guidance%2C%20one%20could%20obtain%20outputs%0Aconsistent%20with%20both%20the%20given%20subject%20and%20input%20text%20prompts.%20We%20validate%20the%0Aefficacy%20of%20our%20approach%20through%20both%20optimization-based%20and%20encoder-based%0Amethods.%20Additionally%2C%20we%20demonstrate%20its%20applicability%20in%20second-order%0Acustomization%20methods%2C%20where%20an%20encoder-based%20model%20is%20fine-tuned%20with%0ADreamBooth.%20Our%20approach%20is%20conceptually%20simple%20and%20requires%20only%20minimal%20code%0Amodifications%2C%20but%20leads%20to%20substantial%20quality%20improvements%2C%20as%20evidenced%20by%0Aour%20evaluations%20and%20user%20studies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01356v1&entry.124074799=Read"},
{"title": "Adaptive Federated Learning with Auto-Tuned Clients", "author": "Junhyung Lyle Kim and Mohammad Taha Toghani and C\u00e9sar A. Uribe and Anastasios Kyrillidis", "abstract": "  Federated learning (FL) is a distributed machine learning framework where the\nglobal model of a central server is trained via multiple collaborative steps by\nparticipating clients without sharing their data. While being a flexible\nframework, where the distribution of local data, participation rate, and\ncomputing power of each client can greatly vary, such flexibility gives rise to\nmany new challenges, especially in the hyperparameter tuning on the client\nside. We propose $\\Delta$-SGD, a simple step size rule for SGD that enables\neach client to use its own step size by adapting to the local smoothness of the\nfunction each client is optimizing. We provide theoretical and empirical\nresults where the benefit of the client adaptivity is shown in various FL\nscenarios.\n", "link": "http://arxiv.org/abs/2306.11201v3", "date": "2024-05-02", "relevancy": 2.2758, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4632}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4518}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4505}], "mailto": "mailto:daeR=997470421.yrtne&3v10211.6032/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.soiranecsA0%LF02%suoirav02%ni02%nwohs02%si02%ytivitpada02%tneilc02%eht02%fo02%tifeneb02%eht02%erehw02%stluserA0%laciripme02%dna02%laciteroeht02%edivorp02%eW02%.gnizimitpo02%si02%tneilc02%hcae02%noitcnufA0%eht02%fo02%ssenhtooms02%lacol02%eht02%ot02%gnitpada02%yb02%ezis02%pets02%nwo02%sti02%esu02%ot02%tneilc02%hcaeA0%selbane02%taht02%DGS02%rof02%elur02%ezis02%pets02%elpmis02%a02%C2%DGS-42%atleDC5%42%02%esoporp02%eW02%.edisA0%tneilc02%eht02%no02%gninut02%retemaraprepyh02%eht02%ni02%yllaicepse02%C2%segnellahc02%wen02%ynamA0%ot02%esir02%sevig02%ytilibixelf02%hcus02%C2%yrav02%yltaerg02%nac02%tneilc02%hcae02%fo02%rewop02%gnitupmocA0%dna02%C2%etar02%noitapicitrap02%C2%atad02%lacol02%fo02%noitubirtsid02%eht02%erehw02%C2%krowemarfA0%elbixelf02%a02%gnieb02%elihW02%.atad02%rieht02%gnirahs02%tuohtiw02%stneilc02%gnitapicitrapA0%yb02%spets02%evitaroballoc02%elpitlum02%aiv02%deniart02%si02%revres02%lartnec02%a02%fo02%ledom02%labolgA0%eht02%erehw02%krowemarf02%gninrael02%enihcam02%detubirtsid02%a02%si02%92%LF82%02%gninrael02%detaredeF02%02%=3328342921.yrtne&sidilliryK02%soisatsanA02%dna02%ebirU02%.A02%ras9A%3C%C02%dna02%inahgoT02%ahaT02%dammahoM02%dna02%miK02%elyL02%gnuyhnuJ=526535609.yrtne&stneilC02%denuT-otuA02%htiw02%gninraeL02%detaredeF02%evitpadA=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Federated%20Learning%20with%20Auto-Tuned%20Clients&body=Title%3A%20Adaptive%20Federated%20Learning%20with%20Auto-Tuned%20Clients%0AAuthor%3A%20Junhyung%20Lyle%20Kim%20and%20Mohammad%20Taha%20Toghani%20and%20C%C3%A9sar%20A.%20Uribe%20and%20Anastasios%20Kyrillidis%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20is%20a%20distributed%20machine%20learning%20framework%20where%20the%0Aglobal%20model%20of%20a%20central%20server%20is%20trained%20via%20multiple%20collaborative%20steps%20by%0Aparticipating%20clients%20without%20sharing%20their%20data.%20While%20being%20a%20flexible%0Aframework%2C%20where%20the%20distribution%20of%20local%20data%2C%20participation%20rate%2C%20and%0Acomputing%20power%20of%20each%20client%20can%20greatly%20vary%2C%20such%20flexibility%20gives%20rise%20to%0Amany%20new%20challenges%2C%20especially%20in%20the%20hyperparameter%20tuning%20on%20the%20client%0Aside.%20We%20propose%20%24%5CDelta%24-SGD%2C%20a%20simple%20step%20size%20rule%20for%20SGD%20that%20enables%0Aeach%20client%20to%20use%20its%20own%20step%20size%20by%20adapting%20to%20the%20local%20smoothness%20of%20the%0Afunction%20each%20client%20is%20optimizing.%20We%20provide%20theoretical%20and%20empirical%0Aresults%20where%20the%20benefit%20of%20the%20client%20adaptivity%20is%20shown%20in%20various%20FL%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.11201v3%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Federated%20Learning%20with%20Auto-Tuned%20Clients&entry.906535625=Junhyung%20Lyle%20Kim%20and%20Mohammad%20Taha%20Toghani%20and%20C%C3%A9sar%20A.%20Uribe%20and%20Anastasios%20Kyrillidis&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20is%20a%20distributed%20machine%20learning%20framework%20where%20the%0Aglobal%20model%20of%20a%20central%20server%20is%20trained%20via%20multiple%20collaborative%20steps%20by%0Aparticipating%20clients%20without%20sharing%20their%20data.%20While%20being%20a%20flexible%0Aframework%2C%20where%20the%20distribution%20of%20local%20data%2C%20participation%20rate%2C%20and%0Acomputing%20power%20of%20each%20client%20can%20greatly%20vary%2C%20such%20flexibility%20gives%20rise%20to%0Amany%20new%20challenges%2C%20especially%20in%20the%20hyperparameter%20tuning%20on%20the%20client%0Aside.%20We%20propose%20%24%5CDelta%24-SGD%2C%20a%20simple%20step%20size%20rule%20for%20SGD%20that%20enables%0Aeach%20client%20to%20use%20its%20own%20step%20size%20by%20adapting%20to%20the%20local%20smoothness%20of%20the%0Afunction%20each%20client%20is%20optimizing.%20We%20provide%20theoretical%20and%20empirical%0Aresults%20where%20the%20benefit%20of%20the%20client%20adaptivity%20is%20shown%20in%20various%20FL%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.11201v3&entry.124074799=Read"},
{"title": "Poisoning Attacks on Federated Learning for Autonomous Driving", "author": "Sonakshi Garg and Hugo J\u00f6nsson and Gustav Kalander and Axel Nilsson and Bhhaanu Pirange and Viktor Valadi and Johan \u00d6stman", "abstract": "  Federated Learning (FL) is a decentralized learning paradigm, enabling\nparties to collaboratively train models while keeping their data confidential.\nWithin autonomous driving, it brings the potential of reducing data storage\ncosts, reducing bandwidth requirements, and to accelerate the learning. FL is,\nhowever, susceptible to poisoning attacks. In this paper, we introduce two\nnovel poisoning attacks on FL tailored to regression tasks within autonomous\ndriving: FLStealth and Off-Track Attack (OTA). FLStealth, an untargeted attack,\naims at providing model updates that deteriorate the global model performance\nwhile appearing benign. OTA, on the other hand, is a targeted attack with the\nobjective to change the global model's behavior when exposed to a certain\ntrigger. We demonstrate the effectiveness of our attacks by conducting\ncomprehensive experiments pertaining to the task of vehicle trajectory\nprediction. In particular, we show that, among five different untargeted\nattacks, FLStealth is the most successful at bypassing the considered defenses\nemployed by the server. For OTA, we demonstrate the inability of common defense\nstrategies to mitigate the attack, highlighting the critical need for new\ndefensive mechanisms against targeted attacks within FL for autonomous driving.\n", "link": "http://arxiv.org/abs/2405.01073v1", "date": "2024-05-02", "relevancy": 2.2756, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4738}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4487}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4429}], "mailto": "mailto:daeR=997470421.yrtne&1v37010.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.gnivird02%suomonotua02%rof02%LF02%nihtiw02%skcatta02%detegrat02%tsniaga02%smsinahcem02%evisnefedA0%wen02%rof02%deen02%lacitirc02%eht02%gnithgilhgih02%C2%kcatta02%eht02%etagitim02%ot02%seigetartsA0%esnefed02%nommoc02%fo02%ytilibani02%eht02%etartsnomed02%ew02%C2%ATO02%roF02%.revres02%eht02%yb02%deyolpmeA0%sesnefed02%deredisnoc02%eht02%gnissapyb02%ta02%lufsseccus02%tsom02%eht02%si02%htlaetSLF02%C2%skcattaA0%detegratnu02%tnereffid02%evif02%gnoma02%C2%taht02%wohs02%ew02%C2%ralucitrap02%nI02%.noitciderpA0%yrotcejart02%elcihev02%fo02%ksat02%eht02%ot02%gniniatrep02%stnemirepxe02%evisneherpmocA0%gnitcudnoc02%yb02%skcatta02%ruo02%fo02%ssenevitceffe02%eht02%etartsnomed02%eW02%.reggirtA0%niatrec02%a02%ot02%desopxe02%nehw02%roivaheb02%s72%ledom02%labolg02%eht02%egnahc02%ot02%evitcejboA0%eht02%htiw02%kcatta02%detegrat02%a02%si02%C2%dnah02%rehto02%eht02%no02%C2%ATO02%.ngineb02%gniraeppa02%elihwA0%ecnamrofrep02%ledom02%labolg02%eht02%etaroireted02%taht02%setadpu02%ledom02%gnidivorp02%ta02%smiaA0%C2%kcatta02%detegratnu02%na02%C2%htlaetSLF02%.92%ATO82%02%kcattA02%kcarT-ffO02%dna02%htlaetSLF02%A3%gnivirdA0%suomonotua02%nihtiw02%sksat02%noisserger02%ot02%deroliat02%LF02%no02%skcatta02%gninosiop02%levonA0%owt02%ecudortni02%ew02%C2%repap02%siht02%nI02%.skcatta02%gninosiop02%ot02%elbitpecsus02%C2%revewohA0%C2%si02%LF02%.gninrael02%eht02%etarelecca02%ot02%dna02%C2%stnemeriuqer02%htdiwdnab02%gnicuder02%C2%stsocA0%egarots02%atad02%gnicuder02%fo02%laitnetop02%eht02%sgnirb02%ti02%C2%gnivird02%suomonotua02%nihtiWA0%.laitnedifnoc02%atad02%rieht02%gnipeek02%elihw02%sledom02%niart02%ylevitaroballoc02%ot02%seitrapA0%gnilbane02%C2%mgidarap02%gninrael02%dezilartneced02%a02%si02%92%LF82%02%gninraeL02%detaredeF02%02%=3328342921.yrtne&namts69%3C%02%nahoJ02%dna02%idalaV02%rotkiV02%dna02%egnariP02%unaahhB02%dna02%nossliN02%lexA02%dna02%rednalaK02%vatsuG02%dna02%nossn6B%3C%J02%oguH02%dna02%graG02%ihskanoS=526535609.yrtne&gnivirD02%suomonotuA02%rof02%gninraeL02%detaredeF02%no02%skcattA02%gninosioP=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Poisoning%20Attacks%20on%20Federated%20Learning%20for%20Autonomous%20Driving&body=Title%3A%20Poisoning%20Attacks%20on%20Federated%20Learning%20for%20Autonomous%20Driving%0AAuthor%3A%20Sonakshi%20Garg%20and%20Hugo%20J%C3%B6nsson%20and%20Gustav%20Kalander%20and%20Axel%20Nilsson%20and%20Bhhaanu%20Pirange%20and%20Viktor%20Valadi%20and%20Johan%20%C3%96stman%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20decentralized%20learning%20paradigm%2C%20enabling%0Aparties%20to%20collaboratively%20train%20models%20while%20keeping%20their%20data%20confidential.%0AWithin%20autonomous%20driving%2C%20it%20brings%20the%20potential%20of%20reducing%20data%20storage%0Acosts%2C%20reducing%20bandwidth%20requirements%2C%20and%20to%20accelerate%20the%20learning.%20FL%20is%2C%0Ahowever%2C%20susceptible%20to%20poisoning%20attacks.%20In%20this%20paper%2C%20we%20introduce%20two%0Anovel%20poisoning%20attacks%20on%20FL%20tailored%20to%20regression%20tasks%20within%20autonomous%0Adriving%3A%20FLStealth%20and%20Off-Track%20Attack%20%28OTA%29.%20FLStealth%2C%20an%20untargeted%20attack%2C%0Aaims%20at%20providing%20model%20updates%20that%20deteriorate%20the%20global%20model%20performance%0Awhile%20appearing%20benign.%20OTA%2C%20on%20the%20other%20hand%2C%20is%20a%20targeted%20attack%20with%20the%0Aobjective%20to%20change%20the%20global%20model%27s%20behavior%20when%20exposed%20to%20a%20certain%0Atrigger.%20We%20demonstrate%20the%20effectiveness%20of%20our%20attacks%20by%20conducting%0Acomprehensive%20experiments%20pertaining%20to%20the%20task%20of%20vehicle%20trajectory%0Aprediction.%20In%20particular%2C%20we%20show%20that%2C%20among%20five%20different%20untargeted%0Aattacks%2C%20FLStealth%20is%20the%20most%20successful%20at%20bypassing%20the%20considered%20defenses%0Aemployed%20by%20the%20server.%20For%20OTA%2C%20we%20demonstrate%20the%20inability%20of%20common%20defense%0Astrategies%20to%20mitigate%20the%20attack%2C%20highlighting%20the%20critical%20need%20for%20new%0Adefensive%20mechanisms%20against%20targeted%20attacks%20within%20FL%20for%20autonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01073v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Poisoning%20Attacks%20on%20Federated%20Learning%20for%20Autonomous%20Driving&entry.906535625=Sonakshi%20Garg%20and%20Hugo%20J%C3%B6nsson%20and%20Gustav%20Kalander%20and%20Axel%20Nilsson%20and%20Bhhaanu%20Pirange%20and%20Viktor%20Valadi%20and%20Johan%20%C3%96stman&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20decentralized%20learning%20paradigm%2C%20enabling%0Aparties%20to%20collaboratively%20train%20models%20while%20keeping%20their%20data%20confidential.%0AWithin%20autonomous%20driving%2C%20it%20brings%20the%20potential%20of%20reducing%20data%20storage%0Acosts%2C%20reducing%20bandwidth%20requirements%2C%20and%20to%20accelerate%20the%20learning.%20FL%20is%2C%0Ahowever%2C%20susceptible%20to%20poisoning%20attacks.%20In%20this%20paper%2C%20we%20introduce%20two%0Anovel%20poisoning%20attacks%20on%20FL%20tailored%20to%20regression%20tasks%20within%20autonomous%0Adriving%3A%20FLStealth%20and%20Off-Track%20Attack%20%28OTA%29.%20FLStealth%2C%20an%20untargeted%20attack%2C%0Aaims%20at%20providing%20model%20updates%20that%20deteriorate%20the%20global%20model%20performance%0Awhile%20appearing%20benign.%20OTA%2C%20on%20the%20other%20hand%2C%20is%20a%20targeted%20attack%20with%20the%0Aobjective%20to%20change%20the%20global%20model%27s%20behavior%20when%20exposed%20to%20a%20certain%0Atrigger.%20We%20demonstrate%20the%20effectiveness%20of%20our%20attacks%20by%20conducting%0Acomprehensive%20experiments%20pertaining%20to%20the%20task%20of%20vehicle%20trajectory%0Aprediction.%20In%20particular%2C%20we%20show%20that%2C%20among%20five%20different%20untargeted%0Aattacks%2C%20FLStealth%20is%20the%20most%20successful%20at%20bypassing%20the%20considered%20defenses%0Aemployed%20by%20the%20server.%20For%20OTA%2C%20we%20demonstrate%20the%20inability%20of%20common%20defense%0Astrategies%20to%20mitigate%20the%20attack%2C%20highlighting%20the%20critical%20need%20for%20new%0Adefensive%20mechanisms%20against%20targeted%20attacks%20within%20FL%20for%20autonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01073v1&entry.124074799=Read"},
{"title": "SeaTurtleID2022: A long-span dataset for reliable sea turtle\n  re-identification", "author": "Luk\u00e1\u0161 Adam and Vojt\u011bch \u010cerm\u00e1k and Kostas Papafitsoros and Luk\u00e1\u0161 Picek", "abstract": "  This paper introduces the first public large-scale, long-span dataset with\nsea turtle photographs captured in the wild --\n\\href{https://www.kaggle.com/datasets/wildlifedatasets/seaturtleid2022}{SeaTurtleID2022}.\nThe dataset contains 8729 photographs of 438 unique individuals collected\nwithin 13 years, making it the longest-spanned dataset for animal\nre-identification. All photographs include various annotations, e.g., identity,\nencounter timestamp, and body parts segmentation masks. Instead of standard\n\"random\" splits, the dataset allows for two realistic and ecologically\nmotivated splits: (i) a \\textit{time-aware closed-set} with training,\nvalidation, and test data from different days/years, and (ii) a\n\\textit{time-aware open-set} with new unknown individuals in test and\nvalidation sets. We show that time-aware splits are essential for benchmarking\nre-identification methods, as random splits lead to performance overestimation.\nFurthermore, a baseline instance segmentation and re-identification performance\nover various body parts is provided. Finally, an end-to-end system for sea\nturtle re-identification is proposed and evaluated. The proposed system based\non Hybrid Task Cascade for head instance segmentation and ArcFace-trained\nfeature-extractor achieved an accuracy of 86.8\\%.\n", "link": "http://arxiv.org/abs/2211.10307v4", "date": "2024-05-01", "relevancy": 2.2704, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4729}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4455}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4438}], "mailto": "mailto:daeR=997470421.yrtne&4v70301.1122/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.52%C5%8.6802%fo02%ycarucca02%na02%deveihca02%rotcartxe-erutaefA0%deniart-ecaFcrA02%dna02%noitatnemges02%ecnatsni02%daeh02%rof02%edacsaC02%ksaT02%dirbyH02%noA0%desab02%metsys02%desoporp02%ehT02%.detaulave02%dna02%desoporp02%si02%noitacifitnedi-er02%eltrutA0%aes02%rof02%metsys02%dne-ot-dne02%na02%C2%yllaniF02%.dedivorp02%si02%strap02%ydob02%suoirav02%revoA0%ecnamrofrep02%noitacifitnedi-er02%dna02%noitatnemges02%ecnatsni02%enilesab02%a02%C2%eromrehtruFA0%.noitamitserevo02%ecnamrofrep02%ot02%dael02%stilps02%modnar02%sa02%C2%sdohtem02%noitacifitnedi-erA0%gnikramhcneb02%rof02%laitnesse02%era02%stilps02%erawa-emit02%taht02%wohs02%eW02%.stes02%noitadilavA0%dna02%tset02%ni02%slaudividni02%nwonknu02%wen02%htiw02%D7%tes-nepo02%erawa-emitB7%titxetC5%A0%a02%92%ii82%02%dna02%C2%sraey/syad02%tnereffid02%morf02%atad02%tset02%dna02%C2%noitadilavA0%C2%gniniart02%htiw02%D7%tes-desolc02%erawa-emitB7%titxetC5%02%a02%92%i82%02%A3%stilps02%detavitomA0%yllacigoloce02%dna02%citsilaer02%owt02%rof02%swolla02%tesatad02%eht02%C2%stilps02%22%modnar22%A0%dradnats02%fo02%daetsnI02%.sksam02%noitatnemges02%strap02%ydob02%dna02%C2%pmatsemit02%retnuocneA0%C2%ytitnedi02%C2%.g.e02%C2%snoitatonna02%suoirav02%edulcni02%shpargotohp02%llA02%.noitacifitnedi-erA0%lamina02%rof02%tesatad02%dennaps-tsegnol02%eht02%ti02%gnikam02%C2%sraey02%3102%nihtiwA0%detcelloc02%slaudividni02%euqinu02%83402%fo02%shpargotohp02%927802%sniatnoc02%tesatad02%ehTA0%.D7%2202DIeltruTaeSB7%D7%2202dieltrutaes/stesatadefildliw/stesatad/moc.elggak.www//A3%sptthB7%ferhC5%A0%--02%dliw02%eht02%ni02%derutpac02%shpargotohp02%eltrut02%aesA0%htiw02%tesatad02%naps-gnol02%C2%elacs-egral02%cilbup02%tsrif02%eht02%secudortni02%repap02%sihT02%02%=3328342921.yrtne&keciP02%1A%5C%1A%3C%kuL02%dna02%sorostifapaP02%satsoK02%dna02%k1A%3C%mreC8%4C%02%hcB9%4C%tjoV02%dna02%madA02%1A%5C%1A%3C%kuL=526535609.yrtne&noitacifitnedi-er02%02%A0%eltrut02%aes02%elbailer02%rof02%tesatad02%naps-gnol02%A02%A3%2202DIeltruTaeS=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20SeaTurtleID2022%3A%20A%20long-span%20dataset%20for%20reliable%20sea%20turtle%0A%20%20re-identification&body=Title%3A%20SeaTurtleID2022%3A%20A%20long-span%20dataset%20for%20reliable%20sea%20turtle%0A%20%20re-identification%0AAuthor%3A%20Luk%C3%A1%C5%A1%20Adam%20and%20Vojt%C4%9Bch%20%C4%8Cerm%C3%A1k%20and%20Kostas%20Papafitsoros%20and%20Luk%C3%A1%C5%A1%20Picek%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20first%20public%20large-scale%2C%20long-span%20dataset%20with%0Asea%20turtle%20photographs%20captured%20in%20the%20wild%20--%0A%5Chref%7Bhttps%3A//www.kaggle.com/datasets/wildlifedatasets/seaturtleid2022%7D%7BSeaTurtleID2022%7D.%0AThe%20dataset%20contains%208729%20photographs%20of%20438%20unique%20individuals%20collected%0Awithin%2013%20years%2C%20making%20it%20the%20longest-spanned%20dataset%20for%20animal%0Are-identification.%20All%20photographs%20include%20various%20annotations%2C%20e.g.%2C%20identity%2C%0Aencounter%20timestamp%2C%20and%20body%20parts%20segmentation%20masks.%20Instead%20of%20standard%0A%22random%22%20splits%2C%20the%20dataset%20allows%20for%20two%20realistic%20and%20ecologically%0Amotivated%20splits%3A%20%28i%29%20a%20%5Ctextit%7Btime-aware%20closed-set%7D%20with%20training%2C%0Avalidation%2C%20and%20test%20data%20from%20different%20days/years%2C%20and%20%28ii%29%20a%0A%5Ctextit%7Btime-aware%20open-set%7D%20with%20new%20unknown%20individuals%20in%20test%20and%0Avalidation%20sets.%20We%20show%20that%20time-aware%20splits%20are%20essential%20for%20benchmarking%0Are-identification%20methods%2C%20as%20random%20splits%20lead%20to%20performance%20overestimation.%0AFurthermore%2C%20a%20baseline%20instance%20segmentation%20and%20re-identification%20performance%0Aover%20various%20body%20parts%20is%20provided.%20Finally%2C%20an%20end-to-end%20system%20for%20sea%0Aturtle%20re-identification%20is%20proposed%20and%20evaluated.%20The%20proposed%20system%20based%0Aon%20Hybrid%20Task%20Cascade%20for%20head%20instance%20segmentation%20and%20ArcFace-trained%0Afeature-extractor%20achieved%20an%20accuracy%20of%2086.8%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.10307v4%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SeaTurtleID2022%3A%20A%20long-span%20dataset%20for%20reliable%20sea%20turtle%0A%20%20re-identification&entry.906535625=Luk%C3%A1%C5%A1%20Adam%20and%20Vojt%C4%9Bch%20%C4%8Cerm%C3%A1k%20and%20Kostas%20Papafitsoros%20and%20Luk%C3%A1%C5%A1%20Picek&entry.1292438233=%20%20This%20paper%20introduces%20the%20first%20public%20large-scale%2C%20long-span%20dataset%20with%0Asea%20turtle%20photographs%20captured%20in%20the%20wild%20--%0A%5Chref%7Bhttps%3A//www.kaggle.com/datasets/wildlifedatasets/seaturtleid2022%7D%7BSeaTurtleID2022%7D.%0AThe%20dataset%20contains%208729%20photographs%20of%20438%20unique%20individuals%20collected%0Awithin%2013%20years%2C%20making%20it%20the%20longest-spanned%20dataset%20for%20animal%0Are-identification.%20All%20photographs%20include%20various%20annotations%2C%20e.g.%2C%20identity%2C%0Aencounter%20timestamp%2C%20and%20body%20parts%20segmentation%20masks.%20Instead%20of%20standard%0A%22random%22%20splits%2C%20the%20dataset%20allows%20for%20two%20realistic%20and%20ecologically%0Amotivated%20splits%3A%20%28i%29%20a%20%5Ctextit%7Btime-aware%20closed-set%7D%20with%20training%2C%0Avalidation%2C%20and%20test%20data%20from%20different%20days/years%2C%20and%20%28ii%29%20a%0A%5Ctextit%7Btime-aware%20open-set%7D%20with%20new%20unknown%20individuals%20in%20test%20and%0Avalidation%20sets.%20We%20show%20that%20time-aware%20splits%20are%20essential%20for%20benchmarking%0Are-identification%20methods%2C%20as%20random%20splits%20lead%20to%20performance%20overestimation.%0AFurthermore%2C%20a%20baseline%20instance%20segmentation%20and%20re-identification%20performance%0Aover%20various%20body%20parts%20is%20provided.%20Finally%2C%20an%20end-to-end%20system%20for%20sea%0Aturtle%20re-identification%20is%20proposed%20and%20evaluated.%20The%20proposed%20system%20based%0Aon%20Hybrid%20Task%20Cascade%20for%20head%20instance%20segmentation%20and%20ArcFace-trained%0Afeature-extractor%20achieved%20an%20accuracy%20of%2086.8%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.10307v4&entry.124074799=Read"},
{"title": "Error-Driven Uncertainty Aware Training", "author": "Pedro Mendes and Paolo Romano and David Garlan", "abstract": "  Neural networks are often overconfident about their predictions, which\nundermines their reliability and trustworthiness. In this work, we present a\nnovel technique, named Error-Driven Uncertainty Aware Training (EUAT), which\naims to enhance the ability of neural models to estimate their uncertainty\ncorrectly, namely to be highly uncertain when they output inaccurate\npredictions and low uncertain when their output is accurate. The EUAT approach\noperates during the model's training phase by selectively employing two loss\nfunctions depending on whether the training examples are correctly or\nincorrectly predicted by the model. This allows for pursuing the twofold goal\nof i) minimizing model uncertainty for correctly predicted inputs and ii)\nmaximizing uncertainty for mispredicted inputs, while preserving the model's\nmisprediction rate. We evaluate EUAT using diverse neural models and datasets\nin the image recognition domains considering both non-adversarial and\nadversarial settings. The results show that EUAT outperforms existing\napproaches for uncertainty estimation (including other uncertainty-aware\ntraining techniques, calibration, ensembles, and DEUP) by providing uncertainty\nestimates that not only have higher quality when evaluated via statistical\nmetrics (e.g., correlation with residuals) but also when employed to build\nbinary classifiers that decide whether the model's output can be trusted or not\nand under distributional data shifts.\n", "link": "http://arxiv.org/abs/2405.01205v1", "date": "2024-05-02", "relevancy": 2.2689, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6376}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5758}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5305}], "mailto": "mailto:daeR=997470421.yrtne&1v50210.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.stfihs02%atad02%lanoitubirtsid02%rednu02%dnaA0%ton02%ro02%detsurt02%eb02%nac02%tuptuo02%s72%ledom02%eht02%rehtehw02%ediced02%taht02%sreifissalc02%yranibA0%dliub02%ot02%deyolpme02%nehw02%osla02%tub02%92%slaudiser02%htiw02%noitalerroc02%C2%.g.e82%02%scirtemA0%lacitsitats02%aiv02%detaulave02%nehw02%ytilauq02%rehgih02%evah02%ylno02%ton02%taht02%setamitseA0%ytniatrecnu02%gnidivorp02%yb02%92%PUED02%dna02%C2%selbmesne02%C2%noitarbilac02%C2%seuqinhcet02%gniniartA0%erawa-ytniatrecnu02%rehto02%gnidulcni82%02%noitamitse02%ytniatrecnu02%rof02%sehcaorppaA0%gnitsixe02%smrofreptuo02%TAUE02%taht02%wohs02%stluser02%ehT02%.sgnittes02%lairasrevdaA0%dna02%lairasrevda-non02%htob02%gniredisnoc02%sniamod02%noitingocer02%egami02%eht02%niA0%stesatad02%dna02%sledom02%laruen02%esrevid02%gnisu02%TAUE02%etaulave02%eW02%.etar02%noitciderpsimA0%s72%ledom02%eht02%gnivreserp02%elihw02%C2%stupni02%detciderpsim02%rof02%ytniatrecnu02%gnizimixamA0%92%ii02%dna02%stupni02%detciderp02%yltcerroc02%rof02%ytniatrecnu02%ledom02%gniziminim02%92%i02%foA0%laog02%dlofowt02%eht02%gniusrup02%rof02%swolla02%sihT02%.ledom02%eht02%yb02%detciderp02%yltcerrocniA0%ro02%yltcerroc02%era02%selpmaxe02%gniniart02%eht02%rehtehw02%no02%gnidneped02%snoitcnufA0%ssol02%owt02%gniyolpme02%ylevitceles02%yb02%esahp02%gniniart02%s72%ledom02%eht02%gnirud02%setarepoA0%hcaorppa02%TAUE02%ehT02%.etarucca02%si02%tuptuo02%rieht02%nehw02%niatrecnu02%wol02%dna02%snoitciderpA0%etaruccani02%tuptuo02%yeht02%nehw02%niatrecnu02%ylhgih02%eb02%ot02%yleman02%C2%yltcerrocA0%ytniatrecnu02%rieht02%etamitse02%ot02%sledom02%laruen02%fo02%ytiliba02%eht02%ecnahne02%ot02%smiaA0%hcihw02%C2%92%TAUE82%02%gniniarT02%erawA02%ytniatrecnU02%nevirD-rorrE02%deman02%C2%euqinhcet02%levonA0%a02%tneserp02%ew02%C2%krow02%siht02%nI02%.ssenihtrowtsurt02%dna02%ytilibailer02%rieht02%senimrednuA0%hcihw02%C2%snoitciderp02%rieht02%tuoba02%tnedifnocrevo02%netfo02%era02%skrowten02%larueN02%02%=3328342921.yrtne&nalraG02%divaD02%dna02%onamoR02%oloaP02%dna02%sedneM02%ordeP=526535609.yrtne&gniniarT02%erawA02%ytniatrecnU02%nevirD-rorrE=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Error-Driven%20Uncertainty%20Aware%20Training&body=Title%3A%20Error-Driven%20Uncertainty%20Aware%20Training%0AAuthor%3A%20Pedro%20Mendes%20and%20Paolo%20Romano%20and%20David%20Garlan%0AAbstract%3A%20%20%20Neural%20networks%20are%20often%20overconfident%20about%20their%20predictions%2C%20which%0Aundermines%20their%20reliability%20and%20trustworthiness.%20In%20this%20work%2C%20we%20present%20a%0Anovel%20technique%2C%20named%20Error-Driven%20Uncertainty%20Aware%20Training%20%28EUAT%29%2C%20which%0Aaims%20to%20enhance%20the%20ability%20of%20neural%20models%20to%20estimate%20their%20uncertainty%0Acorrectly%2C%20namely%20to%20be%20highly%20uncertain%20when%20they%20output%20inaccurate%0Apredictions%20and%20low%20uncertain%20when%20their%20output%20is%20accurate.%20The%20EUAT%20approach%0Aoperates%20during%20the%20model%27s%20training%20phase%20by%20selectively%20employing%20two%20loss%0Afunctions%20depending%20on%20whether%20the%20training%20examples%20are%20correctly%20or%0Aincorrectly%20predicted%20by%20the%20model.%20This%20allows%20for%20pursuing%20the%20twofold%20goal%0Aof%20i%29%20minimizing%20model%20uncertainty%20for%20correctly%20predicted%20inputs%20and%20ii%29%0Amaximizing%20uncertainty%20for%20mispredicted%20inputs%2C%20while%20preserving%20the%20model%27s%0Amisprediction%20rate.%20We%20evaluate%20EUAT%20using%20diverse%20neural%20models%20and%20datasets%0Ain%20the%20image%20recognition%20domains%20considering%20both%20non-adversarial%20and%0Aadversarial%20settings.%20The%20results%20show%20that%20EUAT%20outperforms%20existing%0Aapproaches%20for%20uncertainty%20estimation%20%28including%20other%20uncertainty-aware%0Atraining%20techniques%2C%20calibration%2C%20ensembles%2C%20and%20DEUP%29%20by%20providing%20uncertainty%0Aestimates%20that%20not%20only%20have%20higher%20quality%20when%20evaluated%20via%20statistical%0Ametrics%20%28e.g.%2C%20correlation%20with%20residuals%29%20but%20also%20when%20employed%20to%20build%0Abinary%20classifiers%20that%20decide%20whether%20the%20model%27s%20output%20can%20be%20trusted%20or%20not%0Aand%20under%20distributional%20data%20shifts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01205v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Error-Driven%20Uncertainty%20Aware%20Training&entry.906535625=Pedro%20Mendes%20and%20Paolo%20Romano%20and%20David%20Garlan&entry.1292438233=%20%20Neural%20networks%20are%20often%20overconfident%20about%20their%20predictions%2C%20which%0Aundermines%20their%20reliability%20and%20trustworthiness.%20In%20this%20work%2C%20we%20present%20a%0Anovel%20technique%2C%20named%20Error-Driven%20Uncertainty%20Aware%20Training%20%28EUAT%29%2C%20which%0Aaims%20to%20enhance%20the%20ability%20of%20neural%20models%20to%20estimate%20their%20uncertainty%0Acorrectly%2C%20namely%20to%20be%20highly%20uncertain%20when%20they%20output%20inaccurate%0Apredictions%20and%20low%20uncertain%20when%20their%20output%20is%20accurate.%20The%20EUAT%20approach%0Aoperates%20during%20the%20model%27s%20training%20phase%20by%20selectively%20employing%20two%20loss%0Afunctions%20depending%20on%20whether%20the%20training%20examples%20are%20correctly%20or%0Aincorrectly%20predicted%20by%20the%20model.%20This%20allows%20for%20pursuing%20the%20twofold%20goal%0Aof%20i%29%20minimizing%20model%20uncertainty%20for%20correctly%20predicted%20inputs%20and%20ii%29%0Amaximizing%20uncertainty%20for%20mispredicted%20inputs%2C%20while%20preserving%20the%20model%27s%0Amisprediction%20rate.%20We%20evaluate%20EUAT%20using%20diverse%20neural%20models%20and%20datasets%0Ain%20the%20image%20recognition%20domains%20considering%20both%20non-adversarial%20and%0Aadversarial%20settings.%20The%20results%20show%20that%20EUAT%20outperforms%20existing%0Aapproaches%20for%20uncertainty%20estimation%20%28including%20other%20uncertainty-aware%0Atraining%20techniques%2C%20calibration%2C%20ensembles%2C%20and%20DEUP%29%20by%20providing%20uncertainty%0Aestimates%20that%20not%20only%20have%20higher%20quality%20when%20evaluated%20via%20statistical%0Ametrics%20%28e.g.%2C%20correlation%20with%20residuals%29%20but%20also%20when%20employed%20to%20build%0Abinary%20classifiers%20that%20decide%20whether%20the%20model%27s%20output%20can%20be%20trusted%20or%20not%0Aand%20under%20distributional%20data%20shifts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01205v1&entry.124074799=Read"},
{"title": "Lazy Layers to Make Fine-Tuned Diffusion Models More Traceable", "author": "Haozhe Liu and Wentian Zhang and Bing Li and Bernard Ghanem and J\u00fcrgen Schmidhuber", "abstract": "  Foundational generative models should be traceable to protect their owners\nand facilitate safety regulation. To achieve this, traditional approaches embed\nidentifiers based on supervisory trigger-response signals, which are commonly\nknown as backdoor watermarks. They are prone to failure when the model is\nfine-tuned with nontrigger data. Our experiments show that this vulnerability\nis due to energetic changes in only a few 'busy' layers during fine-tuning.\nThis yields a novel arbitrary-in-arbitrary-out (AIAO) strategy that makes\nwatermarks resilient to fine-tuning-based removal. The trigger-response pairs\nof AIAO samples across various neural network depths can be used to construct\nwatermarked subpaths, employing Monte Carlo sampling to achieve stable\nverification results. In addition, unlike the existing methods of designing a\nbackdoor for the input/output space of diffusion models, in our method, we\npropose to embed the backdoor into the feature space of sampled subpaths, where\na mask-controlled trigger function is proposed to preserve the generation\nperformance and ensure the invisibility of the embedded backdoor. Our empirical\nstudies on the MS-COCO, AFHQ, LSUN, CUB-200, and DreamBooth datasets confirm\nthe robustness of AIAO; while the verification rates of other trigger-based\nmethods fall from ~90% to ~70% after fine-tuning, those of our method remain\nconsistently above 90%.\n", "link": "http://arxiv.org/abs/2405.00466v1", "date": "2024-05-01", "relevancy": 2.2659, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5852}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.565}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5483}], "mailto": "mailto:daeR=997470421.yrtne&1v66400.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.52%0902%evoba02%yltnetsisnocA0%niamer02%dohtem02%ruo02%fo02%esoht02%C2%gninut-enif02%retfa02%52%07~02%ot02%52%09~02%morf02%llaf02%sdohtemA0%desab-reggirt02%rehto02%fo02%setar02%noitacifirev02%eht02%elihw02%B3%OAIA02%fo02%ssentsubor02%ehtA0%mrifnoc02%stesatad02%htooBmaerD02%dna02%C2%002-BUC02%C2%NUSL02%C2%QHFA02%C2%OCOC-SM02%eht02%no02%seidutsA0%laciripme02%ruO02%.roodkcab02%deddebme02%eht02%fo02%ytilibisivni02%eht02%erusne02%dna02%ecnamrofrepA0%noitareneg02%eht02%evreserp02%ot02%desoporp02%si02%noitcnuf02%reggirt02%dellortnoc-ksam02%aA0%erehw02%C2%shtapbus02%delpmas02%fo02%ecaps02%erutaef02%eht02%otni02%roodkcab02%eht02%debme02%ot02%esoporpA0%ew02%C2%dohtem02%ruo02%ni02%C2%sledom02%noisuffid02%fo02%ecaps02%tuptuo/tupni02%eht02%rof02%roodkcabA0%a02%gningised02%fo02%sdohtem02%gnitsixe02%eht02%ekilnu02%C2%noitidda02%nI02%.stluser02%noitacifirevA0%elbats02%eveihca02%ot02%gnilpmas02%olraC02%etnoM02%gniyolpme02%C2%shtapbus02%dekramretawA0%tcurtsnoc02%ot02%desu02%eb02%nac02%shtped02%krowten02%laruen02%suoirav02%ssorca02%selpmas02%OAIA02%foA0%sriap02%esnopser-reggirt02%ehT02%.lavomer02%desab-gninut-enif02%ot02%tneiliser02%skramretawA0%sekam02%taht02%ygetarts02%92%OAIA82%02%tuo-yrartibra-ni-yrartibra02%levon02%a02%sdleiy02%sihTA0%.gninut-enif02%gnirud02%sreyal02%72%ysub72%02%wef02%a02%ylno02%ni02%segnahc02%citegrene02%ot02%eud02%siA0%ytilibarenluv02%siht02%taht02%wohs02%stnemirepxe02%ruO02%.atad02%reggirtnon02%htiw02%denut-enifA0%si02%ledom02%eht02%nehw02%eruliaf02%ot02%enorp02%era02%yehT02%.skramretaw02%roodkcab02%sa02%nwonkA0%ylnommoc02%era02%hcihw02%C2%slangis02%esnopser-reggirt02%yrosivrepus02%no02%desab02%sreifitnediA0%debme02%sehcaorppa02%lanoitidart02%C2%siht02%eveihca02%oT02%.noitaluger02%ytefas02%etatilicaf02%dnaA0%srenwo02%rieht02%tcetorp02%ot02%elbaecart02%eb02%dluohs02%sledom02%evitareneg02%lanoitadnuoF02%02%=3328342921.yrtne&rebuhdimhcS02%negrCB%3C%J02%dna02%menahG02%dranreB02%dna02%iL02%gniB02%dna02%gnahZ02%naitneW02%dna02%uiL02%ehzoaH=526535609.yrtne&elbaecarT02%eroM02%sledoM02%noisuffiD02%denuT-eniF02%ekaM02%ot02%sreyaL02%yzaL=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Lazy%20Layers%20to%20Make%20Fine-Tuned%20Diffusion%20Models%20More%20Traceable&body=Title%3A%20Lazy%20Layers%20to%20Make%20Fine-Tuned%20Diffusion%20Models%20More%20Traceable%0AAuthor%3A%20Haozhe%20Liu%20and%20Wentian%20Zhang%20and%20Bing%20Li%20and%20Bernard%20Ghanem%20and%20J%C3%BCrgen%20Schmidhuber%0AAbstract%3A%20%20%20Foundational%20generative%20models%20should%20be%20traceable%20to%20protect%20their%20owners%0Aand%20facilitate%20safety%20regulation.%20To%20achieve%20this%2C%20traditional%20approaches%20embed%0Aidentifiers%20based%20on%20supervisory%20trigger-response%20signals%2C%20which%20are%20commonly%0Aknown%20as%20backdoor%20watermarks.%20They%20are%20prone%20to%20failure%20when%20the%20model%20is%0Afine-tuned%20with%20nontrigger%20data.%20Our%20experiments%20show%20that%20this%20vulnerability%0Ais%20due%20to%20energetic%20changes%20in%20only%20a%20few%20%27busy%27%20layers%20during%20fine-tuning.%0AThis%20yields%20a%20novel%20arbitrary-in-arbitrary-out%20%28AIAO%29%20strategy%20that%20makes%0Awatermarks%20resilient%20to%20fine-tuning-based%20removal.%20The%20trigger-response%20pairs%0Aof%20AIAO%20samples%20across%20various%20neural%20network%20depths%20can%20be%20used%20to%20construct%0Awatermarked%20subpaths%2C%20employing%20Monte%20Carlo%20sampling%20to%20achieve%20stable%0Averification%20results.%20In%20addition%2C%20unlike%20the%20existing%20methods%20of%20designing%20a%0Abackdoor%20for%20the%20input/output%20space%20of%20diffusion%20models%2C%20in%20our%20method%2C%20we%0Apropose%20to%20embed%20the%20backdoor%20into%20the%20feature%20space%20of%20sampled%20subpaths%2C%20where%0Aa%20mask-controlled%20trigger%20function%20is%20proposed%20to%20preserve%20the%20generation%0Aperformance%20and%20ensure%20the%20invisibility%20of%20the%20embedded%20backdoor.%20Our%20empirical%0Astudies%20on%20the%20MS-COCO%2C%20AFHQ%2C%20LSUN%2C%20CUB-200%2C%20and%20DreamBooth%20datasets%20confirm%0Athe%20robustness%20of%20AIAO%3B%20while%20the%20verification%20rates%20of%20other%20trigger-based%0Amethods%20fall%20from%20~90%25%20to%20~70%25%20after%20fine-tuning%2C%20those%20of%20our%20method%20remain%0Aconsistently%20above%2090%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00466v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lazy%20Layers%20to%20Make%20Fine-Tuned%20Diffusion%20Models%20More%20Traceable&entry.906535625=Haozhe%20Liu%20and%20Wentian%20Zhang%20and%20Bing%20Li%20and%20Bernard%20Ghanem%20and%20J%C3%BCrgen%20Schmidhuber&entry.1292438233=%20%20Foundational%20generative%20models%20should%20be%20traceable%20to%20protect%20their%20owners%0Aand%20facilitate%20safety%20regulation.%20To%20achieve%20this%2C%20traditional%20approaches%20embed%0Aidentifiers%20based%20on%20supervisory%20trigger-response%20signals%2C%20which%20are%20commonly%0Aknown%20as%20backdoor%20watermarks.%20They%20are%20prone%20to%20failure%20when%20the%20model%20is%0Afine-tuned%20with%20nontrigger%20data.%20Our%20experiments%20show%20that%20this%20vulnerability%0Ais%20due%20to%20energetic%20changes%20in%20only%20a%20few%20%27busy%27%20layers%20during%20fine-tuning.%0AThis%20yields%20a%20novel%20arbitrary-in-arbitrary-out%20%28AIAO%29%20strategy%20that%20makes%0Awatermarks%20resilient%20to%20fine-tuning-based%20removal.%20The%20trigger-response%20pairs%0Aof%20AIAO%20samples%20across%20various%20neural%20network%20depths%20can%20be%20used%20to%20construct%0Awatermarked%20subpaths%2C%20employing%20Monte%20Carlo%20sampling%20to%20achieve%20stable%0Averification%20results.%20In%20addition%2C%20unlike%20the%20existing%20methods%20of%20designing%20a%0Abackdoor%20for%20the%20input/output%20space%20of%20diffusion%20models%2C%20in%20our%20method%2C%20we%0Apropose%20to%20embed%20the%20backdoor%20into%20the%20feature%20space%20of%20sampled%20subpaths%2C%20where%0Aa%20mask-controlled%20trigger%20function%20is%20proposed%20to%20preserve%20the%20generation%0Aperformance%20and%20ensure%20the%20invisibility%20of%20the%20embedded%20backdoor.%20Our%20empirical%0Astudies%20on%20the%20MS-COCO%2C%20AFHQ%2C%20LSUN%2C%20CUB-200%2C%20and%20DreamBooth%20datasets%20confirm%0Athe%20robustness%20of%20AIAO%3B%20while%20the%20verification%20rates%20of%20other%20trigger-based%0Amethods%20fall%20from%20~90%25%20to%20~70%25%20after%20fine-tuning%2C%20those%20of%20our%20method%20remain%0Aconsistently%20above%2090%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00466v1&entry.124074799=Read"},
{"title": "Continual Imitation Learning for Prosthetic Limbs", "author": "Sharmita Dey and Benjamin Paassen and Sarath Ravindran Nair and Sabri Boughorbel and Arndt F. Schilling", "abstract": "  Lower limb amputations and neuromuscular impairments severely restrict\nmobility, necessitating advancements beyond conventional prosthetics. Motorized\nbionic limbs offer promise, but their utility depends on mimicking the evolving\nsynergy of human movement in various settings. In this context, we present a\nnovel model for bionic prostheses' application that leverages camera-based\nmotion capture and wearable sensor data, to learn the synergistic coupling of\nthe lower limbs during human locomotion, empowering it to infer the kinematic\nbehavior of a missing lower limb across varied tasks, such as climbing inclines\nand stairs. We propose a model that can multitask, adapt continually,\nanticipate movements, and refine. The core of our method lies in an approach\nwhich we call -- multitask prospective rehearsal -- that anticipates and\nsynthesizes future movements based on the previous prediction and employs a\ncorrective mechanism for subsequent predictions. We design an evolving\narchitecture that merges lightweight, task-specific modules on a shared\nbackbone, ensuring both specificity and scalability. We empirically validate\nour model against various baselines using real-world human gait datasets,\nincluding experiments with transtibial amputees, which encompass a broad\nspectrum of locomotion tasks. The results show that our approach consistently\noutperforms baseline models, particularly under scenarios affected by\ndistributional shifts, adversarial perturbations, and noise.\n", "link": "http://arxiv.org/abs/2405.01114v1", "date": "2024-05-02", "relevancy": 2.2536, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6317}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5706}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5289}], "mailto": "mailto:daeR=997470421.yrtne&1v41110.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.esion02%dna02%C2%snoitabrutrep02%lairasrevda02%C2%stfihs02%lanoitubirtsidA0%yb02%detceffa02%soiranecs02%rednu02%ylralucitrap02%C2%sledom02%enilesab02%smrofreptuoA0%yltnetsisnoc02%hcaorppa02%ruo02%taht02%wohs02%stluser02%ehT02%.sksat02%noitomocol02%fo02%murtcepsA0%daorb02%a02%ssapmocne02%hcihw02%C2%seetupma02%laibitsnart02%htiw02%stnemirepxe02%gnidulcniA0%C2%stesatad02%tiag02%namuh02%dlrow-laer02%gnisu02%senilesab02%suoirav02%tsniaga02%ledom02%ruoA0%etadilav02%yllaciripme02%eW02%.ytilibalacs02%dna02%yticificeps02%htob02%gnirusne02%C2%enobkcabA0%derahs02%a02%no02%seludom02%cificeps-ksat02%C2%thgiewthgil02%segrem02%taht02%erutcetihcraA0%gnivlove02%na02%ngised02%eW02%.snoitciderp02%tneuqesbus02%rof02%msinahcem02%evitcerrocA0%a02%syolpme02%dna02%noitciderp02%suoiverp02%eht02%no02%desab02%stnemevom02%erutuf02%sezisehtnysA0%dna02%setapicitna02%taht02%--02%lasraeher02%evitcepsorp02%ksatitlum02%--02%llac02%ew02%hcihwA0%hcaorppa02%na02%ni02%seil02%dohtem02%ruo02%fo02%eroc02%ehT02%.enifer02%dna02%C2%stnemevom02%etapicitnaA0%C2%yllaunitnoc02%tpada02%C2%ksatitlum02%nac02%taht02%ledom02%a02%esoporp02%eW02%.sriats02%dnaA0%senilcni02%gnibmilc02%sa02%hcus02%C2%sksat02%deirav02%ssorca02%bmil02%rewol02%gnissim02%a02%fo02%roivahebA0%citamenik02%eht02%refni02%ot02%ti02%gnirewopme02%C2%noitomocol02%namuh02%gnirud02%sbmil02%rewol02%ehtA0%fo02%gnilpuoc02%citsigrenys02%eht02%nrael02%ot02%C2%atad02%rosnes02%elbaraew02%dna02%erutpac02%noitomA0%desab-aremac02%segarevel02%taht02%noitacilppa02%72%sesehtsorp02%cinoib02%rof02%ledom02%levonA0%a02%tneserp02%ew02%C2%txetnoc02%siht02%nI02%.sgnittes02%suoirav02%ni02%tnemevom02%namuh02%fo02%ygrenysA0%gnivlove02%eht02%gnikcimim02%no02%sdneped02%ytilitu02%rieht02%tub02%C2%esimorp02%reffo02%sbmil02%cinoibA0%dezirotoM02%.scitehtsorp02%lanoitnevnoc02%dnoyeb02%stnemecnavda02%gnitatissecen02%C2%ytilibomA0%tcirtser02%ylereves02%stnemriapmi02%ralucsumoruen02%dna02%snoitatupma02%bmil02%rewoL02%02%=3328342921.yrtne&gnillihcS02%.F02%tdnrA02%dna02%lebrohguoB02%irbaS02%dna02%riaN02%nardnivaR02%htaraS02%dna02%nessaaP02%nimajneB02%dna02%yeD02%atimrahS=526535609.yrtne&sbmiL02%citehtsorP02%rof02%gninraeL02%noitatimI02%launitnoC=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Continual%20Imitation%20Learning%20for%20Prosthetic%20Limbs&body=Title%3A%20Continual%20Imitation%20Learning%20for%20Prosthetic%20Limbs%0AAuthor%3A%20Sharmita%20Dey%20and%20Benjamin%20Paassen%20and%20Sarath%20Ravindran%20Nair%20and%20Sabri%20Boughorbel%20and%20Arndt%20F.%20Schilling%0AAbstract%3A%20%20%20Lower%20limb%20amputations%20and%20neuromuscular%20impairments%20severely%20restrict%0Amobility%2C%20necessitating%20advancements%20beyond%20conventional%20prosthetics.%20Motorized%0Abionic%20limbs%20offer%20promise%2C%20but%20their%20utility%20depends%20on%20mimicking%20the%20evolving%0Asynergy%20of%20human%20movement%20in%20various%20settings.%20In%20this%20context%2C%20we%20present%20a%0Anovel%20model%20for%20bionic%20prostheses%27%20application%20that%20leverages%20camera-based%0Amotion%20capture%20and%20wearable%20sensor%20data%2C%20to%20learn%20the%20synergistic%20coupling%20of%0Athe%20lower%20limbs%20during%20human%20locomotion%2C%20empowering%20it%20to%20infer%20the%20kinematic%0Abehavior%20of%20a%20missing%20lower%20limb%20across%20varied%20tasks%2C%20such%20as%20climbing%20inclines%0Aand%20stairs.%20We%20propose%20a%20model%20that%20can%20multitask%2C%20adapt%20continually%2C%0Aanticipate%20movements%2C%20and%20refine.%20The%20core%20of%20our%20method%20lies%20in%20an%20approach%0Awhich%20we%20call%20--%20multitask%20prospective%20rehearsal%20--%20that%20anticipates%20and%0Asynthesizes%20future%20movements%20based%20on%20the%20previous%20prediction%20and%20employs%20a%0Acorrective%20mechanism%20for%20subsequent%20predictions.%20We%20design%20an%20evolving%0Aarchitecture%20that%20merges%20lightweight%2C%20task-specific%20modules%20on%20a%20shared%0Abackbone%2C%20ensuring%20both%20specificity%20and%20scalability.%20We%20empirically%20validate%0Aour%20model%20against%20various%20baselines%20using%20real-world%20human%20gait%20datasets%2C%0Aincluding%20experiments%20with%20transtibial%20amputees%2C%20which%20encompass%20a%20broad%0Aspectrum%20of%20locomotion%20tasks.%20The%20results%20show%20that%20our%20approach%20consistently%0Aoutperforms%20baseline%20models%2C%20particularly%20under%20scenarios%20affected%20by%0Adistributional%20shifts%2C%20adversarial%20perturbations%2C%20and%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01114v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Continual%20Imitation%20Learning%20for%20Prosthetic%20Limbs&entry.906535625=Sharmita%20Dey%20and%20Benjamin%20Paassen%20and%20Sarath%20Ravindran%20Nair%20and%20Sabri%20Boughorbel%20and%20Arndt%20F.%20Schilling&entry.1292438233=%20%20Lower%20limb%20amputations%20and%20neuromuscular%20impairments%20severely%20restrict%0Amobility%2C%20necessitating%20advancements%20beyond%20conventional%20prosthetics.%20Motorized%0Abionic%20limbs%20offer%20promise%2C%20but%20their%20utility%20depends%20on%20mimicking%20the%20evolving%0Asynergy%20of%20human%20movement%20in%20various%20settings.%20In%20this%20context%2C%20we%20present%20a%0Anovel%20model%20for%20bionic%20prostheses%27%20application%20that%20leverages%20camera-based%0Amotion%20capture%20and%20wearable%20sensor%20data%2C%20to%20learn%20the%20synergistic%20coupling%20of%0Athe%20lower%20limbs%20during%20human%20locomotion%2C%20empowering%20it%20to%20infer%20the%20kinematic%0Abehavior%20of%20a%20missing%20lower%20limb%20across%20varied%20tasks%2C%20such%20as%20climbing%20inclines%0Aand%20stairs.%20We%20propose%20a%20model%20that%20can%20multitask%2C%20adapt%20continually%2C%0Aanticipate%20movements%2C%20and%20refine.%20The%20core%20of%20our%20method%20lies%20in%20an%20approach%0Awhich%20we%20call%20--%20multitask%20prospective%20rehearsal%20--%20that%20anticipates%20and%0Asynthesizes%20future%20movements%20based%20on%20the%20previous%20prediction%20and%20employs%20a%0Acorrective%20mechanism%20for%20subsequent%20predictions.%20We%20design%20an%20evolving%0Aarchitecture%20that%20merges%20lightweight%2C%20task-specific%20modules%20on%20a%20shared%0Abackbone%2C%20ensuring%20both%20specificity%20and%20scalability.%20We%20empirically%20validate%0Aour%20model%20against%20various%20baselines%20using%20real-world%20human%20gait%20datasets%2C%0Aincluding%20experiments%20with%20transtibial%20amputees%2C%20which%20encompass%20a%20broad%0Aspectrum%20of%20locomotion%20tasks.%20The%20results%20show%20that%20our%20approach%20consistently%0Aoutperforms%20baseline%20models%2C%20particularly%20under%20scenarios%20affected%20by%0Adistributional%20shifts%2C%20adversarial%20perturbations%2C%20and%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01114v1&entry.124074799=Read"},
{"title": "Domain-Transferred Synthetic Data Generation for Improving Monocular\n  Depth Estimation", "author": "Seungyeop Lee and Knut Peterson and Solmaz Arezoomandan and Bill Cai and Peihan Li and Lifeng Zhou and David Han", "abstract": "  A major obstacle to the development of effective monocular depth estimation\nalgorithms is the difficulty in obtaining high-quality depth data that\ncorresponds to collected RGB images. Collecting this data is time-consuming and\ncostly, and even data collected by modern sensors has limited range or\nresolution, and is subject to inconsistencies and noise. To combat this, we\npropose a method of data generation in simulation using 3D synthetic\nenvironments and CycleGAN domain transfer. We compare this method of data\ngeneration to the popular NYUDepth V2 dataset by training a depth estimation\nmodel based on the DenseDepth structure using different training sets of real\nand simulated data. We evaluate the performance of the models on newly\ncollected images and LiDAR depth data from a Husky robot to verify the\ngeneralizability of the approach and show that GAN-transformed data can serve\nas an effective alternative to real-world data, particularly in depth\nestimation.\n", "link": "http://arxiv.org/abs/2405.01113v1", "date": "2024-05-02", "relevancy": 2.2474, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5667}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5634}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5584}], "mailto": "mailto:daeR=997470421.yrtne&1v31110.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.noitamitseA0%htped02%ni02%ylralucitrap02%C2%atad02%dlrow-laer02%ot02%evitanretla02%evitceffe02%na02%saA0%evres02%nac02%atad02%demrofsnart-NAG02%taht02%wohs02%dna02%hcaorppa02%eht02%fo02%ytilibazilarenegA0%eht02%yfirev02%ot02%tobor02%yksuH02%a02%morf02%atad02%htped02%RADiL02%dna02%segami02%detcellocA0%ylwen02%no02%sledom02%eht02%fo02%ecnamrofrep02%eht02%etaulave02%eW02%.atad02%detalumis02%dnaA0%laer02%fo02%stes02%gniniart02%tnereffid02%gnisu02%erutcurts02%htpeDesneD02%eht02%no02%desab02%ledomA0%noitamitse02%htped02%a02%gniniart02%yb02%tesatad02%2V02%htpeDUYN02%ralupop02%eht02%ot02%noitarenegA0%atad02%fo02%dohtem02%siht02%erapmoc02%eW02%.refsnart02%niamod02%NAGelcyC02%dna02%stnemnorivneA0%citehtnys02%D302%gnisu02%noitalumis02%ni02%noitareneg02%atad02%fo02%dohtem02%a02%esoporpA0%ew02%C2%siht02%tabmoc02%oT02%.esion02%dna02%seicnetsisnocni02%ot02%tcejbus02%si02%dna02%C2%noituloserA0%ro02%egnar02%detimil02%sah02%srosnes02%nredom02%yb02%detcelloc02%atad02%neve02%dna02%C2%yltsocA0%dna02%gnimusnoc-emit02%si02%atad02%siht02%gnitcelloC02%.segami02%BGR02%detcelloc02%ot02%sdnopserrocA0%taht02%atad02%htped02%ytilauq-hgih02%gniniatbo02%ni02%ytluciffid02%eht02%si02%smhtiroglaA0%noitamitse02%htped02%raluconom02%evitceffe02%fo02%tnempoleved02%eht02%ot02%elcatsbo02%rojam02%A02%02%=3328342921.yrtne&naH02%divaD02%dna02%uohZ02%gnefiL02%dna02%iL02%nahieP02%dna02%iaC02%lliB02%dna02%nadnamoozerA02%zamloS02%dna02%nosreteP02%tunK02%dna02%eeL02%poeygnueS=526535609.yrtne&noitamitsE02%htpeD02%02%A0%raluconoM02%gnivorpmI02%rof02%noitareneG02%ataD02%citehtnyS02%derrefsnarT-niamoD=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Domain-Transferred%20Synthetic%20Data%20Generation%20for%20Improving%20Monocular%0A%20%20Depth%20Estimation&body=Title%3A%20Domain-Transferred%20Synthetic%20Data%20Generation%20for%20Improving%20Monocular%0A%20%20Depth%20Estimation%0AAuthor%3A%20Seungyeop%20Lee%20and%20Knut%20Peterson%20and%20Solmaz%20Arezoomandan%20and%20Bill%20Cai%20and%20Peihan%20Li%20and%20Lifeng%20Zhou%20and%20David%20Han%0AAbstract%3A%20%20%20A%20major%20obstacle%20to%20the%20development%20of%20effective%20monocular%20depth%20estimation%0Aalgorithms%20is%20the%20difficulty%20in%20obtaining%20high-quality%20depth%20data%20that%0Acorresponds%20to%20collected%20RGB%20images.%20Collecting%20this%20data%20is%20time-consuming%20and%0Acostly%2C%20and%20even%20data%20collected%20by%20modern%20sensors%20has%20limited%20range%20or%0Aresolution%2C%20and%20is%20subject%20to%20inconsistencies%20and%20noise.%20To%20combat%20this%2C%20we%0Apropose%20a%20method%20of%20data%20generation%20in%20simulation%20using%203D%20synthetic%0Aenvironments%20and%20CycleGAN%20domain%20transfer.%20We%20compare%20this%20method%20of%20data%0Ageneration%20to%20the%20popular%20NYUDepth%20V2%20dataset%20by%20training%20a%20depth%20estimation%0Amodel%20based%20on%20the%20DenseDepth%20structure%20using%20different%20training%20sets%20of%20real%0Aand%20simulated%20data.%20We%20evaluate%20the%20performance%20of%20the%20models%20on%20newly%0Acollected%20images%20and%20LiDAR%20depth%20data%20from%20a%20Husky%20robot%20to%20verify%20the%0Ageneralizability%20of%20the%20approach%20and%20show%20that%20GAN-transformed%20data%20can%20serve%0Aas%20an%20effective%20alternative%20to%20real-world%20data%2C%20particularly%20in%20depth%0Aestimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01113v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain-Transferred%20Synthetic%20Data%20Generation%20for%20Improving%20Monocular%0A%20%20Depth%20Estimation&entry.906535625=Seungyeop%20Lee%20and%20Knut%20Peterson%20and%20Solmaz%20Arezoomandan%20and%20Bill%20Cai%20and%20Peihan%20Li%20and%20Lifeng%20Zhou%20and%20David%20Han&entry.1292438233=%20%20A%20major%20obstacle%20to%20the%20development%20of%20effective%20monocular%20depth%20estimation%0Aalgorithms%20is%20the%20difficulty%20in%20obtaining%20high-quality%20depth%20data%20that%0Acorresponds%20to%20collected%20RGB%20images.%20Collecting%20this%20data%20is%20time-consuming%20and%0Acostly%2C%20and%20even%20data%20collected%20by%20modern%20sensors%20has%20limited%20range%20or%0Aresolution%2C%20and%20is%20subject%20to%20inconsistencies%20and%20noise.%20To%20combat%20this%2C%20we%0Apropose%20a%20method%20of%20data%20generation%20in%20simulation%20using%203D%20synthetic%0Aenvironments%20and%20CycleGAN%20domain%20transfer.%20We%20compare%20this%20method%20of%20data%0Ageneration%20to%20the%20popular%20NYUDepth%20V2%20dataset%20by%20training%20a%20depth%20estimation%0Amodel%20based%20on%20the%20DenseDepth%20structure%20using%20different%20training%20sets%20of%20real%0Aand%20simulated%20data.%20We%20evaluate%20the%20performance%20of%20the%20models%20on%20newly%0Acollected%20images%20and%20LiDAR%20depth%20data%20from%20a%20Husky%20robot%20to%20verify%20the%0Ageneralizability%20of%20the%20approach%20and%20show%20that%20GAN-transformed%20data%20can%20serve%0Aas%20an%20effective%20alternative%20to%20real-world%20data%2C%20particularly%20in%20depth%0Aestimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01113v1&entry.124074799=Read"},
{"title": "Enhancing Person Re-Identification via Uncertainty Feature Fusion and\n  Wise Distance Aggregation", "author": "Quang-Huy Che and Le-Chuong Nguyen and Vinh-Tiep Nguyen", "abstract": "  The quest for robust Person re-identification (Re-ID) systems capable of\naccurately identifying subjects across diverse scenarios remains a formidable\nchallenge in surveillance and security applications. This study presents a\nnovel methodology that significantly enhances Person Re-Identification (Re-ID)\nby integrating Uncertainty Feature Fusion (UFFM) with Wise Distance Aggregation\n(WDA). Tested on benchmark datasets - Market-1501, DukeMTMC-ReID, and MSMT17 -\nour approach demonstrates substantial improvements in Rank-1 accuracy and mean\nAverage Precision (mAP). Specifically, UFFM capitalizes on the power of feature\nsynthesis from multiple images to overcome the limitations imposed by the\nvariability of subject appearances across different views. WDA further refines\nthe process by intelligently aggregating similarity metrics, thereby enhancing\nthe system's ability to discern subtle but critical differences between\nsubjects. The empirical results affirm the superiority of our method over\nexisting approaches, achieving new performance benchmarks across all evaluated\ndatasets. Code is available on Github.\n", "link": "http://arxiv.org/abs/2405.01101v1", "date": "2024-05-02", "relevancy": 2.2466, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5724}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5714}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5471}], "mailto": "mailto:daeR=997470421.yrtne&1v10110.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.buhtiG02%no02%elbaliava02%si02%edoC02%.stesatadA0%detaulave02%lla02%ssorca02%skramhcneb02%ecnamrofrep02%wen02%gniveihca02%C2%sehcaorppa02%gnitsixeA0%revo02%dohtem02%ruo02%fo02%ytiroirepus02%eht02%mriffa02%stluser02%laciripme02%ehT02%.stcejbusA0%neewteb02%secnereffid02%lacitirc02%tub02%eltbus02%nrecsid02%ot02%ytiliba02%s72%metsys02%ehtA0%gnicnahne02%ybereht02%C2%scirtem02%ytiralimis02%gnitagergga02%yltnegilletni02%yb02%ssecorp02%ehtA0%senifer02%rehtruf02%ADW02%.sweiv02%tnereffid02%ssorca02%secnaraeppa02%tcejbus02%fo02%ytilibairavA0%eht02%yb02%desopmi02%snoitatimil02%eht02%emocrevo02%ot02%segami02%elpitlum02%morf02%sisehtnysA0%erutaef02%fo02%rewop02%eht02%no02%sezilatipac02%MFFU02%C2%yllacificepS02%.92%PAm82%02%noisicerP02%egarevAA0%naem02%dna02%ycarucca02%1-knaR02%ni02%stnemevorpmi02%laitnatsbus02%setartsnomed02%hcaorppa02%ruoA0%-02%71TMSM02%dna02%C2%DIeR-CMTMekuD02%C2%1051-tekraM02%-02%stesatad02%kramhcneb02%no02%detseT02%.92%ADW82%A0%noitagerggA02%ecnatsiD02%esiW02%htiw02%92%MFFU82%02%noisuF02%erutaeF02%ytniatrecnU02%gnitargetni02%ybA0%92%DI-eR82%02%noitacifitnedI-eR02%nosreP02%secnahne02%yltnacifingis02%taht02%ygolodohtem02%levonA0%a02%stneserp02%yduts02%sihT02%.snoitacilppa02%ytiruces02%dna02%ecnallievrus02%ni02%egnellahcA0%elbadimrof02%a02%sniamer02%soiranecs02%esrevid02%ssorca02%stcejbus02%gniyfitnedi02%yletaruccaA0%fo02%elbapac02%smetsys02%92%DI-eR82%02%noitacifitnedi-er02%nosreP02%tsubor02%rof02%tseuq02%ehT02%02%=3328342921.yrtne&neyugN02%peiT-hniV02%dna02%neyugN02%gnouhC-eL02%dna02%ehC02%yuH-gnauQ=526535609.yrtne&noitagerggA02%ecnatsiD02%esiW02%02%A0%dna02%noisuF02%erutaeF02%ytniatrecnU02%aiv02%noitacifitnedI-eR02%nosreP02%gnicnahnE=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Person%20Re-Identification%20via%20Uncertainty%20Feature%20Fusion%20and%0A%20%20Wise%20Distance%20Aggregation&body=Title%3A%20Enhancing%20Person%20Re-Identification%20via%20Uncertainty%20Feature%20Fusion%20and%0A%20%20Wise%20Distance%20Aggregation%0AAuthor%3A%20Quang-Huy%20Che%20and%20Le-Chuong%20Nguyen%20and%20Vinh-Tiep%20Nguyen%0AAbstract%3A%20%20%20The%20quest%20for%20robust%20Person%20re-identification%20%28Re-ID%29%20systems%20capable%20of%0Aaccurately%20identifying%20subjects%20across%20diverse%20scenarios%20remains%20a%20formidable%0Achallenge%20in%20surveillance%20and%20security%20applications.%20This%20study%20presents%20a%0Anovel%20methodology%20that%20significantly%20enhances%20Person%20Re-Identification%20%28Re-ID%29%0Aby%20integrating%20Uncertainty%20Feature%20Fusion%20%28UFFM%29%20with%20Wise%20Distance%20Aggregation%0A%28WDA%29.%20Tested%20on%20benchmark%20datasets%20-%20Market-1501%2C%20DukeMTMC-ReID%2C%20and%20MSMT17%20-%0Aour%20approach%20demonstrates%20substantial%20improvements%20in%20Rank-1%20accuracy%20and%20mean%0AAverage%20Precision%20%28mAP%29.%20Specifically%2C%20UFFM%20capitalizes%20on%20the%20power%20of%20feature%0Asynthesis%20from%20multiple%20images%20to%20overcome%20the%20limitations%20imposed%20by%20the%0Avariability%20of%20subject%20appearances%20across%20different%20views.%20WDA%20further%20refines%0Athe%20process%20by%20intelligently%20aggregating%20similarity%20metrics%2C%20thereby%20enhancing%0Athe%20system%27s%20ability%20to%20discern%20subtle%20but%20critical%20differences%20between%0Asubjects.%20The%20empirical%20results%20affirm%20the%20superiority%20of%20our%20method%20over%0Aexisting%20approaches%2C%20achieving%20new%20performance%20benchmarks%20across%20all%20evaluated%0Adatasets.%20Code%20is%20available%20on%20Github.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01101v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Person%20Re-Identification%20via%20Uncertainty%20Feature%20Fusion%20and%0A%20%20Wise%20Distance%20Aggregation&entry.906535625=Quang-Huy%20Che%20and%20Le-Chuong%20Nguyen%20and%20Vinh-Tiep%20Nguyen&entry.1292438233=%20%20The%20quest%20for%20robust%20Person%20re-identification%20%28Re-ID%29%20systems%20capable%20of%0Aaccurately%20identifying%20subjects%20across%20diverse%20scenarios%20remains%20a%20formidable%0Achallenge%20in%20surveillance%20and%20security%20applications.%20This%20study%20presents%20a%0Anovel%20methodology%20that%20significantly%20enhances%20Person%20Re-Identification%20%28Re-ID%29%0Aby%20integrating%20Uncertainty%20Feature%20Fusion%20%28UFFM%29%20with%20Wise%20Distance%20Aggregation%0A%28WDA%29.%20Tested%20on%20benchmark%20datasets%20-%20Market-1501%2C%20DukeMTMC-ReID%2C%20and%20MSMT17%20-%0Aour%20approach%20demonstrates%20substantial%20improvements%20in%20Rank-1%20accuracy%20and%20mean%0AAverage%20Precision%20%28mAP%29.%20Specifically%2C%20UFFM%20capitalizes%20on%20the%20power%20of%20feature%0Asynthesis%20from%20multiple%20images%20to%20overcome%20the%20limitations%20imposed%20by%20the%0Avariability%20of%20subject%20appearances%20across%20different%20views.%20WDA%20further%20refines%0Athe%20process%20by%20intelligently%20aggregating%20similarity%20metrics%2C%20thereby%20enhancing%0Athe%20system%27s%20ability%20to%20discern%20subtle%20but%20critical%20differences%20between%0Asubjects.%20The%20empirical%20results%20affirm%20the%20superiority%20of%20our%20method%20over%0Aexisting%20approaches%2C%20achieving%20new%20performance%20benchmarks%20across%20all%20evaluated%0Adatasets.%20Code%20is%20available%20on%20Github.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01101v1&entry.124074799=Read"},
{"title": "RST-LoRA: A Discourse-Aware Low-Rank Adaptation for Long Document\n  Abstractive Summarization", "author": "Dongqi Pu and Vera Demberg", "abstract": "  For long document summarization, discourse structure is important to discern\nthe key content of the text and the differences in importance level between\nsentences. Unfortunately, the integration of rhetorical structure theory (RST)\ninto parameter-efficient fine-tuning strategies for long document summarization\nremains unexplored. Therefore, this paper introduces RST-LoRA and proposes four\nRST-aware variants to explicitly incorporate RST into the LoRA model. Our\nempirical evaluation demonstrates that incorporating the type and uncertainty\nof rhetorical relations can complementarily enhance the performance of LoRA in\nsummarization tasks. Furthermore, the best-performing variant we introduced\noutperforms the vanilla LoRA and full-parameter fine-tuning models, as\nconfirmed by multiple automatic and human evaluations, and even surpasses\nprevious state-of-the-art methods.\n", "link": "http://arxiv.org/abs/2405.00657v1", "date": "2024-05-01", "relevancy": 2.2461, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4632}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4474}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.437}], "mailto": "mailto:daeR=997470421.yrtne&1v75600.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.sdohtem02%tra-eht-fo-etats02%suoiverpA0%sessaprus02%neve02%dna02%C2%snoitaulave02%namuh02%dna02%citamotua02%elpitlum02%yb02%demrifnocA0%sa02%C2%sledom02%gninut-enif02%retemarap-lluf02%dna02%ARoL02%allinav02%eht02%smrofreptuoA0%decudortni02%ew02%tnairav02%gnimrofrep-tseb02%eht02%C2%eromrehtruF02%.sksat02%noitazirammusA0%ni02%ARoL02%fo02%ecnamrofrep02%eht02%ecnahne02%yliratnemelpmoc02%nac02%snoitaler02%lacirotehr02%foA0%ytniatrecnu02%dna02%epyt02%eht02%gnitaroprocni02%taht02%setartsnomed02%noitaulave02%laciripmeA0%ruO02%.ledom02%ARoL02%eht02%otni02%TSR02%etaroprocni02%ylticilpxe02%ot02%stnairav02%erawa-TSRA0%ruof02%sesoporp02%dna02%ARoL-TSR02%secudortni02%repap02%siht02%C2%eroferehT02%.derolpxenu02%sniamerA0%noitazirammus02%tnemucod02%gnol02%rof02%seigetarts02%gninut-enif02%tneiciffe-retemarap02%otniA0%92%TSR82%02%yroeht02%erutcurts02%lacirotehr02%fo02%noitargetni02%eht02%C2%yletanutrofnU02%.secnetnesA0%neewteb02%level02%ecnatropmi02%ni02%secnereffid02%eht02%dna02%txet02%eht02%fo02%tnetnoc02%yek02%ehtA0%nrecsid02%ot02%tnatropmi02%si02%erutcurts02%esruocsid02%C2%noitazirammus02%tnemucod02%gnol02%roF02%02%=3328342921.yrtne&grebmeD02%areV02%dna02%uP02%iqgnoD=526535609.yrtne&noitazirammuS02%evitcartsbA02%02%A0%tnemucoD02%gnoL02%rof02%noitatpadA02%knaR-woL02%erawA-esruocsiD02%A02%A3%ARoL-TSR=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20RST-LoRA%3A%20A%20Discourse-Aware%20Low-Rank%20Adaptation%20for%20Long%20Document%0A%20%20Abstractive%20Summarization&body=Title%3A%20RST-LoRA%3A%20A%20Discourse-Aware%20Low-Rank%20Adaptation%20for%20Long%20Document%0A%20%20Abstractive%20Summarization%0AAuthor%3A%20Dongqi%20Pu%20and%20Vera%20Demberg%0AAbstract%3A%20%20%20For%20long%20document%20summarization%2C%20discourse%20structure%20is%20important%20to%20discern%0Athe%20key%20content%20of%20the%20text%20and%20the%20differences%20in%20importance%20level%20between%0Asentences.%20Unfortunately%2C%20the%20integration%20of%20rhetorical%20structure%20theory%20%28RST%29%0Ainto%20parameter-efficient%20fine-tuning%20strategies%20for%20long%20document%20summarization%0Aremains%20unexplored.%20Therefore%2C%20this%20paper%20introduces%20RST-LoRA%20and%20proposes%20four%0ARST-aware%20variants%20to%20explicitly%20incorporate%20RST%20into%20the%20LoRA%20model.%20Our%0Aempirical%20evaluation%20demonstrates%20that%20incorporating%20the%20type%20and%20uncertainty%0Aof%20rhetorical%20relations%20can%20complementarily%20enhance%20the%20performance%20of%20LoRA%20in%0Asummarization%20tasks.%20Furthermore%2C%20the%20best-performing%20variant%20we%20introduced%0Aoutperforms%20the%20vanilla%20LoRA%20and%20full-parameter%20fine-tuning%20models%2C%20as%0Aconfirmed%20by%20multiple%20automatic%20and%20human%20evaluations%2C%20and%20even%20surpasses%0Aprevious%20state-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00657v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RST-LoRA%3A%20A%20Discourse-Aware%20Low-Rank%20Adaptation%20for%20Long%20Document%0A%20%20Abstractive%20Summarization&entry.906535625=Dongqi%20Pu%20and%20Vera%20Demberg&entry.1292438233=%20%20For%20long%20document%20summarization%2C%20discourse%20structure%20is%20important%20to%20discern%0Athe%20key%20content%20of%20the%20text%20and%20the%20differences%20in%20importance%20level%20between%0Asentences.%20Unfortunately%2C%20the%20integration%20of%20rhetorical%20structure%20theory%20%28RST%29%0Ainto%20parameter-efficient%20fine-tuning%20strategies%20for%20long%20document%20summarization%0Aremains%20unexplored.%20Therefore%2C%20this%20paper%20introduces%20RST-LoRA%20and%20proposes%20four%0ARST-aware%20variants%20to%20explicitly%20incorporate%20RST%20into%20the%20LoRA%20model.%20Our%0Aempirical%20evaluation%20demonstrates%20that%20incorporating%20the%20type%20and%20uncertainty%0Aof%20rhetorical%20relations%20can%20complementarily%20enhance%20the%20performance%20of%20LoRA%20in%0Asummarization%20tasks.%20Furthermore%2C%20the%20best-performing%20variant%20we%20introduced%0Aoutperforms%20the%20vanilla%20LoRA%20and%20full-parameter%20fine-tuning%20models%2C%20as%0Aconfirmed%20by%20multiple%20automatic%20and%20human%20evaluations%2C%20and%20even%20surpasses%0Aprevious%20state-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00657v1&entry.124074799=Read"},
{"title": "Underwater Variable Zoom: Depth-Guided Perception Network for Underwater\n  Image Enhancement", "author": "Zhixiong Huang and Xinying Wang and Jinjiang Li and Shenglan Liu and Lin Feng", "abstract": "  Underwater scenes intrinsically involve degradation problems owing to\nheterogeneous ocean elements. Prevailing underwater image enhancement (UIE)\nmethods stick to straightforward feature modeling to learn the mapping\nfunction, which leads to limited vision gain as it lacks more explicit physical\ncues (e.g., depth). In this work, we investigate injecting the depth prior into\nthe deep UIE model for more precise scene enhancement capability. To this end,\nwe present a novel depth-guided perception UIE framework, dubbed underwater\nvariable zoom (UVZ). Specifically, UVZ resorts to a two-stage pipeline. First,\na depth estimation network is designed to generate critical depth maps,\ncombined with an auxiliary supervision network introduced to suppress\nestimation differences during training. Second, UVZ parses near-far scenarios\nby harnessing the predicted depth maps, enabling local and non-local perceiving\nin different regions. Extensive experiments on five benchmark datasets\ndemonstrate that UVZ achieves superior visual gain and delivers promising\nquantitative metrics. Besides, UVZ is confirmed to exhibit good generalization\nin some visual tasks, especially in unusual lighting conditions. The code,\nmodels and results are available at: https://github.com/WindySprint/UVZ.\n", "link": "http://arxiv.org/abs/2404.17883v2", "date": "2024-05-02", "relevancy": 2.2402, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5814}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5691}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5351}], "mailto": "mailto:daeR=997470421.yrtne&2v38871.4042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.ZVU/tnirpSydniW/moc.buhtig//A3%sptth02%A3%ta02%elbaliava02%era02%stluser02%dna02%sledomA0%C2%edoc02%ehT02%.snoitidnoc02%gnithgil02%lausunu02%ni02%yllaicepse02%C2%sksat02%lausiv02%emos02%niA0%noitazilareneg02%doog02%tibihxe02%ot02%demrifnoc02%si02%ZVU02%C2%sediseB02%.scirtem02%evitatitnauqA0%gnisimorp02%sreviled02%dna02%niag02%lausiv02%roirepus02%seveihca02%ZVU02%taht02%etartsnomedA0%stesatad02%kramhcneb02%evif02%no02%stnemirepxe02%evisnetxE02%.snoiger02%tnereffid02%niA0%gniviecrep02%lacol-non02%dna02%lacol02%gnilbane02%C2%spam02%htped02%detciderp02%eht02%gnissenrah02%ybA0%soiranecs02%raf-raen02%sesrap02%ZVU02%C2%dnoceS02%.gniniart02%gnirud02%secnereffid02%noitamitseA0%sserppus02%ot02%decudortni02%krowten02%noisivrepus02%yrailixua02%na02%htiw02%denibmocA0%C2%spam02%htped02%lacitirc02%etareneg02%ot02%dengised02%si02%krowten02%noitamitse02%htped02%aA0%C2%tsriF02%.enilepip02%egats-owt02%a02%ot02%stroser02%ZVU02%C2%yllacificepS02%.92%ZVU82%02%mooz02%elbairavA0%retawrednu02%debbud02%C2%krowemarf02%EIU02%noitpecrep02%dediug-htped02%levon02%a02%tneserp02%ewA0%C2%dne02%siht02%oT02%.ytilibapac02%tnemecnahne02%enecs02%esicerp02%erom02%rof02%ledom02%EIU02%peed02%ehtA0%otni02%roirp02%htped02%eht02%gnitcejni02%etagitsevni02%ew02%C2%krow02%siht02%nI02%.92%htped02%C2%.g.e82%02%seucA0%lacisyhp02%ticilpxe02%erom02%skcal02%ti02%sa02%niag02%noisiv02%detimil02%ot02%sdael02%hcihw02%C2%noitcnufA0%gnippam02%eht02%nrael02%ot02%gniledom02%erutaef02%drawrofthgiarts02%ot02%kcits02%sdohtemA0%92%EIU82%02%tnemecnahne02%egami02%retawrednu02%gniliaverP02%.stnemele02%naeco02%suoenegoretehA0%ot02%gniwo02%smelborp02%noitadarged02%evlovni02%yllacisnirtni02%senecs02%retawrednU02%02%=3328342921.yrtne&gneF02%niL02%dna02%uiL02%nalgnehS02%dna02%iL02%gnaijniJ02%dna02%gnaW02%gniyniX02%dna02%gnauH02%gnoixihZ=526535609.yrtne&tnemecnahnE02%egamI02%02%A0%retawrednU02%rof02%krowteN02%noitpecreP02%dediuG-htpeD02%A3%mooZ02%elbairaV02%retawrednU=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Underwater%20Variable%20Zoom%3A%20Depth-Guided%20Perception%20Network%20for%20Underwater%0A%20%20Image%20Enhancement&body=Title%3A%20Underwater%20Variable%20Zoom%3A%20Depth-Guided%20Perception%20Network%20for%20Underwater%0A%20%20Image%20Enhancement%0AAuthor%3A%20Zhixiong%20Huang%20and%20Xinying%20Wang%20and%20Jinjiang%20Li%20and%20Shenglan%20Liu%20and%20Lin%20Feng%0AAbstract%3A%20%20%20Underwater%20scenes%20intrinsically%20involve%20degradation%20problems%20owing%20to%0Aheterogeneous%20ocean%20elements.%20Prevailing%20underwater%20image%20enhancement%20%28UIE%29%0Amethods%20stick%20to%20straightforward%20feature%20modeling%20to%20learn%20the%20mapping%0Afunction%2C%20which%20leads%20to%20limited%20vision%20gain%20as%20it%20lacks%20more%20explicit%20physical%0Acues%20%28e.g.%2C%20depth%29.%20In%20this%20work%2C%20we%20investigate%20injecting%20the%20depth%20prior%20into%0Athe%20deep%20UIE%20model%20for%20more%20precise%20scene%20enhancement%20capability.%20To%20this%20end%2C%0Awe%20present%20a%20novel%20depth-guided%20perception%20UIE%20framework%2C%20dubbed%20underwater%0Avariable%20zoom%20%28UVZ%29.%20Specifically%2C%20UVZ%20resorts%20to%20a%20two-stage%20pipeline.%20First%2C%0Aa%20depth%20estimation%20network%20is%20designed%20to%20generate%20critical%20depth%20maps%2C%0Acombined%20with%20an%20auxiliary%20supervision%20network%20introduced%20to%20suppress%0Aestimation%20differences%20during%20training.%20Second%2C%20UVZ%20parses%20near-far%20scenarios%0Aby%20harnessing%20the%20predicted%20depth%20maps%2C%20enabling%20local%20and%20non-local%20perceiving%0Ain%20different%20regions.%20Extensive%20experiments%20on%20five%20benchmark%20datasets%0Ademonstrate%20that%20UVZ%20achieves%20superior%20visual%20gain%20and%20delivers%20promising%0Aquantitative%20metrics.%20Besides%2C%20UVZ%20is%20confirmed%20to%20exhibit%20good%20generalization%0Ain%20some%20visual%20tasks%2C%20especially%20in%20unusual%20lighting%20conditions.%20The%20code%2C%0Amodels%20and%20results%20are%20available%20at%3A%20https%3A//github.com/WindySprint/UVZ.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17883v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Underwater%20Variable%20Zoom%3A%20Depth-Guided%20Perception%20Network%20for%20Underwater%0A%20%20Image%20Enhancement&entry.906535625=Zhixiong%20Huang%20and%20Xinying%20Wang%20and%20Jinjiang%20Li%20and%20Shenglan%20Liu%20and%20Lin%20Feng&entry.1292438233=%20%20Underwater%20scenes%20intrinsically%20involve%20degradation%20problems%20owing%20to%0Aheterogeneous%20ocean%20elements.%20Prevailing%20underwater%20image%20enhancement%20%28UIE%29%0Amethods%20stick%20to%20straightforward%20feature%20modeling%20to%20learn%20the%20mapping%0Afunction%2C%20which%20leads%20to%20limited%20vision%20gain%20as%20it%20lacks%20more%20explicit%20physical%0Acues%20%28e.g.%2C%20depth%29.%20In%20this%20work%2C%20we%20investigate%20injecting%20the%20depth%20prior%20into%0Athe%20deep%20UIE%20model%20for%20more%20precise%20scene%20enhancement%20capability.%20To%20this%20end%2C%0Awe%20present%20a%20novel%20depth-guided%20perception%20UIE%20framework%2C%20dubbed%20underwater%0Avariable%20zoom%20%28UVZ%29.%20Specifically%2C%20UVZ%20resorts%20to%20a%20two-stage%20pipeline.%20First%2C%0Aa%20depth%20estimation%20network%20is%20designed%20to%20generate%20critical%20depth%20maps%2C%0Acombined%20with%20an%20auxiliary%20supervision%20network%20introduced%20to%20suppress%0Aestimation%20differences%20during%20training.%20Second%2C%20UVZ%20parses%20near-far%20scenarios%0Aby%20harnessing%20the%20predicted%20depth%20maps%2C%20enabling%20local%20and%20non-local%20perceiving%0Ain%20different%20regions.%20Extensive%20experiments%20on%20five%20benchmark%20datasets%0Ademonstrate%20that%20UVZ%20achieves%20superior%20visual%20gain%20and%20delivers%20promising%0Aquantitative%20metrics.%20Besides%2C%20UVZ%20is%20confirmed%20to%20exhibit%20good%20generalization%0Ain%20some%20visual%20tasks%2C%20especially%20in%20unusual%20lighting%20conditions.%20The%20code%2C%0Amodels%20and%20results%20are%20available%20at%3A%20https%3A//github.com/WindySprint/UVZ.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17883v2&entry.124074799=Read"},
{"title": "Sifting out communities in large sparse networks", "author": "Sharlee Climer and Kenneth Smith Jr and Wei Yang and Lisa de las Fuentes and Victor G. D\u00e1vila-Rom\u00e1n and C. Charles Gu", "abstract": "  Research data sets are growing to unprecedented sizes and network modeling is\ncommonly used to extract complex relationships in diverse domains, such as\ngenetic interactions involved in disease, logistics, and social communities. As\nthe number of nodes increases in a network, an increasing sparsity of edges is\na practical limitation due to memory restrictions. Moreover, many of these\nsparse networks exhibit very large numbers of nodes with no adjacent edges, as\nwell as disjoint components of nodes with no edges connecting them. A prevalent\naim in network modeling is the identification of clusters, or communities, of\nnodes that are highly interrelated. Several definitions of strong community\nstructure have been introduced to facilitate this task, each with inherent\nassumptions and biases. We introduce an intuitive objective function for\nquantifying the quality of clustering results in large sparse networks. We\nutilize a two-step method for identifying communities which is especially\nwell-suited for this domain as the first step efficiently divides the network\ninto the disjoint components, while the second step optimizes clustering of the\nproduced components based on the new objective. Using simulated networks,\noptimization based on the new objective function consistently yields\nsignificantly higher accuracy than those based on the modularity function, with\nthe widest gaps appearing for the noisiest networks. Additionally, applications\nto benchmark problems illustrate the intuitive correctness of our approach.\nFinally, the practicality of our approach is demonstrated in real-world data in\nwhich we identify complex genetic interactions in large-scale networks\ncomprised of tens of thousands of nodes. Based on these three different types\nof trials, our results clearly demonstrate the usefulness of our two-step\nprocedure and the accuracy of our simple objective.\n", "link": "http://arxiv.org/abs/2405.00816v1", "date": "2024-05-01", "relevancy": 2.2335, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4482}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4472}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4447}], "mailto": "mailto:daeR=997470421.yrtne&1v61800.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.evitcejbo02%elpmis02%ruo02%fo02%ycarucca02%eht02%dna02%erudecorpA0%pets-owt02%ruo02%fo02%ssenlufesu02%eht02%etartsnomed02%ylraelc02%stluser02%ruo02%C2%slairt02%foA0%sepyt02%tnereffid02%eerht02%eseht02%no02%desaB02%.sedon02%fo02%sdnasuoht02%fo02%snet02%fo02%desirpmocA0%skrowten02%elacs-egral02%ni02%snoitcaretni02%citeneg02%xelpmoc02%yfitnedi02%ew02%hcihwA0%ni02%atad02%dlrow-laer02%ni02%detartsnomed02%si02%hcaorppa02%ruo02%fo02%ytilacitcarp02%eht02%C2%yllaniFA0%.hcaorppa02%ruo02%fo02%ssentcerroc02%evitiutni02%eht02%etartsulli02%smelborp02%kramhcneb02%otA0%snoitacilppa02%C2%yllanoitiddA02%.skrowten02%tseision02%eht02%rof02%gniraeppa02%spag02%tsediw02%ehtA0%htiw02%C2%noitcnuf02%ytiraludom02%eht02%no02%desab02%esoht02%naht02%ycarucca02%rehgih02%yltnacifingisA0%sdleiy02%yltnetsisnoc02%noitcnuf02%evitcejbo02%wen02%eht02%no02%desab02%noitazimitpoA0%C2%skrowten02%detalumis02%gnisU02%.evitcejbo02%wen02%eht02%no02%desab02%stnenopmoc02%decudorpA0%eht02%fo02%gniretsulc02%sezimitpo02%pets02%dnoces02%eht02%elihw02%C2%stnenopmoc02%tniojsid02%eht02%otniA0%krowten02%eht02%sedivid02%yltneiciffe02%pets02%tsrif02%eht02%sa02%niamod02%siht02%rof02%detius-llewA0%yllaicepse02%si02%hcihw02%seitinummoc02%gniyfitnedi02%rof02%dohtem02%pets-owt02%a02%ezilituA0%eW02%.skrowten02%esraps02%egral02%ni02%stluser02%gniretsulc02%fo02%ytilauq02%eht02%gniyfitnauqA0%rof02%noitcnuf02%evitcejbo02%evitiutni02%na02%ecudortni02%eW02%.sesaib02%dna02%snoitpmussaA0%tnerehni02%htiw02%hcae02%C2%ksat02%siht02%etatilicaf02%ot02%decudortni02%neeb02%evah02%erutcurtsA0%ytinummoc02%gnorts02%fo02%snoitinifed02%lareveS02%.detalerretni02%ylhgih02%era02%taht02%sedonA0%fo02%C2%seitinummoc02%ro02%C2%sretsulc02%fo02%noitacifitnedi02%eht02%si02%gniledom02%krowten02%ni02%miaA0%tnelaverp02%A02%.meht02%gnitcennoc02%segde02%on02%htiw02%sedon02%fo02%stnenopmoc02%tniojsid02%sa02%llewA0%sa02%C2%segde02%tnecajda02%on02%htiw02%sedon02%fo02%srebmun02%egral02%yrev02%tibihxe02%skrowten02%esrapsA0%eseht02%fo02%ynam02%C2%revoeroM02%.snoitcirtser02%yromem02%ot02%eud02%noitatimil02%lacitcarp02%aA0%si02%segde02%fo02%ytisraps02%gnisaercni02%na02%C2%krowten02%a02%ni02%sesaercni02%sedon02%fo02%rebmun02%ehtA0%sA02%.seitinummoc02%laicos02%dna02%C2%scitsigol02%C2%esaesid02%ni02%devlovni02%snoitcaretni02%citenegA0%sa02%hcus02%C2%sniamod02%esrevid02%ni02%spihsnoitaler02%xelpmoc02%tcartxe02%ot02%desu02%ylnommocA0%si02%gniledom02%krowten02%dna02%sezis02%detnedecerpnu02%ot02%gniworg02%era02%stes02%atad02%hcraeseR02%02%=3328342921.yrtne&uG02%selrahC02%.C02%dna02%n1A%3C%moR-aliv1A%3C%D02%.G02%rotciV02%dna02%setneuF02%sal02%ed02%asiL02%dna02%gnaY02%ieW02%dna02%rJ02%htimS02%htenneK02%dna02%remilC02%eelrahS=526535609.yrtne&skrowten02%esraps02%egral02%ni02%seitinummoc02%tuo02%gnitfiS=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Sifting%20out%20communities%20in%20large%20sparse%20networks&body=Title%3A%20Sifting%20out%20communities%20in%20large%20sparse%20networks%0AAuthor%3A%20Sharlee%20Climer%20and%20Kenneth%20Smith%20Jr%20and%20Wei%20Yang%20and%20Lisa%20de%20las%20Fuentes%20and%20Victor%20G.%20D%C3%A1vila-Rom%C3%A1n%20and%20C.%20Charles%20Gu%0AAbstract%3A%20%20%20Research%20data%20sets%20are%20growing%20to%20unprecedented%20sizes%20and%20network%20modeling%20is%0Acommonly%20used%20to%20extract%20complex%20relationships%20in%20diverse%20domains%2C%20such%20as%0Agenetic%20interactions%20involved%20in%20disease%2C%20logistics%2C%20and%20social%20communities.%20As%0Athe%20number%20of%20nodes%20increases%20in%20a%20network%2C%20an%20increasing%20sparsity%20of%20edges%20is%0Aa%20practical%20limitation%20due%20to%20memory%20restrictions.%20Moreover%2C%20many%20of%20these%0Asparse%20networks%20exhibit%20very%20large%20numbers%20of%20nodes%20with%20no%20adjacent%20edges%2C%20as%0Awell%20as%20disjoint%20components%20of%20nodes%20with%20no%20edges%20connecting%20them.%20A%20prevalent%0Aaim%20in%20network%20modeling%20is%20the%20identification%20of%20clusters%2C%20or%20communities%2C%20of%0Anodes%20that%20are%20highly%20interrelated.%20Several%20definitions%20of%20strong%20community%0Astructure%20have%20been%20introduced%20to%20facilitate%20this%20task%2C%20each%20with%20inherent%0Aassumptions%20and%20biases.%20We%20introduce%20an%20intuitive%20objective%20function%20for%0Aquantifying%20the%20quality%20of%20clustering%20results%20in%20large%20sparse%20networks.%20We%0Autilize%20a%20two-step%20method%20for%20identifying%20communities%20which%20is%20especially%0Awell-suited%20for%20this%20domain%20as%20the%20first%20step%20efficiently%20divides%20the%20network%0Ainto%20the%20disjoint%20components%2C%20while%20the%20second%20step%20optimizes%20clustering%20of%20the%0Aproduced%20components%20based%20on%20the%20new%20objective.%20Using%20simulated%20networks%2C%0Aoptimization%20based%20on%20the%20new%20objective%20function%20consistently%20yields%0Asignificantly%20higher%20accuracy%20than%20those%20based%20on%20the%20modularity%20function%2C%20with%0Athe%20widest%20gaps%20appearing%20for%20the%20noisiest%20networks.%20Additionally%2C%20applications%0Ato%20benchmark%20problems%20illustrate%20the%20intuitive%20correctness%20of%20our%20approach.%0AFinally%2C%20the%20practicality%20of%20our%20approach%20is%20demonstrated%20in%20real-world%20data%20in%0Awhich%20we%20identify%20complex%20genetic%20interactions%20in%20large-scale%20networks%0Acomprised%20of%20tens%20of%20thousands%20of%20nodes.%20Based%20on%20these%20three%20different%20types%0Aof%20trials%2C%20our%20results%20clearly%20demonstrate%20the%20usefulness%20of%20our%20two-step%0Aprocedure%20and%20the%20accuracy%20of%20our%20simple%20objective.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00816v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sifting%20out%20communities%20in%20large%20sparse%20networks&entry.906535625=Sharlee%20Climer%20and%20Kenneth%20Smith%20Jr%20and%20Wei%20Yang%20and%20Lisa%20de%20las%20Fuentes%20and%20Victor%20G.%20D%C3%A1vila-Rom%C3%A1n%20and%20C.%20Charles%20Gu&entry.1292438233=%20%20Research%20data%20sets%20are%20growing%20to%20unprecedented%20sizes%20and%20network%20modeling%20is%0Acommonly%20used%20to%20extract%20complex%20relationships%20in%20diverse%20domains%2C%20such%20as%0Agenetic%20interactions%20involved%20in%20disease%2C%20logistics%2C%20and%20social%20communities.%20As%0Athe%20number%20of%20nodes%20increases%20in%20a%20network%2C%20an%20increasing%20sparsity%20of%20edges%20is%0Aa%20practical%20limitation%20due%20to%20memory%20restrictions.%20Moreover%2C%20many%20of%20these%0Asparse%20networks%20exhibit%20very%20large%20numbers%20of%20nodes%20with%20no%20adjacent%20edges%2C%20as%0Awell%20as%20disjoint%20components%20of%20nodes%20with%20no%20edges%20connecting%20them.%20A%20prevalent%0Aaim%20in%20network%20modeling%20is%20the%20identification%20of%20clusters%2C%20or%20communities%2C%20of%0Anodes%20that%20are%20highly%20interrelated.%20Several%20definitions%20of%20strong%20community%0Astructure%20have%20been%20introduced%20to%20facilitate%20this%20task%2C%20each%20with%20inherent%0Aassumptions%20and%20biases.%20We%20introduce%20an%20intuitive%20objective%20function%20for%0Aquantifying%20the%20quality%20of%20clustering%20results%20in%20large%20sparse%20networks.%20We%0Autilize%20a%20two-step%20method%20for%20identifying%20communities%20which%20is%20especially%0Awell-suited%20for%20this%20domain%20as%20the%20first%20step%20efficiently%20divides%20the%20network%0Ainto%20the%20disjoint%20components%2C%20while%20the%20second%20step%20optimizes%20clustering%20of%20the%0Aproduced%20components%20based%20on%20the%20new%20objective.%20Using%20simulated%20networks%2C%0Aoptimization%20based%20on%20the%20new%20objective%20function%20consistently%20yields%0Asignificantly%20higher%20accuracy%20than%20those%20based%20on%20the%20modularity%20function%2C%20with%0Athe%20widest%20gaps%20appearing%20for%20the%20noisiest%20networks.%20Additionally%2C%20applications%0Ato%20benchmark%20problems%20illustrate%20the%20intuitive%20correctness%20of%20our%20approach.%0AFinally%2C%20the%20practicality%20of%20our%20approach%20is%20demonstrated%20in%20real-world%20data%20in%0Awhich%20we%20identify%20complex%20genetic%20interactions%20in%20large-scale%20networks%0Acomprised%20of%20tens%20of%20thousands%20of%20nodes.%20Based%20on%20these%20three%20different%20types%0Aof%20trials%2C%20our%20results%20clearly%20demonstrate%20the%20usefulness%20of%20our%20two-step%0Aprocedure%20and%20the%20accuracy%20of%20our%20simple%20objective.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00816v1&entry.124074799=Read"},
{"title": "PAM-UNet: Shifting Attention on Region of Interest in Medical Images", "author": "Abhijit Das and Debesh Jha and Vandan Gorade and Koushik Biswas and Hongyi Pan and Zheyuan Zhang and Daniela P. Ladner and Yury Velichko and Amir Borhani and Ulas Bagci", "abstract": "  Computer-aided segmentation methods can assist medical personnel in improving\ndiagnostic outcomes. While recent advancements like UNet and its variants have\nshown promise, they face a critical challenge: balancing accuracy with\ncomputational efficiency. Shallow encoder architectures in UNets often struggle\nto capture crucial spatial features, leading in inaccurate and sparse\nsegmentation. To address this limitation, we propose a novel\n\\underline{P}rogressive \\underline{A}ttention based \\underline{M}obile\n\\underline{UNet} (\\underline{PAM-UNet}) architecture. The inverted residual\n(IR) blocks in PAM-UNet help maintain a lightweight framework, while layerwise\n\\textit{Progressive Luong Attention} ($\\mathcal{PLA}$) promotes precise\nsegmentation by directing attention toward regions of interest during\nsynthesis. Our approach prioritizes both accuracy and speed, achieving a\ncommendable balance with a mean IoU of 74.65 and a dice score of 82.87, while\nrequiring only 1.32 floating-point operations per second (FLOPS) on the Liver\nTumor Segmentation Benchmark (LiTS) 2017 dataset. These results highlight the\nimportance of developing efficient segmentation models to accelerate the\nadoption of AI in clinical practice.\n", "link": "http://arxiv.org/abs/2405.01503v1", "date": "2024-05-02", "relevancy": 2.2233, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.593}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5487}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5215}], "mailto": "mailto:daeR=997470421.yrtne&1v30510.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.ecitcarp02%lacinilc02%ni02%IA02%fo02%noitpodaA0%eht02%etarelecca02%ot02%sledom02%noitatnemges02%tneiciffe02%gnipoleved02%fo02%ecnatropmiA0%eht02%thgilhgih02%stluser02%esehT02%.tesatad02%710202%92%STiL82%02%kramhcneB02%noitatnemgeS02%romuTA0%reviL02%eht02%no02%92%SPOLF82%02%dnoces02%rep02%snoitarepo02%tniop-gnitaolf02%23.102%ylno02%gniriuqerA0%elihw02%C2%78.2802%fo02%erocs02%ecid02%a02%dna02%56.4702%fo02%UoI02%naem02%a02%htiw02%ecnalab02%elbadnemmocA0%a02%gniveihca02%C2%deeps02%dna02%ycarucca02%htob02%sezitiroirp02%hcaorppa02%ruO02%.sisehtnysA0%gnirud02%tseretni02%fo02%snoiger02%drawot02%noitnetta02%gnitcerid02%yb02%noitatnemgesA0%esicerp02%setomorp02%92%42%D7%ALPB7%lachtamC5%42%82%02%D7%noitnettA02%gnouL02%evissergorPB7%titxetC5%A0%esiwreyal02%elihw02%C2%krowemarf02%thgiewthgil02%a02%niatniam02%pleh02%teNU-MAP02%ni02%skcolb02%92%RI82%A0%laudiser02%detrevni02%ehT02%.erutcetihcra02%92%D7%teNU-MAPB7%enilrednuC5%82%02%D7%teNUB7%enilrednuC5%A0%eliboD7%MB7%enilrednuC5%02%desab02%noitnettD7%AB7%enilrednuC5%02%evissergorD7%PB7%enilrednuC5%A0%levon02%a02%esoporp02%ew02%C2%noitatimil02%siht02%sserdda02%oT02%.noitatnemgesA0%esraps02%dna02%etaruccani02%ni02%gnidael02%C2%serutaef02%laitaps02%laicurc02%erutpac02%otA0%elggurts02%netfo02%steNU02%ni02%serutcetihcra02%redocne02%wollahS02%.ycneiciffe02%lanoitatupmocA0%htiw02%ycarucca02%gnicnalab02%A3%egnellahc02%lacitirc02%a02%ecaf02%yeht02%C2%esimorp02%nwohsA0%evah02%stnairav02%sti02%dna02%teNU02%ekil02%stnemecnavda02%tnecer02%elihW02%.semoctuo02%citsongaidA0%gnivorpmi02%ni02%lennosrep02%lacidem02%tsissa02%nac02%sdohtem02%noitatnemges02%dedia-retupmoC02%02%=3328342921.yrtne&icgaB02%salU02%dna02%inahroB02%rimA02%dna02%okhcileV02%yruY02%dna02%rendaL02%.P02%aleinaD02%dna02%gnahZ02%nauyehZ02%dna02%naP02%iygnoH02%dna02%sawsiB02%kihsuoK02%dna02%edaroG02%nadnaV02%dna02%ahJ02%hsebeD02%dna02%saD02%tijihbA=526535609.yrtne&segamI02%lacideM02%ni02%tseretnI02%fo02%noigeR02%no02%noitnettA02%gnitfihS02%A3%teNU-MAP=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20PAM-UNet%3A%20Shifting%20Attention%20on%20Region%20of%20Interest%20in%20Medical%20Images&body=Title%3A%20PAM-UNet%3A%20Shifting%20Attention%20on%20Region%20of%20Interest%20in%20Medical%20Images%0AAuthor%3A%20Abhijit%20Das%20and%20Debesh%20Jha%20and%20Vandan%20Gorade%20and%20Koushik%20Biswas%20and%20Hongyi%20Pan%20and%20Zheyuan%20Zhang%20and%20Daniela%20P.%20Ladner%20and%20Yury%20Velichko%20and%20Amir%20Borhani%20and%20Ulas%20Bagci%0AAbstract%3A%20%20%20Computer-aided%20segmentation%20methods%20can%20assist%20medical%20personnel%20in%20improving%0Adiagnostic%20outcomes.%20While%20recent%20advancements%20like%20UNet%20and%20its%20variants%20have%0Ashown%20promise%2C%20they%20face%20a%20critical%20challenge%3A%20balancing%20accuracy%20with%0Acomputational%20efficiency.%20Shallow%20encoder%20architectures%20in%20UNets%20often%20struggle%0Ato%20capture%20crucial%20spatial%20features%2C%20leading%20in%20inaccurate%20and%20sparse%0Asegmentation.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%0A%5Cunderline%7BP%7Drogressive%20%5Cunderline%7BA%7Dttention%20based%20%5Cunderline%7BM%7Dobile%0A%5Cunderline%7BUNet%7D%20%28%5Cunderline%7BPAM-UNet%7D%29%20architecture.%20The%20inverted%20residual%0A%28IR%29%20blocks%20in%20PAM-UNet%20help%20maintain%20a%20lightweight%20framework%2C%20while%20layerwise%0A%5Ctextit%7BProgressive%20Luong%20Attention%7D%20%28%24%5Cmathcal%7BPLA%7D%24%29%20promotes%20precise%0Asegmentation%20by%20directing%20attention%20toward%20regions%20of%20interest%20during%0Asynthesis.%20Our%20approach%20prioritizes%20both%20accuracy%20and%20speed%2C%20achieving%20a%0Acommendable%20balance%20with%20a%20mean%20IoU%20of%2074.65%20and%20a%20dice%20score%20of%2082.87%2C%20while%0Arequiring%20only%201.32%20floating-point%20operations%20per%20second%20%28FLOPS%29%20on%20the%20Liver%0ATumor%20Segmentation%20Benchmark%20%28LiTS%29%202017%20dataset.%20These%20results%20highlight%20the%0Aimportance%20of%20developing%20efficient%20segmentation%20models%20to%20accelerate%20the%0Aadoption%20of%20AI%20in%20clinical%20practice.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01503v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PAM-UNet%3A%20Shifting%20Attention%20on%20Region%20of%20Interest%20in%20Medical%20Images&entry.906535625=Abhijit%20Das%20and%20Debesh%20Jha%20and%20Vandan%20Gorade%20and%20Koushik%20Biswas%20and%20Hongyi%20Pan%20and%20Zheyuan%20Zhang%20and%20Daniela%20P.%20Ladner%20and%20Yury%20Velichko%20and%20Amir%20Borhani%20and%20Ulas%20Bagci&entry.1292438233=%20%20Computer-aided%20segmentation%20methods%20can%20assist%20medical%20personnel%20in%20improving%0Adiagnostic%20outcomes.%20While%20recent%20advancements%20like%20UNet%20and%20its%20variants%20have%0Ashown%20promise%2C%20they%20face%20a%20critical%20challenge%3A%20balancing%20accuracy%20with%0Acomputational%20efficiency.%20Shallow%20encoder%20architectures%20in%20UNets%20often%20struggle%0Ato%20capture%20crucial%20spatial%20features%2C%20leading%20in%20inaccurate%20and%20sparse%0Asegmentation.%20To%20address%20this%20limitation%2C%20we%20propose%20a%20novel%0A%5Cunderline%7BP%7Drogressive%20%5Cunderline%7BA%7Dttention%20based%20%5Cunderline%7BM%7Dobile%0A%5Cunderline%7BUNet%7D%20%28%5Cunderline%7BPAM-UNet%7D%29%20architecture.%20The%20inverted%20residual%0A%28IR%29%20blocks%20in%20PAM-UNet%20help%20maintain%20a%20lightweight%20framework%2C%20while%20layerwise%0A%5Ctextit%7BProgressive%20Luong%20Attention%7D%20%28%24%5Cmathcal%7BPLA%7D%24%29%20promotes%20precise%0Asegmentation%20by%20directing%20attention%20toward%20regions%20of%20interest%20during%0Asynthesis.%20Our%20approach%20prioritizes%20both%20accuracy%20and%20speed%2C%20achieving%20a%0Acommendable%20balance%20with%20a%20mean%20IoU%20of%2074.65%20and%20a%20dice%20score%20of%2082.87%2C%20while%0Arequiring%20only%201.32%20floating-point%20operations%20per%20second%20%28FLOPS%29%20on%20the%20Liver%0ATumor%20Segmentation%20Benchmark%20%28LiTS%29%202017%20dataset.%20These%20results%20highlight%20the%0Aimportance%20of%20developing%20efficient%20segmentation%20models%20to%20accelerate%20the%0Aadoption%20of%20AI%20in%20clinical%20practice.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01503v1&entry.124074799=Read"},
{"title": "Sharp Bounds for Sequential Federated Learning on Heterogeneous Data", "author": "Yipeng Li and Xinchen Lyu", "abstract": "  There are two paradigms in Federated Learning (FL): parallel FL (PFL), where\nmodels are trained in a parallel manner across clients; and sequential FL\n(SFL), where models are trained in a sequential manner across clients. In\ncontrast to that of PFL, the convergence theory of SFL on heterogeneous data is\nstill lacking. To resolve the theoretical dilemma of SFL, we establish sharp\nconvergence guarantees for SFL on heterogeneous data with both upper and lower\nbounds. Specifically, we derive the upper bounds for strongly convex, general\nconvex and non-convex objective functions, and construct the matching lower\nbounds for the strongly convex and general convex objective functions. Then, we\ncompare the upper bounds of SFL with those of PFL, showing that SFL outperforms\nPFL (at least, when the level of heterogeneity is relatively high).\nExperimental results on quadratic functions and real data sets validate the\ncounterintuitive comparison result.\n", "link": "http://arxiv.org/abs/2405.01142v1", "date": "2024-05-02", "relevancy": 2.213, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4483}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4427}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4368}], "mailto": "mailto:daeR=997470421.yrtne&1v24110.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.tluser02%nosirapmoc02%evitiutniretnuocA0%eht02%etadilav02%stes02%atad02%laer02%dna02%snoitcnuf02%citardauq02%no02%stluser02%latnemirepxEA0%.92%hgih02%ylevitaler02%si02%ytienegoreteh02%fo02%level02%eht02%nehw02%C2%tsael02%ta82%02%LFPA0%smrofreptuo02%LFS02%taht02%gniwohs02%C2%LFP02%fo02%esoht02%htiw02%LFS02%fo02%sdnuob02%reppu02%eht02%erapmocA0%ew02%C2%nehT02%.snoitcnuf02%evitcejbo02%xevnoc02%lareneg02%dna02%xevnoc02%ylgnorts02%eht02%rof02%sdnuobA0%rewol02%gnihctam02%eht02%tcurtsnoc02%dna02%C2%snoitcnuf02%evitcejbo02%xevnoc-non02%dna02%xevnocA0%lareneg02%C2%xevnoc02%ylgnorts02%rof02%sdnuob02%reppu02%eht02%evired02%ew02%C2%yllacificepS02%.sdnuobA0%rewol02%dna02%reppu02%htob02%htiw02%atad02%suoenegoreteh02%no02%LFS02%rof02%seetnaraug02%ecnegrevnocA0%prahs02%hsilbatse02%ew02%C2%LFS02%fo02%ammelid02%laciteroeht02%eht02%evloser02%oT02%.gnikcal02%llitsA0%si02%atad02%suoenegoreteh02%no02%LFS02%fo02%yroeht02%ecnegrevnoc02%eht02%C2%LFP02%fo02%taht02%ot02%tsartnocA0%nI02%.stneilc02%ssorca02%rennam02%laitneuqes02%a02%ni02%deniart02%era02%sledom02%erehw02%C2%92%LFS82%A0%LF02%laitneuqes02%dna02%B3%stneilc02%ssorca02%rennam02%lellarap02%a02%ni02%deniart02%era02%sledomA0%erehw02%C2%92%LFP82%02%LF02%lellarap02%A3%92%LF82%02%gninraeL02%detaredeF02%ni02%smgidarap02%owt02%era02%erehT02%02%=3328342921.yrtne&uyL02%nehcniX02%dna02%iL02%gnepiY=526535609.yrtne&ataD02%suoenegoreteH02%no02%gninraeL02%detaredeF02%laitneuqeS02%rof02%sdnuoB02%prahS=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Sharp%20Bounds%20for%20Sequential%20Federated%20Learning%20on%20Heterogeneous%20Data&body=Title%3A%20Sharp%20Bounds%20for%20Sequential%20Federated%20Learning%20on%20Heterogeneous%20Data%0AAuthor%3A%20Yipeng%20Li%20and%20Xinchen%20Lyu%0AAbstract%3A%20%20%20There%20are%20two%20paradigms%20in%20Federated%20Learning%20%28FL%29%3A%20parallel%20FL%20%28PFL%29%2C%20where%0Amodels%20are%20trained%20in%20a%20parallel%20manner%20across%20clients%3B%20and%20sequential%20FL%0A%28SFL%29%2C%20where%20models%20are%20trained%20in%20a%20sequential%20manner%20across%20clients.%20In%0Acontrast%20to%20that%20of%20PFL%2C%20the%20convergence%20theory%20of%20SFL%20on%20heterogeneous%20data%20is%0Astill%20lacking.%20To%20resolve%20the%20theoretical%20dilemma%20of%20SFL%2C%20we%20establish%20sharp%0Aconvergence%20guarantees%20for%20SFL%20on%20heterogeneous%20data%20with%20both%20upper%20and%20lower%0Abounds.%20Specifically%2C%20we%20derive%20the%20upper%20bounds%20for%20strongly%20convex%2C%20general%0Aconvex%20and%20non-convex%20objective%20functions%2C%20and%20construct%20the%20matching%20lower%0Abounds%20for%20the%20strongly%20convex%20and%20general%20convex%20objective%20functions.%20Then%2C%20we%0Acompare%20the%20upper%20bounds%20of%20SFL%20with%20those%20of%20PFL%2C%20showing%20that%20SFL%20outperforms%0APFL%20%28at%20least%2C%20when%20the%20level%20of%20heterogeneity%20is%20relatively%20high%29.%0AExperimental%20results%20on%20quadratic%20functions%20and%20real%20data%20sets%20validate%20the%0Acounterintuitive%20comparison%20result.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01142v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sharp%20Bounds%20for%20Sequential%20Federated%20Learning%20on%20Heterogeneous%20Data&entry.906535625=Yipeng%20Li%20and%20Xinchen%20Lyu&entry.1292438233=%20%20There%20are%20two%20paradigms%20in%20Federated%20Learning%20%28FL%29%3A%20parallel%20FL%20%28PFL%29%2C%20where%0Amodels%20are%20trained%20in%20a%20parallel%20manner%20across%20clients%3B%20and%20sequential%20FL%0A%28SFL%29%2C%20where%20models%20are%20trained%20in%20a%20sequential%20manner%20across%20clients.%20In%0Acontrast%20to%20that%20of%20PFL%2C%20the%20convergence%20theory%20of%20SFL%20on%20heterogeneous%20data%20is%0Astill%20lacking.%20To%20resolve%20the%20theoretical%20dilemma%20of%20SFL%2C%20we%20establish%20sharp%0Aconvergence%20guarantees%20for%20SFL%20on%20heterogeneous%20data%20with%20both%20upper%20and%20lower%0Abounds.%20Specifically%2C%20we%20derive%20the%20upper%20bounds%20for%20strongly%20convex%2C%20general%0Aconvex%20and%20non-convex%20objective%20functions%2C%20and%20construct%20the%20matching%20lower%0Abounds%20for%20the%20strongly%20convex%20and%20general%20convex%20objective%20functions.%20Then%2C%20we%0Acompare%20the%20upper%20bounds%20of%20SFL%20with%20those%20of%20PFL%2C%20showing%20that%20SFL%20outperforms%0APFL%20%28at%20least%2C%20when%20the%20level%20of%20heterogeneity%20is%20relatively%20high%29.%0AExperimental%20results%20on%20quadratic%20functions%20and%20real%20data%20sets%20validate%20the%0Acounterintuitive%20comparison%20result.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01142v1&entry.124074799=Read"},
{"title": "A Comprehensive Survey of Dynamic Graph Neural Networks: Models,\n  Frameworks, Benchmarks, Experiments and Challenges", "author": "ZhengZhao Feng and Rui Wang and TianXing Wang and Mingli Song and Sai Wu and Shuibing He", "abstract": "  Dynamic Graph Neural Networks (GNNs) combine temporal information with GNNs\nto capture structural, temporal, and contextual relationships in dynamic graphs\nsimultaneously, leading to enhanced performance in various applications. As the\ndemand for dynamic GNNs continues to grow, numerous models and frameworks have\nemerged to cater to different application needs. There is a pressing need for a\ncomprehensive survey that evaluates the performance, strengths, and limitations\nof various approaches in this domain. This paper aims to fill this gap by\noffering a thorough comparative analysis and experimental evaluation of dynamic\nGNNs. It covers 81 dynamic GNN models with a novel taxonomy, 12 dynamic GNN\ntraining frameworks, and commonly used benchmarks. We also conduct experimental\nresults from testing representative nine dynamic GNN models and three\nframeworks on six standard graph datasets. Evaluation metrics focus on\nconvergence accuracy, training efficiency, and GPU memory usage, enabling a\nthorough comparison of performance across various models and frameworks. From\nthe analysis and evaluation results, we identify key challenges and offer\nprinciples for future research to enhance the design of models and frameworks\nin the dynamic GNNs field.\n", "link": "http://arxiv.org/abs/2405.00476v1", "date": "2024-05-01", "relevancy": 2.2093, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5642}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5616}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4994}], "mailto": "mailto:daeR=997470421.yrtne&1v67400.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.dleif02%sNNG02%cimanyd02%eht02%niA0%skrowemarf02%dna02%sledom02%fo02%ngised02%eht02%ecnahne02%ot02%hcraeser02%erutuf02%rof02%selpicnirpA0%reffo02%dna02%segnellahc02%yek02%yfitnedi02%ew02%C2%stluser02%noitaulave02%dna02%sisylana02%ehtA0%morF02%.skrowemarf02%dna02%sledom02%suoirav02%ssorca02%ecnamrofrep02%fo02%nosirapmoc02%hguorohtA0%a02%gnilbane02%C2%egasu02%yromem02%UPG02%dna02%C2%ycneiciffe02%gniniart02%C2%ycarucca02%ecnegrevnocA0%no02%sucof02%scirtem02%noitaulavE02%.stesatad02%hparg02%dradnats02%xis02%no02%skrowemarfA0%eerht02%dna02%sledom02%NNG02%cimanyd02%enin02%evitatneserper02%gnitset02%morf02%stluserA0%latnemirepxe02%tcudnoc02%osla02%eW02%.skramhcneb02%desu02%ylnommoc02%dna02%C2%skrowemarf02%gniniartA0%NNG02%cimanyd02%2102%C2%ymonoxat02%levon02%a02%htiw02%sledom02%NNG02%cimanyd02%1802%srevoc02%tI02%.sNNGA0%cimanyd02%fo02%noitaulave02%latnemirepxe02%dna02%sisylana02%evitarapmoc02%hguoroht02%a02%gnireffoA0%yb02%pag02%siht02%llif02%ot02%smia02%repap02%sihT02%.niamod02%siht02%ni02%sehcaorppa02%suoirav02%foA0%snoitatimil02%dna02%C2%shtgnerts02%C2%ecnamrofrep02%eht02%setaulave02%taht02%yevrus02%evisneherpmocA0%a02%rof02%deen02%gnisserp02%a02%si02%erehT02%.sdeen02%noitacilppa02%tnereffid02%ot02%retac02%ot02%degremeA0%evah02%skrowemarf02%dna02%sledom02%suoremun02%C2%worg02%ot02%seunitnoc02%sNNG02%cimanyd02%rof02%dnamedA0%eht02%sA02%.snoitacilppa02%suoirav02%ni02%ecnamrofrep02%decnahne02%ot02%gnidael02%C2%ylsuoenatlumisA0%shparg02%cimanyd02%ni02%spihsnoitaler02%lautxetnoc02%dna02%C2%laropmet02%C2%larutcurts02%erutpac02%otA0%sNNG02%htiw02%noitamrofni02%laropmet02%enibmoc02%92%sNNG82%02%skrowteN02%larueN02%hparG02%cimanyD02%02%=3328342921.yrtne&eH02%gnibiuhS02%dna02%uW02%iaS02%dna02%gnoS02%ilgniM02%dna02%gnaW02%gniXnaiT02%dna02%gnaW02%iuR02%dna02%gneF02%oahZgnehZ=526535609.yrtne&segnellahC02%dna02%stnemirepxE02%C2%skramhcneB02%C2%skrowemarF02%02%A0%C2%sledoM02%A3%skrowteN02%larueN02%hparG02%cimanyD02%fo02%yevruS02%evisneherpmoC02%A=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Survey%20of%20Dynamic%20Graph%20Neural%20Networks%3A%20Models%2C%0A%20%20Frameworks%2C%20Benchmarks%2C%20Experiments%20and%20Challenges&body=Title%3A%20A%20Comprehensive%20Survey%20of%20Dynamic%20Graph%20Neural%20Networks%3A%20Models%2C%0A%20%20Frameworks%2C%20Benchmarks%2C%20Experiments%20and%20Challenges%0AAuthor%3A%20ZhengZhao%20Feng%20and%20Rui%20Wang%20and%20TianXing%20Wang%20and%20Mingli%20Song%20and%20Sai%20Wu%20and%20Shuibing%20He%0AAbstract%3A%20%20%20Dynamic%20Graph%20Neural%20Networks%20%28GNNs%29%20combine%20temporal%20information%20with%20GNNs%0Ato%20capture%20structural%2C%20temporal%2C%20and%20contextual%20relationships%20in%20dynamic%20graphs%0Asimultaneously%2C%20leading%20to%20enhanced%20performance%20in%20various%20applications.%20As%20the%0Ademand%20for%20dynamic%20GNNs%20continues%20to%20grow%2C%20numerous%20models%20and%20frameworks%20have%0Aemerged%20to%20cater%20to%20different%20application%20needs.%20There%20is%20a%20pressing%20need%20for%20a%0Acomprehensive%20survey%20that%20evaluates%20the%20performance%2C%20strengths%2C%20and%20limitations%0Aof%20various%20approaches%20in%20this%20domain.%20This%20paper%20aims%20to%20fill%20this%20gap%20by%0Aoffering%20a%20thorough%20comparative%20analysis%20and%20experimental%20evaluation%20of%20dynamic%0AGNNs.%20It%20covers%2081%20dynamic%20GNN%20models%20with%20a%20novel%20taxonomy%2C%2012%20dynamic%20GNN%0Atraining%20frameworks%2C%20and%20commonly%20used%20benchmarks.%20We%20also%20conduct%20experimental%0Aresults%20from%20testing%20representative%20nine%20dynamic%20GNN%20models%20and%20three%0Aframeworks%20on%20six%20standard%20graph%20datasets.%20Evaluation%20metrics%20focus%20on%0Aconvergence%20accuracy%2C%20training%20efficiency%2C%20and%20GPU%20memory%20usage%2C%20enabling%20a%0Athorough%20comparison%20of%20performance%20across%20various%20models%20and%20frameworks.%20From%0Athe%20analysis%20and%20evaluation%20results%2C%20we%20identify%20key%20challenges%20and%20offer%0Aprinciples%20for%20future%20research%20to%20enhance%20the%20design%20of%20models%20and%20frameworks%0Ain%20the%20dynamic%20GNNs%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00476v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Survey%20of%20Dynamic%20Graph%20Neural%20Networks%3A%20Models%2C%0A%20%20Frameworks%2C%20Benchmarks%2C%20Experiments%20and%20Challenges&entry.906535625=ZhengZhao%20Feng%20and%20Rui%20Wang%20and%20TianXing%20Wang%20and%20Mingli%20Song%20and%20Sai%20Wu%20and%20Shuibing%20He&entry.1292438233=%20%20Dynamic%20Graph%20Neural%20Networks%20%28GNNs%29%20combine%20temporal%20information%20with%20GNNs%0Ato%20capture%20structural%2C%20temporal%2C%20and%20contextual%20relationships%20in%20dynamic%20graphs%0Asimultaneously%2C%20leading%20to%20enhanced%20performance%20in%20various%20applications.%20As%20the%0Ademand%20for%20dynamic%20GNNs%20continues%20to%20grow%2C%20numerous%20models%20and%20frameworks%20have%0Aemerged%20to%20cater%20to%20different%20application%20needs.%20There%20is%20a%20pressing%20need%20for%20a%0Acomprehensive%20survey%20that%20evaluates%20the%20performance%2C%20strengths%2C%20and%20limitations%0Aof%20various%20approaches%20in%20this%20domain.%20This%20paper%20aims%20to%20fill%20this%20gap%20by%0Aoffering%20a%20thorough%20comparative%20analysis%20and%20experimental%20evaluation%20of%20dynamic%0AGNNs.%20It%20covers%2081%20dynamic%20GNN%20models%20with%20a%20novel%20taxonomy%2C%2012%20dynamic%20GNN%0Atraining%20frameworks%2C%20and%20commonly%20used%20benchmarks.%20We%20also%20conduct%20experimental%0Aresults%20from%20testing%20representative%20nine%20dynamic%20GNN%20models%20and%20three%0Aframeworks%20on%20six%20standard%20graph%20datasets.%20Evaluation%20metrics%20focus%20on%0Aconvergence%20accuracy%2C%20training%20efficiency%2C%20and%20GPU%20memory%20usage%2C%20enabling%20a%0Athorough%20comparison%20of%20performance%20across%20various%20models%20and%20frameworks.%20From%0Athe%20analysis%20and%20evaluation%20results%2C%20we%20identify%20key%20challenges%20and%20offer%0Aprinciples%20for%20future%20research%20to%20enhance%20the%20design%20of%20models%20and%20frameworks%0Ain%20the%20dynamic%20GNNs%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00476v1&entry.124074799=Read"},
{"title": "Efficient and Flexible Method for Reducing Moderate-size Deep Neural\n  Networks with Condensation", "author": "Tianyi Chen and Zhi-Qin John Xu", "abstract": "  Neural networks have been extensively applied to a variety of tasks,\nachieving astounding results. Applying neural networks in the scientific field\nis an important research direction that is gaining increasing attention. In\nscientific applications, the scale of neural networks is generally\nmoderate-size, mainly to ensure the speed of inference during application.\nAdditionally, comparing neural networks to traditional algorithms in scientific\napplications is inevitable. These applications often require rapid\ncomputations, making the reduction of neural network sizes increasingly\nimportant. Existing work has found that the powerful capabilities of neural\nnetworks are primarily due to their non-linearity. Theoretical work has\ndiscovered that under strong non-linearity, neurons in the same layer tend to\nbehave similarly, a phenomenon known as condensation. Condensation offers an\nopportunity to reduce the scale of neural networks to a smaller subnetwork with\nsimilar performance. In this article, we propose a condensation reduction\nalgorithm to verify the feasibility of this idea in practical problems. Our\nreduction method can currently be applied to both fully connected networks and\nconvolutional networks, achieving positive results. In complex combustion\nacceleration tasks, we reduced the size of the neural network to 41.7% of its\noriginal scale while maintaining prediction accuracy. In the CIFAR10 image\nclassification task, we reduced the network size to 11.5% of the original\nscale, still maintaining a satisfactory validation accuracy. Our method can be\napplied to most trained neural networks, reducing computational pressure and\nimproving inference speed.\n", "link": "http://arxiv.org/abs/2405.01041v1", "date": "2024-05-02", "relevancy": 2.2026, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5528}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5513}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5483}], "mailto": "mailto:daeR=997470421.yrtne&1v14010.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.deeps02%ecnerefni02%gnivorpmiA0%dna02%erusserp02%lanoitatupmoc02%gnicuder02%C2%skrowten02%laruen02%deniart02%tsom02%ot02%deilppaA0%eb02%nac02%dohtem02%ruO02%.ycarucca02%noitadilav02%yrotcafsitas02%a02%gniniatniam02%llits02%C2%elacsA0%lanigiro02%eht02%fo02%52%5.1102%ot02%ezis02%krowten02%eht02%decuder02%ew02%C2%ksat02%noitacifissalcA0%egami02%01RAFIC02%eht02%nI02%.ycarucca02%noitciderp02%gniniatniam02%elihw02%elacs02%lanigiroA0%sti02%fo02%52%7.1402%ot02%krowten02%laruen02%eht02%fo02%ezis02%eht02%decuder02%ew02%C2%sksat02%noitareleccaA0%noitsubmoc02%xelpmoc02%nI02%.stluser02%evitisop02%gniveihca02%C2%skrowten02%lanoitulovnocA0%dna02%skrowten02%detcennoc02%ylluf02%htob02%ot02%deilppa02%eb02%yltnerruc02%nac02%dohtem02%noitcuderA0%ruO02%.smelborp02%lacitcarp02%ni02%aedi02%siht02%fo02%ytilibisaef02%eht02%yfirev02%ot02%mhtiroglaA0%noitcuder02%noitasnednoc02%a02%esoporp02%ew02%C2%elcitra02%siht02%nI02%.ecnamrofrep02%ralimisA0%htiw02%krowtenbus02%rellams02%a02%ot02%skrowten02%laruen02%fo02%elacs02%eht02%ecuder02%ot02%ytinutroppoA0%na02%sreffo02%noitasnednoC02%.noitasnednoc02%sa02%nwonk02%nonemonehp02%a02%C2%ylralimis02%evahebA0%ot02%dnet02%reyal02%emas02%eht02%ni02%snoruen02%C2%ytiraenil-non02%gnorts02%rednu02%taht02%derevocsidA0%sah02%krow02%laciteroehT02%.ytiraenil-non02%rieht02%ot02%eud02%yliramirp02%era02%skrowtenA0%laruen02%fo02%seitilibapac02%lufrewop02%eht02%taht02%dnuof02%sah02%krow02%gnitsixE02%.tnatropmiA0%ylgnisaercni02%sezis02%krowten02%laruen02%fo02%noitcuder02%eht02%gnikam02%C2%snoitatupmocA0%dipar02%eriuqer02%netfo02%snoitacilppa02%esehT02%.elbativeni02%si02%snoitacilppaA0%cifitneics02%ni02%smhtirogla02%lanoitidart02%ot02%skrowten02%laruen02%gnirapmoc02%C2%yllanoitiddAA0%.noitacilppa02%gnirud02%ecnerefni02%fo02%deeps02%eht02%erusne02%ot02%ylniam02%C2%ezis-etaredomA0%yllareneg02%si02%skrowten02%laruen02%fo02%elacs02%eht02%C2%snoitacilppa02%cifitneicsA0%nI02%.noitnetta02%gnisaercni02%gniniag02%si02%taht02%noitcerid02%hcraeser02%tnatropmi02%na02%siA0%dleif02%cifitneics02%eht02%ni02%skrowten02%laruen02%gniylppA02%.stluser02%gnidnuotsa02%gniveihcaA0%C2%sksat02%fo02%yteirav02%a02%ot02%deilppa02%ylevisnetxe02%neeb02%evah02%skrowten02%larueN02%02%=3328342921.yrtne&uX02%nhoJ02%niQ-ihZ02%dna02%nehC02%iynaiT=526535609.yrtne&noitasnednoC02%htiw02%skrowteN02%02%A0%larueN02%peeD02%ezis-etaredoM02%gnicudeR02%rof02%dohteM02%elbixelF02%dna02%tneiciffE=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Efficient%20and%20Flexible%20Method%20for%20Reducing%20Moderate-size%20Deep%20Neural%0A%20%20Networks%20with%20Condensation&body=Title%3A%20Efficient%20and%20Flexible%20Method%20for%20Reducing%20Moderate-size%20Deep%20Neural%0A%20%20Networks%20with%20Condensation%0AAuthor%3A%20Tianyi%20Chen%20and%20Zhi-Qin%20John%20Xu%0AAbstract%3A%20%20%20Neural%20networks%20have%20been%20extensively%20applied%20to%20a%20variety%20of%20tasks%2C%0Aachieving%20astounding%20results.%20Applying%20neural%20networks%20in%20the%20scientific%20field%0Ais%20an%20important%20research%20direction%20that%20is%20gaining%20increasing%20attention.%20In%0Ascientific%20applications%2C%20the%20scale%20of%20neural%20networks%20is%20generally%0Amoderate-size%2C%20mainly%20to%20ensure%20the%20speed%20of%20inference%20during%20application.%0AAdditionally%2C%20comparing%20neural%20networks%20to%20traditional%20algorithms%20in%20scientific%0Aapplications%20is%20inevitable.%20These%20applications%20often%20require%20rapid%0Acomputations%2C%20making%20the%20reduction%20of%20neural%20network%20sizes%20increasingly%0Aimportant.%20Existing%20work%20has%20found%20that%20the%20powerful%20capabilities%20of%20neural%0Anetworks%20are%20primarily%20due%20to%20their%20non-linearity.%20Theoretical%20work%20has%0Adiscovered%20that%20under%20strong%20non-linearity%2C%20neurons%20in%20the%20same%20layer%20tend%20to%0Abehave%20similarly%2C%20a%20phenomenon%20known%20as%20condensation.%20Condensation%20offers%20an%0Aopportunity%20to%20reduce%20the%20scale%20of%20neural%20networks%20to%20a%20smaller%20subnetwork%20with%0Asimilar%20performance.%20In%20this%20article%2C%20we%20propose%20a%20condensation%20reduction%0Aalgorithm%20to%20verify%20the%20feasibility%20of%20this%20idea%20in%20practical%20problems.%20Our%0Areduction%20method%20can%20currently%20be%20applied%20to%20both%20fully%20connected%20networks%20and%0Aconvolutional%20networks%2C%20achieving%20positive%20results.%20In%20complex%20combustion%0Aacceleration%20tasks%2C%20we%20reduced%20the%20size%20of%20the%20neural%20network%20to%2041.7%25%20of%20its%0Aoriginal%20scale%20while%20maintaining%20prediction%20accuracy.%20In%20the%20CIFAR10%20image%0Aclassification%20task%2C%20we%20reduced%20the%20network%20size%20to%2011.5%25%20of%20the%20original%0Ascale%2C%20still%20maintaining%20a%20satisfactory%20validation%20accuracy.%20Our%20method%20can%20be%0Aapplied%20to%20most%20trained%20neural%20networks%2C%20reducing%20computational%20pressure%20and%0Aimproving%20inference%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01041v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20and%20Flexible%20Method%20for%20Reducing%20Moderate-size%20Deep%20Neural%0A%20%20Networks%20with%20Condensation&entry.906535625=Tianyi%20Chen%20and%20Zhi-Qin%20John%20Xu&entry.1292438233=%20%20Neural%20networks%20have%20been%20extensively%20applied%20to%20a%20variety%20of%20tasks%2C%0Aachieving%20astounding%20results.%20Applying%20neural%20networks%20in%20the%20scientific%20field%0Ais%20an%20important%20research%20direction%20that%20is%20gaining%20increasing%20attention.%20In%0Ascientific%20applications%2C%20the%20scale%20of%20neural%20networks%20is%20generally%0Amoderate-size%2C%20mainly%20to%20ensure%20the%20speed%20of%20inference%20during%20application.%0AAdditionally%2C%20comparing%20neural%20networks%20to%20traditional%20algorithms%20in%20scientific%0Aapplications%20is%20inevitable.%20These%20applications%20often%20require%20rapid%0Acomputations%2C%20making%20the%20reduction%20of%20neural%20network%20sizes%20increasingly%0Aimportant.%20Existing%20work%20has%20found%20that%20the%20powerful%20capabilities%20of%20neural%0Anetworks%20are%20primarily%20due%20to%20their%20non-linearity.%20Theoretical%20work%20has%0Adiscovered%20that%20under%20strong%20non-linearity%2C%20neurons%20in%20the%20same%20layer%20tend%20to%0Abehave%20similarly%2C%20a%20phenomenon%20known%20as%20condensation.%20Condensation%20offers%20an%0Aopportunity%20to%20reduce%20the%20scale%20of%20neural%20networks%20to%20a%20smaller%20subnetwork%20with%0Asimilar%20performance.%20In%20this%20article%2C%20we%20propose%20a%20condensation%20reduction%0Aalgorithm%20to%20verify%20the%20feasibility%20of%20this%20idea%20in%20practical%20problems.%20Our%0Areduction%20method%20can%20currently%20be%20applied%20to%20both%20fully%20connected%20networks%20and%0Aconvolutional%20networks%2C%20achieving%20positive%20results.%20In%20complex%20combustion%0Aacceleration%20tasks%2C%20we%20reduced%20the%20size%20of%20the%20neural%20network%20to%2041.7%25%20of%20its%0Aoriginal%20scale%20while%20maintaining%20prediction%20accuracy.%20In%20the%20CIFAR10%20image%0Aclassification%20task%2C%20we%20reduced%20the%20network%20size%20to%2011.5%25%20of%20the%20original%0Ascale%2C%20still%20maintaining%20a%20satisfactory%20validation%20accuracy.%20Our%20method%20can%20be%0Aapplied%20to%20most%20trained%20neural%20networks%2C%20reducing%20computational%20pressure%20and%0Aimproving%20inference%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01041v1&entry.124074799=Read"},
{"title": "How Can I Get It Right? Using GPT to Rephrase Incorrect Trainee\n  Responses", "author": "Jionghao Lin and Zifei Han and Danielle R. Thomas and Ashish Gurung and Shivang Gupta and Vincent Aleven and Kenneth R. Koedinger", "abstract": "  One-on-one tutoring is widely acknowledged as an effective instructional\nmethod, conditioned on qualified tutors. However, the high demand for qualified\ntutors remains a challenge, often necessitating the training of novice tutors\n(i.e., trainees) to ensure effective tutoring. Research suggests that providing\ntimely explanatory feedback can facilitate the training process for trainees.\nHowever, it presents challenges due to the time-consuming nature of assessing\ntrainee performance by human experts. Inspired by the recent advancements of\nlarge language models (LLMs), our study employed the GPT-4 model to build an\nexplanatory feedback system. This system identifies trainees' responses in\nbinary form (i.e., correct/incorrect) and automatically provides template-based\nfeedback with responses appropriately rephrased by the GPT-4 model. We\nconducted our study on 410 responses from trainees across three training\nlessons: Giving Effective Praise, Reacting to Errors, and Determining What\nStudents Know. Our findings indicate that: 1) using a few-shot approach, the\nGPT-4 model effectively identifies correct/incorrect trainees' responses from\nthree training lessons with an average F1 score of 0.84 and an AUC score of\n0.85; and 2) using the few-shot approach, the GPT-4 model adeptly rephrases\nincorrect trainees' responses into desired responses, achieving performance\ncomparable to that of human experts.\n", "link": "http://arxiv.org/abs/2405.00970v1", "date": "2024-05-02", "relevancy": 2.195, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4524}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4473}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4173}], "mailto": "mailto:daeR=997470421.yrtne&1v07900.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.strepxe02%namuh02%fo02%taht02%ot02%elbarapmocA0%ecnamrofrep02%gniveihca02%C2%sesnopser02%derised02%otni02%sesnopser02%72%seeniart02%tcerrocniA0%sesarhper02%yltpeda02%ledom02%4-TPG02%eht02%C2%hcaorppa02%tohs-wef02%eht02%gnisu02%92%202%dna02%B3%58.0A0%fo02%erocs02%CUA02%na02%dna02%48.002%fo02%erocs02%1F02%egareva02%na02%htiw02%snossel02%gniniart02%eerhtA0%morf02%sesnopser02%72%seeniart02%tcerrocni/tcerroc02%seifitnedi02%ylevitceffe02%ledom02%4-TPGA0%eht02%C2%hcaorppa02%tohs-wef02%a02%gnisu02%92%102%A3%taht02%etacidni02%sgnidnif02%ruO02%.wonK02%stnedutSA0%tahW02%gninimreteD02%dna02%C2%srorrE02%ot02%gnitcaeR02%C2%esiarP02%evitceffE02%gniviG02%A3%snosselA0%gniniart02%eerht02%ssorca02%seeniart02%morf02%sesnopser02%01402%no02%yduts02%ruo02%detcudnocA0%eW02%.ledom02%4-TPG02%eht02%yb02%desarhper02%yletairporppa02%sesnopser02%htiw02%kcabdeefA0%desab-etalpmet02%sedivorp02%yllacitamotua02%dna02%92%tcerrocni/tcerroc02%C2%.e.i82%02%mrof02%yranibA0%ni02%sesnopser02%72%seeniart02%seifitnedi02%metsys02%sihT02%.metsys02%kcabdeef02%yrotanalpxeA0%na02%dliub02%ot02%ledom02%4-TPG02%eht02%deyolpme02%yduts02%ruo02%C2%92%sMLL82%02%sledom02%egaugnal02%egralA0%fo02%stnemecnavda02%tnecer02%eht02%yb02%deripsnI02%.strepxe02%namuh02%yb02%ecnamrofrep02%eeniartA0%gnissessa02%fo02%erutan02%gnimusnoc-emit02%eht02%ot02%eud02%segnellahc02%stneserp02%ti02%C2%revewoHA0%.seeniart02%rof02%ssecorp02%gniniart02%eht02%etatilicaf02%nac02%kcabdeef02%yrotanalpxe02%ylemitA0%gnidivorp02%taht02%stseggus02%hcraeseR02%.gnirotut02%evitceffe02%erusne02%ot02%92%seeniart02%C2%.e.i82%A0%srotut02%ecivon02%fo02%gniniart02%eht02%gnitatissecen02%netfo02%C2%egnellahc02%a02%sniamer02%srotutA0%deifilauq02%rof02%dnamed02%hgih02%eht02%C2%revewoH02%.srotut02%deifilauq02%no02%denoitidnoc02%C2%dohtemA0%lanoitcurtsni02%evitceffe02%na02%sa02%degdelwonkca02%ylediw02%si02%gnirotut02%eno-no-enO02%02%=3328342921.yrtne&regnideoK02%.R02%htenneK02%dna02%nevelA02%tnecniV02%dna02%atpuG02%gnavihS02%dna02%gnuruG02%hsihsA02%dna02%samohT02%.R02%elleinaD02%dna02%naH02%iefiZ02%dna02%niL02%oahgnoiJ=526535609.yrtne&sesnopseR02%02%A0%eeniarT02%tcerrocnI02%esarhpeR02%ot02%TPG02%gnisU02%F3%thgiR02%tI02%teG02%I02%naC02%woH=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20How%20Can%20I%20Get%20It%20Right%3F%20Using%20GPT%20to%20Rephrase%20Incorrect%20Trainee%0A%20%20Responses&body=Title%3A%20How%20Can%20I%20Get%20It%20Right%3F%20Using%20GPT%20to%20Rephrase%20Incorrect%20Trainee%0A%20%20Responses%0AAuthor%3A%20Jionghao%20Lin%20and%20Zifei%20Han%20and%20Danielle%20R.%20Thomas%20and%20Ashish%20Gurung%20and%20Shivang%20Gupta%20and%20Vincent%20Aleven%20and%20Kenneth%20R.%20Koedinger%0AAbstract%3A%20%20%20One-on-one%20tutoring%20is%20widely%20acknowledged%20as%20an%20effective%20instructional%0Amethod%2C%20conditioned%20on%20qualified%20tutors.%20However%2C%20the%20high%20demand%20for%20qualified%0Atutors%20remains%20a%20challenge%2C%20often%20necessitating%20the%20training%20of%20novice%20tutors%0A%28i.e.%2C%20trainees%29%20to%20ensure%20effective%20tutoring.%20Research%20suggests%20that%20providing%0Atimely%20explanatory%20feedback%20can%20facilitate%20the%20training%20process%20for%20trainees.%0AHowever%2C%20it%20presents%20challenges%20due%20to%20the%20time-consuming%20nature%20of%20assessing%0Atrainee%20performance%20by%20human%20experts.%20Inspired%20by%20the%20recent%20advancements%20of%0Alarge%20language%20models%20%28LLMs%29%2C%20our%20study%20employed%20the%20GPT-4%20model%20to%20build%20an%0Aexplanatory%20feedback%20system.%20This%20system%20identifies%20trainees%27%20responses%20in%0Abinary%20form%20%28i.e.%2C%20correct/incorrect%29%20and%20automatically%20provides%20template-based%0Afeedback%20with%20responses%20appropriately%20rephrased%20by%20the%20GPT-4%20model.%20We%0Aconducted%20our%20study%20on%20410%20responses%20from%20trainees%20across%20three%20training%0Alessons%3A%20Giving%20Effective%20Praise%2C%20Reacting%20to%20Errors%2C%20and%20Determining%20What%0AStudents%20Know.%20Our%20findings%20indicate%20that%3A%201%29%20using%20a%20few-shot%20approach%2C%20the%0AGPT-4%20model%20effectively%20identifies%20correct/incorrect%20trainees%27%20responses%20from%0Athree%20training%20lessons%20with%20an%20average%20F1%20score%20of%200.84%20and%20an%20AUC%20score%20of%0A0.85%3B%20and%202%29%20using%20the%20few-shot%20approach%2C%20the%20GPT-4%20model%20adeptly%20rephrases%0Aincorrect%20trainees%27%20responses%20into%20desired%20responses%2C%20achieving%20performance%0Acomparable%20to%20that%20of%20human%20experts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00970v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Can%20I%20Get%20It%20Right%3F%20Using%20GPT%20to%20Rephrase%20Incorrect%20Trainee%0A%20%20Responses&entry.906535625=Jionghao%20Lin%20and%20Zifei%20Han%20and%20Danielle%20R.%20Thomas%20and%20Ashish%20Gurung%20and%20Shivang%20Gupta%20and%20Vincent%20Aleven%20and%20Kenneth%20R.%20Koedinger&entry.1292438233=%20%20One-on-one%20tutoring%20is%20widely%20acknowledged%20as%20an%20effective%20instructional%0Amethod%2C%20conditioned%20on%20qualified%20tutors.%20However%2C%20the%20high%20demand%20for%20qualified%0Atutors%20remains%20a%20challenge%2C%20often%20necessitating%20the%20training%20of%20novice%20tutors%0A%28i.e.%2C%20trainees%29%20to%20ensure%20effective%20tutoring.%20Research%20suggests%20that%20providing%0Atimely%20explanatory%20feedback%20can%20facilitate%20the%20training%20process%20for%20trainees.%0AHowever%2C%20it%20presents%20challenges%20due%20to%20the%20time-consuming%20nature%20of%20assessing%0Atrainee%20performance%20by%20human%20experts.%20Inspired%20by%20the%20recent%20advancements%20of%0Alarge%20language%20models%20%28LLMs%29%2C%20our%20study%20employed%20the%20GPT-4%20model%20to%20build%20an%0Aexplanatory%20feedback%20system.%20This%20system%20identifies%20trainees%27%20responses%20in%0Abinary%20form%20%28i.e.%2C%20correct/incorrect%29%20and%20automatically%20provides%20template-based%0Afeedback%20with%20responses%20appropriately%20rephrased%20by%20the%20GPT-4%20model.%20We%0Aconducted%20our%20study%20on%20410%20responses%20from%20trainees%20across%20three%20training%0Alessons%3A%20Giving%20Effective%20Praise%2C%20Reacting%20to%20Errors%2C%20and%20Determining%20What%0AStudents%20Know.%20Our%20findings%20indicate%20that%3A%201%29%20using%20a%20few-shot%20approach%2C%20the%0AGPT-4%20model%20effectively%20identifies%20correct/incorrect%20trainees%27%20responses%20from%0Athree%20training%20lessons%20with%20an%20average%20F1%20score%20of%200.84%20and%20an%20AUC%20score%20of%0A0.85%3B%20and%202%29%20using%20the%20few-shot%20approach%2C%20the%20GPT-4%20model%20adeptly%20rephrases%0Aincorrect%20trainees%27%20responses%20into%20desired%20responses%2C%20achieving%20performance%0Acomparable%20to%20that%20of%20human%20experts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00970v1&entry.124074799=Read"},
{"title": "Benchmarking Representations for Speech, Music, and Acoustic Events", "author": "Moreno La Quatra and Alkis Koudounas and Lorenzo Vaiani and Elena Baralis and Luca Cagliero and Paolo Garza and Sabato Marco Siniscalchi", "abstract": "  Limited diversity in standardized benchmarks for evaluating audio\nrepresentation learning (ARL) methods may hinder systematic comparison of\ncurrent methods' capabilities. We present ARCH, a comprehensive benchmark for\nevaluating ARL methods on diverse audio classification domains, covering\nacoustic events, music, and speech. ARCH comprises 12 datasets, that allow us\nto thoroughly assess pre-trained SSL models of different sizes. ARCH\nstreamlines benchmarking of ARL techniques through its unified access to a wide\nrange of domains and its ability to readily incorporate new datasets and\nmodels. To address the current lack of open-source, pre-trained models for\nnon-speech audio, we also release new pre-trained models that demonstrate\nstrong performance on non-speech datasets. We argue that the presented\nwide-ranging evaluation provides valuable insights into state-of-the-art ARL\nmethods, and is useful to pinpoint promising research directions.\n", "link": "http://arxiv.org/abs/2405.00934v1", "date": "2024-05-02", "relevancy": 2.195, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4641}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.433}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4198}], "mailto": "mailto:daeR=997470421.yrtne&1v43900.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.snoitcerid02%hcraeser02%gnisimorp02%tniopnip02%ot02%lufesu02%si02%dna02%C2%sdohtemA0%LRA02%tra-eht-fo-etats02%otni02%sthgisni02%elbaulav02%sedivorp02%noitaulave02%gnignar-ediwA0%detneserp02%eht02%taht02%eugra02%eW02%.stesatad02%hceeps-non02%no02%ecnamrofrep02%gnortsA0%etartsnomed02%taht02%sledom02%deniart-erp02%wen02%esaeler02%osla02%ew02%C2%oidua02%hceeps-nonA0%rof02%sledom02%deniart-erp02%C2%ecruos-nepo02%fo02%kcal02%tnerruc02%eht02%sserdda02%oT02%.sledomA0%dna02%stesatad02%wen02%etaroprocni02%ylidaer02%ot02%ytiliba02%sti02%dna02%sniamod02%fo02%egnarA0%ediw02%a02%ot02%ssecca02%deifinu02%sti02%hguorht02%seuqinhcet02%LRA02%fo02%gnikramhcneb02%senilmaertsA0%HCRA02%.sezis02%tnereffid02%fo02%sledom02%LSS02%deniart-erp02%ssessa02%ylhguoroht02%otA0%su02%wolla02%taht02%C2%stesatad02%2102%sesirpmoc02%HCRA02%.hceeps02%dna02%C2%cisum02%C2%stneve02%citsuocaA0%gnirevoc02%C2%sniamod02%noitacifissalc02%oidua02%esrevid02%no02%sdohtem02%LRA02%gnitaulaveA0%rof02%kramhcneb02%evisneherpmoc02%a02%C2%HCRA02%tneserp02%eW02%.seitilibapac02%72%sdohtem02%tnerrucA0%fo02%nosirapmoc02%citametsys02%rednih02%yam02%sdohtem02%92%LRA82%02%gninrael02%noitatneserperA0%oidua02%gnitaulave02%rof02%skramhcneb02%dezidradnats02%ni02%ytisrevid02%detimiL02%02%=3328342921.yrtne&ihclacsiniS02%ocraM02%otabaS02%dna02%azraG02%oloaP02%dna02%oreilgaC02%acuL02%dna02%silaraB02%anelE02%dna02%inaiaV02%ozneroL02%dna02%sanuoduoK02%siklA02%dna02%artauQ02%aL02%oneroM=526535609.yrtne&stnevE02%citsuocA02%dna02%C2%cisuM02%C2%hceepS02%rof02%snoitatneserpeR02%gnikramhcneB=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Representations%20for%20Speech%2C%20Music%2C%20and%20Acoustic%20Events&body=Title%3A%20Benchmarking%20Representations%20for%20Speech%2C%20Music%2C%20and%20Acoustic%20Events%0AAuthor%3A%20Moreno%20La%20Quatra%20and%20Alkis%20Koudounas%20and%20Lorenzo%20Vaiani%20and%20Elena%20Baralis%20and%20Luca%20Cagliero%20and%20Paolo%20Garza%20and%20Sabato%20Marco%20Siniscalchi%0AAbstract%3A%20%20%20Limited%20diversity%20in%20standardized%20benchmarks%20for%20evaluating%20audio%0Arepresentation%20learning%20%28ARL%29%20methods%20may%20hinder%20systematic%20comparison%20of%0Acurrent%20methods%27%20capabilities.%20We%20present%20ARCH%2C%20a%20comprehensive%20benchmark%20for%0Aevaluating%20ARL%20methods%20on%20diverse%20audio%20classification%20domains%2C%20covering%0Aacoustic%20events%2C%20music%2C%20and%20speech.%20ARCH%20comprises%2012%20datasets%2C%20that%20allow%20us%0Ato%20thoroughly%20assess%20pre-trained%20SSL%20models%20of%20different%20sizes.%20ARCH%0Astreamlines%20benchmarking%20of%20ARL%20techniques%20through%20its%20unified%20access%20to%20a%20wide%0Arange%20of%20domains%20and%20its%20ability%20to%20readily%20incorporate%20new%20datasets%20and%0Amodels.%20To%20address%20the%20current%20lack%20of%20open-source%2C%20pre-trained%20models%20for%0Anon-speech%20audio%2C%20we%20also%20release%20new%20pre-trained%20models%20that%20demonstrate%0Astrong%20performance%20on%20non-speech%20datasets.%20We%20argue%20that%20the%20presented%0Awide-ranging%20evaluation%20provides%20valuable%20insights%20into%20state-of-the-art%20ARL%0Amethods%2C%20and%20is%20useful%20to%20pinpoint%20promising%20research%20directions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00934v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Representations%20for%20Speech%2C%20Music%2C%20and%20Acoustic%20Events&entry.906535625=Moreno%20La%20Quatra%20and%20Alkis%20Koudounas%20and%20Lorenzo%20Vaiani%20and%20Elena%20Baralis%20and%20Luca%20Cagliero%20and%20Paolo%20Garza%20and%20Sabato%20Marco%20Siniscalchi&entry.1292438233=%20%20Limited%20diversity%20in%20standardized%20benchmarks%20for%20evaluating%20audio%0Arepresentation%20learning%20%28ARL%29%20methods%20may%20hinder%20systematic%20comparison%20of%0Acurrent%20methods%27%20capabilities.%20We%20present%20ARCH%2C%20a%20comprehensive%20benchmark%20for%0Aevaluating%20ARL%20methods%20on%20diverse%20audio%20classification%20domains%2C%20covering%0Aacoustic%20events%2C%20music%2C%20and%20speech.%20ARCH%20comprises%2012%20datasets%2C%20that%20allow%20us%0Ato%20thoroughly%20assess%20pre-trained%20SSL%20models%20of%20different%20sizes.%20ARCH%0Astreamlines%20benchmarking%20of%20ARL%20techniques%20through%20its%20unified%20access%20to%20a%20wide%0Arange%20of%20domains%20and%20its%20ability%20to%20readily%20incorporate%20new%20datasets%20and%0Amodels.%20To%20address%20the%20current%20lack%20of%20open-source%2C%20pre-trained%20models%20for%0Anon-speech%20audio%2C%20we%20also%20release%20new%20pre-trained%20models%20that%20demonstrate%0Astrong%20performance%20on%20non-speech%20datasets.%20We%20argue%20that%20the%20presented%0Awide-ranging%20evaluation%20provides%20valuable%20insights%20into%20state-of-the-art%20ARL%0Amethods%2C%20and%20is%20useful%20to%20pinpoint%20promising%20research%20directions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00934v1&entry.124074799=Read"},
{"title": "MTDT: A Multi-Task Deep Learning Digital Twin", "author": "Nooshin Yousefzadeh and Rahul Sengupta and Yashaswi Karnati and Anand Rangarajan and Sanjay Ranka", "abstract": "  Traffic congestion has significant impacts on both the economy and the\nenvironment. Measures of Effectiveness (MOEs) have long been the standard for\nevaluating the level of service and operational efficiency of traffic\nintersections. However, the scarcity of traditional high-resolution loop\ndetector data (ATSPM) presents challenges in accurately measuring MOEs or\ncapturing the intricate temporospatial characteristics inherent in urban\nintersection traffic. In response to this challenge, we have introduced the\nMulti-Task Deep Learning Digital Twin (MTDT) as a solution for multifaceted and\nprecise intersection traffic flow simulation. MTDT enables accurate,\nfine-grained estimation of loop detector waveform time series for each lane of\nmovement, alongside successful estimation of several MOEs for each lane group\nassociated with a traffic phase concurrently and for all approaches of an\narbitrary urban intersection. Unlike existing deep learning methodologies, MTDT\ndistinguishes itself through its adaptability to local temporal and spatial\nfeatures, such as signal timing plans, intersection topology, driving\nbehaviors, and turning movement counts. While maintaining a straightforward\ndesign, our model emphasizes the advantages of multi-task learning in traffic\nmodeling. By consolidating the learning process across multiple tasks, MTDT\ndemonstrates reduced overfitting, increased efficiency, and enhanced\neffectiveness by sharing representations learned by different tasks.\nFurthermore, our approach facilitates sequential computation and lends itself\nto complete parallelization through GPU implementation. This not only\nstreamlines the computational process but also enhances scalability and\nperformance.\n", "link": "http://arxiv.org/abs/2405.00922v1", "date": "2024-05-02", "relevancy": 2.1944, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5545}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5498}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5422}], "mailto": "mailto:daeR=997470421.yrtne&1v22900.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.ecnamrofrepA0%dna02%ytilibalacs02%secnahne02%osla02%tub02%ssecorp02%lanoitatupmoc02%eht02%senilmaertsA0%ylno02%ton02%sihT02%.noitatnemelpmi02%UPG02%hguorht02%noitazilellarap02%etelpmoc02%otA0%flesti02%sdnel02%dna02%noitatupmoc02%laitneuqes02%setatilicaf02%hcaorppa02%ruo02%C2%eromrehtruFA0%.sksat02%tnereffid02%yb02%denrael02%snoitatneserper02%gnirahs02%yb02%ssenevitceffeA0%decnahne02%dna02%C2%ycneiciffe02%desaercni02%C2%gnittifrevo02%decuder02%setartsnomedA0%TDTM02%C2%sksat02%elpitlum02%ssorca02%ssecorp02%gninrael02%eht02%gnitadilosnoc02%yB02%.gniledomA0%ciffart02%ni02%gninrael02%ksat-itlum02%fo02%segatnavda02%eht02%sezisahpme02%ledom02%ruo02%C2%ngisedA0%drawrofthgiarts02%a02%gniniatniam02%elihW02%.stnuoc02%tnemevom02%gninrut02%dna02%C2%sroivahebA0%gnivird02%C2%ygolopot02%noitcesretni02%C2%snalp02%gnimit02%langis02%sa02%hcus02%C2%serutaefA0%laitaps02%dna02%laropmet02%lacol02%ot02%ytilibatpada02%sti02%hguorht02%flesti02%sehsiugnitsidA0%TDTM02%C2%seigolodohtem02%gninrael02%peed02%gnitsixe02%ekilnU02%.noitcesretni02%nabru02%yrartibraA0%na02%fo02%sehcaorppa02%lla02%rof02%dna02%yltnerrucnoc02%esahp02%ciffart02%a02%htiw02%detaicossaA0%puorg02%enal02%hcae02%rof02%sEOM02%lareves02%fo02%noitamitse02%lufsseccus02%edisgnola02%C2%tnemevomA0%fo02%enal02%hcae02%rof02%seires02%emit02%mrofevaw02%rotceted02%pool02%fo02%noitamitse02%deniarg-enifA0%C2%etarucca02%selbane02%TDTM02%.noitalumis02%wolf02%ciffart02%noitcesretni02%esicerpA0%dna02%detecafitlum02%rof02%noitulos02%a02%sa02%92%TDTM82%02%niwT02%latigiD02%gninraeL02%peeD02%ksaT-itluMA0%eht02%decudortni02%evah02%ew02%C2%egnellahc02%siht02%ot02%esnopser02%nI02%.ciffart02%noitcesretniA0%nabru02%ni02%tnerehni02%scitsiretcarahc02%laitapsoropmet02%etacirtni02%eht02%gnirutpacA0%ro02%sEOM02%gnirusaem02%yletarucca02%ni02%segnellahc02%stneserp02%92%MPSTA82%02%atad02%rotcetedA0%pool02%noituloser-hgih02%lanoitidart02%fo02%yticracs02%eht02%C2%revewoH02%.snoitcesretniA0%ciffart02%fo02%ycneiciffe02%lanoitarepo02%dna02%ecivres02%fo02%level02%eht02%gnitaulaveA0%rof02%dradnats02%eht02%neeb02%gnol02%evah02%92%sEOM82%02%ssenevitceffE02%fo02%serusaeM02%.tnemnorivneA0%eht02%dna02%ymonoce02%eht02%htob02%no02%stcapmi02%tnacifingis02%sah02%noitsegnoc02%ciffarT02%02%=3328342921.yrtne&aknaR02%yajnaS02%dna02%najaragnaR02%dnanA02%dna02%itanraK02%iwsahsaY02%dna02%atpugneS02%luhaR02%dna02%hedazfesuoY02%nihsooN=526535609.yrtne&niwT02%latigiD02%gninraeL02%peeD02%ksaT-itluM02%A02%A3%TDTM=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20MTDT%3A%20A%20Multi-Task%20Deep%20Learning%20Digital%20Twin&body=Title%3A%20MTDT%3A%20A%20Multi-Task%20Deep%20Learning%20Digital%20Twin%0AAuthor%3A%20Nooshin%20Yousefzadeh%20and%20Rahul%20Sengupta%20and%20Yashaswi%20Karnati%20and%20Anand%20Rangarajan%20and%20Sanjay%20Ranka%0AAbstract%3A%20%20%20Traffic%20congestion%20has%20significant%20impacts%20on%20both%20the%20economy%20and%20the%0Aenvironment.%20Measures%20of%20Effectiveness%20%28MOEs%29%20have%20long%20been%20the%20standard%20for%0Aevaluating%20the%20level%20of%20service%20and%20operational%20efficiency%20of%20traffic%0Aintersections.%20However%2C%20the%20scarcity%20of%20traditional%20high-resolution%20loop%0Adetector%20data%20%28ATSPM%29%20presents%20challenges%20in%20accurately%20measuring%20MOEs%20or%0Acapturing%20the%20intricate%20temporospatial%20characteristics%20inherent%20in%20urban%0Aintersection%20traffic.%20In%20response%20to%20this%20challenge%2C%20we%20have%20introduced%20the%0AMulti-Task%20Deep%20Learning%20Digital%20Twin%20%28MTDT%29%20as%20a%20solution%20for%20multifaceted%20and%0Aprecise%20intersection%20traffic%20flow%20simulation.%20MTDT%20enables%20accurate%2C%0Afine-grained%20estimation%20of%20loop%20detector%20waveform%20time%20series%20for%20each%20lane%20of%0Amovement%2C%20alongside%20successful%20estimation%20of%20several%20MOEs%20for%20each%20lane%20group%0Aassociated%20with%20a%20traffic%20phase%20concurrently%20and%20for%20all%20approaches%20of%20an%0Aarbitrary%20urban%20intersection.%20Unlike%20existing%20deep%20learning%20methodologies%2C%20MTDT%0Adistinguishes%20itself%20through%20its%20adaptability%20to%20local%20temporal%20and%20spatial%0Afeatures%2C%20such%20as%20signal%20timing%20plans%2C%20intersection%20topology%2C%20driving%0Abehaviors%2C%20and%20turning%20movement%20counts.%20While%20maintaining%20a%20straightforward%0Adesign%2C%20our%20model%20emphasizes%20the%20advantages%20of%20multi-task%20learning%20in%20traffic%0Amodeling.%20By%20consolidating%20the%20learning%20process%20across%20multiple%20tasks%2C%20MTDT%0Ademonstrates%20reduced%20overfitting%2C%20increased%20efficiency%2C%20and%20enhanced%0Aeffectiveness%20by%20sharing%20representations%20learned%20by%20different%20tasks.%0AFurthermore%2C%20our%20approach%20facilitates%20sequential%20computation%20and%20lends%20itself%0Ato%20complete%20parallelization%20through%20GPU%20implementation.%20This%20not%20only%0Astreamlines%20the%20computational%20process%20but%20also%20enhances%20scalability%20and%0Aperformance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00922v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MTDT%3A%20A%20Multi-Task%20Deep%20Learning%20Digital%20Twin&entry.906535625=Nooshin%20Yousefzadeh%20and%20Rahul%20Sengupta%20and%20Yashaswi%20Karnati%20and%20Anand%20Rangarajan%20and%20Sanjay%20Ranka&entry.1292438233=%20%20Traffic%20congestion%20has%20significant%20impacts%20on%20both%20the%20economy%20and%20the%0Aenvironment.%20Measures%20of%20Effectiveness%20%28MOEs%29%20have%20long%20been%20the%20standard%20for%0Aevaluating%20the%20level%20of%20service%20and%20operational%20efficiency%20of%20traffic%0Aintersections.%20However%2C%20the%20scarcity%20of%20traditional%20high-resolution%20loop%0Adetector%20data%20%28ATSPM%29%20presents%20challenges%20in%20accurately%20measuring%20MOEs%20or%0Acapturing%20the%20intricate%20temporospatial%20characteristics%20inherent%20in%20urban%0Aintersection%20traffic.%20In%20response%20to%20this%20challenge%2C%20we%20have%20introduced%20the%0AMulti-Task%20Deep%20Learning%20Digital%20Twin%20%28MTDT%29%20as%20a%20solution%20for%20multifaceted%20and%0Aprecise%20intersection%20traffic%20flow%20simulation.%20MTDT%20enables%20accurate%2C%0Afine-grained%20estimation%20of%20loop%20detector%20waveform%20time%20series%20for%20each%20lane%20of%0Amovement%2C%20alongside%20successful%20estimation%20of%20several%20MOEs%20for%20each%20lane%20group%0Aassociated%20with%20a%20traffic%20phase%20concurrently%20and%20for%20all%20approaches%20of%20an%0Aarbitrary%20urban%20intersection.%20Unlike%20existing%20deep%20learning%20methodologies%2C%20MTDT%0Adistinguishes%20itself%20through%20its%20adaptability%20to%20local%20temporal%20and%20spatial%0Afeatures%2C%20such%20as%20signal%20timing%20plans%2C%20intersection%20topology%2C%20driving%0Abehaviors%2C%20and%20turning%20movement%20counts.%20While%20maintaining%20a%20straightforward%0Adesign%2C%20our%20model%20emphasizes%20the%20advantages%20of%20multi-task%20learning%20in%20traffic%0Amodeling.%20By%20consolidating%20the%20learning%20process%20across%20multiple%20tasks%2C%20MTDT%0Ademonstrates%20reduced%20overfitting%2C%20increased%20efficiency%2C%20and%20enhanced%0Aeffectiveness%20by%20sharing%20representations%20learned%20by%20different%20tasks.%0AFurthermore%2C%20our%20approach%20facilitates%20sequential%20computation%20and%20lends%20itself%0Ato%20complete%20parallelization%20through%20GPU%20implementation.%20This%20not%20only%0Astreamlines%20the%20computational%20process%20but%20also%20enhances%20scalability%20and%0Aperformance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00922v1&entry.124074799=Read"},
{"title": "KAN: Kolmogorov-Arnold Networks", "author": "Ziming Liu and Yixuan Wang and Sachin Vaidya and Fabian Ruehle and James Halverson and Marin Solja\u010di\u0107 and Thomas Y. Hou and Max Tegmark", "abstract": "  Inspired by the Kolmogorov-Arnold representation theorem, we propose\nKolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer\nPerceptrons (MLPs). While MLPs have fixed activation functions on nodes\n(\"neurons\"), KANs have learnable activation functions on edges (\"weights\").\nKANs have no linear weights at all -- every weight parameter is replaced by a\nunivariate function parametrized as a spline. We show that this seemingly\nsimple change makes KANs outperform MLPs in terms of accuracy and\ninterpretability. For accuracy, much smaller KANs can achieve comparable or\nbetter accuracy than much larger MLPs in data fitting and PDE solving.\nTheoretically and empirically, KANs possess faster neural scaling laws than\nMLPs. For interpretability, KANs can be intuitively visualized and can easily\ninteract with human users. Through two examples in mathematics and physics,\nKANs are shown to be useful collaborators helping scientists (re)discover\nmathematical and physical laws. In summary, KANs are promising alternatives for\nMLPs, opening opportunities for further improving today's deep learning models\nwhich rely heavily on MLPs.\n", "link": "http://arxiv.org/abs/2404.19756v2", "date": "2024-05-02", "relevancy": 1.7839, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4828}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4628}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4144}], "mailto": "mailto:daeR=997470421.yrtne&2v65791.4042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.sPLM02%no02%ylivaeh02%yler02%hcihwA0%sledom02%gninrael02%peed02%s72%yadot02%gnivorpmi02%rehtruf02%rof02%seitinutroppo02%gninepo02%C2%sPLMA0%rof02%sevitanretla02%gnisimorp02%era02%sNAK02%C2%yrammus02%nI02%.swal02%lacisyhp02%dna02%lacitamehtamA0%revocsid92%er82%02%stsitneics02%gnipleh02%srotaroballoc02%lufesu02%eb02%ot02%nwohs02%era02%sNAKA0%C2%scisyhp02%dna02%scitamehtam02%ni02%selpmaxe02%owt02%hguorhT02%.sresu02%namuh02%htiw02%tcaretniA0%ylisae02%nac02%dna02%dezilausiv02%ylevitiutni02%eb02%nac02%sNAK02%C2%ytilibaterpretni02%roF02%.sPLMA0%naht02%swal02%gnilacs02%laruen02%retsaf02%ssessop02%sNAK02%C2%yllaciripme02%dna02%yllaciteroehTA0%.gnivlos02%EDP02%dna02%gnittif02%atad02%ni02%sPLM02%regral02%hcum02%naht02%ycarucca02%rettebA0%ro02%elbarapmoc02%eveihca02%nac02%sNAK02%rellams02%hcum02%C2%ycarucca02%roF02%.ytilibaterpretniA0%dna02%ycarucca02%fo02%smret02%ni02%sPLM02%mrofreptuo02%sNAK02%sekam02%egnahc02%elpmisA0%ylgnimees02%siht02%taht02%wohs02%eW02%.enilps02%a02%sa02%dezirtemarap02%noitcnuf02%etairavinuA0%a02%yb02%decalper02%si02%retemarap02%thgiew02%yreve02%--02%lla02%ta02%sthgiew02%raenil02%on02%evah02%sNAKA0%.92%22%sthgiew22%82%02%segde02%no02%snoitcnuf02%noitavitca02%elbanrael02%evah02%sNAK02%C2%92%22%snoruen22%82%A0%sedon02%no02%snoitcnuf02%noitavitca02%dexif02%evah02%sPLM02%elihW02%.92%sPLM82%02%snortpecrePA0%reyaL-itluM02%ot02%sevitanretla02%gnisimorp02%sa02%92%sNAK82%02%skrowteN02%dlonrA-vorogomloKA0%esoporp02%ew02%C2%meroeht02%noitatneserper02%dlonrA-vorogomloK02%eht02%yb02%deripsnI02%02%=3328342921.yrtne&kramgeT02%xaM02%dna02%uoH02%.Y02%samohT02%dna02%78%4C%iD8%4C%ajloS02%niraM02%dna02%nosrevlaH02%semaJ02%dna02%elheuR02%naibaF02%dna02%aydiaV02%nihcaS02%dna02%gnaW02%nauxiY02%dna02%uiL02%gnimiZ=526535609.yrtne&skrowteN02%dlonrA-vorogomloK02%A3%NAK=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20KAN%3A%20Kolmogorov-Arnold%20Networks&body=Title%3A%20KAN%3A%20Kolmogorov-Arnold%20Networks%0AAuthor%3A%20Ziming%20Liu%20and%20Yixuan%20Wang%20and%20Sachin%20Vaidya%20and%20Fabian%20Ruehle%20and%20James%20Halverson%20and%20Marin%20Solja%C4%8Di%C4%87%20and%20Thomas%20Y.%20Hou%20and%20Max%20Tegmark%0AAbstract%3A%20%20%20Inspired%20by%20the%20Kolmogorov-Arnold%20representation%20theorem%2C%20we%20propose%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20as%20promising%20alternatives%20to%20Multi-Layer%0APerceptrons%20%28MLPs%29.%20While%20MLPs%20have%20fixed%20activation%20functions%20on%20nodes%0A%28%22neurons%22%29%2C%20KANs%20have%20learnable%20activation%20functions%20on%20edges%20%28%22weights%22%29.%0AKANs%20have%20no%20linear%20weights%20at%20all%20--%20every%20weight%20parameter%20is%20replaced%20by%20a%0Aunivariate%20function%20parametrized%20as%20a%20spline.%20We%20show%20that%20this%20seemingly%0Asimple%20change%20makes%20KANs%20outperform%20MLPs%20in%20terms%20of%20accuracy%20and%0Ainterpretability.%20For%20accuracy%2C%20much%20smaller%20KANs%20can%20achieve%20comparable%20or%0Abetter%20accuracy%20than%20much%20larger%20MLPs%20in%20data%20fitting%20and%20PDE%20solving.%0ATheoretically%20and%20empirically%2C%20KANs%20possess%20faster%20neural%20scaling%20laws%20than%0AMLPs.%20For%20interpretability%2C%20KANs%20can%20be%20intuitively%20visualized%20and%20can%20easily%0Ainteract%20with%20human%20users.%20Through%20two%20examples%20in%20mathematics%20and%20physics%2C%0AKANs%20are%20shown%20to%20be%20useful%20collaborators%20helping%20scientists%20%28re%29discover%0Amathematical%20and%20physical%20laws.%20In%20summary%2C%20KANs%20are%20promising%20alternatives%20for%0AMLPs%2C%20opening%20opportunities%20for%20further%20improving%20today%27s%20deep%20learning%20models%0Awhich%20rely%20heavily%20on%20MLPs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19756v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KAN%3A%20Kolmogorov-Arnold%20Networks&entry.906535625=Ziming%20Liu%20and%20Yixuan%20Wang%20and%20Sachin%20Vaidya%20and%20Fabian%20Ruehle%20and%20James%20Halverson%20and%20Marin%20Solja%C4%8Di%C4%87%20and%20Thomas%20Y.%20Hou%20and%20Max%20Tegmark&entry.1292438233=%20%20Inspired%20by%20the%20Kolmogorov-Arnold%20representation%20theorem%2C%20we%20propose%0AKolmogorov-Arnold%20Networks%20%28KANs%29%20as%20promising%20alternatives%20to%20Multi-Layer%0APerceptrons%20%28MLPs%29.%20While%20MLPs%20have%20fixed%20activation%20functions%20on%20nodes%0A%28%22neurons%22%29%2C%20KANs%20have%20learnable%20activation%20functions%20on%20edges%20%28%22weights%22%29.%0AKANs%20have%20no%20linear%20weights%20at%20all%20--%20every%20weight%20parameter%20is%20replaced%20by%20a%0Aunivariate%20function%20parametrized%20as%20a%20spline.%20We%20show%20that%20this%20seemingly%0Asimple%20change%20makes%20KANs%20outperform%20MLPs%20in%20terms%20of%20accuracy%20and%0Ainterpretability.%20For%20accuracy%2C%20much%20smaller%20KANs%20can%20achieve%20comparable%20or%0Abetter%20accuracy%20than%20much%20larger%20MLPs%20in%20data%20fitting%20and%20PDE%20solving.%0ATheoretically%20and%20empirically%2C%20KANs%20possess%20faster%20neural%20scaling%20laws%20than%0AMLPs.%20For%20interpretability%2C%20KANs%20can%20be%20intuitively%20visualized%20and%20can%20easily%0Ainteract%20with%20human%20users.%20Through%20two%20examples%20in%20mathematics%20and%20physics%2C%0AKANs%20are%20shown%20to%20be%20useful%20collaborators%20helping%20scientists%20%28re%29discover%0Amathematical%20and%20physical%20laws.%20In%20summary%2C%20KANs%20are%20promising%20alternatives%20for%0AMLPs%2C%20opening%20opportunities%20for%20further%20improving%20today%27s%20deep%20learning%20models%0Awhich%20rely%20heavily%20on%20MLPs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19756v2&entry.124074799=Read"},
{"title": "PARASOL: Parametric Style Control for Diffusion Image Synthesis", "author": "Gemma Canet Tarr\u00e9s and Dan Ruta and Tu Bui and John Collomosse", "abstract": "  We propose PARASOL, a multi-modal synthesis model that enables disentangled,\nparametric control of the visual style of the image by jointly conditioning\nsynthesis on both content and a fine-grained visual style embedding. We train a\nlatent diffusion model (LDM) using specific losses for each modality and adapt\nthe classifier-free guidance for encouraging disentangled control over\nindependent content and style modalities at inference time. We leverage\nauxiliary semantic and style-based search to create training triplets for\nsupervision of the LDM, ensuring complementarity of content and style cues.\nPARASOL shows promise for enabling nuanced control over visual style in\ndiffusion models for image creation and stylization, as well as generative\nsearch where text-based search results may be adapted to more closely match\nuser intent by interpolating both content and style descriptors.\n", "link": "http://arxiv.org/abs/2303.06464v3", "date": "2024-05-02", "relevancy": 1.1528, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6226}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5714}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5352}], "mailto": "mailto:daeR=997470421.yrtne&3v46460.3032/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.srotpircsed02%elyts02%dna02%tnetnoc02%htob02%gnitalopretni02%yb02%tnetni02%resuA0%hctam02%ylesolc02%erom02%ot02%detpada02%eb02%yam02%stluser02%hcraes02%desab-txet02%erehw02%hcraesA0%evitareneg02%sa02%llew02%sa02%C2%noitazilyts02%dna02%noitaerc02%egami02%rof02%sledom02%noisuffidA0%ni02%elyts02%lausiv02%revo02%lortnoc02%decnaun02%gnilbane02%rof02%esimorp02%swohs02%LOSARAPA0%.seuc02%elyts02%dna02%tnetnoc02%fo02%ytiratnemelpmoc02%gnirusne02%C2%MDL02%eht02%fo02%noisivrepusA0%rof02%stelpirt02%gniniart02%etaerc02%ot02%hcraes02%desab-elyts02%dna02%citnames02%yrailixuaA0%egarevel02%eW02%.emit02%ecnerefni02%ta02%seitiladom02%elyts02%dna02%tnetnoc02%tnednepedniA0%revo02%lortnoc02%delgnatnesid02%gnigaruocne02%rof02%ecnadiug02%eerf-reifissalc02%ehtA0%tpada02%dna02%ytiladom02%hcae02%rof02%sessol02%cificeps02%gnisu02%92%MDL82%02%ledom02%noisuffid02%tnetalA0%a02%niart02%eW02%.gniddebme02%elyts02%lausiv02%deniarg-enif02%a02%dna02%tnetnoc02%htob02%no02%sisehtnysA0%gninoitidnoc02%yltnioj02%yb02%egami02%eht02%fo02%elyts02%lausiv02%eht02%fo02%lortnoc02%cirtemarapA0%C2%delgnatnesid02%selbane02%taht02%ledom02%sisehtnys02%ladom-itlum02%a02%C2%LOSARAP02%esoporp02%eW02%02%=3328342921.yrtne&essomolloC02%nhoJ02%dna02%iuB02%uT02%dna02%atuR02%naD02%dna02%s9A%3C%rraT02%tenaC02%ammeG=526535609.yrtne&sisehtnyS02%egamI02%noisuffiD02%rof02%lortnoC02%elytS02%cirtemaraP02%A3%LOSARAP=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20PARASOL%3A%20Parametric%20Style%20Control%20for%20Diffusion%20Image%20Synthesis&body=Title%3A%20PARASOL%3A%20Parametric%20Style%20Control%20for%20Diffusion%20Image%20Synthesis%0AAuthor%3A%20Gemma%20Canet%20Tarr%C3%A9s%20and%20Dan%20Ruta%20and%20Tu%20Bui%20and%20John%20Collomosse%0AAbstract%3A%20%20%20We%20propose%20PARASOL%2C%20a%20multi-modal%20synthesis%20model%20that%20enables%20disentangled%2C%0Aparametric%20control%20of%20the%20visual%20style%20of%20the%20image%20by%20jointly%20conditioning%0Asynthesis%20on%20both%20content%20and%20a%20fine-grained%20visual%20style%20embedding.%20We%20train%20a%0Alatent%20diffusion%20model%20%28LDM%29%20using%20specific%20losses%20for%20each%20modality%20and%20adapt%0Athe%20classifier-free%20guidance%20for%20encouraging%20disentangled%20control%20over%0Aindependent%20content%20and%20style%20modalities%20at%20inference%20time.%20We%20leverage%0Aauxiliary%20semantic%20and%20style-based%20search%20to%20create%20training%20triplets%20for%0Asupervision%20of%20the%20LDM%2C%20ensuring%20complementarity%20of%20content%20and%20style%20cues.%0APARASOL%20shows%20promise%20for%20enabling%20nuanced%20control%20over%20visual%20style%20in%0Adiffusion%20models%20for%20image%20creation%20and%20stylization%2C%20as%20well%20as%20generative%0Asearch%20where%20text-based%20search%20results%20may%20be%20adapted%20to%20more%20closely%20match%0Auser%20intent%20by%20interpolating%20both%20content%20and%20style%20descriptors.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.06464v3%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PARASOL%3A%20Parametric%20Style%20Control%20for%20Diffusion%20Image%20Synthesis&entry.906535625=Gemma%20Canet%20Tarr%C3%A9s%20and%20Dan%20Ruta%20and%20Tu%20Bui%20and%20John%20Collomosse&entry.1292438233=%20%20We%20propose%20PARASOL%2C%20a%20multi-modal%20synthesis%20model%20that%20enables%20disentangled%2C%0Aparametric%20control%20of%20the%20visual%20style%20of%20the%20image%20by%20jointly%20conditioning%0Asynthesis%20on%20both%20content%20and%20a%20fine-grained%20visual%20style%20embedding.%20We%20train%20a%0Alatent%20diffusion%20model%20%28LDM%29%20using%20specific%20losses%20for%20each%20modality%20and%20adapt%0Athe%20classifier-free%20guidance%20for%20encouraging%20disentangled%20control%20over%0Aindependent%20content%20and%20style%20modalities%20at%20inference%20time.%20We%20leverage%0Aauxiliary%20semantic%20and%20style-based%20search%20to%20create%20training%20triplets%20for%0Asupervision%20of%20the%20LDM%2C%20ensuring%20complementarity%20of%20content%20and%20style%20cues.%0APARASOL%20shows%20promise%20for%20enabling%20nuanced%20control%20over%20visual%20style%20in%0Adiffusion%20models%20for%20image%20creation%20and%20stylization%2C%20as%20well%20as%20generative%0Asearch%20where%20text-based%20search%20results%20may%20be%20adapted%20to%20more%20closely%20match%0Auser%20intent%20by%20interpolating%20both%20content%20and%20style%20descriptors.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.06464v3&entry.124074799=Read"},
{"title": "Multigroup Robustness", "author": "Lunjia Hu and Charlotte Peale and Judy Hanwen Shen", "abstract": "  To address the shortcomings of real-world datasets, robust learning\nalgorithms have been designed to overcome arbitrary and indiscriminate data\ncorruption. However, practical processes of gathering data may lead to patterns\nof data corruption that are localized to specific partitions of the training\ndataset. Motivated by critical applications where the learned model is deployed\nto make predictions about people from a rich collection of overlapping\nsubpopulations, we initiate the study of multigroup robust algorithms whose\nrobustness guarantees for each subpopulation only degrade with the amount of\ndata corruption inside that subpopulation. When the data corruption is not\ndistributed uniformly over subpopulations, our algorithms provide more\nmeaningful robustness guarantees than standard guarantees that are oblivious to\nhow the data corruption and the affected subpopulations are related. Our\ntechniques establish a new connection between multigroup fairness and\nrobustness.\n", "link": "http://arxiv.org/abs/2405.00614v1", "date": "2024-05-01", "relevancy": 1.78, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4639}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4342}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4304}], "mailto": "mailto:daeR=997470421.yrtne&1v41600.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.ssentsuborA0%dna02%ssenriaf02%puorgitlum02%neewteb02%noitcennoc02%wen02%a02%hsilbatse02%seuqinhcetA0%ruO02%.detaler02%era02%snoitalupopbus02%detceffa02%eht02%dna02%noitpurroc02%atad02%eht02%wohA0%ot02%suoivilbo02%era02%taht02%seetnaraug02%dradnats02%naht02%seetnaraug02%ssentsubor02%lufgninaemA0%erom02%edivorp02%smhtirogla02%ruo02%C2%snoitalupopbus02%revo02%ylmrofinu02%detubirtsidA0%ton02%si02%noitpurroc02%atad02%eht02%nehW02%.noitalupopbus02%taht02%edisni02%noitpurroc02%atadA0%fo02%tnuoma02%eht02%htiw02%edarged02%ylno02%noitalupopbus02%hcae02%rof02%seetnaraug02%ssentsuborA0%esohw02%smhtirogla02%tsubor02%puorgitlum02%fo02%yduts02%eht02%etaitini02%ew02%C2%snoitalupopbusA0%gnippalrevo02%fo02%noitcelloc02%hcir02%a02%morf02%elpoep02%tuoba02%snoitciderp02%ekam02%otA0%deyolped02%si02%ledom02%denrael02%eht02%erehw02%snoitacilppa02%lacitirc02%yb02%detavitoM02%.tesatadA0%gniniart02%eht02%fo02%snoititrap02%cificeps02%ot02%dezilacol02%era02%taht02%noitpurroc02%atad02%foA0%snrettap02%ot02%dael02%yam02%atad02%gnirehtag02%fo02%sessecorp02%lacitcarp02%C2%revewoH02%.noitpurrocA0%atad02%etanimircsidni02%dna02%yrartibra02%emocrevo02%ot02%dengised02%neeb02%evah02%smhtiroglaA0%gninrael02%tsubor02%C2%stesatad02%dlrow-laer02%fo02%sgnimoctrohs02%eht02%sserdda02%oT02%02%=3328342921.yrtne&nehS02%newnaH02%yduJ02%dna02%elaeP02%ettolrahC02%dna02%uH02%aijnuL=526535609.yrtne&ssentsuboR02%puorgitluM=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Multigroup%20Robustness&body=Title%3A%20Multigroup%20Robustness%0AAuthor%3A%20Lunjia%20Hu%20and%20Charlotte%20Peale%20and%20Judy%20Hanwen%20Shen%0AAbstract%3A%20%20%20To%20address%20the%20shortcomings%20of%20real-world%20datasets%2C%20robust%20learning%0Aalgorithms%20have%20been%20designed%20to%20overcome%20arbitrary%20and%20indiscriminate%20data%0Acorruption.%20However%2C%20practical%20processes%20of%20gathering%20data%20may%20lead%20to%20patterns%0Aof%20data%20corruption%20that%20are%20localized%20to%20specific%20partitions%20of%20the%20training%0Adataset.%20Motivated%20by%20critical%20applications%20where%20the%20learned%20model%20is%20deployed%0Ato%20make%20predictions%20about%20people%20from%20a%20rich%20collection%20of%20overlapping%0Asubpopulations%2C%20we%20initiate%20the%20study%20of%20multigroup%20robust%20algorithms%20whose%0Arobustness%20guarantees%20for%20each%20subpopulation%20only%20degrade%20with%20the%20amount%20of%0Adata%20corruption%20inside%20that%20subpopulation.%20When%20the%20data%20corruption%20is%20not%0Adistributed%20uniformly%20over%20subpopulations%2C%20our%20algorithms%20provide%20more%0Ameaningful%20robustness%20guarantees%20than%20standard%20guarantees%20that%20are%20oblivious%20to%0Ahow%20the%20data%20corruption%20and%20the%20affected%20subpopulations%20are%20related.%20Our%0Atechniques%20establish%20a%20new%20connection%20between%20multigroup%20fairness%20and%0Arobustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00614v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multigroup%20Robustness&entry.906535625=Lunjia%20Hu%20and%20Charlotte%20Peale%20and%20Judy%20Hanwen%20Shen&entry.1292438233=%20%20To%20address%20the%20shortcomings%20of%20real-world%20datasets%2C%20robust%20learning%0Aalgorithms%20have%20been%20designed%20to%20overcome%20arbitrary%20and%20indiscriminate%20data%0Acorruption.%20However%2C%20practical%20processes%20of%20gathering%20data%20may%20lead%20to%20patterns%0Aof%20data%20corruption%20that%20are%20localized%20to%20specific%20partitions%20of%20the%20training%0Adataset.%20Motivated%20by%20critical%20applications%20where%20the%20learned%20model%20is%20deployed%0Ato%20make%20predictions%20about%20people%20from%20a%20rich%20collection%20of%20overlapping%0Asubpopulations%2C%20we%20initiate%20the%20study%20of%20multigroup%20robust%20algorithms%20whose%0Arobustness%20guarantees%20for%20each%20subpopulation%20only%20degrade%20with%20the%20amount%20of%0Adata%20corruption%20inside%20that%20subpopulation.%20When%20the%20data%20corruption%20is%20not%0Adistributed%20uniformly%20over%20subpopulations%2C%20our%20algorithms%20provide%20more%0Ameaningful%20robustness%20guarantees%20than%20standard%20guarantees%20that%20are%20oblivious%20to%0Ahow%20the%20data%20corruption%20and%20the%20affected%20subpopulations%20are%20related.%20Our%0Atechniques%20establish%20a%20new%20connection%20between%20multigroup%20fairness%20and%0Arobustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00614v1&entry.124074799=Read"},
{"title": "HANS, are you clever? Clever Hans Effect Analysis of Neural Systems", "author": "Leonardo Ranaldi and Fabio Massimo Zanzotto", "abstract": "  Instruction-tuned Large Language Models (It-LLMs) have been exhibiting\noutstanding abilities to reason around cognitive states, intentions, and\nreactions of all people involved, letting humans guide and comprehend\nday-to-day social interactions effectively. In fact, several multiple-choice\nquestions (MCQ) benchmarks have been proposed to construct solid assessments of\nthe models' abilities. However, earlier works are demonstrating the presence of\ninherent \"order bias\" in It-LLMs, posing challenges to the appropriate\nevaluation. In this paper, we investigate It-LLMs' resilience abilities towards\na series of probing tests using four MCQ benchmarks. Introducing adversarial\nexamples, we show a significant performance gap, mainly when varying the order\nof the choices, which reveals a selection bias and brings into discussion\nreasoning abilities. Following a correlation between first positions and model\nchoices due to positional bias, we hypothesized the presence of structural\nheuristics in the decision-making process of the It-LLMs, strengthened by\nincluding significant examples in few-shot scenarios. Finally, by using the\nChain-of-Thought (CoT) technique, we elicit the model to reason and mitigate\nthe bias by obtaining more robust models.\n", "link": "http://arxiv.org/abs/2309.12481v2", "date": "2024-05-02", "relevancy": 1.4495, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4972}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4815}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4733}], "mailto": "mailto:daeR=997470421.yrtne&2v18421.9032/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.sledom02%tsubor02%erom02%gniniatbo02%yb02%saib02%ehtA0%etagitim02%dna02%nosaer02%ot02%ledom02%eht02%ticile02%ew02%C2%euqinhcet02%92%ToC82%02%thguohT-fo-niahCA0%eht02%gnisu02%yb02%C2%yllaniF02%.soiranecs02%tohs-wef02%ni02%selpmaxe02%tnacifingis02%gnidulcniA0%yb02%denehtgnerts02%C2%sMLL-tI02%eht02%fo02%ssecorp02%gnikam-noisiced02%eht02%ni02%scitsiruehA0%larutcurts02%fo02%ecneserp02%eht02%dezisehtopyh02%ew02%C2%saib02%lanoitisop02%ot02%eud02%seciohcA0%ledom02%dna02%snoitisop02%tsrif02%neewteb02%noitalerroc02%a02%gniwolloF02%.seitiliba02%gninosaerA0%noissucsid02%otni02%sgnirb02%dna02%saib02%noitceles02%a02%slaever02%hcihw02%C2%seciohc02%eht02%foA0%redro02%eht02%gniyrav02%nehw02%ylniam02%C2%pag02%ecnamrofrep02%tnacifingis02%a02%wohs02%ew02%C2%selpmaxeA0%lairasrevda02%gnicudortnI02%.skramhcneb02%QCM02%ruof02%gnisu02%stset02%gniborp02%fo02%seires02%aA0%sdrawot02%seitiliba02%ecneiliser02%72%sMLL-tI02%etagitsevni02%ew02%C2%repap02%siht02%nI02%.noitaulaveA0%etairporppa02%eht02%ot02%segnellahc02%gnisop02%C2%sMLL-tI02%ni02%22%saib02%redro22%02%tnerehniA0%fo02%ecneserp02%eht02%gnitartsnomed02%era02%skrow02%reilrae02%C2%revewoH02%.seitiliba02%72%sledom02%ehtA0%fo02%stnemssessa02%dilos02%tcurtsnoc02%ot02%desoporp02%neeb02%evah02%skramhcneb02%92%QCM82%02%snoitseuqA0%eciohc-elpitlum02%lareves02%C2%tcaf02%nI02%.ylevitceffe02%snoitcaretni02%laicos02%yad-ot-yadA0%dneherpmoc02%dna02%ediug02%snamuh02%gnittel02%C2%devlovni02%elpoep02%lla02%fo02%snoitcaerA0%dna02%C2%snoitnetni02%C2%setats02%evitingoc02%dnuora02%nosaer02%ot02%seitiliba02%gnidnatstuoA0%gnitibihxe02%neeb02%evah02%92%sMLL-tI82%02%sledoM02%egaugnaL02%egraL02%denut-noitcurtsnI02%02%=3328342921.yrtne&ottoznaZ02%omissaM02%oibaF02%dna02%idlanaR02%odranoeL=526535609.yrtne&smetsyS02%larueN02%fo02%sisylanA02%tceffE02%snaH02%revelC02%F3%revelc02%uoy02%era02%C2%SNAH=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20HANS%2C%20are%20you%20clever%3F%20Clever%20Hans%20Effect%20Analysis%20of%20Neural%20Systems&body=Title%3A%20HANS%2C%20are%20you%20clever%3F%20Clever%20Hans%20Effect%20Analysis%20of%20Neural%20Systems%0AAuthor%3A%20Leonardo%20Ranaldi%20and%20Fabio%20Massimo%20Zanzotto%0AAbstract%3A%20%20%20Instruction-tuned%20Large%20Language%20Models%20%28It-LLMs%29%20have%20been%20exhibiting%0Aoutstanding%20abilities%20to%20reason%20around%20cognitive%20states%2C%20intentions%2C%20and%0Areactions%20of%20all%20people%20involved%2C%20letting%20humans%20guide%20and%20comprehend%0Aday-to-day%20social%20interactions%20effectively.%20In%20fact%2C%20several%20multiple-choice%0Aquestions%20%28MCQ%29%20benchmarks%20have%20been%20proposed%20to%20construct%20solid%20assessments%20of%0Athe%20models%27%20abilities.%20However%2C%20earlier%20works%20are%20demonstrating%20the%20presence%20of%0Ainherent%20%22order%20bias%22%20in%20It-LLMs%2C%20posing%20challenges%20to%20the%20appropriate%0Aevaluation.%20In%20this%20paper%2C%20we%20investigate%20It-LLMs%27%20resilience%20abilities%20towards%0Aa%20series%20of%20probing%20tests%20using%20four%20MCQ%20benchmarks.%20Introducing%20adversarial%0Aexamples%2C%20we%20show%20a%20significant%20performance%20gap%2C%20mainly%20when%20varying%20the%20order%0Aof%20the%20choices%2C%20which%20reveals%20a%20selection%20bias%20and%20brings%20into%20discussion%0Areasoning%20abilities.%20Following%20a%20correlation%20between%20first%20positions%20and%20model%0Achoices%20due%20to%20positional%20bias%2C%20we%20hypothesized%20the%20presence%20of%20structural%0Aheuristics%20in%20the%20decision-making%20process%20of%20the%20It-LLMs%2C%20strengthened%20by%0Aincluding%20significant%20examples%20in%20few-shot%20scenarios.%20Finally%2C%20by%20using%20the%0AChain-of-Thought%20%28CoT%29%20technique%2C%20we%20elicit%20the%20model%20to%20reason%20and%20mitigate%0Athe%20bias%20by%20obtaining%20more%20robust%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.12481v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HANS%2C%20are%20you%20clever%3F%20Clever%20Hans%20Effect%20Analysis%20of%20Neural%20Systems&entry.906535625=Leonardo%20Ranaldi%20and%20Fabio%20Massimo%20Zanzotto&entry.1292438233=%20%20Instruction-tuned%20Large%20Language%20Models%20%28It-LLMs%29%20have%20been%20exhibiting%0Aoutstanding%20abilities%20to%20reason%20around%20cognitive%20states%2C%20intentions%2C%20and%0Areactions%20of%20all%20people%20involved%2C%20letting%20humans%20guide%20and%20comprehend%0Aday-to-day%20social%20interactions%20effectively.%20In%20fact%2C%20several%20multiple-choice%0Aquestions%20%28MCQ%29%20benchmarks%20have%20been%20proposed%20to%20construct%20solid%20assessments%20of%0Athe%20models%27%20abilities.%20However%2C%20earlier%20works%20are%20demonstrating%20the%20presence%20of%0Ainherent%20%22order%20bias%22%20in%20It-LLMs%2C%20posing%20challenges%20to%20the%20appropriate%0Aevaluation.%20In%20this%20paper%2C%20we%20investigate%20It-LLMs%27%20resilience%20abilities%20towards%0Aa%20series%20of%20probing%20tests%20using%20four%20MCQ%20benchmarks.%20Introducing%20adversarial%0Aexamples%2C%20we%20show%20a%20significant%20performance%20gap%2C%20mainly%20when%20varying%20the%20order%0Aof%20the%20choices%2C%20which%20reveals%20a%20selection%20bias%20and%20brings%20into%20discussion%0Areasoning%20abilities.%20Following%20a%20correlation%20between%20first%20positions%20and%20model%0Achoices%20due%20to%20positional%20bias%2C%20we%20hypothesized%20the%20presence%20of%20structural%0Aheuristics%20in%20the%20decision-making%20process%20of%20the%20It-LLMs%2C%20strengthened%20by%0Aincluding%20significant%20examples%20in%20few-shot%20scenarios.%20Finally%2C%20by%20using%20the%0AChain-of-Thought%20%28CoT%29%20technique%2C%20we%20elicit%20the%20model%20to%20reason%20and%20mitigate%0Athe%20bias%20by%20obtaining%20more%20robust%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.12481v2&entry.124074799=Read"},
{"title": "CPLLM: Clinical Prediction with Large Language Models", "author": "Ofir Ben Shoham and Nadav Rappoport", "abstract": "  We present Clinical Prediction with Large Language Models (CPLLM), a method\nthat involves fine-tuning a pre-trained Large Language Model (LLM) for clinical\ndisease and readmission prediction. We utilized quantization and fine-tuned the\nLLM using prompts. For diagnosis prediction, we predict whether patients will\nbe diagnosed with a target disease during their next visit or in the subsequent\ndiagnosis, leveraging their historical diagnosis records. We compared our\nresults to various baselines, including RETAIN, and Med-BERT, the current\nstate-of-the-art model for disease prediction using temporal structured EHR\ndata. In addition, We also evaluated CPLLM for patient hospital readmission\nprediction and compared our method's performance with benchmark baselines. Our\nexperiments have shown that our proposed method, CPLLM, surpasses all the\ntested models in terms of PR-AUC and ROC-AUC metrics, showing state-of-the-art\nresults for diagnosis prediction and patient hospital readmission prediction.\nSuch a method can be easily implemented and integrated into the clinical\nprocess to help care providers estimate the next steps of patients\n", "link": "http://arxiv.org/abs/2309.11295v2", "date": "2024-05-02", "relevancy": 1.3724, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4738}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.458}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4397}], "mailto": "mailto:daeR=997470421.yrtne&2v59211.9032/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%stneitap02%fo02%spets02%txen02%eht02%etamitse02%sredivorp02%erac02%pleh02%ot02%ssecorpA0%lacinilc02%eht02%otni02%detargetni02%dna02%detnemelpmi02%ylisae02%eb02%nac02%dohtem02%a02%hcuSA0%.noitciderp02%noissimdaer02%latipsoh02%tneitap02%dna02%noitciderp02%sisongaid02%rof02%stluserA0%tra-eht-fo-etats02%gniwohs02%C2%scirtem02%CUA-COR02%dna02%CUA-RP02%fo02%smret02%ni02%sledom02%detsetA0%eht02%lla02%sessaprus02%C2%MLLPC02%C2%dohtem02%desoporp02%ruo02%taht02%nwohs02%evah02%stnemirepxeA0%ruO02%.senilesab02%kramhcneb02%htiw02%ecnamrofrep02%s72%dohtem02%ruo02%derapmoc02%dna02%noitciderpA0%noissimdaer02%latipsoh02%tneitap02%rof02%MLLPC02%detaulave02%osla02%eW02%C2%noitidda02%nI02%.atadA0%RHE02%derutcurts02%laropmet02%gnisu02%noitciderp02%esaesid02%rof02%ledom02%tra-eht-fo-etatsA0%tnerruc02%eht02%C2%TREB-deM02%dna02%C2%NIATER02%gnidulcni02%C2%senilesab02%suoirav02%ot02%stluserA0%ruo02%derapmoc02%eW02%.sdrocer02%sisongaid02%lacirotsih02%rieht02%gnigarevel02%C2%sisongaidA0%tneuqesbus02%eht02%ni02%ro02%tisiv02%txen02%rieht02%gnirud02%esaesid02%tegrat02%a02%htiw02%desongaid02%ebA0%lliw02%stneitap02%rehtehw02%tciderp02%ew02%C2%noitciderp02%sisongaid02%roF02%.stpmorp02%gnisu02%MLLA0%eht02%denut-enif02%dna02%noitazitnauq02%dezilitu02%eW02%.noitciderp02%noissimdaer02%dna02%esaesidA0%lacinilc02%rof02%92%MLL82%02%ledoM02%egaugnaL02%egraL02%deniart-erp02%a02%gninut-enif02%sevlovni02%tahtA0%dohtem02%a02%C2%92%MLLPC82%02%sledoM02%egaugnaL02%egraL02%htiw02%noitciderP02%lacinilC02%tneserp02%eW02%02%=3328342921.yrtne&tropoppaR02%vadaN02%dna02%mahohS02%neB02%rifO=526535609.yrtne&sledoM02%egaugnaL02%egraL02%htiw02%noitciderP02%lacinilC02%A3%MLLPC=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20CPLLM%3A%20Clinical%20Prediction%20with%20Large%20Language%20Models&body=Title%3A%20CPLLM%3A%20Clinical%20Prediction%20with%20Large%20Language%20Models%0AAuthor%3A%20Ofir%20Ben%20Shoham%20and%20Nadav%20Rappoport%0AAbstract%3A%20%20%20We%20present%20Clinical%20Prediction%20with%20Large%20Language%20Models%20%28CPLLM%29%2C%20a%20method%0Athat%20involves%20fine-tuning%20a%20pre-trained%20Large%20Language%20Model%20%28LLM%29%20for%20clinical%0Adisease%20and%20readmission%20prediction.%20We%20utilized%20quantization%20and%20fine-tuned%20the%0ALLM%20using%20prompts.%20For%20diagnosis%20prediction%2C%20we%20predict%20whether%20patients%20will%0Abe%20diagnosed%20with%20a%20target%20disease%20during%20their%20next%20visit%20or%20in%20the%20subsequent%0Adiagnosis%2C%20leveraging%20their%20historical%20diagnosis%20records.%20We%20compared%20our%0Aresults%20to%20various%20baselines%2C%20including%20RETAIN%2C%20and%20Med-BERT%2C%20the%20current%0Astate-of-the-art%20model%20for%20disease%20prediction%20using%20temporal%20structured%20EHR%0Adata.%20In%20addition%2C%20We%20also%20evaluated%20CPLLM%20for%20patient%20hospital%20readmission%0Aprediction%20and%20compared%20our%20method%27s%20performance%20with%20benchmark%20baselines.%20Our%0Aexperiments%20have%20shown%20that%20our%20proposed%20method%2C%20CPLLM%2C%20surpasses%20all%20the%0Atested%20models%20in%20terms%20of%20PR-AUC%20and%20ROC-AUC%20metrics%2C%20showing%20state-of-the-art%0Aresults%20for%20diagnosis%20prediction%20and%20patient%20hospital%20readmission%20prediction.%0ASuch%20a%20method%20can%20be%20easily%20implemented%20and%20integrated%20into%20the%20clinical%0Aprocess%20to%20help%20care%20providers%20estimate%20the%20next%20steps%20of%20patients%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.11295v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CPLLM%3A%20Clinical%20Prediction%20with%20Large%20Language%20Models&entry.906535625=Ofir%20Ben%20Shoham%20and%20Nadav%20Rappoport&entry.1292438233=%20%20We%20present%20Clinical%20Prediction%20with%20Large%20Language%20Models%20%28CPLLM%29%2C%20a%20method%0Athat%20involves%20fine-tuning%20a%20pre-trained%20Large%20Language%20Model%20%28LLM%29%20for%20clinical%0Adisease%20and%20readmission%20prediction.%20We%20utilized%20quantization%20and%20fine-tuned%20the%0ALLM%20using%20prompts.%20For%20diagnosis%20prediction%2C%20we%20predict%20whether%20patients%20will%0Abe%20diagnosed%20with%20a%20target%20disease%20during%20their%20next%20visit%20or%20in%20the%20subsequent%0Adiagnosis%2C%20leveraging%20their%20historical%20diagnosis%20records.%20We%20compared%20our%0Aresults%20to%20various%20baselines%2C%20including%20RETAIN%2C%20and%20Med-BERT%2C%20the%20current%0Astate-of-the-art%20model%20for%20disease%20prediction%20using%20temporal%20structured%20EHR%0Adata.%20In%20addition%2C%20We%20also%20evaluated%20CPLLM%20for%20patient%20hospital%20readmission%0Aprediction%20and%20compared%20our%20method%27s%20performance%20with%20benchmark%20baselines.%20Our%0Aexperiments%20have%20shown%20that%20our%20proposed%20method%2C%20CPLLM%2C%20surpasses%20all%20the%0Atested%20models%20in%20terms%20of%20PR-AUC%20and%20ROC-AUC%20metrics%2C%20showing%20state-of-the-art%0Aresults%20for%20diagnosis%20prediction%20and%20patient%20hospital%20readmission%20prediction.%0ASuch%20a%20method%20can%20be%20easily%20implemented%20and%20integrated%20into%20the%20clinical%0Aprocess%20to%20help%20care%20providers%20estimate%20the%20next%20steps%20of%20patients%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.11295v2&entry.124074799=Read"},
{"title": "Probabilistic Sampling of Balanced K-Means using Adiabatic Quantum\n  Computing", "author": "Jan-Nico Zaech and Martin Danelljan and Tolga Birdal and Luc Van Gool", "abstract": "  Adiabatic quantum computing (AQC) is a promising approach for discrete and\noften NP-hard optimization problems. Current AQCs allow to implement problems\nof research interest, which has sparked the development of quantum\nrepresentations for many computer vision tasks. Despite requiring multiple\nmeasurements from the noisy AQC, current approaches only utilize the best\nmeasurement, discarding information contained in the remaining ones. In this\nwork, we explore the potential of using this information for probabilistic\nbalanced k-means clustering. Instead of discarding non-optimal solutions, we\npropose to use them to compute calibrated posterior probabilities with little\nadditional compute cost. This allows us to identify ambiguous solutions and\ndata points, which we demonstrate on a D-Wave AQC on synthetic tasks and real\nvisual data.\n", "link": "http://arxiv.org/abs/2310.12153v2", "date": "2024-05-01", "relevancy": 1.9392, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5021}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4945}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4682}], "mailto": "mailto:daeR=997470421.yrtne&2v35121.0132/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.atad02%lausivA0%laer02%dna02%sksat02%citehtnys02%no02%CQA02%evaW-D02%a02%no02%etartsnomed02%ew02%hcihw02%C2%stniop02%atadA0%dna02%snoitulos02%suougibma02%yfitnedi02%ot02%su02%swolla02%sihT02%.tsoc02%etupmoc02%lanoitiddaA0%elttil02%htiw02%seitilibaborp02%roiretsop02%detarbilac02%etupmoc02%ot02%meht02%esu02%ot02%esoporpA0%ew02%C2%snoitulos02%lamitpo-non02%gnidracsid02%fo02%daetsnI02%.gniretsulc02%snaem-k02%decnalabA0%citsilibaborp02%rof02%noitamrofni02%siht02%gnisu02%fo02%laitnetop02%eht02%erolpxe02%ew02%C2%krowA0%siht02%nI02%.seno02%gniniamer02%eht02%ni02%deniatnoc02%noitamrofni02%gnidracsid02%C2%tnemerusaemA0%tseb02%eht02%ezilitu02%ylno02%sehcaorppa02%tnerruc02%C2%CQA02%ysion02%eht02%morf02%stnemerusaemA0%elpitlum02%gniriuqer02%etipseD02%.sksat02%noisiv02%retupmoc02%ynam02%rof02%snoitatneserperA0%mutnauq02%fo02%tnempoleved02%eht02%dekraps02%sah02%hcihw02%C2%tseretni02%hcraeser02%foA0%smelborp02%tnemelpmi02%ot02%wolla02%sCQA02%tnerruC02%.smelborp02%noitazimitpo02%drah-PN02%netfoA0%dna02%etercsid02%rof02%hcaorppa02%gnisimorp02%a02%si02%92%CQA82%02%gnitupmoc02%mutnauq02%citabaidA02%02%=3328342921.yrtne&looG02%naV02%cuL02%dna02%ladriB02%agloT02%dna02%najllenaD02%nitraM02%dna02%hceaZ02%ociN-naJ=526535609.yrtne&gnitupmoC02%02%A0%mutnauQ02%citabaidA02%gnisu02%snaeM-K02%decnalaB02%fo02%gnilpmaS02%citsilibaborP=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Probabilistic%20Sampling%20of%20Balanced%20K-Means%20using%20Adiabatic%20Quantum%0A%20%20Computing&body=Title%3A%20Probabilistic%20Sampling%20of%20Balanced%20K-Means%20using%20Adiabatic%20Quantum%0A%20%20Computing%0AAuthor%3A%20Jan-Nico%20Zaech%20and%20Martin%20Danelljan%20and%20Tolga%20Birdal%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%20Adiabatic%20quantum%20computing%20%28AQC%29%20is%20a%20promising%20approach%20for%20discrete%20and%0Aoften%20NP-hard%20optimization%20problems.%20Current%20AQCs%20allow%20to%20implement%20problems%0Aof%20research%20interest%2C%20which%20has%20sparked%20the%20development%20of%20quantum%0Arepresentations%20for%20many%20computer%20vision%20tasks.%20Despite%20requiring%20multiple%0Ameasurements%20from%20the%20noisy%20AQC%2C%20current%20approaches%20only%20utilize%20the%20best%0Ameasurement%2C%20discarding%20information%20contained%20in%20the%20remaining%20ones.%20In%20this%0Awork%2C%20we%20explore%20the%20potential%20of%20using%20this%20information%20for%20probabilistic%0Abalanced%20k-means%20clustering.%20Instead%20of%20discarding%20non-optimal%20solutions%2C%20we%0Apropose%20to%20use%20them%20to%20compute%20calibrated%20posterior%20probabilities%20with%20little%0Aadditional%20compute%20cost.%20This%20allows%20us%20to%20identify%20ambiguous%20solutions%20and%0Adata%20points%2C%20which%20we%20demonstrate%20on%20a%20D-Wave%20AQC%20on%20synthetic%20tasks%20and%20real%0Avisual%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12153v2%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Probabilistic%20Sampling%20of%20Balanced%20K-Means%20using%20Adiabatic%20Quantum%0A%20%20Computing&entry.906535625=Jan-Nico%20Zaech%20and%20Martin%20Danelljan%20and%20Tolga%20Birdal%20and%20Luc%20Van%20Gool&entry.1292438233=%20%20Adiabatic%20quantum%20computing%20%28AQC%29%20is%20a%20promising%20approach%20for%20discrete%20and%0Aoften%20NP-hard%20optimization%20problems.%20Current%20AQCs%20allow%20to%20implement%20problems%0Aof%20research%20interest%2C%20which%20has%20sparked%20the%20development%20of%20quantum%0Arepresentations%20for%20many%20computer%20vision%20tasks.%20Despite%20requiring%20multiple%0Ameasurements%20from%20the%20noisy%20AQC%2C%20current%20approaches%20only%20utilize%20the%20best%0Ameasurement%2C%20discarding%20information%20contained%20in%20the%20remaining%20ones.%20In%20this%0Awork%2C%20we%20explore%20the%20potential%20of%20using%20this%20information%20for%20probabilistic%0Abalanced%20k-means%20clustering.%20Instead%20of%20discarding%20non-optimal%20solutions%2C%20we%0Apropose%20to%20use%20them%20to%20compute%20calibrated%20posterior%20probabilities%20with%20little%0Aadditional%20compute%20cost.%20This%20allows%20us%20to%20identify%20ambiguous%20solutions%20and%0Adata%20points%2C%20which%20we%20demonstrate%20on%20a%20D-Wave%20AQC%20on%20synthetic%20tasks%20and%20real%0Avisual%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12153v2&entry.124074799=Read"},
{"title": "Swarm Learning: A Survey of Concepts, Applications, and Trends", "author": "Elham Shammar and Xiaohui Cui and Mohammed A. A. Al-qaness", "abstract": "  Deep learning models have raised privacy and security concerns due to their\nreliance on large datasets on central servers. As the number of Internet of\nThings (IoT) devices increases, artificial intelligence (AI) will be crucial\nfor resource management, data processing, and knowledge acquisition. To address\nthose issues, federated learning (FL) has introduced a novel approach to\nbuilding a versatile, large-scale machine learning framework that operates in a\ndecentralized and hardware-agnostic manner. However, FL faces network bandwidth\nlimitations and data breaches. To reduce the central dependency in FL and\nincrease scalability, swarm learning (SL) has been proposed in collaboration\nwith Hewlett Packard Enterprise (HPE). SL represents a decentralized machine\nlearning framework that leverages blockchain technology for secure, scalable,\nand private data management. A blockchain-based network enables the exchange\nand aggregation of model parameters among participants, thus mitigating the\nrisk of a single point of failure and eliminating communication bottlenecks. To\nthe best of our knowledge, this survey is the first to introduce the principles\nof Swarm Learning, its architectural design, and its fields of application. In\naddition, it highlights numerous research avenues that require further\nexploration by academic and industry communities to unlock the full potential\nand applications of SL.\n", "link": "http://arxiv.org/abs/2405.00556v1", "date": "2024-05-01", "relevancy": 2.0753, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4401}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4055}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3996}], "mailto": "mailto:daeR=997470421.yrtne&1v65500.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.LS02%fo02%snoitacilppa02%dnaA0%laitnetop02%lluf02%eht02%kcolnu02%ot02%seitinummoc02%yrtsudni02%dna02%cimedaca02%yb02%noitarolpxeA0%rehtruf02%eriuqer02%taht02%seuneva02%hcraeser02%suoremun02%sthgilhgih02%ti02%C2%noitiddaA0%nI02%.noitacilppa02%fo02%sdleif02%sti02%dna02%C2%ngised02%larutcetihcra02%sti02%C2%gninraeL02%mrawS02%foA0%selpicnirp02%eht02%ecudortni02%ot02%tsrif02%eht02%si02%yevrus02%siht02%C2%egdelwonk02%ruo02%fo02%tseb02%ehtA0%oT02%.skcenelttob02%noitacinummoc02%gnitanimile02%dna02%eruliaf02%fo02%tniop02%elgnis02%a02%fo02%ksirA0%eht02%gnitagitim02%suht02%C2%stnapicitrap02%gnoma02%sretemarap02%ledom02%fo02%noitagergga02%dnaA0%egnahcxe02%eht02%selbane02%krowten02%desab-niahckcolb02%A02%.tnemeganam02%atad02%etavirp02%dnaA0%C2%elbalacs02%C2%eruces02%rof02%ygolonhcet02%niahckcolb02%segarevel02%taht02%krowemarf02%gninraelA0%enihcam02%dezilartneced02%a02%stneserper02%LS02%.92%EPH82%02%esirpretnE02%drakcaP02%ttelweH02%htiwA0%noitaroballoc02%ni02%desoporp02%neeb02%sah02%92%LS82%02%gninrael02%mraws02%C2%ytilibalacs02%esaercniA0%dna02%LF02%ni02%ycnedneped02%lartnec02%eht02%ecuder02%oT02%.sehcaerb02%atad02%dna02%snoitatimilA0%htdiwdnab02%krowten02%secaf02%LF02%C2%revewoH02%.rennam02%citsonga-erawdrah02%dna02%dezilartnecedA0%a02%ni02%setarepo02%taht02%krowemarf02%gninrael02%enihcam02%elacs-egral02%C2%elitasrev02%a02%gnidliubA0%ot02%hcaorppa02%levon02%a02%decudortni02%sah02%92%LF82%02%gninrael02%detaredef02%C2%seussi02%esohtA0%sserdda02%oT02%.noitisiuqca02%egdelwonk02%dna02%C2%gnissecorp02%atad02%C2%tnemeganam02%ecruoser02%rofA0%laicurc02%eb02%lliw02%92%IA82%02%ecnegilletni02%laicifitra02%C2%sesaercni02%secived02%92%ToI82%02%sgnihTA0%fo02%tenretnI02%fo02%rebmun02%eht02%sA02%.srevres02%lartnec02%no02%stesatad02%egral02%no02%ecnailerA0%rieht02%ot02%eud02%snrecnoc02%ytiruces02%dna02%ycavirp02%desiar02%evah02%sledom02%gninrael02%peeD02%02%=3328342921.yrtne&ssenaq-lA02%.A02%.A02%demmahoM02%dna02%iuC02%iuhoaiX02%dna02%rammahS02%mahlE=526535609.yrtne&sdnerT02%dna02%C2%snoitacilppA02%C2%stpecnoC02%fo02%yevruS02%A02%A3%gninraeL02%mrawS=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Swarm%20Learning%3A%20A%20Survey%20of%20Concepts%2C%20Applications%2C%20and%20Trends&body=Title%3A%20Swarm%20Learning%3A%20A%20Survey%20of%20Concepts%2C%20Applications%2C%20and%20Trends%0AAuthor%3A%20Elham%20Shammar%20and%20Xiaohui%20Cui%20and%20Mohammed%20A.%20A.%20Al-qaness%0AAbstract%3A%20%20%20Deep%20learning%20models%20have%20raised%20privacy%20and%20security%20concerns%20due%20to%20their%0Areliance%20on%20large%20datasets%20on%20central%20servers.%20As%20the%20number%20of%20Internet%20of%0AThings%20%28IoT%29%20devices%20increases%2C%20artificial%20intelligence%20%28AI%29%20will%20be%20crucial%0Afor%20resource%20management%2C%20data%20processing%2C%20and%20knowledge%20acquisition.%20To%20address%0Athose%20issues%2C%20federated%20learning%20%28FL%29%20has%20introduced%20a%20novel%20approach%20to%0Abuilding%20a%20versatile%2C%20large-scale%20machine%20learning%20framework%20that%20operates%20in%20a%0Adecentralized%20and%20hardware-agnostic%20manner.%20However%2C%20FL%20faces%20network%20bandwidth%0Alimitations%20and%20data%20breaches.%20To%20reduce%20the%20central%20dependency%20in%20FL%20and%0Aincrease%20scalability%2C%20swarm%20learning%20%28SL%29%20has%20been%20proposed%20in%20collaboration%0Awith%20Hewlett%20Packard%20Enterprise%20%28HPE%29.%20SL%20represents%20a%20decentralized%20machine%0Alearning%20framework%20that%20leverages%20blockchain%20technology%20for%20secure%2C%20scalable%2C%0Aand%20private%20data%20management.%20A%20blockchain-based%20network%20enables%20the%20exchange%0Aand%20aggregation%20of%20model%20parameters%20among%20participants%2C%20thus%20mitigating%20the%0Arisk%20of%20a%20single%20point%20of%20failure%20and%20eliminating%20communication%20bottlenecks.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20survey%20is%20the%20first%20to%20introduce%20the%20principles%0Aof%20Swarm%20Learning%2C%20its%20architectural%20design%2C%20and%20its%20fields%20of%20application.%20In%0Aaddition%2C%20it%20highlights%20numerous%20research%20avenues%20that%20require%20further%0Aexploration%20by%20academic%20and%20industry%20communities%20to%20unlock%20the%20full%20potential%0Aand%20applications%20of%20SL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00556v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Swarm%20Learning%3A%20A%20Survey%20of%20Concepts%2C%20Applications%2C%20and%20Trends&entry.906535625=Elham%20Shammar%20and%20Xiaohui%20Cui%20and%20Mohammed%20A.%20A.%20Al-qaness&entry.1292438233=%20%20Deep%20learning%20models%20have%20raised%20privacy%20and%20security%20concerns%20due%20to%20their%0Areliance%20on%20large%20datasets%20on%20central%20servers.%20As%20the%20number%20of%20Internet%20of%0AThings%20%28IoT%29%20devices%20increases%2C%20artificial%20intelligence%20%28AI%29%20will%20be%20crucial%0Afor%20resource%20management%2C%20data%20processing%2C%20and%20knowledge%20acquisition.%20To%20address%0Athose%20issues%2C%20federated%20learning%20%28FL%29%20has%20introduced%20a%20novel%20approach%20to%0Abuilding%20a%20versatile%2C%20large-scale%20machine%20learning%20framework%20that%20operates%20in%20a%0Adecentralized%20and%20hardware-agnostic%20manner.%20However%2C%20FL%20faces%20network%20bandwidth%0Alimitations%20and%20data%20breaches.%20To%20reduce%20the%20central%20dependency%20in%20FL%20and%0Aincrease%20scalability%2C%20swarm%20learning%20%28SL%29%20has%20been%20proposed%20in%20collaboration%0Awith%20Hewlett%20Packard%20Enterprise%20%28HPE%29.%20SL%20represents%20a%20decentralized%20machine%0Alearning%20framework%20that%20leverages%20blockchain%20technology%20for%20secure%2C%20scalable%2C%0Aand%20private%20data%20management.%20A%20blockchain-based%20network%20enables%20the%20exchange%0Aand%20aggregation%20of%20model%20parameters%20among%20participants%2C%20thus%20mitigating%20the%0Arisk%20of%20a%20single%20point%20of%20failure%20and%20eliminating%20communication%20bottlenecks.%20To%0Athe%20best%20of%20our%20knowledge%2C%20this%20survey%20is%20the%20first%20to%20introduce%20the%20principles%0Aof%20Swarm%20Learning%2C%20its%20architectural%20design%2C%20and%20its%20fields%20of%20application.%20In%0Aaddition%2C%20it%20highlights%20numerous%20research%20avenues%20that%20require%20further%0Aexploration%20by%20academic%20and%20industry%20communities%20to%20unlock%20the%20full%20potential%0Aand%20applications%20of%20SL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00556v1&entry.124074799=Read"},
{"title": "LLM Inference Unveiled: Survey and Roofline Model Insights", "author": "Zhihang Yuan and Yuzhang Shang and Yang Zhou and Zhen Dong and Zhe Zhou and Chenhao Xue and Bingzhe Wu and Zhikai Li and Qingyi Gu and Yong Jae Lee and Yan Yan and Beidi Chen and Guangyu Sun and Kurt Keutzer", "abstract": "  The field of efficient Large Language Model (LLM) inference is rapidly\nevolving, presenting a unique blend of opportunities and challenges. Although\nthe field has expanded and is vibrant, there hasn't been a concise framework\nthat analyzes the various methods of LLM Inference to provide a clear\nunderstanding of this domain. Our survey stands out from traditional literature\nreviews by not only summarizing the current state of research but also by\nintroducing a framework based on roofline model for systematic analysis of LLM\ninference techniques. This framework identifies the bottlenecks when deploying\nLLMs on hardware devices and provides a clear understanding of practical\nproblems, such as why LLMs are memory-bound, how much memory and computation\nthey need, and how to choose the right hardware. We systematically collate the\nlatest advancements in efficient LLM inference, covering crucial areas such as\nmodel compression (e.g., Knowledge Distillation and Quantization), algorithm\nimprovements (e.g., Early Exit and Mixture-of-Expert), and both hardware and\nsystem-level enhancements. Our survey stands out by analyzing these methods\nwith roofline model, helping us understand their impact on memory access and\ncomputation. This distinctive approach not only showcases the current research\nlandscape but also delivers valuable insights for practical implementation,\npositioning our work as an indispensable resource for researchers new to the\nfield as well as for those seeking to deepen their understanding of efficient\nLLM deployment. The analyze tool, LLM-Viewer, is open-sourced.\n", "link": "http://arxiv.org/abs/2402.16363v6", "date": "2024-05-01", "relevancy": 1.9733, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4969}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4912}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4906}], "mailto": "mailto:daeR=997470421.yrtne&6v36361.2042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.decruos-nepo02%si02%C2%reweiV-MLL02%C2%loot02%ezylana02%ehT02%.tnemyolped02%MLLA0%tneiciffe02%fo02%gnidnatsrednu02%rieht02%nepeed02%ot02%gnikees02%esoht02%rof02%sa02%llew02%sa02%dleifA0%eht02%ot02%wen02%srehcraeser02%rof02%ecruoser02%elbasnepsidni02%na02%sa02%krow02%ruo02%gninoitisopA0%C2%noitatnemelpmi02%lacitcarp02%rof02%sthgisni02%elbaulav02%sreviled02%osla02%tub02%epacsdnalA0%hcraeser02%tnerruc02%eht02%sesacwohs02%ylno02%ton02%hcaorppa02%evitcnitsid02%sihT02%.noitatupmocA0%dna02%ssecca02%yromem02%no02%tcapmi02%rieht02%dnatsrednu02%su02%gnipleh02%C2%ledom02%enilfoor02%htiwA0%sdohtem02%eseht02%gnizylana02%yb02%tuo02%sdnats02%yevrus02%ruO02%.stnemecnahne02%level-metsysA0%dna02%erawdrah02%htob02%dna02%C2%92%trepxE-fo-erutxiM02%dna02%tixE02%ylraE02%C2%.g.e82%02%stnemevorpmiA0%mhtirogla02%C2%92%noitazitnauQ02%dna02%noitallitsiD02%egdelwonK02%C2%.g.e82%02%noisserpmoc02%ledomA0%sa02%hcus02%saera02%laicurc02%gnirevoc02%C2%ecnerefni02%MLL02%tneiciffe02%ni02%stnemecnavda02%tsetalA0%eht02%etalloc02%yllacitametsys02%eW02%.erawdrah02%thgir02%eht02%esoohc02%ot02%woh02%dna02%C2%deen02%yehtA0%noitatupmoc02%dna02%yromem02%hcum02%woh02%C2%dnuob-yromem02%era02%sMLL02%yhw02%sa02%hcus02%C2%smelborpA0%lacitcarp02%fo02%gnidnatsrednu02%raelc02%a02%sedivorp02%dna02%secived02%erawdrah02%no02%sMLLA0%gniyolped02%nehw02%skcenelttob02%eht02%seifitnedi02%krowemarf02%sihT02%.seuqinhcet02%ecnerefniA0%MLL02%fo02%sisylana02%citametsys02%rof02%ledom02%enilfoor02%no02%desab02%krowemarf02%a02%gnicudortniA0%yb02%osla02%tub02%hcraeser02%fo02%etats02%tnerruc02%eht02%gnizirammus02%ylno02%ton02%yb02%sweiverA0%erutaretil02%lanoitidart02%morf02%tuo02%sdnats02%yevrus02%ruO02%.niamod02%siht02%fo02%gnidnatsrednuA0%raelc02%a02%edivorp02%ot02%ecnerefnI02%MLL02%fo02%sdohtem02%suoirav02%eht02%sezylana02%tahtA0%krowemarf02%esicnoc02%a02%neeb02%t72%nsah02%ereht02%C2%tnarbiv02%si02%dna02%dednapxe02%sah02%dleif02%ehtA0%hguohtlA02%.segnellahc02%dna02%seitinutroppo02%fo02%dnelb02%euqinu02%a02%gnitneserp02%C2%gnivloveA0%yldipar02%si02%ecnerefni02%92%MLL82%02%ledoM02%egaugnaL02%egraL02%tneiciffe02%fo02%dleif02%ehT02%02%=3328342921.yrtne&reztueK02%truK02%dna02%nuS02%uygnauG02%dna02%nehC02%idieB02%dna02%naY02%naY02%dna02%eeL02%eaJ02%gnoY02%dna02%uG02%iygniQ02%dna02%iL02%iakihZ02%dna02%uW02%ehzgniB02%dna02%euX02%oahnehC02%dna02%uohZ02%ehZ02%dna02%gnoD02%nehZ02%dna02%uohZ02%gnaY02%dna02%gnahS02%gnahzuY02%dna02%nauY02%gnahihZ=526535609.yrtne&sthgisnI02%ledoM02%enilfooR02%dna02%yevruS02%A3%delievnU02%ecnerefnI02%MLL=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20LLM%20Inference%20Unveiled%3A%20Survey%20and%20Roofline%20Model%20Insights&body=Title%3A%20LLM%20Inference%20Unveiled%3A%20Survey%20and%20Roofline%20Model%20Insights%0AAuthor%3A%20Zhihang%20Yuan%20and%20Yuzhang%20Shang%20and%20Yang%20Zhou%20and%20Zhen%20Dong%20and%20Zhe%20Zhou%20and%20Chenhao%20Xue%20and%20Bingzhe%20Wu%20and%20Zhikai%20Li%20and%20Qingyi%20Gu%20and%20Yong%20Jae%20Lee%20and%20Yan%20Yan%20and%20Beidi%20Chen%20and%20Guangyu%20Sun%20and%20Kurt%20Keutzer%0AAbstract%3A%20%20%20The%20field%20of%20efficient%20Large%20Language%20Model%20%28LLM%29%20inference%20is%20rapidly%0Aevolving%2C%20presenting%20a%20unique%20blend%20of%20opportunities%20and%20challenges.%20Although%0Athe%20field%20has%20expanded%20and%20is%20vibrant%2C%20there%20hasn%27t%20been%20a%20concise%20framework%0Athat%20analyzes%20the%20various%20methods%20of%20LLM%20Inference%20to%20provide%20a%20clear%0Aunderstanding%20of%20this%20domain.%20Our%20survey%20stands%20out%20from%20traditional%20literature%0Areviews%20by%20not%20only%20summarizing%20the%20current%20state%20of%20research%20but%20also%20by%0Aintroducing%20a%20framework%20based%20on%20roofline%20model%20for%20systematic%20analysis%20of%20LLM%0Ainference%20techniques.%20This%20framework%20identifies%20the%20bottlenecks%20when%20deploying%0ALLMs%20on%20hardware%20devices%20and%20provides%20a%20clear%20understanding%20of%20practical%0Aproblems%2C%20such%20as%20why%20LLMs%20are%20memory-bound%2C%20how%20much%20memory%20and%20computation%0Athey%20need%2C%20and%20how%20to%20choose%20the%20right%20hardware.%20We%20systematically%20collate%20the%0Alatest%20advancements%20in%20efficient%20LLM%20inference%2C%20covering%20crucial%20areas%20such%20as%0Amodel%20compression%20%28e.g.%2C%20Knowledge%20Distillation%20and%20Quantization%29%2C%20algorithm%0Aimprovements%20%28e.g.%2C%20Early%20Exit%20and%20Mixture-of-Expert%29%2C%20and%20both%20hardware%20and%0Asystem-level%20enhancements.%20Our%20survey%20stands%20out%20by%20analyzing%20these%20methods%0Awith%20roofline%20model%2C%20helping%20us%20understand%20their%20impact%20on%20memory%20access%20and%0Acomputation.%20This%20distinctive%20approach%20not%20only%20showcases%20the%20current%20research%0Alandscape%20but%20also%20delivers%20valuable%20insights%20for%20practical%20implementation%2C%0Apositioning%20our%20work%20as%20an%20indispensable%20resource%20for%20researchers%20new%20to%20the%0Afield%20as%20well%20as%20for%20those%20seeking%20to%20deepen%20their%20understanding%20of%20efficient%0ALLM%20deployment.%20The%20analyze%20tool%2C%20LLM-Viewer%2C%20is%20open-sourced.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16363v6%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM%20Inference%20Unveiled%3A%20Survey%20and%20Roofline%20Model%20Insights&entry.906535625=Zhihang%20Yuan%20and%20Yuzhang%20Shang%20and%20Yang%20Zhou%20and%20Zhen%20Dong%20and%20Zhe%20Zhou%20and%20Chenhao%20Xue%20and%20Bingzhe%20Wu%20and%20Zhikai%20Li%20and%20Qingyi%20Gu%20and%20Yong%20Jae%20Lee%20and%20Yan%20Yan%20and%20Beidi%20Chen%20and%20Guangyu%20Sun%20and%20Kurt%20Keutzer&entry.1292438233=%20%20The%20field%20of%20efficient%20Large%20Language%20Model%20%28LLM%29%20inference%20is%20rapidly%0Aevolving%2C%20presenting%20a%20unique%20blend%20of%20opportunities%20and%20challenges.%20Although%0Athe%20field%20has%20expanded%20and%20is%20vibrant%2C%20there%20hasn%27t%20been%20a%20concise%20framework%0Athat%20analyzes%20the%20various%20methods%20of%20LLM%20Inference%20to%20provide%20a%20clear%0Aunderstanding%20of%20this%20domain.%20Our%20survey%20stands%20out%20from%20traditional%20literature%0Areviews%20by%20not%20only%20summarizing%20the%20current%20state%20of%20research%20but%20also%20by%0Aintroducing%20a%20framework%20based%20on%20roofline%20model%20for%20systematic%20analysis%20of%20LLM%0Ainference%20techniques.%20This%20framework%20identifies%20the%20bottlenecks%20when%20deploying%0ALLMs%20on%20hardware%20devices%20and%20provides%20a%20clear%20understanding%20of%20practical%0Aproblems%2C%20such%20as%20why%20LLMs%20are%20memory-bound%2C%20how%20much%20memory%20and%20computation%0Athey%20need%2C%20and%20how%20to%20choose%20the%20right%20hardware.%20We%20systematically%20collate%20the%0Alatest%20advancements%20in%20efficient%20LLM%20inference%2C%20covering%20crucial%20areas%20such%20as%0Amodel%20compression%20%28e.g.%2C%20Knowledge%20Distillation%20and%20Quantization%29%2C%20algorithm%0Aimprovements%20%28e.g.%2C%20Early%20Exit%20and%20Mixture-of-Expert%29%2C%20and%20both%20hardware%20and%0Asystem-level%20enhancements.%20Our%20survey%20stands%20out%20by%20analyzing%20these%20methods%0Awith%20roofline%20model%2C%20helping%20us%20understand%20their%20impact%20on%20memory%20access%20and%0Acomputation.%20This%20distinctive%20approach%20not%20only%20showcases%20the%20current%20research%0Alandscape%20but%20also%20delivers%20valuable%20insights%20for%20practical%20implementation%2C%0Apositioning%20our%20work%20as%20an%20indispensable%20resource%20for%20researchers%20new%20to%20the%0Afield%20as%20well%20as%20for%20those%20seeking%20to%20deepen%20their%20understanding%20of%20efficient%0ALLM%20deployment.%20The%20analyze%20tool%2C%20LLM-Viewer%2C%20is%20open-sourced.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16363v6&entry.124074799=Read"},
{"title": "TExplain: Explaining Learned Visual Features via Pre-trained (Frozen)\n  Language Models", "author": "Saeid Asgari Taghanaki and Aliasghar Khani and Ali Saheb Pasand and Amir Khasahmadi and Aditya Sanghi and Karl D. D. Willis and Ali Mahdavi-Amiri", "abstract": "  Interpreting the learned features of vision models has posed a longstanding\nchallenge in the field of machine learning. To address this issue, we propose a\nnovel method that leverages the capabilities of language models to interpret\nthe learned features of pre-trained image classifiers. Our method, called\nTExplain, tackles this task by training a neural network to establish a\nconnection between the feature space of image classifiers and language models.\nThen, during inference, our approach generates a vast number of sentences to\nexplain the features learned by the classifier for a given image. These\nsentences are then used to extract the most frequent words, providing a\ncomprehensive understanding of the learned features and patterns within the\nclassifier. Our method, for the first time, utilizes these frequent words\ncorresponding to a visual representation to provide insights into the\ndecision-making process of the independently trained classifier, enabling the\ndetection of spurious correlations, biases, and a deeper comprehension of its\nbehavior. To validate the effectiveness of our approach, we conduct experiments\non diverse datasets, including ImageNet-9L and Waterbirds. The results\ndemonstrate the potential of our method to enhance the interpretability and\nrobustness of image classifiers.\n", "link": "http://arxiv.org/abs/2309.00733v4", "date": "2024-05-02", "relevancy": 2.0702, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5449}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5057}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.495}], "mailto": "mailto:daeR=997470421.yrtne&4v33700.9032/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.sreifissalc02%egami02%fo02%ssentsuborA0%dna02%ytilibaterpretni02%eht02%ecnahne02%ot02%dohtem02%ruo02%fo02%laitnetop02%eht02%etartsnomedA0%stluser02%ehT02%.sdribretaW02%dna02%L9-teNegamI02%gnidulcni02%C2%stesatad02%esrevid02%noA0%stnemirepxe02%tcudnoc02%ew02%C2%hcaorppa02%ruo02%fo02%ssenevitceffe02%eht02%etadilav02%oT02%.roivahebA0%sti02%fo02%noisneherpmoc02%repeed02%a02%dna02%C2%sesaib02%C2%snoitalerroc02%suoirups02%fo02%noitcetedA0%eht02%gnilbane02%C2%reifissalc02%deniart02%yltnednepedni02%eht02%fo02%ssecorp02%gnikam-noisicedA0%eht02%otni02%sthgisni02%edivorp02%ot02%noitatneserper02%lausiv02%a02%ot02%gnidnopserrocA0%sdrow02%tneuqerf02%eseht02%sezilitu02%C2%emit02%tsrif02%eht02%rof02%C2%dohtem02%ruO02%.reifissalcA0%eht02%nihtiw02%snrettap02%dna02%serutaef02%denrael02%eht02%fo02%gnidnatsrednu02%evisneherpmocA0%a02%gnidivorp02%C2%sdrow02%tneuqerf02%tsom02%eht02%tcartxe02%ot02%desu02%neht02%era02%secnetnesA0%esehT02%.egami02%nevig02%a02%rof02%reifissalc02%eht02%yb02%denrael02%serutaef02%eht02%nialpxeA0%ot02%secnetnes02%fo02%rebmun02%tsav02%a02%setareneg02%hcaorppa02%ruo02%C2%ecnerefni02%gnirud02%C2%nehTA0%.sledom02%egaugnal02%dna02%sreifissalc02%egami02%fo02%ecaps02%erutaef02%eht02%neewteb02%noitcennocA0%a02%hsilbatse02%ot02%krowten02%laruen02%a02%gniniart02%yb02%ksat02%siht02%selkcat02%C2%nialpxETA0%dellac02%C2%dohtem02%ruO02%.sreifissalc02%egami02%deniart-erp02%fo02%serutaef02%denrael02%ehtA0%terpretni02%ot02%sledom02%egaugnal02%fo02%seitilibapac02%eht02%segarevel02%taht02%dohtem02%levonA0%a02%esoporp02%ew02%C2%eussi02%siht02%sserdda02%oT02%.gninrael02%enihcam02%fo02%dleif02%eht02%ni02%egnellahcA0%gnidnatsgnol02%a02%desop02%sah02%sledom02%noisiv02%fo02%serutaef02%denrael02%eht02%gniterpretnI02%02%=3328342921.yrtne&irimA-ivadhaM02%ilA02%dna02%silliW02%.D02%.D02%lraK02%dna02%ihgnaS02%aytidA02%dna02%idamhasahK02%rimA02%dna02%dnasaP02%behaS02%ilA02%dna02%inahK02%rahgsailA02%dna02%ikanahgaT02%iragsA02%dieaS=526535609.yrtne&sledoM02%egaugnaL02%02%A0%92%nezorF82%02%deniart-erP02%aiv02%serutaeF02%lausiV02%denraeL02%gninialpxE02%A3%nialpxET=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20TExplain%3A%20Explaining%20Learned%20Visual%20Features%20via%20Pre-trained%20%28Frozen%29%0A%20%20Language%20Models&body=Title%3A%20TExplain%3A%20Explaining%20Learned%20Visual%20Features%20via%20Pre-trained%20%28Frozen%29%0A%20%20Language%20Models%0AAuthor%3A%20Saeid%20Asgari%20Taghanaki%20and%20Aliasghar%20Khani%20and%20Ali%20Saheb%20Pasand%20and%20Amir%20Khasahmadi%20and%20Aditya%20Sanghi%20and%20Karl%20D.%20D.%20Willis%20and%20Ali%20Mahdavi-Amiri%0AAbstract%3A%20%20%20Interpreting%20the%20learned%20features%20of%20vision%20models%20has%20posed%20a%20longstanding%0Achallenge%20in%20the%20field%20of%20machine%20learning.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Anovel%20method%20that%20leverages%20the%20capabilities%20of%20language%20models%20to%20interpret%0Athe%20learned%20features%20of%20pre-trained%20image%20classifiers.%20Our%20method%2C%20called%0ATExplain%2C%20tackles%20this%20task%20by%20training%20a%20neural%20network%20to%20establish%20a%0Aconnection%20between%20the%20feature%20space%20of%20image%20classifiers%20and%20language%20models.%0AThen%2C%20during%20inference%2C%20our%20approach%20generates%20a%20vast%20number%20of%20sentences%20to%0Aexplain%20the%20features%20learned%20by%20the%20classifier%20for%20a%20given%20image.%20These%0Asentences%20are%20then%20used%20to%20extract%20the%20most%20frequent%20words%2C%20providing%20a%0Acomprehensive%20understanding%20of%20the%20learned%20features%20and%20patterns%20within%20the%0Aclassifier.%20Our%20method%2C%20for%20the%20first%20time%2C%20utilizes%20these%20frequent%20words%0Acorresponding%20to%20a%20visual%20representation%20to%20provide%20insights%20into%20the%0Adecision-making%20process%20of%20the%20independently%20trained%20classifier%2C%20enabling%20the%0Adetection%20of%20spurious%20correlations%2C%20biases%2C%20and%20a%20deeper%20comprehension%20of%20its%0Abehavior.%20To%20validate%20the%20effectiveness%20of%20our%20approach%2C%20we%20conduct%20experiments%0Aon%20diverse%20datasets%2C%20including%20ImageNet-9L%20and%20Waterbirds.%20The%20results%0Ademonstrate%20the%20potential%20of%20our%20method%20to%20enhance%20the%20interpretability%20and%0Arobustness%20of%20image%20classifiers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.00733v4%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TExplain%3A%20Explaining%20Learned%20Visual%20Features%20via%20Pre-trained%20%28Frozen%29%0A%20%20Language%20Models&entry.906535625=Saeid%20Asgari%20Taghanaki%20and%20Aliasghar%20Khani%20and%20Ali%20Saheb%20Pasand%20and%20Amir%20Khasahmadi%20and%20Aditya%20Sanghi%20and%20Karl%20D.%20D.%20Willis%20and%20Ali%20Mahdavi-Amiri&entry.1292438233=%20%20Interpreting%20the%20learned%20features%20of%20vision%20models%20has%20posed%20a%20longstanding%0Achallenge%20in%20the%20field%20of%20machine%20learning.%20To%20address%20this%20issue%2C%20we%20propose%20a%0Anovel%20method%20that%20leverages%20the%20capabilities%20of%20language%20models%20to%20interpret%0Athe%20learned%20features%20of%20pre-trained%20image%20classifiers.%20Our%20method%2C%20called%0ATExplain%2C%20tackles%20this%20task%20by%20training%20a%20neural%20network%20to%20establish%20a%0Aconnection%20between%20the%20feature%20space%20of%20image%20classifiers%20and%20language%20models.%0AThen%2C%20during%20inference%2C%20our%20approach%20generates%20a%20vast%20number%20of%20sentences%20to%0Aexplain%20the%20features%20learned%20by%20the%20classifier%20for%20a%20given%20image.%20These%0Asentences%20are%20then%20used%20to%20extract%20the%20most%20frequent%20words%2C%20providing%20a%0Acomprehensive%20understanding%20of%20the%20learned%20features%20and%20patterns%20within%20the%0Aclassifier.%20Our%20method%2C%20for%20the%20first%20time%2C%20utilizes%20these%20frequent%20words%0Acorresponding%20to%20a%20visual%20representation%20to%20provide%20insights%20into%20the%0Adecision-making%20process%20of%20the%20independently%20trained%20classifier%2C%20enabling%20the%0Adetection%20of%20spurious%20correlations%2C%20biases%2C%20and%20a%20deeper%20comprehension%20of%20its%0Abehavior.%20To%20validate%20the%20effectiveness%20of%20our%20approach%2C%20we%20conduct%20experiments%0Aon%20diverse%20datasets%2C%20including%20ImageNet-9L%20and%20Waterbirds.%20The%20results%0Ademonstrate%20the%20potential%20of%20our%20method%20to%20enhance%20the%20interpretability%20and%0Arobustness%20of%20image%20classifiers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.00733v4&entry.124074799=Read"},
{"title": "Obtaining Favorable Layouts for Multiple Object Generation", "author": "Barak Battash and Amit Rozner and Lior Wolf and Ofir Lindenbaum", "abstract": "  Large-scale text-to-image models that can generate high-quality and diverse\nimages based on textual prompts have shown remarkable success. These models aim\nultimately to create complex scenes, and addressing the challenge of\nmulti-subject generation is a critical step towards this goal. However, the\nexisting state-of-the-art diffusion models face difficulty when generating\nimages that involve multiple subjects. When presented with a prompt containing\nmore than one subject, these models may omit some subjects or merge them\ntogether. To address this challenge, we propose a novel approach based on a\nguiding principle. We allow the diffusion model to initially propose a layout,\nand then we rearrange the layout grid. This is achieved by enforcing\ncross-attention maps (XAMs) to adhere to proposed masks and by migrating pixels\nfrom latent maps to new locations determined by us. We introduce new loss terms\naimed at reducing XAM entropy for clearer spatial definition of subjects,\nreduce the overlap between XAMs, and ensure that XAMs align with their\nrespective masks. We contrast our approach with several alternative methods and\nshow that it more faithfully captures the desired concepts across a variety of\ntext prompts.\n", "link": "http://arxiv.org/abs/2405.00791v1", "date": "2024-05-01", "relevancy": 1.2555, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.6308}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6277}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6247}], "mailto": "mailto:daeR=997470421.yrtne&1v19700.5042/sba/gro.vixra//A3%ptth=8027668381.yrtne&A0%.stpmorp02%txetA0%fo02%yteirav02%a02%ssorca02%stpecnoc02%derised02%eht02%serutpac02%yllufhtiaf02%erom02%ti02%taht02%wohsA0%dna02%sdohtem02%evitanretla02%lareves02%htiw02%hcaorppa02%ruo02%tsartnoc02%eW02%.sksam02%evitcepserA0%rieht02%htiw02%ngila02%sMAX02%taht02%erusne02%dna02%C2%sMAX02%neewteb02%palrevo02%eht02%ecuderA0%C2%stcejbus02%fo02%noitinifed02%laitaps02%reraelc02%rof02%yportne02%MAX02%gnicuder02%ta02%demiaA0%smret02%ssol02%wen02%ecudortni02%eW02%.su02%yb02%denimreted02%snoitacol02%wen02%ot02%spam02%tnetal02%morfA0%slexip02%gnitargim02%yb02%dna02%sksam02%desoporp02%ot02%erehda02%ot02%92%sMAX82%02%spam02%noitnetta-ssorcA0%gnicrofne02%yb02%deveihca02%si02%sihT02%.dirg02%tuoyal02%eht02%egnarraer02%ew02%neht02%dnaA0%C2%tuoyal02%a02%esoporp02%yllaitini02%ot02%ledom02%noisuffid02%eht02%wolla02%eW02%.elpicnirp02%gnidiugA0%a02%no02%desab02%hcaorppa02%levon02%a02%esoporp02%ew02%C2%egnellahc02%siht02%sserdda02%oT02%.rehtegotA0%meht02%egrem02%ro02%stcejbus02%emos02%timo02%yam02%sledom02%eseht02%C2%tcejbus02%eno02%naht02%eromA0%gniniatnoc02%tpmorp02%a02%htiw02%detneserp02%nehW02%.stcejbus02%elpitlum02%evlovni02%taht02%segamiA0%gnitareneg02%nehw02%ytluciffid02%ecaf02%sledom02%noisuffid02%tra-eht-fo-etats02%gnitsixeA0%eht02%C2%revewoH02%.laog02%siht02%sdrawot02%pets02%lacitirc02%a02%si02%noitareneg02%tcejbus-itlumA0%fo02%egnellahc02%eht02%gnisserdda02%dna02%C2%senecs02%xelpmoc02%etaerc02%ot02%yletamitluA0%mia02%sledom02%esehT02%.sseccus02%elbakramer02%nwohs02%evah02%stpmorp02%lautxet02%no02%desab02%segamiA0%esrevid02%dna02%ytilauq-hgih02%etareneg02%nac02%taht02%sledom02%egami-ot-txet02%elacs-egraL02%02%=3328342921.yrtne&muabnedniL02%rifO02%dna02%floW02%roiL02%dna02%renzoR02%timA02%dna02%hsattaB02%karaB=526535609.yrtne&noitareneG02%tcejbO02%elpitluM02%rof02%stuoyaL02%elbarovaF02%gniniatbO=8489290831.yrtne?mrofweiv/QYPUxNNruB9jDr1__K4oaIsq7_m7vvmYWG7Ass9dIhSqFfSfSLQpIAF1/e/d/smrof/moc.elgoog.scod//:sptth@gmail.com?subject=%5BarXrec%5D%20Obtaining%20Favorable%20Layouts%20for%20Multiple%20Object%20Generation&body=Title%3A%20Obtaining%20Favorable%20Layouts%20for%20Multiple%20Object%20Generation%0AAuthor%3A%20Barak%20Battash%20and%20Amit%20Rozner%20and%20Lior%20Wolf%20and%20Ofir%20Lindenbaum%0AAbstract%3A%20%20%20Large-scale%20text-to-image%20models%20that%20can%20generate%20high-quality%20and%20diverse%0Aimages%20based%20on%20textual%20prompts%20have%20shown%20remarkable%20success.%20These%20models%20aim%0Aultimately%20to%20create%20complex%20scenes%2C%20and%20addressing%20the%20challenge%20of%0Amulti-subject%20generation%20is%20a%20critical%20step%20towards%20this%20goal.%20However%2C%20the%0Aexisting%20state-of-the-art%20diffusion%20models%20face%20difficulty%20when%20generating%0Aimages%20that%20involve%20multiple%20subjects.%20When%20presented%20with%20a%20prompt%20containing%0Amore%20than%20one%20subject%2C%20these%20models%20may%20omit%20some%20subjects%20or%20merge%20them%0Atogether.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20approach%20based%20on%20a%0Aguiding%20principle.%20We%20allow%20the%20diffusion%20model%20to%20initially%20propose%20a%20layout%2C%0Aand%20then%20we%20rearrange%20the%20layout%20grid.%20This%20is%20achieved%20by%20enforcing%0Across-attention%20maps%20%28XAMs%29%20to%20adhere%20to%20proposed%20masks%20and%20by%20migrating%20pixels%0Afrom%20latent%20maps%20to%20new%20locations%20determined%20by%20us.%20We%20introduce%20new%20loss%20terms%0Aaimed%20at%20reducing%20XAM%20entropy%20for%20clearer%20spatial%20definition%20of%20subjects%2C%0Areduce%20the%20overlap%20between%20XAMs%2C%20and%20ensure%20that%20XAMs%20align%20with%20their%0Arespective%20masks.%20We%20contrast%20our%20approach%20with%20several%20alternative%20methods%20and%0Ashow%20that%20it%20more%20faithfully%20captures%20the%20desired%20concepts%20across%20a%20variety%20of%0Atext%20prompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00791v1%0AForm%3A%20", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Obtaining%20Favorable%20Layouts%20for%20Multiple%20Object%20Generation&entry.906535625=Barak%20Battash%20and%20Amit%20Rozner%20and%20Lior%20Wolf%20and%20Ofir%20Lindenbaum&entry.1292438233=%20%20Large-scale%20text-to-image%20models%20that%20can%20generate%20high-quality%20and%20diverse%0Aimages%20based%20on%20textual%20prompts%20have%20shown%20remarkable%20success.%20These%20models%20aim%0Aultimately%20to%20create%20complex%20scenes%2C%20and%20addressing%20the%20challenge%20of%0Amulti-subject%20generation%20is%20a%20critical%20step%20towards%20this%20goal.%20However%2C%20the%0Aexisting%20state-of-the-art%20diffusion%20models%20face%20difficulty%20when%20generating%0Aimages%20that%20involve%20multiple%20subjects.%20When%20presented%20with%20a%20prompt%20containing%0Amore%20than%20one%20subject%2C%20these%20models%20may%20omit%20some%20subjects%20or%20merge%20them%0Atogether.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20novel%20approach%20based%20on%20a%0Aguiding%20principle.%20We%20allow%20the%20diffusion%20model%20to%20initially%20propose%20a%20layout%2C%0Aand%20then%20we%20rearrange%20the%20layout%20grid.%20This%20is%20achieved%20by%20enforcing%0Across-attention%20maps%20%28XAMs%29%20to%20adhere%20to%20proposed%20masks%20and%20by%20migrating%20pixels%0Afrom%20latent%20maps%20to%20new%20locations%20determined%20by%20us.%20We%20introduce%20new%20loss%20terms%0Aaimed%20at%20reducing%20XAM%20entropy%20for%20clearer%20spatial%20definition%20of%20subjects%2C%0Areduce%20the%20overlap%20between%20XAMs%2C%20and%20ensure%20that%20XAMs%20align%20with%20their%0Arespective%20masks.%20We%20contrast%20our%20approach%20with%20several%20alternative%20methods%20and%0Ashow%20that%20it%20more%20faithfully%20captures%20the%20desired%20concepts%20across%20a%20variety%20of%0Atext%20prompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00791v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


