<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240701.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Topo4D: Topology-Preserving Gaussian Splatting for High-Fidelity 4D Head\n  Capture", "author": "Xuanchen Li and Yuhao Cheng and Xingyu Ren and Haozhe Jia and Di Xu and Wenhan Zhu and Yichao Yan", "abstract": "  4D head capture aims to generate dynamic topological meshes and corresponding\ntexture maps from videos, which is widely utilized in movies and games for its\nability to simulate facial muscle movements and recover dynamic textures in\npore-squeezing. The industry often adopts the method involving multi-view\nstereo and non-rigid alignment. However, this approach is prone to errors and\nheavily reliant on time-consuming manual processing by artists. To simplify\nthis process, we propose Topo4D, a novel framework for automatic geometry and\ntexture generation, which optimizes densely aligned 4D heads and 8K texture\nmaps directly from calibrated multi-view time-series images. Specifically, we\nfirst represent the time-series faces as a set of dynamic 3D Gaussians with\nfixed topology in which the Gaussian centers are bound to the mesh vertices.\nAfterward, we perform alternative geometry and texture optimization\nframe-by-frame for high-quality geometry and texture learning while maintaining\ntemporal topology stability. Finally, we can extract dynamic facial meshes in\nregular wiring arrangement and high-fidelity textures with pore-level details\nfrom the learned Gaussians. Extensive experiments show that our method achieves\nsuperior results than the current SOTA face reconstruction methods both in the\nquality of meshes and textures. Project page:\nhttps://xuanchenli.github.io/Topo4D/.\n", "link": "http://arxiv.org/abs/2406.00440v2", "date": "2024-07-01", "relevancy": 3.2883, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6845}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6569}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6316}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topo4D%3A%20Topology-Preserving%20Gaussian%20Splatting%20for%20High-Fidelity%204D%20Head%0A%20%20Capture&body=Title%3A%20Topo4D%3A%20Topology-Preserving%20Gaussian%20Splatting%20for%20High-Fidelity%204D%20Head%0A%20%20Capture%0AAuthor%3A%20Xuanchen%20Li%20and%20Yuhao%20Cheng%20and%20Xingyu%20Ren%20and%20Haozhe%20Jia%20and%20Di%20Xu%20and%20Wenhan%20Zhu%20and%20Yichao%20Yan%0AAbstract%3A%20%20%204D%20head%20capture%20aims%20to%20generate%20dynamic%20topological%20meshes%20and%20corresponding%0Atexture%20maps%20from%20videos%2C%20which%20is%20widely%20utilized%20in%20movies%20and%20games%20for%20its%0Aability%20to%20simulate%20facial%20muscle%20movements%20and%20recover%20dynamic%20textures%20in%0Apore-squeezing.%20The%20industry%20often%20adopts%20the%20method%20involving%20multi-view%0Astereo%20and%20non-rigid%20alignment.%20However%2C%20this%20approach%20is%20prone%20to%20errors%20and%0Aheavily%20reliant%20on%20time-consuming%20manual%20processing%20by%20artists.%20To%20simplify%0Athis%20process%2C%20we%20propose%20Topo4D%2C%20a%20novel%20framework%20for%20automatic%20geometry%20and%0Atexture%20generation%2C%20which%20optimizes%20densely%20aligned%204D%20heads%20and%208K%20texture%0Amaps%20directly%20from%20calibrated%20multi-view%20time-series%20images.%20Specifically%2C%20we%0Afirst%20represent%20the%20time-series%20faces%20as%20a%20set%20of%20dynamic%203D%20Gaussians%20with%0Afixed%20topology%20in%20which%20the%20Gaussian%20centers%20are%20bound%20to%20the%20mesh%20vertices.%0AAfterward%2C%20we%20perform%20alternative%20geometry%20and%20texture%20optimization%0Aframe-by-frame%20for%20high-quality%20geometry%20and%20texture%20learning%20while%20maintaining%0Atemporal%20topology%20stability.%20Finally%2C%20we%20can%20extract%20dynamic%20facial%20meshes%20in%0Aregular%20wiring%20arrangement%20and%20high-fidelity%20textures%20with%20pore-level%20details%0Afrom%20the%20learned%20Gaussians.%20Extensive%20experiments%20show%20that%20our%20method%20achieves%0Asuperior%20results%20than%20the%20current%20SOTA%20face%20reconstruction%20methods%20both%20in%20the%0Aquality%20of%20meshes%20and%20textures.%20Project%20page%3A%0Ahttps%3A//xuanchenli.github.io/Topo4D/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00440v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopo4D%253A%2520Topology-Preserving%2520Gaussian%2520Splatting%2520for%2520High-Fidelity%25204D%2520Head%250A%2520%2520Capture%26entry.906535625%3DXuanchen%2520Li%2520and%2520Yuhao%2520Cheng%2520and%2520Xingyu%2520Ren%2520and%2520Haozhe%2520Jia%2520and%2520Di%2520Xu%2520and%2520Wenhan%2520Zhu%2520and%2520Yichao%2520Yan%26entry.1292438233%3D%2520%25204D%2520head%2520capture%2520aims%2520to%2520generate%2520dynamic%2520topological%2520meshes%2520and%2520corresponding%250Atexture%2520maps%2520from%2520videos%252C%2520which%2520is%2520widely%2520utilized%2520in%2520movies%2520and%2520games%2520for%2520its%250Aability%2520to%2520simulate%2520facial%2520muscle%2520movements%2520and%2520recover%2520dynamic%2520textures%2520in%250Apore-squeezing.%2520The%2520industry%2520often%2520adopts%2520the%2520method%2520involving%2520multi-view%250Astereo%2520and%2520non-rigid%2520alignment.%2520However%252C%2520this%2520approach%2520is%2520prone%2520to%2520errors%2520and%250Aheavily%2520reliant%2520on%2520time-consuming%2520manual%2520processing%2520by%2520artists.%2520To%2520simplify%250Athis%2520process%252C%2520we%2520propose%2520Topo4D%252C%2520a%2520novel%2520framework%2520for%2520automatic%2520geometry%2520and%250Atexture%2520generation%252C%2520which%2520optimizes%2520densely%2520aligned%25204D%2520heads%2520and%25208K%2520texture%250Amaps%2520directly%2520from%2520calibrated%2520multi-view%2520time-series%2520images.%2520Specifically%252C%2520we%250Afirst%2520represent%2520the%2520time-series%2520faces%2520as%2520a%2520set%2520of%2520dynamic%25203D%2520Gaussians%2520with%250Afixed%2520topology%2520in%2520which%2520the%2520Gaussian%2520centers%2520are%2520bound%2520to%2520the%2520mesh%2520vertices.%250AAfterward%252C%2520we%2520perform%2520alternative%2520geometry%2520and%2520texture%2520optimization%250Aframe-by-frame%2520for%2520high-quality%2520geometry%2520and%2520texture%2520learning%2520while%2520maintaining%250Atemporal%2520topology%2520stability.%2520Finally%252C%2520we%2520can%2520extract%2520dynamic%2520facial%2520meshes%2520in%250Aregular%2520wiring%2520arrangement%2520and%2520high-fidelity%2520textures%2520with%2520pore-level%2520details%250Afrom%2520the%2520learned%2520Gaussians.%2520Extensive%2520experiments%2520show%2520that%2520our%2520method%2520achieves%250Asuperior%2520results%2520than%2520the%2520current%2520SOTA%2520face%2520reconstruction%2520methods%2520both%2520in%2520the%250Aquality%2520of%2520meshes%2520and%2520textures.%2520Project%2520page%253A%250Ahttps%253A//xuanchenli.github.io/Topo4D/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00440v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topo4D%3A%20Topology-Preserving%20Gaussian%20Splatting%20for%20High-Fidelity%204D%20Head%0A%20%20Capture&entry.906535625=Xuanchen%20Li%20and%20Yuhao%20Cheng%20and%20Xingyu%20Ren%20and%20Haozhe%20Jia%20and%20Di%20Xu%20and%20Wenhan%20Zhu%20and%20Yichao%20Yan&entry.1292438233=%20%204D%20head%20capture%20aims%20to%20generate%20dynamic%20topological%20meshes%20and%20corresponding%0Atexture%20maps%20from%20videos%2C%20which%20is%20widely%20utilized%20in%20movies%20and%20games%20for%20its%0Aability%20to%20simulate%20facial%20muscle%20movements%20and%20recover%20dynamic%20textures%20in%0Apore-squeezing.%20The%20industry%20often%20adopts%20the%20method%20involving%20multi-view%0Astereo%20and%20non-rigid%20alignment.%20However%2C%20this%20approach%20is%20prone%20to%20errors%20and%0Aheavily%20reliant%20on%20time-consuming%20manual%20processing%20by%20artists.%20To%20simplify%0Athis%20process%2C%20we%20propose%20Topo4D%2C%20a%20novel%20framework%20for%20automatic%20geometry%20and%0Atexture%20generation%2C%20which%20optimizes%20densely%20aligned%204D%20heads%20and%208K%20texture%0Amaps%20directly%20from%20calibrated%20multi-view%20time-series%20images.%20Specifically%2C%20we%0Afirst%20represent%20the%20time-series%20faces%20as%20a%20set%20of%20dynamic%203D%20Gaussians%20with%0Afixed%20topology%20in%20which%20the%20Gaussian%20centers%20are%20bound%20to%20the%20mesh%20vertices.%0AAfterward%2C%20we%20perform%20alternative%20geometry%20and%20texture%20optimization%0Aframe-by-frame%20for%20high-quality%20geometry%20and%20texture%20learning%20while%20maintaining%0Atemporal%20topology%20stability.%20Finally%2C%20we%20can%20extract%20dynamic%20facial%20meshes%20in%0Aregular%20wiring%20arrangement%20and%20high-fidelity%20textures%20with%20pore-level%20details%0Afrom%20the%20learned%20Gaussians.%20Extensive%20experiments%20show%20that%20our%20method%20achieves%0Asuperior%20results%20than%20the%20current%20SOTA%20face%20reconstruction%20methods%20both%20in%20the%0Aquality%20of%20meshes%20and%20textures.%20Project%20page%3A%0Ahttps%3A//xuanchenli.github.io/Topo4D/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00440v2&entry.124074799=Read"},
{"title": "Instruction-Guided Scene Text Recognition", "author": "Yongkun Du and Zhineng Chen and Yuchen Su and Caiyan Jia and Yu-Gang Jiang", "abstract": "  Multi-modal models show appealing performance in visual recognition tasks\nrecently, as free-form text-guided training evokes the ability to understand\nfine-grained visual content. However, current models are either inefficient or\ncannot be trivially upgraded to scene text recognition (STR) due to the\ncomposition difference between natural and text images. We propose a novel\ninstruction-guided scene text recognition (IGTR) paradigm that formulates STR\nas an instruction learning problem and understands text images by predicting\ncharacter attributes, e.g., character frequency, position, etc. IGTR first\ndevises $\\left \\langle condition,question,answer\\right \\rangle$ instruction\ntriplets, providing rich and diverse descriptions of character attributes. To\neffectively learn these attributes through question-answering, IGTR develops\nlightweight instruction encoder, cross-modal feature fusion module and\nmulti-task answer head, which guides nuanced text image understanding.\nFurthermore, IGTR realizes different recognition pipelines simply by using\ndifferent instructions, enabling a character-understanding-based text reasoning\nparadigm that considerably differs from current methods. Experiments on English\nand Chinese benchmarks show that IGTR outperforms existing models by\nsignificant margins, while maintaining a small model size and efficient\ninference speed. Moreover, by adjusting the sampling of instructions, IGTR\noffers an elegant way to tackle the recognition of both rarely appearing and\nmorphologically similar characters, which were previous challenges. Code at\n\\href{https://github.com/Topdu/OpenOCR}{this http URL}.\n", "link": "http://arxiv.org/abs/2401.17851v2", "date": "2024-07-01", "relevancy": 2.7794, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5733}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5643}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instruction-Guided%20Scene%20Text%20Recognition&body=Title%3A%20Instruction-Guided%20Scene%20Text%20Recognition%0AAuthor%3A%20Yongkun%20Du%20and%20Zhineng%20Chen%20and%20Yuchen%20Su%20and%20Caiyan%20Jia%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20%20%20Multi-modal%20models%20show%20appealing%20performance%20in%20visual%20recognition%20tasks%0Arecently%2C%20as%20free-form%20text-guided%20training%20evokes%20the%20ability%20to%20understand%0Afine-grained%20visual%20content.%20However%2C%20current%20models%20are%20either%20inefficient%20or%0Acannot%20be%20trivially%20upgraded%20to%20scene%20text%20recognition%20%28STR%29%20due%20to%20the%0Acomposition%20difference%20between%20natural%20and%20text%20images.%20We%20propose%20a%20novel%0Ainstruction-guided%20scene%20text%20recognition%20%28IGTR%29%20paradigm%20that%20formulates%20STR%0Aas%20an%20instruction%20learning%20problem%20and%20understands%20text%20images%20by%20predicting%0Acharacter%20attributes%2C%20e.g.%2C%20character%20frequency%2C%20position%2C%20etc.%20IGTR%20first%0Adevises%20%24%5Cleft%20%5Clangle%20condition%2Cquestion%2Canswer%5Cright%20%5Crangle%24%20instruction%0Atriplets%2C%20providing%20rich%20and%20diverse%20descriptions%20of%20character%20attributes.%20To%0Aeffectively%20learn%20these%20attributes%20through%20question-answering%2C%20IGTR%20develops%0Alightweight%20instruction%20encoder%2C%20cross-modal%20feature%20fusion%20module%20and%0Amulti-task%20answer%20head%2C%20which%20guides%20nuanced%20text%20image%20understanding.%0AFurthermore%2C%20IGTR%20realizes%20different%20recognition%20pipelines%20simply%20by%20using%0Adifferent%20instructions%2C%20enabling%20a%20character-understanding-based%20text%20reasoning%0Aparadigm%20that%20considerably%20differs%20from%20current%20methods.%20Experiments%20on%20English%0Aand%20Chinese%20benchmarks%20show%20that%20IGTR%20outperforms%20existing%20models%20by%0Asignificant%20margins%2C%20while%20maintaining%20a%20small%20model%20size%20and%20efficient%0Ainference%20speed.%20Moreover%2C%20by%20adjusting%20the%20sampling%20of%20instructions%2C%20IGTR%0Aoffers%20an%20elegant%20way%20to%20tackle%20the%20recognition%20of%20both%20rarely%20appearing%20and%0Amorphologically%20similar%20characters%2C%20which%20were%20previous%20challenges.%20Code%20at%0A%5Chref%7Bhttps%3A//github.com/Topdu/OpenOCR%7D%7Bthis%20http%20URL%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.17851v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstruction-Guided%2520Scene%2520Text%2520Recognition%26entry.906535625%3DYongkun%2520Du%2520and%2520Zhineng%2520Chen%2520and%2520Yuchen%2520Su%2520and%2520Caiyan%2520Jia%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3D%2520%2520Multi-modal%2520models%2520show%2520appealing%2520performance%2520in%2520visual%2520recognition%2520tasks%250Arecently%252C%2520as%2520free-form%2520text-guided%2520training%2520evokes%2520the%2520ability%2520to%2520understand%250Afine-grained%2520visual%2520content.%2520However%252C%2520current%2520models%2520are%2520either%2520inefficient%2520or%250Acannot%2520be%2520trivially%2520upgraded%2520to%2520scene%2520text%2520recognition%2520%2528STR%2529%2520due%2520to%2520the%250Acomposition%2520difference%2520between%2520natural%2520and%2520text%2520images.%2520We%2520propose%2520a%2520novel%250Ainstruction-guided%2520scene%2520text%2520recognition%2520%2528IGTR%2529%2520paradigm%2520that%2520formulates%2520STR%250Aas%2520an%2520instruction%2520learning%2520problem%2520and%2520understands%2520text%2520images%2520by%2520predicting%250Acharacter%2520attributes%252C%2520e.g.%252C%2520character%2520frequency%252C%2520position%252C%2520etc.%2520IGTR%2520first%250Adevises%2520%2524%255Cleft%2520%255Clangle%2520condition%252Cquestion%252Canswer%255Cright%2520%255Crangle%2524%2520instruction%250Atriplets%252C%2520providing%2520rich%2520and%2520diverse%2520descriptions%2520of%2520character%2520attributes.%2520To%250Aeffectively%2520learn%2520these%2520attributes%2520through%2520question-answering%252C%2520IGTR%2520develops%250Alightweight%2520instruction%2520encoder%252C%2520cross-modal%2520feature%2520fusion%2520module%2520and%250Amulti-task%2520answer%2520head%252C%2520which%2520guides%2520nuanced%2520text%2520image%2520understanding.%250AFurthermore%252C%2520IGTR%2520realizes%2520different%2520recognition%2520pipelines%2520simply%2520by%2520using%250Adifferent%2520instructions%252C%2520enabling%2520a%2520character-understanding-based%2520text%2520reasoning%250Aparadigm%2520that%2520considerably%2520differs%2520from%2520current%2520methods.%2520Experiments%2520on%2520English%250Aand%2520Chinese%2520benchmarks%2520show%2520that%2520IGTR%2520outperforms%2520existing%2520models%2520by%250Asignificant%2520margins%252C%2520while%2520maintaining%2520a%2520small%2520model%2520size%2520and%2520efficient%250Ainference%2520speed.%2520Moreover%252C%2520by%2520adjusting%2520the%2520sampling%2520of%2520instructions%252C%2520IGTR%250Aoffers%2520an%2520elegant%2520way%2520to%2520tackle%2520the%2520recognition%2520of%2520both%2520rarely%2520appearing%2520and%250Amorphologically%2520similar%2520characters%252C%2520which%2520were%2520previous%2520challenges.%2520Code%2520at%250A%255Chref%257Bhttps%253A//github.com/Topdu/OpenOCR%257D%257Bthis%2520http%2520URL%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.17851v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instruction-Guided%20Scene%20Text%20Recognition&entry.906535625=Yongkun%20Du%20and%20Zhineng%20Chen%20and%20Yuchen%20Su%20and%20Caiyan%20Jia%20and%20Yu-Gang%20Jiang&entry.1292438233=%20%20Multi-modal%20models%20show%20appealing%20performance%20in%20visual%20recognition%20tasks%0Arecently%2C%20as%20free-form%20text-guided%20training%20evokes%20the%20ability%20to%20understand%0Afine-grained%20visual%20content.%20However%2C%20current%20models%20are%20either%20inefficient%20or%0Acannot%20be%20trivially%20upgraded%20to%20scene%20text%20recognition%20%28STR%29%20due%20to%20the%0Acomposition%20difference%20between%20natural%20and%20text%20images.%20We%20propose%20a%20novel%0Ainstruction-guided%20scene%20text%20recognition%20%28IGTR%29%20paradigm%20that%20formulates%20STR%0Aas%20an%20instruction%20learning%20problem%20and%20understands%20text%20images%20by%20predicting%0Acharacter%20attributes%2C%20e.g.%2C%20character%20frequency%2C%20position%2C%20etc.%20IGTR%20first%0Adevises%20%24%5Cleft%20%5Clangle%20condition%2Cquestion%2Canswer%5Cright%20%5Crangle%24%20instruction%0Atriplets%2C%20providing%20rich%20and%20diverse%20descriptions%20of%20character%20attributes.%20To%0Aeffectively%20learn%20these%20attributes%20through%20question-answering%2C%20IGTR%20develops%0Alightweight%20instruction%20encoder%2C%20cross-modal%20feature%20fusion%20module%20and%0Amulti-task%20answer%20head%2C%20which%20guides%20nuanced%20text%20image%20understanding.%0AFurthermore%2C%20IGTR%20realizes%20different%20recognition%20pipelines%20simply%20by%20using%0Adifferent%20instructions%2C%20enabling%20a%20character-understanding-based%20text%20reasoning%0Aparadigm%20that%20considerably%20differs%20from%20current%20methods.%20Experiments%20on%20English%0Aand%20Chinese%20benchmarks%20show%20that%20IGTR%20outperforms%20existing%20models%20by%0Asignificant%20margins%2C%20while%20maintaining%20a%20small%20model%20size%20and%20efficient%0Ainference%20speed.%20Moreover%2C%20by%20adjusting%20the%20sampling%20of%20instructions%2C%20IGTR%0Aoffers%20an%20elegant%20way%20to%20tackle%20the%20recognition%20of%20both%20rarely%20appearing%20and%0Amorphologically%20similar%20characters%2C%20which%20were%20previous%20challenges.%20Code%20at%0A%5Chref%7Bhttps%3A//github.com/Topdu/OpenOCR%7D%7Bthis%20http%20URL%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.17851v2&entry.124074799=Read"},
{"title": "CILF-CIAE: CLIP-driven Image-Language Fusion for Correcting Inverse Age\n  Estimation", "author": "Yuntao Shou and Wei Ai and Tao Meng and Keqin Li", "abstract": "  The age estimation task aims to predict the age of an individual by analyzing\nfacial features in an image. The development of age estimation can improve the\nefficiency and accuracy of various applications (e.g., age verification and\nsecure access control, etc.). In recent years, contrastive language-image\npre-training (CLIP) has been widely used in various multimodal tasks and has\nmade some progress in the field of age estimation. However, existing CLIP-based\nage estimation methods require high memory usage (quadratic complexity) when\nglobally modeling images, and lack an error feedback mechanism to prompt the\nmodel about the quality of age prediction results. To tackle the above issues,\nwe propose a novel CLIP-driven Image-Language Fusion for Correcting Inverse Age\nEstimation (CILF-CIAE). Specifically, we first introduce the CLIP model to\nextract image features and text semantic information respectively, and map them\ninto a highly semantically aligned high-dimensional feature space. Next, we\ndesigned a new Transformer architecture (i.e., FourierFormer) to achieve\nchannel evolution and spatial interaction of images, and to fuse image and text\nsemantic information. Compared with the quadratic complexity of the attention\nmechanism, the proposed Fourierformer is of linear log complexity. To further\nnarrow the semantic gap between image and text features, we utilize an\nefficient contrastive multimodal learning module that supervises the multimodal\nfusion process of FourierFormer through contrastive loss for image-text\nmatching, thereby improving the interaction effect between different\nmodalities. Finally, we introduce reversible age estimation, which uses\nend-to-end error feedback to reduce the error rate of age predictions. Through\nextensive experiments on multiple data sets, CILF-CIAE has achieved better age\nprediction results.\n", "link": "http://arxiv.org/abs/2312.01758v2", "date": "2024-07-01", "relevancy": 2.671, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.545}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.537}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CILF-CIAE%3A%20CLIP-driven%20Image-Language%20Fusion%20for%20Correcting%20Inverse%20Age%0A%20%20Estimation&body=Title%3A%20CILF-CIAE%3A%20CLIP-driven%20Image-Language%20Fusion%20for%20Correcting%20Inverse%20Age%0A%20%20Estimation%0AAuthor%3A%20Yuntao%20Shou%20and%20Wei%20Ai%20and%20Tao%20Meng%20and%20Keqin%20Li%0AAbstract%3A%20%20%20The%20age%20estimation%20task%20aims%20to%20predict%20the%20age%20of%20an%20individual%20by%20analyzing%0Afacial%20features%20in%20an%20image.%20The%20development%20of%20age%20estimation%20can%20improve%20the%0Aefficiency%20and%20accuracy%20of%20various%20applications%20%28e.g.%2C%20age%20verification%20and%0Asecure%20access%20control%2C%20etc.%29.%20In%20recent%20years%2C%20contrastive%20language-image%0Apre-training%20%28CLIP%29%20has%20been%20widely%20used%20in%20various%20multimodal%20tasks%20and%20has%0Amade%20some%20progress%20in%20the%20field%20of%20age%20estimation.%20However%2C%20existing%20CLIP-based%0Aage%20estimation%20methods%20require%20high%20memory%20usage%20%28quadratic%20complexity%29%20when%0Aglobally%20modeling%20images%2C%20and%20lack%20an%20error%20feedback%20mechanism%20to%20prompt%20the%0Amodel%20about%20the%20quality%20of%20age%20prediction%20results.%20To%20tackle%20the%20above%20issues%2C%0Awe%20propose%20a%20novel%20CLIP-driven%20Image-Language%20Fusion%20for%20Correcting%20Inverse%20Age%0AEstimation%20%28CILF-CIAE%29.%20Specifically%2C%20we%20first%20introduce%20the%20CLIP%20model%20to%0Aextract%20image%20features%20and%20text%20semantic%20information%20respectively%2C%20and%20map%20them%0Ainto%20a%20highly%20semantically%20aligned%20high-dimensional%20feature%20space.%20Next%2C%20we%0Adesigned%20a%20new%20Transformer%20architecture%20%28i.e.%2C%20FourierFormer%29%20to%20achieve%0Achannel%20evolution%20and%20spatial%20interaction%20of%20images%2C%20and%20to%20fuse%20image%20and%20text%0Asemantic%20information.%20Compared%20with%20the%20quadratic%20complexity%20of%20the%20attention%0Amechanism%2C%20the%20proposed%20Fourierformer%20is%20of%20linear%20log%20complexity.%20To%20further%0Anarrow%20the%20semantic%20gap%20between%20image%20and%20text%20features%2C%20we%20utilize%20an%0Aefficient%20contrastive%20multimodal%20learning%20module%20that%20supervises%20the%20multimodal%0Afusion%20process%20of%20FourierFormer%20through%20contrastive%20loss%20for%20image-text%0Amatching%2C%20thereby%20improving%20the%20interaction%20effect%20between%20different%0Amodalities.%20Finally%2C%20we%20introduce%20reversible%20age%20estimation%2C%20which%20uses%0Aend-to-end%20error%20feedback%20to%20reduce%20the%20error%20rate%20of%20age%20predictions.%20Through%0Aextensive%20experiments%20on%20multiple%20data%20sets%2C%20CILF-CIAE%20has%20achieved%20better%20age%0Aprediction%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.01758v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCILF-CIAE%253A%2520CLIP-driven%2520Image-Language%2520Fusion%2520for%2520Correcting%2520Inverse%2520Age%250A%2520%2520Estimation%26entry.906535625%3DYuntao%2520Shou%2520and%2520Wei%2520Ai%2520and%2520Tao%2520Meng%2520and%2520Keqin%2520Li%26entry.1292438233%3D%2520%2520The%2520age%2520estimation%2520task%2520aims%2520to%2520predict%2520the%2520age%2520of%2520an%2520individual%2520by%2520analyzing%250Afacial%2520features%2520in%2520an%2520image.%2520The%2520development%2520of%2520age%2520estimation%2520can%2520improve%2520the%250Aefficiency%2520and%2520accuracy%2520of%2520various%2520applications%2520%2528e.g.%252C%2520age%2520verification%2520and%250Asecure%2520access%2520control%252C%2520etc.%2529.%2520In%2520recent%2520years%252C%2520contrastive%2520language-image%250Apre-training%2520%2528CLIP%2529%2520has%2520been%2520widely%2520used%2520in%2520various%2520multimodal%2520tasks%2520and%2520has%250Amade%2520some%2520progress%2520in%2520the%2520field%2520of%2520age%2520estimation.%2520However%252C%2520existing%2520CLIP-based%250Aage%2520estimation%2520methods%2520require%2520high%2520memory%2520usage%2520%2528quadratic%2520complexity%2529%2520when%250Aglobally%2520modeling%2520images%252C%2520and%2520lack%2520an%2520error%2520feedback%2520mechanism%2520to%2520prompt%2520the%250Amodel%2520about%2520the%2520quality%2520of%2520age%2520prediction%2520results.%2520To%2520tackle%2520the%2520above%2520issues%252C%250Awe%2520propose%2520a%2520novel%2520CLIP-driven%2520Image-Language%2520Fusion%2520for%2520Correcting%2520Inverse%2520Age%250AEstimation%2520%2528CILF-CIAE%2529.%2520Specifically%252C%2520we%2520first%2520introduce%2520the%2520CLIP%2520model%2520to%250Aextract%2520image%2520features%2520and%2520text%2520semantic%2520information%2520respectively%252C%2520and%2520map%2520them%250Ainto%2520a%2520highly%2520semantically%2520aligned%2520high-dimensional%2520feature%2520space.%2520Next%252C%2520we%250Adesigned%2520a%2520new%2520Transformer%2520architecture%2520%2528i.e.%252C%2520FourierFormer%2529%2520to%2520achieve%250Achannel%2520evolution%2520and%2520spatial%2520interaction%2520of%2520images%252C%2520and%2520to%2520fuse%2520image%2520and%2520text%250Asemantic%2520information.%2520Compared%2520with%2520the%2520quadratic%2520complexity%2520of%2520the%2520attention%250Amechanism%252C%2520the%2520proposed%2520Fourierformer%2520is%2520of%2520linear%2520log%2520complexity.%2520To%2520further%250Anarrow%2520the%2520semantic%2520gap%2520between%2520image%2520and%2520text%2520features%252C%2520we%2520utilize%2520an%250Aefficient%2520contrastive%2520multimodal%2520learning%2520module%2520that%2520supervises%2520the%2520multimodal%250Afusion%2520process%2520of%2520FourierFormer%2520through%2520contrastive%2520loss%2520for%2520image-text%250Amatching%252C%2520thereby%2520improving%2520the%2520interaction%2520effect%2520between%2520different%250Amodalities.%2520Finally%252C%2520we%2520introduce%2520reversible%2520age%2520estimation%252C%2520which%2520uses%250Aend-to-end%2520error%2520feedback%2520to%2520reduce%2520the%2520error%2520rate%2520of%2520age%2520predictions.%2520Through%250Aextensive%2520experiments%2520on%2520multiple%2520data%2520sets%252C%2520CILF-CIAE%2520has%2520achieved%2520better%2520age%250Aprediction%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.01758v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CILF-CIAE%3A%20CLIP-driven%20Image-Language%20Fusion%20for%20Correcting%20Inverse%20Age%0A%20%20Estimation&entry.906535625=Yuntao%20Shou%20and%20Wei%20Ai%20and%20Tao%20Meng%20and%20Keqin%20Li&entry.1292438233=%20%20The%20age%20estimation%20task%20aims%20to%20predict%20the%20age%20of%20an%20individual%20by%20analyzing%0Afacial%20features%20in%20an%20image.%20The%20development%20of%20age%20estimation%20can%20improve%20the%0Aefficiency%20and%20accuracy%20of%20various%20applications%20%28e.g.%2C%20age%20verification%20and%0Asecure%20access%20control%2C%20etc.%29.%20In%20recent%20years%2C%20contrastive%20language-image%0Apre-training%20%28CLIP%29%20has%20been%20widely%20used%20in%20various%20multimodal%20tasks%20and%20has%0Amade%20some%20progress%20in%20the%20field%20of%20age%20estimation.%20However%2C%20existing%20CLIP-based%0Aage%20estimation%20methods%20require%20high%20memory%20usage%20%28quadratic%20complexity%29%20when%0Aglobally%20modeling%20images%2C%20and%20lack%20an%20error%20feedback%20mechanism%20to%20prompt%20the%0Amodel%20about%20the%20quality%20of%20age%20prediction%20results.%20To%20tackle%20the%20above%20issues%2C%0Awe%20propose%20a%20novel%20CLIP-driven%20Image-Language%20Fusion%20for%20Correcting%20Inverse%20Age%0AEstimation%20%28CILF-CIAE%29.%20Specifically%2C%20we%20first%20introduce%20the%20CLIP%20model%20to%0Aextract%20image%20features%20and%20text%20semantic%20information%20respectively%2C%20and%20map%20them%0Ainto%20a%20highly%20semantically%20aligned%20high-dimensional%20feature%20space.%20Next%2C%20we%0Adesigned%20a%20new%20Transformer%20architecture%20%28i.e.%2C%20FourierFormer%29%20to%20achieve%0Achannel%20evolution%20and%20spatial%20interaction%20of%20images%2C%20and%20to%20fuse%20image%20and%20text%0Asemantic%20information.%20Compared%20with%20the%20quadratic%20complexity%20of%20the%20attention%0Amechanism%2C%20the%20proposed%20Fourierformer%20is%20of%20linear%20log%20complexity.%20To%20further%0Anarrow%20the%20semantic%20gap%20between%20image%20and%20text%20features%2C%20we%20utilize%20an%0Aefficient%20contrastive%20multimodal%20learning%20module%20that%20supervises%20the%20multimodal%0Afusion%20process%20of%20FourierFormer%20through%20contrastive%20loss%20for%20image-text%0Amatching%2C%20thereby%20improving%20the%20interaction%20effect%20between%20different%0Amodalities.%20Finally%2C%20we%20introduce%20reversible%20age%20estimation%2C%20which%20uses%0Aend-to-end%20error%20feedback%20to%20reduce%20the%20error%20rate%20of%20age%20predictions.%20Through%0Aextensive%20experiments%20on%20multiple%20data%20sets%2C%20CILF-CIAE%20has%20achieved%20better%20age%0Aprediction%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.01758v2&entry.124074799=Read"},
{"title": "Local-Aware Global Attention Network for Person Re-Identification Based\n  on Body and Hand Images", "author": "Nathanael L. Baisa", "abstract": "  Learning representative, robust and discriminative information from images is\nessential for effective person re-identification (Re-Id). In this paper, we\npropose a compound approach for end-to-end discriminative deep feature learning\nfor person Re-Id based on both body and hand images. We carefully design the\nLocal-Aware Global Attention Network (LAGA-Net), a multi-branch deep network\narchitecture consisting of one branch for spatial attention, one branch for\nchannel attention, one branch for global feature representations and another\nbranch for local feature representations. The attention branches focus on the\nrelevant features of the image while suppressing the irrelevant backgrounds. In\norder to overcome the weakness of the attention mechanisms, equivariant to\npixel shuffling, we integrate relative positional encodings into the spatial\nattention module to capture the spatial positions of pixels. The global branch\nintends to preserve the global context or structural information. For the the\nlocal branch, which intends to capture the fine-grained information, we perform\nuniform partitioning to generate stripes on the conv-layer horizontally. We\nretrieve the parts by conducting a soft partition without explicitly\npartitioning the images or requiring external cues such as pose estimation. A\nset of ablation study shows that each component contributes to the increased\nperformance of the LAGA-Net. Extensive evaluations on four popular body-based\nperson Re-Id benchmarks and two publicly available hand datasets demonstrate\nthat our proposed method consistently outperforms existing state-of-the-art\nmethods.\n", "link": "http://arxiv.org/abs/2209.04821v3", "date": "2024-07-01", "relevancy": 2.655, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.556}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5367}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Local-Aware%20Global%20Attention%20Network%20for%20Person%20Re-Identification%20Based%0A%20%20on%20Body%20and%20Hand%20Images&body=Title%3A%20Local-Aware%20Global%20Attention%20Network%20for%20Person%20Re-Identification%20Based%0A%20%20on%20Body%20and%20Hand%20Images%0AAuthor%3A%20Nathanael%20L.%20Baisa%0AAbstract%3A%20%20%20Learning%20representative%2C%20robust%20and%20discriminative%20information%20from%20images%20is%0Aessential%20for%20effective%20person%20re-identification%20%28Re-Id%29.%20In%20this%20paper%2C%20we%0Apropose%20a%20compound%20approach%20for%20end-to-end%20discriminative%20deep%20feature%20learning%0Afor%20person%20Re-Id%20based%20on%20both%20body%20and%20hand%20images.%20We%20carefully%20design%20the%0ALocal-Aware%20Global%20Attention%20Network%20%28LAGA-Net%29%2C%20a%20multi-branch%20deep%20network%0Aarchitecture%20consisting%20of%20one%20branch%20for%20spatial%20attention%2C%20one%20branch%20for%0Achannel%20attention%2C%20one%20branch%20for%20global%20feature%20representations%20and%20another%0Abranch%20for%20local%20feature%20representations.%20The%20attention%20branches%20focus%20on%20the%0Arelevant%20features%20of%20the%20image%20while%20suppressing%20the%20irrelevant%20backgrounds.%20In%0Aorder%20to%20overcome%20the%20weakness%20of%20the%20attention%20mechanisms%2C%20equivariant%20to%0Apixel%20shuffling%2C%20we%20integrate%20relative%20positional%20encodings%20into%20the%20spatial%0Aattention%20module%20to%20capture%20the%20spatial%20positions%20of%20pixels.%20The%20global%20branch%0Aintends%20to%20preserve%20the%20global%20context%20or%20structural%20information.%20For%20the%20the%0Alocal%20branch%2C%20which%20intends%20to%20capture%20the%20fine-grained%20information%2C%20we%20perform%0Auniform%20partitioning%20to%20generate%20stripes%20on%20the%20conv-layer%20horizontally.%20We%0Aretrieve%20the%20parts%20by%20conducting%20a%20soft%20partition%20without%20explicitly%0Apartitioning%20the%20images%20or%20requiring%20external%20cues%20such%20as%20pose%20estimation.%20A%0Aset%20of%20ablation%20study%20shows%20that%20each%20component%20contributes%20to%20the%20increased%0Aperformance%20of%20the%20LAGA-Net.%20Extensive%20evaluations%20on%20four%20popular%20body-based%0Aperson%20Re-Id%20benchmarks%20and%20two%20publicly%20available%20hand%20datasets%20demonstrate%0Athat%20our%20proposed%20method%20consistently%20outperforms%20existing%20state-of-the-art%0Amethods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.04821v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocal-Aware%2520Global%2520Attention%2520Network%2520for%2520Person%2520Re-Identification%2520Based%250A%2520%2520on%2520Body%2520and%2520Hand%2520Images%26entry.906535625%3DNathanael%2520L.%2520Baisa%26entry.1292438233%3D%2520%2520Learning%2520representative%252C%2520robust%2520and%2520discriminative%2520information%2520from%2520images%2520is%250Aessential%2520for%2520effective%2520person%2520re-identification%2520%2528Re-Id%2529.%2520In%2520this%2520paper%252C%2520we%250Apropose%2520a%2520compound%2520approach%2520for%2520end-to-end%2520discriminative%2520deep%2520feature%2520learning%250Afor%2520person%2520Re-Id%2520based%2520on%2520both%2520body%2520and%2520hand%2520images.%2520We%2520carefully%2520design%2520the%250ALocal-Aware%2520Global%2520Attention%2520Network%2520%2528LAGA-Net%2529%252C%2520a%2520multi-branch%2520deep%2520network%250Aarchitecture%2520consisting%2520of%2520one%2520branch%2520for%2520spatial%2520attention%252C%2520one%2520branch%2520for%250Achannel%2520attention%252C%2520one%2520branch%2520for%2520global%2520feature%2520representations%2520and%2520another%250Abranch%2520for%2520local%2520feature%2520representations.%2520The%2520attention%2520branches%2520focus%2520on%2520the%250Arelevant%2520features%2520of%2520the%2520image%2520while%2520suppressing%2520the%2520irrelevant%2520backgrounds.%2520In%250Aorder%2520to%2520overcome%2520the%2520weakness%2520of%2520the%2520attention%2520mechanisms%252C%2520equivariant%2520to%250Apixel%2520shuffling%252C%2520we%2520integrate%2520relative%2520positional%2520encodings%2520into%2520the%2520spatial%250Aattention%2520module%2520to%2520capture%2520the%2520spatial%2520positions%2520of%2520pixels.%2520The%2520global%2520branch%250Aintends%2520to%2520preserve%2520the%2520global%2520context%2520or%2520structural%2520information.%2520For%2520the%2520the%250Alocal%2520branch%252C%2520which%2520intends%2520to%2520capture%2520the%2520fine-grained%2520information%252C%2520we%2520perform%250Auniform%2520partitioning%2520to%2520generate%2520stripes%2520on%2520the%2520conv-layer%2520horizontally.%2520We%250Aretrieve%2520the%2520parts%2520by%2520conducting%2520a%2520soft%2520partition%2520without%2520explicitly%250Apartitioning%2520the%2520images%2520or%2520requiring%2520external%2520cues%2520such%2520as%2520pose%2520estimation.%2520A%250Aset%2520of%2520ablation%2520study%2520shows%2520that%2520each%2520component%2520contributes%2520to%2520the%2520increased%250Aperformance%2520of%2520the%2520LAGA-Net.%2520Extensive%2520evaluations%2520on%2520four%2520popular%2520body-based%250Aperson%2520Re-Id%2520benchmarks%2520and%2520two%2520publicly%2520available%2520hand%2520datasets%2520demonstrate%250Athat%2520our%2520proposed%2520method%2520consistently%2520outperforms%2520existing%2520state-of-the-art%250Amethods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.04821v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Local-Aware%20Global%20Attention%20Network%20for%20Person%20Re-Identification%20Based%0A%20%20on%20Body%20and%20Hand%20Images&entry.906535625=Nathanael%20L.%20Baisa&entry.1292438233=%20%20Learning%20representative%2C%20robust%20and%20discriminative%20information%20from%20images%20is%0Aessential%20for%20effective%20person%20re-identification%20%28Re-Id%29.%20In%20this%20paper%2C%20we%0Apropose%20a%20compound%20approach%20for%20end-to-end%20discriminative%20deep%20feature%20learning%0Afor%20person%20Re-Id%20based%20on%20both%20body%20and%20hand%20images.%20We%20carefully%20design%20the%0ALocal-Aware%20Global%20Attention%20Network%20%28LAGA-Net%29%2C%20a%20multi-branch%20deep%20network%0Aarchitecture%20consisting%20of%20one%20branch%20for%20spatial%20attention%2C%20one%20branch%20for%0Achannel%20attention%2C%20one%20branch%20for%20global%20feature%20representations%20and%20another%0Abranch%20for%20local%20feature%20representations.%20The%20attention%20branches%20focus%20on%20the%0Arelevant%20features%20of%20the%20image%20while%20suppressing%20the%20irrelevant%20backgrounds.%20In%0Aorder%20to%20overcome%20the%20weakness%20of%20the%20attention%20mechanisms%2C%20equivariant%20to%0Apixel%20shuffling%2C%20we%20integrate%20relative%20positional%20encodings%20into%20the%20spatial%0Aattention%20module%20to%20capture%20the%20spatial%20positions%20of%20pixels.%20The%20global%20branch%0Aintends%20to%20preserve%20the%20global%20context%20or%20structural%20information.%20For%20the%20the%0Alocal%20branch%2C%20which%20intends%20to%20capture%20the%20fine-grained%20information%2C%20we%20perform%0Auniform%20partitioning%20to%20generate%20stripes%20on%20the%20conv-layer%20horizontally.%20We%0Aretrieve%20the%20parts%20by%20conducting%20a%20soft%20partition%20without%20explicitly%0Apartitioning%20the%20images%20or%20requiring%20external%20cues%20such%20as%20pose%20estimation.%20A%0Aset%20of%20ablation%20study%20shows%20that%20each%20component%20contributes%20to%20the%20increased%0Aperformance%20of%20the%20LAGA-Net.%20Extensive%20evaluations%20on%20four%20popular%20body-based%0Aperson%20Re-Id%20benchmarks%20and%20two%20publicly%20available%20hand%20datasets%20demonstrate%0Athat%20our%20proposed%20method%20consistently%20outperforms%20existing%20state-of-the-art%0Amethods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.04821v3&entry.124074799=Read"},
{"title": "A Geometric Algorithm for Tubular Shape Reconstruction from Skeletal\n  Representation", "author": "Guoqing Zhang and Yang Li", "abstract": "  We introduce a novel approach for the reconstruction of tubular shapes from\nskeletal representations. Our method processes all skeletal points as a whole,\neliminating the need for splitting input structure into multiple segments. We\nrepresent the tubular shape as a truncated signed distance function (TSDF) in a\nvoxel hashing manner, in which the signed distance between a voxel center and\nthe object is computed through a simple geometric algorithm. Our method does\nnot involve any surface sampling scheme or solving large matrix equations, and\ntherefore is a faster and more elegant solution for tubular shape\nreconstruction compared to other approaches. Experiments demonstrate the\nefficiency and effectiveness of the proposed method. Code is avaliable at\nhttps://github.com/wlsdzyzl/Dragon.\n", "link": "http://arxiv.org/abs/2402.12797v3", "date": "2024-07-01", "relevancy": 2.6181, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5547}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5253}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Geometric%20Algorithm%20for%20Tubular%20Shape%20Reconstruction%20from%20Skeletal%0A%20%20Representation&body=Title%3A%20A%20Geometric%20Algorithm%20for%20Tubular%20Shape%20Reconstruction%20from%20Skeletal%0A%20%20Representation%0AAuthor%3A%20Guoqing%20Zhang%20and%20Yang%20Li%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20approach%20for%20the%20reconstruction%20of%20tubular%20shapes%20from%0Askeletal%20representations.%20Our%20method%20processes%20all%20skeletal%20points%20as%20a%20whole%2C%0Aeliminating%20the%20need%20for%20splitting%20input%20structure%20into%20multiple%20segments.%20We%0Arepresent%20the%20tubular%20shape%20as%20a%20truncated%20signed%20distance%20function%20%28TSDF%29%20in%20a%0Avoxel%20hashing%20manner%2C%20in%20which%20the%20signed%20distance%20between%20a%20voxel%20center%20and%0Athe%20object%20is%20computed%20through%20a%20simple%20geometric%20algorithm.%20Our%20method%20does%0Anot%20involve%20any%20surface%20sampling%20scheme%20or%20solving%20large%20matrix%20equations%2C%20and%0Atherefore%20is%20a%20faster%20and%20more%20elegant%20solution%20for%20tubular%20shape%0Areconstruction%20compared%20to%20other%20approaches.%20Experiments%20demonstrate%20the%0Aefficiency%20and%20effectiveness%20of%20the%20proposed%20method.%20Code%20is%20avaliable%20at%0Ahttps%3A//github.com/wlsdzyzl/Dragon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.12797v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Geometric%2520Algorithm%2520for%2520Tubular%2520Shape%2520Reconstruction%2520from%2520Skeletal%250A%2520%2520Representation%26entry.906535625%3DGuoqing%2520Zhang%2520and%2520Yang%2520Li%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520approach%2520for%2520the%2520reconstruction%2520of%2520tubular%2520shapes%2520from%250Askeletal%2520representations.%2520Our%2520method%2520processes%2520all%2520skeletal%2520points%2520as%2520a%2520whole%252C%250Aeliminating%2520the%2520need%2520for%2520splitting%2520input%2520structure%2520into%2520multiple%2520segments.%2520We%250Arepresent%2520the%2520tubular%2520shape%2520as%2520a%2520truncated%2520signed%2520distance%2520function%2520%2528TSDF%2529%2520in%2520a%250Avoxel%2520hashing%2520manner%252C%2520in%2520which%2520the%2520signed%2520distance%2520between%2520a%2520voxel%2520center%2520and%250Athe%2520object%2520is%2520computed%2520through%2520a%2520simple%2520geometric%2520algorithm.%2520Our%2520method%2520does%250Anot%2520involve%2520any%2520surface%2520sampling%2520scheme%2520or%2520solving%2520large%2520matrix%2520equations%252C%2520and%250Atherefore%2520is%2520a%2520faster%2520and%2520more%2520elegant%2520solution%2520for%2520tubular%2520shape%250Areconstruction%2520compared%2520to%2520other%2520approaches.%2520Experiments%2520demonstrate%2520the%250Aefficiency%2520and%2520effectiveness%2520of%2520the%2520proposed%2520method.%2520Code%2520is%2520avaliable%2520at%250Ahttps%253A//github.com/wlsdzyzl/Dragon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.12797v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Geometric%20Algorithm%20for%20Tubular%20Shape%20Reconstruction%20from%20Skeletal%0A%20%20Representation&entry.906535625=Guoqing%20Zhang%20and%20Yang%20Li&entry.1292438233=%20%20We%20introduce%20a%20novel%20approach%20for%20the%20reconstruction%20of%20tubular%20shapes%20from%0Askeletal%20representations.%20Our%20method%20processes%20all%20skeletal%20points%20as%20a%20whole%2C%0Aeliminating%20the%20need%20for%20splitting%20input%20structure%20into%20multiple%20segments.%20We%0Arepresent%20the%20tubular%20shape%20as%20a%20truncated%20signed%20distance%20function%20%28TSDF%29%20in%20a%0Avoxel%20hashing%20manner%2C%20in%20which%20the%20signed%20distance%20between%20a%20voxel%20center%20and%0Athe%20object%20is%20computed%20through%20a%20simple%20geometric%20algorithm.%20Our%20method%20does%0Anot%20involve%20any%20surface%20sampling%20scheme%20or%20solving%20large%20matrix%20equations%2C%20and%0Atherefore%20is%20a%20faster%20and%20more%20elegant%20solution%20for%20tubular%20shape%0Areconstruction%20compared%20to%20other%20approaches.%20Experiments%20demonstrate%20the%0Aefficiency%20and%20effectiveness%20of%20the%20proposed%20method.%20Code%20is%20avaliable%20at%0Ahttps%3A//github.com/wlsdzyzl/Dragon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.12797v3&entry.124074799=Read"},
{"title": "Unleashing the Power of Meta-tuning for Few-shot Generalization Through\n  Sparse Interpolated Experts", "author": "Shengzhuang Chen and Jihoon Tack and Yunqiao Yang and Yee Whye Teh and Jonathan Richard Schwarz and Ying Wei", "abstract": "  Recent successes suggest that parameter-efficient fine-tuning of foundation\nmodels as the state-of-the-art method for transfer learning in vision,\nreplacing the rich literature of alternatives such as meta-learning. In trying\nto harness the best of both worlds, meta-tuning introduces a subsequent\noptimization stage of foundation models but has so far only shown limited\nsuccess and crucially tends to underperform on out-of-distribution (OOD) tasks.\nIn this paper, we introduce Sparse MetA-Tuning (SMAT), a method inspired by\nsparse mixture-of-experts approaches and trained to isolate subsets of\npre-trained parameters automatically for meta-tuning on each task. SMAT\nsuccessfully overcomes OOD sensitivity and delivers on the promise of enhancing\nthe transfer abilities of vision foundation models beyond parameter-efficient\nfine-tuning. We establish new state-of-the-art results on a challenging\ncombination of Meta-Dataset augmented with additional OOD tasks in both\nzero-shot and gradient-based adaptation settings. In addition, we provide a\nthorough analysis of the superiority of learned over hand-designed sparsity\npatterns for sparse expert methods and the pivotal importance of the sparsity\nlevel in balancing between in-distribution and out-of-distribution\ngeneralization. Our code is publicly available.\n", "link": "http://arxiv.org/abs/2403.08477v3", "date": "2024-07-01", "relevancy": 2.5478, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5332}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5072}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Power%20of%20Meta-tuning%20for%20Few-shot%20Generalization%20Through%0A%20%20Sparse%20Interpolated%20Experts&body=Title%3A%20Unleashing%20the%20Power%20of%20Meta-tuning%20for%20Few-shot%20Generalization%20Through%0A%20%20Sparse%20Interpolated%20Experts%0AAuthor%3A%20Shengzhuang%20Chen%20and%20Jihoon%20Tack%20and%20Yunqiao%20Yang%20and%20Yee%20Whye%20Teh%20and%20Jonathan%20Richard%20Schwarz%20and%20Ying%20Wei%0AAbstract%3A%20%20%20Recent%20successes%20suggest%20that%20parameter-efficient%20fine-tuning%20of%20foundation%0Amodels%20as%20the%20state-of-the-art%20method%20for%20transfer%20learning%20in%20vision%2C%0Areplacing%20the%20rich%20literature%20of%20alternatives%20such%20as%20meta-learning.%20In%20trying%0Ato%20harness%20the%20best%20of%20both%20worlds%2C%20meta-tuning%20introduces%20a%20subsequent%0Aoptimization%20stage%20of%20foundation%20models%20but%20has%20so%20far%20only%20shown%20limited%0Asuccess%20and%20crucially%20tends%20to%20underperform%20on%20out-of-distribution%20%28OOD%29%20tasks.%0AIn%20this%20paper%2C%20we%20introduce%20Sparse%20MetA-Tuning%20%28SMAT%29%2C%20a%20method%20inspired%20by%0Asparse%20mixture-of-experts%20approaches%20and%20trained%20to%20isolate%20subsets%20of%0Apre-trained%20parameters%20automatically%20for%20meta-tuning%20on%20each%20task.%20SMAT%0Asuccessfully%20overcomes%20OOD%20sensitivity%20and%20delivers%20on%20the%20promise%20of%20enhancing%0Athe%20transfer%20abilities%20of%20vision%20foundation%20models%20beyond%20parameter-efficient%0Afine-tuning.%20We%20establish%20new%20state-of-the-art%20results%20on%20a%20challenging%0Acombination%20of%20Meta-Dataset%20augmented%20with%20additional%20OOD%20tasks%20in%20both%0Azero-shot%20and%20gradient-based%20adaptation%20settings.%20In%20addition%2C%20we%20provide%20a%0Athorough%20analysis%20of%20the%20superiority%20of%20learned%20over%20hand-designed%20sparsity%0Apatterns%20for%20sparse%20expert%20methods%20and%20the%20pivotal%20importance%20of%20the%20sparsity%0Alevel%20in%20balancing%20between%20in-distribution%20and%20out-of-distribution%0Ageneralization.%20Our%20code%20is%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08477v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520Power%2520of%2520Meta-tuning%2520for%2520Few-shot%2520Generalization%2520Through%250A%2520%2520Sparse%2520Interpolated%2520Experts%26entry.906535625%3DShengzhuang%2520Chen%2520and%2520Jihoon%2520Tack%2520and%2520Yunqiao%2520Yang%2520and%2520Yee%2520Whye%2520Teh%2520and%2520Jonathan%2520Richard%2520Schwarz%2520and%2520Ying%2520Wei%26entry.1292438233%3D%2520%2520Recent%2520successes%2520suggest%2520that%2520parameter-efficient%2520fine-tuning%2520of%2520foundation%250Amodels%2520as%2520the%2520state-of-the-art%2520method%2520for%2520transfer%2520learning%2520in%2520vision%252C%250Areplacing%2520the%2520rich%2520literature%2520of%2520alternatives%2520such%2520as%2520meta-learning.%2520In%2520trying%250Ato%2520harness%2520the%2520best%2520of%2520both%2520worlds%252C%2520meta-tuning%2520introduces%2520a%2520subsequent%250Aoptimization%2520stage%2520of%2520foundation%2520models%2520but%2520has%2520so%2520far%2520only%2520shown%2520limited%250Asuccess%2520and%2520crucially%2520tends%2520to%2520underperform%2520on%2520out-of-distribution%2520%2528OOD%2529%2520tasks.%250AIn%2520this%2520paper%252C%2520we%2520introduce%2520Sparse%2520MetA-Tuning%2520%2528SMAT%2529%252C%2520a%2520method%2520inspired%2520by%250Asparse%2520mixture-of-experts%2520approaches%2520and%2520trained%2520to%2520isolate%2520subsets%2520of%250Apre-trained%2520parameters%2520automatically%2520for%2520meta-tuning%2520on%2520each%2520task.%2520SMAT%250Asuccessfully%2520overcomes%2520OOD%2520sensitivity%2520and%2520delivers%2520on%2520the%2520promise%2520of%2520enhancing%250Athe%2520transfer%2520abilities%2520of%2520vision%2520foundation%2520models%2520beyond%2520parameter-efficient%250Afine-tuning.%2520We%2520establish%2520new%2520state-of-the-art%2520results%2520on%2520a%2520challenging%250Acombination%2520of%2520Meta-Dataset%2520augmented%2520with%2520additional%2520OOD%2520tasks%2520in%2520both%250Azero-shot%2520and%2520gradient-based%2520adaptation%2520settings.%2520In%2520addition%252C%2520we%2520provide%2520a%250Athorough%2520analysis%2520of%2520the%2520superiority%2520of%2520learned%2520over%2520hand-designed%2520sparsity%250Apatterns%2520for%2520sparse%2520expert%2520methods%2520and%2520the%2520pivotal%2520importance%2520of%2520the%2520sparsity%250Alevel%2520in%2520balancing%2520between%2520in-distribution%2520and%2520out-of-distribution%250Ageneralization.%2520Our%2520code%2520is%2520publicly%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08477v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Power%20of%20Meta-tuning%20for%20Few-shot%20Generalization%20Through%0A%20%20Sparse%20Interpolated%20Experts&entry.906535625=Shengzhuang%20Chen%20and%20Jihoon%20Tack%20and%20Yunqiao%20Yang%20and%20Yee%20Whye%20Teh%20and%20Jonathan%20Richard%20Schwarz%20and%20Ying%20Wei&entry.1292438233=%20%20Recent%20successes%20suggest%20that%20parameter-efficient%20fine-tuning%20of%20foundation%0Amodels%20as%20the%20state-of-the-art%20method%20for%20transfer%20learning%20in%20vision%2C%0Areplacing%20the%20rich%20literature%20of%20alternatives%20such%20as%20meta-learning.%20In%20trying%0Ato%20harness%20the%20best%20of%20both%20worlds%2C%20meta-tuning%20introduces%20a%20subsequent%0Aoptimization%20stage%20of%20foundation%20models%20but%20has%20so%20far%20only%20shown%20limited%0Asuccess%20and%20crucially%20tends%20to%20underperform%20on%20out-of-distribution%20%28OOD%29%20tasks.%0AIn%20this%20paper%2C%20we%20introduce%20Sparse%20MetA-Tuning%20%28SMAT%29%2C%20a%20method%20inspired%20by%0Asparse%20mixture-of-experts%20approaches%20and%20trained%20to%20isolate%20subsets%20of%0Apre-trained%20parameters%20automatically%20for%20meta-tuning%20on%20each%20task.%20SMAT%0Asuccessfully%20overcomes%20OOD%20sensitivity%20and%20delivers%20on%20the%20promise%20of%20enhancing%0Athe%20transfer%20abilities%20of%20vision%20foundation%20models%20beyond%20parameter-efficient%0Afine-tuning.%20We%20establish%20new%20state-of-the-art%20results%20on%20a%20challenging%0Acombination%20of%20Meta-Dataset%20augmented%20with%20additional%20OOD%20tasks%20in%20both%0Azero-shot%20and%20gradient-based%20adaptation%20settings.%20In%20addition%2C%20we%20provide%20a%0Athorough%20analysis%20of%20the%20superiority%20of%20learned%20over%20hand-designed%20sparsity%0Apatterns%20for%20sparse%20expert%20methods%20and%20the%20pivotal%20importance%20of%20the%20sparsity%0Alevel%20in%20balancing%20between%20in-distribution%20and%20out-of-distribution%0Ageneralization.%20Our%20code%20is%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08477v3&entry.124074799=Read"},
{"title": "Framing image registration as a landmark detection problem for\n  label-noise-aware task representation (HitR)", "author": "Diana Waldmannstetter and Ivan Ezhov and Benedikt Wiestler and Francesco Campi and Ivan Kukuljan and Stefan Ehrlich and Shankeeth Vinayahalingam and Bhakti Baheti and Satrajit Chakrabarty and Ujjwal Baid and Spyridon Bakas and Julian Schwarting and Marie Metz and Jan S. Kirschke and Daniel Rueckert and Rolf A. Heckemann and Marie Piraud and Bjoern H. Menze and Florian Kofler", "abstract": "  Accurate image registration is pivotal in biomedical image analysis, where\nselecting suitable registration algorithms demands careful consideration. While\nnumerous algorithms are available, the evaluation metrics to assess their\nperformance have remained relatively static. This study addresses this\nchallenge by introducing a novel evaluation metric termed Landmark Hit Rate\n(HitR), which focuses on the clinical relevance of image registration accuracy.\nUnlike traditional metrics such as Target Registration Error, which emphasize\nsubresolution differences, HitR considers whether registration algorithms\nsuccessfully position landmarks within defined confidence zones. This paradigm\nshift acknowledges the inherent annotation noise in medical images, allowing\nfor more meaningful assessments. To equip HitR with label-noise-awareness, we\npropose defining these confidence zones based on an Inter-rater Variance\nanalysis. Consequently, hit rate curves are computed for varying landmark zone\nsizes, enabling performance measurement for a task-specific level of accuracy.\nOur approach offers a more realistic and meaningful assessment of image\nregistration algorithms, reflecting their suitability for clinical and\nbiomedical applications.\n", "link": "http://arxiv.org/abs/2308.01318v2", "date": "2024-07-01", "relevancy": 2.5027, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5052}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5003}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Framing%20image%20registration%20as%20a%20landmark%20detection%20problem%20for%0A%20%20label-noise-aware%20task%20representation%20%28HitR%29&body=Title%3A%20Framing%20image%20registration%20as%20a%20landmark%20detection%20problem%20for%0A%20%20label-noise-aware%20task%20representation%20%28HitR%29%0AAuthor%3A%20Diana%20Waldmannstetter%20and%20Ivan%20Ezhov%20and%20Benedikt%20Wiestler%20and%20Francesco%20Campi%20and%20Ivan%20Kukuljan%20and%20Stefan%20Ehrlich%20and%20Shankeeth%20Vinayahalingam%20and%20Bhakti%20Baheti%20and%20Satrajit%20Chakrabarty%20and%20Ujjwal%20Baid%20and%20Spyridon%20Bakas%20and%20Julian%20Schwarting%20and%20Marie%20Metz%20and%20Jan%20S.%20Kirschke%20and%20Daniel%20Rueckert%20and%20Rolf%20A.%20Heckemann%20and%20Marie%20Piraud%20and%20Bjoern%20H.%20Menze%20and%20Florian%20Kofler%0AAbstract%3A%20%20%20Accurate%20image%20registration%20is%20pivotal%20in%20biomedical%20image%20analysis%2C%20where%0Aselecting%20suitable%20registration%20algorithms%20demands%20careful%20consideration.%20While%0Anumerous%20algorithms%20are%20available%2C%20the%20evaluation%20metrics%20to%20assess%20their%0Aperformance%20have%20remained%20relatively%20static.%20This%20study%20addresses%20this%0Achallenge%20by%20introducing%20a%20novel%20evaluation%20metric%20termed%20Landmark%20Hit%20Rate%0A%28HitR%29%2C%20which%20focuses%20on%20the%20clinical%20relevance%20of%20image%20registration%20accuracy.%0AUnlike%20traditional%20metrics%20such%20as%20Target%20Registration%20Error%2C%20which%20emphasize%0Asubresolution%20differences%2C%20HitR%20considers%20whether%20registration%20algorithms%0Asuccessfully%20position%20landmarks%20within%20defined%20confidence%20zones.%20This%20paradigm%0Ashift%20acknowledges%20the%20inherent%20annotation%20noise%20in%20medical%20images%2C%20allowing%0Afor%20more%20meaningful%20assessments.%20To%20equip%20HitR%20with%20label-noise-awareness%2C%20we%0Apropose%20defining%20these%20confidence%20zones%20based%20on%20an%20Inter-rater%20Variance%0Aanalysis.%20Consequently%2C%20hit%20rate%20curves%20are%20computed%20for%20varying%20landmark%20zone%0Asizes%2C%20enabling%20performance%20measurement%20for%20a%20task-specific%20level%20of%20accuracy.%0AOur%20approach%20offers%20a%20more%20realistic%20and%20meaningful%20assessment%20of%20image%0Aregistration%20algorithms%2C%20reflecting%20their%20suitability%20for%20clinical%20and%0Abiomedical%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.01318v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFraming%2520image%2520registration%2520as%2520a%2520landmark%2520detection%2520problem%2520for%250A%2520%2520label-noise-aware%2520task%2520representation%2520%2528HitR%2529%26entry.906535625%3DDiana%2520Waldmannstetter%2520and%2520Ivan%2520Ezhov%2520and%2520Benedikt%2520Wiestler%2520and%2520Francesco%2520Campi%2520and%2520Ivan%2520Kukuljan%2520and%2520Stefan%2520Ehrlich%2520and%2520Shankeeth%2520Vinayahalingam%2520and%2520Bhakti%2520Baheti%2520and%2520Satrajit%2520Chakrabarty%2520and%2520Ujjwal%2520Baid%2520and%2520Spyridon%2520Bakas%2520and%2520Julian%2520Schwarting%2520and%2520Marie%2520Metz%2520and%2520Jan%2520S.%2520Kirschke%2520and%2520Daniel%2520Rueckert%2520and%2520Rolf%2520A.%2520Heckemann%2520and%2520Marie%2520Piraud%2520and%2520Bjoern%2520H.%2520Menze%2520and%2520Florian%2520Kofler%26entry.1292438233%3D%2520%2520Accurate%2520image%2520registration%2520is%2520pivotal%2520in%2520biomedical%2520image%2520analysis%252C%2520where%250Aselecting%2520suitable%2520registration%2520algorithms%2520demands%2520careful%2520consideration.%2520While%250Anumerous%2520algorithms%2520are%2520available%252C%2520the%2520evaluation%2520metrics%2520to%2520assess%2520their%250Aperformance%2520have%2520remained%2520relatively%2520static.%2520This%2520study%2520addresses%2520this%250Achallenge%2520by%2520introducing%2520a%2520novel%2520evaluation%2520metric%2520termed%2520Landmark%2520Hit%2520Rate%250A%2528HitR%2529%252C%2520which%2520focuses%2520on%2520the%2520clinical%2520relevance%2520of%2520image%2520registration%2520accuracy.%250AUnlike%2520traditional%2520metrics%2520such%2520as%2520Target%2520Registration%2520Error%252C%2520which%2520emphasize%250Asubresolution%2520differences%252C%2520HitR%2520considers%2520whether%2520registration%2520algorithms%250Asuccessfully%2520position%2520landmarks%2520within%2520defined%2520confidence%2520zones.%2520This%2520paradigm%250Ashift%2520acknowledges%2520the%2520inherent%2520annotation%2520noise%2520in%2520medical%2520images%252C%2520allowing%250Afor%2520more%2520meaningful%2520assessments.%2520To%2520equip%2520HitR%2520with%2520label-noise-awareness%252C%2520we%250Apropose%2520defining%2520these%2520confidence%2520zones%2520based%2520on%2520an%2520Inter-rater%2520Variance%250Aanalysis.%2520Consequently%252C%2520hit%2520rate%2520curves%2520are%2520computed%2520for%2520varying%2520landmark%2520zone%250Asizes%252C%2520enabling%2520performance%2520measurement%2520for%2520a%2520task-specific%2520level%2520of%2520accuracy.%250AOur%2520approach%2520offers%2520a%2520more%2520realistic%2520and%2520meaningful%2520assessment%2520of%2520image%250Aregistration%2520algorithms%252C%2520reflecting%2520their%2520suitability%2520for%2520clinical%2520and%250Abiomedical%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.01318v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Framing%20image%20registration%20as%20a%20landmark%20detection%20problem%20for%0A%20%20label-noise-aware%20task%20representation%20%28HitR%29&entry.906535625=Diana%20Waldmannstetter%20and%20Ivan%20Ezhov%20and%20Benedikt%20Wiestler%20and%20Francesco%20Campi%20and%20Ivan%20Kukuljan%20and%20Stefan%20Ehrlich%20and%20Shankeeth%20Vinayahalingam%20and%20Bhakti%20Baheti%20and%20Satrajit%20Chakrabarty%20and%20Ujjwal%20Baid%20and%20Spyridon%20Bakas%20and%20Julian%20Schwarting%20and%20Marie%20Metz%20and%20Jan%20S.%20Kirschke%20and%20Daniel%20Rueckert%20and%20Rolf%20A.%20Heckemann%20and%20Marie%20Piraud%20and%20Bjoern%20H.%20Menze%20and%20Florian%20Kofler&entry.1292438233=%20%20Accurate%20image%20registration%20is%20pivotal%20in%20biomedical%20image%20analysis%2C%20where%0Aselecting%20suitable%20registration%20algorithms%20demands%20careful%20consideration.%20While%0Anumerous%20algorithms%20are%20available%2C%20the%20evaluation%20metrics%20to%20assess%20their%0Aperformance%20have%20remained%20relatively%20static.%20This%20study%20addresses%20this%0Achallenge%20by%20introducing%20a%20novel%20evaluation%20metric%20termed%20Landmark%20Hit%20Rate%0A%28HitR%29%2C%20which%20focuses%20on%20the%20clinical%20relevance%20of%20image%20registration%20accuracy.%0AUnlike%20traditional%20metrics%20such%20as%20Target%20Registration%20Error%2C%20which%20emphasize%0Asubresolution%20differences%2C%20HitR%20considers%20whether%20registration%20algorithms%0Asuccessfully%20position%20landmarks%20within%20defined%20confidence%20zones.%20This%20paradigm%0Ashift%20acknowledges%20the%20inherent%20annotation%20noise%20in%20medical%20images%2C%20allowing%0Afor%20more%20meaningful%20assessments.%20To%20equip%20HitR%20with%20label-noise-awareness%2C%20we%0Apropose%20defining%20these%20confidence%20zones%20based%20on%20an%20Inter-rater%20Variance%0Aanalysis.%20Consequently%2C%20hit%20rate%20curves%20are%20computed%20for%20varying%20landmark%20zone%0Asizes%2C%20enabling%20performance%20measurement%20for%20a%20task-specific%20level%20of%20accuracy.%0AOur%20approach%20offers%20a%20more%20realistic%20and%20meaningful%20assessment%20of%20image%0Aregistration%20algorithms%2C%20reflecting%20their%20suitability%20for%20clinical%20and%0Abiomedical%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.01318v2&entry.124074799=Read"},
{"title": "Fine-tuning can cripple your foundation model; preserving features may\n  be the solution", "author": "Jishnu Mukhoti and Yarin Gal and Philip H. S. Torr and Puneet K. Dokania", "abstract": "  Pre-trained foundation models, due to their enormous capacity and exposure to\nvast amounts of data during pre-training, are known to have learned plenty of\nreal-world concepts. An important step in making these pre-trained models\neffective on downstream tasks is to fine-tune them on related datasets. While\nvarious fine-tuning methods have been devised and have been shown to be highly\neffective, we observe that a fine-tuned model's ability to recognize concepts\non tasks $\\textit{different}$ from the downstream one is reduced significantly\ncompared to its pre-trained counterpart. This is an undesirable effect of\nfine-tuning as a substantial amount of resources was used to learn these\npre-trained concepts in the first place. We call this phenomenon ''concept\nforgetting'' and via experiments show that most end-to-end fine-tuning\napproaches suffer heavily from this side effect. To this end, we propose a\nsimple fix to this problem by designing a new fine-tuning method called\n$\\textit{LDIFS}$ (short for $\\ell_2$ distance in feature space) that, while\nlearning new concepts related to the downstream task, allows a model to\npreserve its pre-trained knowledge as well. Through extensive experiments on 10\nfine-tuning tasks we show that $\\textit{LDIFS}$ significantly reduces concept\nforgetting. Additionally, we show that LDIFS is highly effective in performing\ncontinual fine-tuning on a sequence of tasks as well, in comparison with both\nfine-tuning as well as continual learning baselines.\n", "link": "http://arxiv.org/abs/2308.13320v3", "date": "2024-07-01", "relevancy": 2.4339, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5008}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4848}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4747}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-tuning%20can%20cripple%20your%20foundation%20model%3B%20preserving%20features%20may%0A%20%20be%20the%20solution&body=Title%3A%20Fine-tuning%20can%20cripple%20your%20foundation%20model%3B%20preserving%20features%20may%0A%20%20be%20the%20solution%0AAuthor%3A%20Jishnu%20Mukhoti%20and%20Yarin%20Gal%20and%20Philip%20H.%20S.%20Torr%20and%20Puneet%20K.%20Dokania%0AAbstract%3A%20%20%20Pre-trained%20foundation%20models%2C%20due%20to%20their%20enormous%20capacity%20and%20exposure%20to%0Avast%20amounts%20of%20data%20during%20pre-training%2C%20are%20known%20to%20have%20learned%20plenty%20of%0Areal-world%20concepts.%20An%20important%20step%20in%20making%20these%20pre-trained%20models%0Aeffective%20on%20downstream%20tasks%20is%20to%20fine-tune%20them%20on%20related%20datasets.%20While%0Avarious%20fine-tuning%20methods%20have%20been%20devised%20and%20have%20been%20shown%20to%20be%20highly%0Aeffective%2C%20we%20observe%20that%20a%20fine-tuned%20model%27s%20ability%20to%20recognize%20concepts%0Aon%20tasks%20%24%5Ctextit%7Bdifferent%7D%24%20from%20the%20downstream%20one%20is%20reduced%20significantly%0Acompared%20to%20its%20pre-trained%20counterpart.%20This%20is%20an%20undesirable%20effect%20of%0Afine-tuning%20as%20a%20substantial%20amount%20of%20resources%20was%20used%20to%20learn%20these%0Apre-trained%20concepts%20in%20the%20first%20place.%20We%20call%20this%20phenomenon%20%27%27concept%0Aforgetting%27%27%20and%20via%20experiments%20show%20that%20most%20end-to-end%20fine-tuning%0Aapproaches%20suffer%20heavily%20from%20this%20side%20effect.%20To%20this%20end%2C%20we%20propose%20a%0Asimple%20fix%20to%20this%20problem%20by%20designing%20a%20new%20fine-tuning%20method%20called%0A%24%5Ctextit%7BLDIFS%7D%24%20%28short%20for%20%24%5Cell_2%24%20distance%20in%20feature%20space%29%20that%2C%20while%0Alearning%20new%20concepts%20related%20to%20the%20downstream%20task%2C%20allows%20a%20model%20to%0Apreserve%20its%20pre-trained%20knowledge%20as%20well.%20Through%20extensive%20experiments%20on%2010%0Afine-tuning%20tasks%20we%20show%20that%20%24%5Ctextit%7BLDIFS%7D%24%20significantly%20reduces%20concept%0Aforgetting.%20Additionally%2C%20we%20show%20that%20LDIFS%20is%20highly%20effective%20in%20performing%0Acontinual%20fine-tuning%20on%20a%20sequence%20of%20tasks%20as%20well%2C%20in%20comparison%20with%20both%0Afine-tuning%20as%20well%20as%20continual%20learning%20baselines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.13320v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-tuning%2520can%2520cripple%2520your%2520foundation%2520model%253B%2520preserving%2520features%2520may%250A%2520%2520be%2520the%2520solution%26entry.906535625%3DJishnu%2520Mukhoti%2520and%2520Yarin%2520Gal%2520and%2520Philip%2520H.%2520S.%2520Torr%2520and%2520Puneet%2520K.%2520Dokania%26entry.1292438233%3D%2520%2520Pre-trained%2520foundation%2520models%252C%2520due%2520to%2520their%2520enormous%2520capacity%2520and%2520exposure%2520to%250Avast%2520amounts%2520of%2520data%2520during%2520pre-training%252C%2520are%2520known%2520to%2520have%2520learned%2520plenty%2520of%250Areal-world%2520concepts.%2520An%2520important%2520step%2520in%2520making%2520these%2520pre-trained%2520models%250Aeffective%2520on%2520downstream%2520tasks%2520is%2520to%2520fine-tune%2520them%2520on%2520related%2520datasets.%2520While%250Avarious%2520fine-tuning%2520methods%2520have%2520been%2520devised%2520and%2520have%2520been%2520shown%2520to%2520be%2520highly%250Aeffective%252C%2520we%2520observe%2520that%2520a%2520fine-tuned%2520model%2527s%2520ability%2520to%2520recognize%2520concepts%250Aon%2520tasks%2520%2524%255Ctextit%257Bdifferent%257D%2524%2520from%2520the%2520downstream%2520one%2520is%2520reduced%2520significantly%250Acompared%2520to%2520its%2520pre-trained%2520counterpart.%2520This%2520is%2520an%2520undesirable%2520effect%2520of%250Afine-tuning%2520as%2520a%2520substantial%2520amount%2520of%2520resources%2520was%2520used%2520to%2520learn%2520these%250Apre-trained%2520concepts%2520in%2520the%2520first%2520place.%2520We%2520call%2520this%2520phenomenon%2520%2527%2527concept%250Aforgetting%2527%2527%2520and%2520via%2520experiments%2520show%2520that%2520most%2520end-to-end%2520fine-tuning%250Aapproaches%2520suffer%2520heavily%2520from%2520this%2520side%2520effect.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%250Asimple%2520fix%2520to%2520this%2520problem%2520by%2520designing%2520a%2520new%2520fine-tuning%2520method%2520called%250A%2524%255Ctextit%257BLDIFS%257D%2524%2520%2528short%2520for%2520%2524%255Cell_2%2524%2520distance%2520in%2520feature%2520space%2529%2520that%252C%2520while%250Alearning%2520new%2520concepts%2520related%2520to%2520the%2520downstream%2520task%252C%2520allows%2520a%2520model%2520to%250Apreserve%2520its%2520pre-trained%2520knowledge%2520as%2520well.%2520Through%2520extensive%2520experiments%2520on%252010%250Afine-tuning%2520tasks%2520we%2520show%2520that%2520%2524%255Ctextit%257BLDIFS%257D%2524%2520significantly%2520reduces%2520concept%250Aforgetting.%2520Additionally%252C%2520we%2520show%2520that%2520LDIFS%2520is%2520highly%2520effective%2520in%2520performing%250Acontinual%2520fine-tuning%2520on%2520a%2520sequence%2520of%2520tasks%2520as%2520well%252C%2520in%2520comparison%2520with%2520both%250Afine-tuning%2520as%2520well%2520as%2520continual%2520learning%2520baselines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.13320v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-tuning%20can%20cripple%20your%20foundation%20model%3B%20preserving%20features%20may%0A%20%20be%20the%20solution&entry.906535625=Jishnu%20Mukhoti%20and%20Yarin%20Gal%20and%20Philip%20H.%20S.%20Torr%20and%20Puneet%20K.%20Dokania&entry.1292438233=%20%20Pre-trained%20foundation%20models%2C%20due%20to%20their%20enormous%20capacity%20and%20exposure%20to%0Avast%20amounts%20of%20data%20during%20pre-training%2C%20are%20known%20to%20have%20learned%20plenty%20of%0Areal-world%20concepts.%20An%20important%20step%20in%20making%20these%20pre-trained%20models%0Aeffective%20on%20downstream%20tasks%20is%20to%20fine-tune%20them%20on%20related%20datasets.%20While%0Avarious%20fine-tuning%20methods%20have%20been%20devised%20and%20have%20been%20shown%20to%20be%20highly%0Aeffective%2C%20we%20observe%20that%20a%20fine-tuned%20model%27s%20ability%20to%20recognize%20concepts%0Aon%20tasks%20%24%5Ctextit%7Bdifferent%7D%24%20from%20the%20downstream%20one%20is%20reduced%20significantly%0Acompared%20to%20its%20pre-trained%20counterpart.%20This%20is%20an%20undesirable%20effect%20of%0Afine-tuning%20as%20a%20substantial%20amount%20of%20resources%20was%20used%20to%20learn%20these%0Apre-trained%20concepts%20in%20the%20first%20place.%20We%20call%20this%20phenomenon%20%27%27concept%0Aforgetting%27%27%20and%20via%20experiments%20show%20that%20most%20end-to-end%20fine-tuning%0Aapproaches%20suffer%20heavily%20from%20this%20side%20effect.%20To%20this%20end%2C%20we%20propose%20a%0Asimple%20fix%20to%20this%20problem%20by%20designing%20a%20new%20fine-tuning%20method%20called%0A%24%5Ctextit%7BLDIFS%7D%24%20%28short%20for%20%24%5Cell_2%24%20distance%20in%20feature%20space%29%20that%2C%20while%0Alearning%20new%20concepts%20related%20to%20the%20downstream%20task%2C%20allows%20a%20model%20to%0Apreserve%20its%20pre-trained%20knowledge%20as%20well.%20Through%20extensive%20experiments%20on%2010%0Afine-tuning%20tasks%20we%20show%20that%20%24%5Ctextit%7BLDIFS%7D%24%20significantly%20reduces%20concept%0Aforgetting.%20Additionally%2C%20we%20show%20that%20LDIFS%20is%20highly%20effective%20in%20performing%0Acontinual%20fine-tuning%20on%20a%20sequence%20of%20tasks%20as%20well%2C%20in%20comparison%20with%20both%0Afine-tuning%20as%20well%20as%20continual%20learning%20baselines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.13320v3&entry.124074799=Read"},
{"title": "Efficient Prompt Tuning by Multi-Space Projection and Prompt Fusion", "author": "Pengxiang Lan and Enneng Yang and Yuting Liu and Guibing Guo and Linying Jiang and Jianzhe Zhao and Xingwei Wang", "abstract": "  Prompt tuning is a promising method to fine-tune a pre-trained language model\nwithout retraining its large-scale parameters. Instead, it attaches a soft\nprompt to the input text, whereby downstream tasks can be well adapted by\nmerely learning the embeddings of prompt tokens. Nevertheless, existing methods\nstill suffer from two challenges: (i) they are hard to balance accuracy and\nefficiency. A longer (shorter) soft prompt generally leads to a better(worse)\naccuracy but at the cost of more (less) training time. (ii)The performance may\nnot be consistent when adapting to different downstream tasks. We attribute it\nto the same embedding space but responsible for different requirements of\ndownstream tasks. To address these issues, we propose an Efficient Prompt\nTuning method (EPT) by multi-space projection and prompt fusion. Specifically,\nit decomposes a given soft prompt into a shorter prompt and two low-rank\nmatrices, significantly reducing the training time. Accuracy is also enhanced\nby leveraging low-rank matrices and the short prompt as additional knowledge\nsources to enrich the semantics of the original short prompt. In addition, we\nproject the soft prompt into multiple subspaces to improve the performance\nconsistency, and then adaptively learn the combination weights of different\nspaces through a gating network. Experiments on 13 natural language processing\ndownstream tasks show that our method significantly and consistently\noutperforms 11 comparison methods with the relative percentage of improvements\nup to 12.9%, and training time decreased by 14%.\n", "link": "http://arxiv.org/abs/2405.11464v2", "date": "2024-07-01", "relevancy": 2.382, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4831}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4765}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4696}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Prompt%20Tuning%20by%20Multi-Space%20Projection%20and%20Prompt%20Fusion&body=Title%3A%20Efficient%20Prompt%20Tuning%20by%20Multi-Space%20Projection%20and%20Prompt%20Fusion%0AAuthor%3A%20Pengxiang%20Lan%20and%20Enneng%20Yang%20and%20Yuting%20Liu%20and%20Guibing%20Guo%20and%20Linying%20Jiang%20and%20Jianzhe%20Zhao%20and%20Xingwei%20Wang%0AAbstract%3A%20%20%20Prompt%20tuning%20is%20a%20promising%20method%20to%20fine-tune%20a%20pre-trained%20language%20model%0Awithout%20retraining%20its%20large-scale%20parameters.%20Instead%2C%20it%20attaches%20a%20soft%0Aprompt%20to%20the%20input%20text%2C%20whereby%20downstream%20tasks%20can%20be%20well%20adapted%20by%0Amerely%20learning%20the%20embeddings%20of%20prompt%20tokens.%20Nevertheless%2C%20existing%20methods%0Astill%20suffer%20from%20two%20challenges%3A%20%28i%29%20they%20are%20hard%20to%20balance%20accuracy%20and%0Aefficiency.%20A%20longer%20%28shorter%29%20soft%20prompt%20generally%20leads%20to%20a%20better%28worse%29%0Aaccuracy%20but%20at%20the%20cost%20of%20more%20%28less%29%20training%20time.%20%28ii%29The%20performance%20may%0Anot%20be%20consistent%20when%20adapting%20to%20different%20downstream%20tasks.%20We%20attribute%20it%0Ato%20the%20same%20embedding%20space%20but%20responsible%20for%20different%20requirements%20of%0Adownstream%20tasks.%20To%20address%20these%20issues%2C%20we%20propose%20an%20Efficient%20Prompt%0ATuning%20method%20%28EPT%29%20by%20multi-space%20projection%20and%20prompt%20fusion.%20Specifically%2C%0Ait%20decomposes%20a%20given%20soft%20prompt%20into%20a%20shorter%20prompt%20and%20two%20low-rank%0Amatrices%2C%20significantly%20reducing%20the%20training%20time.%20Accuracy%20is%20also%20enhanced%0Aby%20leveraging%20low-rank%20matrices%20and%20the%20short%20prompt%20as%20additional%20knowledge%0Asources%20to%20enrich%20the%20semantics%20of%20the%20original%20short%20prompt.%20In%20addition%2C%20we%0Aproject%20the%20soft%20prompt%20into%20multiple%20subspaces%20to%20improve%20the%20performance%0Aconsistency%2C%20and%20then%20adaptively%20learn%20the%20combination%20weights%20of%20different%0Aspaces%20through%20a%20gating%20network.%20Experiments%20on%2013%20natural%20language%20processing%0Adownstream%20tasks%20show%20that%20our%20method%20significantly%20and%20consistently%0Aoutperforms%2011%20comparison%20methods%20with%20the%20relative%20percentage%20of%20improvements%0Aup%20to%2012.9%25%2C%20and%20training%20time%20decreased%20by%2014%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.11464v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Prompt%2520Tuning%2520by%2520Multi-Space%2520Projection%2520and%2520Prompt%2520Fusion%26entry.906535625%3DPengxiang%2520Lan%2520and%2520Enneng%2520Yang%2520and%2520Yuting%2520Liu%2520and%2520Guibing%2520Guo%2520and%2520Linying%2520Jiang%2520and%2520Jianzhe%2520Zhao%2520and%2520Xingwei%2520Wang%26entry.1292438233%3D%2520%2520Prompt%2520tuning%2520is%2520a%2520promising%2520method%2520to%2520fine-tune%2520a%2520pre-trained%2520language%2520model%250Awithout%2520retraining%2520its%2520large-scale%2520parameters.%2520Instead%252C%2520it%2520attaches%2520a%2520soft%250Aprompt%2520to%2520the%2520input%2520text%252C%2520whereby%2520downstream%2520tasks%2520can%2520be%2520well%2520adapted%2520by%250Amerely%2520learning%2520the%2520embeddings%2520of%2520prompt%2520tokens.%2520Nevertheless%252C%2520existing%2520methods%250Astill%2520suffer%2520from%2520two%2520challenges%253A%2520%2528i%2529%2520they%2520are%2520hard%2520to%2520balance%2520accuracy%2520and%250Aefficiency.%2520A%2520longer%2520%2528shorter%2529%2520soft%2520prompt%2520generally%2520leads%2520to%2520a%2520better%2528worse%2529%250Aaccuracy%2520but%2520at%2520the%2520cost%2520of%2520more%2520%2528less%2529%2520training%2520time.%2520%2528ii%2529The%2520performance%2520may%250Anot%2520be%2520consistent%2520when%2520adapting%2520to%2520different%2520downstream%2520tasks.%2520We%2520attribute%2520it%250Ato%2520the%2520same%2520embedding%2520space%2520but%2520responsible%2520for%2520different%2520requirements%2520of%250Adownstream%2520tasks.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520an%2520Efficient%2520Prompt%250ATuning%2520method%2520%2528EPT%2529%2520by%2520multi-space%2520projection%2520and%2520prompt%2520fusion.%2520Specifically%252C%250Ait%2520decomposes%2520a%2520given%2520soft%2520prompt%2520into%2520a%2520shorter%2520prompt%2520and%2520two%2520low-rank%250Amatrices%252C%2520significantly%2520reducing%2520the%2520training%2520time.%2520Accuracy%2520is%2520also%2520enhanced%250Aby%2520leveraging%2520low-rank%2520matrices%2520and%2520the%2520short%2520prompt%2520as%2520additional%2520knowledge%250Asources%2520to%2520enrich%2520the%2520semantics%2520of%2520the%2520original%2520short%2520prompt.%2520In%2520addition%252C%2520we%250Aproject%2520the%2520soft%2520prompt%2520into%2520multiple%2520subspaces%2520to%2520improve%2520the%2520performance%250Aconsistency%252C%2520and%2520then%2520adaptively%2520learn%2520the%2520combination%2520weights%2520of%2520different%250Aspaces%2520through%2520a%2520gating%2520network.%2520Experiments%2520on%252013%2520natural%2520language%2520processing%250Adownstream%2520tasks%2520show%2520that%2520our%2520method%2520significantly%2520and%2520consistently%250Aoutperforms%252011%2520comparison%2520methods%2520with%2520the%2520relative%2520percentage%2520of%2520improvements%250Aup%2520to%252012.9%2525%252C%2520and%2520training%2520time%2520decreased%2520by%252014%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.11464v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Prompt%20Tuning%20by%20Multi-Space%20Projection%20and%20Prompt%20Fusion&entry.906535625=Pengxiang%20Lan%20and%20Enneng%20Yang%20and%20Yuting%20Liu%20and%20Guibing%20Guo%20and%20Linying%20Jiang%20and%20Jianzhe%20Zhao%20and%20Xingwei%20Wang&entry.1292438233=%20%20Prompt%20tuning%20is%20a%20promising%20method%20to%20fine-tune%20a%20pre-trained%20language%20model%0Awithout%20retraining%20its%20large-scale%20parameters.%20Instead%2C%20it%20attaches%20a%20soft%0Aprompt%20to%20the%20input%20text%2C%20whereby%20downstream%20tasks%20can%20be%20well%20adapted%20by%0Amerely%20learning%20the%20embeddings%20of%20prompt%20tokens.%20Nevertheless%2C%20existing%20methods%0Astill%20suffer%20from%20two%20challenges%3A%20%28i%29%20they%20are%20hard%20to%20balance%20accuracy%20and%0Aefficiency.%20A%20longer%20%28shorter%29%20soft%20prompt%20generally%20leads%20to%20a%20better%28worse%29%0Aaccuracy%20but%20at%20the%20cost%20of%20more%20%28less%29%20training%20time.%20%28ii%29The%20performance%20may%0Anot%20be%20consistent%20when%20adapting%20to%20different%20downstream%20tasks.%20We%20attribute%20it%0Ato%20the%20same%20embedding%20space%20but%20responsible%20for%20different%20requirements%20of%0Adownstream%20tasks.%20To%20address%20these%20issues%2C%20we%20propose%20an%20Efficient%20Prompt%0ATuning%20method%20%28EPT%29%20by%20multi-space%20projection%20and%20prompt%20fusion.%20Specifically%2C%0Ait%20decomposes%20a%20given%20soft%20prompt%20into%20a%20shorter%20prompt%20and%20two%20low-rank%0Amatrices%2C%20significantly%20reducing%20the%20training%20time.%20Accuracy%20is%20also%20enhanced%0Aby%20leveraging%20low-rank%20matrices%20and%20the%20short%20prompt%20as%20additional%20knowledge%0Asources%20to%20enrich%20the%20semantics%20of%20the%20original%20short%20prompt.%20In%20addition%2C%20we%0Aproject%20the%20soft%20prompt%20into%20multiple%20subspaces%20to%20improve%20the%20performance%0Aconsistency%2C%20and%20then%20adaptively%20learn%20the%20combination%20weights%20of%20different%0Aspaces%20through%20a%20gating%20network.%20Experiments%20on%2013%20natural%20language%20processing%0Adownstream%20tasks%20show%20that%20our%20method%20significantly%20and%20consistently%0Aoutperforms%2011%20comparison%20methods%20with%20the%20relative%20percentage%20of%20improvements%0Aup%20to%2012.9%25%2C%20and%20training%20time%20decreased%20by%2014%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.11464v2&entry.124074799=Read"},
{"title": "Stationary Kernels and Gaussian Processes on Lie Groups and their\n  Homogeneous Spaces II: non-compact symmetric spaces", "author": "Iskander Azangulov and Andrei Smolensky and Alexander Terenin and Viacheslav Borovitskiy", "abstract": "  Gaussian processes are arguably the most important class of spatiotemporal\nmodels within machine learning. They encode prior information about the modeled\nfunction and can be used for exact or approximate Bayesian learning. In many\napplications, particularly in physical sciences and engineering, but also in\nareas such as geostatistics and neuroscience, invariance to symmetries is one\nof the most fundamental forms of prior information one can consider. The\ninvariance of a Gaussian process' covariance to such symmetries gives rise to\nthe most natural generalization of the concept of stationarity to such spaces.\nIn this work, we develop constructive and practical techniques for building\nstationary Gaussian processes on a very large class of non-Euclidean spaces\narising in the context of symmetries. Our techniques make it possible to (i)\ncalculate covariance kernels and (ii) sample from prior and posterior Gaussian\nprocesses defined on such spaces, both in a practical manner. This work is\nsplit into two parts, each involving different technical considerations: part I\nstudies compact spaces, while part II studies non-compact spaces possessing\ncertain structure. Our contributions make the non-Euclidean Gaussian process\nmodels we study compatible with well-understood computational techniques\navailable in standard Gaussian process software packages, thereby making them\naccessible to practitioners.\n", "link": "http://arxiv.org/abs/2301.13088v3", "date": "2024-07-01", "relevancy": 2.3519, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.508}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4838}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4193}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stationary%20Kernels%20and%20Gaussian%20Processes%20on%20Lie%20Groups%20and%20their%0A%20%20Homogeneous%20Spaces%20II%3A%20non-compact%20symmetric%20spaces&body=Title%3A%20Stationary%20Kernels%20and%20Gaussian%20Processes%20on%20Lie%20Groups%20and%20their%0A%20%20Homogeneous%20Spaces%20II%3A%20non-compact%20symmetric%20spaces%0AAuthor%3A%20Iskander%20Azangulov%20and%20Andrei%20Smolensky%20and%20Alexander%20Terenin%20and%20Viacheslav%20Borovitskiy%0AAbstract%3A%20%20%20Gaussian%20processes%20are%20arguably%20the%20most%20important%20class%20of%20spatiotemporal%0Amodels%20within%20machine%20learning.%20They%20encode%20prior%20information%20about%20the%20modeled%0Afunction%20and%20can%20be%20used%20for%20exact%20or%20approximate%20Bayesian%20learning.%20In%20many%0Aapplications%2C%20particularly%20in%20physical%20sciences%20and%20engineering%2C%20but%20also%20in%0Aareas%20such%20as%20geostatistics%20and%20neuroscience%2C%20invariance%20to%20symmetries%20is%20one%0Aof%20the%20most%20fundamental%20forms%20of%20prior%20information%20one%20can%20consider.%20The%0Ainvariance%20of%20a%20Gaussian%20process%27%20covariance%20to%20such%20symmetries%20gives%20rise%20to%0Athe%20most%20natural%20generalization%20of%20the%20concept%20of%20stationarity%20to%20such%20spaces.%0AIn%20this%20work%2C%20we%20develop%20constructive%20and%20practical%20techniques%20for%20building%0Astationary%20Gaussian%20processes%20on%20a%20very%20large%20class%20of%20non-Euclidean%20spaces%0Aarising%20in%20the%20context%20of%20symmetries.%20Our%20techniques%20make%20it%20possible%20to%20%28i%29%0Acalculate%20covariance%20kernels%20and%20%28ii%29%20sample%20from%20prior%20and%20posterior%20Gaussian%0Aprocesses%20defined%20on%20such%20spaces%2C%20both%20in%20a%20practical%20manner.%20This%20work%20is%0Asplit%20into%20two%20parts%2C%20each%20involving%20different%20technical%20considerations%3A%20part%20I%0Astudies%20compact%20spaces%2C%20while%20part%20II%20studies%20non-compact%20spaces%20possessing%0Acertain%20structure.%20Our%20contributions%20make%20the%20non-Euclidean%20Gaussian%20process%0Amodels%20we%20study%20compatible%20with%20well-understood%20computational%20techniques%0Aavailable%20in%20standard%20Gaussian%20process%20software%20packages%2C%20thereby%20making%20them%0Aaccessible%20to%20practitioners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.13088v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStationary%2520Kernels%2520and%2520Gaussian%2520Processes%2520on%2520Lie%2520Groups%2520and%2520their%250A%2520%2520Homogeneous%2520Spaces%2520II%253A%2520non-compact%2520symmetric%2520spaces%26entry.906535625%3DIskander%2520Azangulov%2520and%2520Andrei%2520Smolensky%2520and%2520Alexander%2520Terenin%2520and%2520Viacheslav%2520Borovitskiy%26entry.1292438233%3D%2520%2520Gaussian%2520processes%2520are%2520arguably%2520the%2520most%2520important%2520class%2520of%2520spatiotemporal%250Amodels%2520within%2520machine%2520learning.%2520They%2520encode%2520prior%2520information%2520about%2520the%2520modeled%250Afunction%2520and%2520can%2520be%2520used%2520for%2520exact%2520or%2520approximate%2520Bayesian%2520learning.%2520In%2520many%250Aapplications%252C%2520particularly%2520in%2520physical%2520sciences%2520and%2520engineering%252C%2520but%2520also%2520in%250Aareas%2520such%2520as%2520geostatistics%2520and%2520neuroscience%252C%2520invariance%2520to%2520symmetries%2520is%2520one%250Aof%2520the%2520most%2520fundamental%2520forms%2520of%2520prior%2520information%2520one%2520can%2520consider.%2520The%250Ainvariance%2520of%2520a%2520Gaussian%2520process%2527%2520covariance%2520to%2520such%2520symmetries%2520gives%2520rise%2520to%250Athe%2520most%2520natural%2520generalization%2520of%2520the%2520concept%2520of%2520stationarity%2520to%2520such%2520spaces.%250AIn%2520this%2520work%252C%2520we%2520develop%2520constructive%2520and%2520practical%2520techniques%2520for%2520building%250Astationary%2520Gaussian%2520processes%2520on%2520a%2520very%2520large%2520class%2520of%2520non-Euclidean%2520spaces%250Aarising%2520in%2520the%2520context%2520of%2520symmetries.%2520Our%2520techniques%2520make%2520it%2520possible%2520to%2520%2528i%2529%250Acalculate%2520covariance%2520kernels%2520and%2520%2528ii%2529%2520sample%2520from%2520prior%2520and%2520posterior%2520Gaussian%250Aprocesses%2520defined%2520on%2520such%2520spaces%252C%2520both%2520in%2520a%2520practical%2520manner.%2520This%2520work%2520is%250Asplit%2520into%2520two%2520parts%252C%2520each%2520involving%2520different%2520technical%2520considerations%253A%2520part%2520I%250Astudies%2520compact%2520spaces%252C%2520while%2520part%2520II%2520studies%2520non-compact%2520spaces%2520possessing%250Acertain%2520structure.%2520Our%2520contributions%2520make%2520the%2520non-Euclidean%2520Gaussian%2520process%250Amodels%2520we%2520study%2520compatible%2520with%2520well-understood%2520computational%2520techniques%250Aavailable%2520in%2520standard%2520Gaussian%2520process%2520software%2520packages%252C%2520thereby%2520making%2520them%250Aaccessible%2520to%2520practitioners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.13088v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stationary%20Kernels%20and%20Gaussian%20Processes%20on%20Lie%20Groups%20and%20their%0A%20%20Homogeneous%20Spaces%20II%3A%20non-compact%20symmetric%20spaces&entry.906535625=Iskander%20Azangulov%20and%20Andrei%20Smolensky%20and%20Alexander%20Terenin%20and%20Viacheslav%20Borovitskiy&entry.1292438233=%20%20Gaussian%20processes%20are%20arguably%20the%20most%20important%20class%20of%20spatiotemporal%0Amodels%20within%20machine%20learning.%20They%20encode%20prior%20information%20about%20the%20modeled%0Afunction%20and%20can%20be%20used%20for%20exact%20or%20approximate%20Bayesian%20learning.%20In%20many%0Aapplications%2C%20particularly%20in%20physical%20sciences%20and%20engineering%2C%20but%20also%20in%0Aareas%20such%20as%20geostatistics%20and%20neuroscience%2C%20invariance%20to%20symmetries%20is%20one%0Aof%20the%20most%20fundamental%20forms%20of%20prior%20information%20one%20can%20consider.%20The%0Ainvariance%20of%20a%20Gaussian%20process%27%20covariance%20to%20such%20symmetries%20gives%20rise%20to%0Athe%20most%20natural%20generalization%20of%20the%20concept%20of%20stationarity%20to%20such%20spaces.%0AIn%20this%20work%2C%20we%20develop%20constructive%20and%20practical%20techniques%20for%20building%0Astationary%20Gaussian%20processes%20on%20a%20very%20large%20class%20of%20non-Euclidean%20spaces%0Aarising%20in%20the%20context%20of%20symmetries.%20Our%20techniques%20make%20it%20possible%20to%20%28i%29%0Acalculate%20covariance%20kernels%20and%20%28ii%29%20sample%20from%20prior%20and%20posterior%20Gaussian%0Aprocesses%20defined%20on%20such%20spaces%2C%20both%20in%20a%20practical%20manner.%20This%20work%20is%0Asplit%20into%20two%20parts%2C%20each%20involving%20different%20technical%20considerations%3A%20part%20I%0Astudies%20compact%20spaces%2C%20while%20part%20II%20studies%20non-compact%20spaces%20possessing%0Acertain%20structure.%20Our%20contributions%20make%20the%20non-Euclidean%20Gaussian%20process%0Amodels%20we%20study%20compatible%20with%20well-understood%20computational%20techniques%0Aavailable%20in%20standard%20Gaussian%20process%20software%20packages%2C%20thereby%20making%20them%0Aaccessible%20to%20practitioners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.13088v3&entry.124074799=Read"},
{"title": "Rethinking LLM Memorization through the Lens of Adversarial Compression", "author": "Avi Schwarzschild and Zhili Feng and Pratyush Maini and Zachary C. Lipton and J. Zico Kolter", "abstract": "  Large language models (LLMs) trained on web-scale datasets raise substantial\nconcerns regarding permissible data usage. One major question is whether these\nmodels \"memorize\" all their training data or they integrate many data sources\nin some way more akin to how a human would learn and synthesize information.\nThe answer hinges, to a large degree, on how we define memorization. In this\nwork, we propose the Adversarial Compression Ratio (ACR) as a metric for\nassessing memorization in LLMs. A given string from the training data is\nconsidered memorized if it can be elicited by a prompt (much) shorter than the\nstring itself -- in other words, if these strings can be \"compressed\" with the\nmodel by computing adversarial prompts of fewer tokens. The ACR overcomes the\nlimitations of existing notions of memorization by (i) offering an adversarial\nview of measuring memorization, especially for monitoring unlearning and\ncompliance; and (ii) allowing for the flexibility to measure memorization for\narbitrary strings at a reasonably low compute. Our definition serves as a\npractical tool for determining when model owners may be violating terms around\ndata usage, providing a potential legal tool and a critical lens through which\nto address such scenarios.\n", "link": "http://arxiv.org/abs/2404.15146v2", "date": "2024-07-01", "relevancy": 2.3499, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4858}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.485}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4391}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20LLM%20Memorization%20through%20the%20Lens%20of%20Adversarial%20Compression&body=Title%3A%20Rethinking%20LLM%20Memorization%20through%20the%20Lens%20of%20Adversarial%20Compression%0AAuthor%3A%20Avi%20Schwarzschild%20and%20Zhili%20Feng%20and%20Pratyush%20Maini%20and%20Zachary%20C.%20Lipton%20and%20J.%20Zico%20Kolter%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20trained%20on%20web-scale%20datasets%20raise%20substantial%0Aconcerns%20regarding%20permissible%20data%20usage.%20One%20major%20question%20is%20whether%20these%0Amodels%20%22memorize%22%20all%20their%20training%20data%20or%20they%20integrate%20many%20data%20sources%0Ain%20some%20way%20more%20akin%20to%20how%20a%20human%20would%20learn%20and%20synthesize%20information.%0AThe%20answer%20hinges%2C%20to%20a%20large%20degree%2C%20on%20how%20we%20define%20memorization.%20In%20this%0Awork%2C%20we%20propose%20the%20Adversarial%20Compression%20Ratio%20%28ACR%29%20as%20a%20metric%20for%0Aassessing%20memorization%20in%20LLMs.%20A%20given%20string%20from%20the%20training%20data%20is%0Aconsidered%20memorized%20if%20it%20can%20be%20elicited%20by%20a%20prompt%20%28much%29%20shorter%20than%20the%0Astring%20itself%20--%20in%20other%20words%2C%20if%20these%20strings%20can%20be%20%22compressed%22%20with%20the%0Amodel%20by%20computing%20adversarial%20prompts%20of%20fewer%20tokens.%20The%20ACR%20overcomes%20the%0Alimitations%20of%20existing%20notions%20of%20memorization%20by%20%28i%29%20offering%20an%20adversarial%0Aview%20of%20measuring%20memorization%2C%20especially%20for%20monitoring%20unlearning%20and%0Acompliance%3B%20and%20%28ii%29%20allowing%20for%20the%20flexibility%20to%20measure%20memorization%20for%0Aarbitrary%20strings%20at%20a%20reasonably%20low%20compute.%20Our%20definition%20serves%20as%20a%0Apractical%20tool%20for%20determining%20when%20model%20owners%20may%20be%20violating%20terms%20around%0Adata%20usage%2C%20providing%20a%20potential%20legal%20tool%20and%20a%20critical%20lens%20through%20which%0Ato%20address%20such%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.15146v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520LLM%2520Memorization%2520through%2520the%2520Lens%2520of%2520Adversarial%2520Compression%26entry.906535625%3DAvi%2520Schwarzschild%2520and%2520Zhili%2520Feng%2520and%2520Pratyush%2520Maini%2520and%2520Zachary%2520C.%2520Lipton%2520and%2520J.%2520Zico%2520Kolter%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520trained%2520on%2520web-scale%2520datasets%2520raise%2520substantial%250Aconcerns%2520regarding%2520permissible%2520data%2520usage.%2520One%2520major%2520question%2520is%2520whether%2520these%250Amodels%2520%2522memorize%2522%2520all%2520their%2520training%2520data%2520or%2520they%2520integrate%2520many%2520data%2520sources%250Ain%2520some%2520way%2520more%2520akin%2520to%2520how%2520a%2520human%2520would%2520learn%2520and%2520synthesize%2520information.%250AThe%2520answer%2520hinges%252C%2520to%2520a%2520large%2520degree%252C%2520on%2520how%2520we%2520define%2520memorization.%2520In%2520this%250Awork%252C%2520we%2520propose%2520the%2520Adversarial%2520Compression%2520Ratio%2520%2528ACR%2529%2520as%2520a%2520metric%2520for%250Aassessing%2520memorization%2520in%2520LLMs.%2520A%2520given%2520string%2520from%2520the%2520training%2520data%2520is%250Aconsidered%2520memorized%2520if%2520it%2520can%2520be%2520elicited%2520by%2520a%2520prompt%2520%2528much%2529%2520shorter%2520than%2520the%250Astring%2520itself%2520--%2520in%2520other%2520words%252C%2520if%2520these%2520strings%2520can%2520be%2520%2522compressed%2522%2520with%2520the%250Amodel%2520by%2520computing%2520adversarial%2520prompts%2520of%2520fewer%2520tokens.%2520The%2520ACR%2520overcomes%2520the%250Alimitations%2520of%2520existing%2520notions%2520of%2520memorization%2520by%2520%2528i%2529%2520offering%2520an%2520adversarial%250Aview%2520of%2520measuring%2520memorization%252C%2520especially%2520for%2520monitoring%2520unlearning%2520and%250Acompliance%253B%2520and%2520%2528ii%2529%2520allowing%2520for%2520the%2520flexibility%2520to%2520measure%2520memorization%2520for%250Aarbitrary%2520strings%2520at%2520a%2520reasonably%2520low%2520compute.%2520Our%2520definition%2520serves%2520as%2520a%250Apractical%2520tool%2520for%2520determining%2520when%2520model%2520owners%2520may%2520be%2520violating%2520terms%2520around%250Adata%2520usage%252C%2520providing%2520a%2520potential%2520legal%2520tool%2520and%2520a%2520critical%2520lens%2520through%2520which%250Ato%2520address%2520such%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.15146v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20LLM%20Memorization%20through%20the%20Lens%20of%20Adversarial%20Compression&entry.906535625=Avi%20Schwarzschild%20and%20Zhili%20Feng%20and%20Pratyush%20Maini%20and%20Zachary%20C.%20Lipton%20and%20J.%20Zico%20Kolter&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20trained%20on%20web-scale%20datasets%20raise%20substantial%0Aconcerns%20regarding%20permissible%20data%20usage.%20One%20major%20question%20is%20whether%20these%0Amodels%20%22memorize%22%20all%20their%20training%20data%20or%20they%20integrate%20many%20data%20sources%0Ain%20some%20way%20more%20akin%20to%20how%20a%20human%20would%20learn%20and%20synthesize%20information.%0AThe%20answer%20hinges%2C%20to%20a%20large%20degree%2C%20on%20how%20we%20define%20memorization.%20In%20this%0Awork%2C%20we%20propose%20the%20Adversarial%20Compression%20Ratio%20%28ACR%29%20as%20a%20metric%20for%0Aassessing%20memorization%20in%20LLMs.%20A%20given%20string%20from%20the%20training%20data%20is%0Aconsidered%20memorized%20if%20it%20can%20be%20elicited%20by%20a%20prompt%20%28much%29%20shorter%20than%20the%0Astring%20itself%20--%20in%20other%20words%2C%20if%20these%20strings%20can%20be%20%22compressed%22%20with%20the%0Amodel%20by%20computing%20adversarial%20prompts%20of%20fewer%20tokens.%20The%20ACR%20overcomes%20the%0Alimitations%20of%20existing%20notions%20of%20memorization%20by%20%28i%29%20offering%20an%20adversarial%0Aview%20of%20measuring%20memorization%2C%20especially%20for%20monitoring%20unlearning%20and%0Acompliance%3B%20and%20%28ii%29%20allowing%20for%20the%20flexibility%20to%20measure%20memorization%20for%0Aarbitrary%20strings%20at%20a%20reasonably%20low%20compute.%20Our%20definition%20serves%20as%20a%0Apractical%20tool%20for%20determining%20when%20model%20owners%20may%20be%20violating%20terms%20around%0Adata%20usage%2C%20providing%20a%20potential%20legal%20tool%20and%20a%20critical%20lens%20through%20which%0Ato%20address%20such%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.15146v2&entry.124074799=Read"},
{"title": "DreamPBR: Text-driven Generation of High-resolution SVBRDF with\n  Multi-modal Guidance", "author": "Linxuan Xin and Zheng Zhang and Jinfu Wei and Wei Gao and Duan Gao", "abstract": "  Prior material creation methods had limitations in producing diverse results\nmainly because reconstruction-based methods relied on real-world measurements\nand generation-based methods were trained on relatively small material\ndatasets. To address these challenges, we propose DreamPBR, a novel\ndiffusion-based generative framework designed to create spatially-varying\nappearance properties guided by text and multi-modal controls, providing high\ncontrollability and diversity in material generation. Key to achieving diverse\nand high-quality PBR material generation lies in integrating the capabilities\nof recent large-scale vision-language models trained on billions of text-image\npairs, along with material priors derived from hundreds of PBR material\nsamples. We utilize a novel material Latent Diffusion Model (LDM) to establish\nthe mapping between albedo maps and the corresponding latent space. The latent\nrepresentation is then decoded into full SVBRDF parameter maps using a\nrendering-aware PBR decoder. Our method supports tileable generation through\nconvolution with circular padding. Furthermore, we introduce a multi-modal\nguidance module, which includes pixel-aligned guidance, style image guidance,\nand 3D shape guidance, to enhance the control capabilities of the material LDM.\nWe demonstrate the effectiveness of DreamPBR in material creation, showcasing\nits versatility and user-friendliness on a wide range of controllable\ngeneration and editing applications.\n", "link": "http://arxiv.org/abs/2404.14676v2", "date": "2024-07-01", "relevancy": 2.2491, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5816}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5584}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DreamPBR%3A%20Text-driven%20Generation%20of%20High-resolution%20SVBRDF%20with%0A%20%20Multi-modal%20Guidance&body=Title%3A%20DreamPBR%3A%20Text-driven%20Generation%20of%20High-resolution%20SVBRDF%20with%0A%20%20Multi-modal%20Guidance%0AAuthor%3A%20Linxuan%20Xin%20and%20Zheng%20Zhang%20and%20Jinfu%20Wei%20and%20Wei%20Gao%20and%20Duan%20Gao%0AAbstract%3A%20%20%20Prior%20material%20creation%20methods%20had%20limitations%20in%20producing%20diverse%20results%0Amainly%20because%20reconstruction-based%20methods%20relied%20on%20real-world%20measurements%0Aand%20generation-based%20methods%20were%20trained%20on%20relatively%20small%20material%0Adatasets.%20To%20address%20these%20challenges%2C%20we%20propose%20DreamPBR%2C%20a%20novel%0Adiffusion-based%20generative%20framework%20designed%20to%20create%20spatially-varying%0Aappearance%20properties%20guided%20by%20text%20and%20multi-modal%20controls%2C%20providing%20high%0Acontrollability%20and%20diversity%20in%20material%20generation.%20Key%20to%20achieving%20diverse%0Aand%20high-quality%20PBR%20material%20generation%20lies%20in%20integrating%20the%20capabilities%0Aof%20recent%20large-scale%20vision-language%20models%20trained%20on%20billions%20of%20text-image%0Apairs%2C%20along%20with%20material%20priors%20derived%20from%20hundreds%20of%20PBR%20material%0Asamples.%20We%20utilize%20a%20novel%20material%20Latent%20Diffusion%20Model%20%28LDM%29%20to%20establish%0Athe%20mapping%20between%20albedo%20maps%20and%20the%20corresponding%20latent%20space.%20The%20latent%0Arepresentation%20is%20then%20decoded%20into%20full%20SVBRDF%20parameter%20maps%20using%20a%0Arendering-aware%20PBR%20decoder.%20Our%20method%20supports%20tileable%20generation%20through%0Aconvolution%20with%20circular%20padding.%20Furthermore%2C%20we%20introduce%20a%20multi-modal%0Aguidance%20module%2C%20which%20includes%20pixel-aligned%20guidance%2C%20style%20image%20guidance%2C%0Aand%203D%20shape%20guidance%2C%20to%20enhance%20the%20control%20capabilities%20of%20the%20material%20LDM.%0AWe%20demonstrate%20the%20effectiveness%20of%20DreamPBR%20in%20material%20creation%2C%20showcasing%0Aits%20versatility%20and%20user-friendliness%20on%20a%20wide%20range%20of%20controllable%0Ageneration%20and%20editing%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.14676v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDreamPBR%253A%2520Text-driven%2520Generation%2520of%2520High-resolution%2520SVBRDF%2520with%250A%2520%2520Multi-modal%2520Guidance%26entry.906535625%3DLinxuan%2520Xin%2520and%2520Zheng%2520Zhang%2520and%2520Jinfu%2520Wei%2520and%2520Wei%2520Gao%2520and%2520Duan%2520Gao%26entry.1292438233%3D%2520%2520Prior%2520material%2520creation%2520methods%2520had%2520limitations%2520in%2520producing%2520diverse%2520results%250Amainly%2520because%2520reconstruction-based%2520methods%2520relied%2520on%2520real-world%2520measurements%250Aand%2520generation-based%2520methods%2520were%2520trained%2520on%2520relatively%2520small%2520material%250Adatasets.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520DreamPBR%252C%2520a%2520novel%250Adiffusion-based%2520generative%2520framework%2520designed%2520to%2520create%2520spatially-varying%250Aappearance%2520properties%2520guided%2520by%2520text%2520and%2520multi-modal%2520controls%252C%2520providing%2520high%250Acontrollability%2520and%2520diversity%2520in%2520material%2520generation.%2520Key%2520to%2520achieving%2520diverse%250Aand%2520high-quality%2520PBR%2520material%2520generation%2520lies%2520in%2520integrating%2520the%2520capabilities%250Aof%2520recent%2520large-scale%2520vision-language%2520models%2520trained%2520on%2520billions%2520of%2520text-image%250Apairs%252C%2520along%2520with%2520material%2520priors%2520derived%2520from%2520hundreds%2520of%2520PBR%2520material%250Asamples.%2520We%2520utilize%2520a%2520novel%2520material%2520Latent%2520Diffusion%2520Model%2520%2528LDM%2529%2520to%2520establish%250Athe%2520mapping%2520between%2520albedo%2520maps%2520and%2520the%2520corresponding%2520latent%2520space.%2520The%2520latent%250Arepresentation%2520is%2520then%2520decoded%2520into%2520full%2520SVBRDF%2520parameter%2520maps%2520using%2520a%250Arendering-aware%2520PBR%2520decoder.%2520Our%2520method%2520supports%2520tileable%2520generation%2520through%250Aconvolution%2520with%2520circular%2520padding.%2520Furthermore%252C%2520we%2520introduce%2520a%2520multi-modal%250Aguidance%2520module%252C%2520which%2520includes%2520pixel-aligned%2520guidance%252C%2520style%2520image%2520guidance%252C%250Aand%25203D%2520shape%2520guidance%252C%2520to%2520enhance%2520the%2520control%2520capabilities%2520of%2520the%2520material%2520LDM.%250AWe%2520demonstrate%2520the%2520effectiveness%2520of%2520DreamPBR%2520in%2520material%2520creation%252C%2520showcasing%250Aits%2520versatility%2520and%2520user-friendliness%2520on%2520a%2520wide%2520range%2520of%2520controllable%250Ageneration%2520and%2520editing%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.14676v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DreamPBR%3A%20Text-driven%20Generation%20of%20High-resolution%20SVBRDF%20with%0A%20%20Multi-modal%20Guidance&entry.906535625=Linxuan%20Xin%20and%20Zheng%20Zhang%20and%20Jinfu%20Wei%20and%20Wei%20Gao%20and%20Duan%20Gao&entry.1292438233=%20%20Prior%20material%20creation%20methods%20had%20limitations%20in%20producing%20diverse%20results%0Amainly%20because%20reconstruction-based%20methods%20relied%20on%20real-world%20measurements%0Aand%20generation-based%20methods%20were%20trained%20on%20relatively%20small%20material%0Adatasets.%20To%20address%20these%20challenges%2C%20we%20propose%20DreamPBR%2C%20a%20novel%0Adiffusion-based%20generative%20framework%20designed%20to%20create%20spatially-varying%0Aappearance%20properties%20guided%20by%20text%20and%20multi-modal%20controls%2C%20providing%20high%0Acontrollability%20and%20diversity%20in%20material%20generation.%20Key%20to%20achieving%20diverse%0Aand%20high-quality%20PBR%20material%20generation%20lies%20in%20integrating%20the%20capabilities%0Aof%20recent%20large-scale%20vision-language%20models%20trained%20on%20billions%20of%20text-image%0Apairs%2C%20along%20with%20material%20priors%20derived%20from%20hundreds%20of%20PBR%20material%0Asamples.%20We%20utilize%20a%20novel%20material%20Latent%20Diffusion%20Model%20%28LDM%29%20to%20establish%0Athe%20mapping%20between%20albedo%20maps%20and%20the%20corresponding%20latent%20space.%20The%20latent%0Arepresentation%20is%20then%20decoded%20into%20full%20SVBRDF%20parameter%20maps%20using%20a%0Arendering-aware%20PBR%20decoder.%20Our%20method%20supports%20tileable%20generation%20through%0Aconvolution%20with%20circular%20padding.%20Furthermore%2C%20we%20introduce%20a%20multi-modal%0Aguidance%20module%2C%20which%20includes%20pixel-aligned%20guidance%2C%20style%20image%20guidance%2C%0Aand%203D%20shape%20guidance%2C%20to%20enhance%20the%20control%20capabilities%20of%20the%20material%20LDM.%0AWe%20demonstrate%20the%20effectiveness%20of%20DreamPBR%20in%20material%20creation%2C%20showcasing%0Aits%20versatility%20and%20user-friendliness%20on%20a%20wide%20range%20of%20controllable%0Ageneration%20and%20editing%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.14676v2&entry.124074799=Read"},
{"title": "Evaluation of Deep Learning Semantic Segmentation for Land Cover Mapping\n  on Multispectral, Hyperspectral and High Spatial Aerial Imagery", "author": "Ilham Adi Panuntun and Ying-Nong Chen and Ilham Jamaluddin and Thi Linh Chi Tran", "abstract": "  In the rise of climate change, land cover mapping has become such an urgent\nneed in environmental monitoring. The accuracy of land cover classification has\ngotten increasingly based on the improvement of remote sensing data. Land cover\nclassification using satellite imageries has been explored and become more\nprevalent in recent years, but the methodologies remain some drawbacks of\nsubjective and time-consuming. Some deep learning techniques have been utilized\nto overcome these limitations. However, most studies implemented just one image\ntype to evaluate algorithms for land cover mapping. Therefore, our study\nconducted deep learning semantic segmentation in multispectral, hyperspectral,\nand high spatial aerial image datasets for landcover mapping. This research\nimplemented a semantic segmentation method such as Unet, Linknet, FPN, and\nPSPnet for categorizing vegetation, water, and others (i.e., soil and\nimpervious surface). The LinkNet model obtained high accuracy in IoU\n(Intersection Over Union) at 0.92 in all datasets, which is comparable with\nother mentioned techniques. In evaluation with different image types, the\nmultispectral images showed higher performance with the IoU, and F1-score are\n0.993 and 0.997, respectively. Our outcome highlighted the efficiency and broad\napplicability of LinkNet and multispectral image on land cover classification.\nThis research contributes to establishing an approach on landcover segmentation\nvia open source for long-term future application.\n", "link": "http://arxiv.org/abs/2406.14220v2", "date": "2024-07-01", "relevancy": 2.2231, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5898}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5559}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5217}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluation%20of%20Deep%20Learning%20Semantic%20Segmentation%20for%20Land%20Cover%20Mapping%0A%20%20on%20Multispectral%2C%20Hyperspectral%20and%20High%20Spatial%20Aerial%20Imagery&body=Title%3A%20Evaluation%20of%20Deep%20Learning%20Semantic%20Segmentation%20for%20Land%20Cover%20Mapping%0A%20%20on%20Multispectral%2C%20Hyperspectral%20and%20High%20Spatial%20Aerial%20Imagery%0AAuthor%3A%20Ilham%20Adi%20Panuntun%20and%20Ying-Nong%20Chen%20and%20Ilham%20Jamaluddin%20and%20Thi%20Linh%20Chi%20Tran%0AAbstract%3A%20%20%20In%20the%20rise%20of%20climate%20change%2C%20land%20cover%20mapping%20has%20become%20such%20an%20urgent%0Aneed%20in%20environmental%20monitoring.%20The%20accuracy%20of%20land%20cover%20classification%20has%0Agotten%20increasingly%20based%20on%20the%20improvement%20of%20remote%20sensing%20data.%20Land%20cover%0Aclassification%20using%20satellite%20imageries%20has%20been%20explored%20and%20become%20more%0Aprevalent%20in%20recent%20years%2C%20but%20the%20methodologies%20remain%20some%20drawbacks%20of%0Asubjective%20and%20time-consuming.%20Some%20deep%20learning%20techniques%20have%20been%20utilized%0Ato%20overcome%20these%20limitations.%20However%2C%20most%20studies%20implemented%20just%20one%20image%0Atype%20to%20evaluate%20algorithms%20for%20land%20cover%20mapping.%20Therefore%2C%20our%20study%0Aconducted%20deep%20learning%20semantic%20segmentation%20in%20multispectral%2C%20hyperspectral%2C%0Aand%20high%20spatial%20aerial%20image%20datasets%20for%20landcover%20mapping.%20This%20research%0Aimplemented%20a%20semantic%20segmentation%20method%20such%20as%20Unet%2C%20Linknet%2C%20FPN%2C%20and%0APSPnet%20for%20categorizing%20vegetation%2C%20water%2C%20and%20others%20%28i.e.%2C%20soil%20and%0Aimpervious%20surface%29.%20The%20LinkNet%20model%20obtained%20high%20accuracy%20in%20IoU%0A%28Intersection%20Over%20Union%29%20at%200.92%20in%20all%20datasets%2C%20which%20is%20comparable%20with%0Aother%20mentioned%20techniques.%20In%20evaluation%20with%20different%20image%20types%2C%20the%0Amultispectral%20images%20showed%20higher%20performance%20with%20the%20IoU%2C%20and%20F1-score%20are%0A0.993%20and%200.997%2C%20respectively.%20Our%20outcome%20highlighted%20the%20efficiency%20and%20broad%0Aapplicability%20of%20LinkNet%20and%20multispectral%20image%20on%20land%20cover%20classification.%0AThis%20research%20contributes%20to%20establishing%20an%20approach%20on%20landcover%20segmentation%0Avia%20open%20source%20for%20long-term%20future%20application.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14220v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluation%2520of%2520Deep%2520Learning%2520Semantic%2520Segmentation%2520for%2520Land%2520Cover%2520Mapping%250A%2520%2520on%2520Multispectral%252C%2520Hyperspectral%2520and%2520High%2520Spatial%2520Aerial%2520Imagery%26entry.906535625%3DIlham%2520Adi%2520Panuntun%2520and%2520Ying-Nong%2520Chen%2520and%2520Ilham%2520Jamaluddin%2520and%2520Thi%2520Linh%2520Chi%2520Tran%26entry.1292438233%3D%2520%2520In%2520the%2520rise%2520of%2520climate%2520change%252C%2520land%2520cover%2520mapping%2520has%2520become%2520such%2520an%2520urgent%250Aneed%2520in%2520environmental%2520monitoring.%2520The%2520accuracy%2520of%2520land%2520cover%2520classification%2520has%250Agotten%2520increasingly%2520based%2520on%2520the%2520improvement%2520of%2520remote%2520sensing%2520data.%2520Land%2520cover%250Aclassification%2520using%2520satellite%2520imageries%2520has%2520been%2520explored%2520and%2520become%2520more%250Aprevalent%2520in%2520recent%2520years%252C%2520but%2520the%2520methodologies%2520remain%2520some%2520drawbacks%2520of%250Asubjective%2520and%2520time-consuming.%2520Some%2520deep%2520learning%2520techniques%2520have%2520been%2520utilized%250Ato%2520overcome%2520these%2520limitations.%2520However%252C%2520most%2520studies%2520implemented%2520just%2520one%2520image%250Atype%2520to%2520evaluate%2520algorithms%2520for%2520land%2520cover%2520mapping.%2520Therefore%252C%2520our%2520study%250Aconducted%2520deep%2520learning%2520semantic%2520segmentation%2520in%2520multispectral%252C%2520hyperspectral%252C%250Aand%2520high%2520spatial%2520aerial%2520image%2520datasets%2520for%2520landcover%2520mapping.%2520This%2520research%250Aimplemented%2520a%2520semantic%2520segmentation%2520method%2520such%2520as%2520Unet%252C%2520Linknet%252C%2520FPN%252C%2520and%250APSPnet%2520for%2520categorizing%2520vegetation%252C%2520water%252C%2520and%2520others%2520%2528i.e.%252C%2520soil%2520and%250Aimpervious%2520surface%2529.%2520The%2520LinkNet%2520model%2520obtained%2520high%2520accuracy%2520in%2520IoU%250A%2528Intersection%2520Over%2520Union%2529%2520at%25200.92%2520in%2520all%2520datasets%252C%2520which%2520is%2520comparable%2520with%250Aother%2520mentioned%2520techniques.%2520In%2520evaluation%2520with%2520different%2520image%2520types%252C%2520the%250Amultispectral%2520images%2520showed%2520higher%2520performance%2520with%2520the%2520IoU%252C%2520and%2520F1-score%2520are%250A0.993%2520and%25200.997%252C%2520respectively.%2520Our%2520outcome%2520highlighted%2520the%2520efficiency%2520and%2520broad%250Aapplicability%2520of%2520LinkNet%2520and%2520multispectral%2520image%2520on%2520land%2520cover%2520classification.%250AThis%2520research%2520contributes%2520to%2520establishing%2520an%2520approach%2520on%2520landcover%2520segmentation%250Avia%2520open%2520source%2520for%2520long-term%2520future%2520application.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14220v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluation%20of%20Deep%20Learning%20Semantic%20Segmentation%20for%20Land%20Cover%20Mapping%0A%20%20on%20Multispectral%2C%20Hyperspectral%20and%20High%20Spatial%20Aerial%20Imagery&entry.906535625=Ilham%20Adi%20Panuntun%20and%20Ying-Nong%20Chen%20and%20Ilham%20Jamaluddin%20and%20Thi%20Linh%20Chi%20Tran&entry.1292438233=%20%20In%20the%20rise%20of%20climate%20change%2C%20land%20cover%20mapping%20has%20become%20such%20an%20urgent%0Aneed%20in%20environmental%20monitoring.%20The%20accuracy%20of%20land%20cover%20classification%20has%0Agotten%20increasingly%20based%20on%20the%20improvement%20of%20remote%20sensing%20data.%20Land%20cover%0Aclassification%20using%20satellite%20imageries%20has%20been%20explored%20and%20become%20more%0Aprevalent%20in%20recent%20years%2C%20but%20the%20methodologies%20remain%20some%20drawbacks%20of%0Asubjective%20and%20time-consuming.%20Some%20deep%20learning%20techniques%20have%20been%20utilized%0Ato%20overcome%20these%20limitations.%20However%2C%20most%20studies%20implemented%20just%20one%20image%0Atype%20to%20evaluate%20algorithms%20for%20land%20cover%20mapping.%20Therefore%2C%20our%20study%0Aconducted%20deep%20learning%20semantic%20segmentation%20in%20multispectral%2C%20hyperspectral%2C%0Aand%20high%20spatial%20aerial%20image%20datasets%20for%20landcover%20mapping.%20This%20research%0Aimplemented%20a%20semantic%20segmentation%20method%20such%20as%20Unet%2C%20Linknet%2C%20FPN%2C%20and%0APSPnet%20for%20categorizing%20vegetation%2C%20water%2C%20and%20others%20%28i.e.%2C%20soil%20and%0Aimpervious%20surface%29.%20The%20LinkNet%20model%20obtained%20high%20accuracy%20in%20IoU%0A%28Intersection%20Over%20Union%29%20at%200.92%20in%20all%20datasets%2C%20which%20is%20comparable%20with%0Aother%20mentioned%20techniques.%20In%20evaluation%20with%20different%20image%20types%2C%20the%0Amultispectral%20images%20showed%20higher%20performance%20with%20the%20IoU%2C%20and%20F1-score%20are%0A0.993%20and%200.997%2C%20respectively.%20Our%20outcome%20highlighted%20the%20efficiency%20and%20broad%0Aapplicability%20of%20LinkNet%20and%20multispectral%20image%20on%20land%20cover%20classification.%0AThis%20research%20contributes%20to%20establishing%20an%20approach%20on%20landcover%20segmentation%0Avia%20open%20source%20for%20long-term%20future%20application.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14220v2&entry.124074799=Read"},
{"title": "From Alexnet to Transformers: Measuring the Non-linearity of Deep Neural\n  Networks with Affine Optimal Transport", "author": "Quentin Bouniot and Ievgen Redko and Anton Mallasto and Charlotte Laclau and Karol Arndt and Oliver Struckmeier and Markus Heinonen and Ville Kyrki and Samuel Kaski", "abstract": "  In the last decade, we have witnessed the introduction of several novel deep\nneural network (DNN) architectures exhibiting ever-increasing performance\nacross diverse tasks. Explaining the upward trend of their performance,\nhowever, remains difficult as different DNN architectures of comparable depth\nand width -- common factors associated with their expressive power -- may\nexhibit a drastically different performance even when trained on the same\ndataset. In this paper, we introduce the concept of the non-linearity signature\nof DNN, the first theoretically sound solution for approximately measuring the\nnon-linearity of deep neural networks. Built upon a score derived from\nclosed-form optimal transport mappings, this signature provides a better\nunderstanding of the inner workings of a wide range of DNN architectures and\nlearning paradigms, with a particular emphasis on the computer vision task. We\nprovide extensive experimental results that highlight the practical usefulness\nof the proposed non-linearity signature and its potential for long-reaching\nimplications. The code for our work is available at\nhttps://github.com/qbouniot/AffScoreDeep\n", "link": "http://arxiv.org/abs/2310.11439v3", "date": "2024-07-01", "relevancy": 2.1418, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5647}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5476}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5116}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Alexnet%20to%20Transformers%3A%20Measuring%20the%20Non-linearity%20of%20Deep%20Neural%0A%20%20Networks%20with%20Affine%20Optimal%20Transport&body=Title%3A%20From%20Alexnet%20to%20Transformers%3A%20Measuring%20the%20Non-linearity%20of%20Deep%20Neural%0A%20%20Networks%20with%20Affine%20Optimal%20Transport%0AAuthor%3A%20Quentin%20Bouniot%20and%20Ievgen%20Redko%20and%20Anton%20Mallasto%20and%20Charlotte%20Laclau%20and%20Karol%20Arndt%20and%20Oliver%20Struckmeier%20and%20Markus%20Heinonen%20and%20Ville%20Kyrki%20and%20Samuel%20Kaski%0AAbstract%3A%20%20%20In%20the%20last%20decade%2C%20we%20have%20witnessed%20the%20introduction%20of%20several%20novel%20deep%0Aneural%20network%20%28DNN%29%20architectures%20exhibiting%20ever-increasing%20performance%0Aacross%20diverse%20tasks.%20Explaining%20the%20upward%20trend%20of%20their%20performance%2C%0Ahowever%2C%20remains%20difficult%20as%20different%20DNN%20architectures%20of%20comparable%20depth%0Aand%20width%20--%20common%20factors%20associated%20with%20their%20expressive%20power%20--%20may%0Aexhibit%20a%20drastically%20different%20performance%20even%20when%20trained%20on%20the%20same%0Adataset.%20In%20this%20paper%2C%20we%20introduce%20the%20concept%20of%20the%20non-linearity%20signature%0Aof%20DNN%2C%20the%20first%20theoretically%20sound%20solution%20for%20approximately%20measuring%20the%0Anon-linearity%20of%20deep%20neural%20networks.%20Built%20upon%20a%20score%20derived%20from%0Aclosed-form%20optimal%20transport%20mappings%2C%20this%20signature%20provides%20a%20better%0Aunderstanding%20of%20the%20inner%20workings%20of%20a%20wide%20range%20of%20DNN%20architectures%20and%0Alearning%20paradigms%2C%20with%20a%20particular%20emphasis%20on%20the%20computer%20vision%20task.%20We%0Aprovide%20extensive%20experimental%20results%20that%20highlight%20the%20practical%20usefulness%0Aof%20the%20proposed%20non-linearity%20signature%20and%20its%20potential%20for%20long-reaching%0Aimplications.%20The%20code%20for%20our%20work%20is%20available%20at%0Ahttps%3A//github.com/qbouniot/AffScoreDeep%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.11439v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Alexnet%2520to%2520Transformers%253A%2520Measuring%2520the%2520Non-linearity%2520of%2520Deep%2520Neural%250A%2520%2520Networks%2520with%2520Affine%2520Optimal%2520Transport%26entry.906535625%3DQuentin%2520Bouniot%2520and%2520Ievgen%2520Redko%2520and%2520Anton%2520Mallasto%2520and%2520Charlotte%2520Laclau%2520and%2520Karol%2520Arndt%2520and%2520Oliver%2520Struckmeier%2520and%2520Markus%2520Heinonen%2520and%2520Ville%2520Kyrki%2520and%2520Samuel%2520Kaski%26entry.1292438233%3D%2520%2520In%2520the%2520last%2520decade%252C%2520we%2520have%2520witnessed%2520the%2520introduction%2520of%2520several%2520novel%2520deep%250Aneural%2520network%2520%2528DNN%2529%2520architectures%2520exhibiting%2520ever-increasing%2520performance%250Aacross%2520diverse%2520tasks.%2520Explaining%2520the%2520upward%2520trend%2520of%2520their%2520performance%252C%250Ahowever%252C%2520remains%2520difficult%2520as%2520different%2520DNN%2520architectures%2520of%2520comparable%2520depth%250Aand%2520width%2520--%2520common%2520factors%2520associated%2520with%2520their%2520expressive%2520power%2520--%2520may%250Aexhibit%2520a%2520drastically%2520different%2520performance%2520even%2520when%2520trained%2520on%2520the%2520same%250Adataset.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520concept%2520of%2520the%2520non-linearity%2520signature%250Aof%2520DNN%252C%2520the%2520first%2520theoretically%2520sound%2520solution%2520for%2520approximately%2520measuring%2520the%250Anon-linearity%2520of%2520deep%2520neural%2520networks.%2520Built%2520upon%2520a%2520score%2520derived%2520from%250Aclosed-form%2520optimal%2520transport%2520mappings%252C%2520this%2520signature%2520provides%2520a%2520better%250Aunderstanding%2520of%2520the%2520inner%2520workings%2520of%2520a%2520wide%2520range%2520of%2520DNN%2520architectures%2520and%250Alearning%2520paradigms%252C%2520with%2520a%2520particular%2520emphasis%2520on%2520the%2520computer%2520vision%2520task.%2520We%250Aprovide%2520extensive%2520experimental%2520results%2520that%2520highlight%2520the%2520practical%2520usefulness%250Aof%2520the%2520proposed%2520non-linearity%2520signature%2520and%2520its%2520potential%2520for%2520long-reaching%250Aimplications.%2520The%2520code%2520for%2520our%2520work%2520is%2520available%2520at%250Ahttps%253A//github.com/qbouniot/AffScoreDeep%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.11439v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Alexnet%20to%20Transformers%3A%20Measuring%20the%20Non-linearity%20of%20Deep%20Neural%0A%20%20Networks%20with%20Affine%20Optimal%20Transport&entry.906535625=Quentin%20Bouniot%20and%20Ievgen%20Redko%20and%20Anton%20Mallasto%20and%20Charlotte%20Laclau%20and%20Karol%20Arndt%20and%20Oliver%20Struckmeier%20and%20Markus%20Heinonen%20and%20Ville%20Kyrki%20and%20Samuel%20Kaski&entry.1292438233=%20%20In%20the%20last%20decade%2C%20we%20have%20witnessed%20the%20introduction%20of%20several%20novel%20deep%0Aneural%20network%20%28DNN%29%20architectures%20exhibiting%20ever-increasing%20performance%0Aacross%20diverse%20tasks.%20Explaining%20the%20upward%20trend%20of%20their%20performance%2C%0Ahowever%2C%20remains%20difficult%20as%20different%20DNN%20architectures%20of%20comparable%20depth%0Aand%20width%20--%20common%20factors%20associated%20with%20their%20expressive%20power%20--%20may%0Aexhibit%20a%20drastically%20different%20performance%20even%20when%20trained%20on%20the%20same%0Adataset.%20In%20this%20paper%2C%20we%20introduce%20the%20concept%20of%20the%20non-linearity%20signature%0Aof%20DNN%2C%20the%20first%20theoretically%20sound%20solution%20for%20approximately%20measuring%20the%0Anon-linearity%20of%20deep%20neural%20networks.%20Built%20upon%20a%20score%20derived%20from%0Aclosed-form%20optimal%20transport%20mappings%2C%20this%20signature%20provides%20a%20better%0Aunderstanding%20of%20the%20inner%20workings%20of%20a%20wide%20range%20of%20DNN%20architectures%20and%0Alearning%20paradigms%2C%20with%20a%20particular%20emphasis%20on%20the%20computer%20vision%20task.%20We%0Aprovide%20extensive%20experimental%20results%20that%20highlight%20the%20practical%20usefulness%0Aof%20the%20proposed%20non-linearity%20signature%20and%20its%20potential%20for%20long-reaching%0Aimplications.%20The%20code%20for%20our%20work%20is%20available%20at%0Ahttps%3A//github.com/qbouniot/AffScoreDeep%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.11439v3&entry.124074799=Read"},
{"title": "Affine Invariant Ensemble Transform Methods to Improve Predictive\n  Uncertainty in Neural Networks", "author": "Diksha Bhandari and Jakiw Pidstrigach and Sebastian Reich", "abstract": "  We consider the problem of performing Bayesian inference for logistic\nregression using appropriate extensions of the ensemble Kalman filter. Two\ninteracting particle systems are proposed that sample from an approximate\nposterior and prove quantitative convergence rates of these interacting\nparticle systems to their mean-field limit as the number of particles tends to\ninfinity. Furthermore, we apply these techniques and examine their\neffectiveness as methods of Bayesian approximation for quantifying predictive\nuncertainty in neural networks.\n", "link": "http://arxiv.org/abs/2309.04742v2", "date": "2024-07-01", "relevancy": 2.1208, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5559}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.539}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5111}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Affine%20Invariant%20Ensemble%20Transform%20Methods%20to%20Improve%20Predictive%0A%20%20Uncertainty%20in%20Neural%20Networks&body=Title%3A%20Affine%20Invariant%20Ensemble%20Transform%20Methods%20to%20Improve%20Predictive%0A%20%20Uncertainty%20in%20Neural%20Networks%0AAuthor%3A%20Diksha%20Bhandari%20and%20Jakiw%20Pidstrigach%20and%20Sebastian%20Reich%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20performing%20Bayesian%20inference%20for%20logistic%0Aregression%20using%20appropriate%20extensions%20of%20the%20ensemble%20Kalman%20filter.%20Two%0Ainteracting%20particle%20systems%20are%20proposed%20that%20sample%20from%20an%20approximate%0Aposterior%20and%20prove%20quantitative%20convergence%20rates%20of%20these%20interacting%0Aparticle%20systems%20to%20their%20mean-field%20limit%20as%20the%20number%20of%20particles%20tends%20to%0Ainfinity.%20Furthermore%2C%20we%20apply%20these%20techniques%20and%20examine%20their%0Aeffectiveness%20as%20methods%20of%20Bayesian%20approximation%20for%20quantifying%20predictive%0Auncertainty%20in%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.04742v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAffine%2520Invariant%2520Ensemble%2520Transform%2520Methods%2520to%2520Improve%2520Predictive%250A%2520%2520Uncertainty%2520in%2520Neural%2520Networks%26entry.906535625%3DDiksha%2520Bhandari%2520and%2520Jakiw%2520Pidstrigach%2520and%2520Sebastian%2520Reich%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520performing%2520Bayesian%2520inference%2520for%2520logistic%250Aregression%2520using%2520appropriate%2520extensions%2520of%2520the%2520ensemble%2520Kalman%2520filter.%2520Two%250Ainteracting%2520particle%2520systems%2520are%2520proposed%2520that%2520sample%2520from%2520an%2520approximate%250Aposterior%2520and%2520prove%2520quantitative%2520convergence%2520rates%2520of%2520these%2520interacting%250Aparticle%2520systems%2520to%2520their%2520mean-field%2520limit%2520as%2520the%2520number%2520of%2520particles%2520tends%2520to%250Ainfinity.%2520Furthermore%252C%2520we%2520apply%2520these%2520techniques%2520and%2520examine%2520their%250Aeffectiveness%2520as%2520methods%2520of%2520Bayesian%2520approximation%2520for%2520quantifying%2520predictive%250Auncertainty%2520in%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.04742v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Affine%20Invariant%20Ensemble%20Transform%20Methods%20to%20Improve%20Predictive%0A%20%20Uncertainty%20in%20Neural%20Networks&entry.906535625=Diksha%20Bhandari%20and%20Jakiw%20Pidstrigach%20and%20Sebastian%20Reich&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20performing%20Bayesian%20inference%20for%20logistic%0Aregression%20using%20appropriate%20extensions%20of%20the%20ensemble%20Kalman%20filter.%20Two%0Ainteracting%20particle%20systems%20are%20proposed%20that%20sample%20from%20an%20approximate%0Aposterior%20and%20prove%20quantitative%20convergence%20rates%20of%20these%20interacting%0Aparticle%20systems%20to%20their%20mean-field%20limit%20as%20the%20number%20of%20particles%20tends%20to%0Ainfinity.%20Furthermore%2C%20we%20apply%20these%20techniques%20and%20examine%20their%0Aeffectiveness%20as%20methods%20of%20Bayesian%20approximation%20for%20quantifying%20predictive%0Auncertainty%20in%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.04742v2&entry.124074799=Read"},
{"title": "An Efficient Instance Segmentation Framework Based on Oriented Bounding\n  Boxes", "author": "Zhen Zhou and Junfeng Fan and Yunkai Ma and Sihan Zhao and Fengshui Jing and Min Tan", "abstract": "  Instance segmentation for completely occluded objects and dense objects in\nrobot vision measurement are two challenging tasks. To uniformly deal with\nthem, this paper proposes a unified coarse-to-fine instance segmentation\nframework, CFNet, which uses box prompt-based segmentation foundation models\n(BSMs), e.g., Segment Anything Model. Specifically, CFNet first detects\noriented bounding boxes (OBBs) to distinguish instances and provide coarse\nlocalization information. Then, it predicts OBB prompt-related masks for fine\nsegmentation. CFNet performs instance segmentation with OBBs that only contain\npartial object boundaries on occluders to predict occluded object instances,\nwhich overcomes the difficulty of existing amodal instance segmentation methods\nin directly predicting occluded objects. In addition, since OBBs only serve as\nprompts, CFNet alleviates the over-dependence on bounding box detection\nperformance of current instance segmentation methods using OBBs for dense\nobjects. Moreover, to enable BSMs to handle OBB prompts, we propose a novel OBB\nprompt encoder. To make CFNet more lightweight, we perform knowledge\ndistillation on it and introduce a Gaussian label smoothing method for teacher\nmodel outputs. Experiments demonstrate that CFNet outperforms current instance\nsegmentation methods on both industrial and public datasets. The code is\navailable at https://github.com/zhen6618/OBBInstanceSegmentation.\n", "link": "http://arxiv.org/abs/2401.08174v3", "date": "2024-07-01", "relevancy": 2.0447, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5346}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5142}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4988}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Instance%20Segmentation%20Framework%20Based%20on%20Oriented%20Bounding%0A%20%20Boxes&body=Title%3A%20An%20Efficient%20Instance%20Segmentation%20Framework%20Based%20on%20Oriented%20Bounding%0A%20%20Boxes%0AAuthor%3A%20Zhen%20Zhou%20and%20Junfeng%20Fan%20and%20Yunkai%20Ma%20and%20Sihan%20Zhao%20and%20Fengshui%20Jing%20and%20Min%20Tan%0AAbstract%3A%20%20%20Instance%20segmentation%20for%20completely%20occluded%20objects%20and%20dense%20objects%20in%0Arobot%20vision%20measurement%20are%20two%20challenging%20tasks.%20To%20uniformly%20deal%20with%0Athem%2C%20this%20paper%20proposes%20a%20unified%20coarse-to-fine%20instance%20segmentation%0Aframework%2C%20CFNet%2C%20which%20uses%20box%20prompt-based%20segmentation%20foundation%20models%0A%28BSMs%29%2C%20e.g.%2C%20Segment%20Anything%20Model.%20Specifically%2C%20CFNet%20first%20detects%0Aoriented%20bounding%20boxes%20%28OBBs%29%20to%20distinguish%20instances%20and%20provide%20coarse%0Alocalization%20information.%20Then%2C%20it%20predicts%20OBB%20prompt-related%20masks%20for%20fine%0Asegmentation.%20CFNet%20performs%20instance%20segmentation%20with%20OBBs%20that%20only%20contain%0Apartial%20object%20boundaries%20on%20occluders%20to%20predict%20occluded%20object%20instances%2C%0Awhich%20overcomes%20the%20difficulty%20of%20existing%20amodal%20instance%20segmentation%20methods%0Ain%20directly%20predicting%20occluded%20objects.%20In%20addition%2C%20since%20OBBs%20only%20serve%20as%0Aprompts%2C%20CFNet%20alleviates%20the%20over-dependence%20on%20bounding%20box%20detection%0Aperformance%20of%20current%20instance%20segmentation%20methods%20using%20OBBs%20for%20dense%0Aobjects.%20Moreover%2C%20to%20enable%20BSMs%20to%20handle%20OBB%20prompts%2C%20we%20propose%20a%20novel%20OBB%0Aprompt%20encoder.%20To%20make%20CFNet%20more%20lightweight%2C%20we%20perform%20knowledge%0Adistillation%20on%20it%20and%20introduce%20a%20Gaussian%20label%20smoothing%20method%20for%20teacher%0Amodel%20outputs.%20Experiments%20demonstrate%20that%20CFNet%20outperforms%20current%20instance%0Asegmentation%20methods%20on%20both%20industrial%20and%20public%20datasets.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/zhen6618/OBBInstanceSegmentation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.08174v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAn%2520Efficient%2520Instance%2520Segmentation%2520Framework%2520Based%2520on%2520Oriented%2520Bounding%250A%2520%2520Boxes%26entry.906535625%3DZhen%2520Zhou%2520and%2520Junfeng%2520Fan%2520and%2520Yunkai%2520Ma%2520and%2520Sihan%2520Zhao%2520and%2520Fengshui%2520Jing%2520and%2520Min%2520Tan%26entry.1292438233%3D%2520%2520Instance%2520segmentation%2520for%2520completely%2520occluded%2520objects%2520and%2520dense%2520objects%2520in%250Arobot%2520vision%2520measurement%2520are%2520two%2520challenging%2520tasks.%2520To%2520uniformly%2520deal%2520with%250Athem%252C%2520this%2520paper%2520proposes%2520a%2520unified%2520coarse-to-fine%2520instance%2520segmentation%250Aframework%252C%2520CFNet%252C%2520which%2520uses%2520box%2520prompt-based%2520segmentation%2520foundation%2520models%250A%2528BSMs%2529%252C%2520e.g.%252C%2520Segment%2520Anything%2520Model.%2520Specifically%252C%2520CFNet%2520first%2520detects%250Aoriented%2520bounding%2520boxes%2520%2528OBBs%2529%2520to%2520distinguish%2520instances%2520and%2520provide%2520coarse%250Alocalization%2520information.%2520Then%252C%2520it%2520predicts%2520OBB%2520prompt-related%2520masks%2520for%2520fine%250Asegmentation.%2520CFNet%2520performs%2520instance%2520segmentation%2520with%2520OBBs%2520that%2520only%2520contain%250Apartial%2520object%2520boundaries%2520on%2520occluders%2520to%2520predict%2520occluded%2520object%2520instances%252C%250Awhich%2520overcomes%2520the%2520difficulty%2520of%2520existing%2520amodal%2520instance%2520segmentation%2520methods%250Ain%2520directly%2520predicting%2520occluded%2520objects.%2520In%2520addition%252C%2520since%2520OBBs%2520only%2520serve%2520as%250Aprompts%252C%2520CFNet%2520alleviates%2520the%2520over-dependence%2520on%2520bounding%2520box%2520detection%250Aperformance%2520of%2520current%2520instance%2520segmentation%2520methods%2520using%2520OBBs%2520for%2520dense%250Aobjects.%2520Moreover%252C%2520to%2520enable%2520BSMs%2520to%2520handle%2520OBB%2520prompts%252C%2520we%2520propose%2520a%2520novel%2520OBB%250Aprompt%2520encoder.%2520To%2520make%2520CFNet%2520more%2520lightweight%252C%2520we%2520perform%2520knowledge%250Adistillation%2520on%2520it%2520and%2520introduce%2520a%2520Gaussian%2520label%2520smoothing%2520method%2520for%2520teacher%250Amodel%2520outputs.%2520Experiments%2520demonstrate%2520that%2520CFNet%2520outperforms%2520current%2520instance%250Asegmentation%2520methods%2520on%2520both%2520industrial%2520and%2520public%2520datasets.%2520The%2520code%2520is%250Aavailable%2520at%2520https%253A//github.com/zhen6618/OBBInstanceSegmentation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.08174v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Instance%20Segmentation%20Framework%20Based%20on%20Oriented%20Bounding%0A%20%20Boxes&entry.906535625=Zhen%20Zhou%20and%20Junfeng%20Fan%20and%20Yunkai%20Ma%20and%20Sihan%20Zhao%20and%20Fengshui%20Jing%20and%20Min%20Tan&entry.1292438233=%20%20Instance%20segmentation%20for%20completely%20occluded%20objects%20and%20dense%20objects%20in%0Arobot%20vision%20measurement%20are%20two%20challenging%20tasks.%20To%20uniformly%20deal%20with%0Athem%2C%20this%20paper%20proposes%20a%20unified%20coarse-to-fine%20instance%20segmentation%0Aframework%2C%20CFNet%2C%20which%20uses%20box%20prompt-based%20segmentation%20foundation%20models%0A%28BSMs%29%2C%20e.g.%2C%20Segment%20Anything%20Model.%20Specifically%2C%20CFNet%20first%20detects%0Aoriented%20bounding%20boxes%20%28OBBs%29%20to%20distinguish%20instances%20and%20provide%20coarse%0Alocalization%20information.%20Then%2C%20it%20predicts%20OBB%20prompt-related%20masks%20for%20fine%0Asegmentation.%20CFNet%20performs%20instance%20segmentation%20with%20OBBs%20that%20only%20contain%0Apartial%20object%20boundaries%20on%20occluders%20to%20predict%20occluded%20object%20instances%2C%0Awhich%20overcomes%20the%20difficulty%20of%20existing%20amodal%20instance%20segmentation%20methods%0Ain%20directly%20predicting%20occluded%20objects.%20In%20addition%2C%20since%20OBBs%20only%20serve%20as%0Aprompts%2C%20CFNet%20alleviates%20the%20over-dependence%20on%20bounding%20box%20detection%0Aperformance%20of%20current%20instance%20segmentation%20methods%20using%20OBBs%20for%20dense%0Aobjects.%20Moreover%2C%20to%20enable%20BSMs%20to%20handle%20OBB%20prompts%2C%20we%20propose%20a%20novel%20OBB%0Aprompt%20encoder.%20To%20make%20CFNet%20more%20lightweight%2C%20we%20perform%20knowledge%0Adistillation%20on%20it%20and%20introduce%20a%20Gaussian%20label%20smoothing%20method%20for%20teacher%0Amodel%20outputs.%20Experiments%20demonstrate%20that%20CFNet%20outperforms%20current%20instance%0Asegmentation%20methods%20on%20both%20industrial%20and%20public%20datasets.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/zhen6618/OBBInstanceSegmentation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.08174v3&entry.124074799=Read"},
{"title": "On the Convergence of Multi-objective Optimization under Generalized\n  Smoothness", "author": "Qi Zhang and Peiyao Xiao and Kaiyi Ji and Shaofeng Zou", "abstract": "  Multi-objective optimization (MOO) is receiving more attention in various\nfields such as multi-task learning. Recent works provide some effective\nalgorithms with theoretical analysis but they are limited by the standard\n$L$-smooth or bounded-gradient assumptions, which are typically unsatisfactory\nfor neural networks, such as recurrent neural networks (RNNs) and transformers.\nIn this paper, we study a more general and realistic class of $\\ell$-smooth\nloss functions, where $\\ell$ is a general non-decreasing function of gradient\nnorm. We develop two novel single-loop algorithms for $\\ell$-smooth MOO\nproblems, Generalized Smooth Multi-objective Gradient descent (GSMGrad) and its\nstochastic variant, Stochastic Generalized Smooth Multi-objective Gradient\ndescent (SGSMGrad), which approximate the conflict-avoidant (CA) direction that\nmaximizes the minimum improvement among objectives. We provide a comprehensive\nconvergence analysis of both algorithms and show that they converge to an\n$\\epsilon$-accurate Pareto stationary point with a guaranteed $\\epsilon$-level\naverage CA distance (i.e., the gap between the updating direction and the CA\ndirection) over all iterations, where totally $\\mathcal{O}(\\epsilon^{-2})$ and\n$\\mathcal{O}(\\epsilon^{-4})$ samples are needed for deterministic and\nstochastic settings, respectively. Our algorithms can also guarantee a tighter\n$\\epsilon$-level CA distance in each iteration using more samples. Moreover, we\npropose a practical variant of GSMGrad named GSMGrad-FA using only\nconstant-level time and space, while achieving the same performance guarantee\nas GSMGrad. Our experiments validate our theory and demonstrate the\neffectiveness of the proposed methods.\n", "link": "http://arxiv.org/abs/2405.19440v3", "date": "2024-07-01", "relevancy": 1.955, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5079}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4869}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Convergence%20of%20Multi-objective%20Optimization%20under%20Generalized%0A%20%20Smoothness&body=Title%3A%20On%20the%20Convergence%20of%20Multi-objective%20Optimization%20under%20Generalized%0A%20%20Smoothness%0AAuthor%3A%20Qi%20Zhang%20and%20Peiyao%20Xiao%20and%20Kaiyi%20Ji%20and%20Shaofeng%20Zou%0AAbstract%3A%20%20%20Multi-objective%20optimization%20%28MOO%29%20is%20receiving%20more%20attention%20in%20various%0Afields%20such%20as%20multi-task%20learning.%20Recent%20works%20provide%20some%20effective%0Aalgorithms%20with%20theoretical%20analysis%20but%20they%20are%20limited%20by%20the%20standard%0A%24L%24-smooth%20or%20bounded-gradient%20assumptions%2C%20which%20are%20typically%20unsatisfactory%0Afor%20neural%20networks%2C%20such%20as%20recurrent%20neural%20networks%20%28RNNs%29%20and%20transformers.%0AIn%20this%20paper%2C%20we%20study%20a%20more%20general%20and%20realistic%20class%20of%20%24%5Cell%24-smooth%0Aloss%20functions%2C%20where%20%24%5Cell%24%20is%20a%20general%20non-decreasing%20function%20of%20gradient%0Anorm.%20We%20develop%20two%20novel%20single-loop%20algorithms%20for%20%24%5Cell%24-smooth%20MOO%0Aproblems%2C%20Generalized%20Smooth%20Multi-objective%20Gradient%20descent%20%28GSMGrad%29%20and%20its%0Astochastic%20variant%2C%20Stochastic%20Generalized%20Smooth%20Multi-objective%20Gradient%0Adescent%20%28SGSMGrad%29%2C%20which%20approximate%20the%20conflict-avoidant%20%28CA%29%20direction%20that%0Amaximizes%20the%20minimum%20improvement%20among%20objectives.%20We%20provide%20a%20comprehensive%0Aconvergence%20analysis%20of%20both%20algorithms%20and%20show%20that%20they%20converge%20to%20an%0A%24%5Cepsilon%24-accurate%20Pareto%20stationary%20point%20with%20a%20guaranteed%20%24%5Cepsilon%24-level%0Aaverage%20CA%20distance%20%28i.e.%2C%20the%20gap%20between%20the%20updating%20direction%20and%20the%20CA%0Adirection%29%20over%20all%20iterations%2C%20where%20totally%20%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-2%7D%29%24%20and%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-4%7D%29%24%20samples%20are%20needed%20for%20deterministic%20and%0Astochastic%20settings%2C%20respectively.%20Our%20algorithms%20can%20also%20guarantee%20a%20tighter%0A%24%5Cepsilon%24-level%20CA%20distance%20in%20each%20iteration%20using%20more%20samples.%20Moreover%2C%20we%0Apropose%20a%20practical%20variant%20of%20GSMGrad%20named%20GSMGrad-FA%20using%20only%0Aconstant-level%20time%20and%20space%2C%20while%20achieving%20the%20same%20performance%20guarantee%0Aas%20GSMGrad.%20Our%20experiments%20validate%20our%20theory%20and%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.19440v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Convergence%2520of%2520Multi-objective%2520Optimization%2520under%2520Generalized%250A%2520%2520Smoothness%26entry.906535625%3DQi%2520Zhang%2520and%2520Peiyao%2520Xiao%2520and%2520Kaiyi%2520Ji%2520and%2520Shaofeng%2520Zou%26entry.1292438233%3D%2520%2520Multi-objective%2520optimization%2520%2528MOO%2529%2520is%2520receiving%2520more%2520attention%2520in%2520various%250Afields%2520such%2520as%2520multi-task%2520learning.%2520Recent%2520works%2520provide%2520some%2520effective%250Aalgorithms%2520with%2520theoretical%2520analysis%2520but%2520they%2520are%2520limited%2520by%2520the%2520standard%250A%2524L%2524-smooth%2520or%2520bounded-gradient%2520assumptions%252C%2520which%2520are%2520typically%2520unsatisfactory%250Afor%2520neural%2520networks%252C%2520such%2520as%2520recurrent%2520neural%2520networks%2520%2528RNNs%2529%2520and%2520transformers.%250AIn%2520this%2520paper%252C%2520we%2520study%2520a%2520more%2520general%2520and%2520realistic%2520class%2520of%2520%2524%255Cell%2524-smooth%250Aloss%2520functions%252C%2520where%2520%2524%255Cell%2524%2520is%2520a%2520general%2520non-decreasing%2520function%2520of%2520gradient%250Anorm.%2520We%2520develop%2520two%2520novel%2520single-loop%2520algorithms%2520for%2520%2524%255Cell%2524-smooth%2520MOO%250Aproblems%252C%2520Generalized%2520Smooth%2520Multi-objective%2520Gradient%2520descent%2520%2528GSMGrad%2529%2520and%2520its%250Astochastic%2520variant%252C%2520Stochastic%2520Generalized%2520Smooth%2520Multi-objective%2520Gradient%250Adescent%2520%2528SGSMGrad%2529%252C%2520which%2520approximate%2520the%2520conflict-avoidant%2520%2528CA%2529%2520direction%2520that%250Amaximizes%2520the%2520minimum%2520improvement%2520among%2520objectives.%2520We%2520provide%2520a%2520comprehensive%250Aconvergence%2520analysis%2520of%2520both%2520algorithms%2520and%2520show%2520that%2520they%2520converge%2520to%2520an%250A%2524%255Cepsilon%2524-accurate%2520Pareto%2520stationary%2520point%2520with%2520a%2520guaranteed%2520%2524%255Cepsilon%2524-level%250Aaverage%2520CA%2520distance%2520%2528i.e.%252C%2520the%2520gap%2520between%2520the%2520updating%2520direction%2520and%2520the%2520CA%250Adirection%2529%2520over%2520all%2520iterations%252C%2520where%2520totally%2520%2524%255Cmathcal%257BO%257D%2528%255Cepsilon%255E%257B-2%257D%2529%2524%2520and%250A%2524%255Cmathcal%257BO%257D%2528%255Cepsilon%255E%257B-4%257D%2529%2524%2520samples%2520are%2520needed%2520for%2520deterministic%2520and%250Astochastic%2520settings%252C%2520respectively.%2520Our%2520algorithms%2520can%2520also%2520guarantee%2520a%2520tighter%250A%2524%255Cepsilon%2524-level%2520CA%2520distance%2520in%2520each%2520iteration%2520using%2520more%2520samples.%2520Moreover%252C%2520we%250Apropose%2520a%2520practical%2520variant%2520of%2520GSMGrad%2520named%2520GSMGrad-FA%2520using%2520only%250Aconstant-level%2520time%2520and%2520space%252C%2520while%2520achieving%2520the%2520same%2520performance%2520guarantee%250Aas%2520GSMGrad.%2520Our%2520experiments%2520validate%2520our%2520theory%2520and%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.19440v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Convergence%20of%20Multi-objective%20Optimization%20under%20Generalized%0A%20%20Smoothness&entry.906535625=Qi%20Zhang%20and%20Peiyao%20Xiao%20and%20Kaiyi%20Ji%20and%20Shaofeng%20Zou&entry.1292438233=%20%20Multi-objective%20optimization%20%28MOO%29%20is%20receiving%20more%20attention%20in%20various%0Afields%20such%20as%20multi-task%20learning.%20Recent%20works%20provide%20some%20effective%0Aalgorithms%20with%20theoretical%20analysis%20but%20they%20are%20limited%20by%20the%20standard%0A%24L%24-smooth%20or%20bounded-gradient%20assumptions%2C%20which%20are%20typically%20unsatisfactory%0Afor%20neural%20networks%2C%20such%20as%20recurrent%20neural%20networks%20%28RNNs%29%20and%20transformers.%0AIn%20this%20paper%2C%20we%20study%20a%20more%20general%20and%20realistic%20class%20of%20%24%5Cell%24-smooth%0Aloss%20functions%2C%20where%20%24%5Cell%24%20is%20a%20general%20non-decreasing%20function%20of%20gradient%0Anorm.%20We%20develop%20two%20novel%20single-loop%20algorithms%20for%20%24%5Cell%24-smooth%20MOO%0Aproblems%2C%20Generalized%20Smooth%20Multi-objective%20Gradient%20descent%20%28GSMGrad%29%20and%20its%0Astochastic%20variant%2C%20Stochastic%20Generalized%20Smooth%20Multi-objective%20Gradient%0Adescent%20%28SGSMGrad%29%2C%20which%20approximate%20the%20conflict-avoidant%20%28CA%29%20direction%20that%0Amaximizes%20the%20minimum%20improvement%20among%20objectives.%20We%20provide%20a%20comprehensive%0Aconvergence%20analysis%20of%20both%20algorithms%20and%20show%20that%20they%20converge%20to%20an%0A%24%5Cepsilon%24-accurate%20Pareto%20stationary%20point%20with%20a%20guaranteed%20%24%5Cepsilon%24-level%0Aaverage%20CA%20distance%20%28i.e.%2C%20the%20gap%20between%20the%20updating%20direction%20and%20the%20CA%0Adirection%29%20over%20all%20iterations%2C%20where%20totally%20%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-2%7D%29%24%20and%0A%24%5Cmathcal%7BO%7D%28%5Cepsilon%5E%7B-4%7D%29%24%20samples%20are%20needed%20for%20deterministic%20and%0Astochastic%20settings%2C%20respectively.%20Our%20algorithms%20can%20also%20guarantee%20a%20tighter%0A%24%5Cepsilon%24-level%20CA%20distance%20in%20each%20iteration%20using%20more%20samples.%20Moreover%2C%20we%0Apropose%20a%20practical%20variant%20of%20GSMGrad%20named%20GSMGrad-FA%20using%20only%0Aconstant-level%20time%20and%20space%2C%20while%20achieving%20the%20same%20performance%20guarantee%0Aas%20GSMGrad.%20Our%20experiments%20validate%20our%20theory%20and%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.19440v3&entry.124074799=Read"},
{"title": "Biology-inspired joint distribution neurons based on Hierarchical\n  Correlation Reconstruction allowing for multidirectional neural networks", "author": "Jarek Duda", "abstract": "  Biological neural networks seem qualitatively superior (e.g. in learning,\nflexibility, robustness) from current artificial like Multi-Layer Perceptron\n(MLP) or Kolmogorov-Arnold Network (KAN). Simultaneously, in contrast to them:\nhave fundamentally multidirectional signal propagation~\\cite{axon}, also of\nprobability distributions e.g. for uncertainty estimation, and are believed not\nbeing able to use standard backpropagation training~\\cite{backprop}. There are\nproposed novel artificial neurons based on HCR (Hierarchical Correlation\nReconstruction) removing the above low level differences: with neurons\ncontaining local joint distribution model (of its connections), representing\njoint density on normalized variables as just linear combination among\n$(f_\\mathbf{j})$ orthonormal polynomials: $\\rho(\\mathbf{x})=\\sum_{\\mathbf{j}\\in\nB} a_\\mathbf{j} f_\\mathbf{j}(\\mathbf{x})$ for $\\mathbf{x} \\in [0,1]^d$ and $B$\nsome chosen basis, with basis growth approaching complete description of joint\ndistribution. By various index summations of such $(a_\\mathbf{j})$ tensor as\nneuron parameters, we get simple formulas for e.g. conditional expected values\nfor propagation in any direction, like $E[x|y,z]$, $E[y|x]$, which degenerate\nto KAN-like parametrization if restricting to pairwise dependencies. Such HCR\nnetwork can also propagate probability distributions (also joint) like\n$\\rho(y,z|x)$. It also allows for additional training approaches, like direct\n$(a_\\mathbf{j})$ estimation, through tensor decomposition, or more biologically\nplausible information bottleneck training: layers directly influencing only\nneighbors, optimizing content to maximize information about the next layer, and\nminimizing about the previous to minimize the noise.\n", "link": "http://arxiv.org/abs/2405.05097v3", "date": "2024-07-01", "relevancy": 1.9295, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5242}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.505}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4431}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Biology-inspired%20joint%20distribution%20neurons%20based%20on%20Hierarchical%0A%20%20Correlation%20Reconstruction%20allowing%20for%20multidirectional%20neural%20networks&body=Title%3A%20Biology-inspired%20joint%20distribution%20neurons%20based%20on%20Hierarchical%0A%20%20Correlation%20Reconstruction%20allowing%20for%20multidirectional%20neural%20networks%0AAuthor%3A%20Jarek%20Duda%0AAbstract%3A%20%20%20Biological%20neural%20networks%20seem%20qualitatively%20superior%20%28e.g.%20in%20learning%2C%0Aflexibility%2C%20robustness%29%20from%20current%20artificial%20like%20Multi-Layer%20Perceptron%0A%28MLP%29%20or%20Kolmogorov-Arnold%20Network%20%28KAN%29.%20Simultaneously%2C%20in%20contrast%20to%20them%3A%0Ahave%20fundamentally%20multidirectional%20signal%20propagation~%5Ccite%7Baxon%7D%2C%20also%20of%0Aprobability%20distributions%20e.g.%20for%20uncertainty%20estimation%2C%20and%20are%20believed%20not%0Abeing%20able%20to%20use%20standard%20backpropagation%20training~%5Ccite%7Bbackprop%7D.%20There%20are%0Aproposed%20novel%20artificial%20neurons%20based%20on%20HCR%20%28Hierarchical%20Correlation%0AReconstruction%29%20removing%20the%20above%20low%20level%20differences%3A%20with%20neurons%0Acontaining%20local%20joint%20distribution%20model%20%28of%20its%20connections%29%2C%20representing%0Ajoint%20density%20on%20normalized%20variables%20as%20just%20linear%20combination%20among%0A%24%28f_%5Cmathbf%7Bj%7D%29%24%20orthonormal%20polynomials%3A%20%24%5Crho%28%5Cmathbf%7Bx%7D%29%3D%5Csum_%7B%5Cmathbf%7Bj%7D%5Cin%0AB%7D%20a_%5Cmathbf%7Bj%7D%20f_%5Cmathbf%7Bj%7D%28%5Cmathbf%7Bx%7D%29%24%20for%20%24%5Cmathbf%7Bx%7D%20%5Cin%20%5B0%2C1%5D%5Ed%24%20and%20%24B%24%0Asome%20chosen%20basis%2C%20with%20basis%20growth%20approaching%20complete%20description%20of%20joint%0Adistribution.%20By%20various%20index%20summations%20of%20such%20%24%28a_%5Cmathbf%7Bj%7D%29%24%20tensor%20as%0Aneuron%20parameters%2C%20we%20get%20simple%20formulas%20for%20e.g.%20conditional%20expected%20values%0Afor%20propagation%20in%20any%20direction%2C%20like%20%24E%5Bx%7Cy%2Cz%5D%24%2C%20%24E%5By%7Cx%5D%24%2C%20which%20degenerate%0Ato%20KAN-like%20parametrization%20if%20restricting%20to%20pairwise%20dependencies.%20Such%20HCR%0Anetwork%20can%20also%20propagate%20probability%20distributions%20%28also%20joint%29%20like%0A%24%5Crho%28y%2Cz%7Cx%29%24.%20It%20also%20allows%20for%20additional%20training%20approaches%2C%20like%20direct%0A%24%28a_%5Cmathbf%7Bj%7D%29%24%20estimation%2C%20through%20tensor%20decomposition%2C%20or%20more%20biologically%0Aplausible%20information%20bottleneck%20training%3A%20layers%20directly%20influencing%20only%0Aneighbors%2C%20optimizing%20content%20to%20maximize%20information%20about%20the%20next%20layer%2C%20and%0Aminimizing%20about%20the%20previous%20to%20minimize%20the%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05097v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiology-inspired%2520joint%2520distribution%2520neurons%2520based%2520on%2520Hierarchical%250A%2520%2520Correlation%2520Reconstruction%2520allowing%2520for%2520multidirectional%2520neural%2520networks%26entry.906535625%3DJarek%2520Duda%26entry.1292438233%3D%2520%2520Biological%2520neural%2520networks%2520seem%2520qualitatively%2520superior%2520%2528e.g.%2520in%2520learning%252C%250Aflexibility%252C%2520robustness%2529%2520from%2520current%2520artificial%2520like%2520Multi-Layer%2520Perceptron%250A%2528MLP%2529%2520or%2520Kolmogorov-Arnold%2520Network%2520%2528KAN%2529.%2520Simultaneously%252C%2520in%2520contrast%2520to%2520them%253A%250Ahave%2520fundamentally%2520multidirectional%2520signal%2520propagation~%255Ccite%257Baxon%257D%252C%2520also%2520of%250Aprobability%2520distributions%2520e.g.%2520for%2520uncertainty%2520estimation%252C%2520and%2520are%2520believed%2520not%250Abeing%2520able%2520to%2520use%2520standard%2520backpropagation%2520training~%255Ccite%257Bbackprop%257D.%2520There%2520are%250Aproposed%2520novel%2520artificial%2520neurons%2520based%2520on%2520HCR%2520%2528Hierarchical%2520Correlation%250AReconstruction%2529%2520removing%2520the%2520above%2520low%2520level%2520differences%253A%2520with%2520neurons%250Acontaining%2520local%2520joint%2520distribution%2520model%2520%2528of%2520its%2520connections%2529%252C%2520representing%250Ajoint%2520density%2520on%2520normalized%2520variables%2520as%2520just%2520linear%2520combination%2520among%250A%2524%2528f_%255Cmathbf%257Bj%257D%2529%2524%2520orthonormal%2520polynomials%253A%2520%2524%255Crho%2528%255Cmathbf%257Bx%257D%2529%253D%255Csum_%257B%255Cmathbf%257Bj%257D%255Cin%250AB%257D%2520a_%255Cmathbf%257Bj%257D%2520f_%255Cmathbf%257Bj%257D%2528%255Cmathbf%257Bx%257D%2529%2524%2520for%2520%2524%255Cmathbf%257Bx%257D%2520%255Cin%2520%255B0%252C1%255D%255Ed%2524%2520and%2520%2524B%2524%250Asome%2520chosen%2520basis%252C%2520with%2520basis%2520growth%2520approaching%2520complete%2520description%2520of%2520joint%250Adistribution.%2520By%2520various%2520index%2520summations%2520of%2520such%2520%2524%2528a_%255Cmathbf%257Bj%257D%2529%2524%2520tensor%2520as%250Aneuron%2520parameters%252C%2520we%2520get%2520simple%2520formulas%2520for%2520e.g.%2520conditional%2520expected%2520values%250Afor%2520propagation%2520in%2520any%2520direction%252C%2520like%2520%2524E%255Bx%257Cy%252Cz%255D%2524%252C%2520%2524E%255By%257Cx%255D%2524%252C%2520which%2520degenerate%250Ato%2520KAN-like%2520parametrization%2520if%2520restricting%2520to%2520pairwise%2520dependencies.%2520Such%2520HCR%250Anetwork%2520can%2520also%2520propagate%2520probability%2520distributions%2520%2528also%2520joint%2529%2520like%250A%2524%255Crho%2528y%252Cz%257Cx%2529%2524.%2520It%2520also%2520allows%2520for%2520additional%2520training%2520approaches%252C%2520like%2520direct%250A%2524%2528a_%255Cmathbf%257Bj%257D%2529%2524%2520estimation%252C%2520through%2520tensor%2520decomposition%252C%2520or%2520more%2520biologically%250Aplausible%2520information%2520bottleneck%2520training%253A%2520layers%2520directly%2520influencing%2520only%250Aneighbors%252C%2520optimizing%2520content%2520to%2520maximize%2520information%2520about%2520the%2520next%2520layer%252C%2520and%250Aminimizing%2520about%2520the%2520previous%2520to%2520minimize%2520the%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05097v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Biology-inspired%20joint%20distribution%20neurons%20based%20on%20Hierarchical%0A%20%20Correlation%20Reconstruction%20allowing%20for%20multidirectional%20neural%20networks&entry.906535625=Jarek%20Duda&entry.1292438233=%20%20Biological%20neural%20networks%20seem%20qualitatively%20superior%20%28e.g.%20in%20learning%2C%0Aflexibility%2C%20robustness%29%20from%20current%20artificial%20like%20Multi-Layer%20Perceptron%0A%28MLP%29%20or%20Kolmogorov-Arnold%20Network%20%28KAN%29.%20Simultaneously%2C%20in%20contrast%20to%20them%3A%0Ahave%20fundamentally%20multidirectional%20signal%20propagation~%5Ccite%7Baxon%7D%2C%20also%20of%0Aprobability%20distributions%20e.g.%20for%20uncertainty%20estimation%2C%20and%20are%20believed%20not%0Abeing%20able%20to%20use%20standard%20backpropagation%20training~%5Ccite%7Bbackprop%7D.%20There%20are%0Aproposed%20novel%20artificial%20neurons%20based%20on%20HCR%20%28Hierarchical%20Correlation%0AReconstruction%29%20removing%20the%20above%20low%20level%20differences%3A%20with%20neurons%0Acontaining%20local%20joint%20distribution%20model%20%28of%20its%20connections%29%2C%20representing%0Ajoint%20density%20on%20normalized%20variables%20as%20just%20linear%20combination%20among%0A%24%28f_%5Cmathbf%7Bj%7D%29%24%20orthonormal%20polynomials%3A%20%24%5Crho%28%5Cmathbf%7Bx%7D%29%3D%5Csum_%7B%5Cmathbf%7Bj%7D%5Cin%0AB%7D%20a_%5Cmathbf%7Bj%7D%20f_%5Cmathbf%7Bj%7D%28%5Cmathbf%7Bx%7D%29%24%20for%20%24%5Cmathbf%7Bx%7D%20%5Cin%20%5B0%2C1%5D%5Ed%24%20and%20%24B%24%0Asome%20chosen%20basis%2C%20with%20basis%20growth%20approaching%20complete%20description%20of%20joint%0Adistribution.%20By%20various%20index%20summations%20of%20such%20%24%28a_%5Cmathbf%7Bj%7D%29%24%20tensor%20as%0Aneuron%20parameters%2C%20we%20get%20simple%20formulas%20for%20e.g.%20conditional%20expected%20values%0Afor%20propagation%20in%20any%20direction%2C%20like%20%24E%5Bx%7Cy%2Cz%5D%24%2C%20%24E%5By%7Cx%5D%24%2C%20which%20degenerate%0Ato%20KAN-like%20parametrization%20if%20restricting%20to%20pairwise%20dependencies.%20Such%20HCR%0Anetwork%20can%20also%20propagate%20probability%20distributions%20%28also%20joint%29%20like%0A%24%5Crho%28y%2Cz%7Cx%29%24.%20It%20also%20allows%20for%20additional%20training%20approaches%2C%20like%20direct%0A%24%28a_%5Cmathbf%7Bj%7D%29%24%20estimation%2C%20through%20tensor%20decomposition%2C%20or%20more%20biologically%0Aplausible%20information%20bottleneck%20training%3A%20layers%20directly%20influencing%20only%0Aneighbors%2C%20optimizing%20content%20to%20maximize%20information%20about%20the%20next%20layer%2C%20and%0Aminimizing%20about%20the%20previous%20to%20minimize%20the%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05097v3&entry.124074799=Read"},
{"title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated\n  Text", "author": "Abhimanyu Hans and Avi Schwarzschild and Valeriia Cherepanova and Hamid Kazemi and Aniruddha Saha and Micah Goldblum and Jonas Geiping and Tom Goldstein", "abstract": "  Detecting text generated by modern large language models is thought to be\nhard, as both LLMs and humans can exhibit a wide range of complex behaviors.\nHowever, we find that a score based on contrasting two closely related language\nmodels is highly accurate at separating human-generated and machine-generated\ntext. Based on this mechanism, we propose a novel LLM detector that only\nrequires simple calculations using a pair of pre-trained LLMs. The method,\ncalled Binoculars, achieves state-of-the-art accuracy without any training\ndata. It is capable of spotting machine text from a range of modern LLMs\nwithout any model-specific modifications. We comprehensively evaluate\nBinoculars on a number of text sources and in varied situations. Over a wide\nrange of document types, Binoculars detects over 90% of generated samples from\nChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being\ntrained on any ChatGPT data.\n", "link": "http://arxiv.org/abs/2401.12070v2", "date": "2024-07-01", "relevancy": 1.9207, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5002}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4831}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spotting%20LLMs%20With%20Binoculars%3A%20Zero-Shot%20Detection%20of%20Machine-Generated%0A%20%20Text&body=Title%3A%20Spotting%20LLMs%20With%20Binoculars%3A%20Zero-Shot%20Detection%20of%20Machine-Generated%0A%20%20Text%0AAuthor%3A%20Abhimanyu%20Hans%20and%20Avi%20Schwarzschild%20and%20Valeriia%20Cherepanova%20and%20Hamid%20Kazemi%20and%20Aniruddha%20Saha%20and%20Micah%20Goldblum%20and%20Jonas%20Geiping%20and%20Tom%20Goldstein%0AAbstract%3A%20%20%20Detecting%20text%20generated%20by%20modern%20large%20language%20models%20is%20thought%20to%20be%0Ahard%2C%20as%20both%20LLMs%20and%20humans%20can%20exhibit%20a%20wide%20range%20of%20complex%20behaviors.%0AHowever%2C%20we%20find%20that%20a%20score%20based%20on%20contrasting%20two%20closely%20related%20language%0Amodels%20is%20highly%20accurate%20at%20separating%20human-generated%20and%20machine-generated%0Atext.%20Based%20on%20this%20mechanism%2C%20we%20propose%20a%20novel%20LLM%20detector%20that%20only%0Arequires%20simple%20calculations%20using%20a%20pair%20of%20pre-trained%20LLMs.%20The%20method%2C%0Acalled%20Binoculars%2C%20achieves%20state-of-the-art%20accuracy%20without%20any%20training%0Adata.%20It%20is%20capable%20of%20spotting%20machine%20text%20from%20a%20range%20of%20modern%20LLMs%0Awithout%20any%20model-specific%20modifications.%20We%20comprehensively%20evaluate%0ABinoculars%20on%20a%20number%20of%20text%20sources%20and%20in%20varied%20situations.%20Over%20a%20wide%0Arange%20of%20document%20types%2C%20Binoculars%20detects%20over%2090%25%20of%20generated%20samples%20from%0AChatGPT%20%28and%20other%20LLMs%29%20at%20a%20false%20positive%20rate%20of%200.01%25%2C%20despite%20not%20being%0Atrained%20on%20any%20ChatGPT%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.12070v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpotting%2520LLMs%2520With%2520Binoculars%253A%2520Zero-Shot%2520Detection%2520of%2520Machine-Generated%250A%2520%2520Text%26entry.906535625%3DAbhimanyu%2520Hans%2520and%2520Avi%2520Schwarzschild%2520and%2520Valeriia%2520Cherepanova%2520and%2520Hamid%2520Kazemi%2520and%2520Aniruddha%2520Saha%2520and%2520Micah%2520Goldblum%2520and%2520Jonas%2520Geiping%2520and%2520Tom%2520Goldstein%26entry.1292438233%3D%2520%2520Detecting%2520text%2520generated%2520by%2520modern%2520large%2520language%2520models%2520is%2520thought%2520to%2520be%250Ahard%252C%2520as%2520both%2520LLMs%2520and%2520humans%2520can%2520exhibit%2520a%2520wide%2520range%2520of%2520complex%2520behaviors.%250AHowever%252C%2520we%2520find%2520that%2520a%2520score%2520based%2520on%2520contrasting%2520two%2520closely%2520related%2520language%250Amodels%2520is%2520highly%2520accurate%2520at%2520separating%2520human-generated%2520and%2520machine-generated%250Atext.%2520Based%2520on%2520this%2520mechanism%252C%2520we%2520propose%2520a%2520novel%2520LLM%2520detector%2520that%2520only%250Arequires%2520simple%2520calculations%2520using%2520a%2520pair%2520of%2520pre-trained%2520LLMs.%2520The%2520method%252C%250Acalled%2520Binoculars%252C%2520achieves%2520state-of-the-art%2520accuracy%2520without%2520any%2520training%250Adata.%2520It%2520is%2520capable%2520of%2520spotting%2520machine%2520text%2520from%2520a%2520range%2520of%2520modern%2520LLMs%250Awithout%2520any%2520model-specific%2520modifications.%2520We%2520comprehensively%2520evaluate%250ABinoculars%2520on%2520a%2520number%2520of%2520text%2520sources%2520and%2520in%2520varied%2520situations.%2520Over%2520a%2520wide%250Arange%2520of%2520document%2520types%252C%2520Binoculars%2520detects%2520over%252090%2525%2520of%2520generated%2520samples%2520from%250AChatGPT%2520%2528and%2520other%2520LLMs%2529%2520at%2520a%2520false%2520positive%2520rate%2520of%25200.01%2525%252C%2520despite%2520not%2520being%250Atrained%2520on%2520any%2520ChatGPT%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.12070v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spotting%20LLMs%20With%20Binoculars%3A%20Zero-Shot%20Detection%20of%20Machine-Generated%0A%20%20Text&entry.906535625=Abhimanyu%20Hans%20and%20Avi%20Schwarzschild%20and%20Valeriia%20Cherepanova%20and%20Hamid%20Kazemi%20and%20Aniruddha%20Saha%20and%20Micah%20Goldblum%20and%20Jonas%20Geiping%20and%20Tom%20Goldstein&entry.1292438233=%20%20Detecting%20text%20generated%20by%20modern%20large%20language%20models%20is%20thought%20to%20be%0Ahard%2C%20as%20both%20LLMs%20and%20humans%20can%20exhibit%20a%20wide%20range%20of%20complex%20behaviors.%0AHowever%2C%20we%20find%20that%20a%20score%20based%20on%20contrasting%20two%20closely%20related%20language%0Amodels%20is%20highly%20accurate%20at%20separating%20human-generated%20and%20machine-generated%0Atext.%20Based%20on%20this%20mechanism%2C%20we%20propose%20a%20novel%20LLM%20detector%20that%20only%0Arequires%20simple%20calculations%20using%20a%20pair%20of%20pre-trained%20LLMs.%20The%20method%2C%0Acalled%20Binoculars%2C%20achieves%20state-of-the-art%20accuracy%20without%20any%20training%0Adata.%20It%20is%20capable%20of%20spotting%20machine%20text%20from%20a%20range%20of%20modern%20LLMs%0Awithout%20any%20model-specific%20modifications.%20We%20comprehensively%20evaluate%0ABinoculars%20on%20a%20number%20of%20text%20sources%20and%20in%20varied%20situations.%20Over%20a%20wide%0Arange%20of%20document%20types%2C%20Binoculars%20detects%20over%2090%25%20of%20generated%20samples%20from%0AChatGPT%20%28and%20other%20LLMs%29%20at%20a%20false%20positive%20rate%20of%200.01%25%2C%20despite%20not%20being%0Atrained%20on%20any%20ChatGPT%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.12070v2&entry.124074799=Read"},
{"title": "Federated Temporal Difference Learning with Linear Function\n  Approximation under Environmental Heterogeneity", "author": "Han Wang and Aritra Mitra and Hamed Hassani and George J. Pappas and James Anderson", "abstract": "  We initiate the study of federated reinforcement learning under environmental\nheterogeneity by considering a policy evaluation problem. Our setup involves\n$N$ agents interacting with environments that share the same state and action\nspace but differ in their reward functions and state transition kernels.\nAssuming agents can communicate via a central server, we ask: Does exchanging\ninformation expedite the process of evaluating a common policy? To answer this\nquestion, we provide the first comprehensive finite-time analysis of a\nfederated temporal difference (TD) learning algorithm with linear function\napproximation, while accounting for Markovian sampling, heterogeneity in the\nagents' environments, and multiple local updates to save communication. Our\nanalysis crucially relies on several novel ingredients: (i) deriving\nperturbation bounds on TD fixed points as a function of the heterogeneity in\nthe agents' underlying Markov decision processes (MDPs); (ii) introducing a\nvirtual MDP to closely approximate the dynamics of the federated TD algorithm;\nand (iii) using the virtual MDP to make explicit connections to federated\noptimization. Putting these pieces together, we rigorously prove that in a\nlow-heterogeneity regime, exchanging model estimates leads to linear\nconvergence speedups in the number of agents.\n", "link": "http://arxiv.org/abs/2302.02212v2", "date": "2024-07-01", "relevancy": 1.9041, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4913}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4736}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4723}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Temporal%20Difference%20Learning%20with%20Linear%20Function%0A%20%20Approximation%20under%20Environmental%20Heterogeneity&body=Title%3A%20Federated%20Temporal%20Difference%20Learning%20with%20Linear%20Function%0A%20%20Approximation%20under%20Environmental%20Heterogeneity%0AAuthor%3A%20Han%20Wang%20and%20Aritra%20Mitra%20and%20Hamed%20Hassani%20and%20George%20J.%20Pappas%20and%20James%20Anderson%0AAbstract%3A%20%20%20We%20initiate%20the%20study%20of%20federated%20reinforcement%20learning%20under%20environmental%0Aheterogeneity%20by%20considering%20a%20policy%20evaluation%20problem.%20Our%20setup%20involves%0A%24N%24%20agents%20interacting%20with%20environments%20that%20share%20the%20same%20state%20and%20action%0Aspace%20but%20differ%20in%20their%20reward%20functions%20and%20state%20transition%20kernels.%0AAssuming%20agents%20can%20communicate%20via%20a%20central%20server%2C%20we%20ask%3A%20Does%20exchanging%0Ainformation%20expedite%20the%20process%20of%20evaluating%20a%20common%20policy%3F%20To%20answer%20this%0Aquestion%2C%20we%20provide%20the%20first%20comprehensive%20finite-time%20analysis%20of%20a%0Afederated%20temporal%20difference%20%28TD%29%20learning%20algorithm%20with%20linear%20function%0Aapproximation%2C%20while%20accounting%20for%20Markovian%20sampling%2C%20heterogeneity%20in%20the%0Aagents%27%20environments%2C%20and%20multiple%20local%20updates%20to%20save%20communication.%20Our%0Aanalysis%20crucially%20relies%20on%20several%20novel%20ingredients%3A%20%28i%29%20deriving%0Aperturbation%20bounds%20on%20TD%20fixed%20points%20as%20a%20function%20of%20the%20heterogeneity%20in%0Athe%20agents%27%20underlying%20Markov%20decision%20processes%20%28MDPs%29%3B%20%28ii%29%20introducing%20a%0Avirtual%20MDP%20to%20closely%20approximate%20the%20dynamics%20of%20the%20federated%20TD%20algorithm%3B%0Aand%20%28iii%29%20using%20the%20virtual%20MDP%20to%20make%20explicit%20connections%20to%20federated%0Aoptimization.%20Putting%20these%20pieces%20together%2C%20we%20rigorously%20prove%20that%20in%20a%0Alow-heterogeneity%20regime%2C%20exchanging%20model%20estimates%20leads%20to%20linear%0Aconvergence%20speedups%20in%20the%20number%20of%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.02212v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Temporal%2520Difference%2520Learning%2520with%2520Linear%2520Function%250A%2520%2520Approximation%2520under%2520Environmental%2520Heterogeneity%26entry.906535625%3DHan%2520Wang%2520and%2520Aritra%2520Mitra%2520and%2520Hamed%2520Hassani%2520and%2520George%2520J.%2520Pappas%2520and%2520James%2520Anderson%26entry.1292438233%3D%2520%2520We%2520initiate%2520the%2520study%2520of%2520federated%2520reinforcement%2520learning%2520under%2520environmental%250Aheterogeneity%2520by%2520considering%2520a%2520policy%2520evaluation%2520problem.%2520Our%2520setup%2520involves%250A%2524N%2524%2520agents%2520interacting%2520with%2520environments%2520that%2520share%2520the%2520same%2520state%2520and%2520action%250Aspace%2520but%2520differ%2520in%2520their%2520reward%2520functions%2520and%2520state%2520transition%2520kernels.%250AAssuming%2520agents%2520can%2520communicate%2520via%2520a%2520central%2520server%252C%2520we%2520ask%253A%2520Does%2520exchanging%250Ainformation%2520expedite%2520the%2520process%2520of%2520evaluating%2520a%2520common%2520policy%253F%2520To%2520answer%2520this%250Aquestion%252C%2520we%2520provide%2520the%2520first%2520comprehensive%2520finite-time%2520analysis%2520of%2520a%250Afederated%2520temporal%2520difference%2520%2528TD%2529%2520learning%2520algorithm%2520with%2520linear%2520function%250Aapproximation%252C%2520while%2520accounting%2520for%2520Markovian%2520sampling%252C%2520heterogeneity%2520in%2520the%250Aagents%2527%2520environments%252C%2520and%2520multiple%2520local%2520updates%2520to%2520save%2520communication.%2520Our%250Aanalysis%2520crucially%2520relies%2520on%2520several%2520novel%2520ingredients%253A%2520%2528i%2529%2520deriving%250Aperturbation%2520bounds%2520on%2520TD%2520fixed%2520points%2520as%2520a%2520function%2520of%2520the%2520heterogeneity%2520in%250Athe%2520agents%2527%2520underlying%2520Markov%2520decision%2520processes%2520%2528MDPs%2529%253B%2520%2528ii%2529%2520introducing%2520a%250Avirtual%2520MDP%2520to%2520closely%2520approximate%2520the%2520dynamics%2520of%2520the%2520federated%2520TD%2520algorithm%253B%250Aand%2520%2528iii%2529%2520using%2520the%2520virtual%2520MDP%2520to%2520make%2520explicit%2520connections%2520to%2520federated%250Aoptimization.%2520Putting%2520these%2520pieces%2520together%252C%2520we%2520rigorously%2520prove%2520that%2520in%2520a%250Alow-heterogeneity%2520regime%252C%2520exchanging%2520model%2520estimates%2520leads%2520to%2520linear%250Aconvergence%2520speedups%2520in%2520the%2520number%2520of%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.02212v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Temporal%20Difference%20Learning%20with%20Linear%20Function%0A%20%20Approximation%20under%20Environmental%20Heterogeneity&entry.906535625=Han%20Wang%20and%20Aritra%20Mitra%20and%20Hamed%20Hassani%20and%20George%20J.%20Pappas%20and%20James%20Anderson&entry.1292438233=%20%20We%20initiate%20the%20study%20of%20federated%20reinforcement%20learning%20under%20environmental%0Aheterogeneity%20by%20considering%20a%20policy%20evaluation%20problem.%20Our%20setup%20involves%0A%24N%24%20agents%20interacting%20with%20environments%20that%20share%20the%20same%20state%20and%20action%0Aspace%20but%20differ%20in%20their%20reward%20functions%20and%20state%20transition%20kernels.%0AAssuming%20agents%20can%20communicate%20via%20a%20central%20server%2C%20we%20ask%3A%20Does%20exchanging%0Ainformation%20expedite%20the%20process%20of%20evaluating%20a%20common%20policy%3F%20To%20answer%20this%0Aquestion%2C%20we%20provide%20the%20first%20comprehensive%20finite-time%20analysis%20of%20a%0Afederated%20temporal%20difference%20%28TD%29%20learning%20algorithm%20with%20linear%20function%0Aapproximation%2C%20while%20accounting%20for%20Markovian%20sampling%2C%20heterogeneity%20in%20the%0Aagents%27%20environments%2C%20and%20multiple%20local%20updates%20to%20save%20communication.%20Our%0Aanalysis%20crucially%20relies%20on%20several%20novel%20ingredients%3A%20%28i%29%20deriving%0Aperturbation%20bounds%20on%20TD%20fixed%20points%20as%20a%20function%20of%20the%20heterogeneity%20in%0Athe%20agents%27%20underlying%20Markov%20decision%20processes%20%28MDPs%29%3B%20%28ii%29%20introducing%20a%0Avirtual%20MDP%20to%20closely%20approximate%20the%20dynamics%20of%20the%20federated%20TD%20algorithm%3B%0Aand%20%28iii%29%20using%20the%20virtual%20MDP%20to%20make%20explicit%20connections%20to%20federated%0Aoptimization.%20Putting%20these%20pieces%20together%2C%20we%20rigorously%20prove%20that%20in%20a%0Alow-heterogeneity%20regime%2C%20exchanging%20model%20estimates%20leads%20to%20linear%0Aconvergence%20speedups%20in%20the%20number%20of%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.02212v2&entry.124074799=Read"},
{"title": "Adam-mini: Use Fewer Learning Rates To Gain More", "author": "Yushun Zhang and Congliang Chen and Ziniu Li and Tian Ding and Chenwei Wu and Yinyu Ye and Zhi-Quan Luo and Ruoyu Sun", "abstract": "  We propose Adam-mini, an optimizer that achieves on-par or better performance\nthan AdamW with 45% to 50% less memory footprint. Adam-mini reduces memory by\ncutting down the learning rate resources in Adam (i.e., $1/\\sqrt{v}$). We find\nthat $\\geq$ 90% of these learning rates in $v$ could be harmlessly removed if\nwe (1) carefully partition the parameters into blocks following our proposed\nprinciple on Hessian structure; (2) assign a single but good learning rate to\neach parameter block. We further find that, for each of these parameter blocks,\nthere exists a single high-quality learning rate that can outperform Adam,\nprovided that sufficient resources are available to search it out. We then\nprovide one cost-effective way to find good learning rates and propose\nAdam-mini. Empirically, we verify that Adam-mini performs on par or better than\nAdamW on various language models sized from 125M to 7B for pre-training,\nsupervised fine-tuning, and RLHF. The reduced memory footprint of Adam-mini\nalso alleviates communication overheads among GPUs and CPUs, thereby increasing\nthroughput. For instance, Adam-mini achieves 49.6% higher throughput than AdamW\nwhen pre-training Llama2-7B on $2\\times$ A800-80GB GPUs, which saves 33%\nwall-clock time for pre-training.\n", "link": "http://arxiv.org/abs/2406.16793v4", "date": "2024-07-01", "relevancy": 1.9, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.499}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4962}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adam-mini%3A%20Use%20Fewer%20Learning%20Rates%20To%20Gain%20More&body=Title%3A%20Adam-mini%3A%20Use%20Fewer%20Learning%20Rates%20To%20Gain%20More%0AAuthor%3A%20Yushun%20Zhang%20and%20Congliang%20Chen%20and%20Ziniu%20Li%20and%20Tian%20Ding%20and%20Chenwei%20Wu%20and%20Yinyu%20Ye%20and%20Zhi-Quan%20Luo%20and%20Ruoyu%20Sun%0AAbstract%3A%20%20%20We%20propose%20Adam-mini%2C%20an%20optimizer%20that%20achieves%20on-par%20or%20better%20performance%0Athan%20AdamW%20with%2045%25%20to%2050%25%20less%20memory%20footprint.%20Adam-mini%20reduces%20memory%20by%0Acutting%20down%20the%20learning%20rate%20resources%20in%20Adam%20%28i.e.%2C%20%241/%5Csqrt%7Bv%7D%24%29.%20We%20find%0Athat%20%24%5Cgeq%24%2090%25%20of%20these%20learning%20rates%20in%20%24v%24%20could%20be%20harmlessly%20removed%20if%0Awe%20%281%29%20carefully%20partition%20the%20parameters%20into%20blocks%20following%20our%20proposed%0Aprinciple%20on%20Hessian%20structure%3B%20%282%29%20assign%20a%20single%20but%20good%20learning%20rate%20to%0Aeach%20parameter%20block.%20We%20further%20find%20that%2C%20for%20each%20of%20these%20parameter%20blocks%2C%0Athere%20exists%20a%20single%20high-quality%20learning%20rate%20that%20can%20outperform%20Adam%2C%0Aprovided%20that%20sufficient%20resources%20are%20available%20to%20search%20it%20out.%20We%20then%0Aprovide%20one%20cost-effective%20way%20to%20find%20good%20learning%20rates%20and%20propose%0AAdam-mini.%20Empirically%2C%20we%20verify%20that%20Adam-mini%20performs%20on%20par%20or%20better%20than%0AAdamW%20on%20various%20language%20models%20sized%20from%20125M%20to%207B%20for%20pre-training%2C%0Asupervised%20fine-tuning%2C%20and%20RLHF.%20The%20reduced%20memory%20footprint%20of%20Adam-mini%0Aalso%20alleviates%20communication%20overheads%20among%20GPUs%20and%20CPUs%2C%20thereby%20increasing%0Athroughput.%20For%20instance%2C%20Adam-mini%20achieves%2049.6%25%20higher%20throughput%20than%20AdamW%0Awhen%20pre-training%20Llama2-7B%20on%20%242%5Ctimes%24%20A800-80GB%20GPUs%2C%20which%20saves%2033%25%0Awall-clock%20time%20for%20pre-training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16793v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdam-mini%253A%2520Use%2520Fewer%2520Learning%2520Rates%2520To%2520Gain%2520More%26entry.906535625%3DYushun%2520Zhang%2520and%2520Congliang%2520Chen%2520and%2520Ziniu%2520Li%2520and%2520Tian%2520Ding%2520and%2520Chenwei%2520Wu%2520and%2520Yinyu%2520Ye%2520and%2520Zhi-Quan%2520Luo%2520and%2520Ruoyu%2520Sun%26entry.1292438233%3D%2520%2520We%2520propose%2520Adam-mini%252C%2520an%2520optimizer%2520that%2520achieves%2520on-par%2520or%2520better%2520performance%250Athan%2520AdamW%2520with%252045%2525%2520to%252050%2525%2520less%2520memory%2520footprint.%2520Adam-mini%2520reduces%2520memory%2520by%250Acutting%2520down%2520the%2520learning%2520rate%2520resources%2520in%2520Adam%2520%2528i.e.%252C%2520%25241/%255Csqrt%257Bv%257D%2524%2529.%2520We%2520find%250Athat%2520%2524%255Cgeq%2524%252090%2525%2520of%2520these%2520learning%2520rates%2520in%2520%2524v%2524%2520could%2520be%2520harmlessly%2520removed%2520if%250Awe%2520%25281%2529%2520carefully%2520partition%2520the%2520parameters%2520into%2520blocks%2520following%2520our%2520proposed%250Aprinciple%2520on%2520Hessian%2520structure%253B%2520%25282%2529%2520assign%2520a%2520single%2520but%2520good%2520learning%2520rate%2520to%250Aeach%2520parameter%2520block.%2520We%2520further%2520find%2520that%252C%2520for%2520each%2520of%2520these%2520parameter%2520blocks%252C%250Athere%2520exists%2520a%2520single%2520high-quality%2520learning%2520rate%2520that%2520can%2520outperform%2520Adam%252C%250Aprovided%2520that%2520sufficient%2520resources%2520are%2520available%2520to%2520search%2520it%2520out.%2520We%2520then%250Aprovide%2520one%2520cost-effective%2520way%2520to%2520find%2520good%2520learning%2520rates%2520and%2520propose%250AAdam-mini.%2520Empirically%252C%2520we%2520verify%2520that%2520Adam-mini%2520performs%2520on%2520par%2520or%2520better%2520than%250AAdamW%2520on%2520various%2520language%2520models%2520sized%2520from%2520125M%2520to%25207B%2520for%2520pre-training%252C%250Asupervised%2520fine-tuning%252C%2520and%2520RLHF.%2520The%2520reduced%2520memory%2520footprint%2520of%2520Adam-mini%250Aalso%2520alleviates%2520communication%2520overheads%2520among%2520GPUs%2520and%2520CPUs%252C%2520thereby%2520increasing%250Athroughput.%2520For%2520instance%252C%2520Adam-mini%2520achieves%252049.6%2525%2520higher%2520throughput%2520than%2520AdamW%250Awhen%2520pre-training%2520Llama2-7B%2520on%2520%25242%255Ctimes%2524%2520A800-80GB%2520GPUs%252C%2520which%2520saves%252033%2525%250Awall-clock%2520time%2520for%2520pre-training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16793v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adam-mini%3A%20Use%20Fewer%20Learning%20Rates%20To%20Gain%20More&entry.906535625=Yushun%20Zhang%20and%20Congliang%20Chen%20and%20Ziniu%20Li%20and%20Tian%20Ding%20and%20Chenwei%20Wu%20and%20Yinyu%20Ye%20and%20Zhi-Quan%20Luo%20and%20Ruoyu%20Sun&entry.1292438233=%20%20We%20propose%20Adam-mini%2C%20an%20optimizer%20that%20achieves%20on-par%20or%20better%20performance%0Athan%20AdamW%20with%2045%25%20to%2050%25%20less%20memory%20footprint.%20Adam-mini%20reduces%20memory%20by%0Acutting%20down%20the%20learning%20rate%20resources%20in%20Adam%20%28i.e.%2C%20%241/%5Csqrt%7Bv%7D%24%29.%20We%20find%0Athat%20%24%5Cgeq%24%2090%25%20of%20these%20learning%20rates%20in%20%24v%24%20could%20be%20harmlessly%20removed%20if%0Awe%20%281%29%20carefully%20partition%20the%20parameters%20into%20blocks%20following%20our%20proposed%0Aprinciple%20on%20Hessian%20structure%3B%20%282%29%20assign%20a%20single%20but%20good%20learning%20rate%20to%0Aeach%20parameter%20block.%20We%20further%20find%20that%2C%20for%20each%20of%20these%20parameter%20blocks%2C%0Athere%20exists%20a%20single%20high-quality%20learning%20rate%20that%20can%20outperform%20Adam%2C%0Aprovided%20that%20sufficient%20resources%20are%20available%20to%20search%20it%20out.%20We%20then%0Aprovide%20one%20cost-effective%20way%20to%20find%20good%20learning%20rates%20and%20propose%0AAdam-mini.%20Empirically%2C%20we%20verify%20that%20Adam-mini%20performs%20on%20par%20or%20better%20than%0AAdamW%20on%20various%20language%20models%20sized%20from%20125M%20to%207B%20for%20pre-training%2C%0Asupervised%20fine-tuning%2C%20and%20RLHF.%20The%20reduced%20memory%20footprint%20of%20Adam-mini%0Aalso%20alleviates%20communication%20overheads%20among%20GPUs%20and%20CPUs%2C%20thereby%20increasing%0Athroughput.%20For%20instance%2C%20Adam-mini%20achieves%2049.6%25%20higher%20throughput%20than%20AdamW%0Awhen%20pre-training%20Llama2-7B%20on%20%242%5Ctimes%24%20A800-80GB%20GPUs%2C%20which%20saves%2033%25%0Awall-clock%20time%20for%20pre-training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16793v4&entry.124074799=Read"},
{"title": "Learning the boundary-to-domain mapping using Lifting Product Fourier\n  Neural Operators for partial differential equations", "author": "Aditya Kashi and Arka Daw and Muralikrishnan Gopalakrishnan Meena and Hao Lu", "abstract": "  Neural operators such as the Fourier Neural Operator (FNO) have been shown to\nprovide resolution-independent deep learning models that can learn mappings\nbetween function spaces. For example, an initial condition can be mapped to the\nsolution of a partial differential equation (PDE) at a future time-step using a\nneural operator. Despite the popularity of neural operators, their use to\npredict solution functions over a domain given only data over the boundary\n(such as a spatially varying Dirichlet boundary condition) remains unexplored.\nIn this paper, we refer to such problems as boundary-to-domain problems; they\nhave a wide range of applications in areas such as fluid mechanics, solid\nmechanics, heat transfer etc. We present a novel FNO-based architecture, named\nLifting Product FNO (or LP-FNO) which can map arbitrary boundary functions\ndefined on the lower-dimensional boundary to a solution in the entire domain.\nSpecifically, two FNOs defined on the lower-dimensional boundary are lifted\ninto the higher dimensional domain using our proposed lifting product layer. We\ndemonstrate the efficacy and resolution independence of the proposed LP-FNO for\nthe 2D Poisson equation.\n", "link": "http://arxiv.org/abs/2406.16740v2", "date": "2024-07-01", "relevancy": 1.8227, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4562}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4559}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20the%20boundary-to-domain%20mapping%20using%20Lifting%20Product%20Fourier%0A%20%20Neural%20Operators%20for%20partial%20differential%20equations&body=Title%3A%20Learning%20the%20boundary-to-domain%20mapping%20using%20Lifting%20Product%20Fourier%0A%20%20Neural%20Operators%20for%20partial%20differential%20equations%0AAuthor%3A%20Aditya%20Kashi%20and%20Arka%20Daw%20and%20Muralikrishnan%20Gopalakrishnan%20Meena%20and%20Hao%20Lu%0AAbstract%3A%20%20%20Neural%20operators%20such%20as%20the%20Fourier%20Neural%20Operator%20%28FNO%29%20have%20been%20shown%20to%0Aprovide%20resolution-independent%20deep%20learning%20models%20that%20can%20learn%20mappings%0Abetween%20function%20spaces.%20For%20example%2C%20an%20initial%20condition%20can%20be%20mapped%20to%20the%0Asolution%20of%20a%20partial%20differential%20equation%20%28PDE%29%20at%20a%20future%20time-step%20using%20a%0Aneural%20operator.%20Despite%20the%20popularity%20of%20neural%20operators%2C%20their%20use%20to%0Apredict%20solution%20functions%20over%20a%20domain%20given%20only%20data%20over%20the%20boundary%0A%28such%20as%20a%20spatially%20varying%20Dirichlet%20boundary%20condition%29%20remains%20unexplored.%0AIn%20this%20paper%2C%20we%20refer%20to%20such%20problems%20as%20boundary-to-domain%20problems%3B%20they%0Ahave%20a%20wide%20range%20of%20applications%20in%20areas%20such%20as%20fluid%20mechanics%2C%20solid%0Amechanics%2C%20heat%20transfer%20etc.%20We%20present%20a%20novel%20FNO-based%20architecture%2C%20named%0ALifting%20Product%20FNO%20%28or%20LP-FNO%29%20which%20can%20map%20arbitrary%20boundary%20functions%0Adefined%20on%20the%20lower-dimensional%20boundary%20to%20a%20solution%20in%20the%20entire%20domain.%0ASpecifically%2C%20two%20FNOs%20defined%20on%20the%20lower-dimensional%20boundary%20are%20lifted%0Ainto%20the%20higher%20dimensional%20domain%20using%20our%20proposed%20lifting%20product%20layer.%20We%0Ademonstrate%20the%20efficacy%20and%20resolution%20independence%20of%20the%20proposed%20LP-FNO%20for%0Athe%202D%20Poisson%20equation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16740v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520the%2520boundary-to-domain%2520mapping%2520using%2520Lifting%2520Product%2520Fourier%250A%2520%2520Neural%2520Operators%2520for%2520partial%2520differential%2520equations%26entry.906535625%3DAditya%2520Kashi%2520and%2520Arka%2520Daw%2520and%2520Muralikrishnan%2520Gopalakrishnan%2520Meena%2520and%2520Hao%2520Lu%26entry.1292438233%3D%2520%2520Neural%2520operators%2520such%2520as%2520the%2520Fourier%2520Neural%2520Operator%2520%2528FNO%2529%2520have%2520been%2520shown%2520to%250Aprovide%2520resolution-independent%2520deep%2520learning%2520models%2520that%2520can%2520learn%2520mappings%250Abetween%2520function%2520spaces.%2520For%2520example%252C%2520an%2520initial%2520condition%2520can%2520be%2520mapped%2520to%2520the%250Asolution%2520of%2520a%2520partial%2520differential%2520equation%2520%2528PDE%2529%2520at%2520a%2520future%2520time-step%2520using%2520a%250Aneural%2520operator.%2520Despite%2520the%2520popularity%2520of%2520neural%2520operators%252C%2520their%2520use%2520to%250Apredict%2520solution%2520functions%2520over%2520a%2520domain%2520given%2520only%2520data%2520over%2520the%2520boundary%250A%2528such%2520as%2520a%2520spatially%2520varying%2520Dirichlet%2520boundary%2520condition%2529%2520remains%2520unexplored.%250AIn%2520this%2520paper%252C%2520we%2520refer%2520to%2520such%2520problems%2520as%2520boundary-to-domain%2520problems%253B%2520they%250Ahave%2520a%2520wide%2520range%2520of%2520applications%2520in%2520areas%2520such%2520as%2520fluid%2520mechanics%252C%2520solid%250Amechanics%252C%2520heat%2520transfer%2520etc.%2520We%2520present%2520a%2520novel%2520FNO-based%2520architecture%252C%2520named%250ALifting%2520Product%2520FNO%2520%2528or%2520LP-FNO%2529%2520which%2520can%2520map%2520arbitrary%2520boundary%2520functions%250Adefined%2520on%2520the%2520lower-dimensional%2520boundary%2520to%2520a%2520solution%2520in%2520the%2520entire%2520domain.%250ASpecifically%252C%2520two%2520FNOs%2520defined%2520on%2520the%2520lower-dimensional%2520boundary%2520are%2520lifted%250Ainto%2520the%2520higher%2520dimensional%2520domain%2520using%2520our%2520proposed%2520lifting%2520product%2520layer.%2520We%250Ademonstrate%2520the%2520efficacy%2520and%2520resolution%2520independence%2520of%2520the%2520proposed%2520LP-FNO%2520for%250Athe%25202D%2520Poisson%2520equation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16740v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20the%20boundary-to-domain%20mapping%20using%20Lifting%20Product%20Fourier%0A%20%20Neural%20Operators%20for%20partial%20differential%20equations&entry.906535625=Aditya%20Kashi%20and%20Arka%20Daw%20and%20Muralikrishnan%20Gopalakrishnan%20Meena%20and%20Hao%20Lu&entry.1292438233=%20%20Neural%20operators%20such%20as%20the%20Fourier%20Neural%20Operator%20%28FNO%29%20have%20been%20shown%20to%0Aprovide%20resolution-independent%20deep%20learning%20models%20that%20can%20learn%20mappings%0Abetween%20function%20spaces.%20For%20example%2C%20an%20initial%20condition%20can%20be%20mapped%20to%20the%0Asolution%20of%20a%20partial%20differential%20equation%20%28PDE%29%20at%20a%20future%20time-step%20using%20a%0Aneural%20operator.%20Despite%20the%20popularity%20of%20neural%20operators%2C%20their%20use%20to%0Apredict%20solution%20functions%20over%20a%20domain%20given%20only%20data%20over%20the%20boundary%0A%28such%20as%20a%20spatially%20varying%20Dirichlet%20boundary%20condition%29%20remains%20unexplored.%0AIn%20this%20paper%2C%20we%20refer%20to%20such%20problems%20as%20boundary-to-domain%20problems%3B%20they%0Ahave%20a%20wide%20range%20of%20applications%20in%20areas%20such%20as%20fluid%20mechanics%2C%20solid%0Amechanics%2C%20heat%20transfer%20etc.%20We%20present%20a%20novel%20FNO-based%20architecture%2C%20named%0ALifting%20Product%20FNO%20%28or%20LP-FNO%29%20which%20can%20map%20arbitrary%20boundary%20functions%0Adefined%20on%20the%20lower-dimensional%20boundary%20to%20a%20solution%20in%20the%20entire%20domain.%0ASpecifically%2C%20two%20FNOs%20defined%20on%20the%20lower-dimensional%20boundary%20are%20lifted%0Ainto%20the%20higher%20dimensional%20domain%20using%20our%20proposed%20lifting%20product%20layer.%20We%0Ademonstrate%20the%20efficacy%20and%20resolution%20independence%20of%20the%20proposed%20LP-FNO%20for%0Athe%202D%20Poisson%20equation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16740v2&entry.124074799=Read"},
{"title": "Safe Linear Bandits over Unknown Polytopes", "author": "Aditya Gangrade and Tianrui Chen and Venkatesh Saligrama", "abstract": "  The safe linear bandit problem (SLB) is an online approach to linear\nprogramming with unknown objective and unknown roundwise constraints, under\nstochastic bandit feedback of rewards and safety risks of actions. We study the\ntradeoffs between efficacy and smooth safety costs of SLBs over polytopes, and\nthe role of aggressive doubly-optimistic play in avoiding the strong\nassumptions made by extant pessimistic-optimistic approaches.\n  We first elucidate an inherent hardness in SLBs due the lack of knowledge of\nconstraints: there exist `easy' instances, for which suboptimal extreme points\nhave large `gaps', but on which SLB methods must still incur $\\Omega(\\sqrt{T})$\nregret or safety violations, due to an inability to resolve unknown optima to\narbitrary precision. We then analyse a natural doubly-optimistic strategy for\nthe safe linear bandit problem, DOSS, which uses optimistic estimates of both\nreward and safety risks to select actions, and show that despite the lack of\nknowledge of constraints or feasible points, DOSS simultaneously obtains tight\ninstance-dependent $O(\\log^2 T)$ bounds on efficacy regret, and $\\tilde\nO(\\sqrt{T})$ bounds on safety violations. Further, when safety is demanded to a\nfinite precision, violations improve to $O(\\log^2 T).$ These results rely on a\nnovel dual analysis of linear bandits: we argue that \\algoname proceeds by\nactivating noisy versions of at least $d$ constraints in each round, which\nallows us to separately analyse rounds where a `poor' set of constraints is\nactivated, and rounds where `good' sets of constraints are activated. The costs\nin the former are controlled to $O(\\log^2 T)$ by developing new dual notions of\ngaps, based on global sensitivity analyses of linear programs, that quantify\nthe suboptimality of each such set of constraints. The latter costs are\ncontrolled to $O(1)$ by explicitly analysing the solutions of optimistic play.\n", "link": "http://arxiv.org/abs/2209.13694v3", "date": "2024-07-01", "relevancy": 1.8192, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4862}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4589}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4381}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safe%20Linear%20Bandits%20over%20Unknown%20Polytopes&body=Title%3A%20Safe%20Linear%20Bandits%20over%20Unknown%20Polytopes%0AAuthor%3A%20Aditya%20Gangrade%20and%20Tianrui%20Chen%20and%20Venkatesh%20Saligrama%0AAbstract%3A%20%20%20The%20safe%20linear%20bandit%20problem%20%28SLB%29%20is%20an%20online%20approach%20to%20linear%0Aprogramming%20with%20unknown%20objective%20and%20unknown%20roundwise%20constraints%2C%20under%0Astochastic%20bandit%20feedback%20of%20rewards%20and%20safety%20risks%20of%20actions.%20We%20study%20the%0Atradeoffs%20between%20efficacy%20and%20smooth%20safety%20costs%20of%20SLBs%20over%20polytopes%2C%20and%0Athe%20role%20of%20aggressive%20doubly-optimistic%20play%20in%20avoiding%20the%20strong%0Aassumptions%20made%20by%20extant%20pessimistic-optimistic%20approaches.%0A%20%20We%20first%20elucidate%20an%20inherent%20hardness%20in%20SLBs%20due%20the%20lack%20of%20knowledge%20of%0Aconstraints%3A%20there%20exist%20%60easy%27%20instances%2C%20for%20which%20suboptimal%20extreme%20points%0Ahave%20large%20%60gaps%27%2C%20but%20on%20which%20SLB%20methods%20must%20still%20incur%20%24%5COmega%28%5Csqrt%7BT%7D%29%24%0Aregret%20or%20safety%20violations%2C%20due%20to%20an%20inability%20to%20resolve%20unknown%20optima%20to%0Aarbitrary%20precision.%20We%20then%20analyse%20a%20natural%20doubly-optimistic%20strategy%20for%0Athe%20safe%20linear%20bandit%20problem%2C%20DOSS%2C%20which%20uses%20optimistic%20estimates%20of%20both%0Areward%20and%20safety%20risks%20to%20select%20actions%2C%20and%20show%20that%20despite%20the%20lack%20of%0Aknowledge%20of%20constraints%20or%20feasible%20points%2C%20DOSS%20simultaneously%20obtains%20tight%0Ainstance-dependent%20%24O%28%5Clog%5E2%20T%29%24%20bounds%20on%20efficacy%20regret%2C%20and%20%24%5Ctilde%0AO%28%5Csqrt%7BT%7D%29%24%20bounds%20on%20safety%20violations.%20Further%2C%20when%20safety%20is%20demanded%20to%20a%0Afinite%20precision%2C%20violations%20improve%20to%20%24O%28%5Clog%5E2%20T%29.%24%20These%20results%20rely%20on%20a%0Anovel%20dual%20analysis%20of%20linear%20bandits%3A%20we%20argue%20that%20%5Calgoname%20proceeds%20by%0Aactivating%20noisy%20versions%20of%20at%20least%20%24d%24%20constraints%20in%20each%20round%2C%20which%0Aallows%20us%20to%20separately%20analyse%20rounds%20where%20a%20%60poor%27%20set%20of%20constraints%20is%0Aactivated%2C%20and%20rounds%20where%20%60good%27%20sets%20of%20constraints%20are%20activated.%20The%20costs%0Ain%20the%20former%20are%20controlled%20to%20%24O%28%5Clog%5E2%20T%29%24%20by%20developing%20new%20dual%20notions%20of%0Agaps%2C%20based%20on%20global%20sensitivity%20analyses%20of%20linear%20programs%2C%20that%20quantify%0Athe%20suboptimality%20of%20each%20such%20set%20of%20constraints.%20The%20latter%20costs%20are%0Acontrolled%20to%20%24O%281%29%24%20by%20explicitly%20analysing%20the%20solutions%20of%20optimistic%20play.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2209.13694v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafe%2520Linear%2520Bandits%2520over%2520Unknown%2520Polytopes%26entry.906535625%3DAditya%2520Gangrade%2520and%2520Tianrui%2520Chen%2520and%2520Venkatesh%2520Saligrama%26entry.1292438233%3D%2520%2520The%2520safe%2520linear%2520bandit%2520problem%2520%2528SLB%2529%2520is%2520an%2520online%2520approach%2520to%2520linear%250Aprogramming%2520with%2520unknown%2520objective%2520and%2520unknown%2520roundwise%2520constraints%252C%2520under%250Astochastic%2520bandit%2520feedback%2520of%2520rewards%2520and%2520safety%2520risks%2520of%2520actions.%2520We%2520study%2520the%250Atradeoffs%2520between%2520efficacy%2520and%2520smooth%2520safety%2520costs%2520of%2520SLBs%2520over%2520polytopes%252C%2520and%250Athe%2520role%2520of%2520aggressive%2520doubly-optimistic%2520play%2520in%2520avoiding%2520the%2520strong%250Aassumptions%2520made%2520by%2520extant%2520pessimistic-optimistic%2520approaches.%250A%2520%2520We%2520first%2520elucidate%2520an%2520inherent%2520hardness%2520in%2520SLBs%2520due%2520the%2520lack%2520of%2520knowledge%2520of%250Aconstraints%253A%2520there%2520exist%2520%2560easy%2527%2520instances%252C%2520for%2520which%2520suboptimal%2520extreme%2520points%250Ahave%2520large%2520%2560gaps%2527%252C%2520but%2520on%2520which%2520SLB%2520methods%2520must%2520still%2520incur%2520%2524%255COmega%2528%255Csqrt%257BT%257D%2529%2524%250Aregret%2520or%2520safety%2520violations%252C%2520due%2520to%2520an%2520inability%2520to%2520resolve%2520unknown%2520optima%2520to%250Aarbitrary%2520precision.%2520We%2520then%2520analyse%2520a%2520natural%2520doubly-optimistic%2520strategy%2520for%250Athe%2520safe%2520linear%2520bandit%2520problem%252C%2520DOSS%252C%2520which%2520uses%2520optimistic%2520estimates%2520of%2520both%250Areward%2520and%2520safety%2520risks%2520to%2520select%2520actions%252C%2520and%2520show%2520that%2520despite%2520the%2520lack%2520of%250Aknowledge%2520of%2520constraints%2520or%2520feasible%2520points%252C%2520DOSS%2520simultaneously%2520obtains%2520tight%250Ainstance-dependent%2520%2524O%2528%255Clog%255E2%2520T%2529%2524%2520bounds%2520on%2520efficacy%2520regret%252C%2520and%2520%2524%255Ctilde%250AO%2528%255Csqrt%257BT%257D%2529%2524%2520bounds%2520on%2520safety%2520violations.%2520Further%252C%2520when%2520safety%2520is%2520demanded%2520to%2520a%250Afinite%2520precision%252C%2520violations%2520improve%2520to%2520%2524O%2528%255Clog%255E2%2520T%2529.%2524%2520These%2520results%2520rely%2520on%2520a%250Anovel%2520dual%2520analysis%2520of%2520linear%2520bandits%253A%2520we%2520argue%2520that%2520%255Calgoname%2520proceeds%2520by%250Aactivating%2520noisy%2520versions%2520of%2520at%2520least%2520%2524d%2524%2520constraints%2520in%2520each%2520round%252C%2520which%250Aallows%2520us%2520to%2520separately%2520analyse%2520rounds%2520where%2520a%2520%2560poor%2527%2520set%2520of%2520constraints%2520is%250Aactivated%252C%2520and%2520rounds%2520where%2520%2560good%2527%2520sets%2520of%2520constraints%2520are%2520activated.%2520The%2520costs%250Ain%2520the%2520former%2520are%2520controlled%2520to%2520%2524O%2528%255Clog%255E2%2520T%2529%2524%2520by%2520developing%2520new%2520dual%2520notions%2520of%250Agaps%252C%2520based%2520on%2520global%2520sensitivity%2520analyses%2520of%2520linear%2520programs%252C%2520that%2520quantify%250Athe%2520suboptimality%2520of%2520each%2520such%2520set%2520of%2520constraints.%2520The%2520latter%2520costs%2520are%250Acontrolled%2520to%2520%2524O%25281%2529%2524%2520by%2520explicitly%2520analysing%2520the%2520solutions%2520of%2520optimistic%2520play.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2209.13694v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safe%20Linear%20Bandits%20over%20Unknown%20Polytopes&entry.906535625=Aditya%20Gangrade%20and%20Tianrui%20Chen%20and%20Venkatesh%20Saligrama&entry.1292438233=%20%20The%20safe%20linear%20bandit%20problem%20%28SLB%29%20is%20an%20online%20approach%20to%20linear%0Aprogramming%20with%20unknown%20objective%20and%20unknown%20roundwise%20constraints%2C%20under%0Astochastic%20bandit%20feedback%20of%20rewards%20and%20safety%20risks%20of%20actions.%20We%20study%20the%0Atradeoffs%20between%20efficacy%20and%20smooth%20safety%20costs%20of%20SLBs%20over%20polytopes%2C%20and%0Athe%20role%20of%20aggressive%20doubly-optimistic%20play%20in%20avoiding%20the%20strong%0Aassumptions%20made%20by%20extant%20pessimistic-optimistic%20approaches.%0A%20%20We%20first%20elucidate%20an%20inherent%20hardness%20in%20SLBs%20due%20the%20lack%20of%20knowledge%20of%0Aconstraints%3A%20there%20exist%20%60easy%27%20instances%2C%20for%20which%20suboptimal%20extreme%20points%0Ahave%20large%20%60gaps%27%2C%20but%20on%20which%20SLB%20methods%20must%20still%20incur%20%24%5COmega%28%5Csqrt%7BT%7D%29%24%0Aregret%20or%20safety%20violations%2C%20due%20to%20an%20inability%20to%20resolve%20unknown%20optima%20to%0Aarbitrary%20precision.%20We%20then%20analyse%20a%20natural%20doubly-optimistic%20strategy%20for%0Athe%20safe%20linear%20bandit%20problem%2C%20DOSS%2C%20which%20uses%20optimistic%20estimates%20of%20both%0Areward%20and%20safety%20risks%20to%20select%20actions%2C%20and%20show%20that%20despite%20the%20lack%20of%0Aknowledge%20of%20constraints%20or%20feasible%20points%2C%20DOSS%20simultaneously%20obtains%20tight%0Ainstance-dependent%20%24O%28%5Clog%5E2%20T%29%24%20bounds%20on%20efficacy%20regret%2C%20and%20%24%5Ctilde%0AO%28%5Csqrt%7BT%7D%29%24%20bounds%20on%20safety%20violations.%20Further%2C%20when%20safety%20is%20demanded%20to%20a%0Afinite%20precision%2C%20violations%20improve%20to%20%24O%28%5Clog%5E2%20T%29.%24%20These%20results%20rely%20on%20a%0Anovel%20dual%20analysis%20of%20linear%20bandits%3A%20we%20argue%20that%20%5Calgoname%20proceeds%20by%0Aactivating%20noisy%20versions%20of%20at%20least%20%24d%24%20constraints%20in%20each%20round%2C%20which%0Aallows%20us%20to%20separately%20analyse%20rounds%20where%20a%20%60poor%27%20set%20of%20constraints%20is%0Aactivated%2C%20and%20rounds%20where%20%60good%27%20sets%20of%20constraints%20are%20activated.%20The%20costs%0Ain%20the%20former%20are%20controlled%20to%20%24O%28%5Clog%5E2%20T%29%24%20by%20developing%20new%20dual%20notions%20of%0Agaps%2C%20based%20on%20global%20sensitivity%20analyses%20of%20linear%20programs%2C%20that%20quantify%0Athe%20suboptimality%20of%20each%20such%20set%20of%20constraints.%20The%20latter%20costs%20are%0Acontrolled%20to%20%24O%281%29%24%20by%20explicitly%20analysing%20the%20solutions%20of%20optimistic%20play.%0A&entry.1838667208=http%3A//arxiv.org/abs/2209.13694v3&entry.124074799=Read"},
{"title": "Cutting through buggy adversarial example defenses: fixing 1 line of\n  code breaks Sabre", "author": "Nicholas Carlini", "abstract": "  Sabre is a defense to adversarial examples that was accepted at IEEE S&P\n2024. We first reveal significant flaws in the evaluation that point to clear\nsigns of gradient masking. We then show the cause of this gradient masking: a\nbug in the original evaluation code. By fixing a single line of code in the\noriginal repository, we reduce Sabre's robust accuracy to 0%. In response to\nthis, the authors modify the defense and introduce a new defense component not\ndescribed in the original paper. But this fix contains a second bug; modifying\none more line of code reduces robust accuracy to below baseline levels. After\nwe released the first version of our paper online, the authors introduced\nanother change to the defense; by commenting out one line of code during attack\nwe reduce the robust accuracy to 0% again.\n", "link": "http://arxiv.org/abs/2405.03672v3", "date": "2024-07-01", "relevancy": 1.7755, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4676}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4334}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4108}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cutting%20through%20buggy%20adversarial%20example%20defenses%3A%20fixing%201%20line%20of%0A%20%20code%20breaks%20Sabre&body=Title%3A%20Cutting%20through%20buggy%20adversarial%20example%20defenses%3A%20fixing%201%20line%20of%0A%20%20code%20breaks%20Sabre%0AAuthor%3A%20Nicholas%20Carlini%0AAbstract%3A%20%20%20Sabre%20is%20a%20defense%20to%20adversarial%20examples%20that%20was%20accepted%20at%20IEEE%20S%26P%0A2024.%20We%20first%20reveal%20significant%20flaws%20in%20the%20evaluation%20that%20point%20to%20clear%0Asigns%20of%20gradient%20masking.%20We%20then%20show%20the%20cause%20of%20this%20gradient%20masking%3A%20a%0Abug%20in%20the%20original%20evaluation%20code.%20By%20fixing%20a%20single%20line%20of%20code%20in%20the%0Aoriginal%20repository%2C%20we%20reduce%20Sabre%27s%20robust%20accuracy%20to%200%25.%20In%20response%20to%0Athis%2C%20the%20authors%20modify%20the%20defense%20and%20introduce%20a%20new%20defense%20component%20not%0Adescribed%20in%20the%20original%20paper.%20But%20this%20fix%20contains%20a%20second%20bug%3B%20modifying%0Aone%20more%20line%20of%20code%20reduces%20robust%20accuracy%20to%20below%20baseline%20levels.%20After%0Awe%20released%20the%20first%20version%20of%20our%20paper%20online%2C%20the%20authors%20introduced%0Aanother%20change%20to%20the%20defense%3B%20by%20commenting%20out%20one%20line%20of%20code%20during%20attack%0Awe%20reduce%20the%20robust%20accuracy%20to%200%25%20again.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.03672v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCutting%2520through%2520buggy%2520adversarial%2520example%2520defenses%253A%2520fixing%25201%2520line%2520of%250A%2520%2520code%2520breaks%2520Sabre%26entry.906535625%3DNicholas%2520Carlini%26entry.1292438233%3D%2520%2520Sabre%2520is%2520a%2520defense%2520to%2520adversarial%2520examples%2520that%2520was%2520accepted%2520at%2520IEEE%2520S%2526P%250A2024.%2520We%2520first%2520reveal%2520significant%2520flaws%2520in%2520the%2520evaluation%2520that%2520point%2520to%2520clear%250Asigns%2520of%2520gradient%2520masking.%2520We%2520then%2520show%2520the%2520cause%2520of%2520this%2520gradient%2520masking%253A%2520a%250Abug%2520in%2520the%2520original%2520evaluation%2520code.%2520By%2520fixing%2520a%2520single%2520line%2520of%2520code%2520in%2520the%250Aoriginal%2520repository%252C%2520we%2520reduce%2520Sabre%2527s%2520robust%2520accuracy%2520to%25200%2525.%2520In%2520response%2520to%250Athis%252C%2520the%2520authors%2520modify%2520the%2520defense%2520and%2520introduce%2520a%2520new%2520defense%2520component%2520not%250Adescribed%2520in%2520the%2520original%2520paper.%2520But%2520this%2520fix%2520contains%2520a%2520second%2520bug%253B%2520modifying%250Aone%2520more%2520line%2520of%2520code%2520reduces%2520robust%2520accuracy%2520to%2520below%2520baseline%2520levels.%2520After%250Awe%2520released%2520the%2520first%2520version%2520of%2520our%2520paper%2520online%252C%2520the%2520authors%2520introduced%250Aanother%2520change%2520to%2520the%2520defense%253B%2520by%2520commenting%2520out%2520one%2520line%2520of%2520code%2520during%2520attack%250Awe%2520reduce%2520the%2520robust%2520accuracy%2520to%25200%2525%2520again.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.03672v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cutting%20through%20buggy%20adversarial%20example%20defenses%3A%20fixing%201%20line%20of%0A%20%20code%20breaks%20Sabre&entry.906535625=Nicholas%20Carlini&entry.1292438233=%20%20Sabre%20is%20a%20defense%20to%20adversarial%20examples%20that%20was%20accepted%20at%20IEEE%20S%26P%0A2024.%20We%20first%20reveal%20significant%20flaws%20in%20the%20evaluation%20that%20point%20to%20clear%0Asigns%20of%20gradient%20masking.%20We%20then%20show%20the%20cause%20of%20this%20gradient%20masking%3A%20a%0Abug%20in%20the%20original%20evaluation%20code.%20By%20fixing%20a%20single%20line%20of%20code%20in%20the%0Aoriginal%20repository%2C%20we%20reduce%20Sabre%27s%20robust%20accuracy%20to%200%25.%20In%20response%20to%0Athis%2C%20the%20authors%20modify%20the%20defense%20and%20introduce%20a%20new%20defense%20component%20not%0Adescribed%20in%20the%20original%20paper.%20But%20this%20fix%20contains%20a%20second%20bug%3B%20modifying%0Aone%20more%20line%20of%20code%20reduces%20robust%20accuracy%20to%20below%20baseline%20levels.%20After%0Awe%20released%20the%20first%20version%20of%20our%20paper%20online%2C%20the%20authors%20introduced%0Aanother%20change%20to%20the%20defense%3B%20by%20commenting%20out%20one%20line%20of%20code%20during%20attack%0Awe%20reduce%20the%20robust%20accuracy%20to%200%25%20again.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.03672v3&entry.124074799=Read"},
{"title": "Text2Robot: Evolutionary Robot Design from Text Descriptions", "author": "Ryan P. Ringel and Zachary S. Charlick and Jiaxun Liu and Boxi Xia and Boyuan Chen", "abstract": "  Robot design has traditionally been costly and labor-intensive. Despite\nadvancements in automated processes, it remains challenging to navigate a vast\ndesign space while producing physically manufacturable robots. We introduce\nText2Robot, a framework that converts user text specifications and performance\npreferences into physical quadrupedal robots. Within minutes, Text2Robot can\nuse text-to-3D models to provide strong initializations of diverse\nmorphologies. Within a day, our geometric processing algorithms and\nbody-control co-optimization produce a walking robot by explicitly considering\nreal-world electronics and manufacturability. Text2Robot enables rapid\nprototyping and opens new opportunities for robot design with generative\nmodels.\n", "link": "http://arxiv.org/abs/2406.19963v2", "date": "2024-07-01", "relevancy": 1.7544, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.618}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5668}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text2Robot%3A%20Evolutionary%20Robot%20Design%20from%20Text%20Descriptions&body=Title%3A%20Text2Robot%3A%20Evolutionary%20Robot%20Design%20from%20Text%20Descriptions%0AAuthor%3A%20Ryan%20P.%20Ringel%20and%20Zachary%20S.%20Charlick%20and%20Jiaxun%20Liu%20and%20Boxi%20Xia%20and%20Boyuan%20Chen%0AAbstract%3A%20%20%20Robot%20design%20has%20traditionally%20been%20costly%20and%20labor-intensive.%20Despite%0Aadvancements%20in%20automated%20processes%2C%20it%20remains%20challenging%20to%20navigate%20a%20vast%0Adesign%20space%20while%20producing%20physically%20manufacturable%20robots.%20We%20introduce%0AText2Robot%2C%20a%20framework%20that%20converts%20user%20text%20specifications%20and%20performance%0Apreferences%20into%20physical%20quadrupedal%20robots.%20Within%20minutes%2C%20Text2Robot%20can%0Ause%20text-to-3D%20models%20to%20provide%20strong%20initializations%20of%20diverse%0Amorphologies.%20Within%20a%20day%2C%20our%20geometric%20processing%20algorithms%20and%0Abody-control%20co-optimization%20produce%20a%20walking%20robot%20by%20explicitly%20considering%0Areal-world%20electronics%20and%20manufacturability.%20Text2Robot%20enables%20rapid%0Aprototyping%20and%20opens%20new%20opportunities%20for%20robot%20design%20with%20generative%0Amodels.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.19963v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText2Robot%253A%2520Evolutionary%2520Robot%2520Design%2520from%2520Text%2520Descriptions%26entry.906535625%3DRyan%2520P.%2520Ringel%2520and%2520Zachary%2520S.%2520Charlick%2520and%2520Jiaxun%2520Liu%2520and%2520Boxi%2520Xia%2520and%2520Boyuan%2520Chen%26entry.1292438233%3D%2520%2520Robot%2520design%2520has%2520traditionally%2520been%2520costly%2520and%2520labor-intensive.%2520Despite%250Aadvancements%2520in%2520automated%2520processes%252C%2520it%2520remains%2520challenging%2520to%2520navigate%2520a%2520vast%250Adesign%2520space%2520while%2520producing%2520physically%2520manufacturable%2520robots.%2520We%2520introduce%250AText2Robot%252C%2520a%2520framework%2520that%2520converts%2520user%2520text%2520specifications%2520and%2520performance%250Apreferences%2520into%2520physical%2520quadrupedal%2520robots.%2520Within%2520minutes%252C%2520Text2Robot%2520can%250Ause%2520text-to-3D%2520models%2520to%2520provide%2520strong%2520initializations%2520of%2520diverse%250Amorphologies.%2520Within%2520a%2520day%252C%2520our%2520geometric%2520processing%2520algorithms%2520and%250Abody-control%2520co-optimization%2520produce%2520a%2520walking%2520robot%2520by%2520explicitly%2520considering%250Areal-world%2520electronics%2520and%2520manufacturability.%2520Text2Robot%2520enables%2520rapid%250Aprototyping%2520and%2520opens%2520new%2520opportunities%2520for%2520robot%2520design%2520with%2520generative%250Amodels.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.19963v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text2Robot%3A%20Evolutionary%20Robot%20Design%20from%20Text%20Descriptions&entry.906535625=Ryan%20P.%20Ringel%20and%20Zachary%20S.%20Charlick%20and%20Jiaxun%20Liu%20and%20Boxi%20Xia%20and%20Boyuan%20Chen&entry.1292438233=%20%20Robot%20design%20has%20traditionally%20been%20costly%20and%20labor-intensive.%20Despite%0Aadvancements%20in%20automated%20processes%2C%20it%20remains%20challenging%20to%20navigate%20a%20vast%0Adesign%20space%20while%20producing%20physically%20manufacturable%20robots.%20We%20introduce%0AText2Robot%2C%20a%20framework%20that%20converts%20user%20text%20specifications%20and%20performance%0Apreferences%20into%20physical%20quadrupedal%20robots.%20Within%20minutes%2C%20Text2Robot%20can%0Ause%20text-to-3D%20models%20to%20provide%20strong%20initializations%20of%20diverse%0Amorphologies.%20Within%20a%20day%2C%20our%20geometric%20processing%20algorithms%20and%0Abody-control%20co-optimization%20produce%20a%20walking%20robot%20by%20explicitly%20considering%0Areal-world%20electronics%20and%20manufacturability.%20Text2Robot%20enables%20rapid%0Aprototyping%20and%20opens%20new%20opportunities%20for%20robot%20design%20with%20generative%0Amodels.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.19963v2&entry.124074799=Read"},
{"title": "Comparing AI Algorithms for Optimizing Elliptic Curve Cryptography\n  Parameters in e-Commerce Integrations: A Pre-Quantum Analysis", "author": "Felipe Tellez and Jorge Ortiz", "abstract": "  This paper presents a comparative analysis between the Genetic Algorithm (GA)\nand Particle Swarm Optimization (PSO), two vital artificial intelligence\nalgorithms, focusing on optimizing Elliptic Curve Cryptography (ECC)\nparameters. These encompass the elliptic curve coefficients, prime number,\ngenerator point, group order, and cofactor. The study provides insights into\nwhich of the bio-inspired algorithms yields better optimization results for ECC\nconfigurations, examining performances under the same fitness function. This\nfunction incorporates methods to ensure robust ECC parameters, including\nassessing for singular or anomalous curves and applying Pollard's rho attack\nand Hasse's theorem for optimization precision. The optimized parameters\ngenerated by GA and PSO are tested in a simulated e-commerce environment,\ncontrasting with well-known curves like secp256k1 during the transmission of\norder messages using Elliptic Curve-Diffie Hellman (ECDH) and Hash-based\nMessage Authentication Code (HMAC). Focusing on traditional computing in the\npre-quantum era, this research highlights the efficacy of GA and PSO in ECC\noptimization, with implications for enhancing cybersecurity in third-party\ne-commerce integrations. We recommend the immediate consideration of these\nfindings before quantum computing's widespread adoption.\n", "link": "http://arxiv.org/abs/2310.06752v2", "date": "2024-07-01", "relevancy": 1.7481, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.3784}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.337}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.3334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Comparing%20AI%20Algorithms%20for%20Optimizing%20Elliptic%20Curve%20Cryptography%0A%20%20Parameters%20in%20e-Commerce%20Integrations%3A%20A%20Pre-Quantum%20Analysis&body=Title%3A%20Comparing%20AI%20Algorithms%20for%20Optimizing%20Elliptic%20Curve%20Cryptography%0A%20%20Parameters%20in%20e-Commerce%20Integrations%3A%20A%20Pre-Quantum%20Analysis%0AAuthor%3A%20Felipe%20Tellez%20and%20Jorge%20Ortiz%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comparative%20analysis%20between%20the%20Genetic%20Algorithm%20%28GA%29%0Aand%20Particle%20Swarm%20Optimization%20%28PSO%29%2C%20two%20vital%20artificial%20intelligence%0Aalgorithms%2C%20focusing%20on%20optimizing%20Elliptic%20Curve%20Cryptography%20%28ECC%29%0Aparameters.%20These%20encompass%20the%20elliptic%20curve%20coefficients%2C%20prime%20number%2C%0Agenerator%20point%2C%20group%20order%2C%20and%20cofactor.%20The%20study%20provides%20insights%20into%0Awhich%20of%20the%20bio-inspired%20algorithms%20yields%20better%20optimization%20results%20for%20ECC%0Aconfigurations%2C%20examining%20performances%20under%20the%20same%20fitness%20function.%20This%0Afunction%20incorporates%20methods%20to%20ensure%20robust%20ECC%20parameters%2C%20including%0Aassessing%20for%20singular%20or%20anomalous%20curves%20and%20applying%20Pollard%27s%20rho%20attack%0Aand%20Hasse%27s%20theorem%20for%20optimization%20precision.%20The%20optimized%20parameters%0Agenerated%20by%20GA%20and%20PSO%20are%20tested%20in%20a%20simulated%20e-commerce%20environment%2C%0Acontrasting%20with%20well-known%20curves%20like%20secp256k1%20during%20the%20transmission%20of%0Aorder%20messages%20using%20Elliptic%20Curve-Diffie%20Hellman%20%28ECDH%29%20and%20Hash-based%0AMessage%20Authentication%20Code%20%28HMAC%29.%20Focusing%20on%20traditional%20computing%20in%20the%0Apre-quantum%20era%2C%20this%20research%20highlights%20the%20efficacy%20of%20GA%20and%20PSO%20in%20ECC%0Aoptimization%2C%20with%20implications%20for%20enhancing%20cybersecurity%20in%20third-party%0Ae-commerce%20integrations.%20We%20recommend%20the%20immediate%20consideration%20of%20these%0Afindings%20before%20quantum%20computing%27s%20widespread%20adoption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.06752v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DComparing%2520AI%2520Algorithms%2520for%2520Optimizing%2520Elliptic%2520Curve%2520Cryptography%250A%2520%2520Parameters%2520in%2520e-Commerce%2520Integrations%253A%2520A%2520Pre-Quantum%2520Analysis%26entry.906535625%3DFelipe%2520Tellez%2520and%2520Jorge%2520Ortiz%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comparative%2520analysis%2520between%2520the%2520Genetic%2520Algorithm%2520%2528GA%2529%250Aand%2520Particle%2520Swarm%2520Optimization%2520%2528PSO%2529%252C%2520two%2520vital%2520artificial%2520intelligence%250Aalgorithms%252C%2520focusing%2520on%2520optimizing%2520Elliptic%2520Curve%2520Cryptography%2520%2528ECC%2529%250Aparameters.%2520These%2520encompass%2520the%2520elliptic%2520curve%2520coefficients%252C%2520prime%2520number%252C%250Agenerator%2520point%252C%2520group%2520order%252C%2520and%2520cofactor.%2520The%2520study%2520provides%2520insights%2520into%250Awhich%2520of%2520the%2520bio-inspired%2520algorithms%2520yields%2520better%2520optimization%2520results%2520for%2520ECC%250Aconfigurations%252C%2520examining%2520performances%2520under%2520the%2520same%2520fitness%2520function.%2520This%250Afunction%2520incorporates%2520methods%2520to%2520ensure%2520robust%2520ECC%2520parameters%252C%2520including%250Aassessing%2520for%2520singular%2520or%2520anomalous%2520curves%2520and%2520applying%2520Pollard%2527s%2520rho%2520attack%250Aand%2520Hasse%2527s%2520theorem%2520for%2520optimization%2520precision.%2520The%2520optimized%2520parameters%250Agenerated%2520by%2520GA%2520and%2520PSO%2520are%2520tested%2520in%2520a%2520simulated%2520e-commerce%2520environment%252C%250Acontrasting%2520with%2520well-known%2520curves%2520like%2520secp256k1%2520during%2520the%2520transmission%2520of%250Aorder%2520messages%2520using%2520Elliptic%2520Curve-Diffie%2520Hellman%2520%2528ECDH%2529%2520and%2520Hash-based%250AMessage%2520Authentication%2520Code%2520%2528HMAC%2529.%2520Focusing%2520on%2520traditional%2520computing%2520in%2520the%250Apre-quantum%2520era%252C%2520this%2520research%2520highlights%2520the%2520efficacy%2520of%2520GA%2520and%2520PSO%2520in%2520ECC%250Aoptimization%252C%2520with%2520implications%2520for%2520enhancing%2520cybersecurity%2520in%2520third-party%250Ae-commerce%2520integrations.%2520We%2520recommend%2520the%2520immediate%2520consideration%2520of%2520these%250Afindings%2520before%2520quantum%2520computing%2527s%2520widespread%2520adoption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.06752v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Comparing%20AI%20Algorithms%20for%20Optimizing%20Elliptic%20Curve%20Cryptography%0A%20%20Parameters%20in%20e-Commerce%20Integrations%3A%20A%20Pre-Quantum%20Analysis&entry.906535625=Felipe%20Tellez%20and%20Jorge%20Ortiz&entry.1292438233=%20%20This%20paper%20presents%20a%20comparative%20analysis%20between%20the%20Genetic%20Algorithm%20%28GA%29%0Aand%20Particle%20Swarm%20Optimization%20%28PSO%29%2C%20two%20vital%20artificial%20intelligence%0Aalgorithms%2C%20focusing%20on%20optimizing%20Elliptic%20Curve%20Cryptography%20%28ECC%29%0Aparameters.%20These%20encompass%20the%20elliptic%20curve%20coefficients%2C%20prime%20number%2C%0Agenerator%20point%2C%20group%20order%2C%20and%20cofactor.%20The%20study%20provides%20insights%20into%0Awhich%20of%20the%20bio-inspired%20algorithms%20yields%20better%20optimization%20results%20for%20ECC%0Aconfigurations%2C%20examining%20performances%20under%20the%20same%20fitness%20function.%20This%0Afunction%20incorporates%20methods%20to%20ensure%20robust%20ECC%20parameters%2C%20including%0Aassessing%20for%20singular%20or%20anomalous%20curves%20and%20applying%20Pollard%27s%20rho%20attack%0Aand%20Hasse%27s%20theorem%20for%20optimization%20precision.%20The%20optimized%20parameters%0Agenerated%20by%20GA%20and%20PSO%20are%20tested%20in%20a%20simulated%20e-commerce%20environment%2C%0Acontrasting%20with%20well-known%20curves%20like%20secp256k1%20during%20the%20transmission%20of%0Aorder%20messages%20using%20Elliptic%20Curve-Diffie%20Hellman%20%28ECDH%29%20and%20Hash-based%0AMessage%20Authentication%20Code%20%28HMAC%29.%20Focusing%20on%20traditional%20computing%20in%20the%0Apre-quantum%20era%2C%20this%20research%20highlights%20the%20efficacy%20of%20GA%20and%20PSO%20in%20ECC%0Aoptimization%2C%20with%20implications%20for%20enhancing%20cybersecurity%20in%20third-party%0Ae-commerce%20integrations.%20We%20recommend%20the%20immediate%20consideration%20of%20these%0Afindings%20before%20quantum%20computing%27s%20widespread%20adoption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.06752v2&entry.124074799=Read"},
{"title": "Towards Proactive Safe Human-Robot Collaborations via Data-Efficient\n  Conditional Behavior Prediction", "author": "Ravi Pandya and Zhuoyuan Wang and Yorie Nakahira and Changliu Liu", "abstract": "  We focus on the problem of how we can enable a robot to collaborate\nseamlessly with a human partner, specifically in scenarios where preexisting\ndata is sparse. Much prior work in human-robot collaboration uses observational\nmodels of humans (i.e. models that treat the robot purely as an observer) to\nchoose the robot's behavior, but such models do not account for the influence\nthe robot has on the human's actions, which may lead to inefficient\ninteractions. We instead formulate the problem of optimally choosing a\ncollaborative robot's behavior based on a conditional model of the human that\ndepends on the robot's future behavior. First, we propose a novel model-based\nformulation of conditional behavior prediction that allows the robot to infer\nthe human's intentions based on its future plan in data-sparse environments. We\nthen show how to utilize a conditional model for proactive goal selection and\nsafe trajectory generation around human collaborators. Finally, we use our\nproposed proactive controller in a collaborative task with real users to show\nthat it can improve users' interactions with a robot collaborator\nquantitatively and qualitatively.\n", "link": "http://arxiv.org/abs/2311.11893v2", "date": "2024-07-01", "relevancy": 1.7431, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6397}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6134}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5446}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Proactive%20Safe%20Human-Robot%20Collaborations%20via%20Data-Efficient%0A%20%20Conditional%20Behavior%20Prediction&body=Title%3A%20Towards%20Proactive%20Safe%20Human-Robot%20Collaborations%20via%20Data-Efficient%0A%20%20Conditional%20Behavior%20Prediction%0AAuthor%3A%20Ravi%20Pandya%20and%20Zhuoyuan%20Wang%20and%20Yorie%20Nakahira%20and%20Changliu%20Liu%0AAbstract%3A%20%20%20We%20focus%20on%20the%20problem%20of%20how%20we%20can%20enable%20a%20robot%20to%20collaborate%0Aseamlessly%20with%20a%20human%20partner%2C%20specifically%20in%20scenarios%20where%20preexisting%0Adata%20is%20sparse.%20Much%20prior%20work%20in%20human-robot%20collaboration%20uses%20observational%0Amodels%20of%20humans%20%28i.e.%20models%20that%20treat%20the%20robot%20purely%20as%20an%20observer%29%20to%0Achoose%20the%20robot%27s%20behavior%2C%20but%20such%20models%20do%20not%20account%20for%20the%20influence%0Athe%20robot%20has%20on%20the%20human%27s%20actions%2C%20which%20may%20lead%20to%20inefficient%0Ainteractions.%20We%20instead%20formulate%20the%20problem%20of%20optimally%20choosing%20a%0Acollaborative%20robot%27s%20behavior%20based%20on%20a%20conditional%20model%20of%20the%20human%20that%0Adepends%20on%20the%20robot%27s%20future%20behavior.%20First%2C%20we%20propose%20a%20novel%20model-based%0Aformulation%20of%20conditional%20behavior%20prediction%20that%20allows%20the%20robot%20to%20infer%0Athe%20human%27s%20intentions%20based%20on%20its%20future%20plan%20in%20data-sparse%20environments.%20We%0Athen%20show%20how%20to%20utilize%20a%20conditional%20model%20for%20proactive%20goal%20selection%20and%0Asafe%20trajectory%20generation%20around%20human%20collaborators.%20Finally%2C%20we%20use%20our%0Aproposed%20proactive%20controller%20in%20a%20collaborative%20task%20with%20real%20users%20to%20show%0Athat%20it%20can%20improve%20users%27%20interactions%20with%20a%20robot%20collaborator%0Aquantitatively%20and%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11893v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Proactive%2520Safe%2520Human-Robot%2520Collaborations%2520via%2520Data-Efficient%250A%2520%2520Conditional%2520Behavior%2520Prediction%26entry.906535625%3DRavi%2520Pandya%2520and%2520Zhuoyuan%2520Wang%2520and%2520Yorie%2520Nakahira%2520and%2520Changliu%2520Liu%26entry.1292438233%3D%2520%2520We%2520focus%2520on%2520the%2520problem%2520of%2520how%2520we%2520can%2520enable%2520a%2520robot%2520to%2520collaborate%250Aseamlessly%2520with%2520a%2520human%2520partner%252C%2520specifically%2520in%2520scenarios%2520where%2520preexisting%250Adata%2520is%2520sparse.%2520Much%2520prior%2520work%2520in%2520human-robot%2520collaboration%2520uses%2520observational%250Amodels%2520of%2520humans%2520%2528i.e.%2520models%2520that%2520treat%2520the%2520robot%2520purely%2520as%2520an%2520observer%2529%2520to%250Achoose%2520the%2520robot%2527s%2520behavior%252C%2520but%2520such%2520models%2520do%2520not%2520account%2520for%2520the%2520influence%250Athe%2520robot%2520has%2520on%2520the%2520human%2527s%2520actions%252C%2520which%2520may%2520lead%2520to%2520inefficient%250Ainteractions.%2520We%2520instead%2520formulate%2520the%2520problem%2520of%2520optimally%2520choosing%2520a%250Acollaborative%2520robot%2527s%2520behavior%2520based%2520on%2520a%2520conditional%2520model%2520of%2520the%2520human%2520that%250Adepends%2520on%2520the%2520robot%2527s%2520future%2520behavior.%2520First%252C%2520we%2520propose%2520a%2520novel%2520model-based%250Aformulation%2520of%2520conditional%2520behavior%2520prediction%2520that%2520allows%2520the%2520robot%2520to%2520infer%250Athe%2520human%2527s%2520intentions%2520based%2520on%2520its%2520future%2520plan%2520in%2520data-sparse%2520environments.%2520We%250Athen%2520show%2520how%2520to%2520utilize%2520a%2520conditional%2520model%2520for%2520proactive%2520goal%2520selection%2520and%250Asafe%2520trajectory%2520generation%2520around%2520human%2520collaborators.%2520Finally%252C%2520we%2520use%2520our%250Aproposed%2520proactive%2520controller%2520in%2520a%2520collaborative%2520task%2520with%2520real%2520users%2520to%2520show%250Athat%2520it%2520can%2520improve%2520users%2527%2520interactions%2520with%2520a%2520robot%2520collaborator%250Aquantitatively%2520and%2520qualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.11893v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Proactive%20Safe%20Human-Robot%20Collaborations%20via%20Data-Efficient%0A%20%20Conditional%20Behavior%20Prediction&entry.906535625=Ravi%20Pandya%20and%20Zhuoyuan%20Wang%20and%20Yorie%20Nakahira%20and%20Changliu%20Liu&entry.1292438233=%20%20We%20focus%20on%20the%20problem%20of%20how%20we%20can%20enable%20a%20robot%20to%20collaborate%0Aseamlessly%20with%20a%20human%20partner%2C%20specifically%20in%20scenarios%20where%20preexisting%0Adata%20is%20sparse.%20Much%20prior%20work%20in%20human-robot%20collaboration%20uses%20observational%0Amodels%20of%20humans%20%28i.e.%20models%20that%20treat%20the%20robot%20purely%20as%20an%20observer%29%20to%0Achoose%20the%20robot%27s%20behavior%2C%20but%20such%20models%20do%20not%20account%20for%20the%20influence%0Athe%20robot%20has%20on%20the%20human%27s%20actions%2C%20which%20may%20lead%20to%20inefficient%0Ainteractions.%20We%20instead%20formulate%20the%20problem%20of%20optimally%20choosing%20a%0Acollaborative%20robot%27s%20behavior%20based%20on%20a%20conditional%20model%20of%20the%20human%20that%0Adepends%20on%20the%20robot%27s%20future%20behavior.%20First%2C%20we%20propose%20a%20novel%20model-based%0Aformulation%20of%20conditional%20behavior%20prediction%20that%20allows%20the%20robot%20to%20infer%0Athe%20human%27s%20intentions%20based%20on%20its%20future%20plan%20in%20data-sparse%20environments.%20We%0Athen%20show%20how%20to%20utilize%20a%20conditional%20model%20for%20proactive%20goal%20selection%20and%0Asafe%20trajectory%20generation%20around%20human%20collaborators.%20Finally%2C%20we%20use%20our%0Aproposed%20proactive%20controller%20in%20a%20collaborative%20task%20with%20real%20users%20to%20show%0Athat%20it%20can%20improve%20users%27%20interactions%20with%20a%20robot%20collaborator%0Aquantitatively%20and%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11893v2&entry.124074799=Read"},
{"title": "BeHonest: Benchmarking Honesty of Large Language Models", "author": "Steffi Chern and Zhulin Hu and Yuqing Yang and Ethan Chern and Yuan Guo and Jiahe Jin and Binjie Wang and Pengfei Liu", "abstract": "  Previous works on Large Language Models (LLMs) have mainly focused on\nevaluating their helpfulness or harmlessness. However, honesty, another crucial\nalignment criterion, has received relatively less attention. Dishonest\nbehaviors in LLMs, such as spreading misinformation and defrauding users,\neroding user trust, and causing real-world harm, present severe risks that\nintensify as these models approach superintelligence levels. Enhancing honesty\nin LLMs addresses critical deficiencies and helps uncover latent capabilities\nthat are not readily expressed. This underscores the urgent need for reliable\nmethods and benchmarks to effectively ensure and evaluate the honesty of LLMs.\n  In this paper, we introduce BeHonest, a pioneering benchmark specifically\ndesigned to assess honesty in LLMs comprehensively. BeHonest evaluates three\nessential aspects of honesty: awareness of knowledge boundaries, avoidance of\ndeceit, and consistency in responses. Building on this foundation, we designed\n10 scenarios to evaluate and analyze 9 popular LLMs on the market, including\nboth closed-source and open-source models from different model families with\nvaried model sizes. Our findings indicate that there is still significant room\nfor improvement in the honesty of LLMs. We also encourage the AI community to\nprioritize honesty alignment in LLMs. Our benchmark and code can be found at:\n\\url{https://github.com/GAIR-NLP/BeHonest}.\n", "link": "http://arxiv.org/abs/2406.13261v2", "date": "2024-07-01", "relevancy": 1.725, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4772}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4237}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BeHonest%3A%20Benchmarking%20Honesty%20of%20Large%20Language%20Models&body=Title%3A%20BeHonest%3A%20Benchmarking%20Honesty%20of%20Large%20Language%20Models%0AAuthor%3A%20Steffi%20Chern%20and%20Zhulin%20Hu%20and%20Yuqing%20Yang%20and%20Ethan%20Chern%20and%20Yuan%20Guo%20and%20Jiahe%20Jin%20and%20Binjie%20Wang%20and%20Pengfei%20Liu%0AAbstract%3A%20%20%20Previous%20works%20on%20Large%20Language%20Models%20%28LLMs%29%20have%20mainly%20focused%20on%0Aevaluating%20their%20helpfulness%20or%20harmlessness.%20However%2C%20honesty%2C%20another%20crucial%0Aalignment%20criterion%2C%20has%20received%20relatively%20less%20attention.%20Dishonest%0Abehaviors%20in%20LLMs%2C%20such%20as%20spreading%20misinformation%20and%20defrauding%20users%2C%0Aeroding%20user%20trust%2C%20and%20causing%20real-world%20harm%2C%20present%20severe%20risks%20that%0Aintensify%20as%20these%20models%20approach%20superintelligence%20levels.%20Enhancing%20honesty%0Ain%20LLMs%20addresses%20critical%20deficiencies%20and%20helps%20uncover%20latent%20capabilities%0Athat%20are%20not%20readily%20expressed.%20This%20underscores%20the%20urgent%20need%20for%20reliable%0Amethods%20and%20benchmarks%20to%20effectively%20ensure%20and%20evaluate%20the%20honesty%20of%20LLMs.%0A%20%20In%20this%20paper%2C%20we%20introduce%20BeHonest%2C%20a%20pioneering%20benchmark%20specifically%0Adesigned%20to%20assess%20honesty%20in%20LLMs%20comprehensively.%20BeHonest%20evaluates%20three%0Aessential%20aspects%20of%20honesty%3A%20awareness%20of%20knowledge%20boundaries%2C%20avoidance%20of%0Adeceit%2C%20and%20consistency%20in%20responses.%20Building%20on%20this%20foundation%2C%20we%20designed%0A10%20scenarios%20to%20evaluate%20and%20analyze%209%20popular%20LLMs%20on%20the%20market%2C%20including%0Aboth%20closed-source%20and%20open-source%20models%20from%20different%20model%20families%20with%0Avaried%20model%20sizes.%20Our%20findings%20indicate%20that%20there%20is%20still%20significant%20room%0Afor%20improvement%20in%20the%20honesty%20of%20LLMs.%20We%20also%20encourage%20the%20AI%20community%20to%0Aprioritize%20honesty%20alignment%20in%20LLMs.%20Our%20benchmark%20and%20code%20can%20be%20found%20at%3A%0A%5Curl%7Bhttps%3A//github.com/GAIR-NLP/BeHonest%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.13261v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeHonest%253A%2520Benchmarking%2520Honesty%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DSteffi%2520Chern%2520and%2520Zhulin%2520Hu%2520and%2520Yuqing%2520Yang%2520and%2520Ethan%2520Chern%2520and%2520Yuan%2520Guo%2520and%2520Jiahe%2520Jin%2520and%2520Binjie%2520Wang%2520and%2520Pengfei%2520Liu%26entry.1292438233%3D%2520%2520Previous%2520works%2520on%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520mainly%2520focused%2520on%250Aevaluating%2520their%2520helpfulness%2520or%2520harmlessness.%2520However%252C%2520honesty%252C%2520another%2520crucial%250Aalignment%2520criterion%252C%2520has%2520received%2520relatively%2520less%2520attention.%2520Dishonest%250Abehaviors%2520in%2520LLMs%252C%2520such%2520as%2520spreading%2520misinformation%2520and%2520defrauding%2520users%252C%250Aeroding%2520user%2520trust%252C%2520and%2520causing%2520real-world%2520harm%252C%2520present%2520severe%2520risks%2520that%250Aintensify%2520as%2520these%2520models%2520approach%2520superintelligence%2520levels.%2520Enhancing%2520honesty%250Ain%2520LLMs%2520addresses%2520critical%2520deficiencies%2520and%2520helps%2520uncover%2520latent%2520capabilities%250Athat%2520are%2520not%2520readily%2520expressed.%2520This%2520underscores%2520the%2520urgent%2520need%2520for%2520reliable%250Amethods%2520and%2520benchmarks%2520to%2520effectively%2520ensure%2520and%2520evaluate%2520the%2520honesty%2520of%2520LLMs.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520BeHonest%252C%2520a%2520pioneering%2520benchmark%2520specifically%250Adesigned%2520to%2520assess%2520honesty%2520in%2520LLMs%2520comprehensively.%2520BeHonest%2520evaluates%2520three%250Aessential%2520aspects%2520of%2520honesty%253A%2520awareness%2520of%2520knowledge%2520boundaries%252C%2520avoidance%2520of%250Adeceit%252C%2520and%2520consistency%2520in%2520responses.%2520Building%2520on%2520this%2520foundation%252C%2520we%2520designed%250A10%2520scenarios%2520to%2520evaluate%2520and%2520analyze%25209%2520popular%2520LLMs%2520on%2520the%2520market%252C%2520including%250Aboth%2520closed-source%2520and%2520open-source%2520models%2520from%2520different%2520model%2520families%2520with%250Avaried%2520model%2520sizes.%2520Our%2520findings%2520indicate%2520that%2520there%2520is%2520still%2520significant%2520room%250Afor%2520improvement%2520in%2520the%2520honesty%2520of%2520LLMs.%2520We%2520also%2520encourage%2520the%2520AI%2520community%2520to%250Aprioritize%2520honesty%2520alignment%2520in%2520LLMs.%2520Our%2520benchmark%2520and%2520code%2520can%2520be%2520found%2520at%253A%250A%255Curl%257Bhttps%253A//github.com/GAIR-NLP/BeHonest%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.13261v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BeHonest%3A%20Benchmarking%20Honesty%20of%20Large%20Language%20Models&entry.906535625=Steffi%20Chern%20and%20Zhulin%20Hu%20and%20Yuqing%20Yang%20and%20Ethan%20Chern%20and%20Yuan%20Guo%20and%20Jiahe%20Jin%20and%20Binjie%20Wang%20and%20Pengfei%20Liu&entry.1292438233=%20%20Previous%20works%20on%20Large%20Language%20Models%20%28LLMs%29%20have%20mainly%20focused%20on%0Aevaluating%20their%20helpfulness%20or%20harmlessness.%20However%2C%20honesty%2C%20another%20crucial%0Aalignment%20criterion%2C%20has%20received%20relatively%20less%20attention.%20Dishonest%0Abehaviors%20in%20LLMs%2C%20such%20as%20spreading%20misinformation%20and%20defrauding%20users%2C%0Aeroding%20user%20trust%2C%20and%20causing%20real-world%20harm%2C%20present%20severe%20risks%20that%0Aintensify%20as%20these%20models%20approach%20superintelligence%20levels.%20Enhancing%20honesty%0Ain%20LLMs%20addresses%20critical%20deficiencies%20and%20helps%20uncover%20latent%20capabilities%0Athat%20are%20not%20readily%20expressed.%20This%20underscores%20the%20urgent%20need%20for%20reliable%0Amethods%20and%20benchmarks%20to%20effectively%20ensure%20and%20evaluate%20the%20honesty%20of%20LLMs.%0A%20%20In%20this%20paper%2C%20we%20introduce%20BeHonest%2C%20a%20pioneering%20benchmark%20specifically%0Adesigned%20to%20assess%20honesty%20in%20LLMs%20comprehensively.%20BeHonest%20evaluates%20three%0Aessential%20aspects%20of%20honesty%3A%20awareness%20of%20knowledge%20boundaries%2C%20avoidance%20of%0Adeceit%2C%20and%20consistency%20in%20responses.%20Building%20on%20this%20foundation%2C%20we%20designed%0A10%20scenarios%20to%20evaluate%20and%20analyze%209%20popular%20LLMs%20on%20the%20market%2C%20including%0Aboth%20closed-source%20and%20open-source%20models%20from%20different%20model%20families%20with%0Avaried%20model%20sizes.%20Our%20findings%20indicate%20that%20there%20is%20still%20significant%20room%0Afor%20improvement%20in%20the%20honesty%20of%20LLMs.%20We%20also%20encourage%20the%20AI%20community%20to%0Aprioritize%20honesty%20alignment%20in%20LLMs.%20Our%20benchmark%20and%20code%20can%20be%20found%20at%3A%0A%5Curl%7Bhttps%3A//github.com/GAIR-NLP/BeHonest%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.13261v2&entry.124074799=Read"},
{"title": "Distilling Knowledge from Text-to-Image Generative Models Improves\n  Visio-Linguistic Reasoning in CLIP", "author": "Samyadeep Basu and Shell Xu Hu and Maziar Sanjabi and Daniela Massiceti and Soheil Feizi", "abstract": "  Image-text contrastive models like CLIP have wide applications in zero-shot\nclassification, image-text retrieval, and transfer learning. However, they\noften struggle on compositional visio-linguistic tasks (e.g., attribute-binding\nor object-relationships) where their performance is no better than random\nchance. To address this, we introduce SDS-CLIP, a lightweight and\nsample-efficient distillation method to enhance CLIP's compositional\nvisio-linguistic reasoning. Our approach fine-tunes CLIP using a distillation\nobjective borrowed from large text-to-image generative models like\nStable-Diffusion, which are known for their strong visio-linguistic reasoning\nabilities. On the challenging Winoground benchmark, SDS-CLIP improves the\nvisio-linguistic performance of various CLIP models by up to 7%, while on the\nARO dataset, it boosts performance by up to 3%. This work underscores the\npotential of well-designed distillation objectives from generative models to\nenhance contrastive image-text models with improved visio-linguistic reasoning\ncapabilities.\n", "link": "http://arxiv.org/abs/2307.09233v3", "date": "2024-07-01", "relevancy": 1.7001, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5842}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5698}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distilling%20Knowledge%20from%20Text-to-Image%20Generative%20Models%20Improves%0A%20%20Visio-Linguistic%20Reasoning%20in%20CLIP&body=Title%3A%20Distilling%20Knowledge%20from%20Text-to-Image%20Generative%20Models%20Improves%0A%20%20Visio-Linguistic%20Reasoning%20in%20CLIP%0AAuthor%3A%20Samyadeep%20Basu%20and%20Shell%20Xu%20Hu%20and%20Maziar%20Sanjabi%20and%20Daniela%20Massiceti%20and%20Soheil%20Feizi%0AAbstract%3A%20%20%20Image-text%20contrastive%20models%20like%20CLIP%20have%20wide%20applications%20in%20zero-shot%0Aclassification%2C%20image-text%20retrieval%2C%20and%20transfer%20learning.%20However%2C%20they%0Aoften%20struggle%20on%20compositional%20visio-linguistic%20tasks%20%28e.g.%2C%20attribute-binding%0Aor%20object-relationships%29%20where%20their%20performance%20is%20no%20better%20than%20random%0Achance.%20To%20address%20this%2C%20we%20introduce%20SDS-CLIP%2C%20a%20lightweight%20and%0Asample-efficient%20distillation%20method%20to%20enhance%20CLIP%27s%20compositional%0Avisio-linguistic%20reasoning.%20Our%20approach%20fine-tunes%20CLIP%20using%20a%20distillation%0Aobjective%20borrowed%20from%20large%20text-to-image%20generative%20models%20like%0AStable-Diffusion%2C%20which%20are%20known%20for%20their%20strong%20visio-linguistic%20reasoning%0Aabilities.%20On%20the%20challenging%20Winoground%20benchmark%2C%20SDS-CLIP%20improves%20the%0Avisio-linguistic%20performance%20of%20various%20CLIP%20models%20by%20up%20to%207%25%2C%20while%20on%20the%0AARO%20dataset%2C%20it%20boosts%20performance%20by%20up%20to%203%25.%20This%20work%20underscores%20the%0Apotential%20of%20well-designed%20distillation%20objectives%20from%20generative%20models%20to%0Aenhance%20contrastive%20image-text%20models%20with%20improved%20visio-linguistic%20reasoning%0Acapabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.09233v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistilling%2520Knowledge%2520from%2520Text-to-Image%2520Generative%2520Models%2520Improves%250A%2520%2520Visio-Linguistic%2520Reasoning%2520in%2520CLIP%26entry.906535625%3DSamyadeep%2520Basu%2520and%2520Shell%2520Xu%2520Hu%2520and%2520Maziar%2520Sanjabi%2520and%2520Daniela%2520Massiceti%2520and%2520Soheil%2520Feizi%26entry.1292438233%3D%2520%2520Image-text%2520contrastive%2520models%2520like%2520CLIP%2520have%2520wide%2520applications%2520in%2520zero-shot%250Aclassification%252C%2520image-text%2520retrieval%252C%2520and%2520transfer%2520learning.%2520However%252C%2520they%250Aoften%2520struggle%2520on%2520compositional%2520visio-linguistic%2520tasks%2520%2528e.g.%252C%2520attribute-binding%250Aor%2520object-relationships%2529%2520where%2520their%2520performance%2520is%2520no%2520better%2520than%2520random%250Achance.%2520To%2520address%2520this%252C%2520we%2520introduce%2520SDS-CLIP%252C%2520a%2520lightweight%2520and%250Asample-efficient%2520distillation%2520method%2520to%2520enhance%2520CLIP%2527s%2520compositional%250Avisio-linguistic%2520reasoning.%2520Our%2520approach%2520fine-tunes%2520CLIP%2520using%2520a%2520distillation%250Aobjective%2520borrowed%2520from%2520large%2520text-to-image%2520generative%2520models%2520like%250AStable-Diffusion%252C%2520which%2520are%2520known%2520for%2520their%2520strong%2520visio-linguistic%2520reasoning%250Aabilities.%2520On%2520the%2520challenging%2520Winoground%2520benchmark%252C%2520SDS-CLIP%2520improves%2520the%250Avisio-linguistic%2520performance%2520of%2520various%2520CLIP%2520models%2520by%2520up%2520to%25207%2525%252C%2520while%2520on%2520the%250AARO%2520dataset%252C%2520it%2520boosts%2520performance%2520by%2520up%2520to%25203%2525.%2520This%2520work%2520underscores%2520the%250Apotential%2520of%2520well-designed%2520distillation%2520objectives%2520from%2520generative%2520models%2520to%250Aenhance%2520contrastive%2520image-text%2520models%2520with%2520improved%2520visio-linguistic%2520reasoning%250Acapabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.09233v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilling%20Knowledge%20from%20Text-to-Image%20Generative%20Models%20Improves%0A%20%20Visio-Linguistic%20Reasoning%20in%20CLIP&entry.906535625=Samyadeep%20Basu%20and%20Shell%20Xu%20Hu%20and%20Maziar%20Sanjabi%20and%20Daniela%20Massiceti%20and%20Soheil%20Feizi&entry.1292438233=%20%20Image-text%20contrastive%20models%20like%20CLIP%20have%20wide%20applications%20in%20zero-shot%0Aclassification%2C%20image-text%20retrieval%2C%20and%20transfer%20learning.%20However%2C%20they%0Aoften%20struggle%20on%20compositional%20visio-linguistic%20tasks%20%28e.g.%2C%20attribute-binding%0Aor%20object-relationships%29%20where%20their%20performance%20is%20no%20better%20than%20random%0Achance.%20To%20address%20this%2C%20we%20introduce%20SDS-CLIP%2C%20a%20lightweight%20and%0Asample-efficient%20distillation%20method%20to%20enhance%20CLIP%27s%20compositional%0Avisio-linguistic%20reasoning.%20Our%20approach%20fine-tunes%20CLIP%20using%20a%20distillation%0Aobjective%20borrowed%20from%20large%20text-to-image%20generative%20models%20like%0AStable-Diffusion%2C%20which%20are%20known%20for%20their%20strong%20visio-linguistic%20reasoning%0Aabilities.%20On%20the%20challenging%20Winoground%20benchmark%2C%20SDS-CLIP%20improves%20the%0Avisio-linguistic%20performance%20of%20various%20CLIP%20models%20by%20up%20to%207%25%2C%20while%20on%20the%0AARO%20dataset%2C%20it%20boosts%20performance%20by%20up%20to%203%25.%20This%20work%20underscores%20the%0Apotential%20of%20well-designed%20distillation%20objectives%20from%20generative%20models%20to%0Aenhance%20contrastive%20image-text%20models%20with%20improved%20visio-linguistic%20reasoning%0Acapabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.09233v3&entry.124074799=Read"},
{"title": "AdaFold: Adapting Folding Trajectories of Cloths via Feedback-loop\n  Manipulation", "author": "Alberta Longhini and Michael C. Welle and Zackory Erickson and Danica Kragic", "abstract": "  We present AdaFold, a model-based feedback-loop framework for optimizing\nfolding trajectories. AdaFold extracts a particle-based representation of cloth\nfrom RGB-D images and feeds back the representation to a model predictive\ncontrol to re-plan folding trajectory at every time-step. A key component of\nAdaFold that enables feedback-loop manipulation is the use of semantic\ndescriptors extracted from geometric features. These descriptors enhance the\nparticle representation of the cloth to distinguish between ambiguous point\nclouds of differently folded cloths. Our experiments demonstrate AdaFold's\nability to adapt folding trajectories to cloths with varying physical\nproperties and generalize from simulated training to real-world execution.\n", "link": "http://arxiv.org/abs/2403.06210v2", "date": "2024-07-01", "relevancy": 1.6773, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5772}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5573}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5157}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaFold%3A%20Adapting%20Folding%20Trajectories%20of%20Cloths%20via%20Feedback-loop%0A%20%20Manipulation&body=Title%3A%20AdaFold%3A%20Adapting%20Folding%20Trajectories%20of%20Cloths%20via%20Feedback-loop%0A%20%20Manipulation%0AAuthor%3A%20Alberta%20Longhini%20and%20Michael%20C.%20Welle%20and%20Zackory%20Erickson%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20We%20present%20AdaFold%2C%20a%20model-based%20feedback-loop%20framework%20for%20optimizing%0Afolding%20trajectories.%20AdaFold%20extracts%20a%20particle-based%20representation%20of%20cloth%0Afrom%20RGB-D%20images%20and%20feeds%20back%20the%20representation%20to%20a%20model%20predictive%0Acontrol%20to%20re-plan%20folding%20trajectory%20at%20every%20time-step.%20A%20key%20component%20of%0AAdaFold%20that%20enables%20feedback-loop%20manipulation%20is%20the%20use%20of%20semantic%0Adescriptors%20extracted%20from%20geometric%20features.%20These%20descriptors%20enhance%20the%0Aparticle%20representation%20of%20the%20cloth%20to%20distinguish%20between%20ambiguous%20point%0Aclouds%20of%20differently%20folded%20cloths.%20Our%20experiments%20demonstrate%20AdaFold%27s%0Aability%20to%20adapt%20folding%20trajectories%20to%20cloths%20with%20varying%20physical%0Aproperties%20and%20generalize%20from%20simulated%20training%20to%20real-world%20execution.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.06210v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaFold%253A%2520Adapting%2520Folding%2520Trajectories%2520of%2520Cloths%2520via%2520Feedback-loop%250A%2520%2520Manipulation%26entry.906535625%3DAlberta%2520Longhini%2520and%2520Michael%2520C.%2520Welle%2520and%2520Zackory%2520Erickson%2520and%2520Danica%2520Kragic%26entry.1292438233%3D%2520%2520We%2520present%2520AdaFold%252C%2520a%2520model-based%2520feedback-loop%2520framework%2520for%2520optimizing%250Afolding%2520trajectories.%2520AdaFold%2520extracts%2520a%2520particle-based%2520representation%2520of%2520cloth%250Afrom%2520RGB-D%2520images%2520and%2520feeds%2520back%2520the%2520representation%2520to%2520a%2520model%2520predictive%250Acontrol%2520to%2520re-plan%2520folding%2520trajectory%2520at%2520every%2520time-step.%2520A%2520key%2520component%2520of%250AAdaFold%2520that%2520enables%2520feedback-loop%2520manipulation%2520is%2520the%2520use%2520of%2520semantic%250Adescriptors%2520extracted%2520from%2520geometric%2520features.%2520These%2520descriptors%2520enhance%2520the%250Aparticle%2520representation%2520of%2520the%2520cloth%2520to%2520distinguish%2520between%2520ambiguous%2520point%250Aclouds%2520of%2520differently%2520folded%2520cloths.%2520Our%2520experiments%2520demonstrate%2520AdaFold%2527s%250Aability%2520to%2520adapt%2520folding%2520trajectories%2520to%2520cloths%2520with%2520varying%2520physical%250Aproperties%2520and%2520generalize%2520from%2520simulated%2520training%2520to%2520real-world%2520execution.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.06210v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaFold%3A%20Adapting%20Folding%20Trajectories%20of%20Cloths%20via%20Feedback-loop%0A%20%20Manipulation&entry.906535625=Alberta%20Longhini%20and%20Michael%20C.%20Welle%20and%20Zackory%20Erickson%20and%20Danica%20Kragic&entry.1292438233=%20%20We%20present%20AdaFold%2C%20a%20model-based%20feedback-loop%20framework%20for%20optimizing%0Afolding%20trajectories.%20AdaFold%20extracts%20a%20particle-based%20representation%20of%20cloth%0Afrom%20RGB-D%20images%20and%20feeds%20back%20the%20representation%20to%20a%20model%20predictive%0Acontrol%20to%20re-plan%20folding%20trajectory%20at%20every%20time-step.%20A%20key%20component%20of%0AAdaFold%20that%20enables%20feedback-loop%20manipulation%20is%20the%20use%20of%20semantic%0Adescriptors%20extracted%20from%20geometric%20features.%20These%20descriptors%20enhance%20the%0Aparticle%20representation%20of%20the%20cloth%20to%20distinguish%20between%20ambiguous%20point%0Aclouds%20of%20differently%20folded%20cloths.%20Our%20experiments%20demonstrate%20AdaFold%27s%0Aability%20to%20adapt%20folding%20trajectories%20to%20cloths%20with%20varying%20physical%0Aproperties%20and%20generalize%20from%20simulated%20training%20to%20real-world%20execution.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.06210v2&entry.124074799=Read"},
{"title": "Multi-Agent Strategy Explanations for Human-Robot Collaboration", "author": "Ravi Pandya and Michelle Zhao and Changliu Liu and Reid Simmons and Henny Admoni", "abstract": "  As robots are deployed in human spaces, it is important that they are able to\ncoordinate their actions with the people around them. Part of such coordination\ninvolves ensuring that people have a good understanding of how a robot will act\nin the environment. This can be achieved through explanations of the robot's\npolicy. Much prior work in explainable AI and RL focuses on generating\nexplanations for single-agent policies, but little has been explored in\ngenerating explanations for collaborative policies. In this work, we\ninvestigate how to generate multi-agent strategy explanations for human-robot\ncollaboration. We formulate the problem using a generic multi-agent planner,\nshow how to generate visual explanations through strategy-conditioned landmark\nstates and generate textual explanations by giving the landmarks to an LLM.\nThrough a user study, we find that when presented with explanations from our\nproposed framework, users are able to better explore the full space of\nstrategies and collaborate more efficiently with new robot partners.\n", "link": "http://arxiv.org/abs/2311.11955v2", "date": "2024-07-01", "relevancy": 1.6284, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6265}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.573}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4972}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Agent%20Strategy%20Explanations%20for%20Human-Robot%20Collaboration&body=Title%3A%20Multi-Agent%20Strategy%20Explanations%20for%20Human-Robot%20Collaboration%0AAuthor%3A%20Ravi%20Pandya%20and%20Michelle%20Zhao%20and%20Changliu%20Liu%20and%20Reid%20Simmons%20and%20Henny%20Admoni%0AAbstract%3A%20%20%20As%20robots%20are%20deployed%20in%20human%20spaces%2C%20it%20is%20important%20that%20they%20are%20able%20to%0Acoordinate%20their%20actions%20with%20the%20people%20around%20them.%20Part%20of%20such%20coordination%0Ainvolves%20ensuring%20that%20people%20have%20a%20good%20understanding%20of%20how%20a%20robot%20will%20act%0Ain%20the%20environment.%20This%20can%20be%20achieved%20through%20explanations%20of%20the%20robot%27s%0Apolicy.%20Much%20prior%20work%20in%20explainable%20AI%20and%20RL%20focuses%20on%20generating%0Aexplanations%20for%20single-agent%20policies%2C%20but%20little%20has%20been%20explored%20in%0Agenerating%20explanations%20for%20collaborative%20policies.%20In%20this%20work%2C%20we%0Ainvestigate%20how%20to%20generate%20multi-agent%20strategy%20explanations%20for%20human-robot%0Acollaboration.%20We%20formulate%20the%20problem%20using%20a%20generic%20multi-agent%20planner%2C%0Ashow%20how%20to%20generate%20visual%20explanations%20through%20strategy-conditioned%20landmark%0Astates%20and%20generate%20textual%20explanations%20by%20giving%20the%20landmarks%20to%20an%20LLM.%0AThrough%20a%20user%20study%2C%20we%20find%20that%20when%20presented%20with%20explanations%20from%20our%0Aproposed%20framework%2C%20users%20are%20able%20to%20better%20explore%20the%20full%20space%20of%0Astrategies%20and%20collaborate%20more%20efficiently%20with%20new%20robot%20partners.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.11955v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Agent%2520Strategy%2520Explanations%2520for%2520Human-Robot%2520Collaboration%26entry.906535625%3DRavi%2520Pandya%2520and%2520Michelle%2520Zhao%2520and%2520Changliu%2520Liu%2520and%2520Reid%2520Simmons%2520and%2520Henny%2520Admoni%26entry.1292438233%3D%2520%2520As%2520robots%2520are%2520deployed%2520in%2520human%2520spaces%252C%2520it%2520is%2520important%2520that%2520they%2520are%2520able%2520to%250Acoordinate%2520their%2520actions%2520with%2520the%2520people%2520around%2520them.%2520Part%2520of%2520such%2520coordination%250Ainvolves%2520ensuring%2520that%2520people%2520have%2520a%2520good%2520understanding%2520of%2520how%2520a%2520robot%2520will%2520act%250Ain%2520the%2520environment.%2520This%2520can%2520be%2520achieved%2520through%2520explanations%2520of%2520the%2520robot%2527s%250Apolicy.%2520Much%2520prior%2520work%2520in%2520explainable%2520AI%2520and%2520RL%2520focuses%2520on%2520generating%250Aexplanations%2520for%2520single-agent%2520policies%252C%2520but%2520little%2520has%2520been%2520explored%2520in%250Agenerating%2520explanations%2520for%2520collaborative%2520policies.%2520In%2520this%2520work%252C%2520we%250Ainvestigate%2520how%2520to%2520generate%2520multi-agent%2520strategy%2520explanations%2520for%2520human-robot%250Acollaboration.%2520We%2520formulate%2520the%2520problem%2520using%2520a%2520generic%2520multi-agent%2520planner%252C%250Ashow%2520how%2520to%2520generate%2520visual%2520explanations%2520through%2520strategy-conditioned%2520landmark%250Astates%2520and%2520generate%2520textual%2520explanations%2520by%2520giving%2520the%2520landmarks%2520to%2520an%2520LLM.%250AThrough%2520a%2520user%2520study%252C%2520we%2520find%2520that%2520when%2520presented%2520with%2520explanations%2520from%2520our%250Aproposed%2520framework%252C%2520users%2520are%2520able%2520to%2520better%2520explore%2520the%2520full%2520space%2520of%250Astrategies%2520and%2520collaborate%2520more%2520efficiently%2520with%2520new%2520robot%2520partners.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.11955v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Agent%20Strategy%20Explanations%20for%20Human-Robot%20Collaboration&entry.906535625=Ravi%20Pandya%20and%20Michelle%20Zhao%20and%20Changliu%20Liu%20and%20Reid%20Simmons%20and%20Henny%20Admoni&entry.1292438233=%20%20As%20robots%20are%20deployed%20in%20human%20spaces%2C%20it%20is%20important%20that%20they%20are%20able%20to%0Acoordinate%20their%20actions%20with%20the%20people%20around%20them.%20Part%20of%20such%20coordination%0Ainvolves%20ensuring%20that%20people%20have%20a%20good%20understanding%20of%20how%20a%20robot%20will%20act%0Ain%20the%20environment.%20This%20can%20be%20achieved%20through%20explanations%20of%20the%20robot%27s%0Apolicy.%20Much%20prior%20work%20in%20explainable%20AI%20and%20RL%20focuses%20on%20generating%0Aexplanations%20for%20single-agent%20policies%2C%20but%20little%20has%20been%20explored%20in%0Agenerating%20explanations%20for%20collaborative%20policies.%20In%20this%20work%2C%20we%0Ainvestigate%20how%20to%20generate%20multi-agent%20strategy%20explanations%20for%20human-robot%0Acollaboration.%20We%20formulate%20the%20problem%20using%20a%20generic%20multi-agent%20planner%2C%0Ashow%20how%20to%20generate%20visual%20explanations%20through%20strategy-conditioned%20landmark%0Astates%20and%20generate%20textual%20explanations%20by%20giving%20the%20landmarks%20to%20an%20LLM.%0AThrough%20a%20user%20study%2C%20we%20find%20that%20when%20presented%20with%20explanations%20from%20our%0Aproposed%20framework%2C%20users%20are%20able%20to%20better%20explore%20the%20full%20space%20of%0Astrategies%20and%20collaborate%20more%20efficiently%20with%20new%20robot%20partners.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.11955v2&entry.124074799=Read"},
{"title": "Inverse Evolution Layers: Physics-informed Regularizers for Deep Neural\n  Networks", "author": "Chaoyu Liu and Zhonghua Qiao and Chao Li and Carola-Bibiane Sch\u00f6nlieb", "abstract": "  Traditional image processing methods employing partial differential equations\n(PDEs) offer a multitude of meaningful regularizers, along with valuable\ntheoretical foundations for a wide range of image-related tasks. This makes\ntheir integration into neural networks a promising avenue. In this paper, we\nintroduce a novel regularization approach inspired by the reverse process of\nPDE-based evolution models. Specifically, we propose inverse evolution layers\n(IELs), which serve as bad property amplifiers to penalize neural networks of\nwhich outputs have undesired characteristics. Using IELs, one can achieve\nspecific regularization objectives and endow neural networks' outputs with\ncorresponding properties of the PDE models. Our experiments, focusing on\nsemantic segmentation tasks using heat-diffusion IELs, demonstrate their\neffectiveness in mitigating noisy label effects. Additionally, we develop\ncurve-motion IELs to enforce convex shape regularization in neural\nnetwork-based segmentation models for preventing the generation of concave\noutputs. Theoretical analysis confirms the efficacy of IELs as an effective\nregularization mechanism, particularly in handling training with label issues.\n", "link": "http://arxiv.org/abs/2307.07344v2", "date": "2024-07-01", "relevancy": 1.5982, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5516}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.53}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5263}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inverse%20Evolution%20Layers%3A%20Physics-informed%20Regularizers%20for%20Deep%20Neural%0A%20%20Networks&body=Title%3A%20Inverse%20Evolution%20Layers%3A%20Physics-informed%20Regularizers%20for%20Deep%20Neural%0A%20%20Networks%0AAuthor%3A%20Chaoyu%20Liu%20and%20Zhonghua%20Qiao%20and%20Chao%20Li%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%0AAbstract%3A%20%20%20Traditional%20image%20processing%20methods%20employing%20partial%20differential%20equations%0A%28PDEs%29%20offer%20a%20multitude%20of%20meaningful%20regularizers%2C%20along%20with%20valuable%0Atheoretical%20foundations%20for%20a%20wide%20range%20of%20image-related%20tasks.%20This%20makes%0Atheir%20integration%20into%20neural%20networks%20a%20promising%20avenue.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20regularization%20approach%20inspired%20by%20the%20reverse%20process%20of%0APDE-based%20evolution%20models.%20Specifically%2C%20we%20propose%20inverse%20evolution%20layers%0A%28IELs%29%2C%20which%20serve%20as%20bad%20property%20amplifiers%20to%20penalize%20neural%20networks%20of%0Awhich%20outputs%20have%20undesired%20characteristics.%20Using%20IELs%2C%20one%20can%20achieve%0Aspecific%20regularization%20objectives%20and%20endow%20neural%20networks%27%20outputs%20with%0Acorresponding%20properties%20of%20the%20PDE%20models.%20Our%20experiments%2C%20focusing%20on%0Asemantic%20segmentation%20tasks%20using%20heat-diffusion%20IELs%2C%20demonstrate%20their%0Aeffectiveness%20in%20mitigating%20noisy%20label%20effects.%20Additionally%2C%20we%20develop%0Acurve-motion%20IELs%20to%20enforce%20convex%20shape%20regularization%20in%20neural%0Anetwork-based%20segmentation%20models%20for%20preventing%20the%20generation%20of%20concave%0Aoutputs.%20Theoretical%20analysis%20confirms%20the%20efficacy%20of%20IELs%20as%20an%20effective%0Aregularization%20mechanism%2C%20particularly%20in%20handling%20training%20with%20label%20issues.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.07344v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInverse%2520Evolution%2520Layers%253A%2520Physics-informed%2520Regularizers%2520for%2520Deep%2520Neural%250A%2520%2520Networks%26entry.906535625%3DChaoyu%2520Liu%2520and%2520Zhonghua%2520Qiao%2520and%2520Chao%2520Li%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%26entry.1292438233%3D%2520%2520Traditional%2520image%2520processing%2520methods%2520employing%2520partial%2520differential%2520equations%250A%2528PDEs%2529%2520offer%2520a%2520multitude%2520of%2520meaningful%2520regularizers%252C%2520along%2520with%2520valuable%250Atheoretical%2520foundations%2520for%2520a%2520wide%2520range%2520of%2520image-related%2520tasks.%2520This%2520makes%250Atheir%2520integration%2520into%2520neural%2520networks%2520a%2520promising%2520avenue.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520a%2520novel%2520regularization%2520approach%2520inspired%2520by%2520the%2520reverse%2520process%2520of%250APDE-based%2520evolution%2520models.%2520Specifically%252C%2520we%2520propose%2520inverse%2520evolution%2520layers%250A%2528IELs%2529%252C%2520which%2520serve%2520as%2520bad%2520property%2520amplifiers%2520to%2520penalize%2520neural%2520networks%2520of%250Awhich%2520outputs%2520have%2520undesired%2520characteristics.%2520Using%2520IELs%252C%2520one%2520can%2520achieve%250Aspecific%2520regularization%2520objectives%2520and%2520endow%2520neural%2520networks%2527%2520outputs%2520with%250Acorresponding%2520properties%2520of%2520the%2520PDE%2520models.%2520Our%2520experiments%252C%2520focusing%2520on%250Asemantic%2520segmentation%2520tasks%2520using%2520heat-diffusion%2520IELs%252C%2520demonstrate%2520their%250Aeffectiveness%2520in%2520mitigating%2520noisy%2520label%2520effects.%2520Additionally%252C%2520we%2520develop%250Acurve-motion%2520IELs%2520to%2520enforce%2520convex%2520shape%2520regularization%2520in%2520neural%250Anetwork-based%2520segmentation%2520models%2520for%2520preventing%2520the%2520generation%2520of%2520concave%250Aoutputs.%2520Theoretical%2520analysis%2520confirms%2520the%2520efficacy%2520of%2520IELs%2520as%2520an%2520effective%250Aregularization%2520mechanism%252C%2520particularly%2520in%2520handling%2520training%2520with%2520label%2520issues.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.07344v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inverse%20Evolution%20Layers%3A%20Physics-informed%20Regularizers%20for%20Deep%20Neural%0A%20%20Networks&entry.906535625=Chaoyu%20Liu%20and%20Zhonghua%20Qiao%20and%20Chao%20Li%20and%20Carola-Bibiane%20Sch%C3%B6nlieb&entry.1292438233=%20%20Traditional%20image%20processing%20methods%20employing%20partial%20differential%20equations%0A%28PDEs%29%20offer%20a%20multitude%20of%20meaningful%20regularizers%2C%20along%20with%20valuable%0Atheoretical%20foundations%20for%20a%20wide%20range%20of%20image-related%20tasks.%20This%20makes%0Atheir%20integration%20into%20neural%20networks%20a%20promising%20avenue.%20In%20this%20paper%2C%20we%0Aintroduce%20a%20novel%20regularization%20approach%20inspired%20by%20the%20reverse%20process%20of%0APDE-based%20evolution%20models.%20Specifically%2C%20we%20propose%20inverse%20evolution%20layers%0A%28IELs%29%2C%20which%20serve%20as%20bad%20property%20amplifiers%20to%20penalize%20neural%20networks%20of%0Awhich%20outputs%20have%20undesired%20characteristics.%20Using%20IELs%2C%20one%20can%20achieve%0Aspecific%20regularization%20objectives%20and%20endow%20neural%20networks%27%20outputs%20with%0Acorresponding%20properties%20of%20the%20PDE%20models.%20Our%20experiments%2C%20focusing%20on%0Asemantic%20segmentation%20tasks%20using%20heat-diffusion%20IELs%2C%20demonstrate%20their%0Aeffectiveness%20in%20mitigating%20noisy%20label%20effects.%20Additionally%2C%20we%20develop%0Acurve-motion%20IELs%20to%20enforce%20convex%20shape%20regularization%20in%20neural%0Anetwork-based%20segmentation%20models%20for%20preventing%20the%20generation%20of%20concave%0Aoutputs.%20Theoretical%20analysis%20confirms%20the%20efficacy%20of%20IELs%20as%20an%20effective%0Aregularization%20mechanism%2C%20particularly%20in%20handling%20training%20with%20label%20issues.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.07344v2&entry.124074799=Read"},
{"title": "A Convex Hull Cheapest Insertion Heuristic for the Non-Euclidean TSP", "author": "Mithun Goutham and Meghna Menon and Sarah Garrow and Stephanie Stockar", "abstract": "  The convex hull cheapest insertion heuristic is known to produce good\nsolutions to the Traveling Salesperson Problem in Euclidean spaces, but it has\nnever been extended to the non-Euclidean problem. This paper proposes an\nadaptation that uses multidimensional scaling to first project the points from\na non-Euclidean space into a Euclidean equivalent space, thereby enabling the\ngeneration of a convex hull that initializes the algorithm. To evaluate the\nproposed algorithm, non-Euclidean spaces are created by adding separators to\nthe Euclidean TSPLIB benchmark data-set, or by using the L1 norm as a metric.\nThis adapted heuristic is demonstrated to outperform the commonly used Nearest\nNeighbor heuristic and Nearest Insertion heuristic in 88% and 99% of the cases\nstudied, respectively. When compared with metaheuristic algorithms, the\nproposed heuristic's tour costs are lower than the solutions found by the\ngenetic algorithm and ant colony optimization algorithm in 87% and 95% of the\ninstances, respectively.\n", "link": "http://arxiv.org/abs/2302.06582v4", "date": "2024-07-01", "relevancy": 1.5909, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4085}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3905}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3887}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Convex%20Hull%20Cheapest%20Insertion%20Heuristic%20for%20the%20Non-Euclidean%20TSP&body=Title%3A%20A%20Convex%20Hull%20Cheapest%20Insertion%20Heuristic%20for%20the%20Non-Euclidean%20TSP%0AAuthor%3A%20Mithun%20Goutham%20and%20Meghna%20Menon%20and%20Sarah%20Garrow%20and%20Stephanie%20Stockar%0AAbstract%3A%20%20%20The%20convex%20hull%20cheapest%20insertion%20heuristic%20is%20known%20to%20produce%20good%0Asolutions%20to%20the%20Traveling%20Salesperson%20Problem%20in%20Euclidean%20spaces%2C%20but%20it%20has%0Anever%20been%20extended%20to%20the%20non-Euclidean%20problem.%20This%20paper%20proposes%20an%0Aadaptation%20that%20uses%20multidimensional%20scaling%20to%20first%20project%20the%20points%20from%0Aa%20non-Euclidean%20space%20into%20a%20Euclidean%20equivalent%20space%2C%20thereby%20enabling%20the%0Ageneration%20of%20a%20convex%20hull%20that%20initializes%20the%20algorithm.%20To%20evaluate%20the%0Aproposed%20algorithm%2C%20non-Euclidean%20spaces%20are%20created%20by%20adding%20separators%20to%0Athe%20Euclidean%20TSPLIB%20benchmark%20data-set%2C%20or%20by%20using%20the%20L1%20norm%20as%20a%20metric.%0AThis%20adapted%20heuristic%20is%20demonstrated%20to%20outperform%20the%20commonly%20used%20Nearest%0ANeighbor%20heuristic%20and%20Nearest%20Insertion%20heuristic%20in%2088%25%20and%2099%25%20of%20the%20cases%0Astudied%2C%20respectively.%20When%20compared%20with%20metaheuristic%20algorithms%2C%20the%0Aproposed%20heuristic%27s%20tour%20costs%20are%20lower%20than%20the%20solutions%20found%20by%20the%0Agenetic%20algorithm%20and%20ant%20colony%20optimization%20algorithm%20in%2087%25%20and%2095%25%20of%20the%0Ainstances%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2302.06582v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Convex%2520Hull%2520Cheapest%2520Insertion%2520Heuristic%2520for%2520the%2520Non-Euclidean%2520TSP%26entry.906535625%3DMithun%2520Goutham%2520and%2520Meghna%2520Menon%2520and%2520Sarah%2520Garrow%2520and%2520Stephanie%2520Stockar%26entry.1292438233%3D%2520%2520The%2520convex%2520hull%2520cheapest%2520insertion%2520heuristic%2520is%2520known%2520to%2520produce%2520good%250Asolutions%2520to%2520the%2520Traveling%2520Salesperson%2520Problem%2520in%2520Euclidean%2520spaces%252C%2520but%2520it%2520has%250Anever%2520been%2520extended%2520to%2520the%2520non-Euclidean%2520problem.%2520This%2520paper%2520proposes%2520an%250Aadaptation%2520that%2520uses%2520multidimensional%2520scaling%2520to%2520first%2520project%2520the%2520points%2520from%250Aa%2520non-Euclidean%2520space%2520into%2520a%2520Euclidean%2520equivalent%2520space%252C%2520thereby%2520enabling%2520the%250Ageneration%2520of%2520a%2520convex%2520hull%2520that%2520initializes%2520the%2520algorithm.%2520To%2520evaluate%2520the%250Aproposed%2520algorithm%252C%2520non-Euclidean%2520spaces%2520are%2520created%2520by%2520adding%2520separators%2520to%250Athe%2520Euclidean%2520TSPLIB%2520benchmark%2520data-set%252C%2520or%2520by%2520using%2520the%2520L1%2520norm%2520as%2520a%2520metric.%250AThis%2520adapted%2520heuristic%2520is%2520demonstrated%2520to%2520outperform%2520the%2520commonly%2520used%2520Nearest%250ANeighbor%2520heuristic%2520and%2520Nearest%2520Insertion%2520heuristic%2520in%252088%2525%2520and%252099%2525%2520of%2520the%2520cases%250Astudied%252C%2520respectively.%2520When%2520compared%2520with%2520metaheuristic%2520algorithms%252C%2520the%250Aproposed%2520heuristic%2527s%2520tour%2520costs%2520are%2520lower%2520than%2520the%2520solutions%2520found%2520by%2520the%250Agenetic%2520algorithm%2520and%2520ant%2520colony%2520optimization%2520algorithm%2520in%252087%2525%2520and%252095%2525%2520of%2520the%250Ainstances%252C%2520respectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2302.06582v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Convex%20Hull%20Cheapest%20Insertion%20Heuristic%20for%20the%20Non-Euclidean%20TSP&entry.906535625=Mithun%20Goutham%20and%20Meghna%20Menon%20and%20Sarah%20Garrow%20and%20Stephanie%20Stockar&entry.1292438233=%20%20The%20convex%20hull%20cheapest%20insertion%20heuristic%20is%20known%20to%20produce%20good%0Asolutions%20to%20the%20Traveling%20Salesperson%20Problem%20in%20Euclidean%20spaces%2C%20but%20it%20has%0Anever%20been%20extended%20to%20the%20non-Euclidean%20problem.%20This%20paper%20proposes%20an%0Aadaptation%20that%20uses%20multidimensional%20scaling%20to%20first%20project%20the%20points%20from%0Aa%20non-Euclidean%20space%20into%20a%20Euclidean%20equivalent%20space%2C%20thereby%20enabling%20the%0Ageneration%20of%20a%20convex%20hull%20that%20initializes%20the%20algorithm.%20To%20evaluate%20the%0Aproposed%20algorithm%2C%20non-Euclidean%20spaces%20are%20created%20by%20adding%20separators%20to%0Athe%20Euclidean%20TSPLIB%20benchmark%20data-set%2C%20or%20by%20using%20the%20L1%20norm%20as%20a%20metric.%0AThis%20adapted%20heuristic%20is%20demonstrated%20to%20outperform%20the%20commonly%20used%20Nearest%0ANeighbor%20heuristic%20and%20Nearest%20Insertion%20heuristic%20in%2088%25%20and%2099%25%20of%20the%20cases%0Astudied%2C%20respectively.%20When%20compared%20with%20metaheuristic%20algorithms%2C%20the%0Aproposed%20heuristic%27s%20tour%20costs%20are%20lower%20than%20the%20solutions%20found%20by%20the%0Agenetic%20algorithm%20and%20ant%20colony%20optimization%20algorithm%20in%2087%25%20and%2095%25%20of%20the%0Ainstances%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2302.06582v4&entry.124074799=Read"},
{"title": "Bytes Are All You Need: Transformers Operating Directly On File Bytes", "author": "Maxwell Horton and Sachin Mehta and Ali Farhadi and Mohammad Rastegari", "abstract": "  Modern deep learning approaches usually utilize modality-specific processing.\nFor example, the most common deep learning approach to image classification\ninvolves decoding image file bytes into an RGB tensor which is passed into a\nneural network. Instead, we investigate modality-independent representation\nlearning by performing classification directly on file bytes, without the need\nfor decoding files at inference time. This enables models to operate on various\nmodalities without any hand-designed, modality-specific processing. Our model,\nByteFormer, improves ImageNet Top-1 classification accuracy by $5\\%$ (from\n$72.2\\%$ to $77.33\\%$) relative to DeIT models of similar size. Compared to\nPerceiver IO, our model requires absolutely no modality-specific processing at\ninference time, and uses an order of magnitude fewer parameters at equivalent\naccuracy on ImageNet. We demonstrate that the same ByteFormer architecture can\nperform audio classification without modifications or modality-specific\npreprocessing. We achieve $95.42\\%$ classification accuracy on the Speech\nCommands V2 dataset (comparable to the state-of-the-art accuracy of $98.7\\%$).\nAdditionally, we demonstrate that ByteFormer can operate jointly on images and\naudio, handling joint classification without explicit knowledge of the input\nmodality. We release our code at\nhttps://github.com/apple/corenet/tree/main/projects/byteformer.\n", "link": "http://arxiv.org/abs/2306.00238v2", "date": "2024-07-01", "relevancy": 1.5514, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5384}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5168}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5088}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bytes%20Are%20All%20You%20Need%3A%20Transformers%20Operating%20Directly%20On%20File%20Bytes&body=Title%3A%20Bytes%20Are%20All%20You%20Need%3A%20Transformers%20Operating%20Directly%20On%20File%20Bytes%0AAuthor%3A%20Maxwell%20Horton%20and%20Sachin%20Mehta%20and%20Ali%20Farhadi%20and%20Mohammad%20Rastegari%0AAbstract%3A%20%20%20Modern%20deep%20learning%20approaches%20usually%20utilize%20modality-specific%20processing.%0AFor%20example%2C%20the%20most%20common%20deep%20learning%20approach%20to%20image%20classification%0Ainvolves%20decoding%20image%20file%20bytes%20into%20an%20RGB%20tensor%20which%20is%20passed%20into%20a%0Aneural%20network.%20Instead%2C%20we%20investigate%20modality-independent%20representation%0Alearning%20by%20performing%20classification%20directly%20on%20file%20bytes%2C%20without%20the%20need%0Afor%20decoding%20files%20at%20inference%20time.%20This%20enables%20models%20to%20operate%20on%20various%0Amodalities%20without%20any%20hand-designed%2C%20modality-specific%20processing.%20Our%20model%2C%0AByteFormer%2C%20improves%20ImageNet%20Top-1%20classification%20accuracy%20by%20%245%5C%25%24%20%28from%0A%2472.2%5C%25%24%20to%20%2477.33%5C%25%24%29%20relative%20to%20DeIT%20models%20of%20similar%20size.%20Compared%20to%0APerceiver%20IO%2C%20our%20model%20requires%20absolutely%20no%20modality-specific%20processing%20at%0Ainference%20time%2C%20and%20uses%20an%20order%20of%20magnitude%20fewer%20parameters%20at%20equivalent%0Aaccuracy%20on%20ImageNet.%20We%20demonstrate%20that%20the%20same%20ByteFormer%20architecture%20can%0Aperform%20audio%20classification%20without%20modifications%20or%20modality-specific%0Apreprocessing.%20We%20achieve%20%2495.42%5C%25%24%20classification%20accuracy%20on%20the%20Speech%0ACommands%20V2%20dataset%20%28comparable%20to%20the%20state-of-the-art%20accuracy%20of%20%2498.7%5C%25%24%29.%0AAdditionally%2C%20we%20demonstrate%20that%20ByteFormer%20can%20operate%20jointly%20on%20images%20and%0Aaudio%2C%20handling%20joint%20classification%20without%20explicit%20knowledge%20of%20the%20input%0Amodality.%20We%20release%20our%20code%20at%0Ahttps%3A//github.com/apple/corenet/tree/main/projects/byteformer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.00238v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBytes%2520Are%2520All%2520You%2520Need%253A%2520Transformers%2520Operating%2520Directly%2520On%2520File%2520Bytes%26entry.906535625%3DMaxwell%2520Horton%2520and%2520Sachin%2520Mehta%2520and%2520Ali%2520Farhadi%2520and%2520Mohammad%2520Rastegari%26entry.1292438233%3D%2520%2520Modern%2520deep%2520learning%2520approaches%2520usually%2520utilize%2520modality-specific%2520processing.%250AFor%2520example%252C%2520the%2520most%2520common%2520deep%2520learning%2520approach%2520to%2520image%2520classification%250Ainvolves%2520decoding%2520image%2520file%2520bytes%2520into%2520an%2520RGB%2520tensor%2520which%2520is%2520passed%2520into%2520a%250Aneural%2520network.%2520Instead%252C%2520we%2520investigate%2520modality-independent%2520representation%250Alearning%2520by%2520performing%2520classification%2520directly%2520on%2520file%2520bytes%252C%2520without%2520the%2520need%250Afor%2520decoding%2520files%2520at%2520inference%2520time.%2520This%2520enables%2520models%2520to%2520operate%2520on%2520various%250Amodalities%2520without%2520any%2520hand-designed%252C%2520modality-specific%2520processing.%2520Our%2520model%252C%250AByteFormer%252C%2520improves%2520ImageNet%2520Top-1%2520classification%2520accuracy%2520by%2520%25245%255C%2525%2524%2520%2528from%250A%252472.2%255C%2525%2524%2520to%2520%252477.33%255C%2525%2524%2529%2520relative%2520to%2520DeIT%2520models%2520of%2520similar%2520size.%2520Compared%2520to%250APerceiver%2520IO%252C%2520our%2520model%2520requires%2520absolutely%2520no%2520modality-specific%2520processing%2520at%250Ainference%2520time%252C%2520and%2520uses%2520an%2520order%2520of%2520magnitude%2520fewer%2520parameters%2520at%2520equivalent%250Aaccuracy%2520on%2520ImageNet.%2520We%2520demonstrate%2520that%2520the%2520same%2520ByteFormer%2520architecture%2520can%250Aperform%2520audio%2520classification%2520without%2520modifications%2520or%2520modality-specific%250Apreprocessing.%2520We%2520achieve%2520%252495.42%255C%2525%2524%2520classification%2520accuracy%2520on%2520the%2520Speech%250ACommands%2520V2%2520dataset%2520%2528comparable%2520to%2520the%2520state-of-the-art%2520accuracy%2520of%2520%252498.7%255C%2525%2524%2529.%250AAdditionally%252C%2520we%2520demonstrate%2520that%2520ByteFormer%2520can%2520operate%2520jointly%2520on%2520images%2520and%250Aaudio%252C%2520handling%2520joint%2520classification%2520without%2520explicit%2520knowledge%2520of%2520the%2520input%250Amodality.%2520We%2520release%2520our%2520code%2520at%250Ahttps%253A//github.com/apple/corenet/tree/main/projects/byteformer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.00238v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bytes%20Are%20All%20You%20Need%3A%20Transformers%20Operating%20Directly%20On%20File%20Bytes&entry.906535625=Maxwell%20Horton%20and%20Sachin%20Mehta%20and%20Ali%20Farhadi%20and%20Mohammad%20Rastegari&entry.1292438233=%20%20Modern%20deep%20learning%20approaches%20usually%20utilize%20modality-specific%20processing.%0AFor%20example%2C%20the%20most%20common%20deep%20learning%20approach%20to%20image%20classification%0Ainvolves%20decoding%20image%20file%20bytes%20into%20an%20RGB%20tensor%20which%20is%20passed%20into%20a%0Aneural%20network.%20Instead%2C%20we%20investigate%20modality-independent%20representation%0Alearning%20by%20performing%20classification%20directly%20on%20file%20bytes%2C%20without%20the%20need%0Afor%20decoding%20files%20at%20inference%20time.%20This%20enables%20models%20to%20operate%20on%20various%0Amodalities%20without%20any%20hand-designed%2C%20modality-specific%20processing.%20Our%20model%2C%0AByteFormer%2C%20improves%20ImageNet%20Top-1%20classification%20accuracy%20by%20%245%5C%25%24%20%28from%0A%2472.2%5C%25%24%20to%20%2477.33%5C%25%24%29%20relative%20to%20DeIT%20models%20of%20similar%20size.%20Compared%20to%0APerceiver%20IO%2C%20our%20model%20requires%20absolutely%20no%20modality-specific%20processing%20at%0Ainference%20time%2C%20and%20uses%20an%20order%20of%20magnitude%20fewer%20parameters%20at%20equivalent%0Aaccuracy%20on%20ImageNet.%20We%20demonstrate%20that%20the%20same%20ByteFormer%20architecture%20can%0Aperform%20audio%20classification%20without%20modifications%20or%20modality-specific%0Apreprocessing.%20We%20achieve%20%2495.42%5C%25%24%20classification%20accuracy%20on%20the%20Speech%0ACommands%20V2%20dataset%20%28comparable%20to%20the%20state-of-the-art%20accuracy%20of%20%2498.7%5C%25%24%29.%0AAdditionally%2C%20we%20demonstrate%20that%20ByteFormer%20can%20operate%20jointly%20on%20images%20and%0Aaudio%2C%20handling%20joint%20classification%20without%20explicit%20knowledge%20of%20the%20input%0Amodality.%20We%20release%20our%20code%20at%0Ahttps%3A//github.com/apple/corenet/tree/main/projects/byteformer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.00238v2&entry.124074799=Read"},
{"title": "BMW Agents -- A Framework For Task Automation Through Multi-Agent\n  Collaboration", "author": "Noel Crawford and Edward B. Duffy and Iman Evazzade and Torsten Foehr and Gregory Robbins and Debbrata Kumar Saha and Jiya Varma and Marcin Ziolkowski", "abstract": "  Autonomous agents driven by Large Language Models (LLMs) offer enormous\npotential for automation. Early proof of this technology can be found in\nvarious demonstrations of agents solving complex tasks, interacting with\nexternal systems to augment their knowledge, and triggering actions. In\nparticular, workflows involving multiple agents solving complex tasks in a\ncollaborative fashion exemplify their capacity to operate in less strict and\nless well-defined environments. Thus, a multi-agent approach has great\npotential for serving as a backbone in many industrial applications, ranging\nfrom complex knowledge retrieval systems to next generation robotic process\nautomation. Given the reasoning abilities within the current generation of\nLLMs, complex processes require a multi-step approach that includes a plan of\nwell-defined and modular tasks. Depending on the level of complexity, these\ntasks can be executed either by a single agent or a group of agents. In this\nwork, we focus on designing a flexible agent engineering framework with careful\nattention to planning and execution, capable of handling complex use case\napplications across various domains. The proposed framework provides\nreliability in industrial applications and presents techniques to ensure a\nscalable, flexible, and collaborative workflow for multiple autonomous agents\nworking together towards solving tasks.\n", "link": "http://arxiv.org/abs/2406.20041v2", "date": "2024-07-01", "relevancy": 1.5497, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5437}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5125}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BMW%20Agents%20--%20A%20Framework%20For%20Task%20Automation%20Through%20Multi-Agent%0A%20%20Collaboration&body=Title%3A%20BMW%20Agents%20--%20A%20Framework%20For%20Task%20Automation%20Through%20Multi-Agent%0A%20%20Collaboration%0AAuthor%3A%20Noel%20Crawford%20and%20Edward%20B.%20Duffy%20and%20Iman%20Evazzade%20and%20Torsten%20Foehr%20and%20Gregory%20Robbins%20and%20Debbrata%20Kumar%20Saha%20and%20Jiya%20Varma%20and%20Marcin%20Ziolkowski%0AAbstract%3A%20%20%20Autonomous%20agents%20driven%20by%20Large%20Language%20Models%20%28LLMs%29%20offer%20enormous%0Apotential%20for%20automation.%20Early%20proof%20of%20this%20technology%20can%20be%20found%20in%0Avarious%20demonstrations%20of%20agents%20solving%20complex%20tasks%2C%20interacting%20with%0Aexternal%20systems%20to%20augment%20their%20knowledge%2C%20and%20triggering%20actions.%20In%0Aparticular%2C%20workflows%20involving%20multiple%20agents%20solving%20complex%20tasks%20in%20a%0Acollaborative%20fashion%20exemplify%20their%20capacity%20to%20operate%20in%20less%20strict%20and%0Aless%20well-defined%20environments.%20Thus%2C%20a%20multi-agent%20approach%20has%20great%0Apotential%20for%20serving%20as%20a%20backbone%20in%20many%20industrial%20applications%2C%20ranging%0Afrom%20complex%20knowledge%20retrieval%20systems%20to%20next%20generation%20robotic%20process%0Aautomation.%20Given%20the%20reasoning%20abilities%20within%20the%20current%20generation%20of%0ALLMs%2C%20complex%20processes%20require%20a%20multi-step%20approach%20that%20includes%20a%20plan%20of%0Awell-defined%20and%20modular%20tasks.%20Depending%20on%20the%20level%20of%20complexity%2C%20these%0Atasks%20can%20be%20executed%20either%20by%20a%20single%20agent%20or%20a%20group%20of%20agents.%20In%20this%0Awork%2C%20we%20focus%20on%20designing%20a%20flexible%20agent%20engineering%20framework%20with%20careful%0Aattention%20to%20planning%20and%20execution%2C%20capable%20of%20handling%20complex%20use%20case%0Aapplications%20across%20various%20domains.%20The%20proposed%20framework%20provides%0Areliability%20in%20industrial%20applications%20and%20presents%20techniques%20to%20ensure%20a%0Ascalable%2C%20flexible%2C%20and%20collaborative%20workflow%20for%20multiple%20autonomous%20agents%0Aworking%20together%20towards%20solving%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.20041v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBMW%2520Agents%2520--%2520A%2520Framework%2520For%2520Task%2520Automation%2520Through%2520Multi-Agent%250A%2520%2520Collaboration%26entry.906535625%3DNoel%2520Crawford%2520and%2520Edward%2520B.%2520Duffy%2520and%2520Iman%2520Evazzade%2520and%2520Torsten%2520Foehr%2520and%2520Gregory%2520Robbins%2520and%2520Debbrata%2520Kumar%2520Saha%2520and%2520Jiya%2520Varma%2520and%2520Marcin%2520Ziolkowski%26entry.1292438233%3D%2520%2520Autonomous%2520agents%2520driven%2520by%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520offer%2520enormous%250Apotential%2520for%2520automation.%2520Early%2520proof%2520of%2520this%2520technology%2520can%2520be%2520found%2520in%250Avarious%2520demonstrations%2520of%2520agents%2520solving%2520complex%2520tasks%252C%2520interacting%2520with%250Aexternal%2520systems%2520to%2520augment%2520their%2520knowledge%252C%2520and%2520triggering%2520actions.%2520In%250Aparticular%252C%2520workflows%2520involving%2520multiple%2520agents%2520solving%2520complex%2520tasks%2520in%2520a%250Acollaborative%2520fashion%2520exemplify%2520their%2520capacity%2520to%2520operate%2520in%2520less%2520strict%2520and%250Aless%2520well-defined%2520environments.%2520Thus%252C%2520a%2520multi-agent%2520approach%2520has%2520great%250Apotential%2520for%2520serving%2520as%2520a%2520backbone%2520in%2520many%2520industrial%2520applications%252C%2520ranging%250Afrom%2520complex%2520knowledge%2520retrieval%2520systems%2520to%2520next%2520generation%2520robotic%2520process%250Aautomation.%2520Given%2520the%2520reasoning%2520abilities%2520within%2520the%2520current%2520generation%2520of%250ALLMs%252C%2520complex%2520processes%2520require%2520a%2520multi-step%2520approach%2520that%2520includes%2520a%2520plan%2520of%250Awell-defined%2520and%2520modular%2520tasks.%2520Depending%2520on%2520the%2520level%2520of%2520complexity%252C%2520these%250Atasks%2520can%2520be%2520executed%2520either%2520by%2520a%2520single%2520agent%2520or%2520a%2520group%2520of%2520agents.%2520In%2520this%250Awork%252C%2520we%2520focus%2520on%2520designing%2520a%2520flexible%2520agent%2520engineering%2520framework%2520with%2520careful%250Aattention%2520to%2520planning%2520and%2520execution%252C%2520capable%2520of%2520handling%2520complex%2520use%2520case%250Aapplications%2520across%2520various%2520domains.%2520The%2520proposed%2520framework%2520provides%250Areliability%2520in%2520industrial%2520applications%2520and%2520presents%2520techniques%2520to%2520ensure%2520a%250Ascalable%252C%2520flexible%252C%2520and%2520collaborative%2520workflow%2520for%2520multiple%2520autonomous%2520agents%250Aworking%2520together%2520towards%2520solving%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.20041v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BMW%20Agents%20--%20A%20Framework%20For%20Task%20Automation%20Through%20Multi-Agent%0A%20%20Collaboration&entry.906535625=Noel%20Crawford%20and%20Edward%20B.%20Duffy%20and%20Iman%20Evazzade%20and%20Torsten%20Foehr%20and%20Gregory%20Robbins%20and%20Debbrata%20Kumar%20Saha%20and%20Jiya%20Varma%20and%20Marcin%20Ziolkowski&entry.1292438233=%20%20Autonomous%20agents%20driven%20by%20Large%20Language%20Models%20%28LLMs%29%20offer%20enormous%0Apotential%20for%20automation.%20Early%20proof%20of%20this%20technology%20can%20be%20found%20in%0Avarious%20demonstrations%20of%20agents%20solving%20complex%20tasks%2C%20interacting%20with%0Aexternal%20systems%20to%20augment%20their%20knowledge%2C%20and%20triggering%20actions.%20In%0Aparticular%2C%20workflows%20involving%20multiple%20agents%20solving%20complex%20tasks%20in%20a%0Acollaborative%20fashion%20exemplify%20their%20capacity%20to%20operate%20in%20less%20strict%20and%0Aless%20well-defined%20environments.%20Thus%2C%20a%20multi-agent%20approach%20has%20great%0Apotential%20for%20serving%20as%20a%20backbone%20in%20many%20industrial%20applications%2C%20ranging%0Afrom%20complex%20knowledge%20retrieval%20systems%20to%20next%20generation%20robotic%20process%0Aautomation.%20Given%20the%20reasoning%20abilities%20within%20the%20current%20generation%20of%0ALLMs%2C%20complex%20processes%20require%20a%20multi-step%20approach%20that%20includes%20a%20plan%20of%0Awell-defined%20and%20modular%20tasks.%20Depending%20on%20the%20level%20of%20complexity%2C%20these%0Atasks%20can%20be%20executed%20either%20by%20a%20single%20agent%20or%20a%20group%20of%20agents.%20In%20this%0Awork%2C%20we%20focus%20on%20designing%20a%20flexible%20agent%20engineering%20framework%20with%20careful%0Aattention%20to%20planning%20and%20execution%2C%20capable%20of%20handling%20complex%20use%20case%0Aapplications%20across%20various%20domains.%20The%20proposed%20framework%20provides%0Areliability%20in%20industrial%20applications%20and%20presents%20techniques%20to%20ensure%20a%0Ascalable%2C%20flexible%2C%20and%20collaborative%20workflow%20for%20multiple%20autonomous%20agents%0Aworking%20together%20towards%20solving%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.20041v2&entry.124074799=Read"},
{"title": "VR Isle Academy: A VR Digital Twin Approach for Robotic Surgical Skill\n  Development", "author": "Achilleas Filippidis and Nikolaos Marmaras and Michael Maravgakis and Alexandra Plexousaki and Manos Kamarianakis and George Papagiannakis", "abstract": "  Contemporary progress in the field of robotics, marked by improved efficiency\nand stability, has paved the way for the global adoption of surgical robotic\nsystems (SRS). While these systems enhance surgeons' skills by offering a more\naccurate and less invasive approach to operations, they come at a considerable\ncost. Moreover, SRS components often involve heavy machinery, making the\ntraining process challenging due to limited access to such equipment. In this\npaper we introduce a cost-effective way to facilitate training for a simulator\nof a SRS via a portable, device-agnostic, ultra realistic simulation with hand\ntracking and feet tracking support. Error assessment is accessible in both\nreal-time and offline, which enables the monitoring and tracking of users'\nperformance. The VR application has been objectively evaluated by several\nuntrained testers showcasing significant reduction in error metrics as the\nnumber of training sessions increases. This indicates that the proposed VR\napplication denoted as VR Isle Academy operates efficiently, improving the\nrobot - controlling skills of the testers in an intuitive and immersive way\ntowards reducing the learning curve at minimal cost.\n", "link": "http://arxiv.org/abs/2406.00002v2", "date": "2024-07-01", "relevancy": 1.5471, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5244}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5135}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4961}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VR%20Isle%20Academy%3A%20A%20VR%20Digital%20Twin%20Approach%20for%20Robotic%20Surgical%20Skill%0A%20%20Development&body=Title%3A%20VR%20Isle%20Academy%3A%20A%20VR%20Digital%20Twin%20Approach%20for%20Robotic%20Surgical%20Skill%0A%20%20Development%0AAuthor%3A%20Achilleas%20Filippidis%20and%20Nikolaos%20Marmaras%20and%20Michael%20Maravgakis%20and%20Alexandra%20Plexousaki%20and%20Manos%20Kamarianakis%20and%20George%20Papagiannakis%0AAbstract%3A%20%20%20Contemporary%20progress%20in%20the%20field%20of%20robotics%2C%20marked%20by%20improved%20efficiency%0Aand%20stability%2C%20has%20paved%20the%20way%20for%20the%20global%20adoption%20of%20surgical%20robotic%0Asystems%20%28SRS%29.%20While%20these%20systems%20enhance%20surgeons%27%20skills%20by%20offering%20a%20more%0Aaccurate%20and%20less%20invasive%20approach%20to%20operations%2C%20they%20come%20at%20a%20considerable%0Acost.%20Moreover%2C%20SRS%20components%20often%20involve%20heavy%20machinery%2C%20making%20the%0Atraining%20process%20challenging%20due%20to%20limited%20access%20to%20such%20equipment.%20In%20this%0Apaper%20we%20introduce%20a%20cost-effective%20way%20to%20facilitate%20training%20for%20a%20simulator%0Aof%20a%20SRS%20via%20a%20portable%2C%20device-agnostic%2C%20ultra%20realistic%20simulation%20with%20hand%0Atracking%20and%20feet%20tracking%20support.%20Error%20assessment%20is%20accessible%20in%20both%0Areal-time%20and%20offline%2C%20which%20enables%20the%20monitoring%20and%20tracking%20of%20users%27%0Aperformance.%20The%20VR%20application%20has%20been%20objectively%20evaluated%20by%20several%0Auntrained%20testers%20showcasing%20significant%20reduction%20in%20error%20metrics%20as%20the%0Anumber%20of%20training%20sessions%20increases.%20This%20indicates%20that%20the%20proposed%20VR%0Aapplication%20denoted%20as%20VR%20Isle%20Academy%20operates%20efficiently%2C%20improving%20the%0Arobot%20-%20controlling%20skills%20of%20the%20testers%20in%20an%20intuitive%20and%20immersive%20way%0Atowards%20reducing%20the%20learning%20curve%20at%20minimal%20cost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00002v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVR%2520Isle%2520Academy%253A%2520A%2520VR%2520Digital%2520Twin%2520Approach%2520for%2520Robotic%2520Surgical%2520Skill%250A%2520%2520Development%26entry.906535625%3DAchilleas%2520Filippidis%2520and%2520Nikolaos%2520Marmaras%2520and%2520Michael%2520Maravgakis%2520and%2520Alexandra%2520Plexousaki%2520and%2520Manos%2520Kamarianakis%2520and%2520George%2520Papagiannakis%26entry.1292438233%3D%2520%2520Contemporary%2520progress%2520in%2520the%2520field%2520of%2520robotics%252C%2520marked%2520by%2520improved%2520efficiency%250Aand%2520stability%252C%2520has%2520paved%2520the%2520way%2520for%2520the%2520global%2520adoption%2520of%2520surgical%2520robotic%250Asystems%2520%2528SRS%2529.%2520While%2520these%2520systems%2520enhance%2520surgeons%2527%2520skills%2520by%2520offering%2520a%2520more%250Aaccurate%2520and%2520less%2520invasive%2520approach%2520to%2520operations%252C%2520they%2520come%2520at%2520a%2520considerable%250Acost.%2520Moreover%252C%2520SRS%2520components%2520often%2520involve%2520heavy%2520machinery%252C%2520making%2520the%250Atraining%2520process%2520challenging%2520due%2520to%2520limited%2520access%2520to%2520such%2520equipment.%2520In%2520this%250Apaper%2520we%2520introduce%2520a%2520cost-effective%2520way%2520to%2520facilitate%2520training%2520for%2520a%2520simulator%250Aof%2520a%2520SRS%2520via%2520a%2520portable%252C%2520device-agnostic%252C%2520ultra%2520realistic%2520simulation%2520with%2520hand%250Atracking%2520and%2520feet%2520tracking%2520support.%2520Error%2520assessment%2520is%2520accessible%2520in%2520both%250Areal-time%2520and%2520offline%252C%2520which%2520enables%2520the%2520monitoring%2520and%2520tracking%2520of%2520users%2527%250Aperformance.%2520The%2520VR%2520application%2520has%2520been%2520objectively%2520evaluated%2520by%2520several%250Auntrained%2520testers%2520showcasing%2520significant%2520reduction%2520in%2520error%2520metrics%2520as%2520the%250Anumber%2520of%2520training%2520sessions%2520increases.%2520This%2520indicates%2520that%2520the%2520proposed%2520VR%250Aapplication%2520denoted%2520as%2520VR%2520Isle%2520Academy%2520operates%2520efficiently%252C%2520improving%2520the%250Arobot%2520-%2520controlling%2520skills%2520of%2520the%2520testers%2520in%2520an%2520intuitive%2520and%2520immersive%2520way%250Atowards%2520reducing%2520the%2520learning%2520curve%2520at%2520minimal%2520cost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00002v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VR%20Isle%20Academy%3A%20A%20VR%20Digital%20Twin%20Approach%20for%20Robotic%20Surgical%20Skill%0A%20%20Development&entry.906535625=Achilleas%20Filippidis%20and%20Nikolaos%20Marmaras%20and%20Michael%20Maravgakis%20and%20Alexandra%20Plexousaki%20and%20Manos%20Kamarianakis%20and%20George%20Papagiannakis&entry.1292438233=%20%20Contemporary%20progress%20in%20the%20field%20of%20robotics%2C%20marked%20by%20improved%20efficiency%0Aand%20stability%2C%20has%20paved%20the%20way%20for%20the%20global%20adoption%20of%20surgical%20robotic%0Asystems%20%28SRS%29.%20While%20these%20systems%20enhance%20surgeons%27%20skills%20by%20offering%20a%20more%0Aaccurate%20and%20less%20invasive%20approach%20to%20operations%2C%20they%20come%20at%20a%20considerable%0Acost.%20Moreover%2C%20SRS%20components%20often%20involve%20heavy%20machinery%2C%20making%20the%0Atraining%20process%20challenging%20due%20to%20limited%20access%20to%20such%20equipment.%20In%20this%0Apaper%20we%20introduce%20a%20cost-effective%20way%20to%20facilitate%20training%20for%20a%20simulator%0Aof%20a%20SRS%20via%20a%20portable%2C%20device-agnostic%2C%20ultra%20realistic%20simulation%20with%20hand%0Atracking%20and%20feet%20tracking%20support.%20Error%20assessment%20is%20accessible%20in%20both%0Areal-time%20and%20offline%2C%20which%20enables%20the%20monitoring%20and%20tracking%20of%20users%27%0Aperformance.%20The%20VR%20application%20has%20been%20objectively%20evaluated%20by%20several%0Auntrained%20testers%20showcasing%20significant%20reduction%20in%20error%20metrics%20as%20the%0Anumber%20of%20training%20sessions%20increases.%20This%20indicates%20that%20the%20proposed%20VR%0Aapplication%20denoted%20as%20VR%20Isle%20Academy%20operates%20efficiently%2C%20improving%20the%0Arobot%20-%20controlling%20skills%20of%20the%20testers%20in%20an%20intuitive%20and%20immersive%20way%0Atowards%20reducing%20the%20learning%20curve%20at%20minimal%20cost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00002v2&entry.124074799=Read"},
{"title": "Quadratic Programming-based Reference Spreading Control for Dual-Arm\n  Robotic Manipulation with Planned Simultaneous Impacts", "author": "Jari van Steen and Gijs van den Brandt and Nathan van de Wouw and Jens Kober and Alessandro Saccon", "abstract": "  With the aim of further enabling the exploitation of intentional impacts in\nrobotic manipulation, a control framework is presented that directly tackles\nthe challenges posed by tracking control of robotic manipulators that are\ntasked to perform nominally simultaneous impacts. This framework is an\nextension of the reference spreading control framework, in which overlapping\nante- and post-impact references that are consistent with impact dynamics are\ndefined. In this work, such a reference is constructed starting from a\nteleoperation-based approach. By using the corresponding ante- and post-impact\ncontrol modes in the scope of a quadratic programming control approach, peaking\nof the velocity error and control inputs due to impacts is avoided while\nmaintaining high tracking performance. With the inclusion of a novel interim\nmode, we aim to also avoid input peaks and steps when uncertainty in the\nenvironment causes a series of unplanned single impacts to occur rather than\nthe planned simultaneous impact. This work in particular presents for the first\ntime an experimental evaluation of reference spreading control on a robotic\nsetup, showcasing its robustness against uncertainty in the environment\ncompared to three baseline control approaches.\n", "link": "http://arxiv.org/abs/2305.08643v4", "date": "2024-07-01", "relevancy": 1.5406, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.526}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.522}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quadratic%20Programming-based%20Reference%20Spreading%20Control%20for%20Dual-Arm%0A%20%20Robotic%20Manipulation%20with%20Planned%20Simultaneous%20Impacts&body=Title%3A%20Quadratic%20Programming-based%20Reference%20Spreading%20Control%20for%20Dual-Arm%0A%20%20Robotic%20Manipulation%20with%20Planned%20Simultaneous%20Impacts%0AAuthor%3A%20Jari%20van%20Steen%20and%20Gijs%20van%20den%20Brandt%20and%20Nathan%20van%20de%20Wouw%20and%20Jens%20Kober%20and%20Alessandro%20Saccon%0AAbstract%3A%20%20%20With%20the%20aim%20of%20further%20enabling%20the%20exploitation%20of%20intentional%20impacts%20in%0Arobotic%20manipulation%2C%20a%20control%20framework%20is%20presented%20that%20directly%20tackles%0Athe%20challenges%20posed%20by%20tracking%20control%20of%20robotic%20manipulators%20that%20are%0Atasked%20to%20perform%20nominally%20simultaneous%20impacts.%20This%20framework%20is%20an%0Aextension%20of%20the%20reference%20spreading%20control%20framework%2C%20in%20which%20overlapping%0Aante-%20and%20post-impact%20references%20that%20are%20consistent%20with%20impact%20dynamics%20are%0Adefined.%20In%20this%20work%2C%20such%20a%20reference%20is%20constructed%20starting%20from%20a%0Ateleoperation-based%20approach.%20By%20using%20the%20corresponding%20ante-%20and%20post-impact%0Acontrol%20modes%20in%20the%20scope%20of%20a%20quadratic%20programming%20control%20approach%2C%20peaking%0Aof%20the%20velocity%20error%20and%20control%20inputs%20due%20to%20impacts%20is%20avoided%20while%0Amaintaining%20high%20tracking%20performance.%20With%20the%20inclusion%20of%20a%20novel%20interim%0Amode%2C%20we%20aim%20to%20also%20avoid%20input%20peaks%20and%20steps%20when%20uncertainty%20in%20the%0Aenvironment%20causes%20a%20series%20of%20unplanned%20single%20impacts%20to%20occur%20rather%20than%0Athe%20planned%20simultaneous%20impact.%20This%20work%20in%20particular%20presents%20for%20the%20first%0Atime%20an%20experimental%20evaluation%20of%20reference%20spreading%20control%20on%20a%20robotic%0Asetup%2C%20showcasing%20its%20robustness%20against%20uncertainty%20in%20the%20environment%0Acompared%20to%20three%20baseline%20control%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.08643v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuadratic%2520Programming-based%2520Reference%2520Spreading%2520Control%2520for%2520Dual-Arm%250A%2520%2520Robotic%2520Manipulation%2520with%2520Planned%2520Simultaneous%2520Impacts%26entry.906535625%3DJari%2520van%2520Steen%2520and%2520Gijs%2520van%2520den%2520Brandt%2520and%2520Nathan%2520van%2520de%2520Wouw%2520and%2520Jens%2520Kober%2520and%2520Alessandro%2520Saccon%26entry.1292438233%3D%2520%2520With%2520the%2520aim%2520of%2520further%2520enabling%2520the%2520exploitation%2520of%2520intentional%2520impacts%2520in%250Arobotic%2520manipulation%252C%2520a%2520control%2520framework%2520is%2520presented%2520that%2520directly%2520tackles%250Athe%2520challenges%2520posed%2520by%2520tracking%2520control%2520of%2520robotic%2520manipulators%2520that%2520are%250Atasked%2520to%2520perform%2520nominally%2520simultaneous%2520impacts.%2520This%2520framework%2520is%2520an%250Aextension%2520of%2520the%2520reference%2520spreading%2520control%2520framework%252C%2520in%2520which%2520overlapping%250Aante-%2520and%2520post-impact%2520references%2520that%2520are%2520consistent%2520with%2520impact%2520dynamics%2520are%250Adefined.%2520In%2520this%2520work%252C%2520such%2520a%2520reference%2520is%2520constructed%2520starting%2520from%2520a%250Ateleoperation-based%2520approach.%2520By%2520using%2520the%2520corresponding%2520ante-%2520and%2520post-impact%250Acontrol%2520modes%2520in%2520the%2520scope%2520of%2520a%2520quadratic%2520programming%2520control%2520approach%252C%2520peaking%250Aof%2520the%2520velocity%2520error%2520and%2520control%2520inputs%2520due%2520to%2520impacts%2520is%2520avoided%2520while%250Amaintaining%2520high%2520tracking%2520performance.%2520With%2520the%2520inclusion%2520of%2520a%2520novel%2520interim%250Amode%252C%2520we%2520aim%2520to%2520also%2520avoid%2520input%2520peaks%2520and%2520steps%2520when%2520uncertainty%2520in%2520the%250Aenvironment%2520causes%2520a%2520series%2520of%2520unplanned%2520single%2520impacts%2520to%2520occur%2520rather%2520than%250Athe%2520planned%2520simultaneous%2520impact.%2520This%2520work%2520in%2520particular%2520presents%2520for%2520the%2520first%250Atime%2520an%2520experimental%2520evaluation%2520of%2520reference%2520spreading%2520control%2520on%2520a%2520robotic%250Asetup%252C%2520showcasing%2520its%2520robustness%2520against%2520uncertainty%2520in%2520the%2520environment%250Acompared%2520to%2520three%2520baseline%2520control%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.08643v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quadratic%20Programming-based%20Reference%20Spreading%20Control%20for%20Dual-Arm%0A%20%20Robotic%20Manipulation%20with%20Planned%20Simultaneous%20Impacts&entry.906535625=Jari%20van%20Steen%20and%20Gijs%20van%20den%20Brandt%20and%20Nathan%20van%20de%20Wouw%20and%20Jens%20Kober%20and%20Alessandro%20Saccon&entry.1292438233=%20%20With%20the%20aim%20of%20further%20enabling%20the%20exploitation%20of%20intentional%20impacts%20in%0Arobotic%20manipulation%2C%20a%20control%20framework%20is%20presented%20that%20directly%20tackles%0Athe%20challenges%20posed%20by%20tracking%20control%20of%20robotic%20manipulators%20that%20are%0Atasked%20to%20perform%20nominally%20simultaneous%20impacts.%20This%20framework%20is%20an%0Aextension%20of%20the%20reference%20spreading%20control%20framework%2C%20in%20which%20overlapping%0Aante-%20and%20post-impact%20references%20that%20are%20consistent%20with%20impact%20dynamics%20are%0Adefined.%20In%20this%20work%2C%20such%20a%20reference%20is%20constructed%20starting%20from%20a%0Ateleoperation-based%20approach.%20By%20using%20the%20corresponding%20ante-%20and%20post-impact%0Acontrol%20modes%20in%20the%20scope%20of%20a%20quadratic%20programming%20control%20approach%2C%20peaking%0Aof%20the%20velocity%20error%20and%20control%20inputs%20due%20to%20impacts%20is%20avoided%20while%0Amaintaining%20high%20tracking%20performance.%20With%20the%20inclusion%20of%20a%20novel%20interim%0Amode%2C%20we%20aim%20to%20also%20avoid%20input%20peaks%20and%20steps%20when%20uncertainty%20in%20the%0Aenvironment%20causes%20a%20series%20of%20unplanned%20single%20impacts%20to%20occur%20rather%20than%0Athe%20planned%20simultaneous%20impact.%20This%20work%20in%20particular%20presents%20for%20the%20first%0Atime%20an%20experimental%20evaluation%20of%20reference%20spreading%20control%20on%20a%20robotic%0Asetup%2C%20showcasing%20its%20robustness%20against%20uncertainty%20in%20the%20environment%0Acompared%20to%20three%20baseline%20control%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.08643v4&entry.124074799=Read"},
{"title": "Jailbreak Vision Language Models via Bi-Modal Adversarial Prompt", "author": "Zonghao Ying and Aishan Liu and Tianyuan Zhang and Zhengmin Yu and Siyuan Liang and Xianglong Liu and Dacheng Tao", "abstract": "  In the realm of large vision language models (LVLMs), jailbreak attacks serve\nas a red-teaming approach to bypass guardrails and uncover safety implications.\nExisting jailbreaks predominantly focus on the visual modality, perturbing\nsolely visual inputs in the prompt for attacks. However, they fall short when\nconfronted with aligned models that fuse visual and textual features\nsimultaneously for generation. To address this limitation, this paper\nintroduces the Bi-Modal Adversarial Prompt Attack (BAP), which executes\njailbreaks by optimizing textual and visual prompts cohesively. Initially, we\nadversarially embed universally harmful perturbations in an image, guided by a\nfew-shot query-agnostic corpus (e.g., affirmative prefixes and negative\ninhibitions). This process ensures that image prompt LVLMs to respond\npositively to any harmful queries. Subsequently, leveraging the adversarial\nimage, we optimize textual prompts with specific harmful intent. In particular,\nwe utilize a large language model to analyze jailbreak failures and employ\nchain-of-thought reasoning to refine textual prompts through a\nfeedback-iteration manner. To validate the efficacy of our approach, we\nconducted extensive evaluations on various datasets and LVLMs, demonstrating\nthat our method significantly outperforms other methods by large margins\n(+29.03% in attack success rate on average). Additionally, we showcase the\npotential of our attacks on black-box commercial LVLMs, such as Gemini and\nChatGLM.\n", "link": "http://arxiv.org/abs/2406.04031v2", "date": "2024-07-01", "relevancy": 1.4569, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4973}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.476}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Jailbreak%20Vision%20Language%20Models%20via%20Bi-Modal%20Adversarial%20Prompt&body=Title%3A%20Jailbreak%20Vision%20Language%20Models%20via%20Bi-Modal%20Adversarial%20Prompt%0AAuthor%3A%20Zonghao%20Ying%20and%20Aishan%20Liu%20and%20Tianyuan%20Zhang%20and%20Zhengmin%20Yu%20and%20Siyuan%20Liang%20and%20Xianglong%20Liu%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20In%20the%20realm%20of%20large%20vision%20language%20models%20%28LVLMs%29%2C%20jailbreak%20attacks%20serve%0Aas%20a%20red-teaming%20approach%20to%20bypass%20guardrails%20and%20uncover%20safety%20implications.%0AExisting%20jailbreaks%20predominantly%20focus%20on%20the%20visual%20modality%2C%20perturbing%0Asolely%20visual%20inputs%20in%20the%20prompt%20for%20attacks.%20However%2C%20they%20fall%20short%20when%0Aconfronted%20with%20aligned%20models%20that%20fuse%20visual%20and%20textual%20features%0Asimultaneously%20for%20generation.%20To%20address%20this%20limitation%2C%20this%20paper%0Aintroduces%20the%20Bi-Modal%20Adversarial%20Prompt%20Attack%20%28BAP%29%2C%20which%20executes%0Ajailbreaks%20by%20optimizing%20textual%20and%20visual%20prompts%20cohesively.%20Initially%2C%20we%0Aadversarially%20embed%20universally%20harmful%20perturbations%20in%20an%20image%2C%20guided%20by%20a%0Afew-shot%20query-agnostic%20corpus%20%28e.g.%2C%20affirmative%20prefixes%20and%20negative%0Ainhibitions%29.%20This%20process%20ensures%20that%20image%20prompt%20LVLMs%20to%20respond%0Apositively%20to%20any%20harmful%20queries.%20Subsequently%2C%20leveraging%20the%20adversarial%0Aimage%2C%20we%20optimize%20textual%20prompts%20with%20specific%20harmful%20intent.%20In%20particular%2C%0Awe%20utilize%20a%20large%20language%20model%20to%20analyze%20jailbreak%20failures%20and%20employ%0Achain-of-thought%20reasoning%20to%20refine%20textual%20prompts%20through%20a%0Afeedback-iteration%20manner.%20To%20validate%20the%20efficacy%20of%20our%20approach%2C%20we%0Aconducted%20extensive%20evaluations%20on%20various%20datasets%20and%20LVLMs%2C%20demonstrating%0Athat%20our%20method%20significantly%20outperforms%20other%20methods%20by%20large%20margins%0A%28%2B29.03%25%20in%20attack%20success%20rate%20on%20average%29.%20Additionally%2C%20we%20showcase%20the%0Apotential%20of%20our%20attacks%20on%20black-box%20commercial%20LVLMs%2C%20such%20as%20Gemini%20and%0AChatGLM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.04031v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJailbreak%2520Vision%2520Language%2520Models%2520via%2520Bi-Modal%2520Adversarial%2520Prompt%26entry.906535625%3DZonghao%2520Ying%2520and%2520Aishan%2520Liu%2520and%2520Tianyuan%2520Zhang%2520and%2520Zhengmin%2520Yu%2520and%2520Siyuan%2520Liang%2520and%2520Xianglong%2520Liu%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520large%2520vision%2520language%2520models%2520%2528LVLMs%2529%252C%2520jailbreak%2520attacks%2520serve%250Aas%2520a%2520red-teaming%2520approach%2520to%2520bypass%2520guardrails%2520and%2520uncover%2520safety%2520implications.%250AExisting%2520jailbreaks%2520predominantly%2520focus%2520on%2520the%2520visual%2520modality%252C%2520perturbing%250Asolely%2520visual%2520inputs%2520in%2520the%2520prompt%2520for%2520attacks.%2520However%252C%2520they%2520fall%2520short%2520when%250Aconfronted%2520with%2520aligned%2520models%2520that%2520fuse%2520visual%2520and%2520textual%2520features%250Asimultaneously%2520for%2520generation.%2520To%2520address%2520this%2520limitation%252C%2520this%2520paper%250Aintroduces%2520the%2520Bi-Modal%2520Adversarial%2520Prompt%2520Attack%2520%2528BAP%2529%252C%2520which%2520executes%250Ajailbreaks%2520by%2520optimizing%2520textual%2520and%2520visual%2520prompts%2520cohesively.%2520Initially%252C%2520we%250Aadversarially%2520embed%2520universally%2520harmful%2520perturbations%2520in%2520an%2520image%252C%2520guided%2520by%2520a%250Afew-shot%2520query-agnostic%2520corpus%2520%2528e.g.%252C%2520affirmative%2520prefixes%2520and%2520negative%250Ainhibitions%2529.%2520This%2520process%2520ensures%2520that%2520image%2520prompt%2520LVLMs%2520to%2520respond%250Apositively%2520to%2520any%2520harmful%2520queries.%2520Subsequently%252C%2520leveraging%2520the%2520adversarial%250Aimage%252C%2520we%2520optimize%2520textual%2520prompts%2520with%2520specific%2520harmful%2520intent.%2520In%2520particular%252C%250Awe%2520utilize%2520a%2520large%2520language%2520model%2520to%2520analyze%2520jailbreak%2520failures%2520and%2520employ%250Achain-of-thought%2520reasoning%2520to%2520refine%2520textual%2520prompts%2520through%2520a%250Afeedback-iteration%2520manner.%2520To%2520validate%2520the%2520efficacy%2520of%2520our%2520approach%252C%2520we%250Aconducted%2520extensive%2520evaluations%2520on%2520various%2520datasets%2520and%2520LVLMs%252C%2520demonstrating%250Athat%2520our%2520method%2520significantly%2520outperforms%2520other%2520methods%2520by%2520large%2520margins%250A%2528%252B29.03%2525%2520in%2520attack%2520success%2520rate%2520on%2520average%2529.%2520Additionally%252C%2520we%2520showcase%2520the%250Apotential%2520of%2520our%2520attacks%2520on%2520black-box%2520commercial%2520LVLMs%252C%2520such%2520as%2520Gemini%2520and%250AChatGLM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.04031v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Jailbreak%20Vision%20Language%20Models%20via%20Bi-Modal%20Adversarial%20Prompt&entry.906535625=Zonghao%20Ying%20and%20Aishan%20Liu%20and%20Tianyuan%20Zhang%20and%20Zhengmin%20Yu%20and%20Siyuan%20Liang%20and%20Xianglong%20Liu%20and%20Dacheng%20Tao&entry.1292438233=%20%20In%20the%20realm%20of%20large%20vision%20language%20models%20%28LVLMs%29%2C%20jailbreak%20attacks%20serve%0Aas%20a%20red-teaming%20approach%20to%20bypass%20guardrails%20and%20uncover%20safety%20implications.%0AExisting%20jailbreaks%20predominantly%20focus%20on%20the%20visual%20modality%2C%20perturbing%0Asolely%20visual%20inputs%20in%20the%20prompt%20for%20attacks.%20However%2C%20they%20fall%20short%20when%0Aconfronted%20with%20aligned%20models%20that%20fuse%20visual%20and%20textual%20features%0Asimultaneously%20for%20generation.%20To%20address%20this%20limitation%2C%20this%20paper%0Aintroduces%20the%20Bi-Modal%20Adversarial%20Prompt%20Attack%20%28BAP%29%2C%20which%20executes%0Ajailbreaks%20by%20optimizing%20textual%20and%20visual%20prompts%20cohesively.%20Initially%2C%20we%0Aadversarially%20embed%20universally%20harmful%20perturbations%20in%20an%20image%2C%20guided%20by%20a%0Afew-shot%20query-agnostic%20corpus%20%28e.g.%2C%20affirmative%20prefixes%20and%20negative%0Ainhibitions%29.%20This%20process%20ensures%20that%20image%20prompt%20LVLMs%20to%20respond%0Apositively%20to%20any%20harmful%20queries.%20Subsequently%2C%20leveraging%20the%20adversarial%0Aimage%2C%20we%20optimize%20textual%20prompts%20with%20specific%20harmful%20intent.%20In%20particular%2C%0Awe%20utilize%20a%20large%20language%20model%20to%20analyze%20jailbreak%20failures%20and%20employ%0Achain-of-thought%20reasoning%20to%20refine%20textual%20prompts%20through%20a%0Afeedback-iteration%20manner.%20To%20validate%20the%20efficacy%20of%20our%20approach%2C%20we%0Aconducted%20extensive%20evaluations%20on%20various%20datasets%20and%20LVLMs%2C%20demonstrating%0Athat%20our%20method%20significantly%20outperforms%20other%20methods%20by%20large%20margins%0A%28%2B29.03%25%20in%20attack%20success%20rate%20on%20average%29.%20Additionally%2C%20we%20showcase%20the%0Apotential%20of%20our%20attacks%20on%20black-box%20commercial%20LVLMs%2C%20such%20as%20Gemini%20and%0AChatGLM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.04031v2&entry.124074799=Read"},
{"title": "Towards objective and systematic evaluation of bias in artificial\n  intelligence for medical imaging", "author": "Emma A. M. Stanley and Raissa Souza and Anthony Winder and Vedant Gulve and Kimberly Amador and Matthias Wilms and Nils D. Forkert", "abstract": "  Artificial intelligence (AI) models trained using medical images for clinical\ntasks often exhibit bias in the form of disparities in performance between\nsubgroups. Since not all sources of biases in real-world medical imaging data\nare easily identifiable, it is challenging to comprehensively assess how those\nbiases are encoded in models, and how capable bias mitigation methods are at\nameliorating performance disparities. In this article, we introduce a novel\nanalysis framework for systematically and objectively investigating the impact\nof biases in medical images on AI models. We developed and tested this\nframework for conducting controlled in silico trials to assess bias in medical\nimaging AI using a tool for generating synthetic magnetic resonance images with\nknown disease effects and sources of bias. The feasibility is showcased by\nusing three counterfactual bias scenarios to measure the impact of simulated\nbias effects on a convolutional neural network (CNN) classifier and the\nefficacy of three bias mitigation strategies. The analysis revealed that the\nsimulated biases resulted in expected subgroup performance disparities when the\nCNN was trained on the synthetic datasets. Moreover, reweighing was identified\nas the most successful bias mitigation strategy for this setup, and we\ndemonstrated how explainable AI methods can aid in investigating the\nmanifestation of bias in the model using this framework. Developing fair AI\nmodels is a considerable challenge given that many and often unknown sources of\nbiases can be present in medical imaging datasets. In this work, we present a\nnovel methodology to objectively study the impact of biases and mitigation\nstrategies on deep learning pipelines, which can support the development of\nclinical AI that is robust and responsible.\n", "link": "http://arxiv.org/abs/2311.02115v2", "date": "2024-07-01", "relevancy": 1.451, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5164}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4808}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20objective%20and%20systematic%20evaluation%20of%20bias%20in%20artificial%0A%20%20intelligence%20for%20medical%20imaging&body=Title%3A%20Towards%20objective%20and%20systematic%20evaluation%20of%20bias%20in%20artificial%0A%20%20intelligence%20for%20medical%20imaging%0AAuthor%3A%20Emma%20A.%20M.%20Stanley%20and%20Raissa%20Souza%20and%20Anthony%20Winder%20and%20Vedant%20Gulve%20and%20Kimberly%20Amador%20and%20Matthias%20Wilms%20and%20Nils%20D.%20Forkert%0AAbstract%3A%20%20%20Artificial%20intelligence%20%28AI%29%20models%20trained%20using%20medical%20images%20for%20clinical%0Atasks%20often%20exhibit%20bias%20in%20the%20form%20of%20disparities%20in%20performance%20between%0Asubgroups.%20Since%20not%20all%20sources%20of%20biases%20in%20real-world%20medical%20imaging%20data%0Aare%20easily%20identifiable%2C%20it%20is%20challenging%20to%20comprehensively%20assess%20how%20those%0Abiases%20are%20encoded%20in%20models%2C%20and%20how%20capable%20bias%20mitigation%20methods%20are%20at%0Aameliorating%20performance%20disparities.%20In%20this%20article%2C%20we%20introduce%20a%20novel%0Aanalysis%20framework%20for%20systematically%20and%20objectively%20investigating%20the%20impact%0Aof%20biases%20in%20medical%20images%20on%20AI%20models.%20We%20developed%20and%20tested%20this%0Aframework%20for%20conducting%20controlled%20in%20silico%20trials%20to%20assess%20bias%20in%20medical%0Aimaging%20AI%20using%20a%20tool%20for%20generating%20synthetic%20magnetic%20resonance%20images%20with%0Aknown%20disease%20effects%20and%20sources%20of%20bias.%20The%20feasibility%20is%20showcased%20by%0Ausing%20three%20counterfactual%20bias%20scenarios%20to%20measure%20the%20impact%20of%20simulated%0Abias%20effects%20on%20a%20convolutional%20neural%20network%20%28CNN%29%20classifier%20and%20the%0Aefficacy%20of%20three%20bias%20mitigation%20strategies.%20The%20analysis%20revealed%20that%20the%0Asimulated%20biases%20resulted%20in%20expected%20subgroup%20performance%20disparities%20when%20the%0ACNN%20was%20trained%20on%20the%20synthetic%20datasets.%20Moreover%2C%20reweighing%20was%20identified%0Aas%20the%20most%20successful%20bias%20mitigation%20strategy%20for%20this%20setup%2C%20and%20we%0Ademonstrated%20how%20explainable%20AI%20methods%20can%20aid%20in%20investigating%20the%0Amanifestation%20of%20bias%20in%20the%20model%20using%20this%20framework.%20Developing%20fair%20AI%0Amodels%20is%20a%20considerable%20challenge%20given%20that%20many%20and%20often%20unknown%20sources%20of%0Abiases%20can%20be%20present%20in%20medical%20imaging%20datasets.%20In%20this%20work%2C%20we%20present%20a%0Anovel%20methodology%20to%20objectively%20study%20the%20impact%20of%20biases%20and%20mitigation%0Astrategies%20on%20deep%20learning%20pipelines%2C%20which%20can%20support%20the%20development%20of%0Aclinical%20AI%20that%20is%20robust%20and%20responsible.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.02115v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520objective%2520and%2520systematic%2520evaluation%2520of%2520bias%2520in%2520artificial%250A%2520%2520intelligence%2520for%2520medical%2520imaging%26entry.906535625%3DEmma%2520A.%2520M.%2520Stanley%2520and%2520Raissa%2520Souza%2520and%2520Anthony%2520Winder%2520and%2520Vedant%2520Gulve%2520and%2520Kimberly%2520Amador%2520and%2520Matthias%2520Wilms%2520and%2520Nils%2520D.%2520Forkert%26entry.1292438233%3D%2520%2520Artificial%2520intelligence%2520%2528AI%2529%2520models%2520trained%2520using%2520medical%2520images%2520for%2520clinical%250Atasks%2520often%2520exhibit%2520bias%2520in%2520the%2520form%2520of%2520disparities%2520in%2520performance%2520between%250Asubgroups.%2520Since%2520not%2520all%2520sources%2520of%2520biases%2520in%2520real-world%2520medical%2520imaging%2520data%250Aare%2520easily%2520identifiable%252C%2520it%2520is%2520challenging%2520to%2520comprehensively%2520assess%2520how%2520those%250Abiases%2520are%2520encoded%2520in%2520models%252C%2520and%2520how%2520capable%2520bias%2520mitigation%2520methods%2520are%2520at%250Aameliorating%2520performance%2520disparities.%2520In%2520this%2520article%252C%2520we%2520introduce%2520a%2520novel%250Aanalysis%2520framework%2520for%2520systematically%2520and%2520objectively%2520investigating%2520the%2520impact%250Aof%2520biases%2520in%2520medical%2520images%2520on%2520AI%2520models.%2520We%2520developed%2520and%2520tested%2520this%250Aframework%2520for%2520conducting%2520controlled%2520in%2520silico%2520trials%2520to%2520assess%2520bias%2520in%2520medical%250Aimaging%2520AI%2520using%2520a%2520tool%2520for%2520generating%2520synthetic%2520magnetic%2520resonance%2520images%2520with%250Aknown%2520disease%2520effects%2520and%2520sources%2520of%2520bias.%2520The%2520feasibility%2520is%2520showcased%2520by%250Ausing%2520three%2520counterfactual%2520bias%2520scenarios%2520to%2520measure%2520the%2520impact%2520of%2520simulated%250Abias%2520effects%2520on%2520a%2520convolutional%2520neural%2520network%2520%2528CNN%2529%2520classifier%2520and%2520the%250Aefficacy%2520of%2520three%2520bias%2520mitigation%2520strategies.%2520The%2520analysis%2520revealed%2520that%2520the%250Asimulated%2520biases%2520resulted%2520in%2520expected%2520subgroup%2520performance%2520disparities%2520when%2520the%250ACNN%2520was%2520trained%2520on%2520the%2520synthetic%2520datasets.%2520Moreover%252C%2520reweighing%2520was%2520identified%250Aas%2520the%2520most%2520successful%2520bias%2520mitigation%2520strategy%2520for%2520this%2520setup%252C%2520and%2520we%250Ademonstrated%2520how%2520explainable%2520AI%2520methods%2520can%2520aid%2520in%2520investigating%2520the%250Amanifestation%2520of%2520bias%2520in%2520the%2520model%2520using%2520this%2520framework.%2520Developing%2520fair%2520AI%250Amodels%2520is%2520a%2520considerable%2520challenge%2520given%2520that%2520many%2520and%2520often%2520unknown%2520sources%2520of%250Abiases%2520can%2520be%2520present%2520in%2520medical%2520imaging%2520datasets.%2520In%2520this%2520work%252C%2520we%2520present%2520a%250Anovel%2520methodology%2520to%2520objectively%2520study%2520the%2520impact%2520of%2520biases%2520and%2520mitigation%250Astrategies%2520on%2520deep%2520learning%2520pipelines%252C%2520which%2520can%2520support%2520the%2520development%2520of%250Aclinical%2520AI%2520that%2520is%2520robust%2520and%2520responsible.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.02115v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20objective%20and%20systematic%20evaluation%20of%20bias%20in%20artificial%0A%20%20intelligence%20for%20medical%20imaging&entry.906535625=Emma%20A.%20M.%20Stanley%20and%20Raissa%20Souza%20and%20Anthony%20Winder%20and%20Vedant%20Gulve%20and%20Kimberly%20Amador%20and%20Matthias%20Wilms%20and%20Nils%20D.%20Forkert&entry.1292438233=%20%20Artificial%20intelligence%20%28AI%29%20models%20trained%20using%20medical%20images%20for%20clinical%0Atasks%20often%20exhibit%20bias%20in%20the%20form%20of%20disparities%20in%20performance%20between%0Asubgroups.%20Since%20not%20all%20sources%20of%20biases%20in%20real-world%20medical%20imaging%20data%0Aare%20easily%20identifiable%2C%20it%20is%20challenging%20to%20comprehensively%20assess%20how%20those%0Abiases%20are%20encoded%20in%20models%2C%20and%20how%20capable%20bias%20mitigation%20methods%20are%20at%0Aameliorating%20performance%20disparities.%20In%20this%20article%2C%20we%20introduce%20a%20novel%0Aanalysis%20framework%20for%20systematically%20and%20objectively%20investigating%20the%20impact%0Aof%20biases%20in%20medical%20images%20on%20AI%20models.%20We%20developed%20and%20tested%20this%0Aframework%20for%20conducting%20controlled%20in%20silico%20trials%20to%20assess%20bias%20in%20medical%0Aimaging%20AI%20using%20a%20tool%20for%20generating%20synthetic%20magnetic%20resonance%20images%20with%0Aknown%20disease%20effects%20and%20sources%20of%20bias.%20The%20feasibility%20is%20showcased%20by%0Ausing%20three%20counterfactual%20bias%20scenarios%20to%20measure%20the%20impact%20of%20simulated%0Abias%20effects%20on%20a%20convolutional%20neural%20network%20%28CNN%29%20classifier%20and%20the%0Aefficacy%20of%20three%20bias%20mitigation%20strategies.%20The%20analysis%20revealed%20that%20the%0Asimulated%20biases%20resulted%20in%20expected%20subgroup%20performance%20disparities%20when%20the%0ACNN%20was%20trained%20on%20the%20synthetic%20datasets.%20Moreover%2C%20reweighing%20was%20identified%0Aas%20the%20most%20successful%20bias%20mitigation%20strategy%20for%20this%20setup%2C%20and%20we%0Ademonstrated%20how%20explainable%20AI%20methods%20can%20aid%20in%20investigating%20the%0Amanifestation%20of%20bias%20in%20the%20model%20using%20this%20framework.%20Developing%20fair%20AI%0Amodels%20is%20a%20considerable%20challenge%20given%20that%20many%20and%20often%20unknown%20sources%20of%0Abiases%20can%20be%20present%20in%20medical%20imaging%20datasets.%20In%20this%20work%2C%20we%20present%20a%0Anovel%20methodology%20to%20objectively%20study%20the%20impact%20of%20biases%20and%20mitigation%0Astrategies%20on%20deep%20learning%20pipelines%2C%20which%20can%20support%20the%20development%20of%0Aclinical%20AI%20that%20is%20robust%20and%20responsible.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.02115v2&entry.124074799=Read"},
{"title": "Proceedings of The second international workshop on eXplainable AI for\n  the Arts (XAIxArts)", "author": "Nick Bryan-Kinns and Corey Ford and Shuoyang Zheng and Helen Kennedy and Alan Chamberlain and Makayla Lewis and Drew Hemment and Zijin Li and Qiong Wu and Lanxi Xiao and Gus Xia and Jeba Rezwana and Michael Clemens and Gabriel Vigliensoni", "abstract": "  This second international workshop on explainable AI for the Arts (XAIxArts)\nbrought together a community of researchers in HCI, Interaction Design, AI,\nexplainable AI (XAI), and digital arts to explore the role of XAI for the Arts.\nWorkshop held at the 16th ACM Conference on Creativity and Cognition (C&C\n2024), Chicago, USA.\n", "link": "http://arxiv.org/abs/2406.14485v3", "date": "2024-07-01", "relevancy": 1.4246, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4239}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3429}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.3424}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proceedings%20of%20The%20second%20international%20workshop%20on%20eXplainable%20AI%20for%0A%20%20the%20Arts%20%28XAIxArts%29&body=Title%3A%20Proceedings%20of%20The%20second%20international%20workshop%20on%20eXplainable%20AI%20for%0A%20%20the%20Arts%20%28XAIxArts%29%0AAuthor%3A%20Nick%20Bryan-Kinns%20and%20Corey%20Ford%20and%20Shuoyang%20Zheng%20and%20Helen%20Kennedy%20and%20Alan%20Chamberlain%20and%20Makayla%20Lewis%20and%20Drew%20Hemment%20and%20Zijin%20Li%20and%20Qiong%20Wu%20and%20Lanxi%20Xiao%20and%20Gus%20Xia%20and%20Jeba%20Rezwana%20and%20Michael%20Clemens%20and%20Gabriel%20Vigliensoni%0AAbstract%3A%20%20%20This%20second%20international%20workshop%20on%20explainable%20AI%20for%20the%20Arts%20%28XAIxArts%29%0Abrought%20together%20a%20community%20of%20researchers%20in%20HCI%2C%20Interaction%20Design%2C%20AI%2C%0Aexplainable%20AI%20%28XAI%29%2C%20and%20digital%20arts%20to%20explore%20the%20role%20of%20XAI%20for%20the%20Arts.%0AWorkshop%20held%20at%20the%2016th%20ACM%20Conference%20on%20Creativity%20and%20Cognition%20%28C%26C%0A2024%29%2C%20Chicago%2C%20USA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14485v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProceedings%2520of%2520The%2520second%2520international%2520workshop%2520on%2520eXplainable%2520AI%2520for%250A%2520%2520the%2520Arts%2520%2528XAIxArts%2529%26entry.906535625%3DNick%2520Bryan-Kinns%2520and%2520Corey%2520Ford%2520and%2520Shuoyang%2520Zheng%2520and%2520Helen%2520Kennedy%2520and%2520Alan%2520Chamberlain%2520and%2520Makayla%2520Lewis%2520and%2520Drew%2520Hemment%2520and%2520Zijin%2520Li%2520and%2520Qiong%2520Wu%2520and%2520Lanxi%2520Xiao%2520and%2520Gus%2520Xia%2520and%2520Jeba%2520Rezwana%2520and%2520Michael%2520Clemens%2520and%2520Gabriel%2520Vigliensoni%26entry.1292438233%3D%2520%2520This%2520second%2520international%2520workshop%2520on%2520explainable%2520AI%2520for%2520the%2520Arts%2520%2528XAIxArts%2529%250Abrought%2520together%2520a%2520community%2520of%2520researchers%2520in%2520HCI%252C%2520Interaction%2520Design%252C%2520AI%252C%250Aexplainable%2520AI%2520%2528XAI%2529%252C%2520and%2520digital%2520arts%2520to%2520explore%2520the%2520role%2520of%2520XAI%2520for%2520the%2520Arts.%250AWorkshop%2520held%2520at%2520the%252016th%2520ACM%2520Conference%2520on%2520Creativity%2520and%2520Cognition%2520%2528C%2526C%250A2024%2529%252C%2520Chicago%252C%2520USA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14485v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proceedings%20of%20The%20second%20international%20workshop%20on%20eXplainable%20AI%20for%0A%20%20the%20Arts%20%28XAIxArts%29&entry.906535625=Nick%20Bryan-Kinns%20and%20Corey%20Ford%20and%20Shuoyang%20Zheng%20and%20Helen%20Kennedy%20and%20Alan%20Chamberlain%20and%20Makayla%20Lewis%20and%20Drew%20Hemment%20and%20Zijin%20Li%20and%20Qiong%20Wu%20and%20Lanxi%20Xiao%20and%20Gus%20Xia%20and%20Jeba%20Rezwana%20and%20Michael%20Clemens%20and%20Gabriel%20Vigliensoni&entry.1292438233=%20%20This%20second%20international%20workshop%20on%20explainable%20AI%20for%20the%20Arts%20%28XAIxArts%29%0Abrought%20together%20a%20community%20of%20researchers%20in%20HCI%2C%20Interaction%20Design%2C%20AI%2C%0Aexplainable%20AI%20%28XAI%29%2C%20and%20digital%20arts%20to%20explore%20the%20role%20of%20XAI%20for%20the%20Arts.%0AWorkshop%20held%20at%20the%2016th%20ACM%20Conference%20on%20Creativity%20and%20Cognition%20%28C%26C%0A2024%29%2C%20Chicago%2C%20USA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14485v3&entry.124074799=Read"},
{"title": "Large Language Models Assume People are More Rational than We Really are", "author": "Ryan Liu and Jiayi Geng and Joshua C. Peterson and Ilia Sucholutsky and Thomas L. Griffiths", "abstract": "  In order for AI systems to communicate effectively with people, they must\nunderstand how we make decisions. However, people's decisions are not always\nrational, so the implicit internal models of human decision-making in Large\nLanguage Models (LLMs) must account for this. Previous empirical evidence seems\nto suggest that these implicit models are accurate -- LLMs offer believable\nproxies of human behavior, acting how we expect humans would in everyday\ninteractions. However, by comparing LLM behavior and predictions to a large\ndataset of human decisions, we find that this is actually not the case: when\nboth simulating and predicting people's choices, a suite of cutting-edge LLMs\n(GPT-4o & 4-Turbo, Llama-3-8B & 70B, Claude 3 Opus) assume that people are more\nrational than we really are. Specifically, these models deviate from human\nbehavior and align more closely with a classic model of rational choice --\nexpected value theory. Interestingly, people also tend to assume that other\npeople are rational when interpreting their behavior. As a consequence, when we\ncompare the inferences that LLMs and people draw from the decisions of others\nusing another psychological dataset, we find that these inferences are highly\ncorrelated. Thus, the implicit decision-making models of LLMs appear to be\naligned with the human expectation that other people will act rationally,\nrather than with how people actually act.\n", "link": "http://arxiv.org/abs/2406.17055v2", "date": "2024-07-01", "relevancy": 1.4131, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4767}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4689}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Assume%20People%20are%20More%20Rational%20than%20We%20Really%20are&body=Title%3A%20Large%20Language%20Models%20Assume%20People%20are%20More%20Rational%20than%20We%20Really%20are%0AAuthor%3A%20Ryan%20Liu%20and%20Jiayi%20Geng%20and%20Joshua%20C.%20Peterson%20and%20Ilia%20Sucholutsky%20and%20Thomas%20L.%20Griffiths%0AAbstract%3A%20%20%20In%20order%20for%20AI%20systems%20to%20communicate%20effectively%20with%20people%2C%20they%20must%0Aunderstand%20how%20we%20make%20decisions.%20However%2C%20people%27s%20decisions%20are%20not%20always%0Arational%2C%20so%20the%20implicit%20internal%20models%20of%20human%20decision-making%20in%20Large%0ALanguage%20Models%20%28LLMs%29%20must%20account%20for%20this.%20Previous%20empirical%20evidence%20seems%0Ato%20suggest%20that%20these%20implicit%20models%20are%20accurate%20--%20LLMs%20offer%20believable%0Aproxies%20of%20human%20behavior%2C%20acting%20how%20we%20expect%20humans%20would%20in%20everyday%0Ainteractions.%20However%2C%20by%20comparing%20LLM%20behavior%20and%20predictions%20to%20a%20large%0Adataset%20of%20human%20decisions%2C%20we%20find%20that%20this%20is%20actually%20not%20the%20case%3A%20when%0Aboth%20simulating%20and%20predicting%20people%27s%20choices%2C%20a%20suite%20of%20cutting-edge%20LLMs%0A%28GPT-4o%20%26%204-Turbo%2C%20Llama-3-8B%20%26%2070B%2C%20Claude%203%20Opus%29%20assume%20that%20people%20are%20more%0Arational%20than%20we%20really%20are.%20Specifically%2C%20these%20models%20deviate%20from%20human%0Abehavior%20and%20align%20more%20closely%20with%20a%20classic%20model%20of%20rational%20choice%20--%0Aexpected%20value%20theory.%20Interestingly%2C%20people%20also%20tend%20to%20assume%20that%20other%0Apeople%20are%20rational%20when%20interpreting%20their%20behavior.%20As%20a%20consequence%2C%20when%20we%0Acompare%20the%20inferences%20that%20LLMs%20and%20people%20draw%20from%20the%20decisions%20of%20others%0Ausing%20another%20psychological%20dataset%2C%20we%20find%20that%20these%20inferences%20are%20highly%0Acorrelated.%20Thus%2C%20the%20implicit%20decision-making%20models%20of%20LLMs%20appear%20to%20be%0Aaligned%20with%20the%20human%20expectation%20that%20other%20people%20will%20act%20rationally%2C%0Arather%20than%20with%20how%20people%20actually%20act.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.17055v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Assume%2520People%2520are%2520More%2520Rational%2520than%2520We%2520Really%2520are%26entry.906535625%3DRyan%2520Liu%2520and%2520Jiayi%2520Geng%2520and%2520Joshua%2520C.%2520Peterson%2520and%2520Ilia%2520Sucholutsky%2520and%2520Thomas%2520L.%2520Griffiths%26entry.1292438233%3D%2520%2520In%2520order%2520for%2520AI%2520systems%2520to%2520communicate%2520effectively%2520with%2520people%252C%2520they%2520must%250Aunderstand%2520how%2520we%2520make%2520decisions.%2520However%252C%2520people%2527s%2520decisions%2520are%2520not%2520always%250Arational%252C%2520so%2520the%2520implicit%2520internal%2520models%2520of%2520human%2520decision-making%2520in%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520must%2520account%2520for%2520this.%2520Previous%2520empirical%2520evidence%2520seems%250Ato%2520suggest%2520that%2520these%2520implicit%2520models%2520are%2520accurate%2520--%2520LLMs%2520offer%2520believable%250Aproxies%2520of%2520human%2520behavior%252C%2520acting%2520how%2520we%2520expect%2520humans%2520would%2520in%2520everyday%250Ainteractions.%2520However%252C%2520by%2520comparing%2520LLM%2520behavior%2520and%2520predictions%2520to%2520a%2520large%250Adataset%2520of%2520human%2520decisions%252C%2520we%2520find%2520that%2520this%2520is%2520actually%2520not%2520the%2520case%253A%2520when%250Aboth%2520simulating%2520and%2520predicting%2520people%2527s%2520choices%252C%2520a%2520suite%2520of%2520cutting-edge%2520LLMs%250A%2528GPT-4o%2520%2526%25204-Turbo%252C%2520Llama-3-8B%2520%2526%252070B%252C%2520Claude%25203%2520Opus%2529%2520assume%2520that%2520people%2520are%2520more%250Arational%2520than%2520we%2520really%2520are.%2520Specifically%252C%2520these%2520models%2520deviate%2520from%2520human%250Abehavior%2520and%2520align%2520more%2520closely%2520with%2520a%2520classic%2520model%2520of%2520rational%2520choice%2520--%250Aexpected%2520value%2520theory.%2520Interestingly%252C%2520people%2520also%2520tend%2520to%2520assume%2520that%2520other%250Apeople%2520are%2520rational%2520when%2520interpreting%2520their%2520behavior.%2520As%2520a%2520consequence%252C%2520when%2520we%250Acompare%2520the%2520inferences%2520that%2520LLMs%2520and%2520people%2520draw%2520from%2520the%2520decisions%2520of%2520others%250Ausing%2520another%2520psychological%2520dataset%252C%2520we%2520find%2520that%2520these%2520inferences%2520are%2520highly%250Acorrelated.%2520Thus%252C%2520the%2520implicit%2520decision-making%2520models%2520of%2520LLMs%2520appear%2520to%2520be%250Aaligned%2520with%2520the%2520human%2520expectation%2520that%2520other%2520people%2520will%2520act%2520rationally%252C%250Arather%2520than%2520with%2520how%2520people%2520actually%2520act.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.17055v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Assume%20People%20are%20More%20Rational%20than%20We%20Really%20are&entry.906535625=Ryan%20Liu%20and%20Jiayi%20Geng%20and%20Joshua%20C.%20Peterson%20and%20Ilia%20Sucholutsky%20and%20Thomas%20L.%20Griffiths&entry.1292438233=%20%20In%20order%20for%20AI%20systems%20to%20communicate%20effectively%20with%20people%2C%20they%20must%0Aunderstand%20how%20we%20make%20decisions.%20However%2C%20people%27s%20decisions%20are%20not%20always%0Arational%2C%20so%20the%20implicit%20internal%20models%20of%20human%20decision-making%20in%20Large%0ALanguage%20Models%20%28LLMs%29%20must%20account%20for%20this.%20Previous%20empirical%20evidence%20seems%0Ato%20suggest%20that%20these%20implicit%20models%20are%20accurate%20--%20LLMs%20offer%20believable%0Aproxies%20of%20human%20behavior%2C%20acting%20how%20we%20expect%20humans%20would%20in%20everyday%0Ainteractions.%20However%2C%20by%20comparing%20LLM%20behavior%20and%20predictions%20to%20a%20large%0Adataset%20of%20human%20decisions%2C%20we%20find%20that%20this%20is%20actually%20not%20the%20case%3A%20when%0Aboth%20simulating%20and%20predicting%20people%27s%20choices%2C%20a%20suite%20of%20cutting-edge%20LLMs%0A%28GPT-4o%20%26%204-Turbo%2C%20Llama-3-8B%20%26%2070B%2C%20Claude%203%20Opus%29%20assume%20that%20people%20are%20more%0Arational%20than%20we%20really%20are.%20Specifically%2C%20these%20models%20deviate%20from%20human%0Abehavior%20and%20align%20more%20closely%20with%20a%20classic%20model%20of%20rational%20choice%20--%0Aexpected%20value%20theory.%20Interestingly%2C%20people%20also%20tend%20to%20assume%20that%20other%0Apeople%20are%20rational%20when%20interpreting%20their%20behavior.%20As%20a%20consequence%2C%20when%20we%0Acompare%20the%20inferences%20that%20LLMs%20and%20people%20draw%20from%20the%20decisions%20of%20others%0Ausing%20another%20psychological%20dataset%2C%20we%20find%20that%20these%20inferences%20are%20highly%0Acorrelated.%20Thus%2C%20the%20implicit%20decision-making%20models%20of%20LLMs%20appear%20to%20be%0Aaligned%20with%20the%20human%20expectation%20that%20other%20people%20will%20act%20rationally%2C%0Arather%20than%20with%20how%20people%20actually%20act.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.17055v2&entry.124074799=Read"},
{"title": "Predicting Fairness of ML Software Configurations", "author": "Salvador Robles Herrera and Verya Monjezi and Vladik Kreinovich and Ashutosh Trivedi and Saeid Tizpaz-Niari", "abstract": "  This paper investigates the relationships between hyperparameters of machine\nlearning and fairness. Data-driven solutions are increasingly used in critical\nsocio-technical applications where ensuring fairness is important. Rather than\nexplicitly encoding decision logic via control and data structures, the ML\ndevelopers provide input data, perform some pre-processing, choose ML\nalgorithms, and tune hyperparameters (HPs) to infer a program that encodes the\ndecision logic. Prior works report that the selection of HPs can significantly\ninfluence fairness. However, tuning HPs to find an ideal trade-off between\naccuracy, precision, and fairness has remained an expensive and tedious task.\nCan we predict fairness of HP configuration for a given dataset? Are the\npredictions robust to distribution shifts?\n  We focus on group fairness notions and investigate the HP space of 5 training\nalgorithms. We first find that tree regressors and XGBoots significantly\noutperformed deep neural networks and support vector machines in accurately\npredicting the fairness of HPs. When predicting the fairness of ML\nhyperparameters under temporal distribution shift, the tree regressors\noutperforms the other algorithms with reasonable accuracy. However, the\nprecision depends on the ML training algorithm, dataset, and protected\nattributes. For example, the tree regressor model was robust for training data\nshift from 2014 to 2018 on logistic regression and discriminant analysis HPs\nwith sex as the protected attribute; but not for race and other training\nalgorithms. Our method provides a sound framework to efficiently perform\nfine-tuning of ML training algorithms and understand the relationships between\nHPs and fairness.\n", "link": "http://arxiv.org/abs/2404.19100v2", "date": "2024-07-01", "relevancy": 1.3326, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4676}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4479}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4117}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Predicting%20Fairness%20of%20ML%20Software%20Configurations&body=Title%3A%20Predicting%20Fairness%20of%20ML%20Software%20Configurations%0AAuthor%3A%20Salvador%20Robles%20Herrera%20and%20Verya%20Monjezi%20and%20Vladik%20Kreinovich%20and%20Ashutosh%20Trivedi%20and%20Saeid%20Tizpaz-Niari%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20relationships%20between%20hyperparameters%20of%20machine%0Alearning%20and%20fairness.%20Data-driven%20solutions%20are%20increasingly%20used%20in%20critical%0Asocio-technical%20applications%20where%20ensuring%20fairness%20is%20important.%20Rather%20than%0Aexplicitly%20encoding%20decision%20logic%20via%20control%20and%20data%20structures%2C%20the%20ML%0Adevelopers%20provide%20input%20data%2C%20perform%20some%20pre-processing%2C%20choose%20ML%0Aalgorithms%2C%20and%20tune%20hyperparameters%20%28HPs%29%20to%20infer%20a%20program%20that%20encodes%20the%0Adecision%20logic.%20Prior%20works%20report%20that%20the%20selection%20of%20HPs%20can%20significantly%0Ainfluence%20fairness.%20However%2C%20tuning%20HPs%20to%20find%20an%20ideal%20trade-off%20between%0Aaccuracy%2C%20precision%2C%20and%20fairness%20has%20remained%20an%20expensive%20and%20tedious%20task.%0ACan%20we%20predict%20fairness%20of%20HP%20configuration%20for%20a%20given%20dataset%3F%20Are%20the%0Apredictions%20robust%20to%20distribution%20shifts%3F%0A%20%20We%20focus%20on%20group%20fairness%20notions%20and%20investigate%20the%20HP%20space%20of%205%20training%0Aalgorithms.%20We%20first%20find%20that%20tree%20regressors%20and%20XGBoots%20significantly%0Aoutperformed%20deep%20neural%20networks%20and%20support%20vector%20machines%20in%20accurately%0Apredicting%20the%20fairness%20of%20HPs.%20When%20predicting%20the%20fairness%20of%20ML%0Ahyperparameters%20under%20temporal%20distribution%20shift%2C%20the%20tree%20regressors%0Aoutperforms%20the%20other%20algorithms%20with%20reasonable%20accuracy.%20However%2C%20the%0Aprecision%20depends%20on%20the%20ML%20training%20algorithm%2C%20dataset%2C%20and%20protected%0Aattributes.%20For%20example%2C%20the%20tree%20regressor%20model%20was%20robust%20for%20training%20data%0Ashift%20from%202014%20to%202018%20on%20logistic%20regression%20and%20discriminant%20analysis%20HPs%0Awith%20sex%20as%20the%20protected%20attribute%3B%20but%20not%20for%20race%20and%20other%20training%0Aalgorithms.%20Our%20method%20provides%20a%20sound%20framework%20to%20efficiently%20perform%0Afine-tuning%20of%20ML%20training%20algorithms%20and%20understand%20the%20relationships%20between%0AHPs%20and%20fairness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19100v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPredicting%2520Fairness%2520of%2520ML%2520Software%2520Configurations%26entry.906535625%3DSalvador%2520Robles%2520Herrera%2520and%2520Verya%2520Monjezi%2520and%2520Vladik%2520Kreinovich%2520and%2520Ashutosh%2520Trivedi%2520and%2520Saeid%2520Tizpaz-Niari%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520relationships%2520between%2520hyperparameters%2520of%2520machine%250Alearning%2520and%2520fairness.%2520Data-driven%2520solutions%2520are%2520increasingly%2520used%2520in%2520critical%250Asocio-technical%2520applications%2520where%2520ensuring%2520fairness%2520is%2520important.%2520Rather%2520than%250Aexplicitly%2520encoding%2520decision%2520logic%2520via%2520control%2520and%2520data%2520structures%252C%2520the%2520ML%250Adevelopers%2520provide%2520input%2520data%252C%2520perform%2520some%2520pre-processing%252C%2520choose%2520ML%250Aalgorithms%252C%2520and%2520tune%2520hyperparameters%2520%2528HPs%2529%2520to%2520infer%2520a%2520program%2520that%2520encodes%2520the%250Adecision%2520logic.%2520Prior%2520works%2520report%2520that%2520the%2520selection%2520of%2520HPs%2520can%2520significantly%250Ainfluence%2520fairness.%2520However%252C%2520tuning%2520HPs%2520to%2520find%2520an%2520ideal%2520trade-off%2520between%250Aaccuracy%252C%2520precision%252C%2520and%2520fairness%2520has%2520remained%2520an%2520expensive%2520and%2520tedious%2520task.%250ACan%2520we%2520predict%2520fairness%2520of%2520HP%2520configuration%2520for%2520a%2520given%2520dataset%253F%2520Are%2520the%250Apredictions%2520robust%2520to%2520distribution%2520shifts%253F%250A%2520%2520We%2520focus%2520on%2520group%2520fairness%2520notions%2520and%2520investigate%2520the%2520HP%2520space%2520of%25205%2520training%250Aalgorithms.%2520We%2520first%2520find%2520that%2520tree%2520regressors%2520and%2520XGBoots%2520significantly%250Aoutperformed%2520deep%2520neural%2520networks%2520and%2520support%2520vector%2520machines%2520in%2520accurately%250Apredicting%2520the%2520fairness%2520of%2520HPs.%2520When%2520predicting%2520the%2520fairness%2520of%2520ML%250Ahyperparameters%2520under%2520temporal%2520distribution%2520shift%252C%2520the%2520tree%2520regressors%250Aoutperforms%2520the%2520other%2520algorithms%2520with%2520reasonable%2520accuracy.%2520However%252C%2520the%250Aprecision%2520depends%2520on%2520the%2520ML%2520training%2520algorithm%252C%2520dataset%252C%2520and%2520protected%250Aattributes.%2520For%2520example%252C%2520the%2520tree%2520regressor%2520model%2520was%2520robust%2520for%2520training%2520data%250Ashift%2520from%25202014%2520to%25202018%2520on%2520logistic%2520regression%2520and%2520discriminant%2520analysis%2520HPs%250Awith%2520sex%2520as%2520the%2520protected%2520attribute%253B%2520but%2520not%2520for%2520race%2520and%2520other%2520training%250Aalgorithms.%2520Our%2520method%2520provides%2520a%2520sound%2520framework%2520to%2520efficiently%2520perform%250Afine-tuning%2520of%2520ML%2520training%2520algorithms%2520and%2520understand%2520the%2520relationships%2520between%250AHPs%2520and%2520fairness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19100v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Predicting%20Fairness%20of%20ML%20Software%20Configurations&entry.906535625=Salvador%20Robles%20Herrera%20and%20Verya%20Monjezi%20and%20Vladik%20Kreinovich%20and%20Ashutosh%20Trivedi%20and%20Saeid%20Tizpaz-Niari&entry.1292438233=%20%20This%20paper%20investigates%20the%20relationships%20between%20hyperparameters%20of%20machine%0Alearning%20and%20fairness.%20Data-driven%20solutions%20are%20increasingly%20used%20in%20critical%0Asocio-technical%20applications%20where%20ensuring%20fairness%20is%20important.%20Rather%20than%0Aexplicitly%20encoding%20decision%20logic%20via%20control%20and%20data%20structures%2C%20the%20ML%0Adevelopers%20provide%20input%20data%2C%20perform%20some%20pre-processing%2C%20choose%20ML%0Aalgorithms%2C%20and%20tune%20hyperparameters%20%28HPs%29%20to%20infer%20a%20program%20that%20encodes%20the%0Adecision%20logic.%20Prior%20works%20report%20that%20the%20selection%20of%20HPs%20can%20significantly%0Ainfluence%20fairness.%20However%2C%20tuning%20HPs%20to%20find%20an%20ideal%20trade-off%20between%0Aaccuracy%2C%20precision%2C%20and%20fairness%20has%20remained%20an%20expensive%20and%20tedious%20task.%0ACan%20we%20predict%20fairness%20of%20HP%20configuration%20for%20a%20given%20dataset%3F%20Are%20the%0Apredictions%20robust%20to%20distribution%20shifts%3F%0A%20%20We%20focus%20on%20group%20fairness%20notions%20and%20investigate%20the%20HP%20space%20of%205%20training%0Aalgorithms.%20We%20first%20find%20that%20tree%20regressors%20and%20XGBoots%20significantly%0Aoutperformed%20deep%20neural%20networks%20and%20support%20vector%20machines%20in%20accurately%0Apredicting%20the%20fairness%20of%20HPs.%20When%20predicting%20the%20fairness%20of%20ML%0Ahyperparameters%20under%20temporal%20distribution%20shift%2C%20the%20tree%20regressors%0Aoutperforms%20the%20other%20algorithms%20with%20reasonable%20accuracy.%20However%2C%20the%0Aprecision%20depends%20on%20the%20ML%20training%20algorithm%2C%20dataset%2C%20and%20protected%0Aattributes.%20For%20example%2C%20the%20tree%20regressor%20model%20was%20robust%20for%20training%20data%0Ashift%20from%202014%20to%202018%20on%20logistic%20regression%20and%20discriminant%20analysis%20HPs%0Awith%20sex%20as%20the%20protected%20attribute%3B%20but%20not%20for%20race%20and%20other%20training%0Aalgorithms.%20Our%20method%20provides%20a%20sound%20framework%20to%20efficiently%20perform%0Afine-tuning%20of%20ML%20training%20algorithms%20and%20understand%20the%20relationships%20between%0AHPs%20and%20fairness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19100v2&entry.124074799=Read"},
{"title": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer\n  Models", "author": "Paulo Pirozelli and Marcos M. Jos\u00e9 and Paulo de Tarso P. Filho and Anarosa A. F. Brand\u00e3o and Fabio G. Cozman", "abstract": "  Logical reasoning is central to complex human activities, such as thinking,\ndebating, and planning; it is also a central component of many AI systems as\nwell. In this paper, we investigate the extent to which encoder-only\ntransformer language models (LMs) can reason according to logical rules. We ask\nwhether those LMs can deduce theorems in propositional calculus and first-order\nlogic; if their relative success in these problems reflects general logical\ncapabilities; and which layers contribute the most to the task. First, we show\nfor several encoder-only LMs that they can be trained, to a reasonable degree,\nto determine logical validity on various datasets. Next, by cross-probing\nfine-tuned models on these datasets, we show that LMs have difficulty in\ntransferring their putative logical reasoning ability, which suggests that they\nmay have learned dataset-specific features, instead of a general capability.\nFinally, we conduct a layerwise probing experiment, which shows that the\nhypothesis classification task is mostly solved through higher layers.\n", "link": "http://arxiv.org/abs/2312.11720v2", "date": "2024-07-01", "relevancy": 1.2738, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.445}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.428}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4151}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Assessing%20Logical%20Reasoning%20Capabilities%20of%20Encoder-Only%20Transformer%0A%20%20Models&body=Title%3A%20Assessing%20Logical%20Reasoning%20Capabilities%20of%20Encoder-Only%20Transformer%0A%20%20Models%0AAuthor%3A%20Paulo%20Pirozelli%20and%20Marcos%20M.%20Jos%C3%A9%20and%20Paulo%20de%20Tarso%20P.%20Filho%20and%20Anarosa%20A.%20F.%20Brand%C3%A3o%20and%20Fabio%20G.%20Cozman%0AAbstract%3A%20%20%20Logical%20reasoning%20is%20central%20to%20complex%20human%20activities%2C%20such%20as%20thinking%2C%0Adebating%2C%20and%20planning%3B%20it%20is%20also%20a%20central%20component%20of%20many%20AI%20systems%20as%0Awell.%20In%20this%20paper%2C%20we%20investigate%20the%20extent%20to%20which%20encoder-only%0Atransformer%20language%20models%20%28LMs%29%20can%20reason%20according%20to%20logical%20rules.%20We%20ask%0Awhether%20those%20LMs%20can%20deduce%20theorems%20in%20propositional%20calculus%20and%20first-order%0Alogic%3B%20if%20their%20relative%20success%20in%20these%20problems%20reflects%20general%20logical%0Acapabilities%3B%20and%20which%20layers%20contribute%20the%20most%20to%20the%20task.%20First%2C%20we%20show%0Afor%20several%20encoder-only%20LMs%20that%20they%20can%20be%20trained%2C%20to%20a%20reasonable%20degree%2C%0Ato%20determine%20logical%20validity%20on%20various%20datasets.%20Next%2C%20by%20cross-probing%0Afine-tuned%20models%20on%20these%20datasets%2C%20we%20show%20that%20LMs%20have%20difficulty%20in%0Atransferring%20their%20putative%20logical%20reasoning%20ability%2C%20which%20suggests%20that%20they%0Amay%20have%20learned%20dataset-specific%20features%2C%20instead%20of%20a%20general%20capability.%0AFinally%2C%20we%20conduct%20a%20layerwise%20probing%20experiment%2C%20which%20shows%20that%20the%0Ahypothesis%20classification%20task%20is%20mostly%20solved%20through%20higher%20layers.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.11720v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAssessing%2520Logical%2520Reasoning%2520Capabilities%2520of%2520Encoder-Only%2520Transformer%250A%2520%2520Models%26entry.906535625%3DPaulo%2520Pirozelli%2520and%2520Marcos%2520M.%2520Jos%25C3%25A9%2520and%2520Paulo%2520de%2520Tarso%2520P.%2520Filho%2520and%2520Anarosa%2520A.%2520F.%2520Brand%25C3%25A3o%2520and%2520Fabio%2520G.%2520Cozman%26entry.1292438233%3D%2520%2520Logical%2520reasoning%2520is%2520central%2520to%2520complex%2520human%2520activities%252C%2520such%2520as%2520thinking%252C%250Adebating%252C%2520and%2520planning%253B%2520it%2520is%2520also%2520a%2520central%2520component%2520of%2520many%2520AI%2520systems%2520as%250Awell.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520extent%2520to%2520which%2520encoder-only%250Atransformer%2520language%2520models%2520%2528LMs%2529%2520can%2520reason%2520according%2520to%2520logical%2520rules.%2520We%2520ask%250Awhether%2520those%2520LMs%2520can%2520deduce%2520theorems%2520in%2520propositional%2520calculus%2520and%2520first-order%250Alogic%253B%2520if%2520their%2520relative%2520success%2520in%2520these%2520problems%2520reflects%2520general%2520logical%250Acapabilities%253B%2520and%2520which%2520layers%2520contribute%2520the%2520most%2520to%2520the%2520task.%2520First%252C%2520we%2520show%250Afor%2520several%2520encoder-only%2520LMs%2520that%2520they%2520can%2520be%2520trained%252C%2520to%2520a%2520reasonable%2520degree%252C%250Ato%2520determine%2520logical%2520validity%2520on%2520various%2520datasets.%2520Next%252C%2520by%2520cross-probing%250Afine-tuned%2520models%2520on%2520these%2520datasets%252C%2520we%2520show%2520that%2520LMs%2520have%2520difficulty%2520in%250Atransferring%2520their%2520putative%2520logical%2520reasoning%2520ability%252C%2520which%2520suggests%2520that%2520they%250Amay%2520have%2520learned%2520dataset-specific%2520features%252C%2520instead%2520of%2520a%2520general%2520capability.%250AFinally%252C%2520we%2520conduct%2520a%2520layerwise%2520probing%2520experiment%252C%2520which%2520shows%2520that%2520the%250Ahypothesis%2520classification%2520task%2520is%2520mostly%2520solved%2520through%2520higher%2520layers.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.11720v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Assessing%20Logical%20Reasoning%20Capabilities%20of%20Encoder-Only%20Transformer%0A%20%20Models&entry.906535625=Paulo%20Pirozelli%20and%20Marcos%20M.%20Jos%C3%A9%20and%20Paulo%20de%20Tarso%20P.%20Filho%20and%20Anarosa%20A.%20F.%20Brand%C3%A3o%20and%20Fabio%20G.%20Cozman&entry.1292438233=%20%20Logical%20reasoning%20is%20central%20to%20complex%20human%20activities%2C%20such%20as%20thinking%2C%0Adebating%2C%20and%20planning%3B%20it%20is%20also%20a%20central%20component%20of%20many%20AI%20systems%20as%0Awell.%20In%20this%20paper%2C%20we%20investigate%20the%20extent%20to%20which%20encoder-only%0Atransformer%20language%20models%20%28LMs%29%20can%20reason%20according%20to%20logical%20rules.%20We%20ask%0Awhether%20those%20LMs%20can%20deduce%20theorems%20in%20propositional%20calculus%20and%20first-order%0Alogic%3B%20if%20their%20relative%20success%20in%20these%20problems%20reflects%20general%20logical%0Acapabilities%3B%20and%20which%20layers%20contribute%20the%20most%20to%20the%20task.%20First%2C%20we%20show%0Afor%20several%20encoder-only%20LMs%20that%20they%20can%20be%20trained%2C%20to%20a%20reasonable%20degree%2C%0Ato%20determine%20logical%20validity%20on%20various%20datasets.%20Next%2C%20by%20cross-probing%0Afine-tuned%20models%20on%20these%20datasets%2C%20we%20show%20that%20LMs%20have%20difficulty%20in%0Atransferring%20their%20putative%20logical%20reasoning%20ability%2C%20which%20suggests%20that%20they%0Amay%20have%20learned%20dataset-specific%20features%2C%20instead%20of%20a%20general%20capability.%0AFinally%2C%20we%20conduct%20a%20layerwise%20probing%20experiment%2C%20which%20shows%20that%20the%0Ahypothesis%20classification%20task%20is%20mostly%20solved%20through%20higher%20layers.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.11720v2&entry.124074799=Read"},
{"title": "Decomposing Global Feature Effects Based on Feature Interactions", "author": "Julia Herbinger and Marvin N. Wright and Thomas Nagler and Bernd Bischl and Giuseppe Casalicchio", "abstract": "  Global feature effect methods, such as partial dependence plots, provide an\nintelligible visualization of the expected marginal feature effect. However,\nsuch global feature effect methods can be misleading, as they do not represent\nlocal feature effects of single observations well when feature interactions are\npresent. We formally introduce generalized additive decomposition of global\neffects (GADGET), which is a new framework based on recursive partitioning to\nfind interpretable regions in the feature space such that the\ninteraction-related heterogeneity of local feature effects is minimized. We\nprovide a mathematical foundation of the framework and show that it is\napplicable to the most popular methods to visualize marginal feature effects,\nnamely partial dependence, accumulated local effects, and Shapley additive\nexplanations (SHAP) dependence. Furthermore, we introduce and validate a new\npermutation-based interaction test to detect significant feature interactions\nthat is applicable to any feature effect method that fits into our proposed\nframework. We empirically evaluate the theoretical characteristics of the\nproposed methods based on various feature effect methods in different\nexperimental settings. Moreover, we apply our introduced methodology to three\nreal-world examples to showcase their usefulness.\n", "link": "http://arxiv.org/abs/2306.00541v2", "date": "2024-07-01", "relevancy": 1.2625, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4225}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4223}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decomposing%20Global%20Feature%20Effects%20Based%20on%20Feature%20Interactions&body=Title%3A%20Decomposing%20Global%20Feature%20Effects%20Based%20on%20Feature%20Interactions%0AAuthor%3A%20Julia%20Herbinger%20and%20Marvin%20N.%20Wright%20and%20Thomas%20Nagler%20and%20Bernd%20Bischl%20and%20Giuseppe%20Casalicchio%0AAbstract%3A%20%20%20Global%20feature%20effect%20methods%2C%20such%20as%20partial%20dependence%20plots%2C%20provide%20an%0Aintelligible%20visualization%20of%20the%20expected%20marginal%20feature%20effect.%20However%2C%0Asuch%20global%20feature%20effect%20methods%20can%20be%20misleading%2C%20as%20they%20do%20not%20represent%0Alocal%20feature%20effects%20of%20single%20observations%20well%20when%20feature%20interactions%20are%0Apresent.%20We%20formally%20introduce%20generalized%20additive%20decomposition%20of%20global%0Aeffects%20%28GADGET%29%2C%20which%20is%20a%20new%20framework%20based%20on%20recursive%20partitioning%20to%0Afind%20interpretable%20regions%20in%20the%20feature%20space%20such%20that%20the%0Ainteraction-related%20heterogeneity%20of%20local%20feature%20effects%20is%20minimized.%20We%0Aprovide%20a%20mathematical%20foundation%20of%20the%20framework%20and%20show%20that%20it%20is%0Aapplicable%20to%20the%20most%20popular%20methods%20to%20visualize%20marginal%20feature%20effects%2C%0Anamely%20partial%20dependence%2C%20accumulated%20local%20effects%2C%20and%20Shapley%20additive%0Aexplanations%20%28SHAP%29%20dependence.%20Furthermore%2C%20we%20introduce%20and%20validate%20a%20new%0Apermutation-based%20interaction%20test%20to%20detect%20significant%20feature%20interactions%0Athat%20is%20applicable%20to%20any%20feature%20effect%20method%20that%20fits%20into%20our%20proposed%0Aframework.%20We%20empirically%20evaluate%20the%20theoretical%20characteristics%20of%20the%0Aproposed%20methods%20based%20on%20various%20feature%20effect%20methods%20in%20different%0Aexperimental%20settings.%20Moreover%2C%20we%20apply%20our%20introduced%20methodology%20to%20three%0Areal-world%20examples%20to%20showcase%20their%20usefulness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.00541v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecomposing%2520Global%2520Feature%2520Effects%2520Based%2520on%2520Feature%2520Interactions%26entry.906535625%3DJulia%2520Herbinger%2520and%2520Marvin%2520N.%2520Wright%2520and%2520Thomas%2520Nagler%2520and%2520Bernd%2520Bischl%2520and%2520Giuseppe%2520Casalicchio%26entry.1292438233%3D%2520%2520Global%2520feature%2520effect%2520methods%252C%2520such%2520as%2520partial%2520dependence%2520plots%252C%2520provide%2520an%250Aintelligible%2520visualization%2520of%2520the%2520expected%2520marginal%2520feature%2520effect.%2520However%252C%250Asuch%2520global%2520feature%2520effect%2520methods%2520can%2520be%2520misleading%252C%2520as%2520they%2520do%2520not%2520represent%250Alocal%2520feature%2520effects%2520of%2520single%2520observations%2520well%2520when%2520feature%2520interactions%2520are%250Apresent.%2520We%2520formally%2520introduce%2520generalized%2520additive%2520decomposition%2520of%2520global%250Aeffects%2520%2528GADGET%2529%252C%2520which%2520is%2520a%2520new%2520framework%2520based%2520on%2520recursive%2520partitioning%2520to%250Afind%2520interpretable%2520regions%2520in%2520the%2520feature%2520space%2520such%2520that%2520the%250Ainteraction-related%2520heterogeneity%2520of%2520local%2520feature%2520effects%2520is%2520minimized.%2520We%250Aprovide%2520a%2520mathematical%2520foundation%2520of%2520the%2520framework%2520and%2520show%2520that%2520it%2520is%250Aapplicable%2520to%2520the%2520most%2520popular%2520methods%2520to%2520visualize%2520marginal%2520feature%2520effects%252C%250Anamely%2520partial%2520dependence%252C%2520accumulated%2520local%2520effects%252C%2520and%2520Shapley%2520additive%250Aexplanations%2520%2528SHAP%2529%2520dependence.%2520Furthermore%252C%2520we%2520introduce%2520and%2520validate%2520a%2520new%250Apermutation-based%2520interaction%2520test%2520to%2520detect%2520significant%2520feature%2520interactions%250Athat%2520is%2520applicable%2520to%2520any%2520feature%2520effect%2520method%2520that%2520fits%2520into%2520our%2520proposed%250Aframework.%2520We%2520empirically%2520evaluate%2520the%2520theoretical%2520characteristics%2520of%2520the%250Aproposed%2520methods%2520based%2520on%2520various%2520feature%2520effect%2520methods%2520in%2520different%250Aexperimental%2520settings.%2520Moreover%252C%2520we%2520apply%2520our%2520introduced%2520methodology%2520to%2520three%250Areal-world%2520examples%2520to%2520showcase%2520their%2520usefulness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.00541v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decomposing%20Global%20Feature%20Effects%20Based%20on%20Feature%20Interactions&entry.906535625=Julia%20Herbinger%20and%20Marvin%20N.%20Wright%20and%20Thomas%20Nagler%20and%20Bernd%20Bischl%20and%20Giuseppe%20Casalicchio&entry.1292438233=%20%20Global%20feature%20effect%20methods%2C%20such%20as%20partial%20dependence%20plots%2C%20provide%20an%0Aintelligible%20visualization%20of%20the%20expected%20marginal%20feature%20effect.%20However%2C%0Asuch%20global%20feature%20effect%20methods%20can%20be%20misleading%2C%20as%20they%20do%20not%20represent%0Alocal%20feature%20effects%20of%20single%20observations%20well%20when%20feature%20interactions%20are%0Apresent.%20We%20formally%20introduce%20generalized%20additive%20decomposition%20of%20global%0Aeffects%20%28GADGET%29%2C%20which%20is%20a%20new%20framework%20based%20on%20recursive%20partitioning%20to%0Afind%20interpretable%20regions%20in%20the%20feature%20space%20such%20that%20the%0Ainteraction-related%20heterogeneity%20of%20local%20feature%20effects%20is%20minimized.%20We%0Aprovide%20a%20mathematical%20foundation%20of%20the%20framework%20and%20show%20that%20it%20is%0Aapplicable%20to%20the%20most%20popular%20methods%20to%20visualize%20marginal%20feature%20effects%2C%0Anamely%20partial%20dependence%2C%20accumulated%20local%20effects%2C%20and%20Shapley%20additive%0Aexplanations%20%28SHAP%29%20dependence.%20Furthermore%2C%20we%20introduce%20and%20validate%20a%20new%0Apermutation-based%20interaction%20test%20to%20detect%20significant%20feature%20interactions%0Athat%20is%20applicable%20to%20any%20feature%20effect%20method%20that%20fits%20into%20our%20proposed%0Aframework.%20We%20empirically%20evaluate%20the%20theoretical%20characteristics%20of%20the%0Aproposed%20methods%20based%20on%20various%20feature%20effect%20methods%20in%20different%0Aexperimental%20settings.%20Moreover%2C%20we%20apply%20our%20introduced%20methodology%20to%20three%0Areal-world%20examples%20to%20showcase%20their%20usefulness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.00541v2&entry.124074799=Read"},
{"title": "Embedded FPGA Developments in 130nm and 28nm CMOS for Machine Learning\n  in Particle Detector Readout", "author": "Julia Gonski and Aseem Gupta and Haoyi Jia and Hyunjoon Kim and Lorenzo Rota and Larry Ruckman and Angelo Dragone and Ryan Herbst", "abstract": "  Embedded field programmable gate array (eFPGA) technology allows the\nimplementation of reconfigurable logic within the design of an\napplication-specific integrated circuit (ASIC). This approach offers the low\npower and efficiency of an ASIC along with the ease of FPGA configuration,\nparticularly beneficial for the use case of machine learning in the data\npipeline of next-generation collider experiments. An open-source framework\ncalled \"FABulous\" was used to design eFPGAs using 130 nm and 28 nm CMOS\ntechnology nodes, which were subsequently fabricated and verified through\ntesting. The capability of an eFPGA to act as a front-end readout chip was\nassessed using simulation of high energy particles passing through a silicon\npixel sensor. A machine learning-based classifier, designed for reduction of\nsensor data at the source, was synthesized and configured onto the eFPGA. A\nsuccessful proof-of-concept was demonstrated through reproduction of the\nexpected algorithm result on the eFPGA with perfect accuracy. Further\ndevelopment of the eFPGA technology and its application to collider detector\nreadout is discussed.\n", "link": "http://arxiv.org/abs/2404.17701v2", "date": "2024-07-01", "relevancy": 1.2544, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4518}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4332}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.3987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Embedded%20FPGA%20Developments%20in%20130nm%20and%2028nm%20CMOS%20for%20Machine%20Learning%0A%20%20in%20Particle%20Detector%20Readout&body=Title%3A%20Embedded%20FPGA%20Developments%20in%20130nm%20and%2028nm%20CMOS%20for%20Machine%20Learning%0A%20%20in%20Particle%20Detector%20Readout%0AAuthor%3A%20Julia%20Gonski%20and%20Aseem%20Gupta%20and%20Haoyi%20Jia%20and%20Hyunjoon%20Kim%20and%20Lorenzo%20Rota%20and%20Larry%20Ruckman%20and%20Angelo%20Dragone%20and%20Ryan%20Herbst%0AAbstract%3A%20%20%20Embedded%20field%20programmable%20gate%20array%20%28eFPGA%29%20technology%20allows%20the%0Aimplementation%20of%20reconfigurable%20logic%20within%20the%20design%20of%20an%0Aapplication-specific%20integrated%20circuit%20%28ASIC%29.%20This%20approach%20offers%20the%20low%0Apower%20and%20efficiency%20of%20an%20ASIC%20along%20with%20the%20ease%20of%20FPGA%20configuration%2C%0Aparticularly%20beneficial%20for%20the%20use%20case%20of%20machine%20learning%20in%20the%20data%0Apipeline%20of%20next-generation%20collider%20experiments.%20An%20open-source%20framework%0Acalled%20%22FABulous%22%20was%20used%20to%20design%20eFPGAs%20using%20130%20nm%20and%2028%20nm%20CMOS%0Atechnology%20nodes%2C%20which%20were%20subsequently%20fabricated%20and%20verified%20through%0Atesting.%20The%20capability%20of%20an%20eFPGA%20to%20act%20as%20a%20front-end%20readout%20chip%20was%0Aassessed%20using%20simulation%20of%20high%20energy%20particles%20passing%20through%20a%20silicon%0Apixel%20sensor.%20A%20machine%20learning-based%20classifier%2C%20designed%20for%20reduction%20of%0Asensor%20data%20at%20the%20source%2C%20was%20synthesized%20and%20configured%20onto%20the%20eFPGA.%20A%0Asuccessful%20proof-of-concept%20was%20demonstrated%20through%20reproduction%20of%20the%0Aexpected%20algorithm%20result%20on%20the%20eFPGA%20with%20perfect%20accuracy.%20Further%0Adevelopment%20of%20the%20eFPGA%20technology%20and%20its%20application%20to%20collider%20detector%0Areadout%20is%20discussed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.17701v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmbedded%2520FPGA%2520Developments%2520in%2520130nm%2520and%252028nm%2520CMOS%2520for%2520Machine%2520Learning%250A%2520%2520in%2520Particle%2520Detector%2520Readout%26entry.906535625%3DJulia%2520Gonski%2520and%2520Aseem%2520Gupta%2520and%2520Haoyi%2520Jia%2520and%2520Hyunjoon%2520Kim%2520and%2520Lorenzo%2520Rota%2520and%2520Larry%2520Ruckman%2520and%2520Angelo%2520Dragone%2520and%2520Ryan%2520Herbst%26entry.1292438233%3D%2520%2520Embedded%2520field%2520programmable%2520gate%2520array%2520%2528eFPGA%2529%2520technology%2520allows%2520the%250Aimplementation%2520of%2520reconfigurable%2520logic%2520within%2520the%2520design%2520of%2520an%250Aapplication-specific%2520integrated%2520circuit%2520%2528ASIC%2529.%2520This%2520approach%2520offers%2520the%2520low%250Apower%2520and%2520efficiency%2520of%2520an%2520ASIC%2520along%2520with%2520the%2520ease%2520of%2520FPGA%2520configuration%252C%250Aparticularly%2520beneficial%2520for%2520the%2520use%2520case%2520of%2520machine%2520learning%2520in%2520the%2520data%250Apipeline%2520of%2520next-generation%2520collider%2520experiments.%2520An%2520open-source%2520framework%250Acalled%2520%2522FABulous%2522%2520was%2520used%2520to%2520design%2520eFPGAs%2520using%2520130%2520nm%2520and%252028%2520nm%2520CMOS%250Atechnology%2520nodes%252C%2520which%2520were%2520subsequently%2520fabricated%2520and%2520verified%2520through%250Atesting.%2520The%2520capability%2520of%2520an%2520eFPGA%2520to%2520act%2520as%2520a%2520front-end%2520readout%2520chip%2520was%250Aassessed%2520using%2520simulation%2520of%2520high%2520energy%2520particles%2520passing%2520through%2520a%2520silicon%250Apixel%2520sensor.%2520A%2520machine%2520learning-based%2520classifier%252C%2520designed%2520for%2520reduction%2520of%250Asensor%2520data%2520at%2520the%2520source%252C%2520was%2520synthesized%2520and%2520configured%2520onto%2520the%2520eFPGA.%2520A%250Asuccessful%2520proof-of-concept%2520was%2520demonstrated%2520through%2520reproduction%2520of%2520the%250Aexpected%2520algorithm%2520result%2520on%2520the%2520eFPGA%2520with%2520perfect%2520accuracy.%2520Further%250Adevelopment%2520of%2520the%2520eFPGA%2520technology%2520and%2520its%2520application%2520to%2520collider%2520detector%250Areadout%2520is%2520discussed.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.17701v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Embedded%20FPGA%20Developments%20in%20130nm%20and%2028nm%20CMOS%20for%20Machine%20Learning%0A%20%20in%20Particle%20Detector%20Readout&entry.906535625=Julia%20Gonski%20and%20Aseem%20Gupta%20and%20Haoyi%20Jia%20and%20Hyunjoon%20Kim%20and%20Lorenzo%20Rota%20and%20Larry%20Ruckman%20and%20Angelo%20Dragone%20and%20Ryan%20Herbst&entry.1292438233=%20%20Embedded%20field%20programmable%20gate%20array%20%28eFPGA%29%20technology%20allows%20the%0Aimplementation%20of%20reconfigurable%20logic%20within%20the%20design%20of%20an%0Aapplication-specific%20integrated%20circuit%20%28ASIC%29.%20This%20approach%20offers%20the%20low%0Apower%20and%20efficiency%20of%20an%20ASIC%20along%20with%20the%20ease%20of%20FPGA%20configuration%2C%0Aparticularly%20beneficial%20for%20the%20use%20case%20of%20machine%20learning%20in%20the%20data%0Apipeline%20of%20next-generation%20collider%20experiments.%20An%20open-source%20framework%0Acalled%20%22FABulous%22%20was%20used%20to%20design%20eFPGAs%20using%20130%20nm%20and%2028%20nm%20CMOS%0Atechnology%20nodes%2C%20which%20were%20subsequently%20fabricated%20and%20verified%20through%0Atesting.%20The%20capability%20of%20an%20eFPGA%20to%20act%20as%20a%20front-end%20readout%20chip%20was%0Aassessed%20using%20simulation%20of%20high%20energy%20particles%20passing%20through%20a%20silicon%0Apixel%20sensor.%20A%20machine%20learning-based%20classifier%2C%20designed%20for%20reduction%20of%0Asensor%20data%20at%20the%20source%2C%20was%20synthesized%20and%20configured%20onto%20the%20eFPGA.%20A%0Asuccessful%20proof-of-concept%20was%20demonstrated%20through%20reproduction%20of%20the%0Aexpected%20algorithm%20result%20on%20the%20eFPGA%20with%20perfect%20accuracy.%20Further%0Adevelopment%20of%20the%20eFPGA%20technology%20and%20its%20application%20to%20collider%20detector%0Areadout%20is%20discussed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.17701v2&entry.124074799=Read"},
{"title": "Unmasking Bias in AI: A Systematic Review of Bias Detection and\n  Mitigation Strategies in Electronic Health Record-based Models", "author": "Feng Chen and Liqin Wang and Julie Hong and Jiaqi Jiang and Li Zhou", "abstract": "  Objectives: Leveraging artificial intelligence (AI) in conjunction with\nelectronic health records (EHRs) holds transformative potential to improve\nhealthcare. Yet, addressing bias in AI, which risks worsening healthcare\ndisparities, cannot be overlooked. This study reviews methods to detect and\nmitigate diverse forms of bias in AI models developed using EHR data. Methods:\nWe conducted a systematic review following the Preferred Reporting Items for\nSystematic Reviews and Meta-analyses (PRISMA) guidelines, analyzing articles\nfrom PubMed, Web of Science, and IEEE published between January 1, 2010, and\nDec 17, 2023. The review identified key biases, outlined strategies for\ndetecting and mitigating bias throughout the AI model development process, and\nanalyzed metrics for bias assessment. Results: Of the 450 articles retrieved,\n20 met our criteria, revealing six major bias types: algorithmic, confounding,\nimplicit, measurement, selection, and temporal. The AI models were primarily\ndeveloped for predictive tasks in healthcare settings. Four studies\nconcentrated on the detection of implicit and algorithmic biases employing\nfairness metrics like statistical parity, equal opportunity, and predictive\nequity. Sixty proposed various strategies for mitigating biases, especially\ntargeting implicit and selection biases. These strategies, evaluated through\nboth performance (e.g., accuracy, AUROC) and fairness metrics, predominantly\ninvolved data collection and preprocessing techniques like resampling,\nreweighting, and transformation. Discussion: This review highlights the varied\nand evolving nature of strategies to address bias in EHR-based AI models,\nemphasizing the urgent needs for the establishment of standardized,\ngeneralizable, and interpretable methodologies to foster the creation of\nethical AI systems that promote fairness and equity in healthcare.\n", "link": "http://arxiv.org/abs/2310.19917v3", "date": "2024-07-01", "relevancy": 1.178, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4285}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3825}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unmasking%20Bias%20in%20AI%3A%20A%20Systematic%20Review%20of%20Bias%20Detection%20and%0A%20%20Mitigation%20Strategies%20in%20Electronic%20Health%20Record-based%20Models&body=Title%3A%20Unmasking%20Bias%20in%20AI%3A%20A%20Systematic%20Review%20of%20Bias%20Detection%20and%0A%20%20Mitigation%20Strategies%20in%20Electronic%20Health%20Record-based%20Models%0AAuthor%3A%20Feng%20Chen%20and%20Liqin%20Wang%20and%20Julie%20Hong%20and%20Jiaqi%20Jiang%20and%20Li%20Zhou%0AAbstract%3A%20%20%20Objectives%3A%20Leveraging%20artificial%20intelligence%20%28AI%29%20in%20conjunction%20with%0Aelectronic%20health%20records%20%28EHRs%29%20holds%20transformative%20potential%20to%20improve%0Ahealthcare.%20Yet%2C%20addressing%20bias%20in%20AI%2C%20which%20risks%20worsening%20healthcare%0Adisparities%2C%20cannot%20be%20overlooked.%20This%20study%20reviews%20methods%20to%20detect%20and%0Amitigate%20diverse%20forms%20of%20bias%20in%20AI%20models%20developed%20using%20EHR%20data.%20Methods%3A%0AWe%20conducted%20a%20systematic%20review%20following%20the%20Preferred%20Reporting%20Items%20for%0ASystematic%20Reviews%20and%20Meta-analyses%20%28PRISMA%29%20guidelines%2C%20analyzing%20articles%0Afrom%20PubMed%2C%20Web%20of%20Science%2C%20and%20IEEE%20published%20between%20January%201%2C%202010%2C%20and%0ADec%2017%2C%202023.%20The%20review%20identified%20key%20biases%2C%20outlined%20strategies%20for%0Adetecting%20and%20mitigating%20bias%20throughout%20the%20AI%20model%20development%20process%2C%20and%0Aanalyzed%20metrics%20for%20bias%20assessment.%20Results%3A%20Of%20the%20450%20articles%20retrieved%2C%0A20%20met%20our%20criteria%2C%20revealing%20six%20major%20bias%20types%3A%20algorithmic%2C%20confounding%2C%0Aimplicit%2C%20measurement%2C%20selection%2C%20and%20temporal.%20The%20AI%20models%20were%20primarily%0Adeveloped%20for%20predictive%20tasks%20in%20healthcare%20settings.%20Four%20studies%0Aconcentrated%20on%20the%20detection%20of%20implicit%20and%20algorithmic%20biases%20employing%0Afairness%20metrics%20like%20statistical%20parity%2C%20equal%20opportunity%2C%20and%20predictive%0Aequity.%20Sixty%20proposed%20various%20strategies%20for%20mitigating%20biases%2C%20especially%0Atargeting%20implicit%20and%20selection%20biases.%20These%20strategies%2C%20evaluated%20through%0Aboth%20performance%20%28e.g.%2C%20accuracy%2C%20AUROC%29%20and%20fairness%20metrics%2C%20predominantly%0Ainvolved%20data%20collection%20and%20preprocessing%20techniques%20like%20resampling%2C%0Areweighting%2C%20and%20transformation.%20Discussion%3A%20This%20review%20highlights%20the%20varied%0Aand%20evolving%20nature%20of%20strategies%20to%20address%20bias%20in%20EHR-based%20AI%20models%2C%0Aemphasizing%20the%20urgent%20needs%20for%20the%20establishment%20of%20standardized%2C%0Ageneralizable%2C%20and%20interpretable%20methodologies%20to%20foster%20the%20creation%20of%0Aethical%20AI%20systems%20that%20promote%20fairness%20and%20equity%20in%20healthcare.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.19917v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnmasking%2520Bias%2520in%2520AI%253A%2520A%2520Systematic%2520Review%2520of%2520Bias%2520Detection%2520and%250A%2520%2520Mitigation%2520Strategies%2520in%2520Electronic%2520Health%2520Record-based%2520Models%26entry.906535625%3DFeng%2520Chen%2520and%2520Liqin%2520Wang%2520and%2520Julie%2520Hong%2520and%2520Jiaqi%2520Jiang%2520and%2520Li%2520Zhou%26entry.1292438233%3D%2520%2520Objectives%253A%2520Leveraging%2520artificial%2520intelligence%2520%2528AI%2529%2520in%2520conjunction%2520with%250Aelectronic%2520health%2520records%2520%2528EHRs%2529%2520holds%2520transformative%2520potential%2520to%2520improve%250Ahealthcare.%2520Yet%252C%2520addressing%2520bias%2520in%2520AI%252C%2520which%2520risks%2520worsening%2520healthcare%250Adisparities%252C%2520cannot%2520be%2520overlooked.%2520This%2520study%2520reviews%2520methods%2520to%2520detect%2520and%250Amitigate%2520diverse%2520forms%2520of%2520bias%2520in%2520AI%2520models%2520developed%2520using%2520EHR%2520data.%2520Methods%253A%250AWe%2520conducted%2520a%2520systematic%2520review%2520following%2520the%2520Preferred%2520Reporting%2520Items%2520for%250ASystematic%2520Reviews%2520and%2520Meta-analyses%2520%2528PRISMA%2529%2520guidelines%252C%2520analyzing%2520articles%250Afrom%2520PubMed%252C%2520Web%2520of%2520Science%252C%2520and%2520IEEE%2520published%2520between%2520January%25201%252C%25202010%252C%2520and%250ADec%252017%252C%25202023.%2520The%2520review%2520identified%2520key%2520biases%252C%2520outlined%2520strategies%2520for%250Adetecting%2520and%2520mitigating%2520bias%2520throughout%2520the%2520AI%2520model%2520development%2520process%252C%2520and%250Aanalyzed%2520metrics%2520for%2520bias%2520assessment.%2520Results%253A%2520Of%2520the%2520450%2520articles%2520retrieved%252C%250A20%2520met%2520our%2520criteria%252C%2520revealing%2520six%2520major%2520bias%2520types%253A%2520algorithmic%252C%2520confounding%252C%250Aimplicit%252C%2520measurement%252C%2520selection%252C%2520and%2520temporal.%2520The%2520AI%2520models%2520were%2520primarily%250Adeveloped%2520for%2520predictive%2520tasks%2520in%2520healthcare%2520settings.%2520Four%2520studies%250Aconcentrated%2520on%2520the%2520detection%2520of%2520implicit%2520and%2520algorithmic%2520biases%2520employing%250Afairness%2520metrics%2520like%2520statistical%2520parity%252C%2520equal%2520opportunity%252C%2520and%2520predictive%250Aequity.%2520Sixty%2520proposed%2520various%2520strategies%2520for%2520mitigating%2520biases%252C%2520especially%250Atargeting%2520implicit%2520and%2520selection%2520biases.%2520These%2520strategies%252C%2520evaluated%2520through%250Aboth%2520performance%2520%2528e.g.%252C%2520accuracy%252C%2520AUROC%2529%2520and%2520fairness%2520metrics%252C%2520predominantly%250Ainvolved%2520data%2520collection%2520and%2520preprocessing%2520techniques%2520like%2520resampling%252C%250Areweighting%252C%2520and%2520transformation.%2520Discussion%253A%2520This%2520review%2520highlights%2520the%2520varied%250Aand%2520evolving%2520nature%2520of%2520strategies%2520to%2520address%2520bias%2520in%2520EHR-based%2520AI%2520models%252C%250Aemphasizing%2520the%2520urgent%2520needs%2520for%2520the%2520establishment%2520of%2520standardized%252C%250Ageneralizable%252C%2520and%2520interpretable%2520methodologies%2520to%2520foster%2520the%2520creation%2520of%250Aethical%2520AI%2520systems%2520that%2520promote%2520fairness%2520and%2520equity%2520in%2520healthcare.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.19917v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unmasking%20Bias%20in%20AI%3A%20A%20Systematic%20Review%20of%20Bias%20Detection%20and%0A%20%20Mitigation%20Strategies%20in%20Electronic%20Health%20Record-based%20Models&entry.906535625=Feng%20Chen%20and%20Liqin%20Wang%20and%20Julie%20Hong%20and%20Jiaqi%20Jiang%20and%20Li%20Zhou&entry.1292438233=%20%20Objectives%3A%20Leveraging%20artificial%20intelligence%20%28AI%29%20in%20conjunction%20with%0Aelectronic%20health%20records%20%28EHRs%29%20holds%20transformative%20potential%20to%20improve%0Ahealthcare.%20Yet%2C%20addressing%20bias%20in%20AI%2C%20which%20risks%20worsening%20healthcare%0Adisparities%2C%20cannot%20be%20overlooked.%20This%20study%20reviews%20methods%20to%20detect%20and%0Amitigate%20diverse%20forms%20of%20bias%20in%20AI%20models%20developed%20using%20EHR%20data.%20Methods%3A%0AWe%20conducted%20a%20systematic%20review%20following%20the%20Preferred%20Reporting%20Items%20for%0ASystematic%20Reviews%20and%20Meta-analyses%20%28PRISMA%29%20guidelines%2C%20analyzing%20articles%0Afrom%20PubMed%2C%20Web%20of%20Science%2C%20and%20IEEE%20published%20between%20January%201%2C%202010%2C%20and%0ADec%2017%2C%202023.%20The%20review%20identified%20key%20biases%2C%20outlined%20strategies%20for%0Adetecting%20and%20mitigating%20bias%20throughout%20the%20AI%20model%20development%20process%2C%20and%0Aanalyzed%20metrics%20for%20bias%20assessment.%20Results%3A%20Of%20the%20450%20articles%20retrieved%2C%0A20%20met%20our%20criteria%2C%20revealing%20six%20major%20bias%20types%3A%20algorithmic%2C%20confounding%2C%0Aimplicit%2C%20measurement%2C%20selection%2C%20and%20temporal.%20The%20AI%20models%20were%20primarily%0Adeveloped%20for%20predictive%20tasks%20in%20healthcare%20settings.%20Four%20studies%0Aconcentrated%20on%20the%20detection%20of%20implicit%20and%20algorithmic%20biases%20employing%0Afairness%20metrics%20like%20statistical%20parity%2C%20equal%20opportunity%2C%20and%20predictive%0Aequity.%20Sixty%20proposed%20various%20strategies%20for%20mitigating%20biases%2C%20especially%0Atargeting%20implicit%20and%20selection%20biases.%20These%20strategies%2C%20evaluated%20through%0Aboth%20performance%20%28e.g.%2C%20accuracy%2C%20AUROC%29%20and%20fairness%20metrics%2C%20predominantly%0Ainvolved%20data%20collection%20and%20preprocessing%20techniques%20like%20resampling%2C%0Areweighting%2C%20and%20transformation.%20Discussion%3A%20This%20review%20highlights%20the%20varied%0Aand%20evolving%20nature%20of%20strategies%20to%20address%20bias%20in%20EHR-based%20AI%20models%2C%0Aemphasizing%20the%20urgent%20needs%20for%20the%20establishment%20of%20standardized%2C%0Ageneralizable%2C%20and%20interpretable%20methodologies%20to%20foster%20the%20creation%20of%0Aethical%20AI%20systems%20that%20promote%20fairness%20and%20equity%20in%20healthcare.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.19917v3&entry.124074799=Read"},
{"title": "DCSI -- An improved measure of cluster separability based on separation\n  and connectedness", "author": "Jana Gauss and Fabian Scheipl and Moritz Herrmann", "abstract": "  Whether class labels in a given data set correspond to meaningful clusters is\ncrucial for the evaluation of clustering algorithms using real-world data sets.\nThis property can be quantified by separability measures. The central aspects\nof separability for density-based clustering are between-class separation and\nwithin-class connectedness, and neither classification-based complexity\nmeasures nor cluster validity indices (CVIs) adequately incorporate them. A\nnewly developed measure (density cluster separability index, DCSI) aims to\nquantify these two characteristics and can also be used as a CVI. Extensive\nexperiments on synthetic data indicate that DCSI correlates strongly with the\nperformance of DBSCAN measured via the adjusted Rand index (ARI) but lacks\nrobustness when it comes to multi-class data sets with overlapping classes that\nare ill-suited for density-based hard clustering. Detailed evaluation on\nfrequently used real-world data sets shows that DCSI can correctly identify\ntouching or overlapping classes that do not correspond to meaningful\ndensity-based clusters.\n", "link": "http://arxiv.org/abs/2310.12806v2", "date": "2024-07-01", "relevancy": 1.0745, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3668}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3637}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.3525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DCSI%20--%20An%20improved%20measure%20of%20cluster%20separability%20based%20on%20separation%0A%20%20and%20connectedness&body=Title%3A%20DCSI%20--%20An%20improved%20measure%20of%20cluster%20separability%20based%20on%20separation%0A%20%20and%20connectedness%0AAuthor%3A%20Jana%20Gauss%20and%20Fabian%20Scheipl%20and%20Moritz%20Herrmann%0AAbstract%3A%20%20%20Whether%20class%20labels%20in%20a%20given%20data%20set%20correspond%20to%20meaningful%20clusters%20is%0Acrucial%20for%20the%20evaluation%20of%20clustering%20algorithms%20using%20real-world%20data%20sets.%0AThis%20property%20can%20be%20quantified%20by%20separability%20measures.%20The%20central%20aspects%0Aof%20separability%20for%20density-based%20clustering%20are%20between-class%20separation%20and%0Awithin-class%20connectedness%2C%20and%20neither%20classification-based%20complexity%0Ameasures%20nor%20cluster%20validity%20indices%20%28CVIs%29%20adequately%20incorporate%20them.%20A%0Anewly%20developed%20measure%20%28density%20cluster%20separability%20index%2C%20DCSI%29%20aims%20to%0Aquantify%20these%20two%20characteristics%20and%20can%20also%20be%20used%20as%20a%20CVI.%20Extensive%0Aexperiments%20on%20synthetic%20data%20indicate%20that%20DCSI%20correlates%20strongly%20with%20the%0Aperformance%20of%20DBSCAN%20measured%20via%20the%20adjusted%20Rand%20index%20%28ARI%29%20but%20lacks%0Arobustness%20when%20it%20comes%20to%20multi-class%20data%20sets%20with%20overlapping%20classes%20that%0Aare%20ill-suited%20for%20density-based%20hard%20clustering.%20Detailed%20evaluation%20on%0Afrequently%20used%20real-world%20data%20sets%20shows%20that%20DCSI%20can%20correctly%20identify%0Atouching%20or%20overlapping%20classes%20that%20do%20not%20correspond%20to%20meaningful%0Adensity-based%20clusters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.12806v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDCSI%2520--%2520An%2520improved%2520measure%2520of%2520cluster%2520separability%2520based%2520on%2520separation%250A%2520%2520and%2520connectedness%26entry.906535625%3DJana%2520Gauss%2520and%2520Fabian%2520Scheipl%2520and%2520Moritz%2520Herrmann%26entry.1292438233%3D%2520%2520Whether%2520class%2520labels%2520in%2520a%2520given%2520data%2520set%2520correspond%2520to%2520meaningful%2520clusters%2520is%250Acrucial%2520for%2520the%2520evaluation%2520of%2520clustering%2520algorithms%2520using%2520real-world%2520data%2520sets.%250AThis%2520property%2520can%2520be%2520quantified%2520by%2520separability%2520measures.%2520The%2520central%2520aspects%250Aof%2520separability%2520for%2520density-based%2520clustering%2520are%2520between-class%2520separation%2520and%250Awithin-class%2520connectedness%252C%2520and%2520neither%2520classification-based%2520complexity%250Ameasures%2520nor%2520cluster%2520validity%2520indices%2520%2528CVIs%2529%2520adequately%2520incorporate%2520them.%2520A%250Anewly%2520developed%2520measure%2520%2528density%2520cluster%2520separability%2520index%252C%2520DCSI%2529%2520aims%2520to%250Aquantify%2520these%2520two%2520characteristics%2520and%2520can%2520also%2520be%2520used%2520as%2520a%2520CVI.%2520Extensive%250Aexperiments%2520on%2520synthetic%2520data%2520indicate%2520that%2520DCSI%2520correlates%2520strongly%2520with%2520the%250Aperformance%2520of%2520DBSCAN%2520measured%2520via%2520the%2520adjusted%2520Rand%2520index%2520%2528ARI%2529%2520but%2520lacks%250Arobustness%2520when%2520it%2520comes%2520to%2520multi-class%2520data%2520sets%2520with%2520overlapping%2520classes%2520that%250Aare%2520ill-suited%2520for%2520density-based%2520hard%2520clustering.%2520Detailed%2520evaluation%2520on%250Afrequently%2520used%2520real-world%2520data%2520sets%2520shows%2520that%2520DCSI%2520can%2520correctly%2520identify%250Atouching%2520or%2520overlapping%2520classes%2520that%2520do%2520not%2520correspond%2520to%2520meaningful%250Adensity-based%2520clusters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.12806v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DCSI%20--%20An%20improved%20measure%20of%20cluster%20separability%20based%20on%20separation%0A%20%20and%20connectedness&entry.906535625=Jana%20Gauss%20and%20Fabian%20Scheipl%20and%20Moritz%20Herrmann&entry.1292438233=%20%20Whether%20class%20labels%20in%20a%20given%20data%20set%20correspond%20to%20meaningful%20clusters%20is%0Acrucial%20for%20the%20evaluation%20of%20clustering%20algorithms%20using%20real-world%20data%20sets.%0AThis%20property%20can%20be%20quantified%20by%20separability%20measures.%20The%20central%20aspects%0Aof%20separability%20for%20density-based%20clustering%20are%20between-class%20separation%20and%0Awithin-class%20connectedness%2C%20and%20neither%20classification-based%20complexity%0Ameasures%20nor%20cluster%20validity%20indices%20%28CVIs%29%20adequately%20incorporate%20them.%20A%0Anewly%20developed%20measure%20%28density%20cluster%20separability%20index%2C%20DCSI%29%20aims%20to%0Aquantify%20these%20two%20characteristics%20and%20can%20also%20be%20used%20as%20a%20CVI.%20Extensive%0Aexperiments%20on%20synthetic%20data%20indicate%20that%20DCSI%20correlates%20strongly%20with%20the%0Aperformance%20of%20DBSCAN%20measured%20via%20the%20adjusted%20Rand%20index%20%28ARI%29%20but%20lacks%0Arobustness%20when%20it%20comes%20to%20multi-class%20data%20sets%20with%20overlapping%20classes%20that%0Aare%20ill-suited%20for%20density-based%20hard%20clustering.%20Detailed%20evaluation%20on%0Afrequently%20used%20real-world%20data%20sets%20shows%20that%20DCSI%20can%20correctly%20identify%0Atouching%20or%20overlapping%20classes%20that%20do%20not%20correspond%20to%20meaningful%0Adensity-based%20clusters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.12806v2&entry.124074799=Read"},
{"title": "Robust Model-Based Reinforcement Learning with an Adversarial Auxiliary\n  Model", "author": "Siemen Herremans and Ali Anwar and Siegfried Mercelis", "abstract": "  Reinforcement learning has demonstrated impressive performance in various\nchallenging problems such as robotics, board games, and classical arcade games.\nHowever, its real-world applications can be hindered by the absence of\nrobustness and safety in the learned policies. More specifically, an RL agent\nthat trains in a certain Markov decision process (MDP) often struggles to\nperform well in nearly identical MDPs. To address this issue, we employ the\nframework of Robust MDPs (RMDPs) in a model-based setting and introduce a novel\nlearned transition model. Our method specifically incorporates an auxiliary\npessimistic model, updated adversarially, to estimate the worst-case MDP within\na Kullback-Leibler uncertainty set. In comparison to several existing works,\nour work does not impose any additional conditions on the training environment,\nsuch as the need for a parametric simulator. To test the effectiveness of the\nproposed pessimistic model in enhancing policy robustness, we integrate it into\na practical RL algorithm, called Robust Model-Based Policy Optimization\n(RMBPO). Our experimental results indicate a notable improvement in policy\nrobustness on high-dimensional MuJoCo control tasks, with the auxiliary model\nenhancing the performance of the learned policy in distorted MDPs. We further\nexplore the learned deviation between the proposed auxiliary world model and\nthe nominal model, to examine how pessimism is achieved. By learning a\npessimistic world model and demonstrating its role in improving policy\nrobustness, our research contributes towards making (model-based) RL more\nrobust.\n", "link": "http://arxiv.org/abs/2406.09976v2", "date": "2024-07-01", "relevancy": 1.0462, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5601}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5146}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Model-Based%20Reinforcement%20Learning%20with%20an%20Adversarial%20Auxiliary%0A%20%20Model&body=Title%3A%20Robust%20Model-Based%20Reinforcement%20Learning%20with%20an%20Adversarial%20Auxiliary%0A%20%20Model%0AAuthor%3A%20Siemen%20Herremans%20and%20Ali%20Anwar%20and%20Siegfried%20Mercelis%0AAbstract%3A%20%20%20Reinforcement%20learning%20has%20demonstrated%20impressive%20performance%20in%20various%0Achallenging%20problems%20such%20as%20robotics%2C%20board%20games%2C%20and%20classical%20arcade%20games.%0AHowever%2C%20its%20real-world%20applications%20can%20be%20hindered%20by%20the%20absence%20of%0Arobustness%20and%20safety%20in%20the%20learned%20policies.%20More%20specifically%2C%20an%20RL%20agent%0Athat%20trains%20in%20a%20certain%20Markov%20decision%20process%20%28MDP%29%20often%20struggles%20to%0Aperform%20well%20in%20nearly%20identical%20MDPs.%20To%20address%20this%20issue%2C%20we%20employ%20the%0Aframework%20of%20Robust%20MDPs%20%28RMDPs%29%20in%20a%20model-based%20setting%20and%20introduce%20a%20novel%0Alearned%20transition%20model.%20Our%20method%20specifically%20incorporates%20an%20auxiliary%0Apessimistic%20model%2C%20updated%20adversarially%2C%20to%20estimate%20the%20worst-case%20MDP%20within%0Aa%20Kullback-Leibler%20uncertainty%20set.%20In%20comparison%20to%20several%20existing%20works%2C%0Aour%20work%20does%20not%20impose%20any%20additional%20conditions%20on%20the%20training%20environment%2C%0Asuch%20as%20the%20need%20for%20a%20parametric%20simulator.%20To%20test%20the%20effectiveness%20of%20the%0Aproposed%20pessimistic%20model%20in%20enhancing%20policy%20robustness%2C%20we%20integrate%20it%20into%0Aa%20practical%20RL%20algorithm%2C%20called%20Robust%20Model-Based%20Policy%20Optimization%0A%28RMBPO%29.%20Our%20experimental%20results%20indicate%20a%20notable%20improvement%20in%20policy%0Arobustness%20on%20high-dimensional%20MuJoCo%20control%20tasks%2C%20with%20the%20auxiliary%20model%0Aenhancing%20the%20performance%20of%20the%20learned%20policy%20in%20distorted%20MDPs.%20We%20further%0Aexplore%20the%20learned%20deviation%20between%20the%20proposed%20auxiliary%20world%20model%20and%0Athe%20nominal%20model%2C%20to%20examine%20how%20pessimism%20is%20achieved.%20By%20learning%20a%0Apessimistic%20world%20model%20and%20demonstrating%20its%20role%20in%20improving%20policy%0Arobustness%2C%20our%20research%20contributes%20towards%20making%20%28model-based%29%20RL%20more%0Arobust.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09976v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Model-Based%2520Reinforcement%2520Learning%2520with%2520an%2520Adversarial%2520Auxiliary%250A%2520%2520Model%26entry.906535625%3DSiemen%2520Herremans%2520and%2520Ali%2520Anwar%2520and%2520Siegfried%2520Mercelis%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520has%2520demonstrated%2520impressive%2520performance%2520in%2520various%250Achallenging%2520problems%2520such%2520as%2520robotics%252C%2520board%2520games%252C%2520and%2520classical%2520arcade%2520games.%250AHowever%252C%2520its%2520real-world%2520applications%2520can%2520be%2520hindered%2520by%2520the%2520absence%2520of%250Arobustness%2520and%2520safety%2520in%2520the%2520learned%2520policies.%2520More%2520specifically%252C%2520an%2520RL%2520agent%250Athat%2520trains%2520in%2520a%2520certain%2520Markov%2520decision%2520process%2520%2528MDP%2529%2520often%2520struggles%2520to%250Aperform%2520well%2520in%2520nearly%2520identical%2520MDPs.%2520To%2520address%2520this%2520issue%252C%2520we%2520employ%2520the%250Aframework%2520of%2520Robust%2520MDPs%2520%2528RMDPs%2529%2520in%2520a%2520model-based%2520setting%2520and%2520introduce%2520a%2520novel%250Alearned%2520transition%2520model.%2520Our%2520method%2520specifically%2520incorporates%2520an%2520auxiliary%250Apessimistic%2520model%252C%2520updated%2520adversarially%252C%2520to%2520estimate%2520the%2520worst-case%2520MDP%2520within%250Aa%2520Kullback-Leibler%2520uncertainty%2520set.%2520In%2520comparison%2520to%2520several%2520existing%2520works%252C%250Aour%2520work%2520does%2520not%2520impose%2520any%2520additional%2520conditions%2520on%2520the%2520training%2520environment%252C%250Asuch%2520as%2520the%2520need%2520for%2520a%2520parametric%2520simulator.%2520To%2520test%2520the%2520effectiveness%2520of%2520the%250Aproposed%2520pessimistic%2520model%2520in%2520enhancing%2520policy%2520robustness%252C%2520we%2520integrate%2520it%2520into%250Aa%2520practical%2520RL%2520algorithm%252C%2520called%2520Robust%2520Model-Based%2520Policy%2520Optimization%250A%2528RMBPO%2529.%2520Our%2520experimental%2520results%2520indicate%2520a%2520notable%2520improvement%2520in%2520policy%250Arobustness%2520on%2520high-dimensional%2520MuJoCo%2520control%2520tasks%252C%2520with%2520the%2520auxiliary%2520model%250Aenhancing%2520the%2520performance%2520of%2520the%2520learned%2520policy%2520in%2520distorted%2520MDPs.%2520We%2520further%250Aexplore%2520the%2520learned%2520deviation%2520between%2520the%2520proposed%2520auxiliary%2520world%2520model%2520and%250Athe%2520nominal%2520model%252C%2520to%2520examine%2520how%2520pessimism%2520is%2520achieved.%2520By%2520learning%2520a%250Apessimistic%2520world%2520model%2520and%2520demonstrating%2520its%2520role%2520in%2520improving%2520policy%250Arobustness%252C%2520our%2520research%2520contributes%2520towards%2520making%2520%2528model-based%2529%2520RL%2520more%250Arobust.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09976v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Model-Based%20Reinforcement%20Learning%20with%20an%20Adversarial%20Auxiliary%0A%20%20Model&entry.906535625=Siemen%20Herremans%20and%20Ali%20Anwar%20and%20Siegfried%20Mercelis&entry.1292438233=%20%20Reinforcement%20learning%20has%20demonstrated%20impressive%20performance%20in%20various%0Achallenging%20problems%20such%20as%20robotics%2C%20board%20games%2C%20and%20classical%20arcade%20games.%0AHowever%2C%20its%20real-world%20applications%20can%20be%20hindered%20by%20the%20absence%20of%0Arobustness%20and%20safety%20in%20the%20learned%20policies.%20More%20specifically%2C%20an%20RL%20agent%0Athat%20trains%20in%20a%20certain%20Markov%20decision%20process%20%28MDP%29%20often%20struggles%20to%0Aperform%20well%20in%20nearly%20identical%20MDPs.%20To%20address%20this%20issue%2C%20we%20employ%20the%0Aframework%20of%20Robust%20MDPs%20%28RMDPs%29%20in%20a%20model-based%20setting%20and%20introduce%20a%20novel%0Alearned%20transition%20model.%20Our%20method%20specifically%20incorporates%20an%20auxiliary%0Apessimistic%20model%2C%20updated%20adversarially%2C%20to%20estimate%20the%20worst-case%20MDP%20within%0Aa%20Kullback-Leibler%20uncertainty%20set.%20In%20comparison%20to%20several%20existing%20works%2C%0Aour%20work%20does%20not%20impose%20any%20additional%20conditions%20on%20the%20training%20environment%2C%0Asuch%20as%20the%20need%20for%20a%20parametric%20simulator.%20To%20test%20the%20effectiveness%20of%20the%0Aproposed%20pessimistic%20model%20in%20enhancing%20policy%20robustness%2C%20we%20integrate%20it%20into%0Aa%20practical%20RL%20algorithm%2C%20called%20Robust%20Model-Based%20Policy%20Optimization%0A%28RMBPO%29.%20Our%20experimental%20results%20indicate%20a%20notable%20improvement%20in%20policy%0Arobustness%20on%20high-dimensional%20MuJoCo%20control%20tasks%2C%20with%20the%20auxiliary%20model%0Aenhancing%20the%20performance%20of%20the%20learned%20policy%20in%20distorted%20MDPs.%20We%20further%0Aexplore%20the%20learned%20deviation%20between%20the%20proposed%20auxiliary%20world%20model%20and%0Athe%20nominal%20model%2C%20to%20examine%20how%20pessimism%20is%20achieved.%20By%20learning%20a%0Apessimistic%20world%20model%20and%20demonstrating%20its%20role%20in%20improving%20policy%0Arobustness%2C%20our%20research%20contributes%20towards%20making%20%28model-based%29%20RL%20more%0Arobust.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09976v2&entry.124074799=Read"},
{"title": "Patch-Prompt Aligned Bayesian Prompt Tuning for Vision-Language Models", "author": "Xinyang Liu and Dongsheng Wang and Bowei Fang and Miaoge Li and Zhibin Duan and Yishi Xu and Bo Chen and Mingyuan Zhou", "abstract": "  For downstream applications of vision-language pre-trained models, there has\nbeen significant interest in constructing effective prompts. Existing works on\nprompt engineering, which either require laborious manual designs or optimize\nthe prompt tuning as a point estimation problem, may fail to describe diverse\ncharacteristics of categories and limit their applications. We introduce a\nBayesian probabilistic resolution to prompt tuning, where the label-specific\nstochastic prompts are generated hierarchically by first sampling a latent\nvector from an underlying distribution and then employing a lightweight\ngenerative model. Importantly, we semantically regularize the tuning process by\nminimizing the statistical distance between the visual patches and linguistic\nprompts, which pushes the stochastic label representations to faithfully\ncapture diverse visual concepts, instead of overfitting the training\ncategories. We evaluate the effectiveness of our approach on four tasks:\nfew-shot image recognition, base-to-new generalization, dataset transfer\nlearning, and domain shifts. Extensive results over 15 datasets show promising\ntransferability and generalization performance of our proposed model, both\nquantitatively and qualitatively.\n", "link": "http://arxiv.org/abs/2303.09100v2", "date": "2024-07-01", "relevancy": 1.0338, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5305}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5242}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.496}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Patch-Prompt%20Aligned%20Bayesian%20Prompt%20Tuning%20for%20Vision-Language%20Models&body=Title%3A%20Patch-Prompt%20Aligned%20Bayesian%20Prompt%20Tuning%20for%20Vision-Language%20Models%0AAuthor%3A%20Xinyang%20Liu%20and%20Dongsheng%20Wang%20and%20Bowei%20Fang%20and%20Miaoge%20Li%20and%20Zhibin%20Duan%20and%20Yishi%20Xu%20and%20Bo%20Chen%20and%20Mingyuan%20Zhou%0AAbstract%3A%20%20%20For%20downstream%20applications%20of%20vision-language%20pre-trained%20models%2C%20there%20has%0Abeen%20significant%20interest%20in%20constructing%20effective%20prompts.%20Existing%20works%20on%0Aprompt%20engineering%2C%20which%20either%20require%20laborious%20manual%20designs%20or%20optimize%0Athe%20prompt%20tuning%20as%20a%20point%20estimation%20problem%2C%20may%20fail%20to%20describe%20diverse%0Acharacteristics%20of%20categories%20and%20limit%20their%20applications.%20We%20introduce%20a%0ABayesian%20probabilistic%20resolution%20to%20prompt%20tuning%2C%20where%20the%20label-specific%0Astochastic%20prompts%20are%20generated%20hierarchically%20by%20first%20sampling%20a%20latent%0Avector%20from%20an%20underlying%20distribution%20and%20then%20employing%20a%20lightweight%0Agenerative%20model.%20Importantly%2C%20we%20semantically%20regularize%20the%20tuning%20process%20by%0Aminimizing%20the%20statistical%20distance%20between%20the%20visual%20patches%20and%20linguistic%0Aprompts%2C%20which%20pushes%20the%20stochastic%20label%20representations%20to%20faithfully%0Acapture%20diverse%20visual%20concepts%2C%20instead%20of%20overfitting%20the%20training%0Acategories.%20We%20evaluate%20the%20effectiveness%20of%20our%20approach%20on%20four%20tasks%3A%0Afew-shot%20image%20recognition%2C%20base-to-new%20generalization%2C%20dataset%20transfer%0Alearning%2C%20and%20domain%20shifts.%20Extensive%20results%20over%2015%20datasets%20show%20promising%0Atransferability%20and%20generalization%20performance%20of%20our%20proposed%20model%2C%20both%0Aquantitatively%20and%20qualitatively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.09100v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPatch-Prompt%2520Aligned%2520Bayesian%2520Prompt%2520Tuning%2520for%2520Vision-Language%2520Models%26entry.906535625%3DXinyang%2520Liu%2520and%2520Dongsheng%2520Wang%2520and%2520Bowei%2520Fang%2520and%2520Miaoge%2520Li%2520and%2520Zhibin%2520Duan%2520and%2520Yishi%2520Xu%2520and%2520Bo%2520Chen%2520and%2520Mingyuan%2520Zhou%26entry.1292438233%3D%2520%2520For%2520downstream%2520applications%2520of%2520vision-language%2520pre-trained%2520models%252C%2520there%2520has%250Abeen%2520significant%2520interest%2520in%2520constructing%2520effective%2520prompts.%2520Existing%2520works%2520on%250Aprompt%2520engineering%252C%2520which%2520either%2520require%2520laborious%2520manual%2520designs%2520or%2520optimize%250Athe%2520prompt%2520tuning%2520as%2520a%2520point%2520estimation%2520problem%252C%2520may%2520fail%2520to%2520describe%2520diverse%250Acharacteristics%2520of%2520categories%2520and%2520limit%2520their%2520applications.%2520We%2520introduce%2520a%250ABayesian%2520probabilistic%2520resolution%2520to%2520prompt%2520tuning%252C%2520where%2520the%2520label-specific%250Astochastic%2520prompts%2520are%2520generated%2520hierarchically%2520by%2520first%2520sampling%2520a%2520latent%250Avector%2520from%2520an%2520underlying%2520distribution%2520and%2520then%2520employing%2520a%2520lightweight%250Agenerative%2520model.%2520Importantly%252C%2520we%2520semantically%2520regularize%2520the%2520tuning%2520process%2520by%250Aminimizing%2520the%2520statistical%2520distance%2520between%2520the%2520visual%2520patches%2520and%2520linguistic%250Aprompts%252C%2520which%2520pushes%2520the%2520stochastic%2520label%2520representations%2520to%2520faithfully%250Acapture%2520diverse%2520visual%2520concepts%252C%2520instead%2520of%2520overfitting%2520the%2520training%250Acategories.%2520We%2520evaluate%2520the%2520effectiveness%2520of%2520our%2520approach%2520on%2520four%2520tasks%253A%250Afew-shot%2520image%2520recognition%252C%2520base-to-new%2520generalization%252C%2520dataset%2520transfer%250Alearning%252C%2520and%2520domain%2520shifts.%2520Extensive%2520results%2520over%252015%2520datasets%2520show%2520promising%250Atransferability%2520and%2520generalization%2520performance%2520of%2520our%2520proposed%2520model%252C%2520both%250Aquantitatively%2520and%2520qualitatively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.09100v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Patch-Prompt%20Aligned%20Bayesian%20Prompt%20Tuning%20for%20Vision-Language%20Models&entry.906535625=Xinyang%20Liu%20and%20Dongsheng%20Wang%20and%20Bowei%20Fang%20and%20Miaoge%20Li%20and%20Zhibin%20Duan%20and%20Yishi%20Xu%20and%20Bo%20Chen%20and%20Mingyuan%20Zhou&entry.1292438233=%20%20For%20downstream%20applications%20of%20vision-language%20pre-trained%20models%2C%20there%20has%0Abeen%20significant%20interest%20in%20constructing%20effective%20prompts.%20Existing%20works%20on%0Aprompt%20engineering%2C%20which%20either%20require%20laborious%20manual%20designs%20or%20optimize%0Athe%20prompt%20tuning%20as%20a%20point%20estimation%20problem%2C%20may%20fail%20to%20describe%20diverse%0Acharacteristics%20of%20categories%20and%20limit%20their%20applications.%20We%20introduce%20a%0ABayesian%20probabilistic%20resolution%20to%20prompt%20tuning%2C%20where%20the%20label-specific%0Astochastic%20prompts%20are%20generated%20hierarchically%20by%20first%20sampling%20a%20latent%0Avector%20from%20an%20underlying%20distribution%20and%20then%20employing%20a%20lightweight%0Agenerative%20model.%20Importantly%2C%20we%20semantically%20regularize%20the%20tuning%20process%20by%0Aminimizing%20the%20statistical%20distance%20between%20the%20visual%20patches%20and%20linguistic%0Aprompts%2C%20which%20pushes%20the%20stochastic%20label%20representations%20to%20faithfully%0Acapture%20diverse%20visual%20concepts%2C%20instead%20of%20overfitting%20the%20training%0Acategories.%20We%20evaluate%20the%20effectiveness%20of%20our%20approach%20on%20four%20tasks%3A%0Afew-shot%20image%20recognition%2C%20base-to-new%20generalization%2C%20dataset%20transfer%0Alearning%2C%20and%20domain%20shifts.%20Extensive%20results%20over%2015%20datasets%20show%20promising%0Atransferability%20and%20generalization%20performance%20of%20our%20proposed%20model%2C%20both%0Aquantitatively%20and%20qualitatively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.09100v2&entry.124074799=Read"},
{"title": "Does Writing with Language Models Reduce Content Diversity?", "author": "Vishakh Padmakumar and He He", "abstract": "  Large language models (LLMs) have led to a surge in collaborative writing\nwith model assistance. As different users incorporate suggestions from the same\nmodel, there is a risk of decreased diversity in the produced content,\npotentially limiting diverse perspectives in public discourse. In this work, we\nmeasure the impact of co-writing on diversity via a controlled experiment,\nwhere users write argumentative essays in three setups -- using a base LLM\n(GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We\ndevelop a set of diversity metrics and find that writing with InstructGPT (but\nnot the GPT3) results in a statistically significant reduction in diversity.\nSpecifically, it increases the similarity between the writings of different\nauthors and reduces the overall lexical and content diversity. We additionally\nfind that this effect is mainly attributable to InstructGPT contributing less\ndiverse text to co-written essays. In contrast, the user-contributed text\nremains unaffected by model collaboration. This suggests that the recent\nimprovement in generation quality from adapting models to human feedback might\ncome at the cost of more homogeneous and less diverse content.\n", "link": "http://arxiv.org/abs/2309.05196v3", "date": "2024-07-01", "relevancy": 0.8521, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.433}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4248}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4204}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Does%20Writing%20with%20Language%20Models%20Reduce%20Content%20Diversity%3F&body=Title%3A%20Does%20Writing%20with%20Language%20Models%20Reduce%20Content%20Diversity%3F%0AAuthor%3A%20Vishakh%20Padmakumar%20and%20He%20He%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20led%20to%20a%20surge%20in%20collaborative%20writing%0Awith%20model%20assistance.%20As%20different%20users%20incorporate%20suggestions%20from%20the%20same%0Amodel%2C%20there%20is%20a%20risk%20of%20decreased%20diversity%20in%20the%20produced%20content%2C%0Apotentially%20limiting%20diverse%20perspectives%20in%20public%20discourse.%20In%20this%20work%2C%20we%0Ameasure%20the%20impact%20of%20co-writing%20on%20diversity%20via%20a%20controlled%20experiment%2C%0Awhere%20users%20write%20argumentative%20essays%20in%20three%20setups%20--%20using%20a%20base%20LLM%0A%28GPT3%29%2C%20a%20feedback-tuned%20LLM%20%28InstructGPT%29%2C%20and%20writing%20without%20model%20help.%20We%0Adevelop%20a%20set%20of%20diversity%20metrics%20and%20find%20that%20writing%20with%20InstructGPT%20%28but%0Anot%20the%20GPT3%29%20results%20in%20a%20statistically%20significant%20reduction%20in%20diversity.%0ASpecifically%2C%20it%20increases%20the%20similarity%20between%20the%20writings%20of%20different%0Aauthors%20and%20reduces%20the%20overall%20lexical%20and%20content%20diversity.%20We%20additionally%0Afind%20that%20this%20effect%20is%20mainly%20attributable%20to%20InstructGPT%20contributing%20less%0Adiverse%20text%20to%20co-written%20essays.%20In%20contrast%2C%20the%20user-contributed%20text%0Aremains%20unaffected%20by%20model%20collaboration.%20This%20suggests%20that%20the%20recent%0Aimprovement%20in%20generation%20quality%20from%20adapting%20models%20to%20human%20feedback%20might%0Acome%20at%20the%20cost%20of%20more%20homogeneous%20and%20less%20diverse%20content.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.05196v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDoes%2520Writing%2520with%2520Language%2520Models%2520Reduce%2520Content%2520Diversity%253F%26entry.906535625%3DVishakh%2520Padmakumar%2520and%2520He%2520He%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520led%2520to%2520a%2520surge%2520in%2520collaborative%2520writing%250Awith%2520model%2520assistance.%2520As%2520different%2520users%2520incorporate%2520suggestions%2520from%2520the%2520same%250Amodel%252C%2520there%2520is%2520a%2520risk%2520of%2520decreased%2520diversity%2520in%2520the%2520produced%2520content%252C%250Apotentially%2520limiting%2520diverse%2520perspectives%2520in%2520public%2520discourse.%2520In%2520this%2520work%252C%2520we%250Ameasure%2520the%2520impact%2520of%2520co-writing%2520on%2520diversity%2520via%2520a%2520controlled%2520experiment%252C%250Awhere%2520users%2520write%2520argumentative%2520essays%2520in%2520three%2520setups%2520--%2520using%2520a%2520base%2520LLM%250A%2528GPT3%2529%252C%2520a%2520feedback-tuned%2520LLM%2520%2528InstructGPT%2529%252C%2520and%2520writing%2520without%2520model%2520help.%2520We%250Adevelop%2520a%2520set%2520of%2520diversity%2520metrics%2520and%2520find%2520that%2520writing%2520with%2520InstructGPT%2520%2528but%250Anot%2520the%2520GPT3%2529%2520results%2520in%2520a%2520statistically%2520significant%2520reduction%2520in%2520diversity.%250ASpecifically%252C%2520it%2520increases%2520the%2520similarity%2520between%2520the%2520writings%2520of%2520different%250Aauthors%2520and%2520reduces%2520the%2520overall%2520lexical%2520and%2520content%2520diversity.%2520We%2520additionally%250Afind%2520that%2520this%2520effect%2520is%2520mainly%2520attributable%2520to%2520InstructGPT%2520contributing%2520less%250Adiverse%2520text%2520to%2520co-written%2520essays.%2520In%2520contrast%252C%2520the%2520user-contributed%2520text%250Aremains%2520unaffected%2520by%2520model%2520collaboration.%2520This%2520suggests%2520that%2520the%2520recent%250Aimprovement%2520in%2520generation%2520quality%2520from%2520adapting%2520models%2520to%2520human%2520feedback%2520might%250Acome%2520at%2520the%2520cost%2520of%2520more%2520homogeneous%2520and%2520less%2520diverse%2520content.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.05196v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Does%20Writing%20with%20Language%20Models%20Reduce%20Content%20Diversity%3F&entry.906535625=Vishakh%20Padmakumar%20and%20He%20He&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20led%20to%20a%20surge%20in%20collaborative%20writing%0Awith%20model%20assistance.%20As%20different%20users%20incorporate%20suggestions%20from%20the%20same%0Amodel%2C%20there%20is%20a%20risk%20of%20decreased%20diversity%20in%20the%20produced%20content%2C%0Apotentially%20limiting%20diverse%20perspectives%20in%20public%20discourse.%20In%20this%20work%2C%20we%0Ameasure%20the%20impact%20of%20co-writing%20on%20diversity%20via%20a%20controlled%20experiment%2C%0Awhere%20users%20write%20argumentative%20essays%20in%20three%20setups%20--%20using%20a%20base%20LLM%0A%28GPT3%29%2C%20a%20feedback-tuned%20LLM%20%28InstructGPT%29%2C%20and%20writing%20without%20model%20help.%20We%0Adevelop%20a%20set%20of%20diversity%20metrics%20and%20find%20that%20writing%20with%20InstructGPT%20%28but%0Anot%20the%20GPT3%29%20results%20in%20a%20statistically%20significant%20reduction%20in%20diversity.%0ASpecifically%2C%20it%20increases%20the%20similarity%20between%20the%20writings%20of%20different%0Aauthors%20and%20reduces%20the%20overall%20lexical%20and%20content%20diversity.%20We%20additionally%0Afind%20that%20this%20effect%20is%20mainly%20attributable%20to%20InstructGPT%20contributing%20less%0Adiverse%20text%20to%20co-written%20essays.%20In%20contrast%2C%20the%20user-contributed%20text%0Aremains%20unaffected%20by%20model%20collaboration.%20This%20suggests%20that%20the%20recent%0Aimprovement%20in%20generation%20quality%20from%20adapting%20models%20to%20human%20feedback%20might%0Acome%20at%20the%20cost%20of%20more%20homogeneous%20and%20less%20diverse%20content.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.05196v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


