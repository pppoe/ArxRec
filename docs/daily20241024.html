<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20241023.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "VR-Splatting: Foveated Radiance Field Rendering via 3D Gaussian\n  Splatting and Neural Points", "author": "Linus Franke and Laura Fink and Marc Stamminger", "abstract": "  Recent advances in novel view synthesis (NVS), particularly neural radiance\nfields (NeRF) and Gaussian splatting (3DGS), have demonstrated impressive\nresults in photorealistic scene rendering. These techniques hold great\npotential for applications in virtual tourism and teleportation, where\nimmersive realism is crucial. However, the high-performance demands of virtual\nreality (VR) systems present challenges in directly utilizing even such\nfast-to-render scene representations like 3DGS due to latency and computational\nconstraints.\n  In this paper, we propose foveated rendering as a promising solution to these\nobstacles. We analyze state-of-the-art NVS methods with respect to their\nrendering performance and compatibility with the human visual system. Our\napproach introduces a novel foveated rendering approach for Virtual Reality,\nthat leverages the sharp, detailed output of neural point rendering for the\nfoveal region, fused with a smooth rendering of 3DGS for the peripheral vision.\n  Our evaluation confirms that perceived sharpness and detail-richness are\nincreased by our approach compared to a standard VR-ready 3DGS configuration.\nOur system meets the necessary performance requirements for real-time VR\ninteractions, ultimately enhancing the user's immersive experience.\n  Project page: https://lfranke.github.io/vr_splatting\n", "link": "http://arxiv.org/abs/2410.17932v1", "date": "2024-10-23", "relevancy": 3.1338, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6811}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6033}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5959}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VR-Splatting%3A%20Foveated%20Radiance%20Field%20Rendering%20via%203D%20Gaussian%0A%20%20Splatting%20and%20Neural%20Points&body=Title%3A%20VR-Splatting%3A%20Foveated%20Radiance%20Field%20Rendering%20via%203D%20Gaussian%0A%20%20Splatting%20and%20Neural%20Points%0AAuthor%3A%20Linus%20Franke%20and%20Laura%20Fink%20and%20Marc%20Stamminger%0AAbstract%3A%20%20%20Recent%20advances%20in%20novel%20view%20synthesis%20%28NVS%29%2C%20particularly%20neural%20radiance%0Afields%20%28NeRF%29%20and%20Gaussian%20splatting%20%283DGS%29%2C%20have%20demonstrated%20impressive%0Aresults%20in%20photorealistic%20scene%20rendering.%20These%20techniques%20hold%20great%0Apotential%20for%20applications%20in%20virtual%20tourism%20and%20teleportation%2C%20where%0Aimmersive%20realism%20is%20crucial.%20However%2C%20the%20high-performance%20demands%20of%20virtual%0Areality%20%28VR%29%20systems%20present%20challenges%20in%20directly%20utilizing%20even%20such%0Afast-to-render%20scene%20representations%20like%203DGS%20due%20to%20latency%20and%20computational%0Aconstraints.%0A%20%20In%20this%20paper%2C%20we%20propose%20foveated%20rendering%20as%20a%20promising%20solution%20to%20these%0Aobstacles.%20We%20analyze%20state-of-the-art%20NVS%20methods%20with%20respect%20to%20their%0Arendering%20performance%20and%20compatibility%20with%20the%20human%20visual%20system.%20Our%0Aapproach%20introduces%20a%20novel%20foveated%20rendering%20approach%20for%20Virtual%20Reality%2C%0Athat%20leverages%20the%20sharp%2C%20detailed%20output%20of%20neural%20point%20rendering%20for%20the%0Afoveal%20region%2C%20fused%20with%20a%20smooth%20rendering%20of%203DGS%20for%20the%20peripheral%20vision.%0A%20%20Our%20evaluation%20confirms%20that%20perceived%20sharpness%20and%20detail-richness%20are%0Aincreased%20by%20our%20approach%20compared%20to%20a%20standard%20VR-ready%203DGS%20configuration.%0AOur%20system%20meets%20the%20necessary%20performance%20requirements%20for%20real-time%20VR%0Ainteractions%2C%20ultimately%20enhancing%20the%20user%27s%20immersive%20experience.%0A%20%20Project%20page%3A%20https%3A//lfranke.github.io/vr_splatting%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17932v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVR-Splatting%253A%2520Foveated%2520Radiance%2520Field%2520Rendering%2520via%25203D%2520Gaussian%250A%2520%2520Splatting%2520and%2520Neural%2520Points%26entry.906535625%3DLinus%2520Franke%2520and%2520Laura%2520Fink%2520and%2520Marc%2520Stamminger%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520novel%2520view%2520synthesis%2520%2528NVS%2529%252C%2520particularly%2520neural%2520radiance%250Afields%2520%2528NeRF%2529%2520and%2520Gaussian%2520splatting%2520%25283DGS%2529%252C%2520have%2520demonstrated%2520impressive%250Aresults%2520in%2520photorealistic%2520scene%2520rendering.%2520These%2520techniques%2520hold%2520great%250Apotential%2520for%2520applications%2520in%2520virtual%2520tourism%2520and%2520teleportation%252C%2520where%250Aimmersive%2520realism%2520is%2520crucial.%2520However%252C%2520the%2520high-performance%2520demands%2520of%2520virtual%250Areality%2520%2528VR%2529%2520systems%2520present%2520challenges%2520in%2520directly%2520utilizing%2520even%2520such%250Afast-to-render%2520scene%2520representations%2520like%25203DGS%2520due%2520to%2520latency%2520and%2520computational%250Aconstraints.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520propose%2520foveated%2520rendering%2520as%2520a%2520promising%2520solution%2520to%2520these%250Aobstacles.%2520We%2520analyze%2520state-of-the-art%2520NVS%2520methods%2520with%2520respect%2520to%2520their%250Arendering%2520performance%2520and%2520compatibility%2520with%2520the%2520human%2520visual%2520system.%2520Our%250Aapproach%2520introduces%2520a%2520novel%2520foveated%2520rendering%2520approach%2520for%2520Virtual%2520Reality%252C%250Athat%2520leverages%2520the%2520sharp%252C%2520detailed%2520output%2520of%2520neural%2520point%2520rendering%2520for%2520the%250Afoveal%2520region%252C%2520fused%2520with%2520a%2520smooth%2520rendering%2520of%25203DGS%2520for%2520the%2520peripheral%2520vision.%250A%2520%2520Our%2520evaluation%2520confirms%2520that%2520perceived%2520sharpness%2520and%2520detail-richness%2520are%250Aincreased%2520by%2520our%2520approach%2520compared%2520to%2520a%2520standard%2520VR-ready%25203DGS%2520configuration.%250AOur%2520system%2520meets%2520the%2520necessary%2520performance%2520requirements%2520for%2520real-time%2520VR%250Ainteractions%252C%2520ultimately%2520enhancing%2520the%2520user%2527s%2520immersive%2520experience.%250A%2520%2520Project%2520page%253A%2520https%253A//lfranke.github.io/vr_splatting%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17932v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VR-Splatting%3A%20Foveated%20Radiance%20Field%20Rendering%20via%203D%20Gaussian%0A%20%20Splatting%20and%20Neural%20Points&entry.906535625=Linus%20Franke%20and%20Laura%20Fink%20and%20Marc%20Stamminger&entry.1292438233=%20%20Recent%20advances%20in%20novel%20view%20synthesis%20%28NVS%29%2C%20particularly%20neural%20radiance%0Afields%20%28NeRF%29%20and%20Gaussian%20splatting%20%283DGS%29%2C%20have%20demonstrated%20impressive%0Aresults%20in%20photorealistic%20scene%20rendering.%20These%20techniques%20hold%20great%0Apotential%20for%20applications%20in%20virtual%20tourism%20and%20teleportation%2C%20where%0Aimmersive%20realism%20is%20crucial.%20However%2C%20the%20high-performance%20demands%20of%20virtual%0Areality%20%28VR%29%20systems%20present%20challenges%20in%20directly%20utilizing%20even%20such%0Afast-to-render%20scene%20representations%20like%203DGS%20due%20to%20latency%20and%20computational%0Aconstraints.%0A%20%20In%20this%20paper%2C%20we%20propose%20foveated%20rendering%20as%20a%20promising%20solution%20to%20these%0Aobstacles.%20We%20analyze%20state-of-the-art%20NVS%20methods%20with%20respect%20to%20their%0Arendering%20performance%20and%20compatibility%20with%20the%20human%20visual%20system.%20Our%0Aapproach%20introduces%20a%20novel%20foveated%20rendering%20approach%20for%20Virtual%20Reality%2C%0Athat%20leverages%20the%20sharp%2C%20detailed%20output%20of%20neural%20point%20rendering%20for%20the%0Afoveal%20region%2C%20fused%20with%20a%20smooth%20rendering%20of%203DGS%20for%20the%20peripheral%20vision.%0A%20%20Our%20evaluation%20confirms%20that%20perceived%20sharpness%20and%20detail-richness%20are%0Aincreased%20by%20our%20approach%20compared%20to%20a%20standard%20VR-ready%203DGS%20configuration.%0AOur%20system%20meets%20the%20necessary%20performance%20requirements%20for%20real-time%20VR%0Ainteractions%2C%20ultimately%20enhancing%20the%20user%27s%20immersive%20experience.%0A%20%20Project%20page%3A%20https%3A//lfranke.github.io/vr_splatting%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17932v1&entry.124074799=Read"},
{"title": "Efficient Neural Implicit Representation for 3D Human Reconstruction", "author": "Zexu Huang and Sarah Monazam Erfani and Siying Lu and Mingming Gong", "abstract": "  High-fidelity digital human representations are increasingly in demand in the\ndigital world, particularly for interactive telepresence, AR/VR, 3D graphics,\nand the rapidly evolving metaverse. Even though they work well in small spaces,\nconventional methods for reconstructing 3D human motion frequently require the\nuse of expensive hardware and have high processing costs. This study presents\nHumanAvatar, an innovative approach that efficiently reconstructs precise human\navatars from monocular video sources. At the core of our methodology, we\nintegrate the pre-trained HuMoR, a model celebrated for its proficiency in\nhuman motion estimation. This is adeptly fused with the cutting-edge neural\nradiance field technology, Instant-NGP, and the state-of-the-art articulated\nmodel, Fast-SNARF, to enhance the reconstruction fidelity and speed. By\ncombining these two technologies, a system is created that can render quickly\nand effectively while also providing estimation of human pose parameters that\nare unmatched in accuracy. We have enhanced our system with an advanced\nposture-sensitive space reduction technique, which optimally balances rendering\nquality with computational efficiency. In our detailed experimental analysis\nusing both artificial and real-world monocular videos, we establish the\nadvanced performance of our approach. HumanAvatar consistently equals or\nsurpasses contemporary leading-edge reconstruction techniques in quality.\nFurthermore, it achieves these complex reconstructions in minutes, a fraction\nof the time typically required by existing methods. Our models achieve a\ntraining speed that is 110X faster than that of State-of-The-Art (SoTA)\nNeRF-based models. Our technique performs noticeably better than SoTA dynamic\nhuman NeRF methods if given an identical runtime limit. HumanAvatar can provide\neffective visuals after only 30 seconds of training.\n", "link": "http://arxiv.org/abs/2410.17741v1", "date": "2024-10-23", "relevancy": 3.085, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6174}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6168}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6168}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Neural%20Implicit%20Representation%20for%203D%20Human%20Reconstruction&body=Title%3A%20Efficient%20Neural%20Implicit%20Representation%20for%203D%20Human%20Reconstruction%0AAuthor%3A%20Zexu%20Huang%20and%20Sarah%20Monazam%20Erfani%20and%20Siying%20Lu%20and%20Mingming%20Gong%0AAbstract%3A%20%20%20High-fidelity%20digital%20human%20representations%20are%20increasingly%20in%20demand%20in%20the%0Adigital%20world%2C%20particularly%20for%20interactive%20telepresence%2C%20AR/VR%2C%203D%20graphics%2C%0Aand%20the%20rapidly%20evolving%20metaverse.%20Even%20though%20they%20work%20well%20in%20small%20spaces%2C%0Aconventional%20methods%20for%20reconstructing%203D%20human%20motion%20frequently%20require%20the%0Ause%20of%20expensive%20hardware%20and%20have%20high%20processing%20costs.%20This%20study%20presents%0AHumanAvatar%2C%20an%20innovative%20approach%20that%20efficiently%20reconstructs%20precise%20human%0Aavatars%20from%20monocular%20video%20sources.%20At%20the%20core%20of%20our%20methodology%2C%20we%0Aintegrate%20the%20pre-trained%20HuMoR%2C%20a%20model%20celebrated%20for%20its%20proficiency%20in%0Ahuman%20motion%20estimation.%20This%20is%20adeptly%20fused%20with%20the%20cutting-edge%20neural%0Aradiance%20field%20technology%2C%20Instant-NGP%2C%20and%20the%20state-of-the-art%20articulated%0Amodel%2C%20Fast-SNARF%2C%20to%20enhance%20the%20reconstruction%20fidelity%20and%20speed.%20By%0Acombining%20these%20two%20technologies%2C%20a%20system%20is%20created%20that%20can%20render%20quickly%0Aand%20effectively%20while%20also%20providing%20estimation%20of%20human%20pose%20parameters%20that%0Aare%20unmatched%20in%20accuracy.%20We%20have%20enhanced%20our%20system%20with%20an%20advanced%0Aposture-sensitive%20space%20reduction%20technique%2C%20which%20optimally%20balances%20rendering%0Aquality%20with%20computational%20efficiency.%20In%20our%20detailed%20experimental%20analysis%0Ausing%20both%20artificial%20and%20real-world%20monocular%20videos%2C%20we%20establish%20the%0Aadvanced%20performance%20of%20our%20approach.%20HumanAvatar%20consistently%20equals%20or%0Asurpasses%20contemporary%20leading-edge%20reconstruction%20techniques%20in%20quality.%0AFurthermore%2C%20it%20achieves%20these%20complex%20reconstructions%20in%20minutes%2C%20a%20fraction%0Aof%20the%20time%20typically%20required%20by%20existing%20methods.%20Our%20models%20achieve%20a%0Atraining%20speed%20that%20is%20110X%20faster%20than%20that%20of%20State-of-The-Art%20%28SoTA%29%0ANeRF-based%20models.%20Our%20technique%20performs%20noticeably%20better%20than%20SoTA%20dynamic%0Ahuman%20NeRF%20methods%20if%20given%20an%20identical%20runtime%20limit.%20HumanAvatar%20can%20provide%0Aeffective%20visuals%20after%20only%2030%20seconds%20of%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17741v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Neural%2520Implicit%2520Representation%2520for%25203D%2520Human%2520Reconstruction%26entry.906535625%3DZexu%2520Huang%2520and%2520Sarah%2520Monazam%2520Erfani%2520and%2520Siying%2520Lu%2520and%2520Mingming%2520Gong%26entry.1292438233%3D%2520%2520High-fidelity%2520digital%2520human%2520representations%2520are%2520increasingly%2520in%2520demand%2520in%2520the%250Adigital%2520world%252C%2520particularly%2520for%2520interactive%2520telepresence%252C%2520AR/VR%252C%25203D%2520graphics%252C%250Aand%2520the%2520rapidly%2520evolving%2520metaverse.%2520Even%2520though%2520they%2520work%2520well%2520in%2520small%2520spaces%252C%250Aconventional%2520methods%2520for%2520reconstructing%25203D%2520human%2520motion%2520frequently%2520require%2520the%250Ause%2520of%2520expensive%2520hardware%2520and%2520have%2520high%2520processing%2520costs.%2520This%2520study%2520presents%250AHumanAvatar%252C%2520an%2520innovative%2520approach%2520that%2520efficiently%2520reconstructs%2520precise%2520human%250Aavatars%2520from%2520monocular%2520video%2520sources.%2520At%2520the%2520core%2520of%2520our%2520methodology%252C%2520we%250Aintegrate%2520the%2520pre-trained%2520HuMoR%252C%2520a%2520model%2520celebrated%2520for%2520its%2520proficiency%2520in%250Ahuman%2520motion%2520estimation.%2520This%2520is%2520adeptly%2520fused%2520with%2520the%2520cutting-edge%2520neural%250Aradiance%2520field%2520technology%252C%2520Instant-NGP%252C%2520and%2520the%2520state-of-the-art%2520articulated%250Amodel%252C%2520Fast-SNARF%252C%2520to%2520enhance%2520the%2520reconstruction%2520fidelity%2520and%2520speed.%2520By%250Acombining%2520these%2520two%2520technologies%252C%2520a%2520system%2520is%2520created%2520that%2520can%2520render%2520quickly%250Aand%2520effectively%2520while%2520also%2520providing%2520estimation%2520of%2520human%2520pose%2520parameters%2520that%250Aare%2520unmatched%2520in%2520accuracy.%2520We%2520have%2520enhanced%2520our%2520system%2520with%2520an%2520advanced%250Aposture-sensitive%2520space%2520reduction%2520technique%252C%2520which%2520optimally%2520balances%2520rendering%250Aquality%2520with%2520computational%2520efficiency.%2520In%2520our%2520detailed%2520experimental%2520analysis%250Ausing%2520both%2520artificial%2520and%2520real-world%2520monocular%2520videos%252C%2520we%2520establish%2520the%250Aadvanced%2520performance%2520of%2520our%2520approach.%2520HumanAvatar%2520consistently%2520equals%2520or%250Asurpasses%2520contemporary%2520leading-edge%2520reconstruction%2520techniques%2520in%2520quality.%250AFurthermore%252C%2520it%2520achieves%2520these%2520complex%2520reconstructions%2520in%2520minutes%252C%2520a%2520fraction%250Aof%2520the%2520time%2520typically%2520required%2520by%2520existing%2520methods.%2520Our%2520models%2520achieve%2520a%250Atraining%2520speed%2520that%2520is%2520110X%2520faster%2520than%2520that%2520of%2520State-of-The-Art%2520%2528SoTA%2529%250ANeRF-based%2520models.%2520Our%2520technique%2520performs%2520noticeably%2520better%2520than%2520SoTA%2520dynamic%250Ahuman%2520NeRF%2520methods%2520if%2520given%2520an%2520identical%2520runtime%2520limit.%2520HumanAvatar%2520can%2520provide%250Aeffective%2520visuals%2520after%2520only%252030%2520seconds%2520of%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17741v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Neural%20Implicit%20Representation%20for%203D%20Human%20Reconstruction&entry.906535625=Zexu%20Huang%20and%20Sarah%20Monazam%20Erfani%20and%20Siying%20Lu%20and%20Mingming%20Gong&entry.1292438233=%20%20High-fidelity%20digital%20human%20representations%20are%20increasingly%20in%20demand%20in%20the%0Adigital%20world%2C%20particularly%20for%20interactive%20telepresence%2C%20AR/VR%2C%203D%20graphics%2C%0Aand%20the%20rapidly%20evolving%20metaverse.%20Even%20though%20they%20work%20well%20in%20small%20spaces%2C%0Aconventional%20methods%20for%20reconstructing%203D%20human%20motion%20frequently%20require%20the%0Ause%20of%20expensive%20hardware%20and%20have%20high%20processing%20costs.%20This%20study%20presents%0AHumanAvatar%2C%20an%20innovative%20approach%20that%20efficiently%20reconstructs%20precise%20human%0Aavatars%20from%20monocular%20video%20sources.%20At%20the%20core%20of%20our%20methodology%2C%20we%0Aintegrate%20the%20pre-trained%20HuMoR%2C%20a%20model%20celebrated%20for%20its%20proficiency%20in%0Ahuman%20motion%20estimation.%20This%20is%20adeptly%20fused%20with%20the%20cutting-edge%20neural%0Aradiance%20field%20technology%2C%20Instant-NGP%2C%20and%20the%20state-of-the-art%20articulated%0Amodel%2C%20Fast-SNARF%2C%20to%20enhance%20the%20reconstruction%20fidelity%20and%20speed.%20By%0Acombining%20these%20two%20technologies%2C%20a%20system%20is%20created%20that%20can%20render%20quickly%0Aand%20effectively%20while%20also%20providing%20estimation%20of%20human%20pose%20parameters%20that%0Aare%20unmatched%20in%20accuracy.%20We%20have%20enhanced%20our%20system%20with%20an%20advanced%0Aposture-sensitive%20space%20reduction%20technique%2C%20which%20optimally%20balances%20rendering%0Aquality%20with%20computational%20efficiency.%20In%20our%20detailed%20experimental%20analysis%0Ausing%20both%20artificial%20and%20real-world%20monocular%20videos%2C%20we%20establish%20the%0Aadvanced%20performance%20of%20our%20approach.%20HumanAvatar%20consistently%20equals%20or%0Asurpasses%20contemporary%20leading-edge%20reconstruction%20techniques%20in%20quality.%0AFurthermore%2C%20it%20achieves%20these%20complex%20reconstructions%20in%20minutes%2C%20a%20fraction%0Aof%20the%20time%20typically%20required%20by%20existing%20methods.%20Our%20models%20achieve%20a%0Atraining%20speed%20that%20is%20110X%20faster%20than%20that%20of%20State-of-The-Art%20%28SoTA%29%0ANeRF-based%20models.%20Our%20technique%20performs%20noticeably%20better%20than%20SoTA%20dynamic%0Ahuman%20NeRF%20methods%20if%20given%20an%20identical%20runtime%20limit.%20HumanAvatar%20can%20provide%0Aeffective%20visuals%20after%20only%2030%20seconds%20of%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17741v1&entry.124074799=Read"},
{"title": "GenUDC: High Quality 3D Mesh Generation with Unsigned Dual Contouring\n  Representation", "author": "Ruowei Wang and Jiaqi Li and Dan Zeng and Xueqi Ma and Zixiang Xu and Jianwei Zhang and Qijun Zhao", "abstract": "  Generating high-quality meshes with complex structures and realistic surfaces\nis the primary goal of 3D generative models. Existing methods typically employ\nsequence data or deformable tetrahedral grids for mesh generation. However,\nsequence-based methods have difficulty producing complex structures with many\nfaces due to memory limits. The deformable tetrahedral grid-based method\nMeshDiffusion fails to recover realistic surfaces due to the inherent ambiguity\nin deformable grids. We propose the GenUDC framework to address these\nchallenges by leveraging the Unsigned Dual Contouring (UDC) as the mesh\nrepresentation. UDC discretizes a mesh in a regular grid and divides it into\nthe face and vertex parts, recovering both complex structures and fine details.\nAs a result, the one-to-one mapping between UDC and mesh resolves the ambiguity\nproblem. In addition, GenUDC adopts a two-stage, coarse-to-fine generative\nprocess for 3D mesh generation. It first generates the face part as a rough\nshape and then the vertex part to craft a detailed shape. Extensive evaluations\ndemonstrate the superiority of UDC as a mesh representation and the favorable\nperformance of GenUDC in mesh generation. The code and trained models are\navailable at https://github.com/TrepangCat/GenUDC.\n", "link": "http://arxiv.org/abs/2410.17802v1", "date": "2024-10-23", "relevancy": 3.0714, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6278}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6146}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6004}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenUDC%3A%20High%20Quality%203D%20Mesh%20Generation%20with%20Unsigned%20Dual%20Contouring%0A%20%20Representation&body=Title%3A%20GenUDC%3A%20High%20Quality%203D%20Mesh%20Generation%20with%20Unsigned%20Dual%20Contouring%0A%20%20Representation%0AAuthor%3A%20Ruowei%20Wang%20and%20Jiaqi%20Li%20and%20Dan%20Zeng%20and%20Xueqi%20Ma%20and%20Zixiang%20Xu%20and%20Jianwei%20Zhang%20and%20Qijun%20Zhao%0AAbstract%3A%20%20%20Generating%20high-quality%20meshes%20with%20complex%20structures%20and%20realistic%20surfaces%0Ais%20the%20primary%20goal%20of%203D%20generative%20models.%20Existing%20methods%20typically%20employ%0Asequence%20data%20or%20deformable%20tetrahedral%20grids%20for%20mesh%20generation.%20However%2C%0Asequence-based%20methods%20have%20difficulty%20producing%20complex%20structures%20with%20many%0Afaces%20due%20to%20memory%20limits.%20The%20deformable%20tetrahedral%20grid-based%20method%0AMeshDiffusion%20fails%20to%20recover%20realistic%20surfaces%20due%20to%20the%20inherent%20ambiguity%0Ain%20deformable%20grids.%20We%20propose%20the%20GenUDC%20framework%20to%20address%20these%0Achallenges%20by%20leveraging%20the%20Unsigned%20Dual%20Contouring%20%28UDC%29%20as%20the%20mesh%0Arepresentation.%20UDC%20discretizes%20a%20mesh%20in%20a%20regular%20grid%20and%20divides%20it%20into%0Athe%20face%20and%20vertex%20parts%2C%20recovering%20both%20complex%20structures%20and%20fine%20details.%0AAs%20a%20result%2C%20the%20one-to-one%20mapping%20between%20UDC%20and%20mesh%20resolves%20the%20ambiguity%0Aproblem.%20In%20addition%2C%20GenUDC%20adopts%20a%20two-stage%2C%20coarse-to-fine%20generative%0Aprocess%20for%203D%20mesh%20generation.%20It%20first%20generates%20the%20face%20part%20as%20a%20rough%0Ashape%20and%20then%20the%20vertex%20part%20to%20craft%20a%20detailed%20shape.%20Extensive%20evaluations%0Ademonstrate%20the%20superiority%20of%20UDC%20as%20a%20mesh%20representation%20and%20the%20favorable%0Aperformance%20of%20GenUDC%20in%20mesh%20generation.%20The%20code%20and%20trained%20models%20are%0Aavailable%20at%20https%3A//github.com/TrepangCat/GenUDC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17802v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenUDC%253A%2520High%2520Quality%25203D%2520Mesh%2520Generation%2520with%2520Unsigned%2520Dual%2520Contouring%250A%2520%2520Representation%26entry.906535625%3DRuowei%2520Wang%2520and%2520Jiaqi%2520Li%2520and%2520Dan%2520Zeng%2520and%2520Xueqi%2520Ma%2520and%2520Zixiang%2520Xu%2520and%2520Jianwei%2520Zhang%2520and%2520Qijun%2520Zhao%26entry.1292438233%3D%2520%2520Generating%2520high-quality%2520meshes%2520with%2520complex%2520structures%2520and%2520realistic%2520surfaces%250Ais%2520the%2520primary%2520goal%2520of%25203D%2520generative%2520models.%2520Existing%2520methods%2520typically%2520employ%250Asequence%2520data%2520or%2520deformable%2520tetrahedral%2520grids%2520for%2520mesh%2520generation.%2520However%252C%250Asequence-based%2520methods%2520have%2520difficulty%2520producing%2520complex%2520structures%2520with%2520many%250Afaces%2520due%2520to%2520memory%2520limits.%2520The%2520deformable%2520tetrahedral%2520grid-based%2520method%250AMeshDiffusion%2520fails%2520to%2520recover%2520realistic%2520surfaces%2520due%2520to%2520the%2520inherent%2520ambiguity%250Ain%2520deformable%2520grids.%2520We%2520propose%2520the%2520GenUDC%2520framework%2520to%2520address%2520these%250Achallenges%2520by%2520leveraging%2520the%2520Unsigned%2520Dual%2520Contouring%2520%2528UDC%2529%2520as%2520the%2520mesh%250Arepresentation.%2520UDC%2520discretizes%2520a%2520mesh%2520in%2520a%2520regular%2520grid%2520and%2520divides%2520it%2520into%250Athe%2520face%2520and%2520vertex%2520parts%252C%2520recovering%2520both%2520complex%2520structures%2520and%2520fine%2520details.%250AAs%2520a%2520result%252C%2520the%2520one-to-one%2520mapping%2520between%2520UDC%2520and%2520mesh%2520resolves%2520the%2520ambiguity%250Aproblem.%2520In%2520addition%252C%2520GenUDC%2520adopts%2520a%2520two-stage%252C%2520coarse-to-fine%2520generative%250Aprocess%2520for%25203D%2520mesh%2520generation.%2520It%2520first%2520generates%2520the%2520face%2520part%2520as%2520a%2520rough%250Ashape%2520and%2520then%2520the%2520vertex%2520part%2520to%2520craft%2520a%2520detailed%2520shape.%2520Extensive%2520evaluations%250Ademonstrate%2520the%2520superiority%2520of%2520UDC%2520as%2520a%2520mesh%2520representation%2520and%2520the%2520favorable%250Aperformance%2520of%2520GenUDC%2520in%2520mesh%2520generation.%2520The%2520code%2520and%2520trained%2520models%2520are%250Aavailable%2520at%2520https%253A//github.com/TrepangCat/GenUDC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17802v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenUDC%3A%20High%20Quality%203D%20Mesh%20Generation%20with%20Unsigned%20Dual%20Contouring%0A%20%20Representation&entry.906535625=Ruowei%20Wang%20and%20Jiaqi%20Li%20and%20Dan%20Zeng%20and%20Xueqi%20Ma%20and%20Zixiang%20Xu%20and%20Jianwei%20Zhang%20and%20Qijun%20Zhao&entry.1292438233=%20%20Generating%20high-quality%20meshes%20with%20complex%20structures%20and%20realistic%20surfaces%0Ais%20the%20primary%20goal%20of%203D%20generative%20models.%20Existing%20methods%20typically%20employ%0Asequence%20data%20or%20deformable%20tetrahedral%20grids%20for%20mesh%20generation.%20However%2C%0Asequence-based%20methods%20have%20difficulty%20producing%20complex%20structures%20with%20many%0Afaces%20due%20to%20memory%20limits.%20The%20deformable%20tetrahedral%20grid-based%20method%0AMeshDiffusion%20fails%20to%20recover%20realistic%20surfaces%20due%20to%20the%20inherent%20ambiguity%0Ain%20deformable%20grids.%20We%20propose%20the%20GenUDC%20framework%20to%20address%20these%0Achallenges%20by%20leveraging%20the%20Unsigned%20Dual%20Contouring%20%28UDC%29%20as%20the%20mesh%0Arepresentation.%20UDC%20discretizes%20a%20mesh%20in%20a%20regular%20grid%20and%20divides%20it%20into%0Athe%20face%20and%20vertex%20parts%2C%20recovering%20both%20complex%20structures%20and%20fine%20details.%0AAs%20a%20result%2C%20the%20one-to-one%20mapping%20between%20UDC%20and%20mesh%20resolves%20the%20ambiguity%0Aproblem.%20In%20addition%2C%20GenUDC%20adopts%20a%20two-stage%2C%20coarse-to-fine%20generative%0Aprocess%20for%203D%20mesh%20generation.%20It%20first%20generates%20the%20face%20part%20as%20a%20rough%0Ashape%20and%20then%20the%20vertex%20part%20to%20craft%20a%20detailed%20shape.%20Extensive%20evaluations%0Ademonstrate%20the%20superiority%20of%20UDC%20as%20a%20mesh%20representation%20and%20the%20favorable%0Aperformance%20of%20GenUDC%20in%20mesh%20generation.%20The%20code%20and%20trained%20models%20are%0Aavailable%20at%20https%3A//github.com/TrepangCat/GenUDC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17802v1&entry.124074799=Read"},
{"title": "DynamicCity: Large-Scale LiDAR Generation from Dynamic Scenes", "author": "Hengwei Bian and Lingdong Kong and Haozhe Xie and Liang Pan and Yu Qiao and Ziwei Liu", "abstract": "  LiDAR scene generation has been developing rapidly recently. However,\nexisting methods primarily focus on generating static and single-frame scenes,\noverlooking the inherently dynamic nature of real-world driving environments.\nIn this work, we introduce DynamicCity, a novel 4D LiDAR generation framework\ncapable of generating large-scale, high-quality LiDAR scenes that capture the\ntemporal evolution of dynamic environments. DynamicCity mainly consists of two\nkey models. 1) A VAE model for learning HexPlane as the compact 4D\nrepresentation. Instead of using naive averaging operations, DynamicCity\nemploys a novel Projection Module to effectively compress 4D LiDAR features\ninto six 2D feature maps for HexPlane construction, which significantly\nenhances HexPlane fitting quality (up to 12.56 mIoU gain). Furthermore, we\nutilize an Expansion & Squeeze Strategy to reconstruct 3D feature volumes in\nparallel, which improves both network training efficiency and reconstruction\naccuracy than naively querying each 3D point (up to 7.05 mIoU gain, 2.06x\ntraining speedup, and 70.84% memory reduction). 2) A DiT-based diffusion model\nfor HexPlane generation. To make HexPlane feasible for DiT generation, a Padded\nRollout Operation is proposed to reorganize all six feature planes of the\nHexPlane as a squared 2D feature map. In particular, various conditions could\nbe introduced in the diffusion or sampling process, supporting versatile 4D\ngeneration applications, such as trajectory- and command-driven generation,\ninpainting, and layout-conditioned generation. Extensive experiments on the\nCarlaSC and Waymo datasets demonstrate that DynamicCity significantly\noutperforms existing state-of-the-art 4D LiDAR generation methods across\nmultiple metrics. The code will be released to facilitate future research.\n", "link": "http://arxiv.org/abs/2410.18084v1", "date": "2024-10-23", "relevancy": 3.0475, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6235}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6235}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5815}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynamicCity%3A%20Large-Scale%20LiDAR%20Generation%20from%20Dynamic%20Scenes&body=Title%3A%20DynamicCity%3A%20Large-Scale%20LiDAR%20Generation%20from%20Dynamic%20Scenes%0AAuthor%3A%20Hengwei%20Bian%20and%20Lingdong%20Kong%20and%20Haozhe%20Xie%20and%20Liang%20Pan%20and%20Yu%20Qiao%20and%20Ziwei%20Liu%0AAbstract%3A%20%20%20LiDAR%20scene%20generation%20has%20been%20developing%20rapidly%20recently.%20However%2C%0Aexisting%20methods%20primarily%20focus%20on%20generating%20static%20and%20single-frame%20scenes%2C%0Aoverlooking%20the%20inherently%20dynamic%20nature%20of%20real-world%20driving%20environments.%0AIn%20this%20work%2C%20we%20introduce%20DynamicCity%2C%20a%20novel%204D%20LiDAR%20generation%20framework%0Acapable%20of%20generating%20large-scale%2C%20high-quality%20LiDAR%20scenes%20that%20capture%20the%0Atemporal%20evolution%20of%20dynamic%20environments.%20DynamicCity%20mainly%20consists%20of%20two%0Akey%20models.%201%29%20A%20VAE%20model%20for%20learning%20HexPlane%20as%20the%20compact%204D%0Arepresentation.%20Instead%20of%20using%20naive%20averaging%20operations%2C%20DynamicCity%0Aemploys%20a%20novel%20Projection%20Module%20to%20effectively%20compress%204D%20LiDAR%20features%0Ainto%20six%202D%20feature%20maps%20for%20HexPlane%20construction%2C%20which%20significantly%0Aenhances%20HexPlane%20fitting%20quality%20%28up%20to%2012.56%20mIoU%20gain%29.%20Furthermore%2C%20we%0Autilize%20an%20Expansion%20%26%20Squeeze%20Strategy%20to%20reconstruct%203D%20feature%20volumes%20in%0Aparallel%2C%20which%20improves%20both%20network%20training%20efficiency%20and%20reconstruction%0Aaccuracy%20than%20naively%20querying%20each%203D%20point%20%28up%20to%207.05%20mIoU%20gain%2C%202.06x%0Atraining%20speedup%2C%20and%2070.84%25%20memory%20reduction%29.%202%29%20A%20DiT-based%20diffusion%20model%0Afor%20HexPlane%20generation.%20To%20make%20HexPlane%20feasible%20for%20DiT%20generation%2C%20a%20Padded%0ARollout%20Operation%20is%20proposed%20to%20reorganize%20all%20six%20feature%20planes%20of%20the%0AHexPlane%20as%20a%20squared%202D%20feature%20map.%20In%20particular%2C%20various%20conditions%20could%0Abe%20introduced%20in%20the%20diffusion%20or%20sampling%20process%2C%20supporting%20versatile%204D%0Ageneration%20applications%2C%20such%20as%20trajectory-%20and%20command-driven%20generation%2C%0Ainpainting%2C%20and%20layout-conditioned%20generation.%20Extensive%20experiments%20on%20the%0ACarlaSC%20and%20Waymo%20datasets%20demonstrate%20that%20DynamicCity%20significantly%0Aoutperforms%20existing%20state-of-the-art%204D%20LiDAR%20generation%20methods%20across%0Amultiple%20metrics.%20The%20code%20will%20be%20released%20to%20facilitate%20future%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamicCity%253A%2520Large-Scale%2520LiDAR%2520Generation%2520from%2520Dynamic%2520Scenes%26entry.906535625%3DHengwei%2520Bian%2520and%2520Lingdong%2520Kong%2520and%2520Haozhe%2520Xie%2520and%2520Liang%2520Pan%2520and%2520Yu%2520Qiao%2520and%2520Ziwei%2520Liu%26entry.1292438233%3D%2520%2520LiDAR%2520scene%2520generation%2520has%2520been%2520developing%2520rapidly%2520recently.%2520However%252C%250Aexisting%2520methods%2520primarily%2520focus%2520on%2520generating%2520static%2520and%2520single-frame%2520scenes%252C%250Aoverlooking%2520the%2520inherently%2520dynamic%2520nature%2520of%2520real-world%2520driving%2520environments.%250AIn%2520this%2520work%252C%2520we%2520introduce%2520DynamicCity%252C%2520a%2520novel%25204D%2520LiDAR%2520generation%2520framework%250Acapable%2520of%2520generating%2520large-scale%252C%2520high-quality%2520LiDAR%2520scenes%2520that%2520capture%2520the%250Atemporal%2520evolution%2520of%2520dynamic%2520environments.%2520DynamicCity%2520mainly%2520consists%2520of%2520two%250Akey%2520models.%25201%2529%2520A%2520VAE%2520model%2520for%2520learning%2520HexPlane%2520as%2520the%2520compact%25204D%250Arepresentation.%2520Instead%2520of%2520using%2520naive%2520averaging%2520operations%252C%2520DynamicCity%250Aemploys%2520a%2520novel%2520Projection%2520Module%2520to%2520effectively%2520compress%25204D%2520LiDAR%2520features%250Ainto%2520six%25202D%2520feature%2520maps%2520for%2520HexPlane%2520construction%252C%2520which%2520significantly%250Aenhances%2520HexPlane%2520fitting%2520quality%2520%2528up%2520to%252012.56%2520mIoU%2520gain%2529.%2520Furthermore%252C%2520we%250Autilize%2520an%2520Expansion%2520%2526%2520Squeeze%2520Strategy%2520to%2520reconstruct%25203D%2520feature%2520volumes%2520in%250Aparallel%252C%2520which%2520improves%2520both%2520network%2520training%2520efficiency%2520and%2520reconstruction%250Aaccuracy%2520than%2520naively%2520querying%2520each%25203D%2520point%2520%2528up%2520to%25207.05%2520mIoU%2520gain%252C%25202.06x%250Atraining%2520speedup%252C%2520and%252070.84%2525%2520memory%2520reduction%2529.%25202%2529%2520A%2520DiT-based%2520diffusion%2520model%250Afor%2520HexPlane%2520generation.%2520To%2520make%2520HexPlane%2520feasible%2520for%2520DiT%2520generation%252C%2520a%2520Padded%250ARollout%2520Operation%2520is%2520proposed%2520to%2520reorganize%2520all%2520six%2520feature%2520planes%2520of%2520the%250AHexPlane%2520as%2520a%2520squared%25202D%2520feature%2520map.%2520In%2520particular%252C%2520various%2520conditions%2520could%250Abe%2520introduced%2520in%2520the%2520diffusion%2520or%2520sampling%2520process%252C%2520supporting%2520versatile%25204D%250Ageneration%2520applications%252C%2520such%2520as%2520trajectory-%2520and%2520command-driven%2520generation%252C%250Ainpainting%252C%2520and%2520layout-conditioned%2520generation.%2520Extensive%2520experiments%2520on%2520the%250ACarlaSC%2520and%2520Waymo%2520datasets%2520demonstrate%2520that%2520DynamicCity%2520significantly%250Aoutperforms%2520existing%2520state-of-the-art%25204D%2520LiDAR%2520generation%2520methods%2520across%250Amultiple%2520metrics.%2520The%2520code%2520will%2520be%2520released%2520to%2520facilitate%2520future%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynamicCity%3A%20Large-Scale%20LiDAR%20Generation%20from%20Dynamic%20Scenes&entry.906535625=Hengwei%20Bian%20and%20Lingdong%20Kong%20and%20Haozhe%20Xie%20and%20Liang%20Pan%20and%20Yu%20Qiao%20and%20Ziwei%20Liu&entry.1292438233=%20%20LiDAR%20scene%20generation%20has%20been%20developing%20rapidly%20recently.%20However%2C%0Aexisting%20methods%20primarily%20focus%20on%20generating%20static%20and%20single-frame%20scenes%2C%0Aoverlooking%20the%20inherently%20dynamic%20nature%20of%20real-world%20driving%20environments.%0AIn%20this%20work%2C%20we%20introduce%20DynamicCity%2C%20a%20novel%204D%20LiDAR%20generation%20framework%0Acapable%20of%20generating%20large-scale%2C%20high-quality%20LiDAR%20scenes%20that%20capture%20the%0Atemporal%20evolution%20of%20dynamic%20environments.%20DynamicCity%20mainly%20consists%20of%20two%0Akey%20models.%201%29%20A%20VAE%20model%20for%20learning%20HexPlane%20as%20the%20compact%204D%0Arepresentation.%20Instead%20of%20using%20naive%20averaging%20operations%2C%20DynamicCity%0Aemploys%20a%20novel%20Projection%20Module%20to%20effectively%20compress%204D%20LiDAR%20features%0Ainto%20six%202D%20feature%20maps%20for%20HexPlane%20construction%2C%20which%20significantly%0Aenhances%20HexPlane%20fitting%20quality%20%28up%20to%2012.56%20mIoU%20gain%29.%20Furthermore%2C%20we%0Autilize%20an%20Expansion%20%26%20Squeeze%20Strategy%20to%20reconstruct%203D%20feature%20volumes%20in%0Aparallel%2C%20which%20improves%20both%20network%20training%20efficiency%20and%20reconstruction%0Aaccuracy%20than%20naively%20querying%20each%203D%20point%20%28up%20to%207.05%20mIoU%20gain%2C%202.06x%0Atraining%20speedup%2C%20and%2070.84%25%20memory%20reduction%29.%202%29%20A%20DiT-based%20diffusion%20model%0Afor%20HexPlane%20generation.%20To%20make%20HexPlane%20feasible%20for%20DiT%20generation%2C%20a%20Padded%0ARollout%20Operation%20is%20proposed%20to%20reorganize%20all%20six%20feature%20planes%20of%20the%0AHexPlane%20as%20a%20squared%202D%20feature%20map.%20In%20particular%2C%20various%20conditions%20could%0Abe%20introduced%20in%20the%20diffusion%20or%20sampling%20process%2C%20supporting%20versatile%204D%0Ageneration%20applications%2C%20such%20as%20trajectory-%20and%20command-driven%20generation%2C%0Ainpainting%2C%20and%20layout-conditioned%20generation.%20Extensive%20experiments%20on%20the%0ACarlaSC%20and%20Waymo%20datasets%20demonstrate%20that%20DynamicCity%20significantly%0Aoutperforms%20existing%20state-of-the-art%204D%20LiDAR%20generation%20methods%20across%0Amultiple%20metrics.%20The%20code%20will%20be%20released%20to%20facilitate%20future%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18084v1&entry.124074799=Read"},
{"title": "From Real Artifacts to Virtual Reference: A Robust Framework for\n  Translating Endoscopic Images", "author": "Junyang Wu and Fangfang Xie and Jiayuan Sun and Yun Gu and Guang-Zhong Yang", "abstract": "  Domain adaptation, which bridges the distributions across different\nmodalities, plays a crucial role in multimodal medical image analysis. In\nendoscopic imaging, combining pre-operative data with intra-operative imaging\nis important for surgical planning and navigation. However, existing domain\nadaptation methods are hampered by distribution shift caused by in vivo\nartifacts, necessitating robust techniques for aligning noisy and artifact\nabundant patient endoscopic videos with clean virtual images reconstructed from\npre-operative tomographic data for pose estimation during intraoperative\nguidance. This paper presents an artifact-resilient image translation method\nand an associated benchmark for this purpose. The method incorporates a novel\n``local-global'' translation framework and a noise-resilient feature extraction\nstrategy. For the former, it decouples the image translation process into a\nlocal step for feature denoising, and a global step for global style transfer.\nFor feature extraction, a new contrastive learning strategy is proposed, which\ncan extract noise-resilient features for establishing robust correspondence\nacross domains. Detailed validation on both public and in-house clinical\ndatasets has been conducted, demonstrating significantly improved performance\ncompared to the current state-of-the-art.\n", "link": "http://arxiv.org/abs/2410.13896v2", "date": "2024-10-23", "relevancy": 2.9947, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6214}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5952}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5803}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Real%20Artifacts%20to%20Virtual%20Reference%3A%20A%20Robust%20Framework%20for%0A%20%20Translating%20Endoscopic%20Images&body=Title%3A%20From%20Real%20Artifacts%20to%20Virtual%20Reference%3A%20A%20Robust%20Framework%20for%0A%20%20Translating%20Endoscopic%20Images%0AAuthor%3A%20Junyang%20Wu%20and%20Fangfang%20Xie%20and%20Jiayuan%20Sun%20and%20Yun%20Gu%20and%20Guang-Zhong%20Yang%0AAbstract%3A%20%20%20Domain%20adaptation%2C%20which%20bridges%20the%20distributions%20across%20different%0Amodalities%2C%20plays%20a%20crucial%20role%20in%20multimodal%20medical%20image%20analysis.%20In%0Aendoscopic%20imaging%2C%20combining%20pre-operative%20data%20with%20intra-operative%20imaging%0Ais%20important%20for%20surgical%20planning%20and%20navigation.%20However%2C%20existing%20domain%0Aadaptation%20methods%20are%20hampered%20by%20distribution%20shift%20caused%20by%20in%20vivo%0Aartifacts%2C%20necessitating%20robust%20techniques%20for%20aligning%20noisy%20and%20artifact%0Aabundant%20patient%20endoscopic%20videos%20with%20clean%20virtual%20images%20reconstructed%20from%0Apre-operative%20tomographic%20data%20for%20pose%20estimation%20during%20intraoperative%0Aguidance.%20This%20paper%20presents%20an%20artifact-resilient%20image%20translation%20method%0Aand%20an%20associated%20benchmark%20for%20this%20purpose.%20The%20method%20incorporates%20a%20novel%0A%60%60local-global%27%27%20translation%20framework%20and%20a%20noise-resilient%20feature%20extraction%0Astrategy.%20For%20the%20former%2C%20it%20decouples%20the%20image%20translation%20process%20into%20a%0Alocal%20step%20for%20feature%20denoising%2C%20and%20a%20global%20step%20for%20global%20style%20transfer.%0AFor%20feature%20extraction%2C%20a%20new%20contrastive%20learning%20strategy%20is%20proposed%2C%20which%0Acan%20extract%20noise-resilient%20features%20for%20establishing%20robust%20correspondence%0Aacross%20domains.%20Detailed%20validation%20on%20both%20public%20and%20in-house%20clinical%0Adatasets%20has%20been%20conducted%2C%20demonstrating%20significantly%20improved%20performance%0Acompared%20to%20the%20current%20state-of-the-art.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.13896v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Real%2520Artifacts%2520to%2520Virtual%2520Reference%253A%2520A%2520Robust%2520Framework%2520for%250A%2520%2520Translating%2520Endoscopic%2520Images%26entry.906535625%3DJunyang%2520Wu%2520and%2520Fangfang%2520Xie%2520and%2520Jiayuan%2520Sun%2520and%2520Yun%2520Gu%2520and%2520Guang-Zhong%2520Yang%26entry.1292438233%3D%2520%2520Domain%2520adaptation%252C%2520which%2520bridges%2520the%2520distributions%2520across%2520different%250Amodalities%252C%2520plays%2520a%2520crucial%2520role%2520in%2520multimodal%2520medical%2520image%2520analysis.%2520In%250Aendoscopic%2520imaging%252C%2520combining%2520pre-operative%2520data%2520with%2520intra-operative%2520imaging%250Ais%2520important%2520for%2520surgical%2520planning%2520and%2520navigation.%2520However%252C%2520existing%2520domain%250Aadaptation%2520methods%2520are%2520hampered%2520by%2520distribution%2520shift%2520caused%2520by%2520in%2520vivo%250Aartifacts%252C%2520necessitating%2520robust%2520techniques%2520for%2520aligning%2520noisy%2520and%2520artifact%250Aabundant%2520patient%2520endoscopic%2520videos%2520with%2520clean%2520virtual%2520images%2520reconstructed%2520from%250Apre-operative%2520tomographic%2520data%2520for%2520pose%2520estimation%2520during%2520intraoperative%250Aguidance.%2520This%2520paper%2520presents%2520an%2520artifact-resilient%2520image%2520translation%2520method%250Aand%2520an%2520associated%2520benchmark%2520for%2520this%2520purpose.%2520The%2520method%2520incorporates%2520a%2520novel%250A%2560%2560local-global%2527%2527%2520translation%2520framework%2520and%2520a%2520noise-resilient%2520feature%2520extraction%250Astrategy.%2520For%2520the%2520former%252C%2520it%2520decouples%2520the%2520image%2520translation%2520process%2520into%2520a%250Alocal%2520step%2520for%2520feature%2520denoising%252C%2520and%2520a%2520global%2520step%2520for%2520global%2520style%2520transfer.%250AFor%2520feature%2520extraction%252C%2520a%2520new%2520contrastive%2520learning%2520strategy%2520is%2520proposed%252C%2520which%250Acan%2520extract%2520noise-resilient%2520features%2520for%2520establishing%2520robust%2520correspondence%250Aacross%2520domains.%2520Detailed%2520validation%2520on%2520both%2520public%2520and%2520in-house%2520clinical%250Adatasets%2520has%2520been%2520conducted%252C%2520demonstrating%2520significantly%2520improved%2520performance%250Acompared%2520to%2520the%2520current%2520state-of-the-art.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.13896v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Real%20Artifacts%20to%20Virtual%20Reference%3A%20A%20Robust%20Framework%20for%0A%20%20Translating%20Endoscopic%20Images&entry.906535625=Junyang%20Wu%20and%20Fangfang%20Xie%20and%20Jiayuan%20Sun%20and%20Yun%20Gu%20and%20Guang-Zhong%20Yang&entry.1292438233=%20%20Domain%20adaptation%2C%20which%20bridges%20the%20distributions%20across%20different%0Amodalities%2C%20plays%20a%20crucial%20role%20in%20multimodal%20medical%20image%20analysis.%20In%0Aendoscopic%20imaging%2C%20combining%20pre-operative%20data%20with%20intra-operative%20imaging%0Ais%20important%20for%20surgical%20planning%20and%20navigation.%20However%2C%20existing%20domain%0Aadaptation%20methods%20are%20hampered%20by%20distribution%20shift%20caused%20by%20in%20vivo%0Aartifacts%2C%20necessitating%20robust%20techniques%20for%20aligning%20noisy%20and%20artifact%0Aabundant%20patient%20endoscopic%20videos%20with%20clean%20virtual%20images%20reconstructed%20from%0Apre-operative%20tomographic%20data%20for%20pose%20estimation%20during%20intraoperative%0Aguidance.%20This%20paper%20presents%20an%20artifact-resilient%20image%20translation%20method%0Aand%20an%20associated%20benchmark%20for%20this%20purpose.%20The%20method%20incorporates%20a%20novel%0A%60%60local-global%27%27%20translation%20framework%20and%20a%20noise-resilient%20feature%20extraction%0Astrategy.%20For%20the%20former%2C%20it%20decouples%20the%20image%20translation%20process%20into%20a%0Alocal%20step%20for%20feature%20denoising%2C%20and%20a%20global%20step%20for%20global%20style%20transfer.%0AFor%20feature%20extraction%2C%20a%20new%20contrastive%20learning%20strategy%20is%20proposed%2C%20which%0Acan%20extract%20noise-resilient%20features%20for%20establishing%20robust%20correspondence%0Aacross%20domains.%20Detailed%20validation%20on%20both%20public%20and%20in-house%20clinical%0Adatasets%20has%20been%20conducted%2C%20demonstrating%20significantly%20improved%20performance%0Acompared%20to%20the%20current%20state-of-the-art.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.13896v2&entry.124074799=Read"},
{"title": "ADEM-VL: Adaptive and Embedded Fusion for Efficient Vision-Language\n  Tuning", "author": "Zhiwei Hao and Jianyuan Guo and Li Shen and Yong Luo and Han Hu and Yonggang Wen", "abstract": "  Recent advancements in multimodal fusion have witnessed the remarkable\nsuccess of vision-language (VL) models, which excel in various multimodal\napplications such as image captioning and visual question answering. However,\nbuilding VL models requires substantial hardware resources, where efficiency is\nrestricted by two key factors: the extended input sequence of the language\nmodel with vision features demands more computational operations, and a large\nnumber of additional learnable parameters increase memory complexity. These\nchallenges significantly restrict the broader applicability of such models. To\nbridge this gap, we propose ADEM-VL, an efficient vision-language method that\ntunes VL models based on pretrained large language models (LLMs) by adopting a\nparameter-free cross-attention mechanism for similarity measurements in\nmultimodal fusion. This approach only requires embedding vision features into\nthe language space, significantly reducing the number of trainable parameters\nand accelerating both training and inference speeds. To enhance representation\nlearning in fusion module, we introduce an efficient multiscale feature\ngeneration scheme that requires only a single forward pass through the vision\nencoder. Moreover, we propose an adaptive fusion scheme that dynamically\ndiscards less relevant visual information for each text token based on its\nattention score. This ensures that the fusion process prioritizes the most\npertinent visual features. With experiments on various tasks including visual\nquestion answering, image captioning, and instruction-following, we demonstrate\nthat our framework outperforms existing approaches. Specifically, our method\nsurpasses existing methods by an average accuracy of 0.77% on ScienceQA\ndataset, with reduced training and inference latency, demonstrating the\nsuperiority of our framework. The code is available at\nhttps://github.com/Hao840/ADEM-VL.\n", "link": "http://arxiv.org/abs/2410.17779v1", "date": "2024-10-23", "relevancy": 2.9931, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6051}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5954}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ADEM-VL%3A%20Adaptive%20and%20Embedded%20Fusion%20for%20Efficient%20Vision-Language%0A%20%20Tuning&body=Title%3A%20ADEM-VL%3A%20Adaptive%20and%20Embedded%20Fusion%20for%20Efficient%20Vision-Language%0A%20%20Tuning%0AAuthor%3A%20Zhiwei%20Hao%20and%20Jianyuan%20Guo%20and%20Li%20Shen%20and%20Yong%20Luo%20and%20Han%20Hu%20and%20Yonggang%20Wen%0AAbstract%3A%20%20%20Recent%20advancements%20in%20multimodal%20fusion%20have%20witnessed%20the%20remarkable%0Asuccess%20of%20vision-language%20%28VL%29%20models%2C%20which%20excel%20in%20various%20multimodal%0Aapplications%20such%20as%20image%20captioning%20and%20visual%20question%20answering.%20However%2C%0Abuilding%20VL%20models%20requires%20substantial%20hardware%20resources%2C%20where%20efficiency%20is%0Arestricted%20by%20two%20key%20factors%3A%20the%20extended%20input%20sequence%20of%20the%20language%0Amodel%20with%20vision%20features%20demands%20more%20computational%20operations%2C%20and%20a%20large%0Anumber%20of%20additional%20learnable%20parameters%20increase%20memory%20complexity.%20These%0Achallenges%20significantly%20restrict%20the%20broader%20applicability%20of%20such%20models.%20To%0Abridge%20this%20gap%2C%20we%20propose%20ADEM-VL%2C%20an%20efficient%20vision-language%20method%20that%0Atunes%20VL%20models%20based%20on%20pretrained%20large%20language%20models%20%28LLMs%29%20by%20adopting%20a%0Aparameter-free%20cross-attention%20mechanism%20for%20similarity%20measurements%20in%0Amultimodal%20fusion.%20This%20approach%20only%20requires%20embedding%20vision%20features%20into%0Athe%20language%20space%2C%20significantly%20reducing%20the%20number%20of%20trainable%20parameters%0Aand%20accelerating%20both%20training%20and%20inference%20speeds.%20To%20enhance%20representation%0Alearning%20in%20fusion%20module%2C%20we%20introduce%20an%20efficient%20multiscale%20feature%0Ageneration%20scheme%20that%20requires%20only%20a%20single%20forward%20pass%20through%20the%20vision%0Aencoder.%20Moreover%2C%20we%20propose%20an%20adaptive%20fusion%20scheme%20that%20dynamically%0Adiscards%20less%20relevant%20visual%20information%20for%20each%20text%20token%20based%20on%20its%0Aattention%20score.%20This%20ensures%20that%20the%20fusion%20process%20prioritizes%20the%20most%0Apertinent%20visual%20features.%20With%20experiments%20on%20various%20tasks%20including%20visual%0Aquestion%20answering%2C%20image%20captioning%2C%20and%20instruction-following%2C%20we%20demonstrate%0Athat%20our%20framework%20outperforms%20existing%20approaches.%20Specifically%2C%20our%20method%0Asurpasses%20existing%20methods%20by%20an%20average%20accuracy%20of%200.77%25%20on%20ScienceQA%0Adataset%2C%20with%20reduced%20training%20and%20inference%20latency%2C%20demonstrating%20the%0Asuperiority%20of%20our%20framework.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Hao840/ADEM-VL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17779v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DADEM-VL%253A%2520Adaptive%2520and%2520Embedded%2520Fusion%2520for%2520Efficient%2520Vision-Language%250A%2520%2520Tuning%26entry.906535625%3DZhiwei%2520Hao%2520and%2520Jianyuan%2520Guo%2520and%2520Li%2520Shen%2520and%2520Yong%2520Luo%2520and%2520Han%2520Hu%2520and%2520Yonggang%2520Wen%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520multimodal%2520fusion%2520have%2520witnessed%2520the%2520remarkable%250Asuccess%2520of%2520vision-language%2520%2528VL%2529%2520models%252C%2520which%2520excel%2520in%2520various%2520multimodal%250Aapplications%2520such%2520as%2520image%2520captioning%2520and%2520visual%2520question%2520answering.%2520However%252C%250Abuilding%2520VL%2520models%2520requires%2520substantial%2520hardware%2520resources%252C%2520where%2520efficiency%2520is%250Arestricted%2520by%2520two%2520key%2520factors%253A%2520the%2520extended%2520input%2520sequence%2520of%2520the%2520language%250Amodel%2520with%2520vision%2520features%2520demands%2520more%2520computational%2520operations%252C%2520and%2520a%2520large%250Anumber%2520of%2520additional%2520learnable%2520parameters%2520increase%2520memory%2520complexity.%2520These%250Achallenges%2520significantly%2520restrict%2520the%2520broader%2520applicability%2520of%2520such%2520models.%2520To%250Abridge%2520this%2520gap%252C%2520we%2520propose%2520ADEM-VL%252C%2520an%2520efficient%2520vision-language%2520method%2520that%250Atunes%2520VL%2520models%2520based%2520on%2520pretrained%2520large%2520language%2520models%2520%2528LLMs%2529%2520by%2520adopting%2520a%250Aparameter-free%2520cross-attention%2520mechanism%2520for%2520similarity%2520measurements%2520in%250Amultimodal%2520fusion.%2520This%2520approach%2520only%2520requires%2520embedding%2520vision%2520features%2520into%250Athe%2520language%2520space%252C%2520significantly%2520reducing%2520the%2520number%2520of%2520trainable%2520parameters%250Aand%2520accelerating%2520both%2520training%2520and%2520inference%2520speeds.%2520To%2520enhance%2520representation%250Alearning%2520in%2520fusion%2520module%252C%2520we%2520introduce%2520an%2520efficient%2520multiscale%2520feature%250Ageneration%2520scheme%2520that%2520requires%2520only%2520a%2520single%2520forward%2520pass%2520through%2520the%2520vision%250Aencoder.%2520Moreover%252C%2520we%2520propose%2520an%2520adaptive%2520fusion%2520scheme%2520that%2520dynamically%250Adiscards%2520less%2520relevant%2520visual%2520information%2520for%2520each%2520text%2520token%2520based%2520on%2520its%250Aattention%2520score.%2520This%2520ensures%2520that%2520the%2520fusion%2520process%2520prioritizes%2520the%2520most%250Apertinent%2520visual%2520features.%2520With%2520experiments%2520on%2520various%2520tasks%2520including%2520visual%250Aquestion%2520answering%252C%2520image%2520captioning%252C%2520and%2520instruction-following%252C%2520we%2520demonstrate%250Athat%2520our%2520framework%2520outperforms%2520existing%2520approaches.%2520Specifically%252C%2520our%2520method%250Asurpasses%2520existing%2520methods%2520by%2520an%2520average%2520accuracy%2520of%25200.77%2525%2520on%2520ScienceQA%250Adataset%252C%2520with%2520reduced%2520training%2520and%2520inference%2520latency%252C%2520demonstrating%2520the%250Asuperiority%2520of%2520our%2520framework.%2520The%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/Hao840/ADEM-VL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17779v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ADEM-VL%3A%20Adaptive%20and%20Embedded%20Fusion%20for%20Efficient%20Vision-Language%0A%20%20Tuning&entry.906535625=Zhiwei%20Hao%20and%20Jianyuan%20Guo%20and%20Li%20Shen%20and%20Yong%20Luo%20and%20Han%20Hu%20and%20Yonggang%20Wen&entry.1292438233=%20%20Recent%20advancements%20in%20multimodal%20fusion%20have%20witnessed%20the%20remarkable%0Asuccess%20of%20vision-language%20%28VL%29%20models%2C%20which%20excel%20in%20various%20multimodal%0Aapplications%20such%20as%20image%20captioning%20and%20visual%20question%20answering.%20However%2C%0Abuilding%20VL%20models%20requires%20substantial%20hardware%20resources%2C%20where%20efficiency%20is%0Arestricted%20by%20two%20key%20factors%3A%20the%20extended%20input%20sequence%20of%20the%20language%0Amodel%20with%20vision%20features%20demands%20more%20computational%20operations%2C%20and%20a%20large%0Anumber%20of%20additional%20learnable%20parameters%20increase%20memory%20complexity.%20These%0Achallenges%20significantly%20restrict%20the%20broader%20applicability%20of%20such%20models.%20To%0Abridge%20this%20gap%2C%20we%20propose%20ADEM-VL%2C%20an%20efficient%20vision-language%20method%20that%0Atunes%20VL%20models%20based%20on%20pretrained%20large%20language%20models%20%28LLMs%29%20by%20adopting%20a%0Aparameter-free%20cross-attention%20mechanism%20for%20similarity%20measurements%20in%0Amultimodal%20fusion.%20This%20approach%20only%20requires%20embedding%20vision%20features%20into%0Athe%20language%20space%2C%20significantly%20reducing%20the%20number%20of%20trainable%20parameters%0Aand%20accelerating%20both%20training%20and%20inference%20speeds.%20To%20enhance%20representation%0Alearning%20in%20fusion%20module%2C%20we%20introduce%20an%20efficient%20multiscale%20feature%0Ageneration%20scheme%20that%20requires%20only%20a%20single%20forward%20pass%20through%20the%20vision%0Aencoder.%20Moreover%2C%20we%20propose%20an%20adaptive%20fusion%20scheme%20that%20dynamically%0Adiscards%20less%20relevant%20visual%20information%20for%20each%20text%20token%20based%20on%20its%0Aattention%20score.%20This%20ensures%20that%20the%20fusion%20process%20prioritizes%20the%20most%0Apertinent%20visual%20features.%20With%20experiments%20on%20various%20tasks%20including%20visual%0Aquestion%20answering%2C%20image%20captioning%2C%20and%20instruction-following%2C%20we%20demonstrate%0Athat%20our%20framework%20outperforms%20existing%20approaches.%20Specifically%2C%20our%20method%0Asurpasses%20existing%20methods%20by%20an%20average%20accuracy%20of%200.77%25%20on%20ScienceQA%0Adataset%2C%20with%20reduced%20training%20and%20inference%20latency%2C%20demonstrating%20the%0Asuperiority%20of%20our%20framework.%20The%20code%20is%20available%20at%0Ahttps%3A//github.com/Hao840/ADEM-VL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17779v1&entry.124074799=Read"},
{"title": "FreeVS: Generative View Synthesis on Free Driving Trajectory", "author": "Qitai Wang and Lue Fan and Yuqi Wang and Yuntao Chen and Zhaoxiang Zhang", "abstract": "  Existing reconstruction-based novel view synthesis methods for driving scenes\nfocus on synthesizing camera views along the recorded trajectory of the ego\nvehicle. Their image rendering performance will severely degrade on viewpoints\nfalling out of the recorded trajectory, where camera rays are untrained. We\npropose FreeVS, a novel fully generative approach that can synthesize camera\nviews on free new trajectories in real driving scenes. To control the\ngeneration results to be 3D consistent with the real scenes and accurate in\nviewpoint pose, we propose the pseudo-image representation of view priors to\ncontrol the generation process. Viewpoint transformation simulation is applied\non pseudo-images to simulate camera movement in each direction. Once trained,\nFreeVS can be applied to any validation sequences without reconstruction\nprocess and synthesis views on novel trajectories. Moreover, we propose two new\nchallenging benchmarks tailored to driving scenes, which are novel camera\nsynthesis and novel trajectory synthesis, emphasizing the freedom of\nviewpoints. Given that no ground truth images are available on novel\ntrajectories, we also propose to evaluate the consistency of images synthesized\non novel trajectories with 3D perception models. Experiments on the Waymo Open\nDataset show that FreeVS has a strong image synthesis performance on both the\nrecorded trajectories and novel trajectories. Project Page:\nhttps://freevs24.github.io/\n", "link": "http://arxiv.org/abs/2410.18079v1", "date": "2024-10-23", "relevancy": 2.9623, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5939}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5939}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.5896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FreeVS%3A%20Generative%20View%20Synthesis%20on%20Free%20Driving%20Trajectory&body=Title%3A%20FreeVS%3A%20Generative%20View%20Synthesis%20on%20Free%20Driving%20Trajectory%0AAuthor%3A%20Qitai%20Wang%20and%20Lue%20Fan%20and%20Yuqi%20Wang%20and%20Yuntao%20Chen%20and%20Zhaoxiang%20Zhang%0AAbstract%3A%20%20%20Existing%20reconstruction-based%20novel%20view%20synthesis%20methods%20for%20driving%20scenes%0Afocus%20on%20synthesizing%20camera%20views%20along%20the%20recorded%20trajectory%20of%20the%20ego%0Avehicle.%20Their%20image%20rendering%20performance%20will%20severely%20degrade%20on%20viewpoints%0Afalling%20out%20of%20the%20recorded%20trajectory%2C%20where%20camera%20rays%20are%20untrained.%20We%0Apropose%20FreeVS%2C%20a%20novel%20fully%20generative%20approach%20that%20can%20synthesize%20camera%0Aviews%20on%20free%20new%20trajectories%20in%20real%20driving%20scenes.%20To%20control%20the%0Ageneration%20results%20to%20be%203D%20consistent%20with%20the%20real%20scenes%20and%20accurate%20in%0Aviewpoint%20pose%2C%20we%20propose%20the%20pseudo-image%20representation%20of%20view%20priors%20to%0Acontrol%20the%20generation%20process.%20Viewpoint%20transformation%20simulation%20is%20applied%0Aon%20pseudo-images%20to%20simulate%20camera%20movement%20in%20each%20direction.%20Once%20trained%2C%0AFreeVS%20can%20be%20applied%20to%20any%20validation%20sequences%20without%20reconstruction%0Aprocess%20and%20synthesis%20views%20on%20novel%20trajectories.%20Moreover%2C%20we%20propose%20two%20new%0Achallenging%20benchmarks%20tailored%20to%20driving%20scenes%2C%20which%20are%20novel%20camera%0Asynthesis%20and%20novel%20trajectory%20synthesis%2C%20emphasizing%20the%20freedom%20of%0Aviewpoints.%20Given%20that%20no%20ground%20truth%20images%20are%20available%20on%20novel%0Atrajectories%2C%20we%20also%20propose%20to%20evaluate%20the%20consistency%20of%20images%20synthesized%0Aon%20novel%20trajectories%20with%203D%20perception%20models.%20Experiments%20on%20the%20Waymo%20Open%0ADataset%20show%20that%20FreeVS%20has%20a%20strong%20image%20synthesis%20performance%20on%20both%20the%0Arecorded%20trajectories%20and%20novel%20trajectories.%20Project%20Page%3A%0Ahttps%3A//freevs24.github.io/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFreeVS%253A%2520Generative%2520View%2520Synthesis%2520on%2520Free%2520Driving%2520Trajectory%26entry.906535625%3DQitai%2520Wang%2520and%2520Lue%2520Fan%2520and%2520Yuqi%2520Wang%2520and%2520Yuntao%2520Chen%2520and%2520Zhaoxiang%2520Zhang%26entry.1292438233%3D%2520%2520Existing%2520reconstruction-based%2520novel%2520view%2520synthesis%2520methods%2520for%2520driving%2520scenes%250Afocus%2520on%2520synthesizing%2520camera%2520views%2520along%2520the%2520recorded%2520trajectory%2520of%2520the%2520ego%250Avehicle.%2520Their%2520image%2520rendering%2520performance%2520will%2520severely%2520degrade%2520on%2520viewpoints%250Afalling%2520out%2520of%2520the%2520recorded%2520trajectory%252C%2520where%2520camera%2520rays%2520are%2520untrained.%2520We%250Apropose%2520FreeVS%252C%2520a%2520novel%2520fully%2520generative%2520approach%2520that%2520can%2520synthesize%2520camera%250Aviews%2520on%2520free%2520new%2520trajectories%2520in%2520real%2520driving%2520scenes.%2520To%2520control%2520the%250Ageneration%2520results%2520to%2520be%25203D%2520consistent%2520with%2520the%2520real%2520scenes%2520and%2520accurate%2520in%250Aviewpoint%2520pose%252C%2520we%2520propose%2520the%2520pseudo-image%2520representation%2520of%2520view%2520priors%2520to%250Acontrol%2520the%2520generation%2520process.%2520Viewpoint%2520transformation%2520simulation%2520is%2520applied%250Aon%2520pseudo-images%2520to%2520simulate%2520camera%2520movement%2520in%2520each%2520direction.%2520Once%2520trained%252C%250AFreeVS%2520can%2520be%2520applied%2520to%2520any%2520validation%2520sequences%2520without%2520reconstruction%250Aprocess%2520and%2520synthesis%2520views%2520on%2520novel%2520trajectories.%2520Moreover%252C%2520we%2520propose%2520two%2520new%250Achallenging%2520benchmarks%2520tailored%2520to%2520driving%2520scenes%252C%2520which%2520are%2520novel%2520camera%250Asynthesis%2520and%2520novel%2520trajectory%2520synthesis%252C%2520emphasizing%2520the%2520freedom%2520of%250Aviewpoints.%2520Given%2520that%2520no%2520ground%2520truth%2520images%2520are%2520available%2520on%2520novel%250Atrajectories%252C%2520we%2520also%2520propose%2520to%2520evaluate%2520the%2520consistency%2520of%2520images%2520synthesized%250Aon%2520novel%2520trajectories%2520with%25203D%2520perception%2520models.%2520Experiments%2520on%2520the%2520Waymo%2520Open%250ADataset%2520show%2520that%2520FreeVS%2520has%2520a%2520strong%2520image%2520synthesis%2520performance%2520on%2520both%2520the%250Arecorded%2520trajectories%2520and%2520novel%2520trajectories.%2520Project%2520Page%253A%250Ahttps%253A//freevs24.github.io/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FreeVS%3A%20Generative%20View%20Synthesis%20on%20Free%20Driving%20Trajectory&entry.906535625=Qitai%20Wang%20and%20Lue%20Fan%20and%20Yuqi%20Wang%20and%20Yuntao%20Chen%20and%20Zhaoxiang%20Zhang&entry.1292438233=%20%20Existing%20reconstruction-based%20novel%20view%20synthesis%20methods%20for%20driving%20scenes%0Afocus%20on%20synthesizing%20camera%20views%20along%20the%20recorded%20trajectory%20of%20the%20ego%0Avehicle.%20Their%20image%20rendering%20performance%20will%20severely%20degrade%20on%20viewpoints%0Afalling%20out%20of%20the%20recorded%20trajectory%2C%20where%20camera%20rays%20are%20untrained.%20We%0Apropose%20FreeVS%2C%20a%20novel%20fully%20generative%20approach%20that%20can%20synthesize%20camera%0Aviews%20on%20free%20new%20trajectories%20in%20real%20driving%20scenes.%20To%20control%20the%0Ageneration%20results%20to%20be%203D%20consistent%20with%20the%20real%20scenes%20and%20accurate%20in%0Aviewpoint%20pose%2C%20we%20propose%20the%20pseudo-image%20representation%20of%20view%20priors%20to%0Acontrol%20the%20generation%20process.%20Viewpoint%20transformation%20simulation%20is%20applied%0Aon%20pseudo-images%20to%20simulate%20camera%20movement%20in%20each%20direction.%20Once%20trained%2C%0AFreeVS%20can%20be%20applied%20to%20any%20validation%20sequences%20without%20reconstruction%0Aprocess%20and%20synthesis%20views%20on%20novel%20trajectories.%20Moreover%2C%20we%20propose%20two%20new%0Achallenging%20benchmarks%20tailored%20to%20driving%20scenes%2C%20which%20are%20novel%20camera%0Asynthesis%20and%20novel%20trajectory%20synthesis%2C%20emphasizing%20the%20freedom%20of%0Aviewpoints.%20Given%20that%20no%20ground%20truth%20images%20are%20available%20on%20novel%0Atrajectories%2C%20we%20also%20propose%20to%20evaluate%20the%20consistency%20of%20images%20synthesized%0Aon%20novel%20trajectories%20with%203D%20perception%20models.%20Experiments%20on%20the%20Waymo%20Open%0ADataset%20show%20that%20FreeVS%20has%20a%20strong%20image%20synthesis%20performance%20on%20both%20the%0Arecorded%20trajectories%20and%20novel%20trajectories.%20Project%20Page%3A%0Ahttps%3A//freevs24.github.io/%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18079v1&entry.124074799=Read"},
{"title": "VILA-U: a Unified Foundation Model Integrating Visual Understanding and\n  Generation", "author": "Yecheng Wu and Zhuoyang Zhang and Junyu Chen and Haotian Tang and Dacheng Li and Yunhao Fang and Ligeng Zhu and Enze Xie and Hongxu Yin and Li Yi and Song Han and Yao Lu", "abstract": "  VILA-U is a Unified foundation model that integrates Video, Image, Language\nunderstanding and generation. Traditional visual language models (VLMs) use\nseparate modules for understanding and generating visual content, which can\nlead to misalignment and increased complexity. In contrast, VILA-U employs a\nsingle autoregressive next-token prediction framework for both tasks,\neliminating the need for additional components like diffusion models. This\napproach not only simplifies the model but also achieves near state-of-the-art\nperformance in visual language understanding and generation. The success of\nVILA-U is attributed to two main factors: the unified vision tower that aligns\ndiscrete visual tokens with textual inputs during pretraining, which enhances\nvisual perception, and autoregressive image generation can achieve similar\nquality as diffusion models with high-quality dataset. This allows VILA-U to\nperform comparably to more complex models using a fully token-based\nautoregressive framework.\n", "link": "http://arxiv.org/abs/2409.04429v2", "date": "2024-10-23", "relevancy": 2.9466, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.587}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VILA-U%3A%20a%20Unified%20Foundation%20Model%20Integrating%20Visual%20Understanding%20and%0A%20%20Generation&body=Title%3A%20VILA-U%3A%20a%20Unified%20Foundation%20Model%20Integrating%20Visual%20Understanding%20and%0A%20%20Generation%0AAuthor%3A%20Yecheng%20Wu%20and%20Zhuoyang%20Zhang%20and%20Junyu%20Chen%20and%20Haotian%20Tang%20and%20Dacheng%20Li%20and%20Yunhao%20Fang%20and%20Ligeng%20Zhu%20and%20Enze%20Xie%20and%20Hongxu%20Yin%20and%20Li%20Yi%20and%20Song%20Han%20and%20Yao%20Lu%0AAbstract%3A%20%20%20VILA-U%20is%20a%20Unified%20foundation%20model%20that%20integrates%20Video%2C%20Image%2C%20Language%0Aunderstanding%20and%20generation.%20Traditional%20visual%20language%20models%20%28VLMs%29%20use%0Aseparate%20modules%20for%20understanding%20and%20generating%20visual%20content%2C%20which%20can%0Alead%20to%20misalignment%20and%20increased%20complexity.%20In%20contrast%2C%20VILA-U%20employs%20a%0Asingle%20autoregressive%20next-token%20prediction%20framework%20for%20both%20tasks%2C%0Aeliminating%20the%20need%20for%20additional%20components%20like%20diffusion%20models.%20This%0Aapproach%20not%20only%20simplifies%20the%20model%20but%20also%20achieves%20near%20state-of-the-art%0Aperformance%20in%20visual%20language%20understanding%20and%20generation.%20The%20success%20of%0AVILA-U%20is%20attributed%20to%20two%20main%20factors%3A%20the%20unified%20vision%20tower%20that%20aligns%0Adiscrete%20visual%20tokens%20with%20textual%20inputs%20during%20pretraining%2C%20which%20enhances%0Avisual%20perception%2C%20and%20autoregressive%20image%20generation%20can%20achieve%20similar%0Aquality%20as%20diffusion%20models%20with%20high-quality%20dataset.%20This%20allows%20VILA-U%20to%0Aperform%20comparably%20to%20more%20complex%20models%20using%20a%20fully%20token-based%0Aautoregressive%20framework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.04429v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVILA-U%253A%2520a%2520Unified%2520Foundation%2520Model%2520Integrating%2520Visual%2520Understanding%2520and%250A%2520%2520Generation%26entry.906535625%3DYecheng%2520Wu%2520and%2520Zhuoyang%2520Zhang%2520and%2520Junyu%2520Chen%2520and%2520Haotian%2520Tang%2520and%2520Dacheng%2520Li%2520and%2520Yunhao%2520Fang%2520and%2520Ligeng%2520Zhu%2520and%2520Enze%2520Xie%2520and%2520Hongxu%2520Yin%2520and%2520Li%2520Yi%2520and%2520Song%2520Han%2520and%2520Yao%2520Lu%26entry.1292438233%3D%2520%2520VILA-U%2520is%2520a%2520Unified%2520foundation%2520model%2520that%2520integrates%2520Video%252C%2520Image%252C%2520Language%250Aunderstanding%2520and%2520generation.%2520Traditional%2520visual%2520language%2520models%2520%2528VLMs%2529%2520use%250Aseparate%2520modules%2520for%2520understanding%2520and%2520generating%2520visual%2520content%252C%2520which%2520can%250Alead%2520to%2520misalignment%2520and%2520increased%2520complexity.%2520In%2520contrast%252C%2520VILA-U%2520employs%2520a%250Asingle%2520autoregressive%2520next-token%2520prediction%2520framework%2520for%2520both%2520tasks%252C%250Aeliminating%2520the%2520need%2520for%2520additional%2520components%2520like%2520diffusion%2520models.%2520This%250Aapproach%2520not%2520only%2520simplifies%2520the%2520model%2520but%2520also%2520achieves%2520near%2520state-of-the-art%250Aperformance%2520in%2520visual%2520language%2520understanding%2520and%2520generation.%2520The%2520success%2520of%250AVILA-U%2520is%2520attributed%2520to%2520two%2520main%2520factors%253A%2520the%2520unified%2520vision%2520tower%2520that%2520aligns%250Adiscrete%2520visual%2520tokens%2520with%2520textual%2520inputs%2520during%2520pretraining%252C%2520which%2520enhances%250Avisual%2520perception%252C%2520and%2520autoregressive%2520image%2520generation%2520can%2520achieve%2520similar%250Aquality%2520as%2520diffusion%2520models%2520with%2520high-quality%2520dataset.%2520This%2520allows%2520VILA-U%2520to%250Aperform%2520comparably%2520to%2520more%2520complex%2520models%2520using%2520a%2520fully%2520token-based%250Aautoregressive%2520framework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.04429v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VILA-U%3A%20a%20Unified%20Foundation%20Model%20Integrating%20Visual%20Understanding%20and%0A%20%20Generation&entry.906535625=Yecheng%20Wu%20and%20Zhuoyang%20Zhang%20and%20Junyu%20Chen%20and%20Haotian%20Tang%20and%20Dacheng%20Li%20and%20Yunhao%20Fang%20and%20Ligeng%20Zhu%20and%20Enze%20Xie%20and%20Hongxu%20Yin%20and%20Li%20Yi%20and%20Song%20Han%20and%20Yao%20Lu&entry.1292438233=%20%20VILA-U%20is%20a%20Unified%20foundation%20model%20that%20integrates%20Video%2C%20Image%2C%20Language%0Aunderstanding%20and%20generation.%20Traditional%20visual%20language%20models%20%28VLMs%29%20use%0Aseparate%20modules%20for%20understanding%20and%20generating%20visual%20content%2C%20which%20can%0Alead%20to%20misalignment%20and%20increased%20complexity.%20In%20contrast%2C%20VILA-U%20employs%20a%0Asingle%20autoregressive%20next-token%20prediction%20framework%20for%20both%20tasks%2C%0Aeliminating%20the%20need%20for%20additional%20components%20like%20diffusion%20models.%20This%0Aapproach%20not%20only%20simplifies%20the%20model%20but%20also%20achieves%20near%20state-of-the-art%0Aperformance%20in%20visual%20language%20understanding%20and%20generation.%20The%20success%20of%0AVILA-U%20is%20attributed%20to%20two%20main%20factors%3A%20the%20unified%20vision%20tower%20that%20aligns%0Adiscrete%20visual%20tokens%20with%20textual%20inputs%20during%20pretraining%2C%20which%20enhances%0Avisual%20perception%2C%20and%20autoregressive%20image%20generation%20can%20achieve%20similar%0Aquality%20as%20diffusion%20models%20with%20high-quality%20dataset.%20This%20allows%20VILA-U%20to%0Aperform%20comparably%20to%20more%20complex%20models%20using%20a%20fully%20token-based%0Aautoregressive%20framework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.04429v2&entry.124074799=Read"},
{"title": "PixLore: A Dataset-driven Approach to Rich Image Captioning", "author": "Diego Bonilla-Salvador and Marcelino Mart\u00ednez-Sober and Joan Vila-Franc\u00e9s and Antonio Jos\u00e9 Serrano-L\u00f3pez and Pablo Rodr\u00edguez-Belenguer and Fernando Mateo", "abstract": "  In the domain of vision-language integration, generating detailed image\ncaptions poses a significant challenge due to the lack of curated and rich\ndatasets. This study introduces PixLore, a novel method that leverages Querying\nTransformers through the fine-tuning of the BLIP-2 model using the LoRa method\non a standard commercial GPU. The followed approach, which involves training on\na carefully assembled dataset from state-of-the-art Computer Vision models\ncombined and augmented by ChatGPT, addresses the question of whether intricate\nimage understanding can be achieved with an ensemble of smaller-scale models,\nreferred to as Knowledge Stitching. Comparative evaluations against major\nmodels such as GPT-4 and Google Bard demonstrate that PixLore-2.7B, despite\nhaving considerably fewer parameters, is rated higher than the existing\nState-of-the-Art models in over half of the assessments. Precisely, PixLore\noutperform Bard and BLIP-2, which score approximately 35.18% and 27.98% lower\nthan PixLore in the task of image captioning. This research not only presents a\ngroundbreaking approach but also highlights the importance of well-curated\ndatasets in enhancing the performance of smaller models.\n", "link": "http://arxiv.org/abs/2312.05349v3", "date": "2024-10-23", "relevancy": 2.8656, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5728}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PixLore%3A%20A%20Dataset-driven%20Approach%20to%20Rich%20Image%20Captioning&body=Title%3A%20PixLore%3A%20A%20Dataset-driven%20Approach%20to%20Rich%20Image%20Captioning%0AAuthor%3A%20Diego%20Bonilla-Salvador%20and%20Marcelino%20Mart%C3%ADnez-Sober%20and%20Joan%20Vila-Franc%C3%A9s%20and%20Antonio%20Jos%C3%A9%20Serrano-L%C3%B3pez%20and%20Pablo%20Rodr%C3%ADguez-Belenguer%20and%20Fernando%20Mateo%0AAbstract%3A%20%20%20In%20the%20domain%20of%20vision-language%20integration%2C%20generating%20detailed%20image%0Acaptions%20poses%20a%20significant%20challenge%20due%20to%20the%20lack%20of%20curated%20and%20rich%0Adatasets.%20This%20study%20introduces%20PixLore%2C%20a%20novel%20method%20that%20leverages%20Querying%0ATransformers%20through%20the%20fine-tuning%20of%20the%20BLIP-2%20model%20using%20the%20LoRa%20method%0Aon%20a%20standard%20commercial%20GPU.%20The%20followed%20approach%2C%20which%20involves%20training%20on%0Aa%20carefully%20assembled%20dataset%20from%20state-of-the-art%20Computer%20Vision%20models%0Acombined%20and%20augmented%20by%20ChatGPT%2C%20addresses%20the%20question%20of%20whether%20intricate%0Aimage%20understanding%20can%20be%20achieved%20with%20an%20ensemble%20of%20smaller-scale%20models%2C%0Areferred%20to%20as%20Knowledge%20Stitching.%20Comparative%20evaluations%20against%20major%0Amodels%20such%20as%20GPT-4%20and%20Google%20Bard%20demonstrate%20that%20PixLore-2.7B%2C%20despite%0Ahaving%20considerably%20fewer%20parameters%2C%20is%20rated%20higher%20than%20the%20existing%0AState-of-the-Art%20models%20in%20over%20half%20of%20the%20assessments.%20Precisely%2C%20PixLore%0Aoutperform%20Bard%20and%20BLIP-2%2C%20which%20score%20approximately%2035.18%25%20and%2027.98%25%20lower%0Athan%20PixLore%20in%20the%20task%20of%20image%20captioning.%20This%20research%20not%20only%20presents%20a%0Agroundbreaking%20approach%20but%20also%20highlights%20the%20importance%20of%20well-curated%0Adatasets%20in%20enhancing%20the%20performance%20of%20smaller%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05349v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixLore%253A%2520A%2520Dataset-driven%2520Approach%2520to%2520Rich%2520Image%2520Captioning%26entry.906535625%3DDiego%2520Bonilla-Salvador%2520and%2520Marcelino%2520Mart%25C3%25ADnez-Sober%2520and%2520Joan%2520Vila-Franc%25C3%25A9s%2520and%2520Antonio%2520Jos%25C3%25A9%2520Serrano-L%25C3%25B3pez%2520and%2520Pablo%2520Rodr%25C3%25ADguez-Belenguer%2520and%2520Fernando%2520Mateo%26entry.1292438233%3D%2520%2520In%2520the%2520domain%2520of%2520vision-language%2520integration%252C%2520generating%2520detailed%2520image%250Acaptions%2520poses%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520lack%2520of%2520curated%2520and%2520rich%250Adatasets.%2520This%2520study%2520introduces%2520PixLore%252C%2520a%2520novel%2520method%2520that%2520leverages%2520Querying%250ATransformers%2520through%2520the%2520fine-tuning%2520of%2520the%2520BLIP-2%2520model%2520using%2520the%2520LoRa%2520method%250Aon%2520a%2520standard%2520commercial%2520GPU.%2520The%2520followed%2520approach%252C%2520which%2520involves%2520training%2520on%250Aa%2520carefully%2520assembled%2520dataset%2520from%2520state-of-the-art%2520Computer%2520Vision%2520models%250Acombined%2520and%2520augmented%2520by%2520ChatGPT%252C%2520addresses%2520the%2520question%2520of%2520whether%2520intricate%250Aimage%2520understanding%2520can%2520be%2520achieved%2520with%2520an%2520ensemble%2520of%2520smaller-scale%2520models%252C%250Areferred%2520to%2520as%2520Knowledge%2520Stitching.%2520Comparative%2520evaluations%2520against%2520major%250Amodels%2520such%2520as%2520GPT-4%2520and%2520Google%2520Bard%2520demonstrate%2520that%2520PixLore-2.7B%252C%2520despite%250Ahaving%2520considerably%2520fewer%2520parameters%252C%2520is%2520rated%2520higher%2520than%2520the%2520existing%250AState-of-the-Art%2520models%2520in%2520over%2520half%2520of%2520the%2520assessments.%2520Precisely%252C%2520PixLore%250Aoutperform%2520Bard%2520and%2520BLIP-2%252C%2520which%2520score%2520approximately%252035.18%2525%2520and%252027.98%2525%2520lower%250Athan%2520PixLore%2520in%2520the%2520task%2520of%2520image%2520captioning.%2520This%2520research%2520not%2520only%2520presents%2520a%250Agroundbreaking%2520approach%2520but%2520also%2520highlights%2520the%2520importance%2520of%2520well-curated%250Adatasets%2520in%2520enhancing%2520the%2520performance%2520of%2520smaller%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05349v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PixLore%3A%20A%20Dataset-driven%20Approach%20to%20Rich%20Image%20Captioning&entry.906535625=Diego%20Bonilla-Salvador%20and%20Marcelino%20Mart%C3%ADnez-Sober%20and%20Joan%20Vila-Franc%C3%A9s%20and%20Antonio%20Jos%C3%A9%20Serrano-L%C3%B3pez%20and%20Pablo%20Rodr%C3%ADguez-Belenguer%20and%20Fernando%20Mateo&entry.1292438233=%20%20In%20the%20domain%20of%20vision-language%20integration%2C%20generating%20detailed%20image%0Acaptions%20poses%20a%20significant%20challenge%20due%20to%20the%20lack%20of%20curated%20and%20rich%0Adatasets.%20This%20study%20introduces%20PixLore%2C%20a%20novel%20method%20that%20leverages%20Querying%0ATransformers%20through%20the%20fine-tuning%20of%20the%20BLIP-2%20model%20using%20the%20LoRa%20method%0Aon%20a%20standard%20commercial%20GPU.%20The%20followed%20approach%2C%20which%20involves%20training%20on%0Aa%20carefully%20assembled%20dataset%20from%20state-of-the-art%20Computer%20Vision%20models%0Acombined%20and%20augmented%20by%20ChatGPT%2C%20addresses%20the%20question%20of%20whether%20intricate%0Aimage%20understanding%20can%20be%20achieved%20with%20an%20ensemble%20of%20smaller-scale%20models%2C%0Areferred%20to%20as%20Knowledge%20Stitching.%20Comparative%20evaluations%20against%20major%0Amodels%20such%20as%20GPT-4%20and%20Google%20Bard%20demonstrate%20that%20PixLore-2.7B%2C%20despite%0Ahaving%20considerably%20fewer%20parameters%2C%20is%20rated%20higher%20than%20the%20existing%0AState-of-the-Art%20models%20in%20over%20half%20of%20the%20assessments.%20Precisely%2C%20PixLore%0Aoutperform%20Bard%20and%20BLIP-2%2C%20which%20score%20approximately%2035.18%25%20and%2027.98%25%20lower%0Athan%20PixLore%20in%20the%20task%20of%20image%20captioning.%20This%20research%20not%20only%20presents%20a%0Agroundbreaking%20approach%20but%20also%20highlights%20the%20importance%20of%20well-curated%0Adatasets%20in%20enhancing%20the%20performance%20of%20smaller%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05349v3&entry.124074799=Read"},
{"title": "PnLCalib: Sports Field Registration via Points and Lines Optimization", "author": "Marc Guti\u00e9rrez-P\u00e9rez and Antonio Agudo", "abstract": "  Camera calibration in broadcast sports videos presents numerous challenges\nfor accurate sports field registration due to multiple camera angles, varying\ncamera parameters, and frequent occlusions of the field. Traditional\nsearch-based methods depend on initial camera pose estimates, which can\nstruggle in non-standard positions and dynamic environments. In response, we\npropose an optimization-based calibration pipeline that leverages a 3D soccer\nfield model and a predefined set of keypoints to overcome these limitations.\nOur method also introduces a novel refinement module that improves initial\ncalibration by using detected field lines in a non-linear optimization process.\nThis approach outperforms existing techniques in both multi-view and\nsingle-view 3D camera calibration tasks, while maintaining competitive\nperformance in homography estimation. Extensive experimentation on real-world\nsoccer datasets, including SoccerNet-Calibration, WorldCup 2014, and\nTS-WorldCup, highlights the robustness and accuracy of our method across\ndiverse broadcast scenarios. Our approach offers significant improvements in\ncamera calibration precision and reliability.\n", "link": "http://arxiv.org/abs/2404.08401v3", "date": "2024-10-23", "relevancy": 2.7242, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5663}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5577}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5105}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PnLCalib%3A%20Sports%20Field%20Registration%20via%20Points%20and%20Lines%20Optimization&body=Title%3A%20PnLCalib%3A%20Sports%20Field%20Registration%20via%20Points%20and%20Lines%20Optimization%0AAuthor%3A%20Marc%20Guti%C3%A9rrez-P%C3%A9rez%20and%20Antonio%20Agudo%0AAbstract%3A%20%20%20Camera%20calibration%20in%20broadcast%20sports%20videos%20presents%20numerous%20challenges%0Afor%20accurate%20sports%20field%20registration%20due%20to%20multiple%20camera%20angles%2C%20varying%0Acamera%20parameters%2C%20and%20frequent%20occlusions%20of%20the%20field.%20Traditional%0Asearch-based%20methods%20depend%20on%20initial%20camera%20pose%20estimates%2C%20which%20can%0Astruggle%20in%20non-standard%20positions%20and%20dynamic%20environments.%20In%20response%2C%20we%0Apropose%20an%20optimization-based%20calibration%20pipeline%20that%20leverages%20a%203D%20soccer%0Afield%20model%20and%20a%20predefined%20set%20of%20keypoints%20to%20overcome%20these%20limitations.%0AOur%20method%20also%20introduces%20a%20novel%20refinement%20module%20that%20improves%20initial%0Acalibration%20by%20using%20detected%20field%20lines%20in%20a%20non-linear%20optimization%20process.%0AThis%20approach%20outperforms%20existing%20techniques%20in%20both%20multi-view%20and%0Asingle-view%203D%20camera%20calibration%20tasks%2C%20while%20maintaining%20competitive%0Aperformance%20in%20homography%20estimation.%20Extensive%20experimentation%20on%20real-world%0Asoccer%20datasets%2C%20including%20SoccerNet-Calibration%2C%20WorldCup%202014%2C%20and%0ATS-WorldCup%2C%20highlights%20the%20robustness%20and%20accuracy%20of%20our%20method%20across%0Adiverse%20broadcast%20scenarios.%20Our%20approach%20offers%20significant%20improvements%20in%0Acamera%20calibration%20precision%20and%20reliability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.08401v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPnLCalib%253A%2520Sports%2520Field%2520Registration%2520via%2520Points%2520and%2520Lines%2520Optimization%26entry.906535625%3DMarc%2520Guti%25C3%25A9rrez-P%25C3%25A9rez%2520and%2520Antonio%2520Agudo%26entry.1292438233%3D%2520%2520Camera%2520calibration%2520in%2520broadcast%2520sports%2520videos%2520presents%2520numerous%2520challenges%250Afor%2520accurate%2520sports%2520field%2520registration%2520due%2520to%2520multiple%2520camera%2520angles%252C%2520varying%250Acamera%2520parameters%252C%2520and%2520frequent%2520occlusions%2520of%2520the%2520field.%2520Traditional%250Asearch-based%2520methods%2520depend%2520on%2520initial%2520camera%2520pose%2520estimates%252C%2520which%2520can%250Astruggle%2520in%2520non-standard%2520positions%2520and%2520dynamic%2520environments.%2520In%2520response%252C%2520we%250Apropose%2520an%2520optimization-based%2520calibration%2520pipeline%2520that%2520leverages%2520a%25203D%2520soccer%250Afield%2520model%2520and%2520a%2520predefined%2520set%2520of%2520keypoints%2520to%2520overcome%2520these%2520limitations.%250AOur%2520method%2520also%2520introduces%2520a%2520novel%2520refinement%2520module%2520that%2520improves%2520initial%250Acalibration%2520by%2520using%2520detected%2520field%2520lines%2520in%2520a%2520non-linear%2520optimization%2520process.%250AThis%2520approach%2520outperforms%2520existing%2520techniques%2520in%2520both%2520multi-view%2520and%250Asingle-view%25203D%2520camera%2520calibration%2520tasks%252C%2520while%2520maintaining%2520competitive%250Aperformance%2520in%2520homography%2520estimation.%2520Extensive%2520experimentation%2520on%2520real-world%250Asoccer%2520datasets%252C%2520including%2520SoccerNet-Calibration%252C%2520WorldCup%25202014%252C%2520and%250ATS-WorldCup%252C%2520highlights%2520the%2520robustness%2520and%2520accuracy%2520of%2520our%2520method%2520across%250Adiverse%2520broadcast%2520scenarios.%2520Our%2520approach%2520offers%2520significant%2520improvements%2520in%250Acamera%2520calibration%2520precision%2520and%2520reliability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.08401v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PnLCalib%3A%20Sports%20Field%20Registration%20via%20Points%20and%20Lines%20Optimization&entry.906535625=Marc%20Guti%C3%A9rrez-P%C3%A9rez%20and%20Antonio%20Agudo&entry.1292438233=%20%20Camera%20calibration%20in%20broadcast%20sports%20videos%20presents%20numerous%20challenges%0Afor%20accurate%20sports%20field%20registration%20due%20to%20multiple%20camera%20angles%2C%20varying%0Acamera%20parameters%2C%20and%20frequent%20occlusions%20of%20the%20field.%20Traditional%0Asearch-based%20methods%20depend%20on%20initial%20camera%20pose%20estimates%2C%20which%20can%0Astruggle%20in%20non-standard%20positions%20and%20dynamic%20environments.%20In%20response%2C%20we%0Apropose%20an%20optimization-based%20calibration%20pipeline%20that%20leverages%20a%203D%20soccer%0Afield%20model%20and%20a%20predefined%20set%20of%20keypoints%20to%20overcome%20these%20limitations.%0AOur%20method%20also%20introduces%20a%20novel%20refinement%20module%20that%20improves%20initial%0Acalibration%20by%20using%20detected%20field%20lines%20in%20a%20non-linear%20optimization%20process.%0AThis%20approach%20outperforms%20existing%20techniques%20in%20both%20multi-view%20and%0Asingle-view%203D%20camera%20calibration%20tasks%2C%20while%20maintaining%20competitive%0Aperformance%20in%20homography%20estimation.%20Extensive%20experimentation%20on%20real-world%0Asoccer%20datasets%2C%20including%20SoccerNet-Calibration%2C%20WorldCup%202014%2C%20and%0ATS-WorldCup%2C%20highlights%20the%20robustness%20and%20accuracy%20of%20our%20method%20across%0Adiverse%20broadcast%20scenarios.%20Our%20approach%20offers%20significant%20improvements%20in%0Acamera%20calibration%20precision%20and%20reliability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.08401v3&entry.124074799=Read"},
{"title": "Generalizable Prompt Tuning for Vision-Language Models", "author": "Qian Zhang", "abstract": "  Prompt tuning for vision-language models such as CLIP involves optimizing the\ntext prompts used to generate image-text pairs for specific downstream tasks.\nWhile hand-crafted or template-based prompts are generally applicable to a\nwider range of unseen classes, they tend to perform poorly in downstream tasks\n(i.e., seen classes). Learnable soft prompts, on the other hand, often perform\nwell in downstream tasks but lack generalizability. Additionally, prior\nresearch has predominantly concentrated on the textual modality, with very few\nstudies attempting to explore the prompt's generalization potential from the\nvisual modality. Keeping these limitations in mind, we investigate how to\nprompt tuning to obtain both a competitive downstream performance and\ngeneralization. The study shows that by treating soft and hand-crafted prompts\nas dual views of the textual modality, and maximizing their mutual information,\nwe can better ensemble task-specific and general semantic information.\nMoreover, to generate more expressive prompts, the study introduces a\nclass-wise augmentation from the visual modality, resulting in significant\nrobustness to a wider range of unseen classes. Extensive evaluations on several\nbenchmarks report that the proposed approach achieves competitive results in\nterms of both task-specific performance and general abilities.\n", "link": "http://arxiv.org/abs/2410.03189v2", "date": "2024-10-23", "relevancy": 2.717, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5534}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5384}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5384}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizable%20Prompt%20Tuning%20for%20Vision-Language%20Models&body=Title%3A%20Generalizable%20Prompt%20Tuning%20for%20Vision-Language%20Models%0AAuthor%3A%20Qian%20Zhang%0AAbstract%3A%20%20%20Prompt%20tuning%20for%20vision-language%20models%20such%20as%20CLIP%20involves%20optimizing%20the%0Atext%20prompts%20used%20to%20generate%20image-text%20pairs%20for%20specific%20downstream%20tasks.%0AWhile%20hand-crafted%20or%20template-based%20prompts%20are%20generally%20applicable%20to%20a%0Awider%20range%20of%20unseen%20classes%2C%20they%20tend%20to%20perform%20poorly%20in%20downstream%20tasks%0A%28i.e.%2C%20seen%20classes%29.%20Learnable%20soft%20prompts%2C%20on%20the%20other%20hand%2C%20often%20perform%0Awell%20in%20downstream%20tasks%20but%20lack%20generalizability.%20Additionally%2C%20prior%0Aresearch%20has%20predominantly%20concentrated%20on%20the%20textual%20modality%2C%20with%20very%20few%0Astudies%20attempting%20to%20explore%20the%20prompt%27s%20generalization%20potential%20from%20the%0Avisual%20modality.%20Keeping%20these%20limitations%20in%20mind%2C%20we%20investigate%20how%20to%0Aprompt%20tuning%20to%20obtain%20both%20a%20competitive%20downstream%20performance%20and%0Ageneralization.%20The%20study%20shows%20that%20by%20treating%20soft%20and%20hand-crafted%20prompts%0Aas%20dual%20views%20of%20the%20textual%20modality%2C%20and%20maximizing%20their%20mutual%20information%2C%0Awe%20can%20better%20ensemble%20task-specific%20and%20general%20semantic%20information.%0AMoreover%2C%20to%20generate%20more%20expressive%20prompts%2C%20the%20study%20introduces%20a%0Aclass-wise%20augmentation%20from%20the%20visual%20modality%2C%20resulting%20in%20significant%0Arobustness%20to%20a%20wider%20range%20of%20unseen%20classes.%20Extensive%20evaluations%20on%20several%0Abenchmarks%20report%20that%20the%20proposed%20approach%20achieves%20competitive%20results%20in%0Aterms%20of%20both%20task-specific%20performance%20and%20general%20abilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.03189v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizable%2520Prompt%2520Tuning%2520for%2520Vision-Language%2520Models%26entry.906535625%3DQian%2520Zhang%26entry.1292438233%3D%2520%2520Prompt%2520tuning%2520for%2520vision-language%2520models%2520such%2520as%2520CLIP%2520involves%2520optimizing%2520the%250Atext%2520prompts%2520used%2520to%2520generate%2520image-text%2520pairs%2520for%2520specific%2520downstream%2520tasks.%250AWhile%2520hand-crafted%2520or%2520template-based%2520prompts%2520are%2520generally%2520applicable%2520to%2520a%250Awider%2520range%2520of%2520unseen%2520classes%252C%2520they%2520tend%2520to%2520perform%2520poorly%2520in%2520downstream%2520tasks%250A%2528i.e.%252C%2520seen%2520classes%2529.%2520Learnable%2520soft%2520prompts%252C%2520on%2520the%2520other%2520hand%252C%2520often%2520perform%250Awell%2520in%2520downstream%2520tasks%2520but%2520lack%2520generalizability.%2520Additionally%252C%2520prior%250Aresearch%2520has%2520predominantly%2520concentrated%2520on%2520the%2520textual%2520modality%252C%2520with%2520very%2520few%250Astudies%2520attempting%2520to%2520explore%2520the%2520prompt%2527s%2520generalization%2520potential%2520from%2520the%250Avisual%2520modality.%2520Keeping%2520these%2520limitations%2520in%2520mind%252C%2520we%2520investigate%2520how%2520to%250Aprompt%2520tuning%2520to%2520obtain%2520both%2520a%2520competitive%2520downstream%2520performance%2520and%250Ageneralization.%2520The%2520study%2520shows%2520that%2520by%2520treating%2520soft%2520and%2520hand-crafted%2520prompts%250Aas%2520dual%2520views%2520of%2520the%2520textual%2520modality%252C%2520and%2520maximizing%2520their%2520mutual%2520information%252C%250Awe%2520can%2520better%2520ensemble%2520task-specific%2520and%2520general%2520semantic%2520information.%250AMoreover%252C%2520to%2520generate%2520more%2520expressive%2520prompts%252C%2520the%2520study%2520introduces%2520a%250Aclass-wise%2520augmentation%2520from%2520the%2520visual%2520modality%252C%2520resulting%2520in%2520significant%250Arobustness%2520to%2520a%2520wider%2520range%2520of%2520unseen%2520classes.%2520Extensive%2520evaluations%2520on%2520several%250Abenchmarks%2520report%2520that%2520the%2520proposed%2520approach%2520achieves%2520competitive%2520results%2520in%250Aterms%2520of%2520both%2520task-specific%2520performance%2520and%2520general%2520abilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.03189v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizable%20Prompt%20Tuning%20for%20Vision-Language%20Models&entry.906535625=Qian%20Zhang&entry.1292438233=%20%20Prompt%20tuning%20for%20vision-language%20models%20such%20as%20CLIP%20involves%20optimizing%20the%0Atext%20prompts%20used%20to%20generate%20image-text%20pairs%20for%20specific%20downstream%20tasks.%0AWhile%20hand-crafted%20or%20template-based%20prompts%20are%20generally%20applicable%20to%20a%0Awider%20range%20of%20unseen%20classes%2C%20they%20tend%20to%20perform%20poorly%20in%20downstream%20tasks%0A%28i.e.%2C%20seen%20classes%29.%20Learnable%20soft%20prompts%2C%20on%20the%20other%20hand%2C%20often%20perform%0Awell%20in%20downstream%20tasks%20but%20lack%20generalizability.%20Additionally%2C%20prior%0Aresearch%20has%20predominantly%20concentrated%20on%20the%20textual%20modality%2C%20with%20very%20few%0Astudies%20attempting%20to%20explore%20the%20prompt%27s%20generalization%20potential%20from%20the%0Avisual%20modality.%20Keeping%20these%20limitations%20in%20mind%2C%20we%20investigate%20how%20to%0Aprompt%20tuning%20to%20obtain%20both%20a%20competitive%20downstream%20performance%20and%0Ageneralization.%20The%20study%20shows%20that%20by%20treating%20soft%20and%20hand-crafted%20prompts%0Aas%20dual%20views%20of%20the%20textual%20modality%2C%20and%20maximizing%20their%20mutual%20information%2C%0Awe%20can%20better%20ensemble%20task-specific%20and%20general%20semantic%20information.%0AMoreover%2C%20to%20generate%20more%20expressive%20prompts%2C%20the%20study%20introduces%20a%0Aclass-wise%20augmentation%20from%20the%20visual%20modality%2C%20resulting%20in%20significant%0Arobustness%20to%20a%20wider%20range%20of%20unseen%20classes.%20Extensive%20evaluations%20on%20several%0Abenchmarks%20report%20that%20the%20proposed%20approach%20achieves%20competitive%20results%20in%0Aterms%20of%20both%20task-specific%20performance%20and%20general%20abilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.03189v2&entry.124074799=Read"},
{"title": "Att2CPC: Attention-Guided Lossy Attribute Compression of Point Clouds", "author": "Kai Liu and Kang You and Pan Gao and Manoranjan Paul", "abstract": "  With the great progress of 3D sensing and acquisition technology, the volume\nof point cloud data has grown dramatically, which urges the development of\nefficient point cloud compression methods. In this paper, we focus on the task\nof learned lossy point cloud attribute compression (PCAC). We propose an\nefficient attention-based method for lossy compression of point cloud\nattributes leveraging on an autoencoder architecture. Specifically, at the\nencoding side, we conduct multiple downsampling to best exploit the local\nattribute patterns, in which effective External Cross Attention (ECA) is\ndevised to hierarchically aggregate features by intergrating attributes and\ngeometry contexts. At the decoding side, the attributes of the point cloud are\nprogressively reconstructed based on the multi-scale representation and the\nzero-padding upsampling tactic. To the best of our knowledge, this is the first\napproach to introduce attention mechanism to point-based lossy PCAC task. We\nverify the compression efficiency of our model on various sequences, including\nhuman body frames, sparse objects, and large-scale point cloud scenes.\nExperiments show that our method achieves an average improvement of 1.15 dB and\n2.13 dB in BD-PSNR of Y channel and YUV channel, respectively, when comparing\nwith the state-of-the-art point-based method Deep-PCAC. Codes of this paper are\navailable at https://github.com/I2-Multimedia-Lab/Att2CPC.\n", "link": "http://arxiv.org/abs/2410.17823v1", "date": "2024-10-23", "relevancy": 2.6711, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5946}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5072}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Att2CPC%3A%20Attention-Guided%20Lossy%20Attribute%20Compression%20of%20Point%20Clouds&body=Title%3A%20Att2CPC%3A%20Attention-Guided%20Lossy%20Attribute%20Compression%20of%20Point%20Clouds%0AAuthor%3A%20Kai%20Liu%20and%20Kang%20You%20and%20Pan%20Gao%20and%20Manoranjan%20Paul%0AAbstract%3A%20%20%20With%20the%20great%20progress%20of%203D%20sensing%20and%20acquisition%20technology%2C%20the%20volume%0Aof%20point%20cloud%20data%20has%20grown%20dramatically%2C%20which%20urges%20the%20development%20of%0Aefficient%20point%20cloud%20compression%20methods.%20In%20this%20paper%2C%20we%20focus%20on%20the%20task%0Aof%20learned%20lossy%20point%20cloud%20attribute%20compression%20%28PCAC%29.%20We%20propose%20an%0Aefficient%20attention-based%20method%20for%20lossy%20compression%20of%20point%20cloud%0Aattributes%20leveraging%20on%20an%20autoencoder%20architecture.%20Specifically%2C%20at%20the%0Aencoding%20side%2C%20we%20conduct%20multiple%20downsampling%20to%20best%20exploit%20the%20local%0Aattribute%20patterns%2C%20in%20which%20effective%20External%20Cross%20Attention%20%28ECA%29%20is%0Adevised%20to%20hierarchically%20aggregate%20features%20by%20intergrating%20attributes%20and%0Ageometry%20contexts.%20At%20the%20decoding%20side%2C%20the%20attributes%20of%20the%20point%20cloud%20are%0Aprogressively%20reconstructed%20based%20on%20the%20multi-scale%20representation%20and%20the%0Azero-padding%20upsampling%20tactic.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Aapproach%20to%20introduce%20attention%20mechanism%20to%20point-based%20lossy%20PCAC%20task.%20We%0Averify%20the%20compression%20efficiency%20of%20our%20model%20on%20various%20sequences%2C%20including%0Ahuman%20body%20frames%2C%20sparse%20objects%2C%20and%20large-scale%20point%20cloud%20scenes.%0AExperiments%20show%20that%20our%20method%20achieves%20an%20average%20improvement%20of%201.15%20dB%20and%0A2.13%20dB%20in%20BD-PSNR%20of%20Y%20channel%20and%20YUV%20channel%2C%20respectively%2C%20when%20comparing%0Awith%20the%20state-of-the-art%20point-based%20method%20Deep-PCAC.%20Codes%20of%20this%20paper%20are%0Aavailable%20at%20https%3A//github.com/I2-Multimedia-Lab/Att2CPC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17823v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAtt2CPC%253A%2520Attention-Guided%2520Lossy%2520Attribute%2520Compression%2520of%2520Point%2520Clouds%26entry.906535625%3DKai%2520Liu%2520and%2520Kang%2520You%2520and%2520Pan%2520Gao%2520and%2520Manoranjan%2520Paul%26entry.1292438233%3D%2520%2520With%2520the%2520great%2520progress%2520of%25203D%2520sensing%2520and%2520acquisition%2520technology%252C%2520the%2520volume%250Aof%2520point%2520cloud%2520data%2520has%2520grown%2520dramatically%252C%2520which%2520urges%2520the%2520development%2520of%250Aefficient%2520point%2520cloud%2520compression%2520methods.%2520In%2520this%2520paper%252C%2520we%2520focus%2520on%2520the%2520task%250Aof%2520learned%2520lossy%2520point%2520cloud%2520attribute%2520compression%2520%2528PCAC%2529.%2520We%2520propose%2520an%250Aefficient%2520attention-based%2520method%2520for%2520lossy%2520compression%2520of%2520point%2520cloud%250Aattributes%2520leveraging%2520on%2520an%2520autoencoder%2520architecture.%2520Specifically%252C%2520at%2520the%250Aencoding%2520side%252C%2520we%2520conduct%2520multiple%2520downsampling%2520to%2520best%2520exploit%2520the%2520local%250Aattribute%2520patterns%252C%2520in%2520which%2520effective%2520External%2520Cross%2520Attention%2520%2528ECA%2529%2520is%250Adevised%2520to%2520hierarchically%2520aggregate%2520features%2520by%2520intergrating%2520attributes%2520and%250Ageometry%2520contexts.%2520At%2520the%2520decoding%2520side%252C%2520the%2520attributes%2520of%2520the%2520point%2520cloud%2520are%250Aprogressively%2520reconstructed%2520based%2520on%2520the%2520multi-scale%2520representation%2520and%2520the%250Azero-padding%2520upsampling%2520tactic.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%250Aapproach%2520to%2520introduce%2520attention%2520mechanism%2520to%2520point-based%2520lossy%2520PCAC%2520task.%2520We%250Averify%2520the%2520compression%2520efficiency%2520of%2520our%2520model%2520on%2520various%2520sequences%252C%2520including%250Ahuman%2520body%2520frames%252C%2520sparse%2520objects%252C%2520and%2520large-scale%2520point%2520cloud%2520scenes.%250AExperiments%2520show%2520that%2520our%2520method%2520achieves%2520an%2520average%2520improvement%2520of%25201.15%2520dB%2520and%250A2.13%2520dB%2520in%2520BD-PSNR%2520of%2520Y%2520channel%2520and%2520YUV%2520channel%252C%2520respectively%252C%2520when%2520comparing%250Awith%2520the%2520state-of-the-art%2520point-based%2520method%2520Deep-PCAC.%2520Codes%2520of%2520this%2520paper%2520are%250Aavailable%2520at%2520https%253A//github.com/I2-Multimedia-Lab/Att2CPC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17823v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Att2CPC%3A%20Attention-Guided%20Lossy%20Attribute%20Compression%20of%20Point%20Clouds&entry.906535625=Kai%20Liu%20and%20Kang%20You%20and%20Pan%20Gao%20and%20Manoranjan%20Paul&entry.1292438233=%20%20With%20the%20great%20progress%20of%203D%20sensing%20and%20acquisition%20technology%2C%20the%20volume%0Aof%20point%20cloud%20data%20has%20grown%20dramatically%2C%20which%20urges%20the%20development%20of%0Aefficient%20point%20cloud%20compression%20methods.%20In%20this%20paper%2C%20we%20focus%20on%20the%20task%0Aof%20learned%20lossy%20point%20cloud%20attribute%20compression%20%28PCAC%29.%20We%20propose%20an%0Aefficient%20attention-based%20method%20for%20lossy%20compression%20of%20point%20cloud%0Aattributes%20leveraging%20on%20an%20autoencoder%20architecture.%20Specifically%2C%20at%20the%0Aencoding%20side%2C%20we%20conduct%20multiple%20downsampling%20to%20best%20exploit%20the%20local%0Aattribute%20patterns%2C%20in%20which%20effective%20External%20Cross%20Attention%20%28ECA%29%20is%0Adevised%20to%20hierarchically%20aggregate%20features%20by%20intergrating%20attributes%20and%0Ageometry%20contexts.%20At%20the%20decoding%20side%2C%20the%20attributes%20of%20the%20point%20cloud%20are%0Aprogressively%20reconstructed%20based%20on%20the%20multi-scale%20representation%20and%20the%0Azero-padding%20upsampling%20tactic.%20To%20the%20best%20of%20our%20knowledge%2C%20this%20is%20the%20first%0Aapproach%20to%20introduce%20attention%20mechanism%20to%20point-based%20lossy%20PCAC%20task.%20We%0Averify%20the%20compression%20efficiency%20of%20our%20model%20on%20various%20sequences%2C%20including%0Ahuman%20body%20frames%2C%20sparse%20objects%2C%20and%20large-scale%20point%20cloud%20scenes.%0AExperiments%20show%20that%20our%20method%20achieves%20an%20average%20improvement%20of%201.15%20dB%20and%0A2.13%20dB%20in%20BD-PSNR%20of%20Y%20channel%20and%20YUV%20channel%2C%20respectively%2C%20when%20comparing%0Awith%20the%20state-of-the-art%20point-based%20method%20Deep-PCAC.%20Codes%20of%20this%20paper%20are%0Aavailable%20at%20https%3A//github.com/I2-Multimedia-Lab/Att2CPC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17823v1&entry.124074799=Read"},
{"title": "Interpreting Context Look-ups in Transformers: Investigating\n  Attention-MLP Interactions", "author": "Clement Neo and Shay B. Cohen and Fazl Barez", "abstract": "  Understanding the inner workings of large language models (LLMs) is crucial\nfor advancing their theoretical foundations and real-world applications. While\nthe attention mechanism and multi-layer perceptrons (MLPs) have been studied\nindependently, their interactions remain largely unexplored. This study\ninvestigates how attention heads and next-token neurons interact in LLMs to\npredict new words. We propose a methodology to identify next-token neurons,\nfind prompts that highly activate them, and determine the upstream attention\nheads responsible. We then generate and evaluate explanations for the activity\nof these attention heads in an automated manner. Our findings reveal that some\nattention heads recognize specific contexts relevant to predicting a token and\nactivate a downstream token-predicting neuron accordingly. This mechanism\nprovides a deeper understanding of how attention heads work with MLP neurons to\nperform next-token prediction. Our approach offers a foundation for further\nresearch into the intricate workings of LLMs and their impact on text\ngeneration and understanding.\n", "link": "http://arxiv.org/abs/2402.15055v2", "date": "2024-10-23", "relevancy": 2.6637, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5446}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpreting%20Context%20Look-ups%20in%20Transformers%3A%20Investigating%0A%20%20Attention-MLP%20Interactions&body=Title%3A%20Interpreting%20Context%20Look-ups%20in%20Transformers%3A%20Investigating%0A%20%20Attention-MLP%20Interactions%0AAuthor%3A%20Clement%20Neo%20and%20Shay%20B.%20Cohen%20and%20Fazl%20Barez%0AAbstract%3A%20%20%20Understanding%20the%20inner%20workings%20of%20large%20language%20models%20%28LLMs%29%20is%20crucial%0Afor%20advancing%20their%20theoretical%20foundations%20and%20real-world%20applications.%20While%0Athe%20attention%20mechanism%20and%20multi-layer%20perceptrons%20%28MLPs%29%20have%20been%20studied%0Aindependently%2C%20their%20interactions%20remain%20largely%20unexplored.%20This%20study%0Ainvestigates%20how%20attention%20heads%20and%20next-token%20neurons%20interact%20in%20LLMs%20to%0Apredict%20new%20words.%20We%20propose%20a%20methodology%20to%20identify%20next-token%20neurons%2C%0Afind%20prompts%20that%20highly%20activate%20them%2C%20and%20determine%20the%20upstream%20attention%0Aheads%20responsible.%20We%20then%20generate%20and%20evaluate%20explanations%20for%20the%20activity%0Aof%20these%20attention%20heads%20in%20an%20automated%20manner.%20Our%20findings%20reveal%20that%20some%0Aattention%20heads%20recognize%20specific%20contexts%20relevant%20to%20predicting%20a%20token%20and%0Aactivate%20a%20downstream%20token-predicting%20neuron%20accordingly.%20This%20mechanism%0Aprovides%20a%20deeper%20understanding%20of%20how%20attention%20heads%20work%20with%20MLP%20neurons%20to%0Aperform%20next-token%20prediction.%20Our%20approach%20offers%20a%20foundation%20for%20further%0Aresearch%20into%20the%20intricate%20workings%20of%20LLMs%20and%20their%20impact%20on%20text%0Ageneration%20and%20understanding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15055v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpreting%2520Context%2520Look-ups%2520in%2520Transformers%253A%2520Investigating%250A%2520%2520Attention-MLP%2520Interactions%26entry.906535625%3DClement%2520Neo%2520and%2520Shay%2520B.%2520Cohen%2520and%2520Fazl%2520Barez%26entry.1292438233%3D%2520%2520Understanding%2520the%2520inner%2520workings%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520is%2520crucial%250Afor%2520advancing%2520their%2520theoretical%2520foundations%2520and%2520real-world%2520applications.%2520While%250Athe%2520attention%2520mechanism%2520and%2520multi-layer%2520perceptrons%2520%2528MLPs%2529%2520have%2520been%2520studied%250Aindependently%252C%2520their%2520interactions%2520remain%2520largely%2520unexplored.%2520This%2520study%250Ainvestigates%2520how%2520attention%2520heads%2520and%2520next-token%2520neurons%2520interact%2520in%2520LLMs%2520to%250Apredict%2520new%2520words.%2520We%2520propose%2520a%2520methodology%2520to%2520identify%2520next-token%2520neurons%252C%250Afind%2520prompts%2520that%2520highly%2520activate%2520them%252C%2520and%2520determine%2520the%2520upstream%2520attention%250Aheads%2520responsible.%2520We%2520then%2520generate%2520and%2520evaluate%2520explanations%2520for%2520the%2520activity%250Aof%2520these%2520attention%2520heads%2520in%2520an%2520automated%2520manner.%2520Our%2520findings%2520reveal%2520that%2520some%250Aattention%2520heads%2520recognize%2520specific%2520contexts%2520relevant%2520to%2520predicting%2520a%2520token%2520and%250Aactivate%2520a%2520downstream%2520token-predicting%2520neuron%2520accordingly.%2520This%2520mechanism%250Aprovides%2520a%2520deeper%2520understanding%2520of%2520how%2520attention%2520heads%2520work%2520with%2520MLP%2520neurons%2520to%250Aperform%2520next-token%2520prediction.%2520Our%2520approach%2520offers%2520a%2520foundation%2520for%2520further%250Aresearch%2520into%2520the%2520intricate%2520workings%2520of%2520LLMs%2520and%2520their%2520impact%2520on%2520text%250Ageneration%2520and%2520understanding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.15055v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpreting%20Context%20Look-ups%20in%20Transformers%3A%20Investigating%0A%20%20Attention-MLP%20Interactions&entry.906535625=Clement%20Neo%20and%20Shay%20B.%20Cohen%20and%20Fazl%20Barez&entry.1292438233=%20%20Understanding%20the%20inner%20workings%20of%20large%20language%20models%20%28LLMs%29%20is%20crucial%0Afor%20advancing%20their%20theoretical%20foundations%20and%20real-world%20applications.%20While%0Athe%20attention%20mechanism%20and%20multi-layer%20perceptrons%20%28MLPs%29%20have%20been%20studied%0Aindependently%2C%20their%20interactions%20remain%20largely%20unexplored.%20This%20study%0Ainvestigates%20how%20attention%20heads%20and%20next-token%20neurons%20interact%20in%20LLMs%20to%0Apredict%20new%20words.%20We%20propose%20a%20methodology%20to%20identify%20next-token%20neurons%2C%0Afind%20prompts%20that%20highly%20activate%20them%2C%20and%20determine%20the%20upstream%20attention%0Aheads%20responsible.%20We%20then%20generate%20and%20evaluate%20explanations%20for%20the%20activity%0Aof%20these%20attention%20heads%20in%20an%20automated%20manner.%20Our%20findings%20reveal%20that%20some%0Aattention%20heads%20recognize%20specific%20contexts%20relevant%20to%20predicting%20a%20token%20and%0Aactivate%20a%20downstream%20token-predicting%20neuron%20accordingly.%20This%20mechanism%0Aprovides%20a%20deeper%20understanding%20of%20how%20attention%20heads%20work%20with%20MLP%20neurons%20to%0Aperform%20next-token%20prediction.%20Our%20approach%20offers%20a%20foundation%20for%20further%0Aresearch%20into%20the%20intricate%20workings%20of%20LLMs%20and%20their%20impact%20on%20text%0Ageneration%20and%20understanding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15055v2&entry.124074799=Read"},
{"title": "ODTFormer: Efficient Obstacle Detection and Tracking with Stereo Cameras\n  Based on Transformer", "author": "Tianye Ding and Hongyu Li and Huaizu Jiang", "abstract": "  Obstacle detection and tracking represent a critical component in robot\nautonomous navigation. In this paper, we propose ODTFormer, a Transformer-based\nmodel to address both obstacle detection and tracking problems. For the\ndetection task, our approach leverages deformable attention to construct a 3D\ncost volume, which is decoded progressively in the form of voxel occupancy\ngrids. We further track the obstacles by matching the voxels between\nconsecutive frames. The entire model can be optimized in an end-to-end manner.\nThrough extensive experiments on DrivingStereo and KITTI benchmarks, our model\nachieves state-of-the-art performance in the obstacle detection task. We also\nreport comparable accuracy to state-of-the-art obstacle tracking models while\nrequiring only a fraction of their computation cost, typically ten-fold to\ntwenty-fold less. The code and model weights will be publicly released.\n", "link": "http://arxiv.org/abs/2403.14626v2", "date": "2024-10-23", "relevancy": 2.6505, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5366}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5275}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5262}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ODTFormer%3A%20Efficient%20Obstacle%20Detection%20and%20Tracking%20with%20Stereo%20Cameras%0A%20%20Based%20on%20Transformer&body=Title%3A%20ODTFormer%3A%20Efficient%20Obstacle%20Detection%20and%20Tracking%20with%20Stereo%20Cameras%0A%20%20Based%20on%20Transformer%0AAuthor%3A%20Tianye%20Ding%20and%20Hongyu%20Li%20and%20Huaizu%20Jiang%0AAbstract%3A%20%20%20Obstacle%20detection%20and%20tracking%20represent%20a%20critical%20component%20in%20robot%0Aautonomous%20navigation.%20In%20this%20paper%2C%20we%20propose%20ODTFormer%2C%20a%20Transformer-based%0Amodel%20to%20address%20both%20obstacle%20detection%20and%20tracking%20problems.%20For%20the%0Adetection%20task%2C%20our%20approach%20leverages%20deformable%20attention%20to%20construct%20a%203D%0Acost%20volume%2C%20which%20is%20decoded%20progressively%20in%20the%20form%20of%20voxel%20occupancy%0Agrids.%20We%20further%20track%20the%20obstacles%20by%20matching%20the%20voxels%20between%0Aconsecutive%20frames.%20The%20entire%20model%20can%20be%20optimized%20in%20an%20end-to-end%20manner.%0AThrough%20extensive%20experiments%20on%20DrivingStereo%20and%20KITTI%20benchmarks%2C%20our%20model%0Aachieves%20state-of-the-art%20performance%20in%20the%20obstacle%20detection%20task.%20We%20also%0Areport%20comparable%20accuracy%20to%20state-of-the-art%20obstacle%20tracking%20models%20while%0Arequiring%20only%20a%20fraction%20of%20their%20computation%20cost%2C%20typically%20ten-fold%20to%0Atwenty-fold%20less.%20The%20code%20and%20model%20weights%20will%20be%20publicly%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14626v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DODTFormer%253A%2520Efficient%2520Obstacle%2520Detection%2520and%2520Tracking%2520with%2520Stereo%2520Cameras%250A%2520%2520Based%2520on%2520Transformer%26entry.906535625%3DTianye%2520Ding%2520and%2520Hongyu%2520Li%2520and%2520Huaizu%2520Jiang%26entry.1292438233%3D%2520%2520Obstacle%2520detection%2520and%2520tracking%2520represent%2520a%2520critical%2520component%2520in%2520robot%250Aautonomous%2520navigation.%2520In%2520this%2520paper%252C%2520we%2520propose%2520ODTFormer%252C%2520a%2520Transformer-based%250Amodel%2520to%2520address%2520both%2520obstacle%2520detection%2520and%2520tracking%2520problems.%2520For%2520the%250Adetection%2520task%252C%2520our%2520approach%2520leverages%2520deformable%2520attention%2520to%2520construct%2520a%25203D%250Acost%2520volume%252C%2520which%2520is%2520decoded%2520progressively%2520in%2520the%2520form%2520of%2520voxel%2520occupancy%250Agrids.%2520We%2520further%2520track%2520the%2520obstacles%2520by%2520matching%2520the%2520voxels%2520between%250Aconsecutive%2520frames.%2520The%2520entire%2520model%2520can%2520be%2520optimized%2520in%2520an%2520end-to-end%2520manner.%250AThrough%2520extensive%2520experiments%2520on%2520DrivingStereo%2520and%2520KITTI%2520benchmarks%252C%2520our%2520model%250Aachieves%2520state-of-the-art%2520performance%2520in%2520the%2520obstacle%2520detection%2520task.%2520We%2520also%250Areport%2520comparable%2520accuracy%2520to%2520state-of-the-art%2520obstacle%2520tracking%2520models%2520while%250Arequiring%2520only%2520a%2520fraction%2520of%2520their%2520computation%2520cost%252C%2520typically%2520ten-fold%2520to%250Atwenty-fold%2520less.%2520The%2520code%2520and%2520model%2520weights%2520will%2520be%2520publicly%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14626v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ODTFormer%3A%20Efficient%20Obstacle%20Detection%20and%20Tracking%20with%20Stereo%20Cameras%0A%20%20Based%20on%20Transformer&entry.906535625=Tianye%20Ding%20and%20Hongyu%20Li%20and%20Huaizu%20Jiang&entry.1292438233=%20%20Obstacle%20detection%20and%20tracking%20represent%20a%20critical%20component%20in%20robot%0Aautonomous%20navigation.%20In%20this%20paper%2C%20we%20propose%20ODTFormer%2C%20a%20Transformer-based%0Amodel%20to%20address%20both%20obstacle%20detection%20and%20tracking%20problems.%20For%20the%0Adetection%20task%2C%20our%20approach%20leverages%20deformable%20attention%20to%20construct%20a%203D%0Acost%20volume%2C%20which%20is%20decoded%20progressively%20in%20the%20form%20of%20voxel%20occupancy%0Agrids.%20We%20further%20track%20the%20obstacles%20by%20matching%20the%20voxels%20between%0Aconsecutive%20frames.%20The%20entire%20model%20can%20be%20optimized%20in%20an%20end-to-end%20manner.%0AThrough%20extensive%20experiments%20on%20DrivingStereo%20and%20KITTI%20benchmarks%2C%20our%20model%0Aachieves%20state-of-the-art%20performance%20in%20the%20obstacle%20detection%20task.%20We%20also%0Areport%20comparable%20accuracy%20to%20state-of-the-art%20obstacle%20tracking%20models%20while%0Arequiring%20only%20a%20fraction%20of%20their%20computation%20cost%2C%20typically%20ten-fold%20to%0Atwenty-fold%20less.%20The%20code%20and%20model%20weights%20will%20be%20publicly%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14626v2&entry.124074799=Read"},
{"title": "Exploring the Adversarial Robustness of CLIP for AI-generated Image\n  Detection", "author": "Vincenzo De Rosa and Fabrizio Guillaro and Giovanni Poggi and Davide Cozzolino and Luisa Verdoliva", "abstract": "  In recent years, many forensic detectors have been proposed to detect\nAI-generated images and prevent their use for malicious purposes. Convolutional\nneural networks (CNNs) have long been the dominant architecture in this field\nand have been the subject of intense study. However, recently proposed\nTransformer-based detectors have been shown to match or even outperform\nCNN-based detectors, especially in terms of generalization. In this paper, we\nstudy the adversarial robustness of AI-generated image detectors, focusing on\nContrastive Language-Image Pretraining (CLIP)-based methods that rely on Visual\nTransformer (ViT) backbones and comparing their performance with CNN-based\nmethods. We study the robustness to different adversarial attacks under a\nvariety of conditions and analyze both numerical results and frequency-domain\npatterns. CLIP-based detectors are found to be vulnerable to white-box attacks\njust like CNN-based detectors. However, attacks do not easily transfer between\nCNN-based and CLIP-based methods. This is also confirmed by the different\ndistribution of the adversarial noise patterns in the frequency domain.\nOverall, this analysis provides new insights into the properties of forensic\ndetectors that can help to develop more effective strategies.\n", "link": "http://arxiv.org/abs/2407.19553v2", "date": "2024-10-23", "relevancy": 2.6283, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5496}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5183}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20the%20Adversarial%20Robustness%20of%20CLIP%20for%20AI-generated%20Image%0A%20%20Detection&body=Title%3A%20Exploring%20the%20Adversarial%20Robustness%20of%20CLIP%20for%20AI-generated%20Image%0A%20%20Detection%0AAuthor%3A%20Vincenzo%20De%20Rosa%20and%20Fabrizio%20Guillaro%20and%20Giovanni%20Poggi%20and%20Davide%20Cozzolino%20and%20Luisa%20Verdoliva%0AAbstract%3A%20%20%20In%20recent%20years%2C%20many%20forensic%20detectors%20have%20been%20proposed%20to%20detect%0AAI-generated%20images%20and%20prevent%20their%20use%20for%20malicious%20purposes.%20Convolutional%0Aneural%20networks%20%28CNNs%29%20have%20long%20been%20the%20dominant%20architecture%20in%20this%20field%0Aand%20have%20been%20the%20subject%20of%20intense%20study.%20However%2C%20recently%20proposed%0ATransformer-based%20detectors%20have%20been%20shown%20to%20match%20or%20even%20outperform%0ACNN-based%20detectors%2C%20especially%20in%20terms%20of%20generalization.%20In%20this%20paper%2C%20we%0Astudy%20the%20adversarial%20robustness%20of%20AI-generated%20image%20detectors%2C%20focusing%20on%0AContrastive%20Language-Image%20Pretraining%20%28CLIP%29-based%20methods%20that%20rely%20on%20Visual%0ATransformer%20%28ViT%29%20backbones%20and%20comparing%20their%20performance%20with%20CNN-based%0Amethods.%20We%20study%20the%20robustness%20to%20different%20adversarial%20attacks%20under%20a%0Avariety%20of%20conditions%20and%20analyze%20both%20numerical%20results%20and%20frequency-domain%0Apatterns.%20CLIP-based%20detectors%20are%20found%20to%20be%20vulnerable%20to%20white-box%20attacks%0Ajust%20like%20CNN-based%20detectors.%20However%2C%20attacks%20do%20not%20easily%20transfer%20between%0ACNN-based%20and%20CLIP-based%20methods.%20This%20is%20also%20confirmed%20by%20the%20different%0Adistribution%20of%20the%20adversarial%20noise%20patterns%20in%20the%20frequency%20domain.%0AOverall%2C%20this%20analysis%20provides%20new%20insights%20into%20the%20properties%20of%20forensic%0Adetectors%20that%20can%20help%20to%20develop%20more%20effective%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.19553v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520the%2520Adversarial%2520Robustness%2520of%2520CLIP%2520for%2520AI-generated%2520Image%250A%2520%2520Detection%26entry.906535625%3DVincenzo%2520De%2520Rosa%2520and%2520Fabrizio%2520Guillaro%2520and%2520Giovanni%2520Poggi%2520and%2520Davide%2520Cozzolino%2520and%2520Luisa%2520Verdoliva%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520many%2520forensic%2520detectors%2520have%2520been%2520proposed%2520to%2520detect%250AAI-generated%2520images%2520and%2520prevent%2520their%2520use%2520for%2520malicious%2520purposes.%2520Convolutional%250Aneural%2520networks%2520%2528CNNs%2529%2520have%2520long%2520been%2520the%2520dominant%2520architecture%2520in%2520this%2520field%250Aand%2520have%2520been%2520the%2520subject%2520of%2520intense%2520study.%2520However%252C%2520recently%2520proposed%250ATransformer-based%2520detectors%2520have%2520been%2520shown%2520to%2520match%2520or%2520even%2520outperform%250ACNN-based%2520detectors%252C%2520especially%2520in%2520terms%2520of%2520generalization.%2520In%2520this%2520paper%252C%2520we%250Astudy%2520the%2520adversarial%2520robustness%2520of%2520AI-generated%2520image%2520detectors%252C%2520focusing%2520on%250AContrastive%2520Language-Image%2520Pretraining%2520%2528CLIP%2529-based%2520methods%2520that%2520rely%2520on%2520Visual%250ATransformer%2520%2528ViT%2529%2520backbones%2520and%2520comparing%2520their%2520performance%2520with%2520CNN-based%250Amethods.%2520We%2520study%2520the%2520robustness%2520to%2520different%2520adversarial%2520attacks%2520under%2520a%250Avariety%2520of%2520conditions%2520and%2520analyze%2520both%2520numerical%2520results%2520and%2520frequency-domain%250Apatterns.%2520CLIP-based%2520detectors%2520are%2520found%2520to%2520be%2520vulnerable%2520to%2520white-box%2520attacks%250Ajust%2520like%2520CNN-based%2520detectors.%2520However%252C%2520attacks%2520do%2520not%2520easily%2520transfer%2520between%250ACNN-based%2520and%2520CLIP-based%2520methods.%2520This%2520is%2520also%2520confirmed%2520by%2520the%2520different%250Adistribution%2520of%2520the%2520adversarial%2520noise%2520patterns%2520in%2520the%2520frequency%2520domain.%250AOverall%252C%2520this%2520analysis%2520provides%2520new%2520insights%2520into%2520the%2520properties%2520of%2520forensic%250Adetectors%2520that%2520can%2520help%2520to%2520develop%2520more%2520effective%2520strategies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.19553v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20the%20Adversarial%20Robustness%20of%20CLIP%20for%20AI-generated%20Image%0A%20%20Detection&entry.906535625=Vincenzo%20De%20Rosa%20and%20Fabrizio%20Guillaro%20and%20Giovanni%20Poggi%20and%20Davide%20Cozzolino%20and%20Luisa%20Verdoliva&entry.1292438233=%20%20In%20recent%20years%2C%20many%20forensic%20detectors%20have%20been%20proposed%20to%20detect%0AAI-generated%20images%20and%20prevent%20their%20use%20for%20malicious%20purposes.%20Convolutional%0Aneural%20networks%20%28CNNs%29%20have%20long%20been%20the%20dominant%20architecture%20in%20this%20field%0Aand%20have%20been%20the%20subject%20of%20intense%20study.%20However%2C%20recently%20proposed%0ATransformer-based%20detectors%20have%20been%20shown%20to%20match%20or%20even%20outperform%0ACNN-based%20detectors%2C%20especially%20in%20terms%20of%20generalization.%20In%20this%20paper%2C%20we%0Astudy%20the%20adversarial%20robustness%20of%20AI-generated%20image%20detectors%2C%20focusing%20on%0AContrastive%20Language-Image%20Pretraining%20%28CLIP%29-based%20methods%20that%20rely%20on%20Visual%0ATransformer%20%28ViT%29%20backbones%20and%20comparing%20their%20performance%20with%20CNN-based%0Amethods.%20We%20study%20the%20robustness%20to%20different%20adversarial%20attacks%20under%20a%0Avariety%20of%20conditions%20and%20analyze%20both%20numerical%20results%20and%20frequency-domain%0Apatterns.%20CLIP-based%20detectors%20are%20found%20to%20be%20vulnerable%20to%20white-box%20attacks%0Ajust%20like%20CNN-based%20detectors.%20However%2C%20attacks%20do%20not%20easily%20transfer%20between%0ACNN-based%20and%20CLIP-based%20methods.%20This%20is%20also%20confirmed%20by%20the%20different%0Adistribution%20of%20the%20adversarial%20noise%20patterns%20in%20the%20frequency%20domain.%0AOverall%2C%20this%20analysis%20provides%20new%20insights%20into%20the%20properties%20of%20forensic%0Adetectors%20that%20can%20help%20to%20develop%20more%20effective%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.19553v2&entry.124074799=Read"},
{"title": "Spiking Graph Neural Network on Riemannian Manifolds", "author": "Li Sun and Zhenhao Huang and Qiqi Wan and Hao Peng and Philip S. Yu", "abstract": "  Graph neural networks (GNNs) have become the dominant solution for learning\non graphs, the typical non-Euclidean structures. Conventional GNNs, constructed\nwith the Artificial Neuron Network (ANN), have achieved impressive performance\nat the cost of high computation and energy consumption. In parallel, spiking\nGNNs with brain-like spiking neurons are drawing increasing research attention\nowing to the energy efficiency. So far, existing spiking GNNs consider graphs\nin Euclidean space, ignoring the structural geometry, and suffer from the high\nlatency issue due to Back-Propagation-Through-Time (BPTT) with the surrogate\ngradient. In light of the aforementioned issues, we are devoted to exploring\nspiking GNN on Riemannian manifolds, and present a Manifold-valued Spiking GNN\n(MSG). In particular, we design a new spiking neuron on geodesically complete\nmanifolds with the diffeomorphism, so that BPTT regarding the spikes is\nreplaced by the proposed differentiation via manifold. Theoretically, we show\nthat MSG approximates a solver of the manifold ordinary differential equation.\nExtensive experiments on common graphs show the proposed MSG achieves superior\nperformance to previous spiking GNNs and energy efficiency to conventional\nGNNs.\n", "link": "http://arxiv.org/abs/2410.17941v1", "date": "2024-10-23", "relevancy": 2.6158, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5469}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5185}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5041}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spiking%20Graph%20Neural%20Network%20on%20Riemannian%20Manifolds&body=Title%3A%20Spiking%20Graph%20Neural%20Network%20on%20Riemannian%20Manifolds%0AAuthor%3A%20Li%20Sun%20and%20Zhenhao%20Huang%20and%20Qiqi%20Wan%20and%20Hao%20Peng%20and%20Philip%20S.%20Yu%0AAbstract%3A%20%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20the%20dominant%20solution%20for%20learning%0Aon%20graphs%2C%20the%20typical%20non-Euclidean%20structures.%20Conventional%20GNNs%2C%20constructed%0Awith%20the%20Artificial%20Neuron%20Network%20%28ANN%29%2C%20have%20achieved%20impressive%20performance%0Aat%20the%20cost%20of%20high%20computation%20and%20energy%20consumption.%20In%20parallel%2C%20spiking%0AGNNs%20with%20brain-like%20spiking%20neurons%20are%20drawing%20increasing%20research%20attention%0Aowing%20to%20the%20energy%20efficiency.%20So%20far%2C%20existing%20spiking%20GNNs%20consider%20graphs%0Ain%20Euclidean%20space%2C%20ignoring%20the%20structural%20geometry%2C%20and%20suffer%20from%20the%20high%0Alatency%20issue%20due%20to%20Back-Propagation-Through-Time%20%28BPTT%29%20with%20the%20surrogate%0Agradient.%20In%20light%20of%20the%20aforementioned%20issues%2C%20we%20are%20devoted%20to%20exploring%0Aspiking%20GNN%20on%20Riemannian%20manifolds%2C%20and%20present%20a%20Manifold-valued%20Spiking%20GNN%0A%28MSG%29.%20In%20particular%2C%20we%20design%20a%20new%20spiking%20neuron%20on%20geodesically%20complete%0Amanifolds%20with%20the%20diffeomorphism%2C%20so%20that%20BPTT%20regarding%20the%20spikes%20is%0Areplaced%20by%20the%20proposed%20differentiation%20via%20manifold.%20Theoretically%2C%20we%20show%0Athat%20MSG%20approximates%20a%20solver%20of%20the%20manifold%20ordinary%20differential%20equation.%0AExtensive%20experiments%20on%20common%20graphs%20show%20the%20proposed%20MSG%20achieves%20superior%0Aperformance%20to%20previous%20spiking%20GNNs%20and%20energy%20efficiency%20to%20conventional%0AGNNs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17941v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpiking%2520Graph%2520Neural%2520Network%2520on%2520Riemannian%2520Manifolds%26entry.906535625%3DLi%2520Sun%2520and%2520Zhenhao%2520Huang%2520and%2520Qiqi%2520Wan%2520and%2520Hao%2520Peng%2520and%2520Philip%2520S.%2520Yu%26entry.1292438233%3D%2520%2520Graph%2520neural%2520networks%2520%2528GNNs%2529%2520have%2520become%2520the%2520dominant%2520solution%2520for%2520learning%250Aon%2520graphs%252C%2520the%2520typical%2520non-Euclidean%2520structures.%2520Conventional%2520GNNs%252C%2520constructed%250Awith%2520the%2520Artificial%2520Neuron%2520Network%2520%2528ANN%2529%252C%2520have%2520achieved%2520impressive%2520performance%250Aat%2520the%2520cost%2520of%2520high%2520computation%2520and%2520energy%2520consumption.%2520In%2520parallel%252C%2520spiking%250AGNNs%2520with%2520brain-like%2520spiking%2520neurons%2520are%2520drawing%2520increasing%2520research%2520attention%250Aowing%2520to%2520the%2520energy%2520efficiency.%2520So%2520far%252C%2520existing%2520spiking%2520GNNs%2520consider%2520graphs%250Ain%2520Euclidean%2520space%252C%2520ignoring%2520the%2520structural%2520geometry%252C%2520and%2520suffer%2520from%2520the%2520high%250Alatency%2520issue%2520due%2520to%2520Back-Propagation-Through-Time%2520%2528BPTT%2529%2520with%2520the%2520surrogate%250Agradient.%2520In%2520light%2520of%2520the%2520aforementioned%2520issues%252C%2520we%2520are%2520devoted%2520to%2520exploring%250Aspiking%2520GNN%2520on%2520Riemannian%2520manifolds%252C%2520and%2520present%2520a%2520Manifold-valued%2520Spiking%2520GNN%250A%2528MSG%2529.%2520In%2520particular%252C%2520we%2520design%2520a%2520new%2520spiking%2520neuron%2520on%2520geodesically%2520complete%250Amanifolds%2520with%2520the%2520diffeomorphism%252C%2520so%2520that%2520BPTT%2520regarding%2520the%2520spikes%2520is%250Areplaced%2520by%2520the%2520proposed%2520differentiation%2520via%2520manifold.%2520Theoretically%252C%2520we%2520show%250Athat%2520MSG%2520approximates%2520a%2520solver%2520of%2520the%2520manifold%2520ordinary%2520differential%2520equation.%250AExtensive%2520experiments%2520on%2520common%2520graphs%2520show%2520the%2520proposed%2520MSG%2520achieves%2520superior%250Aperformance%2520to%2520previous%2520spiking%2520GNNs%2520and%2520energy%2520efficiency%2520to%2520conventional%250AGNNs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17941v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spiking%20Graph%20Neural%20Network%20on%20Riemannian%20Manifolds&entry.906535625=Li%20Sun%20and%20Zhenhao%20Huang%20and%20Qiqi%20Wan%20and%20Hao%20Peng%20and%20Philip%20S.%20Yu&entry.1292438233=%20%20Graph%20neural%20networks%20%28GNNs%29%20have%20become%20the%20dominant%20solution%20for%20learning%0Aon%20graphs%2C%20the%20typical%20non-Euclidean%20structures.%20Conventional%20GNNs%2C%20constructed%0Awith%20the%20Artificial%20Neuron%20Network%20%28ANN%29%2C%20have%20achieved%20impressive%20performance%0Aat%20the%20cost%20of%20high%20computation%20and%20energy%20consumption.%20In%20parallel%2C%20spiking%0AGNNs%20with%20brain-like%20spiking%20neurons%20are%20drawing%20increasing%20research%20attention%0Aowing%20to%20the%20energy%20efficiency.%20So%20far%2C%20existing%20spiking%20GNNs%20consider%20graphs%0Ain%20Euclidean%20space%2C%20ignoring%20the%20structural%20geometry%2C%20and%20suffer%20from%20the%20high%0Alatency%20issue%20due%20to%20Back-Propagation-Through-Time%20%28BPTT%29%20with%20the%20surrogate%0Agradient.%20In%20light%20of%20the%20aforementioned%20issues%2C%20we%20are%20devoted%20to%20exploring%0Aspiking%20GNN%20on%20Riemannian%20manifolds%2C%20and%20present%20a%20Manifold-valued%20Spiking%20GNN%0A%28MSG%29.%20In%20particular%2C%20we%20design%20a%20new%20spiking%20neuron%20on%20geodesically%20complete%0Amanifolds%20with%20the%20diffeomorphism%2C%20so%20that%20BPTT%20regarding%20the%20spikes%20is%0Areplaced%20by%20the%20proposed%20differentiation%20via%20manifold.%20Theoretically%2C%20we%20show%0Athat%20MSG%20approximates%20a%20solver%20of%20the%20manifold%20ordinary%20differential%20equation.%0AExtensive%20experiments%20on%20common%20graphs%20show%20the%20proposed%20MSG%20achieves%20superior%0Aperformance%20to%20previous%20spiking%20GNNs%20and%20energy%20efficiency%20to%20conventional%0AGNNs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17941v1&entry.124074799=Read"},
{"title": "CPE-Pro: A Structure-Sensitive Deep Learning Method for Protein\n  Representation and Origin Evaluation", "author": "Wenrui Gou and Wenhui Ge and Yang Tan and Mingchen Li and Guisheng Fan and Huiqun Yu", "abstract": "  Protein structures are important for understanding their functions and\ninteractions. Currently, many protein structure prediction methods are\nenriching the structure database. Discriminating the origin of structures is\ncrucial for distinguishing between experimentally resolved and computationally\npredicted structures, evaluating the reliability of prediction methods, and\nguiding downstream biological studies. Building on works in structure\nprediction, We developed a structure-sensitive supervised deep learning model,\nCrystal vs Predicted Evaluator for Protein Structure (CPE-Pro), to represent\nand discriminate the origin of protein structures. CPE-Pro learns the\nstructural information of proteins and captures inter-structural differences to\nachieve accurate traceability on four data classes, and is expected to be\nextended to more. Simultaneously, we utilized Foldseek to encode protein\nstructures into \"structure-sequences\" and trained a protein Structural Sequence\nLanguage Model, SSLM. Preliminary experiments demonstrated that, compared to\nlarge-scale protein language models pre-trained on vast amounts of amino acid\nsequences, the \"structure-sequence\" enables the language model to learn more\ninformative protein features, enhancing and optimizing structural\nrepresentations. We have provided the code, model weights, and all related\nmaterials on https://github.com/GouWenrui/CPE-Pro-main.git.\n", "link": "http://arxiv.org/abs/2410.15592v2", "date": "2024-10-23", "relevancy": 2.5941, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5326}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4912}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CPE-Pro%3A%20A%20Structure-Sensitive%20Deep%20Learning%20Method%20for%20Protein%0A%20%20Representation%20and%20Origin%20Evaluation&body=Title%3A%20CPE-Pro%3A%20A%20Structure-Sensitive%20Deep%20Learning%20Method%20for%20Protein%0A%20%20Representation%20and%20Origin%20Evaluation%0AAuthor%3A%20Wenrui%20Gou%20and%20Wenhui%20Ge%20and%20Yang%20Tan%20and%20Mingchen%20Li%20and%20Guisheng%20Fan%20and%20Huiqun%20Yu%0AAbstract%3A%20%20%20Protein%20structures%20are%20important%20for%20understanding%20their%20functions%20and%0Ainteractions.%20Currently%2C%20many%20protein%20structure%20prediction%20methods%20are%0Aenriching%20the%20structure%20database.%20Discriminating%20the%20origin%20of%20structures%20is%0Acrucial%20for%20distinguishing%20between%20experimentally%20resolved%20and%20computationally%0Apredicted%20structures%2C%20evaluating%20the%20reliability%20of%20prediction%20methods%2C%20and%0Aguiding%20downstream%20biological%20studies.%20Building%20on%20works%20in%20structure%0Aprediction%2C%20We%20developed%20a%20structure-sensitive%20supervised%20deep%20learning%20model%2C%0ACrystal%20vs%20Predicted%20Evaluator%20for%20Protein%20Structure%20%28CPE-Pro%29%2C%20to%20represent%0Aand%20discriminate%20the%20origin%20of%20protein%20structures.%20CPE-Pro%20learns%20the%0Astructural%20information%20of%20proteins%20and%20captures%20inter-structural%20differences%20to%0Aachieve%20accurate%20traceability%20on%20four%20data%20classes%2C%20and%20is%20expected%20to%20be%0Aextended%20to%20more.%20Simultaneously%2C%20we%20utilized%20Foldseek%20to%20encode%20protein%0Astructures%20into%20%22structure-sequences%22%20and%20trained%20a%20protein%20Structural%20Sequence%0ALanguage%20Model%2C%20SSLM.%20Preliminary%20experiments%20demonstrated%20that%2C%20compared%20to%0Alarge-scale%20protein%20language%20models%20pre-trained%20on%20vast%20amounts%20of%20amino%20acid%0Asequences%2C%20the%20%22structure-sequence%22%20enables%20the%20language%20model%20to%20learn%20more%0Ainformative%20protein%20features%2C%20enhancing%20and%20optimizing%20structural%0Arepresentations.%20We%20have%20provided%20the%20code%2C%20model%20weights%2C%20and%20all%20related%0Amaterials%20on%20https%3A//github.com/GouWenrui/CPE-Pro-main.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15592v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCPE-Pro%253A%2520A%2520Structure-Sensitive%2520Deep%2520Learning%2520Method%2520for%2520Protein%250A%2520%2520Representation%2520and%2520Origin%2520Evaluation%26entry.906535625%3DWenrui%2520Gou%2520and%2520Wenhui%2520Ge%2520and%2520Yang%2520Tan%2520and%2520Mingchen%2520Li%2520and%2520Guisheng%2520Fan%2520and%2520Huiqun%2520Yu%26entry.1292438233%3D%2520%2520Protein%2520structures%2520are%2520important%2520for%2520understanding%2520their%2520functions%2520and%250Ainteractions.%2520Currently%252C%2520many%2520protein%2520structure%2520prediction%2520methods%2520are%250Aenriching%2520the%2520structure%2520database.%2520Discriminating%2520the%2520origin%2520of%2520structures%2520is%250Acrucial%2520for%2520distinguishing%2520between%2520experimentally%2520resolved%2520and%2520computationally%250Apredicted%2520structures%252C%2520evaluating%2520the%2520reliability%2520of%2520prediction%2520methods%252C%2520and%250Aguiding%2520downstream%2520biological%2520studies.%2520Building%2520on%2520works%2520in%2520structure%250Aprediction%252C%2520We%2520developed%2520a%2520structure-sensitive%2520supervised%2520deep%2520learning%2520model%252C%250ACrystal%2520vs%2520Predicted%2520Evaluator%2520for%2520Protein%2520Structure%2520%2528CPE-Pro%2529%252C%2520to%2520represent%250Aand%2520discriminate%2520the%2520origin%2520of%2520protein%2520structures.%2520CPE-Pro%2520learns%2520the%250Astructural%2520information%2520of%2520proteins%2520and%2520captures%2520inter-structural%2520differences%2520to%250Aachieve%2520accurate%2520traceability%2520on%2520four%2520data%2520classes%252C%2520and%2520is%2520expected%2520to%2520be%250Aextended%2520to%2520more.%2520Simultaneously%252C%2520we%2520utilized%2520Foldseek%2520to%2520encode%2520protein%250Astructures%2520into%2520%2522structure-sequences%2522%2520and%2520trained%2520a%2520protein%2520Structural%2520Sequence%250ALanguage%2520Model%252C%2520SSLM.%2520Preliminary%2520experiments%2520demonstrated%2520that%252C%2520compared%2520to%250Alarge-scale%2520protein%2520language%2520models%2520pre-trained%2520on%2520vast%2520amounts%2520of%2520amino%2520acid%250Asequences%252C%2520the%2520%2522structure-sequence%2522%2520enables%2520the%2520language%2520model%2520to%2520learn%2520more%250Ainformative%2520protein%2520features%252C%2520enhancing%2520and%2520optimizing%2520structural%250Arepresentations.%2520We%2520have%2520provided%2520the%2520code%252C%2520model%2520weights%252C%2520and%2520all%2520related%250Amaterials%2520on%2520https%253A//github.com/GouWenrui/CPE-Pro-main.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15592v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CPE-Pro%3A%20A%20Structure-Sensitive%20Deep%20Learning%20Method%20for%20Protein%0A%20%20Representation%20and%20Origin%20Evaluation&entry.906535625=Wenrui%20Gou%20and%20Wenhui%20Ge%20and%20Yang%20Tan%20and%20Mingchen%20Li%20and%20Guisheng%20Fan%20and%20Huiqun%20Yu&entry.1292438233=%20%20Protein%20structures%20are%20important%20for%20understanding%20their%20functions%20and%0Ainteractions.%20Currently%2C%20many%20protein%20structure%20prediction%20methods%20are%0Aenriching%20the%20structure%20database.%20Discriminating%20the%20origin%20of%20structures%20is%0Acrucial%20for%20distinguishing%20between%20experimentally%20resolved%20and%20computationally%0Apredicted%20structures%2C%20evaluating%20the%20reliability%20of%20prediction%20methods%2C%20and%0Aguiding%20downstream%20biological%20studies.%20Building%20on%20works%20in%20structure%0Aprediction%2C%20We%20developed%20a%20structure-sensitive%20supervised%20deep%20learning%20model%2C%0ACrystal%20vs%20Predicted%20Evaluator%20for%20Protein%20Structure%20%28CPE-Pro%29%2C%20to%20represent%0Aand%20discriminate%20the%20origin%20of%20protein%20structures.%20CPE-Pro%20learns%20the%0Astructural%20information%20of%20proteins%20and%20captures%20inter-structural%20differences%20to%0Aachieve%20accurate%20traceability%20on%20four%20data%20classes%2C%20and%20is%20expected%20to%20be%0Aextended%20to%20more.%20Simultaneously%2C%20we%20utilized%20Foldseek%20to%20encode%20protein%0Astructures%20into%20%22structure-sequences%22%20and%20trained%20a%20protein%20Structural%20Sequence%0ALanguage%20Model%2C%20SSLM.%20Preliminary%20experiments%20demonstrated%20that%2C%20compared%20to%0Alarge-scale%20protein%20language%20models%20pre-trained%20on%20vast%20amounts%20of%20amino%20acid%0Asequences%2C%20the%20%22structure-sequence%22%20enables%20the%20language%20model%20to%20learn%20more%0Ainformative%20protein%20features%2C%20enhancing%20and%20optimizing%20structural%0Arepresentations.%20We%20have%20provided%20the%20code%2C%20model%20weights%2C%20and%20all%20related%0Amaterials%20on%20https%3A//github.com/GouWenrui/CPE-Pro-main.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15592v2&entry.124074799=Read"},
{"title": "Federated Class-Incremental Learning with Hierarchical Generative\n  Prototypes", "author": "Riccardo Salami and Pietro Buzzega and Matteo Mosconi and Mattia Verasani and Simone Calderara", "abstract": "  Federated Learning (FL) aims at unburdening the training of deep models by\ndistributing computation across multiple devices (clients) while safeguarding\ndata privacy. On top of that, Federated Continual Learning (FCL) also accounts\nfor data distribution evolving over time, mirroring the dynamic nature of\nreal-world environments. While previous studies have identified Catastrophic\nForgetting and Client Drift as primary causes of performance degradation in\nFCL, we shed light on the importance of Incremental Bias and Federated Bias,\nwhich cause models to prioritize classes that are recently introduced or\nlocally predominant, respectively. Our proposal constrains both biases in the\nlast layer by efficiently finetuning a pre-trained backbone using learnable\nprompts, resulting in clients that produce less biased representations and more\nbiased classifiers. Therefore, instead of solely relying on parameter\naggregation, we leverage generative prototypes to effectively balance the\npredictions of the global model. Our method significantly improves the current\nState Of The Art, providing an average increase of +7.8% in accuracy. Code to\nreproduce the results is provided in the suppl. material.\n", "link": "http://arxiv.org/abs/2406.02447v3", "date": "2024-10-23", "relevancy": 2.5752, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5255}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5102}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5094}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Class-Incremental%20Learning%20with%20Hierarchical%20Generative%0A%20%20Prototypes&body=Title%3A%20Federated%20Class-Incremental%20Learning%20with%20Hierarchical%20Generative%0A%20%20Prototypes%0AAuthor%3A%20Riccardo%20Salami%20and%20Pietro%20Buzzega%20and%20Matteo%20Mosconi%20and%20Mattia%20Verasani%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20aims%20at%20unburdening%20the%20training%20of%20deep%20models%20by%0Adistributing%20computation%20across%20multiple%20devices%20%28clients%29%20while%20safeguarding%0Adata%20privacy.%20On%20top%20of%20that%2C%20Federated%20Continual%20Learning%20%28FCL%29%20also%20accounts%0Afor%20data%20distribution%20evolving%20over%20time%2C%20mirroring%20the%20dynamic%20nature%20of%0Areal-world%20environments.%20While%20previous%20studies%20have%20identified%20Catastrophic%0AForgetting%20and%20Client%20Drift%20as%20primary%20causes%20of%20performance%20degradation%20in%0AFCL%2C%20we%20shed%20light%20on%20the%20importance%20of%20Incremental%20Bias%20and%20Federated%20Bias%2C%0Awhich%20cause%20models%20to%20prioritize%20classes%20that%20are%20recently%20introduced%20or%0Alocally%20predominant%2C%20respectively.%20Our%20proposal%20constrains%20both%20biases%20in%20the%0Alast%20layer%20by%20efficiently%20finetuning%20a%20pre-trained%20backbone%20using%20learnable%0Aprompts%2C%20resulting%20in%20clients%20that%20produce%20less%20biased%20representations%20and%20more%0Abiased%20classifiers.%20Therefore%2C%20instead%20of%20solely%20relying%20on%20parameter%0Aaggregation%2C%20we%20leverage%20generative%20prototypes%20to%20effectively%20balance%20the%0Apredictions%20of%20the%20global%20model.%20Our%20method%20significantly%20improves%20the%20current%0AState%20Of%20The%20Art%2C%20providing%20an%20average%20increase%20of%20%2B7.8%25%20in%20accuracy.%20Code%20to%0Areproduce%20the%20results%20is%20provided%20in%20the%20suppl.%20material.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.02447v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Class-Incremental%2520Learning%2520with%2520Hierarchical%2520Generative%250A%2520%2520Prototypes%26entry.906535625%3DRiccardo%2520Salami%2520and%2520Pietro%2520Buzzega%2520and%2520Matteo%2520Mosconi%2520and%2520Mattia%2520Verasani%2520and%2520Simone%2520Calderara%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520aims%2520at%2520unburdening%2520the%2520training%2520of%2520deep%2520models%2520by%250Adistributing%2520computation%2520across%2520multiple%2520devices%2520%2528clients%2529%2520while%2520safeguarding%250Adata%2520privacy.%2520On%2520top%2520of%2520that%252C%2520Federated%2520Continual%2520Learning%2520%2528FCL%2529%2520also%2520accounts%250Afor%2520data%2520distribution%2520evolving%2520over%2520time%252C%2520mirroring%2520the%2520dynamic%2520nature%2520of%250Areal-world%2520environments.%2520While%2520previous%2520studies%2520have%2520identified%2520Catastrophic%250AForgetting%2520and%2520Client%2520Drift%2520as%2520primary%2520causes%2520of%2520performance%2520degradation%2520in%250AFCL%252C%2520we%2520shed%2520light%2520on%2520the%2520importance%2520of%2520Incremental%2520Bias%2520and%2520Federated%2520Bias%252C%250Awhich%2520cause%2520models%2520to%2520prioritize%2520classes%2520that%2520are%2520recently%2520introduced%2520or%250Alocally%2520predominant%252C%2520respectively.%2520Our%2520proposal%2520constrains%2520both%2520biases%2520in%2520the%250Alast%2520layer%2520by%2520efficiently%2520finetuning%2520a%2520pre-trained%2520backbone%2520using%2520learnable%250Aprompts%252C%2520resulting%2520in%2520clients%2520that%2520produce%2520less%2520biased%2520representations%2520and%2520more%250Abiased%2520classifiers.%2520Therefore%252C%2520instead%2520of%2520solely%2520relying%2520on%2520parameter%250Aaggregation%252C%2520we%2520leverage%2520generative%2520prototypes%2520to%2520effectively%2520balance%2520the%250Apredictions%2520of%2520the%2520global%2520model.%2520Our%2520method%2520significantly%2520improves%2520the%2520current%250AState%2520Of%2520The%2520Art%252C%2520providing%2520an%2520average%2520increase%2520of%2520%252B7.8%2525%2520in%2520accuracy.%2520Code%2520to%250Areproduce%2520the%2520results%2520is%2520provided%2520in%2520the%2520suppl.%2520material.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.02447v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Class-Incremental%20Learning%20with%20Hierarchical%20Generative%0A%20%20Prototypes&entry.906535625=Riccardo%20Salami%20and%20Pietro%20Buzzega%20and%20Matteo%20Mosconi%20and%20Mattia%20Verasani%20and%20Simone%20Calderara&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20aims%20at%20unburdening%20the%20training%20of%20deep%20models%20by%0Adistributing%20computation%20across%20multiple%20devices%20%28clients%29%20while%20safeguarding%0Adata%20privacy.%20On%20top%20of%20that%2C%20Federated%20Continual%20Learning%20%28FCL%29%20also%20accounts%0Afor%20data%20distribution%20evolving%20over%20time%2C%20mirroring%20the%20dynamic%20nature%20of%0Areal-world%20environments.%20While%20previous%20studies%20have%20identified%20Catastrophic%0AForgetting%20and%20Client%20Drift%20as%20primary%20causes%20of%20performance%20degradation%20in%0AFCL%2C%20we%20shed%20light%20on%20the%20importance%20of%20Incremental%20Bias%20and%20Federated%20Bias%2C%0Awhich%20cause%20models%20to%20prioritize%20classes%20that%20are%20recently%20introduced%20or%0Alocally%20predominant%2C%20respectively.%20Our%20proposal%20constrains%20both%20biases%20in%20the%0Alast%20layer%20by%20efficiently%20finetuning%20a%20pre-trained%20backbone%20using%20learnable%0Aprompts%2C%20resulting%20in%20clients%20that%20produce%20less%20biased%20representations%20and%20more%0Abiased%20classifiers.%20Therefore%2C%20instead%20of%20solely%20relying%20on%20parameter%0Aaggregation%2C%20we%20leverage%20generative%20prototypes%20to%20effectively%20balance%20the%0Apredictions%20of%20the%20global%20model.%20Our%20method%20significantly%20improves%20the%20current%0AState%20Of%20The%20Art%2C%20providing%20an%20average%20increase%20of%20%2B7.8%25%20in%20accuracy.%20Code%20to%0Areproduce%20the%20results%20is%20provided%20in%20the%20suppl.%20material.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.02447v3&entry.124074799=Read"},
{"title": "Do Large Language Models Truly Grasp Mathematics? An Empirical\n  Exploration", "author": "Wei Xie and Shuoyoucheng Ma and Zhenhua Wang and Enze Wang and Baosheng Wang and Jinshu Su", "abstract": "  Despite their proficiency in math tasks, the mechanisms underlying LLMs'\nmathematical reasoning abilities remain a subject of debate. Recent studies\nsuggest that chain-of-thought (CoT) prompts can bolster mathematical reasoning\nby encouraging LLMs to employ human-like logical reasoning (System 2), enabling\nthem to excel on the Cognitive Reflection Test (CRT). To assess whether LLMs\ngenuinely possess System 2-like logical reasoning, we introduced targeted\nmodifications to CRT problems. Our findings reveal that, despite the use of CoT\nprompts, mainstream LLMs, including the latest o1-preview model, continue to\nexhibit a significant error rate. Further analysis indicates that they\npredominantly rely on System 1-like intuitive reasoning and pattern matching\nderived from training data, rather than demonstrating mastery of mathematical\nthinking. This discovery challenges the prevailing notion that LLMs possess\ngenuine logical reasoning abilities and that CoT can enhance them.\nConsequently, this work may temper overly optimistic projections regarding\nLLMs' advancement toward artificial general intelligence.\n", "link": "http://arxiv.org/abs/2410.14979v2", "date": "2024-10-23", "relevancy": 2.534, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5264}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5264}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Large%20Language%20Models%20Truly%20Grasp%20Mathematics%3F%20An%20Empirical%0A%20%20Exploration&body=Title%3A%20Do%20Large%20Language%20Models%20Truly%20Grasp%20Mathematics%3F%20An%20Empirical%0A%20%20Exploration%0AAuthor%3A%20Wei%20Xie%20and%20Shuoyoucheng%20Ma%20and%20Zhenhua%20Wang%20and%20Enze%20Wang%20and%20Baosheng%20Wang%20and%20Jinshu%20Su%0AAbstract%3A%20%20%20Despite%20their%20proficiency%20in%20math%20tasks%2C%20the%20mechanisms%20underlying%20LLMs%27%0Amathematical%20reasoning%20abilities%20remain%20a%20subject%20of%20debate.%20Recent%20studies%0Asuggest%20that%20chain-of-thought%20%28CoT%29%20prompts%20can%20bolster%20mathematical%20reasoning%0Aby%20encouraging%20LLMs%20to%20employ%20human-like%20logical%20reasoning%20%28System%202%29%2C%20enabling%0Athem%20to%20excel%20on%20the%20Cognitive%20Reflection%20Test%20%28CRT%29.%20To%20assess%20whether%20LLMs%0Agenuinely%20possess%20System%202-like%20logical%20reasoning%2C%20we%20introduced%20targeted%0Amodifications%20to%20CRT%20problems.%20Our%20findings%20reveal%20that%2C%20despite%20the%20use%20of%20CoT%0Aprompts%2C%20mainstream%20LLMs%2C%20including%20the%20latest%20o1-preview%20model%2C%20continue%20to%0Aexhibit%20a%20significant%20error%20rate.%20Further%20analysis%20indicates%20that%20they%0Apredominantly%20rely%20on%20System%201-like%20intuitive%20reasoning%20and%20pattern%20matching%0Aderived%20from%20training%20data%2C%20rather%20than%20demonstrating%20mastery%20of%20mathematical%0Athinking.%20This%20discovery%20challenges%20the%20prevailing%20notion%20that%20LLMs%20possess%0Agenuine%20logical%20reasoning%20abilities%20and%20that%20CoT%20can%20enhance%20them.%0AConsequently%2C%20this%20work%20may%20temper%20overly%20optimistic%20projections%20regarding%0ALLMs%27%20advancement%20toward%20artificial%20general%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.14979v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Large%2520Language%2520Models%2520Truly%2520Grasp%2520Mathematics%253F%2520An%2520Empirical%250A%2520%2520Exploration%26entry.906535625%3DWei%2520Xie%2520and%2520Shuoyoucheng%2520Ma%2520and%2520Zhenhua%2520Wang%2520and%2520Enze%2520Wang%2520and%2520Baosheng%2520Wang%2520and%2520Jinshu%2520Su%26entry.1292438233%3D%2520%2520Despite%2520their%2520proficiency%2520in%2520math%2520tasks%252C%2520the%2520mechanisms%2520underlying%2520LLMs%2527%250Amathematical%2520reasoning%2520abilities%2520remain%2520a%2520subject%2520of%2520debate.%2520Recent%2520studies%250Asuggest%2520that%2520chain-of-thought%2520%2528CoT%2529%2520prompts%2520can%2520bolster%2520mathematical%2520reasoning%250Aby%2520encouraging%2520LLMs%2520to%2520employ%2520human-like%2520logical%2520reasoning%2520%2528System%25202%2529%252C%2520enabling%250Athem%2520to%2520excel%2520on%2520the%2520Cognitive%2520Reflection%2520Test%2520%2528CRT%2529.%2520To%2520assess%2520whether%2520LLMs%250Agenuinely%2520possess%2520System%25202-like%2520logical%2520reasoning%252C%2520we%2520introduced%2520targeted%250Amodifications%2520to%2520CRT%2520problems.%2520Our%2520findings%2520reveal%2520that%252C%2520despite%2520the%2520use%2520of%2520CoT%250Aprompts%252C%2520mainstream%2520LLMs%252C%2520including%2520the%2520latest%2520o1-preview%2520model%252C%2520continue%2520to%250Aexhibit%2520a%2520significant%2520error%2520rate.%2520Further%2520analysis%2520indicates%2520that%2520they%250Apredominantly%2520rely%2520on%2520System%25201-like%2520intuitive%2520reasoning%2520and%2520pattern%2520matching%250Aderived%2520from%2520training%2520data%252C%2520rather%2520than%2520demonstrating%2520mastery%2520of%2520mathematical%250Athinking.%2520This%2520discovery%2520challenges%2520the%2520prevailing%2520notion%2520that%2520LLMs%2520possess%250Agenuine%2520logical%2520reasoning%2520abilities%2520and%2520that%2520CoT%2520can%2520enhance%2520them.%250AConsequently%252C%2520this%2520work%2520may%2520temper%2520overly%2520optimistic%2520projections%2520regarding%250ALLMs%2527%2520advancement%2520toward%2520artificial%2520general%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.14979v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Large%20Language%20Models%20Truly%20Grasp%20Mathematics%3F%20An%20Empirical%0A%20%20Exploration&entry.906535625=Wei%20Xie%20and%20Shuoyoucheng%20Ma%20and%20Zhenhua%20Wang%20and%20Enze%20Wang%20and%20Baosheng%20Wang%20and%20Jinshu%20Su&entry.1292438233=%20%20Despite%20their%20proficiency%20in%20math%20tasks%2C%20the%20mechanisms%20underlying%20LLMs%27%0Amathematical%20reasoning%20abilities%20remain%20a%20subject%20of%20debate.%20Recent%20studies%0Asuggest%20that%20chain-of-thought%20%28CoT%29%20prompts%20can%20bolster%20mathematical%20reasoning%0Aby%20encouraging%20LLMs%20to%20employ%20human-like%20logical%20reasoning%20%28System%202%29%2C%20enabling%0Athem%20to%20excel%20on%20the%20Cognitive%20Reflection%20Test%20%28CRT%29.%20To%20assess%20whether%20LLMs%0Agenuinely%20possess%20System%202-like%20logical%20reasoning%2C%20we%20introduced%20targeted%0Amodifications%20to%20CRT%20problems.%20Our%20findings%20reveal%20that%2C%20despite%20the%20use%20of%20CoT%0Aprompts%2C%20mainstream%20LLMs%2C%20including%20the%20latest%20o1-preview%20model%2C%20continue%20to%0Aexhibit%20a%20significant%20error%20rate.%20Further%20analysis%20indicates%20that%20they%0Apredominantly%20rely%20on%20System%201-like%20intuitive%20reasoning%20and%20pattern%20matching%0Aderived%20from%20training%20data%2C%20rather%20than%20demonstrating%20mastery%20of%20mathematical%0Athinking.%20This%20discovery%20challenges%20the%20prevailing%20notion%20that%20LLMs%20possess%0Agenuine%20logical%20reasoning%20abilities%20and%20that%20CoT%20can%20enhance%20them.%0AConsequently%2C%20this%20work%20may%20temper%20overly%20optimistic%20projections%20regarding%0ALLMs%27%20advancement%20toward%20artificial%20general%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.14979v2&entry.124074799=Read"},
{"title": "Exploring Large Language Models for Feature Selection: A Data-centric\n  Perspective", "author": "Dawei Li and Zhen Tan and Huan Liu", "abstract": "  The rapid advancement of Large Language Models (LLMs) has significantly\ninfluenced various domains, leveraging their exceptional few-shot and zero-shot\nlearning capabilities. In this work, we aim to explore and understand the\nLLMs-based feature selection methods from a data-centric perspective. We begin\nby categorizing existing feature selection methods with LLMs into two groups:\ndata-driven feature selection which requires numerical values of samples to do\nstatistical inference and text-based feature selection which utilizes prior\nknowledge of LLMs to do semantical associations using descriptive context. We\nconduct experiments in both classification and regression tasks with LLMs in\nvarious sizes (e.g., GPT-4, ChatGPT and LLaMA-2). Our findings emphasize the\neffectiveness and robustness of text-based feature selection methods and\nshowcase their potentials using a real-world medical application. We also\ndiscuss the challenges and future opportunities in employing LLMs for feature\nselection, offering insights for further research and development in this\nemerging field.\n", "link": "http://arxiv.org/abs/2408.12025v2", "date": "2024-10-23", "relevancy": 2.5188, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.514}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.514}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4832}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Large%20Language%20Models%20for%20Feature%20Selection%3A%20A%20Data-centric%0A%20%20Perspective&body=Title%3A%20Exploring%20Large%20Language%20Models%20for%20Feature%20Selection%3A%20A%20Data-centric%0A%20%20Perspective%0AAuthor%3A%20Dawei%20Li%20and%20Zhen%20Tan%20and%20Huan%20Liu%0AAbstract%3A%20%20%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20significantly%0Ainfluenced%20various%20domains%2C%20leveraging%20their%20exceptional%20few-shot%20and%20zero-shot%0Alearning%20capabilities.%20In%20this%20work%2C%20we%20aim%20to%20explore%20and%20understand%20the%0ALLMs-based%20feature%20selection%20methods%20from%20a%20data-centric%20perspective.%20We%20begin%0Aby%20categorizing%20existing%20feature%20selection%20methods%20with%20LLMs%20into%20two%20groups%3A%0Adata-driven%20feature%20selection%20which%20requires%20numerical%20values%20of%20samples%20to%20do%0Astatistical%20inference%20and%20text-based%20feature%20selection%20which%20utilizes%20prior%0Aknowledge%20of%20LLMs%20to%20do%20semantical%20associations%20using%20descriptive%20context.%20We%0Aconduct%20experiments%20in%20both%20classification%20and%20regression%20tasks%20with%20LLMs%20in%0Avarious%20sizes%20%28e.g.%2C%20GPT-4%2C%20ChatGPT%20and%20LLaMA-2%29.%20Our%20findings%20emphasize%20the%0Aeffectiveness%20and%20robustness%20of%20text-based%20feature%20selection%20methods%20and%0Ashowcase%20their%20potentials%20using%20a%20real-world%20medical%20application.%20We%20also%0Adiscuss%20the%20challenges%20and%20future%20opportunities%20in%20employing%20LLMs%20for%20feature%0Aselection%2C%20offering%20insights%20for%20further%20research%20and%20development%20in%20this%0Aemerging%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12025v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Large%2520Language%2520Models%2520for%2520Feature%2520Selection%253A%2520A%2520Data-centric%250A%2520%2520Perspective%26entry.906535625%3DDawei%2520Li%2520and%2520Zhen%2520Tan%2520and%2520Huan%2520Liu%26entry.1292438233%3D%2520%2520The%2520rapid%2520advancement%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520significantly%250Ainfluenced%2520various%2520domains%252C%2520leveraging%2520their%2520exceptional%2520few-shot%2520and%2520zero-shot%250Alearning%2520capabilities.%2520In%2520this%2520work%252C%2520we%2520aim%2520to%2520explore%2520and%2520understand%2520the%250ALLMs-based%2520feature%2520selection%2520methods%2520from%2520a%2520data-centric%2520perspective.%2520We%2520begin%250Aby%2520categorizing%2520existing%2520feature%2520selection%2520methods%2520with%2520LLMs%2520into%2520two%2520groups%253A%250Adata-driven%2520feature%2520selection%2520which%2520requires%2520numerical%2520values%2520of%2520samples%2520to%2520do%250Astatistical%2520inference%2520and%2520text-based%2520feature%2520selection%2520which%2520utilizes%2520prior%250Aknowledge%2520of%2520LLMs%2520to%2520do%2520semantical%2520associations%2520using%2520descriptive%2520context.%2520We%250Aconduct%2520experiments%2520in%2520both%2520classification%2520and%2520regression%2520tasks%2520with%2520LLMs%2520in%250Avarious%2520sizes%2520%2528e.g.%252C%2520GPT-4%252C%2520ChatGPT%2520and%2520LLaMA-2%2529.%2520Our%2520findings%2520emphasize%2520the%250Aeffectiveness%2520and%2520robustness%2520of%2520text-based%2520feature%2520selection%2520methods%2520and%250Ashowcase%2520their%2520potentials%2520using%2520a%2520real-world%2520medical%2520application.%2520We%2520also%250Adiscuss%2520the%2520challenges%2520and%2520future%2520opportunities%2520in%2520employing%2520LLMs%2520for%2520feature%250Aselection%252C%2520offering%2520insights%2520for%2520further%2520research%2520and%2520development%2520in%2520this%250Aemerging%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12025v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Large%20Language%20Models%20for%20Feature%20Selection%3A%20A%20Data-centric%0A%20%20Perspective&entry.906535625=Dawei%20Li%20and%20Zhen%20Tan%20and%20Huan%20Liu&entry.1292438233=%20%20The%20rapid%20advancement%20of%20Large%20Language%20Models%20%28LLMs%29%20has%20significantly%0Ainfluenced%20various%20domains%2C%20leveraging%20their%20exceptional%20few-shot%20and%20zero-shot%0Alearning%20capabilities.%20In%20this%20work%2C%20we%20aim%20to%20explore%20and%20understand%20the%0ALLMs-based%20feature%20selection%20methods%20from%20a%20data-centric%20perspective.%20We%20begin%0Aby%20categorizing%20existing%20feature%20selection%20methods%20with%20LLMs%20into%20two%20groups%3A%0Adata-driven%20feature%20selection%20which%20requires%20numerical%20values%20of%20samples%20to%20do%0Astatistical%20inference%20and%20text-based%20feature%20selection%20which%20utilizes%20prior%0Aknowledge%20of%20LLMs%20to%20do%20semantical%20associations%20using%20descriptive%20context.%20We%0Aconduct%20experiments%20in%20both%20classification%20and%20regression%20tasks%20with%20LLMs%20in%0Avarious%20sizes%20%28e.g.%2C%20GPT-4%2C%20ChatGPT%20and%20LLaMA-2%29.%20Our%20findings%20emphasize%20the%0Aeffectiveness%20and%20robustness%20of%20text-based%20feature%20selection%20methods%20and%0Ashowcase%20their%20potentials%20using%20a%20real-world%20medical%20application.%20We%20also%0Adiscuss%20the%20challenges%20and%20future%20opportunities%20in%20employing%20LLMs%20for%20feature%0Aselection%2C%20offering%20insights%20for%20further%20research%20and%20development%20in%20this%0Aemerging%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12025v2&entry.124074799=Read"},
{"title": "Quantum Architecture Search with Unsupervised Representation Learning", "author": "Yize Sun and Zixin Wu and Yunpu Ma and Volker Tresp", "abstract": "  Unsupervised representation learning presents new opportunities for advancing\nQuantum Architecture Search (QAS) on Noisy Intermediate-Scale Quantum (NISQ)\ndevices. QAS is designed to optimize quantum circuits for Variational Quantum\nAlgorithms (VQAs). Most QAS algorithms tightly couple the search space and\nsearch algorithm, typically requiring the evaluation of numerous quantum\ncircuits, resulting in high computational costs and limiting scalability to\nlarger quantum circuits. Predictor-based QAS algorithms mitigate this issue by\nestimating circuit performance based on structure or embedding. However, these\nmethods often demand time-intensive labeling to optimize gate parameters across\nmany circuits, which is crucial for training accurate predictors. Inspired by\nthe classical neural architecture search algorithm Arch2vec, we investigate the\npotential of unsupervised representation learning for QAS without relying on\npredictors. Our framework decouples unsupervised architecture representation\nlearning from the search process, enabling the learned representations to be\napplied across various downstream tasks. Additionally, it integrates an\nimproved quantum circuit graph encoding scheme, addressing the limitations of\nexisting representations and enhancing search efficiency. This predictor-free\napproach removes the need for large labeled datasets. During the search, we\nemploy REINFORCE and Bayesian Optimization to explore the latent representation\nspace and compare their performance against baseline methods. Our results\ndemonstrate that the framework efficiently identifies high-performing quantum\ncircuits with fewer search iterations.\n", "link": "http://arxiv.org/abs/2401.11576v3", "date": "2024-10-23", "relevancy": 2.5031, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.537}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4941}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum%20Architecture%20Search%20with%20Unsupervised%20Representation%20Learning&body=Title%3A%20Quantum%20Architecture%20Search%20with%20Unsupervised%20Representation%20Learning%0AAuthor%3A%20Yize%20Sun%20and%20Zixin%20Wu%20and%20Yunpu%20Ma%20and%20Volker%20Tresp%0AAbstract%3A%20%20%20Unsupervised%20representation%20learning%20presents%20new%20opportunities%20for%20advancing%0AQuantum%20Architecture%20Search%20%28QAS%29%20on%20Noisy%20Intermediate-Scale%20Quantum%20%28NISQ%29%0Adevices.%20QAS%20is%20designed%20to%20optimize%20quantum%20circuits%20for%20Variational%20Quantum%0AAlgorithms%20%28VQAs%29.%20Most%20QAS%20algorithms%20tightly%20couple%20the%20search%20space%20and%0Asearch%20algorithm%2C%20typically%20requiring%20the%20evaluation%20of%20numerous%20quantum%0Acircuits%2C%20resulting%20in%20high%20computational%20costs%20and%20limiting%20scalability%20to%0Alarger%20quantum%20circuits.%20Predictor-based%20QAS%20algorithms%20mitigate%20this%20issue%20by%0Aestimating%20circuit%20performance%20based%20on%20structure%20or%20embedding.%20However%2C%20these%0Amethods%20often%20demand%20time-intensive%20labeling%20to%20optimize%20gate%20parameters%20across%0Amany%20circuits%2C%20which%20is%20crucial%20for%20training%20accurate%20predictors.%20Inspired%20by%0Athe%20classical%20neural%20architecture%20search%20algorithm%20Arch2vec%2C%20we%20investigate%20the%0Apotential%20of%20unsupervised%20representation%20learning%20for%20QAS%20without%20relying%20on%0Apredictors.%20Our%20framework%20decouples%20unsupervised%20architecture%20representation%0Alearning%20from%20the%20search%20process%2C%20enabling%20the%20learned%20representations%20to%20be%0Aapplied%20across%20various%20downstream%20tasks.%20Additionally%2C%20it%20integrates%20an%0Aimproved%20quantum%20circuit%20graph%20encoding%20scheme%2C%20addressing%20the%20limitations%20of%0Aexisting%20representations%20and%20enhancing%20search%20efficiency.%20This%20predictor-free%0Aapproach%20removes%20the%20need%20for%20large%20labeled%20datasets.%20During%20the%20search%2C%20we%0Aemploy%20REINFORCE%20and%20Bayesian%20Optimization%20to%20explore%20the%20latent%20representation%0Aspace%20and%20compare%20their%20performance%20against%20baseline%20methods.%20Our%20results%0Ademonstrate%20that%20the%20framework%20efficiently%20identifies%20high-performing%20quantum%0Acircuits%20with%20fewer%20search%20iterations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.11576v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum%2520Architecture%2520Search%2520with%2520Unsupervised%2520Representation%2520Learning%26entry.906535625%3DYize%2520Sun%2520and%2520Zixin%2520Wu%2520and%2520Yunpu%2520Ma%2520and%2520Volker%2520Tresp%26entry.1292438233%3D%2520%2520Unsupervised%2520representation%2520learning%2520presents%2520new%2520opportunities%2520for%2520advancing%250AQuantum%2520Architecture%2520Search%2520%2528QAS%2529%2520on%2520Noisy%2520Intermediate-Scale%2520Quantum%2520%2528NISQ%2529%250Adevices.%2520QAS%2520is%2520designed%2520to%2520optimize%2520quantum%2520circuits%2520for%2520Variational%2520Quantum%250AAlgorithms%2520%2528VQAs%2529.%2520Most%2520QAS%2520algorithms%2520tightly%2520couple%2520the%2520search%2520space%2520and%250Asearch%2520algorithm%252C%2520typically%2520requiring%2520the%2520evaluation%2520of%2520numerous%2520quantum%250Acircuits%252C%2520resulting%2520in%2520high%2520computational%2520costs%2520and%2520limiting%2520scalability%2520to%250Alarger%2520quantum%2520circuits.%2520Predictor-based%2520QAS%2520algorithms%2520mitigate%2520this%2520issue%2520by%250Aestimating%2520circuit%2520performance%2520based%2520on%2520structure%2520or%2520embedding.%2520However%252C%2520these%250Amethods%2520often%2520demand%2520time-intensive%2520labeling%2520to%2520optimize%2520gate%2520parameters%2520across%250Amany%2520circuits%252C%2520which%2520is%2520crucial%2520for%2520training%2520accurate%2520predictors.%2520Inspired%2520by%250Athe%2520classical%2520neural%2520architecture%2520search%2520algorithm%2520Arch2vec%252C%2520we%2520investigate%2520the%250Apotential%2520of%2520unsupervised%2520representation%2520learning%2520for%2520QAS%2520without%2520relying%2520on%250Apredictors.%2520Our%2520framework%2520decouples%2520unsupervised%2520architecture%2520representation%250Alearning%2520from%2520the%2520search%2520process%252C%2520enabling%2520the%2520learned%2520representations%2520to%2520be%250Aapplied%2520across%2520various%2520downstream%2520tasks.%2520Additionally%252C%2520it%2520integrates%2520an%250Aimproved%2520quantum%2520circuit%2520graph%2520encoding%2520scheme%252C%2520addressing%2520the%2520limitations%2520of%250Aexisting%2520representations%2520and%2520enhancing%2520search%2520efficiency.%2520This%2520predictor-free%250Aapproach%2520removes%2520the%2520need%2520for%2520large%2520labeled%2520datasets.%2520During%2520the%2520search%252C%2520we%250Aemploy%2520REINFORCE%2520and%2520Bayesian%2520Optimization%2520to%2520explore%2520the%2520latent%2520representation%250Aspace%2520and%2520compare%2520their%2520performance%2520against%2520baseline%2520methods.%2520Our%2520results%250Ademonstrate%2520that%2520the%2520framework%2520efficiently%2520identifies%2520high-performing%2520quantum%250Acircuits%2520with%2520fewer%2520search%2520iterations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.11576v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum%20Architecture%20Search%20with%20Unsupervised%20Representation%20Learning&entry.906535625=Yize%20Sun%20and%20Zixin%20Wu%20and%20Yunpu%20Ma%20and%20Volker%20Tresp&entry.1292438233=%20%20Unsupervised%20representation%20learning%20presents%20new%20opportunities%20for%20advancing%0AQuantum%20Architecture%20Search%20%28QAS%29%20on%20Noisy%20Intermediate-Scale%20Quantum%20%28NISQ%29%0Adevices.%20QAS%20is%20designed%20to%20optimize%20quantum%20circuits%20for%20Variational%20Quantum%0AAlgorithms%20%28VQAs%29.%20Most%20QAS%20algorithms%20tightly%20couple%20the%20search%20space%20and%0Asearch%20algorithm%2C%20typically%20requiring%20the%20evaluation%20of%20numerous%20quantum%0Acircuits%2C%20resulting%20in%20high%20computational%20costs%20and%20limiting%20scalability%20to%0Alarger%20quantum%20circuits.%20Predictor-based%20QAS%20algorithms%20mitigate%20this%20issue%20by%0Aestimating%20circuit%20performance%20based%20on%20structure%20or%20embedding.%20However%2C%20these%0Amethods%20often%20demand%20time-intensive%20labeling%20to%20optimize%20gate%20parameters%20across%0Amany%20circuits%2C%20which%20is%20crucial%20for%20training%20accurate%20predictors.%20Inspired%20by%0Athe%20classical%20neural%20architecture%20search%20algorithm%20Arch2vec%2C%20we%20investigate%20the%0Apotential%20of%20unsupervised%20representation%20learning%20for%20QAS%20without%20relying%20on%0Apredictors.%20Our%20framework%20decouples%20unsupervised%20architecture%20representation%0Alearning%20from%20the%20search%20process%2C%20enabling%20the%20learned%20representations%20to%20be%0Aapplied%20across%20various%20downstream%20tasks.%20Additionally%2C%20it%20integrates%20an%0Aimproved%20quantum%20circuit%20graph%20encoding%20scheme%2C%20addressing%20the%20limitations%20of%0Aexisting%20representations%20and%20enhancing%20search%20efficiency.%20This%20predictor-free%0Aapproach%20removes%20the%20need%20for%20large%20labeled%20datasets.%20During%20the%20search%2C%20we%0Aemploy%20REINFORCE%20and%20Bayesian%20Optimization%20to%20explore%20the%20latent%20representation%0Aspace%20and%20compare%20their%20performance%20against%20baseline%20methods.%20Our%20results%0Ademonstrate%20that%20the%20framework%20efficiently%20identifies%20high-performing%20quantum%0Acircuits%20with%20fewer%20search%20iterations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.11576v3&entry.124074799=Read"},
{"title": "GeoCode-GPT: A Large Language Model for Geospatial Code Generation Tasks", "author": "Shuyang Hou and Zhangxiao Shen and Anqi Zhao and Jianyuan Liang and Zhipeng Gui and Xuefeng Guan and Rui Li and Huayi Wu", "abstract": "  The increasing demand for spatiotemporal data and modeling tasks in\ngeosciences has made geospatial code generation technology a critical factor in\nenhancing productivity. Although large language models (LLMs) have demonstrated\npotential in code generation tasks, they often encounter issues such as refusal\nto code or hallucination in geospatial code generation due to a lack of\ndomain-specific knowledge and code corpora. To address these challenges, this\npaper presents and open-sources the GeoCode-PT and GeoCode-SFT corpora, along\nwith the GeoCode-Eval evaluation dataset. Additionally, by leveraging QLoRA and\nLoRA for pretraining and fine-tuning, we introduce GeoCode-GPT-7B, the first\nLLM focused on geospatial code generation, fine-tuned from Code Llama-7B.\nFurthermore, we establish a comprehensive geospatial code evaluation framework,\nincorporating option matching, expert validation, and prompt engineering\nscoring for LLMs, and systematically evaluate GeoCode-GPT-7B using the\nGeoCode-Eval dataset. Experimental results show that GeoCode-GPT outperforms\nother models in multiple-choice accuracy by 9.1% to 32.1%, in code\nsummarization ability by 1.7% to 25.4%, and in code generation capability by\n1.2% to 25.1%. This paper provides a solution and empirical validation for\nenhancing LLMs' performance in geospatial code generation, extends the\nboundaries of domain-specific model applications, and offers valuable insights\ninto unlocking their potential in geospatial code generation.\n", "link": "http://arxiv.org/abs/2410.17031v2", "date": "2024-10-23", "relevancy": 2.4855, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4994}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4994}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoCode-GPT%3A%20A%20Large%20Language%20Model%20for%20Geospatial%20Code%20Generation%20Tasks&body=Title%3A%20GeoCode-GPT%3A%20A%20Large%20Language%20Model%20for%20Geospatial%20Code%20Generation%20Tasks%0AAuthor%3A%20Shuyang%20Hou%20and%20Zhangxiao%20Shen%20and%20Anqi%20Zhao%20and%20Jianyuan%20Liang%20and%20Zhipeng%20Gui%20and%20Xuefeng%20Guan%20and%20Rui%20Li%20and%20Huayi%20Wu%0AAbstract%3A%20%20%20The%20increasing%20demand%20for%20spatiotemporal%20data%20and%20modeling%20tasks%20in%0Ageosciences%20has%20made%20geospatial%20code%20generation%20technology%20a%20critical%20factor%20in%0Aenhancing%20productivity.%20Although%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%0Apotential%20in%20code%20generation%20tasks%2C%20they%20often%20encounter%20issues%20such%20as%20refusal%0Ato%20code%20or%20hallucination%20in%20geospatial%20code%20generation%20due%20to%20a%20lack%20of%0Adomain-specific%20knowledge%20and%20code%20corpora.%20To%20address%20these%20challenges%2C%20this%0Apaper%20presents%20and%20open-sources%20the%20GeoCode-PT%20and%20GeoCode-SFT%20corpora%2C%20along%0Awith%20the%20GeoCode-Eval%20evaluation%20dataset.%20Additionally%2C%20by%20leveraging%20QLoRA%20and%0ALoRA%20for%20pretraining%20and%20fine-tuning%2C%20we%20introduce%20GeoCode-GPT-7B%2C%20the%20first%0ALLM%20focused%20on%20geospatial%20code%20generation%2C%20fine-tuned%20from%20Code%20Llama-7B.%0AFurthermore%2C%20we%20establish%20a%20comprehensive%20geospatial%20code%20evaluation%20framework%2C%0Aincorporating%20option%20matching%2C%20expert%20validation%2C%20and%20prompt%20engineering%0Ascoring%20for%20LLMs%2C%20and%20systematically%20evaluate%20GeoCode-GPT-7B%20using%20the%0AGeoCode-Eval%20dataset.%20Experimental%20results%20show%20that%20GeoCode-GPT%20outperforms%0Aother%20models%20in%20multiple-choice%20accuracy%20by%209.1%25%20to%2032.1%25%2C%20in%20code%0Asummarization%20ability%20by%201.7%25%20to%2025.4%25%2C%20and%20in%20code%20generation%20capability%20by%0A1.2%25%20to%2025.1%25.%20This%20paper%20provides%20a%20solution%20and%20empirical%20validation%20for%0Aenhancing%20LLMs%27%20performance%20in%20geospatial%20code%20generation%2C%20extends%20the%0Aboundaries%20of%20domain-specific%20model%20applications%2C%20and%20offers%20valuable%20insights%0Ainto%20unlocking%20their%20potential%20in%20geospatial%20code%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17031v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoCode-GPT%253A%2520A%2520Large%2520Language%2520Model%2520for%2520Geospatial%2520Code%2520Generation%2520Tasks%26entry.906535625%3DShuyang%2520Hou%2520and%2520Zhangxiao%2520Shen%2520and%2520Anqi%2520Zhao%2520and%2520Jianyuan%2520Liang%2520and%2520Zhipeng%2520Gui%2520and%2520Xuefeng%2520Guan%2520and%2520Rui%2520Li%2520and%2520Huayi%2520Wu%26entry.1292438233%3D%2520%2520The%2520increasing%2520demand%2520for%2520spatiotemporal%2520data%2520and%2520modeling%2520tasks%2520in%250Ageosciences%2520has%2520made%2520geospatial%2520code%2520generation%2520technology%2520a%2520critical%2520factor%2520in%250Aenhancing%2520productivity.%2520Although%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%250Apotential%2520in%2520code%2520generation%2520tasks%252C%2520they%2520often%2520encounter%2520issues%2520such%2520as%2520refusal%250Ato%2520code%2520or%2520hallucination%2520in%2520geospatial%2520code%2520generation%2520due%2520to%2520a%2520lack%2520of%250Adomain-specific%2520knowledge%2520and%2520code%2520corpora.%2520To%2520address%2520these%2520challenges%252C%2520this%250Apaper%2520presents%2520and%2520open-sources%2520the%2520GeoCode-PT%2520and%2520GeoCode-SFT%2520corpora%252C%2520along%250Awith%2520the%2520GeoCode-Eval%2520evaluation%2520dataset.%2520Additionally%252C%2520by%2520leveraging%2520QLoRA%2520and%250ALoRA%2520for%2520pretraining%2520and%2520fine-tuning%252C%2520we%2520introduce%2520GeoCode-GPT-7B%252C%2520the%2520first%250ALLM%2520focused%2520on%2520geospatial%2520code%2520generation%252C%2520fine-tuned%2520from%2520Code%2520Llama-7B.%250AFurthermore%252C%2520we%2520establish%2520a%2520comprehensive%2520geospatial%2520code%2520evaluation%2520framework%252C%250Aincorporating%2520option%2520matching%252C%2520expert%2520validation%252C%2520and%2520prompt%2520engineering%250Ascoring%2520for%2520LLMs%252C%2520and%2520systematically%2520evaluate%2520GeoCode-GPT-7B%2520using%2520the%250AGeoCode-Eval%2520dataset.%2520Experimental%2520results%2520show%2520that%2520GeoCode-GPT%2520outperforms%250Aother%2520models%2520in%2520multiple-choice%2520accuracy%2520by%25209.1%2525%2520to%252032.1%2525%252C%2520in%2520code%250Asummarization%2520ability%2520by%25201.7%2525%2520to%252025.4%2525%252C%2520and%2520in%2520code%2520generation%2520capability%2520by%250A1.2%2525%2520to%252025.1%2525.%2520This%2520paper%2520provides%2520a%2520solution%2520and%2520empirical%2520validation%2520for%250Aenhancing%2520LLMs%2527%2520performance%2520in%2520geospatial%2520code%2520generation%252C%2520extends%2520the%250Aboundaries%2520of%2520domain-specific%2520model%2520applications%252C%2520and%2520offers%2520valuable%2520insights%250Ainto%2520unlocking%2520their%2520potential%2520in%2520geospatial%2520code%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17031v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoCode-GPT%3A%20A%20Large%20Language%20Model%20for%20Geospatial%20Code%20Generation%20Tasks&entry.906535625=Shuyang%20Hou%20and%20Zhangxiao%20Shen%20and%20Anqi%20Zhao%20and%20Jianyuan%20Liang%20and%20Zhipeng%20Gui%20and%20Xuefeng%20Guan%20and%20Rui%20Li%20and%20Huayi%20Wu&entry.1292438233=%20%20The%20increasing%20demand%20for%20spatiotemporal%20data%20and%20modeling%20tasks%20in%0Ageosciences%20has%20made%20geospatial%20code%20generation%20technology%20a%20critical%20factor%20in%0Aenhancing%20productivity.%20Although%20large%20language%20models%20%28LLMs%29%20have%20demonstrated%0Apotential%20in%20code%20generation%20tasks%2C%20they%20often%20encounter%20issues%20such%20as%20refusal%0Ato%20code%20or%20hallucination%20in%20geospatial%20code%20generation%20due%20to%20a%20lack%20of%0Adomain-specific%20knowledge%20and%20code%20corpora.%20To%20address%20these%20challenges%2C%20this%0Apaper%20presents%20and%20open-sources%20the%20GeoCode-PT%20and%20GeoCode-SFT%20corpora%2C%20along%0Awith%20the%20GeoCode-Eval%20evaluation%20dataset.%20Additionally%2C%20by%20leveraging%20QLoRA%20and%0ALoRA%20for%20pretraining%20and%20fine-tuning%2C%20we%20introduce%20GeoCode-GPT-7B%2C%20the%20first%0ALLM%20focused%20on%20geospatial%20code%20generation%2C%20fine-tuned%20from%20Code%20Llama-7B.%0AFurthermore%2C%20we%20establish%20a%20comprehensive%20geospatial%20code%20evaluation%20framework%2C%0Aincorporating%20option%20matching%2C%20expert%20validation%2C%20and%20prompt%20engineering%0Ascoring%20for%20LLMs%2C%20and%20systematically%20evaluate%20GeoCode-GPT-7B%20using%20the%0AGeoCode-Eval%20dataset.%20Experimental%20results%20show%20that%20GeoCode-GPT%20outperforms%0Aother%20models%20in%20multiple-choice%20accuracy%20by%209.1%25%20to%2032.1%25%2C%20in%20code%0Asummarization%20ability%20by%201.7%25%20to%2025.4%25%2C%20and%20in%20code%20generation%20capability%20by%0A1.2%25%20to%2025.1%25.%20This%20paper%20provides%20a%20solution%20and%20empirical%20validation%20for%0Aenhancing%20LLMs%27%20performance%20in%20geospatial%20code%20generation%2C%20extends%20the%0Aboundaries%20of%20domain-specific%20model%20applications%2C%20and%20offers%20valuable%20insights%0Ainto%20unlocking%20their%20potential%20in%20geospatial%20code%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17031v2&entry.124074799=Read"},
{"title": "Closed-form merging of parameter-efficient modules for Federated\n  Continual Learning", "author": "Riccardo Salami and Pietro Buzzega and Matteo Mosconi and Jacopo Bonato and Luigi Sabetta and Simone Calderara", "abstract": "  Model merging has emerged as a crucial technique in Deep Learning, enabling\nthe integration of multiple models into a unified system while preserving\nperformance and scalability. In this respect, the compositional properties of\nlow-rank adaptation techniques (e.g., LoRA) have proven beneficial, as simple\naveraging LoRA modules yields a single model that mostly integrates the\ncapabilities of all individual modules. Building on LoRA, we take a step\nfurther by imposing that the merged model matches the responses of all learned\nmodules. Solving this objective in closed form yields an indeterminate system\nwith A and B as unknown variables, indicating the existence of infinitely many\nclosed-form solutions. To address this challenge, we introduce LoRM, an\nalternating optimization strategy that trains one LoRA matrix at a time. This\nallows solving for each unknown variable individually, thus finding a unique\nsolution. We apply our proposed methodology to Federated Class-Incremental\nLearning (FCIL), ensuring alignment of model responses both between clients and\nacross tasks. Our method demonstrates state-of-the-art performance across a\nrange of FCIL scenarios.\n", "link": "http://arxiv.org/abs/2410.17961v1", "date": "2024-10-23", "relevancy": 2.4593, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5031}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4866}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4858}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Closed-form%20merging%20of%20parameter-efficient%20modules%20for%20Federated%0A%20%20Continual%20Learning&body=Title%3A%20Closed-form%20merging%20of%20parameter-efficient%20modules%20for%20Federated%0A%20%20Continual%20Learning%0AAuthor%3A%20Riccardo%20Salami%20and%20Pietro%20Buzzega%20and%20Matteo%20Mosconi%20and%20Jacopo%20Bonato%20and%20Luigi%20Sabetta%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20Model%20merging%20has%20emerged%20as%20a%20crucial%20technique%20in%20Deep%20Learning%2C%20enabling%0Athe%20integration%20of%20multiple%20models%20into%20a%20unified%20system%20while%20preserving%0Aperformance%20and%20scalability.%20In%20this%20respect%2C%20the%20compositional%20properties%20of%0Alow-rank%20adaptation%20techniques%20%28e.g.%2C%20LoRA%29%20have%20proven%20beneficial%2C%20as%20simple%0Aaveraging%20LoRA%20modules%20yields%20a%20single%20model%20that%20mostly%20integrates%20the%0Acapabilities%20of%20all%20individual%20modules.%20Building%20on%20LoRA%2C%20we%20take%20a%20step%0Afurther%20by%20imposing%20that%20the%20merged%20model%20matches%20the%20responses%20of%20all%20learned%0Amodules.%20Solving%20this%20objective%20in%20closed%20form%20yields%20an%20indeterminate%20system%0Awith%20A%20and%20B%20as%20unknown%20variables%2C%20indicating%20the%20existence%20of%20infinitely%20many%0Aclosed-form%20solutions.%20To%20address%20this%20challenge%2C%20we%20introduce%20LoRM%2C%20an%0Aalternating%20optimization%20strategy%20that%20trains%20one%20LoRA%20matrix%20at%20a%20time.%20This%0Aallows%20solving%20for%20each%20unknown%20variable%20individually%2C%20thus%20finding%20a%20unique%0Asolution.%20We%20apply%20our%20proposed%20methodology%20to%20Federated%20Class-Incremental%0ALearning%20%28FCIL%29%2C%20ensuring%20alignment%20of%20model%20responses%20both%20between%20clients%20and%0Aacross%20tasks.%20Our%20method%20demonstrates%20state-of-the-art%20performance%20across%20a%0Arange%20of%20FCIL%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClosed-form%2520merging%2520of%2520parameter-efficient%2520modules%2520for%2520Federated%250A%2520%2520Continual%2520Learning%26entry.906535625%3DRiccardo%2520Salami%2520and%2520Pietro%2520Buzzega%2520and%2520Matteo%2520Mosconi%2520and%2520Jacopo%2520Bonato%2520and%2520Luigi%2520Sabetta%2520and%2520Simone%2520Calderara%26entry.1292438233%3D%2520%2520Model%2520merging%2520has%2520emerged%2520as%2520a%2520crucial%2520technique%2520in%2520Deep%2520Learning%252C%2520enabling%250Athe%2520integration%2520of%2520multiple%2520models%2520into%2520a%2520unified%2520system%2520while%2520preserving%250Aperformance%2520and%2520scalability.%2520In%2520this%2520respect%252C%2520the%2520compositional%2520properties%2520of%250Alow-rank%2520adaptation%2520techniques%2520%2528e.g.%252C%2520LoRA%2529%2520have%2520proven%2520beneficial%252C%2520as%2520simple%250Aaveraging%2520LoRA%2520modules%2520yields%2520a%2520single%2520model%2520that%2520mostly%2520integrates%2520the%250Acapabilities%2520of%2520all%2520individual%2520modules.%2520Building%2520on%2520LoRA%252C%2520we%2520take%2520a%2520step%250Afurther%2520by%2520imposing%2520that%2520the%2520merged%2520model%2520matches%2520the%2520responses%2520of%2520all%2520learned%250Amodules.%2520Solving%2520this%2520objective%2520in%2520closed%2520form%2520yields%2520an%2520indeterminate%2520system%250Awith%2520A%2520and%2520B%2520as%2520unknown%2520variables%252C%2520indicating%2520the%2520existence%2520of%2520infinitely%2520many%250Aclosed-form%2520solutions.%2520To%2520address%2520this%2520challenge%252C%2520we%2520introduce%2520LoRM%252C%2520an%250Aalternating%2520optimization%2520strategy%2520that%2520trains%2520one%2520LoRA%2520matrix%2520at%2520a%2520time.%2520This%250Aallows%2520solving%2520for%2520each%2520unknown%2520variable%2520individually%252C%2520thus%2520finding%2520a%2520unique%250Asolution.%2520We%2520apply%2520our%2520proposed%2520methodology%2520to%2520Federated%2520Class-Incremental%250ALearning%2520%2528FCIL%2529%252C%2520ensuring%2520alignment%2520of%2520model%2520responses%2520both%2520between%2520clients%2520and%250Aacross%2520tasks.%2520Our%2520method%2520demonstrates%2520state-of-the-art%2520performance%2520across%2520a%250Arange%2520of%2520FCIL%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Closed-form%20merging%20of%20parameter-efficient%20modules%20for%20Federated%0A%20%20Continual%20Learning&entry.906535625=Riccardo%20Salami%20and%20Pietro%20Buzzega%20and%20Matteo%20Mosconi%20and%20Jacopo%20Bonato%20and%20Luigi%20Sabetta%20and%20Simone%20Calderara&entry.1292438233=%20%20Model%20merging%20has%20emerged%20as%20a%20crucial%20technique%20in%20Deep%20Learning%2C%20enabling%0Athe%20integration%20of%20multiple%20models%20into%20a%20unified%20system%20while%20preserving%0Aperformance%20and%20scalability.%20In%20this%20respect%2C%20the%20compositional%20properties%20of%0Alow-rank%20adaptation%20techniques%20%28e.g.%2C%20LoRA%29%20have%20proven%20beneficial%2C%20as%20simple%0Aaveraging%20LoRA%20modules%20yields%20a%20single%20model%20that%20mostly%20integrates%20the%0Acapabilities%20of%20all%20individual%20modules.%20Building%20on%20LoRA%2C%20we%20take%20a%20step%0Afurther%20by%20imposing%20that%20the%20merged%20model%20matches%20the%20responses%20of%20all%20learned%0Amodules.%20Solving%20this%20objective%20in%20closed%20form%20yields%20an%20indeterminate%20system%0Awith%20A%20and%20B%20as%20unknown%20variables%2C%20indicating%20the%20existence%20of%20infinitely%20many%0Aclosed-form%20solutions.%20To%20address%20this%20challenge%2C%20we%20introduce%20LoRM%2C%20an%0Aalternating%20optimization%20strategy%20that%20trains%20one%20LoRA%20matrix%20at%20a%20time.%20This%0Aallows%20solving%20for%20each%20unknown%20variable%20individually%2C%20thus%20finding%20a%20unique%0Asolution.%20We%20apply%20our%20proposed%20methodology%20to%20Federated%20Class-Incremental%0ALearning%20%28FCIL%29%2C%20ensuring%20alignment%20of%20model%20responses%20both%20between%20clients%20and%0Aacross%20tasks.%20Our%20method%20demonstrates%20state-of-the-art%20performance%20across%20a%0Arange%20of%20FCIL%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17961v1&entry.124074799=Read"},
{"title": "StockGPT: A GenAI Model for Stock Prediction and Trading", "author": "Dat Mai", "abstract": "  This paper introduces StockGPT, an autoregressive ``number'' model trained\nand tested on 70 million daily U.S.\\ stock returns over nearly 100 years.\nTreating each return series as a sequence of tokens, StockGPT automatically\nlearns the hidden patterns predictive of future returns via its attention\nmechanism. On a held-out test sample from 2001 to 2023, daily and monthly\nrebalanced long-short portfolios formed from StockGPT predictions yield strong\nperformance. The StockGPT-based portfolios span momentum and long-/short-term\nreversals, eliminating the need for manually crafted price-based strategies,\nand yield highly significant alphas against leading stock market factors,\nsuggesting a novel AI pricing effect. This highlights the immense promise of\ngenerative AI in surpassing human in making complex financial investment\ndecisions.\n", "link": "http://arxiv.org/abs/2404.05101v3", "date": "2024-10-23", "relevancy": 2.4562, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5175}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4795}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4767}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20StockGPT%3A%20A%20GenAI%20Model%20for%20Stock%20Prediction%20and%20Trading&body=Title%3A%20StockGPT%3A%20A%20GenAI%20Model%20for%20Stock%20Prediction%20and%20Trading%0AAuthor%3A%20Dat%20Mai%0AAbstract%3A%20%20%20This%20paper%20introduces%20StockGPT%2C%20an%20autoregressive%20%60%60number%27%27%20model%20trained%0Aand%20tested%20on%2070%20million%20daily%20U.S.%5C%20stock%20returns%20over%20nearly%20100%20years.%0ATreating%20each%20return%20series%20as%20a%20sequence%20of%20tokens%2C%20StockGPT%20automatically%0Alearns%20the%20hidden%20patterns%20predictive%20of%20future%20returns%20via%20its%20attention%0Amechanism.%20On%20a%20held-out%20test%20sample%20from%202001%20to%202023%2C%20daily%20and%20monthly%0Arebalanced%20long-short%20portfolios%20formed%20from%20StockGPT%20predictions%20yield%20strong%0Aperformance.%20The%20StockGPT-based%20portfolios%20span%20momentum%20and%20long-/short-term%0Areversals%2C%20eliminating%20the%20need%20for%20manually%20crafted%20price-based%20strategies%2C%0Aand%20yield%20highly%20significant%20alphas%20against%20leading%20stock%20market%20factors%2C%0Asuggesting%20a%20novel%20AI%20pricing%20effect.%20This%20highlights%20the%20immense%20promise%20of%0Agenerative%20AI%20in%20surpassing%20human%20in%20making%20complex%20financial%20investment%0Adecisions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.05101v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStockGPT%253A%2520A%2520GenAI%2520Model%2520for%2520Stock%2520Prediction%2520and%2520Trading%26entry.906535625%3DDat%2520Mai%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520StockGPT%252C%2520an%2520autoregressive%2520%2560%2560number%2527%2527%2520model%2520trained%250Aand%2520tested%2520on%252070%2520million%2520daily%2520U.S.%255C%2520stock%2520returns%2520over%2520nearly%2520100%2520years.%250ATreating%2520each%2520return%2520series%2520as%2520a%2520sequence%2520of%2520tokens%252C%2520StockGPT%2520automatically%250Alearns%2520the%2520hidden%2520patterns%2520predictive%2520of%2520future%2520returns%2520via%2520its%2520attention%250Amechanism.%2520On%2520a%2520held-out%2520test%2520sample%2520from%25202001%2520to%25202023%252C%2520daily%2520and%2520monthly%250Arebalanced%2520long-short%2520portfolios%2520formed%2520from%2520StockGPT%2520predictions%2520yield%2520strong%250Aperformance.%2520The%2520StockGPT-based%2520portfolios%2520span%2520momentum%2520and%2520long-/short-term%250Areversals%252C%2520eliminating%2520the%2520need%2520for%2520manually%2520crafted%2520price-based%2520strategies%252C%250Aand%2520yield%2520highly%2520significant%2520alphas%2520against%2520leading%2520stock%2520market%2520factors%252C%250Asuggesting%2520a%2520novel%2520AI%2520pricing%2520effect.%2520This%2520highlights%2520the%2520immense%2520promise%2520of%250Agenerative%2520AI%2520in%2520surpassing%2520human%2520in%2520making%2520complex%2520financial%2520investment%250Adecisions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.05101v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=StockGPT%3A%20A%20GenAI%20Model%20for%20Stock%20Prediction%20and%20Trading&entry.906535625=Dat%20Mai&entry.1292438233=%20%20This%20paper%20introduces%20StockGPT%2C%20an%20autoregressive%20%60%60number%27%27%20model%20trained%0Aand%20tested%20on%2070%20million%20daily%20U.S.%5C%20stock%20returns%20over%20nearly%20100%20years.%0ATreating%20each%20return%20series%20as%20a%20sequence%20of%20tokens%2C%20StockGPT%20automatically%0Alearns%20the%20hidden%20patterns%20predictive%20of%20future%20returns%20via%20its%20attention%0Amechanism.%20On%20a%20held-out%20test%20sample%20from%202001%20to%202023%2C%20daily%20and%20monthly%0Arebalanced%20long-short%20portfolios%20formed%20from%20StockGPT%20predictions%20yield%20strong%0Aperformance.%20The%20StockGPT-based%20portfolios%20span%20momentum%20and%20long-/short-term%0Areversals%2C%20eliminating%20the%20need%20for%20manually%20crafted%20price-based%20strategies%2C%0Aand%20yield%20highly%20significant%20alphas%20against%20leading%20stock%20market%20factors%2C%0Asuggesting%20a%20novel%20AI%20pricing%20effect.%20This%20highlights%20the%20immense%20promise%20of%0Agenerative%20AI%20in%20surpassing%20human%20in%20making%20complex%20financial%20investment%0Adecisions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.05101v3&entry.124074799=Read"},
{"title": "CASCRNet: An Atrous Spatial Pyramid Pooling and Shared Channel Residual\n  based Network for Capsule Endoscopy", "author": "K V Srinanda and M Manvith Prabhu and Shyam Lal", "abstract": "  This manuscript summarizes work on the Capsule Vision Challenge 2024 by\nMISAHUB. To address the multi-class disease classification task, which is\nchallenging due to the complexity and imbalance in the Capsule Vision challenge\ndataset, this paper proposes CASCRNet (Capsule endoscopy-Aspp-SCR-Network), a\nparameter-efficient and novel model that uses Shared Channel Residual (SCR)\nblocks and Atrous Spatial Pyramid Pooling (ASPP) blocks. Further, the\nperformance of the proposed model is compared with other well-known approaches.\nThe experimental results yield that proposed model provides better disease\nclassification results. The proposed model was successful in classifying\ndiseases with an F1 Score of 78.5% and a Mean AUC of 98.3%, which is promising\ngiven its compact architecture.\n", "link": "http://arxiv.org/abs/2410.17863v1", "date": "2024-10-23", "relevancy": 2.4498, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4995}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4948}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CASCRNet%3A%20An%20Atrous%20Spatial%20Pyramid%20Pooling%20and%20Shared%20Channel%20Residual%0A%20%20based%20Network%20for%20Capsule%20Endoscopy&body=Title%3A%20CASCRNet%3A%20An%20Atrous%20Spatial%20Pyramid%20Pooling%20and%20Shared%20Channel%20Residual%0A%20%20based%20Network%20for%20Capsule%20Endoscopy%0AAuthor%3A%20K%20V%20Srinanda%20and%20M%20Manvith%20Prabhu%20and%20Shyam%20Lal%0AAbstract%3A%20%20%20This%20manuscript%20summarizes%20work%20on%20the%20Capsule%20Vision%20Challenge%202024%20by%0AMISAHUB.%20To%20address%20the%20multi-class%20disease%20classification%20task%2C%20which%20is%0Achallenging%20due%20to%20the%20complexity%20and%20imbalance%20in%20the%20Capsule%20Vision%20challenge%0Adataset%2C%20this%20paper%20proposes%20CASCRNet%20%28Capsule%20endoscopy-Aspp-SCR-Network%29%2C%20a%0Aparameter-efficient%20and%20novel%20model%20that%20uses%20Shared%20Channel%20Residual%20%28SCR%29%0Ablocks%20and%20Atrous%20Spatial%20Pyramid%20Pooling%20%28ASPP%29%20blocks.%20Further%2C%20the%0Aperformance%20of%20the%20proposed%20model%20is%20compared%20with%20other%20well-known%20approaches.%0AThe%20experimental%20results%20yield%20that%20proposed%20model%20provides%20better%20disease%0Aclassification%20results.%20The%20proposed%20model%20was%20successful%20in%20classifying%0Adiseases%20with%20an%20F1%20Score%20of%2078.5%25%20and%20a%20Mean%20AUC%20of%2098.3%25%2C%20which%20is%20promising%0Agiven%20its%20compact%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17863v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCASCRNet%253A%2520An%2520Atrous%2520Spatial%2520Pyramid%2520Pooling%2520and%2520Shared%2520Channel%2520Residual%250A%2520%2520based%2520Network%2520for%2520Capsule%2520Endoscopy%26entry.906535625%3DK%2520V%2520Srinanda%2520and%2520M%2520Manvith%2520Prabhu%2520and%2520Shyam%2520Lal%26entry.1292438233%3D%2520%2520This%2520manuscript%2520summarizes%2520work%2520on%2520the%2520Capsule%2520Vision%2520Challenge%25202024%2520by%250AMISAHUB.%2520To%2520address%2520the%2520multi-class%2520disease%2520classification%2520task%252C%2520which%2520is%250Achallenging%2520due%2520to%2520the%2520complexity%2520and%2520imbalance%2520in%2520the%2520Capsule%2520Vision%2520challenge%250Adataset%252C%2520this%2520paper%2520proposes%2520CASCRNet%2520%2528Capsule%2520endoscopy-Aspp-SCR-Network%2529%252C%2520a%250Aparameter-efficient%2520and%2520novel%2520model%2520that%2520uses%2520Shared%2520Channel%2520Residual%2520%2528SCR%2529%250Ablocks%2520and%2520Atrous%2520Spatial%2520Pyramid%2520Pooling%2520%2528ASPP%2529%2520blocks.%2520Further%252C%2520the%250Aperformance%2520of%2520the%2520proposed%2520model%2520is%2520compared%2520with%2520other%2520well-known%2520approaches.%250AThe%2520experimental%2520results%2520yield%2520that%2520proposed%2520model%2520provides%2520better%2520disease%250Aclassification%2520results.%2520The%2520proposed%2520model%2520was%2520successful%2520in%2520classifying%250Adiseases%2520with%2520an%2520F1%2520Score%2520of%252078.5%2525%2520and%2520a%2520Mean%2520AUC%2520of%252098.3%2525%252C%2520which%2520is%2520promising%250Agiven%2520its%2520compact%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17863v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CASCRNet%3A%20An%20Atrous%20Spatial%20Pyramid%20Pooling%20and%20Shared%20Channel%20Residual%0A%20%20based%20Network%20for%20Capsule%20Endoscopy&entry.906535625=K%20V%20Srinanda%20and%20M%20Manvith%20Prabhu%20and%20Shyam%20Lal&entry.1292438233=%20%20This%20manuscript%20summarizes%20work%20on%20the%20Capsule%20Vision%20Challenge%202024%20by%0AMISAHUB.%20To%20address%20the%20multi-class%20disease%20classification%20task%2C%20which%20is%0Achallenging%20due%20to%20the%20complexity%20and%20imbalance%20in%20the%20Capsule%20Vision%20challenge%0Adataset%2C%20this%20paper%20proposes%20CASCRNet%20%28Capsule%20endoscopy-Aspp-SCR-Network%29%2C%20a%0Aparameter-efficient%20and%20novel%20model%20that%20uses%20Shared%20Channel%20Residual%20%28SCR%29%0Ablocks%20and%20Atrous%20Spatial%20Pyramid%20Pooling%20%28ASPP%29%20blocks.%20Further%2C%20the%0Aperformance%20of%20the%20proposed%20model%20is%20compared%20with%20other%20well-known%20approaches.%0AThe%20experimental%20results%20yield%20that%20proposed%20model%20provides%20better%20disease%0Aclassification%20results.%20The%20proposed%20model%20was%20successful%20in%20classifying%0Adiseases%20with%20an%20F1%20Score%20of%2078.5%25%20and%20a%20Mean%20AUC%20of%2098.3%25%2C%20which%20is%20promising%0Agiven%20its%20compact%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17863v1&entry.124074799=Read"},
{"title": "Blendify -- Python rendering framework for Blender", "author": "Vladimir Guzov and Ilya A. Petrov and Gerard Pons-Moll", "abstract": "  With the rapid growth of the volume of research fields like computer vision\nand computer graphics, researchers require effective and user-friendly\nrendering tools to visualize results. While advanced tools like Blender offer\npowerful capabilities, they also require a significant effort to master. This\ntechnical report introduces Blendify, a lightweight Python-based framework that\nseamlessly integrates with Blender, providing a high-level API for scene\ncreation and rendering. Blendify reduces the complexity of working with\nBlender's native API by automating object creation, handling the colors and\nmaterial linking, and implementing features such as shadow-catcher objects\nwhile maintaining support for high-quality ray-tracing rendering output. With a\nfocus on usability Blendify enables efficient and flexible rendering workflow\nfor rendering in common computer vision and computer graphics use cases. The\ncode is available at https://github.com/ptrvilya/blendify\n", "link": "http://arxiv.org/abs/2410.17858v1", "date": "2024-10-23", "relevancy": 2.4482, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4899}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4899}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4892}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Blendify%20--%20Python%20rendering%20framework%20for%20Blender&body=Title%3A%20Blendify%20--%20Python%20rendering%20framework%20for%20Blender%0AAuthor%3A%20Vladimir%20Guzov%20and%20Ilya%20A.%20Petrov%20and%20Gerard%20Pons-Moll%0AAbstract%3A%20%20%20With%20the%20rapid%20growth%20of%20the%20volume%20of%20research%20fields%20like%20computer%20vision%0Aand%20computer%20graphics%2C%20researchers%20require%20effective%20and%20user-friendly%0Arendering%20tools%20to%20visualize%20results.%20While%20advanced%20tools%20like%20Blender%20offer%0Apowerful%20capabilities%2C%20they%20also%20require%20a%20significant%20effort%20to%20master.%20This%0Atechnical%20report%20introduces%20Blendify%2C%20a%20lightweight%20Python-based%20framework%20that%0Aseamlessly%20integrates%20with%20Blender%2C%20providing%20a%20high-level%20API%20for%20scene%0Acreation%20and%20rendering.%20Blendify%20reduces%20the%20complexity%20of%20working%20with%0ABlender%27s%20native%20API%20by%20automating%20object%20creation%2C%20handling%20the%20colors%20and%0Amaterial%20linking%2C%20and%20implementing%20features%20such%20as%20shadow-catcher%20objects%0Awhile%20maintaining%20support%20for%20high-quality%20ray-tracing%20rendering%20output.%20With%20a%0Afocus%20on%20usability%20Blendify%20enables%20efficient%20and%20flexible%20rendering%20workflow%0Afor%20rendering%20in%20common%20computer%20vision%20and%20computer%20graphics%20use%20cases.%20The%0Acode%20is%20available%20at%20https%3A//github.com/ptrvilya/blendify%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17858v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlendify%2520--%2520Python%2520rendering%2520framework%2520for%2520Blender%26entry.906535625%3DVladimir%2520Guzov%2520and%2520Ilya%2520A.%2520Petrov%2520and%2520Gerard%2520Pons-Moll%26entry.1292438233%3D%2520%2520With%2520the%2520rapid%2520growth%2520of%2520the%2520volume%2520of%2520research%2520fields%2520like%2520computer%2520vision%250Aand%2520computer%2520graphics%252C%2520researchers%2520require%2520effective%2520and%2520user-friendly%250Arendering%2520tools%2520to%2520visualize%2520results.%2520While%2520advanced%2520tools%2520like%2520Blender%2520offer%250Apowerful%2520capabilities%252C%2520they%2520also%2520require%2520a%2520significant%2520effort%2520to%2520master.%2520This%250Atechnical%2520report%2520introduces%2520Blendify%252C%2520a%2520lightweight%2520Python-based%2520framework%2520that%250Aseamlessly%2520integrates%2520with%2520Blender%252C%2520providing%2520a%2520high-level%2520API%2520for%2520scene%250Acreation%2520and%2520rendering.%2520Blendify%2520reduces%2520the%2520complexity%2520of%2520working%2520with%250ABlender%2527s%2520native%2520API%2520by%2520automating%2520object%2520creation%252C%2520handling%2520the%2520colors%2520and%250Amaterial%2520linking%252C%2520and%2520implementing%2520features%2520such%2520as%2520shadow-catcher%2520objects%250Awhile%2520maintaining%2520support%2520for%2520high-quality%2520ray-tracing%2520rendering%2520output.%2520With%2520a%250Afocus%2520on%2520usability%2520Blendify%2520enables%2520efficient%2520and%2520flexible%2520rendering%2520workflow%250Afor%2520rendering%2520in%2520common%2520computer%2520vision%2520and%2520computer%2520graphics%2520use%2520cases.%2520The%250Acode%2520is%2520available%2520at%2520https%253A//github.com/ptrvilya/blendify%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17858v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Blendify%20--%20Python%20rendering%20framework%20for%20Blender&entry.906535625=Vladimir%20Guzov%20and%20Ilya%20A.%20Petrov%20and%20Gerard%20Pons-Moll&entry.1292438233=%20%20With%20the%20rapid%20growth%20of%20the%20volume%20of%20research%20fields%20like%20computer%20vision%0Aand%20computer%20graphics%2C%20researchers%20require%20effective%20and%20user-friendly%0Arendering%20tools%20to%20visualize%20results.%20While%20advanced%20tools%20like%20Blender%20offer%0Apowerful%20capabilities%2C%20they%20also%20require%20a%20significant%20effort%20to%20master.%20This%0Atechnical%20report%20introduces%20Blendify%2C%20a%20lightweight%20Python-based%20framework%20that%0Aseamlessly%20integrates%20with%20Blender%2C%20providing%20a%20high-level%20API%20for%20scene%0Acreation%20and%20rendering.%20Blendify%20reduces%20the%20complexity%20of%20working%20with%0ABlender%27s%20native%20API%20by%20automating%20object%20creation%2C%20handling%20the%20colors%20and%0Amaterial%20linking%2C%20and%20implementing%20features%20such%20as%20shadow-catcher%20objects%0Awhile%20maintaining%20support%20for%20high-quality%20ray-tracing%20rendering%20output.%20With%20a%0Afocus%20on%20usability%20Blendify%20enables%20efficient%20and%20flexible%20rendering%20workflow%0Afor%20rendering%20in%20common%20computer%20vision%20and%20computer%20graphics%20use%20cases.%20The%0Acode%20is%20available%20at%20https%3A//github.com/ptrvilya/blendify%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17858v1&entry.124074799=Read"},
{"title": "Understanding Layer Significance in LLM Alignment", "author": "Guangyuan Shi and Zexin Lu and Xiaoyu Dong and Wenlong Zhang and Xuanyu Zhang and Yujie Feng and Xiao-Ming Wu", "abstract": "  Aligning large language models (LLMs) through fine-tuning is essential for\ntailoring them to specific applications. Therefore, understanding what LLMs\nlearn during the alignment process is crucial. Recent studies suggest that\nalignment primarily adjusts a model's presentation style rather than its\nfoundational knowledge, indicating that only certain components of the model\nare significantly impacted. To delve deeper into LLM alignment, we propose to\nidentify which layers within LLMs are most critical to the alignment process,\nthereby uncovering how alignment influences model behavior at a granular level.\nWe propose a novel approach to identify the important layers for LLM alignment\n(ILA). It involves learning a binary mask for each incremental weight matrix in\nthe LoRA algorithm, indicating the significance of each layer. ILA consistently\nidentifies important layers across various alignment datasets, with nearly 90%\noverlap even with substantial dataset differences, highlighting fundamental\npatterns in LLM alignment. Experimental results indicate that freezing\nnon-essential layers improves overall model performance, while selectively\ntuning the most critical layers significantly enhances fine-tuning efficiency\nwith minimal performance loss.\n", "link": "http://arxiv.org/abs/2410.17875v1", "date": "2024-10-23", "relevancy": 2.4469, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Understanding%20Layer%20Significance%20in%20LLM%20Alignment&body=Title%3A%20Understanding%20Layer%20Significance%20in%20LLM%20Alignment%0AAuthor%3A%20Guangyuan%20Shi%20and%20Zexin%20Lu%20and%20Xiaoyu%20Dong%20and%20Wenlong%20Zhang%20and%20Xuanyu%20Zhang%20and%20Yujie%20Feng%20and%20Xiao-Ming%20Wu%0AAbstract%3A%20%20%20Aligning%20large%20language%20models%20%28LLMs%29%20through%20fine-tuning%20is%20essential%20for%0Atailoring%20them%20to%20specific%20applications.%20Therefore%2C%20understanding%20what%20LLMs%0Alearn%20during%20the%20alignment%20process%20is%20crucial.%20Recent%20studies%20suggest%20that%0Aalignment%20primarily%20adjusts%20a%20model%27s%20presentation%20style%20rather%20than%20its%0Afoundational%20knowledge%2C%20indicating%20that%20only%20certain%20components%20of%20the%20model%0Aare%20significantly%20impacted.%20To%20delve%20deeper%20into%20LLM%20alignment%2C%20we%20propose%20to%0Aidentify%20which%20layers%20within%20LLMs%20are%20most%20critical%20to%20the%20alignment%20process%2C%0Athereby%20uncovering%20how%20alignment%20influences%20model%20behavior%20at%20a%20granular%20level.%0AWe%20propose%20a%20novel%20approach%20to%20identify%20the%20important%20layers%20for%20LLM%20alignment%0A%28ILA%29.%20It%20involves%20learning%20a%20binary%20mask%20for%20each%20incremental%20weight%20matrix%20in%0Athe%20LoRA%20algorithm%2C%20indicating%20the%20significance%20of%20each%20layer.%20ILA%20consistently%0Aidentifies%20important%20layers%20across%20various%20alignment%20datasets%2C%20with%20nearly%2090%25%0Aoverlap%20even%20with%20substantial%20dataset%20differences%2C%20highlighting%20fundamental%0Apatterns%20in%20LLM%20alignment.%20Experimental%20results%20indicate%20that%20freezing%0Anon-essential%20layers%20improves%20overall%20model%20performance%2C%20while%20selectively%0Atuning%20the%20most%20critical%20layers%20significantly%20enhances%20fine-tuning%20efficiency%0Awith%20minimal%20performance%20loss.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17875v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnderstanding%2520Layer%2520Significance%2520in%2520LLM%2520Alignment%26entry.906535625%3DGuangyuan%2520Shi%2520and%2520Zexin%2520Lu%2520and%2520Xiaoyu%2520Dong%2520and%2520Wenlong%2520Zhang%2520and%2520Xuanyu%2520Zhang%2520and%2520Yujie%2520Feng%2520and%2520Xiao-Ming%2520Wu%26entry.1292438233%3D%2520%2520Aligning%2520large%2520language%2520models%2520%2528LLMs%2529%2520through%2520fine-tuning%2520is%2520essential%2520for%250Atailoring%2520them%2520to%2520specific%2520applications.%2520Therefore%252C%2520understanding%2520what%2520LLMs%250Alearn%2520during%2520the%2520alignment%2520process%2520is%2520crucial.%2520Recent%2520studies%2520suggest%2520that%250Aalignment%2520primarily%2520adjusts%2520a%2520model%2527s%2520presentation%2520style%2520rather%2520than%2520its%250Afoundational%2520knowledge%252C%2520indicating%2520that%2520only%2520certain%2520components%2520of%2520the%2520model%250Aare%2520significantly%2520impacted.%2520To%2520delve%2520deeper%2520into%2520LLM%2520alignment%252C%2520we%2520propose%2520to%250Aidentify%2520which%2520layers%2520within%2520LLMs%2520are%2520most%2520critical%2520to%2520the%2520alignment%2520process%252C%250Athereby%2520uncovering%2520how%2520alignment%2520influences%2520model%2520behavior%2520at%2520a%2520granular%2520level.%250AWe%2520propose%2520a%2520novel%2520approach%2520to%2520identify%2520the%2520important%2520layers%2520for%2520LLM%2520alignment%250A%2528ILA%2529.%2520It%2520involves%2520learning%2520a%2520binary%2520mask%2520for%2520each%2520incremental%2520weight%2520matrix%2520in%250Athe%2520LoRA%2520algorithm%252C%2520indicating%2520the%2520significance%2520of%2520each%2520layer.%2520ILA%2520consistently%250Aidentifies%2520important%2520layers%2520across%2520various%2520alignment%2520datasets%252C%2520with%2520nearly%252090%2525%250Aoverlap%2520even%2520with%2520substantial%2520dataset%2520differences%252C%2520highlighting%2520fundamental%250Apatterns%2520in%2520LLM%2520alignment.%2520Experimental%2520results%2520indicate%2520that%2520freezing%250Anon-essential%2520layers%2520improves%2520overall%2520model%2520performance%252C%2520while%2520selectively%250Atuning%2520the%2520most%2520critical%2520layers%2520significantly%2520enhances%2520fine-tuning%2520efficiency%250Awith%2520minimal%2520performance%2520loss.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17875v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Understanding%20Layer%20Significance%20in%20LLM%20Alignment&entry.906535625=Guangyuan%20Shi%20and%20Zexin%20Lu%20and%20Xiaoyu%20Dong%20and%20Wenlong%20Zhang%20and%20Xuanyu%20Zhang%20and%20Yujie%20Feng%20and%20Xiao-Ming%20Wu&entry.1292438233=%20%20Aligning%20large%20language%20models%20%28LLMs%29%20through%20fine-tuning%20is%20essential%20for%0Atailoring%20them%20to%20specific%20applications.%20Therefore%2C%20understanding%20what%20LLMs%0Alearn%20during%20the%20alignment%20process%20is%20crucial.%20Recent%20studies%20suggest%20that%0Aalignment%20primarily%20adjusts%20a%20model%27s%20presentation%20style%20rather%20than%20its%0Afoundational%20knowledge%2C%20indicating%20that%20only%20certain%20components%20of%20the%20model%0Aare%20significantly%20impacted.%20To%20delve%20deeper%20into%20LLM%20alignment%2C%20we%20propose%20to%0Aidentify%20which%20layers%20within%20LLMs%20are%20most%20critical%20to%20the%20alignment%20process%2C%0Athereby%20uncovering%20how%20alignment%20influences%20model%20behavior%20at%20a%20granular%20level.%0AWe%20propose%20a%20novel%20approach%20to%20identify%20the%20important%20layers%20for%20LLM%20alignment%0A%28ILA%29.%20It%20involves%20learning%20a%20binary%20mask%20for%20each%20incremental%20weight%20matrix%20in%0Athe%20LoRA%20algorithm%2C%20indicating%20the%20significance%20of%20each%20layer.%20ILA%20consistently%0Aidentifies%20important%20layers%20across%20various%20alignment%20datasets%2C%20with%20nearly%2090%25%0Aoverlap%20even%20with%20substantial%20dataset%20differences%2C%20highlighting%20fundamental%0Apatterns%20in%20LLM%20alignment.%20Experimental%20results%20indicate%20that%20freezing%0Anon-essential%20layers%20improves%20overall%20model%20performance%2C%20while%20selectively%0Atuning%20the%20most%20critical%20layers%20significantly%20enhances%20fine-tuning%20efficiency%0Awith%20minimal%20performance%20loss.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17875v1&entry.124074799=Read"},
{"title": "TAGE: Trustworthy Attribute Group Editing for Stable Few-shot Image\n  Generation", "author": "Ruicheng Zhang and Guoheng Huang and Yejing Huo and Xiaochen Yuan and Zhizhen Zhou and Xuhang Chen and Guo Zhong", "abstract": "  Generative Adversarial Networks (GANs) have emerged as a prominent research\nfocus for image editing tasks, leveraging the powerful image generation\ncapabilities of the GAN framework to produce remarkable results.However,\nprevailing approaches are contingent upon extensive training datasets and\nexplicit supervision, presenting a significant challenge in manipulating the\ndiverse attributes of new image classes with limited sample availability. To\nsurmount this hurdle, we introduce TAGE, an innovative image generation network\ncomprising three integral modules: the Codebook Learning Module (CLM), the Code\nPrediction Module (CPM) and the Prompt-driven Semantic Module (PSM). The CPM\nmodule delves into the semantic dimensions of category-agnostic attributes,\nencapsulating them within a discrete codebook. This module is predicated on the\nconcept that images are assemblages of attributes, and thus, by editing these\ncategory-independent attributes, it is theoretically possible to generate\nimages from unseen categories. Subsequently, the CPM module facilitates\nnaturalistic image editing by predicting indices of category-independent\nattribute vectors within the codebook. Additionally, the PSM module generates\nsemantic cues that are seamlessly integrated into the Transformer architecture\nof the CPM, enhancing the model's comprehension of the targeted attributes for\nediting. With these semantic cues, the model can generate images that\naccentuate desired attributes more prominently while maintaining the integrity\nof the original category, even with a limited number of samples. We have\nconducted extensive experiments utilizing the Animal Faces, Flowers, and\nVGGFaces datasets. The results of these experiments demonstrate that our\nproposed method not only achieves superior performance but also exhibits a high\ndegree of stability when compared to other few-shot image generation\ntechniques.\n", "link": "http://arxiv.org/abs/2410.17855v1", "date": "2024-10-23", "relevancy": 2.3882, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6143}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.605}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TAGE%3A%20Trustworthy%20Attribute%20Group%20Editing%20for%20Stable%20Few-shot%20Image%0A%20%20Generation&body=Title%3A%20TAGE%3A%20Trustworthy%20Attribute%20Group%20Editing%20for%20Stable%20Few-shot%20Image%0A%20%20Generation%0AAuthor%3A%20Ruicheng%20Zhang%20and%20Guoheng%20Huang%20and%20Yejing%20Huo%20and%20Xiaochen%20Yuan%20and%20Zhizhen%20Zhou%20and%20Xuhang%20Chen%20and%20Guo%20Zhong%0AAbstract%3A%20%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20emerged%20as%20a%20prominent%20research%0Afocus%20for%20image%20editing%20tasks%2C%20leveraging%20the%20powerful%20image%20generation%0Acapabilities%20of%20the%20GAN%20framework%20to%20produce%20remarkable%20results.However%2C%0Aprevailing%20approaches%20are%20contingent%20upon%20extensive%20training%20datasets%20and%0Aexplicit%20supervision%2C%20presenting%20a%20significant%20challenge%20in%20manipulating%20the%0Adiverse%20attributes%20of%20new%20image%20classes%20with%20limited%20sample%20availability.%20To%0Asurmount%20this%20hurdle%2C%20we%20introduce%20TAGE%2C%20an%20innovative%20image%20generation%20network%0Acomprising%20three%20integral%20modules%3A%20the%20Codebook%20Learning%20Module%20%28CLM%29%2C%20the%20Code%0APrediction%20Module%20%28CPM%29%20and%20the%20Prompt-driven%20Semantic%20Module%20%28PSM%29.%20The%20CPM%0Amodule%20delves%20into%20the%20semantic%20dimensions%20of%20category-agnostic%20attributes%2C%0Aencapsulating%20them%20within%20a%20discrete%20codebook.%20This%20module%20is%20predicated%20on%20the%0Aconcept%20that%20images%20are%20assemblages%20of%20attributes%2C%20and%20thus%2C%20by%20editing%20these%0Acategory-independent%20attributes%2C%20it%20is%20theoretically%20possible%20to%20generate%0Aimages%20from%20unseen%20categories.%20Subsequently%2C%20the%20CPM%20module%20facilitates%0Anaturalistic%20image%20editing%20by%20predicting%20indices%20of%20category-independent%0Aattribute%20vectors%20within%20the%20codebook.%20Additionally%2C%20the%20PSM%20module%20generates%0Asemantic%20cues%20that%20are%20seamlessly%20integrated%20into%20the%20Transformer%20architecture%0Aof%20the%20CPM%2C%20enhancing%20the%20model%27s%20comprehension%20of%20the%20targeted%20attributes%20for%0Aediting.%20With%20these%20semantic%20cues%2C%20the%20model%20can%20generate%20images%20that%0Aaccentuate%20desired%20attributes%20more%20prominently%20while%20maintaining%20the%20integrity%0Aof%20the%20original%20category%2C%20even%20with%20a%20limited%20number%20of%20samples.%20We%20have%0Aconducted%20extensive%20experiments%20utilizing%20the%20Animal%20Faces%2C%20Flowers%2C%20and%0AVGGFaces%20datasets.%20The%20results%20of%20these%20experiments%20demonstrate%20that%20our%0Aproposed%20method%20not%20only%20achieves%20superior%20performance%20but%20also%20exhibits%20a%20high%0Adegree%20of%20stability%20when%20compared%20to%20other%20few-shot%20image%20generation%0Atechniques.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17855v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTAGE%253A%2520Trustworthy%2520Attribute%2520Group%2520Editing%2520for%2520Stable%2520Few-shot%2520Image%250A%2520%2520Generation%26entry.906535625%3DRuicheng%2520Zhang%2520and%2520Guoheng%2520Huang%2520and%2520Yejing%2520Huo%2520and%2520Xiaochen%2520Yuan%2520and%2520Zhizhen%2520Zhou%2520and%2520Xuhang%2520Chen%2520and%2520Guo%2520Zhong%26entry.1292438233%3D%2520%2520Generative%2520Adversarial%2520Networks%2520%2528GANs%2529%2520have%2520emerged%2520as%2520a%2520prominent%2520research%250Afocus%2520for%2520image%2520editing%2520tasks%252C%2520leveraging%2520the%2520powerful%2520image%2520generation%250Acapabilities%2520of%2520the%2520GAN%2520framework%2520to%2520produce%2520remarkable%2520results.However%252C%250Aprevailing%2520approaches%2520are%2520contingent%2520upon%2520extensive%2520training%2520datasets%2520and%250Aexplicit%2520supervision%252C%2520presenting%2520a%2520significant%2520challenge%2520in%2520manipulating%2520the%250Adiverse%2520attributes%2520of%2520new%2520image%2520classes%2520with%2520limited%2520sample%2520availability.%2520To%250Asurmount%2520this%2520hurdle%252C%2520we%2520introduce%2520TAGE%252C%2520an%2520innovative%2520image%2520generation%2520network%250Acomprising%2520three%2520integral%2520modules%253A%2520the%2520Codebook%2520Learning%2520Module%2520%2528CLM%2529%252C%2520the%2520Code%250APrediction%2520Module%2520%2528CPM%2529%2520and%2520the%2520Prompt-driven%2520Semantic%2520Module%2520%2528PSM%2529.%2520The%2520CPM%250Amodule%2520delves%2520into%2520the%2520semantic%2520dimensions%2520of%2520category-agnostic%2520attributes%252C%250Aencapsulating%2520them%2520within%2520a%2520discrete%2520codebook.%2520This%2520module%2520is%2520predicated%2520on%2520the%250Aconcept%2520that%2520images%2520are%2520assemblages%2520of%2520attributes%252C%2520and%2520thus%252C%2520by%2520editing%2520these%250Acategory-independent%2520attributes%252C%2520it%2520is%2520theoretically%2520possible%2520to%2520generate%250Aimages%2520from%2520unseen%2520categories.%2520Subsequently%252C%2520the%2520CPM%2520module%2520facilitates%250Anaturalistic%2520image%2520editing%2520by%2520predicting%2520indices%2520of%2520category-independent%250Aattribute%2520vectors%2520within%2520the%2520codebook.%2520Additionally%252C%2520the%2520PSM%2520module%2520generates%250Asemantic%2520cues%2520that%2520are%2520seamlessly%2520integrated%2520into%2520the%2520Transformer%2520architecture%250Aof%2520the%2520CPM%252C%2520enhancing%2520the%2520model%2527s%2520comprehension%2520of%2520the%2520targeted%2520attributes%2520for%250Aediting.%2520With%2520these%2520semantic%2520cues%252C%2520the%2520model%2520can%2520generate%2520images%2520that%250Aaccentuate%2520desired%2520attributes%2520more%2520prominently%2520while%2520maintaining%2520the%2520integrity%250Aof%2520the%2520original%2520category%252C%2520even%2520with%2520a%2520limited%2520number%2520of%2520samples.%2520We%2520have%250Aconducted%2520extensive%2520experiments%2520utilizing%2520the%2520Animal%2520Faces%252C%2520Flowers%252C%2520and%250AVGGFaces%2520datasets.%2520The%2520results%2520of%2520these%2520experiments%2520demonstrate%2520that%2520our%250Aproposed%2520method%2520not%2520only%2520achieves%2520superior%2520performance%2520but%2520also%2520exhibits%2520a%2520high%250Adegree%2520of%2520stability%2520when%2520compared%2520to%2520other%2520few-shot%2520image%2520generation%250Atechniques.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17855v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TAGE%3A%20Trustworthy%20Attribute%20Group%20Editing%20for%20Stable%20Few-shot%20Image%0A%20%20Generation&entry.906535625=Ruicheng%20Zhang%20and%20Guoheng%20Huang%20and%20Yejing%20Huo%20and%20Xiaochen%20Yuan%20and%20Zhizhen%20Zhou%20and%20Xuhang%20Chen%20and%20Guo%20Zhong&entry.1292438233=%20%20Generative%20Adversarial%20Networks%20%28GANs%29%20have%20emerged%20as%20a%20prominent%20research%0Afocus%20for%20image%20editing%20tasks%2C%20leveraging%20the%20powerful%20image%20generation%0Acapabilities%20of%20the%20GAN%20framework%20to%20produce%20remarkable%20results.However%2C%0Aprevailing%20approaches%20are%20contingent%20upon%20extensive%20training%20datasets%20and%0Aexplicit%20supervision%2C%20presenting%20a%20significant%20challenge%20in%20manipulating%20the%0Adiverse%20attributes%20of%20new%20image%20classes%20with%20limited%20sample%20availability.%20To%0Asurmount%20this%20hurdle%2C%20we%20introduce%20TAGE%2C%20an%20innovative%20image%20generation%20network%0Acomprising%20three%20integral%20modules%3A%20the%20Codebook%20Learning%20Module%20%28CLM%29%2C%20the%20Code%0APrediction%20Module%20%28CPM%29%20and%20the%20Prompt-driven%20Semantic%20Module%20%28PSM%29.%20The%20CPM%0Amodule%20delves%20into%20the%20semantic%20dimensions%20of%20category-agnostic%20attributes%2C%0Aencapsulating%20them%20within%20a%20discrete%20codebook.%20This%20module%20is%20predicated%20on%20the%0Aconcept%20that%20images%20are%20assemblages%20of%20attributes%2C%20and%20thus%2C%20by%20editing%20these%0Acategory-independent%20attributes%2C%20it%20is%20theoretically%20possible%20to%20generate%0Aimages%20from%20unseen%20categories.%20Subsequently%2C%20the%20CPM%20module%20facilitates%0Anaturalistic%20image%20editing%20by%20predicting%20indices%20of%20category-independent%0Aattribute%20vectors%20within%20the%20codebook.%20Additionally%2C%20the%20PSM%20module%20generates%0Asemantic%20cues%20that%20are%20seamlessly%20integrated%20into%20the%20Transformer%20architecture%0Aof%20the%20CPM%2C%20enhancing%20the%20model%27s%20comprehension%20of%20the%20targeted%20attributes%20for%0Aediting.%20With%20these%20semantic%20cues%2C%20the%20model%20can%20generate%20images%20that%0Aaccentuate%20desired%20attributes%20more%20prominently%20while%20maintaining%20the%20integrity%0Aof%20the%20original%20category%2C%20even%20with%20a%20limited%20number%20of%20samples.%20We%20have%0Aconducted%20extensive%20experiments%20utilizing%20the%20Animal%20Faces%2C%20Flowers%2C%20and%0AVGGFaces%20datasets.%20The%20results%20of%20these%20experiments%20demonstrate%20that%20our%0Aproposed%20method%20not%20only%20achieves%20superior%20performance%20but%20also%20exhibits%20a%20high%0Adegree%20of%20stability%20when%20compared%20to%20other%20few-shot%20image%20generation%0Atechniques.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17855v1&entry.124074799=Read"},
{"title": "GRAMMAR: Grounded and Modular Methodology for Assessment of\n  Closed-Domain Retrieval-Augmented Language Model", "author": "Xinzhe Li and Ming Liu and Shang Gao", "abstract": "  Retrieval-Augmented Generation (RAG) systems are widely used across various\nindustries for querying closed-domain and in-house knowledge bases. However,\nevaluating these systems presents significant challenges due to the private\nnature of closed-domain data and a scarcity of queries with verifiable ground\ntruths. Moreover, there is a lack of analytical methods to diagnose problematic\nmodules and identify types of failure, such as those caused by knowledge\ndeficits or issues with robustness. To address these challenges, we introduce\nGRAMMAR (GRounded And Modular Methodology for Assessment of RAG), an evaluation\nframework comprising a grounded data generation process and an evaluation\nprotocol that effectively pinpoints defective modules. Our validation\nexperiments reveal that GRAMMAR provides a reliable approach for identifying\nvulnerable modules and supports hypothesis testing for textual form\nvulnerabilities. An open-source tool accompanying this framework is available\nin our GitHub repository (see https://github.com/xinzhel/grammar), allowing for\neasy reproduction of our results and enabling reliable and modular evaluation\nin closed-domain settings.\n", "link": "http://arxiv.org/abs/2404.19232v7", "date": "2024-10-23", "relevancy": 2.3836, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4882}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.471}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRAMMAR%3A%20Grounded%20and%20Modular%20Methodology%20for%20Assessment%20of%0A%20%20Closed-Domain%20Retrieval-Augmented%20Language%20Model&body=Title%3A%20GRAMMAR%3A%20Grounded%20and%20Modular%20Methodology%20for%20Assessment%20of%0A%20%20Closed-Domain%20Retrieval-Augmented%20Language%20Model%0AAuthor%3A%20Xinzhe%20Li%20and%20Ming%20Liu%20and%20Shang%20Gao%0AAbstract%3A%20%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20systems%20are%20widely%20used%20across%20various%0Aindustries%20for%20querying%20closed-domain%20and%20in-house%20knowledge%20bases.%20However%2C%0Aevaluating%20these%20systems%20presents%20significant%20challenges%20due%20to%20the%20private%0Anature%20of%20closed-domain%20data%20and%20a%20scarcity%20of%20queries%20with%20verifiable%20ground%0Atruths.%20Moreover%2C%20there%20is%20a%20lack%20of%20analytical%20methods%20to%20diagnose%20problematic%0Amodules%20and%20identify%20types%20of%20failure%2C%20such%20as%20those%20caused%20by%20knowledge%0Adeficits%20or%20issues%20with%20robustness.%20To%20address%20these%20challenges%2C%20we%20introduce%0AGRAMMAR%20%28GRounded%20And%20Modular%20Methodology%20for%20Assessment%20of%20RAG%29%2C%20an%20evaluation%0Aframework%20comprising%20a%20grounded%20data%20generation%20process%20and%20an%20evaluation%0Aprotocol%20that%20effectively%20pinpoints%20defective%20modules.%20Our%20validation%0Aexperiments%20reveal%20that%20GRAMMAR%20provides%20a%20reliable%20approach%20for%20identifying%0Avulnerable%20modules%20and%20supports%20hypothesis%20testing%20for%20textual%20form%0Avulnerabilities.%20An%20open-source%20tool%20accompanying%20this%20framework%20is%20available%0Ain%20our%20GitHub%20repository%20%28see%20https%3A//github.com/xinzhel/grammar%29%2C%20allowing%20for%0Aeasy%20reproduction%20of%20our%20results%20and%20enabling%20reliable%20and%20modular%20evaluation%0Ain%20closed-domain%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.19232v7%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRAMMAR%253A%2520Grounded%2520and%2520Modular%2520Methodology%2520for%2520Assessment%2520of%250A%2520%2520Closed-Domain%2520Retrieval-Augmented%2520Language%2520Model%26entry.906535625%3DXinzhe%2520Li%2520and%2520Ming%2520Liu%2520and%2520Shang%2520Gao%26entry.1292438233%3D%2520%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520systems%2520are%2520widely%2520used%2520across%2520various%250Aindustries%2520for%2520querying%2520closed-domain%2520and%2520in-house%2520knowledge%2520bases.%2520However%252C%250Aevaluating%2520these%2520systems%2520presents%2520significant%2520challenges%2520due%2520to%2520the%2520private%250Anature%2520of%2520closed-domain%2520data%2520and%2520a%2520scarcity%2520of%2520queries%2520with%2520verifiable%2520ground%250Atruths.%2520Moreover%252C%2520there%2520is%2520a%2520lack%2520of%2520analytical%2520methods%2520to%2520diagnose%2520problematic%250Amodules%2520and%2520identify%2520types%2520of%2520failure%252C%2520such%2520as%2520those%2520caused%2520by%2520knowledge%250Adeficits%2520or%2520issues%2520with%2520robustness.%2520To%2520address%2520these%2520challenges%252C%2520we%2520introduce%250AGRAMMAR%2520%2528GRounded%2520And%2520Modular%2520Methodology%2520for%2520Assessment%2520of%2520RAG%2529%252C%2520an%2520evaluation%250Aframework%2520comprising%2520a%2520grounded%2520data%2520generation%2520process%2520and%2520an%2520evaluation%250Aprotocol%2520that%2520effectively%2520pinpoints%2520defective%2520modules.%2520Our%2520validation%250Aexperiments%2520reveal%2520that%2520GRAMMAR%2520provides%2520a%2520reliable%2520approach%2520for%2520identifying%250Avulnerable%2520modules%2520and%2520supports%2520hypothesis%2520testing%2520for%2520textual%2520form%250Avulnerabilities.%2520An%2520open-source%2520tool%2520accompanying%2520this%2520framework%2520is%2520available%250Ain%2520our%2520GitHub%2520repository%2520%2528see%2520https%253A//github.com/xinzhel/grammar%2529%252C%2520allowing%2520for%250Aeasy%2520reproduction%2520of%2520our%2520results%2520and%2520enabling%2520reliable%2520and%2520modular%2520evaluation%250Ain%2520closed-domain%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.19232v7%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRAMMAR%3A%20Grounded%20and%20Modular%20Methodology%20for%20Assessment%20of%0A%20%20Closed-Domain%20Retrieval-Augmented%20Language%20Model&entry.906535625=Xinzhe%20Li%20and%20Ming%20Liu%20and%20Shang%20Gao&entry.1292438233=%20%20Retrieval-Augmented%20Generation%20%28RAG%29%20systems%20are%20widely%20used%20across%20various%0Aindustries%20for%20querying%20closed-domain%20and%20in-house%20knowledge%20bases.%20However%2C%0Aevaluating%20these%20systems%20presents%20significant%20challenges%20due%20to%20the%20private%0Anature%20of%20closed-domain%20data%20and%20a%20scarcity%20of%20queries%20with%20verifiable%20ground%0Atruths.%20Moreover%2C%20there%20is%20a%20lack%20of%20analytical%20methods%20to%20diagnose%20problematic%0Amodules%20and%20identify%20types%20of%20failure%2C%20such%20as%20those%20caused%20by%20knowledge%0Adeficits%20or%20issues%20with%20robustness.%20To%20address%20these%20challenges%2C%20we%20introduce%0AGRAMMAR%20%28GRounded%20And%20Modular%20Methodology%20for%20Assessment%20of%20RAG%29%2C%20an%20evaluation%0Aframework%20comprising%20a%20grounded%20data%20generation%20process%20and%20an%20evaluation%0Aprotocol%20that%20effectively%20pinpoints%20defective%20modules.%20Our%20validation%0Aexperiments%20reveal%20that%20GRAMMAR%20provides%20a%20reliable%20approach%20for%20identifying%0Avulnerable%20modules%20and%20supports%20hypothesis%20testing%20for%20textual%20form%0Avulnerabilities.%20An%20open-source%20tool%20accompanying%20this%20framework%20is%20available%0Ain%20our%20GitHub%20repository%20%28see%20https%3A//github.com/xinzhel/grammar%29%2C%20allowing%20for%0Aeasy%20reproduction%20of%20our%20results%20and%20enabling%20reliable%20and%20modular%20evaluation%0Ain%20closed-domain%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.19232v7&entry.124074799=Read"},
{"title": "STAR: SocioTechnical Approach to Red Teaming Language Models", "author": "Laura Weidinger and John Mellor and Bernat Guillen Pegueroles and Nahema Marchal and Ravin Kumar and Kristian Lum and Canfer Akbulut and Mark Diaz and Stevie Bergman and Mikel Rodriguez and Verena Rieser and William Isaac", "abstract": "  This research introduces STAR, a sociotechnical framework that improves on\ncurrent best practices for red teaming safety of large language models. STAR\nmakes two key contributions: it enhances steerability by generating\nparameterised instructions for human red teamers, leading to improved coverage\nof the risk surface. Parameterised instructions also provide more detailed\ninsights into model failures at no increased cost. Second, STAR improves signal\nquality by matching demographics to assess harms for specific groups, resulting\nin more sensitive annotations. STAR further employs a novel step of arbitration\nto leverage diverse viewpoints and improve label reliability, treating\ndisagreement not as noise but as a valuable contribution to signal quality.\n", "link": "http://arxiv.org/abs/2406.11757v4", "date": "2024-10-23", "relevancy": 2.3519, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4716}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4716}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4681}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STAR%3A%20SocioTechnical%20Approach%20to%20Red%20Teaming%20Language%20Models&body=Title%3A%20STAR%3A%20SocioTechnical%20Approach%20to%20Red%20Teaming%20Language%20Models%0AAuthor%3A%20Laura%20Weidinger%20and%20John%20Mellor%20and%20Bernat%20Guillen%20Pegueroles%20and%20Nahema%20Marchal%20and%20Ravin%20Kumar%20and%20Kristian%20Lum%20and%20Canfer%20Akbulut%20and%20Mark%20Diaz%20and%20Stevie%20Bergman%20and%20Mikel%20Rodriguez%20and%20Verena%20Rieser%20and%20William%20Isaac%0AAbstract%3A%20%20%20This%20research%20introduces%20STAR%2C%20a%20sociotechnical%20framework%20that%20improves%20on%0Acurrent%20best%20practices%20for%20red%20teaming%20safety%20of%20large%20language%20models.%20STAR%0Amakes%20two%20key%20contributions%3A%20it%20enhances%20steerability%20by%20generating%0Aparameterised%20instructions%20for%20human%20red%20teamers%2C%20leading%20to%20improved%20coverage%0Aof%20the%20risk%20surface.%20Parameterised%20instructions%20also%20provide%20more%20detailed%0Ainsights%20into%20model%20failures%20at%20no%20increased%20cost.%20Second%2C%20STAR%20improves%20signal%0Aquality%20by%20matching%20demographics%20to%20assess%20harms%20for%20specific%20groups%2C%20resulting%0Ain%20more%20sensitive%20annotations.%20STAR%20further%20employs%20a%20novel%20step%20of%20arbitration%0Ato%20leverage%20diverse%20viewpoints%20and%20improve%20label%20reliability%2C%20treating%0Adisagreement%20not%20as%20noise%20but%20as%20a%20valuable%20contribution%20to%20signal%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.11757v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTAR%253A%2520SocioTechnical%2520Approach%2520to%2520Red%2520Teaming%2520Language%2520Models%26entry.906535625%3DLaura%2520Weidinger%2520and%2520John%2520Mellor%2520and%2520Bernat%2520Guillen%2520Pegueroles%2520and%2520Nahema%2520Marchal%2520and%2520Ravin%2520Kumar%2520and%2520Kristian%2520Lum%2520and%2520Canfer%2520Akbulut%2520and%2520Mark%2520Diaz%2520and%2520Stevie%2520Bergman%2520and%2520Mikel%2520Rodriguez%2520and%2520Verena%2520Rieser%2520and%2520William%2520Isaac%26entry.1292438233%3D%2520%2520This%2520research%2520introduces%2520STAR%252C%2520a%2520sociotechnical%2520framework%2520that%2520improves%2520on%250Acurrent%2520best%2520practices%2520for%2520red%2520teaming%2520safety%2520of%2520large%2520language%2520models.%2520STAR%250Amakes%2520two%2520key%2520contributions%253A%2520it%2520enhances%2520steerability%2520by%2520generating%250Aparameterised%2520instructions%2520for%2520human%2520red%2520teamers%252C%2520leading%2520to%2520improved%2520coverage%250Aof%2520the%2520risk%2520surface.%2520Parameterised%2520instructions%2520also%2520provide%2520more%2520detailed%250Ainsights%2520into%2520model%2520failures%2520at%2520no%2520increased%2520cost.%2520Second%252C%2520STAR%2520improves%2520signal%250Aquality%2520by%2520matching%2520demographics%2520to%2520assess%2520harms%2520for%2520specific%2520groups%252C%2520resulting%250Ain%2520more%2520sensitive%2520annotations.%2520STAR%2520further%2520employs%2520a%2520novel%2520step%2520of%2520arbitration%250Ato%2520leverage%2520diverse%2520viewpoints%2520and%2520improve%2520label%2520reliability%252C%2520treating%250Adisagreement%2520not%2520as%2520noise%2520but%2520as%2520a%2520valuable%2520contribution%2520to%2520signal%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.11757v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STAR%3A%20SocioTechnical%20Approach%20to%20Red%20Teaming%20Language%20Models&entry.906535625=Laura%20Weidinger%20and%20John%20Mellor%20and%20Bernat%20Guillen%20Pegueroles%20and%20Nahema%20Marchal%20and%20Ravin%20Kumar%20and%20Kristian%20Lum%20and%20Canfer%20Akbulut%20and%20Mark%20Diaz%20and%20Stevie%20Bergman%20and%20Mikel%20Rodriguez%20and%20Verena%20Rieser%20and%20William%20Isaac&entry.1292438233=%20%20This%20research%20introduces%20STAR%2C%20a%20sociotechnical%20framework%20that%20improves%20on%0Acurrent%20best%20practices%20for%20red%20teaming%20safety%20of%20large%20language%20models.%20STAR%0Amakes%20two%20key%20contributions%3A%20it%20enhances%20steerability%20by%20generating%0Aparameterised%20instructions%20for%20human%20red%20teamers%2C%20leading%20to%20improved%20coverage%0Aof%20the%20risk%20surface.%20Parameterised%20instructions%20also%20provide%20more%20detailed%0Ainsights%20into%20model%20failures%20at%20no%20increased%20cost.%20Second%2C%20STAR%20improves%20signal%0Aquality%20by%20matching%20demographics%20to%20assess%20harms%20for%20specific%20groups%2C%20resulting%0Ain%20more%20sensitive%20annotations.%20STAR%20further%20employs%20a%20novel%20step%20of%20arbitration%0Ato%20leverage%20diverse%20viewpoints%20and%20improve%20label%20reliability%2C%20treating%0Adisagreement%20not%20as%20noise%20but%20as%20a%20valuable%20contribution%20to%20signal%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.11757v4&entry.124074799=Read"},
{"title": "Deep Learning for Active Region Classification: A Systematic Study from\n  Convolutional Neural Networks to Vision Transformers", "author": "Edoardo Legnaro and Sabrina Guastavino and Michele Piana and Anna Maria Massone", "abstract": "  A solar active region can significantly disrupt the Sun Earth space\nenvironment, often leading to severe space weather events such as solar flares\nand coronal mass ejections. As a consequence, the automatic classification of\nactive region groups is the crucial starting point for accurately and promptly\npredicting solar activity. This study presents our results concerned with the\napplication of deep learning techniques to the classification of active region\ncutouts based on the Mount Wilson classification scheme. Specifically, we have\nexplored the latest advancements in image classification architectures, from\nConvolutional Neural Networks to Vision Transformers, and reported on their\nperformances for the active region classification task, showing that the\ncrucial point for their effectiveness consists in a robust training process\nbased on the latest advances in the field.\n", "link": "http://arxiv.org/abs/2410.17816v1", "date": "2024-10-23", "relevancy": 2.3392, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4672}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4672}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20Learning%20for%20Active%20Region%20Classification%3A%20A%20Systematic%20Study%20from%0A%20%20Convolutional%20Neural%20Networks%20to%20Vision%20Transformers&body=Title%3A%20Deep%20Learning%20for%20Active%20Region%20Classification%3A%20A%20Systematic%20Study%20from%0A%20%20Convolutional%20Neural%20Networks%20to%20Vision%20Transformers%0AAuthor%3A%20Edoardo%20Legnaro%20and%20Sabrina%20Guastavino%20and%20Michele%20Piana%20and%20Anna%20Maria%20Massone%0AAbstract%3A%20%20%20A%20solar%20active%20region%20can%20significantly%20disrupt%20the%20Sun%20Earth%20space%0Aenvironment%2C%20often%20leading%20to%20severe%20space%20weather%20events%20such%20as%20solar%20flares%0Aand%20coronal%20mass%20ejections.%20As%20a%20consequence%2C%20the%20automatic%20classification%20of%0Aactive%20region%20groups%20is%20the%20crucial%20starting%20point%20for%20accurately%20and%20promptly%0Apredicting%20solar%20activity.%20This%20study%20presents%20our%20results%20concerned%20with%20the%0Aapplication%20of%20deep%20learning%20techniques%20to%20the%20classification%20of%20active%20region%0Acutouts%20based%20on%20the%20Mount%20Wilson%20classification%20scheme.%20Specifically%2C%20we%20have%0Aexplored%20the%20latest%20advancements%20in%20image%20classification%20architectures%2C%20from%0AConvolutional%20Neural%20Networks%20to%20Vision%20Transformers%2C%20and%20reported%20on%20their%0Aperformances%20for%20the%20active%20region%20classification%20task%2C%20showing%20that%20the%0Acrucial%20point%20for%20their%20effectiveness%20consists%20in%20a%20robust%20training%20process%0Abased%20on%20the%20latest%20advances%20in%20the%20field.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520Learning%2520for%2520Active%2520Region%2520Classification%253A%2520A%2520Systematic%2520Study%2520from%250A%2520%2520Convolutional%2520Neural%2520Networks%2520to%2520Vision%2520Transformers%26entry.906535625%3DEdoardo%2520Legnaro%2520and%2520Sabrina%2520Guastavino%2520and%2520Michele%2520Piana%2520and%2520Anna%2520Maria%2520Massone%26entry.1292438233%3D%2520%2520A%2520solar%2520active%2520region%2520can%2520significantly%2520disrupt%2520the%2520Sun%2520Earth%2520space%250Aenvironment%252C%2520often%2520leading%2520to%2520severe%2520space%2520weather%2520events%2520such%2520as%2520solar%2520flares%250Aand%2520coronal%2520mass%2520ejections.%2520As%2520a%2520consequence%252C%2520the%2520automatic%2520classification%2520of%250Aactive%2520region%2520groups%2520is%2520the%2520crucial%2520starting%2520point%2520for%2520accurately%2520and%2520promptly%250Apredicting%2520solar%2520activity.%2520This%2520study%2520presents%2520our%2520results%2520concerned%2520with%2520the%250Aapplication%2520of%2520deep%2520learning%2520techniques%2520to%2520the%2520classification%2520of%2520active%2520region%250Acutouts%2520based%2520on%2520the%2520Mount%2520Wilson%2520classification%2520scheme.%2520Specifically%252C%2520we%2520have%250Aexplored%2520the%2520latest%2520advancements%2520in%2520image%2520classification%2520architectures%252C%2520from%250AConvolutional%2520Neural%2520Networks%2520to%2520Vision%2520Transformers%252C%2520and%2520reported%2520on%2520their%250Aperformances%2520for%2520the%2520active%2520region%2520classification%2520task%252C%2520showing%2520that%2520the%250Acrucial%2520point%2520for%2520their%2520effectiveness%2520consists%2520in%2520a%2520robust%2520training%2520process%250Abased%2520on%2520the%2520latest%2520advances%2520in%2520the%2520field.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20Learning%20for%20Active%20Region%20Classification%3A%20A%20Systematic%20Study%20from%0A%20%20Convolutional%20Neural%20Networks%20to%20Vision%20Transformers&entry.906535625=Edoardo%20Legnaro%20and%20Sabrina%20Guastavino%20and%20Michele%20Piana%20and%20Anna%20Maria%20Massone&entry.1292438233=%20%20A%20solar%20active%20region%20can%20significantly%20disrupt%20the%20Sun%20Earth%20space%0Aenvironment%2C%20often%20leading%20to%20severe%20space%20weather%20events%20such%20as%20solar%20flares%0Aand%20coronal%20mass%20ejections.%20As%20a%20consequence%2C%20the%20automatic%20classification%20of%0Aactive%20region%20groups%20is%20the%20crucial%20starting%20point%20for%20accurately%20and%20promptly%0Apredicting%20solar%20activity.%20This%20study%20presents%20our%20results%20concerned%20with%20the%0Aapplication%20of%20deep%20learning%20techniques%20to%20the%20classification%20of%20active%20region%0Acutouts%20based%20on%20the%20Mount%20Wilson%20classification%20scheme.%20Specifically%2C%20we%20have%0Aexplored%20the%20latest%20advancements%20in%20image%20classification%20architectures%2C%20from%0AConvolutional%20Neural%20Networks%20to%20Vision%20Transformers%2C%20and%20reported%20on%20their%0Aperformances%20for%20the%20active%20region%20classification%20task%2C%20showing%20that%20the%0Acrucial%20point%20for%20their%20effectiveness%20consists%20in%20a%20robust%20training%20process%0Abased%20on%20the%20latest%20advances%20in%20the%20field.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17816v1&entry.124074799=Read"},
{"title": "Emotion Recognition with Facial Attention and Objective Activation\n  Functions", "author": "Andrzej Miskow and Abdulrahman Altahhan", "abstract": "  In this paper, we study the effect of introducing channel and spatial\nattention mechanisms, namely SEN-Net, ECA-Net, and CBAM, to existing CNN\nvision-based models such as VGGNet, ResNet, and ResNetV2 to perform the Facial\nEmotion Recognition task. We show that not only attention can significantly\nimprove the performance of these models but also that combining them with a\ndifferent activation function can further help increase the performance of\nthese models.\n", "link": "http://arxiv.org/abs/2410.17740v1", "date": "2024-10-23", "relevancy": 2.3228, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4646}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4646}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Emotion%20Recognition%20with%20Facial%20Attention%20and%20Objective%20Activation%0A%20%20Functions&body=Title%3A%20Emotion%20Recognition%20with%20Facial%20Attention%20and%20Objective%20Activation%0A%20%20Functions%0AAuthor%3A%20Andrzej%20Miskow%20and%20Abdulrahman%20Altahhan%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20study%20the%20effect%20of%20introducing%20channel%20and%20spatial%0Aattention%20mechanisms%2C%20namely%20SEN-Net%2C%20ECA-Net%2C%20and%20CBAM%2C%20to%20existing%20CNN%0Avision-based%20models%20such%20as%20VGGNet%2C%20ResNet%2C%20and%20ResNetV2%20to%20perform%20the%20Facial%0AEmotion%20Recognition%20task.%20We%20show%20that%20not%20only%20attention%20can%20significantly%0Aimprove%20the%20performance%20of%20these%20models%20but%20also%20that%20combining%20them%20with%20a%0Adifferent%20activation%20function%20can%20further%20help%20increase%20the%20performance%20of%0Athese%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17740v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmotion%2520Recognition%2520with%2520Facial%2520Attention%2520and%2520Objective%2520Activation%250A%2520%2520Functions%26entry.906535625%3DAndrzej%2520Miskow%2520and%2520Abdulrahman%2520Altahhan%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520study%2520the%2520effect%2520of%2520introducing%2520channel%2520and%2520spatial%250Aattention%2520mechanisms%252C%2520namely%2520SEN-Net%252C%2520ECA-Net%252C%2520and%2520CBAM%252C%2520to%2520existing%2520CNN%250Avision-based%2520models%2520such%2520as%2520VGGNet%252C%2520ResNet%252C%2520and%2520ResNetV2%2520to%2520perform%2520the%2520Facial%250AEmotion%2520Recognition%2520task.%2520We%2520show%2520that%2520not%2520only%2520attention%2520can%2520significantly%250Aimprove%2520the%2520performance%2520of%2520these%2520models%2520but%2520also%2520that%2520combining%2520them%2520with%2520a%250Adifferent%2520activation%2520function%2520can%2520further%2520help%2520increase%2520the%2520performance%2520of%250Athese%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17740v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Emotion%20Recognition%20with%20Facial%20Attention%20and%20Objective%20Activation%0A%20%20Functions&entry.906535625=Andrzej%20Miskow%20and%20Abdulrahman%20Altahhan&entry.1292438233=%20%20In%20this%20paper%2C%20we%20study%20the%20effect%20of%20introducing%20channel%20and%20spatial%0Aattention%20mechanisms%2C%20namely%20SEN-Net%2C%20ECA-Net%2C%20and%20CBAM%2C%20to%20existing%20CNN%0Avision-based%20models%20such%20as%20VGGNet%2C%20ResNet%2C%20and%20ResNetV2%20to%20perform%20the%20Facial%0AEmotion%20Recognition%20task.%20We%20show%20that%20not%20only%20attention%20can%20significantly%0Aimprove%20the%20performance%20of%20these%20models%20but%20also%20that%20combining%20them%20with%20a%0Adifferent%20activation%20function%20can%20further%20help%20increase%20the%20performance%20of%0Athese%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17740v1&entry.124074799=Read"},
{"title": "Gaussian Process Distance Fields Obstacle and Ground Constraints for\n  Safe Navigation", "author": "Monisha Mushtary Uttsha and Cedric Le Gentil and Lan Wu and Teresa Vidal-Calleja", "abstract": "  Navigating cluttered environments is a challenging task for any mobile\nsystem. Existing approaches for ground-based mobile systems primarily focus on\nsmall wheeled robots, which face minimal constraints with overhanging obstacles\nand cannot manage steps or stairs, making the problem effectively 2D. However,\nnavigation for legged robots (or even humans) has to consider an extra\ndimension. This paper proposes a tailored scene representation coupled with an\nadvanced trajectory optimisation algorithm to enable safe navigation. Our 3D\nnavigation approach is suitable for any ground-based mobile robot, whether\nwheeled or legged, as well as for human assistance. Given a 3D point cloud of\nthe scene and the segmentation of the ground and non-ground points, we\nformulate two Gaussian Process distance fields to ensure a collision-free path\nand maintain distance to the ground constraints. Our method adeptly handles\nuneven terrain, steps, and overhanging objects through an innovative use of a\nquadtree structure, constructing a multi-resolution map of the free space and\nits connectivity graph based on a 2D projection of the relevant scene.\nEvaluations with both synthetic and real-world datasets demonstrate that this\napproach provides safe and smooth paths, accommodating a wide range of\nground-based mobile systems.\n", "link": "http://arxiv.org/abs/2410.17831v1", "date": "2024-10-23", "relevancy": 2.3197, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6038}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5727}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Process%20Distance%20Fields%20Obstacle%20and%20Ground%20Constraints%20for%0A%20%20Safe%20Navigation&body=Title%3A%20Gaussian%20Process%20Distance%20Fields%20Obstacle%20and%20Ground%20Constraints%20for%0A%20%20Safe%20Navigation%0AAuthor%3A%20Monisha%20Mushtary%20Uttsha%20and%20Cedric%20Le%20Gentil%20and%20Lan%20Wu%20and%20Teresa%20Vidal-Calleja%0AAbstract%3A%20%20%20Navigating%20cluttered%20environments%20is%20a%20challenging%20task%20for%20any%20mobile%0Asystem.%20Existing%20approaches%20for%20ground-based%20mobile%20systems%20primarily%20focus%20on%0Asmall%20wheeled%20robots%2C%20which%20face%20minimal%20constraints%20with%20overhanging%20obstacles%0Aand%20cannot%20manage%20steps%20or%20stairs%2C%20making%20the%20problem%20effectively%202D.%20However%2C%0Anavigation%20for%20legged%20robots%20%28or%20even%20humans%29%20has%20to%20consider%20an%20extra%0Adimension.%20This%20paper%20proposes%20a%20tailored%20scene%20representation%20coupled%20with%20an%0Aadvanced%20trajectory%20optimisation%20algorithm%20to%20enable%20safe%20navigation.%20Our%203D%0Anavigation%20approach%20is%20suitable%20for%20any%20ground-based%20mobile%20robot%2C%20whether%0Awheeled%20or%20legged%2C%20as%20well%20as%20for%20human%20assistance.%20Given%20a%203D%20point%20cloud%20of%0Athe%20scene%20and%20the%20segmentation%20of%20the%20ground%20and%20non-ground%20points%2C%20we%0Aformulate%20two%20Gaussian%20Process%20distance%20fields%20to%20ensure%20a%20collision-free%20path%0Aand%20maintain%20distance%20to%20the%20ground%20constraints.%20Our%20method%20adeptly%20handles%0Auneven%20terrain%2C%20steps%2C%20and%20overhanging%20objects%20through%20an%20innovative%20use%20of%20a%0Aquadtree%20structure%2C%20constructing%20a%20multi-resolution%20map%20of%20the%20free%20space%20and%0Aits%20connectivity%20graph%20based%20on%20a%202D%20projection%20of%20the%20relevant%20scene.%0AEvaluations%20with%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20this%0Aapproach%20provides%20safe%20and%20smooth%20paths%2C%20accommodating%20a%20wide%20range%20of%0Aground-based%20mobile%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17831v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Process%2520Distance%2520Fields%2520Obstacle%2520and%2520Ground%2520Constraints%2520for%250A%2520%2520Safe%2520Navigation%26entry.906535625%3DMonisha%2520Mushtary%2520Uttsha%2520and%2520Cedric%2520Le%2520Gentil%2520and%2520Lan%2520Wu%2520and%2520Teresa%2520Vidal-Calleja%26entry.1292438233%3D%2520%2520Navigating%2520cluttered%2520environments%2520is%2520a%2520challenging%2520task%2520for%2520any%2520mobile%250Asystem.%2520Existing%2520approaches%2520for%2520ground-based%2520mobile%2520systems%2520primarily%2520focus%2520on%250Asmall%2520wheeled%2520robots%252C%2520which%2520face%2520minimal%2520constraints%2520with%2520overhanging%2520obstacles%250Aand%2520cannot%2520manage%2520steps%2520or%2520stairs%252C%2520making%2520the%2520problem%2520effectively%25202D.%2520However%252C%250Anavigation%2520for%2520legged%2520robots%2520%2528or%2520even%2520humans%2529%2520has%2520to%2520consider%2520an%2520extra%250Adimension.%2520This%2520paper%2520proposes%2520a%2520tailored%2520scene%2520representation%2520coupled%2520with%2520an%250Aadvanced%2520trajectory%2520optimisation%2520algorithm%2520to%2520enable%2520safe%2520navigation.%2520Our%25203D%250Anavigation%2520approach%2520is%2520suitable%2520for%2520any%2520ground-based%2520mobile%2520robot%252C%2520whether%250Awheeled%2520or%2520legged%252C%2520as%2520well%2520as%2520for%2520human%2520assistance.%2520Given%2520a%25203D%2520point%2520cloud%2520of%250Athe%2520scene%2520and%2520the%2520segmentation%2520of%2520the%2520ground%2520and%2520non-ground%2520points%252C%2520we%250Aformulate%2520two%2520Gaussian%2520Process%2520distance%2520fields%2520to%2520ensure%2520a%2520collision-free%2520path%250Aand%2520maintain%2520distance%2520to%2520the%2520ground%2520constraints.%2520Our%2520method%2520adeptly%2520handles%250Auneven%2520terrain%252C%2520steps%252C%2520and%2520overhanging%2520objects%2520through%2520an%2520innovative%2520use%2520of%2520a%250Aquadtree%2520structure%252C%2520constructing%2520a%2520multi-resolution%2520map%2520of%2520the%2520free%2520space%2520and%250Aits%2520connectivity%2520graph%2520based%2520on%2520a%25202D%2520projection%2520of%2520the%2520relevant%2520scene.%250AEvaluations%2520with%2520both%2520synthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520this%250Aapproach%2520provides%2520safe%2520and%2520smooth%2520paths%252C%2520accommodating%2520a%2520wide%2520range%2520of%250Aground-based%2520mobile%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17831v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Process%20Distance%20Fields%20Obstacle%20and%20Ground%20Constraints%20for%0A%20%20Safe%20Navigation&entry.906535625=Monisha%20Mushtary%20Uttsha%20and%20Cedric%20Le%20Gentil%20and%20Lan%20Wu%20and%20Teresa%20Vidal-Calleja&entry.1292438233=%20%20Navigating%20cluttered%20environments%20is%20a%20challenging%20task%20for%20any%20mobile%0Asystem.%20Existing%20approaches%20for%20ground-based%20mobile%20systems%20primarily%20focus%20on%0Asmall%20wheeled%20robots%2C%20which%20face%20minimal%20constraints%20with%20overhanging%20obstacles%0Aand%20cannot%20manage%20steps%20or%20stairs%2C%20making%20the%20problem%20effectively%202D.%20However%2C%0Anavigation%20for%20legged%20robots%20%28or%20even%20humans%29%20has%20to%20consider%20an%20extra%0Adimension.%20This%20paper%20proposes%20a%20tailored%20scene%20representation%20coupled%20with%20an%0Aadvanced%20trajectory%20optimisation%20algorithm%20to%20enable%20safe%20navigation.%20Our%203D%0Anavigation%20approach%20is%20suitable%20for%20any%20ground-based%20mobile%20robot%2C%20whether%0Awheeled%20or%20legged%2C%20as%20well%20as%20for%20human%20assistance.%20Given%20a%203D%20point%20cloud%20of%0Athe%20scene%20and%20the%20segmentation%20of%20the%20ground%20and%20non-ground%20points%2C%20we%0Aformulate%20two%20Gaussian%20Process%20distance%20fields%20to%20ensure%20a%20collision-free%20path%0Aand%20maintain%20distance%20to%20the%20ground%20constraints.%20Our%20method%20adeptly%20handles%0Auneven%20terrain%2C%20steps%2C%20and%20overhanging%20objects%20through%20an%20innovative%20use%20of%20a%0Aquadtree%20structure%2C%20constructing%20a%20multi-resolution%20map%20of%20the%20free%20space%20and%0Aits%20connectivity%20graph%20based%20on%20a%202D%20projection%20of%20the%20relevant%20scene.%0AEvaluations%20with%20both%20synthetic%20and%20real-world%20datasets%20demonstrate%20that%20this%0Aapproach%20provides%20safe%20and%20smooth%20paths%2C%20accommodating%20a%20wide%20range%20of%0Aground-based%20mobile%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17831v1&entry.124074799=Read"},
{"title": "Towards Safer Planetary Exploration: A Hybrid Architecture for Terrain\n  Traversability Analysis in Mars Rovers", "author": "Achille Chiuchiarelli and Giacomo Franchini and Francesco Messina and Marcello Chiaberge", "abstract": "  The field of autonomous navigation for unmanned ground vehicles (UGVs) is in\ncontinuous growth and increasing levels of autonomy have been reached in the\nlast few years. However, the task becomes more challenging when the focus is on\nthe exploration of planet surfaces such as Mars. In those situations, UGVs are\nforced to navigate through unstable and rugged terrains which, inevitably, open\nthe vehicle to more hazards, accidents, and, in extreme cases, complete mission\nfailure. The paper addresses the challenges of autonomous navigation for\nunmanned ground vehicles in planetary exploration, particularly on Mars,\nintroducing a hybrid architecture for terrain traversability analysis that\ncombines two approaches: appearance-based and geometry-based. The\nappearance-based method uses semantic segmentation via deep neural networks to\nclassify different terrain types. This is further refined by pixel-level\nterrain roughness classification obtained from the same RGB image, assigning\ndifferent costs based on the physical properties of the soil. The\ngeometry-based method complements the appearance-based approach by evaluating\nthe terrain's geometrical features, identifying hazards that may not be\ndetectable by the appearance-based side. The outputs of both methods are\ncombined into a comprehensive hybrid cost map. The proposed architecture was\ntrained on synthetic datasets and developed as a ROS2 application to integrate\ninto broader autonomous navigation systems for harsh environments. Simulations\nhave been performed in Unity, showing the ability of the method to assess\nonline traversability analysis.\n", "link": "http://arxiv.org/abs/2410.17738v1", "date": "2024-10-23", "relevancy": 2.315, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6051}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5748}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5721}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Safer%20Planetary%20Exploration%3A%20A%20Hybrid%20Architecture%20for%20Terrain%0A%20%20Traversability%20Analysis%20in%20Mars%20Rovers&body=Title%3A%20Towards%20Safer%20Planetary%20Exploration%3A%20A%20Hybrid%20Architecture%20for%20Terrain%0A%20%20Traversability%20Analysis%20in%20Mars%20Rovers%0AAuthor%3A%20Achille%20Chiuchiarelli%20and%20Giacomo%20Franchini%20and%20Francesco%20Messina%20and%20Marcello%20Chiaberge%0AAbstract%3A%20%20%20The%20field%20of%20autonomous%20navigation%20for%20unmanned%20ground%20vehicles%20%28UGVs%29%20is%20in%0Acontinuous%20growth%20and%20increasing%20levels%20of%20autonomy%20have%20been%20reached%20in%20the%0Alast%20few%20years.%20However%2C%20the%20task%20becomes%20more%20challenging%20when%20the%20focus%20is%20on%0Athe%20exploration%20of%20planet%20surfaces%20such%20as%20Mars.%20In%20those%20situations%2C%20UGVs%20are%0Aforced%20to%20navigate%20through%20unstable%20and%20rugged%20terrains%20which%2C%20inevitably%2C%20open%0Athe%20vehicle%20to%20more%20hazards%2C%20accidents%2C%20and%2C%20in%20extreme%20cases%2C%20complete%20mission%0Afailure.%20The%20paper%20addresses%20the%20challenges%20of%20autonomous%20navigation%20for%0Aunmanned%20ground%20vehicles%20in%20planetary%20exploration%2C%20particularly%20on%20Mars%2C%0Aintroducing%20a%20hybrid%20architecture%20for%20terrain%20traversability%20analysis%20that%0Acombines%20two%20approaches%3A%20appearance-based%20and%20geometry-based.%20The%0Aappearance-based%20method%20uses%20semantic%20segmentation%20via%20deep%20neural%20networks%20to%0Aclassify%20different%20terrain%20types.%20This%20is%20further%20refined%20by%20pixel-level%0Aterrain%20roughness%20classification%20obtained%20from%20the%20same%20RGB%20image%2C%20assigning%0Adifferent%20costs%20based%20on%20the%20physical%20properties%20of%20the%20soil.%20The%0Ageometry-based%20method%20complements%20the%20appearance-based%20approach%20by%20evaluating%0Athe%20terrain%27s%20geometrical%20features%2C%20identifying%20hazards%20that%20may%20not%20be%0Adetectable%20by%20the%20appearance-based%20side.%20The%20outputs%20of%20both%20methods%20are%0Acombined%20into%20a%20comprehensive%20hybrid%20cost%20map.%20The%20proposed%20architecture%20was%0Atrained%20on%20synthetic%20datasets%20and%20developed%20as%20a%20ROS2%20application%20to%20integrate%0Ainto%20broader%20autonomous%20navigation%20systems%20for%20harsh%20environments.%20Simulations%0Ahave%20been%20performed%20in%20Unity%2C%20showing%20the%20ability%20of%20the%20method%20to%20assess%0Aonline%20traversability%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Safer%2520Planetary%2520Exploration%253A%2520A%2520Hybrid%2520Architecture%2520for%2520Terrain%250A%2520%2520Traversability%2520Analysis%2520in%2520Mars%2520Rovers%26entry.906535625%3DAchille%2520Chiuchiarelli%2520and%2520Giacomo%2520Franchini%2520and%2520Francesco%2520Messina%2520and%2520Marcello%2520Chiaberge%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520autonomous%2520navigation%2520for%2520unmanned%2520ground%2520vehicles%2520%2528UGVs%2529%2520is%2520in%250Acontinuous%2520growth%2520and%2520increasing%2520levels%2520of%2520autonomy%2520have%2520been%2520reached%2520in%2520the%250Alast%2520few%2520years.%2520However%252C%2520the%2520task%2520becomes%2520more%2520challenging%2520when%2520the%2520focus%2520is%2520on%250Athe%2520exploration%2520of%2520planet%2520surfaces%2520such%2520as%2520Mars.%2520In%2520those%2520situations%252C%2520UGVs%2520are%250Aforced%2520to%2520navigate%2520through%2520unstable%2520and%2520rugged%2520terrains%2520which%252C%2520inevitably%252C%2520open%250Athe%2520vehicle%2520to%2520more%2520hazards%252C%2520accidents%252C%2520and%252C%2520in%2520extreme%2520cases%252C%2520complete%2520mission%250Afailure.%2520The%2520paper%2520addresses%2520the%2520challenges%2520of%2520autonomous%2520navigation%2520for%250Aunmanned%2520ground%2520vehicles%2520in%2520planetary%2520exploration%252C%2520particularly%2520on%2520Mars%252C%250Aintroducing%2520a%2520hybrid%2520architecture%2520for%2520terrain%2520traversability%2520analysis%2520that%250Acombines%2520two%2520approaches%253A%2520appearance-based%2520and%2520geometry-based.%2520The%250Aappearance-based%2520method%2520uses%2520semantic%2520segmentation%2520via%2520deep%2520neural%2520networks%2520to%250Aclassify%2520different%2520terrain%2520types.%2520This%2520is%2520further%2520refined%2520by%2520pixel-level%250Aterrain%2520roughness%2520classification%2520obtained%2520from%2520the%2520same%2520RGB%2520image%252C%2520assigning%250Adifferent%2520costs%2520based%2520on%2520the%2520physical%2520properties%2520of%2520the%2520soil.%2520The%250Ageometry-based%2520method%2520complements%2520the%2520appearance-based%2520approach%2520by%2520evaluating%250Athe%2520terrain%2527s%2520geometrical%2520features%252C%2520identifying%2520hazards%2520that%2520may%2520not%2520be%250Adetectable%2520by%2520the%2520appearance-based%2520side.%2520The%2520outputs%2520of%2520both%2520methods%2520are%250Acombined%2520into%2520a%2520comprehensive%2520hybrid%2520cost%2520map.%2520The%2520proposed%2520architecture%2520was%250Atrained%2520on%2520synthetic%2520datasets%2520and%2520developed%2520as%2520a%2520ROS2%2520application%2520to%2520integrate%250Ainto%2520broader%2520autonomous%2520navigation%2520systems%2520for%2520harsh%2520environments.%2520Simulations%250Ahave%2520been%2520performed%2520in%2520Unity%252C%2520showing%2520the%2520ability%2520of%2520the%2520method%2520to%2520assess%250Aonline%2520traversability%2520analysis.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Safer%20Planetary%20Exploration%3A%20A%20Hybrid%20Architecture%20for%20Terrain%0A%20%20Traversability%20Analysis%20in%20Mars%20Rovers&entry.906535625=Achille%20Chiuchiarelli%20and%20Giacomo%20Franchini%20and%20Francesco%20Messina%20and%20Marcello%20Chiaberge&entry.1292438233=%20%20The%20field%20of%20autonomous%20navigation%20for%20unmanned%20ground%20vehicles%20%28UGVs%29%20is%20in%0Acontinuous%20growth%20and%20increasing%20levels%20of%20autonomy%20have%20been%20reached%20in%20the%0Alast%20few%20years.%20However%2C%20the%20task%20becomes%20more%20challenging%20when%20the%20focus%20is%20on%0Athe%20exploration%20of%20planet%20surfaces%20such%20as%20Mars.%20In%20those%20situations%2C%20UGVs%20are%0Aforced%20to%20navigate%20through%20unstable%20and%20rugged%20terrains%20which%2C%20inevitably%2C%20open%0Athe%20vehicle%20to%20more%20hazards%2C%20accidents%2C%20and%2C%20in%20extreme%20cases%2C%20complete%20mission%0Afailure.%20The%20paper%20addresses%20the%20challenges%20of%20autonomous%20navigation%20for%0Aunmanned%20ground%20vehicles%20in%20planetary%20exploration%2C%20particularly%20on%20Mars%2C%0Aintroducing%20a%20hybrid%20architecture%20for%20terrain%20traversability%20analysis%20that%0Acombines%20two%20approaches%3A%20appearance-based%20and%20geometry-based.%20The%0Aappearance-based%20method%20uses%20semantic%20segmentation%20via%20deep%20neural%20networks%20to%0Aclassify%20different%20terrain%20types.%20This%20is%20further%20refined%20by%20pixel-level%0Aterrain%20roughness%20classification%20obtained%20from%20the%20same%20RGB%20image%2C%20assigning%0Adifferent%20costs%20based%20on%20the%20physical%20properties%20of%20the%20soil.%20The%0Ageometry-based%20method%20complements%20the%20appearance-based%20approach%20by%20evaluating%0Athe%20terrain%27s%20geometrical%20features%2C%20identifying%20hazards%20that%20may%20not%20be%0Adetectable%20by%20the%20appearance-based%20side.%20The%20outputs%20of%20both%20methods%20are%0Acombined%20into%20a%20comprehensive%20hybrid%20cost%20map.%20The%20proposed%20architecture%20was%0Atrained%20on%20synthetic%20datasets%20and%20developed%20as%20a%20ROS2%20application%20to%20integrate%0Ainto%20broader%20autonomous%20navigation%20systems%20for%20harsh%20environments.%20Simulations%0Ahave%20been%20performed%20in%20Unity%2C%20showing%20the%20ability%20of%20the%20method%20to%20assess%0Aonline%20traversability%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17738v1&entry.124074799=Read"},
{"title": "Meteor: Mamba-based Traversal of Rationale for Large Language and Vision\n  Models", "author": "Byung-Kwan Lee and Chae Won Kim and Beomchan Park and Yong Man Ro", "abstract": "  The rapid development of large language and vision models (LLVMs) has been\ndriven by advances in visual instruction tuning. Recently, open-source LLVMs\nhave curated high-quality visual instruction tuning datasets and utilized\nadditional vision encoders or multiple computer vision models in order to\nnarrow the performance gap with powerful closed-source LLVMs. These\nadvancements are attributed to multifaceted information required for diverse\ncapabilities, including fundamental image understanding, real-world knowledge\nabout common-sense and non-object concepts (e.g., charts, diagrams, symbols,\nsigns, and math problems), and step-by-step procedures for solving complex\nquestions. Drawing from the multifaceted information, we present a new\nefficient LLVM, Mamba-based traversal of rationales (Meteor), which leverages\nmultifaceted rationale to enhance understanding and answering capabilities. To\nembed lengthy rationales containing abundant information, we employ the Mamba\narchitecture, capable of processing sequential data with linear time\ncomplexity. We introduce a new concept of traversal of rationale that\nfacilitates efficient embedding of rationale. Subsequently, the backbone\nmultimodal language model (MLM) is trained to generate answers with the aid of\nrationale. Through these steps, Meteor achieves significant improvements in\nvision language performances across multiple evaluation benchmarks requiring\ndiverse capabilities, without scaling up the model size or employing additional\nvision encoders and computer vision models.\n", "link": "http://arxiv.org/abs/2405.15574v4", "date": "2024-10-23", "relevancy": 2.3076, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5871}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5871}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meteor%3A%20Mamba-based%20Traversal%20of%20Rationale%20for%20Large%20Language%20and%20Vision%0A%20%20Models&body=Title%3A%20Meteor%3A%20Mamba-based%20Traversal%20of%20Rationale%20for%20Large%20Language%20and%20Vision%0A%20%20Models%0AAuthor%3A%20Byung-Kwan%20Lee%20and%20Chae%20Won%20Kim%20and%20Beomchan%20Park%20and%20Yong%20Man%20Ro%0AAbstract%3A%20%20%20The%20rapid%20development%20of%20large%20language%20and%20vision%20models%20%28LLVMs%29%20has%20been%0Adriven%20by%20advances%20in%20visual%20instruction%20tuning.%20Recently%2C%20open-source%20LLVMs%0Ahave%20curated%20high-quality%20visual%20instruction%20tuning%20datasets%20and%20utilized%0Aadditional%20vision%20encoders%20or%20multiple%20computer%20vision%20models%20in%20order%20to%0Anarrow%20the%20performance%20gap%20with%20powerful%20closed-source%20LLVMs.%20These%0Aadvancements%20are%20attributed%20to%20multifaceted%20information%20required%20for%20diverse%0Acapabilities%2C%20including%20fundamental%20image%20understanding%2C%20real-world%20knowledge%0Aabout%20common-sense%20and%20non-object%20concepts%20%28e.g.%2C%20charts%2C%20diagrams%2C%20symbols%2C%0Asigns%2C%20and%20math%20problems%29%2C%20and%20step-by-step%20procedures%20for%20solving%20complex%0Aquestions.%20Drawing%20from%20the%20multifaceted%20information%2C%20we%20present%20a%20new%0Aefficient%20LLVM%2C%20Mamba-based%20traversal%20of%20rationales%20%28Meteor%29%2C%20which%20leverages%0Amultifaceted%20rationale%20to%20enhance%20understanding%20and%20answering%20capabilities.%20To%0Aembed%20lengthy%20rationales%20containing%20abundant%20information%2C%20we%20employ%20the%20Mamba%0Aarchitecture%2C%20capable%20of%20processing%20sequential%20data%20with%20linear%20time%0Acomplexity.%20We%20introduce%20a%20new%20concept%20of%20traversal%20of%20rationale%20that%0Afacilitates%20efficient%20embedding%20of%20rationale.%20Subsequently%2C%20the%20backbone%0Amultimodal%20language%20model%20%28MLM%29%20is%20trained%20to%20generate%20answers%20with%20the%20aid%20of%0Arationale.%20Through%20these%20steps%2C%20Meteor%20achieves%20significant%20improvements%20in%0Avision%20language%20performances%20across%20multiple%20evaluation%20benchmarks%20requiring%0Adiverse%20capabilities%2C%20without%20scaling%20up%20the%20model%20size%20or%20employing%20additional%0Avision%20encoders%20and%20computer%20vision%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15574v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeteor%253A%2520Mamba-based%2520Traversal%2520of%2520Rationale%2520for%2520Large%2520Language%2520and%2520Vision%250A%2520%2520Models%26entry.906535625%3DByung-Kwan%2520Lee%2520and%2520Chae%2520Won%2520Kim%2520and%2520Beomchan%2520Park%2520and%2520Yong%2520Man%2520Ro%26entry.1292438233%3D%2520%2520The%2520rapid%2520development%2520of%2520large%2520language%2520and%2520vision%2520models%2520%2528LLVMs%2529%2520has%2520been%250Adriven%2520by%2520advances%2520in%2520visual%2520instruction%2520tuning.%2520Recently%252C%2520open-source%2520LLVMs%250Ahave%2520curated%2520high-quality%2520visual%2520instruction%2520tuning%2520datasets%2520and%2520utilized%250Aadditional%2520vision%2520encoders%2520or%2520multiple%2520computer%2520vision%2520models%2520in%2520order%2520to%250Anarrow%2520the%2520performance%2520gap%2520with%2520powerful%2520closed-source%2520LLVMs.%2520These%250Aadvancements%2520are%2520attributed%2520to%2520multifaceted%2520information%2520required%2520for%2520diverse%250Acapabilities%252C%2520including%2520fundamental%2520image%2520understanding%252C%2520real-world%2520knowledge%250Aabout%2520common-sense%2520and%2520non-object%2520concepts%2520%2528e.g.%252C%2520charts%252C%2520diagrams%252C%2520symbols%252C%250Asigns%252C%2520and%2520math%2520problems%2529%252C%2520and%2520step-by-step%2520procedures%2520for%2520solving%2520complex%250Aquestions.%2520Drawing%2520from%2520the%2520multifaceted%2520information%252C%2520we%2520present%2520a%2520new%250Aefficient%2520LLVM%252C%2520Mamba-based%2520traversal%2520of%2520rationales%2520%2528Meteor%2529%252C%2520which%2520leverages%250Amultifaceted%2520rationale%2520to%2520enhance%2520understanding%2520and%2520answering%2520capabilities.%2520To%250Aembed%2520lengthy%2520rationales%2520containing%2520abundant%2520information%252C%2520we%2520employ%2520the%2520Mamba%250Aarchitecture%252C%2520capable%2520of%2520processing%2520sequential%2520data%2520with%2520linear%2520time%250Acomplexity.%2520We%2520introduce%2520a%2520new%2520concept%2520of%2520traversal%2520of%2520rationale%2520that%250Afacilitates%2520efficient%2520embedding%2520of%2520rationale.%2520Subsequently%252C%2520the%2520backbone%250Amultimodal%2520language%2520model%2520%2528MLM%2529%2520is%2520trained%2520to%2520generate%2520answers%2520with%2520the%2520aid%2520of%250Arationale.%2520Through%2520these%2520steps%252C%2520Meteor%2520achieves%2520significant%2520improvements%2520in%250Avision%2520language%2520performances%2520across%2520multiple%2520evaluation%2520benchmarks%2520requiring%250Adiverse%2520capabilities%252C%2520without%2520scaling%2520up%2520the%2520model%2520size%2520or%2520employing%2520additional%250Avision%2520encoders%2520and%2520computer%2520vision%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15574v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meteor%3A%20Mamba-based%20Traversal%20of%20Rationale%20for%20Large%20Language%20and%20Vision%0A%20%20Models&entry.906535625=Byung-Kwan%20Lee%20and%20Chae%20Won%20Kim%20and%20Beomchan%20Park%20and%20Yong%20Man%20Ro&entry.1292438233=%20%20The%20rapid%20development%20of%20large%20language%20and%20vision%20models%20%28LLVMs%29%20has%20been%0Adriven%20by%20advances%20in%20visual%20instruction%20tuning.%20Recently%2C%20open-source%20LLVMs%0Ahave%20curated%20high-quality%20visual%20instruction%20tuning%20datasets%20and%20utilized%0Aadditional%20vision%20encoders%20or%20multiple%20computer%20vision%20models%20in%20order%20to%0Anarrow%20the%20performance%20gap%20with%20powerful%20closed-source%20LLVMs.%20These%0Aadvancements%20are%20attributed%20to%20multifaceted%20information%20required%20for%20diverse%0Acapabilities%2C%20including%20fundamental%20image%20understanding%2C%20real-world%20knowledge%0Aabout%20common-sense%20and%20non-object%20concepts%20%28e.g.%2C%20charts%2C%20diagrams%2C%20symbols%2C%0Asigns%2C%20and%20math%20problems%29%2C%20and%20step-by-step%20procedures%20for%20solving%20complex%0Aquestions.%20Drawing%20from%20the%20multifaceted%20information%2C%20we%20present%20a%20new%0Aefficient%20LLVM%2C%20Mamba-based%20traversal%20of%20rationales%20%28Meteor%29%2C%20which%20leverages%0Amultifaceted%20rationale%20to%20enhance%20understanding%20and%20answering%20capabilities.%20To%0Aembed%20lengthy%20rationales%20containing%20abundant%20information%2C%20we%20employ%20the%20Mamba%0Aarchitecture%2C%20capable%20of%20processing%20sequential%20data%20with%20linear%20time%0Acomplexity.%20We%20introduce%20a%20new%20concept%20of%20traversal%20of%20rationale%20that%0Afacilitates%20efficient%20embedding%20of%20rationale.%20Subsequently%2C%20the%20backbone%0Amultimodal%20language%20model%20%28MLM%29%20is%20trained%20to%20generate%20answers%20with%20the%20aid%20of%0Arationale.%20Through%20these%20steps%2C%20Meteor%20achieves%20significant%20improvements%20in%0Avision%20language%20performances%20across%20multiple%20evaluation%20benchmarks%20requiring%0Adiverse%20capabilities%2C%20without%20scaling%20up%20the%20model%20size%20or%20employing%20additional%0Avision%20encoders%20and%20computer%20vision%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15574v4&entry.124074799=Read"},
{"title": "WorldSimBench: Towards Video Generation Models as World Simulators", "author": "Yiran Qin and Zhelun Shi and Jiwen Yu and Xijun Wang and Enshen Zhou and Lijun Li and Zhenfei Yin and Xihui Liu and Lu Sheng and Jing Shao and Lei Bai and Wanli Ouyang and Ruimao Zhang", "abstract": "  Recent advancements in predictive models have demonstrated exceptional\ncapabilities in predicting the future state of objects and scenes. However, the\nlack of categorization based on inherent characteristics continues to hinder\nthe progress of predictive model development. Additionally, existing benchmarks\nare unable to effectively evaluate higher-capability, highly embodied\npredictive models from an embodied perspective. In this work, we classify the\nfunctionalities of predictive models into a hierarchy and take the first step\nin evaluating World Simulators by proposing a dual evaluation framework called\nWorldSimBench. WorldSimBench includes Explicit Perceptual Evaluation and\nImplicit Manipulative Evaluation, encompassing human preference assessments\nfrom the visual perspective and action-level evaluations in embodied tasks,\ncovering three representative embodied scenarios: Open-Ended Embodied\nEnvironment, Autonomous, Driving, and Robot Manipulation. In the Explicit\nPerceptual Evaluation, we introduce the HF-Embodied Dataset, a video assessment\ndataset based on fine-grained human feedback, which we use to train a Human\nPreference Evaluator that aligns with human perception and explicitly assesses\nthe visual fidelity of World Simulators. In the Implicit Manipulative\nEvaluation, we assess the video-action consistency of World Simulators by\nevaluating whether the generated situation-aware video can be accurately\ntranslated into the correct control signals in dynamic environments. Our\ncomprehensive evaluation offers key insights that can drive further innovation\nin video generation models, positioning World Simulators as a pivotal\nadvancement toward embodied artificial intelligence.\n", "link": "http://arxiv.org/abs/2410.18072v1", "date": "2024-10-23", "relevancy": 2.3016, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.594}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.566}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5605}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WorldSimBench%3A%20Towards%20Video%20Generation%20Models%20as%20World%20Simulators&body=Title%3A%20WorldSimBench%3A%20Towards%20Video%20Generation%20Models%20as%20World%20Simulators%0AAuthor%3A%20Yiran%20Qin%20and%20Zhelun%20Shi%20and%20Jiwen%20Yu%20and%20Xijun%20Wang%20and%20Enshen%20Zhou%20and%20Lijun%20Li%20and%20Zhenfei%20Yin%20and%20Xihui%20Liu%20and%20Lu%20Sheng%20and%20Jing%20Shao%20and%20Lei%20Bai%20and%20Wanli%20Ouyang%20and%20Ruimao%20Zhang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20predictive%20models%20have%20demonstrated%20exceptional%0Acapabilities%20in%20predicting%20the%20future%20state%20of%20objects%20and%20scenes.%20However%2C%20the%0Alack%20of%20categorization%20based%20on%20inherent%20characteristics%20continues%20to%20hinder%0Athe%20progress%20of%20predictive%20model%20development.%20Additionally%2C%20existing%20benchmarks%0Aare%20unable%20to%20effectively%20evaluate%20higher-capability%2C%20highly%20embodied%0Apredictive%20models%20from%20an%20embodied%20perspective.%20In%20this%20work%2C%20we%20classify%20the%0Afunctionalities%20of%20predictive%20models%20into%20a%20hierarchy%20and%20take%20the%20first%20step%0Ain%20evaluating%20World%20Simulators%20by%20proposing%20a%20dual%20evaluation%20framework%20called%0AWorldSimBench.%20WorldSimBench%20includes%20Explicit%20Perceptual%20Evaluation%20and%0AImplicit%20Manipulative%20Evaluation%2C%20encompassing%20human%20preference%20assessments%0Afrom%20the%20visual%20perspective%20and%20action-level%20evaluations%20in%20embodied%20tasks%2C%0Acovering%20three%20representative%20embodied%20scenarios%3A%20Open-Ended%20Embodied%0AEnvironment%2C%20Autonomous%2C%20Driving%2C%20and%20Robot%20Manipulation.%20In%20the%20Explicit%0APerceptual%20Evaluation%2C%20we%20introduce%20the%20HF-Embodied%20Dataset%2C%20a%20video%20assessment%0Adataset%20based%20on%20fine-grained%20human%20feedback%2C%20which%20we%20use%20to%20train%20a%20Human%0APreference%20Evaluator%20that%20aligns%20with%20human%20perception%20and%20explicitly%20assesses%0Athe%20visual%20fidelity%20of%20World%20Simulators.%20In%20the%20Implicit%20Manipulative%0AEvaluation%2C%20we%20assess%20the%20video-action%20consistency%20of%20World%20Simulators%20by%0Aevaluating%20whether%20the%20generated%20situation-aware%20video%20can%20be%20accurately%0Atranslated%20into%20the%20correct%20control%20signals%20in%20dynamic%20environments.%20Our%0Acomprehensive%20evaluation%20offers%20key%20insights%20that%20can%20drive%20further%20innovation%0Ain%20video%20generation%20models%2C%20positioning%20World%20Simulators%20as%20a%20pivotal%0Aadvancement%20toward%20embodied%20artificial%20intelligence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18072v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWorldSimBench%253A%2520Towards%2520Video%2520Generation%2520Models%2520as%2520World%2520Simulators%26entry.906535625%3DYiran%2520Qin%2520and%2520Zhelun%2520Shi%2520and%2520Jiwen%2520Yu%2520and%2520Xijun%2520Wang%2520and%2520Enshen%2520Zhou%2520and%2520Lijun%2520Li%2520and%2520Zhenfei%2520Yin%2520and%2520Xihui%2520Liu%2520and%2520Lu%2520Sheng%2520and%2520Jing%2520Shao%2520and%2520Lei%2520Bai%2520and%2520Wanli%2520Ouyang%2520and%2520Ruimao%2520Zhang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520predictive%2520models%2520have%2520demonstrated%2520exceptional%250Acapabilities%2520in%2520predicting%2520the%2520future%2520state%2520of%2520objects%2520and%2520scenes.%2520However%252C%2520the%250Alack%2520of%2520categorization%2520based%2520on%2520inherent%2520characteristics%2520continues%2520to%2520hinder%250Athe%2520progress%2520of%2520predictive%2520model%2520development.%2520Additionally%252C%2520existing%2520benchmarks%250Aare%2520unable%2520to%2520effectively%2520evaluate%2520higher-capability%252C%2520highly%2520embodied%250Apredictive%2520models%2520from%2520an%2520embodied%2520perspective.%2520In%2520this%2520work%252C%2520we%2520classify%2520the%250Afunctionalities%2520of%2520predictive%2520models%2520into%2520a%2520hierarchy%2520and%2520take%2520the%2520first%2520step%250Ain%2520evaluating%2520World%2520Simulators%2520by%2520proposing%2520a%2520dual%2520evaluation%2520framework%2520called%250AWorldSimBench.%2520WorldSimBench%2520includes%2520Explicit%2520Perceptual%2520Evaluation%2520and%250AImplicit%2520Manipulative%2520Evaluation%252C%2520encompassing%2520human%2520preference%2520assessments%250Afrom%2520the%2520visual%2520perspective%2520and%2520action-level%2520evaluations%2520in%2520embodied%2520tasks%252C%250Acovering%2520three%2520representative%2520embodied%2520scenarios%253A%2520Open-Ended%2520Embodied%250AEnvironment%252C%2520Autonomous%252C%2520Driving%252C%2520and%2520Robot%2520Manipulation.%2520In%2520the%2520Explicit%250APerceptual%2520Evaluation%252C%2520we%2520introduce%2520the%2520HF-Embodied%2520Dataset%252C%2520a%2520video%2520assessment%250Adataset%2520based%2520on%2520fine-grained%2520human%2520feedback%252C%2520which%2520we%2520use%2520to%2520train%2520a%2520Human%250APreference%2520Evaluator%2520that%2520aligns%2520with%2520human%2520perception%2520and%2520explicitly%2520assesses%250Athe%2520visual%2520fidelity%2520of%2520World%2520Simulators.%2520In%2520the%2520Implicit%2520Manipulative%250AEvaluation%252C%2520we%2520assess%2520the%2520video-action%2520consistency%2520of%2520World%2520Simulators%2520by%250Aevaluating%2520whether%2520the%2520generated%2520situation-aware%2520video%2520can%2520be%2520accurately%250Atranslated%2520into%2520the%2520correct%2520control%2520signals%2520in%2520dynamic%2520environments.%2520Our%250Acomprehensive%2520evaluation%2520offers%2520key%2520insights%2520that%2520can%2520drive%2520further%2520innovation%250Ain%2520video%2520generation%2520models%252C%2520positioning%2520World%2520Simulators%2520as%2520a%2520pivotal%250Aadvancement%2520toward%2520embodied%2520artificial%2520intelligence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18072v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WorldSimBench%3A%20Towards%20Video%20Generation%20Models%20as%20World%20Simulators&entry.906535625=Yiran%20Qin%20and%20Zhelun%20Shi%20and%20Jiwen%20Yu%20and%20Xijun%20Wang%20and%20Enshen%20Zhou%20and%20Lijun%20Li%20and%20Zhenfei%20Yin%20and%20Xihui%20Liu%20and%20Lu%20Sheng%20and%20Jing%20Shao%20and%20Lei%20Bai%20and%20Wanli%20Ouyang%20and%20Ruimao%20Zhang&entry.1292438233=%20%20Recent%20advancements%20in%20predictive%20models%20have%20demonstrated%20exceptional%0Acapabilities%20in%20predicting%20the%20future%20state%20of%20objects%20and%20scenes.%20However%2C%20the%0Alack%20of%20categorization%20based%20on%20inherent%20characteristics%20continues%20to%20hinder%0Athe%20progress%20of%20predictive%20model%20development.%20Additionally%2C%20existing%20benchmarks%0Aare%20unable%20to%20effectively%20evaluate%20higher-capability%2C%20highly%20embodied%0Apredictive%20models%20from%20an%20embodied%20perspective.%20In%20this%20work%2C%20we%20classify%20the%0Afunctionalities%20of%20predictive%20models%20into%20a%20hierarchy%20and%20take%20the%20first%20step%0Ain%20evaluating%20World%20Simulators%20by%20proposing%20a%20dual%20evaluation%20framework%20called%0AWorldSimBench.%20WorldSimBench%20includes%20Explicit%20Perceptual%20Evaluation%20and%0AImplicit%20Manipulative%20Evaluation%2C%20encompassing%20human%20preference%20assessments%0Afrom%20the%20visual%20perspective%20and%20action-level%20evaluations%20in%20embodied%20tasks%2C%0Acovering%20three%20representative%20embodied%20scenarios%3A%20Open-Ended%20Embodied%0AEnvironment%2C%20Autonomous%2C%20Driving%2C%20and%20Robot%20Manipulation.%20In%20the%20Explicit%0APerceptual%20Evaluation%2C%20we%20introduce%20the%20HF-Embodied%20Dataset%2C%20a%20video%20assessment%0Adataset%20based%20on%20fine-grained%20human%20feedback%2C%20which%20we%20use%20to%20train%20a%20Human%0APreference%20Evaluator%20that%20aligns%20with%20human%20perception%20and%20explicitly%20assesses%0Athe%20visual%20fidelity%20of%20World%20Simulators.%20In%20the%20Implicit%20Manipulative%0AEvaluation%2C%20we%20assess%20the%20video-action%20consistency%20of%20World%20Simulators%20by%0Aevaluating%20whether%20the%20generated%20situation-aware%20video%20can%20be%20accurately%0Atranslated%20into%20the%20correct%20control%20signals%20in%20dynamic%20environments.%20Our%0Acomprehensive%20evaluation%20offers%20key%20insights%20that%20can%20drive%20further%20innovation%0Ain%20video%20generation%20models%2C%20positioning%20World%20Simulators%20as%20a%20pivotal%0Aadvancement%20toward%20embodied%20artificial%20intelligence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18072v1&entry.124074799=Read"},
{"title": "JointMotion: Joint Self-Supervision for Joint Motion Prediction", "author": "Royden Wagner and Omer Sahin Tas and Marvin Klemp and Carlos Fernandez", "abstract": "  We present JointMotion, a self-supervised pre-training method for joint\nmotion prediction in self-driving vehicles. Our method jointly optimizes a\nscene-level objective connecting motion and environments, and an instance-level\nobjective to refine learned representations. Scene-level representations are\nlearned via non-contrastive similarity learning of past motion sequences and\nenvironment context. At the instance level, we use masked autoencoding to\nrefine multimodal polyline representations. We complement this with an adaptive\npre-training decoder that enables JointMotion to generalize across different\nenvironment representations, fusion mechanisms, and dataset characteristics.\nNotably, our method reduces the joint final displacement error of Wayformer,\nHPTR, and Scene Transformer models by 3\\%, 8\\%, and 12\\%, respectively; and\nenables transfer learning between the Waymo Open Motion and the Argoverse 2\nMotion Forecasting datasets. Code: https://github.com/kit-mrt/future-motion\n", "link": "http://arxiv.org/abs/2403.05489v2", "date": "2024-10-23", "relevancy": 2.2935, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6085}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5884}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20JointMotion%3A%20Joint%20Self-Supervision%20for%20Joint%20Motion%20Prediction&body=Title%3A%20JointMotion%3A%20Joint%20Self-Supervision%20for%20Joint%20Motion%20Prediction%0AAuthor%3A%20Royden%20Wagner%20and%20Omer%20Sahin%20Tas%20and%20Marvin%20Klemp%20and%20Carlos%20Fernandez%0AAbstract%3A%20%20%20We%20present%20JointMotion%2C%20a%20self-supervised%20pre-training%20method%20for%20joint%0Amotion%20prediction%20in%20self-driving%20vehicles.%20Our%20method%20jointly%20optimizes%20a%0Ascene-level%20objective%20connecting%20motion%20and%20environments%2C%20and%20an%20instance-level%0Aobjective%20to%20refine%20learned%20representations.%20Scene-level%20representations%20are%0Alearned%20via%20non-contrastive%20similarity%20learning%20of%20past%20motion%20sequences%20and%0Aenvironment%20context.%20At%20the%20instance%20level%2C%20we%20use%20masked%20autoencoding%20to%0Arefine%20multimodal%20polyline%20representations.%20We%20complement%20this%20with%20an%20adaptive%0Apre-training%20decoder%20that%20enables%20JointMotion%20to%20generalize%20across%20different%0Aenvironment%20representations%2C%20fusion%20mechanisms%2C%20and%20dataset%20characteristics.%0ANotably%2C%20our%20method%20reduces%20the%20joint%20final%20displacement%20error%20of%20Wayformer%2C%0AHPTR%2C%20and%20Scene%20Transformer%20models%20by%203%5C%25%2C%208%5C%25%2C%20and%2012%5C%25%2C%20respectively%3B%20and%0Aenables%20transfer%20learning%20between%20the%20Waymo%20Open%20Motion%20and%20the%20Argoverse%202%0AMotion%20Forecasting%20datasets.%20Code%3A%20https%3A//github.com/kit-mrt/future-motion%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.05489v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJointMotion%253A%2520Joint%2520Self-Supervision%2520for%2520Joint%2520Motion%2520Prediction%26entry.906535625%3DRoyden%2520Wagner%2520and%2520Omer%2520Sahin%2520Tas%2520and%2520Marvin%2520Klemp%2520and%2520Carlos%2520Fernandez%26entry.1292438233%3D%2520%2520We%2520present%2520JointMotion%252C%2520a%2520self-supervised%2520pre-training%2520method%2520for%2520joint%250Amotion%2520prediction%2520in%2520self-driving%2520vehicles.%2520Our%2520method%2520jointly%2520optimizes%2520a%250Ascene-level%2520objective%2520connecting%2520motion%2520and%2520environments%252C%2520and%2520an%2520instance-level%250Aobjective%2520to%2520refine%2520learned%2520representations.%2520Scene-level%2520representations%2520are%250Alearned%2520via%2520non-contrastive%2520similarity%2520learning%2520of%2520past%2520motion%2520sequences%2520and%250Aenvironment%2520context.%2520At%2520the%2520instance%2520level%252C%2520we%2520use%2520masked%2520autoencoding%2520to%250Arefine%2520multimodal%2520polyline%2520representations.%2520We%2520complement%2520this%2520with%2520an%2520adaptive%250Apre-training%2520decoder%2520that%2520enables%2520JointMotion%2520to%2520generalize%2520across%2520different%250Aenvironment%2520representations%252C%2520fusion%2520mechanisms%252C%2520and%2520dataset%2520characteristics.%250ANotably%252C%2520our%2520method%2520reduces%2520the%2520joint%2520final%2520displacement%2520error%2520of%2520Wayformer%252C%250AHPTR%252C%2520and%2520Scene%2520Transformer%2520models%2520by%25203%255C%2525%252C%25208%255C%2525%252C%2520and%252012%255C%2525%252C%2520respectively%253B%2520and%250Aenables%2520transfer%2520learning%2520between%2520the%2520Waymo%2520Open%2520Motion%2520and%2520the%2520Argoverse%25202%250AMotion%2520Forecasting%2520datasets.%2520Code%253A%2520https%253A//github.com/kit-mrt/future-motion%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.05489v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=JointMotion%3A%20Joint%20Self-Supervision%20for%20Joint%20Motion%20Prediction&entry.906535625=Royden%20Wagner%20and%20Omer%20Sahin%20Tas%20and%20Marvin%20Klemp%20and%20Carlos%20Fernandez&entry.1292438233=%20%20We%20present%20JointMotion%2C%20a%20self-supervised%20pre-training%20method%20for%20joint%0Amotion%20prediction%20in%20self-driving%20vehicles.%20Our%20method%20jointly%20optimizes%20a%0Ascene-level%20objective%20connecting%20motion%20and%20environments%2C%20and%20an%20instance-level%0Aobjective%20to%20refine%20learned%20representations.%20Scene-level%20representations%20are%0Alearned%20via%20non-contrastive%20similarity%20learning%20of%20past%20motion%20sequences%20and%0Aenvironment%20context.%20At%20the%20instance%20level%2C%20we%20use%20masked%20autoencoding%20to%0Arefine%20multimodal%20polyline%20representations.%20We%20complement%20this%20with%20an%20adaptive%0Apre-training%20decoder%20that%20enables%20JointMotion%20to%20generalize%20across%20different%0Aenvironment%20representations%2C%20fusion%20mechanisms%2C%20and%20dataset%20characteristics.%0ANotably%2C%20our%20method%20reduces%20the%20joint%20final%20displacement%20error%20of%20Wayformer%2C%0AHPTR%2C%20and%20Scene%20Transformer%20models%20by%203%5C%25%2C%208%5C%25%2C%20and%2012%5C%25%2C%20respectively%3B%20and%0Aenables%20transfer%20learning%20between%20the%20Waymo%20Open%20Motion%20and%20the%20Argoverse%202%0AMotion%20Forecasting%20datasets.%20Code%3A%20https%3A//github.com/kit-mrt/future-motion%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.05489v2&entry.124074799=Read"},
{"title": "Topology meets Machine Learning: An Introduction using the Euler\n  Characteristic Transform", "author": "Bastian Rieck", "abstract": "  This overview article makes the case for how topological concepts can enrich\nresearch in machine learning. Using the Euler Characteristic Transform (ECT), a\ngeometrical-topological invariant, as a running example, I present different\nuse cases that result in more efficient models for analyzing point clouds,\ngraphs, and meshes. Moreover, I outline a vision for how topological concepts\ncould be used in the future, comprising (1) the learning of functions on\ntopological spaces, (2) the building of hybrid models that imbue neural\nnetworks with knowledge about the topological information in data, and (3) the\nanalysis of qualitative properties of neural networks. With current research\nalready addressing some of these aspects, this article thus serves as an\nintroduction and invitation to this nascent area of research.\n", "link": "http://arxiv.org/abs/2410.17760v1", "date": "2024-10-23", "relevancy": 2.2784, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4597}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4537}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4537}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Topology%20meets%20Machine%20Learning%3A%20An%20Introduction%20using%20the%20Euler%0A%20%20Characteristic%20Transform&body=Title%3A%20Topology%20meets%20Machine%20Learning%3A%20An%20Introduction%20using%20the%20Euler%0A%20%20Characteristic%20Transform%0AAuthor%3A%20Bastian%20Rieck%0AAbstract%3A%20%20%20This%20overview%20article%20makes%20the%20case%20for%20how%20topological%20concepts%20can%20enrich%0Aresearch%20in%20machine%20learning.%20Using%20the%20Euler%20Characteristic%20Transform%20%28ECT%29%2C%20a%0Ageometrical-topological%20invariant%2C%20as%20a%20running%20example%2C%20I%20present%20different%0Ause%20cases%20that%20result%20in%20more%20efficient%20models%20for%20analyzing%20point%20clouds%2C%0Agraphs%2C%20and%20meshes.%20Moreover%2C%20I%20outline%20a%20vision%20for%20how%20topological%20concepts%0Acould%20be%20used%20in%20the%20future%2C%20comprising%20%281%29%20the%20learning%20of%20functions%20on%0Atopological%20spaces%2C%20%282%29%20the%20building%20of%20hybrid%20models%20that%20imbue%20neural%0Anetworks%20with%20knowledge%20about%20the%20topological%20information%20in%20data%2C%20and%20%283%29%20the%0Aanalysis%20of%20qualitative%20properties%20of%20neural%20networks.%20With%20current%20research%0Aalready%20addressing%20some%20of%20these%20aspects%2C%20this%20article%20thus%20serves%20as%20an%0Aintroduction%20and%20invitation%20to%20this%20nascent%20area%20of%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17760v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTopology%2520meets%2520Machine%2520Learning%253A%2520An%2520Introduction%2520using%2520the%2520Euler%250A%2520%2520Characteristic%2520Transform%26entry.906535625%3DBastian%2520Rieck%26entry.1292438233%3D%2520%2520This%2520overview%2520article%2520makes%2520the%2520case%2520for%2520how%2520topological%2520concepts%2520can%2520enrich%250Aresearch%2520in%2520machine%2520learning.%2520Using%2520the%2520Euler%2520Characteristic%2520Transform%2520%2528ECT%2529%252C%2520a%250Ageometrical-topological%2520invariant%252C%2520as%2520a%2520running%2520example%252C%2520I%2520present%2520different%250Ause%2520cases%2520that%2520result%2520in%2520more%2520efficient%2520models%2520for%2520analyzing%2520point%2520clouds%252C%250Agraphs%252C%2520and%2520meshes.%2520Moreover%252C%2520I%2520outline%2520a%2520vision%2520for%2520how%2520topological%2520concepts%250Acould%2520be%2520used%2520in%2520the%2520future%252C%2520comprising%2520%25281%2529%2520the%2520learning%2520of%2520functions%2520on%250Atopological%2520spaces%252C%2520%25282%2529%2520the%2520building%2520of%2520hybrid%2520models%2520that%2520imbue%2520neural%250Anetworks%2520with%2520knowledge%2520about%2520the%2520topological%2520information%2520in%2520data%252C%2520and%2520%25283%2529%2520the%250Aanalysis%2520of%2520qualitative%2520properties%2520of%2520neural%2520networks.%2520With%2520current%2520research%250Aalready%2520addressing%2520some%2520of%2520these%2520aspects%252C%2520this%2520article%2520thus%2520serves%2520as%2520an%250Aintroduction%2520and%2520invitation%2520to%2520this%2520nascent%2520area%2520of%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17760v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Topology%20meets%20Machine%20Learning%3A%20An%20Introduction%20using%20the%20Euler%0A%20%20Characteristic%20Transform&entry.906535625=Bastian%20Rieck&entry.1292438233=%20%20This%20overview%20article%20makes%20the%20case%20for%20how%20topological%20concepts%20can%20enrich%0Aresearch%20in%20machine%20learning.%20Using%20the%20Euler%20Characteristic%20Transform%20%28ECT%29%2C%20a%0Ageometrical-topological%20invariant%2C%20as%20a%20running%20example%2C%20I%20present%20different%0Ause%20cases%20that%20result%20in%20more%20efficient%20models%20for%20analyzing%20point%20clouds%2C%0Agraphs%2C%20and%20meshes.%20Moreover%2C%20I%20outline%20a%20vision%20for%20how%20topological%20concepts%0Acould%20be%20used%20in%20the%20future%2C%20comprising%20%281%29%20the%20learning%20of%20functions%20on%0Atopological%20spaces%2C%20%282%29%20the%20building%20of%20hybrid%20models%20that%20imbue%20neural%0Anetworks%20with%20knowledge%20about%20the%20topological%20information%20in%20data%2C%20and%20%283%29%20the%0Aanalysis%20of%20qualitative%20properties%20of%20neural%20networks.%20With%20current%20research%0Aalready%20addressing%20some%20of%20these%20aspects%2C%20this%20article%20thus%20serves%20as%20an%0Aintroduction%20and%20invitation%20to%20this%20nascent%20area%20of%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17760v1&entry.124074799=Read"},
{"title": "Exploiting Text-Image Latent Spaces for the Description of Visual\n  Concepts", "author": "Laines Schmalwasser and Jakob Gawlikowski and Joachim Denzler and Julia Niebling", "abstract": "  Concept Activation Vectors (CAVs) offer insights into neural network\ndecision-making by linking human friendly concepts to the model's internal\nfeature extraction process. However, when a new set of CAVs is discovered, they\nmust still be translated into a human understandable description. For\nimage-based neural networks, this is typically done by visualizing the most\nrelevant images of a CAV, while the determination of the concept is left to\nhumans. In this work, we introduce an approach to aid the interpretation of\nnewly discovered concept sets by suggesting textual descriptions for each CAV.\nThis is done by mapping the most relevant images representing a CAV into a\ntext-image embedding where a joint description of these relevant images can be\ncomputed. We propose utilizing the most relevant receptive fields instead of\nfull images encoded. We demonstrate the capabilities of this approach in\nmultiple experiments with and without given CAV labels, showing that the\nproposed approach provides accurate descriptions for the CAVs and reduces the\nchallenge of concept interpretation.\n", "link": "http://arxiv.org/abs/2410.17832v1", "date": "2024-10-23", "relevancy": 2.2663, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.571}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5443}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploiting%20Text-Image%20Latent%20Spaces%20for%20the%20Description%20of%20Visual%0A%20%20Concepts&body=Title%3A%20Exploiting%20Text-Image%20Latent%20Spaces%20for%20the%20Description%20of%20Visual%0A%20%20Concepts%0AAuthor%3A%20Laines%20Schmalwasser%20and%20Jakob%20Gawlikowski%20and%20Joachim%20Denzler%20and%20Julia%20Niebling%0AAbstract%3A%20%20%20Concept%20Activation%20Vectors%20%28CAVs%29%20offer%20insights%20into%20neural%20network%0Adecision-making%20by%20linking%20human%20friendly%20concepts%20to%20the%20model%27s%20internal%0Afeature%20extraction%20process.%20However%2C%20when%20a%20new%20set%20of%20CAVs%20is%20discovered%2C%20they%0Amust%20still%20be%20translated%20into%20a%20human%20understandable%20description.%20For%0Aimage-based%20neural%20networks%2C%20this%20is%20typically%20done%20by%20visualizing%20the%20most%0Arelevant%20images%20of%20a%20CAV%2C%20while%20the%20determination%20of%20the%20concept%20is%20left%20to%0Ahumans.%20In%20this%20work%2C%20we%20introduce%20an%20approach%20to%20aid%20the%20interpretation%20of%0Anewly%20discovered%20concept%20sets%20by%20suggesting%20textual%20descriptions%20for%20each%20CAV.%0AThis%20is%20done%20by%20mapping%20the%20most%20relevant%20images%20representing%20a%20CAV%20into%20a%0Atext-image%20embedding%20where%20a%20joint%20description%20of%20these%20relevant%20images%20can%20be%0Acomputed.%20We%20propose%20utilizing%20the%20most%20relevant%20receptive%20fields%20instead%20of%0Afull%20images%20encoded.%20We%20demonstrate%20the%20capabilities%20of%20this%20approach%20in%0Amultiple%20experiments%20with%20and%20without%20given%20CAV%20labels%2C%20showing%20that%20the%0Aproposed%20approach%20provides%20accurate%20descriptions%20for%20the%20CAVs%20and%20reduces%20the%0Achallenge%20of%20concept%20interpretation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploiting%2520Text-Image%2520Latent%2520Spaces%2520for%2520the%2520Description%2520of%2520Visual%250A%2520%2520Concepts%26entry.906535625%3DLaines%2520Schmalwasser%2520and%2520Jakob%2520Gawlikowski%2520and%2520Joachim%2520Denzler%2520and%2520Julia%2520Niebling%26entry.1292438233%3D%2520%2520Concept%2520Activation%2520Vectors%2520%2528CAVs%2529%2520offer%2520insights%2520into%2520neural%2520network%250Adecision-making%2520by%2520linking%2520human%2520friendly%2520concepts%2520to%2520the%2520model%2527s%2520internal%250Afeature%2520extraction%2520process.%2520However%252C%2520when%2520a%2520new%2520set%2520of%2520CAVs%2520is%2520discovered%252C%2520they%250Amust%2520still%2520be%2520translated%2520into%2520a%2520human%2520understandable%2520description.%2520For%250Aimage-based%2520neural%2520networks%252C%2520this%2520is%2520typically%2520done%2520by%2520visualizing%2520the%2520most%250Arelevant%2520images%2520of%2520a%2520CAV%252C%2520while%2520the%2520determination%2520of%2520the%2520concept%2520is%2520left%2520to%250Ahumans.%2520In%2520this%2520work%252C%2520we%2520introduce%2520an%2520approach%2520to%2520aid%2520the%2520interpretation%2520of%250Anewly%2520discovered%2520concept%2520sets%2520by%2520suggesting%2520textual%2520descriptions%2520for%2520each%2520CAV.%250AThis%2520is%2520done%2520by%2520mapping%2520the%2520most%2520relevant%2520images%2520representing%2520a%2520CAV%2520into%2520a%250Atext-image%2520embedding%2520where%2520a%2520joint%2520description%2520of%2520these%2520relevant%2520images%2520can%2520be%250Acomputed.%2520We%2520propose%2520utilizing%2520the%2520most%2520relevant%2520receptive%2520fields%2520instead%2520of%250Afull%2520images%2520encoded.%2520We%2520demonstrate%2520the%2520capabilities%2520of%2520this%2520approach%2520in%250Amultiple%2520experiments%2520with%2520and%2520without%2520given%2520CAV%2520labels%252C%2520showing%2520that%2520the%250Aproposed%2520approach%2520provides%2520accurate%2520descriptions%2520for%2520the%2520CAVs%2520and%2520reduces%2520the%250Achallenge%2520of%2520concept%2520interpretation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploiting%20Text-Image%20Latent%20Spaces%20for%20the%20Description%20of%20Visual%0A%20%20Concepts&entry.906535625=Laines%20Schmalwasser%20and%20Jakob%20Gawlikowski%20and%20Joachim%20Denzler%20and%20Julia%20Niebling&entry.1292438233=%20%20Concept%20Activation%20Vectors%20%28CAVs%29%20offer%20insights%20into%20neural%20network%0Adecision-making%20by%20linking%20human%20friendly%20concepts%20to%20the%20model%27s%20internal%0Afeature%20extraction%20process.%20However%2C%20when%20a%20new%20set%20of%20CAVs%20is%20discovered%2C%20they%0Amust%20still%20be%20translated%20into%20a%20human%20understandable%20description.%20For%0Aimage-based%20neural%20networks%2C%20this%20is%20typically%20done%20by%20visualizing%20the%20most%0Arelevant%20images%20of%20a%20CAV%2C%20while%20the%20determination%20of%20the%20concept%20is%20left%20to%0Ahumans.%20In%20this%20work%2C%20we%20introduce%20an%20approach%20to%20aid%20the%20interpretation%20of%0Anewly%20discovered%20concept%20sets%20by%20suggesting%20textual%20descriptions%20for%20each%20CAV.%0AThis%20is%20done%20by%20mapping%20the%20most%20relevant%20images%20representing%20a%20CAV%20into%20a%0Atext-image%20embedding%20where%20a%20joint%20description%20of%20these%20relevant%20images%20can%20be%0Acomputed.%20We%20propose%20utilizing%20the%20most%20relevant%20receptive%20fields%20instead%20of%0Afull%20images%20encoded.%20We%20demonstrate%20the%20capabilities%20of%20this%20approach%20in%0Amultiple%20experiments%20with%20and%20without%20given%20CAV%20labels%2C%20showing%20that%20the%0Aproposed%20approach%20provides%20accurate%20descriptions%20for%20the%20CAVs%20and%20reduces%20the%0Achallenge%20of%20concept%20interpretation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17832v1&entry.124074799=Read"},
{"title": "UnCLe: Unsupervised Continual Learning of Depth Completion", "author": "Suchisrit Gangopadhyay and Xien Chen and Michael Chu and Patrick Rim and Hyoungseob Park and Alex Wong", "abstract": "  We propose UnCLe, a standardized benchmark for Unsupervised Continual\nLearning of a multimodal depth estimation task: Depth completion aims to infer\na dense depth map from a pair of synchronized RGB image and sparse depth map.\nWe benchmark depth completion models under the practical scenario of\nunsupervised learning over continuous streams of data. Existing methods are\ntypically trained on a static, or stationary, dataset. However, when adapting\nto novel non-stationary distributions, they \"catastrophically forget\"\npreviously learned information. UnCLe simulates these non-stationary\ndistributions by adapting depth completion models to sequences of datasets\ncontaining diverse scenes captured from distinct domains using different visual\nand range sensors. We adopt representative methods from continual learning\nparadigms and translate them to enable unsupervised continual learning of depth\ncompletion. We benchmark these models for indoor and outdoor and investigate\nthe degree of catastrophic forgetting through standard quantitative metrics.\nFurthermore, we introduce model inversion quality as an additional measure of\nforgetting. We find that unsupervised continual learning of depth completion is\nan open problem, and we invite researchers to leverage UnCLe as a development\nplatform.\n", "link": "http://arxiv.org/abs/2410.18074v1", "date": "2024-10-23", "relevancy": 2.2604, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5736}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.567}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UnCLe%3A%20Unsupervised%20Continual%20Learning%20of%20Depth%20Completion&body=Title%3A%20UnCLe%3A%20Unsupervised%20Continual%20Learning%20of%20Depth%20Completion%0AAuthor%3A%20Suchisrit%20Gangopadhyay%20and%20Xien%20Chen%20and%20Michael%20Chu%20and%20Patrick%20Rim%20and%20Hyoungseob%20Park%20and%20Alex%20Wong%0AAbstract%3A%20%20%20We%20propose%20UnCLe%2C%20a%20standardized%20benchmark%20for%20Unsupervised%20Continual%0ALearning%20of%20a%20multimodal%20depth%20estimation%20task%3A%20Depth%20completion%20aims%20to%20infer%0Aa%20dense%20depth%20map%20from%20a%20pair%20of%20synchronized%20RGB%20image%20and%20sparse%20depth%20map.%0AWe%20benchmark%20depth%20completion%20models%20under%20the%20practical%20scenario%20of%0Aunsupervised%20learning%20over%20continuous%20streams%20of%20data.%20Existing%20methods%20are%0Atypically%20trained%20on%20a%20static%2C%20or%20stationary%2C%20dataset.%20However%2C%20when%20adapting%0Ato%20novel%20non-stationary%20distributions%2C%20they%20%22catastrophically%20forget%22%0Apreviously%20learned%20information.%20UnCLe%20simulates%20these%20non-stationary%0Adistributions%20by%20adapting%20depth%20completion%20models%20to%20sequences%20of%20datasets%0Acontaining%20diverse%20scenes%20captured%20from%20distinct%20domains%20using%20different%20visual%0Aand%20range%20sensors.%20We%20adopt%20representative%20methods%20from%20continual%20learning%0Aparadigms%20and%20translate%20them%20to%20enable%20unsupervised%20continual%20learning%20of%20depth%0Acompletion.%20We%20benchmark%20these%20models%20for%20indoor%20and%20outdoor%20and%20investigate%0Athe%20degree%20of%20catastrophic%20forgetting%20through%20standard%20quantitative%20metrics.%0AFurthermore%2C%20we%20introduce%20model%20inversion%20quality%20as%20an%20additional%20measure%20of%0Aforgetting.%20We%20find%20that%20unsupervised%20continual%20learning%20of%20depth%20completion%20is%0Aan%20open%20problem%2C%20and%20we%20invite%20researchers%20to%20leverage%20UnCLe%20as%20a%20development%0Aplatform.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18074v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnCLe%253A%2520Unsupervised%2520Continual%2520Learning%2520of%2520Depth%2520Completion%26entry.906535625%3DSuchisrit%2520Gangopadhyay%2520and%2520Xien%2520Chen%2520and%2520Michael%2520Chu%2520and%2520Patrick%2520Rim%2520and%2520Hyoungseob%2520Park%2520and%2520Alex%2520Wong%26entry.1292438233%3D%2520%2520We%2520propose%2520UnCLe%252C%2520a%2520standardized%2520benchmark%2520for%2520Unsupervised%2520Continual%250ALearning%2520of%2520a%2520multimodal%2520depth%2520estimation%2520task%253A%2520Depth%2520completion%2520aims%2520to%2520infer%250Aa%2520dense%2520depth%2520map%2520from%2520a%2520pair%2520of%2520synchronized%2520RGB%2520image%2520and%2520sparse%2520depth%2520map.%250AWe%2520benchmark%2520depth%2520completion%2520models%2520under%2520the%2520practical%2520scenario%2520of%250Aunsupervised%2520learning%2520over%2520continuous%2520streams%2520of%2520data.%2520Existing%2520methods%2520are%250Atypically%2520trained%2520on%2520a%2520static%252C%2520or%2520stationary%252C%2520dataset.%2520However%252C%2520when%2520adapting%250Ato%2520novel%2520non-stationary%2520distributions%252C%2520they%2520%2522catastrophically%2520forget%2522%250Apreviously%2520learned%2520information.%2520UnCLe%2520simulates%2520these%2520non-stationary%250Adistributions%2520by%2520adapting%2520depth%2520completion%2520models%2520to%2520sequences%2520of%2520datasets%250Acontaining%2520diverse%2520scenes%2520captured%2520from%2520distinct%2520domains%2520using%2520different%2520visual%250Aand%2520range%2520sensors.%2520We%2520adopt%2520representative%2520methods%2520from%2520continual%2520learning%250Aparadigms%2520and%2520translate%2520them%2520to%2520enable%2520unsupervised%2520continual%2520learning%2520of%2520depth%250Acompletion.%2520We%2520benchmark%2520these%2520models%2520for%2520indoor%2520and%2520outdoor%2520and%2520investigate%250Athe%2520degree%2520of%2520catastrophic%2520forgetting%2520through%2520standard%2520quantitative%2520metrics.%250AFurthermore%252C%2520we%2520introduce%2520model%2520inversion%2520quality%2520as%2520an%2520additional%2520measure%2520of%250Aforgetting.%2520We%2520find%2520that%2520unsupervised%2520continual%2520learning%2520of%2520depth%2520completion%2520is%250Aan%2520open%2520problem%252C%2520and%2520we%2520invite%2520researchers%2520to%2520leverage%2520UnCLe%2520as%2520a%2520development%250Aplatform.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18074v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UnCLe%3A%20Unsupervised%20Continual%20Learning%20of%20Depth%20Completion&entry.906535625=Suchisrit%20Gangopadhyay%20and%20Xien%20Chen%20and%20Michael%20Chu%20and%20Patrick%20Rim%20and%20Hyoungseob%20Park%20and%20Alex%20Wong&entry.1292438233=%20%20We%20propose%20UnCLe%2C%20a%20standardized%20benchmark%20for%20Unsupervised%20Continual%0ALearning%20of%20a%20multimodal%20depth%20estimation%20task%3A%20Depth%20completion%20aims%20to%20infer%0Aa%20dense%20depth%20map%20from%20a%20pair%20of%20synchronized%20RGB%20image%20and%20sparse%20depth%20map.%0AWe%20benchmark%20depth%20completion%20models%20under%20the%20practical%20scenario%20of%0Aunsupervised%20learning%20over%20continuous%20streams%20of%20data.%20Existing%20methods%20are%0Atypically%20trained%20on%20a%20static%2C%20or%20stationary%2C%20dataset.%20However%2C%20when%20adapting%0Ato%20novel%20non-stationary%20distributions%2C%20they%20%22catastrophically%20forget%22%0Apreviously%20learned%20information.%20UnCLe%20simulates%20these%20non-stationary%0Adistributions%20by%20adapting%20depth%20completion%20models%20to%20sequences%20of%20datasets%0Acontaining%20diverse%20scenes%20captured%20from%20distinct%20domains%20using%20different%20visual%0Aand%20range%20sensors.%20We%20adopt%20representative%20methods%20from%20continual%20learning%0Aparadigms%20and%20translate%20them%20to%20enable%20unsupervised%20continual%20learning%20of%20depth%0Acompletion.%20We%20benchmark%20these%20models%20for%20indoor%20and%20outdoor%20and%20investigate%0Athe%20degree%20of%20catastrophic%20forgetting%20through%20standard%20quantitative%20metrics.%0AFurthermore%2C%20we%20introduce%20model%20inversion%20quality%20as%20an%20additional%20measure%20of%0Aforgetting.%20We%20find%20that%20unsupervised%20continual%20learning%20of%20depth%20completion%20is%0Aan%20open%20problem%2C%20and%20we%20invite%20researchers%20to%20leverage%20UnCLe%20as%20a%20development%0Aplatform.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18074v1&entry.124074799=Read"},
{"title": "Exploring Stronger Transformer Representation Learning for Occluded\n  Person Re-Identification", "author": "Zhangjian Ji and Donglin Cheng and Kai Feng", "abstract": "  Due to some complex factors (e.g., occlusion, pose variation and diverse\ncamera perspectives), extracting stronger feature representation in person\nre-identification remains a challenging task. In this paper, we proposed a\nnovel self-supervision and supervision combining transformer-based person\nre-identification framework, namely SSSC-TransReID. Different from the general\ntransformer-based person re-identification models, we designed a\nself-supervised contrastive learning branch, which can enhance the feature\nrepresentation for person re-identification without negative samples or\nadditional pre-training. In order to train the contrastive learning branch, we\nalso proposed a novel random rectangle mask strategy to simulate the occlusion\nin real scenes, so as to enhance the feature representation for occlusion.\nFinally, we utilized the joint-training loss function to integrate the\nadvantages of supervised learning with ID tags and self-supervised contrastive\nlearning without negative samples, which can reinforce the ability of our model\nto excavate stronger discriminative features, especially for occlusion.\nExtensive experimental results on several benchmark datasets show our proposed\nmodel obtains superior Re-ID performance consistently and outperforms the\nstate-of-the-art ReID methods by large margins on the mean average accuracy\n(mAP) and Rank-1 accuracy.\n", "link": "http://arxiv.org/abs/2410.15613v2", "date": "2024-10-23", "relevancy": 2.2195, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5664}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5467}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Stronger%20Transformer%20Representation%20Learning%20for%20Occluded%0A%20%20Person%20Re-Identification&body=Title%3A%20Exploring%20Stronger%20Transformer%20Representation%20Learning%20for%20Occluded%0A%20%20Person%20Re-Identification%0AAuthor%3A%20Zhangjian%20Ji%20and%20Donglin%20Cheng%20and%20Kai%20Feng%0AAbstract%3A%20%20%20Due%20to%20some%20complex%20factors%20%28e.g.%2C%20occlusion%2C%20pose%20variation%20and%20diverse%0Acamera%20perspectives%29%2C%20extracting%20stronger%20feature%20representation%20in%20person%0Are-identification%20remains%20a%20challenging%20task.%20In%20this%20paper%2C%20we%20proposed%20a%0Anovel%20self-supervision%20and%20supervision%20combining%20transformer-based%20person%0Are-identification%20framework%2C%20namely%20SSSC-TransReID.%20Different%20from%20the%20general%0Atransformer-based%20person%20re-identification%20models%2C%20we%20designed%20a%0Aself-supervised%20contrastive%20learning%20branch%2C%20which%20can%20enhance%20the%20feature%0Arepresentation%20for%20person%20re-identification%20without%20negative%20samples%20or%0Aadditional%20pre-training.%20In%20order%20to%20train%20the%20contrastive%20learning%20branch%2C%20we%0Aalso%20proposed%20a%20novel%20random%20rectangle%20mask%20strategy%20to%20simulate%20the%20occlusion%0Ain%20real%20scenes%2C%20so%20as%20to%20enhance%20the%20feature%20representation%20for%20occlusion.%0AFinally%2C%20we%20utilized%20the%20joint-training%20loss%20function%20to%20integrate%20the%0Aadvantages%20of%20supervised%20learning%20with%20ID%20tags%20and%20self-supervised%20contrastive%0Alearning%20without%20negative%20samples%2C%20which%20can%20reinforce%20the%20ability%20of%20our%20model%0Ato%20excavate%20stronger%20discriminative%20features%2C%20especially%20for%20occlusion.%0AExtensive%20experimental%20results%20on%20several%20benchmark%20datasets%20show%20our%20proposed%0Amodel%20obtains%20superior%20Re-ID%20performance%20consistently%20and%20outperforms%20the%0Astate-of-the-art%20ReID%20methods%20by%20large%20margins%20on%20the%20mean%20average%20accuracy%0A%28mAP%29%20and%20Rank-1%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.15613v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Stronger%2520Transformer%2520Representation%2520Learning%2520for%2520Occluded%250A%2520%2520Person%2520Re-Identification%26entry.906535625%3DZhangjian%2520Ji%2520and%2520Donglin%2520Cheng%2520and%2520Kai%2520Feng%26entry.1292438233%3D%2520%2520Due%2520to%2520some%2520complex%2520factors%2520%2528e.g.%252C%2520occlusion%252C%2520pose%2520variation%2520and%2520diverse%250Acamera%2520perspectives%2529%252C%2520extracting%2520stronger%2520feature%2520representation%2520in%2520person%250Are-identification%2520remains%2520a%2520challenging%2520task.%2520In%2520this%2520paper%252C%2520we%2520proposed%2520a%250Anovel%2520self-supervision%2520and%2520supervision%2520combining%2520transformer-based%2520person%250Are-identification%2520framework%252C%2520namely%2520SSSC-TransReID.%2520Different%2520from%2520the%2520general%250Atransformer-based%2520person%2520re-identification%2520models%252C%2520we%2520designed%2520a%250Aself-supervised%2520contrastive%2520learning%2520branch%252C%2520which%2520can%2520enhance%2520the%2520feature%250Arepresentation%2520for%2520person%2520re-identification%2520without%2520negative%2520samples%2520or%250Aadditional%2520pre-training.%2520In%2520order%2520to%2520train%2520the%2520contrastive%2520learning%2520branch%252C%2520we%250Aalso%2520proposed%2520a%2520novel%2520random%2520rectangle%2520mask%2520strategy%2520to%2520simulate%2520the%2520occlusion%250Ain%2520real%2520scenes%252C%2520so%2520as%2520to%2520enhance%2520the%2520feature%2520representation%2520for%2520occlusion.%250AFinally%252C%2520we%2520utilized%2520the%2520joint-training%2520loss%2520function%2520to%2520integrate%2520the%250Aadvantages%2520of%2520supervised%2520learning%2520with%2520ID%2520tags%2520and%2520self-supervised%2520contrastive%250Alearning%2520without%2520negative%2520samples%252C%2520which%2520can%2520reinforce%2520the%2520ability%2520of%2520our%2520model%250Ato%2520excavate%2520stronger%2520discriminative%2520features%252C%2520especially%2520for%2520occlusion.%250AExtensive%2520experimental%2520results%2520on%2520several%2520benchmark%2520datasets%2520show%2520our%2520proposed%250Amodel%2520obtains%2520superior%2520Re-ID%2520performance%2520consistently%2520and%2520outperforms%2520the%250Astate-of-the-art%2520ReID%2520methods%2520by%2520large%2520margins%2520on%2520the%2520mean%2520average%2520accuracy%250A%2528mAP%2529%2520and%2520Rank-1%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15613v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Stronger%20Transformer%20Representation%20Learning%20for%20Occluded%0A%20%20Person%20Re-Identification&entry.906535625=Zhangjian%20Ji%20and%20Donglin%20Cheng%20and%20Kai%20Feng&entry.1292438233=%20%20Due%20to%20some%20complex%20factors%20%28e.g.%2C%20occlusion%2C%20pose%20variation%20and%20diverse%0Acamera%20perspectives%29%2C%20extracting%20stronger%20feature%20representation%20in%20person%0Are-identification%20remains%20a%20challenging%20task.%20In%20this%20paper%2C%20we%20proposed%20a%0Anovel%20self-supervision%20and%20supervision%20combining%20transformer-based%20person%0Are-identification%20framework%2C%20namely%20SSSC-TransReID.%20Different%20from%20the%20general%0Atransformer-based%20person%20re-identification%20models%2C%20we%20designed%20a%0Aself-supervised%20contrastive%20learning%20branch%2C%20which%20can%20enhance%20the%20feature%0Arepresentation%20for%20person%20re-identification%20without%20negative%20samples%20or%0Aadditional%20pre-training.%20In%20order%20to%20train%20the%20contrastive%20learning%20branch%2C%20we%0Aalso%20proposed%20a%20novel%20random%20rectangle%20mask%20strategy%20to%20simulate%20the%20occlusion%0Ain%20real%20scenes%2C%20so%20as%20to%20enhance%20the%20feature%20representation%20for%20occlusion.%0AFinally%2C%20we%20utilized%20the%20joint-training%20loss%20function%20to%20integrate%20the%0Aadvantages%20of%20supervised%20learning%20with%20ID%20tags%20and%20self-supervised%20contrastive%0Alearning%20without%20negative%20samples%2C%20which%20can%20reinforce%20the%20ability%20of%20our%20model%0Ato%20excavate%20stronger%20discriminative%20features%2C%20especially%20for%20occlusion.%0AExtensive%20experimental%20results%20on%20several%20benchmark%20datasets%20show%20our%20proposed%0Amodel%20obtains%20superior%20Re-ID%20performance%20consistently%20and%20outperforms%20the%0Astate-of-the-art%20ReID%20methods%20by%20large%20margins%20on%20the%20mean%20average%20accuracy%0A%28mAP%29%20and%20Rank-1%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.15613v2&entry.124074799=Read"},
{"title": "Accessible, At-Home Detection of Parkinson's Disease via Multi-task\n  Video Analysis", "author": "Md Saiful Islam and Tariq Adnan and Jan Freyberg and Sangwu Lee and Abdelrahman Abdelkader and Meghan Pawlik and Cathe Schwartz and Karen Jaffe and Ruth B. Schneider and E Ray Dorsey and Ehsan Hoque", "abstract": "  Limited accessibility to neurological care leads to underdiagnosed\nParkinson's Disease (PD), preventing early intervention. Existing AI-based PD\ndetection methods primarily focus on unimodal analysis of motor or speech\ntasks, overlooking the multifaceted nature of the disease. To address this, we\nintroduce a large-scale, multi-task video dataset consisting of 1102 sessions\n(each containing videos of finger tapping, facial expression, and speech tasks\ncaptured via webcam) from 845 participants (272 with PD). We propose a novel\nUncertainty-calibrated Fusion Network (UFNet) that leverages this multimodal\ndata to enhance diagnostic accuracy. UFNet employs independent task-specific\nnetworks, trained with Monte Carlo Dropout for uncertainty quantification,\nfollowed by self-attended fusion of features, with attention weights\ndynamically adjusted based on task-specific uncertainties. To ensure\npatient-centered evaluation, the participants were randomly split into three\nsets: 60% for training, 20% for model selection, and 20% for final performance\nevaluation. UFNet significantly outperformed single-task models in terms of\naccuracy, area under the ROC curve (AUROC), and sensitivity while maintaining\nnon-inferior specificity. Withholding uncertain predictions further boosted the\nperformance, achieving 88.0+-0.3%$ accuracy, 93.0+-0.2% AUROC, 79.3+-0.9%\nsensitivity, and 92.6+-0.3% specificity, at the expense of not being able to\npredict for 2.3+-0.3% data (+- denotes 95% confidence interval). Further\nanalysis suggests that the trained model does not exhibit any detectable bias\nacross sex and ethnic subgroups and is most effective for individuals aged\nbetween 50 and 80. Requiring only a webcam and microphone, our approach\nfacilitates accessible home-based PD screening, especially in regions with\nlimited healthcare resources.\n", "link": "http://arxiv.org/abs/2406.14856v2", "date": "2024-10-23", "relevancy": 2.2167, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5695}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.56}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accessible%2C%20At-Home%20Detection%20of%20Parkinson%27s%20Disease%20via%20Multi-task%0A%20%20Video%20Analysis&body=Title%3A%20Accessible%2C%20At-Home%20Detection%20of%20Parkinson%27s%20Disease%20via%20Multi-task%0A%20%20Video%20Analysis%0AAuthor%3A%20Md%20Saiful%20Islam%20and%20Tariq%20Adnan%20and%20Jan%20Freyberg%20and%20Sangwu%20Lee%20and%20Abdelrahman%20Abdelkader%20and%20Meghan%20Pawlik%20and%20Cathe%20Schwartz%20and%20Karen%20Jaffe%20and%20Ruth%20B.%20Schneider%20and%20E%20Ray%20Dorsey%20and%20Ehsan%20Hoque%0AAbstract%3A%20%20%20Limited%20accessibility%20to%20neurological%20care%20leads%20to%20underdiagnosed%0AParkinson%27s%20Disease%20%28PD%29%2C%20preventing%20early%20intervention.%20Existing%20AI-based%20PD%0Adetection%20methods%20primarily%20focus%20on%20unimodal%20analysis%20of%20motor%20or%20speech%0Atasks%2C%20overlooking%20the%20multifaceted%20nature%20of%20the%20disease.%20To%20address%20this%2C%20we%0Aintroduce%20a%20large-scale%2C%20multi-task%20video%20dataset%20consisting%20of%201102%20sessions%0A%28each%20containing%20videos%20of%20finger%20tapping%2C%20facial%20expression%2C%20and%20speech%20tasks%0Acaptured%20via%20webcam%29%20from%20845%20participants%20%28272%20with%20PD%29.%20We%20propose%20a%20novel%0AUncertainty-calibrated%20Fusion%20Network%20%28UFNet%29%20that%20leverages%20this%20multimodal%0Adata%20to%20enhance%20diagnostic%20accuracy.%20UFNet%20employs%20independent%20task-specific%0Anetworks%2C%20trained%20with%20Monte%20Carlo%20Dropout%20for%20uncertainty%20quantification%2C%0Afollowed%20by%20self-attended%20fusion%20of%20features%2C%20with%20attention%20weights%0Adynamically%20adjusted%20based%20on%20task-specific%20uncertainties.%20To%20ensure%0Apatient-centered%20evaluation%2C%20the%20participants%20were%20randomly%20split%20into%20three%0Asets%3A%2060%25%20for%20training%2C%2020%25%20for%20model%20selection%2C%20and%2020%25%20for%20final%20performance%0Aevaluation.%20UFNet%20significantly%20outperformed%20single-task%20models%20in%20terms%20of%0Aaccuracy%2C%20area%20under%20the%20ROC%20curve%20%28AUROC%29%2C%20and%20sensitivity%20while%20maintaining%0Anon-inferior%20specificity.%20Withholding%20uncertain%20predictions%20further%20boosted%20the%0Aperformance%2C%20achieving%2088.0%2B-0.3%25%24%20accuracy%2C%2093.0%2B-0.2%25%20AUROC%2C%2079.3%2B-0.9%25%0Asensitivity%2C%20and%2092.6%2B-0.3%25%20specificity%2C%20at%20the%20expense%20of%20not%20being%20able%20to%0Apredict%20for%202.3%2B-0.3%25%20data%20%28%2B-%20denotes%2095%25%20confidence%20interval%29.%20Further%0Aanalysis%20suggests%20that%20the%20trained%20model%20does%20not%20exhibit%20any%20detectable%20bias%0Aacross%20sex%20and%20ethnic%20subgroups%20and%20is%20most%20effective%20for%20individuals%20aged%0Abetween%2050%20and%2080.%20Requiring%20only%20a%20webcam%20and%20microphone%2C%20our%20approach%0Afacilitates%20accessible%20home-based%20PD%20screening%2C%20especially%20in%20regions%20with%0Alimited%20healthcare%20resources.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14856v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccessible%252C%2520At-Home%2520Detection%2520of%2520Parkinson%2527s%2520Disease%2520via%2520Multi-task%250A%2520%2520Video%2520Analysis%26entry.906535625%3DMd%2520Saiful%2520Islam%2520and%2520Tariq%2520Adnan%2520and%2520Jan%2520Freyberg%2520and%2520Sangwu%2520Lee%2520and%2520Abdelrahman%2520Abdelkader%2520and%2520Meghan%2520Pawlik%2520and%2520Cathe%2520Schwartz%2520and%2520Karen%2520Jaffe%2520and%2520Ruth%2520B.%2520Schneider%2520and%2520E%2520Ray%2520Dorsey%2520and%2520Ehsan%2520Hoque%26entry.1292438233%3D%2520%2520Limited%2520accessibility%2520to%2520neurological%2520care%2520leads%2520to%2520underdiagnosed%250AParkinson%2527s%2520Disease%2520%2528PD%2529%252C%2520preventing%2520early%2520intervention.%2520Existing%2520AI-based%2520PD%250Adetection%2520methods%2520primarily%2520focus%2520on%2520unimodal%2520analysis%2520of%2520motor%2520or%2520speech%250Atasks%252C%2520overlooking%2520the%2520multifaceted%2520nature%2520of%2520the%2520disease.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520a%2520large-scale%252C%2520multi-task%2520video%2520dataset%2520consisting%2520of%25201102%2520sessions%250A%2528each%2520containing%2520videos%2520of%2520finger%2520tapping%252C%2520facial%2520expression%252C%2520and%2520speech%2520tasks%250Acaptured%2520via%2520webcam%2529%2520from%2520845%2520participants%2520%2528272%2520with%2520PD%2529.%2520We%2520propose%2520a%2520novel%250AUncertainty-calibrated%2520Fusion%2520Network%2520%2528UFNet%2529%2520that%2520leverages%2520this%2520multimodal%250Adata%2520to%2520enhance%2520diagnostic%2520accuracy.%2520UFNet%2520employs%2520independent%2520task-specific%250Anetworks%252C%2520trained%2520with%2520Monte%2520Carlo%2520Dropout%2520for%2520uncertainty%2520quantification%252C%250Afollowed%2520by%2520self-attended%2520fusion%2520of%2520features%252C%2520with%2520attention%2520weights%250Adynamically%2520adjusted%2520based%2520on%2520task-specific%2520uncertainties.%2520To%2520ensure%250Apatient-centered%2520evaluation%252C%2520the%2520participants%2520were%2520randomly%2520split%2520into%2520three%250Asets%253A%252060%2525%2520for%2520training%252C%252020%2525%2520for%2520model%2520selection%252C%2520and%252020%2525%2520for%2520final%2520performance%250Aevaluation.%2520UFNet%2520significantly%2520outperformed%2520single-task%2520models%2520in%2520terms%2520of%250Aaccuracy%252C%2520area%2520under%2520the%2520ROC%2520curve%2520%2528AUROC%2529%252C%2520and%2520sensitivity%2520while%2520maintaining%250Anon-inferior%2520specificity.%2520Withholding%2520uncertain%2520predictions%2520further%2520boosted%2520the%250Aperformance%252C%2520achieving%252088.0%252B-0.3%2525%2524%2520accuracy%252C%252093.0%252B-0.2%2525%2520AUROC%252C%252079.3%252B-0.9%2525%250Asensitivity%252C%2520and%252092.6%252B-0.3%2525%2520specificity%252C%2520at%2520the%2520expense%2520of%2520not%2520being%2520able%2520to%250Apredict%2520for%25202.3%252B-0.3%2525%2520data%2520%2528%252B-%2520denotes%252095%2525%2520confidence%2520interval%2529.%2520Further%250Aanalysis%2520suggests%2520that%2520the%2520trained%2520model%2520does%2520not%2520exhibit%2520any%2520detectable%2520bias%250Aacross%2520sex%2520and%2520ethnic%2520subgroups%2520and%2520is%2520most%2520effective%2520for%2520individuals%2520aged%250Abetween%252050%2520and%252080.%2520Requiring%2520only%2520a%2520webcam%2520and%2520microphone%252C%2520our%2520approach%250Afacilitates%2520accessible%2520home-based%2520PD%2520screening%252C%2520especially%2520in%2520regions%2520with%250Alimited%2520healthcare%2520resources.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14856v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accessible%2C%20At-Home%20Detection%20of%20Parkinson%27s%20Disease%20via%20Multi-task%0A%20%20Video%20Analysis&entry.906535625=Md%20Saiful%20Islam%20and%20Tariq%20Adnan%20and%20Jan%20Freyberg%20and%20Sangwu%20Lee%20and%20Abdelrahman%20Abdelkader%20and%20Meghan%20Pawlik%20and%20Cathe%20Schwartz%20and%20Karen%20Jaffe%20and%20Ruth%20B.%20Schneider%20and%20E%20Ray%20Dorsey%20and%20Ehsan%20Hoque&entry.1292438233=%20%20Limited%20accessibility%20to%20neurological%20care%20leads%20to%20underdiagnosed%0AParkinson%27s%20Disease%20%28PD%29%2C%20preventing%20early%20intervention.%20Existing%20AI-based%20PD%0Adetection%20methods%20primarily%20focus%20on%20unimodal%20analysis%20of%20motor%20or%20speech%0Atasks%2C%20overlooking%20the%20multifaceted%20nature%20of%20the%20disease.%20To%20address%20this%2C%20we%0Aintroduce%20a%20large-scale%2C%20multi-task%20video%20dataset%20consisting%20of%201102%20sessions%0A%28each%20containing%20videos%20of%20finger%20tapping%2C%20facial%20expression%2C%20and%20speech%20tasks%0Acaptured%20via%20webcam%29%20from%20845%20participants%20%28272%20with%20PD%29.%20We%20propose%20a%20novel%0AUncertainty-calibrated%20Fusion%20Network%20%28UFNet%29%20that%20leverages%20this%20multimodal%0Adata%20to%20enhance%20diagnostic%20accuracy.%20UFNet%20employs%20independent%20task-specific%0Anetworks%2C%20trained%20with%20Monte%20Carlo%20Dropout%20for%20uncertainty%20quantification%2C%0Afollowed%20by%20self-attended%20fusion%20of%20features%2C%20with%20attention%20weights%0Adynamically%20adjusted%20based%20on%20task-specific%20uncertainties.%20To%20ensure%0Apatient-centered%20evaluation%2C%20the%20participants%20were%20randomly%20split%20into%20three%0Asets%3A%2060%25%20for%20training%2C%2020%25%20for%20model%20selection%2C%20and%2020%25%20for%20final%20performance%0Aevaluation.%20UFNet%20significantly%20outperformed%20single-task%20models%20in%20terms%20of%0Aaccuracy%2C%20area%20under%20the%20ROC%20curve%20%28AUROC%29%2C%20and%20sensitivity%20while%20maintaining%0Anon-inferior%20specificity.%20Withholding%20uncertain%20predictions%20further%20boosted%20the%0Aperformance%2C%20achieving%2088.0%2B-0.3%25%24%20accuracy%2C%2093.0%2B-0.2%25%20AUROC%2C%2079.3%2B-0.9%25%0Asensitivity%2C%20and%2092.6%2B-0.3%25%20specificity%2C%20at%20the%20expense%20of%20not%20being%20able%20to%0Apredict%20for%202.3%2B-0.3%25%20data%20%28%2B-%20denotes%2095%25%20confidence%20interval%29.%20Further%0Aanalysis%20suggests%20that%20the%20trained%20model%20does%20not%20exhibit%20any%20detectable%20bias%0Aacross%20sex%20and%20ethnic%20subgroups%20and%20is%20most%20effective%20for%20individuals%20aged%0Abetween%2050%20and%2080.%20Requiring%20only%20a%20webcam%20and%20microphone%2C%20our%20approach%0Afacilitates%20accessible%20home-based%20PD%20screening%2C%20especially%20in%20regions%20with%0Alimited%20healthcare%20resources.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14856v2&entry.124074799=Read"},
{"title": "Robust Two-View Geometry Estimation with Implicit Differentiation", "author": "Vladislav Pyatov and Iaroslav Koshelev and Stamatis Lefkimmiatis", "abstract": "  We present a novel two-view geometry estimation framework which is based on a\ndifferentiable robust loss function fitting. We propose to treat the robust\nfundamental matrix estimation as an implicit layer, which allows us to avoid\nbackpropagation through time and significantly improves the numerical\nstability. To take full advantage of the information from the feature matching\nstage we incorporate learnable weights that depend on the matching confidences.\nIn this way our solution brings together feature extraction, matching and\ntwo-view geometry estimation in a unified end-to-end trainable pipeline. We\nevaluate our approach on the camera pose estimation task in both outdoor and\nindoor scenarios. The experiments on several datasets show that the proposed\nmethod outperforms both classic and learning-based state-of-the-art methods by\na large margin. The project webpage is available at:\nhttps://github.com/VladPyatov/ihls\n", "link": "http://arxiv.org/abs/2410.17983v1", "date": "2024-10-23", "relevancy": 2.2071, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5749}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5359}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Two-View%20Geometry%20Estimation%20with%20Implicit%20Differentiation&body=Title%3A%20Robust%20Two-View%20Geometry%20Estimation%20with%20Implicit%20Differentiation%0AAuthor%3A%20Vladislav%20Pyatov%20and%20Iaroslav%20Koshelev%20and%20Stamatis%20Lefkimmiatis%0AAbstract%3A%20%20%20We%20present%20a%20novel%20two-view%20geometry%20estimation%20framework%20which%20is%20based%20on%20a%0Adifferentiable%20robust%20loss%20function%20fitting.%20We%20propose%20to%20treat%20the%20robust%0Afundamental%20matrix%20estimation%20as%20an%20implicit%20layer%2C%20which%20allows%20us%20to%20avoid%0Abackpropagation%20through%20time%20and%20significantly%20improves%20the%20numerical%0Astability.%20To%20take%20full%20advantage%20of%20the%20information%20from%20the%20feature%20matching%0Astage%20we%20incorporate%20learnable%20weights%20that%20depend%20on%20the%20matching%20confidences.%0AIn%20this%20way%20our%20solution%20brings%20together%20feature%20extraction%2C%20matching%20and%0Atwo-view%20geometry%20estimation%20in%20a%20unified%20end-to-end%20trainable%20pipeline.%20We%0Aevaluate%20our%20approach%20on%20the%20camera%20pose%20estimation%20task%20in%20both%20outdoor%20and%0Aindoor%20scenarios.%20The%20experiments%20on%20several%20datasets%20show%20that%20the%20proposed%0Amethod%20outperforms%20both%20classic%20and%20learning-based%20state-of-the-art%20methods%20by%0Aa%20large%20margin.%20The%20project%20webpage%20is%20available%20at%3A%0Ahttps%3A//github.com/VladPyatov/ihls%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17983v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Two-View%2520Geometry%2520Estimation%2520with%2520Implicit%2520Differentiation%26entry.906535625%3DVladislav%2520Pyatov%2520and%2520Iaroslav%2520Koshelev%2520and%2520Stamatis%2520Lefkimmiatis%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520two-view%2520geometry%2520estimation%2520framework%2520which%2520is%2520based%2520on%2520a%250Adifferentiable%2520robust%2520loss%2520function%2520fitting.%2520We%2520propose%2520to%2520treat%2520the%2520robust%250Afundamental%2520matrix%2520estimation%2520as%2520an%2520implicit%2520layer%252C%2520which%2520allows%2520us%2520to%2520avoid%250Abackpropagation%2520through%2520time%2520and%2520significantly%2520improves%2520the%2520numerical%250Astability.%2520To%2520take%2520full%2520advantage%2520of%2520the%2520information%2520from%2520the%2520feature%2520matching%250Astage%2520we%2520incorporate%2520learnable%2520weights%2520that%2520depend%2520on%2520the%2520matching%2520confidences.%250AIn%2520this%2520way%2520our%2520solution%2520brings%2520together%2520feature%2520extraction%252C%2520matching%2520and%250Atwo-view%2520geometry%2520estimation%2520in%2520a%2520unified%2520end-to-end%2520trainable%2520pipeline.%2520We%250Aevaluate%2520our%2520approach%2520on%2520the%2520camera%2520pose%2520estimation%2520task%2520in%2520both%2520outdoor%2520and%250Aindoor%2520scenarios.%2520The%2520experiments%2520on%2520several%2520datasets%2520show%2520that%2520the%2520proposed%250Amethod%2520outperforms%2520both%2520classic%2520and%2520learning-based%2520state-of-the-art%2520methods%2520by%250Aa%2520large%2520margin.%2520The%2520project%2520webpage%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/VladPyatov/ihls%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17983v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Two-View%20Geometry%20Estimation%20with%20Implicit%20Differentiation&entry.906535625=Vladislav%20Pyatov%20and%20Iaroslav%20Koshelev%20and%20Stamatis%20Lefkimmiatis&entry.1292438233=%20%20We%20present%20a%20novel%20two-view%20geometry%20estimation%20framework%20which%20is%20based%20on%20a%0Adifferentiable%20robust%20loss%20function%20fitting.%20We%20propose%20to%20treat%20the%20robust%0Afundamental%20matrix%20estimation%20as%20an%20implicit%20layer%2C%20which%20allows%20us%20to%20avoid%0Abackpropagation%20through%20time%20and%20significantly%20improves%20the%20numerical%0Astability.%20To%20take%20full%20advantage%20of%20the%20information%20from%20the%20feature%20matching%0Astage%20we%20incorporate%20learnable%20weights%20that%20depend%20on%20the%20matching%20confidences.%0AIn%20this%20way%20our%20solution%20brings%20together%20feature%20extraction%2C%20matching%20and%0Atwo-view%20geometry%20estimation%20in%20a%20unified%20end-to-end%20trainable%20pipeline.%20We%0Aevaluate%20our%20approach%20on%20the%20camera%20pose%20estimation%20task%20in%20both%20outdoor%20and%0Aindoor%20scenarios.%20The%20experiments%20on%20several%20datasets%20show%20that%20the%20proposed%0Amethod%20outperforms%20both%20classic%20and%20learning-based%20state-of-the-art%20methods%20by%0Aa%20large%20margin.%20The%20project%20webpage%20is%20available%20at%3A%0Ahttps%3A//github.com/VladPyatov/ihls%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17983v1&entry.124074799=Read"},
{"title": "EntityCLIP: Entity-Centric Image-Text Matching via Multimodal Attentive\n  Contrastive Learning", "author": "Yaxiong Wang and Yaxiong Wang and Lianwei Wu and Lechao Cheng and Zhun Zhong and Meng Wang", "abstract": "  Recent advancements in image-text matching have been notable, yet prevailing\nmodels predominantly cater to broad queries and struggle with accommodating\nfine-grained query intention. In this paper, we work towards the\n\\textbf{E}ntity-centric \\textbf{I}mage-\\textbf{T}ext \\textbf{M}atching (EITM),\na task that the text and image involve specific entity-related information. The\nchallenge of this task mainly lies in the larger semantic gap in entity\nassociation modeling, comparing with the general image-text matching problem.To\nnarrow the huge semantic gap between the entity-centric text and the images, we\ntake the fundamental CLIP as the backbone and devise a multimodal attentive\ncontrastive learning framework to tam CLIP to adapt EITM problem, developing a\nmodel named EntityCLIP. The key of our multimodal attentive contrastive\nlearning is to generate interpretive explanation text using Large Language\nModels (LLMs) as the bridge clues. In specific, we proceed by extracting\nexplanatory text from off-the-shelf LLMs. This explanation text, coupled with\nthe image and text, is then input into our specially crafted Multimodal\nAttentive Experts (MMAE) module, which effectively integrates explanation texts\nto narrow the gap of the entity-related text and image in a shared semantic\nspace. Building on the enriched features derived from MMAE, we further design\nan effective Gated Integrative Image-text Matching (GI-ITM) strategy. The\nGI-ITM employs an adaptive gating mechanism to aggregate MMAE's features,\nsubsequently applying image-text matching constraints to steer the alignment\nbetween the text and the image. Extensive experiments are conducted on three\nsocial media news benchmarks including N24News, VisualNews, and GoodNews, the\nresults shows that our method surpasses the competition methods with a clear\nmargin.\n", "link": "http://arxiv.org/abs/2410.17810v1", "date": "2024-10-23", "relevancy": 2.1993, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5916}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5203}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EntityCLIP%3A%20Entity-Centric%20Image-Text%20Matching%20via%20Multimodal%20Attentive%0A%20%20Contrastive%20Learning&body=Title%3A%20EntityCLIP%3A%20Entity-Centric%20Image-Text%20Matching%20via%20Multimodal%20Attentive%0A%20%20Contrastive%20Learning%0AAuthor%3A%20Yaxiong%20Wang%20and%20Yaxiong%20Wang%20and%20Lianwei%20Wu%20and%20Lechao%20Cheng%20and%20Zhun%20Zhong%20and%20Meng%20Wang%0AAbstract%3A%20%20%20Recent%20advancements%20in%20image-text%20matching%20have%20been%20notable%2C%20yet%20prevailing%0Amodels%20predominantly%20cater%20to%20broad%20queries%20and%20struggle%20with%20accommodating%0Afine-grained%20query%20intention.%20In%20this%20paper%2C%20we%20work%20towards%20the%0A%5Ctextbf%7BE%7Dntity-centric%20%5Ctextbf%7BI%7Dmage-%5Ctextbf%7BT%7Dext%20%5Ctextbf%7BM%7Datching%20%28EITM%29%2C%0Aa%20task%20that%20the%20text%20and%20image%20involve%20specific%20entity-related%20information.%20The%0Achallenge%20of%20this%20task%20mainly%20lies%20in%20the%20larger%20semantic%20gap%20in%20entity%0Aassociation%20modeling%2C%20comparing%20with%20the%20general%20image-text%20matching%20problem.To%0Anarrow%20the%20huge%20semantic%20gap%20between%20the%20entity-centric%20text%20and%20the%20images%2C%20we%0Atake%20the%20fundamental%20CLIP%20as%20the%20backbone%20and%20devise%20a%20multimodal%20attentive%0Acontrastive%20learning%20framework%20to%20tam%20CLIP%20to%20adapt%20EITM%20problem%2C%20developing%20a%0Amodel%20named%20EntityCLIP.%20The%20key%20of%20our%20multimodal%20attentive%20contrastive%0Alearning%20is%20to%20generate%20interpretive%20explanation%20text%20using%20Large%20Language%0AModels%20%28LLMs%29%20as%20the%20bridge%20clues.%20In%20specific%2C%20we%20proceed%20by%20extracting%0Aexplanatory%20text%20from%20off-the-shelf%20LLMs.%20This%20explanation%20text%2C%20coupled%20with%0Athe%20image%20and%20text%2C%20is%20then%20input%20into%20our%20specially%20crafted%20Multimodal%0AAttentive%20Experts%20%28MMAE%29%20module%2C%20which%20effectively%20integrates%20explanation%20texts%0Ato%20narrow%20the%20gap%20of%20the%20entity-related%20text%20and%20image%20in%20a%20shared%20semantic%0Aspace.%20Building%20on%20the%20enriched%20features%20derived%20from%20MMAE%2C%20we%20further%20design%0Aan%20effective%20Gated%20Integrative%20Image-text%20Matching%20%28GI-ITM%29%20strategy.%20The%0AGI-ITM%20employs%20an%20adaptive%20gating%20mechanism%20to%20aggregate%20MMAE%27s%20features%2C%0Asubsequently%20applying%20image-text%20matching%20constraints%20to%20steer%20the%20alignment%0Abetween%20the%20text%20and%20the%20image.%20Extensive%20experiments%20are%20conducted%20on%20three%0Asocial%20media%20news%20benchmarks%20including%20N24News%2C%20VisualNews%2C%20and%20GoodNews%2C%20the%0Aresults%20shows%20that%20our%20method%20surpasses%20the%20competition%20methods%20with%20a%20clear%0Amargin.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17810v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEntityCLIP%253A%2520Entity-Centric%2520Image-Text%2520Matching%2520via%2520Multimodal%2520Attentive%250A%2520%2520Contrastive%2520Learning%26entry.906535625%3DYaxiong%2520Wang%2520and%2520Yaxiong%2520Wang%2520and%2520Lianwei%2520Wu%2520and%2520Lechao%2520Cheng%2520and%2520Zhun%2520Zhong%2520and%2520Meng%2520Wang%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520image-text%2520matching%2520have%2520been%2520notable%252C%2520yet%2520prevailing%250Amodels%2520predominantly%2520cater%2520to%2520broad%2520queries%2520and%2520struggle%2520with%2520accommodating%250Afine-grained%2520query%2520intention.%2520In%2520this%2520paper%252C%2520we%2520work%2520towards%2520the%250A%255Ctextbf%257BE%257Dntity-centric%2520%255Ctextbf%257BI%257Dmage-%255Ctextbf%257BT%257Dext%2520%255Ctextbf%257BM%257Datching%2520%2528EITM%2529%252C%250Aa%2520task%2520that%2520the%2520text%2520and%2520image%2520involve%2520specific%2520entity-related%2520information.%2520The%250Achallenge%2520of%2520this%2520task%2520mainly%2520lies%2520in%2520the%2520larger%2520semantic%2520gap%2520in%2520entity%250Aassociation%2520modeling%252C%2520comparing%2520with%2520the%2520general%2520image-text%2520matching%2520problem.To%250Anarrow%2520the%2520huge%2520semantic%2520gap%2520between%2520the%2520entity-centric%2520text%2520and%2520the%2520images%252C%2520we%250Atake%2520the%2520fundamental%2520CLIP%2520as%2520the%2520backbone%2520and%2520devise%2520a%2520multimodal%2520attentive%250Acontrastive%2520learning%2520framework%2520to%2520tam%2520CLIP%2520to%2520adapt%2520EITM%2520problem%252C%2520developing%2520a%250Amodel%2520named%2520EntityCLIP.%2520The%2520key%2520of%2520our%2520multimodal%2520attentive%2520contrastive%250Alearning%2520is%2520to%2520generate%2520interpretive%2520explanation%2520text%2520using%2520Large%2520Language%250AModels%2520%2528LLMs%2529%2520as%2520the%2520bridge%2520clues.%2520In%2520specific%252C%2520we%2520proceed%2520by%2520extracting%250Aexplanatory%2520text%2520from%2520off-the-shelf%2520LLMs.%2520This%2520explanation%2520text%252C%2520coupled%2520with%250Athe%2520image%2520and%2520text%252C%2520is%2520then%2520input%2520into%2520our%2520specially%2520crafted%2520Multimodal%250AAttentive%2520Experts%2520%2528MMAE%2529%2520module%252C%2520which%2520effectively%2520integrates%2520explanation%2520texts%250Ato%2520narrow%2520the%2520gap%2520of%2520the%2520entity-related%2520text%2520and%2520image%2520in%2520a%2520shared%2520semantic%250Aspace.%2520Building%2520on%2520the%2520enriched%2520features%2520derived%2520from%2520MMAE%252C%2520we%2520further%2520design%250Aan%2520effective%2520Gated%2520Integrative%2520Image-text%2520Matching%2520%2528GI-ITM%2529%2520strategy.%2520The%250AGI-ITM%2520employs%2520an%2520adaptive%2520gating%2520mechanism%2520to%2520aggregate%2520MMAE%2527s%2520features%252C%250Asubsequently%2520applying%2520image-text%2520matching%2520constraints%2520to%2520steer%2520the%2520alignment%250Abetween%2520the%2520text%2520and%2520the%2520image.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520three%250Asocial%2520media%2520news%2520benchmarks%2520including%2520N24News%252C%2520VisualNews%252C%2520and%2520GoodNews%252C%2520the%250Aresults%2520shows%2520that%2520our%2520method%2520surpasses%2520the%2520competition%2520methods%2520with%2520a%2520clear%250Amargin.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17810v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EntityCLIP%3A%20Entity-Centric%20Image-Text%20Matching%20via%20Multimodal%20Attentive%0A%20%20Contrastive%20Learning&entry.906535625=Yaxiong%20Wang%20and%20Yaxiong%20Wang%20and%20Lianwei%20Wu%20and%20Lechao%20Cheng%20and%20Zhun%20Zhong%20and%20Meng%20Wang&entry.1292438233=%20%20Recent%20advancements%20in%20image-text%20matching%20have%20been%20notable%2C%20yet%20prevailing%0Amodels%20predominantly%20cater%20to%20broad%20queries%20and%20struggle%20with%20accommodating%0Afine-grained%20query%20intention.%20In%20this%20paper%2C%20we%20work%20towards%20the%0A%5Ctextbf%7BE%7Dntity-centric%20%5Ctextbf%7BI%7Dmage-%5Ctextbf%7BT%7Dext%20%5Ctextbf%7BM%7Datching%20%28EITM%29%2C%0Aa%20task%20that%20the%20text%20and%20image%20involve%20specific%20entity-related%20information.%20The%0Achallenge%20of%20this%20task%20mainly%20lies%20in%20the%20larger%20semantic%20gap%20in%20entity%0Aassociation%20modeling%2C%20comparing%20with%20the%20general%20image-text%20matching%20problem.To%0Anarrow%20the%20huge%20semantic%20gap%20between%20the%20entity-centric%20text%20and%20the%20images%2C%20we%0Atake%20the%20fundamental%20CLIP%20as%20the%20backbone%20and%20devise%20a%20multimodal%20attentive%0Acontrastive%20learning%20framework%20to%20tam%20CLIP%20to%20adapt%20EITM%20problem%2C%20developing%20a%0Amodel%20named%20EntityCLIP.%20The%20key%20of%20our%20multimodal%20attentive%20contrastive%0Alearning%20is%20to%20generate%20interpretive%20explanation%20text%20using%20Large%20Language%0AModels%20%28LLMs%29%20as%20the%20bridge%20clues.%20In%20specific%2C%20we%20proceed%20by%20extracting%0Aexplanatory%20text%20from%20off-the-shelf%20LLMs.%20This%20explanation%20text%2C%20coupled%20with%0Athe%20image%20and%20text%2C%20is%20then%20input%20into%20our%20specially%20crafted%20Multimodal%0AAttentive%20Experts%20%28MMAE%29%20module%2C%20which%20effectively%20integrates%20explanation%20texts%0Ato%20narrow%20the%20gap%20of%20the%20entity-related%20text%20and%20image%20in%20a%20shared%20semantic%0Aspace.%20Building%20on%20the%20enriched%20features%20derived%20from%20MMAE%2C%20we%20further%20design%0Aan%20effective%20Gated%20Integrative%20Image-text%20Matching%20%28GI-ITM%29%20strategy.%20The%0AGI-ITM%20employs%20an%20adaptive%20gating%20mechanism%20to%20aggregate%20MMAE%27s%20features%2C%0Asubsequently%20applying%20image-text%20matching%20constraints%20to%20steer%20the%20alignment%0Abetween%20the%20text%20and%20the%20image.%20Extensive%20experiments%20are%20conducted%20on%20three%0Asocial%20media%20news%20benchmarks%20including%20N24News%2C%20VisualNews%2C%20and%20GoodNews%2C%20the%0Aresults%20shows%20that%20our%20method%20surpasses%20the%20competition%20methods%20with%20a%20clear%0Amargin.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17810v1&entry.124074799=Read"},
{"title": "DREB-Net: Dual-stream Restoration Embedding Blur-feature Fusion Network\n  for High-mobility UAV Object Detection", "author": "Qingpeng Li and Yuxin Zhang and Leyuan Fang and Yuhan Kang and Shutao Li and Xiao Xiang Zhu", "abstract": "  Object detection algorithms are pivotal components of unmanned aerial vehicle\n(UAV) imaging systems, extensively employed in complex fields. However, images\ncaptured by high-mobility UAVs often suffer from motion blur cases, which\nsignificantly impedes the performance of advanced object detection algorithms.\nTo address these challenges, we propose an innovative object detection\nalgorithm specifically designed for blurry images, named DREB-Net (Dual-stream\nRestoration Embedding Blur-feature Fusion Network). First, DREB-Net addresses\nthe particularities of blurry image object detection problem by incorporating a\nBlurry image Restoration Auxiliary Branch (BRAB) during the training phase.\nSecond, it fuses the extracted shallow features via Multi-level\nAttention-Guided Feature Fusion (MAGFF) module, to extract richer features.\nHere, the MAGFF module comprises local attention modules and global attention\nmodules, which assign different weights to the branches. Then, during the\ninference phase, the deep feature extraction of the BRAB can be removed to\nreduce computational complexity and improve detection speed. In loss function,\na combined loss of MSE and SSIM is added to the BRAB to restore blurry images.\nFinally, DREB-Net introduces Fast Fourier Transform in the early stages of\nfeature extraction, via a Learnable Frequency domain Amplitude Modulation\nModule (LFAMM), to adjust feature amplitude and enhance feature processing\ncapability. Experimental results indicate that DREB-Net can still effectively\nperform object detection tasks under motion blur in captured images, showcasing\nexcellent performance and broad application prospects. Our source code will be\navailable at https://github.com/EEIC-Lab/DREB-Net.git.\n", "link": "http://arxiv.org/abs/2410.17822v1", "date": "2024-10-23", "relevancy": 2.1903, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5497}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5463}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5455}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DREB-Net%3A%20Dual-stream%20Restoration%20Embedding%20Blur-feature%20Fusion%20Network%0A%20%20for%20High-mobility%20UAV%20Object%20Detection&body=Title%3A%20DREB-Net%3A%20Dual-stream%20Restoration%20Embedding%20Blur-feature%20Fusion%20Network%0A%20%20for%20High-mobility%20UAV%20Object%20Detection%0AAuthor%3A%20Qingpeng%20Li%20and%20Yuxin%20Zhang%20and%20Leyuan%20Fang%20and%20Yuhan%20Kang%20and%20Shutao%20Li%20and%20Xiao%20Xiang%20Zhu%0AAbstract%3A%20%20%20Object%20detection%20algorithms%20are%20pivotal%20components%20of%20unmanned%20aerial%20vehicle%0A%28UAV%29%20imaging%20systems%2C%20extensively%20employed%20in%20complex%20fields.%20However%2C%20images%0Acaptured%20by%20high-mobility%20UAVs%20often%20suffer%20from%20motion%20blur%20cases%2C%20which%0Asignificantly%20impedes%20the%20performance%20of%20advanced%20object%20detection%20algorithms.%0ATo%20address%20these%20challenges%2C%20we%20propose%20an%20innovative%20object%20detection%0Aalgorithm%20specifically%20designed%20for%20blurry%20images%2C%20named%20DREB-Net%20%28Dual-stream%0ARestoration%20Embedding%20Blur-feature%20Fusion%20Network%29.%20First%2C%20DREB-Net%20addresses%0Athe%20particularities%20of%20blurry%20image%20object%20detection%20problem%20by%20incorporating%20a%0ABlurry%20image%20Restoration%20Auxiliary%20Branch%20%28BRAB%29%20during%20the%20training%20phase.%0ASecond%2C%20it%20fuses%20the%20extracted%20shallow%20features%20via%20Multi-level%0AAttention-Guided%20Feature%20Fusion%20%28MAGFF%29%20module%2C%20to%20extract%20richer%20features.%0AHere%2C%20the%20MAGFF%20module%20comprises%20local%20attention%20modules%20and%20global%20attention%0Amodules%2C%20which%20assign%20different%20weights%20to%20the%20branches.%20Then%2C%20during%20the%0Ainference%20phase%2C%20the%20deep%20feature%20extraction%20of%20the%20BRAB%20can%20be%20removed%20to%0Areduce%20computational%20complexity%20and%20improve%20detection%20speed.%20In%20loss%20function%2C%0Aa%20combined%20loss%20of%20MSE%20and%20SSIM%20is%20added%20to%20the%20BRAB%20to%20restore%20blurry%20images.%0AFinally%2C%20DREB-Net%20introduces%20Fast%20Fourier%20Transform%20in%20the%20early%20stages%20of%0Afeature%20extraction%2C%20via%20a%20Learnable%20Frequency%20domain%20Amplitude%20Modulation%0AModule%20%28LFAMM%29%2C%20to%20adjust%20feature%20amplitude%20and%20enhance%20feature%20processing%0Acapability.%20Experimental%20results%20indicate%20that%20DREB-Net%20can%20still%20effectively%0Aperform%20object%20detection%20tasks%20under%20motion%20blur%20in%20captured%20images%2C%20showcasing%0Aexcellent%20performance%20and%20broad%20application%20prospects.%20Our%20source%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/EEIC-Lab/DREB-Net.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17822v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDREB-Net%253A%2520Dual-stream%2520Restoration%2520Embedding%2520Blur-feature%2520Fusion%2520Network%250A%2520%2520for%2520High-mobility%2520UAV%2520Object%2520Detection%26entry.906535625%3DQingpeng%2520Li%2520and%2520Yuxin%2520Zhang%2520and%2520Leyuan%2520Fang%2520and%2520Yuhan%2520Kang%2520and%2520Shutao%2520Li%2520and%2520Xiao%2520Xiang%2520Zhu%26entry.1292438233%3D%2520%2520Object%2520detection%2520algorithms%2520are%2520pivotal%2520components%2520of%2520unmanned%2520aerial%2520vehicle%250A%2528UAV%2529%2520imaging%2520systems%252C%2520extensively%2520employed%2520in%2520complex%2520fields.%2520However%252C%2520images%250Acaptured%2520by%2520high-mobility%2520UAVs%2520often%2520suffer%2520from%2520motion%2520blur%2520cases%252C%2520which%250Asignificantly%2520impedes%2520the%2520performance%2520of%2520advanced%2520object%2520detection%2520algorithms.%250ATo%2520address%2520these%2520challenges%252C%2520we%2520propose%2520an%2520innovative%2520object%2520detection%250Aalgorithm%2520specifically%2520designed%2520for%2520blurry%2520images%252C%2520named%2520DREB-Net%2520%2528Dual-stream%250ARestoration%2520Embedding%2520Blur-feature%2520Fusion%2520Network%2529.%2520First%252C%2520DREB-Net%2520addresses%250Athe%2520particularities%2520of%2520blurry%2520image%2520object%2520detection%2520problem%2520by%2520incorporating%2520a%250ABlurry%2520image%2520Restoration%2520Auxiliary%2520Branch%2520%2528BRAB%2529%2520during%2520the%2520training%2520phase.%250ASecond%252C%2520it%2520fuses%2520the%2520extracted%2520shallow%2520features%2520via%2520Multi-level%250AAttention-Guided%2520Feature%2520Fusion%2520%2528MAGFF%2529%2520module%252C%2520to%2520extract%2520richer%2520features.%250AHere%252C%2520the%2520MAGFF%2520module%2520comprises%2520local%2520attention%2520modules%2520and%2520global%2520attention%250Amodules%252C%2520which%2520assign%2520different%2520weights%2520to%2520the%2520branches.%2520Then%252C%2520during%2520the%250Ainference%2520phase%252C%2520the%2520deep%2520feature%2520extraction%2520of%2520the%2520BRAB%2520can%2520be%2520removed%2520to%250Areduce%2520computational%2520complexity%2520and%2520improve%2520detection%2520speed.%2520In%2520loss%2520function%252C%250Aa%2520combined%2520loss%2520of%2520MSE%2520and%2520SSIM%2520is%2520added%2520to%2520the%2520BRAB%2520to%2520restore%2520blurry%2520images.%250AFinally%252C%2520DREB-Net%2520introduces%2520Fast%2520Fourier%2520Transform%2520in%2520the%2520early%2520stages%2520of%250Afeature%2520extraction%252C%2520via%2520a%2520Learnable%2520Frequency%2520domain%2520Amplitude%2520Modulation%250AModule%2520%2528LFAMM%2529%252C%2520to%2520adjust%2520feature%2520amplitude%2520and%2520enhance%2520feature%2520processing%250Acapability.%2520Experimental%2520results%2520indicate%2520that%2520DREB-Net%2520can%2520still%2520effectively%250Aperform%2520object%2520detection%2520tasks%2520under%2520motion%2520blur%2520in%2520captured%2520images%252C%2520showcasing%250Aexcellent%2520performance%2520and%2520broad%2520application%2520prospects.%2520Our%2520source%2520code%2520will%2520be%250Aavailable%2520at%2520https%253A//github.com/EEIC-Lab/DREB-Net.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17822v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DREB-Net%3A%20Dual-stream%20Restoration%20Embedding%20Blur-feature%20Fusion%20Network%0A%20%20for%20High-mobility%20UAV%20Object%20Detection&entry.906535625=Qingpeng%20Li%20and%20Yuxin%20Zhang%20and%20Leyuan%20Fang%20and%20Yuhan%20Kang%20and%20Shutao%20Li%20and%20Xiao%20Xiang%20Zhu&entry.1292438233=%20%20Object%20detection%20algorithms%20are%20pivotal%20components%20of%20unmanned%20aerial%20vehicle%0A%28UAV%29%20imaging%20systems%2C%20extensively%20employed%20in%20complex%20fields.%20However%2C%20images%0Acaptured%20by%20high-mobility%20UAVs%20often%20suffer%20from%20motion%20blur%20cases%2C%20which%0Asignificantly%20impedes%20the%20performance%20of%20advanced%20object%20detection%20algorithms.%0ATo%20address%20these%20challenges%2C%20we%20propose%20an%20innovative%20object%20detection%0Aalgorithm%20specifically%20designed%20for%20blurry%20images%2C%20named%20DREB-Net%20%28Dual-stream%0ARestoration%20Embedding%20Blur-feature%20Fusion%20Network%29.%20First%2C%20DREB-Net%20addresses%0Athe%20particularities%20of%20blurry%20image%20object%20detection%20problem%20by%20incorporating%20a%0ABlurry%20image%20Restoration%20Auxiliary%20Branch%20%28BRAB%29%20during%20the%20training%20phase.%0ASecond%2C%20it%20fuses%20the%20extracted%20shallow%20features%20via%20Multi-level%0AAttention-Guided%20Feature%20Fusion%20%28MAGFF%29%20module%2C%20to%20extract%20richer%20features.%0AHere%2C%20the%20MAGFF%20module%20comprises%20local%20attention%20modules%20and%20global%20attention%0Amodules%2C%20which%20assign%20different%20weights%20to%20the%20branches.%20Then%2C%20during%20the%0Ainference%20phase%2C%20the%20deep%20feature%20extraction%20of%20the%20BRAB%20can%20be%20removed%20to%0Areduce%20computational%20complexity%20and%20improve%20detection%20speed.%20In%20loss%20function%2C%0Aa%20combined%20loss%20of%20MSE%20and%20SSIM%20is%20added%20to%20the%20BRAB%20to%20restore%20blurry%20images.%0AFinally%2C%20DREB-Net%20introduces%20Fast%20Fourier%20Transform%20in%20the%20early%20stages%20of%0Afeature%20extraction%2C%20via%20a%20Learnable%20Frequency%20domain%20Amplitude%20Modulation%0AModule%20%28LFAMM%29%2C%20to%20adjust%20feature%20amplitude%20and%20enhance%20feature%20processing%0Acapability.%20Experimental%20results%20indicate%20that%20DREB-Net%20can%20still%20effectively%0Aperform%20object%20detection%20tasks%20under%20motion%20blur%20in%20captured%20images%2C%20showcasing%0Aexcellent%20performance%20and%20broad%20application%20prospects.%20Our%20source%20code%20will%20be%0Aavailable%20at%20https%3A//github.com/EEIC-Lab/DREB-Net.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17822v1&entry.124074799=Read"},
{"title": "Enhancing Federated Learning Convergence with Dynamic Data Queue and\n  Data Entropy-driven Participant Selection", "author": "Charuka Herath and Xiaolan Liu and Sangarapillai Lambotharan and Yogachandran Rahulamathavan", "abstract": "  Federated Learning (FL) is a decentralized approach for collaborative model\ntraining on edge devices. This distributed method of model training offers\nadvantages in privacy, security, regulatory compliance, and cost-efficiency.\nOur emphasis in this research lies in addressing statistical complexity in FL,\nespecially when the data stored locally across devices is not identically and\nindependently distributed (non-IID). We have observed an accuracy reduction of\nup to approximately 10\\% to 30\\%, particularly in skewed scenarios where each\nedge device trains with only 1 class of data. This reduction is attributed to\nweight divergence, quantified using the Euclidean distance between device-level\nclass distributions and the population distribution, resulting in a bias term\n(\\(\\delta_k\\)). As a solution, we present a method to improve convergence in FL\nby creating a global subset of data on the server and dynamically distributing\nit across devices using a Dynamic Data queue-driven Federated Learning (DDFL).\nNext, we leverage Data Entropy metrics to observe the process during each\ntraining round and enable reasonable device selection for aggregation.\nFurthermore, we provide a convergence analysis of our proposed DDFL to justify\ntheir viability in practical FL scenarios, aiming for better device selection,\na non-sub-optimal global model, and faster convergence. We observe that our\napproach results in a substantial accuracy boost of approximately 5\\% for the\nMNIST dataset, around 18\\% for CIFAR-10, and 20\\% for CIFAR-100 with a 10\\%\nglobal subset of data, outperforming the state-of-the-art (SOTA) aggregation\nalgorithms.\n", "link": "http://arxiv.org/abs/2410.17792v1", "date": "2024-10-23", "relevancy": 2.1888, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5723}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5337}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5274}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20Federated%20Learning%20Convergence%20with%20Dynamic%20Data%20Queue%20and%0A%20%20Data%20Entropy-driven%20Participant%20Selection&body=Title%3A%20Enhancing%20Federated%20Learning%20Convergence%20with%20Dynamic%20Data%20Queue%20and%0A%20%20Data%20Entropy-driven%20Participant%20Selection%0AAuthor%3A%20Charuka%20Herath%20and%20Xiaolan%20Liu%20and%20Sangarapillai%20Lambotharan%20and%20Yogachandran%20Rahulamathavan%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20decentralized%20approach%20for%20collaborative%20model%0Atraining%20on%20edge%20devices.%20This%20distributed%20method%20of%20model%20training%20offers%0Aadvantages%20in%20privacy%2C%20security%2C%20regulatory%20compliance%2C%20and%20cost-efficiency.%0AOur%20emphasis%20in%20this%20research%20lies%20in%20addressing%20statistical%20complexity%20in%20FL%2C%0Aespecially%20when%20the%20data%20stored%20locally%20across%20devices%20is%20not%20identically%20and%0Aindependently%20distributed%20%28non-IID%29.%20We%20have%20observed%20an%20accuracy%20reduction%20of%0Aup%20to%20approximately%2010%5C%25%20to%2030%5C%25%2C%20particularly%20in%20skewed%20scenarios%20where%20each%0Aedge%20device%20trains%20with%20only%201%20class%20of%20data.%20This%20reduction%20is%20attributed%20to%0Aweight%20divergence%2C%20quantified%20using%20the%20Euclidean%20distance%20between%20device-level%0Aclass%20distributions%20and%20the%20population%20distribution%2C%20resulting%20in%20a%20bias%20term%0A%28%5C%28%5Cdelta_k%5C%29%29.%20As%20a%20solution%2C%20we%20present%20a%20method%20to%20improve%20convergence%20in%20FL%0Aby%20creating%20a%20global%20subset%20of%20data%20on%20the%20server%20and%20dynamically%20distributing%0Ait%20across%20devices%20using%20a%20Dynamic%20Data%20queue-driven%20Federated%20Learning%20%28DDFL%29.%0ANext%2C%20we%20leverage%20Data%20Entropy%20metrics%20to%20observe%20the%20process%20during%20each%0Atraining%20round%20and%20enable%20reasonable%20device%20selection%20for%20aggregation.%0AFurthermore%2C%20we%20provide%20a%20convergence%20analysis%20of%20our%20proposed%20DDFL%20to%20justify%0Atheir%20viability%20in%20practical%20FL%20scenarios%2C%20aiming%20for%20better%20device%20selection%2C%0Aa%20non-sub-optimal%20global%20model%2C%20and%20faster%20convergence.%20We%20observe%20that%20our%0Aapproach%20results%20in%20a%20substantial%20accuracy%20boost%20of%20approximately%205%5C%25%20for%20the%0AMNIST%20dataset%2C%20around%2018%5C%25%20for%20CIFAR-10%2C%20and%2020%5C%25%20for%20CIFAR-100%20with%20a%2010%5C%25%0Aglobal%20subset%20of%20data%2C%20outperforming%20the%20state-of-the-art%20%28SOTA%29%20aggregation%0Aalgorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17792v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520Federated%2520Learning%2520Convergence%2520with%2520Dynamic%2520Data%2520Queue%2520and%250A%2520%2520Data%2520Entropy-driven%2520Participant%2520Selection%26entry.906535625%3DCharuka%2520Herath%2520and%2520Xiaolan%2520Liu%2520and%2520Sangarapillai%2520Lambotharan%2520and%2520Yogachandran%2520Rahulamathavan%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520a%2520decentralized%2520approach%2520for%2520collaborative%2520model%250Atraining%2520on%2520edge%2520devices.%2520This%2520distributed%2520method%2520of%2520model%2520training%2520offers%250Aadvantages%2520in%2520privacy%252C%2520security%252C%2520regulatory%2520compliance%252C%2520and%2520cost-efficiency.%250AOur%2520emphasis%2520in%2520this%2520research%2520lies%2520in%2520addressing%2520statistical%2520complexity%2520in%2520FL%252C%250Aespecially%2520when%2520the%2520data%2520stored%2520locally%2520across%2520devices%2520is%2520not%2520identically%2520and%250Aindependently%2520distributed%2520%2528non-IID%2529.%2520We%2520have%2520observed%2520an%2520accuracy%2520reduction%2520of%250Aup%2520to%2520approximately%252010%255C%2525%2520to%252030%255C%2525%252C%2520particularly%2520in%2520skewed%2520scenarios%2520where%2520each%250Aedge%2520device%2520trains%2520with%2520only%25201%2520class%2520of%2520data.%2520This%2520reduction%2520is%2520attributed%2520to%250Aweight%2520divergence%252C%2520quantified%2520using%2520the%2520Euclidean%2520distance%2520between%2520device-level%250Aclass%2520distributions%2520and%2520the%2520population%2520distribution%252C%2520resulting%2520in%2520a%2520bias%2520term%250A%2528%255C%2528%255Cdelta_k%255C%2529%2529.%2520As%2520a%2520solution%252C%2520we%2520present%2520a%2520method%2520to%2520improve%2520convergence%2520in%2520FL%250Aby%2520creating%2520a%2520global%2520subset%2520of%2520data%2520on%2520the%2520server%2520and%2520dynamically%2520distributing%250Ait%2520across%2520devices%2520using%2520a%2520Dynamic%2520Data%2520queue-driven%2520Federated%2520Learning%2520%2528DDFL%2529.%250ANext%252C%2520we%2520leverage%2520Data%2520Entropy%2520metrics%2520to%2520observe%2520the%2520process%2520during%2520each%250Atraining%2520round%2520and%2520enable%2520reasonable%2520device%2520selection%2520for%2520aggregation.%250AFurthermore%252C%2520we%2520provide%2520a%2520convergence%2520analysis%2520of%2520our%2520proposed%2520DDFL%2520to%2520justify%250Atheir%2520viability%2520in%2520practical%2520FL%2520scenarios%252C%2520aiming%2520for%2520better%2520device%2520selection%252C%250Aa%2520non-sub-optimal%2520global%2520model%252C%2520and%2520faster%2520convergence.%2520We%2520observe%2520that%2520our%250Aapproach%2520results%2520in%2520a%2520substantial%2520accuracy%2520boost%2520of%2520approximately%25205%255C%2525%2520for%2520the%250AMNIST%2520dataset%252C%2520around%252018%255C%2525%2520for%2520CIFAR-10%252C%2520and%252020%255C%2525%2520for%2520CIFAR-100%2520with%2520a%252010%255C%2525%250Aglobal%2520subset%2520of%2520data%252C%2520outperforming%2520the%2520state-of-the-art%2520%2528SOTA%2529%2520aggregation%250Aalgorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17792v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20Federated%20Learning%20Convergence%20with%20Dynamic%20Data%20Queue%20and%0A%20%20Data%20Entropy-driven%20Participant%20Selection&entry.906535625=Charuka%20Herath%20and%20Xiaolan%20Liu%20and%20Sangarapillai%20Lambotharan%20and%20Yogachandran%20Rahulamathavan&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20decentralized%20approach%20for%20collaborative%20model%0Atraining%20on%20edge%20devices.%20This%20distributed%20method%20of%20model%20training%20offers%0Aadvantages%20in%20privacy%2C%20security%2C%20regulatory%20compliance%2C%20and%20cost-efficiency.%0AOur%20emphasis%20in%20this%20research%20lies%20in%20addressing%20statistical%20complexity%20in%20FL%2C%0Aespecially%20when%20the%20data%20stored%20locally%20across%20devices%20is%20not%20identically%20and%0Aindependently%20distributed%20%28non-IID%29.%20We%20have%20observed%20an%20accuracy%20reduction%20of%0Aup%20to%20approximately%2010%5C%25%20to%2030%5C%25%2C%20particularly%20in%20skewed%20scenarios%20where%20each%0Aedge%20device%20trains%20with%20only%201%20class%20of%20data.%20This%20reduction%20is%20attributed%20to%0Aweight%20divergence%2C%20quantified%20using%20the%20Euclidean%20distance%20between%20device-level%0Aclass%20distributions%20and%20the%20population%20distribution%2C%20resulting%20in%20a%20bias%20term%0A%28%5C%28%5Cdelta_k%5C%29%29.%20As%20a%20solution%2C%20we%20present%20a%20method%20to%20improve%20convergence%20in%20FL%0Aby%20creating%20a%20global%20subset%20of%20data%20on%20the%20server%20and%20dynamically%20distributing%0Ait%20across%20devices%20using%20a%20Dynamic%20Data%20queue-driven%20Federated%20Learning%20%28DDFL%29.%0ANext%2C%20we%20leverage%20Data%20Entropy%20metrics%20to%20observe%20the%20process%20during%20each%0Atraining%20round%20and%20enable%20reasonable%20device%20selection%20for%20aggregation.%0AFurthermore%2C%20we%20provide%20a%20convergence%20analysis%20of%20our%20proposed%20DDFL%20to%20justify%0Atheir%20viability%20in%20practical%20FL%20scenarios%2C%20aiming%20for%20better%20device%20selection%2C%0Aa%20non-sub-optimal%20global%20model%2C%20and%20faster%20convergence.%20We%20observe%20that%20our%0Aapproach%20results%20in%20a%20substantial%20accuracy%20boost%20of%20approximately%205%5C%25%20for%20the%0AMNIST%20dataset%2C%20around%2018%5C%25%20for%20CIFAR-10%2C%20and%2020%5C%25%20for%20CIFAR-100%20with%20a%2010%5C%25%0Aglobal%20subset%20of%20data%2C%20outperforming%20the%20state-of-the-art%20%28SOTA%29%20aggregation%0Aalgorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17792v1&entry.124074799=Read"},
{"title": "Key Algorithms for Keyphrase Generation: Instruction-Based LLMs for\n  Russian Scientific Keyphrases", "author": "Anna Glazkova and Dmitry Morozov and Timur Garipov", "abstract": "  Keyphrase selection is a challenging task in natural language processing that\nhas a wide range of applications. Adapting existing supervised and unsupervised\nsolutions for the Russian language faces several limitations due to the rich\nmorphology of Russian and the limited number of training datasets available.\nRecent studies conducted on English texts show that large language models\n(LLMs) successfully address the task of generating keyphrases. LLMs allow\nachieving impressive results without task-specific fine-tuning, using text\nprompts instead. In this work, we access the performance of prompt-based\nmethods for generating keyphrases for Russian scientific abstracts. First, we\ncompare the performance of zero-shot and few-shot prompt-based methods,\nfine-tuned models, and unsupervised methods. Then we assess strategies for\nselecting keyphrase examples in a few-shot setting. We present the outcomes of\nhuman evaluation of the generated keyphrases and analyze the strengths and\nweaknesses of the models through expert assessment. Our results suggest that\nprompt-based methods can outperform common baselines even using simple text\nprompts.\n", "link": "http://arxiv.org/abs/2410.18040v1", "date": "2024-10-23", "relevancy": 2.1827, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4423}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4423}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4251}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Key%20Algorithms%20for%20Keyphrase%20Generation%3A%20Instruction-Based%20LLMs%20for%0A%20%20Russian%20Scientific%20Keyphrases&body=Title%3A%20Key%20Algorithms%20for%20Keyphrase%20Generation%3A%20Instruction-Based%20LLMs%20for%0A%20%20Russian%20Scientific%20Keyphrases%0AAuthor%3A%20Anna%20Glazkova%20and%20Dmitry%20Morozov%20and%20Timur%20Garipov%0AAbstract%3A%20%20%20Keyphrase%20selection%20is%20a%20challenging%20task%20in%20natural%20language%20processing%20that%0Ahas%20a%20wide%20range%20of%20applications.%20Adapting%20existing%20supervised%20and%20unsupervised%0Asolutions%20for%20the%20Russian%20language%20faces%20several%20limitations%20due%20to%20the%20rich%0Amorphology%20of%20Russian%20and%20the%20limited%20number%20of%20training%20datasets%20available.%0ARecent%20studies%20conducted%20on%20English%20texts%20show%20that%20large%20language%20models%0A%28LLMs%29%20successfully%20address%20the%20task%20of%20generating%20keyphrases.%20LLMs%20allow%0Aachieving%20impressive%20results%20without%20task-specific%20fine-tuning%2C%20using%20text%0Aprompts%20instead.%20In%20this%20work%2C%20we%20access%20the%20performance%20of%20prompt-based%0Amethods%20for%20generating%20keyphrases%20for%20Russian%20scientific%20abstracts.%20First%2C%20we%0Acompare%20the%20performance%20of%20zero-shot%20and%20few-shot%20prompt-based%20methods%2C%0Afine-tuned%20models%2C%20and%20unsupervised%20methods.%20Then%20we%20assess%20strategies%20for%0Aselecting%20keyphrase%20examples%20in%20a%20few-shot%20setting.%20We%20present%20the%20outcomes%20of%0Ahuman%20evaluation%20of%20the%20generated%20keyphrases%20and%20analyze%20the%20strengths%20and%0Aweaknesses%20of%20the%20models%20through%20expert%20assessment.%20Our%20results%20suggest%20that%0Aprompt-based%20methods%20can%20outperform%20common%20baselines%20even%20using%20simple%20text%0Aprompts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKey%2520Algorithms%2520for%2520Keyphrase%2520Generation%253A%2520Instruction-Based%2520LLMs%2520for%250A%2520%2520Russian%2520Scientific%2520Keyphrases%26entry.906535625%3DAnna%2520Glazkova%2520and%2520Dmitry%2520Morozov%2520and%2520Timur%2520Garipov%26entry.1292438233%3D%2520%2520Keyphrase%2520selection%2520is%2520a%2520challenging%2520task%2520in%2520natural%2520language%2520processing%2520that%250Ahas%2520a%2520wide%2520range%2520of%2520applications.%2520Adapting%2520existing%2520supervised%2520and%2520unsupervised%250Asolutions%2520for%2520the%2520Russian%2520language%2520faces%2520several%2520limitations%2520due%2520to%2520the%2520rich%250Amorphology%2520of%2520Russian%2520and%2520the%2520limited%2520number%2520of%2520training%2520datasets%2520available.%250ARecent%2520studies%2520conducted%2520on%2520English%2520texts%2520show%2520that%2520large%2520language%2520models%250A%2528LLMs%2529%2520successfully%2520address%2520the%2520task%2520of%2520generating%2520keyphrases.%2520LLMs%2520allow%250Aachieving%2520impressive%2520results%2520without%2520task-specific%2520fine-tuning%252C%2520using%2520text%250Aprompts%2520instead.%2520In%2520this%2520work%252C%2520we%2520access%2520the%2520performance%2520of%2520prompt-based%250Amethods%2520for%2520generating%2520keyphrases%2520for%2520Russian%2520scientific%2520abstracts.%2520First%252C%2520we%250Acompare%2520the%2520performance%2520of%2520zero-shot%2520and%2520few-shot%2520prompt-based%2520methods%252C%250Afine-tuned%2520models%252C%2520and%2520unsupervised%2520methods.%2520Then%2520we%2520assess%2520strategies%2520for%250Aselecting%2520keyphrase%2520examples%2520in%2520a%2520few-shot%2520setting.%2520We%2520present%2520the%2520outcomes%2520of%250Ahuman%2520evaluation%2520of%2520the%2520generated%2520keyphrases%2520and%2520analyze%2520the%2520strengths%2520and%250Aweaknesses%2520of%2520the%2520models%2520through%2520expert%2520assessment.%2520Our%2520results%2520suggest%2520that%250Aprompt-based%2520methods%2520can%2520outperform%2520common%2520baselines%2520even%2520using%2520simple%2520text%250Aprompts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Key%20Algorithms%20for%20Keyphrase%20Generation%3A%20Instruction-Based%20LLMs%20for%0A%20%20Russian%20Scientific%20Keyphrases&entry.906535625=Anna%20Glazkova%20and%20Dmitry%20Morozov%20and%20Timur%20Garipov&entry.1292438233=%20%20Keyphrase%20selection%20is%20a%20challenging%20task%20in%20natural%20language%20processing%20that%0Ahas%20a%20wide%20range%20of%20applications.%20Adapting%20existing%20supervised%20and%20unsupervised%0Asolutions%20for%20the%20Russian%20language%20faces%20several%20limitations%20due%20to%20the%20rich%0Amorphology%20of%20Russian%20and%20the%20limited%20number%20of%20training%20datasets%20available.%0ARecent%20studies%20conducted%20on%20English%20texts%20show%20that%20large%20language%20models%0A%28LLMs%29%20successfully%20address%20the%20task%20of%20generating%20keyphrases.%20LLMs%20allow%0Aachieving%20impressive%20results%20without%20task-specific%20fine-tuning%2C%20using%20text%0Aprompts%20instead.%20In%20this%20work%2C%20we%20access%20the%20performance%20of%20prompt-based%0Amethods%20for%20generating%20keyphrases%20for%20Russian%20scientific%20abstracts.%20First%2C%20we%0Acompare%20the%20performance%20of%20zero-shot%20and%20few-shot%20prompt-based%20methods%2C%0Afine-tuned%20models%2C%20and%20unsupervised%20methods.%20Then%20we%20assess%20strategies%20for%0Aselecting%20keyphrase%20examples%20in%20a%20few-shot%20setting.%20We%20present%20the%20outcomes%20of%0Ahuman%20evaluation%20of%20the%20generated%20keyphrases%20and%20analyze%20the%20strengths%20and%0Aweaknesses%20of%20the%20models%20through%20expert%20assessment.%20Our%20results%20suggest%20that%0Aprompt-based%20methods%20can%20outperform%20common%20baselines%20even%20using%20simple%20text%0Aprompts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18040v1&entry.124074799=Read"},
{"title": "On provable privacy vulnerabilities of graph representations", "author": "Ruofan Wu and Guanhua Fang and Qiying Pan and Mingyang Zhang and Tengfei Liu and Weiqiang Wang", "abstract": "  Graph representation learning (GRL) is critical for extracting insights from\ncomplex network structures, but it also raises security concerns due to\npotential privacy vulnerabilities in these representations. This paper\ninvestigates the structural vulnerabilities in graph neural models where\nsensitive topological information can be inferred through edge reconstruction\nattacks. Our research primarily addresses the theoretical underpinnings of\nsimilarity-based edge reconstruction attacks (SERA), furnishing a\nnon-asymptotic analysis of their reconstruction capacities. Moreover, we\npresent empirical corroboration indicating that such attacks can perfectly\nreconstruct sparse graphs as graph size increases. Conversely, we establish\nthat sparsity is a critical factor for SERA's effectiveness, as demonstrated\nthrough analysis and experiments on (dense) stochastic block models. Finally,\nwe explore the resilience of private graph representations produced via noisy\naggregation (NAG) mechanism against SERA. Through theoretical analysis and\nempirical assessments, we affirm the mitigation of SERA using NAG . In\nparallel, we also empirically delineate instances wherein SERA demonstrates\nboth efficacy and deficiency in its capacity to function as an instrument for\nelucidating the trade-off between privacy and utility.\n", "link": "http://arxiv.org/abs/2402.04033v3", "date": "2024-10-23", "relevancy": 2.1791, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4562}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4282}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4231}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20provable%20privacy%20vulnerabilities%20of%20graph%20representations&body=Title%3A%20On%20provable%20privacy%20vulnerabilities%20of%20graph%20representations%0AAuthor%3A%20Ruofan%20Wu%20and%20Guanhua%20Fang%20and%20Qiying%20Pan%20and%20Mingyang%20Zhang%20and%20Tengfei%20Liu%20and%20Weiqiang%20Wang%0AAbstract%3A%20%20%20Graph%20representation%20learning%20%28GRL%29%20is%20critical%20for%20extracting%20insights%20from%0Acomplex%20network%20structures%2C%20but%20it%20also%20raises%20security%20concerns%20due%20to%0Apotential%20privacy%20vulnerabilities%20in%20these%20representations.%20This%20paper%0Ainvestigates%20the%20structural%20vulnerabilities%20in%20graph%20neural%20models%20where%0Asensitive%20topological%20information%20can%20be%20inferred%20through%20edge%20reconstruction%0Aattacks.%20Our%20research%20primarily%20addresses%20the%20theoretical%20underpinnings%20of%0Asimilarity-based%20edge%20reconstruction%20attacks%20%28SERA%29%2C%20furnishing%20a%0Anon-asymptotic%20analysis%20of%20their%20reconstruction%20capacities.%20Moreover%2C%20we%0Apresent%20empirical%20corroboration%20indicating%20that%20such%20attacks%20can%20perfectly%0Areconstruct%20sparse%20graphs%20as%20graph%20size%20increases.%20Conversely%2C%20we%20establish%0Athat%20sparsity%20is%20a%20critical%20factor%20for%20SERA%27s%20effectiveness%2C%20as%20demonstrated%0Athrough%20analysis%20and%20experiments%20on%20%28dense%29%20stochastic%20block%20models.%20Finally%2C%0Awe%20explore%20the%20resilience%20of%20private%20graph%20representations%20produced%20via%20noisy%0Aaggregation%20%28NAG%29%20mechanism%20against%20SERA.%20Through%20theoretical%20analysis%20and%0Aempirical%20assessments%2C%20we%20affirm%20the%20mitigation%20of%20SERA%20using%20NAG%20.%20In%0Aparallel%2C%20we%20also%20empirically%20delineate%20instances%20wherein%20SERA%20demonstrates%0Aboth%20efficacy%20and%20deficiency%20in%20its%20capacity%20to%20function%20as%20an%20instrument%20for%0Aelucidating%20the%20trade-off%20between%20privacy%20and%20utility.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.04033v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520provable%2520privacy%2520vulnerabilities%2520of%2520graph%2520representations%26entry.906535625%3DRuofan%2520Wu%2520and%2520Guanhua%2520Fang%2520and%2520Qiying%2520Pan%2520and%2520Mingyang%2520Zhang%2520and%2520Tengfei%2520Liu%2520and%2520Weiqiang%2520Wang%26entry.1292438233%3D%2520%2520Graph%2520representation%2520learning%2520%2528GRL%2529%2520is%2520critical%2520for%2520extracting%2520insights%2520from%250Acomplex%2520network%2520structures%252C%2520but%2520it%2520also%2520raises%2520security%2520concerns%2520due%2520to%250Apotential%2520privacy%2520vulnerabilities%2520in%2520these%2520representations.%2520This%2520paper%250Ainvestigates%2520the%2520structural%2520vulnerabilities%2520in%2520graph%2520neural%2520models%2520where%250Asensitive%2520topological%2520information%2520can%2520be%2520inferred%2520through%2520edge%2520reconstruction%250Aattacks.%2520Our%2520research%2520primarily%2520addresses%2520the%2520theoretical%2520underpinnings%2520of%250Asimilarity-based%2520edge%2520reconstruction%2520attacks%2520%2528SERA%2529%252C%2520furnishing%2520a%250Anon-asymptotic%2520analysis%2520of%2520their%2520reconstruction%2520capacities.%2520Moreover%252C%2520we%250Apresent%2520empirical%2520corroboration%2520indicating%2520that%2520such%2520attacks%2520can%2520perfectly%250Areconstruct%2520sparse%2520graphs%2520as%2520graph%2520size%2520increases.%2520Conversely%252C%2520we%2520establish%250Athat%2520sparsity%2520is%2520a%2520critical%2520factor%2520for%2520SERA%2527s%2520effectiveness%252C%2520as%2520demonstrated%250Athrough%2520analysis%2520and%2520experiments%2520on%2520%2528dense%2529%2520stochastic%2520block%2520models.%2520Finally%252C%250Awe%2520explore%2520the%2520resilience%2520of%2520private%2520graph%2520representations%2520produced%2520via%2520noisy%250Aaggregation%2520%2528NAG%2529%2520mechanism%2520against%2520SERA.%2520Through%2520theoretical%2520analysis%2520and%250Aempirical%2520assessments%252C%2520we%2520affirm%2520the%2520mitigation%2520of%2520SERA%2520using%2520NAG%2520.%2520In%250Aparallel%252C%2520we%2520also%2520empirically%2520delineate%2520instances%2520wherein%2520SERA%2520demonstrates%250Aboth%2520efficacy%2520and%2520deficiency%2520in%2520its%2520capacity%2520to%2520function%2520as%2520an%2520instrument%2520for%250Aelucidating%2520the%2520trade-off%2520between%2520privacy%2520and%2520utility.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.04033v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20provable%20privacy%20vulnerabilities%20of%20graph%20representations&entry.906535625=Ruofan%20Wu%20and%20Guanhua%20Fang%20and%20Qiying%20Pan%20and%20Mingyang%20Zhang%20and%20Tengfei%20Liu%20and%20Weiqiang%20Wang&entry.1292438233=%20%20Graph%20representation%20learning%20%28GRL%29%20is%20critical%20for%20extracting%20insights%20from%0Acomplex%20network%20structures%2C%20but%20it%20also%20raises%20security%20concerns%20due%20to%0Apotential%20privacy%20vulnerabilities%20in%20these%20representations.%20This%20paper%0Ainvestigates%20the%20structural%20vulnerabilities%20in%20graph%20neural%20models%20where%0Asensitive%20topological%20information%20can%20be%20inferred%20through%20edge%20reconstruction%0Aattacks.%20Our%20research%20primarily%20addresses%20the%20theoretical%20underpinnings%20of%0Asimilarity-based%20edge%20reconstruction%20attacks%20%28SERA%29%2C%20furnishing%20a%0Anon-asymptotic%20analysis%20of%20their%20reconstruction%20capacities.%20Moreover%2C%20we%0Apresent%20empirical%20corroboration%20indicating%20that%20such%20attacks%20can%20perfectly%0Areconstruct%20sparse%20graphs%20as%20graph%20size%20increases.%20Conversely%2C%20we%20establish%0Athat%20sparsity%20is%20a%20critical%20factor%20for%20SERA%27s%20effectiveness%2C%20as%20demonstrated%0Athrough%20analysis%20and%20experiments%20on%20%28dense%29%20stochastic%20block%20models.%20Finally%2C%0Awe%20explore%20the%20resilience%20of%20private%20graph%20representations%20produced%20via%20noisy%0Aaggregation%20%28NAG%29%20mechanism%20against%20SERA.%20Through%20theoretical%20analysis%20and%0Aempirical%20assessments%2C%20we%20affirm%20the%20mitigation%20of%20SERA%20using%20NAG%20.%20In%0Aparallel%2C%20we%20also%20empirically%20delineate%20instances%20wherein%20SERA%20demonstrates%0Aboth%20efficacy%20and%20deficiency%20in%20its%20capacity%20to%20function%20as%20an%20instrument%20for%0Aelucidating%20the%20trade-off%20between%20privacy%20and%20utility.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.04033v3&entry.124074799=Read"},
{"title": "Prioritized Generative Replay", "author": "Renhao Wang and Kevin Frans and Pieter Abbeel and Sergey Levine and Alexei A. Efros", "abstract": "  Sample-efficient online reinforcement learning often uses replay buffers to\nstore experience for reuse when updating the value function. However, uniform\nreplay is inefficient, since certain classes of transitions can be more\nrelevant to learning. While prioritization of more useful samples is helpful,\nthis strategy can also lead to overfitting, as useful samples are likely to be\nmore rare. In this work, we instead propose a prioritized, parametric version\nof an agent's memory, using generative models to capture online experience.\nThis paradigm enables (1) densification of past experience, with new\ngenerations that benefit from the generative model's generalization capacity\nand (2) guidance via a family of \"relevance functions\" that push these\ngenerations towards more useful parts of an agent's acquired history. We show\nthis recipe can be instantiated using conditional diffusion models and simple\nrelevance functions such as curiosity- or value-based metrics. Our approach\nconsistently improves performance and sample efficiency in both state- and\npixel-based domains. We expose the mechanisms underlying these gains, showing\nhow guidance promotes diversity in our generated transitions and reduces\noverfitting. We also showcase how our approach can train policies with even\nhigher update-to-data ratios than before, opening up avenues to better scale\nonline RL agents.\n", "link": "http://arxiv.org/abs/2410.18082v1", "date": "2024-10-23", "relevancy": 2.1762, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.566}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5324}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prioritized%20Generative%20Replay&body=Title%3A%20Prioritized%20Generative%20Replay%0AAuthor%3A%20Renhao%20Wang%20and%20Kevin%20Frans%20and%20Pieter%20Abbeel%20and%20Sergey%20Levine%20and%20Alexei%20A.%20Efros%0AAbstract%3A%20%20%20Sample-efficient%20online%20reinforcement%20learning%20often%20uses%20replay%20buffers%20to%0Astore%20experience%20for%20reuse%20when%20updating%20the%20value%20function.%20However%2C%20uniform%0Areplay%20is%20inefficient%2C%20since%20certain%20classes%20of%20transitions%20can%20be%20more%0Arelevant%20to%20learning.%20While%20prioritization%20of%20more%20useful%20samples%20is%20helpful%2C%0Athis%20strategy%20can%20also%20lead%20to%20overfitting%2C%20as%20useful%20samples%20are%20likely%20to%20be%0Amore%20rare.%20In%20this%20work%2C%20we%20instead%20propose%20a%20prioritized%2C%20parametric%20version%0Aof%20an%20agent%27s%20memory%2C%20using%20generative%20models%20to%20capture%20online%20experience.%0AThis%20paradigm%20enables%20%281%29%20densification%20of%20past%20experience%2C%20with%20new%0Agenerations%20that%20benefit%20from%20the%20generative%20model%27s%20generalization%20capacity%0Aand%20%282%29%20guidance%20via%20a%20family%20of%20%22relevance%20functions%22%20that%20push%20these%0Agenerations%20towards%20more%20useful%20parts%20of%20an%20agent%27s%20acquired%20history.%20We%20show%0Athis%20recipe%20can%20be%20instantiated%20using%20conditional%20diffusion%20models%20and%20simple%0Arelevance%20functions%20such%20as%20curiosity-%20or%20value-based%20metrics.%20Our%20approach%0Aconsistently%20improves%20performance%20and%20sample%20efficiency%20in%20both%20state-%20and%0Apixel-based%20domains.%20We%20expose%20the%20mechanisms%20underlying%20these%20gains%2C%20showing%0Ahow%20guidance%20promotes%20diversity%20in%20our%20generated%20transitions%20and%20reduces%0Aoverfitting.%20We%20also%20showcase%20how%20our%20approach%20can%20train%20policies%20with%20even%0Ahigher%20update-to-data%20ratios%20than%20before%2C%20opening%20up%20avenues%20to%20better%20scale%0Aonline%20RL%20agents.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrioritized%2520Generative%2520Replay%26entry.906535625%3DRenhao%2520Wang%2520and%2520Kevin%2520Frans%2520and%2520Pieter%2520Abbeel%2520and%2520Sergey%2520Levine%2520and%2520Alexei%2520A.%2520Efros%26entry.1292438233%3D%2520%2520Sample-efficient%2520online%2520reinforcement%2520learning%2520often%2520uses%2520replay%2520buffers%2520to%250Astore%2520experience%2520for%2520reuse%2520when%2520updating%2520the%2520value%2520function.%2520However%252C%2520uniform%250Areplay%2520is%2520inefficient%252C%2520since%2520certain%2520classes%2520of%2520transitions%2520can%2520be%2520more%250Arelevant%2520to%2520learning.%2520While%2520prioritization%2520of%2520more%2520useful%2520samples%2520is%2520helpful%252C%250Athis%2520strategy%2520can%2520also%2520lead%2520to%2520overfitting%252C%2520as%2520useful%2520samples%2520are%2520likely%2520to%2520be%250Amore%2520rare.%2520In%2520this%2520work%252C%2520we%2520instead%2520propose%2520a%2520prioritized%252C%2520parametric%2520version%250Aof%2520an%2520agent%2527s%2520memory%252C%2520using%2520generative%2520models%2520to%2520capture%2520online%2520experience.%250AThis%2520paradigm%2520enables%2520%25281%2529%2520densification%2520of%2520past%2520experience%252C%2520with%2520new%250Agenerations%2520that%2520benefit%2520from%2520the%2520generative%2520model%2527s%2520generalization%2520capacity%250Aand%2520%25282%2529%2520guidance%2520via%2520a%2520family%2520of%2520%2522relevance%2520functions%2522%2520that%2520push%2520these%250Agenerations%2520towards%2520more%2520useful%2520parts%2520of%2520an%2520agent%2527s%2520acquired%2520history.%2520We%2520show%250Athis%2520recipe%2520can%2520be%2520instantiated%2520using%2520conditional%2520diffusion%2520models%2520and%2520simple%250Arelevance%2520functions%2520such%2520as%2520curiosity-%2520or%2520value-based%2520metrics.%2520Our%2520approach%250Aconsistently%2520improves%2520performance%2520and%2520sample%2520efficiency%2520in%2520both%2520state-%2520and%250Apixel-based%2520domains.%2520We%2520expose%2520the%2520mechanisms%2520underlying%2520these%2520gains%252C%2520showing%250Ahow%2520guidance%2520promotes%2520diversity%2520in%2520our%2520generated%2520transitions%2520and%2520reduces%250Aoverfitting.%2520We%2520also%2520showcase%2520how%2520our%2520approach%2520can%2520train%2520policies%2520with%2520even%250Ahigher%2520update-to-data%2520ratios%2520than%2520before%252C%2520opening%2520up%2520avenues%2520to%2520better%2520scale%250Aonline%2520RL%2520agents.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prioritized%20Generative%20Replay&entry.906535625=Renhao%20Wang%20and%20Kevin%20Frans%20and%20Pieter%20Abbeel%20and%20Sergey%20Levine%20and%20Alexei%20A.%20Efros&entry.1292438233=%20%20Sample-efficient%20online%20reinforcement%20learning%20often%20uses%20replay%20buffers%20to%0Astore%20experience%20for%20reuse%20when%20updating%20the%20value%20function.%20However%2C%20uniform%0Areplay%20is%20inefficient%2C%20since%20certain%20classes%20of%20transitions%20can%20be%20more%0Arelevant%20to%20learning.%20While%20prioritization%20of%20more%20useful%20samples%20is%20helpful%2C%0Athis%20strategy%20can%20also%20lead%20to%20overfitting%2C%20as%20useful%20samples%20are%20likely%20to%20be%0Amore%20rare.%20In%20this%20work%2C%20we%20instead%20propose%20a%20prioritized%2C%20parametric%20version%0Aof%20an%20agent%27s%20memory%2C%20using%20generative%20models%20to%20capture%20online%20experience.%0AThis%20paradigm%20enables%20%281%29%20densification%20of%20past%20experience%2C%20with%20new%0Agenerations%20that%20benefit%20from%20the%20generative%20model%27s%20generalization%20capacity%0Aand%20%282%29%20guidance%20via%20a%20family%20of%20%22relevance%20functions%22%20that%20push%20these%0Agenerations%20towards%20more%20useful%20parts%20of%20an%20agent%27s%20acquired%20history.%20We%20show%0Athis%20recipe%20can%20be%20instantiated%20using%20conditional%20diffusion%20models%20and%20simple%0Arelevance%20functions%20such%20as%20curiosity-%20or%20value-based%20metrics.%20Our%20approach%0Aconsistently%20improves%20performance%20and%20sample%20efficiency%20in%20both%20state-%20and%0Apixel-based%20domains.%20We%20expose%20the%20mechanisms%20underlying%20these%20gains%2C%20showing%0Ahow%20guidance%20promotes%20diversity%20in%20our%20generated%20transitions%20and%20reduces%0Aoverfitting.%20We%20also%20showcase%20how%20our%20approach%20can%20train%20policies%20with%20even%0Ahigher%20update-to-data%20ratios%20than%20before%2C%20opening%20up%20avenues%20to%20better%20scale%0Aonline%20RL%20agents.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18082v1&entry.124074799=Read"},
{"title": "Towards Croppable Implicit Neural Representations", "author": "Maor Ashkenazi and Eran Treister", "abstract": "  Implicit Neural Representations (INRs) have peaked interest in recent years\ndue to their ability to encode natural signals using neural networks. While\nINRs allow for useful applications such as interpolating new coordinates and\nsignal compression, their black-box nature makes it difficult to modify them\npost-training. In this paper we explore the idea of editable INRs, and\nspecifically focus on the widely used cropping operation. To this end, we\npresent Local-Global SIRENs -- a novel INR architecture that supports cropping\nby design. Local-Global SIRENs are based on combining local and global feature\nextraction for signal encoding. What makes their design unique is the ability\nto effortlessly remove specific portions of an encoded signal, with a\nproportional weight decrease. This is achieved by eliminating the corresponding\nweights from the network, without the need for retraining. We further show how\nthis architecture can be used to support the straightforward extension of\npreviously encoded signals. Beyond signal editing, we examine how the\nLocal-Global approach can accelerate training, enhance encoding of various\nsignals, improve downstream performance, and be applied to modern INRs such as\nINCODE, highlighting its potential and flexibility. Code is available at\nhttps://github.com/maorash/Local-Global-INRs.\n", "link": "http://arxiv.org/abs/2409.19472v2", "date": "2024-10-23", "relevancy": 2.1745, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5925}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5229}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Croppable%20Implicit%20Neural%20Representations&body=Title%3A%20Towards%20Croppable%20Implicit%20Neural%20Representations%0AAuthor%3A%20Maor%20Ashkenazi%20and%20Eran%20Treister%0AAbstract%3A%20%20%20Implicit%20Neural%20Representations%20%28INRs%29%20have%20peaked%20interest%20in%20recent%20years%0Adue%20to%20their%20ability%20to%20encode%20natural%20signals%20using%20neural%20networks.%20While%0AINRs%20allow%20for%20useful%20applications%20such%20as%20interpolating%20new%20coordinates%20and%0Asignal%20compression%2C%20their%20black-box%20nature%20makes%20it%20difficult%20to%20modify%20them%0Apost-training.%20In%20this%20paper%20we%20explore%20the%20idea%20of%20editable%20INRs%2C%20and%0Aspecifically%20focus%20on%20the%20widely%20used%20cropping%20operation.%20To%20this%20end%2C%20we%0Apresent%20Local-Global%20SIRENs%20--%20a%20novel%20INR%20architecture%20that%20supports%20cropping%0Aby%20design.%20Local-Global%20SIRENs%20are%20based%20on%20combining%20local%20and%20global%20feature%0Aextraction%20for%20signal%20encoding.%20What%20makes%20their%20design%20unique%20is%20the%20ability%0Ato%20effortlessly%20remove%20specific%20portions%20of%20an%20encoded%20signal%2C%20with%20a%0Aproportional%20weight%20decrease.%20This%20is%20achieved%20by%20eliminating%20the%20corresponding%0Aweights%20from%20the%20network%2C%20without%20the%20need%20for%20retraining.%20We%20further%20show%20how%0Athis%20architecture%20can%20be%20used%20to%20support%20the%20straightforward%20extension%20of%0Apreviously%20encoded%20signals.%20Beyond%20signal%20editing%2C%20we%20examine%20how%20the%0ALocal-Global%20approach%20can%20accelerate%20training%2C%20enhance%20encoding%20of%20various%0Asignals%2C%20improve%20downstream%20performance%2C%20and%20be%20applied%20to%20modern%20INRs%20such%20as%0AINCODE%2C%20highlighting%20its%20potential%20and%20flexibility.%20Code%20is%20available%20at%0Ahttps%3A//github.com/maorash/Local-Global-INRs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19472v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Croppable%2520Implicit%2520Neural%2520Representations%26entry.906535625%3DMaor%2520Ashkenazi%2520and%2520Eran%2520Treister%26entry.1292438233%3D%2520%2520Implicit%2520Neural%2520Representations%2520%2528INRs%2529%2520have%2520peaked%2520interest%2520in%2520recent%2520years%250Adue%2520to%2520their%2520ability%2520to%2520encode%2520natural%2520signals%2520using%2520neural%2520networks.%2520While%250AINRs%2520allow%2520for%2520useful%2520applications%2520such%2520as%2520interpolating%2520new%2520coordinates%2520and%250Asignal%2520compression%252C%2520their%2520black-box%2520nature%2520makes%2520it%2520difficult%2520to%2520modify%2520them%250Apost-training.%2520In%2520this%2520paper%2520we%2520explore%2520the%2520idea%2520of%2520editable%2520INRs%252C%2520and%250Aspecifically%2520focus%2520on%2520the%2520widely%2520used%2520cropping%2520operation.%2520To%2520this%2520end%252C%2520we%250Apresent%2520Local-Global%2520SIRENs%2520--%2520a%2520novel%2520INR%2520architecture%2520that%2520supports%2520cropping%250Aby%2520design.%2520Local-Global%2520SIRENs%2520are%2520based%2520on%2520combining%2520local%2520and%2520global%2520feature%250Aextraction%2520for%2520signal%2520encoding.%2520What%2520makes%2520their%2520design%2520unique%2520is%2520the%2520ability%250Ato%2520effortlessly%2520remove%2520specific%2520portions%2520of%2520an%2520encoded%2520signal%252C%2520with%2520a%250Aproportional%2520weight%2520decrease.%2520This%2520is%2520achieved%2520by%2520eliminating%2520the%2520corresponding%250Aweights%2520from%2520the%2520network%252C%2520without%2520the%2520need%2520for%2520retraining.%2520We%2520further%2520show%2520how%250Athis%2520architecture%2520can%2520be%2520used%2520to%2520support%2520the%2520straightforward%2520extension%2520of%250Apreviously%2520encoded%2520signals.%2520Beyond%2520signal%2520editing%252C%2520we%2520examine%2520how%2520the%250ALocal-Global%2520approach%2520can%2520accelerate%2520training%252C%2520enhance%2520encoding%2520of%2520various%250Asignals%252C%2520improve%2520downstream%2520performance%252C%2520and%2520be%2520applied%2520to%2520modern%2520INRs%2520such%2520as%250AINCODE%252C%2520highlighting%2520its%2520potential%2520and%2520flexibility.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/maorash/Local-Global-INRs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19472v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Croppable%20Implicit%20Neural%20Representations&entry.906535625=Maor%20Ashkenazi%20and%20Eran%20Treister&entry.1292438233=%20%20Implicit%20Neural%20Representations%20%28INRs%29%20have%20peaked%20interest%20in%20recent%20years%0Adue%20to%20their%20ability%20to%20encode%20natural%20signals%20using%20neural%20networks.%20While%0AINRs%20allow%20for%20useful%20applications%20such%20as%20interpolating%20new%20coordinates%20and%0Asignal%20compression%2C%20their%20black-box%20nature%20makes%20it%20difficult%20to%20modify%20them%0Apost-training.%20In%20this%20paper%20we%20explore%20the%20idea%20of%20editable%20INRs%2C%20and%0Aspecifically%20focus%20on%20the%20widely%20used%20cropping%20operation.%20To%20this%20end%2C%20we%0Apresent%20Local-Global%20SIRENs%20--%20a%20novel%20INR%20architecture%20that%20supports%20cropping%0Aby%20design.%20Local-Global%20SIRENs%20are%20based%20on%20combining%20local%20and%20global%20feature%0Aextraction%20for%20signal%20encoding.%20What%20makes%20their%20design%20unique%20is%20the%20ability%0Ato%20effortlessly%20remove%20specific%20portions%20of%20an%20encoded%20signal%2C%20with%20a%0Aproportional%20weight%20decrease.%20This%20is%20achieved%20by%20eliminating%20the%20corresponding%0Aweights%20from%20the%20network%2C%20without%20the%20need%20for%20retraining.%20We%20further%20show%20how%0Athis%20architecture%20can%20be%20used%20to%20support%20the%20straightforward%20extension%20of%0Apreviously%20encoded%20signals.%20Beyond%20signal%20editing%2C%20we%20examine%20how%20the%0ALocal-Global%20approach%20can%20accelerate%20training%2C%20enhance%20encoding%20of%20various%0Asignals%2C%20improve%20downstream%20performance%2C%20and%20be%20applied%20to%20modern%20INRs%20such%20as%0AINCODE%2C%20highlighting%20its%20potential%20and%20flexibility.%20Code%20is%20available%20at%0Ahttps%3A//github.com/maorash/Local-Global-INRs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19472v2&entry.124074799=Read"},
{"title": "Quasi-Medial Distance Field (Q-MDF): A Robust Method for Approximating\n  and Discretizing Neural Medial Axis", "author": "Jiayi Kong and Chen Zong and Jun Luo and Shiqing Xin and Fei Hou and Hanqing Jiang and Chen Qian and Ying He", "abstract": "  The medial axis, a lower-dimensional shape descriptor, plays an important\nrole in the field of digital geometry processing. Despite its importance,\nrobust computation of the medial axis transform from diverse inputs, especially\npoint clouds with defects, remains a significant challenge. In this paper, we\ntackle the challenge by proposing a new implicit method that diverges from\nmainstream explicit medial axis computation techniques. Our key technical\ninsight is the difference between the signed distance field (SDF) and the\nmedial field (MF) of a solid shape is the unsigned distance field (UDF) of the\nshape's medial axis. This allows for formulating medial axis computation as an\nimplicit reconstruction problem. Utilizing a modified double covering method,\nwe extract the medial axis as the zero level-set of the UDF. Extensive\nexperiments show that our method has enhanced accuracy and robustness in\nlearning compact medial axis transform from thorny meshes and point clouds\ncompared to existing methods.\n", "link": "http://arxiv.org/abs/2410.17774v1", "date": "2024-10-23", "relevancy": 2.1563, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5453}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5449}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5306}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quasi-Medial%20Distance%20Field%20%28Q-MDF%29%3A%20A%20Robust%20Method%20for%20Approximating%0A%20%20and%20Discretizing%20Neural%20Medial%20Axis&body=Title%3A%20Quasi-Medial%20Distance%20Field%20%28Q-MDF%29%3A%20A%20Robust%20Method%20for%20Approximating%0A%20%20and%20Discretizing%20Neural%20Medial%20Axis%0AAuthor%3A%20Jiayi%20Kong%20and%20Chen%20Zong%20and%20Jun%20Luo%20and%20Shiqing%20Xin%20and%20Fei%20Hou%20and%20Hanqing%20Jiang%20and%20Chen%20Qian%20and%20Ying%20He%0AAbstract%3A%20%20%20The%20medial%20axis%2C%20a%20lower-dimensional%20shape%20descriptor%2C%20plays%20an%20important%0Arole%20in%20the%20field%20of%20digital%20geometry%20processing.%20Despite%20its%20importance%2C%0Arobust%20computation%20of%20the%20medial%20axis%20transform%20from%20diverse%20inputs%2C%20especially%0Apoint%20clouds%20with%20defects%2C%20remains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%0Atackle%20the%20challenge%20by%20proposing%20a%20new%20implicit%20method%20that%20diverges%20from%0Amainstream%20explicit%20medial%20axis%20computation%20techniques.%20Our%20key%20technical%0Ainsight%20is%20the%20difference%20between%20the%20signed%20distance%20field%20%28SDF%29%20and%20the%0Amedial%20field%20%28MF%29%20of%20a%20solid%20shape%20is%20the%20unsigned%20distance%20field%20%28UDF%29%20of%20the%0Ashape%27s%20medial%20axis.%20This%20allows%20for%20formulating%20medial%20axis%20computation%20as%20an%0Aimplicit%20reconstruction%20problem.%20Utilizing%20a%20modified%20double%20covering%20method%2C%0Awe%20extract%20the%20medial%20axis%20as%20the%20zero%20level-set%20of%20the%20UDF.%20Extensive%0Aexperiments%20show%20that%20our%20method%20has%20enhanced%20accuracy%20and%20robustness%20in%0Alearning%20compact%20medial%20axis%20transform%20from%20thorny%20meshes%20and%20point%20clouds%0Acompared%20to%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17774v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuasi-Medial%2520Distance%2520Field%2520%2528Q-MDF%2529%253A%2520A%2520Robust%2520Method%2520for%2520Approximating%250A%2520%2520and%2520Discretizing%2520Neural%2520Medial%2520Axis%26entry.906535625%3DJiayi%2520Kong%2520and%2520Chen%2520Zong%2520and%2520Jun%2520Luo%2520and%2520Shiqing%2520Xin%2520and%2520Fei%2520Hou%2520and%2520Hanqing%2520Jiang%2520and%2520Chen%2520Qian%2520and%2520Ying%2520He%26entry.1292438233%3D%2520%2520The%2520medial%2520axis%252C%2520a%2520lower-dimensional%2520shape%2520descriptor%252C%2520plays%2520an%2520important%250Arole%2520in%2520the%2520field%2520of%2520digital%2520geometry%2520processing.%2520Despite%2520its%2520importance%252C%250Arobust%2520computation%2520of%2520the%2520medial%2520axis%2520transform%2520from%2520diverse%2520inputs%252C%2520especially%250Apoint%2520clouds%2520with%2520defects%252C%2520remains%2520a%2520significant%2520challenge.%2520In%2520this%2520paper%252C%2520we%250Atackle%2520the%2520challenge%2520by%2520proposing%2520a%2520new%2520implicit%2520method%2520that%2520diverges%2520from%250Amainstream%2520explicit%2520medial%2520axis%2520computation%2520techniques.%2520Our%2520key%2520technical%250Ainsight%2520is%2520the%2520difference%2520between%2520the%2520signed%2520distance%2520field%2520%2528SDF%2529%2520and%2520the%250Amedial%2520field%2520%2528MF%2529%2520of%2520a%2520solid%2520shape%2520is%2520the%2520unsigned%2520distance%2520field%2520%2528UDF%2529%2520of%2520the%250Ashape%2527s%2520medial%2520axis.%2520This%2520allows%2520for%2520formulating%2520medial%2520axis%2520computation%2520as%2520an%250Aimplicit%2520reconstruction%2520problem.%2520Utilizing%2520a%2520modified%2520double%2520covering%2520method%252C%250Awe%2520extract%2520the%2520medial%2520axis%2520as%2520the%2520zero%2520level-set%2520of%2520the%2520UDF.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520method%2520has%2520enhanced%2520accuracy%2520and%2520robustness%2520in%250Alearning%2520compact%2520medial%2520axis%2520transform%2520from%2520thorny%2520meshes%2520and%2520point%2520clouds%250Acompared%2520to%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17774v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quasi-Medial%20Distance%20Field%20%28Q-MDF%29%3A%20A%20Robust%20Method%20for%20Approximating%0A%20%20and%20Discretizing%20Neural%20Medial%20Axis&entry.906535625=Jiayi%20Kong%20and%20Chen%20Zong%20and%20Jun%20Luo%20and%20Shiqing%20Xin%20and%20Fei%20Hou%20and%20Hanqing%20Jiang%20and%20Chen%20Qian%20and%20Ying%20He&entry.1292438233=%20%20The%20medial%20axis%2C%20a%20lower-dimensional%20shape%20descriptor%2C%20plays%20an%20important%0Arole%20in%20the%20field%20of%20digital%20geometry%20processing.%20Despite%20its%20importance%2C%0Arobust%20computation%20of%20the%20medial%20axis%20transform%20from%20diverse%20inputs%2C%20especially%0Apoint%20clouds%20with%20defects%2C%20remains%20a%20significant%20challenge.%20In%20this%20paper%2C%20we%0Atackle%20the%20challenge%20by%20proposing%20a%20new%20implicit%20method%20that%20diverges%20from%0Amainstream%20explicit%20medial%20axis%20computation%20techniques.%20Our%20key%20technical%0Ainsight%20is%20the%20difference%20between%20the%20signed%20distance%20field%20%28SDF%29%20and%20the%0Amedial%20field%20%28MF%29%20of%20a%20solid%20shape%20is%20the%20unsigned%20distance%20field%20%28UDF%29%20of%20the%0Ashape%27s%20medial%20axis.%20This%20allows%20for%20formulating%20medial%20axis%20computation%20as%20an%0Aimplicit%20reconstruction%20problem.%20Utilizing%20a%20modified%20double%20covering%20method%2C%0Awe%20extract%20the%20medial%20axis%20as%20the%20zero%20level-set%20of%20the%20UDF.%20Extensive%0Aexperiments%20show%20that%20our%20method%20has%20enhanced%20accuracy%20and%20robustness%20in%0Alearning%20compact%20medial%20axis%20transform%20from%20thorny%20meshes%20and%20point%20clouds%0Acompared%20to%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17774v1&entry.124074799=Read"},
{"title": "Gaze-Assisted Medical Image Segmentation", "author": "Leila Khaertdinova and Ilya Pershin and Tatiana Shmykova and Bulat Ibragimov", "abstract": "  The annotation of patient organs is a crucial part of various diagnostic and\ntreatment procedures, such as radiotherapy planning. Manual annotation is\nextremely time-consuming, while its automation using modern image analysis\ntechniques has not yet reached levels sufficient for clinical adoption. This\npaper investigates the idea of semi-supervised medical image segmentation using\nhuman gaze as interactive input for segmentation correction. In particular, we\nfine-tuned the Segment Anything Model in Medical Images (MedSAM), a public\nsolution that uses various prompt types as additional input for semi-automated\nsegmentation correction. We used human gaze data from reading abdominal images\nas a prompt for fine-tuning MedSAM. The model was validated on a public WORD\ndatabase, which consists of 120 CT scans of 16 abdominal organs. The results of\nthe gaze-assisted MedSAM were shown to be superior to the results of the\nstate-of-the-art segmentation models. In particular, the average Dice\ncoefficient for 16 abdominal organs was 85.8%, 86.7%, 81.7%, and 90.5% for\nnnUNetV2, ResUNet, original MedSAM, and our gaze-assisted MedSAM model,\nrespectively.\n", "link": "http://arxiv.org/abs/2410.17920v1", "date": "2024-10-23", "relevancy": 2.1472, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5597}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5384}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5133}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaze-Assisted%20Medical%20Image%20Segmentation&body=Title%3A%20Gaze-Assisted%20Medical%20Image%20Segmentation%0AAuthor%3A%20Leila%20Khaertdinova%20and%20Ilya%20Pershin%20and%20Tatiana%20Shmykova%20and%20Bulat%20Ibragimov%0AAbstract%3A%20%20%20The%20annotation%20of%20patient%20organs%20is%20a%20crucial%20part%20of%20various%20diagnostic%20and%0Atreatment%20procedures%2C%20such%20as%20radiotherapy%20planning.%20Manual%20annotation%20is%0Aextremely%20time-consuming%2C%20while%20its%20automation%20using%20modern%20image%20analysis%0Atechniques%20has%20not%20yet%20reached%20levels%20sufficient%20for%20clinical%20adoption.%20This%0Apaper%20investigates%20the%20idea%20of%20semi-supervised%20medical%20image%20segmentation%20using%0Ahuman%20gaze%20as%20interactive%20input%20for%20segmentation%20correction.%20In%20particular%2C%20we%0Afine-tuned%20the%20Segment%20Anything%20Model%20in%20Medical%20Images%20%28MedSAM%29%2C%20a%20public%0Asolution%20that%20uses%20various%20prompt%20types%20as%20additional%20input%20for%20semi-automated%0Asegmentation%20correction.%20We%20used%20human%20gaze%20data%20from%20reading%20abdominal%20images%0Aas%20a%20prompt%20for%20fine-tuning%20MedSAM.%20The%20model%20was%20validated%20on%20a%20public%20WORD%0Adatabase%2C%20which%20consists%20of%20120%20CT%20scans%20of%2016%20abdominal%20organs.%20The%20results%20of%0Athe%20gaze-assisted%20MedSAM%20were%20shown%20to%20be%20superior%20to%20the%20results%20of%20the%0Astate-of-the-art%20segmentation%20models.%20In%20particular%2C%20the%20average%20Dice%0Acoefficient%20for%2016%20abdominal%20organs%20was%2085.8%25%2C%2086.7%25%2C%2081.7%25%2C%20and%2090.5%25%20for%0AnnUNetV2%2C%20ResUNet%2C%20original%20MedSAM%2C%20and%20our%20gaze-assisted%20MedSAM%20model%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaze-Assisted%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DLeila%2520Khaertdinova%2520and%2520Ilya%2520Pershin%2520and%2520Tatiana%2520Shmykova%2520and%2520Bulat%2520Ibragimov%26entry.1292438233%3D%2520%2520The%2520annotation%2520of%2520patient%2520organs%2520is%2520a%2520crucial%2520part%2520of%2520various%2520diagnostic%2520and%250Atreatment%2520procedures%252C%2520such%2520as%2520radiotherapy%2520planning.%2520Manual%2520annotation%2520is%250Aextremely%2520time-consuming%252C%2520while%2520its%2520automation%2520using%2520modern%2520image%2520analysis%250Atechniques%2520has%2520not%2520yet%2520reached%2520levels%2520sufficient%2520for%2520clinical%2520adoption.%2520This%250Apaper%2520investigates%2520the%2520idea%2520of%2520semi-supervised%2520medical%2520image%2520segmentation%2520using%250Ahuman%2520gaze%2520as%2520interactive%2520input%2520for%2520segmentation%2520correction.%2520In%2520particular%252C%2520we%250Afine-tuned%2520the%2520Segment%2520Anything%2520Model%2520in%2520Medical%2520Images%2520%2528MedSAM%2529%252C%2520a%2520public%250Asolution%2520that%2520uses%2520various%2520prompt%2520types%2520as%2520additional%2520input%2520for%2520semi-automated%250Asegmentation%2520correction.%2520We%2520used%2520human%2520gaze%2520data%2520from%2520reading%2520abdominal%2520images%250Aas%2520a%2520prompt%2520for%2520fine-tuning%2520MedSAM.%2520The%2520model%2520was%2520validated%2520on%2520a%2520public%2520WORD%250Adatabase%252C%2520which%2520consists%2520of%2520120%2520CT%2520scans%2520of%252016%2520abdominal%2520organs.%2520The%2520results%2520of%250Athe%2520gaze-assisted%2520MedSAM%2520were%2520shown%2520to%2520be%2520superior%2520to%2520the%2520results%2520of%2520the%250Astate-of-the-art%2520segmentation%2520models.%2520In%2520particular%252C%2520the%2520average%2520Dice%250Acoefficient%2520for%252016%2520abdominal%2520organs%2520was%252085.8%2525%252C%252086.7%2525%252C%252081.7%2525%252C%2520and%252090.5%2525%2520for%250AnnUNetV2%252C%2520ResUNet%252C%2520original%2520MedSAM%252C%2520and%2520our%2520gaze-assisted%2520MedSAM%2520model%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaze-Assisted%20Medical%20Image%20Segmentation&entry.906535625=Leila%20Khaertdinova%20and%20Ilya%20Pershin%20and%20Tatiana%20Shmykova%20and%20Bulat%20Ibragimov&entry.1292438233=%20%20The%20annotation%20of%20patient%20organs%20is%20a%20crucial%20part%20of%20various%20diagnostic%20and%0Atreatment%20procedures%2C%20such%20as%20radiotherapy%20planning.%20Manual%20annotation%20is%0Aextremely%20time-consuming%2C%20while%20its%20automation%20using%20modern%20image%20analysis%0Atechniques%20has%20not%20yet%20reached%20levels%20sufficient%20for%20clinical%20adoption.%20This%0Apaper%20investigates%20the%20idea%20of%20semi-supervised%20medical%20image%20segmentation%20using%0Ahuman%20gaze%20as%20interactive%20input%20for%20segmentation%20correction.%20In%20particular%2C%20we%0Afine-tuned%20the%20Segment%20Anything%20Model%20in%20Medical%20Images%20%28MedSAM%29%2C%20a%20public%0Asolution%20that%20uses%20various%20prompt%20types%20as%20additional%20input%20for%20semi-automated%0Asegmentation%20correction.%20We%20used%20human%20gaze%20data%20from%20reading%20abdominal%20images%0Aas%20a%20prompt%20for%20fine-tuning%20MedSAM.%20The%20model%20was%20validated%20on%20a%20public%20WORD%0Adatabase%2C%20which%20consists%20of%20120%20CT%20scans%20of%2016%20abdominal%20organs.%20The%20results%20of%0Athe%20gaze-assisted%20MedSAM%20were%20shown%20to%20be%20superior%20to%20the%20results%20of%20the%0Astate-of-the-art%20segmentation%20models.%20In%20particular%2C%20the%20average%20Dice%0Acoefficient%20for%2016%20abdominal%20organs%20was%2085.8%25%2C%2086.7%25%2C%2081.7%25%2C%20and%2090.5%25%20for%0AnnUNetV2%2C%20ResUNet%2C%20original%20MedSAM%2C%20and%20our%20gaze-assisted%20MedSAM%20model%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17920v1&entry.124074799=Read"},
{"title": "Proof of Thought : Neurosymbolic Program Synthesis allows Robust and\n  Interpretable Reasoning", "author": "Debargha Ganguly and Srinivasan Iyengar and Vipin Chaudhary and Shivkumar Kalyanaraman", "abstract": "  Large Language Models (LLMs) have revolutionized natural language processing,\nyet they struggle with inconsistent reasoning, particularly in novel domains\nand complex logical sequences. This research introduces Proof of Thought, a\nframework that enhances the reliability and transparency of LLM outputs. Our\napproach bridges LLM-generated ideas with formal logic verification, employing\na custom interpreter to convert LLM outputs into First Order Logic constructs\nfor theorem prover scrutiny. Central to our method is an intermediary\nJSON-based Domain-Specific Language, which by design balances precise logical\nstructures with intuitive human concepts. This hybrid representation enables\nboth rigorous validation and accessible human comprehension of LLM reasoning\nprocesses. Key contributions include a robust type system with sort management\nfor enhanced logical integrity, explicit representation of rules for clear\ndistinction between factual and inferential knowledge, and a flexible\narchitecture that allows for easy extension to various domain-specific\napplications. We demonstrate Proof of Thought's effectiveness through\nbenchmarking on StrategyQA and a novel multimodal reasoning task, showing\nimproved performance in open-ended scenarios. By providing verifiable and\ninterpretable results, our technique addresses critical needs for AI system\naccountability and sets a foundation for human-in-the-loop oversight in\nhigh-stakes domains.\n", "link": "http://arxiv.org/abs/2409.17270v2", "date": "2024-10-23", "relevancy": 2.1429, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5394}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5394}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Proof%20of%20Thought%20%3A%20Neurosymbolic%20Program%20Synthesis%20allows%20Robust%20and%0A%20%20Interpretable%20Reasoning&body=Title%3A%20Proof%20of%20Thought%20%3A%20Neurosymbolic%20Program%20Synthesis%20allows%20Robust%20and%0A%20%20Interpretable%20Reasoning%0AAuthor%3A%20Debargha%20Ganguly%20and%20Srinivasan%20Iyengar%20and%20Vipin%20Chaudhary%20and%20Shivkumar%20Kalyanaraman%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20natural%20language%20processing%2C%0Ayet%20they%20struggle%20with%20inconsistent%20reasoning%2C%20particularly%20in%20novel%20domains%0Aand%20complex%20logical%20sequences.%20This%20research%20introduces%20Proof%20of%20Thought%2C%20a%0Aframework%20that%20enhances%20the%20reliability%20and%20transparency%20of%20LLM%20outputs.%20Our%0Aapproach%20bridges%20LLM-generated%20ideas%20with%20formal%20logic%20verification%2C%20employing%0Aa%20custom%20interpreter%20to%20convert%20LLM%20outputs%20into%20First%20Order%20Logic%20constructs%0Afor%20theorem%20prover%20scrutiny.%20Central%20to%20our%20method%20is%20an%20intermediary%0AJSON-based%20Domain-Specific%20Language%2C%20which%20by%20design%20balances%20precise%20logical%0Astructures%20with%20intuitive%20human%20concepts.%20This%20hybrid%20representation%20enables%0Aboth%20rigorous%20validation%20and%20accessible%20human%20comprehension%20of%20LLM%20reasoning%0Aprocesses.%20Key%20contributions%20include%20a%20robust%20type%20system%20with%20sort%20management%0Afor%20enhanced%20logical%20integrity%2C%20explicit%20representation%20of%20rules%20for%20clear%0Adistinction%20between%20factual%20and%20inferential%20knowledge%2C%20and%20a%20flexible%0Aarchitecture%20that%20allows%20for%20easy%20extension%20to%20various%20domain-specific%0Aapplications.%20We%20demonstrate%20Proof%20of%20Thought%27s%20effectiveness%20through%0Abenchmarking%20on%20StrategyQA%20and%20a%20novel%20multimodal%20reasoning%20task%2C%20showing%0Aimproved%20performance%20in%20open-ended%20scenarios.%20By%20providing%20verifiable%20and%0Ainterpretable%20results%2C%20our%20technique%20addresses%20critical%20needs%20for%20AI%20system%0Aaccountability%20and%20sets%20a%20foundation%20for%20human-in-the-loop%20oversight%20in%0Ahigh-stakes%20domains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.17270v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProof%2520of%2520Thought%2520%253A%2520Neurosymbolic%2520Program%2520Synthesis%2520allows%2520Robust%2520and%250A%2520%2520Interpretable%2520Reasoning%26entry.906535625%3DDebargha%2520Ganguly%2520and%2520Srinivasan%2520Iyengar%2520and%2520Vipin%2520Chaudhary%2520and%2520Shivkumar%2520Kalyanaraman%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520revolutionized%2520natural%2520language%2520processing%252C%250Ayet%2520they%2520struggle%2520with%2520inconsistent%2520reasoning%252C%2520particularly%2520in%2520novel%2520domains%250Aand%2520complex%2520logical%2520sequences.%2520This%2520research%2520introduces%2520Proof%2520of%2520Thought%252C%2520a%250Aframework%2520that%2520enhances%2520the%2520reliability%2520and%2520transparency%2520of%2520LLM%2520outputs.%2520Our%250Aapproach%2520bridges%2520LLM-generated%2520ideas%2520with%2520formal%2520logic%2520verification%252C%2520employing%250Aa%2520custom%2520interpreter%2520to%2520convert%2520LLM%2520outputs%2520into%2520First%2520Order%2520Logic%2520constructs%250Afor%2520theorem%2520prover%2520scrutiny.%2520Central%2520to%2520our%2520method%2520is%2520an%2520intermediary%250AJSON-based%2520Domain-Specific%2520Language%252C%2520which%2520by%2520design%2520balances%2520precise%2520logical%250Astructures%2520with%2520intuitive%2520human%2520concepts.%2520This%2520hybrid%2520representation%2520enables%250Aboth%2520rigorous%2520validation%2520and%2520accessible%2520human%2520comprehension%2520of%2520LLM%2520reasoning%250Aprocesses.%2520Key%2520contributions%2520include%2520a%2520robust%2520type%2520system%2520with%2520sort%2520management%250Afor%2520enhanced%2520logical%2520integrity%252C%2520explicit%2520representation%2520of%2520rules%2520for%2520clear%250Adistinction%2520between%2520factual%2520and%2520inferential%2520knowledge%252C%2520and%2520a%2520flexible%250Aarchitecture%2520that%2520allows%2520for%2520easy%2520extension%2520to%2520various%2520domain-specific%250Aapplications.%2520We%2520demonstrate%2520Proof%2520of%2520Thought%2527s%2520effectiveness%2520through%250Abenchmarking%2520on%2520StrategyQA%2520and%2520a%2520novel%2520multimodal%2520reasoning%2520task%252C%2520showing%250Aimproved%2520performance%2520in%2520open-ended%2520scenarios.%2520By%2520providing%2520verifiable%2520and%250Ainterpretable%2520results%252C%2520our%2520technique%2520addresses%2520critical%2520needs%2520for%2520AI%2520system%250Aaccountability%2520and%2520sets%2520a%2520foundation%2520for%2520human-in-the-loop%2520oversight%2520in%250Ahigh-stakes%2520domains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.17270v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Proof%20of%20Thought%20%3A%20Neurosymbolic%20Program%20Synthesis%20allows%20Robust%20and%0A%20%20Interpretable%20Reasoning&entry.906535625=Debargha%20Ganguly%20and%20Srinivasan%20Iyengar%20and%20Vipin%20Chaudhary%20and%20Shivkumar%20Kalyanaraman&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20revolutionized%20natural%20language%20processing%2C%0Ayet%20they%20struggle%20with%20inconsistent%20reasoning%2C%20particularly%20in%20novel%20domains%0Aand%20complex%20logical%20sequences.%20This%20research%20introduces%20Proof%20of%20Thought%2C%20a%0Aframework%20that%20enhances%20the%20reliability%20and%20transparency%20of%20LLM%20outputs.%20Our%0Aapproach%20bridges%20LLM-generated%20ideas%20with%20formal%20logic%20verification%2C%20employing%0Aa%20custom%20interpreter%20to%20convert%20LLM%20outputs%20into%20First%20Order%20Logic%20constructs%0Afor%20theorem%20prover%20scrutiny.%20Central%20to%20our%20method%20is%20an%20intermediary%0AJSON-based%20Domain-Specific%20Language%2C%20which%20by%20design%20balances%20precise%20logical%0Astructures%20with%20intuitive%20human%20concepts.%20This%20hybrid%20representation%20enables%0Aboth%20rigorous%20validation%20and%20accessible%20human%20comprehension%20of%20LLM%20reasoning%0Aprocesses.%20Key%20contributions%20include%20a%20robust%20type%20system%20with%20sort%20management%0Afor%20enhanced%20logical%20integrity%2C%20explicit%20representation%20of%20rules%20for%20clear%0Adistinction%20between%20factual%20and%20inferential%20knowledge%2C%20and%20a%20flexible%0Aarchitecture%20that%20allows%20for%20easy%20extension%20to%20various%20domain-specific%0Aapplications.%20We%20demonstrate%20Proof%20of%20Thought%27s%20effectiveness%20through%0Abenchmarking%20on%20StrategyQA%20and%20a%20novel%20multimodal%20reasoning%20task%2C%20showing%0Aimproved%20performance%20in%20open-ended%20scenarios.%20By%20providing%20verifiable%20and%0Ainterpretable%20results%2C%20our%20technique%20addresses%20critical%20needs%20for%20AI%20system%0Aaccountability%20and%20sets%20a%20foundation%20for%20human-in-the-loop%20oversight%20in%0Ahigh-stakes%20domains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.17270v2&entry.124074799=Read"},
{"title": "Learning Lossless Compression for High Bit-Depth Volumetric Medical\n  Image", "author": "Kai Wang and Yuanchao Bai and Daxin Li and Deming Zhai and Junjun Jiang and Xianming Liu", "abstract": "  Recent advances in learning-based methods have markedly enhanced the\ncapabilities of image compression. However, these methods struggle with high\nbit-depth volumetric medical images, facing issues such as degraded\nperformance, increased memory demand, and reduced processing speed. To address\nthese challenges, this paper presents the Bit-Division based Lossless\nVolumetric Image Compression (BD-LVIC) framework, which is tailored for high\nbit-depth medical volume compression. The BD-LVIC framework skillfully divides\nthe high bit-depth volume into two lower bit-depth segments: the Most\nSignificant Bit-Volume (MSBV) and the Least Significant Bit-Volume (LSBV). The\nMSBV concentrates on the most significant bits of the volumetric medical image,\ncapturing vital structural details in a compact manner. This reduction in\ncomplexity greatly improves compression efficiency using traditional codecs.\nConversely, the LSBV deals with the least significant bits, which encapsulate\nintricate texture details. To compress this detailed information effectively,\nwe introduce an effective learning-based compression model equipped with a\nTransformer-Based Feature Alignment Module, which exploits both intra-slice and\ninter-slice redundancies to accurately align features. Subsequently, a Parallel\nAutoregressive Coding Module merges these features to precisely estimate the\nprobability distribution of the least significant bit-planes. Our extensive\ntesting demonstrates that the BD-LVIC framework not only sets new performance\nbenchmarks across various datasets but also maintains a competitive coding\nspeed, highlighting its significant potential and practical utility in the\nrealm of volumetric medical image compression.\n", "link": "http://arxiv.org/abs/2410.17814v1", "date": "2024-10-23", "relevancy": 2.1418, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5455}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5422}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Lossless%20Compression%20for%20High%20Bit-Depth%20Volumetric%20Medical%0A%20%20Image&body=Title%3A%20Learning%20Lossless%20Compression%20for%20High%20Bit-Depth%20Volumetric%20Medical%0A%20%20Image%0AAuthor%3A%20Kai%20Wang%20and%20Yuanchao%20Bai%20and%20Daxin%20Li%20and%20Deming%20Zhai%20and%20Junjun%20Jiang%20and%20Xianming%20Liu%0AAbstract%3A%20%20%20Recent%20advances%20in%20learning-based%20methods%20have%20markedly%20enhanced%20the%0Acapabilities%20of%20image%20compression.%20However%2C%20these%20methods%20struggle%20with%20high%0Abit-depth%20volumetric%20medical%20images%2C%20facing%20issues%20such%20as%20degraded%0Aperformance%2C%20increased%20memory%20demand%2C%20and%20reduced%20processing%20speed.%20To%20address%0Athese%20challenges%2C%20this%20paper%20presents%20the%20Bit-Division%20based%20Lossless%0AVolumetric%20Image%20Compression%20%28BD-LVIC%29%20framework%2C%20which%20is%20tailored%20for%20high%0Abit-depth%20medical%20volume%20compression.%20The%20BD-LVIC%20framework%20skillfully%20divides%0Athe%20high%20bit-depth%20volume%20into%20two%20lower%20bit-depth%20segments%3A%20the%20Most%0ASignificant%20Bit-Volume%20%28MSBV%29%20and%20the%20Least%20Significant%20Bit-Volume%20%28LSBV%29.%20The%0AMSBV%20concentrates%20on%20the%20most%20significant%20bits%20of%20the%20volumetric%20medical%20image%2C%0Acapturing%20vital%20structural%20details%20in%20a%20compact%20manner.%20This%20reduction%20in%0Acomplexity%20greatly%20improves%20compression%20efficiency%20using%20traditional%20codecs.%0AConversely%2C%20the%20LSBV%20deals%20with%20the%20least%20significant%20bits%2C%20which%20encapsulate%0Aintricate%20texture%20details.%20To%20compress%20this%20detailed%20information%20effectively%2C%0Awe%20introduce%20an%20effective%20learning-based%20compression%20model%20equipped%20with%20a%0ATransformer-Based%20Feature%20Alignment%20Module%2C%20which%20exploits%20both%20intra-slice%20and%0Ainter-slice%20redundancies%20to%20accurately%20align%20features.%20Subsequently%2C%20a%20Parallel%0AAutoregressive%20Coding%20Module%20merges%20these%20features%20to%20precisely%20estimate%20the%0Aprobability%20distribution%20of%20the%20least%20significant%20bit-planes.%20Our%20extensive%0Atesting%20demonstrates%20that%20the%20BD-LVIC%20framework%20not%20only%20sets%20new%20performance%0Abenchmarks%20across%20various%20datasets%20but%20also%20maintains%20a%20competitive%20coding%0Aspeed%2C%20highlighting%20its%20significant%20potential%20and%20practical%20utility%20in%20the%0Arealm%20of%20volumetric%20medical%20image%20compression.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17814v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Lossless%2520Compression%2520for%2520High%2520Bit-Depth%2520Volumetric%2520Medical%250A%2520%2520Image%26entry.906535625%3DKai%2520Wang%2520and%2520Yuanchao%2520Bai%2520and%2520Daxin%2520Li%2520and%2520Deming%2520Zhai%2520and%2520Junjun%2520Jiang%2520and%2520Xianming%2520Liu%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520learning-based%2520methods%2520have%2520markedly%2520enhanced%2520the%250Acapabilities%2520of%2520image%2520compression.%2520However%252C%2520these%2520methods%2520struggle%2520with%2520high%250Abit-depth%2520volumetric%2520medical%2520images%252C%2520facing%2520issues%2520such%2520as%2520degraded%250Aperformance%252C%2520increased%2520memory%2520demand%252C%2520and%2520reduced%2520processing%2520speed.%2520To%2520address%250Athese%2520challenges%252C%2520this%2520paper%2520presents%2520the%2520Bit-Division%2520based%2520Lossless%250AVolumetric%2520Image%2520Compression%2520%2528BD-LVIC%2529%2520framework%252C%2520which%2520is%2520tailored%2520for%2520high%250Abit-depth%2520medical%2520volume%2520compression.%2520The%2520BD-LVIC%2520framework%2520skillfully%2520divides%250Athe%2520high%2520bit-depth%2520volume%2520into%2520two%2520lower%2520bit-depth%2520segments%253A%2520the%2520Most%250ASignificant%2520Bit-Volume%2520%2528MSBV%2529%2520and%2520the%2520Least%2520Significant%2520Bit-Volume%2520%2528LSBV%2529.%2520The%250AMSBV%2520concentrates%2520on%2520the%2520most%2520significant%2520bits%2520of%2520the%2520volumetric%2520medical%2520image%252C%250Acapturing%2520vital%2520structural%2520details%2520in%2520a%2520compact%2520manner.%2520This%2520reduction%2520in%250Acomplexity%2520greatly%2520improves%2520compression%2520efficiency%2520using%2520traditional%2520codecs.%250AConversely%252C%2520the%2520LSBV%2520deals%2520with%2520the%2520least%2520significant%2520bits%252C%2520which%2520encapsulate%250Aintricate%2520texture%2520details.%2520To%2520compress%2520this%2520detailed%2520information%2520effectively%252C%250Awe%2520introduce%2520an%2520effective%2520learning-based%2520compression%2520model%2520equipped%2520with%2520a%250ATransformer-Based%2520Feature%2520Alignment%2520Module%252C%2520which%2520exploits%2520both%2520intra-slice%2520and%250Ainter-slice%2520redundancies%2520to%2520accurately%2520align%2520features.%2520Subsequently%252C%2520a%2520Parallel%250AAutoregressive%2520Coding%2520Module%2520merges%2520these%2520features%2520to%2520precisely%2520estimate%2520the%250Aprobability%2520distribution%2520of%2520the%2520least%2520significant%2520bit-planes.%2520Our%2520extensive%250Atesting%2520demonstrates%2520that%2520the%2520BD-LVIC%2520framework%2520not%2520only%2520sets%2520new%2520performance%250Abenchmarks%2520across%2520various%2520datasets%2520but%2520also%2520maintains%2520a%2520competitive%2520coding%250Aspeed%252C%2520highlighting%2520its%2520significant%2520potential%2520and%2520practical%2520utility%2520in%2520the%250Arealm%2520of%2520volumetric%2520medical%2520image%2520compression.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17814v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Lossless%20Compression%20for%20High%20Bit-Depth%20Volumetric%20Medical%0A%20%20Image&entry.906535625=Kai%20Wang%20and%20Yuanchao%20Bai%20and%20Daxin%20Li%20and%20Deming%20Zhai%20and%20Junjun%20Jiang%20and%20Xianming%20Liu&entry.1292438233=%20%20Recent%20advances%20in%20learning-based%20methods%20have%20markedly%20enhanced%20the%0Acapabilities%20of%20image%20compression.%20However%2C%20these%20methods%20struggle%20with%20high%0Abit-depth%20volumetric%20medical%20images%2C%20facing%20issues%20such%20as%20degraded%0Aperformance%2C%20increased%20memory%20demand%2C%20and%20reduced%20processing%20speed.%20To%20address%0Athese%20challenges%2C%20this%20paper%20presents%20the%20Bit-Division%20based%20Lossless%0AVolumetric%20Image%20Compression%20%28BD-LVIC%29%20framework%2C%20which%20is%20tailored%20for%20high%0Abit-depth%20medical%20volume%20compression.%20The%20BD-LVIC%20framework%20skillfully%20divides%0Athe%20high%20bit-depth%20volume%20into%20two%20lower%20bit-depth%20segments%3A%20the%20Most%0ASignificant%20Bit-Volume%20%28MSBV%29%20and%20the%20Least%20Significant%20Bit-Volume%20%28LSBV%29.%20The%0AMSBV%20concentrates%20on%20the%20most%20significant%20bits%20of%20the%20volumetric%20medical%20image%2C%0Acapturing%20vital%20structural%20details%20in%20a%20compact%20manner.%20This%20reduction%20in%0Acomplexity%20greatly%20improves%20compression%20efficiency%20using%20traditional%20codecs.%0AConversely%2C%20the%20LSBV%20deals%20with%20the%20least%20significant%20bits%2C%20which%20encapsulate%0Aintricate%20texture%20details.%20To%20compress%20this%20detailed%20information%20effectively%2C%0Awe%20introduce%20an%20effective%20learning-based%20compression%20model%20equipped%20with%20a%0ATransformer-Based%20Feature%20Alignment%20Module%2C%20which%20exploits%20both%20intra-slice%20and%0Ainter-slice%20redundancies%20to%20accurately%20align%20features.%20Subsequently%2C%20a%20Parallel%0AAutoregressive%20Coding%20Module%20merges%20these%20features%20to%20precisely%20estimate%20the%0Aprobability%20distribution%20of%20the%20least%20significant%20bit-planes.%20Our%20extensive%0Atesting%20demonstrates%20that%20the%20BD-LVIC%20framework%20not%20only%20sets%20new%20performance%0Abenchmarks%20across%20various%20datasets%20but%20also%20maintains%20a%20competitive%20coding%0Aspeed%2C%20highlighting%20its%20significant%20potential%20and%20practical%20utility%20in%20the%0Arealm%20of%20volumetric%20medical%20image%20compression.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17814v1&entry.124074799=Read"},
{"title": "Telling Stories for Common Sense Zero-Shot Action Recognition", "author": "Shreyank N Gowda and Laura Sevilla-Lara", "abstract": "  Video understanding has long suffered from reliance on large labeled\ndatasets, motivating research into zero-shot learning. Recent progress in\nlanguage modeling presents opportunities to advance zero-shot video analysis,\nbut constructing an effective semantic space relating action classes remains\nchallenging. We address this by introducing a novel dataset, Stories, which\ncontains rich textual descriptions for diverse action classes extracted from\nWikiHow articles. For each class, we extract multi-sentence narratives\ndetailing the necessary steps, scenes, objects, and verbs that characterize the\naction. This contextual data enables modeling of nuanced relationships between\nactions, paving the way for zero-shot transfer. We also propose an approach\nthat harnesses Stories to improve feature generation for training zero-shot\nclassification. Without any target dataset fine-tuning, our method achieves new\nstate-of-the-art on multiple benchmarks, improving top-1 accuracy by up to\n6.1%. We believe Stories provides a valuable resource that can catalyze\nprogress in zero-shot action recognition. The textual narratives forge\nconnections between seen and unseen classes, overcoming the bottleneck of\nlabeled data that has long impeded advancements in this exciting domain. The\ndata can be found here: https://github.com/kini5gowda/Stories .\n", "link": "http://arxiv.org/abs/2309.17327v2", "date": "2024-10-23", "relevancy": 2.1231, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5336}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5335}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Telling%20Stories%20for%20Common%20Sense%20Zero-Shot%20Action%20Recognition&body=Title%3A%20Telling%20Stories%20for%20Common%20Sense%20Zero-Shot%20Action%20Recognition%0AAuthor%3A%20Shreyank%20N%20Gowda%20and%20Laura%20Sevilla-Lara%0AAbstract%3A%20%20%20Video%20understanding%20has%20long%20suffered%20from%20reliance%20on%20large%20labeled%0Adatasets%2C%20motivating%20research%20into%20zero-shot%20learning.%20Recent%20progress%20in%0Alanguage%20modeling%20presents%20opportunities%20to%20advance%20zero-shot%20video%20analysis%2C%0Abut%20constructing%20an%20effective%20semantic%20space%20relating%20action%20classes%20remains%0Achallenging.%20We%20address%20this%20by%20introducing%20a%20novel%20dataset%2C%20Stories%2C%20which%0Acontains%20rich%20textual%20descriptions%20for%20diverse%20action%20classes%20extracted%20from%0AWikiHow%20articles.%20For%20each%20class%2C%20we%20extract%20multi-sentence%20narratives%0Adetailing%20the%20necessary%20steps%2C%20scenes%2C%20objects%2C%20and%20verbs%20that%20characterize%20the%0Aaction.%20This%20contextual%20data%20enables%20modeling%20of%20nuanced%20relationships%20between%0Aactions%2C%20paving%20the%20way%20for%20zero-shot%20transfer.%20We%20also%20propose%20an%20approach%0Athat%20harnesses%20Stories%20to%20improve%20feature%20generation%20for%20training%20zero-shot%0Aclassification.%20Without%20any%20target%20dataset%20fine-tuning%2C%20our%20method%20achieves%20new%0Astate-of-the-art%20on%20multiple%20benchmarks%2C%20improving%20top-1%20accuracy%20by%20up%20to%0A6.1%25.%20We%20believe%20Stories%20provides%20a%20valuable%20resource%20that%20can%20catalyze%0Aprogress%20in%20zero-shot%20action%20recognition.%20The%20textual%20narratives%20forge%0Aconnections%20between%20seen%20and%20unseen%20classes%2C%20overcoming%20the%20bottleneck%20of%0Alabeled%20data%20that%20has%20long%20impeded%20advancements%20in%20this%20exciting%20domain.%20The%0Adata%20can%20be%20found%20here%3A%20https%3A//github.com/kini5gowda/Stories%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.17327v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTelling%2520Stories%2520for%2520Common%2520Sense%2520Zero-Shot%2520Action%2520Recognition%26entry.906535625%3DShreyank%2520N%2520Gowda%2520and%2520Laura%2520Sevilla-Lara%26entry.1292438233%3D%2520%2520Video%2520understanding%2520has%2520long%2520suffered%2520from%2520reliance%2520on%2520large%2520labeled%250Adatasets%252C%2520motivating%2520research%2520into%2520zero-shot%2520learning.%2520Recent%2520progress%2520in%250Alanguage%2520modeling%2520presents%2520opportunities%2520to%2520advance%2520zero-shot%2520video%2520analysis%252C%250Abut%2520constructing%2520an%2520effective%2520semantic%2520space%2520relating%2520action%2520classes%2520remains%250Achallenging.%2520We%2520address%2520this%2520by%2520introducing%2520a%2520novel%2520dataset%252C%2520Stories%252C%2520which%250Acontains%2520rich%2520textual%2520descriptions%2520for%2520diverse%2520action%2520classes%2520extracted%2520from%250AWikiHow%2520articles.%2520For%2520each%2520class%252C%2520we%2520extract%2520multi-sentence%2520narratives%250Adetailing%2520the%2520necessary%2520steps%252C%2520scenes%252C%2520objects%252C%2520and%2520verbs%2520that%2520characterize%2520the%250Aaction.%2520This%2520contextual%2520data%2520enables%2520modeling%2520of%2520nuanced%2520relationships%2520between%250Aactions%252C%2520paving%2520the%2520way%2520for%2520zero-shot%2520transfer.%2520We%2520also%2520propose%2520an%2520approach%250Athat%2520harnesses%2520Stories%2520to%2520improve%2520feature%2520generation%2520for%2520training%2520zero-shot%250Aclassification.%2520Without%2520any%2520target%2520dataset%2520fine-tuning%252C%2520our%2520method%2520achieves%2520new%250Astate-of-the-art%2520on%2520multiple%2520benchmarks%252C%2520improving%2520top-1%2520accuracy%2520by%2520up%2520to%250A6.1%2525.%2520We%2520believe%2520Stories%2520provides%2520a%2520valuable%2520resource%2520that%2520can%2520catalyze%250Aprogress%2520in%2520zero-shot%2520action%2520recognition.%2520The%2520textual%2520narratives%2520forge%250Aconnections%2520between%2520seen%2520and%2520unseen%2520classes%252C%2520overcoming%2520the%2520bottleneck%2520of%250Alabeled%2520data%2520that%2520has%2520long%2520impeded%2520advancements%2520in%2520this%2520exciting%2520domain.%2520The%250Adata%2520can%2520be%2520found%2520here%253A%2520https%253A//github.com/kini5gowda/Stories%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.17327v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Telling%20Stories%20for%20Common%20Sense%20Zero-Shot%20Action%20Recognition&entry.906535625=Shreyank%20N%20Gowda%20and%20Laura%20Sevilla-Lara&entry.1292438233=%20%20Video%20understanding%20has%20long%20suffered%20from%20reliance%20on%20large%20labeled%0Adatasets%2C%20motivating%20research%20into%20zero-shot%20learning.%20Recent%20progress%20in%0Alanguage%20modeling%20presents%20opportunities%20to%20advance%20zero-shot%20video%20analysis%2C%0Abut%20constructing%20an%20effective%20semantic%20space%20relating%20action%20classes%20remains%0Achallenging.%20We%20address%20this%20by%20introducing%20a%20novel%20dataset%2C%20Stories%2C%20which%0Acontains%20rich%20textual%20descriptions%20for%20diverse%20action%20classes%20extracted%20from%0AWikiHow%20articles.%20For%20each%20class%2C%20we%20extract%20multi-sentence%20narratives%0Adetailing%20the%20necessary%20steps%2C%20scenes%2C%20objects%2C%20and%20verbs%20that%20characterize%20the%0Aaction.%20This%20contextual%20data%20enables%20modeling%20of%20nuanced%20relationships%20between%0Aactions%2C%20paving%20the%20way%20for%20zero-shot%20transfer.%20We%20also%20propose%20an%20approach%0Athat%20harnesses%20Stories%20to%20improve%20feature%20generation%20for%20training%20zero-shot%0Aclassification.%20Without%20any%20target%20dataset%20fine-tuning%2C%20our%20method%20achieves%20new%0Astate-of-the-art%20on%20multiple%20benchmarks%2C%20improving%20top-1%20accuracy%20by%20up%20to%0A6.1%25.%20We%20believe%20Stories%20provides%20a%20valuable%20resource%20that%20can%20catalyze%0Aprogress%20in%20zero-shot%20action%20recognition.%20The%20textual%20narratives%20forge%0Aconnections%20between%20seen%20and%20unseen%20classes%2C%20overcoming%20the%20bottleneck%20of%0Alabeled%20data%20that%20has%20long%20impeded%20advancements%20in%20this%20exciting%20domain.%20The%0Adata%20can%20be%20found%20here%3A%20https%3A//github.com/kini5gowda/Stories%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.17327v2&entry.124074799=Read"},
{"title": "Few-Shot Adversarial Prompt Learning on Vision-Language Models", "author": "Yiwei Zhou and Xiaobo Xia and Zhiwei Lin and Bo Han and Tongliang Liu", "abstract": "  The vulnerability of deep neural networks to imperceptible adversarial\nperturbations has attracted widespread attention. Inspired by the success of\nvision-language foundation models, previous efforts achieved zero-shot\nadversarial robustness by aligning adversarial visual features with text\nsupervision. However, in practice, they are still unsatisfactory due to several\nissues, including heavy adaptation cost, suboptimal text supervision, and\nuncontrolled natural generalization capacity. In this paper, to address these\nissues, we propose a few-shot adversarial prompt framework where adapting input\nsequences with limited data makes significant adversarial robustness\nimprovement. Specifically, we achieve this by providing adversarially\ncorrelated text supervision that is end-to-end learned from adversarial\nexamples. We also propose a novel training objective that enhances the\nconsistency of multi-modal features while encourages differentiated uni-modal\nfeatures between natural and adversarial examples. The proposed framework gives\naccess to learn adversarial text supervision, which provides superior\ncross-modal adversarial alignment and matches state-of-the-art zero-shot\nadversarial robustness with only 1% training data. Code is available at:\nhttps://github.com/lionel-w2/FAP.\n", "link": "http://arxiv.org/abs/2403.14774v2", "date": "2024-10-23", "relevancy": 2.1199, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5525}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5295}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-Shot%20Adversarial%20Prompt%20Learning%20on%20Vision-Language%20Models&body=Title%3A%20Few-Shot%20Adversarial%20Prompt%20Learning%20on%20Vision-Language%20Models%0AAuthor%3A%20Yiwei%20Zhou%20and%20Xiaobo%20Xia%20and%20Zhiwei%20Lin%20and%20Bo%20Han%20and%20Tongliang%20Liu%0AAbstract%3A%20%20%20The%20vulnerability%20of%20deep%20neural%20networks%20to%20imperceptible%20adversarial%0Aperturbations%20has%20attracted%20widespread%20attention.%20Inspired%20by%20the%20success%20of%0Avision-language%20foundation%20models%2C%20previous%20efforts%20achieved%20zero-shot%0Aadversarial%20robustness%20by%20aligning%20adversarial%20visual%20features%20with%20text%0Asupervision.%20However%2C%20in%20practice%2C%20they%20are%20still%20unsatisfactory%20due%20to%20several%0Aissues%2C%20including%20heavy%20adaptation%20cost%2C%20suboptimal%20text%20supervision%2C%20and%0Auncontrolled%20natural%20generalization%20capacity.%20In%20this%20paper%2C%20to%20address%20these%0Aissues%2C%20we%20propose%20a%20few-shot%20adversarial%20prompt%20framework%20where%20adapting%20input%0Asequences%20with%20limited%20data%20makes%20significant%20adversarial%20robustness%0Aimprovement.%20Specifically%2C%20we%20achieve%20this%20by%20providing%20adversarially%0Acorrelated%20text%20supervision%20that%20is%20end-to-end%20learned%20from%20adversarial%0Aexamples.%20We%20also%20propose%20a%20novel%20training%20objective%20that%20enhances%20the%0Aconsistency%20of%20multi-modal%20features%20while%20encourages%20differentiated%20uni-modal%0Afeatures%20between%20natural%20and%20adversarial%20examples.%20The%20proposed%20framework%20gives%0Aaccess%20to%20learn%20adversarial%20text%20supervision%2C%20which%20provides%20superior%0Across-modal%20adversarial%20alignment%20and%20matches%20state-of-the-art%20zero-shot%0Aadversarial%20robustness%20with%20only%201%25%20training%20data.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/lionel-w2/FAP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.14774v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-Shot%2520Adversarial%2520Prompt%2520Learning%2520on%2520Vision-Language%2520Models%26entry.906535625%3DYiwei%2520Zhou%2520and%2520Xiaobo%2520Xia%2520and%2520Zhiwei%2520Lin%2520and%2520Bo%2520Han%2520and%2520Tongliang%2520Liu%26entry.1292438233%3D%2520%2520The%2520vulnerability%2520of%2520deep%2520neural%2520networks%2520to%2520imperceptible%2520adversarial%250Aperturbations%2520has%2520attracted%2520widespread%2520attention.%2520Inspired%2520by%2520the%2520success%2520of%250Avision-language%2520foundation%2520models%252C%2520previous%2520efforts%2520achieved%2520zero-shot%250Aadversarial%2520robustness%2520by%2520aligning%2520adversarial%2520visual%2520features%2520with%2520text%250Asupervision.%2520However%252C%2520in%2520practice%252C%2520they%2520are%2520still%2520unsatisfactory%2520due%2520to%2520several%250Aissues%252C%2520including%2520heavy%2520adaptation%2520cost%252C%2520suboptimal%2520text%2520supervision%252C%2520and%250Auncontrolled%2520natural%2520generalization%2520capacity.%2520In%2520this%2520paper%252C%2520to%2520address%2520these%250Aissues%252C%2520we%2520propose%2520a%2520few-shot%2520adversarial%2520prompt%2520framework%2520where%2520adapting%2520input%250Asequences%2520with%2520limited%2520data%2520makes%2520significant%2520adversarial%2520robustness%250Aimprovement.%2520Specifically%252C%2520we%2520achieve%2520this%2520by%2520providing%2520adversarially%250Acorrelated%2520text%2520supervision%2520that%2520is%2520end-to-end%2520learned%2520from%2520adversarial%250Aexamples.%2520We%2520also%2520propose%2520a%2520novel%2520training%2520objective%2520that%2520enhances%2520the%250Aconsistency%2520of%2520multi-modal%2520features%2520while%2520encourages%2520differentiated%2520uni-modal%250Afeatures%2520between%2520natural%2520and%2520adversarial%2520examples.%2520The%2520proposed%2520framework%2520gives%250Aaccess%2520to%2520learn%2520adversarial%2520text%2520supervision%252C%2520which%2520provides%2520superior%250Across-modal%2520adversarial%2520alignment%2520and%2520matches%2520state-of-the-art%2520zero-shot%250Aadversarial%2520robustness%2520with%2520only%25201%2525%2520training%2520data.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/lionel-w2/FAP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.14774v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-Shot%20Adversarial%20Prompt%20Learning%20on%20Vision-Language%20Models&entry.906535625=Yiwei%20Zhou%20and%20Xiaobo%20Xia%20and%20Zhiwei%20Lin%20and%20Bo%20Han%20and%20Tongliang%20Liu&entry.1292438233=%20%20The%20vulnerability%20of%20deep%20neural%20networks%20to%20imperceptible%20adversarial%0Aperturbations%20has%20attracted%20widespread%20attention.%20Inspired%20by%20the%20success%20of%0Avision-language%20foundation%20models%2C%20previous%20efforts%20achieved%20zero-shot%0Aadversarial%20robustness%20by%20aligning%20adversarial%20visual%20features%20with%20text%0Asupervision.%20However%2C%20in%20practice%2C%20they%20are%20still%20unsatisfactory%20due%20to%20several%0Aissues%2C%20including%20heavy%20adaptation%20cost%2C%20suboptimal%20text%20supervision%2C%20and%0Auncontrolled%20natural%20generalization%20capacity.%20In%20this%20paper%2C%20to%20address%20these%0Aissues%2C%20we%20propose%20a%20few-shot%20adversarial%20prompt%20framework%20where%20adapting%20input%0Asequences%20with%20limited%20data%20makes%20significant%20adversarial%20robustness%0Aimprovement.%20Specifically%2C%20we%20achieve%20this%20by%20providing%20adversarially%0Acorrelated%20text%20supervision%20that%20is%20end-to-end%20learned%20from%20adversarial%0Aexamples.%20We%20also%20propose%20a%20novel%20training%20objective%20that%20enhances%20the%0Aconsistency%20of%20multi-modal%20features%20while%20encourages%20differentiated%20uni-modal%0Afeatures%20between%20natural%20and%20adversarial%20examples.%20The%20proposed%20framework%20gives%0Aaccess%20to%20learn%20adversarial%20text%20supervision%2C%20which%20provides%20superior%0Across-modal%20adversarial%20alignment%20and%20matches%20state-of-the-art%20zero-shot%0Aadversarial%20robustness%20with%20only%201%25%20training%20data.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/lionel-w2/FAP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.14774v2&entry.124074799=Read"},
{"title": "Counter-Current Learning: A Biologically Plausible Dual Network Approach\n  for Deep Learning", "author": "Chia-Hsiang Kao and Bharath Hariharan", "abstract": "  Despite its widespread use in neural networks, error backpropagation has\nfaced criticism for its lack of biological plausibility, suffering from issues\nsuch as the backward locking problem and the weight transport problem. These\nlimitations have motivated researchers to explore more biologically plausible\nlearning algorithms that could potentially shed light on how biological neural\nsystems adapt and learn. Inspired by the counter-current exchange mechanisms\nobserved in biological systems, we propose counter-current learning (CCL), a\nbiologically plausible framework for credit assignment in neural networks. This\nframework employs a feedforward network to process input data and a feedback\nnetwork to process targets, with each network enhancing the other through\nanti-parallel signal propagation. By leveraging the more informative signals\nfrom the bottom layer of the feedback network to guide the updates of the top\nlayer of the feedforward network and vice versa, CCL enables the simultaneous\ntransformation of source inputs to target outputs and the dynamic mutual\ninfluence of these transformations. Experimental results on MNIST,\nFashionMNIST, CIFAR10, and CIFAR100 datasets using multi-layer perceptrons and\nconvolutional neural networks demonstrate that CCL achieves comparable\nperformance to other biologically plausible algorithms while offering a more\nbiologically realistic learning mechanism. Furthermore, we showcase the\napplicability of our approach to an autoencoder task, underscoring its\npotential for unsupervised representation learning. Our work presents a\ndirection for biologically inspired and plausible learning algorithms, offering\nan alternative mechanism of learning and adaptation in neural networks.\n", "link": "http://arxiv.org/abs/2409.19841v2", "date": "2024-10-23", "relevancy": 2.1029, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5434}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5141}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5127}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Counter-Current%20Learning%3A%20A%20Biologically%20Plausible%20Dual%20Network%20Approach%0A%20%20for%20Deep%20Learning&body=Title%3A%20Counter-Current%20Learning%3A%20A%20Biologically%20Plausible%20Dual%20Network%20Approach%0A%20%20for%20Deep%20Learning%0AAuthor%3A%20Chia-Hsiang%20Kao%20and%20Bharath%20Hariharan%0AAbstract%3A%20%20%20Despite%20its%20widespread%20use%20in%20neural%20networks%2C%20error%20backpropagation%20has%0Afaced%20criticism%20for%20its%20lack%20of%20biological%20plausibility%2C%20suffering%20from%20issues%0Asuch%20as%20the%20backward%20locking%20problem%20and%20the%20weight%20transport%20problem.%20These%0Alimitations%20have%20motivated%20researchers%20to%20explore%20more%20biologically%20plausible%0Alearning%20algorithms%20that%20could%20potentially%20shed%20light%20on%20how%20biological%20neural%0Asystems%20adapt%20and%20learn.%20Inspired%20by%20the%20counter-current%20exchange%20mechanisms%0Aobserved%20in%20biological%20systems%2C%20we%20propose%20counter-current%20learning%20%28CCL%29%2C%20a%0Abiologically%20plausible%20framework%20for%20credit%20assignment%20in%20neural%20networks.%20This%0Aframework%20employs%20a%20feedforward%20network%20to%20process%20input%20data%20and%20a%20feedback%0Anetwork%20to%20process%20targets%2C%20with%20each%20network%20enhancing%20the%20other%20through%0Aanti-parallel%20signal%20propagation.%20By%20leveraging%20the%20more%20informative%20signals%0Afrom%20the%20bottom%20layer%20of%20the%20feedback%20network%20to%20guide%20the%20updates%20of%20the%20top%0Alayer%20of%20the%20feedforward%20network%20and%20vice%20versa%2C%20CCL%20enables%20the%20simultaneous%0Atransformation%20of%20source%20inputs%20to%20target%20outputs%20and%20the%20dynamic%20mutual%0Ainfluence%20of%20these%20transformations.%20Experimental%20results%20on%20MNIST%2C%0AFashionMNIST%2C%20CIFAR10%2C%20and%20CIFAR100%20datasets%20using%20multi-layer%20perceptrons%20and%0Aconvolutional%20neural%20networks%20demonstrate%20that%20CCL%20achieves%20comparable%0Aperformance%20to%20other%20biologically%20plausible%20algorithms%20while%20offering%20a%20more%0Abiologically%20realistic%20learning%20mechanism.%20Furthermore%2C%20we%20showcase%20the%0Aapplicability%20of%20our%20approach%20to%20an%20autoencoder%20task%2C%20underscoring%20its%0Apotential%20for%20unsupervised%20representation%20learning.%20Our%20work%20presents%20a%0Adirection%20for%20biologically%20inspired%20and%20plausible%20learning%20algorithms%2C%20offering%0Aan%20alternative%20mechanism%20of%20learning%20and%20adaptation%20in%20neural%20networks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2409.19841v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCounter-Current%2520Learning%253A%2520A%2520Biologically%2520Plausible%2520Dual%2520Network%2520Approach%250A%2520%2520for%2520Deep%2520Learning%26entry.906535625%3DChia-Hsiang%2520Kao%2520and%2520Bharath%2520Hariharan%26entry.1292438233%3D%2520%2520Despite%2520its%2520widespread%2520use%2520in%2520neural%2520networks%252C%2520error%2520backpropagation%2520has%250Afaced%2520criticism%2520for%2520its%2520lack%2520of%2520biological%2520plausibility%252C%2520suffering%2520from%2520issues%250Asuch%2520as%2520the%2520backward%2520locking%2520problem%2520and%2520the%2520weight%2520transport%2520problem.%2520These%250Alimitations%2520have%2520motivated%2520researchers%2520to%2520explore%2520more%2520biologically%2520plausible%250Alearning%2520algorithms%2520that%2520could%2520potentially%2520shed%2520light%2520on%2520how%2520biological%2520neural%250Asystems%2520adapt%2520and%2520learn.%2520Inspired%2520by%2520the%2520counter-current%2520exchange%2520mechanisms%250Aobserved%2520in%2520biological%2520systems%252C%2520we%2520propose%2520counter-current%2520learning%2520%2528CCL%2529%252C%2520a%250Abiologically%2520plausible%2520framework%2520for%2520credit%2520assignment%2520in%2520neural%2520networks.%2520This%250Aframework%2520employs%2520a%2520feedforward%2520network%2520to%2520process%2520input%2520data%2520and%2520a%2520feedback%250Anetwork%2520to%2520process%2520targets%252C%2520with%2520each%2520network%2520enhancing%2520the%2520other%2520through%250Aanti-parallel%2520signal%2520propagation.%2520By%2520leveraging%2520the%2520more%2520informative%2520signals%250Afrom%2520the%2520bottom%2520layer%2520of%2520the%2520feedback%2520network%2520to%2520guide%2520the%2520updates%2520of%2520the%2520top%250Alayer%2520of%2520the%2520feedforward%2520network%2520and%2520vice%2520versa%252C%2520CCL%2520enables%2520the%2520simultaneous%250Atransformation%2520of%2520source%2520inputs%2520to%2520target%2520outputs%2520and%2520the%2520dynamic%2520mutual%250Ainfluence%2520of%2520these%2520transformations.%2520Experimental%2520results%2520on%2520MNIST%252C%250AFashionMNIST%252C%2520CIFAR10%252C%2520and%2520CIFAR100%2520datasets%2520using%2520multi-layer%2520perceptrons%2520and%250Aconvolutional%2520neural%2520networks%2520demonstrate%2520that%2520CCL%2520achieves%2520comparable%250Aperformance%2520to%2520other%2520biologically%2520plausible%2520algorithms%2520while%2520offering%2520a%2520more%250Abiologically%2520realistic%2520learning%2520mechanism.%2520Furthermore%252C%2520we%2520showcase%2520the%250Aapplicability%2520of%2520our%2520approach%2520to%2520an%2520autoencoder%2520task%252C%2520underscoring%2520its%250Apotential%2520for%2520unsupervised%2520representation%2520learning.%2520Our%2520work%2520presents%2520a%250Adirection%2520for%2520biologically%2520inspired%2520and%2520plausible%2520learning%2520algorithms%252C%2520offering%250Aan%2520alternative%2520mechanism%2520of%2520learning%2520and%2520adaptation%2520in%2520neural%2520networks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.19841v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Counter-Current%20Learning%3A%20A%20Biologically%20Plausible%20Dual%20Network%20Approach%0A%20%20for%20Deep%20Learning&entry.906535625=Chia-Hsiang%20Kao%20and%20Bharath%20Hariharan&entry.1292438233=%20%20Despite%20its%20widespread%20use%20in%20neural%20networks%2C%20error%20backpropagation%20has%0Afaced%20criticism%20for%20its%20lack%20of%20biological%20plausibility%2C%20suffering%20from%20issues%0Asuch%20as%20the%20backward%20locking%20problem%20and%20the%20weight%20transport%20problem.%20These%0Alimitations%20have%20motivated%20researchers%20to%20explore%20more%20biologically%20plausible%0Alearning%20algorithms%20that%20could%20potentially%20shed%20light%20on%20how%20biological%20neural%0Asystems%20adapt%20and%20learn.%20Inspired%20by%20the%20counter-current%20exchange%20mechanisms%0Aobserved%20in%20biological%20systems%2C%20we%20propose%20counter-current%20learning%20%28CCL%29%2C%20a%0Abiologically%20plausible%20framework%20for%20credit%20assignment%20in%20neural%20networks.%20This%0Aframework%20employs%20a%20feedforward%20network%20to%20process%20input%20data%20and%20a%20feedback%0Anetwork%20to%20process%20targets%2C%20with%20each%20network%20enhancing%20the%20other%20through%0Aanti-parallel%20signal%20propagation.%20By%20leveraging%20the%20more%20informative%20signals%0Afrom%20the%20bottom%20layer%20of%20the%20feedback%20network%20to%20guide%20the%20updates%20of%20the%20top%0Alayer%20of%20the%20feedforward%20network%20and%20vice%20versa%2C%20CCL%20enables%20the%20simultaneous%0Atransformation%20of%20source%20inputs%20to%20target%20outputs%20and%20the%20dynamic%20mutual%0Ainfluence%20of%20these%20transformations.%20Experimental%20results%20on%20MNIST%2C%0AFashionMNIST%2C%20CIFAR10%2C%20and%20CIFAR100%20datasets%20using%20multi-layer%20perceptrons%20and%0Aconvolutional%20neural%20networks%20demonstrate%20that%20CCL%20achieves%20comparable%0Aperformance%20to%20other%20biologically%20plausible%20algorithms%20while%20offering%20a%20more%0Abiologically%20realistic%20learning%20mechanism.%20Furthermore%2C%20we%20showcase%20the%0Aapplicability%20of%20our%20approach%20to%20an%20autoencoder%20task%2C%20underscoring%20its%0Apotential%20for%20unsupervised%20representation%20learning.%20Our%20work%20presents%20a%0Adirection%20for%20biologically%20inspired%20and%20plausible%20learning%20algorithms%2C%20offering%0Aan%20alternative%20mechanism%20of%20learning%20and%20adaptation%20in%20neural%20networks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2409.19841v2&entry.124074799=Read"},
{"title": "Benchmarking Foundation Models on Exceptional Cases: Dataset Creation\n  and Validation", "author": "Suho Kang and Jungyang Park and Joonseo Ha and SoMin Kim and JinHyeong Kim and Subeen Park and Kyungwoo Song", "abstract": "  Foundation models (FMs) have achieved significant success across various\ntasks, leading to research on benchmarks for reasoning abilities. However,\nthere is a lack of studies on FMs performance in exceptional scenarios, which\nwe define as out-of-distribution (OOD) reasoning tasks. This paper is the first\nto address these cases, developing a novel dataset for evaluation of FMs across\nmultiple modalities, including graphic novels, calligraphy, news articles, and\nlyrics. It includes tasks for instance classification, character recognition,\ntoken prediction, and text generation. The paper also proposes prompt\nengineering techniques like Chain-of-Thought (CoT) and CoT+Few-Shot to enhance\nperformance. Validation of FMs using various methods revealed improvements. The\ncode repository is accessible at:\nhttps://github.com/MLAI-Yonsei/ExceptionalBenchmark\n", "link": "http://arxiv.org/abs/2410.18001v1", "date": "2024-10-23", "relevancy": 2.0839, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.527}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.527}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Foundation%20Models%20on%20Exceptional%20Cases%3A%20Dataset%20Creation%0A%20%20and%20Validation&body=Title%3A%20Benchmarking%20Foundation%20Models%20on%20Exceptional%20Cases%3A%20Dataset%20Creation%0A%20%20and%20Validation%0AAuthor%3A%20Suho%20Kang%20and%20Jungyang%20Park%20and%20Joonseo%20Ha%20and%20SoMin%20Kim%20and%20JinHyeong%20Kim%20and%20Subeen%20Park%20and%20Kyungwoo%20Song%0AAbstract%3A%20%20%20Foundation%20models%20%28FMs%29%20have%20achieved%20significant%20success%20across%20various%0Atasks%2C%20leading%20to%20research%20on%20benchmarks%20for%20reasoning%20abilities.%20However%2C%0Athere%20is%20a%20lack%20of%20studies%20on%20FMs%20performance%20in%20exceptional%20scenarios%2C%20which%0Awe%20define%20as%20out-of-distribution%20%28OOD%29%20reasoning%20tasks.%20This%20paper%20is%20the%20first%0Ato%20address%20these%20cases%2C%20developing%20a%20novel%20dataset%20for%20evaluation%20of%20FMs%20across%0Amultiple%20modalities%2C%20including%20graphic%20novels%2C%20calligraphy%2C%20news%20articles%2C%20and%0Alyrics.%20It%20includes%20tasks%20for%20instance%20classification%2C%20character%20recognition%2C%0Atoken%20prediction%2C%20and%20text%20generation.%20The%20paper%20also%20proposes%20prompt%0Aengineering%20techniques%20like%20Chain-of-Thought%20%28CoT%29%20and%20CoT%2BFew-Shot%20to%20enhance%0Aperformance.%20Validation%20of%20FMs%20using%20various%20methods%20revealed%20improvements.%20The%0Acode%20repository%20is%20accessible%20at%3A%0Ahttps%3A//github.com/MLAI-Yonsei/ExceptionalBenchmark%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18001v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Foundation%2520Models%2520on%2520Exceptional%2520Cases%253A%2520Dataset%2520Creation%250A%2520%2520and%2520Validation%26entry.906535625%3DSuho%2520Kang%2520and%2520Jungyang%2520Park%2520and%2520Joonseo%2520Ha%2520and%2520SoMin%2520Kim%2520and%2520JinHyeong%2520Kim%2520and%2520Subeen%2520Park%2520and%2520Kyungwoo%2520Song%26entry.1292438233%3D%2520%2520Foundation%2520models%2520%2528FMs%2529%2520have%2520achieved%2520significant%2520success%2520across%2520various%250Atasks%252C%2520leading%2520to%2520research%2520on%2520benchmarks%2520for%2520reasoning%2520abilities.%2520However%252C%250Athere%2520is%2520a%2520lack%2520of%2520studies%2520on%2520FMs%2520performance%2520in%2520exceptional%2520scenarios%252C%2520which%250Awe%2520define%2520as%2520out-of-distribution%2520%2528OOD%2529%2520reasoning%2520tasks.%2520This%2520paper%2520is%2520the%2520first%250Ato%2520address%2520these%2520cases%252C%2520developing%2520a%2520novel%2520dataset%2520for%2520evaluation%2520of%2520FMs%2520across%250Amultiple%2520modalities%252C%2520including%2520graphic%2520novels%252C%2520calligraphy%252C%2520news%2520articles%252C%2520and%250Alyrics.%2520It%2520includes%2520tasks%2520for%2520instance%2520classification%252C%2520character%2520recognition%252C%250Atoken%2520prediction%252C%2520and%2520text%2520generation.%2520The%2520paper%2520also%2520proposes%2520prompt%250Aengineering%2520techniques%2520like%2520Chain-of-Thought%2520%2528CoT%2529%2520and%2520CoT%252BFew-Shot%2520to%2520enhance%250Aperformance.%2520Validation%2520of%2520FMs%2520using%2520various%2520methods%2520revealed%2520improvements.%2520The%250Acode%2520repository%2520is%2520accessible%2520at%253A%250Ahttps%253A//github.com/MLAI-Yonsei/ExceptionalBenchmark%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18001v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Foundation%20Models%20on%20Exceptional%20Cases%3A%20Dataset%20Creation%0A%20%20and%20Validation&entry.906535625=Suho%20Kang%20and%20Jungyang%20Park%20and%20Joonseo%20Ha%20and%20SoMin%20Kim%20and%20JinHyeong%20Kim%20and%20Subeen%20Park%20and%20Kyungwoo%20Song&entry.1292438233=%20%20Foundation%20models%20%28FMs%29%20have%20achieved%20significant%20success%20across%20various%0Atasks%2C%20leading%20to%20research%20on%20benchmarks%20for%20reasoning%20abilities.%20However%2C%0Athere%20is%20a%20lack%20of%20studies%20on%20FMs%20performance%20in%20exceptional%20scenarios%2C%20which%0Awe%20define%20as%20out-of-distribution%20%28OOD%29%20reasoning%20tasks.%20This%20paper%20is%20the%20first%0Ato%20address%20these%20cases%2C%20developing%20a%20novel%20dataset%20for%20evaluation%20of%20FMs%20across%0Amultiple%20modalities%2C%20including%20graphic%20novels%2C%20calligraphy%2C%20news%20articles%2C%20and%0Alyrics.%20It%20includes%20tasks%20for%20instance%20classification%2C%20character%20recognition%2C%0Atoken%20prediction%2C%20and%20text%20generation.%20The%20paper%20also%20proposes%20prompt%0Aengineering%20techniques%20like%20Chain-of-Thought%20%28CoT%29%20and%20CoT%2BFew-Shot%20to%20enhance%0Aperformance.%20Validation%20of%20FMs%20using%20various%20methods%20revealed%20improvements.%20The%0Acode%20repository%20is%20accessible%20at%3A%0Ahttps%3A//github.com/MLAI-Yonsei/ExceptionalBenchmark%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18001v1&entry.124074799=Read"},
{"title": "Few-shot NeRF by Adaptive Rendering Loss Regularization", "author": "Qingshan Xu and Xuanyu Yi and Jianyao Xu and Wenbing Tao and Yew-Soon Ong and Hanwang Zhang", "abstract": "  Novel view synthesis with sparse inputs poses great challenges to Neural\nRadiance Field (NeRF). Recent works demonstrate that the frequency\nregularization of Positional Encoding (PE) can achieve promising results for\nfew-shot NeRF. In this work, we reveal that there exists an inconsistency\nbetween the frequency regularization of PE and rendering loss. This prevents\nfew-shot NeRF from synthesizing higher-quality novel views. To mitigate this\ninconsistency, we propose Adaptive Rendering loss regularization for few-shot\nNeRF, dubbed AR-NeRF. Specifically, we present a two-phase rendering\nsupervision and an adaptive rendering loss weight learning strategy to align\nthe frequency relationship between PE and 2D-pixel supervision. In this way,\nAR-NeRF can learn global structures better in the early training phase and\nadaptively learn local details throughout the training process. Extensive\nexperiments show that our AR-NeRF achieves state-of-the-art performance on\ndifferent datasets, including object-level and complex scenes.\n", "link": "http://arxiv.org/abs/2410.17839v1", "date": "2024-10-23", "relevancy": 2.0731, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.536}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5104}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Few-shot%20NeRF%20by%20Adaptive%20Rendering%20Loss%20Regularization&body=Title%3A%20Few-shot%20NeRF%20by%20Adaptive%20Rendering%20Loss%20Regularization%0AAuthor%3A%20Qingshan%20Xu%20and%20Xuanyu%20Yi%20and%20Jianyao%20Xu%20and%20Wenbing%20Tao%20and%20Yew-Soon%20Ong%20and%20Hanwang%20Zhang%0AAbstract%3A%20%20%20Novel%20view%20synthesis%20with%20sparse%20inputs%20poses%20great%20challenges%20to%20Neural%0ARadiance%20Field%20%28NeRF%29.%20Recent%20works%20demonstrate%20that%20the%20frequency%0Aregularization%20of%20Positional%20Encoding%20%28PE%29%20can%20achieve%20promising%20results%20for%0Afew-shot%20NeRF.%20In%20this%20work%2C%20we%20reveal%20that%20there%20exists%20an%20inconsistency%0Abetween%20the%20frequency%20regularization%20of%20PE%20and%20rendering%20loss.%20This%20prevents%0Afew-shot%20NeRF%20from%20synthesizing%20higher-quality%20novel%20views.%20To%20mitigate%20this%0Ainconsistency%2C%20we%20propose%20Adaptive%20Rendering%20loss%20regularization%20for%20few-shot%0ANeRF%2C%20dubbed%20AR-NeRF.%20Specifically%2C%20we%20present%20a%20two-phase%20rendering%0Asupervision%20and%20an%20adaptive%20rendering%20loss%20weight%20learning%20strategy%20to%20align%0Athe%20frequency%20relationship%20between%20PE%20and%202D-pixel%20supervision.%20In%20this%20way%2C%0AAR-NeRF%20can%20learn%20global%20structures%20better%20in%20the%20early%20training%20phase%20and%0Aadaptively%20learn%20local%20details%20throughout%20the%20training%20process.%20Extensive%0Aexperiments%20show%20that%20our%20AR-NeRF%20achieves%20state-of-the-art%20performance%20on%0Adifferent%20datasets%2C%20including%20object-level%20and%20complex%20scenes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17839v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFew-shot%2520NeRF%2520by%2520Adaptive%2520Rendering%2520Loss%2520Regularization%26entry.906535625%3DQingshan%2520Xu%2520and%2520Xuanyu%2520Yi%2520and%2520Jianyao%2520Xu%2520and%2520Wenbing%2520Tao%2520and%2520Yew-Soon%2520Ong%2520and%2520Hanwang%2520Zhang%26entry.1292438233%3D%2520%2520Novel%2520view%2520synthesis%2520with%2520sparse%2520inputs%2520poses%2520great%2520challenges%2520to%2520Neural%250ARadiance%2520Field%2520%2528NeRF%2529.%2520Recent%2520works%2520demonstrate%2520that%2520the%2520frequency%250Aregularization%2520of%2520Positional%2520Encoding%2520%2528PE%2529%2520can%2520achieve%2520promising%2520results%2520for%250Afew-shot%2520NeRF.%2520In%2520this%2520work%252C%2520we%2520reveal%2520that%2520there%2520exists%2520an%2520inconsistency%250Abetween%2520the%2520frequency%2520regularization%2520of%2520PE%2520and%2520rendering%2520loss.%2520This%2520prevents%250Afew-shot%2520NeRF%2520from%2520synthesizing%2520higher-quality%2520novel%2520views.%2520To%2520mitigate%2520this%250Ainconsistency%252C%2520we%2520propose%2520Adaptive%2520Rendering%2520loss%2520regularization%2520for%2520few-shot%250ANeRF%252C%2520dubbed%2520AR-NeRF.%2520Specifically%252C%2520we%2520present%2520a%2520two-phase%2520rendering%250Asupervision%2520and%2520an%2520adaptive%2520rendering%2520loss%2520weight%2520learning%2520strategy%2520to%2520align%250Athe%2520frequency%2520relationship%2520between%2520PE%2520and%25202D-pixel%2520supervision.%2520In%2520this%2520way%252C%250AAR-NeRF%2520can%2520learn%2520global%2520structures%2520better%2520in%2520the%2520early%2520training%2520phase%2520and%250Aadaptively%2520learn%2520local%2520details%2520throughout%2520the%2520training%2520process.%2520Extensive%250Aexperiments%2520show%2520that%2520our%2520AR-NeRF%2520achieves%2520state-of-the-art%2520performance%2520on%250Adifferent%2520datasets%252C%2520including%2520object-level%2520and%2520complex%2520scenes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17839v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Few-shot%20NeRF%20by%20Adaptive%20Rendering%20Loss%20Regularization&entry.906535625=Qingshan%20Xu%20and%20Xuanyu%20Yi%20and%20Jianyao%20Xu%20and%20Wenbing%20Tao%20and%20Yew-Soon%20Ong%20and%20Hanwang%20Zhang&entry.1292438233=%20%20Novel%20view%20synthesis%20with%20sparse%20inputs%20poses%20great%20challenges%20to%20Neural%0ARadiance%20Field%20%28NeRF%29.%20Recent%20works%20demonstrate%20that%20the%20frequency%0Aregularization%20of%20Positional%20Encoding%20%28PE%29%20can%20achieve%20promising%20results%20for%0Afew-shot%20NeRF.%20In%20this%20work%2C%20we%20reveal%20that%20there%20exists%20an%20inconsistency%0Abetween%20the%20frequency%20regularization%20of%20PE%20and%20rendering%20loss.%20This%20prevents%0Afew-shot%20NeRF%20from%20synthesizing%20higher-quality%20novel%20views.%20To%20mitigate%20this%0Ainconsistency%2C%20we%20propose%20Adaptive%20Rendering%20loss%20regularization%20for%20few-shot%0ANeRF%2C%20dubbed%20AR-NeRF.%20Specifically%2C%20we%20present%20a%20two-phase%20rendering%0Asupervision%20and%20an%20adaptive%20rendering%20loss%20weight%20learning%20strategy%20to%20align%0Athe%20frequency%20relationship%20between%20PE%20and%202D-pixel%20supervision.%20In%20this%20way%2C%0AAR-NeRF%20can%20learn%20global%20structures%20better%20in%20the%20early%20training%20phase%20and%0Aadaptively%20learn%20local%20details%20throughout%20the%20training%20process.%20Extensive%0Aexperiments%20show%20that%20our%20AR-NeRF%20achieves%20state-of-the-art%20performance%20on%0Adifferent%20datasets%2C%20including%20object-level%20and%20complex%20scenes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17839v1&entry.124074799=Read"},
{"title": "RE-tune: Incremental Fine Tuning of Biomedical Vision-Language Models\n  for Multi-label Chest X-ray Classification", "author": "Marco Mistretta and Andrew D. Bagdanov", "abstract": "  In this paper we introduce RE-tune, a novel approach for fine-tuning\npre-trained Multimodal Biomedical Vision-Language models (VLMs) in Incremental\nLearning scenarios for multi-label chest disease diagnosis. RE-tune freezes the\nbackbones and only trains simple adaptors on top of the Image and Text encoders\nof the VLM. By engineering positive and negative text prompts for diseases, we\nleverage the ability of Large Language Models to steer the training trajectory.\nWe evaluate RE-tune in three realistic incremental learning scenarios:\nclass-incremental, label-incremental, and data-incremental. Our results\ndemonstrate that Biomedical VLMs are natural continual learners and prevent\ncatastrophic forgetting. RE-tune not only achieves accurate multi-label\nclassification results, but also prioritizes patient privacy and it\ndistinguishes itself through exceptional computational efficiency, rendering it\nhighly suitable for broad adoption in real-world healthcare settings.\n", "link": "http://arxiv.org/abs/2410.17827v1", "date": "2024-10-23", "relevancy": 2.0673, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5329}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5195}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4997}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RE-tune%3A%20Incremental%20Fine%20Tuning%20of%20Biomedical%20Vision-Language%20Models%0A%20%20for%20Multi-label%20Chest%20X-ray%20Classification&body=Title%3A%20RE-tune%3A%20Incremental%20Fine%20Tuning%20of%20Biomedical%20Vision-Language%20Models%0A%20%20for%20Multi-label%20Chest%20X-ray%20Classification%0AAuthor%3A%20Marco%20Mistretta%20and%20Andrew%20D.%20Bagdanov%0AAbstract%3A%20%20%20In%20this%20paper%20we%20introduce%20RE-tune%2C%20a%20novel%20approach%20for%20fine-tuning%0Apre-trained%20Multimodal%20Biomedical%20Vision-Language%20models%20%28VLMs%29%20in%20Incremental%0ALearning%20scenarios%20for%20multi-label%20chest%20disease%20diagnosis.%20RE-tune%20freezes%20the%0Abackbones%20and%20only%20trains%20simple%20adaptors%20on%20top%20of%20the%20Image%20and%20Text%20encoders%0Aof%20the%20VLM.%20By%20engineering%20positive%20and%20negative%20text%20prompts%20for%20diseases%2C%20we%0Aleverage%20the%20ability%20of%20Large%20Language%20Models%20to%20steer%20the%20training%20trajectory.%0AWe%20evaluate%20RE-tune%20in%20three%20realistic%20incremental%20learning%20scenarios%3A%0Aclass-incremental%2C%20label-incremental%2C%20and%20data-incremental.%20Our%20results%0Ademonstrate%20that%20Biomedical%20VLMs%20are%20natural%20continual%20learners%20and%20prevent%0Acatastrophic%20forgetting.%20RE-tune%20not%20only%20achieves%20accurate%20multi-label%0Aclassification%20results%2C%20but%20also%20prioritizes%20patient%20privacy%20and%20it%0Adistinguishes%20itself%20through%20exceptional%20computational%20efficiency%2C%20rendering%20it%0Ahighly%20suitable%20for%20broad%20adoption%20in%20real-world%20healthcare%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17827v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRE-tune%253A%2520Incremental%2520Fine%2520Tuning%2520of%2520Biomedical%2520Vision-Language%2520Models%250A%2520%2520for%2520Multi-label%2520Chest%2520X-ray%2520Classification%26entry.906535625%3DMarco%2520Mistretta%2520and%2520Andrew%2520D.%2520Bagdanov%26entry.1292438233%3D%2520%2520In%2520this%2520paper%2520we%2520introduce%2520RE-tune%252C%2520a%2520novel%2520approach%2520for%2520fine-tuning%250Apre-trained%2520Multimodal%2520Biomedical%2520Vision-Language%2520models%2520%2528VLMs%2529%2520in%2520Incremental%250ALearning%2520scenarios%2520for%2520multi-label%2520chest%2520disease%2520diagnosis.%2520RE-tune%2520freezes%2520the%250Abackbones%2520and%2520only%2520trains%2520simple%2520adaptors%2520on%2520top%2520of%2520the%2520Image%2520and%2520Text%2520encoders%250Aof%2520the%2520VLM.%2520By%2520engineering%2520positive%2520and%2520negative%2520text%2520prompts%2520for%2520diseases%252C%2520we%250Aleverage%2520the%2520ability%2520of%2520Large%2520Language%2520Models%2520to%2520steer%2520the%2520training%2520trajectory.%250AWe%2520evaluate%2520RE-tune%2520in%2520three%2520realistic%2520incremental%2520learning%2520scenarios%253A%250Aclass-incremental%252C%2520label-incremental%252C%2520and%2520data-incremental.%2520Our%2520results%250Ademonstrate%2520that%2520Biomedical%2520VLMs%2520are%2520natural%2520continual%2520learners%2520and%2520prevent%250Acatastrophic%2520forgetting.%2520RE-tune%2520not%2520only%2520achieves%2520accurate%2520multi-label%250Aclassification%2520results%252C%2520but%2520also%2520prioritizes%2520patient%2520privacy%2520and%2520it%250Adistinguishes%2520itself%2520through%2520exceptional%2520computational%2520efficiency%252C%2520rendering%2520it%250Ahighly%2520suitable%2520for%2520broad%2520adoption%2520in%2520real-world%2520healthcare%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17827v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RE-tune%3A%20Incremental%20Fine%20Tuning%20of%20Biomedical%20Vision-Language%20Models%0A%20%20for%20Multi-label%20Chest%20X-ray%20Classification&entry.906535625=Marco%20Mistretta%20and%20Andrew%20D.%20Bagdanov&entry.1292438233=%20%20In%20this%20paper%20we%20introduce%20RE-tune%2C%20a%20novel%20approach%20for%20fine-tuning%0Apre-trained%20Multimodal%20Biomedical%20Vision-Language%20models%20%28VLMs%29%20in%20Incremental%0ALearning%20scenarios%20for%20multi-label%20chest%20disease%20diagnosis.%20RE-tune%20freezes%20the%0Abackbones%20and%20only%20trains%20simple%20adaptors%20on%20top%20of%20the%20Image%20and%20Text%20encoders%0Aof%20the%20VLM.%20By%20engineering%20positive%20and%20negative%20text%20prompts%20for%20diseases%2C%20we%0Aleverage%20the%20ability%20of%20Large%20Language%20Models%20to%20steer%20the%20training%20trajectory.%0AWe%20evaluate%20RE-tune%20in%20three%20realistic%20incremental%20learning%20scenarios%3A%0Aclass-incremental%2C%20label-incremental%2C%20and%20data-incremental.%20Our%20results%0Ademonstrate%20that%20Biomedical%20VLMs%20are%20natural%20continual%20learners%20and%20prevent%0Acatastrophic%20forgetting.%20RE-tune%20not%20only%20achieves%20accurate%20multi-label%0Aclassification%20results%2C%20but%20also%20prioritizes%20patient%20privacy%20and%20it%0Adistinguishes%20itself%20through%20exceptional%20computational%20efficiency%2C%20rendering%20it%0Ahighly%20suitable%20for%20broad%20adoption%20in%20real-world%20healthcare%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17827v1&entry.124074799=Read"},
{"title": "Beyond Backpropagation: Optimization with Multi-Tangent Forward\n  Gradients", "author": "Katharina Fl\u00fcgel and Daniel Coquelin and Marie Weiel and Achim Streit and Markus G\u00f6tz", "abstract": "  The gradients used to train neural networks are typically computed using\nbackpropagation. While an efficient way to obtain exact gradients,\nbackpropagation is computationally expensive, hinders parallelization, and is\nbiologically implausible. Forward gradients are an approach to approximate the\ngradients from directional derivatives along random tangents computed by\nforward-mode automatic differentiation. So far, research has focused on using a\nsingle tangent per step. This paper provides an in-depth analysis of\nmulti-tangent forward gradients and introduces an improved approach to\ncombining the forward gradients from multiple tangents based on orthogonal\nprojections. We demonstrate that increasing the number of tangents improves\nboth approximation quality and optimization performance across various tasks.\n", "link": "http://arxiv.org/abs/2410.17764v1", "date": "2024-10-23", "relevancy": 2.0559, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5315}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5033}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Beyond%20Backpropagation%3A%20Optimization%20with%20Multi-Tangent%20Forward%0A%20%20Gradients&body=Title%3A%20Beyond%20Backpropagation%3A%20Optimization%20with%20Multi-Tangent%20Forward%0A%20%20Gradients%0AAuthor%3A%20Katharina%20Fl%C3%BCgel%20and%20Daniel%20Coquelin%20and%20Marie%20Weiel%20and%20Achim%20Streit%20and%20Markus%20G%C3%B6tz%0AAbstract%3A%20%20%20The%20gradients%20used%20to%20train%20neural%20networks%20are%20typically%20computed%20using%0Abackpropagation.%20While%20an%20efficient%20way%20to%20obtain%20exact%20gradients%2C%0Abackpropagation%20is%20computationally%20expensive%2C%20hinders%20parallelization%2C%20and%20is%0Abiologically%20implausible.%20Forward%20gradients%20are%20an%20approach%20to%20approximate%20the%0Agradients%20from%20directional%20derivatives%20along%20random%20tangents%20computed%20by%0Aforward-mode%20automatic%20differentiation.%20So%20far%2C%20research%20has%20focused%20on%20using%20a%0Asingle%20tangent%20per%20step.%20This%20paper%20provides%20an%20in-depth%20analysis%20of%0Amulti-tangent%20forward%20gradients%20and%20introduces%20an%20improved%20approach%20to%0Acombining%20the%20forward%20gradients%20from%20multiple%20tangents%20based%20on%20orthogonal%0Aprojections.%20We%20demonstrate%20that%20increasing%20the%20number%20of%20tangents%20improves%0Aboth%20approximation%20quality%20and%20optimization%20performance%20across%20various%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17764v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeyond%2520Backpropagation%253A%2520Optimization%2520with%2520Multi-Tangent%2520Forward%250A%2520%2520Gradients%26entry.906535625%3DKatharina%2520Fl%25C3%25BCgel%2520and%2520Daniel%2520Coquelin%2520and%2520Marie%2520Weiel%2520and%2520Achim%2520Streit%2520and%2520Markus%2520G%25C3%25B6tz%26entry.1292438233%3D%2520%2520The%2520gradients%2520used%2520to%2520train%2520neural%2520networks%2520are%2520typically%2520computed%2520using%250Abackpropagation.%2520While%2520an%2520efficient%2520way%2520to%2520obtain%2520exact%2520gradients%252C%250Abackpropagation%2520is%2520computationally%2520expensive%252C%2520hinders%2520parallelization%252C%2520and%2520is%250Abiologically%2520implausible.%2520Forward%2520gradients%2520are%2520an%2520approach%2520to%2520approximate%2520the%250Agradients%2520from%2520directional%2520derivatives%2520along%2520random%2520tangents%2520computed%2520by%250Aforward-mode%2520automatic%2520differentiation.%2520So%2520far%252C%2520research%2520has%2520focused%2520on%2520using%2520a%250Asingle%2520tangent%2520per%2520step.%2520This%2520paper%2520provides%2520an%2520in-depth%2520analysis%2520of%250Amulti-tangent%2520forward%2520gradients%2520and%2520introduces%2520an%2520improved%2520approach%2520to%250Acombining%2520the%2520forward%2520gradients%2520from%2520multiple%2520tangents%2520based%2520on%2520orthogonal%250Aprojections.%2520We%2520demonstrate%2520that%2520increasing%2520the%2520number%2520of%2520tangents%2520improves%250Aboth%2520approximation%2520quality%2520and%2520optimization%2520performance%2520across%2520various%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17764v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Beyond%20Backpropagation%3A%20Optimization%20with%20Multi-Tangent%20Forward%0A%20%20Gradients&entry.906535625=Katharina%20Fl%C3%BCgel%20and%20Daniel%20Coquelin%20and%20Marie%20Weiel%20and%20Achim%20Streit%20and%20Markus%20G%C3%B6tz&entry.1292438233=%20%20The%20gradients%20used%20to%20train%20neural%20networks%20are%20typically%20computed%20using%0Abackpropagation.%20While%20an%20efficient%20way%20to%20obtain%20exact%20gradients%2C%0Abackpropagation%20is%20computationally%20expensive%2C%20hinders%20parallelization%2C%20and%20is%0Abiologically%20implausible.%20Forward%20gradients%20are%20an%20approach%20to%20approximate%20the%0Agradients%20from%20directional%20derivatives%20along%20random%20tangents%20computed%20by%0Aforward-mode%20automatic%20differentiation.%20So%20far%2C%20research%20has%20focused%20on%20using%20a%0Asingle%20tangent%20per%20step.%20This%20paper%20provides%20an%20in-depth%20analysis%20of%0Amulti-tangent%20forward%20gradients%20and%20introduces%20an%20improved%20approach%20to%0Acombining%20the%20forward%20gradients%20from%20multiple%20tangents%20based%20on%20orthogonal%0Aprojections.%20We%20demonstrate%20that%20increasing%20the%20number%20of%20tangents%20improves%0Aboth%20approximation%20quality%20and%20optimization%20performance%20across%20various%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17764v1&entry.124074799=Read"},
{"title": "Generative AI Models for Different Steps in Architectural Design: A\n  Literature Review", "author": "Chengyuan Li and Tianyu Zhang and Xusheng Du and Ye Zhang and Haoran Xie", "abstract": "  Recent advances in generative artificial intelligence (AI) technologies have\nbeen significantly driven by models such as generative adversarial networks\n(GANs), variational autoencoders (VAEs), and denoising diffusion probabilistic\nmodels (DDPMs). Although architects recognize the potential of generative AI in\ndesign, personal barriers often restrict their access to the latest\ntechnological developments, thereby causing the application of generative AI in\narchitectural design to lag behind. Therefore, it is essential to comprehend\nthe principles and advancements of generative AI models and analyze their\nrelevance in architecture applications. This paper first provides an overview\nof generative AI technologies, with a focus on probabilistic diffusion models\n(DDPMs), 3D generative models, and foundation models, highlighting their recent\ndevelopments and main application scenarios. Then, the paper explains how the\nabovementioned models could be utilized in architecture. We subdivide the\narchitectural design process into six steps and review related research\nprojects in each step from 2020 to the present. Lastly, this paper discusses\npotential future directions for applying generative AI in the architectural\ndesign steps. This research can help architects quickly understand the\ndevelopment and latest progress of generative AI and contribute to the further\ndevelopment of intelligent architecture.\n", "link": "http://arxiv.org/abs/2404.01335v2", "date": "2024-10-23", "relevancy": 2.0556, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5222}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5168}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5044}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20AI%20Models%20for%20Different%20Steps%20in%20Architectural%20Design%3A%20A%0A%20%20Literature%20Review&body=Title%3A%20Generative%20AI%20Models%20for%20Different%20Steps%20in%20Architectural%20Design%3A%20A%0A%20%20Literature%20Review%0AAuthor%3A%20Chengyuan%20Li%20and%20Tianyu%20Zhang%20and%20Xusheng%20Du%20and%20Ye%20Zhang%20and%20Haoran%20Xie%0AAbstract%3A%20%20%20Recent%20advances%20in%20generative%20artificial%20intelligence%20%28AI%29%20technologies%20have%0Abeen%20significantly%20driven%20by%20models%20such%20as%20generative%20adversarial%20networks%0A%28GANs%29%2C%20variational%20autoencoders%20%28VAEs%29%2C%20and%20denoising%20diffusion%20probabilistic%0Amodels%20%28DDPMs%29.%20Although%20architects%20recognize%20the%20potential%20of%20generative%20AI%20in%0Adesign%2C%20personal%20barriers%20often%20restrict%20their%20access%20to%20the%20latest%0Atechnological%20developments%2C%20thereby%20causing%20the%20application%20of%20generative%20AI%20in%0Aarchitectural%20design%20to%20lag%20behind.%20Therefore%2C%20it%20is%20essential%20to%20comprehend%0Athe%20principles%20and%20advancements%20of%20generative%20AI%20models%20and%20analyze%20their%0Arelevance%20in%20architecture%20applications.%20This%20paper%20first%20provides%20an%20overview%0Aof%20generative%20AI%20technologies%2C%20with%20a%20focus%20on%20probabilistic%20diffusion%20models%0A%28DDPMs%29%2C%203D%20generative%20models%2C%20and%20foundation%20models%2C%20highlighting%20their%20recent%0Adevelopments%20and%20main%20application%20scenarios.%20Then%2C%20the%20paper%20explains%20how%20the%0Aabovementioned%20models%20could%20be%20utilized%20in%20architecture.%20We%20subdivide%20the%0Aarchitectural%20design%20process%20into%20six%20steps%20and%20review%20related%20research%0Aprojects%20in%20each%20step%20from%202020%20to%20the%20present.%20Lastly%2C%20this%20paper%20discusses%0Apotential%20future%20directions%20for%20applying%20generative%20AI%20in%20the%20architectural%0Adesign%20steps.%20This%20research%20can%20help%20architects%20quickly%20understand%20the%0Adevelopment%20and%20latest%20progress%20of%20generative%20AI%20and%20contribute%20to%20the%20further%0Adevelopment%20of%20intelligent%20architecture.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.01335v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520AI%2520Models%2520for%2520Different%2520Steps%2520in%2520Architectural%2520Design%253A%2520A%250A%2520%2520Literature%2520Review%26entry.906535625%3DChengyuan%2520Li%2520and%2520Tianyu%2520Zhang%2520and%2520Xusheng%2520Du%2520and%2520Ye%2520Zhang%2520and%2520Haoran%2520Xie%26entry.1292438233%3D%2520%2520Recent%2520advances%2520in%2520generative%2520artificial%2520intelligence%2520%2528AI%2529%2520technologies%2520have%250Abeen%2520significantly%2520driven%2520by%2520models%2520such%2520as%2520generative%2520adversarial%2520networks%250A%2528GANs%2529%252C%2520variational%2520autoencoders%2520%2528VAEs%2529%252C%2520and%2520denoising%2520diffusion%2520probabilistic%250Amodels%2520%2528DDPMs%2529.%2520Although%2520architects%2520recognize%2520the%2520potential%2520of%2520generative%2520AI%2520in%250Adesign%252C%2520personal%2520barriers%2520often%2520restrict%2520their%2520access%2520to%2520the%2520latest%250Atechnological%2520developments%252C%2520thereby%2520causing%2520the%2520application%2520of%2520generative%2520AI%2520in%250Aarchitectural%2520design%2520to%2520lag%2520behind.%2520Therefore%252C%2520it%2520is%2520essential%2520to%2520comprehend%250Athe%2520principles%2520and%2520advancements%2520of%2520generative%2520AI%2520models%2520and%2520analyze%2520their%250Arelevance%2520in%2520architecture%2520applications.%2520This%2520paper%2520first%2520provides%2520an%2520overview%250Aof%2520generative%2520AI%2520technologies%252C%2520with%2520a%2520focus%2520on%2520probabilistic%2520diffusion%2520models%250A%2528DDPMs%2529%252C%25203D%2520generative%2520models%252C%2520and%2520foundation%2520models%252C%2520highlighting%2520their%2520recent%250Adevelopments%2520and%2520main%2520application%2520scenarios.%2520Then%252C%2520the%2520paper%2520explains%2520how%2520the%250Aabovementioned%2520models%2520could%2520be%2520utilized%2520in%2520architecture.%2520We%2520subdivide%2520the%250Aarchitectural%2520design%2520process%2520into%2520six%2520steps%2520and%2520review%2520related%2520research%250Aprojects%2520in%2520each%2520step%2520from%25202020%2520to%2520the%2520present.%2520Lastly%252C%2520this%2520paper%2520discusses%250Apotential%2520future%2520directions%2520for%2520applying%2520generative%2520AI%2520in%2520the%2520architectural%250Adesign%2520steps.%2520This%2520research%2520can%2520help%2520architects%2520quickly%2520understand%2520the%250Adevelopment%2520and%2520latest%2520progress%2520of%2520generative%2520AI%2520and%2520contribute%2520to%2520the%2520further%250Adevelopment%2520of%2520intelligent%2520architecture.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.01335v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20AI%20Models%20for%20Different%20Steps%20in%20Architectural%20Design%3A%20A%0A%20%20Literature%20Review&entry.906535625=Chengyuan%20Li%20and%20Tianyu%20Zhang%20and%20Xusheng%20Du%20and%20Ye%20Zhang%20and%20Haoran%20Xie&entry.1292438233=%20%20Recent%20advances%20in%20generative%20artificial%20intelligence%20%28AI%29%20technologies%20have%0Abeen%20significantly%20driven%20by%20models%20such%20as%20generative%20adversarial%20networks%0A%28GANs%29%2C%20variational%20autoencoders%20%28VAEs%29%2C%20and%20denoising%20diffusion%20probabilistic%0Amodels%20%28DDPMs%29.%20Although%20architects%20recognize%20the%20potential%20of%20generative%20AI%20in%0Adesign%2C%20personal%20barriers%20often%20restrict%20their%20access%20to%20the%20latest%0Atechnological%20developments%2C%20thereby%20causing%20the%20application%20of%20generative%20AI%20in%0Aarchitectural%20design%20to%20lag%20behind.%20Therefore%2C%20it%20is%20essential%20to%20comprehend%0Athe%20principles%20and%20advancements%20of%20generative%20AI%20models%20and%20analyze%20their%0Arelevance%20in%20architecture%20applications.%20This%20paper%20first%20provides%20an%20overview%0Aof%20generative%20AI%20technologies%2C%20with%20a%20focus%20on%20probabilistic%20diffusion%20models%0A%28DDPMs%29%2C%203D%20generative%20models%2C%20and%20foundation%20models%2C%20highlighting%20their%20recent%0Adevelopments%20and%20main%20application%20scenarios.%20Then%2C%20the%20paper%20explains%20how%20the%0Aabovementioned%20models%20could%20be%20utilized%20in%20architecture.%20We%20subdivide%20the%0Aarchitectural%20design%20process%20into%20six%20steps%20and%20review%20related%20research%0Aprojects%20in%20each%20step%20from%202020%20to%20the%20present.%20Lastly%2C%20this%20paper%20discusses%0Apotential%20future%20directions%20for%20applying%20generative%20AI%20in%20the%20architectural%0Adesign%20steps.%20This%20research%20can%20help%20architects%20quickly%20understand%20the%0Adevelopment%20and%20latest%20progress%20of%20generative%20AI%20and%20contribute%20to%20the%20further%0Adevelopment%20of%20intelligent%20architecture.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.01335v2&entry.124074799=Read"},
{"title": "Simplifying Deep Temporal Difference Learning", "author": "Matteo Gallici and Mattie Fellows and Benjamin Ellis and Bartomeu Pou and Ivan Masmitja and Jakob Nicolaus Foerster and Mario Martin", "abstract": "  Q-learning played a foundational role in the field reinforcement learning\n(RL). However, TD algorithms with off-policy data, such as Q-learning, or\nnonlinear function approximation like deep neural networks require several\nadditional tricks to stabilise training, primarily a replay buffer and target\nnetworks. Unfortunately, the delayed updating of frozen network parameters in\nthe target network harms the sample efficiency and, similarly, the replay\nbuffer introduces memory and implementation overheads. In this paper, we\ninvestigate whether it is possible to accelerate and simplify TD training while\nmaintaining its stability. Our key theoretical result demonstrates for the\nfirst time that regularisation techniques such as LayerNorm can yield provably\nconvergent TD algorithms without the need for a target network, even with\noff-policy data. Empirically, we find that online, parallelised sampling\nenabled by vectorised environments stabilises training without the need of a\nreplay buffer. Motivated by these findings, we propose PQN, our simplified deep\nonline Q-Learning algorithm. Surprisingly, this simple algorithm is competitive\nwith more complex methods like: Rainbow in Atari, R2D2 in Hanabi, QMix in Smax,\nPPO-RNN in Craftax, and can be up to 50x faster than traditional DQN without\nsacrificing sample efficiency. In an era where PPO has become the go-to RL\nalgorithm, PQN reestablishes Q-learning as a viable alternative.\n", "link": "http://arxiv.org/abs/2407.04811v2", "date": "2024-10-23", "relevancy": 2.0432, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5267}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5137}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4937}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Simplifying%20Deep%20Temporal%20Difference%20Learning&body=Title%3A%20Simplifying%20Deep%20Temporal%20Difference%20Learning%0AAuthor%3A%20Matteo%20Gallici%20and%20Mattie%20Fellows%20and%20Benjamin%20Ellis%20and%20Bartomeu%20Pou%20and%20Ivan%20Masmitja%20and%20Jakob%20Nicolaus%20Foerster%20and%20Mario%20Martin%0AAbstract%3A%20%20%20Q-learning%20played%20a%20foundational%20role%20in%20the%20field%20reinforcement%20learning%0A%28RL%29.%20However%2C%20TD%20algorithms%20with%20off-policy%20data%2C%20such%20as%20Q-learning%2C%20or%0Anonlinear%20function%20approximation%20like%20deep%20neural%20networks%20require%20several%0Aadditional%20tricks%20to%20stabilise%20training%2C%20primarily%20a%20replay%20buffer%20and%20target%0Anetworks.%20Unfortunately%2C%20the%20delayed%20updating%20of%20frozen%20network%20parameters%20in%0Athe%20target%20network%20harms%20the%20sample%20efficiency%20and%2C%20similarly%2C%20the%20replay%0Abuffer%20introduces%20memory%20and%20implementation%20overheads.%20In%20this%20paper%2C%20we%0Ainvestigate%20whether%20it%20is%20possible%20to%20accelerate%20and%20simplify%20TD%20training%20while%0Amaintaining%20its%20stability.%20Our%20key%20theoretical%20result%20demonstrates%20for%20the%0Afirst%20time%20that%20regularisation%20techniques%20such%20as%20LayerNorm%20can%20yield%20provably%0Aconvergent%20TD%20algorithms%20without%20the%20need%20for%20a%20target%20network%2C%20even%20with%0Aoff-policy%20data.%20Empirically%2C%20we%20find%20that%20online%2C%20parallelised%20sampling%0Aenabled%20by%20vectorised%20environments%20stabilises%20training%20without%20the%20need%20of%20a%0Areplay%20buffer.%20Motivated%20by%20these%20findings%2C%20we%20propose%20PQN%2C%20our%20simplified%20deep%0Aonline%20Q-Learning%20algorithm.%20Surprisingly%2C%20this%20simple%20algorithm%20is%20competitive%0Awith%20more%20complex%20methods%20like%3A%20Rainbow%20in%20Atari%2C%20R2D2%20in%20Hanabi%2C%20QMix%20in%20Smax%2C%0APPO-RNN%20in%20Craftax%2C%20and%20can%20be%20up%20to%2050x%20faster%20than%20traditional%20DQN%20without%0Asacrificing%20sample%20efficiency.%20In%20an%20era%20where%20PPO%20has%20become%20the%20go-to%20RL%0Aalgorithm%2C%20PQN%20reestablishes%20Q-learning%20as%20a%20viable%20alternative.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.04811v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimplifying%2520Deep%2520Temporal%2520Difference%2520Learning%26entry.906535625%3DMatteo%2520Gallici%2520and%2520Mattie%2520Fellows%2520and%2520Benjamin%2520Ellis%2520and%2520Bartomeu%2520Pou%2520and%2520Ivan%2520Masmitja%2520and%2520Jakob%2520Nicolaus%2520Foerster%2520and%2520Mario%2520Martin%26entry.1292438233%3D%2520%2520Q-learning%2520played%2520a%2520foundational%2520role%2520in%2520the%2520field%2520reinforcement%2520learning%250A%2528RL%2529.%2520However%252C%2520TD%2520algorithms%2520with%2520off-policy%2520data%252C%2520such%2520as%2520Q-learning%252C%2520or%250Anonlinear%2520function%2520approximation%2520like%2520deep%2520neural%2520networks%2520require%2520several%250Aadditional%2520tricks%2520to%2520stabilise%2520training%252C%2520primarily%2520a%2520replay%2520buffer%2520and%2520target%250Anetworks.%2520Unfortunately%252C%2520the%2520delayed%2520updating%2520of%2520frozen%2520network%2520parameters%2520in%250Athe%2520target%2520network%2520harms%2520the%2520sample%2520efficiency%2520and%252C%2520similarly%252C%2520the%2520replay%250Abuffer%2520introduces%2520memory%2520and%2520implementation%2520overheads.%2520In%2520this%2520paper%252C%2520we%250Ainvestigate%2520whether%2520it%2520is%2520possible%2520to%2520accelerate%2520and%2520simplify%2520TD%2520training%2520while%250Amaintaining%2520its%2520stability.%2520Our%2520key%2520theoretical%2520result%2520demonstrates%2520for%2520the%250Afirst%2520time%2520that%2520regularisation%2520techniques%2520such%2520as%2520LayerNorm%2520can%2520yield%2520provably%250Aconvergent%2520TD%2520algorithms%2520without%2520the%2520need%2520for%2520a%2520target%2520network%252C%2520even%2520with%250Aoff-policy%2520data.%2520Empirically%252C%2520we%2520find%2520that%2520online%252C%2520parallelised%2520sampling%250Aenabled%2520by%2520vectorised%2520environments%2520stabilises%2520training%2520without%2520the%2520need%2520of%2520a%250Areplay%2520buffer.%2520Motivated%2520by%2520these%2520findings%252C%2520we%2520propose%2520PQN%252C%2520our%2520simplified%2520deep%250Aonline%2520Q-Learning%2520algorithm.%2520Surprisingly%252C%2520this%2520simple%2520algorithm%2520is%2520competitive%250Awith%2520more%2520complex%2520methods%2520like%253A%2520Rainbow%2520in%2520Atari%252C%2520R2D2%2520in%2520Hanabi%252C%2520QMix%2520in%2520Smax%252C%250APPO-RNN%2520in%2520Craftax%252C%2520and%2520can%2520be%2520up%2520to%252050x%2520faster%2520than%2520traditional%2520DQN%2520without%250Asacrificing%2520sample%2520efficiency.%2520In%2520an%2520era%2520where%2520PPO%2520has%2520become%2520the%2520go-to%2520RL%250Aalgorithm%252C%2520PQN%2520reestablishes%2520Q-learning%2520as%2520a%2520viable%2520alternative.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.04811v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Simplifying%20Deep%20Temporal%20Difference%20Learning&entry.906535625=Matteo%20Gallici%20and%20Mattie%20Fellows%20and%20Benjamin%20Ellis%20and%20Bartomeu%20Pou%20and%20Ivan%20Masmitja%20and%20Jakob%20Nicolaus%20Foerster%20and%20Mario%20Martin&entry.1292438233=%20%20Q-learning%20played%20a%20foundational%20role%20in%20the%20field%20reinforcement%20learning%0A%28RL%29.%20However%2C%20TD%20algorithms%20with%20off-policy%20data%2C%20such%20as%20Q-learning%2C%20or%0Anonlinear%20function%20approximation%20like%20deep%20neural%20networks%20require%20several%0Aadditional%20tricks%20to%20stabilise%20training%2C%20primarily%20a%20replay%20buffer%20and%20target%0Anetworks.%20Unfortunately%2C%20the%20delayed%20updating%20of%20frozen%20network%20parameters%20in%0Athe%20target%20network%20harms%20the%20sample%20efficiency%20and%2C%20similarly%2C%20the%20replay%0Abuffer%20introduces%20memory%20and%20implementation%20overheads.%20In%20this%20paper%2C%20we%0Ainvestigate%20whether%20it%20is%20possible%20to%20accelerate%20and%20simplify%20TD%20training%20while%0Amaintaining%20its%20stability.%20Our%20key%20theoretical%20result%20demonstrates%20for%20the%0Afirst%20time%20that%20regularisation%20techniques%20such%20as%20LayerNorm%20can%20yield%20provably%0Aconvergent%20TD%20algorithms%20without%20the%20need%20for%20a%20target%20network%2C%20even%20with%0Aoff-policy%20data.%20Empirically%2C%20we%20find%20that%20online%2C%20parallelised%20sampling%0Aenabled%20by%20vectorised%20environments%20stabilises%20training%20without%20the%20need%20of%20a%0Areplay%20buffer.%20Motivated%20by%20these%20findings%2C%20we%20propose%20PQN%2C%20our%20simplified%20deep%0Aonline%20Q-Learning%20algorithm.%20Surprisingly%2C%20this%20simple%20algorithm%20is%20competitive%0Awith%20more%20complex%20methods%20like%3A%20Rainbow%20in%20Atari%2C%20R2D2%20in%20Hanabi%2C%20QMix%20in%20Smax%2C%0APPO-RNN%20in%20Craftax%2C%20and%20can%20be%20up%20to%2050x%20faster%20than%20traditional%20DQN%20without%0Asacrificing%20sample%20efficiency.%20In%20an%20era%20where%20PPO%20has%20become%20the%20go-to%20RL%0Aalgorithm%2C%20PQN%20reestablishes%20Q-learning%20as%20a%20viable%20alternative.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.04811v2&entry.124074799=Read"},
{"title": "Leveraging Hallucinations to Reduce Manual Prompt Dependency in\n  Promptable Segmentation", "author": "Jian Hu and Jiayi Lin and Junchi Yan and Shaogang Gong", "abstract": "  Promptable segmentation typically requires instance-specific manual prompts\nto guide the segmentation of each desired object. To minimize such a need,\ntask-generic promptable segmentation has been introduced, which employs a\nsingle task-generic prompt to segment various images of different objects in\nthe same task. Current methods use Multimodal Large Language Models (MLLMs) to\nreason detailed instance-specific prompts from a task-generic prompt for\nimproving segmentation accuracy. The effectiveness of this segmentation heavily\ndepends on the precision of these derived prompts. However, MLLMs often suffer\nhallucinations during reasoning, resulting in inaccurate prompting. While\nexisting methods focus on eliminating hallucinations to improve a model, we\nargue that MLLM hallucinations can reveal valuable contextual insights when\nleveraged correctly, as they represent pre-trained large-scale knowledge beyond\nindividual images. In this paper, we utilize hallucinations to mine\ntask-related information from images and verify its accuracy for enhancing\nprecision of the generated prompts. Specifically, we introduce an iterative\nPrompt-Mask Cycle generation framework (ProMaC) with a prompt generator and a\nmask generator.The prompt generator uses a multi-scale chain of thought\nprompting, initially exploring hallucinations for extracting extended\ncontextual knowledge on a test image.These hallucinations are then reduced to\nformulate precise instance-specific prompts, directing the mask generator to\nproduce masks that are consistent with task semantics by mask semantic\nalignment. The generated masks iteratively induce the prompt generator to focus\nmore on task-relevant image areas and reduce irrelevant hallucinations,\nresulting jointly in better prompts and masks. Experiments on 5 benchmarks\ndemonstrate the effectiveness of ProMaC. Code given in\nhttps://lwpyh.github.io/ProMaC/.\n", "link": "http://arxiv.org/abs/2408.15205v2", "date": "2024-10-23", "relevancy": 2.0225, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5288}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.501}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Leveraging%20Hallucinations%20to%20Reduce%20Manual%20Prompt%20Dependency%20in%0A%20%20Promptable%20Segmentation&body=Title%3A%20Leveraging%20Hallucinations%20to%20Reduce%20Manual%20Prompt%20Dependency%20in%0A%20%20Promptable%20Segmentation%0AAuthor%3A%20Jian%20Hu%20and%20Jiayi%20Lin%20and%20Junchi%20Yan%20and%20Shaogang%20Gong%0AAbstract%3A%20%20%20Promptable%20segmentation%20typically%20requires%20instance-specific%20manual%20prompts%0Ato%20guide%20the%20segmentation%20of%20each%20desired%20object.%20To%20minimize%20such%20a%20need%2C%0Atask-generic%20promptable%20segmentation%20has%20been%20introduced%2C%20which%20employs%20a%0Asingle%20task-generic%20prompt%20to%20segment%20various%20images%20of%20different%20objects%20in%0Athe%20same%20task.%20Current%20methods%20use%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%0Areason%20detailed%20instance-specific%20prompts%20from%20a%20task-generic%20prompt%20for%0Aimproving%20segmentation%20accuracy.%20The%20effectiveness%20of%20this%20segmentation%20heavily%0Adepends%20on%20the%20precision%20of%20these%20derived%20prompts.%20However%2C%20MLLMs%20often%20suffer%0Ahallucinations%20during%20reasoning%2C%20resulting%20in%20inaccurate%20prompting.%20While%0Aexisting%20methods%20focus%20on%20eliminating%20hallucinations%20to%20improve%20a%20model%2C%20we%0Aargue%20that%20MLLM%20hallucinations%20can%20reveal%20valuable%20contextual%20insights%20when%0Aleveraged%20correctly%2C%20as%20they%20represent%20pre-trained%20large-scale%20knowledge%20beyond%0Aindividual%20images.%20In%20this%20paper%2C%20we%20utilize%20hallucinations%20to%20mine%0Atask-related%20information%20from%20images%20and%20verify%20its%20accuracy%20for%20enhancing%0Aprecision%20of%20the%20generated%20prompts.%20Specifically%2C%20we%20introduce%20an%20iterative%0APrompt-Mask%20Cycle%20generation%20framework%20%28ProMaC%29%20with%20a%20prompt%20generator%20and%20a%0Amask%20generator.The%20prompt%20generator%20uses%20a%20multi-scale%20chain%20of%20thought%0Aprompting%2C%20initially%20exploring%20hallucinations%20for%20extracting%20extended%0Acontextual%20knowledge%20on%20a%20test%20image.These%20hallucinations%20are%20then%20reduced%20to%0Aformulate%20precise%20instance-specific%20prompts%2C%20directing%20the%20mask%20generator%20to%0Aproduce%20masks%20that%20are%20consistent%20with%20task%20semantics%20by%20mask%20semantic%0Aalignment.%20The%20generated%20masks%20iteratively%20induce%20the%20prompt%20generator%20to%20focus%0Amore%20on%20task-relevant%20image%20areas%20and%20reduce%20irrelevant%20hallucinations%2C%0Aresulting%20jointly%20in%20better%20prompts%20and%20masks.%20Experiments%20on%205%20benchmarks%0Ademonstrate%20the%20effectiveness%20of%20ProMaC.%20Code%20given%20in%0Ahttps%3A//lwpyh.github.io/ProMaC/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.15205v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeveraging%2520Hallucinations%2520to%2520Reduce%2520Manual%2520Prompt%2520Dependency%2520in%250A%2520%2520Promptable%2520Segmentation%26entry.906535625%3DJian%2520Hu%2520and%2520Jiayi%2520Lin%2520and%2520Junchi%2520Yan%2520and%2520Shaogang%2520Gong%26entry.1292438233%3D%2520%2520Promptable%2520segmentation%2520typically%2520requires%2520instance-specific%2520manual%2520prompts%250Ato%2520guide%2520the%2520segmentation%2520of%2520each%2520desired%2520object.%2520To%2520minimize%2520such%2520a%2520need%252C%250Atask-generic%2520promptable%2520segmentation%2520has%2520been%2520introduced%252C%2520which%2520employs%2520a%250Asingle%2520task-generic%2520prompt%2520to%2520segment%2520various%2520images%2520of%2520different%2520objects%2520in%250Athe%2520same%2520task.%2520Current%2520methods%2520use%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520to%250Areason%2520detailed%2520instance-specific%2520prompts%2520from%2520a%2520task-generic%2520prompt%2520for%250Aimproving%2520segmentation%2520accuracy.%2520The%2520effectiveness%2520of%2520this%2520segmentation%2520heavily%250Adepends%2520on%2520the%2520precision%2520of%2520these%2520derived%2520prompts.%2520However%252C%2520MLLMs%2520often%2520suffer%250Ahallucinations%2520during%2520reasoning%252C%2520resulting%2520in%2520inaccurate%2520prompting.%2520While%250Aexisting%2520methods%2520focus%2520on%2520eliminating%2520hallucinations%2520to%2520improve%2520a%2520model%252C%2520we%250Aargue%2520that%2520MLLM%2520hallucinations%2520can%2520reveal%2520valuable%2520contextual%2520insights%2520when%250Aleveraged%2520correctly%252C%2520as%2520they%2520represent%2520pre-trained%2520large-scale%2520knowledge%2520beyond%250Aindividual%2520images.%2520In%2520this%2520paper%252C%2520we%2520utilize%2520hallucinations%2520to%2520mine%250Atask-related%2520information%2520from%2520images%2520and%2520verify%2520its%2520accuracy%2520for%2520enhancing%250Aprecision%2520of%2520the%2520generated%2520prompts.%2520Specifically%252C%2520we%2520introduce%2520an%2520iterative%250APrompt-Mask%2520Cycle%2520generation%2520framework%2520%2528ProMaC%2529%2520with%2520a%2520prompt%2520generator%2520and%2520a%250Amask%2520generator.The%2520prompt%2520generator%2520uses%2520a%2520multi-scale%2520chain%2520of%2520thought%250Aprompting%252C%2520initially%2520exploring%2520hallucinations%2520for%2520extracting%2520extended%250Acontextual%2520knowledge%2520on%2520a%2520test%2520image.These%2520hallucinations%2520are%2520then%2520reduced%2520to%250Aformulate%2520precise%2520instance-specific%2520prompts%252C%2520directing%2520the%2520mask%2520generator%2520to%250Aproduce%2520masks%2520that%2520are%2520consistent%2520with%2520task%2520semantics%2520by%2520mask%2520semantic%250Aalignment.%2520The%2520generated%2520masks%2520iteratively%2520induce%2520the%2520prompt%2520generator%2520to%2520focus%250Amore%2520on%2520task-relevant%2520image%2520areas%2520and%2520reduce%2520irrelevant%2520hallucinations%252C%250Aresulting%2520jointly%2520in%2520better%2520prompts%2520and%2520masks.%2520Experiments%2520on%25205%2520benchmarks%250Ademonstrate%2520the%2520effectiveness%2520of%2520ProMaC.%2520Code%2520given%2520in%250Ahttps%253A//lwpyh.github.io/ProMaC/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.15205v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Leveraging%20Hallucinations%20to%20Reduce%20Manual%20Prompt%20Dependency%20in%0A%20%20Promptable%20Segmentation&entry.906535625=Jian%20Hu%20and%20Jiayi%20Lin%20and%20Junchi%20Yan%20and%20Shaogang%20Gong&entry.1292438233=%20%20Promptable%20segmentation%20typically%20requires%20instance-specific%20manual%20prompts%0Ato%20guide%20the%20segmentation%20of%20each%20desired%20object.%20To%20minimize%20such%20a%20need%2C%0Atask-generic%20promptable%20segmentation%20has%20been%20introduced%2C%20which%20employs%20a%0Asingle%20task-generic%20prompt%20to%20segment%20various%20images%20of%20different%20objects%20in%0Athe%20same%20task.%20Current%20methods%20use%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20to%0Areason%20detailed%20instance-specific%20prompts%20from%20a%20task-generic%20prompt%20for%0Aimproving%20segmentation%20accuracy.%20The%20effectiveness%20of%20this%20segmentation%20heavily%0Adepends%20on%20the%20precision%20of%20these%20derived%20prompts.%20However%2C%20MLLMs%20often%20suffer%0Ahallucinations%20during%20reasoning%2C%20resulting%20in%20inaccurate%20prompting.%20While%0Aexisting%20methods%20focus%20on%20eliminating%20hallucinations%20to%20improve%20a%20model%2C%20we%0Aargue%20that%20MLLM%20hallucinations%20can%20reveal%20valuable%20contextual%20insights%20when%0Aleveraged%20correctly%2C%20as%20they%20represent%20pre-trained%20large-scale%20knowledge%20beyond%0Aindividual%20images.%20In%20this%20paper%2C%20we%20utilize%20hallucinations%20to%20mine%0Atask-related%20information%20from%20images%20and%20verify%20its%20accuracy%20for%20enhancing%0Aprecision%20of%20the%20generated%20prompts.%20Specifically%2C%20we%20introduce%20an%20iterative%0APrompt-Mask%20Cycle%20generation%20framework%20%28ProMaC%29%20with%20a%20prompt%20generator%20and%20a%0Amask%20generator.The%20prompt%20generator%20uses%20a%20multi-scale%20chain%20of%20thought%0Aprompting%2C%20initially%20exploring%20hallucinations%20for%20extracting%20extended%0Acontextual%20knowledge%20on%20a%20test%20image.These%20hallucinations%20are%20then%20reduced%20to%0Aformulate%20precise%20instance-specific%20prompts%2C%20directing%20the%20mask%20generator%20to%0Aproduce%20masks%20that%20are%20consistent%20with%20task%20semantics%20by%20mask%20semantic%0Aalignment.%20The%20generated%20masks%20iteratively%20induce%20the%20prompt%20generator%20to%20focus%0Amore%20on%20task-relevant%20image%20areas%20and%20reduce%20irrelevant%20hallucinations%2C%0Aresulting%20jointly%20in%20better%20prompts%20and%20masks.%20Experiments%20on%205%20benchmarks%0Ademonstrate%20the%20effectiveness%20of%20ProMaC.%20Code%20given%20in%0Ahttps%3A//lwpyh.github.io/ProMaC/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.15205v2&entry.124074799=Read"},
{"title": "Relaxed Equivariance via Multitask Learning", "author": "Ahmed A. Elhag and T. Konstantin Rusch and Francesco Di Giovanni and Michael Bronstein", "abstract": "  Incorporating equivariance as an inductive bias into deep learning\narchitectures to take advantage of the data symmetry has been successful in\nmultiple applications, such as chemistry and dynamical systems. In particular,\nroto-translations are crucial for effectively modeling geometric graphs and\nmolecules, where understanding the 3D structures enhances generalization.\nHowever, equivariant models often pose challenges due to their high\ncomputational complexity. In this paper, we introduce REMUL, a training\nprocedure for approximating equivariance with multitask learning. We show that\nunconstrained models (which do not build equivariance into the architecture)\ncan learn approximate symmetries by minimizing an additional simple\nequivariance loss. By formulating equivariance as a new learning objective, we\ncan control the level of approximate equivariance in the model. Our method\nachieves competitive performance compared to equivariant baselines while being\n$10 \\times$ faster at inference and $2.5 \\times$ at training.\n", "link": "http://arxiv.org/abs/2410.17878v1", "date": "2024-10-23", "relevancy": 2.019, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5249}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5069}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relaxed%20Equivariance%20via%20Multitask%20Learning&body=Title%3A%20Relaxed%20Equivariance%20via%20Multitask%20Learning%0AAuthor%3A%20Ahmed%20A.%20Elhag%20and%20T.%20Konstantin%20Rusch%20and%20Francesco%20Di%20Giovanni%20and%20Michael%20Bronstein%0AAbstract%3A%20%20%20Incorporating%20equivariance%20as%20an%20inductive%20bias%20into%20deep%20learning%0Aarchitectures%20to%20take%20advantage%20of%20the%20data%20symmetry%20has%20been%20successful%20in%0Amultiple%20applications%2C%20such%20as%20chemistry%20and%20dynamical%20systems.%20In%20particular%2C%0Aroto-translations%20are%20crucial%20for%20effectively%20modeling%20geometric%20graphs%20and%0Amolecules%2C%20where%20understanding%20the%203D%20structures%20enhances%20generalization.%0AHowever%2C%20equivariant%20models%20often%20pose%20challenges%20due%20to%20their%20high%0Acomputational%20complexity.%20In%20this%20paper%2C%20we%20introduce%20REMUL%2C%20a%20training%0Aprocedure%20for%20approximating%20equivariance%20with%20multitask%20learning.%20We%20show%20that%0Aunconstrained%20models%20%28which%20do%20not%20build%20equivariance%20into%20the%20architecture%29%0Acan%20learn%20approximate%20symmetries%20by%20minimizing%20an%20additional%20simple%0Aequivariance%20loss.%20By%20formulating%20equivariance%20as%20a%20new%20learning%20objective%2C%20we%0Acan%20control%20the%20level%20of%20approximate%20equivariance%20in%20the%20model.%20Our%20method%0Aachieves%20competitive%20performance%20compared%20to%20equivariant%20baselines%20while%20being%0A%2410%20%5Ctimes%24%20faster%20at%20inference%20and%20%242.5%20%5Ctimes%24%20at%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17878v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelaxed%2520Equivariance%2520via%2520Multitask%2520Learning%26entry.906535625%3DAhmed%2520A.%2520Elhag%2520and%2520T.%2520Konstantin%2520Rusch%2520and%2520Francesco%2520Di%2520Giovanni%2520and%2520Michael%2520Bronstein%26entry.1292438233%3D%2520%2520Incorporating%2520equivariance%2520as%2520an%2520inductive%2520bias%2520into%2520deep%2520learning%250Aarchitectures%2520to%2520take%2520advantage%2520of%2520the%2520data%2520symmetry%2520has%2520been%2520successful%2520in%250Amultiple%2520applications%252C%2520such%2520as%2520chemistry%2520and%2520dynamical%2520systems.%2520In%2520particular%252C%250Aroto-translations%2520are%2520crucial%2520for%2520effectively%2520modeling%2520geometric%2520graphs%2520and%250Amolecules%252C%2520where%2520understanding%2520the%25203D%2520structures%2520enhances%2520generalization.%250AHowever%252C%2520equivariant%2520models%2520often%2520pose%2520challenges%2520due%2520to%2520their%2520high%250Acomputational%2520complexity.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520REMUL%252C%2520a%2520training%250Aprocedure%2520for%2520approximating%2520equivariance%2520with%2520multitask%2520learning.%2520We%2520show%2520that%250Aunconstrained%2520models%2520%2528which%2520do%2520not%2520build%2520equivariance%2520into%2520the%2520architecture%2529%250Acan%2520learn%2520approximate%2520symmetries%2520by%2520minimizing%2520an%2520additional%2520simple%250Aequivariance%2520loss.%2520By%2520formulating%2520equivariance%2520as%2520a%2520new%2520learning%2520objective%252C%2520we%250Acan%2520control%2520the%2520level%2520of%2520approximate%2520equivariance%2520in%2520the%2520model.%2520Our%2520method%250Aachieves%2520competitive%2520performance%2520compared%2520to%2520equivariant%2520baselines%2520while%2520being%250A%252410%2520%255Ctimes%2524%2520faster%2520at%2520inference%2520and%2520%25242.5%2520%255Ctimes%2524%2520at%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17878v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relaxed%20Equivariance%20via%20Multitask%20Learning&entry.906535625=Ahmed%20A.%20Elhag%20and%20T.%20Konstantin%20Rusch%20and%20Francesco%20Di%20Giovanni%20and%20Michael%20Bronstein&entry.1292438233=%20%20Incorporating%20equivariance%20as%20an%20inductive%20bias%20into%20deep%20learning%0Aarchitectures%20to%20take%20advantage%20of%20the%20data%20symmetry%20has%20been%20successful%20in%0Amultiple%20applications%2C%20such%20as%20chemistry%20and%20dynamical%20systems.%20In%20particular%2C%0Aroto-translations%20are%20crucial%20for%20effectively%20modeling%20geometric%20graphs%20and%0Amolecules%2C%20where%20understanding%20the%203D%20structures%20enhances%20generalization.%0AHowever%2C%20equivariant%20models%20often%20pose%20challenges%20due%20to%20their%20high%0Acomputational%20complexity.%20In%20this%20paper%2C%20we%20introduce%20REMUL%2C%20a%20training%0Aprocedure%20for%20approximating%20equivariance%20with%20multitask%20learning.%20We%20show%20that%0Aunconstrained%20models%20%28which%20do%20not%20build%20equivariance%20into%20the%20architecture%29%0Acan%20learn%20approximate%20symmetries%20by%20minimizing%20an%20additional%20simple%0Aequivariance%20loss.%20By%20formulating%20equivariance%20as%20a%20new%20learning%20objective%2C%20we%0Acan%20control%20the%20level%20of%20approximate%20equivariance%20in%20the%20model.%20Our%20method%0Aachieves%20competitive%20performance%20compared%20to%20equivariant%20baselines%20while%20being%0A%2410%20%5Ctimes%24%20faster%20at%20inference%20and%20%242.5%20%5Ctimes%24%20at%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17878v1&entry.124074799=Read"},
{"title": "AdaRankGrad: Adaptive Gradient-Rank and Moments for Memory-Efficient\n  LLMs Training and Fine-Tuning", "author": "Yehonathan Refael and Jonathan Svirsky and Boris Shustin and Wasim Huleihel and Ofir Lindenbaum", "abstract": "  Training and fine-tuning large language models (LLMs) come with challenges\nrelated to memory and computational requirements due to the increasing size of\nthe model weights and the optimizer states. Various techniques have been\ndeveloped to tackle these challenges, such as low-rank adaptation (LoRA), which\ninvolves introducing a parallel trainable low-rank matrix to the fixed\npre-trained weights at each layer. However, these methods often fall short\ncompared to the full-rank weight training approach, as they restrict the\nparameter search to a low-rank subspace. This limitation can disrupt training\ndynamics and require a full-rank warm start to mitigate the impact. In this\npaper, we introduce a new method inspired by a phenomenon we formally prove: as\ntraining progresses, the rank of the estimated layer gradients gradually\ndecreases, and asymptotically approaches rank one. Leveraging this, our\napproach involves adaptively reducing the rank of the gradients during Adam\noptimization steps, using an efficient online-updating low-rank projections\nrule. We further present a randomized SVD scheme for efficiently finding the\nprojection matrix. Our technique enables full-parameter fine-tuning with\nadaptive low-rank gradient updates, significantly reducing overall memory\nrequirements during training compared to state-of-the-art methods while\nimproving model performance in both pretraining and fine-tuning. Finally, we\nprovide a convergence analysis of our method and demonstrate its merits for\ntraining and fine-tuning language and biological foundation models.\n", "link": "http://arxiv.org/abs/2410.17881v1", "date": "2024-10-23", "relevancy": 2.0117, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5168}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4957}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaRankGrad%3A%20Adaptive%20Gradient-Rank%20and%20Moments%20for%20Memory-Efficient%0A%20%20LLMs%20Training%20and%20Fine-Tuning&body=Title%3A%20AdaRankGrad%3A%20Adaptive%20Gradient-Rank%20and%20Moments%20for%20Memory-Efficient%0A%20%20LLMs%20Training%20and%20Fine-Tuning%0AAuthor%3A%20Yehonathan%20Refael%20and%20Jonathan%20Svirsky%20and%20Boris%20Shustin%20and%20Wasim%20Huleihel%20and%20Ofir%20Lindenbaum%0AAbstract%3A%20%20%20Training%20and%20fine-tuning%20large%20language%20models%20%28LLMs%29%20come%20with%20challenges%0Arelated%20to%20memory%20and%20computational%20requirements%20due%20to%20the%20increasing%20size%20of%0Athe%20model%20weights%20and%20the%20optimizer%20states.%20Various%20techniques%20have%20been%0Adeveloped%20to%20tackle%20these%20challenges%2C%20such%20as%20low-rank%20adaptation%20%28LoRA%29%2C%20which%0Ainvolves%20introducing%20a%20parallel%20trainable%20low-rank%20matrix%20to%20the%20fixed%0Apre-trained%20weights%20at%20each%20layer.%20However%2C%20these%20methods%20often%20fall%20short%0Acompared%20to%20the%20full-rank%20weight%20training%20approach%2C%20as%20they%20restrict%20the%0Aparameter%20search%20to%20a%20low-rank%20subspace.%20This%20limitation%20can%20disrupt%20training%0Adynamics%20and%20require%20a%20full-rank%20warm%20start%20to%20mitigate%20the%20impact.%20In%20this%0Apaper%2C%20we%20introduce%20a%20new%20method%20inspired%20by%20a%20phenomenon%20we%20formally%20prove%3A%20as%0Atraining%20progresses%2C%20the%20rank%20of%20the%20estimated%20layer%20gradients%20gradually%0Adecreases%2C%20and%20asymptotically%20approaches%20rank%20one.%20Leveraging%20this%2C%20our%0Aapproach%20involves%20adaptively%20reducing%20the%20rank%20of%20the%20gradients%20during%20Adam%0Aoptimization%20steps%2C%20using%20an%20efficient%20online-updating%20low-rank%20projections%0Arule.%20We%20further%20present%20a%20randomized%20SVD%20scheme%20for%20efficiently%20finding%20the%0Aprojection%20matrix.%20Our%20technique%20enables%20full-parameter%20fine-tuning%20with%0Aadaptive%20low-rank%20gradient%20updates%2C%20significantly%20reducing%20overall%20memory%0Arequirements%20during%20training%20compared%20to%20state-of-the-art%20methods%20while%0Aimproving%20model%20performance%20in%20both%20pretraining%20and%20fine-tuning.%20Finally%2C%20we%0Aprovide%20a%20convergence%20analysis%20of%20our%20method%20and%20demonstrate%20its%20merits%20for%0Atraining%20and%20fine-tuning%20language%20and%20biological%20foundation%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaRankGrad%253A%2520Adaptive%2520Gradient-Rank%2520and%2520Moments%2520for%2520Memory-Efficient%250A%2520%2520LLMs%2520Training%2520and%2520Fine-Tuning%26entry.906535625%3DYehonathan%2520Refael%2520and%2520Jonathan%2520Svirsky%2520and%2520Boris%2520Shustin%2520and%2520Wasim%2520Huleihel%2520and%2520Ofir%2520Lindenbaum%26entry.1292438233%3D%2520%2520Training%2520and%2520fine-tuning%2520large%2520language%2520models%2520%2528LLMs%2529%2520come%2520with%2520challenges%250Arelated%2520to%2520memory%2520and%2520computational%2520requirements%2520due%2520to%2520the%2520increasing%2520size%2520of%250Athe%2520model%2520weights%2520and%2520the%2520optimizer%2520states.%2520Various%2520techniques%2520have%2520been%250Adeveloped%2520to%2520tackle%2520these%2520challenges%252C%2520such%2520as%2520low-rank%2520adaptation%2520%2528LoRA%2529%252C%2520which%250Ainvolves%2520introducing%2520a%2520parallel%2520trainable%2520low-rank%2520matrix%2520to%2520the%2520fixed%250Apre-trained%2520weights%2520at%2520each%2520layer.%2520However%252C%2520these%2520methods%2520often%2520fall%2520short%250Acompared%2520to%2520the%2520full-rank%2520weight%2520training%2520approach%252C%2520as%2520they%2520restrict%2520the%250Aparameter%2520search%2520to%2520a%2520low-rank%2520subspace.%2520This%2520limitation%2520can%2520disrupt%2520training%250Adynamics%2520and%2520require%2520a%2520full-rank%2520warm%2520start%2520to%2520mitigate%2520the%2520impact.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520a%2520new%2520method%2520inspired%2520by%2520a%2520phenomenon%2520we%2520formally%2520prove%253A%2520as%250Atraining%2520progresses%252C%2520the%2520rank%2520of%2520the%2520estimated%2520layer%2520gradients%2520gradually%250Adecreases%252C%2520and%2520asymptotically%2520approaches%2520rank%2520one.%2520Leveraging%2520this%252C%2520our%250Aapproach%2520involves%2520adaptively%2520reducing%2520the%2520rank%2520of%2520the%2520gradients%2520during%2520Adam%250Aoptimization%2520steps%252C%2520using%2520an%2520efficient%2520online-updating%2520low-rank%2520projections%250Arule.%2520We%2520further%2520present%2520a%2520randomized%2520SVD%2520scheme%2520for%2520efficiently%2520finding%2520the%250Aprojection%2520matrix.%2520Our%2520technique%2520enables%2520full-parameter%2520fine-tuning%2520with%250Aadaptive%2520low-rank%2520gradient%2520updates%252C%2520significantly%2520reducing%2520overall%2520memory%250Arequirements%2520during%2520training%2520compared%2520to%2520state-of-the-art%2520methods%2520while%250Aimproving%2520model%2520performance%2520in%2520both%2520pretraining%2520and%2520fine-tuning.%2520Finally%252C%2520we%250Aprovide%2520a%2520convergence%2520analysis%2520of%2520our%2520method%2520and%2520demonstrate%2520its%2520merits%2520for%250Atraining%2520and%2520fine-tuning%2520language%2520and%2520biological%2520foundation%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaRankGrad%3A%20Adaptive%20Gradient-Rank%20and%20Moments%20for%20Memory-Efficient%0A%20%20LLMs%20Training%20and%20Fine-Tuning&entry.906535625=Yehonathan%20Refael%20and%20Jonathan%20Svirsky%20and%20Boris%20Shustin%20and%20Wasim%20Huleihel%20and%20Ofir%20Lindenbaum&entry.1292438233=%20%20Training%20and%20fine-tuning%20large%20language%20models%20%28LLMs%29%20come%20with%20challenges%0Arelated%20to%20memory%20and%20computational%20requirements%20due%20to%20the%20increasing%20size%20of%0Athe%20model%20weights%20and%20the%20optimizer%20states.%20Various%20techniques%20have%20been%0Adeveloped%20to%20tackle%20these%20challenges%2C%20such%20as%20low-rank%20adaptation%20%28LoRA%29%2C%20which%0Ainvolves%20introducing%20a%20parallel%20trainable%20low-rank%20matrix%20to%20the%20fixed%0Apre-trained%20weights%20at%20each%20layer.%20However%2C%20these%20methods%20often%20fall%20short%0Acompared%20to%20the%20full-rank%20weight%20training%20approach%2C%20as%20they%20restrict%20the%0Aparameter%20search%20to%20a%20low-rank%20subspace.%20This%20limitation%20can%20disrupt%20training%0Adynamics%20and%20require%20a%20full-rank%20warm%20start%20to%20mitigate%20the%20impact.%20In%20this%0Apaper%2C%20we%20introduce%20a%20new%20method%20inspired%20by%20a%20phenomenon%20we%20formally%20prove%3A%20as%0Atraining%20progresses%2C%20the%20rank%20of%20the%20estimated%20layer%20gradients%20gradually%0Adecreases%2C%20and%20asymptotically%20approaches%20rank%20one.%20Leveraging%20this%2C%20our%0Aapproach%20involves%20adaptively%20reducing%20the%20rank%20of%20the%20gradients%20during%20Adam%0Aoptimization%20steps%2C%20using%20an%20efficient%20online-updating%20low-rank%20projections%0Arule.%20We%20further%20present%20a%20randomized%20SVD%20scheme%20for%20efficiently%20finding%20the%0Aprojection%20matrix.%20Our%20technique%20enables%20full-parameter%20fine-tuning%20with%0Aadaptive%20low-rank%20gradient%20updates%2C%20significantly%20reducing%20overall%20memory%0Arequirements%20during%20training%20compared%20to%20state-of-the-art%20methods%20while%0Aimproving%20model%20performance%20in%20both%20pretraining%20and%20fine-tuning.%20Finally%2C%20we%0Aprovide%20a%20convergence%20analysis%20of%20our%20method%20and%20demonstrate%20its%20merits%20for%0Atraining%20and%20fine-tuning%20language%20and%20biological%20foundation%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17881v1&entry.124074799=Read"},
{"title": "ShapefileGPT: A Multi-Agent Large Language Model Framework for Automated\n  Shapefile Processing", "author": "Qingming Lin and Rui Hu and Huaxia Li and Sensen Wu and Yadong Li and Kai Fang and Hailin Feng and Zhenhong Du and Liuchang Xu", "abstract": "  Vector data is one of the two core data structures in geographic information\nscience (GIS), essential for accurately storing and representing geospatial\ninformation. Shapefile, the most widely used vector data format, has become the\nindustry standard supported by all major geographic information systems.\nHowever, processing this data typically requires specialized GIS knowledge and\nskills, creating a barrier for researchers from other fields and impeding\ninterdisciplinary research in spatial data analysis. Moreover, while large\nlanguage models (LLMs) have made significant advancements in natural language\nprocessing and task automation, they still face challenges in handling the\ncomplex spatial and topological relationships inherent in GIS vector data. To\naddress these challenges, we propose ShapefileGPT, an innovative framework\npowered by LLMs, specifically designed to automate Shapefile tasks.\nShapefileGPT utilizes a multi-agent architecture, in which the planner agent is\nresponsible for task decomposition and supervision, while the worker agent\nexecutes the tasks. We developed a specialized function library for handling\nShapefiles and provided comprehensive API documentation, enabling the worker\nagent to operate Shapefiles efficiently through function calling. For\nevaluation, we developed a benchmark dataset based on authoritative textbooks,\nencompassing tasks in categories such as geometric operations and spatial\nqueries. ShapefileGPT achieved a task success rate of 95.24%, outperforming the\nGPT series models. In comparison to traditional LLMs, ShapefileGPT effectively\nhandles complex vector data analysis tasks, overcoming the limitations of\ntraditional LLMs in spatial analysis. This breakthrough opens new pathways for\nadvancing automation and intelligence in the GIS field, with significant\npotential in interdisciplinary data analysis and application contexts.\n", "link": "http://arxiv.org/abs/2410.12376v2", "date": "2024-10-23", "relevancy": 2.009, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5183}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4954}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ShapefileGPT%3A%20A%20Multi-Agent%20Large%20Language%20Model%20Framework%20for%20Automated%0A%20%20Shapefile%20Processing&body=Title%3A%20ShapefileGPT%3A%20A%20Multi-Agent%20Large%20Language%20Model%20Framework%20for%20Automated%0A%20%20Shapefile%20Processing%0AAuthor%3A%20Qingming%20Lin%20and%20Rui%20Hu%20and%20Huaxia%20Li%20and%20Sensen%20Wu%20and%20Yadong%20Li%20and%20Kai%20Fang%20and%20Hailin%20Feng%20and%20Zhenhong%20Du%20and%20Liuchang%20Xu%0AAbstract%3A%20%20%20Vector%20data%20is%20one%20of%20the%20two%20core%20data%20structures%20in%20geographic%20information%0Ascience%20%28GIS%29%2C%20essential%20for%20accurately%20storing%20and%20representing%20geospatial%0Ainformation.%20Shapefile%2C%20the%20most%20widely%20used%20vector%20data%20format%2C%20has%20become%20the%0Aindustry%20standard%20supported%20by%20all%20major%20geographic%20information%20systems.%0AHowever%2C%20processing%20this%20data%20typically%20requires%20specialized%20GIS%20knowledge%20and%0Askills%2C%20creating%20a%20barrier%20for%20researchers%20from%20other%20fields%20and%20impeding%0Ainterdisciplinary%20research%20in%20spatial%20data%20analysis.%20Moreover%2C%20while%20large%0Alanguage%20models%20%28LLMs%29%20have%20made%20significant%20advancements%20in%20natural%20language%0Aprocessing%20and%20task%20automation%2C%20they%20still%20face%20challenges%20in%20handling%20the%0Acomplex%20spatial%20and%20topological%20relationships%20inherent%20in%20GIS%20vector%20data.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20ShapefileGPT%2C%20an%20innovative%20framework%0Apowered%20by%20LLMs%2C%20specifically%20designed%20to%20automate%20Shapefile%20tasks.%0AShapefileGPT%20utilizes%20a%20multi-agent%20architecture%2C%20in%20which%20the%20planner%20agent%20is%0Aresponsible%20for%20task%20decomposition%20and%20supervision%2C%20while%20the%20worker%20agent%0Aexecutes%20the%20tasks.%20We%20developed%20a%20specialized%20function%20library%20for%20handling%0AShapefiles%20and%20provided%20comprehensive%20API%20documentation%2C%20enabling%20the%20worker%0Aagent%20to%20operate%20Shapefiles%20efficiently%20through%20function%20calling.%20For%0Aevaluation%2C%20we%20developed%20a%20benchmark%20dataset%20based%20on%20authoritative%20textbooks%2C%0Aencompassing%20tasks%20in%20categories%20such%20as%20geometric%20operations%20and%20spatial%0Aqueries.%20ShapefileGPT%20achieved%20a%20task%20success%20rate%20of%2095.24%25%2C%20outperforming%20the%0AGPT%20series%20models.%20In%20comparison%20to%20traditional%20LLMs%2C%20ShapefileGPT%20effectively%0Ahandles%20complex%20vector%20data%20analysis%20tasks%2C%20overcoming%20the%20limitations%20of%0Atraditional%20LLMs%20in%20spatial%20analysis.%20This%20breakthrough%20opens%20new%20pathways%20for%0Aadvancing%20automation%20and%20intelligence%20in%20the%20GIS%20field%2C%20with%20significant%0Apotential%20in%20interdisciplinary%20data%20analysis%20and%20application%20contexts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.12376v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShapefileGPT%253A%2520A%2520Multi-Agent%2520Large%2520Language%2520Model%2520Framework%2520for%2520Automated%250A%2520%2520Shapefile%2520Processing%26entry.906535625%3DQingming%2520Lin%2520and%2520Rui%2520Hu%2520and%2520Huaxia%2520Li%2520and%2520Sensen%2520Wu%2520and%2520Yadong%2520Li%2520and%2520Kai%2520Fang%2520and%2520Hailin%2520Feng%2520and%2520Zhenhong%2520Du%2520and%2520Liuchang%2520Xu%26entry.1292438233%3D%2520%2520Vector%2520data%2520is%2520one%2520of%2520the%2520two%2520core%2520data%2520structures%2520in%2520geographic%2520information%250Ascience%2520%2528GIS%2529%252C%2520essential%2520for%2520accurately%2520storing%2520and%2520representing%2520geospatial%250Ainformation.%2520Shapefile%252C%2520the%2520most%2520widely%2520used%2520vector%2520data%2520format%252C%2520has%2520become%2520the%250Aindustry%2520standard%2520supported%2520by%2520all%2520major%2520geographic%2520information%2520systems.%250AHowever%252C%2520processing%2520this%2520data%2520typically%2520requires%2520specialized%2520GIS%2520knowledge%2520and%250Askills%252C%2520creating%2520a%2520barrier%2520for%2520researchers%2520from%2520other%2520fields%2520and%2520impeding%250Ainterdisciplinary%2520research%2520in%2520spatial%2520data%2520analysis.%2520Moreover%252C%2520while%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520have%2520made%2520significant%2520advancements%2520in%2520natural%2520language%250Aprocessing%2520and%2520task%2520automation%252C%2520they%2520still%2520face%2520challenges%2520in%2520handling%2520the%250Acomplex%2520spatial%2520and%2520topological%2520relationships%2520inherent%2520in%2520GIS%2520vector%2520data.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520ShapefileGPT%252C%2520an%2520innovative%2520framework%250Apowered%2520by%2520LLMs%252C%2520specifically%2520designed%2520to%2520automate%2520Shapefile%2520tasks.%250AShapefileGPT%2520utilizes%2520a%2520multi-agent%2520architecture%252C%2520in%2520which%2520the%2520planner%2520agent%2520is%250Aresponsible%2520for%2520task%2520decomposition%2520and%2520supervision%252C%2520while%2520the%2520worker%2520agent%250Aexecutes%2520the%2520tasks.%2520We%2520developed%2520a%2520specialized%2520function%2520library%2520for%2520handling%250AShapefiles%2520and%2520provided%2520comprehensive%2520API%2520documentation%252C%2520enabling%2520the%2520worker%250Aagent%2520to%2520operate%2520Shapefiles%2520efficiently%2520through%2520function%2520calling.%2520For%250Aevaluation%252C%2520we%2520developed%2520a%2520benchmark%2520dataset%2520based%2520on%2520authoritative%2520textbooks%252C%250Aencompassing%2520tasks%2520in%2520categories%2520such%2520as%2520geometric%2520operations%2520and%2520spatial%250Aqueries.%2520ShapefileGPT%2520achieved%2520a%2520task%2520success%2520rate%2520of%252095.24%2525%252C%2520outperforming%2520the%250AGPT%2520series%2520models.%2520In%2520comparison%2520to%2520traditional%2520LLMs%252C%2520ShapefileGPT%2520effectively%250Ahandles%2520complex%2520vector%2520data%2520analysis%2520tasks%252C%2520overcoming%2520the%2520limitations%2520of%250Atraditional%2520LLMs%2520in%2520spatial%2520analysis.%2520This%2520breakthrough%2520opens%2520new%2520pathways%2520for%250Aadvancing%2520automation%2520and%2520intelligence%2520in%2520the%2520GIS%2520field%252C%2520with%2520significant%250Apotential%2520in%2520interdisciplinary%2520data%2520analysis%2520and%2520application%2520contexts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.12376v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ShapefileGPT%3A%20A%20Multi-Agent%20Large%20Language%20Model%20Framework%20for%20Automated%0A%20%20Shapefile%20Processing&entry.906535625=Qingming%20Lin%20and%20Rui%20Hu%20and%20Huaxia%20Li%20and%20Sensen%20Wu%20and%20Yadong%20Li%20and%20Kai%20Fang%20and%20Hailin%20Feng%20and%20Zhenhong%20Du%20and%20Liuchang%20Xu&entry.1292438233=%20%20Vector%20data%20is%20one%20of%20the%20two%20core%20data%20structures%20in%20geographic%20information%0Ascience%20%28GIS%29%2C%20essential%20for%20accurately%20storing%20and%20representing%20geospatial%0Ainformation.%20Shapefile%2C%20the%20most%20widely%20used%20vector%20data%20format%2C%20has%20become%20the%0Aindustry%20standard%20supported%20by%20all%20major%20geographic%20information%20systems.%0AHowever%2C%20processing%20this%20data%20typically%20requires%20specialized%20GIS%20knowledge%20and%0Askills%2C%20creating%20a%20barrier%20for%20researchers%20from%20other%20fields%20and%20impeding%0Ainterdisciplinary%20research%20in%20spatial%20data%20analysis.%20Moreover%2C%20while%20large%0Alanguage%20models%20%28LLMs%29%20have%20made%20significant%20advancements%20in%20natural%20language%0Aprocessing%20and%20task%20automation%2C%20they%20still%20face%20challenges%20in%20handling%20the%0Acomplex%20spatial%20and%20topological%20relationships%20inherent%20in%20GIS%20vector%20data.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20ShapefileGPT%2C%20an%20innovative%20framework%0Apowered%20by%20LLMs%2C%20specifically%20designed%20to%20automate%20Shapefile%20tasks.%0AShapefileGPT%20utilizes%20a%20multi-agent%20architecture%2C%20in%20which%20the%20planner%20agent%20is%0Aresponsible%20for%20task%20decomposition%20and%20supervision%2C%20while%20the%20worker%20agent%0Aexecutes%20the%20tasks.%20We%20developed%20a%20specialized%20function%20library%20for%20handling%0AShapefiles%20and%20provided%20comprehensive%20API%20documentation%2C%20enabling%20the%20worker%0Aagent%20to%20operate%20Shapefiles%20efficiently%20through%20function%20calling.%20For%0Aevaluation%2C%20we%20developed%20a%20benchmark%20dataset%20based%20on%20authoritative%20textbooks%2C%0Aencompassing%20tasks%20in%20categories%20such%20as%20geometric%20operations%20and%20spatial%0Aqueries.%20ShapefileGPT%20achieved%20a%20task%20success%20rate%20of%2095.24%25%2C%20outperforming%20the%0AGPT%20series%20models.%20In%20comparison%20to%20traditional%20LLMs%2C%20ShapefileGPT%20effectively%0Ahandles%20complex%20vector%20data%20analysis%20tasks%2C%20overcoming%20the%20limitations%20of%0Atraditional%20LLMs%20in%20spatial%20analysis.%20This%20breakthrough%20opens%20new%20pathways%20for%0Aadvancing%20automation%20and%20intelligence%20in%20the%20GIS%20field%2C%20with%20significant%0Apotential%20in%20interdisciplinary%20data%20analysis%20and%20application%20contexts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.12376v2&entry.124074799=Read"},
{"title": "Pruning By Explaining Revisited: Optimizing Attribution Methods to Prune\n  CNNs and Transformers", "author": "Sayed Mohammad Vakilzadeh Hatefi and Maximilian Dreyer and Reduan Achtibat and Thomas Wiegand and Wojciech Samek and Sebastian Lapuschkin", "abstract": "  To solve ever more complex problems, Deep Neural Networks are scaled to\nbillions of parameters, leading to huge computational costs. An effective\napproach to reduce computational requirements and increase efficiency is to\nprune unnecessary components of these often over-parameterized networks.\nPrevious work has shown that attribution methods from the field of eXplainable\nAI serve as effective means to extract and prune the least relevant network\ncomponents in a few-shot fashion. We extend the current state by proposing to\nexplicitly optimize hyperparameters of attribution methods for the task of\npruning, and further include transformer-based networks in our analysis. Our\napproach yields higher model compression rates of large transformer- and\nconvolutional architectures (VGG, ResNet, ViT) compared to previous works,\nwhile still attaining high performance on ImageNet classification tasks. Here,\nour experiments indicate that transformers have a higher degree of\nover-parameterization compared to convolutional neural networks. Code is\navailable at https://github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch.\n", "link": "http://arxiv.org/abs/2408.12568v2", "date": "2024-10-23", "relevancy": 2.0061, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5272}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5064}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pruning%20By%20Explaining%20Revisited%3A%20Optimizing%20Attribution%20Methods%20to%20Prune%0A%20%20CNNs%20and%20Transformers&body=Title%3A%20Pruning%20By%20Explaining%20Revisited%3A%20Optimizing%20Attribution%20Methods%20to%20Prune%0A%20%20CNNs%20and%20Transformers%0AAuthor%3A%20Sayed%20Mohammad%20Vakilzadeh%20Hatefi%20and%20Maximilian%20Dreyer%20and%20Reduan%20Achtibat%20and%20Thomas%20Wiegand%20and%20Wojciech%20Samek%20and%20Sebastian%20Lapuschkin%0AAbstract%3A%20%20%20To%20solve%20ever%20more%20complex%20problems%2C%20Deep%20Neural%20Networks%20are%20scaled%20to%0Abillions%20of%20parameters%2C%20leading%20to%20huge%20computational%20costs.%20An%20effective%0Aapproach%20to%20reduce%20computational%20requirements%20and%20increase%20efficiency%20is%20to%0Aprune%20unnecessary%20components%20of%20these%20often%20over-parameterized%20networks.%0APrevious%20work%20has%20shown%20that%20attribution%20methods%20from%20the%20field%20of%20eXplainable%0AAI%20serve%20as%20effective%20means%20to%20extract%20and%20prune%20the%20least%20relevant%20network%0Acomponents%20in%20a%20few-shot%20fashion.%20We%20extend%20the%20current%20state%20by%20proposing%20to%0Aexplicitly%20optimize%20hyperparameters%20of%20attribution%20methods%20for%20the%20task%20of%0Apruning%2C%20and%20further%20include%20transformer-based%20networks%20in%20our%20analysis.%20Our%0Aapproach%20yields%20higher%20model%20compression%20rates%20of%20large%20transformer-%20and%0Aconvolutional%20architectures%20%28VGG%2C%20ResNet%2C%20ViT%29%20compared%20to%20previous%20works%2C%0Awhile%20still%20attaining%20high%20performance%20on%20ImageNet%20classification%20tasks.%20Here%2C%0Aour%20experiments%20indicate%20that%20transformers%20have%20a%20higher%20degree%20of%0Aover-parameterization%20compared%20to%20convolutional%20neural%20networks.%20Code%20is%0Aavailable%20at%20https%3A//github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12568v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPruning%2520By%2520Explaining%2520Revisited%253A%2520Optimizing%2520Attribution%2520Methods%2520to%2520Prune%250A%2520%2520CNNs%2520and%2520Transformers%26entry.906535625%3DSayed%2520Mohammad%2520Vakilzadeh%2520Hatefi%2520and%2520Maximilian%2520Dreyer%2520and%2520Reduan%2520Achtibat%2520and%2520Thomas%2520Wiegand%2520and%2520Wojciech%2520Samek%2520and%2520Sebastian%2520Lapuschkin%26entry.1292438233%3D%2520%2520To%2520solve%2520ever%2520more%2520complex%2520problems%252C%2520Deep%2520Neural%2520Networks%2520are%2520scaled%2520to%250Abillions%2520of%2520parameters%252C%2520leading%2520to%2520huge%2520computational%2520costs.%2520An%2520effective%250Aapproach%2520to%2520reduce%2520computational%2520requirements%2520and%2520increase%2520efficiency%2520is%2520to%250Aprune%2520unnecessary%2520components%2520of%2520these%2520often%2520over-parameterized%2520networks.%250APrevious%2520work%2520has%2520shown%2520that%2520attribution%2520methods%2520from%2520the%2520field%2520of%2520eXplainable%250AAI%2520serve%2520as%2520effective%2520means%2520to%2520extract%2520and%2520prune%2520the%2520least%2520relevant%2520network%250Acomponents%2520in%2520a%2520few-shot%2520fashion.%2520We%2520extend%2520the%2520current%2520state%2520by%2520proposing%2520to%250Aexplicitly%2520optimize%2520hyperparameters%2520of%2520attribution%2520methods%2520for%2520the%2520task%2520of%250Apruning%252C%2520and%2520further%2520include%2520transformer-based%2520networks%2520in%2520our%2520analysis.%2520Our%250Aapproach%2520yields%2520higher%2520model%2520compression%2520rates%2520of%2520large%2520transformer-%2520and%250Aconvolutional%2520architectures%2520%2528VGG%252C%2520ResNet%252C%2520ViT%2529%2520compared%2520to%2520previous%2520works%252C%250Awhile%2520still%2520attaining%2520high%2520performance%2520on%2520ImageNet%2520classification%2520tasks.%2520Here%252C%250Aour%2520experiments%2520indicate%2520that%2520transformers%2520have%2520a%2520higher%2520degree%2520of%250Aover-parameterization%2520compared%2520to%2520convolutional%2520neural%2520networks.%2520Code%2520is%250Aavailable%2520at%2520https%253A//github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12568v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pruning%20By%20Explaining%20Revisited%3A%20Optimizing%20Attribution%20Methods%20to%20Prune%0A%20%20CNNs%20and%20Transformers&entry.906535625=Sayed%20Mohammad%20Vakilzadeh%20Hatefi%20and%20Maximilian%20Dreyer%20and%20Reduan%20Achtibat%20and%20Thomas%20Wiegand%20and%20Wojciech%20Samek%20and%20Sebastian%20Lapuschkin&entry.1292438233=%20%20To%20solve%20ever%20more%20complex%20problems%2C%20Deep%20Neural%20Networks%20are%20scaled%20to%0Abillions%20of%20parameters%2C%20leading%20to%20huge%20computational%20costs.%20An%20effective%0Aapproach%20to%20reduce%20computational%20requirements%20and%20increase%20efficiency%20is%20to%0Aprune%20unnecessary%20components%20of%20these%20often%20over-parameterized%20networks.%0APrevious%20work%20has%20shown%20that%20attribution%20methods%20from%20the%20field%20of%20eXplainable%0AAI%20serve%20as%20effective%20means%20to%20extract%20and%20prune%20the%20least%20relevant%20network%0Acomponents%20in%20a%20few-shot%20fashion.%20We%20extend%20the%20current%20state%20by%20proposing%20to%0Aexplicitly%20optimize%20hyperparameters%20of%20attribution%20methods%20for%20the%20task%20of%0Apruning%2C%20and%20further%20include%20transformer-based%20networks%20in%20our%20analysis.%20Our%0Aapproach%20yields%20higher%20model%20compression%20rates%20of%20large%20transformer-%20and%0Aconvolutional%20architectures%20%28VGG%2C%20ResNet%2C%20ViT%29%20compared%20to%20previous%20works%2C%0Awhile%20still%20attaining%20high%20performance%20on%20ImageNet%20classification%20tasks.%20Here%2C%0Aour%20experiments%20indicate%20that%20transformers%20have%20a%20higher%20degree%20of%0Aover-parameterization%20compared%20to%20convolutional%20neural%20networks.%20Code%20is%0Aavailable%20at%20https%3A//github.com/erfanhatefi/Pruning-by-eXplaining-in-PyTorch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12568v2&entry.124074799=Read"},
{"title": "Anomaly Resilient Temporal QoS Prediction using Hypergraph Convoluted\n  Transformer Network", "author": "Suraj Kumar and Soumi Chattopadhyay and Chandranath Adak", "abstract": "  Quality-of-Service (QoS) prediction is a critical task in the service\nlifecycle, enabling precise and adaptive service recommendations by\nanticipating performance variations over time in response to evolving network\nuncertainties and user preferences. However, contemporary QoS prediction\nmethods frequently encounter data sparsity and cold-start issues, which hinder\naccurate QoS predictions and limit the ability to capture diverse user\npreferences. Additionally, these methods often assume QoS data reliability,\nneglecting potential credibility issues such as outliers and the presence of\ngreysheep users and services with atypical invocation patterns. Furthermore,\ntraditional approaches fail to leverage diverse features, including\ndomain-specific knowledge and complex higher-order patterns, essential for\naccurate QoS predictions. In this paper, we introduce a real-time, trust-aware\nframework for temporal QoS prediction to address the aforementioned challenges,\nfeaturing an end-to-end deep architecture called the Hypergraph Convoluted\nTransformer Network (HCTN). HCTN combines a hypergraph structure with graph\nconvolution over hyper-edges to effectively address high-sparsity issues by\ncapturing complex, high-order correlations. Complementing this, the transformer\nnetwork utilizes multi-head attention along with parallel 1D convolutional\nlayers and fully connected dense blocks to capture both fine-grained and\ncoarse-grained dynamic patterns. Additionally, our approach includes a\nsparsity-resilient solution for detecting greysheep users and services,\nincorporating their unique characteristics to improve prediction accuracy.\nTrained with a robust loss function resistant to outliers, HCTN demonstrated\nstate-of-the-art performance on the large-scale WSDREAM-2 datasets for response\ntime and throughput.\n", "link": "http://arxiv.org/abs/2410.17762v1", "date": "2024-10-23", "relevancy": 2.0009, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5173}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4982}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anomaly%20Resilient%20Temporal%20QoS%20Prediction%20using%20Hypergraph%20Convoluted%0A%20%20Transformer%20Network&body=Title%3A%20Anomaly%20Resilient%20Temporal%20QoS%20Prediction%20using%20Hypergraph%20Convoluted%0A%20%20Transformer%20Network%0AAuthor%3A%20Suraj%20Kumar%20and%20Soumi%20Chattopadhyay%20and%20Chandranath%20Adak%0AAbstract%3A%20%20%20Quality-of-Service%20%28QoS%29%20prediction%20is%20a%20critical%20task%20in%20the%20service%0Alifecycle%2C%20enabling%20precise%20and%20adaptive%20service%20recommendations%20by%0Aanticipating%20performance%20variations%20over%20time%20in%20response%20to%20evolving%20network%0Auncertainties%20and%20user%20preferences.%20However%2C%20contemporary%20QoS%20prediction%0Amethods%20frequently%20encounter%20data%20sparsity%20and%20cold-start%20issues%2C%20which%20hinder%0Aaccurate%20QoS%20predictions%20and%20limit%20the%20ability%20to%20capture%20diverse%20user%0Apreferences.%20Additionally%2C%20these%20methods%20often%20assume%20QoS%20data%20reliability%2C%0Aneglecting%20potential%20credibility%20issues%20such%20as%20outliers%20and%20the%20presence%20of%0Agreysheep%20users%20and%20services%20with%20atypical%20invocation%20patterns.%20Furthermore%2C%0Atraditional%20approaches%20fail%20to%20leverage%20diverse%20features%2C%20including%0Adomain-specific%20knowledge%20and%20complex%20higher-order%20patterns%2C%20essential%20for%0Aaccurate%20QoS%20predictions.%20In%20this%20paper%2C%20we%20introduce%20a%20real-time%2C%20trust-aware%0Aframework%20for%20temporal%20QoS%20prediction%20to%20address%20the%20aforementioned%20challenges%2C%0Afeaturing%20an%20end-to-end%20deep%20architecture%20called%20the%20Hypergraph%20Convoluted%0ATransformer%20Network%20%28HCTN%29.%20HCTN%20combines%20a%20hypergraph%20structure%20with%20graph%0Aconvolution%20over%20hyper-edges%20to%20effectively%20address%20high-sparsity%20issues%20by%0Acapturing%20complex%2C%20high-order%20correlations.%20Complementing%20this%2C%20the%20transformer%0Anetwork%20utilizes%20multi-head%20attention%20along%20with%20parallel%201D%20convolutional%0Alayers%20and%20fully%20connected%20dense%20blocks%20to%20capture%20both%20fine-grained%20and%0Acoarse-grained%20dynamic%20patterns.%20Additionally%2C%20our%20approach%20includes%20a%0Asparsity-resilient%20solution%20for%20detecting%20greysheep%20users%20and%20services%2C%0Aincorporating%20their%20unique%20characteristics%20to%20improve%20prediction%20accuracy.%0ATrained%20with%20a%20robust%20loss%20function%20resistant%20to%20outliers%2C%20HCTN%20demonstrated%0Astate-of-the-art%20performance%20on%20the%20large-scale%20WSDREAM-2%20datasets%20for%20response%0Atime%20and%20throughput.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17762v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnomaly%2520Resilient%2520Temporal%2520QoS%2520Prediction%2520using%2520Hypergraph%2520Convoluted%250A%2520%2520Transformer%2520Network%26entry.906535625%3DSuraj%2520Kumar%2520and%2520Soumi%2520Chattopadhyay%2520and%2520Chandranath%2520Adak%26entry.1292438233%3D%2520%2520Quality-of-Service%2520%2528QoS%2529%2520prediction%2520is%2520a%2520critical%2520task%2520in%2520the%2520service%250Alifecycle%252C%2520enabling%2520precise%2520and%2520adaptive%2520service%2520recommendations%2520by%250Aanticipating%2520performance%2520variations%2520over%2520time%2520in%2520response%2520to%2520evolving%2520network%250Auncertainties%2520and%2520user%2520preferences.%2520However%252C%2520contemporary%2520QoS%2520prediction%250Amethods%2520frequently%2520encounter%2520data%2520sparsity%2520and%2520cold-start%2520issues%252C%2520which%2520hinder%250Aaccurate%2520QoS%2520predictions%2520and%2520limit%2520the%2520ability%2520to%2520capture%2520diverse%2520user%250Apreferences.%2520Additionally%252C%2520these%2520methods%2520often%2520assume%2520QoS%2520data%2520reliability%252C%250Aneglecting%2520potential%2520credibility%2520issues%2520such%2520as%2520outliers%2520and%2520the%2520presence%2520of%250Agreysheep%2520users%2520and%2520services%2520with%2520atypical%2520invocation%2520patterns.%2520Furthermore%252C%250Atraditional%2520approaches%2520fail%2520to%2520leverage%2520diverse%2520features%252C%2520including%250Adomain-specific%2520knowledge%2520and%2520complex%2520higher-order%2520patterns%252C%2520essential%2520for%250Aaccurate%2520QoS%2520predictions.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520real-time%252C%2520trust-aware%250Aframework%2520for%2520temporal%2520QoS%2520prediction%2520to%2520address%2520the%2520aforementioned%2520challenges%252C%250Afeaturing%2520an%2520end-to-end%2520deep%2520architecture%2520called%2520the%2520Hypergraph%2520Convoluted%250ATransformer%2520Network%2520%2528HCTN%2529.%2520HCTN%2520combines%2520a%2520hypergraph%2520structure%2520with%2520graph%250Aconvolution%2520over%2520hyper-edges%2520to%2520effectively%2520address%2520high-sparsity%2520issues%2520by%250Acapturing%2520complex%252C%2520high-order%2520correlations.%2520Complementing%2520this%252C%2520the%2520transformer%250Anetwork%2520utilizes%2520multi-head%2520attention%2520along%2520with%2520parallel%25201D%2520convolutional%250Alayers%2520and%2520fully%2520connected%2520dense%2520blocks%2520to%2520capture%2520both%2520fine-grained%2520and%250Acoarse-grained%2520dynamic%2520patterns.%2520Additionally%252C%2520our%2520approach%2520includes%2520a%250Asparsity-resilient%2520solution%2520for%2520detecting%2520greysheep%2520users%2520and%2520services%252C%250Aincorporating%2520their%2520unique%2520characteristics%2520to%2520improve%2520prediction%2520accuracy.%250ATrained%2520with%2520a%2520robust%2520loss%2520function%2520resistant%2520to%2520outliers%252C%2520HCTN%2520demonstrated%250Astate-of-the-art%2520performance%2520on%2520the%2520large-scale%2520WSDREAM-2%2520datasets%2520for%2520response%250Atime%2520and%2520throughput.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17762v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anomaly%20Resilient%20Temporal%20QoS%20Prediction%20using%20Hypergraph%20Convoluted%0A%20%20Transformer%20Network&entry.906535625=Suraj%20Kumar%20and%20Soumi%20Chattopadhyay%20and%20Chandranath%20Adak&entry.1292438233=%20%20Quality-of-Service%20%28QoS%29%20prediction%20is%20a%20critical%20task%20in%20the%20service%0Alifecycle%2C%20enabling%20precise%20and%20adaptive%20service%20recommendations%20by%0Aanticipating%20performance%20variations%20over%20time%20in%20response%20to%20evolving%20network%0Auncertainties%20and%20user%20preferences.%20However%2C%20contemporary%20QoS%20prediction%0Amethods%20frequently%20encounter%20data%20sparsity%20and%20cold-start%20issues%2C%20which%20hinder%0Aaccurate%20QoS%20predictions%20and%20limit%20the%20ability%20to%20capture%20diverse%20user%0Apreferences.%20Additionally%2C%20these%20methods%20often%20assume%20QoS%20data%20reliability%2C%0Aneglecting%20potential%20credibility%20issues%20such%20as%20outliers%20and%20the%20presence%20of%0Agreysheep%20users%20and%20services%20with%20atypical%20invocation%20patterns.%20Furthermore%2C%0Atraditional%20approaches%20fail%20to%20leverage%20diverse%20features%2C%20including%0Adomain-specific%20knowledge%20and%20complex%20higher-order%20patterns%2C%20essential%20for%0Aaccurate%20QoS%20predictions.%20In%20this%20paper%2C%20we%20introduce%20a%20real-time%2C%20trust-aware%0Aframework%20for%20temporal%20QoS%20prediction%20to%20address%20the%20aforementioned%20challenges%2C%0Afeaturing%20an%20end-to-end%20deep%20architecture%20called%20the%20Hypergraph%20Convoluted%0ATransformer%20Network%20%28HCTN%29.%20HCTN%20combines%20a%20hypergraph%20structure%20with%20graph%0Aconvolution%20over%20hyper-edges%20to%20effectively%20address%20high-sparsity%20issues%20by%0Acapturing%20complex%2C%20high-order%20correlations.%20Complementing%20this%2C%20the%20transformer%0Anetwork%20utilizes%20multi-head%20attention%20along%20with%20parallel%201D%20convolutional%0Alayers%20and%20fully%20connected%20dense%20blocks%20to%20capture%20both%20fine-grained%20and%0Acoarse-grained%20dynamic%20patterns.%20Additionally%2C%20our%20approach%20includes%20a%0Asparsity-resilient%20solution%20for%20detecting%20greysheep%20users%20and%20services%2C%0Aincorporating%20their%20unique%20characteristics%20to%20improve%20prediction%20accuracy.%0ATrained%20with%20a%20robust%20loss%20function%20resistant%20to%20outliers%2C%20HCTN%20demonstrated%0Astate-of-the-art%20performance%20on%20the%20large-scale%20WSDREAM-2%20datasets%20for%20response%0Atime%20and%20throughput.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17762v1&entry.124074799=Read"},
{"title": "Can Uncertainty Quantification Enable Better Learning-based Index\n  Tuning?", "author": "Tao Yu and Zhaonian Zou and Hao Xiong", "abstract": "  Index tuning is crucial for optimizing database performance by selecting\noptimal indexes based on workload. The key to this process lies in an accurate\nand efficient benefit estimator. Traditional methods relying on what-if tools\noften suffer from inefficiency and inaccuracy. In contrast, learning-based\nmodels provide a promising alternative but face challenges such as instability,\nlack of interpretability, and complex management. To overcome these\nlimitations, we adopt a novel approach: quantifying the uncertainty in\nlearning-based models' results, thereby combining the strengths of both\ntraditional and learning-based methods for reliable index tuning. We propose\nBeauty, the first uncertainty-aware framework that enhances learning-based\nmodels with uncertainty quantification and uses what-if tools as a\ncomplementary mechanism to improve reliability and reduce management\ncomplexity. Specifically, we introduce a novel method that combines AutoEncoder\nand Monte Carlo Dropout to jointly quantify uncertainty, tailored to the\ncharacteristics of benefit estimation tasks. In experiments involving sixteen\nmodels, our approach outperformed existing uncertainty quantification methods\nin the majority of cases. We also conducted index tuning tests on six datasets.\nBy applying the Beauty framework, we eliminated worst-case scenarios and more\nthan tripled the occurrence of best-case scenarios.\n", "link": "http://arxiv.org/abs/2410.17748v1", "date": "2024-10-23", "relevancy": 1.9978, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5448}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5042}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4766}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20Uncertainty%20Quantification%20Enable%20Better%20Learning-based%20Index%0A%20%20Tuning%3F&body=Title%3A%20Can%20Uncertainty%20Quantification%20Enable%20Better%20Learning-based%20Index%0A%20%20Tuning%3F%0AAuthor%3A%20Tao%20Yu%20and%20Zhaonian%20Zou%20and%20Hao%20Xiong%0AAbstract%3A%20%20%20Index%20tuning%20is%20crucial%20for%20optimizing%20database%20performance%20by%20selecting%0Aoptimal%20indexes%20based%20on%20workload.%20The%20key%20to%20this%20process%20lies%20in%20an%20accurate%0Aand%20efficient%20benefit%20estimator.%20Traditional%20methods%20relying%20on%20what-if%20tools%0Aoften%20suffer%20from%20inefficiency%20and%20inaccuracy.%20In%20contrast%2C%20learning-based%0Amodels%20provide%20a%20promising%20alternative%20but%20face%20challenges%20such%20as%20instability%2C%0Alack%20of%20interpretability%2C%20and%20complex%20management.%20To%20overcome%20these%0Alimitations%2C%20we%20adopt%20a%20novel%20approach%3A%20quantifying%20the%20uncertainty%20in%0Alearning-based%20models%27%20results%2C%20thereby%20combining%20the%20strengths%20of%20both%0Atraditional%20and%20learning-based%20methods%20for%20reliable%20index%20tuning.%20We%20propose%0ABeauty%2C%20the%20first%20uncertainty-aware%20framework%20that%20enhances%20learning-based%0Amodels%20with%20uncertainty%20quantification%20and%20uses%20what-if%20tools%20as%20a%0Acomplementary%20mechanism%20to%20improve%20reliability%20and%20reduce%20management%0Acomplexity.%20Specifically%2C%20we%20introduce%20a%20novel%20method%20that%20combines%20AutoEncoder%0Aand%20Monte%20Carlo%20Dropout%20to%20jointly%20quantify%20uncertainty%2C%20tailored%20to%20the%0Acharacteristics%20of%20benefit%20estimation%20tasks.%20In%20experiments%20involving%20sixteen%0Amodels%2C%20our%20approach%20outperformed%20existing%20uncertainty%20quantification%20methods%0Ain%20the%20majority%20of%20cases.%20We%20also%20conducted%20index%20tuning%20tests%20on%20six%20datasets.%0ABy%20applying%20the%20Beauty%20framework%2C%20we%20eliminated%20worst-case%20scenarios%20and%20more%0Athan%20tripled%20the%20occurrence%20of%20best-case%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17748v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520Uncertainty%2520Quantification%2520Enable%2520Better%2520Learning-based%2520Index%250A%2520%2520Tuning%253F%26entry.906535625%3DTao%2520Yu%2520and%2520Zhaonian%2520Zou%2520and%2520Hao%2520Xiong%26entry.1292438233%3D%2520%2520Index%2520tuning%2520is%2520crucial%2520for%2520optimizing%2520database%2520performance%2520by%2520selecting%250Aoptimal%2520indexes%2520based%2520on%2520workload.%2520The%2520key%2520to%2520this%2520process%2520lies%2520in%2520an%2520accurate%250Aand%2520efficient%2520benefit%2520estimator.%2520Traditional%2520methods%2520relying%2520on%2520what-if%2520tools%250Aoften%2520suffer%2520from%2520inefficiency%2520and%2520inaccuracy.%2520In%2520contrast%252C%2520learning-based%250Amodels%2520provide%2520a%2520promising%2520alternative%2520but%2520face%2520challenges%2520such%2520as%2520instability%252C%250Alack%2520of%2520interpretability%252C%2520and%2520complex%2520management.%2520To%2520overcome%2520these%250Alimitations%252C%2520we%2520adopt%2520a%2520novel%2520approach%253A%2520quantifying%2520the%2520uncertainty%2520in%250Alearning-based%2520models%2527%2520results%252C%2520thereby%2520combining%2520the%2520strengths%2520of%2520both%250Atraditional%2520and%2520learning-based%2520methods%2520for%2520reliable%2520index%2520tuning.%2520We%2520propose%250ABeauty%252C%2520the%2520first%2520uncertainty-aware%2520framework%2520that%2520enhances%2520learning-based%250Amodels%2520with%2520uncertainty%2520quantification%2520and%2520uses%2520what-if%2520tools%2520as%2520a%250Acomplementary%2520mechanism%2520to%2520improve%2520reliability%2520and%2520reduce%2520management%250Acomplexity.%2520Specifically%252C%2520we%2520introduce%2520a%2520novel%2520method%2520that%2520combines%2520AutoEncoder%250Aand%2520Monte%2520Carlo%2520Dropout%2520to%2520jointly%2520quantify%2520uncertainty%252C%2520tailored%2520to%2520the%250Acharacteristics%2520of%2520benefit%2520estimation%2520tasks.%2520In%2520experiments%2520involving%2520sixteen%250Amodels%252C%2520our%2520approach%2520outperformed%2520existing%2520uncertainty%2520quantification%2520methods%250Ain%2520the%2520majority%2520of%2520cases.%2520We%2520also%2520conducted%2520index%2520tuning%2520tests%2520on%2520six%2520datasets.%250ABy%2520applying%2520the%2520Beauty%2520framework%252C%2520we%2520eliminated%2520worst-case%2520scenarios%2520and%2520more%250Athan%2520tripled%2520the%2520occurrence%2520of%2520best-case%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17748v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20Uncertainty%20Quantification%20Enable%20Better%20Learning-based%20Index%0A%20%20Tuning%3F&entry.906535625=Tao%20Yu%20and%20Zhaonian%20Zou%20and%20Hao%20Xiong&entry.1292438233=%20%20Index%20tuning%20is%20crucial%20for%20optimizing%20database%20performance%20by%20selecting%0Aoptimal%20indexes%20based%20on%20workload.%20The%20key%20to%20this%20process%20lies%20in%20an%20accurate%0Aand%20efficient%20benefit%20estimator.%20Traditional%20methods%20relying%20on%20what-if%20tools%0Aoften%20suffer%20from%20inefficiency%20and%20inaccuracy.%20In%20contrast%2C%20learning-based%0Amodels%20provide%20a%20promising%20alternative%20but%20face%20challenges%20such%20as%20instability%2C%0Alack%20of%20interpretability%2C%20and%20complex%20management.%20To%20overcome%20these%0Alimitations%2C%20we%20adopt%20a%20novel%20approach%3A%20quantifying%20the%20uncertainty%20in%0Alearning-based%20models%27%20results%2C%20thereby%20combining%20the%20strengths%20of%20both%0Atraditional%20and%20learning-based%20methods%20for%20reliable%20index%20tuning.%20We%20propose%0ABeauty%2C%20the%20first%20uncertainty-aware%20framework%20that%20enhances%20learning-based%0Amodels%20with%20uncertainty%20quantification%20and%20uses%20what-if%20tools%20as%20a%0Acomplementary%20mechanism%20to%20improve%20reliability%20and%20reduce%20management%0Acomplexity.%20Specifically%2C%20we%20introduce%20a%20novel%20method%20that%20combines%20AutoEncoder%0Aand%20Monte%20Carlo%20Dropout%20to%20jointly%20quantify%20uncertainty%2C%20tailored%20to%20the%0Acharacteristics%20of%20benefit%20estimation%20tasks.%20In%20experiments%20involving%20sixteen%0Amodels%2C%20our%20approach%20outperformed%20existing%20uncertainty%20quantification%20methods%0Ain%20the%20majority%20of%20cases.%20We%20also%20conducted%20index%20tuning%20tests%20on%20six%20datasets.%0ABy%20applying%20the%20Beauty%20framework%2C%20we%20eliminated%20worst-case%20scenarios%20and%20more%0Athan%20tripled%20the%20occurrence%20of%20best-case%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17748v1&entry.124074799=Read"},
{"title": "DataTales: A Benchmark for Real-World Intelligent Data Narration", "author": "Yajing Yang and Qian Liu and Min-Yen Kan", "abstract": "  We introduce DataTales, a novel benchmark designed to assess the proficiency\nof language models in data narration, a task crucial for transforming complex\ntabular data into accessible narratives. Existing benchmarks often fall short\nin capturing the requisite analytical complexity for practical applications.\nDataTales addresses this gap by offering 4.9k financial reports paired with\ncorresponding market data, showcasing the demand for models to create clear\nnarratives and analyze large datasets while understanding specialized\nterminology in the field. Our findings highlights the significant challenge\nthat language models face in achieving the necessary precision and analytical\ndepth for proficient data narration, suggesting promising avenues for future\nmodel development and evaluation methodologies.\n", "link": "http://arxiv.org/abs/2410.17859v1", "date": "2024-10-23", "relevancy": 1.9926, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5065}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5065}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4564}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DataTales%3A%20A%20Benchmark%20for%20Real-World%20Intelligent%20Data%20Narration&body=Title%3A%20DataTales%3A%20A%20Benchmark%20for%20Real-World%20Intelligent%20Data%20Narration%0AAuthor%3A%20Yajing%20Yang%20and%20Qian%20Liu%20and%20Min-Yen%20Kan%0AAbstract%3A%20%20%20We%20introduce%20DataTales%2C%20a%20novel%20benchmark%20designed%20to%20assess%20the%20proficiency%0Aof%20language%20models%20in%20data%20narration%2C%20a%20task%20crucial%20for%20transforming%20complex%0Atabular%20data%20into%20accessible%20narratives.%20Existing%20benchmarks%20often%20fall%20short%0Ain%20capturing%20the%20requisite%20analytical%20complexity%20for%20practical%20applications.%0ADataTales%20addresses%20this%20gap%20by%20offering%204.9k%20financial%20reports%20paired%20with%0Acorresponding%20market%20data%2C%20showcasing%20the%20demand%20for%20models%20to%20create%20clear%0Anarratives%20and%20analyze%20large%20datasets%20while%20understanding%20specialized%0Aterminology%20in%20the%20field.%20Our%20findings%20highlights%20the%20significant%20challenge%0Athat%20language%20models%20face%20in%20achieving%20the%20necessary%20precision%20and%20analytical%0Adepth%20for%20proficient%20data%20narration%2C%20suggesting%20promising%20avenues%20for%20future%0Amodel%20development%20and%20evaluation%20methodologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17859v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDataTales%253A%2520A%2520Benchmark%2520for%2520Real-World%2520Intelligent%2520Data%2520Narration%26entry.906535625%3DYajing%2520Yang%2520and%2520Qian%2520Liu%2520and%2520Min-Yen%2520Kan%26entry.1292438233%3D%2520%2520We%2520introduce%2520DataTales%252C%2520a%2520novel%2520benchmark%2520designed%2520to%2520assess%2520the%2520proficiency%250Aof%2520language%2520models%2520in%2520data%2520narration%252C%2520a%2520task%2520crucial%2520for%2520transforming%2520complex%250Atabular%2520data%2520into%2520accessible%2520narratives.%2520Existing%2520benchmarks%2520often%2520fall%2520short%250Ain%2520capturing%2520the%2520requisite%2520analytical%2520complexity%2520for%2520practical%2520applications.%250ADataTales%2520addresses%2520this%2520gap%2520by%2520offering%25204.9k%2520financial%2520reports%2520paired%2520with%250Acorresponding%2520market%2520data%252C%2520showcasing%2520the%2520demand%2520for%2520models%2520to%2520create%2520clear%250Anarratives%2520and%2520analyze%2520large%2520datasets%2520while%2520understanding%2520specialized%250Aterminology%2520in%2520the%2520field.%2520Our%2520findings%2520highlights%2520the%2520significant%2520challenge%250Athat%2520language%2520models%2520face%2520in%2520achieving%2520the%2520necessary%2520precision%2520and%2520analytical%250Adepth%2520for%2520proficient%2520data%2520narration%252C%2520suggesting%2520promising%2520avenues%2520for%2520future%250Amodel%2520development%2520and%2520evaluation%2520methodologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17859v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DataTales%3A%20A%20Benchmark%20for%20Real-World%20Intelligent%20Data%20Narration&entry.906535625=Yajing%20Yang%20and%20Qian%20Liu%20and%20Min-Yen%20Kan&entry.1292438233=%20%20We%20introduce%20DataTales%2C%20a%20novel%20benchmark%20designed%20to%20assess%20the%20proficiency%0Aof%20language%20models%20in%20data%20narration%2C%20a%20task%20crucial%20for%20transforming%20complex%0Atabular%20data%20into%20accessible%20narratives.%20Existing%20benchmarks%20often%20fall%20short%0Ain%20capturing%20the%20requisite%20analytical%20complexity%20for%20practical%20applications.%0ADataTales%20addresses%20this%20gap%20by%20offering%204.9k%20financial%20reports%20paired%20with%0Acorresponding%20market%20data%2C%20showcasing%20the%20demand%20for%20models%20to%20create%20clear%0Anarratives%20and%20analyze%20large%20datasets%20while%20understanding%20specialized%0Aterminology%20in%20the%20field.%20Our%20findings%20highlights%20the%20significant%20challenge%0Athat%20language%20models%20face%20in%20achieving%20the%20necessary%20precision%20and%20analytical%0Adepth%20for%20proficient%20data%20narration%2C%20suggesting%20promising%20avenues%20for%20future%0Amodel%20development%20and%20evaluation%20methodologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17859v1&entry.124074799=Read"},
{"title": "STBA: Towards Evaluating the Robustness of DNNs for Query-Limited\n  Black-box Scenario", "author": "Renyang Liu and Kwok-Yan Lam and Wei Zhou and Sixing Wu and Jun Zhao and Dongting Hu and Mingming Gong", "abstract": "  Many attack techniques have been proposed to explore the vulnerability of\nDNNs and further help to improve their robustness. Despite the significant\nprogress made recently, existing black-box attack methods still suffer from\nunsatisfactory performance due to the vast number of queries needed to optimize\ndesired perturbations. Besides, the other critical challenge is that\nadversarial examples built in a noise-adding manner are abnormal and struggle\nto successfully attack robust models, whose robustness is enhanced by\nadversarial training against small perturbations. There is no doubt that these\ntwo issues mentioned above will significantly increase the risk of exposure and\nresult in a failure to dig deeply into the vulnerability of DNNs. Hence, it is\nnecessary to evaluate DNNs' fragility sufficiently under query-limited settings\nin a non-additional way. In this paper, we propose the Spatial Transform\nBlack-box Attack (STBA), a novel framework to craft formidable adversarial\nexamples in the query-limited scenario. Specifically, STBA introduces a flow\nfield to the high-frequency part of clean images to generate adversarial\nexamples and adopts the following two processes to enhance their naturalness\nand significantly improve the query efficiency: a) we apply an estimated flow\nfield to the high-frequency part of clean images to generate adversarial\nexamples instead of introducing external noise to the benign image, and b) we\nleverage an efficient gradient estimation method based on a batch of samples to\noptimize such an ideal flow field under query-limited settings. Compared to\nexisting score-based black-box baselines, extensive experiments indicated that\nSTBA could effectively improve the imperceptibility of the adversarial examples\nand remarkably boost the attack success rate under query-limited settings.\n", "link": "http://arxiv.org/abs/2404.00362v2", "date": "2024-10-23", "relevancy": 1.9915, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5072}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4962}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4958}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STBA%3A%20Towards%20Evaluating%20the%20Robustness%20of%20DNNs%20for%20Query-Limited%0A%20%20Black-box%20Scenario&body=Title%3A%20STBA%3A%20Towards%20Evaluating%20the%20Robustness%20of%20DNNs%20for%20Query-Limited%0A%20%20Black-box%20Scenario%0AAuthor%3A%20Renyang%20Liu%20and%20Kwok-Yan%20Lam%20and%20Wei%20Zhou%20and%20Sixing%20Wu%20and%20Jun%20Zhao%20and%20Dongting%20Hu%20and%20Mingming%20Gong%0AAbstract%3A%20%20%20Many%20attack%20techniques%20have%20been%20proposed%20to%20explore%20the%20vulnerability%20of%0ADNNs%20and%20further%20help%20to%20improve%20their%20robustness.%20Despite%20the%20significant%0Aprogress%20made%20recently%2C%20existing%20black-box%20attack%20methods%20still%20suffer%20from%0Aunsatisfactory%20performance%20due%20to%20the%20vast%20number%20of%20queries%20needed%20to%20optimize%0Adesired%20perturbations.%20Besides%2C%20the%20other%20critical%20challenge%20is%20that%0Aadversarial%20examples%20built%20in%20a%20noise-adding%20manner%20are%20abnormal%20and%20struggle%0Ato%20successfully%20attack%20robust%20models%2C%20whose%20robustness%20is%20enhanced%20by%0Aadversarial%20training%20against%20small%20perturbations.%20There%20is%20no%20doubt%20that%20these%0Atwo%20issues%20mentioned%20above%20will%20significantly%20increase%20the%20risk%20of%20exposure%20and%0Aresult%20in%20a%20failure%20to%20dig%20deeply%20into%20the%20vulnerability%20of%20DNNs.%20Hence%2C%20it%20is%0Anecessary%20to%20evaluate%20DNNs%27%20fragility%20sufficiently%20under%20query-limited%20settings%0Ain%20a%20non-additional%20way.%20In%20this%20paper%2C%20we%20propose%20the%20Spatial%20Transform%0ABlack-box%20Attack%20%28STBA%29%2C%20a%20novel%20framework%20to%20craft%20formidable%20adversarial%0Aexamples%20in%20the%20query-limited%20scenario.%20Specifically%2C%20STBA%20introduces%20a%20flow%0Afield%20to%20the%20high-frequency%20part%20of%20clean%20images%20to%20generate%20adversarial%0Aexamples%20and%20adopts%20the%20following%20two%20processes%20to%20enhance%20their%20naturalness%0Aand%20significantly%20improve%20the%20query%20efficiency%3A%20a%29%20we%20apply%20an%20estimated%20flow%0Afield%20to%20the%20high-frequency%20part%20of%20clean%20images%20to%20generate%20adversarial%0Aexamples%20instead%20of%20introducing%20external%20noise%20to%20the%20benign%20image%2C%20and%20b%29%20we%0Aleverage%20an%20efficient%20gradient%20estimation%20method%20based%20on%20a%20batch%20of%20samples%20to%0Aoptimize%20such%20an%20ideal%20flow%20field%20under%20query-limited%20settings.%20Compared%20to%0Aexisting%20score-based%20black-box%20baselines%2C%20extensive%20experiments%20indicated%20that%0ASTBA%20could%20effectively%20improve%20the%20imperceptibility%20of%20the%20adversarial%20examples%0Aand%20remarkably%20boost%20the%20attack%20success%20rate%20under%20query-limited%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00362v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTBA%253A%2520Towards%2520Evaluating%2520the%2520Robustness%2520of%2520DNNs%2520for%2520Query-Limited%250A%2520%2520Black-box%2520Scenario%26entry.906535625%3DRenyang%2520Liu%2520and%2520Kwok-Yan%2520Lam%2520and%2520Wei%2520Zhou%2520and%2520Sixing%2520Wu%2520and%2520Jun%2520Zhao%2520and%2520Dongting%2520Hu%2520and%2520Mingming%2520Gong%26entry.1292438233%3D%2520%2520Many%2520attack%2520techniques%2520have%2520been%2520proposed%2520to%2520explore%2520the%2520vulnerability%2520of%250ADNNs%2520and%2520further%2520help%2520to%2520improve%2520their%2520robustness.%2520Despite%2520the%2520significant%250Aprogress%2520made%2520recently%252C%2520existing%2520black-box%2520attack%2520methods%2520still%2520suffer%2520from%250Aunsatisfactory%2520performance%2520due%2520to%2520the%2520vast%2520number%2520of%2520queries%2520needed%2520to%2520optimize%250Adesired%2520perturbations.%2520Besides%252C%2520the%2520other%2520critical%2520challenge%2520is%2520that%250Aadversarial%2520examples%2520built%2520in%2520a%2520noise-adding%2520manner%2520are%2520abnormal%2520and%2520struggle%250Ato%2520successfully%2520attack%2520robust%2520models%252C%2520whose%2520robustness%2520is%2520enhanced%2520by%250Aadversarial%2520training%2520against%2520small%2520perturbations.%2520There%2520is%2520no%2520doubt%2520that%2520these%250Atwo%2520issues%2520mentioned%2520above%2520will%2520significantly%2520increase%2520the%2520risk%2520of%2520exposure%2520and%250Aresult%2520in%2520a%2520failure%2520to%2520dig%2520deeply%2520into%2520the%2520vulnerability%2520of%2520DNNs.%2520Hence%252C%2520it%2520is%250Anecessary%2520to%2520evaluate%2520DNNs%2527%2520fragility%2520sufficiently%2520under%2520query-limited%2520settings%250Ain%2520a%2520non-additional%2520way.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520Spatial%2520Transform%250ABlack-box%2520Attack%2520%2528STBA%2529%252C%2520a%2520novel%2520framework%2520to%2520craft%2520formidable%2520adversarial%250Aexamples%2520in%2520the%2520query-limited%2520scenario.%2520Specifically%252C%2520STBA%2520introduces%2520a%2520flow%250Afield%2520to%2520the%2520high-frequency%2520part%2520of%2520clean%2520images%2520to%2520generate%2520adversarial%250Aexamples%2520and%2520adopts%2520the%2520following%2520two%2520processes%2520to%2520enhance%2520their%2520naturalness%250Aand%2520significantly%2520improve%2520the%2520query%2520efficiency%253A%2520a%2529%2520we%2520apply%2520an%2520estimated%2520flow%250Afield%2520to%2520the%2520high-frequency%2520part%2520of%2520clean%2520images%2520to%2520generate%2520adversarial%250Aexamples%2520instead%2520of%2520introducing%2520external%2520noise%2520to%2520the%2520benign%2520image%252C%2520and%2520b%2529%2520we%250Aleverage%2520an%2520efficient%2520gradient%2520estimation%2520method%2520based%2520on%2520a%2520batch%2520of%2520samples%2520to%250Aoptimize%2520such%2520an%2520ideal%2520flow%2520field%2520under%2520query-limited%2520settings.%2520Compared%2520to%250Aexisting%2520score-based%2520black-box%2520baselines%252C%2520extensive%2520experiments%2520indicated%2520that%250ASTBA%2520could%2520effectively%2520improve%2520the%2520imperceptibility%2520of%2520the%2520adversarial%2520examples%250Aand%2520remarkably%2520boost%2520the%2520attack%2520success%2520rate%2520under%2520query-limited%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00362v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STBA%3A%20Towards%20Evaluating%20the%20Robustness%20of%20DNNs%20for%20Query-Limited%0A%20%20Black-box%20Scenario&entry.906535625=Renyang%20Liu%20and%20Kwok-Yan%20Lam%20and%20Wei%20Zhou%20and%20Sixing%20Wu%20and%20Jun%20Zhao%20and%20Dongting%20Hu%20and%20Mingming%20Gong&entry.1292438233=%20%20Many%20attack%20techniques%20have%20been%20proposed%20to%20explore%20the%20vulnerability%20of%0ADNNs%20and%20further%20help%20to%20improve%20their%20robustness.%20Despite%20the%20significant%0Aprogress%20made%20recently%2C%20existing%20black-box%20attack%20methods%20still%20suffer%20from%0Aunsatisfactory%20performance%20due%20to%20the%20vast%20number%20of%20queries%20needed%20to%20optimize%0Adesired%20perturbations.%20Besides%2C%20the%20other%20critical%20challenge%20is%20that%0Aadversarial%20examples%20built%20in%20a%20noise-adding%20manner%20are%20abnormal%20and%20struggle%0Ato%20successfully%20attack%20robust%20models%2C%20whose%20robustness%20is%20enhanced%20by%0Aadversarial%20training%20against%20small%20perturbations.%20There%20is%20no%20doubt%20that%20these%0Atwo%20issues%20mentioned%20above%20will%20significantly%20increase%20the%20risk%20of%20exposure%20and%0Aresult%20in%20a%20failure%20to%20dig%20deeply%20into%20the%20vulnerability%20of%20DNNs.%20Hence%2C%20it%20is%0Anecessary%20to%20evaluate%20DNNs%27%20fragility%20sufficiently%20under%20query-limited%20settings%0Ain%20a%20non-additional%20way.%20In%20this%20paper%2C%20we%20propose%20the%20Spatial%20Transform%0ABlack-box%20Attack%20%28STBA%29%2C%20a%20novel%20framework%20to%20craft%20formidable%20adversarial%0Aexamples%20in%20the%20query-limited%20scenario.%20Specifically%2C%20STBA%20introduces%20a%20flow%0Afield%20to%20the%20high-frequency%20part%20of%20clean%20images%20to%20generate%20adversarial%0Aexamples%20and%20adopts%20the%20following%20two%20processes%20to%20enhance%20their%20naturalness%0Aand%20significantly%20improve%20the%20query%20efficiency%3A%20a%29%20we%20apply%20an%20estimated%20flow%0Afield%20to%20the%20high-frequency%20part%20of%20clean%20images%20to%20generate%20adversarial%0Aexamples%20instead%20of%20introducing%20external%20noise%20to%20the%20benign%20image%2C%20and%20b%29%20we%0Aleverage%20an%20efficient%20gradient%20estimation%20method%20based%20on%20a%20batch%20of%20samples%20to%0Aoptimize%20such%20an%20ideal%20flow%20field%20under%20query-limited%20settings.%20Compared%20to%0Aexisting%20score-based%20black-box%20baselines%2C%20extensive%20experiments%20indicated%20that%0ASTBA%20could%20effectively%20improve%20the%20imperceptibility%20of%20the%20adversarial%20examples%0Aand%20remarkably%20boost%20the%20attack%20success%20rate%20under%20query-limited%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00362v2&entry.124074799=Read"},
{"title": "Automated Contrastive Learning Strategy Search for Time Series", "author": "Baoyu Jing and Yansen Wang and Guoxin Sui and Jing Hong and Jingrui He and Yuqing Yang and Dongsheng Li and Kan Ren", "abstract": "  In recent years, Contrastive Learning (CL) has become a predominant\nrepresentation learning paradigm for time series. Most existing methods\nmanually build specific CL Strategies (CLS) by human heuristics for certain\ndatasets and tasks. However, manually developing CLS usually requires excessive\nprior knowledge about the data, and massive experiments to determine the\ndetailed CL configurations. In this paper, we present an Automated Machine\nLearning (AutoML) practice at Microsoft, which automatically learns CLS for\ntime series datasets and tasks, namely Automated Contrastive Learning (AutoCL).\nWe first construct a principled search space of size over $3\\times10^{12}$,\ncovering data augmentation, embedding transformation, contrastive pair\nconstruction, and contrastive losses. Further, we introduce an efficient\nreinforcement learning algorithm, which optimizes CLS from the performance on\nthe validation tasks, to obtain effective CLS within the space. Experimental\nresults on various real-world datasets demonstrate that AutoCL could\nautomatically find the suitable CLS for the given dataset and task. From the\ncandidate CLS found by AutoCL on several public datasets/tasks, we compose a\ntransferable Generally Good Strategy (GGS), which has a strong performance for\nother datasets. We also provide empirical analysis as a guide for the future\ndesign of CLS.\n", "link": "http://arxiv.org/abs/2403.12641v3", "date": "2024-10-23", "relevancy": 1.9784, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4973}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.497}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.482}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automated%20Contrastive%20Learning%20Strategy%20Search%20for%20Time%20Series&body=Title%3A%20Automated%20Contrastive%20Learning%20Strategy%20Search%20for%20Time%20Series%0AAuthor%3A%20Baoyu%20Jing%20and%20Yansen%20Wang%20and%20Guoxin%20Sui%20and%20Jing%20Hong%20and%20Jingrui%20He%20and%20Yuqing%20Yang%20and%20Dongsheng%20Li%20and%20Kan%20Ren%0AAbstract%3A%20%20%20In%20recent%20years%2C%20Contrastive%20Learning%20%28CL%29%20has%20become%20a%20predominant%0Arepresentation%20learning%20paradigm%20for%20time%20series.%20Most%20existing%20methods%0Amanually%20build%20specific%20CL%20Strategies%20%28CLS%29%20by%20human%20heuristics%20for%20certain%0Adatasets%20and%20tasks.%20However%2C%20manually%20developing%20CLS%20usually%20requires%20excessive%0Aprior%20knowledge%20about%20the%20data%2C%20and%20massive%20experiments%20to%20determine%20the%0Adetailed%20CL%20configurations.%20In%20this%20paper%2C%20we%20present%20an%20Automated%20Machine%0ALearning%20%28AutoML%29%20practice%20at%20Microsoft%2C%20which%20automatically%20learns%20CLS%20for%0Atime%20series%20datasets%20and%20tasks%2C%20namely%20Automated%20Contrastive%20Learning%20%28AutoCL%29.%0AWe%20first%20construct%20a%20principled%20search%20space%20of%20size%20over%20%243%5Ctimes10%5E%7B12%7D%24%2C%0Acovering%20data%20augmentation%2C%20embedding%20transformation%2C%20contrastive%20pair%0Aconstruction%2C%20and%20contrastive%20losses.%20Further%2C%20we%20introduce%20an%20efficient%0Areinforcement%20learning%20algorithm%2C%20which%20optimizes%20CLS%20from%20the%20performance%20on%0Athe%20validation%20tasks%2C%20to%20obtain%20effective%20CLS%20within%20the%20space.%20Experimental%0Aresults%20on%20various%20real-world%20datasets%20demonstrate%20that%20AutoCL%20could%0Aautomatically%20find%20the%20suitable%20CLS%20for%20the%20given%20dataset%20and%20task.%20From%20the%0Acandidate%20CLS%20found%20by%20AutoCL%20on%20several%20public%20datasets/tasks%2C%20we%20compose%20a%0Atransferable%20Generally%20Good%20Strategy%20%28GGS%29%2C%20which%20has%20a%20strong%20performance%20for%0Aother%20datasets.%20We%20also%20provide%20empirical%20analysis%20as%20a%20guide%20for%20the%20future%0Adesign%20of%20CLS.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.12641v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomated%2520Contrastive%2520Learning%2520Strategy%2520Search%2520for%2520Time%2520Series%26entry.906535625%3DBaoyu%2520Jing%2520and%2520Yansen%2520Wang%2520and%2520Guoxin%2520Sui%2520and%2520Jing%2520Hong%2520and%2520Jingrui%2520He%2520and%2520Yuqing%2520Yang%2520and%2520Dongsheng%2520Li%2520and%2520Kan%2520Ren%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520Contrastive%2520Learning%2520%2528CL%2529%2520has%2520become%2520a%2520predominant%250Arepresentation%2520learning%2520paradigm%2520for%2520time%2520series.%2520Most%2520existing%2520methods%250Amanually%2520build%2520specific%2520CL%2520Strategies%2520%2528CLS%2529%2520by%2520human%2520heuristics%2520for%2520certain%250Adatasets%2520and%2520tasks.%2520However%252C%2520manually%2520developing%2520CLS%2520usually%2520requires%2520excessive%250Aprior%2520knowledge%2520about%2520the%2520data%252C%2520and%2520massive%2520experiments%2520to%2520determine%2520the%250Adetailed%2520CL%2520configurations.%2520In%2520this%2520paper%252C%2520we%2520present%2520an%2520Automated%2520Machine%250ALearning%2520%2528AutoML%2529%2520practice%2520at%2520Microsoft%252C%2520which%2520automatically%2520learns%2520CLS%2520for%250Atime%2520series%2520datasets%2520and%2520tasks%252C%2520namely%2520Automated%2520Contrastive%2520Learning%2520%2528AutoCL%2529.%250AWe%2520first%2520construct%2520a%2520principled%2520search%2520space%2520of%2520size%2520over%2520%25243%255Ctimes10%255E%257B12%257D%2524%252C%250Acovering%2520data%2520augmentation%252C%2520embedding%2520transformation%252C%2520contrastive%2520pair%250Aconstruction%252C%2520and%2520contrastive%2520losses.%2520Further%252C%2520we%2520introduce%2520an%2520efficient%250Areinforcement%2520learning%2520algorithm%252C%2520which%2520optimizes%2520CLS%2520from%2520the%2520performance%2520on%250Athe%2520validation%2520tasks%252C%2520to%2520obtain%2520effective%2520CLS%2520within%2520the%2520space.%2520Experimental%250Aresults%2520on%2520various%2520real-world%2520datasets%2520demonstrate%2520that%2520AutoCL%2520could%250Aautomatically%2520find%2520the%2520suitable%2520CLS%2520for%2520the%2520given%2520dataset%2520and%2520task.%2520From%2520the%250Acandidate%2520CLS%2520found%2520by%2520AutoCL%2520on%2520several%2520public%2520datasets/tasks%252C%2520we%2520compose%2520a%250Atransferable%2520Generally%2520Good%2520Strategy%2520%2528GGS%2529%252C%2520which%2520has%2520a%2520strong%2520performance%2520for%250Aother%2520datasets.%2520We%2520also%2520provide%2520empirical%2520analysis%2520as%2520a%2520guide%2520for%2520the%2520future%250Adesign%2520of%2520CLS.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.12641v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automated%20Contrastive%20Learning%20Strategy%20Search%20for%20Time%20Series&entry.906535625=Baoyu%20Jing%20and%20Yansen%20Wang%20and%20Guoxin%20Sui%20and%20Jing%20Hong%20and%20Jingrui%20He%20and%20Yuqing%20Yang%20and%20Dongsheng%20Li%20and%20Kan%20Ren&entry.1292438233=%20%20In%20recent%20years%2C%20Contrastive%20Learning%20%28CL%29%20has%20become%20a%20predominant%0Arepresentation%20learning%20paradigm%20for%20time%20series.%20Most%20existing%20methods%0Amanually%20build%20specific%20CL%20Strategies%20%28CLS%29%20by%20human%20heuristics%20for%20certain%0Adatasets%20and%20tasks.%20However%2C%20manually%20developing%20CLS%20usually%20requires%20excessive%0Aprior%20knowledge%20about%20the%20data%2C%20and%20massive%20experiments%20to%20determine%20the%0Adetailed%20CL%20configurations.%20In%20this%20paper%2C%20we%20present%20an%20Automated%20Machine%0ALearning%20%28AutoML%29%20practice%20at%20Microsoft%2C%20which%20automatically%20learns%20CLS%20for%0Atime%20series%20datasets%20and%20tasks%2C%20namely%20Automated%20Contrastive%20Learning%20%28AutoCL%29.%0AWe%20first%20construct%20a%20principled%20search%20space%20of%20size%20over%20%243%5Ctimes10%5E%7B12%7D%24%2C%0Acovering%20data%20augmentation%2C%20embedding%20transformation%2C%20contrastive%20pair%0Aconstruction%2C%20and%20contrastive%20losses.%20Further%2C%20we%20introduce%20an%20efficient%0Areinforcement%20learning%20algorithm%2C%20which%20optimizes%20CLS%20from%20the%20performance%20on%0Athe%20validation%20tasks%2C%20to%20obtain%20effective%20CLS%20within%20the%20space.%20Experimental%0Aresults%20on%20various%20real-world%20datasets%20demonstrate%20that%20AutoCL%20could%0Aautomatically%20find%20the%20suitable%20CLS%20for%20the%20given%20dataset%20and%20task.%20From%20the%0Acandidate%20CLS%20found%20by%20AutoCL%20on%20several%20public%20datasets/tasks%2C%20we%20compose%20a%0Atransferable%20Generally%20Good%20Strategy%20%28GGS%29%2C%20which%20has%20a%20strong%20performance%20for%0Aother%20datasets.%20We%20also%20provide%20empirical%20analysis%20as%20a%20guide%20for%20the%20future%0Adesign%20of%20CLS.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.12641v3&entry.124074799=Read"},
{"title": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use\n  (Including RAG), Planning, and Feedback Learning", "author": "Xinzhe Li", "abstract": "  Tool use, planning, and feedback learning are currently three prominent\nparadigms for developing Large Language Model (LLM)-based agents across various\ntasks. Although numerous frameworks have been devised for each paradigm, their\nintricate workflows and inconsistent taxonomy create challenges in\nunderstanding and reviewing the frameworks across different paradigms. This\nsurvey introduces a unified taxonomy to systematically review and discuss these\nframeworks. Specifically, 1) the taxonomy defines environments/tasks, common\nLLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models),\nand universally applicable workflows found in prior work, and 2) it enables a\ncomparison of key perspectives on the implementations of LMPRs and workflow\ndesigns across different agent paradigms and frameworks. 3) Finally, we\nidentify three limitations in existing workflow designs and systematically\ndiscuss the future work.\n", "link": "http://arxiv.org/abs/2406.05804v4", "date": "2024-10-23", "relevancy": 1.9767, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.501}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4928}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Review%20of%20Prominent%20Paradigms%20for%20LLM-Based%20Agents%3A%20Tool%20Use%0A%20%20%28Including%20RAG%29%2C%20Planning%2C%20and%20Feedback%20Learning&body=Title%3A%20A%20Review%20of%20Prominent%20Paradigms%20for%20LLM-Based%20Agents%3A%20Tool%20Use%0A%20%20%28Including%20RAG%29%2C%20Planning%2C%20and%20Feedback%20Learning%0AAuthor%3A%20Xinzhe%20Li%0AAbstract%3A%20%20%20Tool%20use%2C%20planning%2C%20and%20feedback%20learning%20are%20currently%20three%20prominent%0Aparadigms%20for%20developing%20Large%20Language%20Model%20%28LLM%29-based%20agents%20across%20various%0Atasks.%20Although%20numerous%20frameworks%20have%20been%20devised%20for%20each%20paradigm%2C%20their%0Aintricate%20workflows%20and%20inconsistent%20taxonomy%20create%20challenges%20in%0Aunderstanding%20and%20reviewing%20the%20frameworks%20across%20different%20paradigms.%20This%0Asurvey%20introduces%20a%20unified%20taxonomy%20to%20systematically%20review%20and%20discuss%20these%0Aframeworks.%20Specifically%2C%201%29%20the%20taxonomy%20defines%20environments/tasks%2C%20common%0ALLM-profiled%20roles%20or%20LMPRs%20%28policy%20models%2C%20evaluators%2C%20and%20dynamic%20models%29%2C%0Aand%20universally%20applicable%20workflows%20found%20in%20prior%20work%2C%20and%202%29%20it%20enables%20a%0Acomparison%20of%20key%20perspectives%20on%20the%20implementations%20of%20LMPRs%20and%20workflow%0Adesigns%20across%20different%20agent%20paradigms%20and%20frameworks.%203%29%20Finally%2C%20we%0Aidentify%20three%20limitations%20in%20existing%20workflow%20designs%20and%20systematically%0Adiscuss%20the%20future%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.05804v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Review%2520of%2520Prominent%2520Paradigms%2520for%2520LLM-Based%2520Agents%253A%2520Tool%2520Use%250A%2520%2520%2528Including%2520RAG%2529%252C%2520Planning%252C%2520and%2520Feedback%2520Learning%26entry.906535625%3DXinzhe%2520Li%26entry.1292438233%3D%2520%2520Tool%2520use%252C%2520planning%252C%2520and%2520feedback%2520learning%2520are%2520currently%2520three%2520prominent%250Aparadigms%2520for%2520developing%2520Large%2520Language%2520Model%2520%2528LLM%2529-based%2520agents%2520across%2520various%250Atasks.%2520Although%2520numerous%2520frameworks%2520have%2520been%2520devised%2520for%2520each%2520paradigm%252C%2520their%250Aintricate%2520workflows%2520and%2520inconsistent%2520taxonomy%2520create%2520challenges%2520in%250Aunderstanding%2520and%2520reviewing%2520the%2520frameworks%2520across%2520different%2520paradigms.%2520This%250Asurvey%2520introduces%2520a%2520unified%2520taxonomy%2520to%2520systematically%2520review%2520and%2520discuss%2520these%250Aframeworks.%2520Specifically%252C%25201%2529%2520the%2520taxonomy%2520defines%2520environments/tasks%252C%2520common%250ALLM-profiled%2520roles%2520or%2520LMPRs%2520%2528policy%2520models%252C%2520evaluators%252C%2520and%2520dynamic%2520models%2529%252C%250Aand%2520universally%2520applicable%2520workflows%2520found%2520in%2520prior%2520work%252C%2520and%25202%2529%2520it%2520enables%2520a%250Acomparison%2520of%2520key%2520perspectives%2520on%2520the%2520implementations%2520of%2520LMPRs%2520and%2520workflow%250Adesigns%2520across%2520different%2520agent%2520paradigms%2520and%2520frameworks.%25203%2529%2520Finally%252C%2520we%250Aidentify%2520three%2520limitations%2520in%2520existing%2520workflow%2520designs%2520and%2520systematically%250Adiscuss%2520the%2520future%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.05804v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Review%20of%20Prominent%20Paradigms%20for%20LLM-Based%20Agents%3A%20Tool%20Use%0A%20%20%28Including%20RAG%29%2C%20Planning%2C%20and%20Feedback%20Learning&entry.906535625=Xinzhe%20Li&entry.1292438233=%20%20Tool%20use%2C%20planning%2C%20and%20feedback%20learning%20are%20currently%20three%20prominent%0Aparadigms%20for%20developing%20Large%20Language%20Model%20%28LLM%29-based%20agents%20across%20various%0Atasks.%20Although%20numerous%20frameworks%20have%20been%20devised%20for%20each%20paradigm%2C%20their%0Aintricate%20workflows%20and%20inconsistent%20taxonomy%20create%20challenges%20in%0Aunderstanding%20and%20reviewing%20the%20frameworks%20across%20different%20paradigms.%20This%0Asurvey%20introduces%20a%20unified%20taxonomy%20to%20systematically%20review%20and%20discuss%20these%0Aframeworks.%20Specifically%2C%201%29%20the%20taxonomy%20defines%20environments/tasks%2C%20common%0ALLM-profiled%20roles%20or%20LMPRs%20%28policy%20models%2C%20evaluators%2C%20and%20dynamic%20models%29%2C%0Aand%20universally%20applicable%20workflows%20found%20in%20prior%20work%2C%20and%202%29%20it%20enables%20a%0Acomparison%20of%20key%20perspectives%20on%20the%20implementations%20of%20LMPRs%20and%20workflow%0Adesigns%20across%20different%20agent%20paradigms%20and%20frameworks.%203%29%20Finally%2C%20we%0Aidentify%20three%20limitations%20in%20existing%20workflow%20designs%20and%20systematically%0Adiscuss%20the%20future%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.05804v4&entry.124074799=Read"},
{"title": "SimRAG: Self-Improving Retrieval-Augmented Generation for Adapting Large\n  Language Models to Specialized Domains", "author": "Ran Xu and Hui Liu and Sreyashi Nag and Zhenwei Dai and Yaochen Xie and Xianfeng Tang and Chen Luo and Yang Li and Joyce C. Ho and Carl Yang and Qi He", "abstract": "  Retrieval-augmented generation (RAG) enhances the question-answering (QA)\nabilities of large language models (LLMs) by integrating external knowledge.\nHowever, adapting general-purpose RAG systems to specialized fields such as\nscience and medicine poses unique challenges due to distribution shifts and\nlimited access to domain-specific data. To tackle this, we propose SimRAG, a\nself-training approach that equips the LLM with joint capabilities of question\nanswering and question generation for domain adaptation. Our method first\nfine-tunes the LLM on instruction-following, question-answering, and\nsearch-related data. Then, it prompts the same LLM to generate diverse\ndomain-relevant questions from unlabeled corpora, with an additional filtering\nstrategy to retain high-quality synthetic examples. By leveraging these\nsynthetic examples, the LLM can improve their performance on domain-specific\nRAG tasks. Experiments on 11 datasets, spanning two backbone sizes and three\ndomains, demonstrate that SimRAG outperforms baselines by 1.2\\%--8.6\\%.\n", "link": "http://arxiv.org/abs/2410.17952v1", "date": "2024-10-23", "relevancy": 1.9705, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5134}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4928}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4842}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimRAG%3A%20Self-Improving%20Retrieval-Augmented%20Generation%20for%20Adapting%20Large%0A%20%20Language%20Models%20to%20Specialized%20Domains&body=Title%3A%20SimRAG%3A%20Self-Improving%20Retrieval-Augmented%20Generation%20for%20Adapting%20Large%0A%20%20Language%20Models%20to%20Specialized%20Domains%0AAuthor%3A%20Ran%20Xu%20and%20Hui%20Liu%20and%20Sreyashi%20Nag%20and%20Zhenwei%20Dai%20and%20Yaochen%20Xie%20and%20Xianfeng%20Tang%20and%20Chen%20Luo%20and%20Yang%20Li%20and%20Joyce%20C.%20Ho%20and%20Carl%20Yang%20and%20Qi%20He%0AAbstract%3A%20%20%20Retrieval-augmented%20generation%20%28RAG%29%20enhances%20the%20question-answering%20%28QA%29%0Aabilities%20of%20large%20language%20models%20%28LLMs%29%20by%20integrating%20external%20knowledge.%0AHowever%2C%20adapting%20general-purpose%20RAG%20systems%20to%20specialized%20fields%20such%20as%0Ascience%20and%20medicine%20poses%20unique%20challenges%20due%20to%20distribution%20shifts%20and%0Alimited%20access%20to%20domain-specific%20data.%20To%20tackle%20this%2C%20we%20propose%20SimRAG%2C%20a%0Aself-training%20approach%20that%20equips%20the%20LLM%20with%20joint%20capabilities%20of%20question%0Aanswering%20and%20question%20generation%20for%20domain%20adaptation.%20Our%20method%20first%0Afine-tunes%20the%20LLM%20on%20instruction-following%2C%20question-answering%2C%20and%0Asearch-related%20data.%20Then%2C%20it%20prompts%20the%20same%20LLM%20to%20generate%20diverse%0Adomain-relevant%20questions%20from%20unlabeled%20corpora%2C%20with%20an%20additional%20filtering%0Astrategy%20to%20retain%20high-quality%20synthetic%20examples.%20By%20leveraging%20these%0Asynthetic%20examples%2C%20the%20LLM%20can%20improve%20their%20performance%20on%20domain-specific%0ARAG%20tasks.%20Experiments%20on%2011%20datasets%2C%20spanning%20two%20backbone%20sizes%20and%20three%0Adomains%2C%20demonstrate%20that%20SimRAG%20outperforms%20baselines%20by%201.2%5C%25--8.6%5C%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17952v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimRAG%253A%2520Self-Improving%2520Retrieval-Augmented%2520Generation%2520for%2520Adapting%2520Large%250A%2520%2520Language%2520Models%2520to%2520Specialized%2520Domains%26entry.906535625%3DRan%2520Xu%2520and%2520Hui%2520Liu%2520and%2520Sreyashi%2520Nag%2520and%2520Zhenwei%2520Dai%2520and%2520Yaochen%2520Xie%2520and%2520Xianfeng%2520Tang%2520and%2520Chen%2520Luo%2520and%2520Yang%2520Li%2520and%2520Joyce%2520C.%2520Ho%2520and%2520Carl%2520Yang%2520and%2520Qi%2520He%26entry.1292438233%3D%2520%2520Retrieval-augmented%2520generation%2520%2528RAG%2529%2520enhances%2520the%2520question-answering%2520%2528QA%2529%250Aabilities%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520by%2520integrating%2520external%2520knowledge.%250AHowever%252C%2520adapting%2520general-purpose%2520RAG%2520systems%2520to%2520specialized%2520fields%2520such%2520as%250Ascience%2520and%2520medicine%2520poses%2520unique%2520challenges%2520due%2520to%2520distribution%2520shifts%2520and%250Alimited%2520access%2520to%2520domain-specific%2520data.%2520To%2520tackle%2520this%252C%2520we%2520propose%2520SimRAG%252C%2520a%250Aself-training%2520approach%2520that%2520equips%2520the%2520LLM%2520with%2520joint%2520capabilities%2520of%2520question%250Aanswering%2520and%2520question%2520generation%2520for%2520domain%2520adaptation.%2520Our%2520method%2520first%250Afine-tunes%2520the%2520LLM%2520on%2520instruction-following%252C%2520question-answering%252C%2520and%250Asearch-related%2520data.%2520Then%252C%2520it%2520prompts%2520the%2520same%2520LLM%2520to%2520generate%2520diverse%250Adomain-relevant%2520questions%2520from%2520unlabeled%2520corpora%252C%2520with%2520an%2520additional%2520filtering%250Astrategy%2520to%2520retain%2520high-quality%2520synthetic%2520examples.%2520By%2520leveraging%2520these%250Asynthetic%2520examples%252C%2520the%2520LLM%2520can%2520improve%2520their%2520performance%2520on%2520domain-specific%250ARAG%2520tasks.%2520Experiments%2520on%252011%2520datasets%252C%2520spanning%2520two%2520backbone%2520sizes%2520and%2520three%250Adomains%252C%2520demonstrate%2520that%2520SimRAG%2520outperforms%2520baselines%2520by%25201.2%255C%2525--8.6%255C%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17952v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimRAG%3A%20Self-Improving%20Retrieval-Augmented%20Generation%20for%20Adapting%20Large%0A%20%20Language%20Models%20to%20Specialized%20Domains&entry.906535625=Ran%20Xu%20and%20Hui%20Liu%20and%20Sreyashi%20Nag%20and%20Zhenwei%20Dai%20and%20Yaochen%20Xie%20and%20Xianfeng%20Tang%20and%20Chen%20Luo%20and%20Yang%20Li%20and%20Joyce%20C.%20Ho%20and%20Carl%20Yang%20and%20Qi%20He&entry.1292438233=%20%20Retrieval-augmented%20generation%20%28RAG%29%20enhances%20the%20question-answering%20%28QA%29%0Aabilities%20of%20large%20language%20models%20%28LLMs%29%20by%20integrating%20external%20knowledge.%0AHowever%2C%20adapting%20general-purpose%20RAG%20systems%20to%20specialized%20fields%20such%20as%0Ascience%20and%20medicine%20poses%20unique%20challenges%20due%20to%20distribution%20shifts%20and%0Alimited%20access%20to%20domain-specific%20data.%20To%20tackle%20this%2C%20we%20propose%20SimRAG%2C%20a%0Aself-training%20approach%20that%20equips%20the%20LLM%20with%20joint%20capabilities%20of%20question%0Aanswering%20and%20question%20generation%20for%20domain%20adaptation.%20Our%20method%20first%0Afine-tunes%20the%20LLM%20on%20instruction-following%2C%20question-answering%2C%20and%0Asearch-related%20data.%20Then%2C%20it%20prompts%20the%20same%20LLM%20to%20generate%20diverse%0Adomain-relevant%20questions%20from%20unlabeled%20corpora%2C%20with%20an%20additional%20filtering%0Astrategy%20to%20retain%20high-quality%20synthetic%20examples.%20By%20leveraging%20these%0Asynthetic%20examples%2C%20the%20LLM%20can%20improve%20their%20performance%20on%20domain-specific%0ARAG%20tasks.%20Experiments%20on%2011%20datasets%2C%20spanning%20two%20backbone%20sizes%20and%20three%0Adomains%2C%20demonstrate%20that%20SimRAG%20outperforms%20baselines%20by%201.2%5C%25--8.6%5C%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17952v1&entry.124074799=Read"},
{"title": "POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM\n  Inference", "author": "Aditya K Kamath and Ramya Prabhu and Jayashree Mohan and Simon Peter and Ramachandran Ramjee and Ashish Panwar", "abstract": "  Each request in LLM inference goes through two phases: compute-bound prefill\nand memory-bandwidth-bound decode. To improve GPU utilization, recent systems\nuse hybrid batching that combines the prefill and decode phases of different\nrequests into the same batch. Hybrid batching works well for linear operations\nas it amortizes the cost of loading model weights from HBM. However, attention\ncomputation in hybrid batches remains inefficient because existing attention\nkernels are optimized for either prefill or decode.\n  In this paper, we present POD-Attention -- the first GPU kernel that\nefficiently computes attention for hybrid batches. POD-Attention aims to\nmaximize the utilization of both compute and memory bandwidth by carefully\nallocating the GPU's resources such that prefill and decode operations happen\nconcurrently on the same multiprocessor. We integrate POD-Attention in a\nstate-of-the-art LLM inference scheduler Sarathi-Serve. POD-Attention speeds up\nattention computation by up to 75% (mean 28%) and increases LLM serving\nthroughput by up to 22% in offline inference. In online inference,\nPOD-Attention enables lower time-to-first-token (TTFT), time-between-tokens\n(TBT), and request execution latency versus Sarathi-Serve.\n", "link": "http://arxiv.org/abs/2410.18038v1", "date": "2024-10-23", "relevancy": 1.9632, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5386}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4855}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20POD-Attention%3A%20Unlocking%20Full%20Prefill-Decode%20Overlap%20for%20Faster%20LLM%0A%20%20Inference&body=Title%3A%20POD-Attention%3A%20Unlocking%20Full%20Prefill-Decode%20Overlap%20for%20Faster%20LLM%0A%20%20Inference%0AAuthor%3A%20Aditya%20K%20Kamath%20and%20Ramya%20Prabhu%20and%20Jayashree%20Mohan%20and%20Simon%20Peter%20and%20Ramachandran%20Ramjee%20and%20Ashish%20Panwar%0AAbstract%3A%20%20%20Each%20request%20in%20LLM%20inference%20goes%20through%20two%20phases%3A%20compute-bound%20prefill%0Aand%20memory-bandwidth-bound%20decode.%20To%20improve%20GPU%20utilization%2C%20recent%20systems%0Ause%20hybrid%20batching%20that%20combines%20the%20prefill%20and%20decode%20phases%20of%20different%0Arequests%20into%20the%20same%20batch.%20Hybrid%20batching%20works%20well%20for%20linear%20operations%0Aas%20it%20amortizes%20the%20cost%20of%20loading%20model%20weights%20from%20HBM.%20However%2C%20attention%0Acomputation%20in%20hybrid%20batches%20remains%20inefficient%20because%20existing%20attention%0Akernels%20are%20optimized%20for%20either%20prefill%20or%20decode.%0A%20%20In%20this%20paper%2C%20we%20present%20POD-Attention%20--%20the%20first%20GPU%20kernel%20that%0Aefficiently%20computes%20attention%20for%20hybrid%20batches.%20POD-Attention%20aims%20to%0Amaximize%20the%20utilization%20of%20both%20compute%20and%20memory%20bandwidth%20by%20carefully%0Aallocating%20the%20GPU%27s%20resources%20such%20that%20prefill%20and%20decode%20operations%20happen%0Aconcurrently%20on%20the%20same%20multiprocessor.%20We%20integrate%20POD-Attention%20in%20a%0Astate-of-the-art%20LLM%20inference%20scheduler%20Sarathi-Serve.%20POD-Attention%20speeds%20up%0Aattention%20computation%20by%20up%20to%2075%25%20%28mean%2028%25%29%20and%20increases%20LLM%20serving%0Athroughput%20by%20up%20to%2022%25%20in%20offline%20inference.%20In%20online%20inference%2C%0APOD-Attention%20enables%20lower%20time-to-first-token%20%28TTFT%29%2C%20time-between-tokens%0A%28TBT%29%2C%20and%20request%20execution%20latency%20versus%20Sarathi-Serve.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPOD-Attention%253A%2520Unlocking%2520Full%2520Prefill-Decode%2520Overlap%2520for%2520Faster%2520LLM%250A%2520%2520Inference%26entry.906535625%3DAditya%2520K%2520Kamath%2520and%2520Ramya%2520Prabhu%2520and%2520Jayashree%2520Mohan%2520and%2520Simon%2520Peter%2520and%2520Ramachandran%2520Ramjee%2520and%2520Ashish%2520Panwar%26entry.1292438233%3D%2520%2520Each%2520request%2520in%2520LLM%2520inference%2520goes%2520through%2520two%2520phases%253A%2520compute-bound%2520prefill%250Aand%2520memory-bandwidth-bound%2520decode.%2520To%2520improve%2520GPU%2520utilization%252C%2520recent%2520systems%250Ause%2520hybrid%2520batching%2520that%2520combines%2520the%2520prefill%2520and%2520decode%2520phases%2520of%2520different%250Arequests%2520into%2520the%2520same%2520batch.%2520Hybrid%2520batching%2520works%2520well%2520for%2520linear%2520operations%250Aas%2520it%2520amortizes%2520the%2520cost%2520of%2520loading%2520model%2520weights%2520from%2520HBM.%2520However%252C%2520attention%250Acomputation%2520in%2520hybrid%2520batches%2520remains%2520inefficient%2520because%2520existing%2520attention%250Akernels%2520are%2520optimized%2520for%2520either%2520prefill%2520or%2520decode.%250A%2520%2520In%2520this%2520paper%252C%2520we%2520present%2520POD-Attention%2520--%2520the%2520first%2520GPU%2520kernel%2520that%250Aefficiently%2520computes%2520attention%2520for%2520hybrid%2520batches.%2520POD-Attention%2520aims%2520to%250Amaximize%2520the%2520utilization%2520of%2520both%2520compute%2520and%2520memory%2520bandwidth%2520by%2520carefully%250Aallocating%2520the%2520GPU%2527s%2520resources%2520such%2520that%2520prefill%2520and%2520decode%2520operations%2520happen%250Aconcurrently%2520on%2520the%2520same%2520multiprocessor.%2520We%2520integrate%2520POD-Attention%2520in%2520a%250Astate-of-the-art%2520LLM%2520inference%2520scheduler%2520Sarathi-Serve.%2520POD-Attention%2520speeds%2520up%250Aattention%2520computation%2520by%2520up%2520to%252075%2525%2520%2528mean%252028%2525%2529%2520and%2520increases%2520LLM%2520serving%250Athroughput%2520by%2520up%2520to%252022%2525%2520in%2520offline%2520inference.%2520In%2520online%2520inference%252C%250APOD-Attention%2520enables%2520lower%2520time-to-first-token%2520%2528TTFT%2529%252C%2520time-between-tokens%250A%2528TBT%2529%252C%2520and%2520request%2520execution%2520latency%2520versus%2520Sarathi-Serve.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=POD-Attention%3A%20Unlocking%20Full%20Prefill-Decode%20Overlap%20for%20Faster%20LLM%0A%20%20Inference&entry.906535625=Aditya%20K%20Kamath%20and%20Ramya%20Prabhu%20and%20Jayashree%20Mohan%20and%20Simon%20Peter%20and%20Ramachandran%20Ramjee%20and%20Ashish%20Panwar&entry.1292438233=%20%20Each%20request%20in%20LLM%20inference%20goes%20through%20two%20phases%3A%20compute-bound%20prefill%0Aand%20memory-bandwidth-bound%20decode.%20To%20improve%20GPU%20utilization%2C%20recent%20systems%0Ause%20hybrid%20batching%20that%20combines%20the%20prefill%20and%20decode%20phases%20of%20different%0Arequests%20into%20the%20same%20batch.%20Hybrid%20batching%20works%20well%20for%20linear%20operations%0Aas%20it%20amortizes%20the%20cost%20of%20loading%20model%20weights%20from%20HBM.%20However%2C%20attention%0Acomputation%20in%20hybrid%20batches%20remains%20inefficient%20because%20existing%20attention%0Akernels%20are%20optimized%20for%20either%20prefill%20or%20decode.%0A%20%20In%20this%20paper%2C%20we%20present%20POD-Attention%20--%20the%20first%20GPU%20kernel%20that%0Aefficiently%20computes%20attention%20for%20hybrid%20batches.%20POD-Attention%20aims%20to%0Amaximize%20the%20utilization%20of%20both%20compute%20and%20memory%20bandwidth%20by%20carefully%0Aallocating%20the%20GPU%27s%20resources%20such%20that%20prefill%20and%20decode%20operations%20happen%0Aconcurrently%20on%20the%20same%20multiprocessor.%20We%20integrate%20POD-Attention%20in%20a%0Astate-of-the-art%20LLM%20inference%20scheduler%20Sarathi-Serve.%20POD-Attention%20speeds%20up%0Aattention%20computation%20by%20up%20to%2075%25%20%28mean%2028%25%29%20and%20increases%20LLM%20serving%0Athroughput%20by%20up%20to%2022%25%20in%20offline%20inference.%20In%20online%20inference%2C%0APOD-Attention%20enables%20lower%20time-to-first-token%20%28TTFT%29%2C%20time-between-tokens%0A%28TBT%29%2C%20and%20request%20execution%20latency%20versus%20Sarathi-Serve.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18038v1&entry.124074799=Read"},
{"title": "Efficient Sign-Based Optimization: Accelerating Convergence via Variance\n  Reduction", "author": "Wei Jiang and Sifan Yang and Wenhao Yang and Lijun Zhang", "abstract": "  Sign stochastic gradient descent (signSGD) is a communication-efficient\nmethod that transmits only the sign of stochastic gradients for parameter\nupdating. Existing literature has demonstrated that signSGD can achieve a\nconvergence rate of $\\mathcal{O}(d^{1/2}T^{-1/4})$, where $d$ represents the\ndimension and $T$ is the iteration number. In this paper, we improve this\nconvergence rate to $\\mathcal{O}(d^{1/2}T^{-1/3})$ by introducing the\nSign-based Stochastic Variance Reduction (SSVR) method, which employs variance\nreduction estimators to track gradients and leverages their signs to update.\nFor finite-sum problems, our method can be further enhanced to achieve a\nconvergence rate of $\\mathcal{O}(m^{1/4}d^{1/2}T^{-1/2})$, where $m$ denotes\nthe number of component functions. Furthermore, we investigate the\nheterogeneous majority vote in distributed settings and introduce two novel\nalgorithms that attain improved convergence rates of\n$\\mathcal{O}(d^{1/2}T^{-1/2} + dn^{-1/2})$ and $\\mathcal{O}(d^{1/4}T^{-1/4})$\nrespectively, outperforming the previous results of $\\mathcal{O}(dT^{-1/4} +\ndn^{-1/2})$ and $\\mathcal{O}(d^{3/8}T^{-1/8})$, where $n$ represents the number\nof nodes. Numerical experiments across different tasks validate the\neffectiveness of our proposed methods.\n", "link": "http://arxiv.org/abs/2406.00489v2", "date": "2024-10-23", "relevancy": 1.9613, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5143}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5017}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4617}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficient%20Sign-Based%20Optimization%3A%20Accelerating%20Convergence%20via%20Variance%0A%20%20Reduction&body=Title%3A%20Efficient%20Sign-Based%20Optimization%3A%20Accelerating%20Convergence%20via%20Variance%0A%20%20Reduction%0AAuthor%3A%20Wei%20Jiang%20and%20Sifan%20Yang%20and%20Wenhao%20Yang%20and%20Lijun%20Zhang%0AAbstract%3A%20%20%20Sign%20stochastic%20gradient%20descent%20%28signSGD%29%20is%20a%20communication-efficient%0Amethod%20that%20transmits%20only%20the%20sign%20of%20stochastic%20gradients%20for%20parameter%0Aupdating.%20Existing%20literature%20has%20demonstrated%20that%20signSGD%20can%20achieve%20a%0Aconvergence%20rate%20of%20%24%5Cmathcal%7BO%7D%28d%5E%7B1/2%7DT%5E%7B-1/4%7D%29%24%2C%20where%20%24d%24%20represents%20the%0Adimension%20and%20%24T%24%20is%20the%20iteration%20number.%20In%20this%20paper%2C%20we%20improve%20this%0Aconvergence%20rate%20to%20%24%5Cmathcal%7BO%7D%28d%5E%7B1/2%7DT%5E%7B-1/3%7D%29%24%20by%20introducing%20the%0ASign-based%20Stochastic%20Variance%20Reduction%20%28SSVR%29%20method%2C%20which%20employs%20variance%0Areduction%20estimators%20to%20track%20gradients%20and%20leverages%20their%20signs%20to%20update.%0AFor%20finite-sum%20problems%2C%20our%20method%20can%20be%20further%20enhanced%20to%20achieve%20a%0Aconvergence%20rate%20of%20%24%5Cmathcal%7BO%7D%28m%5E%7B1/4%7Dd%5E%7B1/2%7DT%5E%7B-1/2%7D%29%24%2C%20where%20%24m%24%20denotes%0Athe%20number%20of%20component%20functions.%20Furthermore%2C%20we%20investigate%20the%0Aheterogeneous%20majority%20vote%20in%20distributed%20settings%20and%20introduce%20two%20novel%0Aalgorithms%20that%20attain%20improved%20convergence%20rates%20of%0A%24%5Cmathcal%7BO%7D%28d%5E%7B1/2%7DT%5E%7B-1/2%7D%20%2B%20dn%5E%7B-1/2%7D%29%24%20and%20%24%5Cmathcal%7BO%7D%28d%5E%7B1/4%7DT%5E%7B-1/4%7D%29%24%0Arespectively%2C%20outperforming%20the%20previous%20results%20of%20%24%5Cmathcal%7BO%7D%28dT%5E%7B-1/4%7D%20%2B%0Adn%5E%7B-1/2%7D%29%24%20and%20%24%5Cmathcal%7BO%7D%28d%5E%7B3/8%7DT%5E%7B-1/8%7D%29%24%2C%20where%20%24n%24%20represents%20the%20number%0Aof%20nodes.%20Numerical%20experiments%20across%20different%20tasks%20validate%20the%0Aeffectiveness%20of%20our%20proposed%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.00489v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficient%2520Sign-Based%2520Optimization%253A%2520Accelerating%2520Convergence%2520via%2520Variance%250A%2520%2520Reduction%26entry.906535625%3DWei%2520Jiang%2520and%2520Sifan%2520Yang%2520and%2520Wenhao%2520Yang%2520and%2520Lijun%2520Zhang%26entry.1292438233%3D%2520%2520Sign%2520stochastic%2520gradient%2520descent%2520%2528signSGD%2529%2520is%2520a%2520communication-efficient%250Amethod%2520that%2520transmits%2520only%2520the%2520sign%2520of%2520stochastic%2520gradients%2520for%2520parameter%250Aupdating.%2520Existing%2520literature%2520has%2520demonstrated%2520that%2520signSGD%2520can%2520achieve%2520a%250Aconvergence%2520rate%2520of%2520%2524%255Cmathcal%257BO%257D%2528d%255E%257B1/2%257DT%255E%257B-1/4%257D%2529%2524%252C%2520where%2520%2524d%2524%2520represents%2520the%250Adimension%2520and%2520%2524T%2524%2520is%2520the%2520iteration%2520number.%2520In%2520this%2520paper%252C%2520we%2520improve%2520this%250Aconvergence%2520rate%2520to%2520%2524%255Cmathcal%257BO%257D%2528d%255E%257B1/2%257DT%255E%257B-1/3%257D%2529%2524%2520by%2520introducing%2520the%250ASign-based%2520Stochastic%2520Variance%2520Reduction%2520%2528SSVR%2529%2520method%252C%2520which%2520employs%2520variance%250Areduction%2520estimators%2520to%2520track%2520gradients%2520and%2520leverages%2520their%2520signs%2520to%2520update.%250AFor%2520finite-sum%2520problems%252C%2520our%2520method%2520can%2520be%2520further%2520enhanced%2520to%2520achieve%2520a%250Aconvergence%2520rate%2520of%2520%2524%255Cmathcal%257BO%257D%2528m%255E%257B1/4%257Dd%255E%257B1/2%257DT%255E%257B-1/2%257D%2529%2524%252C%2520where%2520%2524m%2524%2520denotes%250Athe%2520number%2520of%2520component%2520functions.%2520Furthermore%252C%2520we%2520investigate%2520the%250Aheterogeneous%2520majority%2520vote%2520in%2520distributed%2520settings%2520and%2520introduce%2520two%2520novel%250Aalgorithms%2520that%2520attain%2520improved%2520convergence%2520rates%2520of%250A%2524%255Cmathcal%257BO%257D%2528d%255E%257B1/2%257DT%255E%257B-1/2%257D%2520%252B%2520dn%255E%257B-1/2%257D%2529%2524%2520and%2520%2524%255Cmathcal%257BO%257D%2528d%255E%257B1/4%257DT%255E%257B-1/4%257D%2529%2524%250Arespectively%252C%2520outperforming%2520the%2520previous%2520results%2520of%2520%2524%255Cmathcal%257BO%257D%2528dT%255E%257B-1/4%257D%2520%252B%250Adn%255E%257B-1/2%257D%2529%2524%2520and%2520%2524%255Cmathcal%257BO%257D%2528d%255E%257B3/8%257DT%255E%257B-1/8%257D%2529%2524%252C%2520where%2520%2524n%2524%2520represents%2520the%2520number%250Aof%2520nodes.%2520Numerical%2520experiments%2520across%2520different%2520tasks%2520validate%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.00489v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficient%20Sign-Based%20Optimization%3A%20Accelerating%20Convergence%20via%20Variance%0A%20%20Reduction&entry.906535625=Wei%20Jiang%20and%20Sifan%20Yang%20and%20Wenhao%20Yang%20and%20Lijun%20Zhang&entry.1292438233=%20%20Sign%20stochastic%20gradient%20descent%20%28signSGD%29%20is%20a%20communication-efficient%0Amethod%20that%20transmits%20only%20the%20sign%20of%20stochastic%20gradients%20for%20parameter%0Aupdating.%20Existing%20literature%20has%20demonstrated%20that%20signSGD%20can%20achieve%20a%0Aconvergence%20rate%20of%20%24%5Cmathcal%7BO%7D%28d%5E%7B1/2%7DT%5E%7B-1/4%7D%29%24%2C%20where%20%24d%24%20represents%20the%0Adimension%20and%20%24T%24%20is%20the%20iteration%20number.%20In%20this%20paper%2C%20we%20improve%20this%0Aconvergence%20rate%20to%20%24%5Cmathcal%7BO%7D%28d%5E%7B1/2%7DT%5E%7B-1/3%7D%29%24%20by%20introducing%20the%0ASign-based%20Stochastic%20Variance%20Reduction%20%28SSVR%29%20method%2C%20which%20employs%20variance%0Areduction%20estimators%20to%20track%20gradients%20and%20leverages%20their%20signs%20to%20update.%0AFor%20finite-sum%20problems%2C%20our%20method%20can%20be%20further%20enhanced%20to%20achieve%20a%0Aconvergence%20rate%20of%20%24%5Cmathcal%7BO%7D%28m%5E%7B1/4%7Dd%5E%7B1/2%7DT%5E%7B-1/2%7D%29%24%2C%20where%20%24m%24%20denotes%0Athe%20number%20of%20component%20functions.%20Furthermore%2C%20we%20investigate%20the%0Aheterogeneous%20majority%20vote%20in%20distributed%20settings%20and%20introduce%20two%20novel%0Aalgorithms%20that%20attain%20improved%20convergence%20rates%20of%0A%24%5Cmathcal%7BO%7D%28d%5E%7B1/2%7DT%5E%7B-1/2%7D%20%2B%20dn%5E%7B-1/2%7D%29%24%20and%20%24%5Cmathcal%7BO%7D%28d%5E%7B1/4%7DT%5E%7B-1/4%7D%29%24%0Arespectively%2C%20outperforming%20the%20previous%20results%20of%20%24%5Cmathcal%7BO%7D%28dT%5E%7B-1/4%7D%20%2B%0Adn%5E%7B-1/2%7D%29%24%20and%20%24%5Cmathcal%7BO%7D%28d%5E%7B3/8%7DT%5E%7B-1/8%7D%29%24%2C%20where%20%24n%24%20represents%20the%20number%0Aof%20nodes.%20Numerical%20experiments%20across%20different%20tasks%20validate%20the%0Aeffectiveness%20of%20our%20proposed%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.00489v2&entry.124074799=Read"},
{"title": "Reducing Variance in Meta-Learning via Laplace Approximation for\n  Regression Tasks", "author": "Alfredo Reichlin and Gustaf Tegn\u00e9r and Miguel Vasco and Hang Yin and M\u00e5rten Bj\u00f6rkman and Danica Kragic", "abstract": "  Given a finite set of sample points, meta-learning algorithms aim to learn an\noptimal adaptation strategy for new, unseen tasks. Often, this data can be\nambiguous as it might belong to different tasks concurrently. This is\nparticularly the case in meta-regression tasks. In such cases, the estimated\nadaptation strategy is subject to high variance due to the limited amount of\nsupport data for each task, which often leads to sub-optimal generalization\nperformance. In this work, we address the problem of variance reduction in\ngradient-based meta-learning and formalize the class of problems prone to this,\na condition we refer to as \\emph{task overlap}. Specifically, we propose a\nnovel approach that reduces the variance of the gradient estimate by weighing\neach support point individually by the variance of its posterior over the\nparameters. To estimate the posterior, we utilize the Laplace approximation,\nwhich allows us to express the variance in terms of the curvature of the loss\nlandscape of our meta-learner. Experimental results demonstrate the\neffectiveness of the proposed method and highlight the importance of variance\nreduction in meta-learning.\n", "link": "http://arxiv.org/abs/2410.01476v2", "date": "2024-10-23", "relevancy": 1.9514, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5082}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4901}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4775}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Reducing%20Variance%20in%20Meta-Learning%20via%20Laplace%20Approximation%20for%0A%20%20Regression%20Tasks&body=Title%3A%20Reducing%20Variance%20in%20Meta-Learning%20via%20Laplace%20Approximation%20for%0A%20%20Regression%20Tasks%0AAuthor%3A%20Alfredo%20Reichlin%20and%20Gustaf%20Tegn%C3%A9r%20and%20Miguel%20Vasco%20and%20Hang%20Yin%20and%20M%C3%A5rten%20Bj%C3%B6rkman%20and%20Danica%20Kragic%0AAbstract%3A%20%20%20Given%20a%20finite%20set%20of%20sample%20points%2C%20meta-learning%20algorithms%20aim%20to%20learn%20an%0Aoptimal%20adaptation%20strategy%20for%20new%2C%20unseen%20tasks.%20Often%2C%20this%20data%20can%20be%0Aambiguous%20as%20it%20might%20belong%20to%20different%20tasks%20concurrently.%20This%20is%0Aparticularly%20the%20case%20in%20meta-regression%20tasks.%20In%20such%20cases%2C%20the%20estimated%0Aadaptation%20strategy%20is%20subject%20to%20high%20variance%20due%20to%20the%20limited%20amount%20of%0Asupport%20data%20for%20each%20task%2C%20which%20often%20leads%20to%20sub-optimal%20generalization%0Aperformance.%20In%20this%20work%2C%20we%20address%20the%20problem%20of%20variance%20reduction%20in%0Agradient-based%20meta-learning%20and%20formalize%20the%20class%20of%20problems%20prone%20to%20this%2C%0Aa%20condition%20we%20refer%20to%20as%20%5Cemph%7Btask%20overlap%7D.%20Specifically%2C%20we%20propose%20a%0Anovel%20approach%20that%20reduces%20the%20variance%20of%20the%20gradient%20estimate%20by%20weighing%0Aeach%20support%20point%20individually%20by%20the%20variance%20of%20its%20posterior%20over%20the%0Aparameters.%20To%20estimate%20the%20posterior%2C%20we%20utilize%20the%20Laplace%20approximation%2C%0Awhich%20allows%20us%20to%20express%20the%20variance%20in%20terms%20of%20the%20curvature%20of%20the%20loss%0Alandscape%20of%20our%20meta-learner.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20method%20and%20highlight%20the%20importance%20of%20variance%0Areduction%20in%20meta-learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.01476v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DReducing%2520Variance%2520in%2520Meta-Learning%2520via%2520Laplace%2520Approximation%2520for%250A%2520%2520Regression%2520Tasks%26entry.906535625%3DAlfredo%2520Reichlin%2520and%2520Gustaf%2520Tegn%25C3%25A9r%2520and%2520Miguel%2520Vasco%2520and%2520Hang%2520Yin%2520and%2520M%25C3%25A5rten%2520Bj%25C3%25B6rkman%2520and%2520Danica%2520Kragic%26entry.1292438233%3D%2520%2520Given%2520a%2520finite%2520set%2520of%2520sample%2520points%252C%2520meta-learning%2520algorithms%2520aim%2520to%2520learn%2520an%250Aoptimal%2520adaptation%2520strategy%2520for%2520new%252C%2520unseen%2520tasks.%2520Often%252C%2520this%2520data%2520can%2520be%250Aambiguous%2520as%2520it%2520might%2520belong%2520to%2520different%2520tasks%2520concurrently.%2520This%2520is%250Aparticularly%2520the%2520case%2520in%2520meta-regression%2520tasks.%2520In%2520such%2520cases%252C%2520the%2520estimated%250Aadaptation%2520strategy%2520is%2520subject%2520to%2520high%2520variance%2520due%2520to%2520the%2520limited%2520amount%2520of%250Asupport%2520data%2520for%2520each%2520task%252C%2520which%2520often%2520leads%2520to%2520sub-optimal%2520generalization%250Aperformance.%2520In%2520this%2520work%252C%2520we%2520address%2520the%2520problem%2520of%2520variance%2520reduction%2520in%250Agradient-based%2520meta-learning%2520and%2520formalize%2520the%2520class%2520of%2520problems%2520prone%2520to%2520this%252C%250Aa%2520condition%2520we%2520refer%2520to%2520as%2520%255Cemph%257Btask%2520overlap%257D.%2520Specifically%252C%2520we%2520propose%2520a%250Anovel%2520approach%2520that%2520reduces%2520the%2520variance%2520of%2520the%2520gradient%2520estimate%2520by%2520weighing%250Aeach%2520support%2520point%2520individually%2520by%2520the%2520variance%2520of%2520its%2520posterior%2520over%2520the%250Aparameters.%2520To%2520estimate%2520the%2520posterior%252C%2520we%2520utilize%2520the%2520Laplace%2520approximation%252C%250Awhich%2520allows%2520us%2520to%2520express%2520the%2520variance%2520in%2520terms%2520of%2520the%2520curvature%2520of%2520the%2520loss%250Alandscape%2520of%2520our%2520meta-learner.%2520Experimental%2520results%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520method%2520and%2520highlight%2520the%2520importance%2520of%2520variance%250Areduction%2520in%2520meta-learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.01476v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Reducing%20Variance%20in%20Meta-Learning%20via%20Laplace%20Approximation%20for%0A%20%20Regression%20Tasks&entry.906535625=Alfredo%20Reichlin%20and%20Gustaf%20Tegn%C3%A9r%20and%20Miguel%20Vasco%20and%20Hang%20Yin%20and%20M%C3%A5rten%20Bj%C3%B6rkman%20and%20Danica%20Kragic&entry.1292438233=%20%20Given%20a%20finite%20set%20of%20sample%20points%2C%20meta-learning%20algorithms%20aim%20to%20learn%20an%0Aoptimal%20adaptation%20strategy%20for%20new%2C%20unseen%20tasks.%20Often%2C%20this%20data%20can%20be%0Aambiguous%20as%20it%20might%20belong%20to%20different%20tasks%20concurrently.%20This%20is%0Aparticularly%20the%20case%20in%20meta-regression%20tasks.%20In%20such%20cases%2C%20the%20estimated%0Aadaptation%20strategy%20is%20subject%20to%20high%20variance%20due%20to%20the%20limited%20amount%20of%0Asupport%20data%20for%20each%20task%2C%20which%20often%20leads%20to%20sub-optimal%20generalization%0Aperformance.%20In%20this%20work%2C%20we%20address%20the%20problem%20of%20variance%20reduction%20in%0Agradient-based%20meta-learning%20and%20formalize%20the%20class%20of%20problems%20prone%20to%20this%2C%0Aa%20condition%20we%20refer%20to%20as%20%5Cemph%7Btask%20overlap%7D.%20Specifically%2C%20we%20propose%20a%0Anovel%20approach%20that%20reduces%20the%20variance%20of%20the%20gradient%20estimate%20by%20weighing%0Aeach%20support%20point%20individually%20by%20the%20variance%20of%20its%20posterior%20over%20the%0Aparameters.%20To%20estimate%20the%20posterior%2C%20we%20utilize%20the%20Laplace%20approximation%2C%0Awhich%20allows%20us%20to%20express%20the%20variance%20in%20terms%20of%20the%20curvature%20of%20the%20loss%0Alandscape%20of%20our%20meta-learner.%20Experimental%20results%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20method%20and%20highlight%20the%20importance%20of%20variance%0Areduction%20in%20meta-learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.01476v2&entry.124074799=Read"},
{"title": "Locating Information in Large Language Models via Random Matrix Theory", "author": "Max Staats and Matthias Thamm and Bernd Rosenow", "abstract": "  As large language models (LLMs) become central to AI applications, gaining a\ndeeper understanding of their inner workings is increasingly important. In this\nwork, we analyze the weight matrices of pretrained transformer models --\nspecifically BERT and Llama -- using random matrix theory (RMT) as a\nzero-information hypothesis. While randomly initialized weights perfectly agree\nwith RMT predictions, deviations emerge after training, allowing us to locate\nlearned structures within the models. We identify layer-type specific behaviors\nthat are consistent across all blocks and architectures considered. By\npinpointing regions that deviate from RMT predictions, we highlight areas of\nfeature learning and confirm this through comparisons with the activation\ncovariance matrices of the corresponding layers. Our method provides a\ndiagnostic tool for identifying relevant regions in transformer weights using\nonly the trained matrices. Additionally, we address the ongoing debate\nregarding the significance of small singular values in the context of\nfine-tuning and alignment in LLMs. Our findings reveal that, after fine-tuning,\nsmall singular values play a crucial role in the models' capabilities,\nsuggesting that removing them in an already aligned transformer can be\ndetrimental, as it may compromise model alignment.\n", "link": "http://arxiv.org/abs/2410.17770v1", "date": "2024-10-23", "relevancy": 1.9449, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5333}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4768}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4768}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Locating%20Information%20in%20Large%20Language%20Models%20via%20Random%20Matrix%20Theory&body=Title%3A%20Locating%20Information%20in%20Large%20Language%20Models%20via%20Random%20Matrix%20Theory%0AAuthor%3A%20Max%20Staats%20and%20Matthias%20Thamm%20and%20Bernd%20Rosenow%0AAbstract%3A%20%20%20As%20large%20language%20models%20%28LLMs%29%20become%20central%20to%20AI%20applications%2C%20gaining%20a%0Adeeper%20understanding%20of%20their%20inner%20workings%20is%20increasingly%20important.%20In%20this%0Awork%2C%20we%20analyze%20the%20weight%20matrices%20of%20pretrained%20transformer%20models%20--%0Aspecifically%20BERT%20and%20Llama%20--%20using%20random%20matrix%20theory%20%28RMT%29%20as%20a%0Azero-information%20hypothesis.%20While%20randomly%20initialized%20weights%20perfectly%20agree%0Awith%20RMT%20predictions%2C%20deviations%20emerge%20after%20training%2C%20allowing%20us%20to%20locate%0Alearned%20structures%20within%20the%20models.%20We%20identify%20layer-type%20specific%20behaviors%0Athat%20are%20consistent%20across%20all%20blocks%20and%20architectures%20considered.%20By%0Apinpointing%20regions%20that%20deviate%20from%20RMT%20predictions%2C%20we%20highlight%20areas%20of%0Afeature%20learning%20and%20confirm%20this%20through%20comparisons%20with%20the%20activation%0Acovariance%20matrices%20of%20the%20corresponding%20layers.%20Our%20method%20provides%20a%0Adiagnostic%20tool%20for%20identifying%20relevant%20regions%20in%20transformer%20weights%20using%0Aonly%20the%20trained%20matrices.%20Additionally%2C%20we%20address%20the%20ongoing%20debate%0Aregarding%20the%20significance%20of%20small%20singular%20values%20in%20the%20context%20of%0Afine-tuning%20and%20alignment%20in%20LLMs.%20Our%20findings%20reveal%20that%2C%20after%20fine-tuning%2C%0Asmall%20singular%20values%20play%20a%20crucial%20role%20in%20the%20models%27%20capabilities%2C%0Asuggesting%20that%20removing%20them%20in%20an%20already%20aligned%20transformer%20can%20be%0Adetrimental%2C%20as%20it%20may%20compromise%20model%20alignment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17770v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocating%2520Information%2520in%2520Large%2520Language%2520Models%2520via%2520Random%2520Matrix%2520Theory%26entry.906535625%3DMax%2520Staats%2520and%2520Matthias%2520Thamm%2520and%2520Bernd%2520Rosenow%26entry.1292438233%3D%2520%2520As%2520large%2520language%2520models%2520%2528LLMs%2529%2520become%2520central%2520to%2520AI%2520applications%252C%2520gaining%2520a%250Adeeper%2520understanding%2520of%2520their%2520inner%2520workings%2520is%2520increasingly%2520important.%2520In%2520this%250Awork%252C%2520we%2520analyze%2520the%2520weight%2520matrices%2520of%2520pretrained%2520transformer%2520models%2520--%250Aspecifically%2520BERT%2520and%2520Llama%2520--%2520using%2520random%2520matrix%2520theory%2520%2528RMT%2529%2520as%2520a%250Azero-information%2520hypothesis.%2520While%2520randomly%2520initialized%2520weights%2520perfectly%2520agree%250Awith%2520RMT%2520predictions%252C%2520deviations%2520emerge%2520after%2520training%252C%2520allowing%2520us%2520to%2520locate%250Alearned%2520structures%2520within%2520the%2520models.%2520We%2520identify%2520layer-type%2520specific%2520behaviors%250Athat%2520are%2520consistent%2520across%2520all%2520blocks%2520and%2520architectures%2520considered.%2520By%250Apinpointing%2520regions%2520that%2520deviate%2520from%2520RMT%2520predictions%252C%2520we%2520highlight%2520areas%2520of%250Afeature%2520learning%2520and%2520confirm%2520this%2520through%2520comparisons%2520with%2520the%2520activation%250Acovariance%2520matrices%2520of%2520the%2520corresponding%2520layers.%2520Our%2520method%2520provides%2520a%250Adiagnostic%2520tool%2520for%2520identifying%2520relevant%2520regions%2520in%2520transformer%2520weights%2520using%250Aonly%2520the%2520trained%2520matrices.%2520Additionally%252C%2520we%2520address%2520the%2520ongoing%2520debate%250Aregarding%2520the%2520significance%2520of%2520small%2520singular%2520values%2520in%2520the%2520context%2520of%250Afine-tuning%2520and%2520alignment%2520in%2520LLMs.%2520Our%2520findings%2520reveal%2520that%252C%2520after%2520fine-tuning%252C%250Asmall%2520singular%2520values%2520play%2520a%2520crucial%2520role%2520in%2520the%2520models%2527%2520capabilities%252C%250Asuggesting%2520that%2520removing%2520them%2520in%2520an%2520already%2520aligned%2520transformer%2520can%2520be%250Adetrimental%252C%2520as%2520it%2520may%2520compromise%2520model%2520alignment.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17770v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Locating%20Information%20in%20Large%20Language%20Models%20via%20Random%20Matrix%20Theory&entry.906535625=Max%20Staats%20and%20Matthias%20Thamm%20and%20Bernd%20Rosenow&entry.1292438233=%20%20As%20large%20language%20models%20%28LLMs%29%20become%20central%20to%20AI%20applications%2C%20gaining%20a%0Adeeper%20understanding%20of%20their%20inner%20workings%20is%20increasingly%20important.%20In%20this%0Awork%2C%20we%20analyze%20the%20weight%20matrices%20of%20pretrained%20transformer%20models%20--%0Aspecifically%20BERT%20and%20Llama%20--%20using%20random%20matrix%20theory%20%28RMT%29%20as%20a%0Azero-information%20hypothesis.%20While%20randomly%20initialized%20weights%20perfectly%20agree%0Awith%20RMT%20predictions%2C%20deviations%20emerge%20after%20training%2C%20allowing%20us%20to%20locate%0Alearned%20structures%20within%20the%20models.%20We%20identify%20layer-type%20specific%20behaviors%0Athat%20are%20consistent%20across%20all%20blocks%20and%20architectures%20considered.%20By%0Apinpointing%20regions%20that%20deviate%20from%20RMT%20predictions%2C%20we%20highlight%20areas%20of%0Afeature%20learning%20and%20confirm%20this%20through%20comparisons%20with%20the%20activation%0Acovariance%20matrices%20of%20the%20corresponding%20layers.%20Our%20method%20provides%20a%0Adiagnostic%20tool%20for%20identifying%20relevant%20regions%20in%20transformer%20weights%20using%0Aonly%20the%20trained%20matrices.%20Additionally%2C%20we%20address%20the%20ongoing%20debate%0Aregarding%20the%20significance%20of%20small%20singular%20values%20in%20the%20context%20of%0Afine-tuning%20and%20alignment%20in%20LLMs.%20Our%20findings%20reveal%20that%2C%20after%20fine-tuning%2C%0Asmall%20singular%20values%20play%20a%20crucial%20role%20in%20the%20models%27%20capabilities%2C%0Asuggesting%20that%20removing%20them%20in%20an%20already%20aligned%20transformer%20can%20be%0Adetrimental%2C%20as%20it%20may%20compromise%20model%20alignment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17770v1&entry.124074799=Read"},
{"title": "ProFL: Performative Robust Optimal Federated Learning", "author": "Xue Zheng and Tian Xie and Xuwei Tan and Aylin Yener and Xueru Zhang and Ali Payani and Myungjin Lee", "abstract": "  Performative prediction (PP) is a framework that captures distribution shifts\nthat occur during the training of machine learning models due to their\ndeployment. As the trained model is used, its generated data could cause the\nmodel to evolve, leading to deviations from the original data distribution. The\nimpact of such model-induced distribution shifts in the federated learning (FL)\nsetup remains unexplored despite being increasingly likely to transpire in\nreal-life use cases. Although Jin et al. (2024) recently extended PP to FL in a\nstraightforward manner, the resulting model only converges to a performative\nstable point, which may be far from optimal. The methods in Izzo et al. (2021);\nMiller et al. (2021) can find a performative optimal point in centralized\nsettings, but they require the performative risk to be convex and the training\ndata to be noiseless, assumptions often violated in realistic FL systems. This\npaper overcomes all of these shortcomings and proposes Performative robust\noptimal Federated Learning (ProFL), an algorithm that finds performative\noptimal points in FL from noisy and contaminated data. We present the\nconvergence analysis under the Polyak-Lojasiewicz condition, which applies to\nnon-convex objectives. Extensive experiments on multiple datasets validate our\nproposed algorithms' efficiency.\n", "link": "http://arxiv.org/abs/2410.18075v1", "date": "2024-10-23", "relevancy": 1.9412, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5097}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4792}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4634}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ProFL%3A%20Performative%20Robust%20Optimal%20Federated%20Learning&body=Title%3A%20ProFL%3A%20Performative%20Robust%20Optimal%20Federated%20Learning%0AAuthor%3A%20Xue%20Zheng%20and%20Tian%20Xie%20and%20Xuwei%20Tan%20and%20Aylin%20Yener%20and%20Xueru%20Zhang%20and%20Ali%20Payani%20and%20Myungjin%20Lee%0AAbstract%3A%20%20%20Performative%20prediction%20%28PP%29%20is%20a%20framework%20that%20captures%20distribution%20shifts%0Athat%20occur%20during%20the%20training%20of%20machine%20learning%20models%20due%20to%20their%0Adeployment.%20As%20the%20trained%20model%20is%20used%2C%20its%20generated%20data%20could%20cause%20the%0Amodel%20to%20evolve%2C%20leading%20to%20deviations%20from%20the%20original%20data%20distribution.%20The%0Aimpact%20of%20such%20model-induced%20distribution%20shifts%20in%20the%20federated%20learning%20%28FL%29%0Asetup%20remains%20unexplored%20despite%20being%20increasingly%20likely%20to%20transpire%20in%0Areal-life%20use%20cases.%20Although%20Jin%20et%20al.%20%282024%29%20recently%20extended%20PP%20to%20FL%20in%20a%0Astraightforward%20manner%2C%20the%20resulting%20model%20only%20converges%20to%20a%20performative%0Astable%20point%2C%20which%20may%20be%20far%20from%20optimal.%20The%20methods%20in%20Izzo%20et%20al.%20%282021%29%3B%0AMiller%20et%20al.%20%282021%29%20can%20find%20a%20performative%20optimal%20point%20in%20centralized%0Asettings%2C%20but%20they%20require%20the%20performative%20risk%20to%20be%20convex%20and%20the%20training%0Adata%20to%20be%20noiseless%2C%20assumptions%20often%20violated%20in%20realistic%20FL%20systems.%20This%0Apaper%20overcomes%20all%20of%20these%20shortcomings%20and%20proposes%20Performative%20robust%0Aoptimal%20Federated%20Learning%20%28ProFL%29%2C%20an%20algorithm%20that%20finds%20performative%0Aoptimal%20points%20in%20FL%20from%20noisy%20and%20contaminated%20data.%20We%20present%20the%0Aconvergence%20analysis%20under%20the%20Polyak-Lojasiewicz%20condition%2C%20which%20applies%20to%0Anon-convex%20objectives.%20Extensive%20experiments%20on%20multiple%20datasets%20validate%20our%0Aproposed%20algorithms%27%20efficiency.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18075v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProFL%253A%2520Performative%2520Robust%2520Optimal%2520Federated%2520Learning%26entry.906535625%3DXue%2520Zheng%2520and%2520Tian%2520Xie%2520and%2520Xuwei%2520Tan%2520and%2520Aylin%2520Yener%2520and%2520Xueru%2520Zhang%2520and%2520Ali%2520Payani%2520and%2520Myungjin%2520Lee%26entry.1292438233%3D%2520%2520Performative%2520prediction%2520%2528PP%2529%2520is%2520a%2520framework%2520that%2520captures%2520distribution%2520shifts%250Athat%2520occur%2520during%2520the%2520training%2520of%2520machine%2520learning%2520models%2520due%2520to%2520their%250Adeployment.%2520As%2520the%2520trained%2520model%2520is%2520used%252C%2520its%2520generated%2520data%2520could%2520cause%2520the%250Amodel%2520to%2520evolve%252C%2520leading%2520to%2520deviations%2520from%2520the%2520original%2520data%2520distribution.%2520The%250Aimpact%2520of%2520such%2520model-induced%2520distribution%2520shifts%2520in%2520the%2520federated%2520learning%2520%2528FL%2529%250Asetup%2520remains%2520unexplored%2520despite%2520being%2520increasingly%2520likely%2520to%2520transpire%2520in%250Areal-life%2520use%2520cases.%2520Although%2520Jin%2520et%2520al.%2520%25282024%2529%2520recently%2520extended%2520PP%2520to%2520FL%2520in%2520a%250Astraightforward%2520manner%252C%2520the%2520resulting%2520model%2520only%2520converges%2520to%2520a%2520performative%250Astable%2520point%252C%2520which%2520may%2520be%2520far%2520from%2520optimal.%2520The%2520methods%2520in%2520Izzo%2520et%2520al.%2520%25282021%2529%253B%250AMiller%2520et%2520al.%2520%25282021%2529%2520can%2520find%2520a%2520performative%2520optimal%2520point%2520in%2520centralized%250Asettings%252C%2520but%2520they%2520require%2520the%2520performative%2520risk%2520to%2520be%2520convex%2520and%2520the%2520training%250Adata%2520to%2520be%2520noiseless%252C%2520assumptions%2520often%2520violated%2520in%2520realistic%2520FL%2520systems.%2520This%250Apaper%2520overcomes%2520all%2520of%2520these%2520shortcomings%2520and%2520proposes%2520Performative%2520robust%250Aoptimal%2520Federated%2520Learning%2520%2528ProFL%2529%252C%2520an%2520algorithm%2520that%2520finds%2520performative%250Aoptimal%2520points%2520in%2520FL%2520from%2520noisy%2520and%2520contaminated%2520data.%2520We%2520present%2520the%250Aconvergence%2520analysis%2520under%2520the%2520Polyak-Lojasiewicz%2520condition%252C%2520which%2520applies%2520to%250Anon-convex%2520objectives.%2520Extensive%2520experiments%2520on%2520multiple%2520datasets%2520validate%2520our%250Aproposed%2520algorithms%2527%2520efficiency.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18075v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ProFL%3A%20Performative%20Robust%20Optimal%20Federated%20Learning&entry.906535625=Xue%20Zheng%20and%20Tian%20Xie%20and%20Xuwei%20Tan%20and%20Aylin%20Yener%20and%20Xueru%20Zhang%20and%20Ali%20Payani%20and%20Myungjin%20Lee&entry.1292438233=%20%20Performative%20prediction%20%28PP%29%20is%20a%20framework%20that%20captures%20distribution%20shifts%0Athat%20occur%20during%20the%20training%20of%20machine%20learning%20models%20due%20to%20their%0Adeployment.%20As%20the%20trained%20model%20is%20used%2C%20its%20generated%20data%20could%20cause%20the%0Amodel%20to%20evolve%2C%20leading%20to%20deviations%20from%20the%20original%20data%20distribution.%20The%0Aimpact%20of%20such%20model-induced%20distribution%20shifts%20in%20the%20federated%20learning%20%28FL%29%0Asetup%20remains%20unexplored%20despite%20being%20increasingly%20likely%20to%20transpire%20in%0Areal-life%20use%20cases.%20Although%20Jin%20et%20al.%20%282024%29%20recently%20extended%20PP%20to%20FL%20in%20a%0Astraightforward%20manner%2C%20the%20resulting%20model%20only%20converges%20to%20a%20performative%0Astable%20point%2C%20which%20may%20be%20far%20from%20optimal.%20The%20methods%20in%20Izzo%20et%20al.%20%282021%29%3B%0AMiller%20et%20al.%20%282021%29%20can%20find%20a%20performative%20optimal%20point%20in%20centralized%0Asettings%2C%20but%20they%20require%20the%20performative%20risk%20to%20be%20convex%20and%20the%20training%0Adata%20to%20be%20noiseless%2C%20assumptions%20often%20violated%20in%20realistic%20FL%20systems.%20This%0Apaper%20overcomes%20all%20of%20these%20shortcomings%20and%20proposes%20Performative%20robust%0Aoptimal%20Federated%20Learning%20%28ProFL%29%2C%20an%20algorithm%20that%20finds%20performative%0Aoptimal%20points%20in%20FL%20from%20noisy%20and%20contaminated%20data.%20We%20present%20the%0Aconvergence%20analysis%20under%20the%20Polyak-Lojasiewicz%20condition%2C%20which%20applies%20to%0Anon-convex%20objectives.%20Extensive%20experiments%20on%20multiple%20datasets%20validate%20our%0Aproposed%20algorithms%27%20efficiency.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18075v1&entry.124074799=Read"},
{"title": "Stick-breaking Attention", "author": "Shawn Tan and Yikang Shen and Songlin Yang and Aaron Courville and Rameswar Panda", "abstract": "  The self-attention mechanism traditionally relies on the softmax operator,\nnecessitating positional embeddings like RoPE, or position biases to account\nfor token order. But current methods using still face length generalisation\nchallenges. We propose an alternative attention mechanism based on the\nstick-breaking process: For each token before the current, we determine a break\npoint $\\beta_{i,j}$, which represents the proportion of the remaining stick to\nallocate to the current token. We repeat the process until the stick is fully\nallocated, resulting in a sequence of attention weights. This process naturally\nincorporates recency bias, which has linguistic motivations for grammar parsing\n(Shen et. al., 2017). We study the implications of replacing the conventional\nsoftmax-based attention mechanism with stick-breaking attention. We then\ndiscuss implementation of numerically stable stick-breaking attention and adapt\nFlash Attention to accommodate this mechanism. When used as a drop-in\nreplacement for current softmax+RoPE attention systems, we find that\nstick-breaking attention performs competitively with current methods on length\ngeneralisation and downstream tasks. Stick-breaking also performs well at\nlength generalisation, allowing a model trained with $2^{11}$ context window to\nperform well at $2^{14}$ with perplexity improvements.\n", "link": "http://arxiv.org/abs/2410.17980v1", "date": "2024-10-23", "relevancy": 1.9386, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5086}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4815}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4328}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stick-breaking%20Attention&body=Title%3A%20Stick-breaking%20Attention%0AAuthor%3A%20Shawn%20Tan%20and%20Yikang%20Shen%20and%20Songlin%20Yang%20and%20Aaron%20Courville%20and%20Rameswar%20Panda%0AAbstract%3A%20%20%20The%20self-attention%20mechanism%20traditionally%20relies%20on%20the%20softmax%20operator%2C%0Anecessitating%20positional%20embeddings%20like%20RoPE%2C%20or%20position%20biases%20to%20account%0Afor%20token%20order.%20But%20current%20methods%20using%20still%20face%20length%20generalisation%0Achallenges.%20We%20propose%20an%20alternative%20attention%20mechanism%20based%20on%20the%0Astick-breaking%20process%3A%20For%20each%20token%20before%20the%20current%2C%20we%20determine%20a%20break%0Apoint%20%24%5Cbeta_%7Bi%2Cj%7D%24%2C%20which%20represents%20the%20proportion%20of%20the%20remaining%20stick%20to%0Aallocate%20to%20the%20current%20token.%20We%20repeat%20the%20process%20until%20the%20stick%20is%20fully%0Aallocated%2C%20resulting%20in%20a%20sequence%20of%20attention%20weights.%20This%20process%20naturally%0Aincorporates%20recency%20bias%2C%20which%20has%20linguistic%20motivations%20for%20grammar%20parsing%0A%28Shen%20et.%20al.%2C%202017%29.%20We%20study%20the%20implications%20of%20replacing%20the%20conventional%0Asoftmax-based%20attention%20mechanism%20with%20stick-breaking%20attention.%20We%20then%0Adiscuss%20implementation%20of%20numerically%20stable%20stick-breaking%20attention%20and%20adapt%0AFlash%20Attention%20to%20accommodate%20this%20mechanism.%20When%20used%20as%20a%20drop-in%0Areplacement%20for%20current%20softmax%2BRoPE%20attention%20systems%2C%20we%20find%20that%0Astick-breaking%20attention%20performs%20competitively%20with%20current%20methods%20on%20length%0Ageneralisation%20and%20downstream%20tasks.%20Stick-breaking%20also%20performs%20well%20at%0Alength%20generalisation%2C%20allowing%20a%20model%20trained%20with%20%242%5E%7B11%7D%24%20context%20window%20to%0Aperform%20well%20at%20%242%5E%7B14%7D%24%20with%20perplexity%20improvements.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17980v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStick-breaking%2520Attention%26entry.906535625%3DShawn%2520Tan%2520and%2520Yikang%2520Shen%2520and%2520Songlin%2520Yang%2520and%2520Aaron%2520Courville%2520and%2520Rameswar%2520Panda%26entry.1292438233%3D%2520%2520The%2520self-attention%2520mechanism%2520traditionally%2520relies%2520on%2520the%2520softmax%2520operator%252C%250Anecessitating%2520positional%2520embeddings%2520like%2520RoPE%252C%2520or%2520position%2520biases%2520to%2520account%250Afor%2520token%2520order.%2520But%2520current%2520methods%2520using%2520still%2520face%2520length%2520generalisation%250Achallenges.%2520We%2520propose%2520an%2520alternative%2520attention%2520mechanism%2520based%2520on%2520the%250Astick-breaking%2520process%253A%2520For%2520each%2520token%2520before%2520the%2520current%252C%2520we%2520determine%2520a%2520break%250Apoint%2520%2524%255Cbeta_%257Bi%252Cj%257D%2524%252C%2520which%2520represents%2520the%2520proportion%2520of%2520the%2520remaining%2520stick%2520to%250Aallocate%2520to%2520the%2520current%2520token.%2520We%2520repeat%2520the%2520process%2520until%2520the%2520stick%2520is%2520fully%250Aallocated%252C%2520resulting%2520in%2520a%2520sequence%2520of%2520attention%2520weights.%2520This%2520process%2520naturally%250Aincorporates%2520recency%2520bias%252C%2520which%2520has%2520linguistic%2520motivations%2520for%2520grammar%2520parsing%250A%2528Shen%2520et.%2520al.%252C%25202017%2529.%2520We%2520study%2520the%2520implications%2520of%2520replacing%2520the%2520conventional%250Asoftmax-based%2520attention%2520mechanism%2520with%2520stick-breaking%2520attention.%2520We%2520then%250Adiscuss%2520implementation%2520of%2520numerically%2520stable%2520stick-breaking%2520attention%2520and%2520adapt%250AFlash%2520Attention%2520to%2520accommodate%2520this%2520mechanism.%2520When%2520used%2520as%2520a%2520drop-in%250Areplacement%2520for%2520current%2520softmax%252BRoPE%2520attention%2520systems%252C%2520we%2520find%2520that%250Astick-breaking%2520attention%2520performs%2520competitively%2520with%2520current%2520methods%2520on%2520length%250Ageneralisation%2520and%2520downstream%2520tasks.%2520Stick-breaking%2520also%2520performs%2520well%2520at%250Alength%2520generalisation%252C%2520allowing%2520a%2520model%2520trained%2520with%2520%25242%255E%257B11%257D%2524%2520context%2520window%2520to%250Aperform%2520well%2520at%2520%25242%255E%257B14%257D%2524%2520with%2520perplexity%2520improvements.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17980v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stick-breaking%20Attention&entry.906535625=Shawn%20Tan%20and%20Yikang%20Shen%20and%20Songlin%20Yang%20and%20Aaron%20Courville%20and%20Rameswar%20Panda&entry.1292438233=%20%20The%20self-attention%20mechanism%20traditionally%20relies%20on%20the%20softmax%20operator%2C%0Anecessitating%20positional%20embeddings%20like%20RoPE%2C%20or%20position%20biases%20to%20account%0Afor%20token%20order.%20But%20current%20methods%20using%20still%20face%20length%20generalisation%0Achallenges.%20We%20propose%20an%20alternative%20attention%20mechanism%20based%20on%20the%0Astick-breaking%20process%3A%20For%20each%20token%20before%20the%20current%2C%20we%20determine%20a%20break%0Apoint%20%24%5Cbeta_%7Bi%2Cj%7D%24%2C%20which%20represents%20the%20proportion%20of%20the%20remaining%20stick%20to%0Aallocate%20to%20the%20current%20token.%20We%20repeat%20the%20process%20until%20the%20stick%20is%20fully%0Aallocated%2C%20resulting%20in%20a%20sequence%20of%20attention%20weights.%20This%20process%20naturally%0Aincorporates%20recency%20bias%2C%20which%20has%20linguistic%20motivations%20for%20grammar%20parsing%0A%28Shen%20et.%20al.%2C%202017%29.%20We%20study%20the%20implications%20of%20replacing%20the%20conventional%0Asoftmax-based%20attention%20mechanism%20with%20stick-breaking%20attention.%20We%20then%0Adiscuss%20implementation%20of%20numerically%20stable%20stick-breaking%20attention%20and%20adapt%0AFlash%20Attention%20to%20accommodate%20this%20mechanism.%20When%20used%20as%20a%20drop-in%0Areplacement%20for%20current%20softmax%2BRoPE%20attention%20systems%2C%20we%20find%20that%0Astick-breaking%20attention%20performs%20competitively%20with%20current%20methods%20on%20length%0Ageneralisation%20and%20downstream%20tasks.%20Stick-breaking%20also%20performs%20well%20at%0Alength%20generalisation%2C%20allowing%20a%20model%20trained%20with%20%242%5E%7B11%7D%24%20context%20window%20to%0Aperform%20well%20at%20%242%5E%7B14%7D%24%20with%20perplexity%20improvements.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17980v1&entry.124074799=Read"},
{"title": "Conditional Language Policy: A General Framework for Steerable\n  Multi-Objective Finetuning", "author": "Kaiwen Wang and Rahul Kidambi and Ryan Sullivan and Alekh Agarwal and Christoph Dann and Andrea Michi and Marco Gelmi and Yunxuan Li and Raghav Gupta and Avinava Dubey and Alexandre Ram\u00e9 and Johan Ferret and Geoffrey Cideron and Le Hou and Hongkun Yu and Amr Ahmed and Aranyak Mehta and L\u00e9onard Hussenot and Olivier Bachem and Edouard Leurent", "abstract": "  Reward-based finetuning is crucial for aligning language policies with\nintended behaviors (e.g., creativity and safety). A key challenge is to develop\nsteerable language models that trade-off multiple (conflicting) objectives in a\nflexible and efficient manner. This paper presents Conditional Language Policy\n(CLP), a general framework for finetuning language models on multiple\nobjectives. Building on techniques from multi-task training and\nparameter-efficient finetuning, CLP learn steerable models that effectively\ntrade-off conflicting objectives at inference time. Notably, this does not\nrequire training or maintaining multiple models to achieve different trade-offs\nbetween the objectives. Through extensive experiments and ablations on two\nsummarization datasets, we show that CLP learns steerable language models that\noutperform and Pareto-dominate the existing approaches for multi-objective\nfinetuning.\n", "link": "http://arxiv.org/abs/2407.15762v2", "date": "2024-10-23", "relevancy": 1.9358, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4995}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4809}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conditional%20Language%20Policy%3A%20A%20General%20Framework%20for%20Steerable%0A%20%20Multi-Objective%20Finetuning&body=Title%3A%20Conditional%20Language%20Policy%3A%20A%20General%20Framework%20for%20Steerable%0A%20%20Multi-Objective%20Finetuning%0AAuthor%3A%20Kaiwen%20Wang%20and%20Rahul%20Kidambi%20and%20Ryan%20Sullivan%20and%20Alekh%20Agarwal%20and%20Christoph%20Dann%20and%20Andrea%20Michi%20and%20Marco%20Gelmi%20and%20Yunxuan%20Li%20and%20Raghav%20Gupta%20and%20Avinava%20Dubey%20and%20Alexandre%20Ram%C3%A9%20and%20Johan%20Ferret%20and%20Geoffrey%20Cideron%20and%20Le%20Hou%20and%20Hongkun%20Yu%20and%20Amr%20Ahmed%20and%20Aranyak%20Mehta%20and%20L%C3%A9onard%20Hussenot%20and%20Olivier%20Bachem%20and%20Edouard%20Leurent%0AAbstract%3A%20%20%20Reward-based%20finetuning%20is%20crucial%20for%20aligning%20language%20policies%20with%0Aintended%20behaviors%20%28e.g.%2C%20creativity%20and%20safety%29.%20A%20key%20challenge%20is%20to%20develop%0Asteerable%20language%20models%20that%20trade-off%20multiple%20%28conflicting%29%20objectives%20in%20a%0Aflexible%20and%20efficient%20manner.%20This%20paper%20presents%20Conditional%20Language%20Policy%0A%28CLP%29%2C%20a%20general%20framework%20for%20finetuning%20language%20models%20on%20multiple%0Aobjectives.%20Building%20on%20techniques%20from%20multi-task%20training%20and%0Aparameter-efficient%20finetuning%2C%20CLP%20learn%20steerable%20models%20that%20effectively%0Atrade-off%20conflicting%20objectives%20at%20inference%20time.%20Notably%2C%20this%20does%20not%0Arequire%20training%20or%20maintaining%20multiple%20models%20to%20achieve%20different%20trade-offs%0Abetween%20the%20objectives.%20Through%20extensive%20experiments%20and%20ablations%20on%20two%0Asummarization%20datasets%2C%20we%20show%20that%20CLP%20learns%20steerable%20language%20models%20that%0Aoutperform%20and%20Pareto-dominate%20the%20existing%20approaches%20for%20multi-objective%0Afinetuning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConditional%2520Language%2520Policy%253A%2520A%2520General%2520Framework%2520for%2520Steerable%250A%2520%2520Multi-Objective%2520Finetuning%26entry.906535625%3DKaiwen%2520Wang%2520and%2520Rahul%2520Kidambi%2520and%2520Ryan%2520Sullivan%2520and%2520Alekh%2520Agarwal%2520and%2520Christoph%2520Dann%2520and%2520Andrea%2520Michi%2520and%2520Marco%2520Gelmi%2520and%2520Yunxuan%2520Li%2520and%2520Raghav%2520Gupta%2520and%2520Avinava%2520Dubey%2520and%2520Alexandre%2520Ram%25C3%25A9%2520and%2520Johan%2520Ferret%2520and%2520Geoffrey%2520Cideron%2520and%2520Le%2520Hou%2520and%2520Hongkun%2520Yu%2520and%2520Amr%2520Ahmed%2520and%2520Aranyak%2520Mehta%2520and%2520L%25C3%25A9onard%2520Hussenot%2520and%2520Olivier%2520Bachem%2520and%2520Edouard%2520Leurent%26entry.1292438233%3D%2520%2520Reward-based%2520finetuning%2520is%2520crucial%2520for%2520aligning%2520language%2520policies%2520with%250Aintended%2520behaviors%2520%2528e.g.%252C%2520creativity%2520and%2520safety%2529.%2520A%2520key%2520challenge%2520is%2520to%2520develop%250Asteerable%2520language%2520models%2520that%2520trade-off%2520multiple%2520%2528conflicting%2529%2520objectives%2520in%2520a%250Aflexible%2520and%2520efficient%2520manner.%2520This%2520paper%2520presents%2520Conditional%2520Language%2520Policy%250A%2528CLP%2529%252C%2520a%2520general%2520framework%2520for%2520finetuning%2520language%2520models%2520on%2520multiple%250Aobjectives.%2520Building%2520on%2520techniques%2520from%2520multi-task%2520training%2520and%250Aparameter-efficient%2520finetuning%252C%2520CLP%2520learn%2520steerable%2520models%2520that%2520effectively%250Atrade-off%2520conflicting%2520objectives%2520at%2520inference%2520time.%2520Notably%252C%2520this%2520does%2520not%250Arequire%2520training%2520or%2520maintaining%2520multiple%2520models%2520to%2520achieve%2520different%2520trade-offs%250Abetween%2520the%2520objectives.%2520Through%2520extensive%2520experiments%2520and%2520ablations%2520on%2520two%250Asummarization%2520datasets%252C%2520we%2520show%2520that%2520CLP%2520learns%2520steerable%2520language%2520models%2520that%250Aoutperform%2520and%2520Pareto-dominate%2520the%2520existing%2520approaches%2520for%2520multi-objective%250Afinetuning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conditional%20Language%20Policy%3A%20A%20General%20Framework%20for%20Steerable%0A%20%20Multi-Objective%20Finetuning&entry.906535625=Kaiwen%20Wang%20and%20Rahul%20Kidambi%20and%20Ryan%20Sullivan%20and%20Alekh%20Agarwal%20and%20Christoph%20Dann%20and%20Andrea%20Michi%20and%20Marco%20Gelmi%20and%20Yunxuan%20Li%20and%20Raghav%20Gupta%20and%20Avinava%20Dubey%20and%20Alexandre%20Ram%C3%A9%20and%20Johan%20Ferret%20and%20Geoffrey%20Cideron%20and%20Le%20Hou%20and%20Hongkun%20Yu%20and%20Amr%20Ahmed%20and%20Aranyak%20Mehta%20and%20L%C3%A9onard%20Hussenot%20and%20Olivier%20Bachem%20and%20Edouard%20Leurent&entry.1292438233=%20%20Reward-based%20finetuning%20is%20crucial%20for%20aligning%20language%20policies%20with%0Aintended%20behaviors%20%28e.g.%2C%20creativity%20and%20safety%29.%20A%20key%20challenge%20is%20to%20develop%0Asteerable%20language%20models%20that%20trade-off%20multiple%20%28conflicting%29%20objectives%20in%20a%0Aflexible%20and%20efficient%20manner.%20This%20paper%20presents%20Conditional%20Language%20Policy%0A%28CLP%29%2C%20a%20general%20framework%20for%20finetuning%20language%20models%20on%20multiple%0Aobjectives.%20Building%20on%20techniques%20from%20multi-task%20training%20and%0Aparameter-efficient%20finetuning%2C%20CLP%20learn%20steerable%20models%20that%20effectively%0Atrade-off%20conflicting%20objectives%20at%20inference%20time.%20Notably%2C%20this%20does%20not%0Arequire%20training%20or%20maintaining%20multiple%20models%20to%20achieve%20different%20trade-offs%0Abetween%20the%20objectives.%20Through%20extensive%20experiments%20and%20ablations%20on%20two%0Asummarization%20datasets%2C%20we%20show%20that%20CLP%20learns%20steerable%20language%20models%20that%0Aoutperform%20and%20Pareto-dominate%20the%20existing%20approaches%20for%20multi-objective%0Afinetuning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15762v2&entry.124074799=Read"},
{"title": "Inferring stability properties of chaotic systems on autoencoders'\n  latent spaces", "author": "Elise \u00d6zalp and Luca Magri", "abstract": "  The data-driven learning of solutions of partial differential equations can\nbe based on a divide-and-conquer strategy. First, the high dimensional data is\ncompressed to a latent space with an autoencoder; and, second, the temporal\ndynamics are inferred on the latent space with a form of recurrent neural\nnetwork. In chaotic systems and turbulence, convolutional autoencoders and echo\nstate networks (CAE-ESN) successfully forecast the dynamics, but little is\nknown about whether the stability properties can also be inferred. We show that\nthe CAE-ESN model infers the invariant stability properties and the geometry of\nthe tangent space in the low-dimensional manifold (i.e. the latent space)\nthrough Lyapunov exponents and covariant Lyapunov vectors. This work opens up\nnew opportunities for inferring the stability of high-dimensional chaotic\nsystems in latent spaces.\n", "link": "http://arxiv.org/abs/2410.18003v1", "date": "2024-10-23", "relevancy": 1.9319, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4966}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4746}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inferring%20stability%20properties%20of%20chaotic%20systems%20on%20autoencoders%27%0A%20%20latent%20spaces&body=Title%3A%20Inferring%20stability%20properties%20of%20chaotic%20systems%20on%20autoencoders%27%0A%20%20latent%20spaces%0AAuthor%3A%20Elise%20%C3%96zalp%20and%20Luca%20Magri%0AAbstract%3A%20%20%20The%20data-driven%20learning%20of%20solutions%20of%20partial%20differential%20equations%20can%0Abe%20based%20on%20a%20divide-and-conquer%20strategy.%20First%2C%20the%20high%20dimensional%20data%20is%0Acompressed%20to%20a%20latent%20space%20with%20an%20autoencoder%3B%20and%2C%20second%2C%20the%20temporal%0Adynamics%20are%20inferred%20on%20the%20latent%20space%20with%20a%20form%20of%20recurrent%20neural%0Anetwork.%20In%20chaotic%20systems%20and%20turbulence%2C%20convolutional%20autoencoders%20and%20echo%0Astate%20networks%20%28CAE-ESN%29%20successfully%20forecast%20the%20dynamics%2C%20but%20little%20is%0Aknown%20about%20whether%20the%20stability%20properties%20can%20also%20be%20inferred.%20We%20show%20that%0Athe%20CAE-ESN%20model%20infers%20the%20invariant%20stability%20properties%20and%20the%20geometry%20of%0Athe%20tangent%20space%20in%20the%20low-dimensional%20manifold%20%28i.e.%20the%20latent%20space%29%0Athrough%20Lyapunov%20exponents%20and%20covariant%20Lyapunov%20vectors.%20This%20work%20opens%20up%0Anew%20opportunities%20for%20inferring%20the%20stability%20of%20high-dimensional%20chaotic%0Asystems%20in%20latent%20spaces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.18003v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInferring%2520stability%2520properties%2520of%2520chaotic%2520systems%2520on%2520autoencoders%2527%250A%2520%2520latent%2520spaces%26entry.906535625%3DElise%2520%25C3%2596zalp%2520and%2520Luca%2520Magri%26entry.1292438233%3D%2520%2520The%2520data-driven%2520learning%2520of%2520solutions%2520of%2520partial%2520differential%2520equations%2520can%250Abe%2520based%2520on%2520a%2520divide-and-conquer%2520strategy.%2520First%252C%2520the%2520high%2520dimensional%2520data%2520is%250Acompressed%2520to%2520a%2520latent%2520space%2520with%2520an%2520autoencoder%253B%2520and%252C%2520second%252C%2520the%2520temporal%250Adynamics%2520are%2520inferred%2520on%2520the%2520latent%2520space%2520with%2520a%2520form%2520of%2520recurrent%2520neural%250Anetwork.%2520In%2520chaotic%2520systems%2520and%2520turbulence%252C%2520convolutional%2520autoencoders%2520and%2520echo%250Astate%2520networks%2520%2528CAE-ESN%2529%2520successfully%2520forecast%2520the%2520dynamics%252C%2520but%2520little%2520is%250Aknown%2520about%2520whether%2520the%2520stability%2520properties%2520can%2520also%2520be%2520inferred.%2520We%2520show%2520that%250Athe%2520CAE-ESN%2520model%2520infers%2520the%2520invariant%2520stability%2520properties%2520and%2520the%2520geometry%2520of%250Athe%2520tangent%2520space%2520in%2520the%2520low-dimensional%2520manifold%2520%2528i.e.%2520the%2520latent%2520space%2529%250Athrough%2520Lyapunov%2520exponents%2520and%2520covariant%2520Lyapunov%2520vectors.%2520This%2520work%2520opens%2520up%250Anew%2520opportunities%2520for%2520inferring%2520the%2520stability%2520of%2520high-dimensional%2520chaotic%250Asystems%2520in%2520latent%2520spaces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18003v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inferring%20stability%20properties%20of%20chaotic%20systems%20on%20autoencoders%27%0A%20%20latent%20spaces&entry.906535625=Elise%20%C3%96zalp%20and%20Luca%20Magri&entry.1292438233=%20%20The%20data-driven%20learning%20of%20solutions%20of%20partial%20differential%20equations%20can%0Abe%20based%20on%20a%20divide-and-conquer%20strategy.%20First%2C%20the%20high%20dimensional%20data%20is%0Acompressed%20to%20a%20latent%20space%20with%20an%20autoencoder%3B%20and%2C%20second%2C%20the%20temporal%0Adynamics%20are%20inferred%20on%20the%20latent%20space%20with%20a%20form%20of%20recurrent%20neural%0Anetwork.%20In%20chaotic%20systems%20and%20turbulence%2C%20convolutional%20autoencoders%20and%20echo%0Astate%20networks%20%28CAE-ESN%29%20successfully%20forecast%20the%20dynamics%2C%20but%20little%20is%0Aknown%20about%20whether%20the%20stability%20properties%20can%20also%20be%20inferred.%20We%20show%20that%0Athe%20CAE-ESN%20model%20infers%20the%20invariant%20stability%20properties%20and%20the%20geometry%20of%0Athe%20tangent%20space%20in%20the%20low-dimensional%20manifold%20%28i.e.%20the%20latent%20space%29%0Athrough%20Lyapunov%20exponents%20and%20covariant%20Lyapunov%20vectors.%20This%20work%20opens%20up%0Anew%20opportunities%20for%20inferring%20the%20stability%20of%20high-dimensional%20chaotic%0Asystems%20in%20latent%20spaces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.18003v1&entry.124074799=Read"},
{"title": "DexGrasp-Diffusion: Diffusion-based Unified Functional Grasp Synthesis\n  Method for Multi-Dexterous Robotic Hands", "author": "Zhengshen Zhang and Lei Zhou and Chenchen Liu and Zhiyang Liu and Chengran Yuan and Sheng Guo and Ruiteng Zhao and Marcelo H. Ang Jr. and Francis EH Tay", "abstract": "  The versatility and adaptability of human grasping catalyze advancing\ndexterous robotic manipulation. While significant strides have been made in\ndexterous grasp generation, current research endeavors pivot towards optimizing\nobject manipulation while ensuring functional integrity, emphasizing the\nsynthesis of functional grasps following desired affordance instructions. This\npaper addresses the challenge of synthesizing functional grasps tailored to\ndiverse dexterous robotic hands by proposing DexGrasp-Diffusion, an end-to-end\nmodularized diffusion-based method. DexGrasp-Diffusion integrates\nMultiHandDiffuser, a novel unified data-driven diffusion model for\nmulti-dexterous hands grasp estimation, with DexDiscriminator, which employs a\nPhysics Discriminator and a Functional Discriminator with open-vocabulary\nsetting to filter physically plausible functional grasps based on object\naffordances. The experimental evaluation conducted on the MultiDex dataset\nprovides substantiating evidence supporting the superior performance of\nMultiHandDiffuser over the baseline model in terms of success rate, grasp\ndiversity, and collision depth. Moreover, we demonstrate the capacity of\nDexGrasp-Diffusion to reliably generate functional grasps for household objects\naligned with specific affordance instructions.\n", "link": "http://arxiv.org/abs/2407.09899v2", "date": "2024-10-23", "relevancy": 1.9273, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.729}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.537}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5315}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DexGrasp-Diffusion%3A%20Diffusion-based%20Unified%20Functional%20Grasp%20Synthesis%0A%20%20Method%20for%20Multi-Dexterous%20Robotic%20Hands&body=Title%3A%20DexGrasp-Diffusion%3A%20Diffusion-based%20Unified%20Functional%20Grasp%20Synthesis%0A%20%20Method%20for%20Multi-Dexterous%20Robotic%20Hands%0AAuthor%3A%20Zhengshen%20Zhang%20and%20Lei%20Zhou%20and%20Chenchen%20Liu%20and%20Zhiyang%20Liu%20and%20Chengran%20Yuan%20and%20Sheng%20Guo%20and%20Ruiteng%20Zhao%20and%20Marcelo%20H.%20Ang%20Jr.%20and%20Francis%20EH%20Tay%0AAbstract%3A%20%20%20The%20versatility%20and%20adaptability%20of%20human%20grasping%20catalyze%20advancing%0Adexterous%20robotic%20manipulation.%20While%20significant%20strides%20have%20been%20made%20in%0Adexterous%20grasp%20generation%2C%20current%20research%20endeavors%20pivot%20towards%20optimizing%0Aobject%20manipulation%20while%20ensuring%20functional%20integrity%2C%20emphasizing%20the%0Asynthesis%20of%20functional%20grasps%20following%20desired%20affordance%20instructions.%20This%0Apaper%20addresses%20the%20challenge%20of%20synthesizing%20functional%20grasps%20tailored%20to%0Adiverse%20dexterous%20robotic%20hands%20by%20proposing%20DexGrasp-Diffusion%2C%20an%20end-to-end%0Amodularized%20diffusion-based%20method.%20DexGrasp-Diffusion%20integrates%0AMultiHandDiffuser%2C%20a%20novel%20unified%20data-driven%20diffusion%20model%20for%0Amulti-dexterous%20hands%20grasp%20estimation%2C%20with%20DexDiscriminator%2C%20which%20employs%20a%0APhysics%20Discriminator%20and%20a%20Functional%20Discriminator%20with%20open-vocabulary%0Asetting%20to%20filter%20physically%20plausible%20functional%20grasps%20based%20on%20object%0Aaffordances.%20The%20experimental%20evaluation%20conducted%20on%20the%20MultiDex%20dataset%0Aprovides%20substantiating%20evidence%20supporting%20the%20superior%20performance%20of%0AMultiHandDiffuser%20over%20the%20baseline%20model%20in%20terms%20of%20success%20rate%2C%20grasp%0Adiversity%2C%20and%20collision%20depth.%20Moreover%2C%20we%20demonstrate%20the%20capacity%20of%0ADexGrasp-Diffusion%20to%20reliably%20generate%20functional%20grasps%20for%20household%20objects%0Aaligned%20with%20specific%20affordance%20instructions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.09899v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDexGrasp-Diffusion%253A%2520Diffusion-based%2520Unified%2520Functional%2520Grasp%2520Synthesis%250A%2520%2520Method%2520for%2520Multi-Dexterous%2520Robotic%2520Hands%26entry.906535625%3DZhengshen%2520Zhang%2520and%2520Lei%2520Zhou%2520and%2520Chenchen%2520Liu%2520and%2520Zhiyang%2520Liu%2520and%2520Chengran%2520Yuan%2520and%2520Sheng%2520Guo%2520and%2520Ruiteng%2520Zhao%2520and%2520Marcelo%2520H.%2520Ang%2520Jr.%2520and%2520Francis%2520EH%2520Tay%26entry.1292438233%3D%2520%2520The%2520versatility%2520and%2520adaptability%2520of%2520human%2520grasping%2520catalyze%2520advancing%250Adexterous%2520robotic%2520manipulation.%2520While%2520significant%2520strides%2520have%2520been%2520made%2520in%250Adexterous%2520grasp%2520generation%252C%2520current%2520research%2520endeavors%2520pivot%2520towards%2520optimizing%250Aobject%2520manipulation%2520while%2520ensuring%2520functional%2520integrity%252C%2520emphasizing%2520the%250Asynthesis%2520of%2520functional%2520grasps%2520following%2520desired%2520affordance%2520instructions.%2520This%250Apaper%2520addresses%2520the%2520challenge%2520of%2520synthesizing%2520functional%2520grasps%2520tailored%2520to%250Adiverse%2520dexterous%2520robotic%2520hands%2520by%2520proposing%2520DexGrasp-Diffusion%252C%2520an%2520end-to-end%250Amodularized%2520diffusion-based%2520method.%2520DexGrasp-Diffusion%2520integrates%250AMultiHandDiffuser%252C%2520a%2520novel%2520unified%2520data-driven%2520diffusion%2520model%2520for%250Amulti-dexterous%2520hands%2520grasp%2520estimation%252C%2520with%2520DexDiscriminator%252C%2520which%2520employs%2520a%250APhysics%2520Discriminator%2520and%2520a%2520Functional%2520Discriminator%2520with%2520open-vocabulary%250Asetting%2520to%2520filter%2520physically%2520plausible%2520functional%2520grasps%2520based%2520on%2520object%250Aaffordances.%2520The%2520experimental%2520evaluation%2520conducted%2520on%2520the%2520MultiDex%2520dataset%250Aprovides%2520substantiating%2520evidence%2520supporting%2520the%2520superior%2520performance%2520of%250AMultiHandDiffuser%2520over%2520the%2520baseline%2520model%2520in%2520terms%2520of%2520success%2520rate%252C%2520grasp%250Adiversity%252C%2520and%2520collision%2520depth.%2520Moreover%252C%2520we%2520demonstrate%2520the%2520capacity%2520of%250ADexGrasp-Diffusion%2520to%2520reliably%2520generate%2520functional%2520grasps%2520for%2520household%2520objects%250Aaligned%2520with%2520specific%2520affordance%2520instructions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.09899v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DexGrasp-Diffusion%3A%20Diffusion-based%20Unified%20Functional%20Grasp%20Synthesis%0A%20%20Method%20for%20Multi-Dexterous%20Robotic%20Hands&entry.906535625=Zhengshen%20Zhang%20and%20Lei%20Zhou%20and%20Chenchen%20Liu%20and%20Zhiyang%20Liu%20and%20Chengran%20Yuan%20and%20Sheng%20Guo%20and%20Ruiteng%20Zhao%20and%20Marcelo%20H.%20Ang%20Jr.%20and%20Francis%20EH%20Tay&entry.1292438233=%20%20The%20versatility%20and%20adaptability%20of%20human%20grasping%20catalyze%20advancing%0Adexterous%20robotic%20manipulation.%20While%20significant%20strides%20have%20been%20made%20in%0Adexterous%20grasp%20generation%2C%20current%20research%20endeavors%20pivot%20towards%20optimizing%0Aobject%20manipulation%20while%20ensuring%20functional%20integrity%2C%20emphasizing%20the%0Asynthesis%20of%20functional%20grasps%20following%20desired%20affordance%20instructions.%20This%0Apaper%20addresses%20the%20challenge%20of%20synthesizing%20functional%20grasps%20tailored%20to%0Adiverse%20dexterous%20robotic%20hands%20by%20proposing%20DexGrasp-Diffusion%2C%20an%20end-to-end%0Amodularized%20diffusion-based%20method.%20DexGrasp-Diffusion%20integrates%0AMultiHandDiffuser%2C%20a%20novel%20unified%20data-driven%20diffusion%20model%20for%0Amulti-dexterous%20hands%20grasp%20estimation%2C%20with%20DexDiscriminator%2C%20which%20employs%20a%0APhysics%20Discriminator%20and%20a%20Functional%20Discriminator%20with%20open-vocabulary%0Asetting%20to%20filter%20physically%20plausible%20functional%20grasps%20based%20on%20object%0Aaffordances.%20The%20experimental%20evaluation%20conducted%20on%20the%20MultiDex%20dataset%0Aprovides%20substantiating%20evidence%20supporting%20the%20superior%20performance%20of%0AMultiHandDiffuser%20over%20the%20baseline%20model%20in%20terms%20of%20success%20rate%2C%20grasp%0Adiversity%2C%20and%20collision%20depth.%20Moreover%2C%20we%20demonstrate%20the%20capacity%20of%0ADexGrasp-Diffusion%20to%20reliably%20generate%20functional%20grasps%20for%20household%20objects%0Aaligned%20with%20specific%20affordance%20instructions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.09899v2&entry.124074799=Read"},
{"title": "Evaluating Explanations Through LLMs: Beyond Traditional User Studies", "author": "Francesco Bombassei De Bona and Gabriele Dominici and Tim Miller and Marc Langheinrich and Martin Gjoreski", "abstract": "  As AI becomes fundamental in sectors like healthcare, explainable AI (XAI)\ntools are essential for trust and transparency. However, traditional user\nstudies used to evaluate these tools are often costly, time consuming, and\ndifficult to scale. In this paper, we explore the use of Large Language Models\n(LLMs) to replicate human participants to help streamline XAI evaluation. We\nreproduce a user study comparing counterfactual and causal explanations,\nreplicating human participants with seven LLMs under various settings. Our\nresults show that (i) LLMs can replicate most conclusions from the original\nstudy, (ii) different LLMs yield varying levels of alignment in the results,\nand (iii) experimental factors such as LLM memory and output variability affect\nalignment with human responses. These initial findings suggest that LLMs could\nprovide a scalable and cost-effective way to simplify qualitative XAI\nevaluation.\n", "link": "http://arxiv.org/abs/2410.17781v1", "date": "2024-10-23", "relevancy": 1.9265, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5007}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4778}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Explanations%20Through%20LLMs%3A%20Beyond%20Traditional%20User%20Studies&body=Title%3A%20Evaluating%20Explanations%20Through%20LLMs%3A%20Beyond%20Traditional%20User%20Studies%0AAuthor%3A%20Francesco%20Bombassei%20De%20Bona%20and%20Gabriele%20Dominici%20and%20Tim%20Miller%20and%20Marc%20Langheinrich%20and%20Martin%20Gjoreski%0AAbstract%3A%20%20%20As%20AI%20becomes%20fundamental%20in%20sectors%20like%20healthcare%2C%20explainable%20AI%20%28XAI%29%0Atools%20are%20essential%20for%20trust%20and%20transparency.%20However%2C%20traditional%20user%0Astudies%20used%20to%20evaluate%20these%20tools%20are%20often%20costly%2C%20time%20consuming%2C%20and%0Adifficult%20to%20scale.%20In%20this%20paper%2C%20we%20explore%20the%20use%20of%20Large%20Language%20Models%0A%28LLMs%29%20to%20replicate%20human%20participants%20to%20help%20streamline%20XAI%20evaluation.%20We%0Areproduce%20a%20user%20study%20comparing%20counterfactual%20and%20causal%20explanations%2C%0Areplicating%20human%20participants%20with%20seven%20LLMs%20under%20various%20settings.%20Our%0Aresults%20show%20that%20%28i%29%20LLMs%20can%20replicate%20most%20conclusions%20from%20the%20original%0Astudy%2C%20%28ii%29%20different%20LLMs%20yield%20varying%20levels%20of%20alignment%20in%20the%20results%2C%0Aand%20%28iii%29%20experimental%20factors%20such%20as%20LLM%20memory%20and%20output%20variability%20affect%0Aalignment%20with%20human%20responses.%20These%20initial%20findings%20suggest%20that%20LLMs%20could%0Aprovide%20a%20scalable%20and%20cost-effective%20way%20to%20simplify%20qualitative%20XAI%0Aevaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17781v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Explanations%2520Through%2520LLMs%253A%2520Beyond%2520Traditional%2520User%2520Studies%26entry.906535625%3DFrancesco%2520Bombassei%2520De%2520Bona%2520and%2520Gabriele%2520Dominici%2520and%2520Tim%2520Miller%2520and%2520Marc%2520Langheinrich%2520and%2520Martin%2520Gjoreski%26entry.1292438233%3D%2520%2520As%2520AI%2520becomes%2520fundamental%2520in%2520sectors%2520like%2520healthcare%252C%2520explainable%2520AI%2520%2528XAI%2529%250Atools%2520are%2520essential%2520for%2520trust%2520and%2520transparency.%2520However%252C%2520traditional%2520user%250Astudies%2520used%2520to%2520evaluate%2520these%2520tools%2520are%2520often%2520costly%252C%2520time%2520consuming%252C%2520and%250Adifficult%2520to%2520scale.%2520In%2520this%2520paper%252C%2520we%2520explore%2520the%2520use%2520of%2520Large%2520Language%2520Models%250A%2528LLMs%2529%2520to%2520replicate%2520human%2520participants%2520to%2520help%2520streamline%2520XAI%2520evaluation.%2520We%250Areproduce%2520a%2520user%2520study%2520comparing%2520counterfactual%2520and%2520causal%2520explanations%252C%250Areplicating%2520human%2520participants%2520with%2520seven%2520LLMs%2520under%2520various%2520settings.%2520Our%250Aresults%2520show%2520that%2520%2528i%2529%2520LLMs%2520can%2520replicate%2520most%2520conclusions%2520from%2520the%2520original%250Astudy%252C%2520%2528ii%2529%2520different%2520LLMs%2520yield%2520varying%2520levels%2520of%2520alignment%2520in%2520the%2520results%252C%250Aand%2520%2528iii%2529%2520experimental%2520factors%2520such%2520as%2520LLM%2520memory%2520and%2520output%2520variability%2520affect%250Aalignment%2520with%2520human%2520responses.%2520These%2520initial%2520findings%2520suggest%2520that%2520LLMs%2520could%250Aprovide%2520a%2520scalable%2520and%2520cost-effective%2520way%2520to%2520simplify%2520qualitative%2520XAI%250Aevaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17781v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Explanations%20Through%20LLMs%3A%20Beyond%20Traditional%20User%20Studies&entry.906535625=Francesco%20Bombassei%20De%20Bona%20and%20Gabriele%20Dominici%20and%20Tim%20Miller%20and%20Marc%20Langheinrich%20and%20Martin%20Gjoreski&entry.1292438233=%20%20As%20AI%20becomes%20fundamental%20in%20sectors%20like%20healthcare%2C%20explainable%20AI%20%28XAI%29%0Atools%20are%20essential%20for%20trust%20and%20transparency.%20However%2C%20traditional%20user%0Astudies%20used%20to%20evaluate%20these%20tools%20are%20often%20costly%2C%20time%20consuming%2C%20and%0Adifficult%20to%20scale.%20In%20this%20paper%2C%20we%20explore%20the%20use%20of%20Large%20Language%20Models%0A%28LLMs%29%20to%20replicate%20human%20participants%20to%20help%20streamline%20XAI%20evaluation.%20We%0Areproduce%20a%20user%20study%20comparing%20counterfactual%20and%20causal%20explanations%2C%0Areplicating%20human%20participants%20with%20seven%20LLMs%20under%20various%20settings.%20Our%0Aresults%20show%20that%20%28i%29%20LLMs%20can%20replicate%20most%20conclusions%20from%20the%20original%0Astudy%2C%20%28ii%29%20different%20LLMs%20yield%20varying%20levels%20of%20alignment%20in%20the%20results%2C%0Aand%20%28iii%29%20experimental%20factors%20such%20as%20LLM%20memory%20and%20output%20variability%20affect%0Aalignment%20with%20human%20responses.%20These%20initial%20findings%20suggest%20that%20LLMs%20could%0Aprovide%20a%20scalable%20and%20cost-effective%20way%20to%20simplify%20qualitative%20XAI%0Aevaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17781v1&entry.124074799=Read"},
{"title": "Posterior Sampling-based Online Learning for Episodic POMDPs", "author": "Dengwang Tang and Dongze Ye and Rahul Jain and Ashutosh Nayyar and Pierluigi Nuzzo", "abstract": "  Learning in POMDPs is known to be significantly harder than in MDPs. In this\npaper, we consider the online learning problem for episodic POMDPs with unknown\ntransition and observation models. We propose a Posterior Sampling-based\nreinforcement learning algorithm for POMDPs (PS4POMDPs), which is much simpler\nand more implementable compared to state-of-the-art optimism-based online\nlearning algorithms for POMDPs. We show that the Bayesian regret of the\nproposed algorithm scales as the square root of the number of episodes and is\npolynomial in the other parameters. In a general setting, the regret scales\nexponentially in the horizon length $H$, and we show that this is inevitable by\nproviding a lower bound. However, when the POMDP is undercomplete and weakly\nrevealing (a common assumption in the recent literature), we establish a\npolynomial Bayesian regret bound. We finally propose a posterior sampling\nalgorithm for multi-agent POMDPs, and show it too has sublinear regret.\n", "link": "http://arxiv.org/abs/2310.10107v4", "date": "2024-10-23", "relevancy": 1.9214, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5211}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4758}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Posterior%20Sampling-based%20Online%20Learning%20for%20Episodic%20POMDPs&body=Title%3A%20Posterior%20Sampling-based%20Online%20Learning%20for%20Episodic%20POMDPs%0AAuthor%3A%20Dengwang%20Tang%20and%20Dongze%20Ye%20and%20Rahul%20Jain%20and%20Ashutosh%20Nayyar%20and%20Pierluigi%20Nuzzo%0AAbstract%3A%20%20%20Learning%20in%20POMDPs%20is%20known%20to%20be%20significantly%20harder%20than%20in%20MDPs.%20In%20this%0Apaper%2C%20we%20consider%20the%20online%20learning%20problem%20for%20episodic%20POMDPs%20with%20unknown%0Atransition%20and%20observation%20models.%20We%20propose%20a%20Posterior%20Sampling-based%0Areinforcement%20learning%20algorithm%20for%20POMDPs%20%28PS4POMDPs%29%2C%20which%20is%20much%20simpler%0Aand%20more%20implementable%20compared%20to%20state-of-the-art%20optimism-based%20online%0Alearning%20algorithms%20for%20POMDPs.%20We%20show%20that%20the%20Bayesian%20regret%20of%20the%0Aproposed%20algorithm%20scales%20as%20the%20square%20root%20of%20the%20number%20of%20episodes%20and%20is%0Apolynomial%20in%20the%20other%20parameters.%20In%20a%20general%20setting%2C%20the%20regret%20scales%0Aexponentially%20in%20the%20horizon%20length%20%24H%24%2C%20and%20we%20show%20that%20this%20is%20inevitable%20by%0Aproviding%20a%20lower%20bound.%20However%2C%20when%20the%20POMDP%20is%20undercomplete%20and%20weakly%0Arevealing%20%28a%20common%20assumption%20in%20the%20recent%20literature%29%2C%20we%20establish%20a%0Apolynomial%20Bayesian%20regret%20bound.%20We%20finally%20propose%20a%20posterior%20sampling%0Aalgorithm%20for%20multi-agent%20POMDPs%2C%20and%20show%20it%20too%20has%20sublinear%20regret.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.10107v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosterior%2520Sampling-based%2520Online%2520Learning%2520for%2520Episodic%2520POMDPs%26entry.906535625%3DDengwang%2520Tang%2520and%2520Dongze%2520Ye%2520and%2520Rahul%2520Jain%2520and%2520Ashutosh%2520Nayyar%2520and%2520Pierluigi%2520Nuzzo%26entry.1292438233%3D%2520%2520Learning%2520in%2520POMDPs%2520is%2520known%2520to%2520be%2520significantly%2520harder%2520than%2520in%2520MDPs.%2520In%2520this%250Apaper%252C%2520we%2520consider%2520the%2520online%2520learning%2520problem%2520for%2520episodic%2520POMDPs%2520with%2520unknown%250Atransition%2520and%2520observation%2520models.%2520We%2520propose%2520a%2520Posterior%2520Sampling-based%250Areinforcement%2520learning%2520algorithm%2520for%2520POMDPs%2520%2528PS4POMDPs%2529%252C%2520which%2520is%2520much%2520simpler%250Aand%2520more%2520implementable%2520compared%2520to%2520state-of-the-art%2520optimism-based%2520online%250Alearning%2520algorithms%2520for%2520POMDPs.%2520We%2520show%2520that%2520the%2520Bayesian%2520regret%2520of%2520the%250Aproposed%2520algorithm%2520scales%2520as%2520the%2520square%2520root%2520of%2520the%2520number%2520of%2520episodes%2520and%2520is%250Apolynomial%2520in%2520the%2520other%2520parameters.%2520In%2520a%2520general%2520setting%252C%2520the%2520regret%2520scales%250Aexponentially%2520in%2520the%2520horizon%2520length%2520%2524H%2524%252C%2520and%2520we%2520show%2520that%2520this%2520is%2520inevitable%2520by%250Aproviding%2520a%2520lower%2520bound.%2520However%252C%2520when%2520the%2520POMDP%2520is%2520undercomplete%2520and%2520weakly%250Arevealing%2520%2528a%2520common%2520assumption%2520in%2520the%2520recent%2520literature%2529%252C%2520we%2520establish%2520a%250Apolynomial%2520Bayesian%2520regret%2520bound.%2520We%2520finally%2520propose%2520a%2520posterior%2520sampling%250Aalgorithm%2520for%2520multi-agent%2520POMDPs%252C%2520and%2520show%2520it%2520too%2520has%2520sublinear%2520regret.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.10107v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Posterior%20Sampling-based%20Online%20Learning%20for%20Episodic%20POMDPs&entry.906535625=Dengwang%20Tang%20and%20Dongze%20Ye%20and%20Rahul%20Jain%20and%20Ashutosh%20Nayyar%20and%20Pierluigi%20Nuzzo&entry.1292438233=%20%20Learning%20in%20POMDPs%20is%20known%20to%20be%20significantly%20harder%20than%20in%20MDPs.%20In%20this%0Apaper%2C%20we%20consider%20the%20online%20learning%20problem%20for%20episodic%20POMDPs%20with%20unknown%0Atransition%20and%20observation%20models.%20We%20propose%20a%20Posterior%20Sampling-based%0Areinforcement%20learning%20algorithm%20for%20POMDPs%20%28PS4POMDPs%29%2C%20which%20is%20much%20simpler%0Aand%20more%20implementable%20compared%20to%20state-of-the-art%20optimism-based%20online%0Alearning%20algorithms%20for%20POMDPs.%20We%20show%20that%20the%20Bayesian%20regret%20of%20the%0Aproposed%20algorithm%20scales%20as%20the%20square%20root%20of%20the%20number%20of%20episodes%20and%20is%0Apolynomial%20in%20the%20other%20parameters.%20In%20a%20general%20setting%2C%20the%20regret%20scales%0Aexponentially%20in%20the%20horizon%20length%20%24H%24%2C%20and%20we%20show%20that%20this%20is%20inevitable%20by%0Aproviding%20a%20lower%20bound.%20However%2C%20when%20the%20POMDP%20is%20undercomplete%20and%20weakly%0Arevealing%20%28a%20common%20assumption%20in%20the%20recent%20literature%29%2C%20we%20establish%20a%0Apolynomial%20Bayesian%20regret%20bound.%20We%20finally%20propose%20a%20posterior%20sampling%0Aalgorithm%20for%20multi-agent%20POMDPs%2C%20and%20show%20it%20too%20has%20sublinear%20regret.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.10107v4&entry.124074799=Read"},
{"title": "AdaDiffSR: Adaptive Region-aware Dynamic Acceleration Diffusion Model\n  for Real-World Image Super-Resolution", "author": "Yuanting Fan and Chengxu Liu and Nengzhong Yin and Changlong Gao and Xueming Qian", "abstract": "  Diffusion models (DMs) have shown promising results on single-image\nsuper-resolution and other image-to-image translation tasks. Benefiting from\nmore computational resources and longer inference times, they are able to yield\nmore realistic images. Existing DMs-based super-resolution methods try to\nachieve an overall average recovery over all regions via iterative refinement,\nignoring the consideration that different input image regions require different\ntimesteps to reconstruct. In this work, we notice that previous DMs-based\nsuper-resolution methods suffer from wasting computational resources to\nreconstruct invisible details. To further improve the utilization of\ncomputational resources, we propose AdaDiffSR, a DMs-based SR pipeline with\ndynamic timesteps sampling strategy (DTSS). Specifically, by introducing the\nmulti-metrics latent entropy module (MMLE), we can achieve dynamic perception\nof the latent spatial information gain during the denoising process, thereby\nguiding the dynamic selection of the timesteps. In addition, we adopt a\nprogressive feature injection module (PFJ), which dynamically injects the\noriginal image features into the denoising process based on the current\ninformation gain, so as to generate images with both fidelity and realism.\nExperiments show that our AdaDiffSR achieves comparable performance over\ncurrent state-of-the-art DMs-based SR methods while consuming less\ncomputational resources and inference time on both synthetic and real-world\ndatasets.\n", "link": "http://arxiv.org/abs/2410.17752v1", "date": "2024-10-23", "relevancy": 1.9198, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.7203}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6275}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AdaDiffSR%3A%20Adaptive%20Region-aware%20Dynamic%20Acceleration%20Diffusion%20Model%0A%20%20for%20Real-World%20Image%20Super-Resolution&body=Title%3A%20AdaDiffSR%3A%20Adaptive%20Region-aware%20Dynamic%20Acceleration%20Diffusion%20Model%0A%20%20for%20Real-World%20Image%20Super-Resolution%0AAuthor%3A%20Yuanting%20Fan%20and%20Chengxu%20Liu%20and%20Nengzhong%20Yin%20and%20Changlong%20Gao%20and%20Xueming%20Qian%0AAbstract%3A%20%20%20Diffusion%20models%20%28DMs%29%20have%20shown%20promising%20results%20on%20single-image%0Asuper-resolution%20and%20other%20image-to-image%20translation%20tasks.%20Benefiting%20from%0Amore%20computational%20resources%20and%20longer%20inference%20times%2C%20they%20are%20able%20to%20yield%0Amore%20realistic%20images.%20Existing%20DMs-based%20super-resolution%20methods%20try%20to%0Aachieve%20an%20overall%20average%20recovery%20over%20all%20regions%20via%20iterative%20refinement%2C%0Aignoring%20the%20consideration%20that%20different%20input%20image%20regions%20require%20different%0Atimesteps%20to%20reconstruct.%20In%20this%20work%2C%20we%20notice%20that%20previous%20DMs-based%0Asuper-resolution%20methods%20suffer%20from%20wasting%20computational%20resources%20to%0Areconstruct%20invisible%20details.%20To%20further%20improve%20the%20utilization%20of%0Acomputational%20resources%2C%20we%20propose%20AdaDiffSR%2C%20a%20DMs-based%20SR%20pipeline%20with%0Adynamic%20timesteps%20sampling%20strategy%20%28DTSS%29.%20Specifically%2C%20by%20introducing%20the%0Amulti-metrics%20latent%20entropy%20module%20%28MMLE%29%2C%20we%20can%20achieve%20dynamic%20perception%0Aof%20the%20latent%20spatial%20information%20gain%20during%20the%20denoising%20process%2C%20thereby%0Aguiding%20the%20dynamic%20selection%20of%20the%20timesteps.%20In%20addition%2C%20we%20adopt%20a%0Aprogressive%20feature%20injection%20module%20%28PFJ%29%2C%20which%20dynamically%20injects%20the%0Aoriginal%20image%20features%20into%20the%20denoising%20process%20based%20on%20the%20current%0Ainformation%20gain%2C%20so%20as%20to%20generate%20images%20with%20both%20fidelity%20and%20realism.%0AExperiments%20show%20that%20our%20AdaDiffSR%20achieves%20comparable%20performance%20over%0Acurrent%20state-of-the-art%20DMs-based%20SR%20methods%20while%20consuming%20less%0Acomputational%20resources%20and%20inference%20time%20on%20both%20synthetic%20and%20real-world%0Adatasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17752v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaDiffSR%253A%2520Adaptive%2520Region-aware%2520Dynamic%2520Acceleration%2520Diffusion%2520Model%250A%2520%2520for%2520Real-World%2520Image%2520Super-Resolution%26entry.906535625%3DYuanting%2520Fan%2520and%2520Chengxu%2520Liu%2520and%2520Nengzhong%2520Yin%2520and%2520Changlong%2520Gao%2520and%2520Xueming%2520Qian%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520%2528DMs%2529%2520have%2520shown%2520promising%2520results%2520on%2520single-image%250Asuper-resolution%2520and%2520other%2520image-to-image%2520translation%2520tasks.%2520Benefiting%2520from%250Amore%2520computational%2520resources%2520and%2520longer%2520inference%2520times%252C%2520they%2520are%2520able%2520to%2520yield%250Amore%2520realistic%2520images.%2520Existing%2520DMs-based%2520super-resolution%2520methods%2520try%2520to%250Aachieve%2520an%2520overall%2520average%2520recovery%2520over%2520all%2520regions%2520via%2520iterative%2520refinement%252C%250Aignoring%2520the%2520consideration%2520that%2520different%2520input%2520image%2520regions%2520require%2520different%250Atimesteps%2520to%2520reconstruct.%2520In%2520this%2520work%252C%2520we%2520notice%2520that%2520previous%2520DMs-based%250Asuper-resolution%2520methods%2520suffer%2520from%2520wasting%2520computational%2520resources%2520to%250Areconstruct%2520invisible%2520details.%2520To%2520further%2520improve%2520the%2520utilization%2520of%250Acomputational%2520resources%252C%2520we%2520propose%2520AdaDiffSR%252C%2520a%2520DMs-based%2520SR%2520pipeline%2520with%250Adynamic%2520timesteps%2520sampling%2520strategy%2520%2528DTSS%2529.%2520Specifically%252C%2520by%2520introducing%2520the%250Amulti-metrics%2520latent%2520entropy%2520module%2520%2528MMLE%2529%252C%2520we%2520can%2520achieve%2520dynamic%2520perception%250Aof%2520the%2520latent%2520spatial%2520information%2520gain%2520during%2520the%2520denoising%2520process%252C%2520thereby%250Aguiding%2520the%2520dynamic%2520selection%2520of%2520the%2520timesteps.%2520In%2520addition%252C%2520we%2520adopt%2520a%250Aprogressive%2520feature%2520injection%2520module%2520%2528PFJ%2529%252C%2520which%2520dynamically%2520injects%2520the%250Aoriginal%2520image%2520features%2520into%2520the%2520denoising%2520process%2520based%2520on%2520the%2520current%250Ainformation%2520gain%252C%2520so%2520as%2520to%2520generate%2520images%2520with%2520both%2520fidelity%2520and%2520realism.%250AExperiments%2520show%2520that%2520our%2520AdaDiffSR%2520achieves%2520comparable%2520performance%2520over%250Acurrent%2520state-of-the-art%2520DMs-based%2520SR%2520methods%2520while%2520consuming%2520less%250Acomputational%2520resources%2520and%2520inference%2520time%2520on%2520both%2520synthetic%2520and%2520real-world%250Adatasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17752v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AdaDiffSR%3A%20Adaptive%20Region-aware%20Dynamic%20Acceleration%20Diffusion%20Model%0A%20%20for%20Real-World%20Image%20Super-Resolution&entry.906535625=Yuanting%20Fan%20and%20Chengxu%20Liu%20and%20Nengzhong%20Yin%20and%20Changlong%20Gao%20and%20Xueming%20Qian&entry.1292438233=%20%20Diffusion%20models%20%28DMs%29%20have%20shown%20promising%20results%20on%20single-image%0Asuper-resolution%20and%20other%20image-to-image%20translation%20tasks.%20Benefiting%20from%0Amore%20computational%20resources%20and%20longer%20inference%20times%2C%20they%20are%20able%20to%20yield%0Amore%20realistic%20images.%20Existing%20DMs-based%20super-resolution%20methods%20try%20to%0Aachieve%20an%20overall%20average%20recovery%20over%20all%20regions%20via%20iterative%20refinement%2C%0Aignoring%20the%20consideration%20that%20different%20input%20image%20regions%20require%20different%0Atimesteps%20to%20reconstruct.%20In%20this%20work%2C%20we%20notice%20that%20previous%20DMs-based%0Asuper-resolution%20methods%20suffer%20from%20wasting%20computational%20resources%20to%0Areconstruct%20invisible%20details.%20To%20further%20improve%20the%20utilization%20of%0Acomputational%20resources%2C%20we%20propose%20AdaDiffSR%2C%20a%20DMs-based%20SR%20pipeline%20with%0Adynamic%20timesteps%20sampling%20strategy%20%28DTSS%29.%20Specifically%2C%20by%20introducing%20the%0Amulti-metrics%20latent%20entropy%20module%20%28MMLE%29%2C%20we%20can%20achieve%20dynamic%20perception%0Aof%20the%20latent%20spatial%20information%20gain%20during%20the%20denoising%20process%2C%20thereby%0Aguiding%20the%20dynamic%20selection%20of%20the%20timesteps.%20In%20addition%2C%20we%20adopt%20a%0Aprogressive%20feature%20injection%20module%20%28PFJ%29%2C%20which%20dynamically%20injects%20the%0Aoriginal%20image%20features%20into%20the%20denoising%20process%20based%20on%20the%20current%0Ainformation%20gain%2C%20so%20as%20to%20generate%20images%20with%20both%20fidelity%20and%20realism.%0AExperiments%20show%20that%20our%20AdaDiffSR%20achieves%20comparable%20performance%20over%0Acurrent%20state-of-the-art%20DMs-based%20SR%20methods%20while%20consuming%20less%0Acomputational%20resources%20and%20inference%20time%20on%20both%20synthetic%20and%20real-world%0Adatasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17752v1&entry.124074799=Read"},
{"title": "Escaping the Forest: Sparse Interpretable Neural Networks for Tabular\n  Data", "author": "Salvatore Raieli and Abdulrahman Altahhan and Nathalie Jeanray and St\u00e9phane Gerart and Sebastien Vachenc", "abstract": "  Tabular datasets are widely used in scientific disciplines such as biology.\nWhile these disciplines have already adopted AI methods to enhance their\nfindings and analysis, they mainly use tree-based methods due to their\ninterpretability. At the same time, artificial neural networks have been shown\nto offer superior flexibility and depth for rich and complex non-tabular\nproblems, but they are falling behind tree-based models for tabular data in\nterms of performance and interpretability. Although sparsity has been shown to\nimprove the interpretability and performance of ANN models for complex\nnon-tabular datasets, enforcing sparsity structurally and formatively for\ntabular data before training the model, remains an open question. To address\nthis question, we establish a method that infuses sparsity in neural networks\nby utilising attention mechanisms to capture the features' importance in\ntabular datasets. We show that our models, Sparse TABular NET or sTAB-Net with\nattention mechanisms, are more effective than tree-based models, reaching the\nstate-of-the-art on biological datasets. They further permit the extraction of\ninsights from these datasets and achieve better performance than post-hoc\nmethods like SHAP.\n", "link": "http://arxiv.org/abs/2410.17758v1", "date": "2024-10-23", "relevancy": 1.9166, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4827}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.479}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4707}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Escaping%20the%20Forest%3A%20Sparse%20Interpretable%20Neural%20Networks%20for%20Tabular%0A%20%20Data&body=Title%3A%20Escaping%20the%20Forest%3A%20Sparse%20Interpretable%20Neural%20Networks%20for%20Tabular%0A%20%20Data%0AAuthor%3A%20Salvatore%20Raieli%20and%20Abdulrahman%20Altahhan%20and%20Nathalie%20Jeanray%20and%20St%C3%A9phane%20Gerart%20and%20Sebastien%20Vachenc%0AAbstract%3A%20%20%20Tabular%20datasets%20are%20widely%20used%20in%20scientific%20disciplines%20such%20as%20biology.%0AWhile%20these%20disciplines%20have%20already%20adopted%20AI%20methods%20to%20enhance%20their%0Afindings%20and%20analysis%2C%20they%20mainly%20use%20tree-based%20methods%20due%20to%20their%0Ainterpretability.%20At%20the%20same%20time%2C%20artificial%20neural%20networks%20have%20been%20shown%0Ato%20offer%20superior%20flexibility%20and%20depth%20for%20rich%20and%20complex%20non-tabular%0Aproblems%2C%20but%20they%20are%20falling%20behind%20tree-based%20models%20for%20tabular%20data%20in%0Aterms%20of%20performance%20and%20interpretability.%20Although%20sparsity%20has%20been%20shown%20to%0Aimprove%20the%20interpretability%20and%20performance%20of%20ANN%20models%20for%20complex%0Anon-tabular%20datasets%2C%20enforcing%20sparsity%20structurally%20and%20formatively%20for%0Atabular%20data%20before%20training%20the%20model%2C%20remains%20an%20open%20question.%20To%20address%0Athis%20question%2C%20we%20establish%20a%20method%20that%20infuses%20sparsity%20in%20neural%20networks%0Aby%20utilising%20attention%20mechanisms%20to%20capture%20the%20features%27%20importance%20in%0Atabular%20datasets.%20We%20show%20that%20our%20models%2C%20Sparse%20TABular%20NET%20or%20sTAB-Net%20with%0Aattention%20mechanisms%2C%20are%20more%20effective%20than%20tree-based%20models%2C%20reaching%20the%0Astate-of-the-art%20on%20biological%20datasets.%20They%20further%20permit%20the%20extraction%20of%0Ainsights%20from%20these%20datasets%20and%20achieve%20better%20performance%20than%20post-hoc%0Amethods%20like%20SHAP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17758v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEscaping%2520the%2520Forest%253A%2520Sparse%2520Interpretable%2520Neural%2520Networks%2520for%2520Tabular%250A%2520%2520Data%26entry.906535625%3DSalvatore%2520Raieli%2520and%2520Abdulrahman%2520Altahhan%2520and%2520Nathalie%2520Jeanray%2520and%2520St%25C3%25A9phane%2520Gerart%2520and%2520Sebastien%2520Vachenc%26entry.1292438233%3D%2520%2520Tabular%2520datasets%2520are%2520widely%2520used%2520in%2520scientific%2520disciplines%2520such%2520as%2520biology.%250AWhile%2520these%2520disciplines%2520have%2520already%2520adopted%2520AI%2520methods%2520to%2520enhance%2520their%250Afindings%2520and%2520analysis%252C%2520they%2520mainly%2520use%2520tree-based%2520methods%2520due%2520to%2520their%250Ainterpretability.%2520At%2520the%2520same%2520time%252C%2520artificial%2520neural%2520networks%2520have%2520been%2520shown%250Ato%2520offer%2520superior%2520flexibility%2520and%2520depth%2520for%2520rich%2520and%2520complex%2520non-tabular%250Aproblems%252C%2520but%2520they%2520are%2520falling%2520behind%2520tree-based%2520models%2520for%2520tabular%2520data%2520in%250Aterms%2520of%2520performance%2520and%2520interpretability.%2520Although%2520sparsity%2520has%2520been%2520shown%2520to%250Aimprove%2520the%2520interpretability%2520and%2520performance%2520of%2520ANN%2520models%2520for%2520complex%250Anon-tabular%2520datasets%252C%2520enforcing%2520sparsity%2520structurally%2520and%2520formatively%2520for%250Atabular%2520data%2520before%2520training%2520the%2520model%252C%2520remains%2520an%2520open%2520question.%2520To%2520address%250Athis%2520question%252C%2520we%2520establish%2520a%2520method%2520that%2520infuses%2520sparsity%2520in%2520neural%2520networks%250Aby%2520utilising%2520attention%2520mechanisms%2520to%2520capture%2520the%2520features%2527%2520importance%2520in%250Atabular%2520datasets.%2520We%2520show%2520that%2520our%2520models%252C%2520Sparse%2520TABular%2520NET%2520or%2520sTAB-Net%2520with%250Aattention%2520mechanisms%252C%2520are%2520more%2520effective%2520than%2520tree-based%2520models%252C%2520reaching%2520the%250Astate-of-the-art%2520on%2520biological%2520datasets.%2520They%2520further%2520permit%2520the%2520extraction%2520of%250Ainsights%2520from%2520these%2520datasets%2520and%2520achieve%2520better%2520performance%2520than%2520post-hoc%250Amethods%2520like%2520SHAP.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17758v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Escaping%20the%20Forest%3A%20Sparse%20Interpretable%20Neural%20Networks%20for%20Tabular%0A%20%20Data&entry.906535625=Salvatore%20Raieli%20and%20Abdulrahman%20Altahhan%20and%20Nathalie%20Jeanray%20and%20St%C3%A9phane%20Gerart%20and%20Sebastien%20Vachenc&entry.1292438233=%20%20Tabular%20datasets%20are%20widely%20used%20in%20scientific%20disciplines%20such%20as%20biology.%0AWhile%20these%20disciplines%20have%20already%20adopted%20AI%20methods%20to%20enhance%20their%0Afindings%20and%20analysis%2C%20they%20mainly%20use%20tree-based%20methods%20due%20to%20their%0Ainterpretability.%20At%20the%20same%20time%2C%20artificial%20neural%20networks%20have%20been%20shown%0Ato%20offer%20superior%20flexibility%20and%20depth%20for%20rich%20and%20complex%20non-tabular%0Aproblems%2C%20but%20they%20are%20falling%20behind%20tree-based%20models%20for%20tabular%20data%20in%0Aterms%20of%20performance%20and%20interpretability.%20Although%20sparsity%20has%20been%20shown%20to%0Aimprove%20the%20interpretability%20and%20performance%20of%20ANN%20models%20for%20complex%0Anon-tabular%20datasets%2C%20enforcing%20sparsity%20structurally%20and%20formatively%20for%0Atabular%20data%20before%20training%20the%20model%2C%20remains%20an%20open%20question.%20To%20address%0Athis%20question%2C%20we%20establish%20a%20method%20that%20infuses%20sparsity%20in%20neural%20networks%0Aby%20utilising%20attention%20mechanisms%20to%20capture%20the%20features%27%20importance%20in%0Atabular%20datasets.%20We%20show%20that%20our%20models%2C%20Sparse%20TABular%20NET%20or%20sTAB-Net%20with%0Aattention%20mechanisms%2C%20are%20more%20effective%20than%20tree-based%20models%2C%20reaching%20the%0Astate-of-the-art%20on%20biological%20datasets.%20They%20further%20permit%20the%20extraction%20of%0Ainsights%20from%20these%20datasets%20and%20achieve%20better%20performance%20than%20post-hoc%0Amethods%20like%20SHAP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17758v1&entry.124074799=Read"},
{"title": "Benchmarking Floworks against OpenAI & Anthropic: A Novel Framework for\n  Enhanced LLM Function Calling", "author": "Nirav Bhan and Shival Gupta and Sai Manaswini and Ritik Baba and Narun Yadav and Hillori Desai and Yash Choudhary and Aman Pawar and Sarthak Shrivastava and Sudipta Biswas", "abstract": "  Large Language Models (LLMs) have shown remarkable capabilities in various\ndomains, yet their economic impact has been limited by challenges in tool use\nand function calling. This paper introduces ThorV2, a novel architecture that\nsignificantly enhances LLMs' function calling abilities. We develop a\ncomprehensive benchmark focused on HubSpot CRM operations to evaluate ThorV2\nagainst leading models from OpenAI and Anthropic. Our results demonstrate that\nThorV2 outperforms existing models in accuracy, reliability, latency, and cost\nefficiency for both single and multi-API calling tasks. We also show that\nThorV2 is far more reliable and scales better to multistep tasks compared to\ntraditional models. Our work offers the tantalizing possibility of more\naccurate function-calling compared to today's best-performing models using\nsignificantly smaller LLMs. These advancements have significant implications\nfor the development of more capable AI assistants and the broader application\nof LLMs in real-world scenarios.\n", "link": "http://arxiv.org/abs/2410.17950v1", "date": "2024-10-23", "relevancy": 1.8949, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4795}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4726}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20Floworks%20against%20OpenAI%20%26%20Anthropic%3A%20A%20Novel%20Framework%20for%0A%20%20Enhanced%20LLM%20Function%20Calling&body=Title%3A%20Benchmarking%20Floworks%20against%20OpenAI%20%26%20Anthropic%3A%20A%20Novel%20Framework%20for%0A%20%20Enhanced%20LLM%20Function%20Calling%0AAuthor%3A%20Nirav%20Bhan%20and%20Shival%20Gupta%20and%20Sai%20Manaswini%20and%20Ritik%20Baba%20and%20Narun%20Yadav%20and%20Hillori%20Desai%20and%20Yash%20Choudhary%20and%20Aman%20Pawar%20and%20Sarthak%20Shrivastava%20and%20Sudipta%20Biswas%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20various%0Adomains%2C%20yet%20their%20economic%20impact%20has%20been%20limited%20by%20challenges%20in%20tool%20use%0Aand%20function%20calling.%20This%20paper%20introduces%20ThorV2%2C%20a%20novel%20architecture%20that%0Asignificantly%20enhances%20LLMs%27%20function%20calling%20abilities.%20We%20develop%20a%0Acomprehensive%20benchmark%20focused%20on%20HubSpot%20CRM%20operations%20to%20evaluate%20ThorV2%0Aagainst%20leading%20models%20from%20OpenAI%20and%20Anthropic.%20Our%20results%20demonstrate%20that%0AThorV2%20outperforms%20existing%20models%20in%20accuracy%2C%20reliability%2C%20latency%2C%20and%20cost%0Aefficiency%20for%20both%20single%20and%20multi-API%20calling%20tasks.%20We%20also%20show%20that%0AThorV2%20is%20far%20more%20reliable%20and%20scales%20better%20to%20multistep%20tasks%20compared%20to%0Atraditional%20models.%20Our%20work%20offers%20the%20tantalizing%20possibility%20of%20more%0Aaccurate%20function-calling%20compared%20to%20today%27s%20best-performing%20models%20using%0Asignificantly%20smaller%20LLMs.%20These%20advancements%20have%20significant%20implications%0Afor%20the%20development%20of%20more%20capable%20AI%20assistants%20and%20the%20broader%20application%0Aof%20LLMs%20in%20real-world%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17950v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520Floworks%2520against%2520OpenAI%2520%2526%2520Anthropic%253A%2520A%2520Novel%2520Framework%2520for%250A%2520%2520Enhanced%2520LLM%2520Function%2520Calling%26entry.906535625%3DNirav%2520Bhan%2520and%2520Shival%2520Gupta%2520and%2520Sai%2520Manaswini%2520and%2520Ritik%2520Baba%2520and%2520Narun%2520Yadav%2520and%2520Hillori%2520Desai%2520and%2520Yash%2520Choudhary%2520and%2520Aman%2520Pawar%2520and%2520Sarthak%2520Shrivastava%2520and%2520Sudipta%2520Biswas%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520shown%2520remarkable%2520capabilities%2520in%2520various%250Adomains%252C%2520yet%2520their%2520economic%2520impact%2520has%2520been%2520limited%2520by%2520challenges%2520in%2520tool%2520use%250Aand%2520function%2520calling.%2520This%2520paper%2520introduces%2520ThorV2%252C%2520a%2520novel%2520architecture%2520that%250Asignificantly%2520enhances%2520LLMs%2527%2520function%2520calling%2520abilities.%2520We%2520develop%2520a%250Acomprehensive%2520benchmark%2520focused%2520on%2520HubSpot%2520CRM%2520operations%2520to%2520evaluate%2520ThorV2%250Aagainst%2520leading%2520models%2520from%2520OpenAI%2520and%2520Anthropic.%2520Our%2520results%2520demonstrate%2520that%250AThorV2%2520outperforms%2520existing%2520models%2520in%2520accuracy%252C%2520reliability%252C%2520latency%252C%2520and%2520cost%250Aefficiency%2520for%2520both%2520single%2520and%2520multi-API%2520calling%2520tasks.%2520We%2520also%2520show%2520that%250AThorV2%2520is%2520far%2520more%2520reliable%2520and%2520scales%2520better%2520to%2520multistep%2520tasks%2520compared%2520to%250Atraditional%2520models.%2520Our%2520work%2520offers%2520the%2520tantalizing%2520possibility%2520of%2520more%250Aaccurate%2520function-calling%2520compared%2520to%2520today%2527s%2520best-performing%2520models%2520using%250Asignificantly%2520smaller%2520LLMs.%2520These%2520advancements%2520have%2520significant%2520implications%250Afor%2520the%2520development%2520of%2520more%2520capable%2520AI%2520assistants%2520and%2520the%2520broader%2520application%250Aof%2520LLMs%2520in%2520real-world%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17950v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20Floworks%20against%20OpenAI%20%26%20Anthropic%3A%20A%20Novel%20Framework%20for%0A%20%20Enhanced%20LLM%20Function%20Calling&entry.906535625=Nirav%20Bhan%20and%20Shival%20Gupta%20and%20Sai%20Manaswini%20and%20Ritik%20Baba%20and%20Narun%20Yadav%20and%20Hillori%20Desai%20and%20Yash%20Choudhary%20and%20Aman%20Pawar%20and%20Sarthak%20Shrivastava%20and%20Sudipta%20Biswas&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20shown%20remarkable%20capabilities%20in%20various%0Adomains%2C%20yet%20their%20economic%20impact%20has%20been%20limited%20by%20challenges%20in%20tool%20use%0Aand%20function%20calling.%20This%20paper%20introduces%20ThorV2%2C%20a%20novel%20architecture%20that%0Asignificantly%20enhances%20LLMs%27%20function%20calling%20abilities.%20We%20develop%20a%0Acomprehensive%20benchmark%20focused%20on%20HubSpot%20CRM%20operations%20to%20evaluate%20ThorV2%0Aagainst%20leading%20models%20from%20OpenAI%20and%20Anthropic.%20Our%20results%20demonstrate%20that%0AThorV2%20outperforms%20existing%20models%20in%20accuracy%2C%20reliability%2C%20latency%2C%20and%20cost%0Aefficiency%20for%20both%20single%20and%20multi-API%20calling%20tasks.%20We%20also%20show%20that%0AThorV2%20is%20far%20more%20reliable%20and%20scales%20better%20to%20multistep%20tasks%20compared%20to%0Atraditional%20models.%20Our%20work%20offers%20the%20tantalizing%20possibility%20of%20more%0Aaccurate%20function-calling%20compared%20to%20today%27s%20best-performing%20models%20using%0Asignificantly%20smaller%20LLMs.%20These%20advancements%20have%20significant%20implications%0Afor%20the%20development%20of%20more%20capable%20AI%20assistants%20and%20the%20broader%20application%0Aof%20LLMs%20in%20real-world%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17950v1&entry.124074799=Read"},
{"title": "Do LLMs Have Distinct and Consistent Personality? TRAIT: Personality\n  Testset designed for LLMs with Psychometrics", "author": "Seungbeen Lee and Seungwon Lim and Seungju Han and Giyeong Oh and Hyungjoo Chae and Jiwan Chung and Minju Kim and Beong-woo Kwak and Yeonsoo Lee and Dongha Lee and Jinyoung Yeo and Youngjae Yu", "abstract": "  Recent advancements in Large Language Models (LLMs) have led to their\nadaptation in various domains as conversational agents. We wonder: can\npersonality tests be applied to these agents to analyze their behavior, similar\nto humans? We introduce TRAIT, a new benchmark consisting of 8K multi-choice\nquestions designed to assess the personality of LLMs. TRAIT is built on two\npsychometrically validated small human questionnaires, Big Five Inventory (BFI)\nand Short Dark Triad (SD-3), enhanced with the ATOMIC-10X knowledge graph to a\nvariety of real-world scenarios. TRAIT also outperforms existing personality\ntests for LLMs in terms of reliability and validity, achieving the highest\nscores across four key metrics: Content Validity, Internal Validity, Refusal\nRate, and Reliability. Using TRAIT, we reveal two notable insights into\npersonalities of LLMs: 1) LLMs exhibit distinct and consistent personality,\nwhich is highly influenced by their training data (e.g., data used for\nalignment tuning), and 2) current prompting techniques have limited\neffectiveness in eliciting certain traits, such as high psychopathy or low\nconscientiousness, suggesting the need for further research in this direction.\n", "link": "http://arxiv.org/abs/2406.14703v2", "date": "2024-10-23", "relevancy": 1.8941, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3947}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3709}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3709}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20LLMs%20Have%20Distinct%20and%20Consistent%20Personality%3F%20TRAIT%3A%20Personality%0A%20%20Testset%20designed%20for%20LLMs%20with%20Psychometrics&body=Title%3A%20Do%20LLMs%20Have%20Distinct%20and%20Consistent%20Personality%3F%20TRAIT%3A%20Personality%0A%20%20Testset%20designed%20for%20LLMs%20with%20Psychometrics%0AAuthor%3A%20Seungbeen%20Lee%20and%20Seungwon%20Lim%20and%20Seungju%20Han%20and%20Giyeong%20Oh%20and%20Hyungjoo%20Chae%20and%20Jiwan%20Chung%20and%20Minju%20Kim%20and%20Beong-woo%20Kwak%20and%20Yeonsoo%20Lee%20and%20Dongha%20Lee%20and%20Jinyoung%20Yeo%20and%20Youngjae%20Yu%0AAbstract%3A%20%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20led%20to%20their%0Aadaptation%20in%20various%20domains%20as%20conversational%20agents.%20We%20wonder%3A%20can%0Apersonality%20tests%20be%20applied%20to%20these%20agents%20to%20analyze%20their%20behavior%2C%20similar%0Ato%20humans%3F%20We%20introduce%20TRAIT%2C%20a%20new%20benchmark%20consisting%20of%208K%20multi-choice%0Aquestions%20designed%20to%20assess%20the%20personality%20of%20LLMs.%20TRAIT%20is%20built%20on%20two%0Apsychometrically%20validated%20small%20human%20questionnaires%2C%20Big%20Five%20Inventory%20%28BFI%29%0Aand%20Short%20Dark%20Triad%20%28SD-3%29%2C%20enhanced%20with%20the%20ATOMIC-10X%20knowledge%20graph%20to%20a%0Avariety%20of%20real-world%20scenarios.%20TRAIT%20also%20outperforms%20existing%20personality%0Atests%20for%20LLMs%20in%20terms%20of%20reliability%20and%20validity%2C%20achieving%20the%20highest%0Ascores%20across%20four%20key%20metrics%3A%20Content%20Validity%2C%20Internal%20Validity%2C%20Refusal%0ARate%2C%20and%20Reliability.%20Using%20TRAIT%2C%20we%20reveal%20two%20notable%20insights%20into%0Apersonalities%20of%20LLMs%3A%201%29%20LLMs%20exhibit%20distinct%20and%20consistent%20personality%2C%0Awhich%20is%20highly%20influenced%20by%20their%20training%20data%20%28e.g.%2C%20data%20used%20for%0Aalignment%20tuning%29%2C%20and%202%29%20current%20prompting%20techniques%20have%20limited%0Aeffectiveness%20in%20eliciting%20certain%20traits%2C%20such%20as%20high%20psychopathy%20or%20low%0Aconscientiousness%2C%20suggesting%20the%20need%20for%20further%20research%20in%20this%20direction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14703v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520LLMs%2520Have%2520Distinct%2520and%2520Consistent%2520Personality%253F%2520TRAIT%253A%2520Personality%250A%2520%2520Testset%2520designed%2520for%2520LLMs%2520with%2520Psychometrics%26entry.906535625%3DSeungbeen%2520Lee%2520and%2520Seungwon%2520Lim%2520and%2520Seungju%2520Han%2520and%2520Giyeong%2520Oh%2520and%2520Hyungjoo%2520Chae%2520and%2520Jiwan%2520Chung%2520and%2520Minju%2520Kim%2520and%2520Beong-woo%2520Kwak%2520and%2520Yeonsoo%2520Lee%2520and%2520Dongha%2520Lee%2520and%2520Jinyoung%2520Yeo%2520and%2520Youngjae%2520Yu%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520led%2520to%2520their%250Aadaptation%2520in%2520various%2520domains%2520as%2520conversational%2520agents.%2520We%2520wonder%253A%2520can%250Apersonality%2520tests%2520be%2520applied%2520to%2520these%2520agents%2520to%2520analyze%2520their%2520behavior%252C%2520similar%250Ato%2520humans%253F%2520We%2520introduce%2520TRAIT%252C%2520a%2520new%2520benchmark%2520consisting%2520of%25208K%2520multi-choice%250Aquestions%2520designed%2520to%2520assess%2520the%2520personality%2520of%2520LLMs.%2520TRAIT%2520is%2520built%2520on%2520two%250Apsychometrically%2520validated%2520small%2520human%2520questionnaires%252C%2520Big%2520Five%2520Inventory%2520%2528BFI%2529%250Aand%2520Short%2520Dark%2520Triad%2520%2528SD-3%2529%252C%2520enhanced%2520with%2520the%2520ATOMIC-10X%2520knowledge%2520graph%2520to%2520a%250Avariety%2520of%2520real-world%2520scenarios.%2520TRAIT%2520also%2520outperforms%2520existing%2520personality%250Atests%2520for%2520LLMs%2520in%2520terms%2520of%2520reliability%2520and%2520validity%252C%2520achieving%2520the%2520highest%250Ascores%2520across%2520four%2520key%2520metrics%253A%2520Content%2520Validity%252C%2520Internal%2520Validity%252C%2520Refusal%250ARate%252C%2520and%2520Reliability.%2520Using%2520TRAIT%252C%2520we%2520reveal%2520two%2520notable%2520insights%2520into%250Apersonalities%2520of%2520LLMs%253A%25201%2529%2520LLMs%2520exhibit%2520distinct%2520and%2520consistent%2520personality%252C%250Awhich%2520is%2520highly%2520influenced%2520by%2520their%2520training%2520data%2520%2528e.g.%252C%2520data%2520used%2520for%250Aalignment%2520tuning%2529%252C%2520and%25202%2529%2520current%2520prompting%2520techniques%2520have%2520limited%250Aeffectiveness%2520in%2520eliciting%2520certain%2520traits%252C%2520such%2520as%2520high%2520psychopathy%2520or%2520low%250Aconscientiousness%252C%2520suggesting%2520the%2520need%2520for%2520further%2520research%2520in%2520this%2520direction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14703v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20LLMs%20Have%20Distinct%20and%20Consistent%20Personality%3F%20TRAIT%3A%20Personality%0A%20%20Testset%20designed%20for%20LLMs%20with%20Psychometrics&entry.906535625=Seungbeen%20Lee%20and%20Seungwon%20Lim%20and%20Seungju%20Han%20and%20Giyeong%20Oh%20and%20Hyungjoo%20Chae%20and%20Jiwan%20Chung%20and%20Minju%20Kim%20and%20Beong-woo%20Kwak%20and%20Yeonsoo%20Lee%20and%20Dongha%20Lee%20and%20Jinyoung%20Yeo%20and%20Youngjae%20Yu&entry.1292438233=%20%20Recent%20advancements%20in%20Large%20Language%20Models%20%28LLMs%29%20have%20led%20to%20their%0Aadaptation%20in%20various%20domains%20as%20conversational%20agents.%20We%20wonder%3A%20can%0Apersonality%20tests%20be%20applied%20to%20these%20agents%20to%20analyze%20their%20behavior%2C%20similar%0Ato%20humans%3F%20We%20introduce%20TRAIT%2C%20a%20new%20benchmark%20consisting%20of%208K%20multi-choice%0Aquestions%20designed%20to%20assess%20the%20personality%20of%20LLMs.%20TRAIT%20is%20built%20on%20two%0Apsychometrically%20validated%20small%20human%20questionnaires%2C%20Big%20Five%20Inventory%20%28BFI%29%0Aand%20Short%20Dark%20Triad%20%28SD-3%29%2C%20enhanced%20with%20the%20ATOMIC-10X%20knowledge%20graph%20to%20a%0Avariety%20of%20real-world%20scenarios.%20TRAIT%20also%20outperforms%20existing%20personality%0Atests%20for%20LLMs%20in%20terms%20of%20reliability%20and%20validity%2C%20achieving%20the%20highest%0Ascores%20across%20four%20key%20metrics%3A%20Content%20Validity%2C%20Internal%20Validity%2C%20Refusal%0ARate%2C%20and%20Reliability.%20Using%20TRAIT%2C%20we%20reveal%20two%20notable%20insights%20into%0Apersonalities%20of%20LLMs%3A%201%29%20LLMs%20exhibit%20distinct%20and%20consistent%20personality%2C%0Awhich%20is%20highly%20influenced%20by%20their%20training%20data%20%28e.g.%2C%20data%20used%20for%0Aalignment%20tuning%29%2C%20and%202%29%20current%20prompting%20techniques%20have%20limited%0Aeffectiveness%20in%20eliciting%20certain%20traits%2C%20such%20as%20high%20psychopathy%20or%20low%0Aconscientiousness%2C%20suggesting%20the%20need%20for%20further%20research%20in%20this%20direction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14703v2&entry.124074799=Read"},
{"title": "Conquering the Communication Constraints to Enable Large Pre-Trained\n  Models in Federated Learning", "author": "Guangyu Sun and Umar Khalid and Matias Mendieta and Taojiannan Yang and Pu Wang and Minwoo Lee and Chen Chen", "abstract": "  Federated learning (FL) has emerged as a promising paradigm for enabling the\ncollaborative training of models without centralized access to the raw data on\nlocal devices. In the typical FL paradigm (e.g., FedAvg), model weights are\nsent to and from the server each round to participating clients. Recently, the\nuse of small pre-trained models has been shown effective in federated learning\noptimization and improving convergence. However, recent state-of-the-art\npre-trained models are getting more capable but also have more parameters. In\nconventional FL, sharing the enormous model weights can quickly put a massive\ncommunication burden on the system, especially if more capable models are\nemployed. Can we find a solution to enable those strong and readily-available\npre-trained models in FL to achieve excellent performance while simultaneously\nreducing the communication burden? To this end, we investigate the use of\nparameter-efficient fine-tuning in federated learning and thus introduce a new\nframework: FedPEFT. Specifically, we systemically evaluate the performance of\nFedPEFT across a variety of client stability, data distribution, and\ndifferential privacy settings. By only locally tuning and globally sharing a\nsmall portion of the model weights, significant reductions in the total\ncommunication overhead can be achieved while maintaining competitive or even\nbetter performance in a wide range of federated learning scenarios, providing\ninsight into a new paradigm for practical and effective federated systems.\n", "link": "http://arxiv.org/abs/2210.01708v4", "date": "2024-10-23", "relevancy": 1.4385, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.508}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4763}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4592}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Conquering%20the%20Communication%20Constraints%20to%20Enable%20Large%20Pre-Trained%0A%20%20Models%20in%20Federated%20Learning&body=Title%3A%20Conquering%20the%20Communication%20Constraints%20to%20Enable%20Large%20Pre-Trained%0A%20%20Models%20in%20Federated%20Learning%0AAuthor%3A%20Guangyu%20Sun%20and%20Umar%20Khalid%20and%20Matias%20Mendieta%20and%20Taojiannan%20Yang%20and%20Pu%20Wang%20and%20Minwoo%20Lee%20and%20Chen%20Chen%0AAbstract%3A%20%20%20Federated%20learning%20%28FL%29%20has%20emerged%20as%20a%20promising%20paradigm%20for%20enabling%20the%0Acollaborative%20training%20of%20models%20without%20centralized%20access%20to%20the%20raw%20data%20on%0Alocal%20devices.%20In%20the%20typical%20FL%20paradigm%20%28e.g.%2C%20FedAvg%29%2C%20model%20weights%20are%0Asent%20to%20and%20from%20the%20server%20each%20round%20to%20participating%20clients.%20Recently%2C%20the%0Ause%20of%20small%20pre-trained%20models%20has%20been%20shown%20effective%20in%20federated%20learning%0Aoptimization%20and%20improving%20convergence.%20However%2C%20recent%20state-of-the-art%0Apre-trained%20models%20are%20getting%20more%20capable%20but%20also%20have%20more%20parameters.%20In%0Aconventional%20FL%2C%20sharing%20the%20enormous%20model%20weights%20can%20quickly%20put%20a%20massive%0Acommunication%20burden%20on%20the%20system%2C%20especially%20if%20more%20capable%20models%20are%0Aemployed.%20Can%20we%20find%20a%20solution%20to%20enable%20those%20strong%20and%20readily-available%0Apre-trained%20models%20in%20FL%20to%20achieve%20excellent%20performance%20while%20simultaneously%0Areducing%20the%20communication%20burden%3F%20To%20this%20end%2C%20we%20investigate%20the%20use%20of%0Aparameter-efficient%20fine-tuning%20in%20federated%20learning%20and%20thus%20introduce%20a%20new%0Aframework%3A%20FedPEFT.%20Specifically%2C%20we%20systemically%20evaluate%20the%20performance%20of%0AFedPEFT%20across%20a%20variety%20of%20client%20stability%2C%20data%20distribution%2C%20and%0Adifferential%20privacy%20settings.%20By%20only%20locally%20tuning%20and%20globally%20sharing%20a%0Asmall%20portion%20of%20the%20model%20weights%2C%20significant%20reductions%20in%20the%20total%0Acommunication%20overhead%20can%20be%20achieved%20while%20maintaining%20competitive%20or%20even%0Abetter%20performance%20in%20a%20wide%20range%20of%20federated%20learning%20scenarios%2C%20providing%0Ainsight%20into%20a%20new%20paradigm%20for%20practical%20and%20effective%20federated%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2210.01708v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConquering%2520the%2520Communication%2520Constraints%2520to%2520Enable%2520Large%2520Pre-Trained%250A%2520%2520Models%2520in%2520Federated%2520Learning%26entry.906535625%3DGuangyu%2520Sun%2520and%2520Umar%2520Khalid%2520and%2520Matias%2520Mendieta%2520and%2520Taojiannan%2520Yang%2520and%2520Pu%2520Wang%2520and%2520Minwoo%2520Lee%2520and%2520Chen%2520Chen%26entry.1292438233%3D%2520%2520Federated%2520learning%2520%2528FL%2529%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%2520for%2520enabling%2520the%250Acollaborative%2520training%2520of%2520models%2520without%2520centralized%2520access%2520to%2520the%2520raw%2520data%2520on%250Alocal%2520devices.%2520In%2520the%2520typical%2520FL%2520paradigm%2520%2528e.g.%252C%2520FedAvg%2529%252C%2520model%2520weights%2520are%250Asent%2520to%2520and%2520from%2520the%2520server%2520each%2520round%2520to%2520participating%2520clients.%2520Recently%252C%2520the%250Ause%2520of%2520small%2520pre-trained%2520models%2520has%2520been%2520shown%2520effective%2520in%2520federated%2520learning%250Aoptimization%2520and%2520improving%2520convergence.%2520However%252C%2520recent%2520state-of-the-art%250Apre-trained%2520models%2520are%2520getting%2520more%2520capable%2520but%2520also%2520have%2520more%2520parameters.%2520In%250Aconventional%2520FL%252C%2520sharing%2520the%2520enormous%2520model%2520weights%2520can%2520quickly%2520put%2520a%2520massive%250Acommunication%2520burden%2520on%2520the%2520system%252C%2520especially%2520if%2520more%2520capable%2520models%2520are%250Aemployed.%2520Can%2520we%2520find%2520a%2520solution%2520to%2520enable%2520those%2520strong%2520and%2520readily-available%250Apre-trained%2520models%2520in%2520FL%2520to%2520achieve%2520excellent%2520performance%2520while%2520simultaneously%250Areducing%2520the%2520communication%2520burden%253F%2520To%2520this%2520end%252C%2520we%2520investigate%2520the%2520use%2520of%250Aparameter-efficient%2520fine-tuning%2520in%2520federated%2520learning%2520and%2520thus%2520introduce%2520a%2520new%250Aframework%253A%2520FedPEFT.%2520Specifically%252C%2520we%2520systemically%2520evaluate%2520the%2520performance%2520of%250AFedPEFT%2520across%2520a%2520variety%2520of%2520client%2520stability%252C%2520data%2520distribution%252C%2520and%250Adifferential%2520privacy%2520settings.%2520By%2520only%2520locally%2520tuning%2520and%2520globally%2520sharing%2520a%250Asmall%2520portion%2520of%2520the%2520model%2520weights%252C%2520significant%2520reductions%2520in%2520the%2520total%250Acommunication%2520overhead%2520can%2520be%2520achieved%2520while%2520maintaining%2520competitive%2520or%2520even%250Abetter%2520performance%2520in%2520a%2520wide%2520range%2520of%2520federated%2520learning%2520scenarios%252C%2520providing%250Ainsight%2520into%2520a%2520new%2520paradigm%2520for%2520practical%2520and%2520effective%2520federated%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2210.01708v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Conquering%20the%20Communication%20Constraints%20to%20Enable%20Large%20Pre-Trained%0A%20%20Models%20in%20Federated%20Learning&entry.906535625=Guangyu%20Sun%20and%20Umar%20Khalid%20and%20Matias%20Mendieta%20and%20Taojiannan%20Yang%20and%20Pu%20Wang%20and%20Minwoo%20Lee%20and%20Chen%20Chen&entry.1292438233=%20%20Federated%20learning%20%28FL%29%20has%20emerged%20as%20a%20promising%20paradigm%20for%20enabling%20the%0Acollaborative%20training%20of%20models%20without%20centralized%20access%20to%20the%20raw%20data%20on%0Alocal%20devices.%20In%20the%20typical%20FL%20paradigm%20%28e.g.%2C%20FedAvg%29%2C%20model%20weights%20are%0Asent%20to%20and%20from%20the%20server%20each%20round%20to%20participating%20clients.%20Recently%2C%20the%0Ause%20of%20small%20pre-trained%20models%20has%20been%20shown%20effective%20in%20federated%20learning%0Aoptimization%20and%20improving%20convergence.%20However%2C%20recent%20state-of-the-art%0Apre-trained%20models%20are%20getting%20more%20capable%20but%20also%20have%20more%20parameters.%20In%0Aconventional%20FL%2C%20sharing%20the%20enormous%20model%20weights%20can%20quickly%20put%20a%20massive%0Acommunication%20burden%20on%20the%20system%2C%20especially%20if%20more%20capable%20models%20are%0Aemployed.%20Can%20we%20find%20a%20solution%20to%20enable%20those%20strong%20and%20readily-available%0Apre-trained%20models%20in%20FL%20to%20achieve%20excellent%20performance%20while%20simultaneously%0Areducing%20the%20communication%20burden%3F%20To%20this%20end%2C%20we%20investigate%20the%20use%20of%0Aparameter-efficient%20fine-tuning%20in%20federated%20learning%20and%20thus%20introduce%20a%20new%0Aframework%3A%20FedPEFT.%20Specifically%2C%20we%20systemically%20evaluate%20the%20performance%20of%0AFedPEFT%20across%20a%20variety%20of%20client%20stability%2C%20data%20distribution%2C%20and%0Adifferential%20privacy%20settings.%20By%20only%20locally%20tuning%20and%20globally%20sharing%20a%0Asmall%20portion%20of%20the%20model%20weights%2C%20significant%20reductions%20in%20the%20total%0Acommunication%20overhead%20can%20be%20achieved%20while%20maintaining%20competitive%20or%20even%0Abetter%20performance%20in%20a%20wide%20range%20of%20federated%20learning%20scenarios%2C%20providing%0Ainsight%20into%20a%20new%20paradigm%20for%20practical%20and%20effective%20federated%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2210.01708v4&entry.124074799=Read"},
{"title": "Multi-Layered Safety of Redundant Robot Manipulators via Task-Oriented\n  Planning and Control", "author": "Xinyu Jia and Wenxin Wang and Jun Yang and Yongping Pan and Haoyong Yu", "abstract": "  Ensuring safety is crucial to promote the application of robot manipulators\nin open workspace. Factors such as sensor errors or unpredictable collisions\nmake the environment full of uncertainties. In this work, we investigate these\npotential safety challenges on redundant robot manipulators, and propose a\ntask-oriented planning and control framework to achieve multi-layered safety\nwhile maintaining efficient task execution. Our approach consists of two main\nparts: a task-oriented trajectory planner based on multiple-shooting model\npredictive control method, and a torque controller that allows safe and\nefficient collision reaction using only proprioceptive data. Through extensive\nsimulations and real-hardware experiments, we demonstrate that the proposed\nframework can effectively handle uncertain static or dynamic obstacles, and\nperform disturbance resistance in manipulation tasks when unforeseen contacts\noccur. All code will be open-sourced to benefit the community.\n", "link": "http://arxiv.org/abs/2410.17742v1", "date": "2024-10-23", "relevancy": 1.6969, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5981}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5665}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Layered%20Safety%20of%20Redundant%20Robot%20Manipulators%20via%20Task-Oriented%0A%20%20Planning%20and%20Control&body=Title%3A%20Multi-Layered%20Safety%20of%20Redundant%20Robot%20Manipulators%20via%20Task-Oriented%0A%20%20Planning%20and%20Control%0AAuthor%3A%20Xinyu%20Jia%20and%20Wenxin%20Wang%20and%20Jun%20Yang%20and%20Yongping%20Pan%20and%20Haoyong%20Yu%0AAbstract%3A%20%20%20Ensuring%20safety%20is%20crucial%20to%20promote%20the%20application%20of%20robot%20manipulators%0Ain%20open%20workspace.%20Factors%20such%20as%20sensor%20errors%20or%20unpredictable%20collisions%0Amake%20the%20environment%20full%20of%20uncertainties.%20In%20this%20work%2C%20we%20investigate%20these%0Apotential%20safety%20challenges%20on%20redundant%20robot%20manipulators%2C%20and%20propose%20a%0Atask-oriented%20planning%20and%20control%20framework%20to%20achieve%20multi-layered%20safety%0Awhile%20maintaining%20efficient%20task%20execution.%20Our%20approach%20consists%20of%20two%20main%0Aparts%3A%20a%20task-oriented%20trajectory%20planner%20based%20on%20multiple-shooting%20model%0Apredictive%20control%20method%2C%20and%20a%20torque%20controller%20that%20allows%20safe%20and%0Aefficient%20collision%20reaction%20using%20only%20proprioceptive%20data.%20Through%20extensive%0Asimulations%20and%20real-hardware%20experiments%2C%20we%20demonstrate%20that%20the%20proposed%0Aframework%20can%20effectively%20handle%20uncertain%20static%20or%20dynamic%20obstacles%2C%20and%0Aperform%20disturbance%20resistance%20in%20manipulation%20tasks%20when%20unforeseen%20contacts%0Aoccur.%20All%20code%20will%20be%20open-sourced%20to%20benefit%20the%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17742v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Layered%2520Safety%2520of%2520Redundant%2520Robot%2520Manipulators%2520via%2520Task-Oriented%250A%2520%2520Planning%2520and%2520Control%26entry.906535625%3DXinyu%2520Jia%2520and%2520Wenxin%2520Wang%2520and%2520Jun%2520Yang%2520and%2520Yongping%2520Pan%2520and%2520Haoyong%2520Yu%26entry.1292438233%3D%2520%2520Ensuring%2520safety%2520is%2520crucial%2520to%2520promote%2520the%2520application%2520of%2520robot%2520manipulators%250Ain%2520open%2520workspace.%2520Factors%2520such%2520as%2520sensor%2520errors%2520or%2520unpredictable%2520collisions%250Amake%2520the%2520environment%2520full%2520of%2520uncertainties.%2520In%2520this%2520work%252C%2520we%2520investigate%2520these%250Apotential%2520safety%2520challenges%2520on%2520redundant%2520robot%2520manipulators%252C%2520and%2520propose%2520a%250Atask-oriented%2520planning%2520and%2520control%2520framework%2520to%2520achieve%2520multi-layered%2520safety%250Awhile%2520maintaining%2520efficient%2520task%2520execution.%2520Our%2520approach%2520consists%2520of%2520two%2520main%250Aparts%253A%2520a%2520task-oriented%2520trajectory%2520planner%2520based%2520on%2520multiple-shooting%2520model%250Apredictive%2520control%2520method%252C%2520and%2520a%2520torque%2520controller%2520that%2520allows%2520safe%2520and%250Aefficient%2520collision%2520reaction%2520using%2520only%2520proprioceptive%2520data.%2520Through%2520extensive%250Asimulations%2520and%2520real-hardware%2520experiments%252C%2520we%2520demonstrate%2520that%2520the%2520proposed%250Aframework%2520can%2520effectively%2520handle%2520uncertain%2520static%2520or%2520dynamic%2520obstacles%252C%2520and%250Aperform%2520disturbance%2520resistance%2520in%2520manipulation%2520tasks%2520when%2520unforeseen%2520contacts%250Aoccur.%2520All%2520code%2520will%2520be%2520open-sourced%2520to%2520benefit%2520the%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17742v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Layered%20Safety%20of%20Redundant%20Robot%20Manipulators%20via%20Task-Oriented%0A%20%20Planning%20and%20Control&entry.906535625=Xinyu%20Jia%20and%20Wenxin%20Wang%20and%20Jun%20Yang%20and%20Yongping%20Pan%20and%20Haoyong%20Yu&entry.1292438233=%20%20Ensuring%20safety%20is%20crucial%20to%20promote%20the%20application%20of%20robot%20manipulators%0Ain%20open%20workspace.%20Factors%20such%20as%20sensor%20errors%20or%20unpredictable%20collisions%0Amake%20the%20environment%20full%20of%20uncertainties.%20In%20this%20work%2C%20we%20investigate%20these%0Apotential%20safety%20challenges%20on%20redundant%20robot%20manipulators%2C%20and%20propose%20a%0Atask-oriented%20planning%20and%20control%20framework%20to%20achieve%20multi-layered%20safety%0Awhile%20maintaining%20efficient%20task%20execution.%20Our%20approach%20consists%20of%20two%20main%0Aparts%3A%20a%20task-oriented%20trajectory%20planner%20based%20on%20multiple-shooting%20model%0Apredictive%20control%20method%2C%20and%20a%20torque%20controller%20that%20allows%20safe%20and%0Aefficient%20collision%20reaction%20using%20only%20proprioceptive%20data.%20Through%20extensive%0Asimulations%20and%20real-hardware%20experiments%2C%20we%20demonstrate%20that%20the%20proposed%0Aframework%20can%20effectively%20handle%20uncertain%20static%20or%20dynamic%20obstacles%2C%20and%0Aperform%20disturbance%20resistance%20in%20manipulation%20tasks%20when%20unforeseen%20contacts%0Aoccur.%20All%20code%20will%20be%20open-sourced%20to%20benefit%20the%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17742v1&entry.124074799=Read"},
{"title": "Optimal Streaming Algorithms for Multi-Armed Bandits", "author": "Tianyuan Jin and Keke Huang and Jing Tang and Xiaokui Xiao", "abstract": "  This paper studies two variants of the best arm identification (BAI) problem\nunder the streaming model, where we have a stream of $n$ arms with reward\ndistributions supported on $[0,1]$ with unknown means. The arms in the stream\nare arriving one by one, and the algorithm cannot access an arm unless it is\nstored in a limited size memory.\n  We first study the streaming \\eps-$top$-$k$ arms identification problem,\nwhich asks for $k$ arms whose reward means are lower than that of the $k$-th\nbest arm by at most $\\eps$ with probability at least $1-\\delta$. For general\n$\\eps \\in (0,1)$, the existing solution for this problem assumes $k = 1$ and\nachieves the optimal sample complexity $O(\\frac{n}{\\eps^2} \\log\n\\frac{1}{\\delta})$ using $O(\\log^*(n))$ ($\\log^*(n)$ equals the number of times\nthat we need to apply the logarithm function on $n$ before the results is no\nmore than 1.) memory and a single pass of the stream. We propose an algorithm\nthat works for any $k$ and achieves the optimal sample complexity\n$O(\\frac{n}{\\eps^2} \\log\\frac{k}{\\delta})$ using a single-arm memory and a\nsingle pass of the stream.\n  Second, we study the streaming BAI problem, where the objective is to\nidentify the arm with the maximum reward mean with at least $1-\\delta$\nprobability, using a single-arm memory and as few passes of the input stream as\npossible. We present a single-arm-memory algorithm that achieves a near\ninstance-dependent optimal sample complexity within $O(\\log \\Delta_2^{-1})$\npasses, where $\\Delta_2$ is the gap between the mean of the best arm and that\nof the second best arm.\n", "link": "http://arxiv.org/abs/2410.17835v1", "date": "2024-10-23", "relevancy": 1.3224, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.459}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.434}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4021}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Streaming%20Algorithms%20for%20Multi-Armed%20Bandits&body=Title%3A%20Optimal%20Streaming%20Algorithms%20for%20Multi-Armed%20Bandits%0AAuthor%3A%20Tianyuan%20Jin%20and%20Keke%20Huang%20and%20Jing%20Tang%20and%20Xiaokui%20Xiao%0AAbstract%3A%20%20%20This%20paper%20studies%20two%20variants%20of%20the%20best%20arm%20identification%20%28BAI%29%20problem%0Aunder%20the%20streaming%20model%2C%20where%20we%20have%20a%20stream%20of%20%24n%24%20arms%20with%20reward%0Adistributions%20supported%20on%20%24%5B0%2C1%5D%24%20with%20unknown%20means.%20The%20arms%20in%20the%20stream%0Aare%20arriving%20one%20by%20one%2C%20and%20the%20algorithm%20cannot%20access%20an%20arm%20unless%20it%20is%0Astored%20in%20a%20limited%20size%20memory.%0A%20%20We%20first%20study%20the%20streaming%20%5Ceps-%24top%24-%24k%24%20arms%20identification%20problem%2C%0Awhich%20asks%20for%20%24k%24%20arms%20whose%20reward%20means%20are%20lower%20than%20that%20of%20the%20%24k%24-th%0Abest%20arm%20by%20at%20most%20%24%5Ceps%24%20with%20probability%20at%20least%20%241-%5Cdelta%24.%20For%20general%0A%24%5Ceps%20%5Cin%20%280%2C1%29%24%2C%20the%20existing%20solution%20for%20this%20problem%20assumes%20%24k%20%3D%201%24%20and%0Aachieves%20the%20optimal%20sample%20complexity%20%24O%28%5Cfrac%7Bn%7D%7B%5Ceps%5E2%7D%20%5Clog%0A%5Cfrac%7B1%7D%7B%5Cdelta%7D%29%24%20using%20%24O%28%5Clog%5E%2A%28n%29%29%24%20%28%24%5Clog%5E%2A%28n%29%24%20equals%20the%20number%20of%20times%0Athat%20we%20need%20to%20apply%20the%20logarithm%20function%20on%20%24n%24%20before%20the%20results%20is%20no%0Amore%20than%201.%29%20memory%20and%20a%20single%20pass%20of%20the%20stream.%20We%20propose%20an%20algorithm%0Athat%20works%20for%20any%20%24k%24%20and%20achieves%20the%20optimal%20sample%20complexity%0A%24O%28%5Cfrac%7Bn%7D%7B%5Ceps%5E2%7D%20%5Clog%5Cfrac%7Bk%7D%7B%5Cdelta%7D%29%24%20using%20a%20single-arm%20memory%20and%20a%0Asingle%20pass%20of%20the%20stream.%0A%20%20Second%2C%20we%20study%20the%20streaming%20BAI%20problem%2C%20where%20the%20objective%20is%20to%0Aidentify%20the%20arm%20with%20the%20maximum%20reward%20mean%20with%20at%20least%20%241-%5Cdelta%24%0Aprobability%2C%20using%20a%20single-arm%20memory%20and%20as%20few%20passes%20of%20the%20input%20stream%20as%0Apossible.%20We%20present%20a%20single-arm-memory%20algorithm%20that%20achieves%20a%20near%0Ainstance-dependent%20optimal%20sample%20complexity%20within%20%24O%28%5Clog%20%5CDelta_2%5E%7B-1%7D%29%24%0Apasses%2C%20where%20%24%5CDelta_2%24%20is%20the%20gap%20between%20the%20mean%20of%20the%20best%20arm%20and%20that%0Aof%20the%20second%20best%20arm.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17835v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Streaming%2520Algorithms%2520for%2520Multi-Armed%2520Bandits%26entry.906535625%3DTianyuan%2520Jin%2520and%2520Keke%2520Huang%2520and%2520Jing%2520Tang%2520and%2520Xiaokui%2520Xiao%26entry.1292438233%3D%2520%2520This%2520paper%2520studies%2520two%2520variants%2520of%2520the%2520best%2520arm%2520identification%2520%2528BAI%2529%2520problem%250Aunder%2520the%2520streaming%2520model%252C%2520where%2520we%2520have%2520a%2520stream%2520of%2520%2524n%2524%2520arms%2520with%2520reward%250Adistributions%2520supported%2520on%2520%2524%255B0%252C1%255D%2524%2520with%2520unknown%2520means.%2520The%2520arms%2520in%2520the%2520stream%250Aare%2520arriving%2520one%2520by%2520one%252C%2520and%2520the%2520algorithm%2520cannot%2520access%2520an%2520arm%2520unless%2520it%2520is%250Astored%2520in%2520a%2520limited%2520size%2520memory.%250A%2520%2520We%2520first%2520study%2520the%2520streaming%2520%255Ceps-%2524top%2524-%2524k%2524%2520arms%2520identification%2520problem%252C%250Awhich%2520asks%2520for%2520%2524k%2524%2520arms%2520whose%2520reward%2520means%2520are%2520lower%2520than%2520that%2520of%2520the%2520%2524k%2524-th%250Abest%2520arm%2520by%2520at%2520most%2520%2524%255Ceps%2524%2520with%2520probability%2520at%2520least%2520%25241-%255Cdelta%2524.%2520For%2520general%250A%2524%255Ceps%2520%255Cin%2520%25280%252C1%2529%2524%252C%2520the%2520existing%2520solution%2520for%2520this%2520problem%2520assumes%2520%2524k%2520%253D%25201%2524%2520and%250Aachieves%2520the%2520optimal%2520sample%2520complexity%2520%2524O%2528%255Cfrac%257Bn%257D%257B%255Ceps%255E2%257D%2520%255Clog%250A%255Cfrac%257B1%257D%257B%255Cdelta%257D%2529%2524%2520using%2520%2524O%2528%255Clog%255E%252A%2528n%2529%2529%2524%2520%2528%2524%255Clog%255E%252A%2528n%2529%2524%2520equals%2520the%2520number%2520of%2520times%250Athat%2520we%2520need%2520to%2520apply%2520the%2520logarithm%2520function%2520on%2520%2524n%2524%2520before%2520the%2520results%2520is%2520no%250Amore%2520than%25201.%2529%2520memory%2520and%2520a%2520single%2520pass%2520of%2520the%2520stream.%2520We%2520propose%2520an%2520algorithm%250Athat%2520works%2520for%2520any%2520%2524k%2524%2520and%2520achieves%2520the%2520optimal%2520sample%2520complexity%250A%2524O%2528%255Cfrac%257Bn%257D%257B%255Ceps%255E2%257D%2520%255Clog%255Cfrac%257Bk%257D%257B%255Cdelta%257D%2529%2524%2520using%2520a%2520single-arm%2520memory%2520and%2520a%250Asingle%2520pass%2520of%2520the%2520stream.%250A%2520%2520Second%252C%2520we%2520study%2520the%2520streaming%2520BAI%2520problem%252C%2520where%2520the%2520objective%2520is%2520to%250Aidentify%2520the%2520arm%2520with%2520the%2520maximum%2520reward%2520mean%2520with%2520at%2520least%2520%25241-%255Cdelta%2524%250Aprobability%252C%2520using%2520a%2520single-arm%2520memory%2520and%2520as%2520few%2520passes%2520of%2520the%2520input%2520stream%2520as%250Apossible.%2520We%2520present%2520a%2520single-arm-memory%2520algorithm%2520that%2520achieves%2520a%2520near%250Ainstance-dependent%2520optimal%2520sample%2520complexity%2520within%2520%2524O%2528%255Clog%2520%255CDelta_2%255E%257B-1%257D%2529%2524%250Apasses%252C%2520where%2520%2524%255CDelta_2%2524%2520is%2520the%2520gap%2520between%2520the%2520mean%2520of%2520the%2520best%2520arm%2520and%2520that%250Aof%2520the%2520second%2520best%2520arm.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17835v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Streaming%20Algorithms%20for%20Multi-Armed%20Bandits&entry.906535625=Tianyuan%20Jin%20and%20Keke%20Huang%20and%20Jing%20Tang%20and%20Xiaokui%20Xiao&entry.1292438233=%20%20This%20paper%20studies%20two%20variants%20of%20the%20best%20arm%20identification%20%28BAI%29%20problem%0Aunder%20the%20streaming%20model%2C%20where%20we%20have%20a%20stream%20of%20%24n%24%20arms%20with%20reward%0Adistributions%20supported%20on%20%24%5B0%2C1%5D%24%20with%20unknown%20means.%20The%20arms%20in%20the%20stream%0Aare%20arriving%20one%20by%20one%2C%20and%20the%20algorithm%20cannot%20access%20an%20arm%20unless%20it%20is%0Astored%20in%20a%20limited%20size%20memory.%0A%20%20We%20first%20study%20the%20streaming%20%5Ceps-%24top%24-%24k%24%20arms%20identification%20problem%2C%0Awhich%20asks%20for%20%24k%24%20arms%20whose%20reward%20means%20are%20lower%20than%20that%20of%20the%20%24k%24-th%0Abest%20arm%20by%20at%20most%20%24%5Ceps%24%20with%20probability%20at%20least%20%241-%5Cdelta%24.%20For%20general%0A%24%5Ceps%20%5Cin%20%280%2C1%29%24%2C%20the%20existing%20solution%20for%20this%20problem%20assumes%20%24k%20%3D%201%24%20and%0Aachieves%20the%20optimal%20sample%20complexity%20%24O%28%5Cfrac%7Bn%7D%7B%5Ceps%5E2%7D%20%5Clog%0A%5Cfrac%7B1%7D%7B%5Cdelta%7D%29%24%20using%20%24O%28%5Clog%5E%2A%28n%29%29%24%20%28%24%5Clog%5E%2A%28n%29%24%20equals%20the%20number%20of%20times%0Athat%20we%20need%20to%20apply%20the%20logarithm%20function%20on%20%24n%24%20before%20the%20results%20is%20no%0Amore%20than%201.%29%20memory%20and%20a%20single%20pass%20of%20the%20stream.%20We%20propose%20an%20algorithm%0Athat%20works%20for%20any%20%24k%24%20and%20achieves%20the%20optimal%20sample%20complexity%0A%24O%28%5Cfrac%7Bn%7D%7B%5Ceps%5E2%7D%20%5Clog%5Cfrac%7Bk%7D%7B%5Cdelta%7D%29%24%20using%20a%20single-arm%20memory%20and%20a%0Asingle%20pass%20of%20the%20stream.%0A%20%20Second%2C%20we%20study%20the%20streaming%20BAI%20problem%2C%20where%20the%20objective%20is%20to%0Aidentify%20the%20arm%20with%20the%20maximum%20reward%20mean%20with%20at%20least%20%241-%5Cdelta%24%0Aprobability%2C%20using%20a%20single-arm%20memory%20and%20as%20few%20passes%20of%20the%20input%20stream%20as%0Apossible.%20We%20present%20a%20single-arm-memory%20algorithm%20that%20achieves%20a%20near%0Ainstance-dependent%20optimal%20sample%20complexity%20within%20%24O%28%5Clog%20%5CDelta_2%5E%7B-1%7D%29%24%0Apasses%2C%20where%20%24%5CDelta_2%24%20is%20the%20gap%20between%20the%20mean%20of%20the%20best%20arm%20and%20that%0Aof%20the%20second%20best%20arm.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17835v1&entry.124074799=Read"},
{"title": "Is the GPU Half-Empty or Half-Full? Practical Scheduling Techniques for\n  LLMs", "author": "Ferdi Kossmann and Bruce Fontaine and Daya Khudia and Michael Cafarella and Samuel Madden", "abstract": "  Serving systems for Large Language Models (LLMs) improve throughput by\nprocessing several requests concurrently. However, multiplexing hardware\nresources between concurrent requests involves non-trivial scheduling\ndecisions. Practical serving systems typically implement these decisions at two\nlevels: First, a load balancer routes requests to different servers which each\nhold a replica of the LLM. Then, on each server, an engine-level scheduler\ndecides when to run a request, or when to queue or preempt it. Improved\nscheduling policies may benefit a wide range of LLM deployments and can often\nbe implemented as \"drop-in replacements\" to a system's current policy. In this\nwork, we survey scheduling techniques from the literature and from practical\nserving systems. We find that schedulers from the literature often achieve good\nperformance but introduce significant complexity. In contrast, schedulers in\npractical deployments often leave easy performance gains on the table but are\neasy to implement, deploy and configure. This finding motivates us to introduce\ntwo new scheduling techniques, which are both easy to implement, and outperform\ncurrent techniques on production workload traces.\n", "link": "http://arxiv.org/abs/2410.17840v1", "date": "2024-10-23", "relevancy": 1.7165, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4529}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4249}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4239}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20the%20GPU%20Half-Empty%20or%20Half-Full%3F%20Practical%20Scheduling%20Techniques%20for%0A%20%20LLMs&body=Title%3A%20Is%20the%20GPU%20Half-Empty%20or%20Half-Full%3F%20Practical%20Scheduling%20Techniques%20for%0A%20%20LLMs%0AAuthor%3A%20Ferdi%20Kossmann%20and%20Bruce%20Fontaine%20and%20Daya%20Khudia%20and%20Michael%20Cafarella%20and%20Samuel%20Madden%0AAbstract%3A%20%20%20Serving%20systems%20for%20Large%20Language%20Models%20%28LLMs%29%20improve%20throughput%20by%0Aprocessing%20several%20requests%20concurrently.%20However%2C%20multiplexing%20hardware%0Aresources%20between%20concurrent%20requests%20involves%20non-trivial%20scheduling%0Adecisions.%20Practical%20serving%20systems%20typically%20implement%20these%20decisions%20at%20two%0Alevels%3A%20First%2C%20a%20load%20balancer%20routes%20requests%20to%20different%20servers%20which%20each%0Ahold%20a%20replica%20of%20the%20LLM.%20Then%2C%20on%20each%20server%2C%20an%20engine-level%20scheduler%0Adecides%20when%20to%20run%20a%20request%2C%20or%20when%20to%20queue%20or%20preempt%20it.%20Improved%0Ascheduling%20policies%20may%20benefit%20a%20wide%20range%20of%20LLM%20deployments%20and%20can%20often%0Abe%20implemented%20as%20%22drop-in%20replacements%22%20to%20a%20system%27s%20current%20policy.%20In%20this%0Awork%2C%20we%20survey%20scheduling%20techniques%20from%20the%20literature%20and%20from%20practical%0Aserving%20systems.%20We%20find%20that%20schedulers%20from%20the%20literature%20often%20achieve%20good%0Aperformance%20but%20introduce%20significant%20complexity.%20In%20contrast%2C%20schedulers%20in%0Apractical%20deployments%20often%20leave%20easy%20performance%20gains%20on%20the%20table%20but%20are%0Aeasy%20to%20implement%2C%20deploy%20and%20configure.%20This%20finding%20motivates%20us%20to%20introduce%0Atwo%20new%20scheduling%20techniques%2C%20which%20are%20both%20easy%20to%20implement%2C%20and%20outperform%0Acurrent%20techniques%20on%20production%20workload%20traces.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17840v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520the%2520GPU%2520Half-Empty%2520or%2520Half-Full%253F%2520Practical%2520Scheduling%2520Techniques%2520for%250A%2520%2520LLMs%26entry.906535625%3DFerdi%2520Kossmann%2520and%2520Bruce%2520Fontaine%2520and%2520Daya%2520Khudia%2520and%2520Michael%2520Cafarella%2520and%2520Samuel%2520Madden%26entry.1292438233%3D%2520%2520Serving%2520systems%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520improve%2520throughput%2520by%250Aprocessing%2520several%2520requests%2520concurrently.%2520However%252C%2520multiplexing%2520hardware%250Aresources%2520between%2520concurrent%2520requests%2520involves%2520non-trivial%2520scheduling%250Adecisions.%2520Practical%2520serving%2520systems%2520typically%2520implement%2520these%2520decisions%2520at%2520two%250Alevels%253A%2520First%252C%2520a%2520load%2520balancer%2520routes%2520requests%2520to%2520different%2520servers%2520which%2520each%250Ahold%2520a%2520replica%2520of%2520the%2520LLM.%2520Then%252C%2520on%2520each%2520server%252C%2520an%2520engine-level%2520scheduler%250Adecides%2520when%2520to%2520run%2520a%2520request%252C%2520or%2520when%2520to%2520queue%2520or%2520preempt%2520it.%2520Improved%250Ascheduling%2520policies%2520may%2520benefit%2520a%2520wide%2520range%2520of%2520LLM%2520deployments%2520and%2520can%2520often%250Abe%2520implemented%2520as%2520%2522drop-in%2520replacements%2522%2520to%2520a%2520system%2527s%2520current%2520policy.%2520In%2520this%250Awork%252C%2520we%2520survey%2520scheduling%2520techniques%2520from%2520the%2520literature%2520and%2520from%2520practical%250Aserving%2520systems.%2520We%2520find%2520that%2520schedulers%2520from%2520the%2520literature%2520often%2520achieve%2520good%250Aperformance%2520but%2520introduce%2520significant%2520complexity.%2520In%2520contrast%252C%2520schedulers%2520in%250Apractical%2520deployments%2520often%2520leave%2520easy%2520performance%2520gains%2520on%2520the%2520table%2520but%2520are%250Aeasy%2520to%2520implement%252C%2520deploy%2520and%2520configure.%2520This%2520finding%2520motivates%2520us%2520to%2520introduce%250Atwo%2520new%2520scheduling%2520techniques%252C%2520which%2520are%2520both%2520easy%2520to%2520implement%252C%2520and%2520outperform%250Acurrent%2520techniques%2520on%2520production%2520workload%2520traces.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17840v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20the%20GPU%20Half-Empty%20or%20Half-Full%3F%20Practical%20Scheduling%20Techniques%20for%0A%20%20LLMs&entry.906535625=Ferdi%20Kossmann%20and%20Bruce%20Fontaine%20and%20Daya%20Khudia%20and%20Michael%20Cafarella%20and%20Samuel%20Madden&entry.1292438233=%20%20Serving%20systems%20for%20Large%20Language%20Models%20%28LLMs%29%20improve%20throughput%20by%0Aprocessing%20several%20requests%20concurrently.%20However%2C%20multiplexing%20hardware%0Aresources%20between%20concurrent%20requests%20involves%20non-trivial%20scheduling%0Adecisions.%20Practical%20serving%20systems%20typically%20implement%20these%20decisions%20at%20two%0Alevels%3A%20First%2C%20a%20load%20balancer%20routes%20requests%20to%20different%20servers%20which%20each%0Ahold%20a%20replica%20of%20the%20LLM.%20Then%2C%20on%20each%20server%2C%20an%20engine-level%20scheduler%0Adecides%20when%20to%20run%20a%20request%2C%20or%20when%20to%20queue%20or%20preempt%20it.%20Improved%0Ascheduling%20policies%20may%20benefit%20a%20wide%20range%20of%20LLM%20deployments%20and%20can%20often%0Abe%20implemented%20as%20%22drop-in%20replacements%22%20to%20a%20system%27s%20current%20policy.%20In%20this%0Awork%2C%20we%20survey%20scheduling%20techniques%20from%20the%20literature%20and%20from%20practical%0Aserving%20systems.%20We%20find%20that%20schedulers%20from%20the%20literature%20often%20achieve%20good%0Aperformance%20but%20introduce%20significant%20complexity.%20In%20contrast%2C%20schedulers%20in%0Apractical%20deployments%20often%20leave%20easy%20performance%20gains%20on%20the%20table%20but%20are%0Aeasy%20to%20implement%2C%20deploy%20and%20configure.%20This%20finding%20motivates%20us%20to%20introduce%0Atwo%20new%20scheduling%20techniques%2C%20which%20are%20both%20easy%20to%20implement%2C%20and%20outperform%0Acurrent%20techniques%20on%20production%20workload%20traces.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17840v1&entry.124074799=Read"},
{"title": "Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class\n  Feature Compensator", "author": "Xin Zhang and Jiawei Du and Ping Liu and Joey Tianyi Zhou", "abstract": "  Dataset distillation has emerged as a technique aiming to condense\ninformative features from large, natural datasets into a compact and synthetic\nform. While recent advancements have refined this technique, its performance is\nbottlenecked by the prevailing class-specific synthesis paradigm. Under this\nparadigm, synthetic data is optimized exclusively for a pre-assigned one-hot\nlabel, creating an implicit class barrier in feature condensation. This leads\nto inefficient utilization of the distillation budget and oversight of\ninter-class feature distributions, which ultimately limits the effectiveness\nand efficiency, as demonstrated in our analysis. To overcome these constraints,\nthis paper presents the Inter-class Feature Compensator (INFER), an innovative\ndistillation approach that transcends the class-specific data-label framework\nwidely utilized in current dataset distillation methods. Specifically, INFER\nleverages a Universal Feature Compensator (UFC) to enhance feature integration\nacross classes, enabling the generation of multiple additional synthetic\ninstances from a single UFC input. This significantly improves the efficiency\nof the distillation budget. Moreover, INFER enriches inter-class interactions\nduring the distillation, thereby enhancing the effectiveness and\ngeneralizability of the distilled data. By allowing for the linear\ninterpolation of labels similar to those in the original dataset, INFER\nmeticulously optimizes the synthetic data and dramatically reduces the size of\nsoft labels in the synthetic dataset to almost zero, establishing a new\nbenchmark for efficiency and effectiveness in dataset distillation.\n", "link": "http://arxiv.org/abs/2408.06927v2", "date": "2024-10-23", "relevancy": 1.4513, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4898}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4775}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Breaking%20Class%20Barriers%3A%20Efficient%20Dataset%20Distillation%20via%20Inter-Class%0A%20%20Feature%20Compensator&body=Title%3A%20Breaking%20Class%20Barriers%3A%20Efficient%20Dataset%20Distillation%20via%20Inter-Class%0A%20%20Feature%20Compensator%0AAuthor%3A%20Xin%20Zhang%20and%20Jiawei%20Du%20and%20Ping%20Liu%20and%20Joey%20Tianyi%20Zhou%0AAbstract%3A%20%20%20Dataset%20distillation%20has%20emerged%20as%20a%20technique%20aiming%20to%20condense%0Ainformative%20features%20from%20large%2C%20natural%20datasets%20into%20a%20compact%20and%20synthetic%0Aform.%20While%20recent%20advancements%20have%20refined%20this%20technique%2C%20its%20performance%20is%0Abottlenecked%20by%20the%20prevailing%20class-specific%20synthesis%20paradigm.%20Under%20this%0Aparadigm%2C%20synthetic%20data%20is%20optimized%20exclusively%20for%20a%20pre-assigned%20one-hot%0Alabel%2C%20creating%20an%20implicit%20class%20barrier%20in%20feature%20condensation.%20This%20leads%0Ato%20inefficient%20utilization%20of%20the%20distillation%20budget%20and%20oversight%20of%0Ainter-class%20feature%20distributions%2C%20which%20ultimately%20limits%20the%20effectiveness%0Aand%20efficiency%2C%20as%20demonstrated%20in%20our%20analysis.%20To%20overcome%20these%20constraints%2C%0Athis%20paper%20presents%20the%20Inter-class%20Feature%20Compensator%20%28INFER%29%2C%20an%20innovative%0Adistillation%20approach%20that%20transcends%20the%20class-specific%20data-label%20framework%0Awidely%20utilized%20in%20current%20dataset%20distillation%20methods.%20Specifically%2C%20INFER%0Aleverages%20a%20Universal%20Feature%20Compensator%20%28UFC%29%20to%20enhance%20feature%20integration%0Aacross%20classes%2C%20enabling%20the%20generation%20of%20multiple%20additional%20synthetic%0Ainstances%20from%20a%20single%20UFC%20input.%20This%20significantly%20improves%20the%20efficiency%0Aof%20the%20distillation%20budget.%20Moreover%2C%20INFER%20enriches%20inter-class%20interactions%0Aduring%20the%20distillation%2C%20thereby%20enhancing%20the%20effectiveness%20and%0Ageneralizability%20of%20the%20distilled%20data.%20By%20allowing%20for%20the%20linear%0Ainterpolation%20of%20labels%20similar%20to%20those%20in%20the%20original%20dataset%2C%20INFER%0Ameticulously%20optimizes%20the%20synthetic%20data%20and%20dramatically%20reduces%20the%20size%20of%0Asoft%20labels%20in%20the%20synthetic%20dataset%20to%20almost%20zero%2C%20establishing%20a%20new%0Abenchmark%20for%20efficiency%20and%20effectiveness%20in%20dataset%20distillation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06927v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBreaking%2520Class%2520Barriers%253A%2520Efficient%2520Dataset%2520Distillation%2520via%2520Inter-Class%250A%2520%2520Feature%2520Compensator%26entry.906535625%3DXin%2520Zhang%2520and%2520Jiawei%2520Du%2520and%2520Ping%2520Liu%2520and%2520Joey%2520Tianyi%2520Zhou%26entry.1292438233%3D%2520%2520Dataset%2520distillation%2520has%2520emerged%2520as%2520a%2520technique%2520aiming%2520to%2520condense%250Ainformative%2520features%2520from%2520large%252C%2520natural%2520datasets%2520into%2520a%2520compact%2520and%2520synthetic%250Aform.%2520While%2520recent%2520advancements%2520have%2520refined%2520this%2520technique%252C%2520its%2520performance%2520is%250Abottlenecked%2520by%2520the%2520prevailing%2520class-specific%2520synthesis%2520paradigm.%2520Under%2520this%250Aparadigm%252C%2520synthetic%2520data%2520is%2520optimized%2520exclusively%2520for%2520a%2520pre-assigned%2520one-hot%250Alabel%252C%2520creating%2520an%2520implicit%2520class%2520barrier%2520in%2520feature%2520condensation.%2520This%2520leads%250Ato%2520inefficient%2520utilization%2520of%2520the%2520distillation%2520budget%2520and%2520oversight%2520of%250Ainter-class%2520feature%2520distributions%252C%2520which%2520ultimately%2520limits%2520the%2520effectiveness%250Aand%2520efficiency%252C%2520as%2520demonstrated%2520in%2520our%2520analysis.%2520To%2520overcome%2520these%2520constraints%252C%250Athis%2520paper%2520presents%2520the%2520Inter-class%2520Feature%2520Compensator%2520%2528INFER%2529%252C%2520an%2520innovative%250Adistillation%2520approach%2520that%2520transcends%2520the%2520class-specific%2520data-label%2520framework%250Awidely%2520utilized%2520in%2520current%2520dataset%2520distillation%2520methods.%2520Specifically%252C%2520INFER%250Aleverages%2520a%2520Universal%2520Feature%2520Compensator%2520%2528UFC%2529%2520to%2520enhance%2520feature%2520integration%250Aacross%2520classes%252C%2520enabling%2520the%2520generation%2520of%2520multiple%2520additional%2520synthetic%250Ainstances%2520from%2520a%2520single%2520UFC%2520input.%2520This%2520significantly%2520improves%2520the%2520efficiency%250Aof%2520the%2520distillation%2520budget.%2520Moreover%252C%2520INFER%2520enriches%2520inter-class%2520interactions%250Aduring%2520the%2520distillation%252C%2520thereby%2520enhancing%2520the%2520effectiveness%2520and%250Ageneralizability%2520of%2520the%2520distilled%2520data.%2520By%2520allowing%2520for%2520the%2520linear%250Ainterpolation%2520of%2520labels%2520similar%2520to%2520those%2520in%2520the%2520original%2520dataset%252C%2520INFER%250Ameticulously%2520optimizes%2520the%2520synthetic%2520data%2520and%2520dramatically%2520reduces%2520the%2520size%2520of%250Asoft%2520labels%2520in%2520the%2520synthetic%2520dataset%2520to%2520almost%2520zero%252C%2520establishing%2520a%2520new%250Abenchmark%2520for%2520efficiency%2520and%2520effectiveness%2520in%2520dataset%2520distillation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06927v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Breaking%20Class%20Barriers%3A%20Efficient%20Dataset%20Distillation%20via%20Inter-Class%0A%20%20Feature%20Compensator&entry.906535625=Xin%20Zhang%20and%20Jiawei%20Du%20and%20Ping%20Liu%20and%20Joey%20Tianyi%20Zhou&entry.1292438233=%20%20Dataset%20distillation%20has%20emerged%20as%20a%20technique%20aiming%20to%20condense%0Ainformative%20features%20from%20large%2C%20natural%20datasets%20into%20a%20compact%20and%20synthetic%0Aform.%20While%20recent%20advancements%20have%20refined%20this%20technique%2C%20its%20performance%20is%0Abottlenecked%20by%20the%20prevailing%20class-specific%20synthesis%20paradigm.%20Under%20this%0Aparadigm%2C%20synthetic%20data%20is%20optimized%20exclusively%20for%20a%20pre-assigned%20one-hot%0Alabel%2C%20creating%20an%20implicit%20class%20barrier%20in%20feature%20condensation.%20This%20leads%0Ato%20inefficient%20utilization%20of%20the%20distillation%20budget%20and%20oversight%20of%0Ainter-class%20feature%20distributions%2C%20which%20ultimately%20limits%20the%20effectiveness%0Aand%20efficiency%2C%20as%20demonstrated%20in%20our%20analysis.%20To%20overcome%20these%20constraints%2C%0Athis%20paper%20presents%20the%20Inter-class%20Feature%20Compensator%20%28INFER%29%2C%20an%20innovative%0Adistillation%20approach%20that%20transcends%20the%20class-specific%20data-label%20framework%0Awidely%20utilized%20in%20current%20dataset%20distillation%20methods.%20Specifically%2C%20INFER%0Aleverages%20a%20Universal%20Feature%20Compensator%20%28UFC%29%20to%20enhance%20feature%20integration%0Aacross%20classes%2C%20enabling%20the%20generation%20of%20multiple%20additional%20synthetic%0Ainstances%20from%20a%20single%20UFC%20input.%20This%20significantly%20improves%20the%20efficiency%0Aof%20the%20distillation%20budget.%20Moreover%2C%20INFER%20enriches%20inter-class%20interactions%0Aduring%20the%20distillation%2C%20thereby%20enhancing%20the%20effectiveness%20and%0Ageneralizability%20of%20the%20distilled%20data.%20By%20allowing%20for%20the%20linear%0Ainterpolation%20of%20labels%20similar%20to%20those%20in%20the%20original%20dataset%2C%20INFER%0Ameticulously%20optimizes%20the%20synthetic%20data%20and%20dramatically%20reduces%20the%20size%20of%0Asoft%20labels%20in%20the%20synthetic%20dataset%20to%20almost%20zero%2C%20establishing%20a%20new%0Abenchmark%20for%20efficiency%20and%20effectiveness%20in%20dataset%20distillation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06927v2&entry.124074799=Read"},
{"title": "On the explainability of quantum neural networks based on variational\n  quantum circuits", "author": "Ammar Daskin", "abstract": "  Ridge functions are used to describe and study the lower bound of the\napproximation done by the neural networks which can be written as a linear\ncombination of activation functions. If the activation functions are also ridge\nfunctions, these networks are called explainable neural networks.\n  In this brief paper, we first show that quantum neural networks which are\nbased on variational quantum circuits can be written as a linear combination of\nridge functions by following matrix notations. Consequently, we show that the\ninterpretability and explainability of such quantum neural networks can be\ndirectly considered and studied as an approximation with the linear combination\nof ridge functions.\n", "link": "http://arxiv.org/abs/2301.05549v3", "date": "2024-10-23", "relevancy": 1.4558, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3965}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.3672}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20explainability%20of%20quantum%20neural%20networks%20based%20on%20variational%0A%20%20quantum%20circuits&body=Title%3A%20On%20the%20explainability%20of%20quantum%20neural%20networks%20based%20on%20variational%0A%20%20quantum%20circuits%0AAuthor%3A%20Ammar%20Daskin%0AAbstract%3A%20%20%20Ridge%20functions%20are%20used%20to%20describe%20and%20study%20the%20lower%20bound%20of%20the%0Aapproximation%20done%20by%20the%20neural%20networks%20which%20can%20be%20written%20as%20a%20linear%0Acombination%20of%20activation%20functions.%20If%20the%20activation%20functions%20are%20also%20ridge%0Afunctions%2C%20these%20networks%20are%20called%20explainable%20neural%20networks.%0A%20%20In%20this%20brief%20paper%2C%20we%20first%20show%20that%20quantum%20neural%20networks%20which%20are%0Abased%20on%20variational%20quantum%20circuits%20can%20be%20written%20as%20a%20linear%20combination%20of%0Aridge%20functions%20by%20following%20matrix%20notations.%20Consequently%2C%20we%20show%20that%20the%0Ainterpretability%20and%20explainability%20of%20such%20quantum%20neural%20networks%20can%20be%0Adirectly%20considered%20and%20studied%20as%20an%20approximation%20with%20the%20linear%20combination%0Aof%20ridge%20functions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.05549v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520explainability%2520of%2520quantum%2520neural%2520networks%2520based%2520on%2520variational%250A%2520%2520quantum%2520circuits%26entry.906535625%3DAmmar%2520Daskin%26entry.1292438233%3D%2520%2520Ridge%2520functions%2520are%2520used%2520to%2520describe%2520and%2520study%2520the%2520lower%2520bound%2520of%2520the%250Aapproximation%2520done%2520by%2520the%2520neural%2520networks%2520which%2520can%2520be%2520written%2520as%2520a%2520linear%250Acombination%2520of%2520activation%2520functions.%2520If%2520the%2520activation%2520functions%2520are%2520also%2520ridge%250Afunctions%252C%2520these%2520networks%2520are%2520called%2520explainable%2520neural%2520networks.%250A%2520%2520In%2520this%2520brief%2520paper%252C%2520we%2520first%2520show%2520that%2520quantum%2520neural%2520networks%2520which%2520are%250Abased%2520on%2520variational%2520quantum%2520circuits%2520can%2520be%2520written%2520as%2520a%2520linear%2520combination%2520of%250Aridge%2520functions%2520by%2520following%2520matrix%2520notations.%2520Consequently%252C%2520we%2520show%2520that%2520the%250Ainterpretability%2520and%2520explainability%2520of%2520such%2520quantum%2520neural%2520networks%2520can%2520be%250Adirectly%2520considered%2520and%2520studied%2520as%2520an%2520approximation%2520with%2520the%2520linear%2520combination%250Aof%2520ridge%2520functions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.05549v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20explainability%20of%20quantum%20neural%20networks%20based%20on%20variational%0A%20%20quantum%20circuits&entry.906535625=Ammar%20Daskin&entry.1292438233=%20%20Ridge%20functions%20are%20used%20to%20describe%20and%20study%20the%20lower%20bound%20of%20the%0Aapproximation%20done%20by%20the%20neural%20networks%20which%20can%20be%20written%20as%20a%20linear%0Acombination%20of%20activation%20functions.%20If%20the%20activation%20functions%20are%20also%20ridge%0Afunctions%2C%20these%20networks%20are%20called%20explainable%20neural%20networks.%0A%20%20In%20this%20brief%20paper%2C%20we%20first%20show%20that%20quantum%20neural%20networks%20which%20are%0Abased%20on%20variational%20quantum%20circuits%20can%20be%20written%20as%20a%20linear%20combination%20of%0Aridge%20functions%20by%20following%20matrix%20notations.%20Consequently%2C%20we%20show%20that%20the%0Ainterpretability%20and%20explainability%20of%20such%20quantum%20neural%20networks%20can%20be%0Adirectly%20considered%20and%20studied%20as%20an%20approximation%20with%20the%20linear%20combination%0Aof%20ridge%20functions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.05549v3&entry.124074799=Read"},
{"title": "Characterization of the multiplicity of solutions for camera pose given\n  two vertically-aligned landmarks and accelerometer", "author": "Alexander R. Pruss", "abstract": "  We consider the problem of recovering the position and orientation of a\ncamera equipped with an accelerometer from sensor images of two labeled\nlandmarks whose positions in a coordinate system aligned in a known way with\ngravity are known. This a variant on the much studied P$n$P problem of\nrecovering camera position and orientation from $n$ points without any\ngravitational data. It is proved that in three types of singular cases there\nare infinitely many solutions, in another type of case there is one, and in a\nfinal type of case there are two. A precise characterization of each type of\ncase. In particular, there is always a unique solution in the practically\ninteresting case where the two landmarks are at the same altitude and the\ncamera is at a different altitude. This case is studied by numerical simulation\nand an implementation on a consumer cellphone. It is also proved that if the\ntwo landmarks are unlabeled, then apart from the same singular cases, there are\nstill always one or two solutions.\n", "link": "http://arxiv.org/abs/2410.17997v1", "date": "2024-10-23", "relevancy": 1.8728, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4858}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4588}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.4475}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Characterization%20of%20the%20multiplicity%20of%20solutions%20for%20camera%20pose%20given%0A%20%20two%20vertically-aligned%20landmarks%20and%20accelerometer&body=Title%3A%20Characterization%20of%20the%20multiplicity%20of%20solutions%20for%20camera%20pose%20given%0A%20%20two%20vertically-aligned%20landmarks%20and%20accelerometer%0AAuthor%3A%20Alexander%20R.%20Pruss%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20recovering%20the%20position%20and%20orientation%20of%20a%0Acamera%20equipped%20with%20an%20accelerometer%20from%20sensor%20images%20of%20two%20labeled%0Alandmarks%20whose%20positions%20in%20a%20coordinate%20system%20aligned%20in%20a%20known%20way%20with%0Agravity%20are%20known.%20This%20a%20variant%20on%20the%20much%20studied%20P%24n%24P%20problem%20of%0Arecovering%20camera%20position%20and%20orientation%20from%20%24n%24%20points%20without%20any%0Agravitational%20data.%20It%20is%20proved%20that%20in%20three%20types%20of%20singular%20cases%20there%0Aare%20infinitely%20many%20solutions%2C%20in%20another%20type%20of%20case%20there%20is%20one%2C%20and%20in%20a%0Afinal%20type%20of%20case%20there%20are%20two.%20A%20precise%20characterization%20of%20each%20type%20of%0Acase.%20In%20particular%2C%20there%20is%20always%20a%20unique%20solution%20in%20the%20practically%0Ainteresting%20case%20where%20the%20two%20landmarks%20are%20at%20the%20same%20altitude%20and%20the%0Acamera%20is%20at%20a%20different%20altitude.%20This%20case%20is%20studied%20by%20numerical%20simulation%0Aand%20an%20implementation%20on%20a%20consumer%20cellphone.%20It%20is%20also%20proved%20that%20if%20the%0Atwo%20landmarks%20are%20unlabeled%2C%20then%20apart%20from%20the%20same%20singular%20cases%2C%20there%20are%0Astill%20always%20one%20or%20two%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17997v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCharacterization%2520of%2520the%2520multiplicity%2520of%2520solutions%2520for%2520camera%2520pose%2520given%250A%2520%2520two%2520vertically-aligned%2520landmarks%2520and%2520accelerometer%26entry.906535625%3DAlexander%2520R.%2520Pruss%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520recovering%2520the%2520position%2520and%2520orientation%2520of%2520a%250Acamera%2520equipped%2520with%2520an%2520accelerometer%2520from%2520sensor%2520images%2520of%2520two%2520labeled%250Alandmarks%2520whose%2520positions%2520in%2520a%2520coordinate%2520system%2520aligned%2520in%2520a%2520known%2520way%2520with%250Agravity%2520are%2520known.%2520This%2520a%2520variant%2520on%2520the%2520much%2520studied%2520P%2524n%2524P%2520problem%2520of%250Arecovering%2520camera%2520position%2520and%2520orientation%2520from%2520%2524n%2524%2520points%2520without%2520any%250Agravitational%2520data.%2520It%2520is%2520proved%2520that%2520in%2520three%2520types%2520of%2520singular%2520cases%2520there%250Aare%2520infinitely%2520many%2520solutions%252C%2520in%2520another%2520type%2520of%2520case%2520there%2520is%2520one%252C%2520and%2520in%2520a%250Afinal%2520type%2520of%2520case%2520there%2520are%2520two.%2520A%2520precise%2520characterization%2520of%2520each%2520type%2520of%250Acase.%2520In%2520particular%252C%2520there%2520is%2520always%2520a%2520unique%2520solution%2520in%2520the%2520practically%250Ainteresting%2520case%2520where%2520the%2520two%2520landmarks%2520are%2520at%2520the%2520same%2520altitude%2520and%2520the%250Acamera%2520is%2520at%2520a%2520different%2520altitude.%2520This%2520case%2520is%2520studied%2520by%2520numerical%2520simulation%250Aand%2520an%2520implementation%2520on%2520a%2520consumer%2520cellphone.%2520It%2520is%2520also%2520proved%2520that%2520if%2520the%250Atwo%2520landmarks%2520are%2520unlabeled%252C%2520then%2520apart%2520from%2520the%2520same%2520singular%2520cases%252C%2520there%2520are%250Astill%2520always%2520one%2520or%2520two%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17997v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Characterization%20of%20the%20multiplicity%20of%20solutions%20for%20camera%20pose%20given%0A%20%20two%20vertically-aligned%20landmarks%20and%20accelerometer&entry.906535625=Alexander%20R.%20Pruss&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20recovering%20the%20position%20and%20orientation%20of%20a%0Acamera%20equipped%20with%20an%20accelerometer%20from%20sensor%20images%20of%20two%20labeled%0Alandmarks%20whose%20positions%20in%20a%20coordinate%20system%20aligned%20in%20a%20known%20way%20with%0Agravity%20are%20known.%20This%20a%20variant%20on%20the%20much%20studied%20P%24n%24P%20problem%20of%0Arecovering%20camera%20position%20and%20orientation%20from%20%24n%24%20points%20without%20any%0Agravitational%20data.%20It%20is%20proved%20that%20in%20three%20types%20of%20singular%20cases%20there%0Aare%20infinitely%20many%20solutions%2C%20in%20another%20type%20of%20case%20there%20is%20one%2C%20and%20in%20a%0Afinal%20type%20of%20case%20there%20are%20two.%20A%20precise%20characterization%20of%20each%20type%20of%0Acase.%20In%20particular%2C%20there%20is%20always%20a%20unique%20solution%20in%20the%20practically%0Ainteresting%20case%20where%20the%20two%20landmarks%20are%20at%20the%20same%20altitude%20and%20the%0Acamera%20is%20at%20a%20different%20altitude.%20This%20case%20is%20studied%20by%20numerical%20simulation%0Aand%20an%20implementation%20on%20a%20consumer%20cellphone.%20It%20is%20also%20proved%20that%20if%20the%0Atwo%20landmarks%20are%20unlabeled%2C%20then%20apart%20from%20the%20same%20singular%20cases%2C%20there%20are%0Astill%20always%20one%20or%20two%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17997v1&entry.124074799=Read"},
{"title": "I've Got 99 Problems But FLOPS Ain't One", "author": "Alexandru M. Gherghescu and Vlad-Andrei B\u0103doiu and Alexandru Agache and Mihai-Valentin Dumitru and Iuliu Vasilescu and Radu Mantu and Costin Raiciu", "abstract": "  Hyperscalers dominate the landscape of large network deployments, yet they\nrarely share data or insights about the challenges they face. In light of this\nsupremacy, what problems can we find to solve in this space? We take an\nunconventional approach to find relevant research directions, starting from\npublic plans to build a $100 billion datacenter for machine learning\napplications. Leveraging the language models scaling laws, we discover what\nworkloads such a datacenter might carry and explore the challenges one may\nencounter in doing so, with a focus on networking research. We conclude that\nbuilding the datacenter and training such models is technically possible, but\nthis requires novel wide-area transports for inter-DC communication, a\nmultipath transport and novel datacenter topologies for intra-datacenter\ncommunication, high speed scale-up networks and transports, outlining a rich\nresearch agenda for the networking community.\n", "link": "http://arxiv.org/abs/2407.12819v2", "date": "2024-10-23", "relevancy": 1.2672, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4396}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4189}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4169}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20I%27ve%20Got%2099%20Problems%20But%20FLOPS%20Ain%27t%20One&body=Title%3A%20I%27ve%20Got%2099%20Problems%20But%20FLOPS%20Ain%27t%20One%0AAuthor%3A%20Alexandru%20M.%20Gherghescu%20and%20Vlad-Andrei%20B%C4%83doiu%20and%20Alexandru%20Agache%20and%20Mihai-Valentin%20Dumitru%20and%20Iuliu%20Vasilescu%20and%20Radu%20Mantu%20and%20Costin%20Raiciu%0AAbstract%3A%20%20%20Hyperscalers%20dominate%20the%20landscape%20of%20large%20network%20deployments%2C%20yet%20they%0Ararely%20share%20data%20or%20insights%20about%20the%20challenges%20they%20face.%20In%20light%20of%20this%0Asupremacy%2C%20what%20problems%20can%20we%20find%20to%20solve%20in%20this%20space%3F%20We%20take%20an%0Aunconventional%20approach%20to%20find%20relevant%20research%20directions%2C%20starting%20from%0Apublic%20plans%20to%20build%20a%20%24100%20billion%20datacenter%20for%20machine%20learning%0Aapplications.%20Leveraging%20the%20language%20models%20scaling%20laws%2C%20we%20discover%20what%0Aworkloads%20such%20a%20datacenter%20might%20carry%20and%20explore%20the%20challenges%20one%20may%0Aencounter%20in%20doing%20so%2C%20with%20a%20focus%20on%20networking%20research.%20We%20conclude%20that%0Abuilding%20the%20datacenter%20and%20training%20such%20models%20is%20technically%20possible%2C%20but%0Athis%20requires%20novel%20wide-area%20transports%20for%20inter-DC%20communication%2C%20a%0Amultipath%20transport%20and%20novel%20datacenter%20topologies%20for%20intra-datacenter%0Acommunication%2C%20high%20speed%20scale-up%20networks%20and%20transports%2C%20outlining%20a%20rich%0Aresearch%20agenda%20for%20the%20networking%20community.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.12819v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DI%2527ve%2520Got%252099%2520Problems%2520But%2520FLOPS%2520Ain%2527t%2520One%26entry.906535625%3DAlexandru%2520M.%2520Gherghescu%2520and%2520Vlad-Andrei%2520B%25C4%2583doiu%2520and%2520Alexandru%2520Agache%2520and%2520Mihai-Valentin%2520Dumitru%2520and%2520Iuliu%2520Vasilescu%2520and%2520Radu%2520Mantu%2520and%2520Costin%2520Raiciu%26entry.1292438233%3D%2520%2520Hyperscalers%2520dominate%2520the%2520landscape%2520of%2520large%2520network%2520deployments%252C%2520yet%2520they%250Ararely%2520share%2520data%2520or%2520insights%2520about%2520the%2520challenges%2520they%2520face.%2520In%2520light%2520of%2520this%250Asupremacy%252C%2520what%2520problems%2520can%2520we%2520find%2520to%2520solve%2520in%2520this%2520space%253F%2520We%2520take%2520an%250Aunconventional%2520approach%2520to%2520find%2520relevant%2520research%2520directions%252C%2520starting%2520from%250Apublic%2520plans%2520to%2520build%2520a%2520%2524100%2520billion%2520datacenter%2520for%2520machine%2520learning%250Aapplications.%2520Leveraging%2520the%2520language%2520models%2520scaling%2520laws%252C%2520we%2520discover%2520what%250Aworkloads%2520such%2520a%2520datacenter%2520might%2520carry%2520and%2520explore%2520the%2520challenges%2520one%2520may%250Aencounter%2520in%2520doing%2520so%252C%2520with%2520a%2520focus%2520on%2520networking%2520research.%2520We%2520conclude%2520that%250Abuilding%2520the%2520datacenter%2520and%2520training%2520such%2520models%2520is%2520technically%2520possible%252C%2520but%250Athis%2520requires%2520novel%2520wide-area%2520transports%2520for%2520inter-DC%2520communication%252C%2520a%250Amultipath%2520transport%2520and%2520novel%2520datacenter%2520topologies%2520for%2520intra-datacenter%250Acommunication%252C%2520high%2520speed%2520scale-up%2520networks%2520and%2520transports%252C%2520outlining%2520a%2520rich%250Aresearch%2520agenda%2520for%2520the%2520networking%2520community.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.12819v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=I%27ve%20Got%2099%20Problems%20But%20FLOPS%20Ain%27t%20One&entry.906535625=Alexandru%20M.%20Gherghescu%20and%20Vlad-Andrei%20B%C4%83doiu%20and%20Alexandru%20Agache%20and%20Mihai-Valentin%20Dumitru%20and%20Iuliu%20Vasilescu%20and%20Radu%20Mantu%20and%20Costin%20Raiciu&entry.1292438233=%20%20Hyperscalers%20dominate%20the%20landscape%20of%20large%20network%20deployments%2C%20yet%20they%0Ararely%20share%20data%20or%20insights%20about%20the%20challenges%20they%20face.%20In%20light%20of%20this%0Asupremacy%2C%20what%20problems%20can%20we%20find%20to%20solve%20in%20this%20space%3F%20We%20take%20an%0Aunconventional%20approach%20to%20find%20relevant%20research%20directions%2C%20starting%20from%0Apublic%20plans%20to%20build%20a%20%24100%20billion%20datacenter%20for%20machine%20learning%0Aapplications.%20Leveraging%20the%20language%20models%20scaling%20laws%2C%20we%20discover%20what%0Aworkloads%20such%20a%20datacenter%20might%20carry%20and%20explore%20the%20challenges%20one%20may%0Aencounter%20in%20doing%20so%2C%20with%20a%20focus%20on%20networking%20research.%20We%20conclude%20that%0Abuilding%20the%20datacenter%20and%20training%20such%20models%20is%20technically%20possible%2C%20but%0Athis%20requires%20novel%20wide-area%20transports%20for%20inter-DC%20communication%2C%20a%0Amultipath%20transport%20and%20novel%20datacenter%20topologies%20for%20intra-datacenter%0Acommunication%2C%20high%20speed%20scale-up%20networks%20and%20transports%2C%20outlining%20a%20rich%0Aresearch%20agenda%20for%20the%20networking%20community.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.12819v2&entry.124074799=Read"},
{"title": "Medical Imaging Complexity and its Effects on GAN Performance", "author": "William Cagas and Chan Ko and Blake Hsiao and Shryuk Grandhi and Rishi Bhattacharya and Kevin Zhu and Michael Lam", "abstract": "  The proliferation of machine learning models in diverse clinical applications\nhas led to a growing need for high-fidelity, medical image training data. Such\ndata is often scarce due to cost constraints and privacy concerns. Alleviating\nthis burden, medical image synthesis via generative adversarial networks (GANs)\nemerged as a powerful method for synthetically generating photo-realistic\nimages based on existing sets of real medical images. However, the exact image\nset size required to efficiently train such a GAN is unclear. In this work, we\nexperimentally establish benchmarks that measure the relationship between a\nsample dataset size and the fidelity of the generated images, given the\ndataset's distribution of image complexities. We analyze statistical metrics\nbased on delentropy, an image complexity measure rooted in Shannon's entropy in\ninformation theory. For our pipeline, we conduct experiments with two\nstate-of-the-art GANs, StyleGAN 3 and SPADE-GAN, trained on multiple medical\nimaging datasets with variable sample sizes. Across both GANs, general\nperformance improved with increasing training set size but suffered with\nincreasing complexity.\n", "link": "http://arxiv.org/abs/2410.17959v1", "date": "2024-10-23", "relevancy": 1.6174, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5519}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5456}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5101}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Medical%20Imaging%20Complexity%20and%20its%20Effects%20on%20GAN%20Performance&body=Title%3A%20Medical%20Imaging%20Complexity%20and%20its%20Effects%20on%20GAN%20Performance%0AAuthor%3A%20William%20Cagas%20and%20Chan%20Ko%20and%20Blake%20Hsiao%20and%20Shryuk%20Grandhi%20and%20Rishi%20Bhattacharya%20and%20Kevin%20Zhu%20and%20Michael%20Lam%0AAbstract%3A%20%20%20The%20proliferation%20of%20machine%20learning%20models%20in%20diverse%20clinical%20applications%0Ahas%20led%20to%20a%20growing%20need%20for%20high-fidelity%2C%20medical%20image%20training%20data.%20Such%0Adata%20is%20often%20scarce%20due%20to%20cost%20constraints%20and%20privacy%20concerns.%20Alleviating%0Athis%20burden%2C%20medical%20image%20synthesis%20via%20generative%20adversarial%20networks%20%28GANs%29%0Aemerged%20as%20a%20powerful%20method%20for%20synthetically%20generating%20photo-realistic%0Aimages%20based%20on%20existing%20sets%20of%20real%20medical%20images.%20However%2C%20the%20exact%20image%0Aset%20size%20required%20to%20efficiently%20train%20such%20a%20GAN%20is%20unclear.%20In%20this%20work%2C%20we%0Aexperimentally%20establish%20benchmarks%20that%20measure%20the%20relationship%20between%20a%0Asample%20dataset%20size%20and%20the%20fidelity%20of%20the%20generated%20images%2C%20given%20the%0Adataset%27s%20distribution%20of%20image%20complexities.%20We%20analyze%20statistical%20metrics%0Abased%20on%20delentropy%2C%20an%20image%20complexity%20measure%20rooted%20in%20Shannon%27s%20entropy%20in%0Ainformation%20theory.%20For%20our%20pipeline%2C%20we%20conduct%20experiments%20with%20two%0Astate-of-the-art%20GANs%2C%20StyleGAN%203%20and%20SPADE-GAN%2C%20trained%20on%20multiple%20medical%0Aimaging%20datasets%20with%20variable%20sample%20sizes.%20Across%20both%20GANs%2C%20general%0Aperformance%20improved%20with%20increasing%20training%20set%20size%20but%20suffered%20with%0Aincreasing%20complexity.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17959v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMedical%2520Imaging%2520Complexity%2520and%2520its%2520Effects%2520on%2520GAN%2520Performance%26entry.906535625%3DWilliam%2520Cagas%2520and%2520Chan%2520Ko%2520and%2520Blake%2520Hsiao%2520and%2520Shryuk%2520Grandhi%2520and%2520Rishi%2520Bhattacharya%2520and%2520Kevin%2520Zhu%2520and%2520Michael%2520Lam%26entry.1292438233%3D%2520%2520The%2520proliferation%2520of%2520machine%2520learning%2520models%2520in%2520diverse%2520clinical%2520applications%250Ahas%2520led%2520to%2520a%2520growing%2520need%2520for%2520high-fidelity%252C%2520medical%2520image%2520training%2520data.%2520Such%250Adata%2520is%2520often%2520scarce%2520due%2520to%2520cost%2520constraints%2520and%2520privacy%2520concerns.%2520Alleviating%250Athis%2520burden%252C%2520medical%2520image%2520synthesis%2520via%2520generative%2520adversarial%2520networks%2520%2528GANs%2529%250Aemerged%2520as%2520a%2520powerful%2520method%2520for%2520synthetically%2520generating%2520photo-realistic%250Aimages%2520based%2520on%2520existing%2520sets%2520of%2520real%2520medical%2520images.%2520However%252C%2520the%2520exact%2520image%250Aset%2520size%2520required%2520to%2520efficiently%2520train%2520such%2520a%2520GAN%2520is%2520unclear.%2520In%2520this%2520work%252C%2520we%250Aexperimentally%2520establish%2520benchmarks%2520that%2520measure%2520the%2520relationship%2520between%2520a%250Asample%2520dataset%2520size%2520and%2520the%2520fidelity%2520of%2520the%2520generated%2520images%252C%2520given%2520the%250Adataset%2527s%2520distribution%2520of%2520image%2520complexities.%2520We%2520analyze%2520statistical%2520metrics%250Abased%2520on%2520delentropy%252C%2520an%2520image%2520complexity%2520measure%2520rooted%2520in%2520Shannon%2527s%2520entropy%2520in%250Ainformation%2520theory.%2520For%2520our%2520pipeline%252C%2520we%2520conduct%2520experiments%2520with%2520two%250Astate-of-the-art%2520GANs%252C%2520StyleGAN%25203%2520and%2520SPADE-GAN%252C%2520trained%2520on%2520multiple%2520medical%250Aimaging%2520datasets%2520with%2520variable%2520sample%2520sizes.%2520Across%2520both%2520GANs%252C%2520general%250Aperformance%2520improved%2520with%2520increasing%2520training%2520set%2520size%2520but%2520suffered%2520with%250Aincreasing%2520complexity.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17959v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Medical%20Imaging%20Complexity%20and%20its%20Effects%20on%20GAN%20Performance&entry.906535625=William%20Cagas%20and%20Chan%20Ko%20and%20Blake%20Hsiao%20and%20Shryuk%20Grandhi%20and%20Rishi%20Bhattacharya%20and%20Kevin%20Zhu%20and%20Michael%20Lam&entry.1292438233=%20%20The%20proliferation%20of%20machine%20learning%20models%20in%20diverse%20clinical%20applications%0Ahas%20led%20to%20a%20growing%20need%20for%20high-fidelity%2C%20medical%20image%20training%20data.%20Such%0Adata%20is%20often%20scarce%20due%20to%20cost%20constraints%20and%20privacy%20concerns.%20Alleviating%0Athis%20burden%2C%20medical%20image%20synthesis%20via%20generative%20adversarial%20networks%20%28GANs%29%0Aemerged%20as%20a%20powerful%20method%20for%20synthetically%20generating%20photo-realistic%0Aimages%20based%20on%20existing%20sets%20of%20real%20medical%20images.%20However%2C%20the%20exact%20image%0Aset%20size%20required%20to%20efficiently%20train%20such%20a%20GAN%20is%20unclear.%20In%20this%20work%2C%20we%0Aexperimentally%20establish%20benchmarks%20that%20measure%20the%20relationship%20between%20a%0Asample%20dataset%20size%20and%20the%20fidelity%20of%20the%20generated%20images%2C%20given%20the%0Adataset%27s%20distribution%20of%20image%20complexities.%20We%20analyze%20statistical%20metrics%0Abased%20on%20delentropy%2C%20an%20image%20complexity%20measure%20rooted%20in%20Shannon%27s%20entropy%20in%0Ainformation%20theory.%20For%20our%20pipeline%2C%20we%20conduct%20experiments%20with%20two%0Astate-of-the-art%20GANs%2C%20StyleGAN%203%20and%20SPADE-GAN%2C%20trained%20on%20multiple%20medical%0Aimaging%20datasets%20with%20variable%20sample%20sizes.%20Across%20both%20GANs%2C%20general%0Aperformance%20improved%20with%20increasing%20training%20set%20size%20but%20suffered%20with%0Aincreasing%20complexity.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17959v1&entry.124074799=Read"},
{"title": "Non-intrusive Speech Quality Assessment with Diffusion Models Trained on\n  Clean Speech", "author": "Danilo de Oliveira and Julius Richter and Jean-Marie Lemercier and Simon Welker and Timo Gerkmann", "abstract": "  Diffusion models have found great success in generating high quality, natural\nsamples of speech, but their potential for density estimation for speech has so\nfar remained largely unexplored. In this work, we leverage an unconditional\ndiffusion model trained only on clean speech for the assessment of speech\nquality. We show that the quality of a speech utterance can be assessed by\nestimating the likelihood of a corresponding sample in the terminating Gaussian\ndistribution, obtained via a deterministic noising process. The resulting\nmethod is purely unsupervised, trained only on clean speech, and therefore does\nnot rely on annotations. Our diffusion-based approach leverages clean speech\npriors to assess quality based on how the input relates to the learned\ndistribution of clean data. Our proposed log-likelihoods show promising\nresults, correlating well with intrusive speech quality metrics such as POLQA\nand SI-SDR.\n", "link": "http://arxiv.org/abs/2410.17834v1", "date": "2024-10-23", "relevancy": 1.6304, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5597}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5438}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Non-intrusive%20Speech%20Quality%20Assessment%20with%20Diffusion%20Models%20Trained%20on%0A%20%20Clean%20Speech&body=Title%3A%20Non-intrusive%20Speech%20Quality%20Assessment%20with%20Diffusion%20Models%20Trained%20on%0A%20%20Clean%20Speech%0AAuthor%3A%20Danilo%20de%20Oliveira%20and%20Julius%20Richter%20and%20Jean-Marie%20Lemercier%20and%20Simon%20Welker%20and%20Timo%20Gerkmann%0AAbstract%3A%20%20%20Diffusion%20models%20have%20found%20great%20success%20in%20generating%20high%20quality%2C%20natural%0Asamples%20of%20speech%2C%20but%20their%20potential%20for%20density%20estimation%20for%20speech%20has%20so%0Afar%20remained%20largely%20unexplored.%20In%20this%20work%2C%20we%20leverage%20an%20unconditional%0Adiffusion%20model%20trained%20only%20on%20clean%20speech%20for%20the%20assessment%20of%20speech%0Aquality.%20We%20show%20that%20the%20quality%20of%20a%20speech%20utterance%20can%20be%20assessed%20by%0Aestimating%20the%20likelihood%20of%20a%20corresponding%20sample%20in%20the%20terminating%20Gaussian%0Adistribution%2C%20obtained%20via%20a%20deterministic%20noising%20process.%20The%20resulting%0Amethod%20is%20purely%20unsupervised%2C%20trained%20only%20on%20clean%20speech%2C%20and%20therefore%20does%0Anot%20rely%20on%20annotations.%20Our%20diffusion-based%20approach%20leverages%20clean%20speech%0Apriors%20to%20assess%20quality%20based%20on%20how%20the%20input%20relates%20to%20the%20learned%0Adistribution%20of%20clean%20data.%20Our%20proposed%20log-likelihoods%20show%20promising%0Aresults%2C%20correlating%20well%20with%20intrusive%20speech%20quality%20metrics%20such%20as%20POLQA%0Aand%20SI-SDR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2410.17834v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNon-intrusive%2520Speech%2520Quality%2520Assessment%2520with%2520Diffusion%2520Models%2520Trained%2520on%250A%2520%2520Clean%2520Speech%26entry.906535625%3DDanilo%2520de%2520Oliveira%2520and%2520Julius%2520Richter%2520and%2520Jean-Marie%2520Lemercier%2520and%2520Simon%2520Welker%2520and%2520Timo%2520Gerkmann%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520found%2520great%2520success%2520in%2520generating%2520high%2520quality%252C%2520natural%250Asamples%2520of%2520speech%252C%2520but%2520their%2520potential%2520for%2520density%2520estimation%2520for%2520speech%2520has%2520so%250Afar%2520remained%2520largely%2520unexplored.%2520In%2520this%2520work%252C%2520we%2520leverage%2520an%2520unconditional%250Adiffusion%2520model%2520trained%2520only%2520on%2520clean%2520speech%2520for%2520the%2520assessment%2520of%2520speech%250Aquality.%2520We%2520show%2520that%2520the%2520quality%2520of%2520a%2520speech%2520utterance%2520can%2520be%2520assessed%2520by%250Aestimating%2520the%2520likelihood%2520of%2520a%2520corresponding%2520sample%2520in%2520the%2520terminating%2520Gaussian%250Adistribution%252C%2520obtained%2520via%2520a%2520deterministic%2520noising%2520process.%2520The%2520resulting%250Amethod%2520is%2520purely%2520unsupervised%252C%2520trained%2520only%2520on%2520clean%2520speech%252C%2520and%2520therefore%2520does%250Anot%2520rely%2520on%2520annotations.%2520Our%2520diffusion-based%2520approach%2520leverages%2520clean%2520speech%250Apriors%2520to%2520assess%2520quality%2520based%2520on%2520how%2520the%2520input%2520relates%2520to%2520the%2520learned%250Adistribution%2520of%2520clean%2520data.%2520Our%2520proposed%2520log-likelihoods%2520show%2520promising%250Aresults%252C%2520correlating%2520well%2520with%2520intrusive%2520speech%2520quality%2520metrics%2520such%2520as%2520POLQA%250Aand%2520SI-SDR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.17834v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Non-intrusive%20Speech%20Quality%20Assessment%20with%20Diffusion%20Models%20Trained%20on%0A%20%20Clean%20Speech&entry.906535625=Danilo%20de%20Oliveira%20and%20Julius%20Richter%20and%20Jean-Marie%20Lemercier%20and%20Simon%20Welker%20and%20Timo%20Gerkmann&entry.1292438233=%20%20Diffusion%20models%20have%20found%20great%20success%20in%20generating%20high%20quality%2C%20natural%0Asamples%20of%20speech%2C%20but%20their%20potential%20for%20density%20estimation%20for%20speech%20has%20so%0Afar%20remained%20largely%20unexplored.%20In%20this%20work%2C%20we%20leverage%20an%20unconditional%0Adiffusion%20model%20trained%20only%20on%20clean%20speech%20for%20the%20assessment%20of%20speech%0Aquality.%20We%20show%20that%20the%20quality%20of%20a%20speech%20utterance%20can%20be%20assessed%20by%0Aestimating%20the%20likelihood%20of%20a%20corresponding%20sample%20in%20the%20terminating%20Gaussian%0Adistribution%2C%20obtained%20via%20a%20deterministic%20noising%20process.%20The%20resulting%0Amethod%20is%20purely%20unsupervised%2C%20trained%20only%20on%20clean%20speech%2C%20and%20therefore%20does%0Anot%20rely%20on%20annotations.%20Our%20diffusion-based%20approach%20leverages%20clean%20speech%0Apriors%20to%20assess%20quality%20based%20on%20how%20the%20input%20relates%20to%20the%20learned%0Adistribution%20of%20clean%20data.%20Our%20proposed%20log-likelihoods%20show%20promising%0Aresults%2C%20correlating%20well%20with%20intrusive%20speech%20quality%20metrics%20such%20as%20POLQA%0Aand%20SI-SDR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2410.17834v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


