<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251008.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "RGS-DR: Deferred Reflections and Residual Shading in 2D Gaussian\n  Splatting", "author": "Georgios Kouros and Minye Wu and Tinne Tuytelaars", "abstract": "  In this work, we address specular appearance in inverse rendering using 2D\nGaussian splatting with deferred shading and argue for a refinement stage to\nimprove specular detail, thereby bridging the gap with reconstruction-only\nmethods. Our pipeline estimates editable material properties and environment\nillumination while employing a directional residual pass that captures leftover\nview-dependent effects for further refining novel view synthesis. In contrast\nto per-Gaussian shading with shortest-axis normals and normal residuals, which\ntends to result in more noisy geometry and specular appearance, a\npixel-deferred surfel formulation with specular residuals yields sharper\nhighlights, cleaner materials, and improved editability. We evaluate our\napproach on rendering and reconstruction quality on three popular datasets\nfeaturing glossy objects, and also demonstrate high-quality relighting and\nmaterial editing.\n", "link": "http://arxiv.org/abs/2504.18468v5", "date": "2025-10-08", "relevancy": 3.1971, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6815}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6478}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5889}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGS-DR%3A%20Deferred%20Reflections%20and%20Residual%20Shading%20in%202D%20Gaussian%0A%20%20Splatting&body=Title%3A%20RGS-DR%3A%20Deferred%20Reflections%20and%20Residual%20Shading%20in%202D%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Georgios%20Kouros%20and%20Minye%20Wu%20and%20Tinne%20Tuytelaars%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20address%20specular%20appearance%20in%20inverse%20rendering%20using%202D%0AGaussian%20splatting%20with%20deferred%20shading%20and%20argue%20for%20a%20refinement%20stage%20to%0Aimprove%20specular%20detail%2C%20thereby%20bridging%20the%20gap%20with%20reconstruction-only%0Amethods.%20Our%20pipeline%20estimates%20editable%20material%20properties%20and%20environment%0Aillumination%20while%20employing%20a%20directional%20residual%20pass%20that%20captures%20leftover%0Aview-dependent%20effects%20for%20further%20refining%20novel%20view%20synthesis.%20In%20contrast%0Ato%20per-Gaussian%20shading%20with%20shortest-axis%20normals%20and%20normal%20residuals%2C%20which%0Atends%20to%20result%20in%20more%20noisy%20geometry%20and%20specular%20appearance%2C%20a%0Apixel-deferred%20surfel%20formulation%20with%20specular%20residuals%20yields%20sharper%0Ahighlights%2C%20cleaner%20materials%2C%20and%20improved%20editability.%20We%20evaluate%20our%0Aapproach%20on%20rendering%20and%20reconstruction%20quality%20on%20three%20popular%20datasets%0Afeaturing%20glossy%20objects%2C%20and%20also%20demonstrate%20high-quality%20relighting%20and%0Amaterial%20editing.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2504.18468v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGS-DR%253A%2520Deferred%2520Reflections%2520and%2520Residual%2520Shading%2520in%25202D%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DGeorgios%2520Kouros%2520and%2520Minye%2520Wu%2520and%2520Tinne%2520Tuytelaars%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520address%2520specular%2520appearance%2520in%2520inverse%2520rendering%2520using%25202D%250AGaussian%2520splatting%2520with%2520deferred%2520shading%2520and%2520argue%2520for%2520a%2520refinement%2520stage%2520to%250Aimprove%2520specular%2520detail%252C%2520thereby%2520bridging%2520the%2520gap%2520with%2520reconstruction-only%250Amethods.%2520Our%2520pipeline%2520estimates%2520editable%2520material%2520properties%2520and%2520environment%250Aillumination%2520while%2520employing%2520a%2520directional%2520residual%2520pass%2520that%2520captures%2520leftover%250Aview-dependent%2520effects%2520for%2520further%2520refining%2520novel%2520view%2520synthesis.%2520In%2520contrast%250Ato%2520per-Gaussian%2520shading%2520with%2520shortest-axis%2520normals%2520and%2520normal%2520residuals%252C%2520which%250Atends%2520to%2520result%2520in%2520more%2520noisy%2520geometry%2520and%2520specular%2520appearance%252C%2520a%250Apixel-deferred%2520surfel%2520formulation%2520with%2520specular%2520residuals%2520yields%2520sharper%250Ahighlights%252C%2520cleaner%2520materials%252C%2520and%2520improved%2520editability.%2520We%2520evaluate%2520our%250Aapproach%2520on%2520rendering%2520and%2520reconstruction%2520quality%2520on%2520three%2520popular%2520datasets%250Afeaturing%2520glossy%2520objects%252C%2520and%2520also%2520demonstrate%2520high-quality%2520relighting%2520and%250Amaterial%2520editing.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18468v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGS-DR%3A%20Deferred%20Reflections%20and%20Residual%20Shading%20in%202D%20Gaussian%0A%20%20Splatting&entry.906535625=Georgios%20Kouros%20and%20Minye%20Wu%20and%20Tinne%20Tuytelaars&entry.1292438233=%20%20In%20this%20work%2C%20we%20address%20specular%20appearance%20in%20inverse%20rendering%20using%202D%0AGaussian%20splatting%20with%20deferred%20shading%20and%20argue%20for%20a%20refinement%20stage%20to%0Aimprove%20specular%20detail%2C%20thereby%20bridging%20the%20gap%20with%20reconstruction-only%0Amethods.%20Our%20pipeline%20estimates%20editable%20material%20properties%20and%20environment%0Aillumination%20while%20employing%20a%20directional%20residual%20pass%20that%20captures%20leftover%0Aview-dependent%20effects%20for%20further%20refining%20novel%20view%20synthesis.%20In%20contrast%0Ato%20per-Gaussian%20shading%20with%20shortest-axis%20normals%20and%20normal%20residuals%2C%20which%0Atends%20to%20result%20in%20more%20noisy%20geometry%20and%20specular%20appearance%2C%20a%0Apixel-deferred%20surfel%20formulation%20with%20specular%20residuals%20yields%20sharper%0Ahighlights%2C%20cleaner%20materials%2C%20and%20improved%20editability.%20We%20evaluate%20our%0Aapproach%20on%20rendering%20and%20reconstruction%20quality%20on%20three%20popular%20datasets%0Afeaturing%20glossy%20objects%2C%20and%20also%20demonstrate%20high-quality%20relighting%20and%0Amaterial%20editing.%0A&entry.1838667208=http%3A//arxiv.org/abs/2504.18468v5&entry.124074799=Read"},
{"title": "MMPerspective: Do MLLMs Understand Perspective? A Comprehensive\n  Benchmark for Perspective Perception, Reasoning, and Robustness", "author": "Yolo Yunlong Tang and Pinxin Liu and Mingqian Feng and Zhangyun Tan and Rui Mao and Chao Huang and Jing Bi and Yunzhong Xiao and Susan Liang and Hang Hua and Ali Vosoughi and Luchuan Song and Zeliang Zhang and Chenliang Xu", "abstract": "  Understanding perspective is fundamental to human visual perception, yet the\nextent to which multimodal large language models (MLLMs) internalize\nperspective geometry remains unclear. We introduce MMPerspective, the first\nbenchmark specifically designed to systematically evaluate MLLMs' understanding\nof perspective through 10 carefully crafted tasks across three complementary\ndimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark\ncomprises 2,711 real-world and synthetic image instances with 5,083\nquestion-answer pairs that probe key capabilities, such as vanishing point\nperception and counting, perspective type reasoning, line relationship\nunderstanding in 3D space, invariance to perspective-preserving\ntransformations, etc. Through a comprehensive evaluation of 43 state-of-the-art\nMLLMs, we uncover significant limitations: while models demonstrate competence\non surface-level perceptual tasks, they struggle with compositional reasoning\nand maintaining spatial consistency under perturbations. Our analysis further\nreveals intriguing patterns between model architecture, scale, and perspective\ncapabilities, highlighting both robustness bottlenecks and the benefits of\nchain-of-thought prompting. MMPerspective establishes a valuable testbed for\ndiagnosing and advancing spatial understanding in vision-language systems.\nResources available at: https://yunlong10.github.io/MMPerspective/\n", "link": "http://arxiv.org/abs/2505.20426v2", "date": "2025-10-08", "relevancy": 2.9617, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6174}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6174}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5423}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MMPerspective%3A%20Do%20MLLMs%20Understand%20Perspective%3F%20A%20Comprehensive%0A%20%20Benchmark%20for%20Perspective%20Perception%2C%20Reasoning%2C%20and%20Robustness&body=Title%3A%20MMPerspective%3A%20Do%20MLLMs%20Understand%20Perspective%3F%20A%20Comprehensive%0A%20%20Benchmark%20for%20Perspective%20Perception%2C%20Reasoning%2C%20and%20Robustness%0AAuthor%3A%20Yolo%20Yunlong%20Tang%20and%20Pinxin%20Liu%20and%20Mingqian%20Feng%20and%20Zhangyun%20Tan%20and%20Rui%20Mao%20and%20Chao%20Huang%20and%20Jing%20Bi%20and%20Yunzhong%20Xiao%20and%20Susan%20Liang%20and%20Hang%20Hua%20and%20Ali%20Vosoughi%20and%20Luchuan%20Song%20and%20Zeliang%20Zhang%20and%20Chenliang%20Xu%0AAbstract%3A%20%20%20Understanding%20perspective%20is%20fundamental%20to%20human%20visual%20perception%2C%20yet%20the%0Aextent%20to%20which%20multimodal%20large%20language%20models%20%28MLLMs%29%20internalize%0Aperspective%20geometry%20remains%20unclear.%20We%20introduce%20MMPerspective%2C%20the%20first%0Abenchmark%20specifically%20designed%20to%20systematically%20evaluate%20MLLMs%27%20understanding%0Aof%20perspective%20through%2010%20carefully%20crafted%20tasks%20across%20three%20complementary%0Adimensions%3A%20Perspective%20Perception%2C%20Reasoning%2C%20and%20Robustness.%20Our%20benchmark%0Acomprises%202%2C711%20real-world%20and%20synthetic%20image%20instances%20with%205%2C083%0Aquestion-answer%20pairs%20that%20probe%20key%20capabilities%2C%20such%20as%20vanishing%20point%0Aperception%20and%20counting%2C%20perspective%20type%20reasoning%2C%20line%20relationship%0Aunderstanding%20in%203D%20space%2C%20invariance%20to%20perspective-preserving%0Atransformations%2C%20etc.%20Through%20a%20comprehensive%20evaluation%20of%2043%20state-of-the-art%0AMLLMs%2C%20we%20uncover%20significant%20limitations%3A%20while%20models%20demonstrate%20competence%0Aon%20surface-level%20perceptual%20tasks%2C%20they%20struggle%20with%20compositional%20reasoning%0Aand%20maintaining%20spatial%20consistency%20under%20perturbations.%20Our%20analysis%20further%0Areveals%20intriguing%20patterns%20between%20model%20architecture%2C%20scale%2C%20and%20perspective%0Acapabilities%2C%20highlighting%20both%20robustness%20bottlenecks%20and%20the%20benefits%20of%0Achain-of-thought%20prompting.%20MMPerspective%20establishes%20a%20valuable%20testbed%20for%0Adiagnosing%20and%20advancing%20spatial%20understanding%20in%20vision-language%20systems.%0AResources%20available%20at%3A%20https%3A//yunlong10.github.io/MMPerspective/%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.20426v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMMPerspective%253A%2520Do%2520MLLMs%2520Understand%2520Perspective%253F%2520A%2520Comprehensive%250A%2520%2520Benchmark%2520for%2520Perspective%2520Perception%252C%2520Reasoning%252C%2520and%2520Robustness%26entry.906535625%3DYolo%2520Yunlong%2520Tang%2520and%2520Pinxin%2520Liu%2520and%2520Mingqian%2520Feng%2520and%2520Zhangyun%2520Tan%2520and%2520Rui%2520Mao%2520and%2520Chao%2520Huang%2520and%2520Jing%2520Bi%2520and%2520Yunzhong%2520Xiao%2520and%2520Susan%2520Liang%2520and%2520Hang%2520Hua%2520and%2520Ali%2520Vosoughi%2520and%2520Luchuan%2520Song%2520and%2520Zeliang%2520Zhang%2520and%2520Chenliang%2520Xu%26entry.1292438233%3D%2520%2520Understanding%2520perspective%2520is%2520fundamental%2520to%2520human%2520visual%2520perception%252C%2520yet%2520the%250Aextent%2520to%2520which%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520internalize%250Aperspective%2520geometry%2520remains%2520unclear.%2520We%2520introduce%2520MMPerspective%252C%2520the%2520first%250Abenchmark%2520specifically%2520designed%2520to%2520systematically%2520evaluate%2520MLLMs%2527%2520understanding%250Aof%2520perspective%2520through%252010%2520carefully%2520crafted%2520tasks%2520across%2520three%2520complementary%250Adimensions%253A%2520Perspective%2520Perception%252C%2520Reasoning%252C%2520and%2520Robustness.%2520Our%2520benchmark%250Acomprises%25202%252C711%2520real-world%2520and%2520synthetic%2520image%2520instances%2520with%25205%252C083%250Aquestion-answer%2520pairs%2520that%2520probe%2520key%2520capabilities%252C%2520such%2520as%2520vanishing%2520point%250Aperception%2520and%2520counting%252C%2520perspective%2520type%2520reasoning%252C%2520line%2520relationship%250Aunderstanding%2520in%25203D%2520space%252C%2520invariance%2520to%2520perspective-preserving%250Atransformations%252C%2520etc.%2520Through%2520a%2520comprehensive%2520evaluation%2520of%252043%2520state-of-the-art%250AMLLMs%252C%2520we%2520uncover%2520significant%2520limitations%253A%2520while%2520models%2520demonstrate%2520competence%250Aon%2520surface-level%2520perceptual%2520tasks%252C%2520they%2520struggle%2520with%2520compositional%2520reasoning%250Aand%2520maintaining%2520spatial%2520consistency%2520under%2520perturbations.%2520Our%2520analysis%2520further%250Areveals%2520intriguing%2520patterns%2520between%2520model%2520architecture%252C%2520scale%252C%2520and%2520perspective%250Acapabilities%252C%2520highlighting%2520both%2520robustness%2520bottlenecks%2520and%2520the%2520benefits%2520of%250Achain-of-thought%2520prompting.%2520MMPerspective%2520establishes%2520a%2520valuable%2520testbed%2520for%250Adiagnosing%2520and%2520advancing%2520spatial%2520understanding%2520in%2520vision-language%2520systems.%250AResources%2520available%2520at%253A%2520https%253A//yunlong10.github.io/MMPerspective/%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20426v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MMPerspective%3A%20Do%20MLLMs%20Understand%20Perspective%3F%20A%20Comprehensive%0A%20%20Benchmark%20for%20Perspective%20Perception%2C%20Reasoning%2C%20and%20Robustness&entry.906535625=Yolo%20Yunlong%20Tang%20and%20Pinxin%20Liu%20and%20Mingqian%20Feng%20and%20Zhangyun%20Tan%20and%20Rui%20Mao%20and%20Chao%20Huang%20and%20Jing%20Bi%20and%20Yunzhong%20Xiao%20and%20Susan%20Liang%20and%20Hang%20Hua%20and%20Ali%20Vosoughi%20and%20Luchuan%20Song%20and%20Zeliang%20Zhang%20and%20Chenliang%20Xu&entry.1292438233=%20%20Understanding%20perspective%20is%20fundamental%20to%20human%20visual%20perception%2C%20yet%20the%0Aextent%20to%20which%20multimodal%20large%20language%20models%20%28MLLMs%29%20internalize%0Aperspective%20geometry%20remains%20unclear.%20We%20introduce%20MMPerspective%2C%20the%20first%0Abenchmark%20specifically%20designed%20to%20systematically%20evaluate%20MLLMs%27%20understanding%0Aof%20perspective%20through%2010%20carefully%20crafted%20tasks%20across%20three%20complementary%0Adimensions%3A%20Perspective%20Perception%2C%20Reasoning%2C%20and%20Robustness.%20Our%20benchmark%0Acomprises%202%2C711%20real-world%20and%20synthetic%20image%20instances%20with%205%2C083%0Aquestion-answer%20pairs%20that%20probe%20key%20capabilities%2C%20such%20as%20vanishing%20point%0Aperception%20and%20counting%2C%20perspective%20type%20reasoning%2C%20line%20relationship%0Aunderstanding%20in%203D%20space%2C%20invariance%20to%20perspective-preserving%0Atransformations%2C%20etc.%20Through%20a%20comprehensive%20evaluation%20of%2043%20state-of-the-art%0AMLLMs%2C%20we%20uncover%20significant%20limitations%3A%20while%20models%20demonstrate%20competence%0Aon%20surface-level%20perceptual%20tasks%2C%20they%20struggle%20with%20compositional%20reasoning%0Aand%20maintaining%20spatial%20consistency%20under%20perturbations.%20Our%20analysis%20further%0Areveals%20intriguing%20patterns%20between%20model%20architecture%2C%20scale%2C%20and%20perspective%0Acapabilities%2C%20highlighting%20both%20robustness%20bottlenecks%20and%20the%20benefits%20of%0Achain-of-thought%20prompting.%20MMPerspective%20establishes%20a%20valuable%20testbed%20for%0Adiagnosing%20and%20advancing%20spatial%20understanding%20in%20vision-language%20systems.%0AResources%20available%20at%3A%20https%3A//yunlong10.github.io/MMPerspective/%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.20426v2&entry.124074799=Read"},
{"title": "VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?", "author": "Yolo Yunlong Tang and Junjia Guo and Hang Hua and Susan Liang and Mingqian Feng and Xinyang Li and Rui Mao and Chao Huang and Jing Bi and Zeliang Zhang and Pooyan Fazli and Chenliang Xu", "abstract": "  The advancement of Multimodal Large Language Models (MLLMs) has enabled\nsignificant progress in multimodal understanding, expanding their capacity to\nanalyze video content. However, existing evaluation benchmarks for MLLMs\nprimarily focus on abstract video comprehension, lacking a detailed assessment\nof their ability to understand video compositions, the nuanced interpretation\nof how visual elements combine and interact within highly compiled video\ncontexts. We introduce VidComposition, a new benchmark specifically designed to\nevaluate the video composition understanding capabilities of MLLMs using\ncarefully curated compiled videos and cinematic-level annotations.\nVidComposition includes 982 videos with 1706 multiple-choice questions,\ncovering various compositional aspects such as camera movement, angle, shot\nsize, narrative structure, character actions and emotions, etc. Our\ncomprehensive evaluation of 33 open-source and proprietary MLLMs reveals a\nsignificant performance gap between human and model capabilities. This\nhighlights the limitations of current MLLMs in understanding complex, compiled\nvideo compositions and offers insights into areas for further improvement. The\nleaderboard and evaluation code are available at\nhttps://yunlong10.github.io/VidComposition/.\n", "link": "http://arxiv.org/abs/2411.10979v4", "date": "2025-10-08", "relevancy": 2.7741, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5781}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5781}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VidComposition%3A%20Can%20MLLMs%20Analyze%20Compositions%20in%20Compiled%20Videos%3F&body=Title%3A%20VidComposition%3A%20Can%20MLLMs%20Analyze%20Compositions%20in%20Compiled%20Videos%3F%0AAuthor%3A%20Yolo%20Yunlong%20Tang%20and%20Junjia%20Guo%20and%20Hang%20Hua%20and%20Susan%20Liang%20and%20Mingqian%20Feng%20and%20Xinyang%20Li%20and%20Rui%20Mao%20and%20Chao%20Huang%20and%20Jing%20Bi%20and%20Zeliang%20Zhang%20and%20Pooyan%20Fazli%20and%20Chenliang%20Xu%0AAbstract%3A%20%20%20The%20advancement%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20enabled%0Asignificant%20progress%20in%20multimodal%20understanding%2C%20expanding%20their%20capacity%20to%0Aanalyze%20video%20content.%20However%2C%20existing%20evaluation%20benchmarks%20for%20MLLMs%0Aprimarily%20focus%20on%20abstract%20video%20comprehension%2C%20lacking%20a%20detailed%20assessment%0Aof%20their%20ability%20to%20understand%20video%20compositions%2C%20the%20nuanced%20interpretation%0Aof%20how%20visual%20elements%20combine%20and%20interact%20within%20highly%20compiled%20video%0Acontexts.%20We%20introduce%20VidComposition%2C%20a%20new%20benchmark%20specifically%20designed%20to%0Aevaluate%20the%20video%20composition%20understanding%20capabilities%20of%20MLLMs%20using%0Acarefully%20curated%20compiled%20videos%20and%20cinematic-level%20annotations.%0AVidComposition%20includes%20982%20videos%20with%201706%20multiple-choice%20questions%2C%0Acovering%20various%20compositional%20aspects%20such%20as%20camera%20movement%2C%20angle%2C%20shot%0Asize%2C%20narrative%20structure%2C%20character%20actions%20and%20emotions%2C%20etc.%20Our%0Acomprehensive%20evaluation%20of%2033%20open-source%20and%20proprietary%20MLLMs%20reveals%20a%0Asignificant%20performance%20gap%20between%20human%20and%20model%20capabilities.%20This%0Ahighlights%20the%20limitations%20of%20current%20MLLMs%20in%20understanding%20complex%2C%20compiled%0Avideo%20compositions%20and%20offers%20insights%20into%20areas%20for%20further%20improvement.%20The%0Aleaderboard%20and%20evaluation%20code%20are%20available%20at%0Ahttps%3A//yunlong10.github.io/VidComposition/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2411.10979v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVidComposition%253A%2520Can%2520MLLMs%2520Analyze%2520Compositions%2520in%2520Compiled%2520Videos%253F%26entry.906535625%3DYolo%2520Yunlong%2520Tang%2520and%2520Junjia%2520Guo%2520and%2520Hang%2520Hua%2520and%2520Susan%2520Liang%2520and%2520Mingqian%2520Feng%2520and%2520Xinyang%2520Li%2520and%2520Rui%2520Mao%2520and%2520Chao%2520Huang%2520and%2520Jing%2520Bi%2520and%2520Zeliang%2520Zhang%2520and%2520Pooyan%2520Fazli%2520and%2520Chenliang%2520Xu%26entry.1292438233%3D%2520%2520The%2520advancement%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%2520enabled%250Asignificant%2520progress%2520in%2520multimodal%2520understanding%252C%2520expanding%2520their%2520capacity%2520to%250Aanalyze%2520video%2520content.%2520However%252C%2520existing%2520evaluation%2520benchmarks%2520for%2520MLLMs%250Aprimarily%2520focus%2520on%2520abstract%2520video%2520comprehension%252C%2520lacking%2520a%2520detailed%2520assessment%250Aof%2520their%2520ability%2520to%2520understand%2520video%2520compositions%252C%2520the%2520nuanced%2520interpretation%250Aof%2520how%2520visual%2520elements%2520combine%2520and%2520interact%2520within%2520highly%2520compiled%2520video%250Acontexts.%2520We%2520introduce%2520VidComposition%252C%2520a%2520new%2520benchmark%2520specifically%2520designed%2520to%250Aevaluate%2520the%2520video%2520composition%2520understanding%2520capabilities%2520of%2520MLLMs%2520using%250Acarefully%2520curated%2520compiled%2520videos%2520and%2520cinematic-level%2520annotations.%250AVidComposition%2520includes%2520982%2520videos%2520with%25201706%2520multiple-choice%2520questions%252C%250Acovering%2520various%2520compositional%2520aspects%2520such%2520as%2520camera%2520movement%252C%2520angle%252C%2520shot%250Asize%252C%2520narrative%2520structure%252C%2520character%2520actions%2520and%2520emotions%252C%2520etc.%2520Our%250Acomprehensive%2520evaluation%2520of%252033%2520open-source%2520and%2520proprietary%2520MLLMs%2520reveals%2520a%250Asignificant%2520performance%2520gap%2520between%2520human%2520and%2520model%2520capabilities.%2520This%250Ahighlights%2520the%2520limitations%2520of%2520current%2520MLLMs%2520in%2520understanding%2520complex%252C%2520compiled%250Avideo%2520compositions%2520and%2520offers%2520insights%2520into%2520areas%2520for%2520further%2520improvement.%2520The%250Aleaderboard%2520and%2520evaluation%2520code%2520are%2520available%2520at%250Ahttps%253A//yunlong10.github.io/VidComposition/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.10979v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VidComposition%3A%20Can%20MLLMs%20Analyze%20Compositions%20in%20Compiled%20Videos%3F&entry.906535625=Yolo%20Yunlong%20Tang%20and%20Junjia%20Guo%20and%20Hang%20Hua%20and%20Susan%20Liang%20and%20Mingqian%20Feng%20and%20Xinyang%20Li%20and%20Rui%20Mao%20and%20Chao%20Huang%20and%20Jing%20Bi%20and%20Zeliang%20Zhang%20and%20Pooyan%20Fazli%20and%20Chenliang%20Xu&entry.1292438233=%20%20The%20advancement%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20enabled%0Asignificant%20progress%20in%20multimodal%20understanding%2C%20expanding%20their%20capacity%20to%0Aanalyze%20video%20content.%20However%2C%20existing%20evaluation%20benchmarks%20for%20MLLMs%0Aprimarily%20focus%20on%20abstract%20video%20comprehension%2C%20lacking%20a%20detailed%20assessment%0Aof%20their%20ability%20to%20understand%20video%20compositions%2C%20the%20nuanced%20interpretation%0Aof%20how%20visual%20elements%20combine%20and%20interact%20within%20highly%20compiled%20video%0Acontexts.%20We%20introduce%20VidComposition%2C%20a%20new%20benchmark%20specifically%20designed%20to%0Aevaluate%20the%20video%20composition%20understanding%20capabilities%20of%20MLLMs%20using%0Acarefully%20curated%20compiled%20videos%20and%20cinematic-level%20annotations.%0AVidComposition%20includes%20982%20videos%20with%201706%20multiple-choice%20questions%2C%0Acovering%20various%20compositional%20aspects%20such%20as%20camera%20movement%2C%20angle%2C%20shot%0Asize%2C%20narrative%20structure%2C%20character%20actions%20and%20emotions%2C%20etc.%20Our%0Acomprehensive%20evaluation%20of%2033%20open-source%20and%20proprietary%20MLLMs%20reveals%20a%0Asignificant%20performance%20gap%20between%20human%20and%20model%20capabilities.%20This%0Ahighlights%20the%20limitations%20of%20current%20MLLMs%20in%20understanding%20complex%2C%20compiled%0Avideo%20compositions%20and%20offers%20insights%20into%20areas%20for%20further%20improvement.%20The%0Aleaderboard%20and%20evaluation%20code%20are%20available%20at%0Ahttps%3A//yunlong10.github.io/VidComposition/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2411.10979v4&entry.124074799=Read"},
{"title": "LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event\n  Boundary Captioning", "author": "Yolo Yunlong Tang and Jinrui Zhang and Xiangchen Wang and Teng Wang and Feng Zheng", "abstract": "  Our winning entry for the CVPR 2023 Generic Event Boundary Captioning (GEBC)\ncompetition is detailed in this paper. Unlike conventional video captioning\ntasks, GEBC demands that the captioning model possess an understanding of\nimmediate changes in status around the designated video boundary, making it a\ndifficult task. This paper proposes an effective model LLMVA-GEBC (Large\nLanguage Model with Video Adapter for Generic Event Boundary Captioning): (1)\nWe utilize a pretrained LLM for generating human-like captions with high\nquality. (2) To adapt the model to the GEBC task, we take the video Q-former as\nan adapter and train it with the frozen visual feature extractors and LLM. Our\nproposed method achieved a 76.14 score on the test set and won the first place\nin the challenge. Our code is available at\nhttps://github.com/zjr2000/LLMVA-GEBC .\n", "link": "http://arxiv.org/abs/2306.10354v2", "date": "2025-10-08", "relevancy": 2.7228, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5586}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5375}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMVA-GEBC%3A%20Large%20Language%20Model%20with%20Video%20Adapter%20for%20Generic%20Event%0A%20%20Boundary%20Captioning&body=Title%3A%20LLMVA-GEBC%3A%20Large%20Language%20Model%20with%20Video%20Adapter%20for%20Generic%20Event%0A%20%20Boundary%20Captioning%0AAuthor%3A%20Yolo%20Yunlong%20Tang%20and%20Jinrui%20Zhang%20and%20Xiangchen%20Wang%20and%20Teng%20Wang%20and%20Feng%20Zheng%0AAbstract%3A%20%20%20Our%20winning%20entry%20for%20the%20CVPR%202023%20Generic%20Event%20Boundary%20Captioning%20%28GEBC%29%0Acompetition%20is%20detailed%20in%20this%20paper.%20Unlike%20conventional%20video%20captioning%0Atasks%2C%20GEBC%20demands%20that%20the%20captioning%20model%20possess%20an%20understanding%20of%0Aimmediate%20changes%20in%20status%20around%20the%20designated%20video%20boundary%2C%20making%20it%20a%0Adifficult%20task.%20This%20paper%20proposes%20an%20effective%20model%20LLMVA-GEBC%20%28Large%0ALanguage%20Model%20with%20Video%20Adapter%20for%20Generic%20Event%20Boundary%20Captioning%29%3A%20%281%29%0AWe%20utilize%20a%20pretrained%20LLM%20for%20generating%20human-like%20captions%20with%20high%0Aquality.%20%282%29%20To%20adapt%20the%20model%20to%20the%20GEBC%20task%2C%20we%20take%20the%20video%20Q-former%20as%0Aan%20adapter%20and%20train%20it%20with%20the%20frozen%20visual%20feature%20extractors%20and%20LLM.%20Our%0Aproposed%20method%20achieved%20a%2076.14%20score%20on%20the%20test%20set%20and%20won%20the%20first%20place%0Ain%20the%20challenge.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/zjr2000/LLMVA-GEBC%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2306.10354v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMVA-GEBC%253A%2520Large%2520Language%2520Model%2520with%2520Video%2520Adapter%2520for%2520Generic%2520Event%250A%2520%2520Boundary%2520Captioning%26entry.906535625%3DYolo%2520Yunlong%2520Tang%2520and%2520Jinrui%2520Zhang%2520and%2520Xiangchen%2520Wang%2520and%2520Teng%2520Wang%2520and%2520Feng%2520Zheng%26entry.1292438233%3D%2520%2520Our%2520winning%2520entry%2520for%2520the%2520CVPR%25202023%2520Generic%2520Event%2520Boundary%2520Captioning%2520%2528GEBC%2529%250Acompetition%2520is%2520detailed%2520in%2520this%2520paper.%2520Unlike%2520conventional%2520video%2520captioning%250Atasks%252C%2520GEBC%2520demands%2520that%2520the%2520captioning%2520model%2520possess%2520an%2520understanding%2520of%250Aimmediate%2520changes%2520in%2520status%2520around%2520the%2520designated%2520video%2520boundary%252C%2520making%2520it%2520a%250Adifficult%2520task.%2520This%2520paper%2520proposes%2520an%2520effective%2520model%2520LLMVA-GEBC%2520%2528Large%250ALanguage%2520Model%2520with%2520Video%2520Adapter%2520for%2520Generic%2520Event%2520Boundary%2520Captioning%2529%253A%2520%25281%2529%250AWe%2520utilize%2520a%2520pretrained%2520LLM%2520for%2520generating%2520human-like%2520captions%2520with%2520high%250Aquality.%2520%25282%2529%2520To%2520adapt%2520the%2520model%2520to%2520the%2520GEBC%2520task%252C%2520we%2520take%2520the%2520video%2520Q-former%2520as%250Aan%2520adapter%2520and%2520train%2520it%2520with%2520the%2520frozen%2520visual%2520feature%2520extractors%2520and%2520LLM.%2520Our%250Aproposed%2520method%2520achieved%2520a%252076.14%2520score%2520on%2520the%2520test%2520set%2520and%2520won%2520the%2520first%2520place%250Ain%2520the%2520challenge.%2520Our%2520code%2520is%2520available%2520at%250Ahttps%253A//github.com/zjr2000/LLMVA-GEBC%2520.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2306.10354v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMVA-GEBC%3A%20Large%20Language%20Model%20with%20Video%20Adapter%20for%20Generic%20Event%0A%20%20Boundary%20Captioning&entry.906535625=Yolo%20Yunlong%20Tang%20and%20Jinrui%20Zhang%20and%20Xiangchen%20Wang%20and%20Teng%20Wang%20and%20Feng%20Zheng&entry.1292438233=%20%20Our%20winning%20entry%20for%20the%20CVPR%202023%20Generic%20Event%20Boundary%20Captioning%20%28GEBC%29%0Acompetition%20is%20detailed%20in%20this%20paper.%20Unlike%20conventional%20video%20captioning%0Atasks%2C%20GEBC%20demands%20that%20the%20captioning%20model%20possess%20an%20understanding%20of%0Aimmediate%20changes%20in%20status%20around%20the%20designated%20video%20boundary%2C%20making%20it%20a%0Adifficult%20task.%20This%20paper%20proposes%20an%20effective%20model%20LLMVA-GEBC%20%28Large%0ALanguage%20Model%20with%20Video%20Adapter%20for%20Generic%20Event%20Boundary%20Captioning%29%3A%20%281%29%0AWe%20utilize%20a%20pretrained%20LLM%20for%20generating%20human-like%20captions%20with%20high%0Aquality.%20%282%29%20To%20adapt%20the%20model%20to%20the%20GEBC%20task%2C%20we%20take%20the%20video%20Q-former%20as%0Aan%20adapter%20and%20train%20it%20with%20the%20frozen%20visual%20feature%20extractors%20and%20LLM.%20Our%0Aproposed%20method%20achieved%20a%2076.14%20score%20on%20the%20test%20set%20and%20won%20the%20first%20place%0Ain%20the%20challenge.%20Our%20code%20is%20available%20at%0Ahttps%3A//github.com/zjr2000/LLMVA-GEBC%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2306.10354v2&entry.124074799=Read"},
{"title": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs", "author": "Peize He and Zichen Wen and Yubo Wang and Yuxuan Wang and Xiaoqian Liu and Jiajie Huang and Zehui Lei and Zhuangcheng Gu and Xiangqi Jin and Jiabing Yang and Kai Li and Zhifei Liu and Weijia Li and Cunxiang Wang and Conghui He and Linfeng Zhang", "abstract": "  Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks.\n", "link": "http://arxiv.org/abs/2510.07293v1", "date": "2025-10-08", "relevancy": 2.7142, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5675}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5675}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4936}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AudioMarathon%3A%20A%20Comprehensive%20Benchmark%20for%20Long-Context%20Audio%0A%20%20Understanding%20and%20Efficiency%20in%20Audio%20LLMs&body=Title%3A%20AudioMarathon%3A%20A%20Comprehensive%20Benchmark%20for%20Long-Context%20Audio%0A%20%20Understanding%20and%20Efficiency%20in%20Audio%20LLMs%0AAuthor%3A%20Peize%20He%20and%20Zichen%20Wen%20and%20Yubo%20Wang%20and%20Yuxuan%20Wang%20and%20Xiaoqian%20Liu%20and%20Jiajie%20Huang%20and%20Zehui%20Lei%20and%20Zhuangcheng%20Gu%20and%20Xiangqi%20Jin%20and%20Jiabing%20Yang%20and%20Kai%20Li%20and%20Zhifei%20Liu%20and%20Weijia%20Li%20and%20Cunxiang%20Wang%20and%20Conghui%20He%20and%20Linfeng%20Zhang%0AAbstract%3A%20%20%20Processing%20long-form%20audio%20is%20a%20major%20challenge%20for%20Large%20Audio%20Language%0Amodels%20%28LALMs%29.%20These%20models%20struggle%20with%20the%20quadratic%20cost%20of%20attention%0A%28%24O%28N%5E2%29%24%29%20and%20with%20modeling%20long-range%20temporal%20dependencies.%20Existing%20audio%0Abenchmarks%20are%20built%20mostly%20from%20short%20clips%20and%20do%20not%20evaluate%20models%20in%0Arealistic%20long%20context%20settings.%20To%20address%20this%20gap%2C%20we%20introduce%0AAudioMarathon%2C%20a%20benchmark%20designed%20to%20evaluate%20both%20understanding%20and%0Ainference%20efficiency%20on%20long-form%20audio.%20AudioMarathon%20provides%20a%20diverse%20set%0Aof%20tasks%20built%20upon%20three%20pillars%3A%20long-context%20audio%20inputs%20with%20durations%0Aranging%20from%2090.0%20to%20300.0%20seconds%2C%20which%20correspond%20to%20encoded%20sequences%20of%0A2%2C250%20to%207%2C500%20audio%20tokens%2C%20respectively%2C%20full%20domain%20coverage%20across%20speech%2C%0Asound%2C%20and%20music%2C%20and%20complex%20reasoning%20that%20requires%20multi-hop%20inference.%20We%0Aevaluate%20state-of-the-art%20LALMs%20and%20observe%20clear%20performance%20drops%20as%20audio%0Alength%20grows.%20We%20also%20study%20acceleration%20techniques%20and%20analyze%20the%20trade-offs%0Aof%20token%20pruning%20and%20KV%20cache%20eviction.%20The%20results%20show%20large%20gaps%20across%0Acurrent%20LALMs%20and%20highlight%20the%20need%20for%20better%20temporal%20reasoning%20and%0Amemory-efficient%20architectures.%20We%20believe%20AudioMarathon%20will%20drive%20the%20audio%0Aand%20multimodal%20research%20community%20to%20develop%20more%20advanced%20audio%20understanding%0Amodels%20capable%20of%20solving%20complex%20audio%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07293v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAudioMarathon%253A%2520A%2520Comprehensive%2520Benchmark%2520for%2520Long-Context%2520Audio%250A%2520%2520Understanding%2520and%2520Efficiency%2520in%2520Audio%2520LLMs%26entry.906535625%3DPeize%2520He%2520and%2520Zichen%2520Wen%2520and%2520Yubo%2520Wang%2520and%2520Yuxuan%2520Wang%2520and%2520Xiaoqian%2520Liu%2520and%2520Jiajie%2520Huang%2520and%2520Zehui%2520Lei%2520and%2520Zhuangcheng%2520Gu%2520and%2520Xiangqi%2520Jin%2520and%2520Jiabing%2520Yang%2520and%2520Kai%2520Li%2520and%2520Zhifei%2520Liu%2520and%2520Weijia%2520Li%2520and%2520Cunxiang%2520Wang%2520and%2520Conghui%2520He%2520and%2520Linfeng%2520Zhang%26entry.1292438233%3D%2520%2520Processing%2520long-form%2520audio%2520is%2520a%2520major%2520challenge%2520for%2520Large%2520Audio%2520Language%250Amodels%2520%2528LALMs%2529.%2520These%2520models%2520struggle%2520with%2520the%2520quadratic%2520cost%2520of%2520attention%250A%2528%2524O%2528N%255E2%2529%2524%2529%2520and%2520with%2520modeling%2520long-range%2520temporal%2520dependencies.%2520Existing%2520audio%250Abenchmarks%2520are%2520built%2520mostly%2520from%2520short%2520clips%2520and%2520do%2520not%2520evaluate%2520models%2520in%250Arealistic%2520long%2520context%2520settings.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%250AAudioMarathon%252C%2520a%2520benchmark%2520designed%2520to%2520evaluate%2520both%2520understanding%2520and%250Ainference%2520efficiency%2520on%2520long-form%2520audio.%2520AudioMarathon%2520provides%2520a%2520diverse%2520set%250Aof%2520tasks%2520built%2520upon%2520three%2520pillars%253A%2520long-context%2520audio%2520inputs%2520with%2520durations%250Aranging%2520from%252090.0%2520to%2520300.0%2520seconds%252C%2520which%2520correspond%2520to%2520encoded%2520sequences%2520of%250A2%252C250%2520to%25207%252C500%2520audio%2520tokens%252C%2520respectively%252C%2520full%2520domain%2520coverage%2520across%2520speech%252C%250Asound%252C%2520and%2520music%252C%2520and%2520complex%2520reasoning%2520that%2520requires%2520multi-hop%2520inference.%2520We%250Aevaluate%2520state-of-the-art%2520LALMs%2520and%2520observe%2520clear%2520performance%2520drops%2520as%2520audio%250Alength%2520grows.%2520We%2520also%2520study%2520acceleration%2520techniques%2520and%2520analyze%2520the%2520trade-offs%250Aof%2520token%2520pruning%2520and%2520KV%2520cache%2520eviction.%2520The%2520results%2520show%2520large%2520gaps%2520across%250Acurrent%2520LALMs%2520and%2520highlight%2520the%2520need%2520for%2520better%2520temporal%2520reasoning%2520and%250Amemory-efficient%2520architectures.%2520We%2520believe%2520AudioMarathon%2520will%2520drive%2520the%2520audio%250Aand%2520multimodal%2520research%2520community%2520to%2520develop%2520more%2520advanced%2520audio%2520understanding%250Amodels%2520capable%2520of%2520solving%2520complex%2520audio%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07293v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AudioMarathon%3A%20A%20Comprehensive%20Benchmark%20for%20Long-Context%20Audio%0A%20%20Understanding%20and%20Efficiency%20in%20Audio%20LLMs&entry.906535625=Peize%20He%20and%20Zichen%20Wen%20and%20Yubo%20Wang%20and%20Yuxuan%20Wang%20and%20Xiaoqian%20Liu%20and%20Jiajie%20Huang%20and%20Zehui%20Lei%20and%20Zhuangcheng%20Gu%20and%20Xiangqi%20Jin%20and%20Jiabing%20Yang%20and%20Kai%20Li%20and%20Zhifei%20Liu%20and%20Weijia%20Li%20and%20Cunxiang%20Wang%20and%20Conghui%20He%20and%20Linfeng%20Zhang&entry.1292438233=%20%20Processing%20long-form%20audio%20is%20a%20major%20challenge%20for%20Large%20Audio%20Language%0Amodels%20%28LALMs%29.%20These%20models%20struggle%20with%20the%20quadratic%20cost%20of%20attention%0A%28%24O%28N%5E2%29%24%29%20and%20with%20modeling%20long-range%20temporal%20dependencies.%20Existing%20audio%0Abenchmarks%20are%20built%20mostly%20from%20short%20clips%20and%20do%20not%20evaluate%20models%20in%0Arealistic%20long%20context%20settings.%20To%20address%20this%20gap%2C%20we%20introduce%0AAudioMarathon%2C%20a%20benchmark%20designed%20to%20evaluate%20both%20understanding%20and%0Ainference%20efficiency%20on%20long-form%20audio.%20AudioMarathon%20provides%20a%20diverse%20set%0Aof%20tasks%20built%20upon%20three%20pillars%3A%20long-context%20audio%20inputs%20with%20durations%0Aranging%20from%2090.0%20to%20300.0%20seconds%2C%20which%20correspond%20to%20encoded%20sequences%20of%0A2%2C250%20to%207%2C500%20audio%20tokens%2C%20respectively%2C%20full%20domain%20coverage%20across%20speech%2C%0Asound%2C%20and%20music%2C%20and%20complex%20reasoning%20that%20requires%20multi-hop%20inference.%20We%0Aevaluate%20state-of-the-art%20LALMs%20and%20observe%20clear%20performance%20drops%20as%20audio%0Alength%20grows.%20We%20also%20study%20acceleration%20techniques%20and%20analyze%20the%20trade-offs%0Aof%20token%20pruning%20and%20KV%20cache%20eviction.%20The%20results%20show%20large%20gaps%20across%0Acurrent%20LALMs%20and%20highlight%20the%20need%20for%20better%20temporal%20reasoning%20and%0Amemory-efficient%20architectures.%20We%20believe%20AudioMarathon%20will%20drive%20the%20audio%0Aand%20multimodal%20research%20community%20to%20develop%20more%20advanced%20audio%20understanding%0Amodels%20capable%20of%20solving%20complex%20audio%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07293v1&entry.124074799=Read"},
{"title": "Prefilled responses enhance zero-shot detection of AI-generated images", "author": "Zoher Kachwala and Danishjeet Singh and Danielle Yang and Filippo Menczer", "abstract": "  As AI models generate increasingly realistic images, growing concerns over\npotential misuse underscore the need for reliable detection. Traditional\nsupervised detection methods depend on large, curated datasets for training and\noften fail to generalize to novel, out-of-domain image generators. As an\nalternative, we explore pre-trained Vision-Language Models (VLMs) for zero-shot\ndetection of AI-generated images. We evaluate VLM performance on three diverse\nbenchmarks encompassing synthetic images of human faces, objects, and animals\nproduced by 16 different state-of-the-art image generators. While off-the-shelf\nVLMs perform poorly on these datasets, we find that their reasoning can be\nguided effectively through simple response prefilling -- a method we call\nPrefill-Guided Thinking (PGT). In particular, prefilling a VLM response with\nthe task-aligned phrase \"Let's examine the style and the synthesis artifacts\"\nimproves the Macro F1 scores of three widely used open-source VLMs by up to\n24%.\n", "link": "http://arxiv.org/abs/2506.11031v3", "date": "2025-10-08", "relevancy": 2.6216, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5306}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5248}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5176}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Prefilled%20responses%20enhance%20zero-shot%20detection%20of%20AI-generated%20images&body=Title%3A%20Prefilled%20responses%20enhance%20zero-shot%20detection%20of%20AI-generated%20images%0AAuthor%3A%20Zoher%20Kachwala%20and%20Danishjeet%20Singh%20and%20Danielle%20Yang%20and%20Filippo%20Menczer%0AAbstract%3A%20%20%20As%20AI%20models%20generate%20increasingly%20realistic%20images%2C%20growing%20concerns%20over%0Apotential%20misuse%20underscore%20the%20need%20for%20reliable%20detection.%20Traditional%0Asupervised%20detection%20methods%20depend%20on%20large%2C%20curated%20datasets%20for%20training%20and%0Aoften%20fail%20to%20generalize%20to%20novel%2C%20out-of-domain%20image%20generators.%20As%20an%0Aalternative%2C%20we%20explore%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20for%20zero-shot%0Adetection%20of%20AI-generated%20images.%20We%20evaluate%20VLM%20performance%20on%20three%20diverse%0Abenchmarks%20encompassing%20synthetic%20images%20of%20human%20faces%2C%20objects%2C%20and%20animals%0Aproduced%20by%2016%20different%20state-of-the-art%20image%20generators.%20While%20off-the-shelf%0AVLMs%20perform%20poorly%20on%20these%20datasets%2C%20we%20find%20that%20their%20reasoning%20can%20be%0Aguided%20effectively%20through%20simple%20response%20prefilling%20--%20a%20method%20we%20call%0APrefill-Guided%20Thinking%20%28PGT%29.%20In%20particular%2C%20prefilling%20a%20VLM%20response%20with%0Athe%20task-aligned%20phrase%20%22Let%27s%20examine%20the%20style%20and%20the%20synthesis%20artifacts%22%0Aimproves%20the%20Macro%20F1%20scores%20of%20three%20widely%20used%20open-source%20VLMs%20by%20up%20to%0A24%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.11031v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrefilled%2520responses%2520enhance%2520zero-shot%2520detection%2520of%2520AI-generated%2520images%26entry.906535625%3DZoher%2520Kachwala%2520and%2520Danishjeet%2520Singh%2520and%2520Danielle%2520Yang%2520and%2520Filippo%2520Menczer%26entry.1292438233%3D%2520%2520As%2520AI%2520models%2520generate%2520increasingly%2520realistic%2520images%252C%2520growing%2520concerns%2520over%250Apotential%2520misuse%2520underscore%2520the%2520need%2520for%2520reliable%2520detection.%2520Traditional%250Asupervised%2520detection%2520methods%2520depend%2520on%2520large%252C%2520curated%2520datasets%2520for%2520training%2520and%250Aoften%2520fail%2520to%2520generalize%2520to%2520novel%252C%2520out-of-domain%2520image%2520generators.%2520As%2520an%250Aalternative%252C%2520we%2520explore%2520pre-trained%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520for%2520zero-shot%250Adetection%2520of%2520AI-generated%2520images.%2520We%2520evaluate%2520VLM%2520performance%2520on%2520three%2520diverse%250Abenchmarks%2520encompassing%2520synthetic%2520images%2520of%2520human%2520faces%252C%2520objects%252C%2520and%2520animals%250Aproduced%2520by%252016%2520different%2520state-of-the-art%2520image%2520generators.%2520While%2520off-the-shelf%250AVLMs%2520perform%2520poorly%2520on%2520these%2520datasets%252C%2520we%2520find%2520that%2520their%2520reasoning%2520can%2520be%250Aguided%2520effectively%2520through%2520simple%2520response%2520prefilling%2520--%2520a%2520method%2520we%2520call%250APrefill-Guided%2520Thinking%2520%2528PGT%2529.%2520In%2520particular%252C%2520prefilling%2520a%2520VLM%2520response%2520with%250Athe%2520task-aligned%2520phrase%2520%2522Let%2527s%2520examine%2520the%2520style%2520and%2520the%2520synthesis%2520artifacts%2522%250Aimproves%2520the%2520Macro%2520F1%2520scores%2520of%2520three%2520widely%2520used%2520open-source%2520VLMs%2520by%2520up%2520to%250A24%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.11031v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prefilled%20responses%20enhance%20zero-shot%20detection%20of%20AI-generated%20images&entry.906535625=Zoher%20Kachwala%20and%20Danishjeet%20Singh%20and%20Danielle%20Yang%20and%20Filippo%20Menczer&entry.1292438233=%20%20As%20AI%20models%20generate%20increasingly%20realistic%20images%2C%20growing%20concerns%20over%0Apotential%20misuse%20underscore%20the%20need%20for%20reliable%20detection.%20Traditional%0Asupervised%20detection%20methods%20depend%20on%20large%2C%20curated%20datasets%20for%20training%20and%0Aoften%20fail%20to%20generalize%20to%20novel%2C%20out-of-domain%20image%20generators.%20As%20an%0Aalternative%2C%20we%20explore%20pre-trained%20Vision-Language%20Models%20%28VLMs%29%20for%20zero-shot%0Adetection%20of%20AI-generated%20images.%20We%20evaluate%20VLM%20performance%20on%20three%20diverse%0Abenchmarks%20encompassing%20synthetic%20images%20of%20human%20faces%2C%20objects%2C%20and%20animals%0Aproduced%20by%2016%20different%20state-of-the-art%20image%20generators.%20While%20off-the-shelf%0AVLMs%20perform%20poorly%20on%20these%20datasets%2C%20we%20find%20that%20their%20reasoning%20can%20be%0Aguided%20effectively%20through%20simple%20response%20prefilling%20--%20a%20method%20we%20call%0APrefill-Guided%20Thinking%20%28PGT%29.%20In%20particular%2C%20prefilling%20a%20VLM%20response%20with%0Athe%20task-aligned%20phrase%20%22Let%27s%20examine%20the%20style%20and%20the%20synthesis%20artifacts%22%0Aimproves%20the%20Macro%20F1%20scores%20of%20three%20widely%20used%20open-source%20VLMs%20by%20up%20to%0A24%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.11031v3&entry.124074799=Read"},
{"title": "Agentic generative AI for media content discovery at the national\n  football league", "author": "Henry Wang and Md Sirajus Salekin and Jake Lee and Ross Claytor and Shinan Zhang and Michael Chi", "abstract": "  Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines.\n", "link": "http://arxiv.org/abs/2510.07297v1", "date": "2025-10-08", "relevancy": 2.594, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5394}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5302}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4868}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agentic%20generative%20AI%20for%20media%20content%20discovery%20at%20the%20national%0A%20%20football%20league&body=Title%3A%20Agentic%20generative%20AI%20for%20media%20content%20discovery%20at%20the%20national%0A%20%20football%20league%0AAuthor%3A%20Henry%20Wang%20and%20Md%20Sirajus%20Salekin%20and%20Jake%20Lee%20and%20Ross%20Claytor%20and%20Shinan%20Zhang%20and%20Michael%20Chi%0AAbstract%3A%20%20%20Generative%20AI%20has%20unlocked%20new%20possibilities%20in%20content%20discovery%20and%0Amanagement.%20Through%20collaboration%20with%20the%20National%20Football%20League%20%28NFL%29%2C%20we%0Ademonstrate%20how%20a%20generative-AI%20based%20workflow%20enables%20media%20researchers%20and%0Aanalysts%20to%20query%20relevant%20historical%20plays%20using%20natural%20language%20rather%20than%0Atraditional%20filter-and-click%20interfaces.%20The%20agentic%20workflow%20takes%20a%20user%0Aquery%20as%20input%2C%20breaks%20it%20into%20elements%2C%20and%20translates%20them%20into%20the%0Aunderlying%20database%20query%20language.%20Accuracy%20and%20latency%20are%20further%20improved%0Athrough%20carefully%20designed%20semantic%20caching.%20The%20solution%20achieves%20over%2095%0Apercent%20accuracy%20and%20reduces%20the%20average%20time%20to%20find%20relevant%20videos%20from%2010%0Aminutes%20to%2030%20seconds%2C%20significantly%20increasing%20the%20NFL%27s%20operational%0Aefficiency%20and%20allowing%20users%20to%20focus%20on%20producing%20creative%20content%20and%0Aengaging%20storylines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentic%2520generative%2520AI%2520for%2520media%2520content%2520discovery%2520at%2520the%2520national%250A%2520%2520football%2520league%26entry.906535625%3DHenry%2520Wang%2520and%2520Md%2520Sirajus%2520Salekin%2520and%2520Jake%2520Lee%2520and%2520Ross%2520Claytor%2520and%2520Shinan%2520Zhang%2520and%2520Michael%2520Chi%26entry.1292438233%3D%2520%2520Generative%2520AI%2520has%2520unlocked%2520new%2520possibilities%2520in%2520content%2520discovery%2520and%250Amanagement.%2520Through%2520collaboration%2520with%2520the%2520National%2520Football%2520League%2520%2528NFL%2529%252C%2520we%250Ademonstrate%2520how%2520a%2520generative-AI%2520based%2520workflow%2520enables%2520media%2520researchers%2520and%250Aanalysts%2520to%2520query%2520relevant%2520historical%2520plays%2520using%2520natural%2520language%2520rather%2520than%250Atraditional%2520filter-and-click%2520interfaces.%2520The%2520agentic%2520workflow%2520takes%2520a%2520user%250Aquery%2520as%2520input%252C%2520breaks%2520it%2520into%2520elements%252C%2520and%2520translates%2520them%2520into%2520the%250Aunderlying%2520database%2520query%2520language.%2520Accuracy%2520and%2520latency%2520are%2520further%2520improved%250Athrough%2520carefully%2520designed%2520semantic%2520caching.%2520The%2520solution%2520achieves%2520over%252095%250Apercent%2520accuracy%2520and%2520reduces%2520the%2520average%2520time%2520to%2520find%2520relevant%2520videos%2520from%252010%250Aminutes%2520to%252030%2520seconds%252C%2520significantly%2520increasing%2520the%2520NFL%2527s%2520operational%250Aefficiency%2520and%2520allowing%2520users%2520to%2520focus%2520on%2520producing%2520creative%2520content%2520and%250Aengaging%2520storylines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agentic%20generative%20AI%20for%20media%20content%20discovery%20at%20the%20national%0A%20%20football%20league&entry.906535625=Henry%20Wang%20and%20Md%20Sirajus%20Salekin%20and%20Jake%20Lee%20and%20Ross%20Claytor%20and%20Shinan%20Zhang%20and%20Michael%20Chi&entry.1292438233=%20%20Generative%20AI%20has%20unlocked%20new%20possibilities%20in%20content%20discovery%20and%0Amanagement.%20Through%20collaboration%20with%20the%20National%20Football%20League%20%28NFL%29%2C%20we%0Ademonstrate%20how%20a%20generative-AI%20based%20workflow%20enables%20media%20researchers%20and%0Aanalysts%20to%20query%20relevant%20historical%20plays%20using%20natural%20language%20rather%20than%0Atraditional%20filter-and-click%20interfaces.%20The%20agentic%20workflow%20takes%20a%20user%0Aquery%20as%20input%2C%20breaks%20it%20into%20elements%2C%20and%20translates%20them%20into%20the%0Aunderlying%20database%20query%20language.%20Accuracy%20and%20latency%20are%20further%20improved%0Athrough%20carefully%20designed%20semantic%20caching.%20The%20solution%20achieves%20over%2095%0Apercent%20accuracy%20and%20reduces%20the%20average%20time%20to%20find%20relevant%20videos%20from%2010%0Aminutes%20to%2030%20seconds%2C%20significantly%20increasing%20the%20NFL%27s%20operational%0Aefficiency%20and%20allowing%20users%20to%20focus%20on%20producing%20creative%20content%20and%0Aengaging%20storylines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07297v1&entry.124074799=Read"},
{"title": "Lossy Neural Compression for Geospatial Analytics: A Review", "author": "Carlos Gomes and Isabelle Wittmann and Damien Robert and Johannes Jakubik and Tim Reichelt and Michele Martone and Stefano Maurogiovanni and Rikard Vinge and Jonas Hurst and Erik Scheurer and Rocco Sedona and Thomas Brunschwiler and Stefan Kesselheim and Matej Batic and Philip Stier and Jan Dirk Wegner and Gabriele Cavallaro and Edzer Pebesma and Michael Marszalek and Miguel A Belenguer-Plomer and Kennedy Adriko and Paolo Fraccaro and Romeo Kienzler and Rania Briq and Sabrina Benassou and Michele Lazzarini and Conrad M Albrecht", "abstract": "  Over the past decades, there has been an explosion in the amount of available\nEarth Observation (EO) data. The unprecedented coverage of the Earth's surface\nand atmosphere by satellite imagery has resulted in large volumes of data that\nmust be transmitted to ground stations, stored in data centers, and distributed\nto end users. Modern Earth System Models (ESMs) face similar challenges,\noperating at high spatial and temporal resolutions, producing petabytes of data\nper simulated day. Data compression has gained relevance over the past decade,\nwith neural compression (NC) emerging from deep learning and information\ntheory, making EO data and ESM outputs ideal candidates due to their abundance\nof unlabeled data. In this review, we outline recent developments in NC applied\nto geospatial data. We introduce the fundamental concepts of NC including\nseminal works in its traditional applications to image and video compression\ndomains with focus on lossy compression. We discuss the unique characteristics\nof EO and ESM data, contrasting them with \"natural images\", and explain the\nadditional challenges and opportunities they present. Moreover, we review\ncurrent applications of NC across various EO modalities and explore the limited\nefforts in ESM compression to date. The advent of self-supervised learning\n(SSL) and foundation models (FM) has advanced methods to efficiently distill\nrepresentations from vast unlabeled data. We connect these developments to NC\nfor EO, highlighting the similarities between the two fields and elaborate on\nthe potential of transferring compressed feature representations for\nmachine--to--machine communication. Based on insights drawn from this review,\nwe devise future directions relevant to applications in EO and ESM.\n", "link": "http://arxiv.org/abs/2503.01505v2", "date": "2025-10-08", "relevancy": 2.541, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5184}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5059}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lossy%20Neural%20Compression%20for%20Geospatial%20Analytics%3A%20A%20Review&body=Title%3A%20Lossy%20Neural%20Compression%20for%20Geospatial%20Analytics%3A%20A%20Review%0AAuthor%3A%20Carlos%20Gomes%20and%20Isabelle%20Wittmann%20and%20Damien%20Robert%20and%20Johannes%20Jakubik%20and%20Tim%20Reichelt%20and%20Michele%20Martone%20and%20Stefano%20Maurogiovanni%20and%20Rikard%20Vinge%20and%20Jonas%20Hurst%20and%20Erik%20Scheurer%20and%20Rocco%20Sedona%20and%20Thomas%20Brunschwiler%20and%20Stefan%20Kesselheim%20and%20Matej%20Batic%20and%20Philip%20Stier%20and%20Jan%20Dirk%20Wegner%20and%20Gabriele%20Cavallaro%20and%20Edzer%20Pebesma%20and%20Michael%20Marszalek%20and%20Miguel%20A%20Belenguer-Plomer%20and%20Kennedy%20Adriko%20and%20Paolo%20Fraccaro%20and%20Romeo%20Kienzler%20and%20Rania%20Briq%20and%20Sabrina%20Benassou%20and%20Michele%20Lazzarini%20and%20Conrad%20M%20Albrecht%0AAbstract%3A%20%20%20Over%20the%20past%20decades%2C%20there%20has%20been%20an%20explosion%20in%20the%20amount%20of%20available%0AEarth%20Observation%20%28EO%29%20data.%20The%20unprecedented%20coverage%20of%20the%20Earth%27s%20surface%0Aand%20atmosphere%20by%20satellite%20imagery%20has%20resulted%20in%20large%20volumes%20of%20data%20that%0Amust%20be%20transmitted%20to%20ground%20stations%2C%20stored%20in%20data%20centers%2C%20and%20distributed%0Ato%20end%20users.%20Modern%20Earth%20System%20Models%20%28ESMs%29%20face%20similar%20challenges%2C%0Aoperating%20at%20high%20spatial%20and%20temporal%20resolutions%2C%20producing%20petabytes%20of%20data%0Aper%20simulated%20day.%20Data%20compression%20has%20gained%20relevance%20over%20the%20past%20decade%2C%0Awith%20neural%20compression%20%28NC%29%20emerging%20from%20deep%20learning%20and%20information%0Atheory%2C%20making%20EO%20data%20and%20ESM%20outputs%20ideal%20candidates%20due%20to%20their%20abundance%0Aof%20unlabeled%20data.%20In%20this%20review%2C%20we%20outline%20recent%20developments%20in%20NC%20applied%0Ato%20geospatial%20data.%20We%20introduce%20the%20fundamental%20concepts%20of%20NC%20including%0Aseminal%20works%20in%20its%20traditional%20applications%20to%20image%20and%20video%20compression%0Adomains%20with%20focus%20on%20lossy%20compression.%20We%20discuss%20the%20unique%20characteristics%0Aof%20EO%20and%20ESM%20data%2C%20contrasting%20them%20with%20%22natural%20images%22%2C%20and%20explain%20the%0Aadditional%20challenges%20and%20opportunities%20they%20present.%20Moreover%2C%20we%20review%0Acurrent%20applications%20of%20NC%20across%20various%20EO%20modalities%20and%20explore%20the%20limited%0Aefforts%20in%20ESM%20compression%20to%20date.%20The%20advent%20of%20self-supervised%20learning%0A%28SSL%29%20and%20foundation%20models%20%28FM%29%20has%20advanced%20methods%20to%20efficiently%20distill%0Arepresentations%20from%20vast%20unlabeled%20data.%20We%20connect%20these%20developments%20to%20NC%0Afor%20EO%2C%20highlighting%20the%20similarities%20between%20the%20two%20fields%20and%20elaborate%20on%0Athe%20potential%20of%20transferring%20compressed%20feature%20representations%20for%0Amachine--to--machine%20communication.%20Based%20on%20insights%20drawn%20from%20this%20review%2C%0Awe%20devise%20future%20directions%20relevant%20to%20applications%20in%20EO%20and%20ESM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.01505v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLossy%2520Neural%2520Compression%2520for%2520Geospatial%2520Analytics%253A%2520A%2520Review%26entry.906535625%3DCarlos%2520Gomes%2520and%2520Isabelle%2520Wittmann%2520and%2520Damien%2520Robert%2520and%2520Johannes%2520Jakubik%2520and%2520Tim%2520Reichelt%2520and%2520Michele%2520Martone%2520and%2520Stefano%2520Maurogiovanni%2520and%2520Rikard%2520Vinge%2520and%2520Jonas%2520Hurst%2520and%2520Erik%2520Scheurer%2520and%2520Rocco%2520Sedona%2520and%2520Thomas%2520Brunschwiler%2520and%2520Stefan%2520Kesselheim%2520and%2520Matej%2520Batic%2520and%2520Philip%2520Stier%2520and%2520Jan%2520Dirk%2520Wegner%2520and%2520Gabriele%2520Cavallaro%2520and%2520Edzer%2520Pebesma%2520and%2520Michael%2520Marszalek%2520and%2520Miguel%2520A%2520Belenguer-Plomer%2520and%2520Kennedy%2520Adriko%2520and%2520Paolo%2520Fraccaro%2520and%2520Romeo%2520Kienzler%2520and%2520Rania%2520Briq%2520and%2520Sabrina%2520Benassou%2520and%2520Michele%2520Lazzarini%2520and%2520Conrad%2520M%2520Albrecht%26entry.1292438233%3D%2520%2520Over%2520the%2520past%2520decades%252C%2520there%2520has%2520been%2520an%2520explosion%2520in%2520the%2520amount%2520of%2520available%250AEarth%2520Observation%2520%2528EO%2529%2520data.%2520The%2520unprecedented%2520coverage%2520of%2520the%2520Earth%2527s%2520surface%250Aand%2520atmosphere%2520by%2520satellite%2520imagery%2520has%2520resulted%2520in%2520large%2520volumes%2520of%2520data%2520that%250Amust%2520be%2520transmitted%2520to%2520ground%2520stations%252C%2520stored%2520in%2520data%2520centers%252C%2520and%2520distributed%250Ato%2520end%2520users.%2520Modern%2520Earth%2520System%2520Models%2520%2528ESMs%2529%2520face%2520similar%2520challenges%252C%250Aoperating%2520at%2520high%2520spatial%2520and%2520temporal%2520resolutions%252C%2520producing%2520petabytes%2520of%2520data%250Aper%2520simulated%2520day.%2520Data%2520compression%2520has%2520gained%2520relevance%2520over%2520the%2520past%2520decade%252C%250Awith%2520neural%2520compression%2520%2528NC%2529%2520emerging%2520from%2520deep%2520learning%2520and%2520information%250Atheory%252C%2520making%2520EO%2520data%2520and%2520ESM%2520outputs%2520ideal%2520candidates%2520due%2520to%2520their%2520abundance%250Aof%2520unlabeled%2520data.%2520In%2520this%2520review%252C%2520we%2520outline%2520recent%2520developments%2520in%2520NC%2520applied%250Ato%2520geospatial%2520data.%2520We%2520introduce%2520the%2520fundamental%2520concepts%2520of%2520NC%2520including%250Aseminal%2520works%2520in%2520its%2520traditional%2520applications%2520to%2520image%2520and%2520video%2520compression%250Adomains%2520with%2520focus%2520on%2520lossy%2520compression.%2520We%2520discuss%2520the%2520unique%2520characteristics%250Aof%2520EO%2520and%2520ESM%2520data%252C%2520contrasting%2520them%2520with%2520%2522natural%2520images%2522%252C%2520and%2520explain%2520the%250Aadditional%2520challenges%2520and%2520opportunities%2520they%2520present.%2520Moreover%252C%2520we%2520review%250Acurrent%2520applications%2520of%2520NC%2520across%2520various%2520EO%2520modalities%2520and%2520explore%2520the%2520limited%250Aefforts%2520in%2520ESM%2520compression%2520to%2520date.%2520The%2520advent%2520of%2520self-supervised%2520learning%250A%2528SSL%2529%2520and%2520foundation%2520models%2520%2528FM%2529%2520has%2520advanced%2520methods%2520to%2520efficiently%2520distill%250Arepresentations%2520from%2520vast%2520unlabeled%2520data.%2520We%2520connect%2520these%2520developments%2520to%2520NC%250Afor%2520EO%252C%2520highlighting%2520the%2520similarities%2520between%2520the%2520two%2520fields%2520and%2520elaborate%2520on%250Athe%2520potential%2520of%2520transferring%2520compressed%2520feature%2520representations%2520for%250Amachine--to--machine%2520communication.%2520Based%2520on%2520insights%2520drawn%2520from%2520this%2520review%252C%250Awe%2520devise%2520future%2520directions%2520relevant%2520to%2520applications%2520in%2520EO%2520and%2520ESM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.01505v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lossy%20Neural%20Compression%20for%20Geospatial%20Analytics%3A%20A%20Review&entry.906535625=Carlos%20Gomes%20and%20Isabelle%20Wittmann%20and%20Damien%20Robert%20and%20Johannes%20Jakubik%20and%20Tim%20Reichelt%20and%20Michele%20Martone%20and%20Stefano%20Maurogiovanni%20and%20Rikard%20Vinge%20and%20Jonas%20Hurst%20and%20Erik%20Scheurer%20and%20Rocco%20Sedona%20and%20Thomas%20Brunschwiler%20and%20Stefan%20Kesselheim%20and%20Matej%20Batic%20and%20Philip%20Stier%20and%20Jan%20Dirk%20Wegner%20and%20Gabriele%20Cavallaro%20and%20Edzer%20Pebesma%20and%20Michael%20Marszalek%20and%20Miguel%20A%20Belenguer-Plomer%20and%20Kennedy%20Adriko%20and%20Paolo%20Fraccaro%20and%20Romeo%20Kienzler%20and%20Rania%20Briq%20and%20Sabrina%20Benassou%20and%20Michele%20Lazzarini%20and%20Conrad%20M%20Albrecht&entry.1292438233=%20%20Over%20the%20past%20decades%2C%20there%20has%20been%20an%20explosion%20in%20the%20amount%20of%20available%0AEarth%20Observation%20%28EO%29%20data.%20The%20unprecedented%20coverage%20of%20the%20Earth%27s%20surface%0Aand%20atmosphere%20by%20satellite%20imagery%20has%20resulted%20in%20large%20volumes%20of%20data%20that%0Amust%20be%20transmitted%20to%20ground%20stations%2C%20stored%20in%20data%20centers%2C%20and%20distributed%0Ato%20end%20users.%20Modern%20Earth%20System%20Models%20%28ESMs%29%20face%20similar%20challenges%2C%0Aoperating%20at%20high%20spatial%20and%20temporal%20resolutions%2C%20producing%20petabytes%20of%20data%0Aper%20simulated%20day.%20Data%20compression%20has%20gained%20relevance%20over%20the%20past%20decade%2C%0Awith%20neural%20compression%20%28NC%29%20emerging%20from%20deep%20learning%20and%20information%0Atheory%2C%20making%20EO%20data%20and%20ESM%20outputs%20ideal%20candidates%20due%20to%20their%20abundance%0Aof%20unlabeled%20data.%20In%20this%20review%2C%20we%20outline%20recent%20developments%20in%20NC%20applied%0Ato%20geospatial%20data.%20We%20introduce%20the%20fundamental%20concepts%20of%20NC%20including%0Aseminal%20works%20in%20its%20traditional%20applications%20to%20image%20and%20video%20compression%0Adomains%20with%20focus%20on%20lossy%20compression.%20We%20discuss%20the%20unique%20characteristics%0Aof%20EO%20and%20ESM%20data%2C%20contrasting%20them%20with%20%22natural%20images%22%2C%20and%20explain%20the%0Aadditional%20challenges%20and%20opportunities%20they%20present.%20Moreover%2C%20we%20review%0Acurrent%20applications%20of%20NC%20across%20various%20EO%20modalities%20and%20explore%20the%20limited%0Aefforts%20in%20ESM%20compression%20to%20date.%20The%20advent%20of%20self-supervised%20learning%0A%28SSL%29%20and%20foundation%20models%20%28FM%29%20has%20advanced%20methods%20to%20efficiently%20distill%0Arepresentations%20from%20vast%20unlabeled%20data.%20We%20connect%20these%20developments%20to%20NC%0Afor%20EO%2C%20highlighting%20the%20similarities%20between%20the%20two%20fields%20and%20elaborate%20on%0Athe%20potential%20of%20transferring%20compressed%20feature%20representations%20for%0Amachine--to--machine%20communication.%20Based%20on%20insights%20drawn%20from%20this%20review%2C%0Awe%20devise%20future%20directions%20relevant%20to%20applications%20in%20EO%20and%20ESM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.01505v2&entry.124074799=Read"},
{"title": "MolGA: Molecular Graph Adaptation with Pre-trained 2D Graph Encoder", "author": "Xingtong Yu and Chang Zhou and Xinming Zhang and Yuan Fang", "abstract": "  Molecular graph representation learning is widely used in chemical and\nbiomedical research. While pre-trained 2D graph encoders have demonstrated\nstrong performance, they overlook the rich molecular domain knowledge\nassociated with submolecular instances (atoms and bonds). While molecular\npre-training approaches incorporate such knowledge into their pre-training\nobjectives, they typically employ designs tailored to a specific type of\nknowledge, lacking the flexibility to integrate diverse knowledge present in\nmolecules. Hence, reusing widely available and well-validated pre-trained 2D\nencoders, while incorporating molecular domain knowledge during downstream\nadaptation, offers a more practical alternative. In this work, we propose\nMolGA, which adapts pre-trained 2D graph encoders to downstream molecular\napplications by flexibly incorporating diverse molecular domain knowledge.\nFirst, we propose a molecular alignment strategy that bridge the gap between\npre-trained topological representations with domain-knowledge representations.\nSecond, we introduce a conditional adaptation mechanism that generates\ninstance-specific tokens to enable fine-grained integration of molecular domain\nknowledge for downstream tasks. Finally, we conduct extensive experiments on\neleven public datasets, demonstrating the effectiveness of MolGA.\n", "link": "http://arxiv.org/abs/2510.07289v1", "date": "2025-10-08", "relevancy": 2.4832, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5123}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4918}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MolGA%3A%20Molecular%20Graph%20Adaptation%20with%20Pre-trained%202D%20Graph%20Encoder&body=Title%3A%20MolGA%3A%20Molecular%20Graph%20Adaptation%20with%20Pre-trained%202D%20Graph%20Encoder%0AAuthor%3A%20Xingtong%20Yu%20and%20Chang%20Zhou%20and%20Xinming%20Zhang%20and%20Yuan%20Fang%0AAbstract%3A%20%20%20Molecular%20graph%20representation%20learning%20is%20widely%20used%20in%20chemical%20and%0Abiomedical%20research.%20While%20pre-trained%202D%20graph%20encoders%20have%20demonstrated%0Astrong%20performance%2C%20they%20overlook%20the%20rich%20molecular%20domain%20knowledge%0Aassociated%20with%20submolecular%20instances%20%28atoms%20and%20bonds%29.%20While%20molecular%0Apre-training%20approaches%20incorporate%20such%20knowledge%20into%20their%20pre-training%0Aobjectives%2C%20they%20typically%20employ%20designs%20tailored%20to%20a%20specific%20type%20of%0Aknowledge%2C%20lacking%20the%20flexibility%20to%20integrate%20diverse%20knowledge%20present%20in%0Amolecules.%20Hence%2C%20reusing%20widely%20available%20and%20well-validated%20pre-trained%202D%0Aencoders%2C%20while%20incorporating%20molecular%20domain%20knowledge%20during%20downstream%0Aadaptation%2C%20offers%20a%20more%20practical%20alternative.%20In%20this%20work%2C%20we%20propose%0AMolGA%2C%20which%20adapts%20pre-trained%202D%20graph%20encoders%20to%20downstream%20molecular%0Aapplications%20by%20flexibly%20incorporating%20diverse%20molecular%20domain%20knowledge.%0AFirst%2C%20we%20propose%20a%20molecular%20alignment%20strategy%20that%20bridge%20the%20gap%20between%0Apre-trained%20topological%20representations%20with%20domain-knowledge%20representations.%0ASecond%2C%20we%20introduce%20a%20conditional%20adaptation%20mechanism%20that%20generates%0Ainstance-specific%20tokens%20to%20enable%20fine-grained%20integration%20of%20molecular%20domain%0Aknowledge%20for%20downstream%20tasks.%20Finally%2C%20we%20conduct%20extensive%20experiments%20on%0Aeleven%20public%20datasets%2C%20demonstrating%20the%20effectiveness%20of%20MolGA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07289v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMolGA%253A%2520Molecular%2520Graph%2520Adaptation%2520with%2520Pre-trained%25202D%2520Graph%2520Encoder%26entry.906535625%3DXingtong%2520Yu%2520and%2520Chang%2520Zhou%2520and%2520Xinming%2520Zhang%2520and%2520Yuan%2520Fang%26entry.1292438233%3D%2520%2520Molecular%2520graph%2520representation%2520learning%2520is%2520widely%2520used%2520in%2520chemical%2520and%250Abiomedical%2520research.%2520While%2520pre-trained%25202D%2520graph%2520encoders%2520have%2520demonstrated%250Astrong%2520performance%252C%2520they%2520overlook%2520the%2520rich%2520molecular%2520domain%2520knowledge%250Aassociated%2520with%2520submolecular%2520instances%2520%2528atoms%2520and%2520bonds%2529.%2520While%2520molecular%250Apre-training%2520approaches%2520incorporate%2520such%2520knowledge%2520into%2520their%2520pre-training%250Aobjectives%252C%2520they%2520typically%2520employ%2520designs%2520tailored%2520to%2520a%2520specific%2520type%2520of%250Aknowledge%252C%2520lacking%2520the%2520flexibility%2520to%2520integrate%2520diverse%2520knowledge%2520present%2520in%250Amolecules.%2520Hence%252C%2520reusing%2520widely%2520available%2520and%2520well-validated%2520pre-trained%25202D%250Aencoders%252C%2520while%2520incorporating%2520molecular%2520domain%2520knowledge%2520during%2520downstream%250Aadaptation%252C%2520offers%2520a%2520more%2520practical%2520alternative.%2520In%2520this%2520work%252C%2520we%2520propose%250AMolGA%252C%2520which%2520adapts%2520pre-trained%25202D%2520graph%2520encoders%2520to%2520downstream%2520molecular%250Aapplications%2520by%2520flexibly%2520incorporating%2520diverse%2520molecular%2520domain%2520knowledge.%250AFirst%252C%2520we%2520propose%2520a%2520molecular%2520alignment%2520strategy%2520that%2520bridge%2520the%2520gap%2520between%250Apre-trained%2520topological%2520representations%2520with%2520domain-knowledge%2520representations.%250ASecond%252C%2520we%2520introduce%2520a%2520conditional%2520adaptation%2520mechanism%2520that%2520generates%250Ainstance-specific%2520tokens%2520to%2520enable%2520fine-grained%2520integration%2520of%2520molecular%2520domain%250Aknowledge%2520for%2520downstream%2520tasks.%2520Finally%252C%2520we%2520conduct%2520extensive%2520experiments%2520on%250Aeleven%2520public%2520datasets%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520MolGA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07289v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MolGA%3A%20Molecular%20Graph%20Adaptation%20with%20Pre-trained%202D%20Graph%20Encoder&entry.906535625=Xingtong%20Yu%20and%20Chang%20Zhou%20and%20Xinming%20Zhang%20and%20Yuan%20Fang&entry.1292438233=%20%20Molecular%20graph%20representation%20learning%20is%20widely%20used%20in%20chemical%20and%0Abiomedical%20research.%20While%20pre-trained%202D%20graph%20encoders%20have%20demonstrated%0Astrong%20performance%2C%20they%20overlook%20the%20rich%20molecular%20domain%20knowledge%0Aassociated%20with%20submolecular%20instances%20%28atoms%20and%20bonds%29.%20While%20molecular%0Apre-training%20approaches%20incorporate%20such%20knowledge%20into%20their%20pre-training%0Aobjectives%2C%20they%20typically%20employ%20designs%20tailored%20to%20a%20specific%20type%20of%0Aknowledge%2C%20lacking%20the%20flexibility%20to%20integrate%20diverse%20knowledge%20present%20in%0Amolecules.%20Hence%2C%20reusing%20widely%20available%20and%20well-validated%20pre-trained%202D%0Aencoders%2C%20while%20incorporating%20molecular%20domain%20knowledge%20during%20downstream%0Aadaptation%2C%20offers%20a%20more%20practical%20alternative.%20In%20this%20work%2C%20we%20propose%0AMolGA%2C%20which%20adapts%20pre-trained%202D%20graph%20encoders%20to%20downstream%20molecular%0Aapplications%20by%20flexibly%20incorporating%20diverse%20molecular%20domain%20knowledge.%0AFirst%2C%20we%20propose%20a%20molecular%20alignment%20strategy%20that%20bridge%20the%20gap%20between%0Apre-trained%20topological%20representations%20with%20domain-knowledge%20representations.%0ASecond%2C%20we%20introduce%20a%20conditional%20adaptation%20mechanism%20that%20generates%0Ainstance-specific%20tokens%20to%20enable%20fine-grained%20integration%20of%20molecular%20domain%0Aknowledge%20for%20downstream%20tasks.%20Finally%2C%20we%20conduct%20extensive%20experiments%20on%0Aeleven%20public%20datasets%2C%20demonstrating%20the%20effectiveness%20of%20MolGA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07289v1&entry.124074799=Read"},
{"title": "MATRIX: Mask Track Alignment for Interaction-aware Video Generation", "author": "Siyoon Jin and Seongchan Kim and Dahyun Chung and Jaeho Lee and Hyunwook Choi and Jisu Nam and Jiyoung Kim and Seungryong Kim", "abstract": "  Video DiTs have advanced video generation, yet they still struggle to model\nmulti-instance or subject-object interactions. This raises a key question: How\ndo these models internally represent interactions? To answer this, we curate\nMATRIX-11K, a video dataset with interaction-aware captions and multi-instance\nmask tracks. Using this dataset, we conduct a systematic analysis that\nformalizes two perspectives of video DiTs: semantic grounding, via\nvideo-to-text attention, which evaluates whether noun and verb tokens capture\ninstances and their relations; and semantic propagation, via video-to-video\nattention, which assesses whether instance bindings persist across frames. We\nfind both effects concentrate in a small subset of interaction-dominant layers.\nMotivated by this, we introduce MATRIX, a simple and effective regularization\nthat aligns attention in specific layers of video DiTs with multi-instance mask\ntracks from the MATRIX-11K dataset, enhancing both grounding and propagation.\nWe further propose InterGenEval, an evaluation protocol for interaction-aware\nvideo generation. In experiments, MATRIX improves both interaction fidelity and\nsemantic alignment while reducing drift and hallucination. Extensive ablations\nvalidate our design choices. Codes and weights will be released.\n", "link": "http://arxiv.org/abs/2510.07310v1", "date": "2025-10-08", "relevancy": 2.4794, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6477}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6029}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5926}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MATRIX%3A%20Mask%20Track%20Alignment%20for%20Interaction-aware%20Video%20Generation&body=Title%3A%20MATRIX%3A%20Mask%20Track%20Alignment%20for%20Interaction-aware%20Video%20Generation%0AAuthor%3A%20Siyoon%20Jin%20and%20Seongchan%20Kim%20and%20Dahyun%20Chung%20and%20Jaeho%20Lee%20and%20Hyunwook%20Choi%20and%20Jisu%20Nam%20and%20Jiyoung%20Kim%20and%20Seungryong%20Kim%0AAbstract%3A%20%20%20Video%20DiTs%20have%20advanced%20video%20generation%2C%20yet%20they%20still%20struggle%20to%20model%0Amulti-instance%20or%20subject-object%20interactions.%20This%20raises%20a%20key%20question%3A%20How%0Ado%20these%20models%20internally%20represent%20interactions%3F%20To%20answer%20this%2C%20we%20curate%0AMATRIX-11K%2C%20a%20video%20dataset%20with%20interaction-aware%20captions%20and%20multi-instance%0Amask%20tracks.%20Using%20this%20dataset%2C%20we%20conduct%20a%20systematic%20analysis%20that%0Aformalizes%20two%20perspectives%20of%20video%20DiTs%3A%20semantic%20grounding%2C%20via%0Avideo-to-text%20attention%2C%20which%20evaluates%20whether%20noun%20and%20verb%20tokens%20capture%0Ainstances%20and%20their%20relations%3B%20and%20semantic%20propagation%2C%20via%20video-to-video%0Aattention%2C%20which%20assesses%20whether%20instance%20bindings%20persist%20across%20frames.%20We%0Afind%20both%20effects%20concentrate%20in%20a%20small%20subset%20of%20interaction-dominant%20layers.%0AMotivated%20by%20this%2C%20we%20introduce%20MATRIX%2C%20a%20simple%20and%20effective%20regularization%0Athat%20aligns%20attention%20in%20specific%20layers%20of%20video%20DiTs%20with%20multi-instance%20mask%0Atracks%20from%20the%20MATRIX-11K%20dataset%2C%20enhancing%20both%20grounding%20and%20propagation.%0AWe%20further%20propose%20InterGenEval%2C%20an%20evaluation%20protocol%20for%20interaction-aware%0Avideo%20generation.%20In%20experiments%2C%20MATRIX%20improves%20both%20interaction%20fidelity%20and%0Asemantic%20alignment%20while%20reducing%20drift%20and%20hallucination.%20Extensive%20ablations%0Avalidate%20our%20design%20choices.%20Codes%20and%20weights%20will%20be%20released.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMATRIX%253A%2520Mask%2520Track%2520Alignment%2520for%2520Interaction-aware%2520Video%2520Generation%26entry.906535625%3DSiyoon%2520Jin%2520and%2520Seongchan%2520Kim%2520and%2520Dahyun%2520Chung%2520and%2520Jaeho%2520Lee%2520and%2520Hyunwook%2520Choi%2520and%2520Jisu%2520Nam%2520and%2520Jiyoung%2520Kim%2520and%2520Seungryong%2520Kim%26entry.1292438233%3D%2520%2520Video%2520DiTs%2520have%2520advanced%2520video%2520generation%252C%2520yet%2520they%2520still%2520struggle%2520to%2520model%250Amulti-instance%2520or%2520subject-object%2520interactions.%2520This%2520raises%2520a%2520key%2520question%253A%2520How%250Ado%2520these%2520models%2520internally%2520represent%2520interactions%253F%2520To%2520answer%2520this%252C%2520we%2520curate%250AMATRIX-11K%252C%2520a%2520video%2520dataset%2520with%2520interaction-aware%2520captions%2520and%2520multi-instance%250Amask%2520tracks.%2520Using%2520this%2520dataset%252C%2520we%2520conduct%2520a%2520systematic%2520analysis%2520that%250Aformalizes%2520two%2520perspectives%2520of%2520video%2520DiTs%253A%2520semantic%2520grounding%252C%2520via%250Avideo-to-text%2520attention%252C%2520which%2520evaluates%2520whether%2520noun%2520and%2520verb%2520tokens%2520capture%250Ainstances%2520and%2520their%2520relations%253B%2520and%2520semantic%2520propagation%252C%2520via%2520video-to-video%250Aattention%252C%2520which%2520assesses%2520whether%2520instance%2520bindings%2520persist%2520across%2520frames.%2520We%250Afind%2520both%2520effects%2520concentrate%2520in%2520a%2520small%2520subset%2520of%2520interaction-dominant%2520layers.%250AMotivated%2520by%2520this%252C%2520we%2520introduce%2520MATRIX%252C%2520a%2520simple%2520and%2520effective%2520regularization%250Athat%2520aligns%2520attention%2520in%2520specific%2520layers%2520of%2520video%2520DiTs%2520with%2520multi-instance%2520mask%250Atracks%2520from%2520the%2520MATRIX-11K%2520dataset%252C%2520enhancing%2520both%2520grounding%2520and%2520propagation.%250AWe%2520further%2520propose%2520InterGenEval%252C%2520an%2520evaluation%2520protocol%2520for%2520interaction-aware%250Avideo%2520generation.%2520In%2520experiments%252C%2520MATRIX%2520improves%2520both%2520interaction%2520fidelity%2520and%250Asemantic%2520alignment%2520while%2520reducing%2520drift%2520and%2520hallucination.%2520Extensive%2520ablations%250Avalidate%2520our%2520design%2520choices.%2520Codes%2520and%2520weights%2520will%2520be%2520released.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MATRIX%3A%20Mask%20Track%20Alignment%20for%20Interaction-aware%20Video%20Generation&entry.906535625=Siyoon%20Jin%20and%20Seongchan%20Kim%20and%20Dahyun%20Chung%20and%20Jaeho%20Lee%20and%20Hyunwook%20Choi%20and%20Jisu%20Nam%20and%20Jiyoung%20Kim%20and%20Seungryong%20Kim&entry.1292438233=%20%20Video%20DiTs%20have%20advanced%20video%20generation%2C%20yet%20they%20still%20struggle%20to%20model%0Amulti-instance%20or%20subject-object%20interactions.%20This%20raises%20a%20key%20question%3A%20How%0Ado%20these%20models%20internally%20represent%20interactions%3F%20To%20answer%20this%2C%20we%20curate%0AMATRIX-11K%2C%20a%20video%20dataset%20with%20interaction-aware%20captions%20and%20multi-instance%0Amask%20tracks.%20Using%20this%20dataset%2C%20we%20conduct%20a%20systematic%20analysis%20that%0Aformalizes%20two%20perspectives%20of%20video%20DiTs%3A%20semantic%20grounding%2C%20via%0Avideo-to-text%20attention%2C%20which%20evaluates%20whether%20noun%20and%20verb%20tokens%20capture%0Ainstances%20and%20their%20relations%3B%20and%20semantic%20propagation%2C%20via%20video-to-video%0Aattention%2C%20which%20assesses%20whether%20instance%20bindings%20persist%20across%20frames.%20We%0Afind%20both%20effects%20concentrate%20in%20a%20small%20subset%20of%20interaction-dominant%20layers.%0AMotivated%20by%20this%2C%20we%20introduce%20MATRIX%2C%20a%20simple%20and%20effective%20regularization%0Athat%20aligns%20attention%20in%20specific%20layers%20of%20video%20DiTs%20with%20multi-instance%20mask%0Atracks%20from%20the%20MATRIX-11K%20dataset%2C%20enhancing%20both%20grounding%20and%20propagation.%0AWe%20further%20propose%20InterGenEval%2C%20an%20evaluation%20protocol%20for%20interaction-aware%0Avideo%20generation.%20In%20experiments%2C%20MATRIX%20improves%20both%20interaction%20fidelity%20and%0Asemantic%20alignment%20while%20reducing%20drift%20and%20hallucination.%20Extensive%20ablations%0Avalidate%20our%20design%20choices.%20Codes%20and%20weights%20will%20be%20released.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07310v1&entry.124074799=Read"},
{"title": "Quantum-enhanced Computer Vision: Going Beyond Classical Algorithms", "author": "Natacha Kuete Meli and Shuteng Wang and Marcel Seelbach Benkner and Michele Sasdelli and Tat-Jun Chin and Tolga Birdal and Michael Moeller and Vladislav Golyanik", "abstract": "  Quantum-enhanced Computer Vision (QeCV) is a new research field at the\nintersection of computer vision, optimisation theory, machine learning and\nquantum computing. It has high potential to transform how visual signals are\nprocessed and interpreted with the help of quantum computing that leverages\nquantum-mechanical effects in computations inaccessible to classical (i.e.\nnon-quantum) computers. In scenarios where existing non-quantum methods cannot\nfind a solution in a reasonable time or compute only approximate solutions,\nquantum computers can provide, among others, advantages in terms of better time\nscalability for multiple problem classes. Parametrised quantum circuits can\nalso become, in the long term, a considerable alternative to classical neural\nnetworks in computer vision. However, specialised and fundamentally new\nalgorithms must be developed to enable compatibility with quantum hardware and\nunveil the potential of quantum computational paradigms in computer vision.\nThis survey contributes to the existing literature on QeCV with a holistic\nreview of this research field. It is designed as a quantum computing reference\nfor the computer vision community, targeting computer vision students,\nscientists and readers with related backgrounds who want to familiarise\nthemselves with QeCV. We provide a comprehensive introduction to QeCV, its\nspecifics, and methodologies for formulations compatible with quantum hardware\nand QeCV methods, leveraging two main quantum computational paradigms, i.e.\ngate-based quantum computing and quantum annealing. We elaborate on the\noperational principles of quantum computers and the available tools to access,\nprogram and simulate them in the context of QeCV. Finally, we review existing\nquantum computing tools and learning materials and discuss aspects related to\npublishing and reviewing QeCV papers, open challenges and potential social\nimplications.\n", "link": "http://arxiv.org/abs/2510.07317v1", "date": "2025-10-08", "relevancy": 2.4463, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4902}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4874}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Quantum-enhanced%20Computer%20Vision%3A%20Going%20Beyond%20Classical%20Algorithms&body=Title%3A%20Quantum-enhanced%20Computer%20Vision%3A%20Going%20Beyond%20Classical%20Algorithms%0AAuthor%3A%20Natacha%20Kuete%20Meli%20and%20Shuteng%20Wang%20and%20Marcel%20Seelbach%20Benkner%20and%20Michele%20Sasdelli%20and%20Tat-Jun%20Chin%20and%20Tolga%20Birdal%20and%20Michael%20Moeller%20and%20Vladislav%20Golyanik%0AAbstract%3A%20%20%20Quantum-enhanced%20Computer%20Vision%20%28QeCV%29%20is%20a%20new%20research%20field%20at%20the%0Aintersection%20of%20computer%20vision%2C%20optimisation%20theory%2C%20machine%20learning%20and%0Aquantum%20computing.%20It%20has%20high%20potential%20to%20transform%20how%20visual%20signals%20are%0Aprocessed%20and%20interpreted%20with%20the%20help%20of%20quantum%20computing%20that%20leverages%0Aquantum-mechanical%20effects%20in%20computations%20inaccessible%20to%20classical%20%28i.e.%0Anon-quantum%29%20computers.%20In%20scenarios%20where%20existing%20non-quantum%20methods%20cannot%0Afind%20a%20solution%20in%20a%20reasonable%20time%20or%20compute%20only%20approximate%20solutions%2C%0Aquantum%20computers%20can%20provide%2C%20among%20others%2C%20advantages%20in%20terms%20of%20better%20time%0Ascalability%20for%20multiple%20problem%20classes.%20Parametrised%20quantum%20circuits%20can%0Aalso%20become%2C%20in%20the%20long%20term%2C%20a%20considerable%20alternative%20to%20classical%20neural%0Anetworks%20in%20computer%20vision.%20However%2C%20specialised%20and%20fundamentally%20new%0Aalgorithms%20must%20be%20developed%20to%20enable%20compatibility%20with%20quantum%20hardware%20and%0Aunveil%20the%20potential%20of%20quantum%20computational%20paradigms%20in%20computer%20vision.%0AThis%20survey%20contributes%20to%20the%20existing%20literature%20on%20QeCV%20with%20a%20holistic%0Areview%20of%20this%20research%20field.%20It%20is%20designed%20as%20a%20quantum%20computing%20reference%0Afor%20the%20computer%20vision%20community%2C%20targeting%20computer%20vision%20students%2C%0Ascientists%20and%20readers%20with%20related%20backgrounds%20who%20want%20to%20familiarise%0Athemselves%20with%20QeCV.%20We%20provide%20a%20comprehensive%20introduction%20to%20QeCV%2C%20its%0Aspecifics%2C%20and%20methodologies%20for%20formulations%20compatible%20with%20quantum%20hardware%0Aand%20QeCV%20methods%2C%20leveraging%20two%20main%20quantum%20computational%20paradigms%2C%20i.e.%0Agate-based%20quantum%20computing%20and%20quantum%20annealing.%20We%20elaborate%20on%20the%0Aoperational%20principles%20of%20quantum%20computers%20and%20the%20available%20tools%20to%20access%2C%0Aprogram%20and%20simulate%20them%20in%20the%20context%20of%20QeCV.%20Finally%2C%20we%20review%20existing%0Aquantum%20computing%20tools%20and%20learning%20materials%20and%20discuss%20aspects%20related%20to%0Apublishing%20and%20reviewing%20QeCV%20papers%2C%20open%20challenges%20and%20potential%20social%0Aimplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07317v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQuantum-enhanced%2520Computer%2520Vision%253A%2520Going%2520Beyond%2520Classical%2520Algorithms%26entry.906535625%3DNatacha%2520Kuete%2520Meli%2520and%2520Shuteng%2520Wang%2520and%2520Marcel%2520Seelbach%2520Benkner%2520and%2520Michele%2520Sasdelli%2520and%2520Tat-Jun%2520Chin%2520and%2520Tolga%2520Birdal%2520and%2520Michael%2520Moeller%2520and%2520Vladislav%2520Golyanik%26entry.1292438233%3D%2520%2520Quantum-enhanced%2520Computer%2520Vision%2520%2528QeCV%2529%2520is%2520a%2520new%2520research%2520field%2520at%2520the%250Aintersection%2520of%2520computer%2520vision%252C%2520optimisation%2520theory%252C%2520machine%2520learning%2520and%250Aquantum%2520computing.%2520It%2520has%2520high%2520potential%2520to%2520transform%2520how%2520visual%2520signals%2520are%250Aprocessed%2520and%2520interpreted%2520with%2520the%2520help%2520of%2520quantum%2520computing%2520that%2520leverages%250Aquantum-mechanical%2520effects%2520in%2520computations%2520inaccessible%2520to%2520classical%2520%2528i.e.%250Anon-quantum%2529%2520computers.%2520In%2520scenarios%2520where%2520existing%2520non-quantum%2520methods%2520cannot%250Afind%2520a%2520solution%2520in%2520a%2520reasonable%2520time%2520or%2520compute%2520only%2520approximate%2520solutions%252C%250Aquantum%2520computers%2520can%2520provide%252C%2520among%2520others%252C%2520advantages%2520in%2520terms%2520of%2520better%2520time%250Ascalability%2520for%2520multiple%2520problem%2520classes.%2520Parametrised%2520quantum%2520circuits%2520can%250Aalso%2520become%252C%2520in%2520the%2520long%2520term%252C%2520a%2520considerable%2520alternative%2520to%2520classical%2520neural%250Anetworks%2520in%2520computer%2520vision.%2520However%252C%2520specialised%2520and%2520fundamentally%2520new%250Aalgorithms%2520must%2520be%2520developed%2520to%2520enable%2520compatibility%2520with%2520quantum%2520hardware%2520and%250Aunveil%2520the%2520potential%2520of%2520quantum%2520computational%2520paradigms%2520in%2520computer%2520vision.%250AThis%2520survey%2520contributes%2520to%2520the%2520existing%2520literature%2520on%2520QeCV%2520with%2520a%2520holistic%250Areview%2520of%2520this%2520research%2520field.%2520It%2520is%2520designed%2520as%2520a%2520quantum%2520computing%2520reference%250Afor%2520the%2520computer%2520vision%2520community%252C%2520targeting%2520computer%2520vision%2520students%252C%250Ascientists%2520and%2520readers%2520with%2520related%2520backgrounds%2520who%2520want%2520to%2520familiarise%250Athemselves%2520with%2520QeCV.%2520We%2520provide%2520a%2520comprehensive%2520introduction%2520to%2520QeCV%252C%2520its%250Aspecifics%252C%2520and%2520methodologies%2520for%2520formulations%2520compatible%2520with%2520quantum%2520hardware%250Aand%2520QeCV%2520methods%252C%2520leveraging%2520two%2520main%2520quantum%2520computational%2520paradigms%252C%2520i.e.%250Agate-based%2520quantum%2520computing%2520and%2520quantum%2520annealing.%2520We%2520elaborate%2520on%2520the%250Aoperational%2520principles%2520of%2520quantum%2520computers%2520and%2520the%2520available%2520tools%2520to%2520access%252C%250Aprogram%2520and%2520simulate%2520them%2520in%2520the%2520context%2520of%2520QeCV.%2520Finally%252C%2520we%2520review%2520existing%250Aquantum%2520computing%2520tools%2520and%2520learning%2520materials%2520and%2520discuss%2520aspects%2520related%2520to%250Apublishing%2520and%2520reviewing%2520QeCV%2520papers%252C%2520open%2520challenges%2520and%2520potential%2520social%250Aimplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07317v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Quantum-enhanced%20Computer%20Vision%3A%20Going%20Beyond%20Classical%20Algorithms&entry.906535625=Natacha%20Kuete%20Meli%20and%20Shuteng%20Wang%20and%20Marcel%20Seelbach%20Benkner%20and%20Michele%20Sasdelli%20and%20Tat-Jun%20Chin%20and%20Tolga%20Birdal%20and%20Michael%20Moeller%20and%20Vladislav%20Golyanik&entry.1292438233=%20%20Quantum-enhanced%20Computer%20Vision%20%28QeCV%29%20is%20a%20new%20research%20field%20at%20the%0Aintersection%20of%20computer%20vision%2C%20optimisation%20theory%2C%20machine%20learning%20and%0Aquantum%20computing.%20It%20has%20high%20potential%20to%20transform%20how%20visual%20signals%20are%0Aprocessed%20and%20interpreted%20with%20the%20help%20of%20quantum%20computing%20that%20leverages%0Aquantum-mechanical%20effects%20in%20computations%20inaccessible%20to%20classical%20%28i.e.%0Anon-quantum%29%20computers.%20In%20scenarios%20where%20existing%20non-quantum%20methods%20cannot%0Afind%20a%20solution%20in%20a%20reasonable%20time%20or%20compute%20only%20approximate%20solutions%2C%0Aquantum%20computers%20can%20provide%2C%20among%20others%2C%20advantages%20in%20terms%20of%20better%20time%0Ascalability%20for%20multiple%20problem%20classes.%20Parametrised%20quantum%20circuits%20can%0Aalso%20become%2C%20in%20the%20long%20term%2C%20a%20considerable%20alternative%20to%20classical%20neural%0Anetworks%20in%20computer%20vision.%20However%2C%20specialised%20and%20fundamentally%20new%0Aalgorithms%20must%20be%20developed%20to%20enable%20compatibility%20with%20quantum%20hardware%20and%0Aunveil%20the%20potential%20of%20quantum%20computational%20paradigms%20in%20computer%20vision.%0AThis%20survey%20contributes%20to%20the%20existing%20literature%20on%20QeCV%20with%20a%20holistic%0Areview%20of%20this%20research%20field.%20It%20is%20designed%20as%20a%20quantum%20computing%20reference%0Afor%20the%20computer%20vision%20community%2C%20targeting%20computer%20vision%20students%2C%0Ascientists%20and%20readers%20with%20related%20backgrounds%20who%20want%20to%20familiarise%0Athemselves%20with%20QeCV.%20We%20provide%20a%20comprehensive%20introduction%20to%20QeCV%2C%20its%0Aspecifics%2C%20and%20methodologies%20for%20formulations%20compatible%20with%20quantum%20hardware%0Aand%20QeCV%20methods%2C%20leveraging%20two%20main%20quantum%20computational%20paradigms%2C%20i.e.%0Agate-based%20quantum%20computing%20and%20quantum%20annealing.%20We%20elaborate%20on%20the%0Aoperational%20principles%20of%20quantum%20computers%20and%20the%20available%20tools%20to%20access%2C%0Aprogram%20and%20simulate%20them%20in%20the%20context%20of%20QeCV.%20Finally%2C%20we%20review%20existing%0Aquantum%20computing%20tools%20and%20learning%20materials%20and%20discuss%20aspects%20related%20to%0Apublishing%20and%20reviewing%20QeCV%20papers%2C%20open%20challenges%20and%20potential%20social%0Aimplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07317v1&entry.124074799=Read"},
{"title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense", "author": "Leitian Tao and Ilia Kulikov and Swarnadeep Saha and Tianlu Wang and Jing Xu and Yixuan Li and Jason E Weston and Ping Yu", "abstract": "  Post-training for reasoning of large language models (LLMs) increasingly\nrelies on verifiable rewards: deterministic checkers that provide 0-1\ncorrectness signals. While reliable, such binary feedback is brittle--many\ntasks admit partially correct or alternative answers that verifiers\nunder-credit, and the resulting all-or-nothing supervision limits learning.\nReward models offer richer, continuous feedback, which can serve as a\ncomplementary supervisory signal to verifiers. We introduce HERO (Hybrid\nEnsemble Reward Optimization), a reinforcement learning framework that\nintegrates verifier signals with reward-model scores in a structured way. HERO\nemploys stratified normalization to bound reward-model scores within\nverifier-defined groups, preserving correctness while refining quality\ndistinctions, and variance-aware weighting to emphasize challenging prompts\nwhere dense signals matter most. Across diverse mathematical reasoning\nbenchmarks, HERO consistently outperforms RM-only and verifier-only baselines,\nwith strong gains on both verifiable and hard-to-verify tasks. Our results show\nthat hybrid reward design retains the stability of verifiers while leveraging\nthe nuance of reward models to advance reasoning.\n", "link": "http://arxiv.org/abs/2510.07242v1", "date": "2025-10-08", "relevancy": 2.4389, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4943}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4943}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hybrid%20Reinforcement%3A%20When%20Reward%20Is%20Sparse%2C%20It%27s%20Better%20to%20Be%20Dense&body=Title%3A%20Hybrid%20Reinforcement%3A%20When%20Reward%20Is%20Sparse%2C%20It%27s%20Better%20to%20Be%20Dense%0AAuthor%3A%20Leitian%20Tao%20and%20Ilia%20Kulikov%20and%20Swarnadeep%20Saha%20and%20Tianlu%20Wang%20and%20Jing%20Xu%20and%20Yixuan%20Li%20and%20Jason%20E%20Weston%20and%20Ping%20Yu%0AAbstract%3A%20%20%20Post-training%20for%20reasoning%20of%20large%20language%20models%20%28LLMs%29%20increasingly%0Arelies%20on%20verifiable%20rewards%3A%20deterministic%20checkers%20that%20provide%200-1%0Acorrectness%20signals.%20While%20reliable%2C%20such%20binary%20feedback%20is%20brittle--many%0Atasks%20admit%20partially%20correct%20or%20alternative%20answers%20that%20verifiers%0Aunder-credit%2C%20and%20the%20resulting%20all-or-nothing%20supervision%20limits%20learning.%0AReward%20models%20offer%20richer%2C%20continuous%20feedback%2C%20which%20can%20serve%20as%20a%0Acomplementary%20supervisory%20signal%20to%20verifiers.%20We%20introduce%20HERO%20%28Hybrid%0AEnsemble%20Reward%20Optimization%29%2C%20a%20reinforcement%20learning%20framework%20that%0Aintegrates%20verifier%20signals%20with%20reward-model%20scores%20in%20a%20structured%20way.%20HERO%0Aemploys%20stratified%20normalization%20to%20bound%20reward-model%20scores%20within%0Averifier-defined%20groups%2C%20preserving%20correctness%20while%20refining%20quality%0Adistinctions%2C%20and%20variance-aware%20weighting%20to%20emphasize%20challenging%20prompts%0Awhere%20dense%20signals%20matter%20most.%20Across%20diverse%20mathematical%20reasoning%0Abenchmarks%2C%20HERO%20consistently%20outperforms%20RM-only%20and%20verifier-only%20baselines%2C%0Awith%20strong%20gains%20on%20both%20verifiable%20and%20hard-to-verify%20tasks.%20Our%20results%20show%0Athat%20hybrid%20reward%20design%20retains%20the%20stability%20of%20verifiers%20while%20leveraging%0Athe%20nuance%20of%20reward%20models%20to%20advance%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHybrid%2520Reinforcement%253A%2520When%2520Reward%2520Is%2520Sparse%252C%2520It%2527s%2520Better%2520to%2520Be%2520Dense%26entry.906535625%3DLeitian%2520Tao%2520and%2520Ilia%2520Kulikov%2520and%2520Swarnadeep%2520Saha%2520and%2520Tianlu%2520Wang%2520and%2520Jing%2520Xu%2520and%2520Yixuan%2520Li%2520and%2520Jason%2520E%2520Weston%2520and%2520Ping%2520Yu%26entry.1292438233%3D%2520%2520Post-training%2520for%2520reasoning%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520increasingly%250Arelies%2520on%2520verifiable%2520rewards%253A%2520deterministic%2520checkers%2520that%2520provide%25200-1%250Acorrectness%2520signals.%2520While%2520reliable%252C%2520such%2520binary%2520feedback%2520is%2520brittle--many%250Atasks%2520admit%2520partially%2520correct%2520or%2520alternative%2520answers%2520that%2520verifiers%250Aunder-credit%252C%2520and%2520the%2520resulting%2520all-or-nothing%2520supervision%2520limits%2520learning.%250AReward%2520models%2520offer%2520richer%252C%2520continuous%2520feedback%252C%2520which%2520can%2520serve%2520as%2520a%250Acomplementary%2520supervisory%2520signal%2520to%2520verifiers.%2520We%2520introduce%2520HERO%2520%2528Hybrid%250AEnsemble%2520Reward%2520Optimization%2529%252C%2520a%2520reinforcement%2520learning%2520framework%2520that%250Aintegrates%2520verifier%2520signals%2520with%2520reward-model%2520scores%2520in%2520a%2520structured%2520way.%2520HERO%250Aemploys%2520stratified%2520normalization%2520to%2520bound%2520reward-model%2520scores%2520within%250Averifier-defined%2520groups%252C%2520preserving%2520correctness%2520while%2520refining%2520quality%250Adistinctions%252C%2520and%2520variance-aware%2520weighting%2520to%2520emphasize%2520challenging%2520prompts%250Awhere%2520dense%2520signals%2520matter%2520most.%2520Across%2520diverse%2520mathematical%2520reasoning%250Abenchmarks%252C%2520HERO%2520consistently%2520outperforms%2520RM-only%2520and%2520verifier-only%2520baselines%252C%250Awith%2520strong%2520gains%2520on%2520both%2520verifiable%2520and%2520hard-to-verify%2520tasks.%2520Our%2520results%2520show%250Athat%2520hybrid%2520reward%2520design%2520retains%2520the%2520stability%2520of%2520verifiers%2520while%2520leveraging%250Athe%2520nuance%2520of%2520reward%2520models%2520to%2520advance%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hybrid%20Reinforcement%3A%20When%20Reward%20Is%20Sparse%2C%20It%27s%20Better%20to%20Be%20Dense&entry.906535625=Leitian%20Tao%20and%20Ilia%20Kulikov%20and%20Swarnadeep%20Saha%20and%20Tianlu%20Wang%20and%20Jing%20Xu%20and%20Yixuan%20Li%20and%20Jason%20E%20Weston%20and%20Ping%20Yu&entry.1292438233=%20%20Post-training%20for%20reasoning%20of%20large%20language%20models%20%28LLMs%29%20increasingly%0Arelies%20on%20verifiable%20rewards%3A%20deterministic%20checkers%20that%20provide%200-1%0Acorrectness%20signals.%20While%20reliable%2C%20such%20binary%20feedback%20is%20brittle--many%0Atasks%20admit%20partially%20correct%20or%20alternative%20answers%20that%20verifiers%0Aunder-credit%2C%20and%20the%20resulting%20all-or-nothing%20supervision%20limits%20learning.%0AReward%20models%20offer%20richer%2C%20continuous%20feedback%2C%20which%20can%20serve%20as%20a%0Acomplementary%20supervisory%20signal%20to%20verifiers.%20We%20introduce%20HERO%20%28Hybrid%0AEnsemble%20Reward%20Optimization%29%2C%20a%20reinforcement%20learning%20framework%20that%0Aintegrates%20verifier%20signals%20with%20reward-model%20scores%20in%20a%20structured%20way.%20HERO%0Aemploys%20stratified%20normalization%20to%20bound%20reward-model%20scores%20within%0Averifier-defined%20groups%2C%20preserving%20correctness%20while%20refining%20quality%0Adistinctions%2C%20and%20variance-aware%20weighting%20to%20emphasize%20challenging%20prompts%0Awhere%20dense%20signals%20matter%20most.%20Across%20diverse%20mathematical%20reasoning%0Abenchmarks%2C%20HERO%20consistently%20outperforms%20RM-only%20and%20verifier-only%20baselines%2C%0Awith%20strong%20gains%20on%20both%20verifiable%20and%20hard-to-verify%20tasks.%20Our%20results%20show%0Athat%20hybrid%20reward%20design%20retains%20the%20stability%20of%20verifiers%20while%20leveraging%0Athe%20nuance%20of%20reward%20models%20to%20advance%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07242v1&entry.124074799=Read"},
{"title": "TalkCuts: A Large-Scale Dataset for Multi-Shot Human Speech Video\n  Generation", "author": "Jiaben Chen and Zixin Wang and Ailing Zeng and Yang Fu and Xueyang Yu and Siyuan Cen and Julian Tanke and Yihang Chen and Koichi Saito and Yuki Mitsufuji and Chuang Gan", "abstract": "  In this work, we present TalkCuts, a large-scale dataset designed to\nfacilitate the study of multi-shot human speech video generation. Unlike\nexisting datasets that focus on single-shot, static viewpoints, TalkCuts offers\n164k clips totaling over 500 hours of high-quality human speech videos with\ndiverse camera shots, including close-up, half-body, and full-body views. The\ndataset includes detailed textual descriptions, 2D keypoints and 3D SMPL-X\nmotion annotations, covering over 10k identities, enabling multimodal learning\nand evaluation. As a first attempt to showcase the value of the dataset, we\npresent Orator, an LLM-guided multi-modal generation framework as a simple\nbaseline, where the language model functions as a multi-faceted director,\norchestrating detailed specifications for camera transitions, speaker\ngesticulations, and vocal modulation. This architecture enables the synthesis\nof coherent long-form videos through our integrated multi-modal video\ngeneration module. Extensive experiments in both pose-guided and audio-driven\nsettings show that training on TalkCuts significantly enhances the\ncinematographic coherence and visual appeal of generated multi-shot speech\nvideos. We believe TalkCuts provides a strong foundation for future work in\ncontrollable, multi-shot speech video generation and broader multimodal\nlearning.\n", "link": "http://arxiv.org/abs/2510.07249v1", "date": "2025-10-08", "relevancy": 2.3667, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6327}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5637}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TalkCuts%3A%20A%20Large-Scale%20Dataset%20for%20Multi-Shot%20Human%20Speech%20Video%0A%20%20Generation&body=Title%3A%20TalkCuts%3A%20A%20Large-Scale%20Dataset%20for%20Multi-Shot%20Human%20Speech%20Video%0A%20%20Generation%0AAuthor%3A%20Jiaben%20Chen%20and%20Zixin%20Wang%20and%20Ailing%20Zeng%20and%20Yang%20Fu%20and%20Xueyang%20Yu%20and%20Siyuan%20Cen%20and%20Julian%20Tanke%20and%20Yihang%20Chen%20and%20Koichi%20Saito%20and%20Yuki%20Mitsufuji%20and%20Chuang%20Gan%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20TalkCuts%2C%20a%20large-scale%20dataset%20designed%20to%0Afacilitate%20the%20study%20of%20multi-shot%20human%20speech%20video%20generation.%20Unlike%0Aexisting%20datasets%20that%20focus%20on%20single-shot%2C%20static%20viewpoints%2C%20TalkCuts%20offers%0A164k%20clips%20totaling%20over%20500%20hours%20of%20high-quality%20human%20speech%20videos%20with%0Adiverse%20camera%20shots%2C%20including%20close-up%2C%20half-body%2C%20and%20full-body%20views.%20The%0Adataset%20includes%20detailed%20textual%20descriptions%2C%202D%20keypoints%20and%203D%20SMPL-X%0Amotion%20annotations%2C%20covering%20over%2010k%20identities%2C%20enabling%20multimodal%20learning%0Aand%20evaluation.%20As%20a%20first%20attempt%20to%20showcase%20the%20value%20of%20the%20dataset%2C%20we%0Apresent%20Orator%2C%20an%20LLM-guided%20multi-modal%20generation%20framework%20as%20a%20simple%0Abaseline%2C%20where%20the%20language%20model%20functions%20as%20a%20multi-faceted%20director%2C%0Aorchestrating%20detailed%20specifications%20for%20camera%20transitions%2C%20speaker%0Agesticulations%2C%20and%20vocal%20modulation.%20This%20architecture%20enables%20the%20synthesis%0Aof%20coherent%20long-form%20videos%20through%20our%20integrated%20multi-modal%20video%0Ageneration%20module.%20Extensive%20experiments%20in%20both%20pose-guided%20and%20audio-driven%0Asettings%20show%20that%20training%20on%20TalkCuts%20significantly%20enhances%20the%0Acinematographic%20coherence%20and%20visual%20appeal%20of%20generated%20multi-shot%20speech%0Avideos.%20We%20believe%20TalkCuts%20provides%20a%20strong%20foundation%20for%20future%20work%20in%0Acontrollable%2C%20multi-shot%20speech%20video%20generation%20and%20broader%20multimodal%0Alearning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07249v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTalkCuts%253A%2520A%2520Large-Scale%2520Dataset%2520for%2520Multi-Shot%2520Human%2520Speech%2520Video%250A%2520%2520Generation%26entry.906535625%3DJiaben%2520Chen%2520and%2520Zixin%2520Wang%2520and%2520Ailing%2520Zeng%2520and%2520Yang%2520Fu%2520and%2520Xueyang%2520Yu%2520and%2520Siyuan%2520Cen%2520and%2520Julian%2520Tanke%2520and%2520Yihang%2520Chen%2520and%2520Koichi%2520Saito%2520and%2520Yuki%2520Mitsufuji%2520and%2520Chuang%2520Gan%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520TalkCuts%252C%2520a%2520large-scale%2520dataset%2520designed%2520to%250Afacilitate%2520the%2520study%2520of%2520multi-shot%2520human%2520speech%2520video%2520generation.%2520Unlike%250Aexisting%2520datasets%2520that%2520focus%2520on%2520single-shot%252C%2520static%2520viewpoints%252C%2520TalkCuts%2520offers%250A164k%2520clips%2520totaling%2520over%2520500%2520hours%2520of%2520high-quality%2520human%2520speech%2520videos%2520with%250Adiverse%2520camera%2520shots%252C%2520including%2520close-up%252C%2520half-body%252C%2520and%2520full-body%2520views.%2520The%250Adataset%2520includes%2520detailed%2520textual%2520descriptions%252C%25202D%2520keypoints%2520and%25203D%2520SMPL-X%250Amotion%2520annotations%252C%2520covering%2520over%252010k%2520identities%252C%2520enabling%2520multimodal%2520learning%250Aand%2520evaluation.%2520As%2520a%2520first%2520attempt%2520to%2520showcase%2520the%2520value%2520of%2520the%2520dataset%252C%2520we%250Apresent%2520Orator%252C%2520an%2520LLM-guided%2520multi-modal%2520generation%2520framework%2520as%2520a%2520simple%250Abaseline%252C%2520where%2520the%2520language%2520model%2520functions%2520as%2520a%2520multi-faceted%2520director%252C%250Aorchestrating%2520detailed%2520specifications%2520for%2520camera%2520transitions%252C%2520speaker%250Agesticulations%252C%2520and%2520vocal%2520modulation.%2520This%2520architecture%2520enables%2520the%2520synthesis%250Aof%2520coherent%2520long-form%2520videos%2520through%2520our%2520integrated%2520multi-modal%2520video%250Ageneration%2520module.%2520Extensive%2520experiments%2520in%2520both%2520pose-guided%2520and%2520audio-driven%250Asettings%2520show%2520that%2520training%2520on%2520TalkCuts%2520significantly%2520enhances%2520the%250Acinematographic%2520coherence%2520and%2520visual%2520appeal%2520of%2520generated%2520multi-shot%2520speech%250Avideos.%2520We%2520believe%2520TalkCuts%2520provides%2520a%2520strong%2520foundation%2520for%2520future%2520work%2520in%250Acontrollable%252C%2520multi-shot%2520speech%2520video%2520generation%2520and%2520broader%2520multimodal%250Alearning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07249v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TalkCuts%3A%20A%20Large-Scale%20Dataset%20for%20Multi-Shot%20Human%20Speech%20Video%0A%20%20Generation&entry.906535625=Jiaben%20Chen%20and%20Zixin%20Wang%20and%20Ailing%20Zeng%20and%20Yang%20Fu%20and%20Xueyang%20Yu%20and%20Siyuan%20Cen%20and%20Julian%20Tanke%20and%20Yihang%20Chen%20and%20Koichi%20Saito%20and%20Yuki%20Mitsufuji%20and%20Chuang%20Gan&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20TalkCuts%2C%20a%20large-scale%20dataset%20designed%20to%0Afacilitate%20the%20study%20of%20multi-shot%20human%20speech%20video%20generation.%20Unlike%0Aexisting%20datasets%20that%20focus%20on%20single-shot%2C%20static%20viewpoints%2C%20TalkCuts%20offers%0A164k%20clips%20totaling%20over%20500%20hours%20of%20high-quality%20human%20speech%20videos%20with%0Adiverse%20camera%20shots%2C%20including%20close-up%2C%20half-body%2C%20and%20full-body%20views.%20The%0Adataset%20includes%20detailed%20textual%20descriptions%2C%202D%20keypoints%20and%203D%20SMPL-X%0Amotion%20annotations%2C%20covering%20over%2010k%20identities%2C%20enabling%20multimodal%20learning%0Aand%20evaluation.%20As%20a%20first%20attempt%20to%20showcase%20the%20value%20of%20the%20dataset%2C%20we%0Apresent%20Orator%2C%20an%20LLM-guided%20multi-modal%20generation%20framework%20as%20a%20simple%0Abaseline%2C%20where%20the%20language%20model%20functions%20as%20a%20multi-faceted%20director%2C%0Aorchestrating%20detailed%20specifications%20for%20camera%20transitions%2C%20speaker%0Agesticulations%2C%20and%20vocal%20modulation.%20This%20architecture%20enables%20the%20synthesis%0Aof%20coherent%20long-form%20videos%20through%20our%20integrated%20multi-modal%20video%0Ageneration%20module.%20Extensive%20experiments%20in%20both%20pose-guided%20and%20audio-driven%0Asettings%20show%20that%20training%20on%20TalkCuts%20significantly%20enhances%20the%0Acinematographic%20coherence%20and%20visual%20appeal%20of%20generated%20multi-shot%20speech%0Avideos.%20We%20believe%20TalkCuts%20provides%20a%20strong%20foundation%20for%20future%20work%20in%0Acontrollable%2C%20multi-shot%20speech%20video%20generation%20and%20broader%20multimodal%0Alearning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07249v1&entry.124074799=Read"},
{"title": "On the Convergence of Moral Self-Correction in Large Language Models", "author": "Guangliang Liu and Haitao Mao and Bochuan Cao and Zhiyu Xue and Xitong Zhang and Rongrong Wang and Kristen Marie Johnson", "abstract": "  Large Language Models (LLMs) are able to improve their responses when\ninstructed to do so, a capability known as self-correction. When instructions\nprovide only a general and abstract goal without specific details about\npotential issues in the response, LLMs must rely on their internal knowledge to\nimprove response quality, a process referred to as intrinsic self-correction.\nThe empirical success of intrinsic self-correction is evident in various\napplications, but how and why it is effective remains unknown. Focusing on\nmoral self-correction in LLMs, we reveal a key characteristic of intrinsic\nself-correction: performance convergence through multi-round interactions; and\nprovide a mechanistic analysis of this convergence behavior. Based on our\nexperimental results and analysis, we uncover the underlying mechanism of\nconvergence: consistently injected self-correction instructions activate moral\nconcepts that reduce model uncertainty, leading to converged performance as the\nactivated moral concepts stabilize over successive rounds. This paper\ndemonstrates the strong potential of moral self-correction by showing that it\nexhibits a desirable property of converged performance.\n", "link": "http://arxiv.org/abs/2510.07290v1", "date": "2025-10-08", "relevancy": 2.333, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4723}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4723}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Convergence%20of%20Moral%20Self-Correction%20in%20Large%20Language%20Models&body=Title%3A%20On%20the%20Convergence%20of%20Moral%20Self-Correction%20in%20Large%20Language%20Models%0AAuthor%3A%20Guangliang%20Liu%20and%20Haitao%20Mao%20and%20Bochuan%20Cao%20and%20Zhiyu%20Xue%20and%20Xitong%20Zhang%20and%20Rongrong%20Wang%20and%20Kristen%20Marie%20Johnson%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20are%20able%20to%20improve%20their%20responses%20when%0Ainstructed%20to%20do%20so%2C%20a%20capability%20known%20as%20self-correction.%20When%20instructions%0Aprovide%20only%20a%20general%20and%20abstract%20goal%20without%20specific%20details%20about%0Apotential%20issues%20in%20the%20response%2C%20LLMs%20must%20rely%20on%20their%20internal%20knowledge%20to%0Aimprove%20response%20quality%2C%20a%20process%20referred%20to%20as%20intrinsic%20self-correction.%0AThe%20empirical%20success%20of%20intrinsic%20self-correction%20is%20evident%20in%20various%0Aapplications%2C%20but%20how%20and%20why%20it%20is%20effective%20remains%20unknown.%20Focusing%20on%0Amoral%20self-correction%20in%20LLMs%2C%20we%20reveal%20a%20key%20characteristic%20of%20intrinsic%0Aself-correction%3A%20performance%20convergence%20through%20multi-round%20interactions%3B%20and%0Aprovide%20a%20mechanistic%20analysis%20of%20this%20convergence%20behavior.%20Based%20on%20our%0Aexperimental%20results%20and%20analysis%2C%20we%20uncover%20the%20underlying%20mechanism%20of%0Aconvergence%3A%20consistently%20injected%20self-correction%20instructions%20activate%20moral%0Aconcepts%20that%20reduce%20model%20uncertainty%2C%20leading%20to%20converged%20performance%20as%20the%0Aactivated%20moral%20concepts%20stabilize%20over%20successive%20rounds.%20This%20paper%0Ademonstrates%20the%20strong%20potential%20of%20moral%20self-correction%20by%20showing%20that%20it%0Aexhibits%20a%20desirable%20property%20of%20converged%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07290v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Convergence%2520of%2520Moral%2520Self-Correction%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DGuangliang%2520Liu%2520and%2520Haitao%2520Mao%2520and%2520Bochuan%2520Cao%2520and%2520Zhiyu%2520Xue%2520and%2520Xitong%2520Zhang%2520and%2520Rongrong%2520Wang%2520and%2520Kristen%2520Marie%2520Johnson%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520are%2520able%2520to%2520improve%2520their%2520responses%2520when%250Ainstructed%2520to%2520do%2520so%252C%2520a%2520capability%2520known%2520as%2520self-correction.%2520When%2520instructions%250Aprovide%2520only%2520a%2520general%2520and%2520abstract%2520goal%2520without%2520specific%2520details%2520about%250Apotential%2520issues%2520in%2520the%2520response%252C%2520LLMs%2520must%2520rely%2520on%2520their%2520internal%2520knowledge%2520to%250Aimprove%2520response%2520quality%252C%2520a%2520process%2520referred%2520to%2520as%2520intrinsic%2520self-correction.%250AThe%2520empirical%2520success%2520of%2520intrinsic%2520self-correction%2520is%2520evident%2520in%2520various%250Aapplications%252C%2520but%2520how%2520and%2520why%2520it%2520is%2520effective%2520remains%2520unknown.%2520Focusing%2520on%250Amoral%2520self-correction%2520in%2520LLMs%252C%2520we%2520reveal%2520a%2520key%2520characteristic%2520of%2520intrinsic%250Aself-correction%253A%2520performance%2520convergence%2520through%2520multi-round%2520interactions%253B%2520and%250Aprovide%2520a%2520mechanistic%2520analysis%2520of%2520this%2520convergence%2520behavior.%2520Based%2520on%2520our%250Aexperimental%2520results%2520and%2520analysis%252C%2520we%2520uncover%2520the%2520underlying%2520mechanism%2520of%250Aconvergence%253A%2520consistently%2520injected%2520self-correction%2520instructions%2520activate%2520moral%250Aconcepts%2520that%2520reduce%2520model%2520uncertainty%252C%2520leading%2520to%2520converged%2520performance%2520as%2520the%250Aactivated%2520moral%2520concepts%2520stabilize%2520over%2520successive%2520rounds.%2520This%2520paper%250Ademonstrates%2520the%2520strong%2520potential%2520of%2520moral%2520self-correction%2520by%2520showing%2520that%2520it%250Aexhibits%2520a%2520desirable%2520property%2520of%2520converged%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07290v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Convergence%20of%20Moral%20Self-Correction%20in%20Large%20Language%20Models&entry.906535625=Guangliang%20Liu%20and%20Haitao%20Mao%20and%20Bochuan%20Cao%20and%20Zhiyu%20Xue%20and%20Xitong%20Zhang%20and%20Rongrong%20Wang%20and%20Kristen%20Marie%20Johnson&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20are%20able%20to%20improve%20their%20responses%20when%0Ainstructed%20to%20do%20so%2C%20a%20capability%20known%20as%20self-correction.%20When%20instructions%0Aprovide%20only%20a%20general%20and%20abstract%20goal%20without%20specific%20details%20about%0Apotential%20issues%20in%20the%20response%2C%20LLMs%20must%20rely%20on%20their%20internal%20knowledge%20to%0Aimprove%20response%20quality%2C%20a%20process%20referred%20to%20as%20intrinsic%20self-correction.%0AThe%20empirical%20success%20of%20intrinsic%20self-correction%20is%20evident%20in%20various%0Aapplications%2C%20but%20how%20and%20why%20it%20is%20effective%20remains%20unknown.%20Focusing%20on%0Amoral%20self-correction%20in%20LLMs%2C%20we%20reveal%20a%20key%20characteristic%20of%20intrinsic%0Aself-correction%3A%20performance%20convergence%20through%20multi-round%20interactions%3B%20and%0Aprovide%20a%20mechanistic%20analysis%20of%20this%20convergence%20behavior.%20Based%20on%20our%0Aexperimental%20results%20and%20analysis%2C%20we%20uncover%20the%20underlying%20mechanism%20of%0Aconvergence%3A%20consistently%20injected%20self-correction%20instructions%20activate%20moral%0Aconcepts%20that%20reduce%20model%20uncertainty%2C%20leading%20to%20converged%20performance%20as%20the%0Aactivated%20moral%20concepts%20stabilize%20over%20successive%20rounds.%20This%20paper%0Ademonstrates%20the%20strong%20potential%20of%20moral%20self-correction%20by%20showing%20that%20it%0Aexhibits%20a%20desirable%20property%20of%20converged%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07290v1&entry.124074799=Read"},
{"title": "BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection\n  in Language Model Pretraining", "author": "Jie Hao and Rui Yu and Wei Zhang and Huixia Wang and Jie Xu and Mingrui Liu", "abstract": "  Effective data selection is essential for pretraining large language models\n(LLMs), enhancing efficiency and improving generalization to downstream tasks.\nHowever, existing approaches often require leveraging external pretrained\nmodels, making it difficult to disentangle the effects of data selection from\nthose of the external pretrained models. In addition, they often overlook the\nlong-term impact of selected data if the model is trained to convergence,\nprimarily due to the prohibitive cost of full-scale LLM pretraining. In this\npaper, we introduce BLISS (\\textbf{B}ileve\\textbf{L} \\textbf{I}nfluence\n\\textbf{S}coring method for data \\textbf{S}election): a lightweight data\nselection method that operates entirely \\emph{from scratch}, without relying on\nany external pretrained oracle models, while explicitly accounting for the\nlong-term impact of selected data. BLISS leverages a small proxy model as a\nsurrogate for the LLM and employs a score model to estimate the long-term\ninfluence of training samples if the proxy model is trained to convergence. We\nformulate data selection as a bilevel optimization problem, where the\nupper-level objective optimizes the score model to assign importance weights to\ntraining samples, ensuring that minimizing the lower-level objective (i.e.,\ntraining the proxy model over the weighted training loss until convergence)\nleads to best validation performance. Once optimized, the trained score model\npredicts influence scores for the dataset, enabling efficient selection of\nhigh-quality samples for LLM pretraining. We validate BLISS by pretraining\n410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4\ndataset. Notably, under the 1B model setting, BLISS achieves $1.7\\times$\nspeedup in reaching the same performance as the state-of-the-art method,\ndemonstrating superior performance across multiple downstream tasks.\n", "link": "http://arxiv.org/abs/2510.06048v2", "date": "2025-10-08", "relevancy": 2.3326, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4682}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4682}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4632}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BLISS%3A%20A%20Lightweight%20Bilevel%20Influence%20Scoring%20Method%20for%20Data%20Selection%0A%20%20in%20Language%20Model%20Pretraining&body=Title%3A%20BLISS%3A%20A%20Lightweight%20Bilevel%20Influence%20Scoring%20Method%20for%20Data%20Selection%0A%20%20in%20Language%20Model%20Pretraining%0AAuthor%3A%20Jie%20Hao%20and%20Rui%20Yu%20and%20Wei%20Zhang%20and%20Huixia%20Wang%20and%20Jie%20Xu%20and%20Mingrui%20Liu%0AAbstract%3A%20%20%20Effective%20data%20selection%20is%20essential%20for%20pretraining%20large%20language%20models%0A%28LLMs%29%2C%20enhancing%20efficiency%20and%20improving%20generalization%20to%20downstream%20tasks.%0AHowever%2C%20existing%20approaches%20often%20require%20leveraging%20external%20pretrained%0Amodels%2C%20making%20it%20difficult%20to%20disentangle%20the%20effects%20of%20data%20selection%20from%0Athose%20of%20the%20external%20pretrained%20models.%20In%20addition%2C%20they%20often%20overlook%20the%0Along-term%20impact%20of%20selected%20data%20if%20the%20model%20is%20trained%20to%20convergence%2C%0Aprimarily%20due%20to%20the%20prohibitive%20cost%20of%20full-scale%20LLM%20pretraining.%20In%20this%0Apaper%2C%20we%20introduce%20BLISS%20%28%5Ctextbf%7BB%7Dileve%5Ctextbf%7BL%7D%20%5Ctextbf%7BI%7Dnfluence%0A%5Ctextbf%7BS%7Dcoring%20method%20for%20data%20%5Ctextbf%7BS%7Delection%29%3A%20a%20lightweight%20data%0Aselection%20method%20that%20operates%20entirely%20%5Cemph%7Bfrom%20scratch%7D%2C%20without%20relying%20on%0Aany%20external%20pretrained%20oracle%20models%2C%20while%20explicitly%20accounting%20for%20the%0Along-term%20impact%20of%20selected%20data.%20BLISS%20leverages%20a%20small%20proxy%20model%20as%20a%0Asurrogate%20for%20the%20LLM%20and%20employs%20a%20score%20model%20to%20estimate%20the%20long-term%0Ainfluence%20of%20training%20samples%20if%20the%20proxy%20model%20is%20trained%20to%20convergence.%20We%0Aformulate%20data%20selection%20as%20a%20bilevel%20optimization%20problem%2C%20where%20the%0Aupper-level%20objective%20optimizes%20the%20score%20model%20to%20assign%20importance%20weights%20to%0Atraining%20samples%2C%20ensuring%20that%20minimizing%20the%20lower-level%20objective%20%28i.e.%2C%0Atraining%20the%20proxy%20model%20over%20the%20weighted%20training%20loss%20until%20convergence%29%0Aleads%20to%20best%20validation%20performance.%20Once%20optimized%2C%20the%20trained%20score%20model%0Apredicts%20influence%20scores%20for%20the%20dataset%2C%20enabling%20efficient%20selection%20of%0Ahigh-quality%20samples%20for%20LLM%20pretraining.%20We%20validate%20BLISS%20by%20pretraining%0A410M/1B/2.8B%20Pythia%20and%20LLaMA-0.5B%20models%20on%20selected%20subsets%20of%20the%20C4%0Adataset.%20Notably%2C%20under%20the%201B%20model%20setting%2C%20BLISS%20achieves%20%241.7%5Ctimes%24%0Aspeedup%20in%20reaching%20the%20same%20performance%20as%20the%20state-of-the-art%20method%2C%0Ademonstrating%20superior%20performance%20across%20multiple%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.06048v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBLISS%253A%2520A%2520Lightweight%2520Bilevel%2520Influence%2520Scoring%2520Method%2520for%2520Data%2520Selection%250A%2520%2520in%2520Language%2520Model%2520Pretraining%26entry.906535625%3DJie%2520Hao%2520and%2520Rui%2520Yu%2520and%2520Wei%2520Zhang%2520and%2520Huixia%2520Wang%2520and%2520Jie%2520Xu%2520and%2520Mingrui%2520Liu%26entry.1292438233%3D%2520%2520Effective%2520data%2520selection%2520is%2520essential%2520for%2520pretraining%2520large%2520language%2520models%250A%2528LLMs%2529%252C%2520enhancing%2520efficiency%2520and%2520improving%2520generalization%2520to%2520downstream%2520tasks.%250AHowever%252C%2520existing%2520approaches%2520often%2520require%2520leveraging%2520external%2520pretrained%250Amodels%252C%2520making%2520it%2520difficult%2520to%2520disentangle%2520the%2520effects%2520of%2520data%2520selection%2520from%250Athose%2520of%2520the%2520external%2520pretrained%2520models.%2520In%2520addition%252C%2520they%2520often%2520overlook%2520the%250Along-term%2520impact%2520of%2520selected%2520data%2520if%2520the%2520model%2520is%2520trained%2520to%2520convergence%252C%250Aprimarily%2520due%2520to%2520the%2520prohibitive%2520cost%2520of%2520full-scale%2520LLM%2520pretraining.%2520In%2520this%250Apaper%252C%2520we%2520introduce%2520BLISS%2520%2528%255Ctextbf%257BB%257Dileve%255Ctextbf%257BL%257D%2520%255Ctextbf%257BI%257Dnfluence%250A%255Ctextbf%257BS%257Dcoring%2520method%2520for%2520data%2520%255Ctextbf%257BS%257Delection%2529%253A%2520a%2520lightweight%2520data%250Aselection%2520method%2520that%2520operates%2520entirely%2520%255Cemph%257Bfrom%2520scratch%257D%252C%2520without%2520relying%2520on%250Aany%2520external%2520pretrained%2520oracle%2520models%252C%2520while%2520explicitly%2520accounting%2520for%2520the%250Along-term%2520impact%2520of%2520selected%2520data.%2520BLISS%2520leverages%2520a%2520small%2520proxy%2520model%2520as%2520a%250Asurrogate%2520for%2520the%2520LLM%2520and%2520employs%2520a%2520score%2520model%2520to%2520estimate%2520the%2520long-term%250Ainfluence%2520of%2520training%2520samples%2520if%2520the%2520proxy%2520model%2520is%2520trained%2520to%2520convergence.%2520We%250Aformulate%2520data%2520selection%2520as%2520a%2520bilevel%2520optimization%2520problem%252C%2520where%2520the%250Aupper-level%2520objective%2520optimizes%2520the%2520score%2520model%2520to%2520assign%2520importance%2520weights%2520to%250Atraining%2520samples%252C%2520ensuring%2520that%2520minimizing%2520the%2520lower-level%2520objective%2520%2528i.e.%252C%250Atraining%2520the%2520proxy%2520model%2520over%2520the%2520weighted%2520training%2520loss%2520until%2520convergence%2529%250Aleads%2520to%2520best%2520validation%2520performance.%2520Once%2520optimized%252C%2520the%2520trained%2520score%2520model%250Apredicts%2520influence%2520scores%2520for%2520the%2520dataset%252C%2520enabling%2520efficient%2520selection%2520of%250Ahigh-quality%2520samples%2520for%2520LLM%2520pretraining.%2520We%2520validate%2520BLISS%2520by%2520pretraining%250A410M/1B/2.8B%2520Pythia%2520and%2520LLaMA-0.5B%2520models%2520on%2520selected%2520subsets%2520of%2520the%2520C4%250Adataset.%2520Notably%252C%2520under%2520the%25201B%2520model%2520setting%252C%2520BLISS%2520achieves%2520%25241.7%255Ctimes%2524%250Aspeedup%2520in%2520reaching%2520the%2520same%2520performance%2520as%2520the%2520state-of-the-art%2520method%252C%250Ademonstrating%2520superior%2520performance%2520across%2520multiple%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.06048v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BLISS%3A%20A%20Lightweight%20Bilevel%20Influence%20Scoring%20Method%20for%20Data%20Selection%0A%20%20in%20Language%20Model%20Pretraining&entry.906535625=Jie%20Hao%20and%20Rui%20Yu%20and%20Wei%20Zhang%20and%20Huixia%20Wang%20and%20Jie%20Xu%20and%20Mingrui%20Liu&entry.1292438233=%20%20Effective%20data%20selection%20is%20essential%20for%20pretraining%20large%20language%20models%0A%28LLMs%29%2C%20enhancing%20efficiency%20and%20improving%20generalization%20to%20downstream%20tasks.%0AHowever%2C%20existing%20approaches%20often%20require%20leveraging%20external%20pretrained%0Amodels%2C%20making%20it%20difficult%20to%20disentangle%20the%20effects%20of%20data%20selection%20from%0Athose%20of%20the%20external%20pretrained%20models.%20In%20addition%2C%20they%20often%20overlook%20the%0Along-term%20impact%20of%20selected%20data%20if%20the%20model%20is%20trained%20to%20convergence%2C%0Aprimarily%20due%20to%20the%20prohibitive%20cost%20of%20full-scale%20LLM%20pretraining.%20In%20this%0Apaper%2C%20we%20introduce%20BLISS%20%28%5Ctextbf%7BB%7Dileve%5Ctextbf%7BL%7D%20%5Ctextbf%7BI%7Dnfluence%0A%5Ctextbf%7BS%7Dcoring%20method%20for%20data%20%5Ctextbf%7BS%7Delection%29%3A%20a%20lightweight%20data%0Aselection%20method%20that%20operates%20entirely%20%5Cemph%7Bfrom%20scratch%7D%2C%20without%20relying%20on%0Aany%20external%20pretrained%20oracle%20models%2C%20while%20explicitly%20accounting%20for%20the%0Along-term%20impact%20of%20selected%20data.%20BLISS%20leverages%20a%20small%20proxy%20model%20as%20a%0Asurrogate%20for%20the%20LLM%20and%20employs%20a%20score%20model%20to%20estimate%20the%20long-term%0Ainfluence%20of%20training%20samples%20if%20the%20proxy%20model%20is%20trained%20to%20convergence.%20We%0Aformulate%20data%20selection%20as%20a%20bilevel%20optimization%20problem%2C%20where%20the%0Aupper-level%20objective%20optimizes%20the%20score%20model%20to%20assign%20importance%20weights%20to%0Atraining%20samples%2C%20ensuring%20that%20minimizing%20the%20lower-level%20objective%20%28i.e.%2C%0Atraining%20the%20proxy%20model%20over%20the%20weighted%20training%20loss%20until%20convergence%29%0Aleads%20to%20best%20validation%20performance.%20Once%20optimized%2C%20the%20trained%20score%20model%0Apredicts%20influence%20scores%20for%20the%20dataset%2C%20enabling%20efficient%20selection%20of%0Ahigh-quality%20samples%20for%20LLM%20pretraining.%20We%20validate%20BLISS%20by%20pretraining%0A410M/1B/2.8B%20Pythia%20and%20LLaMA-0.5B%20models%20on%20selected%20subsets%20of%20the%20C4%0Adataset.%20Notably%2C%20under%20the%201B%20model%20setting%2C%20BLISS%20achieves%20%241.7%5Ctimes%24%0Aspeedup%20in%20reaching%20the%20same%20performance%20as%20the%20state-of-the-art%20method%2C%0Ademonstrating%20superior%20performance%20across%20multiple%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.06048v2&entry.124074799=Read"},
{"title": "V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt\n  Instruction Tuning", "author": "Hang Hua and Yolo Yunlong Tang and Chenliang Xu and Jiebo Luo", "abstract": "  Video summarization aims to create short, accurate, and cohesive summaries of\nlonger videos. Despite the existence of various video summarization datasets, a\nnotable limitation is their limited amount of source videos, which hampers the\neffective training of advanced large vision-language models (VLMs).\nAdditionally, most existing datasets are created for video-to-video\nsummarization, overlooking the contemporary need for multimodal video content\nsummarization. Recent efforts have been made to expand from unimodal to\nmultimodal video summarization, categorizing the task into three sub-tasks\nbased on the summary's modality: video-to-video (V2V), video-to-text (V2T), and\na combination of video and text summarization (V2VT). However, the textual\nsummaries in previous multimodal datasets are inadequate. To address these\nissues, we introduce Instruct-V2Xum, a cross-modal video summarization dataset\nfeaturing 30,000 diverse videos sourced from YouTube, with lengths ranging from\n40 to 940 seconds and an average summarization ratio of 16.39%. Each video\nsummary in Instruct-V2Xum is paired with a textual summary that references\nspecific frame indexes, facilitating the generation of aligned video and\ntextual summaries. In addition, we propose a new video summarization framework\nnamed V2Xum-LLM. V2Xum-LLM, specifically V2Xum-LLaMA in this study, is the\nfirst framework that unifies different video summarization tasks into one large\nlanguage model's (LLM) text decoder and achieves task-controllable video\nsummarization with temporal prompts and task instructions. Experiments show\nthat V2Xum-LLaMA outperforms strong baseline models on multiple video\nsummarization tasks. Furthermore, we propose an enhanced evaluation metric for\nV2V and V2VT summarization tasks.\n", "link": "http://arxiv.org/abs/2404.12353v3", "date": "2025-10-08", "relevancy": 2.2885, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5847}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5738}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20V2Xum-LLM%3A%20Cross-Modal%20Video%20Summarization%20with%20Temporal%20Prompt%0A%20%20Instruction%20Tuning&body=Title%3A%20V2Xum-LLM%3A%20Cross-Modal%20Video%20Summarization%20with%20Temporal%20Prompt%0A%20%20Instruction%20Tuning%0AAuthor%3A%20Hang%20Hua%20and%20Yolo%20Yunlong%20Tang%20and%20Chenliang%20Xu%20and%20Jiebo%20Luo%0AAbstract%3A%20%20%20Video%20summarization%20aims%20to%20create%20short%2C%20accurate%2C%20and%20cohesive%20summaries%20of%0Alonger%20videos.%20Despite%20the%20existence%20of%20various%20video%20summarization%20datasets%2C%20a%0Anotable%20limitation%20is%20their%20limited%20amount%20of%20source%20videos%2C%20which%20hampers%20the%0Aeffective%20training%20of%20advanced%20large%20vision-language%20models%20%28VLMs%29.%0AAdditionally%2C%20most%20existing%20datasets%20are%20created%20for%20video-to-video%0Asummarization%2C%20overlooking%20the%20contemporary%20need%20for%20multimodal%20video%20content%0Asummarization.%20Recent%20efforts%20have%20been%20made%20to%20expand%20from%20unimodal%20to%0Amultimodal%20video%20summarization%2C%20categorizing%20the%20task%20into%20three%20sub-tasks%0Abased%20on%20the%20summary%27s%20modality%3A%20video-to-video%20%28V2V%29%2C%20video-to-text%20%28V2T%29%2C%20and%0Aa%20combination%20of%20video%20and%20text%20summarization%20%28V2VT%29.%20However%2C%20the%20textual%0Asummaries%20in%20previous%20multimodal%20datasets%20are%20inadequate.%20To%20address%20these%0Aissues%2C%20we%20introduce%20Instruct-V2Xum%2C%20a%20cross-modal%20video%20summarization%20dataset%0Afeaturing%2030%2C000%20diverse%20videos%20sourced%20from%20YouTube%2C%20with%20lengths%20ranging%20from%0A40%20to%20940%20seconds%20and%20an%20average%20summarization%20ratio%20of%2016.39%25.%20Each%20video%0Asummary%20in%20Instruct-V2Xum%20is%20paired%20with%20a%20textual%20summary%20that%20references%0Aspecific%20frame%20indexes%2C%20facilitating%20the%20generation%20of%20aligned%20video%20and%0Atextual%20summaries.%20In%20addition%2C%20we%20propose%20a%20new%20video%20summarization%20framework%0Anamed%20V2Xum-LLM.%20V2Xum-LLM%2C%20specifically%20V2Xum-LLaMA%20in%20this%20study%2C%20is%20the%0Afirst%20framework%20that%20unifies%20different%20video%20summarization%20tasks%20into%20one%20large%0Alanguage%20model%27s%20%28LLM%29%20text%20decoder%20and%20achieves%20task-controllable%20video%0Asummarization%20with%20temporal%20prompts%20and%20task%20instructions.%20Experiments%20show%0Athat%20V2Xum-LLaMA%20outperforms%20strong%20baseline%20models%20on%20multiple%20video%0Asummarization%20tasks.%20Furthermore%2C%20we%20propose%20an%20enhanced%20evaluation%20metric%20for%0AV2V%20and%20V2VT%20summarization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.12353v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DV2Xum-LLM%253A%2520Cross-Modal%2520Video%2520Summarization%2520with%2520Temporal%2520Prompt%250A%2520%2520Instruction%2520Tuning%26entry.906535625%3DHang%2520Hua%2520and%2520Yolo%2520Yunlong%2520Tang%2520and%2520Chenliang%2520Xu%2520and%2520Jiebo%2520Luo%26entry.1292438233%3D%2520%2520Video%2520summarization%2520aims%2520to%2520create%2520short%252C%2520accurate%252C%2520and%2520cohesive%2520summaries%2520of%250Alonger%2520videos.%2520Despite%2520the%2520existence%2520of%2520various%2520video%2520summarization%2520datasets%252C%2520a%250Anotable%2520limitation%2520is%2520their%2520limited%2520amount%2520of%2520source%2520videos%252C%2520which%2520hampers%2520the%250Aeffective%2520training%2520of%2520advanced%2520large%2520vision-language%2520models%2520%2528VLMs%2529.%250AAdditionally%252C%2520most%2520existing%2520datasets%2520are%2520created%2520for%2520video-to-video%250Asummarization%252C%2520overlooking%2520the%2520contemporary%2520need%2520for%2520multimodal%2520video%2520content%250Asummarization.%2520Recent%2520efforts%2520have%2520been%2520made%2520to%2520expand%2520from%2520unimodal%2520to%250Amultimodal%2520video%2520summarization%252C%2520categorizing%2520the%2520task%2520into%2520three%2520sub-tasks%250Abased%2520on%2520the%2520summary%2527s%2520modality%253A%2520video-to-video%2520%2528V2V%2529%252C%2520video-to-text%2520%2528V2T%2529%252C%2520and%250Aa%2520combination%2520of%2520video%2520and%2520text%2520summarization%2520%2528V2VT%2529.%2520However%252C%2520the%2520textual%250Asummaries%2520in%2520previous%2520multimodal%2520datasets%2520are%2520inadequate.%2520To%2520address%2520these%250Aissues%252C%2520we%2520introduce%2520Instruct-V2Xum%252C%2520a%2520cross-modal%2520video%2520summarization%2520dataset%250Afeaturing%252030%252C000%2520diverse%2520videos%2520sourced%2520from%2520YouTube%252C%2520with%2520lengths%2520ranging%2520from%250A40%2520to%2520940%2520seconds%2520and%2520an%2520average%2520summarization%2520ratio%2520of%252016.39%2525.%2520Each%2520video%250Asummary%2520in%2520Instruct-V2Xum%2520is%2520paired%2520with%2520a%2520textual%2520summary%2520that%2520references%250Aspecific%2520frame%2520indexes%252C%2520facilitating%2520the%2520generation%2520of%2520aligned%2520video%2520and%250Atextual%2520summaries.%2520In%2520addition%252C%2520we%2520propose%2520a%2520new%2520video%2520summarization%2520framework%250Anamed%2520V2Xum-LLM.%2520V2Xum-LLM%252C%2520specifically%2520V2Xum-LLaMA%2520in%2520this%2520study%252C%2520is%2520the%250Afirst%2520framework%2520that%2520unifies%2520different%2520video%2520summarization%2520tasks%2520into%2520one%2520large%250Alanguage%2520model%2527s%2520%2528LLM%2529%2520text%2520decoder%2520and%2520achieves%2520task-controllable%2520video%250Asummarization%2520with%2520temporal%2520prompts%2520and%2520task%2520instructions.%2520Experiments%2520show%250Athat%2520V2Xum-LLaMA%2520outperforms%2520strong%2520baseline%2520models%2520on%2520multiple%2520video%250Asummarization%2520tasks.%2520Furthermore%252C%2520we%2520propose%2520an%2520enhanced%2520evaluation%2520metric%2520for%250AV2V%2520and%2520V2VT%2520summarization%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.12353v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=V2Xum-LLM%3A%20Cross-Modal%20Video%20Summarization%20with%20Temporal%20Prompt%0A%20%20Instruction%20Tuning&entry.906535625=Hang%20Hua%20and%20Yolo%20Yunlong%20Tang%20and%20Chenliang%20Xu%20and%20Jiebo%20Luo&entry.1292438233=%20%20Video%20summarization%20aims%20to%20create%20short%2C%20accurate%2C%20and%20cohesive%20summaries%20of%0Alonger%20videos.%20Despite%20the%20existence%20of%20various%20video%20summarization%20datasets%2C%20a%0Anotable%20limitation%20is%20their%20limited%20amount%20of%20source%20videos%2C%20which%20hampers%20the%0Aeffective%20training%20of%20advanced%20large%20vision-language%20models%20%28VLMs%29.%0AAdditionally%2C%20most%20existing%20datasets%20are%20created%20for%20video-to-video%0Asummarization%2C%20overlooking%20the%20contemporary%20need%20for%20multimodal%20video%20content%0Asummarization.%20Recent%20efforts%20have%20been%20made%20to%20expand%20from%20unimodal%20to%0Amultimodal%20video%20summarization%2C%20categorizing%20the%20task%20into%20three%20sub-tasks%0Abased%20on%20the%20summary%27s%20modality%3A%20video-to-video%20%28V2V%29%2C%20video-to-text%20%28V2T%29%2C%20and%0Aa%20combination%20of%20video%20and%20text%20summarization%20%28V2VT%29.%20However%2C%20the%20textual%0Asummaries%20in%20previous%20multimodal%20datasets%20are%20inadequate.%20To%20address%20these%0Aissues%2C%20we%20introduce%20Instruct-V2Xum%2C%20a%20cross-modal%20video%20summarization%20dataset%0Afeaturing%2030%2C000%20diverse%20videos%20sourced%20from%20YouTube%2C%20with%20lengths%20ranging%20from%0A40%20to%20940%20seconds%20and%20an%20average%20summarization%20ratio%20of%2016.39%25.%20Each%20video%0Asummary%20in%20Instruct-V2Xum%20is%20paired%20with%20a%20textual%20summary%20that%20references%0Aspecific%20frame%20indexes%2C%20facilitating%20the%20generation%20of%20aligned%20video%20and%0Atextual%20summaries.%20In%20addition%2C%20we%20propose%20a%20new%20video%20summarization%20framework%0Anamed%20V2Xum-LLM.%20V2Xum-LLM%2C%20specifically%20V2Xum-LLaMA%20in%20this%20study%2C%20is%20the%0Afirst%20framework%20that%20unifies%20different%20video%20summarization%20tasks%20into%20one%20large%0Alanguage%20model%27s%20%28LLM%29%20text%20decoder%20and%20achieves%20task-controllable%20video%0Asummarization%20with%20temporal%20prompts%20and%20task%20instructions.%20Experiments%20show%0Athat%20V2Xum-LLaMA%20outperforms%20strong%20baseline%20models%20on%20multiple%20video%0Asummarization%20tasks.%20Furthermore%2C%20we%20propose%20an%20enhanced%20evaluation%20metric%20for%0AV2V%20and%20V2VT%20summarization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.12353v3&entry.124074799=Read"},
{"title": "Discriminative Feature Feedback with General Teacher Classes", "author": "Omri Bar Oz and Tosca Lechner and Sivan Sabato", "abstract": "  We study the theoretical properties of the interactive learning protocol\nDiscriminative Feature Feedback (DFF) (Dasgupta et al., 2018). The DFF learning\nprotocol uses feedback in the form of discriminative feature explanations. We\nprovide the first systematic study of DFF in a general framework that is\ncomparable to that of classical protocols such as supervised learning and\nonline learning. We study the optimal mistake bound of DFF in the realizable\nand the non-realizable settings, and obtain novel structural results, as well\nas insights into the differences between Online Learning and settings with\nricher feedback such as DFF. We characterize the mistake bound in the\nrealizable setting using a new notion of dimension. In the non-realizable\nsetting, we provide a mistake upper bound and show that it cannot be improved\nin general. Our results show that unlike Online Learning, in DFF the realizable\ndimension is insufficient to characterize the optimal non-realizable mistake\nbound or the existence of no-regret algorithms.\n", "link": "http://arxiv.org/abs/2510.07245v1", "date": "2025-10-08", "relevancy": 2.2874, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4708}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4511}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discriminative%20Feature%20Feedback%20with%20General%20Teacher%20Classes&body=Title%3A%20Discriminative%20Feature%20Feedback%20with%20General%20Teacher%20Classes%0AAuthor%3A%20Omri%20Bar%20Oz%20and%20Tosca%20Lechner%20and%20Sivan%20Sabato%0AAbstract%3A%20%20%20We%20study%20the%20theoretical%20properties%20of%20the%20interactive%20learning%20protocol%0ADiscriminative%20Feature%20Feedback%20%28DFF%29%20%28Dasgupta%20et%20al.%2C%202018%29.%20The%20DFF%20learning%0Aprotocol%20uses%20feedback%20in%20the%20form%20of%20discriminative%20feature%20explanations.%20We%0Aprovide%20the%20first%20systematic%20study%20of%20DFF%20in%20a%20general%20framework%20that%20is%0Acomparable%20to%20that%20of%20classical%20protocols%20such%20as%20supervised%20learning%20and%0Aonline%20learning.%20We%20study%20the%20optimal%20mistake%20bound%20of%20DFF%20in%20the%20realizable%0Aand%20the%20non-realizable%20settings%2C%20and%20obtain%20novel%20structural%20results%2C%20as%20well%0Aas%20insights%20into%20the%20differences%20between%20Online%20Learning%20and%20settings%20with%0Aricher%20feedback%20such%20as%20DFF.%20We%20characterize%20the%20mistake%20bound%20in%20the%0Arealizable%20setting%20using%20a%20new%20notion%20of%20dimension.%20In%20the%20non-realizable%0Asetting%2C%20we%20provide%20a%20mistake%20upper%20bound%20and%20show%20that%20it%20cannot%20be%20improved%0Ain%20general.%20Our%20results%20show%20that%20unlike%20Online%20Learning%2C%20in%20DFF%20the%20realizable%0Adimension%20is%20insufficient%20to%20characterize%20the%20optimal%20non-realizable%20mistake%0Abound%20or%20the%20existence%20of%20no-regret%20algorithms.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07245v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscriminative%2520Feature%2520Feedback%2520with%2520General%2520Teacher%2520Classes%26entry.906535625%3DOmri%2520Bar%2520Oz%2520and%2520Tosca%2520Lechner%2520and%2520Sivan%2520Sabato%26entry.1292438233%3D%2520%2520We%2520study%2520the%2520theoretical%2520properties%2520of%2520the%2520interactive%2520learning%2520protocol%250ADiscriminative%2520Feature%2520Feedback%2520%2528DFF%2529%2520%2528Dasgupta%2520et%2520al.%252C%25202018%2529.%2520The%2520DFF%2520learning%250Aprotocol%2520uses%2520feedback%2520in%2520the%2520form%2520of%2520discriminative%2520feature%2520explanations.%2520We%250Aprovide%2520the%2520first%2520systematic%2520study%2520of%2520DFF%2520in%2520a%2520general%2520framework%2520that%2520is%250Acomparable%2520to%2520that%2520of%2520classical%2520protocols%2520such%2520as%2520supervised%2520learning%2520and%250Aonline%2520learning.%2520We%2520study%2520the%2520optimal%2520mistake%2520bound%2520of%2520DFF%2520in%2520the%2520realizable%250Aand%2520the%2520non-realizable%2520settings%252C%2520and%2520obtain%2520novel%2520structural%2520results%252C%2520as%2520well%250Aas%2520insights%2520into%2520the%2520differences%2520between%2520Online%2520Learning%2520and%2520settings%2520with%250Aricher%2520feedback%2520such%2520as%2520DFF.%2520We%2520characterize%2520the%2520mistake%2520bound%2520in%2520the%250Arealizable%2520setting%2520using%2520a%2520new%2520notion%2520of%2520dimension.%2520In%2520the%2520non-realizable%250Asetting%252C%2520we%2520provide%2520a%2520mistake%2520upper%2520bound%2520and%2520show%2520that%2520it%2520cannot%2520be%2520improved%250Ain%2520general.%2520Our%2520results%2520show%2520that%2520unlike%2520Online%2520Learning%252C%2520in%2520DFF%2520the%2520realizable%250Adimension%2520is%2520insufficient%2520to%2520characterize%2520the%2520optimal%2520non-realizable%2520mistake%250Abound%2520or%2520the%2520existence%2520of%2520no-regret%2520algorithms.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07245v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discriminative%20Feature%20Feedback%20with%20General%20Teacher%20Classes&entry.906535625=Omri%20Bar%20Oz%20and%20Tosca%20Lechner%20and%20Sivan%20Sabato&entry.1292438233=%20%20We%20study%20the%20theoretical%20properties%20of%20the%20interactive%20learning%20protocol%0ADiscriminative%20Feature%20Feedback%20%28DFF%29%20%28Dasgupta%20et%20al.%2C%202018%29.%20The%20DFF%20learning%0Aprotocol%20uses%20feedback%20in%20the%20form%20of%20discriminative%20feature%20explanations.%20We%0Aprovide%20the%20first%20systematic%20study%20of%20DFF%20in%20a%20general%20framework%20that%20is%0Acomparable%20to%20that%20of%20classical%20protocols%20such%20as%20supervised%20learning%20and%0Aonline%20learning.%20We%20study%20the%20optimal%20mistake%20bound%20of%20DFF%20in%20the%20realizable%0Aand%20the%20non-realizable%20settings%2C%20and%20obtain%20novel%20structural%20results%2C%20as%20well%0Aas%20insights%20into%20the%20differences%20between%20Online%20Learning%20and%20settings%20with%0Aricher%20feedback%20such%20as%20DFF.%20We%20characterize%20the%20mistake%20bound%20in%20the%0Arealizable%20setting%20using%20a%20new%20notion%20of%20dimension.%20In%20the%20non-realizable%0Asetting%2C%20we%20provide%20a%20mistake%20upper%20bound%20and%20show%20that%20it%20cannot%20be%20improved%0Ain%20general.%20Our%20results%20show%20that%20unlike%20Online%20Learning%2C%20in%20DFF%20the%20realizable%0Adimension%20is%20insufficient%20to%20characterize%20the%20optimal%20non-realizable%20mistake%0Abound%20or%20the%20existence%20of%20no-regret%20algorithms.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07245v1&entry.124074799=Read"},
{"title": "Empowering LLMs with Pseudo-Untrimmed Videos for Audio-Visual Temporal\n  Understanding", "author": "Yolo Yunlong Tang and Daiki Shimada and Jing Bi and Mingqian Feng and Hang Hua and Chenliang Xu", "abstract": "  Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language and multimodal domains. By fine-tuning multimodal LLMs with\ntemporal annotations from well-annotated datasets, e.g., dense video captioning\ndatasets, their temporal understanding capacity in video-language tasks can be\nobtained. However, there is a notable lack of untrimmed audio-visual video\ndatasets with precise temporal annotations for events. This deficiency hinders\nLLMs from learning the alignment between time, audio-visual events, and text\ntokens, thus impairing their ability to temporally localize audio-visual events\nin videos. To address this gap, we introduce PU-VALOR, a comprehensive\naudio-visual dataset comprising over 114,000 pseudo-untrimmed videos with\ndetailed temporal annotations. PU-VALOR is derived from the large-scale but\ncoarse-annotated audio-visual dataset VALOR, through a subtle method involving\nevent-based video clustering, random temporal scaling, and permutation. By\nfine-tuning a multimodal LLM on PU-VALOR, we developed AVicuna, a model capable\nof aligning audio-visual events with temporal intervals and corresponding text\ntokens. AVicuna excels in temporal localization and time-aware dialogue\ncapabilities. Our experiments demonstrate that AVicuna effectively handles\ntemporal understanding in audio-visual videos and achieves state-of-the-art\nperformance on open-ended video QA, audio-visual QA, and audio-visual event\ndense localization tasks.\n", "link": "http://arxiv.org/abs/2403.16276v3", "date": "2025-10-08", "relevancy": 2.2713, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5893}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5589}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5499}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empowering%20LLMs%20with%20Pseudo-Untrimmed%20Videos%20for%20Audio-Visual%20Temporal%0A%20%20Understanding&body=Title%3A%20Empowering%20LLMs%20with%20Pseudo-Untrimmed%20Videos%20for%20Audio-Visual%20Temporal%0A%20%20Understanding%0AAuthor%3A%20Yolo%20Yunlong%20Tang%20and%20Daiki%20Shimada%20and%20Jing%20Bi%20and%20Mingqian%20Feng%20and%20Hang%20Hua%20and%20Chenliang%20Xu%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Anatural%20language%20and%20multimodal%20domains.%20By%20fine-tuning%20multimodal%20LLMs%20with%0Atemporal%20annotations%20from%20well-annotated%20datasets%2C%20e.g.%2C%20dense%20video%20captioning%0Adatasets%2C%20their%20temporal%20understanding%20capacity%20in%20video-language%20tasks%20can%20be%0Aobtained.%20However%2C%20there%20is%20a%20notable%20lack%20of%20untrimmed%20audio-visual%20video%0Adatasets%20with%20precise%20temporal%20annotations%20for%20events.%20This%20deficiency%20hinders%0ALLMs%20from%20learning%20the%20alignment%20between%20time%2C%20audio-visual%20events%2C%20and%20text%0Atokens%2C%20thus%20impairing%20their%20ability%20to%20temporally%20localize%20audio-visual%20events%0Ain%20videos.%20To%20address%20this%20gap%2C%20we%20introduce%20PU-VALOR%2C%20a%20comprehensive%0Aaudio-visual%20dataset%20comprising%20over%20114%2C000%20pseudo-untrimmed%20videos%20with%0Adetailed%20temporal%20annotations.%20PU-VALOR%20is%20derived%20from%20the%20large-scale%20but%0Acoarse-annotated%20audio-visual%20dataset%20VALOR%2C%20through%20a%20subtle%20method%20involving%0Aevent-based%20video%20clustering%2C%20random%20temporal%20scaling%2C%20and%20permutation.%20By%0Afine-tuning%20a%20multimodal%20LLM%20on%20PU-VALOR%2C%20we%20developed%20AVicuna%2C%20a%20model%20capable%0Aof%20aligning%20audio-visual%20events%20with%20temporal%20intervals%20and%20corresponding%20text%0Atokens.%20AVicuna%20excels%20in%20temporal%20localization%20and%20time-aware%20dialogue%0Acapabilities.%20Our%20experiments%20demonstrate%20that%20AVicuna%20effectively%20handles%0Atemporal%20understanding%20in%20audio-visual%20videos%20and%20achieves%20state-of-the-art%0Aperformance%20on%20open-ended%20video%20QA%2C%20audio-visual%20QA%2C%20and%20audio-visual%20event%0Adense%20localization%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.16276v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpowering%2520LLMs%2520with%2520Pseudo-Untrimmed%2520Videos%2520for%2520Audio-Visual%2520Temporal%250A%2520%2520Understanding%26entry.906535625%3DYolo%2520Yunlong%2520Tang%2520and%2520Daiki%2520Shimada%2520and%2520Jing%2520Bi%2520and%2520Mingqian%2520Feng%2520and%2520Hang%2520Hua%2520and%2520Chenliang%2520Xu%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520have%2520demonstrated%2520remarkable%2520capabilities%2520in%250Anatural%2520language%2520and%2520multimodal%2520domains.%2520By%2520fine-tuning%2520multimodal%2520LLMs%2520with%250Atemporal%2520annotations%2520from%2520well-annotated%2520datasets%252C%2520e.g.%252C%2520dense%2520video%2520captioning%250Adatasets%252C%2520their%2520temporal%2520understanding%2520capacity%2520in%2520video-language%2520tasks%2520can%2520be%250Aobtained.%2520However%252C%2520there%2520is%2520a%2520notable%2520lack%2520of%2520untrimmed%2520audio-visual%2520video%250Adatasets%2520with%2520precise%2520temporal%2520annotations%2520for%2520events.%2520This%2520deficiency%2520hinders%250ALLMs%2520from%2520learning%2520the%2520alignment%2520between%2520time%252C%2520audio-visual%2520events%252C%2520and%2520text%250Atokens%252C%2520thus%2520impairing%2520their%2520ability%2520to%2520temporally%2520localize%2520audio-visual%2520events%250Ain%2520videos.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520PU-VALOR%252C%2520a%2520comprehensive%250Aaudio-visual%2520dataset%2520comprising%2520over%2520114%252C000%2520pseudo-untrimmed%2520videos%2520with%250Adetailed%2520temporal%2520annotations.%2520PU-VALOR%2520is%2520derived%2520from%2520the%2520large-scale%2520but%250Acoarse-annotated%2520audio-visual%2520dataset%2520VALOR%252C%2520through%2520a%2520subtle%2520method%2520involving%250Aevent-based%2520video%2520clustering%252C%2520random%2520temporal%2520scaling%252C%2520and%2520permutation.%2520By%250Afine-tuning%2520a%2520multimodal%2520LLM%2520on%2520PU-VALOR%252C%2520we%2520developed%2520AVicuna%252C%2520a%2520model%2520capable%250Aof%2520aligning%2520audio-visual%2520events%2520with%2520temporal%2520intervals%2520and%2520corresponding%2520text%250Atokens.%2520AVicuna%2520excels%2520in%2520temporal%2520localization%2520and%2520time-aware%2520dialogue%250Acapabilities.%2520Our%2520experiments%2520demonstrate%2520that%2520AVicuna%2520effectively%2520handles%250Atemporal%2520understanding%2520in%2520audio-visual%2520videos%2520and%2520achieves%2520state-of-the-art%250Aperformance%2520on%2520open-ended%2520video%2520QA%252C%2520audio-visual%2520QA%252C%2520and%2520audio-visual%2520event%250Adense%2520localization%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.16276v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empowering%20LLMs%20with%20Pseudo-Untrimmed%20Videos%20for%20Audio-Visual%20Temporal%0A%20%20Understanding&entry.906535625=Yolo%20Yunlong%20Tang%20and%20Daiki%20Shimada%20and%20Jing%20Bi%20and%20Mingqian%20Feng%20and%20Hang%20Hua%20and%20Chenliang%20Xu&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20have%20demonstrated%20remarkable%20capabilities%20in%0Anatural%20language%20and%20multimodal%20domains.%20By%20fine-tuning%20multimodal%20LLMs%20with%0Atemporal%20annotations%20from%20well-annotated%20datasets%2C%20e.g.%2C%20dense%20video%20captioning%0Adatasets%2C%20their%20temporal%20understanding%20capacity%20in%20video-language%20tasks%20can%20be%0Aobtained.%20However%2C%20there%20is%20a%20notable%20lack%20of%20untrimmed%20audio-visual%20video%0Adatasets%20with%20precise%20temporal%20annotations%20for%20events.%20This%20deficiency%20hinders%0ALLMs%20from%20learning%20the%20alignment%20between%20time%2C%20audio-visual%20events%2C%20and%20text%0Atokens%2C%20thus%20impairing%20their%20ability%20to%20temporally%20localize%20audio-visual%20events%0Ain%20videos.%20To%20address%20this%20gap%2C%20we%20introduce%20PU-VALOR%2C%20a%20comprehensive%0Aaudio-visual%20dataset%20comprising%20over%20114%2C000%20pseudo-untrimmed%20videos%20with%0Adetailed%20temporal%20annotations.%20PU-VALOR%20is%20derived%20from%20the%20large-scale%20but%0Acoarse-annotated%20audio-visual%20dataset%20VALOR%2C%20through%20a%20subtle%20method%20involving%0Aevent-based%20video%20clustering%2C%20random%20temporal%20scaling%2C%20and%20permutation.%20By%0Afine-tuning%20a%20multimodal%20LLM%20on%20PU-VALOR%2C%20we%20developed%20AVicuna%2C%20a%20model%20capable%0Aof%20aligning%20audio-visual%20events%20with%20temporal%20intervals%20and%20corresponding%20text%0Atokens.%20AVicuna%20excels%20in%20temporal%20localization%20and%20time-aware%20dialogue%0Acapabilities.%20Our%20experiments%20demonstrate%20that%20AVicuna%20effectively%20handles%0Atemporal%20understanding%20in%20audio-visual%20videos%20and%20achieves%20state-of-the-art%0Aperformance%20on%20open-ended%20video%20QA%2C%20audio-visual%20QA%2C%20and%20audio-visual%20event%0Adense%20localization%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.16276v3&entry.124074799=Read"},
{"title": "WristWorld: Generating Wrist-Views via 4D World Models for Robotic\n  Manipulation", "author": "Zezhong Qian and Xiaowei Chi and Yuming Li and Shizun Wang and Zhiyuan Qin and Xiaozhu Ju and Sirui Han and Shanghang Zhang", "abstract": "  Wrist-view observations are crucial for VLA models as they capture\nfine-grained hand-object interactions that directly enhance manipulation\nperformance. Yet large-scale datasets rarely include such recordings, resulting\nin a substantial gap between abundant anchor views and scarce wrist views.\nExisting world models cannot bridge this gap, as they require a wrist-view\nfirst frame and thus fail to generate wrist-view videos from anchor views\nalone. Amid this gap, recent visual geometry models such as VGGT emerge with\ngeometric and cross-view priors that make it possible to address extreme\nviewpoint shifts. Inspired by these insights, we propose WristWorld, the first\n4D world model that generates wrist-view videos solely from anchor views.\nWristWorld operates in two stages: (i) Reconstruction, which extends VGGT and\nincorporates our Spatial Projection Consistency (SPC) Loss to estimate\ngeometrically consistent wrist-view poses and 4D point clouds; (ii) Generation,\nwhich employs our video generation model to synthesize temporally coherent\nwrist-view videos from the reconstructed perspective. Experiments on Droid,\nCalvin, and Franka Panda demonstrate state-of-the-art video generation with\nsuperior spatial consistency, while also improving VLA performance, raising the\naverage task completion length on Calvin by 3.81% and closing 42.4% of the\nanchor-wrist view gap.\n", "link": "http://arxiv.org/abs/2510.07313v1", "date": "2025-10-08", "relevancy": 2.2216, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6186}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5436}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WristWorld%3A%20Generating%20Wrist-Views%20via%204D%20World%20Models%20for%20Robotic%0A%20%20Manipulation&body=Title%3A%20WristWorld%3A%20Generating%20Wrist-Views%20via%204D%20World%20Models%20for%20Robotic%0A%20%20Manipulation%0AAuthor%3A%20Zezhong%20Qian%20and%20Xiaowei%20Chi%20and%20Yuming%20Li%20and%20Shizun%20Wang%20and%20Zhiyuan%20Qin%20and%20Xiaozhu%20Ju%20and%20Sirui%20Han%20and%20Shanghang%20Zhang%0AAbstract%3A%20%20%20Wrist-view%20observations%20are%20crucial%20for%20VLA%20models%20as%20they%20capture%0Afine-grained%20hand-object%20interactions%20that%20directly%20enhance%20manipulation%0Aperformance.%20Yet%20large-scale%20datasets%20rarely%20include%20such%20recordings%2C%20resulting%0Ain%20a%20substantial%20gap%20between%20abundant%20anchor%20views%20and%20scarce%20wrist%20views.%0AExisting%20world%20models%20cannot%20bridge%20this%20gap%2C%20as%20they%20require%20a%20wrist-view%0Afirst%20frame%20and%20thus%20fail%20to%20generate%20wrist-view%20videos%20from%20anchor%20views%0Aalone.%20Amid%20this%20gap%2C%20recent%20visual%20geometry%20models%20such%20as%20VGGT%20emerge%20with%0Ageometric%20and%20cross-view%20priors%20that%20make%20it%20possible%20to%20address%20extreme%0Aviewpoint%20shifts.%20Inspired%20by%20these%20insights%2C%20we%20propose%20WristWorld%2C%20the%20first%0A4D%20world%20model%20that%20generates%20wrist-view%20videos%20solely%20from%20anchor%20views.%0AWristWorld%20operates%20in%20two%20stages%3A%20%28i%29%20Reconstruction%2C%20which%20extends%20VGGT%20and%0Aincorporates%20our%20Spatial%20Projection%20Consistency%20%28SPC%29%20Loss%20to%20estimate%0Ageometrically%20consistent%20wrist-view%20poses%20and%204D%20point%20clouds%3B%20%28ii%29%20Generation%2C%0Awhich%20employs%20our%20video%20generation%20model%20to%20synthesize%20temporally%20coherent%0Awrist-view%20videos%20from%20the%20reconstructed%20perspective.%20Experiments%20on%20Droid%2C%0ACalvin%2C%20and%20Franka%20Panda%20demonstrate%20state-of-the-art%20video%20generation%20with%0Asuperior%20spatial%20consistency%2C%20while%20also%20improving%20VLA%20performance%2C%20raising%20the%0Aaverage%20task%20completion%20length%20on%20Calvin%20by%203.81%25%20and%20closing%2042.4%25%20of%20the%0Aanchor-wrist%20view%20gap.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWristWorld%253A%2520Generating%2520Wrist-Views%2520via%25204D%2520World%2520Models%2520for%2520Robotic%250A%2520%2520Manipulation%26entry.906535625%3DZezhong%2520Qian%2520and%2520Xiaowei%2520Chi%2520and%2520Yuming%2520Li%2520and%2520Shizun%2520Wang%2520and%2520Zhiyuan%2520Qin%2520and%2520Xiaozhu%2520Ju%2520and%2520Sirui%2520Han%2520and%2520Shanghang%2520Zhang%26entry.1292438233%3D%2520%2520Wrist-view%2520observations%2520are%2520crucial%2520for%2520VLA%2520models%2520as%2520they%2520capture%250Afine-grained%2520hand-object%2520interactions%2520that%2520directly%2520enhance%2520manipulation%250Aperformance.%2520Yet%2520large-scale%2520datasets%2520rarely%2520include%2520such%2520recordings%252C%2520resulting%250Ain%2520a%2520substantial%2520gap%2520between%2520abundant%2520anchor%2520views%2520and%2520scarce%2520wrist%2520views.%250AExisting%2520world%2520models%2520cannot%2520bridge%2520this%2520gap%252C%2520as%2520they%2520require%2520a%2520wrist-view%250Afirst%2520frame%2520and%2520thus%2520fail%2520to%2520generate%2520wrist-view%2520videos%2520from%2520anchor%2520views%250Aalone.%2520Amid%2520this%2520gap%252C%2520recent%2520visual%2520geometry%2520models%2520such%2520as%2520VGGT%2520emerge%2520with%250Ageometric%2520and%2520cross-view%2520priors%2520that%2520make%2520it%2520possible%2520to%2520address%2520extreme%250Aviewpoint%2520shifts.%2520Inspired%2520by%2520these%2520insights%252C%2520we%2520propose%2520WristWorld%252C%2520the%2520first%250A4D%2520world%2520model%2520that%2520generates%2520wrist-view%2520videos%2520solely%2520from%2520anchor%2520views.%250AWristWorld%2520operates%2520in%2520two%2520stages%253A%2520%2528i%2529%2520Reconstruction%252C%2520which%2520extends%2520VGGT%2520and%250Aincorporates%2520our%2520Spatial%2520Projection%2520Consistency%2520%2528SPC%2529%2520Loss%2520to%2520estimate%250Ageometrically%2520consistent%2520wrist-view%2520poses%2520and%25204D%2520point%2520clouds%253B%2520%2528ii%2529%2520Generation%252C%250Awhich%2520employs%2520our%2520video%2520generation%2520model%2520to%2520synthesize%2520temporally%2520coherent%250Awrist-view%2520videos%2520from%2520the%2520reconstructed%2520perspective.%2520Experiments%2520on%2520Droid%252C%250ACalvin%252C%2520and%2520Franka%2520Panda%2520demonstrate%2520state-of-the-art%2520video%2520generation%2520with%250Asuperior%2520spatial%2520consistency%252C%2520while%2520also%2520improving%2520VLA%2520performance%252C%2520raising%2520the%250Aaverage%2520task%2520completion%2520length%2520on%2520Calvin%2520by%25203.81%2525%2520and%2520closing%252042.4%2525%2520of%2520the%250Aanchor-wrist%2520view%2520gap.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WristWorld%3A%20Generating%20Wrist-Views%20via%204D%20World%20Models%20for%20Robotic%0A%20%20Manipulation&entry.906535625=Zezhong%20Qian%20and%20Xiaowei%20Chi%20and%20Yuming%20Li%20and%20Shizun%20Wang%20and%20Zhiyuan%20Qin%20and%20Xiaozhu%20Ju%20and%20Sirui%20Han%20and%20Shanghang%20Zhang&entry.1292438233=%20%20Wrist-view%20observations%20are%20crucial%20for%20VLA%20models%20as%20they%20capture%0Afine-grained%20hand-object%20interactions%20that%20directly%20enhance%20manipulation%0Aperformance.%20Yet%20large-scale%20datasets%20rarely%20include%20such%20recordings%2C%20resulting%0Ain%20a%20substantial%20gap%20between%20abundant%20anchor%20views%20and%20scarce%20wrist%20views.%0AExisting%20world%20models%20cannot%20bridge%20this%20gap%2C%20as%20they%20require%20a%20wrist-view%0Afirst%20frame%20and%20thus%20fail%20to%20generate%20wrist-view%20videos%20from%20anchor%20views%0Aalone.%20Amid%20this%20gap%2C%20recent%20visual%20geometry%20models%20such%20as%20VGGT%20emerge%20with%0Ageometric%20and%20cross-view%20priors%20that%20make%20it%20possible%20to%20address%20extreme%0Aviewpoint%20shifts.%20Inspired%20by%20these%20insights%2C%20we%20propose%20WristWorld%2C%20the%20first%0A4D%20world%20model%20that%20generates%20wrist-view%20videos%20solely%20from%20anchor%20views.%0AWristWorld%20operates%20in%20two%20stages%3A%20%28i%29%20Reconstruction%2C%20which%20extends%20VGGT%20and%0Aincorporates%20our%20Spatial%20Projection%20Consistency%20%28SPC%29%20Loss%20to%20estimate%0Ageometrically%20consistent%20wrist-view%20poses%20and%204D%20point%20clouds%3B%20%28ii%29%20Generation%2C%0Awhich%20employs%20our%20video%20generation%20model%20to%20synthesize%20temporally%20coherent%0Awrist-view%20videos%20from%20the%20reconstructed%20perspective.%20Experiments%20on%20Droid%2C%0ACalvin%2C%20and%20Franka%20Panda%20demonstrate%20state-of-the-art%20video%20generation%20with%0Asuperior%20spatial%20consistency%2C%20while%20also%20improving%20VLA%20performance%2C%20raising%20the%0Aaverage%20task%20completion%20length%20on%20Calvin%20by%203.81%25%20and%20closing%2042.4%25%20of%20the%0Aanchor-wrist%20view%20gap.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07313v1&entry.124074799=Read"},
{"title": "Temporal Prompting Matters: Rethinking Referring Video Object\n  Segmentation", "author": "Ci-Siang Lin and Min-Hung Chen and I-Jieh Liu and Chien-Yi Wang and Sifei Liu and Yu-Chiang Frank Wang", "abstract": "  Referring Video Object Segmentation (RVOS) aims to segment the object\nreferred to by the query sentence in the video. Most existing methods require\nend-to-end training with dense mask annotations, which could be\ncomputation-consuming and less scalable. In this work, we rethink the RVOS\nproblem and aim to investigate the key to this task. Based on existing\nfoundation segmentation models, we decompose the RVOS task into referring,\nvideo, and segmentation factors, and propose a Temporal Prompt Generation and\nSelection (Tenet) framework to address the referring and video factors while\nleaving the segmentation problem to foundation models. To efficiently adapt\nimage-based foundation segmentation models to referring video object\nsegmentation, we leverage off-the-shelf object detectors and trackers to\nproduce temporal prompts associated with the referring sentence. While\nhigh-quality temporal prompts could be produced, they can not be easily\nidentified from confidence scores. To tackle this issue, we propose Prompt\nPreference Learning to evaluate the quality of the produced temporal prompts.\nBy taking such prompts to instruct image-based foundation segmentation models,\nwe would be able to produce high-quality masks for the referred object,\nenabling efficient model adaptation to referring video object segmentation.\nExperiments on RVOS benchmarks demonstrate the effectiveness of the Tenet\nframework.\n", "link": "http://arxiv.org/abs/2510.07319v1", "date": "2025-10-08", "relevancy": 2.2211, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5591}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Temporal%20Prompting%20Matters%3A%20Rethinking%20Referring%20Video%20Object%0A%20%20Segmentation&body=Title%3A%20Temporal%20Prompting%20Matters%3A%20Rethinking%20Referring%20Video%20Object%0A%20%20Segmentation%0AAuthor%3A%20Ci-Siang%20Lin%20and%20Min-Hung%20Chen%20and%20I-Jieh%20Liu%20and%20Chien-Yi%20Wang%20and%20Sifei%20Liu%20and%20Yu-Chiang%20Frank%20Wang%0AAbstract%3A%20%20%20Referring%20Video%20Object%20Segmentation%20%28RVOS%29%20aims%20to%20segment%20the%20object%0Areferred%20to%20by%20the%20query%20sentence%20in%20the%20video.%20Most%20existing%20methods%20require%0Aend-to-end%20training%20with%20dense%20mask%20annotations%2C%20which%20could%20be%0Acomputation-consuming%20and%20less%20scalable.%20In%20this%20work%2C%20we%20rethink%20the%20RVOS%0Aproblem%20and%20aim%20to%20investigate%20the%20key%20to%20this%20task.%20Based%20on%20existing%0Afoundation%20segmentation%20models%2C%20we%20decompose%20the%20RVOS%20task%20into%20referring%2C%0Avideo%2C%20and%20segmentation%20factors%2C%20and%20propose%20a%20Temporal%20Prompt%20Generation%20and%0ASelection%20%28Tenet%29%20framework%20to%20address%20the%20referring%20and%20video%20factors%20while%0Aleaving%20the%20segmentation%20problem%20to%20foundation%20models.%20To%20efficiently%20adapt%0Aimage-based%20foundation%20segmentation%20models%20to%20referring%20video%20object%0Asegmentation%2C%20we%20leverage%20off-the-shelf%20object%20detectors%20and%20trackers%20to%0Aproduce%20temporal%20prompts%20associated%20with%20the%20referring%20sentence.%20While%0Ahigh-quality%20temporal%20prompts%20could%20be%20produced%2C%20they%20can%20not%20be%20easily%0Aidentified%20from%20confidence%20scores.%20To%20tackle%20this%20issue%2C%20we%20propose%20Prompt%0APreference%20Learning%20to%20evaluate%20the%20quality%20of%20the%20produced%20temporal%20prompts.%0ABy%20taking%20such%20prompts%20to%20instruct%20image-based%20foundation%20segmentation%20models%2C%0Awe%20would%20be%20able%20to%20produce%20high-quality%20masks%20for%20the%20referred%20object%2C%0Aenabling%20efficient%20model%20adaptation%20to%20referring%20video%20object%20segmentation.%0AExperiments%20on%20RVOS%20benchmarks%20demonstrate%20the%20effectiveness%20of%20the%20Tenet%0Aframework.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07319v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTemporal%2520Prompting%2520Matters%253A%2520Rethinking%2520Referring%2520Video%2520Object%250A%2520%2520Segmentation%26entry.906535625%3DCi-Siang%2520Lin%2520and%2520Min-Hung%2520Chen%2520and%2520I-Jieh%2520Liu%2520and%2520Chien-Yi%2520Wang%2520and%2520Sifei%2520Liu%2520and%2520Yu-Chiang%2520Frank%2520Wang%26entry.1292438233%3D%2520%2520Referring%2520Video%2520Object%2520Segmentation%2520%2528RVOS%2529%2520aims%2520to%2520segment%2520the%2520object%250Areferred%2520to%2520by%2520the%2520query%2520sentence%2520in%2520the%2520video.%2520Most%2520existing%2520methods%2520require%250Aend-to-end%2520training%2520with%2520dense%2520mask%2520annotations%252C%2520which%2520could%2520be%250Acomputation-consuming%2520and%2520less%2520scalable.%2520In%2520this%2520work%252C%2520we%2520rethink%2520the%2520RVOS%250Aproblem%2520and%2520aim%2520to%2520investigate%2520the%2520key%2520to%2520this%2520task.%2520Based%2520on%2520existing%250Afoundation%2520segmentation%2520models%252C%2520we%2520decompose%2520the%2520RVOS%2520task%2520into%2520referring%252C%250Avideo%252C%2520and%2520segmentation%2520factors%252C%2520and%2520propose%2520a%2520Temporal%2520Prompt%2520Generation%2520and%250ASelection%2520%2528Tenet%2529%2520framework%2520to%2520address%2520the%2520referring%2520and%2520video%2520factors%2520while%250Aleaving%2520the%2520segmentation%2520problem%2520to%2520foundation%2520models.%2520To%2520efficiently%2520adapt%250Aimage-based%2520foundation%2520segmentation%2520models%2520to%2520referring%2520video%2520object%250Asegmentation%252C%2520we%2520leverage%2520off-the-shelf%2520object%2520detectors%2520and%2520trackers%2520to%250Aproduce%2520temporal%2520prompts%2520associated%2520with%2520the%2520referring%2520sentence.%2520While%250Ahigh-quality%2520temporal%2520prompts%2520could%2520be%2520produced%252C%2520they%2520can%2520not%2520be%2520easily%250Aidentified%2520from%2520confidence%2520scores.%2520To%2520tackle%2520this%2520issue%252C%2520we%2520propose%2520Prompt%250APreference%2520Learning%2520to%2520evaluate%2520the%2520quality%2520of%2520the%2520produced%2520temporal%2520prompts.%250ABy%2520taking%2520such%2520prompts%2520to%2520instruct%2520image-based%2520foundation%2520segmentation%2520models%252C%250Awe%2520would%2520be%2520able%2520to%2520produce%2520high-quality%2520masks%2520for%2520the%2520referred%2520object%252C%250Aenabling%2520efficient%2520model%2520adaptation%2520to%2520referring%2520video%2520object%2520segmentation.%250AExperiments%2520on%2520RVOS%2520benchmarks%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520Tenet%250Aframework.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07319v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Temporal%20Prompting%20Matters%3A%20Rethinking%20Referring%20Video%20Object%0A%20%20Segmentation&entry.906535625=Ci-Siang%20Lin%20and%20Min-Hung%20Chen%20and%20I-Jieh%20Liu%20and%20Chien-Yi%20Wang%20and%20Sifei%20Liu%20and%20Yu-Chiang%20Frank%20Wang&entry.1292438233=%20%20Referring%20Video%20Object%20Segmentation%20%28RVOS%29%20aims%20to%20segment%20the%20object%0Areferred%20to%20by%20the%20query%20sentence%20in%20the%20video.%20Most%20existing%20methods%20require%0Aend-to-end%20training%20with%20dense%20mask%20annotations%2C%20which%20could%20be%0Acomputation-consuming%20and%20less%20scalable.%20In%20this%20work%2C%20we%20rethink%20the%20RVOS%0Aproblem%20and%20aim%20to%20investigate%20the%20key%20to%20this%20task.%20Based%20on%20existing%0Afoundation%20segmentation%20models%2C%20we%20decompose%20the%20RVOS%20task%20into%20referring%2C%0Avideo%2C%20and%20segmentation%20factors%2C%20and%20propose%20a%20Temporal%20Prompt%20Generation%20and%0ASelection%20%28Tenet%29%20framework%20to%20address%20the%20referring%20and%20video%20factors%20while%0Aleaving%20the%20segmentation%20problem%20to%20foundation%20models.%20To%20efficiently%20adapt%0Aimage-based%20foundation%20segmentation%20models%20to%20referring%20video%20object%0Asegmentation%2C%20we%20leverage%20off-the-shelf%20object%20detectors%20and%20trackers%20to%0Aproduce%20temporal%20prompts%20associated%20with%20the%20referring%20sentence.%20While%0Ahigh-quality%20temporal%20prompts%20could%20be%20produced%2C%20they%20can%20not%20be%20easily%0Aidentified%20from%20confidence%20scores.%20To%20tackle%20this%20issue%2C%20we%20propose%20Prompt%0APreference%20Learning%20to%20evaluate%20the%20quality%20of%20the%20produced%20temporal%20prompts.%0ABy%20taking%20such%20prompts%20to%20instruct%20image-based%20foundation%20segmentation%20models%2C%0Awe%20would%20be%20able%20to%20produce%20high-quality%20masks%20for%20the%20referred%20object%2C%0Aenabling%20efficient%20model%20adaptation%20to%20referring%20video%20object%20segmentation.%0AExperiments%20on%20RVOS%20benchmarks%20demonstrate%20the%20effectiveness%20of%20the%20Tenet%0Aframework.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07319v1&entry.124074799=Read"},
{"title": "Evaluating Fundus-Specific Foundation Models for Diabetic Macular Edema\n  Detection", "author": "Franco Javier Arellano and Jos\u00e9 Ignacio Orlando", "abstract": "  Diabetic Macular Edema (DME) is a leading cause of vision loss among patients\nwith Diabetic Retinopathy (DR). While deep learning has shown promising results\nfor automatically detecting this condition from fundus images, its application\nremains challenging due the limited availability of annotated data. Foundation\nModels (FM) have emerged as an alternative solution. However, it is unclear if\nthey can cope with DME detection in particular. In this paper, we\nsystematically compare different FM and standard transfer learning approaches\nfor this task. Specifically, we compare the two most popular FM for retinal\nimages--RETFound and FLAIR--and an EfficientNet-B0 backbone, across different\ntraining regimes and evaluation settings in IDRiD, MESSIDOR-2 and\nOCT-and-Eye-Fundus-Images (OEFI). Results show that despite their scale, FM do\nnot consistently outperform fine-tuned CNNs in this task. In particular, an\nEfficientNet-B0 ranked first or second in terms of area under the ROC and\nprecision/recall curves in most evaluation settings, with RETFound only showing\npromising results in OEFI. FLAIR, on the other hand, demonstrated competitive\nzero-shot performance, achieving notable AUC-PR scores when prompted\nappropriately. These findings reveal that FM might not be a good tool for\nfine-grained ophthalmic tasks such as DME detection even after fine-tuning,\nsuggesting that lightweight CNNs remain strong baselines in data-scarce\nenvironments.\n", "link": "http://arxiv.org/abs/2510.07277v1", "date": "2025-10-08", "relevancy": 2.1924, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5508}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5508}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5345}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20Fundus-Specific%20Foundation%20Models%20for%20Diabetic%20Macular%20Edema%0A%20%20Detection&body=Title%3A%20Evaluating%20Fundus-Specific%20Foundation%20Models%20for%20Diabetic%20Macular%20Edema%0A%20%20Detection%0AAuthor%3A%20Franco%20Javier%20Arellano%20and%20Jos%C3%A9%20Ignacio%20Orlando%0AAbstract%3A%20%20%20Diabetic%20Macular%20Edema%20%28DME%29%20is%20a%20leading%20cause%20of%20vision%20loss%20among%20patients%0Awith%20Diabetic%20Retinopathy%20%28DR%29.%20While%20deep%20learning%20has%20shown%20promising%20results%0Afor%20automatically%20detecting%20this%20condition%20from%20fundus%20images%2C%20its%20application%0Aremains%20challenging%20due%20the%20limited%20availability%20of%20annotated%20data.%20Foundation%0AModels%20%28FM%29%20have%20emerged%20as%20an%20alternative%20solution.%20However%2C%20it%20is%20unclear%20if%0Athey%20can%20cope%20with%20DME%20detection%20in%20particular.%20In%20this%20paper%2C%20we%0Asystematically%20compare%20different%20FM%20and%20standard%20transfer%20learning%20approaches%0Afor%20this%20task.%20Specifically%2C%20we%20compare%20the%20two%20most%20popular%20FM%20for%20retinal%0Aimages--RETFound%20and%20FLAIR--and%20an%20EfficientNet-B0%20backbone%2C%20across%20different%0Atraining%20regimes%20and%20evaluation%20settings%20in%20IDRiD%2C%20MESSIDOR-2%20and%0AOCT-and-Eye-Fundus-Images%20%28OEFI%29.%20Results%20show%20that%20despite%20their%20scale%2C%20FM%20do%0Anot%20consistently%20outperform%20fine-tuned%20CNNs%20in%20this%20task.%20In%20particular%2C%20an%0AEfficientNet-B0%20ranked%20first%20or%20second%20in%20terms%20of%20area%20under%20the%20ROC%20and%0Aprecision/recall%20curves%20in%20most%20evaluation%20settings%2C%20with%20RETFound%20only%20showing%0Apromising%20results%20in%20OEFI.%20FLAIR%2C%20on%20the%20other%20hand%2C%20demonstrated%20competitive%0Azero-shot%20performance%2C%20achieving%20notable%20AUC-PR%20scores%20when%20prompted%0Aappropriately.%20These%20findings%20reveal%20that%20FM%20might%20not%20be%20a%20good%20tool%20for%0Afine-grained%20ophthalmic%20tasks%20such%20as%20DME%20detection%20even%20after%20fine-tuning%2C%0Asuggesting%20that%20lightweight%20CNNs%20remain%20strong%20baselines%20in%20data-scarce%0Aenvironments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07277v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520Fundus-Specific%2520Foundation%2520Models%2520for%2520Diabetic%2520Macular%2520Edema%250A%2520%2520Detection%26entry.906535625%3DFranco%2520Javier%2520Arellano%2520and%2520Jos%25C3%25A9%2520Ignacio%2520Orlando%26entry.1292438233%3D%2520%2520Diabetic%2520Macular%2520Edema%2520%2528DME%2529%2520is%2520a%2520leading%2520cause%2520of%2520vision%2520loss%2520among%2520patients%250Awith%2520Diabetic%2520Retinopathy%2520%2528DR%2529.%2520While%2520deep%2520learning%2520has%2520shown%2520promising%2520results%250Afor%2520automatically%2520detecting%2520this%2520condition%2520from%2520fundus%2520images%252C%2520its%2520application%250Aremains%2520challenging%2520due%2520the%2520limited%2520availability%2520of%2520annotated%2520data.%2520Foundation%250AModels%2520%2528FM%2529%2520have%2520emerged%2520as%2520an%2520alternative%2520solution.%2520However%252C%2520it%2520is%2520unclear%2520if%250Athey%2520can%2520cope%2520with%2520DME%2520detection%2520in%2520particular.%2520In%2520this%2520paper%252C%2520we%250Asystematically%2520compare%2520different%2520FM%2520and%2520standard%2520transfer%2520learning%2520approaches%250Afor%2520this%2520task.%2520Specifically%252C%2520we%2520compare%2520the%2520two%2520most%2520popular%2520FM%2520for%2520retinal%250Aimages--RETFound%2520and%2520FLAIR--and%2520an%2520EfficientNet-B0%2520backbone%252C%2520across%2520different%250Atraining%2520regimes%2520and%2520evaluation%2520settings%2520in%2520IDRiD%252C%2520MESSIDOR-2%2520and%250AOCT-and-Eye-Fundus-Images%2520%2528OEFI%2529.%2520Results%2520show%2520that%2520despite%2520their%2520scale%252C%2520FM%2520do%250Anot%2520consistently%2520outperform%2520fine-tuned%2520CNNs%2520in%2520this%2520task.%2520In%2520particular%252C%2520an%250AEfficientNet-B0%2520ranked%2520first%2520or%2520second%2520in%2520terms%2520of%2520area%2520under%2520the%2520ROC%2520and%250Aprecision/recall%2520curves%2520in%2520most%2520evaluation%2520settings%252C%2520with%2520RETFound%2520only%2520showing%250Apromising%2520results%2520in%2520OEFI.%2520FLAIR%252C%2520on%2520the%2520other%2520hand%252C%2520demonstrated%2520competitive%250Azero-shot%2520performance%252C%2520achieving%2520notable%2520AUC-PR%2520scores%2520when%2520prompted%250Aappropriately.%2520These%2520findings%2520reveal%2520that%2520FM%2520might%2520not%2520be%2520a%2520good%2520tool%2520for%250Afine-grained%2520ophthalmic%2520tasks%2520such%2520as%2520DME%2520detection%2520even%2520after%2520fine-tuning%252C%250Asuggesting%2520that%2520lightweight%2520CNNs%2520remain%2520strong%2520baselines%2520in%2520data-scarce%250Aenvironments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07277v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20Fundus-Specific%20Foundation%20Models%20for%20Diabetic%20Macular%20Edema%0A%20%20Detection&entry.906535625=Franco%20Javier%20Arellano%20and%20Jos%C3%A9%20Ignacio%20Orlando&entry.1292438233=%20%20Diabetic%20Macular%20Edema%20%28DME%29%20is%20a%20leading%20cause%20of%20vision%20loss%20among%20patients%0Awith%20Diabetic%20Retinopathy%20%28DR%29.%20While%20deep%20learning%20has%20shown%20promising%20results%0Afor%20automatically%20detecting%20this%20condition%20from%20fundus%20images%2C%20its%20application%0Aremains%20challenging%20due%20the%20limited%20availability%20of%20annotated%20data.%20Foundation%0AModels%20%28FM%29%20have%20emerged%20as%20an%20alternative%20solution.%20However%2C%20it%20is%20unclear%20if%0Athey%20can%20cope%20with%20DME%20detection%20in%20particular.%20In%20this%20paper%2C%20we%0Asystematically%20compare%20different%20FM%20and%20standard%20transfer%20learning%20approaches%0Afor%20this%20task.%20Specifically%2C%20we%20compare%20the%20two%20most%20popular%20FM%20for%20retinal%0Aimages--RETFound%20and%20FLAIR--and%20an%20EfficientNet-B0%20backbone%2C%20across%20different%0Atraining%20regimes%20and%20evaluation%20settings%20in%20IDRiD%2C%20MESSIDOR-2%20and%0AOCT-and-Eye-Fundus-Images%20%28OEFI%29.%20Results%20show%20that%20despite%20their%20scale%2C%20FM%20do%0Anot%20consistently%20outperform%20fine-tuned%20CNNs%20in%20this%20task.%20In%20particular%2C%20an%0AEfficientNet-B0%20ranked%20first%20or%20second%20in%20terms%20of%20area%20under%20the%20ROC%20and%0Aprecision/recall%20curves%20in%20most%20evaluation%20settings%2C%20with%20RETFound%20only%20showing%0Apromising%20results%20in%20OEFI.%20FLAIR%2C%20on%20the%20other%20hand%2C%20demonstrated%20competitive%0Azero-shot%20performance%2C%20achieving%20notable%20AUC-PR%20scores%20when%20prompted%0Aappropriately.%20These%20findings%20reveal%20that%20FM%20might%20not%20be%20a%20good%20tool%20for%0Afine-grained%20ophthalmic%20tasks%20such%20as%20DME%20detection%20even%20after%20fine-tuning%2C%0Asuggesting%20that%20lightweight%20CNNs%20remain%20strong%20baselines%20in%20data-scarce%0Aenvironments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07277v1&entry.124074799=Read"},
{"title": "Dyna-Think: Synergizing Reasoning, Acting, and World Model Simulation in\n  AI Agents", "author": "Xiao Yu and Baolin Peng and Ruize Xu and Michel Galley and Hao Cheng and Suman Nath and Jianfeng Gao and Zhou Yu", "abstract": "  Recent progress in reasoning with large language models (LLMs), such as\nDeepSeek-R1, demonstrates impressive capabilities in domains like mathematics\nand coding, by exhibiting complex cognitive behaviors such as verification,\ngoal decomposition, and self-reflection. However, it is unclear what behavior\nis effective and what behavior is missing for long-horizon AI agents tasks. In\nthis work, we propose Dyna-Think, a thinking framework that integrates planning\nwith an internal world model with reasoning and acting to enhance AI agent\nperformance. To enable Dyna-Think, we propose Dyna-Think Imitation Learning\n(DIT) and Dyna-Think Dyna Training (DDT). To initialize a policy with\nDyna-Think, DIT reconstructs the thinking process of R1 to focus on performing\nworld model simulation relevant to the proposed (and planned) action, and\ntrains the policy using this reconstructed data. To enhance Dyna-Think, DDT\nuses a two-stage training process to first improve the agent's world modeling\nability via objectives such as state prediction or critique generation, and\nthen improve the agent's action via policy training. We evaluate our methods on\nOSWorld and WindowsAgentArena, and demonstrate that Dyna-Think improves the\nagent's in-domain and out-of-domain performance, achieving similar best-of-n\nperformance compared to R1 while generating 2x less tokens on average. Our\nextensive empirical studies reveal that 1) using critique generation for world\nmodel training is effective to improve policy performance; and 2) AI agents\nwith better performance correlate with better world modeling abilities. We\nbelieve our results suggest a promising research direction to integrate world\nmodel simulation into AI agents to enhance their reasoning, planning, and\nacting capabilities.\n", "link": "http://arxiv.org/abs/2506.00320v2", "date": "2025-10-08", "relevancy": 2.1863, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5625}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5584}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dyna-Think%3A%20Synergizing%20Reasoning%2C%20Acting%2C%20and%20World%20Model%20Simulation%20in%0A%20%20AI%20Agents&body=Title%3A%20Dyna-Think%3A%20Synergizing%20Reasoning%2C%20Acting%2C%20and%20World%20Model%20Simulation%20in%0A%20%20AI%20Agents%0AAuthor%3A%20Xiao%20Yu%20and%20Baolin%20Peng%20and%20Ruize%20Xu%20and%20Michel%20Galley%20and%20Hao%20Cheng%20and%20Suman%20Nath%20and%20Jianfeng%20Gao%20and%20Zhou%20Yu%0AAbstract%3A%20%20%20Recent%20progress%20in%20reasoning%20with%20large%20language%20models%20%28LLMs%29%2C%20such%20as%0ADeepSeek-R1%2C%20demonstrates%20impressive%20capabilities%20in%20domains%20like%20mathematics%0Aand%20coding%2C%20by%20exhibiting%20complex%20cognitive%20behaviors%20such%20as%20verification%2C%0Agoal%20decomposition%2C%20and%20self-reflection.%20However%2C%20it%20is%20unclear%20what%20behavior%0Ais%20effective%20and%20what%20behavior%20is%20missing%20for%20long-horizon%20AI%20agents%20tasks.%20In%0Athis%20work%2C%20we%20propose%20Dyna-Think%2C%20a%20thinking%20framework%20that%20integrates%20planning%0Awith%20an%20internal%20world%20model%20with%20reasoning%20and%20acting%20to%20enhance%20AI%20agent%0Aperformance.%20To%20enable%20Dyna-Think%2C%20we%20propose%20Dyna-Think%20Imitation%20Learning%0A%28DIT%29%20and%20Dyna-Think%20Dyna%20Training%20%28DDT%29.%20To%20initialize%20a%20policy%20with%0ADyna-Think%2C%20DIT%20reconstructs%20the%20thinking%20process%20of%20R1%20to%20focus%20on%20performing%0Aworld%20model%20simulation%20relevant%20to%20the%20proposed%20%28and%20planned%29%20action%2C%20and%0Atrains%20the%20policy%20using%20this%20reconstructed%20data.%20To%20enhance%20Dyna-Think%2C%20DDT%0Auses%20a%20two-stage%20training%20process%20to%20first%20improve%20the%20agent%27s%20world%20modeling%0Aability%20via%20objectives%20such%20as%20state%20prediction%20or%20critique%20generation%2C%20and%0Athen%20improve%20the%20agent%27s%20action%20via%20policy%20training.%20We%20evaluate%20our%20methods%20on%0AOSWorld%20and%20WindowsAgentArena%2C%20and%20demonstrate%20that%20Dyna-Think%20improves%20the%0Aagent%27s%20in-domain%20and%20out-of-domain%20performance%2C%20achieving%20similar%20best-of-n%0Aperformance%20compared%20to%20R1%20while%20generating%202x%20less%20tokens%20on%20average.%20Our%0Aextensive%20empirical%20studies%20reveal%20that%201%29%20using%20critique%20generation%20for%20world%0Amodel%20training%20is%20effective%20to%20improve%20policy%20performance%3B%20and%202%29%20AI%20agents%0Awith%20better%20performance%20correlate%20with%20better%20world%20modeling%20abilities.%20We%0Abelieve%20our%20results%20suggest%20a%20promising%20research%20direction%20to%20integrate%20world%0Amodel%20simulation%20into%20AI%20agents%20to%20enhance%20their%20reasoning%2C%20planning%2C%20and%0Aacting%20capabilities.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.00320v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyna-Think%253A%2520Synergizing%2520Reasoning%252C%2520Acting%252C%2520and%2520World%2520Model%2520Simulation%2520in%250A%2520%2520AI%2520Agents%26entry.906535625%3DXiao%2520Yu%2520and%2520Baolin%2520Peng%2520and%2520Ruize%2520Xu%2520and%2520Michel%2520Galley%2520and%2520Hao%2520Cheng%2520and%2520Suman%2520Nath%2520and%2520Jianfeng%2520Gao%2520and%2520Zhou%2520Yu%26entry.1292438233%3D%2520%2520Recent%2520progress%2520in%2520reasoning%2520with%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520such%2520as%250ADeepSeek-R1%252C%2520demonstrates%2520impressive%2520capabilities%2520in%2520domains%2520like%2520mathematics%250Aand%2520coding%252C%2520by%2520exhibiting%2520complex%2520cognitive%2520behaviors%2520such%2520as%2520verification%252C%250Agoal%2520decomposition%252C%2520and%2520self-reflection.%2520However%252C%2520it%2520is%2520unclear%2520what%2520behavior%250Ais%2520effective%2520and%2520what%2520behavior%2520is%2520missing%2520for%2520long-horizon%2520AI%2520agents%2520tasks.%2520In%250Athis%2520work%252C%2520we%2520propose%2520Dyna-Think%252C%2520a%2520thinking%2520framework%2520that%2520integrates%2520planning%250Awith%2520an%2520internal%2520world%2520model%2520with%2520reasoning%2520and%2520acting%2520to%2520enhance%2520AI%2520agent%250Aperformance.%2520To%2520enable%2520Dyna-Think%252C%2520we%2520propose%2520Dyna-Think%2520Imitation%2520Learning%250A%2528DIT%2529%2520and%2520Dyna-Think%2520Dyna%2520Training%2520%2528DDT%2529.%2520To%2520initialize%2520a%2520policy%2520with%250ADyna-Think%252C%2520DIT%2520reconstructs%2520the%2520thinking%2520process%2520of%2520R1%2520to%2520focus%2520on%2520performing%250Aworld%2520model%2520simulation%2520relevant%2520to%2520the%2520proposed%2520%2528and%2520planned%2529%2520action%252C%2520and%250Atrains%2520the%2520policy%2520using%2520this%2520reconstructed%2520data.%2520To%2520enhance%2520Dyna-Think%252C%2520DDT%250Auses%2520a%2520two-stage%2520training%2520process%2520to%2520first%2520improve%2520the%2520agent%2527s%2520world%2520modeling%250Aability%2520via%2520objectives%2520such%2520as%2520state%2520prediction%2520or%2520critique%2520generation%252C%2520and%250Athen%2520improve%2520the%2520agent%2527s%2520action%2520via%2520policy%2520training.%2520We%2520evaluate%2520our%2520methods%2520on%250AOSWorld%2520and%2520WindowsAgentArena%252C%2520and%2520demonstrate%2520that%2520Dyna-Think%2520improves%2520the%250Aagent%2527s%2520in-domain%2520and%2520out-of-domain%2520performance%252C%2520achieving%2520similar%2520best-of-n%250Aperformance%2520compared%2520to%2520R1%2520while%2520generating%25202x%2520less%2520tokens%2520on%2520average.%2520Our%250Aextensive%2520empirical%2520studies%2520reveal%2520that%25201%2529%2520using%2520critique%2520generation%2520for%2520world%250Amodel%2520training%2520is%2520effective%2520to%2520improve%2520policy%2520performance%253B%2520and%25202%2529%2520AI%2520agents%250Awith%2520better%2520performance%2520correlate%2520with%2520better%2520world%2520modeling%2520abilities.%2520We%250Abelieve%2520our%2520results%2520suggest%2520a%2520promising%2520research%2520direction%2520to%2520integrate%2520world%250Amodel%2520simulation%2520into%2520AI%2520agents%2520to%2520enhance%2520their%2520reasoning%252C%2520planning%252C%2520and%250Aacting%2520capabilities.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.00320v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dyna-Think%3A%20Synergizing%20Reasoning%2C%20Acting%2C%20and%20World%20Model%20Simulation%20in%0A%20%20AI%20Agents&entry.906535625=Xiao%20Yu%20and%20Baolin%20Peng%20and%20Ruize%20Xu%20and%20Michel%20Galley%20and%20Hao%20Cheng%20and%20Suman%20Nath%20and%20Jianfeng%20Gao%20and%20Zhou%20Yu&entry.1292438233=%20%20Recent%20progress%20in%20reasoning%20with%20large%20language%20models%20%28LLMs%29%2C%20such%20as%0ADeepSeek-R1%2C%20demonstrates%20impressive%20capabilities%20in%20domains%20like%20mathematics%0Aand%20coding%2C%20by%20exhibiting%20complex%20cognitive%20behaviors%20such%20as%20verification%2C%0Agoal%20decomposition%2C%20and%20self-reflection.%20However%2C%20it%20is%20unclear%20what%20behavior%0Ais%20effective%20and%20what%20behavior%20is%20missing%20for%20long-horizon%20AI%20agents%20tasks.%20In%0Athis%20work%2C%20we%20propose%20Dyna-Think%2C%20a%20thinking%20framework%20that%20integrates%20planning%0Awith%20an%20internal%20world%20model%20with%20reasoning%20and%20acting%20to%20enhance%20AI%20agent%0Aperformance.%20To%20enable%20Dyna-Think%2C%20we%20propose%20Dyna-Think%20Imitation%20Learning%0A%28DIT%29%20and%20Dyna-Think%20Dyna%20Training%20%28DDT%29.%20To%20initialize%20a%20policy%20with%0ADyna-Think%2C%20DIT%20reconstructs%20the%20thinking%20process%20of%20R1%20to%20focus%20on%20performing%0Aworld%20model%20simulation%20relevant%20to%20the%20proposed%20%28and%20planned%29%20action%2C%20and%0Atrains%20the%20policy%20using%20this%20reconstructed%20data.%20To%20enhance%20Dyna-Think%2C%20DDT%0Auses%20a%20two-stage%20training%20process%20to%20first%20improve%20the%20agent%27s%20world%20modeling%0Aability%20via%20objectives%20such%20as%20state%20prediction%20or%20critique%20generation%2C%20and%0Athen%20improve%20the%20agent%27s%20action%20via%20policy%20training.%20We%20evaluate%20our%20methods%20on%0AOSWorld%20and%20WindowsAgentArena%2C%20and%20demonstrate%20that%20Dyna-Think%20improves%20the%0Aagent%27s%20in-domain%20and%20out-of-domain%20performance%2C%20achieving%20similar%20best-of-n%0Aperformance%20compared%20to%20R1%20while%20generating%202x%20less%20tokens%20on%20average.%20Our%0Aextensive%20empirical%20studies%20reveal%20that%201%29%20using%20critique%20generation%20for%20world%0Amodel%20training%20is%20effective%20to%20improve%20policy%20performance%3B%20and%202%29%20AI%20agents%0Awith%20better%20performance%20correlate%20with%20better%20world%20modeling%20abilities.%20We%0Abelieve%20our%20results%20suggest%20a%20promising%20research%20direction%20to%20integrate%20world%0Amodel%20simulation%20into%20AI%20agents%20to%20enhance%20their%20reasoning%2C%20planning%2C%20and%0Aacting%20capabilities.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.00320v2&entry.124074799=Read"},
{"title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling", "author": "Yunhao Fang and Weihao Yu and Shu Zhong and Qinghao Ye and Xuehan Xiong and Lai Wei", "abstract": "  Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.\n", "link": "http://arxiv.org/abs/2510.07318v1", "date": "2025-10-08", "relevancy": 2.1296, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5349}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5321}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5317}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Hippocampus%20Networks%20for%20Efficient%20Long-Context%20Modeling&body=Title%3A%20Artificial%20Hippocampus%20Networks%20for%20Efficient%20Long-Context%20Modeling%0AAuthor%3A%20Yunhao%20Fang%20and%20Weihao%20Yu%20and%20Shu%20Zhong%20and%20Qinghao%20Ye%20and%20Xuehan%20Xiong%20and%20Lai%20Wei%0AAbstract%3A%20%20%20Long-sequence%20modeling%20faces%20a%20fundamental%20trade-off%20between%20the%20efficiency%0Aof%20compressive%20fixed-size%20memory%20in%20RNN-like%20models%20and%20the%20fidelity%20of%0Alossless%20growing%20memory%20in%20attention-based%20Transformers.%20Inspired%20by%20the%0AMulti-Store%20Model%20in%20cognitive%20science%2C%20we%20introduce%20a%20memory%20framework%20of%0Aartificial%20neural%20networks.%20Our%20method%20maintains%20a%20sliding%20window%20of%20the%0ATransformer%27s%20KV%20cache%20as%20lossless%20short-term%20memory%2C%20while%20a%20learnable%20module%0Atermed%20Artificial%20Hippocampus%20Network%20%28AHN%29%20recurrently%20compresses%0Aout-of-window%20information%20into%20a%20fixed-size%20compact%20long-term%20memory.%20To%0Avalidate%20this%20framework%2C%20we%20instantiate%20AHNs%20using%20modern%20RNN-like%0Aarchitectures%2C%20including%20Mamba2%2C%20DeltaNet%2C%20and%20Gated%20DeltaNet.%20Extensive%0Aexperiments%20on%20long-context%20benchmarks%20LV-Eval%20and%20InfiniteBench%20demonstrate%0Athat%20AHN-augmented%20models%20consistently%20outperform%20sliding%20window%20baselines%20and%0Aachieve%20performance%20comparable%20or%20even%20superior%20to%20full-attention%20models%2C%20while%0Asubstantially%20reducing%20computational%20and%20memory%20requirements.%20For%20instance%2C%0Aaugmenting%20the%20Qwen2.5-3B-Instruct%20with%20AHNs%20reduces%20inference%20FLOPs%20by%2040.5%25%0Aand%20memory%20cache%20by%2074.0%25%2C%20while%20improving%20its%20average%20score%20on%20LV-Eval%20%28128k%0Asequence%20length%29%20from%204.41%20to%205.88.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/ByteDance-Seed/AHN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07318v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Hippocampus%2520Networks%2520for%2520Efficient%2520Long-Context%2520Modeling%26entry.906535625%3DYunhao%2520Fang%2520and%2520Weihao%2520Yu%2520and%2520Shu%2520Zhong%2520and%2520Qinghao%2520Ye%2520and%2520Xuehan%2520Xiong%2520and%2520Lai%2520Wei%26entry.1292438233%3D%2520%2520Long-sequence%2520modeling%2520faces%2520a%2520fundamental%2520trade-off%2520between%2520the%2520efficiency%250Aof%2520compressive%2520fixed-size%2520memory%2520in%2520RNN-like%2520models%2520and%2520the%2520fidelity%2520of%250Alossless%2520growing%2520memory%2520in%2520attention-based%2520Transformers.%2520Inspired%2520by%2520the%250AMulti-Store%2520Model%2520in%2520cognitive%2520science%252C%2520we%2520introduce%2520a%2520memory%2520framework%2520of%250Aartificial%2520neural%2520networks.%2520Our%2520method%2520maintains%2520a%2520sliding%2520window%2520of%2520the%250ATransformer%2527s%2520KV%2520cache%2520as%2520lossless%2520short-term%2520memory%252C%2520while%2520a%2520learnable%2520module%250Atermed%2520Artificial%2520Hippocampus%2520Network%2520%2528AHN%2529%2520recurrently%2520compresses%250Aout-of-window%2520information%2520into%2520a%2520fixed-size%2520compact%2520long-term%2520memory.%2520To%250Avalidate%2520this%2520framework%252C%2520we%2520instantiate%2520AHNs%2520using%2520modern%2520RNN-like%250Aarchitectures%252C%2520including%2520Mamba2%252C%2520DeltaNet%252C%2520and%2520Gated%2520DeltaNet.%2520Extensive%250Aexperiments%2520on%2520long-context%2520benchmarks%2520LV-Eval%2520and%2520InfiniteBench%2520demonstrate%250Athat%2520AHN-augmented%2520models%2520consistently%2520outperform%2520sliding%2520window%2520baselines%2520and%250Aachieve%2520performance%2520comparable%2520or%2520even%2520superior%2520to%2520full-attention%2520models%252C%2520while%250Asubstantially%2520reducing%2520computational%2520and%2520memory%2520requirements.%2520For%2520instance%252C%250Aaugmenting%2520the%2520Qwen2.5-3B-Instruct%2520with%2520AHNs%2520reduces%2520inference%2520FLOPs%2520by%252040.5%2525%250Aand%2520memory%2520cache%2520by%252074.0%2525%252C%2520while%2520improving%2520its%2520average%2520score%2520on%2520LV-Eval%2520%2528128k%250Asequence%2520length%2529%2520from%25204.41%2520to%25205.88.%2520Code%2520is%2520available%2520at%253A%250Ahttps%253A//github.com/ByteDance-Seed/AHN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07318v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Hippocampus%20Networks%20for%20Efficient%20Long-Context%20Modeling&entry.906535625=Yunhao%20Fang%20and%20Weihao%20Yu%20and%20Shu%20Zhong%20and%20Qinghao%20Ye%20and%20Xuehan%20Xiong%20and%20Lai%20Wei&entry.1292438233=%20%20Long-sequence%20modeling%20faces%20a%20fundamental%20trade-off%20between%20the%20efficiency%0Aof%20compressive%20fixed-size%20memory%20in%20RNN-like%20models%20and%20the%20fidelity%20of%0Alossless%20growing%20memory%20in%20attention-based%20Transformers.%20Inspired%20by%20the%0AMulti-Store%20Model%20in%20cognitive%20science%2C%20we%20introduce%20a%20memory%20framework%20of%0Aartificial%20neural%20networks.%20Our%20method%20maintains%20a%20sliding%20window%20of%20the%0ATransformer%27s%20KV%20cache%20as%20lossless%20short-term%20memory%2C%20while%20a%20learnable%20module%0Atermed%20Artificial%20Hippocampus%20Network%20%28AHN%29%20recurrently%20compresses%0Aout-of-window%20information%20into%20a%20fixed-size%20compact%20long-term%20memory.%20To%0Avalidate%20this%20framework%2C%20we%20instantiate%20AHNs%20using%20modern%20RNN-like%0Aarchitectures%2C%20including%20Mamba2%2C%20DeltaNet%2C%20and%20Gated%20DeltaNet.%20Extensive%0Aexperiments%20on%20long-context%20benchmarks%20LV-Eval%20and%20InfiniteBench%20demonstrate%0Athat%20AHN-augmented%20models%20consistently%20outperform%20sliding%20window%20baselines%20and%0Aachieve%20performance%20comparable%20or%20even%20superior%20to%20full-attention%20models%2C%20while%0Asubstantially%20reducing%20computational%20and%20memory%20requirements.%20For%20instance%2C%0Aaugmenting%20the%20Qwen2.5-3B-Instruct%20with%20AHNs%20reduces%20inference%20FLOPs%20by%2040.5%25%0Aand%20memory%20cache%20by%2074.0%25%2C%20while%20improving%20its%20average%20score%20on%20LV-Eval%20%28128k%0Asequence%20length%29%20from%204.41%20to%205.88.%20Code%20is%20available%20at%3A%0Ahttps%3A//github.com/ByteDance-Seed/AHN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07318v1&entry.124074799=Read"},
{"title": "Dual Natural Gradient Descent for Scalable Training of Physics-Informed\n  Neural Networks", "author": "Anas Jnini and Flavio Vella", "abstract": "  Natural-gradient methods markedly accelerate the training of Physics-Informed\nNeural Networks (PINNs), yet their Gauss--Newton update must be solved in the\nparameter space, incurring a prohibitive $O(n^3)$ time complexity, where $n$ is\nthe number of network trainable weights. We show that exactly the same step can\ninstead be formulated in a generally smaller residual space of size $m =\n\\sum_{\\gamma} N_{\\gamma} d_{\\gamma}$, where each residual class $\\gamma$ (e.g.\nPDE interior, boundary, initial data) contributes $N_{\\gamma}$ collocation\npoints of output dimension $d_{\\gamma}$.\n  Building on this insight, we introduce \\textit{Dual Natural Gradient Descent}\n(D-NGD). D-NGD computes the Gauss--Newton step in residual space, augments it\nwith a geodesic-acceleration correction at negligible extra cost, and provides\nboth a dense direct solver for modest $m$ and a Nystrom-preconditioned\nconjugate-gradient solver for larger $m$.\n  Experimentally, D-NGD scales second-order PINN optimization to networks with\nup to 12.8 million parameters, delivers one- to three-order-of-magnitude lower\nfinal error $L^2$ than first-order methods (Adam, SGD) and quasi-Newton\nmethods, and -- crucially -- enables natural-gradient training of PINNs at this\nscale on a single GPU.\n", "link": "http://arxiv.org/abs/2505.21404v2", "date": "2025-10-08", "relevancy": 2.1135, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5396}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5295}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4977}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual%20Natural%20Gradient%20Descent%20for%20Scalable%20Training%20of%20Physics-Informed%0A%20%20Neural%20Networks&body=Title%3A%20Dual%20Natural%20Gradient%20Descent%20for%20Scalable%20Training%20of%20Physics-Informed%0A%20%20Neural%20Networks%0AAuthor%3A%20Anas%20Jnini%20and%20Flavio%20Vella%0AAbstract%3A%20%20%20Natural-gradient%20methods%20markedly%20accelerate%20the%20training%20of%20Physics-Informed%0ANeural%20Networks%20%28PINNs%29%2C%20yet%20their%20Gauss--Newton%20update%20must%20be%20solved%20in%20the%0Aparameter%20space%2C%20incurring%20a%20prohibitive%20%24O%28n%5E3%29%24%20time%20complexity%2C%20where%20%24n%24%20is%0Athe%20number%20of%20network%20trainable%20weights.%20We%20show%20that%20exactly%20the%20same%20step%20can%0Ainstead%20be%20formulated%20in%20a%20generally%20smaller%20residual%20space%20of%20size%20%24m%20%3D%0A%5Csum_%7B%5Cgamma%7D%20N_%7B%5Cgamma%7D%20d_%7B%5Cgamma%7D%24%2C%20where%20each%20residual%20class%20%24%5Cgamma%24%20%28e.g.%0APDE%20interior%2C%20boundary%2C%20initial%20data%29%20contributes%20%24N_%7B%5Cgamma%7D%24%20collocation%0Apoints%20of%20output%20dimension%20%24d_%7B%5Cgamma%7D%24.%0A%20%20Building%20on%20this%20insight%2C%20we%20introduce%20%5Ctextit%7BDual%20Natural%20Gradient%20Descent%7D%0A%28D-NGD%29.%20D-NGD%20computes%20the%20Gauss--Newton%20step%20in%20residual%20space%2C%20augments%20it%0Awith%20a%20geodesic-acceleration%20correction%20at%20negligible%20extra%20cost%2C%20and%20provides%0Aboth%20a%20dense%20direct%20solver%20for%20modest%20%24m%24%20and%20a%20Nystrom-preconditioned%0Aconjugate-gradient%20solver%20for%20larger%20%24m%24.%0A%20%20Experimentally%2C%20D-NGD%20scales%20second-order%20PINN%20optimization%20to%20networks%20with%0Aup%20to%2012.8%20million%20parameters%2C%20delivers%20one-%20to%20three-order-of-magnitude%20lower%0Afinal%20error%20%24L%5E2%24%20than%20first-order%20methods%20%28Adam%2C%20SGD%29%20and%20quasi-Newton%0Amethods%2C%20and%20--%20crucially%20--%20enables%20natural-gradient%20training%20of%20PINNs%20at%20this%0Ascale%20on%20a%20single%20GPU.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2505.21404v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual%2520Natural%2520Gradient%2520Descent%2520for%2520Scalable%2520Training%2520of%2520Physics-Informed%250A%2520%2520Neural%2520Networks%26entry.906535625%3DAnas%2520Jnini%2520and%2520Flavio%2520Vella%26entry.1292438233%3D%2520%2520Natural-gradient%2520methods%2520markedly%2520accelerate%2520the%2520training%2520of%2520Physics-Informed%250ANeural%2520Networks%2520%2528PINNs%2529%252C%2520yet%2520their%2520Gauss--Newton%2520update%2520must%2520be%2520solved%2520in%2520the%250Aparameter%2520space%252C%2520incurring%2520a%2520prohibitive%2520%2524O%2528n%255E3%2529%2524%2520time%2520complexity%252C%2520where%2520%2524n%2524%2520is%250Athe%2520number%2520of%2520network%2520trainable%2520weights.%2520We%2520show%2520that%2520exactly%2520the%2520same%2520step%2520can%250Ainstead%2520be%2520formulated%2520in%2520a%2520generally%2520smaller%2520residual%2520space%2520of%2520size%2520%2524m%2520%253D%250A%255Csum_%257B%255Cgamma%257D%2520N_%257B%255Cgamma%257D%2520d_%257B%255Cgamma%257D%2524%252C%2520where%2520each%2520residual%2520class%2520%2524%255Cgamma%2524%2520%2528e.g.%250APDE%2520interior%252C%2520boundary%252C%2520initial%2520data%2529%2520contributes%2520%2524N_%257B%255Cgamma%257D%2524%2520collocation%250Apoints%2520of%2520output%2520dimension%2520%2524d_%257B%255Cgamma%257D%2524.%250A%2520%2520Building%2520on%2520this%2520insight%252C%2520we%2520introduce%2520%255Ctextit%257BDual%2520Natural%2520Gradient%2520Descent%257D%250A%2528D-NGD%2529.%2520D-NGD%2520computes%2520the%2520Gauss--Newton%2520step%2520in%2520residual%2520space%252C%2520augments%2520it%250Awith%2520a%2520geodesic-acceleration%2520correction%2520at%2520negligible%2520extra%2520cost%252C%2520and%2520provides%250Aboth%2520a%2520dense%2520direct%2520solver%2520for%2520modest%2520%2524m%2524%2520and%2520a%2520Nystrom-preconditioned%250Aconjugate-gradient%2520solver%2520for%2520larger%2520%2524m%2524.%250A%2520%2520Experimentally%252C%2520D-NGD%2520scales%2520second-order%2520PINN%2520optimization%2520to%2520networks%2520with%250Aup%2520to%252012.8%2520million%2520parameters%252C%2520delivers%2520one-%2520to%2520three-order-of-magnitude%2520lower%250Afinal%2520error%2520%2524L%255E2%2524%2520than%2520first-order%2520methods%2520%2528Adam%252C%2520SGD%2529%2520and%2520quasi-Newton%250Amethods%252C%2520and%2520--%2520crucially%2520--%2520enables%2520natural-gradient%2520training%2520of%2520PINNs%2520at%2520this%250Ascale%2520on%2520a%2520single%2520GPU.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.21404v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual%20Natural%20Gradient%20Descent%20for%20Scalable%20Training%20of%20Physics-Informed%0A%20%20Neural%20Networks&entry.906535625=Anas%20Jnini%20and%20Flavio%20Vella&entry.1292438233=%20%20Natural-gradient%20methods%20markedly%20accelerate%20the%20training%20of%20Physics-Informed%0ANeural%20Networks%20%28PINNs%29%2C%20yet%20their%20Gauss--Newton%20update%20must%20be%20solved%20in%20the%0Aparameter%20space%2C%20incurring%20a%20prohibitive%20%24O%28n%5E3%29%24%20time%20complexity%2C%20where%20%24n%24%20is%0Athe%20number%20of%20network%20trainable%20weights.%20We%20show%20that%20exactly%20the%20same%20step%20can%0Ainstead%20be%20formulated%20in%20a%20generally%20smaller%20residual%20space%20of%20size%20%24m%20%3D%0A%5Csum_%7B%5Cgamma%7D%20N_%7B%5Cgamma%7D%20d_%7B%5Cgamma%7D%24%2C%20where%20each%20residual%20class%20%24%5Cgamma%24%20%28e.g.%0APDE%20interior%2C%20boundary%2C%20initial%20data%29%20contributes%20%24N_%7B%5Cgamma%7D%24%20collocation%0Apoints%20of%20output%20dimension%20%24d_%7B%5Cgamma%7D%24.%0A%20%20Building%20on%20this%20insight%2C%20we%20introduce%20%5Ctextit%7BDual%20Natural%20Gradient%20Descent%7D%0A%28D-NGD%29.%20D-NGD%20computes%20the%20Gauss--Newton%20step%20in%20residual%20space%2C%20augments%20it%0Awith%20a%20geodesic-acceleration%20correction%20at%20negligible%20extra%20cost%2C%20and%20provides%0Aboth%20a%20dense%20direct%20solver%20for%20modest%20%24m%24%20and%20a%20Nystrom-preconditioned%0Aconjugate-gradient%20solver%20for%20larger%20%24m%24.%0A%20%20Experimentally%2C%20D-NGD%20scales%20second-order%20PINN%20optimization%20to%20networks%20with%0Aup%20to%2012.8%20million%20parameters%2C%20delivers%20one-%20to%20three-order-of-magnitude%20lower%0Afinal%20error%20%24L%5E2%24%20than%20first-order%20methods%20%28Adam%2C%20SGD%29%20and%20quasi-Newton%0Amethods%2C%20and%20--%20crucially%20--%20enables%20natural-gradient%20training%20of%20PINNs%20at%20this%0Ascale%20on%20a%20single%20GPU.%0A&entry.1838667208=http%3A//arxiv.org/abs/2505.21404v2&entry.124074799=Read"},
{"title": "GTCN-G: A Residual Graph-Temporal Fusion Network for Imbalanced\n  Intrusion Detection (Preprint)", "author": "Tianxiang Xu and Zhichao Wen and Xinyu Zhao and Qi Hu and Yan Li and Chang Liu", "abstract": "  The escalating complexity of network threats and the inherent class imbalance\nin traffic data present formidable challenges for modern Intrusion Detection\nSystems (IDS). While Graph Neural Networks (GNNs) excel in modeling topological\nstructures and Temporal Convolutional Networks (TCNs) are proficient in\ncapturing time-series dependencies, a framework that synergistically integrates\nboth while explicitly addressing data imbalance remains an open challenge. This\npaper introduces a novel deep learning framework, named Gated Temporal\nConvolutional Network and Graph (GTCN-G), engineered to overcome these\nlimitations. Our model uniquely fuses a Gated TCN (G-TCN) for extracting\nhierarchical temporal features from network flows with a Graph Convolutional\nNetwork (GCN) designed to learn from the underlying graph structure. The core\ninnovation lies in the integration of a residual learning mechanism,\nimplemented via a Graph Attention Network (GAT). This mechanism preserves\noriginal feature information through residual connections, which is critical\nfor mitigating the class imbalance problem and enhancing detection sensitivity\nfor rare malicious activities (minority classes). We conducted extensive\nexperiments on two public benchmark datasets, UNSW-NB15 and ToN-IoT, to\nvalidate our approach. The empirical results demonstrate that the proposed\nGTCN-G model achieves state-of-the-art performance, significantly outperforming\nexisting baseline models in both binary and multi-class classification tasks.\n", "link": "http://arxiv.org/abs/2510.07285v1", "date": "2025-10-08", "relevancy": 2.0729, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5317}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5128}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GTCN-G%3A%20A%20Residual%20Graph-Temporal%20Fusion%20Network%20for%20Imbalanced%0A%20%20Intrusion%20Detection%20%28Preprint%29&body=Title%3A%20GTCN-G%3A%20A%20Residual%20Graph-Temporal%20Fusion%20Network%20for%20Imbalanced%0A%20%20Intrusion%20Detection%20%28Preprint%29%0AAuthor%3A%20Tianxiang%20Xu%20and%20Zhichao%20Wen%20and%20Xinyu%20Zhao%20and%20Qi%20Hu%20and%20Yan%20Li%20and%20Chang%20Liu%0AAbstract%3A%20%20%20The%20escalating%20complexity%20of%20network%20threats%20and%20the%20inherent%20class%20imbalance%0Ain%20traffic%20data%20present%20formidable%20challenges%20for%20modern%20Intrusion%20Detection%0ASystems%20%28IDS%29.%20While%20Graph%20Neural%20Networks%20%28GNNs%29%20excel%20in%20modeling%20topological%0Astructures%20and%20Temporal%20Convolutional%20Networks%20%28TCNs%29%20are%20proficient%20in%0Acapturing%20time-series%20dependencies%2C%20a%20framework%20that%20synergistically%20integrates%0Aboth%20while%20explicitly%20addressing%20data%20imbalance%20remains%20an%20open%20challenge.%20This%0Apaper%20introduces%20a%20novel%20deep%20learning%20framework%2C%20named%20Gated%20Temporal%0AConvolutional%20Network%20and%20Graph%20%28GTCN-G%29%2C%20engineered%20to%20overcome%20these%0Alimitations.%20Our%20model%20uniquely%20fuses%20a%20Gated%20TCN%20%28G-TCN%29%20for%20extracting%0Ahierarchical%20temporal%20features%20from%20network%20flows%20with%20a%20Graph%20Convolutional%0ANetwork%20%28GCN%29%20designed%20to%20learn%20from%20the%20underlying%20graph%20structure.%20The%20core%0Ainnovation%20lies%20in%20the%20integration%20of%20a%20residual%20learning%20mechanism%2C%0Aimplemented%20via%20a%20Graph%20Attention%20Network%20%28GAT%29.%20This%20mechanism%20preserves%0Aoriginal%20feature%20information%20through%20residual%20connections%2C%20which%20is%20critical%0Afor%20mitigating%20the%20class%20imbalance%20problem%20and%20enhancing%20detection%20sensitivity%0Afor%20rare%20malicious%20activities%20%28minority%20classes%29.%20We%20conducted%20extensive%0Aexperiments%20on%20two%20public%20benchmark%20datasets%2C%20UNSW-NB15%20and%20ToN-IoT%2C%20to%0Avalidate%20our%20approach.%20The%20empirical%20results%20demonstrate%20that%20the%20proposed%0AGTCN-G%20model%20achieves%20state-of-the-art%20performance%2C%20significantly%20outperforming%0Aexisting%20baseline%20models%20in%20both%20binary%20and%20multi-class%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07285v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGTCN-G%253A%2520A%2520Residual%2520Graph-Temporal%2520Fusion%2520Network%2520for%2520Imbalanced%250A%2520%2520Intrusion%2520Detection%2520%2528Preprint%2529%26entry.906535625%3DTianxiang%2520Xu%2520and%2520Zhichao%2520Wen%2520and%2520Xinyu%2520Zhao%2520and%2520Qi%2520Hu%2520and%2520Yan%2520Li%2520and%2520Chang%2520Liu%26entry.1292438233%3D%2520%2520The%2520escalating%2520complexity%2520of%2520network%2520threats%2520and%2520the%2520inherent%2520class%2520imbalance%250Ain%2520traffic%2520data%2520present%2520formidable%2520challenges%2520for%2520modern%2520Intrusion%2520Detection%250ASystems%2520%2528IDS%2529.%2520While%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%2520excel%2520in%2520modeling%2520topological%250Astructures%2520and%2520Temporal%2520Convolutional%2520Networks%2520%2528TCNs%2529%2520are%2520proficient%2520in%250Acapturing%2520time-series%2520dependencies%252C%2520a%2520framework%2520that%2520synergistically%2520integrates%250Aboth%2520while%2520explicitly%2520addressing%2520data%2520imbalance%2520remains%2520an%2520open%2520challenge.%2520This%250Apaper%2520introduces%2520a%2520novel%2520deep%2520learning%2520framework%252C%2520named%2520Gated%2520Temporal%250AConvolutional%2520Network%2520and%2520Graph%2520%2528GTCN-G%2529%252C%2520engineered%2520to%2520overcome%2520these%250Alimitations.%2520Our%2520model%2520uniquely%2520fuses%2520a%2520Gated%2520TCN%2520%2528G-TCN%2529%2520for%2520extracting%250Ahierarchical%2520temporal%2520features%2520from%2520network%2520flows%2520with%2520a%2520Graph%2520Convolutional%250ANetwork%2520%2528GCN%2529%2520designed%2520to%2520learn%2520from%2520the%2520underlying%2520graph%2520structure.%2520The%2520core%250Ainnovation%2520lies%2520in%2520the%2520integration%2520of%2520a%2520residual%2520learning%2520mechanism%252C%250Aimplemented%2520via%2520a%2520Graph%2520Attention%2520Network%2520%2528GAT%2529.%2520This%2520mechanism%2520preserves%250Aoriginal%2520feature%2520information%2520through%2520residual%2520connections%252C%2520which%2520is%2520critical%250Afor%2520mitigating%2520the%2520class%2520imbalance%2520problem%2520and%2520enhancing%2520detection%2520sensitivity%250Afor%2520rare%2520malicious%2520activities%2520%2528minority%2520classes%2529.%2520We%2520conducted%2520extensive%250Aexperiments%2520on%2520two%2520public%2520benchmark%2520datasets%252C%2520UNSW-NB15%2520and%2520ToN-IoT%252C%2520to%250Avalidate%2520our%2520approach.%2520The%2520empirical%2520results%2520demonstrate%2520that%2520the%2520proposed%250AGTCN-G%2520model%2520achieves%2520state-of-the-art%2520performance%252C%2520significantly%2520outperforming%250Aexisting%2520baseline%2520models%2520in%2520both%2520binary%2520and%2520multi-class%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07285v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GTCN-G%3A%20A%20Residual%20Graph-Temporal%20Fusion%20Network%20for%20Imbalanced%0A%20%20Intrusion%20Detection%20%28Preprint%29&entry.906535625=Tianxiang%20Xu%20and%20Zhichao%20Wen%20and%20Xinyu%20Zhao%20and%20Qi%20Hu%20and%20Yan%20Li%20and%20Chang%20Liu&entry.1292438233=%20%20The%20escalating%20complexity%20of%20network%20threats%20and%20the%20inherent%20class%20imbalance%0Ain%20traffic%20data%20present%20formidable%20challenges%20for%20modern%20Intrusion%20Detection%0ASystems%20%28IDS%29.%20While%20Graph%20Neural%20Networks%20%28GNNs%29%20excel%20in%20modeling%20topological%0Astructures%20and%20Temporal%20Convolutional%20Networks%20%28TCNs%29%20are%20proficient%20in%0Acapturing%20time-series%20dependencies%2C%20a%20framework%20that%20synergistically%20integrates%0Aboth%20while%20explicitly%20addressing%20data%20imbalance%20remains%20an%20open%20challenge.%20This%0Apaper%20introduces%20a%20novel%20deep%20learning%20framework%2C%20named%20Gated%20Temporal%0AConvolutional%20Network%20and%20Graph%20%28GTCN-G%29%2C%20engineered%20to%20overcome%20these%0Alimitations.%20Our%20model%20uniquely%20fuses%20a%20Gated%20TCN%20%28G-TCN%29%20for%20extracting%0Ahierarchical%20temporal%20features%20from%20network%20flows%20with%20a%20Graph%20Convolutional%0ANetwork%20%28GCN%29%20designed%20to%20learn%20from%20the%20underlying%20graph%20structure.%20The%20core%0Ainnovation%20lies%20in%20the%20integration%20of%20a%20residual%20learning%20mechanism%2C%0Aimplemented%20via%20a%20Graph%20Attention%20Network%20%28GAT%29.%20This%20mechanism%20preserves%0Aoriginal%20feature%20information%20through%20residual%20connections%2C%20which%20is%20critical%0Afor%20mitigating%20the%20class%20imbalance%20problem%20and%20enhancing%20detection%20sensitivity%0Afor%20rare%20malicious%20activities%20%28minority%20classes%29.%20We%20conducted%20extensive%0Aexperiments%20on%20two%20public%20benchmark%20datasets%2C%20UNSW-NB15%20and%20ToN-IoT%2C%20to%0Avalidate%20our%20approach.%20The%20empirical%20results%20demonstrate%20that%20the%20proposed%0AGTCN-G%20model%20achieves%20state-of-the-art%20performance%2C%20significantly%20outperforming%0Aexisting%20baseline%20models%20in%20both%20binary%20and%20multi-class%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07285v1&entry.124074799=Read"},
{"title": "GyroSwin: 5D Surrogates for Gyrokinetic Plasma Turbulence Simulations", "author": "Fabian Paischer and Gianluca Galletti and William Hornsby and Paul Setinek and Lorenzo Zanisi and Naomi Carey and Stanislas Pamela and Johannes Brandstetter", "abstract": "  Nuclear fusion plays a pivotal role in the quest for reliable and sustainable\nenergy production. A major roadblock to viable fusion power is understanding\nplasma turbulence, which significantly impairs plasma confinement, and is vital\nfor next-generation reactor design. Plasma turbulence is governed by the\nnonlinear gyrokinetic equation, which evolves a 5D distribution function over\ntime. Due to its high computational cost, reduced-order models are often\nemployed in practice to approximate turbulent transport of energy. However,\nthey omit nonlinear effects unique to the full 5D dynamics. To tackle this, we\nintroduce GyroSwin, the first scalable 5D neural surrogate that can model 5D\nnonlinear gyrokinetic simulations, thereby capturing the physical phenomena\nneglected by reduced models, while providing accurate estimates of turbulent\nheat transport.GyroSwin (i) extends hierarchical Vision Transformers to 5D,\n(ii) introduces cross-attention and integration modules for latent\n3D$\\leftrightarrow$5D interactions between electrostatic potential fields and\nthe distribution function, and (iii) performs channelwise mode separation\ninspired by nonlinear physics. We demonstrate that GyroSwin outperforms widely\nused reduced numerics on heat flux prediction, captures the turbulent energy\ncascade, and reduces the cost of fully resolved nonlinear gyrokinetics by three\norders of magnitude while remaining physically verifiable. GyroSwin shows\npromising scaling laws, tested up to one billion parameters, paving the way for\nscalable neural surrogates for gyrokinetic simulations of plasma turbulence.\n", "link": "http://arxiv.org/abs/2510.07314v1", "date": "2025-10-08", "relevancy": 2.0489, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.522}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5108}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5097}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GyroSwin%3A%205D%20Surrogates%20for%20Gyrokinetic%20Plasma%20Turbulence%20Simulations&body=Title%3A%20GyroSwin%3A%205D%20Surrogates%20for%20Gyrokinetic%20Plasma%20Turbulence%20Simulations%0AAuthor%3A%20Fabian%20Paischer%20and%20Gianluca%20Galletti%20and%20William%20Hornsby%20and%20Paul%20Setinek%20and%20Lorenzo%20Zanisi%20and%20Naomi%20Carey%20and%20Stanislas%20Pamela%20and%20Johannes%20Brandstetter%0AAbstract%3A%20%20%20Nuclear%20fusion%20plays%20a%20pivotal%20role%20in%20the%20quest%20for%20reliable%20and%20sustainable%0Aenergy%20production.%20A%20major%20roadblock%20to%20viable%20fusion%20power%20is%20understanding%0Aplasma%20turbulence%2C%20which%20significantly%20impairs%20plasma%20confinement%2C%20and%20is%20vital%0Afor%20next-generation%20reactor%20design.%20Plasma%20turbulence%20is%20governed%20by%20the%0Anonlinear%20gyrokinetic%20equation%2C%20which%20evolves%20a%205D%20distribution%20function%20over%0Atime.%20Due%20to%20its%20high%20computational%20cost%2C%20reduced-order%20models%20are%20often%0Aemployed%20in%20practice%20to%20approximate%20turbulent%20transport%20of%20energy.%20However%2C%0Athey%20omit%20nonlinear%20effects%20unique%20to%20the%20full%205D%20dynamics.%20To%20tackle%20this%2C%20we%0Aintroduce%20GyroSwin%2C%20the%20first%20scalable%205D%20neural%20surrogate%20that%20can%20model%205D%0Anonlinear%20gyrokinetic%20simulations%2C%20thereby%20capturing%20the%20physical%20phenomena%0Aneglected%20by%20reduced%20models%2C%20while%20providing%20accurate%20estimates%20of%20turbulent%0Aheat%20transport.GyroSwin%20%28i%29%20extends%20hierarchical%20Vision%20Transformers%20to%205D%2C%0A%28ii%29%20introduces%20cross-attention%20and%20integration%20modules%20for%20latent%0A3D%24%5Cleftrightarrow%245D%20interactions%20between%20electrostatic%20potential%20fields%20and%0Athe%20distribution%20function%2C%20and%20%28iii%29%20performs%20channelwise%20mode%20separation%0Ainspired%20by%20nonlinear%20physics.%20We%20demonstrate%20that%20GyroSwin%20outperforms%20widely%0Aused%20reduced%20numerics%20on%20heat%20flux%20prediction%2C%20captures%20the%20turbulent%20energy%0Acascade%2C%20and%20reduces%20the%20cost%20of%20fully%20resolved%20nonlinear%20gyrokinetics%20by%20three%0Aorders%20of%20magnitude%20while%20remaining%20physically%20verifiable.%20GyroSwin%20shows%0Apromising%20scaling%20laws%2C%20tested%20up%20to%20one%20billion%20parameters%2C%20paving%20the%20way%20for%0Ascalable%20neural%20surrogates%20for%20gyrokinetic%20simulations%20of%20plasma%20turbulence.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGyroSwin%253A%25205D%2520Surrogates%2520for%2520Gyrokinetic%2520Plasma%2520Turbulence%2520Simulations%26entry.906535625%3DFabian%2520Paischer%2520and%2520Gianluca%2520Galletti%2520and%2520William%2520Hornsby%2520and%2520Paul%2520Setinek%2520and%2520Lorenzo%2520Zanisi%2520and%2520Naomi%2520Carey%2520and%2520Stanislas%2520Pamela%2520and%2520Johannes%2520Brandstetter%26entry.1292438233%3D%2520%2520Nuclear%2520fusion%2520plays%2520a%2520pivotal%2520role%2520in%2520the%2520quest%2520for%2520reliable%2520and%2520sustainable%250Aenergy%2520production.%2520A%2520major%2520roadblock%2520to%2520viable%2520fusion%2520power%2520is%2520understanding%250Aplasma%2520turbulence%252C%2520which%2520significantly%2520impairs%2520plasma%2520confinement%252C%2520and%2520is%2520vital%250Afor%2520next-generation%2520reactor%2520design.%2520Plasma%2520turbulence%2520is%2520governed%2520by%2520the%250Anonlinear%2520gyrokinetic%2520equation%252C%2520which%2520evolves%2520a%25205D%2520distribution%2520function%2520over%250Atime.%2520Due%2520to%2520its%2520high%2520computational%2520cost%252C%2520reduced-order%2520models%2520are%2520often%250Aemployed%2520in%2520practice%2520to%2520approximate%2520turbulent%2520transport%2520of%2520energy.%2520However%252C%250Athey%2520omit%2520nonlinear%2520effects%2520unique%2520to%2520the%2520full%25205D%2520dynamics.%2520To%2520tackle%2520this%252C%2520we%250Aintroduce%2520GyroSwin%252C%2520the%2520first%2520scalable%25205D%2520neural%2520surrogate%2520that%2520can%2520model%25205D%250Anonlinear%2520gyrokinetic%2520simulations%252C%2520thereby%2520capturing%2520the%2520physical%2520phenomena%250Aneglected%2520by%2520reduced%2520models%252C%2520while%2520providing%2520accurate%2520estimates%2520of%2520turbulent%250Aheat%2520transport.GyroSwin%2520%2528i%2529%2520extends%2520hierarchical%2520Vision%2520Transformers%2520to%25205D%252C%250A%2528ii%2529%2520introduces%2520cross-attention%2520and%2520integration%2520modules%2520for%2520latent%250A3D%2524%255Cleftrightarrow%25245D%2520interactions%2520between%2520electrostatic%2520potential%2520fields%2520and%250Athe%2520distribution%2520function%252C%2520and%2520%2528iii%2529%2520performs%2520channelwise%2520mode%2520separation%250Ainspired%2520by%2520nonlinear%2520physics.%2520We%2520demonstrate%2520that%2520GyroSwin%2520outperforms%2520widely%250Aused%2520reduced%2520numerics%2520on%2520heat%2520flux%2520prediction%252C%2520captures%2520the%2520turbulent%2520energy%250Acascade%252C%2520and%2520reduces%2520the%2520cost%2520of%2520fully%2520resolved%2520nonlinear%2520gyrokinetics%2520by%2520three%250Aorders%2520of%2520magnitude%2520while%2520remaining%2520physically%2520verifiable.%2520GyroSwin%2520shows%250Apromising%2520scaling%2520laws%252C%2520tested%2520up%2520to%2520one%2520billion%2520parameters%252C%2520paving%2520the%2520way%2520for%250Ascalable%2520neural%2520surrogates%2520for%2520gyrokinetic%2520simulations%2520of%2520plasma%2520turbulence.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GyroSwin%3A%205D%20Surrogates%20for%20Gyrokinetic%20Plasma%20Turbulence%20Simulations&entry.906535625=Fabian%20Paischer%20and%20Gianluca%20Galletti%20and%20William%20Hornsby%20and%20Paul%20Setinek%20and%20Lorenzo%20Zanisi%20and%20Naomi%20Carey%20and%20Stanislas%20Pamela%20and%20Johannes%20Brandstetter&entry.1292438233=%20%20Nuclear%20fusion%20plays%20a%20pivotal%20role%20in%20the%20quest%20for%20reliable%20and%20sustainable%0Aenergy%20production.%20A%20major%20roadblock%20to%20viable%20fusion%20power%20is%20understanding%0Aplasma%20turbulence%2C%20which%20significantly%20impairs%20plasma%20confinement%2C%20and%20is%20vital%0Afor%20next-generation%20reactor%20design.%20Plasma%20turbulence%20is%20governed%20by%20the%0Anonlinear%20gyrokinetic%20equation%2C%20which%20evolves%20a%205D%20distribution%20function%20over%0Atime.%20Due%20to%20its%20high%20computational%20cost%2C%20reduced-order%20models%20are%20often%0Aemployed%20in%20practice%20to%20approximate%20turbulent%20transport%20of%20energy.%20However%2C%0Athey%20omit%20nonlinear%20effects%20unique%20to%20the%20full%205D%20dynamics.%20To%20tackle%20this%2C%20we%0Aintroduce%20GyroSwin%2C%20the%20first%20scalable%205D%20neural%20surrogate%20that%20can%20model%205D%0Anonlinear%20gyrokinetic%20simulations%2C%20thereby%20capturing%20the%20physical%20phenomena%0Aneglected%20by%20reduced%20models%2C%20while%20providing%20accurate%20estimates%20of%20turbulent%0Aheat%20transport.GyroSwin%20%28i%29%20extends%20hierarchical%20Vision%20Transformers%20to%205D%2C%0A%28ii%29%20introduces%20cross-attention%20and%20integration%20modules%20for%20latent%0A3D%24%5Cleftrightarrow%245D%20interactions%20between%20electrostatic%20potential%20fields%20and%0Athe%20distribution%20function%2C%20and%20%28iii%29%20performs%20channelwise%20mode%20separation%0Ainspired%20by%20nonlinear%20physics.%20We%20demonstrate%20that%20GyroSwin%20outperforms%20widely%0Aused%20reduced%20numerics%20on%20heat%20flux%20prediction%2C%20captures%20the%20turbulent%20energy%0Acascade%2C%20and%20reduces%20the%20cost%20of%20fully%20resolved%20nonlinear%20gyrokinetics%20by%20three%0Aorders%20of%20magnitude%20while%20remaining%20physically%20verifiable.%20GyroSwin%20shows%0Apromising%20scaling%20laws%2C%20tested%20up%20to%20one%20billion%20parameters%2C%20paving%20the%20way%20for%0Ascalable%20neural%20surrogates%20for%20gyrokinetic%20simulations%20of%20plasma%20turbulence.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07314v1&entry.124074799=Read"},
{"title": "h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement\n  Learning", "author": "Sumeet Ramesh Motwani and Alesia Ivanova and Ziyang Cai and Philip Torr and Riashat Islam and Shital Shah and Christian Schroeder de Witt and Charles London", "abstract": "  Large language models excel at short-horizon reasoning tasks, but performance\ndrops as reasoning horizon lengths increase. Existing approaches to combat this\nrely on inference-time scaffolding or costly step-level supervision, neither of\nwhich scales easily. In this work, we introduce a scalable method to bootstrap\nlong-horizon reasoning capabilities using only existing, abundant short-horizon\ndata. Our approach synthetically composes simple problems into complex,\nmulti-step dependency chains of arbitrary length. We train models on this data\nusing outcome-only rewards under a curriculum that automatically increases in\ncomplexity, allowing RL training to be scaled much further without saturating.\nEmpirically, our method generalizes remarkably well: curriculum training on\ncomposed 6th-grade level math problems (GSM8K) boosts accuracy on longer,\ncompetition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x.\nImportantly, our long-horizon improvements are significantly higher than\nbaselines even at high pass@k, showing that models can learn new reasoning\npaths under RL. Theoretically, we show that curriculum RL with outcome rewards\nachieves an exponential improvement in sample complexity over full-horizon\ntraining, providing training signal comparable to dense supervision. h1\ntherefore introduces an efficient path towards scaling RL for long-horizon\nproblems using only existing data.\n", "link": "http://arxiv.org/abs/2510.07312v1", "date": "2025-10-08", "relevancy": 2.0223, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5083}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.505}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20h1%3A%20Bootstrapping%20LLMs%20to%20Reason%20over%20Longer%20Horizons%20via%20Reinforcement%0A%20%20Learning&body=Title%3A%20h1%3A%20Bootstrapping%20LLMs%20to%20Reason%20over%20Longer%20Horizons%20via%20Reinforcement%0A%20%20Learning%0AAuthor%3A%20Sumeet%20Ramesh%20Motwani%20and%20Alesia%20Ivanova%20and%20Ziyang%20Cai%20and%20Philip%20Torr%20and%20Riashat%20Islam%20and%20Shital%20Shah%20and%20Christian%20Schroeder%20de%20Witt%20and%20Charles%20London%0AAbstract%3A%20%20%20Large%20language%20models%20excel%20at%20short-horizon%20reasoning%20tasks%2C%20but%20performance%0Adrops%20as%20reasoning%20horizon%20lengths%20increase.%20Existing%20approaches%20to%20combat%20this%0Arely%20on%20inference-time%20scaffolding%20or%20costly%20step-level%20supervision%2C%20neither%20of%0Awhich%20scales%20easily.%20In%20this%20work%2C%20we%20introduce%20a%20scalable%20method%20to%20bootstrap%0Along-horizon%20reasoning%20capabilities%20using%20only%20existing%2C%20abundant%20short-horizon%0Adata.%20Our%20approach%20synthetically%20composes%20simple%20problems%20into%20complex%2C%0Amulti-step%20dependency%20chains%20of%20arbitrary%20length.%20We%20train%20models%20on%20this%20data%0Ausing%20outcome-only%20rewards%20under%20a%20curriculum%20that%20automatically%20increases%20in%0Acomplexity%2C%20allowing%20RL%20training%20to%20be%20scaled%20much%20further%20without%20saturating.%0AEmpirically%2C%20our%20method%20generalizes%20remarkably%20well%3A%20curriculum%20training%20on%0Acomposed%206th-grade%20level%20math%20problems%20%28GSM8K%29%20boosts%20accuracy%20on%20longer%2C%0Acompetition-level%20benchmarks%20%28GSM-Symbolic%2C%20MATH-500%2C%20AIME%29%20by%20up%20to%202.06x.%0AImportantly%2C%20our%20long-horizon%20improvements%20are%20significantly%20higher%20than%0Abaselines%20even%20at%20high%20pass%40k%2C%20showing%20that%20models%20can%20learn%20new%20reasoning%0Apaths%20under%20RL.%20Theoretically%2C%20we%20show%20that%20curriculum%20RL%20with%20outcome%20rewards%0Aachieves%20an%20exponential%20improvement%20in%20sample%20complexity%20over%20full-horizon%0Atraining%2C%20providing%20training%20signal%20comparable%20to%20dense%20supervision.%20h1%0Atherefore%20introduces%20an%20efficient%20path%20towards%20scaling%20RL%20for%20long-horizon%0Aproblems%20using%20only%20existing%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07312v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dh1%253A%2520Bootstrapping%2520LLMs%2520to%2520Reason%2520over%2520Longer%2520Horizons%2520via%2520Reinforcement%250A%2520%2520Learning%26entry.906535625%3DSumeet%2520Ramesh%2520Motwani%2520and%2520Alesia%2520Ivanova%2520and%2520Ziyang%2520Cai%2520and%2520Philip%2520Torr%2520and%2520Riashat%2520Islam%2520and%2520Shital%2520Shah%2520and%2520Christian%2520Schroeder%2520de%2520Witt%2520and%2520Charles%2520London%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520excel%2520at%2520short-horizon%2520reasoning%2520tasks%252C%2520but%2520performance%250Adrops%2520as%2520reasoning%2520horizon%2520lengths%2520increase.%2520Existing%2520approaches%2520to%2520combat%2520this%250Arely%2520on%2520inference-time%2520scaffolding%2520or%2520costly%2520step-level%2520supervision%252C%2520neither%2520of%250Awhich%2520scales%2520easily.%2520In%2520this%2520work%252C%2520we%2520introduce%2520a%2520scalable%2520method%2520to%2520bootstrap%250Along-horizon%2520reasoning%2520capabilities%2520using%2520only%2520existing%252C%2520abundant%2520short-horizon%250Adata.%2520Our%2520approach%2520synthetically%2520composes%2520simple%2520problems%2520into%2520complex%252C%250Amulti-step%2520dependency%2520chains%2520of%2520arbitrary%2520length.%2520We%2520train%2520models%2520on%2520this%2520data%250Ausing%2520outcome-only%2520rewards%2520under%2520a%2520curriculum%2520that%2520automatically%2520increases%2520in%250Acomplexity%252C%2520allowing%2520RL%2520training%2520to%2520be%2520scaled%2520much%2520further%2520without%2520saturating.%250AEmpirically%252C%2520our%2520method%2520generalizes%2520remarkably%2520well%253A%2520curriculum%2520training%2520on%250Acomposed%25206th-grade%2520level%2520math%2520problems%2520%2528GSM8K%2529%2520boosts%2520accuracy%2520on%2520longer%252C%250Acompetition-level%2520benchmarks%2520%2528GSM-Symbolic%252C%2520MATH-500%252C%2520AIME%2529%2520by%2520up%2520to%25202.06x.%250AImportantly%252C%2520our%2520long-horizon%2520improvements%2520are%2520significantly%2520higher%2520than%250Abaselines%2520even%2520at%2520high%2520pass%2540k%252C%2520showing%2520that%2520models%2520can%2520learn%2520new%2520reasoning%250Apaths%2520under%2520RL.%2520Theoretically%252C%2520we%2520show%2520that%2520curriculum%2520RL%2520with%2520outcome%2520rewards%250Aachieves%2520an%2520exponential%2520improvement%2520in%2520sample%2520complexity%2520over%2520full-horizon%250Atraining%252C%2520providing%2520training%2520signal%2520comparable%2520to%2520dense%2520supervision.%2520h1%250Atherefore%2520introduces%2520an%2520efficient%2520path%2520towards%2520scaling%2520RL%2520for%2520long-horizon%250Aproblems%2520using%2520only%2520existing%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07312v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=h1%3A%20Bootstrapping%20LLMs%20to%20Reason%20over%20Longer%20Horizons%20via%20Reinforcement%0A%20%20Learning&entry.906535625=Sumeet%20Ramesh%20Motwani%20and%20Alesia%20Ivanova%20and%20Ziyang%20Cai%20and%20Philip%20Torr%20and%20Riashat%20Islam%20and%20Shital%20Shah%20and%20Christian%20Schroeder%20de%20Witt%20and%20Charles%20London&entry.1292438233=%20%20Large%20language%20models%20excel%20at%20short-horizon%20reasoning%20tasks%2C%20but%20performance%0Adrops%20as%20reasoning%20horizon%20lengths%20increase.%20Existing%20approaches%20to%20combat%20this%0Arely%20on%20inference-time%20scaffolding%20or%20costly%20step-level%20supervision%2C%20neither%20of%0Awhich%20scales%20easily.%20In%20this%20work%2C%20we%20introduce%20a%20scalable%20method%20to%20bootstrap%0Along-horizon%20reasoning%20capabilities%20using%20only%20existing%2C%20abundant%20short-horizon%0Adata.%20Our%20approach%20synthetically%20composes%20simple%20problems%20into%20complex%2C%0Amulti-step%20dependency%20chains%20of%20arbitrary%20length.%20We%20train%20models%20on%20this%20data%0Ausing%20outcome-only%20rewards%20under%20a%20curriculum%20that%20automatically%20increases%20in%0Acomplexity%2C%20allowing%20RL%20training%20to%20be%20scaled%20much%20further%20without%20saturating.%0AEmpirically%2C%20our%20method%20generalizes%20remarkably%20well%3A%20curriculum%20training%20on%0Acomposed%206th-grade%20level%20math%20problems%20%28GSM8K%29%20boosts%20accuracy%20on%20longer%2C%0Acompetition-level%20benchmarks%20%28GSM-Symbolic%2C%20MATH-500%2C%20AIME%29%20by%20up%20to%202.06x.%0AImportantly%2C%20our%20long-horizon%20improvements%20are%20significantly%20higher%20than%0Abaselines%20even%20at%20high%20pass%40k%2C%20showing%20that%20models%20can%20learn%20new%20reasoning%0Apaths%20under%20RL.%20Theoretically%2C%20we%20show%20that%20curriculum%20RL%20with%20outcome%20rewards%0Aachieves%20an%20exponential%20improvement%20in%20sample%20complexity%20over%20full-horizon%0Atraining%2C%20providing%20training%20signal%20comparable%20to%20dense%20supervision.%20h1%0Atherefore%20introduces%20an%20efficient%20path%20towards%20scaling%20RL%20for%20long-horizon%0Aproblems%20using%20only%20existing%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07312v1&entry.124074799=Read"},
{"title": "Test-Time Graph Search for Goal-Conditioned Reinforcement Learning", "author": "Evgenii Opryshko and Junwei Quan and Claas Voelcker and Yilun Du and Igor Gilitschenski", "abstract": "  Offline goal-conditioned reinforcement learning (GCRL) trains policies that\nreach user-specified goals at test time, providing a simple, unsupervised,\ndomain-agnostic way to extract diverse behaviors from unlabeled, reward-free\ndatasets. Nonetheless, long-horizon decision making remains difficult for GCRL\nagents due to temporal credit assignment and error accumulation, and the\noffline setting amplifies these effects. To alleviate this issue, we introduce\nTest-Time Graph Search (TTGS), a lightweight planning approach to solve the\nGCRL task. TTGS accepts any state-space distance or cost signal, builds a\nweighted graph over dataset states, and performs fast search to assemble a\nsequence of subgoals that a frozen policy executes. When the base learner is\nvalue-based, the distance is derived directly from the learned goal-conditioned\nvalue function, so no handcrafted metric is needed. TTGS requires no changes to\ntraining, no additional supervision, no online interaction, and no privileged\ninformation, and it runs entirely at inference. On the OGBench benchmark, TTGS\nimproves success rates of multiple base learners on challenging locomotion\ntasks, demonstrating the benefit of simple metric-guided test-time planning for\noffline GCRL.\n", "link": "http://arxiv.org/abs/2510.07257v1", "date": "2025-10-08", "relevancy": 1.9767, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5102}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4983}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Test-Time%20Graph%20Search%20for%20Goal-Conditioned%20Reinforcement%20Learning&body=Title%3A%20Test-Time%20Graph%20Search%20for%20Goal-Conditioned%20Reinforcement%20Learning%0AAuthor%3A%20Evgenii%20Opryshko%20and%20Junwei%20Quan%20and%20Claas%20Voelcker%20and%20Yilun%20Du%20and%20Igor%20Gilitschenski%0AAbstract%3A%20%20%20Offline%20goal-conditioned%20reinforcement%20learning%20%28GCRL%29%20trains%20policies%20that%0Areach%20user-specified%20goals%20at%20test%20time%2C%20providing%20a%20simple%2C%20unsupervised%2C%0Adomain-agnostic%20way%20to%20extract%20diverse%20behaviors%20from%20unlabeled%2C%20reward-free%0Adatasets.%20Nonetheless%2C%20long-horizon%20decision%20making%20remains%20difficult%20for%20GCRL%0Aagents%20due%20to%20temporal%20credit%20assignment%20and%20error%20accumulation%2C%20and%20the%0Aoffline%20setting%20amplifies%20these%20effects.%20To%20alleviate%20this%20issue%2C%20we%20introduce%0ATest-Time%20Graph%20Search%20%28TTGS%29%2C%20a%20lightweight%20planning%20approach%20to%20solve%20the%0AGCRL%20task.%20TTGS%20accepts%20any%20state-space%20distance%20or%20cost%20signal%2C%20builds%20a%0Aweighted%20graph%20over%20dataset%20states%2C%20and%20performs%20fast%20search%20to%20assemble%20a%0Asequence%20of%20subgoals%20that%20a%20frozen%20policy%20executes.%20When%20the%20base%20learner%20is%0Avalue-based%2C%20the%20distance%20is%20derived%20directly%20from%20the%20learned%20goal-conditioned%0Avalue%20function%2C%20so%20no%20handcrafted%20metric%20is%20needed.%20TTGS%20requires%20no%20changes%20to%0Atraining%2C%20no%20additional%20supervision%2C%20no%20online%20interaction%2C%20and%20no%20privileged%0Ainformation%2C%20and%20it%20runs%20entirely%20at%20inference.%20On%20the%20OGBench%20benchmark%2C%20TTGS%0Aimproves%20success%20rates%20of%20multiple%20base%20learners%20on%20challenging%20locomotion%0Atasks%2C%20demonstrating%20the%20benefit%20of%20simple%20metric-guided%20test-time%20planning%20for%0Aoffline%20GCRL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07257v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTest-Time%2520Graph%2520Search%2520for%2520Goal-Conditioned%2520Reinforcement%2520Learning%26entry.906535625%3DEvgenii%2520Opryshko%2520and%2520Junwei%2520Quan%2520and%2520Claas%2520Voelcker%2520and%2520Yilun%2520Du%2520and%2520Igor%2520Gilitschenski%26entry.1292438233%3D%2520%2520Offline%2520goal-conditioned%2520reinforcement%2520learning%2520%2528GCRL%2529%2520trains%2520policies%2520that%250Areach%2520user-specified%2520goals%2520at%2520test%2520time%252C%2520providing%2520a%2520simple%252C%2520unsupervised%252C%250Adomain-agnostic%2520way%2520to%2520extract%2520diverse%2520behaviors%2520from%2520unlabeled%252C%2520reward-free%250Adatasets.%2520Nonetheless%252C%2520long-horizon%2520decision%2520making%2520remains%2520difficult%2520for%2520GCRL%250Aagents%2520due%2520to%2520temporal%2520credit%2520assignment%2520and%2520error%2520accumulation%252C%2520and%2520the%250Aoffline%2520setting%2520amplifies%2520these%2520effects.%2520To%2520alleviate%2520this%2520issue%252C%2520we%2520introduce%250ATest-Time%2520Graph%2520Search%2520%2528TTGS%2529%252C%2520a%2520lightweight%2520planning%2520approach%2520to%2520solve%2520the%250AGCRL%2520task.%2520TTGS%2520accepts%2520any%2520state-space%2520distance%2520or%2520cost%2520signal%252C%2520builds%2520a%250Aweighted%2520graph%2520over%2520dataset%2520states%252C%2520and%2520performs%2520fast%2520search%2520to%2520assemble%2520a%250Asequence%2520of%2520subgoals%2520that%2520a%2520frozen%2520policy%2520executes.%2520When%2520the%2520base%2520learner%2520is%250Avalue-based%252C%2520the%2520distance%2520is%2520derived%2520directly%2520from%2520the%2520learned%2520goal-conditioned%250Avalue%2520function%252C%2520so%2520no%2520handcrafted%2520metric%2520is%2520needed.%2520TTGS%2520requires%2520no%2520changes%2520to%250Atraining%252C%2520no%2520additional%2520supervision%252C%2520no%2520online%2520interaction%252C%2520and%2520no%2520privileged%250Ainformation%252C%2520and%2520it%2520runs%2520entirely%2520at%2520inference.%2520On%2520the%2520OGBench%2520benchmark%252C%2520TTGS%250Aimproves%2520success%2520rates%2520of%2520multiple%2520base%2520learners%2520on%2520challenging%2520locomotion%250Atasks%252C%2520demonstrating%2520the%2520benefit%2520of%2520simple%2520metric-guided%2520test-time%2520planning%2520for%250Aoffline%2520GCRL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07257v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Test-Time%20Graph%20Search%20for%20Goal-Conditioned%20Reinforcement%20Learning&entry.906535625=Evgenii%20Opryshko%20and%20Junwei%20Quan%20and%20Claas%20Voelcker%20and%20Yilun%20Du%20and%20Igor%20Gilitschenski&entry.1292438233=%20%20Offline%20goal-conditioned%20reinforcement%20learning%20%28GCRL%29%20trains%20policies%20that%0Areach%20user-specified%20goals%20at%20test%20time%2C%20providing%20a%20simple%2C%20unsupervised%2C%0Adomain-agnostic%20way%20to%20extract%20diverse%20behaviors%20from%20unlabeled%2C%20reward-free%0Adatasets.%20Nonetheless%2C%20long-horizon%20decision%20making%20remains%20difficult%20for%20GCRL%0Aagents%20due%20to%20temporal%20credit%20assignment%20and%20error%20accumulation%2C%20and%20the%0Aoffline%20setting%20amplifies%20these%20effects.%20To%20alleviate%20this%20issue%2C%20we%20introduce%0ATest-Time%20Graph%20Search%20%28TTGS%29%2C%20a%20lightweight%20planning%20approach%20to%20solve%20the%0AGCRL%20task.%20TTGS%20accepts%20any%20state-space%20distance%20or%20cost%20signal%2C%20builds%20a%0Aweighted%20graph%20over%20dataset%20states%2C%20and%20performs%20fast%20search%20to%20assemble%20a%0Asequence%20of%20subgoals%20that%20a%20frozen%20policy%20executes.%20When%20the%20base%20learner%20is%0Avalue-based%2C%20the%20distance%20is%20derived%20directly%20from%20the%20learned%20goal-conditioned%0Avalue%20function%2C%20so%20no%20handcrafted%20metric%20is%20needed.%20TTGS%20requires%20no%20changes%20to%0Atraining%2C%20no%20additional%20supervision%2C%20no%20online%20interaction%2C%20and%20no%20privileged%0Ainformation%2C%20and%20it%20runs%20entirely%20at%20inference.%20On%20the%20OGBench%20benchmark%2C%20TTGS%0Aimproves%20success%20rates%20of%20multiple%20base%20learners%20on%20challenging%20locomotion%0Atasks%2C%20demonstrating%20the%20benefit%20of%20simple%20metric-guided%20test-time%20planning%20for%0Aoffline%20GCRL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07257v1&entry.124074799=Read"},
{"title": "NdLinear: Preserving Multi-Dimensional Structure for Parameter-Efficient\n  Neural Networks", "author": "Alex Reneau and Jerry Yao-Chieh Hu and Zhongfang Zhuang and Ting-Chun Liu and Xiang He and Judah Goldfeder and Nadav Timor and Allen G Roush and Ravid Shwartz-Ziv", "abstract": "  In deep learning, processing multidimensional inputs (e.g., images, medical\nscans, and time series) is an important task that often requires flattening the\ninputs. We introduce $\\mathit{NdLinear}$, a drop-in replacement for linear\nlayers that operates directly on tensors, requiring no flattening. By applying\ntransformations separately along each dimension, NdLinear preserves native data\nstructure while achieving dramatic parameter reductions, often by orders of\nmagnitude, with minimal memory overhead. We prove NdLinear maintains\nexpressivity through structured Tucker decomposition while preserving\nVC-dimension scaling. Extensive experiments demonstrate NdLinear's capacity to\nachieve significant parameter reductions with substantial wall-clock efficiency\ngains and minimal memory overhead. For instance, our $\\mathit{NdLinear-LoRA}$\nmatches or exceeds standard LoRA on language reasoning tasks using up to\n$9\\times$ fewer parameters. Experiments across CNNs, RNNs, Transformers, and\nMLPs on vision, language, time-series, and tabular tasks consistently\ndemonstrate NdLinear's efficiency gains. While excelling at axis-separable\ntasks, NdLinear has limitations with entangled spatial interactions. By\nprocessing data in its original N-dimensional form, NdLinear provides a\ntheoretically grounded, practical component for building more efficient neural\narchitectures.\n", "link": "http://arxiv.org/abs/2503.17353v3", "date": "2025-10-08", "relevancy": 1.9737, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5048}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4859}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4836}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NdLinear%3A%20Preserving%20Multi-Dimensional%20Structure%20for%20Parameter-Efficient%0A%20%20Neural%20Networks&body=Title%3A%20NdLinear%3A%20Preserving%20Multi-Dimensional%20Structure%20for%20Parameter-Efficient%0A%20%20Neural%20Networks%0AAuthor%3A%20Alex%20Reneau%20and%20Jerry%20Yao-Chieh%20Hu%20and%20Zhongfang%20Zhuang%20and%20Ting-Chun%20Liu%20and%20Xiang%20He%20and%20Judah%20Goldfeder%20and%20Nadav%20Timor%20and%20Allen%20G%20Roush%20and%20Ravid%20Shwartz-Ziv%0AAbstract%3A%20%20%20In%20deep%20learning%2C%20processing%20multidimensional%20inputs%20%28e.g.%2C%20images%2C%20medical%0Ascans%2C%20and%20time%20series%29%20is%20an%20important%20task%20that%20often%20requires%20flattening%20the%0Ainputs.%20We%20introduce%20%24%5Cmathit%7BNdLinear%7D%24%2C%20a%20drop-in%20replacement%20for%20linear%0Alayers%20that%20operates%20directly%20on%20tensors%2C%20requiring%20no%20flattening.%20By%20applying%0Atransformations%20separately%20along%20each%20dimension%2C%20NdLinear%20preserves%20native%20data%0Astructure%20while%20achieving%20dramatic%20parameter%20reductions%2C%20often%20by%20orders%20of%0Amagnitude%2C%20with%20minimal%20memory%20overhead.%20We%20prove%20NdLinear%20maintains%0Aexpressivity%20through%20structured%20Tucker%20decomposition%20while%20preserving%0AVC-dimension%20scaling.%20Extensive%20experiments%20demonstrate%20NdLinear%27s%20capacity%20to%0Aachieve%20significant%20parameter%20reductions%20with%20substantial%20wall-clock%20efficiency%0Agains%20and%20minimal%20memory%20overhead.%20For%20instance%2C%20our%20%24%5Cmathit%7BNdLinear-LoRA%7D%24%0Amatches%20or%20exceeds%20standard%20LoRA%20on%20language%20reasoning%20tasks%20using%20up%20to%0A%249%5Ctimes%24%20fewer%20parameters.%20Experiments%20across%20CNNs%2C%20RNNs%2C%20Transformers%2C%20and%0AMLPs%20on%20vision%2C%20language%2C%20time-series%2C%20and%20tabular%20tasks%20consistently%0Ademonstrate%20NdLinear%27s%20efficiency%20gains.%20While%20excelling%20at%20axis-separable%0Atasks%2C%20NdLinear%20has%20limitations%20with%20entangled%20spatial%20interactions.%20By%0Aprocessing%20data%20in%20its%20original%20N-dimensional%20form%2C%20NdLinear%20provides%20a%0Atheoretically%20grounded%2C%20practical%20component%20for%20building%20more%20efficient%20neural%0Aarchitectures.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2503.17353v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNdLinear%253A%2520Preserving%2520Multi-Dimensional%2520Structure%2520for%2520Parameter-Efficient%250A%2520%2520Neural%2520Networks%26entry.906535625%3DAlex%2520Reneau%2520and%2520Jerry%2520Yao-Chieh%2520Hu%2520and%2520Zhongfang%2520Zhuang%2520and%2520Ting-Chun%2520Liu%2520and%2520Xiang%2520He%2520and%2520Judah%2520Goldfeder%2520and%2520Nadav%2520Timor%2520and%2520Allen%2520G%2520Roush%2520and%2520Ravid%2520Shwartz-Ziv%26entry.1292438233%3D%2520%2520In%2520deep%2520learning%252C%2520processing%2520multidimensional%2520inputs%2520%2528e.g.%252C%2520images%252C%2520medical%250Ascans%252C%2520and%2520time%2520series%2529%2520is%2520an%2520important%2520task%2520that%2520often%2520requires%2520flattening%2520the%250Ainputs.%2520We%2520introduce%2520%2524%255Cmathit%257BNdLinear%257D%2524%252C%2520a%2520drop-in%2520replacement%2520for%2520linear%250Alayers%2520that%2520operates%2520directly%2520on%2520tensors%252C%2520requiring%2520no%2520flattening.%2520By%2520applying%250Atransformations%2520separately%2520along%2520each%2520dimension%252C%2520NdLinear%2520preserves%2520native%2520data%250Astructure%2520while%2520achieving%2520dramatic%2520parameter%2520reductions%252C%2520often%2520by%2520orders%2520of%250Amagnitude%252C%2520with%2520minimal%2520memory%2520overhead.%2520We%2520prove%2520NdLinear%2520maintains%250Aexpressivity%2520through%2520structured%2520Tucker%2520decomposition%2520while%2520preserving%250AVC-dimension%2520scaling.%2520Extensive%2520experiments%2520demonstrate%2520NdLinear%2527s%2520capacity%2520to%250Aachieve%2520significant%2520parameter%2520reductions%2520with%2520substantial%2520wall-clock%2520efficiency%250Agains%2520and%2520minimal%2520memory%2520overhead.%2520For%2520instance%252C%2520our%2520%2524%255Cmathit%257BNdLinear-LoRA%257D%2524%250Amatches%2520or%2520exceeds%2520standard%2520LoRA%2520on%2520language%2520reasoning%2520tasks%2520using%2520up%2520to%250A%25249%255Ctimes%2524%2520fewer%2520parameters.%2520Experiments%2520across%2520CNNs%252C%2520RNNs%252C%2520Transformers%252C%2520and%250AMLPs%2520on%2520vision%252C%2520language%252C%2520time-series%252C%2520and%2520tabular%2520tasks%2520consistently%250Ademonstrate%2520NdLinear%2527s%2520efficiency%2520gains.%2520While%2520excelling%2520at%2520axis-separable%250Atasks%252C%2520NdLinear%2520has%2520limitations%2520with%2520entangled%2520spatial%2520interactions.%2520By%250Aprocessing%2520data%2520in%2520its%2520original%2520N-dimensional%2520form%252C%2520NdLinear%2520provides%2520a%250Atheoretically%2520grounded%252C%2520practical%2520component%2520for%2520building%2520more%2520efficient%2520neural%250Aarchitectures.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17353v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NdLinear%3A%20Preserving%20Multi-Dimensional%20Structure%20for%20Parameter-Efficient%0A%20%20Neural%20Networks&entry.906535625=Alex%20Reneau%20and%20Jerry%20Yao-Chieh%20Hu%20and%20Zhongfang%20Zhuang%20and%20Ting-Chun%20Liu%20and%20Xiang%20He%20and%20Judah%20Goldfeder%20and%20Nadav%20Timor%20and%20Allen%20G%20Roush%20and%20Ravid%20Shwartz-Ziv&entry.1292438233=%20%20In%20deep%20learning%2C%20processing%20multidimensional%20inputs%20%28e.g.%2C%20images%2C%20medical%0Ascans%2C%20and%20time%20series%29%20is%20an%20important%20task%20that%20often%20requires%20flattening%20the%0Ainputs.%20We%20introduce%20%24%5Cmathit%7BNdLinear%7D%24%2C%20a%20drop-in%20replacement%20for%20linear%0Alayers%20that%20operates%20directly%20on%20tensors%2C%20requiring%20no%20flattening.%20By%20applying%0Atransformations%20separately%20along%20each%20dimension%2C%20NdLinear%20preserves%20native%20data%0Astructure%20while%20achieving%20dramatic%20parameter%20reductions%2C%20often%20by%20orders%20of%0Amagnitude%2C%20with%20minimal%20memory%20overhead.%20We%20prove%20NdLinear%20maintains%0Aexpressivity%20through%20structured%20Tucker%20decomposition%20while%20preserving%0AVC-dimension%20scaling.%20Extensive%20experiments%20demonstrate%20NdLinear%27s%20capacity%20to%0Aachieve%20significant%20parameter%20reductions%20with%20substantial%20wall-clock%20efficiency%0Agains%20and%20minimal%20memory%20overhead.%20For%20instance%2C%20our%20%24%5Cmathit%7BNdLinear-LoRA%7D%24%0Amatches%20or%20exceeds%20standard%20LoRA%20on%20language%20reasoning%20tasks%20using%20up%20to%0A%249%5Ctimes%24%20fewer%20parameters.%20Experiments%20across%20CNNs%2C%20RNNs%2C%20Transformers%2C%20and%0AMLPs%20on%20vision%2C%20language%2C%20time-series%2C%20and%20tabular%20tasks%20consistently%0Ademonstrate%20NdLinear%27s%20efficiency%20gains.%20While%20excelling%20at%20axis-separable%0Atasks%2C%20NdLinear%20has%20limitations%20with%20entangled%20spatial%20interactions.%20By%0Aprocessing%20data%20in%20its%20original%20N-dimensional%20form%2C%20NdLinear%20provides%20a%0Atheoretically%20grounded%2C%20practical%20component%20for%20building%20more%20efficient%20neural%0Aarchitectures.%0A&entry.1838667208=http%3A//arxiv.org/abs/2503.17353v3&entry.124074799=Read"},
{"title": "LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM\n  Evaluation", "author": "Joseph Enguehard and Morgane Van Ermengem and Kate Atkinson and Sujeong Cha and Arijit Ghosh Chowdhury and Prashanth Kallur Ramaswamy and Jeremy Roghair and Hannah R Marlowe and Carina Suzana Negreanu and Kitty Boxall and Diana Mincu", "abstract": "  Evaluating large language model (LLM) outputs in the legal domain presents\nunique challenges due to the complex and nuanced nature of legal analysis.\nCurrent evaluation approaches either depend on reference data, which is costly\nto produce, or use standardized assessment methods, both of which have\nsignificant limitations for legal applications.\n  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its\nreliability and effectiveness in legal contexts depend heavily on evaluation\nprocesses unique to the legal industry and how trustworthy the evaluation\nappears to the human legal expert. This is where existing evaluation methods\ncurrently fail and exhibit considerable variability.\n  This paper aims to close the gap: a) we break down lengthy responses into\n'Legal Data Points' (LDPs), self-contained units of information, and introduce\na novel, reference-free evaluation methodology that reflects how lawyers\nevaluate legal answers; b) we demonstrate that our method outperforms a variety\nof baselines on both our proprietary dataset and an open-source dataset\n(LegalBench); c) we show how our method correlates more closely with human\nexpert evaluations and helps improve inter-annotator agreement; and finally d)\nwe open source our Legal Data Points for a subset of LegalBench used in our\nexperiments, allowing the research community to replicate our results and\nadvance research in this vital area of LLM evaluation on legal\nquestion-answering.\n", "link": "http://arxiv.org/abs/2510.07243v1", "date": "2025-10-08", "relevancy": 1.956, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4898}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LeMAJ%20%28Legal%20LLM-as-a-Judge%29%3A%20Bridging%20Legal%20Reasoning%20and%20LLM%0A%20%20Evaluation&body=Title%3A%20LeMAJ%20%28Legal%20LLM-as-a-Judge%29%3A%20Bridging%20Legal%20Reasoning%20and%20LLM%0A%20%20Evaluation%0AAuthor%3A%20Joseph%20Enguehard%20and%20Morgane%20Van%20Ermengem%20and%20Kate%20Atkinson%20and%20Sujeong%20Cha%20and%20Arijit%20Ghosh%20Chowdhury%20and%20Prashanth%20Kallur%20Ramaswamy%20and%20Jeremy%20Roghair%20and%20Hannah%20R%20Marlowe%20and%20Carina%20Suzana%20Negreanu%20and%20Kitty%20Boxall%20and%20Diana%20Mincu%0AAbstract%3A%20%20%20Evaluating%20large%20language%20model%20%28LLM%29%20outputs%20in%20the%20legal%20domain%20presents%0Aunique%20challenges%20due%20to%20the%20complex%20and%20nuanced%20nature%20of%20legal%20analysis.%0ACurrent%20evaluation%20approaches%20either%20depend%20on%20reference%20data%2C%20which%20is%20costly%0Ato%20produce%2C%20or%20use%20standardized%20assessment%20methods%2C%20both%20of%20which%20have%0Asignificant%20limitations%20for%20legal%20applications.%0A%20%20Although%20LLM-as-a-Judge%20has%20emerged%20as%20a%20promising%20evaluation%20technique%2C%20its%0Areliability%20and%20effectiveness%20in%20legal%20contexts%20depend%20heavily%20on%20evaluation%0Aprocesses%20unique%20to%20the%20legal%20industry%20and%20how%20trustworthy%20the%20evaluation%0Aappears%20to%20the%20human%20legal%20expert.%20This%20is%20where%20existing%20evaluation%20methods%0Acurrently%20fail%20and%20exhibit%20considerable%20variability.%0A%20%20This%20paper%20aims%20to%20close%20the%20gap%3A%20a%29%20we%20break%20down%20lengthy%20responses%20into%0A%27Legal%20Data%20Points%27%20%28LDPs%29%2C%20self-contained%20units%20of%20information%2C%20and%20introduce%0Aa%20novel%2C%20reference-free%20evaluation%20methodology%20that%20reflects%20how%20lawyers%0Aevaluate%20legal%20answers%3B%20b%29%20we%20demonstrate%20that%20our%20method%20outperforms%20a%20variety%0Aof%20baselines%20on%20both%20our%20proprietary%20dataset%20and%20an%20open-source%20dataset%0A%28LegalBench%29%3B%20c%29%20we%20show%20how%20our%20method%20correlates%20more%20closely%20with%20human%0Aexpert%20evaluations%20and%20helps%20improve%20inter-annotator%20agreement%3B%20and%20finally%20d%29%0Awe%20open%20source%20our%20Legal%20Data%20Points%20for%20a%20subset%20of%20LegalBench%20used%20in%20our%0Aexperiments%2C%20allowing%20the%20research%20community%20to%20replicate%20our%20results%20and%0Aadvance%20research%20in%20this%20vital%20area%20of%20LLM%20evaluation%20on%20legal%0Aquestion-answering.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLeMAJ%2520%2528Legal%2520LLM-as-a-Judge%2529%253A%2520Bridging%2520Legal%2520Reasoning%2520and%2520LLM%250A%2520%2520Evaluation%26entry.906535625%3DJoseph%2520Enguehard%2520and%2520Morgane%2520Van%2520Ermengem%2520and%2520Kate%2520Atkinson%2520and%2520Sujeong%2520Cha%2520and%2520Arijit%2520Ghosh%2520Chowdhury%2520and%2520Prashanth%2520Kallur%2520Ramaswamy%2520and%2520Jeremy%2520Roghair%2520and%2520Hannah%2520R%2520Marlowe%2520and%2520Carina%2520Suzana%2520Negreanu%2520and%2520Kitty%2520Boxall%2520and%2520Diana%2520Mincu%26entry.1292438233%3D%2520%2520Evaluating%2520large%2520language%2520model%2520%2528LLM%2529%2520outputs%2520in%2520the%2520legal%2520domain%2520presents%250Aunique%2520challenges%2520due%2520to%2520the%2520complex%2520and%2520nuanced%2520nature%2520of%2520legal%2520analysis.%250ACurrent%2520evaluation%2520approaches%2520either%2520depend%2520on%2520reference%2520data%252C%2520which%2520is%2520costly%250Ato%2520produce%252C%2520or%2520use%2520standardized%2520assessment%2520methods%252C%2520both%2520of%2520which%2520have%250Asignificant%2520limitations%2520for%2520legal%2520applications.%250A%2520%2520Although%2520LLM-as-a-Judge%2520has%2520emerged%2520as%2520a%2520promising%2520evaluation%2520technique%252C%2520its%250Areliability%2520and%2520effectiveness%2520in%2520legal%2520contexts%2520depend%2520heavily%2520on%2520evaluation%250Aprocesses%2520unique%2520to%2520the%2520legal%2520industry%2520and%2520how%2520trustworthy%2520the%2520evaluation%250Aappears%2520to%2520the%2520human%2520legal%2520expert.%2520This%2520is%2520where%2520existing%2520evaluation%2520methods%250Acurrently%2520fail%2520and%2520exhibit%2520considerable%2520variability.%250A%2520%2520This%2520paper%2520aims%2520to%2520close%2520the%2520gap%253A%2520a%2529%2520we%2520break%2520down%2520lengthy%2520responses%2520into%250A%2527Legal%2520Data%2520Points%2527%2520%2528LDPs%2529%252C%2520self-contained%2520units%2520of%2520information%252C%2520and%2520introduce%250Aa%2520novel%252C%2520reference-free%2520evaluation%2520methodology%2520that%2520reflects%2520how%2520lawyers%250Aevaluate%2520legal%2520answers%253B%2520b%2529%2520we%2520demonstrate%2520that%2520our%2520method%2520outperforms%2520a%2520variety%250Aof%2520baselines%2520on%2520both%2520our%2520proprietary%2520dataset%2520and%2520an%2520open-source%2520dataset%250A%2528LegalBench%2529%253B%2520c%2529%2520we%2520show%2520how%2520our%2520method%2520correlates%2520more%2520closely%2520with%2520human%250Aexpert%2520evaluations%2520and%2520helps%2520improve%2520inter-annotator%2520agreement%253B%2520and%2520finally%2520d%2529%250Awe%2520open%2520source%2520our%2520Legal%2520Data%2520Points%2520for%2520a%2520subset%2520of%2520LegalBench%2520used%2520in%2520our%250Aexperiments%252C%2520allowing%2520the%2520research%2520community%2520to%2520replicate%2520our%2520results%2520and%250Aadvance%2520research%2520in%2520this%2520vital%2520area%2520of%2520LLM%2520evaluation%2520on%2520legal%250Aquestion-answering.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LeMAJ%20%28Legal%20LLM-as-a-Judge%29%3A%20Bridging%20Legal%20Reasoning%20and%20LLM%0A%20%20Evaluation&entry.906535625=Joseph%20Enguehard%20and%20Morgane%20Van%20Ermengem%20and%20Kate%20Atkinson%20and%20Sujeong%20Cha%20and%20Arijit%20Ghosh%20Chowdhury%20and%20Prashanth%20Kallur%20Ramaswamy%20and%20Jeremy%20Roghair%20and%20Hannah%20R%20Marlowe%20and%20Carina%20Suzana%20Negreanu%20and%20Kitty%20Boxall%20and%20Diana%20Mincu&entry.1292438233=%20%20Evaluating%20large%20language%20model%20%28LLM%29%20outputs%20in%20the%20legal%20domain%20presents%0Aunique%20challenges%20due%20to%20the%20complex%20and%20nuanced%20nature%20of%20legal%20analysis.%0ACurrent%20evaluation%20approaches%20either%20depend%20on%20reference%20data%2C%20which%20is%20costly%0Ato%20produce%2C%20or%20use%20standardized%20assessment%20methods%2C%20both%20of%20which%20have%0Asignificant%20limitations%20for%20legal%20applications.%0A%20%20Although%20LLM-as-a-Judge%20has%20emerged%20as%20a%20promising%20evaluation%20technique%2C%20its%0Areliability%20and%20effectiveness%20in%20legal%20contexts%20depend%20heavily%20on%20evaluation%0Aprocesses%20unique%20to%20the%20legal%20industry%20and%20how%20trustworthy%20the%20evaluation%0Aappears%20to%20the%20human%20legal%20expert.%20This%20is%20where%20existing%20evaluation%20methods%0Acurrently%20fail%20and%20exhibit%20considerable%20variability.%0A%20%20This%20paper%20aims%20to%20close%20the%20gap%3A%20a%29%20we%20break%20down%20lengthy%20responses%20into%0A%27Legal%20Data%20Points%27%20%28LDPs%29%2C%20self-contained%20units%20of%20information%2C%20and%20introduce%0Aa%20novel%2C%20reference-free%20evaluation%20methodology%20that%20reflects%20how%20lawyers%0Aevaluate%20legal%20answers%3B%20b%29%20we%20demonstrate%20that%20our%20method%20outperforms%20a%20variety%0Aof%20baselines%20on%20both%20our%20proprietary%20dataset%20and%20an%20open-source%20dataset%0A%28LegalBench%29%3B%20c%29%20we%20show%20how%20our%20method%20correlates%20more%20closely%20with%20human%0Aexpert%20evaluations%20and%20helps%20improve%20inter-annotator%20agreement%3B%20and%20finally%20d%29%0Awe%20open%20source%20our%20Legal%20Data%20Points%20for%20a%20subset%20of%20LegalBench%20used%20in%20our%0Aexperiments%2C%20allowing%20the%20research%20community%20to%20replicate%20our%20results%20and%0Aadvance%20research%20in%20this%20vital%20area%20of%20LLM%20evaluation%20on%20legal%0Aquestion-answering.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07243v1&entry.124074799=Read"},
{"title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality", "author": "Baochang Ren and Shuofei Qiao and Da Zheng and Huajun Chen and Ningyu Zhang", "abstract": "  Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL.\n", "link": "http://arxiv.org/abs/2506.19807v3", "date": "2025-10-08", "relevancy": 1.9549, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5036}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4892}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4737}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KnowRL%3A%20Exploring%20Knowledgeable%20Reinforcement%20Learning%20for%20Factuality&body=Title%3A%20KnowRL%3A%20Exploring%20Knowledgeable%20Reinforcement%20Learning%20for%20Factuality%0AAuthor%3A%20Baochang%20Ren%20and%20Shuofei%20Qiao%20and%20Da%20Zheng%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%2C%20particularly%20slow-thinking%20models%2C%20often%0Aexhibit%20severe%20hallucination%2C%20outputting%20incorrect%20content%20due%20to%20an%20inability%0Ato%20accurately%20recognize%20knowledge%20boundaries%20during%20reasoning.%20While%0AReinforcement%20Learning%20%28RL%29%20can%20enhance%20complex%20reasoning%20abilities%2C%20its%0Aoutcome-oriented%20reward%20mechanism%20often%20lacks%20factual%20supervision%20over%20the%0Athinking%20process%2C%20further%20exacerbating%20the%20hallucination%20problem.%20To%20address%0Athe%20high%20hallucination%20in%20slow-thinking%20models%2C%20we%20propose%20Knowledge-enhanced%0ARL%2C%20KnowRL.%20KnowRL%20guides%20models%20to%20perform%20fact-based%20slow%20thinking%20by%0Aintegrating%20a%20factuality%20reward%2C%20based%20on%20knowledge%20verification%2C%20into%20the%20RL%0Atraining%20process%2C%20helping%20them%20recognize%20their%20knowledge%20boundaries.%20KnowRL%0Aguides%20models%20to%20perform%20fact-based%20slow%20thinking%20by%20integrating%20a%20factuality%0Areward%2C%20based%20on%20knowledge%20verification%2C%20into%20the%20RL%20training%20process%2C%20helping%0Athem%20recognize%20their%20knowledge%20boundaries.%20This%20targeted%20factual%20input%20during%0ARL%20training%20enables%20the%20model%20to%20learn%20and%20internalize%20fact-based%20reasoning%0Astrategies.%20By%20directly%20rewarding%20adherence%20to%20facts%20within%20the%20reasoning%0Asteps%2C%20KnowRL%20fosters%20a%20more%20reliable%20thinking%20process.%20Experimental%20results%20on%0Athree%20hallucination%20evaluation%20datasets%20and%20two%20reasoning%20evaluation%20datasets%0Ademonstrate%20that%20KnowRL%20effectively%20mitigates%20hallucinations%20in%20slow-thinking%0Amodels%20while%20maintaining%20their%20original%20strong%20reasoning%20capabilities.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/zjunlp/KnowRL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.19807v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowRL%253A%2520Exploring%2520Knowledgeable%2520Reinforcement%2520Learning%2520for%2520Factuality%26entry.906535625%3DBaochang%2520Ren%2520and%2520Shuofei%2520Qiao%2520and%2520Da%2520Zheng%2520and%2520Huajun%2520Chen%2520and%2520Ningyu%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%252C%2520particularly%2520slow-thinking%2520models%252C%2520often%250Aexhibit%2520severe%2520hallucination%252C%2520outputting%2520incorrect%2520content%2520due%2520to%2520an%2520inability%250Ato%2520accurately%2520recognize%2520knowledge%2520boundaries%2520during%2520reasoning.%2520While%250AReinforcement%2520Learning%2520%2528RL%2529%2520can%2520enhance%2520complex%2520reasoning%2520abilities%252C%2520its%250Aoutcome-oriented%2520reward%2520mechanism%2520often%2520lacks%2520factual%2520supervision%2520over%2520the%250Athinking%2520process%252C%2520further%2520exacerbating%2520the%2520hallucination%2520problem.%2520To%2520address%250Athe%2520high%2520hallucination%2520in%2520slow-thinking%2520models%252C%2520we%2520propose%2520Knowledge-enhanced%250ARL%252C%2520KnowRL.%2520KnowRL%2520guides%2520models%2520to%2520perform%2520fact-based%2520slow%2520thinking%2520by%250Aintegrating%2520a%2520factuality%2520reward%252C%2520based%2520on%2520knowledge%2520verification%252C%2520into%2520the%2520RL%250Atraining%2520process%252C%2520helping%2520them%2520recognize%2520their%2520knowledge%2520boundaries.%2520KnowRL%250Aguides%2520models%2520to%2520perform%2520fact-based%2520slow%2520thinking%2520by%2520integrating%2520a%2520factuality%250Areward%252C%2520based%2520on%2520knowledge%2520verification%252C%2520into%2520the%2520RL%2520training%2520process%252C%2520helping%250Athem%2520recognize%2520their%2520knowledge%2520boundaries.%2520This%2520targeted%2520factual%2520input%2520during%250ARL%2520training%2520enables%2520the%2520model%2520to%2520learn%2520and%2520internalize%2520fact-based%2520reasoning%250Astrategies.%2520By%2520directly%2520rewarding%2520adherence%2520to%2520facts%2520within%2520the%2520reasoning%250Asteps%252C%2520KnowRL%2520fosters%2520a%2520more%2520reliable%2520thinking%2520process.%2520Experimental%2520results%2520on%250Athree%2520hallucination%2520evaluation%2520datasets%2520and%2520two%2520reasoning%2520evaluation%2520datasets%250Ademonstrate%2520that%2520KnowRL%2520effectively%2520mitigates%2520hallucinations%2520in%2520slow-thinking%250Amodels%2520while%2520maintaining%2520their%2520original%2520strong%2520reasoning%2520capabilities.%2520Our%2520code%250Ais%2520available%2520at%2520https%253A//github.com/zjunlp/KnowRL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19807v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KnowRL%3A%20Exploring%20Knowledgeable%20Reinforcement%20Learning%20for%20Factuality&entry.906535625=Baochang%20Ren%20and%20Shuofei%20Qiao%20and%20Da%20Zheng%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%2C%20particularly%20slow-thinking%20models%2C%20often%0Aexhibit%20severe%20hallucination%2C%20outputting%20incorrect%20content%20due%20to%20an%20inability%0Ato%20accurately%20recognize%20knowledge%20boundaries%20during%20reasoning.%20While%0AReinforcement%20Learning%20%28RL%29%20can%20enhance%20complex%20reasoning%20abilities%2C%20its%0Aoutcome-oriented%20reward%20mechanism%20often%20lacks%20factual%20supervision%20over%20the%0Athinking%20process%2C%20further%20exacerbating%20the%20hallucination%20problem.%20To%20address%0Athe%20high%20hallucination%20in%20slow-thinking%20models%2C%20we%20propose%20Knowledge-enhanced%0ARL%2C%20KnowRL.%20KnowRL%20guides%20models%20to%20perform%20fact-based%20slow%20thinking%20by%0Aintegrating%20a%20factuality%20reward%2C%20based%20on%20knowledge%20verification%2C%20into%20the%20RL%0Atraining%20process%2C%20helping%20them%20recognize%20their%20knowledge%20boundaries.%20KnowRL%0Aguides%20models%20to%20perform%20fact-based%20slow%20thinking%20by%20integrating%20a%20factuality%0Areward%2C%20based%20on%20knowledge%20verification%2C%20into%20the%20RL%20training%20process%2C%20helping%0Athem%20recognize%20their%20knowledge%20boundaries.%20This%20targeted%20factual%20input%20during%0ARL%20training%20enables%20the%20model%20to%20learn%20and%20internalize%20fact-based%20reasoning%0Astrategies.%20By%20directly%20rewarding%20adherence%20to%20facts%20within%20the%20reasoning%0Asteps%2C%20KnowRL%20fosters%20a%20more%20reliable%20thinking%20process.%20Experimental%20results%20on%0Athree%20hallucination%20evaluation%20datasets%20and%20two%20reasoning%20evaluation%20datasets%0Ademonstrate%20that%20KnowRL%20effectively%20mitigates%20hallucinations%20in%20slow-thinking%0Amodels%20while%20maintaining%20their%20original%20strong%20reasoning%20capabilities.%20Our%20code%0Ais%20available%20at%20https%3A//github.com/zjunlp/KnowRL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.19807v3&entry.124074799=Read"},
{"title": "Vibe Checker: Aligning Code Evaluation with Human Preference", "author": "Ming Zhong and Xiang Zhou and Ting-Yun Chang and Qingze Wang and Nan Xu and Xiance Si and Dan Garrette and Shyam Upadhyay and Jeremiah Liu and Jiawei Han and Benoit Schillings and Jiao Sun", "abstract": "  Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding.\n", "link": "http://arxiv.org/abs/2510.07315v1", "date": "2025-10-08", "relevancy": 1.9543, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4949}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Vibe%20Checker%3A%20Aligning%20Code%20Evaluation%20with%20Human%20Preference&body=Title%3A%20Vibe%20Checker%3A%20Aligning%20Code%20Evaluation%20with%20Human%20Preference%0AAuthor%3A%20Ming%20Zhong%20and%20Xiang%20Zhou%20and%20Ting-Yun%20Chang%20and%20Qingze%20Wang%20and%20Nan%20Xu%20and%20Xiance%20Si%20and%20Dan%20Garrette%20and%20Shyam%20Upadhyay%20and%20Jeremiah%20Liu%20and%20Jiawei%20Han%20and%20Benoit%20Schillings%20and%20Jiao%20Sun%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20catalyzed%20vibe%20coding%2C%20where%20users%20leverage%0ALLMs%20to%20generate%20and%20iteratively%20refine%20code%20through%20natural%20language%0Ainteractions%20until%20it%20passes%20their%20vibe%20check.%20Vibe%20check%20is%20tied%20to%20real-world%0Ahuman%20preference%20and%20goes%20beyond%20functionality%3A%20the%20solution%20should%20feel%20right%2C%0Aread%20cleanly%2C%20preserve%20intent%2C%20and%20remain%20correct.%20However%2C%20current%20code%0Aevaluation%20remains%20anchored%20to%20pass%40k%20and%20captures%20only%20functional%20correctness%2C%0Aoverlooking%20the%20non-functional%20instructions%20that%20users%20routinely%20apply.%20In%20this%0Apaper%2C%20we%20hypothesize%20that%20instruction%20following%20is%20the%20missing%20piece%0Aunderlying%20vibe%20check%20that%20represents%20human%20preference%20in%20coding%20besides%0Afunctional%20correctness.%20To%20quantify%20models%27%20code%20instruction%20following%0Acapabilities%20with%20measurable%20signals%2C%20we%20present%20VeriCode%2C%20a%20taxonomy%20of%2030%0Averifiable%20code%20instructions%20together%20with%20corresponding%20deterministic%0Averifiers.%20We%20use%20the%20taxonomy%20to%20augment%20established%20evaluation%20suites%2C%0Aresulting%20in%20Vibe%20Checker%2C%20a%20testbed%20to%20assess%20both%20code%20instruction%20following%0Aand%20functional%20correctness.%20Upon%20evaluating%2031%20leading%20LLMs%2C%20we%20show%20that%20even%0Athe%20strongest%20models%20struggle%20to%20comply%20with%20multiple%20instructions%20and%20exhibit%0Aclear%20functional%20regression.%20Most%20importantly%2C%20a%20composite%20score%20of%20functional%0Acorrectness%20and%20instruction%20following%20correlates%20the%20best%20with%20human%0Apreference%2C%20with%20the%20latter%20emerging%20as%20the%20primary%20differentiator%20on%0Areal-world%20programming%20tasks.%20Our%20work%20identifies%20core%20factors%20of%20the%20vibe%0Acheck%2C%20providing%20a%20concrete%20path%20for%20benchmarking%20and%20developing%20models%20that%0Abetter%20align%20with%20user%20preferences%20in%20coding.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07315v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVibe%2520Checker%253A%2520Aligning%2520Code%2520Evaluation%2520with%2520Human%2520Preference%26entry.906535625%3DMing%2520Zhong%2520and%2520Xiang%2520Zhou%2520and%2520Ting-Yun%2520Chang%2520and%2520Qingze%2520Wang%2520and%2520Nan%2520Xu%2520and%2520Xiance%2520Si%2520and%2520Dan%2520Garrette%2520and%2520Shyam%2520Upadhyay%2520and%2520Jeremiah%2520Liu%2520and%2520Jiawei%2520Han%2520and%2520Benoit%2520Schillings%2520and%2520Jiao%2520Sun%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520catalyzed%2520vibe%2520coding%252C%2520where%2520users%2520leverage%250ALLMs%2520to%2520generate%2520and%2520iteratively%2520refine%2520code%2520through%2520natural%2520language%250Ainteractions%2520until%2520it%2520passes%2520their%2520vibe%2520check.%2520Vibe%2520check%2520is%2520tied%2520to%2520real-world%250Ahuman%2520preference%2520and%2520goes%2520beyond%2520functionality%253A%2520the%2520solution%2520should%2520feel%2520right%252C%250Aread%2520cleanly%252C%2520preserve%2520intent%252C%2520and%2520remain%2520correct.%2520However%252C%2520current%2520code%250Aevaluation%2520remains%2520anchored%2520to%2520pass%2540k%2520and%2520captures%2520only%2520functional%2520correctness%252C%250Aoverlooking%2520the%2520non-functional%2520instructions%2520that%2520users%2520routinely%2520apply.%2520In%2520this%250Apaper%252C%2520we%2520hypothesize%2520that%2520instruction%2520following%2520is%2520the%2520missing%2520piece%250Aunderlying%2520vibe%2520check%2520that%2520represents%2520human%2520preference%2520in%2520coding%2520besides%250Afunctional%2520correctness.%2520To%2520quantify%2520models%2527%2520code%2520instruction%2520following%250Acapabilities%2520with%2520measurable%2520signals%252C%2520we%2520present%2520VeriCode%252C%2520a%2520taxonomy%2520of%252030%250Averifiable%2520code%2520instructions%2520together%2520with%2520corresponding%2520deterministic%250Averifiers.%2520We%2520use%2520the%2520taxonomy%2520to%2520augment%2520established%2520evaluation%2520suites%252C%250Aresulting%2520in%2520Vibe%2520Checker%252C%2520a%2520testbed%2520to%2520assess%2520both%2520code%2520instruction%2520following%250Aand%2520functional%2520correctness.%2520Upon%2520evaluating%252031%2520leading%2520LLMs%252C%2520we%2520show%2520that%2520even%250Athe%2520strongest%2520models%2520struggle%2520to%2520comply%2520with%2520multiple%2520instructions%2520and%2520exhibit%250Aclear%2520functional%2520regression.%2520Most%2520importantly%252C%2520a%2520composite%2520score%2520of%2520functional%250Acorrectness%2520and%2520instruction%2520following%2520correlates%2520the%2520best%2520with%2520human%250Apreference%252C%2520with%2520the%2520latter%2520emerging%2520as%2520the%2520primary%2520differentiator%2520on%250Areal-world%2520programming%2520tasks.%2520Our%2520work%2520identifies%2520core%2520factors%2520of%2520the%2520vibe%250Acheck%252C%2520providing%2520a%2520concrete%2520path%2520for%2520benchmarking%2520and%2520developing%2520models%2520that%250Abetter%2520align%2520with%2520user%2520preferences%2520in%2520coding.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07315v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Vibe%20Checker%3A%20Aligning%20Code%20Evaluation%20with%20Human%20Preference&entry.906535625=Ming%20Zhong%20and%20Xiang%20Zhou%20and%20Ting-Yun%20Chang%20and%20Qingze%20Wang%20and%20Nan%20Xu%20and%20Xiance%20Si%20and%20Dan%20Garrette%20and%20Shyam%20Upadhyay%20and%20Jeremiah%20Liu%20and%20Jiawei%20Han%20and%20Benoit%20Schillings%20and%20Jiao%20Sun&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20catalyzed%20vibe%20coding%2C%20where%20users%20leverage%0ALLMs%20to%20generate%20and%20iteratively%20refine%20code%20through%20natural%20language%0Ainteractions%20until%20it%20passes%20their%20vibe%20check.%20Vibe%20check%20is%20tied%20to%20real-world%0Ahuman%20preference%20and%20goes%20beyond%20functionality%3A%20the%20solution%20should%20feel%20right%2C%0Aread%20cleanly%2C%20preserve%20intent%2C%20and%20remain%20correct.%20However%2C%20current%20code%0Aevaluation%20remains%20anchored%20to%20pass%40k%20and%20captures%20only%20functional%20correctness%2C%0Aoverlooking%20the%20non-functional%20instructions%20that%20users%20routinely%20apply.%20In%20this%0Apaper%2C%20we%20hypothesize%20that%20instruction%20following%20is%20the%20missing%20piece%0Aunderlying%20vibe%20check%20that%20represents%20human%20preference%20in%20coding%20besides%0Afunctional%20correctness.%20To%20quantify%20models%27%20code%20instruction%20following%0Acapabilities%20with%20measurable%20signals%2C%20we%20present%20VeriCode%2C%20a%20taxonomy%20of%2030%0Averifiable%20code%20instructions%20together%20with%20corresponding%20deterministic%0Averifiers.%20We%20use%20the%20taxonomy%20to%20augment%20established%20evaluation%20suites%2C%0Aresulting%20in%20Vibe%20Checker%2C%20a%20testbed%20to%20assess%20both%20code%20instruction%20following%0Aand%20functional%20correctness.%20Upon%20evaluating%2031%20leading%20LLMs%2C%20we%20show%20that%20even%0Athe%20strongest%20models%20struggle%20to%20comply%20with%20multiple%20instructions%20and%20exhibit%0Aclear%20functional%20regression.%20Most%20importantly%2C%20a%20composite%20score%20of%20functional%0Acorrectness%20and%20instruction%20following%20correlates%20the%20best%20with%20human%0Apreference%2C%20with%20the%20latter%20emerging%20as%20the%20primary%20differentiator%20on%0Areal-world%20programming%20tasks.%20Our%20work%20identifies%20core%20factors%20of%20the%20vibe%0Acheck%2C%20providing%20a%20concrete%20path%20for%20benchmarking%20and%20developing%20models%20that%0Abetter%20align%20with%20user%20preferences%20in%20coding.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07315v1&entry.124074799=Read"},
{"title": "Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers", "author": "Gangwei Xu and Haotong Lin and Hongcheng Luo and Xianqi Wang and Jingfeng Yao and Lianghui Zhu and Yuechuan Pu and Cheng Chi and Haiyang Sun and Bing Wang and Guang Chen and Hangjun Ye and Sida Peng and Xin Yang", "abstract": "  This paper presents Pixel-Perfect Depth, a monocular depth estimation model\nbased on pixel-space diffusion generation that produces high-quality,\nflying-pixel-free point clouds from estimated depth maps. Current generative\ndepth estimation models fine-tune Stable Diffusion and achieve impressive\nperformance. However, they require a VAE to compress depth maps into latent\nspace, which inevitably introduces \\textit{flying pixels} at edges and details.\nOur model addresses this challenge by directly performing diffusion generation\nin the pixel space, avoiding VAE-induced artifacts. To overcome the high\ncomplexity associated with pixel-space generation, we introduce two novel\ndesigns: 1) Semantics-Prompted Diffusion Transformers (SP-DiT), which\nincorporate semantic representations from vision foundation models into DiT to\nprompt the diffusion process, thereby preserving global semantic consistency\nwhile enhancing fine-grained visual details; and 2) Cascade DiT Design that\nprogressively increases the number of tokens to further enhance efficiency and\naccuracy. Our model achieves the best performance among all published\ngenerative models across five benchmarks, and significantly outperforms all\nother models in edge-aware point cloud evaluation.\n", "link": "http://arxiv.org/abs/2510.07316v1", "date": "2025-10-08", "relevancy": 1.9114, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6599}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6322}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6267}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pixel-Perfect%20Depth%20with%20Semantics-Prompted%20Diffusion%20Transformers&body=Title%3A%20Pixel-Perfect%20Depth%20with%20Semantics-Prompted%20Diffusion%20Transformers%0AAuthor%3A%20Gangwei%20Xu%20and%20Haotong%20Lin%20and%20Hongcheng%20Luo%20and%20Xianqi%20Wang%20and%20Jingfeng%20Yao%20and%20Lianghui%20Zhu%20and%20Yuechuan%20Pu%20and%20Cheng%20Chi%20and%20Haiyang%20Sun%20and%20Bing%20Wang%20and%20Guang%20Chen%20and%20Hangjun%20Ye%20and%20Sida%20Peng%20and%20Xin%20Yang%0AAbstract%3A%20%20%20This%20paper%20presents%20Pixel-Perfect%20Depth%2C%20a%20monocular%20depth%20estimation%20model%0Abased%20on%20pixel-space%20diffusion%20generation%20that%20produces%20high-quality%2C%0Aflying-pixel-free%20point%20clouds%20from%20estimated%20depth%20maps.%20Current%20generative%0Adepth%20estimation%20models%20fine-tune%20Stable%20Diffusion%20and%20achieve%20impressive%0Aperformance.%20However%2C%20they%20require%20a%20VAE%20to%20compress%20depth%20maps%20into%20latent%0Aspace%2C%20which%20inevitably%20introduces%20%5Ctextit%7Bflying%20pixels%7D%20at%20edges%20and%20details.%0AOur%20model%20addresses%20this%20challenge%20by%20directly%20performing%20diffusion%20generation%0Ain%20the%20pixel%20space%2C%20avoiding%20VAE-induced%20artifacts.%20To%20overcome%20the%20high%0Acomplexity%20associated%20with%20pixel-space%20generation%2C%20we%20introduce%20two%20novel%0Adesigns%3A%201%29%20Semantics-Prompted%20Diffusion%20Transformers%20%28SP-DiT%29%2C%20which%0Aincorporate%20semantic%20representations%20from%20vision%20foundation%20models%20into%20DiT%20to%0Aprompt%20the%20diffusion%20process%2C%20thereby%20preserving%20global%20semantic%20consistency%0Awhile%20enhancing%20fine-grained%20visual%20details%3B%20and%202%29%20Cascade%20DiT%20Design%20that%0Aprogressively%20increases%20the%20number%20of%20tokens%20to%20further%20enhance%20efficiency%20and%0Aaccuracy.%20Our%20model%20achieves%20the%20best%20performance%20among%20all%20published%0Agenerative%20models%20across%20five%20benchmarks%2C%20and%20significantly%20outperforms%20all%0Aother%20models%20in%20edge-aware%20point%20cloud%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07316v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPixel-Perfect%2520Depth%2520with%2520Semantics-Prompted%2520Diffusion%2520Transformers%26entry.906535625%3DGangwei%2520Xu%2520and%2520Haotong%2520Lin%2520and%2520Hongcheng%2520Luo%2520and%2520Xianqi%2520Wang%2520and%2520Jingfeng%2520Yao%2520and%2520Lianghui%2520Zhu%2520and%2520Yuechuan%2520Pu%2520and%2520Cheng%2520Chi%2520and%2520Haiyang%2520Sun%2520and%2520Bing%2520Wang%2520and%2520Guang%2520Chen%2520and%2520Hangjun%2520Ye%2520and%2520Sida%2520Peng%2520and%2520Xin%2520Yang%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520Pixel-Perfect%2520Depth%252C%2520a%2520monocular%2520depth%2520estimation%2520model%250Abased%2520on%2520pixel-space%2520diffusion%2520generation%2520that%2520produces%2520high-quality%252C%250Aflying-pixel-free%2520point%2520clouds%2520from%2520estimated%2520depth%2520maps.%2520Current%2520generative%250Adepth%2520estimation%2520models%2520fine-tune%2520Stable%2520Diffusion%2520and%2520achieve%2520impressive%250Aperformance.%2520However%252C%2520they%2520require%2520a%2520VAE%2520to%2520compress%2520depth%2520maps%2520into%2520latent%250Aspace%252C%2520which%2520inevitably%2520introduces%2520%255Ctextit%257Bflying%2520pixels%257D%2520at%2520edges%2520and%2520details.%250AOur%2520model%2520addresses%2520this%2520challenge%2520by%2520directly%2520performing%2520diffusion%2520generation%250Ain%2520the%2520pixel%2520space%252C%2520avoiding%2520VAE-induced%2520artifacts.%2520To%2520overcome%2520the%2520high%250Acomplexity%2520associated%2520with%2520pixel-space%2520generation%252C%2520we%2520introduce%2520two%2520novel%250Adesigns%253A%25201%2529%2520Semantics-Prompted%2520Diffusion%2520Transformers%2520%2528SP-DiT%2529%252C%2520which%250Aincorporate%2520semantic%2520representations%2520from%2520vision%2520foundation%2520models%2520into%2520DiT%2520to%250Aprompt%2520the%2520diffusion%2520process%252C%2520thereby%2520preserving%2520global%2520semantic%2520consistency%250Awhile%2520enhancing%2520fine-grained%2520visual%2520details%253B%2520and%25202%2529%2520Cascade%2520DiT%2520Design%2520that%250Aprogressively%2520increases%2520the%2520number%2520of%2520tokens%2520to%2520further%2520enhance%2520efficiency%2520and%250Aaccuracy.%2520Our%2520model%2520achieves%2520the%2520best%2520performance%2520among%2520all%2520published%250Agenerative%2520models%2520across%2520five%2520benchmarks%252C%2520and%2520significantly%2520outperforms%2520all%250Aother%2520models%2520in%2520edge-aware%2520point%2520cloud%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07316v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pixel-Perfect%20Depth%20with%20Semantics-Prompted%20Diffusion%20Transformers&entry.906535625=Gangwei%20Xu%20and%20Haotong%20Lin%20and%20Hongcheng%20Luo%20and%20Xianqi%20Wang%20and%20Jingfeng%20Yao%20and%20Lianghui%20Zhu%20and%20Yuechuan%20Pu%20and%20Cheng%20Chi%20and%20Haiyang%20Sun%20and%20Bing%20Wang%20and%20Guang%20Chen%20and%20Hangjun%20Ye%20and%20Sida%20Peng%20and%20Xin%20Yang&entry.1292438233=%20%20This%20paper%20presents%20Pixel-Perfect%20Depth%2C%20a%20monocular%20depth%20estimation%20model%0Abased%20on%20pixel-space%20diffusion%20generation%20that%20produces%20high-quality%2C%0Aflying-pixel-free%20point%20clouds%20from%20estimated%20depth%20maps.%20Current%20generative%0Adepth%20estimation%20models%20fine-tune%20Stable%20Diffusion%20and%20achieve%20impressive%0Aperformance.%20However%2C%20they%20require%20a%20VAE%20to%20compress%20depth%20maps%20into%20latent%0Aspace%2C%20which%20inevitably%20introduces%20%5Ctextit%7Bflying%20pixels%7D%20at%20edges%20and%20details.%0AOur%20model%20addresses%20this%20challenge%20by%20directly%20performing%20diffusion%20generation%0Ain%20the%20pixel%20space%2C%20avoiding%20VAE-induced%20artifacts.%20To%20overcome%20the%20high%0Acomplexity%20associated%20with%20pixel-space%20generation%2C%20we%20introduce%20two%20novel%0Adesigns%3A%201%29%20Semantics-Prompted%20Diffusion%20Transformers%20%28SP-DiT%29%2C%20which%0Aincorporate%20semantic%20representations%20from%20vision%20foundation%20models%20into%20DiT%20to%0Aprompt%20the%20diffusion%20process%2C%20thereby%20preserving%20global%20semantic%20consistency%0Awhile%20enhancing%20fine-grained%20visual%20details%3B%20and%202%29%20Cascade%20DiT%20Design%20that%0Aprogressively%20increases%20the%20number%20of%20tokens%20to%20further%20enhance%20efficiency%20and%0Aaccuracy.%20Our%20model%20achieves%20the%20best%20performance%20among%20all%20published%0Agenerative%20models%20across%20five%20benchmarks%2C%20and%20significantly%20outperforms%20all%0Aother%20models%20in%20edge-aware%20point%20cloud%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07316v1&entry.124074799=Read"},
{"title": "SpecGuard: Spectral Projection-based Advanced Invisible Watermarking", "author": "Inzamamul Alam and Md Tanvir Islam and Khan Muhammad and Simon S. Woo", "abstract": "  Watermarking embeds imperceptible patterns into images for authenticity\nverification. However, existing methods often lack robustness against various\ntransformations primarily including distortions, image regeneration, and\nadversarial perturbation, creating real-world challenges. In this work, we\nintroduce SpecGuard, a novel watermarking approach for robust and invisible\nimage watermarking. Unlike prior approaches, we embed the message inside hidden\nconvolution layers by converting from the spatial domain to the frequency\ndomain using spectral projection of a higher frequency band that is decomposed\nby wavelet projection. Spectral projection employs Fast Fourier Transform\napproximation to transform spatial data into the frequency domain efficiently.\nIn the encoding phase, a strength factor enhances resilience against diverse\nattacks, including adversarial, geometric, and regeneration-based distortions,\nensuring the preservation of copyrighted information. Meanwhile, the decoder\nleverages Parseval's theorem to effectively learn and extract the watermark\npattern, enabling accurate retrieval under challenging transformations. We\nevaluate the proposed SpecGuard based on the embedded watermark's invisibility,\ncapacity, and robustness. Comprehensive experiments demonstrate the proposed\nSpecGuard outperforms the state-of-the-art models. To ensure reproducibility,\nthe full code is released on\n\\href{https://github.com/inzamamulDU/SpecGuard_ICCV_2025}{\\textcolor{blue}{\\textbf{GitHub}}}.\n", "link": "http://arxiv.org/abs/2510.07302v1", "date": "2025-10-08", "relevancy": 1.8991, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4977}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4607}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4527}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SpecGuard%3A%20Spectral%20Projection-based%20Advanced%20Invisible%20Watermarking&body=Title%3A%20SpecGuard%3A%20Spectral%20Projection-based%20Advanced%20Invisible%20Watermarking%0AAuthor%3A%20Inzamamul%20Alam%20and%20Md%20Tanvir%20Islam%20and%20Khan%20Muhammad%20and%20Simon%20S.%20Woo%0AAbstract%3A%20%20%20Watermarking%20embeds%20imperceptible%20patterns%20into%20images%20for%20authenticity%0Averification.%20However%2C%20existing%20methods%20often%20lack%20robustness%20against%20various%0Atransformations%20primarily%20including%20distortions%2C%20image%20regeneration%2C%20and%0Aadversarial%20perturbation%2C%20creating%20real-world%20challenges.%20In%20this%20work%2C%20we%0Aintroduce%20SpecGuard%2C%20a%20novel%20watermarking%20approach%20for%20robust%20and%20invisible%0Aimage%20watermarking.%20Unlike%20prior%20approaches%2C%20we%20embed%20the%20message%20inside%20hidden%0Aconvolution%20layers%20by%20converting%20from%20the%20spatial%20domain%20to%20the%20frequency%0Adomain%20using%20spectral%20projection%20of%20a%20higher%20frequency%20band%20that%20is%20decomposed%0Aby%20wavelet%20projection.%20Spectral%20projection%20employs%20Fast%20Fourier%20Transform%0Aapproximation%20to%20transform%20spatial%20data%20into%20the%20frequency%20domain%20efficiently.%0AIn%20the%20encoding%20phase%2C%20a%20strength%20factor%20enhances%20resilience%20against%20diverse%0Aattacks%2C%20including%20adversarial%2C%20geometric%2C%20and%20regeneration-based%20distortions%2C%0Aensuring%20the%20preservation%20of%20copyrighted%20information.%20Meanwhile%2C%20the%20decoder%0Aleverages%20Parseval%27s%20theorem%20to%20effectively%20learn%20and%20extract%20the%20watermark%0Apattern%2C%20enabling%20accurate%20retrieval%20under%20challenging%20transformations.%20We%0Aevaluate%20the%20proposed%20SpecGuard%20based%20on%20the%20embedded%20watermark%27s%20invisibility%2C%0Acapacity%2C%20and%20robustness.%20Comprehensive%20experiments%20demonstrate%20the%20proposed%0ASpecGuard%20outperforms%20the%20state-of-the-art%20models.%20To%20ensure%20reproducibility%2C%0Athe%20full%20code%20is%20released%20on%0A%5Chref%7Bhttps%3A//github.com/inzamamulDU/SpecGuard_ICCV_2025%7D%7B%5Ctextcolor%7Bblue%7D%7B%5Ctextbf%7BGitHub%7D%7D%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07302v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpecGuard%253A%2520Spectral%2520Projection-based%2520Advanced%2520Invisible%2520Watermarking%26entry.906535625%3DInzamamul%2520Alam%2520and%2520Md%2520Tanvir%2520Islam%2520and%2520Khan%2520Muhammad%2520and%2520Simon%2520S.%2520Woo%26entry.1292438233%3D%2520%2520Watermarking%2520embeds%2520imperceptible%2520patterns%2520into%2520images%2520for%2520authenticity%250Averification.%2520However%252C%2520existing%2520methods%2520often%2520lack%2520robustness%2520against%2520various%250Atransformations%2520primarily%2520including%2520distortions%252C%2520image%2520regeneration%252C%2520and%250Aadversarial%2520perturbation%252C%2520creating%2520real-world%2520challenges.%2520In%2520this%2520work%252C%2520we%250Aintroduce%2520SpecGuard%252C%2520a%2520novel%2520watermarking%2520approach%2520for%2520robust%2520and%2520invisible%250Aimage%2520watermarking.%2520Unlike%2520prior%2520approaches%252C%2520we%2520embed%2520the%2520message%2520inside%2520hidden%250Aconvolution%2520layers%2520by%2520converting%2520from%2520the%2520spatial%2520domain%2520to%2520the%2520frequency%250Adomain%2520using%2520spectral%2520projection%2520of%2520a%2520higher%2520frequency%2520band%2520that%2520is%2520decomposed%250Aby%2520wavelet%2520projection.%2520Spectral%2520projection%2520employs%2520Fast%2520Fourier%2520Transform%250Aapproximation%2520to%2520transform%2520spatial%2520data%2520into%2520the%2520frequency%2520domain%2520efficiently.%250AIn%2520the%2520encoding%2520phase%252C%2520a%2520strength%2520factor%2520enhances%2520resilience%2520against%2520diverse%250Aattacks%252C%2520including%2520adversarial%252C%2520geometric%252C%2520and%2520regeneration-based%2520distortions%252C%250Aensuring%2520the%2520preservation%2520of%2520copyrighted%2520information.%2520Meanwhile%252C%2520the%2520decoder%250Aleverages%2520Parseval%2527s%2520theorem%2520to%2520effectively%2520learn%2520and%2520extract%2520the%2520watermark%250Apattern%252C%2520enabling%2520accurate%2520retrieval%2520under%2520challenging%2520transformations.%2520We%250Aevaluate%2520the%2520proposed%2520SpecGuard%2520based%2520on%2520the%2520embedded%2520watermark%2527s%2520invisibility%252C%250Acapacity%252C%2520and%2520robustness.%2520Comprehensive%2520experiments%2520demonstrate%2520the%2520proposed%250ASpecGuard%2520outperforms%2520the%2520state-of-the-art%2520models.%2520To%2520ensure%2520reproducibility%252C%250Athe%2520full%2520code%2520is%2520released%2520on%250A%255Chref%257Bhttps%253A//github.com/inzamamulDU/SpecGuard_ICCV_2025%257D%257B%255Ctextcolor%257Bblue%257D%257B%255Ctextbf%257BGitHub%257D%257D%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07302v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SpecGuard%3A%20Spectral%20Projection-based%20Advanced%20Invisible%20Watermarking&entry.906535625=Inzamamul%20Alam%20and%20Md%20Tanvir%20Islam%20and%20Khan%20Muhammad%20and%20Simon%20S.%20Woo&entry.1292438233=%20%20Watermarking%20embeds%20imperceptible%20patterns%20into%20images%20for%20authenticity%0Averification.%20However%2C%20existing%20methods%20often%20lack%20robustness%20against%20various%0Atransformations%20primarily%20including%20distortions%2C%20image%20regeneration%2C%20and%0Aadversarial%20perturbation%2C%20creating%20real-world%20challenges.%20In%20this%20work%2C%20we%0Aintroduce%20SpecGuard%2C%20a%20novel%20watermarking%20approach%20for%20robust%20and%20invisible%0Aimage%20watermarking.%20Unlike%20prior%20approaches%2C%20we%20embed%20the%20message%20inside%20hidden%0Aconvolution%20layers%20by%20converting%20from%20the%20spatial%20domain%20to%20the%20frequency%0Adomain%20using%20spectral%20projection%20of%20a%20higher%20frequency%20band%20that%20is%20decomposed%0Aby%20wavelet%20projection.%20Spectral%20projection%20employs%20Fast%20Fourier%20Transform%0Aapproximation%20to%20transform%20spatial%20data%20into%20the%20frequency%20domain%20efficiently.%0AIn%20the%20encoding%20phase%2C%20a%20strength%20factor%20enhances%20resilience%20against%20diverse%0Aattacks%2C%20including%20adversarial%2C%20geometric%2C%20and%20regeneration-based%20distortions%2C%0Aensuring%20the%20preservation%20of%20copyrighted%20information.%20Meanwhile%2C%20the%20decoder%0Aleverages%20Parseval%27s%20theorem%20to%20effectively%20learn%20and%20extract%20the%20watermark%0Apattern%2C%20enabling%20accurate%20retrieval%20under%20challenging%20transformations.%20We%0Aevaluate%20the%20proposed%20SpecGuard%20based%20on%20the%20embedded%20watermark%27s%20invisibility%2C%0Acapacity%2C%20and%20robustness.%20Comprehensive%20experiments%20demonstrate%20the%20proposed%0ASpecGuard%20outperforms%20the%20state-of-the-art%20models.%20To%20ensure%20reproducibility%2C%0Athe%20full%20code%20is%20released%20on%0A%5Chref%7Bhttps%3A//github.com/inzamamulDU/SpecGuard_ICCV_2025%7D%7B%5Ctextcolor%7Bblue%7D%7B%5Ctextbf%7BGitHub%7D%7D%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07302v1&entry.124074799=Read"},
{"title": "SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation\n  Models", "author": "Jigang Fan and Zhenghong Zhou and Ruofan Jin and Le Cong and Mengdi Wang and Zaixi Zhang", "abstract": "  Proteins play crucial roles in almost all biological processes. The\nadvancement of deep learning has greatly accelerated the development of protein\nfoundation models, leading to significant successes in protein understanding\nand design. However, the lack of systematic red-teaming for these models has\nraised serious concerns about their potential misuse, such as generating\nproteins with biological safety risks. This paper introduces SafeProtein, the\nfirst red-teaming framework designed for protein foundation models to the best\nof our knowledge. SafeProtein combines multimodal prompt engineering and\nheuristic beam search to systematically design red-teaming methods and conduct\ntests on protein foundation models. We also curated SafeProtein-Bench, which\nincludes a manually constructed red-teaming benchmark dataset and a\ncomprehensive evaluation protocol. SafeProtein achieved continuous jailbreaks\non state-of-the-art protein foundation models (up to 70% attack success rate\nfor ESM3), revealing potential biological safety risks in current protein\nfoundation models and providing insights for the development of robust security\nprotection technologies for frontier models. The codes will be made publicly\navailable at https://github.com/jigang-fan/SafeProtein.\n", "link": "http://arxiv.org/abs/2509.03487v2", "date": "2025-10-08", "relevancy": 1.8828, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4777}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4777}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SafeProtein%3A%20Red-Teaming%20Framework%20and%20Benchmark%20for%20Protein%20Foundation%0A%20%20Models&body=Title%3A%20SafeProtein%3A%20Red-Teaming%20Framework%20and%20Benchmark%20for%20Protein%20Foundation%0A%20%20Models%0AAuthor%3A%20Jigang%20Fan%20and%20Zhenghong%20Zhou%20and%20Ruofan%20Jin%20and%20Le%20Cong%20and%20Mengdi%20Wang%20and%20Zaixi%20Zhang%0AAbstract%3A%20%20%20Proteins%20play%20crucial%20roles%20in%20almost%20all%20biological%20processes.%20The%0Aadvancement%20of%20deep%20learning%20has%20greatly%20accelerated%20the%20development%20of%20protein%0Afoundation%20models%2C%20leading%20to%20significant%20successes%20in%20protein%20understanding%0Aand%20design.%20However%2C%20the%20lack%20of%20systematic%20red-teaming%20for%20these%20models%20has%0Araised%20serious%20concerns%20about%20their%20potential%20misuse%2C%20such%20as%20generating%0Aproteins%20with%20biological%20safety%20risks.%20This%20paper%20introduces%20SafeProtein%2C%20the%0Afirst%20red-teaming%20framework%20designed%20for%20protein%20foundation%20models%20to%20the%20best%0Aof%20our%20knowledge.%20SafeProtein%20combines%20multimodal%20prompt%20engineering%20and%0Aheuristic%20beam%20search%20to%20systematically%20design%20red-teaming%20methods%20and%20conduct%0Atests%20on%20protein%20foundation%20models.%20We%20also%20curated%20SafeProtein-Bench%2C%20which%0Aincludes%20a%20manually%20constructed%20red-teaming%20benchmark%20dataset%20and%20a%0Acomprehensive%20evaluation%20protocol.%20SafeProtein%20achieved%20continuous%20jailbreaks%0Aon%20state-of-the-art%20protein%20foundation%20models%20%28up%20to%2070%25%20attack%20success%20rate%0Afor%20ESM3%29%2C%20revealing%20potential%20biological%20safety%20risks%20in%20current%20protein%0Afoundation%20models%20and%20providing%20insights%20for%20the%20development%20of%20robust%20security%0Aprotection%20technologies%20for%20frontier%20models.%20The%20codes%20will%20be%20made%20publicly%0Aavailable%20at%20https%3A//github.com/jigang-fan/SafeProtein.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2509.03487v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafeProtein%253A%2520Red-Teaming%2520Framework%2520and%2520Benchmark%2520for%2520Protein%2520Foundation%250A%2520%2520Models%26entry.906535625%3DJigang%2520Fan%2520and%2520Zhenghong%2520Zhou%2520and%2520Ruofan%2520Jin%2520and%2520Le%2520Cong%2520and%2520Mengdi%2520Wang%2520and%2520Zaixi%2520Zhang%26entry.1292438233%3D%2520%2520Proteins%2520play%2520crucial%2520roles%2520in%2520almost%2520all%2520biological%2520processes.%2520The%250Aadvancement%2520of%2520deep%2520learning%2520has%2520greatly%2520accelerated%2520the%2520development%2520of%2520protein%250Afoundation%2520models%252C%2520leading%2520to%2520significant%2520successes%2520in%2520protein%2520understanding%250Aand%2520design.%2520However%252C%2520the%2520lack%2520of%2520systematic%2520red-teaming%2520for%2520these%2520models%2520has%250Araised%2520serious%2520concerns%2520about%2520their%2520potential%2520misuse%252C%2520such%2520as%2520generating%250Aproteins%2520with%2520biological%2520safety%2520risks.%2520This%2520paper%2520introduces%2520SafeProtein%252C%2520the%250Afirst%2520red-teaming%2520framework%2520designed%2520for%2520protein%2520foundation%2520models%2520to%2520the%2520best%250Aof%2520our%2520knowledge.%2520SafeProtein%2520combines%2520multimodal%2520prompt%2520engineering%2520and%250Aheuristic%2520beam%2520search%2520to%2520systematically%2520design%2520red-teaming%2520methods%2520and%2520conduct%250Atests%2520on%2520protein%2520foundation%2520models.%2520We%2520also%2520curated%2520SafeProtein-Bench%252C%2520which%250Aincludes%2520a%2520manually%2520constructed%2520red-teaming%2520benchmark%2520dataset%2520and%2520a%250Acomprehensive%2520evaluation%2520protocol.%2520SafeProtein%2520achieved%2520continuous%2520jailbreaks%250Aon%2520state-of-the-art%2520protein%2520foundation%2520models%2520%2528up%2520to%252070%2525%2520attack%2520success%2520rate%250Afor%2520ESM3%2529%252C%2520revealing%2520potential%2520biological%2520safety%2520risks%2520in%2520current%2520protein%250Afoundation%2520models%2520and%2520providing%2520insights%2520for%2520the%2520development%2520of%2520robust%2520security%250Aprotection%2520technologies%2520for%2520frontier%2520models.%2520The%2520codes%2520will%2520be%2520made%2520publicly%250Aavailable%2520at%2520https%253A//github.com/jigang-fan/SafeProtein.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.03487v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SafeProtein%3A%20Red-Teaming%20Framework%20and%20Benchmark%20for%20Protein%20Foundation%0A%20%20Models&entry.906535625=Jigang%20Fan%20and%20Zhenghong%20Zhou%20and%20Ruofan%20Jin%20and%20Le%20Cong%20and%20Mengdi%20Wang%20and%20Zaixi%20Zhang&entry.1292438233=%20%20Proteins%20play%20crucial%20roles%20in%20almost%20all%20biological%20processes.%20The%0Aadvancement%20of%20deep%20learning%20has%20greatly%20accelerated%20the%20development%20of%20protein%0Afoundation%20models%2C%20leading%20to%20significant%20successes%20in%20protein%20understanding%0Aand%20design.%20However%2C%20the%20lack%20of%20systematic%20red-teaming%20for%20these%20models%20has%0Araised%20serious%20concerns%20about%20their%20potential%20misuse%2C%20such%20as%20generating%0Aproteins%20with%20biological%20safety%20risks.%20This%20paper%20introduces%20SafeProtein%2C%20the%0Afirst%20red-teaming%20framework%20designed%20for%20protein%20foundation%20models%20to%20the%20best%0Aof%20our%20knowledge.%20SafeProtein%20combines%20multimodal%20prompt%20engineering%20and%0Aheuristic%20beam%20search%20to%20systematically%20design%20red-teaming%20methods%20and%20conduct%0Atests%20on%20protein%20foundation%20models.%20We%20also%20curated%20SafeProtein-Bench%2C%20which%0Aincludes%20a%20manually%20constructed%20red-teaming%20benchmark%20dataset%20and%20a%0Acomprehensive%20evaluation%20protocol.%20SafeProtein%20achieved%20continuous%20jailbreaks%0Aon%20state-of-the-art%20protein%20foundation%20models%20%28up%20to%2070%25%20attack%20success%20rate%0Afor%20ESM3%29%2C%20revealing%20potential%20biological%20safety%20risks%20in%20current%20protein%0Afoundation%20models%20and%20providing%20insights%20for%20the%20development%20of%20robust%20security%0Aprotection%20technologies%20for%20frontier%20models.%20The%20codes%20will%20be%20made%20publicly%0Aavailable%20at%20https%3A//github.com/jigang-fan/SafeProtein.%0A&entry.1838667208=http%3A//arxiv.org/abs/2509.03487v2&entry.124074799=Read"},
{"title": "Is My Data in Your AI? Membership Inference Test (MINT) applied to Face\n  Biometrics", "author": "Daniel DeAlcala and Aythami Morales and Julian Fierrez and Gonzalo Mancera and Ruben Tolosana and Javier Ortega-Garcia", "abstract": "  This article introduces the Membership Inference Test (MINT), a novel\napproach that aims to empirically assess if given data was used during the\ntraining of AI/ML models. Specifically, we propose two MINT architectures\ndesigned to learn the distinct activation patterns that emerge when an Audited\nModel is exposed to data used during its training process. These architectures\nare based on Multilayer Perceptrons (MLPs) and Convolutional Neural Networks\n(CNNs). The experimental framework focuses on the challenging task of Face\nRecognition, considering three state-of-the-art Face Recognition systems.\nExperiments are carried out using six publicly available databases, comprising\nover 22 million face images in total. Different experimental scenarios are\nconsidered depending on the context of the AI model to test. Our proposed MINT\napproach achieves promising results, with up to 90\\% accuracy, indicating the\npotential to recognize if an AI model has been trained with specific data. The\nproposed MINT approach can serve to enforce privacy and fairness in several AI\napplications, e.g., revealing if sensitive or private data was used for\ntraining or tuning Large Language Models (LLMs).\n", "link": "http://arxiv.org/abs/2402.09225v4", "date": "2025-10-08", "relevancy": 1.8602, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4816}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4658}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Is%20My%20Data%20in%20Your%20AI%3F%20Membership%20Inference%20Test%20%28MINT%29%20applied%20to%20Face%0A%20%20Biometrics&body=Title%3A%20Is%20My%20Data%20in%20Your%20AI%3F%20Membership%20Inference%20Test%20%28MINT%29%20applied%20to%20Face%0A%20%20Biometrics%0AAuthor%3A%20Daniel%20DeAlcala%20and%20Aythami%20Morales%20and%20Julian%20Fierrez%20and%20Gonzalo%20Mancera%20and%20Ruben%20Tolosana%20and%20Javier%20Ortega-Garcia%0AAbstract%3A%20%20%20This%20article%20introduces%20the%20Membership%20Inference%20Test%20%28MINT%29%2C%20a%20novel%0Aapproach%20that%20aims%20to%20empirically%20assess%20if%20given%20data%20was%20used%20during%20the%0Atraining%20of%20AI/ML%20models.%20Specifically%2C%20we%20propose%20two%20MINT%20architectures%0Adesigned%20to%20learn%20the%20distinct%20activation%20patterns%20that%20emerge%20when%20an%20Audited%0AModel%20is%20exposed%20to%20data%20used%20during%20its%20training%20process.%20These%20architectures%0Aare%20based%20on%20Multilayer%20Perceptrons%20%28MLPs%29%20and%20Convolutional%20Neural%20Networks%0A%28CNNs%29.%20The%20experimental%20framework%20focuses%20on%20the%20challenging%20task%20of%20Face%0ARecognition%2C%20considering%20three%20state-of-the-art%20Face%20Recognition%20systems.%0AExperiments%20are%20carried%20out%20using%20six%20publicly%20available%20databases%2C%20comprising%0Aover%2022%20million%20face%20images%20in%20total.%20Different%20experimental%20scenarios%20are%0Aconsidered%20depending%20on%20the%20context%20of%20the%20AI%20model%20to%20test.%20Our%20proposed%20MINT%0Aapproach%20achieves%20promising%20results%2C%20with%20up%20to%2090%5C%25%20accuracy%2C%20indicating%20the%0Apotential%20to%20recognize%20if%20an%20AI%20model%20has%20been%20trained%20with%20specific%20data.%20The%0Aproposed%20MINT%20approach%20can%20serve%20to%20enforce%20privacy%20and%20fairness%20in%20several%20AI%0Aapplications%2C%20e.g.%2C%20revealing%20if%20sensitive%20or%20private%20data%20was%20used%20for%0Atraining%20or%20tuning%20Large%20Language%20Models%20%28LLMs%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.09225v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIs%2520My%2520Data%2520in%2520Your%2520AI%253F%2520Membership%2520Inference%2520Test%2520%2528MINT%2529%2520applied%2520to%2520Face%250A%2520%2520Biometrics%26entry.906535625%3DDaniel%2520DeAlcala%2520and%2520Aythami%2520Morales%2520and%2520Julian%2520Fierrez%2520and%2520Gonzalo%2520Mancera%2520and%2520Ruben%2520Tolosana%2520and%2520Javier%2520Ortega-Garcia%26entry.1292438233%3D%2520%2520This%2520article%2520introduces%2520the%2520Membership%2520Inference%2520Test%2520%2528MINT%2529%252C%2520a%2520novel%250Aapproach%2520that%2520aims%2520to%2520empirically%2520assess%2520if%2520given%2520data%2520was%2520used%2520during%2520the%250Atraining%2520of%2520AI/ML%2520models.%2520Specifically%252C%2520we%2520propose%2520two%2520MINT%2520architectures%250Adesigned%2520to%2520learn%2520the%2520distinct%2520activation%2520patterns%2520that%2520emerge%2520when%2520an%2520Audited%250AModel%2520is%2520exposed%2520to%2520data%2520used%2520during%2520its%2520training%2520process.%2520These%2520architectures%250Aare%2520based%2520on%2520Multilayer%2520Perceptrons%2520%2528MLPs%2529%2520and%2520Convolutional%2520Neural%2520Networks%250A%2528CNNs%2529.%2520The%2520experimental%2520framework%2520focuses%2520on%2520the%2520challenging%2520task%2520of%2520Face%250ARecognition%252C%2520considering%2520three%2520state-of-the-art%2520Face%2520Recognition%2520systems.%250AExperiments%2520are%2520carried%2520out%2520using%2520six%2520publicly%2520available%2520databases%252C%2520comprising%250Aover%252022%2520million%2520face%2520images%2520in%2520total.%2520Different%2520experimental%2520scenarios%2520are%250Aconsidered%2520depending%2520on%2520the%2520context%2520of%2520the%2520AI%2520model%2520to%2520test.%2520Our%2520proposed%2520MINT%250Aapproach%2520achieves%2520promising%2520results%252C%2520with%2520up%2520to%252090%255C%2525%2520accuracy%252C%2520indicating%2520the%250Apotential%2520to%2520recognize%2520if%2520an%2520AI%2520model%2520has%2520been%2520trained%2520with%2520specific%2520data.%2520The%250Aproposed%2520MINT%2520approach%2520can%2520serve%2520to%2520enforce%2520privacy%2520and%2520fairness%2520in%2520several%2520AI%250Aapplications%252C%2520e.g.%252C%2520revealing%2520if%2520sensitive%2520or%2520private%2520data%2520was%2520used%2520for%250Atraining%2520or%2520tuning%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.09225v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Is%20My%20Data%20in%20Your%20AI%3F%20Membership%20Inference%20Test%20%28MINT%29%20applied%20to%20Face%0A%20%20Biometrics&entry.906535625=Daniel%20DeAlcala%20and%20Aythami%20Morales%20and%20Julian%20Fierrez%20and%20Gonzalo%20Mancera%20and%20Ruben%20Tolosana%20and%20Javier%20Ortega-Garcia&entry.1292438233=%20%20This%20article%20introduces%20the%20Membership%20Inference%20Test%20%28MINT%29%2C%20a%20novel%0Aapproach%20that%20aims%20to%20empirically%20assess%20if%20given%20data%20was%20used%20during%20the%0Atraining%20of%20AI/ML%20models.%20Specifically%2C%20we%20propose%20two%20MINT%20architectures%0Adesigned%20to%20learn%20the%20distinct%20activation%20patterns%20that%20emerge%20when%20an%20Audited%0AModel%20is%20exposed%20to%20data%20used%20during%20its%20training%20process.%20These%20architectures%0Aare%20based%20on%20Multilayer%20Perceptrons%20%28MLPs%29%20and%20Convolutional%20Neural%20Networks%0A%28CNNs%29.%20The%20experimental%20framework%20focuses%20on%20the%20challenging%20task%20of%20Face%0ARecognition%2C%20considering%20three%20state-of-the-art%20Face%20Recognition%20systems.%0AExperiments%20are%20carried%20out%20using%20six%20publicly%20available%20databases%2C%20comprising%0Aover%2022%20million%20face%20images%20in%20total.%20Different%20experimental%20scenarios%20are%0Aconsidered%20depending%20on%20the%20context%20of%20the%20AI%20model%20to%20test.%20Our%20proposed%20MINT%0Aapproach%20achieves%20promising%20results%2C%20with%20up%20to%2090%5C%25%20accuracy%2C%20indicating%20the%0Apotential%20to%20recognize%20if%20an%20AI%20model%20has%20been%20trained%20with%20specific%20data.%20The%0Aproposed%20MINT%20approach%20can%20serve%20to%20enforce%20privacy%20and%20fairness%20in%20several%20AI%0Aapplications%2C%20e.g.%2C%20revealing%20if%20sensitive%20or%20private%20data%20was%20used%20for%0Atraining%20or%20tuning%20Large%20Language%20Models%20%28LLMs%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.09225v4&entry.124074799=Read"},
{"title": "Dynamic Regret Bounds for Online Omniprediction with Long Term\n  Constraints", "author": "Yahav Bechavod and Jiuyao Lu and Aaron Roth", "abstract": "  We present an algorithm guaranteeing dynamic regret bounds for online\nomniprediction with long term constraints. The goal in this recently introduced\nproblem is for a learner to generate a sequence of predictions which are\nbroadcast to a collection of downstream decision makers. Each decision maker\nhas their own utility function, as well as a vector of constraint functions,\neach mapping their actions and an adversarially selected state to reward or\nconstraint violation terms. The downstream decision makers select actions \"as\nif\" the state predictions are correct, and the goal of the learner is to\nproduce predictions such that all downstream decision makers choose actions\nthat give them worst-case utility guarantees while minimizing worst-case\nconstraint violation. Within this framework, we give the first algorithm that\nobtains simultaneous \\emph{dynamic regret} guarantees for all of the agents --\nwhere regret for each agent is measured against a potentially changing sequence\nof actions across rounds of interaction, while also ensuring vanishing\nconstraint violation for each agent. Our results do not require the agents\nthemselves to maintain any state -- they only solve one-round constrained\noptimization problems defined by the prediction made at that round.\n", "link": "http://arxiv.org/abs/2510.07266v1", "date": "2025-10-08", "relevancy": 1.8546, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4772}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4644}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dynamic%20Regret%20Bounds%20for%20Online%20Omniprediction%20with%20Long%20Term%0A%20%20Constraints&body=Title%3A%20Dynamic%20Regret%20Bounds%20for%20Online%20Omniprediction%20with%20Long%20Term%0A%20%20Constraints%0AAuthor%3A%20Yahav%20Bechavod%20and%20Jiuyao%20Lu%20and%20Aaron%20Roth%0AAbstract%3A%20%20%20We%20present%20an%20algorithm%20guaranteeing%20dynamic%20regret%20bounds%20for%20online%0Aomniprediction%20with%20long%20term%20constraints.%20The%20goal%20in%20this%20recently%20introduced%0Aproblem%20is%20for%20a%20learner%20to%20generate%20a%20sequence%20of%20predictions%20which%20are%0Abroadcast%20to%20a%20collection%20of%20downstream%20decision%20makers.%20Each%20decision%20maker%0Ahas%20their%20own%20utility%20function%2C%20as%20well%20as%20a%20vector%20of%20constraint%20functions%2C%0Aeach%20mapping%20their%20actions%20and%20an%20adversarially%20selected%20state%20to%20reward%20or%0Aconstraint%20violation%20terms.%20The%20downstream%20decision%20makers%20select%20actions%20%22as%0Aif%22%20the%20state%20predictions%20are%20correct%2C%20and%20the%20goal%20of%20the%20learner%20is%20to%0Aproduce%20predictions%20such%20that%20all%20downstream%20decision%20makers%20choose%20actions%0Athat%20give%20them%20worst-case%20utility%20guarantees%20while%20minimizing%20worst-case%0Aconstraint%20violation.%20Within%20this%20framework%2C%20we%20give%20the%20first%20algorithm%20that%0Aobtains%20simultaneous%20%5Cemph%7Bdynamic%20regret%7D%20guarantees%20for%20all%20of%20the%20agents%20--%0Awhere%20regret%20for%20each%20agent%20is%20measured%20against%20a%20potentially%20changing%20sequence%0Aof%20actions%20across%20rounds%20of%20interaction%2C%20while%20also%20ensuring%20vanishing%0Aconstraint%20violation%20for%20each%20agent.%20Our%20results%20do%20not%20require%20the%20agents%0Athemselves%20to%20maintain%20any%20state%20--%20they%20only%20solve%20one-round%20constrained%0Aoptimization%20problems%20defined%20by%20the%20prediction%20made%20at%20that%20round.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07266v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynamic%2520Regret%2520Bounds%2520for%2520Online%2520Omniprediction%2520with%2520Long%2520Term%250A%2520%2520Constraints%26entry.906535625%3DYahav%2520Bechavod%2520and%2520Jiuyao%2520Lu%2520and%2520Aaron%2520Roth%26entry.1292438233%3D%2520%2520We%2520present%2520an%2520algorithm%2520guaranteeing%2520dynamic%2520regret%2520bounds%2520for%2520online%250Aomniprediction%2520with%2520long%2520term%2520constraints.%2520The%2520goal%2520in%2520this%2520recently%2520introduced%250Aproblem%2520is%2520for%2520a%2520learner%2520to%2520generate%2520a%2520sequence%2520of%2520predictions%2520which%2520are%250Abroadcast%2520to%2520a%2520collection%2520of%2520downstream%2520decision%2520makers.%2520Each%2520decision%2520maker%250Ahas%2520their%2520own%2520utility%2520function%252C%2520as%2520well%2520as%2520a%2520vector%2520of%2520constraint%2520functions%252C%250Aeach%2520mapping%2520their%2520actions%2520and%2520an%2520adversarially%2520selected%2520state%2520to%2520reward%2520or%250Aconstraint%2520violation%2520terms.%2520The%2520downstream%2520decision%2520makers%2520select%2520actions%2520%2522as%250Aif%2522%2520the%2520state%2520predictions%2520are%2520correct%252C%2520and%2520the%2520goal%2520of%2520the%2520learner%2520is%2520to%250Aproduce%2520predictions%2520such%2520that%2520all%2520downstream%2520decision%2520makers%2520choose%2520actions%250Athat%2520give%2520them%2520worst-case%2520utility%2520guarantees%2520while%2520minimizing%2520worst-case%250Aconstraint%2520violation.%2520Within%2520this%2520framework%252C%2520we%2520give%2520the%2520first%2520algorithm%2520that%250Aobtains%2520simultaneous%2520%255Cemph%257Bdynamic%2520regret%257D%2520guarantees%2520for%2520all%2520of%2520the%2520agents%2520--%250Awhere%2520regret%2520for%2520each%2520agent%2520is%2520measured%2520against%2520a%2520potentially%2520changing%2520sequence%250Aof%2520actions%2520across%2520rounds%2520of%2520interaction%252C%2520while%2520also%2520ensuring%2520vanishing%250Aconstraint%2520violation%2520for%2520each%2520agent.%2520Our%2520results%2520do%2520not%2520require%2520the%2520agents%250Athemselves%2520to%2520maintain%2520any%2520state%2520--%2520they%2520only%2520solve%2520one-round%2520constrained%250Aoptimization%2520problems%2520defined%2520by%2520the%2520prediction%2520made%2520at%2520that%2520round.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07266v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dynamic%20Regret%20Bounds%20for%20Online%20Omniprediction%20with%20Long%20Term%0A%20%20Constraints&entry.906535625=Yahav%20Bechavod%20and%20Jiuyao%20Lu%20and%20Aaron%20Roth&entry.1292438233=%20%20We%20present%20an%20algorithm%20guaranteeing%20dynamic%20regret%20bounds%20for%20online%0Aomniprediction%20with%20long%20term%20constraints.%20The%20goal%20in%20this%20recently%20introduced%0Aproblem%20is%20for%20a%20learner%20to%20generate%20a%20sequence%20of%20predictions%20which%20are%0Abroadcast%20to%20a%20collection%20of%20downstream%20decision%20makers.%20Each%20decision%20maker%0Ahas%20their%20own%20utility%20function%2C%20as%20well%20as%20a%20vector%20of%20constraint%20functions%2C%0Aeach%20mapping%20their%20actions%20and%20an%20adversarially%20selected%20state%20to%20reward%20or%0Aconstraint%20violation%20terms.%20The%20downstream%20decision%20makers%20select%20actions%20%22as%0Aif%22%20the%20state%20predictions%20are%20correct%2C%20and%20the%20goal%20of%20the%20learner%20is%20to%0Aproduce%20predictions%20such%20that%20all%20downstream%20decision%20makers%20choose%20actions%0Athat%20give%20them%20worst-case%20utility%20guarantees%20while%20minimizing%20worst-case%0Aconstraint%20violation.%20Within%20this%20framework%2C%20we%20give%20the%20first%20algorithm%20that%0Aobtains%20simultaneous%20%5Cemph%7Bdynamic%20regret%7D%20guarantees%20for%20all%20of%20the%20agents%20--%0Awhere%20regret%20for%20each%20agent%20is%20measured%20against%20a%20potentially%20changing%20sequence%0Aof%20actions%20across%20rounds%20of%20interaction%2C%20while%20also%20ensuring%20vanishing%0Aconstraint%20violation%20for%20each%20agent.%20Our%20results%20do%20not%20require%20the%20agents%0Athemselves%20to%20maintain%20any%20state%20--%20they%20only%20solve%20one-round%20constrained%0Aoptimization%20problems%20defined%20by%20the%20prediction%20made%20at%20that%20round.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07266v1&entry.124074799=Read"},
{"title": "Empirical Comparison of Membership Inference Attacks in Deep Transfer\n  Learning", "author": "Yuxuan Bai and Gauri Pradhan and Marlon Tobaben and Antti Honkela", "abstract": "  With the emergence of powerful large-scale foundation models, the training\nparadigm is increasingly shifting from from-scratch training to transfer\nlearning. This enables high utility training with small, domain-specific\ndatasets typical in sensitive applications. Membership inference attacks (MIAs)\nprovide an empirical estimate of the privacy leakage by machine learning\nmodels. Yet, prior assessments of MIAs against models fine-tuned with transfer\nlearning rely on a small subset of possible attacks. We address this by\ncomparing performance of diverse MIAs in transfer learning settings to help\npractitioners identify the most efficient attacks for privacy risk evaluation.\nWe find that attack efficacy decreases with the increase in training data for\nscore-based MIAs. We find that there is no one MIA which captures all privacy\nrisks in models trained with transfer learning. While the Likelihood Ratio\nAttack (LiRA) demonstrates superior performance across most experimental\nscenarios, the Inverse Hessian Attack (IHA) proves to be more effective against\nmodels fine-tuned on PatchCamelyon dataset in high data regime.\n", "link": "http://arxiv.org/abs/2510.05753v2", "date": "2025-10-08", "relevancy": 1.8383, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4797}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4564}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Empirical%20Comparison%20of%20Membership%20Inference%20Attacks%20in%20Deep%20Transfer%0A%20%20Learning&body=Title%3A%20Empirical%20Comparison%20of%20Membership%20Inference%20Attacks%20in%20Deep%20Transfer%0A%20%20Learning%0AAuthor%3A%20Yuxuan%20Bai%20and%20Gauri%20Pradhan%20and%20Marlon%20Tobaben%20and%20Antti%20Honkela%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20powerful%20large-scale%20foundation%20models%2C%20the%20training%0Aparadigm%20is%20increasingly%20shifting%20from%20from-scratch%20training%20to%20transfer%0Alearning.%20This%20enables%20high%20utility%20training%20with%20small%2C%20domain-specific%0Adatasets%20typical%20in%20sensitive%20applications.%20Membership%20inference%20attacks%20%28MIAs%29%0Aprovide%20an%20empirical%20estimate%20of%20the%20privacy%20leakage%20by%20machine%20learning%0Amodels.%20Yet%2C%20prior%20assessments%20of%20MIAs%20against%20models%20fine-tuned%20with%20transfer%0Alearning%20rely%20on%20a%20small%20subset%20of%20possible%20attacks.%20We%20address%20this%20by%0Acomparing%20performance%20of%20diverse%20MIAs%20in%20transfer%20learning%20settings%20to%20help%0Apractitioners%20identify%20the%20most%20efficient%20attacks%20for%20privacy%20risk%20evaluation.%0AWe%20find%20that%20attack%20efficacy%20decreases%20with%20the%20increase%20in%20training%20data%20for%0Ascore-based%20MIAs.%20We%20find%20that%20there%20is%20no%20one%20MIA%20which%20captures%20all%20privacy%0Arisks%20in%20models%20trained%20with%20transfer%20learning.%20While%20the%20Likelihood%20Ratio%0AAttack%20%28LiRA%29%20demonstrates%20superior%20performance%20across%20most%20experimental%0Ascenarios%2C%20the%20Inverse%20Hessian%20Attack%20%28IHA%29%20proves%20to%20be%20more%20effective%20against%0Amodels%20fine-tuned%20on%20PatchCamelyon%20dataset%20in%20high%20data%20regime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.05753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEmpirical%2520Comparison%2520of%2520Membership%2520Inference%2520Attacks%2520in%2520Deep%2520Transfer%250A%2520%2520Learning%26entry.906535625%3DYuxuan%2520Bai%2520and%2520Gauri%2520Pradhan%2520and%2520Marlon%2520Tobaben%2520and%2520Antti%2520Honkela%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520powerful%2520large-scale%2520foundation%2520models%252C%2520the%2520training%250Aparadigm%2520is%2520increasingly%2520shifting%2520from%2520from-scratch%2520training%2520to%2520transfer%250Alearning.%2520This%2520enables%2520high%2520utility%2520training%2520with%2520small%252C%2520domain-specific%250Adatasets%2520typical%2520in%2520sensitive%2520applications.%2520Membership%2520inference%2520attacks%2520%2528MIAs%2529%250Aprovide%2520an%2520empirical%2520estimate%2520of%2520the%2520privacy%2520leakage%2520by%2520machine%2520learning%250Amodels.%2520Yet%252C%2520prior%2520assessments%2520of%2520MIAs%2520against%2520models%2520fine-tuned%2520with%2520transfer%250Alearning%2520rely%2520on%2520a%2520small%2520subset%2520of%2520possible%2520attacks.%2520We%2520address%2520this%2520by%250Acomparing%2520performance%2520of%2520diverse%2520MIAs%2520in%2520transfer%2520learning%2520settings%2520to%2520help%250Apractitioners%2520identify%2520the%2520most%2520efficient%2520attacks%2520for%2520privacy%2520risk%2520evaluation.%250AWe%2520find%2520that%2520attack%2520efficacy%2520decreases%2520with%2520the%2520increase%2520in%2520training%2520data%2520for%250Ascore-based%2520MIAs.%2520We%2520find%2520that%2520there%2520is%2520no%2520one%2520MIA%2520which%2520captures%2520all%2520privacy%250Arisks%2520in%2520models%2520trained%2520with%2520transfer%2520learning.%2520While%2520the%2520Likelihood%2520Ratio%250AAttack%2520%2528LiRA%2529%2520demonstrates%2520superior%2520performance%2520across%2520most%2520experimental%250Ascenarios%252C%2520the%2520Inverse%2520Hessian%2520Attack%2520%2528IHA%2529%2520proves%2520to%2520be%2520more%2520effective%2520against%250Amodels%2520fine-tuned%2520on%2520PatchCamelyon%2520dataset%2520in%2520high%2520data%2520regime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Empirical%20Comparison%20of%20Membership%20Inference%20Attacks%20in%20Deep%20Transfer%0A%20%20Learning&entry.906535625=Yuxuan%20Bai%20and%20Gauri%20Pradhan%20and%20Marlon%20Tobaben%20and%20Antti%20Honkela&entry.1292438233=%20%20With%20the%20emergence%20of%20powerful%20large-scale%20foundation%20models%2C%20the%20training%0Aparadigm%20is%20increasingly%20shifting%20from%20from-scratch%20training%20to%20transfer%0Alearning.%20This%20enables%20high%20utility%20training%20with%20small%2C%20domain-specific%0Adatasets%20typical%20in%20sensitive%20applications.%20Membership%20inference%20attacks%20%28MIAs%29%0Aprovide%20an%20empirical%20estimate%20of%20the%20privacy%20leakage%20by%20machine%20learning%0Amodels.%20Yet%2C%20prior%20assessments%20of%20MIAs%20against%20models%20fine-tuned%20with%20transfer%0Alearning%20rely%20on%20a%20small%20subset%20of%20possible%20attacks.%20We%20address%20this%20by%0Acomparing%20performance%20of%20diverse%20MIAs%20in%20transfer%20learning%20settings%20to%20help%0Apractitioners%20identify%20the%20most%20efficient%20attacks%20for%20privacy%20risk%20evaluation.%0AWe%20find%20that%20attack%20efficacy%20decreases%20with%20the%20increase%20in%20training%20data%20for%0Ascore-based%20MIAs.%20We%20find%20that%20there%20is%20no%20one%20MIA%20which%20captures%20all%20privacy%0Arisks%20in%20models%20trained%20with%20transfer%20learning.%20While%20the%20Likelihood%20Ratio%0AAttack%20%28LiRA%29%20demonstrates%20superior%20performance%20across%20most%20experimental%0Ascenarios%2C%20the%20Inverse%20Hessian%20Attack%20%28IHA%29%20proves%20to%20be%20more%20effective%20against%0Amodels%20fine-tuned%20on%20PatchCamelyon%20dataset%20in%20high%20data%20regime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.05753v2&entry.124074799=Read"},
{"title": "Benchmarking LLM Causal Reasoning with Scientifically Validated\n  Relationships", "author": "Donggyu Lee and Sungwon Park and Yerin Hwang and Hyunwoo Oh and Hyoshin Kim and Jungwon Kim and Meeyoung Cha and Sangyoon Park and Jihee Kim", "abstract": "  Causal reasoning is fundamental for Large Language Models (LLMs) to\nunderstand genuine cause-and-effect relationships beyond pattern matching.\nExisting benchmarks suffer from critical limitations such as reliance on\nsynthetic data and narrow domain coverage. We introduce a novel benchmark\nconstructed from casually identified relationships extracted from top-tier\neconomics and finance journals, drawing on rigorous methodologies including\ninstrumental variables, difference-in-differences, and regression discontinuity\ndesigns. Our benchmark comprises 40,379 evaluation items covering five task\ntypes across domains such as health, environment, technology, law, and culture.\nExperimental results on eight state-of-the-art LLMs reveal substantial\nlimitations, with the best model achieving only 57.6\\% accuracy. Moreover,\nmodel scale does not consistently translate to superior performance, and even\nadvanced reasoning models struggle with fundamental causal relationship\nidentification. These findings underscore a critical gap between current LLM\ncapabilities and demands of reliable causal reasoning in high-stakes\napplications.\n", "link": "http://arxiv.org/abs/2510.07231v1", "date": "2025-10-08", "relevancy": 1.8284, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4609}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4609}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4382}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Benchmarking%20LLM%20Causal%20Reasoning%20with%20Scientifically%20Validated%0A%20%20Relationships&body=Title%3A%20Benchmarking%20LLM%20Causal%20Reasoning%20with%20Scientifically%20Validated%0A%20%20Relationships%0AAuthor%3A%20Donggyu%20Lee%20and%20Sungwon%20Park%20and%20Yerin%20Hwang%20and%20Hyunwoo%20Oh%20and%20Hyoshin%20Kim%20and%20Jungwon%20Kim%20and%20Meeyoung%20Cha%20and%20Sangyoon%20Park%20and%20Jihee%20Kim%0AAbstract%3A%20%20%20Causal%20reasoning%20is%20fundamental%20for%20Large%20Language%20Models%20%28LLMs%29%20to%0Aunderstand%20genuine%20cause-and-effect%20relationships%20beyond%20pattern%20matching.%0AExisting%20benchmarks%20suffer%20from%20critical%20limitations%20such%20as%20reliance%20on%0Asynthetic%20data%20and%20narrow%20domain%20coverage.%20We%20introduce%20a%20novel%20benchmark%0Aconstructed%20from%20casually%20identified%20relationships%20extracted%20from%20top-tier%0Aeconomics%20and%20finance%20journals%2C%20drawing%20on%20rigorous%20methodologies%20including%0Ainstrumental%20variables%2C%20difference-in-differences%2C%20and%20regression%20discontinuity%0Adesigns.%20Our%20benchmark%20comprises%2040%2C379%20evaluation%20items%20covering%20five%20task%0Atypes%20across%20domains%20such%20as%20health%2C%20environment%2C%20technology%2C%20law%2C%20and%20culture.%0AExperimental%20results%20on%20eight%20state-of-the-art%20LLMs%20reveal%20substantial%0Alimitations%2C%20with%20the%20best%20model%20achieving%20only%2057.6%5C%25%20accuracy.%20Moreover%2C%0Amodel%20scale%20does%20not%20consistently%20translate%20to%20superior%20performance%2C%20and%20even%0Aadvanced%20reasoning%20models%20struggle%20with%20fundamental%20causal%20relationship%0Aidentification.%20These%20findings%20underscore%20a%20critical%20gap%20between%20current%20LLM%0Acapabilities%20and%20demands%20of%20reliable%20causal%20reasoning%20in%20high-stakes%0Aapplications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07231v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBenchmarking%2520LLM%2520Causal%2520Reasoning%2520with%2520Scientifically%2520Validated%250A%2520%2520Relationships%26entry.906535625%3DDonggyu%2520Lee%2520and%2520Sungwon%2520Park%2520and%2520Yerin%2520Hwang%2520and%2520Hyunwoo%2520Oh%2520and%2520Hyoshin%2520Kim%2520and%2520Jungwon%2520Kim%2520and%2520Meeyoung%2520Cha%2520and%2520Sangyoon%2520Park%2520and%2520Jihee%2520Kim%26entry.1292438233%3D%2520%2520Causal%2520reasoning%2520is%2520fundamental%2520for%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520to%250Aunderstand%2520genuine%2520cause-and-effect%2520relationships%2520beyond%2520pattern%2520matching.%250AExisting%2520benchmarks%2520suffer%2520from%2520critical%2520limitations%2520such%2520as%2520reliance%2520on%250Asynthetic%2520data%2520and%2520narrow%2520domain%2520coverage.%2520We%2520introduce%2520a%2520novel%2520benchmark%250Aconstructed%2520from%2520casually%2520identified%2520relationships%2520extracted%2520from%2520top-tier%250Aeconomics%2520and%2520finance%2520journals%252C%2520drawing%2520on%2520rigorous%2520methodologies%2520including%250Ainstrumental%2520variables%252C%2520difference-in-differences%252C%2520and%2520regression%2520discontinuity%250Adesigns.%2520Our%2520benchmark%2520comprises%252040%252C379%2520evaluation%2520items%2520covering%2520five%2520task%250Atypes%2520across%2520domains%2520such%2520as%2520health%252C%2520environment%252C%2520technology%252C%2520law%252C%2520and%2520culture.%250AExperimental%2520results%2520on%2520eight%2520state-of-the-art%2520LLMs%2520reveal%2520substantial%250Alimitations%252C%2520with%2520the%2520best%2520model%2520achieving%2520only%252057.6%255C%2525%2520accuracy.%2520Moreover%252C%250Amodel%2520scale%2520does%2520not%2520consistently%2520translate%2520to%2520superior%2520performance%252C%2520and%2520even%250Aadvanced%2520reasoning%2520models%2520struggle%2520with%2520fundamental%2520causal%2520relationship%250Aidentification.%2520These%2520findings%2520underscore%2520a%2520critical%2520gap%2520between%2520current%2520LLM%250Acapabilities%2520and%2520demands%2520of%2520reliable%2520causal%2520reasoning%2520in%2520high-stakes%250Aapplications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07231v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Benchmarking%20LLM%20Causal%20Reasoning%20with%20Scientifically%20Validated%0A%20%20Relationships&entry.906535625=Donggyu%20Lee%20and%20Sungwon%20Park%20and%20Yerin%20Hwang%20and%20Hyunwoo%20Oh%20and%20Hyoshin%20Kim%20and%20Jungwon%20Kim%20and%20Meeyoung%20Cha%20and%20Sangyoon%20Park%20and%20Jihee%20Kim&entry.1292438233=%20%20Causal%20reasoning%20is%20fundamental%20for%20Large%20Language%20Models%20%28LLMs%29%20to%0Aunderstand%20genuine%20cause-and-effect%20relationships%20beyond%20pattern%20matching.%0AExisting%20benchmarks%20suffer%20from%20critical%20limitations%20such%20as%20reliance%20on%0Asynthetic%20data%20and%20narrow%20domain%20coverage.%20We%20introduce%20a%20novel%20benchmark%0Aconstructed%20from%20casually%20identified%20relationships%20extracted%20from%20top-tier%0Aeconomics%20and%20finance%20journals%2C%20drawing%20on%20rigorous%20methodologies%20including%0Ainstrumental%20variables%2C%20difference-in-differences%2C%20and%20regression%20discontinuity%0Adesigns.%20Our%20benchmark%20comprises%2040%2C379%20evaluation%20items%20covering%20five%20task%0Atypes%20across%20domains%20such%20as%20health%2C%20environment%2C%20technology%2C%20law%2C%20and%20culture.%0AExperimental%20results%20on%20eight%20state-of-the-art%20LLMs%20reveal%20substantial%0Alimitations%2C%20with%20the%20best%20model%20achieving%20only%2057.6%5C%25%20accuracy.%20Moreover%2C%0Amodel%20scale%20does%20not%20consistently%20translate%20to%20superior%20performance%2C%20and%20even%0Aadvanced%20reasoning%20models%20struggle%20with%20fundamental%20causal%20relationship%0Aidentification.%20These%20findings%20underscore%20a%20critical%20gap%20between%20current%20LLM%0Acapabilities%20and%20demands%20of%20reliable%20causal%20reasoning%20in%20high-stakes%0Aapplications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07231v1&entry.124074799=Read"},
{"title": "CaRDiff: Video Salient Object Ranking Chain of Thought Reasoning for\n  Saliency Prediction with Diffusion", "author": "Yolo Yunlong Tang and Gen Zhan and Li Yang and Yiting Liao and Chenliang Xu", "abstract": "  Video saliency prediction aims to identify the regions in a video that\nattract human attention and gaze, driven by bottom-up features from the video\nand top-down processes like memory and cognition. Among these top-down\ninfluences, language plays a crucial role in guiding attention by shaping how\nvisual information is interpreted. Existing methods primarily focus on modeling\nperceptual information while neglecting the reasoning process facilitated by\nlanguage, where ranking cues are crucial outcomes of this process and practical\nguidance for saliency prediction. In this paper, we propose CaRDiff (Caption,\nRank, and generate with Diffusion), a framework that imitates the process by\nintegrating a multimodal large language model (MLLM), a grounding module, and a\ndiffusion model, to enhance video saliency prediction. Specifically, we\nintroduce a novel prompting method VSOR-CoT (Video Salient Object Ranking Chain\nof Thought), which utilizes an MLLM with a grounding module to caption video\ncontent and infer salient objects along with their rankings and positions. This\nprocess derives ranking maps that can be sufficiently leveraged by the\ndiffusion model to decode the saliency maps for the given video accurately.\nExtensive experiments show the effectiveness of VSOR-CoT in improving the\nperformance of video saliency prediction. The proposed CaRDiff performs better\nthan state-of-the-art models on the MVS dataset and demonstrates cross-dataset\ncapabilities on the DHF1k dataset through zero-shot evaluation.\n", "link": "http://arxiv.org/abs/2408.12009v2", "date": "2025-10-08", "relevancy": 1.795, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6041}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6012}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CaRDiff%3A%20Video%20Salient%20Object%20Ranking%20Chain%20of%20Thought%20Reasoning%20for%0A%20%20Saliency%20Prediction%20with%20Diffusion&body=Title%3A%20CaRDiff%3A%20Video%20Salient%20Object%20Ranking%20Chain%20of%20Thought%20Reasoning%20for%0A%20%20Saliency%20Prediction%20with%20Diffusion%0AAuthor%3A%20Yolo%20Yunlong%20Tang%20and%20Gen%20Zhan%20and%20Li%20Yang%20and%20Yiting%20Liao%20and%20Chenliang%20Xu%0AAbstract%3A%20%20%20Video%20saliency%20prediction%20aims%20to%20identify%20the%20regions%20in%20a%20video%20that%0Aattract%20human%20attention%20and%20gaze%2C%20driven%20by%20bottom-up%20features%20from%20the%20video%0Aand%20top-down%20processes%20like%20memory%20and%20cognition.%20Among%20these%20top-down%0Ainfluences%2C%20language%20plays%20a%20crucial%20role%20in%20guiding%20attention%20by%20shaping%20how%0Avisual%20information%20is%20interpreted.%20Existing%20methods%20primarily%20focus%20on%20modeling%0Aperceptual%20information%20while%20neglecting%20the%20reasoning%20process%20facilitated%20by%0Alanguage%2C%20where%20ranking%20cues%20are%20crucial%20outcomes%20of%20this%20process%20and%20practical%0Aguidance%20for%20saliency%20prediction.%20In%20this%20paper%2C%20we%20propose%20CaRDiff%20%28Caption%2C%0ARank%2C%20and%20generate%20with%20Diffusion%29%2C%20a%20framework%20that%20imitates%20the%20process%20by%0Aintegrating%20a%20multimodal%20large%20language%20model%20%28MLLM%29%2C%20a%20grounding%20module%2C%20and%20a%0Adiffusion%20model%2C%20to%20enhance%20video%20saliency%20prediction.%20Specifically%2C%20we%0Aintroduce%20a%20novel%20prompting%20method%20VSOR-CoT%20%28Video%20Salient%20Object%20Ranking%20Chain%0Aof%20Thought%29%2C%20which%20utilizes%20an%20MLLM%20with%20a%20grounding%20module%20to%20caption%20video%0Acontent%20and%20infer%20salient%20objects%20along%20with%20their%20rankings%20and%20positions.%20This%0Aprocess%20derives%20ranking%20maps%20that%20can%20be%20sufficiently%20leveraged%20by%20the%0Adiffusion%20model%20to%20decode%20the%20saliency%20maps%20for%20the%20given%20video%20accurately.%0AExtensive%20experiments%20show%20the%20effectiveness%20of%20VSOR-CoT%20in%20improving%20the%0Aperformance%20of%20video%20saliency%20prediction.%20The%20proposed%20CaRDiff%20performs%20better%0Athan%20state-of-the-art%20models%20on%20the%20MVS%20dataset%20and%20demonstrates%20cross-dataset%0Acapabilities%20on%20the%20DHF1k%20dataset%20through%20zero-shot%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.12009v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCaRDiff%253A%2520Video%2520Salient%2520Object%2520Ranking%2520Chain%2520of%2520Thought%2520Reasoning%2520for%250A%2520%2520Saliency%2520Prediction%2520with%2520Diffusion%26entry.906535625%3DYolo%2520Yunlong%2520Tang%2520and%2520Gen%2520Zhan%2520and%2520Li%2520Yang%2520and%2520Yiting%2520Liao%2520and%2520Chenliang%2520Xu%26entry.1292438233%3D%2520%2520Video%2520saliency%2520prediction%2520aims%2520to%2520identify%2520the%2520regions%2520in%2520a%2520video%2520that%250Aattract%2520human%2520attention%2520and%2520gaze%252C%2520driven%2520by%2520bottom-up%2520features%2520from%2520the%2520video%250Aand%2520top-down%2520processes%2520like%2520memory%2520and%2520cognition.%2520Among%2520these%2520top-down%250Ainfluences%252C%2520language%2520plays%2520a%2520crucial%2520role%2520in%2520guiding%2520attention%2520by%2520shaping%2520how%250Avisual%2520information%2520is%2520interpreted.%2520Existing%2520methods%2520primarily%2520focus%2520on%2520modeling%250Aperceptual%2520information%2520while%2520neglecting%2520the%2520reasoning%2520process%2520facilitated%2520by%250Alanguage%252C%2520where%2520ranking%2520cues%2520are%2520crucial%2520outcomes%2520of%2520this%2520process%2520and%2520practical%250Aguidance%2520for%2520saliency%2520prediction.%2520In%2520this%2520paper%252C%2520we%2520propose%2520CaRDiff%2520%2528Caption%252C%250ARank%252C%2520and%2520generate%2520with%2520Diffusion%2529%252C%2520a%2520framework%2520that%2520imitates%2520the%2520process%2520by%250Aintegrating%2520a%2520multimodal%2520large%2520language%2520model%2520%2528MLLM%2529%252C%2520a%2520grounding%2520module%252C%2520and%2520a%250Adiffusion%2520model%252C%2520to%2520enhance%2520video%2520saliency%2520prediction.%2520Specifically%252C%2520we%250Aintroduce%2520a%2520novel%2520prompting%2520method%2520VSOR-CoT%2520%2528Video%2520Salient%2520Object%2520Ranking%2520Chain%250Aof%2520Thought%2529%252C%2520which%2520utilizes%2520an%2520MLLM%2520with%2520a%2520grounding%2520module%2520to%2520caption%2520video%250Acontent%2520and%2520infer%2520salient%2520objects%2520along%2520with%2520their%2520rankings%2520and%2520positions.%2520This%250Aprocess%2520derives%2520ranking%2520maps%2520that%2520can%2520be%2520sufficiently%2520leveraged%2520by%2520the%250Adiffusion%2520model%2520to%2520decode%2520the%2520saliency%2520maps%2520for%2520the%2520given%2520video%2520accurately.%250AExtensive%2520experiments%2520show%2520the%2520effectiveness%2520of%2520VSOR-CoT%2520in%2520improving%2520the%250Aperformance%2520of%2520video%2520saliency%2520prediction.%2520The%2520proposed%2520CaRDiff%2520performs%2520better%250Athan%2520state-of-the-art%2520models%2520on%2520the%2520MVS%2520dataset%2520and%2520demonstrates%2520cross-dataset%250Acapabilities%2520on%2520the%2520DHF1k%2520dataset%2520through%2520zero-shot%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.12009v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CaRDiff%3A%20Video%20Salient%20Object%20Ranking%20Chain%20of%20Thought%20Reasoning%20for%0A%20%20Saliency%20Prediction%20with%20Diffusion&entry.906535625=Yolo%20Yunlong%20Tang%20and%20Gen%20Zhan%20and%20Li%20Yang%20and%20Yiting%20Liao%20and%20Chenliang%20Xu&entry.1292438233=%20%20Video%20saliency%20prediction%20aims%20to%20identify%20the%20regions%20in%20a%20video%20that%0Aattract%20human%20attention%20and%20gaze%2C%20driven%20by%20bottom-up%20features%20from%20the%20video%0Aand%20top-down%20processes%20like%20memory%20and%20cognition.%20Among%20these%20top-down%0Ainfluences%2C%20language%20plays%20a%20crucial%20role%20in%20guiding%20attention%20by%20shaping%20how%0Avisual%20information%20is%20interpreted.%20Existing%20methods%20primarily%20focus%20on%20modeling%0Aperceptual%20information%20while%20neglecting%20the%20reasoning%20process%20facilitated%20by%0Alanguage%2C%20where%20ranking%20cues%20are%20crucial%20outcomes%20of%20this%20process%20and%20practical%0Aguidance%20for%20saliency%20prediction.%20In%20this%20paper%2C%20we%20propose%20CaRDiff%20%28Caption%2C%0ARank%2C%20and%20generate%20with%20Diffusion%29%2C%20a%20framework%20that%20imitates%20the%20process%20by%0Aintegrating%20a%20multimodal%20large%20language%20model%20%28MLLM%29%2C%20a%20grounding%20module%2C%20and%20a%0Adiffusion%20model%2C%20to%20enhance%20video%20saliency%20prediction.%20Specifically%2C%20we%0Aintroduce%20a%20novel%20prompting%20method%20VSOR-CoT%20%28Video%20Salient%20Object%20Ranking%20Chain%0Aof%20Thought%29%2C%20which%20utilizes%20an%20MLLM%20with%20a%20grounding%20module%20to%20caption%20video%0Acontent%20and%20infer%20salient%20objects%20along%20with%20their%20rankings%20and%20positions.%20This%0Aprocess%20derives%20ranking%20maps%20that%20can%20be%20sufficiently%20leveraged%20by%20the%0Adiffusion%20model%20to%20decode%20the%20saliency%20maps%20for%20the%20given%20video%20accurately.%0AExtensive%20experiments%20show%20the%20effectiveness%20of%20VSOR-CoT%20in%20improving%20the%0Aperformance%20of%20video%20saliency%20prediction.%20The%20proposed%20CaRDiff%20performs%20better%0Athan%20state-of-the-art%20models%20on%20the%20MVS%20dataset%20and%20demonstrates%20cross-dataset%0Acapabilities%20on%20the%20DHF1k%20dataset%20through%20zero-shot%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.12009v2&entry.124074799=Read"},
{"title": "Bit-Level Discrete Diffusion with Markov Probabilistic Models: An\n  Improved Framework with Sharp Convergence Bounds under Minimal Assumptions", "author": "Le-Tuyet-Nhi Pham and Dario Shariatian and Antonio Ocello and Giovanni Conforti and Alain Durmus", "abstract": "  This paper introduces Discrete Markov Probabilistic Models (DMPMs), a novel\ndiscrete diffusion algorithm for discrete data generation. The algorithm\noperates in discrete bit space, where the noising process is a continuous-time\nMarkov chain that flips labels uniformly at random. The time-reversal process,\nlike the forward noise process, is a jump process with its intensity governed\nby a discrete analogue of the classical score function. Crucially, this\nintensity is proven to be the conditional expectation of a function of the\nforward process, underlining theoretical alignment with score-based generative\nmodels. We establish convergence bounds for the algorithm under minimal\nassumptions, ensuring robustness and efficiency, which we demonstrate through\nexperiments on low-dimensional Bernoulli-distributed datasets and\nhigh-dimensional binary MNIST data. The results highlight competitive\nperformance in generating discrete structures compared to the state-of-the-art.\nThis work bridges theoretical foundations and practical applications, advancing\nthe development of effective and theoretically grounded discrete generative\nmodeling.\n", "link": "http://arxiv.org/abs/2502.07939v2", "date": "2025-10-08", "relevancy": 1.6772, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5817}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5612}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bit-Level%20Discrete%20Diffusion%20with%20Markov%20Probabilistic%20Models%3A%20An%0A%20%20Improved%20Framework%20with%20Sharp%20Convergence%20Bounds%20under%20Minimal%20Assumptions&body=Title%3A%20Bit-Level%20Discrete%20Diffusion%20with%20Markov%20Probabilistic%20Models%3A%20An%0A%20%20Improved%20Framework%20with%20Sharp%20Convergence%20Bounds%20under%20Minimal%20Assumptions%0AAuthor%3A%20Le-Tuyet-Nhi%20Pham%20and%20Dario%20Shariatian%20and%20Antonio%20Ocello%20and%20Giovanni%20Conforti%20and%20Alain%20Durmus%0AAbstract%3A%20%20%20This%20paper%20introduces%20Discrete%20Markov%20Probabilistic%20Models%20%28DMPMs%29%2C%20a%20novel%0Adiscrete%20diffusion%20algorithm%20for%20discrete%20data%20generation.%20The%20algorithm%0Aoperates%20in%20discrete%20bit%20space%2C%20where%20the%20noising%20process%20is%20a%20continuous-time%0AMarkov%20chain%20that%20flips%20labels%20uniformly%20at%20random.%20The%20time-reversal%20process%2C%0Alike%20the%20forward%20noise%20process%2C%20is%20a%20jump%20process%20with%20its%20intensity%20governed%0Aby%20a%20discrete%20analogue%20of%20the%20classical%20score%20function.%20Crucially%2C%20this%0Aintensity%20is%20proven%20to%20be%20the%20conditional%20expectation%20of%20a%20function%20of%20the%0Aforward%20process%2C%20underlining%20theoretical%20alignment%20with%20score-based%20generative%0Amodels.%20We%20establish%20convergence%20bounds%20for%20the%20algorithm%20under%20minimal%0Aassumptions%2C%20ensuring%20robustness%20and%20efficiency%2C%20which%20we%20demonstrate%20through%0Aexperiments%20on%20low-dimensional%20Bernoulli-distributed%20datasets%20and%0Ahigh-dimensional%20binary%20MNIST%20data.%20The%20results%20highlight%20competitive%0Aperformance%20in%20generating%20discrete%20structures%20compared%20to%20the%20state-of-the-art.%0AThis%20work%20bridges%20theoretical%20foundations%20and%20practical%20applications%2C%20advancing%0Athe%20development%20of%20effective%20and%20theoretically%20grounded%20discrete%20generative%0Amodeling.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2502.07939v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBit-Level%2520Discrete%2520Diffusion%2520with%2520Markov%2520Probabilistic%2520Models%253A%2520An%250A%2520%2520Improved%2520Framework%2520with%2520Sharp%2520Convergence%2520Bounds%2520under%2520Minimal%2520Assumptions%26entry.906535625%3DLe-Tuyet-Nhi%2520Pham%2520and%2520Dario%2520Shariatian%2520and%2520Antonio%2520Ocello%2520and%2520Giovanni%2520Conforti%2520and%2520Alain%2520Durmus%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520Discrete%2520Markov%2520Probabilistic%2520Models%2520%2528DMPMs%2529%252C%2520a%2520novel%250Adiscrete%2520diffusion%2520algorithm%2520for%2520discrete%2520data%2520generation.%2520The%2520algorithm%250Aoperates%2520in%2520discrete%2520bit%2520space%252C%2520where%2520the%2520noising%2520process%2520is%2520a%2520continuous-time%250AMarkov%2520chain%2520that%2520flips%2520labels%2520uniformly%2520at%2520random.%2520The%2520time-reversal%2520process%252C%250Alike%2520the%2520forward%2520noise%2520process%252C%2520is%2520a%2520jump%2520process%2520with%2520its%2520intensity%2520governed%250Aby%2520a%2520discrete%2520analogue%2520of%2520the%2520classical%2520score%2520function.%2520Crucially%252C%2520this%250Aintensity%2520is%2520proven%2520to%2520be%2520the%2520conditional%2520expectation%2520of%2520a%2520function%2520of%2520the%250Aforward%2520process%252C%2520underlining%2520theoretical%2520alignment%2520with%2520score-based%2520generative%250Amodels.%2520We%2520establish%2520convergence%2520bounds%2520for%2520the%2520algorithm%2520under%2520minimal%250Aassumptions%252C%2520ensuring%2520robustness%2520and%2520efficiency%252C%2520which%2520we%2520demonstrate%2520through%250Aexperiments%2520on%2520low-dimensional%2520Bernoulli-distributed%2520datasets%2520and%250Ahigh-dimensional%2520binary%2520MNIST%2520data.%2520The%2520results%2520highlight%2520competitive%250Aperformance%2520in%2520generating%2520discrete%2520structures%2520compared%2520to%2520the%2520state-of-the-art.%250AThis%2520work%2520bridges%2520theoretical%2520foundations%2520and%2520practical%2520applications%252C%2520advancing%250Athe%2520development%2520of%2520effective%2520and%2520theoretically%2520grounded%2520discrete%2520generative%250Amodeling.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.07939v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bit-Level%20Discrete%20Diffusion%20with%20Markov%20Probabilistic%20Models%3A%20An%0A%20%20Improved%20Framework%20with%20Sharp%20Convergence%20Bounds%20under%20Minimal%20Assumptions&entry.906535625=Le-Tuyet-Nhi%20Pham%20and%20Dario%20Shariatian%20and%20Antonio%20Ocello%20and%20Giovanni%20Conforti%20and%20Alain%20Durmus&entry.1292438233=%20%20This%20paper%20introduces%20Discrete%20Markov%20Probabilistic%20Models%20%28DMPMs%29%2C%20a%20novel%0Adiscrete%20diffusion%20algorithm%20for%20discrete%20data%20generation.%20The%20algorithm%0Aoperates%20in%20discrete%20bit%20space%2C%20where%20the%20noising%20process%20is%20a%20continuous-time%0AMarkov%20chain%20that%20flips%20labels%20uniformly%20at%20random.%20The%20time-reversal%20process%2C%0Alike%20the%20forward%20noise%20process%2C%20is%20a%20jump%20process%20with%20its%20intensity%20governed%0Aby%20a%20discrete%20analogue%20of%20the%20classical%20score%20function.%20Crucially%2C%20this%0Aintensity%20is%20proven%20to%20be%20the%20conditional%20expectation%20of%20a%20function%20of%20the%0Aforward%20process%2C%20underlining%20theoretical%20alignment%20with%20score-based%20generative%0Amodels.%20We%20establish%20convergence%20bounds%20for%20the%20algorithm%20under%20minimal%0Aassumptions%2C%20ensuring%20robustness%20and%20efficiency%2C%20which%20we%20demonstrate%20through%0Aexperiments%20on%20low-dimensional%20Bernoulli-distributed%20datasets%20and%0Ahigh-dimensional%20binary%20MNIST%20data.%20The%20results%20highlight%20competitive%0Aperformance%20in%20generating%20discrete%20structures%20compared%20to%20the%20state-of-the-art.%0AThis%20work%20bridges%20theoretical%20foundations%20and%20practical%20applications%2C%20advancing%0Athe%20development%20of%20effective%20and%20theoretically%20grounded%20discrete%20generative%0Amodeling.%0A&entry.1838667208=http%3A//arxiv.org/abs/2502.07939v2&entry.124074799=Read"},
{"title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science", "author": "Yixin Ou and Yujie Luo and Jingsheng Zheng and Lanning Wei and Zhuoyun Yu and Shuofei Qiao and Jintian Zhang and Da Zheng and Yuren Mao and Yunjun Gao and Huajun Chen and Ningyu Zhang", "abstract": "  Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex, innovative tasks. In this\nwork, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework\nthat overcomes these deficiencies through three key advances: (1) a curated\nexpert knowledge base that grounds the agent in domain expert knowledge, (2) an\nagentic knowledgeable tree search algorithm that strategically explores\npossible solutions, and (3) a self-adaptive coding strategy that dynamically\ntailors code generation to task complexity. Evaluations on two automated data\nscience benchmarks demonstrate that AutoMind delivers superior performance\nversus state-of-the-art baselines. Additional analyses confirm favorable\neffectiveness, efficiency, and qualitative solution quality, highlighting\nAutoMind as an efficient and robust step toward fully automated data science.\nCode is at https://github.com/innovatingAI/AutoMind.\n", "link": "http://arxiv.org/abs/2506.10974v3", "date": "2025-10-08", "relevancy": 1.6044, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.542}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5283}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5234}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AutoMind%3A%20Adaptive%20Knowledgeable%20Agent%20for%20Automated%20Data%20Science&body=Title%3A%20AutoMind%3A%20Adaptive%20Knowledgeable%20Agent%20for%20Automated%20Data%20Science%0AAuthor%3A%20Yixin%20Ou%20and%20Yujie%20Luo%20and%20Jingsheng%20Zheng%20and%20Lanning%20Wei%20and%20Zhuoyun%20Yu%20and%20Shuofei%20Qiao%20and%20Jintian%20Zhang%20and%20Da%20Zheng%20and%20Yuren%20Mao%20and%20Yunjun%20Gao%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang%0AAbstract%3A%20%20%20Large%20Language%20Model%20%28LLM%29%20agents%20have%20shown%20great%20potential%20in%20addressing%0Areal-world%20data%20science%20problems.%20LLM-driven%20data%20science%20agents%20promise%20to%0Aautomate%20the%20entire%20machine%20learning%20pipeline%2C%20yet%20their%20real-world%0Aeffectiveness%20remains%20limited.%20Existing%20frameworks%20depend%20on%20rigid%2C%20pre-defined%0Aworkflows%20and%20inflexible%20coding%20strategies%3B%20consequently%2C%20they%20excel%20only%20on%0Arelatively%20simple%2C%20classical%20problems%20and%20fail%20to%20capture%20the%20empirical%0Aexpertise%20that%20human%20practitioners%20bring%20to%20complex%2C%20innovative%20tasks.%20In%20this%0Awork%2C%20we%20introduce%20AutoMind%2C%20an%20adaptive%2C%20knowledgeable%20LLM-agent%20framework%0Athat%20overcomes%20these%20deficiencies%20through%20three%20key%20advances%3A%20%281%29%20a%20curated%0Aexpert%20knowledge%20base%20that%20grounds%20the%20agent%20in%20domain%20expert%20knowledge%2C%20%282%29%20an%0Aagentic%20knowledgeable%20tree%20search%20algorithm%20that%20strategically%20explores%0Apossible%20solutions%2C%20and%20%283%29%20a%20self-adaptive%20coding%20strategy%20that%20dynamically%0Atailors%20code%20generation%20to%20task%20complexity.%20Evaluations%20on%20two%20automated%20data%0Ascience%20benchmarks%20demonstrate%20that%20AutoMind%20delivers%20superior%20performance%0Aversus%20state-of-the-art%20baselines.%20Additional%20analyses%20confirm%20favorable%0Aeffectiveness%2C%20efficiency%2C%20and%20qualitative%20solution%20quality%2C%20highlighting%0AAutoMind%20as%20an%20efficient%20and%20robust%20step%20toward%20fully%20automated%20data%20science.%0ACode%20is%20at%20https%3A//github.com/innovatingAI/AutoMind.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2506.10974v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutoMind%253A%2520Adaptive%2520Knowledgeable%2520Agent%2520for%2520Automated%2520Data%2520Science%26entry.906535625%3DYixin%2520Ou%2520and%2520Yujie%2520Luo%2520and%2520Jingsheng%2520Zheng%2520and%2520Lanning%2520Wei%2520and%2520Zhuoyun%2520Yu%2520and%2520Shuofei%2520Qiao%2520and%2520Jintian%2520Zhang%2520and%2520Da%2520Zheng%2520and%2520Yuren%2520Mao%2520and%2520Yunjun%2520Gao%2520and%2520Huajun%2520Chen%2520and%2520Ningyu%2520Zhang%26entry.1292438233%3D%2520%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520agents%2520have%2520shown%2520great%2520potential%2520in%2520addressing%250Areal-world%2520data%2520science%2520problems.%2520LLM-driven%2520data%2520science%2520agents%2520promise%2520to%250Aautomate%2520the%2520entire%2520machine%2520learning%2520pipeline%252C%2520yet%2520their%2520real-world%250Aeffectiveness%2520remains%2520limited.%2520Existing%2520frameworks%2520depend%2520on%2520rigid%252C%2520pre-defined%250Aworkflows%2520and%2520inflexible%2520coding%2520strategies%253B%2520consequently%252C%2520they%2520excel%2520only%2520on%250Arelatively%2520simple%252C%2520classical%2520problems%2520and%2520fail%2520to%2520capture%2520the%2520empirical%250Aexpertise%2520that%2520human%2520practitioners%2520bring%2520to%2520complex%252C%2520innovative%2520tasks.%2520In%2520this%250Awork%252C%2520we%2520introduce%2520AutoMind%252C%2520an%2520adaptive%252C%2520knowledgeable%2520LLM-agent%2520framework%250Athat%2520overcomes%2520these%2520deficiencies%2520through%2520three%2520key%2520advances%253A%2520%25281%2529%2520a%2520curated%250Aexpert%2520knowledge%2520base%2520that%2520grounds%2520the%2520agent%2520in%2520domain%2520expert%2520knowledge%252C%2520%25282%2529%2520an%250Aagentic%2520knowledgeable%2520tree%2520search%2520algorithm%2520that%2520strategically%2520explores%250Apossible%2520solutions%252C%2520and%2520%25283%2529%2520a%2520self-adaptive%2520coding%2520strategy%2520that%2520dynamically%250Atailors%2520code%2520generation%2520to%2520task%2520complexity.%2520Evaluations%2520on%2520two%2520automated%2520data%250Ascience%2520benchmarks%2520demonstrate%2520that%2520AutoMind%2520delivers%2520superior%2520performance%250Aversus%2520state-of-the-art%2520baselines.%2520Additional%2520analyses%2520confirm%2520favorable%250Aeffectiveness%252C%2520efficiency%252C%2520and%2520qualitative%2520solution%2520quality%252C%2520highlighting%250AAutoMind%2520as%2520an%2520efficient%2520and%2520robust%2520step%2520toward%2520fully%2520automated%2520data%2520science.%250ACode%2520is%2520at%2520https%253A//github.com/innovatingAI/AutoMind.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.10974v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AutoMind%3A%20Adaptive%20Knowledgeable%20Agent%20for%20Automated%20Data%20Science&entry.906535625=Yixin%20Ou%20and%20Yujie%20Luo%20and%20Jingsheng%20Zheng%20and%20Lanning%20Wei%20and%20Zhuoyun%20Yu%20and%20Shuofei%20Qiao%20and%20Jintian%20Zhang%20and%20Da%20Zheng%20and%20Yuren%20Mao%20and%20Yunjun%20Gao%20and%20Huajun%20Chen%20and%20Ningyu%20Zhang&entry.1292438233=%20%20Large%20Language%20Model%20%28LLM%29%20agents%20have%20shown%20great%20potential%20in%20addressing%0Areal-world%20data%20science%20problems.%20LLM-driven%20data%20science%20agents%20promise%20to%0Aautomate%20the%20entire%20machine%20learning%20pipeline%2C%20yet%20their%20real-world%0Aeffectiveness%20remains%20limited.%20Existing%20frameworks%20depend%20on%20rigid%2C%20pre-defined%0Aworkflows%20and%20inflexible%20coding%20strategies%3B%20consequently%2C%20they%20excel%20only%20on%0Arelatively%20simple%2C%20classical%20problems%20and%20fail%20to%20capture%20the%20empirical%0Aexpertise%20that%20human%20practitioners%20bring%20to%20complex%2C%20innovative%20tasks.%20In%20this%0Awork%2C%20we%20introduce%20AutoMind%2C%20an%20adaptive%2C%20knowledgeable%20LLM-agent%20framework%0Athat%20overcomes%20these%20deficiencies%20through%20three%20key%20advances%3A%20%281%29%20a%20curated%0Aexpert%20knowledge%20base%20that%20grounds%20the%20agent%20in%20domain%20expert%20knowledge%2C%20%282%29%20an%0Aagentic%20knowledgeable%20tree%20search%20algorithm%20that%20strategically%20explores%0Apossible%20solutions%2C%20and%20%283%29%20a%20self-adaptive%20coding%20strategy%20that%20dynamically%0Atailors%20code%20generation%20to%20task%20complexity.%20Evaluations%20on%20two%20automated%20data%0Ascience%20benchmarks%20demonstrate%20that%20AutoMind%20delivers%20superior%20performance%0Aversus%20state-of-the-art%20baselines.%20Additional%20analyses%20confirm%20favorable%0Aeffectiveness%2C%20efficiency%2C%20and%20qualitative%20solution%20quality%2C%20highlighting%0AAutoMind%20as%20an%20efficient%20and%20robust%20step%20toward%20fully%20automated%20data%20science.%0ACode%20is%20at%20https%3A//github.com/innovatingAI/AutoMind.%0A&entry.1838667208=http%3A//arxiv.org/abs/2506.10974v3&entry.124074799=Read"},
{"title": "On the false election between regulation and innovation. Ideas for\n  regulation through the responsible use of artificial intelligence in research\n  and education.[Spanish version]", "author": "Pompeu Casanovas", "abstract": "  This short essay is a reworking of the answers offered by the author at the\nDebate Session of the AIHUB (CSIC) and EduCaixa Summer School, organized by\nMarta Garcia-Matos and Lissette Lemus, and coordinated by Albert Sabater\n(OEIAC, UG), with the participation of Vanina Martinez-Posse (IIIA-CSIC),\nEulalia Soler (Eurecat) and Pompeu Casanovas (IIIA-CSIC) on July 4th 2025.\nAlbert Sabater posed three questions: (1) How can regulatory frameworks\npriori-tise the protection of fundamental rights (privacy, non-discrimination,\nautonomy, etc.) in the development of AI, without falling into the false\ndichotomy between regulation and innova-tion? (2) Given the risks of AI (bias,\nmass surveillance, manipulation), what examples of regu-lations or policies\nhave demonstrated that it is possible to foster responsible innovation, putting\nthe public interest before profitability, without giving in to competitive\npressure from actors such as China or the US? (3) In a scenario where the US\nprioritizes flexibility, what mecha-nisms could ensure that international\ncooperation in AI does not become a race to the bottom in rights, but rather a\nglobal standard of accountability? The article attempts to answer these three\nquestions and concludes with some reflections on the relevance of the answers\nfor education and research.\n", "link": "http://arxiv.org/abs/2510.07268v1", "date": "2025-10-08", "relevancy": 1.5492, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4041}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.3906}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3772}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20false%20election%20between%20regulation%20and%20innovation.%20Ideas%20for%0A%20%20regulation%20through%20the%20responsible%20use%20of%20artificial%20intelligence%20in%20research%0A%20%20and%20education.%5BSpanish%20version%5D&body=Title%3A%20On%20the%20false%20election%20between%20regulation%20and%20innovation.%20Ideas%20for%0A%20%20regulation%20through%20the%20responsible%20use%20of%20artificial%20intelligence%20in%20research%0A%20%20and%20education.%5BSpanish%20version%5D%0AAuthor%3A%20Pompeu%20Casanovas%0AAbstract%3A%20%20%20This%20short%20essay%20is%20a%20reworking%20of%20the%20answers%20offered%20by%20the%20author%20at%20the%0ADebate%20Session%20of%20the%20AIHUB%20%28CSIC%29%20and%20EduCaixa%20Summer%20School%2C%20organized%20by%0AMarta%20Garcia-Matos%20and%20Lissette%20Lemus%2C%20and%20coordinated%20by%20Albert%20Sabater%0A%28OEIAC%2C%20UG%29%2C%20with%20the%20participation%20of%20Vanina%20Martinez-Posse%20%28IIIA-CSIC%29%2C%0AEulalia%20Soler%20%28Eurecat%29%20and%20Pompeu%20Casanovas%20%28IIIA-CSIC%29%20on%20July%204th%202025.%0AAlbert%20Sabater%20posed%20three%20questions%3A%20%281%29%20How%20can%20regulatory%20frameworks%0Apriori-tise%20the%20protection%20of%20fundamental%20rights%20%28privacy%2C%20non-discrimination%2C%0Aautonomy%2C%20etc.%29%20in%20the%20development%20of%20AI%2C%20without%20falling%20into%20the%20false%0Adichotomy%20between%20regulation%20and%20innova-tion%3F%20%282%29%20Given%20the%20risks%20of%20AI%20%28bias%2C%0Amass%20surveillance%2C%20manipulation%29%2C%20what%20examples%20of%20regu-lations%20or%20policies%0Ahave%20demonstrated%20that%20it%20is%20possible%20to%20foster%20responsible%20innovation%2C%20putting%0Athe%20public%20interest%20before%20profitability%2C%20without%20giving%20in%20to%20competitive%0Apressure%20from%20actors%20such%20as%20China%20or%20the%20US%3F%20%283%29%20In%20a%20scenario%20where%20the%20US%0Aprioritizes%20flexibility%2C%20what%20mecha-nisms%20could%20ensure%20that%20international%0Acooperation%20in%20AI%20does%20not%20become%20a%20race%20to%20the%20bottom%20in%20rights%2C%20but%20rather%20a%0Aglobal%20standard%20of%20accountability%3F%20The%20article%20attempts%20to%20answer%20these%20three%0Aquestions%20and%20concludes%20with%20some%20reflections%20on%20the%20relevance%20of%20the%20answers%0Afor%20education%20and%20research.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07268v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520false%2520election%2520between%2520regulation%2520and%2520innovation.%2520Ideas%2520for%250A%2520%2520regulation%2520through%2520the%2520responsible%2520use%2520of%2520artificial%2520intelligence%2520in%2520research%250A%2520%2520and%2520education.%255BSpanish%2520version%255D%26entry.906535625%3DPompeu%2520Casanovas%26entry.1292438233%3D%2520%2520This%2520short%2520essay%2520is%2520a%2520reworking%2520of%2520the%2520answers%2520offered%2520by%2520the%2520author%2520at%2520the%250ADebate%2520Session%2520of%2520the%2520AIHUB%2520%2528CSIC%2529%2520and%2520EduCaixa%2520Summer%2520School%252C%2520organized%2520by%250AMarta%2520Garcia-Matos%2520and%2520Lissette%2520Lemus%252C%2520and%2520coordinated%2520by%2520Albert%2520Sabater%250A%2528OEIAC%252C%2520UG%2529%252C%2520with%2520the%2520participation%2520of%2520Vanina%2520Martinez-Posse%2520%2528IIIA-CSIC%2529%252C%250AEulalia%2520Soler%2520%2528Eurecat%2529%2520and%2520Pompeu%2520Casanovas%2520%2528IIIA-CSIC%2529%2520on%2520July%25204th%25202025.%250AAlbert%2520Sabater%2520posed%2520three%2520questions%253A%2520%25281%2529%2520How%2520can%2520regulatory%2520frameworks%250Apriori-tise%2520the%2520protection%2520of%2520fundamental%2520rights%2520%2528privacy%252C%2520non-discrimination%252C%250Aautonomy%252C%2520etc.%2529%2520in%2520the%2520development%2520of%2520AI%252C%2520without%2520falling%2520into%2520the%2520false%250Adichotomy%2520between%2520regulation%2520and%2520innova-tion%253F%2520%25282%2529%2520Given%2520the%2520risks%2520of%2520AI%2520%2528bias%252C%250Amass%2520surveillance%252C%2520manipulation%2529%252C%2520what%2520examples%2520of%2520regu-lations%2520or%2520policies%250Ahave%2520demonstrated%2520that%2520it%2520is%2520possible%2520to%2520foster%2520responsible%2520innovation%252C%2520putting%250Athe%2520public%2520interest%2520before%2520profitability%252C%2520without%2520giving%2520in%2520to%2520competitive%250Apressure%2520from%2520actors%2520such%2520as%2520China%2520or%2520the%2520US%253F%2520%25283%2529%2520In%2520a%2520scenario%2520where%2520the%2520US%250Aprioritizes%2520flexibility%252C%2520what%2520mecha-nisms%2520could%2520ensure%2520that%2520international%250Acooperation%2520in%2520AI%2520does%2520not%2520become%2520a%2520race%2520to%2520the%2520bottom%2520in%2520rights%252C%2520but%2520rather%2520a%250Aglobal%2520standard%2520of%2520accountability%253F%2520The%2520article%2520attempts%2520to%2520answer%2520these%2520three%250Aquestions%2520and%2520concludes%2520with%2520some%2520reflections%2520on%2520the%2520relevance%2520of%2520the%2520answers%250Afor%2520education%2520and%2520research.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07268v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20false%20election%20between%20regulation%20and%20innovation.%20Ideas%20for%0A%20%20regulation%20through%20the%20responsible%20use%20of%20artificial%20intelligence%20in%20research%0A%20%20and%20education.%5BSpanish%20version%5D&entry.906535625=Pompeu%20Casanovas&entry.1292438233=%20%20This%20short%20essay%20is%20a%20reworking%20of%20the%20answers%20offered%20by%20the%20author%20at%20the%0ADebate%20Session%20of%20the%20AIHUB%20%28CSIC%29%20and%20EduCaixa%20Summer%20School%2C%20organized%20by%0AMarta%20Garcia-Matos%20and%20Lissette%20Lemus%2C%20and%20coordinated%20by%20Albert%20Sabater%0A%28OEIAC%2C%20UG%29%2C%20with%20the%20participation%20of%20Vanina%20Martinez-Posse%20%28IIIA-CSIC%29%2C%0AEulalia%20Soler%20%28Eurecat%29%20and%20Pompeu%20Casanovas%20%28IIIA-CSIC%29%20on%20July%204th%202025.%0AAlbert%20Sabater%20posed%20three%20questions%3A%20%281%29%20How%20can%20regulatory%20frameworks%0Apriori-tise%20the%20protection%20of%20fundamental%20rights%20%28privacy%2C%20non-discrimination%2C%0Aautonomy%2C%20etc.%29%20in%20the%20development%20of%20AI%2C%20without%20falling%20into%20the%20false%0Adichotomy%20between%20regulation%20and%20innova-tion%3F%20%282%29%20Given%20the%20risks%20of%20AI%20%28bias%2C%0Amass%20surveillance%2C%20manipulation%29%2C%20what%20examples%20of%20regu-lations%20or%20policies%0Ahave%20demonstrated%20that%20it%20is%20possible%20to%20foster%20responsible%20innovation%2C%20putting%0Athe%20public%20interest%20before%20profitability%2C%20without%20giving%20in%20to%20competitive%0Apressure%20from%20actors%20such%20as%20China%20or%20the%20US%3F%20%283%29%20In%20a%20scenario%20where%20the%20US%0Aprioritizes%20flexibility%2C%20what%20mecha-nisms%20could%20ensure%20that%20international%0Acooperation%20in%20AI%20does%20not%20become%20a%20race%20to%20the%20bottom%20in%20rights%2C%20but%20rather%20a%0Aglobal%20standard%20of%20accountability%3F%20The%20article%20attempts%20to%20answer%20these%20three%0Aquestions%20and%20concludes%20with%20some%20reflections%20on%20the%20relevance%20of%20the%20answers%0Afor%20education%20and%20research.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07268v1&entry.124074799=Read"},
{"title": "Cocoon: A System Architecture for Differentially Private Training with\n  Correlated Noises", "author": "Donghwan Kim and Xin Gu and Jinho Baek and Timothy Lo and Younghoon Min and Kwangsik Shin and Jongryool Kim and Jongse Park and Kiwan Maeng", "abstract": "  Machine learning (ML) models memorize and leak training data, causing serious\nprivacy issues to data owners. Training algorithms with differential privacy\n(DP), such as DP-SGD, have been gaining attention as a solution. However,\nDP-SGD adds a noise at each training iteration, which degrades the accuracy of\nthe trained model. To improve accuracy, a new family of approaches adds\ncarefully designed correlated noises, so that noises cancel out each other\nacross iterations. We performed an extensive characterization study of these\nnew mechanisms, for the first time to the best of our knowledge, and show they\nincur non-negligible overheads when the model is large or uses large embedding\ntables. Motivated by the analysis, we propose Cocoon, a hardware-software\nco-designed framework for efficient training with correlated noises. Cocoon\naccelerates models with embedding tables through pre-computing and storing\ncorrelated noises in a coalesced format (Cocoon-Emb), and supports large models\nthrough a custom near-memory processing device (Cocoon-NMP). On a real system\nwith an FPGA-based NMP device prototype, Cocoon improves the performance by\n2.33-10.82x(Cocoon-Emb) and 1.55-3.06x (Cocoon-NMP).\n", "link": "http://arxiv.org/abs/2510.07304v1", "date": "2025-10-08", "relevancy": 1.4999, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5297}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.497}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cocoon%3A%20A%20System%20Architecture%20for%20Differentially%20Private%20Training%20with%0A%20%20Correlated%20Noises&body=Title%3A%20Cocoon%3A%20A%20System%20Architecture%20for%20Differentially%20Private%20Training%20with%0A%20%20Correlated%20Noises%0AAuthor%3A%20Donghwan%20Kim%20and%20Xin%20Gu%20and%20Jinho%20Baek%20and%20Timothy%20Lo%20and%20Younghoon%20Min%20and%20Kwangsik%20Shin%20and%20Jongryool%20Kim%20and%20Jongse%20Park%20and%20Kiwan%20Maeng%0AAbstract%3A%20%20%20Machine%20learning%20%28ML%29%20models%20memorize%20and%20leak%20training%20data%2C%20causing%20serious%0Aprivacy%20issues%20to%20data%20owners.%20Training%20algorithms%20with%20differential%20privacy%0A%28DP%29%2C%20such%20as%20DP-SGD%2C%20have%20been%20gaining%20attention%20as%20a%20solution.%20However%2C%0ADP-SGD%20adds%20a%20noise%20at%20each%20training%20iteration%2C%20which%20degrades%20the%20accuracy%20of%0Athe%20trained%20model.%20To%20improve%20accuracy%2C%20a%20new%20family%20of%20approaches%20adds%0Acarefully%20designed%20correlated%20noises%2C%20so%20that%20noises%20cancel%20out%20each%20other%0Aacross%20iterations.%20We%20performed%20an%20extensive%20characterization%20study%20of%20these%0Anew%20mechanisms%2C%20for%20the%20first%20time%20to%20the%20best%20of%20our%20knowledge%2C%20and%20show%20they%0Aincur%20non-negligible%20overheads%20when%20the%20model%20is%20large%20or%20uses%20large%20embedding%0Atables.%20Motivated%20by%20the%20analysis%2C%20we%20propose%20Cocoon%2C%20a%20hardware-software%0Aco-designed%20framework%20for%20efficient%20training%20with%20correlated%20noises.%20Cocoon%0Aaccelerates%20models%20with%20embedding%20tables%20through%20pre-computing%20and%20storing%0Acorrelated%20noises%20in%20a%20coalesced%20format%20%28Cocoon-Emb%29%2C%20and%20supports%20large%20models%0Athrough%20a%20custom%20near-memory%20processing%20device%20%28Cocoon-NMP%29.%20On%20a%20real%20system%0Awith%20an%20FPGA-based%20NMP%20device%20prototype%2C%20Cocoon%20improves%20the%20performance%20by%0A2.33-10.82x%28Cocoon-Emb%29%20and%201.55-3.06x%20%28Cocoon-NMP%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07304v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCocoon%253A%2520A%2520System%2520Architecture%2520for%2520Differentially%2520Private%2520Training%2520with%250A%2520%2520Correlated%2520Noises%26entry.906535625%3DDonghwan%2520Kim%2520and%2520Xin%2520Gu%2520and%2520Jinho%2520Baek%2520and%2520Timothy%2520Lo%2520and%2520Younghoon%2520Min%2520and%2520Kwangsik%2520Shin%2520and%2520Jongryool%2520Kim%2520and%2520Jongse%2520Park%2520and%2520Kiwan%2520Maeng%26entry.1292438233%3D%2520%2520Machine%2520learning%2520%2528ML%2529%2520models%2520memorize%2520and%2520leak%2520training%2520data%252C%2520causing%2520serious%250Aprivacy%2520issues%2520to%2520data%2520owners.%2520Training%2520algorithms%2520with%2520differential%2520privacy%250A%2528DP%2529%252C%2520such%2520as%2520DP-SGD%252C%2520have%2520been%2520gaining%2520attention%2520as%2520a%2520solution.%2520However%252C%250ADP-SGD%2520adds%2520a%2520noise%2520at%2520each%2520training%2520iteration%252C%2520which%2520degrades%2520the%2520accuracy%2520of%250Athe%2520trained%2520model.%2520To%2520improve%2520accuracy%252C%2520a%2520new%2520family%2520of%2520approaches%2520adds%250Acarefully%2520designed%2520correlated%2520noises%252C%2520so%2520that%2520noises%2520cancel%2520out%2520each%2520other%250Aacross%2520iterations.%2520We%2520performed%2520an%2520extensive%2520characterization%2520study%2520of%2520these%250Anew%2520mechanisms%252C%2520for%2520the%2520first%2520time%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520and%2520show%2520they%250Aincur%2520non-negligible%2520overheads%2520when%2520the%2520model%2520is%2520large%2520or%2520uses%2520large%2520embedding%250Atables.%2520Motivated%2520by%2520the%2520analysis%252C%2520we%2520propose%2520Cocoon%252C%2520a%2520hardware-software%250Aco-designed%2520framework%2520for%2520efficient%2520training%2520with%2520correlated%2520noises.%2520Cocoon%250Aaccelerates%2520models%2520with%2520embedding%2520tables%2520through%2520pre-computing%2520and%2520storing%250Acorrelated%2520noises%2520in%2520a%2520coalesced%2520format%2520%2528Cocoon-Emb%2529%252C%2520and%2520supports%2520large%2520models%250Athrough%2520a%2520custom%2520near-memory%2520processing%2520device%2520%2528Cocoon-NMP%2529.%2520On%2520a%2520real%2520system%250Awith%2520an%2520FPGA-based%2520NMP%2520device%2520prototype%252C%2520Cocoon%2520improves%2520the%2520performance%2520by%250A2.33-10.82x%2528Cocoon-Emb%2529%2520and%25201.55-3.06x%2520%2528Cocoon-NMP%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07304v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cocoon%3A%20A%20System%20Architecture%20for%20Differentially%20Private%20Training%20with%0A%20%20Correlated%20Noises&entry.906535625=Donghwan%20Kim%20and%20Xin%20Gu%20and%20Jinho%20Baek%20and%20Timothy%20Lo%20and%20Younghoon%20Min%20and%20Kwangsik%20Shin%20and%20Jongryool%20Kim%20and%20Jongse%20Park%20and%20Kiwan%20Maeng&entry.1292438233=%20%20Machine%20learning%20%28ML%29%20models%20memorize%20and%20leak%20training%20data%2C%20causing%20serious%0Aprivacy%20issues%20to%20data%20owners.%20Training%20algorithms%20with%20differential%20privacy%0A%28DP%29%2C%20such%20as%20DP-SGD%2C%20have%20been%20gaining%20attention%20as%20a%20solution.%20However%2C%0ADP-SGD%20adds%20a%20noise%20at%20each%20training%20iteration%2C%20which%20degrades%20the%20accuracy%20of%0Athe%20trained%20model.%20To%20improve%20accuracy%2C%20a%20new%20family%20of%20approaches%20adds%0Acarefully%20designed%20correlated%20noises%2C%20so%20that%20noises%20cancel%20out%20each%20other%0Aacross%20iterations.%20We%20performed%20an%20extensive%20characterization%20study%20of%20these%0Anew%20mechanisms%2C%20for%20the%20first%20time%20to%20the%20best%20of%20our%20knowledge%2C%20and%20show%20they%0Aincur%20non-negligible%20overheads%20when%20the%20model%20is%20large%20or%20uses%20large%20embedding%0Atables.%20Motivated%20by%20the%20analysis%2C%20we%20propose%20Cocoon%2C%20a%20hardware-software%0Aco-designed%20framework%20for%20efficient%20training%20with%20correlated%20noises.%20Cocoon%0Aaccelerates%20models%20with%20embedding%20tables%20through%20pre-computing%20and%20storing%0Acorrelated%20noises%20in%20a%20coalesced%20format%20%28Cocoon-Emb%29%2C%20and%20supports%20large%20models%0Athrough%20a%20custom%20near-memory%20processing%20device%20%28Cocoon-NMP%29.%20On%20a%20real%20system%0Awith%20an%20FPGA-based%20NMP%20device%20prototype%2C%20Cocoon%20improves%20the%20performance%20by%0A2.33-10.82x%28Cocoon-Emb%29%20and%201.55-3.06x%20%28Cocoon-NMP%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07304v1&entry.124074799=Read"},
{"title": "MLE-Smith: Scaling MLE Tasks with Automated Multi-Agent Pipeline", "author": "Rushi Qiang and Yuchen Zhuang and Anikait Singh and Percy Liang and Chao Zhang and Sherry Yang and Bo Dai", "abstract": "  While Language Models (LMs) have made significant progress in automating\nmachine learning engineering (MLE), the acquisition of high-quality MLE\ntraining data is significantly constrained. Current MLE benchmarks suffer from\nlow scalability and limited applicability because they rely on static, manually\ncurated tasks, demanding extensive time and manual effort to produce. We\nintroduce MLE-Smith, a fully automated multi-agent pipeline, to transform raw\ndatasets into competition-style MLE challenges through an efficient\ngenerate-verify-execute paradigm for scaling MLE tasks with verifiable quality,\nreal-world usability, and rich diversity. The proposed multi-agent pipeline in\nMLE-Smith drives structured task design and standardized refactoring, coupled\nwith a hybrid verification mechanism that enforces strict structural rules and\nhigh-level semantic soundness. It further validates empirical solvability and\nreal-world fidelity through interactive execution. We apply MLE-Smith to 224 of\nreal-world datasets and generate 606 tasks spanning multiple categories,\nobjectives, and modalities, demonstrating that MLE-Smith can work effectively\nacross a wide range of real-world datasets. Evaluation on the generated tasks\nshows that the performance of eight mainstream and cutting-edge LLMs on\nMLE-Smith tasks is strongly correlated with their performance on carefully\nhuman-designed tasks, highlighting the effectiveness of the MLE-Smith to\nscaling up MLE tasks, while maintaining task quality.\n", "link": "http://arxiv.org/abs/2510.07307v1", "date": "2025-10-08", "relevancy": 1.4956, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5076}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4981}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4906}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MLE-Smith%3A%20Scaling%20MLE%20Tasks%20with%20Automated%20Multi-Agent%20Pipeline&body=Title%3A%20MLE-Smith%3A%20Scaling%20MLE%20Tasks%20with%20Automated%20Multi-Agent%20Pipeline%0AAuthor%3A%20Rushi%20Qiang%20and%20Yuchen%20Zhuang%20and%20Anikait%20Singh%20and%20Percy%20Liang%20and%20Chao%20Zhang%20and%20Sherry%20Yang%20and%20Bo%20Dai%0AAbstract%3A%20%20%20While%20Language%20Models%20%28LMs%29%20have%20made%20significant%20progress%20in%20automating%0Amachine%20learning%20engineering%20%28MLE%29%2C%20the%20acquisition%20of%20high-quality%20MLE%0Atraining%20data%20is%20significantly%20constrained.%20Current%20MLE%20benchmarks%20suffer%20from%0Alow%20scalability%20and%20limited%20applicability%20because%20they%20rely%20on%20static%2C%20manually%0Acurated%20tasks%2C%20demanding%20extensive%20time%20and%20manual%20effort%20to%20produce.%20We%0Aintroduce%20MLE-Smith%2C%20a%20fully%20automated%20multi-agent%20pipeline%2C%20to%20transform%20raw%0Adatasets%20into%20competition-style%20MLE%20challenges%20through%20an%20efficient%0Agenerate-verify-execute%20paradigm%20for%20scaling%20MLE%20tasks%20with%20verifiable%20quality%2C%0Areal-world%20usability%2C%20and%20rich%20diversity.%20The%20proposed%20multi-agent%20pipeline%20in%0AMLE-Smith%20drives%20structured%20task%20design%20and%20standardized%20refactoring%2C%20coupled%0Awith%20a%20hybrid%20verification%20mechanism%20that%20enforces%20strict%20structural%20rules%20and%0Ahigh-level%20semantic%20soundness.%20It%20further%20validates%20empirical%20solvability%20and%0Areal-world%20fidelity%20through%20interactive%20execution.%20We%20apply%20MLE-Smith%20to%20224%20of%0Areal-world%20datasets%20and%20generate%20606%20tasks%20spanning%20multiple%20categories%2C%0Aobjectives%2C%20and%20modalities%2C%20demonstrating%20that%20MLE-Smith%20can%20work%20effectively%0Aacross%20a%20wide%20range%20of%20real-world%20datasets.%20Evaluation%20on%20the%20generated%20tasks%0Ashows%20that%20the%20performance%20of%20eight%20mainstream%20and%20cutting-edge%20LLMs%20on%0AMLE-Smith%20tasks%20is%20strongly%20correlated%20with%20their%20performance%20on%20carefully%0Ahuman-designed%20tasks%2C%20highlighting%20the%20effectiveness%20of%20the%20MLE-Smith%20to%0Ascaling%20up%20MLE%20tasks%2C%20while%20maintaining%20task%20quality.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMLE-Smith%253A%2520Scaling%2520MLE%2520Tasks%2520with%2520Automated%2520Multi-Agent%2520Pipeline%26entry.906535625%3DRushi%2520Qiang%2520and%2520Yuchen%2520Zhuang%2520and%2520Anikait%2520Singh%2520and%2520Percy%2520Liang%2520and%2520Chao%2520Zhang%2520and%2520Sherry%2520Yang%2520and%2520Bo%2520Dai%26entry.1292438233%3D%2520%2520While%2520Language%2520Models%2520%2528LMs%2529%2520have%2520made%2520significant%2520progress%2520in%2520automating%250Amachine%2520learning%2520engineering%2520%2528MLE%2529%252C%2520the%2520acquisition%2520of%2520high-quality%2520MLE%250Atraining%2520data%2520is%2520significantly%2520constrained.%2520Current%2520MLE%2520benchmarks%2520suffer%2520from%250Alow%2520scalability%2520and%2520limited%2520applicability%2520because%2520they%2520rely%2520on%2520static%252C%2520manually%250Acurated%2520tasks%252C%2520demanding%2520extensive%2520time%2520and%2520manual%2520effort%2520to%2520produce.%2520We%250Aintroduce%2520MLE-Smith%252C%2520a%2520fully%2520automated%2520multi-agent%2520pipeline%252C%2520to%2520transform%2520raw%250Adatasets%2520into%2520competition-style%2520MLE%2520challenges%2520through%2520an%2520efficient%250Agenerate-verify-execute%2520paradigm%2520for%2520scaling%2520MLE%2520tasks%2520with%2520verifiable%2520quality%252C%250Areal-world%2520usability%252C%2520and%2520rich%2520diversity.%2520The%2520proposed%2520multi-agent%2520pipeline%2520in%250AMLE-Smith%2520drives%2520structured%2520task%2520design%2520and%2520standardized%2520refactoring%252C%2520coupled%250Awith%2520a%2520hybrid%2520verification%2520mechanism%2520that%2520enforces%2520strict%2520structural%2520rules%2520and%250Ahigh-level%2520semantic%2520soundness.%2520It%2520further%2520validates%2520empirical%2520solvability%2520and%250Areal-world%2520fidelity%2520through%2520interactive%2520execution.%2520We%2520apply%2520MLE-Smith%2520to%2520224%2520of%250Areal-world%2520datasets%2520and%2520generate%2520606%2520tasks%2520spanning%2520multiple%2520categories%252C%250Aobjectives%252C%2520and%2520modalities%252C%2520demonstrating%2520that%2520MLE-Smith%2520can%2520work%2520effectively%250Aacross%2520a%2520wide%2520range%2520of%2520real-world%2520datasets.%2520Evaluation%2520on%2520the%2520generated%2520tasks%250Ashows%2520that%2520the%2520performance%2520of%2520eight%2520mainstream%2520and%2520cutting-edge%2520LLMs%2520on%250AMLE-Smith%2520tasks%2520is%2520strongly%2520correlated%2520with%2520their%2520performance%2520on%2520carefully%250Ahuman-designed%2520tasks%252C%2520highlighting%2520the%2520effectiveness%2520of%2520the%2520MLE-Smith%2520to%250Ascaling%2520up%2520MLE%2520tasks%252C%2520while%2520maintaining%2520task%2520quality.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MLE-Smith%3A%20Scaling%20MLE%20Tasks%20with%20Automated%20Multi-Agent%20Pipeline&entry.906535625=Rushi%20Qiang%20and%20Yuchen%20Zhuang%20and%20Anikait%20Singh%20and%20Percy%20Liang%20and%20Chao%20Zhang%20and%20Sherry%20Yang%20and%20Bo%20Dai&entry.1292438233=%20%20While%20Language%20Models%20%28LMs%29%20have%20made%20significant%20progress%20in%20automating%0Amachine%20learning%20engineering%20%28MLE%29%2C%20the%20acquisition%20of%20high-quality%20MLE%0Atraining%20data%20is%20significantly%20constrained.%20Current%20MLE%20benchmarks%20suffer%20from%0Alow%20scalability%20and%20limited%20applicability%20because%20they%20rely%20on%20static%2C%20manually%0Acurated%20tasks%2C%20demanding%20extensive%20time%20and%20manual%20effort%20to%20produce.%20We%0Aintroduce%20MLE-Smith%2C%20a%20fully%20automated%20multi-agent%20pipeline%2C%20to%20transform%20raw%0Adatasets%20into%20competition-style%20MLE%20challenges%20through%20an%20efficient%0Agenerate-verify-execute%20paradigm%20for%20scaling%20MLE%20tasks%20with%20verifiable%20quality%2C%0Areal-world%20usability%2C%20and%20rich%20diversity.%20The%20proposed%20multi-agent%20pipeline%20in%0AMLE-Smith%20drives%20structured%20task%20design%20and%20standardized%20refactoring%2C%20coupled%0Awith%20a%20hybrid%20verification%20mechanism%20that%20enforces%20strict%20structural%20rules%20and%0Ahigh-level%20semantic%20soundness.%20It%20further%20validates%20empirical%20solvability%20and%0Areal-world%20fidelity%20through%20interactive%20execution.%20We%20apply%20MLE-Smith%20to%20224%20of%0Areal-world%20datasets%20and%20generate%20606%20tasks%20spanning%20multiple%20categories%2C%0Aobjectives%2C%20and%20modalities%2C%20demonstrating%20that%20MLE-Smith%20can%20work%20effectively%0Aacross%20a%20wide%20range%20of%20real-world%20datasets.%20Evaluation%20on%20the%20generated%20tasks%0Ashows%20that%20the%20performance%20of%20eight%20mainstream%20and%20cutting-edge%20LLMs%20on%0AMLE-Smith%20tasks%20is%20strongly%20correlated%20with%20their%20performance%20on%20carefully%0Ahuman-designed%20tasks%2C%20highlighting%20the%20effectiveness%20of%20the%20MLE-Smith%20to%0Ascaling%20up%20MLE%20tasks%2C%20while%20maintaining%20task%20quality.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07307v1&entry.124074799=Read"},
{"title": "Where to Begin: Efficient Pretraining via Subnetwork Selection and\n  Distillation", "author": "Arjun Krishnakumar and Rhea Sanjay Sukthanker and Hannan Javed Mahadik and Gabriela Kadlecov\u00e1 and Vladyslav Moroshan and Timur Carstensen and Frank Hutter and Aaron Klein", "abstract": "  Small Language models (SLMs) offer an efficient and accessible alternative to\nLarge Language Models (LLMs), delivering strong performance while using far\nfewer resources. We introduce a simple and effective framework for pretraining\nSLMs that brings together three complementary ideas. First, we identify\nstructurally sparse sub-network initializations that consistently outperform\nrandomly initialized models of similar size under the same compute budget.\nSecond, we use evolutionary search to automatically discover high-quality\nsub-network initializations, providing better starting points for pretraining.\nThird, we apply knowledge distillation from larger teacher models to speed up\ntraining and improve generalization. Together, these components make SLM\npretraining substantially more efficient: our best model, discovered using\nevolutionary search and initialized with LLM weights, matches the validation\nperplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining\ntokens. We release all code and models at\nhttps://github.com/whittle-org/whittle/, offering a practical and reproducible\npath toward cost-efficient small language model development at scale.\n", "link": "http://arxiv.org/abs/2510.07227v1", "date": "2025-10-08", "relevancy": 1.4874, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4966}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4896}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Where%20to%20Begin%3A%20Efficient%20Pretraining%20via%20Subnetwork%20Selection%20and%0A%20%20Distillation&body=Title%3A%20Where%20to%20Begin%3A%20Efficient%20Pretraining%20via%20Subnetwork%20Selection%20and%0A%20%20Distillation%0AAuthor%3A%20Arjun%20Krishnakumar%20and%20Rhea%20Sanjay%20Sukthanker%20and%20Hannan%20Javed%20Mahadik%20and%20Gabriela%20Kadlecov%C3%A1%20and%20Vladyslav%20Moroshan%20and%20Timur%20Carstensen%20and%20Frank%20Hutter%20and%20Aaron%20Klein%0AAbstract%3A%20%20%20Small%20Language%20models%20%28SLMs%29%20offer%20an%20efficient%20and%20accessible%20alternative%20to%0ALarge%20Language%20Models%20%28LLMs%29%2C%20delivering%20strong%20performance%20while%20using%20far%0Afewer%20resources.%20We%20introduce%20a%20simple%20and%20effective%20framework%20for%20pretraining%0ASLMs%20that%20brings%20together%20three%20complementary%20ideas.%20First%2C%20we%20identify%0Astructurally%20sparse%20sub-network%20initializations%20that%20consistently%20outperform%0Arandomly%20initialized%20models%20of%20similar%20size%20under%20the%20same%20compute%20budget.%0ASecond%2C%20we%20use%20evolutionary%20search%20to%20automatically%20discover%20high-quality%0Asub-network%20initializations%2C%20providing%20better%20starting%20points%20for%20pretraining.%0AThird%2C%20we%20apply%20knowledge%20distillation%20from%20larger%20teacher%20models%20to%20speed%20up%0Atraining%20and%20improve%20generalization.%20Together%2C%20these%20components%20make%20SLM%0Apretraining%20substantially%20more%20efficient%3A%20our%20best%20model%2C%20discovered%20using%0Aevolutionary%20search%20and%20initialized%20with%20LLM%20weights%2C%20matches%20the%20validation%0Aperplexity%20of%20a%20comparable%20Pythia%20SLM%20while%20requiring%209.2x%20fewer%20pretraining%0Atokens.%20We%20release%20all%20code%20and%20models%20at%0Ahttps%3A//github.com/whittle-org/whittle/%2C%20offering%20a%20practical%20and%20reproducible%0Apath%20toward%20cost-efficient%20small%20language%20model%20development%20at%20scale.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07227v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhere%2520to%2520Begin%253A%2520Efficient%2520Pretraining%2520via%2520Subnetwork%2520Selection%2520and%250A%2520%2520Distillation%26entry.906535625%3DArjun%2520Krishnakumar%2520and%2520Rhea%2520Sanjay%2520Sukthanker%2520and%2520Hannan%2520Javed%2520Mahadik%2520and%2520Gabriela%2520Kadlecov%25C3%25A1%2520and%2520Vladyslav%2520Moroshan%2520and%2520Timur%2520Carstensen%2520and%2520Frank%2520Hutter%2520and%2520Aaron%2520Klein%26entry.1292438233%3D%2520%2520Small%2520Language%2520models%2520%2528SLMs%2529%2520offer%2520an%2520efficient%2520and%2520accessible%2520alternative%2520to%250ALarge%2520Language%2520Models%2520%2528LLMs%2529%252C%2520delivering%2520strong%2520performance%2520while%2520using%2520far%250Afewer%2520resources.%2520We%2520introduce%2520a%2520simple%2520and%2520effective%2520framework%2520for%2520pretraining%250ASLMs%2520that%2520brings%2520together%2520three%2520complementary%2520ideas.%2520First%252C%2520we%2520identify%250Astructurally%2520sparse%2520sub-network%2520initializations%2520that%2520consistently%2520outperform%250Arandomly%2520initialized%2520models%2520of%2520similar%2520size%2520under%2520the%2520same%2520compute%2520budget.%250ASecond%252C%2520we%2520use%2520evolutionary%2520search%2520to%2520automatically%2520discover%2520high-quality%250Asub-network%2520initializations%252C%2520providing%2520better%2520starting%2520points%2520for%2520pretraining.%250AThird%252C%2520we%2520apply%2520knowledge%2520distillation%2520from%2520larger%2520teacher%2520models%2520to%2520speed%2520up%250Atraining%2520and%2520improve%2520generalization.%2520Together%252C%2520these%2520components%2520make%2520SLM%250Apretraining%2520substantially%2520more%2520efficient%253A%2520our%2520best%2520model%252C%2520discovered%2520using%250Aevolutionary%2520search%2520and%2520initialized%2520with%2520LLM%2520weights%252C%2520matches%2520the%2520validation%250Aperplexity%2520of%2520a%2520comparable%2520Pythia%2520SLM%2520while%2520requiring%25209.2x%2520fewer%2520pretraining%250Atokens.%2520We%2520release%2520all%2520code%2520and%2520models%2520at%250Ahttps%253A//github.com/whittle-org/whittle/%252C%2520offering%2520a%2520practical%2520and%2520reproducible%250Apath%2520toward%2520cost-efficient%2520small%2520language%2520model%2520development%2520at%2520scale.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07227v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Where%20to%20Begin%3A%20Efficient%20Pretraining%20via%20Subnetwork%20Selection%20and%0A%20%20Distillation&entry.906535625=Arjun%20Krishnakumar%20and%20Rhea%20Sanjay%20Sukthanker%20and%20Hannan%20Javed%20Mahadik%20and%20Gabriela%20Kadlecov%C3%A1%20and%20Vladyslav%20Moroshan%20and%20Timur%20Carstensen%20and%20Frank%20Hutter%20and%20Aaron%20Klein&entry.1292438233=%20%20Small%20Language%20models%20%28SLMs%29%20offer%20an%20efficient%20and%20accessible%20alternative%20to%0ALarge%20Language%20Models%20%28LLMs%29%2C%20delivering%20strong%20performance%20while%20using%20far%0Afewer%20resources.%20We%20introduce%20a%20simple%20and%20effective%20framework%20for%20pretraining%0ASLMs%20that%20brings%20together%20three%20complementary%20ideas.%20First%2C%20we%20identify%0Astructurally%20sparse%20sub-network%20initializations%20that%20consistently%20outperform%0Arandomly%20initialized%20models%20of%20similar%20size%20under%20the%20same%20compute%20budget.%0ASecond%2C%20we%20use%20evolutionary%20search%20to%20automatically%20discover%20high-quality%0Asub-network%20initializations%2C%20providing%20better%20starting%20points%20for%20pretraining.%0AThird%2C%20we%20apply%20knowledge%20distillation%20from%20larger%20teacher%20models%20to%20speed%20up%0Atraining%20and%20improve%20generalization.%20Together%2C%20these%20components%20make%20SLM%0Apretraining%20substantially%20more%20efficient%3A%20our%20best%20model%2C%20discovered%20using%0Aevolutionary%20search%20and%20initialized%20with%20LLM%20weights%2C%20matches%20the%20validation%0Aperplexity%20of%20a%20comparable%20Pythia%20SLM%20while%20requiring%209.2x%20fewer%20pretraining%0Atokens.%20We%20release%20all%20code%20and%20models%20at%0Ahttps%3A//github.com/whittle-org/whittle/%2C%20offering%20a%20practical%20and%20reproducible%0Apath%20toward%20cost-efficient%20small%20language%20model%20development%20at%20scale.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07227v1&entry.124074799=Read"},
{"title": "Valid Inference with Imperfect Synthetic Data", "author": "Yewon Byun and Shantanu Gupta and Zachary C. Lipton and Rachel Leah Childers and Bryan Wilder", "abstract": "  Predictions and generations from large language models are increasingly being\nexplored as an aid in limited data regimes, such as in computational social\nscience and human subjects research. While prior technical work has mainly\nexplored the potential to use model-predicted labels for unlabeled data in a\nprincipled manner, there is increasing interest in using large language models\nto generate entirely new synthetic samples (e.g., synthetic simulations), such\nas in responses to surveys. However, it remains unclear by what means\npractitioners can combine such data with real data and yet produce\nstatistically valid conclusions upon them. In this paper, we introduce a new\nestimator based on generalized method of moments, providing a\nhyperparameter-free solution with strong theoretical guarantees to address this\nchallenge. Intriguingly, we find that interactions between the moment residuals\nof synthetic data and those of real data (i.e., when they are predictive of\neach other) can greatly improve estimates of the target parameter. We validate\nthe finite-sample performance of our estimator across different tasks in\ncomputational social science applications, demonstrating large empirical gains.\n", "link": "http://arxiv.org/abs/2508.06635v2", "date": "2025-10-08", "relevancy": 1.4789, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5132}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4886}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4866}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Valid%20Inference%20with%20Imperfect%20Synthetic%20Data&body=Title%3A%20Valid%20Inference%20with%20Imperfect%20Synthetic%20Data%0AAuthor%3A%20Yewon%20Byun%20and%20Shantanu%20Gupta%20and%20Zachary%20C.%20Lipton%20and%20Rachel%20Leah%20Childers%20and%20Bryan%20Wilder%0AAbstract%3A%20%20%20Predictions%20and%20generations%20from%20large%20language%20models%20are%20increasingly%20being%0Aexplored%20as%20an%20aid%20in%20limited%20data%20regimes%2C%20such%20as%20in%20computational%20social%0Ascience%20and%20human%20subjects%20research.%20While%20prior%20technical%20work%20has%20mainly%0Aexplored%20the%20potential%20to%20use%20model-predicted%20labels%20for%20unlabeled%20data%20in%20a%0Aprincipled%20manner%2C%20there%20is%20increasing%20interest%20in%20using%20large%20language%20models%0Ato%20generate%20entirely%20new%20synthetic%20samples%20%28e.g.%2C%20synthetic%20simulations%29%2C%20such%0Aas%20in%20responses%20to%20surveys.%20However%2C%20it%20remains%20unclear%20by%20what%20means%0Apractitioners%20can%20combine%20such%20data%20with%20real%20data%20and%20yet%20produce%0Astatistically%20valid%20conclusions%20upon%20them.%20In%20this%20paper%2C%20we%20introduce%20a%20new%0Aestimator%20based%20on%20generalized%20method%20of%20moments%2C%20providing%20a%0Ahyperparameter-free%20solution%20with%20strong%20theoretical%20guarantees%20to%20address%20this%0Achallenge.%20Intriguingly%2C%20we%20find%20that%20interactions%20between%20the%20moment%20residuals%0Aof%20synthetic%20data%20and%20those%20of%20real%20data%20%28i.e.%2C%20when%20they%20are%20predictive%20of%0Aeach%20other%29%20can%20greatly%20improve%20estimates%20of%20the%20target%20parameter.%20We%20validate%0Athe%20finite-sample%20performance%20of%20our%20estimator%20across%20different%20tasks%20in%0Acomputational%20social%20science%20applications%2C%20demonstrating%20large%20empirical%20gains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2508.06635v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DValid%2520Inference%2520with%2520Imperfect%2520Synthetic%2520Data%26entry.906535625%3DYewon%2520Byun%2520and%2520Shantanu%2520Gupta%2520and%2520Zachary%2520C.%2520Lipton%2520and%2520Rachel%2520Leah%2520Childers%2520and%2520Bryan%2520Wilder%26entry.1292438233%3D%2520%2520Predictions%2520and%2520generations%2520from%2520large%2520language%2520models%2520are%2520increasingly%2520being%250Aexplored%2520as%2520an%2520aid%2520in%2520limited%2520data%2520regimes%252C%2520such%2520as%2520in%2520computational%2520social%250Ascience%2520and%2520human%2520subjects%2520research.%2520While%2520prior%2520technical%2520work%2520has%2520mainly%250Aexplored%2520the%2520potential%2520to%2520use%2520model-predicted%2520labels%2520for%2520unlabeled%2520data%2520in%2520a%250Aprincipled%2520manner%252C%2520there%2520is%2520increasing%2520interest%2520in%2520using%2520large%2520language%2520models%250Ato%2520generate%2520entirely%2520new%2520synthetic%2520samples%2520%2528e.g.%252C%2520synthetic%2520simulations%2529%252C%2520such%250Aas%2520in%2520responses%2520to%2520surveys.%2520However%252C%2520it%2520remains%2520unclear%2520by%2520what%2520means%250Apractitioners%2520can%2520combine%2520such%2520data%2520with%2520real%2520data%2520and%2520yet%2520produce%250Astatistically%2520valid%2520conclusions%2520upon%2520them.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520a%2520new%250Aestimator%2520based%2520on%2520generalized%2520method%2520of%2520moments%252C%2520providing%2520a%250Ahyperparameter-free%2520solution%2520with%2520strong%2520theoretical%2520guarantees%2520to%2520address%2520this%250Achallenge.%2520Intriguingly%252C%2520we%2520find%2520that%2520interactions%2520between%2520the%2520moment%2520residuals%250Aof%2520synthetic%2520data%2520and%2520those%2520of%2520real%2520data%2520%2528i.e.%252C%2520when%2520they%2520are%2520predictive%2520of%250Aeach%2520other%2529%2520can%2520greatly%2520improve%2520estimates%2520of%2520the%2520target%2520parameter.%2520We%2520validate%250Athe%2520finite-sample%2520performance%2520of%2520our%2520estimator%2520across%2520different%2520tasks%2520in%250Acomputational%2520social%2520science%2520applications%252C%2520demonstrating%2520large%2520empirical%2520gains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.06635v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Valid%20Inference%20with%20Imperfect%20Synthetic%20Data&entry.906535625=Yewon%20Byun%20and%20Shantanu%20Gupta%20and%20Zachary%20C.%20Lipton%20and%20Rachel%20Leah%20Childers%20and%20Bryan%20Wilder&entry.1292438233=%20%20Predictions%20and%20generations%20from%20large%20language%20models%20are%20increasingly%20being%0Aexplored%20as%20an%20aid%20in%20limited%20data%20regimes%2C%20such%20as%20in%20computational%20social%0Ascience%20and%20human%20subjects%20research.%20While%20prior%20technical%20work%20has%20mainly%0Aexplored%20the%20potential%20to%20use%20model-predicted%20labels%20for%20unlabeled%20data%20in%20a%0Aprincipled%20manner%2C%20there%20is%20increasing%20interest%20in%20using%20large%20language%20models%0Ato%20generate%20entirely%20new%20synthetic%20samples%20%28e.g.%2C%20synthetic%20simulations%29%2C%20such%0Aas%20in%20responses%20to%20surveys.%20However%2C%20it%20remains%20unclear%20by%20what%20means%0Apractitioners%20can%20combine%20such%20data%20with%20real%20data%20and%20yet%20produce%0Astatistically%20valid%20conclusions%20upon%20them.%20In%20this%20paper%2C%20we%20introduce%20a%20new%0Aestimator%20based%20on%20generalized%20method%20of%20moments%2C%20providing%20a%0Ahyperparameter-free%20solution%20with%20strong%20theoretical%20guarantees%20to%20address%20this%0Achallenge.%20Intriguingly%2C%20we%20find%20that%20interactions%20between%20the%20moment%20residuals%0Aof%20synthetic%20data%20and%20those%20of%20real%20data%20%28i.e.%2C%20when%20they%20are%20predictive%20of%0Aeach%20other%29%20can%20greatly%20improve%20estimates%20of%20the%20target%20parameter.%20We%20validate%0Athe%20finite-sample%20performance%20of%20our%20estimator%20across%20different%20tasks%20in%0Acomputational%20social%20science%20applications%2C%20demonstrating%20large%20empirical%20gains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2508.06635v2&entry.124074799=Read"},
{"title": "Multi-Objective Multi-Agent Path Finding with Lexicographic Cost\n  Preferences", "author": "Pulkit Rustagi and Kyle Hollins Wray and Sandhya Saisubramanian", "abstract": "  Many real-world scenarios require multiple agents to coordinate in shared\nenvironments, while balancing trade-offs between multiple, potentially\ncompeting objectives. Current multi-objective multi-agent path finding\n(MO-MAPF) algorithms typically produce conflict-free plans by computing Pareto\nfrontiers. They do not explicitly optimize for user-defined preferences, even\nwhen the preferences are available, and scale poorly with the number of\nobjectives. We propose a lexicographic framework for modeling MO-MAPF, along\nwith an algorithm \\textit{Lexicographic Conflict-Based Search} (LCBS) that\ndirectly computes a single solution aligned with a lexicographic preference\nover objectives. LCBS integrates a priority-aware low-level $A^*$ search with\nconflict-based search, avoiding Pareto frontier construction and enabling\nefficient planning guided by preference over objectives. We provide insights\ninto optimality and scalability, and empirically demonstrate that LCBS computes\noptimal solutions while scaling to instances with up to ten objectives -- far\nbeyond the limits of existing MO-MAPF methods. Evaluations on standard and\nrandomized MAPF benchmarks show consistently higher success rates against\nstate-of-the-art baselines, especially with increasing number of objectives.\n", "link": "http://arxiv.org/abs/2510.07276v1", "date": "2025-10-08", "relevancy": 1.4493, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4946}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4816}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4753}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Objective%20Multi-Agent%20Path%20Finding%20with%20Lexicographic%20Cost%0A%20%20Preferences&body=Title%3A%20Multi-Objective%20Multi-Agent%20Path%20Finding%20with%20Lexicographic%20Cost%0A%20%20Preferences%0AAuthor%3A%20Pulkit%20Rustagi%20and%20Kyle%20Hollins%20Wray%20and%20Sandhya%20Saisubramanian%0AAbstract%3A%20%20%20Many%20real-world%20scenarios%20require%20multiple%20agents%20to%20coordinate%20in%20shared%0Aenvironments%2C%20while%20balancing%20trade-offs%20between%20multiple%2C%20potentially%0Acompeting%20objectives.%20Current%20multi-objective%20multi-agent%20path%20finding%0A%28MO-MAPF%29%20algorithms%20typically%20produce%20conflict-free%20plans%20by%20computing%20Pareto%0Afrontiers.%20They%20do%20not%20explicitly%20optimize%20for%20user-defined%20preferences%2C%20even%0Awhen%20the%20preferences%20are%20available%2C%20and%20scale%20poorly%20with%20the%20number%20of%0Aobjectives.%20We%20propose%20a%20lexicographic%20framework%20for%20modeling%20MO-MAPF%2C%20along%0Awith%20an%20algorithm%20%5Ctextit%7BLexicographic%20Conflict-Based%20Search%7D%20%28LCBS%29%20that%0Adirectly%20computes%20a%20single%20solution%20aligned%20with%20a%20lexicographic%20preference%0Aover%20objectives.%20LCBS%20integrates%20a%20priority-aware%20low-level%20%24A%5E%2A%24%20search%20with%0Aconflict-based%20search%2C%20avoiding%20Pareto%20frontier%20construction%20and%20enabling%0Aefficient%20planning%20guided%20by%20preference%20over%20objectives.%20We%20provide%20insights%0Ainto%20optimality%20and%20scalability%2C%20and%20empirically%20demonstrate%20that%20LCBS%20computes%0Aoptimal%20solutions%20while%20scaling%20to%20instances%20with%20up%20to%20ten%20objectives%20--%20far%0Abeyond%20the%20limits%20of%20existing%20MO-MAPF%20methods.%20Evaluations%20on%20standard%20and%0Arandomized%20MAPF%20benchmarks%20show%20consistently%20higher%20success%20rates%20against%0Astate-of-the-art%20baselines%2C%20especially%20with%20increasing%20number%20of%20objectives.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07276v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Objective%2520Multi-Agent%2520Path%2520Finding%2520with%2520Lexicographic%2520Cost%250A%2520%2520Preferences%26entry.906535625%3DPulkit%2520Rustagi%2520and%2520Kyle%2520Hollins%2520Wray%2520and%2520Sandhya%2520Saisubramanian%26entry.1292438233%3D%2520%2520Many%2520real-world%2520scenarios%2520require%2520multiple%2520agents%2520to%2520coordinate%2520in%2520shared%250Aenvironments%252C%2520while%2520balancing%2520trade-offs%2520between%2520multiple%252C%2520potentially%250Acompeting%2520objectives.%2520Current%2520multi-objective%2520multi-agent%2520path%2520finding%250A%2528MO-MAPF%2529%2520algorithms%2520typically%2520produce%2520conflict-free%2520plans%2520by%2520computing%2520Pareto%250Afrontiers.%2520They%2520do%2520not%2520explicitly%2520optimize%2520for%2520user-defined%2520preferences%252C%2520even%250Awhen%2520the%2520preferences%2520are%2520available%252C%2520and%2520scale%2520poorly%2520with%2520the%2520number%2520of%250Aobjectives.%2520We%2520propose%2520a%2520lexicographic%2520framework%2520for%2520modeling%2520MO-MAPF%252C%2520along%250Awith%2520an%2520algorithm%2520%255Ctextit%257BLexicographic%2520Conflict-Based%2520Search%257D%2520%2528LCBS%2529%2520that%250Adirectly%2520computes%2520a%2520single%2520solution%2520aligned%2520with%2520a%2520lexicographic%2520preference%250Aover%2520objectives.%2520LCBS%2520integrates%2520a%2520priority-aware%2520low-level%2520%2524A%255E%252A%2524%2520search%2520with%250Aconflict-based%2520search%252C%2520avoiding%2520Pareto%2520frontier%2520construction%2520and%2520enabling%250Aefficient%2520planning%2520guided%2520by%2520preference%2520over%2520objectives.%2520We%2520provide%2520insights%250Ainto%2520optimality%2520and%2520scalability%252C%2520and%2520empirically%2520demonstrate%2520that%2520LCBS%2520computes%250Aoptimal%2520solutions%2520while%2520scaling%2520to%2520instances%2520with%2520up%2520to%2520ten%2520objectives%2520--%2520far%250Abeyond%2520the%2520limits%2520of%2520existing%2520MO-MAPF%2520methods.%2520Evaluations%2520on%2520standard%2520and%250Arandomized%2520MAPF%2520benchmarks%2520show%2520consistently%2520higher%2520success%2520rates%2520against%250Astate-of-the-art%2520baselines%252C%2520especially%2520with%2520increasing%2520number%2520of%2520objectives.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07276v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Objective%20Multi-Agent%20Path%20Finding%20with%20Lexicographic%20Cost%0A%20%20Preferences&entry.906535625=Pulkit%20Rustagi%20and%20Kyle%20Hollins%20Wray%20and%20Sandhya%20Saisubramanian&entry.1292438233=%20%20Many%20real-world%20scenarios%20require%20multiple%20agents%20to%20coordinate%20in%20shared%0Aenvironments%2C%20while%20balancing%20trade-offs%20between%20multiple%2C%20potentially%0Acompeting%20objectives.%20Current%20multi-objective%20multi-agent%20path%20finding%0A%28MO-MAPF%29%20algorithms%20typically%20produce%20conflict-free%20plans%20by%20computing%20Pareto%0Afrontiers.%20They%20do%20not%20explicitly%20optimize%20for%20user-defined%20preferences%2C%20even%0Awhen%20the%20preferences%20are%20available%2C%20and%20scale%20poorly%20with%20the%20number%20of%0Aobjectives.%20We%20propose%20a%20lexicographic%20framework%20for%20modeling%20MO-MAPF%2C%20along%0Awith%20an%20algorithm%20%5Ctextit%7BLexicographic%20Conflict-Based%20Search%7D%20%28LCBS%29%20that%0Adirectly%20computes%20a%20single%20solution%20aligned%20with%20a%20lexicographic%20preference%0Aover%20objectives.%20LCBS%20integrates%20a%20priority-aware%20low-level%20%24A%5E%2A%24%20search%20with%0Aconflict-based%20search%2C%20avoiding%20Pareto%20frontier%20construction%20and%20enabling%0Aefficient%20planning%20guided%20by%20preference%20over%20objectives.%20We%20provide%20insights%0Ainto%20optimality%20and%20scalability%2C%20and%20empirically%20demonstrate%20that%20LCBS%20computes%0Aoptimal%20solutions%20while%20scaling%20to%20instances%20with%20up%20to%20ten%20objectives%20--%20far%0Abeyond%20the%20limits%20of%20existing%20MO-MAPF%20methods.%20Evaluations%20on%20standard%20and%0Arandomized%20MAPF%20benchmarks%20show%20consistently%20higher%20success%20rates%20against%0Astate-of-the-art%20baselines%2C%20especially%20with%20increasing%20number%20of%20objectives.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07276v1&entry.124074799=Read"},
{"title": "Evolutionary Profiles for Protein Fitness Prediction", "author": "Jigang Fan and Xiaoran Jiao and Shengdong Lin and Zhanming Liang and Weian Mao and Chenchen Jing and Hao Chen and Chunhua Shen", "abstract": "  Predicting the fitness impact of mutations is central to protein engineering\nbut constrained by limited assays relative to the size of sequence space.\nProtein language models (pLMs) trained with masked language modeling (MLM)\nexhibit strong zero-shot fitness prediction; we provide a unifying view by\ninterpreting natural evolution as implicit reward maximization and MLM as\ninverse reinforcement learning (IRL), in which extant sequences act as expert\ndemonstrations and pLM log-odds serve as fitness estimates. Building on this\nperspective, we introduce EvoIF, a lightweight model that integrates two\ncomplementary sources of evolutionary signal: (i) within-family profiles from\nretrieved homologs and (ii) cross-family structural-evolutionary constraints\ndistilled from inverse folding logits. EvoIF fuses sequence-structure\nrepresentations with these profiles via a compact transition block, yielding\ncalibrated probabilities for log-odds scoring. On ProteinGym (217 mutational\nassays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve\nstate-of-the-art or competitive performance while using only 0.15% of the\ntraining data and fewer parameters than recent large models. Ablations confirm\nthat within-family and cross-family profiles are complementary, improving\nrobustness across function types, MSA depths, taxa, and mutation depths. The\ncodes will be made publicly available at https://github.com/aim-uofa/EvoIF.\n", "link": "http://arxiv.org/abs/2510.07286v1", "date": "2025-10-08", "relevancy": 1.3041, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4567}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4363}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evolutionary%20Profiles%20for%20Protein%20Fitness%20Prediction&body=Title%3A%20Evolutionary%20Profiles%20for%20Protein%20Fitness%20Prediction%0AAuthor%3A%20Jigang%20Fan%20and%20Xiaoran%20Jiao%20and%20Shengdong%20Lin%20and%20Zhanming%20Liang%20and%20Weian%20Mao%20and%20Chenchen%20Jing%20and%20Hao%20Chen%20and%20Chunhua%20Shen%0AAbstract%3A%20%20%20Predicting%20the%20fitness%20impact%20of%20mutations%20is%20central%20to%20protein%20engineering%0Abut%20constrained%20by%20limited%20assays%20relative%20to%20the%20size%20of%20sequence%20space.%0AProtein%20language%20models%20%28pLMs%29%20trained%20with%20masked%20language%20modeling%20%28MLM%29%0Aexhibit%20strong%20zero-shot%20fitness%20prediction%3B%20we%20provide%20a%20unifying%20view%20by%0Ainterpreting%20natural%20evolution%20as%20implicit%20reward%20maximization%20and%20MLM%20as%0Ainverse%20reinforcement%20learning%20%28IRL%29%2C%20in%20which%20extant%20sequences%20act%20as%20expert%0Ademonstrations%20and%20pLM%20log-odds%20serve%20as%20fitness%20estimates.%20Building%20on%20this%0Aperspective%2C%20we%20introduce%20EvoIF%2C%20a%20lightweight%20model%20that%20integrates%20two%0Acomplementary%20sources%20of%20evolutionary%20signal%3A%20%28i%29%20within-family%20profiles%20from%0Aretrieved%20homologs%20and%20%28ii%29%20cross-family%20structural-evolutionary%20constraints%0Adistilled%20from%20inverse%20folding%20logits.%20EvoIF%20fuses%20sequence-structure%0Arepresentations%20with%20these%20profiles%20via%20a%20compact%20transition%20block%2C%20yielding%0Acalibrated%20probabilities%20for%20log-odds%20scoring.%20On%20ProteinGym%20%28217%20mutational%0Aassays%3B%20%3E2.5M%20mutants%29%2C%20EvoIF%20and%20its%20MSA-enabled%20variant%20achieve%0Astate-of-the-art%20or%20competitive%20performance%20while%20using%20only%200.15%25%20of%20the%0Atraining%20data%20and%20fewer%20parameters%20than%20recent%20large%20models.%20Ablations%20confirm%0Athat%20within-family%20and%20cross-family%20profiles%20are%20complementary%2C%20improving%0Arobustness%20across%20function%20types%2C%20MSA%20depths%2C%20taxa%2C%20and%20mutation%20depths.%20The%0Acodes%20will%20be%20made%20publicly%20available%20at%20https%3A//github.com/aim-uofa/EvoIF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07286v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvolutionary%2520Profiles%2520for%2520Protein%2520Fitness%2520Prediction%26entry.906535625%3DJigang%2520Fan%2520and%2520Xiaoran%2520Jiao%2520and%2520Shengdong%2520Lin%2520and%2520Zhanming%2520Liang%2520and%2520Weian%2520Mao%2520and%2520Chenchen%2520Jing%2520and%2520Hao%2520Chen%2520and%2520Chunhua%2520Shen%26entry.1292438233%3D%2520%2520Predicting%2520the%2520fitness%2520impact%2520of%2520mutations%2520is%2520central%2520to%2520protein%2520engineering%250Abut%2520constrained%2520by%2520limited%2520assays%2520relative%2520to%2520the%2520size%2520of%2520sequence%2520space.%250AProtein%2520language%2520models%2520%2528pLMs%2529%2520trained%2520with%2520masked%2520language%2520modeling%2520%2528MLM%2529%250Aexhibit%2520strong%2520zero-shot%2520fitness%2520prediction%253B%2520we%2520provide%2520a%2520unifying%2520view%2520by%250Ainterpreting%2520natural%2520evolution%2520as%2520implicit%2520reward%2520maximization%2520and%2520MLM%2520as%250Ainverse%2520reinforcement%2520learning%2520%2528IRL%2529%252C%2520in%2520which%2520extant%2520sequences%2520act%2520as%2520expert%250Ademonstrations%2520and%2520pLM%2520log-odds%2520serve%2520as%2520fitness%2520estimates.%2520Building%2520on%2520this%250Aperspective%252C%2520we%2520introduce%2520EvoIF%252C%2520a%2520lightweight%2520model%2520that%2520integrates%2520two%250Acomplementary%2520sources%2520of%2520evolutionary%2520signal%253A%2520%2528i%2529%2520within-family%2520profiles%2520from%250Aretrieved%2520homologs%2520and%2520%2528ii%2529%2520cross-family%2520structural-evolutionary%2520constraints%250Adistilled%2520from%2520inverse%2520folding%2520logits.%2520EvoIF%2520fuses%2520sequence-structure%250Arepresentations%2520with%2520these%2520profiles%2520via%2520a%2520compact%2520transition%2520block%252C%2520yielding%250Acalibrated%2520probabilities%2520for%2520log-odds%2520scoring.%2520On%2520ProteinGym%2520%2528217%2520mutational%250Aassays%253B%2520%253E2.5M%2520mutants%2529%252C%2520EvoIF%2520and%2520its%2520MSA-enabled%2520variant%2520achieve%250Astate-of-the-art%2520or%2520competitive%2520performance%2520while%2520using%2520only%25200.15%2525%2520of%2520the%250Atraining%2520data%2520and%2520fewer%2520parameters%2520than%2520recent%2520large%2520models.%2520Ablations%2520confirm%250Athat%2520within-family%2520and%2520cross-family%2520profiles%2520are%2520complementary%252C%2520improving%250Arobustness%2520across%2520function%2520types%252C%2520MSA%2520depths%252C%2520taxa%252C%2520and%2520mutation%2520depths.%2520The%250Acodes%2520will%2520be%2520made%2520publicly%2520available%2520at%2520https%253A//github.com/aim-uofa/EvoIF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07286v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evolutionary%20Profiles%20for%20Protein%20Fitness%20Prediction&entry.906535625=Jigang%20Fan%20and%20Xiaoran%20Jiao%20and%20Shengdong%20Lin%20and%20Zhanming%20Liang%20and%20Weian%20Mao%20and%20Chenchen%20Jing%20and%20Hao%20Chen%20and%20Chunhua%20Shen&entry.1292438233=%20%20Predicting%20the%20fitness%20impact%20of%20mutations%20is%20central%20to%20protein%20engineering%0Abut%20constrained%20by%20limited%20assays%20relative%20to%20the%20size%20of%20sequence%20space.%0AProtein%20language%20models%20%28pLMs%29%20trained%20with%20masked%20language%20modeling%20%28MLM%29%0Aexhibit%20strong%20zero-shot%20fitness%20prediction%3B%20we%20provide%20a%20unifying%20view%20by%0Ainterpreting%20natural%20evolution%20as%20implicit%20reward%20maximization%20and%20MLM%20as%0Ainverse%20reinforcement%20learning%20%28IRL%29%2C%20in%20which%20extant%20sequences%20act%20as%20expert%0Ademonstrations%20and%20pLM%20log-odds%20serve%20as%20fitness%20estimates.%20Building%20on%20this%0Aperspective%2C%20we%20introduce%20EvoIF%2C%20a%20lightweight%20model%20that%20integrates%20two%0Acomplementary%20sources%20of%20evolutionary%20signal%3A%20%28i%29%20within-family%20profiles%20from%0Aretrieved%20homologs%20and%20%28ii%29%20cross-family%20structural-evolutionary%20constraints%0Adistilled%20from%20inverse%20folding%20logits.%20EvoIF%20fuses%20sequence-structure%0Arepresentations%20with%20these%20profiles%20via%20a%20compact%20transition%20block%2C%20yielding%0Acalibrated%20probabilities%20for%20log-odds%20scoring.%20On%20ProteinGym%20%28217%20mutational%0Aassays%3B%20%3E2.5M%20mutants%29%2C%20EvoIF%20and%20its%20MSA-enabled%20variant%20achieve%0Astate-of-the-art%20or%20competitive%20performance%20while%20using%20only%200.15%25%20of%20the%0Atraining%20data%20and%20fewer%20parameters%20than%20recent%20large%20models.%20Ablations%20confirm%0Athat%20within-family%20and%20cross-family%20profiles%20are%20complementary%2C%20improving%0Arobustness%20across%20function%20types%2C%20MSA%20depths%2C%20taxa%2C%20and%20mutation%20depths.%20The%0Acodes%20will%20be%20made%20publicly%20available%20at%20https%3A//github.com/aim-uofa/EvoIF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07286v1&entry.124074799=Read"},
{"title": "Online Rubrics Elicitation from Pairwise Comparisons", "author": "MohammadHossein Rezaei and Robert Vacareanu and Zihao Wang and Clinton Wang and Yunzhong He and Afra Feyza Aky\u00fcrek", "abstract": "  Rubrics provide a flexible way to train LLMs on open-ended long-form answers\nwhere verifiable rewards are not applicable and human preferences provide\ncoarse signals. Prior work shows that reinforcement learning with rubric-based\nrewards leads to consistent gains in LLM post-training. Most existing\napproaches rely on rubrics that remain static over the course of training. Such\nstatic rubrics, however, are vulnerable to reward-hacking type behaviors and\nfail to capture emergent desiderata that arise during training. We introduce\nOnline Rubrics Elicitation (OnlineRubrics), a method that dynamically curates\nevaluation criteria in an online manner through pairwise comparisons of\nresponses from current and reference policies. This online process enables\ncontinuous identification and mitigation of errors as training proceeds.\nEmpirically, this approach yields consistent improvements of up to 8% over\ntraining exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as\nwell as the validation sets of expert questions and rubrics. We qualitatively\nanalyze the elicited criteria and identify prominent themes such as\ntransparency, practicality, organization, and reasoning.\n", "link": "http://arxiv.org/abs/2510.07284v1", "date": "2025-10-08", "relevancy": 0.9213, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4693}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4573}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Online%20Rubrics%20Elicitation%20from%20Pairwise%20Comparisons&body=Title%3A%20Online%20Rubrics%20Elicitation%20from%20Pairwise%20Comparisons%0AAuthor%3A%20MohammadHossein%20Rezaei%20and%20Robert%20Vacareanu%20and%20Zihao%20Wang%20and%20Clinton%20Wang%20and%20Yunzhong%20He%20and%20Afra%20Feyza%20Aky%C3%BCrek%0AAbstract%3A%20%20%20Rubrics%20provide%20a%20flexible%20way%20to%20train%20LLMs%20on%20open-ended%20long-form%20answers%0Awhere%20verifiable%20rewards%20are%20not%20applicable%20and%20human%20preferences%20provide%0Acoarse%20signals.%20Prior%20work%20shows%20that%20reinforcement%20learning%20with%20rubric-based%0Arewards%20leads%20to%20consistent%20gains%20in%20LLM%20post-training.%20Most%20existing%0Aapproaches%20rely%20on%20rubrics%20that%20remain%20static%20over%20the%20course%20of%20training.%20Such%0Astatic%20rubrics%2C%20however%2C%20are%20vulnerable%20to%20reward-hacking%20type%20behaviors%20and%0Afail%20to%20capture%20emergent%20desiderata%20that%20arise%20during%20training.%20We%20introduce%0AOnline%20Rubrics%20Elicitation%20%28OnlineRubrics%29%2C%20a%20method%20that%20dynamically%20curates%0Aevaluation%20criteria%20in%20an%20online%20manner%20through%20pairwise%20comparisons%20of%0Aresponses%20from%20current%20and%20reference%20policies.%20This%20online%20process%20enables%0Acontinuous%20identification%20and%20mitigation%20of%20errors%20as%20training%20proceeds.%0AEmpirically%2C%20this%20approach%20yields%20consistent%20improvements%20of%20up%20to%208%25%20over%0Atraining%20exclusively%20with%20static%20rubrics%20across%20AlpacaEval%2C%20GPQA%2C%20ArenaHard%20as%0Awell%20as%20the%20validation%20sets%20of%20expert%20questions%20and%20rubrics.%20We%20qualitatively%0Aanalyze%20the%20elicited%20criteria%20and%20identify%20prominent%20themes%20such%20as%0Atransparency%2C%20practicality%2C%20organization%2C%20and%20reasoning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2510.07284v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOnline%2520Rubrics%2520Elicitation%2520from%2520Pairwise%2520Comparisons%26entry.906535625%3DMohammadHossein%2520Rezaei%2520and%2520Robert%2520Vacareanu%2520and%2520Zihao%2520Wang%2520and%2520Clinton%2520Wang%2520and%2520Yunzhong%2520He%2520and%2520Afra%2520Feyza%2520Aky%25C3%25BCrek%26entry.1292438233%3D%2520%2520Rubrics%2520provide%2520a%2520flexible%2520way%2520to%2520train%2520LLMs%2520on%2520open-ended%2520long-form%2520answers%250Awhere%2520verifiable%2520rewards%2520are%2520not%2520applicable%2520and%2520human%2520preferences%2520provide%250Acoarse%2520signals.%2520Prior%2520work%2520shows%2520that%2520reinforcement%2520learning%2520with%2520rubric-based%250Arewards%2520leads%2520to%2520consistent%2520gains%2520in%2520LLM%2520post-training.%2520Most%2520existing%250Aapproaches%2520rely%2520on%2520rubrics%2520that%2520remain%2520static%2520over%2520the%2520course%2520of%2520training.%2520Such%250Astatic%2520rubrics%252C%2520however%252C%2520are%2520vulnerable%2520to%2520reward-hacking%2520type%2520behaviors%2520and%250Afail%2520to%2520capture%2520emergent%2520desiderata%2520that%2520arise%2520during%2520training.%2520We%2520introduce%250AOnline%2520Rubrics%2520Elicitation%2520%2528OnlineRubrics%2529%252C%2520a%2520method%2520that%2520dynamically%2520curates%250Aevaluation%2520criteria%2520in%2520an%2520online%2520manner%2520through%2520pairwise%2520comparisons%2520of%250Aresponses%2520from%2520current%2520and%2520reference%2520policies.%2520This%2520online%2520process%2520enables%250Acontinuous%2520identification%2520and%2520mitigation%2520of%2520errors%2520as%2520training%2520proceeds.%250AEmpirically%252C%2520this%2520approach%2520yields%2520consistent%2520improvements%2520of%2520up%2520to%25208%2525%2520over%250Atraining%2520exclusively%2520with%2520static%2520rubrics%2520across%2520AlpacaEval%252C%2520GPQA%252C%2520ArenaHard%2520as%250Awell%2520as%2520the%2520validation%2520sets%2520of%2520expert%2520questions%2520and%2520rubrics.%2520We%2520qualitatively%250Aanalyze%2520the%2520elicited%2520criteria%2520and%2520identify%2520prominent%2520themes%2520such%2520as%250Atransparency%252C%2520practicality%252C%2520organization%252C%2520and%2520reasoning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07284v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Online%20Rubrics%20Elicitation%20from%20Pairwise%20Comparisons&entry.906535625=MohammadHossein%20Rezaei%20and%20Robert%20Vacareanu%20and%20Zihao%20Wang%20and%20Clinton%20Wang%20and%20Yunzhong%20He%20and%20Afra%20Feyza%20Aky%C3%BCrek&entry.1292438233=%20%20Rubrics%20provide%20a%20flexible%20way%20to%20train%20LLMs%20on%20open-ended%20long-form%20answers%0Awhere%20verifiable%20rewards%20are%20not%20applicable%20and%20human%20preferences%20provide%0Acoarse%20signals.%20Prior%20work%20shows%20that%20reinforcement%20learning%20with%20rubric-based%0Arewards%20leads%20to%20consistent%20gains%20in%20LLM%20post-training.%20Most%20existing%0Aapproaches%20rely%20on%20rubrics%20that%20remain%20static%20over%20the%20course%20of%20training.%20Such%0Astatic%20rubrics%2C%20however%2C%20are%20vulnerable%20to%20reward-hacking%20type%20behaviors%20and%0Afail%20to%20capture%20emergent%20desiderata%20that%20arise%20during%20training.%20We%20introduce%0AOnline%20Rubrics%20Elicitation%20%28OnlineRubrics%29%2C%20a%20method%20that%20dynamically%20curates%0Aevaluation%20criteria%20in%20an%20online%20manner%20through%20pairwise%20comparisons%20of%0Aresponses%20from%20current%20and%20reference%20policies.%20This%20online%20process%20enables%0Acontinuous%20identification%20and%20mitigation%20of%20errors%20as%20training%20proceeds.%0AEmpirically%2C%20this%20approach%20yields%20consistent%20improvements%20of%20up%20to%208%25%20over%0Atraining%20exclusively%20with%20static%20rubrics%20across%20AlpacaEval%2C%20GPQA%2C%20ArenaHard%20as%0Awell%20as%20the%20validation%20sets%20of%20expert%20questions%20and%20rubrics.%20We%20qualitatively%0Aanalyze%20the%20elicited%20criteria%20and%20identify%20prominent%20themes%20such%20as%0Atransparency%2C%20practicality%2C%20organization%2C%20and%20reasoning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2510.07284v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


