<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260115.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization", "author": "Wei-Tse Cheng and Yen-Jen Chiou and Yuan-Fu Yang", "abstract": "We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS. Additional details and resources are available at this URL: https://breeze1124.github.io/rgs-slam-project-page/", "link": "http://arxiv.org/abs/2601.00705v3", "date": "2026-01-15", "relevancy": 3.4911, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.8037}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6537}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6372}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGS-SLAM%3A%20Robust%20Gaussian%20Splatting%20SLAM%20with%20One-Shot%20Dense%20Initialization&body=Title%3A%20RGS-SLAM%3A%20Robust%20Gaussian%20Splatting%20SLAM%20with%20One-Shot%20Dense%20Initialization%0AAuthor%3A%20Wei-Tse%20Cheng%20and%20Yen-Jen%20Chiou%20and%20Yuan-Fu%20Yang%0AAbstract%3A%20We%20introduce%20RGS-SLAM%2C%20a%20robust%20Gaussian-splatting%20SLAM%20framework%20that%20replaces%20the%20residual-driven%20densification%20stage%20of%20GS-SLAM%20with%20a%20training-free%20correspondence-to-Gaussian%20initialization.%20Instead%20of%20progressively%20adding%20Gaussians%20as%20residuals%20reveal%20missing%20geometry%2C%20RGS-SLAM%20performs%20a%20one-shot%20triangulation%20of%20dense%20multi-view%20correspondences%20derived%20from%20DINOv3%20descriptors%20refined%20through%20a%20confidence-aware%20inlier%20classifier%2C%20generating%20a%20well-distributed%20and%20structure-aware%20Gaussian%20seed%20prior%20to%20optimization.%20This%20initialization%20stabilizes%20early%20mapping%20and%20accelerates%20convergence%20by%20roughly%2020%5C%25%2C%20yielding%20higher%20rendering%20fidelity%20in%20texture-rich%20and%20cluttered%20scenes%20while%20remaining%20fully%20compatible%20with%20existing%20GS-SLAM%20pipelines.%20Evaluated%20on%20the%20TUM%20RGB-D%20and%20Replica%20datasets%2C%20RGS-SLAM%20achieves%20competitive%20or%20superior%20localization%20and%20reconstruction%20accuracy%20compared%20with%20state-of-the-art%20Gaussian%20and%20point-based%20SLAM%20systems%2C%20sustaining%20real-time%20mapping%20performance%20at%20up%20to%20925%20FPS.%20Additional%20details%20and%20resources%20are%20available%20at%20this%20URL%3A%20https%3A//breeze1124.github.io/rgs-slam-project-page/%0ALink%3A%20http%3A//arxiv.org/abs/2601.00705v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGS-SLAM%253A%2520Robust%2520Gaussian%2520Splatting%2520SLAM%2520with%2520One-Shot%2520Dense%2520Initialization%26entry.906535625%3DWei-Tse%2520Cheng%2520and%2520Yen-Jen%2520Chiou%2520and%2520Yuan-Fu%2520Yang%26entry.1292438233%3DWe%2520introduce%2520RGS-SLAM%252C%2520a%2520robust%2520Gaussian-splatting%2520SLAM%2520framework%2520that%2520replaces%2520the%2520residual-driven%2520densification%2520stage%2520of%2520GS-SLAM%2520with%2520a%2520training-free%2520correspondence-to-Gaussian%2520initialization.%2520Instead%2520of%2520progressively%2520adding%2520Gaussians%2520as%2520residuals%2520reveal%2520missing%2520geometry%252C%2520RGS-SLAM%2520performs%2520a%2520one-shot%2520triangulation%2520of%2520dense%2520multi-view%2520correspondences%2520derived%2520from%2520DINOv3%2520descriptors%2520refined%2520through%2520a%2520confidence-aware%2520inlier%2520classifier%252C%2520generating%2520a%2520well-distributed%2520and%2520structure-aware%2520Gaussian%2520seed%2520prior%2520to%2520optimization.%2520This%2520initialization%2520stabilizes%2520early%2520mapping%2520and%2520accelerates%2520convergence%2520by%2520roughly%252020%255C%2525%252C%2520yielding%2520higher%2520rendering%2520fidelity%2520in%2520texture-rich%2520and%2520cluttered%2520scenes%2520while%2520remaining%2520fully%2520compatible%2520with%2520existing%2520GS-SLAM%2520pipelines.%2520Evaluated%2520on%2520the%2520TUM%2520RGB-D%2520and%2520Replica%2520datasets%252C%2520RGS-SLAM%2520achieves%2520competitive%2520or%2520superior%2520localization%2520and%2520reconstruction%2520accuracy%2520compared%2520with%2520state-of-the-art%2520Gaussian%2520and%2520point-based%2520SLAM%2520systems%252C%2520sustaining%2520real-time%2520mapping%2520performance%2520at%2520up%2520to%2520925%2520FPS.%2520Additional%2520details%2520and%2520resources%2520are%2520available%2520at%2520this%2520URL%253A%2520https%253A//breeze1124.github.io/rgs-slam-project-page/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00705v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGS-SLAM%3A%20Robust%20Gaussian%20Splatting%20SLAM%20with%20One-Shot%20Dense%20Initialization&entry.906535625=Wei-Tse%20Cheng%20and%20Yen-Jen%20Chiou%20and%20Yuan-Fu%20Yang&entry.1292438233=We%20introduce%20RGS-SLAM%2C%20a%20robust%20Gaussian-splatting%20SLAM%20framework%20that%20replaces%20the%20residual-driven%20densification%20stage%20of%20GS-SLAM%20with%20a%20training-free%20correspondence-to-Gaussian%20initialization.%20Instead%20of%20progressively%20adding%20Gaussians%20as%20residuals%20reveal%20missing%20geometry%2C%20RGS-SLAM%20performs%20a%20one-shot%20triangulation%20of%20dense%20multi-view%20correspondences%20derived%20from%20DINOv3%20descriptors%20refined%20through%20a%20confidence-aware%20inlier%20classifier%2C%20generating%20a%20well-distributed%20and%20structure-aware%20Gaussian%20seed%20prior%20to%20optimization.%20This%20initialization%20stabilizes%20early%20mapping%20and%20accelerates%20convergence%20by%20roughly%2020%5C%25%2C%20yielding%20higher%20rendering%20fidelity%20in%20texture-rich%20and%20cluttered%20scenes%20while%20remaining%20fully%20compatible%20with%20existing%20GS-SLAM%20pipelines.%20Evaluated%20on%20the%20TUM%20RGB-D%20and%20Replica%20datasets%2C%20RGS-SLAM%20achieves%20competitive%20or%20superior%20localization%20and%20reconstruction%20accuracy%20compared%20with%20state-of-the-art%20Gaussian%20and%20point-based%20SLAM%20systems%2C%20sustaining%20real-time%20mapping%20performance%20at%20up%20to%20925%20FPS.%20Additional%20details%20and%20resources%20are%20available%20at%20this%20URL%3A%20https%3A//breeze1124.github.io/rgs-slam-project-page/&entry.1838667208=http%3A//arxiv.org/abs/2601.00705v3&entry.124074799=Read"},
{"title": "RSATalker: Realistic Socially-Aware Talking Head Generation for Multi-Turn Conversation", "author": "Peng Chen and Xiaobao Wei and Yi Yang and Naiming Yao and Hui Chen and Feng Tian", "abstract": "Talking head generation is increasingly important in virtual reality (VR), especially for social scenarios involving multi-turn conversation. Existing approaches face notable limitations: mesh-based 3D methods can model dual-person dialogue but lack realistic textures, while large-model-based 2D methods produce natural appearances but incur prohibitive computational costs. Recently, 3D Gaussian Splatting (3DGS) based methods achieve efficient and realistic rendering but remain speaker-only and ignore social relationships. We introduce RSATalker, the first framework that leverages 3DGS for realistic and socially-aware talking head generation with support for multi-turn conversation. Our method first drives mesh-based 3D facial motion from speech, then binds 3D Gaussians to mesh facets to render high-fidelity 2D avatar videos. To capture interpersonal dynamics, we propose a socially-aware module that encodes social relationships, including blood and non-blood as well as equal and unequal, into high-level embeddings through a learnable query mechanism. We design a three-stage training paradigm and construct the RSATalker dataset with speech-mesh-image triplets annotated with social relationships. Extensive experiments demonstrate that RSATalker achieves state-of-the-art performance in both realism and social awareness. The code and dataset will be released.", "link": "http://arxiv.org/abs/2601.10606v1", "date": "2026-01-15", "relevancy": 3.1682, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6453}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6453}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6103}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RSATalker%3A%20Realistic%20Socially-Aware%20Talking%20Head%20Generation%20for%20Multi-Turn%20Conversation&body=Title%3A%20RSATalker%3A%20Realistic%20Socially-Aware%20Talking%20Head%20Generation%20for%20Multi-Turn%20Conversation%0AAuthor%3A%20Peng%20Chen%20and%20Xiaobao%20Wei%20and%20Yi%20Yang%20and%20Naiming%20Yao%20and%20Hui%20Chen%20and%20Feng%20Tian%0AAbstract%3A%20Talking%20head%20generation%20is%20increasingly%20important%20in%20virtual%20reality%20%28VR%29%2C%20especially%20for%20social%20scenarios%20involving%20multi-turn%20conversation.%20Existing%20approaches%20face%20notable%20limitations%3A%20mesh-based%203D%20methods%20can%20model%20dual-person%20dialogue%20but%20lack%20realistic%20textures%2C%20while%20large-model-based%202D%20methods%20produce%20natural%20appearances%20but%20incur%20prohibitive%20computational%20costs.%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20based%20methods%20achieve%20efficient%20and%20realistic%20rendering%20but%20remain%20speaker-only%20and%20ignore%20social%20relationships.%20We%20introduce%20RSATalker%2C%20the%20first%20framework%20that%20leverages%203DGS%20for%20realistic%20and%20socially-aware%20talking%20head%20generation%20with%20support%20for%20multi-turn%20conversation.%20Our%20method%20first%20drives%20mesh-based%203D%20facial%20motion%20from%20speech%2C%20then%20binds%203D%20Gaussians%20to%20mesh%20facets%20to%20render%20high-fidelity%202D%20avatar%20videos.%20To%20capture%20interpersonal%20dynamics%2C%20we%20propose%20a%20socially-aware%20module%20that%20encodes%20social%20relationships%2C%20including%20blood%20and%20non-blood%20as%20well%20as%20equal%20and%20unequal%2C%20into%20high-level%20embeddings%20through%20a%20learnable%20query%20mechanism.%20We%20design%20a%20three-stage%20training%20paradigm%20and%20construct%20the%20RSATalker%20dataset%20with%20speech-mesh-image%20triplets%20annotated%20with%20social%20relationships.%20Extensive%20experiments%20demonstrate%20that%20RSATalker%20achieves%20state-of-the-art%20performance%20in%20both%20realism%20and%20social%20awareness.%20The%20code%20and%20dataset%20will%20be%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRSATalker%253A%2520Realistic%2520Socially-Aware%2520Talking%2520Head%2520Generation%2520for%2520Multi-Turn%2520Conversation%26entry.906535625%3DPeng%2520Chen%2520and%2520Xiaobao%2520Wei%2520and%2520Yi%2520Yang%2520and%2520Naiming%2520Yao%2520and%2520Hui%2520Chen%2520and%2520Feng%2520Tian%26entry.1292438233%3DTalking%2520head%2520generation%2520is%2520increasingly%2520important%2520in%2520virtual%2520reality%2520%2528VR%2529%252C%2520especially%2520for%2520social%2520scenarios%2520involving%2520multi-turn%2520conversation.%2520Existing%2520approaches%2520face%2520notable%2520limitations%253A%2520mesh-based%25203D%2520methods%2520can%2520model%2520dual-person%2520dialogue%2520but%2520lack%2520realistic%2520textures%252C%2520while%2520large-model-based%25202D%2520methods%2520produce%2520natural%2520appearances%2520but%2520incur%2520prohibitive%2520computational%2520costs.%2520Recently%252C%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520based%2520methods%2520achieve%2520efficient%2520and%2520realistic%2520rendering%2520but%2520remain%2520speaker-only%2520and%2520ignore%2520social%2520relationships.%2520We%2520introduce%2520RSATalker%252C%2520the%2520first%2520framework%2520that%2520leverages%25203DGS%2520for%2520realistic%2520and%2520socially-aware%2520talking%2520head%2520generation%2520with%2520support%2520for%2520multi-turn%2520conversation.%2520Our%2520method%2520first%2520drives%2520mesh-based%25203D%2520facial%2520motion%2520from%2520speech%252C%2520then%2520binds%25203D%2520Gaussians%2520to%2520mesh%2520facets%2520to%2520render%2520high-fidelity%25202D%2520avatar%2520videos.%2520To%2520capture%2520interpersonal%2520dynamics%252C%2520we%2520propose%2520a%2520socially-aware%2520module%2520that%2520encodes%2520social%2520relationships%252C%2520including%2520blood%2520and%2520non-blood%2520as%2520well%2520as%2520equal%2520and%2520unequal%252C%2520into%2520high-level%2520embeddings%2520through%2520a%2520learnable%2520query%2520mechanism.%2520We%2520design%2520a%2520three-stage%2520training%2520paradigm%2520and%2520construct%2520the%2520RSATalker%2520dataset%2520with%2520speech-mesh-image%2520triplets%2520annotated%2520with%2520social%2520relationships.%2520Extensive%2520experiments%2520demonstrate%2520that%2520RSATalker%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520realism%2520and%2520social%2520awareness.%2520The%2520code%2520and%2520dataset%2520will%2520be%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RSATalker%3A%20Realistic%20Socially-Aware%20Talking%20Head%20Generation%20for%20Multi-Turn%20Conversation&entry.906535625=Peng%20Chen%20and%20Xiaobao%20Wei%20and%20Yi%20Yang%20and%20Naiming%20Yao%20and%20Hui%20Chen%20and%20Feng%20Tian&entry.1292438233=Talking%20head%20generation%20is%20increasingly%20important%20in%20virtual%20reality%20%28VR%29%2C%20especially%20for%20social%20scenarios%20involving%20multi-turn%20conversation.%20Existing%20approaches%20face%20notable%20limitations%3A%20mesh-based%203D%20methods%20can%20model%20dual-person%20dialogue%20but%20lack%20realistic%20textures%2C%20while%20large-model-based%202D%20methods%20produce%20natural%20appearances%20but%20incur%20prohibitive%20computational%20costs.%20Recently%2C%203D%20Gaussian%20Splatting%20%283DGS%29%20based%20methods%20achieve%20efficient%20and%20realistic%20rendering%20but%20remain%20speaker-only%20and%20ignore%20social%20relationships.%20We%20introduce%20RSATalker%2C%20the%20first%20framework%20that%20leverages%203DGS%20for%20realistic%20and%20socially-aware%20talking%20head%20generation%20with%20support%20for%20multi-turn%20conversation.%20Our%20method%20first%20drives%20mesh-based%203D%20facial%20motion%20from%20speech%2C%20then%20binds%203D%20Gaussians%20to%20mesh%20facets%20to%20render%20high-fidelity%202D%20avatar%20videos.%20To%20capture%20interpersonal%20dynamics%2C%20we%20propose%20a%20socially-aware%20module%20that%20encodes%20social%20relationships%2C%20including%20blood%20and%20non-blood%20as%20well%20as%20equal%20and%20unequal%2C%20into%20high-level%20embeddings%20through%20a%20learnable%20query%20mechanism.%20We%20design%20a%20three-stage%20training%20paradigm%20and%20construct%20the%20RSATalker%20dataset%20with%20speech-mesh-image%20triplets%20annotated%20with%20social%20relationships.%20Extensive%20experiments%20demonstrate%20that%20RSATalker%20achieves%20state-of-the-art%20performance%20in%20both%20realism%20and%20social%20awareness.%20The%20code%20and%20dataset%20will%20be%20released.&entry.1838667208=http%3A//arxiv.org/abs/2601.10606v1&entry.124074799=Read"},
{"title": "Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure", "author": "Luxuan Fu and Chong Liu and Bisheng Yang and Zhen Dong", "abstract": "Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.", "link": "http://arxiv.org/abs/2601.10551v1", "date": "2026-01-15", "relevancy": 2.964, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6088}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6088}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unleashing%20the%20Capabilities%20of%20Large%20Vision-Language%20Models%20for%20Intelligent%20Perception%20of%20Roadside%20Infrastructure&body=Title%3A%20Unleashing%20the%20Capabilities%20of%20Large%20Vision-Language%20Models%20for%20Intelligent%20Perception%20of%20Roadside%20Infrastructure%0AAuthor%3A%20Luxuan%20Fu%20and%20Chong%20Liu%20and%20Bisheng%20Yang%20and%20Zhen%20Dong%0AAbstract%3A%20Automated%20perception%20of%20urban%20roadside%20infrastructure%20is%20crucial%20for%20smart%20city%20management%2C%20yet%20general-purpose%20models%20often%20struggle%20to%20capture%20the%20necessary%20fine-grained%20attributes%20and%20domain%20rules.%20While%20Large%20Vision%20Language%20Models%20%28VLMs%29%20excel%20at%20open-world%20recognition%2C%20they%20often%20struggle%20to%20accurately%20interpret%20complex%20facility%20states%20in%20compliance%20with%20engineering%20standards%2C%20leading%20to%20unreliable%20performance%20in%20real-world%20applications.%20To%20address%20this%2C%20we%20propose%20a%20domain-adapted%20framework%20that%20transforms%20VLMs%20into%20specialized%20agents%20for%20intelligent%20infrastructure%20analysis.%20Our%20approach%20integrates%20a%20data-efficient%20fine-tuning%20strategy%20with%20a%20knowledge-grounded%20reasoning%20mechanism.%20Specifically%2C%20we%20leverage%20open-vocabulary%20fine-tuning%20on%20Grounding%20DINO%20to%20robustly%20localize%20diverse%20assets%20with%20minimal%20supervision%2C%20followed%20by%20LoRA-based%20adaptation%20on%20Qwen-VL%20for%20deep%20semantic%20attribute%20reasoning.%20To%20mitigate%20hallucinations%20and%20enforce%20professional%20compliance%2C%20we%20introduce%20a%20dual-modality%20Retrieval-Augmented%20Generation%20%28RAG%29%20module%20that%20dynamically%20retrieves%20authoritative%20industry%20standards%20and%20visual%20exemplars%20during%20inference.%20Evaluated%20on%20a%20comprehensive%20new%20dataset%20of%20urban%20roadside%20scenes%2C%20our%20framework%20achieves%20a%20detection%20performance%20of%2058.9%20mAP%20and%20an%20attribute%20recognition%20accuracy%20of%2095.5%25%2C%20demonstrating%20a%20robust%20solution%20for%20intelligent%20infrastructure%20monitoring.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10551v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnleashing%2520the%2520Capabilities%2520of%2520Large%2520Vision-Language%2520Models%2520for%2520Intelligent%2520Perception%2520of%2520Roadside%2520Infrastructure%26entry.906535625%3DLuxuan%2520Fu%2520and%2520Chong%2520Liu%2520and%2520Bisheng%2520Yang%2520and%2520Zhen%2520Dong%26entry.1292438233%3DAutomated%2520perception%2520of%2520urban%2520roadside%2520infrastructure%2520is%2520crucial%2520for%2520smart%2520city%2520management%252C%2520yet%2520general-purpose%2520models%2520often%2520struggle%2520to%2520capture%2520the%2520necessary%2520fine-grained%2520attributes%2520and%2520domain%2520rules.%2520While%2520Large%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%2520excel%2520at%2520open-world%2520recognition%252C%2520they%2520often%2520struggle%2520to%2520accurately%2520interpret%2520complex%2520facility%2520states%2520in%2520compliance%2520with%2520engineering%2520standards%252C%2520leading%2520to%2520unreliable%2520performance%2520in%2520real-world%2520applications.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520domain-adapted%2520framework%2520that%2520transforms%2520VLMs%2520into%2520specialized%2520agents%2520for%2520intelligent%2520infrastructure%2520analysis.%2520Our%2520approach%2520integrates%2520a%2520data-efficient%2520fine-tuning%2520strategy%2520with%2520a%2520knowledge-grounded%2520reasoning%2520mechanism.%2520Specifically%252C%2520we%2520leverage%2520open-vocabulary%2520fine-tuning%2520on%2520Grounding%2520DINO%2520to%2520robustly%2520localize%2520diverse%2520assets%2520with%2520minimal%2520supervision%252C%2520followed%2520by%2520LoRA-based%2520adaptation%2520on%2520Qwen-VL%2520for%2520deep%2520semantic%2520attribute%2520reasoning.%2520To%2520mitigate%2520hallucinations%2520and%2520enforce%2520professional%2520compliance%252C%2520we%2520introduce%2520a%2520dual-modality%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520module%2520that%2520dynamically%2520retrieves%2520authoritative%2520industry%2520standards%2520and%2520visual%2520exemplars%2520during%2520inference.%2520Evaluated%2520on%2520a%2520comprehensive%2520new%2520dataset%2520of%2520urban%2520roadside%2520scenes%252C%2520our%2520framework%2520achieves%2520a%2520detection%2520performance%2520of%252058.9%2520mAP%2520and%2520an%2520attribute%2520recognition%2520accuracy%2520of%252095.5%2525%252C%2520demonstrating%2520a%2520robust%2520solution%2520for%2520intelligent%2520infrastructure%2520monitoring.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10551v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unleashing%20the%20Capabilities%20of%20Large%20Vision-Language%20Models%20for%20Intelligent%20Perception%20of%20Roadside%20Infrastructure&entry.906535625=Luxuan%20Fu%20and%20Chong%20Liu%20and%20Bisheng%20Yang%20and%20Zhen%20Dong&entry.1292438233=Automated%20perception%20of%20urban%20roadside%20infrastructure%20is%20crucial%20for%20smart%20city%20management%2C%20yet%20general-purpose%20models%20often%20struggle%20to%20capture%20the%20necessary%20fine-grained%20attributes%20and%20domain%20rules.%20While%20Large%20Vision%20Language%20Models%20%28VLMs%29%20excel%20at%20open-world%20recognition%2C%20they%20often%20struggle%20to%20accurately%20interpret%20complex%20facility%20states%20in%20compliance%20with%20engineering%20standards%2C%20leading%20to%20unreliable%20performance%20in%20real-world%20applications.%20To%20address%20this%2C%20we%20propose%20a%20domain-adapted%20framework%20that%20transforms%20VLMs%20into%20specialized%20agents%20for%20intelligent%20infrastructure%20analysis.%20Our%20approach%20integrates%20a%20data-efficient%20fine-tuning%20strategy%20with%20a%20knowledge-grounded%20reasoning%20mechanism.%20Specifically%2C%20we%20leverage%20open-vocabulary%20fine-tuning%20on%20Grounding%20DINO%20to%20robustly%20localize%20diverse%20assets%20with%20minimal%20supervision%2C%20followed%20by%20LoRA-based%20adaptation%20on%20Qwen-VL%20for%20deep%20semantic%20attribute%20reasoning.%20To%20mitigate%20hallucinations%20and%20enforce%20professional%20compliance%2C%20we%20introduce%20a%20dual-modality%20Retrieval-Augmented%20Generation%20%28RAG%29%20module%20that%20dynamically%20retrieves%20authoritative%20industry%20standards%20and%20visual%20exemplars%20during%20inference.%20Evaluated%20on%20a%20comprehensive%20new%20dataset%20of%20urban%20roadside%20scenes%2C%20our%20framework%20achieves%20a%20detection%20performance%20of%2058.9%20mAP%20and%20an%20attribute%20recognition%20accuracy%20of%2095.5%25%2C%20demonstrating%20a%20robust%20solution%20for%20intelligent%20infrastructure%20monitoring.&entry.1838667208=http%3A//arxiv.org/abs/2601.10551v1&entry.124074799=Read"},
{"title": "SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery", "author": "Chong Liu and Luxuan Fu and Yang Jia and Zhen Dong and Bisheng Yang", "abstract": "The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.", "link": "http://arxiv.org/abs/2601.10535v1", "date": "2026-01-15", "relevancy": 2.9265, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5939}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.581}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SVII-3D%3A%20Advancing%20Roadside%20Infrastructure%20Inventory%20with%20Decimeter-level%203D%20Localization%20and%20Comprehension%20from%20Sparse%20Street%20Imagery&body=Title%3A%20SVII-3D%3A%20Advancing%20Roadside%20Infrastructure%20Inventory%20with%20Decimeter-level%203D%20Localization%20and%20Comprehension%20from%20Sparse%20Street%20Imagery%0AAuthor%3A%20Chong%20Liu%20and%20Luxuan%20Fu%20and%20Yang%20Jia%20and%20Zhen%20Dong%20and%20Bisheng%20Yang%0AAbstract%3A%20The%20automated%20creation%20of%20digital%20twins%20and%20precise%20asset%20inventories%20is%20a%20critical%20task%20in%20smart%20city%20construction%20and%20facility%20lifecycle%20management.%20However%2C%20utilizing%20cost-effective%20sparse%20imagery%20remains%20challenging%20due%20to%20limited%20robustness%2C%20inaccurate%20localization%2C%20and%20a%20lack%20of%20fine-grained%20state%20understanding.%20To%20address%20these%20limitations%2C%20SVII-3D%2C%20a%20unified%20framework%20for%20holistic%20asset%20digitization%2C%20is%20proposed.%20First%2C%20LoRA%20fine-tuned%20open-set%20detection%20is%20fused%20with%20a%20spatial-attention%20matching%20network%20to%20robustly%20associate%20observations%20across%20sparse%20views.%20Second%2C%20a%20geometry-guided%20refinement%20mechanism%20is%20introduced%20to%20resolve%20structural%20errors%2C%20achieving%20precise%20decimeter-level%203D%20localization.%20Third%2C%20transcending%20static%20geometric%20mapping%2C%20a%20Vision-Language%20Model%20agent%20leveraging%20multi-modal%20prompting%20is%20incorporated%20to%20automatically%20diagnose%20fine-grained%20operational%20states.%20Experiments%20demonstrate%20that%20SVII-3D%20significantly%20improves%20identification%20accuracy%20and%20minimizes%20localization%20errors.%20Consequently%2C%20this%20framework%20offers%20a%20scalable%2C%20cost-effective%20solution%20for%20high-fidelity%20infrastructure%20digitization%2C%20effectively%20bridging%20the%20gap%20between%20sparse%20perception%20and%20automated%20intelligent%20maintenance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSVII-3D%253A%2520Advancing%2520Roadside%2520Infrastructure%2520Inventory%2520with%2520Decimeter-level%25203D%2520Localization%2520and%2520Comprehension%2520from%2520Sparse%2520Street%2520Imagery%26entry.906535625%3DChong%2520Liu%2520and%2520Luxuan%2520Fu%2520and%2520Yang%2520Jia%2520and%2520Zhen%2520Dong%2520and%2520Bisheng%2520Yang%26entry.1292438233%3DThe%2520automated%2520creation%2520of%2520digital%2520twins%2520and%2520precise%2520asset%2520inventories%2520is%2520a%2520critical%2520task%2520in%2520smart%2520city%2520construction%2520and%2520facility%2520lifecycle%2520management.%2520However%252C%2520utilizing%2520cost-effective%2520sparse%2520imagery%2520remains%2520challenging%2520due%2520to%2520limited%2520robustness%252C%2520inaccurate%2520localization%252C%2520and%2520a%2520lack%2520of%2520fine-grained%2520state%2520understanding.%2520To%2520address%2520these%2520limitations%252C%2520SVII-3D%252C%2520a%2520unified%2520framework%2520for%2520holistic%2520asset%2520digitization%252C%2520is%2520proposed.%2520First%252C%2520LoRA%2520fine-tuned%2520open-set%2520detection%2520is%2520fused%2520with%2520a%2520spatial-attention%2520matching%2520network%2520to%2520robustly%2520associate%2520observations%2520across%2520sparse%2520views.%2520Second%252C%2520a%2520geometry-guided%2520refinement%2520mechanism%2520is%2520introduced%2520to%2520resolve%2520structural%2520errors%252C%2520achieving%2520precise%2520decimeter-level%25203D%2520localization.%2520Third%252C%2520transcending%2520static%2520geometric%2520mapping%252C%2520a%2520Vision-Language%2520Model%2520agent%2520leveraging%2520multi-modal%2520prompting%2520is%2520incorporated%2520to%2520automatically%2520diagnose%2520fine-grained%2520operational%2520states.%2520Experiments%2520demonstrate%2520that%2520SVII-3D%2520significantly%2520improves%2520identification%2520accuracy%2520and%2520minimizes%2520localization%2520errors.%2520Consequently%252C%2520this%2520framework%2520offers%2520a%2520scalable%252C%2520cost-effective%2520solution%2520for%2520high-fidelity%2520infrastructure%2520digitization%252C%2520effectively%2520bridging%2520the%2520gap%2520between%2520sparse%2520perception%2520and%2520automated%2520intelligent%2520maintenance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SVII-3D%3A%20Advancing%20Roadside%20Infrastructure%20Inventory%20with%20Decimeter-level%203D%20Localization%20and%20Comprehension%20from%20Sparse%20Street%20Imagery&entry.906535625=Chong%20Liu%20and%20Luxuan%20Fu%20and%20Yang%20Jia%20and%20Zhen%20Dong%20and%20Bisheng%20Yang&entry.1292438233=The%20automated%20creation%20of%20digital%20twins%20and%20precise%20asset%20inventories%20is%20a%20critical%20task%20in%20smart%20city%20construction%20and%20facility%20lifecycle%20management.%20However%2C%20utilizing%20cost-effective%20sparse%20imagery%20remains%20challenging%20due%20to%20limited%20robustness%2C%20inaccurate%20localization%2C%20and%20a%20lack%20of%20fine-grained%20state%20understanding.%20To%20address%20these%20limitations%2C%20SVII-3D%2C%20a%20unified%20framework%20for%20holistic%20asset%20digitization%2C%20is%20proposed.%20First%2C%20LoRA%20fine-tuned%20open-set%20detection%20is%20fused%20with%20a%20spatial-attention%20matching%20network%20to%20robustly%20associate%20observations%20across%20sparse%20views.%20Second%2C%20a%20geometry-guided%20refinement%20mechanism%20is%20introduced%20to%20resolve%20structural%20errors%2C%20achieving%20precise%20decimeter-level%203D%20localization.%20Third%2C%20transcending%20static%20geometric%20mapping%2C%20a%20Vision-Language%20Model%20agent%20leveraging%20multi-modal%20prompting%20is%20incorporated%20to%20automatically%20diagnose%20fine-grained%20operational%20states.%20Experiments%20demonstrate%20that%20SVII-3D%20significantly%20improves%20identification%20accuracy%20and%20minimizes%20localization%20errors.%20Consequently%2C%20this%20framework%20offers%20a%20scalable%2C%20cost-effective%20solution%20for%20high-fidelity%20infrastructure%20digitization%2C%20effectively%20bridging%20the%20gap%20between%20sparse%20perception%20and%20automated%20intelligent%20maintenance.&entry.1838667208=http%3A//arxiv.org/abs/2601.10535v1&entry.124074799=Read"},
{"title": "From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion", "author": "Cheng Chen and Yuyu Guo and Pengpeng Zeng and Jingkuan Song and Peng Di and Hang Yu and Lianli Gao", "abstract": "Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.", "link": "http://arxiv.org/abs/2601.10710v1", "date": "2026-01-15", "relevancy": 2.9111, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5842}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5812}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20One-to-One%20to%20Many-to-Many%3A%20Dynamic%20Cross-Layer%20Injection%20for%20Deep%20Vision-Language%20Fusion&body=Title%3A%20From%20One-to-One%20to%20Many-to-Many%3A%20Dynamic%20Cross-Layer%20Injection%20for%20Deep%20Vision-Language%20Fusion%0AAuthor%3A%20Cheng%20Chen%20and%20Yuyu%20Guo%20and%20Pengpeng%20Zeng%20and%20Jingkuan%20Song%20and%20Peng%20Di%20and%20Hang%20Yu%20and%20Lianli%20Gao%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20create%20a%20severe%20visual%20feature%20bottleneck%20by%20using%20a%20crude%2C%20asymmetric%20connection%20that%20links%20only%20the%20output%20of%20the%20vision%20encoder%20to%20the%20input%20of%20the%20large%20language%20model%20%28LLM%29.%20This%20static%20architecture%20fundamentally%20limits%20the%20ability%20of%20LLMs%20to%20achieve%20comprehensive%20alignment%20with%20hierarchical%20visual%20knowledge%2C%20compromising%20their%20capacity%20to%20accurately%20integrate%20local%20details%20with%20global%20semantics%20into%20coherent%20reasoning.%20To%20resolve%20this%2C%20we%20introduce%20Cross-Layer%20Injection%20%28CLI%29%2C%20a%20novel%20and%20lightweight%20framework%20that%20forges%20a%20dynamic%20many-to-many%20bridge%20between%20the%20two%20modalities.%20CLI%20consists%20of%20two%20synergistic%2C%20parameter-efficient%20components%3A%20an%20Adaptive%20Multi-Projection%20%28AMP%29%20module%20that%20harmonizes%20features%20from%20diverse%20vision%20layers%2C%20and%20an%20Adaptive%20Gating%20Fusion%20%28AGF%29%20mechanism%20that%20empowers%20the%20LLM%20to%20selectively%20inject%20the%20most%20relevant%20visual%20information%20based%20on%20its%20real-time%20decoding%20context.%20We%20validate%20the%20effectiveness%20and%20versatility%20of%20CLI%20by%20integrating%20it%20into%20LLaVA-OneVision%20and%20LLaVA-1.5.%20Extensive%20experiments%20on%2018%20diverse%20benchmarks%20demonstrate%20significant%20performance%20improvements%2C%20establishing%20CLI%20as%20a%20scalable%20paradigm%20that%20unlocks%20deeper%20multimodal%20understanding%20by%20granting%20LLMs%20on-demand%20access%20to%20the%20full%20visual%20hierarchy.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10710v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520One-to-One%2520to%2520Many-to-Many%253A%2520Dynamic%2520Cross-Layer%2520Injection%2520for%2520Deep%2520Vision-Language%2520Fusion%26entry.906535625%3DCheng%2520Chen%2520and%2520Yuyu%2520Guo%2520and%2520Pengpeng%2520Zeng%2520and%2520Jingkuan%2520Song%2520and%2520Peng%2520Di%2520and%2520Hang%2520Yu%2520and%2520Lianli%2520Gao%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520create%2520a%2520severe%2520visual%2520feature%2520bottleneck%2520by%2520using%2520a%2520crude%252C%2520asymmetric%2520connection%2520that%2520links%2520only%2520the%2520output%2520of%2520the%2520vision%2520encoder%2520to%2520the%2520input%2520of%2520the%2520large%2520language%2520model%2520%2528LLM%2529.%2520This%2520static%2520architecture%2520fundamentally%2520limits%2520the%2520ability%2520of%2520LLMs%2520to%2520achieve%2520comprehensive%2520alignment%2520with%2520hierarchical%2520visual%2520knowledge%252C%2520compromising%2520their%2520capacity%2520to%2520accurately%2520integrate%2520local%2520details%2520with%2520global%2520semantics%2520into%2520coherent%2520reasoning.%2520To%2520resolve%2520this%252C%2520we%2520introduce%2520Cross-Layer%2520Injection%2520%2528CLI%2529%252C%2520a%2520novel%2520and%2520lightweight%2520framework%2520that%2520forges%2520a%2520dynamic%2520many-to-many%2520bridge%2520between%2520the%2520two%2520modalities.%2520CLI%2520consists%2520of%2520two%2520synergistic%252C%2520parameter-efficient%2520components%253A%2520an%2520Adaptive%2520Multi-Projection%2520%2528AMP%2529%2520module%2520that%2520harmonizes%2520features%2520from%2520diverse%2520vision%2520layers%252C%2520and%2520an%2520Adaptive%2520Gating%2520Fusion%2520%2528AGF%2529%2520mechanism%2520that%2520empowers%2520the%2520LLM%2520to%2520selectively%2520inject%2520the%2520most%2520relevant%2520visual%2520information%2520based%2520on%2520its%2520real-time%2520decoding%2520context.%2520We%2520validate%2520the%2520effectiveness%2520and%2520versatility%2520of%2520CLI%2520by%2520integrating%2520it%2520into%2520LLaVA-OneVision%2520and%2520LLaVA-1.5.%2520Extensive%2520experiments%2520on%252018%2520diverse%2520benchmarks%2520demonstrate%2520significant%2520performance%2520improvements%252C%2520establishing%2520CLI%2520as%2520a%2520scalable%2520paradigm%2520that%2520unlocks%2520deeper%2520multimodal%2520understanding%2520by%2520granting%2520LLMs%2520on-demand%2520access%2520to%2520the%2520full%2520visual%2520hierarchy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10710v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20One-to-One%20to%20Many-to-Many%3A%20Dynamic%20Cross-Layer%20Injection%20for%20Deep%20Vision-Language%20Fusion&entry.906535625=Cheng%20Chen%20and%20Yuyu%20Guo%20and%20Pengpeng%20Zeng%20and%20Jingkuan%20Song%20and%20Peng%20Di%20and%20Hang%20Yu%20and%20Lianli%20Gao&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20create%20a%20severe%20visual%20feature%20bottleneck%20by%20using%20a%20crude%2C%20asymmetric%20connection%20that%20links%20only%20the%20output%20of%20the%20vision%20encoder%20to%20the%20input%20of%20the%20large%20language%20model%20%28LLM%29.%20This%20static%20architecture%20fundamentally%20limits%20the%20ability%20of%20LLMs%20to%20achieve%20comprehensive%20alignment%20with%20hierarchical%20visual%20knowledge%2C%20compromising%20their%20capacity%20to%20accurately%20integrate%20local%20details%20with%20global%20semantics%20into%20coherent%20reasoning.%20To%20resolve%20this%2C%20we%20introduce%20Cross-Layer%20Injection%20%28CLI%29%2C%20a%20novel%20and%20lightweight%20framework%20that%20forges%20a%20dynamic%20many-to-many%20bridge%20between%20the%20two%20modalities.%20CLI%20consists%20of%20two%20synergistic%2C%20parameter-efficient%20components%3A%20an%20Adaptive%20Multi-Projection%20%28AMP%29%20module%20that%20harmonizes%20features%20from%20diverse%20vision%20layers%2C%20and%20an%20Adaptive%20Gating%20Fusion%20%28AGF%29%20mechanism%20that%20empowers%20the%20LLM%20to%20selectively%20inject%20the%20most%20relevant%20visual%20information%20based%20on%20its%20real-time%20decoding%20context.%20We%20validate%20the%20effectiveness%20and%20versatility%20of%20CLI%20by%20integrating%20it%20into%20LLaVA-OneVision%20and%20LLaVA-1.5.%20Extensive%20experiments%20on%2018%20diverse%20benchmarks%20demonstrate%20significant%20performance%20improvements%2C%20establishing%20CLI%20as%20a%20scalable%20paradigm%20that%20unlocks%20deeper%20multimodal%20understanding%20by%20granting%20LLMs%20on-demand%20access%20to%20the%20full%20visual%20hierarchy.&entry.1838667208=http%3A//arxiv.org/abs/2601.10710v1&entry.124074799=Read"},
{"title": "Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models", "author": "Mikel Williams-Lekuona and Georgina Cosma", "abstract": "Vision transformers in vision-language models typically use the same amount of compute for every image, regardless of whether it is simple or complex. We propose ICAR (Image Complexity-Aware Retrieval), an adaptive computation approach that enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both the early-exit and full-depth paths. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance, attaining a Pearson correlation coefficient of 0.959 with human labelling whilst delivering 4.4x faster complexity prediction. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% faster image encoding while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.", "link": "http://arxiv.org/abs/2512.15372v2", "date": "2026-01-15", "relevancy": 2.8799, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5823}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5823}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5633}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Image%20Complexity-Aware%20Adaptive%20Retrieval%20for%20Efficient%20Vision-Language%20Models&body=Title%3A%20Image%20Complexity-Aware%20Adaptive%20Retrieval%20for%20Efficient%20Vision-Language%20Models%0AAuthor%3A%20Mikel%20Williams-Lekuona%20and%20Georgina%20Cosma%0AAbstract%3A%20Vision%20transformers%20in%20vision-language%20models%20typically%20use%20the%20same%20amount%20of%20compute%20for%20every%20image%2C%20regardless%20of%20whether%20it%20is%20simple%20or%20complex.%20We%20propose%20ICAR%20%28Image%20Complexity-Aware%20Retrieval%29%2C%20an%20adaptive%20computation%20approach%20that%20enables%20vision%20transformers%20to%20use%20less%20compute%20for%20simple%20images%20whilst%20processing%20complex%20images%20through%20their%20full%20network%20depth.%20The%20key%20challenge%20is%20maintaining%20cross-modal%20alignment%3A%20embeddings%20from%20different%20processing%20depths%20must%20remain%20compatible%20for%20text%20matching.%20ICAR%20solves%20this%20through%20dual-path%20training%20that%20produces%20compatible%20embeddings%20from%20both%20the%20early-exit%20and%20full-depth%20paths.%20This%20maintains%20compatibility%20between%20image%20representations%20and%20text%20embeddings%20in%20the%20same%20semantic%20space%2C%20whether%20an%20image%20exits%20early%20or%20processes%20fully.%20Unlike%20existing%20two-stage%20approaches%20that%20require%20expensive%20reranking%2C%20ICAR%20enables%20direct%20image-text%20matching%20without%20additional%20overhead.%20To%20determine%20how%20much%20compute%20to%20use%2C%20we%20develop%20ConvNeXt-IC%2C%20which%20treats%20image%20complexity%20assessment%20as%20a%20classification%20task.%20By%20applying%20modern%20classifier%20backbones%20rather%20than%20specialised%20architectures%2C%20ConvNeXt-IC%20achieves%20state-of-the-art%20performance%2C%20attaining%20a%20Pearson%20correlation%20coefficient%20of%200.959%20with%20human%20labelling%20whilst%20delivering%204.4x%20faster%20complexity%20prediction.%20Evaluated%20on%20standard%20benchmarks%20augmented%20with%20real-world%20web%20data%2C%20ICAR%20achieves%2020%25%20faster%20image%20encoding%20while%20maintaining%20category-level%20performance%20and%2095%25%20of%20instance-level%20performance%2C%20enabling%20sustainable%20scaling%20of%20vision-language%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.15372v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImage%2520Complexity-Aware%2520Adaptive%2520Retrieval%2520for%2520Efficient%2520Vision-Language%2520Models%26entry.906535625%3DMikel%2520Williams-Lekuona%2520and%2520Georgina%2520Cosma%26entry.1292438233%3DVision%2520transformers%2520in%2520vision-language%2520models%2520typically%2520use%2520the%2520same%2520amount%2520of%2520compute%2520for%2520every%2520image%252C%2520regardless%2520of%2520whether%2520it%2520is%2520simple%2520or%2520complex.%2520We%2520propose%2520ICAR%2520%2528Image%2520Complexity-Aware%2520Retrieval%2529%252C%2520an%2520adaptive%2520computation%2520approach%2520that%2520enables%2520vision%2520transformers%2520to%2520use%2520less%2520compute%2520for%2520simple%2520images%2520whilst%2520processing%2520complex%2520images%2520through%2520their%2520full%2520network%2520depth.%2520The%2520key%2520challenge%2520is%2520maintaining%2520cross-modal%2520alignment%253A%2520embeddings%2520from%2520different%2520processing%2520depths%2520must%2520remain%2520compatible%2520for%2520text%2520matching.%2520ICAR%2520solves%2520this%2520through%2520dual-path%2520training%2520that%2520produces%2520compatible%2520embeddings%2520from%2520both%2520the%2520early-exit%2520and%2520full-depth%2520paths.%2520This%2520maintains%2520compatibility%2520between%2520image%2520representations%2520and%2520text%2520embeddings%2520in%2520the%2520same%2520semantic%2520space%252C%2520whether%2520an%2520image%2520exits%2520early%2520or%2520processes%2520fully.%2520Unlike%2520existing%2520two-stage%2520approaches%2520that%2520require%2520expensive%2520reranking%252C%2520ICAR%2520enables%2520direct%2520image-text%2520matching%2520without%2520additional%2520overhead.%2520To%2520determine%2520how%2520much%2520compute%2520to%2520use%252C%2520we%2520develop%2520ConvNeXt-IC%252C%2520which%2520treats%2520image%2520complexity%2520assessment%2520as%2520a%2520classification%2520task.%2520By%2520applying%2520modern%2520classifier%2520backbones%2520rather%2520than%2520specialised%2520architectures%252C%2520ConvNeXt-IC%2520achieves%2520state-of-the-art%2520performance%252C%2520attaining%2520a%2520Pearson%2520correlation%2520coefficient%2520of%25200.959%2520with%2520human%2520labelling%2520whilst%2520delivering%25204.4x%2520faster%2520complexity%2520prediction.%2520Evaluated%2520on%2520standard%2520benchmarks%2520augmented%2520with%2520real-world%2520web%2520data%252C%2520ICAR%2520achieves%252020%2525%2520faster%2520image%2520encoding%2520while%2520maintaining%2520category-level%2520performance%2520and%252095%2525%2520of%2520instance-level%2520performance%252C%2520enabling%2520sustainable%2520scaling%2520of%2520vision-language%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.15372v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Image%20Complexity-Aware%20Adaptive%20Retrieval%20for%20Efficient%20Vision-Language%20Models&entry.906535625=Mikel%20Williams-Lekuona%20and%20Georgina%20Cosma&entry.1292438233=Vision%20transformers%20in%20vision-language%20models%20typically%20use%20the%20same%20amount%20of%20compute%20for%20every%20image%2C%20regardless%20of%20whether%20it%20is%20simple%20or%20complex.%20We%20propose%20ICAR%20%28Image%20Complexity-Aware%20Retrieval%29%2C%20an%20adaptive%20computation%20approach%20that%20enables%20vision%20transformers%20to%20use%20less%20compute%20for%20simple%20images%20whilst%20processing%20complex%20images%20through%20their%20full%20network%20depth.%20The%20key%20challenge%20is%20maintaining%20cross-modal%20alignment%3A%20embeddings%20from%20different%20processing%20depths%20must%20remain%20compatible%20for%20text%20matching.%20ICAR%20solves%20this%20through%20dual-path%20training%20that%20produces%20compatible%20embeddings%20from%20both%20the%20early-exit%20and%20full-depth%20paths.%20This%20maintains%20compatibility%20between%20image%20representations%20and%20text%20embeddings%20in%20the%20same%20semantic%20space%2C%20whether%20an%20image%20exits%20early%20or%20processes%20fully.%20Unlike%20existing%20two-stage%20approaches%20that%20require%20expensive%20reranking%2C%20ICAR%20enables%20direct%20image-text%20matching%20without%20additional%20overhead.%20To%20determine%20how%20much%20compute%20to%20use%2C%20we%20develop%20ConvNeXt-IC%2C%20which%20treats%20image%20complexity%20assessment%20as%20a%20classification%20task.%20By%20applying%20modern%20classifier%20backbones%20rather%20than%20specialised%20architectures%2C%20ConvNeXt-IC%20achieves%20state-of-the-art%20performance%2C%20attaining%20a%20Pearson%20correlation%20coefficient%20of%200.959%20with%20human%20labelling%20whilst%20delivering%204.4x%20faster%20complexity%20prediction.%20Evaluated%20on%20standard%20benchmarks%20augmented%20with%20real-world%20web%20data%2C%20ICAR%20achieves%2020%25%20faster%20image%20encoding%20while%20maintaining%20category-level%20performance%20and%2095%25%20of%20instance-level%20performance%2C%20enabling%20sustainable%20scaling%20of%20vision-language%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.15372v2&entry.124074799=Read"},
{"title": "LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked Entities", "author": "Florian Sestak and Artur Toshev and Andreas F\u00fcrst and G\u00fcnter Klambauer and Andreas Mayr and Johannes Brandstetter", "abstract": "Generative models are spearheading recent progress in deep learning, showcasing strong promise for trajectory sampling in dynamical systems as well. However, whereas latent space modeling paradigms have transformed image and video generation, similar approaches are more difficult for most dynamical systems. Such systems -- from chemical molecule structures to collective human behavior -- are described by interactions of entities, making them inherently linked to connectivity patterns, entity conservation, and the traceability of entities over time. Our approach, LaM-SLidE (Latent Space Modeling of Spatial Dynamical Systems via Linked Entities), bridges the gap between: (1) keeping the traceability of individual entities in a latent system representation, and (2) leveraging the efficiency and scalability of recent advances in image and video generation, where pre-trained encoder and decoder enable generative modeling directly in latent space. The core idea of LaM-SLidE is the introduction of identifier representations (IDs) that enable the retrieval of entity properties and entity composition from latent system representations, thus fostering traceability. Experimentally, across different domains, we show that LaM-SLidE performs favorably in terms of speed, accuracy, and generalizability. Code is available at https://github.com/ml-jku/LaM-SLidE .", "link": "http://arxiv.org/abs/2502.12128v5", "date": "2026-01-15", "relevancy": 2.832, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5835}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.565}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5507}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LaM-SLidE%3A%20Latent%20Space%20Modeling%20of%20Spatial%20Dynamical%20Systems%20via%20Linked%20Entities&body=Title%3A%20LaM-SLidE%3A%20Latent%20Space%20Modeling%20of%20Spatial%20Dynamical%20Systems%20via%20Linked%20Entities%0AAuthor%3A%20Florian%20Sestak%20and%20Artur%20Toshev%20and%20Andreas%20F%C3%BCrst%20and%20G%C3%BCnter%20Klambauer%20and%20Andreas%20Mayr%20and%20Johannes%20Brandstetter%0AAbstract%3A%20Generative%20models%20are%20spearheading%20recent%20progress%20in%20deep%20learning%2C%20showcasing%20strong%20promise%20for%20trajectory%20sampling%20in%20dynamical%20systems%20as%20well.%20However%2C%20whereas%20latent%20space%20modeling%20paradigms%20have%20transformed%20image%20and%20video%20generation%2C%20similar%20approaches%20are%20more%20difficult%20for%20most%20dynamical%20systems.%20Such%20systems%20--%20from%20chemical%20molecule%20structures%20to%20collective%20human%20behavior%20--%20are%20described%20by%20interactions%20of%20entities%2C%20making%20them%20inherently%20linked%20to%20connectivity%20patterns%2C%20entity%20conservation%2C%20and%20the%20traceability%20of%20entities%20over%20time.%20Our%20approach%2C%20LaM-SLidE%20%28Latent%20Space%20Modeling%20of%20Spatial%20Dynamical%20Systems%20via%20Linked%20Entities%29%2C%20bridges%20the%20gap%20between%3A%20%281%29%20keeping%20the%20traceability%20of%20individual%20entities%20in%20a%20latent%20system%20representation%2C%20and%20%282%29%20leveraging%20the%20efficiency%20and%20scalability%20of%20recent%20advances%20in%20image%20and%20video%20generation%2C%20where%20pre-trained%20encoder%20and%20decoder%20enable%20generative%20modeling%20directly%20in%20latent%20space.%20The%20core%20idea%20of%20LaM-SLidE%20is%20the%20introduction%20of%20identifier%20representations%20%28IDs%29%20that%20enable%20the%20retrieval%20of%20entity%20properties%20and%20entity%20composition%20from%20latent%20system%20representations%2C%20thus%20fostering%20traceability.%20Experimentally%2C%20across%20different%20domains%2C%20we%20show%20that%20LaM-SLidE%20performs%20favorably%20in%20terms%20of%20speed%2C%20accuracy%2C%20and%20generalizability.%20Code%20is%20available%20at%20https%3A//github.com/ml-jku/LaM-SLidE%20.%0ALink%3A%20http%3A//arxiv.org/abs/2502.12128v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLaM-SLidE%253A%2520Latent%2520Space%2520Modeling%2520of%2520Spatial%2520Dynamical%2520Systems%2520via%2520Linked%2520Entities%26entry.906535625%3DFlorian%2520Sestak%2520and%2520Artur%2520Toshev%2520and%2520Andreas%2520F%25C3%25BCrst%2520and%2520G%25C3%25BCnter%2520Klambauer%2520and%2520Andreas%2520Mayr%2520and%2520Johannes%2520Brandstetter%26entry.1292438233%3DGenerative%2520models%2520are%2520spearheading%2520recent%2520progress%2520in%2520deep%2520learning%252C%2520showcasing%2520strong%2520promise%2520for%2520trajectory%2520sampling%2520in%2520dynamical%2520systems%2520as%2520well.%2520However%252C%2520whereas%2520latent%2520space%2520modeling%2520paradigms%2520have%2520transformed%2520image%2520and%2520video%2520generation%252C%2520similar%2520approaches%2520are%2520more%2520difficult%2520for%2520most%2520dynamical%2520systems.%2520Such%2520systems%2520--%2520from%2520chemical%2520molecule%2520structures%2520to%2520collective%2520human%2520behavior%2520--%2520are%2520described%2520by%2520interactions%2520of%2520entities%252C%2520making%2520them%2520inherently%2520linked%2520to%2520connectivity%2520patterns%252C%2520entity%2520conservation%252C%2520and%2520the%2520traceability%2520of%2520entities%2520over%2520time.%2520Our%2520approach%252C%2520LaM-SLidE%2520%2528Latent%2520Space%2520Modeling%2520of%2520Spatial%2520Dynamical%2520Systems%2520via%2520Linked%2520Entities%2529%252C%2520bridges%2520the%2520gap%2520between%253A%2520%25281%2529%2520keeping%2520the%2520traceability%2520of%2520individual%2520entities%2520in%2520a%2520latent%2520system%2520representation%252C%2520and%2520%25282%2529%2520leveraging%2520the%2520efficiency%2520and%2520scalability%2520of%2520recent%2520advances%2520in%2520image%2520and%2520video%2520generation%252C%2520where%2520pre-trained%2520encoder%2520and%2520decoder%2520enable%2520generative%2520modeling%2520directly%2520in%2520latent%2520space.%2520The%2520core%2520idea%2520of%2520LaM-SLidE%2520is%2520the%2520introduction%2520of%2520identifier%2520representations%2520%2528IDs%2529%2520that%2520enable%2520the%2520retrieval%2520of%2520entity%2520properties%2520and%2520entity%2520composition%2520from%2520latent%2520system%2520representations%252C%2520thus%2520fostering%2520traceability.%2520Experimentally%252C%2520across%2520different%2520domains%252C%2520we%2520show%2520that%2520LaM-SLidE%2520performs%2520favorably%2520in%2520terms%2520of%2520speed%252C%2520accuracy%252C%2520and%2520generalizability.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/ml-jku/LaM-SLidE%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12128v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LaM-SLidE%3A%20Latent%20Space%20Modeling%20of%20Spatial%20Dynamical%20Systems%20via%20Linked%20Entities&entry.906535625=Florian%20Sestak%20and%20Artur%20Toshev%20and%20Andreas%20F%C3%BCrst%20and%20G%C3%BCnter%20Klambauer%20and%20Andreas%20Mayr%20and%20Johannes%20Brandstetter&entry.1292438233=Generative%20models%20are%20spearheading%20recent%20progress%20in%20deep%20learning%2C%20showcasing%20strong%20promise%20for%20trajectory%20sampling%20in%20dynamical%20systems%20as%20well.%20However%2C%20whereas%20latent%20space%20modeling%20paradigms%20have%20transformed%20image%20and%20video%20generation%2C%20similar%20approaches%20are%20more%20difficult%20for%20most%20dynamical%20systems.%20Such%20systems%20--%20from%20chemical%20molecule%20structures%20to%20collective%20human%20behavior%20--%20are%20described%20by%20interactions%20of%20entities%2C%20making%20them%20inherently%20linked%20to%20connectivity%20patterns%2C%20entity%20conservation%2C%20and%20the%20traceability%20of%20entities%20over%20time.%20Our%20approach%2C%20LaM-SLidE%20%28Latent%20Space%20Modeling%20of%20Spatial%20Dynamical%20Systems%20via%20Linked%20Entities%29%2C%20bridges%20the%20gap%20between%3A%20%281%29%20keeping%20the%20traceability%20of%20individual%20entities%20in%20a%20latent%20system%20representation%2C%20and%20%282%29%20leveraging%20the%20efficiency%20and%20scalability%20of%20recent%20advances%20in%20image%20and%20video%20generation%2C%20where%20pre-trained%20encoder%20and%20decoder%20enable%20generative%20modeling%20directly%20in%20latent%20space.%20The%20core%20idea%20of%20LaM-SLidE%20is%20the%20introduction%20of%20identifier%20representations%20%28IDs%29%20that%20enable%20the%20retrieval%20of%20entity%20properties%20and%20entity%20composition%20from%20latent%20system%20representations%2C%20thus%20fostering%20traceability.%20Experimentally%2C%20across%20different%20domains%2C%20we%20show%20that%20LaM-SLidE%20performs%20favorably%20in%20terms%20of%20speed%2C%20accuracy%2C%20and%20generalizability.%20Code%20is%20available%20at%20https%3A//github.com/ml-jku/LaM-SLidE%20.&entry.1838667208=http%3A//arxiv.org/abs/2502.12128v5&entry.124074799=Read"},
{"title": "CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning", "author": "Darshan Singh and Arsha Nagrani and Kawshik Manikantan and Harman Singh and Dinesh Tewari and Tobias Weyand and Cordelia Schmid and Anelia Angelova and Shachi Dave", "abstract": "Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE's reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\\#minerva-cultural", "link": "http://arxiv.org/abs/2601.10649v1", "date": "2026-01-15", "relevancy": 2.8108, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5783}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5783}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5299}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CURVE%3A%20A%20Benchmark%20for%20Cultural%20and%20Multilingual%20Long%20Video%20Reasoning&body=Title%3A%20CURVE%3A%20A%20Benchmark%20for%20Cultural%20and%20Multilingual%20Long%20Video%20Reasoning%0AAuthor%3A%20Darshan%20Singh%20and%20Arsha%20Nagrani%20and%20Kawshik%20Manikantan%20and%20Harman%20Singh%20and%20Dinesh%20Tewari%20and%20Tobias%20Weyand%20and%20Cordelia%20Schmid%20and%20Anelia%20Angelova%20and%20Shachi%20Dave%0AAbstract%3A%20Recent%20advancements%20in%20video%20models%20have%20shown%20tremendous%20progress%2C%20particularly%20in%20long%20video%20understanding.%20However%2C%20current%20benchmarks%20predominantly%20feature%20western-centric%20data%20and%20English%20as%20the%20dominant%20language%2C%20introducing%20significant%20biases%20in%20evaluation.%20To%20address%20this%2C%20we%20introduce%20CURVE%20%28Cultural%20Understanding%20and%20Reasoning%20in%20Video%20Evaluation%29%2C%20a%20challenging%20benchmark%20for%20multicultural%20and%20multilingual%20video%20reasoning.%20CURVE%20comprises%20high-quality%2C%20entirely%20human-generated%20annotations%20from%20diverse%2C%20region-specific%20cultural%20videos%20across%2018%20global%20locales.%20Unlike%20prior%20work%20that%20relies%20on%20automatic%20translations%2C%20CURVE%20provides%20complex%20questions%2C%20answers%2C%20and%20multi-step%20reasoning%20steps%2C%20all%20crafted%20in%20native%20languages.%20Making%20progress%20on%20CURVE%20requires%20a%20deeply%20situated%20understanding%20of%20visual%20cultural%20context.%20Furthermore%2C%20we%20leverage%20CURVE%27s%20reasoning%20traces%20to%20construct%20evidence-based%20graphs%20and%20propose%20a%20novel%20iterative%20strategy%20using%20these%20graphs%20to%20identify%20fine-grained%20errors%20in%20reasoning.%20Our%20evaluations%20reveal%20that%20SoTA%20Video-LLMs%20struggle%20significantly%2C%20performing%20substantially%20below%20human-level%20accuracy%2C%20with%20errors%20primarily%20stemming%20from%20the%20visual%20perception%20of%20cultural%20elements.%20CURVE%20will%20be%20publicly%20available%20under%20https%3A//github.com/google-deepmind/neptune%3Ftab%3Dreadme-ov-file%5C%23minerva-cultural%0ALink%3A%20http%3A//arxiv.org/abs/2601.10649v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCURVE%253A%2520A%2520Benchmark%2520for%2520Cultural%2520and%2520Multilingual%2520Long%2520Video%2520Reasoning%26entry.906535625%3DDarshan%2520Singh%2520and%2520Arsha%2520Nagrani%2520and%2520Kawshik%2520Manikantan%2520and%2520Harman%2520Singh%2520and%2520Dinesh%2520Tewari%2520and%2520Tobias%2520Weyand%2520and%2520Cordelia%2520Schmid%2520and%2520Anelia%2520Angelova%2520and%2520Shachi%2520Dave%26entry.1292438233%3DRecent%2520advancements%2520in%2520video%2520models%2520have%2520shown%2520tremendous%2520progress%252C%2520particularly%2520in%2520long%2520video%2520understanding.%2520However%252C%2520current%2520benchmarks%2520predominantly%2520feature%2520western-centric%2520data%2520and%2520English%2520as%2520the%2520dominant%2520language%252C%2520introducing%2520significant%2520biases%2520in%2520evaluation.%2520To%2520address%2520this%252C%2520we%2520introduce%2520CURVE%2520%2528Cultural%2520Understanding%2520and%2520Reasoning%2520in%2520Video%2520Evaluation%2529%252C%2520a%2520challenging%2520benchmark%2520for%2520multicultural%2520and%2520multilingual%2520video%2520reasoning.%2520CURVE%2520comprises%2520high-quality%252C%2520entirely%2520human-generated%2520annotations%2520from%2520diverse%252C%2520region-specific%2520cultural%2520videos%2520across%252018%2520global%2520locales.%2520Unlike%2520prior%2520work%2520that%2520relies%2520on%2520automatic%2520translations%252C%2520CURVE%2520provides%2520complex%2520questions%252C%2520answers%252C%2520and%2520multi-step%2520reasoning%2520steps%252C%2520all%2520crafted%2520in%2520native%2520languages.%2520Making%2520progress%2520on%2520CURVE%2520requires%2520a%2520deeply%2520situated%2520understanding%2520of%2520visual%2520cultural%2520context.%2520Furthermore%252C%2520we%2520leverage%2520CURVE%2527s%2520reasoning%2520traces%2520to%2520construct%2520evidence-based%2520graphs%2520and%2520propose%2520a%2520novel%2520iterative%2520strategy%2520using%2520these%2520graphs%2520to%2520identify%2520fine-grained%2520errors%2520in%2520reasoning.%2520Our%2520evaluations%2520reveal%2520that%2520SoTA%2520Video-LLMs%2520struggle%2520significantly%252C%2520performing%2520substantially%2520below%2520human-level%2520accuracy%252C%2520with%2520errors%2520primarily%2520stemming%2520from%2520the%2520visual%2520perception%2520of%2520cultural%2520elements.%2520CURVE%2520will%2520be%2520publicly%2520available%2520under%2520https%253A//github.com/google-deepmind/neptune%253Ftab%253Dreadme-ov-file%255C%2523minerva-cultural%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10649v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CURVE%3A%20A%20Benchmark%20for%20Cultural%20and%20Multilingual%20Long%20Video%20Reasoning&entry.906535625=Darshan%20Singh%20and%20Arsha%20Nagrani%20and%20Kawshik%20Manikantan%20and%20Harman%20Singh%20and%20Dinesh%20Tewari%20and%20Tobias%20Weyand%20and%20Cordelia%20Schmid%20and%20Anelia%20Angelova%20and%20Shachi%20Dave&entry.1292438233=Recent%20advancements%20in%20video%20models%20have%20shown%20tremendous%20progress%2C%20particularly%20in%20long%20video%20understanding.%20However%2C%20current%20benchmarks%20predominantly%20feature%20western-centric%20data%20and%20English%20as%20the%20dominant%20language%2C%20introducing%20significant%20biases%20in%20evaluation.%20To%20address%20this%2C%20we%20introduce%20CURVE%20%28Cultural%20Understanding%20and%20Reasoning%20in%20Video%20Evaluation%29%2C%20a%20challenging%20benchmark%20for%20multicultural%20and%20multilingual%20video%20reasoning.%20CURVE%20comprises%20high-quality%2C%20entirely%20human-generated%20annotations%20from%20diverse%2C%20region-specific%20cultural%20videos%20across%2018%20global%20locales.%20Unlike%20prior%20work%20that%20relies%20on%20automatic%20translations%2C%20CURVE%20provides%20complex%20questions%2C%20answers%2C%20and%20multi-step%20reasoning%20steps%2C%20all%20crafted%20in%20native%20languages.%20Making%20progress%20on%20CURVE%20requires%20a%20deeply%20situated%20understanding%20of%20visual%20cultural%20context.%20Furthermore%2C%20we%20leverage%20CURVE%27s%20reasoning%20traces%20to%20construct%20evidence-based%20graphs%20and%20propose%20a%20novel%20iterative%20strategy%20using%20these%20graphs%20to%20identify%20fine-grained%20errors%20in%20reasoning.%20Our%20evaluations%20reveal%20that%20SoTA%20Video-LLMs%20struggle%20significantly%2C%20performing%20substantially%20below%20human-level%20accuracy%2C%20with%20errors%20primarily%20stemming%20from%20the%20visual%20perception%20of%20cultural%20elements.%20CURVE%20will%20be%20publicly%20available%20under%20https%3A//github.com/google-deepmind/neptune%3Ftab%3Dreadme-ov-file%5C%23minerva-cultural&entry.1838667208=http%3A//arxiv.org/abs/2601.10649v1&entry.124074799=Read"},
{"title": "Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding", "author": "Christopher Clark and Jieyu Zhang and Zixian Ma and Jae Sung Park and Mohammadreza Salehi and Rohun Tripathi and Sangho Lee and Zhongzheng Ren and Chris Dongjoo Kim and Yinuo Yang and Vincent Shao and Yue Yang and Weikai Huang and Ziqi Gao and Taira Anderson and Jianrui Zhang and Jitesh Jain and George Stoica and Winson Han and Ali Farhadi and Ranjay Krishna", "abstract": "Today's strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&F on video tracking).", "link": "http://arxiv.org/abs/2601.10611v1", "date": "2026-01-15", "relevancy": 2.7574, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5605}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5605}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5334}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Molmo2%3A%20Open%20Weights%20and%20Data%20for%20Vision-Language%20Models%20with%20Video%20Understanding%20and%20Grounding&body=Title%3A%20Molmo2%3A%20Open%20Weights%20and%20Data%20for%20Vision-Language%20Models%20with%20Video%20Understanding%20and%20Grounding%0AAuthor%3A%20Christopher%20Clark%20and%20Jieyu%20Zhang%20and%20Zixian%20Ma%20and%20Jae%20Sung%20Park%20and%20Mohammadreza%20Salehi%20and%20Rohun%20Tripathi%20and%20Sangho%20Lee%20and%20Zhongzheng%20Ren%20and%20Chris%20Dongjoo%20Kim%20and%20Yinuo%20Yang%20and%20Vincent%20Shao%20and%20Yue%20Yang%20and%20Weikai%20Huang%20and%20Ziqi%20Gao%20and%20Taira%20Anderson%20and%20Jianrui%20Zhang%20and%20Jitesh%20Jain%20and%20George%20Stoica%20and%20Winson%20Han%20and%20Ali%20Farhadi%20and%20Ranjay%20Krishna%0AAbstract%3A%20Today%27s%20strongest%20video-language%20models%20%28VLMs%29%20remain%20proprietary.%20The%20strongest%20open-weight%20models%20either%20rely%20on%20synthetic%20data%20from%20proprietary%20VLMs%2C%20effectively%20distilling%20from%20them%2C%20or%20do%20not%20disclose%20their%20training%20data%20or%20recipe.%20As%20a%20result%2C%20the%20open-source%20community%20lacks%20the%20foundations%20needed%20to%20improve%20on%20the%20state-of-the-art%20video%20%28and%20image%29%20language%20models.%20Crucially%2C%20many%20downstream%20applications%20require%20more%20than%20just%20high-level%20video%20understanding%3B%20they%20require%20grounding%20--%20either%20by%20pointing%20or%20by%20tracking%20in%20pixels.%20Even%20proprietary%20models%20lack%20this%20capability.%20We%20present%20Molmo2%2C%20a%20new%20family%20of%20VLMs%20that%20are%20state-of-the-art%20among%20open-source%20models%20and%20demonstrate%20exceptional%20new%20capabilities%20in%20point-driven%20grounding%20in%20single%20image%2C%20multi-image%2C%20and%20video%20tasks.%20Our%20key%20contribution%20is%20a%20collection%20of%207%20new%20video%20datasets%20and%202%20multi-image%20datasets%2C%20including%20a%20dataset%20of%20highly%20detailed%20video%20captions%20for%20pre-training%2C%20a%20free-form%20video%20Q%26A%20dataset%20for%20fine-tuning%2C%20a%20new%20object%20tracking%20dataset%20with%20complex%20queries%2C%20and%20an%20innovative%20new%20video%20pointing%20dataset%2C%20all%20collected%20without%20the%20use%20of%20closed%20VLMs.%20We%20also%20present%20a%20training%20recipe%20for%20this%20data%20utilizing%20an%20efficient%20packing%20and%20message-tree%20encoding%20scheme%2C%20and%20show%20bi-directional%20attention%20on%20vision%20tokens%20and%20a%20novel%20token-weight%20strategy%20improves%20performance.%20Our%20best-in-class%208B%20model%20outperforms%20others%20in%20the%20class%20of%20open%20weight%20and%20data%20models%20on%20short%20videos%2C%20counting%2C%20and%20captioning%2C%20and%20is%20competitive%20on%20long-videos.%20On%20video-grounding%20Molmo2%20significantly%20outperforms%20existing%20open-weight%20models%20like%20Qwen3-VL%20%2835.5%20vs%2029.6%20accuracy%20on%20video%20counting%29%20and%20surpasses%20proprietary%20models%20like%20Gemini%203%20Pro%20on%20some%20tasks%20%2838.4%20vs%2020.0%20F1%20on%20video%20pointing%20and%2056.2%20vs%2041.1%20J%26F%20on%20video%20tracking%29.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10611v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMolmo2%253A%2520Open%2520Weights%2520and%2520Data%2520for%2520Vision-Language%2520Models%2520with%2520Video%2520Understanding%2520and%2520Grounding%26entry.906535625%3DChristopher%2520Clark%2520and%2520Jieyu%2520Zhang%2520and%2520Zixian%2520Ma%2520and%2520Jae%2520Sung%2520Park%2520and%2520Mohammadreza%2520Salehi%2520and%2520Rohun%2520Tripathi%2520and%2520Sangho%2520Lee%2520and%2520Zhongzheng%2520Ren%2520and%2520Chris%2520Dongjoo%2520Kim%2520and%2520Yinuo%2520Yang%2520and%2520Vincent%2520Shao%2520and%2520Yue%2520Yang%2520and%2520Weikai%2520Huang%2520and%2520Ziqi%2520Gao%2520and%2520Taira%2520Anderson%2520and%2520Jianrui%2520Zhang%2520and%2520Jitesh%2520Jain%2520and%2520George%2520Stoica%2520and%2520Winson%2520Han%2520and%2520Ali%2520Farhadi%2520and%2520Ranjay%2520Krishna%26entry.1292438233%3DToday%2527s%2520strongest%2520video-language%2520models%2520%2528VLMs%2529%2520remain%2520proprietary.%2520The%2520strongest%2520open-weight%2520models%2520either%2520rely%2520on%2520synthetic%2520data%2520from%2520proprietary%2520VLMs%252C%2520effectively%2520distilling%2520from%2520them%252C%2520or%2520do%2520not%2520disclose%2520their%2520training%2520data%2520or%2520recipe.%2520As%2520a%2520result%252C%2520the%2520open-source%2520community%2520lacks%2520the%2520foundations%2520needed%2520to%2520improve%2520on%2520the%2520state-of-the-art%2520video%2520%2528and%2520image%2529%2520language%2520models.%2520Crucially%252C%2520many%2520downstream%2520applications%2520require%2520more%2520than%2520just%2520high-level%2520video%2520understanding%253B%2520they%2520require%2520grounding%2520--%2520either%2520by%2520pointing%2520or%2520by%2520tracking%2520in%2520pixels.%2520Even%2520proprietary%2520models%2520lack%2520this%2520capability.%2520We%2520present%2520Molmo2%252C%2520a%2520new%2520family%2520of%2520VLMs%2520that%2520are%2520state-of-the-art%2520among%2520open-source%2520models%2520and%2520demonstrate%2520exceptional%2520new%2520capabilities%2520in%2520point-driven%2520grounding%2520in%2520single%2520image%252C%2520multi-image%252C%2520and%2520video%2520tasks.%2520Our%2520key%2520contribution%2520is%2520a%2520collection%2520of%25207%2520new%2520video%2520datasets%2520and%25202%2520multi-image%2520datasets%252C%2520including%2520a%2520dataset%2520of%2520highly%2520detailed%2520video%2520captions%2520for%2520pre-training%252C%2520a%2520free-form%2520video%2520Q%2526A%2520dataset%2520for%2520fine-tuning%252C%2520a%2520new%2520object%2520tracking%2520dataset%2520with%2520complex%2520queries%252C%2520and%2520an%2520innovative%2520new%2520video%2520pointing%2520dataset%252C%2520all%2520collected%2520without%2520the%2520use%2520of%2520closed%2520VLMs.%2520We%2520also%2520present%2520a%2520training%2520recipe%2520for%2520this%2520data%2520utilizing%2520an%2520efficient%2520packing%2520and%2520message-tree%2520encoding%2520scheme%252C%2520and%2520show%2520bi-directional%2520attention%2520on%2520vision%2520tokens%2520and%2520a%2520novel%2520token-weight%2520strategy%2520improves%2520performance.%2520Our%2520best-in-class%25208B%2520model%2520outperforms%2520others%2520in%2520the%2520class%2520of%2520open%2520weight%2520and%2520data%2520models%2520on%2520short%2520videos%252C%2520counting%252C%2520and%2520captioning%252C%2520and%2520is%2520competitive%2520on%2520long-videos.%2520On%2520video-grounding%2520Molmo2%2520significantly%2520outperforms%2520existing%2520open-weight%2520models%2520like%2520Qwen3-VL%2520%252835.5%2520vs%252029.6%2520accuracy%2520on%2520video%2520counting%2529%2520and%2520surpasses%2520proprietary%2520models%2520like%2520Gemini%25203%2520Pro%2520on%2520some%2520tasks%2520%252838.4%2520vs%252020.0%2520F1%2520on%2520video%2520pointing%2520and%252056.2%2520vs%252041.1%2520J%2526F%2520on%2520video%2520tracking%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10611v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Molmo2%3A%20Open%20Weights%20and%20Data%20for%20Vision-Language%20Models%20with%20Video%20Understanding%20and%20Grounding&entry.906535625=Christopher%20Clark%20and%20Jieyu%20Zhang%20and%20Zixian%20Ma%20and%20Jae%20Sung%20Park%20and%20Mohammadreza%20Salehi%20and%20Rohun%20Tripathi%20and%20Sangho%20Lee%20and%20Zhongzheng%20Ren%20and%20Chris%20Dongjoo%20Kim%20and%20Yinuo%20Yang%20and%20Vincent%20Shao%20and%20Yue%20Yang%20and%20Weikai%20Huang%20and%20Ziqi%20Gao%20and%20Taira%20Anderson%20and%20Jianrui%20Zhang%20and%20Jitesh%20Jain%20and%20George%20Stoica%20and%20Winson%20Han%20and%20Ali%20Farhadi%20and%20Ranjay%20Krishna&entry.1292438233=Today%27s%20strongest%20video-language%20models%20%28VLMs%29%20remain%20proprietary.%20The%20strongest%20open-weight%20models%20either%20rely%20on%20synthetic%20data%20from%20proprietary%20VLMs%2C%20effectively%20distilling%20from%20them%2C%20or%20do%20not%20disclose%20their%20training%20data%20or%20recipe.%20As%20a%20result%2C%20the%20open-source%20community%20lacks%20the%20foundations%20needed%20to%20improve%20on%20the%20state-of-the-art%20video%20%28and%20image%29%20language%20models.%20Crucially%2C%20many%20downstream%20applications%20require%20more%20than%20just%20high-level%20video%20understanding%3B%20they%20require%20grounding%20--%20either%20by%20pointing%20or%20by%20tracking%20in%20pixels.%20Even%20proprietary%20models%20lack%20this%20capability.%20We%20present%20Molmo2%2C%20a%20new%20family%20of%20VLMs%20that%20are%20state-of-the-art%20among%20open-source%20models%20and%20demonstrate%20exceptional%20new%20capabilities%20in%20point-driven%20grounding%20in%20single%20image%2C%20multi-image%2C%20and%20video%20tasks.%20Our%20key%20contribution%20is%20a%20collection%20of%207%20new%20video%20datasets%20and%202%20multi-image%20datasets%2C%20including%20a%20dataset%20of%20highly%20detailed%20video%20captions%20for%20pre-training%2C%20a%20free-form%20video%20Q%26A%20dataset%20for%20fine-tuning%2C%20a%20new%20object%20tracking%20dataset%20with%20complex%20queries%2C%20and%20an%20innovative%20new%20video%20pointing%20dataset%2C%20all%20collected%20without%20the%20use%20of%20closed%20VLMs.%20We%20also%20present%20a%20training%20recipe%20for%20this%20data%20utilizing%20an%20efficient%20packing%20and%20message-tree%20encoding%20scheme%2C%20and%20show%20bi-directional%20attention%20on%20vision%20tokens%20and%20a%20novel%20token-weight%20strategy%20improves%20performance.%20Our%20best-in-class%208B%20model%20outperforms%20others%20in%20the%20class%20of%20open%20weight%20and%20data%20models%20on%20short%20videos%2C%20counting%2C%20and%20captioning%2C%20and%20is%20competitive%20on%20long-videos.%20On%20video-grounding%20Molmo2%20significantly%20outperforms%20existing%20open-weight%20models%20like%20Qwen3-VL%20%2835.5%20vs%2029.6%20accuracy%20on%20video%20counting%29%20and%20surpasses%20proprietary%20models%20like%20Gemini%203%20Pro%20on%20some%20tasks%20%2838.4%20vs%2020.0%20F1%20on%20video%20pointing%20and%2056.2%20vs%2041.1%20J%26F%20on%20video%20tracking%29.&entry.1838667208=http%3A//arxiv.org/abs/2601.10611v1&entry.124074799=Read"},
{"title": "ROMA: Real-time Omni-Multimodal Assistant with Interactive Streaming Understanding", "author": "Xueyun Tian and Wei Li and Bingbing Xu and Heng Dong and Yuanzhuo Wang and Huawei Shen", "abstract": "Recent Omni-multimodal Large Language Models show promise in unified audio, vision, and text modeling. However, streaming audio-video understanding remains challenging, as existing approaches suffer from disjointed capabilities: they typically exhibit incomplete modality support or lack autonomous proactive monitoring. To address this, we present ROMA, a real-time omni-multimodal assistant for unified reactive and proactive interaction. ROMA processes continuous inputs as synchronized multimodal units, aligning dense audio with discrete video frames to handle granularity mismatches. For online decision-making, we introduce a lightweight speak head that decouples response initiation from generation to ensure precise triggering without task conflict. We train ROMA with a curated streaming dataset and a two-stage curriculum that progressively optimizes for streaming format adaptation and proactive responsiveness. To standardize the fragmented evaluation landscape, we reorganize diverse benchmarks into a unified suite covering both proactive (alert, narration) and reactive (QA) settings. Extensive experiments across 12 benchmarks demonstrate ROMA achieves state-of-the-art performance on proactive tasks while competitive in reactive settings, validating its robustness in unified real-time omni-multimodal understanding.", "link": "http://arxiv.org/abs/2601.10323v1", "date": "2026-01-15", "relevancy": 2.7447, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5523}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5422}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ROMA%3A%20Real-time%20Omni-Multimodal%20Assistant%20with%20Interactive%20Streaming%20Understanding&body=Title%3A%20ROMA%3A%20Real-time%20Omni-Multimodal%20Assistant%20with%20Interactive%20Streaming%20Understanding%0AAuthor%3A%20Xueyun%20Tian%20and%20Wei%20Li%20and%20Bingbing%20Xu%20and%20Heng%20Dong%20and%20Yuanzhuo%20Wang%20and%20Huawei%20Shen%0AAbstract%3A%20Recent%20Omni-multimodal%20Large%20Language%20Models%20show%20promise%20in%20unified%20audio%2C%20vision%2C%20and%20text%20modeling.%20However%2C%20streaming%20audio-video%20understanding%20remains%20challenging%2C%20as%20existing%20approaches%20suffer%20from%20disjointed%20capabilities%3A%20they%20typically%20exhibit%20incomplete%20modality%20support%20or%20lack%20autonomous%20proactive%20monitoring.%20To%20address%20this%2C%20we%20present%20ROMA%2C%20a%20real-time%20omni-multimodal%20assistant%20for%20unified%20reactive%20and%20proactive%20interaction.%20ROMA%20processes%20continuous%20inputs%20as%20synchronized%20multimodal%20units%2C%20aligning%20dense%20audio%20with%20discrete%20video%20frames%20to%20handle%20granularity%20mismatches.%20For%20online%20decision-making%2C%20we%20introduce%20a%20lightweight%20speak%20head%20that%20decouples%20response%20initiation%20from%20generation%20to%20ensure%20precise%20triggering%20without%20task%20conflict.%20We%20train%20ROMA%20with%20a%20curated%20streaming%20dataset%20and%20a%20two-stage%20curriculum%20that%20progressively%20optimizes%20for%20streaming%20format%20adaptation%20and%20proactive%20responsiveness.%20To%20standardize%20the%20fragmented%20evaluation%20landscape%2C%20we%20reorganize%20diverse%20benchmarks%20into%20a%20unified%20suite%20covering%20both%20proactive%20%28alert%2C%20narration%29%20and%20reactive%20%28QA%29%20settings.%20Extensive%20experiments%20across%2012%20benchmarks%20demonstrate%20ROMA%20achieves%20state-of-the-art%20performance%20on%20proactive%20tasks%20while%20competitive%20in%20reactive%20settings%2C%20validating%20its%20robustness%20in%20unified%20real-time%20omni-multimodal%20understanding.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10323v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DROMA%253A%2520Real-time%2520Omni-Multimodal%2520Assistant%2520with%2520Interactive%2520Streaming%2520Understanding%26entry.906535625%3DXueyun%2520Tian%2520and%2520Wei%2520Li%2520and%2520Bingbing%2520Xu%2520and%2520Heng%2520Dong%2520and%2520Yuanzhuo%2520Wang%2520and%2520Huawei%2520Shen%26entry.1292438233%3DRecent%2520Omni-multimodal%2520Large%2520Language%2520Models%2520show%2520promise%2520in%2520unified%2520audio%252C%2520vision%252C%2520and%2520text%2520modeling.%2520However%252C%2520streaming%2520audio-video%2520understanding%2520remains%2520challenging%252C%2520as%2520existing%2520approaches%2520suffer%2520from%2520disjointed%2520capabilities%253A%2520they%2520typically%2520exhibit%2520incomplete%2520modality%2520support%2520or%2520lack%2520autonomous%2520proactive%2520monitoring.%2520To%2520address%2520this%252C%2520we%2520present%2520ROMA%252C%2520a%2520real-time%2520omni-multimodal%2520assistant%2520for%2520unified%2520reactive%2520and%2520proactive%2520interaction.%2520ROMA%2520processes%2520continuous%2520inputs%2520as%2520synchronized%2520multimodal%2520units%252C%2520aligning%2520dense%2520audio%2520with%2520discrete%2520video%2520frames%2520to%2520handle%2520granularity%2520mismatches.%2520For%2520online%2520decision-making%252C%2520we%2520introduce%2520a%2520lightweight%2520speak%2520head%2520that%2520decouples%2520response%2520initiation%2520from%2520generation%2520to%2520ensure%2520precise%2520triggering%2520without%2520task%2520conflict.%2520We%2520train%2520ROMA%2520with%2520a%2520curated%2520streaming%2520dataset%2520and%2520a%2520two-stage%2520curriculum%2520that%2520progressively%2520optimizes%2520for%2520streaming%2520format%2520adaptation%2520and%2520proactive%2520responsiveness.%2520To%2520standardize%2520the%2520fragmented%2520evaluation%2520landscape%252C%2520we%2520reorganize%2520diverse%2520benchmarks%2520into%2520a%2520unified%2520suite%2520covering%2520both%2520proactive%2520%2528alert%252C%2520narration%2529%2520and%2520reactive%2520%2528QA%2529%2520settings.%2520Extensive%2520experiments%2520across%252012%2520benchmarks%2520demonstrate%2520ROMA%2520achieves%2520state-of-the-art%2520performance%2520on%2520proactive%2520tasks%2520while%2520competitive%2520in%2520reactive%2520settings%252C%2520validating%2520its%2520robustness%2520in%2520unified%2520real-time%2520omni-multimodal%2520understanding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10323v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ROMA%3A%20Real-time%20Omni-Multimodal%20Assistant%20with%20Interactive%20Streaming%20Understanding&entry.906535625=Xueyun%20Tian%20and%20Wei%20Li%20and%20Bingbing%20Xu%20and%20Heng%20Dong%20and%20Yuanzhuo%20Wang%20and%20Huawei%20Shen&entry.1292438233=Recent%20Omni-multimodal%20Large%20Language%20Models%20show%20promise%20in%20unified%20audio%2C%20vision%2C%20and%20text%20modeling.%20However%2C%20streaming%20audio-video%20understanding%20remains%20challenging%2C%20as%20existing%20approaches%20suffer%20from%20disjointed%20capabilities%3A%20they%20typically%20exhibit%20incomplete%20modality%20support%20or%20lack%20autonomous%20proactive%20monitoring.%20To%20address%20this%2C%20we%20present%20ROMA%2C%20a%20real-time%20omni-multimodal%20assistant%20for%20unified%20reactive%20and%20proactive%20interaction.%20ROMA%20processes%20continuous%20inputs%20as%20synchronized%20multimodal%20units%2C%20aligning%20dense%20audio%20with%20discrete%20video%20frames%20to%20handle%20granularity%20mismatches.%20For%20online%20decision-making%2C%20we%20introduce%20a%20lightweight%20speak%20head%20that%20decouples%20response%20initiation%20from%20generation%20to%20ensure%20precise%20triggering%20without%20task%20conflict.%20We%20train%20ROMA%20with%20a%20curated%20streaming%20dataset%20and%20a%20two-stage%20curriculum%20that%20progressively%20optimizes%20for%20streaming%20format%20adaptation%20and%20proactive%20responsiveness.%20To%20standardize%20the%20fragmented%20evaluation%20landscape%2C%20we%20reorganize%20diverse%20benchmarks%20into%20a%20unified%20suite%20covering%20both%20proactive%20%28alert%2C%20narration%29%20and%20reactive%20%28QA%29%20settings.%20Extensive%20experiments%20across%2012%20benchmarks%20demonstrate%20ROMA%20achieves%20state-of-the-art%20performance%20on%20proactive%20tasks%20while%20competitive%20in%20reactive%20settings%2C%20validating%20its%20robustness%20in%20unified%20real-time%20omni-multimodal%20understanding.&entry.1838667208=http%3A//arxiv.org/abs/2601.10323v1&entry.124074799=Read"},
{"title": "mergetune: Continued fine-tuning of vision-language models", "author": "Wenqing Wang and Da Li and Xiatian Zhu and Josef Kittler", "abstract": "Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, \\emph{continued fine-tuning (CFT)}, which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6\\% on base-novel generalisation without adding parameters. % We show \\emph{the first time} superior performance than CLIP on both DTD and EuroSAT, on cross-dataset transfer. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at \\href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}.", "link": "http://arxiv.org/abs/2601.10497v1", "date": "2026-01-15", "relevancy": 2.7184, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5885}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5212}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5212}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20mergetune%3A%20Continued%20fine-tuning%20of%20vision-language%20models&body=Title%3A%20mergetune%3A%20Continued%20fine-tuning%20of%20vision-language%20models%0AAuthor%3A%20Wenqing%20Wang%20and%20Da%20Li%20and%20Xiatian%20Zhu%20and%20Josef%20Kittler%0AAbstract%3A%20Fine-tuning%20vision-language%20models%20%28VLMs%29%20such%20as%20CLIP%20often%20leads%20to%20catastrophic%20forgetting%20of%20pretrained%20knowledge.%20Prior%20work%20primarily%20aims%20to%20mitigate%20forgetting%20during%20adaptation%3B%20however%2C%20forgetting%20often%20remains%20inevitable%20during%20this%20process.%20We%20introduce%20a%20novel%20paradigm%2C%20%5Cemph%7Bcontinued%20fine-tuning%20%28CFT%29%7D%2C%20which%20seeks%20to%20recover%20pretrained%20knowledge%20after%20a%20zero-shot%20model%20has%20already%20been%20adapted.%20We%20propose%20a%20simple%2C%20model-agnostic%20CFT%20strategy%20%28named%20MERGETUNE%29%20guided%20by%20linear%20mode%20connectivity%20%28LMC%29%2C%20which%20can%20be%20applied%20post%20hoc%20to%20existing%20fine-tuned%20models%20without%20requiring%20architectural%20changes.%20Given%20a%20fine-tuned%20model%2C%20we%20continue%20fine-tuning%20its%20trainable%20parameters%20%28e.g.%2C%20soft%20prompts%20or%20linear%20heads%29%20to%20search%20for%20a%20continued%20model%20which%20has%20two%20low-loss%20paths%20to%20the%20zero-shot%20%28e.g.%2C%20CLIP%29%20and%20the%20fine-tuned%20%28e.g.%2C%20CoOp%29%20solutions.%20By%20exploiting%20the%20geometry%20of%20the%20loss%20landscape%2C%20the%20continued%20model%20implicitly%20merges%20the%20two%20solutions%2C%20restoring%20pretrained%20knowledge%20lost%20in%20the%20fine-tuned%20counterpart.%20A%20challenge%20is%20that%20the%20vanilla%20LMC%20constraint%20requires%20data%20replay%20from%20the%20pretraining%20task.%20We%20approximate%20this%20constraint%20for%20the%20zero-shot%20model%20via%20a%20second-order%20surrogate%2C%20eliminating%20the%20need%20for%20large-scale%20data%20replay.%20Experiments%20show%20that%20MERGETUNE%20improves%20the%20harmonic%20mean%20of%20CoOp%20by%20%2B5.6%5C%25%20on%20base-novel%20generalisation%20without%20adding%20parameters.%20%25%20We%20show%20%5Cemph%7Bthe%20first%20time%7D%20superior%20performance%20than%20CLIP%20on%20both%20DTD%20and%20EuroSAT%2C%20on%20cross-dataset%20transfer.%20On%20robust%20fine-tuning%20evaluations%2C%20the%20LMC-merged%20model%20from%20MERGETUNE%20surpasses%20ensemble%20baselines%20with%20lower%20inference%20cost%2C%20achieving%20further%20gains%20and%20state-of-the-art%20results%20when%20ensembled%20with%20the%20zero-shot%20model.%20Our%20code%20is%20available%20at%20%5Chref%7Bhttps%3A//github.com/Surrey-UP-Lab/MERGETUNE%7D%7Bhttps%3A//github.com/Surrey-UP-Lab/MERGETUNE%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10497v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dmergetune%253A%2520Continued%2520fine-tuning%2520of%2520vision-language%2520models%26entry.906535625%3DWenqing%2520Wang%2520and%2520Da%2520Li%2520and%2520Xiatian%2520Zhu%2520and%2520Josef%2520Kittler%26entry.1292438233%3DFine-tuning%2520vision-language%2520models%2520%2528VLMs%2529%2520such%2520as%2520CLIP%2520often%2520leads%2520to%2520catastrophic%2520forgetting%2520of%2520pretrained%2520knowledge.%2520Prior%2520work%2520primarily%2520aims%2520to%2520mitigate%2520forgetting%2520during%2520adaptation%253B%2520however%252C%2520forgetting%2520often%2520remains%2520inevitable%2520during%2520this%2520process.%2520We%2520introduce%2520a%2520novel%2520paradigm%252C%2520%255Cemph%257Bcontinued%2520fine-tuning%2520%2528CFT%2529%257D%252C%2520which%2520seeks%2520to%2520recover%2520pretrained%2520knowledge%2520after%2520a%2520zero-shot%2520model%2520has%2520already%2520been%2520adapted.%2520We%2520propose%2520a%2520simple%252C%2520model-agnostic%2520CFT%2520strategy%2520%2528named%2520MERGETUNE%2529%2520guided%2520by%2520linear%2520mode%2520connectivity%2520%2528LMC%2529%252C%2520which%2520can%2520be%2520applied%2520post%2520hoc%2520to%2520existing%2520fine-tuned%2520models%2520without%2520requiring%2520architectural%2520changes.%2520Given%2520a%2520fine-tuned%2520model%252C%2520we%2520continue%2520fine-tuning%2520its%2520trainable%2520parameters%2520%2528e.g.%252C%2520soft%2520prompts%2520or%2520linear%2520heads%2529%2520to%2520search%2520for%2520a%2520continued%2520model%2520which%2520has%2520two%2520low-loss%2520paths%2520to%2520the%2520zero-shot%2520%2528e.g.%252C%2520CLIP%2529%2520and%2520the%2520fine-tuned%2520%2528e.g.%252C%2520CoOp%2529%2520solutions.%2520By%2520exploiting%2520the%2520geometry%2520of%2520the%2520loss%2520landscape%252C%2520the%2520continued%2520model%2520implicitly%2520merges%2520the%2520two%2520solutions%252C%2520restoring%2520pretrained%2520knowledge%2520lost%2520in%2520the%2520fine-tuned%2520counterpart.%2520A%2520challenge%2520is%2520that%2520the%2520vanilla%2520LMC%2520constraint%2520requires%2520data%2520replay%2520from%2520the%2520pretraining%2520task.%2520We%2520approximate%2520this%2520constraint%2520for%2520the%2520zero-shot%2520model%2520via%2520a%2520second-order%2520surrogate%252C%2520eliminating%2520the%2520need%2520for%2520large-scale%2520data%2520replay.%2520Experiments%2520show%2520that%2520MERGETUNE%2520improves%2520the%2520harmonic%2520mean%2520of%2520CoOp%2520by%2520%252B5.6%255C%2525%2520on%2520base-novel%2520generalisation%2520without%2520adding%2520parameters.%2520%2525%2520We%2520show%2520%255Cemph%257Bthe%2520first%2520time%257D%2520superior%2520performance%2520than%2520CLIP%2520on%2520both%2520DTD%2520and%2520EuroSAT%252C%2520on%2520cross-dataset%2520transfer.%2520On%2520robust%2520fine-tuning%2520evaluations%252C%2520the%2520LMC-merged%2520model%2520from%2520MERGETUNE%2520surpasses%2520ensemble%2520baselines%2520with%2520lower%2520inference%2520cost%252C%2520achieving%2520further%2520gains%2520and%2520state-of-the-art%2520results%2520when%2520ensembled%2520with%2520the%2520zero-shot%2520model.%2520Our%2520code%2520is%2520available%2520at%2520%255Chref%257Bhttps%253A//github.com/Surrey-UP-Lab/MERGETUNE%257D%257Bhttps%253A//github.com/Surrey-UP-Lab/MERGETUNE%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10497v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=mergetune%3A%20Continued%20fine-tuning%20of%20vision-language%20models&entry.906535625=Wenqing%20Wang%20and%20Da%20Li%20and%20Xiatian%20Zhu%20and%20Josef%20Kittler&entry.1292438233=Fine-tuning%20vision-language%20models%20%28VLMs%29%20such%20as%20CLIP%20often%20leads%20to%20catastrophic%20forgetting%20of%20pretrained%20knowledge.%20Prior%20work%20primarily%20aims%20to%20mitigate%20forgetting%20during%20adaptation%3B%20however%2C%20forgetting%20often%20remains%20inevitable%20during%20this%20process.%20We%20introduce%20a%20novel%20paradigm%2C%20%5Cemph%7Bcontinued%20fine-tuning%20%28CFT%29%7D%2C%20which%20seeks%20to%20recover%20pretrained%20knowledge%20after%20a%20zero-shot%20model%20has%20already%20been%20adapted.%20We%20propose%20a%20simple%2C%20model-agnostic%20CFT%20strategy%20%28named%20MERGETUNE%29%20guided%20by%20linear%20mode%20connectivity%20%28LMC%29%2C%20which%20can%20be%20applied%20post%20hoc%20to%20existing%20fine-tuned%20models%20without%20requiring%20architectural%20changes.%20Given%20a%20fine-tuned%20model%2C%20we%20continue%20fine-tuning%20its%20trainable%20parameters%20%28e.g.%2C%20soft%20prompts%20or%20linear%20heads%29%20to%20search%20for%20a%20continued%20model%20which%20has%20two%20low-loss%20paths%20to%20the%20zero-shot%20%28e.g.%2C%20CLIP%29%20and%20the%20fine-tuned%20%28e.g.%2C%20CoOp%29%20solutions.%20By%20exploiting%20the%20geometry%20of%20the%20loss%20landscape%2C%20the%20continued%20model%20implicitly%20merges%20the%20two%20solutions%2C%20restoring%20pretrained%20knowledge%20lost%20in%20the%20fine-tuned%20counterpart.%20A%20challenge%20is%20that%20the%20vanilla%20LMC%20constraint%20requires%20data%20replay%20from%20the%20pretraining%20task.%20We%20approximate%20this%20constraint%20for%20the%20zero-shot%20model%20via%20a%20second-order%20surrogate%2C%20eliminating%20the%20need%20for%20large-scale%20data%20replay.%20Experiments%20show%20that%20MERGETUNE%20improves%20the%20harmonic%20mean%20of%20CoOp%20by%20%2B5.6%5C%25%20on%20base-novel%20generalisation%20without%20adding%20parameters.%20%25%20We%20show%20%5Cemph%7Bthe%20first%20time%7D%20superior%20performance%20than%20CLIP%20on%20both%20DTD%20and%20EuroSAT%2C%20on%20cross-dataset%20transfer.%20On%20robust%20fine-tuning%20evaluations%2C%20the%20LMC-merged%20model%20from%20MERGETUNE%20surpasses%20ensemble%20baselines%20with%20lower%20inference%20cost%2C%20achieving%20further%20gains%20and%20state-of-the-art%20results%20when%20ensembled%20with%20the%20zero-shot%20model.%20Our%20code%20is%20available%20at%20%5Chref%7Bhttps%3A//github.com/Surrey-UP-Lab/MERGETUNE%7D%7Bhttps%3A//github.com/Surrey-UP-Lab/MERGETUNE%7D.&entry.1838667208=http%3A//arxiv.org/abs/2601.10497v1&entry.124074799=Read"},
{"title": "Towards Understanding Deep Learning Model in Image Recognition via Coverage Test", "author": "Wenkai Li and Xiaoqi Li and Yingjie Mao and Yishun Wang", "abstract": "Deep neural networks (DNNs) play a crucial role in the field of artificial intelligence, and their security-related testing has been a prominent research focus. By inputting test cases, the behavior of models is examined for anomalies, and coverage metrics are utilized to determine the extent of neurons covered by these test cases. With the widespread application and advancement of DNNs, different types of neural behaviors have garnered attention, leading to the emergence of various coverage metrics for neural networks. However, there is currently a lack of empirical research on these coverage metrics, specifically in analyzing the relationships and patterns between model depth, configuration information, and neural network coverage. This paper aims to investigate the relationships and patterns of four coverage metrics: primary functionality, boundary, hierarchy, and structural coverage. A series of empirical experiments were conducted, selecting LeNet, VGG, and ResNet as different DNN architectures, along with 10 models of varying depths ranging from 5 to 54 layers, to compare and study the relationships between different depths, configuration information, and various neural network coverage metrics. Additionally, an investigation was carried out on the relationships between modified decision/condition coverage and dataset size. Finally, three potential future directions are proposed to further contribute to the security testing of DNN Models.", "link": "http://arxiv.org/abs/2505.08814v2", "date": "2026-01-15", "relevancy": 2.7051, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5459}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5313}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Understanding%20Deep%20Learning%20Model%20in%20Image%20Recognition%20via%20Coverage%20Test&body=Title%3A%20Towards%20Understanding%20Deep%20Learning%20Model%20in%20Image%20Recognition%20via%20Coverage%20Test%0AAuthor%3A%20Wenkai%20Li%20and%20Xiaoqi%20Li%20and%20Yingjie%20Mao%20and%20Yishun%20Wang%0AAbstract%3A%20Deep%20neural%20networks%20%28DNNs%29%20play%20a%20crucial%20role%20in%20the%20field%20of%20artificial%20intelligence%2C%20and%20their%20security-related%20testing%20has%20been%20a%20prominent%20research%20focus.%20By%20inputting%20test%20cases%2C%20the%20behavior%20of%20models%20is%20examined%20for%20anomalies%2C%20and%20coverage%20metrics%20are%20utilized%20to%20determine%20the%20extent%20of%20neurons%20covered%20by%20these%20test%20cases.%20With%20the%20widespread%20application%20and%20advancement%20of%20DNNs%2C%20different%20types%20of%20neural%20behaviors%20have%20garnered%20attention%2C%20leading%20to%20the%20emergence%20of%20various%20coverage%20metrics%20for%20neural%20networks.%20However%2C%20there%20is%20currently%20a%20lack%20of%20empirical%20research%20on%20these%20coverage%20metrics%2C%20specifically%20in%20analyzing%20the%20relationships%20and%20patterns%20between%20model%20depth%2C%20configuration%20information%2C%20and%20neural%20network%20coverage.%20This%20paper%20aims%20to%20investigate%20the%20relationships%20and%20patterns%20of%20four%20coverage%20metrics%3A%20primary%20functionality%2C%20boundary%2C%20hierarchy%2C%20and%20structural%20coverage.%20A%20series%20of%20empirical%20experiments%20were%20conducted%2C%20selecting%20LeNet%2C%20VGG%2C%20and%20ResNet%20as%20different%20DNN%20architectures%2C%20along%20with%2010%20models%20of%20varying%20depths%20ranging%20from%205%20to%2054%20layers%2C%20to%20compare%20and%20study%20the%20relationships%20between%20different%20depths%2C%20configuration%20information%2C%20and%20various%20neural%20network%20coverage%20metrics.%20Additionally%2C%20an%20investigation%20was%20carried%20out%20on%20the%20relationships%20between%20modified%20decision/condition%20coverage%20and%20dataset%20size.%20Finally%2C%20three%20potential%20future%20directions%20are%20proposed%20to%20further%20contribute%20to%20the%20security%20testing%20of%20DNN%20Models.%0ALink%3A%20http%3A//arxiv.org/abs/2505.08814v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Understanding%2520Deep%2520Learning%2520Model%2520in%2520Image%2520Recognition%2520via%2520Coverage%2520Test%26entry.906535625%3DWenkai%2520Li%2520and%2520Xiaoqi%2520Li%2520and%2520Yingjie%2520Mao%2520and%2520Yishun%2520Wang%26entry.1292438233%3DDeep%2520neural%2520networks%2520%2528DNNs%2529%2520play%2520a%2520crucial%2520role%2520in%2520the%2520field%2520of%2520artificial%2520intelligence%252C%2520and%2520their%2520security-related%2520testing%2520has%2520been%2520a%2520prominent%2520research%2520focus.%2520By%2520inputting%2520test%2520cases%252C%2520the%2520behavior%2520of%2520models%2520is%2520examined%2520for%2520anomalies%252C%2520and%2520coverage%2520metrics%2520are%2520utilized%2520to%2520determine%2520the%2520extent%2520of%2520neurons%2520covered%2520by%2520these%2520test%2520cases.%2520With%2520the%2520widespread%2520application%2520and%2520advancement%2520of%2520DNNs%252C%2520different%2520types%2520of%2520neural%2520behaviors%2520have%2520garnered%2520attention%252C%2520leading%2520to%2520the%2520emergence%2520of%2520various%2520coverage%2520metrics%2520for%2520neural%2520networks.%2520However%252C%2520there%2520is%2520currently%2520a%2520lack%2520of%2520empirical%2520research%2520on%2520these%2520coverage%2520metrics%252C%2520specifically%2520in%2520analyzing%2520the%2520relationships%2520and%2520patterns%2520between%2520model%2520depth%252C%2520configuration%2520information%252C%2520and%2520neural%2520network%2520coverage.%2520This%2520paper%2520aims%2520to%2520investigate%2520the%2520relationships%2520and%2520patterns%2520of%2520four%2520coverage%2520metrics%253A%2520primary%2520functionality%252C%2520boundary%252C%2520hierarchy%252C%2520and%2520structural%2520coverage.%2520A%2520series%2520of%2520empirical%2520experiments%2520were%2520conducted%252C%2520selecting%2520LeNet%252C%2520VGG%252C%2520and%2520ResNet%2520as%2520different%2520DNN%2520architectures%252C%2520along%2520with%252010%2520models%2520of%2520varying%2520depths%2520ranging%2520from%25205%2520to%252054%2520layers%252C%2520to%2520compare%2520and%2520study%2520the%2520relationships%2520between%2520different%2520depths%252C%2520configuration%2520information%252C%2520and%2520various%2520neural%2520network%2520coverage%2520metrics.%2520Additionally%252C%2520an%2520investigation%2520was%2520carried%2520out%2520on%2520the%2520relationships%2520between%2520modified%2520decision/condition%2520coverage%2520and%2520dataset%2520size.%2520Finally%252C%2520three%2520potential%2520future%2520directions%2520are%2520proposed%2520to%2520further%2520contribute%2520to%2520the%2520security%2520testing%2520of%2520DNN%2520Models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.08814v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Understanding%20Deep%20Learning%20Model%20in%20Image%20Recognition%20via%20Coverage%20Test&entry.906535625=Wenkai%20Li%20and%20Xiaoqi%20Li%20and%20Yingjie%20Mao%20and%20Yishun%20Wang&entry.1292438233=Deep%20neural%20networks%20%28DNNs%29%20play%20a%20crucial%20role%20in%20the%20field%20of%20artificial%20intelligence%2C%20and%20their%20security-related%20testing%20has%20been%20a%20prominent%20research%20focus.%20By%20inputting%20test%20cases%2C%20the%20behavior%20of%20models%20is%20examined%20for%20anomalies%2C%20and%20coverage%20metrics%20are%20utilized%20to%20determine%20the%20extent%20of%20neurons%20covered%20by%20these%20test%20cases.%20With%20the%20widespread%20application%20and%20advancement%20of%20DNNs%2C%20different%20types%20of%20neural%20behaviors%20have%20garnered%20attention%2C%20leading%20to%20the%20emergence%20of%20various%20coverage%20metrics%20for%20neural%20networks.%20However%2C%20there%20is%20currently%20a%20lack%20of%20empirical%20research%20on%20these%20coverage%20metrics%2C%20specifically%20in%20analyzing%20the%20relationships%20and%20patterns%20between%20model%20depth%2C%20configuration%20information%2C%20and%20neural%20network%20coverage.%20This%20paper%20aims%20to%20investigate%20the%20relationships%20and%20patterns%20of%20four%20coverage%20metrics%3A%20primary%20functionality%2C%20boundary%2C%20hierarchy%2C%20and%20structural%20coverage.%20A%20series%20of%20empirical%20experiments%20were%20conducted%2C%20selecting%20LeNet%2C%20VGG%2C%20and%20ResNet%20as%20different%20DNN%20architectures%2C%20along%20with%2010%20models%20of%20varying%20depths%20ranging%20from%205%20to%2054%20layers%2C%20to%20compare%20and%20study%20the%20relationships%20between%20different%20depths%2C%20configuration%20information%2C%20and%20various%20neural%20network%20coverage%20metrics.%20Additionally%2C%20an%20investigation%20was%20carried%20out%20on%20the%20relationships%20between%20modified%20decision/condition%20coverage%20and%20dataset%20size.%20Finally%2C%20three%20potential%20future%20directions%20are%20proposed%20to%20further%20contribute%20to%20the%20security%20testing%20of%20DNN%20Models.&entry.1838667208=http%3A//arxiv.org/abs/2505.08814v2&entry.124074799=Read"},
{"title": "Training-Trajectory-Aware Token Selection", "author": "Zhanming Shen and Jiaqi Hu and Zeyu Qin and Hao Chen and Wentao Ye and Zenan Huang and Yihong Zhuang and Guoshan Lu and Junlin Zhou and Junbo Zhao", "abstract": "Efficient distillation is a key pathway for converting expensive reasoning capability into deployable efficiency, yet in the frontier regime where the student already has strong reasoning ability, naive continual distillation often yields limited gains or even degradation. We observe a characteristic training phenomenon: even as loss decreases monotonically, all performance metrics can drop sharply at almost the same bottleneck, before gradually recovering. We further uncover a token-level mechanism: confidence bifurcates into steadily increasing Imitation-Anchor Tokens that quickly anchor optimization and other yet-to-learn tokens whose confidence is suppressed until after the bottleneck. And the characteristic that these two types of tokens cannot coexist is the root cause of the failure in continual distillation. To this end, we propose Training-Trajectory-Aware Token Selection (T3S) to reconstruct the training objective at the token level, clearing the optimization path for yet-to-learn tokens. T3 yields consistent gains in both AR and dLLM settings: with only hundreds of examples, Qwen3-8B surpasses DeepSeek-R1 on competitive reasoning benchmarks, Qwen3-32B approaches Qwen3-235B, and T3-trained LLaDA-2.0-Mini exceeds its AR baseline, achieving state-of-the-art performance among all of 16B-scale no-think models.", "link": "http://arxiv.org/abs/2601.10348v1", "date": "2026-01-15", "relevancy": 2.7013, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5908}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5162}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Trajectory-Aware%20Token%20Selection&body=Title%3A%20Training-Trajectory-Aware%20Token%20Selection%0AAuthor%3A%20Zhanming%20Shen%20and%20Jiaqi%20Hu%20and%20Zeyu%20Qin%20and%20Hao%20Chen%20and%20Wentao%20Ye%20and%20Zenan%20Huang%20and%20Yihong%20Zhuang%20and%20Guoshan%20Lu%20and%20Junlin%20Zhou%20and%20Junbo%20Zhao%0AAbstract%3A%20Efficient%20distillation%20is%20a%20key%20pathway%20for%20converting%20expensive%20reasoning%20capability%20into%20deployable%20efficiency%2C%20yet%20in%20the%20frontier%20regime%20where%20the%20student%20already%20has%20strong%20reasoning%20ability%2C%20naive%20continual%20distillation%20often%20yields%20limited%20gains%20or%20even%20degradation.%20We%20observe%20a%20characteristic%20training%20phenomenon%3A%20even%20as%20loss%20decreases%20monotonically%2C%20all%20performance%20metrics%20can%20drop%20sharply%20at%20almost%20the%20same%20bottleneck%2C%20before%20gradually%20recovering.%20We%20further%20uncover%20a%20token-level%20mechanism%3A%20confidence%20bifurcates%20into%20steadily%20increasing%20Imitation-Anchor%20Tokens%20that%20quickly%20anchor%20optimization%20and%20other%20yet-to-learn%20tokens%20whose%20confidence%20is%20suppressed%20until%20after%20the%20bottleneck.%20And%20the%20characteristic%20that%20these%20two%20types%20of%20tokens%20cannot%20coexist%20is%20the%20root%20cause%20of%20the%20failure%20in%20continual%20distillation.%20To%20this%20end%2C%20we%20propose%20Training-Trajectory-Aware%20Token%20Selection%20%28T3S%29%20to%20reconstruct%20the%20training%20objective%20at%20the%20token%20level%2C%20clearing%20the%20optimization%20path%20for%20yet-to-learn%20tokens.%20T3%20yields%20consistent%20gains%20in%20both%20AR%20and%20dLLM%20settings%3A%20with%20only%20hundreds%20of%20examples%2C%20Qwen3-8B%20surpasses%20DeepSeek-R1%20on%20competitive%20reasoning%20benchmarks%2C%20Qwen3-32B%20approaches%20Qwen3-235B%2C%20and%20T3-trained%20LLaDA-2.0-Mini%20exceeds%20its%20AR%20baseline%2C%20achieving%20state-of-the-art%20performance%20among%20all%20of%2016B-scale%20no-think%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10348v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Trajectory-Aware%2520Token%2520Selection%26entry.906535625%3DZhanming%2520Shen%2520and%2520Jiaqi%2520Hu%2520and%2520Zeyu%2520Qin%2520and%2520Hao%2520Chen%2520and%2520Wentao%2520Ye%2520and%2520Zenan%2520Huang%2520and%2520Yihong%2520Zhuang%2520and%2520Guoshan%2520Lu%2520and%2520Junlin%2520Zhou%2520and%2520Junbo%2520Zhao%26entry.1292438233%3DEfficient%2520distillation%2520is%2520a%2520key%2520pathway%2520for%2520converting%2520expensive%2520reasoning%2520capability%2520into%2520deployable%2520efficiency%252C%2520yet%2520in%2520the%2520frontier%2520regime%2520where%2520the%2520student%2520already%2520has%2520strong%2520reasoning%2520ability%252C%2520naive%2520continual%2520distillation%2520often%2520yields%2520limited%2520gains%2520or%2520even%2520degradation.%2520We%2520observe%2520a%2520characteristic%2520training%2520phenomenon%253A%2520even%2520as%2520loss%2520decreases%2520monotonically%252C%2520all%2520performance%2520metrics%2520can%2520drop%2520sharply%2520at%2520almost%2520the%2520same%2520bottleneck%252C%2520before%2520gradually%2520recovering.%2520We%2520further%2520uncover%2520a%2520token-level%2520mechanism%253A%2520confidence%2520bifurcates%2520into%2520steadily%2520increasing%2520Imitation-Anchor%2520Tokens%2520that%2520quickly%2520anchor%2520optimization%2520and%2520other%2520yet-to-learn%2520tokens%2520whose%2520confidence%2520is%2520suppressed%2520until%2520after%2520the%2520bottleneck.%2520And%2520the%2520characteristic%2520that%2520these%2520two%2520types%2520of%2520tokens%2520cannot%2520coexist%2520is%2520the%2520root%2520cause%2520of%2520the%2520failure%2520in%2520continual%2520distillation.%2520To%2520this%2520end%252C%2520we%2520propose%2520Training-Trajectory-Aware%2520Token%2520Selection%2520%2528T3S%2529%2520to%2520reconstruct%2520the%2520training%2520objective%2520at%2520the%2520token%2520level%252C%2520clearing%2520the%2520optimization%2520path%2520for%2520yet-to-learn%2520tokens.%2520T3%2520yields%2520consistent%2520gains%2520in%2520both%2520AR%2520and%2520dLLM%2520settings%253A%2520with%2520only%2520hundreds%2520of%2520examples%252C%2520Qwen3-8B%2520surpasses%2520DeepSeek-R1%2520on%2520competitive%2520reasoning%2520benchmarks%252C%2520Qwen3-32B%2520approaches%2520Qwen3-235B%252C%2520and%2520T3-trained%2520LLaDA-2.0-Mini%2520exceeds%2520its%2520AR%2520baseline%252C%2520achieving%2520state-of-the-art%2520performance%2520among%2520all%2520of%252016B-scale%2520no-think%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10348v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Trajectory-Aware%20Token%20Selection&entry.906535625=Zhanming%20Shen%20and%20Jiaqi%20Hu%20and%20Zeyu%20Qin%20and%20Hao%20Chen%20and%20Wentao%20Ye%20and%20Zenan%20Huang%20and%20Yihong%20Zhuang%20and%20Guoshan%20Lu%20and%20Junlin%20Zhou%20and%20Junbo%20Zhao&entry.1292438233=Efficient%20distillation%20is%20a%20key%20pathway%20for%20converting%20expensive%20reasoning%20capability%20into%20deployable%20efficiency%2C%20yet%20in%20the%20frontier%20regime%20where%20the%20student%20already%20has%20strong%20reasoning%20ability%2C%20naive%20continual%20distillation%20often%20yields%20limited%20gains%20or%20even%20degradation.%20We%20observe%20a%20characteristic%20training%20phenomenon%3A%20even%20as%20loss%20decreases%20monotonically%2C%20all%20performance%20metrics%20can%20drop%20sharply%20at%20almost%20the%20same%20bottleneck%2C%20before%20gradually%20recovering.%20We%20further%20uncover%20a%20token-level%20mechanism%3A%20confidence%20bifurcates%20into%20steadily%20increasing%20Imitation-Anchor%20Tokens%20that%20quickly%20anchor%20optimization%20and%20other%20yet-to-learn%20tokens%20whose%20confidence%20is%20suppressed%20until%20after%20the%20bottleneck.%20And%20the%20characteristic%20that%20these%20two%20types%20of%20tokens%20cannot%20coexist%20is%20the%20root%20cause%20of%20the%20failure%20in%20continual%20distillation.%20To%20this%20end%2C%20we%20propose%20Training-Trajectory-Aware%20Token%20Selection%20%28T3S%29%20to%20reconstruct%20the%20training%20objective%20at%20the%20token%20level%2C%20clearing%20the%20optimization%20path%20for%20yet-to-learn%20tokens.%20T3%20yields%20consistent%20gains%20in%20both%20AR%20and%20dLLM%20settings%3A%20with%20only%20hundreds%20of%20examples%2C%20Qwen3-8B%20surpasses%20DeepSeek-R1%20on%20competitive%20reasoning%20benchmarks%2C%20Qwen3-32B%20approaches%20Qwen3-235B%2C%20and%20T3-trained%20LLaDA-2.0-Mini%20exceeds%20its%20AR%20baseline%2C%20achieving%20state-of-the-art%20performance%20among%20all%20of%2016B-scale%20no-think%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.10348v1&entry.124074799=Read"},
{"title": "DanQing: An Up-to-Date Large-Scale Chinese Vision-Language Pre-training Dataset", "author": "Hengyu Shen and Tiancheng Gu and Bin Qin and Lan Wu and Yuling Wu and Shuo Tan and Zelong Sun and Jun Wang and Nan Wu and Xiang An and Weidong Cai and Ziyong Feng and Kaicheng Yang", "abstract": "Vision-Language Pre-training (VLP) models demonstrate strong performance across various downstream tasks by learning from large-scale image-text pairs through contrastive pretraining. The release of extensive English image-text datasets (e.g., COYO-700M and LAION-400M) has enabled widespread adoption of models such as CLIP and SigLIP in tasks including cross-modal retrieval and image captioning. However, the advancement of Chinese vision-language pretraining has substantially lagged behind, due to the scarcity of high-quality Chinese image-text data. To address this gap, we develop a comprehensive pipeline for constructing a high-quality Chinese cross-modal dataset. As a result, we propose DanQing, which contains 100 million image-text pairs collected from Common Crawl. Different from existing datasets, DanQing is curated through a more rigorous selection process, yielding superior data quality. Moreover, DanQing is primarily built from 2024-2025 web data, enabling models to better capture evolving semantic trends and thus offering greater practical utility. We compare DanQing with existing datasets by continual pre-training of the SigLIP2 model. Experimental results show that DanQing consistently achieves superior performance across a range of Chinese downstream tasks, including zero-shot classification, cross-modal retrieval, and LMM-based evaluations. To facilitate further research in Chinese vision-language pre-training, we will open-source the DanQing dataset under the Creative Common CC-BY 4.0 license.", "link": "http://arxiv.org/abs/2601.10305v1", "date": "2026-01-15", "relevancy": 2.6782, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5444}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5182}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DanQing%3A%20An%20Up-to-Date%20Large-Scale%20Chinese%20Vision-Language%20Pre-training%20Dataset&body=Title%3A%20DanQing%3A%20An%20Up-to-Date%20Large-Scale%20Chinese%20Vision-Language%20Pre-training%20Dataset%0AAuthor%3A%20Hengyu%20Shen%20and%20Tiancheng%20Gu%20and%20Bin%20Qin%20and%20Lan%20Wu%20and%20Yuling%20Wu%20and%20Shuo%20Tan%20and%20Zelong%20Sun%20and%20Jun%20Wang%20and%20Nan%20Wu%20and%20Xiang%20An%20and%20Weidong%20Cai%20and%20Ziyong%20Feng%20and%20Kaicheng%20Yang%0AAbstract%3A%20Vision-Language%20Pre-training%20%28VLP%29%20models%20demonstrate%20strong%20performance%20across%20various%20downstream%20tasks%20by%20learning%20from%20large-scale%20image-text%20pairs%20through%20contrastive%20pretraining.%20The%20release%20of%20extensive%20English%20image-text%20datasets%20%28e.g.%2C%20COYO-700M%20and%20LAION-400M%29%20has%20enabled%20widespread%20adoption%20of%20models%20such%20as%20CLIP%20and%20SigLIP%20in%20tasks%20including%20cross-modal%20retrieval%20and%20image%20captioning.%20However%2C%20the%20advancement%20of%20Chinese%20vision-language%20pretraining%20has%20substantially%20lagged%20behind%2C%20due%20to%20the%20scarcity%20of%20high-quality%20Chinese%20image-text%20data.%20To%20address%20this%20gap%2C%20we%20develop%20a%20comprehensive%20pipeline%20for%20constructing%20a%20high-quality%20Chinese%20cross-modal%20dataset.%20As%20a%20result%2C%20we%20propose%20DanQing%2C%20which%20contains%20100%20million%20image-text%20pairs%20collected%20from%20Common%20Crawl.%20Different%20from%20existing%20datasets%2C%20DanQing%20is%20curated%20through%20a%20more%20rigorous%20selection%20process%2C%20yielding%20superior%20data%20quality.%20Moreover%2C%20DanQing%20is%20primarily%20built%20from%202024-2025%20web%20data%2C%20enabling%20models%20to%20better%20capture%20evolving%20semantic%20trends%20and%20thus%20offering%20greater%20practical%20utility.%20We%20compare%20DanQing%20with%20existing%20datasets%20by%20continual%20pre-training%20of%20the%20SigLIP2%20model.%20Experimental%20results%20show%20that%20DanQing%20consistently%20achieves%20superior%20performance%20across%20a%20range%20of%20Chinese%20downstream%20tasks%2C%20including%20zero-shot%20classification%2C%20cross-modal%20retrieval%2C%20and%20LMM-based%20evaluations.%20To%20facilitate%20further%20research%20in%20Chinese%20vision-language%20pre-training%2C%20we%20will%20open-source%20the%20DanQing%20dataset%20under%20the%20Creative%20Common%20CC-BY%204.0%20license.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10305v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDanQing%253A%2520An%2520Up-to-Date%2520Large-Scale%2520Chinese%2520Vision-Language%2520Pre-training%2520Dataset%26entry.906535625%3DHengyu%2520Shen%2520and%2520Tiancheng%2520Gu%2520and%2520Bin%2520Qin%2520and%2520Lan%2520Wu%2520and%2520Yuling%2520Wu%2520and%2520Shuo%2520Tan%2520and%2520Zelong%2520Sun%2520and%2520Jun%2520Wang%2520and%2520Nan%2520Wu%2520and%2520Xiang%2520An%2520and%2520Weidong%2520Cai%2520and%2520Ziyong%2520Feng%2520and%2520Kaicheng%2520Yang%26entry.1292438233%3DVision-Language%2520Pre-training%2520%2528VLP%2529%2520models%2520demonstrate%2520strong%2520performance%2520across%2520various%2520downstream%2520tasks%2520by%2520learning%2520from%2520large-scale%2520image-text%2520pairs%2520through%2520contrastive%2520pretraining.%2520The%2520release%2520of%2520extensive%2520English%2520image-text%2520datasets%2520%2528e.g.%252C%2520COYO-700M%2520and%2520LAION-400M%2529%2520has%2520enabled%2520widespread%2520adoption%2520of%2520models%2520such%2520as%2520CLIP%2520and%2520SigLIP%2520in%2520tasks%2520including%2520cross-modal%2520retrieval%2520and%2520image%2520captioning.%2520However%252C%2520the%2520advancement%2520of%2520Chinese%2520vision-language%2520pretraining%2520has%2520substantially%2520lagged%2520behind%252C%2520due%2520to%2520the%2520scarcity%2520of%2520high-quality%2520Chinese%2520image-text%2520data.%2520To%2520address%2520this%2520gap%252C%2520we%2520develop%2520a%2520comprehensive%2520pipeline%2520for%2520constructing%2520a%2520high-quality%2520Chinese%2520cross-modal%2520dataset.%2520As%2520a%2520result%252C%2520we%2520propose%2520DanQing%252C%2520which%2520contains%2520100%2520million%2520image-text%2520pairs%2520collected%2520from%2520Common%2520Crawl.%2520Different%2520from%2520existing%2520datasets%252C%2520DanQing%2520is%2520curated%2520through%2520a%2520more%2520rigorous%2520selection%2520process%252C%2520yielding%2520superior%2520data%2520quality.%2520Moreover%252C%2520DanQing%2520is%2520primarily%2520built%2520from%25202024-2025%2520web%2520data%252C%2520enabling%2520models%2520to%2520better%2520capture%2520evolving%2520semantic%2520trends%2520and%2520thus%2520offering%2520greater%2520practical%2520utility.%2520We%2520compare%2520DanQing%2520with%2520existing%2520datasets%2520by%2520continual%2520pre-training%2520of%2520the%2520SigLIP2%2520model.%2520Experimental%2520results%2520show%2520that%2520DanQing%2520consistently%2520achieves%2520superior%2520performance%2520across%2520a%2520range%2520of%2520Chinese%2520downstream%2520tasks%252C%2520including%2520zero-shot%2520classification%252C%2520cross-modal%2520retrieval%252C%2520and%2520LMM-based%2520evaluations.%2520To%2520facilitate%2520further%2520research%2520in%2520Chinese%2520vision-language%2520pre-training%252C%2520we%2520will%2520open-source%2520the%2520DanQing%2520dataset%2520under%2520the%2520Creative%2520Common%2520CC-BY%25204.0%2520license.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10305v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DanQing%3A%20An%20Up-to-Date%20Large-Scale%20Chinese%20Vision-Language%20Pre-training%20Dataset&entry.906535625=Hengyu%20Shen%20and%20Tiancheng%20Gu%20and%20Bin%20Qin%20and%20Lan%20Wu%20and%20Yuling%20Wu%20and%20Shuo%20Tan%20and%20Zelong%20Sun%20and%20Jun%20Wang%20and%20Nan%20Wu%20and%20Xiang%20An%20and%20Weidong%20Cai%20and%20Ziyong%20Feng%20and%20Kaicheng%20Yang&entry.1292438233=Vision-Language%20Pre-training%20%28VLP%29%20models%20demonstrate%20strong%20performance%20across%20various%20downstream%20tasks%20by%20learning%20from%20large-scale%20image-text%20pairs%20through%20contrastive%20pretraining.%20The%20release%20of%20extensive%20English%20image-text%20datasets%20%28e.g.%2C%20COYO-700M%20and%20LAION-400M%29%20has%20enabled%20widespread%20adoption%20of%20models%20such%20as%20CLIP%20and%20SigLIP%20in%20tasks%20including%20cross-modal%20retrieval%20and%20image%20captioning.%20However%2C%20the%20advancement%20of%20Chinese%20vision-language%20pretraining%20has%20substantially%20lagged%20behind%2C%20due%20to%20the%20scarcity%20of%20high-quality%20Chinese%20image-text%20data.%20To%20address%20this%20gap%2C%20we%20develop%20a%20comprehensive%20pipeline%20for%20constructing%20a%20high-quality%20Chinese%20cross-modal%20dataset.%20As%20a%20result%2C%20we%20propose%20DanQing%2C%20which%20contains%20100%20million%20image-text%20pairs%20collected%20from%20Common%20Crawl.%20Different%20from%20existing%20datasets%2C%20DanQing%20is%20curated%20through%20a%20more%20rigorous%20selection%20process%2C%20yielding%20superior%20data%20quality.%20Moreover%2C%20DanQing%20is%20primarily%20built%20from%202024-2025%20web%20data%2C%20enabling%20models%20to%20better%20capture%20evolving%20semantic%20trends%20and%20thus%20offering%20greater%20practical%20utility.%20We%20compare%20DanQing%20with%20existing%20datasets%20by%20continual%20pre-training%20of%20the%20SigLIP2%20model.%20Experimental%20results%20show%20that%20DanQing%20consistently%20achieves%20superior%20performance%20across%20a%20range%20of%20Chinese%20downstream%20tasks%2C%20including%20zero-shot%20classification%2C%20cross-modal%20retrieval%2C%20and%20LMM-based%20evaluations.%20To%20facilitate%20further%20research%20in%20Chinese%20vision-language%20pre-training%2C%20we%20will%20open-source%20the%20DanQing%20dataset%20under%20the%20Creative%20Common%20CC-BY%204.0%20license.&entry.1838667208=http%3A//arxiv.org/abs/2601.10305v1&entry.124074799=Read"},
{"title": "Lunar-G2R: Geometry-to-Reflectance Learning for High-Fidelity Lunar BRDF Estimation", "author": "Clementine Grethen and Nicolas Menga and Roland Brochard and Geraldine Morin and Simone Gasparini and Jeremy Lebreton and Manuel Sanchez Gestido", "abstract": "We address the problem of estimating realistic, spatially varying reflectance for complex planetary surfaces such as the lunar regolith, which is critical for high-fidelity rendering and vision-based navigation. Existing lunar rendering pipelines rely on simplified or spatially uniform BRDF models whose parameters are difficult to estimate and fail to capture local reflectance variations, limiting photometric realism. We propose Lunar-G2R, a geometry-to-reflectance learning framework that predicts spatially varying BRDF parameters directly from a lunar digital elevation model (DEM), without requiring multi-view imagery, controlled illumination, or dedicated reflectance-capture hardware at inference time. The method leverages a U-Net trained with differentiable rendering to minimize photometric discrepancies between real orbital images and physically based renderings under known viewing and illumination geometry. Experiments on a geographically held-out region of the Tycho crater show that our approach reduces photometric error by 38 % compared to a state-of-the-art baseline, while achieving higher PSNR and SSIM and improved perceptual similarity, capturing fine-scale reflectance variations absent from spatially uniform models. To our knowledge, this is the first method to infer a spatially varying reflectance model directly from terrain geometry.", "link": "http://arxiv.org/abs/2601.10449v1", "date": "2026-01-15", "relevancy": 2.6712, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5738}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5194}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5096}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lunar-G2R%3A%20Geometry-to-Reflectance%20Learning%20for%20High-Fidelity%20Lunar%20BRDF%20Estimation&body=Title%3A%20Lunar-G2R%3A%20Geometry-to-Reflectance%20Learning%20for%20High-Fidelity%20Lunar%20BRDF%20Estimation%0AAuthor%3A%20Clementine%20Grethen%20and%20Nicolas%20Menga%20and%20Roland%20Brochard%20and%20Geraldine%20Morin%20and%20Simone%20Gasparini%20and%20Jeremy%20Lebreton%20and%20Manuel%20Sanchez%20Gestido%0AAbstract%3A%20We%20address%20the%20problem%20of%20estimating%20realistic%2C%20spatially%20varying%20reflectance%20for%20complex%20planetary%20surfaces%20such%20as%20the%20lunar%20regolith%2C%20which%20is%20critical%20for%20high-fidelity%20rendering%20and%20vision-based%20navigation.%20Existing%20lunar%20rendering%20pipelines%20rely%20on%20simplified%20or%20spatially%20uniform%20BRDF%20models%20whose%20parameters%20are%20difficult%20to%20estimate%20and%20fail%20to%20capture%20local%20reflectance%20variations%2C%20limiting%20photometric%20realism.%20We%20propose%20Lunar-G2R%2C%20a%20geometry-to-reflectance%20learning%20framework%20that%20predicts%20spatially%20varying%20BRDF%20parameters%20directly%20from%20a%20lunar%20digital%20elevation%20model%20%28DEM%29%2C%20without%20requiring%20multi-view%20imagery%2C%20controlled%20illumination%2C%20or%20dedicated%20reflectance-capture%20hardware%20at%20inference%20time.%20The%20method%20leverages%20a%20U-Net%20trained%20with%20differentiable%20rendering%20to%20minimize%20photometric%20discrepancies%20between%20real%20orbital%20images%20and%20physically%20based%20renderings%20under%20known%20viewing%20and%20illumination%20geometry.%20Experiments%20on%20a%20geographically%20held-out%20region%20of%20the%20Tycho%20crater%20show%20that%20our%20approach%20reduces%20photometric%20error%20by%2038%20%25%20compared%20to%20a%20state-of-the-art%20baseline%2C%20while%20achieving%20higher%20PSNR%20and%20SSIM%20and%20improved%20perceptual%20similarity%2C%20capturing%20fine-scale%20reflectance%20variations%20absent%20from%20spatially%20uniform%20models.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20method%20to%20infer%20a%20spatially%20varying%20reflectance%20model%20directly%20from%20terrain%20geometry.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10449v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLunar-G2R%253A%2520Geometry-to-Reflectance%2520Learning%2520for%2520High-Fidelity%2520Lunar%2520BRDF%2520Estimation%26entry.906535625%3DClementine%2520Grethen%2520and%2520Nicolas%2520Menga%2520and%2520Roland%2520Brochard%2520and%2520Geraldine%2520Morin%2520and%2520Simone%2520Gasparini%2520and%2520Jeremy%2520Lebreton%2520and%2520Manuel%2520Sanchez%2520Gestido%26entry.1292438233%3DWe%2520address%2520the%2520problem%2520of%2520estimating%2520realistic%252C%2520spatially%2520varying%2520reflectance%2520for%2520complex%2520planetary%2520surfaces%2520such%2520as%2520the%2520lunar%2520regolith%252C%2520which%2520is%2520critical%2520for%2520high-fidelity%2520rendering%2520and%2520vision-based%2520navigation.%2520Existing%2520lunar%2520rendering%2520pipelines%2520rely%2520on%2520simplified%2520or%2520spatially%2520uniform%2520BRDF%2520models%2520whose%2520parameters%2520are%2520difficult%2520to%2520estimate%2520and%2520fail%2520to%2520capture%2520local%2520reflectance%2520variations%252C%2520limiting%2520photometric%2520realism.%2520We%2520propose%2520Lunar-G2R%252C%2520a%2520geometry-to-reflectance%2520learning%2520framework%2520that%2520predicts%2520spatially%2520varying%2520BRDF%2520parameters%2520directly%2520from%2520a%2520lunar%2520digital%2520elevation%2520model%2520%2528DEM%2529%252C%2520without%2520requiring%2520multi-view%2520imagery%252C%2520controlled%2520illumination%252C%2520or%2520dedicated%2520reflectance-capture%2520hardware%2520at%2520inference%2520time.%2520The%2520method%2520leverages%2520a%2520U-Net%2520trained%2520with%2520differentiable%2520rendering%2520to%2520minimize%2520photometric%2520discrepancies%2520between%2520real%2520orbital%2520images%2520and%2520physically%2520based%2520renderings%2520under%2520known%2520viewing%2520and%2520illumination%2520geometry.%2520Experiments%2520on%2520a%2520geographically%2520held-out%2520region%2520of%2520the%2520Tycho%2520crater%2520show%2520that%2520our%2520approach%2520reduces%2520photometric%2520error%2520by%252038%2520%2525%2520compared%2520to%2520a%2520state-of-the-art%2520baseline%252C%2520while%2520achieving%2520higher%2520PSNR%2520and%2520SSIM%2520and%2520improved%2520perceptual%2520similarity%252C%2520capturing%2520fine-scale%2520reflectance%2520variations%2520absent%2520from%2520spatially%2520uniform%2520models.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520method%2520to%2520infer%2520a%2520spatially%2520varying%2520reflectance%2520model%2520directly%2520from%2520terrain%2520geometry.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10449v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lunar-G2R%3A%20Geometry-to-Reflectance%20Learning%20for%20High-Fidelity%20Lunar%20BRDF%20Estimation&entry.906535625=Clementine%20Grethen%20and%20Nicolas%20Menga%20and%20Roland%20Brochard%20and%20Geraldine%20Morin%20and%20Simone%20Gasparini%20and%20Jeremy%20Lebreton%20and%20Manuel%20Sanchez%20Gestido&entry.1292438233=We%20address%20the%20problem%20of%20estimating%20realistic%2C%20spatially%20varying%20reflectance%20for%20complex%20planetary%20surfaces%20such%20as%20the%20lunar%20regolith%2C%20which%20is%20critical%20for%20high-fidelity%20rendering%20and%20vision-based%20navigation.%20Existing%20lunar%20rendering%20pipelines%20rely%20on%20simplified%20or%20spatially%20uniform%20BRDF%20models%20whose%20parameters%20are%20difficult%20to%20estimate%20and%20fail%20to%20capture%20local%20reflectance%20variations%2C%20limiting%20photometric%20realism.%20We%20propose%20Lunar-G2R%2C%20a%20geometry-to-reflectance%20learning%20framework%20that%20predicts%20spatially%20varying%20BRDF%20parameters%20directly%20from%20a%20lunar%20digital%20elevation%20model%20%28DEM%29%2C%20without%20requiring%20multi-view%20imagery%2C%20controlled%20illumination%2C%20or%20dedicated%20reflectance-capture%20hardware%20at%20inference%20time.%20The%20method%20leverages%20a%20U-Net%20trained%20with%20differentiable%20rendering%20to%20minimize%20photometric%20discrepancies%20between%20real%20orbital%20images%20and%20physically%20based%20renderings%20under%20known%20viewing%20and%20illumination%20geometry.%20Experiments%20on%20a%20geographically%20held-out%20region%20of%20the%20Tycho%20crater%20show%20that%20our%20approach%20reduces%20photometric%20error%20by%2038%20%25%20compared%20to%20a%20state-of-the-art%20baseline%2C%20while%20achieving%20higher%20PSNR%20and%20SSIM%20and%20improved%20perceptual%20similarity%2C%20capturing%20fine-scale%20reflectance%20variations%20absent%20from%20spatially%20uniform%20models.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20method%20to%20infer%20a%20spatially%20varying%20reflectance%20model%20directly%20from%20terrain%20geometry.&entry.1838667208=http%3A//arxiv.org/abs/2601.10449v1&entry.124074799=Read"},
{"title": "CoMoVi: Co-Generation of 3D Human Motions and Realistic Videos", "author": "Chengfeng Zhao and Jiazhi Shu and Yubo Zhao and Tianyu Huang and Jiahao Lu and Zekai Gu and Chengwei Ren and Zhiyang Dou and Qing Shuai and Yuan Liu", "abstract": "In this paper, we find that the generation of 3D human motions and 2D human videos is intrinsically coupled. 3D motions provide the structural prior for plausibility and consistency in videos, while pre-trained video models offer strong generalization capabilities for motions, which necessitate coupling their generation processes. Based on this, we present CoMoVi, a co-generative framework that couples two video diffusion models (VDMs) to generate 3D human motions and videos synchronously within a single diffusion denoising loop. To achieve this, we first propose an effective 2D human motion representation that can inherit the powerful prior of pre-trained VDMs. Then, we design a dual-branch diffusion model to couple human motion and video generation process with mutual feature interaction and 3D-2D cross attentions. Moreover, we curate CoMoVi Dataset, a large-scale real-world human video dataset with text and motion annotations, covering diverse and challenging human motions. Extensive experiments demonstrate the effectiveness of our method in both 3D human motion and video generation tasks.", "link": "http://arxiv.org/abs/2601.10632v1", "date": "2026-01-15", "relevancy": 2.6543, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7071}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6356}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6247}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoMoVi%3A%20Co-Generation%20of%203D%20Human%20Motions%20and%20Realistic%20Videos&body=Title%3A%20CoMoVi%3A%20Co-Generation%20of%203D%20Human%20Motions%20and%20Realistic%20Videos%0AAuthor%3A%20Chengfeng%20Zhao%20and%20Jiazhi%20Shu%20and%20Yubo%20Zhao%20and%20Tianyu%20Huang%20and%20Jiahao%20Lu%20and%20Zekai%20Gu%20and%20Chengwei%20Ren%20and%20Zhiyang%20Dou%20and%20Qing%20Shuai%20and%20Yuan%20Liu%0AAbstract%3A%20In%20this%20paper%2C%20we%20find%20that%20the%20generation%20of%203D%20human%20motions%20and%202D%20human%20videos%20is%20intrinsically%20coupled.%203D%20motions%20provide%20the%20structural%20prior%20for%20plausibility%20and%20consistency%20in%20videos%2C%20while%20pre-trained%20video%20models%20offer%20strong%20generalization%20capabilities%20for%20motions%2C%20which%20necessitate%20coupling%20their%20generation%20processes.%20Based%20on%20this%2C%20we%20present%20CoMoVi%2C%20a%20co-generative%20framework%20that%20couples%20two%20video%20diffusion%20models%20%28VDMs%29%20to%20generate%203D%20human%20motions%20and%20videos%20synchronously%20within%20a%20single%20diffusion%20denoising%20loop.%20To%20achieve%20this%2C%20we%20first%20propose%20an%20effective%202D%20human%20motion%20representation%20that%20can%20inherit%20the%20powerful%20prior%20of%20pre-trained%20VDMs.%20Then%2C%20we%20design%20a%20dual-branch%20diffusion%20model%20to%20couple%20human%20motion%20and%20video%20generation%20process%20with%20mutual%20feature%20interaction%20and%203D-2D%20cross%20attentions.%20Moreover%2C%20we%20curate%20CoMoVi%20Dataset%2C%20a%20large-scale%20real-world%20human%20video%20dataset%20with%20text%20and%20motion%20annotations%2C%20covering%20diverse%20and%20challenging%20human%20motions.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20both%203D%20human%20motion%20and%20video%20generation%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10632v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoMoVi%253A%2520Co-Generation%2520of%25203D%2520Human%2520Motions%2520and%2520Realistic%2520Videos%26entry.906535625%3DChengfeng%2520Zhao%2520and%2520Jiazhi%2520Shu%2520and%2520Yubo%2520Zhao%2520and%2520Tianyu%2520Huang%2520and%2520Jiahao%2520Lu%2520and%2520Zekai%2520Gu%2520and%2520Chengwei%2520Ren%2520and%2520Zhiyang%2520Dou%2520and%2520Qing%2520Shuai%2520and%2520Yuan%2520Liu%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520find%2520that%2520the%2520generation%2520of%25203D%2520human%2520motions%2520and%25202D%2520human%2520videos%2520is%2520intrinsically%2520coupled.%25203D%2520motions%2520provide%2520the%2520structural%2520prior%2520for%2520plausibility%2520and%2520consistency%2520in%2520videos%252C%2520while%2520pre-trained%2520video%2520models%2520offer%2520strong%2520generalization%2520capabilities%2520for%2520motions%252C%2520which%2520necessitate%2520coupling%2520their%2520generation%2520processes.%2520Based%2520on%2520this%252C%2520we%2520present%2520CoMoVi%252C%2520a%2520co-generative%2520framework%2520that%2520couples%2520two%2520video%2520diffusion%2520models%2520%2528VDMs%2529%2520to%2520generate%25203D%2520human%2520motions%2520and%2520videos%2520synchronously%2520within%2520a%2520single%2520diffusion%2520denoising%2520loop.%2520To%2520achieve%2520this%252C%2520we%2520first%2520propose%2520an%2520effective%25202D%2520human%2520motion%2520representation%2520that%2520can%2520inherit%2520the%2520powerful%2520prior%2520of%2520pre-trained%2520VDMs.%2520Then%252C%2520we%2520design%2520a%2520dual-branch%2520diffusion%2520model%2520to%2520couple%2520human%2520motion%2520and%2520video%2520generation%2520process%2520with%2520mutual%2520feature%2520interaction%2520and%25203D-2D%2520cross%2520attentions.%2520Moreover%252C%2520we%2520curate%2520CoMoVi%2520Dataset%252C%2520a%2520large-scale%2520real-world%2520human%2520video%2520dataset%2520with%2520text%2520and%2520motion%2520annotations%252C%2520covering%2520diverse%2520and%2520challenging%2520human%2520motions.%2520Extensive%2520experiments%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520both%25203D%2520human%2520motion%2520and%2520video%2520generation%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10632v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoMoVi%3A%20Co-Generation%20of%203D%20Human%20Motions%20and%20Realistic%20Videos&entry.906535625=Chengfeng%20Zhao%20and%20Jiazhi%20Shu%20and%20Yubo%20Zhao%20and%20Tianyu%20Huang%20and%20Jiahao%20Lu%20and%20Zekai%20Gu%20and%20Chengwei%20Ren%20and%20Zhiyang%20Dou%20and%20Qing%20Shuai%20and%20Yuan%20Liu&entry.1292438233=In%20this%20paper%2C%20we%20find%20that%20the%20generation%20of%203D%20human%20motions%20and%202D%20human%20videos%20is%20intrinsically%20coupled.%203D%20motions%20provide%20the%20structural%20prior%20for%20plausibility%20and%20consistency%20in%20videos%2C%20while%20pre-trained%20video%20models%20offer%20strong%20generalization%20capabilities%20for%20motions%2C%20which%20necessitate%20coupling%20their%20generation%20processes.%20Based%20on%20this%2C%20we%20present%20CoMoVi%2C%20a%20co-generative%20framework%20that%20couples%20two%20video%20diffusion%20models%20%28VDMs%29%20to%20generate%203D%20human%20motions%20and%20videos%20synchronously%20within%20a%20single%20diffusion%20denoising%20loop.%20To%20achieve%20this%2C%20we%20first%20propose%20an%20effective%202D%20human%20motion%20representation%20that%20can%20inherit%20the%20powerful%20prior%20of%20pre-trained%20VDMs.%20Then%2C%20we%20design%20a%20dual-branch%20diffusion%20model%20to%20couple%20human%20motion%20and%20video%20generation%20process%20with%20mutual%20feature%20interaction%20and%203D-2D%20cross%20attentions.%20Moreover%2C%20we%20curate%20CoMoVi%20Dataset%2C%20a%20large-scale%20real-world%20human%20video%20dataset%20with%20text%20and%20motion%20annotations%2C%20covering%20diverse%20and%20challenging%20human%20motions.%20Extensive%20experiments%20demonstrate%20the%20effectiveness%20of%20our%20method%20in%20both%203D%20human%20motion%20and%20video%20generation%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2601.10632v1&entry.124074799=Read"},
{"title": "CoGen: Creation of Reusable UI Components in Figma via Textual Commands", "author": "Ishani Kanapathipillai and Obhasha Priyankara", "abstract": "The evolution of User Interface design has emphasized the need for efficient, reusable, and editable components to ensure an efficient design process. This research introduces CoGen, a system that uses machine learning techniques to generate reusable UI components directly in Figma, one of the most popular UI design tools. Addressing gaps in current systems, CoGen focuses on creating atomic components such as buttons, labels, and input fields using structured JSON and natural language prompts.\n  The project integrates Figma API data extraction, Seq2Seq models, and fine-tuned T5 transformers for component generation. The key results demonstrate the efficiency of the T5 model in prompt generation, with an accuracy of 98% and a BLEU score of 0.2668, which ensures the mapping of JSON to descriptive prompts. For JSON creation, CoGen achieves a success rate of up to 100% in generating simple JSON outputs for specified component types.", "link": "http://arxiv.org/abs/2601.10536v1", "date": "2026-01-15", "relevancy": 2.6468, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.562}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5305}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CoGen%3A%20Creation%20of%20Reusable%20UI%20Components%20in%20Figma%20via%20Textual%20Commands&body=Title%3A%20CoGen%3A%20Creation%20of%20Reusable%20UI%20Components%20in%20Figma%20via%20Textual%20Commands%0AAuthor%3A%20Ishani%20Kanapathipillai%20and%20Obhasha%20Priyankara%0AAbstract%3A%20The%20evolution%20of%20User%20Interface%20design%20has%20emphasized%20the%20need%20for%20efficient%2C%20reusable%2C%20and%20editable%20components%20to%20ensure%20an%20efficient%20design%20process.%20This%20research%20introduces%20CoGen%2C%20a%20system%20that%20uses%20machine%20learning%20techniques%20to%20generate%20reusable%20UI%20components%20directly%20in%20Figma%2C%20one%20of%20the%20most%20popular%20UI%20design%20tools.%20Addressing%20gaps%20in%20current%20systems%2C%20CoGen%20focuses%20on%20creating%20atomic%20components%20such%20as%20buttons%2C%20labels%2C%20and%20input%20fields%20using%20structured%20JSON%20and%20natural%20language%20prompts.%0A%20%20The%20project%20integrates%20Figma%20API%20data%20extraction%2C%20Seq2Seq%20models%2C%20and%20fine-tuned%20T5%20transformers%20for%20component%20generation.%20The%20key%20results%20demonstrate%20the%20efficiency%20of%20the%20T5%20model%20in%20prompt%20generation%2C%20with%20an%20accuracy%20of%2098%25%20and%20a%20BLEU%20score%20of%200.2668%2C%20which%20ensures%20the%20mapping%20of%20JSON%20to%20descriptive%20prompts.%20For%20JSON%20creation%2C%20CoGen%20achieves%20a%20success%20rate%20of%20up%20to%20100%25%20in%20generating%20simple%20JSON%20outputs%20for%20specified%20component%20types.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCoGen%253A%2520Creation%2520of%2520Reusable%2520UI%2520Components%2520in%2520Figma%2520via%2520Textual%2520Commands%26entry.906535625%3DIshani%2520Kanapathipillai%2520and%2520Obhasha%2520Priyankara%26entry.1292438233%3DThe%2520evolution%2520of%2520User%2520Interface%2520design%2520has%2520emphasized%2520the%2520need%2520for%2520efficient%252C%2520reusable%252C%2520and%2520editable%2520components%2520to%2520ensure%2520an%2520efficient%2520design%2520process.%2520This%2520research%2520introduces%2520CoGen%252C%2520a%2520system%2520that%2520uses%2520machine%2520learning%2520techniques%2520to%2520generate%2520reusable%2520UI%2520components%2520directly%2520in%2520Figma%252C%2520one%2520of%2520the%2520most%2520popular%2520UI%2520design%2520tools.%2520Addressing%2520gaps%2520in%2520current%2520systems%252C%2520CoGen%2520focuses%2520on%2520creating%2520atomic%2520components%2520such%2520as%2520buttons%252C%2520labels%252C%2520and%2520input%2520fields%2520using%2520structured%2520JSON%2520and%2520natural%2520language%2520prompts.%250A%2520%2520The%2520project%2520integrates%2520Figma%2520API%2520data%2520extraction%252C%2520Seq2Seq%2520models%252C%2520and%2520fine-tuned%2520T5%2520transformers%2520for%2520component%2520generation.%2520The%2520key%2520results%2520demonstrate%2520the%2520efficiency%2520of%2520the%2520T5%2520model%2520in%2520prompt%2520generation%252C%2520with%2520an%2520accuracy%2520of%252098%2525%2520and%2520a%2520BLEU%2520score%2520of%25200.2668%252C%2520which%2520ensures%2520the%2520mapping%2520of%2520JSON%2520to%2520descriptive%2520prompts.%2520For%2520JSON%2520creation%252C%2520CoGen%2520achieves%2520a%2520success%2520rate%2520of%2520up%2520to%2520100%2525%2520in%2520generating%2520simple%2520JSON%2520outputs%2520for%2520specified%2520component%2520types.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CoGen%3A%20Creation%20of%20Reusable%20UI%20Components%20in%20Figma%20via%20Textual%20Commands&entry.906535625=Ishani%20Kanapathipillai%20and%20Obhasha%20Priyankara&entry.1292438233=The%20evolution%20of%20User%20Interface%20design%20has%20emphasized%20the%20need%20for%20efficient%2C%20reusable%2C%20and%20editable%20components%20to%20ensure%20an%20efficient%20design%20process.%20This%20research%20introduces%20CoGen%2C%20a%20system%20that%20uses%20machine%20learning%20techniques%20to%20generate%20reusable%20UI%20components%20directly%20in%20Figma%2C%20one%20of%20the%20most%20popular%20UI%20design%20tools.%20Addressing%20gaps%20in%20current%20systems%2C%20CoGen%20focuses%20on%20creating%20atomic%20components%20such%20as%20buttons%2C%20labels%2C%20and%20input%20fields%20using%20structured%20JSON%20and%20natural%20language%20prompts.%0A%20%20The%20project%20integrates%20Figma%20API%20data%20extraction%2C%20Seq2Seq%20models%2C%20and%20fine-tuned%20T5%20transformers%20for%20component%20generation.%20The%20key%20results%20demonstrate%20the%20efficiency%20of%20the%20T5%20model%20in%20prompt%20generation%2C%20with%20an%20accuracy%20of%2098%25%20and%20a%20BLEU%20score%20of%200.2668%2C%20which%20ensures%20the%20mapping%20of%20JSON%20to%20descriptive%20prompts.%20For%20JSON%20creation%2C%20CoGen%20achieves%20a%20success%20rate%20of%20up%20to%20100%25%20in%20generating%20simple%20JSON%20outputs%20for%20specified%20component%20types.&entry.1838667208=http%3A//arxiv.org/abs/2601.10536v1&entry.124074799=Read"},
{"title": "Hierarchical Refinement of Universal Multimodal Attacks on Vision-Language Models", "author": "Peng-Fei Zhang and Zi Huang", "abstract": "Existing adversarial attacks for VLP models are mostly sample-specific, resulting in substantial computational overhead when scaled to large datasets or new scenarios. To overcome this limitation, we propose Hierarchical Refinement Attack (HRA), a multimodal universal attack framework for VLP models. HRA refines universal adversarial perturbations (UAPs) at both the sample level and the optimization level. For the image modality, we disentangle adversarial examples into clean images and perturbations, allowing each component to be handled independently for more effective disruption of cross-modal alignment. We further introduce a ScMix augmentation strategy that diversifies visual contexts and strengthens both global and local utility of UAPs, thereby reducing reliance on spurious features. In addition, we refine the optimization path by leveraging a temporal hierarchy of historical and estimated future gradients to avoid local minima and stabilize universal perturbation learning. For the text modality, HRA identifies globally influential words by combining intra-sentence and inter-sentence importance measures, and subsequently utilizes these words as universal text perturbations. Extensive experiments across various downstream tasks, VLP models, and datasets demonstrate the superiority of the proposed universal multimodal attacks.", "link": "http://arxiv.org/abs/2601.10313v1", "date": "2026-01-15", "relevancy": 2.6213, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5423}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5152}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Hierarchical%20Refinement%20of%20Universal%20Multimodal%20Attacks%20on%20Vision-Language%20Models&body=Title%3A%20Hierarchical%20Refinement%20of%20Universal%20Multimodal%20Attacks%20on%20Vision-Language%20Models%0AAuthor%3A%20Peng-Fei%20Zhang%20and%20Zi%20Huang%0AAbstract%3A%20Existing%20adversarial%20attacks%20for%20VLP%20models%20are%20mostly%20sample-specific%2C%20resulting%20in%20substantial%20computational%20overhead%20when%20scaled%20to%20large%20datasets%20or%20new%20scenarios.%20To%20overcome%20this%20limitation%2C%20we%20propose%20Hierarchical%20Refinement%20Attack%20%28HRA%29%2C%20a%20multimodal%20universal%20attack%20framework%20for%20VLP%20models.%20HRA%20refines%20universal%20adversarial%20perturbations%20%28UAPs%29%20at%20both%20the%20sample%20level%20and%20the%20optimization%20level.%20For%20the%20image%20modality%2C%20we%20disentangle%20adversarial%20examples%20into%20clean%20images%20and%20perturbations%2C%20allowing%20each%20component%20to%20be%20handled%20independently%20for%20more%20effective%20disruption%20of%20cross-modal%20alignment.%20We%20further%20introduce%20a%20ScMix%20augmentation%20strategy%20that%20diversifies%20visual%20contexts%20and%20strengthens%20both%20global%20and%20local%20utility%20of%20UAPs%2C%20thereby%20reducing%20reliance%20on%20spurious%20features.%20In%20addition%2C%20we%20refine%20the%20optimization%20path%20by%20leveraging%20a%20temporal%20hierarchy%20of%20historical%20and%20estimated%20future%20gradients%20to%20avoid%20local%20minima%20and%20stabilize%20universal%20perturbation%20learning.%20For%20the%20text%20modality%2C%20HRA%20identifies%20globally%20influential%20words%20by%20combining%20intra-sentence%20and%20inter-sentence%20importance%20measures%2C%20and%20subsequently%20utilizes%20these%20words%20as%20universal%20text%20perturbations.%20Extensive%20experiments%20across%20various%20downstream%20tasks%2C%20VLP%20models%2C%20and%20datasets%20demonstrate%20the%20superiority%20of%20the%20proposed%20universal%20multimodal%20attacks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10313v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHierarchical%2520Refinement%2520of%2520Universal%2520Multimodal%2520Attacks%2520on%2520Vision-Language%2520Models%26entry.906535625%3DPeng-Fei%2520Zhang%2520and%2520Zi%2520Huang%26entry.1292438233%3DExisting%2520adversarial%2520attacks%2520for%2520VLP%2520models%2520are%2520mostly%2520sample-specific%252C%2520resulting%2520in%2520substantial%2520computational%2520overhead%2520when%2520scaled%2520to%2520large%2520datasets%2520or%2520new%2520scenarios.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520Hierarchical%2520Refinement%2520Attack%2520%2528HRA%2529%252C%2520a%2520multimodal%2520universal%2520attack%2520framework%2520for%2520VLP%2520models.%2520HRA%2520refines%2520universal%2520adversarial%2520perturbations%2520%2528UAPs%2529%2520at%2520both%2520the%2520sample%2520level%2520and%2520the%2520optimization%2520level.%2520For%2520the%2520image%2520modality%252C%2520we%2520disentangle%2520adversarial%2520examples%2520into%2520clean%2520images%2520and%2520perturbations%252C%2520allowing%2520each%2520component%2520to%2520be%2520handled%2520independently%2520for%2520more%2520effective%2520disruption%2520of%2520cross-modal%2520alignment.%2520We%2520further%2520introduce%2520a%2520ScMix%2520augmentation%2520strategy%2520that%2520diversifies%2520visual%2520contexts%2520and%2520strengthens%2520both%2520global%2520and%2520local%2520utility%2520of%2520UAPs%252C%2520thereby%2520reducing%2520reliance%2520on%2520spurious%2520features.%2520In%2520addition%252C%2520we%2520refine%2520the%2520optimization%2520path%2520by%2520leveraging%2520a%2520temporal%2520hierarchy%2520of%2520historical%2520and%2520estimated%2520future%2520gradients%2520to%2520avoid%2520local%2520minima%2520and%2520stabilize%2520universal%2520perturbation%2520learning.%2520For%2520the%2520text%2520modality%252C%2520HRA%2520identifies%2520globally%2520influential%2520words%2520by%2520combining%2520intra-sentence%2520and%2520inter-sentence%2520importance%2520measures%252C%2520and%2520subsequently%2520utilizes%2520these%2520words%2520as%2520universal%2520text%2520perturbations.%2520Extensive%2520experiments%2520across%2520various%2520downstream%2520tasks%252C%2520VLP%2520models%252C%2520and%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%2520universal%2520multimodal%2520attacks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10313v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Hierarchical%20Refinement%20of%20Universal%20Multimodal%20Attacks%20on%20Vision-Language%20Models&entry.906535625=Peng-Fei%20Zhang%20and%20Zi%20Huang&entry.1292438233=Existing%20adversarial%20attacks%20for%20VLP%20models%20are%20mostly%20sample-specific%2C%20resulting%20in%20substantial%20computational%20overhead%20when%20scaled%20to%20large%20datasets%20or%20new%20scenarios.%20To%20overcome%20this%20limitation%2C%20we%20propose%20Hierarchical%20Refinement%20Attack%20%28HRA%29%2C%20a%20multimodal%20universal%20attack%20framework%20for%20VLP%20models.%20HRA%20refines%20universal%20adversarial%20perturbations%20%28UAPs%29%20at%20both%20the%20sample%20level%20and%20the%20optimization%20level.%20For%20the%20image%20modality%2C%20we%20disentangle%20adversarial%20examples%20into%20clean%20images%20and%20perturbations%2C%20allowing%20each%20component%20to%20be%20handled%20independently%20for%20more%20effective%20disruption%20of%20cross-modal%20alignment.%20We%20further%20introduce%20a%20ScMix%20augmentation%20strategy%20that%20diversifies%20visual%20contexts%20and%20strengthens%20both%20global%20and%20local%20utility%20of%20UAPs%2C%20thereby%20reducing%20reliance%20on%20spurious%20features.%20In%20addition%2C%20we%20refine%20the%20optimization%20path%20by%20leveraging%20a%20temporal%20hierarchy%20of%20historical%20and%20estimated%20future%20gradients%20to%20avoid%20local%20minima%20and%20stabilize%20universal%20perturbation%20learning.%20For%20the%20text%20modality%2C%20HRA%20identifies%20globally%20influential%20words%20by%20combining%20intra-sentence%20and%20inter-sentence%20importance%20measures%2C%20and%20subsequently%20utilizes%20these%20words%20as%20universal%20text%20perturbations.%20Extensive%20experiments%20across%20various%20downstream%20tasks%2C%20VLP%20models%2C%20and%20datasets%20demonstrate%20the%20superiority%20of%20the%20proposed%20universal%20multimodal%20attacks.&entry.1838667208=http%3A//arxiv.org/abs/2601.10313v1&entry.124074799=Read"},
{"title": "Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning", "author": "Guoqiang Liang and Jianyi Wang and Zhonghua Wu and Shangchen Zhou", "abstract": "Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or providing low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA by jointly generating quality descriptions and scores. However, existing VLM-based IQA methods often suffer from unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions, and 2) reinforcement learning (RL) for dynamic policy exploration, stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, with a Progressive Re-sampling Strategy for mitigating annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.", "link": "http://arxiv.org/abs/2601.02918v2", "date": "2026-01-15", "relevancy": 2.6156, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5239}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5216}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Zoom-IQA%3A%20Image%20Quality%20Assessment%20with%20Reliable%20Region-Aware%20Reasoning&body=Title%3A%20Zoom-IQA%3A%20Image%20Quality%20Assessment%20with%20Reliable%20Region-Aware%20Reasoning%0AAuthor%3A%20Guoqiang%20Liang%20and%20Jianyi%20Wang%20and%20Zhonghua%20Wu%20and%20Shangchen%20Zhou%0AAbstract%3A%20Image%20Quality%20Assessment%20%28IQA%29%20is%20a%20long-standing%20problem%20in%20computer%20vision.%20Previous%20methods%20typically%20focus%20on%20predicting%20numerical%20scores%20without%20explanation%20or%20providing%20low-level%20descriptions%20lacking%20precise%20scores.%20Recent%20reasoning-based%20vision%20language%20models%20%28VLMs%29%20have%20shown%20strong%20potential%20for%20IQA%20by%20jointly%20generating%20quality%20descriptions%20and%20scores.%20However%2C%20existing%20VLM-based%20IQA%20methods%20often%20suffer%20from%20unreliable%20reasoning%20due%20to%20their%20limited%20capability%20of%20integrating%20visual%20and%20textual%20cues.%20In%20this%20work%2C%20we%20introduce%20Zoom-IQA%2C%20a%20VLM-based%20IQA%20model%20to%20explicitly%20emulate%20key%20cognitive%20behaviors%3A%20uncertainty%20awareness%2C%20region%20reasoning%2C%20and%20iterative%20refinement.%20Specifically%2C%20we%20present%20a%20two-stage%20training%20pipeline%3A%201%29%20supervised%20fine-tuning%20%28SFT%29%20on%20our%20Grounded-Rationale-IQA%20%28GR-IQA%29%20dataset%20to%20teach%20the%20model%20to%20ground%20its%20assessments%20in%20key%20regions%2C%20and%202%29%20reinforcement%20learning%20%28RL%29%20for%20dynamic%20policy%20exploration%2C%20stabilized%20by%20our%20KL-Coverage%20regularizer%20to%20prevent%20reasoning%20and%20scoring%20diversity%20collapse%2C%20with%20a%20Progressive%20Re-sampling%20Strategy%20for%20mitigating%20annotation%20bias.%20Extensive%20experiments%20show%20that%20Zoom-IQA%20achieves%20improved%20robustness%2C%20explainability%2C%20and%20generalization.%20The%20application%20to%20downstream%20tasks%2C%20such%20as%20image%20restoration%2C%20further%20demonstrates%20the%20effectiveness%20of%20Zoom-IQA.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02918v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DZoom-IQA%253A%2520Image%2520Quality%2520Assessment%2520with%2520Reliable%2520Region-Aware%2520Reasoning%26entry.906535625%3DGuoqiang%2520Liang%2520and%2520Jianyi%2520Wang%2520and%2520Zhonghua%2520Wu%2520and%2520Shangchen%2520Zhou%26entry.1292438233%3DImage%2520Quality%2520Assessment%2520%2528IQA%2529%2520is%2520a%2520long-standing%2520problem%2520in%2520computer%2520vision.%2520Previous%2520methods%2520typically%2520focus%2520on%2520predicting%2520numerical%2520scores%2520without%2520explanation%2520or%2520providing%2520low-level%2520descriptions%2520lacking%2520precise%2520scores.%2520Recent%2520reasoning-based%2520vision%2520language%2520models%2520%2528VLMs%2529%2520have%2520shown%2520strong%2520potential%2520for%2520IQA%2520by%2520jointly%2520generating%2520quality%2520descriptions%2520and%2520scores.%2520However%252C%2520existing%2520VLM-based%2520IQA%2520methods%2520often%2520suffer%2520from%2520unreliable%2520reasoning%2520due%2520to%2520their%2520limited%2520capability%2520of%2520integrating%2520visual%2520and%2520textual%2520cues.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Zoom-IQA%252C%2520a%2520VLM-based%2520IQA%2520model%2520to%2520explicitly%2520emulate%2520key%2520cognitive%2520behaviors%253A%2520uncertainty%2520awareness%252C%2520region%2520reasoning%252C%2520and%2520iterative%2520refinement.%2520Specifically%252C%2520we%2520present%2520a%2520two-stage%2520training%2520pipeline%253A%25201%2529%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520on%2520our%2520Grounded-Rationale-IQA%2520%2528GR-IQA%2529%2520dataset%2520to%2520teach%2520the%2520model%2520to%2520ground%2520its%2520assessments%2520in%2520key%2520regions%252C%2520and%25202%2529%2520reinforcement%2520learning%2520%2528RL%2529%2520for%2520dynamic%2520policy%2520exploration%252C%2520stabilized%2520by%2520our%2520KL-Coverage%2520regularizer%2520to%2520prevent%2520reasoning%2520and%2520scoring%2520diversity%2520collapse%252C%2520with%2520a%2520Progressive%2520Re-sampling%2520Strategy%2520for%2520mitigating%2520annotation%2520bias.%2520Extensive%2520experiments%2520show%2520that%2520Zoom-IQA%2520achieves%2520improved%2520robustness%252C%2520explainability%252C%2520and%2520generalization.%2520The%2520application%2520to%2520downstream%2520tasks%252C%2520such%2520as%2520image%2520restoration%252C%2520further%2520demonstrates%2520the%2520effectiveness%2520of%2520Zoom-IQA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02918v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Zoom-IQA%3A%20Image%20Quality%20Assessment%20with%20Reliable%20Region-Aware%20Reasoning&entry.906535625=Guoqiang%20Liang%20and%20Jianyi%20Wang%20and%20Zhonghua%20Wu%20and%20Shangchen%20Zhou&entry.1292438233=Image%20Quality%20Assessment%20%28IQA%29%20is%20a%20long-standing%20problem%20in%20computer%20vision.%20Previous%20methods%20typically%20focus%20on%20predicting%20numerical%20scores%20without%20explanation%20or%20providing%20low-level%20descriptions%20lacking%20precise%20scores.%20Recent%20reasoning-based%20vision%20language%20models%20%28VLMs%29%20have%20shown%20strong%20potential%20for%20IQA%20by%20jointly%20generating%20quality%20descriptions%20and%20scores.%20However%2C%20existing%20VLM-based%20IQA%20methods%20often%20suffer%20from%20unreliable%20reasoning%20due%20to%20their%20limited%20capability%20of%20integrating%20visual%20and%20textual%20cues.%20In%20this%20work%2C%20we%20introduce%20Zoom-IQA%2C%20a%20VLM-based%20IQA%20model%20to%20explicitly%20emulate%20key%20cognitive%20behaviors%3A%20uncertainty%20awareness%2C%20region%20reasoning%2C%20and%20iterative%20refinement.%20Specifically%2C%20we%20present%20a%20two-stage%20training%20pipeline%3A%201%29%20supervised%20fine-tuning%20%28SFT%29%20on%20our%20Grounded-Rationale-IQA%20%28GR-IQA%29%20dataset%20to%20teach%20the%20model%20to%20ground%20its%20assessments%20in%20key%20regions%2C%20and%202%29%20reinforcement%20learning%20%28RL%29%20for%20dynamic%20policy%20exploration%2C%20stabilized%20by%20our%20KL-Coverage%20regularizer%20to%20prevent%20reasoning%20and%20scoring%20diversity%20collapse%2C%20with%20a%20Progressive%20Re-sampling%20Strategy%20for%20mitigating%20annotation%20bias.%20Extensive%20experiments%20show%20that%20Zoom-IQA%20achieves%20improved%20robustness%2C%20explainability%2C%20and%20generalization.%20The%20application%20to%20downstream%20tasks%2C%20such%20as%20image%20restoration%2C%20further%20demonstrates%20the%20effectiveness%20of%20Zoom-IQA.&entry.1838667208=http%3A//arxiv.org/abs/2601.02918v2&entry.124074799=Read"},
{"title": "SPATIALGEN: Layout-guided 3D Indoor Scene Generation", "author": "Chuan Fang and Heng Li and Yixun Liang and Jia Zheng and Yongsen Mao and Yuan Liu and Rui Tang and Zihan Zhou and Ping Tan", "abstract": "Creating high-fidelity 3D models of indoor environments is essential for applications in design, virtual reality, and robotics. However, manual 3D modeling remains time-consuming and labor-intensive. While recent advances in generative AI have enabled automated scene synthesis, existing methods often face challenges in balancing visual quality, diversity, semantic consistency, and user control. A major bottleneck is the lack of a large-scale, high-quality dataset tailored to this task. To address this gap, we introduce a comprehensive synthetic dataset, featuring 12,328 structured annotated scenes with 57,431 rooms, and 4.7M photorealistic 2D renderings. Leveraging this dataset, we present SpatialGen, a novel multi-view multi-modal diffusion model that generates realistic and semantically consistent 3D indoor scenes. Given a 3D layout and a reference image (derived from a text prompt), our model synthesizes appearance (color image), geometry (scene coordinate map), and semantic (semantic segmentation map) from arbitrary viewpoints, while preserving spatial consistency across modalities. SpatialGen consistently generates superior results to previous methods in our experiments. We are open-sourcing our data and models to empower the community and advance the field of indoor scene understanding and generation.", "link": "http://arxiv.org/abs/2509.14981v4", "date": "2026-01-15", "relevancy": 2.6131, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6551}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6551}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6439}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SPATIALGEN%3A%20Layout-guided%203D%20Indoor%20Scene%20Generation&body=Title%3A%20SPATIALGEN%3A%20Layout-guided%203D%20Indoor%20Scene%20Generation%0AAuthor%3A%20Chuan%20Fang%20and%20Heng%20Li%20and%20Yixun%20Liang%20and%20Jia%20Zheng%20and%20Yongsen%20Mao%20and%20Yuan%20Liu%20and%20Rui%20Tang%20and%20Zihan%20Zhou%20and%20Ping%20Tan%0AAbstract%3A%20Creating%20high-fidelity%203D%20models%20of%20indoor%20environments%20is%20essential%20for%20applications%20in%20design%2C%20virtual%20reality%2C%20and%20robotics.%20However%2C%20manual%203D%20modeling%20remains%20time-consuming%20and%20labor-intensive.%20While%20recent%20advances%20in%20generative%20AI%20have%20enabled%20automated%20scene%20synthesis%2C%20existing%20methods%20often%20face%20challenges%20in%20balancing%20visual%20quality%2C%20diversity%2C%20semantic%20consistency%2C%20and%20user%20control.%20A%20major%20bottleneck%20is%20the%20lack%20of%20a%20large-scale%2C%20high-quality%20dataset%20tailored%20to%20this%20task.%20To%20address%20this%20gap%2C%20we%20introduce%20a%20comprehensive%20synthetic%20dataset%2C%20featuring%2012%2C328%20structured%20annotated%20scenes%20with%2057%2C431%20rooms%2C%20and%204.7M%20photorealistic%202D%20renderings.%20Leveraging%20this%20dataset%2C%20we%20present%20SpatialGen%2C%20a%20novel%20multi-view%20multi-modal%20diffusion%20model%20that%20generates%20realistic%20and%20semantically%20consistent%203D%20indoor%20scenes.%20Given%20a%203D%20layout%20and%20a%20reference%20image%20%28derived%20from%20a%20text%20prompt%29%2C%20our%20model%20synthesizes%20appearance%20%28color%20image%29%2C%20geometry%20%28scene%20coordinate%20map%29%2C%20and%20semantic%20%28semantic%20segmentation%20map%29%20from%20arbitrary%20viewpoints%2C%20while%20preserving%20spatial%20consistency%20across%20modalities.%20SpatialGen%20consistently%20generates%20superior%20results%20to%20previous%20methods%20in%20our%20experiments.%20We%20are%20open-sourcing%20our%20data%20and%20models%20to%20empower%20the%20community%20and%20advance%20the%20field%20of%20indoor%20scene%20understanding%20and%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2509.14981v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSPATIALGEN%253A%2520Layout-guided%25203D%2520Indoor%2520Scene%2520Generation%26entry.906535625%3DChuan%2520Fang%2520and%2520Heng%2520Li%2520and%2520Yixun%2520Liang%2520and%2520Jia%2520Zheng%2520and%2520Yongsen%2520Mao%2520and%2520Yuan%2520Liu%2520and%2520Rui%2520Tang%2520and%2520Zihan%2520Zhou%2520and%2520Ping%2520Tan%26entry.1292438233%3DCreating%2520high-fidelity%25203D%2520models%2520of%2520indoor%2520environments%2520is%2520essential%2520for%2520applications%2520in%2520design%252C%2520virtual%2520reality%252C%2520and%2520robotics.%2520However%252C%2520manual%25203D%2520modeling%2520remains%2520time-consuming%2520and%2520labor-intensive.%2520While%2520recent%2520advances%2520in%2520generative%2520AI%2520have%2520enabled%2520automated%2520scene%2520synthesis%252C%2520existing%2520methods%2520often%2520face%2520challenges%2520in%2520balancing%2520visual%2520quality%252C%2520diversity%252C%2520semantic%2520consistency%252C%2520and%2520user%2520control.%2520A%2520major%2520bottleneck%2520is%2520the%2520lack%2520of%2520a%2520large-scale%252C%2520high-quality%2520dataset%2520tailored%2520to%2520this%2520task.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520a%2520comprehensive%2520synthetic%2520dataset%252C%2520featuring%252012%252C328%2520structured%2520annotated%2520scenes%2520with%252057%252C431%2520rooms%252C%2520and%25204.7M%2520photorealistic%25202D%2520renderings.%2520Leveraging%2520this%2520dataset%252C%2520we%2520present%2520SpatialGen%252C%2520a%2520novel%2520multi-view%2520multi-modal%2520diffusion%2520model%2520that%2520generates%2520realistic%2520and%2520semantically%2520consistent%25203D%2520indoor%2520scenes.%2520Given%2520a%25203D%2520layout%2520and%2520a%2520reference%2520image%2520%2528derived%2520from%2520a%2520text%2520prompt%2529%252C%2520our%2520model%2520synthesizes%2520appearance%2520%2528color%2520image%2529%252C%2520geometry%2520%2528scene%2520coordinate%2520map%2529%252C%2520and%2520semantic%2520%2528semantic%2520segmentation%2520map%2529%2520from%2520arbitrary%2520viewpoints%252C%2520while%2520preserving%2520spatial%2520consistency%2520across%2520modalities.%2520SpatialGen%2520consistently%2520generates%2520superior%2520results%2520to%2520previous%2520methods%2520in%2520our%2520experiments.%2520We%2520are%2520open-sourcing%2520our%2520data%2520and%2520models%2520to%2520empower%2520the%2520community%2520and%2520advance%2520the%2520field%2520of%2520indoor%2520scene%2520understanding%2520and%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.14981v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SPATIALGEN%3A%20Layout-guided%203D%20Indoor%20Scene%20Generation&entry.906535625=Chuan%20Fang%20and%20Heng%20Li%20and%20Yixun%20Liang%20and%20Jia%20Zheng%20and%20Yongsen%20Mao%20and%20Yuan%20Liu%20and%20Rui%20Tang%20and%20Zihan%20Zhou%20and%20Ping%20Tan&entry.1292438233=Creating%20high-fidelity%203D%20models%20of%20indoor%20environments%20is%20essential%20for%20applications%20in%20design%2C%20virtual%20reality%2C%20and%20robotics.%20However%2C%20manual%203D%20modeling%20remains%20time-consuming%20and%20labor-intensive.%20While%20recent%20advances%20in%20generative%20AI%20have%20enabled%20automated%20scene%20synthesis%2C%20existing%20methods%20often%20face%20challenges%20in%20balancing%20visual%20quality%2C%20diversity%2C%20semantic%20consistency%2C%20and%20user%20control.%20A%20major%20bottleneck%20is%20the%20lack%20of%20a%20large-scale%2C%20high-quality%20dataset%20tailored%20to%20this%20task.%20To%20address%20this%20gap%2C%20we%20introduce%20a%20comprehensive%20synthetic%20dataset%2C%20featuring%2012%2C328%20structured%20annotated%20scenes%20with%2057%2C431%20rooms%2C%20and%204.7M%20photorealistic%202D%20renderings.%20Leveraging%20this%20dataset%2C%20we%20present%20SpatialGen%2C%20a%20novel%20multi-view%20multi-modal%20diffusion%20model%20that%20generates%20realistic%20and%20semantically%20consistent%203D%20indoor%20scenes.%20Given%20a%203D%20layout%20and%20a%20reference%20image%20%28derived%20from%20a%20text%20prompt%29%2C%20our%20model%20synthesizes%20appearance%20%28color%20image%29%2C%20geometry%20%28scene%20coordinate%20map%29%2C%20and%20semantic%20%28semantic%20segmentation%20map%29%20from%20arbitrary%20viewpoints%2C%20while%20preserving%20spatial%20consistency%20across%20modalities.%20SpatialGen%20consistently%20generates%20superior%20results%20to%20previous%20methods%20in%20our%20experiments.%20We%20are%20open-sourcing%20our%20data%20and%20models%20to%20empower%20the%20community%20and%20advance%20the%20field%20of%20indoor%20scene%20understanding%20and%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2509.14981v4&entry.124074799=Read"},
{"title": "NoReGeo: Non-Reasoning Geometry Benchmark", "author": "Irina Abdullaeva and Anton Vasiliuk and Elizaveta Goncharova and Temurbek Rahmatullaev and Zagorulko Ivan and Maxim Kurkin and Andrey Kuznetsov", "abstract": "We present NoReGeo, a novel benchmark designed to evaluate the intrinsic geometric understanding of large language models (LLMs) without relying on reasoning or algebraic computation. Unlike existing benchmarks that primarily assess models' proficiency in reasoning-based geometry-where solutions are derived using algebraic methods-NoReGeo focuses on evaluating whether LLMs can inherently encode spatial relationships and recognize geometric properties directly. Our benchmark comprises 2,500 trivial geometric problems spanning 25 categories, each carefully crafted to be solvable purely through native geometric understanding, assuming known object locations. We assess a range of state-of-the-art models on NoReGeo, including frontier models like GPT-4, observing that even the most advanced systems achieve an overall maximum of 65% accuracy in binary classification tasks. Further, our ablation experiments demonstrate that such geometric understanding does not emerge through fine-tuning alone, indicating that effective training for geometric comprehension requires a specialized approach from the outset. Our findings highlight a significant gap in current LLMs' ability to natively grasp geometric concepts, providing a foundation for future research toward models with true geometric cognition.", "link": "http://arxiv.org/abs/2601.10254v1", "date": "2026-01-15", "relevancy": 2.6031, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5244}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5244}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5132}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NoReGeo%3A%20Non-Reasoning%20Geometry%20Benchmark&body=Title%3A%20NoReGeo%3A%20Non-Reasoning%20Geometry%20Benchmark%0AAuthor%3A%20Irina%20Abdullaeva%20and%20Anton%20Vasiliuk%20and%20Elizaveta%20Goncharova%20and%20Temurbek%20Rahmatullaev%20and%20Zagorulko%20Ivan%20and%20Maxim%20Kurkin%20and%20Andrey%20Kuznetsov%0AAbstract%3A%20We%20present%20NoReGeo%2C%20a%20novel%20benchmark%20designed%20to%20evaluate%20the%20intrinsic%20geometric%20understanding%20of%20large%20language%20models%20%28LLMs%29%20without%20relying%20on%20reasoning%20or%20algebraic%20computation.%20Unlike%20existing%20benchmarks%20that%20primarily%20assess%20models%27%20proficiency%20in%20reasoning-based%20geometry-where%20solutions%20are%20derived%20using%20algebraic%20methods-NoReGeo%20focuses%20on%20evaluating%20whether%20LLMs%20can%20inherently%20encode%20spatial%20relationships%20and%20recognize%20geometric%20properties%20directly.%20Our%20benchmark%20comprises%202%2C500%20trivial%20geometric%20problems%20spanning%2025%20categories%2C%20each%20carefully%20crafted%20to%20be%20solvable%20purely%20through%20native%20geometric%20understanding%2C%20assuming%20known%20object%20locations.%20We%20assess%20a%20range%20of%20state-of-the-art%20models%20on%20NoReGeo%2C%20including%20frontier%20models%20like%20GPT-4%2C%20observing%20that%20even%20the%20most%20advanced%20systems%20achieve%20an%20overall%20maximum%20of%2065%25%20accuracy%20in%20binary%20classification%20tasks.%20Further%2C%20our%20ablation%20experiments%20demonstrate%20that%20such%20geometric%20understanding%20does%20not%20emerge%20through%20fine-tuning%20alone%2C%20indicating%20that%20effective%20training%20for%20geometric%20comprehension%20requires%20a%20specialized%20approach%20from%20the%20outset.%20Our%20findings%20highlight%20a%20significant%20gap%20in%20current%20LLMs%27%20ability%20to%20natively%20grasp%20geometric%20concepts%2C%20providing%20a%20foundation%20for%20future%20research%20toward%20models%20with%20true%20geometric%20cognition.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10254v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNoReGeo%253A%2520Non-Reasoning%2520Geometry%2520Benchmark%26entry.906535625%3DIrina%2520Abdullaeva%2520and%2520Anton%2520Vasiliuk%2520and%2520Elizaveta%2520Goncharova%2520and%2520Temurbek%2520Rahmatullaev%2520and%2520Zagorulko%2520Ivan%2520and%2520Maxim%2520Kurkin%2520and%2520Andrey%2520Kuznetsov%26entry.1292438233%3DWe%2520present%2520NoReGeo%252C%2520a%2520novel%2520benchmark%2520designed%2520to%2520evaluate%2520the%2520intrinsic%2520geometric%2520understanding%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520without%2520relying%2520on%2520reasoning%2520or%2520algebraic%2520computation.%2520Unlike%2520existing%2520benchmarks%2520that%2520primarily%2520assess%2520models%2527%2520proficiency%2520in%2520reasoning-based%2520geometry-where%2520solutions%2520are%2520derived%2520using%2520algebraic%2520methods-NoReGeo%2520focuses%2520on%2520evaluating%2520whether%2520LLMs%2520can%2520inherently%2520encode%2520spatial%2520relationships%2520and%2520recognize%2520geometric%2520properties%2520directly.%2520Our%2520benchmark%2520comprises%25202%252C500%2520trivial%2520geometric%2520problems%2520spanning%252025%2520categories%252C%2520each%2520carefully%2520crafted%2520to%2520be%2520solvable%2520purely%2520through%2520native%2520geometric%2520understanding%252C%2520assuming%2520known%2520object%2520locations.%2520We%2520assess%2520a%2520range%2520of%2520state-of-the-art%2520models%2520on%2520NoReGeo%252C%2520including%2520frontier%2520models%2520like%2520GPT-4%252C%2520observing%2520that%2520even%2520the%2520most%2520advanced%2520systems%2520achieve%2520an%2520overall%2520maximum%2520of%252065%2525%2520accuracy%2520in%2520binary%2520classification%2520tasks.%2520Further%252C%2520our%2520ablation%2520experiments%2520demonstrate%2520that%2520such%2520geometric%2520understanding%2520does%2520not%2520emerge%2520through%2520fine-tuning%2520alone%252C%2520indicating%2520that%2520effective%2520training%2520for%2520geometric%2520comprehension%2520requires%2520a%2520specialized%2520approach%2520from%2520the%2520outset.%2520Our%2520findings%2520highlight%2520a%2520significant%2520gap%2520in%2520current%2520LLMs%2527%2520ability%2520to%2520natively%2520grasp%2520geometric%2520concepts%252C%2520providing%2520a%2520foundation%2520for%2520future%2520research%2520toward%2520models%2520with%2520true%2520geometric%2520cognition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10254v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NoReGeo%3A%20Non-Reasoning%20Geometry%20Benchmark&entry.906535625=Irina%20Abdullaeva%20and%20Anton%20Vasiliuk%20and%20Elizaveta%20Goncharova%20and%20Temurbek%20Rahmatullaev%20and%20Zagorulko%20Ivan%20and%20Maxim%20Kurkin%20and%20Andrey%20Kuznetsov&entry.1292438233=We%20present%20NoReGeo%2C%20a%20novel%20benchmark%20designed%20to%20evaluate%20the%20intrinsic%20geometric%20understanding%20of%20large%20language%20models%20%28LLMs%29%20without%20relying%20on%20reasoning%20or%20algebraic%20computation.%20Unlike%20existing%20benchmarks%20that%20primarily%20assess%20models%27%20proficiency%20in%20reasoning-based%20geometry-where%20solutions%20are%20derived%20using%20algebraic%20methods-NoReGeo%20focuses%20on%20evaluating%20whether%20LLMs%20can%20inherently%20encode%20spatial%20relationships%20and%20recognize%20geometric%20properties%20directly.%20Our%20benchmark%20comprises%202%2C500%20trivial%20geometric%20problems%20spanning%2025%20categories%2C%20each%20carefully%20crafted%20to%20be%20solvable%20purely%20through%20native%20geometric%20understanding%2C%20assuming%20known%20object%20locations.%20We%20assess%20a%20range%20of%20state-of-the-art%20models%20on%20NoReGeo%2C%20including%20frontier%20models%20like%20GPT-4%2C%20observing%20that%20even%20the%20most%20advanced%20systems%20achieve%20an%20overall%20maximum%20of%2065%25%20accuracy%20in%20binary%20classification%20tasks.%20Further%2C%20our%20ablation%20experiments%20demonstrate%20that%20such%20geometric%20understanding%20does%20not%20emerge%20through%20fine-tuning%20alone%2C%20indicating%20that%20effective%20training%20for%20geometric%20comprehension%20requires%20a%20specialized%20approach%20from%20the%20outset.%20Our%20findings%20highlight%20a%20significant%20gap%20in%20current%20LLMs%27%20ability%20to%20natively%20grasp%20geometric%20concepts%2C%20providing%20a%20foundation%20for%20future%20research%20toward%20models%20with%20true%20geometric%20cognition.&entry.1838667208=http%3A//arxiv.org/abs/2601.10254v1&entry.124074799=Read"},
{"title": "RS2-SAM2: Customized SAM2 for Referring Remote Sensing Image Segmentation", "author": "Fu Rong and Meng Lan and Qian Zhang and Lefei Zhang", "abstract": "Referring Remote Sensing Image Segmentation (RRSIS) aims to segment target objects in remote sensing (RS) images based on textual descriptions. Although Segment Anything Model 2 (SAM2) has shown remarkable performance in various segmentation tasks, its application to RRSIS presents several challenges, including understanding the text-described RS scenes and generating effective prompts from text. To address these issues, we propose \\textbf{RS2-SAM2}, a novel framework that adapts SAM2 to RRSIS by aligning the adapted RS features and textual features while providing pseudo-mask-based dense prompts. Specifically, we employ a union encoder to jointly encode the visual and textual inputs, generating aligned visual and text embeddings as well as multimodal class tokens. A bidirectional hierarchical fusion module is introduced to adapt SAM2 to RS scenes and align adapted visual features with the visually enhanced text embeddings, improving the model's interpretation of text-described RS scenes. To provide precise target cues for SAM2, we design a mask prompt generator, which takes the visual embeddings and class tokens as input and produces a pseudo-mask as the dense prompt of SAM2. Experimental results on several RRSIS benchmarks demonstrate that RS2-SAM2 achieves state-of-the-art performance.", "link": "http://arxiv.org/abs/2503.07266v4", "date": "2026-01-15", "relevancy": 2.5927, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5267}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5239}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RS2-SAM2%3A%20Customized%20SAM2%20for%20Referring%20Remote%20Sensing%20Image%20Segmentation&body=Title%3A%20RS2-SAM2%3A%20Customized%20SAM2%20for%20Referring%20Remote%20Sensing%20Image%20Segmentation%0AAuthor%3A%20Fu%20Rong%20and%20Meng%20Lan%20and%20Qian%20Zhang%20and%20Lefei%20Zhang%0AAbstract%3A%20Referring%20Remote%20Sensing%20Image%20Segmentation%20%28RRSIS%29%20aims%20to%20segment%20target%20objects%20in%20remote%20sensing%20%28RS%29%20images%20based%20on%20textual%20descriptions.%20Although%20Segment%20Anything%20Model%202%20%28SAM2%29%20has%20shown%20remarkable%20performance%20in%20various%20segmentation%20tasks%2C%20its%20application%20to%20RRSIS%20presents%20several%20challenges%2C%20including%20understanding%20the%20text-described%20RS%20scenes%20and%20generating%20effective%20prompts%20from%20text.%20To%20address%20these%20issues%2C%20we%20propose%20%5Ctextbf%7BRS2-SAM2%7D%2C%20a%20novel%20framework%20that%20adapts%20SAM2%20to%20RRSIS%20by%20aligning%20the%20adapted%20RS%20features%20and%20textual%20features%20while%20providing%20pseudo-mask-based%20dense%20prompts.%20Specifically%2C%20we%20employ%20a%20union%20encoder%20to%20jointly%20encode%20the%20visual%20and%20textual%20inputs%2C%20generating%20aligned%20visual%20and%20text%20embeddings%20as%20well%20as%20multimodal%20class%20tokens.%20A%20bidirectional%20hierarchical%20fusion%20module%20is%20introduced%20to%20adapt%20SAM2%20to%20RS%20scenes%20and%20align%20adapted%20visual%20features%20with%20the%20visually%20enhanced%20text%20embeddings%2C%20improving%20the%20model%27s%20interpretation%20of%20text-described%20RS%20scenes.%20To%20provide%20precise%20target%20cues%20for%20SAM2%2C%20we%20design%20a%20mask%20prompt%20generator%2C%20which%20takes%20the%20visual%20embeddings%20and%20class%20tokens%20as%20input%20and%20produces%20a%20pseudo-mask%20as%20the%20dense%20prompt%20of%20SAM2.%20Experimental%20results%20on%20several%20RRSIS%20benchmarks%20demonstrate%20that%20RS2-SAM2%20achieves%20state-of-the-art%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2503.07266v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRS2-SAM2%253A%2520Customized%2520SAM2%2520for%2520Referring%2520Remote%2520Sensing%2520Image%2520Segmentation%26entry.906535625%3DFu%2520Rong%2520and%2520Meng%2520Lan%2520and%2520Qian%2520Zhang%2520and%2520Lefei%2520Zhang%26entry.1292438233%3DReferring%2520Remote%2520Sensing%2520Image%2520Segmentation%2520%2528RRSIS%2529%2520aims%2520to%2520segment%2520target%2520objects%2520in%2520remote%2520sensing%2520%2528RS%2529%2520images%2520based%2520on%2520textual%2520descriptions.%2520Although%2520Segment%2520Anything%2520Model%25202%2520%2528SAM2%2529%2520has%2520shown%2520remarkable%2520performance%2520in%2520various%2520segmentation%2520tasks%252C%2520its%2520application%2520to%2520RRSIS%2520presents%2520several%2520challenges%252C%2520including%2520understanding%2520the%2520text-described%2520RS%2520scenes%2520and%2520generating%2520effective%2520prompts%2520from%2520text.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520%255Ctextbf%257BRS2-SAM2%257D%252C%2520a%2520novel%2520framework%2520that%2520adapts%2520SAM2%2520to%2520RRSIS%2520by%2520aligning%2520the%2520adapted%2520RS%2520features%2520and%2520textual%2520features%2520while%2520providing%2520pseudo-mask-based%2520dense%2520prompts.%2520Specifically%252C%2520we%2520employ%2520a%2520union%2520encoder%2520to%2520jointly%2520encode%2520the%2520visual%2520and%2520textual%2520inputs%252C%2520generating%2520aligned%2520visual%2520and%2520text%2520embeddings%2520as%2520well%2520as%2520multimodal%2520class%2520tokens.%2520A%2520bidirectional%2520hierarchical%2520fusion%2520module%2520is%2520introduced%2520to%2520adapt%2520SAM2%2520to%2520RS%2520scenes%2520and%2520align%2520adapted%2520visual%2520features%2520with%2520the%2520visually%2520enhanced%2520text%2520embeddings%252C%2520improving%2520the%2520model%2527s%2520interpretation%2520of%2520text-described%2520RS%2520scenes.%2520To%2520provide%2520precise%2520target%2520cues%2520for%2520SAM2%252C%2520we%2520design%2520a%2520mask%2520prompt%2520generator%252C%2520which%2520takes%2520the%2520visual%2520embeddings%2520and%2520class%2520tokens%2520as%2520input%2520and%2520produces%2520a%2520pseudo-mask%2520as%2520the%2520dense%2520prompt%2520of%2520SAM2.%2520Experimental%2520results%2520on%2520several%2520RRSIS%2520benchmarks%2520demonstrate%2520that%2520RS2-SAM2%2520achieves%2520state-of-the-art%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07266v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RS2-SAM2%3A%20Customized%20SAM2%20for%20Referring%20Remote%20Sensing%20Image%20Segmentation&entry.906535625=Fu%20Rong%20and%20Meng%20Lan%20and%20Qian%20Zhang%20and%20Lefei%20Zhang&entry.1292438233=Referring%20Remote%20Sensing%20Image%20Segmentation%20%28RRSIS%29%20aims%20to%20segment%20target%20objects%20in%20remote%20sensing%20%28RS%29%20images%20based%20on%20textual%20descriptions.%20Although%20Segment%20Anything%20Model%202%20%28SAM2%29%20has%20shown%20remarkable%20performance%20in%20various%20segmentation%20tasks%2C%20its%20application%20to%20RRSIS%20presents%20several%20challenges%2C%20including%20understanding%20the%20text-described%20RS%20scenes%20and%20generating%20effective%20prompts%20from%20text.%20To%20address%20these%20issues%2C%20we%20propose%20%5Ctextbf%7BRS2-SAM2%7D%2C%20a%20novel%20framework%20that%20adapts%20SAM2%20to%20RRSIS%20by%20aligning%20the%20adapted%20RS%20features%20and%20textual%20features%20while%20providing%20pseudo-mask-based%20dense%20prompts.%20Specifically%2C%20we%20employ%20a%20union%20encoder%20to%20jointly%20encode%20the%20visual%20and%20textual%20inputs%2C%20generating%20aligned%20visual%20and%20text%20embeddings%20as%20well%20as%20multimodal%20class%20tokens.%20A%20bidirectional%20hierarchical%20fusion%20module%20is%20introduced%20to%20adapt%20SAM2%20to%20RS%20scenes%20and%20align%20adapted%20visual%20features%20with%20the%20visually%20enhanced%20text%20embeddings%2C%20improving%20the%20model%27s%20interpretation%20of%20text-described%20RS%20scenes.%20To%20provide%20precise%20target%20cues%20for%20SAM2%2C%20we%20design%20a%20mask%20prompt%20generator%2C%20which%20takes%20the%20visual%20embeddings%20and%20class%20tokens%20as%20input%20and%20produces%20a%20pseudo-mask%20as%20the%20dense%20prompt%20of%20SAM2.%20Experimental%20results%20on%20several%20RRSIS%20benchmarks%20demonstrate%20that%20RS2-SAM2%20achieves%20state-of-the-art%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2503.07266v4&entry.124074799=Read"},
{"title": "From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA", "author": "Kimia Abedini and Farzad Shami and Gianmaria Silvello", "abstract": "Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.", "link": "http://arxiv.org/abs/2601.10581v1", "date": "2026-01-15", "relevancy": 2.5426, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5189}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5112}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4954}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Single%20to%20Multi-Agent%20Reasoning%3A%20Advancing%20GeneGPT%20for%20Genomics%20QA&body=Title%3A%20From%20Single%20to%20Multi-Agent%20Reasoning%3A%20Advancing%20GeneGPT%20for%20Genomics%20QA%0AAuthor%3A%20Kimia%20Abedini%20and%20Farzad%20Shami%20and%20Gianmaria%20Silvello%0AAbstract%3A%20Comprehending%20genomic%20information%20is%20essential%20for%20biomedical%20research%2C%20yet%20extracting%20data%20from%20complex%20distributed%20databases%20remains%20challenging.%20Large%20language%20models%20%28LLMs%29%20offer%20potential%20for%20genomic%20Question%20Answering%20%28QA%29%20but%20face%20limitations%20due%20to%20restricted%20access%20to%20domain-specific%20databases.%20GeneGPT%20is%20the%20current%20state-of-the-art%20system%20that%20enhances%20LLMs%20by%20utilizing%20specialized%20API%20calls%2C%20though%20it%20is%20constrained%20by%20rigid%20API%20dependencies%20and%20limited%20adaptability.%20We%20replicate%20GeneGPT%20and%20propose%20GenomAgent%2C%20a%20multi-agent%20framework%20that%20efficiently%20coordinates%20specialized%20agents%20for%20complex%20genomics%20queries.%20Evaluated%20on%20nine%20tasks%20from%20the%20GeneTuring%20benchmark%2C%20GenomAgent%20outperforms%20GeneGPT%20by%2012%25%20on%20average%2C%20and%20its%20flexible%20architecture%20extends%20beyond%20genomics%20to%20various%20scientific%20domains%20needing%20expert%20knowledge%20extraction.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10581v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Single%2520to%2520Multi-Agent%2520Reasoning%253A%2520Advancing%2520GeneGPT%2520for%2520Genomics%2520QA%26entry.906535625%3DKimia%2520Abedini%2520and%2520Farzad%2520Shami%2520and%2520Gianmaria%2520Silvello%26entry.1292438233%3DComprehending%2520genomic%2520information%2520is%2520essential%2520for%2520biomedical%2520research%252C%2520yet%2520extracting%2520data%2520from%2520complex%2520distributed%2520databases%2520remains%2520challenging.%2520Large%2520language%2520models%2520%2528LLMs%2529%2520offer%2520potential%2520for%2520genomic%2520Question%2520Answering%2520%2528QA%2529%2520but%2520face%2520limitations%2520due%2520to%2520restricted%2520access%2520to%2520domain-specific%2520databases.%2520GeneGPT%2520is%2520the%2520current%2520state-of-the-art%2520system%2520that%2520enhances%2520LLMs%2520by%2520utilizing%2520specialized%2520API%2520calls%252C%2520though%2520it%2520is%2520constrained%2520by%2520rigid%2520API%2520dependencies%2520and%2520limited%2520adaptability.%2520We%2520replicate%2520GeneGPT%2520and%2520propose%2520GenomAgent%252C%2520a%2520multi-agent%2520framework%2520that%2520efficiently%2520coordinates%2520specialized%2520agents%2520for%2520complex%2520genomics%2520queries.%2520Evaluated%2520on%2520nine%2520tasks%2520from%2520the%2520GeneTuring%2520benchmark%252C%2520GenomAgent%2520outperforms%2520GeneGPT%2520by%252012%2525%2520on%2520average%252C%2520and%2520its%2520flexible%2520architecture%2520extends%2520beyond%2520genomics%2520to%2520various%2520scientific%2520domains%2520needing%2520expert%2520knowledge%2520extraction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10581v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Single%20to%20Multi-Agent%20Reasoning%3A%20Advancing%20GeneGPT%20for%20Genomics%20QA&entry.906535625=Kimia%20Abedini%20and%20Farzad%20Shami%20and%20Gianmaria%20Silvello&entry.1292438233=Comprehending%20genomic%20information%20is%20essential%20for%20biomedical%20research%2C%20yet%20extracting%20data%20from%20complex%20distributed%20databases%20remains%20challenging.%20Large%20language%20models%20%28LLMs%29%20offer%20potential%20for%20genomic%20Question%20Answering%20%28QA%29%20but%20face%20limitations%20due%20to%20restricted%20access%20to%20domain-specific%20databases.%20GeneGPT%20is%20the%20current%20state-of-the-art%20system%20that%20enhances%20LLMs%20by%20utilizing%20specialized%20API%20calls%2C%20though%20it%20is%20constrained%20by%20rigid%20API%20dependencies%20and%20limited%20adaptability.%20We%20replicate%20GeneGPT%20and%20propose%20GenomAgent%2C%20a%20multi-agent%20framework%20that%20efficiently%20coordinates%20specialized%20agents%20for%20complex%20genomics%20queries.%20Evaluated%20on%20nine%20tasks%20from%20the%20GeneTuring%20benchmark%2C%20GenomAgent%20outperforms%20GeneGPT%20by%2012%25%20on%20average%2C%20and%20its%20flexible%20architecture%20extends%20beyond%20genomics%20to%20various%20scientific%20domains%20needing%20expert%20knowledge%20extraction.&entry.1838667208=http%3A//arxiv.org/abs/2601.10581v1&entry.124074799=Read"},
{"title": "Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis", "author": "Chun Hei Michael Shiu and Chih Wei Ling", "abstract": "Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work introduced a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), which achieves both objectives simultaneously. CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a randomized vector quantizer whose quantization error is equivalent to a prescribed noise, which can be tuned to customize privacy protection between parties. In this work, we theoretically analyze the privacy guarantees and convergence properties of CEPAM. Moreover, we assess CEPAM's utility performance through experimental evaluations, including convergence profiles compared with other baselines, and accuracy-privacy trade-offs between different parties.", "link": "http://arxiv.org/abs/2601.10701v1", "date": "2026-01-15", "relevancy": 2.5368, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5125}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5082}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Communication-Efficient%20and%20Privacy-Adaptable%20Mechanism%20--%20a%20Federated%20Learning%20Scheme%20with%20Convergence%20Analysis&body=Title%3A%20Communication-Efficient%20and%20Privacy-Adaptable%20Mechanism%20--%20a%20Federated%20Learning%20Scheme%20with%20Convergence%20Analysis%0AAuthor%3A%20Chun%20Hei%20Michael%20Shiu%20and%20Chih%20Wei%20Ling%0AAbstract%3A%20Federated%20learning%20enables%20multiple%20parties%20to%20jointly%20train%20learning%20models%20without%20sharing%20their%20own%20underlying%20data%2C%20offering%20a%20practical%20pathway%20to%20privacy-preserving%20collaboration%20under%20data-governance%20constraints.%20Continued%20study%20of%20federated%20learning%20is%20essential%20to%20address%20key%20challenges%20in%20it%2C%20including%20communication%20efficiency%20and%20privacy%20protection%20between%20parties.%20A%20recent%20line%20of%20work%20introduced%20a%20novel%20approach%20called%20the%20Communication-Efficient%20and%20Privacy-Adaptable%20Mechanism%20%28CEPAM%29%2C%20which%20achieves%20both%20objectives%20simultaneously.%20CEPAM%20leverages%20the%20rejection-sampled%20universal%20quantizer%20%28RSUQ%29%2C%20a%20randomized%20vector%20quantizer%20whose%20quantization%20error%20is%20equivalent%20to%20a%20prescribed%20noise%2C%20which%20can%20be%20tuned%20to%20customize%20privacy%20protection%20between%20parties.%20In%20this%20work%2C%20we%20theoretically%20analyze%20the%20privacy%20guarantees%20and%20convergence%20properties%20of%20CEPAM.%20Moreover%2C%20we%20assess%20CEPAM%27s%20utility%20performance%20through%20experimental%20evaluations%2C%20including%20convergence%20profiles%20compared%20with%20other%20baselines%2C%20and%20accuracy-privacy%20trade-offs%20between%20different%20parties.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10701v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCommunication-Efficient%2520and%2520Privacy-Adaptable%2520Mechanism%2520--%2520a%2520Federated%2520Learning%2520Scheme%2520with%2520Convergence%2520Analysis%26entry.906535625%3DChun%2520Hei%2520Michael%2520Shiu%2520and%2520Chih%2520Wei%2520Ling%26entry.1292438233%3DFederated%2520learning%2520enables%2520multiple%2520parties%2520to%2520jointly%2520train%2520learning%2520models%2520without%2520sharing%2520their%2520own%2520underlying%2520data%252C%2520offering%2520a%2520practical%2520pathway%2520to%2520privacy-preserving%2520collaboration%2520under%2520data-governance%2520constraints.%2520Continued%2520study%2520of%2520federated%2520learning%2520is%2520essential%2520to%2520address%2520key%2520challenges%2520in%2520it%252C%2520including%2520communication%2520efficiency%2520and%2520privacy%2520protection%2520between%2520parties.%2520A%2520recent%2520line%2520of%2520work%2520introduced%2520a%2520novel%2520approach%2520called%2520the%2520Communication-Efficient%2520and%2520Privacy-Adaptable%2520Mechanism%2520%2528CEPAM%2529%252C%2520which%2520achieves%2520both%2520objectives%2520simultaneously.%2520CEPAM%2520leverages%2520the%2520rejection-sampled%2520universal%2520quantizer%2520%2528RSUQ%2529%252C%2520a%2520randomized%2520vector%2520quantizer%2520whose%2520quantization%2520error%2520is%2520equivalent%2520to%2520a%2520prescribed%2520noise%252C%2520which%2520can%2520be%2520tuned%2520to%2520customize%2520privacy%2520protection%2520between%2520parties.%2520In%2520this%2520work%252C%2520we%2520theoretically%2520analyze%2520the%2520privacy%2520guarantees%2520and%2520convergence%2520properties%2520of%2520CEPAM.%2520Moreover%252C%2520we%2520assess%2520CEPAM%2527s%2520utility%2520performance%2520through%2520experimental%2520evaluations%252C%2520including%2520convergence%2520profiles%2520compared%2520with%2520other%2520baselines%252C%2520and%2520accuracy-privacy%2520trade-offs%2520between%2520different%2520parties.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10701v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Communication-Efficient%20and%20Privacy-Adaptable%20Mechanism%20--%20a%20Federated%20Learning%20Scheme%20with%20Convergence%20Analysis&entry.906535625=Chun%20Hei%20Michael%20Shiu%20and%20Chih%20Wei%20Ling&entry.1292438233=Federated%20learning%20enables%20multiple%20parties%20to%20jointly%20train%20learning%20models%20without%20sharing%20their%20own%20underlying%20data%2C%20offering%20a%20practical%20pathway%20to%20privacy-preserving%20collaboration%20under%20data-governance%20constraints.%20Continued%20study%20of%20federated%20learning%20is%20essential%20to%20address%20key%20challenges%20in%20it%2C%20including%20communication%20efficiency%20and%20privacy%20protection%20between%20parties.%20A%20recent%20line%20of%20work%20introduced%20a%20novel%20approach%20called%20the%20Communication-Efficient%20and%20Privacy-Adaptable%20Mechanism%20%28CEPAM%29%2C%20which%20achieves%20both%20objectives%20simultaneously.%20CEPAM%20leverages%20the%20rejection-sampled%20universal%20quantizer%20%28RSUQ%29%2C%20a%20randomized%20vector%20quantizer%20whose%20quantization%20error%20is%20equivalent%20to%20a%20prescribed%20noise%2C%20which%20can%20be%20tuned%20to%20customize%20privacy%20protection%20between%20parties.%20In%20this%20work%2C%20we%20theoretically%20analyze%20the%20privacy%20guarantees%20and%20convergence%20properties%20of%20CEPAM.%20Moreover%2C%20we%20assess%20CEPAM%27s%20utility%20performance%20through%20experimental%20evaluations%2C%20including%20convergence%20profiles%20compared%20with%20other%20baselines%2C%20and%20accuracy-privacy%20trade-offs%20between%20different%20parties.&entry.1838667208=http%3A//arxiv.org/abs/2601.10701v1&entry.124074799=Read"},
{"title": "STEP3-VL-10B Technical Report", "author": "Ailin Huang and Chengyuan Yao and Chunrui Han and Fanqi Wan and Hangyu Guo and Haoran Lv and Hongyu Zhou and Jia Wang and Jian Zhou and Jianjian Sun and Jingcheng Hu and Kangheng Lin and Liang Zhao and Mitt Huang and Song Yuan and Wenwen Qu and Xiangfeng Wang and Yanlin Lai and Yingxiu Zhao and Yinmin Zhang and Yukang Shi and Yuyang Chen and Zejia Weng and Ziyang Meng and Ang Li and Aobo Kong and Bo Dong and Changyi Wan and David Wang and Di Qi and Dingming Li and En Yu and Guopeng Li and Haiquan Yin and Han Zhou and Hanshan Zhang and Haolong Yan and Hebin Zhou and Hongbo Peng and Jiaran Zhang and Jiashu Lv and Jiayi Fu and Jie Cheng and Jie Zhou and Jisheng Yin and Jingjing Xie and Jingwei Wu and Jun Zhang and Junfeng Liu and Kaijun Tan and Kaiwen Yan and Liangyu Chen and Lina Chen and Mingliang Li and Qian Zhao and Quan Sun and Shaoliang Pang and Shengjie Fan and Shijie Shang and Siyuan Zhang and Tianhao You and Wei Ji and Wuxun Xie and Xiaobo Yang and Xiaojie Hou and Xiaoran Jiao and Xiaoxiao Ren and Xiangwen Kong and Xin Huang and Xin Wu and Xing Chen and Xinran Wang and Xuelin Zhang and Yana Wei and Yang Li and Yanming Xu and Yeqing Shen and Yuang Peng and Yue Peng and Yu Zhou and Yusheng Li and Yuxiang Yang and Yuyang Zhang and Zhe Xie and Zhewei Huang and Zhenyi Lu and Zhimin Fan and Zihui Cheng and Daxin Jiang and Qi Han and Xiangyu Zhang and Yibo Zhu and Zheng Ge", "abstract": "We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10$\\times$-20$\\times$ larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.", "link": "http://arxiv.org/abs/2601.09668v2", "date": "2026-01-15", "relevancy": 2.5277, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6446}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6446}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5684}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STEP3-VL-10B%20Technical%20Report&body=Title%3A%20STEP3-VL-10B%20Technical%20Report%0AAuthor%3A%20Ailin%20Huang%20and%20Chengyuan%20Yao%20and%20Chunrui%20Han%20and%20Fanqi%20Wan%20and%20Hangyu%20Guo%20and%20Haoran%20Lv%20and%20Hongyu%20Zhou%20and%20Jia%20Wang%20and%20Jian%20Zhou%20and%20Jianjian%20Sun%20and%20Jingcheng%20Hu%20and%20Kangheng%20Lin%20and%20Liang%20Zhao%20and%20Mitt%20Huang%20and%20Song%20Yuan%20and%20Wenwen%20Qu%20and%20Xiangfeng%20Wang%20and%20Yanlin%20Lai%20and%20Yingxiu%20Zhao%20and%20Yinmin%20Zhang%20and%20Yukang%20Shi%20and%20Yuyang%20Chen%20and%20Zejia%20Weng%20and%20Ziyang%20Meng%20and%20Ang%20Li%20and%20Aobo%20Kong%20and%20Bo%20Dong%20and%20Changyi%20Wan%20and%20David%20Wang%20and%20Di%20Qi%20and%20Dingming%20Li%20and%20En%20Yu%20and%20Guopeng%20Li%20and%20Haiquan%20Yin%20and%20Han%20Zhou%20and%20Hanshan%20Zhang%20and%20Haolong%20Yan%20and%20Hebin%20Zhou%20and%20Hongbo%20Peng%20and%20Jiaran%20Zhang%20and%20Jiashu%20Lv%20and%20Jiayi%20Fu%20and%20Jie%20Cheng%20and%20Jie%20Zhou%20and%20Jisheng%20Yin%20and%20Jingjing%20Xie%20and%20Jingwei%20Wu%20and%20Jun%20Zhang%20and%20Junfeng%20Liu%20and%20Kaijun%20Tan%20and%20Kaiwen%20Yan%20and%20Liangyu%20Chen%20and%20Lina%20Chen%20and%20Mingliang%20Li%20and%20Qian%20Zhao%20and%20Quan%20Sun%20and%20Shaoliang%20Pang%20and%20Shengjie%20Fan%20and%20Shijie%20Shang%20and%20Siyuan%20Zhang%20and%20Tianhao%20You%20and%20Wei%20Ji%20and%20Wuxun%20Xie%20and%20Xiaobo%20Yang%20and%20Xiaojie%20Hou%20and%20Xiaoran%20Jiao%20and%20Xiaoxiao%20Ren%20and%20Xiangwen%20Kong%20and%20Xin%20Huang%20and%20Xin%20Wu%20and%20Xing%20Chen%20and%20Xinran%20Wang%20and%20Xuelin%20Zhang%20and%20Yana%20Wei%20and%20Yang%20Li%20and%20Yanming%20Xu%20and%20Yeqing%20Shen%20and%20Yuang%20Peng%20and%20Yue%20Peng%20and%20Yu%20Zhou%20and%20Yusheng%20Li%20and%20Yuxiang%20Yang%20and%20Yuyang%20Zhang%20and%20Zhe%20Xie%20and%20Zhewei%20Huang%20and%20Zhenyi%20Lu%20and%20Zhimin%20Fan%20and%20Zihui%20Cheng%20and%20Daxin%20Jiang%20and%20Qi%20Han%20and%20Xiangyu%20Zhang%20and%20Yibo%20Zhu%20and%20Zheng%20Ge%0AAbstract%3A%20We%20present%20STEP3-VL-10B%2C%20a%20lightweight%20open-source%20foundation%20model%20designed%20to%20redefine%20the%20trade-off%20between%20compact%20efficiency%20and%20frontier-level%20multimodal%20intelligence.%20STEP3-VL-10B%20is%20realized%20through%20two%20strategic%20shifts%3A%20first%2C%20a%20unified%2C%20fully%20unfrozen%20pre-training%20strategy%20on%201.2T%20multimodal%20tokens%20that%20integrates%20a%20language-aligned%20Perception%20Encoder%20with%20a%20Qwen3-8B%20decoder%20to%20establish%20intrinsic%20vision-language%20synergy%3B%20and%20second%2C%20a%20scaled%20post-training%20pipeline%20featuring%20over%201k%20iterations%20of%20reinforcement%20learning.%20Crucially%2C%20we%20implement%20Parallel%20Coordinated%20Reasoning%20%28PaCoRe%29%20to%20scale%20test-time%20compute%2C%20allocating%20resources%20to%20scalable%20perceptual%20reasoning%20that%20explores%20and%20synthesizes%20diverse%20visual%20hypotheses.%20Consequently%2C%20despite%20its%20compact%2010B%20footprint%2C%20STEP3-VL-10B%20rivals%20or%20surpasses%20models%2010%24%5Ctimes%24-20%24%5Ctimes%24%20larger%20%28e.g.%2C%20GLM-4.6V-106B%2C%20Qwen3-VL-235B%29%20and%20top-tier%20proprietary%20flagships%20like%20Gemini%202.5%20Pro%20and%20Seed-1.5-VL.%20Delivering%20best-in-class%20performance%2C%20it%20records%2092.2%25%20on%20MMBench%20and%2080.11%25%20on%20MMMU%2C%20while%20excelling%20in%20complex%20reasoning%20with%2094.43%25%20on%20AIME2025%20and%2075.95%25%20on%20MathVision.%20We%20release%20the%20full%20model%20suite%20to%20provide%20the%20community%20with%20a%20powerful%2C%20efficient%2C%20and%20reproducible%20baseline.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09668v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTEP3-VL-10B%2520Technical%2520Report%26entry.906535625%3DAilin%2520Huang%2520and%2520Chengyuan%2520Yao%2520and%2520Chunrui%2520Han%2520and%2520Fanqi%2520Wan%2520and%2520Hangyu%2520Guo%2520and%2520Haoran%2520Lv%2520and%2520Hongyu%2520Zhou%2520and%2520Jia%2520Wang%2520and%2520Jian%2520Zhou%2520and%2520Jianjian%2520Sun%2520and%2520Jingcheng%2520Hu%2520and%2520Kangheng%2520Lin%2520and%2520Liang%2520Zhao%2520and%2520Mitt%2520Huang%2520and%2520Song%2520Yuan%2520and%2520Wenwen%2520Qu%2520and%2520Xiangfeng%2520Wang%2520and%2520Yanlin%2520Lai%2520and%2520Yingxiu%2520Zhao%2520and%2520Yinmin%2520Zhang%2520and%2520Yukang%2520Shi%2520and%2520Yuyang%2520Chen%2520and%2520Zejia%2520Weng%2520and%2520Ziyang%2520Meng%2520and%2520Ang%2520Li%2520and%2520Aobo%2520Kong%2520and%2520Bo%2520Dong%2520and%2520Changyi%2520Wan%2520and%2520David%2520Wang%2520and%2520Di%2520Qi%2520and%2520Dingming%2520Li%2520and%2520En%2520Yu%2520and%2520Guopeng%2520Li%2520and%2520Haiquan%2520Yin%2520and%2520Han%2520Zhou%2520and%2520Hanshan%2520Zhang%2520and%2520Haolong%2520Yan%2520and%2520Hebin%2520Zhou%2520and%2520Hongbo%2520Peng%2520and%2520Jiaran%2520Zhang%2520and%2520Jiashu%2520Lv%2520and%2520Jiayi%2520Fu%2520and%2520Jie%2520Cheng%2520and%2520Jie%2520Zhou%2520and%2520Jisheng%2520Yin%2520and%2520Jingjing%2520Xie%2520and%2520Jingwei%2520Wu%2520and%2520Jun%2520Zhang%2520and%2520Junfeng%2520Liu%2520and%2520Kaijun%2520Tan%2520and%2520Kaiwen%2520Yan%2520and%2520Liangyu%2520Chen%2520and%2520Lina%2520Chen%2520and%2520Mingliang%2520Li%2520and%2520Qian%2520Zhao%2520and%2520Quan%2520Sun%2520and%2520Shaoliang%2520Pang%2520and%2520Shengjie%2520Fan%2520and%2520Shijie%2520Shang%2520and%2520Siyuan%2520Zhang%2520and%2520Tianhao%2520You%2520and%2520Wei%2520Ji%2520and%2520Wuxun%2520Xie%2520and%2520Xiaobo%2520Yang%2520and%2520Xiaojie%2520Hou%2520and%2520Xiaoran%2520Jiao%2520and%2520Xiaoxiao%2520Ren%2520and%2520Xiangwen%2520Kong%2520and%2520Xin%2520Huang%2520and%2520Xin%2520Wu%2520and%2520Xing%2520Chen%2520and%2520Xinran%2520Wang%2520and%2520Xuelin%2520Zhang%2520and%2520Yana%2520Wei%2520and%2520Yang%2520Li%2520and%2520Yanming%2520Xu%2520and%2520Yeqing%2520Shen%2520and%2520Yuang%2520Peng%2520and%2520Yue%2520Peng%2520and%2520Yu%2520Zhou%2520and%2520Yusheng%2520Li%2520and%2520Yuxiang%2520Yang%2520and%2520Yuyang%2520Zhang%2520and%2520Zhe%2520Xie%2520and%2520Zhewei%2520Huang%2520and%2520Zhenyi%2520Lu%2520and%2520Zhimin%2520Fan%2520and%2520Zihui%2520Cheng%2520and%2520Daxin%2520Jiang%2520and%2520Qi%2520Han%2520and%2520Xiangyu%2520Zhang%2520and%2520Yibo%2520Zhu%2520and%2520Zheng%2520Ge%26entry.1292438233%3DWe%2520present%2520STEP3-VL-10B%252C%2520a%2520lightweight%2520open-source%2520foundation%2520model%2520designed%2520to%2520redefine%2520the%2520trade-off%2520between%2520compact%2520efficiency%2520and%2520frontier-level%2520multimodal%2520intelligence.%2520STEP3-VL-10B%2520is%2520realized%2520through%2520two%2520strategic%2520shifts%253A%2520first%252C%2520a%2520unified%252C%2520fully%2520unfrozen%2520pre-training%2520strategy%2520on%25201.2T%2520multimodal%2520tokens%2520that%2520integrates%2520a%2520language-aligned%2520Perception%2520Encoder%2520with%2520a%2520Qwen3-8B%2520decoder%2520to%2520establish%2520intrinsic%2520vision-language%2520synergy%253B%2520and%2520second%252C%2520a%2520scaled%2520post-training%2520pipeline%2520featuring%2520over%25201k%2520iterations%2520of%2520reinforcement%2520learning.%2520Crucially%252C%2520we%2520implement%2520Parallel%2520Coordinated%2520Reasoning%2520%2528PaCoRe%2529%2520to%2520scale%2520test-time%2520compute%252C%2520allocating%2520resources%2520to%2520scalable%2520perceptual%2520reasoning%2520that%2520explores%2520and%2520synthesizes%2520diverse%2520visual%2520hypotheses.%2520Consequently%252C%2520despite%2520its%2520compact%252010B%2520footprint%252C%2520STEP3-VL-10B%2520rivals%2520or%2520surpasses%2520models%252010%2524%255Ctimes%2524-20%2524%255Ctimes%2524%2520larger%2520%2528e.g.%252C%2520GLM-4.6V-106B%252C%2520Qwen3-VL-235B%2529%2520and%2520top-tier%2520proprietary%2520flagships%2520like%2520Gemini%25202.5%2520Pro%2520and%2520Seed-1.5-VL.%2520Delivering%2520best-in-class%2520performance%252C%2520it%2520records%252092.2%2525%2520on%2520MMBench%2520and%252080.11%2525%2520on%2520MMMU%252C%2520while%2520excelling%2520in%2520complex%2520reasoning%2520with%252094.43%2525%2520on%2520AIME2025%2520and%252075.95%2525%2520on%2520MathVision.%2520We%2520release%2520the%2520full%2520model%2520suite%2520to%2520provide%2520the%2520community%2520with%2520a%2520powerful%252C%2520efficient%252C%2520and%2520reproducible%2520baseline.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09668v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STEP3-VL-10B%20Technical%20Report&entry.906535625=Ailin%20Huang%20and%20Chengyuan%20Yao%20and%20Chunrui%20Han%20and%20Fanqi%20Wan%20and%20Hangyu%20Guo%20and%20Haoran%20Lv%20and%20Hongyu%20Zhou%20and%20Jia%20Wang%20and%20Jian%20Zhou%20and%20Jianjian%20Sun%20and%20Jingcheng%20Hu%20and%20Kangheng%20Lin%20and%20Liang%20Zhao%20and%20Mitt%20Huang%20and%20Song%20Yuan%20and%20Wenwen%20Qu%20and%20Xiangfeng%20Wang%20and%20Yanlin%20Lai%20and%20Yingxiu%20Zhao%20and%20Yinmin%20Zhang%20and%20Yukang%20Shi%20and%20Yuyang%20Chen%20and%20Zejia%20Weng%20and%20Ziyang%20Meng%20and%20Ang%20Li%20and%20Aobo%20Kong%20and%20Bo%20Dong%20and%20Changyi%20Wan%20and%20David%20Wang%20and%20Di%20Qi%20and%20Dingming%20Li%20and%20En%20Yu%20and%20Guopeng%20Li%20and%20Haiquan%20Yin%20and%20Han%20Zhou%20and%20Hanshan%20Zhang%20and%20Haolong%20Yan%20and%20Hebin%20Zhou%20and%20Hongbo%20Peng%20and%20Jiaran%20Zhang%20and%20Jiashu%20Lv%20and%20Jiayi%20Fu%20and%20Jie%20Cheng%20and%20Jie%20Zhou%20and%20Jisheng%20Yin%20and%20Jingjing%20Xie%20and%20Jingwei%20Wu%20and%20Jun%20Zhang%20and%20Junfeng%20Liu%20and%20Kaijun%20Tan%20and%20Kaiwen%20Yan%20and%20Liangyu%20Chen%20and%20Lina%20Chen%20and%20Mingliang%20Li%20and%20Qian%20Zhao%20and%20Quan%20Sun%20and%20Shaoliang%20Pang%20and%20Shengjie%20Fan%20and%20Shijie%20Shang%20and%20Siyuan%20Zhang%20and%20Tianhao%20You%20and%20Wei%20Ji%20and%20Wuxun%20Xie%20and%20Xiaobo%20Yang%20and%20Xiaojie%20Hou%20and%20Xiaoran%20Jiao%20and%20Xiaoxiao%20Ren%20and%20Xiangwen%20Kong%20and%20Xin%20Huang%20and%20Xin%20Wu%20and%20Xing%20Chen%20and%20Xinran%20Wang%20and%20Xuelin%20Zhang%20and%20Yana%20Wei%20and%20Yang%20Li%20and%20Yanming%20Xu%20and%20Yeqing%20Shen%20and%20Yuang%20Peng%20and%20Yue%20Peng%20and%20Yu%20Zhou%20and%20Yusheng%20Li%20and%20Yuxiang%20Yang%20and%20Yuyang%20Zhang%20and%20Zhe%20Xie%20and%20Zhewei%20Huang%20and%20Zhenyi%20Lu%20and%20Zhimin%20Fan%20and%20Zihui%20Cheng%20and%20Daxin%20Jiang%20and%20Qi%20Han%20and%20Xiangyu%20Zhang%20and%20Yibo%20Zhu%20and%20Zheng%20Ge&entry.1292438233=We%20present%20STEP3-VL-10B%2C%20a%20lightweight%20open-source%20foundation%20model%20designed%20to%20redefine%20the%20trade-off%20between%20compact%20efficiency%20and%20frontier-level%20multimodal%20intelligence.%20STEP3-VL-10B%20is%20realized%20through%20two%20strategic%20shifts%3A%20first%2C%20a%20unified%2C%20fully%20unfrozen%20pre-training%20strategy%20on%201.2T%20multimodal%20tokens%20that%20integrates%20a%20language-aligned%20Perception%20Encoder%20with%20a%20Qwen3-8B%20decoder%20to%20establish%20intrinsic%20vision-language%20synergy%3B%20and%20second%2C%20a%20scaled%20post-training%20pipeline%20featuring%20over%201k%20iterations%20of%20reinforcement%20learning.%20Crucially%2C%20we%20implement%20Parallel%20Coordinated%20Reasoning%20%28PaCoRe%29%20to%20scale%20test-time%20compute%2C%20allocating%20resources%20to%20scalable%20perceptual%20reasoning%20that%20explores%20and%20synthesizes%20diverse%20visual%20hypotheses.%20Consequently%2C%20despite%20its%20compact%2010B%20footprint%2C%20STEP3-VL-10B%20rivals%20or%20surpasses%20models%2010%24%5Ctimes%24-20%24%5Ctimes%24%20larger%20%28e.g.%2C%20GLM-4.6V-106B%2C%20Qwen3-VL-235B%29%20and%20top-tier%20proprietary%20flagships%20like%20Gemini%202.5%20Pro%20and%20Seed-1.5-VL.%20Delivering%20best-in-class%20performance%2C%20it%20records%2092.2%25%20on%20MMBench%20and%2080.11%25%20on%20MMMU%2C%20while%20excelling%20in%20complex%20reasoning%20with%2094.43%25%20on%20AIME2025%20and%2075.95%25%20on%20MathVision.%20We%20release%20the%20full%20model%20suite%20to%20provide%20the%20community%20with%20a%20powerful%2C%20efficient%2C%20and%20reproducible%20baseline.&entry.1838667208=http%3A//arxiv.org/abs/2601.09668v2&entry.124074799=Read"},
{"title": "Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection", "author": "Frank Bobe and Gregory D. Vetaw and Chase Pavlick and Darshan Bryner and Matthew Cook and Jose Salas-Vernis", "abstract": "The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover the root causes of their generalization failures. Our investigation reveals three critical findings: (1) Generalization is driven by a powerful synergy between architecture and data diversity. The Gemma 2 9B model achieves state-of-the-art performance (>91\\% F1), but only when trained on a stylistically diverse ``generalist'' dataset. (2) Generalization is highly architecture-dependent. We diagnose a specific failure mode in Llama 3.1 8B, which performs well on a narrow domain but cannot integrate diverse data, leading to a significant performance drop. (3) Some architectures are inherently more generalizable. The Mistral model proves to be a consistent and resilient performer across multiple training paradigms. By pinpointing the flawed heuristics responsible for these failures, our work provides a concrete methodology for diagnosing and understanding generalization failures, underscoring that reliable AI requires deep validation of the interplay between architecture, data, and training strategy.", "link": "http://arxiv.org/abs/2601.10524v1", "date": "2026-01-15", "relevancy": 2.5137, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5192}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4945}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4945}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Diagnosing%20Generalization%20Failures%20in%20Fine-Tuned%20LLMs%3A%20A%20Cross-Architectural%20Study%20on%20Phishing%20Detection&body=Title%3A%20Diagnosing%20Generalization%20Failures%20in%20Fine-Tuned%20LLMs%3A%20A%20Cross-Architectural%20Study%20on%20Phishing%20Detection%0AAuthor%3A%20Frank%20Bobe%20and%20Gregory%20D.%20Vetaw%20and%20Chase%20Pavlick%20and%20Darshan%20Bryner%20and%20Matthew%20Cook%20and%20Jose%20Salas-Vernis%0AAbstract%3A%20The%20practice%20of%20fine-tuning%20Large%20Language%20Models%20%28LLMs%29%20has%20achieved%20state-of-the-art%20performance%20on%20specialized%20tasks%2C%20yet%20diagnosing%20why%20these%20models%20become%20brittle%20and%20fail%20to%20generalize%20remains%20a%20critical%20open%20problem.%20To%20address%20this%2C%20we%20introduce%20and%20apply%20a%20multi-layered%20diagnostic%20framework%20to%20a%20cross-architectural%20study.%20We%20fine-tune%20Llama%203.1%208B%2C%20Gemma%202%209B%2C%20and%20Mistral%20models%20on%20a%20high-stakes%20phishing%20detection%20task%20and%20use%20SHAP%20analysis%20and%20mechanistic%20interpretability%20to%20uncover%20the%20root%20causes%20of%20their%20generalization%20failures.%20Our%20investigation%20reveals%20three%20critical%20findings%3A%20%281%29%20Generalization%20is%20driven%20by%20a%20powerful%20synergy%20between%20architecture%20and%20data%20diversity.%20The%20Gemma%202%209B%20model%20achieves%20state-of-the-art%20performance%20%28%3E91%5C%25%20F1%29%2C%20but%20only%20when%20trained%20on%20a%20stylistically%20diverse%20%60%60generalist%27%27%20dataset.%20%282%29%20Generalization%20is%20highly%20architecture-dependent.%20We%20diagnose%20a%20specific%20failure%20mode%20in%20Llama%203.1%208B%2C%20which%20performs%20well%20on%20a%20narrow%20domain%20but%20cannot%20integrate%20diverse%20data%2C%20leading%20to%20a%20significant%20performance%20drop.%20%283%29%20Some%20architectures%20are%20inherently%20more%20generalizable.%20The%20Mistral%20model%20proves%20to%20be%20a%20consistent%20and%20resilient%20performer%20across%20multiple%20training%20paradigms.%20By%20pinpointing%20the%20flawed%20heuristics%20responsible%20for%20these%20failures%2C%20our%20work%20provides%20a%20concrete%20methodology%20for%20diagnosing%20and%20understanding%20generalization%20failures%2C%20underscoring%20that%20reliable%20AI%20requires%20deep%20validation%20of%20the%20interplay%20between%20architecture%2C%20data%2C%20and%20training%20strategy.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10524v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiagnosing%2520Generalization%2520Failures%2520in%2520Fine-Tuned%2520LLMs%253A%2520A%2520Cross-Architectural%2520Study%2520on%2520Phishing%2520Detection%26entry.906535625%3DFrank%2520Bobe%2520and%2520Gregory%2520D.%2520Vetaw%2520and%2520Chase%2520Pavlick%2520and%2520Darshan%2520Bryner%2520and%2520Matthew%2520Cook%2520and%2520Jose%2520Salas-Vernis%26entry.1292438233%3DThe%2520practice%2520of%2520fine-tuning%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520has%2520achieved%2520state-of-the-art%2520performance%2520on%2520specialized%2520tasks%252C%2520yet%2520diagnosing%2520why%2520these%2520models%2520become%2520brittle%2520and%2520fail%2520to%2520generalize%2520remains%2520a%2520critical%2520open%2520problem.%2520To%2520address%2520this%252C%2520we%2520introduce%2520and%2520apply%2520a%2520multi-layered%2520diagnostic%2520framework%2520to%2520a%2520cross-architectural%2520study.%2520We%2520fine-tune%2520Llama%25203.1%25208B%252C%2520Gemma%25202%25209B%252C%2520and%2520Mistral%2520models%2520on%2520a%2520high-stakes%2520phishing%2520detection%2520task%2520and%2520use%2520SHAP%2520analysis%2520and%2520mechanistic%2520interpretability%2520to%2520uncover%2520the%2520root%2520causes%2520of%2520their%2520generalization%2520failures.%2520Our%2520investigation%2520reveals%2520three%2520critical%2520findings%253A%2520%25281%2529%2520Generalization%2520is%2520driven%2520by%2520a%2520powerful%2520synergy%2520between%2520architecture%2520and%2520data%2520diversity.%2520The%2520Gemma%25202%25209B%2520model%2520achieves%2520state-of-the-art%2520performance%2520%2528%253E91%255C%2525%2520F1%2529%252C%2520but%2520only%2520when%2520trained%2520on%2520a%2520stylistically%2520diverse%2520%2560%2560generalist%2527%2527%2520dataset.%2520%25282%2529%2520Generalization%2520is%2520highly%2520architecture-dependent.%2520We%2520diagnose%2520a%2520specific%2520failure%2520mode%2520in%2520Llama%25203.1%25208B%252C%2520which%2520performs%2520well%2520on%2520a%2520narrow%2520domain%2520but%2520cannot%2520integrate%2520diverse%2520data%252C%2520leading%2520to%2520a%2520significant%2520performance%2520drop.%2520%25283%2529%2520Some%2520architectures%2520are%2520inherently%2520more%2520generalizable.%2520The%2520Mistral%2520model%2520proves%2520to%2520be%2520a%2520consistent%2520and%2520resilient%2520performer%2520across%2520multiple%2520training%2520paradigms.%2520By%2520pinpointing%2520the%2520flawed%2520heuristics%2520responsible%2520for%2520these%2520failures%252C%2520our%2520work%2520provides%2520a%2520concrete%2520methodology%2520for%2520diagnosing%2520and%2520understanding%2520generalization%2520failures%252C%2520underscoring%2520that%2520reliable%2520AI%2520requires%2520deep%2520validation%2520of%2520the%2520interplay%2520between%2520architecture%252C%2520data%252C%2520and%2520training%2520strategy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10524v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diagnosing%20Generalization%20Failures%20in%20Fine-Tuned%20LLMs%3A%20A%20Cross-Architectural%20Study%20on%20Phishing%20Detection&entry.906535625=Frank%20Bobe%20and%20Gregory%20D.%20Vetaw%20and%20Chase%20Pavlick%20and%20Darshan%20Bryner%20and%20Matthew%20Cook%20and%20Jose%20Salas-Vernis&entry.1292438233=The%20practice%20of%20fine-tuning%20Large%20Language%20Models%20%28LLMs%29%20has%20achieved%20state-of-the-art%20performance%20on%20specialized%20tasks%2C%20yet%20diagnosing%20why%20these%20models%20become%20brittle%20and%20fail%20to%20generalize%20remains%20a%20critical%20open%20problem.%20To%20address%20this%2C%20we%20introduce%20and%20apply%20a%20multi-layered%20diagnostic%20framework%20to%20a%20cross-architectural%20study.%20We%20fine-tune%20Llama%203.1%208B%2C%20Gemma%202%209B%2C%20and%20Mistral%20models%20on%20a%20high-stakes%20phishing%20detection%20task%20and%20use%20SHAP%20analysis%20and%20mechanistic%20interpretability%20to%20uncover%20the%20root%20causes%20of%20their%20generalization%20failures.%20Our%20investigation%20reveals%20three%20critical%20findings%3A%20%281%29%20Generalization%20is%20driven%20by%20a%20powerful%20synergy%20between%20architecture%20and%20data%20diversity.%20The%20Gemma%202%209B%20model%20achieves%20state-of-the-art%20performance%20%28%3E91%5C%25%20F1%29%2C%20but%20only%20when%20trained%20on%20a%20stylistically%20diverse%20%60%60generalist%27%27%20dataset.%20%282%29%20Generalization%20is%20highly%20architecture-dependent.%20We%20diagnose%20a%20specific%20failure%20mode%20in%20Llama%203.1%208B%2C%20which%20performs%20well%20on%20a%20narrow%20domain%20but%20cannot%20integrate%20diverse%20data%2C%20leading%20to%20a%20significant%20performance%20drop.%20%283%29%20Some%20architectures%20are%20inherently%20more%20generalizable.%20The%20Mistral%20model%20proves%20to%20be%20a%20consistent%20and%20resilient%20performer%20across%20multiple%20training%20paradigms.%20By%20pinpointing%20the%20flawed%20heuristics%20responsible%20for%20these%20failures%2C%20our%20work%20provides%20a%20concrete%20methodology%20for%20diagnosing%20and%20understanding%20generalization%20failures%2C%20underscoring%20that%20reliable%20AI%20requires%20deep%20validation%20of%20the%20interplay%20between%20architecture%2C%20data%2C%20and%20training%20strategy.&entry.1838667208=http%3A//arxiv.org/abs/2601.10524v1&entry.124074799=Read"},
{"title": "SurgGoal: Rethinking Surgical Planning Evaluation via Goal-Satisfiability", "author": "Ruochen Li and Kun Yuan and Yufei Xia and Yue Zhou and Qingyu Lu and Weihang Li and Youxiang Zhu and Nassir Navab", "abstract": "Surgical planning integrates visual perception, long-horizon reasoning, and procedural knowledge, yet it remains unclear whether current evaluation protocols reliably assess vision-language models (VLMs) in safety-critical settings. Motivated by a goal-oriented view of surgical planning, we define planning correctness via phase-goal satisfiability, where plan validity is determined by expert-defined surgical rules. Based on this definition, we introduce a multicentric meta-evaluation benchmark with valid procedural variations and invalid plans containing order and content errors. Using this benchmark, we show that sequence similarity metrics systematically misjudge planning quality, penalizing valid plans while failing to identify invalid ones. We therefore adopt a rule-based goal-satisfiability metric as a high-precision meta-evaluation reference to assess Video-LLMs under progressively constrained settings, revealing failures due to perception errors and under-constrained reasoning. Structural knowledge consistently improves performance, whereas semantic guidance alone is unreliable and benefits larger models only when combined with structural constraints.", "link": "http://arxiv.org/abs/2601.10455v1", "date": "2026-01-15", "relevancy": 2.5096, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5063}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5063}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4931}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SurgGoal%3A%20Rethinking%20Surgical%20Planning%20Evaluation%20via%20Goal-Satisfiability&body=Title%3A%20SurgGoal%3A%20Rethinking%20Surgical%20Planning%20Evaluation%20via%20Goal-Satisfiability%0AAuthor%3A%20Ruochen%20Li%20and%20Kun%20Yuan%20and%20Yufei%20Xia%20and%20Yue%20Zhou%20and%20Qingyu%20Lu%20and%20Weihang%20Li%20and%20Youxiang%20Zhu%20and%20Nassir%20Navab%0AAbstract%3A%20Surgical%20planning%20integrates%20visual%20perception%2C%20long-horizon%20reasoning%2C%20and%20procedural%20knowledge%2C%20yet%20it%20remains%20unclear%20whether%20current%20evaluation%20protocols%20reliably%20assess%20vision-language%20models%20%28VLMs%29%20in%20safety-critical%20settings.%20Motivated%20by%20a%20goal-oriented%20view%20of%20surgical%20planning%2C%20we%20define%20planning%20correctness%20via%20phase-goal%20satisfiability%2C%20where%20plan%20validity%20is%20determined%20by%20expert-defined%20surgical%20rules.%20Based%20on%20this%20definition%2C%20we%20introduce%20a%20multicentric%20meta-evaluation%20benchmark%20with%20valid%20procedural%20variations%20and%20invalid%20plans%20containing%20order%20and%20content%20errors.%20Using%20this%20benchmark%2C%20we%20show%20that%20sequence%20similarity%20metrics%20systematically%20misjudge%20planning%20quality%2C%20penalizing%20valid%20plans%20while%20failing%20to%20identify%20invalid%20ones.%20We%20therefore%20adopt%20a%20rule-based%20goal-satisfiability%20metric%20as%20a%20high-precision%20meta-evaluation%20reference%20to%20assess%20Video-LLMs%20under%20progressively%20constrained%20settings%2C%20revealing%20failures%20due%20to%20perception%20errors%20and%20under-constrained%20reasoning.%20Structural%20knowledge%20consistently%20improves%20performance%2C%20whereas%20semantic%20guidance%20alone%20is%20unreliable%20and%20benefits%20larger%20models%20only%20when%20combined%20with%20structural%20constraints.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSurgGoal%253A%2520Rethinking%2520Surgical%2520Planning%2520Evaluation%2520via%2520Goal-Satisfiability%26entry.906535625%3DRuochen%2520Li%2520and%2520Kun%2520Yuan%2520and%2520Yufei%2520Xia%2520and%2520Yue%2520Zhou%2520and%2520Qingyu%2520Lu%2520and%2520Weihang%2520Li%2520and%2520Youxiang%2520Zhu%2520and%2520Nassir%2520Navab%26entry.1292438233%3DSurgical%2520planning%2520integrates%2520visual%2520perception%252C%2520long-horizon%2520reasoning%252C%2520and%2520procedural%2520knowledge%252C%2520yet%2520it%2520remains%2520unclear%2520whether%2520current%2520evaluation%2520protocols%2520reliably%2520assess%2520vision-language%2520models%2520%2528VLMs%2529%2520in%2520safety-critical%2520settings.%2520Motivated%2520by%2520a%2520goal-oriented%2520view%2520of%2520surgical%2520planning%252C%2520we%2520define%2520planning%2520correctness%2520via%2520phase-goal%2520satisfiability%252C%2520where%2520plan%2520validity%2520is%2520determined%2520by%2520expert-defined%2520surgical%2520rules.%2520Based%2520on%2520this%2520definition%252C%2520we%2520introduce%2520a%2520multicentric%2520meta-evaluation%2520benchmark%2520with%2520valid%2520procedural%2520variations%2520and%2520invalid%2520plans%2520containing%2520order%2520and%2520content%2520errors.%2520Using%2520this%2520benchmark%252C%2520we%2520show%2520that%2520sequence%2520similarity%2520metrics%2520systematically%2520misjudge%2520planning%2520quality%252C%2520penalizing%2520valid%2520plans%2520while%2520failing%2520to%2520identify%2520invalid%2520ones.%2520We%2520therefore%2520adopt%2520a%2520rule-based%2520goal-satisfiability%2520metric%2520as%2520a%2520high-precision%2520meta-evaluation%2520reference%2520to%2520assess%2520Video-LLMs%2520under%2520progressively%2520constrained%2520settings%252C%2520revealing%2520failures%2520due%2520to%2520perception%2520errors%2520and%2520under-constrained%2520reasoning.%2520Structural%2520knowledge%2520consistently%2520improves%2520performance%252C%2520whereas%2520semantic%2520guidance%2520alone%2520is%2520unreliable%2520and%2520benefits%2520larger%2520models%2520only%2520when%2520combined%2520with%2520structural%2520constraints.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SurgGoal%3A%20Rethinking%20Surgical%20Planning%20Evaluation%20via%20Goal-Satisfiability&entry.906535625=Ruochen%20Li%20and%20Kun%20Yuan%20and%20Yufei%20Xia%20and%20Yue%20Zhou%20and%20Qingyu%20Lu%20and%20Weihang%20Li%20and%20Youxiang%20Zhu%20and%20Nassir%20Navab&entry.1292438233=Surgical%20planning%20integrates%20visual%20perception%2C%20long-horizon%20reasoning%2C%20and%20procedural%20knowledge%2C%20yet%20it%20remains%20unclear%20whether%20current%20evaluation%20protocols%20reliably%20assess%20vision-language%20models%20%28VLMs%29%20in%20safety-critical%20settings.%20Motivated%20by%20a%20goal-oriented%20view%20of%20surgical%20planning%2C%20we%20define%20planning%20correctness%20via%20phase-goal%20satisfiability%2C%20where%20plan%20validity%20is%20determined%20by%20expert-defined%20surgical%20rules.%20Based%20on%20this%20definition%2C%20we%20introduce%20a%20multicentric%20meta-evaluation%20benchmark%20with%20valid%20procedural%20variations%20and%20invalid%20plans%20containing%20order%20and%20content%20errors.%20Using%20this%20benchmark%2C%20we%20show%20that%20sequence%20similarity%20metrics%20systematically%20misjudge%20planning%20quality%2C%20penalizing%20valid%20plans%20while%20failing%20to%20identify%20invalid%20ones.%20We%20therefore%20adopt%20a%20rule-based%20goal-satisfiability%20metric%20as%20a%20high-precision%20meta-evaluation%20reference%20to%20assess%20Video-LLMs%20under%20progressively%20constrained%20settings%2C%20revealing%20failures%20due%20to%20perception%20errors%20and%20under-constrained%20reasoning.%20Structural%20knowledge%20consistently%20improves%20performance%2C%20whereas%20semantic%20guidance%20alone%20is%20unreliable%20and%20benefits%20larger%20models%20only%20when%20combined%20with%20structural%20constraints.&entry.1838667208=http%3A//arxiv.org/abs/2601.10455v1&entry.124074799=Read"},
{"title": "SRAW-Attack: Space-Reweighted Adversarial Warping Attack for SAR Target Recognition", "author": "Yiming Zhang and Weibo Qin and Yuntian Liu and Feng Wang", "abstract": "Synthetic aperture radar (SAR) imagery exhibits intrinsic information sparsity due to its unique electromagnetic scattering mechanism. Despite the widespread adoption of deep neural network (DNN)-based SAR automatic target recognition (SAR-ATR) systems, they remain vulnerable to adversarial examples and tend to over-rely on background regions, leading to degraded adversarial robustness. Existing adversarial attacks for SAR-ATR often require visually perceptible distortions to achieve effective performance, thereby necessitating an attack method that balances effectiveness and stealthiness. In this paper, a novel attack method termed Space-Reweighted Adversarial Warping (SRAW) is proposed, which generates adversarial examples through optimized spatial deformation with reweighted budgets across foreground and background regions. Extensive experiments demonstrate that SRAW significantly degrades the performance of state-of-the-art SAR-ATR models and consistently outperforms existing methods in terms of imperceptibility and adversarial transferability. Code is made available at https://github.com/boremycin/SAR-ATR-TransAttack.", "link": "http://arxiv.org/abs/2601.10324v1", "date": "2026-01-15", "relevancy": 2.4846, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5388}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5006}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SRAW-Attack%3A%20Space-Reweighted%20Adversarial%20Warping%20Attack%20for%20SAR%20Target%20Recognition&body=Title%3A%20SRAW-Attack%3A%20Space-Reweighted%20Adversarial%20Warping%20Attack%20for%20SAR%20Target%20Recognition%0AAuthor%3A%20Yiming%20Zhang%20and%20Weibo%20Qin%20and%20Yuntian%20Liu%20and%20Feng%20Wang%0AAbstract%3A%20Synthetic%20aperture%20radar%20%28SAR%29%20imagery%20exhibits%20intrinsic%20information%20sparsity%20due%20to%20its%20unique%20electromagnetic%20scattering%20mechanism.%20Despite%20the%20widespread%20adoption%20of%20deep%20neural%20network%20%28DNN%29-based%20SAR%20automatic%20target%20recognition%20%28SAR-ATR%29%20systems%2C%20they%20remain%20vulnerable%20to%20adversarial%20examples%20and%20tend%20to%20over-rely%20on%20background%20regions%2C%20leading%20to%20degraded%20adversarial%20robustness.%20Existing%20adversarial%20attacks%20for%20SAR-ATR%20often%20require%20visually%20perceptible%20distortions%20to%20achieve%20effective%20performance%2C%20thereby%20necessitating%20an%20attack%20method%20that%20balances%20effectiveness%20and%20stealthiness.%20In%20this%20paper%2C%20a%20novel%20attack%20method%20termed%20Space-Reweighted%20Adversarial%20Warping%20%28SRAW%29%20is%20proposed%2C%20which%20generates%20adversarial%20examples%20through%20optimized%20spatial%20deformation%20with%20reweighted%20budgets%20across%20foreground%20and%20background%20regions.%20Extensive%20experiments%20demonstrate%20that%20SRAW%20significantly%20degrades%20the%20performance%20of%20state-of-the-art%20SAR-ATR%20models%20and%20consistently%20outperforms%20existing%20methods%20in%20terms%20of%20imperceptibility%20and%20adversarial%20transferability.%20Code%20is%20made%20available%20at%20https%3A//github.com/boremycin/SAR-ATR-TransAttack.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10324v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSRAW-Attack%253A%2520Space-Reweighted%2520Adversarial%2520Warping%2520Attack%2520for%2520SAR%2520Target%2520Recognition%26entry.906535625%3DYiming%2520Zhang%2520and%2520Weibo%2520Qin%2520and%2520Yuntian%2520Liu%2520and%2520Feng%2520Wang%26entry.1292438233%3DSynthetic%2520aperture%2520radar%2520%2528SAR%2529%2520imagery%2520exhibits%2520intrinsic%2520information%2520sparsity%2520due%2520to%2520its%2520unique%2520electromagnetic%2520scattering%2520mechanism.%2520Despite%2520the%2520widespread%2520adoption%2520of%2520deep%2520neural%2520network%2520%2528DNN%2529-based%2520SAR%2520automatic%2520target%2520recognition%2520%2528SAR-ATR%2529%2520systems%252C%2520they%2520remain%2520vulnerable%2520to%2520adversarial%2520examples%2520and%2520tend%2520to%2520over-rely%2520on%2520background%2520regions%252C%2520leading%2520to%2520degraded%2520adversarial%2520robustness.%2520Existing%2520adversarial%2520attacks%2520for%2520SAR-ATR%2520often%2520require%2520visually%2520perceptible%2520distortions%2520to%2520achieve%2520effective%2520performance%252C%2520thereby%2520necessitating%2520an%2520attack%2520method%2520that%2520balances%2520effectiveness%2520and%2520stealthiness.%2520In%2520this%2520paper%252C%2520a%2520novel%2520attack%2520method%2520termed%2520Space-Reweighted%2520Adversarial%2520Warping%2520%2528SRAW%2529%2520is%2520proposed%252C%2520which%2520generates%2520adversarial%2520examples%2520through%2520optimized%2520spatial%2520deformation%2520with%2520reweighted%2520budgets%2520across%2520foreground%2520and%2520background%2520regions.%2520Extensive%2520experiments%2520demonstrate%2520that%2520SRAW%2520significantly%2520degrades%2520the%2520performance%2520of%2520state-of-the-art%2520SAR-ATR%2520models%2520and%2520consistently%2520outperforms%2520existing%2520methods%2520in%2520terms%2520of%2520imperceptibility%2520and%2520adversarial%2520transferability.%2520Code%2520is%2520made%2520available%2520at%2520https%253A//github.com/boremycin/SAR-ATR-TransAttack.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10324v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SRAW-Attack%3A%20Space-Reweighted%20Adversarial%20Warping%20Attack%20for%20SAR%20Target%20Recognition&entry.906535625=Yiming%20Zhang%20and%20Weibo%20Qin%20and%20Yuntian%20Liu%20and%20Feng%20Wang&entry.1292438233=Synthetic%20aperture%20radar%20%28SAR%29%20imagery%20exhibits%20intrinsic%20information%20sparsity%20due%20to%20its%20unique%20electromagnetic%20scattering%20mechanism.%20Despite%20the%20widespread%20adoption%20of%20deep%20neural%20network%20%28DNN%29-based%20SAR%20automatic%20target%20recognition%20%28SAR-ATR%29%20systems%2C%20they%20remain%20vulnerable%20to%20adversarial%20examples%20and%20tend%20to%20over-rely%20on%20background%20regions%2C%20leading%20to%20degraded%20adversarial%20robustness.%20Existing%20adversarial%20attacks%20for%20SAR-ATR%20often%20require%20visually%20perceptible%20distortions%20to%20achieve%20effective%20performance%2C%20thereby%20necessitating%20an%20attack%20method%20that%20balances%20effectiveness%20and%20stealthiness.%20In%20this%20paper%2C%20a%20novel%20attack%20method%20termed%20Space-Reweighted%20Adversarial%20Warping%20%28SRAW%29%20is%20proposed%2C%20which%20generates%20adversarial%20examples%20through%20optimized%20spatial%20deformation%20with%20reweighted%20budgets%20across%20foreground%20and%20background%20regions.%20Extensive%20experiments%20demonstrate%20that%20SRAW%20significantly%20degrades%20the%20performance%20of%20state-of-the-art%20SAR-ATR%20models%20and%20consistently%20outperforms%20existing%20methods%20in%20terms%20of%20imperceptibility%20and%20adversarial%20transferability.%20Code%20is%20made%20available%20at%20https%3A//github.com/boremycin/SAR-ATR-TransAttack.&entry.1838667208=http%3A//arxiv.org/abs/2601.10324v1&entry.124074799=Read"},
{"title": "Searching for Quantum Effects in the Brain: A Bell-Type Test for Nonclassical Latent Representations in Autoencoders", "author": "I. K. Kominis and C. Xie and S. Li and M. Skotiniotis and G. P. Tsironis", "abstract": "Whether neural information processing is entirely classical or involves quantum-mechanical elements remains an open question. Here we propose a model-agnostic, information-theoretic test of nonclassicality that bypasses microscopic assumptions and instead probes the structure of neural representations themselves. Using autoencoders as a transparent model system, we introduce a Bell-type consistency test in latent space, and ask whether decoding statistics obtained under multiple readout contexts can be jointly explained by a single positive latent-variable distribution. By shifting the search for quantum-like signatures in neural systems from microscopic dynamics to experimentally testable constraints on information processing, this work opens a new route for probing the fundamental physics of neural computation.", "link": "http://arxiv.org/abs/2601.10588v1", "date": "2026-01-15", "relevancy": 2.4713, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4951}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4938}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Searching%20for%20Quantum%20Effects%20in%20the%20Brain%3A%20A%20Bell-Type%20Test%20for%20Nonclassical%20Latent%20Representations%20in%20Autoencoders&body=Title%3A%20Searching%20for%20Quantum%20Effects%20in%20the%20Brain%3A%20A%20Bell-Type%20Test%20for%20Nonclassical%20Latent%20Representations%20in%20Autoencoders%0AAuthor%3A%20I.%20K.%20Kominis%20and%20C.%20Xie%20and%20S.%20Li%20and%20M.%20Skotiniotis%20and%20G.%20P.%20Tsironis%0AAbstract%3A%20Whether%20neural%20information%20processing%20is%20entirely%20classical%20or%20involves%20quantum-mechanical%20elements%20remains%20an%20open%20question.%20Here%20we%20propose%20a%20model-agnostic%2C%20information-theoretic%20test%20of%20nonclassicality%20that%20bypasses%20microscopic%20assumptions%20and%20instead%20probes%20the%20structure%20of%20neural%20representations%20themselves.%20Using%20autoencoders%20as%20a%20transparent%20model%20system%2C%20we%20introduce%20a%20Bell-type%20consistency%20test%20in%20latent%20space%2C%20and%20ask%20whether%20decoding%20statistics%20obtained%20under%20multiple%20readout%20contexts%20can%20be%20jointly%20explained%20by%20a%20single%20positive%20latent-variable%20distribution.%20By%20shifting%20the%20search%20for%20quantum-like%20signatures%20in%20neural%20systems%20from%20microscopic%20dynamics%20to%20experimentally%20testable%20constraints%20on%20information%20processing%2C%20this%20work%20opens%20a%20new%20route%20for%20probing%20the%20fundamental%20physics%20of%20neural%20computation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10588v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSearching%2520for%2520Quantum%2520Effects%2520in%2520the%2520Brain%253A%2520A%2520Bell-Type%2520Test%2520for%2520Nonclassical%2520Latent%2520Representations%2520in%2520Autoencoders%26entry.906535625%3DI.%2520K.%2520Kominis%2520and%2520C.%2520Xie%2520and%2520S.%2520Li%2520and%2520M.%2520Skotiniotis%2520and%2520G.%2520P.%2520Tsironis%26entry.1292438233%3DWhether%2520neural%2520information%2520processing%2520is%2520entirely%2520classical%2520or%2520involves%2520quantum-mechanical%2520elements%2520remains%2520an%2520open%2520question.%2520Here%2520we%2520propose%2520a%2520model-agnostic%252C%2520information-theoretic%2520test%2520of%2520nonclassicality%2520that%2520bypasses%2520microscopic%2520assumptions%2520and%2520instead%2520probes%2520the%2520structure%2520of%2520neural%2520representations%2520themselves.%2520Using%2520autoencoders%2520as%2520a%2520transparent%2520model%2520system%252C%2520we%2520introduce%2520a%2520Bell-type%2520consistency%2520test%2520in%2520latent%2520space%252C%2520and%2520ask%2520whether%2520decoding%2520statistics%2520obtained%2520under%2520multiple%2520readout%2520contexts%2520can%2520be%2520jointly%2520explained%2520by%2520a%2520single%2520positive%2520latent-variable%2520distribution.%2520By%2520shifting%2520the%2520search%2520for%2520quantum-like%2520signatures%2520in%2520neural%2520systems%2520from%2520microscopic%2520dynamics%2520to%2520experimentally%2520testable%2520constraints%2520on%2520information%2520processing%252C%2520this%2520work%2520opens%2520a%2520new%2520route%2520for%2520probing%2520the%2520fundamental%2520physics%2520of%2520neural%2520computation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10588v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Searching%20for%20Quantum%20Effects%20in%20the%20Brain%3A%20A%20Bell-Type%20Test%20for%20Nonclassical%20Latent%20Representations%20in%20Autoencoders&entry.906535625=I.%20K.%20Kominis%20and%20C.%20Xie%20and%20S.%20Li%20and%20M.%20Skotiniotis%20and%20G.%20P.%20Tsironis&entry.1292438233=Whether%20neural%20information%20processing%20is%20entirely%20classical%20or%20involves%20quantum-mechanical%20elements%20remains%20an%20open%20question.%20Here%20we%20propose%20a%20model-agnostic%2C%20information-theoretic%20test%20of%20nonclassicality%20that%20bypasses%20microscopic%20assumptions%20and%20instead%20probes%20the%20structure%20of%20neural%20representations%20themselves.%20Using%20autoencoders%20as%20a%20transparent%20model%20system%2C%20we%20introduce%20a%20Bell-type%20consistency%20test%20in%20latent%20space%2C%20and%20ask%20whether%20decoding%20statistics%20obtained%20under%20multiple%20readout%20contexts%20can%20be%20jointly%20explained%20by%20a%20single%20positive%20latent-variable%20distribution.%20By%20shifting%20the%20search%20for%20quantum-like%20signatures%20in%20neural%20systems%20from%20microscopic%20dynamics%20to%20experimentally%20testable%20constraints%20on%20information%20processing%2C%20this%20work%20opens%20a%20new%20route%20for%20probing%20the%20fundamental%20physics%20of%20neural%20computation.&entry.1838667208=http%3A//arxiv.org/abs/2601.10588v1&entry.124074799=Read"},
{"title": "Inference-time Physics Alignment of Video Generative Models with Latent World Models", "author": "Jianhao Yuan and Xiaofeng Zhang and Felix Friedrich and Nicolas Beltran-Velez and Melissa Hall and Reyhane Askari-Hemmat and Xiaochuang Han and Nicolas Ballas and Michal Drozdzal and Adriana Romero-Soriano", "abstract": "State-of-the-art video generative models produce promising visual content yet often violate basic physics principles, limiting their utility. While some attribute this deficiency to insufficient physics understanding from pre-training, we find that the shortfall in physics plausibility also stems from suboptimal inference strategies. We therefore introduce WMReward and treat improving physics plausibility of video generation as an inference-time alignment problem. In particular, we leverage the strong physics prior of a latent world model (here, VJEPA-2) as a reward to search and steer multiple candidate denoising trajectories, enabling scaling test-time compute for better generation performance. Empirically, our approach substantially improves physics plausibility across image-conditioned, multiframe-conditioned, and text-conditioned generation settings, with validation from human preference study. Notably, in the ICCV 2025 Perception Test PhysicsIQ Challenge, we achieve a final score of 62.64%, winning first place and outperforming the previous state of the art by 7.42%. Our work demonstrates the viability of using latent world models to improve physics plausibility of video generation, beyond this specific instantiation or parameterization.", "link": "http://arxiv.org/abs/2601.10553v1", "date": "2026-01-15", "relevancy": 2.4498, "topK": [{"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.667}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5753}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.569}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Inference-time%20Physics%20Alignment%20of%20Video%20Generative%20Models%20with%20Latent%20World%20Models&body=Title%3A%20Inference-time%20Physics%20Alignment%20of%20Video%20Generative%20Models%20with%20Latent%20World%20Models%0AAuthor%3A%20Jianhao%20Yuan%20and%20Xiaofeng%20Zhang%20and%20Felix%20Friedrich%20and%20Nicolas%20Beltran-Velez%20and%20Melissa%20Hall%20and%20Reyhane%20Askari-Hemmat%20and%20Xiaochuang%20Han%20and%20Nicolas%20Ballas%20and%20Michal%20Drozdzal%20and%20Adriana%20Romero-Soriano%0AAbstract%3A%20State-of-the-art%20video%20generative%20models%20produce%20promising%20visual%20content%20yet%20often%20violate%20basic%20physics%20principles%2C%20limiting%20their%20utility.%20While%20some%20attribute%20this%20deficiency%20to%20insufficient%20physics%20understanding%20from%20pre-training%2C%20we%20find%20that%20the%20shortfall%20in%20physics%20plausibility%20also%20stems%20from%20suboptimal%20inference%20strategies.%20We%20therefore%20introduce%20WMReward%20and%20treat%20improving%20physics%20plausibility%20of%20video%20generation%20as%20an%20inference-time%20alignment%20problem.%20In%20particular%2C%20we%20leverage%20the%20strong%20physics%20prior%20of%20a%20latent%20world%20model%20%28here%2C%20VJEPA-2%29%20as%20a%20reward%20to%20search%20and%20steer%20multiple%20candidate%20denoising%20trajectories%2C%20enabling%20scaling%20test-time%20compute%20for%20better%20generation%20performance.%20Empirically%2C%20our%20approach%20substantially%20improves%20physics%20plausibility%20across%20image-conditioned%2C%20multiframe-conditioned%2C%20and%20text-conditioned%20generation%20settings%2C%20with%20validation%20from%20human%20preference%20study.%20Notably%2C%20in%20the%20ICCV%202025%20Perception%20Test%20PhysicsIQ%20Challenge%2C%20we%20achieve%20a%20final%20score%20of%2062.64%25%2C%20winning%20first%20place%20and%20outperforming%20the%20previous%20state%20of%20the%20art%20by%207.42%25.%20Our%20work%20demonstrates%20the%20viability%20of%20using%20latent%20world%20models%20to%20improve%20physics%20plausibility%20of%20video%20generation%2C%20beyond%20this%20specific%20instantiation%20or%20parameterization.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10553v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInference-time%2520Physics%2520Alignment%2520of%2520Video%2520Generative%2520Models%2520with%2520Latent%2520World%2520Models%26entry.906535625%3DJianhao%2520Yuan%2520and%2520Xiaofeng%2520Zhang%2520and%2520Felix%2520Friedrich%2520and%2520Nicolas%2520Beltran-Velez%2520and%2520Melissa%2520Hall%2520and%2520Reyhane%2520Askari-Hemmat%2520and%2520Xiaochuang%2520Han%2520and%2520Nicolas%2520Ballas%2520and%2520Michal%2520Drozdzal%2520and%2520Adriana%2520Romero-Soriano%26entry.1292438233%3DState-of-the-art%2520video%2520generative%2520models%2520produce%2520promising%2520visual%2520content%2520yet%2520often%2520violate%2520basic%2520physics%2520principles%252C%2520limiting%2520their%2520utility.%2520While%2520some%2520attribute%2520this%2520deficiency%2520to%2520insufficient%2520physics%2520understanding%2520from%2520pre-training%252C%2520we%2520find%2520that%2520the%2520shortfall%2520in%2520physics%2520plausibility%2520also%2520stems%2520from%2520suboptimal%2520inference%2520strategies.%2520We%2520therefore%2520introduce%2520WMReward%2520and%2520treat%2520improving%2520physics%2520plausibility%2520of%2520video%2520generation%2520as%2520an%2520inference-time%2520alignment%2520problem.%2520In%2520particular%252C%2520we%2520leverage%2520the%2520strong%2520physics%2520prior%2520of%2520a%2520latent%2520world%2520model%2520%2528here%252C%2520VJEPA-2%2529%2520as%2520a%2520reward%2520to%2520search%2520and%2520steer%2520multiple%2520candidate%2520denoising%2520trajectories%252C%2520enabling%2520scaling%2520test-time%2520compute%2520for%2520better%2520generation%2520performance.%2520Empirically%252C%2520our%2520approach%2520substantially%2520improves%2520physics%2520plausibility%2520across%2520image-conditioned%252C%2520multiframe-conditioned%252C%2520and%2520text-conditioned%2520generation%2520settings%252C%2520with%2520validation%2520from%2520human%2520preference%2520study.%2520Notably%252C%2520in%2520the%2520ICCV%25202025%2520Perception%2520Test%2520PhysicsIQ%2520Challenge%252C%2520we%2520achieve%2520a%2520final%2520score%2520of%252062.64%2525%252C%2520winning%2520first%2520place%2520and%2520outperforming%2520the%2520previous%2520state%2520of%2520the%2520art%2520by%25207.42%2525.%2520Our%2520work%2520demonstrates%2520the%2520viability%2520of%2520using%2520latent%2520world%2520models%2520to%2520improve%2520physics%2520plausibility%2520of%2520video%2520generation%252C%2520beyond%2520this%2520specific%2520instantiation%2520or%2520parameterization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10553v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Inference-time%20Physics%20Alignment%20of%20Video%20Generative%20Models%20with%20Latent%20World%20Models&entry.906535625=Jianhao%20Yuan%20and%20Xiaofeng%20Zhang%20and%20Felix%20Friedrich%20and%20Nicolas%20Beltran-Velez%20and%20Melissa%20Hall%20and%20Reyhane%20Askari-Hemmat%20and%20Xiaochuang%20Han%20and%20Nicolas%20Ballas%20and%20Michal%20Drozdzal%20and%20Adriana%20Romero-Soriano&entry.1292438233=State-of-the-art%20video%20generative%20models%20produce%20promising%20visual%20content%20yet%20often%20violate%20basic%20physics%20principles%2C%20limiting%20their%20utility.%20While%20some%20attribute%20this%20deficiency%20to%20insufficient%20physics%20understanding%20from%20pre-training%2C%20we%20find%20that%20the%20shortfall%20in%20physics%20plausibility%20also%20stems%20from%20suboptimal%20inference%20strategies.%20We%20therefore%20introduce%20WMReward%20and%20treat%20improving%20physics%20plausibility%20of%20video%20generation%20as%20an%20inference-time%20alignment%20problem.%20In%20particular%2C%20we%20leverage%20the%20strong%20physics%20prior%20of%20a%20latent%20world%20model%20%28here%2C%20VJEPA-2%29%20as%20a%20reward%20to%20search%20and%20steer%20multiple%20candidate%20denoising%20trajectories%2C%20enabling%20scaling%20test-time%20compute%20for%20better%20generation%20performance.%20Empirically%2C%20our%20approach%20substantially%20improves%20physics%20plausibility%20across%20image-conditioned%2C%20multiframe-conditioned%2C%20and%20text-conditioned%20generation%20settings%2C%20with%20validation%20from%20human%20preference%20study.%20Notably%2C%20in%20the%20ICCV%202025%20Perception%20Test%20PhysicsIQ%20Challenge%2C%20we%20achieve%20a%20final%20score%20of%2062.64%25%2C%20winning%20first%20place%20and%20outperforming%20the%20previous%20state%20of%20the%20art%20by%207.42%25.%20Our%20work%20demonstrates%20the%20viability%20of%20using%20latent%20world%20models%20to%20improve%20physics%20plausibility%20of%20video%20generation%2C%20beyond%20this%20specific%20instantiation%20or%20parameterization.&entry.1838667208=http%3A//arxiv.org/abs/2601.10553v1&entry.124074799=Read"},
{"title": "Subjective evaluation of UHD video coded using VVC with LCEVC and ML-VVC", "author": "Naeem Ramzan and Muhammad Tufail Khan", "abstract": "This paper presents the results of a subjective quality assessment of a multilayer video coding configuration in which Low Complexity Enhancement Video Coding (LCEVC) is applied as an enhancement layer on top of a Versatile Video Coding (VVC) base layer. The evaluation follows the same test methodology and conditions previously defined for MPEG multilayer video coding assessments, with the LCEVC enhancement layer encoded using version 8.1 of the LCEVC Test Model (LTM). The test compares reconstructed UHD output generated from an HD VVC base layer with LCEVC enhancement against two reference cases: upsampled VVC base layer decoding and multilayer VVC (ML-VVC). Two operating points are considered, corresponding to enhancement layers representing approximately 10% and 50% of the total bitrate. Subjective assessment was conducted using the Degradation Category Rating (DCR) methodology with twenty five participants, across a dataset comprising fifteen SDR and HDR sequences. The reported results include Mean Opinion Scores (MOS) with associated 95% confidence intervals, enabling comparison of perceptual quality across coding approaches and operating points within the defined test scope.", "link": "http://arxiv.org/abs/2601.10448v1", "date": "2026-01-15", "relevancy": 2.4393, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4929}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4778}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Subjective%20evaluation%20of%20UHD%20video%20coded%20using%20VVC%20with%20LCEVC%20and%20ML-VVC&body=Title%3A%20Subjective%20evaluation%20of%20UHD%20video%20coded%20using%20VVC%20with%20LCEVC%20and%20ML-VVC%0AAuthor%3A%20Naeem%20Ramzan%20and%20Muhammad%20Tufail%20Khan%0AAbstract%3A%20This%20paper%20presents%20the%20results%20of%20a%20subjective%20quality%20assessment%20of%20a%20multilayer%20video%20coding%20configuration%20in%20which%20Low%20Complexity%20Enhancement%20Video%20Coding%20%28LCEVC%29%20is%20applied%20as%20an%20enhancement%20layer%20on%20top%20of%20a%20Versatile%20Video%20Coding%20%28VVC%29%20base%20layer.%20The%20evaluation%20follows%20the%20same%20test%20methodology%20and%20conditions%20previously%20defined%20for%20MPEG%20multilayer%20video%20coding%20assessments%2C%20with%20the%20LCEVC%20enhancement%20layer%20encoded%20using%20version%208.1%20of%20the%20LCEVC%20Test%20Model%20%28LTM%29.%20The%20test%20compares%20reconstructed%20UHD%20output%20generated%20from%20an%20HD%20VVC%20base%20layer%20with%20LCEVC%20enhancement%20against%20two%20reference%20cases%3A%20upsampled%20VVC%20base%20layer%20decoding%20and%20multilayer%20VVC%20%28ML-VVC%29.%20Two%20operating%20points%20are%20considered%2C%20corresponding%20to%20enhancement%20layers%20representing%20approximately%2010%25%20and%2050%25%20of%20the%20total%20bitrate.%20Subjective%20assessment%20was%20conducted%20using%20the%20Degradation%20Category%20Rating%20%28DCR%29%20methodology%20with%20twenty%20five%20participants%2C%20across%20a%20dataset%20comprising%20fifteen%20SDR%20and%20HDR%20sequences.%20The%20reported%20results%20include%20Mean%20Opinion%20Scores%20%28MOS%29%20with%20associated%2095%25%20confidence%20intervals%2C%20enabling%20comparison%20of%20perceptual%20quality%20across%20coding%20approaches%20and%20operating%20points%20within%20the%20defined%20test%20scope.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10448v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSubjective%2520evaluation%2520of%2520UHD%2520video%2520coded%2520using%2520VVC%2520with%2520LCEVC%2520and%2520ML-VVC%26entry.906535625%3DNaeem%2520Ramzan%2520and%2520Muhammad%2520Tufail%2520Khan%26entry.1292438233%3DThis%2520paper%2520presents%2520the%2520results%2520of%2520a%2520subjective%2520quality%2520assessment%2520of%2520a%2520multilayer%2520video%2520coding%2520configuration%2520in%2520which%2520Low%2520Complexity%2520Enhancement%2520Video%2520Coding%2520%2528LCEVC%2529%2520is%2520applied%2520as%2520an%2520enhancement%2520layer%2520on%2520top%2520of%2520a%2520Versatile%2520Video%2520Coding%2520%2528VVC%2529%2520base%2520layer.%2520The%2520evaluation%2520follows%2520the%2520same%2520test%2520methodology%2520and%2520conditions%2520previously%2520defined%2520for%2520MPEG%2520multilayer%2520video%2520coding%2520assessments%252C%2520with%2520the%2520LCEVC%2520enhancement%2520layer%2520encoded%2520using%2520version%25208.1%2520of%2520the%2520LCEVC%2520Test%2520Model%2520%2528LTM%2529.%2520The%2520test%2520compares%2520reconstructed%2520UHD%2520output%2520generated%2520from%2520an%2520HD%2520VVC%2520base%2520layer%2520with%2520LCEVC%2520enhancement%2520against%2520two%2520reference%2520cases%253A%2520upsampled%2520VVC%2520base%2520layer%2520decoding%2520and%2520multilayer%2520VVC%2520%2528ML-VVC%2529.%2520Two%2520operating%2520points%2520are%2520considered%252C%2520corresponding%2520to%2520enhancement%2520layers%2520representing%2520approximately%252010%2525%2520and%252050%2525%2520of%2520the%2520total%2520bitrate.%2520Subjective%2520assessment%2520was%2520conducted%2520using%2520the%2520Degradation%2520Category%2520Rating%2520%2528DCR%2529%2520methodology%2520with%2520twenty%2520five%2520participants%252C%2520across%2520a%2520dataset%2520comprising%2520fifteen%2520SDR%2520and%2520HDR%2520sequences.%2520The%2520reported%2520results%2520include%2520Mean%2520Opinion%2520Scores%2520%2528MOS%2529%2520with%2520associated%252095%2525%2520confidence%2520intervals%252C%2520enabling%2520comparison%2520of%2520perceptual%2520quality%2520across%2520coding%2520approaches%2520and%2520operating%2520points%2520within%2520the%2520defined%2520test%2520scope.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10448v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Subjective%20evaluation%20of%20UHD%20video%20coded%20using%20VVC%20with%20LCEVC%20and%20ML-VVC&entry.906535625=Naeem%20Ramzan%20and%20Muhammad%20Tufail%20Khan&entry.1292438233=This%20paper%20presents%20the%20results%20of%20a%20subjective%20quality%20assessment%20of%20a%20multilayer%20video%20coding%20configuration%20in%20which%20Low%20Complexity%20Enhancement%20Video%20Coding%20%28LCEVC%29%20is%20applied%20as%20an%20enhancement%20layer%20on%20top%20of%20a%20Versatile%20Video%20Coding%20%28VVC%29%20base%20layer.%20The%20evaluation%20follows%20the%20same%20test%20methodology%20and%20conditions%20previously%20defined%20for%20MPEG%20multilayer%20video%20coding%20assessments%2C%20with%20the%20LCEVC%20enhancement%20layer%20encoded%20using%20version%208.1%20of%20the%20LCEVC%20Test%20Model%20%28LTM%29.%20The%20test%20compares%20reconstructed%20UHD%20output%20generated%20from%20an%20HD%20VVC%20base%20layer%20with%20LCEVC%20enhancement%20against%20two%20reference%20cases%3A%20upsampled%20VVC%20base%20layer%20decoding%20and%20multilayer%20VVC%20%28ML-VVC%29.%20Two%20operating%20points%20are%20considered%2C%20corresponding%20to%20enhancement%20layers%20representing%20approximately%2010%25%20and%2050%25%20of%20the%20total%20bitrate.%20Subjective%20assessment%20was%20conducted%20using%20the%20Degradation%20Category%20Rating%20%28DCR%29%20methodology%20with%20twenty%20five%20participants%2C%20across%20a%20dataset%20comprising%20fifteen%20SDR%20and%20HDR%20sequences.%20The%20reported%20results%20include%20Mean%20Opinion%20Scores%20%28MOS%29%20with%20associated%2095%25%20confidence%20intervals%2C%20enabling%20comparison%20of%20perceptual%20quality%20across%20coding%20approaches%20and%20operating%20points%20within%20the%20defined%20test%20scope.&entry.1838667208=http%3A//arxiv.org/abs/2601.10448v1&entry.124074799=Read"},
{"title": "LangLasso: Interactive Cluster Descriptions through LLM Explanation", "author": "Raphael Buchm\u00fcller and Dennis Collaris and Linhao Meng and Angelos Chatzimparmpas", "abstract": "Dimensionality reduction is a powerful technique for revealing structure and potential clusters in data. However, as the axes are complex, non-linear combinations of features, they often lack semantic interpretability. Existing visual analytics (VA) methods support cluster interpretation through feature comparison and interactive exploration, but they require technical expertise and intense human effort. We present \\textit{LangLasso}, a novel method that complements VA approaches through interactive, natural language descriptions of clusters using large language models (LLMs). It produces human-readable descriptions that make cluster interpretation accessible to non-experts and allow integration of external contextual knowledge beyond the dataset. We systematically evaluate the reliability of these explanations and demonstrate that \\langlasso provides an effective first step for engaging broader audiences in cluster interpretation. The tool is available at https://langlasso.vercel.app", "link": "http://arxiv.org/abs/2601.10458v1", "date": "2026-01-15", "relevancy": 2.4357, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.492}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4847}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4847}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LangLasso%3A%20Interactive%20Cluster%20Descriptions%20through%20LLM%20Explanation&body=Title%3A%20LangLasso%3A%20Interactive%20Cluster%20Descriptions%20through%20LLM%20Explanation%0AAuthor%3A%20Raphael%20Buchm%C3%BCller%20and%20Dennis%20Collaris%20and%20Linhao%20Meng%20and%20Angelos%20Chatzimparmpas%0AAbstract%3A%20Dimensionality%20reduction%20is%20a%20powerful%20technique%20for%20revealing%20structure%20and%20potential%20clusters%20in%20data.%20However%2C%20as%20the%20axes%20are%20complex%2C%20non-linear%20combinations%20of%20features%2C%20they%20often%20lack%20semantic%20interpretability.%20Existing%20visual%20analytics%20%28VA%29%20methods%20support%20cluster%20interpretation%20through%20feature%20comparison%20and%20interactive%20exploration%2C%20but%20they%20require%20technical%20expertise%20and%20intense%20human%20effort.%20We%20present%20%5Ctextit%7BLangLasso%7D%2C%20a%20novel%20method%20that%20complements%20VA%20approaches%20through%20interactive%2C%20natural%20language%20descriptions%20of%20clusters%20using%20large%20language%20models%20%28LLMs%29.%20It%20produces%20human-readable%20descriptions%20that%20make%20cluster%20interpretation%20accessible%20to%20non-experts%20and%20allow%20integration%20of%20external%20contextual%20knowledge%20beyond%20the%20dataset.%20We%20systematically%20evaluate%20the%20reliability%20of%20these%20explanations%20and%20demonstrate%20that%20%5Clanglasso%20provides%20an%20effective%20first%20step%20for%20engaging%20broader%20audiences%20in%20cluster%20interpretation.%20The%20tool%20is%20available%20at%20https%3A//langlasso.vercel.app%0ALink%3A%20http%3A//arxiv.org/abs/2601.10458v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLangLasso%253A%2520Interactive%2520Cluster%2520Descriptions%2520through%2520LLM%2520Explanation%26entry.906535625%3DRaphael%2520Buchm%25C3%25BCller%2520and%2520Dennis%2520Collaris%2520and%2520Linhao%2520Meng%2520and%2520Angelos%2520Chatzimparmpas%26entry.1292438233%3DDimensionality%2520reduction%2520is%2520a%2520powerful%2520technique%2520for%2520revealing%2520structure%2520and%2520potential%2520clusters%2520in%2520data.%2520However%252C%2520as%2520the%2520axes%2520are%2520complex%252C%2520non-linear%2520combinations%2520of%2520features%252C%2520they%2520often%2520lack%2520semantic%2520interpretability.%2520Existing%2520visual%2520analytics%2520%2528VA%2529%2520methods%2520support%2520cluster%2520interpretation%2520through%2520feature%2520comparison%2520and%2520interactive%2520exploration%252C%2520but%2520they%2520require%2520technical%2520expertise%2520and%2520intense%2520human%2520effort.%2520We%2520present%2520%255Ctextit%257BLangLasso%257D%252C%2520a%2520novel%2520method%2520that%2520complements%2520VA%2520approaches%2520through%2520interactive%252C%2520natural%2520language%2520descriptions%2520of%2520clusters%2520using%2520large%2520language%2520models%2520%2528LLMs%2529.%2520It%2520produces%2520human-readable%2520descriptions%2520that%2520make%2520cluster%2520interpretation%2520accessible%2520to%2520non-experts%2520and%2520allow%2520integration%2520of%2520external%2520contextual%2520knowledge%2520beyond%2520the%2520dataset.%2520We%2520systematically%2520evaluate%2520the%2520reliability%2520of%2520these%2520explanations%2520and%2520demonstrate%2520that%2520%255Clanglasso%2520provides%2520an%2520effective%2520first%2520step%2520for%2520engaging%2520broader%2520audiences%2520in%2520cluster%2520interpretation.%2520The%2520tool%2520is%2520available%2520at%2520https%253A//langlasso.vercel.app%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10458v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LangLasso%3A%20Interactive%20Cluster%20Descriptions%20through%20LLM%20Explanation&entry.906535625=Raphael%20Buchm%C3%BCller%20and%20Dennis%20Collaris%20and%20Linhao%20Meng%20and%20Angelos%20Chatzimparmpas&entry.1292438233=Dimensionality%20reduction%20is%20a%20powerful%20technique%20for%20revealing%20structure%20and%20potential%20clusters%20in%20data.%20However%2C%20as%20the%20axes%20are%20complex%2C%20non-linear%20combinations%20of%20features%2C%20they%20often%20lack%20semantic%20interpretability.%20Existing%20visual%20analytics%20%28VA%29%20methods%20support%20cluster%20interpretation%20through%20feature%20comparison%20and%20interactive%20exploration%2C%20but%20they%20require%20technical%20expertise%20and%20intense%20human%20effort.%20We%20present%20%5Ctextit%7BLangLasso%7D%2C%20a%20novel%20method%20that%20complements%20VA%20approaches%20through%20interactive%2C%20natural%20language%20descriptions%20of%20clusters%20using%20large%20language%20models%20%28LLMs%29.%20It%20produces%20human-readable%20descriptions%20that%20make%20cluster%20interpretation%20accessible%20to%20non-experts%20and%20allow%20integration%20of%20external%20contextual%20knowledge%20beyond%20the%20dataset.%20We%20systematically%20evaluate%20the%20reliability%20of%20these%20explanations%20and%20demonstrate%20that%20%5Clanglasso%20provides%20an%20effective%20first%20step%20for%20engaging%20broader%20audiences%20in%20cluster%20interpretation.%20The%20tool%20is%20available%20at%20https%3A//langlasso.vercel.app&entry.1838667208=http%3A//arxiv.org/abs/2601.10458v1&entry.124074799=Read"},
{"title": "Towards Efficient Low-rate Image Compression with Frequency-aware Diffusion Prior Refinement", "author": "Yichong Xia and Yimin Zhou and Jinpeng Wang and Bin Chen", "abstract": "Recent advancements in diffusion-based generative priors have enabled visually plausible image compression at extremely low bit rates. However, existing approaches suffer from slow sampling processes and suboptimal bit allocation due to fragmented training paradigms. In this work, we propose Accelerate \\textbf{Diff}usion-based Image Compression via \\textbf{C}onsistency Prior \\textbf{R}efinement (DiffCR), a novel compression framework for efficient and high-fidelity image reconstruction. At the heart of DiffCR is a Frequency-aware Skip Estimation (FaSE) module that refines the $\u03b5$-prediction prior from a pre-trained latent diffusion model and aligns it with compressed latents at different timesteps via Frequency Decoupling Attention (FDA). Furthermore, a lightweight consistency estimator enables fast \\textbf{two-step decoding} by preserving the semantic trajectory of diffusion sampling. Without updating the backbone diffusion model, DiffCR achieves substantial bitrate savings (27.2\\% BD-rate (LPIPS) and 65.1\\% BD-rate (PSNR)) and over $10\\times$ speed-up compared to SOTA diffusion-based compression baselines.", "link": "http://arxiv.org/abs/2601.10373v1", "date": "2026-01-15", "relevancy": 2.4225, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6428}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6067}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5897}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Efficient%20Low-rate%20Image%20Compression%20with%20Frequency-aware%20Diffusion%20Prior%20Refinement&body=Title%3A%20Towards%20Efficient%20Low-rate%20Image%20Compression%20with%20Frequency-aware%20Diffusion%20Prior%20Refinement%0AAuthor%3A%20Yichong%20Xia%20and%20Yimin%20Zhou%20and%20Jinpeng%20Wang%20and%20Bin%20Chen%0AAbstract%3A%20Recent%20advancements%20in%20diffusion-based%20generative%20priors%20have%20enabled%20visually%20plausible%20image%20compression%20at%20extremely%20low%20bit%20rates.%20However%2C%20existing%20approaches%20suffer%20from%20slow%20sampling%20processes%20and%20suboptimal%20bit%20allocation%20due%20to%20fragmented%20training%20paradigms.%20In%20this%20work%2C%20we%20propose%20Accelerate%20%5Ctextbf%7BDiff%7Dusion-based%20Image%20Compression%20via%20%5Ctextbf%7BC%7Donsistency%20Prior%20%5Ctextbf%7BR%7Definement%20%28DiffCR%29%2C%20a%20novel%20compression%20framework%20for%20efficient%20and%20high-fidelity%20image%20reconstruction.%20At%20the%20heart%20of%20DiffCR%20is%20a%20Frequency-aware%20Skip%20Estimation%20%28FaSE%29%20module%20that%20refines%20the%20%24%CE%B5%24-prediction%20prior%20from%20a%20pre-trained%20latent%20diffusion%20model%20and%20aligns%20it%20with%20compressed%20latents%20at%20different%20timesteps%20via%20Frequency%20Decoupling%20Attention%20%28FDA%29.%20Furthermore%2C%20a%20lightweight%20consistency%20estimator%20enables%20fast%20%5Ctextbf%7Btwo-step%20decoding%7D%20by%20preserving%20the%20semantic%20trajectory%20of%20diffusion%20sampling.%20Without%20updating%20the%20backbone%20diffusion%20model%2C%20DiffCR%20achieves%20substantial%20bitrate%20savings%20%2827.2%5C%25%20BD-rate%20%28LPIPS%29%20and%2065.1%5C%25%20BD-rate%20%28PSNR%29%29%20and%20over%20%2410%5Ctimes%24%20speed-up%20compared%20to%20SOTA%20diffusion-based%20compression%20baselines.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10373v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Efficient%2520Low-rate%2520Image%2520Compression%2520with%2520Frequency-aware%2520Diffusion%2520Prior%2520Refinement%26entry.906535625%3DYichong%2520Xia%2520and%2520Yimin%2520Zhou%2520and%2520Jinpeng%2520Wang%2520and%2520Bin%2520Chen%26entry.1292438233%3DRecent%2520advancements%2520in%2520diffusion-based%2520generative%2520priors%2520have%2520enabled%2520visually%2520plausible%2520image%2520compression%2520at%2520extremely%2520low%2520bit%2520rates.%2520However%252C%2520existing%2520approaches%2520suffer%2520from%2520slow%2520sampling%2520processes%2520and%2520suboptimal%2520bit%2520allocation%2520due%2520to%2520fragmented%2520training%2520paradigms.%2520In%2520this%2520work%252C%2520we%2520propose%2520Accelerate%2520%255Ctextbf%257BDiff%257Dusion-based%2520Image%2520Compression%2520via%2520%255Ctextbf%257BC%257Donsistency%2520Prior%2520%255Ctextbf%257BR%257Definement%2520%2528DiffCR%2529%252C%2520a%2520novel%2520compression%2520framework%2520for%2520efficient%2520and%2520high-fidelity%2520image%2520reconstruction.%2520At%2520the%2520heart%2520of%2520DiffCR%2520is%2520a%2520Frequency-aware%2520Skip%2520Estimation%2520%2528FaSE%2529%2520module%2520that%2520refines%2520the%2520%2524%25CE%25B5%2524-prediction%2520prior%2520from%2520a%2520pre-trained%2520latent%2520diffusion%2520model%2520and%2520aligns%2520it%2520with%2520compressed%2520latents%2520at%2520different%2520timesteps%2520via%2520Frequency%2520Decoupling%2520Attention%2520%2528FDA%2529.%2520Furthermore%252C%2520a%2520lightweight%2520consistency%2520estimator%2520enables%2520fast%2520%255Ctextbf%257Btwo-step%2520decoding%257D%2520by%2520preserving%2520the%2520semantic%2520trajectory%2520of%2520diffusion%2520sampling.%2520Without%2520updating%2520the%2520backbone%2520diffusion%2520model%252C%2520DiffCR%2520achieves%2520substantial%2520bitrate%2520savings%2520%252827.2%255C%2525%2520BD-rate%2520%2528LPIPS%2529%2520and%252065.1%255C%2525%2520BD-rate%2520%2528PSNR%2529%2529%2520and%2520over%2520%252410%255Ctimes%2524%2520speed-up%2520compared%2520to%2520SOTA%2520diffusion-based%2520compression%2520baselines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10373v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Efficient%20Low-rate%20Image%20Compression%20with%20Frequency-aware%20Diffusion%20Prior%20Refinement&entry.906535625=Yichong%20Xia%20and%20Yimin%20Zhou%20and%20Jinpeng%20Wang%20and%20Bin%20Chen&entry.1292438233=Recent%20advancements%20in%20diffusion-based%20generative%20priors%20have%20enabled%20visually%20plausible%20image%20compression%20at%20extremely%20low%20bit%20rates.%20However%2C%20existing%20approaches%20suffer%20from%20slow%20sampling%20processes%20and%20suboptimal%20bit%20allocation%20due%20to%20fragmented%20training%20paradigms.%20In%20this%20work%2C%20we%20propose%20Accelerate%20%5Ctextbf%7BDiff%7Dusion-based%20Image%20Compression%20via%20%5Ctextbf%7BC%7Donsistency%20Prior%20%5Ctextbf%7BR%7Definement%20%28DiffCR%29%2C%20a%20novel%20compression%20framework%20for%20efficient%20and%20high-fidelity%20image%20reconstruction.%20At%20the%20heart%20of%20DiffCR%20is%20a%20Frequency-aware%20Skip%20Estimation%20%28FaSE%29%20module%20that%20refines%20the%20%24%CE%B5%24-prediction%20prior%20from%20a%20pre-trained%20latent%20diffusion%20model%20and%20aligns%20it%20with%20compressed%20latents%20at%20different%20timesteps%20via%20Frequency%20Decoupling%20Attention%20%28FDA%29.%20Furthermore%2C%20a%20lightweight%20consistency%20estimator%20enables%20fast%20%5Ctextbf%7Btwo-step%20decoding%7D%20by%20preserving%20the%20semantic%20trajectory%20of%20diffusion%20sampling.%20Without%20updating%20the%20backbone%20diffusion%20model%2C%20DiffCR%20achieves%20substantial%20bitrate%20savings%20%2827.2%5C%25%20BD-rate%20%28LPIPS%29%20and%2065.1%5C%25%20BD-rate%20%28PSNR%29%29%20and%20over%20%2410%5Ctimes%24%20speed-up%20compared%20to%20SOTA%20diffusion-based%20compression%20baselines.&entry.1838667208=http%3A//arxiv.org/abs/2601.10373v1&entry.124074799=Read"},
{"title": "Early Fault Detection on CMAPSS with Unsupervised LSTM Autoencoders", "author": "P. S\u00e1nchez and K. Reyes and B. Radu and E. Fern\u00e1ndez", "abstract": "This paper introduces an unsupervised health-monitoring framework for turbofan engines that does not require run-to-failure labels. First, operating-condition effects in NASA CMAPSS sensor streams are removed via regression-based normalisation; then a Long Short-Term Memory (LSTM) autoencoder is trained only on the healthy portion of each trajectory. Persistent reconstruction error, estimated using an adaptive data-driven threshold, triggers real-time alerts without hand-tuned rules. Benchmark results show high recall and low false-alarm rates across multiple operating regimes, demonstrating that the method can be deployed quickly, scale to diverse fleets, and serve as a complementary early-warning layer to Remaining Useful Life models.", "link": "http://arxiv.org/abs/2601.10269v1", "date": "2026-01-15", "relevancy": 2.4209, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.505}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4913}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Early%20Fault%20Detection%20on%20CMAPSS%20with%20Unsupervised%20LSTM%20Autoencoders&body=Title%3A%20Early%20Fault%20Detection%20on%20CMAPSS%20with%20Unsupervised%20LSTM%20Autoencoders%0AAuthor%3A%20P.%20S%C3%A1nchez%20and%20K.%20Reyes%20and%20B.%20Radu%20and%20E.%20Fern%C3%A1ndez%0AAbstract%3A%20This%20paper%20introduces%20an%20unsupervised%20health-monitoring%20framework%20for%20turbofan%20engines%20that%20does%20not%20require%20run-to-failure%20labels.%20First%2C%20operating-condition%20effects%20in%20NASA%20CMAPSS%20sensor%20streams%20are%20removed%20via%20regression-based%20normalisation%3B%20then%20a%20Long%20Short-Term%20Memory%20%28LSTM%29%20autoencoder%20is%20trained%20only%20on%20the%20healthy%20portion%20of%20each%20trajectory.%20Persistent%20reconstruction%20error%2C%20estimated%20using%20an%20adaptive%20data-driven%20threshold%2C%20triggers%20real-time%20alerts%20without%20hand-tuned%20rules.%20Benchmark%20results%20show%20high%20recall%20and%20low%20false-alarm%20rates%20across%20multiple%20operating%20regimes%2C%20demonstrating%20that%20the%20method%20can%20be%20deployed%20quickly%2C%20scale%20to%20diverse%20fleets%2C%20and%20serve%20as%20a%20complementary%20early-warning%20layer%20to%20Remaining%20Useful%20Life%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10269v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarly%2520Fault%2520Detection%2520on%2520CMAPSS%2520with%2520Unsupervised%2520LSTM%2520Autoencoders%26entry.906535625%3DP.%2520S%25C3%25A1nchez%2520and%2520K.%2520Reyes%2520and%2520B.%2520Radu%2520and%2520E.%2520Fern%25C3%25A1ndez%26entry.1292438233%3DThis%2520paper%2520introduces%2520an%2520unsupervised%2520health-monitoring%2520framework%2520for%2520turbofan%2520engines%2520that%2520does%2520not%2520require%2520run-to-failure%2520labels.%2520First%252C%2520operating-condition%2520effects%2520in%2520NASA%2520CMAPSS%2520sensor%2520streams%2520are%2520removed%2520via%2520regression-based%2520normalisation%253B%2520then%2520a%2520Long%2520Short-Term%2520Memory%2520%2528LSTM%2529%2520autoencoder%2520is%2520trained%2520only%2520on%2520the%2520healthy%2520portion%2520of%2520each%2520trajectory.%2520Persistent%2520reconstruction%2520error%252C%2520estimated%2520using%2520an%2520adaptive%2520data-driven%2520threshold%252C%2520triggers%2520real-time%2520alerts%2520without%2520hand-tuned%2520rules.%2520Benchmark%2520results%2520show%2520high%2520recall%2520and%2520low%2520false-alarm%2520rates%2520across%2520multiple%2520operating%2520regimes%252C%2520demonstrating%2520that%2520the%2520method%2520can%2520be%2520deployed%2520quickly%252C%2520scale%2520to%2520diverse%2520fleets%252C%2520and%2520serve%2520as%2520a%2520complementary%2520early-warning%2520layer%2520to%2520Remaining%2520Useful%2520Life%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10269v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Early%20Fault%20Detection%20on%20CMAPSS%20with%20Unsupervised%20LSTM%20Autoencoders&entry.906535625=P.%20S%C3%A1nchez%20and%20K.%20Reyes%20and%20B.%20Radu%20and%20E.%20Fern%C3%A1ndez&entry.1292438233=This%20paper%20introduces%20an%20unsupervised%20health-monitoring%20framework%20for%20turbofan%20engines%20that%20does%20not%20require%20run-to-failure%20labels.%20First%2C%20operating-condition%20effects%20in%20NASA%20CMAPSS%20sensor%20streams%20are%20removed%20via%20regression-based%20normalisation%3B%20then%20a%20Long%20Short-Term%20Memory%20%28LSTM%29%20autoencoder%20is%20trained%20only%20on%20the%20healthy%20portion%20of%20each%20trajectory.%20Persistent%20reconstruction%20error%2C%20estimated%20using%20an%20adaptive%20data-driven%20threshold%2C%20triggers%20real-time%20alerts%20without%20hand-tuned%20rules.%20Benchmark%20results%20show%20high%20recall%20and%20low%20false-alarm%20rates%20across%20multiple%20operating%20regimes%2C%20demonstrating%20that%20the%20method%20can%20be%20deployed%20quickly%2C%20scale%20to%20diverse%20fleets%2C%20and%20serve%20as%20a%20complementary%20early-warning%20layer%20to%20Remaining%20Useful%20Life%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.10269v1&entry.124074799=Read"},
{"title": "RTV-Bench: Benchmarking MLLM Continuous Perception, Understanding and Reasoning through Real-Time Video", "author": "Shuhang Xun and Sicheng Tao and Jungang Li and Yibo Shi and Zhixin Lin and Zhanhui Zhu and Yibo Yan and Hanqian Li and Linghao Zhang and Shikang Wang and Yixin Liu and Hanbo Zhang and Ying Ma and Xuming Hu", "abstract": "Multimodal Large Language Models (MLLMs) have made rapid progress in perception, understanding, and reasoning, yet existing benchmarks fall short in evaluating these abilities under continuous and dynamic real-world video streams. Such settings require models to maintain coherent understanding and reasoning as visual scenes evolve over time. **We introduce RTV-Bench, a fine-grained benchmark for real-time video analysis with MLLMs**. It is built upon three key principles: multi-timestamp question answering, hierarchical question structures spanning perception and reasoning, and multi-dimensional evaluation of continuous perception, understanding, and reasoning. RTV-Bench comprises 552 diverse videos and 4,608 carefully curated QA pairs covering a wide range of dynamic scenarios. We evaluate a broad range of state-of-the-art MLLMs, including proprietary, open-source offline, and open-source real-time models. Our results show that real-time models generally outperform offline counterparts but still lag behind leading proprietary systems. While scaling model capacity generally yields performance gains, simply increasing the density of sampled input frames does not consistently translate into improved results. These observations suggest inherent limitations in current architectures when handling long-horizon video streams, underscoring the need for models explicitly designed for streaming video processing and analysis.", "link": "http://arxiv.org/abs/2505.02064v4", "date": "2026-01-15", "relevancy": 2.4094, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6136}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6136}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5462}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RTV-Bench%3A%20Benchmarking%20MLLM%20Continuous%20Perception%2C%20Understanding%20and%20Reasoning%20through%20Real-Time%20Video&body=Title%3A%20RTV-Bench%3A%20Benchmarking%20MLLM%20Continuous%20Perception%2C%20Understanding%20and%20Reasoning%20through%20Real-Time%20Video%0AAuthor%3A%20Shuhang%20Xun%20and%20Sicheng%20Tao%20and%20Jungang%20Li%20and%20Yibo%20Shi%20and%20Zhixin%20Lin%20and%20Zhanhui%20Zhu%20and%20Yibo%20Yan%20and%20Hanqian%20Li%20and%20Linghao%20Zhang%20and%20Shikang%20Wang%20and%20Yixin%20Liu%20and%20Hanbo%20Zhang%20and%20Ying%20Ma%20and%20Xuming%20Hu%0AAbstract%3A%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20made%20rapid%20progress%20in%20perception%2C%20understanding%2C%20and%20reasoning%2C%20yet%20existing%20benchmarks%20fall%20short%20in%20evaluating%20these%20abilities%20under%20continuous%20and%20dynamic%20real-world%20video%20streams.%20Such%20settings%20require%20models%20to%20maintain%20coherent%20understanding%20and%20reasoning%20as%20visual%20scenes%20evolve%20over%20time.%20%2A%2AWe%20introduce%20RTV-Bench%2C%20a%20fine-grained%20benchmark%20for%20real-time%20video%20analysis%20with%20MLLMs%2A%2A.%20It%20is%20built%20upon%20three%20key%20principles%3A%20multi-timestamp%20question%20answering%2C%20hierarchical%20question%20structures%20spanning%20perception%20and%20reasoning%2C%20and%20multi-dimensional%20evaluation%20of%20continuous%20perception%2C%20understanding%2C%20and%20reasoning.%20RTV-Bench%20comprises%20552%20diverse%20videos%20and%204%2C608%20carefully%20curated%20QA%20pairs%20covering%20a%20wide%20range%20of%20dynamic%20scenarios.%20We%20evaluate%20a%20broad%20range%20of%20state-of-the-art%20MLLMs%2C%20including%20proprietary%2C%20open-source%20offline%2C%20and%20open-source%20real-time%20models.%20Our%20results%20show%20that%20real-time%20models%20generally%20outperform%20offline%20counterparts%20but%20still%20lag%20behind%20leading%20proprietary%20systems.%20While%20scaling%20model%20capacity%20generally%20yields%20performance%20gains%2C%20simply%20increasing%20the%20density%20of%20sampled%20input%20frames%20does%20not%20consistently%20translate%20into%20improved%20results.%20These%20observations%20suggest%20inherent%20limitations%20in%20current%20architectures%20when%20handling%20long-horizon%20video%20streams%2C%20underscoring%20the%20need%20for%20models%20explicitly%20designed%20for%20streaming%20video%20processing%20and%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2505.02064v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRTV-Bench%253A%2520Benchmarking%2520MLLM%2520Continuous%2520Perception%252C%2520Understanding%2520and%2520Reasoning%2520through%2520Real-Time%2520Video%26entry.906535625%3DShuhang%2520Xun%2520and%2520Sicheng%2520Tao%2520and%2520Jungang%2520Li%2520and%2520Yibo%2520Shi%2520and%2520Zhixin%2520Lin%2520and%2520Zhanhui%2520Zhu%2520and%2520Yibo%2520Yan%2520and%2520Hanqian%2520Li%2520and%2520Linghao%2520Zhang%2520and%2520Shikang%2520Wang%2520and%2520Yixin%2520Liu%2520and%2520Hanbo%2520Zhang%2520and%2520Ying%2520Ma%2520and%2520Xuming%2520Hu%26entry.1292438233%3DMultimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520have%2520made%2520rapid%2520progress%2520in%2520perception%252C%2520understanding%252C%2520and%2520reasoning%252C%2520yet%2520existing%2520benchmarks%2520fall%2520short%2520in%2520evaluating%2520these%2520abilities%2520under%2520continuous%2520and%2520dynamic%2520real-world%2520video%2520streams.%2520Such%2520settings%2520require%2520models%2520to%2520maintain%2520coherent%2520understanding%2520and%2520reasoning%2520as%2520visual%2520scenes%2520evolve%2520over%2520time.%2520%252A%252AWe%2520introduce%2520RTV-Bench%252C%2520a%2520fine-grained%2520benchmark%2520for%2520real-time%2520video%2520analysis%2520with%2520MLLMs%252A%252A.%2520It%2520is%2520built%2520upon%2520three%2520key%2520principles%253A%2520multi-timestamp%2520question%2520answering%252C%2520hierarchical%2520question%2520structures%2520spanning%2520perception%2520and%2520reasoning%252C%2520and%2520multi-dimensional%2520evaluation%2520of%2520continuous%2520perception%252C%2520understanding%252C%2520and%2520reasoning.%2520RTV-Bench%2520comprises%2520552%2520diverse%2520videos%2520and%25204%252C608%2520carefully%2520curated%2520QA%2520pairs%2520covering%2520a%2520wide%2520range%2520of%2520dynamic%2520scenarios.%2520We%2520evaluate%2520a%2520broad%2520range%2520of%2520state-of-the-art%2520MLLMs%252C%2520including%2520proprietary%252C%2520open-source%2520offline%252C%2520and%2520open-source%2520real-time%2520models.%2520Our%2520results%2520show%2520that%2520real-time%2520models%2520generally%2520outperform%2520offline%2520counterparts%2520but%2520still%2520lag%2520behind%2520leading%2520proprietary%2520systems.%2520While%2520scaling%2520model%2520capacity%2520generally%2520yields%2520performance%2520gains%252C%2520simply%2520increasing%2520the%2520density%2520of%2520sampled%2520input%2520frames%2520does%2520not%2520consistently%2520translate%2520into%2520improved%2520results.%2520These%2520observations%2520suggest%2520inherent%2520limitations%2520in%2520current%2520architectures%2520when%2520handling%2520long-horizon%2520video%2520streams%252C%2520underscoring%2520the%2520need%2520for%2520models%2520explicitly%2520designed%2520for%2520streaming%2520video%2520processing%2520and%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.02064v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RTV-Bench%3A%20Benchmarking%20MLLM%20Continuous%20Perception%2C%20Understanding%20and%20Reasoning%20through%20Real-Time%20Video&entry.906535625=Shuhang%20Xun%20and%20Sicheng%20Tao%20and%20Jungang%20Li%20and%20Yibo%20Shi%20and%20Zhixin%20Lin%20and%20Zhanhui%20Zhu%20and%20Yibo%20Yan%20and%20Hanqian%20Li%20and%20Linghao%20Zhang%20and%20Shikang%20Wang%20and%20Yixin%20Liu%20and%20Hanbo%20Zhang%20and%20Ying%20Ma%20and%20Xuming%20Hu&entry.1292438233=Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20have%20made%20rapid%20progress%20in%20perception%2C%20understanding%2C%20and%20reasoning%2C%20yet%20existing%20benchmarks%20fall%20short%20in%20evaluating%20these%20abilities%20under%20continuous%20and%20dynamic%20real-world%20video%20streams.%20Such%20settings%20require%20models%20to%20maintain%20coherent%20understanding%20and%20reasoning%20as%20visual%20scenes%20evolve%20over%20time.%20%2A%2AWe%20introduce%20RTV-Bench%2C%20a%20fine-grained%20benchmark%20for%20real-time%20video%20analysis%20with%20MLLMs%2A%2A.%20It%20is%20built%20upon%20three%20key%20principles%3A%20multi-timestamp%20question%20answering%2C%20hierarchical%20question%20structures%20spanning%20perception%20and%20reasoning%2C%20and%20multi-dimensional%20evaluation%20of%20continuous%20perception%2C%20understanding%2C%20and%20reasoning.%20RTV-Bench%20comprises%20552%20diverse%20videos%20and%204%2C608%20carefully%20curated%20QA%20pairs%20covering%20a%20wide%20range%20of%20dynamic%20scenarios.%20We%20evaluate%20a%20broad%20range%20of%20state-of-the-art%20MLLMs%2C%20including%20proprietary%2C%20open-source%20offline%2C%20and%20open-source%20real-time%20models.%20Our%20results%20show%20that%20real-time%20models%20generally%20outperform%20offline%20counterparts%20but%20still%20lag%20behind%20leading%20proprietary%20systems.%20While%20scaling%20model%20capacity%20generally%20yields%20performance%20gains%2C%20simply%20increasing%20the%20density%20of%20sampled%20input%20frames%20does%20not%20consistently%20translate%20into%20improved%20results.%20These%20observations%20suggest%20inherent%20limitations%20in%20current%20architectures%20when%20handling%20long-horizon%20video%20streams%2C%20underscoring%20the%20need%20for%20models%20explicitly%20designed%20for%20streaming%20video%20processing%20and%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2505.02064v4&entry.124074799=Read"},
{"title": "Are Language Models Efficient Reasoners? A Perspective from Logic Programming", "author": "Andreas Opedal and Yanick Zengaffinen and Haruki Shirakami and Clemente Pasti and Mrinmaya Sachan and Abulhair Saparov and Ryan Cotterell and Bernhard Sch\u00f6lkopf", "abstract": "Modern language models (LMs) exhibit strong deductive reasoning capabilities, yet standard evaluations emphasize correctness while overlooking a key aspect of reasoning: efficiency. In real-world reasoning scenarios, much of the available information is irrelevant, and effective deductive inference requires identifying and ignoring such distractions. We propose a framework for assessing LM reasoning efficiency through the lens of logic programming, introducing a simple method to align proofs written in natural language -- as generated by an LM -- with shortest proofs found by executing the logic program. Efficiency is quantified by measuring how well a model avoids unnecessary inference. Empirically, we construct a dataset of math word problems injected with various number of irrelevant axioms that vary in semantic overlap with the goal theorem. We find that current LMs show marked accuracy declines under such conditions -- even with minimal, domain-consistent distractions -- and the proofs they generate frequently exhibit detours through irrelevant inferences.", "link": "http://arxiv.org/abs/2510.25626v2", "date": "2026-01-15", "relevancy": 2.4071, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5014}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5014}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Language%20Models%20Efficient%20Reasoners%3F%20A%20Perspective%20from%20Logic%20Programming&body=Title%3A%20Are%20Language%20Models%20Efficient%20Reasoners%3F%20A%20Perspective%20from%20Logic%20Programming%0AAuthor%3A%20Andreas%20Opedal%20and%20Yanick%20Zengaffinen%20and%20Haruki%20Shirakami%20and%20Clemente%20Pasti%20and%20Mrinmaya%20Sachan%20and%20Abulhair%20Saparov%20and%20Ryan%20Cotterell%20and%20Bernhard%20Sch%C3%B6lkopf%0AAbstract%3A%20Modern%20language%20models%20%28LMs%29%20exhibit%20strong%20deductive%20reasoning%20capabilities%2C%20yet%20standard%20evaluations%20emphasize%20correctness%20while%20overlooking%20a%20key%20aspect%20of%20reasoning%3A%20efficiency.%20In%20real-world%20reasoning%20scenarios%2C%20much%20of%20the%20available%20information%20is%20irrelevant%2C%20and%20effective%20deductive%20inference%20requires%20identifying%20and%20ignoring%20such%20distractions.%20We%20propose%20a%20framework%20for%20assessing%20LM%20reasoning%20efficiency%20through%20the%20lens%20of%20logic%20programming%2C%20introducing%20a%20simple%20method%20to%20align%20proofs%20written%20in%20natural%20language%20--%20as%20generated%20by%20an%20LM%20--%20with%20shortest%20proofs%20found%20by%20executing%20the%20logic%20program.%20Efficiency%20is%20quantified%20by%20measuring%20how%20well%20a%20model%20avoids%20unnecessary%20inference.%20Empirically%2C%20we%20construct%20a%20dataset%20of%20math%20word%20problems%20injected%20with%20various%20number%20of%20irrelevant%20axioms%20that%20vary%20in%20semantic%20overlap%20with%20the%20goal%20theorem.%20We%20find%20that%20current%20LMs%20show%20marked%20accuracy%20declines%20under%20such%20conditions%20--%20even%20with%20minimal%2C%20domain-consistent%20distractions%20--%20and%20the%20proofs%20they%20generate%20frequently%20exhibit%20detours%20through%20irrelevant%20inferences.%0ALink%3A%20http%3A//arxiv.org/abs/2510.25626v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Language%2520Models%2520Efficient%2520Reasoners%253F%2520A%2520Perspective%2520from%2520Logic%2520Programming%26entry.906535625%3DAndreas%2520Opedal%2520and%2520Yanick%2520Zengaffinen%2520and%2520Haruki%2520Shirakami%2520and%2520Clemente%2520Pasti%2520and%2520Mrinmaya%2520Sachan%2520and%2520Abulhair%2520Saparov%2520and%2520Ryan%2520Cotterell%2520and%2520Bernhard%2520Sch%25C3%25B6lkopf%26entry.1292438233%3DModern%2520language%2520models%2520%2528LMs%2529%2520exhibit%2520strong%2520deductive%2520reasoning%2520capabilities%252C%2520yet%2520standard%2520evaluations%2520emphasize%2520correctness%2520while%2520overlooking%2520a%2520key%2520aspect%2520of%2520reasoning%253A%2520efficiency.%2520In%2520real-world%2520reasoning%2520scenarios%252C%2520much%2520of%2520the%2520available%2520information%2520is%2520irrelevant%252C%2520and%2520effective%2520deductive%2520inference%2520requires%2520identifying%2520and%2520ignoring%2520such%2520distractions.%2520We%2520propose%2520a%2520framework%2520for%2520assessing%2520LM%2520reasoning%2520efficiency%2520through%2520the%2520lens%2520of%2520logic%2520programming%252C%2520introducing%2520a%2520simple%2520method%2520to%2520align%2520proofs%2520written%2520in%2520natural%2520language%2520--%2520as%2520generated%2520by%2520an%2520LM%2520--%2520with%2520shortest%2520proofs%2520found%2520by%2520executing%2520the%2520logic%2520program.%2520Efficiency%2520is%2520quantified%2520by%2520measuring%2520how%2520well%2520a%2520model%2520avoids%2520unnecessary%2520inference.%2520Empirically%252C%2520we%2520construct%2520a%2520dataset%2520of%2520math%2520word%2520problems%2520injected%2520with%2520various%2520number%2520of%2520irrelevant%2520axioms%2520that%2520vary%2520in%2520semantic%2520overlap%2520with%2520the%2520goal%2520theorem.%2520We%2520find%2520that%2520current%2520LMs%2520show%2520marked%2520accuracy%2520declines%2520under%2520such%2520conditions%2520--%2520even%2520with%2520minimal%252C%2520domain-consistent%2520distractions%2520--%2520and%2520the%2520proofs%2520they%2520generate%2520frequently%2520exhibit%2520detours%2520through%2520irrelevant%2520inferences.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.25626v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Language%20Models%20Efficient%20Reasoners%3F%20A%20Perspective%20from%20Logic%20Programming&entry.906535625=Andreas%20Opedal%20and%20Yanick%20Zengaffinen%20and%20Haruki%20Shirakami%20and%20Clemente%20Pasti%20and%20Mrinmaya%20Sachan%20and%20Abulhair%20Saparov%20and%20Ryan%20Cotterell%20and%20Bernhard%20Sch%C3%B6lkopf&entry.1292438233=Modern%20language%20models%20%28LMs%29%20exhibit%20strong%20deductive%20reasoning%20capabilities%2C%20yet%20standard%20evaluations%20emphasize%20correctness%20while%20overlooking%20a%20key%20aspect%20of%20reasoning%3A%20efficiency.%20In%20real-world%20reasoning%20scenarios%2C%20much%20of%20the%20available%20information%20is%20irrelevant%2C%20and%20effective%20deductive%20inference%20requires%20identifying%20and%20ignoring%20such%20distractions.%20We%20propose%20a%20framework%20for%20assessing%20LM%20reasoning%20efficiency%20through%20the%20lens%20of%20logic%20programming%2C%20introducing%20a%20simple%20method%20to%20align%20proofs%20written%20in%20natural%20language%20--%20as%20generated%20by%20an%20LM%20--%20with%20shortest%20proofs%20found%20by%20executing%20the%20logic%20program.%20Efficiency%20is%20quantified%20by%20measuring%20how%20well%20a%20model%20avoids%20unnecessary%20inference.%20Empirically%2C%20we%20construct%20a%20dataset%20of%20math%20word%20problems%20injected%20with%20various%20number%20of%20irrelevant%20axioms%20that%20vary%20in%20semantic%20overlap%20with%20the%20goal%20theorem.%20We%20find%20that%20current%20LMs%20show%20marked%20accuracy%20declines%20under%20such%20conditions%20--%20even%20with%20minimal%2C%20domain-consistent%20distractions%20--%20and%20the%20proofs%20they%20generate%20frequently%20exhibit%20detours%20through%20irrelevant%20inferences.&entry.1838667208=http%3A//arxiv.org/abs/2510.25626v2&entry.124074799=Read"},
{"title": "On the Failure of Latent State Persistence in Large Language Models", "author": "Jen-tse Huang and Kaiser Sun and Wenxuan Wang and Mark Dredze", "abstract": "While Large Language Models (LLMs) excel in reasoning, whether they can sustain persistent latent states remains under-explored. The capacity to maintain and manipulate unexpressed, internal representations-analogous to human working memory-is a cornerstone of complex reasoning. In this paper, we formalize and quantify the \"Latent State Persistence\" (LSP) gap through three novel experiments. First, we utilize a Number Guessing Game, demonstrating that across independent queries, LLMs fail to allocate probability mass to a singular hidden choice, violating a fundamental probabilistic principle. Second, we employ a Yes-No Game to show that as the number of questions increases, LLMs suffer from \"concept drift,\" leading to inevitable self-contradictions due to the lack of LSP. Finally, inspired by Mathematical Mentalism, we task models with tracking transformations on hidden variables, revealing a failure in variable binding and state evolution when the initial state is not explicitly present in the context. Collectively, these findings suggest that LLMs function as reactive post-hoc solvers rather than proactive planners with LSP. Our work provides a framework for evaluating the fidelity of internal representations and highlights a fundamental architectural divergence between autoregressive transformers and human-like cognition.", "link": "http://arxiv.org/abs/2505.10571v4", "date": "2026-01-15", "relevancy": 2.3948, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.483}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.483}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4708}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Failure%20of%20Latent%20State%20Persistence%20in%20Large%20Language%20Models&body=Title%3A%20On%20the%20Failure%20of%20Latent%20State%20Persistence%20in%20Large%20Language%20Models%0AAuthor%3A%20Jen-tse%20Huang%20and%20Kaiser%20Sun%20and%20Wenxuan%20Wang%20and%20Mark%20Dredze%0AAbstract%3A%20While%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20reasoning%2C%20whether%20they%20can%20sustain%20persistent%20latent%20states%20remains%20under-explored.%20The%20capacity%20to%20maintain%20and%20manipulate%20unexpressed%2C%20internal%20representations-analogous%20to%20human%20working%20memory-is%20a%20cornerstone%20of%20complex%20reasoning.%20In%20this%20paper%2C%20we%20formalize%20and%20quantify%20the%20%22Latent%20State%20Persistence%22%20%28LSP%29%20gap%20through%20three%20novel%20experiments.%20First%2C%20we%20utilize%20a%20Number%20Guessing%20Game%2C%20demonstrating%20that%20across%20independent%20queries%2C%20LLMs%20fail%20to%20allocate%20probability%20mass%20to%20a%20singular%20hidden%20choice%2C%20violating%20a%20fundamental%20probabilistic%20principle.%20Second%2C%20we%20employ%20a%20Yes-No%20Game%20to%20show%20that%20as%20the%20number%20of%20questions%20increases%2C%20LLMs%20suffer%20from%20%22concept%20drift%2C%22%20leading%20to%20inevitable%20self-contradictions%20due%20to%20the%20lack%20of%20LSP.%20Finally%2C%20inspired%20by%20Mathematical%20Mentalism%2C%20we%20task%20models%20with%20tracking%20transformations%20on%20hidden%20variables%2C%20revealing%20a%20failure%20in%20variable%20binding%20and%20state%20evolution%20when%20the%20initial%20state%20is%20not%20explicitly%20present%20in%20the%20context.%20Collectively%2C%20these%20findings%20suggest%20that%20LLMs%20function%20as%20reactive%20post-hoc%20solvers%20rather%20than%20proactive%20planners%20with%20LSP.%20Our%20work%20provides%20a%20framework%20for%20evaluating%20the%20fidelity%20of%20internal%20representations%20and%20highlights%20a%20fundamental%20architectural%20divergence%20between%20autoregressive%20transformers%20and%20human-like%20cognition.%0ALink%3A%20http%3A//arxiv.org/abs/2505.10571v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Failure%2520of%2520Latent%2520State%2520Persistence%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DJen-tse%2520Huang%2520and%2520Kaiser%2520Sun%2520and%2520Wenxuan%2520Wang%2520and%2520Mark%2520Dredze%26entry.1292438233%3DWhile%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520excel%2520in%2520reasoning%252C%2520whether%2520they%2520can%2520sustain%2520persistent%2520latent%2520states%2520remains%2520under-explored.%2520The%2520capacity%2520to%2520maintain%2520and%2520manipulate%2520unexpressed%252C%2520internal%2520representations-analogous%2520to%2520human%2520working%2520memory-is%2520a%2520cornerstone%2520of%2520complex%2520reasoning.%2520In%2520this%2520paper%252C%2520we%2520formalize%2520and%2520quantify%2520the%2520%2522Latent%2520State%2520Persistence%2522%2520%2528LSP%2529%2520gap%2520through%2520three%2520novel%2520experiments.%2520First%252C%2520we%2520utilize%2520a%2520Number%2520Guessing%2520Game%252C%2520demonstrating%2520that%2520across%2520independent%2520queries%252C%2520LLMs%2520fail%2520to%2520allocate%2520probability%2520mass%2520to%2520a%2520singular%2520hidden%2520choice%252C%2520violating%2520a%2520fundamental%2520probabilistic%2520principle.%2520Second%252C%2520we%2520employ%2520a%2520Yes-No%2520Game%2520to%2520show%2520that%2520as%2520the%2520number%2520of%2520questions%2520increases%252C%2520LLMs%2520suffer%2520from%2520%2522concept%2520drift%252C%2522%2520leading%2520to%2520inevitable%2520self-contradictions%2520due%2520to%2520the%2520lack%2520of%2520LSP.%2520Finally%252C%2520inspired%2520by%2520Mathematical%2520Mentalism%252C%2520we%2520task%2520models%2520with%2520tracking%2520transformations%2520on%2520hidden%2520variables%252C%2520revealing%2520a%2520failure%2520in%2520variable%2520binding%2520and%2520state%2520evolution%2520when%2520the%2520initial%2520state%2520is%2520not%2520explicitly%2520present%2520in%2520the%2520context.%2520Collectively%252C%2520these%2520findings%2520suggest%2520that%2520LLMs%2520function%2520as%2520reactive%2520post-hoc%2520solvers%2520rather%2520than%2520proactive%2520planners%2520with%2520LSP.%2520Our%2520work%2520provides%2520a%2520framework%2520for%2520evaluating%2520the%2520fidelity%2520of%2520internal%2520representations%2520and%2520highlights%2520a%2520fundamental%2520architectural%2520divergence%2520between%2520autoregressive%2520transformers%2520and%2520human-like%2520cognition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.10571v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Failure%20of%20Latent%20State%20Persistence%20in%20Large%20Language%20Models&entry.906535625=Jen-tse%20Huang%20and%20Kaiser%20Sun%20and%20Wenxuan%20Wang%20and%20Mark%20Dredze&entry.1292438233=While%20Large%20Language%20Models%20%28LLMs%29%20excel%20in%20reasoning%2C%20whether%20they%20can%20sustain%20persistent%20latent%20states%20remains%20under-explored.%20The%20capacity%20to%20maintain%20and%20manipulate%20unexpressed%2C%20internal%20representations-analogous%20to%20human%20working%20memory-is%20a%20cornerstone%20of%20complex%20reasoning.%20In%20this%20paper%2C%20we%20formalize%20and%20quantify%20the%20%22Latent%20State%20Persistence%22%20%28LSP%29%20gap%20through%20three%20novel%20experiments.%20First%2C%20we%20utilize%20a%20Number%20Guessing%20Game%2C%20demonstrating%20that%20across%20independent%20queries%2C%20LLMs%20fail%20to%20allocate%20probability%20mass%20to%20a%20singular%20hidden%20choice%2C%20violating%20a%20fundamental%20probabilistic%20principle.%20Second%2C%20we%20employ%20a%20Yes-No%20Game%20to%20show%20that%20as%20the%20number%20of%20questions%20increases%2C%20LLMs%20suffer%20from%20%22concept%20drift%2C%22%20leading%20to%20inevitable%20self-contradictions%20due%20to%20the%20lack%20of%20LSP.%20Finally%2C%20inspired%20by%20Mathematical%20Mentalism%2C%20we%20task%20models%20with%20tracking%20transformations%20on%20hidden%20variables%2C%20revealing%20a%20failure%20in%20variable%20binding%20and%20state%20evolution%20when%20the%20initial%20state%20is%20not%20explicitly%20present%20in%20the%20context.%20Collectively%2C%20these%20findings%20suggest%20that%20LLMs%20function%20as%20reactive%20post-hoc%20solvers%20rather%20than%20proactive%20planners%20with%20LSP.%20Our%20work%20provides%20a%20framework%20for%20evaluating%20the%20fidelity%20of%20internal%20representations%20and%20highlights%20a%20fundamental%20architectural%20divergence%20between%20autoregressive%20transformers%20and%20human-like%20cognition.&entry.1838667208=http%3A//arxiv.org/abs/2505.10571v4&entry.124074799=Read"},
{"title": "MixMin: Finding Data Mixtures via Convex Minimization", "author": "Anvith Thudi and Evianne Rovers and Yangjun Ruan and Tristan Thrush and Chris J. Maddison", "abstract": "Modern machine learning pipelines are increasingly combining and mixing data from diverse and disparate sources, e.g., pre-training large language models. Yet, finding the optimal data mixture is a challenging and open problem. We formalize this data mixing problem as a bi-level objective: the best mixture is the one that would lead to the best model for a downstream objective. Unfortunately, this objective is generally intractable. In this paper, we make the observation that the bi-level data mixing objective becomes convex as our model class becomes larger. We develop and study a gradient-based approach for optimizing this convex objective, which we call MixMin, and test it on language modeling and chemistry tasks. MixMin was the only method that uniformly improved the data mixture in all our experiments. With MixMin, we improved the data mixture using less than 0.2% additional compute for a pythia-410M model trained on 8.2B tokens, resulting between 1-5% relative improvement to negative log likelihood on PIQA, ARC Easy, SciQ, and OpenWebMath. Crucially, we found that MixMin mixtures for smaller models improved training of larger models, suggesting that MixMin mixtures may be scale-invariant. When mixing bioassay data to train an XGBoost model, we saw improvements to average precision scores of 0.03-0.15.", "link": "http://arxiv.org/abs/2502.10510v3", "date": "2026-01-15", "relevancy": 2.3945, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4849}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4827}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4691}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MixMin%3A%20Finding%20Data%20Mixtures%20via%20Convex%20Minimization&body=Title%3A%20MixMin%3A%20Finding%20Data%20Mixtures%20via%20Convex%20Minimization%0AAuthor%3A%20Anvith%20Thudi%20and%20Evianne%20Rovers%20and%20Yangjun%20Ruan%20and%20Tristan%20Thrush%20and%20Chris%20J.%20Maddison%0AAbstract%3A%20Modern%20machine%20learning%20pipelines%20are%20increasingly%20combining%20and%20mixing%20data%20from%20diverse%20and%20disparate%20sources%2C%20e.g.%2C%20pre-training%20large%20language%20models.%20Yet%2C%20finding%20the%20optimal%20data%20mixture%20is%20a%20challenging%20and%20open%20problem.%20We%20formalize%20this%20data%20mixing%20problem%20as%20a%20bi-level%20objective%3A%20the%20best%20mixture%20is%20the%20one%20that%20would%20lead%20to%20the%20best%20model%20for%20a%20downstream%20objective.%20Unfortunately%2C%20this%20objective%20is%20generally%20intractable.%20In%20this%20paper%2C%20we%20make%20the%20observation%20that%20the%20bi-level%20data%20mixing%20objective%20becomes%20convex%20as%20our%20model%20class%20becomes%20larger.%20We%20develop%20and%20study%20a%20gradient-based%20approach%20for%20optimizing%20this%20convex%20objective%2C%20which%20we%20call%20MixMin%2C%20and%20test%20it%20on%20language%20modeling%20and%20chemistry%20tasks.%20MixMin%20was%20the%20only%20method%20that%20uniformly%20improved%20the%20data%20mixture%20in%20all%20our%20experiments.%20With%20MixMin%2C%20we%20improved%20the%20data%20mixture%20using%20less%20than%200.2%25%20additional%20compute%20for%20a%20pythia-410M%20model%20trained%20on%208.2B%20tokens%2C%20resulting%20between%201-5%25%20relative%20improvement%20to%20negative%20log%20likelihood%20on%20PIQA%2C%20ARC%20Easy%2C%20SciQ%2C%20and%20OpenWebMath.%20Crucially%2C%20we%20found%20that%20MixMin%20mixtures%20for%20smaller%20models%20improved%20training%20of%20larger%20models%2C%20suggesting%20that%20MixMin%20mixtures%20may%20be%20scale-invariant.%20When%20mixing%20bioassay%20data%20to%20train%20an%20XGBoost%20model%2C%20we%20saw%20improvements%20to%20average%20precision%20scores%20of%200.03-0.15.%0ALink%3A%20http%3A//arxiv.org/abs/2502.10510v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixMin%253A%2520Finding%2520Data%2520Mixtures%2520via%2520Convex%2520Minimization%26entry.906535625%3DAnvith%2520Thudi%2520and%2520Evianne%2520Rovers%2520and%2520Yangjun%2520Ruan%2520and%2520Tristan%2520Thrush%2520and%2520Chris%2520J.%2520Maddison%26entry.1292438233%3DModern%2520machine%2520learning%2520pipelines%2520are%2520increasingly%2520combining%2520and%2520mixing%2520data%2520from%2520diverse%2520and%2520disparate%2520sources%252C%2520e.g.%252C%2520pre-training%2520large%2520language%2520models.%2520Yet%252C%2520finding%2520the%2520optimal%2520data%2520mixture%2520is%2520a%2520challenging%2520and%2520open%2520problem.%2520We%2520formalize%2520this%2520data%2520mixing%2520problem%2520as%2520a%2520bi-level%2520objective%253A%2520the%2520best%2520mixture%2520is%2520the%2520one%2520that%2520would%2520lead%2520to%2520the%2520best%2520model%2520for%2520a%2520downstream%2520objective.%2520Unfortunately%252C%2520this%2520objective%2520is%2520generally%2520intractable.%2520In%2520this%2520paper%252C%2520we%2520make%2520the%2520observation%2520that%2520the%2520bi-level%2520data%2520mixing%2520objective%2520becomes%2520convex%2520as%2520our%2520model%2520class%2520becomes%2520larger.%2520We%2520develop%2520and%2520study%2520a%2520gradient-based%2520approach%2520for%2520optimizing%2520this%2520convex%2520objective%252C%2520which%2520we%2520call%2520MixMin%252C%2520and%2520test%2520it%2520on%2520language%2520modeling%2520and%2520chemistry%2520tasks.%2520MixMin%2520was%2520the%2520only%2520method%2520that%2520uniformly%2520improved%2520the%2520data%2520mixture%2520in%2520all%2520our%2520experiments.%2520With%2520MixMin%252C%2520we%2520improved%2520the%2520data%2520mixture%2520using%2520less%2520than%25200.2%2525%2520additional%2520compute%2520for%2520a%2520pythia-410M%2520model%2520trained%2520on%25208.2B%2520tokens%252C%2520resulting%2520between%25201-5%2525%2520relative%2520improvement%2520to%2520negative%2520log%2520likelihood%2520on%2520PIQA%252C%2520ARC%2520Easy%252C%2520SciQ%252C%2520and%2520OpenWebMath.%2520Crucially%252C%2520we%2520found%2520that%2520MixMin%2520mixtures%2520for%2520smaller%2520models%2520improved%2520training%2520of%2520larger%2520models%252C%2520suggesting%2520that%2520MixMin%2520mixtures%2520may%2520be%2520scale-invariant.%2520When%2520mixing%2520bioassay%2520data%2520to%2520train%2520an%2520XGBoost%2520model%252C%2520we%2520saw%2520improvements%2520to%2520average%2520precision%2520scores%2520of%25200.03-0.15.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.10510v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MixMin%3A%20Finding%20Data%20Mixtures%20via%20Convex%20Minimization&entry.906535625=Anvith%20Thudi%20and%20Evianne%20Rovers%20and%20Yangjun%20Ruan%20and%20Tristan%20Thrush%20and%20Chris%20J.%20Maddison&entry.1292438233=Modern%20machine%20learning%20pipelines%20are%20increasingly%20combining%20and%20mixing%20data%20from%20diverse%20and%20disparate%20sources%2C%20e.g.%2C%20pre-training%20large%20language%20models.%20Yet%2C%20finding%20the%20optimal%20data%20mixture%20is%20a%20challenging%20and%20open%20problem.%20We%20formalize%20this%20data%20mixing%20problem%20as%20a%20bi-level%20objective%3A%20the%20best%20mixture%20is%20the%20one%20that%20would%20lead%20to%20the%20best%20model%20for%20a%20downstream%20objective.%20Unfortunately%2C%20this%20objective%20is%20generally%20intractable.%20In%20this%20paper%2C%20we%20make%20the%20observation%20that%20the%20bi-level%20data%20mixing%20objective%20becomes%20convex%20as%20our%20model%20class%20becomes%20larger.%20We%20develop%20and%20study%20a%20gradient-based%20approach%20for%20optimizing%20this%20convex%20objective%2C%20which%20we%20call%20MixMin%2C%20and%20test%20it%20on%20language%20modeling%20and%20chemistry%20tasks.%20MixMin%20was%20the%20only%20method%20that%20uniformly%20improved%20the%20data%20mixture%20in%20all%20our%20experiments.%20With%20MixMin%2C%20we%20improved%20the%20data%20mixture%20using%20less%20than%200.2%25%20additional%20compute%20for%20a%20pythia-410M%20model%20trained%20on%208.2B%20tokens%2C%20resulting%20between%201-5%25%20relative%20improvement%20to%20negative%20log%20likelihood%20on%20PIQA%2C%20ARC%20Easy%2C%20SciQ%2C%20and%20OpenWebMath.%20Crucially%2C%20we%20found%20that%20MixMin%20mixtures%20for%20smaller%20models%20improved%20training%20of%20larger%20models%2C%20suggesting%20that%20MixMin%20mixtures%20may%20be%20scale-invariant.%20When%20mixing%20bioassay%20data%20to%20train%20an%20XGBoost%20model%2C%20we%20saw%20improvements%20to%20average%20precision%20scores%20of%200.03-0.15.&entry.1838667208=http%3A//arxiv.org/abs/2502.10510v3&entry.124074799=Read"},
{"title": "A Geometric Unification of Generative AI with Manifold-Probabilistic Projection Models", "author": "Leah Bar and Liron Mor Yosef and Shai Zucker and Neta Shoham and Inbar Seroussi and Nir Sochen", "abstract": "Most models of generative AI for images assume that images are inherently low-dimensional objects embedded within a high-dimensional space. Additionally, it is often implicitly assumed that thematic image datasets form smooth or piecewise smooth manifolds. Common approaches overlook the geometric structure and focus solely on probabilistic methods, approximating the probability distribution through universal approximation techniques such as the kernel method. In some generative models the low dimensional nature of the data manifest itself by the introduction of a lower dimensional latent space. Yet, the probability distribution in the latent or the manifold's coordinate space is considered uninteresting and is predefined or considered uniform. In this study, we address the problem of Blind Image Denoising (BID), and to some extent, the problem of generating images from noise by unifying geometric and probabilistic perspectives. We introduce a novel framework that improves upon existing probabilistic approaches by incorporating geometric assumptions that enable the effective use of kernel-based probabilistic methods. Furthermore, the proposed framework extends prior geometric approaches by combining explicit and implicit manifold descriptions through the introduction of a distance function. The resulting framework demystifies diffusion models by interpreting them as a projection mechanism onto the manifold of ``good images''. This interpretation leads to the construction of a new deterministic model, the Manifold-Probabilistic Projection Model (MPPM), which operates in both the representation (pixel) space and the latent space. We demonstrate that the Latent MPPM (LMPPM) outperforms the Latent Diffusion Model (LDM) across various datasets, achieving superior results in terms of image restoration and generation.", "link": "http://arxiv.org/abs/2510.00666v2", "date": "2026-01-15", "relevancy": 2.3933, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6384}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5967}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5839}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Geometric%20Unification%20of%20Generative%20AI%20with%20Manifold-Probabilistic%20Projection%20Models&body=Title%3A%20A%20Geometric%20Unification%20of%20Generative%20AI%20with%20Manifold-Probabilistic%20Projection%20Models%0AAuthor%3A%20Leah%20Bar%20and%20Liron%20Mor%20Yosef%20and%20Shai%20Zucker%20and%20Neta%20Shoham%20and%20Inbar%20Seroussi%20and%20Nir%20Sochen%0AAbstract%3A%20Most%20models%20of%20generative%20AI%20for%20images%20assume%20that%20images%20are%20inherently%20low-dimensional%20objects%20embedded%20within%20a%20high-dimensional%20space.%20Additionally%2C%20it%20is%20often%20implicitly%20assumed%20that%20thematic%20image%20datasets%20form%20smooth%20or%20piecewise%20smooth%20manifolds.%20Common%20approaches%20overlook%20the%20geometric%20structure%20and%20focus%20solely%20on%20probabilistic%20methods%2C%20approximating%20the%20probability%20distribution%20through%20universal%20approximation%20techniques%20such%20as%20the%20kernel%20method.%20In%20some%20generative%20models%20the%20low%20dimensional%20nature%20of%20the%20data%20manifest%20itself%20by%20the%20introduction%20of%20a%20lower%20dimensional%20latent%20space.%20Yet%2C%20the%20probability%20distribution%20in%20the%20latent%20or%20the%20manifold%27s%20coordinate%20space%20is%20considered%20uninteresting%20and%20is%20predefined%20or%20considered%20uniform.%20In%20this%20study%2C%20we%20address%20the%20problem%20of%20Blind%20Image%20Denoising%20%28BID%29%2C%20and%20to%20some%20extent%2C%20the%20problem%20of%20generating%20images%20from%20noise%20by%20unifying%20geometric%20and%20probabilistic%20perspectives.%20We%20introduce%20a%20novel%20framework%20that%20improves%20upon%20existing%20probabilistic%20approaches%20by%20incorporating%20geometric%20assumptions%20that%20enable%20the%20effective%20use%20of%20kernel-based%20probabilistic%20methods.%20Furthermore%2C%20the%20proposed%20framework%20extends%20prior%20geometric%20approaches%20by%20combining%20explicit%20and%20implicit%20manifold%20descriptions%20through%20the%20introduction%20of%20a%20distance%20function.%20The%20resulting%20framework%20demystifies%20diffusion%20models%20by%20interpreting%20them%20as%20a%20projection%20mechanism%20onto%20the%20manifold%20of%20%60%60good%20images%27%27.%20This%20interpretation%20leads%20to%20the%20construction%20of%20a%20new%20deterministic%20model%2C%20the%20Manifold-Probabilistic%20Projection%20Model%20%28MPPM%29%2C%20which%20operates%20in%20both%20the%20representation%20%28pixel%29%20space%20and%20the%20latent%20space.%20We%20demonstrate%20that%20the%20Latent%20MPPM%20%28LMPPM%29%20outperforms%20the%20Latent%20Diffusion%20Model%20%28LDM%29%20across%20various%20datasets%2C%20achieving%20superior%20results%20in%20terms%20of%20image%20restoration%20and%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2510.00666v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Geometric%2520Unification%2520of%2520Generative%2520AI%2520with%2520Manifold-Probabilistic%2520Projection%2520Models%26entry.906535625%3DLeah%2520Bar%2520and%2520Liron%2520Mor%2520Yosef%2520and%2520Shai%2520Zucker%2520and%2520Neta%2520Shoham%2520and%2520Inbar%2520Seroussi%2520and%2520Nir%2520Sochen%26entry.1292438233%3DMost%2520models%2520of%2520generative%2520AI%2520for%2520images%2520assume%2520that%2520images%2520are%2520inherently%2520low-dimensional%2520objects%2520embedded%2520within%2520a%2520high-dimensional%2520space.%2520Additionally%252C%2520it%2520is%2520often%2520implicitly%2520assumed%2520that%2520thematic%2520image%2520datasets%2520form%2520smooth%2520or%2520piecewise%2520smooth%2520manifolds.%2520Common%2520approaches%2520overlook%2520the%2520geometric%2520structure%2520and%2520focus%2520solely%2520on%2520probabilistic%2520methods%252C%2520approximating%2520the%2520probability%2520distribution%2520through%2520universal%2520approximation%2520techniques%2520such%2520as%2520the%2520kernel%2520method.%2520In%2520some%2520generative%2520models%2520the%2520low%2520dimensional%2520nature%2520of%2520the%2520data%2520manifest%2520itself%2520by%2520the%2520introduction%2520of%2520a%2520lower%2520dimensional%2520latent%2520space.%2520Yet%252C%2520the%2520probability%2520distribution%2520in%2520the%2520latent%2520or%2520the%2520manifold%2527s%2520coordinate%2520space%2520is%2520considered%2520uninteresting%2520and%2520is%2520predefined%2520or%2520considered%2520uniform.%2520In%2520this%2520study%252C%2520we%2520address%2520the%2520problem%2520of%2520Blind%2520Image%2520Denoising%2520%2528BID%2529%252C%2520and%2520to%2520some%2520extent%252C%2520the%2520problem%2520of%2520generating%2520images%2520from%2520noise%2520by%2520unifying%2520geometric%2520and%2520probabilistic%2520perspectives.%2520We%2520introduce%2520a%2520novel%2520framework%2520that%2520improves%2520upon%2520existing%2520probabilistic%2520approaches%2520by%2520incorporating%2520geometric%2520assumptions%2520that%2520enable%2520the%2520effective%2520use%2520of%2520kernel-based%2520probabilistic%2520methods.%2520Furthermore%252C%2520the%2520proposed%2520framework%2520extends%2520prior%2520geometric%2520approaches%2520by%2520combining%2520explicit%2520and%2520implicit%2520manifold%2520descriptions%2520through%2520the%2520introduction%2520of%2520a%2520distance%2520function.%2520The%2520resulting%2520framework%2520demystifies%2520diffusion%2520models%2520by%2520interpreting%2520them%2520as%2520a%2520projection%2520mechanism%2520onto%2520the%2520manifold%2520of%2520%2560%2560good%2520images%2527%2527.%2520This%2520interpretation%2520leads%2520to%2520the%2520construction%2520of%2520a%2520new%2520deterministic%2520model%252C%2520the%2520Manifold-Probabilistic%2520Projection%2520Model%2520%2528MPPM%2529%252C%2520which%2520operates%2520in%2520both%2520the%2520representation%2520%2528pixel%2529%2520space%2520and%2520the%2520latent%2520space.%2520We%2520demonstrate%2520that%2520the%2520Latent%2520MPPM%2520%2528LMPPM%2529%2520outperforms%2520the%2520Latent%2520Diffusion%2520Model%2520%2528LDM%2529%2520across%2520various%2520datasets%252C%2520achieving%2520superior%2520results%2520in%2520terms%2520of%2520image%2520restoration%2520and%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.00666v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Geometric%20Unification%20of%20Generative%20AI%20with%20Manifold-Probabilistic%20Projection%20Models&entry.906535625=Leah%20Bar%20and%20Liron%20Mor%20Yosef%20and%20Shai%20Zucker%20and%20Neta%20Shoham%20and%20Inbar%20Seroussi%20and%20Nir%20Sochen&entry.1292438233=Most%20models%20of%20generative%20AI%20for%20images%20assume%20that%20images%20are%20inherently%20low-dimensional%20objects%20embedded%20within%20a%20high-dimensional%20space.%20Additionally%2C%20it%20is%20often%20implicitly%20assumed%20that%20thematic%20image%20datasets%20form%20smooth%20or%20piecewise%20smooth%20manifolds.%20Common%20approaches%20overlook%20the%20geometric%20structure%20and%20focus%20solely%20on%20probabilistic%20methods%2C%20approximating%20the%20probability%20distribution%20through%20universal%20approximation%20techniques%20such%20as%20the%20kernel%20method.%20In%20some%20generative%20models%20the%20low%20dimensional%20nature%20of%20the%20data%20manifest%20itself%20by%20the%20introduction%20of%20a%20lower%20dimensional%20latent%20space.%20Yet%2C%20the%20probability%20distribution%20in%20the%20latent%20or%20the%20manifold%27s%20coordinate%20space%20is%20considered%20uninteresting%20and%20is%20predefined%20or%20considered%20uniform.%20In%20this%20study%2C%20we%20address%20the%20problem%20of%20Blind%20Image%20Denoising%20%28BID%29%2C%20and%20to%20some%20extent%2C%20the%20problem%20of%20generating%20images%20from%20noise%20by%20unifying%20geometric%20and%20probabilistic%20perspectives.%20We%20introduce%20a%20novel%20framework%20that%20improves%20upon%20existing%20probabilistic%20approaches%20by%20incorporating%20geometric%20assumptions%20that%20enable%20the%20effective%20use%20of%20kernel-based%20probabilistic%20methods.%20Furthermore%2C%20the%20proposed%20framework%20extends%20prior%20geometric%20approaches%20by%20combining%20explicit%20and%20implicit%20manifold%20descriptions%20through%20the%20introduction%20of%20a%20distance%20function.%20The%20resulting%20framework%20demystifies%20diffusion%20models%20by%20interpreting%20them%20as%20a%20projection%20mechanism%20onto%20the%20manifold%20of%20%60%60good%20images%27%27.%20This%20interpretation%20leads%20to%20the%20construction%20of%20a%20new%20deterministic%20model%2C%20the%20Manifold-Probabilistic%20Projection%20Model%20%28MPPM%29%2C%20which%20operates%20in%20both%20the%20representation%20%28pixel%29%20space%20and%20the%20latent%20space.%20We%20demonstrate%20that%20the%20Latent%20MPPM%20%28LMPPM%29%20outperforms%20the%20Latent%20Diffusion%20Model%20%28LDM%29%20across%20various%20datasets%2C%20achieving%20superior%20results%20in%20terms%20of%20image%20restoration%20and%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2510.00666v2&entry.124074799=Read"},
{"title": "UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories", "author": "Yanghong Mei and Yirong Yang and Longteng Guo and Qunbo Wang and Ming-Ming Yu and Xingjian He and Wenjun Wu and Jing Liu", "abstract": "Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.", "link": "http://arxiv.org/abs/2512.09607v2", "date": "2026-01-15", "relevancy": 2.3837, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6039}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5957}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.588}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UrbanNav%3A%20Learning%20Language-Guided%20Urban%20Navigation%20from%20Web-Scale%20Human%20Trajectories&body=Title%3A%20UrbanNav%3A%20Learning%20Language-Guided%20Urban%20Navigation%20from%20Web-Scale%20Human%20Trajectories%0AAuthor%3A%20Yanghong%20Mei%20and%20Yirong%20Yang%20and%20Longteng%20Guo%20and%20Qunbo%20Wang%20and%20Ming-Ming%20Yu%20and%20Xingjian%20He%20and%20Wenjun%20Wu%20and%20Jing%20Liu%0AAbstract%3A%20Navigating%20complex%20urban%20environments%20using%20natural%20language%20instructions%20poses%20significant%20challenges%20for%20embodied%20agents%2C%20including%20noisy%20language%20instructions%2C%20ambiguous%20spatial%20references%2C%20diverse%20landmarks%2C%20and%20dynamic%20street%20scenes.%20Current%20visual%20navigation%20methods%20are%20typically%20limited%20to%20simulated%20or%20off-street%20environments%2C%20and%20often%20rely%20on%20precise%20goal%20formats%2C%20such%20as%20specific%20coordinates%20or%20images.%20This%20limits%20their%20effectiveness%20for%20autonomous%20agents%20like%20last-mile%20delivery%20robots%20navigating%20unfamiliar%20cities.%20To%20address%20these%20limitations%2C%20we%20introduce%20UrbanNav%2C%20a%20scalable%20framework%20that%20trains%20embodied%20agents%20to%20follow%20free-form%20language%20instructions%20in%20diverse%20urban%20settings.%20Leveraging%20web-scale%20city%20walking%20videos%2C%20we%20develop%20an%20scalable%20annotation%20pipeline%20that%20aligns%20human%20navigation%20trajectories%20with%20language%20instructions%20grounded%20in%20real-world%20landmarks.%20UrbanNav%20encompasses%20over%201%2C500%20hours%20of%20navigation%20data%20and%203%20million%20instruction-trajectory-landmark%20triplets%2C%20capturing%20a%20wide%20range%20of%20urban%20scenarios.%20Our%20model%20learns%20robust%20navigation%20policies%20to%20tackle%20complex%20urban%20scenarios%2C%20demonstrating%20superior%20spatial%20reasoning%2C%20robustness%20to%20noisy%20instructions%2C%20and%20generalization%20to%20unseen%20urban%20settings.%20Experimental%20results%20show%20that%20UrbanNav%20significantly%20outperforms%20existing%20methods%2C%20highlighting%20the%20potential%20of%20large-scale%20web%20video%20data%20to%20enable%20language-guided%2C%20real-world%20urban%20navigation%20for%20embodied%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2512.09607v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrbanNav%253A%2520Learning%2520Language-Guided%2520Urban%2520Navigation%2520from%2520Web-Scale%2520Human%2520Trajectories%26entry.906535625%3DYanghong%2520Mei%2520and%2520Yirong%2520Yang%2520and%2520Longteng%2520Guo%2520and%2520Qunbo%2520Wang%2520and%2520Ming-Ming%2520Yu%2520and%2520Xingjian%2520He%2520and%2520Wenjun%2520Wu%2520and%2520Jing%2520Liu%26entry.1292438233%3DNavigating%2520complex%2520urban%2520environments%2520using%2520natural%2520language%2520instructions%2520poses%2520significant%2520challenges%2520for%2520embodied%2520agents%252C%2520including%2520noisy%2520language%2520instructions%252C%2520ambiguous%2520spatial%2520references%252C%2520diverse%2520landmarks%252C%2520and%2520dynamic%2520street%2520scenes.%2520Current%2520visual%2520navigation%2520methods%2520are%2520typically%2520limited%2520to%2520simulated%2520or%2520off-street%2520environments%252C%2520and%2520often%2520rely%2520on%2520precise%2520goal%2520formats%252C%2520such%2520as%2520specific%2520coordinates%2520or%2520images.%2520This%2520limits%2520their%2520effectiveness%2520for%2520autonomous%2520agents%2520like%2520last-mile%2520delivery%2520robots%2520navigating%2520unfamiliar%2520cities.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520UrbanNav%252C%2520a%2520scalable%2520framework%2520that%2520trains%2520embodied%2520agents%2520to%2520follow%2520free-form%2520language%2520instructions%2520in%2520diverse%2520urban%2520settings.%2520Leveraging%2520web-scale%2520city%2520walking%2520videos%252C%2520we%2520develop%2520an%2520scalable%2520annotation%2520pipeline%2520that%2520aligns%2520human%2520navigation%2520trajectories%2520with%2520language%2520instructions%2520grounded%2520in%2520real-world%2520landmarks.%2520UrbanNav%2520encompasses%2520over%25201%252C500%2520hours%2520of%2520navigation%2520data%2520and%25203%2520million%2520instruction-trajectory-landmark%2520triplets%252C%2520capturing%2520a%2520wide%2520range%2520of%2520urban%2520scenarios.%2520Our%2520model%2520learns%2520robust%2520navigation%2520policies%2520to%2520tackle%2520complex%2520urban%2520scenarios%252C%2520demonstrating%2520superior%2520spatial%2520reasoning%252C%2520robustness%2520to%2520noisy%2520instructions%252C%2520and%2520generalization%2520to%2520unseen%2520urban%2520settings.%2520Experimental%2520results%2520show%2520that%2520UrbanNav%2520significantly%2520outperforms%2520existing%2520methods%252C%2520highlighting%2520the%2520potential%2520of%2520large-scale%2520web%2520video%2520data%2520to%2520enable%2520language-guided%252C%2520real-world%2520urban%2520navigation%2520for%2520embodied%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.09607v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UrbanNav%3A%20Learning%20Language-Guided%20Urban%20Navigation%20from%20Web-Scale%20Human%20Trajectories&entry.906535625=Yanghong%20Mei%20and%20Yirong%20Yang%20and%20Longteng%20Guo%20and%20Qunbo%20Wang%20and%20Ming-Ming%20Yu%20and%20Xingjian%20He%20and%20Wenjun%20Wu%20and%20Jing%20Liu&entry.1292438233=Navigating%20complex%20urban%20environments%20using%20natural%20language%20instructions%20poses%20significant%20challenges%20for%20embodied%20agents%2C%20including%20noisy%20language%20instructions%2C%20ambiguous%20spatial%20references%2C%20diverse%20landmarks%2C%20and%20dynamic%20street%20scenes.%20Current%20visual%20navigation%20methods%20are%20typically%20limited%20to%20simulated%20or%20off-street%20environments%2C%20and%20often%20rely%20on%20precise%20goal%20formats%2C%20such%20as%20specific%20coordinates%20or%20images.%20This%20limits%20their%20effectiveness%20for%20autonomous%20agents%20like%20last-mile%20delivery%20robots%20navigating%20unfamiliar%20cities.%20To%20address%20these%20limitations%2C%20we%20introduce%20UrbanNav%2C%20a%20scalable%20framework%20that%20trains%20embodied%20agents%20to%20follow%20free-form%20language%20instructions%20in%20diverse%20urban%20settings.%20Leveraging%20web-scale%20city%20walking%20videos%2C%20we%20develop%20an%20scalable%20annotation%20pipeline%20that%20aligns%20human%20navigation%20trajectories%20with%20language%20instructions%20grounded%20in%20real-world%20landmarks.%20UrbanNav%20encompasses%20over%201%2C500%20hours%20of%20navigation%20data%20and%203%20million%20instruction-trajectory-landmark%20triplets%2C%20capturing%20a%20wide%20range%20of%20urban%20scenarios.%20Our%20model%20learns%20robust%20navigation%20policies%20to%20tackle%20complex%20urban%20scenarios%2C%20demonstrating%20superior%20spatial%20reasoning%2C%20robustness%20to%20noisy%20instructions%2C%20and%20generalization%20to%20unseen%20urban%20settings.%20Experimental%20results%20show%20that%20UrbanNav%20significantly%20outperforms%20existing%20methods%2C%20highlighting%20the%20potential%20of%20large-scale%20web%20video%20data%20to%20enable%20language-guided%2C%20real-world%20urban%20navigation%20for%20embodied%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2512.09607v2&entry.124074799=Read"},
{"title": "SERA-H: Beyond Native Sentinel Spatial Limits for High-Resolution Canopy Height Mapping", "author": "Thomas Boudras and Martin Schwartz and Rasmus Fensholt and Martin Brandt and Ibrahim Fayad and Jean-Pierre Wigneron and Gabriel Belouze and Fajwel Fogel and Philippe Ciais", "abstract": "High-resolution mapping of canopy height is essential for forest management and biodiversity monitoring. Although recent studies have led to the advent of deep learning methods using satellite imagery to predict height maps, these approaches often face a trade-off between data accessibility and spatial resolution. To overcome these limitations, we present SERA-H, an end-to-end model combining a super-resolution module (EDSR) and temporal attention encoding (UTAE). Trained under the supervision of high-density LiDAR data (ALS), our model generates 2.5 m resolution height maps from freely available Sentinel-1 and Sentinel-2 (10 m) time series data. Evaluated on an open-source benchmark dataset in France, SERA-H, with a MAE of 2.6 m and a coefficient of determination of 0.82, not only outperforms standard Sentinel-1/2 baselines but also achieves performance comparable to or better than methods relying on commercial very high-resolution imagery (SPOT-6/7, PlanetScope, Maxar). These results demonstrate that combining high-resolution supervision with the spatiotemporal information embedded in time series enables the reconstruction of details beyond the input sensors' native resolution. SERA-H opens the possibility of freely mapping forests with high revisit frequency, achieving accuracy comparable to that of costly commercial imagery.", "link": "http://arxiv.org/abs/2512.18128v2", "date": "2026-01-15", "relevancy": 2.3806, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4864}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4824}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4597}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SERA-H%3A%20Beyond%20Native%20Sentinel%20Spatial%20Limits%20for%20High-Resolution%20Canopy%20Height%20Mapping&body=Title%3A%20SERA-H%3A%20Beyond%20Native%20Sentinel%20Spatial%20Limits%20for%20High-Resolution%20Canopy%20Height%20Mapping%0AAuthor%3A%20Thomas%20Boudras%20and%20Martin%20Schwartz%20and%20Rasmus%20Fensholt%20and%20Martin%20Brandt%20and%20Ibrahim%20Fayad%20and%20Jean-Pierre%20Wigneron%20and%20Gabriel%20Belouze%20and%20Fajwel%20Fogel%20and%20Philippe%20Ciais%0AAbstract%3A%20High-resolution%20mapping%20of%20canopy%20height%20is%20essential%20for%20forest%20management%20and%20biodiversity%20monitoring.%20Although%20recent%20studies%20have%20led%20to%20the%20advent%20of%20deep%20learning%20methods%20using%20satellite%20imagery%20to%20predict%20height%20maps%2C%20these%20approaches%20often%20face%20a%20trade-off%20between%20data%20accessibility%20and%20spatial%20resolution.%20To%20overcome%20these%20limitations%2C%20we%20present%20SERA-H%2C%20an%20end-to-end%20model%20combining%20a%20super-resolution%20module%20%28EDSR%29%20and%20temporal%20attention%20encoding%20%28UTAE%29.%20Trained%20under%20the%20supervision%20of%20high-density%20LiDAR%20data%20%28ALS%29%2C%20our%20model%20generates%202.5%20m%20resolution%20height%20maps%20from%20freely%20available%20Sentinel-1%20and%20Sentinel-2%20%2810%20m%29%20time%20series%20data.%20Evaluated%20on%20an%20open-source%20benchmark%20dataset%20in%20France%2C%20SERA-H%2C%20with%20a%20MAE%20of%202.6%20m%20and%20a%20coefficient%20of%20determination%20of%200.82%2C%20not%20only%20outperforms%20standard%20Sentinel-1/2%20baselines%20but%20also%20achieves%20performance%20comparable%20to%20or%20better%20than%20methods%20relying%20on%20commercial%20very%20high-resolution%20imagery%20%28SPOT-6/7%2C%20PlanetScope%2C%20Maxar%29.%20These%20results%20demonstrate%20that%20combining%20high-resolution%20supervision%20with%20the%20spatiotemporal%20information%20embedded%20in%20time%20series%20enables%20the%20reconstruction%20of%20details%20beyond%20the%20input%20sensors%27%20native%20resolution.%20SERA-H%20opens%20the%20possibility%20of%20freely%20mapping%20forests%20with%20high%20revisit%20frequency%2C%20achieving%20accuracy%20comparable%20to%20that%20of%20costly%20commercial%20imagery.%0ALink%3A%20http%3A//arxiv.org/abs/2512.18128v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSERA-H%253A%2520Beyond%2520Native%2520Sentinel%2520Spatial%2520Limits%2520for%2520High-Resolution%2520Canopy%2520Height%2520Mapping%26entry.906535625%3DThomas%2520Boudras%2520and%2520Martin%2520Schwartz%2520and%2520Rasmus%2520Fensholt%2520and%2520Martin%2520Brandt%2520and%2520Ibrahim%2520Fayad%2520and%2520Jean-Pierre%2520Wigneron%2520and%2520Gabriel%2520Belouze%2520and%2520Fajwel%2520Fogel%2520and%2520Philippe%2520Ciais%26entry.1292438233%3DHigh-resolution%2520mapping%2520of%2520canopy%2520height%2520is%2520essential%2520for%2520forest%2520management%2520and%2520biodiversity%2520monitoring.%2520Although%2520recent%2520studies%2520have%2520led%2520to%2520the%2520advent%2520of%2520deep%2520learning%2520methods%2520using%2520satellite%2520imagery%2520to%2520predict%2520height%2520maps%252C%2520these%2520approaches%2520often%2520face%2520a%2520trade-off%2520between%2520data%2520accessibility%2520and%2520spatial%2520resolution.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520present%2520SERA-H%252C%2520an%2520end-to-end%2520model%2520combining%2520a%2520super-resolution%2520module%2520%2528EDSR%2529%2520and%2520temporal%2520attention%2520encoding%2520%2528UTAE%2529.%2520Trained%2520under%2520the%2520supervision%2520of%2520high-density%2520LiDAR%2520data%2520%2528ALS%2529%252C%2520our%2520model%2520generates%25202.5%2520m%2520resolution%2520height%2520maps%2520from%2520freely%2520available%2520Sentinel-1%2520and%2520Sentinel-2%2520%252810%2520m%2529%2520time%2520series%2520data.%2520Evaluated%2520on%2520an%2520open-source%2520benchmark%2520dataset%2520in%2520France%252C%2520SERA-H%252C%2520with%2520a%2520MAE%2520of%25202.6%2520m%2520and%2520a%2520coefficient%2520of%2520determination%2520of%25200.82%252C%2520not%2520only%2520outperforms%2520standard%2520Sentinel-1/2%2520baselines%2520but%2520also%2520achieves%2520performance%2520comparable%2520to%2520or%2520better%2520than%2520methods%2520relying%2520on%2520commercial%2520very%2520high-resolution%2520imagery%2520%2528SPOT-6/7%252C%2520PlanetScope%252C%2520Maxar%2529.%2520These%2520results%2520demonstrate%2520that%2520combining%2520high-resolution%2520supervision%2520with%2520the%2520spatiotemporal%2520information%2520embedded%2520in%2520time%2520series%2520enables%2520the%2520reconstruction%2520of%2520details%2520beyond%2520the%2520input%2520sensors%2527%2520native%2520resolution.%2520SERA-H%2520opens%2520the%2520possibility%2520of%2520freely%2520mapping%2520forests%2520with%2520high%2520revisit%2520frequency%252C%2520achieving%2520accuracy%2520comparable%2520to%2520that%2520of%2520costly%2520commercial%2520imagery.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.18128v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SERA-H%3A%20Beyond%20Native%20Sentinel%20Spatial%20Limits%20for%20High-Resolution%20Canopy%20Height%20Mapping&entry.906535625=Thomas%20Boudras%20and%20Martin%20Schwartz%20and%20Rasmus%20Fensholt%20and%20Martin%20Brandt%20and%20Ibrahim%20Fayad%20and%20Jean-Pierre%20Wigneron%20and%20Gabriel%20Belouze%20and%20Fajwel%20Fogel%20and%20Philippe%20Ciais&entry.1292438233=High-resolution%20mapping%20of%20canopy%20height%20is%20essential%20for%20forest%20management%20and%20biodiversity%20monitoring.%20Although%20recent%20studies%20have%20led%20to%20the%20advent%20of%20deep%20learning%20methods%20using%20satellite%20imagery%20to%20predict%20height%20maps%2C%20these%20approaches%20often%20face%20a%20trade-off%20between%20data%20accessibility%20and%20spatial%20resolution.%20To%20overcome%20these%20limitations%2C%20we%20present%20SERA-H%2C%20an%20end-to-end%20model%20combining%20a%20super-resolution%20module%20%28EDSR%29%20and%20temporal%20attention%20encoding%20%28UTAE%29.%20Trained%20under%20the%20supervision%20of%20high-density%20LiDAR%20data%20%28ALS%29%2C%20our%20model%20generates%202.5%20m%20resolution%20height%20maps%20from%20freely%20available%20Sentinel-1%20and%20Sentinel-2%20%2810%20m%29%20time%20series%20data.%20Evaluated%20on%20an%20open-source%20benchmark%20dataset%20in%20France%2C%20SERA-H%2C%20with%20a%20MAE%20of%202.6%20m%20and%20a%20coefficient%20of%20determination%20of%200.82%2C%20not%20only%20outperforms%20standard%20Sentinel-1/2%20baselines%20but%20also%20achieves%20performance%20comparable%20to%20or%20better%20than%20methods%20relying%20on%20commercial%20very%20high-resolution%20imagery%20%28SPOT-6/7%2C%20PlanetScope%2C%20Maxar%29.%20These%20results%20demonstrate%20that%20combining%20high-resolution%20supervision%20with%20the%20spatiotemporal%20information%20embedded%20in%20time%20series%20enables%20the%20reconstruction%20of%20details%20beyond%20the%20input%20sensors%27%20native%20resolution.%20SERA-H%20opens%20the%20possibility%20of%20freely%20mapping%20forests%20with%20high%20revisit%20frequency%2C%20achieving%20accuracy%20comparable%20to%20that%20of%20costly%20commercial%20imagery.&entry.1838667208=http%3A//arxiv.org/abs/2512.18128v2&entry.124074799=Read"},
{"title": "X-SAM: Boosting Sharpness-Aware Minimization with Dominant-Eigenvector Gradient Correction", "author": "Hongru Duan and Yongle Chen and Lei Guan", "abstract": "Sharpness-Aware Minimization (SAM) aims to improve generalization by minimizing a worst-case perturbed loss over a small neighborhood of model parameters. However, during training, its optimization behavior does not always align with theoretical expectations, since both sharp and flat regions may yield a small perturbed loss. In such cases, the gradient may still point toward sharp regions, failing to achieve the intended effect of SAM. To address this issue, we investigate SAM from a spectral and geometric perspective: specifically, we utilize the angle between the gradient and the leading eigenvector of the Hessian as a measure of sharpness. Our analysis illustrates that when this angle is less than or equal to ninety degrees, the effect of SAM's sharpness regularization can be weakened. Furthermore, we propose an explicit eigenvector-aligned SAM (X-SAM), which corrects the gradient via orthogonal decomposition along the top eigenvector, enabling more direct and efficient regularization of the Hessian's maximum eigenvalue. We prove X-SAM's convergence and superior generalization, with extensive experimental evaluations confirming both theoretical and practical advantages.", "link": "http://arxiv.org/abs/2601.10251v1", "date": "2026-01-15", "relevancy": 2.3772, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.492}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4785}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20X-SAM%3A%20Boosting%20Sharpness-Aware%20Minimization%20with%20Dominant-Eigenvector%20Gradient%20Correction&body=Title%3A%20X-SAM%3A%20Boosting%20Sharpness-Aware%20Minimization%20with%20Dominant-Eigenvector%20Gradient%20Correction%0AAuthor%3A%20Hongru%20Duan%20and%20Yongle%20Chen%20and%20Lei%20Guan%0AAbstract%3A%20Sharpness-Aware%20Minimization%20%28SAM%29%20aims%20to%20improve%20generalization%20by%20minimizing%20a%20worst-case%20perturbed%20loss%20over%20a%20small%20neighborhood%20of%20model%20parameters.%20However%2C%20during%20training%2C%20its%20optimization%20behavior%20does%20not%20always%20align%20with%20theoretical%20expectations%2C%20since%20both%20sharp%20and%20flat%20regions%20may%20yield%20a%20small%20perturbed%20loss.%20In%20such%20cases%2C%20the%20gradient%20may%20still%20point%20toward%20sharp%20regions%2C%20failing%20to%20achieve%20the%20intended%20effect%20of%20SAM.%20To%20address%20this%20issue%2C%20we%20investigate%20SAM%20from%20a%20spectral%20and%20geometric%20perspective%3A%20specifically%2C%20we%20utilize%20the%20angle%20between%20the%20gradient%20and%20the%20leading%20eigenvector%20of%20the%20Hessian%20as%20a%20measure%20of%20sharpness.%20Our%20analysis%20illustrates%20that%20when%20this%20angle%20is%20less%20than%20or%20equal%20to%20ninety%20degrees%2C%20the%20effect%20of%20SAM%27s%20sharpness%20regularization%20can%20be%20weakened.%20Furthermore%2C%20we%20propose%20an%20explicit%20eigenvector-aligned%20SAM%20%28X-SAM%29%2C%20which%20corrects%20the%20gradient%20via%20orthogonal%20decomposition%20along%20the%20top%20eigenvector%2C%20enabling%20more%20direct%20and%20efficient%20regularization%20of%20the%20Hessian%27s%20maximum%20eigenvalue.%20We%20prove%20X-SAM%27s%20convergence%20and%20superior%20generalization%2C%20with%20extensive%20experimental%20evaluations%20confirming%20both%20theoretical%20and%20practical%20advantages.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DX-SAM%253A%2520Boosting%2520Sharpness-Aware%2520Minimization%2520with%2520Dominant-Eigenvector%2520Gradient%2520Correction%26entry.906535625%3DHongru%2520Duan%2520and%2520Yongle%2520Chen%2520and%2520Lei%2520Guan%26entry.1292438233%3DSharpness-Aware%2520Minimization%2520%2528SAM%2529%2520aims%2520to%2520improve%2520generalization%2520by%2520minimizing%2520a%2520worst-case%2520perturbed%2520loss%2520over%2520a%2520small%2520neighborhood%2520of%2520model%2520parameters.%2520However%252C%2520during%2520training%252C%2520its%2520optimization%2520behavior%2520does%2520not%2520always%2520align%2520with%2520theoretical%2520expectations%252C%2520since%2520both%2520sharp%2520and%2520flat%2520regions%2520may%2520yield%2520a%2520small%2520perturbed%2520loss.%2520In%2520such%2520cases%252C%2520the%2520gradient%2520may%2520still%2520point%2520toward%2520sharp%2520regions%252C%2520failing%2520to%2520achieve%2520the%2520intended%2520effect%2520of%2520SAM.%2520To%2520address%2520this%2520issue%252C%2520we%2520investigate%2520SAM%2520from%2520a%2520spectral%2520and%2520geometric%2520perspective%253A%2520specifically%252C%2520we%2520utilize%2520the%2520angle%2520between%2520the%2520gradient%2520and%2520the%2520leading%2520eigenvector%2520of%2520the%2520Hessian%2520as%2520a%2520measure%2520of%2520sharpness.%2520Our%2520analysis%2520illustrates%2520that%2520when%2520this%2520angle%2520is%2520less%2520than%2520or%2520equal%2520to%2520ninety%2520degrees%252C%2520the%2520effect%2520of%2520SAM%2527s%2520sharpness%2520regularization%2520can%2520be%2520weakened.%2520Furthermore%252C%2520we%2520propose%2520an%2520explicit%2520eigenvector-aligned%2520SAM%2520%2528X-SAM%2529%252C%2520which%2520corrects%2520the%2520gradient%2520via%2520orthogonal%2520decomposition%2520along%2520the%2520top%2520eigenvector%252C%2520enabling%2520more%2520direct%2520and%2520efficient%2520regularization%2520of%2520the%2520Hessian%2527s%2520maximum%2520eigenvalue.%2520We%2520prove%2520X-SAM%2527s%2520convergence%2520and%2520superior%2520generalization%252C%2520with%2520extensive%2520experimental%2520evaluations%2520confirming%2520both%2520theoretical%2520and%2520practical%2520advantages.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=X-SAM%3A%20Boosting%20Sharpness-Aware%20Minimization%20with%20Dominant-Eigenvector%20Gradient%20Correction&entry.906535625=Hongru%20Duan%20and%20Yongle%20Chen%20and%20Lei%20Guan&entry.1292438233=Sharpness-Aware%20Minimization%20%28SAM%29%20aims%20to%20improve%20generalization%20by%20minimizing%20a%20worst-case%20perturbed%20loss%20over%20a%20small%20neighborhood%20of%20model%20parameters.%20However%2C%20during%20training%2C%20its%20optimization%20behavior%20does%20not%20always%20align%20with%20theoretical%20expectations%2C%20since%20both%20sharp%20and%20flat%20regions%20may%20yield%20a%20small%20perturbed%20loss.%20In%20such%20cases%2C%20the%20gradient%20may%20still%20point%20toward%20sharp%20regions%2C%20failing%20to%20achieve%20the%20intended%20effect%20of%20SAM.%20To%20address%20this%20issue%2C%20we%20investigate%20SAM%20from%20a%20spectral%20and%20geometric%20perspective%3A%20specifically%2C%20we%20utilize%20the%20angle%20between%20the%20gradient%20and%20the%20leading%20eigenvector%20of%20the%20Hessian%20as%20a%20measure%20of%20sharpness.%20Our%20analysis%20illustrates%20that%20when%20this%20angle%20is%20less%20than%20or%20equal%20to%20ninety%20degrees%2C%20the%20effect%20of%20SAM%27s%20sharpness%20regularization%20can%20be%20weakened.%20Furthermore%2C%20we%20propose%20an%20explicit%20eigenvector-aligned%20SAM%20%28X-SAM%29%2C%20which%20corrects%20the%20gradient%20via%20orthogonal%20decomposition%20along%20the%20top%20eigenvector%2C%20enabling%20more%20direct%20and%20efficient%20regularization%20of%20the%20Hessian%27s%20maximum%20eigenvalue.%20We%20prove%20X-SAM%27s%20convergence%20and%20superior%20generalization%2C%20with%20extensive%20experimental%20evaluations%20confirming%20both%20theoretical%20and%20practical%20advantages.&entry.1838667208=http%3A//arxiv.org/abs/2601.10251v1&entry.124074799=Read"},
{"title": "LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals", "author": "Gilat Toker and Nitay Calderon and Ohad Amosy and Roi Reichart", "abstract": "Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.", "link": "http://arxiv.org/abs/2601.10700v1", "date": "2026-01-15", "relevancy": 2.376, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4795}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4795}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LIBERTy%3A%20A%20Causal%20Framework%20for%20Benchmarking%20Concept-Based%20Explanations%20of%20LLMs%20with%20Structural%20Counterfactuals&body=Title%3A%20LIBERTy%3A%20A%20Causal%20Framework%20for%20Benchmarking%20Concept-Based%20Explanations%20of%20LLMs%20with%20Structural%20Counterfactuals%0AAuthor%3A%20Gilat%20Toker%20and%20Nitay%20Calderon%20and%20Ohad%20Amosy%20and%20Roi%20Reichart%0AAbstract%3A%20Concept-based%20explanations%20quantify%20how%20high-level%20concepts%20%28e.g.%2C%20gender%20or%20experience%29%20influence%20model%20behavior%2C%20which%20is%20crucial%20for%20decision-makers%20in%20high-stakes%20domains.%20Recent%20work%20evaluates%20the%20faithfulness%20of%20such%20explanations%20by%20comparing%20them%20to%20reference%20causal%20effects%20estimated%20from%20counterfactuals.%20In%20practice%2C%20existing%20benchmarks%20rely%20on%20costly%20human-written%20counterfactuals%20that%20serve%20as%20an%20imperfect%20proxy.%20To%20address%20this%2C%20we%20introduce%20a%20framework%20for%20constructing%20datasets%20containing%20structural%20counterfactual%20pairs%3A%20LIBERTy%20%28LLM-based%20Interventional%20Benchmark%20for%20Explainability%20with%20Reference%20Targets%29.%20LIBERTy%20is%20grounded%20in%20explicitly%20defined%20Structured%20Causal%20Models%20%28SCMs%29%20of%20the%20text%20generation%2C%20interventions%20on%20a%20concept%20propagate%20through%20the%20SCM%20until%20an%20LLM%20generates%20the%20counterfactual.%20We%20introduce%20three%20datasets%20%28disease%20detection%2C%20CV%20screening%2C%20and%20workplace%20violence%20prediction%29%20together%20with%20a%20new%20evaluation%20metric%2C%20order-faithfulness.%20Using%20them%2C%20we%20evaluate%20a%20wide%20range%20of%20methods%20across%20five%20models%20and%20identify%20substantial%20headroom%20for%20improving%20concept-based%20explanations.%20LIBERTy%20also%20enables%20systematic%20analysis%20of%20model%20sensitivity%20to%20interventions%3A%20we%20find%20that%20proprietary%20LLMs%20show%20markedly%20reduced%20sensitivity%20to%20demographic%20concepts%2C%20likely%20due%20to%20post-training%20mitigation.%20Overall%2C%20LIBERTy%20provides%20a%20much-needed%20benchmark%20for%20developing%20faithful%20explainability%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLIBERTy%253A%2520A%2520Causal%2520Framework%2520for%2520Benchmarking%2520Concept-Based%2520Explanations%2520of%2520LLMs%2520with%2520Structural%2520Counterfactuals%26entry.906535625%3DGilat%2520Toker%2520and%2520Nitay%2520Calderon%2520and%2520Ohad%2520Amosy%2520and%2520Roi%2520Reichart%26entry.1292438233%3DConcept-based%2520explanations%2520quantify%2520how%2520high-level%2520concepts%2520%2528e.g.%252C%2520gender%2520or%2520experience%2529%2520influence%2520model%2520behavior%252C%2520which%2520is%2520crucial%2520for%2520decision-makers%2520in%2520high-stakes%2520domains.%2520Recent%2520work%2520evaluates%2520the%2520faithfulness%2520of%2520such%2520explanations%2520by%2520comparing%2520them%2520to%2520reference%2520causal%2520effects%2520estimated%2520from%2520counterfactuals.%2520In%2520practice%252C%2520existing%2520benchmarks%2520rely%2520on%2520costly%2520human-written%2520counterfactuals%2520that%2520serve%2520as%2520an%2520imperfect%2520proxy.%2520To%2520address%2520this%252C%2520we%2520introduce%2520a%2520framework%2520for%2520constructing%2520datasets%2520containing%2520structural%2520counterfactual%2520pairs%253A%2520LIBERTy%2520%2528LLM-based%2520Interventional%2520Benchmark%2520for%2520Explainability%2520with%2520Reference%2520Targets%2529.%2520LIBERTy%2520is%2520grounded%2520in%2520explicitly%2520defined%2520Structured%2520Causal%2520Models%2520%2528SCMs%2529%2520of%2520the%2520text%2520generation%252C%2520interventions%2520on%2520a%2520concept%2520propagate%2520through%2520the%2520SCM%2520until%2520an%2520LLM%2520generates%2520the%2520counterfactual.%2520We%2520introduce%2520three%2520datasets%2520%2528disease%2520detection%252C%2520CV%2520screening%252C%2520and%2520workplace%2520violence%2520prediction%2529%2520together%2520with%2520a%2520new%2520evaluation%2520metric%252C%2520order-faithfulness.%2520Using%2520them%252C%2520we%2520evaluate%2520a%2520wide%2520range%2520of%2520methods%2520across%2520five%2520models%2520and%2520identify%2520substantial%2520headroom%2520for%2520improving%2520concept-based%2520explanations.%2520LIBERTy%2520also%2520enables%2520systematic%2520analysis%2520of%2520model%2520sensitivity%2520to%2520interventions%253A%2520we%2520find%2520that%2520proprietary%2520LLMs%2520show%2520markedly%2520reduced%2520sensitivity%2520to%2520demographic%2520concepts%252C%2520likely%2520due%2520to%2520post-training%2520mitigation.%2520Overall%252C%2520LIBERTy%2520provides%2520a%2520much-needed%2520benchmark%2520for%2520developing%2520faithful%2520explainability%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LIBERTy%3A%20A%20Causal%20Framework%20for%20Benchmarking%20Concept-Based%20Explanations%20of%20LLMs%20with%20Structural%20Counterfactuals&entry.906535625=Gilat%20Toker%20and%20Nitay%20Calderon%20and%20Ohad%20Amosy%20and%20Roi%20Reichart&entry.1292438233=Concept-based%20explanations%20quantify%20how%20high-level%20concepts%20%28e.g.%2C%20gender%20or%20experience%29%20influence%20model%20behavior%2C%20which%20is%20crucial%20for%20decision-makers%20in%20high-stakes%20domains.%20Recent%20work%20evaluates%20the%20faithfulness%20of%20such%20explanations%20by%20comparing%20them%20to%20reference%20causal%20effects%20estimated%20from%20counterfactuals.%20In%20practice%2C%20existing%20benchmarks%20rely%20on%20costly%20human-written%20counterfactuals%20that%20serve%20as%20an%20imperfect%20proxy.%20To%20address%20this%2C%20we%20introduce%20a%20framework%20for%20constructing%20datasets%20containing%20structural%20counterfactual%20pairs%3A%20LIBERTy%20%28LLM-based%20Interventional%20Benchmark%20for%20Explainability%20with%20Reference%20Targets%29.%20LIBERTy%20is%20grounded%20in%20explicitly%20defined%20Structured%20Causal%20Models%20%28SCMs%29%20of%20the%20text%20generation%2C%20interventions%20on%20a%20concept%20propagate%20through%20the%20SCM%20until%20an%20LLM%20generates%20the%20counterfactual.%20We%20introduce%20three%20datasets%20%28disease%20detection%2C%20CV%20screening%2C%20and%20workplace%20violence%20prediction%29%20together%20with%20a%20new%20evaluation%20metric%2C%20order-faithfulness.%20Using%20them%2C%20we%20evaluate%20a%20wide%20range%20of%20methods%20across%20five%20models%20and%20identify%20substantial%20headroom%20for%20improving%20concept-based%20explanations.%20LIBERTy%20also%20enables%20systematic%20analysis%20of%20model%20sensitivity%20to%20interventions%3A%20we%20find%20that%20proprietary%20LLMs%20show%20markedly%20reduced%20sensitivity%20to%20demographic%20concepts%2C%20likely%20due%20to%20post-training%20mitigation.%20Overall%2C%20LIBERTy%20provides%20a%20much-needed%20benchmark%20for%20developing%20faithful%20explainability%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.10700v1&entry.124074799=Read"},
{"title": "Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning", "author": "Ting-Hao 'Kenneth' Huang and Ryan A. Rossi and Sungchul Kim and Tong Yu and Ting-Yao E. Hsu and Ho Yin and  Ng and C. Lee Giles", "abstract": "Between 2021 and 2025, the SciCap project grew from a small seed-funded idea at The Pennsylvania State University (Penn State) into one of the central efforts shaping the scientific figure-captioning landscape. Supported by a Penn State seed grant, Adobe, and the Alfred P. Sloan Foundation, what began as our attempt to test whether domain-specific training, which was successful in text models like SciBERT, could also work for figure captions expanded into a multi-institution collaboration. Over these five years, we curated, released, and continually updated a large collection of figure-caption pairs from arXiv papers, conducted extensive automatic and human evaluations on both generated and author-written captions, navigated the rapid rise of large language models (LLMs), launched annual challenges, and built interactive systems that help scientists write better captions. In this piece, we look back at the first five years of SciCap and summarize the key technical and methodological lessons we learned. We then outline five major unsolved challenges and propose directions for the next phase of research in scientific figure captioning.", "link": "http://arxiv.org/abs/2512.21789v2", "date": "2026-01-15", "relevancy": 2.3741, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4752}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4752}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4742}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Five%20Years%20of%20SciCap%3A%20What%20We%20Learned%20and%20Future%20Directions%20for%20Scientific%20Figure%20Captioning&body=Title%3A%20Five%20Years%20of%20SciCap%3A%20What%20We%20Learned%20and%20Future%20Directions%20for%20Scientific%20Figure%20Captioning%0AAuthor%3A%20Ting-Hao%20%27Kenneth%27%20Huang%20and%20Ryan%20A.%20Rossi%20and%20Sungchul%20Kim%20and%20Tong%20Yu%20and%20Ting-Yao%20E.%20Hsu%20and%20Ho%20Yin%20and%20%20Ng%20and%20C.%20Lee%20Giles%0AAbstract%3A%20Between%202021%20and%202025%2C%20the%20SciCap%20project%20grew%20from%20a%20small%20seed-funded%20idea%20at%20The%20Pennsylvania%20State%20University%20%28Penn%20State%29%20into%20one%20of%20the%20central%20efforts%20shaping%20the%20scientific%20figure-captioning%20landscape.%20Supported%20by%20a%20Penn%20State%20seed%20grant%2C%20Adobe%2C%20and%20the%20Alfred%20P.%20Sloan%20Foundation%2C%20what%20began%20as%20our%20attempt%20to%20test%20whether%20domain-specific%20training%2C%20which%20was%20successful%20in%20text%20models%20like%20SciBERT%2C%20could%20also%20work%20for%20figure%20captions%20expanded%20into%20a%20multi-institution%20collaboration.%20Over%20these%20five%20years%2C%20we%20curated%2C%20released%2C%20and%20continually%20updated%20a%20large%20collection%20of%20figure-caption%20pairs%20from%20arXiv%20papers%2C%20conducted%20extensive%20automatic%20and%20human%20evaluations%20on%20both%20generated%20and%20author-written%20captions%2C%20navigated%20the%20rapid%20rise%20of%20large%20language%20models%20%28LLMs%29%2C%20launched%20annual%20challenges%2C%20and%20built%20interactive%20systems%20that%20help%20scientists%20write%20better%20captions.%20In%20this%20piece%2C%20we%20look%20back%20at%20the%20first%20five%20years%20of%20SciCap%20and%20summarize%20the%20key%20technical%20and%20methodological%20lessons%20we%20learned.%20We%20then%20outline%20five%20major%20unsolved%20challenges%20and%20propose%20directions%20for%20the%20next%20phase%20of%20research%20in%20scientific%20figure%20captioning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.21789v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFive%2520Years%2520of%2520SciCap%253A%2520What%2520We%2520Learned%2520and%2520Future%2520Directions%2520for%2520Scientific%2520Figure%2520Captioning%26entry.906535625%3DTing-Hao%2520%2527Kenneth%2527%2520Huang%2520and%2520Ryan%2520A.%2520Rossi%2520and%2520Sungchul%2520Kim%2520and%2520Tong%2520Yu%2520and%2520Ting-Yao%2520E.%2520Hsu%2520and%2520Ho%2520Yin%2520and%2520%2520Ng%2520and%2520C.%2520Lee%2520Giles%26entry.1292438233%3DBetween%25202021%2520and%25202025%252C%2520the%2520SciCap%2520project%2520grew%2520from%2520a%2520small%2520seed-funded%2520idea%2520at%2520The%2520Pennsylvania%2520State%2520University%2520%2528Penn%2520State%2529%2520into%2520one%2520of%2520the%2520central%2520efforts%2520shaping%2520the%2520scientific%2520figure-captioning%2520landscape.%2520Supported%2520by%2520a%2520Penn%2520State%2520seed%2520grant%252C%2520Adobe%252C%2520and%2520the%2520Alfred%2520P.%2520Sloan%2520Foundation%252C%2520what%2520began%2520as%2520our%2520attempt%2520to%2520test%2520whether%2520domain-specific%2520training%252C%2520which%2520was%2520successful%2520in%2520text%2520models%2520like%2520SciBERT%252C%2520could%2520also%2520work%2520for%2520figure%2520captions%2520expanded%2520into%2520a%2520multi-institution%2520collaboration.%2520Over%2520these%2520five%2520years%252C%2520we%2520curated%252C%2520released%252C%2520and%2520continually%2520updated%2520a%2520large%2520collection%2520of%2520figure-caption%2520pairs%2520from%2520arXiv%2520papers%252C%2520conducted%2520extensive%2520automatic%2520and%2520human%2520evaluations%2520on%2520both%2520generated%2520and%2520author-written%2520captions%252C%2520navigated%2520the%2520rapid%2520rise%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520launched%2520annual%2520challenges%252C%2520and%2520built%2520interactive%2520systems%2520that%2520help%2520scientists%2520write%2520better%2520captions.%2520In%2520this%2520piece%252C%2520we%2520look%2520back%2520at%2520the%2520first%2520five%2520years%2520of%2520SciCap%2520and%2520summarize%2520the%2520key%2520technical%2520and%2520methodological%2520lessons%2520we%2520learned.%2520We%2520then%2520outline%2520five%2520major%2520unsolved%2520challenges%2520and%2520propose%2520directions%2520for%2520the%2520next%2520phase%2520of%2520research%2520in%2520scientific%2520figure%2520captioning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.21789v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Five%20Years%20of%20SciCap%3A%20What%20We%20Learned%20and%20Future%20Directions%20for%20Scientific%20Figure%20Captioning&entry.906535625=Ting-Hao%20%27Kenneth%27%20Huang%20and%20Ryan%20A.%20Rossi%20and%20Sungchul%20Kim%20and%20Tong%20Yu%20and%20Ting-Yao%20E.%20Hsu%20and%20Ho%20Yin%20and%20%20Ng%20and%20C.%20Lee%20Giles&entry.1292438233=Between%202021%20and%202025%2C%20the%20SciCap%20project%20grew%20from%20a%20small%20seed-funded%20idea%20at%20The%20Pennsylvania%20State%20University%20%28Penn%20State%29%20into%20one%20of%20the%20central%20efforts%20shaping%20the%20scientific%20figure-captioning%20landscape.%20Supported%20by%20a%20Penn%20State%20seed%20grant%2C%20Adobe%2C%20and%20the%20Alfred%20P.%20Sloan%20Foundation%2C%20what%20began%20as%20our%20attempt%20to%20test%20whether%20domain-specific%20training%2C%20which%20was%20successful%20in%20text%20models%20like%20SciBERT%2C%20could%20also%20work%20for%20figure%20captions%20expanded%20into%20a%20multi-institution%20collaboration.%20Over%20these%20five%20years%2C%20we%20curated%2C%20released%2C%20and%20continually%20updated%20a%20large%20collection%20of%20figure-caption%20pairs%20from%20arXiv%20papers%2C%20conducted%20extensive%20automatic%20and%20human%20evaluations%20on%20both%20generated%20and%20author-written%20captions%2C%20navigated%20the%20rapid%20rise%20of%20large%20language%20models%20%28LLMs%29%2C%20launched%20annual%20challenges%2C%20and%20built%20interactive%20systems%20that%20help%20scientists%20write%20better%20captions.%20In%20this%20piece%2C%20we%20look%20back%20at%20the%20first%20five%20years%20of%20SciCap%20and%20summarize%20the%20key%20technical%20and%20methodological%20lessons%20we%20learned.%20We%20then%20outline%20five%20major%20unsolved%20challenges%20and%20propose%20directions%20for%20the%20next%20phase%20of%20research%20in%20scientific%20figure%20captioning.&entry.1838667208=http%3A//arxiv.org/abs/2512.21789v2&entry.124074799=Read"},
{"title": "Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs", "author": "Ali Al-Kaswan and Claudio Spiess and Prem Devanbu and Arie van Deursen and Maliheh Izadi", "abstract": "Large language models are increasingly used for code generation and debugging, but their outputs can still contain bugs, that originate from training data. Distinguishing whether an LLM prefers correct code, or a familiar incorrect version might be influenced by what it's been exposed to during training. We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model's preference. Using the ManySStuBs4J benchmark, we apply Data Portraits for membership testing on the Stack-V2 corpus to estimate whether each buggy and fixed variant was seen during training. We then stratify examples by exposure and compare model preference using code completion as well as multiple likelihood-based scoring metrics We find that most examples (67%) have neither variant in the training data, and when only one is present, fixes are more frequently present than bugs. In model generations, models reproduce buggy lines far more often than fixes, with bug-exposed examples amplifying this tendency and fix-exposed examples showing only marginal improvement. In likelihood scoring, minimum and maximum token-probability metrics consistently prefer the fixed code across all conditions, indicating a stable bias toward correct fixes. In contrast, metrics like the Gini coefficient reverse preference when only the buggy variant was seen. Our results indicate that exposure can skew bug-fix evaluations and highlight the risk that LLMs may propagate memorised errors in practice.", "link": "http://arxiv.org/abs/2601.10496v1", "date": "2026-01-15", "relevancy": 2.3731, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.471}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.471}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20See%2C%20Model%20Do%3F%20Exposure-Aware%20Evaluation%20of%20Bug-vs-Fix%20Preference%20in%20Code%20LLMs&body=Title%3A%20Model%20See%2C%20Model%20Do%3F%20Exposure-Aware%20Evaluation%20of%20Bug-vs-Fix%20Preference%20in%20Code%20LLMs%0AAuthor%3A%20Ali%20Al-Kaswan%20and%20Claudio%20Spiess%20and%20Prem%20Devanbu%20and%20Arie%20van%20Deursen%20and%20Maliheh%20Izadi%0AAbstract%3A%20Large%20language%20models%20are%20increasingly%20used%20for%20code%20generation%20and%20debugging%2C%20but%20their%20outputs%20can%20still%20contain%20bugs%2C%20that%20originate%20from%20training%20data.%20Distinguishing%20whether%20an%20LLM%20prefers%20correct%20code%2C%20or%20a%20familiar%20incorrect%20version%20might%20be%20influenced%20by%20what%20it%27s%20been%20exposed%20to%20during%20training.%20We%20introduce%20an%20exposure-aware%20evaluation%20framework%20that%20quantifies%20how%20prior%20exposure%20to%20buggy%20versus%20fixed%20code%20influences%20a%20model%27s%20preference.%20Using%20the%20ManySStuBs4J%20benchmark%2C%20we%20apply%20Data%20Portraits%20for%20membership%20testing%20on%20the%20Stack-V2%20corpus%20to%20estimate%20whether%20each%20buggy%20and%20fixed%20variant%20was%20seen%20during%20training.%20We%20then%20stratify%20examples%20by%20exposure%20and%20compare%20model%20preference%20using%20code%20completion%20as%20well%20as%20multiple%20likelihood-based%20scoring%20metrics%20We%20find%20that%20most%20examples%20%2867%25%29%20have%20neither%20variant%20in%20the%20training%20data%2C%20and%20when%20only%20one%20is%20present%2C%20fixes%20are%20more%20frequently%20present%20than%20bugs.%20In%20model%20generations%2C%20models%20reproduce%20buggy%20lines%20far%20more%20often%20than%20fixes%2C%20with%20bug-exposed%20examples%20amplifying%20this%20tendency%20and%20fix-exposed%20examples%20showing%20only%20marginal%20improvement.%20In%20likelihood%20scoring%2C%20minimum%20and%20maximum%20token-probability%20metrics%20consistently%20prefer%20the%20fixed%20code%20across%20all%20conditions%2C%20indicating%20a%20stable%20bias%20toward%20correct%20fixes.%20In%20contrast%2C%20metrics%20like%20the%20Gini%20coefficient%20reverse%20preference%20when%20only%20the%20buggy%20variant%20was%20seen.%20Our%20results%20indicate%20that%20exposure%20can%20skew%20bug-fix%20evaluations%20and%20highlight%20the%20risk%20that%20LLMs%20may%20propagate%20memorised%20errors%20in%20practice.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10496v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520See%252C%2520Model%2520Do%253F%2520Exposure-Aware%2520Evaluation%2520of%2520Bug-vs-Fix%2520Preference%2520in%2520Code%2520LLMs%26entry.906535625%3DAli%2520Al-Kaswan%2520and%2520Claudio%2520Spiess%2520and%2520Prem%2520Devanbu%2520and%2520Arie%2520van%2520Deursen%2520and%2520Maliheh%2520Izadi%26entry.1292438233%3DLarge%2520language%2520models%2520are%2520increasingly%2520used%2520for%2520code%2520generation%2520and%2520debugging%252C%2520but%2520their%2520outputs%2520can%2520still%2520contain%2520bugs%252C%2520that%2520originate%2520from%2520training%2520data.%2520Distinguishing%2520whether%2520an%2520LLM%2520prefers%2520correct%2520code%252C%2520or%2520a%2520familiar%2520incorrect%2520version%2520might%2520be%2520influenced%2520by%2520what%2520it%2527s%2520been%2520exposed%2520to%2520during%2520training.%2520We%2520introduce%2520an%2520exposure-aware%2520evaluation%2520framework%2520that%2520quantifies%2520how%2520prior%2520exposure%2520to%2520buggy%2520versus%2520fixed%2520code%2520influences%2520a%2520model%2527s%2520preference.%2520Using%2520the%2520ManySStuBs4J%2520benchmark%252C%2520we%2520apply%2520Data%2520Portraits%2520for%2520membership%2520testing%2520on%2520the%2520Stack-V2%2520corpus%2520to%2520estimate%2520whether%2520each%2520buggy%2520and%2520fixed%2520variant%2520was%2520seen%2520during%2520training.%2520We%2520then%2520stratify%2520examples%2520by%2520exposure%2520and%2520compare%2520model%2520preference%2520using%2520code%2520completion%2520as%2520well%2520as%2520multiple%2520likelihood-based%2520scoring%2520metrics%2520We%2520find%2520that%2520most%2520examples%2520%252867%2525%2529%2520have%2520neither%2520variant%2520in%2520the%2520training%2520data%252C%2520and%2520when%2520only%2520one%2520is%2520present%252C%2520fixes%2520are%2520more%2520frequently%2520present%2520than%2520bugs.%2520In%2520model%2520generations%252C%2520models%2520reproduce%2520buggy%2520lines%2520far%2520more%2520often%2520than%2520fixes%252C%2520with%2520bug-exposed%2520examples%2520amplifying%2520this%2520tendency%2520and%2520fix-exposed%2520examples%2520showing%2520only%2520marginal%2520improvement.%2520In%2520likelihood%2520scoring%252C%2520minimum%2520and%2520maximum%2520token-probability%2520metrics%2520consistently%2520prefer%2520the%2520fixed%2520code%2520across%2520all%2520conditions%252C%2520indicating%2520a%2520stable%2520bias%2520toward%2520correct%2520fixes.%2520In%2520contrast%252C%2520metrics%2520like%2520the%2520Gini%2520coefficient%2520reverse%2520preference%2520when%2520only%2520the%2520buggy%2520variant%2520was%2520seen.%2520Our%2520results%2520indicate%2520that%2520exposure%2520can%2520skew%2520bug-fix%2520evaluations%2520and%2520highlight%2520the%2520risk%2520that%2520LLMs%2520may%2520propagate%2520memorised%2520errors%2520in%2520practice.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10496v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20See%2C%20Model%20Do%3F%20Exposure-Aware%20Evaluation%20of%20Bug-vs-Fix%20Preference%20in%20Code%20LLMs&entry.906535625=Ali%20Al-Kaswan%20and%20Claudio%20Spiess%20and%20Prem%20Devanbu%20and%20Arie%20van%20Deursen%20and%20Maliheh%20Izadi&entry.1292438233=Large%20language%20models%20are%20increasingly%20used%20for%20code%20generation%20and%20debugging%2C%20but%20their%20outputs%20can%20still%20contain%20bugs%2C%20that%20originate%20from%20training%20data.%20Distinguishing%20whether%20an%20LLM%20prefers%20correct%20code%2C%20or%20a%20familiar%20incorrect%20version%20might%20be%20influenced%20by%20what%20it%27s%20been%20exposed%20to%20during%20training.%20We%20introduce%20an%20exposure-aware%20evaluation%20framework%20that%20quantifies%20how%20prior%20exposure%20to%20buggy%20versus%20fixed%20code%20influences%20a%20model%27s%20preference.%20Using%20the%20ManySStuBs4J%20benchmark%2C%20we%20apply%20Data%20Portraits%20for%20membership%20testing%20on%20the%20Stack-V2%20corpus%20to%20estimate%20whether%20each%20buggy%20and%20fixed%20variant%20was%20seen%20during%20training.%20We%20then%20stratify%20examples%20by%20exposure%20and%20compare%20model%20preference%20using%20code%20completion%20as%20well%20as%20multiple%20likelihood-based%20scoring%20metrics%20We%20find%20that%20most%20examples%20%2867%25%29%20have%20neither%20variant%20in%20the%20training%20data%2C%20and%20when%20only%20one%20is%20present%2C%20fixes%20are%20more%20frequently%20present%20than%20bugs.%20In%20model%20generations%2C%20models%20reproduce%20buggy%20lines%20far%20more%20often%20than%20fixes%2C%20with%20bug-exposed%20examples%20amplifying%20this%20tendency%20and%20fix-exposed%20examples%20showing%20only%20marginal%20improvement.%20In%20likelihood%20scoring%2C%20minimum%20and%20maximum%20token-probability%20metrics%20consistently%20prefer%20the%20fixed%20code%20across%20all%20conditions%2C%20indicating%20a%20stable%20bias%20toward%20correct%20fixes.%20In%20contrast%2C%20metrics%20like%20the%20Gini%20coefficient%20reverse%20preference%20when%20only%20the%20buggy%20variant%20was%20seen.%20Our%20results%20indicate%20that%20exposure%20can%20skew%20bug-fix%20evaluations%20and%20highlight%20the%20risk%20that%20LLMs%20may%20propagate%20memorised%20errors%20in%20practice.&entry.1838667208=http%3A//arxiv.org/abs/2601.10496v1&entry.124074799=Read"},
{"title": "Curating art exhibitions using machine learning", "author": "Eurico Covas", "abstract": "Here we present a series of artificial models - a total of four related models - based on machine learning techniques that attempt to learn from existing exhibitions which have been curated by human experts, in order to be able to do similar curatorship work. Out of our four artificial intelligence models, three achieve a reasonable ability at imitating these various curators responsible for all those exhibitions, with various degrees of precision and curatorial coherence. In particular, we can conclude two key insights: first, that there is sufficient information in these exhibitions to construct an artificial intelligence model that replicates past exhibitions with an accuracy well above random choices; and second, that using feature engineering and carefully designing the architecture of modest size models can make them almost as good as those using the so-called large language models such as GPT in a brute force approach.", "link": "http://arxiv.org/abs/2506.19813v3", "date": "2026-01-15", "relevancy": 2.3669, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4735}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curating%20art%20exhibitions%20using%20machine%20learning&body=Title%3A%20Curating%20art%20exhibitions%20using%20machine%20learning%0AAuthor%3A%20Eurico%20Covas%0AAbstract%3A%20Here%20we%20present%20a%20series%20of%20artificial%20models%20-%20a%20total%20of%20four%20related%20models%20-%20based%20on%20machine%20learning%20techniques%20that%20attempt%20to%20learn%20from%20existing%20exhibitions%20which%20have%20been%20curated%20by%20human%20experts%2C%20in%20order%20to%20be%20able%20to%20do%20similar%20curatorship%20work.%20Out%20of%20our%20four%20artificial%20intelligence%20models%2C%20three%20achieve%20a%20reasonable%20ability%20at%20imitating%20these%20various%20curators%20responsible%20for%20all%20those%20exhibitions%2C%20with%20various%20degrees%20of%20precision%20and%20curatorial%20coherence.%20In%20particular%2C%20we%20can%20conclude%20two%20key%20insights%3A%20first%2C%20that%20there%20is%20sufficient%20information%20in%20these%20exhibitions%20to%20construct%20an%20artificial%20intelligence%20model%20that%20replicates%20past%20exhibitions%20with%20an%20accuracy%20well%20above%20random%20choices%3B%20and%20second%2C%20that%20using%20feature%20engineering%20and%20carefully%20designing%20the%20architecture%20of%20modest%20size%20models%20can%20make%20them%20almost%20as%20good%20as%20those%20using%20the%20so-called%20large%20language%20models%20such%20as%20GPT%20in%20a%20brute%20force%20approach.%0ALink%3A%20http%3A//arxiv.org/abs/2506.19813v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurating%2520art%2520exhibitions%2520using%2520machine%2520learning%26entry.906535625%3DEurico%2520Covas%26entry.1292438233%3DHere%2520we%2520present%2520a%2520series%2520of%2520artificial%2520models%2520-%2520a%2520total%2520of%2520four%2520related%2520models%2520-%2520based%2520on%2520machine%2520learning%2520techniques%2520that%2520attempt%2520to%2520learn%2520from%2520existing%2520exhibitions%2520which%2520have%2520been%2520curated%2520by%2520human%2520experts%252C%2520in%2520order%2520to%2520be%2520able%2520to%2520do%2520similar%2520curatorship%2520work.%2520Out%2520of%2520our%2520four%2520artificial%2520intelligence%2520models%252C%2520three%2520achieve%2520a%2520reasonable%2520ability%2520at%2520imitating%2520these%2520various%2520curators%2520responsible%2520for%2520all%2520those%2520exhibitions%252C%2520with%2520various%2520degrees%2520of%2520precision%2520and%2520curatorial%2520coherence.%2520In%2520particular%252C%2520we%2520can%2520conclude%2520two%2520key%2520insights%253A%2520first%252C%2520that%2520there%2520is%2520sufficient%2520information%2520in%2520these%2520exhibitions%2520to%2520construct%2520an%2520artificial%2520intelligence%2520model%2520that%2520replicates%2520past%2520exhibitions%2520with%2520an%2520accuracy%2520well%2520above%2520random%2520choices%253B%2520and%2520second%252C%2520that%2520using%2520feature%2520engineering%2520and%2520carefully%2520designing%2520the%2520architecture%2520of%2520modest%2520size%2520models%2520can%2520make%2520them%2520almost%2520as%2520good%2520as%2520those%2520using%2520the%2520so-called%2520large%2520language%2520models%2520such%2520as%2520GPT%2520in%2520a%2520brute%2520force%2520approach.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.19813v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curating%20art%20exhibitions%20using%20machine%20learning&entry.906535625=Eurico%20Covas&entry.1292438233=Here%20we%20present%20a%20series%20of%20artificial%20models%20-%20a%20total%20of%20four%20related%20models%20-%20based%20on%20machine%20learning%20techniques%20that%20attempt%20to%20learn%20from%20existing%20exhibitions%20which%20have%20been%20curated%20by%20human%20experts%2C%20in%20order%20to%20be%20able%20to%20do%20similar%20curatorship%20work.%20Out%20of%20our%20four%20artificial%20intelligence%20models%2C%20three%20achieve%20a%20reasonable%20ability%20at%20imitating%20these%20various%20curators%20responsible%20for%20all%20those%20exhibitions%2C%20with%20various%20degrees%20of%20precision%20and%20curatorial%20coherence.%20In%20particular%2C%20we%20can%20conclude%20two%20key%20insights%3A%20first%2C%20that%20there%20is%20sufficient%20information%20in%20these%20exhibitions%20to%20construct%20an%20artificial%20intelligence%20model%20that%20replicates%20past%20exhibitions%20with%20an%20accuracy%20well%20above%20random%20choices%3B%20and%20second%2C%20that%20using%20feature%20engineering%20and%20carefully%20designing%20the%20architecture%20of%20modest%20size%20models%20can%20make%20them%20almost%20as%20good%20as%20those%20using%20the%20so-called%20large%20language%20models%20such%20as%20GPT%20in%20a%20brute%20force%20approach.&entry.1838667208=http%3A//arxiv.org/abs/2506.19813v3&entry.124074799=Read"},
{"title": "Classification Imbalance as Transfer Learning", "author": "Eric Xia and Jason M. Klusowski", "abstract": "Classification imbalance arises when one class is much rarer than the other. We frame this setting as transfer learning under label (prior) shift between an imbalanced source distribution induced by the observed data and a balanced target distribution under which performance is evaluated. Within this framework, we study a family of oversampling procedures that augment the training data by generating synthetic samples from an estimated minority-class distribution to roughly balance the classes, among which the celebrated SMOTE algorithm is a canonical example. We show that the excess risk decomposes into the rate achievable under balanced training (as if the data had been drawn from the balanced target distribution) and an additional term, the cost of transfer, which quantifies the discrepancy between the estimated and true minority-class distributions. In particular, we show that the cost of transfer for SMOTE dominates that of bootstrapping (random oversampling) in moderately high dimensions, suggesting that we should expect bootstrapping to have better performance than SMOTE in general. We corroborate these findings with experimental evidence. More broadly, our results provide guidance for choosing among augmentation strategies for imbalanced classification.", "link": "http://arxiv.org/abs/2601.10630v1", "date": "2026-01-15", "relevancy": 2.3661, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5405}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4421}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4371}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Classification%20Imbalance%20as%20Transfer%20Learning&body=Title%3A%20Classification%20Imbalance%20as%20Transfer%20Learning%0AAuthor%3A%20Eric%20Xia%20and%20Jason%20M.%20Klusowski%0AAbstract%3A%20Classification%20imbalance%20arises%20when%20one%20class%20is%20much%20rarer%20than%20the%20other.%20We%20frame%20this%20setting%20as%20transfer%20learning%20under%20label%20%28prior%29%20shift%20between%20an%20imbalanced%20source%20distribution%20induced%20by%20the%20observed%20data%20and%20a%20balanced%20target%20distribution%20under%20which%20performance%20is%20evaluated.%20Within%20this%20framework%2C%20we%20study%20a%20family%20of%20oversampling%20procedures%20that%20augment%20the%20training%20data%20by%20generating%20synthetic%20samples%20from%20an%20estimated%20minority-class%20distribution%20to%20roughly%20balance%20the%20classes%2C%20among%20which%20the%20celebrated%20SMOTE%20algorithm%20is%20a%20canonical%20example.%20We%20show%20that%20the%20excess%20risk%20decomposes%20into%20the%20rate%20achievable%20under%20balanced%20training%20%28as%20if%20the%20data%20had%20been%20drawn%20from%20the%20balanced%20target%20distribution%29%20and%20an%20additional%20term%2C%20the%20cost%20of%20transfer%2C%20which%20quantifies%20the%20discrepancy%20between%20the%20estimated%20and%20true%20minority-class%20distributions.%20In%20particular%2C%20we%20show%20that%20the%20cost%20of%20transfer%20for%20SMOTE%20dominates%20that%20of%20bootstrapping%20%28random%20oversampling%29%20in%20moderately%20high%20dimensions%2C%20suggesting%20that%20we%20should%20expect%20bootstrapping%20to%20have%20better%20performance%20than%20SMOTE%20in%20general.%20We%20corroborate%20these%20findings%20with%20experimental%20evidence.%20More%20broadly%2C%20our%20results%20provide%20guidance%20for%20choosing%20among%20augmentation%20strategies%20for%20imbalanced%20classification.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DClassification%2520Imbalance%2520as%2520Transfer%2520Learning%26entry.906535625%3DEric%2520Xia%2520and%2520Jason%2520M.%2520Klusowski%26entry.1292438233%3DClassification%2520imbalance%2520arises%2520when%2520one%2520class%2520is%2520much%2520rarer%2520than%2520the%2520other.%2520We%2520frame%2520this%2520setting%2520as%2520transfer%2520learning%2520under%2520label%2520%2528prior%2529%2520shift%2520between%2520an%2520imbalanced%2520source%2520distribution%2520induced%2520by%2520the%2520observed%2520data%2520and%2520a%2520balanced%2520target%2520distribution%2520under%2520which%2520performance%2520is%2520evaluated.%2520Within%2520this%2520framework%252C%2520we%2520study%2520a%2520family%2520of%2520oversampling%2520procedures%2520that%2520augment%2520the%2520training%2520data%2520by%2520generating%2520synthetic%2520samples%2520from%2520an%2520estimated%2520minority-class%2520distribution%2520to%2520roughly%2520balance%2520the%2520classes%252C%2520among%2520which%2520the%2520celebrated%2520SMOTE%2520algorithm%2520is%2520a%2520canonical%2520example.%2520We%2520show%2520that%2520the%2520excess%2520risk%2520decomposes%2520into%2520the%2520rate%2520achievable%2520under%2520balanced%2520training%2520%2528as%2520if%2520the%2520data%2520had%2520been%2520drawn%2520from%2520the%2520balanced%2520target%2520distribution%2529%2520and%2520an%2520additional%2520term%252C%2520the%2520cost%2520of%2520transfer%252C%2520which%2520quantifies%2520the%2520discrepancy%2520between%2520the%2520estimated%2520and%2520true%2520minority-class%2520distributions.%2520In%2520particular%252C%2520we%2520show%2520that%2520the%2520cost%2520of%2520transfer%2520for%2520SMOTE%2520dominates%2520that%2520of%2520bootstrapping%2520%2528random%2520oversampling%2529%2520in%2520moderately%2520high%2520dimensions%252C%2520suggesting%2520that%2520we%2520should%2520expect%2520bootstrapping%2520to%2520have%2520better%2520performance%2520than%2520SMOTE%2520in%2520general.%2520We%2520corroborate%2520these%2520findings%2520with%2520experimental%2520evidence.%2520More%2520broadly%252C%2520our%2520results%2520provide%2520guidance%2520for%2520choosing%2520among%2520augmentation%2520strategies%2520for%2520imbalanced%2520classification.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Classification%20Imbalance%20as%20Transfer%20Learning&entry.906535625=Eric%20Xia%20and%20Jason%20M.%20Klusowski&entry.1292438233=Classification%20imbalance%20arises%20when%20one%20class%20is%20much%20rarer%20than%20the%20other.%20We%20frame%20this%20setting%20as%20transfer%20learning%20under%20label%20%28prior%29%20shift%20between%20an%20imbalanced%20source%20distribution%20induced%20by%20the%20observed%20data%20and%20a%20balanced%20target%20distribution%20under%20which%20performance%20is%20evaluated.%20Within%20this%20framework%2C%20we%20study%20a%20family%20of%20oversampling%20procedures%20that%20augment%20the%20training%20data%20by%20generating%20synthetic%20samples%20from%20an%20estimated%20minority-class%20distribution%20to%20roughly%20balance%20the%20classes%2C%20among%20which%20the%20celebrated%20SMOTE%20algorithm%20is%20a%20canonical%20example.%20We%20show%20that%20the%20excess%20risk%20decomposes%20into%20the%20rate%20achievable%20under%20balanced%20training%20%28as%20if%20the%20data%20had%20been%20drawn%20from%20the%20balanced%20target%20distribution%29%20and%20an%20additional%20term%2C%20the%20cost%20of%20transfer%2C%20which%20quantifies%20the%20discrepancy%20between%20the%20estimated%20and%20true%20minority-class%20distributions.%20In%20particular%2C%20we%20show%20that%20the%20cost%20of%20transfer%20for%20SMOTE%20dominates%20that%20of%20bootstrapping%20%28random%20oversampling%29%20in%20moderately%20high%20dimensions%2C%20suggesting%20that%20we%20should%20expect%20bootstrapping%20to%20have%20better%20performance%20than%20SMOTE%20in%20general.%20We%20corroborate%20these%20findings%20with%20experimental%20evidence.%20More%20broadly%2C%20our%20results%20provide%20guidance%20for%20choosing%20among%20augmentation%20strategies%20for%20imbalanced%20classification.&entry.1838667208=http%3A//arxiv.org/abs/2601.10630v1&entry.124074799=Read"},
{"title": "Knowledge Homophily in Large Language Models", "author": "Utkarsh Sahu and Zhisheng Qi and Mahantesh Halappanavar and Nedim Lipka and Ryan A. Rossi and Franck Dernoncourt and Yu Zhang and Yao Ma and Yu Wang", "abstract": "Large Language Models (LLMs) have been increasingly studied as neural knowledge bases for supporting knowledge-intensive applications such as question answering and fact checking. However, the structural organization of their knowledge remains unexplored. Inspired by cognitive neuroscience findings, such as semantic clustering and priming, where knowing one fact increases the likelihood of recalling related facts, we investigate an analogous knowledge homophily pattern in LLMs. To this end, we map LLM knowledge into a graph representation through knowledge checking at both the triplet and entity levels. After that, we analyze the knowledgeability relationship between an entity and its neighbors, discovering that LLMs tend to possess a similar level of knowledge about entities positioned closer in the graph. Motivated by this homophily principle, we propose a Graph Neural Network (GNN) regression model to estimate entity-level knowledgeability scores for triplets by leveraging their neighborhood scores. The predicted knowledgeability enables us to prioritize checking less well-known triplets, thereby maximizing knowledge coverage under the same labeling budget. This not only improves the efficiency of active labeling for fine-tuning to inject knowledge into LLMs but also enhances multi-hop path retrieval in reasoning-intensive question answering.", "link": "http://arxiv.org/abs/2509.23773v2", "date": "2026-01-15", "relevancy": 2.3274, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4689}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4689}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Homophily%20in%20Large%20Language%20Models&body=Title%3A%20Knowledge%20Homophily%20in%20Large%20Language%20Models%0AAuthor%3A%20Utkarsh%20Sahu%20and%20Zhisheng%20Qi%20and%20Mahantesh%20Halappanavar%20and%20Nedim%20Lipka%20and%20Ryan%20A.%20Rossi%20and%20Franck%20Dernoncourt%20and%20Yu%20Zhang%20and%20Yao%20Ma%20and%20Yu%20Wang%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20been%20increasingly%20studied%20as%20neural%20knowledge%20bases%20for%20supporting%20knowledge-intensive%20applications%20such%20as%20question%20answering%20and%20fact%20checking.%20However%2C%20the%20structural%20organization%20of%20their%20knowledge%20remains%20unexplored.%20Inspired%20by%20cognitive%20neuroscience%20findings%2C%20such%20as%20semantic%20clustering%20and%20priming%2C%20where%20knowing%20one%20fact%20increases%20the%20likelihood%20of%20recalling%20related%20facts%2C%20we%20investigate%20an%20analogous%20knowledge%20homophily%20pattern%20in%20LLMs.%20To%20this%20end%2C%20we%20map%20LLM%20knowledge%20into%20a%20graph%20representation%20through%20knowledge%20checking%20at%20both%20the%20triplet%20and%20entity%20levels.%20After%20that%2C%20we%20analyze%20the%20knowledgeability%20relationship%20between%20an%20entity%20and%20its%20neighbors%2C%20discovering%20that%20LLMs%20tend%20to%20possess%20a%20similar%20level%20of%20knowledge%20about%20entities%20positioned%20closer%20in%20the%20graph.%20Motivated%20by%20this%20homophily%20principle%2C%20we%20propose%20a%20Graph%20Neural%20Network%20%28GNN%29%20regression%20model%20to%20estimate%20entity-level%20knowledgeability%20scores%20for%20triplets%20by%20leveraging%20their%20neighborhood%20scores.%20The%20predicted%20knowledgeability%20enables%20us%20to%20prioritize%20checking%20less%20well-known%20triplets%2C%20thereby%20maximizing%20knowledge%20coverage%20under%20the%20same%20labeling%20budget.%20This%20not%20only%20improves%20the%20efficiency%20of%20active%20labeling%20for%20fine-tuning%20to%20inject%20knowledge%20into%20LLMs%20but%20also%20enhances%20multi-hop%20path%20retrieval%20in%20reasoning-intensive%20question%20answering.%0ALink%3A%20http%3A//arxiv.org/abs/2509.23773v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Homophily%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DUtkarsh%2520Sahu%2520and%2520Zhisheng%2520Qi%2520and%2520Mahantesh%2520Halappanavar%2520and%2520Nedim%2520Lipka%2520and%2520Ryan%2520A.%2520Rossi%2520and%2520Franck%2520Dernoncourt%2520and%2520Yu%2520Zhang%2520and%2520Yao%2520Ma%2520and%2520Yu%2520Wang%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520been%2520increasingly%2520studied%2520as%2520neural%2520knowledge%2520bases%2520for%2520supporting%2520knowledge-intensive%2520applications%2520such%2520as%2520question%2520answering%2520and%2520fact%2520checking.%2520However%252C%2520the%2520structural%2520organization%2520of%2520their%2520knowledge%2520remains%2520unexplored.%2520Inspired%2520by%2520cognitive%2520neuroscience%2520findings%252C%2520such%2520as%2520semantic%2520clustering%2520and%2520priming%252C%2520where%2520knowing%2520one%2520fact%2520increases%2520the%2520likelihood%2520of%2520recalling%2520related%2520facts%252C%2520we%2520investigate%2520an%2520analogous%2520knowledge%2520homophily%2520pattern%2520in%2520LLMs.%2520To%2520this%2520end%252C%2520we%2520map%2520LLM%2520knowledge%2520into%2520a%2520graph%2520representation%2520through%2520knowledge%2520checking%2520at%2520both%2520the%2520triplet%2520and%2520entity%2520levels.%2520After%2520that%252C%2520we%2520analyze%2520the%2520knowledgeability%2520relationship%2520between%2520an%2520entity%2520and%2520its%2520neighbors%252C%2520discovering%2520that%2520LLMs%2520tend%2520to%2520possess%2520a%2520similar%2520level%2520of%2520knowledge%2520about%2520entities%2520positioned%2520closer%2520in%2520the%2520graph.%2520Motivated%2520by%2520this%2520homophily%2520principle%252C%2520we%2520propose%2520a%2520Graph%2520Neural%2520Network%2520%2528GNN%2529%2520regression%2520model%2520to%2520estimate%2520entity-level%2520knowledgeability%2520scores%2520for%2520triplets%2520by%2520leveraging%2520their%2520neighborhood%2520scores.%2520The%2520predicted%2520knowledgeability%2520enables%2520us%2520to%2520prioritize%2520checking%2520less%2520well-known%2520triplets%252C%2520thereby%2520maximizing%2520knowledge%2520coverage%2520under%2520the%2520same%2520labeling%2520budget.%2520This%2520not%2520only%2520improves%2520the%2520efficiency%2520of%2520active%2520labeling%2520for%2520fine-tuning%2520to%2520inject%2520knowledge%2520into%2520LLMs%2520but%2520also%2520enhances%2520multi-hop%2520path%2520retrieval%2520in%2520reasoning-intensive%2520question%2520answering.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23773v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Homophily%20in%20Large%20Language%20Models&entry.906535625=Utkarsh%20Sahu%20and%20Zhisheng%20Qi%20and%20Mahantesh%20Halappanavar%20and%20Nedim%20Lipka%20and%20Ryan%20A.%20Rossi%20and%20Franck%20Dernoncourt%20and%20Yu%20Zhang%20and%20Yao%20Ma%20and%20Yu%20Wang&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20been%20increasingly%20studied%20as%20neural%20knowledge%20bases%20for%20supporting%20knowledge-intensive%20applications%20such%20as%20question%20answering%20and%20fact%20checking.%20However%2C%20the%20structural%20organization%20of%20their%20knowledge%20remains%20unexplored.%20Inspired%20by%20cognitive%20neuroscience%20findings%2C%20such%20as%20semantic%20clustering%20and%20priming%2C%20where%20knowing%20one%20fact%20increases%20the%20likelihood%20of%20recalling%20related%20facts%2C%20we%20investigate%20an%20analogous%20knowledge%20homophily%20pattern%20in%20LLMs.%20To%20this%20end%2C%20we%20map%20LLM%20knowledge%20into%20a%20graph%20representation%20through%20knowledge%20checking%20at%20both%20the%20triplet%20and%20entity%20levels.%20After%20that%2C%20we%20analyze%20the%20knowledgeability%20relationship%20between%20an%20entity%20and%20its%20neighbors%2C%20discovering%20that%20LLMs%20tend%20to%20possess%20a%20similar%20level%20of%20knowledge%20about%20entities%20positioned%20closer%20in%20the%20graph.%20Motivated%20by%20this%20homophily%20principle%2C%20we%20propose%20a%20Graph%20Neural%20Network%20%28GNN%29%20regression%20model%20to%20estimate%20entity-level%20knowledgeability%20scores%20for%20triplets%20by%20leveraging%20their%20neighborhood%20scores.%20The%20predicted%20knowledgeability%20enables%20us%20to%20prioritize%20checking%20less%20well-known%20triplets%2C%20thereby%20maximizing%20knowledge%20coverage%20under%20the%20same%20labeling%20budget.%20This%20not%20only%20improves%20the%20efficiency%20of%20active%20labeling%20for%20fine-tuning%20to%20inject%20knowledge%20into%20LLMs%20but%20also%20enhances%20multi-hop%20path%20retrieval%20in%20reasoning-intensive%20question%20answering.&entry.1838667208=http%3A//arxiv.org/abs/2509.23773v2&entry.124074799=Read"},
{"title": "A Study of Commonsense Reasoning over Visual Object Properties", "author": "Abhishek Kolari and Mohammadhossein Khojasteh and Yifan Jiang and Floris den Hengst and Filip Ilievski", "abstract": "Inspired by human categorization, object property reasoning involves identifying and recognizing low-level details and higher-level abstractions. While current visual question answering (VQA) studies consider multiple object properties, such as size, they typically blend perception and reasoning and lack representativeness in terms of reasoning and image categories, making it unclear whether and how vision-language models (VLMs) abstract and reason over depicted objects. To this end, we introduce a systematic evaluation framework comprising images of three representative types, three reasoning levels of increasing complexity, and four object property dimensions, informed by prior work on common sense. We develop a procedure to instantiate this framework in two VQA object reasoning benchmarks: OPTICS-CNT, comprising 360 images paired with 1,080 multi-level, count-based questions, and OPTICS-CMP, with 2.1k comparison questions. Experiments with 12 state-of-the-art VLMs in zero-shot settings reveal significant limitations relative to humans, with the best-performing model achieving below 40% counting and 70% comparison accuracy. VLMs struggle particularly with photographic images, counterfactual reasoning, physical and functional properties, and higher counts. We make the OPTICS benchmark data and code available to support future work on scalable benchmarking methods, generalized annotation guidelines, and advanced reasoning VLMs.", "link": "http://arxiv.org/abs/2508.10956v2", "date": "2026-01-15", "relevancy": 2.3156, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5838}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5838}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5545}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Study%20of%20Commonsense%20Reasoning%20over%20Visual%20Object%20Properties&body=Title%3A%20A%20Study%20of%20Commonsense%20Reasoning%20over%20Visual%20Object%20Properties%0AAuthor%3A%20Abhishek%20Kolari%20and%20Mohammadhossein%20Khojasteh%20and%20Yifan%20Jiang%20and%20Floris%20den%20Hengst%20and%20Filip%20Ilievski%0AAbstract%3A%20Inspired%20by%20human%20categorization%2C%20object%20property%20reasoning%20involves%20identifying%20and%20recognizing%20low-level%20details%20and%20higher-level%20abstractions.%20While%20current%20visual%20question%20answering%20%28VQA%29%20studies%20consider%20multiple%20object%20properties%2C%20such%20as%20size%2C%20they%20typically%20blend%20perception%20and%20reasoning%20and%20lack%20representativeness%20in%20terms%20of%20reasoning%20and%20image%20categories%2C%20making%20it%20unclear%20whether%20and%20how%20vision-language%20models%20%28VLMs%29%20abstract%20and%20reason%20over%20depicted%20objects.%20To%20this%20end%2C%20we%20introduce%20a%20systematic%20evaluation%20framework%20comprising%20images%20of%20three%20representative%20types%2C%20three%20reasoning%20levels%20of%20increasing%20complexity%2C%20and%20four%20object%20property%20dimensions%2C%20informed%20by%20prior%20work%20on%20common%20sense.%20We%20develop%20a%20procedure%20to%20instantiate%20this%20framework%20in%20two%20VQA%20object%20reasoning%20benchmarks%3A%20OPTICS-CNT%2C%20comprising%20360%20images%20paired%20with%201%2C080%20multi-level%2C%20count-based%20questions%2C%20and%20OPTICS-CMP%2C%20with%202.1k%20comparison%20questions.%20Experiments%20with%2012%20state-of-the-art%20VLMs%20in%20zero-shot%20settings%20reveal%20significant%20limitations%20relative%20to%20humans%2C%20with%20the%20best-performing%20model%20achieving%20below%2040%25%20counting%20and%2070%25%20comparison%20accuracy.%20VLMs%20struggle%20particularly%20with%20photographic%20images%2C%20counterfactual%20reasoning%2C%20physical%20and%20functional%20properties%2C%20and%20higher%20counts.%20We%20make%20the%20OPTICS%20benchmark%20data%20and%20code%20available%20to%20support%20future%20work%20on%20scalable%20benchmarking%20methods%2C%20generalized%20annotation%20guidelines%2C%20and%20advanced%20reasoning%20VLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2508.10956v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Study%2520of%2520Commonsense%2520Reasoning%2520over%2520Visual%2520Object%2520Properties%26entry.906535625%3DAbhishek%2520Kolari%2520and%2520Mohammadhossein%2520Khojasteh%2520and%2520Yifan%2520Jiang%2520and%2520Floris%2520den%2520Hengst%2520and%2520Filip%2520Ilievski%26entry.1292438233%3DInspired%2520by%2520human%2520categorization%252C%2520object%2520property%2520reasoning%2520involves%2520identifying%2520and%2520recognizing%2520low-level%2520details%2520and%2520higher-level%2520abstractions.%2520While%2520current%2520visual%2520question%2520answering%2520%2528VQA%2529%2520studies%2520consider%2520multiple%2520object%2520properties%252C%2520such%2520as%2520size%252C%2520they%2520typically%2520blend%2520perception%2520and%2520reasoning%2520and%2520lack%2520representativeness%2520in%2520terms%2520of%2520reasoning%2520and%2520image%2520categories%252C%2520making%2520it%2520unclear%2520whether%2520and%2520how%2520vision-language%2520models%2520%2528VLMs%2529%2520abstract%2520and%2520reason%2520over%2520depicted%2520objects.%2520To%2520this%2520end%252C%2520we%2520introduce%2520a%2520systematic%2520evaluation%2520framework%2520comprising%2520images%2520of%2520three%2520representative%2520types%252C%2520three%2520reasoning%2520levels%2520of%2520increasing%2520complexity%252C%2520and%2520four%2520object%2520property%2520dimensions%252C%2520informed%2520by%2520prior%2520work%2520on%2520common%2520sense.%2520We%2520develop%2520a%2520procedure%2520to%2520instantiate%2520this%2520framework%2520in%2520two%2520VQA%2520object%2520reasoning%2520benchmarks%253A%2520OPTICS-CNT%252C%2520comprising%2520360%2520images%2520paired%2520with%25201%252C080%2520multi-level%252C%2520count-based%2520questions%252C%2520and%2520OPTICS-CMP%252C%2520with%25202.1k%2520comparison%2520questions.%2520Experiments%2520with%252012%2520state-of-the-art%2520VLMs%2520in%2520zero-shot%2520settings%2520reveal%2520significant%2520limitations%2520relative%2520to%2520humans%252C%2520with%2520the%2520best-performing%2520model%2520achieving%2520below%252040%2525%2520counting%2520and%252070%2525%2520comparison%2520accuracy.%2520VLMs%2520struggle%2520particularly%2520with%2520photographic%2520images%252C%2520counterfactual%2520reasoning%252C%2520physical%2520and%2520functional%2520properties%252C%2520and%2520higher%2520counts.%2520We%2520make%2520the%2520OPTICS%2520benchmark%2520data%2520and%2520code%2520available%2520to%2520support%2520future%2520work%2520on%2520scalable%2520benchmarking%2520methods%252C%2520generalized%2520annotation%2520guidelines%252C%2520and%2520advanced%2520reasoning%2520VLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.10956v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Study%20of%20Commonsense%20Reasoning%20over%20Visual%20Object%20Properties&entry.906535625=Abhishek%20Kolari%20and%20Mohammadhossein%20Khojasteh%20and%20Yifan%20Jiang%20and%20Floris%20den%20Hengst%20and%20Filip%20Ilievski&entry.1292438233=Inspired%20by%20human%20categorization%2C%20object%20property%20reasoning%20involves%20identifying%20and%20recognizing%20low-level%20details%20and%20higher-level%20abstractions.%20While%20current%20visual%20question%20answering%20%28VQA%29%20studies%20consider%20multiple%20object%20properties%2C%20such%20as%20size%2C%20they%20typically%20blend%20perception%20and%20reasoning%20and%20lack%20representativeness%20in%20terms%20of%20reasoning%20and%20image%20categories%2C%20making%20it%20unclear%20whether%20and%20how%20vision-language%20models%20%28VLMs%29%20abstract%20and%20reason%20over%20depicted%20objects.%20To%20this%20end%2C%20we%20introduce%20a%20systematic%20evaluation%20framework%20comprising%20images%20of%20three%20representative%20types%2C%20three%20reasoning%20levels%20of%20increasing%20complexity%2C%20and%20four%20object%20property%20dimensions%2C%20informed%20by%20prior%20work%20on%20common%20sense.%20We%20develop%20a%20procedure%20to%20instantiate%20this%20framework%20in%20two%20VQA%20object%20reasoning%20benchmarks%3A%20OPTICS-CNT%2C%20comprising%20360%20images%20paired%20with%201%2C080%20multi-level%2C%20count-based%20questions%2C%20and%20OPTICS-CMP%2C%20with%202.1k%20comparison%20questions.%20Experiments%20with%2012%20state-of-the-art%20VLMs%20in%20zero-shot%20settings%20reveal%20significant%20limitations%20relative%20to%20humans%2C%20with%20the%20best-performing%20model%20achieving%20below%2040%25%20counting%20and%2070%25%20comparison%20accuracy.%20VLMs%20struggle%20particularly%20with%20photographic%20images%2C%20counterfactual%20reasoning%2C%20physical%20and%20functional%20properties%2C%20and%20higher%20counts.%20We%20make%20the%20OPTICS%20benchmark%20data%20and%20code%20available%20to%20support%20future%20work%20on%20scalable%20benchmarking%20methods%2C%20generalized%20annotation%20guidelines%2C%20and%20advanced%20reasoning%20VLMs.&entry.1838667208=http%3A//arxiv.org/abs/2508.10956v2&entry.124074799=Read"},
{"title": "Urban Socio-Semantic Segmentation with Vision-Language Reasoning", "author": "Yu Wang and Yi Wang and Rui Dai and Yujie Wang and Kaikui Liu and Xiangxiang Chu and Yansheng Li", "abstract": "As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach's gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.", "link": "http://arxiv.org/abs/2601.10477v1", "date": "2026-01-15", "relevancy": 2.3136, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5817}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5777}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Urban%20Socio-Semantic%20Segmentation%20with%20Vision-Language%20Reasoning&body=Title%3A%20Urban%20Socio-Semantic%20Segmentation%20with%20Vision-Language%20Reasoning%0AAuthor%3A%20Yu%20Wang%20and%20Yi%20Wang%20and%20Rui%20Dai%20and%20Yujie%20Wang%20and%20Kaikui%20Liu%20and%20Xiangxiang%20Chu%20and%20Yansheng%20Li%0AAbstract%3A%20As%20hubs%20of%20human%20activity%2C%20urban%20surfaces%20consist%20of%20a%20wealth%20of%20semantic%20entities.%20Segmenting%20these%20various%20entities%20from%20satellite%20imagery%20is%20crucial%20for%20a%20range%20of%20downstream%20applications.%20Current%20advanced%20segmentation%20models%20can%20reliably%20segment%20entities%20defined%20by%20physical%20attributes%20%28e.g.%2C%20buildings%2C%20water%20bodies%29%20but%20still%20struggle%20with%20socially%20defined%20categories%20%28e.g.%2C%20schools%2C%20parks%29.%20In%20this%20work%2C%20we%20achieve%20socio-semantic%20segmentation%20by%20vision-language%20model%20reasoning.%20To%20facilitate%20this%2C%20we%20introduce%20the%20Urban%20Socio-Semantic%20Segmentation%20dataset%20named%20SocioSeg%2C%20a%20new%20resource%20comprising%20satellite%20imagery%2C%20digital%20maps%2C%20and%20pixel-level%20labels%20of%20social%20semantic%20entities%20organized%20in%20a%20hierarchical%20structure.%20Additionally%2C%20we%20propose%20a%20novel%20vision-language%20reasoning%20framework%20called%20SocioReasoner%20that%20simulates%20the%20human%20process%20of%20identifying%20and%20annotating%20social%20semantic%20entities%20via%20cross-modal%20recognition%20and%20multi-stage%20reasoning.%20We%20employ%20reinforcement%20learning%20to%20optimize%20this%20non-differentiable%20process%20and%20elicit%20the%20reasoning%20capabilities%20of%20the%20vision-language%20model.%20Experiments%20demonstrate%20our%20approach%27s%20gains%20over%20state-of-the-art%20models%20and%20strong%20zero-shot%20generalization.%20Our%20dataset%20and%20code%20are%20available%20in%20https%3A//github.com/AMAP-ML/SocioReasoner.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10477v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUrban%2520Socio-Semantic%2520Segmentation%2520with%2520Vision-Language%2520Reasoning%26entry.906535625%3DYu%2520Wang%2520and%2520Yi%2520Wang%2520and%2520Rui%2520Dai%2520and%2520Yujie%2520Wang%2520and%2520Kaikui%2520Liu%2520and%2520Xiangxiang%2520Chu%2520and%2520Yansheng%2520Li%26entry.1292438233%3DAs%2520hubs%2520of%2520human%2520activity%252C%2520urban%2520surfaces%2520consist%2520of%2520a%2520wealth%2520of%2520semantic%2520entities.%2520Segmenting%2520these%2520various%2520entities%2520from%2520satellite%2520imagery%2520is%2520crucial%2520for%2520a%2520range%2520of%2520downstream%2520applications.%2520Current%2520advanced%2520segmentation%2520models%2520can%2520reliably%2520segment%2520entities%2520defined%2520by%2520physical%2520attributes%2520%2528e.g.%252C%2520buildings%252C%2520water%2520bodies%2529%2520but%2520still%2520struggle%2520with%2520socially%2520defined%2520categories%2520%2528e.g.%252C%2520schools%252C%2520parks%2529.%2520In%2520this%2520work%252C%2520we%2520achieve%2520socio-semantic%2520segmentation%2520by%2520vision-language%2520model%2520reasoning.%2520To%2520facilitate%2520this%252C%2520we%2520introduce%2520the%2520Urban%2520Socio-Semantic%2520Segmentation%2520dataset%2520named%2520SocioSeg%252C%2520a%2520new%2520resource%2520comprising%2520satellite%2520imagery%252C%2520digital%2520maps%252C%2520and%2520pixel-level%2520labels%2520of%2520social%2520semantic%2520entities%2520organized%2520in%2520a%2520hierarchical%2520structure.%2520Additionally%252C%2520we%2520propose%2520a%2520novel%2520vision-language%2520reasoning%2520framework%2520called%2520SocioReasoner%2520that%2520simulates%2520the%2520human%2520process%2520of%2520identifying%2520and%2520annotating%2520social%2520semantic%2520entities%2520via%2520cross-modal%2520recognition%2520and%2520multi-stage%2520reasoning.%2520We%2520employ%2520reinforcement%2520learning%2520to%2520optimize%2520this%2520non-differentiable%2520process%2520and%2520elicit%2520the%2520reasoning%2520capabilities%2520of%2520the%2520vision-language%2520model.%2520Experiments%2520demonstrate%2520our%2520approach%2527s%2520gains%2520over%2520state-of-the-art%2520models%2520and%2520strong%2520zero-shot%2520generalization.%2520Our%2520dataset%2520and%2520code%2520are%2520available%2520in%2520https%253A//github.com/AMAP-ML/SocioReasoner.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10477v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Urban%20Socio-Semantic%20Segmentation%20with%20Vision-Language%20Reasoning&entry.906535625=Yu%20Wang%20and%20Yi%20Wang%20and%20Rui%20Dai%20and%20Yujie%20Wang%20and%20Kaikui%20Liu%20and%20Xiangxiang%20Chu%20and%20Yansheng%20Li&entry.1292438233=As%20hubs%20of%20human%20activity%2C%20urban%20surfaces%20consist%20of%20a%20wealth%20of%20semantic%20entities.%20Segmenting%20these%20various%20entities%20from%20satellite%20imagery%20is%20crucial%20for%20a%20range%20of%20downstream%20applications.%20Current%20advanced%20segmentation%20models%20can%20reliably%20segment%20entities%20defined%20by%20physical%20attributes%20%28e.g.%2C%20buildings%2C%20water%20bodies%29%20but%20still%20struggle%20with%20socially%20defined%20categories%20%28e.g.%2C%20schools%2C%20parks%29.%20In%20this%20work%2C%20we%20achieve%20socio-semantic%20segmentation%20by%20vision-language%20model%20reasoning.%20To%20facilitate%20this%2C%20we%20introduce%20the%20Urban%20Socio-Semantic%20Segmentation%20dataset%20named%20SocioSeg%2C%20a%20new%20resource%20comprising%20satellite%20imagery%2C%20digital%20maps%2C%20and%20pixel-level%20labels%20of%20social%20semantic%20entities%20organized%20in%20a%20hierarchical%20structure.%20Additionally%2C%20we%20propose%20a%20novel%20vision-language%20reasoning%20framework%20called%20SocioReasoner%20that%20simulates%20the%20human%20process%20of%20identifying%20and%20annotating%20social%20semantic%20entities%20via%20cross-modal%20recognition%20and%20multi-stage%20reasoning.%20We%20employ%20reinforcement%20learning%20to%20optimize%20this%20non-differentiable%20process%20and%20elicit%20the%20reasoning%20capabilities%20of%20the%20vision-language%20model.%20Experiments%20demonstrate%20our%20approach%27s%20gains%20over%20state-of-the-art%20models%20and%20strong%20zero-shot%20generalization.%20Our%20dataset%20and%20code%20are%20available%20in%20https%3A//github.com/AMAP-ML/SocioReasoner.&entry.1838667208=http%3A//arxiv.org/abs/2601.10477v1&entry.124074799=Read"},
{"title": "Semantic Misalignment in Vision-Language Models under Perceptual Degradation", "author": "Guo Cheng", "abstract": "Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.", "link": "http://arxiv.org/abs/2601.08355v2", "date": "2026-01-15", "relevancy": 2.2993, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5788}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.574}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.574}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantic%20Misalignment%20in%20Vision-Language%20Models%20under%20Perceptual%20Degradation&body=Title%3A%20Semantic%20Misalignment%20in%20Vision-Language%20Models%20under%20Perceptual%20Degradation%0AAuthor%3A%20Guo%20Cheng%0AAbstract%3A%20Vision-Language%20Models%20%28VLMs%29%20are%20increasingly%20deployed%20in%20autonomous%20driving%20and%20embodied%20AI%20systems%2C%20where%20reliable%20perception%20is%20critical%20for%20safe%20semantic%20reasoning%20and%20decision-making.%20While%20recent%20VLMs%20demonstrate%20strong%20performance%20on%20multimodal%20benchmarks%2C%20their%20robustness%20to%20realistic%20perception%20degradation%20remains%20poorly%20understood.%20In%20this%20work%2C%20we%20systematically%20study%20semantic%20misalignment%20in%20VLMs%20under%20controlled%20degradation%20of%20upstream%20visual%20perception%2C%20using%20semantic%20segmentation%20on%20the%20Cityscapes%20dataset%20as%20a%20representative%20perception%20module.%20We%20introduce%20perception-realistic%20corruptions%20that%20induce%20only%20moderate%20drops%20in%20conventional%20segmentation%20metrics%2C%20yet%20observe%20severe%20failures%20in%20downstream%20VLM%20behavior%2C%20including%20hallucinated%20object%20mentions%2C%20omission%20of%20safety-critical%20entities%2C%20and%20inconsistent%20safety%20judgments.%20To%20quantify%20these%20effects%2C%20we%20propose%20a%20set%20of%20language-level%20misalignment%20metrics%20that%20capture%20hallucination%2C%20critical%20omission%2C%20and%20safety%20misinterpretation%2C%20and%20analyze%20their%20relationship%20with%20segmentation%20quality%20across%20multiple%20contrastive%20and%20generative%20VLMs.%20Our%20results%20reveal%20a%20clear%20disconnect%20between%20pixel-level%20robustness%20and%20multimodal%20semantic%20reliability%2C%20highlighting%20a%20critical%20limitation%20of%20current%20VLM-based%20systems%20and%20motivating%20the%20need%20for%20evaluation%20frameworks%20that%20explicitly%20account%20for%20perception%20uncertainty%20in%20safety-critical%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08355v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantic%2520Misalignment%2520in%2520Vision-Language%2520Models%2520under%2520Perceptual%2520Degradation%26entry.906535625%3DGuo%2520Cheng%26entry.1292438233%3DVision-Language%2520Models%2520%2528VLMs%2529%2520are%2520increasingly%2520deployed%2520in%2520autonomous%2520driving%2520and%2520embodied%2520AI%2520systems%252C%2520where%2520reliable%2520perception%2520is%2520critical%2520for%2520safe%2520semantic%2520reasoning%2520and%2520decision-making.%2520While%2520recent%2520VLMs%2520demonstrate%2520strong%2520performance%2520on%2520multimodal%2520benchmarks%252C%2520their%2520robustness%2520to%2520realistic%2520perception%2520degradation%2520remains%2520poorly%2520understood.%2520In%2520this%2520work%252C%2520we%2520systematically%2520study%2520semantic%2520misalignment%2520in%2520VLMs%2520under%2520controlled%2520degradation%2520of%2520upstream%2520visual%2520perception%252C%2520using%2520semantic%2520segmentation%2520on%2520the%2520Cityscapes%2520dataset%2520as%2520a%2520representative%2520perception%2520module.%2520We%2520introduce%2520perception-realistic%2520corruptions%2520that%2520induce%2520only%2520moderate%2520drops%2520in%2520conventional%2520segmentation%2520metrics%252C%2520yet%2520observe%2520severe%2520failures%2520in%2520downstream%2520VLM%2520behavior%252C%2520including%2520hallucinated%2520object%2520mentions%252C%2520omission%2520of%2520safety-critical%2520entities%252C%2520and%2520inconsistent%2520safety%2520judgments.%2520To%2520quantify%2520these%2520effects%252C%2520we%2520propose%2520a%2520set%2520of%2520language-level%2520misalignment%2520metrics%2520that%2520capture%2520hallucination%252C%2520critical%2520omission%252C%2520and%2520safety%2520misinterpretation%252C%2520and%2520analyze%2520their%2520relationship%2520with%2520segmentation%2520quality%2520across%2520multiple%2520contrastive%2520and%2520generative%2520VLMs.%2520Our%2520results%2520reveal%2520a%2520clear%2520disconnect%2520between%2520pixel-level%2520robustness%2520and%2520multimodal%2520semantic%2520reliability%252C%2520highlighting%2520a%2520critical%2520limitation%2520of%2520current%2520VLM-based%2520systems%2520and%2520motivating%2520the%2520need%2520for%2520evaluation%2520frameworks%2520that%2520explicitly%2520account%2520for%2520perception%2520uncertainty%2520in%2520safety-critical%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08355v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantic%20Misalignment%20in%20Vision-Language%20Models%20under%20Perceptual%20Degradation&entry.906535625=Guo%20Cheng&entry.1292438233=Vision-Language%20Models%20%28VLMs%29%20are%20increasingly%20deployed%20in%20autonomous%20driving%20and%20embodied%20AI%20systems%2C%20where%20reliable%20perception%20is%20critical%20for%20safe%20semantic%20reasoning%20and%20decision-making.%20While%20recent%20VLMs%20demonstrate%20strong%20performance%20on%20multimodal%20benchmarks%2C%20their%20robustness%20to%20realistic%20perception%20degradation%20remains%20poorly%20understood.%20In%20this%20work%2C%20we%20systematically%20study%20semantic%20misalignment%20in%20VLMs%20under%20controlled%20degradation%20of%20upstream%20visual%20perception%2C%20using%20semantic%20segmentation%20on%20the%20Cityscapes%20dataset%20as%20a%20representative%20perception%20module.%20We%20introduce%20perception-realistic%20corruptions%20that%20induce%20only%20moderate%20drops%20in%20conventional%20segmentation%20metrics%2C%20yet%20observe%20severe%20failures%20in%20downstream%20VLM%20behavior%2C%20including%20hallucinated%20object%20mentions%2C%20omission%20of%20safety-critical%20entities%2C%20and%20inconsistent%20safety%20judgments.%20To%20quantify%20these%20effects%2C%20we%20propose%20a%20set%20of%20language-level%20misalignment%20metrics%20that%20capture%20hallucination%2C%20critical%20omission%2C%20and%20safety%20misinterpretation%2C%20and%20analyze%20their%20relationship%20with%20segmentation%20quality%20across%20multiple%20contrastive%20and%20generative%20VLMs.%20Our%20results%20reveal%20a%20clear%20disconnect%20between%20pixel-level%20robustness%20and%20multimodal%20semantic%20reliability%2C%20highlighting%20a%20critical%20limitation%20of%20current%20VLM-based%20systems%20and%20motivating%20the%20need%20for%20evaluation%20frameworks%20that%20explicitly%20account%20for%20perception%20uncertainty%20in%20safety-critical%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2601.08355v2&entry.124074799=Read"},
{"title": "ChartComplete: A Taxonomy-based Inclusive Chart Dataset", "author": "Ahmad Mustapha and Charbel Toumieh and Mariette Awad", "abstract": "With advancements in deep learning (DL) and computer vision techniques, the field of chart understanding is evolving rapidly. In particular, multimodal large language models (MLLMs) are proving to be efficient and accurate in understanding charts. To accurately measure the performance of MLLMs, the research community has developed multiple datasets to serve as benchmarks. By examining these datasets, we found that they are all limited to a small set of chart types. To bridge this gap, we propose the ChartComplete dataset. The dataset is based on a chart taxonomy borrowed from the visualization community, and it covers thirty different chart types. The dataset is a collection of classified chart images and does not include a learning signal. We present the ChartComplete dataset as is to the community to build upon it.", "link": "http://arxiv.org/abs/2601.10462v1", "date": "2026-01-15", "relevancy": 2.2891, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4837}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4454}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChartComplete%3A%20A%20Taxonomy-based%20Inclusive%20Chart%20Dataset&body=Title%3A%20ChartComplete%3A%20A%20Taxonomy-based%20Inclusive%20Chart%20Dataset%0AAuthor%3A%20Ahmad%20Mustapha%20and%20Charbel%20Toumieh%20and%20Mariette%20Awad%0AAbstract%3A%20With%20advancements%20in%20deep%20learning%20%28DL%29%20and%20computer%20vision%20techniques%2C%20the%20field%20of%20chart%20understanding%20is%20evolving%20rapidly.%20In%20particular%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%20are%20proving%20to%20be%20efficient%20and%20accurate%20in%20understanding%20charts.%20To%20accurately%20measure%20the%20performance%20of%20MLLMs%2C%20the%20research%20community%20has%20developed%20multiple%20datasets%20to%20serve%20as%20benchmarks.%20By%20examining%20these%20datasets%2C%20we%20found%20that%20they%20are%20all%20limited%20to%20a%20small%20set%20of%20chart%20types.%20To%20bridge%20this%20gap%2C%20we%20propose%20the%20ChartComplete%20dataset.%20The%20dataset%20is%20based%20on%20a%20chart%20taxonomy%20borrowed%20from%20the%20visualization%20community%2C%20and%20it%20covers%20thirty%20different%20chart%20types.%20The%20dataset%20is%20a%20collection%20of%20classified%20chart%20images%20and%20does%20not%20include%20a%20learning%20signal.%20We%20present%20the%20ChartComplete%20dataset%20as%20is%20to%20the%20community%20to%20build%20upon%20it.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChartComplete%253A%2520A%2520Taxonomy-based%2520Inclusive%2520Chart%2520Dataset%26entry.906535625%3DAhmad%2520Mustapha%2520and%2520Charbel%2520Toumieh%2520and%2520Mariette%2520Awad%26entry.1292438233%3DWith%2520advancements%2520in%2520deep%2520learning%2520%2528DL%2529%2520and%2520computer%2520vision%2520techniques%252C%2520the%2520field%2520of%2520chart%2520understanding%2520is%2520evolving%2520rapidly.%2520In%2520particular%252C%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529%2520are%2520proving%2520to%2520be%2520efficient%2520and%2520accurate%2520in%2520understanding%2520charts.%2520To%2520accurately%2520measure%2520the%2520performance%2520of%2520MLLMs%252C%2520the%2520research%2520community%2520has%2520developed%2520multiple%2520datasets%2520to%2520serve%2520as%2520benchmarks.%2520By%2520examining%2520these%2520datasets%252C%2520we%2520found%2520that%2520they%2520are%2520all%2520limited%2520to%2520a%2520small%2520set%2520of%2520chart%2520types.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520the%2520ChartComplete%2520dataset.%2520The%2520dataset%2520is%2520based%2520on%2520a%2520chart%2520taxonomy%2520borrowed%2520from%2520the%2520visualization%2520community%252C%2520and%2520it%2520covers%2520thirty%2520different%2520chart%2520types.%2520The%2520dataset%2520is%2520a%2520collection%2520of%2520classified%2520chart%2520images%2520and%2520does%2520not%2520include%2520a%2520learning%2520signal.%2520We%2520present%2520the%2520ChartComplete%2520dataset%2520as%2520is%2520to%2520the%2520community%2520to%2520build%2520upon%2520it.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChartComplete%3A%20A%20Taxonomy-based%20Inclusive%20Chart%20Dataset&entry.906535625=Ahmad%20Mustapha%20and%20Charbel%20Toumieh%20and%20Mariette%20Awad&entry.1292438233=With%20advancements%20in%20deep%20learning%20%28DL%29%20and%20computer%20vision%20techniques%2C%20the%20field%20of%20chart%20understanding%20is%20evolving%20rapidly.%20In%20particular%2C%20multimodal%20large%20language%20models%20%28MLLMs%29%20are%20proving%20to%20be%20efficient%20and%20accurate%20in%20understanding%20charts.%20To%20accurately%20measure%20the%20performance%20of%20MLLMs%2C%20the%20research%20community%20has%20developed%20multiple%20datasets%20to%20serve%20as%20benchmarks.%20By%20examining%20these%20datasets%2C%20we%20found%20that%20they%20are%20all%20limited%20to%20a%20small%20set%20of%20chart%20types.%20To%20bridge%20this%20gap%2C%20we%20propose%20the%20ChartComplete%20dataset.%20The%20dataset%20is%20based%20on%20a%20chart%20taxonomy%20borrowed%20from%20the%20visualization%20community%2C%20and%20it%20covers%20thirty%20different%20chart%20types.%20The%20dataset%20is%20a%20collection%20of%20classified%20chart%20images%20and%20does%20not%20include%20a%20learning%20signal.%20We%20present%20the%20ChartComplete%20dataset%20as%20is%20to%20the%20community%20to%20build%20upon%20it.&entry.1838667208=http%3A//arxiv.org/abs/2601.10462v1&entry.124074799=Read"},
{"title": "Relative Information Gain and Gaussian Process Regression", "author": "Hamish Flynn", "abstract": "The sample complexity of estimating or maximising an unknown function in a reproducing kernel Hilbert space is known to be linked to both the effective dimension and the information gain associated with the kernel. While the information gain has an attractive information-theoretic interpretation, the effective dimension typically results in better rates. We introduce a new quantity called the relative information gain, which measures the sensitivity of the information gain with respect to the observation noise. We show that the relative information gain smoothly interpolates between the effective dimension and the information gain, and that the relative information gain has the same growth rate as the effective dimension. In the second half of the paper, we prove a new PAC-Bayesian excess risk bound for Gaussian process regression. The relative information gain arises naturally from the complexity term in this PAC-Bayesian bound. We prove bounds on the relative information gain that depend on the spectral properties of the kernel. When these upper bounds are combined with our excess risk bound, we obtain minimax-optimal rates of convergence.", "link": "http://arxiv.org/abs/2510.04277v2", "date": "2026-01-15", "relevancy": 2.2877, "topK": [{"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4809}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4504}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4414}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Relative%20Information%20Gain%20and%20Gaussian%20Process%20Regression&body=Title%3A%20Relative%20Information%20Gain%20and%20Gaussian%20Process%20Regression%0AAuthor%3A%20Hamish%20Flynn%0AAbstract%3A%20The%20sample%20complexity%20of%20estimating%20or%20maximising%20an%20unknown%20function%20in%20a%20reproducing%20kernel%20Hilbert%20space%20is%20known%20to%20be%20linked%20to%20both%20the%20effective%20dimension%20and%20the%20information%20gain%20associated%20with%20the%20kernel.%20While%20the%20information%20gain%20has%20an%20attractive%20information-theoretic%20interpretation%2C%20the%20effective%20dimension%20typically%20results%20in%20better%20rates.%20We%20introduce%20a%20new%20quantity%20called%20the%20relative%20information%20gain%2C%20which%20measures%20the%20sensitivity%20of%20the%20information%20gain%20with%20respect%20to%20the%20observation%20noise.%20We%20show%20that%20the%20relative%20information%20gain%20smoothly%20interpolates%20between%20the%20effective%20dimension%20and%20the%20information%20gain%2C%20and%20that%20the%20relative%20information%20gain%20has%20the%20same%20growth%20rate%20as%20the%20effective%20dimension.%20In%20the%20second%20half%20of%20the%20paper%2C%20we%20prove%20a%20new%20PAC-Bayesian%20excess%20risk%20bound%20for%20Gaussian%20process%20regression.%20The%20relative%20information%20gain%20arises%20naturally%20from%20the%20complexity%20term%20in%20this%20PAC-Bayesian%20bound.%20We%20prove%20bounds%20on%20the%20relative%20information%20gain%20that%20depend%20on%20the%20spectral%20properties%20of%20the%20kernel.%20When%20these%20upper%20bounds%20are%20combined%20with%20our%20excess%20risk%20bound%2C%20we%20obtain%20minimax-optimal%20rates%20of%20convergence.%0ALink%3A%20http%3A//arxiv.org/abs/2510.04277v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRelative%2520Information%2520Gain%2520and%2520Gaussian%2520Process%2520Regression%26entry.906535625%3DHamish%2520Flynn%26entry.1292438233%3DThe%2520sample%2520complexity%2520of%2520estimating%2520or%2520maximising%2520an%2520unknown%2520function%2520in%2520a%2520reproducing%2520kernel%2520Hilbert%2520space%2520is%2520known%2520to%2520be%2520linked%2520to%2520both%2520the%2520effective%2520dimension%2520and%2520the%2520information%2520gain%2520associated%2520with%2520the%2520kernel.%2520While%2520the%2520information%2520gain%2520has%2520an%2520attractive%2520information-theoretic%2520interpretation%252C%2520the%2520effective%2520dimension%2520typically%2520results%2520in%2520better%2520rates.%2520We%2520introduce%2520a%2520new%2520quantity%2520called%2520the%2520relative%2520information%2520gain%252C%2520which%2520measures%2520the%2520sensitivity%2520of%2520the%2520information%2520gain%2520with%2520respect%2520to%2520the%2520observation%2520noise.%2520We%2520show%2520that%2520the%2520relative%2520information%2520gain%2520smoothly%2520interpolates%2520between%2520the%2520effective%2520dimension%2520and%2520the%2520information%2520gain%252C%2520and%2520that%2520the%2520relative%2520information%2520gain%2520has%2520the%2520same%2520growth%2520rate%2520as%2520the%2520effective%2520dimension.%2520In%2520the%2520second%2520half%2520of%2520the%2520paper%252C%2520we%2520prove%2520a%2520new%2520PAC-Bayesian%2520excess%2520risk%2520bound%2520for%2520Gaussian%2520process%2520regression.%2520The%2520relative%2520information%2520gain%2520arises%2520naturally%2520from%2520the%2520complexity%2520term%2520in%2520this%2520PAC-Bayesian%2520bound.%2520We%2520prove%2520bounds%2520on%2520the%2520relative%2520information%2520gain%2520that%2520depend%2520on%2520the%2520spectral%2520properties%2520of%2520the%2520kernel.%2520When%2520these%2520upper%2520bounds%2520are%2520combined%2520with%2520our%2520excess%2520risk%2520bound%252C%2520we%2520obtain%2520minimax-optimal%2520rates%2520of%2520convergence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.04277v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Relative%20Information%20Gain%20and%20Gaussian%20Process%20Regression&entry.906535625=Hamish%20Flynn&entry.1292438233=The%20sample%20complexity%20of%20estimating%20or%20maximising%20an%20unknown%20function%20in%20a%20reproducing%20kernel%20Hilbert%20space%20is%20known%20to%20be%20linked%20to%20both%20the%20effective%20dimension%20and%20the%20information%20gain%20associated%20with%20the%20kernel.%20While%20the%20information%20gain%20has%20an%20attractive%20information-theoretic%20interpretation%2C%20the%20effective%20dimension%20typically%20results%20in%20better%20rates.%20We%20introduce%20a%20new%20quantity%20called%20the%20relative%20information%20gain%2C%20which%20measures%20the%20sensitivity%20of%20the%20information%20gain%20with%20respect%20to%20the%20observation%20noise.%20We%20show%20that%20the%20relative%20information%20gain%20smoothly%20interpolates%20between%20the%20effective%20dimension%20and%20the%20information%20gain%2C%20and%20that%20the%20relative%20information%20gain%20has%20the%20same%20growth%20rate%20as%20the%20effective%20dimension.%20In%20the%20second%20half%20of%20the%20paper%2C%20we%20prove%20a%20new%20PAC-Bayesian%20excess%20risk%20bound%20for%20Gaussian%20process%20regression.%20The%20relative%20information%20gain%20arises%20naturally%20from%20the%20complexity%20term%20in%20this%20PAC-Bayesian%20bound.%20We%20prove%20bounds%20on%20the%20relative%20information%20gain%20that%20depend%20on%20the%20spectral%20properties%20of%20the%20kernel.%20When%20these%20upper%20bounds%20are%20combined%20with%20our%20excess%20risk%20bound%2C%20we%20obtain%20minimax-optimal%20rates%20of%20convergence.&entry.1838667208=http%3A//arxiv.org/abs/2510.04277v2&entry.124074799=Read"},
{"title": "Multi-Temporal Frames Projection for Dynamic Processes Fusion in Fluorescence Microscopy", "author": "Hassan Eshkiki and Sarah Costa and Mostafa Mohammadpour and Farinaz Tanhaei and Christopher H. George and Fabio Caraffini", "abstract": "Fluorescence microscopy is widely employed for the analysis of living biological samples; however, the utility of the resulting recordings is frequently constrained by noise, temporal variability, and inconsistent visualisation of signals that oscillate over time. We present a unique computational framework that integrates information from multiple time-resolved frames into a single high-quality image, while preserving the underlying biological content of the original video. We evaluate the proposed method through an extensive number of configurations (n = 111) and on a challenging dataset comprising dynamic, heterogeneous, and morphologically complex 2D monolayers of cardiac cells. Results show that our framework, which consists of a combination of explainable techniques from different computer vision application fields, is capable of generating composite images that preserve and enhance the quality and information of individual microscopy frames, yielding 44% average increase in cell count compared to previous methods. The proposed pipeline is applicable to other imaging domains that require the fusion of multi-temporal image stacks into high-quality 2D images, thereby facilitating annotation and downstream segmentation.", "link": "http://arxiv.org/abs/2601.10392v1", "date": "2026-01-15", "relevancy": 2.285, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5969}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5661}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Temporal%20Frames%20Projection%20for%20Dynamic%20Processes%20Fusion%20in%20Fluorescence%20Microscopy&body=Title%3A%20Multi-Temporal%20Frames%20Projection%20for%20Dynamic%20Processes%20Fusion%20in%20Fluorescence%20Microscopy%0AAuthor%3A%20Hassan%20Eshkiki%20and%20Sarah%20Costa%20and%20Mostafa%20Mohammadpour%20and%20Farinaz%20Tanhaei%20and%20Christopher%20H.%20George%20and%20Fabio%20Caraffini%0AAbstract%3A%20Fluorescence%20microscopy%20is%20widely%20employed%20for%20the%20analysis%20of%20living%20biological%20samples%3B%20however%2C%20the%20utility%20of%20the%20resulting%20recordings%20is%20frequently%20constrained%20by%20noise%2C%20temporal%20variability%2C%20and%20inconsistent%20visualisation%20of%20signals%20that%20oscillate%20over%20time.%20We%20present%20a%20unique%20computational%20framework%20that%20integrates%20information%20from%20multiple%20time-resolved%20frames%20into%20a%20single%20high-quality%20image%2C%20while%20preserving%20the%20underlying%20biological%20content%20of%20the%20original%20video.%20We%20evaluate%20the%20proposed%20method%20through%20an%20extensive%20number%20of%20configurations%20%28n%20%3D%20111%29%20and%20on%20a%20challenging%20dataset%20comprising%20dynamic%2C%20heterogeneous%2C%20and%20morphologically%20complex%202D%20monolayers%20of%20cardiac%20cells.%20Results%20show%20that%20our%20framework%2C%20which%20consists%20of%20a%20combination%20of%20explainable%20techniques%20from%20different%20computer%20vision%20application%20fields%2C%20is%20capable%20of%20generating%20composite%20images%20that%20preserve%20and%20enhance%20the%20quality%20and%20information%20of%20individual%20microscopy%20frames%2C%20yielding%2044%25%20average%20increase%20in%20cell%20count%20compared%20to%20previous%20methods.%20The%20proposed%20pipeline%20is%20applicable%20to%20other%20imaging%20domains%20that%20require%20the%20fusion%20of%20multi-temporal%20image%20stacks%20into%20high-quality%202D%20images%2C%20thereby%20facilitating%20annotation%20and%20downstream%20segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10392v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Temporal%2520Frames%2520Projection%2520for%2520Dynamic%2520Processes%2520Fusion%2520in%2520Fluorescence%2520Microscopy%26entry.906535625%3DHassan%2520Eshkiki%2520and%2520Sarah%2520Costa%2520and%2520Mostafa%2520Mohammadpour%2520and%2520Farinaz%2520Tanhaei%2520and%2520Christopher%2520H.%2520George%2520and%2520Fabio%2520Caraffini%26entry.1292438233%3DFluorescence%2520microscopy%2520is%2520widely%2520employed%2520for%2520the%2520analysis%2520of%2520living%2520biological%2520samples%253B%2520however%252C%2520the%2520utility%2520of%2520the%2520resulting%2520recordings%2520is%2520frequently%2520constrained%2520by%2520noise%252C%2520temporal%2520variability%252C%2520and%2520inconsistent%2520visualisation%2520of%2520signals%2520that%2520oscillate%2520over%2520time.%2520We%2520present%2520a%2520unique%2520computational%2520framework%2520that%2520integrates%2520information%2520from%2520multiple%2520time-resolved%2520frames%2520into%2520a%2520single%2520high-quality%2520image%252C%2520while%2520preserving%2520the%2520underlying%2520biological%2520content%2520of%2520the%2520original%2520video.%2520We%2520evaluate%2520the%2520proposed%2520method%2520through%2520an%2520extensive%2520number%2520of%2520configurations%2520%2528n%2520%253D%2520111%2529%2520and%2520on%2520a%2520challenging%2520dataset%2520comprising%2520dynamic%252C%2520heterogeneous%252C%2520and%2520morphologically%2520complex%25202D%2520monolayers%2520of%2520cardiac%2520cells.%2520Results%2520show%2520that%2520our%2520framework%252C%2520which%2520consists%2520of%2520a%2520combination%2520of%2520explainable%2520techniques%2520from%2520different%2520computer%2520vision%2520application%2520fields%252C%2520is%2520capable%2520of%2520generating%2520composite%2520images%2520that%2520preserve%2520and%2520enhance%2520the%2520quality%2520and%2520information%2520of%2520individual%2520microscopy%2520frames%252C%2520yielding%252044%2525%2520average%2520increase%2520in%2520cell%2520count%2520compared%2520to%2520previous%2520methods.%2520The%2520proposed%2520pipeline%2520is%2520applicable%2520to%2520other%2520imaging%2520domains%2520that%2520require%2520the%2520fusion%2520of%2520multi-temporal%2520image%2520stacks%2520into%2520high-quality%25202D%2520images%252C%2520thereby%2520facilitating%2520annotation%2520and%2520downstream%2520segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10392v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Temporal%20Frames%20Projection%20for%20Dynamic%20Processes%20Fusion%20in%20Fluorescence%20Microscopy&entry.906535625=Hassan%20Eshkiki%20and%20Sarah%20Costa%20and%20Mostafa%20Mohammadpour%20and%20Farinaz%20Tanhaei%20and%20Christopher%20H.%20George%20and%20Fabio%20Caraffini&entry.1292438233=Fluorescence%20microscopy%20is%20widely%20employed%20for%20the%20analysis%20of%20living%20biological%20samples%3B%20however%2C%20the%20utility%20of%20the%20resulting%20recordings%20is%20frequently%20constrained%20by%20noise%2C%20temporal%20variability%2C%20and%20inconsistent%20visualisation%20of%20signals%20that%20oscillate%20over%20time.%20We%20present%20a%20unique%20computational%20framework%20that%20integrates%20information%20from%20multiple%20time-resolved%20frames%20into%20a%20single%20high-quality%20image%2C%20while%20preserving%20the%20underlying%20biological%20content%20of%20the%20original%20video.%20We%20evaluate%20the%20proposed%20method%20through%20an%20extensive%20number%20of%20configurations%20%28n%20%3D%20111%29%20and%20on%20a%20challenging%20dataset%20comprising%20dynamic%2C%20heterogeneous%2C%20and%20morphologically%20complex%202D%20monolayers%20of%20cardiac%20cells.%20Results%20show%20that%20our%20framework%2C%20which%20consists%20of%20a%20combination%20of%20explainable%20techniques%20from%20different%20computer%20vision%20application%20fields%2C%20is%20capable%20of%20generating%20composite%20images%20that%20preserve%20and%20enhance%20the%20quality%20and%20information%20of%20individual%20microscopy%20frames%2C%20yielding%2044%25%20average%20increase%20in%20cell%20count%20compared%20to%20previous%20methods.%20The%20proposed%20pipeline%20is%20applicable%20to%20other%20imaging%20domains%20that%20require%20the%20fusion%20of%20multi-temporal%20image%20stacks%20into%20high-quality%202D%20images%2C%20thereby%20facilitating%20annotation%20and%20downstream%20segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2601.10392v1&entry.124074799=Read"},
{"title": "Global Context Compression with Interleaved Vision-Text Transformation", "author": "Dian Jiao and Jiaxin Duan and Shuai Zhao and Jiabing Leng and Yiran Zhang and Feng Huang", "abstract": "Recent achievements of vision-language models in end-to-end OCR point to a new avenue for low-loss compression of textual information. This motivates earlier works that render the Transformer's input into images for prefilling, which effectively reduces the number of tokens through visual encoding, thereby alleviating the quadratically increased Attention computations. However, this partial compression fails to save computational or memory costs at token-by-token inference. In this paper, we investigate global context compression, which saves tokens at both prefilling and inference stages. Consequently, we propose VIST2, a novel Transformer that interleaves input text chunks alongside their visual encoding, while depending exclusively on visual tokens in the pre-context to predict the next text token distribution. Around this idea, we render text chunks into sketch images and train VIST2 in multiple stages, starting from curriculum-scheduled pretraining for optical language modeling, followed by modal-interleaved instruction tuning. We conduct extensive experiments using VIST2 families scaled from 0.6B to 8B to explore the training recipe and hyperparameters. With a 4$\\times$ compression ratio, the resulting models demonstrate significant superiority over baselines on long writing tasks, achieving, on average, a 3$\\times$ speedup in first-token generation, 77% reduction in memory usage, and 74% reduction in FLOPS. Our codes and datasets will be public to support further studies.", "link": "http://arxiv.org/abs/2601.10378v1", "date": "2026-01-15", "relevancy": 2.2837, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5811}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5689}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Context%20Compression%20with%20Interleaved%20Vision-Text%20Transformation&body=Title%3A%20Global%20Context%20Compression%20with%20Interleaved%20Vision-Text%20Transformation%0AAuthor%3A%20Dian%20Jiao%20and%20Jiaxin%20Duan%20and%20Shuai%20Zhao%20and%20Jiabing%20Leng%20and%20Yiran%20Zhang%20and%20Feng%20Huang%0AAbstract%3A%20Recent%20achievements%20of%20vision-language%20models%20in%20end-to-end%20OCR%20point%20to%20a%20new%20avenue%20for%20low-loss%20compression%20of%20textual%20information.%20This%20motivates%20earlier%20works%20that%20render%20the%20Transformer%27s%20input%20into%20images%20for%20prefilling%2C%20which%20effectively%20reduces%20the%20number%20of%20tokens%20through%20visual%20encoding%2C%20thereby%20alleviating%20the%20quadratically%20increased%20Attention%20computations.%20However%2C%20this%20partial%20compression%20fails%20to%20save%20computational%20or%20memory%20costs%20at%20token-by-token%20inference.%20In%20this%20paper%2C%20we%20investigate%20global%20context%20compression%2C%20which%20saves%20tokens%20at%20both%20prefilling%20and%20inference%20stages.%20Consequently%2C%20we%20propose%20VIST2%2C%20a%20novel%20Transformer%20that%20interleaves%20input%20text%20chunks%20alongside%20their%20visual%20encoding%2C%20while%20depending%20exclusively%20on%20visual%20tokens%20in%20the%20pre-context%20to%20predict%20the%20next%20text%20token%20distribution.%20Around%20this%20idea%2C%20we%20render%20text%20chunks%20into%20sketch%20images%20and%20train%20VIST2%20in%20multiple%20stages%2C%20starting%20from%20curriculum-scheduled%20pretraining%20for%20optical%20language%20modeling%2C%20followed%20by%20modal-interleaved%20instruction%20tuning.%20We%20conduct%20extensive%20experiments%20using%20VIST2%20families%20scaled%20from%200.6B%20to%208B%20to%20explore%20the%20training%20recipe%20and%20hyperparameters.%20With%20a%204%24%5Ctimes%24%20compression%20ratio%2C%20the%20resulting%20models%20demonstrate%20significant%20superiority%20over%20baselines%20on%20long%20writing%20tasks%2C%20achieving%2C%20on%20average%2C%20a%203%24%5Ctimes%24%20speedup%20in%20first-token%20generation%2C%2077%25%20reduction%20in%20memory%20usage%2C%20and%2074%25%20reduction%20in%20FLOPS.%20Our%20codes%20and%20datasets%20will%20be%20public%20to%20support%20further%20studies.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10378v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Context%2520Compression%2520with%2520Interleaved%2520Vision-Text%2520Transformation%26entry.906535625%3DDian%2520Jiao%2520and%2520Jiaxin%2520Duan%2520and%2520Shuai%2520Zhao%2520and%2520Jiabing%2520Leng%2520and%2520Yiran%2520Zhang%2520and%2520Feng%2520Huang%26entry.1292438233%3DRecent%2520achievements%2520of%2520vision-language%2520models%2520in%2520end-to-end%2520OCR%2520point%2520to%2520a%2520new%2520avenue%2520for%2520low-loss%2520compression%2520of%2520textual%2520information.%2520This%2520motivates%2520earlier%2520works%2520that%2520render%2520the%2520Transformer%2527s%2520input%2520into%2520images%2520for%2520prefilling%252C%2520which%2520effectively%2520reduces%2520the%2520number%2520of%2520tokens%2520through%2520visual%2520encoding%252C%2520thereby%2520alleviating%2520the%2520quadratically%2520increased%2520Attention%2520computations.%2520However%252C%2520this%2520partial%2520compression%2520fails%2520to%2520save%2520computational%2520or%2520memory%2520costs%2520at%2520token-by-token%2520inference.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520global%2520context%2520compression%252C%2520which%2520saves%2520tokens%2520at%2520both%2520prefilling%2520and%2520inference%2520stages.%2520Consequently%252C%2520we%2520propose%2520VIST2%252C%2520a%2520novel%2520Transformer%2520that%2520interleaves%2520input%2520text%2520chunks%2520alongside%2520their%2520visual%2520encoding%252C%2520while%2520depending%2520exclusively%2520on%2520visual%2520tokens%2520in%2520the%2520pre-context%2520to%2520predict%2520the%2520next%2520text%2520token%2520distribution.%2520Around%2520this%2520idea%252C%2520we%2520render%2520text%2520chunks%2520into%2520sketch%2520images%2520and%2520train%2520VIST2%2520in%2520multiple%2520stages%252C%2520starting%2520from%2520curriculum-scheduled%2520pretraining%2520for%2520optical%2520language%2520modeling%252C%2520followed%2520by%2520modal-interleaved%2520instruction%2520tuning.%2520We%2520conduct%2520extensive%2520experiments%2520using%2520VIST2%2520families%2520scaled%2520from%25200.6B%2520to%25208B%2520to%2520explore%2520the%2520training%2520recipe%2520and%2520hyperparameters.%2520With%2520a%25204%2524%255Ctimes%2524%2520compression%2520ratio%252C%2520the%2520resulting%2520models%2520demonstrate%2520significant%2520superiority%2520over%2520baselines%2520on%2520long%2520writing%2520tasks%252C%2520achieving%252C%2520on%2520average%252C%2520a%25203%2524%255Ctimes%2524%2520speedup%2520in%2520first-token%2520generation%252C%252077%2525%2520reduction%2520in%2520memory%2520usage%252C%2520and%252074%2525%2520reduction%2520in%2520FLOPS.%2520Our%2520codes%2520and%2520datasets%2520will%2520be%2520public%2520to%2520support%2520further%2520studies.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10378v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Context%20Compression%20with%20Interleaved%20Vision-Text%20Transformation&entry.906535625=Dian%20Jiao%20and%20Jiaxin%20Duan%20and%20Shuai%20Zhao%20and%20Jiabing%20Leng%20and%20Yiran%20Zhang%20and%20Feng%20Huang&entry.1292438233=Recent%20achievements%20of%20vision-language%20models%20in%20end-to-end%20OCR%20point%20to%20a%20new%20avenue%20for%20low-loss%20compression%20of%20textual%20information.%20This%20motivates%20earlier%20works%20that%20render%20the%20Transformer%27s%20input%20into%20images%20for%20prefilling%2C%20which%20effectively%20reduces%20the%20number%20of%20tokens%20through%20visual%20encoding%2C%20thereby%20alleviating%20the%20quadratically%20increased%20Attention%20computations.%20However%2C%20this%20partial%20compression%20fails%20to%20save%20computational%20or%20memory%20costs%20at%20token-by-token%20inference.%20In%20this%20paper%2C%20we%20investigate%20global%20context%20compression%2C%20which%20saves%20tokens%20at%20both%20prefilling%20and%20inference%20stages.%20Consequently%2C%20we%20propose%20VIST2%2C%20a%20novel%20Transformer%20that%20interleaves%20input%20text%20chunks%20alongside%20their%20visual%20encoding%2C%20while%20depending%20exclusively%20on%20visual%20tokens%20in%20the%20pre-context%20to%20predict%20the%20next%20text%20token%20distribution.%20Around%20this%20idea%2C%20we%20render%20text%20chunks%20into%20sketch%20images%20and%20train%20VIST2%20in%20multiple%20stages%2C%20starting%20from%20curriculum-scheduled%20pretraining%20for%20optical%20language%20modeling%2C%20followed%20by%20modal-interleaved%20instruction%20tuning.%20We%20conduct%20extensive%20experiments%20using%20VIST2%20families%20scaled%20from%200.6B%20to%208B%20to%20explore%20the%20training%20recipe%20and%20hyperparameters.%20With%20a%204%24%5Ctimes%24%20compression%20ratio%2C%20the%20resulting%20models%20demonstrate%20significant%20superiority%20over%20baselines%20on%20long%20writing%20tasks%2C%20achieving%2C%20on%20average%2C%20a%203%24%5Ctimes%24%20speedup%20in%20first-token%20generation%2C%2077%25%20reduction%20in%20memory%20usage%2C%20and%2074%25%20reduction%20in%20FLOPS.%20Our%20codes%20and%20datasets%20will%20be%20public%20to%20support%20further%20studies.&entry.1838667208=http%3A//arxiv.org/abs/2601.10378v1&entry.124074799=Read"},
{"title": "TranslateGemma Technical Report", "author": "Mara Finkelstein and Isaac Caswell and Tobias Domhan and Jan-Thorsten Peter and Juraj Juraska and Parker Riley and Daniel Deutsch and Cole Dilanni and Colin Cherry and Eleftheria Briakou and Elizabeth Nielsen and Jiaming Luo and Kat Black and Ryan Mullins and Sweta Agrawal and Wenda Xu and Erin Kats and Stephane Jaskiewicz and Markus Freitag and David Vilar", "abstract": "We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.", "link": "http://arxiv.org/abs/2601.09012v2", "date": "2026-01-15", "relevancy": 2.274, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4744}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4453}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4447}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TranslateGemma%20Technical%20Report&body=Title%3A%20TranslateGemma%20Technical%20Report%0AAuthor%3A%20Mara%20Finkelstein%20and%20Isaac%20Caswell%20and%20Tobias%20Domhan%20and%20Jan-Thorsten%20Peter%20and%20Juraj%20Juraska%20and%20Parker%20Riley%20and%20Daniel%20Deutsch%20and%20Cole%20Dilanni%20and%20Colin%20Cherry%20and%20Eleftheria%20Briakou%20and%20Elizabeth%20Nielsen%20and%20Jiaming%20Luo%20and%20Kat%20Black%20and%20Ryan%20Mullins%20and%20Sweta%20Agrawal%20and%20Wenda%20Xu%20and%20Erin%20Kats%20and%20Stephane%20Jaskiewicz%20and%20Markus%20Freitag%20and%20David%20Vilar%0AAbstract%3A%20We%20present%20TranslateGemma%2C%20a%20suite%20of%20open%20machine%20translation%20models%20based%20on%20the%20Gemma%203%20foundation%20models.%20To%20enhance%20the%20inherent%20multilingual%20capabilities%20of%20Gemma%203%20for%20the%20translation%20task%2C%20we%20employ%20a%20two-stage%20fine-tuning%20process.%20First%2C%20supervised%20fine-tuning%20is%20performed%20using%20a%20rich%20mixture%20of%20high-quality%20large-scale%20synthetic%20parallel%20data%20generated%20via%20state-of-the-art%20models%20and%20human-translated%20parallel%20data.%20This%20is%20followed%20by%20a%20reinforcement%20learning%20phase%2C%20where%20we%20optimize%20translation%20quality%20using%20an%20ensemble%20of%20reward%20models%2C%20including%20MetricX-QE%20and%20AutoMQM%2C%20targeting%20translation%20quality.%20We%20demonstrate%20the%20effectiveness%20of%20TranslateGemma%20with%20human%20evaluation%20on%20the%20WMT25%20test%20set%20across%2010%20language%20pairs%20and%20with%20automatic%20evaluation%20on%20the%20WMT24%2B%2B%20benchmark%20across%2055%20language%20pairs.%20Automatic%20metrics%20show%20consistent%20and%20substantial%20gains%20over%20the%20baseline%20Gemma%203%20models%20across%20all%20sizes.%20Notably%2C%20smaller%20TranslateGemma%20models%20often%20achieve%20performance%20comparable%20to%20larger%20baseline%20models%2C%20offering%20improved%20efficiency.%20We%20also%20show%20that%20TranslateGemma%20models%20retain%20strong%20multimodal%20capabilities%2C%20with%20enhanced%20performance%20on%20the%20Vistra%20image%20translation%20benchmark.%20The%20release%20of%20the%20open%20TranslateGemma%20models%20aims%20to%20provide%20the%20research%20community%20with%20powerful%20and%20adaptable%20tools%20for%20machine%20translation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09012v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTranslateGemma%2520Technical%2520Report%26entry.906535625%3DMara%2520Finkelstein%2520and%2520Isaac%2520Caswell%2520and%2520Tobias%2520Domhan%2520and%2520Jan-Thorsten%2520Peter%2520and%2520Juraj%2520Juraska%2520and%2520Parker%2520Riley%2520and%2520Daniel%2520Deutsch%2520and%2520Cole%2520Dilanni%2520and%2520Colin%2520Cherry%2520and%2520Eleftheria%2520Briakou%2520and%2520Elizabeth%2520Nielsen%2520and%2520Jiaming%2520Luo%2520and%2520Kat%2520Black%2520and%2520Ryan%2520Mullins%2520and%2520Sweta%2520Agrawal%2520and%2520Wenda%2520Xu%2520and%2520Erin%2520Kats%2520and%2520Stephane%2520Jaskiewicz%2520and%2520Markus%2520Freitag%2520and%2520David%2520Vilar%26entry.1292438233%3DWe%2520present%2520TranslateGemma%252C%2520a%2520suite%2520of%2520open%2520machine%2520translation%2520models%2520based%2520on%2520the%2520Gemma%25203%2520foundation%2520models.%2520To%2520enhance%2520the%2520inherent%2520multilingual%2520capabilities%2520of%2520Gemma%25203%2520for%2520the%2520translation%2520task%252C%2520we%2520employ%2520a%2520two-stage%2520fine-tuning%2520process.%2520First%252C%2520supervised%2520fine-tuning%2520is%2520performed%2520using%2520a%2520rich%2520mixture%2520of%2520high-quality%2520large-scale%2520synthetic%2520parallel%2520data%2520generated%2520via%2520state-of-the-art%2520models%2520and%2520human-translated%2520parallel%2520data.%2520This%2520is%2520followed%2520by%2520a%2520reinforcement%2520learning%2520phase%252C%2520where%2520we%2520optimize%2520translation%2520quality%2520using%2520an%2520ensemble%2520of%2520reward%2520models%252C%2520including%2520MetricX-QE%2520and%2520AutoMQM%252C%2520targeting%2520translation%2520quality.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520TranslateGemma%2520with%2520human%2520evaluation%2520on%2520the%2520WMT25%2520test%2520set%2520across%252010%2520language%2520pairs%2520and%2520with%2520automatic%2520evaluation%2520on%2520the%2520WMT24%252B%252B%2520benchmark%2520across%252055%2520language%2520pairs.%2520Automatic%2520metrics%2520show%2520consistent%2520and%2520substantial%2520gains%2520over%2520the%2520baseline%2520Gemma%25203%2520models%2520across%2520all%2520sizes.%2520Notably%252C%2520smaller%2520TranslateGemma%2520models%2520often%2520achieve%2520performance%2520comparable%2520to%2520larger%2520baseline%2520models%252C%2520offering%2520improved%2520efficiency.%2520We%2520also%2520show%2520that%2520TranslateGemma%2520models%2520retain%2520strong%2520multimodal%2520capabilities%252C%2520with%2520enhanced%2520performance%2520on%2520the%2520Vistra%2520image%2520translation%2520benchmark.%2520The%2520release%2520of%2520the%2520open%2520TranslateGemma%2520models%2520aims%2520to%2520provide%2520the%2520research%2520community%2520with%2520powerful%2520and%2520adaptable%2520tools%2520for%2520machine%2520translation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09012v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TranslateGemma%20Technical%20Report&entry.906535625=Mara%20Finkelstein%20and%20Isaac%20Caswell%20and%20Tobias%20Domhan%20and%20Jan-Thorsten%20Peter%20and%20Juraj%20Juraska%20and%20Parker%20Riley%20and%20Daniel%20Deutsch%20and%20Cole%20Dilanni%20and%20Colin%20Cherry%20and%20Eleftheria%20Briakou%20and%20Elizabeth%20Nielsen%20and%20Jiaming%20Luo%20and%20Kat%20Black%20and%20Ryan%20Mullins%20and%20Sweta%20Agrawal%20and%20Wenda%20Xu%20and%20Erin%20Kats%20and%20Stephane%20Jaskiewicz%20and%20Markus%20Freitag%20and%20David%20Vilar&entry.1292438233=We%20present%20TranslateGemma%2C%20a%20suite%20of%20open%20machine%20translation%20models%20based%20on%20the%20Gemma%203%20foundation%20models.%20To%20enhance%20the%20inherent%20multilingual%20capabilities%20of%20Gemma%203%20for%20the%20translation%20task%2C%20we%20employ%20a%20two-stage%20fine-tuning%20process.%20First%2C%20supervised%20fine-tuning%20is%20performed%20using%20a%20rich%20mixture%20of%20high-quality%20large-scale%20synthetic%20parallel%20data%20generated%20via%20state-of-the-art%20models%20and%20human-translated%20parallel%20data.%20This%20is%20followed%20by%20a%20reinforcement%20learning%20phase%2C%20where%20we%20optimize%20translation%20quality%20using%20an%20ensemble%20of%20reward%20models%2C%20including%20MetricX-QE%20and%20AutoMQM%2C%20targeting%20translation%20quality.%20We%20demonstrate%20the%20effectiveness%20of%20TranslateGemma%20with%20human%20evaluation%20on%20the%20WMT25%20test%20set%20across%2010%20language%20pairs%20and%20with%20automatic%20evaluation%20on%20the%20WMT24%2B%2B%20benchmark%20across%2055%20language%20pairs.%20Automatic%20metrics%20show%20consistent%20and%20substantial%20gains%20over%20the%20baseline%20Gemma%203%20models%20across%20all%20sizes.%20Notably%2C%20smaller%20TranslateGemma%20models%20often%20achieve%20performance%20comparable%20to%20larger%20baseline%20models%2C%20offering%20improved%20efficiency.%20We%20also%20show%20that%20TranslateGemma%20models%20retain%20strong%20multimodal%20capabilities%2C%20with%20enhanced%20performance%20on%20the%20Vistra%20image%20translation%20benchmark.%20The%20release%20of%20the%20open%20TranslateGemma%20models%20aims%20to%20provide%20the%20research%20community%20with%20powerful%20and%20adaptable%20tools%20for%20machine%20translation.&entry.1838667208=http%3A//arxiv.org/abs/2601.09012v2&entry.124074799=Read"},
{"title": "Action100M: A Large-scale Video Action Dataset", "author": "Delong Chen and Tejaswi Kasarla and Yejin Bang and Mustafa Shukor and Willy Chung and Jade Yu and Allen Bolourchi and Theo Moutakanni and Pascale Fung", "abstract": "Inferring physical actions from visual observations is a fundamental capability for advancing machine intelligence in the physical world. Achieving this requires large-scale, open-vocabulary video action datasets that span broad domains. We introduce Action100M, a large-scale dataset constructed from 1.2M Internet instructional videos (14.6 years of duration), yielding O(100 million) temporally localized segments with open-vocabulary action supervision and rich captions. Action100M is generated by a fully automated pipeline that (i) performs hierarchical temporal segmentation using V-JEPA 2 embeddings, (ii) produces multi-level frame and segment captions organized as a Tree-of-Captions, and (iii) aggregates evidence with a reasoning model (GPT-OSS-120B) under a multi-round Self-Refine procedure to output structured annotations (brief/detailed action, actor, brief/detailed caption). Training VL-JEPA on Action100M demonstrates consistent data-scaling improvements and strong zero-shot performance across diverse action recognition benchmarks, establishing Action100M as a new foundation for scalable research in video understanding and world modeling.", "link": "http://arxiv.org/abs/2601.10592v1", "date": "2026-01-15", "relevancy": 2.2554, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5771}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5567}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5485}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Action100M%3A%20A%20Large-scale%20Video%20Action%20Dataset&body=Title%3A%20Action100M%3A%20A%20Large-scale%20Video%20Action%20Dataset%0AAuthor%3A%20Delong%20Chen%20and%20Tejaswi%20Kasarla%20and%20Yejin%20Bang%20and%20Mustafa%20Shukor%20and%20Willy%20Chung%20and%20Jade%20Yu%20and%20Allen%20Bolourchi%20and%20Theo%20Moutakanni%20and%20Pascale%20Fung%0AAbstract%3A%20Inferring%20physical%20actions%20from%20visual%20observations%20is%20a%20fundamental%20capability%20for%20advancing%20machine%20intelligence%20in%20the%20physical%20world.%20Achieving%20this%20requires%20large-scale%2C%20open-vocabulary%20video%20action%20datasets%20that%20span%20broad%20domains.%20We%20introduce%20Action100M%2C%20a%20large-scale%20dataset%20constructed%20from%201.2M%20Internet%20instructional%20videos%20%2814.6%20years%20of%20duration%29%2C%20yielding%20O%28100%20million%29%20temporally%20localized%20segments%20with%20open-vocabulary%20action%20supervision%20and%20rich%20captions.%20Action100M%20is%20generated%20by%20a%20fully%20automated%20pipeline%20that%20%28i%29%20performs%20hierarchical%20temporal%20segmentation%20using%20V-JEPA%202%20embeddings%2C%20%28ii%29%20produces%20multi-level%20frame%20and%20segment%20captions%20organized%20as%20a%20Tree-of-Captions%2C%20and%20%28iii%29%20aggregates%20evidence%20with%20a%20reasoning%20model%20%28GPT-OSS-120B%29%20under%20a%20multi-round%20Self-Refine%20procedure%20to%20output%20structured%20annotations%20%28brief/detailed%20action%2C%20actor%2C%20brief/detailed%20caption%29.%20Training%20VL-JEPA%20on%20Action100M%20demonstrates%20consistent%20data-scaling%20improvements%20and%20strong%20zero-shot%20performance%20across%20diverse%20action%20recognition%20benchmarks%2C%20establishing%20Action100M%20as%20a%20new%20foundation%20for%20scalable%20research%20in%20video%20understanding%20and%20world%20modeling.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10592v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAction100M%253A%2520A%2520Large-scale%2520Video%2520Action%2520Dataset%26entry.906535625%3DDelong%2520Chen%2520and%2520Tejaswi%2520Kasarla%2520and%2520Yejin%2520Bang%2520and%2520Mustafa%2520Shukor%2520and%2520Willy%2520Chung%2520and%2520Jade%2520Yu%2520and%2520Allen%2520Bolourchi%2520and%2520Theo%2520Moutakanni%2520and%2520Pascale%2520Fung%26entry.1292438233%3DInferring%2520physical%2520actions%2520from%2520visual%2520observations%2520is%2520a%2520fundamental%2520capability%2520for%2520advancing%2520machine%2520intelligence%2520in%2520the%2520physical%2520world.%2520Achieving%2520this%2520requires%2520large-scale%252C%2520open-vocabulary%2520video%2520action%2520datasets%2520that%2520span%2520broad%2520domains.%2520We%2520introduce%2520Action100M%252C%2520a%2520large-scale%2520dataset%2520constructed%2520from%25201.2M%2520Internet%2520instructional%2520videos%2520%252814.6%2520years%2520of%2520duration%2529%252C%2520yielding%2520O%2528100%2520million%2529%2520temporally%2520localized%2520segments%2520with%2520open-vocabulary%2520action%2520supervision%2520and%2520rich%2520captions.%2520Action100M%2520is%2520generated%2520by%2520a%2520fully%2520automated%2520pipeline%2520that%2520%2528i%2529%2520performs%2520hierarchical%2520temporal%2520segmentation%2520using%2520V-JEPA%25202%2520embeddings%252C%2520%2528ii%2529%2520produces%2520multi-level%2520frame%2520and%2520segment%2520captions%2520organized%2520as%2520a%2520Tree-of-Captions%252C%2520and%2520%2528iii%2529%2520aggregates%2520evidence%2520with%2520a%2520reasoning%2520model%2520%2528GPT-OSS-120B%2529%2520under%2520a%2520multi-round%2520Self-Refine%2520procedure%2520to%2520output%2520structured%2520annotations%2520%2528brief/detailed%2520action%252C%2520actor%252C%2520brief/detailed%2520caption%2529.%2520Training%2520VL-JEPA%2520on%2520Action100M%2520demonstrates%2520consistent%2520data-scaling%2520improvements%2520and%2520strong%2520zero-shot%2520performance%2520across%2520diverse%2520action%2520recognition%2520benchmarks%252C%2520establishing%2520Action100M%2520as%2520a%2520new%2520foundation%2520for%2520scalable%2520research%2520in%2520video%2520understanding%2520and%2520world%2520modeling.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10592v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Action100M%3A%20A%20Large-scale%20Video%20Action%20Dataset&entry.906535625=Delong%20Chen%20and%20Tejaswi%20Kasarla%20and%20Yejin%20Bang%20and%20Mustafa%20Shukor%20and%20Willy%20Chung%20and%20Jade%20Yu%20and%20Allen%20Bolourchi%20and%20Theo%20Moutakanni%20and%20Pascale%20Fung&entry.1292438233=Inferring%20physical%20actions%20from%20visual%20observations%20is%20a%20fundamental%20capability%20for%20advancing%20machine%20intelligence%20in%20the%20physical%20world.%20Achieving%20this%20requires%20large-scale%2C%20open-vocabulary%20video%20action%20datasets%20that%20span%20broad%20domains.%20We%20introduce%20Action100M%2C%20a%20large-scale%20dataset%20constructed%20from%201.2M%20Internet%20instructional%20videos%20%2814.6%20years%20of%20duration%29%2C%20yielding%20O%28100%20million%29%20temporally%20localized%20segments%20with%20open-vocabulary%20action%20supervision%20and%20rich%20captions.%20Action100M%20is%20generated%20by%20a%20fully%20automated%20pipeline%20that%20%28i%29%20performs%20hierarchical%20temporal%20segmentation%20using%20V-JEPA%202%20embeddings%2C%20%28ii%29%20produces%20multi-level%20frame%20and%20segment%20captions%20organized%20as%20a%20Tree-of-Captions%2C%20and%20%28iii%29%20aggregates%20evidence%20with%20a%20reasoning%20model%20%28GPT-OSS-120B%29%20under%20a%20multi-round%20Self-Refine%20procedure%20to%20output%20structured%20annotations%20%28brief/detailed%20action%2C%20actor%2C%20brief/detailed%20caption%29.%20Training%20VL-JEPA%20on%20Action100M%20demonstrates%20consistent%20data-scaling%20improvements%20and%20strong%20zero-shot%20performance%20across%20diverse%20action%20recognition%20benchmarks%2C%20establishing%20Action100M%20as%20a%20new%20foundation%20for%20scalable%20research%20in%20video%20understanding%20and%20world%20modeling.&entry.1838667208=http%3A//arxiv.org/abs/2601.10592v1&entry.124074799=Read"},
{"title": "Can LLMs Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels", "author": "Anika Sharma and Malavika Mampally and Chidaksh Ravuru and Kandyce Brennan and Neil Gaikwad", "abstract": "As Large Language Models (LLMs) increasingly mediate stigmatized health decisions, their capacity to understand complex psychological phenomena remains inadequately assessed. Can LLMs understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across cognitive, interpersonal, and structural levels. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS), examining representation at cognitive (self-judgment), interpersonal (worries about judgment and isolation), and structural (community condemnation and disclosure patterns) levels. Models fail tests of genuine understanding across all dimensions. They underestimate cognitive stigma while overestimating interpersonal stigma, introduce demographic biases assigning higher stigma to younger, less educated, and non-White personas, and treat secrecy as universal despite 36% of humans reporting openness. Most critically, models produce internal contradictions: they overestimate isolation yet predict isolated individuals are less secretive, revealing incoherent representations. These patterns show current alignment approaches ensure appropriate language but not coherent understanding across levels. This work provides empirical evidence that LLMs lack coherent understanding of psychological constructs operating across multiple dimensions. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.", "link": "http://arxiv.org/abs/2512.13142v4", "date": "2026-01-15", "relevancy": 2.2424, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4556}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4556}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20LLMs%20Understand%20What%20We%20Cannot%20Say%3F%20Measuring%20Multilevel%20Alignment%20Through%20Abortion%20Stigma%20Across%20Cognitive%2C%20Interpersonal%2C%20and%20Structural%20Levels&body=Title%3A%20Can%20LLMs%20Understand%20What%20We%20Cannot%20Say%3F%20Measuring%20Multilevel%20Alignment%20Through%20Abortion%20Stigma%20Across%20Cognitive%2C%20Interpersonal%2C%20and%20Structural%20Levels%0AAuthor%3A%20Anika%20Sharma%20and%20Malavika%20Mampally%20and%20Chidaksh%20Ravuru%20and%20Kandyce%20Brennan%20and%20Neil%20Gaikwad%0AAbstract%3A%20As%20Large%20Language%20Models%20%28LLMs%29%20increasingly%20mediate%20stigmatized%20health%20decisions%2C%20their%20capacity%20to%20understand%20complex%20psychological%20phenomena%20remains%20inadequately%20assessed.%20Can%20LLMs%20understand%20what%20we%20cannot%20say%3F%20We%20investigate%20whether%20LLMs%20coherently%20represent%20abortion%20stigma%20across%20cognitive%2C%20interpersonal%2C%20and%20structural%20levels.%20We%20systematically%20tested%20627%20demographically%20diverse%20personas%20across%20five%20leading%20LLMs%20using%20the%20validated%20Individual%20Level%20Abortion%20Stigma%20Scale%20%28ILAS%29%2C%20examining%20representation%20at%20cognitive%20%28self-judgment%29%2C%20interpersonal%20%28worries%20about%20judgment%20and%20isolation%29%2C%20and%20structural%20%28community%20condemnation%20and%20disclosure%20patterns%29%20levels.%20Models%20fail%20tests%20of%20genuine%20understanding%20across%20all%20dimensions.%20They%20underestimate%20cognitive%20stigma%20while%20overestimating%20interpersonal%20stigma%2C%20introduce%20demographic%20biases%20assigning%20higher%20stigma%20to%20younger%2C%20less%20educated%2C%20and%20non-White%20personas%2C%20and%20treat%20secrecy%20as%20universal%20despite%2036%25%20of%20humans%20reporting%20openness.%20Most%20critically%2C%20models%20produce%20internal%20contradictions%3A%20they%20overestimate%20isolation%20yet%20predict%20isolated%20individuals%20are%20less%20secretive%2C%20revealing%20incoherent%20representations.%20These%20patterns%20show%20current%20alignment%20approaches%20ensure%20appropriate%20language%20but%20not%20coherent%20understanding%20across%20levels.%20This%20work%20provides%20empirical%20evidence%20that%20LLMs%20lack%20coherent%20understanding%20of%20psychological%20constructs%20operating%20across%20multiple%20dimensions.%20AI%20safety%20in%20high-stakes%20contexts%20demands%20new%20approaches%20to%20design%20%28multilevel%20coherence%29%2C%20evaluation%20%28continuous%20auditing%29%2C%20governance%20and%20regulation%20%28mandatory%20audits%2C%20accountability%2C%20deployment%20restrictions%29%2C%20and%20AI%20literacy%20in%20domains%20where%20understanding%20what%20people%20cannot%20say%20determines%20whether%20support%20helps%20or%20harms.%0ALink%3A%20http%3A//arxiv.org/abs/2512.13142v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520LLMs%2520Understand%2520What%2520We%2520Cannot%2520Say%253F%2520Measuring%2520Multilevel%2520Alignment%2520Through%2520Abortion%2520Stigma%2520Across%2520Cognitive%252C%2520Interpersonal%252C%2520and%2520Structural%2520Levels%26entry.906535625%3DAnika%2520Sharma%2520and%2520Malavika%2520Mampally%2520and%2520Chidaksh%2520Ravuru%2520and%2520Kandyce%2520Brennan%2520and%2520Neil%2520Gaikwad%26entry.1292438233%3DAs%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520increasingly%2520mediate%2520stigmatized%2520health%2520decisions%252C%2520their%2520capacity%2520to%2520understand%2520complex%2520psychological%2520phenomena%2520remains%2520inadequately%2520assessed.%2520Can%2520LLMs%2520understand%2520what%2520we%2520cannot%2520say%253F%2520We%2520investigate%2520whether%2520LLMs%2520coherently%2520represent%2520abortion%2520stigma%2520across%2520cognitive%252C%2520interpersonal%252C%2520and%2520structural%2520levels.%2520We%2520systematically%2520tested%2520627%2520demographically%2520diverse%2520personas%2520across%2520five%2520leading%2520LLMs%2520using%2520the%2520validated%2520Individual%2520Level%2520Abortion%2520Stigma%2520Scale%2520%2528ILAS%2529%252C%2520examining%2520representation%2520at%2520cognitive%2520%2528self-judgment%2529%252C%2520interpersonal%2520%2528worries%2520about%2520judgment%2520and%2520isolation%2529%252C%2520and%2520structural%2520%2528community%2520condemnation%2520and%2520disclosure%2520patterns%2529%2520levels.%2520Models%2520fail%2520tests%2520of%2520genuine%2520understanding%2520across%2520all%2520dimensions.%2520They%2520underestimate%2520cognitive%2520stigma%2520while%2520overestimating%2520interpersonal%2520stigma%252C%2520introduce%2520demographic%2520biases%2520assigning%2520higher%2520stigma%2520to%2520younger%252C%2520less%2520educated%252C%2520and%2520non-White%2520personas%252C%2520and%2520treat%2520secrecy%2520as%2520universal%2520despite%252036%2525%2520of%2520humans%2520reporting%2520openness.%2520Most%2520critically%252C%2520models%2520produce%2520internal%2520contradictions%253A%2520they%2520overestimate%2520isolation%2520yet%2520predict%2520isolated%2520individuals%2520are%2520less%2520secretive%252C%2520revealing%2520incoherent%2520representations.%2520These%2520patterns%2520show%2520current%2520alignment%2520approaches%2520ensure%2520appropriate%2520language%2520but%2520not%2520coherent%2520understanding%2520across%2520levels.%2520This%2520work%2520provides%2520empirical%2520evidence%2520that%2520LLMs%2520lack%2520coherent%2520understanding%2520of%2520psychological%2520constructs%2520operating%2520across%2520multiple%2520dimensions.%2520AI%2520safety%2520in%2520high-stakes%2520contexts%2520demands%2520new%2520approaches%2520to%2520design%2520%2528multilevel%2520coherence%2529%252C%2520evaluation%2520%2528continuous%2520auditing%2529%252C%2520governance%2520and%2520regulation%2520%2528mandatory%2520audits%252C%2520accountability%252C%2520deployment%2520restrictions%2529%252C%2520and%2520AI%2520literacy%2520in%2520domains%2520where%2520understanding%2520what%2520people%2520cannot%2520say%2520determines%2520whether%2520support%2520helps%2520or%2520harms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.13142v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20LLMs%20Understand%20What%20We%20Cannot%20Say%3F%20Measuring%20Multilevel%20Alignment%20Through%20Abortion%20Stigma%20Across%20Cognitive%2C%20Interpersonal%2C%20and%20Structural%20Levels&entry.906535625=Anika%20Sharma%20and%20Malavika%20Mampally%20and%20Chidaksh%20Ravuru%20and%20Kandyce%20Brennan%20and%20Neil%20Gaikwad&entry.1292438233=As%20Large%20Language%20Models%20%28LLMs%29%20increasingly%20mediate%20stigmatized%20health%20decisions%2C%20their%20capacity%20to%20understand%20complex%20psychological%20phenomena%20remains%20inadequately%20assessed.%20Can%20LLMs%20understand%20what%20we%20cannot%20say%3F%20We%20investigate%20whether%20LLMs%20coherently%20represent%20abortion%20stigma%20across%20cognitive%2C%20interpersonal%2C%20and%20structural%20levels.%20We%20systematically%20tested%20627%20demographically%20diverse%20personas%20across%20five%20leading%20LLMs%20using%20the%20validated%20Individual%20Level%20Abortion%20Stigma%20Scale%20%28ILAS%29%2C%20examining%20representation%20at%20cognitive%20%28self-judgment%29%2C%20interpersonal%20%28worries%20about%20judgment%20and%20isolation%29%2C%20and%20structural%20%28community%20condemnation%20and%20disclosure%20patterns%29%20levels.%20Models%20fail%20tests%20of%20genuine%20understanding%20across%20all%20dimensions.%20They%20underestimate%20cognitive%20stigma%20while%20overestimating%20interpersonal%20stigma%2C%20introduce%20demographic%20biases%20assigning%20higher%20stigma%20to%20younger%2C%20less%20educated%2C%20and%20non-White%20personas%2C%20and%20treat%20secrecy%20as%20universal%20despite%2036%25%20of%20humans%20reporting%20openness.%20Most%20critically%2C%20models%20produce%20internal%20contradictions%3A%20they%20overestimate%20isolation%20yet%20predict%20isolated%20individuals%20are%20less%20secretive%2C%20revealing%20incoherent%20representations.%20These%20patterns%20show%20current%20alignment%20approaches%20ensure%20appropriate%20language%20but%20not%20coherent%20understanding%20across%20levels.%20This%20work%20provides%20empirical%20evidence%20that%20LLMs%20lack%20coherent%20understanding%20of%20psychological%20constructs%20operating%20across%20multiple%20dimensions.%20AI%20safety%20in%20high-stakes%20contexts%20demands%20new%20approaches%20to%20design%20%28multilevel%20coherence%29%2C%20evaluation%20%28continuous%20auditing%29%2C%20governance%20and%20regulation%20%28mandatory%20audits%2C%20accountability%2C%20deployment%20restrictions%29%2C%20and%20AI%20literacy%20in%20domains%20where%20understanding%20what%20people%20cannot%20say%20determines%20whether%20support%20helps%20or%20harms.&entry.1838667208=http%3A//arxiv.org/abs/2512.13142v4&entry.124074799=Read"},
{"title": "3D Wavelet-Based Structural Priors for Controlled Diffusion in Whole-Body Low-Dose PET Denoising", "author": "Peiyuan Jing and Yue Tang and Chun-Wun Cheng and Zhenxuan Zhang and Liutao Yang and Thiago V. Lima and Klaus Strobel and Antoine Leimgruber and Angelica Aviles-Rivero and Guang Yang and Javier Montoya", "abstract": "Low-dose Positron Emission Tomography (PET) imaging reduces patient radiation exposure but suffers from increased noise that degrades image quality and diagnostic reliability. Although diffusion models have demonstrated strong denoising capability, their stochastic nature makes it challenging to enforce anatomically consistent structures, particularly in low signal-to-noise regimes and volumetric whole-body imaging. We propose Wavelet-Conditioned ControlNet (WCC-Net), a fully 3D diffusion-based framework that introduces explicit frequency-domain structural priors via wavelet representations to guide volumetric PET denoising. By injecting wavelet-based structural guidance into a frozen pretrained diffusion backbone through a lightweight control branch, WCC-Net decouples anatomical structure from noise while preserving generative expressiveness and 3D structural continuity. Extensive experiments demonstrate that WCC-Net consistently outperforms CNN-, GAN-, and diffusion-based baselines. On the internal 1/20-dose test set, WCC-Net improves PSNR by +1.21 dB and SSIM by +0.008 over a strong diffusion baseline, while reducing structural distortion (GMSD) and intensity error (NMAE). Moreover, WCC-Net generalizes robustly to unseen dose levels (1/50 and 1/4), achieving superior quantitative performance and improved volumetric anatomical consistency.", "link": "http://arxiv.org/abs/2601.07093v2", "date": "2026-01-15", "relevancy": 2.2198, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5882}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5515}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Wavelet-Based%20Structural%20Priors%20for%20Controlled%20Diffusion%20in%20Whole-Body%20Low-Dose%20PET%20Denoising&body=Title%3A%203D%20Wavelet-Based%20Structural%20Priors%20for%20Controlled%20Diffusion%20in%20Whole-Body%20Low-Dose%20PET%20Denoising%0AAuthor%3A%20Peiyuan%20Jing%20and%20Yue%20Tang%20and%20Chun-Wun%20Cheng%20and%20Zhenxuan%20Zhang%20and%20Liutao%20Yang%20and%20Thiago%20V.%20Lima%20and%20Klaus%20Strobel%20and%20Antoine%20Leimgruber%20and%20Angelica%20Aviles-Rivero%20and%20Guang%20Yang%20and%20Javier%20Montoya%0AAbstract%3A%20Low-dose%20Positron%20Emission%20Tomography%20%28PET%29%20imaging%20reduces%20patient%20radiation%20exposure%20but%20suffers%20from%20increased%20noise%20that%20degrades%20image%20quality%20and%20diagnostic%20reliability.%20Although%20diffusion%20models%20have%20demonstrated%20strong%20denoising%20capability%2C%20their%20stochastic%20nature%20makes%20it%20challenging%20to%20enforce%20anatomically%20consistent%20structures%2C%20particularly%20in%20low%20signal-to-noise%20regimes%20and%20volumetric%20whole-body%20imaging.%20We%20propose%20Wavelet-Conditioned%20ControlNet%20%28WCC-Net%29%2C%20a%20fully%203D%20diffusion-based%20framework%20that%20introduces%20explicit%20frequency-domain%20structural%20priors%20via%20wavelet%20representations%20to%20guide%20volumetric%20PET%20denoising.%20By%20injecting%20wavelet-based%20structural%20guidance%20into%20a%20frozen%20pretrained%20diffusion%20backbone%20through%20a%20lightweight%20control%20branch%2C%20WCC-Net%20decouples%20anatomical%20structure%20from%20noise%20while%20preserving%20generative%20expressiveness%20and%203D%20structural%20continuity.%20Extensive%20experiments%20demonstrate%20that%20WCC-Net%20consistently%20outperforms%20CNN-%2C%20GAN-%2C%20and%20diffusion-based%20baselines.%20On%20the%20internal%201/20-dose%20test%20set%2C%20WCC-Net%20improves%20PSNR%20by%20%2B1.21%20dB%20and%20SSIM%20by%20%2B0.008%20over%20a%20strong%20diffusion%20baseline%2C%20while%20reducing%20structural%20distortion%20%28GMSD%29%20and%20intensity%20error%20%28NMAE%29.%20Moreover%2C%20WCC-Net%20generalizes%20robustly%20to%20unseen%20dose%20levels%20%281/50%20and%201/4%29%2C%20achieving%20superior%20quantitative%20performance%20and%20improved%20volumetric%20anatomical%20consistency.%0ALink%3A%20http%3A//arxiv.org/abs/2601.07093v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Wavelet-Based%2520Structural%2520Priors%2520for%2520Controlled%2520Diffusion%2520in%2520Whole-Body%2520Low-Dose%2520PET%2520Denoising%26entry.906535625%3DPeiyuan%2520Jing%2520and%2520Yue%2520Tang%2520and%2520Chun-Wun%2520Cheng%2520and%2520Zhenxuan%2520Zhang%2520and%2520Liutao%2520Yang%2520and%2520Thiago%2520V.%2520Lima%2520and%2520Klaus%2520Strobel%2520and%2520Antoine%2520Leimgruber%2520and%2520Angelica%2520Aviles-Rivero%2520and%2520Guang%2520Yang%2520and%2520Javier%2520Montoya%26entry.1292438233%3DLow-dose%2520Positron%2520Emission%2520Tomography%2520%2528PET%2529%2520imaging%2520reduces%2520patient%2520radiation%2520exposure%2520but%2520suffers%2520from%2520increased%2520noise%2520that%2520degrades%2520image%2520quality%2520and%2520diagnostic%2520reliability.%2520Although%2520diffusion%2520models%2520have%2520demonstrated%2520strong%2520denoising%2520capability%252C%2520their%2520stochastic%2520nature%2520makes%2520it%2520challenging%2520to%2520enforce%2520anatomically%2520consistent%2520structures%252C%2520particularly%2520in%2520low%2520signal-to-noise%2520regimes%2520and%2520volumetric%2520whole-body%2520imaging.%2520We%2520propose%2520Wavelet-Conditioned%2520ControlNet%2520%2528WCC-Net%2529%252C%2520a%2520fully%25203D%2520diffusion-based%2520framework%2520that%2520introduces%2520explicit%2520frequency-domain%2520structural%2520priors%2520via%2520wavelet%2520representations%2520to%2520guide%2520volumetric%2520PET%2520denoising.%2520By%2520injecting%2520wavelet-based%2520structural%2520guidance%2520into%2520a%2520frozen%2520pretrained%2520diffusion%2520backbone%2520through%2520a%2520lightweight%2520control%2520branch%252C%2520WCC-Net%2520decouples%2520anatomical%2520structure%2520from%2520noise%2520while%2520preserving%2520generative%2520expressiveness%2520and%25203D%2520structural%2520continuity.%2520Extensive%2520experiments%2520demonstrate%2520that%2520WCC-Net%2520consistently%2520outperforms%2520CNN-%252C%2520GAN-%252C%2520and%2520diffusion-based%2520baselines.%2520On%2520the%2520internal%25201/20-dose%2520test%2520set%252C%2520WCC-Net%2520improves%2520PSNR%2520by%2520%252B1.21%2520dB%2520and%2520SSIM%2520by%2520%252B0.008%2520over%2520a%2520strong%2520diffusion%2520baseline%252C%2520while%2520reducing%2520structural%2520distortion%2520%2528GMSD%2529%2520and%2520intensity%2520error%2520%2528NMAE%2529.%2520Moreover%252C%2520WCC-Net%2520generalizes%2520robustly%2520to%2520unseen%2520dose%2520levels%2520%25281/50%2520and%25201/4%2529%252C%2520achieving%2520superior%2520quantitative%2520performance%2520and%2520improved%2520volumetric%2520anatomical%2520consistency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.07093v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Wavelet-Based%20Structural%20Priors%20for%20Controlled%20Diffusion%20in%20Whole-Body%20Low-Dose%20PET%20Denoising&entry.906535625=Peiyuan%20Jing%20and%20Yue%20Tang%20and%20Chun-Wun%20Cheng%20and%20Zhenxuan%20Zhang%20and%20Liutao%20Yang%20and%20Thiago%20V.%20Lima%20and%20Klaus%20Strobel%20and%20Antoine%20Leimgruber%20and%20Angelica%20Aviles-Rivero%20and%20Guang%20Yang%20and%20Javier%20Montoya&entry.1292438233=Low-dose%20Positron%20Emission%20Tomography%20%28PET%29%20imaging%20reduces%20patient%20radiation%20exposure%20but%20suffers%20from%20increased%20noise%20that%20degrades%20image%20quality%20and%20diagnostic%20reliability.%20Although%20diffusion%20models%20have%20demonstrated%20strong%20denoising%20capability%2C%20their%20stochastic%20nature%20makes%20it%20challenging%20to%20enforce%20anatomically%20consistent%20structures%2C%20particularly%20in%20low%20signal-to-noise%20regimes%20and%20volumetric%20whole-body%20imaging.%20We%20propose%20Wavelet-Conditioned%20ControlNet%20%28WCC-Net%29%2C%20a%20fully%203D%20diffusion-based%20framework%20that%20introduces%20explicit%20frequency-domain%20structural%20priors%20via%20wavelet%20representations%20to%20guide%20volumetric%20PET%20denoising.%20By%20injecting%20wavelet-based%20structural%20guidance%20into%20a%20frozen%20pretrained%20diffusion%20backbone%20through%20a%20lightweight%20control%20branch%2C%20WCC-Net%20decouples%20anatomical%20structure%20from%20noise%20while%20preserving%20generative%20expressiveness%20and%203D%20structural%20continuity.%20Extensive%20experiments%20demonstrate%20that%20WCC-Net%20consistently%20outperforms%20CNN-%2C%20GAN-%2C%20and%20diffusion-based%20baselines.%20On%20the%20internal%201/20-dose%20test%20set%2C%20WCC-Net%20improves%20PSNR%20by%20%2B1.21%20dB%20and%20SSIM%20by%20%2B0.008%20over%20a%20strong%20diffusion%20baseline%2C%20while%20reducing%20structural%20distortion%20%28GMSD%29%20and%20intensity%20error%20%28NMAE%29.%20Moreover%2C%20WCC-Net%20generalizes%20robustly%20to%20unseen%20dose%20levels%20%281/50%20and%201/4%29%2C%20achieving%20superior%20quantitative%20performance%20and%20improved%20volumetric%20anatomical%20consistency.&entry.1838667208=http%3A//arxiv.org/abs/2601.07093v2&entry.124074799=Read"},
{"title": "Learning Quadrotor Control From Visual Features Using Differentiable Simulation", "author": "Johannes Heeg and Yunlong Song and Davide Scaramuzza", "abstract": "The sample inefficiency of reinforcement learning (RL) remains a significant challenge in robotics. RL requires large-scale simulation and can still cause long training times, slowing research and innovation. This issue is particularly pronounced in vision-based control tasks where reliable state estimates are not accessible. Differentiable simulation offers an alternative by enabling gradient back-propagation through the dynamics model, providing low-variance analytical policy gradients and, hence, higher sample efficiency. However, its usage for real-world robotic tasks has yet been limited. This work demonstrates the great potential of differentiable simulation for learning quadrotor control. We show that training in differentiable simulation significantly outperforms model-free RL in terms of both sample efficiency and training time, allowing a policy to learn to recover a quadrotor in seconds when providing vehicle states and in minutes when relying solely on visual features. The key to our success is two-fold. First, the use of a simple surrogate model for gradient computation greatly accelerates training without sacrificing control performance. Second, combining state representation learning with policy learning enhances convergence speed in tasks where only visual features are observable. These findings highlight the potential of differentiable simulation for real-world robotics and offer a compelling alternative to conventional RL approaches.", "link": "http://arxiv.org/abs/2410.15979v3", "date": "2026-01-15", "relevancy": 2.1789, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5663}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.544}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5368}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Quadrotor%20Control%20From%20Visual%20Features%20Using%20Differentiable%20Simulation&body=Title%3A%20Learning%20Quadrotor%20Control%20From%20Visual%20Features%20Using%20Differentiable%20Simulation%0AAuthor%3A%20Johannes%20Heeg%20and%20Yunlong%20Song%20and%20Davide%20Scaramuzza%0AAbstract%3A%20The%20sample%20inefficiency%20of%20reinforcement%20learning%20%28RL%29%20remains%20a%20significant%20challenge%20in%20robotics.%20RL%20requires%20large-scale%20simulation%20and%20can%20still%20cause%20long%20training%20times%2C%20slowing%20research%20and%20innovation.%20This%20issue%20is%20particularly%20pronounced%20in%20vision-based%20control%20tasks%20where%20reliable%20state%20estimates%20are%20not%20accessible.%20Differentiable%20simulation%20offers%20an%20alternative%20by%20enabling%20gradient%20back-propagation%20through%20the%20dynamics%20model%2C%20providing%20low-variance%20analytical%20policy%20gradients%20and%2C%20hence%2C%20higher%20sample%20efficiency.%20However%2C%20its%20usage%20for%20real-world%20robotic%20tasks%20has%20yet%20been%20limited.%20This%20work%20demonstrates%20the%20great%20potential%20of%20differentiable%20simulation%20for%20learning%20quadrotor%20control.%20We%20show%20that%20training%20in%20differentiable%20simulation%20significantly%20outperforms%20model-free%20RL%20in%20terms%20of%20both%20sample%20efficiency%20and%20training%20time%2C%20allowing%20a%20policy%20to%20learn%20to%20recover%20a%20quadrotor%20in%20seconds%20when%20providing%20vehicle%20states%20and%20in%20minutes%20when%20relying%20solely%20on%20visual%20features.%20The%20key%20to%20our%20success%20is%20two-fold.%20First%2C%20the%20use%20of%20a%20simple%20surrogate%20model%20for%20gradient%20computation%20greatly%20accelerates%20training%20without%20sacrificing%20control%20performance.%20Second%2C%20combining%20state%20representation%20learning%20with%20policy%20learning%20enhances%20convergence%20speed%20in%20tasks%20where%20only%20visual%20features%20are%20observable.%20These%20findings%20highlight%20the%20potential%20of%20differentiable%20simulation%20for%20real-world%20robotics%20and%20offer%20a%20compelling%20alternative%20to%20conventional%20RL%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2410.15979v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Quadrotor%2520Control%2520From%2520Visual%2520Features%2520Using%2520Differentiable%2520Simulation%26entry.906535625%3DJohannes%2520Heeg%2520and%2520Yunlong%2520Song%2520and%2520Davide%2520Scaramuzza%26entry.1292438233%3DThe%2520sample%2520inefficiency%2520of%2520reinforcement%2520learning%2520%2528RL%2529%2520remains%2520a%2520significant%2520challenge%2520in%2520robotics.%2520RL%2520requires%2520large-scale%2520simulation%2520and%2520can%2520still%2520cause%2520long%2520training%2520times%252C%2520slowing%2520research%2520and%2520innovation.%2520This%2520issue%2520is%2520particularly%2520pronounced%2520in%2520vision-based%2520control%2520tasks%2520where%2520reliable%2520state%2520estimates%2520are%2520not%2520accessible.%2520Differentiable%2520simulation%2520offers%2520an%2520alternative%2520by%2520enabling%2520gradient%2520back-propagation%2520through%2520the%2520dynamics%2520model%252C%2520providing%2520low-variance%2520analytical%2520policy%2520gradients%2520and%252C%2520hence%252C%2520higher%2520sample%2520efficiency.%2520However%252C%2520its%2520usage%2520for%2520real-world%2520robotic%2520tasks%2520has%2520yet%2520been%2520limited.%2520This%2520work%2520demonstrates%2520the%2520great%2520potential%2520of%2520differentiable%2520simulation%2520for%2520learning%2520quadrotor%2520control.%2520We%2520show%2520that%2520training%2520in%2520differentiable%2520simulation%2520significantly%2520outperforms%2520model-free%2520RL%2520in%2520terms%2520of%2520both%2520sample%2520efficiency%2520and%2520training%2520time%252C%2520allowing%2520a%2520policy%2520to%2520learn%2520to%2520recover%2520a%2520quadrotor%2520in%2520seconds%2520when%2520providing%2520vehicle%2520states%2520and%2520in%2520minutes%2520when%2520relying%2520solely%2520on%2520visual%2520features.%2520The%2520key%2520to%2520our%2520success%2520is%2520two-fold.%2520First%252C%2520the%2520use%2520of%2520a%2520simple%2520surrogate%2520model%2520for%2520gradient%2520computation%2520greatly%2520accelerates%2520training%2520without%2520sacrificing%2520control%2520performance.%2520Second%252C%2520combining%2520state%2520representation%2520learning%2520with%2520policy%2520learning%2520enhances%2520convergence%2520speed%2520in%2520tasks%2520where%2520only%2520visual%2520features%2520are%2520observable.%2520These%2520findings%2520highlight%2520the%2520potential%2520of%2520differentiable%2520simulation%2520for%2520real-world%2520robotics%2520and%2520offer%2520a%2520compelling%2520alternative%2520to%2520conventional%2520RL%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.15979v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Quadrotor%20Control%20From%20Visual%20Features%20Using%20Differentiable%20Simulation&entry.906535625=Johannes%20Heeg%20and%20Yunlong%20Song%20and%20Davide%20Scaramuzza&entry.1292438233=The%20sample%20inefficiency%20of%20reinforcement%20learning%20%28RL%29%20remains%20a%20significant%20challenge%20in%20robotics.%20RL%20requires%20large-scale%20simulation%20and%20can%20still%20cause%20long%20training%20times%2C%20slowing%20research%20and%20innovation.%20This%20issue%20is%20particularly%20pronounced%20in%20vision-based%20control%20tasks%20where%20reliable%20state%20estimates%20are%20not%20accessible.%20Differentiable%20simulation%20offers%20an%20alternative%20by%20enabling%20gradient%20back-propagation%20through%20the%20dynamics%20model%2C%20providing%20low-variance%20analytical%20policy%20gradients%20and%2C%20hence%2C%20higher%20sample%20efficiency.%20However%2C%20its%20usage%20for%20real-world%20robotic%20tasks%20has%20yet%20been%20limited.%20This%20work%20demonstrates%20the%20great%20potential%20of%20differentiable%20simulation%20for%20learning%20quadrotor%20control.%20We%20show%20that%20training%20in%20differentiable%20simulation%20significantly%20outperforms%20model-free%20RL%20in%20terms%20of%20both%20sample%20efficiency%20and%20training%20time%2C%20allowing%20a%20policy%20to%20learn%20to%20recover%20a%20quadrotor%20in%20seconds%20when%20providing%20vehicle%20states%20and%20in%20minutes%20when%20relying%20solely%20on%20visual%20features.%20The%20key%20to%20our%20success%20is%20two-fold.%20First%2C%20the%20use%20of%20a%20simple%20surrogate%20model%20for%20gradient%20computation%20greatly%20accelerates%20training%20without%20sacrificing%20control%20performance.%20Second%2C%20combining%20state%20representation%20learning%20with%20policy%20learning%20enhances%20convergence%20speed%20in%20tasks%20where%20only%20visual%20features%20are%20observable.%20These%20findings%20highlight%20the%20potential%20of%20differentiable%20simulation%20for%20real-world%20robotics%20and%20offer%20a%20compelling%20alternative%20to%20conventional%20RL%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2410.15979v3&entry.124074799=Read"},
{"title": "STEM: Scaling Transformers with Embedding Modules", "author": "Ranajoy Sadhukhan and Sheng Cao and Harry Dong and Changsheng Zhao and Attiano Purpura-Pontoniere and Yuandong Tian and Zechun Liu and Beidi Chen", "abstract": "Fine-grained sparsity promises higher parametric capacity without proportional per-token compute, but often suffers from training instability, load balancing, and communication overhead. We introduce STEM (Scaling Transformers with Embedding Modules), a static, token-indexed approach that replaces the FFN up-projection with a layer-local embedding lookup while keeping the gate and down-projection dense. This removes runtime routing, enables CPU offload with asynchronous prefetch, and decouples capacity from both per-token FLOPs and cross-device communication. Empirically, STEM trains stably despite extreme sparsity. It improves downstream performance over dense baselines while reducing per-token FLOPs and parameter accesses (eliminating roughly one-third of FFN parameters). STEM learns embedding spaces with large angular spread which enhances its knowledge storage capacity. More interestingly, this enhanced knowledge capacity comes with better interpretability. The token-indexed nature of STEM embeddings allows simple ways to perform knowledge editing and knowledge injection in an interpretable manner without any intervention in the input text or additional computation. In addition, STEM strengthens long-context performance: as sequence length grows, more distinct parameters are activated, yielding practical test-time capacity scaling. Across 350M and 1B model scales, STEM delivers up to ~3--4% accuracy improvements overall, with notable gains on knowledge and reasoning-heavy benchmarks (ARC-Challenge, OpenBookQA, GSM8K, MMLU). Overall, STEM is an effective way of scaling parametric memory while providing better interpretability, better training stability and improved efficiency.", "link": "http://arxiv.org/abs/2601.10639v1", "date": "2026-01-15", "relevancy": 2.1718, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.586}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5457}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20STEM%3A%20Scaling%20Transformers%20with%20Embedding%20Modules&body=Title%3A%20STEM%3A%20Scaling%20Transformers%20with%20Embedding%20Modules%0AAuthor%3A%20Ranajoy%20Sadhukhan%20and%20Sheng%20Cao%20and%20Harry%20Dong%20and%20Changsheng%20Zhao%20and%20Attiano%20Purpura-Pontoniere%20and%20Yuandong%20Tian%20and%20Zechun%20Liu%20and%20Beidi%20Chen%0AAbstract%3A%20Fine-grained%20sparsity%20promises%20higher%20parametric%20capacity%20without%20proportional%20per-token%20compute%2C%20but%20often%20suffers%20from%20training%20instability%2C%20load%20balancing%2C%20and%20communication%20overhead.%20We%20introduce%20STEM%20%28Scaling%20Transformers%20with%20Embedding%20Modules%29%2C%20a%20static%2C%20token-indexed%20approach%20that%20replaces%20the%20FFN%20up-projection%20with%20a%20layer-local%20embedding%20lookup%20while%20keeping%20the%20gate%20and%20down-projection%20dense.%20This%20removes%20runtime%20routing%2C%20enables%20CPU%20offload%20with%20asynchronous%20prefetch%2C%20and%20decouples%20capacity%20from%20both%20per-token%20FLOPs%20and%20cross-device%20communication.%20Empirically%2C%20STEM%20trains%20stably%20despite%20extreme%20sparsity.%20It%20improves%20downstream%20performance%20over%20dense%20baselines%20while%20reducing%20per-token%20FLOPs%20and%20parameter%20accesses%20%28eliminating%20roughly%20one-third%20of%20FFN%20parameters%29.%20STEM%20learns%20embedding%20spaces%20with%20large%20angular%20spread%20which%20enhances%20its%20knowledge%20storage%20capacity.%20More%20interestingly%2C%20this%20enhanced%20knowledge%20capacity%20comes%20with%20better%20interpretability.%20The%20token-indexed%20nature%20of%20STEM%20embeddings%20allows%20simple%20ways%20to%20perform%20knowledge%20editing%20and%20knowledge%20injection%20in%20an%20interpretable%20manner%20without%20any%20intervention%20in%20the%20input%20text%20or%20additional%20computation.%20In%20addition%2C%20STEM%20strengthens%20long-context%20performance%3A%20as%20sequence%20length%20grows%2C%20more%20distinct%20parameters%20are%20activated%2C%20yielding%20practical%20test-time%20capacity%20scaling.%20Across%20350M%20and%201B%20model%20scales%2C%20STEM%20delivers%20up%20to%20~3--4%25%20accuracy%20improvements%20overall%2C%20with%20notable%20gains%20on%20knowledge%20and%20reasoning-heavy%20benchmarks%20%28ARC-Challenge%2C%20OpenBookQA%2C%20GSM8K%2C%20MMLU%29.%20Overall%2C%20STEM%20is%20an%20effective%20way%20of%20scaling%20parametric%20memory%20while%20providing%20better%20interpretability%2C%20better%20training%20stability%20and%20improved%20efficiency.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10639v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSTEM%253A%2520Scaling%2520Transformers%2520with%2520Embedding%2520Modules%26entry.906535625%3DRanajoy%2520Sadhukhan%2520and%2520Sheng%2520Cao%2520and%2520Harry%2520Dong%2520and%2520Changsheng%2520Zhao%2520and%2520Attiano%2520Purpura-Pontoniere%2520and%2520Yuandong%2520Tian%2520and%2520Zechun%2520Liu%2520and%2520Beidi%2520Chen%26entry.1292438233%3DFine-grained%2520sparsity%2520promises%2520higher%2520parametric%2520capacity%2520without%2520proportional%2520per-token%2520compute%252C%2520but%2520often%2520suffers%2520from%2520training%2520instability%252C%2520load%2520balancing%252C%2520and%2520communication%2520overhead.%2520We%2520introduce%2520STEM%2520%2528Scaling%2520Transformers%2520with%2520Embedding%2520Modules%2529%252C%2520a%2520static%252C%2520token-indexed%2520approach%2520that%2520replaces%2520the%2520FFN%2520up-projection%2520with%2520a%2520layer-local%2520embedding%2520lookup%2520while%2520keeping%2520the%2520gate%2520and%2520down-projection%2520dense.%2520This%2520removes%2520runtime%2520routing%252C%2520enables%2520CPU%2520offload%2520with%2520asynchronous%2520prefetch%252C%2520and%2520decouples%2520capacity%2520from%2520both%2520per-token%2520FLOPs%2520and%2520cross-device%2520communication.%2520Empirically%252C%2520STEM%2520trains%2520stably%2520despite%2520extreme%2520sparsity.%2520It%2520improves%2520downstream%2520performance%2520over%2520dense%2520baselines%2520while%2520reducing%2520per-token%2520FLOPs%2520and%2520parameter%2520accesses%2520%2528eliminating%2520roughly%2520one-third%2520of%2520FFN%2520parameters%2529.%2520STEM%2520learns%2520embedding%2520spaces%2520with%2520large%2520angular%2520spread%2520which%2520enhances%2520its%2520knowledge%2520storage%2520capacity.%2520More%2520interestingly%252C%2520this%2520enhanced%2520knowledge%2520capacity%2520comes%2520with%2520better%2520interpretability.%2520The%2520token-indexed%2520nature%2520of%2520STEM%2520embeddings%2520allows%2520simple%2520ways%2520to%2520perform%2520knowledge%2520editing%2520and%2520knowledge%2520injection%2520in%2520an%2520interpretable%2520manner%2520without%2520any%2520intervention%2520in%2520the%2520input%2520text%2520or%2520additional%2520computation.%2520In%2520addition%252C%2520STEM%2520strengthens%2520long-context%2520performance%253A%2520as%2520sequence%2520length%2520grows%252C%2520more%2520distinct%2520parameters%2520are%2520activated%252C%2520yielding%2520practical%2520test-time%2520capacity%2520scaling.%2520Across%2520350M%2520and%25201B%2520model%2520scales%252C%2520STEM%2520delivers%2520up%2520to%2520~3--4%2525%2520accuracy%2520improvements%2520overall%252C%2520with%2520notable%2520gains%2520on%2520knowledge%2520and%2520reasoning-heavy%2520benchmarks%2520%2528ARC-Challenge%252C%2520OpenBookQA%252C%2520GSM8K%252C%2520MMLU%2529.%2520Overall%252C%2520STEM%2520is%2520an%2520effective%2520way%2520of%2520scaling%2520parametric%2520memory%2520while%2520providing%2520better%2520interpretability%252C%2520better%2520training%2520stability%2520and%2520improved%2520efficiency.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10639v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=STEM%3A%20Scaling%20Transformers%20with%20Embedding%20Modules&entry.906535625=Ranajoy%20Sadhukhan%20and%20Sheng%20Cao%20and%20Harry%20Dong%20and%20Changsheng%20Zhao%20and%20Attiano%20Purpura-Pontoniere%20and%20Yuandong%20Tian%20and%20Zechun%20Liu%20and%20Beidi%20Chen&entry.1292438233=Fine-grained%20sparsity%20promises%20higher%20parametric%20capacity%20without%20proportional%20per-token%20compute%2C%20but%20often%20suffers%20from%20training%20instability%2C%20load%20balancing%2C%20and%20communication%20overhead.%20We%20introduce%20STEM%20%28Scaling%20Transformers%20with%20Embedding%20Modules%29%2C%20a%20static%2C%20token-indexed%20approach%20that%20replaces%20the%20FFN%20up-projection%20with%20a%20layer-local%20embedding%20lookup%20while%20keeping%20the%20gate%20and%20down-projection%20dense.%20This%20removes%20runtime%20routing%2C%20enables%20CPU%20offload%20with%20asynchronous%20prefetch%2C%20and%20decouples%20capacity%20from%20both%20per-token%20FLOPs%20and%20cross-device%20communication.%20Empirically%2C%20STEM%20trains%20stably%20despite%20extreme%20sparsity.%20It%20improves%20downstream%20performance%20over%20dense%20baselines%20while%20reducing%20per-token%20FLOPs%20and%20parameter%20accesses%20%28eliminating%20roughly%20one-third%20of%20FFN%20parameters%29.%20STEM%20learns%20embedding%20spaces%20with%20large%20angular%20spread%20which%20enhances%20its%20knowledge%20storage%20capacity.%20More%20interestingly%2C%20this%20enhanced%20knowledge%20capacity%20comes%20with%20better%20interpretability.%20The%20token-indexed%20nature%20of%20STEM%20embeddings%20allows%20simple%20ways%20to%20perform%20knowledge%20editing%20and%20knowledge%20injection%20in%20an%20interpretable%20manner%20without%20any%20intervention%20in%20the%20input%20text%20or%20additional%20computation.%20In%20addition%2C%20STEM%20strengthens%20long-context%20performance%3A%20as%20sequence%20length%20grows%2C%20more%20distinct%20parameters%20are%20activated%2C%20yielding%20practical%20test-time%20capacity%20scaling.%20Across%20350M%20and%201B%20model%20scales%2C%20STEM%20delivers%20up%20to%20~3--4%25%20accuracy%20improvements%20overall%2C%20with%20notable%20gains%20on%20knowledge%20and%20reasoning-heavy%20benchmarks%20%28ARC-Challenge%2C%20OpenBookQA%2C%20GSM8K%2C%20MMLU%29.%20Overall%2C%20STEM%20is%20an%20effective%20way%20of%20scaling%20parametric%20memory%20while%20providing%20better%20interpretability%2C%20better%20training%20stability%20and%20improved%20efficiency.&entry.1838667208=http%3A//arxiv.org/abs/2601.10639v1&entry.124074799=Read"},
{"title": "Mixtures of Transparent Local Models", "author": "Niffa Cheick Oumar Diaby and Thierry Duchesne and Mario Marchand", "abstract": "The predominance of machine learning models in many spheres of human activity has led to a growing demand for their transparency. The transparency of models makes it possible to discern some factors, such as security or non-discrimination. In this paper, we propose a mixture of transparent local models as an alternative solution for designing interpretable (or transparent) models. Our approach is designed for the situations where a simple and transparent function is suitable for modeling the label of instances in some localities/regions of the input space, but may change abruptly as we move from one locality to another. Consequently, the proposed algorithm is to learn both the transparent labeling function and the locality of the input space where the labeling function achieves a small risk in its assigned locality. By using a new multi-predictor (and multi-locality) loss function, we established rigorous PAC-Bayesian risk bounds for the case of binary linear classification problem and that of linear regression. In both cases, synthetic data sets were used to illustrate how the learning algorithms work. The results obtained from real data sets highlight the competitiveness of our approach compared to other existing methods as well as certain opaque models. Keywords: PAC-Bayes, risk bounds, local models, transparent models, mixtures of local transparent models.", "link": "http://arxiv.org/abs/2601.10541v1", "date": "2026-01-15", "relevancy": 2.1712, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5758}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5209}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5153}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixtures%20of%20Transparent%20Local%20Models&body=Title%3A%20Mixtures%20of%20Transparent%20Local%20Models%0AAuthor%3A%20Niffa%20Cheick%20Oumar%20Diaby%20and%20Thierry%20Duchesne%20and%20Mario%20Marchand%0AAbstract%3A%20The%20predominance%20of%20machine%20learning%20models%20in%20many%20spheres%20of%20human%20activity%20has%20led%20to%20a%20growing%20demand%20for%20their%20transparency.%20The%20transparency%20of%20models%20makes%20it%20possible%20to%20discern%20some%20factors%2C%20such%20as%20security%20or%20non-discrimination.%20In%20this%20paper%2C%20we%20propose%20a%20mixture%20of%20transparent%20local%20models%20as%20an%20alternative%20solution%20for%20designing%20interpretable%20%28or%20transparent%29%20models.%20Our%20approach%20is%20designed%20for%20the%20situations%20where%20a%20simple%20and%20transparent%20function%20is%20suitable%20for%20modeling%20the%20label%20of%20instances%20in%20some%20localities/regions%20of%20the%20input%20space%2C%20but%20may%20change%20abruptly%20as%20we%20move%20from%20one%20locality%20to%20another.%20Consequently%2C%20the%20proposed%20algorithm%20is%20to%20learn%20both%20the%20transparent%20labeling%20function%20and%20the%20locality%20of%20the%20input%20space%20where%20the%20labeling%20function%20achieves%20a%20small%20risk%20in%20its%20assigned%20locality.%20By%20using%20a%20new%20multi-predictor%20%28and%20multi-locality%29%20loss%20function%2C%20we%20established%20rigorous%20PAC-Bayesian%20risk%20bounds%20for%20the%20case%20of%20binary%20linear%20classification%20problem%20and%20that%20of%20linear%20regression.%20In%20both%20cases%2C%20synthetic%20data%20sets%20were%20used%20to%20illustrate%20how%20the%20learning%20algorithms%20work.%20The%20results%20obtained%20from%20real%20data%20sets%20highlight%20the%20competitiveness%20of%20our%20approach%20compared%20to%20other%20existing%20methods%20as%20well%20as%20certain%20opaque%20models.%20Keywords%3A%20PAC-Bayes%2C%20risk%20bounds%2C%20local%20models%2C%20transparent%20models%2C%20mixtures%20of%20local%20transparent%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixtures%2520of%2520Transparent%2520Local%2520Models%26entry.906535625%3DNiffa%2520Cheick%2520Oumar%2520Diaby%2520and%2520Thierry%2520Duchesne%2520and%2520Mario%2520Marchand%26entry.1292438233%3DThe%2520predominance%2520of%2520machine%2520learning%2520models%2520in%2520many%2520spheres%2520of%2520human%2520activity%2520has%2520led%2520to%2520a%2520growing%2520demand%2520for%2520their%2520transparency.%2520The%2520transparency%2520of%2520models%2520makes%2520it%2520possible%2520to%2520discern%2520some%2520factors%252C%2520such%2520as%2520security%2520or%2520non-discrimination.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520mixture%2520of%2520transparent%2520local%2520models%2520as%2520an%2520alternative%2520solution%2520for%2520designing%2520interpretable%2520%2528or%2520transparent%2529%2520models.%2520Our%2520approach%2520is%2520designed%2520for%2520the%2520situations%2520where%2520a%2520simple%2520and%2520transparent%2520function%2520is%2520suitable%2520for%2520modeling%2520the%2520label%2520of%2520instances%2520in%2520some%2520localities/regions%2520of%2520the%2520input%2520space%252C%2520but%2520may%2520change%2520abruptly%2520as%2520we%2520move%2520from%2520one%2520locality%2520to%2520another.%2520Consequently%252C%2520the%2520proposed%2520algorithm%2520is%2520to%2520learn%2520both%2520the%2520transparent%2520labeling%2520function%2520and%2520the%2520locality%2520of%2520the%2520input%2520space%2520where%2520the%2520labeling%2520function%2520achieves%2520a%2520small%2520risk%2520in%2520its%2520assigned%2520locality.%2520By%2520using%2520a%2520new%2520multi-predictor%2520%2528and%2520multi-locality%2529%2520loss%2520function%252C%2520we%2520established%2520rigorous%2520PAC-Bayesian%2520risk%2520bounds%2520for%2520the%2520case%2520of%2520binary%2520linear%2520classification%2520problem%2520and%2520that%2520of%2520linear%2520regression.%2520In%2520both%2520cases%252C%2520synthetic%2520data%2520sets%2520were%2520used%2520to%2520illustrate%2520how%2520the%2520learning%2520algorithms%2520work.%2520The%2520results%2520obtained%2520from%2520real%2520data%2520sets%2520highlight%2520the%2520competitiveness%2520of%2520our%2520approach%2520compared%2520to%2520other%2520existing%2520methods%2520as%2520well%2520as%2520certain%2520opaque%2520models.%2520Keywords%253A%2520PAC-Bayes%252C%2520risk%2520bounds%252C%2520local%2520models%252C%2520transparent%2520models%252C%2520mixtures%2520of%2520local%2520transparent%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixtures%20of%20Transparent%20Local%20Models&entry.906535625=Niffa%20Cheick%20Oumar%20Diaby%20and%20Thierry%20Duchesne%20and%20Mario%20Marchand&entry.1292438233=The%20predominance%20of%20machine%20learning%20models%20in%20many%20spheres%20of%20human%20activity%20has%20led%20to%20a%20growing%20demand%20for%20their%20transparency.%20The%20transparency%20of%20models%20makes%20it%20possible%20to%20discern%20some%20factors%2C%20such%20as%20security%20or%20non-discrimination.%20In%20this%20paper%2C%20we%20propose%20a%20mixture%20of%20transparent%20local%20models%20as%20an%20alternative%20solution%20for%20designing%20interpretable%20%28or%20transparent%29%20models.%20Our%20approach%20is%20designed%20for%20the%20situations%20where%20a%20simple%20and%20transparent%20function%20is%20suitable%20for%20modeling%20the%20label%20of%20instances%20in%20some%20localities/regions%20of%20the%20input%20space%2C%20but%20may%20change%20abruptly%20as%20we%20move%20from%20one%20locality%20to%20another.%20Consequently%2C%20the%20proposed%20algorithm%20is%20to%20learn%20both%20the%20transparent%20labeling%20function%20and%20the%20locality%20of%20the%20input%20space%20where%20the%20labeling%20function%20achieves%20a%20small%20risk%20in%20its%20assigned%20locality.%20By%20using%20a%20new%20multi-predictor%20%28and%20multi-locality%29%20loss%20function%2C%20we%20established%20rigorous%20PAC-Bayesian%20risk%20bounds%20for%20the%20case%20of%20binary%20linear%20classification%20problem%20and%20that%20of%20linear%20regression.%20In%20both%20cases%2C%20synthetic%20data%20sets%20were%20used%20to%20illustrate%20how%20the%20learning%20algorithms%20work.%20The%20results%20obtained%20from%20real%20data%20sets%20highlight%20the%20competitiveness%20of%20our%20approach%20compared%20to%20other%20existing%20methods%20as%20well%20as%20certain%20opaque%20models.%20Keywords%3A%20PAC-Bayes%2C%20risk%20bounds%2C%20local%20models%2C%20transparent%20models%2C%20mixtures%20of%20local%20transparent%20models.&entry.1838667208=http%3A//arxiv.org/abs/2601.10541v1&entry.124074799=Read"},
{"title": "GANeXt: A Fully ConvNeXt-Enhanced Generative Adversarial Network for MRI- and CBCT-to-CT Synthesis", "author": "Siyuan Mei and Yan Xia and Fuxin Fan and Andreas Maier", "abstract": "The synthesis of computed tomography (CT) from magnetic resonance imaging (MRI) and cone-beam CT (CBCT) plays a critical role in clinical treatment planning by enabling accurate anatomical representation in adaptive radiotherapy. In this work, we propose GANeXt, a 3D patch-based, fully ConvNeXt-powered generative adversarial network for unified CT synthesis across different modalities and anatomical regions. Specifically, GANeXt employs an efficient U-shaped generator constructed from stacked 3D ConvNeXt blocks with compact convolution kernels, while the discriminator adopts a conditional PatchGAN. To improve synthesis quality, we incorporate a combination of loss functions, including mean absolute error (MAE), perceptual loss, segmentation-based masked MAE, and adversarial loss and a combination of Dice loss and cross-entropy for multi-head segmentation discriminator. For both tasks, training is performed with a batch size of 8 using two separate AdamW optimizers for the generator and discriminator, each equipped with a warmup and cosine decay scheduler, with learning rates of $5\\times10^{-4}$ and $1\\times10^{-3}$, respectively. Data preprocessing includes deformable registration, foreground cropping, percentile normalization for the input modality, and linear normalization of the CT to the range $[-1024, 1000]$. Data augmentation involves random zooming within $(0.8, 1.3)$ (for MRI-to-CT only), fixed-size cropping to $32\\times160\\times192$ for MRI-to-CT and $32\\times128\\times128$ for CBCT-to-CT, and random flipping. During inference, we apply a sliding-window approach with $0.8$ overlap and average folding to reconstruct the full-size sCT, followed by inversion of the CT normalization. After joint training on all regions without any fine-tuning, the final models are selected at the end of 3000 epochs for MRI-to-CT and 1000 epochs for CBCT-to-CT using the full training dataset.", "link": "http://arxiv.org/abs/2512.19336v2", "date": "2026-01-15", "relevancy": 2.1673, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5463}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5426}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GANeXt%3A%20A%20Fully%20ConvNeXt-Enhanced%20Generative%20Adversarial%20Network%20for%20MRI-%20and%20CBCT-to-CT%20Synthesis&body=Title%3A%20GANeXt%3A%20A%20Fully%20ConvNeXt-Enhanced%20Generative%20Adversarial%20Network%20for%20MRI-%20and%20CBCT-to-CT%20Synthesis%0AAuthor%3A%20Siyuan%20Mei%20and%20Yan%20Xia%20and%20Fuxin%20Fan%20and%20Andreas%20Maier%0AAbstract%3A%20The%20synthesis%20of%20computed%20tomography%20%28CT%29%20from%20magnetic%20resonance%20imaging%20%28MRI%29%20and%20cone-beam%20CT%20%28CBCT%29%20plays%20a%20critical%20role%20in%20clinical%20treatment%20planning%20by%20enabling%20accurate%20anatomical%20representation%20in%20adaptive%20radiotherapy.%20In%20this%20work%2C%20we%20propose%20GANeXt%2C%20a%203D%20patch-based%2C%20fully%20ConvNeXt-powered%20generative%20adversarial%20network%20for%20unified%20CT%20synthesis%20across%20different%20modalities%20and%20anatomical%20regions.%20Specifically%2C%20GANeXt%20employs%20an%20efficient%20U-shaped%20generator%20constructed%20from%20stacked%203D%20ConvNeXt%20blocks%20with%20compact%20convolution%20kernels%2C%20while%20the%20discriminator%20adopts%20a%20conditional%20PatchGAN.%20To%20improve%20synthesis%20quality%2C%20we%20incorporate%20a%20combination%20of%20loss%20functions%2C%20including%20mean%20absolute%20error%20%28MAE%29%2C%20perceptual%20loss%2C%20segmentation-based%20masked%20MAE%2C%20and%20adversarial%20loss%20and%20a%20combination%20of%20Dice%20loss%20and%20cross-entropy%20for%20multi-head%20segmentation%20discriminator.%20For%20both%20tasks%2C%20training%20is%20performed%20with%20a%20batch%20size%20of%208%20using%20two%20separate%20AdamW%20optimizers%20for%20the%20generator%20and%20discriminator%2C%20each%20equipped%20with%20a%20warmup%20and%20cosine%20decay%20scheduler%2C%20with%20learning%20rates%20of%20%245%5Ctimes10%5E%7B-4%7D%24%20and%20%241%5Ctimes10%5E%7B-3%7D%24%2C%20respectively.%20Data%20preprocessing%20includes%20deformable%20registration%2C%20foreground%20cropping%2C%20percentile%20normalization%20for%20the%20input%20modality%2C%20and%20linear%20normalization%20of%20the%20CT%20to%20the%20range%20%24%5B-1024%2C%201000%5D%24.%20Data%20augmentation%20involves%20random%20zooming%20within%20%24%280.8%2C%201.3%29%24%20%28for%20MRI-to-CT%20only%29%2C%20fixed-size%20cropping%20to%20%2432%5Ctimes160%5Ctimes192%24%20for%20MRI-to-CT%20and%20%2432%5Ctimes128%5Ctimes128%24%20for%20CBCT-to-CT%2C%20and%20random%20flipping.%20During%20inference%2C%20we%20apply%20a%20sliding-window%20approach%20with%20%240.8%24%20overlap%20and%20average%20folding%20to%20reconstruct%20the%20full-size%20sCT%2C%20followed%20by%20inversion%20of%20the%20CT%20normalization.%20After%20joint%20training%20on%20all%20regions%20without%20any%20fine-tuning%2C%20the%20final%20models%20are%20selected%20at%20the%20end%20of%203000%20epochs%20for%20MRI-to-CT%20and%201000%20epochs%20for%20CBCT-to-CT%20using%20the%20full%20training%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/2512.19336v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGANeXt%253A%2520A%2520Fully%2520ConvNeXt-Enhanced%2520Generative%2520Adversarial%2520Network%2520for%2520MRI-%2520and%2520CBCT-to-CT%2520Synthesis%26entry.906535625%3DSiyuan%2520Mei%2520and%2520Yan%2520Xia%2520and%2520Fuxin%2520Fan%2520and%2520Andreas%2520Maier%26entry.1292438233%3DThe%2520synthesis%2520of%2520computed%2520tomography%2520%2528CT%2529%2520from%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%2520and%2520cone-beam%2520CT%2520%2528CBCT%2529%2520plays%2520a%2520critical%2520role%2520in%2520clinical%2520treatment%2520planning%2520by%2520enabling%2520accurate%2520anatomical%2520representation%2520in%2520adaptive%2520radiotherapy.%2520In%2520this%2520work%252C%2520we%2520propose%2520GANeXt%252C%2520a%25203D%2520patch-based%252C%2520fully%2520ConvNeXt-powered%2520generative%2520adversarial%2520network%2520for%2520unified%2520CT%2520synthesis%2520across%2520different%2520modalities%2520and%2520anatomical%2520regions.%2520Specifically%252C%2520GANeXt%2520employs%2520an%2520efficient%2520U-shaped%2520generator%2520constructed%2520from%2520stacked%25203D%2520ConvNeXt%2520blocks%2520with%2520compact%2520convolution%2520kernels%252C%2520while%2520the%2520discriminator%2520adopts%2520a%2520conditional%2520PatchGAN.%2520To%2520improve%2520synthesis%2520quality%252C%2520we%2520incorporate%2520a%2520combination%2520of%2520loss%2520functions%252C%2520including%2520mean%2520absolute%2520error%2520%2528MAE%2529%252C%2520perceptual%2520loss%252C%2520segmentation-based%2520masked%2520MAE%252C%2520and%2520adversarial%2520loss%2520and%2520a%2520combination%2520of%2520Dice%2520loss%2520and%2520cross-entropy%2520for%2520multi-head%2520segmentation%2520discriminator.%2520For%2520both%2520tasks%252C%2520training%2520is%2520performed%2520with%2520a%2520batch%2520size%2520of%25208%2520using%2520two%2520separate%2520AdamW%2520optimizers%2520for%2520the%2520generator%2520and%2520discriminator%252C%2520each%2520equipped%2520with%2520a%2520warmup%2520and%2520cosine%2520decay%2520scheduler%252C%2520with%2520learning%2520rates%2520of%2520%25245%255Ctimes10%255E%257B-4%257D%2524%2520and%2520%25241%255Ctimes10%255E%257B-3%257D%2524%252C%2520respectively.%2520Data%2520preprocessing%2520includes%2520deformable%2520registration%252C%2520foreground%2520cropping%252C%2520percentile%2520normalization%2520for%2520the%2520input%2520modality%252C%2520and%2520linear%2520normalization%2520of%2520the%2520CT%2520to%2520the%2520range%2520%2524%255B-1024%252C%25201000%255D%2524.%2520Data%2520augmentation%2520involves%2520random%2520zooming%2520within%2520%2524%25280.8%252C%25201.3%2529%2524%2520%2528for%2520MRI-to-CT%2520only%2529%252C%2520fixed-size%2520cropping%2520to%2520%252432%255Ctimes160%255Ctimes192%2524%2520for%2520MRI-to-CT%2520and%2520%252432%255Ctimes128%255Ctimes128%2524%2520for%2520CBCT-to-CT%252C%2520and%2520random%2520flipping.%2520During%2520inference%252C%2520we%2520apply%2520a%2520sliding-window%2520approach%2520with%2520%25240.8%2524%2520overlap%2520and%2520average%2520folding%2520to%2520reconstruct%2520the%2520full-size%2520sCT%252C%2520followed%2520by%2520inversion%2520of%2520the%2520CT%2520normalization.%2520After%2520joint%2520training%2520on%2520all%2520regions%2520without%2520any%2520fine-tuning%252C%2520the%2520final%2520models%2520are%2520selected%2520at%2520the%2520end%2520of%25203000%2520epochs%2520for%2520MRI-to-CT%2520and%25201000%2520epochs%2520for%2520CBCT-to-CT%2520using%2520the%2520full%2520training%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.19336v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GANeXt%3A%20A%20Fully%20ConvNeXt-Enhanced%20Generative%20Adversarial%20Network%20for%20MRI-%20and%20CBCT-to-CT%20Synthesis&entry.906535625=Siyuan%20Mei%20and%20Yan%20Xia%20and%20Fuxin%20Fan%20and%20Andreas%20Maier&entry.1292438233=The%20synthesis%20of%20computed%20tomography%20%28CT%29%20from%20magnetic%20resonance%20imaging%20%28MRI%29%20and%20cone-beam%20CT%20%28CBCT%29%20plays%20a%20critical%20role%20in%20clinical%20treatment%20planning%20by%20enabling%20accurate%20anatomical%20representation%20in%20adaptive%20radiotherapy.%20In%20this%20work%2C%20we%20propose%20GANeXt%2C%20a%203D%20patch-based%2C%20fully%20ConvNeXt-powered%20generative%20adversarial%20network%20for%20unified%20CT%20synthesis%20across%20different%20modalities%20and%20anatomical%20regions.%20Specifically%2C%20GANeXt%20employs%20an%20efficient%20U-shaped%20generator%20constructed%20from%20stacked%203D%20ConvNeXt%20blocks%20with%20compact%20convolution%20kernels%2C%20while%20the%20discriminator%20adopts%20a%20conditional%20PatchGAN.%20To%20improve%20synthesis%20quality%2C%20we%20incorporate%20a%20combination%20of%20loss%20functions%2C%20including%20mean%20absolute%20error%20%28MAE%29%2C%20perceptual%20loss%2C%20segmentation-based%20masked%20MAE%2C%20and%20adversarial%20loss%20and%20a%20combination%20of%20Dice%20loss%20and%20cross-entropy%20for%20multi-head%20segmentation%20discriminator.%20For%20both%20tasks%2C%20training%20is%20performed%20with%20a%20batch%20size%20of%208%20using%20two%20separate%20AdamW%20optimizers%20for%20the%20generator%20and%20discriminator%2C%20each%20equipped%20with%20a%20warmup%20and%20cosine%20decay%20scheduler%2C%20with%20learning%20rates%20of%20%245%5Ctimes10%5E%7B-4%7D%24%20and%20%241%5Ctimes10%5E%7B-3%7D%24%2C%20respectively.%20Data%20preprocessing%20includes%20deformable%20registration%2C%20foreground%20cropping%2C%20percentile%20normalization%20for%20the%20input%20modality%2C%20and%20linear%20normalization%20of%20the%20CT%20to%20the%20range%20%24%5B-1024%2C%201000%5D%24.%20Data%20augmentation%20involves%20random%20zooming%20within%20%24%280.8%2C%201.3%29%24%20%28for%20MRI-to-CT%20only%29%2C%20fixed-size%20cropping%20to%20%2432%5Ctimes160%5Ctimes192%24%20for%20MRI-to-CT%20and%20%2432%5Ctimes128%5Ctimes128%24%20for%20CBCT-to-CT%2C%20and%20random%20flipping.%20During%20inference%2C%20we%20apply%20a%20sliding-window%20approach%20with%20%240.8%24%20overlap%20and%20average%20folding%20to%20reconstruct%20the%20full-size%20sCT%2C%20followed%20by%20inversion%20of%20the%20CT%20normalization.%20After%20joint%20training%20on%20all%20regions%20without%20any%20fine-tuning%2C%20the%20final%20models%20are%20selected%20at%20the%20end%20of%203000%20epochs%20for%20MRI-to-CT%20and%201000%20epochs%20for%20CBCT-to-CT%20using%20the%20full%20training%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/2512.19336v2&entry.124074799=Read"},
{"title": "The impact of tactile sensor configurations on grasp learning efficiency -- a comparative evaluation in simulation", "author": "Eszter Birtalan and Mikl\u00f3s Koller", "abstract": "Tactile sensors are breaking into the field of robotics to provide direct information related to contact surfaces, including contact events, slip events and even texture identification. These events are especially important for robotic hand designs, including prosthetics, as they can greatly improve grasp stability. Most presently published robotic hand designs, however, implement them in vastly different densities and layouts on the hand surface, often reserving the majority of the available space. We used simulations to evaluate 6 different tactile sensor configurations with different densities and layouts, based on their impact on reinforcement learning. Our two-setup system allows for robust results that are not dependent on the use of a given physics simulator, robotic hand model or machine learning algorithm. Our results show setup-specific, as well as generalized effects across the 6 sensorized simulations, and we identify one configuration as consistently yielding the best performance across both setups. These results could help future research aimed at robotic hand designs, including prostheses.", "link": "http://arxiv.org/abs/2601.10268v1", "date": "2026-01-15", "relevancy": 2.1565, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5743}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5426}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5215}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20impact%20of%20tactile%20sensor%20configurations%20on%20grasp%20learning%20efficiency%20--%20a%20comparative%20evaluation%20in%20simulation&body=Title%3A%20The%20impact%20of%20tactile%20sensor%20configurations%20on%20grasp%20learning%20efficiency%20--%20a%20comparative%20evaluation%20in%20simulation%0AAuthor%3A%20Eszter%20Birtalan%20and%20Mikl%C3%B3s%20Koller%0AAbstract%3A%20Tactile%20sensors%20are%20breaking%20into%20the%20field%20of%20robotics%20to%20provide%20direct%20information%20related%20to%20contact%20surfaces%2C%20including%20contact%20events%2C%20slip%20events%20and%20even%20texture%20identification.%20These%20events%20are%20especially%20important%20for%20robotic%20hand%20designs%2C%20including%20prosthetics%2C%20as%20they%20can%20greatly%20improve%20grasp%20stability.%20Most%20presently%20published%20robotic%20hand%20designs%2C%20however%2C%20implement%20them%20in%20vastly%20different%20densities%20and%20layouts%20on%20the%20hand%20surface%2C%20often%20reserving%20the%20majority%20of%20the%20available%20space.%20We%20used%20simulations%20to%20evaluate%206%20different%20tactile%20sensor%20configurations%20with%20different%20densities%20and%20layouts%2C%20based%20on%20their%20impact%20on%20reinforcement%20learning.%20Our%20two-setup%20system%20allows%20for%20robust%20results%20that%20are%20not%20dependent%20on%20the%20use%20of%20a%20given%20physics%20simulator%2C%20robotic%20hand%20model%20or%20machine%20learning%20algorithm.%20Our%20results%20show%20setup-specific%2C%20as%20well%20as%20generalized%20effects%20across%20the%206%20sensorized%20simulations%2C%20and%20we%20identify%20one%20configuration%20as%20consistently%20yielding%20the%20best%20performance%20across%20both%20setups.%20These%20results%20could%20help%20future%20research%20aimed%20at%20robotic%20hand%20designs%2C%20including%20prostheses.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10268v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520impact%2520of%2520tactile%2520sensor%2520configurations%2520on%2520grasp%2520learning%2520efficiency%2520--%2520a%2520comparative%2520evaluation%2520in%2520simulation%26entry.906535625%3DEszter%2520Birtalan%2520and%2520Mikl%25C3%25B3s%2520Koller%26entry.1292438233%3DTactile%2520sensors%2520are%2520breaking%2520into%2520the%2520field%2520of%2520robotics%2520to%2520provide%2520direct%2520information%2520related%2520to%2520contact%2520surfaces%252C%2520including%2520contact%2520events%252C%2520slip%2520events%2520and%2520even%2520texture%2520identification.%2520These%2520events%2520are%2520especially%2520important%2520for%2520robotic%2520hand%2520designs%252C%2520including%2520prosthetics%252C%2520as%2520they%2520can%2520greatly%2520improve%2520grasp%2520stability.%2520Most%2520presently%2520published%2520robotic%2520hand%2520designs%252C%2520however%252C%2520implement%2520them%2520in%2520vastly%2520different%2520densities%2520and%2520layouts%2520on%2520the%2520hand%2520surface%252C%2520often%2520reserving%2520the%2520majority%2520of%2520the%2520available%2520space.%2520We%2520used%2520simulations%2520to%2520evaluate%25206%2520different%2520tactile%2520sensor%2520configurations%2520with%2520different%2520densities%2520and%2520layouts%252C%2520based%2520on%2520their%2520impact%2520on%2520reinforcement%2520learning.%2520Our%2520two-setup%2520system%2520allows%2520for%2520robust%2520results%2520that%2520are%2520not%2520dependent%2520on%2520the%2520use%2520of%2520a%2520given%2520physics%2520simulator%252C%2520robotic%2520hand%2520model%2520or%2520machine%2520learning%2520algorithm.%2520Our%2520results%2520show%2520setup-specific%252C%2520as%2520well%2520as%2520generalized%2520effects%2520across%2520the%25206%2520sensorized%2520simulations%252C%2520and%2520we%2520identify%2520one%2520configuration%2520as%2520consistently%2520yielding%2520the%2520best%2520performance%2520across%2520both%2520setups.%2520These%2520results%2520could%2520help%2520future%2520research%2520aimed%2520at%2520robotic%2520hand%2520designs%252C%2520including%2520prostheses.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10268v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20impact%20of%20tactile%20sensor%20configurations%20on%20grasp%20learning%20efficiency%20--%20a%20comparative%20evaluation%20in%20simulation&entry.906535625=Eszter%20Birtalan%20and%20Mikl%C3%B3s%20Koller&entry.1292438233=Tactile%20sensors%20are%20breaking%20into%20the%20field%20of%20robotics%20to%20provide%20direct%20information%20related%20to%20contact%20surfaces%2C%20including%20contact%20events%2C%20slip%20events%20and%20even%20texture%20identification.%20These%20events%20are%20especially%20important%20for%20robotic%20hand%20designs%2C%20including%20prosthetics%2C%20as%20they%20can%20greatly%20improve%20grasp%20stability.%20Most%20presently%20published%20robotic%20hand%20designs%2C%20however%2C%20implement%20them%20in%20vastly%20different%20densities%20and%20layouts%20on%20the%20hand%20surface%2C%20often%20reserving%20the%20majority%20of%20the%20available%20space.%20We%20used%20simulations%20to%20evaluate%206%20different%20tactile%20sensor%20configurations%20with%20different%20densities%20and%20layouts%2C%20based%20on%20their%20impact%20on%20reinforcement%20learning.%20Our%20two-setup%20system%20allows%20for%20robust%20results%20that%20are%20not%20dependent%20on%20the%20use%20of%20a%20given%20physics%20simulator%2C%20robotic%20hand%20model%20or%20machine%20learning%20algorithm.%20Our%20results%20show%20setup-specific%2C%20as%20well%20as%20generalized%20effects%20across%20the%206%20sensorized%20simulations%2C%20and%20we%20identify%20one%20configuration%20as%20consistently%20yielding%20the%20best%20performance%20across%20both%20setups.%20These%20results%20could%20help%20future%20research%20aimed%20at%20robotic%20hand%20designs%2C%20including%20prostheses.&entry.1838667208=http%3A//arxiv.org/abs/2601.10268v1&entry.124074799=Read"},
{"title": "AgentGuardian: Learning Access Control Policies to Govern AI Agent Behavior", "author": "Nadya Abaev and Denis Klimov and Gerard Levinov and David Mimran and Yuval Elovici and Asaf Shabtai", "abstract": "Artificial intelligence (AI) agents are increasingly used in a variety of domains to automate tasks, interact with users, and make decisions based on data inputs. Ensuring that AI agents perform only authorized actions and handle inputs appropriately is essential for maintaining system integrity and preventing misuse. In this study, we introduce the AgentGuardian, a novel security framework that governs and protects AI agent operations by enforcing context-aware access-control policies. During a controlled staging phase, the framework monitors execution traces to learn legitimate agent behaviors and input patterns. From this phase, it derives adaptive policies that regulate tool calls made by the agent, guided by both real-time input context and the control flow dependencies of multi-step agent actions. Evaluation across two real-world AI agent applications demonstrates that AgentGuardian effectively detects malicious or misleading inputs while preserving normal agent functionality. Moreover, its control-flow-based governance mechanism mitigates hallucination-driven errors and other orchestration-level malfunctions.", "link": "http://arxiv.org/abs/2601.10440v1", "date": "2026-01-15", "relevancy": 2.1532, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5983}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5012}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4809}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AgentGuardian%3A%20Learning%20Access%20Control%20Policies%20to%20Govern%20AI%20Agent%20Behavior&body=Title%3A%20AgentGuardian%3A%20Learning%20Access%20Control%20Policies%20to%20Govern%20AI%20Agent%20Behavior%0AAuthor%3A%20Nadya%20Abaev%20and%20Denis%20Klimov%20and%20Gerard%20Levinov%20and%20David%20Mimran%20and%20Yuval%20Elovici%20and%20Asaf%20Shabtai%0AAbstract%3A%20Artificial%20intelligence%20%28AI%29%20agents%20are%20increasingly%20used%20in%20a%20variety%20of%20domains%20to%20automate%20tasks%2C%20interact%20with%20users%2C%20and%20make%20decisions%20based%20on%20data%20inputs.%20Ensuring%20that%20AI%20agents%20perform%20only%20authorized%20actions%20and%20handle%20inputs%20appropriately%20is%20essential%20for%20maintaining%20system%20integrity%20and%20preventing%20misuse.%20In%20this%20study%2C%20we%20introduce%20the%20AgentGuardian%2C%20a%20novel%20security%20framework%20that%20governs%20and%20protects%20AI%20agent%20operations%20by%20enforcing%20context-aware%20access-control%20policies.%20During%20a%20controlled%20staging%20phase%2C%20the%20framework%20monitors%20execution%20traces%20to%20learn%20legitimate%20agent%20behaviors%20and%20input%20patterns.%20From%20this%20phase%2C%20it%20derives%20adaptive%20policies%20that%20regulate%20tool%20calls%20made%20by%20the%20agent%2C%20guided%20by%20both%20real-time%20input%20context%20and%20the%20control%20flow%20dependencies%20of%20multi-step%20agent%20actions.%20Evaluation%20across%20two%20real-world%20AI%20agent%20applications%20demonstrates%20that%20AgentGuardian%20effectively%20detects%20malicious%20or%20misleading%20inputs%20while%20preserving%20normal%20agent%20functionality.%20Moreover%2C%20its%20control-flow-based%20governance%20mechanism%20mitigates%20hallucination-driven%20errors%20and%20other%20orchestration-level%20malfunctions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10440v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgentGuardian%253A%2520Learning%2520Access%2520Control%2520Policies%2520to%2520Govern%2520AI%2520Agent%2520Behavior%26entry.906535625%3DNadya%2520Abaev%2520and%2520Denis%2520Klimov%2520and%2520Gerard%2520Levinov%2520and%2520David%2520Mimran%2520and%2520Yuval%2520Elovici%2520and%2520Asaf%2520Shabtai%26entry.1292438233%3DArtificial%2520intelligence%2520%2528AI%2529%2520agents%2520are%2520increasingly%2520used%2520in%2520a%2520variety%2520of%2520domains%2520to%2520automate%2520tasks%252C%2520interact%2520with%2520users%252C%2520and%2520make%2520decisions%2520based%2520on%2520data%2520inputs.%2520Ensuring%2520that%2520AI%2520agents%2520perform%2520only%2520authorized%2520actions%2520and%2520handle%2520inputs%2520appropriately%2520is%2520essential%2520for%2520maintaining%2520system%2520integrity%2520and%2520preventing%2520misuse.%2520In%2520this%2520study%252C%2520we%2520introduce%2520the%2520AgentGuardian%252C%2520a%2520novel%2520security%2520framework%2520that%2520governs%2520and%2520protects%2520AI%2520agent%2520operations%2520by%2520enforcing%2520context-aware%2520access-control%2520policies.%2520During%2520a%2520controlled%2520staging%2520phase%252C%2520the%2520framework%2520monitors%2520execution%2520traces%2520to%2520learn%2520legitimate%2520agent%2520behaviors%2520and%2520input%2520patterns.%2520From%2520this%2520phase%252C%2520it%2520derives%2520adaptive%2520policies%2520that%2520regulate%2520tool%2520calls%2520made%2520by%2520the%2520agent%252C%2520guided%2520by%2520both%2520real-time%2520input%2520context%2520and%2520the%2520control%2520flow%2520dependencies%2520of%2520multi-step%2520agent%2520actions.%2520Evaluation%2520across%2520two%2520real-world%2520AI%2520agent%2520applications%2520demonstrates%2520that%2520AgentGuardian%2520effectively%2520detects%2520malicious%2520or%2520misleading%2520inputs%2520while%2520preserving%2520normal%2520agent%2520functionality.%2520Moreover%252C%2520its%2520control-flow-based%2520governance%2520mechanism%2520mitigates%2520hallucination-driven%2520errors%2520and%2520other%2520orchestration-level%2520malfunctions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10440v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AgentGuardian%3A%20Learning%20Access%20Control%20Policies%20to%20Govern%20AI%20Agent%20Behavior&entry.906535625=Nadya%20Abaev%20and%20Denis%20Klimov%20and%20Gerard%20Levinov%20and%20David%20Mimran%20and%20Yuval%20Elovici%20and%20Asaf%20Shabtai&entry.1292438233=Artificial%20intelligence%20%28AI%29%20agents%20are%20increasingly%20used%20in%20a%20variety%20of%20domains%20to%20automate%20tasks%2C%20interact%20with%20users%2C%20and%20make%20decisions%20based%20on%20data%20inputs.%20Ensuring%20that%20AI%20agents%20perform%20only%20authorized%20actions%20and%20handle%20inputs%20appropriately%20is%20essential%20for%20maintaining%20system%20integrity%20and%20preventing%20misuse.%20In%20this%20study%2C%20we%20introduce%20the%20AgentGuardian%2C%20a%20novel%20security%20framework%20that%20governs%20and%20protects%20AI%20agent%20operations%20by%20enforcing%20context-aware%20access-control%20policies.%20During%20a%20controlled%20staging%20phase%2C%20the%20framework%20monitors%20execution%20traces%20to%20learn%20legitimate%20agent%20behaviors%20and%20input%20patterns.%20From%20this%20phase%2C%20it%20derives%20adaptive%20policies%20that%20regulate%20tool%20calls%20made%20by%20the%20agent%2C%20guided%20by%20both%20real-time%20input%20context%20and%20the%20control%20flow%20dependencies%20of%20multi-step%20agent%20actions.%20Evaluation%20across%20two%20real-world%20AI%20agent%20applications%20demonstrates%20that%20AgentGuardian%20effectively%20detects%20malicious%20or%20misleading%20inputs%20while%20preserving%20normal%20agent%20functionality.%20Moreover%2C%20its%20control-flow-based%20governance%20mechanism%20mitigates%20hallucination-driven%20errors%20and%20other%20orchestration-level%20malfunctions.&entry.1838667208=http%3A//arxiv.org/abs/2601.10440v1&entry.124074799=Read"},
{"title": "A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5", "author": "Xingjun Ma and Yixu Wang and Hengyuan Xu and Yutao Wu and Yifan Ding and Yunhan Zhao and Zilong Wang and Jiabin Hua and Ming Wen and Jianan Liu and Ranjie Duan and Yifeng Gao and Yingshui Tan and Yunhao Chen and Hui Xue and Xin Wang and Wei Cheng and Jingjing Chen and Zuxuan Wu and Bo Li and Yu-Gang Jiang", "abstract": "The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5. We evaluate each model across language, vision-language, and image generation settings using a unified protocol that integrates benchmark evaluation, adversarial evaluation, multilingual evaluation, and compliance evaluation. Aggregating our evaluations into safety leaderboards and model safety profiles across multiple evaluation modes reveals a sharply heterogeneous safety landscape. While GPT-5.2 demonstrates consistently strong and balanced safety performance across evaluations, other models exhibit pronounced trade-offs among benchmark safety, adversarial alignment, multilingual generalization, and regulatory compliance. Both language and vision-language modalities show significant vulnerability under adversarial evaluation, with all models degrading substantially despite strong results on standard benchmarks. Text-to-image models achieve relatively stronger alignment in regulated visual risk categories, yet remain brittle under adversarial or semantically ambiguous prompts. Overall, these results show that safety in frontier models is inherently multidimensional--shaped by modality, language, and evaluation scheme, underscoring the need for standardized safety evaluations to accurately assess real-world risk and guide responsible model development and deployment.", "link": "http://arxiv.org/abs/2601.10527v1", "date": "2026-01-15", "relevancy": 2.15, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5403}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5403}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Safety%20Report%20on%20GPT-5.2%2C%20Gemini%203%20Pro%2C%20Qwen3-VL%2C%20Doubao%201.8%2C%20Grok%204.1%20Fast%2C%20Nano%20Banana%20Pro%2C%20and%20Seedream%204.5&body=Title%3A%20A%20Safety%20Report%20on%20GPT-5.2%2C%20Gemini%203%20Pro%2C%20Qwen3-VL%2C%20Doubao%201.8%2C%20Grok%204.1%20Fast%2C%20Nano%20Banana%20Pro%2C%20and%20Seedream%204.5%0AAuthor%3A%20Xingjun%20Ma%20and%20Yixu%20Wang%20and%20Hengyuan%20Xu%20and%20Yutao%20Wu%20and%20Yifan%20Ding%20and%20Yunhan%20Zhao%20and%20Zilong%20Wang%20and%20Jiabin%20Hua%20and%20Ming%20Wen%20and%20Jianan%20Liu%20and%20Ranjie%20Duan%20and%20Yifeng%20Gao%20and%20Yingshui%20Tan%20and%20Yunhao%20Chen%20and%20Hui%20Xue%20and%20Xin%20Wang%20and%20Wei%20Cheng%20and%20Jingjing%20Chen%20and%20Zuxuan%20Wu%20and%20Bo%20Li%20and%20Yu-Gang%20Jiang%0AAbstract%3A%20The%20rapid%20evolution%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20produced%20substantial%20gains%20in%20reasoning%2C%20perception%2C%20and%20generative%20capability%20across%20language%20and%20vision.%20However%2C%20whether%20these%20advances%20yield%20commensurate%20improvements%20in%20safety%20remains%20unclear%2C%20in%20part%20due%20to%20fragmented%20evaluation%20practices%20limited%20to%20single%20modalities%20or%20threat%20models.%20In%20this%20report%2C%20we%20present%20an%20integrated%20safety%20evaluation%20of%207%20frontier%20models%3A%20GPT-5.2%2C%20Gemini%203%20Pro%2C%20Qwen3-VL%2C%20Doubao%201.8%2C%20Grok%204.1%20Fast%2C%20Nano%20Banana%20Pro%2C%20and%20Seedream%204.5.%20We%20evaluate%20each%20model%20across%20language%2C%20vision-language%2C%20and%20image%20generation%20settings%20using%20a%20unified%20protocol%20that%20integrates%20benchmark%20evaluation%2C%20adversarial%20evaluation%2C%20multilingual%20evaluation%2C%20and%20compliance%20evaluation.%20Aggregating%20our%20evaluations%20into%20safety%20leaderboards%20and%20model%20safety%20profiles%20across%20multiple%20evaluation%20modes%20reveals%20a%20sharply%20heterogeneous%20safety%20landscape.%20While%20GPT-5.2%20demonstrates%20consistently%20strong%20and%20balanced%20safety%20performance%20across%20evaluations%2C%20other%20models%20exhibit%20pronounced%20trade-offs%20among%20benchmark%20safety%2C%20adversarial%20alignment%2C%20multilingual%20generalization%2C%20and%20regulatory%20compliance.%20Both%20language%20and%20vision-language%20modalities%20show%20significant%20vulnerability%20under%20adversarial%20evaluation%2C%20with%20all%20models%20degrading%20substantially%20despite%20strong%20results%20on%20standard%20benchmarks.%20Text-to-image%20models%20achieve%20relatively%20stronger%20alignment%20in%20regulated%20visual%20risk%20categories%2C%20yet%20remain%20brittle%20under%20adversarial%20or%20semantically%20ambiguous%20prompts.%20Overall%2C%20these%20results%20show%20that%20safety%20in%20frontier%20models%20is%20inherently%20multidimensional--shaped%20by%20modality%2C%20language%2C%20and%20evaluation%20scheme%2C%20underscoring%20the%20need%20for%20standardized%20safety%20evaluations%20to%20accurately%20assess%20real-world%20risk%20and%20guide%20responsible%20model%20development%20and%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Safety%2520Report%2520on%2520GPT-5.2%252C%2520Gemini%25203%2520Pro%252C%2520Qwen3-VL%252C%2520Doubao%25201.8%252C%2520Grok%25204.1%2520Fast%252C%2520Nano%2520Banana%2520Pro%252C%2520and%2520Seedream%25204.5%26entry.906535625%3DXingjun%2520Ma%2520and%2520Yixu%2520Wang%2520and%2520Hengyuan%2520Xu%2520and%2520Yutao%2520Wu%2520and%2520Yifan%2520Ding%2520and%2520Yunhan%2520Zhao%2520and%2520Zilong%2520Wang%2520and%2520Jiabin%2520Hua%2520and%2520Ming%2520Wen%2520and%2520Jianan%2520Liu%2520and%2520Ranjie%2520Duan%2520and%2520Yifeng%2520Gao%2520and%2520Yingshui%2520Tan%2520and%2520Yunhao%2520Chen%2520and%2520Hui%2520Xue%2520and%2520Xin%2520Wang%2520and%2520Wei%2520Cheng%2520and%2520Jingjing%2520Chen%2520and%2520Zuxuan%2520Wu%2520and%2520Bo%2520Li%2520and%2520Yu-Gang%2520Jiang%26entry.1292438233%3DThe%2520rapid%2520evolution%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520and%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520has%2520produced%2520substantial%2520gains%2520in%2520reasoning%252C%2520perception%252C%2520and%2520generative%2520capability%2520across%2520language%2520and%2520vision.%2520However%252C%2520whether%2520these%2520advances%2520yield%2520commensurate%2520improvements%2520in%2520safety%2520remains%2520unclear%252C%2520in%2520part%2520due%2520to%2520fragmented%2520evaluation%2520practices%2520limited%2520to%2520single%2520modalities%2520or%2520threat%2520models.%2520In%2520this%2520report%252C%2520we%2520present%2520an%2520integrated%2520safety%2520evaluation%2520of%25207%2520frontier%2520models%253A%2520GPT-5.2%252C%2520Gemini%25203%2520Pro%252C%2520Qwen3-VL%252C%2520Doubao%25201.8%252C%2520Grok%25204.1%2520Fast%252C%2520Nano%2520Banana%2520Pro%252C%2520and%2520Seedream%25204.5.%2520We%2520evaluate%2520each%2520model%2520across%2520language%252C%2520vision-language%252C%2520and%2520image%2520generation%2520settings%2520using%2520a%2520unified%2520protocol%2520that%2520integrates%2520benchmark%2520evaluation%252C%2520adversarial%2520evaluation%252C%2520multilingual%2520evaluation%252C%2520and%2520compliance%2520evaluation.%2520Aggregating%2520our%2520evaluations%2520into%2520safety%2520leaderboards%2520and%2520model%2520safety%2520profiles%2520across%2520multiple%2520evaluation%2520modes%2520reveals%2520a%2520sharply%2520heterogeneous%2520safety%2520landscape.%2520While%2520GPT-5.2%2520demonstrates%2520consistently%2520strong%2520and%2520balanced%2520safety%2520performance%2520across%2520evaluations%252C%2520other%2520models%2520exhibit%2520pronounced%2520trade-offs%2520among%2520benchmark%2520safety%252C%2520adversarial%2520alignment%252C%2520multilingual%2520generalization%252C%2520and%2520regulatory%2520compliance.%2520Both%2520language%2520and%2520vision-language%2520modalities%2520show%2520significant%2520vulnerability%2520under%2520adversarial%2520evaluation%252C%2520with%2520all%2520models%2520degrading%2520substantially%2520despite%2520strong%2520results%2520on%2520standard%2520benchmarks.%2520Text-to-image%2520models%2520achieve%2520relatively%2520stronger%2520alignment%2520in%2520regulated%2520visual%2520risk%2520categories%252C%2520yet%2520remain%2520brittle%2520under%2520adversarial%2520or%2520semantically%2520ambiguous%2520prompts.%2520Overall%252C%2520these%2520results%2520show%2520that%2520safety%2520in%2520frontier%2520models%2520is%2520inherently%2520multidimensional--shaped%2520by%2520modality%252C%2520language%252C%2520and%2520evaluation%2520scheme%252C%2520underscoring%2520the%2520need%2520for%2520standardized%2520safety%2520evaluations%2520to%2520accurately%2520assess%2520real-world%2520risk%2520and%2520guide%2520responsible%2520model%2520development%2520and%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Safety%20Report%20on%20GPT-5.2%2C%20Gemini%203%20Pro%2C%20Qwen3-VL%2C%20Doubao%201.8%2C%20Grok%204.1%20Fast%2C%20Nano%20Banana%20Pro%2C%20and%20Seedream%204.5&entry.906535625=Xingjun%20Ma%20and%20Yixu%20Wang%20and%20Hengyuan%20Xu%20and%20Yutao%20Wu%20and%20Yifan%20Ding%20and%20Yunhan%20Zhao%20and%20Zilong%20Wang%20and%20Jiabin%20Hua%20and%20Ming%20Wen%20and%20Jianan%20Liu%20and%20Ranjie%20Duan%20and%20Yifeng%20Gao%20and%20Yingshui%20Tan%20and%20Yunhao%20Chen%20and%20Hui%20Xue%20and%20Xin%20Wang%20and%20Wei%20Cheng%20and%20Jingjing%20Chen%20and%20Zuxuan%20Wu%20and%20Bo%20Li%20and%20Yu-Gang%20Jiang&entry.1292438233=The%20rapid%20evolution%20of%20Large%20Language%20Models%20%28LLMs%29%20and%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20has%20produced%20substantial%20gains%20in%20reasoning%2C%20perception%2C%20and%20generative%20capability%20across%20language%20and%20vision.%20However%2C%20whether%20these%20advances%20yield%20commensurate%20improvements%20in%20safety%20remains%20unclear%2C%20in%20part%20due%20to%20fragmented%20evaluation%20practices%20limited%20to%20single%20modalities%20or%20threat%20models.%20In%20this%20report%2C%20we%20present%20an%20integrated%20safety%20evaluation%20of%207%20frontier%20models%3A%20GPT-5.2%2C%20Gemini%203%20Pro%2C%20Qwen3-VL%2C%20Doubao%201.8%2C%20Grok%204.1%20Fast%2C%20Nano%20Banana%20Pro%2C%20and%20Seedream%204.5.%20We%20evaluate%20each%20model%20across%20language%2C%20vision-language%2C%20and%20image%20generation%20settings%20using%20a%20unified%20protocol%20that%20integrates%20benchmark%20evaluation%2C%20adversarial%20evaluation%2C%20multilingual%20evaluation%2C%20and%20compliance%20evaluation.%20Aggregating%20our%20evaluations%20into%20safety%20leaderboards%20and%20model%20safety%20profiles%20across%20multiple%20evaluation%20modes%20reveals%20a%20sharply%20heterogeneous%20safety%20landscape.%20While%20GPT-5.2%20demonstrates%20consistently%20strong%20and%20balanced%20safety%20performance%20across%20evaluations%2C%20other%20models%20exhibit%20pronounced%20trade-offs%20among%20benchmark%20safety%2C%20adversarial%20alignment%2C%20multilingual%20generalization%2C%20and%20regulatory%20compliance.%20Both%20language%20and%20vision-language%20modalities%20show%20significant%20vulnerability%20under%20adversarial%20evaluation%2C%20with%20all%20models%20degrading%20substantially%20despite%20strong%20results%20on%20standard%20benchmarks.%20Text-to-image%20models%20achieve%20relatively%20stronger%20alignment%20in%20regulated%20visual%20risk%20categories%2C%20yet%20remain%20brittle%20under%20adversarial%20or%20semantically%20ambiguous%20prompts.%20Overall%2C%20these%20results%20show%20that%20safety%20in%20frontier%20models%20is%20inherently%20multidimensional--shaped%20by%20modality%2C%20language%2C%20and%20evaluation%20scheme%2C%20underscoring%20the%20need%20for%20standardized%20safety%20evaluations%20to%20accurately%20assess%20real-world%20risk%20and%20guide%20responsible%20model%20development%20and%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2601.10527v1&entry.124074799=Read"},
{"title": "Lil: Less is Less When Applying Post-Training Sparse-Attention Algorithms in Long-Decode Stage", "author": "Junhao Hu and Fangze Li and Mingtao Xu and Feifan Meng and Shiju Zhao and Tiancheng Hu and Ting Peng and Anmin Liu and Wenrui Huang and Chenxu Liu and Ziyue Hua and Tao Xie", "abstract": "Large language models (LLMs) demonstrate strong capabilities across a wide range of complex tasks and are increasingly deployed at scale, placing significant demands on inference efficiency. Prior work typically decomposes inference into prefill and decode stages, with the decode stage dominating total latency. To reduce time and memory complexity in the decode stage, a line of work introduces sparse-attention algorithms. In this paper, we show, both empirically and theoretically, that sparse attention can paradoxically increase end-to-end complexity: information loss often induces significantly longer sequences, a phenomenon we term ``Less is Less'' (Lil). To mitigate the Lil problem, we propose an early-stopping algorithm that detects the threshold where information loss exceeds information gain during sparse decoding. Our early-stopping algorithm reduces token consumption by up to 90% with a marginal accuracy degradation of less than 2% across reasoning-intensive benchmarks.", "link": "http://arxiv.org/abs/2601.03043v2", "date": "2026-01-15", "relevancy": 2.1451, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5735}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5253}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5034}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lil%3A%20Less%20is%20Less%20When%20Applying%20Post-Training%20Sparse-Attention%20Algorithms%20in%20Long-Decode%20Stage&body=Title%3A%20Lil%3A%20Less%20is%20Less%20When%20Applying%20Post-Training%20Sparse-Attention%20Algorithms%20in%20Long-Decode%20Stage%0AAuthor%3A%20Junhao%20Hu%20and%20Fangze%20Li%20and%20Mingtao%20Xu%20and%20Feifan%20Meng%20and%20Shiju%20Zhao%20and%20Tiancheng%20Hu%20and%20Ting%20Peng%20and%20Anmin%20Liu%20and%20Wenrui%20Huang%20and%20Chenxu%20Liu%20and%20Ziyue%20Hua%20and%20Tao%20Xie%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20demonstrate%20strong%20capabilities%20across%20a%20wide%20range%20of%20complex%20tasks%20and%20are%20increasingly%20deployed%20at%20scale%2C%20placing%20significant%20demands%20on%20inference%20efficiency.%20Prior%20work%20typically%20decomposes%20inference%20into%20prefill%20and%20decode%20stages%2C%20with%20the%20decode%20stage%20dominating%20total%20latency.%20To%20reduce%20time%20and%20memory%20complexity%20in%20the%20decode%20stage%2C%20a%20line%20of%20work%20introduces%20sparse-attention%20algorithms.%20In%20this%20paper%2C%20we%20show%2C%20both%20empirically%20and%20theoretically%2C%20that%20sparse%20attention%20can%20paradoxically%20increase%20end-to-end%20complexity%3A%20information%20loss%20often%20induces%20significantly%20longer%20sequences%2C%20a%20phenomenon%20we%20term%20%60%60Less%20is%20Less%27%27%20%28Lil%29.%20To%20mitigate%20the%20Lil%20problem%2C%20we%20propose%20an%20early-stopping%20algorithm%20that%20detects%20the%20threshold%20where%20information%20loss%20exceeds%20information%20gain%20during%20sparse%20decoding.%20Our%20early-stopping%20algorithm%20reduces%20token%20consumption%20by%20up%20to%2090%25%20with%20a%20marginal%20accuracy%20degradation%20of%20less%20than%202%25%20across%20reasoning-intensive%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2601.03043v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLil%253A%2520Less%2520is%2520Less%2520When%2520Applying%2520Post-Training%2520Sparse-Attention%2520Algorithms%2520in%2520Long-Decode%2520Stage%26entry.906535625%3DJunhao%2520Hu%2520and%2520Fangze%2520Li%2520and%2520Mingtao%2520Xu%2520and%2520Feifan%2520Meng%2520and%2520Shiju%2520Zhao%2520and%2520Tiancheng%2520Hu%2520and%2520Ting%2520Peng%2520and%2520Anmin%2520Liu%2520and%2520Wenrui%2520Huang%2520and%2520Chenxu%2520Liu%2520and%2520Ziyue%2520Hua%2520and%2520Tao%2520Xie%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520demonstrate%2520strong%2520capabilities%2520across%2520a%2520wide%2520range%2520of%2520complex%2520tasks%2520and%2520are%2520increasingly%2520deployed%2520at%2520scale%252C%2520placing%2520significant%2520demands%2520on%2520inference%2520efficiency.%2520Prior%2520work%2520typically%2520decomposes%2520inference%2520into%2520prefill%2520and%2520decode%2520stages%252C%2520with%2520the%2520decode%2520stage%2520dominating%2520total%2520latency.%2520To%2520reduce%2520time%2520and%2520memory%2520complexity%2520in%2520the%2520decode%2520stage%252C%2520a%2520line%2520of%2520work%2520introduces%2520sparse-attention%2520algorithms.%2520In%2520this%2520paper%252C%2520we%2520show%252C%2520both%2520empirically%2520and%2520theoretically%252C%2520that%2520sparse%2520attention%2520can%2520paradoxically%2520increase%2520end-to-end%2520complexity%253A%2520information%2520loss%2520often%2520induces%2520significantly%2520longer%2520sequences%252C%2520a%2520phenomenon%2520we%2520term%2520%2560%2560Less%2520is%2520Less%2527%2527%2520%2528Lil%2529.%2520To%2520mitigate%2520the%2520Lil%2520problem%252C%2520we%2520propose%2520an%2520early-stopping%2520algorithm%2520that%2520detects%2520the%2520threshold%2520where%2520information%2520loss%2520exceeds%2520information%2520gain%2520during%2520sparse%2520decoding.%2520Our%2520early-stopping%2520algorithm%2520reduces%2520token%2520consumption%2520by%2520up%2520to%252090%2525%2520with%2520a%2520marginal%2520accuracy%2520degradation%2520of%2520less%2520than%25202%2525%2520across%2520reasoning-intensive%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.03043v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lil%3A%20Less%20is%20Less%20When%20Applying%20Post-Training%20Sparse-Attention%20Algorithms%20in%20Long-Decode%20Stage&entry.906535625=Junhao%20Hu%20and%20Fangze%20Li%20and%20Mingtao%20Xu%20and%20Feifan%20Meng%20and%20Shiju%20Zhao%20and%20Tiancheng%20Hu%20and%20Ting%20Peng%20and%20Anmin%20Liu%20and%20Wenrui%20Huang%20and%20Chenxu%20Liu%20and%20Ziyue%20Hua%20and%20Tao%20Xie&entry.1292438233=Large%20language%20models%20%28LLMs%29%20demonstrate%20strong%20capabilities%20across%20a%20wide%20range%20of%20complex%20tasks%20and%20are%20increasingly%20deployed%20at%20scale%2C%20placing%20significant%20demands%20on%20inference%20efficiency.%20Prior%20work%20typically%20decomposes%20inference%20into%20prefill%20and%20decode%20stages%2C%20with%20the%20decode%20stage%20dominating%20total%20latency.%20To%20reduce%20time%20and%20memory%20complexity%20in%20the%20decode%20stage%2C%20a%20line%20of%20work%20introduces%20sparse-attention%20algorithms.%20In%20this%20paper%2C%20we%20show%2C%20both%20empirically%20and%20theoretically%2C%20that%20sparse%20attention%20can%20paradoxically%20increase%20end-to-end%20complexity%3A%20information%20loss%20often%20induces%20significantly%20longer%20sequences%2C%20a%20phenomenon%20we%20term%20%60%60Less%20is%20Less%27%27%20%28Lil%29.%20To%20mitigate%20the%20Lil%20problem%2C%20we%20propose%20an%20early-stopping%20algorithm%20that%20detects%20the%20threshold%20where%20information%20loss%20exceeds%20information%20gain%20during%20sparse%20decoding.%20Our%20early-stopping%20algorithm%20reduces%20token%20consumption%20by%20up%20to%2090%25%20with%20a%20marginal%20accuracy%20degradation%20of%20less%20than%202%25%20across%20reasoning-intensive%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2601.03043v2&entry.124074799=Read"},
{"title": "In-Context Source and Channel Coding", "author": "Ziqiong Wang and Tianqi Ren and Rongpeng Li and Zhifeng Zhao and Honggang Zhang", "abstract": "Separate Source-Channel Coding (SSCC) remains attractive for text transmission due to its modularity and compatibility with mature entropy coders and powerful channel codes. However, SSCC often suffers from a pronounced cliff effect in low Signal-to-Noise Ratio (SNR) regimes, where residual bit errors after channel decoding can catastrophically break lossless source decoding, especially for Arithmetic Coding (AC) driven by Large Language Models (LLMs). This paper proposes a receiver-side In-Context Decoding (ICD) framework that enhances SSCC robustness without modifying the transmitter. ICD leverages an Error Correction Code Transformer (ECCT) to obtain bit-wise reliability for the decoded information bits. Based on the context-consistent bitstream, ICD constructs a confidence-ranked candidate pool via reliability-guided bit flipping, samples a compact yet diverse subset of candidates, and applies an LLM-based arithmetic decoder to obtain both reconstructions and sequence-level log-likelihoods. A reliability-likelihood fusion rule then selects the final output. We further provide theoretical guarantees on the stability and convergence of the proposed sampling procedure. Extensive experiments over Additive White Gaussian Noise (AWGN) and Rayleigh fading channels demonstrate consistent gains compared with conventional SSCC baselines and representative Joint Source-Channel Coding (JSCC) schemes.", "link": "http://arxiv.org/abs/2601.10267v1", "date": "2026-01-15", "relevancy": 2.1378, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4186}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4186}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20In-Context%20Source%20and%20Channel%20Coding&body=Title%3A%20In-Context%20Source%20and%20Channel%20Coding%0AAuthor%3A%20Ziqiong%20Wang%20and%20Tianqi%20Ren%20and%20Rongpeng%20Li%20and%20Zhifeng%20Zhao%20and%20Honggang%20Zhang%0AAbstract%3A%20Separate%20Source-Channel%20Coding%20%28SSCC%29%20remains%20attractive%20for%20text%20transmission%20due%20to%20its%20modularity%20and%20compatibility%20with%20mature%20entropy%20coders%20and%20powerful%20channel%20codes.%20However%2C%20SSCC%20often%20suffers%20from%20a%20pronounced%20cliff%20effect%20in%20low%20Signal-to-Noise%20Ratio%20%28SNR%29%20regimes%2C%20where%20residual%20bit%20errors%20after%20channel%20decoding%20can%20catastrophically%20break%20lossless%20source%20decoding%2C%20especially%20for%20Arithmetic%20Coding%20%28AC%29%20driven%20by%20Large%20Language%20Models%20%28LLMs%29.%20This%20paper%20proposes%20a%20receiver-side%20In-Context%20Decoding%20%28ICD%29%20framework%20that%20enhances%20SSCC%20robustness%20without%20modifying%20the%20transmitter.%20ICD%20leverages%20an%20Error%20Correction%20Code%20Transformer%20%28ECCT%29%20to%20obtain%20bit-wise%20reliability%20for%20the%20decoded%20information%20bits.%20Based%20on%20the%20context-consistent%20bitstream%2C%20ICD%20constructs%20a%20confidence-ranked%20candidate%20pool%20via%20reliability-guided%20bit%20flipping%2C%20samples%20a%20compact%20yet%20diverse%20subset%20of%20candidates%2C%20and%20applies%20an%20LLM-based%20arithmetic%20decoder%20to%20obtain%20both%20reconstructions%20and%20sequence-level%20log-likelihoods.%20A%20reliability-likelihood%20fusion%20rule%20then%20selects%20the%20final%20output.%20We%20further%20provide%20theoretical%20guarantees%20on%20the%20stability%20and%20convergence%20of%20the%20proposed%20sampling%20procedure.%20Extensive%20experiments%20over%20Additive%20White%20Gaussian%20Noise%20%28AWGN%29%20and%20Rayleigh%20fading%20channels%20demonstrate%20consistent%20gains%20compared%20with%20conventional%20SSCC%20baselines%20and%20representative%20Joint%20Source-Channel%20Coding%20%28JSCC%29%20schemes.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10267v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIn-Context%2520Source%2520and%2520Channel%2520Coding%26entry.906535625%3DZiqiong%2520Wang%2520and%2520Tianqi%2520Ren%2520and%2520Rongpeng%2520Li%2520and%2520Zhifeng%2520Zhao%2520and%2520Honggang%2520Zhang%26entry.1292438233%3DSeparate%2520Source-Channel%2520Coding%2520%2528SSCC%2529%2520remains%2520attractive%2520for%2520text%2520transmission%2520due%2520to%2520its%2520modularity%2520and%2520compatibility%2520with%2520mature%2520entropy%2520coders%2520and%2520powerful%2520channel%2520codes.%2520However%252C%2520SSCC%2520often%2520suffers%2520from%2520a%2520pronounced%2520cliff%2520effect%2520in%2520low%2520Signal-to-Noise%2520Ratio%2520%2528SNR%2529%2520regimes%252C%2520where%2520residual%2520bit%2520errors%2520after%2520channel%2520decoding%2520can%2520catastrophically%2520break%2520lossless%2520source%2520decoding%252C%2520especially%2520for%2520Arithmetic%2520Coding%2520%2528AC%2529%2520driven%2520by%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520This%2520paper%2520proposes%2520a%2520receiver-side%2520In-Context%2520Decoding%2520%2528ICD%2529%2520framework%2520that%2520enhances%2520SSCC%2520robustness%2520without%2520modifying%2520the%2520transmitter.%2520ICD%2520leverages%2520an%2520Error%2520Correction%2520Code%2520Transformer%2520%2528ECCT%2529%2520to%2520obtain%2520bit-wise%2520reliability%2520for%2520the%2520decoded%2520information%2520bits.%2520Based%2520on%2520the%2520context-consistent%2520bitstream%252C%2520ICD%2520constructs%2520a%2520confidence-ranked%2520candidate%2520pool%2520via%2520reliability-guided%2520bit%2520flipping%252C%2520samples%2520a%2520compact%2520yet%2520diverse%2520subset%2520of%2520candidates%252C%2520and%2520applies%2520an%2520LLM-based%2520arithmetic%2520decoder%2520to%2520obtain%2520both%2520reconstructions%2520and%2520sequence-level%2520log-likelihoods.%2520A%2520reliability-likelihood%2520fusion%2520rule%2520then%2520selects%2520the%2520final%2520output.%2520We%2520further%2520provide%2520theoretical%2520guarantees%2520on%2520the%2520stability%2520and%2520convergence%2520of%2520the%2520proposed%2520sampling%2520procedure.%2520Extensive%2520experiments%2520over%2520Additive%2520White%2520Gaussian%2520Noise%2520%2528AWGN%2529%2520and%2520Rayleigh%2520fading%2520channels%2520demonstrate%2520consistent%2520gains%2520compared%2520with%2520conventional%2520SSCC%2520baselines%2520and%2520representative%2520Joint%2520Source-Channel%2520Coding%2520%2528JSCC%2529%2520schemes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10267v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=In-Context%20Source%20and%20Channel%20Coding&entry.906535625=Ziqiong%20Wang%20and%20Tianqi%20Ren%20and%20Rongpeng%20Li%20and%20Zhifeng%20Zhao%20and%20Honggang%20Zhang&entry.1292438233=Separate%20Source-Channel%20Coding%20%28SSCC%29%20remains%20attractive%20for%20text%20transmission%20due%20to%20its%20modularity%20and%20compatibility%20with%20mature%20entropy%20coders%20and%20powerful%20channel%20codes.%20However%2C%20SSCC%20often%20suffers%20from%20a%20pronounced%20cliff%20effect%20in%20low%20Signal-to-Noise%20Ratio%20%28SNR%29%20regimes%2C%20where%20residual%20bit%20errors%20after%20channel%20decoding%20can%20catastrophically%20break%20lossless%20source%20decoding%2C%20especially%20for%20Arithmetic%20Coding%20%28AC%29%20driven%20by%20Large%20Language%20Models%20%28LLMs%29.%20This%20paper%20proposes%20a%20receiver-side%20In-Context%20Decoding%20%28ICD%29%20framework%20that%20enhances%20SSCC%20robustness%20without%20modifying%20the%20transmitter.%20ICD%20leverages%20an%20Error%20Correction%20Code%20Transformer%20%28ECCT%29%20to%20obtain%20bit-wise%20reliability%20for%20the%20decoded%20information%20bits.%20Based%20on%20the%20context-consistent%20bitstream%2C%20ICD%20constructs%20a%20confidence-ranked%20candidate%20pool%20via%20reliability-guided%20bit%20flipping%2C%20samples%20a%20compact%20yet%20diverse%20subset%20of%20candidates%2C%20and%20applies%20an%20LLM-based%20arithmetic%20decoder%20to%20obtain%20both%20reconstructions%20and%20sequence-level%20log-likelihoods.%20A%20reliability-likelihood%20fusion%20rule%20then%20selects%20the%20final%20output.%20We%20further%20provide%20theoretical%20guarantees%20on%20the%20stability%20and%20convergence%20of%20the%20proposed%20sampling%20procedure.%20Extensive%20experiments%20over%20Additive%20White%20Gaussian%20Noise%20%28AWGN%29%20and%20Rayleigh%20fading%20channels%20demonstrate%20consistent%20gains%20compared%20with%20conventional%20SSCC%20baselines%20and%20representative%20Joint%20Source-Channel%20Coding%20%28JSCC%29%20schemes.&entry.1838667208=http%3A//arxiv.org/abs/2601.10267v1&entry.124074799=Read"},
{"title": "Tuning-Free Adaptive Style Incorporation for Structure-Consistent Text-Driven Style Transfer", "author": "Yanqi Ge and Jiaqi Liu and Qingnan Fan and Xi Jiang and Ye Huang and Shuai Qin and Hong Gu and Wen Li and Lixin Duan", "abstract": "In this work, we target the task of text-driven style transfer in the context of text-to-image (T2I) diffusion models. The main challenge is consistent structure preservation while enabling effective style transfer effects. The past approaches in this field directly concatenate the content and style prompts for a prompt-level style injection, leading to unavoidable structure distortions. In this work, we propose a novel solution to the text-driven style transfer task, namely, Adaptive Style Incorporation~(ASI), to achieve fine-grained feature-level style incorporation. It consists of the Siamese Cross-Attention~(SiCA) to decouple the single-track cross-attention to a dual-track structure to obtain separate content and style features, and the Adaptive Content-Style Blending (AdaBlending) module to couple the content and style information from a structure-consistent manner. Experimentally, our method exhibits much better performance in both structure preservation and stylized effects.", "link": "http://arxiv.org/abs/2404.06835v2", "date": "2026-01-15", "relevancy": 2.1308, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5644}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5298}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.523}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tuning-Free%20Adaptive%20Style%20Incorporation%20for%20Structure-Consistent%20Text-Driven%20Style%20Transfer&body=Title%3A%20Tuning-Free%20Adaptive%20Style%20Incorporation%20for%20Structure-Consistent%20Text-Driven%20Style%20Transfer%0AAuthor%3A%20Yanqi%20Ge%20and%20Jiaqi%20Liu%20and%20Qingnan%20Fan%20and%20Xi%20Jiang%20and%20Ye%20Huang%20and%20Shuai%20Qin%20and%20Hong%20Gu%20and%20Wen%20Li%20and%20Lixin%20Duan%0AAbstract%3A%20In%20this%20work%2C%20we%20target%20the%20task%20of%20text-driven%20style%20transfer%20in%20the%20context%20of%20text-to-image%20%28T2I%29%20diffusion%20models.%20The%20main%20challenge%20is%20consistent%20structure%20preservation%20while%20enabling%20effective%20style%20transfer%20effects.%20The%20past%20approaches%20in%20this%20field%20directly%20concatenate%20the%20content%20and%20style%20prompts%20for%20a%20prompt-level%20style%20injection%2C%20leading%20to%20unavoidable%20structure%20distortions.%20In%20this%20work%2C%20we%20propose%20a%20novel%20solution%20to%20the%20text-driven%20style%20transfer%20task%2C%20namely%2C%20Adaptive%20Style%20Incorporation~%28ASI%29%2C%20to%20achieve%20fine-grained%20feature-level%20style%20incorporation.%20It%20consists%20of%20the%20Siamese%20Cross-Attention~%28SiCA%29%20to%20decouple%20the%20single-track%20cross-attention%20to%20a%20dual-track%20structure%20to%20obtain%20separate%20content%20and%20style%20features%2C%20and%20the%20Adaptive%20Content-Style%20Blending%20%28AdaBlending%29%20module%20to%20couple%20the%20content%20and%20style%20information%20from%20a%20structure-consistent%20manner.%20Experimentally%2C%20our%20method%20exhibits%20much%20better%20performance%20in%20both%20structure%20preservation%20and%20stylized%20effects.%0ALink%3A%20http%3A//arxiv.org/abs/2404.06835v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTuning-Free%2520Adaptive%2520Style%2520Incorporation%2520for%2520Structure-Consistent%2520Text-Driven%2520Style%2520Transfer%26entry.906535625%3DYanqi%2520Ge%2520and%2520Jiaqi%2520Liu%2520and%2520Qingnan%2520Fan%2520and%2520Xi%2520Jiang%2520and%2520Ye%2520Huang%2520and%2520Shuai%2520Qin%2520and%2520Hong%2520Gu%2520and%2520Wen%2520Li%2520and%2520Lixin%2520Duan%26entry.1292438233%3DIn%2520this%2520work%252C%2520we%2520target%2520the%2520task%2520of%2520text-driven%2520style%2520transfer%2520in%2520the%2520context%2520of%2520text-to-image%2520%2528T2I%2529%2520diffusion%2520models.%2520The%2520main%2520challenge%2520is%2520consistent%2520structure%2520preservation%2520while%2520enabling%2520effective%2520style%2520transfer%2520effects.%2520The%2520past%2520approaches%2520in%2520this%2520field%2520directly%2520concatenate%2520the%2520content%2520and%2520style%2520prompts%2520for%2520a%2520prompt-level%2520style%2520injection%252C%2520leading%2520to%2520unavoidable%2520structure%2520distortions.%2520In%2520this%2520work%252C%2520we%2520propose%2520a%2520novel%2520solution%2520to%2520the%2520text-driven%2520style%2520transfer%2520task%252C%2520namely%252C%2520Adaptive%2520Style%2520Incorporation~%2528ASI%2529%252C%2520to%2520achieve%2520fine-grained%2520feature-level%2520style%2520incorporation.%2520It%2520consists%2520of%2520the%2520Siamese%2520Cross-Attention~%2528SiCA%2529%2520to%2520decouple%2520the%2520single-track%2520cross-attention%2520to%2520a%2520dual-track%2520structure%2520to%2520obtain%2520separate%2520content%2520and%2520style%2520features%252C%2520and%2520the%2520Adaptive%2520Content-Style%2520Blending%2520%2528AdaBlending%2529%2520module%2520to%2520couple%2520the%2520content%2520and%2520style%2520information%2520from%2520a%2520structure-consistent%2520manner.%2520Experimentally%252C%2520our%2520method%2520exhibits%2520much%2520better%2520performance%2520in%2520both%2520structure%2520preservation%2520and%2520stylized%2520effects.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.06835v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tuning-Free%20Adaptive%20Style%20Incorporation%20for%20Structure-Consistent%20Text-Driven%20Style%20Transfer&entry.906535625=Yanqi%20Ge%20and%20Jiaqi%20Liu%20and%20Qingnan%20Fan%20and%20Xi%20Jiang%20and%20Ye%20Huang%20and%20Shuai%20Qin%20and%20Hong%20Gu%20and%20Wen%20Li%20and%20Lixin%20Duan&entry.1292438233=In%20this%20work%2C%20we%20target%20the%20task%20of%20text-driven%20style%20transfer%20in%20the%20context%20of%20text-to-image%20%28T2I%29%20diffusion%20models.%20The%20main%20challenge%20is%20consistent%20structure%20preservation%20while%20enabling%20effective%20style%20transfer%20effects.%20The%20past%20approaches%20in%20this%20field%20directly%20concatenate%20the%20content%20and%20style%20prompts%20for%20a%20prompt-level%20style%20injection%2C%20leading%20to%20unavoidable%20structure%20distortions.%20In%20this%20work%2C%20we%20propose%20a%20novel%20solution%20to%20the%20text-driven%20style%20transfer%20task%2C%20namely%2C%20Adaptive%20Style%20Incorporation~%28ASI%29%2C%20to%20achieve%20fine-grained%20feature-level%20style%20incorporation.%20It%20consists%20of%20the%20Siamese%20Cross-Attention~%28SiCA%29%20to%20decouple%20the%20single-track%20cross-attention%20to%20a%20dual-track%20structure%20to%20obtain%20separate%20content%20and%20style%20features%2C%20and%20the%20Adaptive%20Content-Style%20Blending%20%28AdaBlending%29%20module%20to%20couple%20the%20content%20and%20style%20information%20from%20a%20structure-consistent%20manner.%20Experimentally%2C%20our%20method%20exhibits%20much%20better%20performance%20in%20both%20structure%20preservation%20and%20stylized%20effects.&entry.1838667208=http%3A//arxiv.org/abs/2404.06835v2&entry.124074799=Read"},
{"title": "Encoder-Only Image Registration", "author": "Xiang Chen and Renjiu Hu and Jinwei Zhang and Yuxi Zhang and Xinyao Yu and Min Liu and Yaonan Wang and Hang Zhang", "abstract": "Learning-based techniques have significantly improved the accuracy and speed of deformable image registration. However, challenges such as reducing computational complexity and handling large deformations persist. To address these challenges, we analyze how convolutional neural networks (ConvNets) influence registration performance using the Horn-Schunck optical flow equation. Supported by prior studies and our empirical experiments, we observe that ConvNets play two key roles in registration: linearizing local intensities and harmonizing global contrast variations. Based on these insights, we propose the Encoder-Only Image Registration (EOIR) framework, designed to achieve a better accuracy-efficiency trade-off. EOIR separates feature learning from flow estimation, employing only a 3-layer ConvNet for feature extraction and a set of 3-layer flow estimators to construct a Laplacian feature pyramid, progressively composing diffeomorphic deformations under a large-deformation model. Results on five datasets across different modalities and anatomical regions demonstrate EOIR's effectiveness, achieving superior accuracy-efficiency and accuracy-smoothness trade-offs. With comparable accuracy, EOIR provides better efficiency and smoothness, and vice versa. The source code of EOIR is publicly available on https://github.com/XiangChen1994/EOIR.", "link": "http://arxiv.org/abs/2509.00451v3", "date": "2026-01-15", "relevancy": 2.1256, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5541}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5231}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Encoder-Only%20Image%20Registration&body=Title%3A%20Encoder-Only%20Image%20Registration%0AAuthor%3A%20Xiang%20Chen%20and%20Renjiu%20Hu%20and%20Jinwei%20Zhang%20and%20Yuxi%20Zhang%20and%20Xinyao%20Yu%20and%20Min%20Liu%20and%20Yaonan%20Wang%20and%20Hang%20Zhang%0AAbstract%3A%20Learning-based%20techniques%20have%20significantly%20improved%20the%20accuracy%20and%20speed%20of%20deformable%20image%20registration.%20However%2C%20challenges%20such%20as%20reducing%20computational%20complexity%20and%20handling%20large%20deformations%20persist.%20To%20address%20these%20challenges%2C%20we%20analyze%20how%20convolutional%20neural%20networks%20%28ConvNets%29%20influence%20registration%20performance%20using%20the%20Horn-Schunck%20optical%20flow%20equation.%20Supported%20by%20prior%20studies%20and%20our%20empirical%20experiments%2C%20we%20observe%20that%20ConvNets%20play%20two%20key%20roles%20in%20registration%3A%20linearizing%20local%20intensities%20and%20harmonizing%20global%20contrast%20variations.%20Based%20on%20these%20insights%2C%20we%20propose%20the%20Encoder-Only%20Image%20Registration%20%28EOIR%29%20framework%2C%20designed%20to%20achieve%20a%20better%20accuracy-efficiency%20trade-off.%20EOIR%20separates%20feature%20learning%20from%20flow%20estimation%2C%20employing%20only%20a%203-layer%20ConvNet%20for%20feature%20extraction%20and%20a%20set%20of%203-layer%20flow%20estimators%20to%20construct%20a%20Laplacian%20feature%20pyramid%2C%20progressively%20composing%20diffeomorphic%20deformations%20under%20a%20large-deformation%20model.%20Results%20on%20five%20datasets%20across%20different%20modalities%20and%20anatomical%20regions%20demonstrate%20EOIR%27s%20effectiveness%2C%20achieving%20superior%20accuracy-efficiency%20and%20accuracy-smoothness%20trade-offs.%20With%20comparable%20accuracy%2C%20EOIR%20provides%20better%20efficiency%20and%20smoothness%2C%20and%20vice%20versa.%20The%20source%20code%20of%20EOIR%20is%20publicly%20available%20on%20https%3A//github.com/XiangChen1994/EOIR.%0ALink%3A%20http%3A//arxiv.org/abs/2509.00451v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEncoder-Only%2520Image%2520Registration%26entry.906535625%3DXiang%2520Chen%2520and%2520Renjiu%2520Hu%2520and%2520Jinwei%2520Zhang%2520and%2520Yuxi%2520Zhang%2520and%2520Xinyao%2520Yu%2520and%2520Min%2520Liu%2520and%2520Yaonan%2520Wang%2520and%2520Hang%2520Zhang%26entry.1292438233%3DLearning-based%2520techniques%2520have%2520significantly%2520improved%2520the%2520accuracy%2520and%2520speed%2520of%2520deformable%2520image%2520registration.%2520However%252C%2520challenges%2520such%2520as%2520reducing%2520computational%2520complexity%2520and%2520handling%2520large%2520deformations%2520persist.%2520To%2520address%2520these%2520challenges%252C%2520we%2520analyze%2520how%2520convolutional%2520neural%2520networks%2520%2528ConvNets%2529%2520influence%2520registration%2520performance%2520using%2520the%2520Horn-Schunck%2520optical%2520flow%2520equation.%2520Supported%2520by%2520prior%2520studies%2520and%2520our%2520empirical%2520experiments%252C%2520we%2520observe%2520that%2520ConvNets%2520play%2520two%2520key%2520roles%2520in%2520registration%253A%2520linearizing%2520local%2520intensities%2520and%2520harmonizing%2520global%2520contrast%2520variations.%2520Based%2520on%2520these%2520insights%252C%2520we%2520propose%2520the%2520Encoder-Only%2520Image%2520Registration%2520%2528EOIR%2529%2520framework%252C%2520designed%2520to%2520achieve%2520a%2520better%2520accuracy-efficiency%2520trade-off.%2520EOIR%2520separates%2520feature%2520learning%2520from%2520flow%2520estimation%252C%2520employing%2520only%2520a%25203-layer%2520ConvNet%2520for%2520feature%2520extraction%2520and%2520a%2520set%2520of%25203-layer%2520flow%2520estimators%2520to%2520construct%2520a%2520Laplacian%2520feature%2520pyramid%252C%2520progressively%2520composing%2520diffeomorphic%2520deformations%2520under%2520a%2520large-deformation%2520model.%2520Results%2520on%2520five%2520datasets%2520across%2520different%2520modalities%2520and%2520anatomical%2520regions%2520demonstrate%2520EOIR%2527s%2520effectiveness%252C%2520achieving%2520superior%2520accuracy-efficiency%2520and%2520accuracy-smoothness%2520trade-offs.%2520With%2520comparable%2520accuracy%252C%2520EOIR%2520provides%2520better%2520efficiency%2520and%2520smoothness%252C%2520and%2520vice%2520versa.%2520The%2520source%2520code%2520of%2520EOIR%2520is%2520publicly%2520available%2520on%2520https%253A//github.com/XiangChen1994/EOIR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.00451v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Encoder-Only%20Image%20Registration&entry.906535625=Xiang%20Chen%20and%20Renjiu%20Hu%20and%20Jinwei%20Zhang%20and%20Yuxi%20Zhang%20and%20Xinyao%20Yu%20and%20Min%20Liu%20and%20Yaonan%20Wang%20and%20Hang%20Zhang&entry.1292438233=Learning-based%20techniques%20have%20significantly%20improved%20the%20accuracy%20and%20speed%20of%20deformable%20image%20registration.%20However%2C%20challenges%20such%20as%20reducing%20computational%20complexity%20and%20handling%20large%20deformations%20persist.%20To%20address%20these%20challenges%2C%20we%20analyze%20how%20convolutional%20neural%20networks%20%28ConvNets%29%20influence%20registration%20performance%20using%20the%20Horn-Schunck%20optical%20flow%20equation.%20Supported%20by%20prior%20studies%20and%20our%20empirical%20experiments%2C%20we%20observe%20that%20ConvNets%20play%20two%20key%20roles%20in%20registration%3A%20linearizing%20local%20intensities%20and%20harmonizing%20global%20contrast%20variations.%20Based%20on%20these%20insights%2C%20we%20propose%20the%20Encoder-Only%20Image%20Registration%20%28EOIR%29%20framework%2C%20designed%20to%20achieve%20a%20better%20accuracy-efficiency%20trade-off.%20EOIR%20separates%20feature%20learning%20from%20flow%20estimation%2C%20employing%20only%20a%203-layer%20ConvNet%20for%20feature%20extraction%20and%20a%20set%20of%203-layer%20flow%20estimators%20to%20construct%20a%20Laplacian%20feature%20pyramid%2C%20progressively%20composing%20diffeomorphic%20deformations%20under%20a%20large-deformation%20model.%20Results%20on%20five%20datasets%20across%20different%20modalities%20and%20anatomical%20regions%20demonstrate%20EOIR%27s%20effectiveness%2C%20achieving%20superior%20accuracy-efficiency%20and%20accuracy-smoothness%20trade-offs.%20With%20comparable%20accuracy%2C%20EOIR%20provides%20better%20efficiency%20and%20smoothness%2C%20and%20vice%20versa.%20The%20source%20code%20of%20EOIR%20is%20publicly%20available%20on%20https%3A//github.com/XiangChen1994/EOIR.&entry.1838667208=http%3A//arxiv.org/abs/2509.00451v3&entry.124074799=Read"},
{"title": "Mamba Goes HoME: Hierarchical Soft Mixture-of-Experts for 3D Medical Image Segmentation", "author": "Szymon P\u0142otka and Gizem Mert and Maciej Chrabaszcz and Ewa Szczurek and Arkadiusz Sitek", "abstract": "In recent years, artificial intelligence has significantly advanced medical image segmentation. Nonetheless, challenges remain, including efficient 3D medical image processing across diverse modalities and handling data variability. In this work, we introduce Hierarchical Soft Mixture-of-Experts (HoME), a two-level token-routing layer for efficient long-context modeling, specifically designed for 3D medical image segmentation. Built on the Mamba Selective State Space Model (SSM) backbone, HoME enhances sequential modeling through adaptive expert routing. In the first level, a Soft Mixture-of-Experts (SMoE) layer partitions input sequences into local groups, routing tokens to specialized per-group experts for localized feature extraction. The second level aggregates these outputs through a global SMoE layer, enabling cross-group information fusion and global context refinement. This hierarchical design, combining local expert routing with global expert refinement, enhances generalizability and segmentation performance, surpassing state-of-the-art results across datasets from the three most widely used 3D medical imaging modalities and varying data qualities. The code is publicly available at https://github.com/gmum/MambaHoME.", "link": "http://arxiv.org/abs/2507.06363v3", "date": "2026-01-15", "relevancy": 2.1246, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5373}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5272}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5258}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mamba%20Goes%20HoME%3A%20Hierarchical%20Soft%20Mixture-of-Experts%20for%203D%20Medical%20Image%20Segmentation&body=Title%3A%20Mamba%20Goes%20HoME%3A%20Hierarchical%20Soft%20Mixture-of-Experts%20for%203D%20Medical%20Image%20Segmentation%0AAuthor%3A%20Szymon%20P%C5%82otka%20and%20Gizem%20Mert%20and%20Maciej%20Chrabaszcz%20and%20Ewa%20Szczurek%20and%20Arkadiusz%20Sitek%0AAbstract%3A%20In%20recent%20years%2C%20artificial%20intelligence%20has%20significantly%20advanced%20medical%20image%20segmentation.%20Nonetheless%2C%20challenges%20remain%2C%20including%20efficient%203D%20medical%20image%20processing%20across%20diverse%20modalities%20and%20handling%20data%20variability.%20In%20this%20work%2C%20we%20introduce%20Hierarchical%20Soft%20Mixture-of-Experts%20%28HoME%29%2C%20a%20two-level%20token-routing%20layer%20for%20efficient%20long-context%20modeling%2C%20specifically%20designed%20for%203D%20medical%20image%20segmentation.%20Built%20on%20the%20Mamba%20Selective%20State%20Space%20Model%20%28SSM%29%20backbone%2C%20HoME%20enhances%20sequential%20modeling%20through%20adaptive%20expert%20routing.%20In%20the%20first%20level%2C%20a%20Soft%20Mixture-of-Experts%20%28SMoE%29%20layer%20partitions%20input%20sequences%20into%20local%20groups%2C%20routing%20tokens%20to%20specialized%20per-group%20experts%20for%20localized%20feature%20extraction.%20The%20second%20level%20aggregates%20these%20outputs%20through%20a%20global%20SMoE%20layer%2C%20enabling%20cross-group%20information%20fusion%20and%20global%20context%20refinement.%20This%20hierarchical%20design%2C%20combining%20local%20expert%20routing%20with%20global%20expert%20refinement%2C%20enhances%20generalizability%20and%20segmentation%20performance%2C%20surpassing%20state-of-the-art%20results%20across%20datasets%20from%20the%20three%20most%20widely%20used%203D%20medical%20imaging%20modalities%20and%20varying%20data%20qualities.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/gmum/MambaHoME.%0ALink%3A%20http%3A//arxiv.org/abs/2507.06363v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMamba%2520Goes%2520HoME%253A%2520Hierarchical%2520Soft%2520Mixture-of-Experts%2520for%25203D%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DSzymon%2520P%25C5%2582otka%2520and%2520Gizem%2520Mert%2520and%2520Maciej%2520Chrabaszcz%2520and%2520Ewa%2520Szczurek%2520and%2520Arkadiusz%2520Sitek%26entry.1292438233%3DIn%2520recent%2520years%252C%2520artificial%2520intelligence%2520has%2520significantly%2520advanced%2520medical%2520image%2520segmentation.%2520Nonetheless%252C%2520challenges%2520remain%252C%2520including%2520efficient%25203D%2520medical%2520image%2520processing%2520across%2520diverse%2520modalities%2520and%2520handling%2520data%2520variability.%2520In%2520this%2520work%252C%2520we%2520introduce%2520Hierarchical%2520Soft%2520Mixture-of-Experts%2520%2528HoME%2529%252C%2520a%2520two-level%2520token-routing%2520layer%2520for%2520efficient%2520long-context%2520modeling%252C%2520specifically%2520designed%2520for%25203D%2520medical%2520image%2520segmentation.%2520Built%2520on%2520the%2520Mamba%2520Selective%2520State%2520Space%2520Model%2520%2528SSM%2529%2520backbone%252C%2520HoME%2520enhances%2520sequential%2520modeling%2520through%2520adaptive%2520expert%2520routing.%2520In%2520the%2520first%2520level%252C%2520a%2520Soft%2520Mixture-of-Experts%2520%2528SMoE%2529%2520layer%2520partitions%2520input%2520sequences%2520into%2520local%2520groups%252C%2520routing%2520tokens%2520to%2520specialized%2520per-group%2520experts%2520for%2520localized%2520feature%2520extraction.%2520The%2520second%2520level%2520aggregates%2520these%2520outputs%2520through%2520a%2520global%2520SMoE%2520layer%252C%2520enabling%2520cross-group%2520information%2520fusion%2520and%2520global%2520context%2520refinement.%2520This%2520hierarchical%2520design%252C%2520combining%2520local%2520expert%2520routing%2520with%2520global%2520expert%2520refinement%252C%2520enhances%2520generalizability%2520and%2520segmentation%2520performance%252C%2520surpassing%2520state-of-the-art%2520results%2520across%2520datasets%2520from%2520the%2520three%2520most%2520widely%2520used%25203D%2520medical%2520imaging%2520modalities%2520and%2520varying%2520data%2520qualities.%2520The%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/gmum/MambaHoME.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.06363v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mamba%20Goes%20HoME%3A%20Hierarchical%20Soft%20Mixture-of-Experts%20for%203D%20Medical%20Image%20Segmentation&entry.906535625=Szymon%20P%C5%82otka%20and%20Gizem%20Mert%20and%20Maciej%20Chrabaszcz%20and%20Ewa%20Szczurek%20and%20Arkadiusz%20Sitek&entry.1292438233=In%20recent%20years%2C%20artificial%20intelligence%20has%20significantly%20advanced%20medical%20image%20segmentation.%20Nonetheless%2C%20challenges%20remain%2C%20including%20efficient%203D%20medical%20image%20processing%20across%20diverse%20modalities%20and%20handling%20data%20variability.%20In%20this%20work%2C%20we%20introduce%20Hierarchical%20Soft%20Mixture-of-Experts%20%28HoME%29%2C%20a%20two-level%20token-routing%20layer%20for%20efficient%20long-context%20modeling%2C%20specifically%20designed%20for%203D%20medical%20image%20segmentation.%20Built%20on%20the%20Mamba%20Selective%20State%20Space%20Model%20%28SSM%29%20backbone%2C%20HoME%20enhances%20sequential%20modeling%20through%20adaptive%20expert%20routing.%20In%20the%20first%20level%2C%20a%20Soft%20Mixture-of-Experts%20%28SMoE%29%20layer%20partitions%20input%20sequences%20into%20local%20groups%2C%20routing%20tokens%20to%20specialized%20per-group%20experts%20for%20localized%20feature%20extraction.%20The%20second%20level%20aggregates%20these%20outputs%20through%20a%20global%20SMoE%20layer%2C%20enabling%20cross-group%20information%20fusion%20and%20global%20context%20refinement.%20This%20hierarchical%20design%2C%20combining%20local%20expert%20routing%20with%20global%20expert%20refinement%2C%20enhances%20generalizability%20and%20segmentation%20performance%2C%20surpassing%20state-of-the-art%20results%20across%20datasets%20from%20the%20three%20most%20widely%20used%203D%20medical%20imaging%20modalities%20and%20varying%20data%20qualities.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/gmum/MambaHoME.&entry.1838667208=http%3A//arxiv.org/abs/2507.06363v3&entry.124074799=Read"},
{"title": "VICON: Vision In-Context Operator Networks for Multi-Physics Fluid Dynamics Prediction", "author": "Yadi Cao and Yuxuan Liu and Liu Yang and Rose Yu and Hayden Schaeffer and Stanley Osher", "abstract": "In-Context Operator Networks (ICONs) have demonstrated the ability to learn operators across diverse partial differential equations using few-shot, in-context learning. However, existing ICONs process each spatial point as an individual token, severely limiting computational efficiency when handling dense data in higher spatial dimensions. We propose Vision In-Context Operator Networks (VICON), which integrates vision transformer architectures to efficiently process 2D data through patch-wise operations while preserving ICON's adaptability to multiphysics systems and varying timesteps. Evaluated across three fluid dynamics benchmarks, VICON significantly outperforms state-of-the-art baselines: DPOT and MPP, reducing the averaged last-step rollout error by 37.9% compared to DPOT and 44.7% compared to MPP, while requiring only 72.5% and 34.8% of their respective inference times. VICON naturally supports flexible rollout strategies with varying timestep strides, enabling immediate deployment in imperfect measurement systems where sampling frequencies may differ or frames might be dropped - common challenges in real-world settings - without requiring retraining or interpolation. In these realistic scenarios, VICON exhibits remarkable robustness, experiencing only 24.41% relative performance degradation compared to 71.37%-74.49% degradation in baseline methods, demonstrating its versatility for deploying in realistic applications. Our scripts for processing datasets and code are publicly available at https://github.com/Eydcao/VICON.", "link": "http://arxiv.org/abs/2411.16063v5", "date": "2026-01-15", "relevancy": 2.1208, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5316}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5316}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5233}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VICON%3A%20Vision%20In-Context%20Operator%20Networks%20for%20Multi-Physics%20Fluid%20Dynamics%20Prediction&body=Title%3A%20VICON%3A%20Vision%20In-Context%20Operator%20Networks%20for%20Multi-Physics%20Fluid%20Dynamics%20Prediction%0AAuthor%3A%20Yadi%20Cao%20and%20Yuxuan%20Liu%20and%20Liu%20Yang%20and%20Rose%20Yu%20and%20Hayden%20Schaeffer%20and%20Stanley%20Osher%0AAbstract%3A%20In-Context%20Operator%20Networks%20%28ICONs%29%20have%20demonstrated%20the%20ability%20to%20learn%20operators%20across%20diverse%20partial%20differential%20equations%20using%20few-shot%2C%20in-context%20learning.%20However%2C%20existing%20ICONs%20process%20each%20spatial%20point%20as%20an%20individual%20token%2C%20severely%20limiting%20computational%20efficiency%20when%20handling%20dense%20data%20in%20higher%20spatial%20dimensions.%20We%20propose%20Vision%20In-Context%20Operator%20Networks%20%28VICON%29%2C%20which%20integrates%20vision%20transformer%20architectures%20to%20efficiently%20process%202D%20data%20through%20patch-wise%20operations%20while%20preserving%20ICON%27s%20adaptability%20to%20multiphysics%20systems%20and%20varying%20timesteps.%20Evaluated%20across%20three%20fluid%20dynamics%20benchmarks%2C%20VICON%20significantly%20outperforms%20state-of-the-art%20baselines%3A%20DPOT%20and%20MPP%2C%20reducing%20the%20averaged%20last-step%20rollout%20error%20by%2037.9%25%20compared%20to%20DPOT%20and%2044.7%25%20compared%20to%20MPP%2C%20while%20requiring%20only%2072.5%25%20and%2034.8%25%20of%20their%20respective%20inference%20times.%20VICON%20naturally%20supports%20flexible%20rollout%20strategies%20with%20varying%20timestep%20strides%2C%20enabling%20immediate%20deployment%20in%20imperfect%20measurement%20systems%20where%20sampling%20frequencies%20may%20differ%20or%20frames%20might%20be%20dropped%20-%20common%20challenges%20in%20real-world%20settings%20-%20without%20requiring%20retraining%20or%20interpolation.%20In%20these%20realistic%20scenarios%2C%20VICON%20exhibits%20remarkable%20robustness%2C%20experiencing%20only%2024.41%25%20relative%20performance%20degradation%20compared%20to%2071.37%25-74.49%25%20degradation%20in%20baseline%20methods%2C%20demonstrating%20its%20versatility%20for%20deploying%20in%20realistic%20applications.%20Our%20scripts%20for%20processing%20datasets%20and%20code%20are%20publicly%20available%20at%20https%3A//github.com/Eydcao/VICON.%0ALink%3A%20http%3A//arxiv.org/abs/2411.16063v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVICON%253A%2520Vision%2520In-Context%2520Operator%2520Networks%2520for%2520Multi-Physics%2520Fluid%2520Dynamics%2520Prediction%26entry.906535625%3DYadi%2520Cao%2520and%2520Yuxuan%2520Liu%2520and%2520Liu%2520Yang%2520and%2520Rose%2520Yu%2520and%2520Hayden%2520Schaeffer%2520and%2520Stanley%2520Osher%26entry.1292438233%3DIn-Context%2520Operator%2520Networks%2520%2528ICONs%2529%2520have%2520demonstrated%2520the%2520ability%2520to%2520learn%2520operators%2520across%2520diverse%2520partial%2520differential%2520equations%2520using%2520few-shot%252C%2520in-context%2520learning.%2520However%252C%2520existing%2520ICONs%2520process%2520each%2520spatial%2520point%2520as%2520an%2520individual%2520token%252C%2520severely%2520limiting%2520computational%2520efficiency%2520when%2520handling%2520dense%2520data%2520in%2520higher%2520spatial%2520dimensions.%2520We%2520propose%2520Vision%2520In-Context%2520Operator%2520Networks%2520%2528VICON%2529%252C%2520which%2520integrates%2520vision%2520transformer%2520architectures%2520to%2520efficiently%2520process%25202D%2520data%2520through%2520patch-wise%2520operations%2520while%2520preserving%2520ICON%2527s%2520adaptability%2520to%2520multiphysics%2520systems%2520and%2520varying%2520timesteps.%2520Evaluated%2520across%2520three%2520fluid%2520dynamics%2520benchmarks%252C%2520VICON%2520significantly%2520outperforms%2520state-of-the-art%2520baselines%253A%2520DPOT%2520and%2520MPP%252C%2520reducing%2520the%2520averaged%2520last-step%2520rollout%2520error%2520by%252037.9%2525%2520compared%2520to%2520DPOT%2520and%252044.7%2525%2520compared%2520to%2520MPP%252C%2520while%2520requiring%2520only%252072.5%2525%2520and%252034.8%2525%2520of%2520their%2520respective%2520inference%2520times.%2520VICON%2520naturally%2520supports%2520flexible%2520rollout%2520strategies%2520with%2520varying%2520timestep%2520strides%252C%2520enabling%2520immediate%2520deployment%2520in%2520imperfect%2520measurement%2520systems%2520where%2520sampling%2520frequencies%2520may%2520differ%2520or%2520frames%2520might%2520be%2520dropped%2520-%2520common%2520challenges%2520in%2520real-world%2520settings%2520-%2520without%2520requiring%2520retraining%2520or%2520interpolation.%2520In%2520these%2520realistic%2520scenarios%252C%2520VICON%2520exhibits%2520remarkable%2520robustness%252C%2520experiencing%2520only%252024.41%2525%2520relative%2520performance%2520degradation%2520compared%2520to%252071.37%2525-74.49%2525%2520degradation%2520in%2520baseline%2520methods%252C%2520demonstrating%2520its%2520versatility%2520for%2520deploying%2520in%2520realistic%2520applications.%2520Our%2520scripts%2520for%2520processing%2520datasets%2520and%2520code%2520are%2520publicly%2520available%2520at%2520https%253A//github.com/Eydcao/VICON.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2411.16063v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VICON%3A%20Vision%20In-Context%20Operator%20Networks%20for%20Multi-Physics%20Fluid%20Dynamics%20Prediction&entry.906535625=Yadi%20Cao%20and%20Yuxuan%20Liu%20and%20Liu%20Yang%20and%20Rose%20Yu%20and%20Hayden%20Schaeffer%20and%20Stanley%20Osher&entry.1292438233=In-Context%20Operator%20Networks%20%28ICONs%29%20have%20demonstrated%20the%20ability%20to%20learn%20operators%20across%20diverse%20partial%20differential%20equations%20using%20few-shot%2C%20in-context%20learning.%20However%2C%20existing%20ICONs%20process%20each%20spatial%20point%20as%20an%20individual%20token%2C%20severely%20limiting%20computational%20efficiency%20when%20handling%20dense%20data%20in%20higher%20spatial%20dimensions.%20We%20propose%20Vision%20In-Context%20Operator%20Networks%20%28VICON%29%2C%20which%20integrates%20vision%20transformer%20architectures%20to%20efficiently%20process%202D%20data%20through%20patch-wise%20operations%20while%20preserving%20ICON%27s%20adaptability%20to%20multiphysics%20systems%20and%20varying%20timesteps.%20Evaluated%20across%20three%20fluid%20dynamics%20benchmarks%2C%20VICON%20significantly%20outperforms%20state-of-the-art%20baselines%3A%20DPOT%20and%20MPP%2C%20reducing%20the%20averaged%20last-step%20rollout%20error%20by%2037.9%25%20compared%20to%20DPOT%20and%2044.7%25%20compared%20to%20MPP%2C%20while%20requiring%20only%2072.5%25%20and%2034.8%25%20of%20their%20respective%20inference%20times.%20VICON%20naturally%20supports%20flexible%20rollout%20strategies%20with%20varying%20timestep%20strides%2C%20enabling%20immediate%20deployment%20in%20imperfect%20measurement%20systems%20where%20sampling%20frequencies%20may%20differ%20or%20frames%20might%20be%20dropped%20-%20common%20challenges%20in%20real-world%20settings%20-%20without%20requiring%20retraining%20or%20interpolation.%20In%20these%20realistic%20scenarios%2C%20VICON%20exhibits%20remarkable%20robustness%2C%20experiencing%20only%2024.41%25%20relative%20performance%20degradation%20compared%20to%2071.37%25-74.49%25%20degradation%20in%20baseline%20methods%2C%20demonstrating%20its%20versatility%20for%20deploying%20in%20realistic%20applications.%20Our%20scripts%20for%20processing%20datasets%20and%20code%20are%20publicly%20available%20at%20https%3A//github.com/Eydcao/VICON.&entry.1838667208=http%3A//arxiv.org/abs/2411.16063v5&entry.124074799=Read"},
{"title": "Queueing-Aware Optimization of Reasoning Tokens for Accuracy-Latency Trade-offs in LLM Servers", "author": "Emre Ozbas and Melih Bastopcu", "abstract": "We consider a single large language model (LLM) server that serves a heterogeneous stream of queries belonging to $N$ distinct task types. Queries arrive according to a Poisson process, and each type occurs with a known prior probability. For each task type, the server allocates a fixed number of internal thinking tokens, which determines the computational effort devoted to that query. The token allocation induces an accuracy-latency trade-off: the service time follows an approximately affine function of the allocated tokens, while the probability of a correct response exhibits diminishing returns. Under a first-in, first-out (FIFO) service discipline, the system operates as an $M/G/1$ queue, and the mean system time depends on the first and second moments of the resulting service-time distribution. We formulate a constrained optimization problem that maximizes a weighted average accuracy objective penalized by the mean system time, subject to architectural token-budget constraints and queue-stability conditions. The objective function is shown to be strictly concave over the stability region, which ensures existence and uniqueness of the optimal token allocation. The first-order optimality conditions yield a coupled projected fixed-point characterization of the optimum, together with an iterative solution and an explicit sufficient condition for contraction. Moreover, a projected gradient method with a computable global step-size bound is developed to guarantee convergence beyond the contractive regime. Finally, integer-valued token allocations are attained via rounding of the continuous solution, and the resulting performance loss is evaluated in simulation results.", "link": "http://arxiv.org/abs/2601.10274v1", "date": "2026-01-15", "relevancy": 2.1127, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4581}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4053}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Queueing-Aware%20Optimization%20of%20Reasoning%20Tokens%20for%20Accuracy-Latency%20Trade-offs%20in%20LLM%20Servers&body=Title%3A%20Queueing-Aware%20Optimization%20of%20Reasoning%20Tokens%20for%20Accuracy-Latency%20Trade-offs%20in%20LLM%20Servers%0AAuthor%3A%20Emre%20Ozbas%20and%20Melih%20Bastopcu%0AAbstract%3A%20We%20consider%20a%20single%20large%20language%20model%20%28LLM%29%20server%20that%20serves%20a%20heterogeneous%20stream%20of%20queries%20belonging%20to%20%24N%24%20distinct%20task%20types.%20Queries%20arrive%20according%20to%20a%20Poisson%20process%2C%20and%20each%20type%20occurs%20with%20a%20known%20prior%20probability.%20For%20each%20task%20type%2C%20the%20server%20allocates%20a%20fixed%20number%20of%20internal%20thinking%20tokens%2C%20which%20determines%20the%20computational%20effort%20devoted%20to%20that%20query.%20The%20token%20allocation%20induces%20an%20accuracy-latency%20trade-off%3A%20the%20service%20time%20follows%20an%20approximately%20affine%20function%20of%20the%20allocated%20tokens%2C%20while%20the%20probability%20of%20a%20correct%20response%20exhibits%20diminishing%20returns.%20Under%20a%20first-in%2C%20first-out%20%28FIFO%29%20service%20discipline%2C%20the%20system%20operates%20as%20an%20%24M/G/1%24%20queue%2C%20and%20the%20mean%20system%20time%20depends%20on%20the%20first%20and%20second%20moments%20of%20the%20resulting%20service-time%20distribution.%20We%20formulate%20a%20constrained%20optimization%20problem%20that%20maximizes%20a%20weighted%20average%20accuracy%20objective%20penalized%20by%20the%20mean%20system%20time%2C%20subject%20to%20architectural%20token-budget%20constraints%20and%20queue-stability%20conditions.%20The%20objective%20function%20is%20shown%20to%20be%20strictly%20concave%20over%20the%20stability%20region%2C%20which%20ensures%20existence%20and%20uniqueness%20of%20the%20optimal%20token%20allocation.%20The%20first-order%20optimality%20conditions%20yield%20a%20coupled%20projected%20fixed-point%20characterization%20of%20the%20optimum%2C%20together%20with%20an%20iterative%20solution%20and%20an%20explicit%20sufficient%20condition%20for%20contraction.%20Moreover%2C%20a%20projected%20gradient%20method%20with%20a%20computable%20global%20step-size%20bound%20is%20developed%20to%20guarantee%20convergence%20beyond%20the%20contractive%20regime.%20Finally%2C%20integer-valued%20token%20allocations%20are%20attained%20via%20rounding%20of%20the%20continuous%20solution%2C%20and%20the%20resulting%20performance%20loss%20is%20evaluated%20in%20simulation%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10274v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQueueing-Aware%2520Optimization%2520of%2520Reasoning%2520Tokens%2520for%2520Accuracy-Latency%2520Trade-offs%2520in%2520LLM%2520Servers%26entry.906535625%3DEmre%2520Ozbas%2520and%2520Melih%2520Bastopcu%26entry.1292438233%3DWe%2520consider%2520a%2520single%2520large%2520language%2520model%2520%2528LLM%2529%2520server%2520that%2520serves%2520a%2520heterogeneous%2520stream%2520of%2520queries%2520belonging%2520to%2520%2524N%2524%2520distinct%2520task%2520types.%2520Queries%2520arrive%2520according%2520to%2520a%2520Poisson%2520process%252C%2520and%2520each%2520type%2520occurs%2520with%2520a%2520known%2520prior%2520probability.%2520For%2520each%2520task%2520type%252C%2520the%2520server%2520allocates%2520a%2520fixed%2520number%2520of%2520internal%2520thinking%2520tokens%252C%2520which%2520determines%2520the%2520computational%2520effort%2520devoted%2520to%2520that%2520query.%2520The%2520token%2520allocation%2520induces%2520an%2520accuracy-latency%2520trade-off%253A%2520the%2520service%2520time%2520follows%2520an%2520approximately%2520affine%2520function%2520of%2520the%2520allocated%2520tokens%252C%2520while%2520the%2520probability%2520of%2520a%2520correct%2520response%2520exhibits%2520diminishing%2520returns.%2520Under%2520a%2520first-in%252C%2520first-out%2520%2528FIFO%2529%2520service%2520discipline%252C%2520the%2520system%2520operates%2520as%2520an%2520%2524M/G/1%2524%2520queue%252C%2520and%2520the%2520mean%2520system%2520time%2520depends%2520on%2520the%2520first%2520and%2520second%2520moments%2520of%2520the%2520resulting%2520service-time%2520distribution.%2520We%2520formulate%2520a%2520constrained%2520optimization%2520problem%2520that%2520maximizes%2520a%2520weighted%2520average%2520accuracy%2520objective%2520penalized%2520by%2520the%2520mean%2520system%2520time%252C%2520subject%2520to%2520architectural%2520token-budget%2520constraints%2520and%2520queue-stability%2520conditions.%2520The%2520objective%2520function%2520is%2520shown%2520to%2520be%2520strictly%2520concave%2520over%2520the%2520stability%2520region%252C%2520which%2520ensures%2520existence%2520and%2520uniqueness%2520of%2520the%2520optimal%2520token%2520allocation.%2520The%2520first-order%2520optimality%2520conditions%2520yield%2520a%2520coupled%2520projected%2520fixed-point%2520characterization%2520of%2520the%2520optimum%252C%2520together%2520with%2520an%2520iterative%2520solution%2520and%2520an%2520explicit%2520sufficient%2520condition%2520for%2520contraction.%2520Moreover%252C%2520a%2520projected%2520gradient%2520method%2520with%2520a%2520computable%2520global%2520step-size%2520bound%2520is%2520developed%2520to%2520guarantee%2520convergence%2520beyond%2520the%2520contractive%2520regime.%2520Finally%252C%2520integer-valued%2520token%2520allocations%2520are%2520attained%2520via%2520rounding%2520of%2520the%2520continuous%2520solution%252C%2520and%2520the%2520resulting%2520performance%2520loss%2520is%2520evaluated%2520in%2520simulation%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10274v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Queueing-Aware%20Optimization%20of%20Reasoning%20Tokens%20for%20Accuracy-Latency%20Trade-offs%20in%20LLM%20Servers&entry.906535625=Emre%20Ozbas%20and%20Melih%20Bastopcu&entry.1292438233=We%20consider%20a%20single%20large%20language%20model%20%28LLM%29%20server%20that%20serves%20a%20heterogeneous%20stream%20of%20queries%20belonging%20to%20%24N%24%20distinct%20task%20types.%20Queries%20arrive%20according%20to%20a%20Poisson%20process%2C%20and%20each%20type%20occurs%20with%20a%20known%20prior%20probability.%20For%20each%20task%20type%2C%20the%20server%20allocates%20a%20fixed%20number%20of%20internal%20thinking%20tokens%2C%20which%20determines%20the%20computational%20effort%20devoted%20to%20that%20query.%20The%20token%20allocation%20induces%20an%20accuracy-latency%20trade-off%3A%20the%20service%20time%20follows%20an%20approximately%20affine%20function%20of%20the%20allocated%20tokens%2C%20while%20the%20probability%20of%20a%20correct%20response%20exhibits%20diminishing%20returns.%20Under%20a%20first-in%2C%20first-out%20%28FIFO%29%20service%20discipline%2C%20the%20system%20operates%20as%20an%20%24M/G/1%24%20queue%2C%20and%20the%20mean%20system%20time%20depends%20on%20the%20first%20and%20second%20moments%20of%20the%20resulting%20service-time%20distribution.%20We%20formulate%20a%20constrained%20optimization%20problem%20that%20maximizes%20a%20weighted%20average%20accuracy%20objective%20penalized%20by%20the%20mean%20system%20time%2C%20subject%20to%20architectural%20token-budget%20constraints%20and%20queue-stability%20conditions.%20The%20objective%20function%20is%20shown%20to%20be%20strictly%20concave%20over%20the%20stability%20region%2C%20which%20ensures%20existence%20and%20uniqueness%20of%20the%20optimal%20token%20allocation.%20The%20first-order%20optimality%20conditions%20yield%20a%20coupled%20projected%20fixed-point%20characterization%20of%20the%20optimum%2C%20together%20with%20an%20iterative%20solution%20and%20an%20explicit%20sufficient%20condition%20for%20contraction.%20Moreover%2C%20a%20projected%20gradient%20method%20with%20a%20computable%20global%20step-size%20bound%20is%20developed%20to%20guarantee%20convergence%20beyond%20the%20contractive%20regime.%20Finally%2C%20integer-valued%20token%20allocations%20are%20attained%20via%20rounding%20of%20the%20continuous%20solution%2C%20and%20the%20resulting%20performance%20loss%20is%20evaluated%20in%20simulation%20results.&entry.1838667208=http%3A//arxiv.org/abs/2601.10274v1&entry.124074799=Read"},
{"title": "BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition", "author": "Max A. Buettner and Kanak Mazumder and Luca Koecher and Mario Finkbeiner and Sebastian Niebler and Fabian B. Flohr", "abstract": "Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle's perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist's viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under https://iv.ee.hm.edu/bikeactions/.", "link": "http://arxiv.org/abs/2601.10521v1", "date": "2026-01-15", "relevancy": 2.1007, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5382}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.536}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BikeActions%3A%20An%20Open%20Platform%20and%20Benchmark%20for%20Cyclist-Centric%20VRU%20Action%20Recognition&body=Title%3A%20BikeActions%3A%20An%20Open%20Platform%20and%20Benchmark%20for%20Cyclist-Centric%20VRU%20Action%20Recognition%0AAuthor%3A%20Max%20A.%20Buettner%20and%20Kanak%20Mazumder%20and%20Luca%20Koecher%20and%20Mario%20Finkbeiner%20and%20Sebastian%20Niebler%20and%20Fabian%20B.%20Flohr%0AAbstract%3A%20Anticipating%20the%20intentions%20of%20Vulnerable%20Road%20Users%20%28VRUs%29%20is%20a%20critical%20challenge%20for%20safe%20autonomous%20driving%20%28AD%29%20and%20mobile%20robotics.%20While%20current%20research%20predominantly%20focuses%20on%20pedestrian%20crossing%20behaviors%20from%20a%20vehicle%27s%20perspective%2C%20interactions%20within%20dense%20shared%20spaces%20remain%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20introduce%20FUSE-Bike%2C%20the%20first%20fully%20open%20perception%20platform%20of%20its%20kind.%20Equipped%20with%20two%20LiDARs%2C%20a%20camera%2C%20and%20GNSS%2C%20it%20facilitates%20high-fidelity%2C%20close-range%20data%20capture%20directly%20from%20a%20cyclist%27s%20viewpoint.%20Leveraging%20this%20platform%2C%20we%20present%20BikeActions%2C%20a%20novel%20multi-modal%20dataset%20comprising%20852%20annotated%20samples%20across%205%20distinct%20action%20classes%2C%20specifically%20tailored%20to%20improve%20VRU%20behavior%20modeling.%20We%20establish%20a%20rigorous%20benchmark%20by%20evaluating%20state-of-the-art%20graph%20convolution%20and%20transformer-based%20models%20on%20our%20publicly%20released%20data%20splits%2C%20establishing%20the%20first%20performance%20baselines%20for%20this%20challenging%20task.%20We%20release%20the%20full%20dataset%20together%20with%20data%20curation%20tools%2C%20the%20open%20hardware%20design%2C%20and%20the%20benchmark%20code%20to%20foster%20future%20research%20in%20VRU%20action%20understanding%20under%20https%3A//iv.ee.hm.edu/bikeactions/.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10521v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBikeActions%253A%2520An%2520Open%2520Platform%2520and%2520Benchmark%2520for%2520Cyclist-Centric%2520VRU%2520Action%2520Recognition%26entry.906535625%3DMax%2520A.%2520Buettner%2520and%2520Kanak%2520Mazumder%2520and%2520Luca%2520Koecher%2520and%2520Mario%2520Finkbeiner%2520and%2520Sebastian%2520Niebler%2520and%2520Fabian%2520B.%2520Flohr%26entry.1292438233%3DAnticipating%2520the%2520intentions%2520of%2520Vulnerable%2520Road%2520Users%2520%2528VRUs%2529%2520is%2520a%2520critical%2520challenge%2520for%2520safe%2520autonomous%2520driving%2520%2528AD%2529%2520and%2520mobile%2520robotics.%2520While%2520current%2520research%2520predominantly%2520focuses%2520on%2520pedestrian%2520crossing%2520behaviors%2520from%2520a%2520vehicle%2527s%2520perspective%252C%2520interactions%2520within%2520dense%2520shared%2520spaces%2520remain%2520underexplored.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520FUSE-Bike%252C%2520the%2520first%2520fully%2520open%2520perception%2520platform%2520of%2520its%2520kind.%2520Equipped%2520with%2520two%2520LiDARs%252C%2520a%2520camera%252C%2520and%2520GNSS%252C%2520it%2520facilitates%2520high-fidelity%252C%2520close-range%2520data%2520capture%2520directly%2520from%2520a%2520cyclist%2527s%2520viewpoint.%2520Leveraging%2520this%2520platform%252C%2520we%2520present%2520BikeActions%252C%2520a%2520novel%2520multi-modal%2520dataset%2520comprising%2520852%2520annotated%2520samples%2520across%25205%2520distinct%2520action%2520classes%252C%2520specifically%2520tailored%2520to%2520improve%2520VRU%2520behavior%2520modeling.%2520We%2520establish%2520a%2520rigorous%2520benchmark%2520by%2520evaluating%2520state-of-the-art%2520graph%2520convolution%2520and%2520transformer-based%2520models%2520on%2520our%2520publicly%2520released%2520data%2520splits%252C%2520establishing%2520the%2520first%2520performance%2520baselines%2520for%2520this%2520challenging%2520task.%2520We%2520release%2520the%2520full%2520dataset%2520together%2520with%2520data%2520curation%2520tools%252C%2520the%2520open%2520hardware%2520design%252C%2520and%2520the%2520benchmark%2520code%2520to%2520foster%2520future%2520research%2520in%2520VRU%2520action%2520understanding%2520under%2520https%253A//iv.ee.hm.edu/bikeactions/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10521v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BikeActions%3A%20An%20Open%20Platform%20and%20Benchmark%20for%20Cyclist-Centric%20VRU%20Action%20Recognition&entry.906535625=Max%20A.%20Buettner%20and%20Kanak%20Mazumder%20and%20Luca%20Koecher%20and%20Mario%20Finkbeiner%20and%20Sebastian%20Niebler%20and%20Fabian%20B.%20Flohr&entry.1292438233=Anticipating%20the%20intentions%20of%20Vulnerable%20Road%20Users%20%28VRUs%29%20is%20a%20critical%20challenge%20for%20safe%20autonomous%20driving%20%28AD%29%20and%20mobile%20robotics.%20While%20current%20research%20predominantly%20focuses%20on%20pedestrian%20crossing%20behaviors%20from%20a%20vehicle%27s%20perspective%2C%20interactions%20within%20dense%20shared%20spaces%20remain%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20introduce%20FUSE-Bike%2C%20the%20first%20fully%20open%20perception%20platform%20of%20its%20kind.%20Equipped%20with%20two%20LiDARs%2C%20a%20camera%2C%20and%20GNSS%2C%20it%20facilitates%20high-fidelity%2C%20close-range%20data%20capture%20directly%20from%20a%20cyclist%27s%20viewpoint.%20Leveraging%20this%20platform%2C%20we%20present%20BikeActions%2C%20a%20novel%20multi-modal%20dataset%20comprising%20852%20annotated%20samples%20across%205%20distinct%20action%20classes%2C%20specifically%20tailored%20to%20improve%20VRU%20behavior%20modeling.%20We%20establish%20a%20rigorous%20benchmark%20by%20evaluating%20state-of-the-art%20graph%20convolution%20and%20transformer-based%20models%20on%20our%20publicly%20released%20data%20splits%2C%20establishing%20the%20first%20performance%20baselines%20for%20this%20challenging%20task.%20We%20release%20the%20full%20dataset%20together%20with%20data%20curation%20tools%2C%20the%20open%20hardware%20design%2C%20and%20the%20benchmark%20code%20to%20foster%20future%20research%20in%20VRU%20action%20understanding%20under%20https%3A//iv.ee.hm.edu/bikeactions/.&entry.1838667208=http%3A//arxiv.org/abs/2601.10521v1&entry.124074799=Read"},
{"title": "Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning", "author": "Zhiyuan Hu and Yunhai Hu and Juncheng Liu and Shuyue Stella Li and Yucheng Wang and Zhen Xu and See-Kiong Ng and Anh Tuan Luu and Xinxing Xu and Bryan Hooi and Cynthia Breazeal and Hae Won Park", "abstract": "Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \\textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\\% over a multi-agent baseline, and by 8.67\\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.", "link": "http://arxiv.org/abs/2601.09667v2", "date": "2026-01-15", "relevancy": 2.0992, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5306}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5241}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.512}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Multi-Agent%20Test-Time%20Reinforcement%20Learning%20for%20Reasoning&body=Title%3A%20Collaborative%20Multi-Agent%20Test-Time%20Reinforcement%20Learning%20for%20Reasoning%0AAuthor%3A%20Zhiyuan%20Hu%20and%20Yunhai%20Hu%20and%20Juncheng%20Liu%20and%20Shuyue%20Stella%20Li%20and%20Yucheng%20Wang%20and%20Zhen%20Xu%20and%20See-Kiong%20Ng%20and%20Anh%20Tuan%20Luu%20and%20Xinxing%20Xu%20and%20Bryan%20Hooi%20and%20Cynthia%20Breazeal%20and%20Hae%20Won%20Park%0AAbstract%3A%20Multi-agent%20systems%20have%20evolved%20into%20practical%20LLM-driven%20collaborators%20for%20many%20applications%2C%20gaining%20robustness%20from%20diversity%20and%20cross-checking.%20However%2C%20multi-agent%20RL%20%28MARL%29%20training%20is%20resource-intensive%20and%20unstable%3A%20co-adapting%20teammates%20induce%20non-stationarity%2C%20and%20rewards%20are%20often%20sparse%20and%20high-variance.%20Therefore%2C%20we%20introduce%20%5Ctextbf%7BMulti-Agent%20Test-Time%20Reinforcement%20Learning%20%28MATTRL%29%7D%2C%20a%20framework%20that%20injects%20structured%20textual%20experience%20into%20multi-agent%20deliberation%20at%20inference%20time.%20MATTRL%20forms%20a%20multi-expert%20team%20of%20specialists%20for%20multi-turn%20discussions%2C%20retrieves%20and%20integrates%20test-time%20experiences%2C%20and%20reaches%20consensus%20for%20final%20decision-making.%20We%20also%20study%20credit%20assignment%20for%20constructing%20a%20turn-level%20experience%20pool%2C%20then%20reinjecting%20it%20into%20the%20dialogue.%20Across%20challenging%20benchmarks%20in%20medicine%2C%20math%2C%20and%20education%2C%20MATTRL%20improves%20accuracy%20by%20an%20average%20of%203.67%5C%25%20over%20a%20multi-agent%20baseline%2C%20and%20by%208.67%5C%25%20over%20comparable%20single-agent%20baselines.%20Ablation%20studies%20examine%20different%20credit-assignment%20schemes%20and%20provide%20a%20detailed%20comparison%20of%20how%20they%20affect%20training%20outcomes.%20MATTRL%20offers%20a%20stable%2C%20effective%20and%20efficient%20path%20to%20distribution-shift-robust%20multi-agent%20reasoning%20without%20tuning.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09667v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCollaborative%2520Multi-Agent%2520Test-Time%2520Reinforcement%2520Learning%2520for%2520Reasoning%26entry.906535625%3DZhiyuan%2520Hu%2520and%2520Yunhai%2520Hu%2520and%2520Juncheng%2520Liu%2520and%2520Shuyue%2520Stella%2520Li%2520and%2520Yucheng%2520Wang%2520and%2520Zhen%2520Xu%2520and%2520See-Kiong%2520Ng%2520and%2520Anh%2520Tuan%2520Luu%2520and%2520Xinxing%2520Xu%2520and%2520Bryan%2520Hooi%2520and%2520Cynthia%2520Breazeal%2520and%2520Hae%2520Won%2520Park%26entry.1292438233%3DMulti-agent%2520systems%2520have%2520evolved%2520into%2520practical%2520LLM-driven%2520collaborators%2520for%2520many%2520applications%252C%2520gaining%2520robustness%2520from%2520diversity%2520and%2520cross-checking.%2520However%252C%2520multi-agent%2520RL%2520%2528MARL%2529%2520training%2520is%2520resource-intensive%2520and%2520unstable%253A%2520co-adapting%2520teammates%2520induce%2520non-stationarity%252C%2520and%2520rewards%2520are%2520often%2520sparse%2520and%2520high-variance.%2520Therefore%252C%2520we%2520introduce%2520%255Ctextbf%257BMulti-Agent%2520Test-Time%2520Reinforcement%2520Learning%2520%2528MATTRL%2529%257D%252C%2520a%2520framework%2520that%2520injects%2520structured%2520textual%2520experience%2520into%2520multi-agent%2520deliberation%2520at%2520inference%2520time.%2520MATTRL%2520forms%2520a%2520multi-expert%2520team%2520of%2520specialists%2520for%2520multi-turn%2520discussions%252C%2520retrieves%2520and%2520integrates%2520test-time%2520experiences%252C%2520and%2520reaches%2520consensus%2520for%2520final%2520decision-making.%2520We%2520also%2520study%2520credit%2520assignment%2520for%2520constructing%2520a%2520turn-level%2520experience%2520pool%252C%2520then%2520reinjecting%2520it%2520into%2520the%2520dialogue.%2520Across%2520challenging%2520benchmarks%2520in%2520medicine%252C%2520math%252C%2520and%2520education%252C%2520MATTRL%2520improves%2520accuracy%2520by%2520an%2520average%2520of%25203.67%255C%2525%2520over%2520a%2520multi-agent%2520baseline%252C%2520and%2520by%25208.67%255C%2525%2520over%2520comparable%2520single-agent%2520baselines.%2520Ablation%2520studies%2520examine%2520different%2520credit-assignment%2520schemes%2520and%2520provide%2520a%2520detailed%2520comparison%2520of%2520how%2520they%2520affect%2520training%2520outcomes.%2520MATTRL%2520offers%2520a%2520stable%252C%2520effective%2520and%2520efficient%2520path%2520to%2520distribution-shift-robust%2520multi-agent%2520reasoning%2520without%2520tuning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09667v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Multi-Agent%20Test-Time%20Reinforcement%20Learning%20for%20Reasoning&entry.906535625=Zhiyuan%20Hu%20and%20Yunhai%20Hu%20and%20Juncheng%20Liu%20and%20Shuyue%20Stella%20Li%20and%20Yucheng%20Wang%20and%20Zhen%20Xu%20and%20See-Kiong%20Ng%20and%20Anh%20Tuan%20Luu%20and%20Xinxing%20Xu%20and%20Bryan%20Hooi%20and%20Cynthia%20Breazeal%20and%20Hae%20Won%20Park&entry.1292438233=Multi-agent%20systems%20have%20evolved%20into%20practical%20LLM-driven%20collaborators%20for%20many%20applications%2C%20gaining%20robustness%20from%20diversity%20and%20cross-checking.%20However%2C%20multi-agent%20RL%20%28MARL%29%20training%20is%20resource-intensive%20and%20unstable%3A%20co-adapting%20teammates%20induce%20non-stationarity%2C%20and%20rewards%20are%20often%20sparse%20and%20high-variance.%20Therefore%2C%20we%20introduce%20%5Ctextbf%7BMulti-Agent%20Test-Time%20Reinforcement%20Learning%20%28MATTRL%29%7D%2C%20a%20framework%20that%20injects%20structured%20textual%20experience%20into%20multi-agent%20deliberation%20at%20inference%20time.%20MATTRL%20forms%20a%20multi-expert%20team%20of%20specialists%20for%20multi-turn%20discussions%2C%20retrieves%20and%20integrates%20test-time%20experiences%2C%20and%20reaches%20consensus%20for%20final%20decision-making.%20We%20also%20study%20credit%20assignment%20for%20constructing%20a%20turn-level%20experience%20pool%2C%20then%20reinjecting%20it%20into%20the%20dialogue.%20Across%20challenging%20benchmarks%20in%20medicine%2C%20math%2C%20and%20education%2C%20MATTRL%20improves%20accuracy%20by%20an%20average%20of%203.67%5C%25%20over%20a%20multi-agent%20baseline%2C%20and%20by%208.67%5C%25%20over%20comparable%20single-agent%20baselines.%20Ablation%20studies%20examine%20different%20credit-assignment%20schemes%20and%20provide%20a%20detailed%20comparison%20of%20how%20they%20affect%20training%20outcomes.%20MATTRL%20offers%20a%20stable%2C%20effective%20and%20efficient%20path%20to%20distribution-shift-robust%20multi-agent%20reasoning%20without%20tuning.&entry.1838667208=http%3A//arxiv.org/abs/2601.09667v2&entry.124074799=Read"},
{"title": "Loop as a Bridge: Can Looped Transformers Truly Link Representation Space and Natural Language Outputs?", "author": "Guanxu Chen and Dongrui Liu and Jing Shao", "abstract": "Large Language Models (LLMs) often exhibit a gap between their internal knowledge and their explicit linguistic outputs. In this report, we empirically investigate whether Looped Transformers (LTs)--architectures that increase computational depth by iterating shared layers--can bridge this gap by utilizing their iterative nature as a form of introspection. Our experiments reveal that while increasing loop iterations narrows the gap, it is partly driven by a degradation of their internal knowledge carried by representations. Moreover, another empirical analysis suggests that current LTs' ability to perceive representations does not improve across loops; it is only present in the final loop. These results suggest that while LTs offer a promising direction for scaling computational depth, they have yet to achieve the introspection required to truly link representation space and natural language.", "link": "http://arxiv.org/abs/2601.10242v1", "date": "2026-01-15", "relevancy": 2.0981, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5267}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5138}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Loop%20as%20a%20Bridge%3A%20Can%20Looped%20Transformers%20Truly%20Link%20Representation%20Space%20and%20Natural%20Language%20Outputs%3F&body=Title%3A%20Loop%20as%20a%20Bridge%3A%20Can%20Looped%20Transformers%20Truly%20Link%20Representation%20Space%20and%20Natural%20Language%20Outputs%3F%0AAuthor%3A%20Guanxu%20Chen%20and%20Dongrui%20Liu%20and%20Jing%20Shao%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20often%20exhibit%20a%20gap%20between%20their%20internal%20knowledge%20and%20their%20explicit%20linguistic%20outputs.%20In%20this%20report%2C%20we%20empirically%20investigate%20whether%20Looped%20Transformers%20%28LTs%29--architectures%20that%20increase%20computational%20depth%20by%20iterating%20shared%20layers--can%20bridge%20this%20gap%20by%20utilizing%20their%20iterative%20nature%20as%20a%20form%20of%20introspection.%20Our%20experiments%20reveal%20that%20while%20increasing%20loop%20iterations%20narrows%20the%20gap%2C%20it%20is%20partly%20driven%20by%20a%20degradation%20of%20their%20internal%20knowledge%20carried%20by%20representations.%20Moreover%2C%20another%20empirical%20analysis%20suggests%20that%20current%20LTs%27%20ability%20to%20perceive%20representations%20does%20not%20improve%20across%20loops%3B%20it%20is%20only%20present%20in%20the%20final%20loop.%20These%20results%20suggest%20that%20while%20LTs%20offer%20a%20promising%20direction%20for%20scaling%20computational%20depth%2C%20they%20have%20yet%20to%20achieve%20the%20introspection%20required%20to%20truly%20link%20representation%20space%20and%20natural%20language.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10242v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLoop%2520as%2520a%2520Bridge%253A%2520Can%2520Looped%2520Transformers%2520Truly%2520Link%2520Representation%2520Space%2520and%2520Natural%2520Language%2520Outputs%253F%26entry.906535625%3DGuanxu%2520Chen%2520and%2520Dongrui%2520Liu%2520and%2520Jing%2520Shao%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520often%2520exhibit%2520a%2520gap%2520between%2520their%2520internal%2520knowledge%2520and%2520their%2520explicit%2520linguistic%2520outputs.%2520In%2520this%2520report%252C%2520we%2520empirically%2520investigate%2520whether%2520Looped%2520Transformers%2520%2528LTs%2529--architectures%2520that%2520increase%2520computational%2520depth%2520by%2520iterating%2520shared%2520layers--can%2520bridge%2520this%2520gap%2520by%2520utilizing%2520their%2520iterative%2520nature%2520as%2520a%2520form%2520of%2520introspection.%2520Our%2520experiments%2520reveal%2520that%2520while%2520increasing%2520loop%2520iterations%2520narrows%2520the%2520gap%252C%2520it%2520is%2520partly%2520driven%2520by%2520a%2520degradation%2520of%2520their%2520internal%2520knowledge%2520carried%2520by%2520representations.%2520Moreover%252C%2520another%2520empirical%2520analysis%2520suggests%2520that%2520current%2520LTs%2527%2520ability%2520to%2520perceive%2520representations%2520does%2520not%2520improve%2520across%2520loops%253B%2520it%2520is%2520only%2520present%2520in%2520the%2520final%2520loop.%2520These%2520results%2520suggest%2520that%2520while%2520LTs%2520offer%2520a%2520promising%2520direction%2520for%2520scaling%2520computational%2520depth%252C%2520they%2520have%2520yet%2520to%2520achieve%2520the%2520introspection%2520required%2520to%2520truly%2520link%2520representation%2520space%2520and%2520natural%2520language.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10242v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Loop%20as%20a%20Bridge%3A%20Can%20Looped%20Transformers%20Truly%20Link%20Representation%20Space%20and%20Natural%20Language%20Outputs%3F&entry.906535625=Guanxu%20Chen%20and%20Dongrui%20Liu%20and%20Jing%20Shao&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20often%20exhibit%20a%20gap%20between%20their%20internal%20knowledge%20and%20their%20explicit%20linguistic%20outputs.%20In%20this%20report%2C%20we%20empirically%20investigate%20whether%20Looped%20Transformers%20%28LTs%29--architectures%20that%20increase%20computational%20depth%20by%20iterating%20shared%20layers--can%20bridge%20this%20gap%20by%20utilizing%20their%20iterative%20nature%20as%20a%20form%20of%20introspection.%20Our%20experiments%20reveal%20that%20while%20increasing%20loop%20iterations%20narrows%20the%20gap%2C%20it%20is%20partly%20driven%20by%20a%20degradation%20of%20their%20internal%20knowledge%20carried%20by%20representations.%20Moreover%2C%20another%20empirical%20analysis%20suggests%20that%20current%20LTs%27%20ability%20to%20perceive%20representations%20does%20not%20improve%20across%20loops%3B%20it%20is%20only%20present%20in%20the%20final%20loop.%20These%20results%20suggest%20that%20while%20LTs%20offer%20a%20promising%20direction%20for%20scaling%20computational%20depth%2C%20they%20have%20yet%20to%20achieve%20the%20introspection%20required%20to%20truly%20link%20representation%20space%20and%20natural%20language.&entry.1838667208=http%3A//arxiv.org/abs/2601.10242v1&entry.124074799=Read"},
{"title": "Spatial As Deep: Spatial CNN for Traffic Scene Understanding", "author": "Xingang Pan and Xiaohang Zhan and Jianping Shi and Ping Luo and Xiaogang Wang and Xiaoou Tang", "abstract": "Convolutional neural networks (CNNs) are usually built by stacking convolutional operations layer-by-layer. Although CNN has shown strong capability to extract semantics from raw pixels, its capacity to capture spatial relationships of pixels across rows and columns of an image is not fully explored. These relationships are important to learn semantic objects with strong shape priors but weak appearance coherences, such as traffic lanes, which are often occluded or not even painted on the road surface as shown in Fig. 1 (a). In this paper, we propose Spatial CNN (SCNN), which generalizes traditional deep layer-by-layer convolutions to slice-byslice convolutions within feature maps, thus enabling message passings between pixels across rows and columns in a layer. Such SCNN is particular suitable for long continuous shape structure or large objects, with strong spatial relationship but less appearance clues, such as traffic lanes, poles, and wall. We apply SCNN on a newly released very challenging traffic lane detection dataset and Cityscapse dataset. The results show that SCNN could learn the spatial relationship for structure output and significantly improves the performance. We show that SCNN outperforms the recurrent neural network (RNN) based ReNet and MRF+CNN (MRFNet) in the lane detection dataset by 8.7% and 4.6% respectively. Moreover, our SCNN won the 1st place on the TuSimple Benchmark Lane Detection Challenge, with an accuracy of 96.53%.", "link": "http://arxiv.org/abs/1712.06080v2", "date": "2026-01-15", "relevancy": 2.0965, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5434}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.529}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spatial%20As%20Deep%3A%20Spatial%20CNN%20for%20Traffic%20Scene%20Understanding&body=Title%3A%20Spatial%20As%20Deep%3A%20Spatial%20CNN%20for%20Traffic%20Scene%20Understanding%0AAuthor%3A%20Xingang%20Pan%20and%20Xiaohang%20Zhan%20and%20Jianping%20Shi%20and%20Ping%20Luo%20and%20Xiaogang%20Wang%20and%20Xiaoou%20Tang%0AAbstract%3A%20Convolutional%20neural%20networks%20%28CNNs%29%20are%20usually%20built%20by%20stacking%20convolutional%20operations%20layer-by-layer.%20Although%20CNN%20has%20shown%20strong%20capability%20to%20extract%20semantics%20from%20raw%20pixels%2C%20its%20capacity%20to%20capture%20spatial%20relationships%20of%20pixels%20across%20rows%20and%20columns%20of%20an%20image%20is%20not%20fully%20explored.%20These%20relationships%20are%20important%20to%20learn%20semantic%20objects%20with%20strong%20shape%20priors%20but%20weak%20appearance%20coherences%2C%20such%20as%20traffic%20lanes%2C%20which%20are%20often%20occluded%20or%20not%20even%20painted%20on%20the%20road%20surface%20as%20shown%20in%20Fig.%201%20%28a%29.%20In%20this%20paper%2C%20we%20propose%20Spatial%20CNN%20%28SCNN%29%2C%20which%20generalizes%20traditional%20deep%20layer-by-layer%20convolutions%20to%20slice-byslice%20convolutions%20within%20feature%20maps%2C%20thus%20enabling%20message%20passings%20between%20pixels%20across%20rows%20and%20columns%20in%20a%20layer.%20Such%20SCNN%20is%20particular%20suitable%20for%20long%20continuous%20shape%20structure%20or%20large%20objects%2C%20with%20strong%20spatial%20relationship%20but%20less%20appearance%20clues%2C%20such%20as%20traffic%20lanes%2C%20poles%2C%20and%20wall.%20We%20apply%20SCNN%20on%20a%20newly%20released%20very%20challenging%20traffic%20lane%20detection%20dataset%20and%20Cityscapse%20dataset.%20The%20results%20show%20that%20SCNN%20could%20learn%20the%20spatial%20relationship%20for%20structure%20output%20and%20significantly%20improves%20the%20performance.%20We%20show%20that%20SCNN%20outperforms%20the%20recurrent%20neural%20network%20%28RNN%29%20based%20ReNet%20and%20MRF%2BCNN%20%28MRFNet%29%20in%20the%20lane%20detection%20dataset%20by%208.7%25%20and%204.6%25%20respectively.%20Moreover%2C%20our%20SCNN%20won%20the%201st%20place%20on%20the%20TuSimple%20Benchmark%20Lane%20Detection%20Challenge%2C%20with%20an%20accuracy%20of%2096.53%25.%0ALink%3A%20http%3A//arxiv.org/abs/1712.06080v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpatial%2520As%2520Deep%253A%2520Spatial%2520CNN%2520for%2520Traffic%2520Scene%2520Understanding%26entry.906535625%3DXingang%2520Pan%2520and%2520Xiaohang%2520Zhan%2520and%2520Jianping%2520Shi%2520and%2520Ping%2520Luo%2520and%2520Xiaogang%2520Wang%2520and%2520Xiaoou%2520Tang%26entry.1292438233%3DConvolutional%2520neural%2520networks%2520%2528CNNs%2529%2520are%2520usually%2520built%2520by%2520stacking%2520convolutional%2520operations%2520layer-by-layer.%2520Although%2520CNN%2520has%2520shown%2520strong%2520capability%2520to%2520extract%2520semantics%2520from%2520raw%2520pixels%252C%2520its%2520capacity%2520to%2520capture%2520spatial%2520relationships%2520of%2520pixels%2520across%2520rows%2520and%2520columns%2520of%2520an%2520image%2520is%2520not%2520fully%2520explored.%2520These%2520relationships%2520are%2520important%2520to%2520learn%2520semantic%2520objects%2520with%2520strong%2520shape%2520priors%2520but%2520weak%2520appearance%2520coherences%252C%2520such%2520as%2520traffic%2520lanes%252C%2520which%2520are%2520often%2520occluded%2520or%2520not%2520even%2520painted%2520on%2520the%2520road%2520surface%2520as%2520shown%2520in%2520Fig.%25201%2520%2528a%2529.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Spatial%2520CNN%2520%2528SCNN%2529%252C%2520which%2520generalizes%2520traditional%2520deep%2520layer-by-layer%2520convolutions%2520to%2520slice-byslice%2520convolutions%2520within%2520feature%2520maps%252C%2520thus%2520enabling%2520message%2520passings%2520between%2520pixels%2520across%2520rows%2520and%2520columns%2520in%2520a%2520layer.%2520Such%2520SCNN%2520is%2520particular%2520suitable%2520for%2520long%2520continuous%2520shape%2520structure%2520or%2520large%2520objects%252C%2520with%2520strong%2520spatial%2520relationship%2520but%2520less%2520appearance%2520clues%252C%2520such%2520as%2520traffic%2520lanes%252C%2520poles%252C%2520and%2520wall.%2520We%2520apply%2520SCNN%2520on%2520a%2520newly%2520released%2520very%2520challenging%2520traffic%2520lane%2520detection%2520dataset%2520and%2520Cityscapse%2520dataset.%2520The%2520results%2520show%2520that%2520SCNN%2520could%2520learn%2520the%2520spatial%2520relationship%2520for%2520structure%2520output%2520and%2520significantly%2520improves%2520the%2520performance.%2520We%2520show%2520that%2520SCNN%2520outperforms%2520the%2520recurrent%2520neural%2520network%2520%2528RNN%2529%2520based%2520ReNet%2520and%2520MRF%252BCNN%2520%2528MRFNet%2529%2520in%2520the%2520lane%2520detection%2520dataset%2520by%25208.7%2525%2520and%25204.6%2525%2520respectively.%2520Moreover%252C%2520our%2520SCNN%2520won%2520the%25201st%2520place%2520on%2520the%2520TuSimple%2520Benchmark%2520Lane%2520Detection%2520Challenge%252C%2520with%2520an%2520accuracy%2520of%252096.53%2525.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/1712.06080v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spatial%20As%20Deep%3A%20Spatial%20CNN%20for%20Traffic%20Scene%20Understanding&entry.906535625=Xingang%20Pan%20and%20Xiaohang%20Zhan%20and%20Jianping%20Shi%20and%20Ping%20Luo%20and%20Xiaogang%20Wang%20and%20Xiaoou%20Tang&entry.1292438233=Convolutional%20neural%20networks%20%28CNNs%29%20are%20usually%20built%20by%20stacking%20convolutional%20operations%20layer-by-layer.%20Although%20CNN%20has%20shown%20strong%20capability%20to%20extract%20semantics%20from%20raw%20pixels%2C%20its%20capacity%20to%20capture%20spatial%20relationships%20of%20pixels%20across%20rows%20and%20columns%20of%20an%20image%20is%20not%20fully%20explored.%20These%20relationships%20are%20important%20to%20learn%20semantic%20objects%20with%20strong%20shape%20priors%20but%20weak%20appearance%20coherences%2C%20such%20as%20traffic%20lanes%2C%20which%20are%20often%20occluded%20or%20not%20even%20painted%20on%20the%20road%20surface%20as%20shown%20in%20Fig.%201%20%28a%29.%20In%20this%20paper%2C%20we%20propose%20Spatial%20CNN%20%28SCNN%29%2C%20which%20generalizes%20traditional%20deep%20layer-by-layer%20convolutions%20to%20slice-byslice%20convolutions%20within%20feature%20maps%2C%20thus%20enabling%20message%20passings%20between%20pixels%20across%20rows%20and%20columns%20in%20a%20layer.%20Such%20SCNN%20is%20particular%20suitable%20for%20long%20continuous%20shape%20structure%20or%20large%20objects%2C%20with%20strong%20spatial%20relationship%20but%20less%20appearance%20clues%2C%20such%20as%20traffic%20lanes%2C%20poles%2C%20and%20wall.%20We%20apply%20SCNN%20on%20a%20newly%20released%20very%20challenging%20traffic%20lane%20detection%20dataset%20and%20Cityscapse%20dataset.%20The%20results%20show%20that%20SCNN%20could%20learn%20the%20spatial%20relationship%20for%20structure%20output%20and%20significantly%20improves%20the%20performance.%20We%20show%20that%20SCNN%20outperforms%20the%20recurrent%20neural%20network%20%28RNN%29%20based%20ReNet%20and%20MRF%2BCNN%20%28MRFNet%29%20in%20the%20lane%20detection%20dataset%20by%208.7%25%20and%204.6%25%20respectively.%20Moreover%2C%20our%20SCNN%20won%20the%201st%20place%20on%20the%20TuSimple%20Benchmark%20Lane%20Detection%20Challenge%2C%20with%20an%20accuracy%20of%2096.53%25.&entry.1838667208=http%3A//arxiv.org/abs/1712.06080v2&entry.124074799=Read"},
{"title": "NSR-Boost: A Neuro-Symbolic Residual Boosting Framework for Industrial Legacy Models", "author": "Ziming Dai and Dabiao Ma and Jinle Tong and Mengyuan Han and Jian Yang and Haojun Fei", "abstract": "Although the Gradient Boosted Decision Trees (GBDTs) dominate industrial tabular applications, upgrading legacy models in high-concurrency production environments still faces prohibitive retraining costs and systemic risks. To address this problem, we present NSR-Boost, a neuro-symbolic residual boosting framework designed specifically for industrial scenarios. Its core advantage lies in being \"non-intrusive\". It treats the legacy model as a frozen model and performs targeted repairs on \"hard regions\" where predictions fail. The framework comprises three key stages: first, finding hard regions through residuals, then generating interpretable experts by generating symbolic code structures using Large Language Model (LLM) and fine-tuning parameters using Bayesian optimization, and finally dynamically integrating experts with legacy model output through a lightweight aggregator. We report on the successful deployment of NSR-Boost within the core financial risk control system at Qfin Holdings. This framework not only significantly outperforms state-of-the-art (SOTA) baselines across six public datasets and one private dataset, more importantly, shows excellent performance gains on real-world online data. In conclusion, it effectively captures long-tail risks missed by traditional models and offers a safe, low-cost evolutionary paradigm for industry.", "link": "http://arxiv.org/abs/2601.10457v1", "date": "2026-01-15", "relevancy": 2.0951, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5876}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4822}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NSR-Boost%3A%20A%20Neuro-Symbolic%20Residual%20Boosting%20Framework%20for%20Industrial%20Legacy%20Models&body=Title%3A%20NSR-Boost%3A%20A%20Neuro-Symbolic%20Residual%20Boosting%20Framework%20for%20Industrial%20Legacy%20Models%0AAuthor%3A%20Ziming%20Dai%20and%20Dabiao%20Ma%20and%20Jinle%20Tong%20and%20Mengyuan%20Han%20and%20Jian%20Yang%20and%20Haojun%20Fei%0AAbstract%3A%20Although%20the%20Gradient%20Boosted%20Decision%20Trees%20%28GBDTs%29%20dominate%20industrial%20tabular%20applications%2C%20upgrading%20legacy%20models%20in%20high-concurrency%20production%20environments%20still%20faces%20prohibitive%20retraining%20costs%20and%20systemic%20risks.%20To%20address%20this%20problem%2C%20we%20present%20NSR-Boost%2C%20a%20neuro-symbolic%20residual%20boosting%20framework%20designed%20specifically%20for%20industrial%20scenarios.%20Its%20core%20advantage%20lies%20in%20being%20%22non-intrusive%22.%20It%20treats%20the%20legacy%20model%20as%20a%20frozen%20model%20and%20performs%20targeted%20repairs%20on%20%22hard%20regions%22%20where%20predictions%20fail.%20The%20framework%20comprises%20three%20key%20stages%3A%20first%2C%20finding%20hard%20regions%20through%20residuals%2C%20then%20generating%20interpretable%20experts%20by%20generating%20symbolic%20code%20structures%20using%20Large%20Language%20Model%20%28LLM%29%20and%20fine-tuning%20parameters%20using%20Bayesian%20optimization%2C%20and%20finally%20dynamically%20integrating%20experts%20with%20legacy%20model%20output%20through%20a%20lightweight%20aggregator.%20We%20report%20on%20the%20successful%20deployment%20of%20NSR-Boost%20within%20the%20core%20financial%20risk%20control%20system%20at%20Qfin%20Holdings.%20This%20framework%20not%20only%20significantly%20outperforms%20state-of-the-art%20%28SOTA%29%20baselines%20across%20six%20public%20datasets%20and%20one%20private%20dataset%2C%20more%20importantly%2C%20shows%20excellent%20performance%20gains%20on%20real-world%20online%20data.%20In%20conclusion%2C%20it%20effectively%20captures%20long-tail%20risks%20missed%20by%20traditional%20models%20and%20offers%20a%20safe%2C%20low-cost%20evolutionary%20paradigm%20for%20industry.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10457v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNSR-Boost%253A%2520A%2520Neuro-Symbolic%2520Residual%2520Boosting%2520Framework%2520for%2520Industrial%2520Legacy%2520Models%26entry.906535625%3DZiming%2520Dai%2520and%2520Dabiao%2520Ma%2520and%2520Jinle%2520Tong%2520and%2520Mengyuan%2520Han%2520and%2520Jian%2520Yang%2520and%2520Haojun%2520Fei%26entry.1292438233%3DAlthough%2520the%2520Gradient%2520Boosted%2520Decision%2520Trees%2520%2528GBDTs%2529%2520dominate%2520industrial%2520tabular%2520applications%252C%2520upgrading%2520legacy%2520models%2520in%2520high-concurrency%2520production%2520environments%2520still%2520faces%2520prohibitive%2520retraining%2520costs%2520and%2520systemic%2520risks.%2520To%2520address%2520this%2520problem%252C%2520we%2520present%2520NSR-Boost%252C%2520a%2520neuro-symbolic%2520residual%2520boosting%2520framework%2520designed%2520specifically%2520for%2520industrial%2520scenarios.%2520Its%2520core%2520advantage%2520lies%2520in%2520being%2520%2522non-intrusive%2522.%2520It%2520treats%2520the%2520legacy%2520model%2520as%2520a%2520frozen%2520model%2520and%2520performs%2520targeted%2520repairs%2520on%2520%2522hard%2520regions%2522%2520where%2520predictions%2520fail.%2520The%2520framework%2520comprises%2520three%2520key%2520stages%253A%2520first%252C%2520finding%2520hard%2520regions%2520through%2520residuals%252C%2520then%2520generating%2520interpretable%2520experts%2520by%2520generating%2520symbolic%2520code%2520structures%2520using%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520and%2520fine-tuning%2520parameters%2520using%2520Bayesian%2520optimization%252C%2520and%2520finally%2520dynamically%2520integrating%2520experts%2520with%2520legacy%2520model%2520output%2520through%2520a%2520lightweight%2520aggregator.%2520We%2520report%2520on%2520the%2520successful%2520deployment%2520of%2520NSR-Boost%2520within%2520the%2520core%2520financial%2520risk%2520control%2520system%2520at%2520Qfin%2520Holdings.%2520This%2520framework%2520not%2520only%2520significantly%2520outperforms%2520state-of-the-art%2520%2528SOTA%2529%2520baselines%2520across%2520six%2520public%2520datasets%2520and%2520one%2520private%2520dataset%252C%2520more%2520importantly%252C%2520shows%2520excellent%2520performance%2520gains%2520on%2520real-world%2520online%2520data.%2520In%2520conclusion%252C%2520it%2520effectively%2520captures%2520long-tail%2520risks%2520missed%2520by%2520traditional%2520models%2520and%2520offers%2520a%2520safe%252C%2520low-cost%2520evolutionary%2520paradigm%2520for%2520industry.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10457v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NSR-Boost%3A%20A%20Neuro-Symbolic%20Residual%20Boosting%20Framework%20for%20Industrial%20Legacy%20Models&entry.906535625=Ziming%20Dai%20and%20Dabiao%20Ma%20and%20Jinle%20Tong%20and%20Mengyuan%20Han%20and%20Jian%20Yang%20and%20Haojun%20Fei&entry.1292438233=Although%20the%20Gradient%20Boosted%20Decision%20Trees%20%28GBDTs%29%20dominate%20industrial%20tabular%20applications%2C%20upgrading%20legacy%20models%20in%20high-concurrency%20production%20environments%20still%20faces%20prohibitive%20retraining%20costs%20and%20systemic%20risks.%20To%20address%20this%20problem%2C%20we%20present%20NSR-Boost%2C%20a%20neuro-symbolic%20residual%20boosting%20framework%20designed%20specifically%20for%20industrial%20scenarios.%20Its%20core%20advantage%20lies%20in%20being%20%22non-intrusive%22.%20It%20treats%20the%20legacy%20model%20as%20a%20frozen%20model%20and%20performs%20targeted%20repairs%20on%20%22hard%20regions%22%20where%20predictions%20fail.%20The%20framework%20comprises%20three%20key%20stages%3A%20first%2C%20finding%20hard%20regions%20through%20residuals%2C%20then%20generating%20interpretable%20experts%20by%20generating%20symbolic%20code%20structures%20using%20Large%20Language%20Model%20%28LLM%29%20and%20fine-tuning%20parameters%20using%20Bayesian%20optimization%2C%20and%20finally%20dynamically%20integrating%20experts%20with%20legacy%20model%20output%20through%20a%20lightweight%20aggregator.%20We%20report%20on%20the%20successful%20deployment%20of%20NSR-Boost%20within%20the%20core%20financial%20risk%20control%20system%20at%20Qfin%20Holdings.%20This%20framework%20not%20only%20significantly%20outperforms%20state-of-the-art%20%28SOTA%29%20baselines%20across%20six%20public%20datasets%20and%20one%20private%20dataset%2C%20more%20importantly%2C%20shows%20excellent%20performance%20gains%20on%20real-world%20online%20data.%20In%20conclusion%2C%20it%20effectively%20captures%20long-tail%20risks%20missed%20by%20traditional%20models%20and%20offers%20a%20safe%2C%20low-cost%20evolutionary%20paradigm%20for%20industry.&entry.1838667208=http%3A//arxiv.org/abs/2601.10457v1&entry.124074799=Read"},
{"title": "MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching", "author": "Changle Qu and Sunhao Dai and Hengyi Cai and Jun Xu and Shuaiqiang Wang and Dawei Yin", "abstract": "Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.", "link": "http://arxiv.org/abs/2601.10712v1", "date": "2026-01-15", "relevancy": 2.0804, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5401}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.506}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5054}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MatchTIR%3A%20Fine-Grained%20Supervision%20for%20Tool-Integrated%20Reasoning%20via%20Bipartite%20Matching&body=Title%3A%20MatchTIR%3A%20Fine-Grained%20Supervision%20for%20Tool-Integrated%20Reasoning%20via%20Bipartite%20Matching%0AAuthor%3A%20Changle%20Qu%20and%20Sunhao%20Dai%20and%20Hengyi%20Cai%20and%20Jun%20Xu%20and%20Shuaiqiang%20Wang%20and%20Dawei%20Yin%0AAbstract%3A%20Tool-Integrated%20Reasoning%20%28TIR%29%20empowers%20large%20language%20models%20%28LLMs%29%20to%20tackle%20complex%20tasks%20by%20interleaving%20reasoning%20steps%20with%20external%20tool%20interactions.%20However%2C%20existing%20reinforcement%20learning%20methods%20typically%20rely%20on%20outcome-%20or%20trajectory-level%20rewards%2C%20assigning%20uniform%20advantages%20to%20all%20steps%20within%20a%20trajectory.%20This%20coarse-grained%20credit%20assignment%20fails%20to%20distinguish%20effective%20tool%20calls%20from%20redundant%20or%20erroneous%20ones%2C%20particularly%20in%20long-horizon%20multi-turn%20scenarios.%20To%20address%20this%2C%20we%20propose%20MatchTIR%2C%20a%20framework%20that%20introduces%20fine-grained%20supervision%20via%20bipartite%20matching-based%20turn-level%20reward%20assignment%20and%20dual-level%20advantage%20estimation.%20Specifically%2C%20we%20formulate%20credit%20assignment%20as%20a%20bipartite%20matching%20problem%20between%20predicted%20and%20ground-truth%20traces%2C%20utilizing%20two%20assignment%20strategies%20to%20derive%20dense%20turn-level%20rewards.%20Furthermore%2C%20to%20balance%20local%20step%20precision%20with%20global%20task%20success%2C%20we%20introduce%20a%20dual-level%20advantage%20estimation%20scheme%20that%20integrates%20turn-level%20and%20trajectory-level%20signals%2C%20assigning%20distinct%20advantage%20values%20to%20individual%20interaction%20turns.%20Extensive%20experiments%20on%20three%20benchmarks%20demonstrate%20the%20superiority%20of%20MatchTIR.%20Notably%2C%20our%204B%20model%20surpasses%20the%20majority%20of%208B%20competitors%2C%20particularly%20in%20long-horizon%20and%20multi-turn%20tasks.%20Our%20codes%20are%20available%20at%20https%3A//github.com/quchangle1/MatchTIR.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10712v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatchTIR%253A%2520Fine-Grained%2520Supervision%2520for%2520Tool-Integrated%2520Reasoning%2520via%2520Bipartite%2520Matching%26entry.906535625%3DChangle%2520Qu%2520and%2520Sunhao%2520Dai%2520and%2520Hengyi%2520Cai%2520and%2520Jun%2520Xu%2520and%2520Shuaiqiang%2520Wang%2520and%2520Dawei%2520Yin%26entry.1292438233%3DTool-Integrated%2520Reasoning%2520%2528TIR%2529%2520empowers%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520tackle%2520complex%2520tasks%2520by%2520interleaving%2520reasoning%2520steps%2520with%2520external%2520tool%2520interactions.%2520However%252C%2520existing%2520reinforcement%2520learning%2520methods%2520typically%2520rely%2520on%2520outcome-%2520or%2520trajectory-level%2520rewards%252C%2520assigning%2520uniform%2520advantages%2520to%2520all%2520steps%2520within%2520a%2520trajectory.%2520This%2520coarse-grained%2520credit%2520assignment%2520fails%2520to%2520distinguish%2520effective%2520tool%2520calls%2520from%2520redundant%2520or%2520erroneous%2520ones%252C%2520particularly%2520in%2520long-horizon%2520multi-turn%2520scenarios.%2520To%2520address%2520this%252C%2520we%2520propose%2520MatchTIR%252C%2520a%2520framework%2520that%2520introduces%2520fine-grained%2520supervision%2520via%2520bipartite%2520matching-based%2520turn-level%2520reward%2520assignment%2520and%2520dual-level%2520advantage%2520estimation.%2520Specifically%252C%2520we%2520formulate%2520credit%2520assignment%2520as%2520a%2520bipartite%2520matching%2520problem%2520between%2520predicted%2520and%2520ground-truth%2520traces%252C%2520utilizing%2520two%2520assignment%2520strategies%2520to%2520derive%2520dense%2520turn-level%2520rewards.%2520Furthermore%252C%2520to%2520balance%2520local%2520step%2520precision%2520with%2520global%2520task%2520success%252C%2520we%2520introduce%2520a%2520dual-level%2520advantage%2520estimation%2520scheme%2520that%2520integrates%2520turn-level%2520and%2520trajectory-level%2520signals%252C%2520assigning%2520distinct%2520advantage%2520values%2520to%2520individual%2520interaction%2520turns.%2520Extensive%2520experiments%2520on%2520three%2520benchmarks%2520demonstrate%2520the%2520superiority%2520of%2520MatchTIR.%2520Notably%252C%2520our%25204B%2520model%2520surpasses%2520the%2520majority%2520of%25208B%2520competitors%252C%2520particularly%2520in%2520long-horizon%2520and%2520multi-turn%2520tasks.%2520Our%2520codes%2520are%2520available%2520at%2520https%253A//github.com/quchangle1/MatchTIR.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10712v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MatchTIR%3A%20Fine-Grained%20Supervision%20for%20Tool-Integrated%20Reasoning%20via%20Bipartite%20Matching&entry.906535625=Changle%20Qu%20and%20Sunhao%20Dai%20and%20Hengyi%20Cai%20and%20Jun%20Xu%20and%20Shuaiqiang%20Wang%20and%20Dawei%20Yin&entry.1292438233=Tool-Integrated%20Reasoning%20%28TIR%29%20empowers%20large%20language%20models%20%28LLMs%29%20to%20tackle%20complex%20tasks%20by%20interleaving%20reasoning%20steps%20with%20external%20tool%20interactions.%20However%2C%20existing%20reinforcement%20learning%20methods%20typically%20rely%20on%20outcome-%20or%20trajectory-level%20rewards%2C%20assigning%20uniform%20advantages%20to%20all%20steps%20within%20a%20trajectory.%20This%20coarse-grained%20credit%20assignment%20fails%20to%20distinguish%20effective%20tool%20calls%20from%20redundant%20or%20erroneous%20ones%2C%20particularly%20in%20long-horizon%20multi-turn%20scenarios.%20To%20address%20this%2C%20we%20propose%20MatchTIR%2C%20a%20framework%20that%20introduces%20fine-grained%20supervision%20via%20bipartite%20matching-based%20turn-level%20reward%20assignment%20and%20dual-level%20advantage%20estimation.%20Specifically%2C%20we%20formulate%20credit%20assignment%20as%20a%20bipartite%20matching%20problem%20between%20predicted%20and%20ground-truth%20traces%2C%20utilizing%20two%20assignment%20strategies%20to%20derive%20dense%20turn-level%20rewards.%20Furthermore%2C%20to%20balance%20local%20step%20precision%20with%20global%20task%20success%2C%20we%20introduce%20a%20dual-level%20advantage%20estimation%20scheme%20that%20integrates%20turn-level%20and%20trajectory-level%20signals%2C%20assigning%20distinct%20advantage%20values%20to%20individual%20interaction%20turns.%20Extensive%20experiments%20on%20three%20benchmarks%20demonstrate%20the%20superiority%20of%20MatchTIR.%20Notably%2C%20our%204B%20model%20surpasses%20the%20majority%20of%208B%20competitors%2C%20particularly%20in%20long-horizon%20and%20multi-turn%20tasks.%20Our%20codes%20are%20available%20at%20https%3A//github.com/quchangle1/MatchTIR.&entry.1838667208=http%3A//arxiv.org/abs/2601.10712v1&entry.124074799=Read"},
{"title": "Explicit Abstention Knobs for Predictable Reliability in Video Question Answering", "author": "Jorge Ortiz", "abstract": "High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f", "link": "http://arxiv.org/abs/2601.00138v2", "date": "2026-01-15", "relevancy": 2.0802, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.561}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5035}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4857}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explicit%20Abstention%20Knobs%20for%20Predictable%20Reliability%20in%20Video%20Question%20Answering&body=Title%3A%20Explicit%20Abstention%20Knobs%20for%20Predictable%20Reliability%20in%20Video%20Question%20Answering%0AAuthor%3A%20Jorge%20Ortiz%0AAbstract%3A%20High-stakes%20deployment%20of%20vision-language%20models%20%28VLMs%29%20requires%20selective%20prediction%2C%20where%20systems%20abstain%20when%20uncertain%20rather%20than%20risk%20costly%20errors.%20We%20investigate%20whether%20confidence-based%20abstention%20provides%20reliable%20control%20over%20error%20rates%20in%20video%20question%20answering%2C%20and%20whether%20that%20control%20remains%20robust%20under%20distribution%20shift.%20Using%20NExT-QA%20and%20Gemini%202.0%20Flash%2C%20we%20establish%20two%20findings.%20First%2C%20confidence%20thresholding%20provides%20mechanistic%20control%20in-distribution.%20Sweeping%20threshold%20epsilon%20produces%20smooth%20risk-coverage%20tradeoffs%2C%20reducing%20error%20rates%20f%0ALink%3A%20http%3A//arxiv.org/abs/2601.00138v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplicit%2520Abstention%2520Knobs%2520for%2520Predictable%2520Reliability%2520in%2520Video%2520Question%2520Answering%26entry.906535625%3DJorge%2520Ortiz%26entry.1292438233%3DHigh-stakes%2520deployment%2520of%2520vision-language%2520models%2520%2528VLMs%2529%2520requires%2520selective%2520prediction%252C%2520where%2520systems%2520abstain%2520when%2520uncertain%2520rather%2520than%2520risk%2520costly%2520errors.%2520We%2520investigate%2520whether%2520confidence-based%2520abstention%2520provides%2520reliable%2520control%2520over%2520error%2520rates%2520in%2520video%2520question%2520answering%252C%2520and%2520whether%2520that%2520control%2520remains%2520robust%2520under%2520distribution%2520shift.%2520Using%2520NExT-QA%2520and%2520Gemini%25202.0%2520Flash%252C%2520we%2520establish%2520two%2520findings.%2520First%252C%2520confidence%2520thresholding%2520provides%2520mechanistic%2520control%2520in-distribution.%2520Sweeping%2520threshold%2520epsilon%2520produces%2520smooth%2520risk-coverage%2520tradeoffs%252C%2520reducing%2520error%2520rates%2520f%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.00138v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicit%20Abstention%20Knobs%20for%20Predictable%20Reliability%20in%20Video%20Question%20Answering&entry.906535625=Jorge%20Ortiz&entry.1292438233=High-stakes%20deployment%20of%20vision-language%20models%20%28VLMs%29%20requires%20selective%20prediction%2C%20where%20systems%20abstain%20when%20uncertain%20rather%20than%20risk%20costly%20errors.%20We%20investigate%20whether%20confidence-based%20abstention%20provides%20reliable%20control%20over%20error%20rates%20in%20video%20question%20answering%2C%20and%20whether%20that%20control%20remains%20robust%20under%20distribution%20shift.%20Using%20NExT-QA%20and%20Gemini%202.0%20Flash%2C%20we%20establish%20two%20findings.%20First%2C%20confidence%20thresholding%20provides%20mechanistic%20control%20in-distribution.%20Sweeping%20threshold%20epsilon%20produces%20smooth%20risk-coverage%20tradeoffs%2C%20reducing%20error%20rates%20f&entry.1838667208=http%3A//arxiv.org/abs/2601.00138v2&entry.124074799=Read"},
{"title": "Instance-level quantitative saliency in multiple sclerosis lesion segmentation", "author": "Federico Spagnolo and Nataliia Molchanova and Meritxell Bach Cuadra and Mario Ocampo Pineda and Lester Melie-Garcia and Cristina Granziera and Vincent Andrearczyk and Adrien Depeursinge", "abstract": "Explainable artificial intelligence (XAI) methods have been proposed to interpret model decisions in classification and, more recently, in semantic segmentation. However, instance-level XAI for semantic segmentation, namely explanations focused on a single object among multiple instances of the same class, remains largely unexplored. Such explanations are particularly important in multi-lesional diseases to understand what drives the detection and contouring of a specific lesion. We propose instance-level explanation maps for semantic segmentation by extending SmoothGrad and Grad-CAM++ to obtain quantitative instance saliency. These methods were applied to the segmentation of white matter lesions (WMLs), a magnetic resonance imaging biomarker in multiple sclerosis. We used 4023 FLAIR and MPRAGE MRI scans from 687 patients collected at the University Hospital of Basel, Switzerland, with WML masks annotated by four expert clinicians. Three deep learning architectures, a 3D U-Net, nnU-Net, and Swin UNETR, were trained and evaluated, achieving normalized Dice scores of 0.71, 0.78, and 0.80, respectively. Instance saliency maps showed that the models relied primarily on FLAIR rather than MPRAGE for WML segmentation, with positive saliency inside lesions and negative saliency in their immediate neighborhood, consistent with clinical practice. Peak saliency values differed significantly across correct and incorrect predictions, suggesting that quantitative instance saliency may help identify segmentation errors. In conclusion, we introduce two architecture-agnostic XAI methods that provide quantitative instance-level explanations for semantic segmentation and support clinically meaningful interpretation of model decisions.", "link": "http://arxiv.org/abs/2406.09335v3", "date": "2026-01-15", "relevancy": 2.0732, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.519}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.519}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5148}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Instance-level%20quantitative%20saliency%20in%20multiple%20sclerosis%20lesion%20segmentation&body=Title%3A%20Instance-level%20quantitative%20saliency%20in%20multiple%20sclerosis%20lesion%20segmentation%0AAuthor%3A%20Federico%20Spagnolo%20and%20Nataliia%20Molchanova%20and%20Meritxell%20Bach%20Cuadra%20and%20Mario%20Ocampo%20Pineda%20and%20Lester%20Melie-Garcia%20and%20Cristina%20Granziera%20and%20Vincent%20Andrearczyk%20and%20Adrien%20Depeursinge%0AAbstract%3A%20Explainable%20artificial%20intelligence%20%28XAI%29%20methods%20have%20been%20proposed%20to%20interpret%20model%20decisions%20in%20classification%20and%2C%20more%20recently%2C%20in%20semantic%20segmentation.%20However%2C%20instance-level%20XAI%20for%20semantic%20segmentation%2C%20namely%20explanations%20focused%20on%20a%20single%20object%20among%20multiple%20instances%20of%20the%20same%20class%2C%20remains%20largely%20unexplored.%20Such%20explanations%20are%20particularly%20important%20in%20multi-lesional%20diseases%20to%20understand%20what%20drives%20the%20detection%20and%20contouring%20of%20a%20specific%20lesion.%20We%20propose%20instance-level%20explanation%20maps%20for%20semantic%20segmentation%20by%20extending%20SmoothGrad%20and%20Grad-CAM%2B%2B%20to%20obtain%20quantitative%20instance%20saliency.%20These%20methods%20were%20applied%20to%20the%20segmentation%20of%20white%20matter%20lesions%20%28WMLs%29%2C%20a%20magnetic%20resonance%20imaging%20biomarker%20in%20multiple%20sclerosis.%20We%20used%204023%20FLAIR%20and%20MPRAGE%20MRI%20scans%20from%20687%20patients%20collected%20at%20the%20University%20Hospital%20of%20Basel%2C%20Switzerland%2C%20with%20WML%20masks%20annotated%20by%20four%20expert%20clinicians.%20Three%20deep%20learning%20architectures%2C%20a%203D%20U-Net%2C%20nnU-Net%2C%20and%20Swin%20UNETR%2C%20were%20trained%20and%20evaluated%2C%20achieving%20normalized%20Dice%20scores%20of%200.71%2C%200.78%2C%20and%200.80%2C%20respectively.%20Instance%20saliency%20maps%20showed%20that%20the%20models%20relied%20primarily%20on%20FLAIR%20rather%20than%20MPRAGE%20for%20WML%20segmentation%2C%20with%20positive%20saliency%20inside%20lesions%20and%20negative%20saliency%20in%20their%20immediate%20neighborhood%2C%20consistent%20with%20clinical%20practice.%20Peak%20saliency%20values%20differed%20significantly%20across%20correct%20and%20incorrect%20predictions%2C%20suggesting%20that%20quantitative%20instance%20saliency%20may%20help%20identify%20segmentation%20errors.%20In%20conclusion%2C%20we%20introduce%20two%20architecture-agnostic%20XAI%20methods%20that%20provide%20quantitative%20instance-level%20explanations%20for%20semantic%20segmentation%20and%20support%20clinically%20meaningful%20interpretation%20of%20model%20decisions.%0ALink%3A%20http%3A//arxiv.org/abs/2406.09335v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInstance-level%2520quantitative%2520saliency%2520in%2520multiple%2520sclerosis%2520lesion%2520segmentation%26entry.906535625%3DFederico%2520Spagnolo%2520and%2520Nataliia%2520Molchanova%2520and%2520Meritxell%2520Bach%2520Cuadra%2520and%2520Mario%2520Ocampo%2520Pineda%2520and%2520Lester%2520Melie-Garcia%2520and%2520Cristina%2520Granziera%2520and%2520Vincent%2520Andrearczyk%2520and%2520Adrien%2520Depeursinge%26entry.1292438233%3DExplainable%2520artificial%2520intelligence%2520%2528XAI%2529%2520methods%2520have%2520been%2520proposed%2520to%2520interpret%2520model%2520decisions%2520in%2520classification%2520and%252C%2520more%2520recently%252C%2520in%2520semantic%2520segmentation.%2520However%252C%2520instance-level%2520XAI%2520for%2520semantic%2520segmentation%252C%2520namely%2520explanations%2520focused%2520on%2520a%2520single%2520object%2520among%2520multiple%2520instances%2520of%2520the%2520same%2520class%252C%2520remains%2520largely%2520unexplored.%2520Such%2520explanations%2520are%2520particularly%2520important%2520in%2520multi-lesional%2520diseases%2520to%2520understand%2520what%2520drives%2520the%2520detection%2520and%2520contouring%2520of%2520a%2520specific%2520lesion.%2520We%2520propose%2520instance-level%2520explanation%2520maps%2520for%2520semantic%2520segmentation%2520by%2520extending%2520SmoothGrad%2520and%2520Grad-CAM%252B%252B%2520to%2520obtain%2520quantitative%2520instance%2520saliency.%2520These%2520methods%2520were%2520applied%2520to%2520the%2520segmentation%2520of%2520white%2520matter%2520lesions%2520%2528WMLs%2529%252C%2520a%2520magnetic%2520resonance%2520imaging%2520biomarker%2520in%2520multiple%2520sclerosis.%2520We%2520used%25204023%2520FLAIR%2520and%2520MPRAGE%2520MRI%2520scans%2520from%2520687%2520patients%2520collected%2520at%2520the%2520University%2520Hospital%2520of%2520Basel%252C%2520Switzerland%252C%2520with%2520WML%2520masks%2520annotated%2520by%2520four%2520expert%2520clinicians.%2520Three%2520deep%2520learning%2520architectures%252C%2520a%25203D%2520U-Net%252C%2520nnU-Net%252C%2520and%2520Swin%2520UNETR%252C%2520were%2520trained%2520and%2520evaluated%252C%2520achieving%2520normalized%2520Dice%2520scores%2520of%25200.71%252C%25200.78%252C%2520and%25200.80%252C%2520respectively.%2520Instance%2520saliency%2520maps%2520showed%2520that%2520the%2520models%2520relied%2520primarily%2520on%2520FLAIR%2520rather%2520than%2520MPRAGE%2520for%2520WML%2520segmentation%252C%2520with%2520positive%2520saliency%2520inside%2520lesions%2520and%2520negative%2520saliency%2520in%2520their%2520immediate%2520neighborhood%252C%2520consistent%2520with%2520clinical%2520practice.%2520Peak%2520saliency%2520values%2520differed%2520significantly%2520across%2520correct%2520and%2520incorrect%2520predictions%252C%2520suggesting%2520that%2520quantitative%2520instance%2520saliency%2520may%2520help%2520identify%2520segmentation%2520errors.%2520In%2520conclusion%252C%2520we%2520introduce%2520two%2520architecture-agnostic%2520XAI%2520methods%2520that%2520provide%2520quantitative%2520instance-level%2520explanations%2520for%2520semantic%2520segmentation%2520and%2520support%2520clinically%2520meaningful%2520interpretation%2520of%2520model%2520decisions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09335v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Instance-level%20quantitative%20saliency%20in%20multiple%20sclerosis%20lesion%20segmentation&entry.906535625=Federico%20Spagnolo%20and%20Nataliia%20Molchanova%20and%20Meritxell%20Bach%20Cuadra%20and%20Mario%20Ocampo%20Pineda%20and%20Lester%20Melie-Garcia%20and%20Cristina%20Granziera%20and%20Vincent%20Andrearczyk%20and%20Adrien%20Depeursinge&entry.1292438233=Explainable%20artificial%20intelligence%20%28XAI%29%20methods%20have%20been%20proposed%20to%20interpret%20model%20decisions%20in%20classification%20and%2C%20more%20recently%2C%20in%20semantic%20segmentation.%20However%2C%20instance-level%20XAI%20for%20semantic%20segmentation%2C%20namely%20explanations%20focused%20on%20a%20single%20object%20among%20multiple%20instances%20of%20the%20same%20class%2C%20remains%20largely%20unexplored.%20Such%20explanations%20are%20particularly%20important%20in%20multi-lesional%20diseases%20to%20understand%20what%20drives%20the%20detection%20and%20contouring%20of%20a%20specific%20lesion.%20We%20propose%20instance-level%20explanation%20maps%20for%20semantic%20segmentation%20by%20extending%20SmoothGrad%20and%20Grad-CAM%2B%2B%20to%20obtain%20quantitative%20instance%20saliency.%20These%20methods%20were%20applied%20to%20the%20segmentation%20of%20white%20matter%20lesions%20%28WMLs%29%2C%20a%20magnetic%20resonance%20imaging%20biomarker%20in%20multiple%20sclerosis.%20We%20used%204023%20FLAIR%20and%20MPRAGE%20MRI%20scans%20from%20687%20patients%20collected%20at%20the%20University%20Hospital%20of%20Basel%2C%20Switzerland%2C%20with%20WML%20masks%20annotated%20by%20four%20expert%20clinicians.%20Three%20deep%20learning%20architectures%2C%20a%203D%20U-Net%2C%20nnU-Net%2C%20and%20Swin%20UNETR%2C%20were%20trained%20and%20evaluated%2C%20achieving%20normalized%20Dice%20scores%20of%200.71%2C%200.78%2C%20and%200.80%2C%20respectively.%20Instance%20saliency%20maps%20showed%20that%20the%20models%20relied%20primarily%20on%20FLAIR%20rather%20than%20MPRAGE%20for%20WML%20segmentation%2C%20with%20positive%20saliency%20inside%20lesions%20and%20negative%20saliency%20in%20their%20immediate%20neighborhood%2C%20consistent%20with%20clinical%20practice.%20Peak%20saliency%20values%20differed%20significantly%20across%20correct%20and%20incorrect%20predictions%2C%20suggesting%20that%20quantitative%20instance%20saliency%20may%20help%20identify%20segmentation%20errors.%20In%20conclusion%2C%20we%20introduce%20two%20architecture-agnostic%20XAI%20methods%20that%20provide%20quantitative%20instance-level%20explanations%20for%20semantic%20segmentation%20and%20support%20clinically%20meaningful%20interpretation%20of%20model%20decisions.&entry.1838667208=http%3A//arxiv.org/abs/2406.09335v3&entry.124074799=Read"},
{"title": "Sampling-Based Constrained Motion Planning with Products of Experts", "author": "Amirreza Razmjoo and Teng Xue and Suhan Shetty and Sylvain Calinon", "abstract": "We present a novel approach to enhance the performance of sampling-based Model Predictive Control (MPC) in constrained optimization by leveraging products of experts. Our methodology divides the main problem into two components: one focused on optimality and the other on feasibility. By combining the solutions from each component, represented as distributions, we apply products of experts to implement a project-then-sample strategy. In this strategy, the optimality distribution is projected into the feasible area, allowing for more efficient sampling. This approach contrasts with the traditional sample-then-project and naive sample-then-reject method, leading to more diverse exploration and reducing the accumulation of samples on the boundaries. We demonstrate an effective implementation of this principle using a tensor train-based distribution model, which is characterized by its non-parametric nature, ease of combination with other distributions at the task level, and straightforward sampling technique. We adapt existing tensor train models to suit this purpose and validate the efficacy of our approach through experiments in various tasks, including obstacle avoidance, non-prehensile manipulation, and tasks involving staying in a restricted volume. Our experimental results demonstrate that the proposed method consistently outperforms known baselines, providing strong empirical support for its effectiveness. Sample codes for this project are available at https://github.com/idiap/smpc_poe.", "link": "http://arxiv.org/abs/2412.17462v2", "date": "2026-01-15", "relevancy": 2.0586, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5468}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5098}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5067}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sampling-Based%20Constrained%20Motion%20Planning%20with%20Products%20of%20Experts&body=Title%3A%20Sampling-Based%20Constrained%20Motion%20Planning%20with%20Products%20of%20Experts%0AAuthor%3A%20Amirreza%20Razmjoo%20and%20Teng%20Xue%20and%20Suhan%20Shetty%20and%20Sylvain%20Calinon%0AAbstract%3A%20We%20present%20a%20novel%20approach%20to%20enhance%20the%20performance%20of%20sampling-based%20Model%20Predictive%20Control%20%28MPC%29%20in%20constrained%20optimization%20by%20leveraging%20products%20of%20experts.%20Our%20methodology%20divides%20the%20main%20problem%20into%20two%20components%3A%20one%20focused%20on%20optimality%20and%20the%20other%20on%20feasibility.%20By%20combining%20the%20solutions%20from%20each%20component%2C%20represented%20as%20distributions%2C%20we%20apply%20products%20of%20experts%20to%20implement%20a%20project-then-sample%20strategy.%20In%20this%20strategy%2C%20the%20optimality%20distribution%20is%20projected%20into%20the%20feasible%20area%2C%20allowing%20for%20more%20efficient%20sampling.%20This%20approach%20contrasts%20with%20the%20traditional%20sample-then-project%20and%20naive%20sample-then-reject%20method%2C%20leading%20to%20more%20diverse%20exploration%20and%20reducing%20the%20accumulation%20of%20samples%20on%20the%20boundaries.%20We%20demonstrate%20an%20effective%20implementation%20of%20this%20principle%20using%20a%20tensor%20train-based%20distribution%20model%2C%20which%20is%20characterized%20by%20its%20non-parametric%20nature%2C%20ease%20of%20combination%20with%20other%20distributions%20at%20the%20task%20level%2C%20and%20straightforward%20sampling%20technique.%20We%20adapt%20existing%20tensor%20train%20models%20to%20suit%20this%20purpose%20and%20validate%20the%20efficacy%20of%20our%20approach%20through%20experiments%20in%20various%20tasks%2C%20including%20obstacle%20avoidance%2C%20non-prehensile%20manipulation%2C%20and%20tasks%20involving%20staying%20in%20a%20restricted%20volume.%20Our%20experimental%20results%20demonstrate%20that%20the%20proposed%20method%20consistently%20outperforms%20known%20baselines%2C%20providing%20strong%20empirical%20support%20for%20its%20effectiveness.%20Sample%20codes%20for%20this%20project%20are%20available%20at%20https%3A//github.com/idiap/smpc_poe.%0ALink%3A%20http%3A//arxiv.org/abs/2412.17462v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSampling-Based%2520Constrained%2520Motion%2520Planning%2520with%2520Products%2520of%2520Experts%26entry.906535625%3DAmirreza%2520Razmjoo%2520and%2520Teng%2520Xue%2520and%2520Suhan%2520Shetty%2520and%2520Sylvain%2520Calinon%26entry.1292438233%3DWe%2520present%2520a%2520novel%2520approach%2520to%2520enhance%2520the%2520performance%2520of%2520sampling-based%2520Model%2520Predictive%2520Control%2520%2528MPC%2529%2520in%2520constrained%2520optimization%2520by%2520leveraging%2520products%2520of%2520experts.%2520Our%2520methodology%2520divides%2520the%2520main%2520problem%2520into%2520two%2520components%253A%2520one%2520focused%2520on%2520optimality%2520and%2520the%2520other%2520on%2520feasibility.%2520By%2520combining%2520the%2520solutions%2520from%2520each%2520component%252C%2520represented%2520as%2520distributions%252C%2520we%2520apply%2520products%2520of%2520experts%2520to%2520implement%2520a%2520project-then-sample%2520strategy.%2520In%2520this%2520strategy%252C%2520the%2520optimality%2520distribution%2520is%2520projected%2520into%2520the%2520feasible%2520area%252C%2520allowing%2520for%2520more%2520efficient%2520sampling.%2520This%2520approach%2520contrasts%2520with%2520the%2520traditional%2520sample-then-project%2520and%2520naive%2520sample-then-reject%2520method%252C%2520leading%2520to%2520more%2520diverse%2520exploration%2520and%2520reducing%2520the%2520accumulation%2520of%2520samples%2520on%2520the%2520boundaries.%2520We%2520demonstrate%2520an%2520effective%2520implementation%2520of%2520this%2520principle%2520using%2520a%2520tensor%2520train-based%2520distribution%2520model%252C%2520which%2520is%2520characterized%2520by%2520its%2520non-parametric%2520nature%252C%2520ease%2520of%2520combination%2520with%2520other%2520distributions%2520at%2520the%2520task%2520level%252C%2520and%2520straightforward%2520sampling%2520technique.%2520We%2520adapt%2520existing%2520tensor%2520train%2520models%2520to%2520suit%2520this%2520purpose%2520and%2520validate%2520the%2520efficacy%2520of%2520our%2520approach%2520through%2520experiments%2520in%2520various%2520tasks%252C%2520including%2520obstacle%2520avoidance%252C%2520non-prehensile%2520manipulation%252C%2520and%2520tasks%2520involving%2520staying%2520in%2520a%2520restricted%2520volume.%2520Our%2520experimental%2520results%2520demonstrate%2520that%2520the%2520proposed%2520method%2520consistently%2520outperforms%2520known%2520baselines%252C%2520providing%2520strong%2520empirical%2520support%2520for%2520its%2520effectiveness.%2520Sample%2520codes%2520for%2520this%2520project%2520are%2520available%2520at%2520https%253A//github.com/idiap/smpc_poe.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.17462v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sampling-Based%20Constrained%20Motion%20Planning%20with%20Products%20of%20Experts&entry.906535625=Amirreza%20Razmjoo%20and%20Teng%20Xue%20and%20Suhan%20Shetty%20and%20Sylvain%20Calinon&entry.1292438233=We%20present%20a%20novel%20approach%20to%20enhance%20the%20performance%20of%20sampling-based%20Model%20Predictive%20Control%20%28MPC%29%20in%20constrained%20optimization%20by%20leveraging%20products%20of%20experts.%20Our%20methodology%20divides%20the%20main%20problem%20into%20two%20components%3A%20one%20focused%20on%20optimality%20and%20the%20other%20on%20feasibility.%20By%20combining%20the%20solutions%20from%20each%20component%2C%20represented%20as%20distributions%2C%20we%20apply%20products%20of%20experts%20to%20implement%20a%20project-then-sample%20strategy.%20In%20this%20strategy%2C%20the%20optimality%20distribution%20is%20projected%20into%20the%20feasible%20area%2C%20allowing%20for%20more%20efficient%20sampling.%20This%20approach%20contrasts%20with%20the%20traditional%20sample-then-project%20and%20naive%20sample-then-reject%20method%2C%20leading%20to%20more%20diverse%20exploration%20and%20reducing%20the%20accumulation%20of%20samples%20on%20the%20boundaries.%20We%20demonstrate%20an%20effective%20implementation%20of%20this%20principle%20using%20a%20tensor%20train-based%20distribution%20model%2C%20which%20is%20characterized%20by%20its%20non-parametric%20nature%2C%20ease%20of%20combination%20with%20other%20distributions%20at%20the%20task%20level%2C%20and%20straightforward%20sampling%20technique.%20We%20adapt%20existing%20tensor%20train%20models%20to%20suit%20this%20purpose%20and%20validate%20the%20efficacy%20of%20our%20approach%20through%20experiments%20in%20various%20tasks%2C%20including%20obstacle%20avoidance%2C%20non-prehensile%20manipulation%2C%20and%20tasks%20involving%20staying%20in%20a%20restricted%20volume.%20Our%20experimental%20results%20demonstrate%20that%20the%20proposed%20method%20consistently%20outperforms%20known%20baselines%2C%20providing%20strong%20empirical%20support%20for%20its%20effectiveness.%20Sample%20codes%20for%20this%20project%20are%20available%20at%20https%3A//github.com/idiap/smpc_poe.&entry.1838667208=http%3A//arxiv.org/abs/2412.17462v2&entry.124074799=Read"},
{"title": "Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing", "author": "Yinzhi Zhao and Ming Wang and Shi Feng and Xiaocui Yang and Daling Wang and Yifei Zhang", "abstract": "Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading model utility. In this work, we examine the decoding process of LLMs and make a key observation: even when successfully jailbroken, models internally exhibit latent safety-related signals during generation. However, these signals are overridden by the model's drive for fluent continuation, preventing timely self-correction or refusal. Building on this observation, we propose a simple yet effective approach that explicitly surfaces and leverages these latent safety signals for early detection of unsafe content during decoding. Experiments across diverse jailbreak attacks demonstrate that our approach significantly enhances safety, while maintaining low over-refusal rates on benign inputs and preserving response quality. Our results suggest that activating intrinsic safety-awareness during decoding offers a promising and complementary direction for defending against jailbreak attacks. Code is available at: https://github.com/zyz13590/SafeProbing.", "link": "http://arxiv.org/abs/2601.10543v1", "date": "2026-01-15", "relevancy": 2.0524, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.52}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.52}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Defending%20Large%20Language%20Models%20Against%20Jailbreak%20Attacks%20via%20In-Decoding%20Safety-Awareness%20Probing&body=Title%3A%20Defending%20Large%20Language%20Models%20Against%20Jailbreak%20Attacks%20via%20In-Decoding%20Safety-Awareness%20Probing%0AAuthor%3A%20Yinzhi%20Zhao%20and%20Ming%20Wang%20and%20Shi%20Feng%20and%20Xiaocui%20Yang%20and%20Daling%20Wang%20and%20Yifei%20Zhang%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20have%20achieved%20impressive%20performance%20across%20natural%20language%20tasks%20and%20are%20increasingly%20deployed%20in%20real-world%20applications.%20Despite%20extensive%20safety%20alignment%20efforts%2C%20recent%20studies%20show%20that%20such%20alignment%20is%20often%20shallow%20and%20remains%20vulnerable%20to%20jailbreak%20attacks.%20Existing%20defense%20mechanisms%2C%20including%20decoding-based%20constraints%20and%20post-hoc%20content%20detectors%2C%20struggle%20against%20sophisticated%20jailbreaks%2C%20often%20intervening%20robust%20detection%20or%20excessively%20degrading%20model%20utility.%20In%20this%20work%2C%20we%20examine%20the%20decoding%20process%20of%20LLMs%20and%20make%20a%20key%20observation%3A%20even%20when%20successfully%20jailbroken%2C%20models%20internally%20exhibit%20latent%20safety-related%20signals%20during%20generation.%20However%2C%20these%20signals%20are%20overridden%20by%20the%20model%27s%20drive%20for%20fluent%20continuation%2C%20preventing%20timely%20self-correction%20or%20refusal.%20Building%20on%20this%20observation%2C%20we%20propose%20a%20simple%20yet%20effective%20approach%20that%20explicitly%20surfaces%20and%20leverages%20these%20latent%20safety%20signals%20for%20early%20detection%20of%20unsafe%20content%20during%20decoding.%20Experiments%20across%20diverse%20jailbreak%20attacks%20demonstrate%20that%20our%20approach%20significantly%20enhances%20safety%2C%20while%20maintaining%20low%20over-refusal%20rates%20on%20benign%20inputs%20and%20preserving%20response%20quality.%20Our%20results%20suggest%20that%20activating%20intrinsic%20safety-awareness%20during%20decoding%20offers%20a%20promising%20and%20complementary%20direction%20for%20defending%20against%20jailbreak%20attacks.%20Code%20is%20available%20at%3A%20https%3A//github.com/zyz13590/SafeProbing.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDefending%2520Large%2520Language%2520Models%2520Against%2520Jailbreak%2520Attacks%2520via%2520In-Decoding%2520Safety-Awareness%2520Probing%26entry.906535625%3DYinzhi%2520Zhao%2520and%2520Ming%2520Wang%2520and%2520Shi%2520Feng%2520and%2520Xiaocui%2520Yang%2520and%2520Daling%2520Wang%2520and%2520Yifei%2520Zhang%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520have%2520achieved%2520impressive%2520performance%2520across%2520natural%2520language%2520tasks%2520and%2520are%2520increasingly%2520deployed%2520in%2520real-world%2520applications.%2520Despite%2520extensive%2520safety%2520alignment%2520efforts%252C%2520recent%2520studies%2520show%2520that%2520such%2520alignment%2520is%2520often%2520shallow%2520and%2520remains%2520vulnerable%2520to%2520jailbreak%2520attacks.%2520Existing%2520defense%2520mechanisms%252C%2520including%2520decoding-based%2520constraints%2520and%2520post-hoc%2520content%2520detectors%252C%2520struggle%2520against%2520sophisticated%2520jailbreaks%252C%2520often%2520intervening%2520robust%2520detection%2520or%2520excessively%2520degrading%2520model%2520utility.%2520In%2520this%2520work%252C%2520we%2520examine%2520the%2520decoding%2520process%2520of%2520LLMs%2520and%2520make%2520a%2520key%2520observation%253A%2520even%2520when%2520successfully%2520jailbroken%252C%2520models%2520internally%2520exhibit%2520latent%2520safety-related%2520signals%2520during%2520generation.%2520However%252C%2520these%2520signals%2520are%2520overridden%2520by%2520the%2520model%2527s%2520drive%2520for%2520fluent%2520continuation%252C%2520preventing%2520timely%2520self-correction%2520or%2520refusal.%2520Building%2520on%2520this%2520observation%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520approach%2520that%2520explicitly%2520surfaces%2520and%2520leverages%2520these%2520latent%2520safety%2520signals%2520for%2520early%2520detection%2520of%2520unsafe%2520content%2520during%2520decoding.%2520Experiments%2520across%2520diverse%2520jailbreak%2520attacks%2520demonstrate%2520that%2520our%2520approach%2520significantly%2520enhances%2520safety%252C%2520while%2520maintaining%2520low%2520over-refusal%2520rates%2520on%2520benign%2520inputs%2520and%2520preserving%2520response%2520quality.%2520Our%2520results%2520suggest%2520that%2520activating%2520intrinsic%2520safety-awareness%2520during%2520decoding%2520offers%2520a%2520promising%2520and%2520complementary%2520direction%2520for%2520defending%2520against%2520jailbreak%2520attacks.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/zyz13590/SafeProbing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Defending%20Large%20Language%20Models%20Against%20Jailbreak%20Attacks%20via%20In-Decoding%20Safety-Awareness%20Probing&entry.906535625=Yinzhi%20Zhao%20and%20Ming%20Wang%20and%20Shi%20Feng%20and%20Xiaocui%20Yang%20and%20Daling%20Wang%20and%20Yifei%20Zhang&entry.1292438233=Large%20language%20models%20%28LLMs%29%20have%20achieved%20impressive%20performance%20across%20natural%20language%20tasks%20and%20are%20increasingly%20deployed%20in%20real-world%20applications.%20Despite%20extensive%20safety%20alignment%20efforts%2C%20recent%20studies%20show%20that%20such%20alignment%20is%20often%20shallow%20and%20remains%20vulnerable%20to%20jailbreak%20attacks.%20Existing%20defense%20mechanisms%2C%20including%20decoding-based%20constraints%20and%20post-hoc%20content%20detectors%2C%20struggle%20against%20sophisticated%20jailbreaks%2C%20often%20intervening%20robust%20detection%20or%20excessively%20degrading%20model%20utility.%20In%20this%20work%2C%20we%20examine%20the%20decoding%20process%20of%20LLMs%20and%20make%20a%20key%20observation%3A%20even%20when%20successfully%20jailbroken%2C%20models%20internally%20exhibit%20latent%20safety-related%20signals%20during%20generation.%20However%2C%20these%20signals%20are%20overridden%20by%20the%20model%27s%20drive%20for%20fluent%20continuation%2C%20preventing%20timely%20self-correction%20or%20refusal.%20Building%20on%20this%20observation%2C%20we%20propose%20a%20simple%20yet%20effective%20approach%20that%20explicitly%20surfaces%20and%20leverages%20these%20latent%20safety%20signals%20for%20early%20detection%20of%20unsafe%20content%20during%20decoding.%20Experiments%20across%20diverse%20jailbreak%20attacks%20demonstrate%20that%20our%20approach%20significantly%20enhances%20safety%2C%20while%20maintaining%20low%20over-refusal%20rates%20on%20benign%20inputs%20and%20preserving%20response%20quality.%20Our%20results%20suggest%20that%20activating%20intrinsic%20safety-awareness%20during%20decoding%20offers%20a%20promising%20and%20complementary%20direction%20for%20defending%20against%20jailbreak%20attacks.%20Code%20is%20available%20at%3A%20https%3A//github.com/zyz13590/SafeProbing.&entry.1838667208=http%3A//arxiv.org/abs/2601.10543v1&entry.124074799=Read"},
{"title": "MoST: Mixing Speech and Text with Modality-Aware Mixture of Experts", "author": "Yuxuan Lou and Kai Yang and Yang You", "abstract": "We present MoST (Mixture of Speech and Text), a novel multimodal large language model that seamlessly integrates speech and text processing through our proposed Modality-Aware Mixture of Experts (MAMoE) architecture. While current multimodal models typically process diverse modality representations with identical parameters, disregarding their inherent representational differences, we introduce specialized routing pathways that direct tokens to modality-appropriate experts based on input type. MAMoE simultaneously enhances modality-specific learning and cross-modal understanding through two complementary components: modality-specific expert groups that capture domain-specific patterns and shared experts that facilitate information transfer between modalities. Building on this architecture, we develop an efficient transformation pipeline that adapts the pretrained MoE language model through strategic post-training on ASR and TTS datasets, followed by fine-tuning with a carefully curated speech-text instruction dataset. A key feature of this pipeline is that it relies exclusively on fully accessible, open-source datasets to achieve strong performance and data efficiency. Comprehensive evaluations across ASR, TTS, audio language modeling, and spoken question answering benchmarks show that MoST consistently outperforms existing models of comparable parameter counts. Our ablation studies confirm that the modality-specific routing mechanism and shared experts design significantly contribute to performance gains across all tested domains. To our knowledge, MoST represents the first fully open-source speech-text LLM built on a Mixture of Experts architecture. \\footnote{We release MoST model, training code, inference code, and training data at https://github.com/NUS-HPC-AI-Lab/MoST", "link": "http://arxiv.org/abs/2601.10272v1", "date": "2026-01-15", "relevancy": 2.0453, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5523}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4837}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4814}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MoST%3A%20Mixing%20Speech%20and%20Text%20with%20Modality-Aware%20Mixture%20of%20Experts&body=Title%3A%20MoST%3A%20Mixing%20Speech%20and%20Text%20with%20Modality-Aware%20Mixture%20of%20Experts%0AAuthor%3A%20Yuxuan%20Lou%20and%20Kai%20Yang%20and%20Yang%20You%0AAbstract%3A%20We%20present%20MoST%20%28Mixture%20of%20Speech%20and%20Text%29%2C%20a%20novel%20multimodal%20large%20language%20model%20that%20seamlessly%20integrates%20speech%20and%20text%20processing%20through%20our%20proposed%20Modality-Aware%20Mixture%20of%20Experts%20%28MAMoE%29%20architecture.%20While%20current%20multimodal%20models%20typically%20process%20diverse%20modality%20representations%20with%20identical%20parameters%2C%20disregarding%20their%20inherent%20representational%20differences%2C%20we%20introduce%20specialized%20routing%20pathways%20that%20direct%20tokens%20to%20modality-appropriate%20experts%20based%20on%20input%20type.%20MAMoE%20simultaneously%20enhances%20modality-specific%20learning%20and%20cross-modal%20understanding%20through%20two%20complementary%20components%3A%20modality-specific%20expert%20groups%20that%20capture%20domain-specific%20patterns%20and%20shared%20experts%20that%20facilitate%20information%20transfer%20between%20modalities.%20Building%20on%20this%20architecture%2C%20we%20develop%20an%20efficient%20transformation%20pipeline%20that%20adapts%20the%20pretrained%20MoE%20language%20model%20through%20strategic%20post-training%20on%20ASR%20and%20TTS%20datasets%2C%20followed%20by%20fine-tuning%20with%20a%20carefully%20curated%20speech-text%20instruction%20dataset.%20A%20key%20feature%20of%20this%20pipeline%20is%20that%20it%20relies%20exclusively%20on%20fully%20accessible%2C%20open-source%20datasets%20to%20achieve%20strong%20performance%20and%20data%20efficiency.%20Comprehensive%20evaluations%20across%20ASR%2C%20TTS%2C%20audio%20language%20modeling%2C%20and%20spoken%20question%20answering%20benchmarks%20show%20that%20MoST%20consistently%20outperforms%20existing%20models%20of%20comparable%20parameter%20counts.%20Our%20ablation%20studies%20confirm%20that%20the%20modality-specific%20routing%20mechanism%20and%20shared%20experts%20design%20significantly%20contribute%20to%20performance%20gains%20across%20all%20tested%20domains.%20To%20our%20knowledge%2C%20MoST%20represents%20the%20first%20fully%20open-source%20speech-text%20LLM%20built%20on%20a%20Mixture%20of%20Experts%20architecture.%20%5Cfootnote%7BWe%20release%20MoST%20model%2C%20training%20code%2C%20inference%20code%2C%20and%20training%20data%20at%20https%3A//github.com/NUS-HPC-AI-Lab/MoST%0ALink%3A%20http%3A//arxiv.org/abs/2601.10272v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMoST%253A%2520Mixing%2520Speech%2520and%2520Text%2520with%2520Modality-Aware%2520Mixture%2520of%2520Experts%26entry.906535625%3DYuxuan%2520Lou%2520and%2520Kai%2520Yang%2520and%2520Yang%2520You%26entry.1292438233%3DWe%2520present%2520MoST%2520%2528Mixture%2520of%2520Speech%2520and%2520Text%2529%252C%2520a%2520novel%2520multimodal%2520large%2520language%2520model%2520that%2520seamlessly%2520integrates%2520speech%2520and%2520text%2520processing%2520through%2520our%2520proposed%2520Modality-Aware%2520Mixture%2520of%2520Experts%2520%2528MAMoE%2529%2520architecture.%2520While%2520current%2520multimodal%2520models%2520typically%2520process%2520diverse%2520modality%2520representations%2520with%2520identical%2520parameters%252C%2520disregarding%2520their%2520inherent%2520representational%2520differences%252C%2520we%2520introduce%2520specialized%2520routing%2520pathways%2520that%2520direct%2520tokens%2520to%2520modality-appropriate%2520experts%2520based%2520on%2520input%2520type.%2520MAMoE%2520simultaneously%2520enhances%2520modality-specific%2520learning%2520and%2520cross-modal%2520understanding%2520through%2520two%2520complementary%2520components%253A%2520modality-specific%2520expert%2520groups%2520that%2520capture%2520domain-specific%2520patterns%2520and%2520shared%2520experts%2520that%2520facilitate%2520information%2520transfer%2520between%2520modalities.%2520Building%2520on%2520this%2520architecture%252C%2520we%2520develop%2520an%2520efficient%2520transformation%2520pipeline%2520that%2520adapts%2520the%2520pretrained%2520MoE%2520language%2520model%2520through%2520strategic%2520post-training%2520on%2520ASR%2520and%2520TTS%2520datasets%252C%2520followed%2520by%2520fine-tuning%2520with%2520a%2520carefully%2520curated%2520speech-text%2520instruction%2520dataset.%2520A%2520key%2520feature%2520of%2520this%2520pipeline%2520is%2520that%2520it%2520relies%2520exclusively%2520on%2520fully%2520accessible%252C%2520open-source%2520datasets%2520to%2520achieve%2520strong%2520performance%2520and%2520data%2520efficiency.%2520Comprehensive%2520evaluations%2520across%2520ASR%252C%2520TTS%252C%2520audio%2520language%2520modeling%252C%2520and%2520spoken%2520question%2520answering%2520benchmarks%2520show%2520that%2520MoST%2520consistently%2520outperforms%2520existing%2520models%2520of%2520comparable%2520parameter%2520counts.%2520Our%2520ablation%2520studies%2520confirm%2520that%2520the%2520modality-specific%2520routing%2520mechanism%2520and%2520shared%2520experts%2520design%2520significantly%2520contribute%2520to%2520performance%2520gains%2520across%2520all%2520tested%2520domains.%2520To%2520our%2520knowledge%252C%2520MoST%2520represents%2520the%2520first%2520fully%2520open-source%2520speech-text%2520LLM%2520built%2520on%2520a%2520Mixture%2520of%2520Experts%2520architecture.%2520%255Cfootnote%257BWe%2520release%2520MoST%2520model%252C%2520training%2520code%252C%2520inference%2520code%252C%2520and%2520training%2520data%2520at%2520https%253A//github.com/NUS-HPC-AI-Lab/MoST%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10272v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoST%3A%20Mixing%20Speech%20and%20Text%20with%20Modality-Aware%20Mixture%20of%20Experts&entry.906535625=Yuxuan%20Lou%20and%20Kai%20Yang%20and%20Yang%20You&entry.1292438233=We%20present%20MoST%20%28Mixture%20of%20Speech%20and%20Text%29%2C%20a%20novel%20multimodal%20large%20language%20model%20that%20seamlessly%20integrates%20speech%20and%20text%20processing%20through%20our%20proposed%20Modality-Aware%20Mixture%20of%20Experts%20%28MAMoE%29%20architecture.%20While%20current%20multimodal%20models%20typically%20process%20diverse%20modality%20representations%20with%20identical%20parameters%2C%20disregarding%20their%20inherent%20representational%20differences%2C%20we%20introduce%20specialized%20routing%20pathways%20that%20direct%20tokens%20to%20modality-appropriate%20experts%20based%20on%20input%20type.%20MAMoE%20simultaneously%20enhances%20modality-specific%20learning%20and%20cross-modal%20understanding%20through%20two%20complementary%20components%3A%20modality-specific%20expert%20groups%20that%20capture%20domain-specific%20patterns%20and%20shared%20experts%20that%20facilitate%20information%20transfer%20between%20modalities.%20Building%20on%20this%20architecture%2C%20we%20develop%20an%20efficient%20transformation%20pipeline%20that%20adapts%20the%20pretrained%20MoE%20language%20model%20through%20strategic%20post-training%20on%20ASR%20and%20TTS%20datasets%2C%20followed%20by%20fine-tuning%20with%20a%20carefully%20curated%20speech-text%20instruction%20dataset.%20A%20key%20feature%20of%20this%20pipeline%20is%20that%20it%20relies%20exclusively%20on%20fully%20accessible%2C%20open-source%20datasets%20to%20achieve%20strong%20performance%20and%20data%20efficiency.%20Comprehensive%20evaluations%20across%20ASR%2C%20TTS%2C%20audio%20language%20modeling%2C%20and%20spoken%20question%20answering%20benchmarks%20show%20that%20MoST%20consistently%20outperforms%20existing%20models%20of%20comparable%20parameter%20counts.%20Our%20ablation%20studies%20confirm%20that%20the%20modality-specific%20routing%20mechanism%20and%20shared%20experts%20design%20significantly%20contribute%20to%20performance%20gains%20across%20all%20tested%20domains.%20To%20our%20knowledge%2C%20MoST%20represents%20the%20first%20fully%20open-source%20speech-text%20LLM%20built%20on%20a%20Mixture%20of%20Experts%20architecture.%20%5Cfootnote%7BWe%20release%20MoST%20model%2C%20training%20code%2C%20inference%20code%2C%20and%20training%20data%20at%20https%3A//github.com/NUS-HPC-AI-Lab/MoST&entry.1838667208=http%3A//arxiv.org/abs/2601.10272v1&entry.124074799=Read"},
{"title": "PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution", "author": "Minghao Yan and Bo Peng and Benjamin Coleman and Ziqi Chen and Zhouhang Xie and Zhankui He and Noveen Sachdeva and Isabella Ye and Weili Wang and Chi Wang and Ed H. Chi and Wang-Cheng Kang and Derek Zhiyuan Cheng and Beidou Wang", "abstract": "Large Language Models (LLMs) have emerged as powerful operators for evolutionary search, yet the design of efficient search scaffolds remains ad hoc. While promising, current LLM-in-the-loop systems lack a systematic approach to managing the evolutionary process. We identify three distinct failure modes: Context Pollution, where experiment history biases future candidate generation; Mode Collapse, where agents stagnate in local minima due to poor exploration-exploitation balance; and Weak Collaboration, where rigid crossover strategies fail to leverage parallel search trajectories effectively. We introduce Progress-Aware Consistent Evolution (PACEvolve), a framework designed to robustly govern the agent's context and search dynamics, to address these challenges. PACEvolve combines hierarchical context management (HCM) with pruning to address context pollution; momentum-based backtracking (MBB) to escape local minima; and a self-adaptive sampling policy that unifies backtracking and crossover for dynamic search coordination (CE), allowing agents to balance internal refinement with cross-trajectory collaboration. We demonstrate that PACEvolve provides a systematic path to consistent, long-horizon self-improvement, achieving state-of-the-art results on LLM-SR and KernelBench, while discovering solutions surpassing the record on Modded NanoGPT.", "link": "http://arxiv.org/abs/2601.10657v1", "date": "2026-01-15", "relevancy": 2.0396, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5117}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5117}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PACEvolve%3A%20Enabling%20Long-Horizon%20Progress-Aware%20Consistent%20Evolution&body=Title%3A%20PACEvolve%3A%20Enabling%20Long-Horizon%20Progress-Aware%20Consistent%20Evolution%0AAuthor%3A%20Minghao%20Yan%20and%20Bo%20Peng%20and%20Benjamin%20Coleman%20and%20Ziqi%20Chen%20and%20Zhouhang%20Xie%20and%20Zhankui%20He%20and%20Noveen%20Sachdeva%20and%20Isabella%20Ye%20and%20Weili%20Wang%20and%20Chi%20Wang%20and%20Ed%20H.%20Chi%20and%20Wang-Cheng%20Kang%20and%20Derek%20Zhiyuan%20Cheng%20and%20Beidou%20Wang%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20powerful%20operators%20for%20evolutionary%20search%2C%20yet%20the%20design%20of%20efficient%20search%20scaffolds%20remains%20ad%20hoc.%20While%20promising%2C%20current%20LLM-in-the-loop%20systems%20lack%20a%20systematic%20approach%20to%20managing%20the%20evolutionary%20process.%20We%20identify%20three%20distinct%20failure%20modes%3A%20Context%20Pollution%2C%20where%20experiment%20history%20biases%20future%20candidate%20generation%3B%20Mode%20Collapse%2C%20where%20agents%20stagnate%20in%20local%20minima%20due%20to%20poor%20exploration-exploitation%20balance%3B%20and%20Weak%20Collaboration%2C%20where%20rigid%20crossover%20strategies%20fail%20to%20leverage%20parallel%20search%20trajectories%20effectively.%20We%20introduce%20Progress-Aware%20Consistent%20Evolution%20%28PACEvolve%29%2C%20a%20framework%20designed%20to%20robustly%20govern%20the%20agent%27s%20context%20and%20search%20dynamics%2C%20to%20address%20these%20challenges.%20PACEvolve%20combines%20hierarchical%20context%20management%20%28HCM%29%20with%20pruning%20to%20address%20context%20pollution%3B%20momentum-based%20backtracking%20%28MBB%29%20to%20escape%20local%20minima%3B%20and%20a%20self-adaptive%20sampling%20policy%20that%20unifies%20backtracking%20and%20crossover%20for%20dynamic%20search%20coordination%20%28CE%29%2C%20allowing%20agents%20to%20balance%20internal%20refinement%20with%20cross-trajectory%20collaboration.%20We%20demonstrate%20that%20PACEvolve%20provides%20a%20systematic%20path%20to%20consistent%2C%20long-horizon%20self-improvement%2C%20achieving%20state-of-the-art%20results%20on%20LLM-SR%20and%20KernelBench%2C%20while%20discovering%20solutions%20surpassing%20the%20record%20on%20Modded%20NanoGPT.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10657v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPACEvolve%253A%2520Enabling%2520Long-Horizon%2520Progress-Aware%2520Consistent%2520Evolution%26entry.906535625%3DMinghao%2520Yan%2520and%2520Bo%2520Peng%2520and%2520Benjamin%2520Coleman%2520and%2520Ziqi%2520Chen%2520and%2520Zhouhang%2520Xie%2520and%2520Zhankui%2520He%2520and%2520Noveen%2520Sachdeva%2520and%2520Isabella%2520Ye%2520and%2520Weili%2520Wang%2520and%2520Chi%2520Wang%2520and%2520Ed%2520H.%2520Chi%2520and%2520Wang-Cheng%2520Kang%2520and%2520Derek%2520Zhiyuan%2520Cheng%2520and%2520Beidou%2520Wang%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520emerged%2520as%2520powerful%2520operators%2520for%2520evolutionary%2520search%252C%2520yet%2520the%2520design%2520of%2520efficient%2520search%2520scaffolds%2520remains%2520ad%2520hoc.%2520While%2520promising%252C%2520current%2520LLM-in-the-loop%2520systems%2520lack%2520a%2520systematic%2520approach%2520to%2520managing%2520the%2520evolutionary%2520process.%2520We%2520identify%2520three%2520distinct%2520failure%2520modes%253A%2520Context%2520Pollution%252C%2520where%2520experiment%2520history%2520biases%2520future%2520candidate%2520generation%253B%2520Mode%2520Collapse%252C%2520where%2520agents%2520stagnate%2520in%2520local%2520minima%2520due%2520to%2520poor%2520exploration-exploitation%2520balance%253B%2520and%2520Weak%2520Collaboration%252C%2520where%2520rigid%2520crossover%2520strategies%2520fail%2520to%2520leverage%2520parallel%2520search%2520trajectories%2520effectively.%2520We%2520introduce%2520Progress-Aware%2520Consistent%2520Evolution%2520%2528PACEvolve%2529%252C%2520a%2520framework%2520designed%2520to%2520robustly%2520govern%2520the%2520agent%2527s%2520context%2520and%2520search%2520dynamics%252C%2520to%2520address%2520these%2520challenges.%2520PACEvolve%2520combines%2520hierarchical%2520context%2520management%2520%2528HCM%2529%2520with%2520pruning%2520to%2520address%2520context%2520pollution%253B%2520momentum-based%2520backtracking%2520%2528MBB%2529%2520to%2520escape%2520local%2520minima%253B%2520and%2520a%2520self-adaptive%2520sampling%2520policy%2520that%2520unifies%2520backtracking%2520and%2520crossover%2520for%2520dynamic%2520search%2520coordination%2520%2528CE%2529%252C%2520allowing%2520agents%2520to%2520balance%2520internal%2520refinement%2520with%2520cross-trajectory%2520collaboration.%2520We%2520demonstrate%2520that%2520PACEvolve%2520provides%2520a%2520systematic%2520path%2520to%2520consistent%252C%2520long-horizon%2520self-improvement%252C%2520achieving%2520state-of-the-art%2520results%2520on%2520LLM-SR%2520and%2520KernelBench%252C%2520while%2520discovering%2520solutions%2520surpassing%2520the%2520record%2520on%2520Modded%2520NanoGPT.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10657v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PACEvolve%3A%20Enabling%20Long-Horizon%20Progress-Aware%20Consistent%20Evolution&entry.906535625=Minghao%20Yan%20and%20Bo%20Peng%20and%20Benjamin%20Coleman%20and%20Ziqi%20Chen%20and%20Zhouhang%20Xie%20and%20Zhankui%20He%20and%20Noveen%20Sachdeva%20and%20Isabella%20Ye%20and%20Weili%20Wang%20and%20Chi%20Wang%20and%20Ed%20H.%20Chi%20and%20Wang-Cheng%20Kang%20and%20Derek%20Zhiyuan%20Cheng%20and%20Beidou%20Wang&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20have%20emerged%20as%20powerful%20operators%20for%20evolutionary%20search%2C%20yet%20the%20design%20of%20efficient%20search%20scaffolds%20remains%20ad%20hoc.%20While%20promising%2C%20current%20LLM-in-the-loop%20systems%20lack%20a%20systematic%20approach%20to%20managing%20the%20evolutionary%20process.%20We%20identify%20three%20distinct%20failure%20modes%3A%20Context%20Pollution%2C%20where%20experiment%20history%20biases%20future%20candidate%20generation%3B%20Mode%20Collapse%2C%20where%20agents%20stagnate%20in%20local%20minima%20due%20to%20poor%20exploration-exploitation%20balance%3B%20and%20Weak%20Collaboration%2C%20where%20rigid%20crossover%20strategies%20fail%20to%20leverage%20parallel%20search%20trajectories%20effectively.%20We%20introduce%20Progress-Aware%20Consistent%20Evolution%20%28PACEvolve%29%2C%20a%20framework%20designed%20to%20robustly%20govern%20the%20agent%27s%20context%20and%20search%20dynamics%2C%20to%20address%20these%20challenges.%20PACEvolve%20combines%20hierarchical%20context%20management%20%28HCM%29%20with%20pruning%20to%20address%20context%20pollution%3B%20momentum-based%20backtracking%20%28MBB%29%20to%20escape%20local%20minima%3B%20and%20a%20self-adaptive%20sampling%20policy%20that%20unifies%20backtracking%20and%20crossover%20for%20dynamic%20search%20coordination%20%28CE%29%2C%20allowing%20agents%20to%20balance%20internal%20refinement%20with%20cross-trajectory%20collaboration.%20We%20demonstrate%20that%20PACEvolve%20provides%20a%20systematic%20path%20to%20consistent%2C%20long-horizon%20self-improvement%2C%20achieving%20state-of-the-art%20results%20on%20LLM-SR%20and%20KernelBench%2C%20while%20discovering%20solutions%20surpassing%20the%20record%20on%20Modded%20NanoGPT.&entry.1838667208=http%3A//arxiv.org/abs/2601.10657v1&entry.124074799=Read"},
{"title": "PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus", "author": "Shahriar Noroozizadeh and Sayantan Kumar and George H. Chen and Jeremy C. Weiss", "abstract": "Clinical narratives encode temporal dynamics essential for modeling patient trajectories, yet large-scale temporally annotated resources are scarce. We introduce PMOA-TTS, a corpus of 124,699 single-patient PubMed Open Access case reports converted into structured textual timelines of (event, time) pairs using a scalable large-language-model pipeline (Llama 3.3 70B and DeepSeek-R1). The corpus comprises over 5.6 million timestamped events, alongside extracted demographics and diagnoses. Technical validation uses a clinician-curated gold set and three measures: semantic event matching, temporal concordance (c-index), and alignment error summarized with Area Under the Log-Time CDF (AULTC). We benchmark alternative prompting and model choices and provide documentation to support reproduction. PMOA-TTS enables research on timeline extraction, temporal reasoning, survival modeling and event forecasting from narrative text, and offers broad diagnostic and demographic coverage. Data and code are openly available in public repositories.", "link": "http://arxiv.org/abs/2505.20323v2", "date": "2026-01-15", "relevancy": 2.0374, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4288}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.3984}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.3952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PMOA-TTS%3A%20Introducing%20the%20PubMed%20Open%20Access%20Textual%20Times%20Series%20Corpus&body=Title%3A%20PMOA-TTS%3A%20Introducing%20the%20PubMed%20Open%20Access%20Textual%20Times%20Series%20Corpus%0AAuthor%3A%20Shahriar%20Noroozizadeh%20and%20Sayantan%20Kumar%20and%20George%20H.%20Chen%20and%20Jeremy%20C.%20Weiss%0AAbstract%3A%20Clinical%20narratives%20encode%20temporal%20dynamics%20essential%20for%20modeling%20patient%20trajectories%2C%20yet%20large-scale%20temporally%20annotated%20resources%20are%20scarce.%20We%20introduce%20PMOA-TTS%2C%20a%20corpus%20of%20124%2C699%20single-patient%20PubMed%20Open%20Access%20case%20reports%20converted%20into%20structured%20textual%20timelines%20of%20%28event%2C%20time%29%20pairs%20using%20a%20scalable%20large-language-model%20pipeline%20%28Llama%203.3%2070B%20and%20DeepSeek-R1%29.%20The%20corpus%20comprises%20over%205.6%20million%20timestamped%20events%2C%20alongside%20extracted%20demographics%20and%20diagnoses.%20Technical%20validation%20uses%20a%20clinician-curated%20gold%20set%20and%20three%20measures%3A%20semantic%20event%20matching%2C%20temporal%20concordance%20%28c-index%29%2C%20and%20alignment%20error%20summarized%20with%20Area%20Under%20the%20Log-Time%20CDF%20%28AULTC%29.%20We%20benchmark%20alternative%20prompting%20and%20model%20choices%20and%20provide%20documentation%20to%20support%20reproduction.%20PMOA-TTS%20enables%20research%20on%20timeline%20extraction%2C%20temporal%20reasoning%2C%20survival%20modeling%20and%20event%20forecasting%20from%20narrative%20text%2C%20and%20offers%20broad%20diagnostic%20and%20demographic%20coverage.%20Data%20and%20code%20are%20openly%20available%20in%20public%20repositories.%0ALink%3A%20http%3A//arxiv.org/abs/2505.20323v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPMOA-TTS%253A%2520Introducing%2520the%2520PubMed%2520Open%2520Access%2520Textual%2520Times%2520Series%2520Corpus%26entry.906535625%3DShahriar%2520Noroozizadeh%2520and%2520Sayantan%2520Kumar%2520and%2520George%2520H.%2520Chen%2520and%2520Jeremy%2520C.%2520Weiss%26entry.1292438233%3DClinical%2520narratives%2520encode%2520temporal%2520dynamics%2520essential%2520for%2520modeling%2520patient%2520trajectories%252C%2520yet%2520large-scale%2520temporally%2520annotated%2520resources%2520are%2520scarce.%2520We%2520introduce%2520PMOA-TTS%252C%2520a%2520corpus%2520of%2520124%252C699%2520single-patient%2520PubMed%2520Open%2520Access%2520case%2520reports%2520converted%2520into%2520structured%2520textual%2520timelines%2520of%2520%2528event%252C%2520time%2529%2520pairs%2520using%2520a%2520scalable%2520large-language-model%2520pipeline%2520%2528Llama%25203.3%252070B%2520and%2520DeepSeek-R1%2529.%2520The%2520corpus%2520comprises%2520over%25205.6%2520million%2520timestamped%2520events%252C%2520alongside%2520extracted%2520demographics%2520and%2520diagnoses.%2520Technical%2520validation%2520uses%2520a%2520clinician-curated%2520gold%2520set%2520and%2520three%2520measures%253A%2520semantic%2520event%2520matching%252C%2520temporal%2520concordance%2520%2528c-index%2529%252C%2520and%2520alignment%2520error%2520summarized%2520with%2520Area%2520Under%2520the%2520Log-Time%2520CDF%2520%2528AULTC%2529.%2520We%2520benchmark%2520alternative%2520prompting%2520and%2520model%2520choices%2520and%2520provide%2520documentation%2520to%2520support%2520reproduction.%2520PMOA-TTS%2520enables%2520research%2520on%2520timeline%2520extraction%252C%2520temporal%2520reasoning%252C%2520survival%2520modeling%2520and%2520event%2520forecasting%2520from%2520narrative%2520text%252C%2520and%2520offers%2520broad%2520diagnostic%2520and%2520demographic%2520coverage.%2520Data%2520and%2520code%2520are%2520openly%2520available%2520in%2520public%2520repositories.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.20323v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PMOA-TTS%3A%20Introducing%20the%20PubMed%20Open%20Access%20Textual%20Times%20Series%20Corpus&entry.906535625=Shahriar%20Noroozizadeh%20and%20Sayantan%20Kumar%20and%20George%20H.%20Chen%20and%20Jeremy%20C.%20Weiss&entry.1292438233=Clinical%20narratives%20encode%20temporal%20dynamics%20essential%20for%20modeling%20patient%20trajectories%2C%20yet%20large-scale%20temporally%20annotated%20resources%20are%20scarce.%20We%20introduce%20PMOA-TTS%2C%20a%20corpus%20of%20124%2C699%20single-patient%20PubMed%20Open%20Access%20case%20reports%20converted%20into%20structured%20textual%20timelines%20of%20%28event%2C%20time%29%20pairs%20using%20a%20scalable%20large-language-model%20pipeline%20%28Llama%203.3%2070B%20and%20DeepSeek-R1%29.%20The%20corpus%20comprises%20over%205.6%20million%20timestamped%20events%2C%20alongside%20extracted%20demographics%20and%20diagnoses.%20Technical%20validation%20uses%20a%20clinician-curated%20gold%20set%20and%20three%20measures%3A%20semantic%20event%20matching%2C%20temporal%20concordance%20%28c-index%29%2C%20and%20alignment%20error%20summarized%20with%20Area%20Under%20the%20Log-Time%20CDF%20%28AULTC%29.%20We%20benchmark%20alternative%20prompting%20and%20model%20choices%20and%20provide%20documentation%20to%20support%20reproduction.%20PMOA-TTS%20enables%20research%20on%20timeline%20extraction%2C%20temporal%20reasoning%2C%20survival%20modeling%20and%20event%20forecasting%20from%20narrative%20text%2C%20and%20offers%20broad%20diagnostic%20and%20demographic%20coverage.%20Data%20and%20code%20are%20openly%20available%20in%20public%20repositories.&entry.1838667208=http%3A//arxiv.org/abs/2505.20323v2&entry.124074799=Read"},
{"title": "Contextual StereoSet: Stress-Testing Bias Alignment Robustness in Large Language Models", "author": "Abhinaba Basu and Pavan Chakraborty", "abstract": "A model that avoids stereotypes in a lab benchmark may not avoid them in deployment. We show that measured bias shifts dramatically when prompts mention different places, times, or audiences -- no adversarial prompting required.\n  We introduce Contextual StereoSet, a benchmark that holds stereotype content fixed while systematically varying contextual framing. Testing 13 models across two protocols, we find striking patterns: anchoring to 1990 (vs. 2030) raises stereotype selection in all models tested on this contrast (p<0.05); gossip framing raises it in 5 of 6 full-grid models; out-group observer framing shifts it by up to 13 percentage points. These effects replicate in hiring, lending, and help-seeking vignettes.\n  We propose Context Sensitivity Fingerprints (CSF): a compact profile of per-dimension dispersion and paired contrasts with bootstrap CIs and FDR correction. Two evaluation tracks support different use cases -- a 360-context diagnostic grid for deep analysis and a budgeted protocol covering 4,229 items for production screening.\n  The implication is methodological: bias scores from fixed-condition tests may not generalize.This is not a claim about ground-truth bias rates; it is a stress test of evaluation robustness. CSF forces evaluators to ask, \"Under what conditions does bias appear?\" rather than \"Is this model biased?\" We release our benchmark, code, and results.", "link": "http://arxiv.org/abs/2601.10460v1", "date": "2026-01-15", "relevancy": 2.0328, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5181}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5181}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4589}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Contextual%20StereoSet%3A%20Stress-Testing%20Bias%20Alignment%20Robustness%20in%20Large%20Language%20Models&body=Title%3A%20Contextual%20StereoSet%3A%20Stress-Testing%20Bias%20Alignment%20Robustness%20in%20Large%20Language%20Models%0AAuthor%3A%20Abhinaba%20Basu%20and%20Pavan%20Chakraborty%0AAbstract%3A%20A%20model%20that%20avoids%20stereotypes%20in%20a%20lab%20benchmark%20may%20not%20avoid%20them%20in%20deployment.%20We%20show%20that%20measured%20bias%20shifts%20dramatically%20when%20prompts%20mention%20different%20places%2C%20times%2C%20or%20audiences%20--%20no%20adversarial%20prompting%20required.%0A%20%20We%20introduce%20Contextual%20StereoSet%2C%20a%20benchmark%20that%20holds%20stereotype%20content%20fixed%20while%20systematically%20varying%20contextual%20framing.%20Testing%2013%20models%20across%20two%20protocols%2C%20we%20find%20striking%20patterns%3A%20anchoring%20to%201990%20%28vs.%202030%29%20raises%20stereotype%20selection%20in%20all%20models%20tested%20on%20this%20contrast%20%28p%3C0.05%29%3B%20gossip%20framing%20raises%20it%20in%205%20of%206%20full-grid%20models%3B%20out-group%20observer%20framing%20shifts%20it%20by%20up%20to%2013%20percentage%20points.%20These%20effects%20replicate%20in%20hiring%2C%20lending%2C%20and%20help-seeking%20vignettes.%0A%20%20We%20propose%20Context%20Sensitivity%20Fingerprints%20%28CSF%29%3A%20a%20compact%20profile%20of%20per-dimension%20dispersion%20and%20paired%20contrasts%20with%20bootstrap%20CIs%20and%20FDR%20correction.%20Two%20evaluation%20tracks%20support%20different%20use%20cases%20--%20a%20360-context%20diagnostic%20grid%20for%20deep%20analysis%20and%20a%20budgeted%20protocol%20covering%204%2C229%20items%20for%20production%20screening.%0A%20%20The%20implication%20is%20methodological%3A%20bias%20scores%20from%20fixed-condition%20tests%20may%20not%20generalize.This%20is%20not%20a%20claim%20about%20ground-truth%20bias%20rates%3B%20it%20is%20a%20stress%20test%20of%20evaluation%20robustness.%20CSF%20forces%20evaluators%20to%20ask%2C%20%22Under%20what%20conditions%20does%20bias%20appear%3F%22%20rather%20than%20%22Is%20this%20model%20biased%3F%22%20We%20release%20our%20benchmark%2C%20code%2C%20and%20results.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10460v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextual%2520StereoSet%253A%2520Stress-Testing%2520Bias%2520Alignment%2520Robustness%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DAbhinaba%2520Basu%2520and%2520Pavan%2520Chakraborty%26entry.1292438233%3DA%2520model%2520that%2520avoids%2520stereotypes%2520in%2520a%2520lab%2520benchmark%2520may%2520not%2520avoid%2520them%2520in%2520deployment.%2520We%2520show%2520that%2520measured%2520bias%2520shifts%2520dramatically%2520when%2520prompts%2520mention%2520different%2520places%252C%2520times%252C%2520or%2520audiences%2520--%2520no%2520adversarial%2520prompting%2520required.%250A%2520%2520We%2520introduce%2520Contextual%2520StereoSet%252C%2520a%2520benchmark%2520that%2520holds%2520stereotype%2520content%2520fixed%2520while%2520systematically%2520varying%2520contextual%2520framing.%2520Testing%252013%2520models%2520across%2520two%2520protocols%252C%2520we%2520find%2520striking%2520patterns%253A%2520anchoring%2520to%25201990%2520%2528vs.%25202030%2529%2520raises%2520stereotype%2520selection%2520in%2520all%2520models%2520tested%2520on%2520this%2520contrast%2520%2528p%253C0.05%2529%253B%2520gossip%2520framing%2520raises%2520it%2520in%25205%2520of%25206%2520full-grid%2520models%253B%2520out-group%2520observer%2520framing%2520shifts%2520it%2520by%2520up%2520to%252013%2520percentage%2520points.%2520These%2520effects%2520replicate%2520in%2520hiring%252C%2520lending%252C%2520and%2520help-seeking%2520vignettes.%250A%2520%2520We%2520propose%2520Context%2520Sensitivity%2520Fingerprints%2520%2528CSF%2529%253A%2520a%2520compact%2520profile%2520of%2520per-dimension%2520dispersion%2520and%2520paired%2520contrasts%2520with%2520bootstrap%2520CIs%2520and%2520FDR%2520correction.%2520Two%2520evaluation%2520tracks%2520support%2520different%2520use%2520cases%2520--%2520a%2520360-context%2520diagnostic%2520grid%2520for%2520deep%2520analysis%2520and%2520a%2520budgeted%2520protocol%2520covering%25204%252C229%2520items%2520for%2520production%2520screening.%250A%2520%2520The%2520implication%2520is%2520methodological%253A%2520bias%2520scores%2520from%2520fixed-condition%2520tests%2520may%2520not%2520generalize.This%2520is%2520not%2520a%2520claim%2520about%2520ground-truth%2520bias%2520rates%253B%2520it%2520is%2520a%2520stress%2520test%2520of%2520evaluation%2520robustness.%2520CSF%2520forces%2520evaluators%2520to%2520ask%252C%2520%2522Under%2520what%2520conditions%2520does%2520bias%2520appear%253F%2522%2520rather%2520than%2520%2522Is%2520this%2520model%2520biased%253F%2522%2520We%2520release%2520our%2520benchmark%252C%2520code%252C%2520and%2520results.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10460v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Contextual%20StereoSet%3A%20Stress-Testing%20Bias%20Alignment%20Robustness%20in%20Large%20Language%20Models&entry.906535625=Abhinaba%20Basu%20and%20Pavan%20Chakraborty&entry.1292438233=A%20model%20that%20avoids%20stereotypes%20in%20a%20lab%20benchmark%20may%20not%20avoid%20them%20in%20deployment.%20We%20show%20that%20measured%20bias%20shifts%20dramatically%20when%20prompts%20mention%20different%20places%2C%20times%2C%20or%20audiences%20--%20no%20adversarial%20prompting%20required.%0A%20%20We%20introduce%20Contextual%20StereoSet%2C%20a%20benchmark%20that%20holds%20stereotype%20content%20fixed%20while%20systematically%20varying%20contextual%20framing.%20Testing%2013%20models%20across%20two%20protocols%2C%20we%20find%20striking%20patterns%3A%20anchoring%20to%201990%20%28vs.%202030%29%20raises%20stereotype%20selection%20in%20all%20models%20tested%20on%20this%20contrast%20%28p%3C0.05%29%3B%20gossip%20framing%20raises%20it%20in%205%20of%206%20full-grid%20models%3B%20out-group%20observer%20framing%20shifts%20it%20by%20up%20to%2013%20percentage%20points.%20These%20effects%20replicate%20in%20hiring%2C%20lending%2C%20and%20help-seeking%20vignettes.%0A%20%20We%20propose%20Context%20Sensitivity%20Fingerprints%20%28CSF%29%3A%20a%20compact%20profile%20of%20per-dimension%20dispersion%20and%20paired%20contrasts%20with%20bootstrap%20CIs%20and%20FDR%20correction.%20Two%20evaluation%20tracks%20support%20different%20use%20cases%20--%20a%20360-context%20diagnostic%20grid%20for%20deep%20analysis%20and%20a%20budgeted%20protocol%20covering%204%2C229%20items%20for%20production%20screening.%0A%20%20The%20implication%20is%20methodological%3A%20bias%20scores%20from%20fixed-condition%20tests%20may%20not%20generalize.This%20is%20not%20a%20claim%20about%20ground-truth%20bias%20rates%3B%20it%20is%20a%20stress%20test%20of%20evaluation%20robustness.%20CSF%20forces%20evaluators%20to%20ask%2C%20%22Under%20what%20conditions%20does%20bias%20appear%3F%22%20rather%20than%20%22Is%20this%20model%20biased%3F%22%20We%20release%20our%20benchmark%2C%20code%2C%20and%20results.&entry.1838667208=http%3A//arxiv.org/abs/2601.10460v1&entry.124074799=Read"},
{"title": "The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load", "author": "Han Jiang and Yao Xiao and Rachel Hurley and Shichao Liu", "abstract": "Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users' prior expertise and interaction strategies through prompting.", "link": "http://arxiv.org/abs/2601.10696v1", "date": "2026-01-15", "relevancy": 2.0318, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5311}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.4942}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Impact%20of%20Generative%20AI%20on%20Architectural%20Conceptual%20Design%3A%20Performance%2C%20Creative%20Self-Efficacy%20and%20Cognitive%20Load&body=Title%3A%20The%20Impact%20of%20Generative%20AI%20on%20Architectural%20Conceptual%20Design%3A%20Performance%2C%20Creative%20Self-Efficacy%20and%20Cognitive%20Load%0AAuthor%3A%20Han%20Jiang%20and%20Yao%20Xiao%20and%20Rachel%20Hurley%20and%20Shichao%20Liu%0AAbstract%3A%20Our%20study%20examines%20how%20generative%20AI%20%28GenAI%29%20influences%20performance%2C%20creative%20self-efficacy%2C%20and%20cognitive%20load%20in%20architectural%20conceptual%20design%20tasks.%20Thirty-six%20student%20participants%20from%20Architectural%20Engineering%20and%20other%20disciplines%20completed%20a%20two-phase%20architectural%20design%20task%2C%20first%20independently%20and%20then%20with%20external%20tools%20%28GenAI-assisted%20condition%20and%20control%20condition%20using%20an%20online%20repository%20of%20existing%20architectural%20projects%29.%20Design%20outcomes%20were%20evaluated%20by%20expert%20raters%2C%20while%20self-efficacy%20and%20cognitive%20load%20were%20self-reported%20after%20each%20phase.%20Difference-in-differences%20analyses%20revealed%20no%20overall%20performance%20advantage%20of%20GenAI%20across%20participants%3B%20however%2C%20subgroup%20analyses%20showed%20that%20GenAI%20significantly%20improved%20design%20performance%20for%20novice%20designers.%20In%20contrast%2C%20general%20creative%20self-efficacy%20declined%20for%20students%20using%20GenAI.%20Cognitive%20load%20did%20not%20differ%20significantly%20between%20conditions%2C%20though%20prompt%20usage%20patterns%20showed%20that%20iterative%20idea%20generation%20and%20visual%20feedback%20prompts%20were%20linked%20to%20greater%20reductions%20in%20cognitive%20load.%20These%20findings%20suggest%20that%20GenAI%20effectiveness%20depends%20on%20users%27%20prior%20expertise%20and%20interaction%20strategies%20through%20prompting.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10696v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Impact%2520of%2520Generative%2520AI%2520on%2520Architectural%2520Conceptual%2520Design%253A%2520Performance%252C%2520Creative%2520Self-Efficacy%2520and%2520Cognitive%2520Load%26entry.906535625%3DHan%2520Jiang%2520and%2520Yao%2520Xiao%2520and%2520Rachel%2520Hurley%2520and%2520Shichao%2520Liu%26entry.1292438233%3DOur%2520study%2520examines%2520how%2520generative%2520AI%2520%2528GenAI%2529%2520influences%2520performance%252C%2520creative%2520self-efficacy%252C%2520and%2520cognitive%2520load%2520in%2520architectural%2520conceptual%2520design%2520tasks.%2520Thirty-six%2520student%2520participants%2520from%2520Architectural%2520Engineering%2520and%2520other%2520disciplines%2520completed%2520a%2520two-phase%2520architectural%2520design%2520task%252C%2520first%2520independently%2520and%2520then%2520with%2520external%2520tools%2520%2528GenAI-assisted%2520condition%2520and%2520control%2520condition%2520using%2520an%2520online%2520repository%2520of%2520existing%2520architectural%2520projects%2529.%2520Design%2520outcomes%2520were%2520evaluated%2520by%2520expert%2520raters%252C%2520while%2520self-efficacy%2520and%2520cognitive%2520load%2520were%2520self-reported%2520after%2520each%2520phase.%2520Difference-in-differences%2520analyses%2520revealed%2520no%2520overall%2520performance%2520advantage%2520of%2520GenAI%2520across%2520participants%253B%2520however%252C%2520subgroup%2520analyses%2520showed%2520that%2520GenAI%2520significantly%2520improved%2520design%2520performance%2520for%2520novice%2520designers.%2520In%2520contrast%252C%2520general%2520creative%2520self-efficacy%2520declined%2520for%2520students%2520using%2520GenAI.%2520Cognitive%2520load%2520did%2520not%2520differ%2520significantly%2520between%2520conditions%252C%2520though%2520prompt%2520usage%2520patterns%2520showed%2520that%2520iterative%2520idea%2520generation%2520and%2520visual%2520feedback%2520prompts%2520were%2520linked%2520to%2520greater%2520reductions%2520in%2520cognitive%2520load.%2520These%2520findings%2520suggest%2520that%2520GenAI%2520effectiveness%2520depends%2520on%2520users%2527%2520prior%2520expertise%2520and%2520interaction%2520strategies%2520through%2520prompting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10696v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Impact%20of%20Generative%20AI%20on%20Architectural%20Conceptual%20Design%3A%20Performance%2C%20Creative%20Self-Efficacy%20and%20Cognitive%20Load&entry.906535625=Han%20Jiang%20and%20Yao%20Xiao%20and%20Rachel%20Hurley%20and%20Shichao%20Liu&entry.1292438233=Our%20study%20examines%20how%20generative%20AI%20%28GenAI%29%20influences%20performance%2C%20creative%20self-efficacy%2C%20and%20cognitive%20load%20in%20architectural%20conceptual%20design%20tasks.%20Thirty-six%20student%20participants%20from%20Architectural%20Engineering%20and%20other%20disciplines%20completed%20a%20two-phase%20architectural%20design%20task%2C%20first%20independently%20and%20then%20with%20external%20tools%20%28GenAI-assisted%20condition%20and%20control%20condition%20using%20an%20online%20repository%20of%20existing%20architectural%20projects%29.%20Design%20outcomes%20were%20evaluated%20by%20expert%20raters%2C%20while%20self-efficacy%20and%20cognitive%20load%20were%20self-reported%20after%20each%20phase.%20Difference-in-differences%20analyses%20revealed%20no%20overall%20performance%20advantage%20of%20GenAI%20across%20participants%3B%20however%2C%20subgroup%20analyses%20showed%20that%20GenAI%20significantly%20improved%20design%20performance%20for%20novice%20designers.%20In%20contrast%2C%20general%20creative%20self-efficacy%20declined%20for%20students%20using%20GenAI.%20Cognitive%20load%20did%20not%20differ%20significantly%20between%20conditions%2C%20though%20prompt%20usage%20patterns%20showed%20that%20iterative%20idea%20generation%20and%20visual%20feedback%20prompts%20were%20linked%20to%20greater%20reductions%20in%20cognitive%20load.%20These%20findings%20suggest%20that%20GenAI%20effectiveness%20depends%20on%20users%27%20prior%20expertise%20and%20interaction%20strategies%20through%20prompting.&entry.1838667208=http%3A//arxiv.org/abs/2601.10696v1&entry.124074799=Read"},
{"title": "Representation-Aware Unlearning via Activation Signatures: From Suppression to Knowledge-Signature Erasure", "author": "Syed Naveed Mahmood and Md. Rezaur Rahman Bhuiyan and Tasfia Zaman and Jareen Tasneem Khondaker and Md. Sameer Sakib and Nazia Tasnim and Farig Sadeque", "abstract": "Selective knowledge erasure from LLMs is critical for GDPR compliance and model safety, yet current unlearning methods conflate behavioral suppression with true knowledge removal, allowing latent capabilities to persist beneath surface-level refusals. In this work, we address this challenge by introducing Knowledge Immunization Framework (KIF), a representation-aware architecture that distinguishes genuine erasure from obfuscation by targeting internal activation signatures rather than surface outputs. Our approach combines dynamic suppression of subject-specific representations with parameter-efficient adaptation, enabling durable unlearning without full model retraining. KIF achieves near-oracle erasure (FQ approx 0.99 vs. 1.00) while preserving utility at oracle levels (MU = 0.62), effectively breaking the stability-erasure tradeoff that has constrained all prior work. We evaluate both standard foundation models (Llama and Mistral) and reasoning-prior models (Qwen and DeepSeek) across 3B to 14B parameters. Our observation shows that standard models exhibit scale-independent true erasure (<3% utility drift), while reasoning-prior models reveal fundamental architectural divergence. Our comprehensive dual-metric evaluation protocol, combining surface-level leakage with latent trace persistence, operationalizes the obfuscation - erasure distinction and enables the first systematic diagnosis of mechanism-level forgetting behavior across model families and scales.", "link": "http://arxiv.org/abs/2601.10566v1", "date": "2026-01-15", "relevancy": 2.0288, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5455}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5205}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4637}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Representation-Aware%20Unlearning%20via%20Activation%20Signatures%3A%20From%20Suppression%20to%20Knowledge-Signature%20Erasure&body=Title%3A%20Representation-Aware%20Unlearning%20via%20Activation%20Signatures%3A%20From%20Suppression%20to%20Knowledge-Signature%20Erasure%0AAuthor%3A%20Syed%20Naveed%20Mahmood%20and%20Md.%20Rezaur%20Rahman%20Bhuiyan%20and%20Tasfia%20Zaman%20and%20Jareen%20Tasneem%20Khondaker%20and%20Md.%20Sameer%20Sakib%20and%20Nazia%20Tasnim%20and%20Farig%20Sadeque%0AAbstract%3A%20Selective%20knowledge%20erasure%20from%20LLMs%20is%20critical%20for%20GDPR%20compliance%20and%20model%20safety%2C%20yet%20current%20unlearning%20methods%20conflate%20behavioral%20suppression%20with%20true%20knowledge%20removal%2C%20allowing%20latent%20capabilities%20to%20persist%20beneath%20surface-level%20refusals.%20In%20this%20work%2C%20we%20address%20this%20challenge%20by%20introducing%20Knowledge%20Immunization%20Framework%20%28KIF%29%2C%20a%20representation-aware%20architecture%20that%20distinguishes%20genuine%20erasure%20from%20obfuscation%20by%20targeting%20internal%20activation%20signatures%20rather%20than%20surface%20outputs.%20Our%20approach%20combines%20dynamic%20suppression%20of%20subject-specific%20representations%20with%20parameter-efficient%20adaptation%2C%20enabling%20durable%20unlearning%20without%20full%20model%20retraining.%20KIF%20achieves%20near-oracle%20erasure%20%28FQ%20approx%200.99%20vs.%201.00%29%20while%20preserving%20utility%20at%20oracle%20levels%20%28MU%20%3D%200.62%29%2C%20effectively%20breaking%20the%20stability-erasure%20tradeoff%20that%20has%20constrained%20all%20prior%20work.%20We%20evaluate%20both%20standard%20foundation%20models%20%28Llama%20and%20Mistral%29%20and%20reasoning-prior%20models%20%28Qwen%20and%20DeepSeek%29%20across%203B%20to%2014B%20parameters.%20Our%20observation%20shows%20that%20standard%20models%20exhibit%20scale-independent%20true%20erasure%20%28%3C3%25%20utility%20drift%29%2C%20while%20reasoning-prior%20models%20reveal%20fundamental%20architectural%20divergence.%20Our%20comprehensive%20dual-metric%20evaluation%20protocol%2C%20combining%20surface-level%20leakage%20with%20latent%20trace%20persistence%2C%20operationalizes%20the%20obfuscation%20-%20erasure%20distinction%20and%20enables%20the%20first%20systematic%20diagnosis%20of%20mechanism-level%20forgetting%20behavior%20across%20model%20families%20and%20scales.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10566v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRepresentation-Aware%2520Unlearning%2520via%2520Activation%2520Signatures%253A%2520From%2520Suppression%2520to%2520Knowledge-Signature%2520Erasure%26entry.906535625%3DSyed%2520Naveed%2520Mahmood%2520and%2520Md.%2520Rezaur%2520Rahman%2520Bhuiyan%2520and%2520Tasfia%2520Zaman%2520and%2520Jareen%2520Tasneem%2520Khondaker%2520and%2520Md.%2520Sameer%2520Sakib%2520and%2520Nazia%2520Tasnim%2520and%2520Farig%2520Sadeque%26entry.1292438233%3DSelective%2520knowledge%2520erasure%2520from%2520LLMs%2520is%2520critical%2520for%2520GDPR%2520compliance%2520and%2520model%2520safety%252C%2520yet%2520current%2520unlearning%2520methods%2520conflate%2520behavioral%2520suppression%2520with%2520true%2520knowledge%2520removal%252C%2520allowing%2520latent%2520capabilities%2520to%2520persist%2520beneath%2520surface-level%2520refusals.%2520In%2520this%2520work%252C%2520we%2520address%2520this%2520challenge%2520by%2520introducing%2520Knowledge%2520Immunization%2520Framework%2520%2528KIF%2529%252C%2520a%2520representation-aware%2520architecture%2520that%2520distinguishes%2520genuine%2520erasure%2520from%2520obfuscation%2520by%2520targeting%2520internal%2520activation%2520signatures%2520rather%2520than%2520surface%2520outputs.%2520Our%2520approach%2520combines%2520dynamic%2520suppression%2520of%2520subject-specific%2520representations%2520with%2520parameter-efficient%2520adaptation%252C%2520enabling%2520durable%2520unlearning%2520without%2520full%2520model%2520retraining.%2520KIF%2520achieves%2520near-oracle%2520erasure%2520%2528FQ%2520approx%25200.99%2520vs.%25201.00%2529%2520while%2520preserving%2520utility%2520at%2520oracle%2520levels%2520%2528MU%2520%253D%25200.62%2529%252C%2520effectively%2520breaking%2520the%2520stability-erasure%2520tradeoff%2520that%2520has%2520constrained%2520all%2520prior%2520work.%2520We%2520evaluate%2520both%2520standard%2520foundation%2520models%2520%2528Llama%2520and%2520Mistral%2529%2520and%2520reasoning-prior%2520models%2520%2528Qwen%2520and%2520DeepSeek%2529%2520across%25203B%2520to%252014B%2520parameters.%2520Our%2520observation%2520shows%2520that%2520standard%2520models%2520exhibit%2520scale-independent%2520true%2520erasure%2520%2528%253C3%2525%2520utility%2520drift%2529%252C%2520while%2520reasoning-prior%2520models%2520reveal%2520fundamental%2520architectural%2520divergence.%2520Our%2520comprehensive%2520dual-metric%2520evaluation%2520protocol%252C%2520combining%2520surface-level%2520leakage%2520with%2520latent%2520trace%2520persistence%252C%2520operationalizes%2520the%2520obfuscation%2520-%2520erasure%2520distinction%2520and%2520enables%2520the%2520first%2520systematic%2520diagnosis%2520of%2520mechanism-level%2520forgetting%2520behavior%2520across%2520model%2520families%2520and%2520scales.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10566v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Representation-Aware%20Unlearning%20via%20Activation%20Signatures%3A%20From%20Suppression%20to%20Knowledge-Signature%20Erasure&entry.906535625=Syed%20Naveed%20Mahmood%20and%20Md.%20Rezaur%20Rahman%20Bhuiyan%20and%20Tasfia%20Zaman%20and%20Jareen%20Tasneem%20Khondaker%20and%20Md.%20Sameer%20Sakib%20and%20Nazia%20Tasnim%20and%20Farig%20Sadeque&entry.1292438233=Selective%20knowledge%20erasure%20from%20LLMs%20is%20critical%20for%20GDPR%20compliance%20and%20model%20safety%2C%20yet%20current%20unlearning%20methods%20conflate%20behavioral%20suppression%20with%20true%20knowledge%20removal%2C%20allowing%20latent%20capabilities%20to%20persist%20beneath%20surface-level%20refusals.%20In%20this%20work%2C%20we%20address%20this%20challenge%20by%20introducing%20Knowledge%20Immunization%20Framework%20%28KIF%29%2C%20a%20representation-aware%20architecture%20that%20distinguishes%20genuine%20erasure%20from%20obfuscation%20by%20targeting%20internal%20activation%20signatures%20rather%20than%20surface%20outputs.%20Our%20approach%20combines%20dynamic%20suppression%20of%20subject-specific%20representations%20with%20parameter-efficient%20adaptation%2C%20enabling%20durable%20unlearning%20without%20full%20model%20retraining.%20KIF%20achieves%20near-oracle%20erasure%20%28FQ%20approx%200.99%20vs.%201.00%29%20while%20preserving%20utility%20at%20oracle%20levels%20%28MU%20%3D%200.62%29%2C%20effectively%20breaking%20the%20stability-erasure%20tradeoff%20that%20has%20constrained%20all%20prior%20work.%20We%20evaluate%20both%20standard%20foundation%20models%20%28Llama%20and%20Mistral%29%20and%20reasoning-prior%20models%20%28Qwen%20and%20DeepSeek%29%20across%203B%20to%2014B%20parameters.%20Our%20observation%20shows%20that%20standard%20models%20exhibit%20scale-independent%20true%20erasure%20%28%3C3%25%20utility%20drift%29%2C%20while%20reasoning-prior%20models%20reveal%20fundamental%20architectural%20divergence.%20Our%20comprehensive%20dual-metric%20evaluation%20protocol%2C%20combining%20surface-level%20leakage%20with%20latent%20trace%20persistence%2C%20operationalizes%20the%20obfuscation%20-%20erasure%20distinction%20and%20enables%20the%20first%20systematic%20diagnosis%20of%20mechanism-level%20forgetting%20behavior%20across%20model%20families%20and%20scales.&entry.1838667208=http%3A//arxiv.org/abs/2601.10566v1&entry.124074799=Read"},
{"title": "Learning Regularization Functionals for Inverse Problems: A Comparative Study", "author": "Johannes Hertrich and Hok Shing Wong and Alexander Denker and Stanislas Ducotterd and Zhenghan Fang and Markus Haltmeier and \u017deljko Kereta and Erich Kobler and Oscar Leong and Mohammad Sadegh Salehi and Carola-Bibiane Sch\u00f6nlieb and Johannes Schwab and Zakhar Shumaylov and Jeremias Sulam and German Sh\u00e2ma Wache and Martin Zach and Yasi Zhang and Matthias J. Ehrhardt and Sebastian Neumayer", "abstract": "In recent years, a variety of learned regularization frameworks for solving inverse problems in imaging have emerged. These offer flexible modeling together with mathematical insights. The proposed methods differ in their architectural design and training strategies, making direct comparison challenging due to non-modular implementations. We address this gap by collecting and unifying the available code into a common framework. This unified view allows us to systematically compare the approaches and highlight their strengths and limitations, providing valuable insights into their future potential. We also provide concise descriptions of each method, complemented by practical guidelines.", "link": "http://arxiv.org/abs/2510.01755v2", "date": "2026-01-15", "relevancy": 2.0245, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5115}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5032}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Regularization%20Functionals%20for%20Inverse%20Problems%3A%20A%20Comparative%20Study&body=Title%3A%20Learning%20Regularization%20Functionals%20for%20Inverse%20Problems%3A%20A%20Comparative%20Study%0AAuthor%3A%20Johannes%20Hertrich%20and%20Hok%20Shing%20Wong%20and%20Alexander%20Denker%20and%20Stanislas%20Ducotterd%20and%20Zhenghan%20Fang%20and%20Markus%20Haltmeier%20and%20%C5%BDeljko%20Kereta%20and%20Erich%20Kobler%20and%20Oscar%20Leong%20and%20Mohammad%20Sadegh%20Salehi%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Johannes%20Schwab%20and%20Zakhar%20Shumaylov%20and%20Jeremias%20Sulam%20and%20German%20Sh%C3%A2ma%20Wache%20and%20Martin%20Zach%20and%20Yasi%20Zhang%20and%20Matthias%20J.%20Ehrhardt%20and%20Sebastian%20Neumayer%0AAbstract%3A%20In%20recent%20years%2C%20a%20variety%20of%20learned%20regularization%20frameworks%20for%20solving%20inverse%20problems%20in%20imaging%20have%20emerged.%20These%20offer%20flexible%20modeling%20together%20with%20mathematical%20insights.%20The%20proposed%20methods%20differ%20in%20their%20architectural%20design%20and%20training%20strategies%2C%20making%20direct%20comparison%20challenging%20due%20to%20non-modular%20implementations.%20We%20address%20this%20gap%20by%20collecting%20and%20unifying%20the%20available%20code%20into%20a%20common%20framework.%20This%20unified%20view%20allows%20us%20to%20systematically%20compare%20the%20approaches%20and%20highlight%20their%20strengths%20and%20limitations%2C%20providing%20valuable%20insights%20into%20their%20future%20potential.%20We%20also%20provide%20concise%20descriptions%20of%20each%20method%2C%20complemented%20by%20practical%20guidelines.%0ALink%3A%20http%3A//arxiv.org/abs/2510.01755v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Regularization%2520Functionals%2520for%2520Inverse%2520Problems%253A%2520A%2520Comparative%2520Study%26entry.906535625%3DJohannes%2520Hertrich%2520and%2520Hok%2520Shing%2520Wong%2520and%2520Alexander%2520Denker%2520and%2520Stanislas%2520Ducotterd%2520and%2520Zhenghan%2520Fang%2520and%2520Markus%2520Haltmeier%2520and%2520%25C5%25BDeljko%2520Kereta%2520and%2520Erich%2520Kobler%2520and%2520Oscar%2520Leong%2520and%2520Mohammad%2520Sadegh%2520Salehi%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Johannes%2520Schwab%2520and%2520Zakhar%2520Shumaylov%2520and%2520Jeremias%2520Sulam%2520and%2520German%2520Sh%25C3%25A2ma%2520Wache%2520and%2520Martin%2520Zach%2520and%2520Yasi%2520Zhang%2520and%2520Matthias%2520J.%2520Ehrhardt%2520and%2520Sebastian%2520Neumayer%26entry.1292438233%3DIn%2520recent%2520years%252C%2520a%2520variety%2520of%2520learned%2520regularization%2520frameworks%2520for%2520solving%2520inverse%2520problems%2520in%2520imaging%2520have%2520emerged.%2520These%2520offer%2520flexible%2520modeling%2520together%2520with%2520mathematical%2520insights.%2520The%2520proposed%2520methods%2520differ%2520in%2520their%2520architectural%2520design%2520and%2520training%2520strategies%252C%2520making%2520direct%2520comparison%2520challenging%2520due%2520to%2520non-modular%2520implementations.%2520We%2520address%2520this%2520gap%2520by%2520collecting%2520and%2520unifying%2520the%2520available%2520code%2520into%2520a%2520common%2520framework.%2520This%2520unified%2520view%2520allows%2520us%2520to%2520systematically%2520compare%2520the%2520approaches%2520and%2520highlight%2520their%2520strengths%2520and%2520limitations%252C%2520providing%2520valuable%2520insights%2520into%2520their%2520future%2520potential.%2520We%2520also%2520provide%2520concise%2520descriptions%2520of%2520each%2520method%252C%2520complemented%2520by%2520practical%2520guidelines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.01755v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Regularization%20Functionals%20for%20Inverse%20Problems%3A%20A%20Comparative%20Study&entry.906535625=Johannes%20Hertrich%20and%20Hok%20Shing%20Wong%20and%20Alexander%20Denker%20and%20Stanislas%20Ducotterd%20and%20Zhenghan%20Fang%20and%20Markus%20Haltmeier%20and%20%C5%BDeljko%20Kereta%20and%20Erich%20Kobler%20and%20Oscar%20Leong%20and%20Mohammad%20Sadegh%20Salehi%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Johannes%20Schwab%20and%20Zakhar%20Shumaylov%20and%20Jeremias%20Sulam%20and%20German%20Sh%C3%A2ma%20Wache%20and%20Martin%20Zach%20and%20Yasi%20Zhang%20and%20Matthias%20J.%20Ehrhardt%20and%20Sebastian%20Neumayer&entry.1292438233=In%20recent%20years%2C%20a%20variety%20of%20learned%20regularization%20frameworks%20for%20solving%20inverse%20problems%20in%20imaging%20have%20emerged.%20These%20offer%20flexible%20modeling%20together%20with%20mathematical%20insights.%20The%20proposed%20methods%20differ%20in%20their%20architectural%20design%20and%20training%20strategies%2C%20making%20direct%20comparison%20challenging%20due%20to%20non-modular%20implementations.%20We%20address%20this%20gap%20by%20collecting%20and%20unifying%20the%20available%20code%20into%20a%20common%20framework.%20This%20unified%20view%20allows%20us%20to%20systematically%20compare%20the%20approaches%20and%20highlight%20their%20strengths%20and%20limitations%2C%20providing%20valuable%20insights%20into%20their%20future%20potential.%20We%20also%20provide%20concise%20descriptions%20of%20each%20method%2C%20complemented%20by%20practical%20guidelines.&entry.1838667208=http%3A//arxiv.org/abs/2510.01755v2&entry.124074799=Read"},
{"title": "Text Classification Under Class Distribution Shift: A Survey", "author": "Adriana Valentina Costache and Silviu Florin Gheorghe and Eduard Gabriel Poesina and Paul Irofti and Radu Tudor Ionescu", "abstract": "The basic underlying assumption of machine learning (ML) models is that the training and test data are sampled from the same distribution. However, in daily practice, this assumption is often broken, i.e. the distribution of the test data changes over time, which hinders the application of conventional ML models. One domain where the distribution shift naturally occurs is text classification, since people always find new topics to discuss. To this end, we survey research articles studying open-set text classification and related tasks. We divide the methods in this area based on the constraints that define the kind of distribution shift and the corresponding problem formulation, i.e. learning with the Universum, zero-shot learning, and open-set learning. We next discuss the predominant mitigation approaches for each problem setup. We further identify several future work directions, aiming to push the boundaries beyond the state of the art. Finally, we explain how continual learning can solve many of the issues caused by the shifting class distribution. We maintain a list of relevant papers at https://github.com/Eduard6421/Open-Set-Survey.", "link": "http://arxiv.org/abs/2502.12965v3", "date": "2026-01-15", "relevancy": 1.7681, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4584}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.435}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Text%20Classification%20Under%20Class%20Distribution%20Shift%3A%20A%20Survey&body=Title%3A%20Text%20Classification%20Under%20Class%20Distribution%20Shift%3A%20A%20Survey%0AAuthor%3A%20Adriana%20Valentina%20Costache%20and%20Silviu%20Florin%20Gheorghe%20and%20Eduard%20Gabriel%20Poesina%20and%20Paul%20Irofti%20and%20Radu%20Tudor%20Ionescu%0AAbstract%3A%20The%20basic%20underlying%20assumption%20of%20machine%20learning%20%28ML%29%20models%20is%20that%20the%20training%20and%20test%20data%20are%20sampled%20from%20the%20same%20distribution.%20However%2C%20in%20daily%20practice%2C%20this%20assumption%20is%20often%20broken%2C%20i.e.%20the%20distribution%20of%20the%20test%20data%20changes%20over%20time%2C%20which%20hinders%20the%20application%20of%20conventional%20ML%20models.%20One%20domain%20where%20the%20distribution%20shift%20naturally%20occurs%20is%20text%20classification%2C%20since%20people%20always%20find%20new%20topics%20to%20discuss.%20To%20this%20end%2C%20we%20survey%20research%20articles%20studying%20open-set%20text%20classification%20and%20related%20tasks.%20We%20divide%20the%20methods%20in%20this%20area%20based%20on%20the%20constraints%20that%20define%20the%20kind%20of%20distribution%20shift%20and%20the%20corresponding%20problem%20formulation%2C%20i.e.%20learning%20with%20the%20Universum%2C%20zero-shot%20learning%2C%20and%20open-set%20learning.%20We%20next%20discuss%20the%20predominant%20mitigation%20approaches%20for%20each%20problem%20setup.%20We%20further%20identify%20several%20future%20work%20directions%2C%20aiming%20to%20push%20the%20boundaries%20beyond%20the%20state%20of%20the%20art.%20Finally%2C%20we%20explain%20how%20continual%20learning%20can%20solve%20many%20of%20the%20issues%20caused%20by%20the%20shifting%20class%20distribution.%20We%20maintain%20a%20list%20of%20relevant%20papers%20at%20https%3A//github.com/Eduard6421/Open-Set-Survey.%0ALink%3A%20http%3A//arxiv.org/abs/2502.12965v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DText%2520Classification%2520Under%2520Class%2520Distribution%2520Shift%253A%2520A%2520Survey%26entry.906535625%3DAdriana%2520Valentina%2520Costache%2520and%2520Silviu%2520Florin%2520Gheorghe%2520and%2520Eduard%2520Gabriel%2520Poesina%2520and%2520Paul%2520Irofti%2520and%2520Radu%2520Tudor%2520Ionescu%26entry.1292438233%3DThe%2520basic%2520underlying%2520assumption%2520of%2520machine%2520learning%2520%2528ML%2529%2520models%2520is%2520that%2520the%2520training%2520and%2520test%2520data%2520are%2520sampled%2520from%2520the%2520same%2520distribution.%2520However%252C%2520in%2520daily%2520practice%252C%2520this%2520assumption%2520is%2520often%2520broken%252C%2520i.e.%2520the%2520distribution%2520of%2520the%2520test%2520data%2520changes%2520over%2520time%252C%2520which%2520hinders%2520the%2520application%2520of%2520conventional%2520ML%2520models.%2520One%2520domain%2520where%2520the%2520distribution%2520shift%2520naturally%2520occurs%2520is%2520text%2520classification%252C%2520since%2520people%2520always%2520find%2520new%2520topics%2520to%2520discuss.%2520To%2520this%2520end%252C%2520we%2520survey%2520research%2520articles%2520studying%2520open-set%2520text%2520classification%2520and%2520related%2520tasks.%2520We%2520divide%2520the%2520methods%2520in%2520this%2520area%2520based%2520on%2520the%2520constraints%2520that%2520define%2520the%2520kind%2520of%2520distribution%2520shift%2520and%2520the%2520corresponding%2520problem%2520formulation%252C%2520i.e.%2520learning%2520with%2520the%2520Universum%252C%2520zero-shot%2520learning%252C%2520and%2520open-set%2520learning.%2520We%2520next%2520discuss%2520the%2520predominant%2520mitigation%2520approaches%2520for%2520each%2520problem%2520setup.%2520We%2520further%2520identify%2520several%2520future%2520work%2520directions%252C%2520aiming%2520to%2520push%2520the%2520boundaries%2520beyond%2520the%2520state%2520of%2520the%2520art.%2520Finally%252C%2520we%2520explain%2520how%2520continual%2520learning%2520can%2520solve%2520many%2520of%2520the%2520issues%2520caused%2520by%2520the%2520shifting%2520class%2520distribution.%2520We%2520maintain%2520a%2520list%2520of%2520relevant%2520papers%2520at%2520https%253A//github.com/Eduard6421/Open-Set-Survey.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12965v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Text%20Classification%20Under%20Class%20Distribution%20Shift%3A%20A%20Survey&entry.906535625=Adriana%20Valentina%20Costache%20and%20Silviu%20Florin%20Gheorghe%20and%20Eduard%20Gabriel%20Poesina%20and%20Paul%20Irofti%20and%20Radu%20Tudor%20Ionescu&entry.1292438233=The%20basic%20underlying%20assumption%20of%20machine%20learning%20%28ML%29%20models%20is%20that%20the%20training%20and%20test%20data%20are%20sampled%20from%20the%20same%20distribution.%20However%2C%20in%20daily%20practice%2C%20this%20assumption%20is%20often%20broken%2C%20i.e.%20the%20distribution%20of%20the%20test%20data%20changes%20over%20time%2C%20which%20hinders%20the%20application%20of%20conventional%20ML%20models.%20One%20domain%20where%20the%20distribution%20shift%20naturally%20occurs%20is%20text%20classification%2C%20since%20people%20always%20find%20new%20topics%20to%20discuss.%20To%20this%20end%2C%20we%20survey%20research%20articles%20studying%20open-set%20text%20classification%20and%20related%20tasks.%20We%20divide%20the%20methods%20in%20this%20area%20based%20on%20the%20constraints%20that%20define%20the%20kind%20of%20distribution%20shift%20and%20the%20corresponding%20problem%20formulation%2C%20i.e.%20learning%20with%20the%20Universum%2C%20zero-shot%20learning%2C%20and%20open-set%20learning.%20We%20next%20discuss%20the%20predominant%20mitigation%20approaches%20for%20each%20problem%20setup.%20We%20further%20identify%20several%20future%20work%20directions%2C%20aiming%20to%20push%20the%20boundaries%20beyond%20the%20state%20of%20the%20art.%20Finally%2C%20we%20explain%20how%20continual%20learning%20can%20solve%20many%20of%20the%20issues%20caused%20by%20the%20shifting%20class%20distribution.%20We%20maintain%20a%20list%20of%20relevant%20papers%20at%20https%3A//github.com/Eduard6421/Open-Set-Survey.&entry.1838667208=http%3A//arxiv.org/abs/2502.12965v3&entry.124074799=Read"},
{"title": "RMBRec: Robust Multi-Behavior Recommendation towards Target Behaviors", "author": "Miaomiao Cai and Zhijie Zhang and Junfeng Fang and Zhiyong Cheng and Xiang Wang and Meng Wang", "abstract": "Multi-behavior recommendation faces a critical challenge in practice: auxiliary behaviors (e.g., clicks, carts) are often noisy, weakly correlated, or semantically misaligned with the target behavior (e.g., purchase), which leads to biased preference learning and suboptimal performance. While existing methods attempt to fuse these heterogeneous signals, they inherently lack a principled mechanism to ensure robustness against such behavioral inconsistency.\n  In this work, we propose Robust Multi-Behavior Recommendation towards Target Behaviors (RMBRec), a robust multi-behavior recommendation framework grounded in an information-theoretic robustness principle. We interpret robustness as a joint process of maximizing predictive information while minimizing its variance across heterogeneous behavioral environments. Under this perspective, the Representation Robustness Module (RRM) enhances local semantic consistency by maximizing the mutual information between users' auxiliary and target representations, whereas the Optimization Robustness Module (ORM) enforces global stability by minimizing the variance of predictive risks across behaviors, which is an efficient approximation to invariant risk minimization. This local-global collaboration bridges representation purification and optimization invariance in a theoretically coherent way. Extensive experiments on three real-world datasets demonstrate that RMBRec not only outperforms state-of-the-art methods in accuracy but also maintains remarkable stability under various noise perturbations. For reproducibility, our code is available at https://github.com/miaomiao-cai2/RMBRec/.", "link": "http://arxiv.org/abs/2601.08705v2", "date": "2026-01-15", "relevancy": 1.4856, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5628}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4935}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RMBRec%3A%20Robust%20Multi-Behavior%20Recommendation%20towards%20Target%20Behaviors&body=Title%3A%20RMBRec%3A%20Robust%20Multi-Behavior%20Recommendation%20towards%20Target%20Behaviors%0AAuthor%3A%20Miaomiao%20Cai%20and%20Zhijie%20Zhang%20and%20Junfeng%20Fang%20and%20Zhiyong%20Cheng%20and%20Xiang%20Wang%20and%20Meng%20Wang%0AAbstract%3A%20Multi-behavior%20recommendation%20faces%20a%20critical%20challenge%20in%20practice%3A%20auxiliary%20behaviors%20%28e.g.%2C%20clicks%2C%20carts%29%20are%20often%20noisy%2C%20weakly%20correlated%2C%20or%20semantically%20misaligned%20with%20the%20target%20behavior%20%28e.g.%2C%20purchase%29%2C%20which%20leads%20to%20biased%20preference%20learning%20and%20suboptimal%20performance.%20While%20existing%20methods%20attempt%20to%20fuse%20these%20heterogeneous%20signals%2C%20they%20inherently%20lack%20a%20principled%20mechanism%20to%20ensure%20robustness%20against%20such%20behavioral%20inconsistency.%0A%20%20In%20this%20work%2C%20we%20propose%20Robust%20Multi-Behavior%20Recommendation%20towards%20Target%20Behaviors%20%28RMBRec%29%2C%20a%20robust%20multi-behavior%20recommendation%20framework%20grounded%20in%20an%20information-theoretic%20robustness%20principle.%20We%20interpret%20robustness%20as%20a%20joint%20process%20of%20maximizing%20predictive%20information%20while%20minimizing%20its%20variance%20across%20heterogeneous%20behavioral%20environments.%20Under%20this%20perspective%2C%20the%20Representation%20Robustness%20Module%20%28RRM%29%20enhances%20local%20semantic%20consistency%20by%20maximizing%20the%20mutual%20information%20between%20users%27%20auxiliary%20and%20target%20representations%2C%20whereas%20the%20Optimization%20Robustness%20Module%20%28ORM%29%20enforces%20global%20stability%20by%20minimizing%20the%20variance%20of%20predictive%20risks%20across%20behaviors%2C%20which%20is%20an%20efficient%20approximation%20to%20invariant%20risk%20minimization.%20This%20local-global%20collaboration%20bridges%20representation%20purification%20and%20optimization%20invariance%20in%20a%20theoretically%20coherent%20way.%20Extensive%20experiments%20on%20three%20real-world%20datasets%20demonstrate%20that%20RMBRec%20not%20only%20outperforms%20state-of-the-art%20methods%20in%20accuracy%20but%20also%20maintains%20remarkable%20stability%20under%20various%20noise%20perturbations.%20For%20reproducibility%2C%20our%20code%20is%20available%20at%20https%3A//github.com/miaomiao-cai2/RMBRec/.%0ALink%3A%20http%3A//arxiv.org/abs/2601.08705v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRMBRec%253A%2520Robust%2520Multi-Behavior%2520Recommendation%2520towards%2520Target%2520Behaviors%26entry.906535625%3DMiaomiao%2520Cai%2520and%2520Zhijie%2520Zhang%2520and%2520Junfeng%2520Fang%2520and%2520Zhiyong%2520Cheng%2520and%2520Xiang%2520Wang%2520and%2520Meng%2520Wang%26entry.1292438233%3DMulti-behavior%2520recommendation%2520faces%2520a%2520critical%2520challenge%2520in%2520practice%253A%2520auxiliary%2520behaviors%2520%2528e.g.%252C%2520clicks%252C%2520carts%2529%2520are%2520often%2520noisy%252C%2520weakly%2520correlated%252C%2520or%2520semantically%2520misaligned%2520with%2520the%2520target%2520behavior%2520%2528e.g.%252C%2520purchase%2529%252C%2520which%2520leads%2520to%2520biased%2520preference%2520learning%2520and%2520suboptimal%2520performance.%2520While%2520existing%2520methods%2520attempt%2520to%2520fuse%2520these%2520heterogeneous%2520signals%252C%2520they%2520inherently%2520lack%2520a%2520principled%2520mechanism%2520to%2520ensure%2520robustness%2520against%2520such%2520behavioral%2520inconsistency.%250A%2520%2520In%2520this%2520work%252C%2520we%2520propose%2520Robust%2520Multi-Behavior%2520Recommendation%2520towards%2520Target%2520Behaviors%2520%2528RMBRec%2529%252C%2520a%2520robust%2520multi-behavior%2520recommendation%2520framework%2520grounded%2520in%2520an%2520information-theoretic%2520robustness%2520principle.%2520We%2520interpret%2520robustness%2520as%2520a%2520joint%2520process%2520of%2520maximizing%2520predictive%2520information%2520while%2520minimizing%2520its%2520variance%2520across%2520heterogeneous%2520behavioral%2520environments.%2520Under%2520this%2520perspective%252C%2520the%2520Representation%2520Robustness%2520Module%2520%2528RRM%2529%2520enhances%2520local%2520semantic%2520consistency%2520by%2520maximizing%2520the%2520mutual%2520information%2520between%2520users%2527%2520auxiliary%2520and%2520target%2520representations%252C%2520whereas%2520the%2520Optimization%2520Robustness%2520Module%2520%2528ORM%2529%2520enforces%2520global%2520stability%2520by%2520minimizing%2520the%2520variance%2520of%2520predictive%2520risks%2520across%2520behaviors%252C%2520which%2520is%2520an%2520efficient%2520approximation%2520to%2520invariant%2520risk%2520minimization.%2520This%2520local-global%2520collaboration%2520bridges%2520representation%2520purification%2520and%2520optimization%2520invariance%2520in%2520a%2520theoretically%2520coherent%2520way.%2520Extensive%2520experiments%2520on%2520three%2520real-world%2520datasets%2520demonstrate%2520that%2520RMBRec%2520not%2520only%2520outperforms%2520state-of-the-art%2520methods%2520in%2520accuracy%2520but%2520also%2520maintains%2520remarkable%2520stability%2520under%2520various%2520noise%2520perturbations.%2520For%2520reproducibility%252C%2520our%2520code%2520is%2520available%2520at%2520https%253A//github.com/miaomiao-cai2/RMBRec/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08705v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RMBRec%3A%20Robust%20Multi-Behavior%20Recommendation%20towards%20Target%20Behaviors&entry.906535625=Miaomiao%20Cai%20and%20Zhijie%20Zhang%20and%20Junfeng%20Fang%20and%20Zhiyong%20Cheng%20and%20Xiang%20Wang%20and%20Meng%20Wang&entry.1292438233=Multi-behavior%20recommendation%20faces%20a%20critical%20challenge%20in%20practice%3A%20auxiliary%20behaviors%20%28e.g.%2C%20clicks%2C%20carts%29%20are%20often%20noisy%2C%20weakly%20correlated%2C%20or%20semantically%20misaligned%20with%20the%20target%20behavior%20%28e.g.%2C%20purchase%29%2C%20which%20leads%20to%20biased%20preference%20learning%20and%20suboptimal%20performance.%20While%20existing%20methods%20attempt%20to%20fuse%20these%20heterogeneous%20signals%2C%20they%20inherently%20lack%20a%20principled%20mechanism%20to%20ensure%20robustness%20against%20such%20behavioral%20inconsistency.%0A%20%20In%20this%20work%2C%20we%20propose%20Robust%20Multi-Behavior%20Recommendation%20towards%20Target%20Behaviors%20%28RMBRec%29%2C%20a%20robust%20multi-behavior%20recommendation%20framework%20grounded%20in%20an%20information-theoretic%20robustness%20principle.%20We%20interpret%20robustness%20as%20a%20joint%20process%20of%20maximizing%20predictive%20information%20while%20minimizing%20its%20variance%20across%20heterogeneous%20behavioral%20environments.%20Under%20this%20perspective%2C%20the%20Representation%20Robustness%20Module%20%28RRM%29%20enhances%20local%20semantic%20consistency%20by%20maximizing%20the%20mutual%20information%20between%20users%27%20auxiliary%20and%20target%20representations%2C%20whereas%20the%20Optimization%20Robustness%20Module%20%28ORM%29%20enforces%20global%20stability%20by%20minimizing%20the%20variance%20of%20predictive%20risks%20across%20behaviors%2C%20which%20is%20an%20efficient%20approximation%20to%20invariant%20risk%20minimization.%20This%20local-global%20collaboration%20bridges%20representation%20purification%20and%20optimization%20invariance%20in%20a%20theoretically%20coherent%20way.%20Extensive%20experiments%20on%20three%20real-world%20datasets%20demonstrate%20that%20RMBRec%20not%20only%20outperforms%20state-of-the-art%20methods%20in%20accuracy%20but%20also%20maintains%20remarkable%20stability%20under%20various%20noise%20perturbations.%20For%20reproducibility%2C%20our%20code%20is%20available%20at%20https%3A//github.com/miaomiao-cai2/RMBRec/.&entry.1838667208=http%3A//arxiv.org/abs/2601.08705v2&entry.124074799=Read"},
{"title": "OctoBench: Benchmarking Scaffold-Aware Instruction Following in Repository-Grounded Agentic Coding", "author": "Deming Ding and Shichun Liu and Enhui Yang and Jiahang Lin and Ziying Chen and Shihan Dou and Honglin Guo and Weiyu Cheng and Pengyu Zhao and Chengjun Xiao and Qunhong Zeng and Qi Zhang and Xuanjing Huang and Qidi Xu and Tao Gui", "abstract": "Modern coding scaffolds turn LLMs into capable software agents, but their ability to follow scaffold-specified instructions remains under-examined, especially when constraints are heterogeneous and persist across interactions. To fill this gap, we introduce OctoBench, which benchmarks scaffold-aware instruction following in repository-grounded agentic coding. OctoBench includes 34 environments and 217 tasks instantiated under three scaffold types, and is paired with 7,098 objective checklist items. To disentangle solving the task from following the rules, we provide an automated observation-and-scoring toolkit that captures full trajectories and performs fine-grained checks. Experiments on eight representative models reveal a systematic gap between task-solving and scaffold-aware compliance, underscoring the need for training and evaluation that explicitly targets heterogeneous instruction following. We release the benchmark to support reproducible benchmarking and to accelerate the development of more scaffold-aware coding agents.", "link": "http://arxiv.org/abs/2601.10343v1", "date": "2026-01-15", "relevancy": 1.4707, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5241}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4969}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OctoBench%3A%20Benchmarking%20Scaffold-Aware%20Instruction%20Following%20in%20Repository-Grounded%20Agentic%20Coding&body=Title%3A%20OctoBench%3A%20Benchmarking%20Scaffold-Aware%20Instruction%20Following%20in%20Repository-Grounded%20Agentic%20Coding%0AAuthor%3A%20Deming%20Ding%20and%20Shichun%20Liu%20and%20Enhui%20Yang%20and%20Jiahang%20Lin%20and%20Ziying%20Chen%20and%20Shihan%20Dou%20and%20Honglin%20Guo%20and%20Weiyu%20Cheng%20and%20Pengyu%20Zhao%20and%20Chengjun%20Xiao%20and%20Qunhong%20Zeng%20and%20Qi%20Zhang%20and%20Xuanjing%20Huang%20and%20Qidi%20Xu%20and%20Tao%20Gui%0AAbstract%3A%20Modern%20coding%20scaffolds%20turn%20LLMs%20into%20capable%20software%20agents%2C%20but%20their%20ability%20to%20follow%20scaffold-specified%20instructions%20remains%20under-examined%2C%20especially%20when%20constraints%20are%20heterogeneous%20and%20persist%20across%20interactions.%20To%20fill%20this%20gap%2C%20we%20introduce%20OctoBench%2C%20which%20benchmarks%20scaffold-aware%20instruction%20following%20in%20repository-grounded%20agentic%20coding.%20OctoBench%20includes%2034%20environments%20and%20217%20tasks%20instantiated%20under%20three%20scaffold%20types%2C%20and%20is%20paired%20with%207%2C098%20objective%20checklist%20items.%20To%20disentangle%20solving%20the%20task%20from%20following%20the%20rules%2C%20we%20provide%20an%20automated%20observation-and-scoring%20toolkit%20that%20captures%20full%20trajectories%20and%20performs%20fine-grained%20checks.%20Experiments%20on%20eight%20representative%20models%20reveal%20a%20systematic%20gap%20between%20task-solving%20and%20scaffold-aware%20compliance%2C%20underscoring%20the%20need%20for%20training%20and%20evaluation%20that%20explicitly%20targets%20heterogeneous%20instruction%20following.%20We%20release%20the%20benchmark%20to%20support%20reproducible%20benchmarking%20and%20to%20accelerate%20the%20development%20of%20more%20scaffold-aware%20coding%20agents.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10343v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOctoBench%253A%2520Benchmarking%2520Scaffold-Aware%2520Instruction%2520Following%2520in%2520Repository-Grounded%2520Agentic%2520Coding%26entry.906535625%3DDeming%2520Ding%2520and%2520Shichun%2520Liu%2520and%2520Enhui%2520Yang%2520and%2520Jiahang%2520Lin%2520and%2520Ziying%2520Chen%2520and%2520Shihan%2520Dou%2520and%2520Honglin%2520Guo%2520and%2520Weiyu%2520Cheng%2520and%2520Pengyu%2520Zhao%2520and%2520Chengjun%2520Xiao%2520and%2520Qunhong%2520Zeng%2520and%2520Qi%2520Zhang%2520and%2520Xuanjing%2520Huang%2520and%2520Qidi%2520Xu%2520and%2520Tao%2520Gui%26entry.1292438233%3DModern%2520coding%2520scaffolds%2520turn%2520LLMs%2520into%2520capable%2520software%2520agents%252C%2520but%2520their%2520ability%2520to%2520follow%2520scaffold-specified%2520instructions%2520remains%2520under-examined%252C%2520especially%2520when%2520constraints%2520are%2520heterogeneous%2520and%2520persist%2520across%2520interactions.%2520To%2520fill%2520this%2520gap%252C%2520we%2520introduce%2520OctoBench%252C%2520which%2520benchmarks%2520scaffold-aware%2520instruction%2520following%2520in%2520repository-grounded%2520agentic%2520coding.%2520OctoBench%2520includes%252034%2520environments%2520and%2520217%2520tasks%2520instantiated%2520under%2520three%2520scaffold%2520types%252C%2520and%2520is%2520paired%2520with%25207%252C098%2520objective%2520checklist%2520items.%2520To%2520disentangle%2520solving%2520the%2520task%2520from%2520following%2520the%2520rules%252C%2520we%2520provide%2520an%2520automated%2520observation-and-scoring%2520toolkit%2520that%2520captures%2520full%2520trajectories%2520and%2520performs%2520fine-grained%2520checks.%2520Experiments%2520on%2520eight%2520representative%2520models%2520reveal%2520a%2520systematic%2520gap%2520between%2520task-solving%2520and%2520scaffold-aware%2520compliance%252C%2520underscoring%2520the%2520need%2520for%2520training%2520and%2520evaluation%2520that%2520explicitly%2520targets%2520heterogeneous%2520instruction%2520following.%2520We%2520release%2520the%2520benchmark%2520to%2520support%2520reproducible%2520benchmarking%2520and%2520to%2520accelerate%2520the%2520development%2520of%2520more%2520scaffold-aware%2520coding%2520agents.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10343v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OctoBench%3A%20Benchmarking%20Scaffold-Aware%20Instruction%20Following%20in%20Repository-Grounded%20Agentic%20Coding&entry.906535625=Deming%20Ding%20and%20Shichun%20Liu%20and%20Enhui%20Yang%20and%20Jiahang%20Lin%20and%20Ziying%20Chen%20and%20Shihan%20Dou%20and%20Honglin%20Guo%20and%20Weiyu%20Cheng%20and%20Pengyu%20Zhao%20and%20Chengjun%20Xiao%20and%20Qunhong%20Zeng%20and%20Qi%20Zhang%20and%20Xuanjing%20Huang%20and%20Qidi%20Xu%20and%20Tao%20Gui&entry.1292438233=Modern%20coding%20scaffolds%20turn%20LLMs%20into%20capable%20software%20agents%2C%20but%20their%20ability%20to%20follow%20scaffold-specified%20instructions%20remains%20under-examined%2C%20especially%20when%20constraints%20are%20heterogeneous%20and%20persist%20across%20interactions.%20To%20fill%20this%20gap%2C%20we%20introduce%20OctoBench%2C%20which%20benchmarks%20scaffold-aware%20instruction%20following%20in%20repository-grounded%20agentic%20coding.%20OctoBench%20includes%2034%20environments%20and%20217%20tasks%20instantiated%20under%20three%20scaffold%20types%2C%20and%20is%20paired%20with%207%2C098%20objective%20checklist%20items.%20To%20disentangle%20solving%20the%20task%20from%20following%20the%20rules%2C%20we%20provide%20an%20automated%20observation-and-scoring%20toolkit%20that%20captures%20full%20trajectories%20and%20performs%20fine-grained%20checks.%20Experiments%20on%20eight%20representative%20models%20reveal%20a%20systematic%20gap%20between%20task-solving%20and%20scaffold-aware%20compliance%2C%20underscoring%20the%20need%20for%20training%20and%20evaluation%20that%20explicitly%20targets%20heterogeneous%20instruction%20following.%20We%20release%20the%20benchmark%20to%20support%20reproducible%20benchmarking%20and%20to%20accelerate%20the%20development%20of%20more%20scaffold-aware%20coding%20agents.&entry.1838667208=http%3A//arxiv.org/abs/2601.10343v1&entry.124074799=Read"},
{"title": "Parallel Test-Time Scaling for Latent Reasoning Models", "author": "Runyang You and Yongqi Li and Meng Liu and Wenjie Wang and Liqiang Nie and Wenjie Li", "abstract": "Parallel test-time scaling (TTS) is a pivotal approach for enhancing large language models (LLMs), typically by sampling multiple token-based chains-of-thought in parallel and aggregating outcomes through voting or search. Recent advances in latent reasoning, where intermediate reasoning unfolds in continuous vector spaces, offer a more efficient alternative to explicit Chain-of-Thought, yet whether such latent models can similarly benefit from parallel TTS remains open, mainly due to the absence of sampling mechanisms in continuous space, and the lack of probabilistic signals for advanced trajectory aggregation. This work enables parallel TTS for latent reasoning models by addressing the above issues. For sampling, we introduce two uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM) trained with step-wise contrastive objective to score and guide latent reasoning. Extensive experiments and visualization analyses show that both sampling strategies scale effectively with compute and exhibit distinct exploration dynamics, while LatentRM enables effective trajectory selection. Together, our explorations open a new direction for scalable inference in continuous spaces. Code and checkpoints released at https://github.com/ModalityDance/LatentTTS", "link": "http://arxiv.org/abs/2510.07745v3", "date": "2026-01-15", "relevancy": 1.5897, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5348}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5337}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5264}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Parallel%20Test-Time%20Scaling%20for%20Latent%20Reasoning%20Models&body=Title%3A%20Parallel%20Test-Time%20Scaling%20for%20Latent%20Reasoning%20Models%0AAuthor%3A%20Runyang%20You%20and%20Yongqi%20Li%20and%20Meng%20Liu%20and%20Wenjie%20Wang%20and%20Liqiang%20Nie%20and%20Wenjie%20Li%0AAbstract%3A%20Parallel%20test-time%20scaling%20%28TTS%29%20is%20a%20pivotal%20approach%20for%20enhancing%20large%20language%20models%20%28LLMs%29%2C%20typically%20by%20sampling%20multiple%20token-based%20chains-of-thought%20in%20parallel%20and%20aggregating%20outcomes%20through%20voting%20or%20search.%20Recent%20advances%20in%20latent%20reasoning%2C%20where%20intermediate%20reasoning%20unfolds%20in%20continuous%20vector%20spaces%2C%20offer%20a%20more%20efficient%20alternative%20to%20explicit%20Chain-of-Thought%2C%20yet%20whether%20such%20latent%20models%20can%20similarly%20benefit%20from%20parallel%20TTS%20remains%20open%2C%20mainly%20due%20to%20the%20absence%20of%20sampling%20mechanisms%20in%20continuous%20space%2C%20and%20the%20lack%20of%20probabilistic%20signals%20for%20advanced%20trajectory%20aggregation.%20This%20work%20enables%20parallel%20TTS%20for%20latent%20reasoning%20models%20by%20addressing%20the%20above%20issues.%20For%20sampling%2C%20we%20introduce%20two%20uncertainty-inspired%20stochastic%20strategies%3A%20Monte%20Carlo%20Dropout%20and%20Additive%20Gaussian%20Noise.%20For%20aggregation%2C%20we%20design%20a%20Latent%20Reward%20Model%20%28LatentRM%29%20trained%20with%20step-wise%20contrastive%20objective%20to%20score%20and%20guide%20latent%20reasoning.%20Extensive%20experiments%20and%20visualization%20analyses%20show%20that%20both%20sampling%20strategies%20scale%20effectively%20with%20compute%20and%20exhibit%20distinct%20exploration%20dynamics%2C%20while%20LatentRM%20enables%20effective%20trajectory%20selection.%20Together%2C%20our%20explorations%20open%20a%20new%20direction%20for%20scalable%20inference%20in%20continuous%20spaces.%20Code%20and%20checkpoints%20released%20at%20https%3A//github.com/ModalityDance/LatentTTS%0ALink%3A%20http%3A//arxiv.org/abs/2510.07745v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DParallel%2520Test-Time%2520Scaling%2520for%2520Latent%2520Reasoning%2520Models%26entry.906535625%3DRunyang%2520You%2520and%2520Yongqi%2520Li%2520and%2520Meng%2520Liu%2520and%2520Wenjie%2520Wang%2520and%2520Liqiang%2520Nie%2520and%2520Wenjie%2520Li%26entry.1292438233%3DParallel%2520test-time%2520scaling%2520%2528TTS%2529%2520is%2520a%2520pivotal%2520approach%2520for%2520enhancing%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520typically%2520by%2520sampling%2520multiple%2520token-based%2520chains-of-thought%2520in%2520parallel%2520and%2520aggregating%2520outcomes%2520through%2520voting%2520or%2520search.%2520Recent%2520advances%2520in%2520latent%2520reasoning%252C%2520where%2520intermediate%2520reasoning%2520unfolds%2520in%2520continuous%2520vector%2520spaces%252C%2520offer%2520a%2520more%2520efficient%2520alternative%2520to%2520explicit%2520Chain-of-Thought%252C%2520yet%2520whether%2520such%2520latent%2520models%2520can%2520similarly%2520benefit%2520from%2520parallel%2520TTS%2520remains%2520open%252C%2520mainly%2520due%2520to%2520the%2520absence%2520of%2520sampling%2520mechanisms%2520in%2520continuous%2520space%252C%2520and%2520the%2520lack%2520of%2520probabilistic%2520signals%2520for%2520advanced%2520trajectory%2520aggregation.%2520This%2520work%2520enables%2520parallel%2520TTS%2520for%2520latent%2520reasoning%2520models%2520by%2520addressing%2520the%2520above%2520issues.%2520For%2520sampling%252C%2520we%2520introduce%2520two%2520uncertainty-inspired%2520stochastic%2520strategies%253A%2520Monte%2520Carlo%2520Dropout%2520and%2520Additive%2520Gaussian%2520Noise.%2520For%2520aggregation%252C%2520we%2520design%2520a%2520Latent%2520Reward%2520Model%2520%2528LatentRM%2529%2520trained%2520with%2520step-wise%2520contrastive%2520objective%2520to%2520score%2520and%2520guide%2520latent%2520reasoning.%2520Extensive%2520experiments%2520and%2520visualization%2520analyses%2520show%2520that%2520both%2520sampling%2520strategies%2520scale%2520effectively%2520with%2520compute%2520and%2520exhibit%2520distinct%2520exploration%2520dynamics%252C%2520while%2520LatentRM%2520enables%2520effective%2520trajectory%2520selection.%2520Together%252C%2520our%2520explorations%2520open%2520a%2520new%2520direction%2520for%2520scalable%2520inference%2520in%2520continuous%2520spaces.%2520Code%2520and%2520checkpoints%2520released%2520at%2520https%253A//github.com/ModalityDance/LatentTTS%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.07745v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Parallel%20Test-Time%20Scaling%20for%20Latent%20Reasoning%20Models&entry.906535625=Runyang%20You%20and%20Yongqi%20Li%20and%20Meng%20Liu%20and%20Wenjie%20Wang%20and%20Liqiang%20Nie%20and%20Wenjie%20Li&entry.1292438233=Parallel%20test-time%20scaling%20%28TTS%29%20is%20a%20pivotal%20approach%20for%20enhancing%20large%20language%20models%20%28LLMs%29%2C%20typically%20by%20sampling%20multiple%20token-based%20chains-of-thought%20in%20parallel%20and%20aggregating%20outcomes%20through%20voting%20or%20search.%20Recent%20advances%20in%20latent%20reasoning%2C%20where%20intermediate%20reasoning%20unfolds%20in%20continuous%20vector%20spaces%2C%20offer%20a%20more%20efficient%20alternative%20to%20explicit%20Chain-of-Thought%2C%20yet%20whether%20such%20latent%20models%20can%20similarly%20benefit%20from%20parallel%20TTS%20remains%20open%2C%20mainly%20due%20to%20the%20absence%20of%20sampling%20mechanisms%20in%20continuous%20space%2C%20and%20the%20lack%20of%20probabilistic%20signals%20for%20advanced%20trajectory%20aggregation.%20This%20work%20enables%20parallel%20TTS%20for%20latent%20reasoning%20models%20by%20addressing%20the%20above%20issues.%20For%20sampling%2C%20we%20introduce%20two%20uncertainty-inspired%20stochastic%20strategies%3A%20Monte%20Carlo%20Dropout%20and%20Additive%20Gaussian%20Noise.%20For%20aggregation%2C%20we%20design%20a%20Latent%20Reward%20Model%20%28LatentRM%29%20trained%20with%20step-wise%20contrastive%20objective%20to%20score%20and%20guide%20latent%20reasoning.%20Extensive%20experiments%20and%20visualization%20analyses%20show%20that%20both%20sampling%20strategies%20scale%20effectively%20with%20compute%20and%20exhibit%20distinct%20exploration%20dynamics%2C%20while%20LatentRM%20enables%20effective%20trajectory%20selection.%20Together%2C%20our%20explorations%20open%20a%20new%20direction%20for%20scalable%20inference%20in%20continuous%20spaces.%20Code%20and%20checkpoints%20released%20at%20https%3A//github.com/ModalityDance/LatentTTS&entry.1838667208=http%3A//arxiv.org/abs/2510.07745v3&entry.124074799=Read"},
{"title": "Are Language Models Models?", "author": "Philip Resnik", "abstract": "Futrell and Mahowald claim LMs \"serve as model systems\", but an assessment at each of Marr's three levels suggests the claim is clearly not true at the implementation level, poorly motivated at the algorithmic-representational level, and problematic at the computational theory level. LMs are good candidates as tools; calling them cognitive models overstates the case and unnecessarily feeds LLM hype.", "link": "http://arxiv.org/abs/2601.10421v1", "date": "2026-01-15", "relevancy": 2.0147, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4132}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4132}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Are%20Language%20Models%20Models%3F&body=Title%3A%20Are%20Language%20Models%20Models%3F%0AAuthor%3A%20Philip%20Resnik%0AAbstract%3A%20Futrell%20and%20Mahowald%20claim%20LMs%20%22serve%20as%20model%20systems%22%2C%20but%20an%20assessment%20at%20each%20of%20Marr%27s%20three%20levels%20suggests%20the%20claim%20is%20clearly%20not%20true%20at%20the%20implementation%20level%2C%20poorly%20motivated%20at%20the%20algorithmic-representational%20level%2C%20and%20problematic%20at%20the%20computational%20theory%20level.%20LMs%20are%20good%20candidates%20as%20tools%3B%20calling%20them%20cognitive%20models%20overstates%20the%20case%20and%20unnecessarily%20feeds%20LLM%20hype.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10421v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAre%2520Language%2520Models%2520Models%253F%26entry.906535625%3DPhilip%2520Resnik%26entry.1292438233%3DFutrell%2520and%2520Mahowald%2520claim%2520LMs%2520%2522serve%2520as%2520model%2520systems%2522%252C%2520but%2520an%2520assessment%2520at%2520each%2520of%2520Marr%2527s%2520three%2520levels%2520suggests%2520the%2520claim%2520is%2520clearly%2520not%2520true%2520at%2520the%2520implementation%2520level%252C%2520poorly%2520motivated%2520at%2520the%2520algorithmic-representational%2520level%252C%2520and%2520problematic%2520at%2520the%2520computational%2520theory%2520level.%2520LMs%2520are%2520good%2520candidates%2520as%2520tools%253B%2520calling%2520them%2520cognitive%2520models%2520overstates%2520the%2520case%2520and%2520unnecessarily%2520feeds%2520LLM%2520hype.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10421v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Are%20Language%20Models%20Models%3F&entry.906535625=Philip%20Resnik&entry.1292438233=Futrell%20and%20Mahowald%20claim%20LMs%20%22serve%20as%20model%20systems%22%2C%20but%20an%20assessment%20at%20each%20of%20Marr%27s%20three%20levels%20suggests%20the%20claim%20is%20clearly%20not%20true%20at%20the%20implementation%20level%2C%20poorly%20motivated%20at%20the%20algorithmic-representational%20level%2C%20and%20problematic%20at%20the%20computational%20theory%20level.%20LMs%20are%20good%20candidates%20as%20tools%3B%20calling%20them%20cognitive%20models%20overstates%20the%20case%20and%20unnecessarily%20feeds%20LLM%20hype.&entry.1838667208=http%3A//arxiv.org/abs/2601.10421v1&entry.124074799=Read"},
{"title": "How Quantization Shapes Bias in Large Language Models", "author": "Federico Marcuzzi and Xuefei Ning and Roy Schwartz and Iryna Gurevych", "abstract": "This work presents a comprehensive evaluation of how quantization affects model bias, with particular attention to its impact on individual demographic subgroups. We focus on weight and activation quantization strategies and examine their effects across a broad range of bias types, including stereotypes, fairness, toxicity, and sentiment. We employ both probability- and generated text-based metrics across 13 benchmarks and evaluate models that differ in architecture family and reasoning ability. Our findings show that quantization has a nuanced impact on bias: while it can reduce model toxicity and does not significantly impact sentiment, it tends to slightly increase stereotypes and unfairness in generative tasks, especially under aggressive compression. These trends are generally consistent across demographic categories and subgroups, and model types, although their magnitude depends on the specific setting. Overall, our results highlight the importance of carefully balancing efficiency and ethical considerations when applying quantization in practice.", "link": "http://arxiv.org/abs/2508.18088v2", "date": "2026-01-15", "relevancy": 0.9288, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.495}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4542}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20How%20Quantization%20Shapes%20Bias%20in%20Large%20Language%20Models&body=Title%3A%20How%20Quantization%20Shapes%20Bias%20in%20Large%20Language%20Models%0AAuthor%3A%20Federico%20Marcuzzi%20and%20Xuefei%20Ning%20and%20Roy%20Schwartz%20and%20Iryna%20Gurevych%0AAbstract%3A%20This%20work%20presents%20a%20comprehensive%20evaluation%20of%20how%20quantization%20affects%20model%20bias%2C%20with%20particular%20attention%20to%20its%20impact%20on%20individual%20demographic%20subgroups.%20We%20focus%20on%20weight%20and%20activation%20quantization%20strategies%20and%20examine%20their%20effects%20across%20a%20broad%20range%20of%20bias%20types%2C%20including%20stereotypes%2C%20fairness%2C%20toxicity%2C%20and%20sentiment.%20We%20employ%20both%20probability-%20and%20generated%20text-based%20metrics%20across%2013%20benchmarks%20and%20evaluate%20models%20that%20differ%20in%20architecture%20family%20and%20reasoning%20ability.%20Our%20findings%20show%20that%20quantization%20has%20a%20nuanced%20impact%20on%20bias%3A%20while%20it%20can%20reduce%20model%20toxicity%20and%20does%20not%20significantly%20impact%20sentiment%2C%20it%20tends%20to%20slightly%20increase%20stereotypes%20and%20unfairness%20in%20generative%20tasks%2C%20especially%20under%20aggressive%20compression.%20These%20trends%20are%20generally%20consistent%20across%20demographic%20categories%20and%20subgroups%2C%20and%20model%20types%2C%20although%20their%20magnitude%20depends%20on%20the%20specific%20setting.%20Overall%2C%20our%20results%20highlight%20the%20importance%20of%20carefully%20balancing%20efficiency%20and%20ethical%20considerations%20when%20applying%20quantization%20in%20practice.%0ALink%3A%20http%3A//arxiv.org/abs/2508.18088v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHow%2520Quantization%2520Shapes%2520Bias%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DFederico%2520Marcuzzi%2520and%2520Xuefei%2520Ning%2520and%2520Roy%2520Schwartz%2520and%2520Iryna%2520Gurevych%26entry.1292438233%3DThis%2520work%2520presents%2520a%2520comprehensive%2520evaluation%2520of%2520how%2520quantization%2520affects%2520model%2520bias%252C%2520with%2520particular%2520attention%2520to%2520its%2520impact%2520on%2520individual%2520demographic%2520subgroups.%2520We%2520focus%2520on%2520weight%2520and%2520activation%2520quantization%2520strategies%2520and%2520examine%2520their%2520effects%2520across%2520a%2520broad%2520range%2520of%2520bias%2520types%252C%2520including%2520stereotypes%252C%2520fairness%252C%2520toxicity%252C%2520and%2520sentiment.%2520We%2520employ%2520both%2520probability-%2520and%2520generated%2520text-based%2520metrics%2520across%252013%2520benchmarks%2520and%2520evaluate%2520models%2520that%2520differ%2520in%2520architecture%2520family%2520and%2520reasoning%2520ability.%2520Our%2520findings%2520show%2520that%2520quantization%2520has%2520a%2520nuanced%2520impact%2520on%2520bias%253A%2520while%2520it%2520can%2520reduce%2520model%2520toxicity%2520and%2520does%2520not%2520significantly%2520impact%2520sentiment%252C%2520it%2520tends%2520to%2520slightly%2520increase%2520stereotypes%2520and%2520unfairness%2520in%2520generative%2520tasks%252C%2520especially%2520under%2520aggressive%2520compression.%2520These%2520trends%2520are%2520generally%2520consistent%2520across%2520demographic%2520categories%2520and%2520subgroups%252C%2520and%2520model%2520types%252C%2520although%2520their%2520magnitude%2520depends%2520on%2520the%2520specific%2520setting.%2520Overall%252C%2520our%2520results%2520highlight%2520the%2520importance%2520of%2520carefully%2520balancing%2520efficiency%2520and%2520ethical%2520considerations%2520when%2520applying%2520quantization%2520in%2520practice.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.18088v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=How%20Quantization%20Shapes%20Bias%20in%20Large%20Language%20Models&entry.906535625=Federico%20Marcuzzi%20and%20Xuefei%20Ning%20and%20Roy%20Schwartz%20and%20Iryna%20Gurevych&entry.1292438233=This%20work%20presents%20a%20comprehensive%20evaluation%20of%20how%20quantization%20affects%20model%20bias%2C%20with%20particular%20attention%20to%20its%20impact%20on%20individual%20demographic%20subgroups.%20We%20focus%20on%20weight%20and%20activation%20quantization%20strategies%20and%20examine%20their%20effects%20across%20a%20broad%20range%20of%20bias%20types%2C%20including%20stereotypes%2C%20fairness%2C%20toxicity%2C%20and%20sentiment.%20We%20employ%20both%20probability-%20and%20generated%20text-based%20metrics%20across%2013%20benchmarks%20and%20evaluate%20models%20that%20differ%20in%20architecture%20family%20and%20reasoning%20ability.%20Our%20findings%20show%20that%20quantization%20has%20a%20nuanced%20impact%20on%20bias%3A%20while%20it%20can%20reduce%20model%20toxicity%20and%20does%20not%20significantly%20impact%20sentiment%2C%20it%20tends%20to%20slightly%20increase%20stereotypes%20and%20unfairness%20in%20generative%20tasks%2C%20especially%20under%20aggressive%20compression.%20These%20trends%20are%20generally%20consistent%20across%20demographic%20categories%20and%20subgroups%2C%20and%20model%20types%2C%20although%20their%20magnitude%20depends%20on%20the%20specific%20setting.%20Overall%2C%20our%20results%20highlight%20the%20importance%20of%20carefully%20balancing%20efficiency%20and%20ethical%20considerations%20when%20applying%20quantization%20in%20practice.&entry.1838667208=http%3A//arxiv.org/abs/2508.18088v2&entry.124074799=Read"},
{"title": "Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models", "author": "Linlu Qiu and Fei Sha and Kelsey Allen and Yoon Kim and Tal Linzen and Sjoerd van Steenkiste", "abstract": "Large language models (LLMs) are increasingly used as agents that interact with users and with the world. To do so successfully, LLMs must construct representations of the world and form probabilistic beliefs about them. To provide personalized recommendations, for example, the LLM needs to infer a user's preferences from their behavior over multiple interactions. The Bayesian inference framework lays out the optimal way for an agent to update its beliefs as it receives new information. We first show that LLMs fall far short of the standard defined by the Bayesian framework. We then show that by teaching LLMs to mimic the predictions of the normative Bayesian model, we can dramatically improve their ability to update their beliefs; this ability generalizes to new tasks. We conclude that LLMs can effectively learn reasoning skills from examples and generalize those skills to new domains.", "link": "http://arxiv.org/abs/2503.17523v3", "date": "2026-01-15", "relevancy": 1.9409, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5737}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.474}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4611}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Teaching%20Enables%20Probabilistic%20Reasoning%20in%20Large%20Language%20Models&body=Title%3A%20Bayesian%20Teaching%20Enables%20Probabilistic%20Reasoning%20in%20Large%20Language%20Models%0AAuthor%3A%20Linlu%20Qiu%20and%20Fei%20Sha%20and%20Kelsey%20Allen%20and%20Yoon%20Kim%20and%20Tal%20Linzen%20and%20Sjoerd%20van%20Steenkiste%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20as%20agents%20that%20interact%20with%20users%20and%20with%20the%20world.%20To%20do%20so%20successfully%2C%20LLMs%20must%20construct%20representations%20of%20the%20world%20and%20form%20probabilistic%20beliefs%20about%20them.%20To%20provide%20personalized%20recommendations%2C%20for%20example%2C%20the%20LLM%20needs%20to%20infer%20a%20user%27s%20preferences%20from%20their%20behavior%20over%20multiple%20interactions.%20The%20Bayesian%20inference%20framework%20lays%20out%20the%20optimal%20way%20for%20an%20agent%20to%20update%20its%20beliefs%20as%20it%20receives%20new%20information.%20We%20first%20show%20that%20LLMs%20fall%20far%20short%20of%20the%20standard%20defined%20by%20the%20Bayesian%20framework.%20We%20then%20show%20that%20by%20teaching%20LLMs%20to%20mimic%20the%20predictions%20of%20the%20normative%20Bayesian%20model%2C%20we%20can%20dramatically%20improve%20their%20ability%20to%20update%20their%20beliefs%3B%20this%20ability%20generalizes%20to%20new%20tasks.%20We%20conclude%20that%20LLMs%20can%20effectively%20learn%20reasoning%20skills%20from%20examples%20and%20generalize%20those%20skills%20to%20new%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2503.17523v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Teaching%2520Enables%2520Probabilistic%2520Reasoning%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DLinlu%2520Qiu%2520and%2520Fei%2520Sha%2520and%2520Kelsey%2520Allen%2520and%2520Yoon%2520Kim%2520and%2520Tal%2520Linzen%2520and%2520Sjoerd%2520van%2520Steenkiste%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520increasingly%2520used%2520as%2520agents%2520that%2520interact%2520with%2520users%2520and%2520with%2520the%2520world.%2520To%2520do%2520so%2520successfully%252C%2520LLMs%2520must%2520construct%2520representations%2520of%2520the%2520world%2520and%2520form%2520probabilistic%2520beliefs%2520about%2520them.%2520To%2520provide%2520personalized%2520recommendations%252C%2520for%2520example%252C%2520the%2520LLM%2520needs%2520to%2520infer%2520a%2520user%2527s%2520preferences%2520from%2520their%2520behavior%2520over%2520multiple%2520interactions.%2520The%2520Bayesian%2520inference%2520framework%2520lays%2520out%2520the%2520optimal%2520way%2520for%2520an%2520agent%2520to%2520update%2520its%2520beliefs%2520as%2520it%2520receives%2520new%2520information.%2520We%2520first%2520show%2520that%2520LLMs%2520fall%2520far%2520short%2520of%2520the%2520standard%2520defined%2520by%2520the%2520Bayesian%2520framework.%2520We%2520then%2520show%2520that%2520by%2520teaching%2520LLMs%2520to%2520mimic%2520the%2520predictions%2520of%2520the%2520normative%2520Bayesian%2520model%252C%2520we%2520can%2520dramatically%2520improve%2520their%2520ability%2520to%2520update%2520their%2520beliefs%253B%2520this%2520ability%2520generalizes%2520to%2520new%2520tasks.%2520We%2520conclude%2520that%2520LLMs%2520can%2520effectively%2520learn%2520reasoning%2520skills%2520from%2520examples%2520and%2520generalize%2520those%2520skills%2520to%2520new%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.17523v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Teaching%20Enables%20Probabilistic%20Reasoning%20in%20Large%20Language%20Models&entry.906535625=Linlu%20Qiu%20and%20Fei%20Sha%20and%20Kelsey%20Allen%20and%20Yoon%20Kim%20and%20Tal%20Linzen%20and%20Sjoerd%20van%20Steenkiste&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20increasingly%20used%20as%20agents%20that%20interact%20with%20users%20and%20with%20the%20world.%20To%20do%20so%20successfully%2C%20LLMs%20must%20construct%20representations%20of%20the%20world%20and%20form%20probabilistic%20beliefs%20about%20them.%20To%20provide%20personalized%20recommendations%2C%20for%20example%2C%20the%20LLM%20needs%20to%20infer%20a%20user%27s%20preferences%20from%20their%20behavior%20over%20multiple%20interactions.%20The%20Bayesian%20inference%20framework%20lays%20out%20the%20optimal%20way%20for%20an%20agent%20to%20update%20its%20beliefs%20as%20it%20receives%20new%20information.%20We%20first%20show%20that%20LLMs%20fall%20far%20short%20of%20the%20standard%20defined%20by%20the%20Bayesian%20framework.%20We%20then%20show%20that%20by%20teaching%20LLMs%20to%20mimic%20the%20predictions%20of%20the%20normative%20Bayesian%20model%2C%20we%20can%20dramatically%20improve%20their%20ability%20to%20update%20their%20beliefs%3B%20this%20ability%20generalizes%20to%20new%20tasks.%20We%20conclude%20that%20LLMs%20can%20effectively%20learn%20reasoning%20skills%20from%20examples%20and%20generalize%20those%20skills%20to%20new%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2503.17523v3&entry.124074799=Read"},
{"title": "Enhancing the quality of gauge images captured in smoke and haze scenes through deep learning", "author": "Oscar H. Ram\u00edrez-Agudelo and Akshay N. Shewatkar and Edoardo Milana and Roland C. Aydin and Kai Franke", "abstract": "Images captured in hazy and smoky environments suffer from reduced visibility, posing a challenge when monitoring infrastructures and hindering emergency services during critical situations. The proposed work investigates the use of the deep learning models to enhance the automatic, machine-based readability of gauge in smoky environments, with accurate gauge data interpretation serving as a valuable tool for first responders. The study utilizes two deep learning architectures, FFA-Net and AECR-Net, to improve the visibility of gauge images, corrupted with light up to dense haze and smoke. Since benchmark datasets of analog gauge images are unavailable, a new synthetic dataset, containing over 14,000 images, was generated using the Unreal Engine. The models were trained with an 80\\% train, 10\\% validation, and 10\\% test split for the haze and smoke dataset, respectively. For the synthetic haze dataset, the SSIM and PSNR metrics are about 0.98 and 43\\,dB, respectively, comparing well to state-of-the art results. Additionally, more robust results are retrieved from the AECR-Net, when compared to the FFA-Net. Although the results from the synthetic smoke dataset are poorer, the trained models achieve interesting results. In general, imaging in the presence of smoke are more difficult to enhance given the inhomogeneity and high density. Secondly, FFA-Net and AECR-Net are implemented to dehaze and not to desmoke images. This work shows that use of deep learning architectures can improve the quality of analog gauge images captured in smoke and haze scenes immensely. Finally, the enhanced output images can be successfully post-processed for automatic autonomous reading of gauges", "link": "http://arxiv.org/abs/2601.10537v1", "date": "2026-01-15", "relevancy": 1.5888, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5554}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5256}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5209}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Enhancing%20the%20quality%20of%20gauge%20images%20captured%20in%20smoke%20and%20haze%20scenes%20through%20deep%20learning&body=Title%3A%20Enhancing%20the%20quality%20of%20gauge%20images%20captured%20in%20smoke%20and%20haze%20scenes%20through%20deep%20learning%0AAuthor%3A%20Oscar%20H.%20Ram%C3%ADrez-Agudelo%20and%20Akshay%20N.%20Shewatkar%20and%20Edoardo%20Milana%20and%20Roland%20C.%20Aydin%20and%20Kai%20Franke%0AAbstract%3A%20Images%20captured%20in%20hazy%20and%20smoky%20environments%20suffer%20from%20reduced%20visibility%2C%20posing%20a%20challenge%20when%20monitoring%20infrastructures%20and%20hindering%20emergency%20services%20during%20critical%20situations.%20The%20proposed%20work%20investigates%20the%20use%20of%20the%20deep%20learning%20models%20to%20enhance%20the%20automatic%2C%20machine-based%20readability%20of%20gauge%20in%20smoky%20environments%2C%20with%20accurate%20gauge%20data%20interpretation%20serving%20as%20a%20valuable%20tool%20for%20first%20responders.%20The%20study%20utilizes%20two%20deep%20learning%20architectures%2C%20FFA-Net%20and%20AECR-Net%2C%20to%20improve%20the%20visibility%20of%20gauge%20images%2C%20corrupted%20with%20light%20up%20to%20dense%20haze%20and%20smoke.%20Since%20benchmark%20datasets%20of%20analog%20gauge%20images%20are%20unavailable%2C%20a%20new%20synthetic%20dataset%2C%20containing%20over%2014%2C000%20images%2C%20was%20generated%20using%20the%20Unreal%20Engine.%20The%20models%20were%20trained%20with%20an%2080%5C%25%20train%2C%2010%5C%25%20validation%2C%20and%2010%5C%25%20test%20split%20for%20the%20haze%20and%20smoke%20dataset%2C%20respectively.%20For%20the%20synthetic%20haze%20dataset%2C%20the%20SSIM%20and%20PSNR%20metrics%20are%20about%200.98%20and%2043%5C%2CdB%2C%20respectively%2C%20comparing%20well%20to%20state-of-the%20art%20results.%20Additionally%2C%20more%20robust%20results%20are%20retrieved%20from%20the%20AECR-Net%2C%20when%20compared%20to%20the%20FFA-Net.%20Although%20the%20results%20from%20the%20synthetic%20smoke%20dataset%20are%20poorer%2C%20the%20trained%20models%20achieve%20interesting%20results.%20In%20general%2C%20imaging%20in%20the%20presence%20of%20smoke%20are%20more%20difficult%20to%20enhance%20given%20the%20inhomogeneity%20and%20high%20density.%20Secondly%2C%20FFA-Net%20and%20AECR-Net%20are%20implemented%20to%20dehaze%20and%20not%20to%20desmoke%20images.%20This%20work%20shows%20that%20use%20of%20deep%20learning%20architectures%20can%20improve%20the%20quality%20of%20analog%20gauge%20images%20captured%20in%20smoke%20and%20haze%20scenes%20immensely.%20Finally%2C%20the%20enhanced%20output%20images%20can%20be%20successfully%20post-processed%20for%20automatic%20autonomous%20reading%20of%20gauges%0ALink%3A%20http%3A//arxiv.org/abs/2601.10537v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnhancing%2520the%2520quality%2520of%2520gauge%2520images%2520captured%2520in%2520smoke%2520and%2520haze%2520scenes%2520through%2520deep%2520learning%26entry.906535625%3DOscar%2520H.%2520Ram%25C3%25ADrez-Agudelo%2520and%2520Akshay%2520N.%2520Shewatkar%2520and%2520Edoardo%2520Milana%2520and%2520Roland%2520C.%2520Aydin%2520and%2520Kai%2520Franke%26entry.1292438233%3DImages%2520captured%2520in%2520hazy%2520and%2520smoky%2520environments%2520suffer%2520from%2520reduced%2520visibility%252C%2520posing%2520a%2520challenge%2520when%2520monitoring%2520infrastructures%2520and%2520hindering%2520emergency%2520services%2520during%2520critical%2520situations.%2520The%2520proposed%2520work%2520investigates%2520the%2520use%2520of%2520the%2520deep%2520learning%2520models%2520to%2520enhance%2520the%2520automatic%252C%2520machine-based%2520readability%2520of%2520gauge%2520in%2520smoky%2520environments%252C%2520with%2520accurate%2520gauge%2520data%2520interpretation%2520serving%2520as%2520a%2520valuable%2520tool%2520for%2520first%2520responders.%2520The%2520study%2520utilizes%2520two%2520deep%2520learning%2520architectures%252C%2520FFA-Net%2520and%2520AECR-Net%252C%2520to%2520improve%2520the%2520visibility%2520of%2520gauge%2520images%252C%2520corrupted%2520with%2520light%2520up%2520to%2520dense%2520haze%2520and%2520smoke.%2520Since%2520benchmark%2520datasets%2520of%2520analog%2520gauge%2520images%2520are%2520unavailable%252C%2520a%2520new%2520synthetic%2520dataset%252C%2520containing%2520over%252014%252C000%2520images%252C%2520was%2520generated%2520using%2520the%2520Unreal%2520Engine.%2520The%2520models%2520were%2520trained%2520with%2520an%252080%255C%2525%2520train%252C%252010%255C%2525%2520validation%252C%2520and%252010%255C%2525%2520test%2520split%2520for%2520the%2520haze%2520and%2520smoke%2520dataset%252C%2520respectively.%2520For%2520the%2520synthetic%2520haze%2520dataset%252C%2520the%2520SSIM%2520and%2520PSNR%2520metrics%2520are%2520about%25200.98%2520and%252043%255C%252CdB%252C%2520respectively%252C%2520comparing%2520well%2520to%2520state-of-the%2520art%2520results.%2520Additionally%252C%2520more%2520robust%2520results%2520are%2520retrieved%2520from%2520the%2520AECR-Net%252C%2520when%2520compared%2520to%2520the%2520FFA-Net.%2520Although%2520the%2520results%2520from%2520the%2520synthetic%2520smoke%2520dataset%2520are%2520poorer%252C%2520the%2520trained%2520models%2520achieve%2520interesting%2520results.%2520In%2520general%252C%2520imaging%2520in%2520the%2520presence%2520of%2520smoke%2520are%2520more%2520difficult%2520to%2520enhance%2520given%2520the%2520inhomogeneity%2520and%2520high%2520density.%2520Secondly%252C%2520FFA-Net%2520and%2520AECR-Net%2520are%2520implemented%2520to%2520dehaze%2520and%2520not%2520to%2520desmoke%2520images.%2520This%2520work%2520shows%2520that%2520use%2520of%2520deep%2520learning%2520architectures%2520can%2520improve%2520the%2520quality%2520of%2520analog%2520gauge%2520images%2520captured%2520in%2520smoke%2520and%2520haze%2520scenes%2520immensely.%2520Finally%252C%2520the%2520enhanced%2520output%2520images%2520can%2520be%2520successfully%2520post-processed%2520for%2520automatic%2520autonomous%2520reading%2520of%2520gauges%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10537v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Enhancing%20the%20quality%20of%20gauge%20images%20captured%20in%20smoke%20and%20haze%20scenes%20through%20deep%20learning&entry.906535625=Oscar%20H.%20Ram%C3%ADrez-Agudelo%20and%20Akshay%20N.%20Shewatkar%20and%20Edoardo%20Milana%20and%20Roland%20C.%20Aydin%20and%20Kai%20Franke&entry.1292438233=Images%20captured%20in%20hazy%20and%20smoky%20environments%20suffer%20from%20reduced%20visibility%2C%20posing%20a%20challenge%20when%20monitoring%20infrastructures%20and%20hindering%20emergency%20services%20during%20critical%20situations.%20The%20proposed%20work%20investigates%20the%20use%20of%20the%20deep%20learning%20models%20to%20enhance%20the%20automatic%2C%20machine-based%20readability%20of%20gauge%20in%20smoky%20environments%2C%20with%20accurate%20gauge%20data%20interpretation%20serving%20as%20a%20valuable%20tool%20for%20first%20responders.%20The%20study%20utilizes%20two%20deep%20learning%20architectures%2C%20FFA-Net%20and%20AECR-Net%2C%20to%20improve%20the%20visibility%20of%20gauge%20images%2C%20corrupted%20with%20light%20up%20to%20dense%20haze%20and%20smoke.%20Since%20benchmark%20datasets%20of%20analog%20gauge%20images%20are%20unavailable%2C%20a%20new%20synthetic%20dataset%2C%20containing%20over%2014%2C000%20images%2C%20was%20generated%20using%20the%20Unreal%20Engine.%20The%20models%20were%20trained%20with%20an%2080%5C%25%20train%2C%2010%5C%25%20validation%2C%20and%2010%5C%25%20test%20split%20for%20the%20haze%20and%20smoke%20dataset%2C%20respectively.%20For%20the%20synthetic%20haze%20dataset%2C%20the%20SSIM%20and%20PSNR%20metrics%20are%20about%200.98%20and%2043%5C%2CdB%2C%20respectively%2C%20comparing%20well%20to%20state-of-the%20art%20results.%20Additionally%2C%20more%20robust%20results%20are%20retrieved%20from%20the%20AECR-Net%2C%20when%20compared%20to%20the%20FFA-Net.%20Although%20the%20results%20from%20the%20synthetic%20smoke%20dataset%20are%20poorer%2C%20the%20trained%20models%20achieve%20interesting%20results.%20In%20general%2C%20imaging%20in%20the%20presence%20of%20smoke%20are%20more%20difficult%20to%20enhance%20given%20the%20inhomogeneity%20and%20high%20density.%20Secondly%2C%20FFA-Net%20and%20AECR-Net%20are%20implemented%20to%20dehaze%20and%20not%20to%20desmoke%20images.%20This%20work%20shows%20that%20use%20of%20deep%20learning%20architectures%20can%20improve%20the%20quality%20of%20analog%20gauge%20images%20captured%20in%20smoke%20and%20haze%20scenes%20immensely.%20Finally%2C%20the%20enhanced%20output%20images%20can%20be%20successfully%20post-processed%20for%20automatic%20autonomous%20reading%20of%20gauges&entry.1838667208=http%3A//arxiv.org/abs/2601.10537v1&entry.124074799=Read"},
{"title": "Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering", "author": "Xinyu Zhu and Yuzhu Cai and Zexi Liu and Bingyang Zheng and Cheng Wang and Rui Ye and Jiaao Chen and Hanrui Wang and Wei-Chen Wang and Yuzhi Zhang and Linfeng Zhang and Weinan E and Di Jin and Siheng Chen", "abstract": "The advancement of artificial intelligence toward agentic science is currently bottlenecked by the challenge of ultra-long-horizon autonomy, the ability to sustain strategic coherence and iterative correction over experimental cycles spanning days or weeks. While Large Language Models (LLMs) have demonstrated prowess in short-horizon reasoning, they are easily overwhelmed by execution details in the high-dimensional, delayed-feedback environments of real-world research, failing to consolidate sparse feedback into coherent long-term guidance. Here, we present ML-Master 2.0, an autonomous agent that masters ultra-long-horizon machine learning engineering (MLE) which is a representative microcosm of scientific discovery. By reframing context management as a process of cognitive accumulation, our approach introduces Hierarchical Cognitive Caching (HCC), a multi-tiered architecture inspired by computer systems that enables the structural differentiation of experience over time. By dynamically distilling transient execution traces into stable knowledge and cross-task wisdom, HCC allows agents to decouple immediate execution from long-term experimental strategy, effectively overcoming the scaling limits of static context windows. In evaluations on OpenAI's MLE-Bench under 24-hour budgets, ML-Master 2.0 achieves a state-of-the-art medal rate of 56.44%. Our findings demonstrate that ultra-long-horizon autonomy provides a scalable blueprint for AI capable of autonomous exploration beyond human-precedent complexities.", "link": "http://arxiv.org/abs/2601.10402v1", "date": "2026-01-15", "relevancy": 1.6025, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5471}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5268}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5091}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Ultra-Long-Horizon%20Agentic%20Science%3A%20Cognitive%20Accumulation%20for%20Machine%20Learning%20Engineering&body=Title%3A%20Toward%20Ultra-Long-Horizon%20Agentic%20Science%3A%20Cognitive%20Accumulation%20for%20Machine%20Learning%20Engineering%0AAuthor%3A%20Xinyu%20Zhu%20and%20Yuzhu%20Cai%20and%20Zexi%20Liu%20and%20Bingyang%20Zheng%20and%20Cheng%20Wang%20and%20Rui%20Ye%20and%20Jiaao%20Chen%20and%20Hanrui%20Wang%20and%20Wei-Chen%20Wang%20and%20Yuzhi%20Zhang%20and%20Linfeng%20Zhang%20and%20Weinan%20E%20and%20Di%20Jin%20and%20Siheng%20Chen%0AAbstract%3A%20The%20advancement%20of%20artificial%20intelligence%20toward%20agentic%20science%20is%20currently%20bottlenecked%20by%20the%20challenge%20of%20ultra-long-horizon%20autonomy%2C%20the%20ability%20to%20sustain%20strategic%20coherence%20and%20iterative%20correction%20over%20experimental%20cycles%20spanning%20days%20or%20weeks.%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20prowess%20in%20short-horizon%20reasoning%2C%20they%20are%20easily%20overwhelmed%20by%20execution%20details%20in%20the%20high-dimensional%2C%20delayed-feedback%20environments%20of%20real-world%20research%2C%20failing%20to%20consolidate%20sparse%20feedback%20into%20coherent%20long-term%20guidance.%20Here%2C%20we%20present%20ML-Master%202.0%2C%20an%20autonomous%20agent%20that%20masters%20ultra-long-horizon%20machine%20learning%20engineering%20%28MLE%29%20which%20is%20a%20representative%20microcosm%20of%20scientific%20discovery.%20By%20reframing%20context%20management%20as%20a%20process%20of%20cognitive%20accumulation%2C%20our%20approach%20introduces%20Hierarchical%20Cognitive%20Caching%20%28HCC%29%2C%20a%20multi-tiered%20architecture%20inspired%20by%20computer%20systems%20that%20enables%20the%20structural%20differentiation%20of%20experience%20over%20time.%20By%20dynamically%20distilling%20transient%20execution%20traces%20into%20stable%20knowledge%20and%20cross-task%20wisdom%2C%20HCC%20allows%20agents%20to%20decouple%20immediate%20execution%20from%20long-term%20experimental%20strategy%2C%20effectively%20overcoming%20the%20scaling%20limits%20of%20static%20context%20windows.%20In%20evaluations%20on%20OpenAI%27s%20MLE-Bench%20under%2024-hour%20budgets%2C%20ML-Master%202.0%20achieves%20a%20state-of-the-art%20medal%20rate%20of%2056.44%25.%20Our%20findings%20demonstrate%20that%20ultra-long-horizon%20autonomy%20provides%20a%20scalable%20blueprint%20for%20AI%20capable%20of%20autonomous%20exploration%20beyond%20human-precedent%20complexities.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10402v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Ultra-Long-Horizon%2520Agentic%2520Science%253A%2520Cognitive%2520Accumulation%2520for%2520Machine%2520Learning%2520Engineering%26entry.906535625%3DXinyu%2520Zhu%2520and%2520Yuzhu%2520Cai%2520and%2520Zexi%2520Liu%2520and%2520Bingyang%2520Zheng%2520and%2520Cheng%2520Wang%2520and%2520Rui%2520Ye%2520and%2520Jiaao%2520Chen%2520and%2520Hanrui%2520Wang%2520and%2520Wei-Chen%2520Wang%2520and%2520Yuzhi%2520Zhang%2520and%2520Linfeng%2520Zhang%2520and%2520Weinan%2520E%2520and%2520Di%2520Jin%2520and%2520Siheng%2520Chen%26entry.1292438233%3DThe%2520advancement%2520of%2520artificial%2520intelligence%2520toward%2520agentic%2520science%2520is%2520currently%2520bottlenecked%2520by%2520the%2520challenge%2520of%2520ultra-long-horizon%2520autonomy%252C%2520the%2520ability%2520to%2520sustain%2520strategic%2520coherence%2520and%2520iterative%2520correction%2520over%2520experimental%2520cycles%2520spanning%2520days%2520or%2520weeks.%2520While%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520demonstrated%2520prowess%2520in%2520short-horizon%2520reasoning%252C%2520they%2520are%2520easily%2520overwhelmed%2520by%2520execution%2520details%2520in%2520the%2520high-dimensional%252C%2520delayed-feedback%2520environments%2520of%2520real-world%2520research%252C%2520failing%2520to%2520consolidate%2520sparse%2520feedback%2520into%2520coherent%2520long-term%2520guidance.%2520Here%252C%2520we%2520present%2520ML-Master%25202.0%252C%2520an%2520autonomous%2520agent%2520that%2520masters%2520ultra-long-horizon%2520machine%2520learning%2520engineering%2520%2528MLE%2529%2520which%2520is%2520a%2520representative%2520microcosm%2520of%2520scientific%2520discovery.%2520By%2520reframing%2520context%2520management%2520as%2520a%2520process%2520of%2520cognitive%2520accumulation%252C%2520our%2520approach%2520introduces%2520Hierarchical%2520Cognitive%2520Caching%2520%2528HCC%2529%252C%2520a%2520multi-tiered%2520architecture%2520inspired%2520by%2520computer%2520systems%2520that%2520enables%2520the%2520structural%2520differentiation%2520of%2520experience%2520over%2520time.%2520By%2520dynamically%2520distilling%2520transient%2520execution%2520traces%2520into%2520stable%2520knowledge%2520and%2520cross-task%2520wisdom%252C%2520HCC%2520allows%2520agents%2520to%2520decouple%2520immediate%2520execution%2520from%2520long-term%2520experimental%2520strategy%252C%2520effectively%2520overcoming%2520the%2520scaling%2520limits%2520of%2520static%2520context%2520windows.%2520In%2520evaluations%2520on%2520OpenAI%2527s%2520MLE-Bench%2520under%252024-hour%2520budgets%252C%2520ML-Master%25202.0%2520achieves%2520a%2520state-of-the-art%2520medal%2520rate%2520of%252056.44%2525.%2520Our%2520findings%2520demonstrate%2520that%2520ultra-long-horizon%2520autonomy%2520provides%2520a%2520scalable%2520blueprint%2520for%2520AI%2520capable%2520of%2520autonomous%2520exploration%2520beyond%2520human-precedent%2520complexities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10402v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Ultra-Long-Horizon%20Agentic%20Science%3A%20Cognitive%20Accumulation%20for%20Machine%20Learning%20Engineering&entry.906535625=Xinyu%20Zhu%20and%20Yuzhu%20Cai%20and%20Zexi%20Liu%20and%20Bingyang%20Zheng%20and%20Cheng%20Wang%20and%20Rui%20Ye%20and%20Jiaao%20Chen%20and%20Hanrui%20Wang%20and%20Wei-Chen%20Wang%20and%20Yuzhi%20Zhang%20and%20Linfeng%20Zhang%20and%20Weinan%20E%20and%20Di%20Jin%20and%20Siheng%20Chen&entry.1292438233=The%20advancement%20of%20artificial%20intelligence%20toward%20agentic%20science%20is%20currently%20bottlenecked%20by%20the%20challenge%20of%20ultra-long-horizon%20autonomy%2C%20the%20ability%20to%20sustain%20strategic%20coherence%20and%20iterative%20correction%20over%20experimental%20cycles%20spanning%20days%20or%20weeks.%20While%20Large%20Language%20Models%20%28LLMs%29%20have%20demonstrated%20prowess%20in%20short-horizon%20reasoning%2C%20they%20are%20easily%20overwhelmed%20by%20execution%20details%20in%20the%20high-dimensional%2C%20delayed-feedback%20environments%20of%20real-world%20research%2C%20failing%20to%20consolidate%20sparse%20feedback%20into%20coherent%20long-term%20guidance.%20Here%2C%20we%20present%20ML-Master%202.0%2C%20an%20autonomous%20agent%20that%20masters%20ultra-long-horizon%20machine%20learning%20engineering%20%28MLE%29%20which%20is%20a%20representative%20microcosm%20of%20scientific%20discovery.%20By%20reframing%20context%20management%20as%20a%20process%20of%20cognitive%20accumulation%2C%20our%20approach%20introduces%20Hierarchical%20Cognitive%20Caching%20%28HCC%29%2C%20a%20multi-tiered%20architecture%20inspired%20by%20computer%20systems%20that%20enables%20the%20structural%20differentiation%20of%20experience%20over%20time.%20By%20dynamically%20distilling%20transient%20execution%20traces%20into%20stable%20knowledge%20and%20cross-task%20wisdom%2C%20HCC%20allows%20agents%20to%20decouple%20immediate%20execution%20from%20long-term%20experimental%20strategy%2C%20effectively%20overcoming%20the%20scaling%20limits%20of%20static%20context%20windows.%20In%20evaluations%20on%20OpenAI%27s%20MLE-Bench%20under%2024-hour%20budgets%2C%20ML-Master%202.0%20achieves%20a%20state-of-the-art%20medal%20rate%20of%2056.44%25.%20Our%20findings%20demonstrate%20that%20ultra-long-horizon%20autonomy%20provides%20a%20scalable%20blueprint%20for%20AI%20capable%20of%20autonomous%20exploration%20beyond%20human-precedent%20complexities.&entry.1838667208=http%3A//arxiv.org/abs/2601.10402v1&entry.124074799=Read"},
{"title": "FastStair: Learning to Run Up Stairs with Humanoid Robots", "author": "Yan Liu and Tao Yu and Haolin Song and Hongbo Zhu and Nianzong Hu and Yuzhi Hao and Xiuyong Yao and Xizhe Zang and Hua Chen and Jie Zhao", "abstract": "Running up stairs is effortless for humans but remains extremely challenging for humanoid robots due to the simultaneous requirements of high agility and strict stability. Model-free reinforcement learning (RL) can generate dynamic locomotion, yet implicit stability rewards and heavy reliance on task-specific reward shaping tend to result in unsafe behaviors, especially on stairs; conversely, model-based foothold planners encode contact feasibility and stability structure, but enforcing their hard constraints often induces conservative motion that limits speed. We present FastStair, a planner-guided, multi-stage learning framework that reconciles these complementary strengths to achieve fast and stable stair ascent. FastStair integrates a parallel model-based foothold planner into the RL training loop to bias exploration toward dynamically feasible contacts and to pretrain a safety-focused base policy. To mitigate planner-induced conservatism and the discrepancy between low- and high-speed action distributions, the base policy was fine-tuned into speed-specialized experts and then integrated via Low-Rank Adaptation (LoRA) to enable smooth operation across the full commanded-speed range. We deploy the resulting controller on the Oli humanoid robot, achieving stable stair ascent at commanded speeds up to 1.65 m/s and traversing a 33-step spiral staircase (17 cm rise per step) in 12 s, demonstrating robust high-speed performance on long staircases. Notably, the proposed approach served as the champion solution in the Canton Tower Robot Run Up Competition.", "link": "http://arxiv.org/abs/2601.10365v1", "date": "2026-01-15", "relevancy": 1.5021, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5335}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.497}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4891}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastStair%3A%20Learning%20to%20Run%20Up%20Stairs%20with%20Humanoid%20Robots&body=Title%3A%20FastStair%3A%20Learning%20to%20Run%20Up%20Stairs%20with%20Humanoid%20Robots%0AAuthor%3A%20Yan%20Liu%20and%20Tao%20Yu%20and%20Haolin%20Song%20and%20Hongbo%20Zhu%20and%20Nianzong%20Hu%20and%20Yuzhi%20Hao%20and%20Xiuyong%20Yao%20and%20Xizhe%20Zang%20and%20Hua%20Chen%20and%20Jie%20Zhao%0AAbstract%3A%20Running%20up%20stairs%20is%20effortless%20for%20humans%20but%20remains%20extremely%20challenging%20for%20humanoid%20robots%20due%20to%20the%20simultaneous%20requirements%20of%20high%20agility%20and%20strict%20stability.%20Model-free%20reinforcement%20learning%20%28RL%29%20can%20generate%20dynamic%20locomotion%2C%20yet%20implicit%20stability%20rewards%20and%20heavy%20reliance%20on%20task-specific%20reward%20shaping%20tend%20to%20result%20in%20unsafe%20behaviors%2C%20especially%20on%20stairs%3B%20conversely%2C%20model-based%20foothold%20planners%20encode%20contact%20feasibility%20and%20stability%20structure%2C%20but%20enforcing%20their%20hard%20constraints%20often%20induces%20conservative%20motion%20that%20limits%20speed.%20We%20present%20FastStair%2C%20a%20planner-guided%2C%20multi-stage%20learning%20framework%20that%20reconciles%20these%20complementary%20strengths%20to%20achieve%20fast%20and%20stable%20stair%20ascent.%20FastStair%20integrates%20a%20parallel%20model-based%20foothold%20planner%20into%20the%20RL%20training%20loop%20to%20bias%20exploration%20toward%20dynamically%20feasible%20contacts%20and%20to%20pretrain%20a%20safety-focused%20base%20policy.%20To%20mitigate%20planner-induced%20conservatism%20and%20the%20discrepancy%20between%20low-%20and%20high-speed%20action%20distributions%2C%20the%20base%20policy%20was%20fine-tuned%20into%20speed-specialized%20experts%20and%20then%20integrated%20via%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20enable%20smooth%20operation%20across%20the%20full%20commanded-speed%20range.%20We%20deploy%20the%20resulting%20controller%20on%20the%20Oli%20humanoid%20robot%2C%20achieving%20stable%20stair%20ascent%20at%20commanded%20speeds%20up%20to%201.65%20m/s%20and%20traversing%20a%2033-step%20spiral%20staircase%20%2817%20cm%20rise%20per%20step%29%20in%2012%20s%2C%20demonstrating%20robust%20high-speed%20performance%20on%20long%20staircases.%20Notably%2C%20the%20proposed%20approach%20served%20as%20the%20champion%20solution%20in%20the%20Canton%20Tower%20Robot%20Run%20Up%20Competition.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10365v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastStair%253A%2520Learning%2520to%2520Run%2520Up%2520Stairs%2520with%2520Humanoid%2520Robots%26entry.906535625%3DYan%2520Liu%2520and%2520Tao%2520Yu%2520and%2520Haolin%2520Song%2520and%2520Hongbo%2520Zhu%2520and%2520Nianzong%2520Hu%2520and%2520Yuzhi%2520Hao%2520and%2520Xiuyong%2520Yao%2520and%2520Xizhe%2520Zang%2520and%2520Hua%2520Chen%2520and%2520Jie%2520Zhao%26entry.1292438233%3DRunning%2520up%2520stairs%2520is%2520effortless%2520for%2520humans%2520but%2520remains%2520extremely%2520challenging%2520for%2520humanoid%2520robots%2520due%2520to%2520the%2520simultaneous%2520requirements%2520of%2520high%2520agility%2520and%2520strict%2520stability.%2520Model-free%2520reinforcement%2520learning%2520%2528RL%2529%2520can%2520generate%2520dynamic%2520locomotion%252C%2520yet%2520implicit%2520stability%2520rewards%2520and%2520heavy%2520reliance%2520on%2520task-specific%2520reward%2520shaping%2520tend%2520to%2520result%2520in%2520unsafe%2520behaviors%252C%2520especially%2520on%2520stairs%253B%2520conversely%252C%2520model-based%2520foothold%2520planners%2520encode%2520contact%2520feasibility%2520and%2520stability%2520structure%252C%2520but%2520enforcing%2520their%2520hard%2520constraints%2520often%2520induces%2520conservative%2520motion%2520that%2520limits%2520speed.%2520We%2520present%2520FastStair%252C%2520a%2520planner-guided%252C%2520multi-stage%2520learning%2520framework%2520that%2520reconciles%2520these%2520complementary%2520strengths%2520to%2520achieve%2520fast%2520and%2520stable%2520stair%2520ascent.%2520FastStair%2520integrates%2520a%2520parallel%2520model-based%2520foothold%2520planner%2520into%2520the%2520RL%2520training%2520loop%2520to%2520bias%2520exploration%2520toward%2520dynamically%2520feasible%2520contacts%2520and%2520to%2520pretrain%2520a%2520safety-focused%2520base%2520policy.%2520To%2520mitigate%2520planner-induced%2520conservatism%2520and%2520the%2520discrepancy%2520between%2520low-%2520and%2520high-speed%2520action%2520distributions%252C%2520the%2520base%2520policy%2520was%2520fine-tuned%2520into%2520speed-specialized%2520experts%2520and%2520then%2520integrated%2520via%2520Low-Rank%2520Adaptation%2520%2528LoRA%2529%2520to%2520enable%2520smooth%2520operation%2520across%2520the%2520full%2520commanded-speed%2520range.%2520We%2520deploy%2520the%2520resulting%2520controller%2520on%2520the%2520Oli%2520humanoid%2520robot%252C%2520achieving%2520stable%2520stair%2520ascent%2520at%2520commanded%2520speeds%2520up%2520to%25201.65%2520m/s%2520and%2520traversing%2520a%252033-step%2520spiral%2520staircase%2520%252817%2520cm%2520rise%2520per%2520step%2529%2520in%252012%2520s%252C%2520demonstrating%2520robust%2520high-speed%2520performance%2520on%2520long%2520staircases.%2520Notably%252C%2520the%2520proposed%2520approach%2520served%2520as%2520the%2520champion%2520solution%2520in%2520the%2520Canton%2520Tower%2520Robot%2520Run%2520Up%2520Competition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10365v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastStair%3A%20Learning%20to%20Run%20Up%20Stairs%20with%20Humanoid%20Robots&entry.906535625=Yan%20Liu%20and%20Tao%20Yu%20and%20Haolin%20Song%20and%20Hongbo%20Zhu%20and%20Nianzong%20Hu%20and%20Yuzhi%20Hao%20and%20Xiuyong%20Yao%20and%20Xizhe%20Zang%20and%20Hua%20Chen%20and%20Jie%20Zhao&entry.1292438233=Running%20up%20stairs%20is%20effortless%20for%20humans%20but%20remains%20extremely%20challenging%20for%20humanoid%20robots%20due%20to%20the%20simultaneous%20requirements%20of%20high%20agility%20and%20strict%20stability.%20Model-free%20reinforcement%20learning%20%28RL%29%20can%20generate%20dynamic%20locomotion%2C%20yet%20implicit%20stability%20rewards%20and%20heavy%20reliance%20on%20task-specific%20reward%20shaping%20tend%20to%20result%20in%20unsafe%20behaviors%2C%20especially%20on%20stairs%3B%20conversely%2C%20model-based%20foothold%20planners%20encode%20contact%20feasibility%20and%20stability%20structure%2C%20but%20enforcing%20their%20hard%20constraints%20often%20induces%20conservative%20motion%20that%20limits%20speed.%20We%20present%20FastStair%2C%20a%20planner-guided%2C%20multi-stage%20learning%20framework%20that%20reconciles%20these%20complementary%20strengths%20to%20achieve%20fast%20and%20stable%20stair%20ascent.%20FastStair%20integrates%20a%20parallel%20model-based%20foothold%20planner%20into%20the%20RL%20training%20loop%20to%20bias%20exploration%20toward%20dynamically%20feasible%20contacts%20and%20to%20pretrain%20a%20safety-focused%20base%20policy.%20To%20mitigate%20planner-induced%20conservatism%20and%20the%20discrepancy%20between%20low-%20and%20high-speed%20action%20distributions%2C%20the%20base%20policy%20was%20fine-tuned%20into%20speed-specialized%20experts%20and%20then%20integrated%20via%20Low-Rank%20Adaptation%20%28LoRA%29%20to%20enable%20smooth%20operation%20across%20the%20full%20commanded-speed%20range.%20We%20deploy%20the%20resulting%20controller%20on%20the%20Oli%20humanoid%20robot%2C%20achieving%20stable%20stair%20ascent%20at%20commanded%20speeds%20up%20to%201.65%20m/s%20and%20traversing%20a%2033-step%20spiral%20staircase%20%2817%20cm%20rise%20per%20step%29%20in%2012%20s%2C%20demonstrating%20robust%20high-speed%20performance%20on%20long%20staircases.%20Notably%2C%20the%20proposed%20approach%20served%20as%20the%20champion%20solution%20in%20the%20Canton%20Tower%20Robot%20Run%20Up%20Competition.&entry.1838667208=http%3A//arxiv.org/abs/2601.10365v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


