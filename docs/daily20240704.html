<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240703.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "Expressive Gaussian Human Avatars from Monocular RGB Video", "author": "Hezhen Hu and Zhiwen Fan and Tianhao Wu and Yihan Xi and Seoyoung Lee and Georgios Pavlakos and Zhangyang Wang", "abstract": "  Nuanced expressiveness, particularly through fine-grained hand and facial\nexpressions, is pivotal for enhancing the realism and vitality of digital human\nrepresentations. In this work, we focus on investigating the expressiveness of\nhuman avatars when learned from monocular RGB video; a setting that introduces\nnew challenges in capturing and animating fine-grained details. To this end, we\nintroduce EVA, a drivable human model that meticulously sculpts fine details\nbased on 3D Gaussians and SMPL-X, an expressive parametric human model. Focused\non enhancing expressiveness, our work makes three key contributions. First, we\nhighlight the critical importance of aligning the SMPL-X model with RGB frames\nfor effective avatar learning. Recognizing the limitations of current SMPL-X\nprediction methods for in-the-wild videos, we introduce a plug-and-play module\nthat significantly ameliorates misalignment issues. Second, we propose a\ncontext-aware adaptive density control strategy, which is adaptively adjusting\nthe gradient thresholds to accommodate the varied granularity across body\nparts. Last but not least, we develop a feedback mechanism that predicts\nper-pixel confidence to better guide the learning of 3D Gaussians. Extensive\nexperiments on two benchmarks demonstrate the superiority of our framework both\nquantitatively and qualitatively, especially on the fine-grained hand and\nfacial details. See the project website at \\url{https://evahuman.github.io}\n", "link": "http://arxiv.org/abs/2407.03204v1", "date": "2024-07-03", "relevancy": 3.4967, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7265}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7265}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Expressive%20Gaussian%20Human%20Avatars%20from%20Monocular%20RGB%20Video&body=Title%3A%20Expressive%20Gaussian%20Human%20Avatars%20from%20Monocular%20RGB%20Video%0AAuthor%3A%20Hezhen%20Hu%20and%20Zhiwen%20Fan%20and%20Tianhao%20Wu%20and%20Yihan%20Xi%20and%20Seoyoung%20Lee%20and%20Georgios%20Pavlakos%20and%20Zhangyang%20Wang%0AAbstract%3A%20%20%20Nuanced%20expressiveness%2C%20particularly%20through%20fine-grained%20hand%20and%20facial%0Aexpressions%2C%20is%20pivotal%20for%20enhancing%20the%20realism%20and%20vitality%20of%20digital%20human%0Arepresentations.%20In%20this%20work%2C%20we%20focus%20on%20investigating%20the%20expressiveness%20of%0Ahuman%20avatars%20when%20learned%20from%20monocular%20RGB%20video%3B%20a%20setting%20that%20introduces%0Anew%20challenges%20in%20capturing%20and%20animating%20fine-grained%20details.%20To%20this%20end%2C%20we%0Aintroduce%20EVA%2C%20a%20drivable%20human%20model%20that%20meticulously%20sculpts%20fine%20details%0Abased%20on%203D%20Gaussians%20and%20SMPL-X%2C%20an%20expressive%20parametric%20human%20model.%20Focused%0Aon%20enhancing%20expressiveness%2C%20our%20work%20makes%20three%20key%20contributions.%20First%2C%20we%0Ahighlight%20the%20critical%20importance%20of%20aligning%20the%20SMPL-X%20model%20with%20RGB%20frames%0Afor%20effective%20avatar%20learning.%20Recognizing%20the%20limitations%20of%20current%20SMPL-X%0Aprediction%20methods%20for%20in-the-wild%20videos%2C%20we%20introduce%20a%20plug-and-play%20module%0Athat%20significantly%20ameliorates%20misalignment%20issues.%20Second%2C%20we%20propose%20a%0Acontext-aware%20adaptive%20density%20control%20strategy%2C%20which%20is%20adaptively%20adjusting%0Athe%20gradient%20thresholds%20to%20accommodate%20the%20varied%20granularity%20across%20body%0Aparts.%20Last%20but%20not%20least%2C%20we%20develop%20a%20feedback%20mechanism%20that%20predicts%0Aper-pixel%20confidence%20to%20better%20guide%20the%20learning%20of%203D%20Gaussians.%20Extensive%0Aexperiments%20on%20two%20benchmarks%20demonstrate%20the%20superiority%20of%20our%20framework%20both%0Aquantitatively%20and%20qualitatively%2C%20especially%20on%20the%20fine-grained%20hand%20and%0Afacial%20details.%20See%20the%20project%20website%20at%20%5Curl%7Bhttps%3A//evahuman.github.io%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03204v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExpressive%2520Gaussian%2520Human%2520Avatars%2520from%2520Monocular%2520RGB%2520Video%26entry.906535625%3DHezhen%2520Hu%2520and%2520Zhiwen%2520Fan%2520and%2520Tianhao%2520Wu%2520and%2520Yihan%2520Xi%2520and%2520Seoyoung%2520Lee%2520and%2520Georgios%2520Pavlakos%2520and%2520Zhangyang%2520Wang%26entry.1292438233%3D%2520%2520Nuanced%2520expressiveness%252C%2520particularly%2520through%2520fine-grained%2520hand%2520and%2520facial%250Aexpressions%252C%2520is%2520pivotal%2520for%2520enhancing%2520the%2520realism%2520and%2520vitality%2520of%2520digital%2520human%250Arepresentations.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520investigating%2520the%2520expressiveness%2520of%250Ahuman%2520avatars%2520when%2520learned%2520from%2520monocular%2520RGB%2520video%253B%2520a%2520setting%2520that%2520introduces%250Anew%2520challenges%2520in%2520capturing%2520and%2520animating%2520fine-grained%2520details.%2520To%2520this%2520end%252C%2520we%250Aintroduce%2520EVA%252C%2520a%2520drivable%2520human%2520model%2520that%2520meticulously%2520sculpts%2520fine%2520details%250Abased%2520on%25203D%2520Gaussians%2520and%2520SMPL-X%252C%2520an%2520expressive%2520parametric%2520human%2520model.%2520Focused%250Aon%2520enhancing%2520expressiveness%252C%2520our%2520work%2520makes%2520three%2520key%2520contributions.%2520First%252C%2520we%250Ahighlight%2520the%2520critical%2520importance%2520of%2520aligning%2520the%2520SMPL-X%2520model%2520with%2520RGB%2520frames%250Afor%2520effective%2520avatar%2520learning.%2520Recognizing%2520the%2520limitations%2520of%2520current%2520SMPL-X%250Aprediction%2520methods%2520for%2520in-the-wild%2520videos%252C%2520we%2520introduce%2520a%2520plug-and-play%2520module%250Athat%2520significantly%2520ameliorates%2520misalignment%2520issues.%2520Second%252C%2520we%2520propose%2520a%250Acontext-aware%2520adaptive%2520density%2520control%2520strategy%252C%2520which%2520is%2520adaptively%2520adjusting%250Athe%2520gradient%2520thresholds%2520to%2520accommodate%2520the%2520varied%2520granularity%2520across%2520body%250Aparts.%2520Last%2520but%2520not%2520least%252C%2520we%2520develop%2520a%2520feedback%2520mechanism%2520that%2520predicts%250Aper-pixel%2520confidence%2520to%2520better%2520guide%2520the%2520learning%2520of%25203D%2520Gaussians.%2520Extensive%250Aexperiments%2520on%2520two%2520benchmarks%2520demonstrate%2520the%2520superiority%2520of%2520our%2520framework%2520both%250Aquantitatively%2520and%2520qualitatively%252C%2520especially%2520on%2520the%2520fine-grained%2520hand%2520and%250Afacial%2520details.%2520See%2520the%2520project%2520website%2520at%2520%255Curl%257Bhttps%253A//evahuman.github.io%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03204v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Expressive%20Gaussian%20Human%20Avatars%20from%20Monocular%20RGB%20Video&entry.906535625=Hezhen%20Hu%20and%20Zhiwen%20Fan%20and%20Tianhao%20Wu%20and%20Yihan%20Xi%20and%20Seoyoung%20Lee%20and%20Georgios%20Pavlakos%20and%20Zhangyang%20Wang&entry.1292438233=%20%20Nuanced%20expressiveness%2C%20particularly%20through%20fine-grained%20hand%20and%20facial%0Aexpressions%2C%20is%20pivotal%20for%20enhancing%20the%20realism%20and%20vitality%20of%20digital%20human%0Arepresentations.%20In%20this%20work%2C%20we%20focus%20on%20investigating%20the%20expressiveness%20of%0Ahuman%20avatars%20when%20learned%20from%20monocular%20RGB%20video%3B%20a%20setting%20that%20introduces%0Anew%20challenges%20in%20capturing%20and%20animating%20fine-grained%20details.%20To%20this%20end%2C%20we%0Aintroduce%20EVA%2C%20a%20drivable%20human%20model%20that%20meticulously%20sculpts%20fine%20details%0Abased%20on%203D%20Gaussians%20and%20SMPL-X%2C%20an%20expressive%20parametric%20human%20model.%20Focused%0Aon%20enhancing%20expressiveness%2C%20our%20work%20makes%20three%20key%20contributions.%20First%2C%20we%0Ahighlight%20the%20critical%20importance%20of%20aligning%20the%20SMPL-X%20model%20with%20RGB%20frames%0Afor%20effective%20avatar%20learning.%20Recognizing%20the%20limitations%20of%20current%20SMPL-X%0Aprediction%20methods%20for%20in-the-wild%20videos%2C%20we%20introduce%20a%20plug-and-play%20module%0Athat%20significantly%20ameliorates%20misalignment%20issues.%20Second%2C%20we%20propose%20a%0Acontext-aware%20adaptive%20density%20control%20strategy%2C%20which%20is%20adaptively%20adjusting%0Athe%20gradient%20thresholds%20to%20accommodate%20the%20varied%20granularity%20across%20body%0Aparts.%20Last%20but%20not%20least%2C%20we%20develop%20a%20feedback%20mechanism%20that%20predicts%0Aper-pixel%20confidence%20to%20better%20guide%20the%20learning%20of%203D%20Gaussians.%20Extensive%0Aexperiments%20on%20two%20benchmarks%20demonstrate%20the%20superiority%20of%20our%20framework%20both%0Aquantitatively%20and%20qualitatively%2C%20especially%20on%20the%20fine-grained%20hand%20and%0Afacial%20details.%20See%20the%20project%20website%20at%20%5Curl%7Bhttps%3A//evahuman.github.io%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03204v1&entry.124074799=Read"},
{"title": "A Unified Framework for 3D Scene Understanding", "author": "Wei Xu and Chunsheng Shi and Sifan Tu and Xin Zhou and Dingkang Liang and Xiang Bai", "abstract": "  We propose UniSeg3D, a unified 3D segmentation framework that achieves\npanoptic, semantic, instance, interactive, referring, and open-vocabulary\nsemantic segmentation tasks within a single model. Most previous 3D\nsegmentation approaches are specialized for a specific task, thereby limiting\ntheir understanding of 3D scenes to a task-specific perspective. In contrast,\nthe proposed method unifies six tasks into unified representations processed by\nthe same Transformer. It facilitates inter-task knowledge sharing and,\ntherefore, promotes comprehensive 3D scene understanding. To take advantage of\nmulti-task unification, we enhance the performance by leveraging task\nconnections. Specifically, we design a knowledge distillation method and a\ncontrastive learning method to transfer task-specific knowledge across\ndifferent tasks. Benefiting from extensive inter-task knowledge sharing, our\nUniSeg3D becomes more powerful. Experiments on three benchmarks, including the\nScanNet20, ScanRefer, and ScanNet200, demonstrate that the UniSeg3D\nconsistently outperforms current SOTA methods, even those specialized for\nindividual tasks. We hope UniSeg3D can serve as a solid unified baseline and\ninspire future work. The code will be available at\nhttps://dk-liang.github.io/UniSeg3D/.\n", "link": "http://arxiv.org/abs/2407.03263v1", "date": "2024-07-03", "relevancy": 3.021, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6205}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6081}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Unified%20Framework%20for%203D%20Scene%20Understanding&body=Title%3A%20A%20Unified%20Framework%20for%203D%20Scene%20Understanding%0AAuthor%3A%20Wei%20Xu%20and%20Chunsheng%20Shi%20and%20Sifan%20Tu%20and%20Xin%20Zhou%20and%20Dingkang%20Liang%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20We%20propose%20UniSeg3D%2C%20a%20unified%203D%20segmentation%20framework%20that%20achieves%0Apanoptic%2C%20semantic%2C%20instance%2C%20interactive%2C%20referring%2C%20and%20open-vocabulary%0Asemantic%20segmentation%20tasks%20within%20a%20single%20model.%20Most%20previous%203D%0Asegmentation%20approaches%20are%20specialized%20for%20a%20specific%20task%2C%20thereby%20limiting%0Atheir%20understanding%20of%203D%20scenes%20to%20a%20task-specific%20perspective.%20In%20contrast%2C%0Athe%20proposed%20method%20unifies%20six%20tasks%20into%20unified%20representations%20processed%20by%0Athe%20same%20Transformer.%20It%20facilitates%20inter-task%20knowledge%20sharing%20and%2C%0Atherefore%2C%20promotes%20comprehensive%203D%20scene%20understanding.%20To%20take%20advantage%20of%0Amulti-task%20unification%2C%20we%20enhance%20the%20performance%20by%20leveraging%20task%0Aconnections.%20Specifically%2C%20we%20design%20a%20knowledge%20distillation%20method%20and%20a%0Acontrastive%20learning%20method%20to%20transfer%20task-specific%20knowledge%20across%0Adifferent%20tasks.%20Benefiting%20from%20extensive%20inter-task%20knowledge%20sharing%2C%20our%0AUniSeg3D%20becomes%20more%20powerful.%20Experiments%20on%20three%20benchmarks%2C%20including%20the%0AScanNet20%2C%20ScanRefer%2C%20and%20ScanNet200%2C%20demonstrate%20that%20the%20UniSeg3D%0Aconsistently%20outperforms%20current%20SOTA%20methods%2C%20even%20those%20specialized%20for%0Aindividual%20tasks.%20We%20hope%20UniSeg3D%20can%20serve%20as%20a%20solid%20unified%20baseline%20and%0Ainspire%20future%20work.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//dk-liang.github.io/UniSeg3D/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03263v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Unified%2520Framework%2520for%25203D%2520Scene%2520Understanding%26entry.906535625%3DWei%2520Xu%2520and%2520Chunsheng%2520Shi%2520and%2520Sifan%2520Tu%2520and%2520Xin%2520Zhou%2520and%2520Dingkang%2520Liang%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520We%2520propose%2520UniSeg3D%252C%2520a%2520unified%25203D%2520segmentation%2520framework%2520that%2520achieves%250Apanoptic%252C%2520semantic%252C%2520instance%252C%2520interactive%252C%2520referring%252C%2520and%2520open-vocabulary%250Asemantic%2520segmentation%2520tasks%2520within%2520a%2520single%2520model.%2520Most%2520previous%25203D%250Asegmentation%2520approaches%2520are%2520specialized%2520for%2520a%2520specific%2520task%252C%2520thereby%2520limiting%250Atheir%2520understanding%2520of%25203D%2520scenes%2520to%2520a%2520task-specific%2520perspective.%2520In%2520contrast%252C%250Athe%2520proposed%2520method%2520unifies%2520six%2520tasks%2520into%2520unified%2520representations%2520processed%2520by%250Athe%2520same%2520Transformer.%2520It%2520facilitates%2520inter-task%2520knowledge%2520sharing%2520and%252C%250Atherefore%252C%2520promotes%2520comprehensive%25203D%2520scene%2520understanding.%2520To%2520take%2520advantage%2520of%250Amulti-task%2520unification%252C%2520we%2520enhance%2520the%2520performance%2520by%2520leveraging%2520task%250Aconnections.%2520Specifically%252C%2520we%2520design%2520a%2520knowledge%2520distillation%2520method%2520and%2520a%250Acontrastive%2520learning%2520method%2520to%2520transfer%2520task-specific%2520knowledge%2520across%250Adifferent%2520tasks.%2520Benefiting%2520from%2520extensive%2520inter-task%2520knowledge%2520sharing%252C%2520our%250AUniSeg3D%2520becomes%2520more%2520powerful.%2520Experiments%2520on%2520three%2520benchmarks%252C%2520including%2520the%250AScanNet20%252C%2520ScanRefer%252C%2520and%2520ScanNet200%252C%2520demonstrate%2520that%2520the%2520UniSeg3D%250Aconsistently%2520outperforms%2520current%2520SOTA%2520methods%252C%2520even%2520those%2520specialized%2520for%250Aindividual%2520tasks.%2520We%2520hope%2520UniSeg3D%2520can%2520serve%2520as%2520a%2520solid%2520unified%2520baseline%2520and%250Ainspire%2520future%2520work.%2520The%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//dk-liang.github.io/UniSeg3D/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03263v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Unified%20Framework%20for%203D%20Scene%20Understanding&entry.906535625=Wei%20Xu%20and%20Chunsheng%20Shi%20and%20Sifan%20Tu%20and%20Xin%20Zhou%20and%20Dingkang%20Liang%20and%20Xiang%20Bai&entry.1292438233=%20%20We%20propose%20UniSeg3D%2C%20a%20unified%203D%20segmentation%20framework%20that%20achieves%0Apanoptic%2C%20semantic%2C%20instance%2C%20interactive%2C%20referring%2C%20and%20open-vocabulary%0Asemantic%20segmentation%20tasks%20within%20a%20single%20model.%20Most%20previous%203D%0Asegmentation%20approaches%20are%20specialized%20for%20a%20specific%20task%2C%20thereby%20limiting%0Atheir%20understanding%20of%203D%20scenes%20to%20a%20task-specific%20perspective.%20In%20contrast%2C%0Athe%20proposed%20method%20unifies%20six%20tasks%20into%20unified%20representations%20processed%20by%0Athe%20same%20Transformer.%20It%20facilitates%20inter-task%20knowledge%20sharing%20and%2C%0Atherefore%2C%20promotes%20comprehensive%203D%20scene%20understanding.%20To%20take%20advantage%20of%0Amulti-task%20unification%2C%20we%20enhance%20the%20performance%20by%20leveraging%20task%0Aconnections.%20Specifically%2C%20we%20design%20a%20knowledge%20distillation%20method%20and%20a%0Acontrastive%20learning%20method%20to%20transfer%20task-specific%20knowledge%20across%0Adifferent%20tasks.%20Benefiting%20from%20extensive%20inter-task%20knowledge%20sharing%2C%20our%0AUniSeg3D%20becomes%20more%20powerful.%20Experiments%20on%20three%20benchmarks%2C%20including%20the%0AScanNet20%2C%20ScanRefer%2C%20and%20ScanNet200%2C%20demonstrate%20that%20the%20UniSeg3D%0Aconsistently%20outperforms%20current%20SOTA%20methods%2C%20even%20those%20specialized%20for%0Aindividual%20tasks.%20We%20hope%20UniSeg3D%20can%20serve%20as%20a%20solid%20unified%20baseline%20and%0Ainspire%20future%20work.%20The%20code%20will%20be%20available%20at%0Ahttps%3A//dk-liang.github.io/UniSeg3D/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03263v1&entry.124074799=Read"},
{"title": "Cyclic Refiner: Object-Aware Temporal Representation Learning for\n  Multi-View 3D Detection and Tracking", "author": "Mingzhe Guo and Zhipeng Zhang and Liping Jing and Yuan He and Ke Wang and Heng Fan", "abstract": "  We propose a unified object-aware temporal learning framework for multi-view\n3D detection and tracking tasks. Having observed that the efficacy of the\ntemporal fusion strategy in recent multi-view perception methods may be\nweakened by distractors and background clutters in historical frames, we\npropose a cyclic learning mechanism to improve the robustness of multi-view\nrepresentation learning. The essence is constructing a backward bridge to\npropagate information from model predictions (e.g., object locations and sizes)\nto image and BEV features, which forms a circle with regular inference. After\nbackward refinement, the responses of target-irrelevant regions in historical\nframes would be suppressed, decreasing the risk of polluting future frames and\nimproving the object awareness ability of temporal fusion. We further tailor an\nobject-aware association strategy for tracking based on the cyclic learning\nmodel. The cyclic learning model not only provides refined features, but also\ndelivers finer clues (e.g., scale level) for tracklet association. The proposed\ncycle learning method and association module together contribute a novel and\nunified multi-task framework. Experiments on nuScenes show that the proposed\nmodel achieves consistent performance gains over baselines of different designs\n(i.e., dense query-based BEVFormer, sparse query-based SparseBEV and LSS-based\nBEVDet4D) on both detection and tracking evaluation.\n", "link": "http://arxiv.org/abs/2407.03240v1", "date": "2024-07-03", "relevancy": 2.9767, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.611}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5906}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5845}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cyclic%20Refiner%3A%20Object-Aware%20Temporal%20Representation%20Learning%20for%0A%20%20Multi-View%203D%20Detection%20and%20Tracking&body=Title%3A%20Cyclic%20Refiner%3A%20Object-Aware%20Temporal%20Representation%20Learning%20for%0A%20%20Multi-View%203D%20Detection%20and%20Tracking%0AAuthor%3A%20Mingzhe%20Guo%20and%20Zhipeng%20Zhang%20and%20Liping%20Jing%20and%20Yuan%20He%20and%20Ke%20Wang%20and%20Heng%20Fan%0AAbstract%3A%20%20%20We%20propose%20a%20unified%20object-aware%20temporal%20learning%20framework%20for%20multi-view%0A3D%20detection%20and%20tracking%20tasks.%20Having%20observed%20that%20the%20efficacy%20of%20the%0Atemporal%20fusion%20strategy%20in%20recent%20multi-view%20perception%20methods%20may%20be%0Aweakened%20by%20distractors%20and%20background%20clutters%20in%20historical%20frames%2C%20we%0Apropose%20a%20cyclic%20learning%20mechanism%20to%20improve%20the%20robustness%20of%20multi-view%0Arepresentation%20learning.%20The%20essence%20is%20constructing%20a%20backward%20bridge%20to%0Apropagate%20information%20from%20model%20predictions%20%28e.g.%2C%20object%20locations%20and%20sizes%29%0Ato%20image%20and%20BEV%20features%2C%20which%20forms%20a%20circle%20with%20regular%20inference.%20After%0Abackward%20refinement%2C%20the%20responses%20of%20target-irrelevant%20regions%20in%20historical%0Aframes%20would%20be%20suppressed%2C%20decreasing%20the%20risk%20of%20polluting%20future%20frames%20and%0Aimproving%20the%20object%20awareness%20ability%20of%20temporal%20fusion.%20We%20further%20tailor%20an%0Aobject-aware%20association%20strategy%20for%20tracking%20based%20on%20the%20cyclic%20learning%0Amodel.%20The%20cyclic%20learning%20model%20not%20only%20provides%20refined%20features%2C%20but%20also%0Adelivers%20finer%20clues%20%28e.g.%2C%20scale%20level%29%20for%20tracklet%20association.%20The%20proposed%0Acycle%20learning%20method%20and%20association%20module%20together%20contribute%20a%20novel%20and%0Aunified%20multi-task%20framework.%20Experiments%20on%20nuScenes%20show%20that%20the%20proposed%0Amodel%20achieves%20consistent%20performance%20gains%20over%20baselines%20of%20different%20designs%0A%28i.e.%2C%20dense%20query-based%20BEVFormer%2C%20sparse%20query-based%20SparseBEV%20and%20LSS-based%0ABEVDet4D%29%20on%20both%20detection%20and%20tracking%20evaluation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03240v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCyclic%2520Refiner%253A%2520Object-Aware%2520Temporal%2520Representation%2520Learning%2520for%250A%2520%2520Multi-View%25203D%2520Detection%2520and%2520Tracking%26entry.906535625%3DMingzhe%2520Guo%2520and%2520Zhipeng%2520Zhang%2520and%2520Liping%2520Jing%2520and%2520Yuan%2520He%2520and%2520Ke%2520Wang%2520and%2520Heng%2520Fan%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520unified%2520object-aware%2520temporal%2520learning%2520framework%2520for%2520multi-view%250A3D%2520detection%2520and%2520tracking%2520tasks.%2520Having%2520observed%2520that%2520the%2520efficacy%2520of%2520the%250Atemporal%2520fusion%2520strategy%2520in%2520recent%2520multi-view%2520perception%2520methods%2520may%2520be%250Aweakened%2520by%2520distractors%2520and%2520background%2520clutters%2520in%2520historical%2520frames%252C%2520we%250Apropose%2520a%2520cyclic%2520learning%2520mechanism%2520to%2520improve%2520the%2520robustness%2520of%2520multi-view%250Arepresentation%2520learning.%2520The%2520essence%2520is%2520constructing%2520a%2520backward%2520bridge%2520to%250Apropagate%2520information%2520from%2520model%2520predictions%2520%2528e.g.%252C%2520object%2520locations%2520and%2520sizes%2529%250Ato%2520image%2520and%2520BEV%2520features%252C%2520which%2520forms%2520a%2520circle%2520with%2520regular%2520inference.%2520After%250Abackward%2520refinement%252C%2520the%2520responses%2520of%2520target-irrelevant%2520regions%2520in%2520historical%250Aframes%2520would%2520be%2520suppressed%252C%2520decreasing%2520the%2520risk%2520of%2520polluting%2520future%2520frames%2520and%250Aimproving%2520the%2520object%2520awareness%2520ability%2520of%2520temporal%2520fusion.%2520We%2520further%2520tailor%2520an%250Aobject-aware%2520association%2520strategy%2520for%2520tracking%2520based%2520on%2520the%2520cyclic%2520learning%250Amodel.%2520The%2520cyclic%2520learning%2520model%2520not%2520only%2520provides%2520refined%2520features%252C%2520but%2520also%250Adelivers%2520finer%2520clues%2520%2528e.g.%252C%2520scale%2520level%2529%2520for%2520tracklet%2520association.%2520The%2520proposed%250Acycle%2520learning%2520method%2520and%2520association%2520module%2520together%2520contribute%2520a%2520novel%2520and%250Aunified%2520multi-task%2520framework.%2520Experiments%2520on%2520nuScenes%2520show%2520that%2520the%2520proposed%250Amodel%2520achieves%2520consistent%2520performance%2520gains%2520over%2520baselines%2520of%2520different%2520designs%250A%2528i.e.%252C%2520dense%2520query-based%2520BEVFormer%252C%2520sparse%2520query-based%2520SparseBEV%2520and%2520LSS-based%250ABEVDet4D%2529%2520on%2520both%2520detection%2520and%2520tracking%2520evaluation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03240v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cyclic%20Refiner%3A%20Object-Aware%20Temporal%20Representation%20Learning%20for%0A%20%20Multi-View%203D%20Detection%20and%20Tracking&entry.906535625=Mingzhe%20Guo%20and%20Zhipeng%20Zhang%20and%20Liping%20Jing%20and%20Yuan%20He%20and%20Ke%20Wang%20and%20Heng%20Fan&entry.1292438233=%20%20We%20propose%20a%20unified%20object-aware%20temporal%20learning%20framework%20for%20multi-view%0A3D%20detection%20and%20tracking%20tasks.%20Having%20observed%20that%20the%20efficacy%20of%20the%0Atemporal%20fusion%20strategy%20in%20recent%20multi-view%20perception%20methods%20may%20be%0Aweakened%20by%20distractors%20and%20background%20clutters%20in%20historical%20frames%2C%20we%0Apropose%20a%20cyclic%20learning%20mechanism%20to%20improve%20the%20robustness%20of%20multi-view%0Arepresentation%20learning.%20The%20essence%20is%20constructing%20a%20backward%20bridge%20to%0Apropagate%20information%20from%20model%20predictions%20%28e.g.%2C%20object%20locations%20and%20sizes%29%0Ato%20image%20and%20BEV%20features%2C%20which%20forms%20a%20circle%20with%20regular%20inference.%20After%0Abackward%20refinement%2C%20the%20responses%20of%20target-irrelevant%20regions%20in%20historical%0Aframes%20would%20be%20suppressed%2C%20decreasing%20the%20risk%20of%20polluting%20future%20frames%20and%0Aimproving%20the%20object%20awareness%20ability%20of%20temporal%20fusion.%20We%20further%20tailor%20an%0Aobject-aware%20association%20strategy%20for%20tracking%20based%20on%20the%20cyclic%20learning%0Amodel.%20The%20cyclic%20learning%20model%20not%20only%20provides%20refined%20features%2C%20but%20also%0Adelivers%20finer%20clues%20%28e.g.%2C%20scale%20level%29%20for%20tracklet%20association.%20The%20proposed%0Acycle%20learning%20method%20and%20association%20module%20together%20contribute%20a%20novel%20and%0Aunified%20multi-task%20framework.%20Experiments%20on%20nuScenes%20show%20that%20the%20proposed%0Amodel%20achieves%20consistent%20performance%20gains%20over%20baselines%20of%20different%20designs%0A%28i.e.%2C%20dense%20query-based%20BEVFormer%2C%20sparse%20query-based%20SparseBEV%20and%20LSS-based%0ABEVDet4D%29%20on%20both%20detection%20and%20tracking%20evaluation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03240v1&entry.124074799=Read"},
{"title": "EarthMatch: Iterative Coregistration for Fine-grained Localization of\n  Astronaut Photography", "author": "Gabriele Berton and Gabriele Goletto and Gabriele Trivigno and Alex Stoken and Barbara Caputo and Carlo Masone", "abstract": "  Precise, pixel-wise geolocalization of astronaut photography is critical to\nunlocking the potential of this unique type of remotely sensed Earth data,\nparticularly for its use in disaster management and climate change research.\nRecent works have established the Astronaut Photography Localization task, but\nhave either proved too costly for mass deployment or generated too coarse a\nlocalization. Thus, we present EarthMatch, an iterative homography estimation\nmethod that produces fine-grained localization of astronaut photographs while\nmaintaining an emphasis on speed. We refocus the astronaut photography\nbenchmark, AIMS, on the geolocalization task itself, and prove our method's\nefficacy on this dataset. In addition, we offer a new, fair method for image\nmatcher comparison, and an extensive evaluation of different matching models\nwithin our localization pipeline. Our method will enable fast and accurate\nlocalization of the 4.5 million and growing collection of astronaut photography\nof Earth. Webpage with code and data at\nhttps://earthloc-and-earthmatch.github.io\n", "link": "http://arxiv.org/abs/2405.05422v2", "date": "2024-07-03", "relevancy": 2.8586, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5954}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5912}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EarthMatch%3A%20Iterative%20Coregistration%20for%20Fine-grained%20Localization%20of%0A%20%20Astronaut%20Photography&body=Title%3A%20EarthMatch%3A%20Iterative%20Coregistration%20for%20Fine-grained%20Localization%20of%0A%20%20Astronaut%20Photography%0AAuthor%3A%20Gabriele%20Berton%20and%20Gabriele%20Goletto%20and%20Gabriele%20Trivigno%20and%20Alex%20Stoken%20and%20Barbara%20Caputo%20and%20Carlo%20Masone%0AAbstract%3A%20%20%20Precise%2C%20pixel-wise%20geolocalization%20of%20astronaut%20photography%20is%20critical%20to%0Aunlocking%20the%20potential%20of%20this%20unique%20type%20of%20remotely%20sensed%20Earth%20data%2C%0Aparticularly%20for%20its%20use%20in%20disaster%20management%20and%20climate%20change%20research.%0ARecent%20works%20have%20established%20the%20Astronaut%20Photography%20Localization%20task%2C%20but%0Ahave%20either%20proved%20too%20costly%20for%20mass%20deployment%20or%20generated%20too%20coarse%20a%0Alocalization.%20Thus%2C%20we%20present%20EarthMatch%2C%20an%20iterative%20homography%20estimation%0Amethod%20that%20produces%20fine-grained%20localization%20of%20astronaut%20photographs%20while%0Amaintaining%20an%20emphasis%20on%20speed.%20We%20refocus%20the%20astronaut%20photography%0Abenchmark%2C%20AIMS%2C%20on%20the%20geolocalization%20task%20itself%2C%20and%20prove%20our%20method%27s%0Aefficacy%20on%20this%20dataset.%20In%20addition%2C%20we%20offer%20a%20new%2C%20fair%20method%20for%20image%0Amatcher%20comparison%2C%20and%20an%20extensive%20evaluation%20of%20different%20matching%20models%0Awithin%20our%20localization%20pipeline.%20Our%20method%20will%20enable%20fast%20and%20accurate%0Alocalization%20of%20the%204.5%20million%20and%20growing%20collection%20of%20astronaut%20photography%0Aof%20Earth.%20Webpage%20with%20code%20and%20data%20at%0Ahttps%3A//earthloc-and-earthmatch.github.io%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05422v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEarthMatch%253A%2520Iterative%2520Coregistration%2520for%2520Fine-grained%2520Localization%2520of%250A%2520%2520Astronaut%2520Photography%26entry.906535625%3DGabriele%2520Berton%2520and%2520Gabriele%2520Goletto%2520and%2520Gabriele%2520Trivigno%2520and%2520Alex%2520Stoken%2520and%2520Barbara%2520Caputo%2520and%2520Carlo%2520Masone%26entry.1292438233%3D%2520%2520Precise%252C%2520pixel-wise%2520geolocalization%2520of%2520astronaut%2520photography%2520is%2520critical%2520to%250Aunlocking%2520the%2520potential%2520of%2520this%2520unique%2520type%2520of%2520remotely%2520sensed%2520Earth%2520data%252C%250Aparticularly%2520for%2520its%2520use%2520in%2520disaster%2520management%2520and%2520climate%2520change%2520research.%250ARecent%2520works%2520have%2520established%2520the%2520Astronaut%2520Photography%2520Localization%2520task%252C%2520but%250Ahave%2520either%2520proved%2520too%2520costly%2520for%2520mass%2520deployment%2520or%2520generated%2520too%2520coarse%2520a%250Alocalization.%2520Thus%252C%2520we%2520present%2520EarthMatch%252C%2520an%2520iterative%2520homography%2520estimation%250Amethod%2520that%2520produces%2520fine-grained%2520localization%2520of%2520astronaut%2520photographs%2520while%250Amaintaining%2520an%2520emphasis%2520on%2520speed.%2520We%2520refocus%2520the%2520astronaut%2520photography%250Abenchmark%252C%2520AIMS%252C%2520on%2520the%2520geolocalization%2520task%2520itself%252C%2520and%2520prove%2520our%2520method%2527s%250Aefficacy%2520on%2520this%2520dataset.%2520In%2520addition%252C%2520we%2520offer%2520a%2520new%252C%2520fair%2520method%2520for%2520image%250Amatcher%2520comparison%252C%2520and%2520an%2520extensive%2520evaluation%2520of%2520different%2520matching%2520models%250Awithin%2520our%2520localization%2520pipeline.%2520Our%2520method%2520will%2520enable%2520fast%2520and%2520accurate%250Alocalization%2520of%2520the%25204.5%2520million%2520and%2520growing%2520collection%2520of%2520astronaut%2520photography%250Aof%2520Earth.%2520Webpage%2520with%2520code%2520and%2520data%2520at%250Ahttps%253A//earthloc-and-earthmatch.github.io%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05422v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EarthMatch%3A%20Iterative%20Coregistration%20for%20Fine-grained%20Localization%20of%0A%20%20Astronaut%20Photography&entry.906535625=Gabriele%20Berton%20and%20Gabriele%20Goletto%20and%20Gabriele%20Trivigno%20and%20Alex%20Stoken%20and%20Barbara%20Caputo%20and%20Carlo%20Masone&entry.1292438233=%20%20Precise%2C%20pixel-wise%20geolocalization%20of%20astronaut%20photography%20is%20critical%20to%0Aunlocking%20the%20potential%20of%20this%20unique%20type%20of%20remotely%20sensed%20Earth%20data%2C%0Aparticularly%20for%20its%20use%20in%20disaster%20management%20and%20climate%20change%20research.%0ARecent%20works%20have%20established%20the%20Astronaut%20Photography%20Localization%20task%2C%20but%0Ahave%20either%20proved%20too%20costly%20for%20mass%20deployment%20or%20generated%20too%20coarse%20a%0Alocalization.%20Thus%2C%20we%20present%20EarthMatch%2C%20an%20iterative%20homography%20estimation%0Amethod%20that%20produces%20fine-grained%20localization%20of%20astronaut%20photographs%20while%0Amaintaining%20an%20emphasis%20on%20speed.%20We%20refocus%20the%20astronaut%20photography%0Abenchmark%2C%20AIMS%2C%20on%20the%20geolocalization%20task%20itself%2C%20and%20prove%20our%20method%27s%0Aefficacy%20on%20this%20dataset.%20In%20addition%2C%20we%20offer%20a%20new%2C%20fair%20method%20for%20image%0Amatcher%20comparison%2C%20and%20an%20extensive%20evaluation%20of%20different%20matching%20models%0Awithin%20our%20localization%20pipeline.%20Our%20method%20will%20enable%20fast%20and%20accurate%0Alocalization%20of%20the%204.5%20million%20and%20growing%20collection%20of%20astronaut%20photography%0Aof%20Earth.%20Webpage%20with%20code%20and%20data%20at%0Ahttps%3A//earthloc-and-earthmatch.github.io%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05422v2&entry.124074799=Read"},
{"title": "DyFADet: Dynamic Feature Aggregation for Temporal Action Detection", "author": "Le Yang and Ziwei Zheng and Yizeng Han and Hao Cheng and Shiji Song and Gao Huang and Fan Li", "abstract": "  Recent proposed neural network-based Temporal Action Detection (TAD) models\nare inherently limited to extracting the discriminative representations and\nmodeling action instances with various lengths from complex scenes by\nshared-weights detection heads. Inspired by the successes in dynamic neural\nnetworks, in this paper, we build a novel dynamic feature aggregation (DFA)\nmodule that can simultaneously adapt kernel weights and receptive fields at\ndifferent timestamps. Based on DFA, the proposed dynamic encoder layer\naggregates the temporal features within the action time ranges and guarantees\nthe discriminability of the extracted representations. Moreover, using DFA\nhelps to develop a Dynamic TAD head (DyHead), which adaptively aggregates the\nmulti-scale features with adjusted parameters and learned receptive fields\nbetter to detect the action instances with diverse ranges from videos. With the\nproposed encoder layer and DyHead, a new dynamic TAD model, DyFADet, achieves\npromising performance on a series of challenging TAD benchmarks, including\nHACS-Segment, THUMOS14, ActivityNet-1.3, Epic-Kitchen 100, Ego4D-Moment\nQueriesV1.0, and FineAction. Code is released to\nhttps://github.com/yangle15/DyFADet-pytorch.\n", "link": "http://arxiv.org/abs/2407.03197v1", "date": "2024-07-03", "relevancy": 2.858, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.6211}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5601}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5336}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DyFADet%3A%20Dynamic%20Feature%20Aggregation%20for%20Temporal%20Action%20Detection&body=Title%3A%20DyFADet%3A%20Dynamic%20Feature%20Aggregation%20for%20Temporal%20Action%20Detection%0AAuthor%3A%20Le%20Yang%20and%20Ziwei%20Zheng%20and%20Yizeng%20Han%20and%20Hao%20Cheng%20and%20Shiji%20Song%20and%20Gao%20Huang%20and%20Fan%20Li%0AAbstract%3A%20%20%20Recent%20proposed%20neural%20network-based%20Temporal%20Action%20Detection%20%28TAD%29%20models%0Aare%20inherently%20limited%20to%20extracting%20the%20discriminative%20representations%20and%0Amodeling%20action%20instances%20with%20various%20lengths%20from%20complex%20scenes%20by%0Ashared-weights%20detection%20heads.%20Inspired%20by%20the%20successes%20in%20dynamic%20neural%0Anetworks%2C%20in%20this%20paper%2C%20we%20build%20a%20novel%20dynamic%20feature%20aggregation%20%28DFA%29%0Amodule%20that%20can%20simultaneously%20adapt%20kernel%20weights%20and%20receptive%20fields%20at%0Adifferent%20timestamps.%20Based%20on%20DFA%2C%20the%20proposed%20dynamic%20encoder%20layer%0Aaggregates%20the%20temporal%20features%20within%20the%20action%20time%20ranges%20and%20guarantees%0Athe%20discriminability%20of%20the%20extracted%20representations.%20Moreover%2C%20using%20DFA%0Ahelps%20to%20develop%20a%20Dynamic%20TAD%20head%20%28DyHead%29%2C%20which%20adaptively%20aggregates%20the%0Amulti-scale%20features%20with%20adjusted%20parameters%20and%20learned%20receptive%20fields%0Abetter%20to%20detect%20the%20action%20instances%20with%20diverse%20ranges%20from%20videos.%20With%20the%0Aproposed%20encoder%20layer%20and%20DyHead%2C%20a%20new%20dynamic%20TAD%20model%2C%20DyFADet%2C%20achieves%0Apromising%20performance%20on%20a%20series%20of%20challenging%20TAD%20benchmarks%2C%20including%0AHACS-Segment%2C%20THUMOS14%2C%20ActivityNet-1.3%2C%20Epic-Kitchen%20100%2C%20Ego4D-Moment%0AQueriesV1.0%2C%20and%20FineAction.%20Code%20is%20released%20to%0Ahttps%3A//github.com/yangle15/DyFADet-pytorch.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03197v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDyFADet%253A%2520Dynamic%2520Feature%2520Aggregation%2520for%2520Temporal%2520Action%2520Detection%26entry.906535625%3DLe%2520Yang%2520and%2520Ziwei%2520Zheng%2520and%2520Yizeng%2520Han%2520and%2520Hao%2520Cheng%2520and%2520Shiji%2520Song%2520and%2520Gao%2520Huang%2520and%2520Fan%2520Li%26entry.1292438233%3D%2520%2520Recent%2520proposed%2520neural%2520network-based%2520Temporal%2520Action%2520Detection%2520%2528TAD%2529%2520models%250Aare%2520inherently%2520limited%2520to%2520extracting%2520the%2520discriminative%2520representations%2520and%250Amodeling%2520action%2520instances%2520with%2520various%2520lengths%2520from%2520complex%2520scenes%2520by%250Ashared-weights%2520detection%2520heads.%2520Inspired%2520by%2520the%2520successes%2520in%2520dynamic%2520neural%250Anetworks%252C%2520in%2520this%2520paper%252C%2520we%2520build%2520a%2520novel%2520dynamic%2520feature%2520aggregation%2520%2528DFA%2529%250Amodule%2520that%2520can%2520simultaneously%2520adapt%2520kernel%2520weights%2520and%2520receptive%2520fields%2520at%250Adifferent%2520timestamps.%2520Based%2520on%2520DFA%252C%2520the%2520proposed%2520dynamic%2520encoder%2520layer%250Aaggregates%2520the%2520temporal%2520features%2520within%2520the%2520action%2520time%2520ranges%2520and%2520guarantees%250Athe%2520discriminability%2520of%2520the%2520extracted%2520representations.%2520Moreover%252C%2520using%2520DFA%250Ahelps%2520to%2520develop%2520a%2520Dynamic%2520TAD%2520head%2520%2528DyHead%2529%252C%2520which%2520adaptively%2520aggregates%2520the%250Amulti-scale%2520features%2520with%2520adjusted%2520parameters%2520and%2520learned%2520receptive%2520fields%250Abetter%2520to%2520detect%2520the%2520action%2520instances%2520with%2520diverse%2520ranges%2520from%2520videos.%2520With%2520the%250Aproposed%2520encoder%2520layer%2520and%2520DyHead%252C%2520a%2520new%2520dynamic%2520TAD%2520model%252C%2520DyFADet%252C%2520achieves%250Apromising%2520performance%2520on%2520a%2520series%2520of%2520challenging%2520TAD%2520benchmarks%252C%2520including%250AHACS-Segment%252C%2520THUMOS14%252C%2520ActivityNet-1.3%252C%2520Epic-Kitchen%2520100%252C%2520Ego4D-Moment%250AQueriesV1.0%252C%2520and%2520FineAction.%2520Code%2520is%2520released%2520to%250Ahttps%253A//github.com/yangle15/DyFADet-pytorch.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03197v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DyFADet%3A%20Dynamic%20Feature%20Aggregation%20for%20Temporal%20Action%20Detection&entry.906535625=Le%20Yang%20and%20Ziwei%20Zheng%20and%20Yizeng%20Han%20and%20Hao%20Cheng%20and%20Shiji%20Song%20and%20Gao%20Huang%20and%20Fan%20Li&entry.1292438233=%20%20Recent%20proposed%20neural%20network-based%20Temporal%20Action%20Detection%20%28TAD%29%20models%0Aare%20inherently%20limited%20to%20extracting%20the%20discriminative%20representations%20and%0Amodeling%20action%20instances%20with%20various%20lengths%20from%20complex%20scenes%20by%0Ashared-weights%20detection%20heads.%20Inspired%20by%20the%20successes%20in%20dynamic%20neural%0Anetworks%2C%20in%20this%20paper%2C%20we%20build%20a%20novel%20dynamic%20feature%20aggregation%20%28DFA%29%0Amodule%20that%20can%20simultaneously%20adapt%20kernel%20weights%20and%20receptive%20fields%20at%0Adifferent%20timestamps.%20Based%20on%20DFA%2C%20the%20proposed%20dynamic%20encoder%20layer%0Aaggregates%20the%20temporal%20features%20within%20the%20action%20time%20ranges%20and%20guarantees%0Athe%20discriminability%20of%20the%20extracted%20representations.%20Moreover%2C%20using%20DFA%0Ahelps%20to%20develop%20a%20Dynamic%20TAD%20head%20%28DyHead%29%2C%20which%20adaptively%20aggregates%20the%0Amulti-scale%20features%20with%20adjusted%20parameters%20and%20learned%20receptive%20fields%0Abetter%20to%20detect%20the%20action%20instances%20with%20diverse%20ranges%20from%20videos.%20With%20the%0Aproposed%20encoder%20layer%20and%20DyHead%2C%20a%20new%20dynamic%20TAD%20model%2C%20DyFADet%2C%20achieves%0Apromising%20performance%20on%20a%20series%20of%20challenging%20TAD%20benchmarks%2C%20including%0AHACS-Segment%2C%20THUMOS14%2C%20ActivityNet-1.3%2C%20Epic-Kitchen%20100%2C%20Ego4D-Moment%0AQueriesV1.0%2C%20and%20FineAction.%20Code%20is%20released%20to%0Ahttps%3A//github.com/yangle15/DyFADet-pytorch.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03197v1&entry.124074799=Read"},
{"title": "DialogGen: Multi-modal Interactive Dialogue System for Multi-turn\n  Text-to-Image Generation", "author": "Minbin Huang and Yanxin Long and Xinchi Deng and Ruihang Chu and Jiangfeng Xiong and Xiaodan Liang and Hong Cheng and Qinglin Lu and Wei Liu", "abstract": "  Text-to-image (T2I) generation models have significantly advanced in recent\nyears. However, effective interaction with these models is challenging for\naverage users due to the need for specialized prompt engineering knowledge and\nthe inability to perform multi-turn image generation, hindering a dynamic and\niterative creation process. Recent attempts have tried to equip Multi-modal\nLarge Language Models (MLLMs) with T2I models to bring the user's natural\nlanguage instructions into reality. Hence, the output modality of MLLMs is\nextended, and the multi-turn generation quality of T2I models is enhanced\nthanks to the strong multi-modal comprehension ability of MLLMs. However, many\nof these works face challenges in identifying correct output modalities and\ngenerating coherent images accordingly as the number of output modalities\nincreases and the conversations go deeper. Therefore, we propose DialogGen, an\neffective pipeline to align off-the-shelf MLLMs and T2I models to build a\nMulti-modal Interactive Dialogue System (MIDS) for multi-turn Text-to-Image\ngeneration. It is composed of drawing prompt alignment, careful training data\ncuration, and error correction. Moreover, as the field of MIDS flourishes,\ncomprehensive benchmarks are urgently needed to evaluate MIDS fairly in terms\nof output modality correctness and multi-modal output coherence. To address\nthis issue, we introduce the Multi-modal Dialogue Benchmark (DialogBen), a\ncomprehensive bilingual benchmark designed to assess the ability of MLLMs to\ngenerate accurate and coherent multi-modal content that supports image editing.\nIt contains two evaluation metrics to measure the model's ability to switch\nmodalities and the coherence of the output images. Our extensive experiments on\nDialogBen and user study demonstrate the effectiveness of DialogGen compared\nwith other State-of-the-Art models.\n", "link": "http://arxiv.org/abs/2403.08857v2", "date": "2024-07-03", "relevancy": 2.8332, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5783}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5676}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DialogGen%3A%20Multi-modal%20Interactive%20Dialogue%20System%20for%20Multi-turn%0A%20%20Text-to-Image%20Generation&body=Title%3A%20DialogGen%3A%20Multi-modal%20Interactive%20Dialogue%20System%20for%20Multi-turn%0A%20%20Text-to-Image%20Generation%0AAuthor%3A%20Minbin%20Huang%20and%20Yanxin%20Long%20and%20Xinchi%20Deng%20and%20Ruihang%20Chu%20and%20Jiangfeng%20Xiong%20and%20Xiaodan%20Liang%20and%20Hong%20Cheng%20and%20Qinglin%20Lu%20and%20Wei%20Liu%0AAbstract%3A%20%20%20Text-to-image%20%28T2I%29%20generation%20models%20have%20significantly%20advanced%20in%20recent%0Ayears.%20However%2C%20effective%20interaction%20with%20these%20models%20is%20challenging%20for%0Aaverage%20users%20due%20to%20the%20need%20for%20specialized%20prompt%20engineering%20knowledge%20and%0Athe%20inability%20to%20perform%20multi-turn%20image%20generation%2C%20hindering%20a%20dynamic%20and%0Aiterative%20creation%20process.%20Recent%20attempts%20have%20tried%20to%20equip%20Multi-modal%0ALarge%20Language%20Models%20%28MLLMs%29%20with%20T2I%20models%20to%20bring%20the%20user%27s%20natural%0Alanguage%20instructions%20into%20reality.%20Hence%2C%20the%20output%20modality%20of%20MLLMs%20is%0Aextended%2C%20and%20the%20multi-turn%20generation%20quality%20of%20T2I%20models%20is%20enhanced%0Athanks%20to%20the%20strong%20multi-modal%20comprehension%20ability%20of%20MLLMs.%20However%2C%20many%0Aof%20these%20works%20face%20challenges%20in%20identifying%20correct%20output%20modalities%20and%0Agenerating%20coherent%20images%20accordingly%20as%20the%20number%20of%20output%20modalities%0Aincreases%20and%20the%20conversations%20go%20deeper.%20Therefore%2C%20we%20propose%20DialogGen%2C%20an%0Aeffective%20pipeline%20to%20align%20off-the-shelf%20MLLMs%20and%20T2I%20models%20to%20build%20a%0AMulti-modal%20Interactive%20Dialogue%20System%20%28MIDS%29%20for%20multi-turn%20Text-to-Image%0Ageneration.%20It%20is%20composed%20of%20drawing%20prompt%20alignment%2C%20careful%20training%20data%0Acuration%2C%20and%20error%20correction.%20Moreover%2C%20as%20the%20field%20of%20MIDS%20flourishes%2C%0Acomprehensive%20benchmarks%20are%20urgently%20needed%20to%20evaluate%20MIDS%20fairly%20in%20terms%0Aof%20output%20modality%20correctness%20and%20multi-modal%20output%20coherence.%20To%20address%0Athis%20issue%2C%20we%20introduce%20the%20Multi-modal%20Dialogue%20Benchmark%20%28DialogBen%29%2C%20a%0Acomprehensive%20bilingual%20benchmark%20designed%20to%20assess%20the%20ability%20of%20MLLMs%20to%0Agenerate%20accurate%20and%20coherent%20multi-modal%20content%20that%20supports%20image%20editing.%0AIt%20contains%20two%20evaluation%20metrics%20to%20measure%20the%20model%27s%20ability%20to%20switch%0Amodalities%20and%20the%20coherence%20of%20the%20output%20images.%20Our%20extensive%20experiments%20on%0ADialogBen%20and%20user%20study%20demonstrate%20the%20effectiveness%20of%20DialogGen%20compared%0Awith%20other%20State-of-the-Art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08857v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDialogGen%253A%2520Multi-modal%2520Interactive%2520Dialogue%2520System%2520for%2520Multi-turn%250A%2520%2520Text-to-Image%2520Generation%26entry.906535625%3DMinbin%2520Huang%2520and%2520Yanxin%2520Long%2520and%2520Xinchi%2520Deng%2520and%2520Ruihang%2520Chu%2520and%2520Jiangfeng%2520Xiong%2520and%2520Xiaodan%2520Liang%2520and%2520Hong%2520Cheng%2520and%2520Qinglin%2520Lu%2520and%2520Wei%2520Liu%26entry.1292438233%3D%2520%2520Text-to-image%2520%2528T2I%2529%2520generation%2520models%2520have%2520significantly%2520advanced%2520in%2520recent%250Ayears.%2520However%252C%2520effective%2520interaction%2520with%2520these%2520models%2520is%2520challenging%2520for%250Aaverage%2520users%2520due%2520to%2520the%2520need%2520for%2520specialized%2520prompt%2520engineering%2520knowledge%2520and%250Athe%2520inability%2520to%2520perform%2520multi-turn%2520image%2520generation%252C%2520hindering%2520a%2520dynamic%2520and%250Aiterative%2520creation%2520process.%2520Recent%2520attempts%2520have%2520tried%2520to%2520equip%2520Multi-modal%250ALarge%2520Language%2520Models%2520%2528MLLMs%2529%2520with%2520T2I%2520models%2520to%2520bring%2520the%2520user%2527s%2520natural%250Alanguage%2520instructions%2520into%2520reality.%2520Hence%252C%2520the%2520output%2520modality%2520of%2520MLLMs%2520is%250Aextended%252C%2520and%2520the%2520multi-turn%2520generation%2520quality%2520of%2520T2I%2520models%2520is%2520enhanced%250Athanks%2520to%2520the%2520strong%2520multi-modal%2520comprehension%2520ability%2520of%2520MLLMs.%2520However%252C%2520many%250Aof%2520these%2520works%2520face%2520challenges%2520in%2520identifying%2520correct%2520output%2520modalities%2520and%250Agenerating%2520coherent%2520images%2520accordingly%2520as%2520the%2520number%2520of%2520output%2520modalities%250Aincreases%2520and%2520the%2520conversations%2520go%2520deeper.%2520Therefore%252C%2520we%2520propose%2520DialogGen%252C%2520an%250Aeffective%2520pipeline%2520to%2520align%2520off-the-shelf%2520MLLMs%2520and%2520T2I%2520models%2520to%2520build%2520a%250AMulti-modal%2520Interactive%2520Dialogue%2520System%2520%2528MIDS%2529%2520for%2520multi-turn%2520Text-to-Image%250Ageneration.%2520It%2520is%2520composed%2520of%2520drawing%2520prompt%2520alignment%252C%2520careful%2520training%2520data%250Acuration%252C%2520and%2520error%2520correction.%2520Moreover%252C%2520as%2520the%2520field%2520of%2520MIDS%2520flourishes%252C%250Acomprehensive%2520benchmarks%2520are%2520urgently%2520needed%2520to%2520evaluate%2520MIDS%2520fairly%2520in%2520terms%250Aof%2520output%2520modality%2520correctness%2520and%2520multi-modal%2520output%2520coherence.%2520To%2520address%250Athis%2520issue%252C%2520we%2520introduce%2520the%2520Multi-modal%2520Dialogue%2520Benchmark%2520%2528DialogBen%2529%252C%2520a%250Acomprehensive%2520bilingual%2520benchmark%2520designed%2520to%2520assess%2520the%2520ability%2520of%2520MLLMs%2520to%250Agenerate%2520accurate%2520and%2520coherent%2520multi-modal%2520content%2520that%2520supports%2520image%2520editing.%250AIt%2520contains%2520two%2520evaluation%2520metrics%2520to%2520measure%2520the%2520model%2527s%2520ability%2520to%2520switch%250Amodalities%2520and%2520the%2520coherence%2520of%2520the%2520output%2520images.%2520Our%2520extensive%2520experiments%2520on%250ADialogBen%2520and%2520user%2520study%2520demonstrate%2520the%2520effectiveness%2520of%2520DialogGen%2520compared%250Awith%2520other%2520State-of-the-Art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.08857v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DialogGen%3A%20Multi-modal%20Interactive%20Dialogue%20System%20for%20Multi-turn%0A%20%20Text-to-Image%20Generation&entry.906535625=Minbin%20Huang%20and%20Yanxin%20Long%20and%20Xinchi%20Deng%20and%20Ruihang%20Chu%20and%20Jiangfeng%20Xiong%20and%20Xiaodan%20Liang%20and%20Hong%20Cheng%20and%20Qinglin%20Lu%20and%20Wei%20Liu&entry.1292438233=%20%20Text-to-image%20%28T2I%29%20generation%20models%20have%20significantly%20advanced%20in%20recent%0Ayears.%20However%2C%20effective%20interaction%20with%20these%20models%20is%20challenging%20for%0Aaverage%20users%20due%20to%20the%20need%20for%20specialized%20prompt%20engineering%20knowledge%20and%0Athe%20inability%20to%20perform%20multi-turn%20image%20generation%2C%20hindering%20a%20dynamic%20and%0Aiterative%20creation%20process.%20Recent%20attempts%20have%20tried%20to%20equip%20Multi-modal%0ALarge%20Language%20Models%20%28MLLMs%29%20with%20T2I%20models%20to%20bring%20the%20user%27s%20natural%0Alanguage%20instructions%20into%20reality.%20Hence%2C%20the%20output%20modality%20of%20MLLMs%20is%0Aextended%2C%20and%20the%20multi-turn%20generation%20quality%20of%20T2I%20models%20is%20enhanced%0Athanks%20to%20the%20strong%20multi-modal%20comprehension%20ability%20of%20MLLMs.%20However%2C%20many%0Aof%20these%20works%20face%20challenges%20in%20identifying%20correct%20output%20modalities%20and%0Agenerating%20coherent%20images%20accordingly%20as%20the%20number%20of%20output%20modalities%0Aincreases%20and%20the%20conversations%20go%20deeper.%20Therefore%2C%20we%20propose%20DialogGen%2C%20an%0Aeffective%20pipeline%20to%20align%20off-the-shelf%20MLLMs%20and%20T2I%20models%20to%20build%20a%0AMulti-modal%20Interactive%20Dialogue%20System%20%28MIDS%29%20for%20multi-turn%20Text-to-Image%0Ageneration.%20It%20is%20composed%20of%20drawing%20prompt%20alignment%2C%20careful%20training%20data%0Acuration%2C%20and%20error%20correction.%20Moreover%2C%20as%20the%20field%20of%20MIDS%20flourishes%2C%0Acomprehensive%20benchmarks%20are%20urgently%20needed%20to%20evaluate%20MIDS%20fairly%20in%20terms%0Aof%20output%20modality%20correctness%20and%20multi-modal%20output%20coherence.%20To%20address%0Athis%20issue%2C%20we%20introduce%20the%20Multi-modal%20Dialogue%20Benchmark%20%28DialogBen%29%2C%20a%0Acomprehensive%20bilingual%20benchmark%20designed%20to%20assess%20the%20ability%20of%20MLLMs%20to%0Agenerate%20accurate%20and%20coherent%20multi-modal%20content%20that%20supports%20image%20editing.%0AIt%20contains%20two%20evaluation%20metrics%20to%20measure%20the%20model%27s%20ability%20to%20switch%0Amodalities%20and%20the%20coherence%20of%20the%20output%20images.%20Our%20extensive%20experiments%20on%0ADialogBen%20and%20user%20study%20demonstrate%20the%20effectiveness%20of%20DialogGen%20compared%0Awith%20other%20State-of-the-Art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08857v2&entry.124074799=Read"},
{"title": "Depth Priors in Removal Neural Radiance Fields", "author": "Zhihao Guo and Peng Wang", "abstract": "  Neural Radiance Fields (NeRF) have achieved impressive results in 3D\nreconstruction and novel view generation. A significant challenge within NeRF\ninvolves editing reconstructed 3D scenes, such as object removal, which demands\nconsistency across multiple views and the synthesis of high-quality\nperspectives. Previous studies have integrated depth priors, typically sourced\nfrom LiDAR or sparse depth estimates from COLMAP, to enhance NeRF's performance\nin object removal. However, these methods are either expensive or\ntime-consuming. This paper proposes a new pipeline that leverages SpinNeRF and\nmonocular depth estimation models like ZoeDepth to enhance NeRF's performance\nin complex object removal with improved efficiency. A thorough evaluation of\nCOLMAP's dense depth reconstruction on the KITTI dataset is conducted to\ndemonstrate that COLMAP can be viewed as a cost-effective and scalable\nalternative for acquiring depth ground truth compared to traditional methods\nlike LiDAR. This serves as the basis for evaluating the performance of\nmonocular depth estimation models to determine the best one for generating\ndepth priors for SpinNeRF. The new pipeline is tested in various scenarios\ninvolving 3D reconstruction and object removal, and the results indicate that\nour pipeline significantly reduces the time required for the acquisition of\ndepth priors for object removal and enhances the fidelity of the synthesized\nviews, suggesting substantial potential for building high-fidelity digital twin\nsystems with increased efficiency in the future.\n", "link": "http://arxiv.org/abs/2405.00630v3", "date": "2024-07-03", "relevancy": 2.7388, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5737}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5348}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Depth%20Priors%20in%20Removal%20Neural%20Radiance%20Fields&body=Title%3A%20Depth%20Priors%20in%20Removal%20Neural%20Radiance%20Fields%0AAuthor%3A%20Zhihao%20Guo%20and%20Peng%20Wang%0AAbstract%3A%20%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20achieved%20impressive%20results%20in%203D%0Areconstruction%20and%20novel%20view%20generation.%20A%20significant%20challenge%20within%20NeRF%0Ainvolves%20editing%20reconstructed%203D%20scenes%2C%20such%20as%20object%20removal%2C%20which%20demands%0Aconsistency%20across%20multiple%20views%20and%20the%20synthesis%20of%20high-quality%0Aperspectives.%20Previous%20studies%20have%20integrated%20depth%20priors%2C%20typically%20sourced%0Afrom%20LiDAR%20or%20sparse%20depth%20estimates%20from%20COLMAP%2C%20to%20enhance%20NeRF%27s%20performance%0Ain%20object%20removal.%20However%2C%20these%20methods%20are%20either%20expensive%20or%0Atime-consuming.%20This%20paper%20proposes%20a%20new%20pipeline%20that%20leverages%20SpinNeRF%20and%0Amonocular%20depth%20estimation%20models%20like%20ZoeDepth%20to%20enhance%20NeRF%27s%20performance%0Ain%20complex%20object%20removal%20with%20improved%20efficiency.%20A%20thorough%20evaluation%20of%0ACOLMAP%27s%20dense%20depth%20reconstruction%20on%20the%20KITTI%20dataset%20is%20conducted%20to%0Ademonstrate%20that%20COLMAP%20can%20be%20viewed%20as%20a%20cost-effective%20and%20scalable%0Aalternative%20for%20acquiring%20depth%20ground%20truth%20compared%20to%20traditional%20methods%0Alike%20LiDAR.%20This%20serves%20as%20the%20basis%20for%20evaluating%20the%20performance%20of%0Amonocular%20depth%20estimation%20models%20to%20determine%20the%20best%20one%20for%20generating%0Adepth%20priors%20for%20SpinNeRF.%20The%20new%20pipeline%20is%20tested%20in%20various%20scenarios%0Ainvolving%203D%20reconstruction%20and%20object%20removal%2C%20and%20the%20results%20indicate%20that%0Aour%20pipeline%20significantly%20reduces%20the%20time%20required%20for%20the%20acquisition%20of%0Adepth%20priors%20for%20object%20removal%20and%20enhances%20the%20fidelity%20of%20the%20synthesized%0Aviews%2C%20suggesting%20substantial%20potential%20for%20building%20high-fidelity%20digital%20twin%0Asystems%20with%20increased%20efficiency%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.00630v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDepth%2520Priors%2520in%2520Removal%2520Neural%2520Radiance%2520Fields%26entry.906535625%3DZhihao%2520Guo%2520and%2520Peng%2520Wang%26entry.1292438233%3D%2520%2520Neural%2520Radiance%2520Fields%2520%2528NeRF%2529%2520have%2520achieved%2520impressive%2520results%2520in%25203D%250Areconstruction%2520and%2520novel%2520view%2520generation.%2520A%2520significant%2520challenge%2520within%2520NeRF%250Ainvolves%2520editing%2520reconstructed%25203D%2520scenes%252C%2520such%2520as%2520object%2520removal%252C%2520which%2520demands%250Aconsistency%2520across%2520multiple%2520views%2520and%2520the%2520synthesis%2520of%2520high-quality%250Aperspectives.%2520Previous%2520studies%2520have%2520integrated%2520depth%2520priors%252C%2520typically%2520sourced%250Afrom%2520LiDAR%2520or%2520sparse%2520depth%2520estimates%2520from%2520COLMAP%252C%2520to%2520enhance%2520NeRF%2527s%2520performance%250Ain%2520object%2520removal.%2520However%252C%2520these%2520methods%2520are%2520either%2520expensive%2520or%250Atime-consuming.%2520This%2520paper%2520proposes%2520a%2520new%2520pipeline%2520that%2520leverages%2520SpinNeRF%2520and%250Amonocular%2520depth%2520estimation%2520models%2520like%2520ZoeDepth%2520to%2520enhance%2520NeRF%2527s%2520performance%250Ain%2520complex%2520object%2520removal%2520with%2520improved%2520efficiency.%2520A%2520thorough%2520evaluation%2520of%250ACOLMAP%2527s%2520dense%2520depth%2520reconstruction%2520on%2520the%2520KITTI%2520dataset%2520is%2520conducted%2520to%250Ademonstrate%2520that%2520COLMAP%2520can%2520be%2520viewed%2520as%2520a%2520cost-effective%2520and%2520scalable%250Aalternative%2520for%2520acquiring%2520depth%2520ground%2520truth%2520compared%2520to%2520traditional%2520methods%250Alike%2520LiDAR.%2520This%2520serves%2520as%2520the%2520basis%2520for%2520evaluating%2520the%2520performance%2520of%250Amonocular%2520depth%2520estimation%2520models%2520to%2520determine%2520the%2520best%2520one%2520for%2520generating%250Adepth%2520priors%2520for%2520SpinNeRF.%2520The%2520new%2520pipeline%2520is%2520tested%2520in%2520various%2520scenarios%250Ainvolving%25203D%2520reconstruction%2520and%2520object%2520removal%252C%2520and%2520the%2520results%2520indicate%2520that%250Aour%2520pipeline%2520significantly%2520reduces%2520the%2520time%2520required%2520for%2520the%2520acquisition%2520of%250Adepth%2520priors%2520for%2520object%2520removal%2520and%2520enhances%2520the%2520fidelity%2520of%2520the%2520synthesized%250Aviews%252C%2520suggesting%2520substantial%2520potential%2520for%2520building%2520high-fidelity%2520digital%2520twin%250Asystems%2520with%2520increased%2520efficiency%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.00630v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Depth%20Priors%20in%20Removal%20Neural%20Radiance%20Fields&entry.906535625=Zhihao%20Guo%20and%20Peng%20Wang&entry.1292438233=%20%20Neural%20Radiance%20Fields%20%28NeRF%29%20have%20achieved%20impressive%20results%20in%203D%0Areconstruction%20and%20novel%20view%20generation.%20A%20significant%20challenge%20within%20NeRF%0Ainvolves%20editing%20reconstructed%203D%20scenes%2C%20such%20as%20object%20removal%2C%20which%20demands%0Aconsistency%20across%20multiple%20views%20and%20the%20synthesis%20of%20high-quality%0Aperspectives.%20Previous%20studies%20have%20integrated%20depth%20priors%2C%20typically%20sourced%0Afrom%20LiDAR%20or%20sparse%20depth%20estimates%20from%20COLMAP%2C%20to%20enhance%20NeRF%27s%20performance%0Ain%20object%20removal.%20However%2C%20these%20methods%20are%20either%20expensive%20or%0Atime-consuming.%20This%20paper%20proposes%20a%20new%20pipeline%20that%20leverages%20SpinNeRF%20and%0Amonocular%20depth%20estimation%20models%20like%20ZoeDepth%20to%20enhance%20NeRF%27s%20performance%0Ain%20complex%20object%20removal%20with%20improved%20efficiency.%20A%20thorough%20evaluation%20of%0ACOLMAP%27s%20dense%20depth%20reconstruction%20on%20the%20KITTI%20dataset%20is%20conducted%20to%0Ademonstrate%20that%20COLMAP%20can%20be%20viewed%20as%20a%20cost-effective%20and%20scalable%0Aalternative%20for%20acquiring%20depth%20ground%20truth%20compared%20to%20traditional%20methods%0Alike%20LiDAR.%20This%20serves%20as%20the%20basis%20for%20evaluating%20the%20performance%20of%0Amonocular%20depth%20estimation%20models%20to%20determine%20the%20best%20one%20for%20generating%0Adepth%20priors%20for%20SpinNeRF.%20The%20new%20pipeline%20is%20tested%20in%20various%20scenarios%0Ainvolving%203D%20reconstruction%20and%20object%20removal%2C%20and%20the%20results%20indicate%20that%0Aour%20pipeline%20significantly%20reduces%20the%20time%20required%20for%20the%20acquisition%20of%0Adepth%20priors%20for%20object%20removal%20and%20enhances%20the%20fidelity%20of%20the%20synthesized%0Aviews%2C%20suggesting%20substantial%20potential%20for%20building%20high-fidelity%20digital%20twin%0Asystems%20with%20increased%20efficiency%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.00630v3&entry.124074799=Read"},
{"title": "ACTRESS: Active Retraining for Semi-supervised Visual Grounding", "author": "Weitai Kang and Mengxue Qu and Yunchao Wei and Yan Yan", "abstract": "  Semi-Supervised Visual Grounding (SSVG) is a new challenge for its sparse\nlabeled data with the need for multimodel understanding. A previous study,\nRefTeacher, makes the first attempt to tackle this task by adopting the\nteacher-student framework to provide pseudo confidence supervision and\nattention-based supervision. However, this approach is incompatible with\ncurrent state-of-the-art visual grounding models, which follow the\nTransformer-based pipeline. These pipelines directly regress results without\nregion proposals or foreground binary classification, rendering them unsuitable\nfor fitting in RefTeacher due to the absence of confidence scores. Furthermore,\nthe geometric difference in teacher and student inputs, stemming from different\ndata augmentations, induces natural misalignment in attention-based\nconstraints. To establish a compatible SSVG framework, our paper proposes the\nACTive REtraining approach for Semi-Supervised Visual Grounding, abbreviated as\nACTRESS. Initially, the model is enhanced by incorporating an additional\nquantized detection head to expose its detection confidence. Building upon\nthis, ACTRESS consists of an active sampling strategy and a selective\nretraining strategy. The active sampling strategy iteratively selects\nhigh-quality pseudo labels by evaluating three crucial aspects: Faithfulness,\nRobustness, and Confidence, optimizing the utilization of unlabeled data. The\nselective retraining strategy retrains the model with periodic\nre-initialization of specific parameters, facilitating the model's escape from\nlocal minima. Extensive experiments demonstrates our superior performance on\nwidely-used benchmark datasets.\n", "link": "http://arxiv.org/abs/2407.03251v1", "date": "2024-07-03", "relevancy": 2.7049, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5658}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5289}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ACTRESS%3A%20Active%20Retraining%20for%20Semi-supervised%20Visual%20Grounding&body=Title%3A%20ACTRESS%3A%20Active%20Retraining%20for%20Semi-supervised%20Visual%20Grounding%0AAuthor%3A%20Weitai%20Kang%20and%20Mengxue%20Qu%20and%20Yunchao%20Wei%20and%20Yan%20Yan%0AAbstract%3A%20%20%20Semi-Supervised%20Visual%20Grounding%20%28SSVG%29%20is%20a%20new%20challenge%20for%20its%20sparse%0Alabeled%20data%20with%20the%20need%20for%20multimodel%20understanding.%20A%20previous%20study%2C%0ARefTeacher%2C%20makes%20the%20first%20attempt%20to%20tackle%20this%20task%20by%20adopting%20the%0Ateacher-student%20framework%20to%20provide%20pseudo%20confidence%20supervision%20and%0Aattention-based%20supervision.%20However%2C%20this%20approach%20is%20incompatible%20with%0Acurrent%20state-of-the-art%20visual%20grounding%20models%2C%20which%20follow%20the%0ATransformer-based%20pipeline.%20These%20pipelines%20directly%20regress%20results%20without%0Aregion%20proposals%20or%20foreground%20binary%20classification%2C%20rendering%20them%20unsuitable%0Afor%20fitting%20in%20RefTeacher%20due%20to%20the%20absence%20of%20confidence%20scores.%20Furthermore%2C%0Athe%20geometric%20difference%20in%20teacher%20and%20student%20inputs%2C%20stemming%20from%20different%0Adata%20augmentations%2C%20induces%20natural%20misalignment%20in%20attention-based%0Aconstraints.%20To%20establish%20a%20compatible%20SSVG%20framework%2C%20our%20paper%20proposes%20the%0AACTive%20REtraining%20approach%20for%20Semi-Supervised%20Visual%20Grounding%2C%20abbreviated%20as%0AACTRESS.%20Initially%2C%20the%20model%20is%20enhanced%20by%20incorporating%20an%20additional%0Aquantized%20detection%20head%20to%20expose%20its%20detection%20confidence.%20Building%20upon%0Athis%2C%20ACTRESS%20consists%20of%20an%20active%20sampling%20strategy%20and%20a%20selective%0Aretraining%20strategy.%20The%20active%20sampling%20strategy%20iteratively%20selects%0Ahigh-quality%20pseudo%20labels%20by%20evaluating%20three%20crucial%20aspects%3A%20Faithfulness%2C%0ARobustness%2C%20and%20Confidence%2C%20optimizing%20the%20utilization%20of%20unlabeled%20data.%20The%0Aselective%20retraining%20strategy%20retrains%20the%20model%20with%20periodic%0Are-initialization%20of%20specific%20parameters%2C%20facilitating%20the%20model%27s%20escape%20from%0Alocal%20minima.%20Extensive%20experiments%20demonstrates%20our%20superior%20performance%20on%0Awidely-used%20benchmark%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DACTRESS%253A%2520Active%2520Retraining%2520for%2520Semi-supervised%2520Visual%2520Grounding%26entry.906535625%3DWeitai%2520Kang%2520and%2520Mengxue%2520Qu%2520and%2520Yunchao%2520Wei%2520and%2520Yan%2520Yan%26entry.1292438233%3D%2520%2520Semi-Supervised%2520Visual%2520Grounding%2520%2528SSVG%2529%2520is%2520a%2520new%2520challenge%2520for%2520its%2520sparse%250Alabeled%2520data%2520with%2520the%2520need%2520for%2520multimodel%2520understanding.%2520A%2520previous%2520study%252C%250ARefTeacher%252C%2520makes%2520the%2520first%2520attempt%2520to%2520tackle%2520this%2520task%2520by%2520adopting%2520the%250Ateacher-student%2520framework%2520to%2520provide%2520pseudo%2520confidence%2520supervision%2520and%250Aattention-based%2520supervision.%2520However%252C%2520this%2520approach%2520is%2520incompatible%2520with%250Acurrent%2520state-of-the-art%2520visual%2520grounding%2520models%252C%2520which%2520follow%2520the%250ATransformer-based%2520pipeline.%2520These%2520pipelines%2520directly%2520regress%2520results%2520without%250Aregion%2520proposals%2520or%2520foreground%2520binary%2520classification%252C%2520rendering%2520them%2520unsuitable%250Afor%2520fitting%2520in%2520RefTeacher%2520due%2520to%2520the%2520absence%2520of%2520confidence%2520scores.%2520Furthermore%252C%250Athe%2520geometric%2520difference%2520in%2520teacher%2520and%2520student%2520inputs%252C%2520stemming%2520from%2520different%250Adata%2520augmentations%252C%2520induces%2520natural%2520misalignment%2520in%2520attention-based%250Aconstraints.%2520To%2520establish%2520a%2520compatible%2520SSVG%2520framework%252C%2520our%2520paper%2520proposes%2520the%250AACTive%2520REtraining%2520approach%2520for%2520Semi-Supervised%2520Visual%2520Grounding%252C%2520abbreviated%2520as%250AACTRESS.%2520Initially%252C%2520the%2520model%2520is%2520enhanced%2520by%2520incorporating%2520an%2520additional%250Aquantized%2520detection%2520head%2520to%2520expose%2520its%2520detection%2520confidence.%2520Building%2520upon%250Athis%252C%2520ACTRESS%2520consists%2520of%2520an%2520active%2520sampling%2520strategy%2520and%2520a%2520selective%250Aretraining%2520strategy.%2520The%2520active%2520sampling%2520strategy%2520iteratively%2520selects%250Ahigh-quality%2520pseudo%2520labels%2520by%2520evaluating%2520three%2520crucial%2520aspects%253A%2520Faithfulness%252C%250ARobustness%252C%2520and%2520Confidence%252C%2520optimizing%2520the%2520utilization%2520of%2520unlabeled%2520data.%2520The%250Aselective%2520retraining%2520strategy%2520retrains%2520the%2520model%2520with%2520periodic%250Are-initialization%2520of%2520specific%2520parameters%252C%2520facilitating%2520the%2520model%2527s%2520escape%2520from%250Alocal%2520minima.%2520Extensive%2520experiments%2520demonstrates%2520our%2520superior%2520performance%2520on%250Awidely-used%2520benchmark%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ACTRESS%3A%20Active%20Retraining%20for%20Semi-supervised%20Visual%20Grounding&entry.906535625=Weitai%20Kang%20and%20Mengxue%20Qu%20and%20Yunchao%20Wei%20and%20Yan%20Yan&entry.1292438233=%20%20Semi-Supervised%20Visual%20Grounding%20%28SSVG%29%20is%20a%20new%20challenge%20for%20its%20sparse%0Alabeled%20data%20with%20the%20need%20for%20multimodel%20understanding.%20A%20previous%20study%2C%0ARefTeacher%2C%20makes%20the%20first%20attempt%20to%20tackle%20this%20task%20by%20adopting%20the%0Ateacher-student%20framework%20to%20provide%20pseudo%20confidence%20supervision%20and%0Aattention-based%20supervision.%20However%2C%20this%20approach%20is%20incompatible%20with%0Acurrent%20state-of-the-art%20visual%20grounding%20models%2C%20which%20follow%20the%0ATransformer-based%20pipeline.%20These%20pipelines%20directly%20regress%20results%20without%0Aregion%20proposals%20or%20foreground%20binary%20classification%2C%20rendering%20them%20unsuitable%0Afor%20fitting%20in%20RefTeacher%20due%20to%20the%20absence%20of%20confidence%20scores.%20Furthermore%2C%0Athe%20geometric%20difference%20in%20teacher%20and%20student%20inputs%2C%20stemming%20from%20different%0Adata%20augmentations%2C%20induces%20natural%20misalignment%20in%20attention-based%0Aconstraints.%20To%20establish%20a%20compatible%20SSVG%20framework%2C%20our%20paper%20proposes%20the%0AACTive%20REtraining%20approach%20for%20Semi-Supervised%20Visual%20Grounding%2C%20abbreviated%20as%0AACTRESS.%20Initially%2C%20the%20model%20is%20enhanced%20by%20incorporating%20an%20additional%0Aquantized%20detection%20head%20to%20expose%20its%20detection%20confidence.%20Building%20upon%0Athis%2C%20ACTRESS%20consists%20of%20an%20active%20sampling%20strategy%20and%20a%20selective%0Aretraining%20strategy.%20The%20active%20sampling%20strategy%20iteratively%20selects%0Ahigh-quality%20pseudo%20labels%20by%20evaluating%20three%20crucial%20aspects%3A%20Faithfulness%2C%0ARobustness%2C%20and%20Confidence%2C%20optimizing%20the%20utilization%20of%20unlabeled%20data.%20The%0Aselective%20retraining%20strategy%20retrains%20the%20model%20with%20periodic%0Are-initialization%20of%20specific%20parameters%2C%20facilitating%20the%20model%27s%20escape%20from%0Alocal%20minima.%20Extensive%20experiments%20demonstrates%20our%20superior%20performance%20on%0Awidely-used%20benchmark%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03251v1&entry.124074799=Read"},
{"title": "Large-scale Pre-trained Models are Surprisingly Strong in Incremental\n  Novel Class Discovery", "author": "Mingxuan Liu and Subhankar Roy and Zhun Zhong and Nicu Sebe and Elisa Ricci", "abstract": "  Discovering novel concepts in unlabelled datasets and in a continuous manner\nis an important desideratum of lifelong learners. In the literature such\nproblems have been partially addressed under very restricted settings, where\nnovel classes are learned by jointly accessing a related labelled set (e.g.,\nNCD) or by leveraging only a supervisedly pre-trained model (e.g., class-iNCD).\nIn this work we challenge the status quo in class-iNCD and propose a learning\nparadigm where class discovery occurs continuously and truly unsupervisedly,\nwithout needing any related labelled set. In detail, we propose to exploit the\nricher priors from strong self-supervised pre-trained models (PTM). To this\nend, we propose simple baselines, composed of a frozen PTM backbone and a\nlearnable linear classifier, that are not only simple to implement but also\nresilient under longer learning scenarios. We conduct extensive empirical\nevaluation on a multitude of benchmarks and show the effectiveness of our\nproposed baselines when compared with sophisticated state-of-the-art methods.\nThe code is open source.\n", "link": "http://arxiv.org/abs/2303.15975v3", "date": "2024-07-03", "relevancy": 2.6889, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5499}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5379}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5255}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large-scale%20Pre-trained%20Models%20are%20Surprisingly%20Strong%20in%20Incremental%0A%20%20Novel%20Class%20Discovery&body=Title%3A%20Large-scale%20Pre-trained%20Models%20are%20Surprisingly%20Strong%20in%20Incremental%0A%20%20Novel%20Class%20Discovery%0AAuthor%3A%20Mingxuan%20Liu%20and%20Subhankar%20Roy%20and%20Zhun%20Zhong%20and%20Nicu%20Sebe%20and%20Elisa%20Ricci%0AAbstract%3A%20%20%20Discovering%20novel%20concepts%20in%20unlabelled%20datasets%20and%20in%20a%20continuous%20manner%0Ais%20an%20important%20desideratum%20of%20lifelong%20learners.%20In%20the%20literature%20such%0Aproblems%20have%20been%20partially%20addressed%20under%20very%20restricted%20settings%2C%20where%0Anovel%20classes%20are%20learned%20by%20jointly%20accessing%20a%20related%20labelled%20set%20%28e.g.%2C%0ANCD%29%20or%20by%20leveraging%20only%20a%20supervisedly%20pre-trained%20model%20%28e.g.%2C%20class-iNCD%29.%0AIn%20this%20work%20we%20challenge%20the%20status%20quo%20in%20class-iNCD%20and%20propose%20a%20learning%0Aparadigm%20where%20class%20discovery%20occurs%20continuously%20and%20truly%20unsupervisedly%2C%0Awithout%20needing%20any%20related%20labelled%20set.%20In%20detail%2C%20we%20propose%20to%20exploit%20the%0Aricher%20priors%20from%20strong%20self-supervised%20pre-trained%20models%20%28PTM%29.%20To%20this%0Aend%2C%20we%20propose%20simple%20baselines%2C%20composed%20of%20a%20frozen%20PTM%20backbone%20and%20a%0Alearnable%20linear%20classifier%2C%20that%20are%20not%20only%20simple%20to%20implement%20but%20also%0Aresilient%20under%20longer%20learning%20scenarios.%20We%20conduct%20extensive%20empirical%0Aevaluation%20on%20a%20multitude%20of%20benchmarks%20and%20show%20the%20effectiveness%20of%20our%0Aproposed%20baselines%20when%20compared%20with%20sophisticated%20state-of-the-art%20methods.%0AThe%20code%20is%20open%20source.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2303.15975v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge-scale%2520Pre-trained%2520Models%2520are%2520Surprisingly%2520Strong%2520in%2520Incremental%250A%2520%2520Novel%2520Class%2520Discovery%26entry.906535625%3DMingxuan%2520Liu%2520and%2520Subhankar%2520Roy%2520and%2520Zhun%2520Zhong%2520and%2520Nicu%2520Sebe%2520and%2520Elisa%2520Ricci%26entry.1292438233%3D%2520%2520Discovering%2520novel%2520concepts%2520in%2520unlabelled%2520datasets%2520and%2520in%2520a%2520continuous%2520manner%250Ais%2520an%2520important%2520desideratum%2520of%2520lifelong%2520learners.%2520In%2520the%2520literature%2520such%250Aproblems%2520have%2520been%2520partially%2520addressed%2520under%2520very%2520restricted%2520settings%252C%2520where%250Anovel%2520classes%2520are%2520learned%2520by%2520jointly%2520accessing%2520a%2520related%2520labelled%2520set%2520%2528e.g.%252C%250ANCD%2529%2520or%2520by%2520leveraging%2520only%2520a%2520supervisedly%2520pre-trained%2520model%2520%2528e.g.%252C%2520class-iNCD%2529.%250AIn%2520this%2520work%2520we%2520challenge%2520the%2520status%2520quo%2520in%2520class-iNCD%2520and%2520propose%2520a%2520learning%250Aparadigm%2520where%2520class%2520discovery%2520occurs%2520continuously%2520and%2520truly%2520unsupervisedly%252C%250Awithout%2520needing%2520any%2520related%2520labelled%2520set.%2520In%2520detail%252C%2520we%2520propose%2520to%2520exploit%2520the%250Aricher%2520priors%2520from%2520strong%2520self-supervised%2520pre-trained%2520models%2520%2528PTM%2529.%2520To%2520this%250Aend%252C%2520we%2520propose%2520simple%2520baselines%252C%2520composed%2520of%2520a%2520frozen%2520PTM%2520backbone%2520and%2520a%250Alearnable%2520linear%2520classifier%252C%2520that%2520are%2520not%2520only%2520simple%2520to%2520implement%2520but%2520also%250Aresilient%2520under%2520longer%2520learning%2520scenarios.%2520We%2520conduct%2520extensive%2520empirical%250Aevaluation%2520on%2520a%2520multitude%2520of%2520benchmarks%2520and%2520show%2520the%2520effectiveness%2520of%2520our%250Aproposed%2520baselines%2520when%2520compared%2520with%2520sophisticated%2520state-of-the-art%2520methods.%250AThe%2520code%2520is%2520open%2520source.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2303.15975v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large-scale%20Pre-trained%20Models%20are%20Surprisingly%20Strong%20in%20Incremental%0A%20%20Novel%20Class%20Discovery&entry.906535625=Mingxuan%20Liu%20and%20Subhankar%20Roy%20and%20Zhun%20Zhong%20and%20Nicu%20Sebe%20and%20Elisa%20Ricci&entry.1292438233=%20%20Discovering%20novel%20concepts%20in%20unlabelled%20datasets%20and%20in%20a%20continuous%20manner%0Ais%20an%20important%20desideratum%20of%20lifelong%20learners.%20In%20the%20literature%20such%0Aproblems%20have%20been%20partially%20addressed%20under%20very%20restricted%20settings%2C%20where%0Anovel%20classes%20are%20learned%20by%20jointly%20accessing%20a%20related%20labelled%20set%20%28e.g.%2C%0ANCD%29%20or%20by%20leveraging%20only%20a%20supervisedly%20pre-trained%20model%20%28e.g.%2C%20class-iNCD%29.%0AIn%20this%20work%20we%20challenge%20the%20status%20quo%20in%20class-iNCD%20and%20propose%20a%20learning%0Aparadigm%20where%20class%20discovery%20occurs%20continuously%20and%20truly%20unsupervisedly%2C%0Awithout%20needing%20any%20related%20labelled%20set.%20In%20detail%2C%20we%20propose%20to%20exploit%20the%0Aricher%20priors%20from%20strong%20self-supervised%20pre-trained%20models%20%28PTM%29.%20To%20this%0Aend%2C%20we%20propose%20simple%20baselines%2C%20composed%20of%20a%20frozen%20PTM%20backbone%20and%20a%0Alearnable%20linear%20classifier%2C%20that%20are%20not%20only%20simple%20to%20implement%20but%20also%0Aresilient%20under%20longer%20learning%20scenarios.%20We%20conduct%20extensive%20empirical%0Aevaluation%20on%20a%20multitude%20of%20benchmarks%20and%20show%20the%20effectiveness%20of%20our%0Aproposed%20baselines%20when%20compared%20with%20sophisticated%20state-of-the-art%20methods.%0AThe%20code%20is%20open%20source.%0A&entry.1838667208=http%3A//arxiv.org/abs/2303.15975v3&entry.124074799=Read"},
{"title": "Model Guidance via Explanations Turns Image Classifiers into\n  Segmentation Models", "author": "Xiaoyan Yu and Jannik Franzen and Wojciech Samek and Marina M. -C. H\u00f6hne and Dagmar Kainmueller", "abstract": "  Heatmaps generated on inputs of image classification networks via explainable\nAI methods like Grad-CAM and LRP have been observed to resemble segmentations\nof input images in many cases. Consequently, heatmaps have also been leveraged\nfor achieving weakly supervised segmentation with image-level supervision. On\nthe other hand, losses can be imposed on differentiable heatmaps, which has\nbeen shown to serve for (1)~improving heatmaps to be more human-interpretable,\n(2)~regularization of networks towards better generalization, (3)~training\ndiverse ensembles of networks, and (4)~for explicitly ignoring confounding\ninput features. Due to the latter use case, the paradigm of imposing losses on\nheatmaps is often referred to as \"Right for the right reasons\". We unify these\ntwo lines of research by investigating semi-supervised segmentation as a novel\nuse case for the Right for the Right Reasons paradigm. First, we show formal\nparallels between differentiable heatmap architectures and standard\nencoder-decoder architectures for image segmentation. Second, we show that such\ndifferentiable heatmap architectures yield competitive results when trained\nwith standard segmentation losses. Third, we show that such architectures allow\nfor training with weak supervision in the form of image-level labels and small\nnumbers of pixel-level labels, outperforming comparable encoder-decoder models.\nCode is available at \\url{https://github.com/Kainmueller-Lab/TW-autoencoder}.\n", "link": "http://arxiv.org/abs/2407.03009v1", "date": "2024-07-03", "relevancy": 2.6849, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5426}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5383}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5301}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Guidance%20via%20Explanations%20Turns%20Image%20Classifiers%20into%0A%20%20Segmentation%20Models&body=Title%3A%20Model%20Guidance%20via%20Explanations%20Turns%20Image%20Classifiers%20into%0A%20%20Segmentation%20Models%0AAuthor%3A%20Xiaoyan%20Yu%20and%20Jannik%20Franzen%20and%20Wojciech%20Samek%20and%20Marina%20M.%20-C.%20H%C3%B6hne%20and%20Dagmar%20Kainmueller%0AAbstract%3A%20%20%20Heatmaps%20generated%20on%20inputs%20of%20image%20classification%20networks%20via%20explainable%0AAI%20methods%20like%20Grad-CAM%20and%20LRP%20have%20been%20observed%20to%20resemble%20segmentations%0Aof%20input%20images%20in%20many%20cases.%20Consequently%2C%20heatmaps%20have%20also%20been%20leveraged%0Afor%20achieving%20weakly%20supervised%20segmentation%20with%20image-level%20supervision.%20On%0Athe%20other%20hand%2C%20losses%20can%20be%20imposed%20on%20differentiable%20heatmaps%2C%20which%20has%0Abeen%20shown%20to%20serve%20for%20%281%29~improving%20heatmaps%20to%20be%20more%20human-interpretable%2C%0A%282%29~regularization%20of%20networks%20towards%20better%20generalization%2C%20%283%29~training%0Adiverse%20ensembles%20of%20networks%2C%20and%20%284%29~for%20explicitly%20ignoring%20confounding%0Ainput%20features.%20Due%20to%20the%20latter%20use%20case%2C%20the%20paradigm%20of%20imposing%20losses%20on%0Aheatmaps%20is%20often%20referred%20to%20as%20%22Right%20for%20the%20right%20reasons%22.%20We%20unify%20these%0Atwo%20lines%20of%20research%20by%20investigating%20semi-supervised%20segmentation%20as%20a%20novel%0Ause%20case%20for%20the%20Right%20for%20the%20Right%20Reasons%20paradigm.%20First%2C%20we%20show%20formal%0Aparallels%20between%20differentiable%20heatmap%20architectures%20and%20standard%0Aencoder-decoder%20architectures%20for%20image%20segmentation.%20Second%2C%20we%20show%20that%20such%0Adifferentiable%20heatmap%20architectures%20yield%20competitive%20results%20when%20trained%0Awith%20standard%20segmentation%20losses.%20Third%2C%20we%20show%20that%20such%20architectures%20allow%0Afor%20training%20with%20weak%20supervision%20in%20the%20form%20of%20image-level%20labels%20and%20small%0Anumbers%20of%20pixel-level%20labels%2C%20outperforming%20comparable%20encoder-decoder%20models.%0ACode%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/Kainmueller-Lab/TW-autoencoder%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03009v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Guidance%2520via%2520Explanations%2520Turns%2520Image%2520Classifiers%2520into%250A%2520%2520Segmentation%2520Models%26entry.906535625%3DXiaoyan%2520Yu%2520and%2520Jannik%2520Franzen%2520and%2520Wojciech%2520Samek%2520and%2520Marina%2520M.%2520-C.%2520H%25C3%25B6hne%2520and%2520Dagmar%2520Kainmueller%26entry.1292438233%3D%2520%2520Heatmaps%2520generated%2520on%2520inputs%2520of%2520image%2520classification%2520networks%2520via%2520explainable%250AAI%2520methods%2520like%2520Grad-CAM%2520and%2520LRP%2520have%2520been%2520observed%2520to%2520resemble%2520segmentations%250Aof%2520input%2520images%2520in%2520many%2520cases.%2520Consequently%252C%2520heatmaps%2520have%2520also%2520been%2520leveraged%250Afor%2520achieving%2520weakly%2520supervised%2520segmentation%2520with%2520image-level%2520supervision.%2520On%250Athe%2520other%2520hand%252C%2520losses%2520can%2520be%2520imposed%2520on%2520differentiable%2520heatmaps%252C%2520which%2520has%250Abeen%2520shown%2520to%2520serve%2520for%2520%25281%2529~improving%2520heatmaps%2520to%2520be%2520more%2520human-interpretable%252C%250A%25282%2529~regularization%2520of%2520networks%2520towards%2520better%2520generalization%252C%2520%25283%2529~training%250Adiverse%2520ensembles%2520of%2520networks%252C%2520and%2520%25284%2529~for%2520explicitly%2520ignoring%2520confounding%250Ainput%2520features.%2520Due%2520to%2520the%2520latter%2520use%2520case%252C%2520the%2520paradigm%2520of%2520imposing%2520losses%2520on%250Aheatmaps%2520is%2520often%2520referred%2520to%2520as%2520%2522Right%2520for%2520the%2520right%2520reasons%2522.%2520We%2520unify%2520these%250Atwo%2520lines%2520of%2520research%2520by%2520investigating%2520semi-supervised%2520segmentation%2520as%2520a%2520novel%250Ause%2520case%2520for%2520the%2520Right%2520for%2520the%2520Right%2520Reasons%2520paradigm.%2520First%252C%2520we%2520show%2520formal%250Aparallels%2520between%2520differentiable%2520heatmap%2520architectures%2520and%2520standard%250Aencoder-decoder%2520architectures%2520for%2520image%2520segmentation.%2520Second%252C%2520we%2520show%2520that%2520such%250Adifferentiable%2520heatmap%2520architectures%2520yield%2520competitive%2520results%2520when%2520trained%250Awith%2520standard%2520segmentation%2520losses.%2520Third%252C%2520we%2520show%2520that%2520such%2520architectures%2520allow%250Afor%2520training%2520with%2520weak%2520supervision%2520in%2520the%2520form%2520of%2520image-level%2520labels%2520and%2520small%250Anumbers%2520of%2520pixel-level%2520labels%252C%2520outperforming%2520comparable%2520encoder-decoder%2520models.%250ACode%2520is%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/Kainmueller-Lab/TW-autoencoder%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03009v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Guidance%20via%20Explanations%20Turns%20Image%20Classifiers%20into%0A%20%20Segmentation%20Models&entry.906535625=Xiaoyan%20Yu%20and%20Jannik%20Franzen%20and%20Wojciech%20Samek%20and%20Marina%20M.%20-C.%20H%C3%B6hne%20and%20Dagmar%20Kainmueller&entry.1292438233=%20%20Heatmaps%20generated%20on%20inputs%20of%20image%20classification%20networks%20via%20explainable%0AAI%20methods%20like%20Grad-CAM%20and%20LRP%20have%20been%20observed%20to%20resemble%20segmentations%0Aof%20input%20images%20in%20many%20cases.%20Consequently%2C%20heatmaps%20have%20also%20been%20leveraged%0Afor%20achieving%20weakly%20supervised%20segmentation%20with%20image-level%20supervision.%20On%0Athe%20other%20hand%2C%20losses%20can%20be%20imposed%20on%20differentiable%20heatmaps%2C%20which%20has%0Abeen%20shown%20to%20serve%20for%20%281%29~improving%20heatmaps%20to%20be%20more%20human-interpretable%2C%0A%282%29~regularization%20of%20networks%20towards%20better%20generalization%2C%20%283%29~training%0Adiverse%20ensembles%20of%20networks%2C%20and%20%284%29~for%20explicitly%20ignoring%20confounding%0Ainput%20features.%20Due%20to%20the%20latter%20use%20case%2C%20the%20paradigm%20of%20imposing%20losses%20on%0Aheatmaps%20is%20often%20referred%20to%20as%20%22Right%20for%20the%20right%20reasons%22.%20We%20unify%20these%0Atwo%20lines%20of%20research%20by%20investigating%20semi-supervised%20segmentation%20as%20a%20novel%0Ause%20case%20for%20the%20Right%20for%20the%20Right%20Reasons%20paradigm.%20First%2C%20we%20show%20formal%0Aparallels%20between%20differentiable%20heatmap%20architectures%20and%20standard%0Aencoder-decoder%20architectures%20for%20image%20segmentation.%20Second%2C%20we%20show%20that%20such%0Adifferentiable%20heatmap%20architectures%20yield%20competitive%20results%20when%20trained%0Awith%20standard%20segmentation%20losses.%20Third%2C%20we%20show%20that%20such%20architectures%20allow%0Afor%20training%20with%20weak%20supervision%20in%20the%20form%20of%20image-level%20labels%20and%20small%0Anumbers%20of%20pixel-level%20labels%2C%20outperforming%20comparable%20encoder-decoder%20models.%0ACode%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/Kainmueller-Lab/TW-autoencoder%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03009v1&entry.124074799=Read"},
{"title": "A Simple Baseline for Spoken Language to Sign Language Translation with\n  3D Avatars", "author": "Ronglai Zuo and Fangyun Wei and Zenggui Chen and Brian Mak and Jiaolong Yang and Xin Tong", "abstract": "  The objective of this paper is to develop a functional system for translating\nspoken languages into sign languages, referred to as Spoken2Sign translation.\nThe Spoken2Sign task is orthogonal and complementary to traditional sign\nlanguage to spoken language (Sign2Spoken) translation. To enable Spoken2Sign\ntranslation, we present a simple baseline consisting of three steps: 1)\ncreating a gloss-video dictionary using existing Sign2Spoken benchmarks; 2)\nestimating a 3D sign for each sign video in the dictionary; 3) training a\nSpoken2Sign model, which is composed of a Text2Gloss translator, a sign\nconnector, and a rendering module, with the aid of the yielded gloss-3D sign\ndictionary. The translation results are then displayed through a sign avatar.\nAs far as we know, we are the first to present the Spoken2Sign task in an\noutput format of 3D signs. In addition to its capability of Spoken2Sign\ntranslation, we also demonstrate that two by-products of our approach-3D\nkeypoint augmentation and multi-view understanding-can assist in keypoint-based\nsign language understanding. Code and models are available at\nhttps://github.com/FangyunWei/SLRT.\n", "link": "http://arxiv.org/abs/2401.04730v2", "date": "2024-07-03", "relevancy": 2.6746, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5581}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5244}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5222}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Simple%20Baseline%20for%20Spoken%20Language%20to%20Sign%20Language%20Translation%20with%0A%20%203D%20Avatars&body=Title%3A%20A%20Simple%20Baseline%20for%20Spoken%20Language%20to%20Sign%20Language%20Translation%20with%0A%20%203D%20Avatars%0AAuthor%3A%20Ronglai%20Zuo%20and%20Fangyun%20Wei%20and%20Zenggui%20Chen%20and%20Brian%20Mak%20and%20Jiaolong%20Yang%20and%20Xin%20Tong%0AAbstract%3A%20%20%20The%20objective%20of%20this%20paper%20is%20to%20develop%20a%20functional%20system%20for%20translating%0Aspoken%20languages%20into%20sign%20languages%2C%20referred%20to%20as%20Spoken2Sign%20translation.%0AThe%20Spoken2Sign%20task%20is%20orthogonal%20and%20complementary%20to%20traditional%20sign%0Alanguage%20to%20spoken%20language%20%28Sign2Spoken%29%20translation.%20To%20enable%20Spoken2Sign%0Atranslation%2C%20we%20present%20a%20simple%20baseline%20consisting%20of%20three%20steps%3A%201%29%0Acreating%20a%20gloss-video%20dictionary%20using%20existing%20Sign2Spoken%20benchmarks%3B%202%29%0Aestimating%20a%203D%20sign%20for%20each%20sign%20video%20in%20the%20dictionary%3B%203%29%20training%20a%0ASpoken2Sign%20model%2C%20which%20is%20composed%20of%20a%20Text2Gloss%20translator%2C%20a%20sign%0Aconnector%2C%20and%20a%20rendering%20module%2C%20with%20the%20aid%20of%20the%20yielded%20gloss-3D%20sign%0Adictionary.%20The%20translation%20results%20are%20then%20displayed%20through%20a%20sign%20avatar.%0AAs%20far%20as%20we%20know%2C%20we%20are%20the%20first%20to%20present%20the%20Spoken2Sign%20task%20in%20an%0Aoutput%20format%20of%203D%20signs.%20In%20addition%20to%20its%20capability%20of%20Spoken2Sign%0Atranslation%2C%20we%20also%20demonstrate%20that%20two%20by-products%20of%20our%20approach-3D%0Akeypoint%20augmentation%20and%20multi-view%20understanding-can%20assist%20in%20keypoint-based%0Asign%20language%20understanding.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/FangyunWei/SLRT.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.04730v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Simple%2520Baseline%2520for%2520Spoken%2520Language%2520to%2520Sign%2520Language%2520Translation%2520with%250A%2520%25203D%2520Avatars%26entry.906535625%3DRonglai%2520Zuo%2520and%2520Fangyun%2520Wei%2520and%2520Zenggui%2520Chen%2520and%2520Brian%2520Mak%2520and%2520Jiaolong%2520Yang%2520and%2520Xin%2520Tong%26entry.1292438233%3D%2520%2520The%2520objective%2520of%2520this%2520paper%2520is%2520to%2520develop%2520a%2520functional%2520system%2520for%2520translating%250Aspoken%2520languages%2520into%2520sign%2520languages%252C%2520referred%2520to%2520as%2520Spoken2Sign%2520translation.%250AThe%2520Spoken2Sign%2520task%2520is%2520orthogonal%2520and%2520complementary%2520to%2520traditional%2520sign%250Alanguage%2520to%2520spoken%2520language%2520%2528Sign2Spoken%2529%2520translation.%2520To%2520enable%2520Spoken2Sign%250Atranslation%252C%2520we%2520present%2520a%2520simple%2520baseline%2520consisting%2520of%2520three%2520steps%253A%25201%2529%250Acreating%2520a%2520gloss-video%2520dictionary%2520using%2520existing%2520Sign2Spoken%2520benchmarks%253B%25202%2529%250Aestimating%2520a%25203D%2520sign%2520for%2520each%2520sign%2520video%2520in%2520the%2520dictionary%253B%25203%2529%2520training%2520a%250ASpoken2Sign%2520model%252C%2520which%2520is%2520composed%2520of%2520a%2520Text2Gloss%2520translator%252C%2520a%2520sign%250Aconnector%252C%2520and%2520a%2520rendering%2520module%252C%2520with%2520the%2520aid%2520of%2520the%2520yielded%2520gloss-3D%2520sign%250Adictionary.%2520The%2520translation%2520results%2520are%2520then%2520displayed%2520through%2520a%2520sign%2520avatar.%250AAs%2520far%2520as%2520we%2520know%252C%2520we%2520are%2520the%2520first%2520to%2520present%2520the%2520Spoken2Sign%2520task%2520in%2520an%250Aoutput%2520format%2520of%25203D%2520signs.%2520In%2520addition%2520to%2520its%2520capability%2520of%2520Spoken2Sign%250Atranslation%252C%2520we%2520also%2520demonstrate%2520that%2520two%2520by-products%2520of%2520our%2520approach-3D%250Akeypoint%2520augmentation%2520and%2520multi-view%2520understanding-can%2520assist%2520in%2520keypoint-based%250Asign%2520language%2520understanding.%2520Code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/FangyunWei/SLRT.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2401.04730v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Simple%20Baseline%20for%20Spoken%20Language%20to%20Sign%20Language%20Translation%20with%0A%20%203D%20Avatars&entry.906535625=Ronglai%20Zuo%20and%20Fangyun%20Wei%20and%20Zenggui%20Chen%20and%20Brian%20Mak%20and%20Jiaolong%20Yang%20and%20Xin%20Tong&entry.1292438233=%20%20The%20objective%20of%20this%20paper%20is%20to%20develop%20a%20functional%20system%20for%20translating%0Aspoken%20languages%20into%20sign%20languages%2C%20referred%20to%20as%20Spoken2Sign%20translation.%0AThe%20Spoken2Sign%20task%20is%20orthogonal%20and%20complementary%20to%20traditional%20sign%0Alanguage%20to%20spoken%20language%20%28Sign2Spoken%29%20translation.%20To%20enable%20Spoken2Sign%0Atranslation%2C%20we%20present%20a%20simple%20baseline%20consisting%20of%20three%20steps%3A%201%29%0Acreating%20a%20gloss-video%20dictionary%20using%20existing%20Sign2Spoken%20benchmarks%3B%202%29%0Aestimating%20a%203D%20sign%20for%20each%20sign%20video%20in%20the%20dictionary%3B%203%29%20training%20a%0ASpoken2Sign%20model%2C%20which%20is%20composed%20of%20a%20Text2Gloss%20translator%2C%20a%20sign%0Aconnector%2C%20and%20a%20rendering%20module%2C%20with%20the%20aid%20of%20the%20yielded%20gloss-3D%20sign%0Adictionary.%20The%20translation%20results%20are%20then%20displayed%20through%20a%20sign%20avatar.%0AAs%20far%20as%20we%20know%2C%20we%20are%20the%20first%20to%20present%20the%20Spoken2Sign%20task%20in%20an%0Aoutput%20format%20of%203D%20signs.%20In%20addition%20to%20its%20capability%20of%20Spoken2Sign%0Atranslation%2C%20we%20also%20demonstrate%20that%20two%20by-products%20of%20our%20approach-3D%0Akeypoint%20augmentation%20and%20multi-view%20understanding-can%20assist%20in%20keypoint-based%0Asign%20language%20understanding.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/FangyunWei/SLRT.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.04730v2&entry.124074799=Read"},
{"title": "Applying Extended Object Tracking for Self-Localization of Roadside\n  Radar Sensors", "author": "Longfei Han and Qiuyu Xu and Klaus Kefferp\u00fctz and Gordon Elger and J\u00fcrgen Beyerer", "abstract": "  Intelligent Transportation Systems (ITS) can benefit from roadside 4D mmWave\nradar sensors for large-scale traffic monitoring due to their weatherproof\nfunctionality, long sensing range and low manufacturing cost. However, the\nlocalization method using external measurement devices has limitations in urban\nenvironments. Furthermore, if the sensor mount exhibits changes due to\nenvironmental influences, they cannot be corrected when the measurement is\nperformed only during the installation. In this paper, we propose\nself-localization of roadside radar data using Extended Object Tracking (EOT).\nThe method analyses both the tracked trajectories of the vehicles observed by\nthe sensor and the aerial laser scan of city streets, assigns labels of driving\nbehaviors such as \"straight ahead\", \"left turn\", \"right turn\" to trajectory\nsections and road segments, and performs Semantic Iterative Closest Points\n(SICP) algorithm to register the point cloud. The method exploits the result\nfrom a down stream task -- object tracking -- for localization. We demonstrate\nhigh accuracy in the sub-meter range along with very low orientation error. The\nmethod also shows good data efficiency. The evaluation is done in both\nsimulation and real-world tests.\n", "link": "http://arxiv.org/abs/2407.03084v1", "date": "2024-07-03", "relevancy": 2.6507, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5444}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5237}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Applying%20Extended%20Object%20Tracking%20for%20Self-Localization%20of%20Roadside%0A%20%20Radar%20Sensors&body=Title%3A%20Applying%20Extended%20Object%20Tracking%20for%20Self-Localization%20of%20Roadside%0A%20%20Radar%20Sensors%0AAuthor%3A%20Longfei%20Han%20and%20Qiuyu%20Xu%20and%20Klaus%20Kefferp%C3%BCtz%20and%20Gordon%20Elger%20and%20J%C3%BCrgen%20Beyerer%0AAbstract%3A%20%20%20Intelligent%20Transportation%20Systems%20%28ITS%29%20can%20benefit%20from%20roadside%204D%20mmWave%0Aradar%20sensors%20for%20large-scale%20traffic%20monitoring%20due%20to%20their%20weatherproof%0Afunctionality%2C%20long%20sensing%20range%20and%20low%20manufacturing%20cost.%20However%2C%20the%0Alocalization%20method%20using%20external%20measurement%20devices%20has%20limitations%20in%20urban%0Aenvironments.%20Furthermore%2C%20if%20the%20sensor%20mount%20exhibits%20changes%20due%20to%0Aenvironmental%20influences%2C%20they%20cannot%20be%20corrected%20when%20the%20measurement%20is%0Aperformed%20only%20during%20the%20installation.%20In%20this%20paper%2C%20we%20propose%0Aself-localization%20of%20roadside%20radar%20data%20using%20Extended%20Object%20Tracking%20%28EOT%29.%0AThe%20method%20analyses%20both%20the%20tracked%20trajectories%20of%20the%20vehicles%20observed%20by%0Athe%20sensor%20and%20the%20aerial%20laser%20scan%20of%20city%20streets%2C%20assigns%20labels%20of%20driving%0Abehaviors%20such%20as%20%22straight%20ahead%22%2C%20%22left%20turn%22%2C%20%22right%20turn%22%20to%20trajectory%0Asections%20and%20road%20segments%2C%20and%20performs%20Semantic%20Iterative%20Closest%20Points%0A%28SICP%29%20algorithm%20to%20register%20the%20point%20cloud.%20The%20method%20exploits%20the%20result%0Afrom%20a%20down%20stream%20task%20--%20object%20tracking%20--%20for%20localization.%20We%20demonstrate%0Ahigh%20accuracy%20in%20the%20sub-meter%20range%20along%20with%20very%20low%20orientation%20error.%20The%0Amethod%20also%20shows%20good%20data%20efficiency.%20The%20evaluation%20is%20done%20in%20both%0Asimulation%20and%20real-world%20tests.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DApplying%2520Extended%2520Object%2520Tracking%2520for%2520Self-Localization%2520of%2520Roadside%250A%2520%2520Radar%2520Sensors%26entry.906535625%3DLongfei%2520Han%2520and%2520Qiuyu%2520Xu%2520and%2520Klaus%2520Kefferp%25C3%25BCtz%2520and%2520Gordon%2520Elger%2520and%2520J%25C3%25BCrgen%2520Beyerer%26entry.1292438233%3D%2520%2520Intelligent%2520Transportation%2520Systems%2520%2528ITS%2529%2520can%2520benefit%2520from%2520roadside%25204D%2520mmWave%250Aradar%2520sensors%2520for%2520large-scale%2520traffic%2520monitoring%2520due%2520to%2520their%2520weatherproof%250Afunctionality%252C%2520long%2520sensing%2520range%2520and%2520low%2520manufacturing%2520cost.%2520However%252C%2520the%250Alocalization%2520method%2520using%2520external%2520measurement%2520devices%2520has%2520limitations%2520in%2520urban%250Aenvironments.%2520Furthermore%252C%2520if%2520the%2520sensor%2520mount%2520exhibits%2520changes%2520due%2520to%250Aenvironmental%2520influences%252C%2520they%2520cannot%2520be%2520corrected%2520when%2520the%2520measurement%2520is%250Aperformed%2520only%2520during%2520the%2520installation.%2520In%2520this%2520paper%252C%2520we%2520propose%250Aself-localization%2520of%2520roadside%2520radar%2520data%2520using%2520Extended%2520Object%2520Tracking%2520%2528EOT%2529.%250AThe%2520method%2520analyses%2520both%2520the%2520tracked%2520trajectories%2520of%2520the%2520vehicles%2520observed%2520by%250Athe%2520sensor%2520and%2520the%2520aerial%2520laser%2520scan%2520of%2520city%2520streets%252C%2520assigns%2520labels%2520of%2520driving%250Abehaviors%2520such%2520as%2520%2522straight%2520ahead%2522%252C%2520%2522left%2520turn%2522%252C%2520%2522right%2520turn%2522%2520to%2520trajectory%250Asections%2520and%2520road%2520segments%252C%2520and%2520performs%2520Semantic%2520Iterative%2520Closest%2520Points%250A%2528SICP%2529%2520algorithm%2520to%2520register%2520the%2520point%2520cloud.%2520The%2520method%2520exploits%2520the%2520result%250Afrom%2520a%2520down%2520stream%2520task%2520--%2520object%2520tracking%2520--%2520for%2520localization.%2520We%2520demonstrate%250Ahigh%2520accuracy%2520in%2520the%2520sub-meter%2520range%2520along%2520with%2520very%2520low%2520orientation%2520error.%2520The%250Amethod%2520also%2520shows%2520good%2520data%2520efficiency.%2520The%2520evaluation%2520is%2520done%2520in%2520both%250Asimulation%2520and%2520real-world%2520tests.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Applying%20Extended%20Object%20Tracking%20for%20Self-Localization%20of%20Roadside%0A%20%20Radar%20Sensors&entry.906535625=Longfei%20Han%20and%20Qiuyu%20Xu%20and%20Klaus%20Kefferp%C3%BCtz%20and%20Gordon%20Elger%20and%20J%C3%BCrgen%20Beyerer&entry.1292438233=%20%20Intelligent%20Transportation%20Systems%20%28ITS%29%20can%20benefit%20from%20roadside%204D%20mmWave%0Aradar%20sensors%20for%20large-scale%20traffic%20monitoring%20due%20to%20their%20weatherproof%0Afunctionality%2C%20long%20sensing%20range%20and%20low%20manufacturing%20cost.%20However%2C%20the%0Alocalization%20method%20using%20external%20measurement%20devices%20has%20limitations%20in%20urban%0Aenvironments.%20Furthermore%2C%20if%20the%20sensor%20mount%20exhibits%20changes%20due%20to%0Aenvironmental%20influences%2C%20they%20cannot%20be%20corrected%20when%20the%20measurement%20is%0Aperformed%20only%20during%20the%20installation.%20In%20this%20paper%2C%20we%20propose%0Aself-localization%20of%20roadside%20radar%20data%20using%20Extended%20Object%20Tracking%20%28EOT%29.%0AThe%20method%20analyses%20both%20the%20tracked%20trajectories%20of%20the%20vehicles%20observed%20by%0Athe%20sensor%20and%20the%20aerial%20laser%20scan%20of%20city%20streets%2C%20assigns%20labels%20of%20driving%0Abehaviors%20such%20as%20%22straight%20ahead%22%2C%20%22left%20turn%22%2C%20%22right%20turn%22%20to%20trajectory%0Asections%20and%20road%20segments%2C%20and%20performs%20Semantic%20Iterative%20Closest%20Points%0A%28SICP%29%20algorithm%20to%20register%20the%20point%20cloud.%20The%20method%20exploits%20the%20result%0Afrom%20a%20down%20stream%20task%20--%20object%20tracking%20--%20for%20localization.%20We%20demonstrate%0Ahigh%20accuracy%20in%20the%20sub-meter%20range%20along%20with%20very%20low%20orientation%20error.%20The%0Amethod%20also%20shows%20good%20data%20efficiency.%20The%20evaluation%20is%20done%20in%20both%0Asimulation%20and%20real-world%20tests.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03084v1&entry.124074799=Read"},
{"title": "IMC 2024 Methods & Solutions Review", "author": "Shyam Gupta and Dhanisha Sharma and Songling Huang", "abstract": "  For the past three years, Kaggle has been hosting the Image Matching\nChallenge, which focuses on solving a 3D image reconstruction problem using a\ncollection of 2D images. Each year, this competition fosters the development of\ninnovative and effective methodologies by its participants. In this paper, we\nintroduce an advanced ensemble technique that we developed, achieving a score\nof 0.153449 on the private leaderboard and securing the 160th position out of\nover 1,000 participants. Additionally, we conduct a comprehensive review of\nexisting methods and techniques employed by top-performing teams in the\ncompetition. Our solution, alongside the insights gathered from other leading\napproaches, contributes to the ongoing advancement in the field of 3D image\nreconstruction. This research provides valuable knowledge for future\nparticipants and researchers aiming to excel in similar image matching and\nreconstruction challenges.\n", "link": "http://arxiv.org/abs/2407.03172v1", "date": "2024-07-03", "relevancy": 2.6048, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5368}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.513}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IMC%202024%20Methods%20%26%20Solutions%20Review&body=Title%3A%20IMC%202024%20Methods%20%26%20Solutions%20Review%0AAuthor%3A%20Shyam%20Gupta%20and%20Dhanisha%20Sharma%20and%20Songling%20Huang%0AAbstract%3A%20%20%20For%20the%20past%20three%20years%2C%20Kaggle%20has%20been%20hosting%20the%20Image%20Matching%0AChallenge%2C%20which%20focuses%20on%20solving%20a%203D%20image%20reconstruction%20problem%20using%20a%0Acollection%20of%202D%20images.%20Each%20year%2C%20this%20competition%20fosters%20the%20development%20of%0Ainnovative%20and%20effective%20methodologies%20by%20its%20participants.%20In%20this%20paper%2C%20we%0Aintroduce%20an%20advanced%20ensemble%20technique%20that%20we%20developed%2C%20achieving%20a%20score%0Aof%200.153449%20on%20the%20private%20leaderboard%20and%20securing%20the%20160th%20position%20out%20of%0Aover%201%2C000%20participants.%20Additionally%2C%20we%20conduct%20a%20comprehensive%20review%20of%0Aexisting%20methods%20and%20techniques%20employed%20by%20top-performing%20teams%20in%20the%0Acompetition.%20Our%20solution%2C%20alongside%20the%20insights%20gathered%20from%20other%20leading%0Aapproaches%2C%20contributes%20to%20the%20ongoing%20advancement%20in%20the%20field%20of%203D%20image%0Areconstruction.%20This%20research%20provides%20valuable%20knowledge%20for%20future%0Aparticipants%20and%20researchers%20aiming%20to%20excel%20in%20similar%20image%20matching%20and%0Areconstruction%20challenges.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03172v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIMC%25202024%2520Methods%2520%2526%2520Solutions%2520Review%26entry.906535625%3DShyam%2520Gupta%2520and%2520Dhanisha%2520Sharma%2520and%2520Songling%2520Huang%26entry.1292438233%3D%2520%2520For%2520the%2520past%2520three%2520years%252C%2520Kaggle%2520has%2520been%2520hosting%2520the%2520Image%2520Matching%250AChallenge%252C%2520which%2520focuses%2520on%2520solving%2520a%25203D%2520image%2520reconstruction%2520problem%2520using%2520a%250Acollection%2520of%25202D%2520images.%2520Each%2520year%252C%2520this%2520competition%2520fosters%2520the%2520development%2520of%250Ainnovative%2520and%2520effective%2520methodologies%2520by%2520its%2520participants.%2520In%2520this%2520paper%252C%2520we%250Aintroduce%2520an%2520advanced%2520ensemble%2520technique%2520that%2520we%2520developed%252C%2520achieving%2520a%2520score%250Aof%25200.153449%2520on%2520the%2520private%2520leaderboard%2520and%2520securing%2520the%2520160th%2520position%2520out%2520of%250Aover%25201%252C000%2520participants.%2520Additionally%252C%2520we%2520conduct%2520a%2520comprehensive%2520review%2520of%250Aexisting%2520methods%2520and%2520techniques%2520employed%2520by%2520top-performing%2520teams%2520in%2520the%250Acompetition.%2520Our%2520solution%252C%2520alongside%2520the%2520insights%2520gathered%2520from%2520other%2520leading%250Aapproaches%252C%2520contributes%2520to%2520the%2520ongoing%2520advancement%2520in%2520the%2520field%2520of%25203D%2520image%250Areconstruction.%2520This%2520research%2520provides%2520valuable%2520knowledge%2520for%2520future%250Aparticipants%2520and%2520researchers%2520aiming%2520to%2520excel%2520in%2520similar%2520image%2520matching%2520and%250Areconstruction%2520challenges.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03172v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IMC%202024%20Methods%20%26%20Solutions%20Review&entry.906535625=Shyam%20Gupta%20and%20Dhanisha%20Sharma%20and%20Songling%20Huang&entry.1292438233=%20%20For%20the%20past%20three%20years%2C%20Kaggle%20has%20been%20hosting%20the%20Image%20Matching%0AChallenge%2C%20which%20focuses%20on%20solving%20a%203D%20image%20reconstruction%20problem%20using%20a%0Acollection%20of%202D%20images.%20Each%20year%2C%20this%20competition%20fosters%20the%20development%20of%0Ainnovative%20and%20effective%20methodologies%20by%20its%20participants.%20In%20this%20paper%2C%20we%0Aintroduce%20an%20advanced%20ensemble%20technique%20that%20we%20developed%2C%20achieving%20a%20score%0Aof%200.153449%20on%20the%20private%20leaderboard%20and%20securing%20the%20160th%20position%20out%20of%0Aover%201%2C000%20participants.%20Additionally%2C%20we%20conduct%20a%20comprehensive%20review%20of%0Aexisting%20methods%20and%20techniques%20employed%20by%20top-performing%20teams%20in%20the%0Acompetition.%20Our%20solution%2C%20alongside%20the%20insights%20gathered%20from%20other%20leading%0Aapproaches%2C%20contributes%20to%20the%20ongoing%20advancement%20in%20the%20field%20of%203D%20image%0Areconstruction.%20This%20research%20provides%20valuable%20knowledge%20for%20future%0Aparticipants%20and%20researchers%20aiming%20to%20excel%20in%20similar%20image%20matching%20and%0Areconstruction%20challenges.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03172v1&entry.124074799=Read"},
{"title": "SAFT: Towards Out-of-Distribution Generalization in Fine-Tuning", "author": "Bac Nguyen and Stefan Uhlich and Fabien Cardinaux and Lukas Mauch and Marzieh Edraki and Aaron Courville", "abstract": "  Handling distribution shifts from training data, known as out-of-distribution\n(OOD) generalization, poses a significant challenge in the field of machine\nlearning. While a pre-trained vision-language model like CLIP has demonstrated\nremarkable zero-shot performance, further adaptation of the model to downstream\ntasks leads to undesirable degradation for OOD data. In this work, we introduce\nSparse Adaptation for Fine-Tuning (SAFT), a method that prevents fine-tuning\nfrom forgetting the general knowledge in the pre-trained model. SAFT only\nupdates a small subset of important parameters whose gradient magnitude is\nlarge, while keeping the other parameters frozen. SAFT is straightforward to\nimplement and conceptually simple. Extensive experiments show that with only\n0.1% of the model parameters, SAFT can significantly improve the performance of\nCLIP. It consistently outperforms baseline methods across several benchmarks.\nOn the few-shot learning benchmark of ImageNet and its variants, SAFT gives a\ngain of 5.15% on average over the conventional fine-tuning method in OOD\nsettings.\n", "link": "http://arxiv.org/abs/2407.03036v1", "date": "2024-07-03", "relevancy": 2.5987, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5448}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5251}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SAFT%3A%20Towards%20Out-of-Distribution%20Generalization%20in%20Fine-Tuning&body=Title%3A%20SAFT%3A%20Towards%20Out-of-Distribution%20Generalization%20in%20Fine-Tuning%0AAuthor%3A%20Bac%20Nguyen%20and%20Stefan%20Uhlich%20and%20Fabien%20Cardinaux%20and%20Lukas%20Mauch%20and%20Marzieh%20Edraki%20and%20Aaron%20Courville%0AAbstract%3A%20%20%20Handling%20distribution%20shifts%20from%20training%20data%2C%20known%20as%20out-of-distribution%0A%28OOD%29%20generalization%2C%20poses%20a%20significant%20challenge%20in%20the%20field%20of%20machine%0Alearning.%20While%20a%20pre-trained%20vision-language%20model%20like%20CLIP%20has%20demonstrated%0Aremarkable%20zero-shot%20performance%2C%20further%20adaptation%20of%20the%20model%20to%20downstream%0Atasks%20leads%20to%20undesirable%20degradation%20for%20OOD%20data.%20In%20this%20work%2C%20we%20introduce%0ASparse%20Adaptation%20for%20Fine-Tuning%20%28SAFT%29%2C%20a%20method%20that%20prevents%20fine-tuning%0Afrom%20forgetting%20the%20general%20knowledge%20in%20the%20pre-trained%20model.%20SAFT%20only%0Aupdates%20a%20small%20subset%20of%20important%20parameters%20whose%20gradient%20magnitude%20is%0Alarge%2C%20while%20keeping%20the%20other%20parameters%20frozen.%20SAFT%20is%20straightforward%20to%0Aimplement%20and%20conceptually%20simple.%20Extensive%20experiments%20show%20that%20with%20only%0A0.1%25%20of%20the%20model%20parameters%2C%20SAFT%20can%20significantly%20improve%20the%20performance%20of%0ACLIP.%20It%20consistently%20outperforms%20baseline%20methods%20across%20several%20benchmarks.%0AOn%20the%20few-shot%20learning%20benchmark%20of%20ImageNet%20and%20its%20variants%2C%20SAFT%20gives%20a%0Again%20of%205.15%25%20on%20average%20over%20the%20conventional%20fine-tuning%20method%20in%20OOD%0Asettings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03036v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSAFT%253A%2520Towards%2520Out-of-Distribution%2520Generalization%2520in%2520Fine-Tuning%26entry.906535625%3DBac%2520Nguyen%2520and%2520Stefan%2520Uhlich%2520and%2520Fabien%2520Cardinaux%2520and%2520Lukas%2520Mauch%2520and%2520Marzieh%2520Edraki%2520and%2520Aaron%2520Courville%26entry.1292438233%3D%2520%2520Handling%2520distribution%2520shifts%2520from%2520training%2520data%252C%2520known%2520as%2520out-of-distribution%250A%2528OOD%2529%2520generalization%252C%2520poses%2520a%2520significant%2520challenge%2520in%2520the%2520field%2520of%2520machine%250Alearning.%2520While%2520a%2520pre-trained%2520vision-language%2520model%2520like%2520CLIP%2520has%2520demonstrated%250Aremarkable%2520zero-shot%2520performance%252C%2520further%2520adaptation%2520of%2520the%2520model%2520to%2520downstream%250Atasks%2520leads%2520to%2520undesirable%2520degradation%2520for%2520OOD%2520data.%2520In%2520this%2520work%252C%2520we%2520introduce%250ASparse%2520Adaptation%2520for%2520Fine-Tuning%2520%2528SAFT%2529%252C%2520a%2520method%2520that%2520prevents%2520fine-tuning%250Afrom%2520forgetting%2520the%2520general%2520knowledge%2520in%2520the%2520pre-trained%2520model.%2520SAFT%2520only%250Aupdates%2520a%2520small%2520subset%2520of%2520important%2520parameters%2520whose%2520gradient%2520magnitude%2520is%250Alarge%252C%2520while%2520keeping%2520the%2520other%2520parameters%2520frozen.%2520SAFT%2520is%2520straightforward%2520to%250Aimplement%2520and%2520conceptually%2520simple.%2520Extensive%2520experiments%2520show%2520that%2520with%2520only%250A0.1%2525%2520of%2520the%2520model%2520parameters%252C%2520SAFT%2520can%2520significantly%2520improve%2520the%2520performance%2520of%250ACLIP.%2520It%2520consistently%2520outperforms%2520baseline%2520methods%2520across%2520several%2520benchmarks.%250AOn%2520the%2520few-shot%2520learning%2520benchmark%2520of%2520ImageNet%2520and%2520its%2520variants%252C%2520SAFT%2520gives%2520a%250Again%2520of%25205.15%2525%2520on%2520average%2520over%2520the%2520conventional%2520fine-tuning%2520method%2520in%2520OOD%250Asettings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03036v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SAFT%3A%20Towards%20Out-of-Distribution%20Generalization%20in%20Fine-Tuning&entry.906535625=Bac%20Nguyen%20and%20Stefan%20Uhlich%20and%20Fabien%20Cardinaux%20and%20Lukas%20Mauch%20and%20Marzieh%20Edraki%20and%20Aaron%20Courville&entry.1292438233=%20%20Handling%20distribution%20shifts%20from%20training%20data%2C%20known%20as%20out-of-distribution%0A%28OOD%29%20generalization%2C%20poses%20a%20significant%20challenge%20in%20the%20field%20of%20machine%0Alearning.%20While%20a%20pre-trained%20vision-language%20model%20like%20CLIP%20has%20demonstrated%0Aremarkable%20zero-shot%20performance%2C%20further%20adaptation%20of%20the%20model%20to%20downstream%0Atasks%20leads%20to%20undesirable%20degradation%20for%20OOD%20data.%20In%20this%20work%2C%20we%20introduce%0ASparse%20Adaptation%20for%20Fine-Tuning%20%28SAFT%29%2C%20a%20method%20that%20prevents%20fine-tuning%0Afrom%20forgetting%20the%20general%20knowledge%20in%20the%20pre-trained%20model.%20SAFT%20only%0Aupdates%20a%20small%20subset%20of%20important%20parameters%20whose%20gradient%20magnitude%20is%0Alarge%2C%20while%20keeping%20the%20other%20parameters%20frozen.%20SAFT%20is%20straightforward%20to%0Aimplement%20and%20conceptually%20simple.%20Extensive%20experiments%20show%20that%20with%20only%0A0.1%25%20of%20the%20model%20parameters%2C%20SAFT%20can%20significantly%20improve%20the%20performance%20of%0ACLIP.%20It%20consistently%20outperforms%20baseline%20methods%20across%20several%20benchmarks.%0AOn%20the%20few-shot%20learning%20benchmark%20of%20ImageNet%20and%20its%20variants%2C%20SAFT%20gives%20a%0Again%20of%205.15%25%20on%20average%20over%20the%20conventional%20fine-tuning%20method%20in%20OOD%0Asettings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03036v1&entry.124074799=Read"},
{"title": "Semantically Rich Local Dataset Generation for Explainable AI in\n  Genomics", "author": "Pedro Barbosa and Rosina Savisaar and Alcides Fonseca", "abstract": "  Black box deep learning models trained on genomic sequences excel at\npredicting the outcomes of different gene regulatory mechanisms. Therefore,\ninterpreting these models may provide novel insights into the underlying\nbiology, supporting downstream biomedical applications. Due to their\ncomplexity, interpretable surrogate models can only be built for local\nexplanations (e.g., a single instance). However, accomplishing this requires\ngenerating a dataset in the neighborhood of the input, which must maintain\nsyntactic similarity to the original data while introducing semantic\nvariability in the model's predictions. This task is challenging due to the\ncomplex sequence-to-function relationship of DNA.\n  We propose using Genetic Programming to generate datasets by evolving\nperturbations in sequences that contribute to their semantic diversity. Our\ncustom, domain-guided individual representation effectively constrains\nsyntactic similarity, and we provide two alternative fitness functions that\npromote diversity with no computational effort. Applied to the RNA splicing\ndomain, our approach quickly achieves good diversity and significantly\noutperforms a random baseline in exploring the search space, as shown by our\nproof-of-concept, short RNA sequence. Furthermore, we assess its\ngeneralizability and demonstrate scalability to larger sequences, resulting in\na $\\approx$30\\% improvement over the baseline.\n", "link": "http://arxiv.org/abs/2407.02984v1", "date": "2024-07-03", "relevancy": 2.5375, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5222}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5005}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4998}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semantically%20Rich%20Local%20Dataset%20Generation%20for%20Explainable%20AI%20in%0A%20%20Genomics&body=Title%3A%20Semantically%20Rich%20Local%20Dataset%20Generation%20for%20Explainable%20AI%20in%0A%20%20Genomics%0AAuthor%3A%20Pedro%20Barbosa%20and%20Rosina%20Savisaar%20and%20Alcides%20Fonseca%0AAbstract%3A%20%20%20Black%20box%20deep%20learning%20models%20trained%20on%20genomic%20sequences%20excel%20at%0Apredicting%20the%20outcomes%20of%20different%20gene%20regulatory%20mechanisms.%20Therefore%2C%0Ainterpreting%20these%20models%20may%20provide%20novel%20insights%20into%20the%20underlying%0Abiology%2C%20supporting%20downstream%20biomedical%20applications.%20Due%20to%20their%0Acomplexity%2C%20interpretable%20surrogate%20models%20can%20only%20be%20built%20for%20local%0Aexplanations%20%28e.g.%2C%20a%20single%20instance%29.%20However%2C%20accomplishing%20this%20requires%0Agenerating%20a%20dataset%20in%20the%20neighborhood%20of%20the%20input%2C%20which%20must%20maintain%0Asyntactic%20similarity%20to%20the%20original%20data%20while%20introducing%20semantic%0Avariability%20in%20the%20model%27s%20predictions.%20This%20task%20is%20challenging%20due%20to%20the%0Acomplex%20sequence-to-function%20relationship%20of%20DNA.%0A%20%20We%20propose%20using%20Genetic%20Programming%20to%20generate%20datasets%20by%20evolving%0Aperturbations%20in%20sequences%20that%20contribute%20to%20their%20semantic%20diversity.%20Our%0Acustom%2C%20domain-guided%20individual%20representation%20effectively%20constrains%0Asyntactic%20similarity%2C%20and%20we%20provide%20two%20alternative%20fitness%20functions%20that%0Apromote%20diversity%20with%20no%20computational%20effort.%20Applied%20to%20the%20RNA%20splicing%0Adomain%2C%20our%20approach%20quickly%20achieves%20good%20diversity%20and%20significantly%0Aoutperforms%20a%20random%20baseline%20in%20exploring%20the%20search%20space%2C%20as%20shown%20by%20our%0Aproof-of-concept%2C%20short%20RNA%20sequence.%20Furthermore%2C%20we%20assess%20its%0Ageneralizability%20and%20demonstrate%20scalability%20to%20larger%20sequences%2C%20resulting%20in%0Aa%20%24%5Capprox%2430%5C%25%20improvement%20over%20the%20baseline.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02984v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemantically%2520Rich%2520Local%2520Dataset%2520Generation%2520for%2520Explainable%2520AI%2520in%250A%2520%2520Genomics%26entry.906535625%3DPedro%2520Barbosa%2520and%2520Rosina%2520Savisaar%2520and%2520Alcides%2520Fonseca%26entry.1292438233%3D%2520%2520Black%2520box%2520deep%2520learning%2520models%2520trained%2520on%2520genomic%2520sequences%2520excel%2520at%250Apredicting%2520the%2520outcomes%2520of%2520different%2520gene%2520regulatory%2520mechanisms.%2520Therefore%252C%250Ainterpreting%2520these%2520models%2520may%2520provide%2520novel%2520insights%2520into%2520the%2520underlying%250Abiology%252C%2520supporting%2520downstream%2520biomedical%2520applications.%2520Due%2520to%2520their%250Acomplexity%252C%2520interpretable%2520surrogate%2520models%2520can%2520only%2520be%2520built%2520for%2520local%250Aexplanations%2520%2528e.g.%252C%2520a%2520single%2520instance%2529.%2520However%252C%2520accomplishing%2520this%2520requires%250Agenerating%2520a%2520dataset%2520in%2520the%2520neighborhood%2520of%2520the%2520input%252C%2520which%2520must%2520maintain%250Asyntactic%2520similarity%2520to%2520the%2520original%2520data%2520while%2520introducing%2520semantic%250Avariability%2520in%2520the%2520model%2527s%2520predictions.%2520This%2520task%2520is%2520challenging%2520due%2520to%2520the%250Acomplex%2520sequence-to-function%2520relationship%2520of%2520DNA.%250A%2520%2520We%2520propose%2520using%2520Genetic%2520Programming%2520to%2520generate%2520datasets%2520by%2520evolving%250Aperturbations%2520in%2520sequences%2520that%2520contribute%2520to%2520their%2520semantic%2520diversity.%2520Our%250Acustom%252C%2520domain-guided%2520individual%2520representation%2520effectively%2520constrains%250Asyntactic%2520similarity%252C%2520and%2520we%2520provide%2520two%2520alternative%2520fitness%2520functions%2520that%250Apromote%2520diversity%2520with%2520no%2520computational%2520effort.%2520Applied%2520to%2520the%2520RNA%2520splicing%250Adomain%252C%2520our%2520approach%2520quickly%2520achieves%2520good%2520diversity%2520and%2520significantly%250Aoutperforms%2520a%2520random%2520baseline%2520in%2520exploring%2520the%2520search%2520space%252C%2520as%2520shown%2520by%2520our%250Aproof-of-concept%252C%2520short%2520RNA%2520sequence.%2520Furthermore%252C%2520we%2520assess%2520its%250Ageneralizability%2520and%2520demonstrate%2520scalability%2520to%2520larger%2520sequences%252C%2520resulting%2520in%250Aa%2520%2524%255Capprox%252430%255C%2525%2520improvement%2520over%2520the%2520baseline.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02984v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semantically%20Rich%20Local%20Dataset%20Generation%20for%20Explainable%20AI%20in%0A%20%20Genomics&entry.906535625=Pedro%20Barbosa%20and%20Rosina%20Savisaar%20and%20Alcides%20Fonseca&entry.1292438233=%20%20Black%20box%20deep%20learning%20models%20trained%20on%20genomic%20sequences%20excel%20at%0Apredicting%20the%20outcomes%20of%20different%20gene%20regulatory%20mechanisms.%20Therefore%2C%0Ainterpreting%20these%20models%20may%20provide%20novel%20insights%20into%20the%20underlying%0Abiology%2C%20supporting%20downstream%20biomedical%20applications.%20Due%20to%20their%0Acomplexity%2C%20interpretable%20surrogate%20models%20can%20only%20be%20built%20for%20local%0Aexplanations%20%28e.g.%2C%20a%20single%20instance%29.%20However%2C%20accomplishing%20this%20requires%0Agenerating%20a%20dataset%20in%20the%20neighborhood%20of%20the%20input%2C%20which%20must%20maintain%0Asyntactic%20similarity%20to%20the%20original%20data%20while%20introducing%20semantic%0Avariability%20in%20the%20model%27s%20predictions.%20This%20task%20is%20challenging%20due%20to%20the%0Acomplex%20sequence-to-function%20relationship%20of%20DNA.%0A%20%20We%20propose%20using%20Genetic%20Programming%20to%20generate%20datasets%20by%20evolving%0Aperturbations%20in%20sequences%20that%20contribute%20to%20their%20semantic%20diversity.%20Our%0Acustom%2C%20domain-guided%20individual%20representation%20effectively%20constrains%0Asyntactic%20similarity%2C%20and%20we%20provide%20two%20alternative%20fitness%20functions%20that%0Apromote%20diversity%20with%20no%20computational%20effort.%20Applied%20to%20the%20RNA%20splicing%0Adomain%2C%20our%20approach%20quickly%20achieves%20good%20diversity%20and%20significantly%0Aoutperforms%20a%20random%20baseline%20in%20exploring%20the%20search%20space%2C%20as%20shown%20by%20our%0Aproof-of-concept%2C%20short%20RNA%20sequence.%20Furthermore%2C%20we%20assess%20its%0Ageneralizability%20and%20demonstrate%20scalability%20to%20larger%20sequences%2C%20resulting%20in%0Aa%20%24%5Capprox%2430%5C%25%20improvement%20over%20the%20baseline.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02984v1&entry.124074799=Read"},
{"title": "Consistent Point Orientation for Manifold Surfaces via Boundary\n  Integration", "author": "Weizhou Liu and Xingce Wang and Haichuan Zhao and Xingfei Xue and Zhongke Wu and Xuequan Lu and Ying He", "abstract": "  This paper introduces a new approach for generating globally consistent\nnormals for point clouds sampled from manifold surfaces. Given that the\ngeneralized winding number (GWN) field generated by a point cloud with globally\nconsistent normals is a solution to a PDE with jump boundary conditions and\npossesses harmonic properties, and the Dirichlet energy of the GWN field can be\ndefined as an integral over the boundary surface, we formulate a boundary\nenergy derived from the Dirichlet energy of the GWN. Taking as input a point\ncloud with randomly oriented normals, we optimize this energy to restore the\nglobal harmonicity of the GWN field, thereby recovering the globally consistent\nnormals. Experiments show that our method outperforms state-of-the-art\napproaches, exhibiting enhanced robustness to noise, outliers, complex\ntopologies, and thin structures. Our code can be found at\n\\url{https://github.com/liuweizhou319/BIM}.\n", "link": "http://arxiv.org/abs/2407.03165v1", "date": "2024-07-03", "relevancy": 2.4921, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5609}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4674}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Consistent%20Point%20Orientation%20for%20Manifold%20Surfaces%20via%20Boundary%0A%20%20Integration&body=Title%3A%20Consistent%20Point%20Orientation%20for%20Manifold%20Surfaces%20via%20Boundary%0A%20%20Integration%0AAuthor%3A%20Weizhou%20Liu%20and%20Xingce%20Wang%20and%20Haichuan%20Zhao%20and%20Xingfei%20Xue%20and%20Zhongke%20Wu%20and%20Xuequan%20Lu%20and%20Ying%20He%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20new%20approach%20for%20generating%20globally%20consistent%0Anormals%20for%20point%20clouds%20sampled%20from%20manifold%20surfaces.%20Given%20that%20the%0Ageneralized%20winding%20number%20%28GWN%29%20field%20generated%20by%20a%20point%20cloud%20with%20globally%0Aconsistent%20normals%20is%20a%20solution%20to%20a%20PDE%20with%20jump%20boundary%20conditions%20and%0Apossesses%20harmonic%20properties%2C%20and%20the%20Dirichlet%20energy%20of%20the%20GWN%20field%20can%20be%0Adefined%20as%20an%20integral%20over%20the%20boundary%20surface%2C%20we%20formulate%20a%20boundary%0Aenergy%20derived%20from%20the%20Dirichlet%20energy%20of%20the%20GWN.%20Taking%20as%20input%20a%20point%0Acloud%20with%20randomly%20oriented%20normals%2C%20we%20optimize%20this%20energy%20to%20restore%20the%0Aglobal%20harmonicity%20of%20the%20GWN%20field%2C%20thereby%20recovering%20the%20globally%20consistent%0Anormals.%20Experiments%20show%20that%20our%20method%20outperforms%20state-of-the-art%0Aapproaches%2C%20exhibiting%20enhanced%20robustness%20to%20noise%2C%20outliers%2C%20complex%0Atopologies%2C%20and%20thin%20structures.%20Our%20code%20can%20be%20found%20at%0A%5Curl%7Bhttps%3A//github.com/liuweizhou319/BIM%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConsistent%2520Point%2520Orientation%2520for%2520Manifold%2520Surfaces%2520via%2520Boundary%250A%2520%2520Integration%26entry.906535625%3DWeizhou%2520Liu%2520and%2520Xingce%2520Wang%2520and%2520Haichuan%2520Zhao%2520and%2520Xingfei%2520Xue%2520and%2520Zhongke%2520Wu%2520and%2520Xuequan%2520Lu%2520and%2520Ying%2520He%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520new%2520approach%2520for%2520generating%2520globally%2520consistent%250Anormals%2520for%2520point%2520clouds%2520sampled%2520from%2520manifold%2520surfaces.%2520Given%2520that%2520the%250Ageneralized%2520winding%2520number%2520%2528GWN%2529%2520field%2520generated%2520by%2520a%2520point%2520cloud%2520with%2520globally%250Aconsistent%2520normals%2520is%2520a%2520solution%2520to%2520a%2520PDE%2520with%2520jump%2520boundary%2520conditions%2520and%250Apossesses%2520harmonic%2520properties%252C%2520and%2520the%2520Dirichlet%2520energy%2520of%2520the%2520GWN%2520field%2520can%2520be%250Adefined%2520as%2520an%2520integral%2520over%2520the%2520boundary%2520surface%252C%2520we%2520formulate%2520a%2520boundary%250Aenergy%2520derived%2520from%2520the%2520Dirichlet%2520energy%2520of%2520the%2520GWN.%2520Taking%2520as%2520input%2520a%2520point%250Acloud%2520with%2520randomly%2520oriented%2520normals%252C%2520we%2520optimize%2520this%2520energy%2520to%2520restore%2520the%250Aglobal%2520harmonicity%2520of%2520the%2520GWN%2520field%252C%2520thereby%2520recovering%2520the%2520globally%2520consistent%250Anormals.%2520Experiments%2520show%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%250Aapproaches%252C%2520exhibiting%2520enhanced%2520robustness%2520to%2520noise%252C%2520outliers%252C%2520complex%250Atopologies%252C%2520and%2520thin%2520structures.%2520Our%2520code%2520can%2520be%2520found%2520at%250A%255Curl%257Bhttps%253A//github.com/liuweizhou319/BIM%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Consistent%20Point%20Orientation%20for%20Manifold%20Surfaces%20via%20Boundary%0A%20%20Integration&entry.906535625=Weizhou%20Liu%20and%20Xingce%20Wang%20and%20Haichuan%20Zhao%20and%20Xingfei%20Xue%20and%20Zhongke%20Wu%20and%20Xuequan%20Lu%20and%20Ying%20He&entry.1292438233=%20%20This%20paper%20introduces%20a%20new%20approach%20for%20generating%20globally%20consistent%0Anormals%20for%20point%20clouds%20sampled%20from%20manifold%20surfaces.%20Given%20that%20the%0Ageneralized%20winding%20number%20%28GWN%29%20field%20generated%20by%20a%20point%20cloud%20with%20globally%0Aconsistent%20normals%20is%20a%20solution%20to%20a%20PDE%20with%20jump%20boundary%20conditions%20and%0Apossesses%20harmonic%20properties%2C%20and%20the%20Dirichlet%20energy%20of%20the%20GWN%20field%20can%20be%0Adefined%20as%20an%20integral%20over%20the%20boundary%20surface%2C%20we%20formulate%20a%20boundary%0Aenergy%20derived%20from%20the%20Dirichlet%20energy%20of%20the%20GWN.%20Taking%20as%20input%20a%20point%0Acloud%20with%20randomly%20oriented%20normals%2C%20we%20optimize%20this%20energy%20to%20restore%20the%0Aglobal%20harmonicity%20of%20the%20GWN%20field%2C%20thereby%20recovering%20the%20globally%20consistent%0Anormals.%20Experiments%20show%20that%20our%20method%20outperforms%20state-of-the-art%0Aapproaches%2C%20exhibiting%20enhanced%20robustness%20to%20noise%2C%20outliers%2C%20complex%0Atopologies%2C%20and%20thin%20structures.%20Our%20code%20can%20be%20found%20at%0A%5Curl%7Bhttps%3A//github.com/liuweizhou319/BIM%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03165v1&entry.124074799=Read"},
{"title": "ChatGPT Code Detection: Techniques for Uncovering the Source of Code", "author": "Marc Oedingen and Raphael C. Engelhardt and Robin Denz and Maximilian Hammer and Wolfgang Konen", "abstract": "  In recent times, large language models (LLMs) have made significant strides\nin generating computer code, blurring the lines between code created by humans\nand code produced by artificial intelligence (AI). As these technologies evolve\nrapidly, it is crucial to explore how they influence code generation,\nespecially given the risk of misuse in areas like higher education. This paper\nexplores this issue by using advanced classification techniques to\ndifferentiate between code written by humans and that generated by ChatGPT, a\ntype of LLM. We employ a new approach that combines powerful embedding features\n(black-box) with supervised learning algorithms - including Deep Neural\nNetworks, Random Forests, and Extreme Gradient Boosting - to achieve this\ndifferentiation with an impressive accuracy of 98%. For the successful\ncombinations, we also examine their model calibration, showing that some of the\nmodels are extremely well calibrated. Additionally, we present white-box\nfeatures and an interpretable Bayes classifier to elucidate critical\ndifferences between the code sources, enhancing the explainability and\ntransparency of our approach. Both approaches work well but provide at most\n85-88% accuracy. We also show that untrained humans solve the same task not\nbetter than random guessing. This study is crucial in understanding and\nmitigating the potential risks associated with using AI in code generation,\nparticularly in the context of higher education, software development, and\ncompetitive programming.\n", "link": "http://arxiv.org/abs/2405.15512v2", "date": "2024-07-03", "relevancy": 2.49, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5451}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4773}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ChatGPT%20Code%20Detection%3A%20Techniques%20for%20Uncovering%20the%20Source%20of%20Code&body=Title%3A%20ChatGPT%20Code%20Detection%3A%20Techniques%20for%20Uncovering%20the%20Source%20of%20Code%0AAuthor%3A%20Marc%20Oedingen%20and%20Raphael%20C.%20Engelhardt%20and%20Robin%20Denz%20and%20Maximilian%20Hammer%20and%20Wolfgang%20Konen%0AAbstract%3A%20%20%20In%20recent%20times%2C%20large%20language%20models%20%28LLMs%29%20have%20made%20significant%20strides%0Ain%20generating%20computer%20code%2C%20blurring%20the%20lines%20between%20code%20created%20by%20humans%0Aand%20code%20produced%20by%20artificial%20intelligence%20%28AI%29.%20As%20these%20technologies%20evolve%0Arapidly%2C%20it%20is%20crucial%20to%20explore%20how%20they%20influence%20code%20generation%2C%0Aespecially%20given%20the%20risk%20of%20misuse%20in%20areas%20like%20higher%20education.%20This%20paper%0Aexplores%20this%20issue%20by%20using%20advanced%20classification%20techniques%20to%0Adifferentiate%20between%20code%20written%20by%20humans%20and%20that%20generated%20by%20ChatGPT%2C%20a%0Atype%20of%20LLM.%20We%20employ%20a%20new%20approach%20that%20combines%20powerful%20embedding%20features%0A%28black-box%29%20with%20supervised%20learning%20algorithms%20-%20including%20Deep%20Neural%0ANetworks%2C%20Random%20Forests%2C%20and%20Extreme%20Gradient%20Boosting%20-%20to%20achieve%20this%0Adifferentiation%20with%20an%20impressive%20accuracy%20of%2098%25.%20For%20the%20successful%0Acombinations%2C%20we%20also%20examine%20their%20model%20calibration%2C%20showing%20that%20some%20of%20the%0Amodels%20are%20extremely%20well%20calibrated.%20Additionally%2C%20we%20present%20white-box%0Afeatures%20and%20an%20interpretable%20Bayes%20classifier%20to%20elucidate%20critical%0Adifferences%20between%20the%20code%20sources%2C%20enhancing%20the%20explainability%20and%0Atransparency%20of%20our%20approach.%20Both%20approaches%20work%20well%20but%20provide%20at%20most%0A85-88%25%20accuracy.%20We%20also%20show%20that%20untrained%20humans%20solve%20the%20same%20task%20not%0Abetter%20than%20random%20guessing.%20This%20study%20is%20crucial%20in%20understanding%20and%0Amitigating%20the%20potential%20risks%20associated%20with%20using%20AI%20in%20code%20generation%2C%0Aparticularly%20in%20the%20context%20of%20higher%20education%2C%20software%20development%2C%20and%0Acompetitive%20programming.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.15512v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChatGPT%2520Code%2520Detection%253A%2520Techniques%2520for%2520Uncovering%2520the%2520Source%2520of%2520Code%26entry.906535625%3DMarc%2520Oedingen%2520and%2520Raphael%2520C.%2520Engelhardt%2520and%2520Robin%2520Denz%2520and%2520Maximilian%2520Hammer%2520and%2520Wolfgang%2520Konen%26entry.1292438233%3D%2520%2520In%2520recent%2520times%252C%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520made%2520significant%2520strides%250Ain%2520generating%2520computer%2520code%252C%2520blurring%2520the%2520lines%2520between%2520code%2520created%2520by%2520humans%250Aand%2520code%2520produced%2520by%2520artificial%2520intelligence%2520%2528AI%2529.%2520As%2520these%2520technologies%2520evolve%250Arapidly%252C%2520it%2520is%2520crucial%2520to%2520explore%2520how%2520they%2520influence%2520code%2520generation%252C%250Aespecially%2520given%2520the%2520risk%2520of%2520misuse%2520in%2520areas%2520like%2520higher%2520education.%2520This%2520paper%250Aexplores%2520this%2520issue%2520by%2520using%2520advanced%2520classification%2520techniques%2520to%250Adifferentiate%2520between%2520code%2520written%2520by%2520humans%2520and%2520that%2520generated%2520by%2520ChatGPT%252C%2520a%250Atype%2520of%2520LLM.%2520We%2520employ%2520a%2520new%2520approach%2520that%2520combines%2520powerful%2520embedding%2520features%250A%2528black-box%2529%2520with%2520supervised%2520learning%2520algorithms%2520-%2520including%2520Deep%2520Neural%250ANetworks%252C%2520Random%2520Forests%252C%2520and%2520Extreme%2520Gradient%2520Boosting%2520-%2520to%2520achieve%2520this%250Adifferentiation%2520with%2520an%2520impressive%2520accuracy%2520of%252098%2525.%2520For%2520the%2520successful%250Acombinations%252C%2520we%2520also%2520examine%2520their%2520model%2520calibration%252C%2520showing%2520that%2520some%2520of%2520the%250Amodels%2520are%2520extremely%2520well%2520calibrated.%2520Additionally%252C%2520we%2520present%2520white-box%250Afeatures%2520and%2520an%2520interpretable%2520Bayes%2520classifier%2520to%2520elucidate%2520critical%250Adifferences%2520between%2520the%2520code%2520sources%252C%2520enhancing%2520the%2520explainability%2520and%250Atransparency%2520of%2520our%2520approach.%2520Both%2520approaches%2520work%2520well%2520but%2520provide%2520at%2520most%250A85-88%2525%2520accuracy.%2520We%2520also%2520show%2520that%2520untrained%2520humans%2520solve%2520the%2520same%2520task%2520not%250Abetter%2520than%2520random%2520guessing.%2520This%2520study%2520is%2520crucial%2520in%2520understanding%2520and%250Amitigating%2520the%2520potential%2520risks%2520associated%2520with%2520using%2520AI%2520in%2520code%2520generation%252C%250Aparticularly%2520in%2520the%2520context%2520of%2520higher%2520education%252C%2520software%2520development%252C%2520and%250Acompetitive%2520programming.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.15512v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ChatGPT%20Code%20Detection%3A%20Techniques%20for%20Uncovering%20the%20Source%20of%20Code&entry.906535625=Marc%20Oedingen%20and%20Raphael%20C.%20Engelhardt%20and%20Robin%20Denz%20and%20Maximilian%20Hammer%20and%20Wolfgang%20Konen&entry.1292438233=%20%20In%20recent%20times%2C%20large%20language%20models%20%28LLMs%29%20have%20made%20significant%20strides%0Ain%20generating%20computer%20code%2C%20blurring%20the%20lines%20between%20code%20created%20by%20humans%0Aand%20code%20produced%20by%20artificial%20intelligence%20%28AI%29.%20As%20these%20technologies%20evolve%0Arapidly%2C%20it%20is%20crucial%20to%20explore%20how%20they%20influence%20code%20generation%2C%0Aespecially%20given%20the%20risk%20of%20misuse%20in%20areas%20like%20higher%20education.%20This%20paper%0Aexplores%20this%20issue%20by%20using%20advanced%20classification%20techniques%20to%0Adifferentiate%20between%20code%20written%20by%20humans%20and%20that%20generated%20by%20ChatGPT%2C%20a%0Atype%20of%20LLM.%20We%20employ%20a%20new%20approach%20that%20combines%20powerful%20embedding%20features%0A%28black-box%29%20with%20supervised%20learning%20algorithms%20-%20including%20Deep%20Neural%0ANetworks%2C%20Random%20Forests%2C%20and%20Extreme%20Gradient%20Boosting%20-%20to%20achieve%20this%0Adifferentiation%20with%20an%20impressive%20accuracy%20of%2098%25.%20For%20the%20successful%0Acombinations%2C%20we%20also%20examine%20their%20model%20calibration%2C%20showing%20that%20some%20of%20the%0Amodels%20are%20extremely%20well%20calibrated.%20Additionally%2C%20we%20present%20white-box%0Afeatures%20and%20an%20interpretable%20Bayes%20classifier%20to%20elucidate%20critical%0Adifferences%20between%20the%20code%20sources%2C%20enhancing%20the%20explainability%20and%0Atransparency%20of%20our%20approach.%20Both%20approaches%20work%20well%20but%20provide%20at%20most%0A85-88%25%20accuracy.%20We%20also%20show%20that%20untrained%20humans%20solve%20the%20same%20task%20not%0Abetter%20than%20random%20guessing.%20This%20study%20is%20crucial%20in%20understanding%20and%0Amitigating%20the%20potential%20risks%20associated%20with%20using%20AI%20in%20code%20generation%2C%0Aparticularly%20in%20the%20context%20of%20higher%20education%2C%20software%20development%2C%20and%0Acompetitive%20programming.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.15512v2&entry.124074799=Read"},
{"title": "Incremental Gauss--Newton Methods with Superlinear Convergence Rates", "author": "Zhiling Zhou and Zhuanghua Liu and Chengchang Liu and Luo Luo", "abstract": "  This paper addresses the challenge of solving large-scale nonlinear equations\nwith H\\\"older continuous Jacobians. We introduce a novel Incremental\nGauss--Newton (IGN) method within explicit superlinear convergence rate, which\noutperforms existing methods that only achieve linear convergence rate. In\nparticular, we formulate our problem by the nonlinear least squares with\nfinite-sum structure, and our method incrementally iterates with the\ninformation of one component in each round. We also provide a mini-batch\nextension to our IGN method that obtains an even faster superlinear convergence\nrate. Furthermore, we conduct numerical experiments to show the advantages of\nthe proposed methods.\n", "link": "http://arxiv.org/abs/2407.03195v1", "date": "2024-07-03", "relevancy": 2.4766, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5223}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5016}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4622}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Incremental%20Gauss--Newton%20Methods%20with%20Superlinear%20Convergence%20Rates&body=Title%3A%20Incremental%20Gauss--Newton%20Methods%20with%20Superlinear%20Convergence%20Rates%0AAuthor%3A%20Zhiling%20Zhou%20and%20Zhuanghua%20Liu%20and%20Chengchang%20Liu%20and%20Luo%20Luo%0AAbstract%3A%20%20%20This%20paper%20addresses%20the%20challenge%20of%20solving%20large-scale%20nonlinear%20equations%0Awith%20H%5C%22older%20continuous%20Jacobians.%20We%20introduce%20a%20novel%20Incremental%0AGauss--Newton%20%28IGN%29%20method%20within%20explicit%20superlinear%20convergence%20rate%2C%20which%0Aoutperforms%20existing%20methods%20that%20only%20achieve%20linear%20convergence%20rate.%20In%0Aparticular%2C%20we%20formulate%20our%20problem%20by%20the%20nonlinear%20least%20squares%20with%0Afinite-sum%20structure%2C%20and%20our%20method%20incrementally%20iterates%20with%20the%0Ainformation%20of%20one%20component%20in%20each%20round.%20We%20also%20provide%20a%20mini-batch%0Aextension%20to%20our%20IGN%20method%20that%20obtains%20an%20even%20faster%20superlinear%20convergence%0Arate.%20Furthermore%2C%20we%20conduct%20numerical%20experiments%20to%20show%20the%20advantages%20of%0Athe%20proposed%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03195v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIncremental%2520Gauss--Newton%2520Methods%2520with%2520Superlinear%2520Convergence%2520Rates%26entry.906535625%3DZhiling%2520Zhou%2520and%2520Zhuanghua%2520Liu%2520and%2520Chengchang%2520Liu%2520and%2520Luo%2520Luo%26entry.1292438233%3D%2520%2520This%2520paper%2520addresses%2520the%2520challenge%2520of%2520solving%2520large-scale%2520nonlinear%2520equations%250Awith%2520H%255C%2522older%2520continuous%2520Jacobians.%2520We%2520introduce%2520a%2520novel%2520Incremental%250AGauss--Newton%2520%2528IGN%2529%2520method%2520within%2520explicit%2520superlinear%2520convergence%2520rate%252C%2520which%250Aoutperforms%2520existing%2520methods%2520that%2520only%2520achieve%2520linear%2520convergence%2520rate.%2520In%250Aparticular%252C%2520we%2520formulate%2520our%2520problem%2520by%2520the%2520nonlinear%2520least%2520squares%2520with%250Afinite-sum%2520structure%252C%2520and%2520our%2520method%2520incrementally%2520iterates%2520with%2520the%250Ainformation%2520of%2520one%2520component%2520in%2520each%2520round.%2520We%2520also%2520provide%2520a%2520mini-batch%250Aextension%2520to%2520our%2520IGN%2520method%2520that%2520obtains%2520an%2520even%2520faster%2520superlinear%2520convergence%250Arate.%2520Furthermore%252C%2520we%2520conduct%2520numerical%2520experiments%2520to%2520show%2520the%2520advantages%2520of%250Athe%2520proposed%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03195v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Incremental%20Gauss--Newton%20Methods%20with%20Superlinear%20Convergence%20Rates&entry.906535625=Zhiling%20Zhou%20and%20Zhuanghua%20Liu%20and%20Chengchang%20Liu%20and%20Luo%20Luo&entry.1292438233=%20%20This%20paper%20addresses%20the%20challenge%20of%20solving%20large-scale%20nonlinear%20equations%0Awith%20H%5C%22older%20continuous%20Jacobians.%20We%20introduce%20a%20novel%20Incremental%0AGauss--Newton%20%28IGN%29%20method%20within%20explicit%20superlinear%20convergence%20rate%2C%20which%0Aoutperforms%20existing%20methods%20that%20only%20achieve%20linear%20convergence%20rate.%20In%0Aparticular%2C%20we%20formulate%20our%20problem%20by%20the%20nonlinear%20least%20squares%20with%0Afinite-sum%20structure%2C%20and%20our%20method%20incrementally%20iterates%20with%20the%0Ainformation%20of%20one%20component%20in%20each%20round.%20We%20also%20provide%20a%20mini-batch%0Aextension%20to%20our%20IGN%20method%20that%20obtains%20an%20even%20faster%20superlinear%20convergence%0Arate.%20Furthermore%2C%20we%20conduct%20numerical%20experiments%20to%20show%20the%20advantages%20of%0Athe%20proposed%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03195v1&entry.124074799=Read"},
{"title": "Self-Cooperation Knowledge Distillation for Novel Class Discovery", "author": "Yuzheng Wang and Zhaoyu Chen and Dingkang Yang and Yunquan Sun and Lizhe Qi", "abstract": "  Novel Class Discovery (NCD) aims to discover unknown and novel classes in an\nunlabeled set by leveraging knowledge already learned about known classes.\nExisting works focus on instance-level or class-level knowledge representation\nand build a shared representation space to achieve performance improvements.\nHowever, a long-neglected issue is the potential imbalanced number of samples\nfrom known and novel classes, pushing the model towards dominant classes.\nTherefore, these methods suffer from a challenging trade-off between reviewing\nknown classes and discovering novel classes. Based on this observation, we\npropose a Self-Cooperation Knowledge Distillation (SCKD) method to utilize each\ntraining sample (whether known or novel, labeled or unlabeled) for both review\nand discovery. Specifically, the model's feature representations of known and\nnovel classes are used to construct two disjoint representation spaces. Through\nspatial mutual information, we design a self-cooperation learning to encourage\nmodel learning from the two feature representation spaces from itself.\nExtensive experiments on six datasets demonstrate that our method can achieve\nsignificant performance improvements, achieving state-of-the-art performance.\n", "link": "http://arxiv.org/abs/2407.01930v2", "date": "2024-07-03", "relevancy": 2.3904, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4997}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4686}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.466}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Cooperation%20Knowledge%20Distillation%20for%20Novel%20Class%20Discovery&body=Title%3A%20Self-Cooperation%20Knowledge%20Distillation%20for%20Novel%20Class%20Discovery%0AAuthor%3A%20Yuzheng%20Wang%20and%20Zhaoyu%20Chen%20and%20Dingkang%20Yang%20and%20Yunquan%20Sun%20and%20Lizhe%20Qi%0AAbstract%3A%20%20%20Novel%20Class%20Discovery%20%28NCD%29%20aims%20to%20discover%20unknown%20and%20novel%20classes%20in%20an%0Aunlabeled%20set%20by%20leveraging%20knowledge%20already%20learned%20about%20known%20classes.%0AExisting%20works%20focus%20on%20instance-level%20or%20class-level%20knowledge%20representation%0Aand%20build%20a%20shared%20representation%20space%20to%20achieve%20performance%20improvements.%0AHowever%2C%20a%20long-neglected%20issue%20is%20the%20potential%20imbalanced%20number%20of%20samples%0Afrom%20known%20and%20novel%20classes%2C%20pushing%20the%20model%20towards%20dominant%20classes.%0ATherefore%2C%20these%20methods%20suffer%20from%20a%20challenging%20trade-off%20between%20reviewing%0Aknown%20classes%20and%20discovering%20novel%20classes.%20Based%20on%20this%20observation%2C%20we%0Apropose%20a%20Self-Cooperation%20Knowledge%20Distillation%20%28SCKD%29%20method%20to%20utilize%20each%0Atraining%20sample%20%28whether%20known%20or%20novel%2C%20labeled%20or%20unlabeled%29%20for%20both%20review%0Aand%20discovery.%20Specifically%2C%20the%20model%27s%20feature%20representations%20of%20known%20and%0Anovel%20classes%20are%20used%20to%20construct%20two%20disjoint%20representation%20spaces.%20Through%0Aspatial%20mutual%20information%2C%20we%20design%20a%20self-cooperation%20learning%20to%20encourage%0Amodel%20learning%20from%20the%20two%20feature%20representation%20spaces%20from%20itself.%0AExtensive%20experiments%20on%20six%20datasets%20demonstrate%20that%20our%20method%20can%20achieve%0Asignificant%20performance%20improvements%2C%20achieving%20state-of-the-art%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01930v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Cooperation%2520Knowledge%2520Distillation%2520for%2520Novel%2520Class%2520Discovery%26entry.906535625%3DYuzheng%2520Wang%2520and%2520Zhaoyu%2520Chen%2520and%2520Dingkang%2520Yang%2520and%2520Yunquan%2520Sun%2520and%2520Lizhe%2520Qi%26entry.1292438233%3D%2520%2520Novel%2520Class%2520Discovery%2520%2528NCD%2529%2520aims%2520to%2520discover%2520unknown%2520and%2520novel%2520classes%2520in%2520an%250Aunlabeled%2520set%2520by%2520leveraging%2520knowledge%2520already%2520learned%2520about%2520known%2520classes.%250AExisting%2520works%2520focus%2520on%2520instance-level%2520or%2520class-level%2520knowledge%2520representation%250Aand%2520build%2520a%2520shared%2520representation%2520space%2520to%2520achieve%2520performance%2520improvements.%250AHowever%252C%2520a%2520long-neglected%2520issue%2520is%2520the%2520potential%2520imbalanced%2520number%2520of%2520samples%250Afrom%2520known%2520and%2520novel%2520classes%252C%2520pushing%2520the%2520model%2520towards%2520dominant%2520classes.%250ATherefore%252C%2520these%2520methods%2520suffer%2520from%2520a%2520challenging%2520trade-off%2520between%2520reviewing%250Aknown%2520classes%2520and%2520discovering%2520novel%2520classes.%2520Based%2520on%2520this%2520observation%252C%2520we%250Apropose%2520a%2520Self-Cooperation%2520Knowledge%2520Distillation%2520%2528SCKD%2529%2520method%2520to%2520utilize%2520each%250Atraining%2520sample%2520%2528whether%2520known%2520or%2520novel%252C%2520labeled%2520or%2520unlabeled%2529%2520for%2520both%2520review%250Aand%2520discovery.%2520Specifically%252C%2520the%2520model%2527s%2520feature%2520representations%2520of%2520known%2520and%250Anovel%2520classes%2520are%2520used%2520to%2520construct%2520two%2520disjoint%2520representation%2520spaces.%2520Through%250Aspatial%2520mutual%2520information%252C%2520we%2520design%2520a%2520self-cooperation%2520learning%2520to%2520encourage%250Amodel%2520learning%2520from%2520the%2520two%2520feature%2520representation%2520spaces%2520from%2520itself.%250AExtensive%2520experiments%2520on%2520six%2520datasets%2520demonstrate%2520that%2520our%2520method%2520can%2520achieve%250Asignificant%2520performance%2520improvements%252C%2520achieving%2520state-of-the-art%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01930v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Cooperation%20Knowledge%20Distillation%20for%20Novel%20Class%20Discovery&entry.906535625=Yuzheng%20Wang%20and%20Zhaoyu%20Chen%20and%20Dingkang%20Yang%20and%20Yunquan%20Sun%20and%20Lizhe%20Qi&entry.1292438233=%20%20Novel%20Class%20Discovery%20%28NCD%29%20aims%20to%20discover%20unknown%20and%20novel%20classes%20in%20an%0Aunlabeled%20set%20by%20leveraging%20knowledge%20already%20learned%20about%20known%20classes.%0AExisting%20works%20focus%20on%20instance-level%20or%20class-level%20knowledge%20representation%0Aand%20build%20a%20shared%20representation%20space%20to%20achieve%20performance%20improvements.%0AHowever%2C%20a%20long-neglected%20issue%20is%20the%20potential%20imbalanced%20number%20of%20samples%0Afrom%20known%20and%20novel%20classes%2C%20pushing%20the%20model%20towards%20dominant%20classes.%0ATherefore%2C%20these%20methods%20suffer%20from%20a%20challenging%20trade-off%20between%20reviewing%0Aknown%20classes%20and%20discovering%20novel%20classes.%20Based%20on%20this%20observation%2C%20we%0Apropose%20a%20Self-Cooperation%20Knowledge%20Distillation%20%28SCKD%29%20method%20to%20utilize%20each%0Atraining%20sample%20%28whether%20known%20or%20novel%2C%20labeled%20or%20unlabeled%29%20for%20both%20review%0Aand%20discovery.%20Specifically%2C%20the%20model%27s%20feature%20representations%20of%20known%20and%0Anovel%20classes%20are%20used%20to%20construct%20two%20disjoint%20representation%20spaces.%20Through%0Aspatial%20mutual%20information%2C%20we%20design%20a%20self-cooperation%20learning%20to%20encourage%0Amodel%20learning%20from%20the%20two%20feature%20representation%20spaces%20from%20itself.%0AExtensive%20experiments%20on%20six%20datasets%20demonstrate%20that%20our%20method%20can%20achieve%0Asignificant%20performance%20improvements%2C%20achieving%20state-of-the-art%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01930v2&entry.124074799=Read"},
{"title": "Venomancer: Towards Imperceptible and Target-on-Demand Backdoor Attacks\n  in Federated Learning", "author": "Son Nguyen and Thinh Nguyen and Khoa Doan and Kok-Seng Wong", "abstract": "  Federated Learning (FL) is a distributed machine learning approach that\nmaintains data privacy by training on decentralized data sources. Similar to\ncentralized machine learning, FL is also susceptible to backdoor attacks. Most\nbackdoor attacks in FL assume a predefined target class and require control\nover a large number of clients or knowledge of benign clients' information.\nFurthermore, they are not imperceptible and are easily detected by human\ninspection due to clear artifacts left on the poison data. To overcome these\nchallenges, we propose Venomancer, an effective backdoor attack that is\nimperceptible and allows target-on-demand. Specifically, imperceptibility is\nachieved by using a visual loss function to make the poison data visually\nindistinguishable from the original data. Target-on-demand property allows the\nattacker to choose arbitrary target classes via conditional adversarial\ntraining. Additionally, experiments showed that the method is robust against\nstate-of-the-art defenses such as Norm Clipping, Weak DP, Krum, and Multi-Krum.\nThe source code is available at\nhttps://anonymous.4open.science/r/Venomancer-3426.\n", "link": "http://arxiv.org/abs/2407.03144v1", "date": "2024-07-03", "relevancy": 2.3662, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4976}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4636}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4586}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Venomancer%3A%20Towards%20Imperceptible%20and%20Target-on-Demand%20Backdoor%20Attacks%0A%20%20in%20Federated%20Learning&body=Title%3A%20Venomancer%3A%20Towards%20Imperceptible%20and%20Target-on-Demand%20Backdoor%20Attacks%0A%20%20in%20Federated%20Learning%0AAuthor%3A%20Son%20Nguyen%20and%20Thinh%20Nguyen%20and%20Khoa%20Doan%20and%20Kok-Seng%20Wong%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20is%20a%20distributed%20machine%20learning%20approach%20that%0Amaintains%20data%20privacy%20by%20training%20on%20decentralized%20data%20sources.%20Similar%20to%0Acentralized%20machine%20learning%2C%20FL%20is%20also%20susceptible%20to%20backdoor%20attacks.%20Most%0Abackdoor%20attacks%20in%20FL%20assume%20a%20predefined%20target%20class%20and%20require%20control%0Aover%20a%20large%20number%20of%20clients%20or%20knowledge%20of%20benign%20clients%27%20information.%0AFurthermore%2C%20they%20are%20not%20imperceptible%20and%20are%20easily%20detected%20by%20human%0Ainspection%20due%20to%20clear%20artifacts%20left%20on%20the%20poison%20data.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20Venomancer%2C%20an%20effective%20backdoor%20attack%20that%20is%0Aimperceptible%20and%20allows%20target-on-demand.%20Specifically%2C%20imperceptibility%20is%0Aachieved%20by%20using%20a%20visual%20loss%20function%20to%20make%20the%20poison%20data%20visually%0Aindistinguishable%20from%20the%20original%20data.%20Target-on-demand%20property%20allows%20the%0Aattacker%20to%20choose%20arbitrary%20target%20classes%20via%20conditional%20adversarial%0Atraining.%20Additionally%2C%20experiments%20showed%20that%20the%20method%20is%20robust%20against%0Astate-of-the-art%20defenses%20such%20as%20Norm%20Clipping%2C%20Weak%20DP%2C%20Krum%2C%20and%20Multi-Krum.%0AThe%20source%20code%20is%20available%20at%0Ahttps%3A//anonymous.4open.science/r/Venomancer-3426.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03144v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVenomancer%253A%2520Towards%2520Imperceptible%2520and%2520Target-on-Demand%2520Backdoor%2520Attacks%250A%2520%2520in%2520Federated%2520Learning%26entry.906535625%3DSon%2520Nguyen%2520and%2520Thinh%2520Nguyen%2520and%2520Khoa%2520Doan%2520and%2520Kok-Seng%2520Wong%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520is%2520a%2520distributed%2520machine%2520learning%2520approach%2520that%250Amaintains%2520data%2520privacy%2520by%2520training%2520on%2520decentralized%2520data%2520sources.%2520Similar%2520to%250Acentralized%2520machine%2520learning%252C%2520FL%2520is%2520also%2520susceptible%2520to%2520backdoor%2520attacks.%2520Most%250Abackdoor%2520attacks%2520in%2520FL%2520assume%2520a%2520predefined%2520target%2520class%2520and%2520require%2520control%250Aover%2520a%2520large%2520number%2520of%2520clients%2520or%2520knowledge%2520of%2520benign%2520clients%2527%2520information.%250AFurthermore%252C%2520they%2520are%2520not%2520imperceptible%2520and%2520are%2520easily%2520detected%2520by%2520human%250Ainspection%2520due%2520to%2520clear%2520artifacts%2520left%2520on%2520the%2520poison%2520data.%2520To%2520overcome%2520these%250Achallenges%252C%2520we%2520propose%2520Venomancer%252C%2520an%2520effective%2520backdoor%2520attack%2520that%2520is%250Aimperceptible%2520and%2520allows%2520target-on-demand.%2520Specifically%252C%2520imperceptibility%2520is%250Aachieved%2520by%2520using%2520a%2520visual%2520loss%2520function%2520to%2520make%2520the%2520poison%2520data%2520visually%250Aindistinguishable%2520from%2520the%2520original%2520data.%2520Target-on-demand%2520property%2520allows%2520the%250Aattacker%2520to%2520choose%2520arbitrary%2520target%2520classes%2520via%2520conditional%2520adversarial%250Atraining.%2520Additionally%252C%2520experiments%2520showed%2520that%2520the%2520method%2520is%2520robust%2520against%250Astate-of-the-art%2520defenses%2520such%2520as%2520Norm%2520Clipping%252C%2520Weak%2520DP%252C%2520Krum%252C%2520and%2520Multi-Krum.%250AThe%2520source%2520code%2520is%2520available%2520at%250Ahttps%253A//anonymous.4open.science/r/Venomancer-3426.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03144v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Venomancer%3A%20Towards%20Imperceptible%20and%20Target-on-Demand%20Backdoor%20Attacks%0A%20%20in%20Federated%20Learning&entry.906535625=Son%20Nguyen%20and%20Thinh%20Nguyen%20and%20Khoa%20Doan%20and%20Kok-Seng%20Wong&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20is%20a%20distributed%20machine%20learning%20approach%20that%0Amaintains%20data%20privacy%20by%20training%20on%20decentralized%20data%20sources.%20Similar%20to%0Acentralized%20machine%20learning%2C%20FL%20is%20also%20susceptible%20to%20backdoor%20attacks.%20Most%0Abackdoor%20attacks%20in%20FL%20assume%20a%20predefined%20target%20class%20and%20require%20control%0Aover%20a%20large%20number%20of%20clients%20or%20knowledge%20of%20benign%20clients%27%20information.%0AFurthermore%2C%20they%20are%20not%20imperceptible%20and%20are%20easily%20detected%20by%20human%0Ainspection%20due%20to%20clear%20artifacts%20left%20on%20the%20poison%20data.%20To%20overcome%20these%0Achallenges%2C%20we%20propose%20Venomancer%2C%20an%20effective%20backdoor%20attack%20that%20is%0Aimperceptible%20and%20allows%20target-on-demand.%20Specifically%2C%20imperceptibility%20is%0Aachieved%20by%20using%20a%20visual%20loss%20function%20to%20make%20the%20poison%20data%20visually%0Aindistinguishable%20from%20the%20original%20data.%20Target-on-demand%20property%20allows%20the%0Aattacker%20to%20choose%20arbitrary%20target%20classes%20via%20conditional%20adversarial%0Atraining.%20Additionally%2C%20experiments%20showed%20that%20the%20method%20is%20robust%20against%0Astate-of-the-art%20defenses%20such%20as%20Norm%20Clipping%2C%20Weak%20DP%2C%20Krum%2C%20and%20Multi-Krum.%0AThe%20source%20code%20is%20available%20at%0Ahttps%3A//anonymous.4open.science/r/Venomancer-3426.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03144v1&entry.124074799=Read"},
{"title": "Stable Heterogeneous Treatment Effect Estimation across\n  Out-of-Distribution Populations", "author": "Yuling Zhang and Anpeng Wu and Kun Kuang and Liang Du and Zixun Sun and Zhi Wang", "abstract": "  Heterogeneous treatment effect (HTE) estimation is vital for understanding\nthe change of treatment effect across individuals or subgroups. Most existing\nHTE estimation methods focus on addressing selection bias induced by imbalanced\ndistributions of confounders between treated and control units, but ignore\ndistribution shifts across populations. Thereby, their applicability has been\nlimited to the in-distribution (ID) population, which shares a similar\ndistribution with the training dataset. In real-world applications, where\npopulation distributions are subject to continuous changes, there is an urgent\nneed for stable HTE estimation across out-of-distribution (OOD) populations,\nwhich, however, remains an open problem. As pioneers in resolving this problem,\nwe propose a novel Stable Balanced Representation Learning with\nHierarchical-Attention Paradigm (SBRL-HAP) framework, which consists of 1)\nBalancing Regularizer for eliminating selection bias, 2) Independence\nRegularizer for addressing the distribution shift issue, 3)\nHierarchical-Attention Paradigm for coordination between balance and\nindependence. In this way, SBRL-HAP regresses counterfactual outcomes using ID\ndata, while ensuring the resulting HTE estimation can be successfully\ngeneralized to out-of-distribution scenarios, thereby enhancing the model's\napplicability in real-world settings. Extensive experiments conducted on\nsynthetic and real-world datasets demonstrate the effectiveness of our SBRL-HAP\nin achieving stable HTE estimation across OOD populations, with an average 10%\nreduction in the error metric PEHE and 11% decrease in the ATE bias, compared\nto the SOTA methods.\n", "link": "http://arxiv.org/abs/2407.03082v1", "date": "2024-07-03", "relevancy": 2.3585, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4882}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4735}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4534}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Stable%20Heterogeneous%20Treatment%20Effect%20Estimation%20across%0A%20%20Out-of-Distribution%20Populations&body=Title%3A%20Stable%20Heterogeneous%20Treatment%20Effect%20Estimation%20across%0A%20%20Out-of-Distribution%20Populations%0AAuthor%3A%20Yuling%20Zhang%20and%20Anpeng%20Wu%20and%20Kun%20Kuang%20and%20Liang%20Du%20and%20Zixun%20Sun%20and%20Zhi%20Wang%0AAbstract%3A%20%20%20Heterogeneous%20treatment%20effect%20%28HTE%29%20estimation%20is%20vital%20for%20understanding%0Athe%20change%20of%20treatment%20effect%20across%20individuals%20or%20subgroups.%20Most%20existing%0AHTE%20estimation%20methods%20focus%20on%20addressing%20selection%20bias%20induced%20by%20imbalanced%0Adistributions%20of%20confounders%20between%20treated%20and%20control%20units%2C%20but%20ignore%0Adistribution%20shifts%20across%20populations.%20Thereby%2C%20their%20applicability%20has%20been%0Alimited%20to%20the%20in-distribution%20%28ID%29%20population%2C%20which%20shares%20a%20similar%0Adistribution%20with%20the%20training%20dataset.%20In%20real-world%20applications%2C%20where%0Apopulation%20distributions%20are%20subject%20to%20continuous%20changes%2C%20there%20is%20an%20urgent%0Aneed%20for%20stable%20HTE%20estimation%20across%20out-of-distribution%20%28OOD%29%20populations%2C%0Awhich%2C%20however%2C%20remains%20an%20open%20problem.%20As%20pioneers%20in%20resolving%20this%20problem%2C%0Awe%20propose%20a%20novel%20Stable%20Balanced%20Representation%20Learning%20with%0AHierarchical-Attention%20Paradigm%20%28SBRL-HAP%29%20framework%2C%20which%20consists%20of%201%29%0ABalancing%20Regularizer%20for%20eliminating%20selection%20bias%2C%202%29%20Independence%0ARegularizer%20for%20addressing%20the%20distribution%20shift%20issue%2C%203%29%0AHierarchical-Attention%20Paradigm%20for%20coordination%20between%20balance%20and%0Aindependence.%20In%20this%20way%2C%20SBRL-HAP%20regresses%20counterfactual%20outcomes%20using%20ID%0Adata%2C%20while%20ensuring%20the%20resulting%20HTE%20estimation%20can%20be%20successfully%0Ageneralized%20to%20out-of-distribution%20scenarios%2C%20thereby%20enhancing%20the%20model%27s%0Aapplicability%20in%20real-world%20settings.%20Extensive%20experiments%20conducted%20on%0Asynthetic%20and%20real-world%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20SBRL-HAP%0Ain%20achieving%20stable%20HTE%20estimation%20across%20OOD%20populations%2C%20with%20an%20average%2010%25%0Areduction%20in%20the%20error%20metric%20PEHE%20and%2011%25%20decrease%20in%20the%20ATE%20bias%2C%20compared%0Ato%20the%20SOTA%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03082v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStable%2520Heterogeneous%2520Treatment%2520Effect%2520Estimation%2520across%250A%2520%2520Out-of-Distribution%2520Populations%26entry.906535625%3DYuling%2520Zhang%2520and%2520Anpeng%2520Wu%2520and%2520Kun%2520Kuang%2520and%2520Liang%2520Du%2520and%2520Zixun%2520Sun%2520and%2520Zhi%2520Wang%26entry.1292438233%3D%2520%2520Heterogeneous%2520treatment%2520effect%2520%2528HTE%2529%2520estimation%2520is%2520vital%2520for%2520understanding%250Athe%2520change%2520of%2520treatment%2520effect%2520across%2520individuals%2520or%2520subgroups.%2520Most%2520existing%250AHTE%2520estimation%2520methods%2520focus%2520on%2520addressing%2520selection%2520bias%2520induced%2520by%2520imbalanced%250Adistributions%2520of%2520confounders%2520between%2520treated%2520and%2520control%2520units%252C%2520but%2520ignore%250Adistribution%2520shifts%2520across%2520populations.%2520Thereby%252C%2520their%2520applicability%2520has%2520been%250Alimited%2520to%2520the%2520in-distribution%2520%2528ID%2529%2520population%252C%2520which%2520shares%2520a%2520similar%250Adistribution%2520with%2520the%2520training%2520dataset.%2520In%2520real-world%2520applications%252C%2520where%250Apopulation%2520distributions%2520are%2520subject%2520to%2520continuous%2520changes%252C%2520there%2520is%2520an%2520urgent%250Aneed%2520for%2520stable%2520HTE%2520estimation%2520across%2520out-of-distribution%2520%2528OOD%2529%2520populations%252C%250Awhich%252C%2520however%252C%2520remains%2520an%2520open%2520problem.%2520As%2520pioneers%2520in%2520resolving%2520this%2520problem%252C%250Awe%2520propose%2520a%2520novel%2520Stable%2520Balanced%2520Representation%2520Learning%2520with%250AHierarchical-Attention%2520Paradigm%2520%2528SBRL-HAP%2529%2520framework%252C%2520which%2520consists%2520of%25201%2529%250ABalancing%2520Regularizer%2520for%2520eliminating%2520selection%2520bias%252C%25202%2529%2520Independence%250ARegularizer%2520for%2520addressing%2520the%2520distribution%2520shift%2520issue%252C%25203%2529%250AHierarchical-Attention%2520Paradigm%2520for%2520coordination%2520between%2520balance%2520and%250Aindependence.%2520In%2520this%2520way%252C%2520SBRL-HAP%2520regresses%2520counterfactual%2520outcomes%2520using%2520ID%250Adata%252C%2520while%2520ensuring%2520the%2520resulting%2520HTE%2520estimation%2520can%2520be%2520successfully%250Ageneralized%2520to%2520out-of-distribution%2520scenarios%252C%2520thereby%2520enhancing%2520the%2520model%2527s%250Aapplicability%2520in%2520real-world%2520settings.%2520Extensive%2520experiments%2520conducted%2520on%250Asynthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520SBRL-HAP%250Ain%2520achieving%2520stable%2520HTE%2520estimation%2520across%2520OOD%2520populations%252C%2520with%2520an%2520average%252010%2525%250Areduction%2520in%2520the%2520error%2520metric%2520PEHE%2520and%252011%2525%2520decrease%2520in%2520the%2520ATE%2520bias%252C%2520compared%250Ato%2520the%2520SOTA%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03082v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stable%20Heterogeneous%20Treatment%20Effect%20Estimation%20across%0A%20%20Out-of-Distribution%20Populations&entry.906535625=Yuling%20Zhang%20and%20Anpeng%20Wu%20and%20Kun%20Kuang%20and%20Liang%20Du%20and%20Zixun%20Sun%20and%20Zhi%20Wang&entry.1292438233=%20%20Heterogeneous%20treatment%20effect%20%28HTE%29%20estimation%20is%20vital%20for%20understanding%0Athe%20change%20of%20treatment%20effect%20across%20individuals%20or%20subgroups.%20Most%20existing%0AHTE%20estimation%20methods%20focus%20on%20addressing%20selection%20bias%20induced%20by%20imbalanced%0Adistributions%20of%20confounders%20between%20treated%20and%20control%20units%2C%20but%20ignore%0Adistribution%20shifts%20across%20populations.%20Thereby%2C%20their%20applicability%20has%20been%0Alimited%20to%20the%20in-distribution%20%28ID%29%20population%2C%20which%20shares%20a%20similar%0Adistribution%20with%20the%20training%20dataset.%20In%20real-world%20applications%2C%20where%0Apopulation%20distributions%20are%20subject%20to%20continuous%20changes%2C%20there%20is%20an%20urgent%0Aneed%20for%20stable%20HTE%20estimation%20across%20out-of-distribution%20%28OOD%29%20populations%2C%0Awhich%2C%20however%2C%20remains%20an%20open%20problem.%20As%20pioneers%20in%20resolving%20this%20problem%2C%0Awe%20propose%20a%20novel%20Stable%20Balanced%20Representation%20Learning%20with%0AHierarchical-Attention%20Paradigm%20%28SBRL-HAP%29%20framework%2C%20which%20consists%20of%201%29%0ABalancing%20Regularizer%20for%20eliminating%20selection%20bias%2C%202%29%20Independence%0ARegularizer%20for%20addressing%20the%20distribution%20shift%20issue%2C%203%29%0AHierarchical-Attention%20Paradigm%20for%20coordination%20between%20balance%20and%0Aindependence.%20In%20this%20way%2C%20SBRL-HAP%20regresses%20counterfactual%20outcomes%20using%20ID%0Adata%2C%20while%20ensuring%20the%20resulting%20HTE%20estimation%20can%20be%20successfully%0Ageneralized%20to%20out-of-distribution%20scenarios%2C%20thereby%20enhancing%20the%20model%27s%0Aapplicability%20in%20real-world%20settings.%20Extensive%20experiments%20conducted%20on%0Asynthetic%20and%20real-world%20datasets%20demonstrate%20the%20effectiveness%20of%20our%20SBRL-HAP%0Ain%20achieving%20stable%20HTE%20estimation%20across%20OOD%20populations%2C%20with%20an%20average%2010%25%0Areduction%20in%20the%20error%20metric%20PEHE%20and%2011%25%20decrease%20in%20the%20ATE%20bias%2C%20compared%0Ato%20the%20SOTA%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03082v1&entry.124074799=Read"},
{"title": "LivePortrait: Efficient Portrait Animation with Stitching and\n  Retargeting Control", "author": "Jianzhu Guo and Dingyun Zhang and Xiaoqiang Liu and Zhizhou Zhong and Yuan Zhang and Pengfei Wan and Di Zhang", "abstract": "  Portrait Animation aims to synthesize a lifelike video from a single source\nimage, using it as an appearance reference, with motion (i.e., facial\nexpressions and head pose) derived from a driving video, audio, text, or\ngeneration. Instead of following mainstream diffusion-based methods, we explore\nand extend the potential of the implicit-keypoint-based framework, which\neffectively balances computational efficiency and controllability. Building\nupon this, we develop a video-driven portrait animation framework named\nLivePortrait with a focus on better generalization, controllability, and\nefficiency for practical usage. To enhance the generation quality and\ngeneralization ability, we scale up the training data to about 69 million\nhigh-quality frames, adopt a mixed image-video training strategy, upgrade the\nnetwork architecture, and design better motion transformation and optimization\nobjectives. Additionally, we discover that compact implicit keypoints can\neffectively represent a kind of blendshapes and meticulously propose a\nstitching and two retargeting modules, which utilize a small MLP with\nnegligible computational overhead, to enhance the controllability. Experimental\nresults demonstrate the efficacy of our framework even compared to\ndiffusion-based methods. The generation speed remarkably reaches 12.8ms on an\nRTX 4090 GPU with PyTorch. The inference code and models are available at\nhttps://github.com/KwaiVGI/LivePortrait\n", "link": "http://arxiv.org/abs/2407.03168v1", "date": "2024-07-03", "relevancy": 2.3511, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6031}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5805}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5754}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LivePortrait%3A%20Efficient%20Portrait%20Animation%20with%20Stitching%20and%0A%20%20Retargeting%20Control&body=Title%3A%20LivePortrait%3A%20Efficient%20Portrait%20Animation%20with%20Stitching%20and%0A%20%20Retargeting%20Control%0AAuthor%3A%20Jianzhu%20Guo%20and%20Dingyun%20Zhang%20and%20Xiaoqiang%20Liu%20and%20Zhizhou%20Zhong%20and%20Yuan%20Zhang%20and%20Pengfei%20Wan%20and%20Di%20Zhang%0AAbstract%3A%20%20%20Portrait%20Animation%20aims%20to%20synthesize%20a%20lifelike%20video%20from%20a%20single%20source%0Aimage%2C%20using%20it%20as%20an%20appearance%20reference%2C%20with%20motion%20%28i.e.%2C%20facial%0Aexpressions%20and%20head%20pose%29%20derived%20from%20a%20driving%20video%2C%20audio%2C%20text%2C%20or%0Ageneration.%20Instead%20of%20following%20mainstream%20diffusion-based%20methods%2C%20we%20explore%0Aand%20extend%20the%20potential%20of%20the%20implicit-keypoint-based%20framework%2C%20which%0Aeffectively%20balances%20computational%20efficiency%20and%20controllability.%20Building%0Aupon%20this%2C%20we%20develop%20a%20video-driven%20portrait%20animation%20framework%20named%0ALivePortrait%20with%20a%20focus%20on%20better%20generalization%2C%20controllability%2C%20and%0Aefficiency%20for%20practical%20usage.%20To%20enhance%20the%20generation%20quality%20and%0Ageneralization%20ability%2C%20we%20scale%20up%20the%20training%20data%20to%20about%2069%20million%0Ahigh-quality%20frames%2C%20adopt%20a%20mixed%20image-video%20training%20strategy%2C%20upgrade%20the%0Anetwork%20architecture%2C%20and%20design%20better%20motion%20transformation%20and%20optimization%0Aobjectives.%20Additionally%2C%20we%20discover%20that%20compact%20implicit%20keypoints%20can%0Aeffectively%20represent%20a%20kind%20of%20blendshapes%20and%20meticulously%20propose%20a%0Astitching%20and%20two%20retargeting%20modules%2C%20which%20utilize%20a%20small%20MLP%20with%0Anegligible%20computational%20overhead%2C%20to%20enhance%20the%20controllability.%20Experimental%0Aresults%20demonstrate%20the%20efficacy%20of%20our%20framework%20even%20compared%20to%0Adiffusion-based%20methods.%20The%20generation%20speed%20remarkably%20reaches%2012.8ms%20on%20an%0ARTX%204090%20GPU%20with%20PyTorch.%20The%20inference%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/KwaiVGI/LivePortrait%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03168v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLivePortrait%253A%2520Efficient%2520Portrait%2520Animation%2520with%2520Stitching%2520and%250A%2520%2520Retargeting%2520Control%26entry.906535625%3DJianzhu%2520Guo%2520and%2520Dingyun%2520Zhang%2520and%2520Xiaoqiang%2520Liu%2520and%2520Zhizhou%2520Zhong%2520and%2520Yuan%2520Zhang%2520and%2520Pengfei%2520Wan%2520and%2520Di%2520Zhang%26entry.1292438233%3D%2520%2520Portrait%2520Animation%2520aims%2520to%2520synthesize%2520a%2520lifelike%2520video%2520from%2520a%2520single%2520source%250Aimage%252C%2520using%2520it%2520as%2520an%2520appearance%2520reference%252C%2520with%2520motion%2520%2528i.e.%252C%2520facial%250Aexpressions%2520and%2520head%2520pose%2529%2520derived%2520from%2520a%2520driving%2520video%252C%2520audio%252C%2520text%252C%2520or%250Ageneration.%2520Instead%2520of%2520following%2520mainstream%2520diffusion-based%2520methods%252C%2520we%2520explore%250Aand%2520extend%2520the%2520potential%2520of%2520the%2520implicit-keypoint-based%2520framework%252C%2520which%250Aeffectively%2520balances%2520computational%2520efficiency%2520and%2520controllability.%2520Building%250Aupon%2520this%252C%2520we%2520develop%2520a%2520video-driven%2520portrait%2520animation%2520framework%2520named%250ALivePortrait%2520with%2520a%2520focus%2520on%2520better%2520generalization%252C%2520controllability%252C%2520and%250Aefficiency%2520for%2520practical%2520usage.%2520To%2520enhance%2520the%2520generation%2520quality%2520and%250Ageneralization%2520ability%252C%2520we%2520scale%2520up%2520the%2520training%2520data%2520to%2520about%252069%2520million%250Ahigh-quality%2520frames%252C%2520adopt%2520a%2520mixed%2520image-video%2520training%2520strategy%252C%2520upgrade%2520the%250Anetwork%2520architecture%252C%2520and%2520design%2520better%2520motion%2520transformation%2520and%2520optimization%250Aobjectives.%2520Additionally%252C%2520we%2520discover%2520that%2520compact%2520implicit%2520keypoints%2520can%250Aeffectively%2520represent%2520a%2520kind%2520of%2520blendshapes%2520and%2520meticulously%2520propose%2520a%250Astitching%2520and%2520two%2520retargeting%2520modules%252C%2520which%2520utilize%2520a%2520small%2520MLP%2520with%250Anegligible%2520computational%2520overhead%252C%2520to%2520enhance%2520the%2520controllability.%2520Experimental%250Aresults%2520demonstrate%2520the%2520efficacy%2520of%2520our%2520framework%2520even%2520compared%2520to%250Adiffusion-based%2520methods.%2520The%2520generation%2520speed%2520remarkably%2520reaches%252012.8ms%2520on%2520an%250ARTX%25204090%2520GPU%2520with%2520PyTorch.%2520The%2520inference%2520code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/KwaiVGI/LivePortrait%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03168v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LivePortrait%3A%20Efficient%20Portrait%20Animation%20with%20Stitching%20and%0A%20%20Retargeting%20Control&entry.906535625=Jianzhu%20Guo%20and%20Dingyun%20Zhang%20and%20Xiaoqiang%20Liu%20and%20Zhizhou%20Zhong%20and%20Yuan%20Zhang%20and%20Pengfei%20Wan%20and%20Di%20Zhang&entry.1292438233=%20%20Portrait%20Animation%20aims%20to%20synthesize%20a%20lifelike%20video%20from%20a%20single%20source%0Aimage%2C%20using%20it%20as%20an%20appearance%20reference%2C%20with%20motion%20%28i.e.%2C%20facial%0Aexpressions%20and%20head%20pose%29%20derived%20from%20a%20driving%20video%2C%20audio%2C%20text%2C%20or%0Ageneration.%20Instead%20of%20following%20mainstream%20diffusion-based%20methods%2C%20we%20explore%0Aand%20extend%20the%20potential%20of%20the%20implicit-keypoint-based%20framework%2C%20which%0Aeffectively%20balances%20computational%20efficiency%20and%20controllability.%20Building%0Aupon%20this%2C%20we%20develop%20a%20video-driven%20portrait%20animation%20framework%20named%0ALivePortrait%20with%20a%20focus%20on%20better%20generalization%2C%20controllability%2C%20and%0Aefficiency%20for%20practical%20usage.%20To%20enhance%20the%20generation%20quality%20and%0Ageneralization%20ability%2C%20we%20scale%20up%20the%20training%20data%20to%20about%2069%20million%0Ahigh-quality%20frames%2C%20adopt%20a%20mixed%20image-video%20training%20strategy%2C%20upgrade%20the%0Anetwork%20architecture%2C%20and%20design%20better%20motion%20transformation%20and%20optimization%0Aobjectives.%20Additionally%2C%20we%20discover%20that%20compact%20implicit%20keypoints%20can%0Aeffectively%20represent%20a%20kind%20of%20blendshapes%20and%20meticulously%20propose%20a%0Astitching%20and%20two%20retargeting%20modules%2C%20which%20utilize%20a%20small%20MLP%20with%0Anegligible%20computational%20overhead%2C%20to%20enhance%20the%20controllability.%20Experimental%0Aresults%20demonstrate%20the%20efficacy%20of%20our%20framework%20even%20compared%20to%0Adiffusion-based%20methods.%20The%20generation%20speed%20remarkably%20reaches%2012.8ms%20on%20an%0ARTX%204090%20GPU%20with%20PyTorch.%20The%20inference%20code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/KwaiVGI/LivePortrait%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03168v1&entry.124074799=Read"},
{"title": "Deconvolving Complex Neuronal Networks into Interpretable Task-Specific\n  Connectomes", "author": "Yifan Wang and Vikram Ravindra and Ananth Grama", "abstract": "  Task-specific functional MRI (fMRI) images provide excellent modalities for\nstudying the neuronal basis of cognitive processes. We use fMRI data to\nformulate and solve the problem of deconvolving task-specific aggregate\nneuronal networks into a set of basic building blocks called canonical\nnetworks, to use these networks for functional characterization, and to\ncharacterize the physiological basis of these responses by mapping them to\nregions of the brain. Our results show excellent task-specificity of canonical\nnetworks, i.e., the expression of a small number of canonical networks can be\nused to accurately predict tasks; generalizability across cohorts, i.e.,\ncanonical networks are conserved across diverse populations, studies, and\nacquisition protocols; and that canonical networks have strong anatomical and\nphysiological basis. From a methods perspective, the problem of identifying\nthese canonical networks poses challenges rooted in the high dimensionality,\nsmall sample size, acquisition variability, and noise. Our deconvolution\ntechnique is based on non-negative matrix factorization (NMF) that identifies\ncanonical networks as factors of a suitably constructed matrix. We demonstrate\nthat our method scales to large datasets, yields stable and accurate factors,\nand is robust to noise.\n", "link": "http://arxiv.org/abs/2407.00201v2", "date": "2024-07-03", "relevancy": 2.345, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4694}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4694}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4682}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deconvolving%20Complex%20Neuronal%20Networks%20into%20Interpretable%20Task-Specific%0A%20%20Connectomes&body=Title%3A%20Deconvolving%20Complex%20Neuronal%20Networks%20into%20Interpretable%20Task-Specific%0A%20%20Connectomes%0AAuthor%3A%20Yifan%20Wang%20and%20Vikram%20Ravindra%20and%20Ananth%20Grama%0AAbstract%3A%20%20%20Task-specific%20functional%20MRI%20%28fMRI%29%20images%20provide%20excellent%20modalities%20for%0Astudying%20the%20neuronal%20basis%20of%20cognitive%20processes.%20We%20use%20fMRI%20data%20to%0Aformulate%20and%20solve%20the%20problem%20of%20deconvolving%20task-specific%20aggregate%0Aneuronal%20networks%20into%20a%20set%20of%20basic%20building%20blocks%20called%20canonical%0Anetworks%2C%20to%20use%20these%20networks%20for%20functional%20characterization%2C%20and%20to%0Acharacterize%20the%20physiological%20basis%20of%20these%20responses%20by%20mapping%20them%20to%0Aregions%20of%20the%20brain.%20Our%20results%20show%20excellent%20task-specificity%20of%20canonical%0Anetworks%2C%20i.e.%2C%20the%20expression%20of%20a%20small%20number%20of%20canonical%20networks%20can%20be%0Aused%20to%20accurately%20predict%20tasks%3B%20generalizability%20across%20cohorts%2C%20i.e.%2C%0Acanonical%20networks%20are%20conserved%20across%20diverse%20populations%2C%20studies%2C%20and%0Aacquisition%20protocols%3B%20and%20that%20canonical%20networks%20have%20strong%20anatomical%20and%0Aphysiological%20basis.%20From%20a%20methods%20perspective%2C%20the%20problem%20of%20identifying%0Athese%20canonical%20networks%20poses%20challenges%20rooted%20in%20the%20high%20dimensionality%2C%0Asmall%20sample%20size%2C%20acquisition%20variability%2C%20and%20noise.%20Our%20deconvolution%0Atechnique%20is%20based%20on%20non-negative%20matrix%20factorization%20%28NMF%29%20that%20identifies%0Acanonical%20networks%20as%20factors%20of%20a%20suitably%20constructed%20matrix.%20We%20demonstrate%0Athat%20our%20method%20scales%20to%20large%20datasets%2C%20yields%20stable%20and%20accurate%20factors%2C%0Aand%20is%20robust%20to%20noise.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.00201v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeconvolving%2520Complex%2520Neuronal%2520Networks%2520into%2520Interpretable%2520Task-Specific%250A%2520%2520Connectomes%26entry.906535625%3DYifan%2520Wang%2520and%2520Vikram%2520Ravindra%2520and%2520Ananth%2520Grama%26entry.1292438233%3D%2520%2520Task-specific%2520functional%2520MRI%2520%2528fMRI%2529%2520images%2520provide%2520excellent%2520modalities%2520for%250Astudying%2520the%2520neuronal%2520basis%2520of%2520cognitive%2520processes.%2520We%2520use%2520fMRI%2520data%2520to%250Aformulate%2520and%2520solve%2520the%2520problem%2520of%2520deconvolving%2520task-specific%2520aggregate%250Aneuronal%2520networks%2520into%2520a%2520set%2520of%2520basic%2520building%2520blocks%2520called%2520canonical%250Anetworks%252C%2520to%2520use%2520these%2520networks%2520for%2520functional%2520characterization%252C%2520and%2520to%250Acharacterize%2520the%2520physiological%2520basis%2520of%2520these%2520responses%2520by%2520mapping%2520them%2520to%250Aregions%2520of%2520the%2520brain.%2520Our%2520results%2520show%2520excellent%2520task-specificity%2520of%2520canonical%250Anetworks%252C%2520i.e.%252C%2520the%2520expression%2520of%2520a%2520small%2520number%2520of%2520canonical%2520networks%2520can%2520be%250Aused%2520to%2520accurately%2520predict%2520tasks%253B%2520generalizability%2520across%2520cohorts%252C%2520i.e.%252C%250Acanonical%2520networks%2520are%2520conserved%2520across%2520diverse%2520populations%252C%2520studies%252C%2520and%250Aacquisition%2520protocols%253B%2520and%2520that%2520canonical%2520networks%2520have%2520strong%2520anatomical%2520and%250Aphysiological%2520basis.%2520From%2520a%2520methods%2520perspective%252C%2520the%2520problem%2520of%2520identifying%250Athese%2520canonical%2520networks%2520poses%2520challenges%2520rooted%2520in%2520the%2520high%2520dimensionality%252C%250Asmall%2520sample%2520size%252C%2520acquisition%2520variability%252C%2520and%2520noise.%2520Our%2520deconvolution%250Atechnique%2520is%2520based%2520on%2520non-negative%2520matrix%2520factorization%2520%2528NMF%2529%2520that%2520identifies%250Acanonical%2520networks%2520as%2520factors%2520of%2520a%2520suitably%2520constructed%2520matrix.%2520We%2520demonstrate%250Athat%2520our%2520method%2520scales%2520to%2520large%2520datasets%252C%2520yields%2520stable%2520and%2520accurate%2520factors%252C%250Aand%2520is%2520robust%2520to%2520noise.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.00201v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deconvolving%20Complex%20Neuronal%20Networks%20into%20Interpretable%20Task-Specific%0A%20%20Connectomes&entry.906535625=Yifan%20Wang%20and%20Vikram%20Ravindra%20and%20Ananth%20Grama&entry.1292438233=%20%20Task-specific%20functional%20MRI%20%28fMRI%29%20images%20provide%20excellent%20modalities%20for%0Astudying%20the%20neuronal%20basis%20of%20cognitive%20processes.%20We%20use%20fMRI%20data%20to%0Aformulate%20and%20solve%20the%20problem%20of%20deconvolving%20task-specific%20aggregate%0Aneuronal%20networks%20into%20a%20set%20of%20basic%20building%20blocks%20called%20canonical%0Anetworks%2C%20to%20use%20these%20networks%20for%20functional%20characterization%2C%20and%20to%0Acharacterize%20the%20physiological%20basis%20of%20these%20responses%20by%20mapping%20them%20to%0Aregions%20of%20the%20brain.%20Our%20results%20show%20excellent%20task-specificity%20of%20canonical%0Anetworks%2C%20i.e.%2C%20the%20expression%20of%20a%20small%20number%20of%20canonical%20networks%20can%20be%0Aused%20to%20accurately%20predict%20tasks%3B%20generalizability%20across%20cohorts%2C%20i.e.%2C%0Acanonical%20networks%20are%20conserved%20across%20diverse%20populations%2C%20studies%2C%20and%0Aacquisition%20protocols%3B%20and%20that%20canonical%20networks%20have%20strong%20anatomical%20and%0Aphysiological%20basis.%20From%20a%20methods%20perspective%2C%20the%20problem%20of%20identifying%0Athese%20canonical%20networks%20poses%20challenges%20rooted%20in%20the%20high%20dimensionality%2C%0Asmall%20sample%20size%2C%20acquisition%20variability%2C%20and%20noise.%20Our%20deconvolution%0Atechnique%20is%20based%20on%20non-negative%20matrix%20factorization%20%28NMF%29%20that%20identifies%0Acanonical%20networks%20as%20factors%20of%20a%20suitably%20constructed%20matrix.%20We%20demonstrate%0Athat%20our%20method%20scales%20to%20large%20datasets%2C%20yields%20stable%20and%20accurate%20factors%2C%0Aand%20is%20robust%20to%20noise.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.00201v2&entry.124074799=Read"},
{"title": "IntentionNet: Map-Lite Visual Navigation at the Kilometre Scale", "author": "Wei Gao and Bo Ai and Joel Loo and  Vinay and David Hsu", "abstract": "  This work explores the challenges of creating a scalable and robust robot\nnavigation system that can traverse both indoor and outdoor environments to\nreach distant goals. We propose a navigation system architecture called\nIntentionNet that employs a monolithic neural network as the low-level\nplanner/controller, and uses a general interface that we call intentions to\nsteer the controller. The paper proposes two types of intentions, Local Path\nand Environment (LPE) and Discretised Local Move (DLM), and shows that DLM is\nrobust to significant metric positioning and mapping errors. The paper also\npresents Kilo-IntentionNet, an instance of the IntentionNet system using the\nDLM intention that is deployed on a Boston Dynamics Spot robot, and which\nsuccessfully navigates through complex indoor and outdoor environments over\ndistances of up to a kilometre with only noisy odometry.\n", "link": "http://arxiv.org/abs/2407.03122v1", "date": "2024-07-03", "relevancy": 2.3372, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5912}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5895}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IntentionNet%3A%20Map-Lite%20Visual%20Navigation%20at%20the%20Kilometre%20Scale&body=Title%3A%20IntentionNet%3A%20Map-Lite%20Visual%20Navigation%20at%20the%20Kilometre%20Scale%0AAuthor%3A%20Wei%20Gao%20and%20Bo%20Ai%20and%20Joel%20Loo%20and%20%20Vinay%20and%20David%20Hsu%0AAbstract%3A%20%20%20This%20work%20explores%20the%20challenges%20of%20creating%20a%20scalable%20and%20robust%20robot%0Anavigation%20system%20that%20can%20traverse%20both%20indoor%20and%20outdoor%20environments%20to%0Areach%20distant%20goals.%20We%20propose%20a%20navigation%20system%20architecture%20called%0AIntentionNet%20that%20employs%20a%20monolithic%20neural%20network%20as%20the%20low-level%0Aplanner/controller%2C%20and%20uses%20a%20general%20interface%20that%20we%20call%20intentions%20to%0Asteer%20the%20controller.%20The%20paper%20proposes%20two%20types%20of%20intentions%2C%20Local%20Path%0Aand%20Environment%20%28LPE%29%20and%20Discretised%20Local%20Move%20%28DLM%29%2C%20and%20shows%20that%20DLM%20is%0Arobust%20to%20significant%20metric%20positioning%20and%20mapping%20errors.%20The%20paper%20also%0Apresents%20Kilo-IntentionNet%2C%20an%20instance%20of%20the%20IntentionNet%20system%20using%20the%0ADLM%20intention%20that%20is%20deployed%20on%20a%20Boston%20Dynamics%20Spot%20robot%2C%20and%20which%0Asuccessfully%20navigates%20through%20complex%20indoor%20and%20outdoor%20environments%20over%0Adistances%20of%20up%20to%20a%20kilometre%20with%20only%20noisy%20odometry.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03122v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIntentionNet%253A%2520Map-Lite%2520Visual%2520Navigation%2520at%2520the%2520Kilometre%2520Scale%26entry.906535625%3DWei%2520Gao%2520and%2520Bo%2520Ai%2520and%2520Joel%2520Loo%2520and%2520%2520Vinay%2520and%2520David%2520Hsu%26entry.1292438233%3D%2520%2520This%2520work%2520explores%2520the%2520challenges%2520of%2520creating%2520a%2520scalable%2520and%2520robust%2520robot%250Anavigation%2520system%2520that%2520can%2520traverse%2520both%2520indoor%2520and%2520outdoor%2520environments%2520to%250Areach%2520distant%2520goals.%2520We%2520propose%2520a%2520navigation%2520system%2520architecture%2520called%250AIntentionNet%2520that%2520employs%2520a%2520monolithic%2520neural%2520network%2520as%2520the%2520low-level%250Aplanner/controller%252C%2520and%2520uses%2520a%2520general%2520interface%2520that%2520we%2520call%2520intentions%2520to%250Asteer%2520the%2520controller.%2520The%2520paper%2520proposes%2520two%2520types%2520of%2520intentions%252C%2520Local%2520Path%250Aand%2520Environment%2520%2528LPE%2529%2520and%2520Discretised%2520Local%2520Move%2520%2528DLM%2529%252C%2520and%2520shows%2520that%2520DLM%2520is%250Arobust%2520to%2520significant%2520metric%2520positioning%2520and%2520mapping%2520errors.%2520The%2520paper%2520also%250Apresents%2520Kilo-IntentionNet%252C%2520an%2520instance%2520of%2520the%2520IntentionNet%2520system%2520using%2520the%250ADLM%2520intention%2520that%2520is%2520deployed%2520on%2520a%2520Boston%2520Dynamics%2520Spot%2520robot%252C%2520and%2520which%250Asuccessfully%2520navigates%2520through%2520complex%2520indoor%2520and%2520outdoor%2520environments%2520over%250Adistances%2520of%2520up%2520to%2520a%2520kilometre%2520with%2520only%2520noisy%2520odometry.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03122v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IntentionNet%3A%20Map-Lite%20Visual%20Navigation%20at%20the%20Kilometre%20Scale&entry.906535625=Wei%20Gao%20and%20Bo%20Ai%20and%20Joel%20Loo%20and%20%20Vinay%20and%20David%20Hsu&entry.1292438233=%20%20This%20work%20explores%20the%20challenges%20of%20creating%20a%20scalable%20and%20robust%20robot%0Anavigation%20system%20that%20can%20traverse%20both%20indoor%20and%20outdoor%20environments%20to%0Areach%20distant%20goals.%20We%20propose%20a%20navigation%20system%20architecture%20called%0AIntentionNet%20that%20employs%20a%20monolithic%20neural%20network%20as%20the%20low-level%0Aplanner/controller%2C%20and%20uses%20a%20general%20interface%20that%20we%20call%20intentions%20to%0Asteer%20the%20controller.%20The%20paper%20proposes%20two%20types%20of%20intentions%2C%20Local%20Path%0Aand%20Environment%20%28LPE%29%20and%20Discretised%20Local%20Move%20%28DLM%29%2C%20and%20shows%20that%20DLM%20is%0Arobust%20to%20significant%20metric%20positioning%20and%20mapping%20errors.%20The%20paper%20also%0Apresents%20Kilo-IntentionNet%2C%20an%20instance%20of%20the%20IntentionNet%20system%20using%20the%0ADLM%20intention%20that%20is%20deployed%20on%20a%20Boston%20Dynamics%20Spot%20robot%2C%20and%20which%0Asuccessfully%20navigates%20through%20complex%20indoor%20and%20outdoor%20environments%20over%0Adistances%20of%20up%20to%20a%20kilometre%20with%20only%20noisy%20odometry.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03122v1&entry.124074799=Read"},
{"title": "Four Ways to Improve Verbo-visual Fusion for Dense 3D Visual Grounding", "author": "Ozan Unal and Christos Sakaridis and Suman Saha and Luc Van Gool", "abstract": "  3D visual grounding is the task of localizing the object in a 3D scene which\nis referred by a description in natural language. With a wide range of\napplications ranging from autonomous indoor robotics to AR/VR, the task has\nrecently risen in popularity. A common formulation to tackle 3D visual\ngrounding is grounding-by-detection, where localization is done via bounding\nboxes. However, for real-life applications that require physical interactions,\na bounding box insufficiently describes the geometry of an object. We therefore\ntackle the problem of dense 3D visual grounding, i.e. referral-based 3D\ninstance segmentation. We propose a dense 3D grounding network ConcreteNet,\nfeaturing four novel stand-alone modules that aim to improve grounding\nperformance for challenging repetitive instances, i.e. instances with\ndistractors of the same semantic class. First, we introduce a bottom-up\nattentive fusion module that aims to disambiguate inter-instance relational\ncues, next, we construct a contrastive training scheme to induce separation in\nthe latent space, we then resolve view-dependent utterances via a learned\nglobal camera token, and finally we employ multi-view ensembling to improve\nreferred mask quality. ConcreteNet ranks 1st on the challenging ScanRefer\nonline benchmark and has won the ICCV 3rd Workshop on Language for 3D Scenes\n\"3D Object Localization\" challenge.\n", "link": "http://arxiv.org/abs/2309.04561v2", "date": "2024-07-03", "relevancy": 2.3341, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6151}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5684}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.558}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Four%20Ways%20to%20Improve%20Verbo-visual%20Fusion%20for%20Dense%203D%20Visual%20Grounding&body=Title%3A%20Four%20Ways%20to%20Improve%20Verbo-visual%20Fusion%20for%20Dense%203D%20Visual%20Grounding%0AAuthor%3A%20Ozan%20Unal%20and%20Christos%20Sakaridis%20and%20Suman%20Saha%20and%20Luc%20Van%20Gool%0AAbstract%3A%20%20%203D%20visual%20grounding%20is%20the%20task%20of%20localizing%20the%20object%20in%20a%203D%20scene%20which%0Ais%20referred%20by%20a%20description%20in%20natural%20language.%20With%20a%20wide%20range%20of%0Aapplications%20ranging%20from%20autonomous%20indoor%20robotics%20to%20AR/VR%2C%20the%20task%20has%0Arecently%20risen%20in%20popularity.%20A%20common%20formulation%20to%20tackle%203D%20visual%0Agrounding%20is%20grounding-by-detection%2C%20where%20localization%20is%20done%20via%20bounding%0Aboxes.%20However%2C%20for%20real-life%20applications%20that%20require%20physical%20interactions%2C%0Aa%20bounding%20box%20insufficiently%20describes%20the%20geometry%20of%20an%20object.%20We%20therefore%0Atackle%20the%20problem%20of%20dense%203D%20visual%20grounding%2C%20i.e.%20referral-based%203D%0Ainstance%20segmentation.%20We%20propose%20a%20dense%203D%20grounding%20network%20ConcreteNet%2C%0Afeaturing%20four%20novel%20stand-alone%20modules%20that%20aim%20to%20improve%20grounding%0Aperformance%20for%20challenging%20repetitive%20instances%2C%20i.e.%20instances%20with%0Adistractors%20of%20the%20same%20semantic%20class.%20First%2C%20we%20introduce%20a%20bottom-up%0Aattentive%20fusion%20module%20that%20aims%20to%20disambiguate%20inter-instance%20relational%0Acues%2C%20next%2C%20we%20construct%20a%20contrastive%20training%20scheme%20to%20induce%20separation%20in%0Athe%20latent%20space%2C%20we%20then%20resolve%20view-dependent%20utterances%20via%20a%20learned%0Aglobal%20camera%20token%2C%20and%20finally%20we%20employ%20multi-view%20ensembling%20to%20improve%0Areferred%20mask%20quality.%20ConcreteNet%20ranks%201st%20on%20the%20challenging%20ScanRefer%0Aonline%20benchmark%20and%20has%20won%20the%20ICCV%203rd%20Workshop%20on%20Language%20for%203D%20Scenes%0A%223D%20Object%20Localization%22%20challenge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.04561v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFour%2520Ways%2520to%2520Improve%2520Verbo-visual%2520Fusion%2520for%2520Dense%25203D%2520Visual%2520Grounding%26entry.906535625%3DOzan%2520Unal%2520and%2520Christos%2520Sakaridis%2520and%2520Suman%2520Saha%2520and%2520Luc%2520Van%2520Gool%26entry.1292438233%3D%2520%25203D%2520visual%2520grounding%2520is%2520the%2520task%2520of%2520localizing%2520the%2520object%2520in%2520a%25203D%2520scene%2520which%250Ais%2520referred%2520by%2520a%2520description%2520in%2520natural%2520language.%2520With%2520a%2520wide%2520range%2520of%250Aapplications%2520ranging%2520from%2520autonomous%2520indoor%2520robotics%2520to%2520AR/VR%252C%2520the%2520task%2520has%250Arecently%2520risen%2520in%2520popularity.%2520A%2520common%2520formulation%2520to%2520tackle%25203D%2520visual%250Agrounding%2520is%2520grounding-by-detection%252C%2520where%2520localization%2520is%2520done%2520via%2520bounding%250Aboxes.%2520However%252C%2520for%2520real-life%2520applications%2520that%2520require%2520physical%2520interactions%252C%250Aa%2520bounding%2520box%2520insufficiently%2520describes%2520the%2520geometry%2520of%2520an%2520object.%2520We%2520therefore%250Atackle%2520the%2520problem%2520of%2520dense%25203D%2520visual%2520grounding%252C%2520i.e.%2520referral-based%25203D%250Ainstance%2520segmentation.%2520We%2520propose%2520a%2520dense%25203D%2520grounding%2520network%2520ConcreteNet%252C%250Afeaturing%2520four%2520novel%2520stand-alone%2520modules%2520that%2520aim%2520to%2520improve%2520grounding%250Aperformance%2520for%2520challenging%2520repetitive%2520instances%252C%2520i.e.%2520instances%2520with%250Adistractors%2520of%2520the%2520same%2520semantic%2520class.%2520First%252C%2520we%2520introduce%2520a%2520bottom-up%250Aattentive%2520fusion%2520module%2520that%2520aims%2520to%2520disambiguate%2520inter-instance%2520relational%250Acues%252C%2520next%252C%2520we%2520construct%2520a%2520contrastive%2520training%2520scheme%2520to%2520induce%2520separation%2520in%250Athe%2520latent%2520space%252C%2520we%2520then%2520resolve%2520view-dependent%2520utterances%2520via%2520a%2520learned%250Aglobal%2520camera%2520token%252C%2520and%2520finally%2520we%2520employ%2520multi-view%2520ensembling%2520to%2520improve%250Areferred%2520mask%2520quality.%2520ConcreteNet%2520ranks%25201st%2520on%2520the%2520challenging%2520ScanRefer%250Aonline%2520benchmark%2520and%2520has%2520won%2520the%2520ICCV%25203rd%2520Workshop%2520on%2520Language%2520for%25203D%2520Scenes%250A%25223D%2520Object%2520Localization%2522%2520challenge.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.04561v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Four%20Ways%20to%20Improve%20Verbo-visual%20Fusion%20for%20Dense%203D%20Visual%20Grounding&entry.906535625=Ozan%20Unal%20and%20Christos%20Sakaridis%20and%20Suman%20Saha%20and%20Luc%20Van%20Gool&entry.1292438233=%20%203D%20visual%20grounding%20is%20the%20task%20of%20localizing%20the%20object%20in%20a%203D%20scene%20which%0Ais%20referred%20by%20a%20description%20in%20natural%20language.%20With%20a%20wide%20range%20of%0Aapplications%20ranging%20from%20autonomous%20indoor%20robotics%20to%20AR/VR%2C%20the%20task%20has%0Arecently%20risen%20in%20popularity.%20A%20common%20formulation%20to%20tackle%203D%20visual%0Agrounding%20is%20grounding-by-detection%2C%20where%20localization%20is%20done%20via%20bounding%0Aboxes.%20However%2C%20for%20real-life%20applications%20that%20require%20physical%20interactions%2C%0Aa%20bounding%20box%20insufficiently%20describes%20the%20geometry%20of%20an%20object.%20We%20therefore%0Atackle%20the%20problem%20of%20dense%203D%20visual%20grounding%2C%20i.e.%20referral-based%203D%0Ainstance%20segmentation.%20We%20propose%20a%20dense%203D%20grounding%20network%20ConcreteNet%2C%0Afeaturing%20four%20novel%20stand-alone%20modules%20that%20aim%20to%20improve%20grounding%0Aperformance%20for%20challenging%20repetitive%20instances%2C%20i.e.%20instances%20with%0Adistractors%20of%20the%20same%20semantic%20class.%20First%2C%20we%20introduce%20a%20bottom-up%0Aattentive%20fusion%20module%20that%20aims%20to%20disambiguate%20inter-instance%20relational%0Acues%2C%20next%2C%20we%20construct%20a%20contrastive%20training%20scheme%20to%20induce%20separation%20in%0Athe%20latent%20space%2C%20we%20then%20resolve%20view-dependent%20utterances%20via%20a%20learned%0Aglobal%20camera%20token%2C%20and%20finally%20we%20employ%20multi-view%20ensembling%20to%20improve%0Areferred%20mask%20quality.%20ConcreteNet%20ranks%201st%20on%20the%20challenging%20ScanRefer%0Aonline%20benchmark%20and%20has%20won%20the%20ICCV%203rd%20Workshop%20on%20Language%20for%203D%20Scenes%0A%223D%20Object%20Localization%22%20challenge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.04561v2&entry.124074799=Read"},
{"title": "BeNeRF: Neural Radiance Fields from a Single Blurry Image and Event\n  Stream", "author": "Wenpu Li and Pian Wan and Peng Wang and Jinghang Li and Yi Zhou and Peidong Liu", "abstract": "  Neural implicit representation of visual scenes has attracted a lot of\nattention in recent research of computer vision and graphics. Most prior\nmethods focus on how to reconstruct 3D scene representation from a set of\nimages. In this work, we demonstrate the possibility to recover the neural\nradiance fields (NeRF) from a single blurry image and its corresponding event\nstream. We model the camera motion with a cubic B-Spline in SE(3) space. Both\nthe blurry image and the brightness change within a time interval, can then be\nsynthesized from the 3D scene representation given the 6-DoF poses interpolated\nfrom the cubic B-Spline. Our method can jointly learn both the implicit neural\nscene representation and recover the camera motion by minimizing the\ndifferences between the synthesized data and the real measurements without\npre-computed camera poses from COLMAP. We evaluate the proposed method with\nboth synthetic and real datasets. The experimental results demonstrate that we\nare able to render view-consistent latent sharp images from the learned NeRF\nand bring a blurry image alive in high quality. Code and data are available at\nhttps://github.com/WU-CVGL/BeNeRF.\n", "link": "http://arxiv.org/abs/2407.02174v2", "date": "2024-07-03", "relevancy": 2.3252, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5839}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5813}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BeNeRF%3A%20Neural%20Radiance%20Fields%20from%20a%20Single%20Blurry%20Image%20and%20Event%0A%20%20Stream&body=Title%3A%20BeNeRF%3A%20Neural%20Radiance%20Fields%20from%20a%20Single%20Blurry%20Image%20and%20Event%0A%20%20Stream%0AAuthor%3A%20Wenpu%20Li%20and%20Pian%20Wan%20and%20Peng%20Wang%20and%20Jinghang%20Li%20and%20Yi%20Zhou%20and%20Peidong%20Liu%0AAbstract%3A%20%20%20Neural%20implicit%20representation%20of%20visual%20scenes%20has%20attracted%20a%20lot%20of%0Aattention%20in%20recent%20research%20of%20computer%20vision%20and%20graphics.%20Most%20prior%0Amethods%20focus%20on%20how%20to%20reconstruct%203D%20scene%20representation%20from%20a%20set%20of%0Aimages.%20In%20this%20work%2C%20we%20demonstrate%20the%20possibility%20to%20recover%20the%20neural%0Aradiance%20fields%20%28NeRF%29%20from%20a%20single%20blurry%20image%20and%20its%20corresponding%20event%0Astream.%20We%20model%20the%20camera%20motion%20with%20a%20cubic%20B-Spline%20in%20SE%283%29%20space.%20Both%0Athe%20blurry%20image%20and%20the%20brightness%20change%20within%20a%20time%20interval%2C%20can%20then%20be%0Asynthesized%20from%20the%203D%20scene%20representation%20given%20the%206-DoF%20poses%20interpolated%0Afrom%20the%20cubic%20B-Spline.%20Our%20method%20can%20jointly%20learn%20both%20the%20implicit%20neural%0Ascene%20representation%20and%20recover%20the%20camera%20motion%20by%20minimizing%20the%0Adifferences%20between%20the%20synthesized%20data%20and%20the%20real%20measurements%20without%0Apre-computed%20camera%20poses%20from%20COLMAP.%20We%20evaluate%20the%20proposed%20method%20with%0Aboth%20synthetic%20and%20real%20datasets.%20The%20experimental%20results%20demonstrate%20that%20we%0Aare%20able%20to%20render%20view-consistent%20latent%20sharp%20images%20from%20the%20learned%20NeRF%0Aand%20bring%20a%20blurry%20image%20alive%20in%20high%20quality.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/WU-CVGL/BeNeRF.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02174v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBeNeRF%253A%2520Neural%2520Radiance%2520Fields%2520from%2520a%2520Single%2520Blurry%2520Image%2520and%2520Event%250A%2520%2520Stream%26entry.906535625%3DWenpu%2520Li%2520and%2520Pian%2520Wan%2520and%2520Peng%2520Wang%2520and%2520Jinghang%2520Li%2520and%2520Yi%2520Zhou%2520and%2520Peidong%2520Liu%26entry.1292438233%3D%2520%2520Neural%2520implicit%2520representation%2520of%2520visual%2520scenes%2520has%2520attracted%2520a%2520lot%2520of%250Aattention%2520in%2520recent%2520research%2520of%2520computer%2520vision%2520and%2520graphics.%2520Most%2520prior%250Amethods%2520focus%2520on%2520how%2520to%2520reconstruct%25203D%2520scene%2520representation%2520from%2520a%2520set%2520of%250Aimages.%2520In%2520this%2520work%252C%2520we%2520demonstrate%2520the%2520possibility%2520to%2520recover%2520the%2520neural%250Aradiance%2520fields%2520%2528NeRF%2529%2520from%2520a%2520single%2520blurry%2520image%2520and%2520its%2520corresponding%2520event%250Astream.%2520We%2520model%2520the%2520camera%2520motion%2520with%2520a%2520cubic%2520B-Spline%2520in%2520SE%25283%2529%2520space.%2520Both%250Athe%2520blurry%2520image%2520and%2520the%2520brightness%2520change%2520within%2520a%2520time%2520interval%252C%2520can%2520then%2520be%250Asynthesized%2520from%2520the%25203D%2520scene%2520representation%2520given%2520the%25206-DoF%2520poses%2520interpolated%250Afrom%2520the%2520cubic%2520B-Spline.%2520Our%2520method%2520can%2520jointly%2520learn%2520both%2520the%2520implicit%2520neural%250Ascene%2520representation%2520and%2520recover%2520the%2520camera%2520motion%2520by%2520minimizing%2520the%250Adifferences%2520between%2520the%2520synthesized%2520data%2520and%2520the%2520real%2520measurements%2520without%250Apre-computed%2520camera%2520poses%2520from%2520COLMAP.%2520We%2520evaluate%2520the%2520proposed%2520method%2520with%250Aboth%2520synthetic%2520and%2520real%2520datasets.%2520The%2520experimental%2520results%2520demonstrate%2520that%2520we%250Aare%2520able%2520to%2520render%2520view-consistent%2520latent%2520sharp%2520images%2520from%2520the%2520learned%2520NeRF%250Aand%2520bring%2520a%2520blurry%2520image%2520alive%2520in%2520high%2520quality.%2520Code%2520and%2520data%2520are%2520available%2520at%250Ahttps%253A//github.com/WU-CVGL/BeNeRF.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02174v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BeNeRF%3A%20Neural%20Radiance%20Fields%20from%20a%20Single%20Blurry%20Image%20and%20Event%0A%20%20Stream&entry.906535625=Wenpu%20Li%20and%20Pian%20Wan%20and%20Peng%20Wang%20and%20Jinghang%20Li%20and%20Yi%20Zhou%20and%20Peidong%20Liu&entry.1292438233=%20%20Neural%20implicit%20representation%20of%20visual%20scenes%20has%20attracted%20a%20lot%20of%0Aattention%20in%20recent%20research%20of%20computer%20vision%20and%20graphics.%20Most%20prior%0Amethods%20focus%20on%20how%20to%20reconstruct%203D%20scene%20representation%20from%20a%20set%20of%0Aimages.%20In%20this%20work%2C%20we%20demonstrate%20the%20possibility%20to%20recover%20the%20neural%0Aradiance%20fields%20%28NeRF%29%20from%20a%20single%20blurry%20image%20and%20its%20corresponding%20event%0Astream.%20We%20model%20the%20camera%20motion%20with%20a%20cubic%20B-Spline%20in%20SE%283%29%20space.%20Both%0Athe%20blurry%20image%20and%20the%20brightness%20change%20within%20a%20time%20interval%2C%20can%20then%20be%0Asynthesized%20from%20the%203D%20scene%20representation%20given%20the%206-DoF%20poses%20interpolated%0Afrom%20the%20cubic%20B-Spline.%20Our%20method%20can%20jointly%20learn%20both%20the%20implicit%20neural%0Ascene%20representation%20and%20recover%20the%20camera%20motion%20by%20minimizing%20the%0Adifferences%20between%20the%20synthesized%20data%20and%20the%20real%20measurements%20without%0Apre-computed%20camera%20poses%20from%20COLMAP.%20We%20evaluate%20the%20proposed%20method%20with%0Aboth%20synthetic%20and%20real%20datasets.%20The%20experimental%20results%20demonstrate%20that%20we%0Aare%20able%20to%20render%20view-consistent%20latent%20sharp%20images%20from%20the%20learned%20NeRF%0Aand%20bring%20a%20blurry%20image%20alive%20in%20high%20quality.%20Code%20and%20data%20are%20available%20at%0Ahttps%3A//github.com/WU-CVGL/BeNeRF.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02174v2&entry.124074799=Read"},
{"title": "SparseSSP: 3D Subcellular Structure Prediction from Sparse-View\n  Transmitted Light Images", "author": "Jintu Zheng and Yi Ding and Qizhe Liu and Yi Cao and Ying Hu and Zenan Wang", "abstract": "  Traditional fluorescence staining is phototoxic to live cells, slow, and\nexpensive; thus, the subcellular structure prediction (SSP) from transmitted\nlight (TL) images is emerging as a label-free, faster, low-cost alternative.\nHowever, existing approaches utilize 3D networks for one-to-one voxel level\ndense prediction, which necessitates a frequent and time-consuming Z-axis\nimaging process. Moreover, 3D convolutions inevitably lead to significant\ncomputation and GPU memory overhead. Therefore, we propose an efficient\nframework, SparseSSP, predicting fluorescent intensities within the target\nvoxel grid in an efficient paradigm instead of relying entirely on 3D\ntopologies. In particular, SparseSSP makes two pivotal improvements to prior\nworks. First, SparseSSP introduces a one-to-many voxel mapping paradigm, which\npermits the sparse TL slices to reconstruct the subcellular structure.\nSecondly, we propose a hybrid dimensions topology, which folds the Z-axis\ninformation into channel features, enabling the 2D network layers to tackle SSP\nunder low computational cost. We conduct extensive experiments to validate the\neffectiveness and advantages of SparseSSP on diverse sparse imaging ratios, and\nour approach achieves a leading performance compared to pure 3D topologies.\nSparseSSP reduces imaging frequencies compared to previous dense-view SSP\n(i.e., the number of imaging is reduced up to 87.5% at most), which is\nsignificant in visualizing rapid biological dynamics on low-cost devices and\nsamples.\n", "link": "http://arxiv.org/abs/2407.02159v2", "date": "2024-07-03", "relevancy": 2.3126, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6064}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5785}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SparseSSP%3A%203D%20Subcellular%20Structure%20Prediction%20from%20Sparse-View%0A%20%20Transmitted%20Light%20Images&body=Title%3A%20SparseSSP%3A%203D%20Subcellular%20Structure%20Prediction%20from%20Sparse-View%0A%20%20Transmitted%20Light%20Images%0AAuthor%3A%20Jintu%20Zheng%20and%20Yi%20Ding%20and%20Qizhe%20Liu%20and%20Yi%20Cao%20and%20Ying%20Hu%20and%20Zenan%20Wang%0AAbstract%3A%20%20%20Traditional%20fluorescence%20staining%20is%20phototoxic%20to%20live%20cells%2C%20slow%2C%20and%0Aexpensive%3B%20thus%2C%20the%20subcellular%20structure%20prediction%20%28SSP%29%20from%20transmitted%0Alight%20%28TL%29%20images%20is%20emerging%20as%20a%20label-free%2C%20faster%2C%20low-cost%20alternative.%0AHowever%2C%20existing%20approaches%20utilize%203D%20networks%20for%20one-to-one%20voxel%20level%0Adense%20prediction%2C%20which%20necessitates%20a%20frequent%20and%20time-consuming%20Z-axis%0Aimaging%20process.%20Moreover%2C%203D%20convolutions%20inevitably%20lead%20to%20significant%0Acomputation%20and%20GPU%20memory%20overhead.%20Therefore%2C%20we%20propose%20an%20efficient%0Aframework%2C%20SparseSSP%2C%20predicting%20fluorescent%20intensities%20within%20the%20target%0Avoxel%20grid%20in%20an%20efficient%20paradigm%20instead%20of%20relying%20entirely%20on%203D%0Atopologies.%20In%20particular%2C%20SparseSSP%20makes%20two%20pivotal%20improvements%20to%20prior%0Aworks.%20First%2C%20SparseSSP%20introduces%20a%20one-to-many%20voxel%20mapping%20paradigm%2C%20which%0Apermits%20the%20sparse%20TL%20slices%20to%20reconstruct%20the%20subcellular%20structure.%0ASecondly%2C%20we%20propose%20a%20hybrid%20dimensions%20topology%2C%20which%20folds%20the%20Z-axis%0Ainformation%20into%20channel%20features%2C%20enabling%20the%202D%20network%20layers%20to%20tackle%20SSP%0Aunder%20low%20computational%20cost.%20We%20conduct%20extensive%20experiments%20to%20validate%20the%0Aeffectiveness%20and%20advantages%20of%20SparseSSP%20on%20diverse%20sparse%20imaging%20ratios%2C%20and%0Aour%20approach%20achieves%20a%20leading%20performance%20compared%20to%20pure%203D%20topologies.%0ASparseSSP%20reduces%20imaging%20frequencies%20compared%20to%20previous%20dense-view%20SSP%0A%28i.e.%2C%20the%20number%20of%20imaging%20is%20reduced%20up%20to%2087.5%25%20at%20most%29%2C%20which%20is%0Asignificant%20in%20visualizing%20rapid%20biological%20dynamics%20on%20low-cost%20devices%20and%0Asamples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02159v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSparseSSP%253A%25203D%2520Subcellular%2520Structure%2520Prediction%2520from%2520Sparse-View%250A%2520%2520Transmitted%2520Light%2520Images%26entry.906535625%3DJintu%2520Zheng%2520and%2520Yi%2520Ding%2520and%2520Qizhe%2520Liu%2520and%2520Yi%2520Cao%2520and%2520Ying%2520Hu%2520and%2520Zenan%2520Wang%26entry.1292438233%3D%2520%2520Traditional%2520fluorescence%2520staining%2520is%2520phototoxic%2520to%2520live%2520cells%252C%2520slow%252C%2520and%250Aexpensive%253B%2520thus%252C%2520the%2520subcellular%2520structure%2520prediction%2520%2528SSP%2529%2520from%2520transmitted%250Alight%2520%2528TL%2529%2520images%2520is%2520emerging%2520as%2520a%2520label-free%252C%2520faster%252C%2520low-cost%2520alternative.%250AHowever%252C%2520existing%2520approaches%2520utilize%25203D%2520networks%2520for%2520one-to-one%2520voxel%2520level%250Adense%2520prediction%252C%2520which%2520necessitates%2520a%2520frequent%2520and%2520time-consuming%2520Z-axis%250Aimaging%2520process.%2520Moreover%252C%25203D%2520convolutions%2520inevitably%2520lead%2520to%2520significant%250Acomputation%2520and%2520GPU%2520memory%2520overhead.%2520Therefore%252C%2520we%2520propose%2520an%2520efficient%250Aframework%252C%2520SparseSSP%252C%2520predicting%2520fluorescent%2520intensities%2520within%2520the%2520target%250Avoxel%2520grid%2520in%2520an%2520efficient%2520paradigm%2520instead%2520of%2520relying%2520entirely%2520on%25203D%250Atopologies.%2520In%2520particular%252C%2520SparseSSP%2520makes%2520two%2520pivotal%2520improvements%2520to%2520prior%250Aworks.%2520First%252C%2520SparseSSP%2520introduces%2520a%2520one-to-many%2520voxel%2520mapping%2520paradigm%252C%2520which%250Apermits%2520the%2520sparse%2520TL%2520slices%2520to%2520reconstruct%2520the%2520subcellular%2520structure.%250ASecondly%252C%2520we%2520propose%2520a%2520hybrid%2520dimensions%2520topology%252C%2520which%2520folds%2520the%2520Z-axis%250Ainformation%2520into%2520channel%2520features%252C%2520enabling%2520the%25202D%2520network%2520layers%2520to%2520tackle%2520SSP%250Aunder%2520low%2520computational%2520cost.%2520We%2520conduct%2520extensive%2520experiments%2520to%2520validate%2520the%250Aeffectiveness%2520and%2520advantages%2520of%2520SparseSSP%2520on%2520diverse%2520sparse%2520imaging%2520ratios%252C%2520and%250Aour%2520approach%2520achieves%2520a%2520leading%2520performance%2520compared%2520to%2520pure%25203D%2520topologies.%250ASparseSSP%2520reduces%2520imaging%2520frequencies%2520compared%2520to%2520previous%2520dense-view%2520SSP%250A%2528i.e.%252C%2520the%2520number%2520of%2520imaging%2520is%2520reduced%2520up%2520to%252087.5%2525%2520at%2520most%2529%252C%2520which%2520is%250Asignificant%2520in%2520visualizing%2520rapid%2520biological%2520dynamics%2520on%2520low-cost%2520devices%2520and%250Asamples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02159v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SparseSSP%3A%203D%20Subcellular%20Structure%20Prediction%20from%20Sparse-View%0A%20%20Transmitted%20Light%20Images&entry.906535625=Jintu%20Zheng%20and%20Yi%20Ding%20and%20Qizhe%20Liu%20and%20Yi%20Cao%20and%20Ying%20Hu%20and%20Zenan%20Wang&entry.1292438233=%20%20Traditional%20fluorescence%20staining%20is%20phototoxic%20to%20live%20cells%2C%20slow%2C%20and%0Aexpensive%3B%20thus%2C%20the%20subcellular%20structure%20prediction%20%28SSP%29%20from%20transmitted%0Alight%20%28TL%29%20images%20is%20emerging%20as%20a%20label-free%2C%20faster%2C%20low-cost%20alternative.%0AHowever%2C%20existing%20approaches%20utilize%203D%20networks%20for%20one-to-one%20voxel%20level%0Adense%20prediction%2C%20which%20necessitates%20a%20frequent%20and%20time-consuming%20Z-axis%0Aimaging%20process.%20Moreover%2C%203D%20convolutions%20inevitably%20lead%20to%20significant%0Acomputation%20and%20GPU%20memory%20overhead.%20Therefore%2C%20we%20propose%20an%20efficient%0Aframework%2C%20SparseSSP%2C%20predicting%20fluorescent%20intensities%20within%20the%20target%0Avoxel%20grid%20in%20an%20efficient%20paradigm%20instead%20of%20relying%20entirely%20on%203D%0Atopologies.%20In%20particular%2C%20SparseSSP%20makes%20two%20pivotal%20improvements%20to%20prior%0Aworks.%20First%2C%20SparseSSP%20introduces%20a%20one-to-many%20voxel%20mapping%20paradigm%2C%20which%0Apermits%20the%20sparse%20TL%20slices%20to%20reconstruct%20the%20subcellular%20structure.%0ASecondly%2C%20we%20propose%20a%20hybrid%20dimensions%20topology%2C%20which%20folds%20the%20Z-axis%0Ainformation%20into%20channel%20features%2C%20enabling%20the%202D%20network%20layers%20to%20tackle%20SSP%0Aunder%20low%20computational%20cost.%20We%20conduct%20extensive%20experiments%20to%20validate%20the%0Aeffectiveness%20and%20advantages%20of%20SparseSSP%20on%20diverse%20sparse%20imaging%20ratios%2C%20and%0Aour%20approach%20achieves%20a%20leading%20performance%20compared%20to%20pure%203D%20topologies.%0ASparseSSP%20reduces%20imaging%20frequencies%20compared%20to%20previous%20dense-view%20SSP%0A%28i.e.%2C%20the%20number%20of%20imaging%20is%20reduced%20up%20to%2087.5%25%20at%20most%29%2C%20which%20is%0Asignificant%20in%20visualizing%20rapid%20biological%20dynamics%20on%20low-cost%20devices%20and%0Asamples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02159v2&entry.124074799=Read"},
{"title": "GMM-ResNext: Combining Generative and Discriminative Models for Speaker\n  Verification", "author": "Hui Yan and Zhenchun Lei and Changhong Liu and Yong Zhou", "abstract": "  With the development of deep learning, many different network architectures\nhave been explored in speaker verification. However, most network architectures\nrely on a single deep learning architecture, and hybrid networks combining\ndifferent architectures have been little studied in ASV tasks. In this paper,\nwe propose the GMM-ResNext model for speaker verification. Conventional GMM\ndoes not consider the score distribution of each frame feature over all\nGaussian components and ignores the relationship between neighboring speech\nframes. So, we extract the log Gaussian probability features based on the raw\nacoustic features and use ResNext-based network as the backbone to extract the\nspeaker embedding. GMM-ResNext combines Generative and Discriminative Models to\nimprove the generalization ability of deep learning models and allows one to\nmore easily specify meaningful priors on model parameters. A two-path\nGMM-ResNext model based on two gender-related GMMs has also been proposed. The\nExperimental results show that the proposed GMM-ResNext achieves relative\nimprovements of 48.1\\% and 11.3\\% in EER compared with ResNet34 and ECAPA-TDNN\non VoxCeleb1-O test set.\n", "link": "http://arxiv.org/abs/2407.03135v1", "date": "2024-07-03", "relevancy": 2.3007, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4637}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4623}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GMM-ResNext%3A%20Combining%20Generative%20and%20Discriminative%20Models%20for%20Speaker%0A%20%20Verification&body=Title%3A%20GMM-ResNext%3A%20Combining%20Generative%20and%20Discriminative%20Models%20for%20Speaker%0A%20%20Verification%0AAuthor%3A%20Hui%20Yan%20and%20Zhenchun%20Lei%20and%20Changhong%20Liu%20and%20Yong%20Zhou%0AAbstract%3A%20%20%20With%20the%20development%20of%20deep%20learning%2C%20many%20different%20network%20architectures%0Ahave%20been%20explored%20in%20speaker%20verification.%20However%2C%20most%20network%20architectures%0Arely%20on%20a%20single%20deep%20learning%20architecture%2C%20and%20hybrid%20networks%20combining%0Adifferent%20architectures%20have%20been%20little%20studied%20in%20ASV%20tasks.%20In%20this%20paper%2C%0Awe%20propose%20the%20GMM-ResNext%20model%20for%20speaker%20verification.%20Conventional%20GMM%0Adoes%20not%20consider%20the%20score%20distribution%20of%20each%20frame%20feature%20over%20all%0AGaussian%20components%20and%20ignores%20the%20relationship%20between%20neighboring%20speech%0Aframes.%20So%2C%20we%20extract%20the%20log%20Gaussian%20probability%20features%20based%20on%20the%20raw%0Aacoustic%20features%20and%20use%20ResNext-based%20network%20as%20the%20backbone%20to%20extract%20the%0Aspeaker%20embedding.%20GMM-ResNext%20combines%20Generative%20and%20Discriminative%20Models%20to%0Aimprove%20the%20generalization%20ability%20of%20deep%20learning%20models%20and%20allows%20one%20to%0Amore%20easily%20specify%20meaningful%20priors%20on%20model%20parameters.%20A%20two-path%0AGMM-ResNext%20model%20based%20on%20two%20gender-related%20GMMs%20has%20also%20been%20proposed.%20The%0AExperimental%20results%20show%20that%20the%20proposed%20GMM-ResNext%20achieves%20relative%0Aimprovements%20of%2048.1%5C%25%20and%2011.3%5C%25%20in%20EER%20compared%20with%20ResNet34%20and%20ECAPA-TDNN%0Aon%20VoxCeleb1-O%20test%20set.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03135v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGMM-ResNext%253A%2520Combining%2520Generative%2520and%2520Discriminative%2520Models%2520for%2520Speaker%250A%2520%2520Verification%26entry.906535625%3DHui%2520Yan%2520and%2520Zhenchun%2520Lei%2520and%2520Changhong%2520Liu%2520and%2520Yong%2520Zhou%26entry.1292438233%3D%2520%2520With%2520the%2520development%2520of%2520deep%2520learning%252C%2520many%2520different%2520network%2520architectures%250Ahave%2520been%2520explored%2520in%2520speaker%2520verification.%2520However%252C%2520most%2520network%2520architectures%250Arely%2520on%2520a%2520single%2520deep%2520learning%2520architecture%252C%2520and%2520hybrid%2520networks%2520combining%250Adifferent%2520architectures%2520have%2520been%2520little%2520studied%2520in%2520ASV%2520tasks.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520the%2520GMM-ResNext%2520model%2520for%2520speaker%2520verification.%2520Conventional%2520GMM%250Adoes%2520not%2520consider%2520the%2520score%2520distribution%2520of%2520each%2520frame%2520feature%2520over%2520all%250AGaussian%2520components%2520and%2520ignores%2520the%2520relationship%2520between%2520neighboring%2520speech%250Aframes.%2520So%252C%2520we%2520extract%2520the%2520log%2520Gaussian%2520probability%2520features%2520based%2520on%2520the%2520raw%250Aacoustic%2520features%2520and%2520use%2520ResNext-based%2520network%2520as%2520the%2520backbone%2520to%2520extract%2520the%250Aspeaker%2520embedding.%2520GMM-ResNext%2520combines%2520Generative%2520and%2520Discriminative%2520Models%2520to%250Aimprove%2520the%2520generalization%2520ability%2520of%2520deep%2520learning%2520models%2520and%2520allows%2520one%2520to%250Amore%2520easily%2520specify%2520meaningful%2520priors%2520on%2520model%2520parameters.%2520A%2520two-path%250AGMM-ResNext%2520model%2520based%2520on%2520two%2520gender-related%2520GMMs%2520has%2520also%2520been%2520proposed.%2520The%250AExperimental%2520results%2520show%2520that%2520the%2520proposed%2520GMM-ResNext%2520achieves%2520relative%250Aimprovements%2520of%252048.1%255C%2525%2520and%252011.3%255C%2525%2520in%2520EER%2520compared%2520with%2520ResNet34%2520and%2520ECAPA-TDNN%250Aon%2520VoxCeleb1-O%2520test%2520set.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03135v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GMM-ResNext%3A%20Combining%20Generative%20and%20Discriminative%20Models%20for%20Speaker%0A%20%20Verification&entry.906535625=Hui%20Yan%20and%20Zhenchun%20Lei%20and%20Changhong%20Liu%20and%20Yong%20Zhou&entry.1292438233=%20%20With%20the%20development%20of%20deep%20learning%2C%20many%20different%20network%20architectures%0Ahave%20been%20explored%20in%20speaker%20verification.%20However%2C%20most%20network%20architectures%0Arely%20on%20a%20single%20deep%20learning%20architecture%2C%20and%20hybrid%20networks%20combining%0Adifferent%20architectures%20have%20been%20little%20studied%20in%20ASV%20tasks.%20In%20this%20paper%2C%0Awe%20propose%20the%20GMM-ResNext%20model%20for%20speaker%20verification.%20Conventional%20GMM%0Adoes%20not%20consider%20the%20score%20distribution%20of%20each%20frame%20feature%20over%20all%0AGaussian%20components%20and%20ignores%20the%20relationship%20between%20neighboring%20speech%0Aframes.%20So%2C%20we%20extract%20the%20log%20Gaussian%20probability%20features%20based%20on%20the%20raw%0Aacoustic%20features%20and%20use%20ResNext-based%20network%20as%20the%20backbone%20to%20extract%20the%0Aspeaker%20embedding.%20GMM-ResNext%20combines%20Generative%20and%20Discriminative%20Models%20to%0Aimprove%20the%20generalization%20ability%20of%20deep%20learning%20models%20and%20allows%20one%20to%0Amore%20easily%20specify%20meaningful%20priors%20on%20model%20parameters.%20A%20two-path%0AGMM-ResNext%20model%20based%20on%20two%20gender-related%20GMMs%20has%20also%20been%20proposed.%20The%0AExperimental%20results%20show%20that%20the%20proposed%20GMM-ResNext%20achieves%20relative%0Aimprovements%20of%2048.1%5C%25%20and%2011.3%5C%25%20in%20EER%20compared%20with%20ResNet34%20and%20ECAPA-TDNN%0Aon%20VoxCeleb1-O%20test%20set.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03135v1&entry.124074799=Read"},
{"title": "Federated Continual Learning Goes Online: Leveraging Uncertainty for\n  Modality-Agnostic Class-Incremental Learning", "author": "Giuseppe Serra and Florian Buettner", "abstract": "  Given the ability to model more realistic and dynamic problems, Federated\nContinual Learning (FCL) has been increasingly investigated recently. A\nwell-known problem encountered in this setting is the so-called catastrophic\nforgetting, for which the learning model is inclined to focus on more recent\ntasks while forgetting the previously learned knowledge. The majority of the\ncurrent approaches in FCL propose generative-based solutions to solve said\nproblem. However, this setting requires multiple training epochs over the data,\nimplying an offline setting where datasets are stored locally and remain\nunchanged over time. Furthermore, the proposed solutions are tailored for\nvision tasks solely. To overcome these limitations, we propose a new\nmodality-agnostic approach to deal with the online scenario where new data\narrive in streams of mini-batches that can only be processed once. To solve\ncatastrophic forgetting, we propose an uncertainty-aware memory-based approach.\nIn particular, we suggest using an estimator based on the Bregman Information\n(BI) to compute the model's variance at the sample level. Through measures of\npredictive uncertainty, we retrieve samples with specific characteristics, and\n- by retraining the model on such samples - we demonstrate the potential of\nthis approach to reduce the forgetting effect in realistic settings.\n", "link": "http://arxiv.org/abs/2405.18925v2", "date": "2024-07-03", "relevancy": 2.2987, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.593}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5714}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5577}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Continual%20Learning%20Goes%20Online%3A%20Leveraging%20Uncertainty%20for%0A%20%20Modality-Agnostic%20Class-Incremental%20Learning&body=Title%3A%20Federated%20Continual%20Learning%20Goes%20Online%3A%20Leveraging%20Uncertainty%20for%0A%20%20Modality-Agnostic%20Class-Incremental%20Learning%0AAuthor%3A%20Giuseppe%20Serra%20and%20Florian%20Buettner%0AAbstract%3A%20%20%20Given%20the%20ability%20to%20model%20more%20realistic%20and%20dynamic%20problems%2C%20Federated%0AContinual%20Learning%20%28FCL%29%20has%20been%20increasingly%20investigated%20recently.%20A%0Awell-known%20problem%20encountered%20in%20this%20setting%20is%20the%20so-called%20catastrophic%0Aforgetting%2C%20for%20which%20the%20learning%20model%20is%20inclined%20to%20focus%20on%20more%20recent%0Atasks%20while%20forgetting%20the%20previously%20learned%20knowledge.%20The%20majority%20of%20the%0Acurrent%20approaches%20in%20FCL%20propose%20generative-based%20solutions%20to%20solve%20said%0Aproblem.%20However%2C%20this%20setting%20requires%20multiple%20training%20epochs%20over%20the%20data%2C%0Aimplying%20an%20offline%20setting%20where%20datasets%20are%20stored%20locally%20and%20remain%0Aunchanged%20over%20time.%20Furthermore%2C%20the%20proposed%20solutions%20are%20tailored%20for%0Avision%20tasks%20solely.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20new%0Amodality-agnostic%20approach%20to%20deal%20with%20the%20online%20scenario%20where%20new%20data%0Aarrive%20in%20streams%20of%20mini-batches%20that%20can%20only%20be%20processed%20once.%20To%20solve%0Acatastrophic%20forgetting%2C%20we%20propose%20an%20uncertainty-aware%20memory-based%20approach.%0AIn%20particular%2C%20we%20suggest%20using%20an%20estimator%20based%20on%20the%20Bregman%20Information%0A%28BI%29%20to%20compute%20the%20model%27s%20variance%20at%20the%20sample%20level.%20Through%20measures%20of%0Apredictive%20uncertainty%2C%20we%20retrieve%20samples%20with%20specific%20characteristics%2C%20and%0A-%20by%20retraining%20the%20model%20on%20such%20samples%20-%20we%20demonstrate%20the%20potential%20of%0Athis%20approach%20to%20reduce%20the%20forgetting%20effect%20in%20realistic%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.18925v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Continual%2520Learning%2520Goes%2520Online%253A%2520Leveraging%2520Uncertainty%2520for%250A%2520%2520Modality-Agnostic%2520Class-Incremental%2520Learning%26entry.906535625%3DGiuseppe%2520Serra%2520and%2520Florian%2520Buettner%26entry.1292438233%3D%2520%2520Given%2520the%2520ability%2520to%2520model%2520more%2520realistic%2520and%2520dynamic%2520problems%252C%2520Federated%250AContinual%2520Learning%2520%2528FCL%2529%2520has%2520been%2520increasingly%2520investigated%2520recently.%2520A%250Awell-known%2520problem%2520encountered%2520in%2520this%2520setting%2520is%2520the%2520so-called%2520catastrophic%250Aforgetting%252C%2520for%2520which%2520the%2520learning%2520model%2520is%2520inclined%2520to%2520focus%2520on%2520more%2520recent%250Atasks%2520while%2520forgetting%2520the%2520previously%2520learned%2520knowledge.%2520The%2520majority%2520of%2520the%250Acurrent%2520approaches%2520in%2520FCL%2520propose%2520generative-based%2520solutions%2520to%2520solve%2520said%250Aproblem.%2520However%252C%2520this%2520setting%2520requires%2520multiple%2520training%2520epochs%2520over%2520the%2520data%252C%250Aimplying%2520an%2520offline%2520setting%2520where%2520datasets%2520are%2520stored%2520locally%2520and%2520remain%250Aunchanged%2520over%2520time.%2520Furthermore%252C%2520the%2520proposed%2520solutions%2520are%2520tailored%2520for%250Avision%2520tasks%2520solely.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520a%2520new%250Amodality-agnostic%2520approach%2520to%2520deal%2520with%2520the%2520online%2520scenario%2520where%2520new%2520data%250Aarrive%2520in%2520streams%2520of%2520mini-batches%2520that%2520can%2520only%2520be%2520processed%2520once.%2520To%2520solve%250Acatastrophic%2520forgetting%252C%2520we%2520propose%2520an%2520uncertainty-aware%2520memory-based%2520approach.%250AIn%2520particular%252C%2520we%2520suggest%2520using%2520an%2520estimator%2520based%2520on%2520the%2520Bregman%2520Information%250A%2528BI%2529%2520to%2520compute%2520the%2520model%2527s%2520variance%2520at%2520the%2520sample%2520level.%2520Through%2520measures%2520of%250Apredictive%2520uncertainty%252C%2520we%2520retrieve%2520samples%2520with%2520specific%2520characteristics%252C%2520and%250A-%2520by%2520retraining%2520the%2520model%2520on%2520such%2520samples%2520-%2520we%2520demonstrate%2520the%2520potential%2520of%250Athis%2520approach%2520to%2520reduce%2520the%2520forgetting%2520effect%2520in%2520realistic%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.18925v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Continual%20Learning%20Goes%20Online%3A%20Leveraging%20Uncertainty%20for%0A%20%20Modality-Agnostic%20Class-Incremental%20Learning&entry.906535625=Giuseppe%20Serra%20and%20Florian%20Buettner&entry.1292438233=%20%20Given%20the%20ability%20to%20model%20more%20realistic%20and%20dynamic%20problems%2C%20Federated%0AContinual%20Learning%20%28FCL%29%20has%20been%20increasingly%20investigated%20recently.%20A%0Awell-known%20problem%20encountered%20in%20this%20setting%20is%20the%20so-called%20catastrophic%0Aforgetting%2C%20for%20which%20the%20learning%20model%20is%20inclined%20to%20focus%20on%20more%20recent%0Atasks%20while%20forgetting%20the%20previously%20learned%20knowledge.%20The%20majority%20of%20the%0Acurrent%20approaches%20in%20FCL%20propose%20generative-based%20solutions%20to%20solve%20said%0Aproblem.%20However%2C%20this%20setting%20requires%20multiple%20training%20epochs%20over%20the%20data%2C%0Aimplying%20an%20offline%20setting%20where%20datasets%20are%20stored%20locally%20and%20remain%0Aunchanged%20over%20time.%20Furthermore%2C%20the%20proposed%20solutions%20are%20tailored%20for%0Avision%20tasks%20solely.%20To%20overcome%20these%20limitations%2C%20we%20propose%20a%20new%0Amodality-agnostic%20approach%20to%20deal%20with%20the%20online%20scenario%20where%20new%20data%0Aarrive%20in%20streams%20of%20mini-batches%20that%20can%20only%20be%20processed%20once.%20To%20solve%0Acatastrophic%20forgetting%2C%20we%20propose%20an%20uncertainty-aware%20memory-based%20approach.%0AIn%20particular%2C%20we%20suggest%20using%20an%20estimator%20based%20on%20the%20Bregman%20Information%0A%28BI%29%20to%20compute%20the%20model%27s%20variance%20at%20the%20sample%20level.%20Through%20measures%20of%0Apredictive%20uncertainty%2C%20we%20retrieve%20samples%20with%20specific%20characteristics%2C%20and%0A-%20by%20retraining%20the%20model%20on%20such%20samples%20-%20we%20demonstrate%20the%20potential%20of%0Athis%20approach%20to%20reduce%20the%20forgetting%20effect%20in%20realistic%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.18925v2&entry.124074799=Read"},
{"title": "Self-distilled Masked Attention guided masked image modeling with noise\n  Regularized Teacher (SMART) for medical image analysis", "author": "Jue Jiang and Aneesh Rangnekar and Chloe Min Seo Choi and Harini Veeraraghavan", "abstract": "  Pretraining vision transformers (ViT) with attention guided masked image\nmodeling (MIM) has shown to increase downstream accuracy for natural image\nanalysis. Hierarchical shifted window (Swin) transformer, often used in medical\nimage analysis cannot use attention guided masking as it lacks an explicit\n[CLS] token, needed for computing attention maps for selective masking. We thus\nenhanced Swin with semantic class attention. We developed a co-distilled Swin\ntransformer that combines a noisy momentum updated teacher to guide selective\nmasking for MIM. Our approach called \\textsc{s}e\\textsc{m}antic\n\\textsc{a}ttention guided co-distillation with noisy teacher\n\\textsc{r}egularized Swin \\textsc{T}rans\\textsc{F}ormer (SMARTFormer) was\napplied for analyzing 3D computed tomography datasets with lung nodules and\nmalignant lung cancers (LC). We also analyzed the impact of semantic attention\nand noisy teacher on pretraining and downstream accuracy. SMARTFormer\nclassified lesions (malignant from benign) with a high accuracy of 0.895 of\n1000 nodules, predicted LC treatment response with accuracy of 0.74, and\nachieved high accuracies even in limited data regimes. Pretraining with\nsemantic attention and noisy teacher improved ability to distinguish\nsemantically meaningful structures such as organs in a unsupervised clustering\ntask and localize abnormal structures like tumors. Code, models will be made\navailable through GitHub upon paper acceptance.\n", "link": "http://arxiv.org/abs/2310.01209v2", "date": "2024-07-03", "relevancy": 2.2919, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5876}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5643}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-distilled%20Masked%20Attention%20guided%20masked%20image%20modeling%20with%20noise%0A%20%20Regularized%20Teacher%20%28SMART%29%20for%20medical%20image%20analysis&body=Title%3A%20Self-distilled%20Masked%20Attention%20guided%20masked%20image%20modeling%20with%20noise%0A%20%20Regularized%20Teacher%20%28SMART%29%20for%20medical%20image%20analysis%0AAuthor%3A%20Jue%20Jiang%20and%20Aneesh%20Rangnekar%20and%20Chloe%20Min%20Seo%20Choi%20and%20Harini%20Veeraraghavan%0AAbstract%3A%20%20%20Pretraining%20vision%20transformers%20%28ViT%29%20with%20attention%20guided%20masked%20image%0Amodeling%20%28MIM%29%20has%20shown%20to%20increase%20downstream%20accuracy%20for%20natural%20image%0Aanalysis.%20Hierarchical%20shifted%20window%20%28Swin%29%20transformer%2C%20often%20used%20in%20medical%0Aimage%20analysis%20cannot%20use%20attention%20guided%20masking%20as%20it%20lacks%20an%20explicit%0A%5BCLS%5D%20token%2C%20needed%20for%20computing%20attention%20maps%20for%20selective%20masking.%20We%20thus%0Aenhanced%20Swin%20with%20semantic%20class%20attention.%20We%20developed%20a%20co-distilled%20Swin%0Atransformer%20that%20combines%20a%20noisy%20momentum%20updated%20teacher%20to%20guide%20selective%0Amasking%20for%20MIM.%20Our%20approach%20called%20%5Ctextsc%7Bs%7De%5Ctextsc%7Bm%7Dantic%0A%5Ctextsc%7Ba%7Dttention%20guided%20co-distillation%20with%20noisy%20teacher%0A%5Ctextsc%7Br%7Degularized%20Swin%20%5Ctextsc%7BT%7Drans%5Ctextsc%7BF%7Dormer%20%28SMARTFormer%29%20was%0Aapplied%20for%20analyzing%203D%20computed%20tomography%20datasets%20with%20lung%20nodules%20and%0Amalignant%20lung%20cancers%20%28LC%29.%20We%20also%20analyzed%20the%20impact%20of%20semantic%20attention%0Aand%20noisy%20teacher%20on%20pretraining%20and%20downstream%20accuracy.%20SMARTFormer%0Aclassified%20lesions%20%28malignant%20from%20benign%29%20with%20a%20high%20accuracy%20of%200.895%20of%0A1000%20nodules%2C%20predicted%20LC%20treatment%20response%20with%20accuracy%20of%200.74%2C%20and%0Aachieved%20high%20accuracies%20even%20in%20limited%20data%20regimes.%20Pretraining%20with%0Asemantic%20attention%20and%20noisy%20teacher%20improved%20ability%20to%20distinguish%0Asemantically%20meaningful%20structures%20such%20as%20organs%20in%20a%20unsupervised%20clustering%0Atask%20and%20localize%20abnormal%20structures%20like%20tumors.%20Code%2C%20models%20will%20be%20made%0Aavailable%20through%20GitHub%20upon%20paper%20acceptance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01209v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-distilled%2520Masked%2520Attention%2520guided%2520masked%2520image%2520modeling%2520with%2520noise%250A%2520%2520Regularized%2520Teacher%2520%2528SMART%2529%2520for%2520medical%2520image%2520analysis%26entry.906535625%3DJue%2520Jiang%2520and%2520Aneesh%2520Rangnekar%2520and%2520Chloe%2520Min%2520Seo%2520Choi%2520and%2520Harini%2520Veeraraghavan%26entry.1292438233%3D%2520%2520Pretraining%2520vision%2520transformers%2520%2528ViT%2529%2520with%2520attention%2520guided%2520masked%2520image%250Amodeling%2520%2528MIM%2529%2520has%2520shown%2520to%2520increase%2520downstream%2520accuracy%2520for%2520natural%2520image%250Aanalysis.%2520Hierarchical%2520shifted%2520window%2520%2528Swin%2529%2520transformer%252C%2520often%2520used%2520in%2520medical%250Aimage%2520analysis%2520cannot%2520use%2520attention%2520guided%2520masking%2520as%2520it%2520lacks%2520an%2520explicit%250A%255BCLS%255D%2520token%252C%2520needed%2520for%2520computing%2520attention%2520maps%2520for%2520selective%2520masking.%2520We%2520thus%250Aenhanced%2520Swin%2520with%2520semantic%2520class%2520attention.%2520We%2520developed%2520a%2520co-distilled%2520Swin%250Atransformer%2520that%2520combines%2520a%2520noisy%2520momentum%2520updated%2520teacher%2520to%2520guide%2520selective%250Amasking%2520for%2520MIM.%2520Our%2520approach%2520called%2520%255Ctextsc%257Bs%257De%255Ctextsc%257Bm%257Dantic%250A%255Ctextsc%257Ba%257Dttention%2520guided%2520co-distillation%2520with%2520noisy%2520teacher%250A%255Ctextsc%257Br%257Degularized%2520Swin%2520%255Ctextsc%257BT%257Drans%255Ctextsc%257BF%257Dormer%2520%2528SMARTFormer%2529%2520was%250Aapplied%2520for%2520analyzing%25203D%2520computed%2520tomography%2520datasets%2520with%2520lung%2520nodules%2520and%250Amalignant%2520lung%2520cancers%2520%2528LC%2529.%2520We%2520also%2520analyzed%2520the%2520impact%2520of%2520semantic%2520attention%250Aand%2520noisy%2520teacher%2520on%2520pretraining%2520and%2520downstream%2520accuracy.%2520SMARTFormer%250Aclassified%2520lesions%2520%2528malignant%2520from%2520benign%2529%2520with%2520a%2520high%2520accuracy%2520of%25200.895%2520of%250A1000%2520nodules%252C%2520predicted%2520LC%2520treatment%2520response%2520with%2520accuracy%2520of%25200.74%252C%2520and%250Aachieved%2520high%2520accuracies%2520even%2520in%2520limited%2520data%2520regimes.%2520Pretraining%2520with%250Asemantic%2520attention%2520and%2520noisy%2520teacher%2520improved%2520ability%2520to%2520distinguish%250Asemantically%2520meaningful%2520structures%2520such%2520as%2520organs%2520in%2520a%2520unsupervised%2520clustering%250Atask%2520and%2520localize%2520abnormal%2520structures%2520like%2520tumors.%2520Code%252C%2520models%2520will%2520be%2520made%250Aavailable%2520through%2520GitHub%2520upon%2520paper%2520acceptance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.01209v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-distilled%20Masked%20Attention%20guided%20masked%20image%20modeling%20with%20noise%0A%20%20Regularized%20Teacher%20%28SMART%29%20for%20medical%20image%20analysis&entry.906535625=Jue%20Jiang%20and%20Aneesh%20Rangnekar%20and%20Chloe%20Min%20Seo%20Choi%20and%20Harini%20Veeraraghavan&entry.1292438233=%20%20Pretraining%20vision%20transformers%20%28ViT%29%20with%20attention%20guided%20masked%20image%0Amodeling%20%28MIM%29%20has%20shown%20to%20increase%20downstream%20accuracy%20for%20natural%20image%0Aanalysis.%20Hierarchical%20shifted%20window%20%28Swin%29%20transformer%2C%20often%20used%20in%20medical%0Aimage%20analysis%20cannot%20use%20attention%20guided%20masking%20as%20it%20lacks%20an%20explicit%0A%5BCLS%5D%20token%2C%20needed%20for%20computing%20attention%20maps%20for%20selective%20masking.%20We%20thus%0Aenhanced%20Swin%20with%20semantic%20class%20attention.%20We%20developed%20a%20co-distilled%20Swin%0Atransformer%20that%20combines%20a%20noisy%20momentum%20updated%20teacher%20to%20guide%20selective%0Amasking%20for%20MIM.%20Our%20approach%20called%20%5Ctextsc%7Bs%7De%5Ctextsc%7Bm%7Dantic%0A%5Ctextsc%7Ba%7Dttention%20guided%20co-distillation%20with%20noisy%20teacher%0A%5Ctextsc%7Br%7Degularized%20Swin%20%5Ctextsc%7BT%7Drans%5Ctextsc%7BF%7Dormer%20%28SMARTFormer%29%20was%0Aapplied%20for%20analyzing%203D%20computed%20tomography%20datasets%20with%20lung%20nodules%20and%0Amalignant%20lung%20cancers%20%28LC%29.%20We%20also%20analyzed%20the%20impact%20of%20semantic%20attention%0Aand%20noisy%20teacher%20on%20pretraining%20and%20downstream%20accuracy.%20SMARTFormer%0Aclassified%20lesions%20%28malignant%20from%20benign%29%20with%20a%20high%20accuracy%20of%200.895%20of%0A1000%20nodules%2C%20predicted%20LC%20treatment%20response%20with%20accuracy%20of%200.74%2C%20and%0Aachieved%20high%20accuracies%20even%20in%20limited%20data%20regimes.%20Pretraining%20with%0Asemantic%20attention%20and%20noisy%20teacher%20improved%20ability%20to%20distinguish%0Asemantically%20meaningful%20structures%20such%20as%20organs%20in%20a%20unsupervised%20clustering%0Atask%20and%20localize%20abnormal%20structures%20like%20tumors.%20Code%2C%20models%20will%20be%20made%0Aavailable%20through%20GitHub%20upon%20paper%20acceptance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01209v2&entry.124074799=Read"},
{"title": "Learning Disentangled Representation in Object-Centric Models for Visual\n  Dynamics Prediction via Transformers", "author": "Sanket Gandhi and  Atul and Samanyu Mahajan and Vishal Sharma and Rushil Gupta and Arnab Kumar Mondal and Parag Singla", "abstract": "  Recent work has shown that object-centric representations can greatly help\nimprove the accuracy of learning dynamics while also bringing interpretability.\nIn this work, we take this idea one step further, ask the following question:\n\"can learning disentangled representation further improve the accuracy of\nvisual dynamics prediction in object-centric models?\" While there has been some\nattempt to learn such disentangled representations for the case of static\nimages \\citep{nsb}, to the best of our knowledge, ours is the first work which\ntries to do this in a general setting for video, without making any specific\nassumptions about the kind of attributes that an object might have. The key\nbuilding block of our architecture is the notion of a {\\em block}, where\nseveral blocks together constitute an object. Each block is represented as a\nlinear combination of a given number of learnable concept vectors, which is\niteratively refined during the learning process. The blocks in our model are\ndiscovered in an unsupervised manner, by attending over object masks, in a\nstyle similar to discovery of slots \\citep{slot_attention}, for learning a\ndense object-centric representation. We employ self-attention via transformers\nover the discovered blocks to predict the next state resulting in discovery of\nvisual dynamics. We perform a series of experiments on several benchmark 2-D,\nand 3-D datasets demonstrating that our architecture (1) can discover\nsemantically meaningful blocks (2) help improve accuracy of dynamics prediction\ncompared to SOTA object-centric models (3) perform significantly better in OOD\nsetting where the specific attribute combinations are not seen earlier during\ntraining. Our experiments highlight the importance discovery of disentangled\nrepresentation for visual dynamics prediction.\n", "link": "http://arxiv.org/abs/2407.03216v1", "date": "2024-07-03", "relevancy": 2.287, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5729}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5715}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5715}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Disentangled%20Representation%20in%20Object-Centric%20Models%20for%20Visual%0A%20%20Dynamics%20Prediction%20via%20Transformers&body=Title%3A%20Learning%20Disentangled%20Representation%20in%20Object-Centric%20Models%20for%20Visual%0A%20%20Dynamics%20Prediction%20via%20Transformers%0AAuthor%3A%20Sanket%20Gandhi%20and%20%20Atul%20and%20Samanyu%20Mahajan%20and%20Vishal%20Sharma%20and%20Rushil%20Gupta%20and%20Arnab%20Kumar%20Mondal%20and%20Parag%20Singla%0AAbstract%3A%20%20%20Recent%20work%20has%20shown%20that%20object-centric%20representations%20can%20greatly%20help%0Aimprove%20the%20accuracy%20of%20learning%20dynamics%20while%20also%20bringing%20interpretability.%0AIn%20this%20work%2C%20we%20take%20this%20idea%20one%20step%20further%2C%20ask%20the%20following%20question%3A%0A%22can%20learning%20disentangled%20representation%20further%20improve%20the%20accuracy%20of%0Avisual%20dynamics%20prediction%20in%20object-centric%20models%3F%22%20While%20there%20has%20been%20some%0Aattempt%20to%20learn%20such%20disentangled%20representations%20for%20the%20case%20of%20static%0Aimages%20%5Ccitep%7Bnsb%7D%2C%20to%20the%20best%20of%20our%20knowledge%2C%20ours%20is%20the%20first%20work%20which%0Atries%20to%20do%20this%20in%20a%20general%20setting%20for%20video%2C%20without%20making%20any%20specific%0Aassumptions%20about%20the%20kind%20of%20attributes%20that%20an%20object%20might%20have.%20The%20key%0Abuilding%20block%20of%20our%20architecture%20is%20the%20notion%20of%20a%20%7B%5Cem%20block%7D%2C%20where%0Aseveral%20blocks%20together%20constitute%20an%20object.%20Each%20block%20is%20represented%20as%20a%0Alinear%20combination%20of%20a%20given%20number%20of%20learnable%20concept%20vectors%2C%20which%20is%0Aiteratively%20refined%20during%20the%20learning%20process.%20The%20blocks%20in%20our%20model%20are%0Adiscovered%20in%20an%20unsupervised%20manner%2C%20by%20attending%20over%20object%20masks%2C%20in%20a%0Astyle%20similar%20to%20discovery%20of%20slots%20%5Ccitep%7Bslot_attention%7D%2C%20for%20learning%20a%0Adense%20object-centric%20representation.%20We%20employ%20self-attention%20via%20transformers%0Aover%20the%20discovered%20blocks%20to%20predict%20the%20next%20state%20resulting%20in%20discovery%20of%0Avisual%20dynamics.%20We%20perform%20a%20series%20of%20experiments%20on%20several%20benchmark%202-D%2C%0Aand%203-D%20datasets%20demonstrating%20that%20our%20architecture%20%281%29%20can%20discover%0Asemantically%20meaningful%20blocks%20%282%29%20help%20improve%20accuracy%20of%20dynamics%20prediction%0Acompared%20to%20SOTA%20object-centric%20models%20%283%29%20perform%20significantly%20better%20in%20OOD%0Asetting%20where%20the%20specific%20attribute%20combinations%20are%20not%20seen%20earlier%20during%0Atraining.%20Our%20experiments%20highlight%20the%20importance%20discovery%20of%20disentangled%0Arepresentation%20for%20visual%20dynamics%20prediction.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03216v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Disentangled%2520Representation%2520in%2520Object-Centric%2520Models%2520for%2520Visual%250A%2520%2520Dynamics%2520Prediction%2520via%2520Transformers%26entry.906535625%3DSanket%2520Gandhi%2520and%2520%2520Atul%2520and%2520Samanyu%2520Mahajan%2520and%2520Vishal%2520Sharma%2520and%2520Rushil%2520Gupta%2520and%2520Arnab%2520Kumar%2520Mondal%2520and%2520Parag%2520Singla%26entry.1292438233%3D%2520%2520Recent%2520work%2520has%2520shown%2520that%2520object-centric%2520representations%2520can%2520greatly%2520help%250Aimprove%2520the%2520accuracy%2520of%2520learning%2520dynamics%2520while%2520also%2520bringing%2520interpretability.%250AIn%2520this%2520work%252C%2520we%2520take%2520this%2520idea%2520one%2520step%2520further%252C%2520ask%2520the%2520following%2520question%253A%250A%2522can%2520learning%2520disentangled%2520representation%2520further%2520improve%2520the%2520accuracy%2520of%250Avisual%2520dynamics%2520prediction%2520in%2520object-centric%2520models%253F%2522%2520While%2520there%2520has%2520been%2520some%250Aattempt%2520to%2520learn%2520such%2520disentangled%2520representations%2520for%2520the%2520case%2520of%2520static%250Aimages%2520%255Ccitep%257Bnsb%257D%252C%2520to%2520the%2520best%2520of%2520our%2520knowledge%252C%2520ours%2520is%2520the%2520first%2520work%2520which%250Atries%2520to%2520do%2520this%2520in%2520a%2520general%2520setting%2520for%2520video%252C%2520without%2520making%2520any%2520specific%250Aassumptions%2520about%2520the%2520kind%2520of%2520attributes%2520that%2520an%2520object%2520might%2520have.%2520The%2520key%250Abuilding%2520block%2520of%2520our%2520architecture%2520is%2520the%2520notion%2520of%2520a%2520%257B%255Cem%2520block%257D%252C%2520where%250Aseveral%2520blocks%2520together%2520constitute%2520an%2520object.%2520Each%2520block%2520is%2520represented%2520as%2520a%250Alinear%2520combination%2520of%2520a%2520given%2520number%2520of%2520learnable%2520concept%2520vectors%252C%2520which%2520is%250Aiteratively%2520refined%2520during%2520the%2520learning%2520process.%2520The%2520blocks%2520in%2520our%2520model%2520are%250Adiscovered%2520in%2520an%2520unsupervised%2520manner%252C%2520by%2520attending%2520over%2520object%2520masks%252C%2520in%2520a%250Astyle%2520similar%2520to%2520discovery%2520of%2520slots%2520%255Ccitep%257Bslot_attention%257D%252C%2520for%2520learning%2520a%250Adense%2520object-centric%2520representation.%2520We%2520employ%2520self-attention%2520via%2520transformers%250Aover%2520the%2520discovered%2520blocks%2520to%2520predict%2520the%2520next%2520state%2520resulting%2520in%2520discovery%2520of%250Avisual%2520dynamics.%2520We%2520perform%2520a%2520series%2520of%2520experiments%2520on%2520several%2520benchmark%25202-D%252C%250Aand%25203-D%2520datasets%2520demonstrating%2520that%2520our%2520architecture%2520%25281%2529%2520can%2520discover%250Asemantically%2520meaningful%2520blocks%2520%25282%2529%2520help%2520improve%2520accuracy%2520of%2520dynamics%2520prediction%250Acompared%2520to%2520SOTA%2520object-centric%2520models%2520%25283%2529%2520perform%2520significantly%2520better%2520in%2520OOD%250Asetting%2520where%2520the%2520specific%2520attribute%2520combinations%2520are%2520not%2520seen%2520earlier%2520during%250Atraining.%2520Our%2520experiments%2520highlight%2520the%2520importance%2520discovery%2520of%2520disentangled%250Arepresentation%2520for%2520visual%2520dynamics%2520prediction.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03216v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Disentangled%20Representation%20in%20Object-Centric%20Models%20for%20Visual%0A%20%20Dynamics%20Prediction%20via%20Transformers&entry.906535625=Sanket%20Gandhi%20and%20%20Atul%20and%20Samanyu%20Mahajan%20and%20Vishal%20Sharma%20and%20Rushil%20Gupta%20and%20Arnab%20Kumar%20Mondal%20and%20Parag%20Singla&entry.1292438233=%20%20Recent%20work%20has%20shown%20that%20object-centric%20representations%20can%20greatly%20help%0Aimprove%20the%20accuracy%20of%20learning%20dynamics%20while%20also%20bringing%20interpretability.%0AIn%20this%20work%2C%20we%20take%20this%20idea%20one%20step%20further%2C%20ask%20the%20following%20question%3A%0A%22can%20learning%20disentangled%20representation%20further%20improve%20the%20accuracy%20of%0Avisual%20dynamics%20prediction%20in%20object-centric%20models%3F%22%20While%20there%20has%20been%20some%0Aattempt%20to%20learn%20such%20disentangled%20representations%20for%20the%20case%20of%20static%0Aimages%20%5Ccitep%7Bnsb%7D%2C%20to%20the%20best%20of%20our%20knowledge%2C%20ours%20is%20the%20first%20work%20which%0Atries%20to%20do%20this%20in%20a%20general%20setting%20for%20video%2C%20without%20making%20any%20specific%0Aassumptions%20about%20the%20kind%20of%20attributes%20that%20an%20object%20might%20have.%20The%20key%0Abuilding%20block%20of%20our%20architecture%20is%20the%20notion%20of%20a%20%7B%5Cem%20block%7D%2C%20where%0Aseveral%20blocks%20together%20constitute%20an%20object.%20Each%20block%20is%20represented%20as%20a%0Alinear%20combination%20of%20a%20given%20number%20of%20learnable%20concept%20vectors%2C%20which%20is%0Aiteratively%20refined%20during%20the%20learning%20process.%20The%20blocks%20in%20our%20model%20are%0Adiscovered%20in%20an%20unsupervised%20manner%2C%20by%20attending%20over%20object%20masks%2C%20in%20a%0Astyle%20similar%20to%20discovery%20of%20slots%20%5Ccitep%7Bslot_attention%7D%2C%20for%20learning%20a%0Adense%20object-centric%20representation.%20We%20employ%20self-attention%20via%20transformers%0Aover%20the%20discovered%20blocks%20to%20predict%20the%20next%20state%20resulting%20in%20discovery%20of%0Avisual%20dynamics.%20We%20perform%20a%20series%20of%20experiments%20on%20several%20benchmark%202-D%2C%0Aand%203-D%20datasets%20demonstrating%20that%20our%20architecture%20%281%29%20can%20discover%0Asemantically%20meaningful%20blocks%20%282%29%20help%20improve%20accuracy%20of%20dynamics%20prediction%0Acompared%20to%20SOTA%20object-centric%20models%20%283%29%20perform%20significantly%20better%20in%20OOD%0Asetting%20where%20the%20specific%20attribute%20combinations%20are%20not%20seen%20earlier%20during%0Atraining.%20Our%20experiments%20highlight%20the%20importance%20discovery%20of%20disentangled%0Arepresentation%20for%20visual%20dynamics%20prediction.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03216v1&entry.124074799=Read"},
{"title": "Ultra-Lightweight Collaborative Mapping for Robot Swarms", "author": "Vlad Niculescu and Tommaso Polonelli and Michele Magno and Luca Benini", "abstract": "  A key requirement in robotics is the ability to simultaneously self-localize\nand map a previously unknown environment, relying primarily on onboard sensing\nand computation. Achieving fully onboard accurate simultaneous localization and\nmapping (SLAM) is feasible for high-end robotic platforms, whereas small and\ninexpensive robots face challenges due to constrained hardware, therefore\nfrequently resorting to external infrastructure for sensing and computation.\nThe challenge is further exacerbated in swarms of robots, where coordination,\nscalability, and latency are crucial concerns. This work introduces a\ndecentralized and lightweight collaborative SLAM approach that enables mapping\non virtually any robot, even those equipped with low-cost hardware, including\nminiaturized insect-size devices. Moreover, the proposed solution supports\nlarge swarm formations with the capability to coordinate hundreds of agents. To\nsubstantiate our claims, we have successfully implemented collaborative SLAM on\ncentimeter-size drones weighing only 46 grams. Remarkably, we achieve results\ncomparable to high-end state-of-the-art solutions while reducing the cost,\nmemory, and computation requirements by two orders of magnitude. Our approach\nis innovative in three main aspects. First, it enables onboard\ninfrastructure-less collaborative mapping with a lightweight and cost-effective\nsolution in terms of sensing and computation. Second, we optimize the data\ntraffic within the swarm to support hundreds of cooperative agents using\nstandard wireless protocols such as ultra-wideband (UWB), Bluetooth, or WiFi.\nLast, we implement a distributed swarm coordination policy to decrease mapping\nlatency and enhance accuracy.\n", "link": "http://arxiv.org/abs/2407.03136v1", "date": "2024-07-03", "relevancy": 2.2843, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6111}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.552}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ultra-Lightweight%20Collaborative%20Mapping%20for%20Robot%20Swarms&body=Title%3A%20Ultra-Lightweight%20Collaborative%20Mapping%20for%20Robot%20Swarms%0AAuthor%3A%20Vlad%20Niculescu%20and%20Tommaso%20Polonelli%20and%20Michele%20Magno%20and%20Luca%20Benini%0AAbstract%3A%20%20%20A%20key%20requirement%20in%20robotics%20is%20the%20ability%20to%20simultaneously%20self-localize%0Aand%20map%20a%20previously%20unknown%20environment%2C%20relying%20primarily%20on%20onboard%20sensing%0Aand%20computation.%20Achieving%20fully%20onboard%20accurate%20simultaneous%20localization%20and%0Amapping%20%28SLAM%29%20is%20feasible%20for%20high-end%20robotic%20platforms%2C%20whereas%20small%20and%0Ainexpensive%20robots%20face%20challenges%20due%20to%20constrained%20hardware%2C%20therefore%0Afrequently%20resorting%20to%20external%20infrastructure%20for%20sensing%20and%20computation.%0AThe%20challenge%20is%20further%20exacerbated%20in%20swarms%20of%20robots%2C%20where%20coordination%2C%0Ascalability%2C%20and%20latency%20are%20crucial%20concerns.%20This%20work%20introduces%20a%0Adecentralized%20and%20lightweight%20collaborative%20SLAM%20approach%20that%20enables%20mapping%0Aon%20virtually%20any%20robot%2C%20even%20those%20equipped%20with%20low-cost%20hardware%2C%20including%0Aminiaturized%20insect-size%20devices.%20Moreover%2C%20the%20proposed%20solution%20supports%0Alarge%20swarm%20formations%20with%20the%20capability%20to%20coordinate%20hundreds%20of%20agents.%20To%0Asubstantiate%20our%20claims%2C%20we%20have%20successfully%20implemented%20collaborative%20SLAM%20on%0Acentimeter-size%20drones%20weighing%20only%2046%20grams.%20Remarkably%2C%20we%20achieve%20results%0Acomparable%20to%20high-end%20state-of-the-art%20solutions%20while%20reducing%20the%20cost%2C%0Amemory%2C%20and%20computation%20requirements%20by%20two%20orders%20of%20magnitude.%20Our%20approach%0Ais%20innovative%20in%20three%20main%20aspects.%20First%2C%20it%20enables%20onboard%0Ainfrastructure-less%20collaborative%20mapping%20with%20a%20lightweight%20and%20cost-effective%0Asolution%20in%20terms%20of%20sensing%20and%20computation.%20Second%2C%20we%20optimize%20the%20data%0Atraffic%20within%20the%20swarm%20to%20support%20hundreds%20of%20cooperative%20agents%20using%0Astandard%20wireless%20protocols%20such%20as%20ultra-wideband%20%28UWB%29%2C%20Bluetooth%2C%20or%20WiFi.%0ALast%2C%20we%20implement%20a%20distributed%20swarm%20coordination%20policy%20to%20decrease%20mapping%0Alatency%20and%20enhance%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03136v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUltra-Lightweight%2520Collaborative%2520Mapping%2520for%2520Robot%2520Swarms%26entry.906535625%3DVlad%2520Niculescu%2520and%2520Tommaso%2520Polonelli%2520and%2520Michele%2520Magno%2520and%2520Luca%2520Benini%26entry.1292438233%3D%2520%2520A%2520key%2520requirement%2520in%2520robotics%2520is%2520the%2520ability%2520to%2520simultaneously%2520self-localize%250Aand%2520map%2520a%2520previously%2520unknown%2520environment%252C%2520relying%2520primarily%2520on%2520onboard%2520sensing%250Aand%2520computation.%2520Achieving%2520fully%2520onboard%2520accurate%2520simultaneous%2520localization%2520and%250Amapping%2520%2528SLAM%2529%2520is%2520feasible%2520for%2520high-end%2520robotic%2520platforms%252C%2520whereas%2520small%2520and%250Ainexpensive%2520robots%2520face%2520challenges%2520due%2520to%2520constrained%2520hardware%252C%2520therefore%250Afrequently%2520resorting%2520to%2520external%2520infrastructure%2520for%2520sensing%2520and%2520computation.%250AThe%2520challenge%2520is%2520further%2520exacerbated%2520in%2520swarms%2520of%2520robots%252C%2520where%2520coordination%252C%250Ascalability%252C%2520and%2520latency%2520are%2520crucial%2520concerns.%2520This%2520work%2520introduces%2520a%250Adecentralized%2520and%2520lightweight%2520collaborative%2520SLAM%2520approach%2520that%2520enables%2520mapping%250Aon%2520virtually%2520any%2520robot%252C%2520even%2520those%2520equipped%2520with%2520low-cost%2520hardware%252C%2520including%250Aminiaturized%2520insect-size%2520devices.%2520Moreover%252C%2520the%2520proposed%2520solution%2520supports%250Alarge%2520swarm%2520formations%2520with%2520the%2520capability%2520to%2520coordinate%2520hundreds%2520of%2520agents.%2520To%250Asubstantiate%2520our%2520claims%252C%2520we%2520have%2520successfully%2520implemented%2520collaborative%2520SLAM%2520on%250Acentimeter-size%2520drones%2520weighing%2520only%252046%2520grams.%2520Remarkably%252C%2520we%2520achieve%2520results%250Acomparable%2520to%2520high-end%2520state-of-the-art%2520solutions%2520while%2520reducing%2520the%2520cost%252C%250Amemory%252C%2520and%2520computation%2520requirements%2520by%2520two%2520orders%2520of%2520magnitude.%2520Our%2520approach%250Ais%2520innovative%2520in%2520three%2520main%2520aspects.%2520First%252C%2520it%2520enables%2520onboard%250Ainfrastructure-less%2520collaborative%2520mapping%2520with%2520a%2520lightweight%2520and%2520cost-effective%250Asolution%2520in%2520terms%2520of%2520sensing%2520and%2520computation.%2520Second%252C%2520we%2520optimize%2520the%2520data%250Atraffic%2520within%2520the%2520swarm%2520to%2520support%2520hundreds%2520of%2520cooperative%2520agents%2520using%250Astandard%2520wireless%2520protocols%2520such%2520as%2520ultra-wideband%2520%2528UWB%2529%252C%2520Bluetooth%252C%2520or%2520WiFi.%250ALast%252C%2520we%2520implement%2520a%2520distributed%2520swarm%2520coordination%2520policy%2520to%2520decrease%2520mapping%250Alatency%2520and%2520enhance%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03136v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ultra-Lightweight%20Collaborative%20Mapping%20for%20Robot%20Swarms&entry.906535625=Vlad%20Niculescu%20and%20Tommaso%20Polonelli%20and%20Michele%20Magno%20and%20Luca%20Benini&entry.1292438233=%20%20A%20key%20requirement%20in%20robotics%20is%20the%20ability%20to%20simultaneously%20self-localize%0Aand%20map%20a%20previously%20unknown%20environment%2C%20relying%20primarily%20on%20onboard%20sensing%0Aand%20computation.%20Achieving%20fully%20onboard%20accurate%20simultaneous%20localization%20and%0Amapping%20%28SLAM%29%20is%20feasible%20for%20high-end%20robotic%20platforms%2C%20whereas%20small%20and%0Ainexpensive%20robots%20face%20challenges%20due%20to%20constrained%20hardware%2C%20therefore%0Afrequently%20resorting%20to%20external%20infrastructure%20for%20sensing%20and%20computation.%0AThe%20challenge%20is%20further%20exacerbated%20in%20swarms%20of%20robots%2C%20where%20coordination%2C%0Ascalability%2C%20and%20latency%20are%20crucial%20concerns.%20This%20work%20introduces%20a%0Adecentralized%20and%20lightweight%20collaborative%20SLAM%20approach%20that%20enables%20mapping%0Aon%20virtually%20any%20robot%2C%20even%20those%20equipped%20with%20low-cost%20hardware%2C%20including%0Aminiaturized%20insect-size%20devices.%20Moreover%2C%20the%20proposed%20solution%20supports%0Alarge%20swarm%20formations%20with%20the%20capability%20to%20coordinate%20hundreds%20of%20agents.%20To%0Asubstantiate%20our%20claims%2C%20we%20have%20successfully%20implemented%20collaborative%20SLAM%20on%0Acentimeter-size%20drones%20weighing%20only%2046%20grams.%20Remarkably%2C%20we%20achieve%20results%0Acomparable%20to%20high-end%20state-of-the-art%20solutions%20while%20reducing%20the%20cost%2C%0Amemory%2C%20and%20computation%20requirements%20by%20two%20orders%20of%20magnitude.%20Our%20approach%0Ais%20innovative%20in%20three%20main%20aspects.%20First%2C%20it%20enables%20onboard%0Ainfrastructure-less%20collaborative%20mapping%20with%20a%20lightweight%20and%20cost-effective%0Asolution%20in%20terms%20of%20sensing%20and%20computation.%20Second%2C%20we%20optimize%20the%20data%0Atraffic%20within%20the%20swarm%20to%20support%20hundreds%20of%20cooperative%20agents%20using%0Astandard%20wireless%20protocols%20such%20as%20ultra-wideband%20%28UWB%29%2C%20Bluetooth%2C%20or%20WiFi.%0ALast%2C%20we%20implement%20a%20distributed%20swarm%20coordination%20policy%20to%20decrease%20mapping%0Alatency%20and%20enhance%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03136v1&entry.124074799=Read"},
{"title": "Localization in Dynamic Planar Environments Using Few Distance\n  Measurements", "author": "Michael M. Bilevich and Shahar Guini Menashe and Dan Halperin", "abstract": "  We present a method for determining the unknown location of a sensor placed\nin a known 2D environment in the presence of unknown dynamic obstacles, using\nonly few distance measurements. We present guarantees on the quality of the\nlocalization, which are robust under mild assumptions on the density of the\nunknown/dynamic obstacles in the known environment. We demonstrate the\neffectiveness of our method in simulated experiments for different environments\nand varying dynamic-obstacle density. Our open source software is available at\nhttps://github.com/TAU-CGL/vb-fdml2-public.\n", "link": "http://arxiv.org/abs/2407.03219v1", "date": "2024-07-03", "relevancy": 2.2513, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5784}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5602}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5303}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Localization%20in%20Dynamic%20Planar%20Environments%20Using%20Few%20Distance%0A%20%20Measurements&body=Title%3A%20Localization%20in%20Dynamic%20Planar%20Environments%20Using%20Few%20Distance%0A%20%20Measurements%0AAuthor%3A%20Michael%20M.%20Bilevich%20and%20Shahar%20Guini%20Menashe%20and%20Dan%20Halperin%0AAbstract%3A%20%20%20We%20present%20a%20method%20for%20determining%20the%20unknown%20location%20of%20a%20sensor%20placed%0Ain%20a%20known%202D%20environment%20in%20the%20presence%20of%20unknown%20dynamic%20obstacles%2C%20using%0Aonly%20few%20distance%20measurements.%20We%20present%20guarantees%20on%20the%20quality%20of%20the%0Alocalization%2C%20which%20are%20robust%20under%20mild%20assumptions%20on%20the%20density%20of%20the%0Aunknown/dynamic%20obstacles%20in%20the%20known%20environment.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20in%20simulated%20experiments%20for%20different%20environments%0Aand%20varying%20dynamic-obstacle%20density.%20Our%20open%20source%20software%20is%20available%20at%0Ahttps%3A//github.com/TAU-CGL/vb-fdml2-public.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03219v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLocalization%2520in%2520Dynamic%2520Planar%2520Environments%2520Using%2520Few%2520Distance%250A%2520%2520Measurements%26entry.906535625%3DMichael%2520M.%2520Bilevich%2520and%2520Shahar%2520Guini%2520Menashe%2520and%2520Dan%2520Halperin%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520method%2520for%2520determining%2520the%2520unknown%2520location%2520of%2520a%2520sensor%2520placed%250Ain%2520a%2520known%25202D%2520environment%2520in%2520the%2520presence%2520of%2520unknown%2520dynamic%2520obstacles%252C%2520using%250Aonly%2520few%2520distance%2520measurements.%2520We%2520present%2520guarantees%2520on%2520the%2520quality%2520of%2520the%250Alocalization%252C%2520which%2520are%2520robust%2520under%2520mild%2520assumptions%2520on%2520the%2520density%2520of%2520the%250Aunknown/dynamic%2520obstacles%2520in%2520the%2520known%2520environment.%2520We%2520demonstrate%2520the%250Aeffectiveness%2520of%2520our%2520method%2520in%2520simulated%2520experiments%2520for%2520different%2520environments%250Aand%2520varying%2520dynamic-obstacle%2520density.%2520Our%2520open%2520source%2520software%2520is%2520available%2520at%250Ahttps%253A//github.com/TAU-CGL/vb-fdml2-public.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03219v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Localization%20in%20Dynamic%20Planar%20Environments%20Using%20Few%20Distance%0A%20%20Measurements&entry.906535625=Michael%20M.%20Bilevich%20and%20Shahar%20Guini%20Menashe%20and%20Dan%20Halperin&entry.1292438233=%20%20We%20present%20a%20method%20for%20determining%20the%20unknown%20location%20of%20a%20sensor%20placed%0Ain%20a%20known%202D%20environment%20in%20the%20presence%20of%20unknown%20dynamic%20obstacles%2C%20using%0Aonly%20few%20distance%20measurements.%20We%20present%20guarantees%20on%20the%20quality%20of%20the%0Alocalization%2C%20which%20are%20robust%20under%20mild%20assumptions%20on%20the%20density%20of%20the%0Aunknown/dynamic%20obstacles%20in%20the%20known%20environment.%20We%20demonstrate%20the%0Aeffectiveness%20of%20our%20method%20in%20simulated%20experiments%20for%20different%20environments%0Aand%20varying%20dynamic-obstacle%20density.%20Our%20open%20source%20software%20is%20available%20at%0Ahttps%3A//github.com/TAU-CGL/vb-fdml2-public.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03219v1&entry.124074799=Read"},
{"title": "Graph and Skipped Transformer: Exploiting Spatial and Temporal Modeling\n  Capacities for Efficient 3D Human Pose Estimation", "author": "Mengmeng Cui and Kunbo Zhang and Zhenan Sun", "abstract": "  In recent years, 2D-to-3D pose uplifting in monocular 3D Human Pose\nEstimation (HPE) has attracted widespread research interest. GNN-based methods\nand Transformer-based methods have become mainstream architectures due to their\nadvanced spatial and temporal feature learning capacities. However, existing\napproaches typically construct joint-wise and frame-wise attention alignments\nin spatial and temporal domains, resulting in dense connections that introduce\nconsiderable local redundancy and computational overhead. In this paper, we\ntake a global approach to exploit spatio-temporal information and realise\nefficient 3D HPE with a concise Graph and Skipped Transformer architecture.\nSpecifically, in Spatial Encoding stage, coarse-grained body parts are deployed\nto construct Spatial Graph Network with a fully data-driven adaptive topology,\nensuring model flexibility and generalizability across various poses. In\nTemporal Encoding and Decoding stages, a simple yet effective Skipped\nTransformer is proposed to capture long-range temporal dependencies and\nimplement hierarchical feature aggregation. A straightforward Data Rolling\nstrategy is also developed to introduce dynamic information into 2D pose\nsequence. Extensive experiments are conducted on Human3.6M, MPI-INF-3DHP and\nHuman-Eva benchmarks. G-SFormer series methods achieve superior performances\ncompared with previous state-of-the-arts with only around ten percent of\nparameters and significantly reduced computational complexity. Additionally,\nG-SFormer also exhibits outstanding robustness to inaccuracies in detected 2D\nposes.\n", "link": "http://arxiv.org/abs/2407.02990v1", "date": "2024-07-03", "relevancy": 2.2421, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5643}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5627}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20and%20Skipped%20Transformer%3A%20Exploiting%20Spatial%20and%20Temporal%20Modeling%0A%20%20Capacities%20for%20Efficient%203D%20Human%20Pose%20Estimation&body=Title%3A%20Graph%20and%20Skipped%20Transformer%3A%20Exploiting%20Spatial%20and%20Temporal%20Modeling%0A%20%20Capacities%20for%20Efficient%203D%20Human%20Pose%20Estimation%0AAuthor%3A%20Mengmeng%20Cui%20and%20Kunbo%20Zhang%20and%20Zhenan%20Sun%0AAbstract%3A%20%20%20In%20recent%20years%2C%202D-to-3D%20pose%20uplifting%20in%20monocular%203D%20Human%20Pose%0AEstimation%20%28HPE%29%20has%20attracted%20widespread%20research%20interest.%20GNN-based%20methods%0Aand%20Transformer-based%20methods%20have%20become%20mainstream%20architectures%20due%20to%20their%0Aadvanced%20spatial%20and%20temporal%20feature%20learning%20capacities.%20However%2C%20existing%0Aapproaches%20typically%20construct%20joint-wise%20and%20frame-wise%20attention%20alignments%0Ain%20spatial%20and%20temporal%20domains%2C%20resulting%20in%20dense%20connections%20that%20introduce%0Aconsiderable%20local%20redundancy%20and%20computational%20overhead.%20In%20this%20paper%2C%20we%0Atake%20a%20global%20approach%20to%20exploit%20spatio-temporal%20information%20and%20realise%0Aefficient%203D%20HPE%20with%20a%20concise%20Graph%20and%20Skipped%20Transformer%20architecture.%0ASpecifically%2C%20in%20Spatial%20Encoding%20stage%2C%20coarse-grained%20body%20parts%20are%20deployed%0Ato%20construct%20Spatial%20Graph%20Network%20with%20a%20fully%20data-driven%20adaptive%20topology%2C%0Aensuring%20model%20flexibility%20and%20generalizability%20across%20various%20poses.%20In%0ATemporal%20Encoding%20and%20Decoding%20stages%2C%20a%20simple%20yet%20effective%20Skipped%0ATransformer%20is%20proposed%20to%20capture%20long-range%20temporal%20dependencies%20and%0Aimplement%20hierarchical%20feature%20aggregation.%20A%20straightforward%20Data%20Rolling%0Astrategy%20is%20also%20developed%20to%20introduce%20dynamic%20information%20into%202D%20pose%0Asequence.%20Extensive%20experiments%20are%20conducted%20on%20Human3.6M%2C%20MPI-INF-3DHP%20and%0AHuman-Eva%20benchmarks.%20G-SFormer%20series%20methods%20achieve%20superior%20performances%0Acompared%20with%20previous%20state-of-the-arts%20with%20only%20around%20ten%20percent%20of%0Aparameters%20and%20significantly%20reduced%20computational%20complexity.%20Additionally%2C%0AG-SFormer%20also%20exhibits%20outstanding%20robustness%20to%20inaccuracies%20in%20detected%202D%0Aposes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02990v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520and%2520Skipped%2520Transformer%253A%2520Exploiting%2520Spatial%2520and%2520Temporal%2520Modeling%250A%2520%2520Capacities%2520for%2520Efficient%25203D%2520Human%2520Pose%2520Estimation%26entry.906535625%3DMengmeng%2520Cui%2520and%2520Kunbo%2520Zhang%2520and%2520Zhenan%2520Sun%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%25202D-to-3D%2520pose%2520uplifting%2520in%2520monocular%25203D%2520Human%2520Pose%250AEstimation%2520%2528HPE%2529%2520has%2520attracted%2520widespread%2520research%2520interest.%2520GNN-based%2520methods%250Aand%2520Transformer-based%2520methods%2520have%2520become%2520mainstream%2520architectures%2520due%2520to%2520their%250Aadvanced%2520spatial%2520and%2520temporal%2520feature%2520learning%2520capacities.%2520However%252C%2520existing%250Aapproaches%2520typically%2520construct%2520joint-wise%2520and%2520frame-wise%2520attention%2520alignments%250Ain%2520spatial%2520and%2520temporal%2520domains%252C%2520resulting%2520in%2520dense%2520connections%2520that%2520introduce%250Aconsiderable%2520local%2520redundancy%2520and%2520computational%2520overhead.%2520In%2520this%2520paper%252C%2520we%250Atake%2520a%2520global%2520approach%2520to%2520exploit%2520spatio-temporal%2520information%2520and%2520realise%250Aefficient%25203D%2520HPE%2520with%2520a%2520concise%2520Graph%2520and%2520Skipped%2520Transformer%2520architecture.%250ASpecifically%252C%2520in%2520Spatial%2520Encoding%2520stage%252C%2520coarse-grained%2520body%2520parts%2520are%2520deployed%250Ato%2520construct%2520Spatial%2520Graph%2520Network%2520with%2520a%2520fully%2520data-driven%2520adaptive%2520topology%252C%250Aensuring%2520model%2520flexibility%2520and%2520generalizability%2520across%2520various%2520poses.%2520In%250ATemporal%2520Encoding%2520and%2520Decoding%2520stages%252C%2520a%2520simple%2520yet%2520effective%2520Skipped%250ATransformer%2520is%2520proposed%2520to%2520capture%2520long-range%2520temporal%2520dependencies%2520and%250Aimplement%2520hierarchical%2520feature%2520aggregation.%2520A%2520straightforward%2520Data%2520Rolling%250Astrategy%2520is%2520also%2520developed%2520to%2520introduce%2520dynamic%2520information%2520into%25202D%2520pose%250Asequence.%2520Extensive%2520experiments%2520are%2520conducted%2520on%2520Human3.6M%252C%2520MPI-INF-3DHP%2520and%250AHuman-Eva%2520benchmarks.%2520G-SFormer%2520series%2520methods%2520achieve%2520superior%2520performances%250Acompared%2520with%2520previous%2520state-of-the-arts%2520with%2520only%2520around%2520ten%2520percent%2520of%250Aparameters%2520and%2520significantly%2520reduced%2520computational%2520complexity.%2520Additionally%252C%250AG-SFormer%2520also%2520exhibits%2520outstanding%2520robustness%2520to%2520inaccuracies%2520in%2520detected%25202D%250Aposes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02990v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20and%20Skipped%20Transformer%3A%20Exploiting%20Spatial%20and%20Temporal%20Modeling%0A%20%20Capacities%20for%20Efficient%203D%20Human%20Pose%20Estimation&entry.906535625=Mengmeng%20Cui%20and%20Kunbo%20Zhang%20and%20Zhenan%20Sun&entry.1292438233=%20%20In%20recent%20years%2C%202D-to-3D%20pose%20uplifting%20in%20monocular%203D%20Human%20Pose%0AEstimation%20%28HPE%29%20has%20attracted%20widespread%20research%20interest.%20GNN-based%20methods%0Aand%20Transformer-based%20methods%20have%20become%20mainstream%20architectures%20due%20to%20their%0Aadvanced%20spatial%20and%20temporal%20feature%20learning%20capacities.%20However%2C%20existing%0Aapproaches%20typically%20construct%20joint-wise%20and%20frame-wise%20attention%20alignments%0Ain%20spatial%20and%20temporal%20domains%2C%20resulting%20in%20dense%20connections%20that%20introduce%0Aconsiderable%20local%20redundancy%20and%20computational%20overhead.%20In%20this%20paper%2C%20we%0Atake%20a%20global%20approach%20to%20exploit%20spatio-temporal%20information%20and%20realise%0Aefficient%203D%20HPE%20with%20a%20concise%20Graph%20and%20Skipped%20Transformer%20architecture.%0ASpecifically%2C%20in%20Spatial%20Encoding%20stage%2C%20coarse-grained%20body%20parts%20are%20deployed%0Ato%20construct%20Spatial%20Graph%20Network%20with%20a%20fully%20data-driven%20adaptive%20topology%2C%0Aensuring%20model%20flexibility%20and%20generalizability%20across%20various%20poses.%20In%0ATemporal%20Encoding%20and%20Decoding%20stages%2C%20a%20simple%20yet%20effective%20Skipped%0ATransformer%20is%20proposed%20to%20capture%20long-range%20temporal%20dependencies%20and%0Aimplement%20hierarchical%20feature%20aggregation.%20A%20straightforward%20Data%20Rolling%0Astrategy%20is%20also%20developed%20to%20introduce%20dynamic%20information%20into%202D%20pose%0Asequence.%20Extensive%20experiments%20are%20conducted%20on%20Human3.6M%2C%20MPI-INF-3DHP%20and%0AHuman-Eva%20benchmarks.%20G-SFormer%20series%20methods%20achieve%20superior%20performances%0Acompared%20with%20previous%20state-of-the-arts%20with%20only%20around%20ten%20percent%20of%0Aparameters%20and%20significantly%20reduced%20computational%20complexity.%20Additionally%2C%0AG-SFormer%20also%20exhibits%20outstanding%20robustness%20to%20inaccuracies%20in%20detected%202D%0Aposes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02990v1&entry.124074799=Read"},
{"title": "Motion meets Attention: Video Motion Prompts", "author": "Qixiang Chen and Lei Wang and Piotr Koniusz and Tom Gedeon", "abstract": "  Videos contain rich spatio-temporal information. Traditional methods for\nextracting motion, used in tasks such as action recognition, often rely on\nvisual contents rather than precise motion features. This phenomenon is\nreferred to as 'blind motion extraction' behavior, which proves inefficient in\ncapturing motions of interest due to a lack of motion-guided cues. Recently,\nattention mechanisms have enhanced many computer vision tasks by effectively\nhighlighting salient visual areas. Inspired by this, we propose using a\nmodified Sigmoid function with learnable slope and shift parameters as an\nattention mechanism to activate and modulate motion signals derived from frame\ndifferencing maps. This approach generates a sequence of attention maps that\nenhance the processing of motion-related video content. To ensure temporally\ncontinuity and smoothness of the attention maps, we apply pair-wise temporal\nattention variation regularization to remove unwanted motions (e.g., noise)\nwhile preserving important ones. We then perform Hadamard product between each\npair of attention maps and the original video frames to highlight the evolving\nmotions of interest over time. These highlighted motions, termed video motion\nprompts, are subsequently used as inputs to the model instead of the original\nvideo frames. We formalize this process as a motion prompt layer and\nincorporate the regularization term into the loss function to learn better\nmotion prompts. This layer serves as an adapter between the model and the video\ndata, bridging the gap between traditional 'blind motion extraction' and the\nextraction of relevant motions of interest.\n", "link": "http://arxiv.org/abs/2407.03179v1", "date": "2024-07-03", "relevancy": 2.223, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.66}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5412}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5286}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion%20meets%20Attention%3A%20Video%20Motion%20Prompts&body=Title%3A%20Motion%20meets%20Attention%3A%20Video%20Motion%20Prompts%0AAuthor%3A%20Qixiang%20Chen%20and%20Lei%20Wang%20and%20Piotr%20Koniusz%20and%20Tom%20Gedeon%0AAbstract%3A%20%20%20Videos%20contain%20rich%20spatio-temporal%20information.%20Traditional%20methods%20for%0Aextracting%20motion%2C%20used%20in%20tasks%20such%20as%20action%20recognition%2C%20often%20rely%20on%0Avisual%20contents%20rather%20than%20precise%20motion%20features.%20This%20phenomenon%20is%0Areferred%20to%20as%20%27blind%20motion%20extraction%27%20behavior%2C%20which%20proves%20inefficient%20in%0Acapturing%20motions%20of%20interest%20due%20to%20a%20lack%20of%20motion-guided%20cues.%20Recently%2C%0Aattention%20mechanisms%20have%20enhanced%20many%20computer%20vision%20tasks%20by%20effectively%0Ahighlighting%20salient%20visual%20areas.%20Inspired%20by%20this%2C%20we%20propose%20using%20a%0Amodified%20Sigmoid%20function%20with%20learnable%20slope%20and%20shift%20parameters%20as%20an%0Aattention%20mechanism%20to%20activate%20and%20modulate%20motion%20signals%20derived%20from%20frame%0Adifferencing%20maps.%20This%20approach%20generates%20a%20sequence%20of%20attention%20maps%20that%0Aenhance%20the%20processing%20of%20motion-related%20video%20content.%20To%20ensure%20temporally%0Acontinuity%20and%20smoothness%20of%20the%20attention%20maps%2C%20we%20apply%20pair-wise%20temporal%0Aattention%20variation%20regularization%20to%20remove%20unwanted%20motions%20%28e.g.%2C%20noise%29%0Awhile%20preserving%20important%20ones.%20We%20then%20perform%20Hadamard%20product%20between%20each%0Apair%20of%20attention%20maps%20and%20the%20original%20video%20frames%20to%20highlight%20the%20evolving%0Amotions%20of%20interest%20over%20time.%20These%20highlighted%20motions%2C%20termed%20video%20motion%0Aprompts%2C%20are%20subsequently%20used%20as%20inputs%20to%20the%20model%20instead%20of%20the%20original%0Avideo%20frames.%20We%20formalize%20this%20process%20as%20a%20motion%20prompt%20layer%20and%0Aincorporate%20the%20regularization%20term%20into%20the%20loss%20function%20to%20learn%20better%0Amotion%20prompts.%20This%20layer%20serves%20as%20an%20adapter%20between%20the%20model%20and%20the%20video%0Adata%2C%20bridging%20the%20gap%20between%20traditional%20%27blind%20motion%20extraction%27%20and%20the%0Aextraction%20of%20relevant%20motions%20of%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03179v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion%2520meets%2520Attention%253A%2520Video%2520Motion%2520Prompts%26entry.906535625%3DQixiang%2520Chen%2520and%2520Lei%2520Wang%2520and%2520Piotr%2520Koniusz%2520and%2520Tom%2520Gedeon%26entry.1292438233%3D%2520%2520Videos%2520contain%2520rich%2520spatio-temporal%2520information.%2520Traditional%2520methods%2520for%250Aextracting%2520motion%252C%2520used%2520in%2520tasks%2520such%2520as%2520action%2520recognition%252C%2520often%2520rely%2520on%250Avisual%2520contents%2520rather%2520than%2520precise%2520motion%2520features.%2520This%2520phenomenon%2520is%250Areferred%2520to%2520as%2520%2527blind%2520motion%2520extraction%2527%2520behavior%252C%2520which%2520proves%2520inefficient%2520in%250Acapturing%2520motions%2520of%2520interest%2520due%2520to%2520a%2520lack%2520of%2520motion-guided%2520cues.%2520Recently%252C%250Aattention%2520mechanisms%2520have%2520enhanced%2520many%2520computer%2520vision%2520tasks%2520by%2520effectively%250Ahighlighting%2520salient%2520visual%2520areas.%2520Inspired%2520by%2520this%252C%2520we%2520propose%2520using%2520a%250Amodified%2520Sigmoid%2520function%2520with%2520learnable%2520slope%2520and%2520shift%2520parameters%2520as%2520an%250Aattention%2520mechanism%2520to%2520activate%2520and%2520modulate%2520motion%2520signals%2520derived%2520from%2520frame%250Adifferencing%2520maps.%2520This%2520approach%2520generates%2520a%2520sequence%2520of%2520attention%2520maps%2520that%250Aenhance%2520the%2520processing%2520of%2520motion-related%2520video%2520content.%2520To%2520ensure%2520temporally%250Acontinuity%2520and%2520smoothness%2520of%2520the%2520attention%2520maps%252C%2520we%2520apply%2520pair-wise%2520temporal%250Aattention%2520variation%2520regularization%2520to%2520remove%2520unwanted%2520motions%2520%2528e.g.%252C%2520noise%2529%250Awhile%2520preserving%2520important%2520ones.%2520We%2520then%2520perform%2520Hadamard%2520product%2520between%2520each%250Apair%2520of%2520attention%2520maps%2520and%2520the%2520original%2520video%2520frames%2520to%2520highlight%2520the%2520evolving%250Amotions%2520of%2520interest%2520over%2520time.%2520These%2520highlighted%2520motions%252C%2520termed%2520video%2520motion%250Aprompts%252C%2520are%2520subsequently%2520used%2520as%2520inputs%2520to%2520the%2520model%2520instead%2520of%2520the%2520original%250Avideo%2520frames.%2520We%2520formalize%2520this%2520process%2520as%2520a%2520motion%2520prompt%2520layer%2520and%250Aincorporate%2520the%2520regularization%2520term%2520into%2520the%2520loss%2520function%2520to%2520learn%2520better%250Amotion%2520prompts.%2520This%2520layer%2520serves%2520as%2520an%2520adapter%2520between%2520the%2520model%2520and%2520the%2520video%250Adata%252C%2520bridging%2520the%2520gap%2520between%2520traditional%2520%2527blind%2520motion%2520extraction%2527%2520and%2520the%250Aextraction%2520of%2520relevant%2520motions%2520of%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03179v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion%20meets%20Attention%3A%20Video%20Motion%20Prompts&entry.906535625=Qixiang%20Chen%20and%20Lei%20Wang%20and%20Piotr%20Koniusz%20and%20Tom%20Gedeon&entry.1292438233=%20%20Videos%20contain%20rich%20spatio-temporal%20information.%20Traditional%20methods%20for%0Aextracting%20motion%2C%20used%20in%20tasks%20such%20as%20action%20recognition%2C%20often%20rely%20on%0Avisual%20contents%20rather%20than%20precise%20motion%20features.%20This%20phenomenon%20is%0Areferred%20to%20as%20%27blind%20motion%20extraction%27%20behavior%2C%20which%20proves%20inefficient%20in%0Acapturing%20motions%20of%20interest%20due%20to%20a%20lack%20of%20motion-guided%20cues.%20Recently%2C%0Aattention%20mechanisms%20have%20enhanced%20many%20computer%20vision%20tasks%20by%20effectively%0Ahighlighting%20salient%20visual%20areas.%20Inspired%20by%20this%2C%20we%20propose%20using%20a%0Amodified%20Sigmoid%20function%20with%20learnable%20slope%20and%20shift%20parameters%20as%20an%0Aattention%20mechanism%20to%20activate%20and%20modulate%20motion%20signals%20derived%20from%20frame%0Adifferencing%20maps.%20This%20approach%20generates%20a%20sequence%20of%20attention%20maps%20that%0Aenhance%20the%20processing%20of%20motion-related%20video%20content.%20To%20ensure%20temporally%0Acontinuity%20and%20smoothness%20of%20the%20attention%20maps%2C%20we%20apply%20pair-wise%20temporal%0Aattention%20variation%20regularization%20to%20remove%20unwanted%20motions%20%28e.g.%2C%20noise%29%0Awhile%20preserving%20important%20ones.%20We%20then%20perform%20Hadamard%20product%20between%20each%0Apair%20of%20attention%20maps%20and%20the%20original%20video%20frames%20to%20highlight%20the%20evolving%0Amotions%20of%20interest%20over%20time.%20These%20highlighted%20motions%2C%20termed%20video%20motion%0Aprompts%2C%20are%20subsequently%20used%20as%20inputs%20to%20the%20model%20instead%20of%20the%20original%0Avideo%20frames.%20We%20formalize%20this%20process%20as%20a%20motion%20prompt%20layer%20and%0Aincorporate%20the%20regularization%20term%20into%20the%20loss%20function%20to%20learn%20better%0Amotion%20prompts.%20This%20layer%20serves%20as%20an%20adapter%20between%20the%20model%20and%20the%20video%0Adata%2C%20bridging%20the%20gap%20between%20traditional%20%27blind%20motion%20extraction%27%20and%20the%0Aextraction%20of%20relevant%20motions%20of%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03179v1&entry.124074799=Read"},
{"title": "Context-Aware Video Instance Segmentation", "author": "Seunghun Lee and Jiwan Seo and Kiljoon Han and Minwoo Choi and Sunghoon Im", "abstract": "  In this paper, we introduce the Context-Aware Video Instance Segmentation\n(CAVIS), a novel framework designed to enhance instance association by\nintegrating contextual information adjacent to each object. To efficiently\nextract and leverage this information, we propose the Context-Aware Instance\nTracker (CAIT), which merges contextual data surrounding the instances with the\ncore instance features to improve tracking accuracy. Additionally, we introduce\nthe Prototypical Cross-frame Contrastive (PCC) loss, which ensures consistency\nin object-level features across frames, thereby significantly enhancing\ninstance matching accuracy. CAVIS demonstrates superior performance over\nstate-of-the-art methods on all benchmark datasets in video instance\nsegmentation (VIS) and video panoptic segmentation (VPS). Notably, our method\nexcels on the OVIS dataset, which is known for its particularly challenging\nvideos.\n", "link": "http://arxiv.org/abs/2407.03010v1", "date": "2024-07-03", "relevancy": 2.2167, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5743}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5485}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5363}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Context-Aware%20Video%20Instance%20Segmentation&body=Title%3A%20Context-Aware%20Video%20Instance%20Segmentation%0AAuthor%3A%20Seunghun%20Lee%20and%20Jiwan%20Seo%20and%20Kiljoon%20Han%20and%20Minwoo%20Choi%20and%20Sunghoon%20Im%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20introduce%20the%20Context-Aware%20Video%20Instance%20Segmentation%0A%28CAVIS%29%2C%20a%20novel%20framework%20designed%20to%20enhance%20instance%20association%20by%0Aintegrating%20contextual%20information%20adjacent%20to%20each%20object.%20To%20efficiently%0Aextract%20and%20leverage%20this%20information%2C%20we%20propose%20the%20Context-Aware%20Instance%0ATracker%20%28CAIT%29%2C%20which%20merges%20contextual%20data%20surrounding%20the%20instances%20with%20the%0Acore%20instance%20features%20to%20improve%20tracking%20accuracy.%20Additionally%2C%20we%20introduce%0Athe%20Prototypical%20Cross-frame%20Contrastive%20%28PCC%29%20loss%2C%20which%20ensures%20consistency%0Ain%20object-level%20features%20across%20frames%2C%20thereby%20significantly%20enhancing%0Ainstance%20matching%20accuracy.%20CAVIS%20demonstrates%20superior%20performance%20over%0Astate-of-the-art%20methods%20on%20all%20benchmark%20datasets%20in%20video%20instance%0Asegmentation%20%28VIS%29%20and%20video%20panoptic%20segmentation%20%28VPS%29.%20Notably%2C%20our%20method%0Aexcels%20on%20the%20OVIS%20dataset%2C%20which%20is%20known%20for%20its%20particularly%20challenging%0Avideos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03010v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContext-Aware%2520Video%2520Instance%2520Segmentation%26entry.906535625%3DSeunghun%2520Lee%2520and%2520Jiwan%2520Seo%2520and%2520Kiljoon%2520Han%2520and%2520Minwoo%2520Choi%2520and%2520Sunghoon%2520Im%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520introduce%2520the%2520Context-Aware%2520Video%2520Instance%2520Segmentation%250A%2528CAVIS%2529%252C%2520a%2520novel%2520framework%2520designed%2520to%2520enhance%2520instance%2520association%2520by%250Aintegrating%2520contextual%2520information%2520adjacent%2520to%2520each%2520object.%2520To%2520efficiently%250Aextract%2520and%2520leverage%2520this%2520information%252C%2520we%2520propose%2520the%2520Context-Aware%2520Instance%250ATracker%2520%2528CAIT%2529%252C%2520which%2520merges%2520contextual%2520data%2520surrounding%2520the%2520instances%2520with%2520the%250Acore%2520instance%2520features%2520to%2520improve%2520tracking%2520accuracy.%2520Additionally%252C%2520we%2520introduce%250Athe%2520Prototypical%2520Cross-frame%2520Contrastive%2520%2528PCC%2529%2520loss%252C%2520which%2520ensures%2520consistency%250Ain%2520object-level%2520features%2520across%2520frames%252C%2520thereby%2520significantly%2520enhancing%250Ainstance%2520matching%2520accuracy.%2520CAVIS%2520demonstrates%2520superior%2520performance%2520over%250Astate-of-the-art%2520methods%2520on%2520all%2520benchmark%2520datasets%2520in%2520video%2520instance%250Asegmentation%2520%2528VIS%2529%2520and%2520video%2520panoptic%2520segmentation%2520%2528VPS%2529.%2520Notably%252C%2520our%2520method%250Aexcels%2520on%2520the%2520OVIS%2520dataset%252C%2520which%2520is%2520known%2520for%2520its%2520particularly%2520challenging%250Avideos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03010v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Context-Aware%20Video%20Instance%20Segmentation&entry.906535625=Seunghun%20Lee%20and%20Jiwan%20Seo%20and%20Kiljoon%20Han%20and%20Minwoo%20Choi%20and%20Sunghoon%20Im&entry.1292438233=%20%20In%20this%20paper%2C%20we%20introduce%20the%20Context-Aware%20Video%20Instance%20Segmentation%0A%28CAVIS%29%2C%20a%20novel%20framework%20designed%20to%20enhance%20instance%20association%20by%0Aintegrating%20contextual%20information%20adjacent%20to%20each%20object.%20To%20efficiently%0Aextract%20and%20leverage%20this%20information%2C%20we%20propose%20the%20Context-Aware%20Instance%0ATracker%20%28CAIT%29%2C%20which%20merges%20contextual%20data%20surrounding%20the%20instances%20with%20the%0Acore%20instance%20features%20to%20improve%20tracking%20accuracy.%20Additionally%2C%20we%20introduce%0Athe%20Prototypical%20Cross-frame%20Contrastive%20%28PCC%29%20loss%2C%20which%20ensures%20consistency%0Ain%20object-level%20features%20across%20frames%2C%20thereby%20significantly%20enhancing%0Ainstance%20matching%20accuracy.%20CAVIS%20demonstrates%20superior%20performance%20over%0Astate-of-the-art%20methods%20on%20all%20benchmark%20datasets%20in%20video%20instance%0Asegmentation%20%28VIS%29%20and%20video%20panoptic%20segmentation%20%28VPS%29.%20Notably%2C%20our%20method%0Aexcels%20on%20the%20OVIS%20dataset%2C%20which%20is%20known%20for%20its%20particularly%20challenging%0Avideos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03010v1&entry.124074799=Read"},
{"title": "Back to the Color: Learning Depth to Specific Color Transformation for\n  Unsupervised Depth Estimation", "author": "Yufan Zhu and Chongzhi Ran and Mingtao Feng and Fangfang Wu and Le Dong and Weisheng Dong and Antonio M. L\u00f3pez and Guangming Shi", "abstract": "  Virtual engines can generate dense depth maps for various synthetic scenes,\nmaking them invaluable for training depth estimation models. However,\ndiscrepancies between synthetic and real-world colors pose significant\nchallenges for depth estimation in real-world scenes, especially in complex and\nuncertain environments encountered in unsupervised monocular depth estimation\ntasks. To address this issue, we propose Back2Color, a framework that predicts\nrealistic colors from depth using a model trained on real-world data, thus\ntransforming synthetic colors into their real-world counterparts. Additionally,\nwe introduce the Syn-Real CutMix method for joint training with both real-world\nunsupervised and synthetic supervised depth samples, enhancing monocular depth\nestimation performance in real-world scenes. Furthermore, to mitigate the\nimpact of non-rigid motions on depth estimation, we present an auto-learning\nuncertainty temporal-spatial fusion method (Auto-UTSF), which leverages the\nstrengths of unsupervised learning in both temporal and spatial dimensions. We\nalso designed VADepth, based on the Vision Attention Network, which offers\nlower computational complexity and higher accuracy than transformers. Our\nBack2Color framework achieves state-of-the-art performance on the Kitti\ndataset, as evidenced by improvements in performance metrics and the production\nof fine-grained details. This is particularly evident on more challenging\ndatasets such as Cityscapes for unsupervised depth estimation.\n", "link": "http://arxiv.org/abs/2406.07741v3", "date": "2024-07-03", "relevancy": 2.2111, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5774}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5501}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5456}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Back%20to%20the%20Color%3A%20Learning%20Depth%20to%20Specific%20Color%20Transformation%20for%0A%20%20Unsupervised%20Depth%20Estimation&body=Title%3A%20Back%20to%20the%20Color%3A%20Learning%20Depth%20to%20Specific%20Color%20Transformation%20for%0A%20%20Unsupervised%20Depth%20Estimation%0AAuthor%3A%20Yufan%20Zhu%20and%20Chongzhi%20Ran%20and%20Mingtao%20Feng%20and%20Fangfang%20Wu%20and%20Le%20Dong%20and%20Weisheng%20Dong%20and%20Antonio%20M.%20L%C3%B3pez%20and%20Guangming%20Shi%0AAbstract%3A%20%20%20Virtual%20engines%20can%20generate%20dense%20depth%20maps%20for%20various%20synthetic%20scenes%2C%0Amaking%20them%20invaluable%20for%20training%20depth%20estimation%20models.%20However%2C%0Adiscrepancies%20between%20synthetic%20and%20real-world%20colors%20pose%20significant%0Achallenges%20for%20depth%20estimation%20in%20real-world%20scenes%2C%20especially%20in%20complex%20and%0Auncertain%20environments%20encountered%20in%20unsupervised%20monocular%20depth%20estimation%0Atasks.%20To%20address%20this%20issue%2C%20we%20propose%20Back2Color%2C%20a%20framework%20that%20predicts%0Arealistic%20colors%20from%20depth%20using%20a%20model%20trained%20on%20real-world%20data%2C%20thus%0Atransforming%20synthetic%20colors%20into%20their%20real-world%20counterparts.%20Additionally%2C%0Awe%20introduce%20the%20Syn-Real%20CutMix%20method%20for%20joint%20training%20with%20both%20real-world%0Aunsupervised%20and%20synthetic%20supervised%20depth%20samples%2C%20enhancing%20monocular%20depth%0Aestimation%20performance%20in%20real-world%20scenes.%20Furthermore%2C%20to%20mitigate%20the%0Aimpact%20of%20non-rigid%20motions%20on%20depth%20estimation%2C%20we%20present%20an%20auto-learning%0Auncertainty%20temporal-spatial%20fusion%20method%20%28Auto-UTSF%29%2C%20which%20leverages%20the%0Astrengths%20of%20unsupervised%20learning%20in%20both%20temporal%20and%20spatial%20dimensions.%20We%0Aalso%20designed%20VADepth%2C%20based%20on%20the%20Vision%20Attention%20Network%2C%20which%20offers%0Alower%20computational%20complexity%20and%20higher%20accuracy%20than%20transformers.%20Our%0ABack2Color%20framework%20achieves%20state-of-the-art%20performance%20on%20the%20Kitti%0Adataset%2C%20as%20evidenced%20by%20improvements%20in%20performance%20metrics%20and%20the%20production%0Aof%20fine-grained%20details.%20This%20is%20particularly%20evident%20on%20more%20challenging%0Adatasets%20such%20as%20Cityscapes%20for%20unsupervised%20depth%20estimation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07741v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBack%2520to%2520the%2520Color%253A%2520Learning%2520Depth%2520to%2520Specific%2520Color%2520Transformation%2520for%250A%2520%2520Unsupervised%2520Depth%2520Estimation%26entry.906535625%3DYufan%2520Zhu%2520and%2520Chongzhi%2520Ran%2520and%2520Mingtao%2520Feng%2520and%2520Fangfang%2520Wu%2520and%2520Le%2520Dong%2520and%2520Weisheng%2520Dong%2520and%2520Antonio%2520M.%2520L%25C3%25B3pez%2520and%2520Guangming%2520Shi%26entry.1292438233%3D%2520%2520Virtual%2520engines%2520can%2520generate%2520dense%2520depth%2520maps%2520for%2520various%2520synthetic%2520scenes%252C%250Amaking%2520them%2520invaluable%2520for%2520training%2520depth%2520estimation%2520models.%2520However%252C%250Adiscrepancies%2520between%2520synthetic%2520and%2520real-world%2520colors%2520pose%2520significant%250Achallenges%2520for%2520depth%2520estimation%2520in%2520real-world%2520scenes%252C%2520especially%2520in%2520complex%2520and%250Auncertain%2520environments%2520encountered%2520in%2520unsupervised%2520monocular%2520depth%2520estimation%250Atasks.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520Back2Color%252C%2520a%2520framework%2520that%2520predicts%250Arealistic%2520colors%2520from%2520depth%2520using%2520a%2520model%2520trained%2520on%2520real-world%2520data%252C%2520thus%250Atransforming%2520synthetic%2520colors%2520into%2520their%2520real-world%2520counterparts.%2520Additionally%252C%250Awe%2520introduce%2520the%2520Syn-Real%2520CutMix%2520method%2520for%2520joint%2520training%2520with%2520both%2520real-world%250Aunsupervised%2520and%2520synthetic%2520supervised%2520depth%2520samples%252C%2520enhancing%2520monocular%2520depth%250Aestimation%2520performance%2520in%2520real-world%2520scenes.%2520Furthermore%252C%2520to%2520mitigate%2520the%250Aimpact%2520of%2520non-rigid%2520motions%2520on%2520depth%2520estimation%252C%2520we%2520present%2520an%2520auto-learning%250Auncertainty%2520temporal-spatial%2520fusion%2520method%2520%2528Auto-UTSF%2529%252C%2520which%2520leverages%2520the%250Astrengths%2520of%2520unsupervised%2520learning%2520in%2520both%2520temporal%2520and%2520spatial%2520dimensions.%2520We%250Aalso%2520designed%2520VADepth%252C%2520based%2520on%2520the%2520Vision%2520Attention%2520Network%252C%2520which%2520offers%250Alower%2520computational%2520complexity%2520and%2520higher%2520accuracy%2520than%2520transformers.%2520Our%250ABack2Color%2520framework%2520achieves%2520state-of-the-art%2520performance%2520on%2520the%2520Kitti%250Adataset%252C%2520as%2520evidenced%2520by%2520improvements%2520in%2520performance%2520metrics%2520and%2520the%2520production%250Aof%2520fine-grained%2520details.%2520This%2520is%2520particularly%2520evident%2520on%2520more%2520challenging%250Adatasets%2520such%2520as%2520Cityscapes%2520for%2520unsupervised%2520depth%2520estimation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07741v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Back%20to%20the%20Color%3A%20Learning%20Depth%20to%20Specific%20Color%20Transformation%20for%0A%20%20Unsupervised%20Depth%20Estimation&entry.906535625=Yufan%20Zhu%20and%20Chongzhi%20Ran%20and%20Mingtao%20Feng%20and%20Fangfang%20Wu%20and%20Le%20Dong%20and%20Weisheng%20Dong%20and%20Antonio%20M.%20L%C3%B3pez%20and%20Guangming%20Shi&entry.1292438233=%20%20Virtual%20engines%20can%20generate%20dense%20depth%20maps%20for%20various%20synthetic%20scenes%2C%0Amaking%20them%20invaluable%20for%20training%20depth%20estimation%20models.%20However%2C%0Adiscrepancies%20between%20synthetic%20and%20real-world%20colors%20pose%20significant%0Achallenges%20for%20depth%20estimation%20in%20real-world%20scenes%2C%20especially%20in%20complex%20and%0Auncertain%20environments%20encountered%20in%20unsupervised%20monocular%20depth%20estimation%0Atasks.%20To%20address%20this%20issue%2C%20we%20propose%20Back2Color%2C%20a%20framework%20that%20predicts%0Arealistic%20colors%20from%20depth%20using%20a%20model%20trained%20on%20real-world%20data%2C%20thus%0Atransforming%20synthetic%20colors%20into%20their%20real-world%20counterparts.%20Additionally%2C%0Awe%20introduce%20the%20Syn-Real%20CutMix%20method%20for%20joint%20training%20with%20both%20real-world%0Aunsupervised%20and%20synthetic%20supervised%20depth%20samples%2C%20enhancing%20monocular%20depth%0Aestimation%20performance%20in%20real-world%20scenes.%20Furthermore%2C%20to%20mitigate%20the%0Aimpact%20of%20non-rigid%20motions%20on%20depth%20estimation%2C%20we%20present%20an%20auto-learning%0Auncertainty%20temporal-spatial%20fusion%20method%20%28Auto-UTSF%29%2C%20which%20leverages%20the%0Astrengths%20of%20unsupervised%20learning%20in%20both%20temporal%20and%20spatial%20dimensions.%20We%0Aalso%20designed%20VADepth%2C%20based%20on%20the%20Vision%20Attention%20Network%2C%20which%20offers%0Alower%20computational%20complexity%20and%20higher%20accuracy%20than%20transformers.%20Our%0ABack2Color%20framework%20achieves%20state-of-the-art%20performance%20on%20the%20Kitti%0Adataset%2C%20as%20evidenced%20by%20improvements%20in%20performance%20metrics%20and%20the%20production%0Aof%20fine-grained%20details.%20This%20is%20particularly%20evident%20on%20more%20challenging%0Adatasets%20such%20as%20Cityscapes%20for%20unsupervised%20depth%20estimation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07741v3&entry.124074799=Read"},
{"title": "Visual Grounding with Attention-Driven Constraint Balancing", "author": "Weitai Kang and Luowei Zhou and Junyi Wu and Changchang Sun and Yan Yan", "abstract": "  Unlike Object Detection, Visual Grounding task necessitates the detection of\nan object described by complex free-form language. To simultaneously model such\ncomplex semantic and visual representations, recent state-of-the-art studies\nadopt transformer-based models to fuse features from both modalities, further\nintroducing various modules that modulate visual features to align with the\nlanguage expressions and eliminate the irrelevant redundant information.\nHowever, their loss function, still adopting common Object Detection losses,\nsolely governs the bounding box regression output, failing to fully optimize\nfor the above objectives. To tackle this problem, in this paper, we first\nanalyze the attention mechanisms of transformer-based models. Building upon\nthis, we further propose a novel framework named Attention-Driven Constraint\nBalancing (AttBalance) to optimize the behavior of visual features within\nlanguage-relevant regions. Extensive experimental results show that our method\nbrings impressive improvements. Specifically, we achieve constant improvements\nover five different models evaluated on four different benchmarks. Moreover, we\nattain a new state-of-the-art performance by integrating our method into QRNet.\n", "link": "http://arxiv.org/abs/2407.03243v1", "date": "2024-07-03", "relevancy": 2.1945, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5876}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5457}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visual%20Grounding%20with%20Attention-Driven%20Constraint%20Balancing&body=Title%3A%20Visual%20Grounding%20with%20Attention-Driven%20Constraint%20Balancing%0AAuthor%3A%20Weitai%20Kang%20and%20Luowei%20Zhou%20and%20Junyi%20Wu%20and%20Changchang%20Sun%20and%20Yan%20Yan%0AAbstract%3A%20%20%20Unlike%20Object%20Detection%2C%20Visual%20Grounding%20task%20necessitates%20the%20detection%20of%0Aan%20object%20described%20by%20complex%20free-form%20language.%20To%20simultaneously%20model%20such%0Acomplex%20semantic%20and%20visual%20representations%2C%20recent%20state-of-the-art%20studies%0Aadopt%20transformer-based%20models%20to%20fuse%20features%20from%20both%20modalities%2C%20further%0Aintroducing%20various%20modules%20that%20modulate%20visual%20features%20to%20align%20with%20the%0Alanguage%20expressions%20and%20eliminate%20the%20irrelevant%20redundant%20information.%0AHowever%2C%20their%20loss%20function%2C%20still%20adopting%20common%20Object%20Detection%20losses%2C%0Asolely%20governs%20the%20bounding%20box%20regression%20output%2C%20failing%20to%20fully%20optimize%0Afor%20the%20above%20objectives.%20To%20tackle%20this%20problem%2C%20in%20this%20paper%2C%20we%20first%0Aanalyze%20the%20attention%20mechanisms%20of%20transformer-based%20models.%20Building%20upon%0Athis%2C%20we%20further%20propose%20a%20novel%20framework%20named%20Attention-Driven%20Constraint%0ABalancing%20%28AttBalance%29%20to%20optimize%20the%20behavior%20of%20visual%20features%20within%0Alanguage-relevant%20regions.%20Extensive%20experimental%20results%20show%20that%20our%20method%0Abrings%20impressive%20improvements.%20Specifically%2C%20we%20achieve%20constant%20improvements%0Aover%20five%20different%20models%20evaluated%20on%20four%20different%20benchmarks.%20Moreover%2C%20we%0Aattain%20a%20new%20state-of-the-art%20performance%20by%20integrating%20our%20method%20into%20QRNet.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03243v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisual%2520Grounding%2520with%2520Attention-Driven%2520Constraint%2520Balancing%26entry.906535625%3DWeitai%2520Kang%2520and%2520Luowei%2520Zhou%2520and%2520Junyi%2520Wu%2520and%2520Changchang%2520Sun%2520and%2520Yan%2520Yan%26entry.1292438233%3D%2520%2520Unlike%2520Object%2520Detection%252C%2520Visual%2520Grounding%2520task%2520necessitates%2520the%2520detection%2520of%250Aan%2520object%2520described%2520by%2520complex%2520free-form%2520language.%2520To%2520simultaneously%2520model%2520such%250Acomplex%2520semantic%2520and%2520visual%2520representations%252C%2520recent%2520state-of-the-art%2520studies%250Aadopt%2520transformer-based%2520models%2520to%2520fuse%2520features%2520from%2520both%2520modalities%252C%2520further%250Aintroducing%2520various%2520modules%2520that%2520modulate%2520visual%2520features%2520to%2520align%2520with%2520the%250Alanguage%2520expressions%2520and%2520eliminate%2520the%2520irrelevant%2520redundant%2520information.%250AHowever%252C%2520their%2520loss%2520function%252C%2520still%2520adopting%2520common%2520Object%2520Detection%2520losses%252C%250Asolely%2520governs%2520the%2520bounding%2520box%2520regression%2520output%252C%2520failing%2520to%2520fully%2520optimize%250Afor%2520the%2520above%2520objectives.%2520To%2520tackle%2520this%2520problem%252C%2520in%2520this%2520paper%252C%2520we%2520first%250Aanalyze%2520the%2520attention%2520mechanisms%2520of%2520transformer-based%2520models.%2520Building%2520upon%250Athis%252C%2520we%2520further%2520propose%2520a%2520novel%2520framework%2520named%2520Attention-Driven%2520Constraint%250ABalancing%2520%2528AttBalance%2529%2520to%2520optimize%2520the%2520behavior%2520of%2520visual%2520features%2520within%250Alanguage-relevant%2520regions.%2520Extensive%2520experimental%2520results%2520show%2520that%2520our%2520method%250Abrings%2520impressive%2520improvements.%2520Specifically%252C%2520we%2520achieve%2520constant%2520improvements%250Aover%2520five%2520different%2520models%2520evaluated%2520on%2520four%2520different%2520benchmarks.%2520Moreover%252C%2520we%250Aattain%2520a%2520new%2520state-of-the-art%2520performance%2520by%2520integrating%2520our%2520method%2520into%2520QRNet.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03243v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visual%20Grounding%20with%20Attention-Driven%20Constraint%20Balancing&entry.906535625=Weitai%20Kang%20and%20Luowei%20Zhou%20and%20Junyi%20Wu%20and%20Changchang%20Sun%20and%20Yan%20Yan&entry.1292438233=%20%20Unlike%20Object%20Detection%2C%20Visual%20Grounding%20task%20necessitates%20the%20detection%20of%0Aan%20object%20described%20by%20complex%20free-form%20language.%20To%20simultaneously%20model%20such%0Acomplex%20semantic%20and%20visual%20representations%2C%20recent%20state-of-the-art%20studies%0Aadopt%20transformer-based%20models%20to%20fuse%20features%20from%20both%20modalities%2C%20further%0Aintroducing%20various%20modules%20that%20modulate%20visual%20features%20to%20align%20with%20the%0Alanguage%20expressions%20and%20eliminate%20the%20irrelevant%20redundant%20information.%0AHowever%2C%20their%20loss%20function%2C%20still%20adopting%20common%20Object%20Detection%20losses%2C%0Asolely%20governs%20the%20bounding%20box%20regression%20output%2C%20failing%20to%20fully%20optimize%0Afor%20the%20above%20objectives.%20To%20tackle%20this%20problem%2C%20in%20this%20paper%2C%20we%20first%0Aanalyze%20the%20attention%20mechanisms%20of%20transformer-based%20models.%20Building%20upon%0Athis%2C%20we%20further%20propose%20a%20novel%20framework%20named%20Attention-Driven%20Constraint%0ABalancing%20%28AttBalance%29%20to%20optimize%20the%20behavior%20of%20visual%20features%20within%0Alanguage-relevant%20regions.%20Extensive%20experimental%20results%20show%20that%20our%20method%0Abrings%20impressive%20improvements.%20Specifically%2C%20we%20achieve%20constant%20improvements%0Aover%20five%20different%20models%20evaluated%20on%20four%20different%20benchmarks.%20Moreover%2C%20we%0Aattain%20a%20new%20state-of-the-art%20performance%20by%20integrating%20our%20method%20into%20QRNet.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03243v1&entry.124074799=Read"},
{"title": "SegVG: Transferring Object Bounding Box to Segmentation for Visual\n  Grounding", "author": "Weitai Kang and Gaowen Liu and Mubarak Shah and Yan Yan", "abstract": "  Different from Object Detection, Visual Grounding deals with detecting a\nbounding box for each text-image pair. This one box for each text-image data\nprovides sparse supervision signals. Although previous works achieve impressive\nresults, their passive utilization of annotation, i.e. the sole use of the box\nannotation as regression ground truth, results in a suboptimal performance. In\nthis paper, we present SegVG, a novel method transfers the box-level annotation\nas Segmentation signals to provide an additional pixel-level supervision for\nVisual Grounding. Specifically, we propose the Multi-layer Multi-task\nEncoder-Decoder as the target grounding stage, where we learn a regression\nquery and multiple segmentation queries to ground the target by regression and\nsegmentation of the box in each decoding layer, respectively. This approach\nallows us to iteratively exploit the annotation as signals for both box-level\nregression and pixel-level segmentation. Moreover, as the backbones are\ntypically initialized by pretrained parameters learned from unimodal tasks and\nthe queries for both regression and segmentation are static learnable\nembeddings, a domain discrepancy remains among these three types of features,\nwhich impairs subsequent target grounding. To mitigate this discrepancy, we\nintroduce the Triple Alignment module, where the query, text, and vision tokens\nare triangularly updated to share the same space by triple attention mechanism.\nExtensive experiments on five widely used datasets validate our\nstate-of-the-art (SOTA) performance.\n", "link": "http://arxiv.org/abs/2407.03200v1", "date": "2024-07-03", "relevancy": 2.1493, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5536}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5259}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5252}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegVG%3A%20Transferring%20Object%20Bounding%20Box%20to%20Segmentation%20for%20Visual%0A%20%20Grounding&body=Title%3A%20SegVG%3A%20Transferring%20Object%20Bounding%20Box%20to%20Segmentation%20for%20Visual%0A%20%20Grounding%0AAuthor%3A%20Weitai%20Kang%20and%20Gaowen%20Liu%20and%20Mubarak%20Shah%20and%20Yan%20Yan%0AAbstract%3A%20%20%20Different%20from%20Object%20Detection%2C%20Visual%20Grounding%20deals%20with%20detecting%20a%0Abounding%20box%20for%20each%20text-image%20pair.%20This%20one%20box%20for%20each%20text-image%20data%0Aprovides%20sparse%20supervision%20signals.%20Although%20previous%20works%20achieve%20impressive%0Aresults%2C%20their%20passive%20utilization%20of%20annotation%2C%20i.e.%20the%20sole%20use%20of%20the%20box%0Aannotation%20as%20regression%20ground%20truth%2C%20results%20in%20a%20suboptimal%20performance.%20In%0Athis%20paper%2C%20we%20present%20SegVG%2C%20a%20novel%20method%20transfers%20the%20box-level%20annotation%0Aas%20Segmentation%20signals%20to%20provide%20an%20additional%20pixel-level%20supervision%20for%0AVisual%20Grounding.%20Specifically%2C%20we%20propose%20the%20Multi-layer%20Multi-task%0AEncoder-Decoder%20as%20the%20target%20grounding%20stage%2C%20where%20we%20learn%20a%20regression%0Aquery%20and%20multiple%20segmentation%20queries%20to%20ground%20the%20target%20by%20regression%20and%0Asegmentation%20of%20the%20box%20in%20each%20decoding%20layer%2C%20respectively.%20This%20approach%0Aallows%20us%20to%20iteratively%20exploit%20the%20annotation%20as%20signals%20for%20both%20box-level%0Aregression%20and%20pixel-level%20segmentation.%20Moreover%2C%20as%20the%20backbones%20are%0Atypically%20initialized%20by%20pretrained%20parameters%20learned%20from%20unimodal%20tasks%20and%0Athe%20queries%20for%20both%20regression%20and%20segmentation%20are%20static%20learnable%0Aembeddings%2C%20a%20domain%20discrepancy%20remains%20among%20these%20three%20types%20of%20features%2C%0Awhich%20impairs%20subsequent%20target%20grounding.%20To%20mitigate%20this%20discrepancy%2C%20we%0Aintroduce%20the%20Triple%20Alignment%20module%2C%20where%20the%20query%2C%20text%2C%20and%20vision%20tokens%0Aare%20triangularly%20updated%20to%20share%20the%20same%20space%20by%20triple%20attention%20mechanism.%0AExtensive%20experiments%20on%20five%20widely%20used%20datasets%20validate%20our%0Astate-of-the-art%20%28SOTA%29%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03200v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegVG%253A%2520Transferring%2520Object%2520Bounding%2520Box%2520to%2520Segmentation%2520for%2520Visual%250A%2520%2520Grounding%26entry.906535625%3DWeitai%2520Kang%2520and%2520Gaowen%2520Liu%2520and%2520Mubarak%2520Shah%2520and%2520Yan%2520Yan%26entry.1292438233%3D%2520%2520Different%2520from%2520Object%2520Detection%252C%2520Visual%2520Grounding%2520deals%2520with%2520detecting%2520a%250Abounding%2520box%2520for%2520each%2520text-image%2520pair.%2520This%2520one%2520box%2520for%2520each%2520text-image%2520data%250Aprovides%2520sparse%2520supervision%2520signals.%2520Although%2520previous%2520works%2520achieve%2520impressive%250Aresults%252C%2520their%2520passive%2520utilization%2520of%2520annotation%252C%2520i.e.%2520the%2520sole%2520use%2520of%2520the%2520box%250Aannotation%2520as%2520regression%2520ground%2520truth%252C%2520results%2520in%2520a%2520suboptimal%2520performance.%2520In%250Athis%2520paper%252C%2520we%2520present%2520SegVG%252C%2520a%2520novel%2520method%2520transfers%2520the%2520box-level%2520annotation%250Aas%2520Segmentation%2520signals%2520to%2520provide%2520an%2520additional%2520pixel-level%2520supervision%2520for%250AVisual%2520Grounding.%2520Specifically%252C%2520we%2520propose%2520the%2520Multi-layer%2520Multi-task%250AEncoder-Decoder%2520as%2520the%2520target%2520grounding%2520stage%252C%2520where%2520we%2520learn%2520a%2520regression%250Aquery%2520and%2520multiple%2520segmentation%2520queries%2520to%2520ground%2520the%2520target%2520by%2520regression%2520and%250Asegmentation%2520of%2520the%2520box%2520in%2520each%2520decoding%2520layer%252C%2520respectively.%2520This%2520approach%250Aallows%2520us%2520to%2520iteratively%2520exploit%2520the%2520annotation%2520as%2520signals%2520for%2520both%2520box-level%250Aregression%2520and%2520pixel-level%2520segmentation.%2520Moreover%252C%2520as%2520the%2520backbones%2520are%250Atypically%2520initialized%2520by%2520pretrained%2520parameters%2520learned%2520from%2520unimodal%2520tasks%2520and%250Athe%2520queries%2520for%2520both%2520regression%2520and%2520segmentation%2520are%2520static%2520learnable%250Aembeddings%252C%2520a%2520domain%2520discrepancy%2520remains%2520among%2520these%2520three%2520types%2520of%2520features%252C%250Awhich%2520impairs%2520subsequent%2520target%2520grounding.%2520To%2520mitigate%2520this%2520discrepancy%252C%2520we%250Aintroduce%2520the%2520Triple%2520Alignment%2520module%252C%2520where%2520the%2520query%252C%2520text%252C%2520and%2520vision%2520tokens%250Aare%2520triangularly%2520updated%2520to%2520share%2520the%2520same%2520space%2520by%2520triple%2520attention%2520mechanism.%250AExtensive%2520experiments%2520on%2520five%2520widely%2520used%2520datasets%2520validate%2520our%250Astate-of-the-art%2520%2528SOTA%2529%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03200v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegVG%3A%20Transferring%20Object%20Bounding%20Box%20to%20Segmentation%20for%20Visual%0A%20%20Grounding&entry.906535625=Weitai%20Kang%20and%20Gaowen%20Liu%20and%20Mubarak%20Shah%20and%20Yan%20Yan&entry.1292438233=%20%20Different%20from%20Object%20Detection%2C%20Visual%20Grounding%20deals%20with%20detecting%20a%0Abounding%20box%20for%20each%20text-image%20pair.%20This%20one%20box%20for%20each%20text-image%20data%0Aprovides%20sparse%20supervision%20signals.%20Although%20previous%20works%20achieve%20impressive%0Aresults%2C%20their%20passive%20utilization%20of%20annotation%2C%20i.e.%20the%20sole%20use%20of%20the%20box%0Aannotation%20as%20regression%20ground%20truth%2C%20results%20in%20a%20suboptimal%20performance.%20In%0Athis%20paper%2C%20we%20present%20SegVG%2C%20a%20novel%20method%20transfers%20the%20box-level%20annotation%0Aas%20Segmentation%20signals%20to%20provide%20an%20additional%20pixel-level%20supervision%20for%0AVisual%20Grounding.%20Specifically%2C%20we%20propose%20the%20Multi-layer%20Multi-task%0AEncoder-Decoder%20as%20the%20target%20grounding%20stage%2C%20where%20we%20learn%20a%20regression%0Aquery%20and%20multiple%20segmentation%20queries%20to%20ground%20the%20target%20by%20regression%20and%0Asegmentation%20of%20the%20box%20in%20each%20decoding%20layer%2C%20respectively.%20This%20approach%0Aallows%20us%20to%20iteratively%20exploit%20the%20annotation%20as%20signals%20for%20both%20box-level%0Aregression%20and%20pixel-level%20segmentation.%20Moreover%2C%20as%20the%20backbones%20are%0Atypically%20initialized%20by%20pretrained%20parameters%20learned%20from%20unimodal%20tasks%20and%0Athe%20queries%20for%20both%20regression%20and%20segmentation%20are%20static%20learnable%0Aembeddings%2C%20a%20domain%20discrepancy%20remains%20among%20these%20three%20types%20of%20features%2C%0Awhich%20impairs%20subsequent%20target%20grounding.%20To%20mitigate%20this%20discrepancy%2C%20we%0Aintroduce%20the%20Triple%20Alignment%20module%2C%20where%20the%20query%2C%20text%2C%20and%20vision%20tokens%0Aare%20triangularly%20updated%20to%20share%20the%20same%20space%20by%20triple%20attention%20mechanism.%0AExtensive%20experiments%20on%20five%20widely%20used%20datasets%20validate%20our%0Astate-of-the-art%20%28SOTA%29%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03200v1&entry.124074799=Read"},
{"title": "On Generalization for Generative Flow Networks", "author": "Anas Krichel and Nikolay Malkin and Salem Lahlou and Yoshua Bengio", "abstract": "  Generative Flow Networks (GFlowNets) have emerged as an innovative learning\nparadigm designed to address the challenge of sampling from an unnormalized\nprobability distribution, called the reward function. This framework learns a\npolicy on a constructed graph, which enables sampling from an approximation of\nthe target probability distribution through successive steps of sampling from\nthe learned policy. To achieve this, GFlowNets can be trained with various\nobjectives, each of which can lead to the model s ultimate goal. The\naspirational strength of GFlowNets lies in their potential to discern intricate\npatterns within the reward function and their capacity to generalize\neffectively to novel, unseen parts of the reward function. This paper attempts\nto formalize generalization in the context of GFlowNets, to link generalization\nwith stability, and also to design experiments that assess the capacity of\nthese models to uncover unseen parts of the reward function. The experiments\nwill focus on length generalization meaning generalization to states that can\nbe constructed only by longer trajectories than those seen in training.\n", "link": "http://arxiv.org/abs/2407.03105v1", "date": "2024-07-03", "relevancy": 2.1271, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.584}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5307}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20Generalization%20for%20Generative%20Flow%20Networks&body=Title%3A%20On%20Generalization%20for%20Generative%20Flow%20Networks%0AAuthor%3A%20Anas%20Krichel%20and%20Nikolay%20Malkin%20and%20Salem%20Lahlou%20and%20Yoshua%20Bengio%0AAbstract%3A%20%20%20Generative%20Flow%20Networks%20%28GFlowNets%29%20have%20emerged%20as%20an%20innovative%20learning%0Aparadigm%20designed%20to%20address%20the%20challenge%20of%20sampling%20from%20an%20unnormalized%0Aprobability%20distribution%2C%20called%20the%20reward%20function.%20This%20framework%20learns%20a%0Apolicy%20on%20a%20constructed%20graph%2C%20which%20enables%20sampling%20from%20an%20approximation%20of%0Athe%20target%20probability%20distribution%20through%20successive%20steps%20of%20sampling%20from%0Athe%20learned%20policy.%20To%20achieve%20this%2C%20GFlowNets%20can%20be%20trained%20with%20various%0Aobjectives%2C%20each%20of%20which%20can%20lead%20to%20the%20model%20s%20ultimate%20goal.%20The%0Aaspirational%20strength%20of%20GFlowNets%20lies%20in%20their%20potential%20to%20discern%20intricate%0Apatterns%20within%20the%20reward%20function%20and%20their%20capacity%20to%20generalize%0Aeffectively%20to%20novel%2C%20unseen%20parts%20of%20the%20reward%20function.%20This%20paper%20attempts%0Ato%20formalize%20generalization%20in%20the%20context%20of%20GFlowNets%2C%20to%20link%20generalization%0Awith%20stability%2C%20and%20also%20to%20design%20experiments%20that%20assess%20the%20capacity%20of%0Athese%20models%20to%20uncover%20unseen%20parts%20of%20the%20reward%20function.%20The%20experiments%0Awill%20focus%20on%20length%20generalization%20meaning%20generalization%20to%20states%20that%20can%0Abe%20constructed%20only%20by%20longer%20trajectories%20than%20those%20seen%20in%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03105v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520Generalization%2520for%2520Generative%2520Flow%2520Networks%26entry.906535625%3DAnas%2520Krichel%2520and%2520Nikolay%2520Malkin%2520and%2520Salem%2520Lahlou%2520and%2520Yoshua%2520Bengio%26entry.1292438233%3D%2520%2520Generative%2520Flow%2520Networks%2520%2528GFlowNets%2529%2520have%2520emerged%2520as%2520an%2520innovative%2520learning%250Aparadigm%2520designed%2520to%2520address%2520the%2520challenge%2520of%2520sampling%2520from%2520an%2520unnormalized%250Aprobability%2520distribution%252C%2520called%2520the%2520reward%2520function.%2520This%2520framework%2520learns%2520a%250Apolicy%2520on%2520a%2520constructed%2520graph%252C%2520which%2520enables%2520sampling%2520from%2520an%2520approximation%2520of%250Athe%2520target%2520probability%2520distribution%2520through%2520successive%2520steps%2520of%2520sampling%2520from%250Athe%2520learned%2520policy.%2520To%2520achieve%2520this%252C%2520GFlowNets%2520can%2520be%2520trained%2520with%2520various%250Aobjectives%252C%2520each%2520of%2520which%2520can%2520lead%2520to%2520the%2520model%2520s%2520ultimate%2520goal.%2520The%250Aaspirational%2520strength%2520of%2520GFlowNets%2520lies%2520in%2520their%2520potential%2520to%2520discern%2520intricate%250Apatterns%2520within%2520the%2520reward%2520function%2520and%2520their%2520capacity%2520to%2520generalize%250Aeffectively%2520to%2520novel%252C%2520unseen%2520parts%2520of%2520the%2520reward%2520function.%2520This%2520paper%2520attempts%250Ato%2520formalize%2520generalization%2520in%2520the%2520context%2520of%2520GFlowNets%252C%2520to%2520link%2520generalization%250Awith%2520stability%252C%2520and%2520also%2520to%2520design%2520experiments%2520that%2520assess%2520the%2520capacity%2520of%250Athese%2520models%2520to%2520uncover%2520unseen%2520parts%2520of%2520the%2520reward%2520function.%2520The%2520experiments%250Awill%2520focus%2520on%2520length%2520generalization%2520meaning%2520generalization%2520to%2520states%2520that%2520can%250Abe%2520constructed%2520only%2520by%2520longer%2520trajectories%2520than%2520those%2520seen%2520in%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03105v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20Generalization%20for%20Generative%20Flow%20Networks&entry.906535625=Anas%20Krichel%20and%20Nikolay%20Malkin%20and%20Salem%20Lahlou%20and%20Yoshua%20Bengio&entry.1292438233=%20%20Generative%20Flow%20Networks%20%28GFlowNets%29%20have%20emerged%20as%20an%20innovative%20learning%0Aparadigm%20designed%20to%20address%20the%20challenge%20of%20sampling%20from%20an%20unnormalized%0Aprobability%20distribution%2C%20called%20the%20reward%20function.%20This%20framework%20learns%20a%0Apolicy%20on%20a%20constructed%20graph%2C%20which%20enables%20sampling%20from%20an%20approximation%20of%0Athe%20target%20probability%20distribution%20through%20successive%20steps%20of%20sampling%20from%0Athe%20learned%20policy.%20To%20achieve%20this%2C%20GFlowNets%20can%20be%20trained%20with%20various%0Aobjectives%2C%20each%20of%20which%20can%20lead%20to%20the%20model%20s%20ultimate%20goal.%20The%0Aaspirational%20strength%20of%20GFlowNets%20lies%20in%20their%20potential%20to%20discern%20intricate%0Apatterns%20within%20the%20reward%20function%20and%20their%20capacity%20to%20generalize%0Aeffectively%20to%20novel%2C%20unseen%20parts%20of%20the%20reward%20function.%20This%20paper%20attempts%0Ato%20formalize%20generalization%20in%20the%20context%20of%20GFlowNets%2C%20to%20link%20generalization%0Awith%20stability%2C%20and%20also%20to%20design%20experiments%20that%20assess%20the%20capacity%20of%0Athese%20models%20to%20uncover%20unseen%20parts%20of%20the%20reward%20function.%20The%20experiments%0Awill%20focus%20on%20length%20generalization%20meaning%20generalization%20to%20states%20that%20can%0Abe%20constructed%20only%20by%20longer%20trajectories%20than%20those%20seen%20in%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03105v1&entry.124074799=Read"},
{"title": "VCHAR:Variance-Driven Complex Human Activity Recognition framework with\n  Generative Representation", "author": "Yuan Sun and Navid Salami Pargoo and Taqiya Ehsan and Zhao Zhang Jorge Ortiz", "abstract": "  Complex human activity recognition (CHAR) remains a pivotal challenge within\nubiquitous computing, especially in the context of smart environments. Existing\nstudies typically require meticulous labeling of both atomic and complex\nactivities, a task that is labor-intensive and prone to errors due to the\nscarcity and inaccuracies of available datasets. Most prior research has\nfocused on datasets that either precisely label atomic activities or, at\nminimum, their sequence approaches that are often impractical in real world\nsettings.In response, we introduce VCHAR (Variance-Driven Complex Human\nActivity Recognition), a novel framework that treats the outputs of atomic\nactivities as a distribution over specified intervals. Leveraging generative\nmethodologies, VCHAR elucidates the reasoning behind complex activity\nclassifications through video-based explanations, accessible to users without\nprior machine learning expertise. Our evaluation across three publicly\navailable datasets demonstrates that VCHAR enhances the accuracy of complex\nactivity recognition without necessitating precise temporal or sequential\nlabeling of atomic activities. Furthermore, user studies confirm that VCHAR's\nexplanations are more intelligible compared to existing methods, facilitating a\nbroader understanding of complex activity recognition among non-experts.\n", "link": "http://arxiv.org/abs/2407.03291v1", "date": "2024-07-03", "relevancy": 2.1194, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.53}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5299}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5292}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VCHAR%3AVariance-Driven%20Complex%20Human%20Activity%20Recognition%20framework%20with%0A%20%20Generative%20Representation&body=Title%3A%20VCHAR%3AVariance-Driven%20Complex%20Human%20Activity%20Recognition%20framework%20with%0A%20%20Generative%20Representation%0AAuthor%3A%20Yuan%20Sun%20and%20Navid%20Salami%20Pargoo%20and%20Taqiya%20Ehsan%20and%20Zhao%20Zhang%20Jorge%20Ortiz%0AAbstract%3A%20%20%20Complex%20human%20activity%20recognition%20%28CHAR%29%20remains%20a%20pivotal%20challenge%20within%0Aubiquitous%20computing%2C%20especially%20in%20the%20context%20of%20smart%20environments.%20Existing%0Astudies%20typically%20require%20meticulous%20labeling%20of%20both%20atomic%20and%20complex%0Aactivities%2C%20a%20task%20that%20is%20labor-intensive%20and%20prone%20to%20errors%20due%20to%20the%0Ascarcity%20and%20inaccuracies%20of%20available%20datasets.%20Most%20prior%20research%20has%0Afocused%20on%20datasets%20that%20either%20precisely%20label%20atomic%20activities%20or%2C%20at%0Aminimum%2C%20their%20sequence%20approaches%20that%20are%20often%20impractical%20in%20real%20world%0Asettings.In%20response%2C%20we%20introduce%20VCHAR%20%28Variance-Driven%20Complex%20Human%0AActivity%20Recognition%29%2C%20a%20novel%20framework%20that%20treats%20the%20outputs%20of%20atomic%0Aactivities%20as%20a%20distribution%20over%20specified%20intervals.%20Leveraging%20generative%0Amethodologies%2C%20VCHAR%20elucidates%20the%20reasoning%20behind%20complex%20activity%0Aclassifications%20through%20video-based%20explanations%2C%20accessible%20to%20users%20without%0Aprior%20machine%20learning%20expertise.%20Our%20evaluation%20across%20three%20publicly%0Aavailable%20datasets%20demonstrates%20that%20VCHAR%20enhances%20the%20accuracy%20of%20complex%0Aactivity%20recognition%20without%20necessitating%20precise%20temporal%20or%20sequential%0Alabeling%20of%20atomic%20activities.%20Furthermore%2C%20user%20studies%20confirm%20that%20VCHAR%27s%0Aexplanations%20are%20more%20intelligible%20compared%20to%20existing%20methods%2C%20facilitating%20a%0Abroader%20understanding%20of%20complex%20activity%20recognition%20among%20non-experts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03291v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVCHAR%253AVariance-Driven%2520Complex%2520Human%2520Activity%2520Recognition%2520framework%2520with%250A%2520%2520Generative%2520Representation%26entry.906535625%3DYuan%2520Sun%2520and%2520Navid%2520Salami%2520Pargoo%2520and%2520Taqiya%2520Ehsan%2520and%2520Zhao%2520Zhang%2520Jorge%2520Ortiz%26entry.1292438233%3D%2520%2520Complex%2520human%2520activity%2520recognition%2520%2528CHAR%2529%2520remains%2520a%2520pivotal%2520challenge%2520within%250Aubiquitous%2520computing%252C%2520especially%2520in%2520the%2520context%2520of%2520smart%2520environments.%2520Existing%250Astudies%2520typically%2520require%2520meticulous%2520labeling%2520of%2520both%2520atomic%2520and%2520complex%250Aactivities%252C%2520a%2520task%2520that%2520is%2520labor-intensive%2520and%2520prone%2520to%2520errors%2520due%2520to%2520the%250Ascarcity%2520and%2520inaccuracies%2520of%2520available%2520datasets.%2520Most%2520prior%2520research%2520has%250Afocused%2520on%2520datasets%2520that%2520either%2520precisely%2520label%2520atomic%2520activities%2520or%252C%2520at%250Aminimum%252C%2520their%2520sequence%2520approaches%2520that%2520are%2520often%2520impractical%2520in%2520real%2520world%250Asettings.In%2520response%252C%2520we%2520introduce%2520VCHAR%2520%2528Variance-Driven%2520Complex%2520Human%250AActivity%2520Recognition%2529%252C%2520a%2520novel%2520framework%2520that%2520treats%2520the%2520outputs%2520of%2520atomic%250Aactivities%2520as%2520a%2520distribution%2520over%2520specified%2520intervals.%2520Leveraging%2520generative%250Amethodologies%252C%2520VCHAR%2520elucidates%2520the%2520reasoning%2520behind%2520complex%2520activity%250Aclassifications%2520through%2520video-based%2520explanations%252C%2520accessible%2520to%2520users%2520without%250Aprior%2520machine%2520learning%2520expertise.%2520Our%2520evaluation%2520across%2520three%2520publicly%250Aavailable%2520datasets%2520demonstrates%2520that%2520VCHAR%2520enhances%2520the%2520accuracy%2520of%2520complex%250Aactivity%2520recognition%2520without%2520necessitating%2520precise%2520temporal%2520or%2520sequential%250Alabeling%2520of%2520atomic%2520activities.%2520Furthermore%252C%2520user%2520studies%2520confirm%2520that%2520VCHAR%2527s%250Aexplanations%2520are%2520more%2520intelligible%2520compared%2520to%2520existing%2520methods%252C%2520facilitating%2520a%250Abroader%2520understanding%2520of%2520complex%2520activity%2520recognition%2520among%2520non-experts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03291v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VCHAR%3AVariance-Driven%20Complex%20Human%20Activity%20Recognition%20framework%20with%0A%20%20Generative%20Representation&entry.906535625=Yuan%20Sun%20and%20Navid%20Salami%20Pargoo%20and%20Taqiya%20Ehsan%20and%20Zhao%20Zhang%20Jorge%20Ortiz&entry.1292438233=%20%20Complex%20human%20activity%20recognition%20%28CHAR%29%20remains%20a%20pivotal%20challenge%20within%0Aubiquitous%20computing%2C%20especially%20in%20the%20context%20of%20smart%20environments.%20Existing%0Astudies%20typically%20require%20meticulous%20labeling%20of%20both%20atomic%20and%20complex%0Aactivities%2C%20a%20task%20that%20is%20labor-intensive%20and%20prone%20to%20errors%20due%20to%20the%0Ascarcity%20and%20inaccuracies%20of%20available%20datasets.%20Most%20prior%20research%20has%0Afocused%20on%20datasets%20that%20either%20precisely%20label%20atomic%20activities%20or%2C%20at%0Aminimum%2C%20their%20sequence%20approaches%20that%20are%20often%20impractical%20in%20real%20world%0Asettings.In%20response%2C%20we%20introduce%20VCHAR%20%28Variance-Driven%20Complex%20Human%0AActivity%20Recognition%29%2C%20a%20novel%20framework%20that%20treats%20the%20outputs%20of%20atomic%0Aactivities%20as%20a%20distribution%20over%20specified%20intervals.%20Leveraging%20generative%0Amethodologies%2C%20VCHAR%20elucidates%20the%20reasoning%20behind%20complex%20activity%0Aclassifications%20through%20video-based%20explanations%2C%20accessible%20to%20users%20without%0Aprior%20machine%20learning%20expertise.%20Our%20evaluation%20across%20three%20publicly%0Aavailable%20datasets%20demonstrates%20that%20VCHAR%20enhances%20the%20accuracy%20of%20complex%0Aactivity%20recognition%20without%20necessitating%20precise%20temporal%20or%20sequential%0Alabeling%20of%20atomic%20activities.%20Furthermore%2C%20user%20studies%20confirm%20that%20VCHAR%27s%0Aexplanations%20are%20more%20intelligible%20compared%20to%20existing%20methods%2C%20facilitating%20a%0Abroader%20understanding%20of%20complex%20activity%20recognition%20among%20non-experts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03291v1&entry.124074799=Read"},
{"title": "Mixture-of-Experts for Open Set Domain Adaptation: A Dual-Space\n  Detection Approach", "author": "Zhenbang Du and Jiayu An and Yunlu Tu and Jiahao Hong and Dongrui Wu", "abstract": "  Open Set Domain Adaptation (OSDA) aims to cope with the distribution and\nlabel shifts between the source and target domains simultaneously, performing\naccurate classification for known classes while identifying unknown class\nsamples in the target domain. Most existing OSDA approaches, depending on the\nfinal image feature space of deep models, require manually-tuned thresholds,\nand may easily misclassify unknown samples as known classes. Mixture-of-Experts\n(MoE) could be a remedy. Within a MoE, different experts handle distinct input\nfeatures, producing unique expert routing patterns for various classes in a\nrouting feature space. As a result, unknown class samples may display different\nexpert routing patterns to known classes. In this paper, we propose Dual-Space\nDetection, which exploits the inconsistencies between the image feature space\nand the routing feature space to detect unknown class samples without any\nthreshold. Graph Router is further introduced to better make use of the spatial\ninformation among image patches. Experiments on three different datasets\nvalidated the effectiveness and superiority of our approach.\n", "link": "http://arxiv.org/abs/2311.00285v2", "date": "2024-07-03", "relevancy": 2.1138, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5326}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5279}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5194}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture-of-Experts%20for%20Open%20Set%20Domain%20Adaptation%3A%20A%20Dual-Space%0A%20%20Detection%20Approach&body=Title%3A%20Mixture-of-Experts%20for%20Open%20Set%20Domain%20Adaptation%3A%20A%20Dual-Space%0A%20%20Detection%20Approach%0AAuthor%3A%20Zhenbang%20Du%20and%20Jiayu%20An%20and%20Yunlu%20Tu%20and%20Jiahao%20Hong%20and%20Dongrui%20Wu%0AAbstract%3A%20%20%20Open%20Set%20Domain%20Adaptation%20%28OSDA%29%20aims%20to%20cope%20with%20the%20distribution%20and%0Alabel%20shifts%20between%20the%20source%20and%20target%20domains%20simultaneously%2C%20performing%0Aaccurate%20classification%20for%20known%20classes%20while%20identifying%20unknown%20class%0Asamples%20in%20the%20target%20domain.%20Most%20existing%20OSDA%20approaches%2C%20depending%20on%20the%0Afinal%20image%20feature%20space%20of%20deep%20models%2C%20require%20manually-tuned%20thresholds%2C%0Aand%20may%20easily%20misclassify%20unknown%20samples%20as%20known%20classes.%20Mixture-of-Experts%0A%28MoE%29%20could%20be%20a%20remedy.%20Within%20a%20MoE%2C%20different%20experts%20handle%20distinct%20input%0Afeatures%2C%20producing%20unique%20expert%20routing%20patterns%20for%20various%20classes%20in%20a%0Arouting%20feature%20space.%20As%20a%20result%2C%20unknown%20class%20samples%20may%20display%20different%0Aexpert%20routing%20patterns%20to%20known%20classes.%20In%20this%20paper%2C%20we%20propose%20Dual-Space%0ADetection%2C%20which%20exploits%20the%20inconsistencies%20between%20the%20image%20feature%20space%0Aand%20the%20routing%20feature%20space%20to%20detect%20unknown%20class%20samples%20without%20any%0Athreshold.%20Graph%20Router%20is%20further%20introduced%20to%20better%20make%20use%20of%20the%20spatial%0Ainformation%20among%20image%20patches.%20Experiments%20on%20three%20different%20datasets%0Avalidated%20the%20effectiveness%20and%20superiority%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.00285v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture-of-Experts%2520for%2520Open%2520Set%2520Domain%2520Adaptation%253A%2520A%2520Dual-Space%250A%2520%2520Detection%2520Approach%26entry.906535625%3DZhenbang%2520Du%2520and%2520Jiayu%2520An%2520and%2520Yunlu%2520Tu%2520and%2520Jiahao%2520Hong%2520and%2520Dongrui%2520Wu%26entry.1292438233%3D%2520%2520Open%2520Set%2520Domain%2520Adaptation%2520%2528OSDA%2529%2520aims%2520to%2520cope%2520with%2520the%2520distribution%2520and%250Alabel%2520shifts%2520between%2520the%2520source%2520and%2520target%2520domains%2520simultaneously%252C%2520performing%250Aaccurate%2520classification%2520for%2520known%2520classes%2520while%2520identifying%2520unknown%2520class%250Asamples%2520in%2520the%2520target%2520domain.%2520Most%2520existing%2520OSDA%2520approaches%252C%2520depending%2520on%2520the%250Afinal%2520image%2520feature%2520space%2520of%2520deep%2520models%252C%2520require%2520manually-tuned%2520thresholds%252C%250Aand%2520may%2520easily%2520misclassify%2520unknown%2520samples%2520as%2520known%2520classes.%2520Mixture-of-Experts%250A%2528MoE%2529%2520could%2520be%2520a%2520remedy.%2520Within%2520a%2520MoE%252C%2520different%2520experts%2520handle%2520distinct%2520input%250Afeatures%252C%2520producing%2520unique%2520expert%2520routing%2520patterns%2520for%2520various%2520classes%2520in%2520a%250Arouting%2520feature%2520space.%2520As%2520a%2520result%252C%2520unknown%2520class%2520samples%2520may%2520display%2520different%250Aexpert%2520routing%2520patterns%2520to%2520known%2520classes.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Dual-Space%250ADetection%252C%2520which%2520exploits%2520the%2520inconsistencies%2520between%2520the%2520image%2520feature%2520space%250Aand%2520the%2520routing%2520feature%2520space%2520to%2520detect%2520unknown%2520class%2520samples%2520without%2520any%250Athreshold.%2520Graph%2520Router%2520is%2520further%2520introduced%2520to%2520better%2520make%2520use%2520of%2520the%2520spatial%250Ainformation%2520among%2520image%2520patches.%2520Experiments%2520on%2520three%2520different%2520datasets%250Avalidated%2520the%2520effectiveness%2520and%2520superiority%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.00285v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture-of-Experts%20for%20Open%20Set%20Domain%20Adaptation%3A%20A%20Dual-Space%0A%20%20Detection%20Approach&entry.906535625=Zhenbang%20Du%20and%20Jiayu%20An%20and%20Yunlu%20Tu%20and%20Jiahao%20Hong%20and%20Dongrui%20Wu&entry.1292438233=%20%20Open%20Set%20Domain%20Adaptation%20%28OSDA%29%20aims%20to%20cope%20with%20the%20distribution%20and%0Alabel%20shifts%20between%20the%20source%20and%20target%20domains%20simultaneously%2C%20performing%0Aaccurate%20classification%20for%20known%20classes%20while%20identifying%20unknown%20class%0Asamples%20in%20the%20target%20domain.%20Most%20existing%20OSDA%20approaches%2C%20depending%20on%20the%0Afinal%20image%20feature%20space%20of%20deep%20models%2C%20require%20manually-tuned%20thresholds%2C%0Aand%20may%20easily%20misclassify%20unknown%20samples%20as%20known%20classes.%20Mixture-of-Experts%0A%28MoE%29%20could%20be%20a%20remedy.%20Within%20a%20MoE%2C%20different%20experts%20handle%20distinct%20input%0Afeatures%2C%20producing%20unique%20expert%20routing%20patterns%20for%20various%20classes%20in%20a%0Arouting%20feature%20space.%20As%20a%20result%2C%20unknown%20class%20samples%20may%20display%20different%0Aexpert%20routing%20patterns%20to%20known%20classes.%20In%20this%20paper%2C%20we%20propose%20Dual-Space%0ADetection%2C%20which%20exploits%20the%20inconsistencies%20between%20the%20image%20feature%20space%0Aand%20the%20routing%20feature%20space%20to%20detect%20unknown%20class%20samples%20without%20any%0Athreshold.%20Graph%20Router%20is%20further%20introduced%20to%20better%20make%20use%20of%20the%20spatial%0Ainformation%20among%20image%20patches.%20Experiments%20on%20three%20different%20datasets%0Avalidated%20the%20effectiveness%20and%20superiority%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.00285v2&entry.124074799=Read"},
{"title": "MHNet: Multi-view High-order Network for Diagnosing Neurodevelopmental\n  Disorders Using Resting-state fMRI", "author": "Yueyang Li and Weiming Zeng and Wenhao Dong and Luhui Cai and Lei Wang and Hongyu Chen and Hongjie Yan and Lingbin Bian and Nizhuan Wang", "abstract": "  Background: Deep learning models have shown promise in diagnosing\nneurodevelopmental disorders (NDD) like ASD and ADHD. However, many models\neither use graph neural networks (GNN) to construct single-level brain\nfunctional networks (BFNs) or employ spatial convolution filtering for local\ninformation extraction from rs-fMRI data, often neglecting high-order features\ncrucial for NDD classification. Methods: We introduce a Multi-view High-order\nNetwork (MHNet) to capture hierarchical and high-order features from multi-view\nBFNs derived from rs-fMRI data for NDD prediction. MHNet has two branches: the\nEuclidean Space Features Extraction (ESFE) module and the Non-Euclidean Space\nFeatures Extraction (Non-ESFE) module, followed by a Feature Fusion-based\nClassification (FFC) module for NDD identification. ESFE includes a Functional\nConnectivity Generation (FCG) module and a High-order Convolutional Neural\nNetwork (HCNN) module to extract local and high-order features from BFNs in\nEuclidean space. Non-ESFE comprises a Generic Internet-like Brain Hierarchical\nNetwork Generation (G-IBHN-G) module and a High-order Graph Neural Network\n(HGNN) module to capture topological and high-order features in non-Euclidean\nspace. Results: Experiments on three public datasets show that MHNet\noutperforms state-of-the-art methods using both AAL1 and Brainnetome Atlas\ntemplates. Extensive ablation studies confirm the superiority of MHNet and the\neffectiveness of using multi-view fMRI information and high-order features. Our\nstudy also offers atlas options for constructing more sophisticated\nhierarchical networks and explains the association between key brain regions\nand NDD. Conclusion: MHNet leverages multi-view feature learning from both\nEuclidean and non-Euclidean spaces, incorporating high-order information from\nBFNs to enhance NDD classification performance.\n", "link": "http://arxiv.org/abs/2407.03217v1", "date": "2024-07-03", "relevancy": 2.111, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5531}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5153}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4956}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MHNet%3A%20Multi-view%20High-order%20Network%20for%20Diagnosing%20Neurodevelopmental%0A%20%20Disorders%20Using%20Resting-state%20fMRI&body=Title%3A%20MHNet%3A%20Multi-view%20High-order%20Network%20for%20Diagnosing%20Neurodevelopmental%0A%20%20Disorders%20Using%20Resting-state%20fMRI%0AAuthor%3A%20Yueyang%20Li%20and%20Weiming%20Zeng%20and%20Wenhao%20Dong%20and%20Luhui%20Cai%20and%20Lei%20Wang%20and%20Hongyu%20Chen%20and%20Hongjie%20Yan%20and%20Lingbin%20Bian%20and%20Nizhuan%20Wang%0AAbstract%3A%20%20%20Background%3A%20Deep%20learning%20models%20have%20shown%20promise%20in%20diagnosing%0Aneurodevelopmental%20disorders%20%28NDD%29%20like%20ASD%20and%20ADHD.%20However%2C%20many%20models%0Aeither%20use%20graph%20neural%20networks%20%28GNN%29%20to%20construct%20single-level%20brain%0Afunctional%20networks%20%28BFNs%29%20or%20employ%20spatial%20convolution%20filtering%20for%20local%0Ainformation%20extraction%20from%20rs-fMRI%20data%2C%20often%20neglecting%20high-order%20features%0Acrucial%20for%20NDD%20classification.%20Methods%3A%20We%20introduce%20a%20Multi-view%20High-order%0ANetwork%20%28MHNet%29%20to%20capture%20hierarchical%20and%20high-order%20features%20from%20multi-view%0ABFNs%20derived%20from%20rs-fMRI%20data%20for%20NDD%20prediction.%20MHNet%20has%20two%20branches%3A%20the%0AEuclidean%20Space%20Features%20Extraction%20%28ESFE%29%20module%20and%20the%20Non-Euclidean%20Space%0AFeatures%20Extraction%20%28Non-ESFE%29%20module%2C%20followed%20by%20a%20Feature%20Fusion-based%0AClassification%20%28FFC%29%20module%20for%20NDD%20identification.%20ESFE%20includes%20a%20Functional%0AConnectivity%20Generation%20%28FCG%29%20module%20and%20a%20High-order%20Convolutional%20Neural%0ANetwork%20%28HCNN%29%20module%20to%20extract%20local%20and%20high-order%20features%20from%20BFNs%20in%0AEuclidean%20space.%20Non-ESFE%20comprises%20a%20Generic%20Internet-like%20Brain%20Hierarchical%0ANetwork%20Generation%20%28G-IBHN-G%29%20module%20and%20a%20High-order%20Graph%20Neural%20Network%0A%28HGNN%29%20module%20to%20capture%20topological%20and%20high-order%20features%20in%20non-Euclidean%0Aspace.%20Results%3A%20Experiments%20on%20three%20public%20datasets%20show%20that%20MHNet%0Aoutperforms%20state-of-the-art%20methods%20using%20both%20AAL1%20and%20Brainnetome%20Atlas%0Atemplates.%20Extensive%20ablation%20studies%20confirm%20the%20superiority%20of%20MHNet%20and%20the%0Aeffectiveness%20of%20using%20multi-view%20fMRI%20information%20and%20high-order%20features.%20Our%0Astudy%20also%20offers%20atlas%20options%20for%20constructing%20more%20sophisticated%0Ahierarchical%20networks%20and%20explains%20the%20association%20between%20key%20brain%20regions%0Aand%20NDD.%20Conclusion%3A%20MHNet%20leverages%20multi-view%20feature%20learning%20from%20both%0AEuclidean%20and%20non-Euclidean%20spaces%2C%20incorporating%20high-order%20information%20from%0ABFNs%20to%20enhance%20NDD%20classification%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03217v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMHNet%253A%2520Multi-view%2520High-order%2520Network%2520for%2520Diagnosing%2520Neurodevelopmental%250A%2520%2520Disorders%2520Using%2520Resting-state%2520fMRI%26entry.906535625%3DYueyang%2520Li%2520and%2520Weiming%2520Zeng%2520and%2520Wenhao%2520Dong%2520and%2520Luhui%2520Cai%2520and%2520Lei%2520Wang%2520and%2520Hongyu%2520Chen%2520and%2520Hongjie%2520Yan%2520and%2520Lingbin%2520Bian%2520and%2520Nizhuan%2520Wang%26entry.1292438233%3D%2520%2520Background%253A%2520Deep%2520learning%2520models%2520have%2520shown%2520promise%2520in%2520diagnosing%250Aneurodevelopmental%2520disorders%2520%2528NDD%2529%2520like%2520ASD%2520and%2520ADHD.%2520However%252C%2520many%2520models%250Aeither%2520use%2520graph%2520neural%2520networks%2520%2528GNN%2529%2520to%2520construct%2520single-level%2520brain%250Afunctional%2520networks%2520%2528BFNs%2529%2520or%2520employ%2520spatial%2520convolution%2520filtering%2520for%2520local%250Ainformation%2520extraction%2520from%2520rs-fMRI%2520data%252C%2520often%2520neglecting%2520high-order%2520features%250Acrucial%2520for%2520NDD%2520classification.%2520Methods%253A%2520We%2520introduce%2520a%2520Multi-view%2520High-order%250ANetwork%2520%2528MHNet%2529%2520to%2520capture%2520hierarchical%2520and%2520high-order%2520features%2520from%2520multi-view%250ABFNs%2520derived%2520from%2520rs-fMRI%2520data%2520for%2520NDD%2520prediction.%2520MHNet%2520has%2520two%2520branches%253A%2520the%250AEuclidean%2520Space%2520Features%2520Extraction%2520%2528ESFE%2529%2520module%2520and%2520the%2520Non-Euclidean%2520Space%250AFeatures%2520Extraction%2520%2528Non-ESFE%2529%2520module%252C%2520followed%2520by%2520a%2520Feature%2520Fusion-based%250AClassification%2520%2528FFC%2529%2520module%2520for%2520NDD%2520identification.%2520ESFE%2520includes%2520a%2520Functional%250AConnectivity%2520Generation%2520%2528FCG%2529%2520module%2520and%2520a%2520High-order%2520Convolutional%2520Neural%250ANetwork%2520%2528HCNN%2529%2520module%2520to%2520extract%2520local%2520and%2520high-order%2520features%2520from%2520BFNs%2520in%250AEuclidean%2520space.%2520Non-ESFE%2520comprises%2520a%2520Generic%2520Internet-like%2520Brain%2520Hierarchical%250ANetwork%2520Generation%2520%2528G-IBHN-G%2529%2520module%2520and%2520a%2520High-order%2520Graph%2520Neural%2520Network%250A%2528HGNN%2529%2520module%2520to%2520capture%2520topological%2520and%2520high-order%2520features%2520in%2520non-Euclidean%250Aspace.%2520Results%253A%2520Experiments%2520on%2520three%2520public%2520datasets%2520show%2520that%2520MHNet%250Aoutperforms%2520state-of-the-art%2520methods%2520using%2520both%2520AAL1%2520and%2520Brainnetome%2520Atlas%250Atemplates.%2520Extensive%2520ablation%2520studies%2520confirm%2520the%2520superiority%2520of%2520MHNet%2520and%2520the%250Aeffectiveness%2520of%2520using%2520multi-view%2520fMRI%2520information%2520and%2520high-order%2520features.%2520Our%250Astudy%2520also%2520offers%2520atlas%2520options%2520for%2520constructing%2520more%2520sophisticated%250Ahierarchical%2520networks%2520and%2520explains%2520the%2520association%2520between%2520key%2520brain%2520regions%250Aand%2520NDD.%2520Conclusion%253A%2520MHNet%2520leverages%2520multi-view%2520feature%2520learning%2520from%2520both%250AEuclidean%2520and%2520non-Euclidean%2520spaces%252C%2520incorporating%2520high-order%2520information%2520from%250ABFNs%2520to%2520enhance%2520NDD%2520classification%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03217v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MHNet%3A%20Multi-view%20High-order%20Network%20for%20Diagnosing%20Neurodevelopmental%0A%20%20Disorders%20Using%20Resting-state%20fMRI&entry.906535625=Yueyang%20Li%20and%20Weiming%20Zeng%20and%20Wenhao%20Dong%20and%20Luhui%20Cai%20and%20Lei%20Wang%20and%20Hongyu%20Chen%20and%20Hongjie%20Yan%20and%20Lingbin%20Bian%20and%20Nizhuan%20Wang&entry.1292438233=%20%20Background%3A%20Deep%20learning%20models%20have%20shown%20promise%20in%20diagnosing%0Aneurodevelopmental%20disorders%20%28NDD%29%20like%20ASD%20and%20ADHD.%20However%2C%20many%20models%0Aeither%20use%20graph%20neural%20networks%20%28GNN%29%20to%20construct%20single-level%20brain%0Afunctional%20networks%20%28BFNs%29%20or%20employ%20spatial%20convolution%20filtering%20for%20local%0Ainformation%20extraction%20from%20rs-fMRI%20data%2C%20often%20neglecting%20high-order%20features%0Acrucial%20for%20NDD%20classification.%20Methods%3A%20We%20introduce%20a%20Multi-view%20High-order%0ANetwork%20%28MHNet%29%20to%20capture%20hierarchical%20and%20high-order%20features%20from%20multi-view%0ABFNs%20derived%20from%20rs-fMRI%20data%20for%20NDD%20prediction.%20MHNet%20has%20two%20branches%3A%20the%0AEuclidean%20Space%20Features%20Extraction%20%28ESFE%29%20module%20and%20the%20Non-Euclidean%20Space%0AFeatures%20Extraction%20%28Non-ESFE%29%20module%2C%20followed%20by%20a%20Feature%20Fusion-based%0AClassification%20%28FFC%29%20module%20for%20NDD%20identification.%20ESFE%20includes%20a%20Functional%0AConnectivity%20Generation%20%28FCG%29%20module%20and%20a%20High-order%20Convolutional%20Neural%0ANetwork%20%28HCNN%29%20module%20to%20extract%20local%20and%20high-order%20features%20from%20BFNs%20in%0AEuclidean%20space.%20Non-ESFE%20comprises%20a%20Generic%20Internet-like%20Brain%20Hierarchical%0ANetwork%20Generation%20%28G-IBHN-G%29%20module%20and%20a%20High-order%20Graph%20Neural%20Network%0A%28HGNN%29%20module%20to%20capture%20topological%20and%20high-order%20features%20in%20non-Euclidean%0Aspace.%20Results%3A%20Experiments%20on%20three%20public%20datasets%20show%20that%20MHNet%0Aoutperforms%20state-of-the-art%20methods%20using%20both%20AAL1%20and%20Brainnetome%20Atlas%0Atemplates.%20Extensive%20ablation%20studies%20confirm%20the%20superiority%20of%20MHNet%20and%20the%0Aeffectiveness%20of%20using%20multi-view%20fMRI%20information%20and%20high-order%20features.%20Our%0Astudy%20also%20offers%20atlas%20options%20for%20constructing%20more%20sophisticated%0Ahierarchical%20networks%20and%20explains%20the%20association%20between%20key%20brain%20regions%0Aand%20NDD.%20Conclusion%3A%20MHNet%20leverages%20multi-view%20feature%20learning%20from%20both%0AEuclidean%20and%20non-Euclidean%20spaces%2C%20incorporating%20high-order%20information%20from%0ABFNs%20to%20enhance%20NDD%20classification%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03217v1&entry.124074799=Read"},
{"title": "Unveiling and Controlling Anomalous Attention Distribution in\n  Transformers", "author": "Ruiqing Yan and Xingbo Du and Haoyu Deng and Linghan Zheng and Qiuzhuang Sun and Jifang Hu and Yuhang Shao and Penghao Jiang and Jinrong Jiang and Lian Zhao", "abstract": "  With the advent of large models based on the Transformer architecture,\nresearchers have observed an anomalous phenomenon in the Attention\nmechanism--there is a very high attention on the first element, which is\nprevalent across Transformer-based models. It is crucial to understand it for\nthe development of techniques focusing on attention distribution, such as\nKey-Value (KV) Cache compression and infinite extrapolation; however, the\nlatent cause leaves to be unknown. In this paper, we analyze such a phenomenon\nfrom the perspective of waiver phenomenon, which involves reducing the internal\nvalues of certain elements in the sequence, allowing them to absorb excess\nattention without affecting their contribution to information. In specific\nmodels, due to differences in positional encoding and attention patterns, we\nhave found that the selection of waiver elements by the model can be\ncategorized into two methods: positional-encoding-based and\nfeature-distribution-within-elements-based.\n", "link": "http://arxiv.org/abs/2407.01601v2", "date": "2024-07-03", "relevancy": 2.1095, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5677}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5603}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4782}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unveiling%20and%20Controlling%20Anomalous%20Attention%20Distribution%20in%0A%20%20Transformers&body=Title%3A%20Unveiling%20and%20Controlling%20Anomalous%20Attention%20Distribution%20in%0A%20%20Transformers%0AAuthor%3A%20Ruiqing%20Yan%20and%20Xingbo%20Du%20and%20Haoyu%20Deng%20and%20Linghan%20Zheng%20and%20Qiuzhuang%20Sun%20and%20Jifang%20Hu%20and%20Yuhang%20Shao%20and%20Penghao%20Jiang%20and%20Jinrong%20Jiang%20and%20Lian%20Zhao%0AAbstract%3A%20%20%20With%20the%20advent%20of%20large%20models%20based%20on%20the%20Transformer%20architecture%2C%0Aresearchers%20have%20observed%20an%20anomalous%20phenomenon%20in%20the%20Attention%0Amechanism--there%20is%20a%20very%20high%20attention%20on%20the%20first%20element%2C%20which%20is%0Aprevalent%20across%20Transformer-based%20models.%20It%20is%20crucial%20to%20understand%20it%20for%0Athe%20development%20of%20techniques%20focusing%20on%20attention%20distribution%2C%20such%20as%0AKey-Value%20%28KV%29%20Cache%20compression%20and%20infinite%20extrapolation%3B%20however%2C%20the%0Alatent%20cause%20leaves%20to%20be%20unknown.%20In%20this%20paper%2C%20we%20analyze%20such%20a%20phenomenon%0Afrom%20the%20perspective%20of%20waiver%20phenomenon%2C%20which%20involves%20reducing%20the%20internal%0Avalues%20of%20certain%20elements%20in%20the%20sequence%2C%20allowing%20them%20to%20absorb%20excess%0Aattention%20without%20affecting%20their%20contribution%20to%20information.%20In%20specific%0Amodels%2C%20due%20to%20differences%20in%20positional%20encoding%20and%20attention%20patterns%2C%20we%0Ahave%20found%20that%20the%20selection%20of%20waiver%20elements%20by%20the%20model%20can%20be%0Acategorized%20into%20two%20methods%3A%20positional-encoding-based%20and%0Afeature-distribution-within-elements-based.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01601v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnveiling%2520and%2520Controlling%2520Anomalous%2520Attention%2520Distribution%2520in%250A%2520%2520Transformers%26entry.906535625%3DRuiqing%2520Yan%2520and%2520Xingbo%2520Du%2520and%2520Haoyu%2520Deng%2520and%2520Linghan%2520Zheng%2520and%2520Qiuzhuang%2520Sun%2520and%2520Jifang%2520Hu%2520and%2520Yuhang%2520Shao%2520and%2520Penghao%2520Jiang%2520and%2520Jinrong%2520Jiang%2520and%2520Lian%2520Zhao%26entry.1292438233%3D%2520%2520With%2520the%2520advent%2520of%2520large%2520models%2520based%2520on%2520the%2520Transformer%2520architecture%252C%250Aresearchers%2520have%2520observed%2520an%2520anomalous%2520phenomenon%2520in%2520the%2520Attention%250Amechanism--there%2520is%2520a%2520very%2520high%2520attention%2520on%2520the%2520first%2520element%252C%2520which%2520is%250Aprevalent%2520across%2520Transformer-based%2520models.%2520It%2520is%2520crucial%2520to%2520understand%2520it%2520for%250Athe%2520development%2520of%2520techniques%2520focusing%2520on%2520attention%2520distribution%252C%2520such%2520as%250AKey-Value%2520%2528KV%2529%2520Cache%2520compression%2520and%2520infinite%2520extrapolation%253B%2520however%252C%2520the%250Alatent%2520cause%2520leaves%2520to%2520be%2520unknown.%2520In%2520this%2520paper%252C%2520we%2520analyze%2520such%2520a%2520phenomenon%250Afrom%2520the%2520perspective%2520of%2520waiver%2520phenomenon%252C%2520which%2520involves%2520reducing%2520the%2520internal%250Avalues%2520of%2520certain%2520elements%2520in%2520the%2520sequence%252C%2520allowing%2520them%2520to%2520absorb%2520excess%250Aattention%2520without%2520affecting%2520their%2520contribution%2520to%2520information.%2520In%2520specific%250Amodels%252C%2520due%2520to%2520differences%2520in%2520positional%2520encoding%2520and%2520attention%2520patterns%252C%2520we%250Ahave%2520found%2520that%2520the%2520selection%2520of%2520waiver%2520elements%2520by%2520the%2520model%2520can%2520be%250Acategorized%2520into%2520two%2520methods%253A%2520positional-encoding-based%2520and%250Afeature-distribution-within-elements-based.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01601v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unveiling%20and%20Controlling%20Anomalous%20Attention%20Distribution%20in%0A%20%20Transformers&entry.906535625=Ruiqing%20Yan%20and%20Xingbo%20Du%20and%20Haoyu%20Deng%20and%20Linghan%20Zheng%20and%20Qiuzhuang%20Sun%20and%20Jifang%20Hu%20and%20Yuhang%20Shao%20and%20Penghao%20Jiang%20and%20Jinrong%20Jiang%20and%20Lian%20Zhao&entry.1292438233=%20%20With%20the%20advent%20of%20large%20models%20based%20on%20the%20Transformer%20architecture%2C%0Aresearchers%20have%20observed%20an%20anomalous%20phenomenon%20in%20the%20Attention%0Amechanism--there%20is%20a%20very%20high%20attention%20on%20the%20first%20element%2C%20which%20is%0Aprevalent%20across%20Transformer-based%20models.%20It%20is%20crucial%20to%20understand%20it%20for%0Athe%20development%20of%20techniques%20focusing%20on%20attention%20distribution%2C%20such%20as%0AKey-Value%20%28KV%29%20Cache%20compression%20and%20infinite%20extrapolation%3B%20however%2C%20the%0Alatent%20cause%20leaves%20to%20be%20unknown.%20In%20this%20paper%2C%20we%20analyze%20such%20a%20phenomenon%0Afrom%20the%20perspective%20of%20waiver%20phenomenon%2C%20which%20involves%20reducing%20the%20internal%0Avalues%20of%20certain%20elements%20in%20the%20sequence%2C%20allowing%20them%20to%20absorb%20excess%0Aattention%20without%20affecting%20their%20contribution%20to%20information.%20In%20specific%0Amodels%2C%20due%20to%20differences%20in%20positional%20encoding%20and%20attention%20patterns%2C%20we%0Ahave%20found%20that%20the%20selection%20of%20waiver%20elements%20by%20the%20model%20can%20be%0Acategorized%20into%20two%20methods%3A%20positional-encoding-based%20and%0Afeature-distribution-within-elements-based.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01601v2&entry.124074799=Read"},
{"title": "PPO-based Dynamic Control of Uncertain Floating Platforms in the Zero-G\n  Environment", "author": "Mahya Ramezani and M. Amin Alandihallaj and Andreas M. Hein", "abstract": "  In the field of space exploration, floating platforms play a crucial role in\nscientific investigations and technological advancements. However, controlling\nthese platforms in zero-gravity environments presents unique challenges,\nincluding uncertainties and disturbances. This paper introduces an innovative\napproach that combines Proximal Policy Optimization (PPO) with Model Predictive\nControl (MPC) in the zero-gravity laboratory (Zero-G Lab) at the University of\nLuxembourg. This approach leverages PPO's reinforcement learning power and\nMPC's precision to navigate the complex control dynamics of floating platforms.\nUnlike traditional control methods, this PPO-MPC approach learns from MPC\npredictions, adapting to unmodeled dynamics and disturbances, resulting in a\nresilient control framework tailored to the zero-gravity environment.\nSimulations and experiments in the Zero-G Lab validate this approach,\nshowcasing the adaptability of the PPO agent. This research opens new\npossibilities for controlling floating platforms in zero-gravity settings,\npromising advancements in space exploration.\n", "link": "http://arxiv.org/abs/2407.03224v1", "date": "2024-07-03", "relevancy": 2.0943, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5449}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.523}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5156}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PPO-based%20Dynamic%20Control%20of%20Uncertain%20Floating%20Platforms%20in%20the%20Zero-G%0A%20%20Environment&body=Title%3A%20PPO-based%20Dynamic%20Control%20of%20Uncertain%20Floating%20Platforms%20in%20the%20Zero-G%0A%20%20Environment%0AAuthor%3A%20Mahya%20Ramezani%20and%20M.%20Amin%20Alandihallaj%20and%20Andreas%20M.%20Hein%0AAbstract%3A%20%20%20In%20the%20field%20of%20space%20exploration%2C%20floating%20platforms%20play%20a%20crucial%20role%20in%0Ascientific%20investigations%20and%20technological%20advancements.%20However%2C%20controlling%0Athese%20platforms%20in%20zero-gravity%20environments%20presents%20unique%20challenges%2C%0Aincluding%20uncertainties%20and%20disturbances.%20This%20paper%20introduces%20an%20innovative%0Aapproach%20that%20combines%20Proximal%20Policy%20Optimization%20%28PPO%29%20with%20Model%20Predictive%0AControl%20%28MPC%29%20in%20the%20zero-gravity%20laboratory%20%28Zero-G%20Lab%29%20at%20the%20University%20of%0ALuxembourg.%20This%20approach%20leverages%20PPO%27s%20reinforcement%20learning%20power%20and%0AMPC%27s%20precision%20to%20navigate%20the%20complex%20control%20dynamics%20of%20floating%20platforms.%0AUnlike%20traditional%20control%20methods%2C%20this%20PPO-MPC%20approach%20learns%20from%20MPC%0Apredictions%2C%20adapting%20to%20unmodeled%20dynamics%20and%20disturbances%2C%20resulting%20in%20a%0Aresilient%20control%20framework%20tailored%20to%20the%20zero-gravity%20environment.%0ASimulations%20and%20experiments%20in%20the%20Zero-G%20Lab%20validate%20this%20approach%2C%0Ashowcasing%20the%20adaptability%20of%20the%20PPO%20agent.%20This%20research%20opens%20new%0Apossibilities%20for%20controlling%20floating%20platforms%20in%20zero-gravity%20settings%2C%0Apromising%20advancements%20in%20space%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03224v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPPO-based%2520Dynamic%2520Control%2520of%2520Uncertain%2520Floating%2520Platforms%2520in%2520the%2520Zero-G%250A%2520%2520Environment%26entry.906535625%3DMahya%2520Ramezani%2520and%2520M.%2520Amin%2520Alandihallaj%2520and%2520Andreas%2520M.%2520Hein%26entry.1292438233%3D%2520%2520In%2520the%2520field%2520of%2520space%2520exploration%252C%2520floating%2520platforms%2520play%2520a%2520crucial%2520role%2520in%250Ascientific%2520investigations%2520and%2520technological%2520advancements.%2520However%252C%2520controlling%250Athese%2520platforms%2520in%2520zero-gravity%2520environments%2520presents%2520unique%2520challenges%252C%250Aincluding%2520uncertainties%2520and%2520disturbances.%2520This%2520paper%2520introduces%2520an%2520innovative%250Aapproach%2520that%2520combines%2520Proximal%2520Policy%2520Optimization%2520%2528PPO%2529%2520with%2520Model%2520Predictive%250AControl%2520%2528MPC%2529%2520in%2520the%2520zero-gravity%2520laboratory%2520%2528Zero-G%2520Lab%2529%2520at%2520the%2520University%2520of%250ALuxembourg.%2520This%2520approach%2520leverages%2520PPO%2527s%2520reinforcement%2520learning%2520power%2520and%250AMPC%2527s%2520precision%2520to%2520navigate%2520the%2520complex%2520control%2520dynamics%2520of%2520floating%2520platforms.%250AUnlike%2520traditional%2520control%2520methods%252C%2520this%2520PPO-MPC%2520approach%2520learns%2520from%2520MPC%250Apredictions%252C%2520adapting%2520to%2520unmodeled%2520dynamics%2520and%2520disturbances%252C%2520resulting%2520in%2520a%250Aresilient%2520control%2520framework%2520tailored%2520to%2520the%2520zero-gravity%2520environment.%250ASimulations%2520and%2520experiments%2520in%2520the%2520Zero-G%2520Lab%2520validate%2520this%2520approach%252C%250Ashowcasing%2520the%2520adaptability%2520of%2520the%2520PPO%2520agent.%2520This%2520research%2520opens%2520new%250Apossibilities%2520for%2520controlling%2520floating%2520platforms%2520in%2520zero-gravity%2520settings%252C%250Apromising%2520advancements%2520in%2520space%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03224v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PPO-based%20Dynamic%20Control%20of%20Uncertain%20Floating%20Platforms%20in%20the%20Zero-G%0A%20%20Environment&entry.906535625=Mahya%20Ramezani%20and%20M.%20Amin%20Alandihallaj%20and%20Andreas%20M.%20Hein&entry.1292438233=%20%20In%20the%20field%20of%20space%20exploration%2C%20floating%20platforms%20play%20a%20crucial%20role%20in%0Ascientific%20investigations%20and%20technological%20advancements.%20However%2C%20controlling%0Athese%20platforms%20in%20zero-gravity%20environments%20presents%20unique%20challenges%2C%0Aincluding%20uncertainties%20and%20disturbances.%20This%20paper%20introduces%20an%20innovative%0Aapproach%20that%20combines%20Proximal%20Policy%20Optimization%20%28PPO%29%20with%20Model%20Predictive%0AControl%20%28MPC%29%20in%20the%20zero-gravity%20laboratory%20%28Zero-G%20Lab%29%20at%20the%20University%20of%0ALuxembourg.%20This%20approach%20leverages%20PPO%27s%20reinforcement%20learning%20power%20and%0AMPC%27s%20precision%20to%20navigate%20the%20complex%20control%20dynamics%20of%20floating%20platforms.%0AUnlike%20traditional%20control%20methods%2C%20this%20PPO-MPC%20approach%20learns%20from%20MPC%0Apredictions%2C%20adapting%20to%20unmodeled%20dynamics%20and%20disturbances%2C%20resulting%20in%20a%0Aresilient%20control%20framework%20tailored%20to%20the%20zero-gravity%20environment.%0ASimulations%20and%20experiments%20in%20the%20Zero-G%20Lab%20validate%20this%20approach%2C%0Ashowcasing%20the%20adaptability%20of%20the%20PPO%20agent.%20This%20research%20opens%20new%0Apossibilities%20for%20controlling%20floating%20platforms%20in%20zero-gravity%20settings%2C%0Apromising%20advancements%20in%20space%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03224v1&entry.124074799=Read"},
{"title": "Solving the inverse problem of microscopy deconvolution with a residual\n  Beylkin-Coifman-Rokhlin neural network", "author": "Rui Li and Mikhail Kudryashev and Artur Yakimovich", "abstract": "  Optic deconvolution in light microscopy (LM) refers to recovering the object\ndetails from images, revealing the ground truth of samples. Traditional\nexplicit methods in LM rely on the point spread function (PSF) during image\nacquisition. Yet, these approaches often fall short due to inaccurate PSF\nmodels and noise artifacts, hampering the overall restoration quality. In this\npaper, we approached the optic deconvolution as an inverse problem. Motivated\nby the nonstandard-form compression scheme introduced by Beylkin, Coifman, and\nRokhlin (BCR), we proposed an innovative physics-informed neural network\nMulti-Stage Residual-BCR Net (m-rBCR) to approximate the optic deconvolution.\nWe validated the m-rBCR model on four microscopy datasets - two simulated\nmicroscopy datasets from ImageNet and BioSR, real dSTORM microscopy images, and\nreal widefield microscopy images. In contrast to the explicit deconvolution\nmethods (e.g. Richardson-Lucy) and other state-of-the-art NN models (U-Net,\nDDPM, CARE, DnCNN, ESRGAN, RCAN, Noise2Noise, MPRNet, and MIMO-U-Net), the\nm-rBCR model demonstrates superior performance to other candidates by PSNR and\nSSIM in two real microscopy datasets and the simulated BioSR dataset. In the\nsimulated ImageNet dataset, m-rBCR ranks the second-best place (right after\nMIMO-U-Net). With the backbone from the optical physics, m-rBCR exploits the\ntrainable parameters with better performances (from ~30 times fewer than the\nbenchmark MIMO-U-Net to ~210 times than ESRGAN). This enables m-rBCR to achieve\na shorter runtime (from ~3 times faster than MIMO-U-Net to ~300 times faster\nthan DDPM). To summarize, by leveraging physics constraints our model reduced\npotentially redundant parameters significantly in expertise-oriented NN\ncandidates and achieved high efficiency with superior performance.\n", "link": "http://arxiv.org/abs/2407.03239v1", "date": "2024-07-03", "relevancy": 2.0934, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5497}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5215}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5147}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20the%20inverse%20problem%20of%20microscopy%20deconvolution%20with%20a%20residual%0A%20%20Beylkin-Coifman-Rokhlin%20neural%20network&body=Title%3A%20Solving%20the%20inverse%20problem%20of%20microscopy%20deconvolution%20with%20a%20residual%0A%20%20Beylkin-Coifman-Rokhlin%20neural%20network%0AAuthor%3A%20Rui%20Li%20and%20Mikhail%20Kudryashev%20and%20Artur%20Yakimovich%0AAbstract%3A%20%20%20Optic%20deconvolution%20in%20light%20microscopy%20%28LM%29%20refers%20to%20recovering%20the%20object%0Adetails%20from%20images%2C%20revealing%20the%20ground%20truth%20of%20samples.%20Traditional%0Aexplicit%20methods%20in%20LM%20rely%20on%20the%20point%20spread%20function%20%28PSF%29%20during%20image%0Aacquisition.%20Yet%2C%20these%20approaches%20often%20fall%20short%20due%20to%20inaccurate%20PSF%0Amodels%20and%20noise%20artifacts%2C%20hampering%20the%20overall%20restoration%20quality.%20In%20this%0Apaper%2C%20we%20approached%20the%20optic%20deconvolution%20as%20an%20inverse%20problem.%20Motivated%0Aby%20the%20nonstandard-form%20compression%20scheme%20introduced%20by%20Beylkin%2C%20Coifman%2C%20and%0ARokhlin%20%28BCR%29%2C%20we%20proposed%20an%20innovative%20physics-informed%20neural%20network%0AMulti-Stage%20Residual-BCR%20Net%20%28m-rBCR%29%20to%20approximate%20the%20optic%20deconvolution.%0AWe%20validated%20the%20m-rBCR%20model%20on%20four%20microscopy%20datasets%20-%20two%20simulated%0Amicroscopy%20datasets%20from%20ImageNet%20and%20BioSR%2C%20real%20dSTORM%20microscopy%20images%2C%20and%0Areal%20widefield%20microscopy%20images.%20In%20contrast%20to%20the%20explicit%20deconvolution%0Amethods%20%28e.g.%20Richardson-Lucy%29%20and%20other%20state-of-the-art%20NN%20models%20%28U-Net%2C%0ADDPM%2C%20CARE%2C%20DnCNN%2C%20ESRGAN%2C%20RCAN%2C%20Noise2Noise%2C%20MPRNet%2C%20and%20MIMO-U-Net%29%2C%20the%0Am-rBCR%20model%20demonstrates%20superior%20performance%20to%20other%20candidates%20by%20PSNR%20and%0ASSIM%20in%20two%20real%20microscopy%20datasets%20and%20the%20simulated%20BioSR%20dataset.%20In%20the%0Asimulated%20ImageNet%20dataset%2C%20m-rBCR%20ranks%20the%20second-best%20place%20%28right%20after%0AMIMO-U-Net%29.%20With%20the%20backbone%20from%20the%20optical%20physics%2C%20m-rBCR%20exploits%20the%0Atrainable%20parameters%20with%20better%20performances%20%28from%20~30%20times%20fewer%20than%20the%0Abenchmark%20MIMO-U-Net%20to%20~210%20times%20than%20ESRGAN%29.%20This%20enables%20m-rBCR%20to%20achieve%0Aa%20shorter%20runtime%20%28from%20~3%20times%20faster%20than%20MIMO-U-Net%20to%20~300%20times%20faster%0Athan%20DDPM%29.%20To%20summarize%2C%20by%20leveraging%20physics%20constraints%20our%20model%20reduced%0Apotentially%20redundant%20parameters%20significantly%20in%20expertise-oriented%20NN%0Acandidates%20and%20achieved%20high%20efficiency%20with%20superior%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03239v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520the%2520inverse%2520problem%2520of%2520microscopy%2520deconvolution%2520with%2520a%2520residual%250A%2520%2520Beylkin-Coifman-Rokhlin%2520neural%2520network%26entry.906535625%3DRui%2520Li%2520and%2520Mikhail%2520Kudryashev%2520and%2520Artur%2520Yakimovich%26entry.1292438233%3D%2520%2520Optic%2520deconvolution%2520in%2520light%2520microscopy%2520%2528LM%2529%2520refers%2520to%2520recovering%2520the%2520object%250Adetails%2520from%2520images%252C%2520revealing%2520the%2520ground%2520truth%2520of%2520samples.%2520Traditional%250Aexplicit%2520methods%2520in%2520LM%2520rely%2520on%2520the%2520point%2520spread%2520function%2520%2528PSF%2529%2520during%2520image%250Aacquisition.%2520Yet%252C%2520these%2520approaches%2520often%2520fall%2520short%2520due%2520to%2520inaccurate%2520PSF%250Amodels%2520and%2520noise%2520artifacts%252C%2520hampering%2520the%2520overall%2520restoration%2520quality.%2520In%2520this%250Apaper%252C%2520we%2520approached%2520the%2520optic%2520deconvolution%2520as%2520an%2520inverse%2520problem.%2520Motivated%250Aby%2520the%2520nonstandard-form%2520compression%2520scheme%2520introduced%2520by%2520Beylkin%252C%2520Coifman%252C%2520and%250ARokhlin%2520%2528BCR%2529%252C%2520we%2520proposed%2520an%2520innovative%2520physics-informed%2520neural%2520network%250AMulti-Stage%2520Residual-BCR%2520Net%2520%2528m-rBCR%2529%2520to%2520approximate%2520the%2520optic%2520deconvolution.%250AWe%2520validated%2520the%2520m-rBCR%2520model%2520on%2520four%2520microscopy%2520datasets%2520-%2520two%2520simulated%250Amicroscopy%2520datasets%2520from%2520ImageNet%2520and%2520BioSR%252C%2520real%2520dSTORM%2520microscopy%2520images%252C%2520and%250Areal%2520widefield%2520microscopy%2520images.%2520In%2520contrast%2520to%2520the%2520explicit%2520deconvolution%250Amethods%2520%2528e.g.%2520Richardson-Lucy%2529%2520and%2520other%2520state-of-the-art%2520NN%2520models%2520%2528U-Net%252C%250ADDPM%252C%2520CARE%252C%2520DnCNN%252C%2520ESRGAN%252C%2520RCAN%252C%2520Noise2Noise%252C%2520MPRNet%252C%2520and%2520MIMO-U-Net%2529%252C%2520the%250Am-rBCR%2520model%2520demonstrates%2520superior%2520performance%2520to%2520other%2520candidates%2520by%2520PSNR%2520and%250ASSIM%2520in%2520two%2520real%2520microscopy%2520datasets%2520and%2520the%2520simulated%2520BioSR%2520dataset.%2520In%2520the%250Asimulated%2520ImageNet%2520dataset%252C%2520m-rBCR%2520ranks%2520the%2520second-best%2520place%2520%2528right%2520after%250AMIMO-U-Net%2529.%2520With%2520the%2520backbone%2520from%2520the%2520optical%2520physics%252C%2520m-rBCR%2520exploits%2520the%250Atrainable%2520parameters%2520with%2520better%2520performances%2520%2528from%2520~30%2520times%2520fewer%2520than%2520the%250Abenchmark%2520MIMO-U-Net%2520to%2520~210%2520times%2520than%2520ESRGAN%2529.%2520This%2520enables%2520m-rBCR%2520to%2520achieve%250Aa%2520shorter%2520runtime%2520%2528from%2520~3%2520times%2520faster%2520than%2520MIMO-U-Net%2520to%2520~300%2520times%2520faster%250Athan%2520DDPM%2529.%2520To%2520summarize%252C%2520by%2520leveraging%2520physics%2520constraints%2520our%2520model%2520reduced%250Apotentially%2520redundant%2520parameters%2520significantly%2520in%2520expertise-oriented%2520NN%250Acandidates%2520and%2520achieved%2520high%2520efficiency%2520with%2520superior%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03239v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20the%20inverse%20problem%20of%20microscopy%20deconvolution%20with%20a%20residual%0A%20%20Beylkin-Coifman-Rokhlin%20neural%20network&entry.906535625=Rui%20Li%20and%20Mikhail%20Kudryashev%20and%20Artur%20Yakimovich&entry.1292438233=%20%20Optic%20deconvolution%20in%20light%20microscopy%20%28LM%29%20refers%20to%20recovering%20the%20object%0Adetails%20from%20images%2C%20revealing%20the%20ground%20truth%20of%20samples.%20Traditional%0Aexplicit%20methods%20in%20LM%20rely%20on%20the%20point%20spread%20function%20%28PSF%29%20during%20image%0Aacquisition.%20Yet%2C%20these%20approaches%20often%20fall%20short%20due%20to%20inaccurate%20PSF%0Amodels%20and%20noise%20artifacts%2C%20hampering%20the%20overall%20restoration%20quality.%20In%20this%0Apaper%2C%20we%20approached%20the%20optic%20deconvolution%20as%20an%20inverse%20problem.%20Motivated%0Aby%20the%20nonstandard-form%20compression%20scheme%20introduced%20by%20Beylkin%2C%20Coifman%2C%20and%0ARokhlin%20%28BCR%29%2C%20we%20proposed%20an%20innovative%20physics-informed%20neural%20network%0AMulti-Stage%20Residual-BCR%20Net%20%28m-rBCR%29%20to%20approximate%20the%20optic%20deconvolution.%0AWe%20validated%20the%20m-rBCR%20model%20on%20four%20microscopy%20datasets%20-%20two%20simulated%0Amicroscopy%20datasets%20from%20ImageNet%20and%20BioSR%2C%20real%20dSTORM%20microscopy%20images%2C%20and%0Areal%20widefield%20microscopy%20images.%20In%20contrast%20to%20the%20explicit%20deconvolution%0Amethods%20%28e.g.%20Richardson-Lucy%29%20and%20other%20state-of-the-art%20NN%20models%20%28U-Net%2C%0ADDPM%2C%20CARE%2C%20DnCNN%2C%20ESRGAN%2C%20RCAN%2C%20Noise2Noise%2C%20MPRNet%2C%20and%20MIMO-U-Net%29%2C%20the%0Am-rBCR%20model%20demonstrates%20superior%20performance%20to%20other%20candidates%20by%20PSNR%20and%0ASSIM%20in%20two%20real%20microscopy%20datasets%20and%20the%20simulated%20BioSR%20dataset.%20In%20the%0Asimulated%20ImageNet%20dataset%2C%20m-rBCR%20ranks%20the%20second-best%20place%20%28right%20after%0AMIMO-U-Net%29.%20With%20the%20backbone%20from%20the%20optical%20physics%2C%20m-rBCR%20exploits%20the%0Atrainable%20parameters%20with%20better%20performances%20%28from%20~30%20times%20fewer%20than%20the%0Abenchmark%20MIMO-U-Net%20to%20~210%20times%20than%20ESRGAN%29.%20This%20enables%20m-rBCR%20to%20achieve%0Aa%20shorter%20runtime%20%28from%20~3%20times%20faster%20than%20MIMO-U-Net%20to%20~300%20times%20faster%0Athan%20DDPM%29.%20To%20summarize%2C%20by%20leveraging%20physics%20constraints%20our%20model%20reduced%0Apotentially%20redundant%20parameters%20significantly%20in%20expertise-oriented%20NN%0Acandidates%20and%20achieved%20high%20efficiency%20with%20superior%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03239v1&entry.124074799=Read"},
{"title": "InternLM-XComposer-2.5: A Versatile Large Vision Language Model\n  Supporting Long-Contextual Input and Output", "author": "Pan Zhang and Xiaoyi Dong and Yuhang Zang and Yuhang Cao and Rui Qian and Lin Chen and Qipeng Guo and Haodong Duan and Bin Wang and Linke Ouyang and Songyang Zhang and Wenwei Zhang and Yining Li and Yang Gao and Peng Sun and Xinyue Zhang and Wei Li and Jingwen Li and Wenhai Wang and Hang Yan and Conghui He and Xingcheng Zhang and Kai Chen and Jifeng Dai and Yu Qiao and Dahua Lin and Jiaqi Wang", "abstract": "  We present InternLM-XComposer-2.5 (IXC-2.5), a versatile large-vision\nlanguage model that supports long-contextual input and output. IXC-2.5 excels\nin various text-image comprehension and composition applications, achieving\nGPT-4V level capabilities with merely 7B LLM backend. Trained with 24K\ninterleaved image-text contexts, it can seamlessly extend to 96K long contexts\nvia RoPE extrapolation. This long-context capability allows IXC-2.5 to excel in\ntasks requiring extensive input and output contexts. Compared to its previous\n2.0 version, InternLM-XComposer-2.5 features three major upgrades in\nvision-language comprehension: (1) Ultra-High Resolution Understanding, (2)\nFine-Grained Video Understanding, and (3) Multi-Turn Multi-Image Dialogue. In\naddition to comprehension, IXC-2.5 extends to two compelling applications using\nextra LoRA parameters for text-image composition: (1) Crafting Webpages and (2)\nComposing High-Quality Text-Image Articles. IXC-2.5 has been evaluated on 28\nbenchmarks, outperforming existing open-source state-of-the-art models on 16\nbenchmarks. It also surpasses or competes closely with GPT-4V and Gemini Pro on\n16 key tasks. The InternLM-XComposer-2.5 is publicly available at\nhttps://github.com/InternLM/InternLM-XComposer.\n", "link": "http://arxiv.org/abs/2407.03320v1", "date": "2024-07-03", "relevancy": 2.073, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5421}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5197}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4938}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InternLM-XComposer-2.5%3A%20A%20Versatile%20Large%20Vision%20Language%20Model%0A%20%20Supporting%20Long-Contextual%20Input%20and%20Output&body=Title%3A%20InternLM-XComposer-2.5%3A%20A%20Versatile%20Large%20Vision%20Language%20Model%0A%20%20Supporting%20Long-Contextual%20Input%20and%20Output%0AAuthor%3A%20Pan%20Zhang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Rui%20Qian%20and%20Lin%20Chen%20and%20Qipeng%20Guo%20and%20Haodong%20Duan%20and%20Bin%20Wang%20and%20Linke%20Ouyang%20and%20Songyang%20Zhang%20and%20Wenwei%20Zhang%20and%20Yining%20Li%20and%20Yang%20Gao%20and%20Peng%20Sun%20and%20Xinyue%20Zhang%20and%20Wei%20Li%20and%20Jingwen%20Li%20and%20Wenhai%20Wang%20and%20Hang%20Yan%20and%20Conghui%20He%20and%20Xingcheng%20Zhang%20and%20Kai%20Chen%20and%20Jifeng%20Dai%20and%20Yu%20Qiao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang%0AAbstract%3A%20%20%20We%20present%20InternLM-XComposer-2.5%20%28IXC-2.5%29%2C%20a%20versatile%20large-vision%0Alanguage%20model%20that%20supports%20long-contextual%20input%20and%20output.%20IXC-2.5%20excels%0Ain%20various%20text-image%20comprehension%20and%20composition%20applications%2C%20achieving%0AGPT-4V%20level%20capabilities%20with%20merely%207B%20LLM%20backend.%20Trained%20with%2024K%0Ainterleaved%20image-text%20contexts%2C%20it%20can%20seamlessly%20extend%20to%2096K%20long%20contexts%0Avia%20RoPE%20extrapolation.%20This%20long-context%20capability%20allows%20IXC-2.5%20to%20excel%20in%0Atasks%20requiring%20extensive%20input%20and%20output%20contexts.%20Compared%20to%20its%20previous%0A2.0%20version%2C%20InternLM-XComposer-2.5%20features%20three%20major%20upgrades%20in%0Avision-language%20comprehension%3A%20%281%29%20Ultra-High%20Resolution%20Understanding%2C%20%282%29%0AFine-Grained%20Video%20Understanding%2C%20and%20%283%29%20Multi-Turn%20Multi-Image%20Dialogue.%20In%0Aaddition%20to%20comprehension%2C%20IXC-2.5%20extends%20to%20two%20compelling%20applications%20using%0Aextra%20LoRA%20parameters%20for%20text-image%20composition%3A%20%281%29%20Crafting%20Webpages%20and%20%282%29%0AComposing%20High-Quality%20Text-Image%20Articles.%20IXC-2.5%20has%20been%20evaluated%20on%2028%0Abenchmarks%2C%20outperforming%20existing%20open-source%20state-of-the-art%20models%20on%2016%0Abenchmarks.%20It%20also%20surpasses%20or%20competes%20closely%20with%20GPT-4V%20and%20Gemini%20Pro%20on%0A16%20key%20tasks.%20The%20InternLM-XComposer-2.5%20is%20publicly%20available%20at%0Ahttps%3A//github.com/InternLM/InternLM-XComposer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03320v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInternLM-XComposer-2.5%253A%2520A%2520Versatile%2520Large%2520Vision%2520Language%2520Model%250A%2520%2520Supporting%2520Long-Contextual%2520Input%2520and%2520Output%26entry.906535625%3DPan%2520Zhang%2520and%2520Xiaoyi%2520Dong%2520and%2520Yuhang%2520Zang%2520and%2520Yuhang%2520Cao%2520and%2520Rui%2520Qian%2520and%2520Lin%2520Chen%2520and%2520Qipeng%2520Guo%2520and%2520Haodong%2520Duan%2520and%2520Bin%2520Wang%2520and%2520Linke%2520Ouyang%2520and%2520Songyang%2520Zhang%2520and%2520Wenwei%2520Zhang%2520and%2520Yining%2520Li%2520and%2520Yang%2520Gao%2520and%2520Peng%2520Sun%2520and%2520Xinyue%2520Zhang%2520and%2520Wei%2520Li%2520and%2520Jingwen%2520Li%2520and%2520Wenhai%2520Wang%2520and%2520Hang%2520Yan%2520and%2520Conghui%2520He%2520and%2520Xingcheng%2520Zhang%2520and%2520Kai%2520Chen%2520and%2520Jifeng%2520Dai%2520and%2520Yu%2520Qiao%2520and%2520Dahua%2520Lin%2520and%2520Jiaqi%2520Wang%26entry.1292438233%3D%2520%2520We%2520present%2520InternLM-XComposer-2.5%2520%2528IXC-2.5%2529%252C%2520a%2520versatile%2520large-vision%250Alanguage%2520model%2520that%2520supports%2520long-contextual%2520input%2520and%2520output.%2520IXC-2.5%2520excels%250Ain%2520various%2520text-image%2520comprehension%2520and%2520composition%2520applications%252C%2520achieving%250AGPT-4V%2520level%2520capabilities%2520with%2520merely%25207B%2520LLM%2520backend.%2520Trained%2520with%252024K%250Ainterleaved%2520image-text%2520contexts%252C%2520it%2520can%2520seamlessly%2520extend%2520to%252096K%2520long%2520contexts%250Avia%2520RoPE%2520extrapolation.%2520This%2520long-context%2520capability%2520allows%2520IXC-2.5%2520to%2520excel%2520in%250Atasks%2520requiring%2520extensive%2520input%2520and%2520output%2520contexts.%2520Compared%2520to%2520its%2520previous%250A2.0%2520version%252C%2520InternLM-XComposer-2.5%2520features%2520three%2520major%2520upgrades%2520in%250Avision-language%2520comprehension%253A%2520%25281%2529%2520Ultra-High%2520Resolution%2520Understanding%252C%2520%25282%2529%250AFine-Grained%2520Video%2520Understanding%252C%2520and%2520%25283%2529%2520Multi-Turn%2520Multi-Image%2520Dialogue.%2520In%250Aaddition%2520to%2520comprehension%252C%2520IXC-2.5%2520extends%2520to%2520two%2520compelling%2520applications%2520using%250Aextra%2520LoRA%2520parameters%2520for%2520text-image%2520composition%253A%2520%25281%2529%2520Crafting%2520Webpages%2520and%2520%25282%2529%250AComposing%2520High-Quality%2520Text-Image%2520Articles.%2520IXC-2.5%2520has%2520been%2520evaluated%2520on%252028%250Abenchmarks%252C%2520outperforming%2520existing%2520open-source%2520state-of-the-art%2520models%2520on%252016%250Abenchmarks.%2520It%2520also%2520surpasses%2520or%2520competes%2520closely%2520with%2520GPT-4V%2520and%2520Gemini%2520Pro%2520on%250A16%2520key%2520tasks.%2520The%2520InternLM-XComposer-2.5%2520is%2520publicly%2520available%2520at%250Ahttps%253A//github.com/InternLM/InternLM-XComposer.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03320v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InternLM-XComposer-2.5%3A%20A%20Versatile%20Large%20Vision%20Language%20Model%0A%20%20Supporting%20Long-Contextual%20Input%20and%20Output&entry.906535625=Pan%20Zhang%20and%20Xiaoyi%20Dong%20and%20Yuhang%20Zang%20and%20Yuhang%20Cao%20and%20Rui%20Qian%20and%20Lin%20Chen%20and%20Qipeng%20Guo%20and%20Haodong%20Duan%20and%20Bin%20Wang%20and%20Linke%20Ouyang%20and%20Songyang%20Zhang%20and%20Wenwei%20Zhang%20and%20Yining%20Li%20and%20Yang%20Gao%20and%20Peng%20Sun%20and%20Xinyue%20Zhang%20and%20Wei%20Li%20and%20Jingwen%20Li%20and%20Wenhai%20Wang%20and%20Hang%20Yan%20and%20Conghui%20He%20and%20Xingcheng%20Zhang%20and%20Kai%20Chen%20and%20Jifeng%20Dai%20and%20Yu%20Qiao%20and%20Dahua%20Lin%20and%20Jiaqi%20Wang&entry.1292438233=%20%20We%20present%20InternLM-XComposer-2.5%20%28IXC-2.5%29%2C%20a%20versatile%20large-vision%0Alanguage%20model%20that%20supports%20long-contextual%20input%20and%20output.%20IXC-2.5%20excels%0Ain%20various%20text-image%20comprehension%20and%20composition%20applications%2C%20achieving%0AGPT-4V%20level%20capabilities%20with%20merely%207B%20LLM%20backend.%20Trained%20with%2024K%0Ainterleaved%20image-text%20contexts%2C%20it%20can%20seamlessly%20extend%20to%2096K%20long%20contexts%0Avia%20RoPE%20extrapolation.%20This%20long-context%20capability%20allows%20IXC-2.5%20to%20excel%20in%0Atasks%20requiring%20extensive%20input%20and%20output%20contexts.%20Compared%20to%20its%20previous%0A2.0%20version%2C%20InternLM-XComposer-2.5%20features%20three%20major%20upgrades%20in%0Avision-language%20comprehension%3A%20%281%29%20Ultra-High%20Resolution%20Understanding%2C%20%282%29%0AFine-Grained%20Video%20Understanding%2C%20and%20%283%29%20Multi-Turn%20Multi-Image%20Dialogue.%20In%0Aaddition%20to%20comprehension%2C%20IXC-2.5%20extends%20to%20two%20compelling%20applications%20using%0Aextra%20LoRA%20parameters%20for%20text-image%20composition%3A%20%281%29%20Crafting%20Webpages%20and%20%282%29%0AComposing%20High-Quality%20Text-Image%20Articles.%20IXC-2.5%20has%20been%20evaluated%20on%2028%0Abenchmarks%2C%20outperforming%20existing%20open-source%20state-of-the-art%20models%20on%2016%0Abenchmarks.%20It%20also%20surpasses%20or%20competes%20closely%20with%20GPT-4V%20and%20Gemini%20Pro%20on%0A16%20key%20tasks.%20The%20InternLM-XComposer-2.5%20is%20publicly%20available%20at%0Ahttps%3A//github.com/InternLM/InternLM-XComposer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03320v1&entry.124074799=Read"},
{"title": "Using Buckingham's $\u03c0$ Theorem for Multi-System Learning Transfer: a\n  Case-study with 3 Vehicles Sharing a Database", "author": "William Therrien and Olivier Lecompte and Alexandre Girard", "abstract": "  Many advanced driver assistance schemes or autonomous vehicle controllers are\nbased on a motion model of the vehicle behavior, i.e., a function predicting\nhow the vehicle will react to a given control input. Data-driven models, based\non experimental or simulated data, are very useful, especially for vehicles\ndifficult to model analytically, for instance, ground vehicles for which the\nground-tire interaction is hard to model from first principles. However,\nlearning schemes are limited by the difficulty of collecting large amounts of\nexperimental data or having to rely on high-fidelity simulations. This paper\nexplores the potential of an approach that uses dimensionless numbers based on\nBuckingham's $\\pi$ theorem to improve the efficiency of data for learning\nmodels, with the goal of facilitating knowledge sharing between similar\nsystems. A case study using car-like vehicles compares traditional and\ndimensionless models on simulated and experimental data to validate the\nbenefits of the new dimensionless learning approach. Prediction accuracy\nimprovements with the dimensionless scheme when using a shared database, that\nis, predicting the motion of a vehicle based on data from various different\nvehicles was found to be 480\\% more accurate for predicting a simple no-slip\nmaneuver based on simulated data and 11\\% more accurate to predict a highly\ndynamic braking maneuver based on experimental data. A modified\nphysics-informed learning scheme with hand-crafted dimensionless features was\nalso shown to increase the improvement to precision gains of 917\\% and 28\\%\nrespectively. A comparative study also shows that using Buckingham's $\\pi$\ntheorem is a much more effective preprocessing step for this task than\nprincipal component analysis (PCA) or simply normalizing the data.\n", "link": "http://arxiv.org/abs/2310.17545v2", "date": "2024-07-03", "relevancy": 2.0695, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5914}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5065}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4986}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Buckingham%27s%20%24%CF%80%24%20Theorem%20for%20Multi-System%20Learning%20Transfer%3A%20a%0A%20%20Case-study%20with%203%20Vehicles%20Sharing%20a%20Database&body=Title%3A%20Using%20Buckingham%27s%20%24%CF%80%24%20Theorem%20for%20Multi-System%20Learning%20Transfer%3A%20a%0A%20%20Case-study%20with%203%20Vehicles%20Sharing%20a%20Database%0AAuthor%3A%20William%20Therrien%20and%20Olivier%20Lecompte%20and%20Alexandre%20Girard%0AAbstract%3A%20%20%20Many%20advanced%20driver%20assistance%20schemes%20or%20autonomous%20vehicle%20controllers%20are%0Abased%20on%20a%20motion%20model%20of%20the%20vehicle%20behavior%2C%20i.e.%2C%20a%20function%20predicting%0Ahow%20the%20vehicle%20will%20react%20to%20a%20given%20control%20input.%20Data-driven%20models%2C%20based%0Aon%20experimental%20or%20simulated%20data%2C%20are%20very%20useful%2C%20especially%20for%20vehicles%0Adifficult%20to%20model%20analytically%2C%20for%20instance%2C%20ground%20vehicles%20for%20which%20the%0Aground-tire%20interaction%20is%20hard%20to%20model%20from%20first%20principles.%20However%2C%0Alearning%20schemes%20are%20limited%20by%20the%20difficulty%20of%20collecting%20large%20amounts%20of%0Aexperimental%20data%20or%20having%20to%20rely%20on%20high-fidelity%20simulations.%20This%20paper%0Aexplores%20the%20potential%20of%20an%20approach%20that%20uses%20dimensionless%20numbers%20based%20on%0ABuckingham%27s%20%24%5Cpi%24%20theorem%20to%20improve%20the%20efficiency%20of%20data%20for%20learning%0Amodels%2C%20with%20the%20goal%20of%20facilitating%20knowledge%20sharing%20between%20similar%0Asystems.%20A%20case%20study%20using%20car-like%20vehicles%20compares%20traditional%20and%0Adimensionless%20models%20on%20simulated%20and%20experimental%20data%20to%20validate%20the%0Abenefits%20of%20the%20new%20dimensionless%20learning%20approach.%20Prediction%20accuracy%0Aimprovements%20with%20the%20dimensionless%20scheme%20when%20using%20a%20shared%20database%2C%20that%0Ais%2C%20predicting%20the%20motion%20of%20a%20vehicle%20based%20on%20data%20from%20various%20different%0Avehicles%20was%20found%20to%20be%20480%5C%25%20more%20accurate%20for%20predicting%20a%20simple%20no-slip%0Amaneuver%20based%20on%20simulated%20data%20and%2011%5C%25%20more%20accurate%20to%20predict%20a%20highly%0Adynamic%20braking%20maneuver%20based%20on%20experimental%20data.%20A%20modified%0Aphysics-informed%20learning%20scheme%20with%20hand-crafted%20dimensionless%20features%20was%0Aalso%20shown%20to%20increase%20the%20improvement%20to%20precision%20gains%20of%20917%5C%25%20and%2028%5C%25%0Arespectively.%20A%20comparative%20study%20also%20shows%20that%20using%20Buckingham%27s%20%24%5Cpi%24%0Atheorem%20is%20a%20much%20more%20effective%20preprocessing%20step%20for%20this%20task%20than%0Aprincipal%20component%20analysis%20%28PCA%29%20or%20simply%20normalizing%20the%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.17545v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Buckingham%2527s%2520%2524%25CF%2580%2524%2520Theorem%2520for%2520Multi-System%2520Learning%2520Transfer%253A%2520a%250A%2520%2520Case-study%2520with%25203%2520Vehicles%2520Sharing%2520a%2520Database%26entry.906535625%3DWilliam%2520Therrien%2520and%2520Olivier%2520Lecompte%2520and%2520Alexandre%2520Girard%26entry.1292438233%3D%2520%2520Many%2520advanced%2520driver%2520assistance%2520schemes%2520or%2520autonomous%2520vehicle%2520controllers%2520are%250Abased%2520on%2520a%2520motion%2520model%2520of%2520the%2520vehicle%2520behavior%252C%2520i.e.%252C%2520a%2520function%2520predicting%250Ahow%2520the%2520vehicle%2520will%2520react%2520to%2520a%2520given%2520control%2520input.%2520Data-driven%2520models%252C%2520based%250Aon%2520experimental%2520or%2520simulated%2520data%252C%2520are%2520very%2520useful%252C%2520especially%2520for%2520vehicles%250Adifficult%2520to%2520model%2520analytically%252C%2520for%2520instance%252C%2520ground%2520vehicles%2520for%2520which%2520the%250Aground-tire%2520interaction%2520is%2520hard%2520to%2520model%2520from%2520first%2520principles.%2520However%252C%250Alearning%2520schemes%2520are%2520limited%2520by%2520the%2520difficulty%2520of%2520collecting%2520large%2520amounts%2520of%250Aexperimental%2520data%2520or%2520having%2520to%2520rely%2520on%2520high-fidelity%2520simulations.%2520This%2520paper%250Aexplores%2520the%2520potential%2520of%2520an%2520approach%2520that%2520uses%2520dimensionless%2520numbers%2520based%2520on%250ABuckingham%2527s%2520%2524%255Cpi%2524%2520theorem%2520to%2520improve%2520the%2520efficiency%2520of%2520data%2520for%2520learning%250Amodels%252C%2520with%2520the%2520goal%2520of%2520facilitating%2520knowledge%2520sharing%2520between%2520similar%250Asystems.%2520A%2520case%2520study%2520using%2520car-like%2520vehicles%2520compares%2520traditional%2520and%250Adimensionless%2520models%2520on%2520simulated%2520and%2520experimental%2520data%2520to%2520validate%2520the%250Abenefits%2520of%2520the%2520new%2520dimensionless%2520learning%2520approach.%2520Prediction%2520accuracy%250Aimprovements%2520with%2520the%2520dimensionless%2520scheme%2520when%2520using%2520a%2520shared%2520database%252C%2520that%250Ais%252C%2520predicting%2520the%2520motion%2520of%2520a%2520vehicle%2520based%2520on%2520data%2520from%2520various%2520different%250Avehicles%2520was%2520found%2520to%2520be%2520480%255C%2525%2520more%2520accurate%2520for%2520predicting%2520a%2520simple%2520no-slip%250Amaneuver%2520based%2520on%2520simulated%2520data%2520and%252011%255C%2525%2520more%2520accurate%2520to%2520predict%2520a%2520highly%250Adynamic%2520braking%2520maneuver%2520based%2520on%2520experimental%2520data.%2520A%2520modified%250Aphysics-informed%2520learning%2520scheme%2520with%2520hand-crafted%2520dimensionless%2520features%2520was%250Aalso%2520shown%2520to%2520increase%2520the%2520improvement%2520to%2520precision%2520gains%2520of%2520917%255C%2525%2520and%252028%255C%2525%250Arespectively.%2520A%2520comparative%2520study%2520also%2520shows%2520that%2520using%2520Buckingham%2527s%2520%2524%255Cpi%2524%250Atheorem%2520is%2520a%2520much%2520more%2520effective%2520preprocessing%2520step%2520for%2520this%2520task%2520than%250Aprincipal%2520component%2520analysis%2520%2528PCA%2529%2520or%2520simply%2520normalizing%2520the%2520data.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.17545v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Buckingham%27s%20%24%CF%80%24%20Theorem%20for%20Multi-System%20Learning%20Transfer%3A%20a%0A%20%20Case-study%20with%203%20Vehicles%20Sharing%20a%20Database&entry.906535625=William%20Therrien%20and%20Olivier%20Lecompte%20and%20Alexandre%20Girard&entry.1292438233=%20%20Many%20advanced%20driver%20assistance%20schemes%20or%20autonomous%20vehicle%20controllers%20are%0Abased%20on%20a%20motion%20model%20of%20the%20vehicle%20behavior%2C%20i.e.%2C%20a%20function%20predicting%0Ahow%20the%20vehicle%20will%20react%20to%20a%20given%20control%20input.%20Data-driven%20models%2C%20based%0Aon%20experimental%20or%20simulated%20data%2C%20are%20very%20useful%2C%20especially%20for%20vehicles%0Adifficult%20to%20model%20analytically%2C%20for%20instance%2C%20ground%20vehicles%20for%20which%20the%0Aground-tire%20interaction%20is%20hard%20to%20model%20from%20first%20principles.%20However%2C%0Alearning%20schemes%20are%20limited%20by%20the%20difficulty%20of%20collecting%20large%20amounts%20of%0Aexperimental%20data%20or%20having%20to%20rely%20on%20high-fidelity%20simulations.%20This%20paper%0Aexplores%20the%20potential%20of%20an%20approach%20that%20uses%20dimensionless%20numbers%20based%20on%0ABuckingham%27s%20%24%5Cpi%24%20theorem%20to%20improve%20the%20efficiency%20of%20data%20for%20learning%0Amodels%2C%20with%20the%20goal%20of%20facilitating%20knowledge%20sharing%20between%20similar%0Asystems.%20A%20case%20study%20using%20car-like%20vehicles%20compares%20traditional%20and%0Adimensionless%20models%20on%20simulated%20and%20experimental%20data%20to%20validate%20the%0Abenefits%20of%20the%20new%20dimensionless%20learning%20approach.%20Prediction%20accuracy%0Aimprovements%20with%20the%20dimensionless%20scheme%20when%20using%20a%20shared%20database%2C%20that%0Ais%2C%20predicting%20the%20motion%20of%20a%20vehicle%20based%20on%20data%20from%20various%20different%0Avehicles%20was%20found%20to%20be%20480%5C%25%20more%20accurate%20for%20predicting%20a%20simple%20no-slip%0Amaneuver%20based%20on%20simulated%20data%20and%2011%5C%25%20more%20accurate%20to%20predict%20a%20highly%0Adynamic%20braking%20maneuver%20based%20on%20experimental%20data.%20A%20modified%0Aphysics-informed%20learning%20scheme%20with%20hand-crafted%20dimensionless%20features%20was%0Aalso%20shown%20to%20increase%20the%20improvement%20to%20precision%20gains%20of%20917%5C%25%20and%2028%5C%25%0Arespectively.%20A%20comparative%20study%20also%20shows%20that%20using%20Buckingham%27s%20%24%5Cpi%24%0Atheorem%20is%20a%20much%20more%20effective%20preprocessing%20step%20for%20this%20task%20than%0Aprincipal%20component%20analysis%20%28PCA%29%20or%20simply%20normalizing%20the%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.17545v2&entry.124074799=Read"},
{"title": "Attention Incorporated Network for Sharing Low-rank, Image and K-space\n  Information during MR Image Reconstruction to Achieve Single Breath-hold\n  Cardiac Cine Imaging", "author": "Siying Xu and Kerstin Hammernik and Andreas Lingg and Jens Kuebler and Patrick Krumm and Daniel Rueckert and Sergios Gatidis and Thomas Kuestner", "abstract": "  Cardiac Cine Magnetic Resonance Imaging (MRI) provides an accurate assessment\nof heart morphology and function in clinical practice. However, MRI requires\nlong acquisition times, with recent deep learning-based methods showing great\npromise to accelerate imaging and enhance reconstruction quality. Existing\nnetworks exhibit some common limitations that constrain further acceleration\npossibilities, including single-domain learning, reliance on a single\nregularization term, and equal feature contribution. To address these\nlimitations, we propose to embed information from multiple domains, including\nlow-rank, image, and k-space, in a novel deep learning network for MRI\nreconstruction, which we denote as A-LIKNet. A-LIKNet adopts a parallel-branch\nstructure, enabling independent learning in the k-space and image domain.\nCoupled information sharing layers realize the information exchange between\ndomains. Furthermore, we introduce attention mechanisms into the network to\nassign greater weights to more critical coils or important temporal frames.\nTraining and testing were conducted on an in-house dataset, including 91\ncardiovascular patients and 38 healthy subjects scanned with 2D cardiac Cine\nusing retrospective undersampling. Additionally, we evaluated A-LIKNet on the\nreal-time 8x prospectively undersampled data from the OCMR dataset. The results\ndemonstrate that our proposed A-LIKNet outperforms existing methods and\nprovides high-quality reconstructions. The network can effectively reconstruct\nhighly retrospectively undersampled dynamic MR images up to 24x accelerations,\nindicating its potential for single breath-hold imaging.\n", "link": "http://arxiv.org/abs/2407.03034v1", "date": "2024-07-03", "relevancy": 2.0661, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5197}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5159}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5136}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention%20Incorporated%20Network%20for%20Sharing%20Low-rank%2C%20Image%20and%20K-space%0A%20%20Information%20during%20MR%20Image%20Reconstruction%20to%20Achieve%20Single%20Breath-hold%0A%20%20Cardiac%20Cine%20Imaging&body=Title%3A%20Attention%20Incorporated%20Network%20for%20Sharing%20Low-rank%2C%20Image%20and%20K-space%0A%20%20Information%20during%20MR%20Image%20Reconstruction%20to%20Achieve%20Single%20Breath-hold%0A%20%20Cardiac%20Cine%20Imaging%0AAuthor%3A%20Siying%20Xu%20and%20Kerstin%20Hammernik%20and%20Andreas%20Lingg%20and%20Jens%20Kuebler%20and%20Patrick%20Krumm%20and%20Daniel%20Rueckert%20and%20Sergios%20Gatidis%20and%20Thomas%20Kuestner%0AAbstract%3A%20%20%20Cardiac%20Cine%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20provides%20an%20accurate%20assessment%0Aof%20heart%20morphology%20and%20function%20in%20clinical%20practice.%20However%2C%20MRI%20requires%0Along%20acquisition%20times%2C%20with%20recent%20deep%20learning-based%20methods%20showing%20great%0Apromise%20to%20accelerate%20imaging%20and%20enhance%20reconstruction%20quality.%20Existing%0Anetworks%20exhibit%20some%20common%20limitations%20that%20constrain%20further%20acceleration%0Apossibilities%2C%20including%20single-domain%20learning%2C%20reliance%20on%20a%20single%0Aregularization%20term%2C%20and%20equal%20feature%20contribution.%20To%20address%20these%0Alimitations%2C%20we%20propose%20to%20embed%20information%20from%20multiple%20domains%2C%20including%0Alow-rank%2C%20image%2C%20and%20k-space%2C%20in%20a%20novel%20deep%20learning%20network%20for%20MRI%0Areconstruction%2C%20which%20we%20denote%20as%20A-LIKNet.%20A-LIKNet%20adopts%20a%20parallel-branch%0Astructure%2C%20enabling%20independent%20learning%20in%20the%20k-space%20and%20image%20domain.%0ACoupled%20information%20sharing%20layers%20realize%20the%20information%20exchange%20between%0Adomains.%20Furthermore%2C%20we%20introduce%20attention%20mechanisms%20into%20the%20network%20to%0Aassign%20greater%20weights%20to%20more%20critical%20coils%20or%20important%20temporal%20frames.%0ATraining%20and%20testing%20were%20conducted%20on%20an%20in-house%20dataset%2C%20including%2091%0Acardiovascular%20patients%20and%2038%20healthy%20subjects%20scanned%20with%202D%20cardiac%20Cine%0Ausing%20retrospective%20undersampling.%20Additionally%2C%20we%20evaluated%20A-LIKNet%20on%20the%0Areal-time%208x%20prospectively%20undersampled%20data%20from%20the%20OCMR%20dataset.%20The%20results%0Ademonstrate%20that%20our%20proposed%20A-LIKNet%20outperforms%20existing%20methods%20and%0Aprovides%20high-quality%20reconstructions.%20The%20network%20can%20effectively%20reconstruct%0Ahighly%20retrospectively%20undersampled%20dynamic%20MR%20images%20up%20to%2024x%20accelerations%2C%0Aindicating%20its%20potential%20for%20single%20breath-hold%20imaging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03034v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention%2520Incorporated%2520Network%2520for%2520Sharing%2520Low-rank%252C%2520Image%2520and%2520K-space%250A%2520%2520Information%2520during%2520MR%2520Image%2520Reconstruction%2520to%2520Achieve%2520Single%2520Breath-hold%250A%2520%2520Cardiac%2520Cine%2520Imaging%26entry.906535625%3DSiying%2520Xu%2520and%2520Kerstin%2520Hammernik%2520and%2520Andreas%2520Lingg%2520and%2520Jens%2520Kuebler%2520and%2520Patrick%2520Krumm%2520and%2520Daniel%2520Rueckert%2520and%2520Sergios%2520Gatidis%2520and%2520Thomas%2520Kuestner%26entry.1292438233%3D%2520%2520Cardiac%2520Cine%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520provides%2520an%2520accurate%2520assessment%250Aof%2520heart%2520morphology%2520and%2520function%2520in%2520clinical%2520practice.%2520However%252C%2520MRI%2520requires%250Along%2520acquisition%2520times%252C%2520with%2520recent%2520deep%2520learning-based%2520methods%2520showing%2520great%250Apromise%2520to%2520accelerate%2520imaging%2520and%2520enhance%2520reconstruction%2520quality.%2520Existing%250Anetworks%2520exhibit%2520some%2520common%2520limitations%2520that%2520constrain%2520further%2520acceleration%250Apossibilities%252C%2520including%2520single-domain%2520learning%252C%2520reliance%2520on%2520a%2520single%250Aregularization%2520term%252C%2520and%2520equal%2520feature%2520contribution.%2520To%2520address%2520these%250Alimitations%252C%2520we%2520propose%2520to%2520embed%2520information%2520from%2520multiple%2520domains%252C%2520including%250Alow-rank%252C%2520image%252C%2520and%2520k-space%252C%2520in%2520a%2520novel%2520deep%2520learning%2520network%2520for%2520MRI%250Areconstruction%252C%2520which%2520we%2520denote%2520as%2520A-LIKNet.%2520A-LIKNet%2520adopts%2520a%2520parallel-branch%250Astructure%252C%2520enabling%2520independent%2520learning%2520in%2520the%2520k-space%2520and%2520image%2520domain.%250ACoupled%2520information%2520sharing%2520layers%2520realize%2520the%2520information%2520exchange%2520between%250Adomains.%2520Furthermore%252C%2520we%2520introduce%2520attention%2520mechanisms%2520into%2520the%2520network%2520to%250Aassign%2520greater%2520weights%2520to%2520more%2520critical%2520coils%2520or%2520important%2520temporal%2520frames.%250ATraining%2520and%2520testing%2520were%2520conducted%2520on%2520an%2520in-house%2520dataset%252C%2520including%252091%250Acardiovascular%2520patients%2520and%252038%2520healthy%2520subjects%2520scanned%2520with%25202D%2520cardiac%2520Cine%250Ausing%2520retrospective%2520undersampling.%2520Additionally%252C%2520we%2520evaluated%2520A-LIKNet%2520on%2520the%250Areal-time%25208x%2520prospectively%2520undersampled%2520data%2520from%2520the%2520OCMR%2520dataset.%2520The%2520results%250Ademonstrate%2520that%2520our%2520proposed%2520A-LIKNet%2520outperforms%2520existing%2520methods%2520and%250Aprovides%2520high-quality%2520reconstructions.%2520The%2520network%2520can%2520effectively%2520reconstruct%250Ahighly%2520retrospectively%2520undersampled%2520dynamic%2520MR%2520images%2520up%2520to%252024x%2520accelerations%252C%250Aindicating%2520its%2520potential%2520for%2520single%2520breath-hold%2520imaging.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03034v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention%20Incorporated%20Network%20for%20Sharing%20Low-rank%2C%20Image%20and%20K-space%0A%20%20Information%20during%20MR%20Image%20Reconstruction%20to%20Achieve%20Single%20Breath-hold%0A%20%20Cardiac%20Cine%20Imaging&entry.906535625=Siying%20Xu%20and%20Kerstin%20Hammernik%20and%20Andreas%20Lingg%20and%20Jens%20Kuebler%20and%20Patrick%20Krumm%20and%20Daniel%20Rueckert%20and%20Sergios%20Gatidis%20and%20Thomas%20Kuestner&entry.1292438233=%20%20Cardiac%20Cine%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20provides%20an%20accurate%20assessment%0Aof%20heart%20morphology%20and%20function%20in%20clinical%20practice.%20However%2C%20MRI%20requires%0Along%20acquisition%20times%2C%20with%20recent%20deep%20learning-based%20methods%20showing%20great%0Apromise%20to%20accelerate%20imaging%20and%20enhance%20reconstruction%20quality.%20Existing%0Anetworks%20exhibit%20some%20common%20limitations%20that%20constrain%20further%20acceleration%0Apossibilities%2C%20including%20single-domain%20learning%2C%20reliance%20on%20a%20single%0Aregularization%20term%2C%20and%20equal%20feature%20contribution.%20To%20address%20these%0Alimitations%2C%20we%20propose%20to%20embed%20information%20from%20multiple%20domains%2C%20including%0Alow-rank%2C%20image%2C%20and%20k-space%2C%20in%20a%20novel%20deep%20learning%20network%20for%20MRI%0Areconstruction%2C%20which%20we%20denote%20as%20A-LIKNet.%20A-LIKNet%20adopts%20a%20parallel-branch%0Astructure%2C%20enabling%20independent%20learning%20in%20the%20k-space%20and%20image%20domain.%0ACoupled%20information%20sharing%20layers%20realize%20the%20information%20exchange%20between%0Adomains.%20Furthermore%2C%20we%20introduce%20attention%20mechanisms%20into%20the%20network%20to%0Aassign%20greater%20weights%20to%20more%20critical%20coils%20or%20important%20temporal%20frames.%0ATraining%20and%20testing%20were%20conducted%20on%20an%20in-house%20dataset%2C%20including%2091%0Acardiovascular%20patients%20and%2038%20healthy%20subjects%20scanned%20with%202D%20cardiac%20Cine%0Ausing%20retrospective%20undersampling.%20Additionally%2C%20we%20evaluated%20A-LIKNet%20on%20the%0Areal-time%208x%20prospectively%20undersampled%20data%20from%20the%20OCMR%20dataset.%20The%20results%0Ademonstrate%20that%20our%20proposed%20A-LIKNet%20outperforms%20existing%20methods%20and%0Aprovides%20high-quality%20reconstructions.%20The%20network%20can%20effectively%20reconstruct%0Ahighly%20retrospectively%20undersampled%20dynamic%20MR%20images%20up%20to%2024x%20accelerations%2C%0Aindicating%20its%20potential%20for%20single%20breath-hold%20imaging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03034v1&entry.124074799=Read"},
{"title": "IM-MoCo: Self-supervised MRI Motion Correction using Motion-Guided\n  Implicit Neural Representations", "author": "Ziad Al-Haj Hemidi and Christian Weihsbach and Mattias P. Heinrich", "abstract": "  Motion artifacts in Magnetic Resonance Imaging (MRI) arise due to relatively\nlong acquisition times and can compromise the clinical utility of acquired\nimages. Traditional motion correction methods often fail to address severe\nmotion, leading to distorted and unreliable results. Deep Learning (DL)\nalleviated such pitfalls through generalization with the cost of vanishing\nstructures and hallucinations, making it challenging to apply in the medical\nfield where hallucinated structures can tremendously impact the diagnostic\noutcome. In this work, we present an instance-wise motion correction pipeline\nthat leverages motion-guided Implicit Neural Representations (INRs) to mitigate\nthe impact of motion artifacts while retaining anatomical structure. Our method\nis evaluated using the NYU fastMRI dataset with different degrees of simulated\nmotion severity. For the correction alone, we can improve over state-of-the-art\nimage reconstruction methods by $+5\\%$ SSIM, $+5\\:db$ PSNR, and $+14\\%$\nHaarPSI. Clinical relevance is demonstrated by a subsequent experiment, where\nour method improves classification outcomes by at least $+1.5$ accuracy\npercentage points compared to motion-corrupted images.\n", "link": "http://arxiv.org/abs/2407.02974v1", "date": "2024-07-03", "relevancy": 2.0601, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5254}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.525}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5006}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IM-MoCo%3A%20Self-supervised%20MRI%20Motion%20Correction%20using%20Motion-Guided%0A%20%20Implicit%20Neural%20Representations&body=Title%3A%20IM-MoCo%3A%20Self-supervised%20MRI%20Motion%20Correction%20using%20Motion-Guided%0A%20%20Implicit%20Neural%20Representations%0AAuthor%3A%20Ziad%20Al-Haj%20Hemidi%20and%20Christian%20Weihsbach%20and%20Mattias%20P.%20Heinrich%0AAbstract%3A%20%20%20Motion%20artifacts%20in%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20arise%20due%20to%20relatively%0Along%20acquisition%20times%20and%20can%20compromise%20the%20clinical%20utility%20of%20acquired%0Aimages.%20Traditional%20motion%20correction%20methods%20often%20fail%20to%20address%20severe%0Amotion%2C%20leading%20to%20distorted%20and%20unreliable%20results.%20Deep%20Learning%20%28DL%29%0Aalleviated%20such%20pitfalls%20through%20generalization%20with%20the%20cost%20of%20vanishing%0Astructures%20and%20hallucinations%2C%20making%20it%20challenging%20to%20apply%20in%20the%20medical%0Afield%20where%20hallucinated%20structures%20can%20tremendously%20impact%20the%20diagnostic%0Aoutcome.%20In%20this%20work%2C%20we%20present%20an%20instance-wise%20motion%20correction%20pipeline%0Athat%20leverages%20motion-guided%20Implicit%20Neural%20Representations%20%28INRs%29%20to%20mitigate%0Athe%20impact%20of%20motion%20artifacts%20while%20retaining%20anatomical%20structure.%20Our%20method%0Ais%20evaluated%20using%20the%20NYU%20fastMRI%20dataset%20with%20different%20degrees%20of%20simulated%0Amotion%20severity.%20For%20the%20correction%20alone%2C%20we%20can%20improve%20over%20state-of-the-art%0Aimage%20reconstruction%20methods%20by%20%24%2B5%5C%25%24%20SSIM%2C%20%24%2B5%5C%3Adb%24%20PSNR%2C%20and%20%24%2B14%5C%25%24%0AHaarPSI.%20Clinical%20relevance%20is%20demonstrated%20by%20a%20subsequent%20experiment%2C%20where%0Aour%20method%20improves%20classification%20outcomes%20by%20at%20least%20%24%2B1.5%24%20accuracy%0Apercentage%20points%20compared%20to%20motion-corrupted%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02974v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIM-MoCo%253A%2520Self-supervised%2520MRI%2520Motion%2520Correction%2520using%2520Motion-Guided%250A%2520%2520Implicit%2520Neural%2520Representations%26entry.906535625%3DZiad%2520Al-Haj%2520Hemidi%2520and%2520Christian%2520Weihsbach%2520and%2520Mattias%2520P.%2520Heinrich%26entry.1292438233%3D%2520%2520Motion%2520artifacts%2520in%2520Magnetic%2520Resonance%2520Imaging%2520%2528MRI%2529%2520arise%2520due%2520to%2520relatively%250Along%2520acquisition%2520times%2520and%2520can%2520compromise%2520the%2520clinical%2520utility%2520of%2520acquired%250Aimages.%2520Traditional%2520motion%2520correction%2520methods%2520often%2520fail%2520to%2520address%2520severe%250Amotion%252C%2520leading%2520to%2520distorted%2520and%2520unreliable%2520results.%2520Deep%2520Learning%2520%2528DL%2529%250Aalleviated%2520such%2520pitfalls%2520through%2520generalization%2520with%2520the%2520cost%2520of%2520vanishing%250Astructures%2520and%2520hallucinations%252C%2520making%2520it%2520challenging%2520to%2520apply%2520in%2520the%2520medical%250Afield%2520where%2520hallucinated%2520structures%2520can%2520tremendously%2520impact%2520the%2520diagnostic%250Aoutcome.%2520In%2520this%2520work%252C%2520we%2520present%2520an%2520instance-wise%2520motion%2520correction%2520pipeline%250Athat%2520leverages%2520motion-guided%2520Implicit%2520Neural%2520Representations%2520%2528INRs%2529%2520to%2520mitigate%250Athe%2520impact%2520of%2520motion%2520artifacts%2520while%2520retaining%2520anatomical%2520structure.%2520Our%2520method%250Ais%2520evaluated%2520using%2520the%2520NYU%2520fastMRI%2520dataset%2520with%2520different%2520degrees%2520of%2520simulated%250Amotion%2520severity.%2520For%2520the%2520correction%2520alone%252C%2520we%2520can%2520improve%2520over%2520state-of-the-art%250Aimage%2520reconstruction%2520methods%2520by%2520%2524%252B5%255C%2525%2524%2520SSIM%252C%2520%2524%252B5%255C%253Adb%2524%2520PSNR%252C%2520and%2520%2524%252B14%255C%2525%2524%250AHaarPSI.%2520Clinical%2520relevance%2520is%2520demonstrated%2520by%2520a%2520subsequent%2520experiment%252C%2520where%250Aour%2520method%2520improves%2520classification%2520outcomes%2520by%2520at%2520least%2520%2524%252B1.5%2524%2520accuracy%250Apercentage%2520points%2520compared%2520to%2520motion-corrupted%2520images.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02974v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IM-MoCo%3A%20Self-supervised%20MRI%20Motion%20Correction%20using%20Motion-Guided%0A%20%20Implicit%20Neural%20Representations&entry.906535625=Ziad%20Al-Haj%20Hemidi%20and%20Christian%20Weihsbach%20and%20Mattias%20P.%20Heinrich&entry.1292438233=%20%20Motion%20artifacts%20in%20Magnetic%20Resonance%20Imaging%20%28MRI%29%20arise%20due%20to%20relatively%0Along%20acquisition%20times%20and%20can%20compromise%20the%20clinical%20utility%20of%20acquired%0Aimages.%20Traditional%20motion%20correction%20methods%20often%20fail%20to%20address%20severe%0Amotion%2C%20leading%20to%20distorted%20and%20unreliable%20results.%20Deep%20Learning%20%28DL%29%0Aalleviated%20such%20pitfalls%20through%20generalization%20with%20the%20cost%20of%20vanishing%0Astructures%20and%20hallucinations%2C%20making%20it%20challenging%20to%20apply%20in%20the%20medical%0Afield%20where%20hallucinated%20structures%20can%20tremendously%20impact%20the%20diagnostic%0Aoutcome.%20In%20this%20work%2C%20we%20present%20an%20instance-wise%20motion%20correction%20pipeline%0Athat%20leverages%20motion-guided%20Implicit%20Neural%20Representations%20%28INRs%29%20to%20mitigate%0Athe%20impact%20of%20motion%20artifacts%20while%20retaining%20anatomical%20structure.%20Our%20method%0Ais%20evaluated%20using%20the%20NYU%20fastMRI%20dataset%20with%20different%20degrees%20of%20simulated%0Amotion%20severity.%20For%20the%20correction%20alone%2C%20we%20can%20improve%20over%20state-of-the-art%0Aimage%20reconstruction%20methods%20by%20%24%2B5%5C%25%24%20SSIM%2C%20%24%2B5%5C%3Adb%24%20PSNR%2C%20and%20%24%2B14%5C%25%24%0AHaarPSI.%20Clinical%20relevance%20is%20demonstrated%20by%20a%20subsequent%20experiment%2C%20where%0Aour%20method%20improves%20classification%20outcomes%20by%20at%20least%20%24%2B1.5%24%20accuracy%0Apercentage%20points%20compared%20to%20motion-corrupted%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02974v1&entry.124074799=Read"},
{"title": "KeyVideoLLM: Towards Large-scale Video Keyframe Selection", "author": "Hao Liang and Jiapeng Li and Tianyi Bai and Chong Chen and Conghui He and Bin Cui and Wentao Zhang", "abstract": "  Recently, with the rise of web videos, managing and understanding large-scale\nvideo datasets has become increasingly important. Video Large Language Models\n(VideoLLMs) have emerged in recent years due to their strong video\nunderstanding capabilities. However, training and inference processes for\nVideoLLMs demand vast amounts of data, presenting significant challenges to\ndata management, particularly regarding efficiency, robustness, and\neffectiveness. In this work, we present KeyVideoLLM, a text-video frame\nsimilarity-based keyframe selection method designed to manage VideoLLM data\nefficiently, robustly, and effectively. Specifically, KeyVideoLLM achieves a\nremarkable data compression rate of up to 60.9 times, substantially lowering\ndisk space requirements, which proves its high efficiency. Additionally, it\nmaintains a 100% selection success rate across all video formats and scales,\nenhances processing speed by up to 200 times compared to existing keyframe\nselection methods, and does not require hyperparameter tuning. Beyond its\noutstanding efficiency and robustness, KeyVideoLLM further improves model\nperformance in video question-answering tasks during both training and\ninference stages. Notably, it consistently achieved the state-of-the-art (SoTA)\nexperimental results on diverse datasets.\n", "link": "http://arxiv.org/abs/2407.03104v1", "date": "2024-07-03", "relevancy": 2.0567, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5569}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5198}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4915}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KeyVideoLLM%3A%20Towards%20Large-scale%20Video%20Keyframe%20Selection&body=Title%3A%20KeyVideoLLM%3A%20Towards%20Large-scale%20Video%20Keyframe%20Selection%0AAuthor%3A%20Hao%20Liang%20and%20Jiapeng%20Li%20and%20Tianyi%20Bai%20and%20Chong%20Chen%20and%20Conghui%20He%20and%20Bin%20Cui%20and%20Wentao%20Zhang%0AAbstract%3A%20%20%20Recently%2C%20with%20the%20rise%20of%20web%20videos%2C%20managing%20and%20understanding%20large-scale%0Avideo%20datasets%20has%20become%20increasingly%20important.%20Video%20Large%20Language%20Models%0A%28VideoLLMs%29%20have%20emerged%20in%20recent%20years%20due%20to%20their%20strong%20video%0Aunderstanding%20capabilities.%20However%2C%20training%20and%20inference%20processes%20for%0AVideoLLMs%20demand%20vast%20amounts%20of%20data%2C%20presenting%20significant%20challenges%20to%0Adata%20management%2C%20particularly%20regarding%20efficiency%2C%20robustness%2C%20and%0Aeffectiveness.%20In%20this%20work%2C%20we%20present%20KeyVideoLLM%2C%20a%20text-video%20frame%0Asimilarity-based%20keyframe%20selection%20method%20designed%20to%20manage%20VideoLLM%20data%0Aefficiently%2C%20robustly%2C%20and%20effectively.%20Specifically%2C%20KeyVideoLLM%20achieves%20a%0Aremarkable%20data%20compression%20rate%20of%20up%20to%2060.9%20times%2C%20substantially%20lowering%0Adisk%20space%20requirements%2C%20which%20proves%20its%20high%20efficiency.%20Additionally%2C%20it%0Amaintains%20a%20100%25%20selection%20success%20rate%20across%20all%20video%20formats%20and%20scales%2C%0Aenhances%20processing%20speed%20by%20up%20to%20200%20times%20compared%20to%20existing%20keyframe%0Aselection%20methods%2C%20and%20does%20not%20require%20hyperparameter%20tuning.%20Beyond%20its%0Aoutstanding%20efficiency%20and%20robustness%2C%20KeyVideoLLM%20further%20improves%20model%0Aperformance%20in%20video%20question-answering%20tasks%20during%20both%20training%20and%0Ainference%20stages.%20Notably%2C%20it%20consistently%20achieved%20the%20state-of-the-art%20%28SoTA%29%0Aexperimental%20results%20on%20diverse%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03104v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKeyVideoLLM%253A%2520Towards%2520Large-scale%2520Video%2520Keyframe%2520Selection%26entry.906535625%3DHao%2520Liang%2520and%2520Jiapeng%2520Li%2520and%2520Tianyi%2520Bai%2520and%2520Chong%2520Chen%2520and%2520Conghui%2520He%2520and%2520Bin%2520Cui%2520and%2520Wentao%2520Zhang%26entry.1292438233%3D%2520%2520Recently%252C%2520with%2520the%2520rise%2520of%2520web%2520videos%252C%2520managing%2520and%2520understanding%2520large-scale%250Avideo%2520datasets%2520has%2520become%2520increasingly%2520important.%2520Video%2520Large%2520Language%2520Models%250A%2528VideoLLMs%2529%2520have%2520emerged%2520in%2520recent%2520years%2520due%2520to%2520their%2520strong%2520video%250Aunderstanding%2520capabilities.%2520However%252C%2520training%2520and%2520inference%2520processes%2520for%250AVideoLLMs%2520demand%2520vast%2520amounts%2520of%2520data%252C%2520presenting%2520significant%2520challenges%2520to%250Adata%2520management%252C%2520particularly%2520regarding%2520efficiency%252C%2520robustness%252C%2520and%250Aeffectiveness.%2520In%2520this%2520work%252C%2520we%2520present%2520KeyVideoLLM%252C%2520a%2520text-video%2520frame%250Asimilarity-based%2520keyframe%2520selection%2520method%2520designed%2520to%2520manage%2520VideoLLM%2520data%250Aefficiently%252C%2520robustly%252C%2520and%2520effectively.%2520Specifically%252C%2520KeyVideoLLM%2520achieves%2520a%250Aremarkable%2520data%2520compression%2520rate%2520of%2520up%2520to%252060.9%2520times%252C%2520substantially%2520lowering%250Adisk%2520space%2520requirements%252C%2520which%2520proves%2520its%2520high%2520efficiency.%2520Additionally%252C%2520it%250Amaintains%2520a%2520100%2525%2520selection%2520success%2520rate%2520across%2520all%2520video%2520formats%2520and%2520scales%252C%250Aenhances%2520processing%2520speed%2520by%2520up%2520to%2520200%2520times%2520compared%2520to%2520existing%2520keyframe%250Aselection%2520methods%252C%2520and%2520does%2520not%2520require%2520hyperparameter%2520tuning.%2520Beyond%2520its%250Aoutstanding%2520efficiency%2520and%2520robustness%252C%2520KeyVideoLLM%2520further%2520improves%2520model%250Aperformance%2520in%2520video%2520question-answering%2520tasks%2520during%2520both%2520training%2520and%250Ainference%2520stages.%2520Notably%252C%2520it%2520consistently%2520achieved%2520the%2520state-of-the-art%2520%2528SoTA%2529%250Aexperimental%2520results%2520on%2520diverse%2520datasets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03104v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KeyVideoLLM%3A%20Towards%20Large-scale%20Video%20Keyframe%20Selection&entry.906535625=Hao%20Liang%20and%20Jiapeng%20Li%20and%20Tianyi%20Bai%20and%20Chong%20Chen%20and%20Conghui%20He%20and%20Bin%20Cui%20and%20Wentao%20Zhang&entry.1292438233=%20%20Recently%2C%20with%20the%20rise%20of%20web%20videos%2C%20managing%20and%20understanding%20large-scale%0Avideo%20datasets%20has%20become%20increasingly%20important.%20Video%20Large%20Language%20Models%0A%28VideoLLMs%29%20have%20emerged%20in%20recent%20years%20due%20to%20their%20strong%20video%0Aunderstanding%20capabilities.%20However%2C%20training%20and%20inference%20processes%20for%0AVideoLLMs%20demand%20vast%20amounts%20of%20data%2C%20presenting%20significant%20challenges%20to%0Adata%20management%2C%20particularly%20regarding%20efficiency%2C%20robustness%2C%20and%0Aeffectiveness.%20In%20this%20work%2C%20we%20present%20KeyVideoLLM%2C%20a%20text-video%20frame%0Asimilarity-based%20keyframe%20selection%20method%20designed%20to%20manage%20VideoLLM%20data%0Aefficiently%2C%20robustly%2C%20and%20effectively.%20Specifically%2C%20KeyVideoLLM%20achieves%20a%0Aremarkable%20data%20compression%20rate%20of%20up%20to%2060.9%20times%2C%20substantially%20lowering%0Adisk%20space%20requirements%2C%20which%20proves%20its%20high%20efficiency.%20Additionally%2C%20it%0Amaintains%20a%20100%25%20selection%20success%20rate%20across%20all%20video%20formats%20and%20scales%2C%0Aenhances%20processing%20speed%20by%20up%20to%20200%20times%20compared%20to%20existing%20keyframe%0Aselection%20methods%2C%20and%20does%20not%20require%20hyperparameter%20tuning.%20Beyond%20its%0Aoutstanding%20efficiency%20and%20robustness%2C%20KeyVideoLLM%20further%20improves%20model%0Aperformance%20in%20video%20question-answering%20tasks%20during%20both%20training%20and%0Ainference%20stages.%20Notably%2C%20it%20consistently%20achieved%20the%20state-of-the-art%20%28SoTA%29%0Aexperimental%20results%20on%20diverse%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03104v1&entry.124074799=Read"},
{"title": "Artificial Inductive Bias for Synthetic Tabular Data Generation in\n  Data-Scarce Scenarios", "author": "Patricia A. Apell\u00e1niz and Ana Jim\u00e9nez and Borja Arroyo Galende and Juan Parras and Santiago Zazo", "abstract": "  While synthetic tabular data generation using Deep Generative Models (DGMs)\noffers a compelling solution to data scarcity and privacy concerns, their\neffectiveness relies on substantial training data, often unavailable in\nreal-world applications. This paper addresses this challenge by proposing a\nnovel methodology for generating realistic and reliable synthetic tabular data\nwith DGMs in limited real-data environments. Our approach proposes several ways\nto generate an artificial inductive bias in a DGM through transfer learning and\nmeta-learning techniques. We explore and compare four different methods within\nthis framework, demonstrating that transfer learning strategies like\npre-training and model averaging outperform meta-learning approaches, like\nModel-Agnostic Meta-Learning, and Domain Randomized Search. We validate our\napproach using two state-of-the-art DGMs, namely, a Variational Autoencoder and\na Generative Adversarial Network, to show that our artificial inductive bias\nfuels superior synthetic data quality, as measured by Jensen-Shannon\ndivergence, achieving relative gains of up to 50\\% when using our proposed\napproach. This methodology has broad applicability in various DGMs and machine\nlearning tasks, particularly in areas like healthcare and finance, where data\nscarcity is often a critical issue.\n", "link": "http://arxiv.org/abs/2407.03080v1", "date": "2024-07-03", "relevancy": 2.0567, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5238}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5093}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Artificial%20Inductive%20Bias%20for%20Synthetic%20Tabular%20Data%20Generation%20in%0A%20%20Data-Scarce%20Scenarios&body=Title%3A%20Artificial%20Inductive%20Bias%20for%20Synthetic%20Tabular%20Data%20Generation%20in%0A%20%20Data-Scarce%20Scenarios%0AAuthor%3A%20Patricia%20A.%20Apell%C3%A1niz%20and%20Ana%20Jim%C3%A9nez%20and%20Borja%20Arroyo%20Galende%20and%20Juan%20Parras%20and%20Santiago%20Zazo%0AAbstract%3A%20%20%20While%20synthetic%20tabular%20data%20generation%20using%20Deep%20Generative%20Models%20%28DGMs%29%0Aoffers%20a%20compelling%20solution%20to%20data%20scarcity%20and%20privacy%20concerns%2C%20their%0Aeffectiveness%20relies%20on%20substantial%20training%20data%2C%20often%20unavailable%20in%0Areal-world%20applications.%20This%20paper%20addresses%20this%20challenge%20by%20proposing%20a%0Anovel%20methodology%20for%20generating%20realistic%20and%20reliable%20synthetic%20tabular%20data%0Awith%20DGMs%20in%20limited%20real-data%20environments.%20Our%20approach%20proposes%20several%20ways%0Ato%20generate%20an%20artificial%20inductive%20bias%20in%20a%20DGM%20through%20transfer%20learning%20and%0Ameta-learning%20techniques.%20We%20explore%20and%20compare%20four%20different%20methods%20within%0Athis%20framework%2C%20demonstrating%20that%20transfer%20learning%20strategies%20like%0Apre-training%20and%20model%20averaging%20outperform%20meta-learning%20approaches%2C%20like%0AModel-Agnostic%20Meta-Learning%2C%20and%20Domain%20Randomized%20Search.%20We%20validate%20our%0Aapproach%20using%20two%20state-of-the-art%20DGMs%2C%20namely%2C%20a%20Variational%20Autoencoder%20and%0Aa%20Generative%20Adversarial%20Network%2C%20to%20show%20that%20our%20artificial%20inductive%20bias%0Afuels%20superior%20synthetic%20data%20quality%2C%20as%20measured%20by%20Jensen-Shannon%0Adivergence%2C%20achieving%20relative%20gains%20of%20up%20to%2050%5C%25%20when%20using%20our%20proposed%0Aapproach.%20This%20methodology%20has%20broad%20applicability%20in%20various%20DGMs%20and%20machine%0Alearning%20tasks%2C%20particularly%20in%20areas%20like%20healthcare%20and%20finance%2C%20where%20data%0Ascarcity%20is%20often%20a%20critical%20issue.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03080v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DArtificial%2520Inductive%2520Bias%2520for%2520Synthetic%2520Tabular%2520Data%2520Generation%2520in%250A%2520%2520Data-Scarce%2520Scenarios%26entry.906535625%3DPatricia%2520A.%2520Apell%25C3%25A1niz%2520and%2520Ana%2520Jim%25C3%25A9nez%2520and%2520Borja%2520Arroyo%2520Galende%2520and%2520Juan%2520Parras%2520and%2520Santiago%2520Zazo%26entry.1292438233%3D%2520%2520While%2520synthetic%2520tabular%2520data%2520generation%2520using%2520Deep%2520Generative%2520Models%2520%2528DGMs%2529%250Aoffers%2520a%2520compelling%2520solution%2520to%2520data%2520scarcity%2520and%2520privacy%2520concerns%252C%2520their%250Aeffectiveness%2520relies%2520on%2520substantial%2520training%2520data%252C%2520often%2520unavailable%2520in%250Areal-world%2520applications.%2520This%2520paper%2520addresses%2520this%2520challenge%2520by%2520proposing%2520a%250Anovel%2520methodology%2520for%2520generating%2520realistic%2520and%2520reliable%2520synthetic%2520tabular%2520data%250Awith%2520DGMs%2520in%2520limited%2520real-data%2520environments.%2520Our%2520approach%2520proposes%2520several%2520ways%250Ato%2520generate%2520an%2520artificial%2520inductive%2520bias%2520in%2520a%2520DGM%2520through%2520transfer%2520learning%2520and%250Ameta-learning%2520techniques.%2520We%2520explore%2520and%2520compare%2520four%2520different%2520methods%2520within%250Athis%2520framework%252C%2520demonstrating%2520that%2520transfer%2520learning%2520strategies%2520like%250Apre-training%2520and%2520model%2520averaging%2520outperform%2520meta-learning%2520approaches%252C%2520like%250AModel-Agnostic%2520Meta-Learning%252C%2520and%2520Domain%2520Randomized%2520Search.%2520We%2520validate%2520our%250Aapproach%2520using%2520two%2520state-of-the-art%2520DGMs%252C%2520namely%252C%2520a%2520Variational%2520Autoencoder%2520and%250Aa%2520Generative%2520Adversarial%2520Network%252C%2520to%2520show%2520that%2520our%2520artificial%2520inductive%2520bias%250Afuels%2520superior%2520synthetic%2520data%2520quality%252C%2520as%2520measured%2520by%2520Jensen-Shannon%250Adivergence%252C%2520achieving%2520relative%2520gains%2520of%2520up%2520to%252050%255C%2525%2520when%2520using%2520our%2520proposed%250Aapproach.%2520This%2520methodology%2520has%2520broad%2520applicability%2520in%2520various%2520DGMs%2520and%2520machine%250Alearning%2520tasks%252C%2520particularly%2520in%2520areas%2520like%2520healthcare%2520and%2520finance%252C%2520where%2520data%250Ascarcity%2520is%2520often%2520a%2520critical%2520issue.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03080v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Artificial%20Inductive%20Bias%20for%20Synthetic%20Tabular%20Data%20Generation%20in%0A%20%20Data-Scarce%20Scenarios&entry.906535625=Patricia%20A.%20Apell%C3%A1niz%20and%20Ana%20Jim%C3%A9nez%20and%20Borja%20Arroyo%20Galende%20and%20Juan%20Parras%20and%20Santiago%20Zazo&entry.1292438233=%20%20While%20synthetic%20tabular%20data%20generation%20using%20Deep%20Generative%20Models%20%28DGMs%29%0Aoffers%20a%20compelling%20solution%20to%20data%20scarcity%20and%20privacy%20concerns%2C%20their%0Aeffectiveness%20relies%20on%20substantial%20training%20data%2C%20often%20unavailable%20in%0Areal-world%20applications.%20This%20paper%20addresses%20this%20challenge%20by%20proposing%20a%0Anovel%20methodology%20for%20generating%20realistic%20and%20reliable%20synthetic%20tabular%20data%0Awith%20DGMs%20in%20limited%20real-data%20environments.%20Our%20approach%20proposes%20several%20ways%0Ato%20generate%20an%20artificial%20inductive%20bias%20in%20a%20DGM%20through%20transfer%20learning%20and%0Ameta-learning%20techniques.%20We%20explore%20and%20compare%20four%20different%20methods%20within%0Athis%20framework%2C%20demonstrating%20that%20transfer%20learning%20strategies%20like%0Apre-training%20and%20model%20averaging%20outperform%20meta-learning%20approaches%2C%20like%0AModel-Agnostic%20Meta-Learning%2C%20and%20Domain%20Randomized%20Search.%20We%20validate%20our%0Aapproach%20using%20two%20state-of-the-art%20DGMs%2C%20namely%2C%20a%20Variational%20Autoencoder%20and%0Aa%20Generative%20Adversarial%20Network%2C%20to%20show%20that%20our%20artificial%20inductive%20bias%0Afuels%20superior%20synthetic%20data%20quality%2C%20as%20measured%20by%20Jensen-Shannon%0Adivergence%2C%20achieving%20relative%20gains%20of%20up%20to%2050%5C%25%20when%20using%20our%20proposed%0Aapproach.%20This%20methodology%20has%20broad%20applicability%20in%20various%20DGMs%20and%20machine%0Alearning%20tasks%2C%20particularly%20in%20areas%20like%20healthcare%20and%20finance%2C%20where%20data%0Ascarcity%20is%20often%20a%20critical%20issue.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03080v1&entry.124074799=Read"},
{"title": "Align and Aggregate: Compositional Reasoning with Video Alignment and\n  Answer Aggregation for Video Question-Answering", "author": "Zhaohe Liao and Jiangtong Li and Li Niu and Liqing Zhang", "abstract": "  Despite the recent progress made in Video Question-Answering (VideoQA), these\nmethods typically function as black-boxes, making it difficult to understand\ntheir reasoning processes and perform consistent compositional reasoning. To\naddress these challenges, we propose a \\textit{model-agnostic} Video Alignment\nand Answer Aggregation (VA$^{3}$) framework, which is capable of enhancing both\ncompositional consistency and accuracy of existing VidQA methods by integrating\nvideo aligner and answer aggregator modules. The video aligner hierarchically\nselects the relevant video clips based on the question, while the answer\naggregator deduces the answer to the question based on its sub-questions, with\ncompositional consistency ensured by the information flow along question\ndecomposition graph and the contrastive learning strategy. We evaluate our\nframework on three settings of the AGQA-Decomp dataset with three baseline\nmethods, and propose new metrics to measure the compositional consistency of\nVidQA methods more comprehensively. Moreover, we propose a large language model\n(LLM) based automatic question decomposition pipeline to apply our framework to\nany VidQA dataset. We extend MSVD and NExT-QA datasets with it to evaluate our\nVA$^3$ framework on broader scenarios. Extensive experiments show that our\nframework improves both compositional consistency and accuracy of existing\nmethods, leading to more interpretable real-world VidQA models.\n", "link": "http://arxiv.org/abs/2407.03008v1", "date": "2024-07-03", "relevancy": 2.0537, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5165}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5163}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4987}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Align%20and%20Aggregate%3A%20Compositional%20Reasoning%20with%20Video%20Alignment%20and%0A%20%20Answer%20Aggregation%20for%20Video%20Question-Answering&body=Title%3A%20Align%20and%20Aggregate%3A%20Compositional%20Reasoning%20with%20Video%20Alignment%20and%0A%20%20Answer%20Aggregation%20for%20Video%20Question-Answering%0AAuthor%3A%20Zhaohe%20Liao%20and%20Jiangtong%20Li%20and%20Li%20Niu%20and%20Liqing%20Zhang%0AAbstract%3A%20%20%20Despite%20the%20recent%20progress%20made%20in%20Video%20Question-Answering%20%28VideoQA%29%2C%20these%0Amethods%20typically%20function%20as%20black-boxes%2C%20making%20it%20difficult%20to%20understand%0Atheir%20reasoning%20processes%20and%20perform%20consistent%20compositional%20reasoning.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20%5Ctextit%7Bmodel-agnostic%7D%20Video%20Alignment%0Aand%20Answer%20Aggregation%20%28VA%24%5E%7B3%7D%24%29%20framework%2C%20which%20is%20capable%20of%20enhancing%20both%0Acompositional%20consistency%20and%20accuracy%20of%20existing%20VidQA%20methods%20by%20integrating%0Avideo%20aligner%20and%20answer%20aggregator%20modules.%20The%20video%20aligner%20hierarchically%0Aselects%20the%20relevant%20video%20clips%20based%20on%20the%20question%2C%20while%20the%20answer%0Aaggregator%20deduces%20the%20answer%20to%20the%20question%20based%20on%20its%20sub-questions%2C%20with%0Acompositional%20consistency%20ensured%20by%20the%20information%20flow%20along%20question%0Adecomposition%20graph%20and%20the%20contrastive%20learning%20strategy.%20We%20evaluate%20our%0Aframework%20on%20three%20settings%20of%20the%20AGQA-Decomp%20dataset%20with%20three%20baseline%0Amethods%2C%20and%20propose%20new%20metrics%20to%20measure%20the%20compositional%20consistency%20of%0AVidQA%20methods%20more%20comprehensively.%20Moreover%2C%20we%20propose%20a%20large%20language%20model%0A%28LLM%29%20based%20automatic%20question%20decomposition%20pipeline%20to%20apply%20our%20framework%20to%0Aany%20VidQA%20dataset.%20We%20extend%20MSVD%20and%20NExT-QA%20datasets%20with%20it%20to%20evaluate%20our%0AVA%24%5E3%24%20framework%20on%20broader%20scenarios.%20Extensive%20experiments%20show%20that%20our%0Aframework%20improves%20both%20compositional%20consistency%20and%20accuracy%20of%20existing%0Amethods%2C%20leading%20to%20more%20interpretable%20real-world%20VidQA%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03008v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlign%2520and%2520Aggregate%253A%2520Compositional%2520Reasoning%2520with%2520Video%2520Alignment%2520and%250A%2520%2520Answer%2520Aggregation%2520for%2520Video%2520Question-Answering%26entry.906535625%3DZhaohe%2520Liao%2520and%2520Jiangtong%2520Li%2520and%2520Li%2520Niu%2520and%2520Liqing%2520Zhang%26entry.1292438233%3D%2520%2520Despite%2520the%2520recent%2520progress%2520made%2520in%2520Video%2520Question-Answering%2520%2528VideoQA%2529%252C%2520these%250Amethods%2520typically%2520function%2520as%2520black-boxes%252C%2520making%2520it%2520difficult%2520to%2520understand%250Atheir%2520reasoning%2520processes%2520and%2520perform%2520consistent%2520compositional%2520reasoning.%2520To%250Aaddress%2520these%2520challenges%252C%2520we%2520propose%2520a%2520%255Ctextit%257Bmodel-agnostic%257D%2520Video%2520Alignment%250Aand%2520Answer%2520Aggregation%2520%2528VA%2524%255E%257B3%257D%2524%2529%2520framework%252C%2520which%2520is%2520capable%2520of%2520enhancing%2520both%250Acompositional%2520consistency%2520and%2520accuracy%2520of%2520existing%2520VidQA%2520methods%2520by%2520integrating%250Avideo%2520aligner%2520and%2520answer%2520aggregator%2520modules.%2520The%2520video%2520aligner%2520hierarchically%250Aselects%2520the%2520relevant%2520video%2520clips%2520based%2520on%2520the%2520question%252C%2520while%2520the%2520answer%250Aaggregator%2520deduces%2520the%2520answer%2520to%2520the%2520question%2520based%2520on%2520its%2520sub-questions%252C%2520with%250Acompositional%2520consistency%2520ensured%2520by%2520the%2520information%2520flow%2520along%2520question%250Adecomposition%2520graph%2520and%2520the%2520contrastive%2520learning%2520strategy.%2520We%2520evaluate%2520our%250Aframework%2520on%2520three%2520settings%2520of%2520the%2520AGQA-Decomp%2520dataset%2520with%2520three%2520baseline%250Amethods%252C%2520and%2520propose%2520new%2520metrics%2520to%2520measure%2520the%2520compositional%2520consistency%2520of%250AVidQA%2520methods%2520more%2520comprehensively.%2520Moreover%252C%2520we%2520propose%2520a%2520large%2520language%2520model%250A%2528LLM%2529%2520based%2520automatic%2520question%2520decomposition%2520pipeline%2520to%2520apply%2520our%2520framework%2520to%250Aany%2520VidQA%2520dataset.%2520We%2520extend%2520MSVD%2520and%2520NExT-QA%2520datasets%2520with%2520it%2520to%2520evaluate%2520our%250AVA%2524%255E3%2524%2520framework%2520on%2520broader%2520scenarios.%2520Extensive%2520experiments%2520show%2520that%2520our%250Aframework%2520improves%2520both%2520compositional%2520consistency%2520and%2520accuracy%2520of%2520existing%250Amethods%252C%2520leading%2520to%2520more%2520interpretable%2520real-world%2520VidQA%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03008v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Align%20and%20Aggregate%3A%20Compositional%20Reasoning%20with%20Video%20Alignment%20and%0A%20%20Answer%20Aggregation%20for%20Video%20Question-Answering&entry.906535625=Zhaohe%20Liao%20and%20Jiangtong%20Li%20and%20Li%20Niu%20and%20Liqing%20Zhang&entry.1292438233=%20%20Despite%20the%20recent%20progress%20made%20in%20Video%20Question-Answering%20%28VideoQA%29%2C%20these%0Amethods%20typically%20function%20as%20black-boxes%2C%20making%20it%20difficult%20to%20understand%0Atheir%20reasoning%20processes%20and%20perform%20consistent%20compositional%20reasoning.%20To%0Aaddress%20these%20challenges%2C%20we%20propose%20a%20%5Ctextit%7Bmodel-agnostic%7D%20Video%20Alignment%0Aand%20Answer%20Aggregation%20%28VA%24%5E%7B3%7D%24%29%20framework%2C%20which%20is%20capable%20of%20enhancing%20both%0Acompositional%20consistency%20and%20accuracy%20of%20existing%20VidQA%20methods%20by%20integrating%0Avideo%20aligner%20and%20answer%20aggregator%20modules.%20The%20video%20aligner%20hierarchically%0Aselects%20the%20relevant%20video%20clips%20based%20on%20the%20question%2C%20while%20the%20answer%0Aaggregator%20deduces%20the%20answer%20to%20the%20question%20based%20on%20its%20sub-questions%2C%20with%0Acompositional%20consistency%20ensured%20by%20the%20information%20flow%20along%20question%0Adecomposition%20graph%20and%20the%20contrastive%20learning%20strategy.%20We%20evaluate%20our%0Aframework%20on%20three%20settings%20of%20the%20AGQA-Decomp%20dataset%20with%20three%20baseline%0Amethods%2C%20and%20propose%20new%20metrics%20to%20measure%20the%20compositional%20consistency%20of%0AVidQA%20methods%20more%20comprehensively.%20Moreover%2C%20we%20propose%20a%20large%20language%20model%0A%28LLM%29%20based%20automatic%20question%20decomposition%20pipeline%20to%20apply%20our%20framework%20to%0Aany%20VidQA%20dataset.%20We%20extend%20MSVD%20and%20NExT-QA%20datasets%20with%20it%20to%20evaluate%20our%0AVA%24%5E3%24%20framework%20on%20broader%20scenarios.%20Extensive%20experiments%20show%20that%20our%0Aframework%20improves%20both%20compositional%20consistency%20and%20accuracy%20of%20existing%0Amethods%2C%20leading%20to%20more%20interpretable%20real-world%20VidQA%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03008v1&entry.124074799=Read"},
{"title": "Individual Tree Detection in Large-Scale Urban Environments using\n  High-Resolution Multispectral Imagery", "author": "Jonathan Ventura and Camille Pawlak and Milo Honsberger and Cameron Gonsalves and Julian Rice and Natalie L. R. Love and Skyler Han and Viet Nguyen and Keilana Sugano and Jacqueline Doremus and G. Andrew Fricker and Jenn Yost and Matt Ritter", "abstract": "  We introduce a novel deep learning method for detection of individual trees\nin urban environments using high-resolution multispectral aerial imagery. We\nuse a convolutional neural network to regress a confidence map indicating the\nlocations of individual trees, which are localized using a peak finding\nalgorithm. Our method provides complete spatial coverage by detecting trees in\nboth public and private spaces, and can scale to very large areas. We performed\na thorough evaluation of our method, supported by a new dataset of over 1,500\nimages and almost 100,000 tree annotations, covering eight cities, six climate\nzones, and three image capture years. We trained our model on data from\nSouthern California, and achieved a precision of 73.6% and recall of 73.3%\nusing test data from this region. We generally observed similar precision and\nslightly lower recall when extrapolating to other California climate zones and\nimage capture dates. We used our method to produce a map of trees in the entire\nurban forest of California, and estimated the total number of urban trees in\nCalifornia to be about 43.5 million. Our study indicates the potential for deep\nlearning methods to support future urban forestry studies at unprecedented\nscales.\n", "link": "http://arxiv.org/abs/2208.10607v4", "date": "2024-07-03", "relevancy": 2.0505, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5364}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4979}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4901}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Individual%20Tree%20Detection%20in%20Large-Scale%20Urban%20Environments%20using%0A%20%20High-Resolution%20Multispectral%20Imagery&body=Title%3A%20Individual%20Tree%20Detection%20in%20Large-Scale%20Urban%20Environments%20using%0A%20%20High-Resolution%20Multispectral%20Imagery%0AAuthor%3A%20Jonathan%20Ventura%20and%20Camille%20Pawlak%20and%20Milo%20Honsberger%20and%20Cameron%20Gonsalves%20and%20Julian%20Rice%20and%20Natalie%20L.%20R.%20Love%20and%20Skyler%20Han%20and%20Viet%20Nguyen%20and%20Keilana%20Sugano%20and%20Jacqueline%20Doremus%20and%20G.%20Andrew%20Fricker%20and%20Jenn%20Yost%20and%20Matt%20Ritter%0AAbstract%3A%20%20%20We%20introduce%20a%20novel%20deep%20learning%20method%20for%20detection%20of%20individual%20trees%0Ain%20urban%20environments%20using%20high-resolution%20multispectral%20aerial%20imagery.%20We%0Ause%20a%20convolutional%20neural%20network%20to%20regress%20a%20confidence%20map%20indicating%20the%0Alocations%20of%20individual%20trees%2C%20which%20are%20localized%20using%20a%20peak%20finding%0Aalgorithm.%20Our%20method%20provides%20complete%20spatial%20coverage%20by%20detecting%20trees%20in%0Aboth%20public%20and%20private%20spaces%2C%20and%20can%20scale%20to%20very%20large%20areas.%20We%20performed%0Aa%20thorough%20evaluation%20of%20our%20method%2C%20supported%20by%20a%20new%20dataset%20of%20over%201%2C500%0Aimages%20and%20almost%20100%2C000%20tree%20annotations%2C%20covering%20eight%20cities%2C%20six%20climate%0Azones%2C%20and%20three%20image%20capture%20years.%20We%20trained%20our%20model%20on%20data%20from%0ASouthern%20California%2C%20and%20achieved%20a%20precision%20of%2073.6%25%20and%20recall%20of%2073.3%25%0Ausing%20test%20data%20from%20this%20region.%20We%20generally%20observed%20similar%20precision%20and%0Aslightly%20lower%20recall%20when%20extrapolating%20to%20other%20California%20climate%20zones%20and%0Aimage%20capture%20dates.%20We%20used%20our%20method%20to%20produce%20a%20map%20of%20trees%20in%20the%20entire%0Aurban%20forest%20of%20California%2C%20and%20estimated%20the%20total%20number%20of%20urban%20trees%20in%0ACalifornia%20to%20be%20about%2043.5%20million.%20Our%20study%20indicates%20the%20potential%20for%20deep%0Alearning%20methods%20to%20support%20future%20urban%20forestry%20studies%20at%20unprecedented%0Ascales.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2208.10607v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIndividual%2520Tree%2520Detection%2520in%2520Large-Scale%2520Urban%2520Environments%2520using%250A%2520%2520High-Resolution%2520Multispectral%2520Imagery%26entry.906535625%3DJonathan%2520Ventura%2520and%2520Camille%2520Pawlak%2520and%2520Milo%2520Honsberger%2520and%2520Cameron%2520Gonsalves%2520and%2520Julian%2520Rice%2520and%2520Natalie%2520L.%2520R.%2520Love%2520and%2520Skyler%2520Han%2520and%2520Viet%2520Nguyen%2520and%2520Keilana%2520Sugano%2520and%2520Jacqueline%2520Doremus%2520and%2520G.%2520Andrew%2520Fricker%2520and%2520Jenn%2520Yost%2520and%2520Matt%2520Ritter%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520novel%2520deep%2520learning%2520method%2520for%2520detection%2520of%2520individual%2520trees%250Ain%2520urban%2520environments%2520using%2520high-resolution%2520multispectral%2520aerial%2520imagery.%2520We%250Ause%2520a%2520convolutional%2520neural%2520network%2520to%2520regress%2520a%2520confidence%2520map%2520indicating%2520the%250Alocations%2520of%2520individual%2520trees%252C%2520which%2520are%2520localized%2520using%2520a%2520peak%2520finding%250Aalgorithm.%2520Our%2520method%2520provides%2520complete%2520spatial%2520coverage%2520by%2520detecting%2520trees%2520in%250Aboth%2520public%2520and%2520private%2520spaces%252C%2520and%2520can%2520scale%2520to%2520very%2520large%2520areas.%2520We%2520performed%250Aa%2520thorough%2520evaluation%2520of%2520our%2520method%252C%2520supported%2520by%2520a%2520new%2520dataset%2520of%2520over%25201%252C500%250Aimages%2520and%2520almost%2520100%252C000%2520tree%2520annotations%252C%2520covering%2520eight%2520cities%252C%2520six%2520climate%250Azones%252C%2520and%2520three%2520image%2520capture%2520years.%2520We%2520trained%2520our%2520model%2520on%2520data%2520from%250ASouthern%2520California%252C%2520and%2520achieved%2520a%2520precision%2520of%252073.6%2525%2520and%2520recall%2520of%252073.3%2525%250Ausing%2520test%2520data%2520from%2520this%2520region.%2520We%2520generally%2520observed%2520similar%2520precision%2520and%250Aslightly%2520lower%2520recall%2520when%2520extrapolating%2520to%2520other%2520California%2520climate%2520zones%2520and%250Aimage%2520capture%2520dates.%2520We%2520used%2520our%2520method%2520to%2520produce%2520a%2520map%2520of%2520trees%2520in%2520the%2520entire%250Aurban%2520forest%2520of%2520California%252C%2520and%2520estimated%2520the%2520total%2520number%2520of%2520urban%2520trees%2520in%250ACalifornia%2520to%2520be%2520about%252043.5%2520million.%2520Our%2520study%2520indicates%2520the%2520potential%2520for%2520deep%250Alearning%2520methods%2520to%2520support%2520future%2520urban%2520forestry%2520studies%2520at%2520unprecedented%250Ascales.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2208.10607v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Individual%20Tree%20Detection%20in%20Large-Scale%20Urban%20Environments%20using%0A%20%20High-Resolution%20Multispectral%20Imagery&entry.906535625=Jonathan%20Ventura%20and%20Camille%20Pawlak%20and%20Milo%20Honsberger%20and%20Cameron%20Gonsalves%20and%20Julian%20Rice%20and%20Natalie%20L.%20R.%20Love%20and%20Skyler%20Han%20and%20Viet%20Nguyen%20and%20Keilana%20Sugano%20and%20Jacqueline%20Doremus%20and%20G.%20Andrew%20Fricker%20and%20Jenn%20Yost%20and%20Matt%20Ritter&entry.1292438233=%20%20We%20introduce%20a%20novel%20deep%20learning%20method%20for%20detection%20of%20individual%20trees%0Ain%20urban%20environments%20using%20high-resolution%20multispectral%20aerial%20imagery.%20We%0Ause%20a%20convolutional%20neural%20network%20to%20regress%20a%20confidence%20map%20indicating%20the%0Alocations%20of%20individual%20trees%2C%20which%20are%20localized%20using%20a%20peak%20finding%0Aalgorithm.%20Our%20method%20provides%20complete%20spatial%20coverage%20by%20detecting%20trees%20in%0Aboth%20public%20and%20private%20spaces%2C%20and%20can%20scale%20to%20very%20large%20areas.%20We%20performed%0Aa%20thorough%20evaluation%20of%20our%20method%2C%20supported%20by%20a%20new%20dataset%20of%20over%201%2C500%0Aimages%20and%20almost%20100%2C000%20tree%20annotations%2C%20covering%20eight%20cities%2C%20six%20climate%0Azones%2C%20and%20three%20image%20capture%20years.%20We%20trained%20our%20model%20on%20data%20from%0ASouthern%20California%2C%20and%20achieved%20a%20precision%20of%2073.6%25%20and%20recall%20of%2073.3%25%0Ausing%20test%20data%20from%20this%20region.%20We%20generally%20observed%20similar%20precision%20and%0Aslightly%20lower%20recall%20when%20extrapolating%20to%20other%20California%20climate%20zones%20and%0Aimage%20capture%20dates.%20We%20used%20our%20method%20to%20produce%20a%20map%20of%20trees%20in%20the%20entire%0Aurban%20forest%20of%20California%2C%20and%20estimated%20the%20total%20number%20of%20urban%20trees%20in%0ACalifornia%20to%20be%20about%2043.5%20million.%20Our%20study%20indicates%20the%20potential%20for%20deep%0Alearning%20methods%20to%20support%20future%20urban%20forestry%20studies%20at%20unprecedented%0Ascales.%0A&entry.1838667208=http%3A//arxiv.org/abs/2208.10607v4&entry.124074799=Read"},
{"title": "Towards Efficient Pixel Labeling for Industrial Anomaly Detection and\n  Localization", "author": "Hanxi Li and Jingqi Wu and Lin Yuanbo and Hao Chen and Deyin Liu and Chunhua Shen", "abstract": "  In the realm of practical Anomaly Detection (AD) tasks, manual labeling of\nanomalous pixels proves to be a costly endeavor. Consequently, many AD methods\nare crafted as one-class classifiers, tailored for training sets completely\ndevoid of anomalies, ensuring a more cost-effective approach. While some\npioneering work has demonstrated heightened AD accuracy by incorporating real\nanomaly samples in training, this enhancement comes at the price of\nlabor-intensive labeling processes. This paper strikes the balance between AD\naccuracy and labeling expenses by introducing ADClick, a novel Interactive\nImage Segmentation (IIS) algorithm. ADClick efficiently generates\n\"ground-truth\" anomaly masks for real defective images, leveraging innovative\nresidual features and meticulously crafted language prompts. Notably, ADClick\nshowcases a significantly elevated generalization capacity compared to existing\nstate-of-the-art IIS approaches. Functioning as an anomaly labeling tool,\nADClick generates high-quality anomaly labels (AP $= 94.1\\%$ on MVTec AD) based\non only $3$ to $5$ manual click annotations per training image. Furthermore, we\nextend the capabilities of ADClick into ADClick-Seg, an enhanced model designed\nfor anomaly detection and localization. By fine-tuning the ADClick-Seg model\nusing the weak labels inferred by ADClick, we establish the state-of-the-art\nperformances in supervised AD tasks (AP $= 86.4\\%$ on MVTec AD and AP $=\n78.4\\%$, PRO $= 98.6\\%$ on KSDD2).\n", "link": "http://arxiv.org/abs/2407.03130v1", "date": "2024-07-03", "relevancy": 2.0474, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5472}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5089}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5007}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Efficient%20Pixel%20Labeling%20for%20Industrial%20Anomaly%20Detection%20and%0A%20%20Localization&body=Title%3A%20Towards%20Efficient%20Pixel%20Labeling%20for%20Industrial%20Anomaly%20Detection%20and%0A%20%20Localization%0AAuthor%3A%20Hanxi%20Li%20and%20Jingqi%20Wu%20and%20Lin%20Yuanbo%20and%20Hao%20Chen%20and%20Deyin%20Liu%20and%20Chunhua%20Shen%0AAbstract%3A%20%20%20In%20the%20realm%20of%20practical%20Anomaly%20Detection%20%28AD%29%20tasks%2C%20manual%20labeling%20of%0Aanomalous%20pixels%20proves%20to%20be%20a%20costly%20endeavor.%20Consequently%2C%20many%20AD%20methods%0Aare%20crafted%20as%20one-class%20classifiers%2C%20tailored%20for%20training%20sets%20completely%0Adevoid%20of%20anomalies%2C%20ensuring%20a%20more%20cost-effective%20approach.%20While%20some%0Apioneering%20work%20has%20demonstrated%20heightened%20AD%20accuracy%20by%20incorporating%20real%0Aanomaly%20samples%20in%20training%2C%20this%20enhancement%20comes%20at%20the%20price%20of%0Alabor-intensive%20labeling%20processes.%20This%20paper%20strikes%20the%20balance%20between%20AD%0Aaccuracy%20and%20labeling%20expenses%20by%20introducing%20ADClick%2C%20a%20novel%20Interactive%0AImage%20Segmentation%20%28IIS%29%20algorithm.%20ADClick%20efficiently%20generates%0A%22ground-truth%22%20anomaly%20masks%20for%20real%20defective%20images%2C%20leveraging%20innovative%0Aresidual%20features%20and%20meticulously%20crafted%20language%20prompts.%20Notably%2C%20ADClick%0Ashowcases%20a%20significantly%20elevated%20generalization%20capacity%20compared%20to%20existing%0Astate-of-the-art%20IIS%20approaches.%20Functioning%20as%20an%20anomaly%20labeling%20tool%2C%0AADClick%20generates%20high-quality%20anomaly%20labels%20%28AP%20%24%3D%2094.1%5C%25%24%20on%20MVTec%20AD%29%20based%0Aon%20only%20%243%24%20to%20%245%24%20manual%20click%20annotations%20per%20training%20image.%20Furthermore%2C%20we%0Aextend%20the%20capabilities%20of%20ADClick%20into%20ADClick-Seg%2C%20an%20enhanced%20model%20designed%0Afor%20anomaly%20detection%20and%20localization.%20By%20fine-tuning%20the%20ADClick-Seg%20model%0Ausing%20the%20weak%20labels%20inferred%20by%20ADClick%2C%20we%20establish%20the%20state-of-the-art%0Aperformances%20in%20supervised%20AD%20tasks%20%28AP%20%24%3D%2086.4%5C%25%24%20on%20MVTec%20AD%20and%20AP%20%24%3D%0A78.4%5C%25%24%2C%20PRO%20%24%3D%2098.6%5C%25%24%20on%20KSDD2%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03130v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Efficient%2520Pixel%2520Labeling%2520for%2520Industrial%2520Anomaly%2520Detection%2520and%250A%2520%2520Localization%26entry.906535625%3DHanxi%2520Li%2520and%2520Jingqi%2520Wu%2520and%2520Lin%2520Yuanbo%2520and%2520Hao%2520Chen%2520and%2520Deyin%2520Liu%2520and%2520Chunhua%2520Shen%26entry.1292438233%3D%2520%2520In%2520the%2520realm%2520of%2520practical%2520Anomaly%2520Detection%2520%2528AD%2529%2520tasks%252C%2520manual%2520labeling%2520of%250Aanomalous%2520pixels%2520proves%2520to%2520be%2520a%2520costly%2520endeavor.%2520Consequently%252C%2520many%2520AD%2520methods%250Aare%2520crafted%2520as%2520one-class%2520classifiers%252C%2520tailored%2520for%2520training%2520sets%2520completely%250Adevoid%2520of%2520anomalies%252C%2520ensuring%2520a%2520more%2520cost-effective%2520approach.%2520While%2520some%250Apioneering%2520work%2520has%2520demonstrated%2520heightened%2520AD%2520accuracy%2520by%2520incorporating%2520real%250Aanomaly%2520samples%2520in%2520training%252C%2520this%2520enhancement%2520comes%2520at%2520the%2520price%2520of%250Alabor-intensive%2520labeling%2520processes.%2520This%2520paper%2520strikes%2520the%2520balance%2520between%2520AD%250Aaccuracy%2520and%2520labeling%2520expenses%2520by%2520introducing%2520ADClick%252C%2520a%2520novel%2520Interactive%250AImage%2520Segmentation%2520%2528IIS%2529%2520algorithm.%2520ADClick%2520efficiently%2520generates%250A%2522ground-truth%2522%2520anomaly%2520masks%2520for%2520real%2520defective%2520images%252C%2520leveraging%2520innovative%250Aresidual%2520features%2520and%2520meticulously%2520crafted%2520language%2520prompts.%2520Notably%252C%2520ADClick%250Ashowcases%2520a%2520significantly%2520elevated%2520generalization%2520capacity%2520compared%2520to%2520existing%250Astate-of-the-art%2520IIS%2520approaches.%2520Functioning%2520as%2520an%2520anomaly%2520labeling%2520tool%252C%250AADClick%2520generates%2520high-quality%2520anomaly%2520labels%2520%2528AP%2520%2524%253D%252094.1%255C%2525%2524%2520on%2520MVTec%2520AD%2529%2520based%250Aon%2520only%2520%25243%2524%2520to%2520%25245%2524%2520manual%2520click%2520annotations%2520per%2520training%2520image.%2520Furthermore%252C%2520we%250Aextend%2520the%2520capabilities%2520of%2520ADClick%2520into%2520ADClick-Seg%252C%2520an%2520enhanced%2520model%2520designed%250Afor%2520anomaly%2520detection%2520and%2520localization.%2520By%2520fine-tuning%2520the%2520ADClick-Seg%2520model%250Ausing%2520the%2520weak%2520labels%2520inferred%2520by%2520ADClick%252C%2520we%2520establish%2520the%2520state-of-the-art%250Aperformances%2520in%2520supervised%2520AD%2520tasks%2520%2528AP%2520%2524%253D%252086.4%255C%2525%2524%2520on%2520MVTec%2520AD%2520and%2520AP%2520%2524%253D%250A78.4%255C%2525%2524%252C%2520PRO%2520%2524%253D%252098.6%255C%2525%2524%2520on%2520KSDD2%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03130v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Efficient%20Pixel%20Labeling%20for%20Industrial%20Anomaly%20Detection%20and%0A%20%20Localization&entry.906535625=Hanxi%20Li%20and%20Jingqi%20Wu%20and%20Lin%20Yuanbo%20and%20Hao%20Chen%20and%20Deyin%20Liu%20and%20Chunhua%20Shen&entry.1292438233=%20%20In%20the%20realm%20of%20practical%20Anomaly%20Detection%20%28AD%29%20tasks%2C%20manual%20labeling%20of%0Aanomalous%20pixels%20proves%20to%20be%20a%20costly%20endeavor.%20Consequently%2C%20many%20AD%20methods%0Aare%20crafted%20as%20one-class%20classifiers%2C%20tailored%20for%20training%20sets%20completely%0Adevoid%20of%20anomalies%2C%20ensuring%20a%20more%20cost-effective%20approach.%20While%20some%0Apioneering%20work%20has%20demonstrated%20heightened%20AD%20accuracy%20by%20incorporating%20real%0Aanomaly%20samples%20in%20training%2C%20this%20enhancement%20comes%20at%20the%20price%20of%0Alabor-intensive%20labeling%20processes.%20This%20paper%20strikes%20the%20balance%20between%20AD%0Aaccuracy%20and%20labeling%20expenses%20by%20introducing%20ADClick%2C%20a%20novel%20Interactive%0AImage%20Segmentation%20%28IIS%29%20algorithm.%20ADClick%20efficiently%20generates%0A%22ground-truth%22%20anomaly%20masks%20for%20real%20defective%20images%2C%20leveraging%20innovative%0Aresidual%20features%20and%20meticulously%20crafted%20language%20prompts.%20Notably%2C%20ADClick%0Ashowcases%20a%20significantly%20elevated%20generalization%20capacity%20compared%20to%20existing%0Astate-of-the-art%20IIS%20approaches.%20Functioning%20as%20an%20anomaly%20labeling%20tool%2C%0AADClick%20generates%20high-quality%20anomaly%20labels%20%28AP%20%24%3D%2094.1%5C%25%24%20on%20MVTec%20AD%29%20based%0Aon%20only%20%243%24%20to%20%245%24%20manual%20click%20annotations%20per%20training%20image.%20Furthermore%2C%20we%0Aextend%20the%20capabilities%20of%20ADClick%20into%20ADClick-Seg%2C%20an%20enhanced%20model%20designed%0Afor%20anomaly%20detection%20and%20localization.%20By%20fine-tuning%20the%20ADClick-Seg%20model%0Ausing%20the%20weak%20labels%20inferred%20by%20ADClick%2C%20we%20establish%20the%20state-of-the-art%0Aperformances%20in%20supervised%20AD%20tasks%20%28AP%20%24%3D%2086.4%5C%25%24%20on%20MVTec%20AD%20and%20AP%20%24%3D%0A78.4%5C%25%24%2C%20PRO%20%24%3D%2098.6%5C%25%24%20on%20KSDD2%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03130v1&entry.124074799=Read"},
{"title": "HoloHisto: End-to-end Gigapixel WSI Segmentation with 4K Resolution\n  Sequential Tokenization", "author": "Yucheng Tang and Yufan He and Vishwesh Nath and Pengfeig Guo and Ruining Deng and Tianyuan Yao and Quan Liu and Can Cui and Mengmeng Yin and Ziyue Xu and Holger Roth and Daguang Xu and Haichun Yang and Yuankai Huo", "abstract": "  In digital pathology, the traditional method for deep learning-based image\nsegmentation typically involves a two-stage process: initially segmenting\nhigh-resolution whole slide images (WSI) into smaller patches (e.g., 256x256,\n512x512, 1024x1024) and subsequently reconstructing them to their original\nscale. This method often struggles to capture the complex details and vast\nscope of WSIs. In this paper, we propose the holistic histopathology\n(HoloHisto) segmentation method to achieve end-to-end segmentation on gigapixel\nWSIs, whose maximum resolution is above 80,000$\\times$70,000 pixels. HoloHisto\nfundamentally shifts the paradigm of WSI segmentation to an end-to-end learning\nfashion with 1) a large (4K) resolution base patch for elevated visual\ninformation inclusion and efficient processing, and 2) a novel sequential\ntokenization mechanism to properly model the contextual relationships and\nefficiently model the rich information from the 4K input. To our best\nknowledge, HoloHisto presents the first holistic approach for gigapixel\nresolution WSI segmentation, supporting direct I/O of complete WSI and their\ncorresponding gigapixel masks. Under the HoloHisto platform, we unveil a random\n4K sampler that transcends ultra-high resolution, delivering 31 and 10 times\nmore pixels than standard 2D and 3D patches, respectively, for advancing\ncomputational capabilities. To facilitate efficient 4K resolution dense\nprediction, we leverage sequential tokenization, utilizing a pre-trained image\ntokenizer to group image features into a discrete token grid. To assess the\nperformance, our team curated a new kidney pathology image segmentation (KPIs)\ndataset with WSI-level glomeruli segmentation from whole mouse kidneys. From\nthe results, HoloHisto-4K delivers remarkable performance gains over previous\nstate-of-the-art models.\n", "link": "http://arxiv.org/abs/2407.03307v1", "date": "2024-07-03", "relevancy": 2.0444, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.531}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4976}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoloHisto%3A%20End-to-end%20Gigapixel%20WSI%20Segmentation%20with%204K%20Resolution%0A%20%20Sequential%20Tokenization&body=Title%3A%20HoloHisto%3A%20End-to-end%20Gigapixel%20WSI%20Segmentation%20with%204K%20Resolution%0A%20%20Sequential%20Tokenization%0AAuthor%3A%20Yucheng%20Tang%20and%20Yufan%20He%20and%20Vishwesh%20Nath%20and%20Pengfeig%20Guo%20and%20Ruining%20Deng%20and%20Tianyuan%20Yao%20and%20Quan%20Liu%20and%20Can%20Cui%20and%20Mengmeng%20Yin%20and%20Ziyue%20Xu%20and%20Holger%20Roth%20and%20Daguang%20Xu%20and%20Haichun%20Yang%20and%20Yuankai%20Huo%0AAbstract%3A%20%20%20In%20digital%20pathology%2C%20the%20traditional%20method%20for%20deep%20learning-based%20image%0Asegmentation%20typically%20involves%20a%20two-stage%20process%3A%20initially%20segmenting%0Ahigh-resolution%20whole%20slide%20images%20%28WSI%29%20into%20smaller%20patches%20%28e.g.%2C%20256x256%2C%0A512x512%2C%201024x1024%29%20and%20subsequently%20reconstructing%20them%20to%20their%20original%0Ascale.%20This%20method%20often%20struggles%20to%20capture%20the%20complex%20details%20and%20vast%0Ascope%20of%20WSIs.%20In%20this%20paper%2C%20we%20propose%20the%20holistic%20histopathology%0A%28HoloHisto%29%20segmentation%20method%20to%20achieve%20end-to-end%20segmentation%20on%20gigapixel%0AWSIs%2C%20whose%20maximum%20resolution%20is%20above%2080%2C000%24%5Ctimes%2470%2C000%20pixels.%20HoloHisto%0Afundamentally%20shifts%20the%20paradigm%20of%20WSI%20segmentation%20to%20an%20end-to-end%20learning%0Afashion%20with%201%29%20a%20large%20%284K%29%20resolution%20base%20patch%20for%20elevated%20visual%0Ainformation%20inclusion%20and%20efficient%20processing%2C%20and%202%29%20a%20novel%20sequential%0Atokenization%20mechanism%20to%20properly%20model%20the%20contextual%20relationships%20and%0Aefficiently%20model%20the%20rich%20information%20from%20the%204K%20input.%20To%20our%20best%0Aknowledge%2C%20HoloHisto%20presents%20the%20first%20holistic%20approach%20for%20gigapixel%0Aresolution%20WSI%20segmentation%2C%20supporting%20direct%20I/O%20of%20complete%20WSI%20and%20their%0Acorresponding%20gigapixel%20masks.%20Under%20the%20HoloHisto%20platform%2C%20we%20unveil%20a%20random%0A4K%20sampler%20that%20transcends%20ultra-high%20resolution%2C%20delivering%2031%20and%2010%20times%0Amore%20pixels%20than%20standard%202D%20and%203D%20patches%2C%20respectively%2C%20for%20advancing%0Acomputational%20capabilities.%20To%20facilitate%20efficient%204K%20resolution%20dense%0Aprediction%2C%20we%20leverage%20sequential%20tokenization%2C%20utilizing%20a%20pre-trained%20image%0Atokenizer%20to%20group%20image%20features%20into%20a%20discrete%20token%20grid.%20To%20assess%20the%0Aperformance%2C%20our%20team%20curated%20a%20new%20kidney%20pathology%20image%20segmentation%20%28KPIs%29%0Adataset%20with%20WSI-level%20glomeruli%20segmentation%20from%20whole%20mouse%20kidneys.%20From%0Athe%20results%2C%20HoloHisto-4K%20delivers%20remarkable%20performance%20gains%20over%20previous%0Astate-of-the-art%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03307v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoloHisto%253A%2520End-to-end%2520Gigapixel%2520WSI%2520Segmentation%2520with%25204K%2520Resolution%250A%2520%2520Sequential%2520Tokenization%26entry.906535625%3DYucheng%2520Tang%2520and%2520Yufan%2520He%2520and%2520Vishwesh%2520Nath%2520and%2520Pengfeig%2520Guo%2520and%2520Ruining%2520Deng%2520and%2520Tianyuan%2520Yao%2520and%2520Quan%2520Liu%2520and%2520Can%2520Cui%2520and%2520Mengmeng%2520Yin%2520and%2520Ziyue%2520Xu%2520and%2520Holger%2520Roth%2520and%2520Daguang%2520Xu%2520and%2520Haichun%2520Yang%2520and%2520Yuankai%2520Huo%26entry.1292438233%3D%2520%2520In%2520digital%2520pathology%252C%2520the%2520traditional%2520method%2520for%2520deep%2520learning-based%2520image%250Asegmentation%2520typically%2520involves%2520a%2520two-stage%2520process%253A%2520initially%2520segmenting%250Ahigh-resolution%2520whole%2520slide%2520images%2520%2528WSI%2529%2520into%2520smaller%2520patches%2520%2528e.g.%252C%2520256x256%252C%250A512x512%252C%25201024x1024%2529%2520and%2520subsequently%2520reconstructing%2520them%2520to%2520their%2520original%250Ascale.%2520This%2520method%2520often%2520struggles%2520to%2520capture%2520the%2520complex%2520details%2520and%2520vast%250Ascope%2520of%2520WSIs.%2520In%2520this%2520paper%252C%2520we%2520propose%2520the%2520holistic%2520histopathology%250A%2528HoloHisto%2529%2520segmentation%2520method%2520to%2520achieve%2520end-to-end%2520segmentation%2520on%2520gigapixel%250AWSIs%252C%2520whose%2520maximum%2520resolution%2520is%2520above%252080%252C000%2524%255Ctimes%252470%252C000%2520pixels.%2520HoloHisto%250Afundamentally%2520shifts%2520the%2520paradigm%2520of%2520WSI%2520segmentation%2520to%2520an%2520end-to-end%2520learning%250Afashion%2520with%25201%2529%2520a%2520large%2520%25284K%2529%2520resolution%2520base%2520patch%2520for%2520elevated%2520visual%250Ainformation%2520inclusion%2520and%2520efficient%2520processing%252C%2520and%25202%2529%2520a%2520novel%2520sequential%250Atokenization%2520mechanism%2520to%2520properly%2520model%2520the%2520contextual%2520relationships%2520and%250Aefficiently%2520model%2520the%2520rich%2520information%2520from%2520the%25204K%2520input.%2520To%2520our%2520best%250Aknowledge%252C%2520HoloHisto%2520presents%2520the%2520first%2520holistic%2520approach%2520for%2520gigapixel%250Aresolution%2520WSI%2520segmentation%252C%2520supporting%2520direct%2520I/O%2520of%2520complete%2520WSI%2520and%2520their%250Acorresponding%2520gigapixel%2520masks.%2520Under%2520the%2520HoloHisto%2520platform%252C%2520we%2520unveil%2520a%2520random%250A4K%2520sampler%2520that%2520transcends%2520ultra-high%2520resolution%252C%2520delivering%252031%2520and%252010%2520times%250Amore%2520pixels%2520than%2520standard%25202D%2520and%25203D%2520patches%252C%2520respectively%252C%2520for%2520advancing%250Acomputational%2520capabilities.%2520To%2520facilitate%2520efficient%25204K%2520resolution%2520dense%250Aprediction%252C%2520we%2520leverage%2520sequential%2520tokenization%252C%2520utilizing%2520a%2520pre-trained%2520image%250Atokenizer%2520to%2520group%2520image%2520features%2520into%2520a%2520discrete%2520token%2520grid.%2520To%2520assess%2520the%250Aperformance%252C%2520our%2520team%2520curated%2520a%2520new%2520kidney%2520pathology%2520image%2520segmentation%2520%2528KPIs%2529%250Adataset%2520with%2520WSI-level%2520glomeruli%2520segmentation%2520from%2520whole%2520mouse%2520kidneys.%2520From%250Athe%2520results%252C%2520HoloHisto-4K%2520delivers%2520remarkable%2520performance%2520gains%2520over%2520previous%250Astate-of-the-art%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03307v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoloHisto%3A%20End-to-end%20Gigapixel%20WSI%20Segmentation%20with%204K%20Resolution%0A%20%20Sequential%20Tokenization&entry.906535625=Yucheng%20Tang%20and%20Yufan%20He%20and%20Vishwesh%20Nath%20and%20Pengfeig%20Guo%20and%20Ruining%20Deng%20and%20Tianyuan%20Yao%20and%20Quan%20Liu%20and%20Can%20Cui%20and%20Mengmeng%20Yin%20and%20Ziyue%20Xu%20and%20Holger%20Roth%20and%20Daguang%20Xu%20and%20Haichun%20Yang%20and%20Yuankai%20Huo&entry.1292438233=%20%20In%20digital%20pathology%2C%20the%20traditional%20method%20for%20deep%20learning-based%20image%0Asegmentation%20typically%20involves%20a%20two-stage%20process%3A%20initially%20segmenting%0Ahigh-resolution%20whole%20slide%20images%20%28WSI%29%20into%20smaller%20patches%20%28e.g.%2C%20256x256%2C%0A512x512%2C%201024x1024%29%20and%20subsequently%20reconstructing%20them%20to%20their%20original%0Ascale.%20This%20method%20often%20struggles%20to%20capture%20the%20complex%20details%20and%20vast%0Ascope%20of%20WSIs.%20In%20this%20paper%2C%20we%20propose%20the%20holistic%20histopathology%0A%28HoloHisto%29%20segmentation%20method%20to%20achieve%20end-to-end%20segmentation%20on%20gigapixel%0AWSIs%2C%20whose%20maximum%20resolution%20is%20above%2080%2C000%24%5Ctimes%2470%2C000%20pixels.%20HoloHisto%0Afundamentally%20shifts%20the%20paradigm%20of%20WSI%20segmentation%20to%20an%20end-to-end%20learning%0Afashion%20with%201%29%20a%20large%20%284K%29%20resolution%20base%20patch%20for%20elevated%20visual%0Ainformation%20inclusion%20and%20efficient%20processing%2C%20and%202%29%20a%20novel%20sequential%0Atokenization%20mechanism%20to%20properly%20model%20the%20contextual%20relationships%20and%0Aefficiently%20model%20the%20rich%20information%20from%20the%204K%20input.%20To%20our%20best%0Aknowledge%2C%20HoloHisto%20presents%20the%20first%20holistic%20approach%20for%20gigapixel%0Aresolution%20WSI%20segmentation%2C%20supporting%20direct%20I/O%20of%20complete%20WSI%20and%20their%0Acorresponding%20gigapixel%20masks.%20Under%20the%20HoloHisto%20platform%2C%20we%20unveil%20a%20random%0A4K%20sampler%20that%20transcends%20ultra-high%20resolution%2C%20delivering%2031%20and%2010%20times%0Amore%20pixels%20than%20standard%202D%20and%203D%20patches%2C%20respectively%2C%20for%20advancing%0Acomputational%20capabilities.%20To%20facilitate%20efficient%204K%20resolution%20dense%0Aprediction%2C%20we%20leverage%20sequential%20tokenization%2C%20utilizing%20a%20pre-trained%20image%0Atokenizer%20to%20group%20image%20features%20into%20a%20discrete%20token%20grid.%20To%20assess%20the%0Aperformance%2C%20our%20team%20curated%20a%20new%20kidney%20pathology%20image%20segmentation%20%28KPIs%29%0Adataset%20with%20WSI-level%20glomeruli%20segmentation%20from%20whole%20mouse%20kidneys.%20From%0Athe%20results%2C%20HoloHisto-4K%20delivers%20remarkable%20performance%20gains%20over%20previous%0Astate-of-the-art%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03307v1&entry.124074799=Read"},
{"title": "VIVA: A Benchmark for Vision-Grounded Decision-Making with Human Values", "author": "Zhe Hu and Yixiao Ren and Jing Li and Yu Yin", "abstract": "  This paper introduces VIVA, a benchmark for VIsion-grounded decision-making\ndriven by human VAlues. While most large vision-language models (VLMs) focus on\nphysical-level skills, our work is the first to examine their multimodal\ncapabilities in leveraging human values to make decisions under a\nvision-depicted situation. VIVA contains 1,062 images depicting diverse\nreal-world situations and the manually annotated decisions grounded in them.\nGiven an image there, the model should select the most appropriate action to\naddress the situation and provide the relevant human values and reason\nunderlying the decision. Extensive experiments based on VIVA show the\nlimitation of VLMs in using human values to make multimodal decisions. Further\nanalyses indicate the potential benefits of exploiting action consequences and\npredicted human values.\n", "link": "http://arxiv.org/abs/2407.03000v1", "date": "2024-07-03", "relevancy": 2.0116, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5282}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4854}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIVA%3A%20A%20Benchmark%20for%20Vision-Grounded%20Decision-Making%20with%20Human%20Values&body=Title%3A%20VIVA%3A%20A%20Benchmark%20for%20Vision-Grounded%20Decision-Making%20with%20Human%20Values%0AAuthor%3A%20Zhe%20Hu%20and%20Yixiao%20Ren%20and%20Jing%20Li%20and%20Yu%20Yin%0AAbstract%3A%20%20%20This%20paper%20introduces%20VIVA%2C%20a%20benchmark%20for%20VIsion-grounded%20decision-making%0Adriven%20by%20human%20VAlues.%20While%20most%20large%20vision-language%20models%20%28VLMs%29%20focus%20on%0Aphysical-level%20skills%2C%20our%20work%20is%20the%20first%20to%20examine%20their%20multimodal%0Acapabilities%20in%20leveraging%20human%20values%20to%20make%20decisions%20under%20a%0Avision-depicted%20situation.%20VIVA%20contains%201%2C062%20images%20depicting%20diverse%0Areal-world%20situations%20and%20the%20manually%20annotated%20decisions%20grounded%20in%20them.%0AGiven%20an%20image%20there%2C%20the%20model%20should%20select%20the%20most%20appropriate%20action%20to%0Aaddress%20the%20situation%20and%20provide%20the%20relevant%20human%20values%20and%20reason%0Aunderlying%20the%20decision.%20Extensive%20experiments%20based%20on%20VIVA%20show%20the%0Alimitation%20of%20VLMs%20in%20using%20human%20values%20to%20make%20multimodal%20decisions.%20Further%0Aanalyses%20indicate%20the%20potential%20benefits%20of%20exploiting%20action%20consequences%20and%0Apredicted%20human%20values.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03000v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIVA%253A%2520A%2520Benchmark%2520for%2520Vision-Grounded%2520Decision-Making%2520with%2520Human%2520Values%26entry.906535625%3DZhe%2520Hu%2520and%2520Yixiao%2520Ren%2520and%2520Jing%2520Li%2520and%2520Yu%2520Yin%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520VIVA%252C%2520a%2520benchmark%2520for%2520VIsion-grounded%2520decision-making%250Adriven%2520by%2520human%2520VAlues.%2520While%2520most%2520large%2520vision-language%2520models%2520%2528VLMs%2529%2520focus%2520on%250Aphysical-level%2520skills%252C%2520our%2520work%2520is%2520the%2520first%2520to%2520examine%2520their%2520multimodal%250Acapabilities%2520in%2520leveraging%2520human%2520values%2520to%2520make%2520decisions%2520under%2520a%250Avision-depicted%2520situation.%2520VIVA%2520contains%25201%252C062%2520images%2520depicting%2520diverse%250Areal-world%2520situations%2520and%2520the%2520manually%2520annotated%2520decisions%2520grounded%2520in%2520them.%250AGiven%2520an%2520image%2520there%252C%2520the%2520model%2520should%2520select%2520the%2520most%2520appropriate%2520action%2520to%250Aaddress%2520the%2520situation%2520and%2520provide%2520the%2520relevant%2520human%2520values%2520and%2520reason%250Aunderlying%2520the%2520decision.%2520Extensive%2520experiments%2520based%2520on%2520VIVA%2520show%2520the%250Alimitation%2520of%2520VLMs%2520in%2520using%2520human%2520values%2520to%2520make%2520multimodal%2520decisions.%2520Further%250Aanalyses%2520indicate%2520the%2520potential%2520benefits%2520of%2520exploiting%2520action%2520consequences%2520and%250Apredicted%2520human%2520values.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03000v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIVA%3A%20A%20Benchmark%20for%20Vision-Grounded%20Decision-Making%20with%20Human%20Values&entry.906535625=Zhe%20Hu%20and%20Yixiao%20Ren%20and%20Jing%20Li%20and%20Yu%20Yin&entry.1292438233=%20%20This%20paper%20introduces%20VIVA%2C%20a%20benchmark%20for%20VIsion-grounded%20decision-making%0Adriven%20by%20human%20VAlues.%20While%20most%20large%20vision-language%20models%20%28VLMs%29%20focus%20on%0Aphysical-level%20skills%2C%20our%20work%20is%20the%20first%20to%20examine%20their%20multimodal%0Acapabilities%20in%20leveraging%20human%20values%20to%20make%20decisions%20under%20a%0Avision-depicted%20situation.%20VIVA%20contains%201%2C062%20images%20depicting%20diverse%0Areal-world%20situations%20and%20the%20manually%20annotated%20decisions%20grounded%20in%20them.%0AGiven%20an%20image%20there%2C%20the%20model%20should%20select%20the%20most%20appropriate%20action%20to%0Aaddress%20the%20situation%20and%20provide%20the%20relevant%20human%20values%20and%20reason%0Aunderlying%20the%20decision.%20Extensive%20experiments%20based%20on%20VIVA%20show%20the%0Alimitation%20of%20VLMs%20in%20using%20human%20values%20to%20make%20multimodal%20decisions.%20Further%0Aanalyses%20indicate%20the%20potential%20benefits%20of%20exploiting%20action%20consequences%20and%0Apredicted%20human%20values.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03000v1&entry.124074799=Read"},
{"title": "Towards a Scalable Reference-Free Evaluation of Generative Models", "author": "Azim Ospanov and Jingwei Zhang and Mohammad Jalali and Xuenan Cao and Andrej Bogdanov and Farzan Farnia", "abstract": "  While standard evaluation scores for generative models are mostly\nreference-based, a reference-dependent assessment of generative models could be\ngenerally difficult due to the unavailability of applicable reference datasets.\nRecently, the reference-free entropy scores, VENDI and RKE, have been proposed\nto evaluate the diversity of generated data. However, estimating these scores\nfrom data leads to significant computational costs for large-scale generative\nmodels. In this work, we leverage the random Fourier features framework to\nreduce the computational price and propose the Fourier-based Kernel Entropy\nApproximation (FKEA) method. We utilize FKEA's approximated eigenspectrum of\nthe kernel matrix to efficiently estimate the mentioned entropy scores.\nFurthermore, we show the application of FKEA's proxy eigenvectors to reveal the\nmethod's identified modes in evaluating the diversity of produced samples. We\nprovide a stochastic implementation of the FKEA assessment algorithm with a\ncomplexity $O(n)$ linearly growing with sample size $n$. We extensively\nevaluate FKEA's numerical performance in application to standard image, text,\nand video datasets. Our empirical results indicate the method's scalability and\ninterpretability applied to large-scale generative models. The codebase is\navailable at https://github.com/aziksh-ospanov/FKEA.\n", "link": "http://arxiv.org/abs/2407.02961v1", "date": "2024-07-03", "relevancy": 2.009, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5172}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5023}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20a%20Scalable%20Reference-Free%20Evaluation%20of%20Generative%20Models&body=Title%3A%20Towards%20a%20Scalable%20Reference-Free%20Evaluation%20of%20Generative%20Models%0AAuthor%3A%20Azim%20Ospanov%20and%20Jingwei%20Zhang%20and%20Mohammad%20Jalali%20and%20Xuenan%20Cao%20and%20Andrej%20Bogdanov%20and%20Farzan%20Farnia%0AAbstract%3A%20%20%20While%20standard%20evaluation%20scores%20for%20generative%20models%20are%20mostly%0Areference-based%2C%20a%20reference-dependent%20assessment%20of%20generative%20models%20could%20be%0Agenerally%20difficult%20due%20to%20the%20unavailability%20of%20applicable%20reference%20datasets.%0ARecently%2C%20the%20reference-free%20entropy%20scores%2C%20VENDI%20and%20RKE%2C%20have%20been%20proposed%0Ato%20evaluate%20the%20diversity%20of%20generated%20data.%20However%2C%20estimating%20these%20scores%0Afrom%20data%20leads%20to%20significant%20computational%20costs%20for%20large-scale%20generative%0Amodels.%20In%20this%20work%2C%20we%20leverage%20the%20random%20Fourier%20features%20framework%20to%0Areduce%20the%20computational%20price%20and%20propose%20the%20Fourier-based%20Kernel%20Entropy%0AApproximation%20%28FKEA%29%20method.%20We%20utilize%20FKEA%27s%20approximated%20eigenspectrum%20of%0Athe%20kernel%20matrix%20to%20efficiently%20estimate%20the%20mentioned%20entropy%20scores.%0AFurthermore%2C%20we%20show%20the%20application%20of%20FKEA%27s%20proxy%20eigenvectors%20to%20reveal%20the%0Amethod%27s%20identified%20modes%20in%20evaluating%20the%20diversity%20of%20produced%20samples.%20We%0Aprovide%20a%20stochastic%20implementation%20of%20the%20FKEA%20assessment%20algorithm%20with%20a%0Acomplexity%20%24O%28n%29%24%20linearly%20growing%20with%20sample%20size%20%24n%24.%20We%20extensively%0Aevaluate%20FKEA%27s%20numerical%20performance%20in%20application%20to%20standard%20image%2C%20text%2C%0Aand%20video%20datasets.%20Our%20empirical%20results%20indicate%20the%20method%27s%20scalability%20and%0Ainterpretability%20applied%20to%20large-scale%20generative%20models.%20The%20codebase%20is%0Aavailable%20at%20https%3A//github.com/aziksh-ospanov/FKEA.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02961v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520a%2520Scalable%2520Reference-Free%2520Evaluation%2520of%2520Generative%2520Models%26entry.906535625%3DAzim%2520Ospanov%2520and%2520Jingwei%2520Zhang%2520and%2520Mohammad%2520Jalali%2520and%2520Xuenan%2520Cao%2520and%2520Andrej%2520Bogdanov%2520and%2520Farzan%2520Farnia%26entry.1292438233%3D%2520%2520While%2520standard%2520evaluation%2520scores%2520for%2520generative%2520models%2520are%2520mostly%250Areference-based%252C%2520a%2520reference-dependent%2520assessment%2520of%2520generative%2520models%2520could%2520be%250Agenerally%2520difficult%2520due%2520to%2520the%2520unavailability%2520of%2520applicable%2520reference%2520datasets.%250ARecently%252C%2520the%2520reference-free%2520entropy%2520scores%252C%2520VENDI%2520and%2520RKE%252C%2520have%2520been%2520proposed%250Ato%2520evaluate%2520the%2520diversity%2520of%2520generated%2520data.%2520However%252C%2520estimating%2520these%2520scores%250Afrom%2520data%2520leads%2520to%2520significant%2520computational%2520costs%2520for%2520large-scale%2520generative%250Amodels.%2520In%2520this%2520work%252C%2520we%2520leverage%2520the%2520random%2520Fourier%2520features%2520framework%2520to%250Areduce%2520the%2520computational%2520price%2520and%2520propose%2520the%2520Fourier-based%2520Kernel%2520Entropy%250AApproximation%2520%2528FKEA%2529%2520method.%2520We%2520utilize%2520FKEA%2527s%2520approximated%2520eigenspectrum%2520of%250Athe%2520kernel%2520matrix%2520to%2520efficiently%2520estimate%2520the%2520mentioned%2520entropy%2520scores.%250AFurthermore%252C%2520we%2520show%2520the%2520application%2520of%2520FKEA%2527s%2520proxy%2520eigenvectors%2520to%2520reveal%2520the%250Amethod%2527s%2520identified%2520modes%2520in%2520evaluating%2520the%2520diversity%2520of%2520produced%2520samples.%2520We%250Aprovide%2520a%2520stochastic%2520implementation%2520of%2520the%2520FKEA%2520assessment%2520algorithm%2520with%2520a%250Acomplexity%2520%2524O%2528n%2529%2524%2520linearly%2520growing%2520with%2520sample%2520size%2520%2524n%2524.%2520We%2520extensively%250Aevaluate%2520FKEA%2527s%2520numerical%2520performance%2520in%2520application%2520to%2520standard%2520image%252C%2520text%252C%250Aand%2520video%2520datasets.%2520Our%2520empirical%2520results%2520indicate%2520the%2520method%2527s%2520scalability%2520and%250Ainterpretability%2520applied%2520to%2520large-scale%2520generative%2520models.%2520The%2520codebase%2520is%250Aavailable%2520at%2520https%253A//github.com/aziksh-ospanov/FKEA.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02961v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20a%20Scalable%20Reference-Free%20Evaluation%20of%20Generative%20Models&entry.906535625=Azim%20Ospanov%20and%20Jingwei%20Zhang%20and%20Mohammad%20Jalali%20and%20Xuenan%20Cao%20and%20Andrej%20Bogdanov%20and%20Farzan%20Farnia&entry.1292438233=%20%20While%20standard%20evaluation%20scores%20for%20generative%20models%20are%20mostly%0Areference-based%2C%20a%20reference-dependent%20assessment%20of%20generative%20models%20could%20be%0Agenerally%20difficult%20due%20to%20the%20unavailability%20of%20applicable%20reference%20datasets.%0ARecently%2C%20the%20reference-free%20entropy%20scores%2C%20VENDI%20and%20RKE%2C%20have%20been%20proposed%0Ato%20evaluate%20the%20diversity%20of%20generated%20data.%20However%2C%20estimating%20these%20scores%0Afrom%20data%20leads%20to%20significant%20computational%20costs%20for%20large-scale%20generative%0Amodels.%20In%20this%20work%2C%20we%20leverage%20the%20random%20Fourier%20features%20framework%20to%0Areduce%20the%20computational%20price%20and%20propose%20the%20Fourier-based%20Kernel%20Entropy%0AApproximation%20%28FKEA%29%20method.%20We%20utilize%20FKEA%27s%20approximated%20eigenspectrum%20of%0Athe%20kernel%20matrix%20to%20efficiently%20estimate%20the%20mentioned%20entropy%20scores.%0AFurthermore%2C%20we%20show%20the%20application%20of%20FKEA%27s%20proxy%20eigenvectors%20to%20reveal%20the%0Amethod%27s%20identified%20modes%20in%20evaluating%20the%20diversity%20of%20produced%20samples.%20We%0Aprovide%20a%20stochastic%20implementation%20of%20the%20FKEA%20assessment%20algorithm%20with%20a%0Acomplexity%20%24O%28n%29%24%20linearly%20growing%20with%20sample%20size%20%24n%24.%20We%20extensively%0Aevaluate%20FKEA%27s%20numerical%20performance%20in%20application%20to%20standard%20image%2C%20text%2C%0Aand%20video%20datasets.%20Our%20empirical%20results%20indicate%20the%20method%27s%20scalability%20and%0Ainterpretability%20applied%20to%20large-scale%20generative%20models.%20The%20codebase%20is%0Aavailable%20at%20https%3A//github.com/aziksh-ospanov/FKEA.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02961v1&entry.124074799=Read"},
{"title": "Federated Distillation for Medical Image Classification: Towards\n  Trustworthy Computer-Aided Diagnosis", "author": "Sufen Ren and Yule Hu and Shengchao Chen and Guanjun Wang", "abstract": "  Medical image classification plays a crucial role in computer-aided clinical\ndiagnosis. While deep learning techniques have significantly enhanced\nefficiency and reduced costs, the privacy-sensitive nature of medical imaging\ndata complicates centralized storage and model training. Furthermore,\nlow-resource healthcare organizations face challenges related to communication\noverhead and efficiency due to increasing data and model scales. This paper\nproposes a novel privacy-preserving medical image classification framework\nbased on federated learning to address these issues, named FedMIC. The\nframework enables healthcare organizations to learn from both global and local\nknowledge, enhancing local representation of private data despite statistical\nheterogeneity. It provides customized models for organizations with diverse\ndata distributions while minimizing communication overhead and improving\nefficiency without compromising performance. Our FedMIC enhances robustness and\npractical applicability under resource-constrained conditions. We demonstrate\nFedMIC's effectiveness using four public medical image datasets for classical\nmedical image classification tasks.\n", "link": "http://arxiv.org/abs/2407.02261v2", "date": "2024-07-03", "relevancy": 2.0048, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5511}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4977}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4848}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Distillation%20for%20Medical%20Image%20Classification%3A%20Towards%0A%20%20Trustworthy%20Computer-Aided%20Diagnosis&body=Title%3A%20Federated%20Distillation%20for%20Medical%20Image%20Classification%3A%20Towards%0A%20%20Trustworthy%20Computer-Aided%20Diagnosis%0AAuthor%3A%20Sufen%20Ren%20and%20Yule%20Hu%20and%20Shengchao%20Chen%20and%20Guanjun%20Wang%0AAbstract%3A%20%20%20Medical%20image%20classification%20plays%20a%20crucial%20role%20in%20computer-aided%20clinical%0Adiagnosis.%20While%20deep%20learning%20techniques%20have%20significantly%20enhanced%0Aefficiency%20and%20reduced%20costs%2C%20the%20privacy-sensitive%20nature%20of%20medical%20imaging%0Adata%20complicates%20centralized%20storage%20and%20model%20training.%20Furthermore%2C%0Alow-resource%20healthcare%20organizations%20face%20challenges%20related%20to%20communication%0Aoverhead%20and%20efficiency%20due%20to%20increasing%20data%20and%20model%20scales.%20This%20paper%0Aproposes%20a%20novel%20privacy-preserving%20medical%20image%20classification%20framework%0Abased%20on%20federated%20learning%20to%20address%20these%20issues%2C%20named%20FedMIC.%20The%0Aframework%20enables%20healthcare%20organizations%20to%20learn%20from%20both%20global%20and%20local%0Aknowledge%2C%20enhancing%20local%20representation%20of%20private%20data%20despite%20statistical%0Aheterogeneity.%20It%20provides%20customized%20models%20for%20organizations%20with%20diverse%0Adata%20distributions%20while%20minimizing%20communication%20overhead%20and%20improving%0Aefficiency%20without%20compromising%20performance.%20Our%20FedMIC%20enhances%20robustness%20and%0Apractical%20applicability%20under%20resource-constrained%20conditions.%20We%20demonstrate%0AFedMIC%27s%20effectiveness%20using%20four%20public%20medical%20image%20datasets%20for%20classical%0Amedical%20image%20classification%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02261v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Distillation%2520for%2520Medical%2520Image%2520Classification%253A%2520Towards%250A%2520%2520Trustworthy%2520Computer-Aided%2520Diagnosis%26entry.906535625%3DSufen%2520Ren%2520and%2520Yule%2520Hu%2520and%2520Shengchao%2520Chen%2520and%2520Guanjun%2520Wang%26entry.1292438233%3D%2520%2520Medical%2520image%2520classification%2520plays%2520a%2520crucial%2520role%2520in%2520computer-aided%2520clinical%250Adiagnosis.%2520While%2520deep%2520learning%2520techniques%2520have%2520significantly%2520enhanced%250Aefficiency%2520and%2520reduced%2520costs%252C%2520the%2520privacy-sensitive%2520nature%2520of%2520medical%2520imaging%250Adata%2520complicates%2520centralized%2520storage%2520and%2520model%2520training.%2520Furthermore%252C%250Alow-resource%2520healthcare%2520organizations%2520face%2520challenges%2520related%2520to%2520communication%250Aoverhead%2520and%2520efficiency%2520due%2520to%2520increasing%2520data%2520and%2520model%2520scales.%2520This%2520paper%250Aproposes%2520a%2520novel%2520privacy-preserving%2520medical%2520image%2520classification%2520framework%250Abased%2520on%2520federated%2520learning%2520to%2520address%2520these%2520issues%252C%2520named%2520FedMIC.%2520The%250Aframework%2520enables%2520healthcare%2520organizations%2520to%2520learn%2520from%2520both%2520global%2520and%2520local%250Aknowledge%252C%2520enhancing%2520local%2520representation%2520of%2520private%2520data%2520despite%2520statistical%250Aheterogeneity.%2520It%2520provides%2520customized%2520models%2520for%2520organizations%2520with%2520diverse%250Adata%2520distributions%2520while%2520minimizing%2520communication%2520overhead%2520and%2520improving%250Aefficiency%2520without%2520compromising%2520performance.%2520Our%2520FedMIC%2520enhances%2520robustness%2520and%250Apractical%2520applicability%2520under%2520resource-constrained%2520conditions.%2520We%2520demonstrate%250AFedMIC%2527s%2520effectiveness%2520using%2520four%2520public%2520medical%2520image%2520datasets%2520for%2520classical%250Amedical%2520image%2520classification%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02261v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Distillation%20for%20Medical%20Image%20Classification%3A%20Towards%0A%20%20Trustworthy%20Computer-Aided%20Diagnosis&entry.906535625=Sufen%20Ren%20and%20Yule%20Hu%20and%20Shengchao%20Chen%20and%20Guanjun%20Wang&entry.1292438233=%20%20Medical%20image%20classification%20plays%20a%20crucial%20role%20in%20computer-aided%20clinical%0Adiagnosis.%20While%20deep%20learning%20techniques%20have%20significantly%20enhanced%0Aefficiency%20and%20reduced%20costs%2C%20the%20privacy-sensitive%20nature%20of%20medical%20imaging%0Adata%20complicates%20centralized%20storage%20and%20model%20training.%20Furthermore%2C%0Alow-resource%20healthcare%20organizations%20face%20challenges%20related%20to%20communication%0Aoverhead%20and%20efficiency%20due%20to%20increasing%20data%20and%20model%20scales.%20This%20paper%0Aproposes%20a%20novel%20privacy-preserving%20medical%20image%20classification%20framework%0Abased%20on%20federated%20learning%20to%20address%20these%20issues%2C%20named%20FedMIC.%20The%0Aframework%20enables%20healthcare%20organizations%20to%20learn%20from%20both%20global%20and%20local%0Aknowledge%2C%20enhancing%20local%20representation%20of%20private%20data%20despite%20statistical%0Aheterogeneity.%20It%20provides%20customized%20models%20for%20organizations%20with%20diverse%0Adata%20distributions%20while%20minimizing%20communication%20overhead%20and%20improving%0Aefficiency%20without%20compromising%20performance.%20Our%20FedMIC%20enhances%20robustness%20and%0Apractical%20applicability%20under%20resource-constrained%20conditions.%20We%20demonstrate%0AFedMIC%27s%20effectiveness%20using%20four%20public%20medical%20image%20datasets%20for%20classical%0Amedical%20image%20classification%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02261v2&entry.124074799=Read"},
{"title": "ENOT: Expectile Regularization for Fast and Accurate Training of Neural\n  Optimal Transport", "author": "Nazar Buzun and Maksim Bobrin and Dmitry V. Dylov", "abstract": "  We present a new approach for Neural Optimal Transport (NOT) training\nprocedure, capable of accurately and efficiently estimating optimal\ntransportation plan via specific regularization on dual Kantorovich potentials.\nThe main bottleneck of existing NOT solvers is associated with the procedure of\nfinding a near-exact approximation of the conjugate operator (i.e., the\nc-transform), which is done either by optimizing over non-convex max-min\nobjectives or by the computationally intensive fine-tuning of the initial\napproximated prediction. We resolve both issues by proposing a new,\ntheoretically justified loss in the form of expectile regularisation which\nenforces binding conditions on the learning process of dual potentials. Such a\nregularization provides the upper bound estimation over the distribution of\npossible conjugate potentials and makes the learning stable, completely\neliminating the need for additional extensive fine-tuning. Proposed method,\ncalled Expectile-Regularised Neural Optimal Transport (ENOT), outperforms\nprevious state-of-the-art approaches on the established Wasserstein-2 benchmark\ntasks by a large margin (up to a 3-fold improvement in quality and up to a\n10-fold improvement in runtime). Moreover, we showcase performance of ENOT for\nvarying cost functions on different tasks such as image generation, showing\nrobustness of proposed algorithm. OTT-JAX library includes our implementation\nof ENOT algorithm https://ott-jax.readthedocs.io/en/latest/tutorials/ENOT.html\n", "link": "http://arxiv.org/abs/2403.03777v3", "date": "2024-07-03", "relevancy": 2.0025, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5142}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4948}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4811}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ENOT%3A%20Expectile%20Regularization%20for%20Fast%20and%20Accurate%20Training%20of%20Neural%0A%20%20Optimal%20Transport&body=Title%3A%20ENOT%3A%20Expectile%20Regularization%20for%20Fast%20and%20Accurate%20Training%20of%20Neural%0A%20%20Optimal%20Transport%0AAuthor%3A%20Nazar%20Buzun%20and%20Maksim%20Bobrin%20and%20Dmitry%20V.%20Dylov%0AAbstract%3A%20%20%20We%20present%20a%20new%20approach%20for%20Neural%20Optimal%20Transport%20%28NOT%29%20training%0Aprocedure%2C%20capable%20of%20accurately%20and%20efficiently%20estimating%20optimal%0Atransportation%20plan%20via%20specific%20regularization%20on%20dual%20Kantorovich%20potentials.%0AThe%20main%20bottleneck%20of%20existing%20NOT%20solvers%20is%20associated%20with%20the%20procedure%20of%0Afinding%20a%20near-exact%20approximation%20of%20the%20conjugate%20operator%20%28i.e.%2C%20the%0Ac-transform%29%2C%20which%20is%20done%20either%20by%20optimizing%20over%20non-convex%20max-min%0Aobjectives%20or%20by%20the%20computationally%20intensive%20fine-tuning%20of%20the%20initial%0Aapproximated%20prediction.%20We%20resolve%20both%20issues%20by%20proposing%20a%20new%2C%0Atheoretically%20justified%20loss%20in%20the%20form%20of%20expectile%20regularisation%20which%0Aenforces%20binding%20conditions%20on%20the%20learning%20process%20of%20dual%20potentials.%20Such%20a%0Aregularization%20provides%20the%20upper%20bound%20estimation%20over%20the%20distribution%20of%0Apossible%20conjugate%20potentials%20and%20makes%20the%20learning%20stable%2C%20completely%0Aeliminating%20the%20need%20for%20additional%20extensive%20fine-tuning.%20Proposed%20method%2C%0Acalled%20Expectile-Regularised%20Neural%20Optimal%20Transport%20%28ENOT%29%2C%20outperforms%0Aprevious%20state-of-the-art%20approaches%20on%20the%20established%20Wasserstein-2%20benchmark%0Atasks%20by%20a%20large%20margin%20%28up%20to%20a%203-fold%20improvement%20in%20quality%20and%20up%20to%20a%0A10-fold%20improvement%20in%20runtime%29.%20Moreover%2C%20we%20showcase%20performance%20of%20ENOT%20for%0Avarying%20cost%20functions%20on%20different%20tasks%20such%20as%20image%20generation%2C%20showing%0Arobustness%20of%20proposed%20algorithm.%20OTT-JAX%20library%20includes%20our%20implementation%0Aof%20ENOT%20algorithm%20https%3A//ott-jax.readthedocs.io/en/latest/tutorials/ENOT.html%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.03777v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DENOT%253A%2520Expectile%2520Regularization%2520for%2520Fast%2520and%2520Accurate%2520Training%2520of%2520Neural%250A%2520%2520Optimal%2520Transport%26entry.906535625%3DNazar%2520Buzun%2520and%2520Maksim%2520Bobrin%2520and%2520Dmitry%2520V.%2520Dylov%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520new%2520approach%2520for%2520Neural%2520Optimal%2520Transport%2520%2528NOT%2529%2520training%250Aprocedure%252C%2520capable%2520of%2520accurately%2520and%2520efficiently%2520estimating%2520optimal%250Atransportation%2520plan%2520via%2520specific%2520regularization%2520on%2520dual%2520Kantorovich%2520potentials.%250AThe%2520main%2520bottleneck%2520of%2520existing%2520NOT%2520solvers%2520is%2520associated%2520with%2520the%2520procedure%2520of%250Afinding%2520a%2520near-exact%2520approximation%2520of%2520the%2520conjugate%2520operator%2520%2528i.e.%252C%2520the%250Ac-transform%2529%252C%2520which%2520is%2520done%2520either%2520by%2520optimizing%2520over%2520non-convex%2520max-min%250Aobjectives%2520or%2520by%2520the%2520computationally%2520intensive%2520fine-tuning%2520of%2520the%2520initial%250Aapproximated%2520prediction.%2520We%2520resolve%2520both%2520issues%2520by%2520proposing%2520a%2520new%252C%250Atheoretically%2520justified%2520loss%2520in%2520the%2520form%2520of%2520expectile%2520regularisation%2520which%250Aenforces%2520binding%2520conditions%2520on%2520the%2520learning%2520process%2520of%2520dual%2520potentials.%2520Such%2520a%250Aregularization%2520provides%2520the%2520upper%2520bound%2520estimation%2520over%2520the%2520distribution%2520of%250Apossible%2520conjugate%2520potentials%2520and%2520makes%2520the%2520learning%2520stable%252C%2520completely%250Aeliminating%2520the%2520need%2520for%2520additional%2520extensive%2520fine-tuning.%2520Proposed%2520method%252C%250Acalled%2520Expectile-Regularised%2520Neural%2520Optimal%2520Transport%2520%2528ENOT%2529%252C%2520outperforms%250Aprevious%2520state-of-the-art%2520approaches%2520on%2520the%2520established%2520Wasserstein-2%2520benchmark%250Atasks%2520by%2520a%2520large%2520margin%2520%2528up%2520to%2520a%25203-fold%2520improvement%2520in%2520quality%2520and%2520up%2520to%2520a%250A10-fold%2520improvement%2520in%2520runtime%2529.%2520Moreover%252C%2520we%2520showcase%2520performance%2520of%2520ENOT%2520for%250Avarying%2520cost%2520functions%2520on%2520different%2520tasks%2520such%2520as%2520image%2520generation%252C%2520showing%250Arobustness%2520of%2520proposed%2520algorithm.%2520OTT-JAX%2520library%2520includes%2520our%2520implementation%250Aof%2520ENOT%2520algorithm%2520https%253A//ott-jax.readthedocs.io/en/latest/tutorials/ENOT.html%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.03777v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ENOT%3A%20Expectile%20Regularization%20for%20Fast%20and%20Accurate%20Training%20of%20Neural%0A%20%20Optimal%20Transport&entry.906535625=Nazar%20Buzun%20and%20Maksim%20Bobrin%20and%20Dmitry%20V.%20Dylov&entry.1292438233=%20%20We%20present%20a%20new%20approach%20for%20Neural%20Optimal%20Transport%20%28NOT%29%20training%0Aprocedure%2C%20capable%20of%20accurately%20and%20efficiently%20estimating%20optimal%0Atransportation%20plan%20via%20specific%20regularization%20on%20dual%20Kantorovich%20potentials.%0AThe%20main%20bottleneck%20of%20existing%20NOT%20solvers%20is%20associated%20with%20the%20procedure%20of%0Afinding%20a%20near-exact%20approximation%20of%20the%20conjugate%20operator%20%28i.e.%2C%20the%0Ac-transform%29%2C%20which%20is%20done%20either%20by%20optimizing%20over%20non-convex%20max-min%0Aobjectives%20or%20by%20the%20computationally%20intensive%20fine-tuning%20of%20the%20initial%0Aapproximated%20prediction.%20We%20resolve%20both%20issues%20by%20proposing%20a%20new%2C%0Atheoretically%20justified%20loss%20in%20the%20form%20of%20expectile%20regularisation%20which%0Aenforces%20binding%20conditions%20on%20the%20learning%20process%20of%20dual%20potentials.%20Such%20a%0Aregularization%20provides%20the%20upper%20bound%20estimation%20over%20the%20distribution%20of%0Apossible%20conjugate%20potentials%20and%20makes%20the%20learning%20stable%2C%20completely%0Aeliminating%20the%20need%20for%20additional%20extensive%20fine-tuning.%20Proposed%20method%2C%0Acalled%20Expectile-Regularised%20Neural%20Optimal%20Transport%20%28ENOT%29%2C%20outperforms%0Aprevious%20state-of-the-art%20approaches%20on%20the%20established%20Wasserstein-2%20benchmark%0Atasks%20by%20a%20large%20margin%20%28up%20to%20a%203-fold%20improvement%20in%20quality%20and%20up%20to%20a%0A10-fold%20improvement%20in%20runtime%29.%20Moreover%2C%20we%20showcase%20performance%20of%20ENOT%20for%0Avarying%20cost%20functions%20on%20different%20tasks%20such%20as%20image%20generation%2C%20showing%0Arobustness%20of%20proposed%20algorithm.%20OTT-JAX%20library%20includes%20our%20implementation%0Aof%20ENOT%20algorithm%20https%3A//ott-jax.readthedocs.io/en/latest/tutorials/ENOT.html%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.03777v3&entry.124074799=Read"},
{"title": "BACON: Supercharge Your VLM with Bag-of-Concept Graph to Mitigate\n  Hallucinations", "author": "Zhantao Yang and Ruili Feng and Keyu Yan and Huangji Wang and Zhicai Wang and Shangwen Zhu and Han Zhang and Jie Xiao and Pingyu Wu and Kai Zhu and Jixuan Chen and Chen-Wei Xie and Chaojie Mao and Yue Yang and Hongyang Zhang and Yu Liu and Fan Cheng", "abstract": "  This paper presents Bag-of-Concept Graph (BACON) to gift models with limited\nlinguistic abilities to taste the privilege of Vision Language Models (VLMs)\nand boost downstream tasks such as detection, visual question answering (VQA),\nand image generation. Since the visual scenes in physical worlds are structured\nwith complex relations between objects, BACON breaks down annotations into\nbasic minimum elements and presents them in a graph structure. Element-wise\nstyle enables easy understanding, and structural composition liberates\ndifficult locating. Careful prompt design births the BACON captions with the\nhelp of public-available VLMs and segmentation methods. In this way, we gather\na dataset with 100K annotated images, which endow VLMs with remarkable\ncapabilities, such as accurately generating BACON, transforming prompts into\nBACON format, envisioning scenarios in the style of BACONr, and dynamically\nmodifying elements within BACON through interactive dialogue and more. Wide\nrepresentative experiments, including detection, VQA, and image generation\ntasks, tell BACON as a lifeline to achieve previous out-of-reach tasks or excel\nin their current cutting-edge solutions.\n", "link": "http://arxiv.org/abs/2407.03314v1", "date": "2024-07-03", "relevancy": 1.9934, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5013}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4963}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4962}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BACON%3A%20Supercharge%20Your%20VLM%20with%20Bag-of-Concept%20Graph%20to%20Mitigate%0A%20%20Hallucinations&body=Title%3A%20BACON%3A%20Supercharge%20Your%20VLM%20with%20Bag-of-Concept%20Graph%20to%20Mitigate%0A%20%20Hallucinations%0AAuthor%3A%20Zhantao%20Yang%20and%20Ruili%20Feng%20and%20Keyu%20Yan%20and%20Huangji%20Wang%20and%20Zhicai%20Wang%20and%20Shangwen%20Zhu%20and%20Han%20Zhang%20and%20Jie%20Xiao%20and%20Pingyu%20Wu%20and%20Kai%20Zhu%20and%20Jixuan%20Chen%20and%20Chen-Wei%20Xie%20and%20Chaojie%20Mao%20and%20Yue%20Yang%20and%20Hongyang%20Zhang%20and%20Yu%20Liu%20and%20Fan%20Cheng%0AAbstract%3A%20%20%20This%20paper%20presents%20Bag-of-Concept%20Graph%20%28BACON%29%20to%20gift%20models%20with%20limited%0Alinguistic%20abilities%20to%20taste%20the%20privilege%20of%20Vision%20Language%20Models%20%28VLMs%29%0Aand%20boost%20downstream%20tasks%20such%20as%20detection%2C%20visual%20question%20answering%20%28VQA%29%2C%0Aand%20image%20generation.%20Since%20the%20visual%20scenes%20in%20physical%20worlds%20are%20structured%0Awith%20complex%20relations%20between%20objects%2C%20BACON%20breaks%20down%20annotations%20into%0Abasic%20minimum%20elements%20and%20presents%20them%20in%20a%20graph%20structure.%20Element-wise%0Astyle%20enables%20easy%20understanding%2C%20and%20structural%20composition%20liberates%0Adifficult%20locating.%20Careful%20prompt%20design%20births%20the%20BACON%20captions%20with%20the%0Ahelp%20of%20public-available%20VLMs%20and%20segmentation%20methods.%20In%20this%20way%2C%20we%20gather%0Aa%20dataset%20with%20100K%20annotated%20images%2C%20which%20endow%20VLMs%20with%20remarkable%0Acapabilities%2C%20such%20as%20accurately%20generating%20BACON%2C%20transforming%20prompts%20into%0ABACON%20format%2C%20envisioning%20scenarios%20in%20the%20style%20of%20BACONr%2C%20and%20dynamically%0Amodifying%20elements%20within%20BACON%20through%20interactive%20dialogue%20and%20more.%20Wide%0Arepresentative%20experiments%2C%20including%20detection%2C%20VQA%2C%20and%20image%20generation%0Atasks%2C%20tell%20BACON%20as%20a%20lifeline%20to%20achieve%20previous%20out-of-reach%20tasks%20or%20excel%0Ain%20their%20current%20cutting-edge%20solutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03314v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBACON%253A%2520Supercharge%2520Your%2520VLM%2520with%2520Bag-of-Concept%2520Graph%2520to%2520Mitigate%250A%2520%2520Hallucinations%26entry.906535625%3DZhantao%2520Yang%2520and%2520Ruili%2520Feng%2520and%2520Keyu%2520Yan%2520and%2520Huangji%2520Wang%2520and%2520Zhicai%2520Wang%2520and%2520Shangwen%2520Zhu%2520and%2520Han%2520Zhang%2520and%2520Jie%2520Xiao%2520and%2520Pingyu%2520Wu%2520and%2520Kai%2520Zhu%2520and%2520Jixuan%2520Chen%2520and%2520Chen-Wei%2520Xie%2520and%2520Chaojie%2520Mao%2520and%2520Yue%2520Yang%2520and%2520Hongyang%2520Zhang%2520and%2520Yu%2520Liu%2520and%2520Fan%2520Cheng%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520Bag-of-Concept%2520Graph%2520%2528BACON%2529%2520to%2520gift%2520models%2520with%2520limited%250Alinguistic%2520abilities%2520to%2520taste%2520the%2520privilege%2520of%2520Vision%2520Language%2520Models%2520%2528VLMs%2529%250Aand%2520boost%2520downstream%2520tasks%2520such%2520as%2520detection%252C%2520visual%2520question%2520answering%2520%2528VQA%2529%252C%250Aand%2520image%2520generation.%2520Since%2520the%2520visual%2520scenes%2520in%2520physical%2520worlds%2520are%2520structured%250Awith%2520complex%2520relations%2520between%2520objects%252C%2520BACON%2520breaks%2520down%2520annotations%2520into%250Abasic%2520minimum%2520elements%2520and%2520presents%2520them%2520in%2520a%2520graph%2520structure.%2520Element-wise%250Astyle%2520enables%2520easy%2520understanding%252C%2520and%2520structural%2520composition%2520liberates%250Adifficult%2520locating.%2520Careful%2520prompt%2520design%2520births%2520the%2520BACON%2520captions%2520with%2520the%250Ahelp%2520of%2520public-available%2520VLMs%2520and%2520segmentation%2520methods.%2520In%2520this%2520way%252C%2520we%2520gather%250Aa%2520dataset%2520with%2520100K%2520annotated%2520images%252C%2520which%2520endow%2520VLMs%2520with%2520remarkable%250Acapabilities%252C%2520such%2520as%2520accurately%2520generating%2520BACON%252C%2520transforming%2520prompts%2520into%250ABACON%2520format%252C%2520envisioning%2520scenarios%2520in%2520the%2520style%2520of%2520BACONr%252C%2520and%2520dynamically%250Amodifying%2520elements%2520within%2520BACON%2520through%2520interactive%2520dialogue%2520and%2520more.%2520Wide%250Arepresentative%2520experiments%252C%2520including%2520detection%252C%2520VQA%252C%2520and%2520image%2520generation%250Atasks%252C%2520tell%2520BACON%2520as%2520a%2520lifeline%2520to%2520achieve%2520previous%2520out-of-reach%2520tasks%2520or%2520excel%250Ain%2520their%2520current%2520cutting-edge%2520solutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03314v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BACON%3A%20Supercharge%20Your%20VLM%20with%20Bag-of-Concept%20Graph%20to%20Mitigate%0A%20%20Hallucinations&entry.906535625=Zhantao%20Yang%20and%20Ruili%20Feng%20and%20Keyu%20Yan%20and%20Huangji%20Wang%20and%20Zhicai%20Wang%20and%20Shangwen%20Zhu%20and%20Han%20Zhang%20and%20Jie%20Xiao%20and%20Pingyu%20Wu%20and%20Kai%20Zhu%20and%20Jixuan%20Chen%20and%20Chen-Wei%20Xie%20and%20Chaojie%20Mao%20and%20Yue%20Yang%20and%20Hongyang%20Zhang%20and%20Yu%20Liu%20and%20Fan%20Cheng&entry.1292438233=%20%20This%20paper%20presents%20Bag-of-Concept%20Graph%20%28BACON%29%20to%20gift%20models%20with%20limited%0Alinguistic%20abilities%20to%20taste%20the%20privilege%20of%20Vision%20Language%20Models%20%28VLMs%29%0Aand%20boost%20downstream%20tasks%20such%20as%20detection%2C%20visual%20question%20answering%20%28VQA%29%2C%0Aand%20image%20generation.%20Since%20the%20visual%20scenes%20in%20physical%20worlds%20are%20structured%0Awith%20complex%20relations%20between%20objects%2C%20BACON%20breaks%20down%20annotations%20into%0Abasic%20minimum%20elements%20and%20presents%20them%20in%20a%20graph%20structure.%20Element-wise%0Astyle%20enables%20easy%20understanding%2C%20and%20structural%20composition%20liberates%0Adifficult%20locating.%20Careful%20prompt%20design%20births%20the%20BACON%20captions%20with%20the%0Ahelp%20of%20public-available%20VLMs%20and%20segmentation%20methods.%20In%20this%20way%2C%20we%20gather%0Aa%20dataset%20with%20100K%20annotated%20images%2C%20which%20endow%20VLMs%20with%20remarkable%0Acapabilities%2C%20such%20as%20accurately%20generating%20BACON%2C%20transforming%20prompts%20into%0ABACON%20format%2C%20envisioning%20scenarios%20in%20the%20style%20of%20BACONr%2C%20and%20dynamically%0Amodifying%20elements%20within%20BACON%20through%20interactive%20dialogue%20and%20more.%20Wide%0Arepresentative%20experiments%2C%20including%20detection%2C%20VQA%2C%20and%20image%20generation%0Atasks%2C%20tell%20BACON%20as%20a%20lifeline%20to%20achieve%20previous%20out-of-reach%20tasks%20or%20excel%0Ain%20their%20current%20cutting-edge%20solutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03314v1&entry.124074799=Read"},
{"title": "Improving Zero-shot Generalization of Learned Prompts via Unsupervised\n  Knowledge Distillation", "author": "Marco Mistretta and Alberto Baldrati and Marco Bertini and Andrew D. Bagdanov", "abstract": "  Vision-Language Models (VLMs) demonstrate remarkable zero-shot generalization\nto unseen tasks, but fall short of the performance of supervised methods in\ngeneralizing to downstream tasks with limited data. Prompt learning is emerging\nas a parameter-efficient method for adapting VLMs, but state-of-the-art\napproaches require annotated samples. In this paper we propose a novel approach\nto prompt learning based on unsupervised knowledge distillation from more\npowerful models. Our approach, which we call Knowledge Distillation Prompt\nLearning (KDPL), can be integrated into existing prompt learning techniques and\neliminates the need for labeled examples during adaptation. Our experiments on\nmore than ten standard benchmark datasets demonstrate that KDPL is very\neffective at improving generalization of learned prompts for zero-shot domain\ngeneralization, zero-shot cross-dataset generalization, and zero-shot\nbase-to-novel class generalization problems. KDPL requires no ground-truth\nlabels for adaptation, and moreover we show that even in the absence of any\nknowledge of training class names it can be used to effectively transfer\nknowledge. The code is publicly available at https://github.com/miccunifi/KDPL.\n", "link": "http://arxiv.org/abs/2407.03056v1", "date": "2024-07-03", "relevancy": 1.9821, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5015}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4925}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4908}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improving%20Zero-shot%20Generalization%20of%20Learned%20Prompts%20via%20Unsupervised%0A%20%20Knowledge%20Distillation&body=Title%3A%20Improving%20Zero-shot%20Generalization%20of%20Learned%20Prompts%20via%20Unsupervised%0A%20%20Knowledge%20Distillation%0AAuthor%3A%20Marco%20Mistretta%20and%20Alberto%20Baldrati%20and%20Marco%20Bertini%20and%20Andrew%20D.%20Bagdanov%0AAbstract%3A%20%20%20Vision-Language%20Models%20%28VLMs%29%20demonstrate%20remarkable%20zero-shot%20generalization%0Ato%20unseen%20tasks%2C%20but%20fall%20short%20of%20the%20performance%20of%20supervised%20methods%20in%0Ageneralizing%20to%20downstream%20tasks%20with%20limited%20data.%20Prompt%20learning%20is%20emerging%0Aas%20a%20parameter-efficient%20method%20for%20adapting%20VLMs%2C%20but%20state-of-the-art%0Aapproaches%20require%20annotated%20samples.%20In%20this%20paper%20we%20propose%20a%20novel%20approach%0Ato%20prompt%20learning%20based%20on%20unsupervised%20knowledge%20distillation%20from%20more%0Apowerful%20models.%20Our%20approach%2C%20which%20we%20call%20Knowledge%20Distillation%20Prompt%0ALearning%20%28KDPL%29%2C%20can%20be%20integrated%20into%20existing%20prompt%20learning%20techniques%20and%0Aeliminates%20the%20need%20for%20labeled%20examples%20during%20adaptation.%20Our%20experiments%20on%0Amore%20than%20ten%20standard%20benchmark%20datasets%20demonstrate%20that%20KDPL%20is%20very%0Aeffective%20at%20improving%20generalization%20of%20learned%20prompts%20for%20zero-shot%20domain%0Ageneralization%2C%20zero-shot%20cross-dataset%20generalization%2C%20and%20zero-shot%0Abase-to-novel%20class%20generalization%20problems.%20KDPL%20requires%20no%20ground-truth%0Alabels%20for%20adaptation%2C%20and%20moreover%20we%20show%20that%20even%20in%20the%20absence%20of%20any%0Aknowledge%20of%20training%20class%20names%20it%20can%20be%20used%20to%20effectively%20transfer%0Aknowledge.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/miccunifi/KDPL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproving%2520Zero-shot%2520Generalization%2520of%2520Learned%2520Prompts%2520via%2520Unsupervised%250A%2520%2520Knowledge%2520Distillation%26entry.906535625%3DMarco%2520Mistretta%2520and%2520Alberto%2520Baldrati%2520and%2520Marco%2520Bertini%2520and%2520Andrew%2520D.%2520Bagdanov%26entry.1292438233%3D%2520%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520demonstrate%2520remarkable%2520zero-shot%2520generalization%250Ato%2520unseen%2520tasks%252C%2520but%2520fall%2520short%2520of%2520the%2520performance%2520of%2520supervised%2520methods%2520in%250Ageneralizing%2520to%2520downstream%2520tasks%2520with%2520limited%2520data.%2520Prompt%2520learning%2520is%2520emerging%250Aas%2520a%2520parameter-efficient%2520method%2520for%2520adapting%2520VLMs%252C%2520but%2520state-of-the-art%250Aapproaches%2520require%2520annotated%2520samples.%2520In%2520this%2520paper%2520we%2520propose%2520a%2520novel%2520approach%250Ato%2520prompt%2520learning%2520based%2520on%2520unsupervised%2520knowledge%2520distillation%2520from%2520more%250Apowerful%2520models.%2520Our%2520approach%252C%2520which%2520we%2520call%2520Knowledge%2520Distillation%2520Prompt%250ALearning%2520%2528KDPL%2529%252C%2520can%2520be%2520integrated%2520into%2520existing%2520prompt%2520learning%2520techniques%2520and%250Aeliminates%2520the%2520need%2520for%2520labeled%2520examples%2520during%2520adaptation.%2520Our%2520experiments%2520on%250Amore%2520than%2520ten%2520standard%2520benchmark%2520datasets%2520demonstrate%2520that%2520KDPL%2520is%2520very%250Aeffective%2520at%2520improving%2520generalization%2520of%2520learned%2520prompts%2520for%2520zero-shot%2520domain%250Ageneralization%252C%2520zero-shot%2520cross-dataset%2520generalization%252C%2520and%2520zero-shot%250Abase-to-novel%2520class%2520generalization%2520problems.%2520KDPL%2520requires%2520no%2520ground-truth%250Alabels%2520for%2520adaptation%252C%2520and%2520moreover%2520we%2520show%2520that%2520even%2520in%2520the%2520absence%2520of%2520any%250Aknowledge%2520of%2520training%2520class%2520names%2520it%2520can%2520be%2520used%2520to%2520effectively%2520transfer%250Aknowledge.%2520The%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/miccunifi/KDPL.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improving%20Zero-shot%20Generalization%20of%20Learned%20Prompts%20via%20Unsupervised%0A%20%20Knowledge%20Distillation&entry.906535625=Marco%20Mistretta%20and%20Alberto%20Baldrati%20and%20Marco%20Bertini%20and%20Andrew%20D.%20Bagdanov&entry.1292438233=%20%20Vision-Language%20Models%20%28VLMs%29%20demonstrate%20remarkable%20zero-shot%20generalization%0Ato%20unseen%20tasks%2C%20but%20fall%20short%20of%20the%20performance%20of%20supervised%20methods%20in%0Ageneralizing%20to%20downstream%20tasks%20with%20limited%20data.%20Prompt%20learning%20is%20emerging%0Aas%20a%20parameter-efficient%20method%20for%20adapting%20VLMs%2C%20but%20state-of-the-art%0Aapproaches%20require%20annotated%20samples.%20In%20this%20paper%20we%20propose%20a%20novel%20approach%0Ato%20prompt%20learning%20based%20on%20unsupervised%20knowledge%20distillation%20from%20more%0Apowerful%20models.%20Our%20approach%2C%20which%20we%20call%20Knowledge%20Distillation%20Prompt%0ALearning%20%28KDPL%29%2C%20can%20be%20integrated%20into%20existing%20prompt%20learning%20techniques%20and%0Aeliminates%20the%20need%20for%20labeled%20examples%20during%20adaptation.%20Our%20experiments%20on%0Amore%20than%20ten%20standard%20benchmark%20datasets%20demonstrate%20that%20KDPL%20is%20very%0Aeffective%20at%20improving%20generalization%20of%20learned%20prompts%20for%20zero-shot%20domain%0Ageneralization%2C%20zero-shot%20cross-dataset%20generalization%2C%20and%20zero-shot%0Abase-to-novel%20class%20generalization%20problems.%20KDPL%20requires%20no%20ground-truth%0Alabels%20for%20adaptation%2C%20and%20moreover%20we%20show%20that%20even%20in%20the%20absence%20of%20any%0Aknowledge%20of%20training%20class%20names%20it%20can%20be%20used%20to%20effectively%20transfer%0Aknowledge.%20The%20code%20is%20publicly%20available%20at%20https%3A//github.com/miccunifi/KDPL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03056v1&entry.124074799=Read"},
{"title": "LACIE: Listener-Aware Finetuning for Confidence Calibration in Large\n  Language Models", "author": "Elias Stengel-Eskin and Peter Hase and Mohit Bansal", "abstract": "  When answering questions, LLMs can convey not only an answer, but a level of\nconfidence about the answer being correct. This includes explicit confidence\nmarkers (e.g. giving a numeric score) as well as implicit markers, like an\nauthoritative tone or elaborating with additional knowledge. For LLMs to be\ntrustworthy knowledge sources, the confidence they convey should match their\nactual expertise; however, most current models tend towards overconfidence. To\ncalibrate both implicit and explicit confidence markers, we introduce a\npragmatic, listener-aware finetuning method (LACIE) that models the listener,\nconsidering not only whether an answer is right, but whether it will be\naccepted by a listener. We cast calibration as preference optimization,\ncreating data via a two-agent game, where a speaker model's outputs are judged\nby a simulated listener. We then finetune three LLMs (Mistral-7B, Llama3-8B,\nLlama3-70B) with LACIE, and show that the resulting models are better\ncalibrated w.r.t. a simulated listener. Crucially, these trends transfer to\nhuman listeners, helping them correctly predict model correctness: we conduct a\nhuman evaluation where annotators accept or reject an LLM's answers, finding\nthat training with LACIE results in 47% fewer incorrect answers being accepted\nwhile maintaining the same level of acceptance for correct answers.\nFurthermore, LACIE generalizes to another dataset, resulting in a large\nincrease in truthfulness on TruthfulQA when trained on TriviaQA. Our analysis\nindicates that LACIE leads to a better confidence separation between correct\nand incorrect examples. Qualitatively, we find that a LACIE-trained model\nhedges more and implicitly signals certainty when it is correct by using an\nauthoritative tone or including details. Finally, LACIE finetuning leads to an\nemergent increase in model abstention (e.g. saying \"I don't know\") for answers\nthat are likely wrong.\n", "link": "http://arxiv.org/abs/2405.21028v2", "date": "2024-07-03", "relevancy": 1.98, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.546}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4874}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4822}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LACIE%3A%20Listener-Aware%20Finetuning%20for%20Confidence%20Calibration%20in%20Large%0A%20%20Language%20Models&body=Title%3A%20LACIE%3A%20Listener-Aware%20Finetuning%20for%20Confidence%20Calibration%20in%20Large%0A%20%20Language%20Models%0AAuthor%3A%20Elias%20Stengel-Eskin%20and%20Peter%20Hase%20and%20Mohit%20Bansal%0AAbstract%3A%20%20%20When%20answering%20questions%2C%20LLMs%20can%20convey%20not%20only%20an%20answer%2C%20but%20a%20level%20of%0Aconfidence%20about%20the%20answer%20being%20correct.%20This%20includes%20explicit%20confidence%0Amarkers%20%28e.g.%20giving%20a%20numeric%20score%29%20as%20well%20as%20implicit%20markers%2C%20like%20an%0Aauthoritative%20tone%20or%20elaborating%20with%20additional%20knowledge.%20For%20LLMs%20to%20be%0Atrustworthy%20knowledge%20sources%2C%20the%20confidence%20they%20convey%20should%20match%20their%0Aactual%20expertise%3B%20however%2C%20most%20current%20models%20tend%20towards%20overconfidence.%20To%0Acalibrate%20both%20implicit%20and%20explicit%20confidence%20markers%2C%20we%20introduce%20a%0Apragmatic%2C%20listener-aware%20finetuning%20method%20%28LACIE%29%20that%20models%20the%20listener%2C%0Aconsidering%20not%20only%20whether%20an%20answer%20is%20right%2C%20but%20whether%20it%20will%20be%0Aaccepted%20by%20a%20listener.%20We%20cast%20calibration%20as%20preference%20optimization%2C%0Acreating%20data%20via%20a%20two-agent%20game%2C%20where%20a%20speaker%20model%27s%20outputs%20are%20judged%0Aby%20a%20simulated%20listener.%20We%20then%20finetune%20three%20LLMs%20%28Mistral-7B%2C%20Llama3-8B%2C%0ALlama3-70B%29%20with%20LACIE%2C%20and%20show%20that%20the%20resulting%20models%20are%20better%0Acalibrated%20w.r.t.%20a%20simulated%20listener.%20Crucially%2C%20these%20trends%20transfer%20to%0Ahuman%20listeners%2C%20helping%20them%20correctly%20predict%20model%20correctness%3A%20we%20conduct%20a%0Ahuman%20evaluation%20where%20annotators%20accept%20or%20reject%20an%20LLM%27s%20answers%2C%20finding%0Athat%20training%20with%20LACIE%20results%20in%2047%25%20fewer%20incorrect%20answers%20being%20accepted%0Awhile%20maintaining%20the%20same%20level%20of%20acceptance%20for%20correct%20answers.%0AFurthermore%2C%20LACIE%20generalizes%20to%20another%20dataset%2C%20resulting%20in%20a%20large%0Aincrease%20in%20truthfulness%20on%20TruthfulQA%20when%20trained%20on%20TriviaQA.%20Our%20analysis%0Aindicates%20that%20LACIE%20leads%20to%20a%20better%20confidence%20separation%20between%20correct%0Aand%20incorrect%20examples.%20Qualitatively%2C%20we%20find%20that%20a%20LACIE-trained%20model%0Ahedges%20more%20and%20implicitly%20signals%20certainty%20when%20it%20is%20correct%20by%20using%20an%0Aauthoritative%20tone%20or%20including%20details.%20Finally%2C%20LACIE%20finetuning%20leads%20to%20an%0Aemergent%20increase%20in%20model%20abstention%20%28e.g.%20saying%20%22I%20don%27t%20know%22%29%20for%20answers%0Athat%20are%20likely%20wrong.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.21028v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLACIE%253A%2520Listener-Aware%2520Finetuning%2520for%2520Confidence%2520Calibration%2520in%2520Large%250A%2520%2520Language%2520Models%26entry.906535625%3DElias%2520Stengel-Eskin%2520and%2520Peter%2520Hase%2520and%2520Mohit%2520Bansal%26entry.1292438233%3D%2520%2520When%2520answering%2520questions%252C%2520LLMs%2520can%2520convey%2520not%2520only%2520an%2520answer%252C%2520but%2520a%2520level%2520of%250Aconfidence%2520about%2520the%2520answer%2520being%2520correct.%2520This%2520includes%2520explicit%2520confidence%250Amarkers%2520%2528e.g.%2520giving%2520a%2520numeric%2520score%2529%2520as%2520well%2520as%2520implicit%2520markers%252C%2520like%2520an%250Aauthoritative%2520tone%2520or%2520elaborating%2520with%2520additional%2520knowledge.%2520For%2520LLMs%2520to%2520be%250Atrustworthy%2520knowledge%2520sources%252C%2520the%2520confidence%2520they%2520convey%2520should%2520match%2520their%250Aactual%2520expertise%253B%2520however%252C%2520most%2520current%2520models%2520tend%2520towards%2520overconfidence.%2520To%250Acalibrate%2520both%2520implicit%2520and%2520explicit%2520confidence%2520markers%252C%2520we%2520introduce%2520a%250Apragmatic%252C%2520listener-aware%2520finetuning%2520method%2520%2528LACIE%2529%2520that%2520models%2520the%2520listener%252C%250Aconsidering%2520not%2520only%2520whether%2520an%2520answer%2520is%2520right%252C%2520but%2520whether%2520it%2520will%2520be%250Aaccepted%2520by%2520a%2520listener.%2520We%2520cast%2520calibration%2520as%2520preference%2520optimization%252C%250Acreating%2520data%2520via%2520a%2520two-agent%2520game%252C%2520where%2520a%2520speaker%2520model%2527s%2520outputs%2520are%2520judged%250Aby%2520a%2520simulated%2520listener.%2520We%2520then%2520finetune%2520three%2520LLMs%2520%2528Mistral-7B%252C%2520Llama3-8B%252C%250ALlama3-70B%2529%2520with%2520LACIE%252C%2520and%2520show%2520that%2520the%2520resulting%2520models%2520are%2520better%250Acalibrated%2520w.r.t.%2520a%2520simulated%2520listener.%2520Crucially%252C%2520these%2520trends%2520transfer%2520to%250Ahuman%2520listeners%252C%2520helping%2520them%2520correctly%2520predict%2520model%2520correctness%253A%2520we%2520conduct%2520a%250Ahuman%2520evaluation%2520where%2520annotators%2520accept%2520or%2520reject%2520an%2520LLM%2527s%2520answers%252C%2520finding%250Athat%2520training%2520with%2520LACIE%2520results%2520in%252047%2525%2520fewer%2520incorrect%2520answers%2520being%2520accepted%250Awhile%2520maintaining%2520the%2520same%2520level%2520of%2520acceptance%2520for%2520correct%2520answers.%250AFurthermore%252C%2520LACIE%2520generalizes%2520to%2520another%2520dataset%252C%2520resulting%2520in%2520a%2520large%250Aincrease%2520in%2520truthfulness%2520on%2520TruthfulQA%2520when%2520trained%2520on%2520TriviaQA.%2520Our%2520analysis%250Aindicates%2520that%2520LACIE%2520leads%2520to%2520a%2520better%2520confidence%2520separation%2520between%2520correct%250Aand%2520incorrect%2520examples.%2520Qualitatively%252C%2520we%2520find%2520that%2520a%2520LACIE-trained%2520model%250Ahedges%2520more%2520and%2520implicitly%2520signals%2520certainty%2520when%2520it%2520is%2520correct%2520by%2520using%2520an%250Aauthoritative%2520tone%2520or%2520including%2520details.%2520Finally%252C%2520LACIE%2520finetuning%2520leads%2520to%2520an%250Aemergent%2520increase%2520in%2520model%2520abstention%2520%2528e.g.%2520saying%2520%2522I%2520don%2527t%2520know%2522%2529%2520for%2520answers%250Athat%2520are%2520likely%2520wrong.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.21028v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LACIE%3A%20Listener-Aware%20Finetuning%20for%20Confidence%20Calibration%20in%20Large%0A%20%20Language%20Models&entry.906535625=Elias%20Stengel-Eskin%20and%20Peter%20Hase%20and%20Mohit%20Bansal&entry.1292438233=%20%20When%20answering%20questions%2C%20LLMs%20can%20convey%20not%20only%20an%20answer%2C%20but%20a%20level%20of%0Aconfidence%20about%20the%20answer%20being%20correct.%20This%20includes%20explicit%20confidence%0Amarkers%20%28e.g.%20giving%20a%20numeric%20score%29%20as%20well%20as%20implicit%20markers%2C%20like%20an%0Aauthoritative%20tone%20or%20elaborating%20with%20additional%20knowledge.%20For%20LLMs%20to%20be%0Atrustworthy%20knowledge%20sources%2C%20the%20confidence%20they%20convey%20should%20match%20their%0Aactual%20expertise%3B%20however%2C%20most%20current%20models%20tend%20towards%20overconfidence.%20To%0Acalibrate%20both%20implicit%20and%20explicit%20confidence%20markers%2C%20we%20introduce%20a%0Apragmatic%2C%20listener-aware%20finetuning%20method%20%28LACIE%29%20that%20models%20the%20listener%2C%0Aconsidering%20not%20only%20whether%20an%20answer%20is%20right%2C%20but%20whether%20it%20will%20be%0Aaccepted%20by%20a%20listener.%20We%20cast%20calibration%20as%20preference%20optimization%2C%0Acreating%20data%20via%20a%20two-agent%20game%2C%20where%20a%20speaker%20model%27s%20outputs%20are%20judged%0Aby%20a%20simulated%20listener.%20We%20then%20finetune%20three%20LLMs%20%28Mistral-7B%2C%20Llama3-8B%2C%0ALlama3-70B%29%20with%20LACIE%2C%20and%20show%20that%20the%20resulting%20models%20are%20better%0Acalibrated%20w.r.t.%20a%20simulated%20listener.%20Crucially%2C%20these%20trends%20transfer%20to%0Ahuman%20listeners%2C%20helping%20them%20correctly%20predict%20model%20correctness%3A%20we%20conduct%20a%0Ahuman%20evaluation%20where%20annotators%20accept%20or%20reject%20an%20LLM%27s%20answers%2C%20finding%0Athat%20training%20with%20LACIE%20results%20in%2047%25%20fewer%20incorrect%20answers%20being%20accepted%0Awhile%20maintaining%20the%20same%20level%20of%20acceptance%20for%20correct%20answers.%0AFurthermore%2C%20LACIE%20generalizes%20to%20another%20dataset%2C%20resulting%20in%20a%20large%0Aincrease%20in%20truthfulness%20on%20TruthfulQA%20when%20trained%20on%20TriviaQA.%20Our%20analysis%0Aindicates%20that%20LACIE%20leads%20to%20a%20better%20confidence%20separation%20between%20correct%0Aand%20incorrect%20examples.%20Qualitatively%2C%20we%20find%20that%20a%20LACIE-trained%20model%0Ahedges%20more%20and%20implicitly%20signals%20certainty%20when%20it%20is%20correct%20by%20using%20an%0Aauthoritative%20tone%20or%20including%20details.%20Finally%2C%20LACIE%20finetuning%20leads%20to%20an%0Aemergent%20increase%20in%20model%20abstention%20%28e.g.%20saying%20%22I%20don%27t%20know%22%29%20for%20answers%0Athat%20are%20likely%20wrong.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.21028v2&entry.124074799=Read"},
{"title": "Nearly Linear Sparsification of $\\ell_p$ Subspace Approximation", "author": "David P. Woodruff and Taisuke Yasuda", "abstract": "  The $\\ell_p$ subspace approximation problem is an NP-hard low rank\napproximation problem that generalizes the median hyperplane problem ($p = 1$),\nprincipal component analysis ($p = 2$), and the center hyperplane problem ($p =\n\\infty$). A popular approach to cope with the NP-hardness of this problem is to\ncompute a strong coreset, which is a small weighted subset of the input points\nwhich simultaneously approximates the cost of every $k$-dimensional subspace,\ntypically to $(1+\\varepsilon)$ relative error for a small constant\n$\\varepsilon$.\n  We obtain the first algorithm for constructing a strong coreset for $\\ell_p$\nsubspace approximation with a nearly optimal dependence on the rank parameter\n$k$, obtaining a nearly linear bound of $\\tilde\nO(k)\\mathrm{poly}(\\varepsilon^{-1})$ for $p<2$ and $\\tilde\nO(k^{p/2})\\mathrm{poly}(\\varepsilon^{-1})$ for $p>2$. Prior constructions\neither achieved a similar size bound but produced a coreset with a modification\nof the original points [SW18, FKW21], or produced a coreset of the original\npoints but lost $\\mathrm{poly}(k)$ factors in the coreset size [HV20, WY23].\n  Our techniques also lead to the first nearly optimal online strong coresets\nfor $\\ell_p$ subspace approximation with similar bounds as the offline setting,\nresolving a problem of [WY23]. All prior approaches lose $\\mathrm{poly}(k)$\nfactors in this setting, even when allowed to modify the original points.\n", "link": "http://arxiv.org/abs/2407.03262v1", "date": "2024-07-03", "relevancy": 1.9669, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.3997}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3945}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.3859}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Nearly%20Linear%20Sparsification%20of%20%24%5Cell_p%24%20Subspace%20Approximation&body=Title%3A%20Nearly%20Linear%20Sparsification%20of%20%24%5Cell_p%24%20Subspace%20Approximation%0AAuthor%3A%20David%20P.%20Woodruff%20and%20Taisuke%20Yasuda%0AAbstract%3A%20%20%20The%20%24%5Cell_p%24%20subspace%20approximation%20problem%20is%20an%20NP-hard%20low%20rank%0Aapproximation%20problem%20that%20generalizes%20the%20median%20hyperplane%20problem%20%28%24p%20%3D%201%24%29%2C%0Aprincipal%20component%20analysis%20%28%24p%20%3D%202%24%29%2C%20and%20the%20center%20hyperplane%20problem%20%28%24p%20%3D%0A%5Cinfty%24%29.%20A%20popular%20approach%20to%20cope%20with%20the%20NP-hardness%20of%20this%20problem%20is%20to%0Acompute%20a%20strong%20coreset%2C%20which%20is%20a%20small%20weighted%20subset%20of%20the%20input%20points%0Awhich%20simultaneously%20approximates%20the%20cost%20of%20every%20%24k%24-dimensional%20subspace%2C%0Atypically%20to%20%24%281%2B%5Cvarepsilon%29%24%20relative%20error%20for%20a%20small%20constant%0A%24%5Cvarepsilon%24.%0A%20%20We%20obtain%20the%20first%20algorithm%20for%20constructing%20a%20strong%20coreset%20for%20%24%5Cell_p%24%0Asubspace%20approximation%20with%20a%20nearly%20optimal%20dependence%20on%20the%20rank%20parameter%0A%24k%24%2C%20obtaining%20a%20nearly%20linear%20bound%20of%20%24%5Ctilde%0AO%28k%29%5Cmathrm%7Bpoly%7D%28%5Cvarepsilon%5E%7B-1%7D%29%24%20for%20%24p%3C2%24%20and%20%24%5Ctilde%0AO%28k%5E%7Bp/2%7D%29%5Cmathrm%7Bpoly%7D%28%5Cvarepsilon%5E%7B-1%7D%29%24%20for%20%24p%3E2%24.%20Prior%20constructions%0Aeither%20achieved%20a%20similar%20size%20bound%20but%20produced%20a%20coreset%20with%20a%20modification%0Aof%20the%20original%20points%20%5BSW18%2C%20FKW21%5D%2C%20or%20produced%20a%20coreset%20of%20the%20original%0Apoints%20but%20lost%20%24%5Cmathrm%7Bpoly%7D%28k%29%24%20factors%20in%20the%20coreset%20size%20%5BHV20%2C%20WY23%5D.%0A%20%20Our%20techniques%20also%20lead%20to%20the%20first%20nearly%20optimal%20online%20strong%20coresets%0Afor%20%24%5Cell_p%24%20subspace%20approximation%20with%20similar%20bounds%20as%20the%20offline%20setting%2C%0Aresolving%20a%20problem%20of%20%5BWY23%5D.%20All%20prior%20approaches%20lose%20%24%5Cmathrm%7Bpoly%7D%28k%29%24%0Afactors%20in%20this%20setting%2C%20even%20when%20allowed%20to%20modify%20the%20original%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03262v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNearly%2520Linear%2520Sparsification%2520of%2520%2524%255Cell_p%2524%2520Subspace%2520Approximation%26entry.906535625%3DDavid%2520P.%2520Woodruff%2520and%2520Taisuke%2520Yasuda%26entry.1292438233%3D%2520%2520The%2520%2524%255Cell_p%2524%2520subspace%2520approximation%2520problem%2520is%2520an%2520NP-hard%2520low%2520rank%250Aapproximation%2520problem%2520that%2520generalizes%2520the%2520median%2520hyperplane%2520problem%2520%2528%2524p%2520%253D%25201%2524%2529%252C%250Aprincipal%2520component%2520analysis%2520%2528%2524p%2520%253D%25202%2524%2529%252C%2520and%2520the%2520center%2520hyperplane%2520problem%2520%2528%2524p%2520%253D%250A%255Cinfty%2524%2529.%2520A%2520popular%2520approach%2520to%2520cope%2520with%2520the%2520NP-hardness%2520of%2520this%2520problem%2520is%2520to%250Acompute%2520a%2520strong%2520coreset%252C%2520which%2520is%2520a%2520small%2520weighted%2520subset%2520of%2520the%2520input%2520points%250Awhich%2520simultaneously%2520approximates%2520the%2520cost%2520of%2520every%2520%2524k%2524-dimensional%2520subspace%252C%250Atypically%2520to%2520%2524%25281%252B%255Cvarepsilon%2529%2524%2520relative%2520error%2520for%2520a%2520small%2520constant%250A%2524%255Cvarepsilon%2524.%250A%2520%2520We%2520obtain%2520the%2520first%2520algorithm%2520for%2520constructing%2520a%2520strong%2520coreset%2520for%2520%2524%255Cell_p%2524%250Asubspace%2520approximation%2520with%2520a%2520nearly%2520optimal%2520dependence%2520on%2520the%2520rank%2520parameter%250A%2524k%2524%252C%2520obtaining%2520a%2520nearly%2520linear%2520bound%2520of%2520%2524%255Ctilde%250AO%2528k%2529%255Cmathrm%257Bpoly%257D%2528%255Cvarepsilon%255E%257B-1%257D%2529%2524%2520for%2520%2524p%253C2%2524%2520and%2520%2524%255Ctilde%250AO%2528k%255E%257Bp/2%257D%2529%255Cmathrm%257Bpoly%257D%2528%255Cvarepsilon%255E%257B-1%257D%2529%2524%2520for%2520%2524p%253E2%2524.%2520Prior%2520constructions%250Aeither%2520achieved%2520a%2520similar%2520size%2520bound%2520but%2520produced%2520a%2520coreset%2520with%2520a%2520modification%250Aof%2520the%2520original%2520points%2520%255BSW18%252C%2520FKW21%255D%252C%2520or%2520produced%2520a%2520coreset%2520of%2520the%2520original%250Apoints%2520but%2520lost%2520%2524%255Cmathrm%257Bpoly%257D%2528k%2529%2524%2520factors%2520in%2520the%2520coreset%2520size%2520%255BHV20%252C%2520WY23%255D.%250A%2520%2520Our%2520techniques%2520also%2520lead%2520to%2520the%2520first%2520nearly%2520optimal%2520online%2520strong%2520coresets%250Afor%2520%2524%255Cell_p%2524%2520subspace%2520approximation%2520with%2520similar%2520bounds%2520as%2520the%2520offline%2520setting%252C%250Aresolving%2520a%2520problem%2520of%2520%255BWY23%255D.%2520All%2520prior%2520approaches%2520lose%2520%2524%255Cmathrm%257Bpoly%257D%2528k%2529%2524%250Afactors%2520in%2520this%2520setting%252C%2520even%2520when%2520allowed%2520to%2520modify%2520the%2520original%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03262v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nearly%20Linear%20Sparsification%20of%20%24%5Cell_p%24%20Subspace%20Approximation&entry.906535625=David%20P.%20Woodruff%20and%20Taisuke%20Yasuda&entry.1292438233=%20%20The%20%24%5Cell_p%24%20subspace%20approximation%20problem%20is%20an%20NP-hard%20low%20rank%0Aapproximation%20problem%20that%20generalizes%20the%20median%20hyperplane%20problem%20%28%24p%20%3D%201%24%29%2C%0Aprincipal%20component%20analysis%20%28%24p%20%3D%202%24%29%2C%20and%20the%20center%20hyperplane%20problem%20%28%24p%20%3D%0A%5Cinfty%24%29.%20A%20popular%20approach%20to%20cope%20with%20the%20NP-hardness%20of%20this%20problem%20is%20to%0Acompute%20a%20strong%20coreset%2C%20which%20is%20a%20small%20weighted%20subset%20of%20the%20input%20points%0Awhich%20simultaneously%20approximates%20the%20cost%20of%20every%20%24k%24-dimensional%20subspace%2C%0Atypically%20to%20%24%281%2B%5Cvarepsilon%29%24%20relative%20error%20for%20a%20small%20constant%0A%24%5Cvarepsilon%24.%0A%20%20We%20obtain%20the%20first%20algorithm%20for%20constructing%20a%20strong%20coreset%20for%20%24%5Cell_p%24%0Asubspace%20approximation%20with%20a%20nearly%20optimal%20dependence%20on%20the%20rank%20parameter%0A%24k%24%2C%20obtaining%20a%20nearly%20linear%20bound%20of%20%24%5Ctilde%0AO%28k%29%5Cmathrm%7Bpoly%7D%28%5Cvarepsilon%5E%7B-1%7D%29%24%20for%20%24p%3C2%24%20and%20%24%5Ctilde%0AO%28k%5E%7Bp/2%7D%29%5Cmathrm%7Bpoly%7D%28%5Cvarepsilon%5E%7B-1%7D%29%24%20for%20%24p%3E2%24.%20Prior%20constructions%0Aeither%20achieved%20a%20similar%20size%20bound%20but%20produced%20a%20coreset%20with%20a%20modification%0Aof%20the%20original%20points%20%5BSW18%2C%20FKW21%5D%2C%20or%20produced%20a%20coreset%20of%20the%20original%0Apoints%20but%20lost%20%24%5Cmathrm%7Bpoly%7D%28k%29%24%20factors%20in%20the%20coreset%20size%20%5BHV20%2C%20WY23%5D.%0A%20%20Our%20techniques%20also%20lead%20to%20the%20first%20nearly%20optimal%20online%20strong%20coresets%0Afor%20%24%5Cell_p%24%20subspace%20approximation%20with%20similar%20bounds%20as%20the%20offline%20setting%2C%0Aresolving%20a%20problem%20of%20%5BWY23%5D.%20All%20prior%20approaches%20lose%20%24%5Cmathrm%7Bpoly%7D%28k%29%24%0Afactors%20in%20this%20setting%2C%20even%20when%20allowed%20to%20modify%20the%20original%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03262v1&entry.124074799=Read"},
{"title": "Biomechanics-informed Non-rigid Medical Image Registration and its\n  Inverse Material Property Estimation with Linear and Nonlinear Elasticity", "author": "Zhe Min and Zachary M. C. Baum and Shaheer U. Saeed and Mark Emberton and Dean C. Barratt and Zeike A. Taylor and Yipeng Hu", "abstract": "  This paper investigates both biomechanical-constrained non-rigid medical\nimage registrations and accurate identifications of material properties for\nsoft tissues, using physics-informed neural networks (PINNs). The complex\nnonlinear elasticity theory is leveraged to formally establish the partial\ndifferential equations (PDEs) representing physics laws of biomechanical\nconstraints that need to be satisfied, with which registration and\nidentification tasks are treated as forward (i.e., data-driven solutions of\nPDEs) and inverse (i.e., parameter estimation) problems under PINNs\nrespectively. Two net configurations (i.e., Cfg1 and Cfg2) have also been\ncompared for both linear and nonlinear physics model. Two sets of experiments\nhave been conducted, using pairs of undeformed and deformed MR images from\nclinical cases of prostate cancer biopsy.\n  Our contributions are summarised as follows. 1) We developed a learning-based\nbiomechanical-constrained non-rigid registration algorithm using PINNs, where\nlinear elasticity is generalised to the nonlinear version. 2) We demonstrated\nextensively that nonlinear elasticity shows no statistical significance against\nlinear models in computing point-wise displacement vectors but their respective\nbenefits may depend on specific patients, with finite-element (FE) computed\nground-truth. 3) We formulated and solved the inverse parameter estimation\nproblem, under the joint optimisation scheme of registration and parameter\nidentification using PINNs, whose solutions can be accurately found by locating\nsaddle points.\n", "link": "http://arxiv.org/abs/2407.03292v1", "date": "2024-07-03", "relevancy": 1.9624, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5356}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4905}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Biomechanics-informed%20Non-rigid%20Medical%20Image%20Registration%20and%20its%0A%20%20Inverse%20Material%20Property%20Estimation%20with%20Linear%20and%20Nonlinear%20Elasticity&body=Title%3A%20Biomechanics-informed%20Non-rigid%20Medical%20Image%20Registration%20and%20its%0A%20%20Inverse%20Material%20Property%20Estimation%20with%20Linear%20and%20Nonlinear%20Elasticity%0AAuthor%3A%20Zhe%20Min%20and%20Zachary%20M.%20C.%20Baum%20and%20Shaheer%20U.%20Saeed%20and%20Mark%20Emberton%20and%20Dean%20C.%20Barratt%20and%20Zeike%20A.%20Taylor%20and%20Yipeng%20Hu%0AAbstract%3A%20%20%20This%20paper%20investigates%20both%20biomechanical-constrained%20non-rigid%20medical%0Aimage%20registrations%20and%20accurate%20identifications%20of%20material%20properties%20for%0Asoft%20tissues%2C%20using%20physics-informed%20neural%20networks%20%28PINNs%29.%20The%20complex%0Anonlinear%20elasticity%20theory%20is%20leveraged%20to%20formally%20establish%20the%20partial%0Adifferential%20equations%20%28PDEs%29%20representing%20physics%20laws%20of%20biomechanical%0Aconstraints%20that%20need%20to%20be%20satisfied%2C%20with%20which%20registration%20and%0Aidentification%20tasks%20are%20treated%20as%20forward%20%28i.e.%2C%20data-driven%20solutions%20of%0APDEs%29%20and%20inverse%20%28i.e.%2C%20parameter%20estimation%29%20problems%20under%20PINNs%0Arespectively.%20Two%20net%20configurations%20%28i.e.%2C%20Cfg1%20and%20Cfg2%29%20have%20also%20been%0Acompared%20for%20both%20linear%20and%20nonlinear%20physics%20model.%20Two%20sets%20of%20experiments%0Ahave%20been%20conducted%2C%20using%20pairs%20of%20undeformed%20and%20deformed%20MR%20images%20from%0Aclinical%20cases%20of%20prostate%20cancer%20biopsy.%0A%20%20Our%20contributions%20are%20summarised%20as%20follows.%201%29%20We%20developed%20a%20learning-based%0Abiomechanical-constrained%20non-rigid%20registration%20algorithm%20using%20PINNs%2C%20where%0Alinear%20elasticity%20is%20generalised%20to%20the%20nonlinear%20version.%202%29%20We%20demonstrated%0Aextensively%20that%20nonlinear%20elasticity%20shows%20no%20statistical%20significance%20against%0Alinear%20models%20in%20computing%20point-wise%20displacement%20vectors%20but%20their%20respective%0Abenefits%20may%20depend%20on%20specific%20patients%2C%20with%20finite-element%20%28FE%29%20computed%0Aground-truth.%203%29%20We%20formulated%20and%20solved%20the%20inverse%20parameter%20estimation%0Aproblem%2C%20under%20the%20joint%20optimisation%20scheme%20of%20registration%20and%20parameter%0Aidentification%20using%20PINNs%2C%20whose%20solutions%20can%20be%20accurately%20found%20by%20locating%0Asaddle%20points.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03292v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBiomechanics-informed%2520Non-rigid%2520Medical%2520Image%2520Registration%2520and%2520its%250A%2520%2520Inverse%2520Material%2520Property%2520Estimation%2520with%2520Linear%2520and%2520Nonlinear%2520Elasticity%26entry.906535625%3DZhe%2520Min%2520and%2520Zachary%2520M.%2520C.%2520Baum%2520and%2520Shaheer%2520U.%2520Saeed%2520and%2520Mark%2520Emberton%2520and%2520Dean%2520C.%2520Barratt%2520and%2520Zeike%2520A.%2520Taylor%2520and%2520Yipeng%2520Hu%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520both%2520biomechanical-constrained%2520non-rigid%2520medical%250Aimage%2520registrations%2520and%2520accurate%2520identifications%2520of%2520material%2520properties%2520for%250Asoft%2520tissues%252C%2520using%2520physics-informed%2520neural%2520networks%2520%2528PINNs%2529.%2520The%2520complex%250Anonlinear%2520elasticity%2520theory%2520is%2520leveraged%2520to%2520formally%2520establish%2520the%2520partial%250Adifferential%2520equations%2520%2528PDEs%2529%2520representing%2520physics%2520laws%2520of%2520biomechanical%250Aconstraints%2520that%2520need%2520to%2520be%2520satisfied%252C%2520with%2520which%2520registration%2520and%250Aidentification%2520tasks%2520are%2520treated%2520as%2520forward%2520%2528i.e.%252C%2520data-driven%2520solutions%2520of%250APDEs%2529%2520and%2520inverse%2520%2528i.e.%252C%2520parameter%2520estimation%2529%2520problems%2520under%2520PINNs%250Arespectively.%2520Two%2520net%2520configurations%2520%2528i.e.%252C%2520Cfg1%2520and%2520Cfg2%2529%2520have%2520also%2520been%250Acompared%2520for%2520both%2520linear%2520and%2520nonlinear%2520physics%2520model.%2520Two%2520sets%2520of%2520experiments%250Ahave%2520been%2520conducted%252C%2520using%2520pairs%2520of%2520undeformed%2520and%2520deformed%2520MR%2520images%2520from%250Aclinical%2520cases%2520of%2520prostate%2520cancer%2520biopsy.%250A%2520%2520Our%2520contributions%2520are%2520summarised%2520as%2520follows.%25201%2529%2520We%2520developed%2520a%2520learning-based%250Abiomechanical-constrained%2520non-rigid%2520registration%2520algorithm%2520using%2520PINNs%252C%2520where%250Alinear%2520elasticity%2520is%2520generalised%2520to%2520the%2520nonlinear%2520version.%25202%2529%2520We%2520demonstrated%250Aextensively%2520that%2520nonlinear%2520elasticity%2520shows%2520no%2520statistical%2520significance%2520against%250Alinear%2520models%2520in%2520computing%2520point-wise%2520displacement%2520vectors%2520but%2520their%2520respective%250Abenefits%2520may%2520depend%2520on%2520specific%2520patients%252C%2520with%2520finite-element%2520%2528FE%2529%2520computed%250Aground-truth.%25203%2529%2520We%2520formulated%2520and%2520solved%2520the%2520inverse%2520parameter%2520estimation%250Aproblem%252C%2520under%2520the%2520joint%2520optimisation%2520scheme%2520of%2520registration%2520and%2520parameter%250Aidentification%2520using%2520PINNs%252C%2520whose%2520solutions%2520can%2520be%2520accurately%2520found%2520by%2520locating%250Asaddle%2520points.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03292v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Biomechanics-informed%20Non-rigid%20Medical%20Image%20Registration%20and%20its%0A%20%20Inverse%20Material%20Property%20Estimation%20with%20Linear%20and%20Nonlinear%20Elasticity&entry.906535625=Zhe%20Min%20and%20Zachary%20M.%20C.%20Baum%20and%20Shaheer%20U.%20Saeed%20and%20Mark%20Emberton%20and%20Dean%20C.%20Barratt%20and%20Zeike%20A.%20Taylor%20and%20Yipeng%20Hu&entry.1292438233=%20%20This%20paper%20investigates%20both%20biomechanical-constrained%20non-rigid%20medical%0Aimage%20registrations%20and%20accurate%20identifications%20of%20material%20properties%20for%0Asoft%20tissues%2C%20using%20physics-informed%20neural%20networks%20%28PINNs%29.%20The%20complex%0Anonlinear%20elasticity%20theory%20is%20leveraged%20to%20formally%20establish%20the%20partial%0Adifferential%20equations%20%28PDEs%29%20representing%20physics%20laws%20of%20biomechanical%0Aconstraints%20that%20need%20to%20be%20satisfied%2C%20with%20which%20registration%20and%0Aidentification%20tasks%20are%20treated%20as%20forward%20%28i.e.%2C%20data-driven%20solutions%20of%0APDEs%29%20and%20inverse%20%28i.e.%2C%20parameter%20estimation%29%20problems%20under%20PINNs%0Arespectively.%20Two%20net%20configurations%20%28i.e.%2C%20Cfg1%20and%20Cfg2%29%20have%20also%20been%0Acompared%20for%20both%20linear%20and%20nonlinear%20physics%20model.%20Two%20sets%20of%20experiments%0Ahave%20been%20conducted%2C%20using%20pairs%20of%20undeformed%20and%20deformed%20MR%20images%20from%0Aclinical%20cases%20of%20prostate%20cancer%20biopsy.%0A%20%20Our%20contributions%20are%20summarised%20as%20follows.%201%29%20We%20developed%20a%20learning-based%0Abiomechanical-constrained%20non-rigid%20registration%20algorithm%20using%20PINNs%2C%20where%0Alinear%20elasticity%20is%20generalised%20to%20the%20nonlinear%20version.%202%29%20We%20demonstrated%0Aextensively%20that%20nonlinear%20elasticity%20shows%20no%20statistical%20significance%20against%0Alinear%20models%20in%20computing%20point-wise%20displacement%20vectors%20but%20their%20respective%0Abenefits%20may%20depend%20on%20specific%20patients%2C%20with%20finite-element%20%28FE%29%20computed%0Aground-truth.%203%29%20We%20formulated%20and%20solved%20the%20inverse%20parameter%20estimation%0Aproblem%2C%20under%20the%20joint%20optimisation%20scheme%20of%20registration%20and%20parameter%0Aidentification%20using%20PINNs%2C%20whose%20solutions%20can%20be%20accurately%20found%20by%20locating%0Asaddle%20points.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03292v1&entry.124074799=Read"},
{"title": "Effective Heterogeneous Federated Learning via Efficient\n  Hypernetwork-based Weight Generation", "author": "Yujin Shin and Kichang Lee and Sungmin Lee and You Rim Choi and Hyung-Sin Kim and JeongGil Ko", "abstract": "  While federated learning leverages distributed client resources, it faces\nchallenges due to heterogeneous client capabilities. This necessitates\nallocating models suited to clients' resources and careful parameter\naggregation to accommodate this heterogeneity. We propose HypeMeFed, a novel\nfederated learning framework for supporting client heterogeneity by combining a\nmulti-exit network architecture with hypernetwork-based model weight\ngeneration. This approach aligns the feature spaces of heterogeneous model\nlayers and resolves per-layer information disparity during weight aggregation.\nTo practically realize HypeMeFed, we also propose a low-rank factorization\napproach to minimize computation and memory overhead associated with\nhypernetworks. Our evaluations on a real-world heterogeneous device testbed\nindicate that HypeMeFed enhances accuracy by 5.12% over FedAvg, reduces the\nhypernetwork memory requirements by 98.22%, and accelerates its operations by\n1.86 times compared to a naive hypernetwork approach. These results demonstrate\nHypeMeFed's effectiveness in leveraging and engaging heterogeneous clients for\nfederated learning.\n", "link": "http://arxiv.org/abs/2407.03086v1", "date": "2024-07-03", "relevancy": 1.9548, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.519}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4714}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4561}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Effective%20Heterogeneous%20Federated%20Learning%20via%20Efficient%0A%20%20Hypernetwork-based%20Weight%20Generation&body=Title%3A%20Effective%20Heterogeneous%20Federated%20Learning%20via%20Efficient%0A%20%20Hypernetwork-based%20Weight%20Generation%0AAuthor%3A%20Yujin%20Shin%20and%20Kichang%20Lee%20and%20Sungmin%20Lee%20and%20You%20Rim%20Choi%20and%20Hyung-Sin%20Kim%20and%20JeongGil%20Ko%0AAbstract%3A%20%20%20While%20federated%20learning%20leverages%20distributed%20client%20resources%2C%20it%20faces%0Achallenges%20due%20to%20heterogeneous%20client%20capabilities.%20This%20necessitates%0Aallocating%20models%20suited%20to%20clients%27%20resources%20and%20careful%20parameter%0Aaggregation%20to%20accommodate%20this%20heterogeneity.%20We%20propose%20HypeMeFed%2C%20a%20novel%0Afederated%20learning%20framework%20for%20supporting%20client%20heterogeneity%20by%20combining%20a%0Amulti-exit%20network%20architecture%20with%20hypernetwork-based%20model%20weight%0Ageneration.%20This%20approach%20aligns%20the%20feature%20spaces%20of%20heterogeneous%20model%0Alayers%20and%20resolves%20per-layer%20information%20disparity%20during%20weight%20aggregation.%0ATo%20practically%20realize%20HypeMeFed%2C%20we%20also%20propose%20a%20low-rank%20factorization%0Aapproach%20to%20minimize%20computation%20and%20memory%20overhead%20associated%20with%0Ahypernetworks.%20Our%20evaluations%20on%20a%20real-world%20heterogeneous%20device%20testbed%0Aindicate%20that%20HypeMeFed%20enhances%20accuracy%20by%205.12%25%20over%20FedAvg%2C%20reduces%20the%0Ahypernetwork%20memory%20requirements%20by%2098.22%25%2C%20and%20accelerates%20its%20operations%20by%0A1.86%20times%20compared%20to%20a%20naive%20hypernetwork%20approach.%20These%20results%20demonstrate%0AHypeMeFed%27s%20effectiveness%20in%20leveraging%20and%20engaging%20heterogeneous%20clients%20for%0Afederated%20learning.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03086v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEffective%2520Heterogeneous%2520Federated%2520Learning%2520via%2520Efficient%250A%2520%2520Hypernetwork-based%2520Weight%2520Generation%26entry.906535625%3DYujin%2520Shin%2520and%2520Kichang%2520Lee%2520and%2520Sungmin%2520Lee%2520and%2520You%2520Rim%2520Choi%2520and%2520Hyung-Sin%2520Kim%2520and%2520JeongGil%2520Ko%26entry.1292438233%3D%2520%2520While%2520federated%2520learning%2520leverages%2520distributed%2520client%2520resources%252C%2520it%2520faces%250Achallenges%2520due%2520to%2520heterogeneous%2520client%2520capabilities.%2520This%2520necessitates%250Aallocating%2520models%2520suited%2520to%2520clients%2527%2520resources%2520and%2520careful%2520parameter%250Aaggregation%2520to%2520accommodate%2520this%2520heterogeneity.%2520We%2520propose%2520HypeMeFed%252C%2520a%2520novel%250Afederated%2520learning%2520framework%2520for%2520supporting%2520client%2520heterogeneity%2520by%2520combining%2520a%250Amulti-exit%2520network%2520architecture%2520with%2520hypernetwork-based%2520model%2520weight%250Ageneration.%2520This%2520approach%2520aligns%2520the%2520feature%2520spaces%2520of%2520heterogeneous%2520model%250Alayers%2520and%2520resolves%2520per-layer%2520information%2520disparity%2520during%2520weight%2520aggregation.%250ATo%2520practically%2520realize%2520HypeMeFed%252C%2520we%2520also%2520propose%2520a%2520low-rank%2520factorization%250Aapproach%2520to%2520minimize%2520computation%2520and%2520memory%2520overhead%2520associated%2520with%250Ahypernetworks.%2520Our%2520evaluations%2520on%2520a%2520real-world%2520heterogeneous%2520device%2520testbed%250Aindicate%2520that%2520HypeMeFed%2520enhances%2520accuracy%2520by%25205.12%2525%2520over%2520FedAvg%252C%2520reduces%2520the%250Ahypernetwork%2520memory%2520requirements%2520by%252098.22%2525%252C%2520and%2520accelerates%2520its%2520operations%2520by%250A1.86%2520times%2520compared%2520to%2520a%2520naive%2520hypernetwork%2520approach.%2520These%2520results%2520demonstrate%250AHypeMeFed%2527s%2520effectiveness%2520in%2520leveraging%2520and%2520engaging%2520heterogeneous%2520clients%2520for%250Afederated%2520learning.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03086v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Effective%20Heterogeneous%20Federated%20Learning%20via%20Efficient%0A%20%20Hypernetwork-based%20Weight%20Generation&entry.906535625=Yujin%20Shin%20and%20Kichang%20Lee%20and%20Sungmin%20Lee%20and%20You%20Rim%20Choi%20and%20Hyung-Sin%20Kim%20and%20JeongGil%20Ko&entry.1292438233=%20%20While%20federated%20learning%20leverages%20distributed%20client%20resources%2C%20it%20faces%0Achallenges%20due%20to%20heterogeneous%20client%20capabilities.%20This%20necessitates%0Aallocating%20models%20suited%20to%20clients%27%20resources%20and%20careful%20parameter%0Aaggregation%20to%20accommodate%20this%20heterogeneity.%20We%20propose%20HypeMeFed%2C%20a%20novel%0Afederated%20learning%20framework%20for%20supporting%20client%20heterogeneity%20by%20combining%20a%0Amulti-exit%20network%20architecture%20with%20hypernetwork-based%20model%20weight%0Ageneration.%20This%20approach%20aligns%20the%20feature%20spaces%20of%20heterogeneous%20model%0Alayers%20and%20resolves%20per-layer%20information%20disparity%20during%20weight%20aggregation.%0ATo%20practically%20realize%20HypeMeFed%2C%20we%20also%20propose%20a%20low-rank%20factorization%0Aapproach%20to%20minimize%20computation%20and%20memory%20overhead%20associated%20with%0Ahypernetworks.%20Our%20evaluations%20on%20a%20real-world%20heterogeneous%20device%20testbed%0Aindicate%20that%20HypeMeFed%20enhances%20accuracy%20by%205.12%25%20over%20FedAvg%2C%20reduces%20the%0Ahypernetwork%20memory%20requirements%20by%2098.22%25%2C%20and%20accelerates%20its%20operations%20by%0A1.86%20times%20compared%20to%20a%20naive%20hypernetwork%20approach.%20These%20results%20demonstrate%0AHypeMeFed%27s%20effectiveness%20in%20leveraging%20and%20engaging%20heterogeneous%20clients%20for%0Afederated%20learning.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03086v1&entry.124074799=Read"},
{"title": "On the Client Preference of LLM Fine-tuning in Federated Learning", "author": "Feijie Wu and Xiaoze Liu and Haoyu Wang and Xingchen Wang and Jing Gao", "abstract": "  Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained\nlarge language model (LLM) using preference datasets, enabling the LLM to\ngenerate outputs that align with human preferences. Given the sensitive nature\nof these preference datasets held by various clients, there is a need to\nimplement RLHF within a federated learning (FL) framework, where clients are\nreluctant to share their data due to privacy concerns. To address this, we\nintroduce a feasible framework in which clients collaboratively train a binary\nselector with their preference datasets using our proposed FedBis. With a\nwell-trained selector, we can further enhance the LLM that generates\nhuman-preferred completions. Meanwhile, we propose a novel algorithm,\nFedBiscuit, that trains multiple selectors by organizing clients into balanced\nand disjoint clusters based on their preferences. Compared to the FedBis,\nFedBiscuit demonstrates superior performance in simulating human preferences\nfor pairwise completions. Our extensive experiments on federated human\npreference datasets -- marking the first benchmark to address heterogeneous\ndata partitioning among clients -- demonstrate that FedBiscuit outperforms\nFedBis and even surpasses traditional centralized training.\n", "link": "http://arxiv.org/abs/2407.03038v1", "date": "2024-07-03", "relevancy": 1.9514, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.515}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.486}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4615}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Client%20Preference%20of%20LLM%20Fine-tuning%20in%20Federated%20Learning&body=Title%3A%20On%20the%20Client%20Preference%20of%20LLM%20Fine-tuning%20in%20Federated%20Learning%0AAuthor%3A%20Feijie%20Wu%20and%20Xiaoze%20Liu%20and%20Haoyu%20Wang%20and%20Xingchen%20Wang%20and%20Jing%20Gao%0AAbstract%3A%20%20%20Reinforcement%20learning%20with%20human%20feedback%20%28RLHF%29%20fine-tunes%20a%20pretrained%0Alarge%20language%20model%20%28LLM%29%20using%20preference%20datasets%2C%20enabling%20the%20LLM%20to%0Agenerate%20outputs%20that%20align%20with%20human%20preferences.%20Given%20the%20sensitive%20nature%0Aof%20these%20preference%20datasets%20held%20by%20various%20clients%2C%20there%20is%20a%20need%20to%0Aimplement%20RLHF%20within%20a%20federated%20learning%20%28FL%29%20framework%2C%20where%20clients%20are%0Areluctant%20to%20share%20their%20data%20due%20to%20privacy%20concerns.%20To%20address%20this%2C%20we%0Aintroduce%20a%20feasible%20framework%20in%20which%20clients%20collaboratively%20train%20a%20binary%0Aselector%20with%20their%20preference%20datasets%20using%20our%20proposed%20FedBis.%20With%20a%0Awell-trained%20selector%2C%20we%20can%20further%20enhance%20the%20LLM%20that%20generates%0Ahuman-preferred%20completions.%20Meanwhile%2C%20we%20propose%20a%20novel%20algorithm%2C%0AFedBiscuit%2C%20that%20trains%20multiple%20selectors%20by%20organizing%20clients%20into%20balanced%0Aand%20disjoint%20clusters%20based%20on%20their%20preferences.%20Compared%20to%20the%20FedBis%2C%0AFedBiscuit%20demonstrates%20superior%20performance%20in%20simulating%20human%20preferences%0Afor%20pairwise%20completions.%20Our%20extensive%20experiments%20on%20federated%20human%0Apreference%20datasets%20--%20marking%20the%20first%20benchmark%20to%20address%20heterogeneous%0Adata%20partitioning%20among%20clients%20--%20demonstrate%20that%20FedBiscuit%20outperforms%0AFedBis%20and%20even%20surpasses%20traditional%20centralized%20training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Client%2520Preference%2520of%2520LLM%2520Fine-tuning%2520in%2520Federated%2520Learning%26entry.906535625%3DFeijie%2520Wu%2520and%2520Xiaoze%2520Liu%2520and%2520Haoyu%2520Wang%2520and%2520Xingchen%2520Wang%2520and%2520Jing%2520Gao%26entry.1292438233%3D%2520%2520Reinforcement%2520learning%2520with%2520human%2520feedback%2520%2528RLHF%2529%2520fine-tunes%2520a%2520pretrained%250Alarge%2520language%2520model%2520%2528LLM%2529%2520using%2520preference%2520datasets%252C%2520enabling%2520the%2520LLM%2520to%250Agenerate%2520outputs%2520that%2520align%2520with%2520human%2520preferences.%2520Given%2520the%2520sensitive%2520nature%250Aof%2520these%2520preference%2520datasets%2520held%2520by%2520various%2520clients%252C%2520there%2520is%2520a%2520need%2520to%250Aimplement%2520RLHF%2520within%2520a%2520federated%2520learning%2520%2528FL%2529%2520framework%252C%2520where%2520clients%2520are%250Areluctant%2520to%2520share%2520their%2520data%2520due%2520to%2520privacy%2520concerns.%2520To%2520address%2520this%252C%2520we%250Aintroduce%2520a%2520feasible%2520framework%2520in%2520which%2520clients%2520collaboratively%2520train%2520a%2520binary%250Aselector%2520with%2520their%2520preference%2520datasets%2520using%2520our%2520proposed%2520FedBis.%2520With%2520a%250Awell-trained%2520selector%252C%2520we%2520can%2520further%2520enhance%2520the%2520LLM%2520that%2520generates%250Ahuman-preferred%2520completions.%2520Meanwhile%252C%2520we%2520propose%2520a%2520novel%2520algorithm%252C%250AFedBiscuit%252C%2520that%2520trains%2520multiple%2520selectors%2520by%2520organizing%2520clients%2520into%2520balanced%250Aand%2520disjoint%2520clusters%2520based%2520on%2520their%2520preferences.%2520Compared%2520to%2520the%2520FedBis%252C%250AFedBiscuit%2520demonstrates%2520superior%2520performance%2520in%2520simulating%2520human%2520preferences%250Afor%2520pairwise%2520completions.%2520Our%2520extensive%2520experiments%2520on%2520federated%2520human%250Apreference%2520datasets%2520--%2520marking%2520the%2520first%2520benchmark%2520to%2520address%2520heterogeneous%250Adata%2520partitioning%2520among%2520clients%2520--%2520demonstrate%2520that%2520FedBiscuit%2520outperforms%250AFedBis%2520and%2520even%2520surpasses%2520traditional%2520centralized%2520training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Client%20Preference%20of%20LLM%20Fine-tuning%20in%20Federated%20Learning&entry.906535625=Feijie%20Wu%20and%20Xiaoze%20Liu%20and%20Haoyu%20Wang%20and%20Xingchen%20Wang%20and%20Jing%20Gao&entry.1292438233=%20%20Reinforcement%20learning%20with%20human%20feedback%20%28RLHF%29%20fine-tunes%20a%20pretrained%0Alarge%20language%20model%20%28LLM%29%20using%20preference%20datasets%2C%20enabling%20the%20LLM%20to%0Agenerate%20outputs%20that%20align%20with%20human%20preferences.%20Given%20the%20sensitive%20nature%0Aof%20these%20preference%20datasets%20held%20by%20various%20clients%2C%20there%20is%20a%20need%20to%0Aimplement%20RLHF%20within%20a%20federated%20learning%20%28FL%29%20framework%2C%20where%20clients%20are%0Areluctant%20to%20share%20their%20data%20due%20to%20privacy%20concerns.%20To%20address%20this%2C%20we%0Aintroduce%20a%20feasible%20framework%20in%20which%20clients%20collaboratively%20train%20a%20binary%0Aselector%20with%20their%20preference%20datasets%20using%20our%20proposed%20FedBis.%20With%20a%0Awell-trained%20selector%2C%20we%20can%20further%20enhance%20the%20LLM%20that%20generates%0Ahuman-preferred%20completions.%20Meanwhile%2C%20we%20propose%20a%20novel%20algorithm%2C%0AFedBiscuit%2C%20that%20trains%20multiple%20selectors%20by%20organizing%20clients%20into%20balanced%0Aand%20disjoint%20clusters%20based%20on%20their%20preferences.%20Compared%20to%20the%20FedBis%2C%0AFedBiscuit%20demonstrates%20superior%20performance%20in%20simulating%20human%20preferences%0Afor%20pairwise%20completions.%20Our%20extensive%20experiments%20on%20federated%20human%0Apreference%20datasets%20--%20marking%20the%20first%20benchmark%20to%20address%20heterogeneous%0Adata%20partitioning%20among%20clients%20--%20demonstrate%20that%20FedBiscuit%20outperforms%0AFedBis%20and%20even%20surpasses%20traditional%20centralized%20training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03038v1&entry.124074799=Read"},
{"title": "ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary\n  LLMs on Private Datasets", "author": "Ahmed Frikha and Nassim Walha and Ricardo Mendes and Krishna Kanth Nakka and Xue Jiang and Xuebing Zhou", "abstract": "  This work addresses the timely yet underexplored problem of performing\ninference and finetuning of a proprietary LLM owned by a model provider entity\non the confidential/private data of another data owner entity, in a way that\nensures the confidentiality of both the model and the data. Hereby, the\nfinetuning is conducted offsite, i.e., on the computation infrastructure of a\nthird-party cloud provider. We tackle this problem by proposing ObfuscaTune, a\nnovel, efficient and fully utility-preserving approach that combines a simple\nyet effective obfuscation technique with an efficient usage of confidential\ncomputing (only 5% of the model parameters are placed on TEE). We empirically\ndemonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models\nwith different sizes on four NLP benchmark datasets. Finally, we compare to a\nna\\\"ive version of our approach to highlight the necessity of using random\nmatrices with low condition numbers in our approach to reduce errors induced by\nthe obfuscation.\n", "link": "http://arxiv.org/abs/2407.02960v1", "date": "2024-07-03", "relevancy": 1.9504, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5122}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4961}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4596}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ObfuscaTune%3A%20Obfuscated%20Offsite%20Fine-tuning%20and%20Inference%20of%20Proprietary%0A%20%20LLMs%20on%20Private%20Datasets&body=Title%3A%20ObfuscaTune%3A%20Obfuscated%20Offsite%20Fine-tuning%20and%20Inference%20of%20Proprietary%0A%20%20LLMs%20on%20Private%20Datasets%0AAuthor%3A%20Ahmed%20Frikha%20and%20Nassim%20Walha%20and%20Ricardo%20Mendes%20and%20Krishna%20Kanth%20Nakka%20and%20Xue%20Jiang%20and%20Xuebing%20Zhou%0AAbstract%3A%20%20%20This%20work%20addresses%20the%20timely%20yet%20underexplored%20problem%20of%20performing%0Ainference%20and%20finetuning%20of%20a%20proprietary%20LLM%20owned%20by%20a%20model%20provider%20entity%0Aon%20the%20confidential/private%20data%20of%20another%20data%20owner%20entity%2C%20in%20a%20way%20that%0Aensures%20the%20confidentiality%20of%20both%20the%20model%20and%20the%20data.%20Hereby%2C%20the%0Afinetuning%20is%20conducted%20offsite%2C%20i.e.%2C%20on%20the%20computation%20infrastructure%20of%20a%0Athird-party%20cloud%20provider.%20We%20tackle%20this%20problem%20by%20proposing%20ObfuscaTune%2C%20a%0Anovel%2C%20efficient%20and%20fully%20utility-preserving%20approach%20that%20combines%20a%20simple%0Ayet%20effective%20obfuscation%20technique%20with%20an%20efficient%20usage%20of%20confidential%0Acomputing%20%28only%205%25%20of%20the%20model%20parameters%20are%20placed%20on%20TEE%29.%20We%20empirically%0Ademonstrate%20the%20effectiveness%20of%20ObfuscaTune%20by%20validating%20it%20on%20GPT-2%20models%0Awith%20different%20sizes%20on%20four%20NLP%20benchmark%20datasets.%20Finally%2C%20we%20compare%20to%20a%0Ana%5C%22ive%20version%20of%20our%20approach%20to%20highlight%20the%20necessity%20of%20using%20random%0Amatrices%20with%20low%20condition%20numbers%20in%20our%20approach%20to%20reduce%20errors%20induced%20by%0Athe%20obfuscation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02960v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObfuscaTune%253A%2520Obfuscated%2520Offsite%2520Fine-tuning%2520and%2520Inference%2520of%2520Proprietary%250A%2520%2520LLMs%2520on%2520Private%2520Datasets%26entry.906535625%3DAhmed%2520Frikha%2520and%2520Nassim%2520Walha%2520and%2520Ricardo%2520Mendes%2520and%2520Krishna%2520Kanth%2520Nakka%2520and%2520Xue%2520Jiang%2520and%2520Xuebing%2520Zhou%26entry.1292438233%3D%2520%2520This%2520work%2520addresses%2520the%2520timely%2520yet%2520underexplored%2520problem%2520of%2520performing%250Ainference%2520and%2520finetuning%2520of%2520a%2520proprietary%2520LLM%2520owned%2520by%2520a%2520model%2520provider%2520entity%250Aon%2520the%2520confidential/private%2520data%2520of%2520another%2520data%2520owner%2520entity%252C%2520in%2520a%2520way%2520that%250Aensures%2520the%2520confidentiality%2520of%2520both%2520the%2520model%2520and%2520the%2520data.%2520Hereby%252C%2520the%250Afinetuning%2520is%2520conducted%2520offsite%252C%2520i.e.%252C%2520on%2520the%2520computation%2520infrastructure%2520of%2520a%250Athird-party%2520cloud%2520provider.%2520We%2520tackle%2520this%2520problem%2520by%2520proposing%2520ObfuscaTune%252C%2520a%250Anovel%252C%2520efficient%2520and%2520fully%2520utility-preserving%2520approach%2520that%2520combines%2520a%2520simple%250Ayet%2520effective%2520obfuscation%2520technique%2520with%2520an%2520efficient%2520usage%2520of%2520confidential%250Acomputing%2520%2528only%25205%2525%2520of%2520the%2520model%2520parameters%2520are%2520placed%2520on%2520TEE%2529.%2520We%2520empirically%250Ademonstrate%2520the%2520effectiveness%2520of%2520ObfuscaTune%2520by%2520validating%2520it%2520on%2520GPT-2%2520models%250Awith%2520different%2520sizes%2520on%2520four%2520NLP%2520benchmark%2520datasets.%2520Finally%252C%2520we%2520compare%2520to%2520a%250Ana%255C%2522ive%2520version%2520of%2520our%2520approach%2520to%2520highlight%2520the%2520necessity%2520of%2520using%2520random%250Amatrices%2520with%2520low%2520condition%2520numbers%2520in%2520our%2520approach%2520to%2520reduce%2520errors%2520induced%2520by%250Athe%2520obfuscation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02960v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ObfuscaTune%3A%20Obfuscated%20Offsite%20Fine-tuning%20and%20Inference%20of%20Proprietary%0A%20%20LLMs%20on%20Private%20Datasets&entry.906535625=Ahmed%20Frikha%20and%20Nassim%20Walha%20and%20Ricardo%20Mendes%20and%20Krishna%20Kanth%20Nakka%20and%20Xue%20Jiang%20and%20Xuebing%20Zhou&entry.1292438233=%20%20This%20work%20addresses%20the%20timely%20yet%20underexplored%20problem%20of%20performing%0Ainference%20and%20finetuning%20of%20a%20proprietary%20LLM%20owned%20by%20a%20model%20provider%20entity%0Aon%20the%20confidential/private%20data%20of%20another%20data%20owner%20entity%2C%20in%20a%20way%20that%0Aensures%20the%20confidentiality%20of%20both%20the%20model%20and%20the%20data.%20Hereby%2C%20the%0Afinetuning%20is%20conducted%20offsite%2C%20i.e.%2C%20on%20the%20computation%20infrastructure%20of%20a%0Athird-party%20cloud%20provider.%20We%20tackle%20this%20problem%20by%20proposing%20ObfuscaTune%2C%20a%0Anovel%2C%20efficient%20and%20fully%20utility-preserving%20approach%20that%20combines%20a%20simple%0Ayet%20effective%20obfuscation%20technique%20with%20an%20efficient%20usage%20of%20confidential%0Acomputing%20%28only%205%25%20of%20the%20model%20parameters%20are%20placed%20on%20TEE%29.%20We%20empirically%0Ademonstrate%20the%20effectiveness%20of%20ObfuscaTune%20by%20validating%20it%20on%20GPT-2%20models%0Awith%20different%20sizes%20on%20four%20NLP%20benchmark%20datasets.%20Finally%2C%20we%20compare%20to%20a%0Ana%5C%22ive%20version%20of%20our%20approach%20to%20highlight%20the%20necessity%20of%20using%20random%0Amatrices%20with%20low%20condition%20numbers%20in%20our%20approach%20to%20reduce%20errors%20induced%20by%0Athe%20obfuscation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02960v1&entry.124074799=Read"},
{"title": "Spectral Estimators for Structured Generalized Linear Models via\n  Approximate Message Passing", "author": "Yihan Zhang and Hong Chang Ji and Ramji Venkataramanan and Marco Mondelli", "abstract": "  We consider the problem of parameter estimation in a high-dimensional\ngeneralized linear model. Spectral methods obtained via the principal\neigenvector of a suitable data-dependent matrix provide a simple yet\nsurprisingly effective solution. However, despite their wide use, a rigorous\nperformance characterization, as well as a principled way to preprocess the\ndata, are available only for unstructured (i.i.d.\\ Gaussian and Haar\northogonal) designs. In contrast, real-world data matrices are highly\nstructured and exhibit non-trivial correlations. To address the problem, we\nconsider correlated Gaussian designs capturing the anisotropic nature of the\nfeatures via a covariance matrix $\\Sigma$. Our main result is a precise\nasymptotic characterization of the performance of spectral estimators. This\nallows us to identify the optimal preprocessing that minimizes the number of\nsamples needed for parameter estimation. Surprisingly, such preprocessing is\nuniversal across a broad set of designs, which partly addresses a conjecture on\noptimal spectral estimators for rotationally invariant models. Our principled\napproach vastly improves upon previous heuristic methods, including for designs\ncommon in computational imaging and genetics. The proposed methodology, based\non approximate message passing, is broadly applicable and opens the way to the\nprecise characterization of spiked matrices and of the corresponding spectral\nmethods in a variety of settings.\n", "link": "http://arxiv.org/abs/2308.14507v3", "date": "2024-07-03", "relevancy": 1.9488, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4947}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4914}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4581}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spectral%20Estimators%20for%20Structured%20Generalized%20Linear%20Models%20via%0A%20%20Approximate%20Message%20Passing&body=Title%3A%20Spectral%20Estimators%20for%20Structured%20Generalized%20Linear%20Models%20via%0A%20%20Approximate%20Message%20Passing%0AAuthor%3A%20Yihan%20Zhang%20and%20Hong%20Chang%20Ji%20and%20Ramji%20Venkataramanan%20and%20Marco%20Mondelli%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20parameter%20estimation%20in%20a%20high-dimensional%0Ageneralized%20linear%20model.%20Spectral%20methods%20obtained%20via%20the%20principal%0Aeigenvector%20of%20a%20suitable%20data-dependent%20matrix%20provide%20a%20simple%20yet%0Asurprisingly%20effective%20solution.%20However%2C%20despite%20their%20wide%20use%2C%20a%20rigorous%0Aperformance%20characterization%2C%20as%20well%20as%20a%20principled%20way%20to%20preprocess%20the%0Adata%2C%20are%20available%20only%20for%20unstructured%20%28i.i.d.%5C%20Gaussian%20and%20Haar%0Aorthogonal%29%20designs.%20In%20contrast%2C%20real-world%20data%20matrices%20are%20highly%0Astructured%20and%20exhibit%20non-trivial%20correlations.%20To%20address%20the%20problem%2C%20we%0Aconsider%20correlated%20Gaussian%20designs%20capturing%20the%20anisotropic%20nature%20of%20the%0Afeatures%20via%20a%20covariance%20matrix%20%24%5CSigma%24.%20Our%20main%20result%20is%20a%20precise%0Aasymptotic%20characterization%20of%20the%20performance%20of%20spectral%20estimators.%20This%0Aallows%20us%20to%20identify%20the%20optimal%20preprocessing%20that%20minimizes%20the%20number%20of%0Asamples%20needed%20for%20parameter%20estimation.%20Surprisingly%2C%20such%20preprocessing%20is%0Auniversal%20across%20a%20broad%20set%20of%20designs%2C%20which%20partly%20addresses%20a%20conjecture%20on%0Aoptimal%20spectral%20estimators%20for%20rotationally%20invariant%20models.%20Our%20principled%0Aapproach%20vastly%20improves%20upon%20previous%20heuristic%20methods%2C%20including%20for%20designs%0Acommon%20in%20computational%20imaging%20and%20genetics.%20The%20proposed%20methodology%2C%20based%0Aon%20approximate%20message%20passing%2C%20is%20broadly%20applicable%20and%20opens%20the%20way%20to%20the%0Aprecise%20characterization%20of%20spiked%20matrices%20and%20of%20the%20corresponding%20spectral%0Amethods%20in%20a%20variety%20of%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.14507v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpectral%2520Estimators%2520for%2520Structured%2520Generalized%2520Linear%2520Models%2520via%250A%2520%2520Approximate%2520Message%2520Passing%26entry.906535625%3DYihan%2520Zhang%2520and%2520Hong%2520Chang%2520Ji%2520and%2520Ramji%2520Venkataramanan%2520and%2520Marco%2520Mondelli%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520parameter%2520estimation%2520in%2520a%2520high-dimensional%250Ageneralized%2520linear%2520model.%2520Spectral%2520methods%2520obtained%2520via%2520the%2520principal%250Aeigenvector%2520of%2520a%2520suitable%2520data-dependent%2520matrix%2520provide%2520a%2520simple%2520yet%250Asurprisingly%2520effective%2520solution.%2520However%252C%2520despite%2520their%2520wide%2520use%252C%2520a%2520rigorous%250Aperformance%2520characterization%252C%2520as%2520well%2520as%2520a%2520principled%2520way%2520to%2520preprocess%2520the%250Adata%252C%2520are%2520available%2520only%2520for%2520unstructured%2520%2528i.i.d.%255C%2520Gaussian%2520and%2520Haar%250Aorthogonal%2529%2520designs.%2520In%2520contrast%252C%2520real-world%2520data%2520matrices%2520are%2520highly%250Astructured%2520and%2520exhibit%2520non-trivial%2520correlations.%2520To%2520address%2520the%2520problem%252C%2520we%250Aconsider%2520correlated%2520Gaussian%2520designs%2520capturing%2520the%2520anisotropic%2520nature%2520of%2520the%250Afeatures%2520via%2520a%2520covariance%2520matrix%2520%2524%255CSigma%2524.%2520Our%2520main%2520result%2520is%2520a%2520precise%250Aasymptotic%2520characterization%2520of%2520the%2520performance%2520of%2520spectral%2520estimators.%2520This%250Aallows%2520us%2520to%2520identify%2520the%2520optimal%2520preprocessing%2520that%2520minimizes%2520the%2520number%2520of%250Asamples%2520needed%2520for%2520parameter%2520estimation.%2520Surprisingly%252C%2520such%2520preprocessing%2520is%250Auniversal%2520across%2520a%2520broad%2520set%2520of%2520designs%252C%2520which%2520partly%2520addresses%2520a%2520conjecture%2520on%250Aoptimal%2520spectral%2520estimators%2520for%2520rotationally%2520invariant%2520models.%2520Our%2520principled%250Aapproach%2520vastly%2520improves%2520upon%2520previous%2520heuristic%2520methods%252C%2520including%2520for%2520designs%250Acommon%2520in%2520computational%2520imaging%2520and%2520genetics.%2520The%2520proposed%2520methodology%252C%2520based%250Aon%2520approximate%2520message%2520passing%252C%2520is%2520broadly%2520applicable%2520and%2520opens%2520the%2520way%2520to%2520the%250Aprecise%2520characterization%2520of%2520spiked%2520matrices%2520and%2520of%2520the%2520corresponding%2520spectral%250Amethods%2520in%2520a%2520variety%2520of%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.14507v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spectral%20Estimators%20for%20Structured%20Generalized%20Linear%20Models%20via%0A%20%20Approximate%20Message%20Passing&entry.906535625=Yihan%20Zhang%20and%20Hong%20Chang%20Ji%20and%20Ramji%20Venkataramanan%20and%20Marco%20Mondelli&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20parameter%20estimation%20in%20a%20high-dimensional%0Ageneralized%20linear%20model.%20Spectral%20methods%20obtained%20via%20the%20principal%0Aeigenvector%20of%20a%20suitable%20data-dependent%20matrix%20provide%20a%20simple%20yet%0Asurprisingly%20effective%20solution.%20However%2C%20despite%20their%20wide%20use%2C%20a%20rigorous%0Aperformance%20characterization%2C%20as%20well%20as%20a%20principled%20way%20to%20preprocess%20the%0Adata%2C%20are%20available%20only%20for%20unstructured%20%28i.i.d.%5C%20Gaussian%20and%20Haar%0Aorthogonal%29%20designs.%20In%20contrast%2C%20real-world%20data%20matrices%20are%20highly%0Astructured%20and%20exhibit%20non-trivial%20correlations.%20To%20address%20the%20problem%2C%20we%0Aconsider%20correlated%20Gaussian%20designs%20capturing%20the%20anisotropic%20nature%20of%20the%0Afeatures%20via%20a%20covariance%20matrix%20%24%5CSigma%24.%20Our%20main%20result%20is%20a%20precise%0Aasymptotic%20characterization%20of%20the%20performance%20of%20spectral%20estimators.%20This%0Aallows%20us%20to%20identify%20the%20optimal%20preprocessing%20that%20minimizes%20the%20number%20of%0Asamples%20needed%20for%20parameter%20estimation.%20Surprisingly%2C%20such%20preprocessing%20is%0Auniversal%20across%20a%20broad%20set%20of%20designs%2C%20which%20partly%20addresses%20a%20conjecture%20on%0Aoptimal%20spectral%20estimators%20for%20rotationally%20invariant%20models.%20Our%20principled%0Aapproach%20vastly%20improves%20upon%20previous%20heuristic%20methods%2C%20including%20for%20designs%0Acommon%20in%20computational%20imaging%20and%20genetics.%20The%20proposed%20methodology%2C%20based%0Aon%20approximate%20message%20passing%2C%20is%20broadly%20applicable%20and%20opens%20the%20way%20to%20the%0Aprecise%20characterization%20of%20spiked%20matrices%20and%20of%20the%20corresponding%20spectral%0Amethods%20in%20a%20variety%20of%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.14507v3&entry.124074799=Read"},
{"title": "Frequency-Controlled Diffusion Model for Versatile Text-Guided\n  Image-to-Image Translation", "author": "Xiang Gao and Zhengbo Xu and Junhan Zhao and Jiaying Liu", "abstract": "  Recently, large-scale text-to-image (T2I) diffusion models have emerged as a\npowerful tool for image-to-image translation (I2I), allowing open-domain image\ntranslation via user-provided text prompts. This paper proposes\nfrequency-controlled diffusion model (FCDiffusion), an end-to-end\ndiffusion-based framework that contributes a novel solution to text-guided I2I\nfrom a frequency-domain perspective. At the heart of our framework is a\nfeature-space frequency-domain filtering module based on Discrete Cosine\nTransform, which filters the latent features of the source image in the DCT\ndomain, yielding filtered image features bearing different DCT spectral bands\nas different control signals to the pre-trained Latent Diffusion Model. We\nreveal that control signals of different DCT spectral bands bridge the source\nimage and the T2I generated image in different correlations (e.g., style,\nstructure, layout, contour, etc.), and thus enable versatile I2I applications\nemphasizing different I2I correlations, including style-guided content\ncreation, image semantic manipulation, image scene translation, and image style\ntranslation. Different from related approaches, FCDiffusion establishes a\nunified text-guided I2I framework suitable for diverse image translation tasks\nsimply by switching among different frequency control branches at inference\ntime. The effectiveness and superiority of our method for text-guided I2I are\ndemonstrated with extensive experiments both qualitatively and quantitatively.\nThe code is publicly available at: https://github.com/XiangGao1102/FCDiffusion.\n", "link": "http://arxiv.org/abs/2407.03006v1", "date": "2024-07-03", "relevancy": 1.9455, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.652}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6506}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6397}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Frequency-Controlled%20Diffusion%20Model%20for%20Versatile%20Text-Guided%0A%20%20Image-to-Image%20Translation&body=Title%3A%20Frequency-Controlled%20Diffusion%20Model%20for%20Versatile%20Text-Guided%0A%20%20Image-to-Image%20Translation%0AAuthor%3A%20Xiang%20Gao%20and%20Zhengbo%20Xu%20and%20Junhan%20Zhao%20and%20Jiaying%20Liu%0AAbstract%3A%20%20%20Recently%2C%20large-scale%20text-to-image%20%28T2I%29%20diffusion%20models%20have%20emerged%20as%20a%0Apowerful%20tool%20for%20image-to-image%20translation%20%28I2I%29%2C%20allowing%20open-domain%20image%0Atranslation%20via%20user-provided%20text%20prompts.%20This%20paper%20proposes%0Afrequency-controlled%20diffusion%20model%20%28FCDiffusion%29%2C%20an%20end-to-end%0Adiffusion-based%20framework%20that%20contributes%20a%20novel%20solution%20to%20text-guided%20I2I%0Afrom%20a%20frequency-domain%20perspective.%20At%20the%20heart%20of%20our%20framework%20is%20a%0Afeature-space%20frequency-domain%20filtering%20module%20based%20on%20Discrete%20Cosine%0ATransform%2C%20which%20filters%20the%20latent%20features%20of%20the%20source%20image%20in%20the%20DCT%0Adomain%2C%20yielding%20filtered%20image%20features%20bearing%20different%20DCT%20spectral%20bands%0Aas%20different%20control%20signals%20to%20the%20pre-trained%20Latent%20Diffusion%20Model.%20We%0Areveal%20that%20control%20signals%20of%20different%20DCT%20spectral%20bands%20bridge%20the%20source%0Aimage%20and%20the%20T2I%20generated%20image%20in%20different%20correlations%20%28e.g.%2C%20style%2C%0Astructure%2C%20layout%2C%20contour%2C%20etc.%29%2C%20and%20thus%20enable%20versatile%20I2I%20applications%0Aemphasizing%20different%20I2I%20correlations%2C%20including%20style-guided%20content%0Acreation%2C%20image%20semantic%20manipulation%2C%20image%20scene%20translation%2C%20and%20image%20style%0Atranslation.%20Different%20from%20related%20approaches%2C%20FCDiffusion%20establishes%20a%0Aunified%20text-guided%20I2I%20framework%20suitable%20for%20diverse%20image%20translation%20tasks%0Asimply%20by%20switching%20among%20different%20frequency%20control%20branches%20at%20inference%0Atime.%20The%20effectiveness%20and%20superiority%20of%20our%20method%20for%20text-guided%20I2I%20are%0Ademonstrated%20with%20extensive%20experiments%20both%20qualitatively%20and%20quantitatively.%0AThe%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/XiangGao1102/FCDiffusion.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03006v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrequency-Controlled%2520Diffusion%2520Model%2520for%2520Versatile%2520Text-Guided%250A%2520%2520Image-to-Image%2520Translation%26entry.906535625%3DXiang%2520Gao%2520and%2520Zhengbo%2520Xu%2520and%2520Junhan%2520Zhao%2520and%2520Jiaying%2520Liu%26entry.1292438233%3D%2520%2520Recently%252C%2520large-scale%2520text-to-image%2520%2528T2I%2529%2520diffusion%2520models%2520have%2520emerged%2520as%2520a%250Apowerful%2520tool%2520for%2520image-to-image%2520translation%2520%2528I2I%2529%252C%2520allowing%2520open-domain%2520image%250Atranslation%2520via%2520user-provided%2520text%2520prompts.%2520This%2520paper%2520proposes%250Afrequency-controlled%2520diffusion%2520model%2520%2528FCDiffusion%2529%252C%2520an%2520end-to-end%250Adiffusion-based%2520framework%2520that%2520contributes%2520a%2520novel%2520solution%2520to%2520text-guided%2520I2I%250Afrom%2520a%2520frequency-domain%2520perspective.%2520At%2520the%2520heart%2520of%2520our%2520framework%2520is%2520a%250Afeature-space%2520frequency-domain%2520filtering%2520module%2520based%2520on%2520Discrete%2520Cosine%250ATransform%252C%2520which%2520filters%2520the%2520latent%2520features%2520of%2520the%2520source%2520image%2520in%2520the%2520DCT%250Adomain%252C%2520yielding%2520filtered%2520image%2520features%2520bearing%2520different%2520DCT%2520spectral%2520bands%250Aas%2520different%2520control%2520signals%2520to%2520the%2520pre-trained%2520Latent%2520Diffusion%2520Model.%2520We%250Areveal%2520that%2520control%2520signals%2520of%2520different%2520DCT%2520spectral%2520bands%2520bridge%2520the%2520source%250Aimage%2520and%2520the%2520T2I%2520generated%2520image%2520in%2520different%2520correlations%2520%2528e.g.%252C%2520style%252C%250Astructure%252C%2520layout%252C%2520contour%252C%2520etc.%2529%252C%2520and%2520thus%2520enable%2520versatile%2520I2I%2520applications%250Aemphasizing%2520different%2520I2I%2520correlations%252C%2520including%2520style-guided%2520content%250Acreation%252C%2520image%2520semantic%2520manipulation%252C%2520image%2520scene%2520translation%252C%2520and%2520image%2520style%250Atranslation.%2520Different%2520from%2520related%2520approaches%252C%2520FCDiffusion%2520establishes%2520a%250Aunified%2520text-guided%2520I2I%2520framework%2520suitable%2520for%2520diverse%2520image%2520translation%2520tasks%250Asimply%2520by%2520switching%2520among%2520different%2520frequency%2520control%2520branches%2520at%2520inference%250Atime.%2520The%2520effectiveness%2520and%2520superiority%2520of%2520our%2520method%2520for%2520text-guided%2520I2I%2520are%250Ademonstrated%2520with%2520extensive%2520experiments%2520both%2520qualitatively%2520and%2520quantitatively.%250AThe%2520code%2520is%2520publicly%2520available%2520at%253A%2520https%253A//github.com/XiangGao1102/FCDiffusion.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03006v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frequency-Controlled%20Diffusion%20Model%20for%20Versatile%20Text-Guided%0A%20%20Image-to-Image%20Translation&entry.906535625=Xiang%20Gao%20and%20Zhengbo%20Xu%20and%20Junhan%20Zhao%20and%20Jiaying%20Liu&entry.1292438233=%20%20Recently%2C%20large-scale%20text-to-image%20%28T2I%29%20diffusion%20models%20have%20emerged%20as%20a%0Apowerful%20tool%20for%20image-to-image%20translation%20%28I2I%29%2C%20allowing%20open-domain%20image%0Atranslation%20via%20user-provided%20text%20prompts.%20This%20paper%20proposes%0Afrequency-controlled%20diffusion%20model%20%28FCDiffusion%29%2C%20an%20end-to-end%0Adiffusion-based%20framework%20that%20contributes%20a%20novel%20solution%20to%20text-guided%20I2I%0Afrom%20a%20frequency-domain%20perspective.%20At%20the%20heart%20of%20our%20framework%20is%20a%0Afeature-space%20frequency-domain%20filtering%20module%20based%20on%20Discrete%20Cosine%0ATransform%2C%20which%20filters%20the%20latent%20features%20of%20the%20source%20image%20in%20the%20DCT%0Adomain%2C%20yielding%20filtered%20image%20features%20bearing%20different%20DCT%20spectral%20bands%0Aas%20different%20control%20signals%20to%20the%20pre-trained%20Latent%20Diffusion%20Model.%20We%0Areveal%20that%20control%20signals%20of%20different%20DCT%20spectral%20bands%20bridge%20the%20source%0Aimage%20and%20the%20T2I%20generated%20image%20in%20different%20correlations%20%28e.g.%2C%20style%2C%0Astructure%2C%20layout%2C%20contour%2C%20etc.%29%2C%20and%20thus%20enable%20versatile%20I2I%20applications%0Aemphasizing%20different%20I2I%20correlations%2C%20including%20style-guided%20content%0Acreation%2C%20image%20semantic%20manipulation%2C%20image%20scene%20translation%2C%20and%20image%20style%0Atranslation.%20Different%20from%20related%20approaches%2C%20FCDiffusion%20establishes%20a%0Aunified%20text-guided%20I2I%20framework%20suitable%20for%20diverse%20image%20translation%20tasks%0Asimply%20by%20switching%20among%20different%20frequency%20control%20branches%20at%20inference%0Atime.%20The%20effectiveness%20and%20superiority%20of%20our%20method%20for%20text-guided%20I2I%20are%0Ademonstrated%20with%20extensive%20experiments%20both%20qualitatively%20and%20quantitatively.%0AThe%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/XiangGao1102/FCDiffusion.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03006v1&entry.124074799=Read"},
{"title": "Out-of-distribution Detection in Medical Image Analysis: A survey", "author": "Zesheng Hong and Yubiao Yue and Yubin Chen and Lele Cong and Huanjie Lin and Yuanmei Luo and Mini Han Wang and Weidong Wang and Jialong Xu and Xiaoqi Yang and Hechang Chen and Zhenzhang Li and Sihong Xie", "abstract": "  Computer-aided diagnostics has benefited from the development of deep\nlearning-based computer vision techniques in these years. Traditional\nsupervised deep learning methods assume that the test sample is drawn from the\nidentical distribution as the training data. However, it is possible to\nencounter out-of-distribution samples in real-world clinical scenarios, which\nmay cause silent failure in deep learning-based medical image analysis tasks.\nRecently, research has explored various out-of-distribution (OOD) detection\nsituations and techniques to enable a trustworthy medical AI system. In this\nsurvey, we systematically review the recent advances in OOD detection in\nmedical image analysis. We first explore several factors that may cause a\ndistributional shift when using a deep-learning-based model in clinic\nscenarios, with three different types of distributional shift well defined on\ntop of these factors. Then a framework is suggested to categorize and feature\nexisting solutions, while the previous studies are reviewed based on the\nmethodology taxonomy. Our discussion also includes evaluation protocols and\nmetrics, as well as the challenge and a research direction lack of exploration.\n", "link": "http://arxiv.org/abs/2404.18279v2", "date": "2024-07-03", "relevancy": 1.9448, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5299}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4824}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4725}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Out-of-distribution%20Detection%20in%20Medical%20Image%20Analysis%3A%20A%20survey&body=Title%3A%20Out-of-distribution%20Detection%20in%20Medical%20Image%20Analysis%3A%20A%20survey%0AAuthor%3A%20Zesheng%20Hong%20and%20Yubiao%20Yue%20and%20Yubin%20Chen%20and%20Lele%20Cong%20and%20Huanjie%20Lin%20and%20Yuanmei%20Luo%20and%20Mini%20Han%20Wang%20and%20Weidong%20Wang%20and%20Jialong%20Xu%20and%20Xiaoqi%20Yang%20and%20Hechang%20Chen%20and%20Zhenzhang%20Li%20and%20Sihong%20Xie%0AAbstract%3A%20%20%20Computer-aided%20diagnostics%20has%20benefited%20from%20the%20development%20of%20deep%0Alearning-based%20computer%20vision%20techniques%20in%20these%20years.%20Traditional%0Asupervised%20deep%20learning%20methods%20assume%20that%20the%20test%20sample%20is%20drawn%20from%20the%0Aidentical%20distribution%20as%20the%20training%20data.%20However%2C%20it%20is%20possible%20to%0Aencounter%20out-of-distribution%20samples%20in%20real-world%20clinical%20scenarios%2C%20which%0Amay%20cause%20silent%20failure%20in%20deep%20learning-based%20medical%20image%20analysis%20tasks.%0ARecently%2C%20research%20has%20explored%20various%20out-of-distribution%20%28OOD%29%20detection%0Asituations%20and%20techniques%20to%20enable%20a%20trustworthy%20medical%20AI%20system.%20In%20this%0Asurvey%2C%20we%20systematically%20review%20the%20recent%20advances%20in%20OOD%20detection%20in%0Amedical%20image%20analysis.%20We%20first%20explore%20several%20factors%20that%20may%20cause%20a%0Adistributional%20shift%20when%20using%20a%20deep-learning-based%20model%20in%20clinic%0Ascenarios%2C%20with%20three%20different%20types%20of%20distributional%20shift%20well%20defined%20on%0Atop%20of%20these%20factors.%20Then%20a%20framework%20is%20suggested%20to%20categorize%20and%20feature%0Aexisting%20solutions%2C%20while%20the%20previous%20studies%20are%20reviewed%20based%20on%20the%0Amethodology%20taxonomy.%20Our%20discussion%20also%20includes%20evaluation%20protocols%20and%0Ametrics%2C%20as%20well%20as%20the%20challenge%20and%20a%20research%20direction%20lack%20of%20exploration.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.18279v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOut-of-distribution%2520Detection%2520in%2520Medical%2520Image%2520Analysis%253A%2520A%2520survey%26entry.906535625%3DZesheng%2520Hong%2520and%2520Yubiao%2520Yue%2520and%2520Yubin%2520Chen%2520and%2520Lele%2520Cong%2520and%2520Huanjie%2520Lin%2520and%2520Yuanmei%2520Luo%2520and%2520Mini%2520Han%2520Wang%2520and%2520Weidong%2520Wang%2520and%2520Jialong%2520Xu%2520and%2520Xiaoqi%2520Yang%2520and%2520Hechang%2520Chen%2520and%2520Zhenzhang%2520Li%2520and%2520Sihong%2520Xie%26entry.1292438233%3D%2520%2520Computer-aided%2520diagnostics%2520has%2520benefited%2520from%2520the%2520development%2520of%2520deep%250Alearning-based%2520computer%2520vision%2520techniques%2520in%2520these%2520years.%2520Traditional%250Asupervised%2520deep%2520learning%2520methods%2520assume%2520that%2520the%2520test%2520sample%2520is%2520drawn%2520from%2520the%250Aidentical%2520distribution%2520as%2520the%2520training%2520data.%2520However%252C%2520it%2520is%2520possible%2520to%250Aencounter%2520out-of-distribution%2520samples%2520in%2520real-world%2520clinical%2520scenarios%252C%2520which%250Amay%2520cause%2520silent%2520failure%2520in%2520deep%2520learning-based%2520medical%2520image%2520analysis%2520tasks.%250ARecently%252C%2520research%2520has%2520explored%2520various%2520out-of-distribution%2520%2528OOD%2529%2520detection%250Asituations%2520and%2520techniques%2520to%2520enable%2520a%2520trustworthy%2520medical%2520AI%2520system.%2520In%2520this%250Asurvey%252C%2520we%2520systematically%2520review%2520the%2520recent%2520advances%2520in%2520OOD%2520detection%2520in%250Amedical%2520image%2520analysis.%2520We%2520first%2520explore%2520several%2520factors%2520that%2520may%2520cause%2520a%250Adistributional%2520shift%2520when%2520using%2520a%2520deep-learning-based%2520model%2520in%2520clinic%250Ascenarios%252C%2520with%2520three%2520different%2520types%2520of%2520distributional%2520shift%2520well%2520defined%2520on%250Atop%2520of%2520these%2520factors.%2520Then%2520a%2520framework%2520is%2520suggested%2520to%2520categorize%2520and%2520feature%250Aexisting%2520solutions%252C%2520while%2520the%2520previous%2520studies%2520are%2520reviewed%2520based%2520on%2520the%250Amethodology%2520taxonomy.%2520Our%2520discussion%2520also%2520includes%2520evaluation%2520protocols%2520and%250Ametrics%252C%2520as%2520well%2520as%2520the%2520challenge%2520and%2520a%2520research%2520direction%2520lack%2520of%2520exploration.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18279v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Out-of-distribution%20Detection%20in%20Medical%20Image%20Analysis%3A%20A%20survey&entry.906535625=Zesheng%20Hong%20and%20Yubiao%20Yue%20and%20Yubin%20Chen%20and%20Lele%20Cong%20and%20Huanjie%20Lin%20and%20Yuanmei%20Luo%20and%20Mini%20Han%20Wang%20and%20Weidong%20Wang%20and%20Jialong%20Xu%20and%20Xiaoqi%20Yang%20and%20Hechang%20Chen%20and%20Zhenzhang%20Li%20and%20Sihong%20Xie&entry.1292438233=%20%20Computer-aided%20diagnostics%20has%20benefited%20from%20the%20development%20of%20deep%0Alearning-based%20computer%20vision%20techniques%20in%20these%20years.%20Traditional%0Asupervised%20deep%20learning%20methods%20assume%20that%20the%20test%20sample%20is%20drawn%20from%20the%0Aidentical%20distribution%20as%20the%20training%20data.%20However%2C%20it%20is%20possible%20to%0Aencounter%20out-of-distribution%20samples%20in%20real-world%20clinical%20scenarios%2C%20which%0Amay%20cause%20silent%20failure%20in%20deep%20learning-based%20medical%20image%20analysis%20tasks.%0ARecently%2C%20research%20has%20explored%20various%20out-of-distribution%20%28OOD%29%20detection%0Asituations%20and%20techniques%20to%20enable%20a%20trustworthy%20medical%20AI%20system.%20In%20this%0Asurvey%2C%20we%20systematically%20review%20the%20recent%20advances%20in%20OOD%20detection%20in%0Amedical%20image%20analysis.%20We%20first%20explore%20several%20factors%20that%20may%20cause%20a%0Adistributional%20shift%20when%20using%20a%20deep-learning-based%20model%20in%20clinic%0Ascenarios%2C%20with%20three%20different%20types%20of%20distributional%20shift%20well%20defined%20on%0Atop%20of%20these%20factors.%20Then%20a%20framework%20is%20suggested%20to%20categorize%20and%20feature%0Aexisting%20solutions%2C%20while%20the%20previous%20studies%20are%20reviewed%20based%20on%20the%0Amethodology%20taxonomy.%20Our%20discussion%20also%20includes%20evaluation%20protocols%20and%0Ametrics%2C%20as%20well%20as%20the%20challenge%20and%20a%20research%20direction%20lack%20of%20exploration.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.18279v2&entry.124074799=Read"},
{"title": "MuDiT & MuSiT: Alignment with Colloquial Expression in\n  Description-to-Song Generation", "author": "Zihao Wang and Haoxuan Liu and Jiaxing Yu and Tao Zhang and Yan Liu and Kejun Zhang", "abstract": "  Amid the rising intersection of generative AI and human artistic processes,\nthis study probes the critical yet less-explored terrain of alignment in\nhuman-centric automatic song composition. We propose a novel task of Colloquial\nDescription-to-Song Generation, which focuses on aligning the generated content\nwith colloquial human expressions. This task is aimed at bridging the gap\nbetween colloquial language understanding and auditory expression within an AI\nmodel, with the ultimate goal of creating songs that accurately satisfy human\nauditory expectations and structurally align with musical norms. Current\ndatasets are limited due to their narrow descriptive scope, semantic gaps and\ninaccuracies. To overcome data scarcity in this domain, we present the Caichong\nMusic Dataset (CaiMD). CaiMD is manually annotated by both professional\nmusicians and amateurs, offering diverse perspectives and a comprehensive\nunderstanding of colloquial descriptions. Unlike existing datasets pre-set with\nexpert annotations or auto-generated ones with inherent biases, CaiMD caters\nmore sufficiently to our purpose of aligning AI-generated music with widespread\nuser-desired results. Moreover, we propose an innovative single-stage framework\ncalled MuDiT/MuSiT for enabling effective human-machine alignment in song\ncreation. This framework not only achieves cross-modal comprehension between\ncolloquial language and auditory music perceptions but also ensures generated\nsongs align with user-desired results. MuDiT/MuSiT employs one DiT/SiT model\nfor end-to-end generation of musical components like melody, harmony, rhythm,\nvocals, and instrumentation. The approach ensures harmonious sonic cohesiveness\namongst all generated musical components, facilitating better resonance with\nhuman auditory expectations.\n", "link": "http://arxiv.org/abs/2407.03188v1", "date": "2024-07-03", "relevancy": 1.9372, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4964}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4818}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4732}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MuDiT%20%26%20MuSiT%3A%20Alignment%20with%20Colloquial%20Expression%20in%0A%20%20Description-to-Song%20Generation&body=Title%3A%20MuDiT%20%26%20MuSiT%3A%20Alignment%20with%20Colloquial%20Expression%20in%0A%20%20Description-to-Song%20Generation%0AAuthor%3A%20Zihao%20Wang%20and%20Haoxuan%20Liu%20and%20Jiaxing%20Yu%20and%20Tao%20Zhang%20and%20Yan%20Liu%20and%20Kejun%20Zhang%0AAbstract%3A%20%20%20Amid%20the%20rising%20intersection%20of%20generative%20AI%20and%20human%20artistic%20processes%2C%0Athis%20study%20probes%20the%20critical%20yet%20less-explored%20terrain%20of%20alignment%20in%0Ahuman-centric%20automatic%20song%20composition.%20We%20propose%20a%20novel%20task%20of%20Colloquial%0ADescription-to-Song%20Generation%2C%20which%20focuses%20on%20aligning%20the%20generated%20content%0Awith%20colloquial%20human%20expressions.%20This%20task%20is%20aimed%20at%20bridging%20the%20gap%0Abetween%20colloquial%20language%20understanding%20and%20auditory%20expression%20within%20an%20AI%0Amodel%2C%20with%20the%20ultimate%20goal%20of%20creating%20songs%20that%20accurately%20satisfy%20human%0Aauditory%20expectations%20and%20structurally%20align%20with%20musical%20norms.%20Current%0Adatasets%20are%20limited%20due%20to%20their%20narrow%20descriptive%20scope%2C%20semantic%20gaps%20and%0Ainaccuracies.%20To%20overcome%20data%20scarcity%20in%20this%20domain%2C%20we%20present%20the%20Caichong%0AMusic%20Dataset%20%28CaiMD%29.%20CaiMD%20is%20manually%20annotated%20by%20both%20professional%0Amusicians%20and%20amateurs%2C%20offering%20diverse%20perspectives%20and%20a%20comprehensive%0Aunderstanding%20of%20colloquial%20descriptions.%20Unlike%20existing%20datasets%20pre-set%20with%0Aexpert%20annotations%20or%20auto-generated%20ones%20with%20inherent%20biases%2C%20CaiMD%20caters%0Amore%20sufficiently%20to%20our%20purpose%20of%20aligning%20AI-generated%20music%20with%20widespread%0Auser-desired%20results.%20Moreover%2C%20we%20propose%20an%20innovative%20single-stage%20framework%0Acalled%20MuDiT/MuSiT%20for%20enabling%20effective%20human-machine%20alignment%20in%20song%0Acreation.%20This%20framework%20not%20only%20achieves%20cross-modal%20comprehension%20between%0Acolloquial%20language%20and%20auditory%20music%20perceptions%20but%20also%20ensures%20generated%0Asongs%20align%20with%20user-desired%20results.%20MuDiT/MuSiT%20employs%20one%20DiT/SiT%20model%0Afor%20end-to-end%20generation%20of%20musical%20components%20like%20melody%2C%20harmony%2C%20rhythm%2C%0Avocals%2C%20and%20instrumentation.%20The%20approach%20ensures%20harmonious%20sonic%20cohesiveness%0Aamongst%20all%20generated%20musical%20components%2C%20facilitating%20better%20resonance%20with%0Ahuman%20auditory%20expectations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMuDiT%2520%2526%2520MuSiT%253A%2520Alignment%2520with%2520Colloquial%2520Expression%2520in%250A%2520%2520Description-to-Song%2520Generation%26entry.906535625%3DZihao%2520Wang%2520and%2520Haoxuan%2520Liu%2520and%2520Jiaxing%2520Yu%2520and%2520Tao%2520Zhang%2520and%2520Yan%2520Liu%2520and%2520Kejun%2520Zhang%26entry.1292438233%3D%2520%2520Amid%2520the%2520rising%2520intersection%2520of%2520generative%2520AI%2520and%2520human%2520artistic%2520processes%252C%250Athis%2520study%2520probes%2520the%2520critical%2520yet%2520less-explored%2520terrain%2520of%2520alignment%2520in%250Ahuman-centric%2520automatic%2520song%2520composition.%2520We%2520propose%2520a%2520novel%2520task%2520of%2520Colloquial%250ADescription-to-Song%2520Generation%252C%2520which%2520focuses%2520on%2520aligning%2520the%2520generated%2520content%250Awith%2520colloquial%2520human%2520expressions.%2520This%2520task%2520is%2520aimed%2520at%2520bridging%2520the%2520gap%250Abetween%2520colloquial%2520language%2520understanding%2520and%2520auditory%2520expression%2520within%2520an%2520AI%250Amodel%252C%2520with%2520the%2520ultimate%2520goal%2520of%2520creating%2520songs%2520that%2520accurately%2520satisfy%2520human%250Aauditory%2520expectations%2520and%2520structurally%2520align%2520with%2520musical%2520norms.%2520Current%250Adatasets%2520are%2520limited%2520due%2520to%2520their%2520narrow%2520descriptive%2520scope%252C%2520semantic%2520gaps%2520and%250Ainaccuracies.%2520To%2520overcome%2520data%2520scarcity%2520in%2520this%2520domain%252C%2520we%2520present%2520the%2520Caichong%250AMusic%2520Dataset%2520%2528CaiMD%2529.%2520CaiMD%2520is%2520manually%2520annotated%2520by%2520both%2520professional%250Amusicians%2520and%2520amateurs%252C%2520offering%2520diverse%2520perspectives%2520and%2520a%2520comprehensive%250Aunderstanding%2520of%2520colloquial%2520descriptions.%2520Unlike%2520existing%2520datasets%2520pre-set%2520with%250Aexpert%2520annotations%2520or%2520auto-generated%2520ones%2520with%2520inherent%2520biases%252C%2520CaiMD%2520caters%250Amore%2520sufficiently%2520to%2520our%2520purpose%2520of%2520aligning%2520AI-generated%2520music%2520with%2520widespread%250Auser-desired%2520results.%2520Moreover%252C%2520we%2520propose%2520an%2520innovative%2520single-stage%2520framework%250Acalled%2520MuDiT/MuSiT%2520for%2520enabling%2520effective%2520human-machine%2520alignment%2520in%2520song%250Acreation.%2520This%2520framework%2520not%2520only%2520achieves%2520cross-modal%2520comprehension%2520between%250Acolloquial%2520language%2520and%2520auditory%2520music%2520perceptions%2520but%2520also%2520ensures%2520generated%250Asongs%2520align%2520with%2520user-desired%2520results.%2520MuDiT/MuSiT%2520employs%2520one%2520DiT/SiT%2520model%250Afor%2520end-to-end%2520generation%2520of%2520musical%2520components%2520like%2520melody%252C%2520harmony%252C%2520rhythm%252C%250Avocals%252C%2520and%2520instrumentation.%2520The%2520approach%2520ensures%2520harmonious%2520sonic%2520cohesiveness%250Aamongst%2520all%2520generated%2520musical%2520components%252C%2520facilitating%2520better%2520resonance%2520with%250Ahuman%2520auditory%2520expectations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MuDiT%20%26%20MuSiT%3A%20Alignment%20with%20Colloquial%20Expression%20in%0A%20%20Description-to-Song%20Generation&entry.906535625=Zihao%20Wang%20and%20Haoxuan%20Liu%20and%20Jiaxing%20Yu%20and%20Tao%20Zhang%20and%20Yan%20Liu%20and%20Kejun%20Zhang&entry.1292438233=%20%20Amid%20the%20rising%20intersection%20of%20generative%20AI%20and%20human%20artistic%20processes%2C%0Athis%20study%20probes%20the%20critical%20yet%20less-explored%20terrain%20of%20alignment%20in%0Ahuman-centric%20automatic%20song%20composition.%20We%20propose%20a%20novel%20task%20of%20Colloquial%0ADescription-to-Song%20Generation%2C%20which%20focuses%20on%20aligning%20the%20generated%20content%0Awith%20colloquial%20human%20expressions.%20This%20task%20is%20aimed%20at%20bridging%20the%20gap%0Abetween%20colloquial%20language%20understanding%20and%20auditory%20expression%20within%20an%20AI%0Amodel%2C%20with%20the%20ultimate%20goal%20of%20creating%20songs%20that%20accurately%20satisfy%20human%0Aauditory%20expectations%20and%20structurally%20align%20with%20musical%20norms.%20Current%0Adatasets%20are%20limited%20due%20to%20their%20narrow%20descriptive%20scope%2C%20semantic%20gaps%20and%0Ainaccuracies.%20To%20overcome%20data%20scarcity%20in%20this%20domain%2C%20we%20present%20the%20Caichong%0AMusic%20Dataset%20%28CaiMD%29.%20CaiMD%20is%20manually%20annotated%20by%20both%20professional%0Amusicians%20and%20amateurs%2C%20offering%20diverse%20perspectives%20and%20a%20comprehensive%0Aunderstanding%20of%20colloquial%20descriptions.%20Unlike%20existing%20datasets%20pre-set%20with%0Aexpert%20annotations%20or%20auto-generated%20ones%20with%20inherent%20biases%2C%20CaiMD%20caters%0Amore%20sufficiently%20to%20our%20purpose%20of%20aligning%20AI-generated%20music%20with%20widespread%0Auser-desired%20results.%20Moreover%2C%20we%20propose%20an%20innovative%20single-stage%20framework%0Acalled%20MuDiT/MuSiT%20for%20enabling%20effective%20human-machine%20alignment%20in%20song%0Acreation.%20This%20framework%20not%20only%20achieves%20cross-modal%20comprehension%20between%0Acolloquial%20language%20and%20auditory%20music%20perceptions%20but%20also%20ensures%20generated%0Asongs%20align%20with%20user-desired%20results.%20MuDiT/MuSiT%20employs%20one%20DiT/SiT%20model%0Afor%20end-to-end%20generation%20of%20musical%20components%20like%20melody%2C%20harmony%2C%20rhythm%2C%0Avocals%2C%20and%20instrumentation.%20The%20approach%20ensures%20harmonious%20sonic%20cohesiveness%0Aamongst%20all%20generated%20musical%20components%2C%20facilitating%20better%20resonance%20with%0Ahuman%20auditory%20expectations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03188v1&entry.124074799=Read"},
{"title": "Foundations and Frontiers of Graph Learning Theory", "author": "Yu Huang and Min Zhou and Menglin Yang and Zhen Wang and Muhan Zhang and Jie Wang and Hong Xie and Hao Wang and Defu Lian and Enhong Chen", "abstract": "  Recent advancements in graph learning have revolutionized the way to\nunderstand and analyze data with complex structures. Notably, Graph Neural\nNetworks (GNNs), i.e. neural network architectures designed for learning graph\nrepresentations, have become a popular paradigm. With these models being\nusually characterized by intuition-driven design or highly intricate\ncomponents, placing them within the theoretical analysis framework to distill\nthe core concepts, helps understand the key principles that drive the\nfunctionality better and guide further development. Given this surge in\ninterest, this article provides a comprehensive summary of the theoretical\nfoundations and breakthroughs concerning the approximation and learning\nbehaviors intrinsic to prevalent graph learning models. Encompassing\ndiscussions on fundamental aspects such as expressiveness power,\ngeneralization, optimization, and unique phenomena such as over-smoothing and\nover-squashing, this piece delves into the theoretical foundations and frontier\ndriving the evolution of graph learning. In addition, this article also\npresents several challenges and further initiates discussions on possible\nsolutions.\n", "link": "http://arxiv.org/abs/2407.03125v1", "date": "2024-07-03", "relevancy": 1.9337, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5319}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4597}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Foundations%20and%20Frontiers%20of%20Graph%20Learning%20Theory&body=Title%3A%20Foundations%20and%20Frontiers%20of%20Graph%20Learning%20Theory%0AAuthor%3A%20Yu%20Huang%20and%20Min%20Zhou%20and%20Menglin%20Yang%20and%20Zhen%20Wang%20and%20Muhan%20Zhang%20and%20Jie%20Wang%20and%20Hong%20Xie%20and%20Hao%20Wang%20and%20Defu%20Lian%20and%20Enhong%20Chen%0AAbstract%3A%20%20%20Recent%20advancements%20in%20graph%20learning%20have%20revolutionized%20the%20way%20to%0Aunderstand%20and%20analyze%20data%20with%20complex%20structures.%20Notably%2C%20Graph%20Neural%0ANetworks%20%28GNNs%29%2C%20i.e.%20neural%20network%20architectures%20designed%20for%20learning%20graph%0Arepresentations%2C%20have%20become%20a%20popular%20paradigm.%20With%20these%20models%20being%0Ausually%20characterized%20by%20intuition-driven%20design%20or%20highly%20intricate%0Acomponents%2C%20placing%20them%20within%20the%20theoretical%20analysis%20framework%20to%20distill%0Athe%20core%20concepts%2C%20helps%20understand%20the%20key%20principles%20that%20drive%20the%0Afunctionality%20better%20and%20guide%20further%20development.%20Given%20this%20surge%20in%0Ainterest%2C%20this%20article%20provides%20a%20comprehensive%20summary%20of%20the%20theoretical%0Afoundations%20and%20breakthroughs%20concerning%20the%20approximation%20and%20learning%0Abehaviors%20intrinsic%20to%20prevalent%20graph%20learning%20models.%20Encompassing%0Adiscussions%20on%20fundamental%20aspects%20such%20as%20expressiveness%20power%2C%0Ageneralization%2C%20optimization%2C%20and%20unique%20phenomena%20such%20as%20over-smoothing%20and%0Aover-squashing%2C%20this%20piece%20delves%20into%20the%20theoretical%20foundations%20and%20frontier%0Adriving%20the%20evolution%20of%20graph%20learning.%20In%20addition%2C%20this%20article%20also%0Apresents%20several%20challenges%20and%20further%20initiates%20discussions%20on%20possible%0Asolutions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03125v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFoundations%2520and%2520Frontiers%2520of%2520Graph%2520Learning%2520Theory%26entry.906535625%3DYu%2520Huang%2520and%2520Min%2520Zhou%2520and%2520Menglin%2520Yang%2520and%2520Zhen%2520Wang%2520and%2520Muhan%2520Zhang%2520and%2520Jie%2520Wang%2520and%2520Hong%2520Xie%2520and%2520Hao%2520Wang%2520and%2520Defu%2520Lian%2520and%2520Enhong%2520Chen%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520graph%2520learning%2520have%2520revolutionized%2520the%2520way%2520to%250Aunderstand%2520and%2520analyze%2520data%2520with%2520complex%2520structures.%2520Notably%252C%2520Graph%2520Neural%250ANetworks%2520%2528GNNs%2529%252C%2520i.e.%2520neural%2520network%2520architectures%2520designed%2520for%2520learning%2520graph%250Arepresentations%252C%2520have%2520become%2520a%2520popular%2520paradigm.%2520With%2520these%2520models%2520being%250Ausually%2520characterized%2520by%2520intuition-driven%2520design%2520or%2520highly%2520intricate%250Acomponents%252C%2520placing%2520them%2520within%2520the%2520theoretical%2520analysis%2520framework%2520to%2520distill%250Athe%2520core%2520concepts%252C%2520helps%2520understand%2520the%2520key%2520principles%2520that%2520drive%2520the%250Afunctionality%2520better%2520and%2520guide%2520further%2520development.%2520Given%2520this%2520surge%2520in%250Ainterest%252C%2520this%2520article%2520provides%2520a%2520comprehensive%2520summary%2520of%2520the%2520theoretical%250Afoundations%2520and%2520breakthroughs%2520concerning%2520the%2520approximation%2520and%2520learning%250Abehaviors%2520intrinsic%2520to%2520prevalent%2520graph%2520learning%2520models.%2520Encompassing%250Adiscussions%2520on%2520fundamental%2520aspects%2520such%2520as%2520expressiveness%2520power%252C%250Ageneralization%252C%2520optimization%252C%2520and%2520unique%2520phenomena%2520such%2520as%2520over-smoothing%2520and%250Aover-squashing%252C%2520this%2520piece%2520delves%2520into%2520the%2520theoretical%2520foundations%2520and%2520frontier%250Adriving%2520the%2520evolution%2520of%2520graph%2520learning.%2520In%2520addition%252C%2520this%2520article%2520also%250Apresents%2520several%2520challenges%2520and%2520further%2520initiates%2520discussions%2520on%2520possible%250Asolutions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03125v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Foundations%20and%20Frontiers%20of%20Graph%20Learning%20Theory&entry.906535625=Yu%20Huang%20and%20Min%20Zhou%20and%20Menglin%20Yang%20and%20Zhen%20Wang%20and%20Muhan%20Zhang%20and%20Jie%20Wang%20and%20Hong%20Xie%20and%20Hao%20Wang%20and%20Defu%20Lian%20and%20Enhong%20Chen&entry.1292438233=%20%20Recent%20advancements%20in%20graph%20learning%20have%20revolutionized%20the%20way%20to%0Aunderstand%20and%20analyze%20data%20with%20complex%20structures.%20Notably%2C%20Graph%20Neural%0ANetworks%20%28GNNs%29%2C%20i.e.%20neural%20network%20architectures%20designed%20for%20learning%20graph%0Arepresentations%2C%20have%20become%20a%20popular%20paradigm.%20With%20these%20models%20being%0Ausually%20characterized%20by%20intuition-driven%20design%20or%20highly%20intricate%0Acomponents%2C%20placing%20them%20within%20the%20theoretical%20analysis%20framework%20to%20distill%0Athe%20core%20concepts%2C%20helps%20understand%20the%20key%20principles%20that%20drive%20the%0Afunctionality%20better%20and%20guide%20further%20development.%20Given%20this%20surge%20in%0Ainterest%2C%20this%20article%20provides%20a%20comprehensive%20summary%20of%20the%20theoretical%0Afoundations%20and%20breakthroughs%20concerning%20the%20approximation%20and%20learning%0Abehaviors%20intrinsic%20to%20prevalent%20graph%20learning%20models.%20Encompassing%0Adiscussions%20on%20fundamental%20aspects%20such%20as%20expressiveness%20power%2C%0Ageneralization%2C%20optimization%2C%20and%20unique%20phenomena%20such%20as%20over-smoothing%20and%0Aover-squashing%2C%20this%20piece%20delves%20into%20the%20theoretical%20foundations%20and%20frontier%0Adriving%20the%20evolution%20of%20graph%20learning.%20In%20addition%2C%20this%20article%20also%0Apresents%20several%20challenges%20and%20further%20initiates%20discussions%20on%20possible%0Asolutions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03125v1&entry.124074799=Read"},
{"title": "YOLOv5, YOLOv8 and YOLOv10: The Go-To Detectors for Real-time Vision", "author": "Muhammad Hussain", "abstract": "  This paper presents a comprehensive review of the evolution of the YOLO (You\nOnly Look Once) object detection algorithm, focusing on YOLOv5, YOLOv8, and\nYOLOv10. We analyze the architectural advancements, performance improvements,\nand suitability for edge deployment across these versions. YOLOv5 introduced\nsignificant innovations such as the CSPDarknet backbone and Mosaic\nAugmentation, balancing speed and accuracy. YOLOv8 built upon this foundation\nwith enhanced feature extraction and anchor-free detection, improving\nversatility and performance. YOLOv10 represents a leap forward with NMS-free\ntraining, spatial-channel decoupled downsampling, and large-kernel\nconvolutions, achieving state-of-the-art performance with reduced computational\noverhead. Our findings highlight the progressive enhancements in accuracy,\nefficiency, and real-time performance, particularly emphasizing their\napplicability in resource-constrained environments. This review provides\ninsights into the trade-offs between model complexity and detection accuracy,\noffering guidance for selecting the most appropriate YOLO version for specific\nedge computing applications.\n", "link": "http://arxiv.org/abs/2407.02988v1", "date": "2024-07-03", "relevancy": 1.9296, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5089}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4772}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.477}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20YOLOv5%2C%20YOLOv8%20and%20YOLOv10%3A%20The%20Go-To%20Detectors%20for%20Real-time%20Vision&body=Title%3A%20YOLOv5%2C%20YOLOv8%20and%20YOLOv10%3A%20The%20Go-To%20Detectors%20for%20Real-time%20Vision%0AAuthor%3A%20Muhammad%20Hussain%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20comprehensive%20review%20of%20the%20evolution%20of%20the%20YOLO%20%28You%0AOnly%20Look%20Once%29%20object%20detection%20algorithm%2C%20focusing%20on%20YOLOv5%2C%20YOLOv8%2C%20and%0AYOLOv10.%20We%20analyze%20the%20architectural%20advancements%2C%20performance%20improvements%2C%0Aand%20suitability%20for%20edge%20deployment%20across%20these%20versions.%20YOLOv5%20introduced%0Asignificant%20innovations%20such%20as%20the%20CSPDarknet%20backbone%20and%20Mosaic%0AAugmentation%2C%20balancing%20speed%20and%20accuracy.%20YOLOv8%20built%20upon%20this%20foundation%0Awith%20enhanced%20feature%20extraction%20and%20anchor-free%20detection%2C%20improving%0Aversatility%20and%20performance.%20YOLOv10%20represents%20a%20leap%20forward%20with%20NMS-free%0Atraining%2C%20spatial-channel%20decoupled%20downsampling%2C%20and%20large-kernel%0Aconvolutions%2C%20achieving%20state-of-the-art%20performance%20with%20reduced%20computational%0Aoverhead.%20Our%20findings%20highlight%20the%20progressive%20enhancements%20in%20accuracy%2C%0Aefficiency%2C%20and%20real-time%20performance%2C%20particularly%20emphasizing%20their%0Aapplicability%20in%20resource-constrained%20environments.%20This%20review%20provides%0Ainsights%20into%20the%20trade-offs%20between%20model%20complexity%20and%20detection%20accuracy%2C%0Aoffering%20guidance%20for%20selecting%20the%20most%20appropriate%20YOLO%20version%20for%20specific%0Aedge%20computing%20applications.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02988v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYOLOv5%252C%2520YOLOv8%2520and%2520YOLOv10%253A%2520The%2520Go-To%2520Detectors%2520for%2520Real-time%2520Vision%26entry.906535625%3DMuhammad%2520Hussain%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520comprehensive%2520review%2520of%2520the%2520evolution%2520of%2520the%2520YOLO%2520%2528You%250AOnly%2520Look%2520Once%2529%2520object%2520detection%2520algorithm%252C%2520focusing%2520on%2520YOLOv5%252C%2520YOLOv8%252C%2520and%250AYOLOv10.%2520We%2520analyze%2520the%2520architectural%2520advancements%252C%2520performance%2520improvements%252C%250Aand%2520suitability%2520for%2520edge%2520deployment%2520across%2520these%2520versions.%2520YOLOv5%2520introduced%250Asignificant%2520innovations%2520such%2520as%2520the%2520CSPDarknet%2520backbone%2520and%2520Mosaic%250AAugmentation%252C%2520balancing%2520speed%2520and%2520accuracy.%2520YOLOv8%2520built%2520upon%2520this%2520foundation%250Awith%2520enhanced%2520feature%2520extraction%2520and%2520anchor-free%2520detection%252C%2520improving%250Aversatility%2520and%2520performance.%2520YOLOv10%2520represents%2520a%2520leap%2520forward%2520with%2520NMS-free%250Atraining%252C%2520spatial-channel%2520decoupled%2520downsampling%252C%2520and%2520large-kernel%250Aconvolutions%252C%2520achieving%2520state-of-the-art%2520performance%2520with%2520reduced%2520computational%250Aoverhead.%2520Our%2520findings%2520highlight%2520the%2520progressive%2520enhancements%2520in%2520accuracy%252C%250Aefficiency%252C%2520and%2520real-time%2520performance%252C%2520particularly%2520emphasizing%2520their%250Aapplicability%2520in%2520resource-constrained%2520environments.%2520This%2520review%2520provides%250Ainsights%2520into%2520the%2520trade-offs%2520between%2520model%2520complexity%2520and%2520detection%2520accuracy%252C%250Aoffering%2520guidance%2520for%2520selecting%2520the%2520most%2520appropriate%2520YOLO%2520version%2520for%2520specific%250Aedge%2520computing%2520applications.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02988v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=YOLOv5%2C%20YOLOv8%20and%20YOLOv10%3A%20The%20Go-To%20Detectors%20for%20Real-time%20Vision&entry.906535625=Muhammad%20Hussain&entry.1292438233=%20%20This%20paper%20presents%20a%20comprehensive%20review%20of%20the%20evolution%20of%20the%20YOLO%20%28You%0AOnly%20Look%20Once%29%20object%20detection%20algorithm%2C%20focusing%20on%20YOLOv5%2C%20YOLOv8%2C%20and%0AYOLOv10.%20We%20analyze%20the%20architectural%20advancements%2C%20performance%20improvements%2C%0Aand%20suitability%20for%20edge%20deployment%20across%20these%20versions.%20YOLOv5%20introduced%0Asignificant%20innovations%20such%20as%20the%20CSPDarknet%20backbone%20and%20Mosaic%0AAugmentation%2C%20balancing%20speed%20and%20accuracy.%20YOLOv8%20built%20upon%20this%20foundation%0Awith%20enhanced%20feature%20extraction%20and%20anchor-free%20detection%2C%20improving%0Aversatility%20and%20performance.%20YOLOv10%20represents%20a%20leap%20forward%20with%20NMS-free%0Atraining%2C%20spatial-channel%20decoupled%20downsampling%2C%20and%20large-kernel%0Aconvolutions%2C%20achieving%20state-of-the-art%20performance%20with%20reduced%20computational%0Aoverhead.%20Our%20findings%20highlight%20the%20progressive%20enhancements%20in%20accuracy%2C%0Aefficiency%2C%20and%20real-time%20performance%2C%20particularly%20emphasizing%20their%0Aapplicability%20in%20resource-constrained%20environments.%20This%20review%20provides%0Ainsights%20into%20the%20trade-offs%20between%20model%20complexity%20and%20detection%20accuracy%2C%0Aoffering%20guidance%20for%20selecting%20the%20most%20appropriate%20YOLO%20version%20for%20specific%0Aedge%20computing%20applications.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02988v1&entry.124074799=Read"},
{"title": "NeuraLUT: Hiding Neural Network Density in Boolean Synthesizable\n  Functions", "author": "Marta Andronic and George A. Constantinides", "abstract": "  Field-Programmable Gate Array (FPGA) accelerators have proven successful in\nhandling latency- and resource-critical deep neural network (DNN) inference\ntasks. Among the most computationally intensive operations in a neural network\n(NN) is the dot product between the feature and weight vectors. Thus, some\nprevious FPGA acceleration works have proposed mapping neurons with quantized\ninputs and outputs directly to lookup tables (LUTs) for hardware\nimplementation. In these works, the boundaries of the neurons coincide with the\nboundaries of the LUTs. We propose relaxing these boundaries and mapping entire\nsub-networks to a single LUT. As the sub-networks are absorbed within the LUT,\nthe NN topology and precision within a partition do not affect the size of the\nlookup tables generated. Therefore, we utilize fully connected layers with\nfloating-point precision inside each partition, which benefit from being\nuniversal function approximators, but with rigid sparsity and quantization\nenforced between partitions, where the NN topology becomes exposed to the\ncircuit topology. Although cheap to implement, this approach can lead to very\ndeep NNs, and so to tackle challenges like vanishing gradients, we also\nintroduce skip connections inside the partitions. The resulting methodology can\nbe seen as training DNNs with a specific FPGA hardware-inspired sparsity\npattern that allows them to be mapped to much shallower circuit-level networks,\nthereby significantly improving latency. We validate our proposed method on a\nknown latency-critical task, jet substructure tagging, and on the classical\ncomputer vision task, digit classification using MNIST. Our approach allows for\ngreater function expressivity within the LUTs compared to existing work,\nleading to up to $4.3\\times$ lower latency NNs for the same accuracy.\n", "link": "http://arxiv.org/abs/2403.00849v2", "date": "2024-07-03", "relevancy": 1.9239, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5293}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4726}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.47}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NeuraLUT%3A%20Hiding%20Neural%20Network%20Density%20in%20Boolean%20Synthesizable%0A%20%20Functions&body=Title%3A%20NeuraLUT%3A%20Hiding%20Neural%20Network%20Density%20in%20Boolean%20Synthesizable%0A%20%20Functions%0AAuthor%3A%20Marta%20Andronic%20and%20George%20A.%20Constantinides%0AAbstract%3A%20%20%20Field-Programmable%20Gate%20Array%20%28FPGA%29%20accelerators%20have%20proven%20successful%20in%0Ahandling%20latency-%20and%20resource-critical%20deep%20neural%20network%20%28DNN%29%20inference%0Atasks.%20Among%20the%20most%20computationally%20intensive%20operations%20in%20a%20neural%20network%0A%28NN%29%20is%20the%20dot%20product%20between%20the%20feature%20and%20weight%20vectors.%20Thus%2C%20some%0Aprevious%20FPGA%20acceleration%20works%20have%20proposed%20mapping%20neurons%20with%20quantized%0Ainputs%20and%20outputs%20directly%20to%20lookup%20tables%20%28LUTs%29%20for%20hardware%0Aimplementation.%20In%20these%20works%2C%20the%20boundaries%20of%20the%20neurons%20coincide%20with%20the%0Aboundaries%20of%20the%20LUTs.%20We%20propose%20relaxing%20these%20boundaries%20and%20mapping%20entire%0Asub-networks%20to%20a%20single%20LUT.%20As%20the%20sub-networks%20are%20absorbed%20within%20the%20LUT%2C%0Athe%20NN%20topology%20and%20precision%20within%20a%20partition%20do%20not%20affect%20the%20size%20of%20the%0Alookup%20tables%20generated.%20Therefore%2C%20we%20utilize%20fully%20connected%20layers%20with%0Afloating-point%20precision%20inside%20each%20partition%2C%20which%20benefit%20from%20being%0Auniversal%20function%20approximators%2C%20but%20with%20rigid%20sparsity%20and%20quantization%0Aenforced%20between%20partitions%2C%20where%20the%20NN%20topology%20becomes%20exposed%20to%20the%0Acircuit%20topology.%20Although%20cheap%20to%20implement%2C%20this%20approach%20can%20lead%20to%20very%0Adeep%20NNs%2C%20and%20so%20to%20tackle%20challenges%20like%20vanishing%20gradients%2C%20we%20also%0Aintroduce%20skip%20connections%20inside%20the%20partitions.%20The%20resulting%20methodology%20can%0Abe%20seen%20as%20training%20DNNs%20with%20a%20specific%20FPGA%20hardware-inspired%20sparsity%0Apattern%20that%20allows%20them%20to%20be%20mapped%20to%20much%20shallower%20circuit-level%20networks%2C%0Athereby%20significantly%20improving%20latency.%20We%20validate%20our%20proposed%20method%20on%20a%0Aknown%20latency-critical%20task%2C%20jet%20substructure%20tagging%2C%20and%20on%20the%20classical%0Acomputer%20vision%20task%2C%20digit%20classification%20using%20MNIST.%20Our%20approach%20allows%20for%0Agreater%20function%20expressivity%20within%20the%20LUTs%20compared%20to%20existing%20work%2C%0Aleading%20to%20up%20to%20%244.3%5Ctimes%24%20lower%20latency%20NNs%20for%20the%20same%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.00849v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNeuraLUT%253A%2520Hiding%2520Neural%2520Network%2520Density%2520in%2520Boolean%2520Synthesizable%250A%2520%2520Functions%26entry.906535625%3DMarta%2520Andronic%2520and%2520George%2520A.%2520Constantinides%26entry.1292438233%3D%2520%2520Field-Programmable%2520Gate%2520Array%2520%2528FPGA%2529%2520accelerators%2520have%2520proven%2520successful%2520in%250Ahandling%2520latency-%2520and%2520resource-critical%2520deep%2520neural%2520network%2520%2528DNN%2529%2520inference%250Atasks.%2520Among%2520the%2520most%2520computationally%2520intensive%2520operations%2520in%2520a%2520neural%2520network%250A%2528NN%2529%2520is%2520the%2520dot%2520product%2520between%2520the%2520feature%2520and%2520weight%2520vectors.%2520Thus%252C%2520some%250Aprevious%2520FPGA%2520acceleration%2520works%2520have%2520proposed%2520mapping%2520neurons%2520with%2520quantized%250Ainputs%2520and%2520outputs%2520directly%2520to%2520lookup%2520tables%2520%2528LUTs%2529%2520for%2520hardware%250Aimplementation.%2520In%2520these%2520works%252C%2520the%2520boundaries%2520of%2520the%2520neurons%2520coincide%2520with%2520the%250Aboundaries%2520of%2520the%2520LUTs.%2520We%2520propose%2520relaxing%2520these%2520boundaries%2520and%2520mapping%2520entire%250Asub-networks%2520to%2520a%2520single%2520LUT.%2520As%2520the%2520sub-networks%2520are%2520absorbed%2520within%2520the%2520LUT%252C%250Athe%2520NN%2520topology%2520and%2520precision%2520within%2520a%2520partition%2520do%2520not%2520affect%2520the%2520size%2520of%2520the%250Alookup%2520tables%2520generated.%2520Therefore%252C%2520we%2520utilize%2520fully%2520connected%2520layers%2520with%250Afloating-point%2520precision%2520inside%2520each%2520partition%252C%2520which%2520benefit%2520from%2520being%250Auniversal%2520function%2520approximators%252C%2520but%2520with%2520rigid%2520sparsity%2520and%2520quantization%250Aenforced%2520between%2520partitions%252C%2520where%2520the%2520NN%2520topology%2520becomes%2520exposed%2520to%2520the%250Acircuit%2520topology.%2520Although%2520cheap%2520to%2520implement%252C%2520this%2520approach%2520can%2520lead%2520to%2520very%250Adeep%2520NNs%252C%2520and%2520so%2520to%2520tackle%2520challenges%2520like%2520vanishing%2520gradients%252C%2520we%2520also%250Aintroduce%2520skip%2520connections%2520inside%2520the%2520partitions.%2520The%2520resulting%2520methodology%2520can%250Abe%2520seen%2520as%2520training%2520DNNs%2520with%2520a%2520specific%2520FPGA%2520hardware-inspired%2520sparsity%250Apattern%2520that%2520allows%2520them%2520to%2520be%2520mapped%2520to%2520much%2520shallower%2520circuit-level%2520networks%252C%250Athereby%2520significantly%2520improving%2520latency.%2520We%2520validate%2520our%2520proposed%2520method%2520on%2520a%250Aknown%2520latency-critical%2520task%252C%2520jet%2520substructure%2520tagging%252C%2520and%2520on%2520the%2520classical%250Acomputer%2520vision%2520task%252C%2520digit%2520classification%2520using%2520MNIST.%2520Our%2520approach%2520allows%2520for%250Agreater%2520function%2520expressivity%2520within%2520the%2520LUTs%2520compared%2520to%2520existing%2520work%252C%250Aleading%2520to%2520up%2520to%2520%25244.3%255Ctimes%2524%2520lower%2520latency%2520NNs%2520for%2520the%2520same%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.00849v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NeuraLUT%3A%20Hiding%20Neural%20Network%20Density%20in%20Boolean%20Synthesizable%0A%20%20Functions&entry.906535625=Marta%20Andronic%20and%20George%20A.%20Constantinides&entry.1292438233=%20%20Field-Programmable%20Gate%20Array%20%28FPGA%29%20accelerators%20have%20proven%20successful%20in%0Ahandling%20latency-%20and%20resource-critical%20deep%20neural%20network%20%28DNN%29%20inference%0Atasks.%20Among%20the%20most%20computationally%20intensive%20operations%20in%20a%20neural%20network%0A%28NN%29%20is%20the%20dot%20product%20between%20the%20feature%20and%20weight%20vectors.%20Thus%2C%20some%0Aprevious%20FPGA%20acceleration%20works%20have%20proposed%20mapping%20neurons%20with%20quantized%0Ainputs%20and%20outputs%20directly%20to%20lookup%20tables%20%28LUTs%29%20for%20hardware%0Aimplementation.%20In%20these%20works%2C%20the%20boundaries%20of%20the%20neurons%20coincide%20with%20the%0Aboundaries%20of%20the%20LUTs.%20We%20propose%20relaxing%20these%20boundaries%20and%20mapping%20entire%0Asub-networks%20to%20a%20single%20LUT.%20As%20the%20sub-networks%20are%20absorbed%20within%20the%20LUT%2C%0Athe%20NN%20topology%20and%20precision%20within%20a%20partition%20do%20not%20affect%20the%20size%20of%20the%0Alookup%20tables%20generated.%20Therefore%2C%20we%20utilize%20fully%20connected%20layers%20with%0Afloating-point%20precision%20inside%20each%20partition%2C%20which%20benefit%20from%20being%0Auniversal%20function%20approximators%2C%20but%20with%20rigid%20sparsity%20and%20quantization%0Aenforced%20between%20partitions%2C%20where%20the%20NN%20topology%20becomes%20exposed%20to%20the%0Acircuit%20topology.%20Although%20cheap%20to%20implement%2C%20this%20approach%20can%20lead%20to%20very%0Adeep%20NNs%2C%20and%20so%20to%20tackle%20challenges%20like%20vanishing%20gradients%2C%20we%20also%0Aintroduce%20skip%20connections%20inside%20the%20partitions.%20The%20resulting%20methodology%20can%0Abe%20seen%20as%20training%20DNNs%20with%20a%20specific%20FPGA%20hardware-inspired%20sparsity%0Apattern%20that%20allows%20them%20to%20be%20mapped%20to%20much%20shallower%20circuit-level%20networks%2C%0Athereby%20significantly%20improving%20latency.%20We%20validate%20our%20proposed%20method%20on%20a%0Aknown%20latency-critical%20task%2C%20jet%20substructure%20tagging%2C%20and%20on%20the%20classical%0Acomputer%20vision%20task%2C%20digit%20classification%20using%20MNIST.%20Our%20approach%20allows%20for%0Agreater%20function%20expressivity%20within%20the%20LUTs%20compared%20to%20existing%20work%2C%0Aleading%20to%20up%20to%20%244.3%5Ctimes%24%20lower%20latency%20NNs%20for%20the%20same%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.00849v2&entry.124074799=Read"},
{"title": "Universal Length Generalization with Turing Programs", "author": "Kaiying Hou and David Brandfonbrener and Sham Kakade and Samy Jelassi and Eran Malach", "abstract": "  Length generalization refers to the ability to extrapolate from short\ntraining sequences to long test sequences and is a challenge for current large\nlanguage models. While prior work has proposed some architecture or data format\nchanges to achieve length generalization, these proposals typically apply to a\nlimited set of tasks. Building on prior scratchpad and Chain-of-Thought (CoT)\ntechniques, we propose Turing Programs, a novel CoT strategy that decomposes an\nalgorithmic task into steps mimicking the computation of a Turing Machine. This\nframework is both universal, as it can accommodate any algorithmic task, and\nsimple, requiring only copying text from the context with small modifications.\nWe show that by using Turing Programs, we obtain robust length generalization\non a range of algorithmic tasks: addition, multiplication and in-context SGD.\nWe then demonstrate that transformers achieve length generalization on random\nTuring Programs, suggesting that length generalization is possible for any\nalgorithmic task. Finally, we theoretically prove that transformers can\nimplement Turing Programs, constructing a simple RASP (Weiss et al.) program\nthat simulates an arbitrary Turing machine.\n", "link": "http://arxiv.org/abs/2407.03310v1", "date": "2024-07-03", "relevancy": 1.9215, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4964}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4827}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Universal%20Length%20Generalization%20with%20Turing%20Programs&body=Title%3A%20Universal%20Length%20Generalization%20with%20Turing%20Programs%0AAuthor%3A%20Kaiying%20Hou%20and%20David%20Brandfonbrener%20and%20Sham%20Kakade%20and%20Samy%20Jelassi%20and%20Eran%20Malach%0AAbstract%3A%20%20%20Length%20generalization%20refers%20to%20the%20ability%20to%20extrapolate%20from%20short%0Atraining%20sequences%20to%20long%20test%20sequences%20and%20is%20a%20challenge%20for%20current%20large%0Alanguage%20models.%20While%20prior%20work%20has%20proposed%20some%20architecture%20or%20data%20format%0Achanges%20to%20achieve%20length%20generalization%2C%20these%20proposals%20typically%20apply%20to%20a%0Alimited%20set%20of%20tasks.%20Building%20on%20prior%20scratchpad%20and%20Chain-of-Thought%20%28CoT%29%0Atechniques%2C%20we%20propose%20Turing%20Programs%2C%20a%20novel%20CoT%20strategy%20that%20decomposes%20an%0Aalgorithmic%20task%20into%20steps%20mimicking%20the%20computation%20of%20a%20Turing%20Machine.%20This%0Aframework%20is%20both%20universal%2C%20as%20it%20can%20accommodate%20any%20algorithmic%20task%2C%20and%0Asimple%2C%20requiring%20only%20copying%20text%20from%20the%20context%20with%20small%20modifications.%0AWe%20show%20that%20by%20using%20Turing%20Programs%2C%20we%20obtain%20robust%20length%20generalization%0Aon%20a%20range%20of%20algorithmic%20tasks%3A%20addition%2C%20multiplication%20and%20in-context%20SGD.%0AWe%20then%20demonstrate%20that%20transformers%20achieve%20length%20generalization%20on%20random%0ATuring%20Programs%2C%20suggesting%20that%20length%20generalization%20is%20possible%20for%20any%0Aalgorithmic%20task.%20Finally%2C%20we%20theoretically%20prove%20that%20transformers%20can%0Aimplement%20Turing%20Programs%2C%20constructing%20a%20simple%20RASP%20%28Weiss%20et%20al.%29%20program%0Athat%20simulates%20an%20arbitrary%20Turing%20machine.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03310v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniversal%2520Length%2520Generalization%2520with%2520Turing%2520Programs%26entry.906535625%3DKaiying%2520Hou%2520and%2520David%2520Brandfonbrener%2520and%2520Sham%2520Kakade%2520and%2520Samy%2520Jelassi%2520and%2520Eran%2520Malach%26entry.1292438233%3D%2520%2520Length%2520generalization%2520refers%2520to%2520the%2520ability%2520to%2520extrapolate%2520from%2520short%250Atraining%2520sequences%2520to%2520long%2520test%2520sequences%2520and%2520is%2520a%2520challenge%2520for%2520current%2520large%250Alanguage%2520models.%2520While%2520prior%2520work%2520has%2520proposed%2520some%2520architecture%2520or%2520data%2520format%250Achanges%2520to%2520achieve%2520length%2520generalization%252C%2520these%2520proposals%2520typically%2520apply%2520to%2520a%250Alimited%2520set%2520of%2520tasks.%2520Building%2520on%2520prior%2520scratchpad%2520and%2520Chain-of-Thought%2520%2528CoT%2529%250Atechniques%252C%2520we%2520propose%2520Turing%2520Programs%252C%2520a%2520novel%2520CoT%2520strategy%2520that%2520decomposes%2520an%250Aalgorithmic%2520task%2520into%2520steps%2520mimicking%2520the%2520computation%2520of%2520a%2520Turing%2520Machine.%2520This%250Aframework%2520is%2520both%2520universal%252C%2520as%2520it%2520can%2520accommodate%2520any%2520algorithmic%2520task%252C%2520and%250Asimple%252C%2520requiring%2520only%2520copying%2520text%2520from%2520the%2520context%2520with%2520small%2520modifications.%250AWe%2520show%2520that%2520by%2520using%2520Turing%2520Programs%252C%2520we%2520obtain%2520robust%2520length%2520generalization%250Aon%2520a%2520range%2520of%2520algorithmic%2520tasks%253A%2520addition%252C%2520multiplication%2520and%2520in-context%2520SGD.%250AWe%2520then%2520demonstrate%2520that%2520transformers%2520achieve%2520length%2520generalization%2520on%2520random%250ATuring%2520Programs%252C%2520suggesting%2520that%2520length%2520generalization%2520is%2520possible%2520for%2520any%250Aalgorithmic%2520task.%2520Finally%252C%2520we%2520theoretically%2520prove%2520that%2520transformers%2520can%250Aimplement%2520Turing%2520Programs%252C%2520constructing%2520a%2520simple%2520RASP%2520%2528Weiss%2520et%2520al.%2529%2520program%250Athat%2520simulates%2520an%2520arbitrary%2520Turing%2520machine.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03310v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Universal%20Length%20Generalization%20with%20Turing%20Programs&entry.906535625=Kaiying%20Hou%20and%20David%20Brandfonbrener%20and%20Sham%20Kakade%20and%20Samy%20Jelassi%20and%20Eran%20Malach&entry.1292438233=%20%20Length%20generalization%20refers%20to%20the%20ability%20to%20extrapolate%20from%20short%0Atraining%20sequences%20to%20long%20test%20sequences%20and%20is%20a%20challenge%20for%20current%20large%0Alanguage%20models.%20While%20prior%20work%20has%20proposed%20some%20architecture%20or%20data%20format%0Achanges%20to%20achieve%20length%20generalization%2C%20these%20proposals%20typically%20apply%20to%20a%0Alimited%20set%20of%20tasks.%20Building%20on%20prior%20scratchpad%20and%20Chain-of-Thought%20%28CoT%29%0Atechniques%2C%20we%20propose%20Turing%20Programs%2C%20a%20novel%20CoT%20strategy%20that%20decomposes%20an%0Aalgorithmic%20task%20into%20steps%20mimicking%20the%20computation%20of%20a%20Turing%20Machine.%20This%0Aframework%20is%20both%20universal%2C%20as%20it%20can%20accommodate%20any%20algorithmic%20task%2C%20and%0Asimple%2C%20requiring%20only%20copying%20text%20from%20the%20context%20with%20small%20modifications.%0AWe%20show%20that%20by%20using%20Turing%20Programs%2C%20we%20obtain%20robust%20length%20generalization%0Aon%20a%20range%20of%20algorithmic%20tasks%3A%20addition%2C%20multiplication%20and%20in-context%20SGD.%0AWe%20then%20demonstrate%20that%20transformers%20achieve%20length%20generalization%20on%20random%0ATuring%20Programs%2C%20suggesting%20that%20length%20generalization%20is%20possible%20for%20any%0Aalgorithmic%20task.%20Finally%2C%20we%20theoretically%20prove%20that%20transformers%20can%0Aimplement%20Turing%20Programs%2C%20constructing%20a%20simple%20RASP%20%28Weiss%20et%20al.%29%20program%0Athat%20simulates%20an%20arbitrary%20Turing%20machine.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03310v1&entry.124074799=Read"},
{"title": "For a semiotic AI: Bridging computer vision and visual semiotics for\n  computational observation of large scale facial image archives", "author": "Lia Morra and Antonio Santangelo and Pietro Basci and Luca Piano and Fabio Garcea and Fabrizio Lamberti and Massimo Leone", "abstract": "  Social networks are creating a digital world in which the cognitive,\nemotional, and pragmatic value of the imagery of human faces and bodies is\narguably changing. However, researchers in the digital humanities are often\nill-equipped to study these phenomena at scale. This work presents FRESCO (Face\nRepresentation in E-Societies through Computational Observation), a framework\ndesigned to explore the socio-cultural implications of images on social media\nplatforms at scale. FRESCO deconstructs images into numerical and categorical\nvariables using state-of-the-art computer vision techniques, aligning with the\nprinciples of visual semiotics. The framework analyzes images across three\nlevels: the plastic level, encompassing fundamental visual features like lines\nand colors; the figurative level, representing specific entities or concepts;\nand the enunciation level, which focuses particularly on constructing the point\nof view of the spectator and observer. These levels are analyzed to discern\ndeeper narrative layers within the imagery. Experimental validation confirms\nthe reliability and utility of FRESCO, and we assess its consistency and\nprecision across two public datasets. Subsequently, we introduce the FRESCO\nscore, a metric derived from the framework's output that serves as a reliable\nmeasure of similarity in image content.\n", "link": "http://arxiv.org/abs/2407.03268v1", "date": "2024-07-03", "relevancy": 1.9213, "topK": [{"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4952}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4778}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20For%20a%20semiotic%20AI%3A%20Bridging%20computer%20vision%20and%20visual%20semiotics%20for%0A%20%20computational%20observation%20of%20large%20scale%20facial%20image%20archives&body=Title%3A%20For%20a%20semiotic%20AI%3A%20Bridging%20computer%20vision%20and%20visual%20semiotics%20for%0A%20%20computational%20observation%20of%20large%20scale%20facial%20image%20archives%0AAuthor%3A%20Lia%20Morra%20and%20Antonio%20Santangelo%20and%20Pietro%20Basci%20and%20Luca%20Piano%20and%20Fabio%20Garcea%20and%20Fabrizio%20Lamberti%20and%20Massimo%20Leone%0AAbstract%3A%20%20%20Social%20networks%20are%20creating%20a%20digital%20world%20in%20which%20the%20cognitive%2C%0Aemotional%2C%20and%20pragmatic%20value%20of%20the%20imagery%20of%20human%20faces%20and%20bodies%20is%0Aarguably%20changing.%20However%2C%20researchers%20in%20the%20digital%20humanities%20are%20often%0Aill-equipped%20to%20study%20these%20phenomena%20at%20scale.%20This%20work%20presents%20FRESCO%20%28Face%0ARepresentation%20in%20E-Societies%20through%20Computational%20Observation%29%2C%20a%20framework%0Adesigned%20to%20explore%20the%20socio-cultural%20implications%20of%20images%20on%20social%20media%0Aplatforms%20at%20scale.%20FRESCO%20deconstructs%20images%20into%20numerical%20and%20categorical%0Avariables%20using%20state-of-the-art%20computer%20vision%20techniques%2C%20aligning%20with%20the%0Aprinciples%20of%20visual%20semiotics.%20The%20framework%20analyzes%20images%20across%20three%0Alevels%3A%20the%20plastic%20level%2C%20encompassing%20fundamental%20visual%20features%20like%20lines%0Aand%20colors%3B%20the%20figurative%20level%2C%20representing%20specific%20entities%20or%20concepts%3B%0Aand%20the%20enunciation%20level%2C%20which%20focuses%20particularly%20on%20constructing%20the%20point%0Aof%20view%20of%20the%20spectator%20and%20observer.%20These%20levels%20are%20analyzed%20to%20discern%0Adeeper%20narrative%20layers%20within%20the%20imagery.%20Experimental%20validation%20confirms%0Athe%20reliability%20and%20utility%20of%20FRESCO%2C%20and%20we%20assess%20its%20consistency%20and%0Aprecision%20across%20two%20public%20datasets.%20Subsequently%2C%20we%20introduce%20the%20FRESCO%0Ascore%2C%20a%20metric%20derived%20from%20the%20framework%27s%20output%20that%20serves%20as%20a%20reliable%0Ameasure%20of%20similarity%20in%20image%20content.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03268v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFor%2520a%2520semiotic%2520AI%253A%2520Bridging%2520computer%2520vision%2520and%2520visual%2520semiotics%2520for%250A%2520%2520computational%2520observation%2520of%2520large%2520scale%2520facial%2520image%2520archives%26entry.906535625%3DLia%2520Morra%2520and%2520Antonio%2520Santangelo%2520and%2520Pietro%2520Basci%2520and%2520Luca%2520Piano%2520and%2520Fabio%2520Garcea%2520and%2520Fabrizio%2520Lamberti%2520and%2520Massimo%2520Leone%26entry.1292438233%3D%2520%2520Social%2520networks%2520are%2520creating%2520a%2520digital%2520world%2520in%2520which%2520the%2520cognitive%252C%250Aemotional%252C%2520and%2520pragmatic%2520value%2520of%2520the%2520imagery%2520of%2520human%2520faces%2520and%2520bodies%2520is%250Aarguably%2520changing.%2520However%252C%2520researchers%2520in%2520the%2520digital%2520humanities%2520are%2520often%250Aill-equipped%2520to%2520study%2520these%2520phenomena%2520at%2520scale.%2520This%2520work%2520presents%2520FRESCO%2520%2528Face%250ARepresentation%2520in%2520E-Societies%2520through%2520Computational%2520Observation%2529%252C%2520a%2520framework%250Adesigned%2520to%2520explore%2520the%2520socio-cultural%2520implications%2520of%2520images%2520on%2520social%2520media%250Aplatforms%2520at%2520scale.%2520FRESCO%2520deconstructs%2520images%2520into%2520numerical%2520and%2520categorical%250Avariables%2520using%2520state-of-the-art%2520computer%2520vision%2520techniques%252C%2520aligning%2520with%2520the%250Aprinciples%2520of%2520visual%2520semiotics.%2520The%2520framework%2520analyzes%2520images%2520across%2520three%250Alevels%253A%2520the%2520plastic%2520level%252C%2520encompassing%2520fundamental%2520visual%2520features%2520like%2520lines%250Aand%2520colors%253B%2520the%2520figurative%2520level%252C%2520representing%2520specific%2520entities%2520or%2520concepts%253B%250Aand%2520the%2520enunciation%2520level%252C%2520which%2520focuses%2520particularly%2520on%2520constructing%2520the%2520point%250Aof%2520view%2520of%2520the%2520spectator%2520and%2520observer.%2520These%2520levels%2520are%2520analyzed%2520to%2520discern%250Adeeper%2520narrative%2520layers%2520within%2520the%2520imagery.%2520Experimental%2520validation%2520confirms%250Athe%2520reliability%2520and%2520utility%2520of%2520FRESCO%252C%2520and%2520we%2520assess%2520its%2520consistency%2520and%250Aprecision%2520across%2520two%2520public%2520datasets.%2520Subsequently%252C%2520we%2520introduce%2520the%2520FRESCO%250Ascore%252C%2520a%2520metric%2520derived%2520from%2520the%2520framework%2527s%2520output%2520that%2520serves%2520as%2520a%2520reliable%250Ameasure%2520of%2520similarity%2520in%2520image%2520content.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03268v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=For%20a%20semiotic%20AI%3A%20Bridging%20computer%20vision%20and%20visual%20semiotics%20for%0A%20%20computational%20observation%20of%20large%20scale%20facial%20image%20archives&entry.906535625=Lia%20Morra%20and%20Antonio%20Santangelo%20and%20Pietro%20Basci%20and%20Luca%20Piano%20and%20Fabio%20Garcea%20and%20Fabrizio%20Lamberti%20and%20Massimo%20Leone&entry.1292438233=%20%20Social%20networks%20are%20creating%20a%20digital%20world%20in%20which%20the%20cognitive%2C%0Aemotional%2C%20and%20pragmatic%20value%20of%20the%20imagery%20of%20human%20faces%20and%20bodies%20is%0Aarguably%20changing.%20However%2C%20researchers%20in%20the%20digital%20humanities%20are%20often%0Aill-equipped%20to%20study%20these%20phenomena%20at%20scale.%20This%20work%20presents%20FRESCO%20%28Face%0ARepresentation%20in%20E-Societies%20through%20Computational%20Observation%29%2C%20a%20framework%0Adesigned%20to%20explore%20the%20socio-cultural%20implications%20of%20images%20on%20social%20media%0Aplatforms%20at%20scale.%20FRESCO%20deconstructs%20images%20into%20numerical%20and%20categorical%0Avariables%20using%20state-of-the-art%20computer%20vision%20techniques%2C%20aligning%20with%20the%0Aprinciples%20of%20visual%20semiotics.%20The%20framework%20analyzes%20images%20across%20three%0Alevels%3A%20the%20plastic%20level%2C%20encompassing%20fundamental%20visual%20features%20like%20lines%0Aand%20colors%3B%20the%20figurative%20level%2C%20representing%20specific%20entities%20or%20concepts%3B%0Aand%20the%20enunciation%20level%2C%20which%20focuses%20particularly%20on%20constructing%20the%20point%0Aof%20view%20of%20the%20spectator%20and%20observer.%20These%20levels%20are%20analyzed%20to%20discern%0Adeeper%20narrative%20layers%20within%20the%20imagery.%20Experimental%20validation%20confirms%0Athe%20reliability%20and%20utility%20of%20FRESCO%2C%20and%20we%20assess%20its%20consistency%20and%0Aprecision%20across%20two%20public%20datasets.%20Subsequently%2C%20we%20introduce%20the%20FRESCO%0Ascore%2C%20a%20metric%20derived%20from%20the%20framework%27s%20output%20that%20serves%20as%20a%20reliable%0Ameasure%20of%20similarity%20in%20image%20content.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03268v1&entry.124074799=Read"},
{"title": "Anti-Collapse Loss for Deep Metric Learning Based on Coding Rate Metric", "author": "Xiruo Jiang and Yazhou Yao and Xili Dai and Fumin Shen and Xian-Sheng Hua and Heng-Tao Shen", "abstract": "  Deep metric learning (DML) aims to learn a discriminative high-dimensional\nembedding space for downstream tasks like classification, clustering, and\nretrieval. Prior literature predominantly focuses on pair-based and proxy-based\nmethods to maximize inter-class discrepancy and minimize intra-class diversity.\nHowever, these methods tend to suffer from the collapse of the embedding space\ndue to their over-reliance on label information. This leads to sub-optimal\nfeature representation and inferior model performance. To maintain the\nstructure of embedding space and avoid feature collapse, we propose a novel\nloss function called Anti-Collapse Loss. Specifically, our proposed loss\nprimarily draws inspiration from the principle of Maximal Coding Rate\nReduction. It promotes the sparseness of feature clusters in the embedding\nspace to prevent collapse by maximizing the average coding rate of sample\nfeatures or class proxies. Moreover, we integrate our proposed loss with\npair-based and proxy-based methods, resulting in notable performance\nimprovement. Comprehensive experiments on benchmark datasets demonstrate that\nour proposed method outperforms existing state-of-the-art methods. Extensive\nablation studies verify the effectiveness of our method in preventing embedding\nspace collapse and promoting generalization performance.\n", "link": "http://arxiv.org/abs/2407.03106v1", "date": "2024-07-03", "relevancy": 1.9154, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4807}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.479}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4739}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Anti-Collapse%20Loss%20for%20Deep%20Metric%20Learning%20Based%20on%20Coding%20Rate%20Metric&body=Title%3A%20Anti-Collapse%20Loss%20for%20Deep%20Metric%20Learning%20Based%20on%20Coding%20Rate%20Metric%0AAuthor%3A%20Xiruo%20Jiang%20and%20Yazhou%20Yao%20and%20Xili%20Dai%20and%20Fumin%20Shen%20and%20Xian-Sheng%20Hua%20and%20Heng-Tao%20Shen%0AAbstract%3A%20%20%20Deep%20metric%20learning%20%28DML%29%20aims%20to%20learn%20a%20discriminative%20high-dimensional%0Aembedding%20space%20for%20downstream%20tasks%20like%20classification%2C%20clustering%2C%20and%0Aretrieval.%20Prior%20literature%20predominantly%20focuses%20on%20pair-based%20and%20proxy-based%0Amethods%20to%20maximize%20inter-class%20discrepancy%20and%20minimize%20intra-class%20diversity.%0AHowever%2C%20these%20methods%20tend%20to%20suffer%20from%20the%20collapse%20of%20the%20embedding%20space%0Adue%20to%20their%20over-reliance%20on%20label%20information.%20This%20leads%20to%20sub-optimal%0Afeature%20representation%20and%20inferior%20model%20performance.%20To%20maintain%20the%0Astructure%20of%20embedding%20space%20and%20avoid%20feature%20collapse%2C%20we%20propose%20a%20novel%0Aloss%20function%20called%20Anti-Collapse%20Loss.%20Specifically%2C%20our%20proposed%20loss%0Aprimarily%20draws%20inspiration%20from%20the%20principle%20of%20Maximal%20Coding%20Rate%0AReduction.%20It%20promotes%20the%20sparseness%20of%20feature%20clusters%20in%20the%20embedding%0Aspace%20to%20prevent%20collapse%20by%20maximizing%20the%20average%20coding%20rate%20of%20sample%0Afeatures%20or%20class%20proxies.%20Moreover%2C%20we%20integrate%20our%20proposed%20loss%20with%0Apair-based%20and%20proxy-based%20methods%2C%20resulting%20in%20notable%20performance%0Aimprovement.%20Comprehensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%0Aour%20proposed%20method%20outperforms%20existing%20state-of-the-art%20methods.%20Extensive%0Aablation%20studies%20verify%20the%20effectiveness%20of%20our%20method%20in%20preventing%20embedding%0Aspace%20collapse%20and%20promoting%20generalization%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03106v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnti-Collapse%2520Loss%2520for%2520Deep%2520Metric%2520Learning%2520Based%2520on%2520Coding%2520Rate%2520Metric%26entry.906535625%3DXiruo%2520Jiang%2520and%2520Yazhou%2520Yao%2520and%2520Xili%2520Dai%2520and%2520Fumin%2520Shen%2520and%2520Xian-Sheng%2520Hua%2520and%2520Heng-Tao%2520Shen%26entry.1292438233%3D%2520%2520Deep%2520metric%2520learning%2520%2528DML%2529%2520aims%2520to%2520learn%2520a%2520discriminative%2520high-dimensional%250Aembedding%2520space%2520for%2520downstream%2520tasks%2520like%2520classification%252C%2520clustering%252C%2520and%250Aretrieval.%2520Prior%2520literature%2520predominantly%2520focuses%2520on%2520pair-based%2520and%2520proxy-based%250Amethods%2520to%2520maximize%2520inter-class%2520discrepancy%2520and%2520minimize%2520intra-class%2520diversity.%250AHowever%252C%2520these%2520methods%2520tend%2520to%2520suffer%2520from%2520the%2520collapse%2520of%2520the%2520embedding%2520space%250Adue%2520to%2520their%2520over-reliance%2520on%2520label%2520information.%2520This%2520leads%2520to%2520sub-optimal%250Afeature%2520representation%2520and%2520inferior%2520model%2520performance.%2520To%2520maintain%2520the%250Astructure%2520of%2520embedding%2520space%2520and%2520avoid%2520feature%2520collapse%252C%2520we%2520propose%2520a%2520novel%250Aloss%2520function%2520called%2520Anti-Collapse%2520Loss.%2520Specifically%252C%2520our%2520proposed%2520loss%250Aprimarily%2520draws%2520inspiration%2520from%2520the%2520principle%2520of%2520Maximal%2520Coding%2520Rate%250AReduction.%2520It%2520promotes%2520the%2520sparseness%2520of%2520feature%2520clusters%2520in%2520the%2520embedding%250Aspace%2520to%2520prevent%2520collapse%2520by%2520maximizing%2520the%2520average%2520coding%2520rate%2520of%2520sample%250Afeatures%2520or%2520class%2520proxies.%2520Moreover%252C%2520we%2520integrate%2520our%2520proposed%2520loss%2520with%250Apair-based%2520and%2520proxy-based%2520methods%252C%2520resulting%2520in%2520notable%2520performance%250Aimprovement.%2520Comprehensive%2520experiments%2520on%2520benchmark%2520datasets%2520demonstrate%2520that%250Aour%2520proposed%2520method%2520outperforms%2520existing%2520state-of-the-art%2520methods.%2520Extensive%250Aablation%2520studies%2520verify%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520preventing%2520embedding%250Aspace%2520collapse%2520and%2520promoting%2520generalization%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03106v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Anti-Collapse%20Loss%20for%20Deep%20Metric%20Learning%20Based%20on%20Coding%20Rate%20Metric&entry.906535625=Xiruo%20Jiang%20and%20Yazhou%20Yao%20and%20Xili%20Dai%20and%20Fumin%20Shen%20and%20Xian-Sheng%20Hua%20and%20Heng-Tao%20Shen&entry.1292438233=%20%20Deep%20metric%20learning%20%28DML%29%20aims%20to%20learn%20a%20discriminative%20high-dimensional%0Aembedding%20space%20for%20downstream%20tasks%20like%20classification%2C%20clustering%2C%20and%0Aretrieval.%20Prior%20literature%20predominantly%20focuses%20on%20pair-based%20and%20proxy-based%0Amethods%20to%20maximize%20inter-class%20discrepancy%20and%20minimize%20intra-class%20diversity.%0AHowever%2C%20these%20methods%20tend%20to%20suffer%20from%20the%20collapse%20of%20the%20embedding%20space%0Adue%20to%20their%20over-reliance%20on%20label%20information.%20This%20leads%20to%20sub-optimal%0Afeature%20representation%20and%20inferior%20model%20performance.%20To%20maintain%20the%0Astructure%20of%20embedding%20space%20and%20avoid%20feature%20collapse%2C%20we%20propose%20a%20novel%0Aloss%20function%20called%20Anti-Collapse%20Loss.%20Specifically%2C%20our%20proposed%20loss%0Aprimarily%20draws%20inspiration%20from%20the%20principle%20of%20Maximal%20Coding%20Rate%0AReduction.%20It%20promotes%20the%20sparseness%20of%20feature%20clusters%20in%20the%20embedding%0Aspace%20to%20prevent%20collapse%20by%20maximizing%20the%20average%20coding%20rate%20of%20sample%0Afeatures%20or%20class%20proxies.%20Moreover%2C%20we%20integrate%20our%20proposed%20loss%20with%0Apair-based%20and%20proxy-based%20methods%2C%20resulting%20in%20notable%20performance%0Aimprovement.%20Comprehensive%20experiments%20on%20benchmark%20datasets%20demonstrate%20that%0Aour%20proposed%20method%20outperforms%20existing%20state-of-the-art%20methods.%20Extensive%0Aablation%20studies%20verify%20the%20effectiveness%20of%20our%20method%20in%20preventing%20embedding%0Aspace%20collapse%20and%20promoting%20generalization%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03106v1&entry.124074799=Read"},
{"title": "Estimating Treatment Effects under Recommender Interference: A\n  Structured Neural Networks Approach", "author": "Ruohan Zhan and Shichao Han and Yuchen Hu and Zhenling Jiang", "abstract": "  Recommender systems are essential for content-sharing platforms by curating\npersonalized content. To evaluate updates to recommender systems targeting\ncontent creators, platforms frequently rely on creator-side randomized\nexperiments. The treatment effect measures the change in outcomes when a new\nalgorithm is implemented compared to the status quo. We show that the standard\ndifference-in-means estimator can lead to biased estimates due to recommender\ninterference that arises when treated and control creators compete for\nexposure. We propose a \"recommender choice model\" that describes which item\ngets exposed from a pool containing both treated and control items. By\ncombining a structural choice model with neural networks, this framework\ndirectly models the interference pathway while accounting for rich\nviewer-content heterogeneity. We construct a debiased estimator of the\ntreatment effect and prove it is $\\sqrt n$-consistent and asymptotically normal\nwith potentially correlated samples. We validate our estimator's empirical\nperformance with a field experiment on Weixin short-video platform. In addition\nto the standard creator-side experiment, we conduct a costly double-sided\nrandomization design to obtain a benchmark estimate free from interference\nbias. We show that the proposed estimator yields results comparable to the\nbenchmark, whereas the standard difference-in-means estimator can exhibit\nsignificant bias and even produce reversed signs.\n", "link": "http://arxiv.org/abs/2406.14380v2", "date": "2024-07-03", "relevancy": 1.915, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5127}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4657}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.45}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Estimating%20Treatment%20Effects%20under%20Recommender%20Interference%3A%20A%0A%20%20Structured%20Neural%20Networks%20Approach&body=Title%3A%20Estimating%20Treatment%20Effects%20under%20Recommender%20Interference%3A%20A%0A%20%20Structured%20Neural%20Networks%20Approach%0AAuthor%3A%20Ruohan%20Zhan%20and%20Shichao%20Han%20and%20Yuchen%20Hu%20and%20Zhenling%20Jiang%0AAbstract%3A%20%20%20Recommender%20systems%20are%20essential%20for%20content-sharing%20platforms%20by%20curating%0Apersonalized%20content.%20To%20evaluate%20updates%20to%20recommender%20systems%20targeting%0Acontent%20creators%2C%20platforms%20frequently%20rely%20on%20creator-side%20randomized%0Aexperiments.%20The%20treatment%20effect%20measures%20the%20change%20in%20outcomes%20when%20a%20new%0Aalgorithm%20is%20implemented%20compared%20to%20the%20status%20quo.%20We%20show%20that%20the%20standard%0Adifference-in-means%20estimator%20can%20lead%20to%20biased%20estimates%20due%20to%20recommender%0Ainterference%20that%20arises%20when%20treated%20and%20control%20creators%20compete%20for%0Aexposure.%20We%20propose%20a%20%22recommender%20choice%20model%22%20that%20describes%20which%20item%0Agets%20exposed%20from%20a%20pool%20containing%20both%20treated%20and%20control%20items.%20By%0Acombining%20a%20structural%20choice%20model%20with%20neural%20networks%2C%20this%20framework%0Adirectly%20models%20the%20interference%20pathway%20while%20accounting%20for%20rich%0Aviewer-content%20heterogeneity.%20We%20construct%20a%20debiased%20estimator%20of%20the%0Atreatment%20effect%20and%20prove%20it%20is%20%24%5Csqrt%20n%24-consistent%20and%20asymptotically%20normal%0Awith%20potentially%20correlated%20samples.%20We%20validate%20our%20estimator%27s%20empirical%0Aperformance%20with%20a%20field%20experiment%20on%20Weixin%20short-video%20platform.%20In%20addition%0Ato%20the%20standard%20creator-side%20experiment%2C%20we%20conduct%20a%20costly%20double-sided%0Arandomization%20design%20to%20obtain%20a%20benchmark%20estimate%20free%20from%20interference%0Abias.%20We%20show%20that%20the%20proposed%20estimator%20yields%20results%20comparable%20to%20the%0Abenchmark%2C%20whereas%20the%20standard%20difference-in-means%20estimator%20can%20exhibit%0Asignificant%20bias%20and%20even%20produce%20reversed%20signs.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.14380v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEstimating%2520Treatment%2520Effects%2520under%2520Recommender%2520Interference%253A%2520A%250A%2520%2520Structured%2520Neural%2520Networks%2520Approach%26entry.906535625%3DRuohan%2520Zhan%2520and%2520Shichao%2520Han%2520and%2520Yuchen%2520Hu%2520and%2520Zhenling%2520Jiang%26entry.1292438233%3D%2520%2520Recommender%2520systems%2520are%2520essential%2520for%2520content-sharing%2520platforms%2520by%2520curating%250Apersonalized%2520content.%2520To%2520evaluate%2520updates%2520to%2520recommender%2520systems%2520targeting%250Acontent%2520creators%252C%2520platforms%2520frequently%2520rely%2520on%2520creator-side%2520randomized%250Aexperiments.%2520The%2520treatment%2520effect%2520measures%2520the%2520change%2520in%2520outcomes%2520when%2520a%2520new%250Aalgorithm%2520is%2520implemented%2520compared%2520to%2520the%2520status%2520quo.%2520We%2520show%2520that%2520the%2520standard%250Adifference-in-means%2520estimator%2520can%2520lead%2520to%2520biased%2520estimates%2520due%2520to%2520recommender%250Ainterference%2520that%2520arises%2520when%2520treated%2520and%2520control%2520creators%2520compete%2520for%250Aexposure.%2520We%2520propose%2520a%2520%2522recommender%2520choice%2520model%2522%2520that%2520describes%2520which%2520item%250Agets%2520exposed%2520from%2520a%2520pool%2520containing%2520both%2520treated%2520and%2520control%2520items.%2520By%250Acombining%2520a%2520structural%2520choice%2520model%2520with%2520neural%2520networks%252C%2520this%2520framework%250Adirectly%2520models%2520the%2520interference%2520pathway%2520while%2520accounting%2520for%2520rich%250Aviewer-content%2520heterogeneity.%2520We%2520construct%2520a%2520debiased%2520estimator%2520of%2520the%250Atreatment%2520effect%2520and%2520prove%2520it%2520is%2520%2524%255Csqrt%2520n%2524-consistent%2520and%2520asymptotically%2520normal%250Awith%2520potentially%2520correlated%2520samples.%2520We%2520validate%2520our%2520estimator%2527s%2520empirical%250Aperformance%2520with%2520a%2520field%2520experiment%2520on%2520Weixin%2520short-video%2520platform.%2520In%2520addition%250Ato%2520the%2520standard%2520creator-side%2520experiment%252C%2520we%2520conduct%2520a%2520costly%2520double-sided%250Arandomization%2520design%2520to%2520obtain%2520a%2520benchmark%2520estimate%2520free%2520from%2520interference%250Abias.%2520We%2520show%2520that%2520the%2520proposed%2520estimator%2520yields%2520results%2520comparable%2520to%2520the%250Abenchmark%252C%2520whereas%2520the%2520standard%2520difference-in-means%2520estimator%2520can%2520exhibit%250Asignificant%2520bias%2520and%2520even%2520produce%2520reversed%2520signs.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.14380v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Estimating%20Treatment%20Effects%20under%20Recommender%20Interference%3A%20A%0A%20%20Structured%20Neural%20Networks%20Approach&entry.906535625=Ruohan%20Zhan%20and%20Shichao%20Han%20and%20Yuchen%20Hu%20and%20Zhenling%20Jiang&entry.1292438233=%20%20Recommender%20systems%20are%20essential%20for%20content-sharing%20platforms%20by%20curating%0Apersonalized%20content.%20To%20evaluate%20updates%20to%20recommender%20systems%20targeting%0Acontent%20creators%2C%20platforms%20frequently%20rely%20on%20creator-side%20randomized%0Aexperiments.%20The%20treatment%20effect%20measures%20the%20change%20in%20outcomes%20when%20a%20new%0Aalgorithm%20is%20implemented%20compared%20to%20the%20status%20quo.%20We%20show%20that%20the%20standard%0Adifference-in-means%20estimator%20can%20lead%20to%20biased%20estimates%20due%20to%20recommender%0Ainterference%20that%20arises%20when%20treated%20and%20control%20creators%20compete%20for%0Aexposure.%20We%20propose%20a%20%22recommender%20choice%20model%22%20that%20describes%20which%20item%0Agets%20exposed%20from%20a%20pool%20containing%20both%20treated%20and%20control%20items.%20By%0Acombining%20a%20structural%20choice%20model%20with%20neural%20networks%2C%20this%20framework%0Adirectly%20models%20the%20interference%20pathway%20while%20accounting%20for%20rich%0Aviewer-content%20heterogeneity.%20We%20construct%20a%20debiased%20estimator%20of%20the%0Atreatment%20effect%20and%20prove%20it%20is%20%24%5Csqrt%20n%24-consistent%20and%20asymptotically%20normal%0Awith%20potentially%20correlated%20samples.%20We%20validate%20our%20estimator%27s%20empirical%0Aperformance%20with%20a%20field%20experiment%20on%20Weixin%20short-video%20platform.%20In%20addition%0Ato%20the%20standard%20creator-side%20experiment%2C%20we%20conduct%20a%20costly%20double-sided%0Arandomization%20design%20to%20obtain%20a%20benchmark%20estimate%20free%20from%20interference%0Abias.%20We%20show%20that%20the%20proposed%20estimator%20yields%20results%20comparable%20to%20the%0Abenchmark%2C%20whereas%20the%20standard%20difference-in-means%20estimator%20can%20exhibit%0Asignificant%20bias%20and%20even%20produce%20reversed%20signs.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.14380v2&entry.124074799=Read"},
{"title": "Self-Evaluation as a Defense Against Adversarial Attacks on LLMs", "author": "Hannah Brown and Leon Lin and Kenji Kawaguchi and Michael Shieh", "abstract": "  When LLMs are deployed in sensitive, human-facing settings, it is crucial\nthat they do not output unsafe, biased, or privacy-violating outputs. For this\nreason, models are both trained and instructed to refuse to answer unsafe\nprompts such as \"Tell me how to build a bomb.\" We find that, despite these\nsafeguards, it is possible to break model defenses simply by appending a space\nto the end of a model's input. In a study of eight open-source models, we\ndemonstrate that this acts as a strong enough attack to cause the majority of\nmodels to generate harmful outputs with very high success rates. We examine the\ncauses of this behavior, finding that the contexts in which single spaces occur\nin tokenized training data encourage models to generate lists when prompted,\noverriding training signals to refuse to answer unsafe requests. Our findings\nunderscore the fragile state of current model alignment and promote the\nimportance of developing more robust alignment methods. Code and data will be\nmade available at https://github.com/Linlt-leon/Adversarial-Alignments.\n", "link": "http://arxiv.org/abs/2407.03234v1", "date": "2024-07-03", "relevancy": 1.9128, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4866}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4855}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4669}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Evaluation%20as%20a%20Defense%20Against%20Adversarial%20Attacks%20on%20LLMs&body=Title%3A%20Self-Evaluation%20as%20a%20Defense%20Against%20Adversarial%20Attacks%20on%20LLMs%0AAuthor%3A%20Hannah%20Brown%20and%20Leon%20Lin%20and%20Kenji%20Kawaguchi%20and%20Michael%20Shieh%0AAbstract%3A%20%20%20When%20LLMs%20are%20deployed%20in%20sensitive%2C%20human-facing%20settings%2C%20it%20is%20crucial%0Athat%20they%20do%20not%20output%20unsafe%2C%20biased%2C%20or%20privacy-violating%20outputs.%20For%20this%0Areason%2C%20models%20are%20both%20trained%20and%20instructed%20to%20refuse%20to%20answer%20unsafe%0Aprompts%20such%20as%20%22Tell%20me%20how%20to%20build%20a%20bomb.%22%20We%20find%20that%2C%20despite%20these%0Asafeguards%2C%20it%20is%20possible%20to%20break%20model%20defenses%20simply%20by%20appending%20a%20space%0Ato%20the%20end%20of%20a%20model%27s%20input.%20In%20a%20study%20of%20eight%20open-source%20models%2C%20we%0Ademonstrate%20that%20this%20acts%20as%20a%20strong%20enough%20attack%20to%20cause%20the%20majority%20of%0Amodels%20to%20generate%20harmful%20outputs%20with%20very%20high%20success%20rates.%20We%20examine%20the%0Acauses%20of%20this%20behavior%2C%20finding%20that%20the%20contexts%20in%20which%20single%20spaces%20occur%0Ain%20tokenized%20training%20data%20encourage%20models%20to%20generate%20lists%20when%20prompted%2C%0Aoverriding%20training%20signals%20to%20refuse%20to%20answer%20unsafe%20requests.%20Our%20findings%0Aunderscore%20the%20fragile%20state%20of%20current%20model%20alignment%20and%20promote%20the%0Aimportance%20of%20developing%20more%20robust%20alignment%20methods.%20Code%20and%20data%20will%20be%0Amade%20available%20at%20https%3A//github.com/Linlt-leon/Adversarial-Alignments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03234v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Evaluation%2520as%2520a%2520Defense%2520Against%2520Adversarial%2520Attacks%2520on%2520LLMs%26entry.906535625%3DHannah%2520Brown%2520and%2520Leon%2520Lin%2520and%2520Kenji%2520Kawaguchi%2520and%2520Michael%2520Shieh%26entry.1292438233%3D%2520%2520When%2520LLMs%2520are%2520deployed%2520in%2520sensitive%252C%2520human-facing%2520settings%252C%2520it%2520is%2520crucial%250Athat%2520they%2520do%2520not%2520output%2520unsafe%252C%2520biased%252C%2520or%2520privacy-violating%2520outputs.%2520For%2520this%250Areason%252C%2520models%2520are%2520both%2520trained%2520and%2520instructed%2520to%2520refuse%2520to%2520answer%2520unsafe%250Aprompts%2520such%2520as%2520%2522Tell%2520me%2520how%2520to%2520build%2520a%2520bomb.%2522%2520We%2520find%2520that%252C%2520despite%2520these%250Asafeguards%252C%2520it%2520is%2520possible%2520to%2520break%2520model%2520defenses%2520simply%2520by%2520appending%2520a%2520space%250Ato%2520the%2520end%2520of%2520a%2520model%2527s%2520input.%2520In%2520a%2520study%2520of%2520eight%2520open-source%2520models%252C%2520we%250Ademonstrate%2520that%2520this%2520acts%2520as%2520a%2520strong%2520enough%2520attack%2520to%2520cause%2520the%2520majority%2520of%250Amodels%2520to%2520generate%2520harmful%2520outputs%2520with%2520very%2520high%2520success%2520rates.%2520We%2520examine%2520the%250Acauses%2520of%2520this%2520behavior%252C%2520finding%2520that%2520the%2520contexts%2520in%2520which%2520single%2520spaces%2520occur%250Ain%2520tokenized%2520training%2520data%2520encourage%2520models%2520to%2520generate%2520lists%2520when%2520prompted%252C%250Aoverriding%2520training%2520signals%2520to%2520refuse%2520to%2520answer%2520unsafe%2520requests.%2520Our%2520findings%250Aunderscore%2520the%2520fragile%2520state%2520of%2520current%2520model%2520alignment%2520and%2520promote%2520the%250Aimportance%2520of%2520developing%2520more%2520robust%2520alignment%2520methods.%2520Code%2520and%2520data%2520will%2520be%250Amade%2520available%2520at%2520https%253A//github.com/Linlt-leon/Adversarial-Alignments.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03234v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Evaluation%20as%20a%20Defense%20Against%20Adversarial%20Attacks%20on%20LLMs&entry.906535625=Hannah%20Brown%20and%20Leon%20Lin%20and%20Kenji%20Kawaguchi%20and%20Michael%20Shieh&entry.1292438233=%20%20When%20LLMs%20are%20deployed%20in%20sensitive%2C%20human-facing%20settings%2C%20it%20is%20crucial%0Athat%20they%20do%20not%20output%20unsafe%2C%20biased%2C%20or%20privacy-violating%20outputs.%20For%20this%0Areason%2C%20models%20are%20both%20trained%20and%20instructed%20to%20refuse%20to%20answer%20unsafe%0Aprompts%20such%20as%20%22Tell%20me%20how%20to%20build%20a%20bomb.%22%20We%20find%20that%2C%20despite%20these%0Asafeguards%2C%20it%20is%20possible%20to%20break%20model%20defenses%20simply%20by%20appending%20a%20space%0Ato%20the%20end%20of%20a%20model%27s%20input.%20In%20a%20study%20of%20eight%20open-source%20models%2C%20we%0Ademonstrate%20that%20this%20acts%20as%20a%20strong%20enough%20attack%20to%20cause%20the%20majority%20of%0Amodels%20to%20generate%20harmful%20outputs%20with%20very%20high%20success%20rates.%20We%20examine%20the%0Acauses%20of%20this%20behavior%2C%20finding%20that%20the%20contexts%20in%20which%20single%20spaces%20occur%0Ain%20tokenized%20training%20data%20encourage%20models%20to%20generate%20lists%20when%20prompted%2C%0Aoverriding%20training%20signals%20to%20refuse%20to%20answer%20unsafe%20requests.%20Our%20findings%0Aunderscore%20the%20fragile%20state%20of%20current%20model%20alignment%20and%20promote%20the%0Aimportance%20of%20developing%20more%20robust%20alignment%20methods.%20Code%20and%20data%20will%20be%0Amade%20available%20at%20https%3A//github.com/Linlt-leon/Adversarial-Alignments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03234v1&entry.124074799=Read"},
{"title": "Modern Neighborhood Components Analysis: A Deep Tabular Baseline Two\n  Decades Later", "author": "Han-Jia Ye and Huai-Hong Yin and De-Chuan Zhan", "abstract": "  The growing success of deep learning in various domains has prompted\ninvestigations into its application to tabular data, where deep models have\nshown promising results compared to traditional tree-based methods. In this\npaper, we revisit Neighborhood Component Analysis (NCA), a classic tabular\nprediction method introduced in 2004, designed to learn a linear projection\nthat captures semantic similarities between instances. We find that minor\nmodifications, such as adjustments to the learning objectives and the\nintegration of deep learning architectures, significantly enhance NCA's\nperformance, enabling it to surpass most modern deep tabular models.\nAdditionally, we introduce a stochastic neighbor sampling strategy that\nimproves both the efficiency and predictive accuracy of our proposed ModernNCA\n-- sampling only a subset of neighbors during training, while utilizing the\nentire neighborhood during inference. Extensive experiments demonstrate that\nour ModernNCA achieves state-of-the-art results in both classification and\nregression tasks across various tabular datasets, outperforming both tree-based\nand other deep tabular models, while also reducing training time and model\nsize.\n", "link": "http://arxiv.org/abs/2407.03257v1", "date": "2024-07-03", "relevancy": 1.9098, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4857}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4761}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4755}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modern%20Neighborhood%20Components%20Analysis%3A%20A%20Deep%20Tabular%20Baseline%20Two%0A%20%20Decades%20Later&body=Title%3A%20Modern%20Neighborhood%20Components%20Analysis%3A%20A%20Deep%20Tabular%20Baseline%20Two%0A%20%20Decades%20Later%0AAuthor%3A%20Han-Jia%20Ye%20and%20Huai-Hong%20Yin%20and%20De-Chuan%20Zhan%0AAbstract%3A%20%20%20The%20growing%20success%20of%20deep%20learning%20in%20various%20domains%20has%20prompted%0Ainvestigations%20into%20its%20application%20to%20tabular%20data%2C%20where%20deep%20models%20have%0Ashown%20promising%20results%20compared%20to%20traditional%20tree-based%20methods.%20In%20this%0Apaper%2C%20we%20revisit%20Neighborhood%20Component%20Analysis%20%28NCA%29%2C%20a%20classic%20tabular%0Aprediction%20method%20introduced%20in%202004%2C%20designed%20to%20learn%20a%20linear%20projection%0Athat%20captures%20semantic%20similarities%20between%20instances.%20We%20find%20that%20minor%0Amodifications%2C%20such%20as%20adjustments%20to%20the%20learning%20objectives%20and%20the%0Aintegration%20of%20deep%20learning%20architectures%2C%20significantly%20enhance%20NCA%27s%0Aperformance%2C%20enabling%20it%20to%20surpass%20most%20modern%20deep%20tabular%20models.%0AAdditionally%2C%20we%20introduce%20a%20stochastic%20neighbor%20sampling%20strategy%20that%0Aimproves%20both%20the%20efficiency%20and%20predictive%20accuracy%20of%20our%20proposed%20ModernNCA%0A--%20sampling%20only%20a%20subset%20of%20neighbors%20during%20training%2C%20while%20utilizing%20the%0Aentire%20neighborhood%20during%20inference.%20Extensive%20experiments%20demonstrate%20that%0Aour%20ModernNCA%20achieves%20state-of-the-art%20results%20in%20both%20classification%20and%0Aregression%20tasks%20across%20various%20tabular%20datasets%2C%20outperforming%20both%20tree-based%0Aand%20other%20deep%20tabular%20models%2C%20while%20also%20reducing%20training%20time%20and%20model%0Asize.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03257v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModern%2520Neighborhood%2520Components%2520Analysis%253A%2520A%2520Deep%2520Tabular%2520Baseline%2520Two%250A%2520%2520Decades%2520Later%26entry.906535625%3DHan-Jia%2520Ye%2520and%2520Huai-Hong%2520Yin%2520and%2520De-Chuan%2520Zhan%26entry.1292438233%3D%2520%2520The%2520growing%2520success%2520of%2520deep%2520learning%2520in%2520various%2520domains%2520has%2520prompted%250Ainvestigations%2520into%2520its%2520application%2520to%2520tabular%2520data%252C%2520where%2520deep%2520models%2520have%250Ashown%2520promising%2520results%2520compared%2520to%2520traditional%2520tree-based%2520methods.%2520In%2520this%250Apaper%252C%2520we%2520revisit%2520Neighborhood%2520Component%2520Analysis%2520%2528NCA%2529%252C%2520a%2520classic%2520tabular%250Aprediction%2520method%2520introduced%2520in%25202004%252C%2520designed%2520to%2520learn%2520a%2520linear%2520projection%250Athat%2520captures%2520semantic%2520similarities%2520between%2520instances.%2520We%2520find%2520that%2520minor%250Amodifications%252C%2520such%2520as%2520adjustments%2520to%2520the%2520learning%2520objectives%2520and%2520the%250Aintegration%2520of%2520deep%2520learning%2520architectures%252C%2520significantly%2520enhance%2520NCA%2527s%250Aperformance%252C%2520enabling%2520it%2520to%2520surpass%2520most%2520modern%2520deep%2520tabular%2520models.%250AAdditionally%252C%2520we%2520introduce%2520a%2520stochastic%2520neighbor%2520sampling%2520strategy%2520that%250Aimproves%2520both%2520the%2520efficiency%2520and%2520predictive%2520accuracy%2520of%2520our%2520proposed%2520ModernNCA%250A--%2520sampling%2520only%2520a%2520subset%2520of%2520neighbors%2520during%2520training%252C%2520while%2520utilizing%2520the%250Aentire%2520neighborhood%2520during%2520inference.%2520Extensive%2520experiments%2520demonstrate%2520that%250Aour%2520ModernNCA%2520achieves%2520state-of-the-art%2520results%2520in%2520both%2520classification%2520and%250Aregression%2520tasks%2520across%2520various%2520tabular%2520datasets%252C%2520outperforming%2520both%2520tree-based%250Aand%2520other%2520deep%2520tabular%2520models%252C%2520while%2520also%2520reducing%2520training%2520time%2520and%2520model%250Asize.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03257v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modern%20Neighborhood%20Components%20Analysis%3A%20A%20Deep%20Tabular%20Baseline%20Two%0A%20%20Decades%20Later&entry.906535625=Han-Jia%20Ye%20and%20Huai-Hong%20Yin%20and%20De-Chuan%20Zhan&entry.1292438233=%20%20The%20growing%20success%20of%20deep%20learning%20in%20various%20domains%20has%20prompted%0Ainvestigations%20into%20its%20application%20to%20tabular%20data%2C%20where%20deep%20models%20have%0Ashown%20promising%20results%20compared%20to%20traditional%20tree-based%20methods.%20In%20this%0Apaper%2C%20we%20revisit%20Neighborhood%20Component%20Analysis%20%28NCA%29%2C%20a%20classic%20tabular%0Aprediction%20method%20introduced%20in%202004%2C%20designed%20to%20learn%20a%20linear%20projection%0Athat%20captures%20semantic%20similarities%20between%20instances.%20We%20find%20that%20minor%0Amodifications%2C%20such%20as%20adjustments%20to%20the%20learning%20objectives%20and%20the%0Aintegration%20of%20deep%20learning%20architectures%2C%20significantly%20enhance%20NCA%27s%0Aperformance%2C%20enabling%20it%20to%20surpass%20most%20modern%20deep%20tabular%20models.%0AAdditionally%2C%20we%20introduce%20a%20stochastic%20neighbor%20sampling%20strategy%20that%0Aimproves%20both%20the%20efficiency%20and%20predictive%20accuracy%20of%20our%20proposed%20ModernNCA%0A--%20sampling%20only%20a%20subset%20of%20neighbors%20during%20training%2C%20while%20utilizing%20the%0Aentire%20neighborhood%20during%20inference.%20Extensive%20experiments%20demonstrate%20that%0Aour%20ModernNCA%20achieves%20state-of-the-art%20results%20in%20both%20classification%20and%0Aregression%20tasks%20across%20various%20tabular%20datasets%2C%20outperforming%20both%20tree-based%0Aand%20other%20deep%20tabular%20models%2C%20while%20also%20reducing%20training%20time%20and%20model%0Asize.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03257v1&entry.124074799=Read"},
{"title": "Position and Altitude of the Nao Camera Head from Two Points on the\n  Soccer Field plus the Gravitational Direction", "author": "Stijn Oomes and Arnoud Visser", "abstract": "  To be able to play soccer, a robot needs a good estimate of its current\nposition on the field. Ideally, multiple features are visible that have known\nlocations. By applying trigonometry we can estimate the viewpoint from where\nthis observation was actually made. Given that the Nao robots of the Standard\nPlatform League have quite a limited field of view, a given camera frame\ntypically only allows for one or two points to be recognized.\n  In this paper we propose a method for determining the (x, y) coordinates on\nthe field and the height h of the camera from the geometry of a simplified\ntetrahedron. This configuration is formed by two observed points on the ground\nplane plus the gravitational direction. When the distance between the two\npoints is known, and the directions to the points plus the gravitational\ndirection are measured, all dimensions of the tetrahedron can be determined.\n  By performing these calculations with rational trigonometry instead of\nclassical trigonometry, the computations turn out to be 28.7% faster, with\nequal numerical accuracy. The position of the head of the Nao can also be\nexternally measured with the OptiTrack system. The difference between\nexternally measured and internally predicted position from sensor data gives us\nmean absolute errors in the 3-6 centimeters range, when we estimated the\ngravitational direction from the vanishing point of the outer edges of the goal\nposts.\n", "link": "http://arxiv.org/abs/2407.03041v1", "date": "2024-07-03", "relevancy": 1.9032, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4783}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4772}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4727}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Position%20and%20Altitude%20of%20the%20Nao%20Camera%20Head%20from%20Two%20Points%20on%20the%0A%20%20Soccer%20Field%20plus%20the%20Gravitational%20Direction&body=Title%3A%20Position%20and%20Altitude%20of%20the%20Nao%20Camera%20Head%20from%20Two%20Points%20on%20the%0A%20%20Soccer%20Field%20plus%20the%20Gravitational%20Direction%0AAuthor%3A%20Stijn%20Oomes%20and%20Arnoud%20Visser%0AAbstract%3A%20%20%20To%20be%20able%20to%20play%20soccer%2C%20a%20robot%20needs%20a%20good%20estimate%20of%20its%20current%0Aposition%20on%20the%20field.%20Ideally%2C%20multiple%20features%20are%20visible%20that%20have%20known%0Alocations.%20By%20applying%20trigonometry%20we%20can%20estimate%20the%20viewpoint%20from%20where%0Athis%20observation%20was%20actually%20made.%20Given%20that%20the%20Nao%20robots%20of%20the%20Standard%0APlatform%20League%20have%20quite%20a%20limited%20field%20of%20view%2C%20a%20given%20camera%20frame%0Atypically%20only%20allows%20for%20one%20or%20two%20points%20to%20be%20recognized.%0A%20%20In%20this%20paper%20we%20propose%20a%20method%20for%20determining%20the%20%28x%2C%20y%29%20coordinates%20on%0Athe%20field%20and%20the%20height%20h%20of%20the%20camera%20from%20the%20geometry%20of%20a%20simplified%0Atetrahedron.%20This%20configuration%20is%20formed%20by%20two%20observed%20points%20on%20the%20ground%0Aplane%20plus%20the%20gravitational%20direction.%20When%20the%20distance%20between%20the%20two%0Apoints%20is%20known%2C%20and%20the%20directions%20to%20the%20points%20plus%20the%20gravitational%0Adirection%20are%20measured%2C%20all%20dimensions%20of%20the%20tetrahedron%20can%20be%20determined.%0A%20%20By%20performing%20these%20calculations%20with%20rational%20trigonometry%20instead%20of%0Aclassical%20trigonometry%2C%20the%20computations%20turn%20out%20to%20be%2028.7%25%20faster%2C%20with%0Aequal%20numerical%20accuracy.%20The%20position%20of%20the%20head%20of%20the%20Nao%20can%20also%20be%0Aexternally%20measured%20with%20the%20OptiTrack%20system.%20The%20difference%20between%0Aexternally%20measured%20and%20internally%20predicted%20position%20from%20sensor%20data%20gives%20us%0Amean%20absolute%20errors%20in%20the%203-6%20centimeters%20range%2C%20when%20we%20estimated%20the%0Agravitational%20direction%20from%20the%20vanishing%20point%20of%20the%20outer%20edges%20of%20the%20goal%0Aposts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03041v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPosition%2520and%2520Altitude%2520of%2520the%2520Nao%2520Camera%2520Head%2520from%2520Two%2520Points%2520on%2520the%250A%2520%2520Soccer%2520Field%2520plus%2520the%2520Gravitational%2520Direction%26entry.906535625%3DStijn%2520Oomes%2520and%2520Arnoud%2520Visser%26entry.1292438233%3D%2520%2520To%2520be%2520able%2520to%2520play%2520soccer%252C%2520a%2520robot%2520needs%2520a%2520good%2520estimate%2520of%2520its%2520current%250Aposition%2520on%2520the%2520field.%2520Ideally%252C%2520multiple%2520features%2520are%2520visible%2520that%2520have%2520known%250Alocations.%2520By%2520applying%2520trigonometry%2520we%2520can%2520estimate%2520the%2520viewpoint%2520from%2520where%250Athis%2520observation%2520was%2520actually%2520made.%2520Given%2520that%2520the%2520Nao%2520robots%2520of%2520the%2520Standard%250APlatform%2520League%2520have%2520quite%2520a%2520limited%2520field%2520of%2520view%252C%2520a%2520given%2520camera%2520frame%250Atypically%2520only%2520allows%2520for%2520one%2520or%2520two%2520points%2520to%2520be%2520recognized.%250A%2520%2520In%2520this%2520paper%2520we%2520propose%2520a%2520method%2520for%2520determining%2520the%2520%2528x%252C%2520y%2529%2520coordinates%2520on%250Athe%2520field%2520and%2520the%2520height%2520h%2520of%2520the%2520camera%2520from%2520the%2520geometry%2520of%2520a%2520simplified%250Atetrahedron.%2520This%2520configuration%2520is%2520formed%2520by%2520two%2520observed%2520points%2520on%2520the%2520ground%250Aplane%2520plus%2520the%2520gravitational%2520direction.%2520When%2520the%2520distance%2520between%2520the%2520two%250Apoints%2520is%2520known%252C%2520and%2520the%2520directions%2520to%2520the%2520points%2520plus%2520the%2520gravitational%250Adirection%2520are%2520measured%252C%2520all%2520dimensions%2520of%2520the%2520tetrahedron%2520can%2520be%2520determined.%250A%2520%2520By%2520performing%2520these%2520calculations%2520with%2520rational%2520trigonometry%2520instead%2520of%250Aclassical%2520trigonometry%252C%2520the%2520computations%2520turn%2520out%2520to%2520be%252028.7%2525%2520faster%252C%2520with%250Aequal%2520numerical%2520accuracy.%2520The%2520position%2520of%2520the%2520head%2520of%2520the%2520Nao%2520can%2520also%2520be%250Aexternally%2520measured%2520with%2520the%2520OptiTrack%2520system.%2520The%2520difference%2520between%250Aexternally%2520measured%2520and%2520internally%2520predicted%2520position%2520from%2520sensor%2520data%2520gives%2520us%250Amean%2520absolute%2520errors%2520in%2520the%25203-6%2520centimeters%2520range%252C%2520when%2520we%2520estimated%2520the%250Agravitational%2520direction%2520from%2520the%2520vanishing%2520point%2520of%2520the%2520outer%2520edges%2520of%2520the%2520goal%250Aposts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03041v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Position%20and%20Altitude%20of%20the%20Nao%20Camera%20Head%20from%20Two%20Points%20on%20the%0A%20%20Soccer%20Field%20plus%20the%20Gravitational%20Direction&entry.906535625=Stijn%20Oomes%20and%20Arnoud%20Visser&entry.1292438233=%20%20To%20be%20able%20to%20play%20soccer%2C%20a%20robot%20needs%20a%20good%20estimate%20of%20its%20current%0Aposition%20on%20the%20field.%20Ideally%2C%20multiple%20features%20are%20visible%20that%20have%20known%0Alocations.%20By%20applying%20trigonometry%20we%20can%20estimate%20the%20viewpoint%20from%20where%0Athis%20observation%20was%20actually%20made.%20Given%20that%20the%20Nao%20robots%20of%20the%20Standard%0APlatform%20League%20have%20quite%20a%20limited%20field%20of%20view%2C%20a%20given%20camera%20frame%0Atypically%20only%20allows%20for%20one%20or%20two%20points%20to%20be%20recognized.%0A%20%20In%20this%20paper%20we%20propose%20a%20method%20for%20determining%20the%20%28x%2C%20y%29%20coordinates%20on%0Athe%20field%20and%20the%20height%20h%20of%20the%20camera%20from%20the%20geometry%20of%20a%20simplified%0Atetrahedron.%20This%20configuration%20is%20formed%20by%20two%20observed%20points%20on%20the%20ground%0Aplane%20plus%20the%20gravitational%20direction.%20When%20the%20distance%20between%20the%20two%0Apoints%20is%20known%2C%20and%20the%20directions%20to%20the%20points%20plus%20the%20gravitational%0Adirection%20are%20measured%2C%20all%20dimensions%20of%20the%20tetrahedron%20can%20be%20determined.%0A%20%20By%20performing%20these%20calculations%20with%20rational%20trigonometry%20instead%20of%0Aclassical%20trigonometry%2C%20the%20computations%20turn%20out%20to%20be%2028.7%25%20faster%2C%20with%0Aequal%20numerical%20accuracy.%20The%20position%20of%20the%20head%20of%20the%20Nao%20can%20also%20be%0Aexternally%20measured%20with%20the%20OptiTrack%20system.%20The%20difference%20between%0Aexternally%20measured%20and%20internally%20predicted%20position%20from%20sensor%20data%20gives%20us%0Amean%20absolute%20errors%20in%20the%203-6%20centimeters%20range%2C%20when%20we%20estimated%20the%0Agravitational%20direction%20from%20the%20vanishing%20point%20of%20the%20outer%20edges%20of%20the%20goal%0Aposts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03041v1&entry.124074799=Read"},
{"title": "Smaug: Fixing Failure Modes of Preference Optimisation with DPO-Positive", "author": "Arka Pal and Deep Karkhanis and Samuel Dooley and Manley Roberts and Siddartha Naidu and Colin White", "abstract": "  Direct Preference Optimisation (DPO) is effective at significantly improving\nthe performance of large language models (LLMs) on downstream tasks such as\nreasoning, summarisation, and alignment. Using pairs of preferred and\ndispreferred data, DPO models the relative probability of picking one response\nover another. In this work, first we show theoretically that the standard DPO\nloss can lead to a reduction of the model's likelihood of the preferred\nexamples, as long as the relative probability between the preferred and\ndispreferred classes increases. We then show empirically that this phenomenon\noccurs when fine-tuning LLMs on common datasets, especially datasets in which\nthe edit distance between pairs of completions is low. Using these insights, we\ndesign DPO-Positive (DPOP), a new loss function and training procedure which\navoids this failure mode. Surprisingly, we find that DPOP outperforms DPO and\nother fine-tuning procedures across a wide variety of datasets and downstream\ntasks, including datasets with high edit distances between completions.\nFurthermore, we find that the DPOP-tuned model outperforms the DPO-tuned model\n(all else equal) on benchmarks independent of the fine-tuning data, such as\nMT-Bench. Finally, using DPOP, we create and open-source Smaug-34B and\nSmaug-72B, with the latter becoming the first open-source LLM to surpass an\naverage accuracy of 80% on the HuggingFace Open LLM Leaderboard.\n", "link": "http://arxiv.org/abs/2402.13228v2", "date": "2024-07-03", "relevancy": 1.9027, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4804}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4795}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4694}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Smaug%3A%20Fixing%20Failure%20Modes%20of%20Preference%20Optimisation%20with%20DPO-Positive&body=Title%3A%20Smaug%3A%20Fixing%20Failure%20Modes%20of%20Preference%20Optimisation%20with%20DPO-Positive%0AAuthor%3A%20Arka%20Pal%20and%20Deep%20Karkhanis%20and%20Samuel%20Dooley%20and%20Manley%20Roberts%20and%20Siddartha%20Naidu%20and%20Colin%20White%0AAbstract%3A%20%20%20Direct%20Preference%20Optimisation%20%28DPO%29%20is%20effective%20at%20significantly%20improving%0Athe%20performance%20of%20large%20language%20models%20%28LLMs%29%20on%20downstream%20tasks%20such%20as%0Areasoning%2C%20summarisation%2C%20and%20alignment.%20Using%20pairs%20of%20preferred%20and%0Adispreferred%20data%2C%20DPO%20models%20the%20relative%20probability%20of%20picking%20one%20response%0Aover%20another.%20In%20this%20work%2C%20first%20we%20show%20theoretically%20that%20the%20standard%20DPO%0Aloss%20can%20lead%20to%20a%20reduction%20of%20the%20model%27s%20likelihood%20of%20the%20preferred%0Aexamples%2C%20as%20long%20as%20the%20relative%20probability%20between%20the%20preferred%20and%0Adispreferred%20classes%20increases.%20We%20then%20show%20empirically%20that%20this%20phenomenon%0Aoccurs%20when%20fine-tuning%20LLMs%20on%20common%20datasets%2C%20especially%20datasets%20in%20which%0Athe%20edit%20distance%20between%20pairs%20of%20completions%20is%20low.%20Using%20these%20insights%2C%20we%0Adesign%20DPO-Positive%20%28DPOP%29%2C%20a%20new%20loss%20function%20and%20training%20procedure%20which%0Aavoids%20this%20failure%20mode.%20Surprisingly%2C%20we%20find%20that%20DPOP%20outperforms%20DPO%20and%0Aother%20fine-tuning%20procedures%20across%20a%20wide%20variety%20of%20datasets%20and%20downstream%0Atasks%2C%20including%20datasets%20with%20high%20edit%20distances%20between%20completions.%0AFurthermore%2C%20we%20find%20that%20the%20DPOP-tuned%20model%20outperforms%20the%20DPO-tuned%20model%0A%28all%20else%20equal%29%20on%20benchmarks%20independent%20of%20the%20fine-tuning%20data%2C%20such%20as%0AMT-Bench.%20Finally%2C%20using%20DPOP%2C%20we%20create%20and%20open-source%20Smaug-34B%20and%0ASmaug-72B%2C%20with%20the%20latter%20becoming%20the%20first%20open-source%20LLM%20to%20surpass%20an%0Aaverage%20accuracy%20of%2080%25%20on%20the%20HuggingFace%20Open%20LLM%20Leaderboard.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.13228v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmaug%253A%2520Fixing%2520Failure%2520Modes%2520of%2520Preference%2520Optimisation%2520with%2520DPO-Positive%26entry.906535625%3DArka%2520Pal%2520and%2520Deep%2520Karkhanis%2520and%2520Samuel%2520Dooley%2520and%2520Manley%2520Roberts%2520and%2520Siddartha%2520Naidu%2520and%2520Colin%2520White%26entry.1292438233%3D%2520%2520Direct%2520Preference%2520Optimisation%2520%2528DPO%2529%2520is%2520effective%2520at%2520significantly%2520improving%250Athe%2520performance%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520on%2520downstream%2520tasks%2520such%2520as%250Areasoning%252C%2520summarisation%252C%2520and%2520alignment.%2520Using%2520pairs%2520of%2520preferred%2520and%250Adispreferred%2520data%252C%2520DPO%2520models%2520the%2520relative%2520probability%2520of%2520picking%2520one%2520response%250Aover%2520another.%2520In%2520this%2520work%252C%2520first%2520we%2520show%2520theoretically%2520that%2520the%2520standard%2520DPO%250Aloss%2520can%2520lead%2520to%2520a%2520reduction%2520of%2520the%2520model%2527s%2520likelihood%2520of%2520the%2520preferred%250Aexamples%252C%2520as%2520long%2520as%2520the%2520relative%2520probability%2520between%2520the%2520preferred%2520and%250Adispreferred%2520classes%2520increases.%2520We%2520then%2520show%2520empirically%2520that%2520this%2520phenomenon%250Aoccurs%2520when%2520fine-tuning%2520LLMs%2520on%2520common%2520datasets%252C%2520especially%2520datasets%2520in%2520which%250Athe%2520edit%2520distance%2520between%2520pairs%2520of%2520completions%2520is%2520low.%2520Using%2520these%2520insights%252C%2520we%250Adesign%2520DPO-Positive%2520%2528DPOP%2529%252C%2520a%2520new%2520loss%2520function%2520and%2520training%2520procedure%2520which%250Aavoids%2520this%2520failure%2520mode.%2520Surprisingly%252C%2520we%2520find%2520that%2520DPOP%2520outperforms%2520DPO%2520and%250Aother%2520fine-tuning%2520procedures%2520across%2520a%2520wide%2520variety%2520of%2520datasets%2520and%2520downstream%250Atasks%252C%2520including%2520datasets%2520with%2520high%2520edit%2520distances%2520between%2520completions.%250AFurthermore%252C%2520we%2520find%2520that%2520the%2520DPOP-tuned%2520model%2520outperforms%2520the%2520DPO-tuned%2520model%250A%2528all%2520else%2520equal%2529%2520on%2520benchmarks%2520independent%2520of%2520the%2520fine-tuning%2520data%252C%2520such%2520as%250AMT-Bench.%2520Finally%252C%2520using%2520DPOP%252C%2520we%2520create%2520and%2520open-source%2520Smaug-34B%2520and%250ASmaug-72B%252C%2520with%2520the%2520latter%2520becoming%2520the%2520first%2520open-source%2520LLM%2520to%2520surpass%2520an%250Aaverage%2520accuracy%2520of%252080%2525%2520on%2520the%2520HuggingFace%2520Open%2520LLM%2520Leaderboard.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.13228v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Smaug%3A%20Fixing%20Failure%20Modes%20of%20Preference%20Optimisation%20with%20DPO-Positive&entry.906535625=Arka%20Pal%20and%20Deep%20Karkhanis%20and%20Samuel%20Dooley%20and%20Manley%20Roberts%20and%20Siddartha%20Naidu%20and%20Colin%20White&entry.1292438233=%20%20Direct%20Preference%20Optimisation%20%28DPO%29%20is%20effective%20at%20significantly%20improving%0Athe%20performance%20of%20large%20language%20models%20%28LLMs%29%20on%20downstream%20tasks%20such%20as%0Areasoning%2C%20summarisation%2C%20and%20alignment.%20Using%20pairs%20of%20preferred%20and%0Adispreferred%20data%2C%20DPO%20models%20the%20relative%20probability%20of%20picking%20one%20response%0Aover%20another.%20In%20this%20work%2C%20first%20we%20show%20theoretically%20that%20the%20standard%20DPO%0Aloss%20can%20lead%20to%20a%20reduction%20of%20the%20model%27s%20likelihood%20of%20the%20preferred%0Aexamples%2C%20as%20long%20as%20the%20relative%20probability%20between%20the%20preferred%20and%0Adispreferred%20classes%20increases.%20We%20then%20show%20empirically%20that%20this%20phenomenon%0Aoccurs%20when%20fine-tuning%20LLMs%20on%20common%20datasets%2C%20especially%20datasets%20in%20which%0Athe%20edit%20distance%20between%20pairs%20of%20completions%20is%20low.%20Using%20these%20insights%2C%20we%0Adesign%20DPO-Positive%20%28DPOP%29%2C%20a%20new%20loss%20function%20and%20training%20procedure%20which%0Aavoids%20this%20failure%20mode.%20Surprisingly%2C%20we%20find%20that%20DPOP%20outperforms%20DPO%20and%0Aother%20fine-tuning%20procedures%20across%20a%20wide%20variety%20of%20datasets%20and%20downstream%0Atasks%2C%20including%20datasets%20with%20high%20edit%20distances%20between%20completions.%0AFurthermore%2C%20we%20find%20that%20the%20DPOP-tuned%20model%20outperforms%20the%20DPO-tuned%20model%0A%28all%20else%20equal%29%20on%20benchmarks%20independent%20of%20the%20fine-tuning%20data%2C%20such%20as%0AMT-Bench.%20Finally%2C%20using%20DPOP%2C%20we%20create%20and%20open-source%20Smaug-34B%20and%0ASmaug-72B%2C%20with%20the%20latter%20becoming%20the%20first%20open-source%20LLM%20to%20surpass%20an%0Aaverage%20accuracy%20of%2080%25%20on%20the%20HuggingFace%20Open%20LLM%20Leaderboard.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.13228v2&entry.124074799=Read"},
{"title": "Adam-mini: Use Fewer Learning Rates To Gain More", "author": "Yushun Zhang and Congliang Chen and Ziniu Li and Tian Ding and Chenwei Wu and Yinyu Ye and Zhi-Quan Luo and Ruoyu Sun", "abstract": "  We propose Adam-mini, an optimizer that achieves on-par or better performance\nthan AdamW with 45% to 50% less memory footprint. Adam-mini reduces memory by\ncutting down the learning rate resources in Adam (i.e., $1/\\sqrt{v}$). We find\nthat $\\geq$ 90% of these learning rates in $v$ could be harmlessly removed if\nwe (1) carefully partition the parameters into blocks following our proposed\nprinciple on Hessian structure; (2) assign a single but good learning rate to\neach parameter block. We further find that, for each of these parameter blocks,\nthere exists a single high-quality learning rate that can outperform Adam,\nprovided that sufficient resources are available to search it out. We then\nprovide one cost-effective way to find good learning rates and propose\nAdam-mini. Empirically, we verify that Adam-mini performs on par or better than\nAdamW on various language models sized from 125M to 7B for pre-training,\nsupervised fine-tuning, and RLHF. The reduced memory footprint of Adam-mini\nalso alleviates communication overheads among GPUs and CPUs, thereby increasing\nthroughput. For instance, Adam-mini achieves 49.6% higher throughput than AdamW\nwhen pre-training Llama2-7B on $2\\times$ A800-80GB GPUs, which saves 33%\nwall-clock time for pre-training.\n", "link": "http://arxiv.org/abs/2406.16793v5", "date": "2024-07-03", "relevancy": 1.9, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.499}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4962}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4442}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adam-mini%3A%20Use%20Fewer%20Learning%20Rates%20To%20Gain%20More&body=Title%3A%20Adam-mini%3A%20Use%20Fewer%20Learning%20Rates%20To%20Gain%20More%0AAuthor%3A%20Yushun%20Zhang%20and%20Congliang%20Chen%20and%20Ziniu%20Li%20and%20Tian%20Ding%20and%20Chenwei%20Wu%20and%20Yinyu%20Ye%20and%20Zhi-Quan%20Luo%20and%20Ruoyu%20Sun%0AAbstract%3A%20%20%20We%20propose%20Adam-mini%2C%20an%20optimizer%20that%20achieves%20on-par%20or%20better%20performance%0Athan%20AdamW%20with%2045%25%20to%2050%25%20less%20memory%20footprint.%20Adam-mini%20reduces%20memory%20by%0Acutting%20down%20the%20learning%20rate%20resources%20in%20Adam%20%28i.e.%2C%20%241/%5Csqrt%7Bv%7D%24%29.%20We%20find%0Athat%20%24%5Cgeq%24%2090%25%20of%20these%20learning%20rates%20in%20%24v%24%20could%20be%20harmlessly%20removed%20if%0Awe%20%281%29%20carefully%20partition%20the%20parameters%20into%20blocks%20following%20our%20proposed%0Aprinciple%20on%20Hessian%20structure%3B%20%282%29%20assign%20a%20single%20but%20good%20learning%20rate%20to%0Aeach%20parameter%20block.%20We%20further%20find%20that%2C%20for%20each%20of%20these%20parameter%20blocks%2C%0Athere%20exists%20a%20single%20high-quality%20learning%20rate%20that%20can%20outperform%20Adam%2C%0Aprovided%20that%20sufficient%20resources%20are%20available%20to%20search%20it%20out.%20We%20then%0Aprovide%20one%20cost-effective%20way%20to%20find%20good%20learning%20rates%20and%20propose%0AAdam-mini.%20Empirically%2C%20we%20verify%20that%20Adam-mini%20performs%20on%20par%20or%20better%20than%0AAdamW%20on%20various%20language%20models%20sized%20from%20125M%20to%207B%20for%20pre-training%2C%0Asupervised%20fine-tuning%2C%20and%20RLHF.%20The%20reduced%20memory%20footprint%20of%20Adam-mini%0Aalso%20alleviates%20communication%20overheads%20among%20GPUs%20and%20CPUs%2C%20thereby%20increasing%0Athroughput.%20For%20instance%2C%20Adam-mini%20achieves%2049.6%25%20higher%20throughput%20than%20AdamW%0Awhen%20pre-training%20Llama2-7B%20on%20%242%5Ctimes%24%20A800-80GB%20GPUs%2C%20which%20saves%2033%25%0Awall-clock%20time%20for%20pre-training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16793v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdam-mini%253A%2520Use%2520Fewer%2520Learning%2520Rates%2520To%2520Gain%2520More%26entry.906535625%3DYushun%2520Zhang%2520and%2520Congliang%2520Chen%2520and%2520Ziniu%2520Li%2520and%2520Tian%2520Ding%2520and%2520Chenwei%2520Wu%2520and%2520Yinyu%2520Ye%2520and%2520Zhi-Quan%2520Luo%2520and%2520Ruoyu%2520Sun%26entry.1292438233%3D%2520%2520We%2520propose%2520Adam-mini%252C%2520an%2520optimizer%2520that%2520achieves%2520on-par%2520or%2520better%2520performance%250Athan%2520AdamW%2520with%252045%2525%2520to%252050%2525%2520less%2520memory%2520footprint.%2520Adam-mini%2520reduces%2520memory%2520by%250Acutting%2520down%2520the%2520learning%2520rate%2520resources%2520in%2520Adam%2520%2528i.e.%252C%2520%25241/%255Csqrt%257Bv%257D%2524%2529.%2520We%2520find%250Athat%2520%2524%255Cgeq%2524%252090%2525%2520of%2520these%2520learning%2520rates%2520in%2520%2524v%2524%2520could%2520be%2520harmlessly%2520removed%2520if%250Awe%2520%25281%2529%2520carefully%2520partition%2520the%2520parameters%2520into%2520blocks%2520following%2520our%2520proposed%250Aprinciple%2520on%2520Hessian%2520structure%253B%2520%25282%2529%2520assign%2520a%2520single%2520but%2520good%2520learning%2520rate%2520to%250Aeach%2520parameter%2520block.%2520We%2520further%2520find%2520that%252C%2520for%2520each%2520of%2520these%2520parameter%2520blocks%252C%250Athere%2520exists%2520a%2520single%2520high-quality%2520learning%2520rate%2520that%2520can%2520outperform%2520Adam%252C%250Aprovided%2520that%2520sufficient%2520resources%2520are%2520available%2520to%2520search%2520it%2520out.%2520We%2520then%250Aprovide%2520one%2520cost-effective%2520way%2520to%2520find%2520good%2520learning%2520rates%2520and%2520propose%250AAdam-mini.%2520Empirically%252C%2520we%2520verify%2520that%2520Adam-mini%2520performs%2520on%2520par%2520or%2520better%2520than%250AAdamW%2520on%2520various%2520language%2520models%2520sized%2520from%2520125M%2520to%25207B%2520for%2520pre-training%252C%250Asupervised%2520fine-tuning%252C%2520and%2520RLHF.%2520The%2520reduced%2520memory%2520footprint%2520of%2520Adam-mini%250Aalso%2520alleviates%2520communication%2520overheads%2520among%2520GPUs%2520and%2520CPUs%252C%2520thereby%2520increasing%250Athroughput.%2520For%2520instance%252C%2520Adam-mini%2520achieves%252049.6%2525%2520higher%2520throughput%2520than%2520AdamW%250Awhen%2520pre-training%2520Llama2-7B%2520on%2520%25242%255Ctimes%2524%2520A800-80GB%2520GPUs%252C%2520which%2520saves%252033%2525%250Awall-clock%2520time%2520for%2520pre-training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16793v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adam-mini%3A%20Use%20Fewer%20Learning%20Rates%20To%20Gain%20More&entry.906535625=Yushun%20Zhang%20and%20Congliang%20Chen%20and%20Ziniu%20Li%20and%20Tian%20Ding%20and%20Chenwei%20Wu%20and%20Yinyu%20Ye%20and%20Zhi-Quan%20Luo%20and%20Ruoyu%20Sun&entry.1292438233=%20%20We%20propose%20Adam-mini%2C%20an%20optimizer%20that%20achieves%20on-par%20or%20better%20performance%0Athan%20AdamW%20with%2045%25%20to%2050%25%20less%20memory%20footprint.%20Adam-mini%20reduces%20memory%20by%0Acutting%20down%20the%20learning%20rate%20resources%20in%20Adam%20%28i.e.%2C%20%241/%5Csqrt%7Bv%7D%24%29.%20We%20find%0Athat%20%24%5Cgeq%24%2090%25%20of%20these%20learning%20rates%20in%20%24v%24%20could%20be%20harmlessly%20removed%20if%0Awe%20%281%29%20carefully%20partition%20the%20parameters%20into%20blocks%20following%20our%20proposed%0Aprinciple%20on%20Hessian%20structure%3B%20%282%29%20assign%20a%20single%20but%20good%20learning%20rate%20to%0Aeach%20parameter%20block.%20We%20further%20find%20that%2C%20for%20each%20of%20these%20parameter%20blocks%2C%0Athere%20exists%20a%20single%20high-quality%20learning%20rate%20that%20can%20outperform%20Adam%2C%0Aprovided%20that%20sufficient%20resources%20are%20available%20to%20search%20it%20out.%20We%20then%0Aprovide%20one%20cost-effective%20way%20to%20find%20good%20learning%20rates%20and%20propose%0AAdam-mini.%20Empirically%2C%20we%20verify%20that%20Adam-mini%20performs%20on%20par%20or%20better%20than%0AAdamW%20on%20various%20language%20models%20sized%20from%20125M%20to%207B%20for%20pre-training%2C%0Asupervised%20fine-tuning%2C%20and%20RLHF.%20The%20reduced%20memory%20footprint%20of%20Adam-mini%0Aalso%20alleviates%20communication%20overheads%20among%20GPUs%20and%20CPUs%2C%20thereby%20increasing%0Athroughput.%20For%20instance%2C%20Adam-mini%20achieves%2049.6%25%20higher%20throughput%20than%20AdamW%0Awhen%20pre-training%20Llama2-7B%20on%20%242%5Ctimes%24%20A800-80GB%20GPUs%2C%20which%20saves%2033%25%0Awall-clock%20time%20for%20pre-training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16793v5&entry.124074799=Read"},
{"title": "Raw Text is All you Need: Knowledge-intensive Multi-turn Instruction\n  Tuning for Large Language Model", "author": "Xia Hou and Qifeng Li and Jian Yang and Tongliang Li and Linzheng Chai and Xianjie Wu and Hangyuan Ji and Zhoujun Li and Jixuan Nie and Jingbo Dun and Wenfeng Song", "abstract": "  Instruction tuning as an effective technique aligns the outputs of large\nlanguage models (LLMs) with human preference. But how to generate the seasonal\nmulti-turn dialogues from raw documents for instruction tuning still requires\nfurther exploration. In this paper, we present a novel framework named R2S that\nleverages the CoD-Chain of Dialogue logic to guide large language models (LLMs)\nin generating knowledge-intensive multi-turn dialogues for instruction tuning.\nBy integrating raw documents from both open-source datasets and domain-specific\nweb-crawled documents into a benchmark K-BENCH, we cover diverse areas such as\nWikipedia (English), Science (Chinese), and Artifacts (Chinese). Our approach\nfirst decides the logic flow of the current dialogue and then prompts LLMs to\nproduce key phrases for sourcing relevant response content. This methodology\nenables the creation of the G I NSTRUCT instruction dataset, retaining raw\ndocument knowledge within dialoguestyle interactions. Utilizing this dataset,\nwe fine-tune GLLM, a model designed to transform raw documents into structured\nmulti-turn dialogues, thereby injecting comprehensive domain knowledge into the\nSFT model for enhanced instruction tuning. This work signifies a stride towards\nrefining the adaptability and effectiveness of LLMs in processing and\ngenerating more accurate, contextually nuanced responses across various fields.\n", "link": "http://arxiv.org/abs/2407.03040v1", "date": "2024-07-03", "relevancy": 1.8965, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4845}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.477}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4671}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Raw%20Text%20is%20All%20you%20Need%3A%20Knowledge-intensive%20Multi-turn%20Instruction%0A%20%20Tuning%20for%20Large%20Language%20Model&body=Title%3A%20Raw%20Text%20is%20All%20you%20Need%3A%20Knowledge-intensive%20Multi-turn%20Instruction%0A%20%20Tuning%20for%20Large%20Language%20Model%0AAuthor%3A%20Xia%20Hou%20and%20Qifeng%20Li%20and%20Jian%20Yang%20and%20Tongliang%20Li%20and%20Linzheng%20Chai%20and%20Xianjie%20Wu%20and%20Hangyuan%20Ji%20and%20Zhoujun%20Li%20and%20Jixuan%20Nie%20and%20Jingbo%20Dun%20and%20Wenfeng%20Song%0AAbstract%3A%20%20%20Instruction%20tuning%20as%20an%20effective%20technique%20aligns%20the%20outputs%20of%20large%0Alanguage%20models%20%28LLMs%29%20with%20human%20preference.%20But%20how%20to%20generate%20the%20seasonal%0Amulti-turn%20dialogues%20from%20raw%20documents%20for%20instruction%20tuning%20still%20requires%0Afurther%20exploration.%20In%20this%20paper%2C%20we%20present%20a%20novel%20framework%20named%20R2S%20that%0Aleverages%20the%20CoD-Chain%20of%20Dialogue%20logic%20to%20guide%20large%20language%20models%20%28LLMs%29%0Ain%20generating%20knowledge-intensive%20multi-turn%20dialogues%20for%20instruction%20tuning.%0ABy%20integrating%20raw%20documents%20from%20both%20open-source%20datasets%20and%20domain-specific%0Aweb-crawled%20documents%20into%20a%20benchmark%20K-BENCH%2C%20we%20cover%20diverse%20areas%20such%20as%0AWikipedia%20%28English%29%2C%20Science%20%28Chinese%29%2C%20and%20Artifacts%20%28Chinese%29.%20Our%20approach%0Afirst%20decides%20the%20logic%20flow%20of%20the%20current%20dialogue%20and%20then%20prompts%20LLMs%20to%0Aproduce%20key%20phrases%20for%20sourcing%20relevant%20response%20content.%20This%20methodology%0Aenables%20the%20creation%20of%20the%20G%20I%20NSTRUCT%20instruction%20dataset%2C%20retaining%20raw%0Adocument%20knowledge%20within%20dialoguestyle%20interactions.%20Utilizing%20this%20dataset%2C%0Awe%20fine-tune%20GLLM%2C%20a%20model%20designed%20to%20transform%20raw%20documents%20into%20structured%0Amulti-turn%20dialogues%2C%20thereby%20injecting%20comprehensive%20domain%20knowledge%20into%20the%0ASFT%20model%20for%20enhanced%20instruction%20tuning.%20This%20work%20signifies%20a%20stride%20towards%0Arefining%20the%20adaptability%20and%20effectiveness%20of%20LLMs%20in%20processing%20and%0Agenerating%20more%20accurate%2C%20contextually%20nuanced%20responses%20across%20various%20fields.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03040v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRaw%2520Text%2520is%2520All%2520you%2520Need%253A%2520Knowledge-intensive%2520Multi-turn%2520Instruction%250A%2520%2520Tuning%2520for%2520Large%2520Language%2520Model%26entry.906535625%3DXia%2520Hou%2520and%2520Qifeng%2520Li%2520and%2520Jian%2520Yang%2520and%2520Tongliang%2520Li%2520and%2520Linzheng%2520Chai%2520and%2520Xianjie%2520Wu%2520and%2520Hangyuan%2520Ji%2520and%2520Zhoujun%2520Li%2520and%2520Jixuan%2520Nie%2520and%2520Jingbo%2520Dun%2520and%2520Wenfeng%2520Song%26entry.1292438233%3D%2520%2520Instruction%2520tuning%2520as%2520an%2520effective%2520technique%2520aligns%2520the%2520outputs%2520of%2520large%250Alanguage%2520models%2520%2528LLMs%2529%2520with%2520human%2520preference.%2520But%2520how%2520to%2520generate%2520the%2520seasonal%250Amulti-turn%2520dialogues%2520from%2520raw%2520documents%2520for%2520instruction%2520tuning%2520still%2520requires%250Afurther%2520exploration.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520novel%2520framework%2520named%2520R2S%2520that%250Aleverages%2520the%2520CoD-Chain%2520of%2520Dialogue%2520logic%2520to%2520guide%2520large%2520language%2520models%2520%2528LLMs%2529%250Ain%2520generating%2520knowledge-intensive%2520multi-turn%2520dialogues%2520for%2520instruction%2520tuning.%250ABy%2520integrating%2520raw%2520documents%2520from%2520both%2520open-source%2520datasets%2520and%2520domain-specific%250Aweb-crawled%2520documents%2520into%2520a%2520benchmark%2520K-BENCH%252C%2520we%2520cover%2520diverse%2520areas%2520such%2520as%250AWikipedia%2520%2528English%2529%252C%2520Science%2520%2528Chinese%2529%252C%2520and%2520Artifacts%2520%2528Chinese%2529.%2520Our%2520approach%250Afirst%2520decides%2520the%2520logic%2520flow%2520of%2520the%2520current%2520dialogue%2520and%2520then%2520prompts%2520LLMs%2520to%250Aproduce%2520key%2520phrases%2520for%2520sourcing%2520relevant%2520response%2520content.%2520This%2520methodology%250Aenables%2520the%2520creation%2520of%2520the%2520G%2520I%2520NSTRUCT%2520instruction%2520dataset%252C%2520retaining%2520raw%250Adocument%2520knowledge%2520within%2520dialoguestyle%2520interactions.%2520Utilizing%2520this%2520dataset%252C%250Awe%2520fine-tune%2520GLLM%252C%2520a%2520model%2520designed%2520to%2520transform%2520raw%2520documents%2520into%2520structured%250Amulti-turn%2520dialogues%252C%2520thereby%2520injecting%2520comprehensive%2520domain%2520knowledge%2520into%2520the%250ASFT%2520model%2520for%2520enhanced%2520instruction%2520tuning.%2520This%2520work%2520signifies%2520a%2520stride%2520towards%250Arefining%2520the%2520adaptability%2520and%2520effectiveness%2520of%2520LLMs%2520in%2520processing%2520and%250Agenerating%2520more%2520accurate%252C%2520contextually%2520nuanced%2520responses%2520across%2520various%2520fields.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03040v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Raw%20Text%20is%20All%20you%20Need%3A%20Knowledge-intensive%20Multi-turn%20Instruction%0A%20%20Tuning%20for%20Large%20Language%20Model&entry.906535625=Xia%20Hou%20and%20Qifeng%20Li%20and%20Jian%20Yang%20and%20Tongliang%20Li%20and%20Linzheng%20Chai%20and%20Xianjie%20Wu%20and%20Hangyuan%20Ji%20and%20Zhoujun%20Li%20and%20Jixuan%20Nie%20and%20Jingbo%20Dun%20and%20Wenfeng%20Song&entry.1292438233=%20%20Instruction%20tuning%20as%20an%20effective%20technique%20aligns%20the%20outputs%20of%20large%0Alanguage%20models%20%28LLMs%29%20with%20human%20preference.%20But%20how%20to%20generate%20the%20seasonal%0Amulti-turn%20dialogues%20from%20raw%20documents%20for%20instruction%20tuning%20still%20requires%0Afurther%20exploration.%20In%20this%20paper%2C%20we%20present%20a%20novel%20framework%20named%20R2S%20that%0Aleverages%20the%20CoD-Chain%20of%20Dialogue%20logic%20to%20guide%20large%20language%20models%20%28LLMs%29%0Ain%20generating%20knowledge-intensive%20multi-turn%20dialogues%20for%20instruction%20tuning.%0ABy%20integrating%20raw%20documents%20from%20both%20open-source%20datasets%20and%20domain-specific%0Aweb-crawled%20documents%20into%20a%20benchmark%20K-BENCH%2C%20we%20cover%20diverse%20areas%20such%20as%0AWikipedia%20%28English%29%2C%20Science%20%28Chinese%29%2C%20and%20Artifacts%20%28Chinese%29.%20Our%20approach%0Afirst%20decides%20the%20logic%20flow%20of%20the%20current%20dialogue%20and%20then%20prompts%20LLMs%20to%0Aproduce%20key%20phrases%20for%20sourcing%20relevant%20response%20content.%20This%20methodology%0Aenables%20the%20creation%20of%20the%20G%20I%20NSTRUCT%20instruction%20dataset%2C%20retaining%20raw%0Adocument%20knowledge%20within%20dialoguestyle%20interactions.%20Utilizing%20this%20dataset%2C%0Awe%20fine-tune%20GLLM%2C%20a%20model%20designed%20to%20transform%20raw%20documents%20into%20structured%0Amulti-turn%20dialogues%2C%20thereby%20injecting%20comprehensive%20domain%20knowledge%20into%20the%0ASFT%20model%20for%20enhanced%20instruction%20tuning.%20This%20work%20signifies%20a%20stride%20towards%0Arefining%20the%20adaptability%20and%20effectiveness%20of%20LLMs%20in%20processing%20and%0Agenerating%20more%20accurate%2C%20contextually%20nuanced%20responses%20across%20various%20fields.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03040v1&entry.124074799=Read"},
{"title": "Explainable AI for Comparative Analysis of Intrusion Detection Models", "author": "Pap M. Corea and Yongxin Liu and Jian Wang and Shuteng Niu and Houbing Song", "abstract": "  Explainable Artificial Intelligence (XAI) has become a widely discussed\ntopic, the related technologies facilitate better understanding of conventional\nblack-box models like Random Forest, Neural Networks and etc. However,\ndomain-specific applications of XAI are still insufficient. To fill this gap,\nthis research analyzes various machine learning models to the tasks of binary\nand multi-class classification for intrusion detection from network traffic on\nthe same dataset using occlusion sensitivity. The models evaluated include\nLinear Regression, Logistic Regression, Linear Support Vector Machine (SVM),\nK-Nearest Neighbors (KNN), Random Forest, Decision Trees, and Multi-Layer\nPerceptrons (MLP). We trained all models to the accuracy of 90\\% on the\nUNSW-NB15 Dataset. We found that most classifiers leverage only less than three\ncritical features to achieve such accuracies, indicating that effective feature\nengineering could actually be far more important for intrusion detection than\napplying complicated models. We also discover that Random Forest provides the\nbest performance in terms of accuracy, time efficiency and robustness. Data and\ncode available at https://github.com/pcwhy/XML-IntrusionDetection.git\n", "link": "http://arxiv.org/abs/2406.09684v2", "date": "2024-07-03", "relevancy": 1.8964, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4828}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.471}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explainable%20AI%20for%20Comparative%20Analysis%20of%20Intrusion%20Detection%20Models&body=Title%3A%20Explainable%20AI%20for%20Comparative%20Analysis%20of%20Intrusion%20Detection%20Models%0AAuthor%3A%20Pap%20M.%20Corea%20and%20Yongxin%20Liu%20and%20Jian%20Wang%20and%20Shuteng%20Niu%20and%20Houbing%20Song%0AAbstract%3A%20%20%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20has%20become%20a%20widely%20discussed%0Atopic%2C%20the%20related%20technologies%20facilitate%20better%20understanding%20of%20conventional%0Ablack-box%20models%20like%20Random%20Forest%2C%20Neural%20Networks%20and%20etc.%20However%2C%0Adomain-specific%20applications%20of%20XAI%20are%20still%20insufficient.%20To%20fill%20this%20gap%2C%0Athis%20research%20analyzes%20various%20machine%20learning%20models%20to%20the%20tasks%20of%20binary%0Aand%20multi-class%20classification%20for%20intrusion%20detection%20from%20network%20traffic%20on%0Athe%20same%20dataset%20using%20occlusion%20sensitivity.%20The%20models%20evaluated%20include%0ALinear%20Regression%2C%20Logistic%20Regression%2C%20Linear%20Support%20Vector%20Machine%20%28SVM%29%2C%0AK-Nearest%20Neighbors%20%28KNN%29%2C%20Random%20Forest%2C%20Decision%20Trees%2C%20and%20Multi-Layer%0APerceptrons%20%28MLP%29.%20We%20trained%20all%20models%20to%20the%20accuracy%20of%2090%5C%25%20on%20the%0AUNSW-NB15%20Dataset.%20We%20found%20that%20most%20classifiers%20leverage%20only%20less%20than%20three%0Acritical%20features%20to%20achieve%20such%20accuracies%2C%20indicating%20that%20effective%20feature%0Aengineering%20could%20actually%20be%20far%20more%20important%20for%20intrusion%20detection%20than%0Aapplying%20complicated%20models.%20We%20also%20discover%20that%20Random%20Forest%20provides%20the%0Abest%20performance%20in%20terms%20of%20accuracy%2C%20time%20efficiency%20and%20robustness.%20Data%20and%0Acode%20available%20at%20https%3A//github.com/pcwhy/XML-IntrusionDetection.git%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.09684v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplainable%2520AI%2520for%2520Comparative%2520Analysis%2520of%2520Intrusion%2520Detection%2520Models%26entry.906535625%3DPap%2520M.%2520Corea%2520and%2520Yongxin%2520Liu%2520and%2520Jian%2520Wang%2520and%2520Shuteng%2520Niu%2520and%2520Houbing%2520Song%26entry.1292438233%3D%2520%2520Explainable%2520Artificial%2520Intelligence%2520%2528XAI%2529%2520has%2520become%2520a%2520widely%2520discussed%250Atopic%252C%2520the%2520related%2520technologies%2520facilitate%2520better%2520understanding%2520of%2520conventional%250Ablack-box%2520models%2520like%2520Random%2520Forest%252C%2520Neural%2520Networks%2520and%2520etc.%2520However%252C%250Adomain-specific%2520applications%2520of%2520XAI%2520are%2520still%2520insufficient.%2520To%2520fill%2520this%2520gap%252C%250Athis%2520research%2520analyzes%2520various%2520machine%2520learning%2520models%2520to%2520the%2520tasks%2520of%2520binary%250Aand%2520multi-class%2520classification%2520for%2520intrusion%2520detection%2520from%2520network%2520traffic%2520on%250Athe%2520same%2520dataset%2520using%2520occlusion%2520sensitivity.%2520The%2520models%2520evaluated%2520include%250ALinear%2520Regression%252C%2520Logistic%2520Regression%252C%2520Linear%2520Support%2520Vector%2520Machine%2520%2528SVM%2529%252C%250AK-Nearest%2520Neighbors%2520%2528KNN%2529%252C%2520Random%2520Forest%252C%2520Decision%2520Trees%252C%2520and%2520Multi-Layer%250APerceptrons%2520%2528MLP%2529.%2520We%2520trained%2520all%2520models%2520to%2520the%2520accuracy%2520of%252090%255C%2525%2520on%2520the%250AUNSW-NB15%2520Dataset.%2520We%2520found%2520that%2520most%2520classifiers%2520leverage%2520only%2520less%2520than%2520three%250Acritical%2520features%2520to%2520achieve%2520such%2520accuracies%252C%2520indicating%2520that%2520effective%2520feature%250Aengineering%2520could%2520actually%2520be%2520far%2520more%2520important%2520for%2520intrusion%2520detection%2520than%250Aapplying%2520complicated%2520models.%2520We%2520also%2520discover%2520that%2520Random%2520Forest%2520provides%2520the%250Abest%2520performance%2520in%2520terms%2520of%2520accuracy%252C%2520time%2520efficiency%2520and%2520robustness.%2520Data%2520and%250Acode%2520available%2520at%2520https%253A//github.com/pcwhy/XML-IntrusionDetection.git%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.09684v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explainable%20AI%20for%20Comparative%20Analysis%20of%20Intrusion%20Detection%20Models&entry.906535625=Pap%20M.%20Corea%20and%20Yongxin%20Liu%20and%20Jian%20Wang%20and%20Shuteng%20Niu%20and%20Houbing%20Song&entry.1292438233=%20%20Explainable%20Artificial%20Intelligence%20%28XAI%29%20has%20become%20a%20widely%20discussed%0Atopic%2C%20the%20related%20technologies%20facilitate%20better%20understanding%20of%20conventional%0Ablack-box%20models%20like%20Random%20Forest%2C%20Neural%20Networks%20and%20etc.%20However%2C%0Adomain-specific%20applications%20of%20XAI%20are%20still%20insufficient.%20To%20fill%20this%20gap%2C%0Athis%20research%20analyzes%20various%20machine%20learning%20models%20to%20the%20tasks%20of%20binary%0Aand%20multi-class%20classification%20for%20intrusion%20detection%20from%20network%20traffic%20on%0Athe%20same%20dataset%20using%20occlusion%20sensitivity.%20The%20models%20evaluated%20include%0ALinear%20Regression%2C%20Logistic%20Regression%2C%20Linear%20Support%20Vector%20Machine%20%28SVM%29%2C%0AK-Nearest%20Neighbors%20%28KNN%29%2C%20Random%20Forest%2C%20Decision%20Trees%2C%20and%20Multi-Layer%0APerceptrons%20%28MLP%29.%20We%20trained%20all%20models%20to%20the%20accuracy%20of%2090%5C%25%20on%20the%0AUNSW-NB15%20Dataset.%20We%20found%20that%20most%20classifiers%20leverage%20only%20less%20than%20three%0Acritical%20features%20to%20achieve%20such%20accuracies%2C%20indicating%20that%20effective%20feature%0Aengineering%20could%20actually%20be%20far%20more%20important%20for%20intrusion%20detection%20than%0Aapplying%20complicated%20models.%20We%20also%20discover%20that%20Random%20Forest%20provides%20the%0Abest%20performance%20in%20terms%20of%20accuracy%2C%20time%20efficiency%20and%20robustness.%20Data%20and%0Acode%20available%20at%20https%3A//github.com/pcwhy/XML-IntrusionDetection.git%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.09684v2&entry.124074799=Read"},
{"title": "Mast Kalandar at SemEval-2024 Task 8: On the Trail of Textual Origins:\n  RoBERTa-BiLSTM Approach to Detect AI-Generated Text", "author": "Jainit Sushil Bafna and Hardik Mittal and Suyash Sethia and Manish Shrivastava and Radhika Mamidi", "abstract": "  Large Language Models (LLMs) have showcased impressive abilities in\ngenerating fluent responses to diverse user queries. However, concerns\nregarding the potential misuse of such texts in journalism, educational, and\nacademic contexts have surfaced. SemEval 2024 introduces the task of\nMultigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text\nDetection, aiming to develop automated systems for identifying\nmachine-generated text and detecting potential misuse. In this paper, we i)\npropose a RoBERTa-BiLSTM based classifier designed to classify text into two\ncategories: AI-generated or human ii) conduct a comparative study of our model\nwith baseline approaches to evaluate its effectiveness. This paper contributes\nto the advancement of automatic text detection systems in addressing the\nchallenges posed by machine-generated text misuse. Our architecture ranked 46th\non the official leaderboard with an accuracy of 80.83 among 125.\n", "link": "http://arxiv.org/abs/2407.02978v1", "date": "2024-07-03", "relevancy": 1.8934, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4894}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4622}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.461}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mast%20Kalandar%20at%20SemEval-2024%20Task%208%3A%20On%20the%20Trail%20of%20Textual%20Origins%3A%0A%20%20RoBERTa-BiLSTM%20Approach%20to%20Detect%20AI-Generated%20Text&body=Title%3A%20Mast%20Kalandar%20at%20SemEval-2024%20Task%208%3A%20On%20the%20Trail%20of%20Textual%20Origins%3A%0A%20%20RoBERTa-BiLSTM%20Approach%20to%20Detect%20AI-Generated%20Text%0AAuthor%3A%20Jainit%20Sushil%20Bafna%20and%20Hardik%20Mittal%20and%20Suyash%20Sethia%20and%20Manish%20Shrivastava%20and%20Radhika%20Mamidi%0AAbstract%3A%20%20%20Large%20Language%20Models%20%28LLMs%29%20have%20showcased%20impressive%20abilities%20in%0Agenerating%20fluent%20responses%20to%20diverse%20user%20queries.%20However%2C%20concerns%0Aregarding%20the%20potential%20misuse%20of%20such%20texts%20in%20journalism%2C%20educational%2C%20and%0Aacademic%20contexts%20have%20surfaced.%20SemEval%202024%20introduces%20the%20task%20of%0AMultigenerator%2C%20Multidomain%2C%20and%20Multilingual%20Black-Box%20Machine-Generated%20Text%0ADetection%2C%20aiming%20to%20develop%20automated%20systems%20for%20identifying%0Amachine-generated%20text%20and%20detecting%20potential%20misuse.%20In%20this%20paper%2C%20we%20i%29%0Apropose%20a%20RoBERTa-BiLSTM%20based%20classifier%20designed%20to%20classify%20text%20into%20two%0Acategories%3A%20AI-generated%20or%20human%20ii%29%20conduct%20a%20comparative%20study%20of%20our%20model%0Awith%20baseline%20approaches%20to%20evaluate%20its%20effectiveness.%20This%20paper%20contributes%0Ato%20the%20advancement%20of%20automatic%20text%20detection%20systems%20in%20addressing%20the%0Achallenges%20posed%20by%20machine-generated%20text%20misuse.%20Our%20architecture%20ranked%2046th%0Aon%20the%20official%20leaderboard%20with%20an%20accuracy%20of%2080.83%20among%20125.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02978v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMast%2520Kalandar%2520at%2520SemEval-2024%2520Task%25208%253A%2520On%2520the%2520Trail%2520of%2520Textual%2520Origins%253A%250A%2520%2520RoBERTa-BiLSTM%2520Approach%2520to%2520Detect%2520AI-Generated%2520Text%26entry.906535625%3DJainit%2520Sushil%2520Bafna%2520and%2520Hardik%2520Mittal%2520and%2520Suyash%2520Sethia%2520and%2520Manish%2520Shrivastava%2520and%2520Radhika%2520Mamidi%26entry.1292438233%3D%2520%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520have%2520showcased%2520impressive%2520abilities%2520in%250Agenerating%2520fluent%2520responses%2520to%2520diverse%2520user%2520queries.%2520However%252C%2520concerns%250Aregarding%2520the%2520potential%2520misuse%2520of%2520such%2520texts%2520in%2520journalism%252C%2520educational%252C%2520and%250Aacademic%2520contexts%2520have%2520surfaced.%2520SemEval%25202024%2520introduces%2520the%2520task%2520of%250AMultigenerator%252C%2520Multidomain%252C%2520and%2520Multilingual%2520Black-Box%2520Machine-Generated%2520Text%250ADetection%252C%2520aiming%2520to%2520develop%2520automated%2520systems%2520for%2520identifying%250Amachine-generated%2520text%2520and%2520detecting%2520potential%2520misuse.%2520In%2520this%2520paper%252C%2520we%2520i%2529%250Apropose%2520a%2520RoBERTa-BiLSTM%2520based%2520classifier%2520designed%2520to%2520classify%2520text%2520into%2520two%250Acategories%253A%2520AI-generated%2520or%2520human%2520ii%2529%2520conduct%2520a%2520comparative%2520study%2520of%2520our%2520model%250Awith%2520baseline%2520approaches%2520to%2520evaluate%2520its%2520effectiveness.%2520This%2520paper%2520contributes%250Ato%2520the%2520advancement%2520of%2520automatic%2520text%2520detection%2520systems%2520in%2520addressing%2520the%250Achallenges%2520posed%2520by%2520machine-generated%2520text%2520misuse.%2520Our%2520architecture%2520ranked%252046th%250Aon%2520the%2520official%2520leaderboard%2520with%2520an%2520accuracy%2520of%252080.83%2520among%2520125.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02978v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mast%20Kalandar%20at%20SemEval-2024%20Task%208%3A%20On%20the%20Trail%20of%20Textual%20Origins%3A%0A%20%20RoBERTa-BiLSTM%20Approach%20to%20Detect%20AI-Generated%20Text&entry.906535625=Jainit%20Sushil%20Bafna%20and%20Hardik%20Mittal%20and%20Suyash%20Sethia%20and%20Manish%20Shrivastava%20and%20Radhika%20Mamidi&entry.1292438233=%20%20Large%20Language%20Models%20%28LLMs%29%20have%20showcased%20impressive%20abilities%20in%0Agenerating%20fluent%20responses%20to%20diverse%20user%20queries.%20However%2C%20concerns%0Aregarding%20the%20potential%20misuse%20of%20such%20texts%20in%20journalism%2C%20educational%2C%20and%0Aacademic%20contexts%20have%20surfaced.%20SemEval%202024%20introduces%20the%20task%20of%0AMultigenerator%2C%20Multidomain%2C%20and%20Multilingual%20Black-Box%20Machine-Generated%20Text%0ADetection%2C%20aiming%20to%20develop%20automated%20systems%20for%20identifying%0Amachine-generated%20text%20and%20detecting%20potential%20misuse.%20In%20this%20paper%2C%20we%20i%29%0Apropose%20a%20RoBERTa-BiLSTM%20based%20classifier%20designed%20to%20classify%20text%20into%20two%0Acategories%3A%20AI-generated%20or%20human%20ii%29%20conduct%20a%20comparative%20study%20of%20our%20model%0Awith%20baseline%20approaches%20to%20evaluate%20its%20effectiveness.%20This%20paper%20contributes%0Ato%20the%20advancement%20of%20automatic%20text%20detection%20systems%20in%20addressing%20the%0Achallenges%20posed%20by%20machine-generated%20text%20misuse.%20Our%20architecture%20ranked%2046th%0Aon%20the%20official%20leaderboard%20with%20an%20accuracy%20of%2080.83%20among%20125.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02978v1&entry.124074799=Read"},
{"title": "Improved Noise Schedule for Diffusion Training", "author": "Tiankai Hang and Shuyang Gu", "abstract": "  Diffusion models have emerged as the de facto choice for generating visual\nsignals. However, training a single model to predict noise across various\nlevels poses significant challenges, necessitating numerous iterations and\nincurring significant computational costs. Various approaches, such as loss\nweighting strategy design and architectural refinements, have been introduced\nto expedite convergence. In this study, we propose a novel approach to design\nthe noise schedule for enhancing the training of diffusion models. Our key\ninsight is that the importance sampling of the logarithm of the Signal-to-Noise\nratio (logSNR), theoretically equivalent to a modified noise schedule, is\nparticularly beneficial for training efficiency when increasing the sample\nfrequency around $\\log \\text{SNR}=0$. We empirically demonstrate the\nsuperiority of our noise schedule over the standard cosine schedule.\nFurthermore, we highlight the advantages of our noise schedule design on the\nImageNet benchmark, showing that the designed schedule consistently benefits\ndifferent prediction targets.\n", "link": "http://arxiv.org/abs/2407.03297v1", "date": "2024-07-03", "relevancy": 1.8682, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6557}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.626}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.6083}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%20Noise%20Schedule%20for%20Diffusion%20Training&body=Title%3A%20Improved%20Noise%20Schedule%20for%20Diffusion%20Training%0AAuthor%3A%20Tiankai%20Hang%20and%20Shuyang%20Gu%0AAbstract%3A%20%20%20Diffusion%20models%20have%20emerged%20as%20the%20de%20facto%20choice%20for%20generating%20visual%0Asignals.%20However%2C%20training%20a%20single%20model%20to%20predict%20noise%20across%20various%0Alevels%20poses%20significant%20challenges%2C%20necessitating%20numerous%20iterations%20and%0Aincurring%20significant%20computational%20costs.%20Various%20approaches%2C%20such%20as%20loss%0Aweighting%20strategy%20design%20and%20architectural%20refinements%2C%20have%20been%20introduced%0Ato%20expedite%20convergence.%20In%20this%20study%2C%20we%20propose%20a%20novel%20approach%20to%20design%0Athe%20noise%20schedule%20for%20enhancing%20the%20training%20of%20diffusion%20models.%20Our%20key%0Ainsight%20is%20that%20the%20importance%20sampling%20of%20the%20logarithm%20of%20the%20Signal-to-Noise%0Aratio%20%28logSNR%29%2C%20theoretically%20equivalent%20to%20a%20modified%20noise%20schedule%2C%20is%0Aparticularly%20beneficial%20for%20training%20efficiency%20when%20increasing%20the%20sample%0Afrequency%20around%20%24%5Clog%20%5Ctext%7BSNR%7D%3D0%24.%20We%20empirically%20demonstrate%20the%0Asuperiority%20of%20our%20noise%20schedule%20over%20the%20standard%20cosine%20schedule.%0AFurthermore%2C%20we%20highlight%20the%20advantages%20of%20our%20noise%20schedule%20design%20on%20the%0AImageNet%20benchmark%2C%20showing%20that%20the%20designed%20schedule%20consistently%20benefits%0Adifferent%20prediction%20targets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03297v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%2520Noise%2520Schedule%2520for%2520Diffusion%2520Training%26entry.906535625%3DTiankai%2520Hang%2520and%2520Shuyang%2520Gu%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520have%2520emerged%2520as%2520the%2520de%2520facto%2520choice%2520for%2520generating%2520visual%250Asignals.%2520However%252C%2520training%2520a%2520single%2520model%2520to%2520predict%2520noise%2520across%2520various%250Alevels%2520poses%2520significant%2520challenges%252C%2520necessitating%2520numerous%2520iterations%2520and%250Aincurring%2520significant%2520computational%2520costs.%2520Various%2520approaches%252C%2520such%2520as%2520loss%250Aweighting%2520strategy%2520design%2520and%2520architectural%2520refinements%252C%2520have%2520been%2520introduced%250Ato%2520expedite%2520convergence.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%2520approach%2520to%2520design%250Athe%2520noise%2520schedule%2520for%2520enhancing%2520the%2520training%2520of%2520diffusion%2520models.%2520Our%2520key%250Ainsight%2520is%2520that%2520the%2520importance%2520sampling%2520of%2520the%2520logarithm%2520of%2520the%2520Signal-to-Noise%250Aratio%2520%2528logSNR%2529%252C%2520theoretically%2520equivalent%2520to%2520a%2520modified%2520noise%2520schedule%252C%2520is%250Aparticularly%2520beneficial%2520for%2520training%2520efficiency%2520when%2520increasing%2520the%2520sample%250Afrequency%2520around%2520%2524%255Clog%2520%255Ctext%257BSNR%257D%253D0%2524.%2520We%2520empirically%2520demonstrate%2520the%250Asuperiority%2520of%2520our%2520noise%2520schedule%2520over%2520the%2520standard%2520cosine%2520schedule.%250AFurthermore%252C%2520we%2520highlight%2520the%2520advantages%2520of%2520our%2520noise%2520schedule%2520design%2520on%2520the%250AImageNet%2520benchmark%252C%2520showing%2520that%2520the%2520designed%2520schedule%2520consistently%2520benefits%250Adifferent%2520prediction%2520targets.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03297v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%20Noise%20Schedule%20for%20Diffusion%20Training&entry.906535625=Tiankai%20Hang%20and%20Shuyang%20Gu&entry.1292438233=%20%20Diffusion%20models%20have%20emerged%20as%20the%20de%20facto%20choice%20for%20generating%20visual%0Asignals.%20However%2C%20training%20a%20single%20model%20to%20predict%20noise%20across%20various%0Alevels%20poses%20significant%20challenges%2C%20necessitating%20numerous%20iterations%20and%0Aincurring%20significant%20computational%20costs.%20Various%20approaches%2C%20such%20as%20loss%0Aweighting%20strategy%20design%20and%20architectural%20refinements%2C%20have%20been%20introduced%0Ato%20expedite%20convergence.%20In%20this%20study%2C%20we%20propose%20a%20novel%20approach%20to%20design%0Athe%20noise%20schedule%20for%20enhancing%20the%20training%20of%20diffusion%20models.%20Our%20key%0Ainsight%20is%20that%20the%20importance%20sampling%20of%20the%20logarithm%20of%20the%20Signal-to-Noise%0Aratio%20%28logSNR%29%2C%20theoretically%20equivalent%20to%20a%20modified%20noise%20schedule%2C%20is%0Aparticularly%20beneficial%20for%20training%20efficiency%20when%20increasing%20the%20sample%0Afrequency%20around%20%24%5Clog%20%5Ctext%7BSNR%7D%3D0%24.%20We%20empirically%20demonstrate%20the%0Asuperiority%20of%20our%20noise%20schedule%20over%20the%20standard%20cosine%20schedule.%0AFurthermore%2C%20we%20highlight%20the%20advantages%20of%20our%20noise%20schedule%20design%20on%20the%0AImageNet%20benchmark%2C%20showing%20that%20the%20designed%20schedule%20consistently%20benefits%0Adifferent%20prediction%20targets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03297v1&entry.124074799=Read"},
{"title": "SlerpFace: Face Template Protection via Spherical Linear Interpolation", "author": "Zhizhou Zhong and Yuxi Mi and Yuge Huang and Jianqing Xu and Guodong Mu and Shouhong Ding and Jingyun Zhang and Rizen Guo and Yunsheng Wu and Shuigeng Zhou", "abstract": "  Contemporary face recognition systems use feature templates extracted from\nface images to identify persons. To enhance privacy, face template protection\ntechniques are widely employed to conceal sensitive identity and appearance\ninformation stored in the template. This paper identifies an emerging privacy\nattack form utilizing diffusion models that could nullify prior protection,\nreferred to as inversion attacks. The attack can synthesize high-quality,\nidentity-preserving face images from templates, revealing persons' appearance.\nBased on studies of the diffusion model's generative capability, this paper\nproposes a defense to deteriorate the attack, by rotating templates to a\nnoise-like distribution. This is achieved efficiently by spherically and\nlinearly interpolating templates, or slerp, on their located hypersphere. This\npaper further proposes to group-wisely divide and drop out templates' feature\ndimensions, to enhance the irreversibility of rotated templates. The division\nof groups and dropouts within each group are learned in a recognition-favored\nway. The proposed techniques are concretized as a novel face template\nprotection technique, SlerpFace. Extensive experiments show that SlerpFace\nprovides satisfactory recognition accuracy and comprehensive privacy protection\nagainst inversion and other attack forms, superior to prior arts.\n", "link": "http://arxiv.org/abs/2407.03043v1", "date": "2024-07-03", "relevancy": 1.8657, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.4864}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4628}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.4621}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SlerpFace%3A%20Face%20Template%20Protection%20via%20Spherical%20Linear%20Interpolation&body=Title%3A%20SlerpFace%3A%20Face%20Template%20Protection%20via%20Spherical%20Linear%20Interpolation%0AAuthor%3A%20Zhizhou%20Zhong%20and%20Yuxi%20Mi%20and%20Yuge%20Huang%20and%20Jianqing%20Xu%20and%20Guodong%20Mu%20and%20Shouhong%20Ding%20and%20Jingyun%20Zhang%20and%20Rizen%20Guo%20and%20Yunsheng%20Wu%20and%20Shuigeng%20Zhou%0AAbstract%3A%20%20%20Contemporary%20face%20recognition%20systems%20use%20feature%20templates%20extracted%20from%0Aface%20images%20to%20identify%20persons.%20To%20enhance%20privacy%2C%20face%20template%20protection%0Atechniques%20are%20widely%20employed%20to%20conceal%20sensitive%20identity%20and%20appearance%0Ainformation%20stored%20in%20the%20template.%20This%20paper%20identifies%20an%20emerging%20privacy%0Aattack%20form%20utilizing%20diffusion%20models%20that%20could%20nullify%20prior%20protection%2C%0Areferred%20to%20as%20inversion%20attacks.%20The%20attack%20can%20synthesize%20high-quality%2C%0Aidentity-preserving%20face%20images%20from%20templates%2C%20revealing%20persons%27%20appearance.%0ABased%20on%20studies%20of%20the%20diffusion%20model%27s%20generative%20capability%2C%20this%20paper%0Aproposes%20a%20defense%20to%20deteriorate%20the%20attack%2C%20by%20rotating%20templates%20to%20a%0Anoise-like%20distribution.%20This%20is%20achieved%20efficiently%20by%20spherically%20and%0Alinearly%20interpolating%20templates%2C%20or%20slerp%2C%20on%20their%20located%20hypersphere.%20This%0Apaper%20further%20proposes%20to%20group-wisely%20divide%20and%20drop%20out%20templates%27%20feature%0Adimensions%2C%20to%20enhance%20the%20irreversibility%20of%20rotated%20templates.%20The%20division%0Aof%20groups%20and%20dropouts%20within%20each%20group%20are%20learned%20in%20a%20recognition-favored%0Away.%20The%20proposed%20techniques%20are%20concretized%20as%20a%20novel%20face%20template%0Aprotection%20technique%2C%20SlerpFace.%20Extensive%20experiments%20show%20that%20SlerpFace%0Aprovides%20satisfactory%20recognition%20accuracy%20and%20comprehensive%20privacy%20protection%0Aagainst%20inversion%20and%20other%20attack%20forms%2C%20superior%20to%20prior%20arts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03043v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSlerpFace%253A%2520Face%2520Template%2520Protection%2520via%2520Spherical%2520Linear%2520Interpolation%26entry.906535625%3DZhizhou%2520Zhong%2520and%2520Yuxi%2520Mi%2520and%2520Yuge%2520Huang%2520and%2520Jianqing%2520Xu%2520and%2520Guodong%2520Mu%2520and%2520Shouhong%2520Ding%2520and%2520Jingyun%2520Zhang%2520and%2520Rizen%2520Guo%2520and%2520Yunsheng%2520Wu%2520and%2520Shuigeng%2520Zhou%26entry.1292438233%3D%2520%2520Contemporary%2520face%2520recognition%2520systems%2520use%2520feature%2520templates%2520extracted%2520from%250Aface%2520images%2520to%2520identify%2520persons.%2520To%2520enhance%2520privacy%252C%2520face%2520template%2520protection%250Atechniques%2520are%2520widely%2520employed%2520to%2520conceal%2520sensitive%2520identity%2520and%2520appearance%250Ainformation%2520stored%2520in%2520the%2520template.%2520This%2520paper%2520identifies%2520an%2520emerging%2520privacy%250Aattack%2520form%2520utilizing%2520diffusion%2520models%2520that%2520could%2520nullify%2520prior%2520protection%252C%250Areferred%2520to%2520as%2520inversion%2520attacks.%2520The%2520attack%2520can%2520synthesize%2520high-quality%252C%250Aidentity-preserving%2520face%2520images%2520from%2520templates%252C%2520revealing%2520persons%2527%2520appearance.%250ABased%2520on%2520studies%2520of%2520the%2520diffusion%2520model%2527s%2520generative%2520capability%252C%2520this%2520paper%250Aproposes%2520a%2520defense%2520to%2520deteriorate%2520the%2520attack%252C%2520by%2520rotating%2520templates%2520to%2520a%250Anoise-like%2520distribution.%2520This%2520is%2520achieved%2520efficiently%2520by%2520spherically%2520and%250Alinearly%2520interpolating%2520templates%252C%2520or%2520slerp%252C%2520on%2520their%2520located%2520hypersphere.%2520This%250Apaper%2520further%2520proposes%2520to%2520group-wisely%2520divide%2520and%2520drop%2520out%2520templates%2527%2520feature%250Adimensions%252C%2520to%2520enhance%2520the%2520irreversibility%2520of%2520rotated%2520templates.%2520The%2520division%250Aof%2520groups%2520and%2520dropouts%2520within%2520each%2520group%2520are%2520learned%2520in%2520a%2520recognition-favored%250Away.%2520The%2520proposed%2520techniques%2520are%2520concretized%2520as%2520a%2520novel%2520face%2520template%250Aprotection%2520technique%252C%2520SlerpFace.%2520Extensive%2520experiments%2520show%2520that%2520SlerpFace%250Aprovides%2520satisfactory%2520recognition%2520accuracy%2520and%2520comprehensive%2520privacy%2520protection%250Aagainst%2520inversion%2520and%2520other%2520attack%2520forms%252C%2520superior%2520to%2520prior%2520arts.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03043v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SlerpFace%3A%20Face%20Template%20Protection%20via%20Spherical%20Linear%20Interpolation&entry.906535625=Zhizhou%20Zhong%20and%20Yuxi%20Mi%20and%20Yuge%20Huang%20and%20Jianqing%20Xu%20and%20Guodong%20Mu%20and%20Shouhong%20Ding%20and%20Jingyun%20Zhang%20and%20Rizen%20Guo%20and%20Yunsheng%20Wu%20and%20Shuigeng%20Zhou&entry.1292438233=%20%20Contemporary%20face%20recognition%20systems%20use%20feature%20templates%20extracted%20from%0Aface%20images%20to%20identify%20persons.%20To%20enhance%20privacy%2C%20face%20template%20protection%0Atechniques%20are%20widely%20employed%20to%20conceal%20sensitive%20identity%20and%20appearance%0Ainformation%20stored%20in%20the%20template.%20This%20paper%20identifies%20an%20emerging%20privacy%0Aattack%20form%20utilizing%20diffusion%20models%20that%20could%20nullify%20prior%20protection%2C%0Areferred%20to%20as%20inversion%20attacks.%20The%20attack%20can%20synthesize%20high-quality%2C%0Aidentity-preserving%20face%20images%20from%20templates%2C%20revealing%20persons%27%20appearance.%0ABased%20on%20studies%20of%20the%20diffusion%20model%27s%20generative%20capability%2C%20this%20paper%0Aproposes%20a%20defense%20to%20deteriorate%20the%20attack%2C%20by%20rotating%20templates%20to%20a%0Anoise-like%20distribution.%20This%20is%20achieved%20efficiently%20by%20spherically%20and%0Alinearly%20interpolating%20templates%2C%20or%20slerp%2C%20on%20their%20located%20hypersphere.%20This%0Apaper%20further%20proposes%20to%20group-wisely%20divide%20and%20drop%20out%20templates%27%20feature%0Adimensions%2C%20to%20enhance%20the%20irreversibility%20of%20rotated%20templates.%20The%20division%0Aof%20groups%20and%20dropouts%20within%20each%20group%20are%20learned%20in%20a%20recognition-favored%0Away.%20The%20proposed%20techniques%20are%20concretized%20as%20a%20novel%20face%20template%0Aprotection%20technique%2C%20SlerpFace.%20Extensive%20experiments%20show%20that%20SlerpFace%0Aprovides%20satisfactory%20recognition%20accuracy%20and%20comprehensive%20privacy%20protection%0Aagainst%20inversion%20and%20other%20attack%20forms%2C%20superior%20to%20prior%20arts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03043v1&entry.124074799=Read"},
{"title": "Backstepping Neural Operators for $2\\times 2$ Hyperbolic PDEs", "author": "Shanshan Wang and Mamadou Diagne and Miroslav Krsti\u0107", "abstract": "  Deep neural network approximation of nonlinear operators, commonly referred\nto as DeepONet, has proven capable of approximating PDE backstepping designs in\nwhich a single Goursat-form PDE governs a single feedback gain function. In\nboundary control of coupled PDEs, coupled Goursat-form PDEs govern two or more\ngain kernels-a PDE structure unaddressed thus far with DeepONet. In this paper,\nwe explore the subject of approximating systems of gain kernel PDEs for\nhyperbolic PDE plants by considering a simple counter-convecting $2\\times 2$\ncoupled system in whose control a $2\\times 2$ kernel PDE system in Goursat form\narises. Engineering applications include oil drilling, the Saint-Venant model\nof shallow water waves, and the Aw-Rascle-Zhang model of stop-and-go\ninstability in congested traffic flow. We establish the continuity of the\nmapping from a total of five plant PDE functional coefficients to the kernel\nPDE solutions, prove the existence of an arbitrarily close DeepONet\napproximation to the kernel PDEs, and ensure that the DeepONet-approximated\ngains guarantee stabilization when replacing the exact backstepping gain\nkernels. Taking into account anti-collocated boundary actuation and sensing,\nour $L^2$-Globally-exponentially stabilizing (GES) approximate gain\nkernel-based output feedback design implies the deep learning of both the\ncontroller's and the observer's gains. Moreover, the encoding of the\noutput-feedback law into DeepONet ensures semi-global practical exponential\nstability (SG-PES). The DeepONet operator speeds up the computation of the\ncontroller gains by multiple orders of magnitude. Its theoretically proven\nstabilizing capability is demonstrated through simulations.\n", "link": "http://arxiv.org/abs/2312.16762v3", "date": "2024-07-03", "relevancy": 1.8647, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5011}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4475}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4387}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Backstepping%20Neural%20Operators%20for%20%242%5Ctimes%202%24%20Hyperbolic%20PDEs&body=Title%3A%20Backstepping%20Neural%20Operators%20for%20%242%5Ctimes%202%24%20Hyperbolic%20PDEs%0AAuthor%3A%20Shanshan%20Wang%20and%20Mamadou%20Diagne%20and%20Miroslav%20Krsti%C4%87%0AAbstract%3A%20%20%20Deep%20neural%20network%20approximation%20of%20nonlinear%20operators%2C%20commonly%20referred%0Ato%20as%20DeepONet%2C%20has%20proven%20capable%20of%20approximating%20PDE%20backstepping%20designs%20in%0Awhich%20a%20single%20Goursat-form%20PDE%20governs%20a%20single%20feedback%20gain%20function.%20In%0Aboundary%20control%20of%20coupled%20PDEs%2C%20coupled%20Goursat-form%20PDEs%20govern%20two%20or%20more%0Again%20kernels-a%20PDE%20structure%20unaddressed%20thus%20far%20with%20DeepONet.%20In%20this%20paper%2C%0Awe%20explore%20the%20subject%20of%20approximating%20systems%20of%20gain%20kernel%20PDEs%20for%0Ahyperbolic%20PDE%20plants%20by%20considering%20a%20simple%20counter-convecting%20%242%5Ctimes%202%24%0Acoupled%20system%20in%20whose%20control%20a%20%242%5Ctimes%202%24%20kernel%20PDE%20system%20in%20Goursat%20form%0Aarises.%20Engineering%20applications%20include%20oil%20drilling%2C%20the%20Saint-Venant%20model%0Aof%20shallow%20water%20waves%2C%20and%20the%20Aw-Rascle-Zhang%20model%20of%20stop-and-go%0Ainstability%20in%20congested%20traffic%20flow.%20We%20establish%20the%20continuity%20of%20the%0Amapping%20from%20a%20total%20of%20five%20plant%20PDE%20functional%20coefficients%20to%20the%20kernel%0APDE%20solutions%2C%20prove%20the%20existence%20of%20an%20arbitrarily%20close%20DeepONet%0Aapproximation%20to%20the%20kernel%20PDEs%2C%20and%20ensure%20that%20the%20DeepONet-approximated%0Agains%20guarantee%20stabilization%20when%20replacing%20the%20exact%20backstepping%20gain%0Akernels.%20Taking%20into%20account%20anti-collocated%20boundary%20actuation%20and%20sensing%2C%0Aour%20%24L%5E2%24-Globally-exponentially%20stabilizing%20%28GES%29%20approximate%20gain%0Akernel-based%20output%20feedback%20design%20implies%20the%20deep%20learning%20of%20both%20the%0Acontroller%27s%20and%20the%20observer%27s%20gains.%20Moreover%2C%20the%20encoding%20of%20the%0Aoutput-feedback%20law%20into%20DeepONet%20ensures%20semi-global%20practical%20exponential%0Astability%20%28SG-PES%29.%20The%20DeepONet%20operator%20speeds%20up%20the%20computation%20of%20the%0Acontroller%20gains%20by%20multiple%20orders%20of%20magnitude.%20Its%20theoretically%20proven%0Astabilizing%20capability%20is%20demonstrated%20through%20simulations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.16762v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBackstepping%2520Neural%2520Operators%2520for%2520%25242%255Ctimes%25202%2524%2520Hyperbolic%2520PDEs%26entry.906535625%3DShanshan%2520Wang%2520and%2520Mamadou%2520Diagne%2520and%2520Miroslav%2520Krsti%25C4%2587%26entry.1292438233%3D%2520%2520Deep%2520neural%2520network%2520approximation%2520of%2520nonlinear%2520operators%252C%2520commonly%2520referred%250Ato%2520as%2520DeepONet%252C%2520has%2520proven%2520capable%2520of%2520approximating%2520PDE%2520backstepping%2520designs%2520in%250Awhich%2520a%2520single%2520Goursat-form%2520PDE%2520governs%2520a%2520single%2520feedback%2520gain%2520function.%2520In%250Aboundary%2520control%2520of%2520coupled%2520PDEs%252C%2520coupled%2520Goursat-form%2520PDEs%2520govern%2520two%2520or%2520more%250Again%2520kernels-a%2520PDE%2520structure%2520unaddressed%2520thus%2520far%2520with%2520DeepONet.%2520In%2520this%2520paper%252C%250Awe%2520explore%2520the%2520subject%2520of%2520approximating%2520systems%2520of%2520gain%2520kernel%2520PDEs%2520for%250Ahyperbolic%2520PDE%2520plants%2520by%2520considering%2520a%2520simple%2520counter-convecting%2520%25242%255Ctimes%25202%2524%250Acoupled%2520system%2520in%2520whose%2520control%2520a%2520%25242%255Ctimes%25202%2524%2520kernel%2520PDE%2520system%2520in%2520Goursat%2520form%250Aarises.%2520Engineering%2520applications%2520include%2520oil%2520drilling%252C%2520the%2520Saint-Venant%2520model%250Aof%2520shallow%2520water%2520waves%252C%2520and%2520the%2520Aw-Rascle-Zhang%2520model%2520of%2520stop-and-go%250Ainstability%2520in%2520congested%2520traffic%2520flow.%2520We%2520establish%2520the%2520continuity%2520of%2520the%250Amapping%2520from%2520a%2520total%2520of%2520five%2520plant%2520PDE%2520functional%2520coefficients%2520to%2520the%2520kernel%250APDE%2520solutions%252C%2520prove%2520the%2520existence%2520of%2520an%2520arbitrarily%2520close%2520DeepONet%250Aapproximation%2520to%2520the%2520kernel%2520PDEs%252C%2520and%2520ensure%2520that%2520the%2520DeepONet-approximated%250Agains%2520guarantee%2520stabilization%2520when%2520replacing%2520the%2520exact%2520backstepping%2520gain%250Akernels.%2520Taking%2520into%2520account%2520anti-collocated%2520boundary%2520actuation%2520and%2520sensing%252C%250Aour%2520%2524L%255E2%2524-Globally-exponentially%2520stabilizing%2520%2528GES%2529%2520approximate%2520gain%250Akernel-based%2520output%2520feedback%2520design%2520implies%2520the%2520deep%2520learning%2520of%2520both%2520the%250Acontroller%2527s%2520and%2520the%2520observer%2527s%2520gains.%2520Moreover%252C%2520the%2520encoding%2520of%2520the%250Aoutput-feedback%2520law%2520into%2520DeepONet%2520ensures%2520semi-global%2520practical%2520exponential%250Astability%2520%2528SG-PES%2529.%2520The%2520DeepONet%2520operator%2520speeds%2520up%2520the%2520computation%2520of%2520the%250Acontroller%2520gains%2520by%2520multiple%2520orders%2520of%2520magnitude.%2520Its%2520theoretically%2520proven%250Astabilizing%2520capability%2520is%2520demonstrated%2520through%2520simulations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.16762v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Backstepping%20Neural%20Operators%20for%20%242%5Ctimes%202%24%20Hyperbolic%20PDEs&entry.906535625=Shanshan%20Wang%20and%20Mamadou%20Diagne%20and%20Miroslav%20Krsti%C4%87&entry.1292438233=%20%20Deep%20neural%20network%20approximation%20of%20nonlinear%20operators%2C%20commonly%20referred%0Ato%20as%20DeepONet%2C%20has%20proven%20capable%20of%20approximating%20PDE%20backstepping%20designs%20in%0Awhich%20a%20single%20Goursat-form%20PDE%20governs%20a%20single%20feedback%20gain%20function.%20In%0Aboundary%20control%20of%20coupled%20PDEs%2C%20coupled%20Goursat-form%20PDEs%20govern%20two%20or%20more%0Again%20kernels-a%20PDE%20structure%20unaddressed%20thus%20far%20with%20DeepONet.%20In%20this%20paper%2C%0Awe%20explore%20the%20subject%20of%20approximating%20systems%20of%20gain%20kernel%20PDEs%20for%0Ahyperbolic%20PDE%20plants%20by%20considering%20a%20simple%20counter-convecting%20%242%5Ctimes%202%24%0Acoupled%20system%20in%20whose%20control%20a%20%242%5Ctimes%202%24%20kernel%20PDE%20system%20in%20Goursat%20form%0Aarises.%20Engineering%20applications%20include%20oil%20drilling%2C%20the%20Saint-Venant%20model%0Aof%20shallow%20water%20waves%2C%20and%20the%20Aw-Rascle-Zhang%20model%20of%20stop-and-go%0Ainstability%20in%20congested%20traffic%20flow.%20We%20establish%20the%20continuity%20of%20the%0Amapping%20from%20a%20total%20of%20five%20plant%20PDE%20functional%20coefficients%20to%20the%20kernel%0APDE%20solutions%2C%20prove%20the%20existence%20of%20an%20arbitrarily%20close%20DeepONet%0Aapproximation%20to%20the%20kernel%20PDEs%2C%20and%20ensure%20that%20the%20DeepONet-approximated%0Agains%20guarantee%20stabilization%20when%20replacing%20the%20exact%20backstepping%20gain%0Akernels.%20Taking%20into%20account%20anti-collocated%20boundary%20actuation%20and%20sensing%2C%0Aour%20%24L%5E2%24-Globally-exponentially%20stabilizing%20%28GES%29%20approximate%20gain%0Akernel-based%20output%20feedback%20design%20implies%20the%20deep%20learning%20of%20both%20the%0Acontroller%27s%20and%20the%20observer%27s%20gains.%20Moreover%2C%20the%20encoding%20of%20the%0Aoutput-feedback%20law%20into%20DeepONet%20ensures%20semi-global%20practical%20exponential%0Astability%20%28SG-PES%29.%20The%20DeepONet%20operator%20speeds%20up%20the%20computation%20of%20the%0Acontroller%20gains%20by%20multiple%20orders%20of%20magnitude.%20Its%20theoretically%20proven%0Astabilizing%20capability%20is%20demonstrated%20through%20simulations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.16762v3&entry.124074799=Read"},
{"title": "Towards Human-AI Collaboration in Healthcare: Guided Deferral Systems\n  with Large Language Models", "author": "Joshua Strong and Qianhui Men and Alison Noble", "abstract": "  Large language models (LLMs) present a valuable technology for various\napplications in healthcare, but their tendency to hallucinate introduces\nunacceptable uncertainty in critical decision-making situations. Human-AI\ncollaboration (HAIC) can mitigate this uncertainty by combining human and AI\nstrengths for better outcomes. This paper presents a novel guided deferral\nsystem that provides intelligent guidance when AI defers cases to human\ndecision-makers. We leverage LLMs' verbalisation capabilities and internal\nstates to create this system, demonstrating that fine-tuning small-scale LLMs\nwith data from large-scale LLMs greatly enhances performance while maintaining\ncomputational efficiency and data privacy. A pilot study showcases the\neffectiveness of our proposed deferral system.\n", "link": "http://arxiv.org/abs/2406.07212v2", "date": "2024-07-03", "relevancy": 1.8337, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5052}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4491}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Human-AI%20Collaboration%20in%20Healthcare%3A%20Guided%20Deferral%20Systems%0A%20%20with%20Large%20Language%20Models&body=Title%3A%20Towards%20Human-AI%20Collaboration%20in%20Healthcare%3A%20Guided%20Deferral%20Systems%0A%20%20with%20Large%20Language%20Models%0AAuthor%3A%20Joshua%20Strong%20and%20Qianhui%20Men%20and%20Alison%20Noble%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%20present%20a%20valuable%20technology%20for%20various%0Aapplications%20in%20healthcare%2C%20but%20their%20tendency%20to%20hallucinate%20introduces%0Aunacceptable%20uncertainty%20in%20critical%20decision-making%20situations.%20Human-AI%0Acollaboration%20%28HAIC%29%20can%20mitigate%20this%20uncertainty%20by%20combining%20human%20and%20AI%0Astrengths%20for%20better%20outcomes.%20This%20paper%20presents%20a%20novel%20guided%20deferral%0Asystem%20that%20provides%20intelligent%20guidance%20when%20AI%20defers%20cases%20to%20human%0Adecision-makers.%20We%20leverage%20LLMs%27%20verbalisation%20capabilities%20and%20internal%0Astates%20to%20create%20this%20system%2C%20demonstrating%20that%20fine-tuning%20small-scale%20LLMs%0Awith%20data%20from%20large-scale%20LLMs%20greatly%20enhances%20performance%20while%20maintaining%0Acomputational%20efficiency%20and%20data%20privacy.%20A%20pilot%20study%20showcases%20the%0Aeffectiveness%20of%20our%20proposed%20deferral%20system.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.07212v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Human-AI%2520Collaboration%2520in%2520Healthcare%253A%2520Guided%2520Deferral%2520Systems%250A%2520%2520with%2520Large%2520Language%2520Models%26entry.906535625%3DJoshua%2520Strong%2520and%2520Qianhui%2520Men%2520and%2520Alison%2520Noble%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%2520present%2520a%2520valuable%2520technology%2520for%2520various%250Aapplications%2520in%2520healthcare%252C%2520but%2520their%2520tendency%2520to%2520hallucinate%2520introduces%250Aunacceptable%2520uncertainty%2520in%2520critical%2520decision-making%2520situations.%2520Human-AI%250Acollaboration%2520%2528HAIC%2529%2520can%2520mitigate%2520this%2520uncertainty%2520by%2520combining%2520human%2520and%2520AI%250Astrengths%2520for%2520better%2520outcomes.%2520This%2520paper%2520presents%2520a%2520novel%2520guided%2520deferral%250Asystem%2520that%2520provides%2520intelligent%2520guidance%2520when%2520AI%2520defers%2520cases%2520to%2520human%250Adecision-makers.%2520We%2520leverage%2520LLMs%2527%2520verbalisation%2520capabilities%2520and%2520internal%250Astates%2520to%2520create%2520this%2520system%252C%2520demonstrating%2520that%2520fine-tuning%2520small-scale%2520LLMs%250Awith%2520data%2520from%2520large-scale%2520LLMs%2520greatly%2520enhances%2520performance%2520while%2520maintaining%250Acomputational%2520efficiency%2520and%2520data%2520privacy.%2520A%2520pilot%2520study%2520showcases%2520the%250Aeffectiveness%2520of%2520our%2520proposed%2520deferral%2520system.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.07212v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Human-AI%20Collaboration%20in%20Healthcare%3A%20Guided%20Deferral%20Systems%0A%20%20with%20Large%20Language%20Models&entry.906535625=Joshua%20Strong%20and%20Qianhui%20Men%20and%20Alison%20Noble&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%20present%20a%20valuable%20technology%20for%20various%0Aapplications%20in%20healthcare%2C%20but%20their%20tendency%20to%20hallucinate%20introduces%0Aunacceptable%20uncertainty%20in%20critical%20decision-making%20situations.%20Human-AI%0Acollaboration%20%28HAIC%29%20can%20mitigate%20this%20uncertainty%20by%20combining%20human%20and%20AI%0Astrengths%20for%20better%20outcomes.%20This%20paper%20presents%20a%20novel%20guided%20deferral%0Asystem%20that%20provides%20intelligent%20guidance%20when%20AI%20defers%20cases%20to%20human%0Adecision-makers.%20We%20leverage%20LLMs%27%20verbalisation%20capabilities%20and%20internal%0Astates%20to%20create%20this%20system%2C%20demonstrating%20that%20fine-tuning%20small-scale%20LLMs%0Awith%20data%20from%20large-scale%20LLMs%20greatly%20enhances%20performance%20while%20maintaining%0Acomputational%20efficiency%20and%20data%20privacy.%20A%20pilot%20study%20showcases%20the%0Aeffectiveness%20of%20our%20proposed%20deferral%20system.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.07212v2&entry.124074799=Read"},
{"title": "Qifusion-Net: Layer-adapted Stream/Non-stream Model for End-to-End\n  Multi-Accent Speech Recognition", "author": "Jinming Chen and Jingyi Fang and Yuanzhong Zheng and Yaoxuan Wang and Haojun Fei", "abstract": "  Currently, end-to-end (E2E) speech recognition methods have achieved\npromising performance. However, auto speech recognition (ASR) models still face\nchallenges in recognizing multi-accent speech accurately. We propose a\nlayer-adapted fusion (LAF) model, called Qifusion-Net, which does not require\nany prior knowledge about the target accent. Based on dynamic chunk strategy,\nour approach enables streaming decoding and can extract frame-level acoustic\nfeature, facilitating fine-grained information fusion. Experiment results\ndemonstrate that our proposed methods outperform the baseline with relative\nreductions of 22.1$\\%$ and 17.2$\\%$ in character error rate (CER) across multi\naccent test datasets on KeSpeech and MagicData-RMAC.\n", "link": "http://arxiv.org/abs/2407.03026v1", "date": "2024-07-03", "relevancy": 0.9313, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4745}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4624}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4601}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Qifusion-Net%3A%20Layer-adapted%20Stream/Non-stream%20Model%20for%20End-to-End%0A%20%20Multi-Accent%20Speech%20Recognition&body=Title%3A%20Qifusion-Net%3A%20Layer-adapted%20Stream/Non-stream%20Model%20for%20End-to-End%0A%20%20Multi-Accent%20Speech%20Recognition%0AAuthor%3A%20Jinming%20Chen%20and%20Jingyi%20Fang%20and%20Yuanzhong%20Zheng%20and%20Yaoxuan%20Wang%20and%20Haojun%20Fei%0AAbstract%3A%20%20%20Currently%2C%20end-to-end%20%28E2E%29%20speech%20recognition%20methods%20have%20achieved%0Apromising%20performance.%20However%2C%20auto%20speech%20recognition%20%28ASR%29%20models%20still%20face%0Achallenges%20in%20recognizing%20multi-accent%20speech%20accurately.%20We%20propose%20a%0Alayer-adapted%20fusion%20%28LAF%29%20model%2C%20called%20Qifusion-Net%2C%20which%20does%20not%20require%0Aany%20prior%20knowledge%20about%20the%20target%20accent.%20Based%20on%20dynamic%20chunk%20strategy%2C%0Aour%20approach%20enables%20streaming%20decoding%20and%20can%20extract%20frame-level%20acoustic%0Afeature%2C%20facilitating%20fine-grained%20information%20fusion.%20Experiment%20results%0Ademonstrate%20that%20our%20proposed%20methods%20outperform%20the%20baseline%20with%20relative%0Areductions%20of%2022.1%24%5C%25%24%20and%2017.2%24%5C%25%24%20in%20character%20error%20rate%20%28CER%29%20across%20multi%0Aaccent%20test%20datasets%20on%20KeSpeech%20and%20MagicData-RMAC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03026v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQifusion-Net%253A%2520Layer-adapted%2520Stream/Non-stream%2520Model%2520for%2520End-to-End%250A%2520%2520Multi-Accent%2520Speech%2520Recognition%26entry.906535625%3DJinming%2520Chen%2520and%2520Jingyi%2520Fang%2520and%2520Yuanzhong%2520Zheng%2520and%2520Yaoxuan%2520Wang%2520and%2520Haojun%2520Fei%26entry.1292438233%3D%2520%2520Currently%252C%2520end-to-end%2520%2528E2E%2529%2520speech%2520recognition%2520methods%2520have%2520achieved%250Apromising%2520performance.%2520However%252C%2520auto%2520speech%2520recognition%2520%2528ASR%2529%2520models%2520still%2520face%250Achallenges%2520in%2520recognizing%2520multi-accent%2520speech%2520accurately.%2520We%2520propose%2520a%250Alayer-adapted%2520fusion%2520%2528LAF%2529%2520model%252C%2520called%2520Qifusion-Net%252C%2520which%2520does%2520not%2520require%250Aany%2520prior%2520knowledge%2520about%2520the%2520target%2520accent.%2520Based%2520on%2520dynamic%2520chunk%2520strategy%252C%250Aour%2520approach%2520enables%2520streaming%2520decoding%2520and%2520can%2520extract%2520frame-level%2520acoustic%250Afeature%252C%2520facilitating%2520fine-grained%2520information%2520fusion.%2520Experiment%2520results%250Ademonstrate%2520that%2520our%2520proposed%2520methods%2520outperform%2520the%2520baseline%2520with%2520relative%250Areductions%2520of%252022.1%2524%255C%2525%2524%2520and%252017.2%2524%255C%2525%2524%2520in%2520character%2520error%2520rate%2520%2528CER%2529%2520across%2520multi%250Aaccent%2520test%2520datasets%2520on%2520KeSpeech%2520and%2520MagicData-RMAC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03026v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Qifusion-Net%3A%20Layer-adapted%20Stream/Non-stream%20Model%20for%20End-to-End%0A%20%20Multi-Accent%20Speech%20Recognition&entry.906535625=Jinming%20Chen%20and%20Jingyi%20Fang%20and%20Yuanzhong%20Zheng%20and%20Yaoxuan%20Wang%20and%20Haojun%20Fei&entry.1292438233=%20%20Currently%2C%20end-to-end%20%28E2E%29%20speech%20recognition%20methods%20have%20achieved%0Apromising%20performance.%20However%2C%20auto%20speech%20recognition%20%28ASR%29%20models%20still%20face%0Achallenges%20in%20recognizing%20multi-accent%20speech%20accurately.%20We%20propose%20a%0Alayer-adapted%20fusion%20%28LAF%29%20model%2C%20called%20Qifusion-Net%2C%20which%20does%20not%20require%0Aany%20prior%20knowledge%20about%20the%20target%20accent.%20Based%20on%20dynamic%20chunk%20strategy%2C%0Aour%20approach%20enables%20streaming%20decoding%20and%20can%20extract%20frame-level%20acoustic%0Afeature%2C%20facilitating%20fine-grained%20information%20fusion.%20Experiment%20results%0Ademonstrate%20that%20our%20proposed%20methods%20outperform%20the%20baseline%20with%20relative%0Areductions%20of%2022.1%24%5C%25%24%20and%2017.2%24%5C%25%24%20in%20character%20error%20rate%20%28CER%29%20across%20multi%0Aaccent%20test%20datasets%20on%20KeSpeech%20and%20MagicData-RMAC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03026v1&entry.124074799=Read"},
{"title": "Malign Overfitting: Interpolation Can Provably Preclude Invariance", "author": "Yoav Wald and Gal Yona and Uri Shalit and Yair Carmon", "abstract": "  Learned classifiers should often possess certain invariance properties meant\nto encourage fairness, robustness, or out-of-distribution generalization.\nHowever, multiple recent works empirically demonstrate that common\ninvariance-inducing regularizers are ineffective in the over-parameterized\nregime, in which classifiers perfectly fit (i.e. interpolate) the training\ndata. This suggests that the phenomenon of \"benign overfitting\", in which\nmodels generalize well despite interpolating, might not favorably extend to\nsettings in which robustness or fairness are desirable.\n  In this work we provide a theoretical justification for these observations.\nWe prove that -- even in the simplest of settings -- any interpolating learning\nrule (with arbitrarily small margin) will not satisfy these invariance\nproperties. We then propose and analyze an algorithm that -- in the same\nsetting -- successfully learns a non-interpolating classifier that is provably\ninvariant. We validate our theoretical observations on simulated data and the\nWaterbirds dataset.\n", "link": "http://arxiv.org/abs/2211.15724v2", "date": "2024-07-03", "relevancy": 1.7682, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4489}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4411}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Malign%20Overfitting%3A%20Interpolation%20Can%20Provably%20Preclude%20Invariance&body=Title%3A%20Malign%20Overfitting%3A%20Interpolation%20Can%20Provably%20Preclude%20Invariance%0AAuthor%3A%20Yoav%20Wald%20and%20Gal%20Yona%20and%20Uri%20Shalit%20and%20Yair%20Carmon%0AAbstract%3A%20%20%20Learned%20classifiers%20should%20often%20possess%20certain%20invariance%20properties%20meant%0Ato%20encourage%20fairness%2C%20robustness%2C%20or%20out-of-distribution%20generalization.%0AHowever%2C%20multiple%20recent%20works%20empirically%20demonstrate%20that%20common%0Ainvariance-inducing%20regularizers%20are%20ineffective%20in%20the%20over-parameterized%0Aregime%2C%20in%20which%20classifiers%20perfectly%20fit%20%28i.e.%20interpolate%29%20the%20training%0Adata.%20This%20suggests%20that%20the%20phenomenon%20of%20%22benign%20overfitting%22%2C%20in%20which%0Amodels%20generalize%20well%20despite%20interpolating%2C%20might%20not%20favorably%20extend%20to%0Asettings%20in%20which%20robustness%20or%20fairness%20are%20desirable.%0A%20%20In%20this%20work%20we%20provide%20a%20theoretical%20justification%20for%20these%20observations.%0AWe%20prove%20that%20--%20even%20in%20the%20simplest%20of%20settings%20--%20any%20interpolating%20learning%0Arule%20%28with%20arbitrarily%20small%20margin%29%20will%20not%20satisfy%20these%20invariance%0Aproperties.%20We%20then%20propose%20and%20analyze%20an%20algorithm%20that%20--%20in%20the%20same%0Asetting%20--%20successfully%20learns%20a%20non-interpolating%20classifier%20that%20is%20provably%0Ainvariant.%20We%20validate%20our%20theoretical%20observations%20on%20simulated%20data%20and%20the%0AWaterbirds%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2211.15724v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMalign%2520Overfitting%253A%2520Interpolation%2520Can%2520Provably%2520Preclude%2520Invariance%26entry.906535625%3DYoav%2520Wald%2520and%2520Gal%2520Yona%2520and%2520Uri%2520Shalit%2520and%2520Yair%2520Carmon%26entry.1292438233%3D%2520%2520Learned%2520classifiers%2520should%2520often%2520possess%2520certain%2520invariance%2520properties%2520meant%250Ato%2520encourage%2520fairness%252C%2520robustness%252C%2520or%2520out-of-distribution%2520generalization.%250AHowever%252C%2520multiple%2520recent%2520works%2520empirically%2520demonstrate%2520that%2520common%250Ainvariance-inducing%2520regularizers%2520are%2520ineffective%2520in%2520the%2520over-parameterized%250Aregime%252C%2520in%2520which%2520classifiers%2520perfectly%2520fit%2520%2528i.e.%2520interpolate%2529%2520the%2520training%250Adata.%2520This%2520suggests%2520that%2520the%2520phenomenon%2520of%2520%2522benign%2520overfitting%2522%252C%2520in%2520which%250Amodels%2520generalize%2520well%2520despite%2520interpolating%252C%2520might%2520not%2520favorably%2520extend%2520to%250Asettings%2520in%2520which%2520robustness%2520or%2520fairness%2520are%2520desirable.%250A%2520%2520In%2520this%2520work%2520we%2520provide%2520a%2520theoretical%2520justification%2520for%2520these%2520observations.%250AWe%2520prove%2520that%2520--%2520even%2520in%2520the%2520simplest%2520of%2520settings%2520--%2520any%2520interpolating%2520learning%250Arule%2520%2528with%2520arbitrarily%2520small%2520margin%2529%2520will%2520not%2520satisfy%2520these%2520invariance%250Aproperties.%2520We%2520then%2520propose%2520and%2520analyze%2520an%2520algorithm%2520that%2520--%2520in%2520the%2520same%250Asetting%2520--%2520successfully%2520learns%2520a%2520non-interpolating%2520classifier%2520that%2520is%2520provably%250Ainvariant.%2520We%2520validate%2520our%2520theoretical%2520observations%2520on%2520simulated%2520data%2520and%2520the%250AWaterbirds%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2211.15724v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Malign%20Overfitting%3A%20Interpolation%20Can%20Provably%20Preclude%20Invariance&entry.906535625=Yoav%20Wald%20and%20Gal%20Yona%20and%20Uri%20Shalit%20and%20Yair%20Carmon&entry.1292438233=%20%20Learned%20classifiers%20should%20often%20possess%20certain%20invariance%20properties%20meant%0Ato%20encourage%20fairness%2C%20robustness%2C%20or%20out-of-distribution%20generalization.%0AHowever%2C%20multiple%20recent%20works%20empirically%20demonstrate%20that%20common%0Ainvariance-inducing%20regularizers%20are%20ineffective%20in%20the%20over-parameterized%0Aregime%2C%20in%20which%20classifiers%20perfectly%20fit%20%28i.e.%20interpolate%29%20the%20training%0Adata.%20This%20suggests%20that%20the%20phenomenon%20of%20%22benign%20overfitting%22%2C%20in%20which%0Amodels%20generalize%20well%20despite%20interpolating%2C%20might%20not%20favorably%20extend%20to%0Asettings%20in%20which%20robustness%20or%20fairness%20are%20desirable.%0A%20%20In%20this%20work%20we%20provide%20a%20theoretical%20justification%20for%20these%20observations.%0AWe%20prove%20that%20--%20even%20in%20the%20simplest%20of%20settings%20--%20any%20interpolating%20learning%0Arule%20%28with%20arbitrarily%20small%20margin%29%20will%20not%20satisfy%20these%20invariance%0Aproperties.%20We%20then%20propose%20and%20analyze%20an%20algorithm%20that%20--%20in%20the%20same%0Asetting%20--%20successfully%20learns%20a%20non-interpolating%20classifier%20that%20is%20provably%0Ainvariant.%20We%20validate%20our%20theoretical%20observations%20on%20simulated%20data%20and%20the%0AWaterbirds%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2211.15724v2&entry.124074799=Read"},
{"title": "Labeling Sentences with Symbolic and Deictic Gestures via Semantic\n  Similarity", "author": "Ariel Gjaci and Carmine Tommaso Recchiuto and Antonio Sgorbissa", "abstract": "  Co-speech gesture generation on artificial agents has gained attention\nrecently, mainly when it is based on data-driven models. However, end-to-end\nmethods often fail to generate co-speech gestures related to semantics with\nspecific forms, i.e., Symbolic and Deictic gestures. In this work, we identify\nwhich words in a sentence are contextually related to Symbolic and Deictic\ngestures. Firstly, we appropriately chose 12 gestures recognized by people from\nthe Italian culture, which different humanoid robots can reproduce. Then, we\nimplemented two rule-based algorithms to label sentences with Symbolic and\nDeictic gestures. The rules depend on the semantic similarity scores computed\nwith the RoBerta model between sentences that heuristically represent gestures\nand sub-sentences inside an objective sentence that artificial agents have to\npronounce. We also implemented a baseline algorithm that assigns gestures\nwithout computing similarity scores. Finally, to validate the results, we asked\n30 persons to label a set of sentences with Deictic and Symbolic gestures\nthrough a Graphical User Interface (GUI), and we compared the labels with the\nones produced by our algorithms. For this scope, we computed Average Precision\n(AP) and Intersection Over Union (IOU) scores, and we evaluated the Average\nComputational Time (ACT). Our results show that semantic similarity scores are\nuseful for finding Symbolic and Deictic gestures in utterances.\n", "link": "http://arxiv.org/abs/2407.02151v2", "date": "2024-07-03", "relevancy": 1.5497, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5323}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5157}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5106}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Labeling%20Sentences%20with%20Symbolic%20and%20Deictic%20Gestures%20via%20Semantic%0A%20%20Similarity&body=Title%3A%20Labeling%20Sentences%20with%20Symbolic%20and%20Deictic%20Gestures%20via%20Semantic%0A%20%20Similarity%0AAuthor%3A%20Ariel%20Gjaci%20and%20Carmine%20Tommaso%20Recchiuto%20and%20Antonio%20Sgorbissa%0AAbstract%3A%20%20%20Co-speech%20gesture%20generation%20on%20artificial%20agents%20has%20gained%20attention%0Arecently%2C%20mainly%20when%20it%20is%20based%20on%20data-driven%20models.%20However%2C%20end-to-end%0Amethods%20often%20fail%20to%20generate%20co-speech%20gestures%20related%20to%20semantics%20with%0Aspecific%20forms%2C%20i.e.%2C%20Symbolic%20and%20Deictic%20gestures.%20In%20this%20work%2C%20we%20identify%0Awhich%20words%20in%20a%20sentence%20are%20contextually%20related%20to%20Symbolic%20and%20Deictic%0Agestures.%20Firstly%2C%20we%20appropriately%20chose%2012%20gestures%20recognized%20by%20people%20from%0Athe%20Italian%20culture%2C%20which%20different%20humanoid%20robots%20can%20reproduce.%20Then%2C%20we%0Aimplemented%20two%20rule-based%20algorithms%20to%20label%20sentences%20with%20Symbolic%20and%0ADeictic%20gestures.%20The%20rules%20depend%20on%20the%20semantic%20similarity%20scores%20computed%0Awith%20the%20RoBerta%20model%20between%20sentences%20that%20heuristically%20represent%20gestures%0Aand%20sub-sentences%20inside%20an%20objective%20sentence%20that%20artificial%20agents%20have%20to%0Apronounce.%20We%20also%20implemented%20a%20baseline%20algorithm%20that%20assigns%20gestures%0Awithout%20computing%20similarity%20scores.%20Finally%2C%20to%20validate%20the%20results%2C%20we%20asked%0A30%20persons%20to%20label%20a%20set%20of%20sentences%20with%20Deictic%20and%20Symbolic%20gestures%0Athrough%20a%20Graphical%20User%20Interface%20%28GUI%29%2C%20and%20we%20compared%20the%20labels%20with%20the%0Aones%20produced%20by%20our%20algorithms.%20For%20this%20scope%2C%20we%20computed%20Average%20Precision%0A%28AP%29%20and%20Intersection%20Over%20Union%20%28IOU%29%20scores%2C%20and%20we%20evaluated%20the%20Average%0AComputational%20Time%20%28ACT%29.%20Our%20results%20show%20that%20semantic%20similarity%20scores%20are%0Auseful%20for%20finding%20Symbolic%20and%20Deictic%20gestures%20in%20utterances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.02151v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLabeling%2520Sentences%2520with%2520Symbolic%2520and%2520Deictic%2520Gestures%2520via%2520Semantic%250A%2520%2520Similarity%26entry.906535625%3DAriel%2520Gjaci%2520and%2520Carmine%2520Tommaso%2520Recchiuto%2520and%2520Antonio%2520Sgorbissa%26entry.1292438233%3D%2520%2520Co-speech%2520gesture%2520generation%2520on%2520artificial%2520agents%2520has%2520gained%2520attention%250Arecently%252C%2520mainly%2520when%2520it%2520is%2520based%2520on%2520data-driven%2520models.%2520However%252C%2520end-to-end%250Amethods%2520often%2520fail%2520to%2520generate%2520co-speech%2520gestures%2520related%2520to%2520semantics%2520with%250Aspecific%2520forms%252C%2520i.e.%252C%2520Symbolic%2520and%2520Deictic%2520gestures.%2520In%2520this%2520work%252C%2520we%2520identify%250Awhich%2520words%2520in%2520a%2520sentence%2520are%2520contextually%2520related%2520to%2520Symbolic%2520and%2520Deictic%250Agestures.%2520Firstly%252C%2520we%2520appropriately%2520chose%252012%2520gestures%2520recognized%2520by%2520people%2520from%250Athe%2520Italian%2520culture%252C%2520which%2520different%2520humanoid%2520robots%2520can%2520reproduce.%2520Then%252C%2520we%250Aimplemented%2520two%2520rule-based%2520algorithms%2520to%2520label%2520sentences%2520with%2520Symbolic%2520and%250ADeictic%2520gestures.%2520The%2520rules%2520depend%2520on%2520the%2520semantic%2520similarity%2520scores%2520computed%250Awith%2520the%2520RoBerta%2520model%2520between%2520sentences%2520that%2520heuristically%2520represent%2520gestures%250Aand%2520sub-sentences%2520inside%2520an%2520objective%2520sentence%2520that%2520artificial%2520agents%2520have%2520to%250Apronounce.%2520We%2520also%2520implemented%2520a%2520baseline%2520algorithm%2520that%2520assigns%2520gestures%250Awithout%2520computing%2520similarity%2520scores.%2520Finally%252C%2520to%2520validate%2520the%2520results%252C%2520we%2520asked%250A30%2520persons%2520to%2520label%2520a%2520set%2520of%2520sentences%2520with%2520Deictic%2520and%2520Symbolic%2520gestures%250Athrough%2520a%2520Graphical%2520User%2520Interface%2520%2528GUI%2529%252C%2520and%2520we%2520compared%2520the%2520labels%2520with%2520the%250Aones%2520produced%2520by%2520our%2520algorithms.%2520For%2520this%2520scope%252C%2520we%2520computed%2520Average%2520Precision%250A%2528AP%2529%2520and%2520Intersection%2520Over%2520Union%2520%2528IOU%2529%2520scores%252C%2520and%2520we%2520evaluated%2520the%2520Average%250AComputational%2520Time%2520%2528ACT%2529.%2520Our%2520results%2520show%2520that%2520semantic%2520similarity%2520scores%2520are%250Auseful%2520for%2520finding%2520Symbolic%2520and%2520Deictic%2520gestures%2520in%2520utterances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.02151v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Labeling%20Sentences%20with%20Symbolic%20and%20Deictic%20Gestures%20via%20Semantic%0A%20%20Similarity&entry.906535625=Ariel%20Gjaci%20and%20Carmine%20Tommaso%20Recchiuto%20and%20Antonio%20Sgorbissa&entry.1292438233=%20%20Co-speech%20gesture%20generation%20on%20artificial%20agents%20has%20gained%20attention%0Arecently%2C%20mainly%20when%20it%20is%20based%20on%20data-driven%20models.%20However%2C%20end-to-end%0Amethods%20often%20fail%20to%20generate%20co-speech%20gestures%20related%20to%20semantics%20with%0Aspecific%20forms%2C%20i.e.%2C%20Symbolic%20and%20Deictic%20gestures.%20In%20this%20work%2C%20we%20identify%0Awhich%20words%20in%20a%20sentence%20are%20contextually%20related%20to%20Symbolic%20and%20Deictic%0Agestures.%20Firstly%2C%20we%20appropriately%20chose%2012%20gestures%20recognized%20by%20people%20from%0Athe%20Italian%20culture%2C%20which%20different%20humanoid%20robots%20can%20reproduce.%20Then%2C%20we%0Aimplemented%20two%20rule-based%20algorithms%20to%20label%20sentences%20with%20Symbolic%20and%0ADeictic%20gestures.%20The%20rules%20depend%20on%20the%20semantic%20similarity%20scores%20computed%0Awith%20the%20RoBerta%20model%20between%20sentences%20that%20heuristically%20represent%20gestures%0Aand%20sub-sentences%20inside%20an%20objective%20sentence%20that%20artificial%20agents%20have%20to%0Apronounce.%20We%20also%20implemented%20a%20baseline%20algorithm%20that%20assigns%20gestures%0Awithout%20computing%20similarity%20scores.%20Finally%2C%20to%20validate%20the%20results%2C%20we%20asked%0A30%20persons%20to%20label%20a%20set%20of%20sentences%20with%20Deictic%20and%20Symbolic%20gestures%0Athrough%20a%20Graphical%20User%20Interface%20%28GUI%29%2C%20and%20we%20compared%20the%20labels%20with%20the%0Aones%20produced%20by%20our%20algorithms.%20For%20this%20scope%2C%20we%20computed%20Average%20Precision%0A%28AP%29%20and%20Intersection%20Over%20Union%20%28IOU%29%20scores%2C%20and%20we%20evaluated%20the%20Average%0AComputational%20Time%20%28ACT%29.%20Our%20results%20show%20that%20semantic%20similarity%20scores%20are%0Auseful%20for%20finding%20Symbolic%20and%20Deictic%20gestures%20in%20utterances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.02151v2&entry.124074799=Read"},
{"title": "Terrain Classification Enhanced with Uncertainty for Space Exploration\n  Robots from Proprioceptive Data", "author": "Mariela De Lucas \u00c1lvarez and Jichen Guo and Raul Dom\u00ednguez and Matias Valdenegro-Toro", "abstract": "  Terrain Classification is an essential task in space exploration, where\nunpredictable environments are difficult to observe using only exteroceptive\nsensors such as vision. Implementing Neural Network classifiers can have high\nperformance but can be deemed untrustworthy as they lack transparency, which\nmakes them unreliable for taking high-stakes decisions during mission planning.\nWe address this by proposing Neural Networks with Uncertainty Quantification in\nTerrain Classification. We enable our Neural Networks with Monte Carlo Dropout,\nDropConnect, and Flipout in time series-capable architectures using only\nproprioceptive data as input. We use Bayesian Optimization with Hyperband for\nefficient hyperparameter optimization to find optimal models for trustworthy\nterrain classification.\n", "link": "http://arxiv.org/abs/2407.03241v1", "date": "2024-07-03", "relevancy": 1.8138, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.635}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6151}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Terrain%20Classification%20Enhanced%20with%20Uncertainty%20for%20Space%20Exploration%0A%20%20Robots%20from%20Proprioceptive%20Data&body=Title%3A%20Terrain%20Classification%20Enhanced%20with%20Uncertainty%20for%20Space%20Exploration%0A%20%20Robots%20from%20Proprioceptive%20Data%0AAuthor%3A%20Mariela%20De%20Lucas%20%C3%81lvarez%20and%20Jichen%20Guo%20and%20Raul%20Dom%C3%ADnguez%20and%20Matias%20Valdenegro-Toro%0AAbstract%3A%20%20%20Terrain%20Classification%20is%20an%20essential%20task%20in%20space%20exploration%2C%20where%0Aunpredictable%20environments%20are%20difficult%20to%20observe%20using%20only%20exteroceptive%0Asensors%20such%20as%20vision.%20Implementing%20Neural%20Network%20classifiers%20can%20have%20high%0Aperformance%20but%20can%20be%20deemed%20untrustworthy%20as%20they%20lack%20transparency%2C%20which%0Amakes%20them%20unreliable%20for%20taking%20high-stakes%20decisions%20during%20mission%20planning.%0AWe%20address%20this%20by%20proposing%20Neural%20Networks%20with%20Uncertainty%20Quantification%20in%0ATerrain%20Classification.%20We%20enable%20our%20Neural%20Networks%20with%20Monte%20Carlo%20Dropout%2C%0ADropConnect%2C%20and%20Flipout%20in%20time%20series-capable%20architectures%20using%20only%0Aproprioceptive%20data%20as%20input.%20We%20use%20Bayesian%20Optimization%20with%20Hyperband%20for%0Aefficient%20hyperparameter%20optimization%20to%20find%20optimal%20models%20for%20trustworthy%0Aterrain%20classification.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03241v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTerrain%2520Classification%2520Enhanced%2520with%2520Uncertainty%2520for%2520Space%2520Exploration%250A%2520%2520Robots%2520from%2520Proprioceptive%2520Data%26entry.906535625%3DMariela%2520De%2520Lucas%2520%25C3%2581lvarez%2520and%2520Jichen%2520Guo%2520and%2520Raul%2520Dom%25C3%25ADnguez%2520and%2520Matias%2520Valdenegro-Toro%26entry.1292438233%3D%2520%2520Terrain%2520Classification%2520is%2520an%2520essential%2520task%2520in%2520space%2520exploration%252C%2520where%250Aunpredictable%2520environments%2520are%2520difficult%2520to%2520observe%2520using%2520only%2520exteroceptive%250Asensors%2520such%2520as%2520vision.%2520Implementing%2520Neural%2520Network%2520classifiers%2520can%2520have%2520high%250Aperformance%2520but%2520can%2520be%2520deemed%2520untrustworthy%2520as%2520they%2520lack%2520transparency%252C%2520which%250Amakes%2520them%2520unreliable%2520for%2520taking%2520high-stakes%2520decisions%2520during%2520mission%2520planning.%250AWe%2520address%2520this%2520by%2520proposing%2520Neural%2520Networks%2520with%2520Uncertainty%2520Quantification%2520in%250ATerrain%2520Classification.%2520We%2520enable%2520our%2520Neural%2520Networks%2520with%2520Monte%2520Carlo%2520Dropout%252C%250ADropConnect%252C%2520and%2520Flipout%2520in%2520time%2520series-capable%2520architectures%2520using%2520only%250Aproprioceptive%2520data%2520as%2520input.%2520We%2520use%2520Bayesian%2520Optimization%2520with%2520Hyperband%2520for%250Aefficient%2520hyperparameter%2520optimization%2520to%2520find%2520optimal%2520models%2520for%2520trustworthy%250Aterrain%2520classification.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03241v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Terrain%20Classification%20Enhanced%20with%20Uncertainty%20for%20Space%20Exploration%0A%20%20Robots%20from%20Proprioceptive%20Data&entry.906535625=Mariela%20De%20Lucas%20%C3%81lvarez%20and%20Jichen%20Guo%20and%20Raul%20Dom%C3%ADnguez%20and%20Matias%20Valdenegro-Toro&entry.1292438233=%20%20Terrain%20Classification%20is%20an%20essential%20task%20in%20space%20exploration%2C%20where%0Aunpredictable%20environments%20are%20difficult%20to%20observe%20using%20only%20exteroceptive%0Asensors%20such%20as%20vision.%20Implementing%20Neural%20Network%20classifiers%20can%20have%20high%0Aperformance%20but%20can%20be%20deemed%20untrustworthy%20as%20they%20lack%20transparency%2C%20which%0Amakes%20them%20unreliable%20for%20taking%20high-stakes%20decisions%20during%20mission%20planning.%0AWe%20address%20this%20by%20proposing%20Neural%20Networks%20with%20Uncertainty%20Quantification%20in%0ATerrain%20Classification.%20We%20enable%20our%20Neural%20Networks%20with%20Monte%20Carlo%20Dropout%2C%0ADropConnect%2C%20and%20Flipout%20in%20time%20series-capable%20architectures%20using%20only%0Aproprioceptive%20data%20as%20input.%20We%20use%20Bayesian%20Optimization%20with%20Hyperband%20for%0Aefficient%20hyperparameter%20optimization%20to%20find%20optimal%20models%20for%20trustworthy%0Aterrain%20classification.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03241v1&entry.124074799=Read"},
{"title": "Revisiting the Performance of Deep Learning-Based Vulnerability\n  Detection on Realistic Datasets", "author": "Partha Chakraborty and Krishna Kanth Arumugam and Mahmoud Alfadel and Meiyappan Nagappan and Shane McIntosh", "abstract": "  The impact of software vulnerabilities on everyday software systems is\nsignificant. Despite deep learning models being proposed for vulnerability\ndetection, their reliability is questionable. Prior evaluations show high\nrecall/F1 scores of up to 99%, but these models underperform in practical\nscenarios, particularly when assessed on entire codebases rather than just the\nfixing commit. This paper introduces Real-Vul, a comprehensive dataset\nrepresenting real-world scenarios for evaluating vulnerability detection\nmodels. Evaluating DeepWukong, LineVul, ReVeal, and IVDetect shows a\nsignificant drop in performance, with precision decreasing by up to 95\npercentage points and F1 scores by up to 91 points. Furthermore, Model\nperformance fluctuates based on vulnerability characteristics, with better F1\nscores for information leaks or code injection than for path resolution or\npredictable return values. The results highlight a significant performance gap\nthat needs addressing before deploying deep learning-based vulnerability\ndetection in practical settings. Overfitting is identified as a key issue, and\nan augmentation technique is proposed, potentially improving performance by up\nto 30%. Contributions include a dataset creation approach for better model\nevaluation, Real-Vul dataset, and empirical evidence of deep learning models\nstruggling in real-world settings.\n", "link": "http://arxiv.org/abs/2407.03093v1", "date": "2024-07-03", "relevancy": 1.4162, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5143}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4802}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4519}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20the%20Performance%20of%20Deep%20Learning-Based%20Vulnerability%0A%20%20Detection%20on%20Realistic%20Datasets&body=Title%3A%20Revisiting%20the%20Performance%20of%20Deep%20Learning-Based%20Vulnerability%0A%20%20Detection%20on%20Realistic%20Datasets%0AAuthor%3A%20Partha%20Chakraborty%20and%20Krishna%20Kanth%20Arumugam%20and%20Mahmoud%20Alfadel%20and%20Meiyappan%20Nagappan%20and%20Shane%20McIntosh%0AAbstract%3A%20%20%20The%20impact%20of%20software%20vulnerabilities%20on%20everyday%20software%20systems%20is%0Asignificant.%20Despite%20deep%20learning%20models%20being%20proposed%20for%20vulnerability%0Adetection%2C%20their%20reliability%20is%20questionable.%20Prior%20evaluations%20show%20high%0Arecall/F1%20scores%20of%20up%20to%2099%25%2C%20but%20these%20models%20underperform%20in%20practical%0Ascenarios%2C%20particularly%20when%20assessed%20on%20entire%20codebases%20rather%20than%20just%20the%0Afixing%20commit.%20This%20paper%20introduces%20Real-Vul%2C%20a%20comprehensive%20dataset%0Arepresenting%20real-world%20scenarios%20for%20evaluating%20vulnerability%20detection%0Amodels.%20Evaluating%20DeepWukong%2C%20LineVul%2C%20ReVeal%2C%20and%20IVDetect%20shows%20a%0Asignificant%20drop%20in%20performance%2C%20with%20precision%20decreasing%20by%20up%20to%2095%0Apercentage%20points%20and%20F1%20scores%20by%20up%20to%2091%20points.%20Furthermore%2C%20Model%0Aperformance%20fluctuates%20based%20on%20vulnerability%20characteristics%2C%20with%20better%20F1%0Ascores%20for%20information%20leaks%20or%20code%20injection%20than%20for%20path%20resolution%20or%0Apredictable%20return%20values.%20The%20results%20highlight%20a%20significant%20performance%20gap%0Athat%20needs%20addressing%20before%20deploying%20deep%20learning-based%20vulnerability%0Adetection%20in%20practical%20settings.%20Overfitting%20is%20identified%20as%20a%20key%20issue%2C%20and%0Aan%20augmentation%20technique%20is%20proposed%2C%20potentially%20improving%20performance%20by%20up%0Ato%2030%25.%20Contributions%20include%20a%20dataset%20creation%20approach%20for%20better%20model%0Aevaluation%2C%20Real-Vul%20dataset%2C%20and%20empirical%20evidence%20of%20deep%20learning%20models%0Astruggling%20in%20real-world%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03093v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520the%2520Performance%2520of%2520Deep%2520Learning-Based%2520Vulnerability%250A%2520%2520Detection%2520on%2520Realistic%2520Datasets%26entry.906535625%3DPartha%2520Chakraborty%2520and%2520Krishna%2520Kanth%2520Arumugam%2520and%2520Mahmoud%2520Alfadel%2520and%2520Meiyappan%2520Nagappan%2520and%2520Shane%2520McIntosh%26entry.1292438233%3D%2520%2520The%2520impact%2520of%2520software%2520vulnerabilities%2520on%2520everyday%2520software%2520systems%2520is%250Asignificant.%2520Despite%2520deep%2520learning%2520models%2520being%2520proposed%2520for%2520vulnerability%250Adetection%252C%2520their%2520reliability%2520is%2520questionable.%2520Prior%2520evaluations%2520show%2520high%250Arecall/F1%2520scores%2520of%2520up%2520to%252099%2525%252C%2520but%2520these%2520models%2520underperform%2520in%2520practical%250Ascenarios%252C%2520particularly%2520when%2520assessed%2520on%2520entire%2520codebases%2520rather%2520than%2520just%2520the%250Afixing%2520commit.%2520This%2520paper%2520introduces%2520Real-Vul%252C%2520a%2520comprehensive%2520dataset%250Arepresenting%2520real-world%2520scenarios%2520for%2520evaluating%2520vulnerability%2520detection%250Amodels.%2520Evaluating%2520DeepWukong%252C%2520LineVul%252C%2520ReVeal%252C%2520and%2520IVDetect%2520shows%2520a%250Asignificant%2520drop%2520in%2520performance%252C%2520with%2520precision%2520decreasing%2520by%2520up%2520to%252095%250Apercentage%2520points%2520and%2520F1%2520scores%2520by%2520up%2520to%252091%2520points.%2520Furthermore%252C%2520Model%250Aperformance%2520fluctuates%2520based%2520on%2520vulnerability%2520characteristics%252C%2520with%2520better%2520F1%250Ascores%2520for%2520information%2520leaks%2520or%2520code%2520injection%2520than%2520for%2520path%2520resolution%2520or%250Apredictable%2520return%2520values.%2520The%2520results%2520highlight%2520a%2520significant%2520performance%2520gap%250Athat%2520needs%2520addressing%2520before%2520deploying%2520deep%2520learning-based%2520vulnerability%250Adetection%2520in%2520practical%2520settings.%2520Overfitting%2520is%2520identified%2520as%2520a%2520key%2520issue%252C%2520and%250Aan%2520augmentation%2520technique%2520is%2520proposed%252C%2520potentially%2520improving%2520performance%2520by%2520up%250Ato%252030%2525.%2520Contributions%2520include%2520a%2520dataset%2520creation%2520approach%2520for%2520better%2520model%250Aevaluation%252C%2520Real-Vul%2520dataset%252C%2520and%2520empirical%2520evidence%2520of%2520deep%2520learning%2520models%250Astruggling%2520in%2520real-world%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03093v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20the%20Performance%20of%20Deep%20Learning-Based%20Vulnerability%0A%20%20Detection%20on%20Realistic%20Datasets&entry.906535625=Partha%20Chakraborty%20and%20Krishna%20Kanth%20Arumugam%20and%20Mahmoud%20Alfadel%20and%20Meiyappan%20Nagappan%20and%20Shane%20McIntosh&entry.1292438233=%20%20The%20impact%20of%20software%20vulnerabilities%20on%20everyday%20software%20systems%20is%0Asignificant.%20Despite%20deep%20learning%20models%20being%20proposed%20for%20vulnerability%0Adetection%2C%20their%20reliability%20is%20questionable.%20Prior%20evaluations%20show%20high%0Arecall/F1%20scores%20of%20up%20to%2099%25%2C%20but%20these%20models%20underperform%20in%20practical%0Ascenarios%2C%20particularly%20when%20assessed%20on%20entire%20codebases%20rather%20than%20just%20the%0Afixing%20commit.%20This%20paper%20introduces%20Real-Vul%2C%20a%20comprehensive%20dataset%0Arepresenting%20real-world%20scenarios%20for%20evaluating%20vulnerability%20detection%0Amodels.%20Evaluating%20DeepWukong%2C%20LineVul%2C%20ReVeal%2C%20and%20IVDetect%20shows%20a%0Asignificant%20drop%20in%20performance%2C%20with%20precision%20decreasing%20by%20up%20to%2095%0Apercentage%20points%20and%20F1%20scores%20by%20up%20to%2091%20points.%20Furthermore%2C%20Model%0Aperformance%20fluctuates%20based%20on%20vulnerability%20characteristics%2C%20with%20better%20F1%0Ascores%20for%20information%20leaks%20or%20code%20injection%20than%20for%20path%20resolution%20or%0Apredictable%20return%20values.%20The%20results%20highlight%20a%20significant%20performance%20gap%0Athat%20needs%20addressing%20before%20deploying%20deep%20learning-based%20vulnerability%0Adetection%20in%20practical%20settings.%20Overfitting%20is%20identified%20as%20a%20key%20issue%2C%20and%0Aan%20augmentation%20technique%20is%20proposed%2C%20potentially%20improving%20performance%20by%20up%0Ato%2030%25.%20Contributions%20include%20a%20dataset%20creation%20approach%20for%20better%20model%0Aevaluation%2C%20Real-Vul%20dataset%2C%20and%20empirical%20evidence%20of%20deep%20learning%20models%0Astruggling%20in%20real-world%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03093v1&entry.124074799=Read"},
{"title": "Accelerated Proton Resonance Frequency-based Magnetic Resonance\n  Thermometry by Optimized Deep Learning Method", "author": "Sijie Xu and Shenyan Zong and Chang-Sheng Mei and Guofeng Shen and Yueran Zhao and He Wang", "abstract": "  Proton resonance frequency (PRF) based MR thermometry is essential for\nfocused ultrasound (FUS) thermal ablation therapies. This work aims to enhance\ntemporal resolution in dynamic MR temperature map reconstruction using an\nimproved deep learning method. The training-optimized methods and five\nclassical neural networks were applied on the 2-fold and 4-fold under-sampling\nk-space data to reconstruct the temperature maps. The enhanced training modules\nincluded offline/online data augmentations, knowledge distillation, and the\namplitude-phase decoupling loss function. The heating experiments were\nperformed by a FUS transducer on phantom and ex vivo tissues, respectively.\nThese data were manually under-sampled to imitate acceleration procedures and\ntrained in our method to get the reconstruction model. The additional dozen or\nso testing datasets were separately obtained for evaluating the real-time\nperformance and temperature accuracy. Acceleration factors of 1.9 and 3.7 were\nfound for 2 times and 4 times k-space under-sampling strategies and the\nResUNet-based deep learning reconstruction performed exceptionally well. In\n2-fold acceleration scenario, the RMSE of temperature map patches provided the\nvalues of 0.888 degree centigrade and 1.145 degree centigrade on phantom and ex\nvivo testing datasets. The DICE value of temperature areas enclosed by 43\ndegree centigrade isotherm was 0.809, and the Bland-Altman analysis showed a\nbias of -0.253 degree centigrade with the apart of plus or minus 2.16 degree\ncentigrade. In 4 times under-sampling case, these evaluating values decreased\nby approximately 10%. This study demonstrates that deep learning-based\nreconstruction can significantly enhance the accuracy and efficiency of MR\nthermometry for clinical FUS thermal therapies.\n", "link": "http://arxiv.org/abs/2407.03308v1", "date": "2024-07-03", "relevancy": 1.4332, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4813}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4769}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4762}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Accelerated%20Proton%20Resonance%20Frequency-based%20Magnetic%20Resonance%0A%20%20Thermometry%20by%20Optimized%20Deep%20Learning%20Method&body=Title%3A%20Accelerated%20Proton%20Resonance%20Frequency-based%20Magnetic%20Resonance%0A%20%20Thermometry%20by%20Optimized%20Deep%20Learning%20Method%0AAuthor%3A%20Sijie%20Xu%20and%20Shenyan%20Zong%20and%20Chang-Sheng%20Mei%20and%20Guofeng%20Shen%20and%20Yueran%20Zhao%20and%20He%20Wang%0AAbstract%3A%20%20%20Proton%20resonance%20frequency%20%28PRF%29%20based%20MR%20thermometry%20is%20essential%20for%0Afocused%20ultrasound%20%28FUS%29%20thermal%20ablation%20therapies.%20This%20work%20aims%20to%20enhance%0Atemporal%20resolution%20in%20dynamic%20MR%20temperature%20map%20reconstruction%20using%20an%0Aimproved%20deep%20learning%20method.%20The%20training-optimized%20methods%20and%20five%0Aclassical%20neural%20networks%20were%20applied%20on%20the%202-fold%20and%204-fold%20under-sampling%0Ak-space%20data%20to%20reconstruct%20the%20temperature%20maps.%20The%20enhanced%20training%20modules%0Aincluded%20offline/online%20data%20augmentations%2C%20knowledge%20distillation%2C%20and%20the%0Aamplitude-phase%20decoupling%20loss%20function.%20The%20heating%20experiments%20were%0Aperformed%20by%20a%20FUS%20transducer%20on%20phantom%20and%20ex%20vivo%20tissues%2C%20respectively.%0AThese%20data%20were%20manually%20under-sampled%20to%20imitate%20acceleration%20procedures%20and%0Atrained%20in%20our%20method%20to%20get%20the%20reconstruction%20model.%20The%20additional%20dozen%20or%0Aso%20testing%20datasets%20were%20separately%20obtained%20for%20evaluating%20the%20real-time%0Aperformance%20and%20temperature%20accuracy.%20Acceleration%20factors%20of%201.9%20and%203.7%20were%0Afound%20for%202%20times%20and%204%20times%20k-space%20under-sampling%20strategies%20and%20the%0AResUNet-based%20deep%20learning%20reconstruction%20performed%20exceptionally%20well.%20In%0A2-fold%20acceleration%20scenario%2C%20the%20RMSE%20of%20temperature%20map%20patches%20provided%20the%0Avalues%20of%200.888%20degree%20centigrade%20and%201.145%20degree%20centigrade%20on%20phantom%20and%20ex%0Avivo%20testing%20datasets.%20The%20DICE%20value%20of%20temperature%20areas%20enclosed%20by%2043%0Adegree%20centigrade%20isotherm%20was%200.809%2C%20and%20the%20Bland-Altman%20analysis%20showed%20a%0Abias%20of%20-0.253%20degree%20centigrade%20with%20the%20apart%20of%20plus%20or%20minus%202.16%20degree%0Acentigrade.%20In%204%20times%20under-sampling%20case%2C%20these%20evaluating%20values%20decreased%0Aby%20approximately%2010%25.%20This%20study%20demonstrates%20that%20deep%20learning-based%0Areconstruction%20can%20significantly%20enhance%20the%20accuracy%20and%20efficiency%20of%20MR%0Athermometry%20for%20clinical%20FUS%20thermal%20therapies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.03308v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAccelerated%2520Proton%2520Resonance%2520Frequency-based%2520Magnetic%2520Resonance%250A%2520%2520Thermometry%2520by%2520Optimized%2520Deep%2520Learning%2520Method%26entry.906535625%3DSijie%2520Xu%2520and%2520Shenyan%2520Zong%2520and%2520Chang-Sheng%2520Mei%2520and%2520Guofeng%2520Shen%2520and%2520Yueran%2520Zhao%2520and%2520He%2520Wang%26entry.1292438233%3D%2520%2520Proton%2520resonance%2520frequency%2520%2528PRF%2529%2520based%2520MR%2520thermometry%2520is%2520essential%2520for%250Afocused%2520ultrasound%2520%2528FUS%2529%2520thermal%2520ablation%2520therapies.%2520This%2520work%2520aims%2520to%2520enhance%250Atemporal%2520resolution%2520in%2520dynamic%2520MR%2520temperature%2520map%2520reconstruction%2520using%2520an%250Aimproved%2520deep%2520learning%2520method.%2520The%2520training-optimized%2520methods%2520and%2520five%250Aclassical%2520neural%2520networks%2520were%2520applied%2520on%2520the%25202-fold%2520and%25204-fold%2520under-sampling%250Ak-space%2520data%2520to%2520reconstruct%2520the%2520temperature%2520maps.%2520The%2520enhanced%2520training%2520modules%250Aincluded%2520offline/online%2520data%2520augmentations%252C%2520knowledge%2520distillation%252C%2520and%2520the%250Aamplitude-phase%2520decoupling%2520loss%2520function.%2520The%2520heating%2520experiments%2520were%250Aperformed%2520by%2520a%2520FUS%2520transducer%2520on%2520phantom%2520and%2520ex%2520vivo%2520tissues%252C%2520respectively.%250AThese%2520data%2520were%2520manually%2520under-sampled%2520to%2520imitate%2520acceleration%2520procedures%2520and%250Atrained%2520in%2520our%2520method%2520to%2520get%2520the%2520reconstruction%2520model.%2520The%2520additional%2520dozen%2520or%250Aso%2520testing%2520datasets%2520were%2520separately%2520obtained%2520for%2520evaluating%2520the%2520real-time%250Aperformance%2520and%2520temperature%2520accuracy.%2520Acceleration%2520factors%2520of%25201.9%2520and%25203.7%2520were%250Afound%2520for%25202%2520times%2520and%25204%2520times%2520k-space%2520under-sampling%2520strategies%2520and%2520the%250AResUNet-based%2520deep%2520learning%2520reconstruction%2520performed%2520exceptionally%2520well.%2520In%250A2-fold%2520acceleration%2520scenario%252C%2520the%2520RMSE%2520of%2520temperature%2520map%2520patches%2520provided%2520the%250Avalues%2520of%25200.888%2520degree%2520centigrade%2520and%25201.145%2520degree%2520centigrade%2520on%2520phantom%2520and%2520ex%250Avivo%2520testing%2520datasets.%2520The%2520DICE%2520value%2520of%2520temperature%2520areas%2520enclosed%2520by%252043%250Adegree%2520centigrade%2520isotherm%2520was%25200.809%252C%2520and%2520the%2520Bland-Altman%2520analysis%2520showed%2520a%250Abias%2520of%2520-0.253%2520degree%2520centigrade%2520with%2520the%2520apart%2520of%2520plus%2520or%2520minus%25202.16%2520degree%250Acentigrade.%2520In%25204%2520times%2520under-sampling%2520case%252C%2520these%2520evaluating%2520values%2520decreased%250Aby%2520approximately%252010%2525.%2520This%2520study%2520demonstrates%2520that%2520deep%2520learning-based%250Areconstruction%2520can%2520significantly%2520enhance%2520the%2520accuracy%2520and%2520efficiency%2520of%2520MR%250Athermometry%2520for%2520clinical%2520FUS%2520thermal%2520therapies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.03308v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Accelerated%20Proton%20Resonance%20Frequency-based%20Magnetic%20Resonance%0A%20%20Thermometry%20by%20Optimized%20Deep%20Learning%20Method&entry.906535625=Sijie%20Xu%20and%20Shenyan%20Zong%20and%20Chang-Sheng%20Mei%20and%20Guofeng%20Shen%20and%20Yueran%20Zhao%20and%20He%20Wang&entry.1292438233=%20%20Proton%20resonance%20frequency%20%28PRF%29%20based%20MR%20thermometry%20is%20essential%20for%0Afocused%20ultrasound%20%28FUS%29%20thermal%20ablation%20therapies.%20This%20work%20aims%20to%20enhance%0Atemporal%20resolution%20in%20dynamic%20MR%20temperature%20map%20reconstruction%20using%20an%0Aimproved%20deep%20learning%20method.%20The%20training-optimized%20methods%20and%20five%0Aclassical%20neural%20networks%20were%20applied%20on%20the%202-fold%20and%204-fold%20under-sampling%0Ak-space%20data%20to%20reconstruct%20the%20temperature%20maps.%20The%20enhanced%20training%20modules%0Aincluded%20offline/online%20data%20augmentations%2C%20knowledge%20distillation%2C%20and%20the%0Aamplitude-phase%20decoupling%20loss%20function.%20The%20heating%20experiments%20were%0Aperformed%20by%20a%20FUS%20transducer%20on%20phantom%20and%20ex%20vivo%20tissues%2C%20respectively.%0AThese%20data%20were%20manually%20under-sampled%20to%20imitate%20acceleration%20procedures%20and%0Atrained%20in%20our%20method%20to%20get%20the%20reconstruction%20model.%20The%20additional%20dozen%20or%0Aso%20testing%20datasets%20were%20separately%20obtained%20for%20evaluating%20the%20real-time%0Aperformance%20and%20temperature%20accuracy.%20Acceleration%20factors%20of%201.9%20and%203.7%20were%0Afound%20for%202%20times%20and%204%20times%20k-space%20under-sampling%20strategies%20and%20the%0AResUNet-based%20deep%20learning%20reconstruction%20performed%20exceptionally%20well.%20In%0A2-fold%20acceleration%20scenario%2C%20the%20RMSE%20of%20temperature%20map%20patches%20provided%20the%0Avalues%20of%200.888%20degree%20centigrade%20and%201.145%20degree%20centigrade%20on%20phantom%20and%20ex%0Avivo%20testing%20datasets.%20The%20DICE%20value%20of%20temperature%20areas%20enclosed%20by%2043%0Adegree%20centigrade%20isotherm%20was%200.809%2C%20and%20the%20Bland-Altman%20analysis%20showed%20a%0Abias%20of%20-0.253%20degree%20centigrade%20with%20the%20apart%20of%20plus%20or%20minus%202.16%20degree%0Acentigrade.%20In%204%20times%20under-sampling%20case%2C%20these%20evaluating%20values%20decreased%0Aby%20approximately%2010%25.%20This%20study%20demonstrates%20that%20deep%20learning-based%0Areconstruction%20can%20significantly%20enhance%20the%20accuracy%20and%20efficiency%20of%20MR%0Athermometry%20for%20clinical%20FUS%20thermal%20therapies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.03308v1&entry.124074799=Read"},
{"title": "Meta-Learning Based Optimization for Large Scale Wireless Systems", "author": "Rafael Cerna Loli and Bruno Clerckx", "abstract": "  Optimization algorithms for wireless systems play a fundamental role in\nimproving their performance and efficiency. However, it is known that the\ncomplexity of conventional optimization algorithms in the literature often\nexponentially increases with the number of transmit antennas and communication\nusers in the wireless system. Therefore, in the large scale regime, the\nastronomically large complexity of these optimization algorithms prohibits\ntheir use and prevents assessing large scale wireless systems performance under\noptimized conditions. To overcome this limitation, this work proposes instead\nthe use of an unsupervised meta-learning based approach to directly perform\nnon-convex optimization at significantly reduced complexity. To demonstrate the\neffectiveness of the proposed meta-learning based solution, the sum-rate (SR)\nmaximization problem for the following three emerging 6G technologies is\ncontemplated: hierarchical rate-splitting multiple access (H-RSMA), integrated\nsensing and communication (ISAC), and beyond-diagonal reconfigurable\nintelligent surfaces (BD-RIS). Through numerical results, it is demonstrated\nthat the proposed meta-learning based optimization framework is able to\nsuccessfully optimize the performance and also reveal unknown aspects of the\noperation in the large scale regime for the considered three 6G technologies.\n", "link": "http://arxiv.org/abs/2407.01823v2", "date": "2024-07-03", "relevancy": 1.7374, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4551}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4253}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4173}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Meta-Learning%20Based%20Optimization%20for%20Large%20Scale%20Wireless%20Systems&body=Title%3A%20Meta-Learning%20Based%20Optimization%20for%20Large%20Scale%20Wireless%20Systems%0AAuthor%3A%20Rafael%20Cerna%20Loli%20and%20Bruno%20Clerckx%0AAbstract%3A%20%20%20Optimization%20algorithms%20for%20wireless%20systems%20play%20a%20fundamental%20role%20in%0Aimproving%20their%20performance%20and%20efficiency.%20However%2C%20it%20is%20known%20that%20the%0Acomplexity%20of%20conventional%20optimization%20algorithms%20in%20the%20literature%20often%0Aexponentially%20increases%20with%20the%20number%20of%20transmit%20antennas%20and%20communication%0Ausers%20in%20the%20wireless%20system.%20Therefore%2C%20in%20the%20large%20scale%20regime%2C%20the%0Aastronomically%20large%20complexity%20of%20these%20optimization%20algorithms%20prohibits%0Atheir%20use%20and%20prevents%20assessing%20large%20scale%20wireless%20systems%20performance%20under%0Aoptimized%20conditions.%20To%20overcome%20this%20limitation%2C%20this%20work%20proposes%20instead%0Athe%20use%20of%20an%20unsupervised%20meta-learning%20based%20approach%20to%20directly%20perform%0Anon-convex%20optimization%20at%20significantly%20reduced%20complexity.%20To%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20meta-learning%20based%20solution%2C%20the%20sum-rate%20%28SR%29%0Amaximization%20problem%20for%20the%20following%20three%20emerging%206G%20technologies%20is%0Acontemplated%3A%20hierarchical%20rate-splitting%20multiple%20access%20%28H-RSMA%29%2C%20integrated%0Asensing%20and%20communication%20%28ISAC%29%2C%20and%20beyond-diagonal%20reconfigurable%0Aintelligent%20surfaces%20%28BD-RIS%29.%20Through%20numerical%20results%2C%20it%20is%20demonstrated%0Athat%20the%20proposed%20meta-learning%20based%20optimization%20framework%20is%20able%20to%0Asuccessfully%20optimize%20the%20performance%20and%20also%20reveal%20unknown%20aspects%20of%20the%0Aoperation%20in%20the%20large%20scale%20regime%20for%20the%20considered%20three%206G%20technologies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.01823v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMeta-Learning%2520Based%2520Optimization%2520for%2520Large%2520Scale%2520Wireless%2520Systems%26entry.906535625%3DRafael%2520Cerna%2520Loli%2520and%2520Bruno%2520Clerckx%26entry.1292438233%3D%2520%2520Optimization%2520algorithms%2520for%2520wireless%2520systems%2520play%2520a%2520fundamental%2520role%2520in%250Aimproving%2520their%2520performance%2520and%2520efficiency.%2520However%252C%2520it%2520is%2520known%2520that%2520the%250Acomplexity%2520of%2520conventional%2520optimization%2520algorithms%2520in%2520the%2520literature%2520often%250Aexponentially%2520increases%2520with%2520the%2520number%2520of%2520transmit%2520antennas%2520and%2520communication%250Ausers%2520in%2520the%2520wireless%2520system.%2520Therefore%252C%2520in%2520the%2520large%2520scale%2520regime%252C%2520the%250Aastronomically%2520large%2520complexity%2520of%2520these%2520optimization%2520algorithms%2520prohibits%250Atheir%2520use%2520and%2520prevents%2520assessing%2520large%2520scale%2520wireless%2520systems%2520performance%2520under%250Aoptimized%2520conditions.%2520To%2520overcome%2520this%2520limitation%252C%2520this%2520work%2520proposes%2520instead%250Athe%2520use%2520of%2520an%2520unsupervised%2520meta-learning%2520based%2520approach%2520to%2520directly%2520perform%250Anon-convex%2520optimization%2520at%2520significantly%2520reduced%2520complexity.%2520To%2520demonstrate%2520the%250Aeffectiveness%2520of%2520the%2520proposed%2520meta-learning%2520based%2520solution%252C%2520the%2520sum-rate%2520%2528SR%2529%250Amaximization%2520problem%2520for%2520the%2520following%2520three%2520emerging%25206G%2520technologies%2520is%250Acontemplated%253A%2520hierarchical%2520rate-splitting%2520multiple%2520access%2520%2528H-RSMA%2529%252C%2520integrated%250Asensing%2520and%2520communication%2520%2528ISAC%2529%252C%2520and%2520beyond-diagonal%2520reconfigurable%250Aintelligent%2520surfaces%2520%2528BD-RIS%2529.%2520Through%2520numerical%2520results%252C%2520it%2520is%2520demonstrated%250Athat%2520the%2520proposed%2520meta-learning%2520based%2520optimization%2520framework%2520is%2520able%2520to%250Asuccessfully%2520optimize%2520the%2520performance%2520and%2520also%2520reveal%2520unknown%2520aspects%2520of%2520the%250Aoperation%2520in%2520the%2520large%2520scale%2520regime%2520for%2520the%2520considered%2520three%25206G%2520technologies.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.01823v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Meta-Learning%20Based%20Optimization%20for%20Large%20Scale%20Wireless%20Systems&entry.906535625=Rafael%20Cerna%20Loli%20and%20Bruno%20Clerckx&entry.1292438233=%20%20Optimization%20algorithms%20for%20wireless%20systems%20play%20a%20fundamental%20role%20in%0Aimproving%20their%20performance%20and%20efficiency.%20However%2C%20it%20is%20known%20that%20the%0Acomplexity%20of%20conventional%20optimization%20algorithms%20in%20the%20literature%20often%0Aexponentially%20increases%20with%20the%20number%20of%20transmit%20antennas%20and%20communication%0Ausers%20in%20the%20wireless%20system.%20Therefore%2C%20in%20the%20large%20scale%20regime%2C%20the%0Aastronomically%20large%20complexity%20of%20these%20optimization%20algorithms%20prohibits%0Atheir%20use%20and%20prevents%20assessing%20large%20scale%20wireless%20systems%20performance%20under%0Aoptimized%20conditions.%20To%20overcome%20this%20limitation%2C%20this%20work%20proposes%20instead%0Athe%20use%20of%20an%20unsupervised%20meta-learning%20based%20approach%20to%20directly%20perform%0Anon-convex%20optimization%20at%20significantly%20reduced%20complexity.%20To%20demonstrate%20the%0Aeffectiveness%20of%20the%20proposed%20meta-learning%20based%20solution%2C%20the%20sum-rate%20%28SR%29%0Amaximization%20problem%20for%20the%20following%20three%20emerging%206G%20technologies%20is%0Acontemplated%3A%20hierarchical%20rate-splitting%20multiple%20access%20%28H-RSMA%29%2C%20integrated%0Asensing%20and%20communication%20%28ISAC%29%2C%20and%20beyond-diagonal%20reconfigurable%0Aintelligent%20surfaces%20%28BD-RIS%29.%20Through%20numerical%20results%2C%20it%20is%20demonstrated%0Athat%20the%20proposed%20meta-learning%20based%20optimization%20framework%20is%20able%20to%0Asuccessfully%20optimize%20the%20performance%20and%20also%20reveal%20unknown%20aspects%20of%20the%0Aoperation%20in%20the%20large%20scale%20regime%20for%20the%20considered%20three%206G%20technologies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.01823v2&entry.124074799=Read"},
{"title": "Found in the Middle: Calibrating Positional Attention Bias Improves Long\n  Context Utilization", "author": "Cheng-Yu Hsieh and Yung-Sung Chuang and Chun-Liang Li and Zifeng Wang and Long T. Le and Abhishek Kumar and James Glass and Alexander Ratner and Chen-Yu Lee and Ranjay Krishna and Tomas Pfister", "abstract": "  Large language models (LLMs), even when specifically trained to process long\ninput contexts, struggle to capture relevant information located in the middle\nof their input. This phenomenon has been known as the lost-in-the-middle\nproblem. In this work, we make three contributions. First, we set out to\nunderstand the factors that cause this phenomenon. In doing so, we establish a\nconnection between lost-in-the-middle to LLMs' intrinsic attention bias: LLMs\nexhibit a U-shaped attention bias where the tokens at the beginning and at the\nend of its input receive higher attention, regardless of their relevance.\nSecond, we mitigate this positional bias through a calibration mechanism,\nfound-in-the-middle, that allows the model to attend to contexts faithfully\naccording to their relevance, even though when they are in the middle. Third,\nwe show found-in-the-middle not only achieves better performance in locating\nrelevant information within a long context, but also eventually leads to\nimproved retrieval-augmented generation (RAG) performance across various tasks,\noutperforming existing methods by up to 15 percentage points. These findings\nopen up future directions in understanding LLM attention bias and its potential\nconsequences.\n", "link": "http://arxiv.org/abs/2406.16008v2", "date": "2024-07-03", "relevancy": 1.4938, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5121}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4966}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4928}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Found%20in%20the%20Middle%3A%20Calibrating%20Positional%20Attention%20Bias%20Improves%20Long%0A%20%20Context%20Utilization&body=Title%3A%20Found%20in%20the%20Middle%3A%20Calibrating%20Positional%20Attention%20Bias%20Improves%20Long%0A%20%20Context%20Utilization%0AAuthor%3A%20Cheng-Yu%20Hsieh%20and%20Yung-Sung%20Chuang%20and%20Chun-Liang%20Li%20and%20Zifeng%20Wang%20and%20Long%20T.%20Le%20and%20Abhishek%20Kumar%20and%20James%20Glass%20and%20Alexander%20Ratner%20and%20Chen-Yu%20Lee%20and%20Ranjay%20Krishna%20and%20Tomas%20Pfister%0AAbstract%3A%20%20%20Large%20language%20models%20%28LLMs%29%2C%20even%20when%20specifically%20trained%20to%20process%20long%0Ainput%20contexts%2C%20struggle%20to%20capture%20relevant%20information%20located%20in%20the%20middle%0Aof%20their%20input.%20This%20phenomenon%20has%20been%20known%20as%20the%20lost-in-the-middle%0Aproblem.%20In%20this%20work%2C%20we%20make%20three%20contributions.%20First%2C%20we%20set%20out%20to%0Aunderstand%20the%20factors%20that%20cause%20this%20phenomenon.%20In%20doing%20so%2C%20we%20establish%20a%0Aconnection%20between%20lost-in-the-middle%20to%20LLMs%27%20intrinsic%20attention%20bias%3A%20LLMs%0Aexhibit%20a%20U-shaped%20attention%20bias%20where%20the%20tokens%20at%20the%20beginning%20and%20at%20the%0Aend%20of%20its%20input%20receive%20higher%20attention%2C%20regardless%20of%20their%20relevance.%0ASecond%2C%20we%20mitigate%20this%20positional%20bias%20through%20a%20calibration%20mechanism%2C%0Afound-in-the-middle%2C%20that%20allows%20the%20model%20to%20attend%20to%20contexts%20faithfully%0Aaccording%20to%20their%20relevance%2C%20even%20though%20when%20they%20are%20in%20the%20middle.%20Third%2C%0Awe%20show%20found-in-the-middle%20not%20only%20achieves%20better%20performance%20in%20locating%0Arelevant%20information%20within%20a%20long%20context%2C%20but%20also%20eventually%20leads%20to%0Aimproved%20retrieval-augmented%20generation%20%28RAG%29%20performance%20across%20various%20tasks%2C%0Aoutperforming%20existing%20methods%20by%20up%20to%2015%20percentage%20points.%20These%20findings%0Aopen%20up%20future%20directions%20in%20understanding%20LLM%20attention%20bias%20and%20its%20potential%0Aconsequences.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16008v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFound%2520in%2520the%2520Middle%253A%2520Calibrating%2520Positional%2520Attention%2520Bias%2520Improves%2520Long%250A%2520%2520Context%2520Utilization%26entry.906535625%3DCheng-Yu%2520Hsieh%2520and%2520Yung-Sung%2520Chuang%2520and%2520Chun-Liang%2520Li%2520and%2520Zifeng%2520Wang%2520and%2520Long%2520T.%2520Le%2520and%2520Abhishek%2520Kumar%2520and%2520James%2520Glass%2520and%2520Alexander%2520Ratner%2520and%2520Chen-Yu%2520Lee%2520and%2520Ranjay%2520Krishna%2520and%2520Tomas%2520Pfister%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520%2528LLMs%2529%252C%2520even%2520when%2520specifically%2520trained%2520to%2520process%2520long%250Ainput%2520contexts%252C%2520struggle%2520to%2520capture%2520relevant%2520information%2520located%2520in%2520the%2520middle%250Aof%2520their%2520input.%2520This%2520phenomenon%2520has%2520been%2520known%2520as%2520the%2520lost-in-the-middle%250Aproblem.%2520In%2520this%2520work%252C%2520we%2520make%2520three%2520contributions.%2520First%252C%2520we%2520set%2520out%2520to%250Aunderstand%2520the%2520factors%2520that%2520cause%2520this%2520phenomenon.%2520In%2520doing%2520so%252C%2520we%2520establish%2520a%250Aconnection%2520between%2520lost-in-the-middle%2520to%2520LLMs%2527%2520intrinsic%2520attention%2520bias%253A%2520LLMs%250Aexhibit%2520a%2520U-shaped%2520attention%2520bias%2520where%2520the%2520tokens%2520at%2520the%2520beginning%2520and%2520at%2520the%250Aend%2520of%2520its%2520input%2520receive%2520higher%2520attention%252C%2520regardless%2520of%2520their%2520relevance.%250ASecond%252C%2520we%2520mitigate%2520this%2520positional%2520bias%2520through%2520a%2520calibration%2520mechanism%252C%250Afound-in-the-middle%252C%2520that%2520allows%2520the%2520model%2520to%2520attend%2520to%2520contexts%2520faithfully%250Aaccording%2520to%2520their%2520relevance%252C%2520even%2520though%2520when%2520they%2520are%2520in%2520the%2520middle.%2520Third%252C%250Awe%2520show%2520found-in-the-middle%2520not%2520only%2520achieves%2520better%2520performance%2520in%2520locating%250Arelevant%2520information%2520within%2520a%2520long%2520context%252C%2520but%2520also%2520eventually%2520leads%2520to%250Aimproved%2520retrieval-augmented%2520generation%2520%2528RAG%2529%2520performance%2520across%2520various%2520tasks%252C%250Aoutperforming%2520existing%2520methods%2520by%2520up%2520to%252015%2520percentage%2520points.%2520These%2520findings%250Aopen%2520up%2520future%2520directions%2520in%2520understanding%2520LLM%2520attention%2520bias%2520and%2520its%2520potential%250Aconsequences.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16008v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Found%20in%20the%20Middle%3A%20Calibrating%20Positional%20Attention%20Bias%20Improves%20Long%0A%20%20Context%20Utilization&entry.906535625=Cheng-Yu%20Hsieh%20and%20Yung-Sung%20Chuang%20and%20Chun-Liang%20Li%20and%20Zifeng%20Wang%20and%20Long%20T.%20Le%20and%20Abhishek%20Kumar%20and%20James%20Glass%20and%20Alexander%20Ratner%20and%20Chen-Yu%20Lee%20and%20Ranjay%20Krishna%20and%20Tomas%20Pfister&entry.1292438233=%20%20Large%20language%20models%20%28LLMs%29%2C%20even%20when%20specifically%20trained%20to%20process%20long%0Ainput%20contexts%2C%20struggle%20to%20capture%20relevant%20information%20located%20in%20the%20middle%0Aof%20their%20input.%20This%20phenomenon%20has%20been%20known%20as%20the%20lost-in-the-middle%0Aproblem.%20In%20this%20work%2C%20we%20make%20three%20contributions.%20First%2C%20we%20set%20out%20to%0Aunderstand%20the%20factors%20that%20cause%20this%20phenomenon.%20In%20doing%20so%2C%20we%20establish%20a%0Aconnection%20between%20lost-in-the-middle%20to%20LLMs%27%20intrinsic%20attention%20bias%3A%20LLMs%0Aexhibit%20a%20U-shaped%20attention%20bias%20where%20the%20tokens%20at%20the%20beginning%20and%20at%20the%0Aend%20of%20its%20input%20receive%20higher%20attention%2C%20regardless%20of%20their%20relevance.%0ASecond%2C%20we%20mitigate%20this%20positional%20bias%20through%20a%20calibration%20mechanism%2C%0Afound-in-the-middle%2C%20that%20allows%20the%20model%20to%20attend%20to%20contexts%20faithfully%0Aaccording%20to%20their%20relevance%2C%20even%20though%20when%20they%20are%20in%20the%20middle.%20Third%2C%0Awe%20show%20found-in-the-middle%20not%20only%20achieves%20better%20performance%20in%20locating%0Arelevant%20information%20within%20a%20long%20context%2C%20but%20also%20eventually%20leads%20to%0Aimproved%20retrieval-augmented%20generation%20%28RAG%29%20performance%20across%20various%20tasks%2C%0Aoutperforming%20existing%20methods%20by%20up%20to%2015%20percentage%20points.%20These%20findings%0Aopen%20up%20future%20directions%20in%20understanding%20LLM%20attention%20bias%20and%20its%20potential%0Aconsequences.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16008v2&entry.124074799=Read"},
{"title": "LLM-Oracle Machines", "author": "Jie Wang", "abstract": "  Contemporary AI applications leverage large language models (LLMs) to harness\ntheir knowledge and reasoning abilities for natural language processing tasks.\nThis approach shares similarities with the concept of oracle Turing machines\n(OTMs). To capture the broader potential of these computations, including those\nnot yet realized, we propose an extension to OTMs: the LLM-oracle machine\n(LLM-OM), by employing a cluster of LLMs as the oracle. Each LLM acts as a\nblack box, capable of answering queries within its expertise, albeit with a\ndelay. We introduce four variants of the LLM-OM: basic, augmented,\nfault-avoidance, and $\\epsilon$-fault. The first two are commonly observed in\nexisting AI applications. The latter two are specifically designed to address\nthe challenges of LLM hallucinations, biases, and inconsistencies, aiming to\nensure reliable outcomes.\n", "link": "http://arxiv.org/abs/2406.12213v3", "date": "2024-07-03", "relevancy": 1.3551, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4647}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4513}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4398}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLM-Oracle%20Machines&body=Title%3A%20LLM-Oracle%20Machines%0AAuthor%3A%20Jie%20Wang%0AAbstract%3A%20%20%20Contemporary%20AI%20applications%20leverage%20large%20language%20models%20%28LLMs%29%20to%20harness%0Atheir%20knowledge%20and%20reasoning%20abilities%20for%20natural%20language%20processing%20tasks.%0AThis%20approach%20shares%20similarities%20with%20the%20concept%20of%20oracle%20Turing%20machines%0A%28OTMs%29.%20To%20capture%20the%20broader%20potential%20of%20these%20computations%2C%20including%20those%0Anot%20yet%20realized%2C%20we%20propose%20an%20extension%20to%20OTMs%3A%20the%20LLM-oracle%20machine%0A%28LLM-OM%29%2C%20by%20employing%20a%20cluster%20of%20LLMs%20as%20the%20oracle.%20Each%20LLM%20acts%20as%20a%0Ablack%20box%2C%20capable%20of%20answering%20queries%20within%20its%20expertise%2C%20albeit%20with%20a%0Adelay.%20We%20introduce%20four%20variants%20of%20the%20LLM-OM%3A%20basic%2C%20augmented%2C%0Afault-avoidance%2C%20and%20%24%5Cepsilon%24-fault.%20The%20first%20two%20are%20commonly%20observed%20in%0Aexisting%20AI%20applications.%20The%20latter%20two%20are%20specifically%20designed%20to%20address%0Athe%20challenges%20of%20LLM%20hallucinations%2C%20biases%2C%20and%20inconsistencies%2C%20aiming%20to%0Aensure%20reliable%20outcomes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.12213v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLM-Oracle%2520Machines%26entry.906535625%3DJie%2520Wang%26entry.1292438233%3D%2520%2520Contemporary%2520AI%2520applications%2520leverage%2520large%2520language%2520models%2520%2528LLMs%2529%2520to%2520harness%250Atheir%2520knowledge%2520and%2520reasoning%2520abilities%2520for%2520natural%2520language%2520processing%2520tasks.%250AThis%2520approach%2520shares%2520similarities%2520with%2520the%2520concept%2520of%2520oracle%2520Turing%2520machines%250A%2528OTMs%2529.%2520To%2520capture%2520the%2520broader%2520potential%2520of%2520these%2520computations%252C%2520including%2520those%250Anot%2520yet%2520realized%252C%2520we%2520propose%2520an%2520extension%2520to%2520OTMs%253A%2520the%2520LLM-oracle%2520machine%250A%2528LLM-OM%2529%252C%2520by%2520employing%2520a%2520cluster%2520of%2520LLMs%2520as%2520the%2520oracle.%2520Each%2520LLM%2520acts%2520as%2520a%250Ablack%2520box%252C%2520capable%2520of%2520answering%2520queries%2520within%2520its%2520expertise%252C%2520albeit%2520with%2520a%250Adelay.%2520We%2520introduce%2520four%2520variants%2520of%2520the%2520LLM-OM%253A%2520basic%252C%2520augmented%252C%250Afault-avoidance%252C%2520and%2520%2524%255Cepsilon%2524-fault.%2520The%2520first%2520two%2520are%2520commonly%2520observed%2520in%250Aexisting%2520AI%2520applications.%2520The%2520latter%2520two%2520are%2520specifically%2520designed%2520to%2520address%250Athe%2520challenges%2520of%2520LLM%2520hallucinations%252C%2520biases%252C%2520and%2520inconsistencies%252C%2520aiming%2520to%250Aensure%2520reliable%2520outcomes.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.12213v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLM-Oracle%20Machines&entry.906535625=Jie%20Wang&entry.1292438233=%20%20Contemporary%20AI%20applications%20leverage%20large%20language%20models%20%28LLMs%29%20to%20harness%0Atheir%20knowledge%20and%20reasoning%20abilities%20for%20natural%20language%20processing%20tasks.%0AThis%20approach%20shares%20similarities%20with%20the%20concept%20of%20oracle%20Turing%20machines%0A%28OTMs%29.%20To%20capture%20the%20broader%20potential%20of%20these%20computations%2C%20including%20those%0Anot%20yet%20realized%2C%20we%20propose%20an%20extension%20to%20OTMs%3A%20the%20LLM-oracle%20machine%0A%28LLM-OM%29%2C%20by%20employing%20a%20cluster%20of%20LLMs%20as%20the%20oracle.%20Each%20LLM%20acts%20as%20a%0Ablack%20box%2C%20capable%20of%20answering%20queries%20within%20its%20expertise%2C%20albeit%20with%20a%0Adelay.%20We%20introduce%20four%20variants%20of%20the%20LLM-OM%3A%20basic%2C%20augmented%2C%0Afault-avoidance%2C%20and%20%24%5Cepsilon%24-fault.%20The%20first%20two%20are%20commonly%20observed%20in%0Aexisting%20AI%20applications.%20The%20latter%20two%20are%20specifically%20designed%20to%20address%0Athe%20challenges%20of%20LLM%20hallucinations%2C%20biases%2C%20and%20inconsistencies%2C%20aiming%20to%0Aensure%20reliable%20outcomes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.12213v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


