<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20260119.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "FastGHA: Generalized Few-Shot 3D Gaussian Head Avatars with Real-Time Animation", "author": "Xinya Ji and Sebastian Weiss and Manuel Kansy and Jacek Naruniec and Xun Cao and Barbara Solenthaler and Derek Bradley", "abstract": "Despite recent progress in 3D Gaussian-based head avatar modeling, efficiently generating high fidelity avatars remains a challenge. Current methods typically rely on extensive multi-view capture setups or monocular videos with per-identity optimization during inference, limiting their scalability and ease of use on unseen subjects. To overcome these efficiency drawbacks, we propose \\OURS, a feed-forward method to generate high-quality Gaussian head avatars from only a few input images while supporting real-time animation. Our approach directly learns a per-pixel Gaussian representation from the input images, and aggregates multi-view information using a transformer-based encoder that fuses image features from both DINOv3 and Stable Diffusion VAE. For real-time animation, we extend the explicit Gaussian representations with per-Gaussian features and introduce a lightweight MLP-based dynamic network to predict 3D Gaussian deformations from expression codes. Furthermore, to enhance geometric smoothness of the 3D head, we employ point maps from a pre-trained large reconstruction model as geometry supervision. Experiments show that our approach significantly outperforms existing methods in both rendering quality and inference efficiency, while supporting real-time dynamic avatar animation.", "link": "http://arxiv.org/abs/2601.13837v1", "date": "2026-01-20", "relevancy": 3.7324, "topK": [{"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7901}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.7901}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6593}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FastGHA%3A%20Generalized%20Few-Shot%203D%20Gaussian%20Head%20Avatars%20with%20Real-Time%20Animation&body=Title%3A%20FastGHA%3A%20Generalized%20Few-Shot%203D%20Gaussian%20Head%20Avatars%20with%20Real-Time%20Animation%0AAuthor%3A%20Xinya%20Ji%20and%20Sebastian%20Weiss%20and%20Manuel%20Kansy%20and%20Jacek%20Naruniec%20and%20Xun%20Cao%20and%20Barbara%20Solenthaler%20and%20Derek%20Bradley%0AAbstract%3A%20Despite%20recent%20progress%20in%203D%20Gaussian-based%20head%20avatar%20modeling%2C%20efficiently%20generating%20high%20fidelity%20avatars%20remains%20a%20challenge.%20Current%20methods%20typically%20rely%20on%20extensive%20multi-view%20capture%20setups%20or%20monocular%20videos%20with%20per-identity%20optimization%20during%20inference%2C%20limiting%20their%20scalability%20and%20ease%20of%20use%20on%20unseen%20subjects.%20To%20overcome%20these%20efficiency%20drawbacks%2C%20we%20propose%20%5COURS%2C%20a%20feed-forward%20method%20to%20generate%20high-quality%20Gaussian%20head%20avatars%20from%20only%20a%20few%20input%20images%20while%20supporting%20real-time%20animation.%20Our%20approach%20directly%20learns%20a%20per-pixel%20Gaussian%20representation%20from%20the%20input%20images%2C%20and%20aggregates%20multi-view%20information%20using%20a%20transformer-based%20encoder%20that%20fuses%20image%20features%20from%20both%20DINOv3%20and%20Stable%20Diffusion%20VAE.%20For%20real-time%20animation%2C%20we%20extend%20the%20explicit%20Gaussian%20representations%20with%20per-Gaussian%20features%20and%20introduce%20a%20lightweight%20MLP-based%20dynamic%20network%20to%20predict%203D%20Gaussian%20deformations%20from%20expression%20codes.%20Furthermore%2C%20to%20enhance%20geometric%20smoothness%20of%20the%203D%20head%2C%20we%20employ%20point%20maps%20from%20a%20pre-trained%20large%20reconstruction%20model%20as%20geometry%20supervision.%20Experiments%20show%20that%20our%20approach%20significantly%20outperforms%20existing%20methods%20in%20both%20rendering%20quality%20and%20inference%20efficiency%2C%20while%20supporting%20real-time%20dynamic%20avatar%20animation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13837v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFastGHA%253A%2520Generalized%2520Few-Shot%25203D%2520Gaussian%2520Head%2520Avatars%2520with%2520Real-Time%2520Animation%26entry.906535625%3DXinya%2520Ji%2520and%2520Sebastian%2520Weiss%2520and%2520Manuel%2520Kansy%2520and%2520Jacek%2520Naruniec%2520and%2520Xun%2520Cao%2520and%2520Barbara%2520Solenthaler%2520and%2520Derek%2520Bradley%26entry.1292438233%3DDespite%2520recent%2520progress%2520in%25203D%2520Gaussian-based%2520head%2520avatar%2520modeling%252C%2520efficiently%2520generating%2520high%2520fidelity%2520avatars%2520remains%2520a%2520challenge.%2520Current%2520methods%2520typically%2520rely%2520on%2520extensive%2520multi-view%2520capture%2520setups%2520or%2520monocular%2520videos%2520with%2520per-identity%2520optimization%2520during%2520inference%252C%2520limiting%2520their%2520scalability%2520and%2520ease%2520of%2520use%2520on%2520unseen%2520subjects.%2520To%2520overcome%2520these%2520efficiency%2520drawbacks%252C%2520we%2520propose%2520%255COURS%252C%2520a%2520feed-forward%2520method%2520to%2520generate%2520high-quality%2520Gaussian%2520head%2520avatars%2520from%2520only%2520a%2520few%2520input%2520images%2520while%2520supporting%2520real-time%2520animation.%2520Our%2520approach%2520directly%2520learns%2520a%2520per-pixel%2520Gaussian%2520representation%2520from%2520the%2520input%2520images%252C%2520and%2520aggregates%2520multi-view%2520information%2520using%2520a%2520transformer-based%2520encoder%2520that%2520fuses%2520image%2520features%2520from%2520both%2520DINOv3%2520and%2520Stable%2520Diffusion%2520VAE.%2520For%2520real-time%2520animation%252C%2520we%2520extend%2520the%2520explicit%2520Gaussian%2520representations%2520with%2520per-Gaussian%2520features%2520and%2520introduce%2520a%2520lightweight%2520MLP-based%2520dynamic%2520network%2520to%2520predict%25203D%2520Gaussian%2520deformations%2520from%2520expression%2520codes.%2520Furthermore%252C%2520to%2520enhance%2520geometric%2520smoothness%2520of%2520the%25203D%2520head%252C%2520we%2520employ%2520point%2520maps%2520from%2520a%2520pre-trained%2520large%2520reconstruction%2520model%2520as%2520geometry%2520supervision.%2520Experiments%2520show%2520that%2520our%2520approach%2520significantly%2520outperforms%2520existing%2520methods%2520in%2520both%2520rendering%2520quality%2520and%2520inference%2520efficiency%252C%2520while%2520supporting%2520real-time%2520dynamic%2520avatar%2520animation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13837v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FastGHA%3A%20Generalized%20Few-Shot%203D%20Gaussian%20Head%20Avatars%20with%20Real-Time%20Animation&entry.906535625=Xinya%20Ji%20and%20Sebastian%20Weiss%20and%20Manuel%20Kansy%20and%20Jacek%20Naruniec%20and%20Xun%20Cao%20and%20Barbara%20Solenthaler%20and%20Derek%20Bradley&entry.1292438233=Despite%20recent%20progress%20in%203D%20Gaussian-based%20head%20avatar%20modeling%2C%20efficiently%20generating%20high%20fidelity%20avatars%20remains%20a%20challenge.%20Current%20methods%20typically%20rely%20on%20extensive%20multi-view%20capture%20setups%20or%20monocular%20videos%20with%20per-identity%20optimization%20during%20inference%2C%20limiting%20their%20scalability%20and%20ease%20of%20use%20on%20unseen%20subjects.%20To%20overcome%20these%20efficiency%20drawbacks%2C%20we%20propose%20%5COURS%2C%20a%20feed-forward%20method%20to%20generate%20high-quality%20Gaussian%20head%20avatars%20from%20only%20a%20few%20input%20images%20while%20supporting%20real-time%20animation.%20Our%20approach%20directly%20learns%20a%20per-pixel%20Gaussian%20representation%20from%20the%20input%20images%2C%20and%20aggregates%20multi-view%20information%20using%20a%20transformer-based%20encoder%20that%20fuses%20image%20features%20from%20both%20DINOv3%20and%20Stable%20Diffusion%20VAE.%20For%20real-time%20animation%2C%20we%20extend%20the%20explicit%20Gaussian%20representations%20with%20per-Gaussian%20features%20and%20introduce%20a%20lightweight%20MLP-based%20dynamic%20network%20to%20predict%203D%20Gaussian%20deformations%20from%20expression%20codes.%20Furthermore%2C%20to%20enhance%20geometric%20smoothness%20of%20the%203D%20head%2C%20we%20employ%20point%20maps%20from%20a%20pre-trained%20large%20reconstruction%20model%20as%20geometry%20supervision.%20Experiments%20show%20that%20our%20approach%20significantly%20outperforms%20existing%20methods%20in%20both%20rendering%20quality%20and%20inference%20efficiency%2C%20while%20supporting%20real-time%20dynamic%20avatar%20animation.&entry.1838667208=http%3A//arxiv.org/abs/2601.13837v1&entry.124074799=Read"},
{"title": "Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis", "author": "Hongyuan Chen and Xingyu Chen and Youjia Zhang and Zexiang Xu and Anpei Chen", "abstract": "We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.", "link": "http://arxiv.org/abs/2601.14253v1", "date": "2026-01-20", "relevancy": 3.3575, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7282}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6521}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6342}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Motion%203-to-4%3A%203D%20Motion%20Reconstruction%20for%204D%20Synthesis&body=Title%3A%20Motion%203-to-4%3A%203D%20Motion%20Reconstruction%20for%204D%20Synthesis%0AAuthor%3A%20Hongyuan%20Chen%20and%20Xingyu%20Chen%20and%20Youjia%20Zhang%20and%20Zexiang%20Xu%20and%20Anpei%20Chen%0AAbstract%3A%20We%20present%20Motion%203-to-4%2C%20a%20feed-forward%20framework%20for%20synthesising%20high-quality%204D%20dynamic%20objects%20from%20a%20single%20monocular%20video%20and%20an%20optional%203D%20reference%20mesh.%20While%20recent%20advances%20have%20significantly%20improved%202D%2C%20video%2C%20and%203D%20content%20generation%2C%204D%20synthesis%20remains%20difficult%20due%20to%20limited%20training%20data%20and%20the%20inherent%20ambiguity%20of%20recovering%20geometry%20and%20motion%20from%20a%20monocular%20viewpoint.%20Motion%203-to-4%20addresses%20these%20challenges%20by%20decomposing%204D%20synthesis%20into%20static%203D%20shape%20generation%20and%20motion%20reconstruction.%20Using%20a%20canonical%20reference%20mesh%2C%20our%20model%20learns%20a%20compact%20motion%20latent%20representation%20and%20predicts%20per-frame%20vertex%20trajectories%20to%20recover%20complete%2C%20temporally%20coherent%20geometry.%20A%20scalable%20frame-wise%20transformer%20further%20enables%20robustness%20to%20varying%20sequence%20lengths.%20Evaluations%20on%20both%20standard%20benchmarks%20and%20a%20new%20dataset%20with%20accurate%20ground-truth%20geometry%20show%20that%20Motion%203-to-4%20delivers%20superior%20fidelity%20and%20spatial%20consistency%20compared%20to%20prior%20work.%20Project%20page%20is%20available%20at%20https%3A//motion3-to-4.github.io/.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14253v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMotion%25203-to-4%253A%25203D%2520Motion%2520Reconstruction%2520for%25204D%2520Synthesis%26entry.906535625%3DHongyuan%2520Chen%2520and%2520Xingyu%2520Chen%2520and%2520Youjia%2520Zhang%2520and%2520Zexiang%2520Xu%2520and%2520Anpei%2520Chen%26entry.1292438233%3DWe%2520present%2520Motion%25203-to-4%252C%2520a%2520feed-forward%2520framework%2520for%2520synthesising%2520high-quality%25204D%2520dynamic%2520objects%2520from%2520a%2520single%2520monocular%2520video%2520and%2520an%2520optional%25203D%2520reference%2520mesh.%2520While%2520recent%2520advances%2520have%2520significantly%2520improved%25202D%252C%2520video%252C%2520and%25203D%2520content%2520generation%252C%25204D%2520synthesis%2520remains%2520difficult%2520due%2520to%2520limited%2520training%2520data%2520and%2520the%2520inherent%2520ambiguity%2520of%2520recovering%2520geometry%2520and%2520motion%2520from%2520a%2520monocular%2520viewpoint.%2520Motion%25203-to-4%2520addresses%2520these%2520challenges%2520by%2520decomposing%25204D%2520synthesis%2520into%2520static%25203D%2520shape%2520generation%2520and%2520motion%2520reconstruction.%2520Using%2520a%2520canonical%2520reference%2520mesh%252C%2520our%2520model%2520learns%2520a%2520compact%2520motion%2520latent%2520representation%2520and%2520predicts%2520per-frame%2520vertex%2520trajectories%2520to%2520recover%2520complete%252C%2520temporally%2520coherent%2520geometry.%2520A%2520scalable%2520frame-wise%2520transformer%2520further%2520enables%2520robustness%2520to%2520varying%2520sequence%2520lengths.%2520Evaluations%2520on%2520both%2520standard%2520benchmarks%2520and%2520a%2520new%2520dataset%2520with%2520accurate%2520ground-truth%2520geometry%2520show%2520that%2520Motion%25203-to-4%2520delivers%2520superior%2520fidelity%2520and%2520spatial%2520consistency%2520compared%2520to%2520prior%2520work.%2520Project%2520page%2520is%2520available%2520at%2520https%253A//motion3-to-4.github.io/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14253v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Motion%203-to-4%3A%203D%20Motion%20Reconstruction%20for%204D%20Synthesis&entry.906535625=Hongyuan%20Chen%20and%20Xingyu%20Chen%20and%20Youjia%20Zhang%20and%20Zexiang%20Xu%20and%20Anpei%20Chen&entry.1292438233=We%20present%20Motion%203-to-4%2C%20a%20feed-forward%20framework%20for%20synthesising%20high-quality%204D%20dynamic%20objects%20from%20a%20single%20monocular%20video%20and%20an%20optional%203D%20reference%20mesh.%20While%20recent%20advances%20have%20significantly%20improved%202D%2C%20video%2C%20and%203D%20content%20generation%2C%204D%20synthesis%20remains%20difficult%20due%20to%20limited%20training%20data%20and%20the%20inherent%20ambiguity%20of%20recovering%20geometry%20and%20motion%20from%20a%20monocular%20viewpoint.%20Motion%203-to-4%20addresses%20these%20challenges%20by%20decomposing%204D%20synthesis%20into%20static%203D%20shape%20generation%20and%20motion%20reconstruction.%20Using%20a%20canonical%20reference%20mesh%2C%20our%20model%20learns%20a%20compact%20motion%20latent%20representation%20and%20predicts%20per-frame%20vertex%20trajectories%20to%20recover%20complete%2C%20temporally%20coherent%20geometry.%20A%20scalable%20frame-wise%20transformer%20further%20enables%20robustness%20to%20varying%20sequence%20lengths.%20Evaluations%20on%20both%20standard%20benchmarks%20and%20a%20new%20dataset%20with%20accurate%20ground-truth%20geometry%20show%20that%20Motion%203-to-4%20delivers%20superior%20fidelity%20and%20spatial%20consistency%20compared%20to%20prior%20work.%20Project%20page%20is%20available%20at%20https%3A//motion3-to-4.github.io/.&entry.1838667208=http%3A//arxiv.org/abs/2601.14253v1&entry.124074799=Read"},
{"title": "Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting", "author": "Nitin Kulkarni and Akhil Devarashetti and Charlie Cluss and Livio Forte and Dan Buckmaster and Philip Schneider and Chunming Qiao and Alina Vereshchaka", "abstract": "Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage. The 3D model enables inspectors and customers to rotate, zoom, and slice through the undercarriage, allowing them to detect rust, leaks, or impact damage in seconds, thereby improving both workplace safety and buyer confidence. Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes. Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig. We use a constrained matching strategy with learned components, the DISK feature extractor, and the attention-based LightGlue matcher to generate high-quality sparse point clouds that are often unattainable with standard SfM pipelines. These point clouds seed the Gaussian splatting process to generate photorealistic undercarriage models that render in real-time. Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.", "link": "http://arxiv.org/abs/2601.14208v1", "date": "2026-01-20", "relevancy": 3.2624, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6834}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6514}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6227}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rig-Aware%203D%20Reconstruction%20of%20Vehicle%20Undercarriages%20using%20Gaussian%20Splatting&body=Title%3A%20Rig-Aware%203D%20Reconstruction%20of%20Vehicle%20Undercarriages%20using%20Gaussian%20Splatting%0AAuthor%3A%20Nitin%20Kulkarni%20and%20Akhil%20Devarashetti%20and%20Charlie%20Cluss%20and%20Livio%20Forte%20and%20Dan%20Buckmaster%20and%20Philip%20Schneider%20and%20Chunming%20Qiao%20and%20Alina%20Vereshchaka%0AAbstract%3A%20Inspecting%20the%20undercarriage%20of%20used%20vehicles%20is%20a%20labor-intensive%20task%20that%20requires%20inspectors%20to%20crouch%20or%20crawl%20underneath%20each%20vehicle%20to%20thoroughly%20examine%20it.%20Additionally%2C%20online%20buyers%20rarely%20see%20undercarriage%20photos.%20We%20present%20an%20end-to-end%20pipeline%20that%20utilizes%20a%20three-camera%20rig%20to%20capture%20videos%20of%20the%20undercarriage%20as%20the%20vehicle%20drives%20over%20it%2C%20and%20produces%20an%20interactive%203D%20model%20of%20the%20undercarriage.%20The%203D%20model%20enables%20inspectors%20and%20customers%20to%20rotate%2C%20zoom%2C%20and%20slice%20through%20the%20undercarriage%2C%20allowing%20them%20to%20detect%20rust%2C%20leaks%2C%20or%20impact%20damage%20in%20seconds%2C%20thereby%20improving%20both%20workplace%20safety%20and%20buyer%20confidence.%20Our%20primary%20contribution%20is%20a%20rig-aware%20Structure-from-Motion%20%28SfM%29%20pipeline%20specifically%20designed%20to%20overcome%20the%20challenges%20of%20wide-angle%20lens%20distortion%20and%20low-parallax%20scenes.%20Our%20method%20overcomes%20the%20challenges%20of%20wide-angle%20lens%20distortion%20and%20low-parallax%20scenes%20by%20integrating%20precise%20camera%20calibration%2C%20synchronized%20video%20streams%2C%20and%20strong%20geometric%20priors%20from%20the%20camera%20rig.%20We%20use%20a%20constrained%20matching%20strategy%20with%20learned%20components%2C%20the%20DISK%20feature%20extractor%2C%20and%20the%20attention-based%20LightGlue%20matcher%20to%20generate%20high-quality%20sparse%20point%20clouds%20that%20are%20often%20unattainable%20with%20standard%20SfM%20pipelines.%20These%20point%20clouds%20seed%20the%20Gaussian%20splatting%20process%20to%20generate%20photorealistic%20undercarriage%20models%20that%20render%20in%20real-time.%20Our%20experiments%20and%20ablation%20studies%20demonstrate%20that%20our%20design%20choices%20are%20essential%20to%20achieve%20state-of-the-art%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14208v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRig-Aware%25203D%2520Reconstruction%2520of%2520Vehicle%2520Undercarriages%2520using%2520Gaussian%2520Splatting%26entry.906535625%3DNitin%2520Kulkarni%2520and%2520Akhil%2520Devarashetti%2520and%2520Charlie%2520Cluss%2520and%2520Livio%2520Forte%2520and%2520Dan%2520Buckmaster%2520and%2520Philip%2520Schneider%2520and%2520Chunming%2520Qiao%2520and%2520Alina%2520Vereshchaka%26entry.1292438233%3DInspecting%2520the%2520undercarriage%2520of%2520used%2520vehicles%2520is%2520a%2520labor-intensive%2520task%2520that%2520requires%2520inspectors%2520to%2520crouch%2520or%2520crawl%2520underneath%2520each%2520vehicle%2520to%2520thoroughly%2520examine%2520it.%2520Additionally%252C%2520online%2520buyers%2520rarely%2520see%2520undercarriage%2520photos.%2520We%2520present%2520an%2520end-to-end%2520pipeline%2520that%2520utilizes%2520a%2520three-camera%2520rig%2520to%2520capture%2520videos%2520of%2520the%2520undercarriage%2520as%2520the%2520vehicle%2520drives%2520over%2520it%252C%2520and%2520produces%2520an%2520interactive%25203D%2520model%2520of%2520the%2520undercarriage.%2520The%25203D%2520model%2520enables%2520inspectors%2520and%2520customers%2520to%2520rotate%252C%2520zoom%252C%2520and%2520slice%2520through%2520the%2520undercarriage%252C%2520allowing%2520them%2520to%2520detect%2520rust%252C%2520leaks%252C%2520or%2520impact%2520damage%2520in%2520seconds%252C%2520thereby%2520improving%2520both%2520workplace%2520safety%2520and%2520buyer%2520confidence.%2520Our%2520primary%2520contribution%2520is%2520a%2520rig-aware%2520Structure-from-Motion%2520%2528SfM%2529%2520pipeline%2520specifically%2520designed%2520to%2520overcome%2520the%2520challenges%2520of%2520wide-angle%2520lens%2520distortion%2520and%2520low-parallax%2520scenes.%2520Our%2520method%2520overcomes%2520the%2520challenges%2520of%2520wide-angle%2520lens%2520distortion%2520and%2520low-parallax%2520scenes%2520by%2520integrating%2520precise%2520camera%2520calibration%252C%2520synchronized%2520video%2520streams%252C%2520and%2520strong%2520geometric%2520priors%2520from%2520the%2520camera%2520rig.%2520We%2520use%2520a%2520constrained%2520matching%2520strategy%2520with%2520learned%2520components%252C%2520the%2520DISK%2520feature%2520extractor%252C%2520and%2520the%2520attention-based%2520LightGlue%2520matcher%2520to%2520generate%2520high-quality%2520sparse%2520point%2520clouds%2520that%2520are%2520often%2520unattainable%2520with%2520standard%2520SfM%2520pipelines.%2520These%2520point%2520clouds%2520seed%2520the%2520Gaussian%2520splatting%2520process%2520to%2520generate%2520photorealistic%2520undercarriage%2520models%2520that%2520render%2520in%2520real-time.%2520Our%2520experiments%2520and%2520ablation%2520studies%2520demonstrate%2520that%2520our%2520design%2520choices%2520are%2520essential%2520to%2520achieve%2520state-of-the-art%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14208v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rig-Aware%203D%20Reconstruction%20of%20Vehicle%20Undercarriages%20using%20Gaussian%20Splatting&entry.906535625=Nitin%20Kulkarni%20and%20Akhil%20Devarashetti%20and%20Charlie%20Cluss%20and%20Livio%20Forte%20and%20Dan%20Buckmaster%20and%20Philip%20Schneider%20and%20Chunming%20Qiao%20and%20Alina%20Vereshchaka&entry.1292438233=Inspecting%20the%20undercarriage%20of%20used%20vehicles%20is%20a%20labor-intensive%20task%20that%20requires%20inspectors%20to%20crouch%20or%20crawl%20underneath%20each%20vehicle%20to%20thoroughly%20examine%20it.%20Additionally%2C%20online%20buyers%20rarely%20see%20undercarriage%20photos.%20We%20present%20an%20end-to-end%20pipeline%20that%20utilizes%20a%20three-camera%20rig%20to%20capture%20videos%20of%20the%20undercarriage%20as%20the%20vehicle%20drives%20over%20it%2C%20and%20produces%20an%20interactive%203D%20model%20of%20the%20undercarriage.%20The%203D%20model%20enables%20inspectors%20and%20customers%20to%20rotate%2C%20zoom%2C%20and%20slice%20through%20the%20undercarriage%2C%20allowing%20them%20to%20detect%20rust%2C%20leaks%2C%20or%20impact%20damage%20in%20seconds%2C%20thereby%20improving%20both%20workplace%20safety%20and%20buyer%20confidence.%20Our%20primary%20contribution%20is%20a%20rig-aware%20Structure-from-Motion%20%28SfM%29%20pipeline%20specifically%20designed%20to%20overcome%20the%20challenges%20of%20wide-angle%20lens%20distortion%20and%20low-parallax%20scenes.%20Our%20method%20overcomes%20the%20challenges%20of%20wide-angle%20lens%20distortion%20and%20low-parallax%20scenes%20by%20integrating%20precise%20camera%20calibration%2C%20synchronized%20video%20streams%2C%20and%20strong%20geometric%20priors%20from%20the%20camera%20rig.%20We%20use%20a%20constrained%20matching%20strategy%20with%20learned%20components%2C%20the%20DISK%20feature%20extractor%2C%20and%20the%20attention-based%20LightGlue%20matcher%20to%20generate%20high-quality%20sparse%20point%20clouds%20that%20are%20often%20unattainable%20with%20standard%20SfM%20pipelines.%20These%20point%20clouds%20seed%20the%20Gaussian%20splatting%20process%20to%20generate%20photorealistic%20undercarriage%20models%20that%20render%20in%20real-time.%20Our%20experiments%20and%20ablation%20studies%20demonstrate%20that%20our%20design%20choices%20are%20essential%20to%20achieve%20state-of-the-art%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2601.14208v1&entry.124074799=Read"},
{"title": "Revisiting Multi-Task Visual Representation Learning", "author": "Shangzhe Di and Zhonghua Zhai and Weidi Xie", "abstract": "Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity \"expert\" models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves \"best-of-both-worlds\" performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.", "link": "http://arxiv.org/abs/2601.13886v1", "date": "2026-01-20", "relevancy": 3.2491, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6737}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6379}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6379}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revisiting%20Multi-Task%20Visual%20Representation%20Learning&body=Title%3A%20Revisiting%20Multi-Task%20Visual%20Representation%20Learning%0AAuthor%3A%20Shangzhe%20Di%20and%20Zhonghua%20Zhai%20and%20Weidi%20Xie%0AAbstract%3A%20Current%20visual%20representation%20learning%20remains%20bifurcated%3A%20vision-language%20models%20%28e.g.%2C%20CLIP%29%20excel%20at%20global%20semantic%20alignment%20but%20lack%20spatial%20precision%2C%20while%20self-supervised%20methods%20%28e.g.%2C%20MAE%2C%20DINO%29%20capture%20intricate%20local%20structures%20yet%20struggle%20with%20high-level%20semantic%20context.%20We%20argue%20that%20these%20paradigms%20are%20fundamentally%20complementary%20and%20can%20be%20integrated%20into%20a%20principled%20multi-task%20framework%2C%20further%20enhanced%20by%20dense%20spatial%20supervision.%20We%20introduce%20MTV%2C%20a%20multi-task%20visual%20pretraining%20framework%20that%20jointly%20optimizes%20a%20shared%20backbone%20across%20vision-language%20contrastive%2C%20self-supervised%2C%20and%20dense%20spatial%20objectives.%20To%20mitigate%20the%20need%20for%20manual%20annotations%2C%20we%20leverage%20high-capacity%20%22expert%22%20models%20--%20such%20as%20Depth%20Anything%20V2%20and%20OWLv2%20--%20to%20synthesize%20dense%2C%20structured%20pseudo-labels%20at%20scale.%20Beyond%20the%20framework%2C%20we%20provide%20a%20systematic%20investigation%20into%20the%20mechanics%20of%20multi-task%20visual%20learning%2C%20analyzing%3A%20%28i%29%20the%20marginal%20gain%20of%20each%20objective%2C%20%28ii%29%20task%20synergies%20versus%20interference%2C%20and%20%28iii%29%20scaling%20behavior%20across%20varying%20data%20and%20model%20scales.%20Our%20results%20demonstrate%20that%20MTV%20achieves%20%22best-of-both-worlds%22%20performance%2C%20significantly%20enhancing%20fine-grained%20spatial%20reasoning%20without%20compromising%20global%20semantic%20understanding.%20Our%20findings%20suggest%20that%20multi-task%20learning%2C%20fueled%20by%20high-quality%20pseudo-supervision%2C%20is%20a%20scalable%20path%20toward%20more%20general%20visual%20encoders.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13886v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevisiting%2520Multi-Task%2520Visual%2520Representation%2520Learning%26entry.906535625%3DShangzhe%2520Di%2520and%2520Zhonghua%2520Zhai%2520and%2520Weidi%2520Xie%26entry.1292438233%3DCurrent%2520visual%2520representation%2520learning%2520remains%2520bifurcated%253A%2520vision-language%2520models%2520%2528e.g.%252C%2520CLIP%2529%2520excel%2520at%2520global%2520semantic%2520alignment%2520but%2520lack%2520spatial%2520precision%252C%2520while%2520self-supervised%2520methods%2520%2528e.g.%252C%2520MAE%252C%2520DINO%2529%2520capture%2520intricate%2520local%2520structures%2520yet%2520struggle%2520with%2520high-level%2520semantic%2520context.%2520We%2520argue%2520that%2520these%2520paradigms%2520are%2520fundamentally%2520complementary%2520and%2520can%2520be%2520integrated%2520into%2520a%2520principled%2520multi-task%2520framework%252C%2520further%2520enhanced%2520by%2520dense%2520spatial%2520supervision.%2520We%2520introduce%2520MTV%252C%2520a%2520multi-task%2520visual%2520pretraining%2520framework%2520that%2520jointly%2520optimizes%2520a%2520shared%2520backbone%2520across%2520vision-language%2520contrastive%252C%2520self-supervised%252C%2520and%2520dense%2520spatial%2520objectives.%2520To%2520mitigate%2520the%2520need%2520for%2520manual%2520annotations%252C%2520we%2520leverage%2520high-capacity%2520%2522expert%2522%2520models%2520--%2520such%2520as%2520Depth%2520Anything%2520V2%2520and%2520OWLv2%2520--%2520to%2520synthesize%2520dense%252C%2520structured%2520pseudo-labels%2520at%2520scale.%2520Beyond%2520the%2520framework%252C%2520we%2520provide%2520a%2520systematic%2520investigation%2520into%2520the%2520mechanics%2520of%2520multi-task%2520visual%2520learning%252C%2520analyzing%253A%2520%2528i%2529%2520the%2520marginal%2520gain%2520of%2520each%2520objective%252C%2520%2528ii%2529%2520task%2520synergies%2520versus%2520interference%252C%2520and%2520%2528iii%2529%2520scaling%2520behavior%2520across%2520varying%2520data%2520and%2520model%2520scales.%2520Our%2520results%2520demonstrate%2520that%2520MTV%2520achieves%2520%2522best-of-both-worlds%2522%2520performance%252C%2520significantly%2520enhancing%2520fine-grained%2520spatial%2520reasoning%2520without%2520compromising%2520global%2520semantic%2520understanding.%2520Our%2520findings%2520suggest%2520that%2520multi-task%2520learning%252C%2520fueled%2520by%2520high-quality%2520pseudo-supervision%252C%2520is%2520a%2520scalable%2520path%2520toward%2520more%2520general%2520visual%2520encoders.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13886v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revisiting%20Multi-Task%20Visual%20Representation%20Learning&entry.906535625=Shangzhe%20Di%20and%20Zhonghua%20Zhai%20and%20Weidi%20Xie&entry.1292438233=Current%20visual%20representation%20learning%20remains%20bifurcated%3A%20vision-language%20models%20%28e.g.%2C%20CLIP%29%20excel%20at%20global%20semantic%20alignment%20but%20lack%20spatial%20precision%2C%20while%20self-supervised%20methods%20%28e.g.%2C%20MAE%2C%20DINO%29%20capture%20intricate%20local%20structures%20yet%20struggle%20with%20high-level%20semantic%20context.%20We%20argue%20that%20these%20paradigms%20are%20fundamentally%20complementary%20and%20can%20be%20integrated%20into%20a%20principled%20multi-task%20framework%2C%20further%20enhanced%20by%20dense%20spatial%20supervision.%20We%20introduce%20MTV%2C%20a%20multi-task%20visual%20pretraining%20framework%20that%20jointly%20optimizes%20a%20shared%20backbone%20across%20vision-language%20contrastive%2C%20self-supervised%2C%20and%20dense%20spatial%20objectives.%20To%20mitigate%20the%20need%20for%20manual%20annotations%2C%20we%20leverage%20high-capacity%20%22expert%22%20models%20--%20such%20as%20Depth%20Anything%20V2%20and%20OWLv2%20--%20to%20synthesize%20dense%2C%20structured%20pseudo-labels%20at%20scale.%20Beyond%20the%20framework%2C%20we%20provide%20a%20systematic%20investigation%20into%20the%20mechanics%20of%20multi-task%20visual%20learning%2C%20analyzing%3A%20%28i%29%20the%20marginal%20gain%20of%20each%20objective%2C%20%28ii%29%20task%20synergies%20versus%20interference%2C%20and%20%28iii%29%20scaling%20behavior%20across%20varying%20data%20and%20model%20scales.%20Our%20results%20demonstrate%20that%20MTV%20achieves%20%22best-of-both-worlds%22%20performance%2C%20significantly%20enhancing%20fine-grained%20spatial%20reasoning%20without%20compromising%20global%20semantic%20understanding.%20Our%20findings%20suggest%20that%20multi-task%20learning%2C%20fueled%20by%20high-quality%20pseudo-supervision%2C%20is%20a%20scalable%20path%20toward%20more%20general%20visual%20encoders.&entry.1838667208=http%3A//arxiv.org/abs/2601.13886v1&entry.124074799=Read"},
{"title": "SuperGSeg: Open-Vocabulary 3D Segmentation with Structured Super-Gaussians", "author": "Siyun Liang and Sen Wang and Kunyi Li and Michael Niemeyer and Stefano Gasperini and Hendrik P. A. Lensch and Nassir Navab and Federico Tombari", "abstract": "3D Gaussian Splatting has recently gained traction for its efficient training and real-time rendering. While its vanilla representation is mainly designed for view synthesis, recent works extended it to scene understanding with language features. However, storing additional high-dimensional features per Gaussian for semantic information is memory-intensive, which limits their ability to segment and interpret challenging scenes. To this end, we introduce SuperGSeg, a novel approach that fosters cohesive, context-aware hierarchical scene representation by disentangling segmentation and language field distillation. SuperGSeg first employs neural 3D Gaussians to learn geometry, instance and hierarchical segmentation features from multi-view images with the aid of off-the-shelf 2D masks. These features are then leveraged to create a sparse set of \\acrlong{superg}s. \\acrlong{superg}s facilitate the lifting and distillation of 2D language features into 3D space. They enable hierarchical scene understanding with high-dimensional language feature rendering at moderate GPU memory costs. Extensive experiments demonstrate that SuperGSeg achieves remarkable performance on both open-vocabulary object selection and semantic segmentation tasks.", "link": "http://arxiv.org/abs/2412.10231v3", "date": "2026-01-20", "relevancy": 3.2008, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6591}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6443}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6171}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SuperGSeg%3A%20Open-Vocabulary%203D%20Segmentation%20with%20Structured%20Super-Gaussians&body=Title%3A%20SuperGSeg%3A%20Open-Vocabulary%203D%20Segmentation%20with%20Structured%20Super-Gaussians%0AAuthor%3A%20Siyun%20Liang%20and%20Sen%20Wang%20and%20Kunyi%20Li%20and%20Michael%20Niemeyer%20and%20Stefano%20Gasperini%20and%20Hendrik%20P.%20A.%20Lensch%20and%20Nassir%20Navab%20and%20Federico%20Tombari%0AAbstract%3A%203D%20Gaussian%20Splatting%20has%20recently%20gained%20traction%20for%20its%20efficient%20training%20and%20real-time%20rendering.%20While%20its%20vanilla%20representation%20is%20mainly%20designed%20for%20view%20synthesis%2C%20recent%20works%20extended%20it%20to%20scene%20understanding%20with%20language%20features.%20However%2C%20storing%20additional%20high-dimensional%20features%20per%20Gaussian%20for%20semantic%20information%20is%20memory-intensive%2C%20which%20limits%20their%20ability%20to%20segment%20and%20interpret%20challenging%20scenes.%20To%20this%20end%2C%20we%20introduce%20SuperGSeg%2C%20a%20novel%20approach%20that%20fosters%20cohesive%2C%20context-aware%20hierarchical%20scene%20representation%20by%20disentangling%20segmentation%20and%20language%20field%20distillation.%20SuperGSeg%20first%20employs%20neural%203D%20Gaussians%20to%20learn%20geometry%2C%20instance%20and%20hierarchical%20segmentation%20features%20from%20multi-view%20images%20with%20the%20aid%20of%20off-the-shelf%202D%20masks.%20These%20features%20are%20then%20leveraged%20to%20create%20a%20sparse%20set%20of%20%5Cacrlong%7Bsuperg%7Ds.%20%5Cacrlong%7Bsuperg%7Ds%20facilitate%20the%20lifting%20and%20distillation%20of%202D%20language%20features%20into%203D%20space.%20They%20enable%20hierarchical%20scene%20understanding%20with%20high-dimensional%20language%20feature%20rendering%20at%20moderate%20GPU%20memory%20costs.%20Extensive%20experiments%20demonstrate%20that%20SuperGSeg%20achieves%20remarkable%20performance%20on%20both%20open-vocabulary%20object%20selection%20and%20semantic%20segmentation%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2412.10231v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSuperGSeg%253A%2520Open-Vocabulary%25203D%2520Segmentation%2520with%2520Structured%2520Super-Gaussians%26entry.906535625%3DSiyun%2520Liang%2520and%2520Sen%2520Wang%2520and%2520Kunyi%2520Li%2520and%2520Michael%2520Niemeyer%2520and%2520Stefano%2520Gasperini%2520and%2520Hendrik%2520P.%2520A.%2520Lensch%2520and%2520Nassir%2520Navab%2520and%2520Federico%2520Tombari%26entry.1292438233%3D3D%2520Gaussian%2520Splatting%2520has%2520recently%2520gained%2520traction%2520for%2520its%2520efficient%2520training%2520and%2520real-time%2520rendering.%2520While%2520its%2520vanilla%2520representation%2520is%2520mainly%2520designed%2520for%2520view%2520synthesis%252C%2520recent%2520works%2520extended%2520it%2520to%2520scene%2520understanding%2520with%2520language%2520features.%2520However%252C%2520storing%2520additional%2520high-dimensional%2520features%2520per%2520Gaussian%2520for%2520semantic%2520information%2520is%2520memory-intensive%252C%2520which%2520limits%2520their%2520ability%2520to%2520segment%2520and%2520interpret%2520challenging%2520scenes.%2520To%2520this%2520end%252C%2520we%2520introduce%2520SuperGSeg%252C%2520a%2520novel%2520approach%2520that%2520fosters%2520cohesive%252C%2520context-aware%2520hierarchical%2520scene%2520representation%2520by%2520disentangling%2520segmentation%2520and%2520language%2520field%2520distillation.%2520SuperGSeg%2520first%2520employs%2520neural%25203D%2520Gaussians%2520to%2520learn%2520geometry%252C%2520instance%2520and%2520hierarchical%2520segmentation%2520features%2520from%2520multi-view%2520images%2520with%2520the%2520aid%2520of%2520off-the-shelf%25202D%2520masks.%2520These%2520features%2520are%2520then%2520leveraged%2520to%2520create%2520a%2520sparse%2520set%2520of%2520%255Cacrlong%257Bsuperg%257Ds.%2520%255Cacrlong%257Bsuperg%257Ds%2520facilitate%2520the%2520lifting%2520and%2520distillation%2520of%25202D%2520language%2520features%2520into%25203D%2520space.%2520They%2520enable%2520hierarchical%2520scene%2520understanding%2520with%2520high-dimensional%2520language%2520feature%2520rendering%2520at%2520moderate%2520GPU%2520memory%2520costs.%2520Extensive%2520experiments%2520demonstrate%2520that%2520SuperGSeg%2520achieves%2520remarkable%2520performance%2520on%2520both%2520open-vocabulary%2520object%2520selection%2520and%2520semantic%2520segmentation%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2412.10231v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SuperGSeg%3A%20Open-Vocabulary%203D%20Segmentation%20with%20Structured%20Super-Gaussians&entry.906535625=Siyun%20Liang%20and%20Sen%20Wang%20and%20Kunyi%20Li%20and%20Michael%20Niemeyer%20and%20Stefano%20Gasperini%20and%20Hendrik%20P.%20A.%20Lensch%20and%20Nassir%20Navab%20and%20Federico%20Tombari&entry.1292438233=3D%20Gaussian%20Splatting%20has%20recently%20gained%20traction%20for%20its%20efficient%20training%20and%20real-time%20rendering.%20While%20its%20vanilla%20representation%20is%20mainly%20designed%20for%20view%20synthesis%2C%20recent%20works%20extended%20it%20to%20scene%20understanding%20with%20language%20features.%20However%2C%20storing%20additional%20high-dimensional%20features%20per%20Gaussian%20for%20semantic%20information%20is%20memory-intensive%2C%20which%20limits%20their%20ability%20to%20segment%20and%20interpret%20challenging%20scenes.%20To%20this%20end%2C%20we%20introduce%20SuperGSeg%2C%20a%20novel%20approach%20that%20fosters%20cohesive%2C%20context-aware%20hierarchical%20scene%20representation%20by%20disentangling%20segmentation%20and%20language%20field%20distillation.%20SuperGSeg%20first%20employs%20neural%203D%20Gaussians%20to%20learn%20geometry%2C%20instance%20and%20hierarchical%20segmentation%20features%20from%20multi-view%20images%20with%20the%20aid%20of%20off-the-shelf%202D%20masks.%20These%20features%20are%20then%20leveraged%20to%20create%20a%20sparse%20set%20of%20%5Cacrlong%7Bsuperg%7Ds.%20%5Cacrlong%7Bsuperg%7Ds%20facilitate%20the%20lifting%20and%20distillation%20of%202D%20language%20features%20into%203D%20space.%20They%20enable%20hierarchical%20scene%20understanding%20with%20high-dimensional%20language%20feature%20rendering%20at%20moderate%20GPU%20memory%20costs.%20Extensive%20experiments%20demonstrate%20that%20SuperGSeg%20achieves%20remarkable%20performance%20on%20both%20open-vocabulary%20object%20selection%20and%20semantic%20segmentation%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2412.10231v3&entry.124074799=Read"},
{"title": "Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders", "author": "Kai Wittenmayer and Sukrut Rao and Amin Parchami-Araghi and Bernt Schiele and Jonas Fischer", "abstract": "Language-aligned vision foundation models perform strongly across diverse downstream tasks. Yet, their learned representations remain opaque, making interpreting their decision-making hard. Recent works decompose these representations into human-interpretable concepts, but provide poor spatial grounding and are limited to image classification tasks. In this work, we propose Insight, a language-aligned concept foundation model that provides fine-grained concepts, which are human-interpretable and spatially grounded in the input image. We leverage a hierarchical sparse autoencoder and a foundation model with strong semantic representations to automatically extract concepts at various granularities. Examining local co-occurrence dependencies of concepts allows us to define concept relationships. Through these relations we further improve concept naming and obtain richer explanations. On benchmark data, we show that Insight provides performance on classification and segmentation that is competitive with opaque foundation models while providing fine-grained, high quality concept-based explanations. Code is available at https://github.com/kawi19/Insight.", "link": "http://arxiv.org/abs/2601.13798v1", "date": "2026-01-20", "relevancy": 3.1252, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6693}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6693}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Insight%3A%20Interpretable%20Semantic%20Hierarchies%20in%20Vision-Language%20Encoders&body=Title%3A%20Insight%3A%20Interpretable%20Semantic%20Hierarchies%20in%20Vision-Language%20Encoders%0AAuthor%3A%20Kai%20Wittenmayer%20and%20Sukrut%20Rao%20and%20Amin%20Parchami-Araghi%20and%20Bernt%20Schiele%20and%20Jonas%20Fischer%0AAbstract%3A%20Language-aligned%20vision%20foundation%20models%20perform%20strongly%20across%20diverse%20downstream%20tasks.%20Yet%2C%20their%20learned%20representations%20remain%20opaque%2C%20making%20interpreting%20their%20decision-making%20hard.%20Recent%20works%20decompose%20these%20representations%20into%20human-interpretable%20concepts%2C%20but%20provide%20poor%20spatial%20grounding%20and%20are%20limited%20to%20image%20classification%20tasks.%20In%20this%20work%2C%20we%20propose%20Insight%2C%20a%20language-aligned%20concept%20foundation%20model%20that%20provides%20fine-grained%20concepts%2C%20which%20are%20human-interpretable%20and%20spatially%20grounded%20in%20the%20input%20image.%20We%20leverage%20a%20hierarchical%20sparse%20autoencoder%20and%20a%20foundation%20model%20with%20strong%20semantic%20representations%20to%20automatically%20extract%20concepts%20at%20various%20granularities.%20Examining%20local%20co-occurrence%20dependencies%20of%20concepts%20allows%20us%20to%20define%20concept%20relationships.%20Through%20these%20relations%20we%20further%20improve%20concept%20naming%20and%20obtain%20richer%20explanations.%20On%20benchmark%20data%2C%20we%20show%20that%20Insight%20provides%20performance%20on%20classification%20and%20segmentation%20that%20is%20competitive%20with%20opaque%20foundation%20models%20while%20providing%20fine-grained%2C%20high%20quality%20concept-based%20explanations.%20Code%20is%20available%20at%20https%3A//github.com/kawi19/Insight.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInsight%253A%2520Interpretable%2520Semantic%2520Hierarchies%2520in%2520Vision-Language%2520Encoders%26entry.906535625%3DKai%2520Wittenmayer%2520and%2520Sukrut%2520Rao%2520and%2520Amin%2520Parchami-Araghi%2520and%2520Bernt%2520Schiele%2520and%2520Jonas%2520Fischer%26entry.1292438233%3DLanguage-aligned%2520vision%2520foundation%2520models%2520perform%2520strongly%2520across%2520diverse%2520downstream%2520tasks.%2520Yet%252C%2520their%2520learned%2520representations%2520remain%2520opaque%252C%2520making%2520interpreting%2520their%2520decision-making%2520hard.%2520Recent%2520works%2520decompose%2520these%2520representations%2520into%2520human-interpretable%2520concepts%252C%2520but%2520provide%2520poor%2520spatial%2520grounding%2520and%2520are%2520limited%2520to%2520image%2520classification%2520tasks.%2520In%2520this%2520work%252C%2520we%2520propose%2520Insight%252C%2520a%2520language-aligned%2520concept%2520foundation%2520model%2520that%2520provides%2520fine-grained%2520concepts%252C%2520which%2520are%2520human-interpretable%2520and%2520spatially%2520grounded%2520in%2520the%2520input%2520image.%2520We%2520leverage%2520a%2520hierarchical%2520sparse%2520autoencoder%2520and%2520a%2520foundation%2520model%2520with%2520strong%2520semantic%2520representations%2520to%2520automatically%2520extract%2520concepts%2520at%2520various%2520granularities.%2520Examining%2520local%2520co-occurrence%2520dependencies%2520of%2520concepts%2520allows%2520us%2520to%2520define%2520concept%2520relationships.%2520Through%2520these%2520relations%2520we%2520further%2520improve%2520concept%2520naming%2520and%2520obtain%2520richer%2520explanations.%2520On%2520benchmark%2520data%252C%2520we%2520show%2520that%2520Insight%2520provides%2520performance%2520on%2520classification%2520and%2520segmentation%2520that%2520is%2520competitive%2520with%2520opaque%2520foundation%2520models%2520while%2520providing%2520fine-grained%252C%2520high%2520quality%2520concept-based%2520explanations.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/kawi19/Insight.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Insight%3A%20Interpretable%20Semantic%20Hierarchies%20in%20Vision-Language%20Encoders&entry.906535625=Kai%20Wittenmayer%20and%20Sukrut%20Rao%20and%20Amin%20Parchami-Araghi%20and%20Bernt%20Schiele%20and%20Jonas%20Fischer&entry.1292438233=Language-aligned%20vision%20foundation%20models%20perform%20strongly%20across%20diverse%20downstream%20tasks.%20Yet%2C%20their%20learned%20representations%20remain%20opaque%2C%20making%20interpreting%20their%20decision-making%20hard.%20Recent%20works%20decompose%20these%20representations%20into%20human-interpretable%20concepts%2C%20but%20provide%20poor%20spatial%20grounding%20and%20are%20limited%20to%20image%20classification%20tasks.%20In%20this%20work%2C%20we%20propose%20Insight%2C%20a%20language-aligned%20concept%20foundation%20model%20that%20provides%20fine-grained%20concepts%2C%20which%20are%20human-interpretable%20and%20spatially%20grounded%20in%20the%20input%20image.%20We%20leverage%20a%20hierarchical%20sparse%20autoencoder%20and%20a%20foundation%20model%20with%20strong%20semantic%20representations%20to%20automatically%20extract%20concepts%20at%20various%20granularities.%20Examining%20local%20co-occurrence%20dependencies%20of%20concepts%20allows%20us%20to%20define%20concept%20relationships.%20Through%20these%20relations%20we%20further%20improve%20concept%20naming%20and%20obtain%20richer%20explanations.%20On%20benchmark%20data%2C%20we%20show%20that%20Insight%20provides%20performance%20on%20classification%20and%20segmentation%20that%20is%20competitive%20with%20opaque%20foundation%20models%20while%20providing%20fine-grained%2C%20high%20quality%20concept-based%20explanations.%20Code%20is%20available%20at%20https%3A//github.com/kawi19/Insight.&entry.1838667208=http%3A//arxiv.org/abs/2601.13798v1&entry.124074799=Read"},
{"title": "CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction", "author": "Xianghui Xie and Bowen Wen and Yan Chang and Hesam Rabeti and Jiefeng Li and Ye Yuan and Gerard Pons-Moll and Stan Birchfield", "abstract": "Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.", "link": "http://arxiv.org/abs/2512.11988v2", "date": "2026-01-20", "relevancy": 3.0213, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6125}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6001}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6001}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CARI4D%3A%20Category%20Agnostic%204D%20Reconstruction%20of%20Human-Object%20Interaction&body=Title%3A%20CARI4D%3A%20Category%20Agnostic%204D%20Reconstruction%20of%20Human-Object%20Interaction%0AAuthor%3A%20Xianghui%20Xie%20and%20Bowen%20Wen%20and%20Yan%20Chang%20and%20Hesam%20Rabeti%20and%20Jiefeng%20Li%20and%20Ye%20Yuan%20and%20Gerard%20Pons-Moll%20and%20Stan%20Birchfield%0AAbstract%3A%20Accurate%20capture%20of%20human-object%20interaction%20from%20ubiquitous%20sensors%20like%20RGB%20cameras%20is%20important%20for%20applications%20in%20human%20understanding%2C%20gaming%2C%20and%20robot%20learning.%20However%2C%20inferring%204D%20interactions%20from%20a%20single%20RGB%20view%20is%20highly%20challenging%20due%20to%20the%20unknown%20object%20and%20human%20information%2C%20depth%20ambiguity%2C%20occlusion%2C%20and%20complex%20motion%2C%20which%20hinder%20consistent%203D%20and%20temporal%20reconstruction.%20Previous%20methods%20simplify%20the%20setup%20by%20assuming%20ground%20truth%20object%20template%20or%20constraining%20to%20a%20limited%20set%20of%20object%20categories.%20We%20present%20CARI4D%2C%20the%20first%20category-agnostic%20method%20that%20reconstructs%20spatially%20and%20temporarily%20consistent%204D%20human-object%20interaction%20at%20metric%20scale%20from%20monocular%20RGB%20videos.%20To%20this%20end%2C%20we%20propose%20a%20pose%20hypothesis%20selection%20algorithm%20that%20robustly%20integrates%20the%20individual%20predictions%20from%20foundation%20models%2C%20jointly%20refine%20them%20through%20a%20learned%20render-and-compare%20paradigm%20to%20ensure%20spatial%2C%20temporal%20and%20pixel%20alignment%2C%20and%20finally%20reasoning%20about%20intricate%20contacts%20for%20further%20refinement%20satisfying%20physical%20constraints.%20Experiments%20show%20that%20our%20method%20outperforms%20prior%20art%20by%2038%25%20on%20in-distribution%20dataset%20and%2036%25%20on%20unseen%20dataset%20in%20terms%20of%20reconstruction%20error.%20Our%20model%20generalizes%20beyond%20the%20training%20categories%20and%20thus%20can%20be%20applied%20zero-shot%20to%20in-the-wild%20internet%20videos.%20Our%20code%20and%20pretrained%20models%20will%20be%20publicly%20released.%0ALink%3A%20http%3A//arxiv.org/abs/2512.11988v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCARI4D%253A%2520Category%2520Agnostic%25204D%2520Reconstruction%2520of%2520Human-Object%2520Interaction%26entry.906535625%3DXianghui%2520Xie%2520and%2520Bowen%2520Wen%2520and%2520Yan%2520Chang%2520and%2520Hesam%2520Rabeti%2520and%2520Jiefeng%2520Li%2520and%2520Ye%2520Yuan%2520and%2520Gerard%2520Pons-Moll%2520and%2520Stan%2520Birchfield%26entry.1292438233%3DAccurate%2520capture%2520of%2520human-object%2520interaction%2520from%2520ubiquitous%2520sensors%2520like%2520RGB%2520cameras%2520is%2520important%2520for%2520applications%2520in%2520human%2520understanding%252C%2520gaming%252C%2520and%2520robot%2520learning.%2520However%252C%2520inferring%25204D%2520interactions%2520from%2520a%2520single%2520RGB%2520view%2520is%2520highly%2520challenging%2520due%2520to%2520the%2520unknown%2520object%2520and%2520human%2520information%252C%2520depth%2520ambiguity%252C%2520occlusion%252C%2520and%2520complex%2520motion%252C%2520which%2520hinder%2520consistent%25203D%2520and%2520temporal%2520reconstruction.%2520Previous%2520methods%2520simplify%2520the%2520setup%2520by%2520assuming%2520ground%2520truth%2520object%2520template%2520or%2520constraining%2520to%2520a%2520limited%2520set%2520of%2520object%2520categories.%2520We%2520present%2520CARI4D%252C%2520the%2520first%2520category-agnostic%2520method%2520that%2520reconstructs%2520spatially%2520and%2520temporarily%2520consistent%25204D%2520human-object%2520interaction%2520at%2520metric%2520scale%2520from%2520monocular%2520RGB%2520videos.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520pose%2520hypothesis%2520selection%2520algorithm%2520that%2520robustly%2520integrates%2520the%2520individual%2520predictions%2520from%2520foundation%2520models%252C%2520jointly%2520refine%2520them%2520through%2520a%2520learned%2520render-and-compare%2520paradigm%2520to%2520ensure%2520spatial%252C%2520temporal%2520and%2520pixel%2520alignment%252C%2520and%2520finally%2520reasoning%2520about%2520intricate%2520contacts%2520for%2520further%2520refinement%2520satisfying%2520physical%2520constraints.%2520Experiments%2520show%2520that%2520our%2520method%2520outperforms%2520prior%2520art%2520by%252038%2525%2520on%2520in-distribution%2520dataset%2520and%252036%2525%2520on%2520unseen%2520dataset%2520in%2520terms%2520of%2520reconstruction%2520error.%2520Our%2520model%2520generalizes%2520beyond%2520the%2520training%2520categories%2520and%2520thus%2520can%2520be%2520applied%2520zero-shot%2520to%2520in-the-wild%2520internet%2520videos.%2520Our%2520code%2520and%2520pretrained%2520models%2520will%2520be%2520publicly%2520released.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.11988v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CARI4D%3A%20Category%20Agnostic%204D%20Reconstruction%20of%20Human-Object%20Interaction&entry.906535625=Xianghui%20Xie%20and%20Bowen%20Wen%20and%20Yan%20Chang%20and%20Hesam%20Rabeti%20and%20Jiefeng%20Li%20and%20Ye%20Yuan%20and%20Gerard%20Pons-Moll%20and%20Stan%20Birchfield&entry.1292438233=Accurate%20capture%20of%20human-object%20interaction%20from%20ubiquitous%20sensors%20like%20RGB%20cameras%20is%20important%20for%20applications%20in%20human%20understanding%2C%20gaming%2C%20and%20robot%20learning.%20However%2C%20inferring%204D%20interactions%20from%20a%20single%20RGB%20view%20is%20highly%20challenging%20due%20to%20the%20unknown%20object%20and%20human%20information%2C%20depth%20ambiguity%2C%20occlusion%2C%20and%20complex%20motion%2C%20which%20hinder%20consistent%203D%20and%20temporal%20reconstruction.%20Previous%20methods%20simplify%20the%20setup%20by%20assuming%20ground%20truth%20object%20template%20or%20constraining%20to%20a%20limited%20set%20of%20object%20categories.%20We%20present%20CARI4D%2C%20the%20first%20category-agnostic%20method%20that%20reconstructs%20spatially%20and%20temporarily%20consistent%204D%20human-object%20interaction%20at%20metric%20scale%20from%20monocular%20RGB%20videos.%20To%20this%20end%2C%20we%20propose%20a%20pose%20hypothesis%20selection%20algorithm%20that%20robustly%20integrates%20the%20individual%20predictions%20from%20foundation%20models%2C%20jointly%20refine%20them%20through%20a%20learned%20render-and-compare%20paradigm%20to%20ensure%20spatial%2C%20temporal%20and%20pixel%20alignment%2C%20and%20finally%20reasoning%20about%20intricate%20contacts%20for%20further%20refinement%20satisfying%20physical%20constraints.%20Experiments%20show%20that%20our%20method%20outperforms%20prior%20art%20by%2038%25%20on%20in-distribution%20dataset%20and%2036%25%20on%20unseen%20dataset%20in%20terms%20of%20reconstruction%20error.%20Our%20model%20generalizes%20beyond%20the%20training%20categories%20and%20thus%20can%20be%20applied%20zero-shot%20to%20in-the-wild%20internet%20videos.%20Our%20code%20and%20pretrained%20models%20will%20be%20publicly%20released.&entry.1838667208=http%3A//arxiv.org/abs/2512.11988v2&entry.124074799=Read"},
{"title": "PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval", "author": "Gabriele Serussi and David Vainshtein and Jonathan Kouchly and Dotan Di Castro and Chaim Baskin", "abstract": "Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.", "link": "http://arxiv.org/abs/2601.13797v1", "date": "2026-01-20", "relevancy": 3.006, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6057}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6057}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5922}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PREGEN%3A%20Uncovering%20Latent%20Thoughts%20in%20Composed%20Video%20Retrieval&body=Title%3A%20PREGEN%3A%20Uncovering%20Latent%20Thoughts%20in%20Composed%20Video%20Retrieval%0AAuthor%3A%20Gabriele%20Serussi%20and%20David%20Vainshtein%20and%20Jonathan%20Kouchly%20and%20Dotan%20Di%20Castro%20and%20Chaim%20Baskin%0AAbstract%3A%20Composed%20Video%20Retrieval%20%28CoVR%29%20aims%20to%20retrieve%20a%20video%20based%20on%20a%20query%20video%20and%20a%20modifying%20text.%20Current%20CoVR%20methods%20fail%20to%20fully%20exploit%20modern%20Vision-Language%20Models%20%28VLMs%29%2C%20either%20using%20outdated%20architectures%20or%20requiring%20computationally%20expensive%20fine-tuning%20and%20slow%20caption%20generation.%20We%20introduce%20PREGEN%20%28PRE%20GENeration%20extraction%29%2C%20an%20efficient%20and%20powerful%20CoVR%20framework%20that%20overcomes%20these%20limitations.%20Our%20approach%20uniquely%20pairs%20a%20frozen%2C%20pre-trained%20VLM%20with%20a%20lightweight%20encoding%20model%2C%20eliminating%20the%20need%20for%20any%20VLM%20fine-tuning.%20We%20feed%20the%20query%20video%20and%20modifying%20text%20into%20the%20VLM%20and%20extract%20the%20hidden%20state%20of%20the%20final%20token%20from%20each%20layer.%20A%20simple%20encoder%20is%20then%20trained%20on%20these%20pooled%20representations%2C%20creating%20a%20semantically%20rich%20and%20compact%20embedding%20for%20retrieval.%20PREGEN%20significantly%20advances%20the%20state%20of%20the%20art%2C%20surpassing%20all%20prior%20methods%20on%20standard%20CoVR%20benchmarks%20with%20substantial%20gains%20in%20Recall%401%20of%20%2B27.23%20and%20%2B69.59.%20Our%20method%20demonstrates%20robustness%20across%20different%20VLM%20backbones%20and%20exhibits%20strong%20zero-shot%20generalization%20to%20more%20complex%20textual%20modifications%2C%20highlighting%20its%20effectiveness%20and%20semantic%20capabilities.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13797v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPREGEN%253A%2520Uncovering%2520Latent%2520Thoughts%2520in%2520Composed%2520Video%2520Retrieval%26entry.906535625%3DGabriele%2520Serussi%2520and%2520David%2520Vainshtein%2520and%2520Jonathan%2520Kouchly%2520and%2520Dotan%2520Di%2520Castro%2520and%2520Chaim%2520Baskin%26entry.1292438233%3DComposed%2520Video%2520Retrieval%2520%2528CoVR%2529%2520aims%2520to%2520retrieve%2520a%2520video%2520based%2520on%2520a%2520query%2520video%2520and%2520a%2520modifying%2520text.%2520Current%2520CoVR%2520methods%2520fail%2520to%2520fully%2520exploit%2520modern%2520Vision-Language%2520Models%2520%2528VLMs%2529%252C%2520either%2520using%2520outdated%2520architectures%2520or%2520requiring%2520computationally%2520expensive%2520fine-tuning%2520and%2520slow%2520caption%2520generation.%2520We%2520introduce%2520PREGEN%2520%2528PRE%2520GENeration%2520extraction%2529%252C%2520an%2520efficient%2520and%2520powerful%2520CoVR%2520framework%2520that%2520overcomes%2520these%2520limitations.%2520Our%2520approach%2520uniquely%2520pairs%2520a%2520frozen%252C%2520pre-trained%2520VLM%2520with%2520a%2520lightweight%2520encoding%2520model%252C%2520eliminating%2520the%2520need%2520for%2520any%2520VLM%2520fine-tuning.%2520We%2520feed%2520the%2520query%2520video%2520and%2520modifying%2520text%2520into%2520the%2520VLM%2520and%2520extract%2520the%2520hidden%2520state%2520of%2520the%2520final%2520token%2520from%2520each%2520layer.%2520A%2520simple%2520encoder%2520is%2520then%2520trained%2520on%2520these%2520pooled%2520representations%252C%2520creating%2520a%2520semantically%2520rich%2520and%2520compact%2520embedding%2520for%2520retrieval.%2520PREGEN%2520significantly%2520advances%2520the%2520state%2520of%2520the%2520art%252C%2520surpassing%2520all%2520prior%2520methods%2520on%2520standard%2520CoVR%2520benchmarks%2520with%2520substantial%2520gains%2520in%2520Recall%25401%2520of%2520%252B27.23%2520and%2520%252B69.59.%2520Our%2520method%2520demonstrates%2520robustness%2520across%2520different%2520VLM%2520backbones%2520and%2520exhibits%2520strong%2520zero-shot%2520generalization%2520to%2520more%2520complex%2520textual%2520modifications%252C%2520highlighting%2520its%2520effectiveness%2520and%2520semantic%2520capabilities.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13797v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PREGEN%3A%20Uncovering%20Latent%20Thoughts%20in%20Composed%20Video%20Retrieval&entry.906535625=Gabriele%20Serussi%20and%20David%20Vainshtein%20and%20Jonathan%20Kouchly%20and%20Dotan%20Di%20Castro%20and%20Chaim%20Baskin&entry.1292438233=Composed%20Video%20Retrieval%20%28CoVR%29%20aims%20to%20retrieve%20a%20video%20based%20on%20a%20query%20video%20and%20a%20modifying%20text.%20Current%20CoVR%20methods%20fail%20to%20fully%20exploit%20modern%20Vision-Language%20Models%20%28VLMs%29%2C%20either%20using%20outdated%20architectures%20or%20requiring%20computationally%20expensive%20fine-tuning%20and%20slow%20caption%20generation.%20We%20introduce%20PREGEN%20%28PRE%20GENeration%20extraction%29%2C%20an%20efficient%20and%20powerful%20CoVR%20framework%20that%20overcomes%20these%20limitations.%20Our%20approach%20uniquely%20pairs%20a%20frozen%2C%20pre-trained%20VLM%20with%20a%20lightweight%20encoding%20model%2C%20eliminating%20the%20need%20for%20any%20VLM%20fine-tuning.%20We%20feed%20the%20query%20video%20and%20modifying%20text%20into%20the%20VLM%20and%20extract%20the%20hidden%20state%20of%20the%20final%20token%20from%20each%20layer.%20A%20simple%20encoder%20is%20then%20trained%20on%20these%20pooled%20representations%2C%20creating%20a%20semantically%20rich%20and%20compact%20embedding%20for%20retrieval.%20PREGEN%20significantly%20advances%20the%20state%20of%20the%20art%2C%20surpassing%20all%20prior%20methods%20on%20standard%20CoVR%20benchmarks%20with%20substantial%20gains%20in%20Recall%401%20of%20%2B27.23%20and%20%2B69.59.%20Our%20method%20demonstrates%20robustness%20across%20different%20VLM%20backbones%20and%20exhibits%20strong%20zero-shot%20generalization%20to%20more%20complex%20textual%20modifications%2C%20highlighting%20its%20effectiveness%20and%20semantic%20capabilities.&entry.1838667208=http%3A//arxiv.org/abs/2601.13797v1&entry.124074799=Read"},
{"title": "VTONGuard: Automatic Detection and Authentication of AI-Generated Virtual Try-On Content", "author": "Shengyi Wu and Yan Hong and Shengyao Chen and Zheng Wang and Xianbing Sun and Jiahui Zhan and Jun Lan and Jianfu Zhang", "abstract": "With the rapid advancement of generative AI, virtual try-on (VTON) systems are becoming increasingly common in e-commerce and digital entertainment. However, the growing realism of AI-generated try-on content raises pressing concerns about authenticity and responsible use. To address this, we present VTONGuard, a large-scale benchmark dataset containing over 775,000 real and synthetic try-on images. The dataset covers diverse real-world conditions, including variations in pose, background, and garment styles, and provides both authentic and manipulated examples. Based on this benchmark, we conduct a systematic evaluation of multiple detection paradigms under unified training and testing protocols. Our results reveal each method's strengths and weaknesses and highlight the persistent challenge of cross-paradigm generalization. To further advance detection, we design a multi-task framework that integrates auxiliary segmentation to enhance boundary-aware feature learning, achieving the best overall performance on VTONGuard. We expect this benchmark to enable fair comparisons, facilitate the development of more robust detection models, and promote the safe and responsible deployment of VTON technologies in practice.", "link": "http://arxiv.org/abs/2601.13951v1", "date": "2026-01-20", "relevancy": 3.0042, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.6268}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6092}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VTONGuard%3A%20Automatic%20Detection%20and%20Authentication%20of%20AI-Generated%20Virtual%20Try-On%20Content&body=Title%3A%20VTONGuard%3A%20Automatic%20Detection%20and%20Authentication%20of%20AI-Generated%20Virtual%20Try-On%20Content%0AAuthor%3A%20Shengyi%20Wu%20and%20Yan%20Hong%20and%20Shengyao%20Chen%20and%20Zheng%20Wang%20and%20Xianbing%20Sun%20and%20Jiahui%20Zhan%20and%20Jun%20Lan%20and%20Jianfu%20Zhang%0AAbstract%3A%20With%20the%20rapid%20advancement%20of%20generative%20AI%2C%20virtual%20try-on%20%28VTON%29%20systems%20are%20becoming%20increasingly%20common%20in%20e-commerce%20and%20digital%20entertainment.%20However%2C%20the%20growing%20realism%20of%20AI-generated%20try-on%20content%20raises%20pressing%20concerns%20about%20authenticity%20and%20responsible%20use.%20To%20address%20this%2C%20we%20present%20VTONGuard%2C%20a%20large-scale%20benchmark%20dataset%20containing%20over%20775%2C000%20real%20and%20synthetic%20try-on%20images.%20The%20dataset%20covers%20diverse%20real-world%20conditions%2C%20including%20variations%20in%20pose%2C%20background%2C%20and%20garment%20styles%2C%20and%20provides%20both%20authentic%20and%20manipulated%20examples.%20Based%20on%20this%20benchmark%2C%20we%20conduct%20a%20systematic%20evaluation%20of%20multiple%20detection%20paradigms%20under%20unified%20training%20and%20testing%20protocols.%20Our%20results%20reveal%20each%20method%27s%20strengths%20and%20weaknesses%20and%20highlight%20the%20persistent%20challenge%20of%20cross-paradigm%20generalization.%20To%20further%20advance%20detection%2C%20we%20design%20a%20multi-task%20framework%20that%20integrates%20auxiliary%20segmentation%20to%20enhance%20boundary-aware%20feature%20learning%2C%20achieving%20the%20best%20overall%20performance%20on%20VTONGuard.%20We%20expect%20this%20benchmark%20to%20enable%20fair%20comparisons%2C%20facilitate%20the%20development%20of%20more%20robust%20detection%20models%2C%20and%20promote%20the%20safe%20and%20responsible%20deployment%20of%20VTON%20technologies%20in%20practice.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13951v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVTONGuard%253A%2520Automatic%2520Detection%2520and%2520Authentication%2520of%2520AI-Generated%2520Virtual%2520Try-On%2520Content%26entry.906535625%3DShengyi%2520Wu%2520and%2520Yan%2520Hong%2520and%2520Shengyao%2520Chen%2520and%2520Zheng%2520Wang%2520and%2520Xianbing%2520Sun%2520and%2520Jiahui%2520Zhan%2520and%2520Jun%2520Lan%2520and%2520Jianfu%2520Zhang%26entry.1292438233%3DWith%2520the%2520rapid%2520advancement%2520of%2520generative%2520AI%252C%2520virtual%2520try-on%2520%2528VTON%2529%2520systems%2520are%2520becoming%2520increasingly%2520common%2520in%2520e-commerce%2520and%2520digital%2520entertainment.%2520However%252C%2520the%2520growing%2520realism%2520of%2520AI-generated%2520try-on%2520content%2520raises%2520pressing%2520concerns%2520about%2520authenticity%2520and%2520responsible%2520use.%2520To%2520address%2520this%252C%2520we%2520present%2520VTONGuard%252C%2520a%2520large-scale%2520benchmark%2520dataset%2520containing%2520over%2520775%252C000%2520real%2520and%2520synthetic%2520try-on%2520images.%2520The%2520dataset%2520covers%2520diverse%2520real-world%2520conditions%252C%2520including%2520variations%2520in%2520pose%252C%2520background%252C%2520and%2520garment%2520styles%252C%2520and%2520provides%2520both%2520authentic%2520and%2520manipulated%2520examples.%2520Based%2520on%2520this%2520benchmark%252C%2520we%2520conduct%2520a%2520systematic%2520evaluation%2520of%2520multiple%2520detection%2520paradigms%2520under%2520unified%2520training%2520and%2520testing%2520protocols.%2520Our%2520results%2520reveal%2520each%2520method%2527s%2520strengths%2520and%2520weaknesses%2520and%2520highlight%2520the%2520persistent%2520challenge%2520of%2520cross-paradigm%2520generalization.%2520To%2520further%2520advance%2520detection%252C%2520we%2520design%2520a%2520multi-task%2520framework%2520that%2520integrates%2520auxiliary%2520segmentation%2520to%2520enhance%2520boundary-aware%2520feature%2520learning%252C%2520achieving%2520the%2520best%2520overall%2520performance%2520on%2520VTONGuard.%2520We%2520expect%2520this%2520benchmark%2520to%2520enable%2520fair%2520comparisons%252C%2520facilitate%2520the%2520development%2520of%2520more%2520robust%2520detection%2520models%252C%2520and%2520promote%2520the%2520safe%2520and%2520responsible%2520deployment%2520of%2520VTON%2520technologies%2520in%2520practice.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13951v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VTONGuard%3A%20Automatic%20Detection%20and%20Authentication%20of%20AI-Generated%20Virtual%20Try-On%20Content&entry.906535625=Shengyi%20Wu%20and%20Yan%20Hong%20and%20Shengyao%20Chen%20and%20Zheng%20Wang%20and%20Xianbing%20Sun%20and%20Jiahui%20Zhan%20and%20Jun%20Lan%20and%20Jianfu%20Zhang&entry.1292438233=With%20the%20rapid%20advancement%20of%20generative%20AI%2C%20virtual%20try-on%20%28VTON%29%20systems%20are%20becoming%20increasingly%20common%20in%20e-commerce%20and%20digital%20entertainment.%20However%2C%20the%20growing%20realism%20of%20AI-generated%20try-on%20content%20raises%20pressing%20concerns%20about%20authenticity%20and%20responsible%20use.%20To%20address%20this%2C%20we%20present%20VTONGuard%2C%20a%20large-scale%20benchmark%20dataset%20containing%20over%20775%2C000%20real%20and%20synthetic%20try-on%20images.%20The%20dataset%20covers%20diverse%20real-world%20conditions%2C%20including%20variations%20in%20pose%2C%20background%2C%20and%20garment%20styles%2C%20and%20provides%20both%20authentic%20and%20manipulated%20examples.%20Based%20on%20this%20benchmark%2C%20we%20conduct%20a%20systematic%20evaluation%20of%20multiple%20detection%20paradigms%20under%20unified%20training%20and%20testing%20protocols.%20Our%20results%20reveal%20each%20method%27s%20strengths%20and%20weaknesses%20and%20highlight%20the%20persistent%20challenge%20of%20cross-paradigm%20generalization.%20To%20further%20advance%20detection%2C%20we%20design%20a%20multi-task%20framework%20that%20integrates%20auxiliary%20segmentation%20to%20enhance%20boundary-aware%20feature%20learning%2C%20achieving%20the%20best%20overall%20performance%20on%20VTONGuard.%20We%20expect%20this%20benchmark%20to%20enable%20fair%20comparisons%2C%20facilitate%20the%20development%20of%20more%20robust%20detection%20models%2C%20and%20promote%20the%20safe%20and%20responsible%20deployment%20of%20VTON%20technologies%20in%20practice.&entry.1838667208=http%3A//arxiv.org/abs/2601.13951v1&entry.124074799=Read"},
{"title": "Interp3D: Correspondence-aware Interpolation for Generative Textured 3D Morphing", "author": "Xiaolu Liu and Yicong Li and Qiyuan He and Jiayin Zhu and Wei Ji and Angela Yao and Jianke Zhu", "abstract": "Textured 3D morphing seeks to generate smooth and plausible transitions between two 3D assets, preserving both structural coherence and fine-grained appearance. This ability is crucial not only for advancing 3D generation research but also for practical applications in animation, editing, and digital content creation. Existing approaches either operate directly on geometry, limiting them to shape-only morphing while neglecting textures, or extend 2D interpolation strategies into 3D, which often causes semantic ambiguity, structural misalignment, and texture blurring. These challenges underscore the necessity to jointly preserve geometric consistency, texture alignment, and robustness throughout the transition process. To address this, we propose Interp3D, a novel training-free framework for textured 3D morphing. It harnesses generative priors and adopts a progressive alignment principle to ensure both geometric fidelity and texture coherence. Starting from semantically aligned interpolation in condition space, Interp3D enforces structural consistency via SLAT (Structured Latent)-guided structure interpolation, and finally transfers appearance details through fine-grained texture fusion. For comprehensive evaluations, we construct a dedicated dataset, Interp3DData, with graded difficulty levels and assess generation results from fidelity, transition smoothness, and plausibility. Both quantitative metrics and human studies demonstrate the significant advantages of our proposed approach over previous methods. Source code is available at https://github.com/xiaolul2/Interp3D.", "link": "http://arxiv.org/abs/2601.14103v1", "date": "2026-01-20", "relevancy": 2.9613, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6311}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5812}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5645}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interp3D%3A%20Correspondence-aware%20Interpolation%20for%20Generative%20Textured%203D%20Morphing&body=Title%3A%20Interp3D%3A%20Correspondence-aware%20Interpolation%20for%20Generative%20Textured%203D%20Morphing%0AAuthor%3A%20Xiaolu%20Liu%20and%20Yicong%20Li%20and%20Qiyuan%20He%20and%20Jiayin%20Zhu%20and%20Wei%20Ji%20and%20Angela%20Yao%20and%20Jianke%20Zhu%0AAbstract%3A%20Textured%203D%20morphing%20seeks%20to%20generate%20smooth%20and%20plausible%20transitions%20between%20two%203D%20assets%2C%20preserving%20both%20structural%20coherence%20and%20fine-grained%20appearance.%20This%20ability%20is%20crucial%20not%20only%20for%20advancing%203D%20generation%20research%20but%20also%20for%20practical%20applications%20in%20animation%2C%20editing%2C%20and%20digital%20content%20creation.%20Existing%20approaches%20either%20operate%20directly%20on%20geometry%2C%20limiting%20them%20to%20shape-only%20morphing%20while%20neglecting%20textures%2C%20or%20extend%202D%20interpolation%20strategies%20into%203D%2C%20which%20often%20causes%20semantic%20ambiguity%2C%20structural%20misalignment%2C%20and%20texture%20blurring.%20These%20challenges%20underscore%20the%20necessity%20to%20jointly%20preserve%20geometric%20consistency%2C%20texture%20alignment%2C%20and%20robustness%20throughout%20the%20transition%20process.%20To%20address%20this%2C%20we%20propose%20Interp3D%2C%20a%20novel%20training-free%20framework%20for%20textured%203D%20morphing.%20It%20harnesses%20generative%20priors%20and%20adopts%20a%20progressive%20alignment%20principle%20to%20ensure%20both%20geometric%20fidelity%20and%20texture%20coherence.%20Starting%20from%20semantically%20aligned%20interpolation%20in%20condition%20space%2C%20Interp3D%20enforces%20structural%20consistency%20via%20SLAT%20%28Structured%20Latent%29-guided%20structure%20interpolation%2C%20and%20finally%20transfers%20appearance%20details%20through%20fine-grained%20texture%20fusion.%20For%20comprehensive%20evaluations%2C%20we%20construct%20a%20dedicated%20dataset%2C%20Interp3DData%2C%20with%20graded%20difficulty%20levels%20and%20assess%20generation%20results%20from%20fidelity%2C%20transition%20smoothness%2C%20and%20plausibility.%20Both%20quantitative%20metrics%20and%20human%20studies%20demonstrate%20the%20significant%20advantages%20of%20our%20proposed%20approach%20over%20previous%20methods.%20Source%20code%20is%20available%20at%20https%3A//github.com/xiaolul2/Interp3D.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14103v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterp3D%253A%2520Correspondence-aware%2520Interpolation%2520for%2520Generative%2520Textured%25203D%2520Morphing%26entry.906535625%3DXiaolu%2520Liu%2520and%2520Yicong%2520Li%2520and%2520Qiyuan%2520He%2520and%2520Jiayin%2520Zhu%2520and%2520Wei%2520Ji%2520and%2520Angela%2520Yao%2520and%2520Jianke%2520Zhu%26entry.1292438233%3DTextured%25203D%2520morphing%2520seeks%2520to%2520generate%2520smooth%2520and%2520plausible%2520transitions%2520between%2520two%25203D%2520assets%252C%2520preserving%2520both%2520structural%2520coherence%2520and%2520fine-grained%2520appearance.%2520This%2520ability%2520is%2520crucial%2520not%2520only%2520for%2520advancing%25203D%2520generation%2520research%2520but%2520also%2520for%2520practical%2520applications%2520in%2520animation%252C%2520editing%252C%2520and%2520digital%2520content%2520creation.%2520Existing%2520approaches%2520either%2520operate%2520directly%2520on%2520geometry%252C%2520limiting%2520them%2520to%2520shape-only%2520morphing%2520while%2520neglecting%2520textures%252C%2520or%2520extend%25202D%2520interpolation%2520strategies%2520into%25203D%252C%2520which%2520often%2520causes%2520semantic%2520ambiguity%252C%2520structural%2520misalignment%252C%2520and%2520texture%2520blurring.%2520These%2520challenges%2520underscore%2520the%2520necessity%2520to%2520jointly%2520preserve%2520geometric%2520consistency%252C%2520texture%2520alignment%252C%2520and%2520robustness%2520throughout%2520the%2520transition%2520process.%2520To%2520address%2520this%252C%2520we%2520propose%2520Interp3D%252C%2520a%2520novel%2520training-free%2520framework%2520for%2520textured%25203D%2520morphing.%2520It%2520harnesses%2520generative%2520priors%2520and%2520adopts%2520a%2520progressive%2520alignment%2520principle%2520to%2520ensure%2520both%2520geometric%2520fidelity%2520and%2520texture%2520coherence.%2520Starting%2520from%2520semantically%2520aligned%2520interpolation%2520in%2520condition%2520space%252C%2520Interp3D%2520enforces%2520structural%2520consistency%2520via%2520SLAT%2520%2528Structured%2520Latent%2529-guided%2520structure%2520interpolation%252C%2520and%2520finally%2520transfers%2520appearance%2520details%2520through%2520fine-grained%2520texture%2520fusion.%2520For%2520comprehensive%2520evaluations%252C%2520we%2520construct%2520a%2520dedicated%2520dataset%252C%2520Interp3DData%252C%2520with%2520graded%2520difficulty%2520levels%2520and%2520assess%2520generation%2520results%2520from%2520fidelity%252C%2520transition%2520smoothness%252C%2520and%2520plausibility.%2520Both%2520quantitative%2520metrics%2520and%2520human%2520studies%2520demonstrate%2520the%2520significant%2520advantages%2520of%2520our%2520proposed%2520approach%2520over%2520previous%2520methods.%2520Source%2520code%2520is%2520available%2520at%2520https%253A//github.com/xiaolul2/Interp3D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14103v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interp3D%3A%20Correspondence-aware%20Interpolation%20for%20Generative%20Textured%203D%20Morphing&entry.906535625=Xiaolu%20Liu%20and%20Yicong%20Li%20and%20Qiyuan%20He%20and%20Jiayin%20Zhu%20and%20Wei%20Ji%20and%20Angela%20Yao%20and%20Jianke%20Zhu&entry.1292438233=Textured%203D%20morphing%20seeks%20to%20generate%20smooth%20and%20plausible%20transitions%20between%20two%203D%20assets%2C%20preserving%20both%20structural%20coherence%20and%20fine-grained%20appearance.%20This%20ability%20is%20crucial%20not%20only%20for%20advancing%203D%20generation%20research%20but%20also%20for%20practical%20applications%20in%20animation%2C%20editing%2C%20and%20digital%20content%20creation.%20Existing%20approaches%20either%20operate%20directly%20on%20geometry%2C%20limiting%20them%20to%20shape-only%20morphing%20while%20neglecting%20textures%2C%20or%20extend%202D%20interpolation%20strategies%20into%203D%2C%20which%20often%20causes%20semantic%20ambiguity%2C%20structural%20misalignment%2C%20and%20texture%20blurring.%20These%20challenges%20underscore%20the%20necessity%20to%20jointly%20preserve%20geometric%20consistency%2C%20texture%20alignment%2C%20and%20robustness%20throughout%20the%20transition%20process.%20To%20address%20this%2C%20we%20propose%20Interp3D%2C%20a%20novel%20training-free%20framework%20for%20textured%203D%20morphing.%20It%20harnesses%20generative%20priors%20and%20adopts%20a%20progressive%20alignment%20principle%20to%20ensure%20both%20geometric%20fidelity%20and%20texture%20coherence.%20Starting%20from%20semantically%20aligned%20interpolation%20in%20condition%20space%2C%20Interp3D%20enforces%20structural%20consistency%20via%20SLAT%20%28Structured%20Latent%29-guided%20structure%20interpolation%2C%20and%20finally%20transfers%20appearance%20details%20through%20fine-grained%20texture%20fusion.%20For%20comprehensive%20evaluations%2C%20we%20construct%20a%20dedicated%20dataset%2C%20Interp3DData%2C%20with%20graded%20difficulty%20levels%20and%20assess%20generation%20results%20from%20fidelity%2C%20transition%20smoothness%2C%20and%20plausibility.%20Both%20quantitative%20metrics%20and%20human%20studies%20demonstrate%20the%20significant%20advantages%20of%20our%20proposed%20approach%20over%20previous%20methods.%20Source%20code%20is%20available%20at%20https%3A//github.com/xiaolul2/Interp3D.&entry.1838667208=http%3A//arxiv.org/abs/2601.14103v1&entry.124074799=Read"},
{"title": "Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints", "author": "Rotem Gatenyo and Ohad Fried", "abstract": "We study zero-shot 3D alignment of two given meshes, using a text prompt describing their spatial relation -- an essential capability for content creation and scene assembly. Earlier approaches primarily rely on geometric alignment procedures, while recent work leverages pretrained 2D diffusion models to model language-conditioned object-object spatial relationships. In contrast, we directly optimize the relative pose at test time, updating translation, rotation, and isotropic scale with CLIP-driven gradients via a differentiable renderer, without training a new model. Our framework augments language supervision with geometry-aware objectives: a variant of soft-Iterative Closest Point (ICP) term to encourage surface attachment and a penetration loss to discourage interpenetration. A phased schedule strengthens contact constraints over time, and camera control concentrates the optimization on the interaction region. To enable evaluation, we curate a benchmark containing diverse categories and relations, and compare against baselines. Our method outperforms all alternatives, yielding semantically faithful and physically plausible alignments.", "link": "http://arxiv.org/abs/2601.14207v1", "date": "2026-01-20", "relevancy": 2.9543, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6129}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5853}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5743}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Copy-Trasform-Paste%3A%20Zero-Shot%20Object-Object%20Alignment%20Guided%20by%20Vision-Language%20and%20Geometric%20Constraints&body=Title%3A%20Copy-Trasform-Paste%3A%20Zero-Shot%20Object-Object%20Alignment%20Guided%20by%20Vision-Language%20and%20Geometric%20Constraints%0AAuthor%3A%20Rotem%20Gatenyo%20and%20Ohad%20Fried%0AAbstract%3A%20We%20study%20zero-shot%203D%20alignment%20of%20two%20given%20meshes%2C%20using%20a%20text%20prompt%20describing%20their%20spatial%20relation%20--%20an%20essential%20capability%20for%20content%20creation%20and%20scene%20assembly.%20Earlier%20approaches%20primarily%20rely%20on%20geometric%20alignment%20procedures%2C%20while%20recent%20work%20leverages%20pretrained%202D%20diffusion%20models%20to%20model%20language-conditioned%20object-object%20spatial%20relationships.%20In%20contrast%2C%20we%20directly%20optimize%20the%20relative%20pose%20at%20test%20time%2C%20updating%20translation%2C%20rotation%2C%20and%20isotropic%20scale%20with%20CLIP-driven%20gradients%20via%20a%20differentiable%20renderer%2C%20without%20training%20a%20new%20model.%20Our%20framework%20augments%20language%20supervision%20with%20geometry-aware%20objectives%3A%20a%20variant%20of%20soft-Iterative%20Closest%20Point%20%28ICP%29%20term%20to%20encourage%20surface%20attachment%20and%20a%20penetration%20loss%20to%20discourage%20interpenetration.%20A%20phased%20schedule%20strengthens%20contact%20constraints%20over%20time%2C%20and%20camera%20control%20concentrates%20the%20optimization%20on%20the%20interaction%20region.%20To%20enable%20evaluation%2C%20we%20curate%20a%20benchmark%20containing%20diverse%20categories%20and%20relations%2C%20and%20compare%20against%20baselines.%20Our%20method%20outperforms%20all%20alternatives%2C%20yielding%20semantically%20faithful%20and%20physically%20plausible%20alignments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14207v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCopy-Trasform-Paste%253A%2520Zero-Shot%2520Object-Object%2520Alignment%2520Guided%2520by%2520Vision-Language%2520and%2520Geometric%2520Constraints%26entry.906535625%3DRotem%2520Gatenyo%2520and%2520Ohad%2520Fried%26entry.1292438233%3DWe%2520study%2520zero-shot%25203D%2520alignment%2520of%2520two%2520given%2520meshes%252C%2520using%2520a%2520text%2520prompt%2520describing%2520their%2520spatial%2520relation%2520--%2520an%2520essential%2520capability%2520for%2520content%2520creation%2520and%2520scene%2520assembly.%2520Earlier%2520approaches%2520primarily%2520rely%2520on%2520geometric%2520alignment%2520procedures%252C%2520while%2520recent%2520work%2520leverages%2520pretrained%25202D%2520diffusion%2520models%2520to%2520model%2520language-conditioned%2520object-object%2520spatial%2520relationships.%2520In%2520contrast%252C%2520we%2520directly%2520optimize%2520the%2520relative%2520pose%2520at%2520test%2520time%252C%2520updating%2520translation%252C%2520rotation%252C%2520and%2520isotropic%2520scale%2520with%2520CLIP-driven%2520gradients%2520via%2520a%2520differentiable%2520renderer%252C%2520without%2520training%2520a%2520new%2520model.%2520Our%2520framework%2520augments%2520language%2520supervision%2520with%2520geometry-aware%2520objectives%253A%2520a%2520variant%2520of%2520soft-Iterative%2520Closest%2520Point%2520%2528ICP%2529%2520term%2520to%2520encourage%2520surface%2520attachment%2520and%2520a%2520penetration%2520loss%2520to%2520discourage%2520interpenetration.%2520A%2520phased%2520schedule%2520strengthens%2520contact%2520constraints%2520over%2520time%252C%2520and%2520camera%2520control%2520concentrates%2520the%2520optimization%2520on%2520the%2520interaction%2520region.%2520To%2520enable%2520evaluation%252C%2520we%2520curate%2520a%2520benchmark%2520containing%2520diverse%2520categories%2520and%2520relations%252C%2520and%2520compare%2520against%2520baselines.%2520Our%2520method%2520outperforms%2520all%2520alternatives%252C%2520yielding%2520semantically%2520faithful%2520and%2520physically%2520plausible%2520alignments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14207v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Copy-Trasform-Paste%3A%20Zero-Shot%20Object-Object%20Alignment%20Guided%20by%20Vision-Language%20and%20Geometric%20Constraints&entry.906535625=Rotem%20Gatenyo%20and%20Ohad%20Fried&entry.1292438233=We%20study%20zero-shot%203D%20alignment%20of%20two%20given%20meshes%2C%20using%20a%20text%20prompt%20describing%20their%20spatial%20relation%20--%20an%20essential%20capability%20for%20content%20creation%20and%20scene%20assembly.%20Earlier%20approaches%20primarily%20rely%20on%20geometric%20alignment%20procedures%2C%20while%20recent%20work%20leverages%20pretrained%202D%20diffusion%20models%20to%20model%20language-conditioned%20object-object%20spatial%20relationships.%20In%20contrast%2C%20we%20directly%20optimize%20the%20relative%20pose%20at%20test%20time%2C%20updating%20translation%2C%20rotation%2C%20and%20isotropic%20scale%20with%20CLIP-driven%20gradients%20via%20a%20differentiable%20renderer%2C%20without%20training%20a%20new%20model.%20Our%20framework%20augments%20language%20supervision%20with%20geometry-aware%20objectives%3A%20a%20variant%20of%20soft-Iterative%20Closest%20Point%20%28ICP%29%20term%20to%20encourage%20surface%20attachment%20and%20a%20penetration%20loss%20to%20discourage%20interpenetration.%20A%20phased%20schedule%20strengthens%20contact%20constraints%20over%20time%2C%20and%20camera%20control%20concentrates%20the%20optimization%20on%20the%20interaction%20region.%20To%20enable%20evaluation%2C%20we%20curate%20a%20benchmark%20containing%20diverse%20categories%20and%20relations%2C%20and%20compare%20against%20baselines.%20Our%20method%20outperforms%20all%20alternatives%2C%20yielding%20semantically%20faithful%20and%20physically%20plausible%20alignments.&entry.1838667208=http%3A//arxiv.org/abs/2601.14207v1&entry.124074799=Read"},
{"title": "Implicit Neural Representation Facilitates Unified Universal Vision Encoding", "author": "Matthew Gwilliam and Xiao Wang and Xuefeng Hu and Zhenheng Yang", "abstract": "Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.", "link": "http://arxiv.org/abs/2601.14256v1", "date": "2026-01-20", "relevancy": 2.9028, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6177}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5735}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Neural%20Representation%20Facilitates%20Unified%20Universal%20Vision%20Encoding&body=Title%3A%20Implicit%20Neural%20Representation%20Facilitates%20Unified%20Universal%20Vision%20Encoding%0AAuthor%3A%20Matthew%20Gwilliam%20and%20Xiao%20Wang%20and%20Xuefeng%20Hu%20and%20Zhenheng%20Yang%0AAbstract%3A%20Models%20for%20image%20representation%20learning%20are%20typically%20designed%20for%20either%20recognition%20or%20generation.%20Various%20forms%20of%20contrastive%20learning%20help%20models%20learn%20to%20convert%20images%20to%20embeddings%20that%20are%20useful%20for%20classification%2C%20detection%2C%20and%20segmentation.%20On%20the%20other%20hand%2C%20models%20can%20be%20trained%20to%20reconstruct%20images%20with%20pixel-wise%2C%20perceptual%2C%20and%20adversarial%20losses%20in%20order%20to%20learn%20a%20latent%20space%20that%20is%20useful%20for%20image%20generation.%20We%20seek%20to%20unify%20these%20two%20directions%20with%20a%20first-of-its-kind%20model%20that%20learns%20representations%20which%20are%20simultaneously%20useful%20for%20recognition%20and%20generation.%20We%20train%20our%20model%20as%20a%20hyper-network%20for%20implicit%20neural%20representation%2C%20which%20learns%20to%20map%20images%20to%20model%20weights%20for%20fast%2C%20accurate%20reconstruction.%20We%20further%20integrate%20our%20INR%20hyper-network%20with%20knowledge%20distillation%20to%20improve%20its%20generalization%20and%20performance.%20Beyond%20the%20novel%20training%20design%2C%20the%20model%20also%20learns%20an%20unprecedented%20compressed%20embedding%20space%20with%20outstanding%20performance%20for%20various%20visual%20tasks.%20The%20complete%20model%20competes%20with%20state-of-the-art%20results%20for%20image%20representation%20learning%2C%20while%20also%20enabling%20generative%20capabilities%20with%20its%20high-quality%20tiny%20embeddings.%20The%20code%20is%20available%20at%20https%3A//github.com/tiktok/huvr.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14256v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Neural%2520Representation%2520Facilitates%2520Unified%2520Universal%2520Vision%2520Encoding%26entry.906535625%3DMatthew%2520Gwilliam%2520and%2520Xiao%2520Wang%2520and%2520Xuefeng%2520Hu%2520and%2520Zhenheng%2520Yang%26entry.1292438233%3DModels%2520for%2520image%2520representation%2520learning%2520are%2520typically%2520designed%2520for%2520either%2520recognition%2520or%2520generation.%2520Various%2520forms%2520of%2520contrastive%2520learning%2520help%2520models%2520learn%2520to%2520convert%2520images%2520to%2520embeddings%2520that%2520are%2520useful%2520for%2520classification%252C%2520detection%252C%2520and%2520segmentation.%2520On%2520the%2520other%2520hand%252C%2520models%2520can%2520be%2520trained%2520to%2520reconstruct%2520images%2520with%2520pixel-wise%252C%2520perceptual%252C%2520and%2520adversarial%2520losses%2520in%2520order%2520to%2520learn%2520a%2520latent%2520space%2520that%2520is%2520useful%2520for%2520image%2520generation.%2520We%2520seek%2520to%2520unify%2520these%2520two%2520directions%2520with%2520a%2520first-of-its-kind%2520model%2520that%2520learns%2520representations%2520which%2520are%2520simultaneously%2520useful%2520for%2520recognition%2520and%2520generation.%2520We%2520train%2520our%2520model%2520as%2520a%2520hyper-network%2520for%2520implicit%2520neural%2520representation%252C%2520which%2520learns%2520to%2520map%2520images%2520to%2520model%2520weights%2520for%2520fast%252C%2520accurate%2520reconstruction.%2520We%2520further%2520integrate%2520our%2520INR%2520hyper-network%2520with%2520knowledge%2520distillation%2520to%2520improve%2520its%2520generalization%2520and%2520performance.%2520Beyond%2520the%2520novel%2520training%2520design%252C%2520the%2520model%2520also%2520learns%2520an%2520unprecedented%2520compressed%2520embedding%2520space%2520with%2520outstanding%2520performance%2520for%2520various%2520visual%2520tasks.%2520The%2520complete%2520model%2520competes%2520with%2520state-of-the-art%2520results%2520for%2520image%2520representation%2520learning%252C%2520while%2520also%2520enabling%2520generative%2520capabilities%2520with%2520its%2520high-quality%2520tiny%2520embeddings.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/tiktok/huvr.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14256v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Neural%20Representation%20Facilitates%20Unified%20Universal%20Vision%20Encoding&entry.906535625=Matthew%20Gwilliam%20and%20Xiao%20Wang%20and%20Xuefeng%20Hu%20and%20Zhenheng%20Yang&entry.1292438233=Models%20for%20image%20representation%20learning%20are%20typically%20designed%20for%20either%20recognition%20or%20generation.%20Various%20forms%20of%20contrastive%20learning%20help%20models%20learn%20to%20convert%20images%20to%20embeddings%20that%20are%20useful%20for%20classification%2C%20detection%2C%20and%20segmentation.%20On%20the%20other%20hand%2C%20models%20can%20be%20trained%20to%20reconstruct%20images%20with%20pixel-wise%2C%20perceptual%2C%20and%20adversarial%20losses%20in%20order%20to%20learn%20a%20latent%20space%20that%20is%20useful%20for%20image%20generation.%20We%20seek%20to%20unify%20these%20two%20directions%20with%20a%20first-of-its-kind%20model%20that%20learns%20representations%20which%20are%20simultaneously%20useful%20for%20recognition%20and%20generation.%20We%20train%20our%20model%20as%20a%20hyper-network%20for%20implicit%20neural%20representation%2C%20which%20learns%20to%20map%20images%20to%20model%20weights%20for%20fast%2C%20accurate%20reconstruction.%20We%20further%20integrate%20our%20INR%20hyper-network%20with%20knowledge%20distillation%20to%20improve%20its%20generalization%20and%20performance.%20Beyond%20the%20novel%20training%20design%2C%20the%20model%20also%20learns%20an%20unprecedented%20compressed%20embedding%20space%20with%20outstanding%20performance%20for%20various%20visual%20tasks.%20The%20complete%20model%20competes%20with%20state-of-the-art%20results%20for%20image%20representation%20learning%2C%20while%20also%20enabling%20generative%20capabilities%20with%20its%20high-quality%20tiny%20embeddings.%20The%20code%20is%20available%20at%20https%3A//github.com/tiktok/huvr.&entry.1838667208=http%3A//arxiv.org/abs/2601.14256v1&entry.124074799=Read"},
{"title": "Fine-Grained Zero-Shot Composed Image Retrieval with Complementary Visual-Semantic Integration", "author": "Yongcong Ye and Kai Zhang and Yanghai Zhang and Enhong Chen and Longfei Li and Jun Zhou", "abstract": "Zero-shot composed image retrieval (ZS-CIR) is a rapidly growing area with significant practical applications, allowing users to retrieve a target image by providing a reference image and a relative caption describing the desired modifications. Existing ZS-CIR methods often struggle to capture fine-grained changes and integrate visual and semantic information effectively. They primarily rely on either transforming the multimodal query into a single text using image-to-text models or employing large language models for target image description generation, approaches that often fail to capture complementary visual information and complete semantic context. To address these limitations, we propose a novel Fine-Grained Zero-Shot Composed Image Retrieval method with Complementary Visual-Semantic Integration (CVSI). Specifically, CVSI leverages three key components: (1) Visual Information Extraction, which not only extracts global image features but also uses a pre-trained mapping network to convert the image into a pseudo token, combining it with the modification text and the objects most likely to be added. (2) Semantic Information Extraction, which involves using a pre-trained captioning model to generate multiple captions for the reference image, followed by leveraging an LLM to generate the modified captions and the objects most likely to be added. (3) Complementary Information Retrieval, which integrates information extracted from both the query and database images to retrieve the target image, enabling the system to efficiently handle retrieval queries in a variety of situations. Extensive experiments on three public datasets (e.g., CIRR, CIRCO, and FashionIQ) demonstrate that CVSI significantly outperforms existing state-of-the-art methods. Our code is available at https://github.com/yyc6631/CVSI.", "link": "http://arxiv.org/abs/2601.14060v1", "date": "2026-01-20", "relevancy": 2.8273, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5673}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5646}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5646}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fine-Grained%20Zero-Shot%20Composed%20Image%20Retrieval%20with%20Complementary%20Visual-Semantic%20Integration&body=Title%3A%20Fine-Grained%20Zero-Shot%20Composed%20Image%20Retrieval%20with%20Complementary%20Visual-Semantic%20Integration%0AAuthor%3A%20Yongcong%20Ye%20and%20Kai%20Zhang%20and%20Yanghai%20Zhang%20and%20Enhong%20Chen%20and%20Longfei%20Li%20and%20Jun%20Zhou%0AAbstract%3A%20Zero-shot%20composed%20image%20retrieval%20%28ZS-CIR%29%20is%20a%20rapidly%20growing%20area%20with%20significant%20practical%20applications%2C%20allowing%20users%20to%20retrieve%20a%20target%20image%20by%20providing%20a%20reference%20image%20and%20a%20relative%20caption%20describing%20the%20desired%20modifications.%20Existing%20ZS-CIR%20methods%20often%20struggle%20to%20capture%20fine-grained%20changes%20and%20integrate%20visual%20and%20semantic%20information%20effectively.%20They%20primarily%20rely%20on%20either%20transforming%20the%20multimodal%20query%20into%20a%20single%20text%20using%20image-to-text%20models%20or%20employing%20large%20language%20models%20for%20target%20image%20description%20generation%2C%20approaches%20that%20often%20fail%20to%20capture%20complementary%20visual%20information%20and%20complete%20semantic%20context.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20Fine-Grained%20Zero-Shot%20Composed%20Image%20Retrieval%20method%20with%20Complementary%20Visual-Semantic%20Integration%20%28CVSI%29.%20Specifically%2C%20CVSI%20leverages%20three%20key%20components%3A%20%281%29%20Visual%20Information%20Extraction%2C%20which%20not%20only%20extracts%20global%20image%20features%20but%20also%20uses%20a%20pre-trained%20mapping%20network%20to%20convert%20the%20image%20into%20a%20pseudo%20token%2C%20combining%20it%20with%20the%20modification%20text%20and%20the%20objects%20most%20likely%20to%20be%20added.%20%282%29%20Semantic%20Information%20Extraction%2C%20which%20involves%20using%20a%20pre-trained%20captioning%20model%20to%20generate%20multiple%20captions%20for%20the%20reference%20image%2C%20followed%20by%20leveraging%20an%20LLM%20to%20generate%20the%20modified%20captions%20and%20the%20objects%20most%20likely%20to%20be%20added.%20%283%29%20Complementary%20Information%20Retrieval%2C%20which%20integrates%20information%20extracted%20from%20both%20the%20query%20and%20database%20images%20to%20retrieve%20the%20target%20image%2C%20enabling%20the%20system%20to%20efficiently%20handle%20retrieval%20queries%20in%20a%20variety%20of%20situations.%20Extensive%20experiments%20on%20three%20public%20datasets%20%28e.g.%2C%20CIRR%2C%20CIRCO%2C%20and%20FashionIQ%29%20demonstrate%20that%20CVSI%20significantly%20outperforms%20existing%20state-of-the-art%20methods.%20Our%20code%20is%20available%20at%20https%3A//github.com/yyc6631/CVSI.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14060v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFine-Grained%2520Zero-Shot%2520Composed%2520Image%2520Retrieval%2520with%2520Complementary%2520Visual-Semantic%2520Integration%26entry.906535625%3DYongcong%2520Ye%2520and%2520Kai%2520Zhang%2520and%2520Yanghai%2520Zhang%2520and%2520Enhong%2520Chen%2520and%2520Longfei%2520Li%2520and%2520Jun%2520Zhou%26entry.1292438233%3DZero-shot%2520composed%2520image%2520retrieval%2520%2528ZS-CIR%2529%2520is%2520a%2520rapidly%2520growing%2520area%2520with%2520significant%2520practical%2520applications%252C%2520allowing%2520users%2520to%2520retrieve%2520a%2520target%2520image%2520by%2520providing%2520a%2520reference%2520image%2520and%2520a%2520relative%2520caption%2520describing%2520the%2520desired%2520modifications.%2520Existing%2520ZS-CIR%2520methods%2520often%2520struggle%2520to%2520capture%2520fine-grained%2520changes%2520and%2520integrate%2520visual%2520and%2520semantic%2520information%2520effectively.%2520They%2520primarily%2520rely%2520on%2520either%2520transforming%2520the%2520multimodal%2520query%2520into%2520a%2520single%2520text%2520using%2520image-to-text%2520models%2520or%2520employing%2520large%2520language%2520models%2520for%2520target%2520image%2520description%2520generation%252C%2520approaches%2520that%2520often%2520fail%2520to%2520capture%2520complementary%2520visual%2520information%2520and%2520complete%2520semantic%2520context.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520Fine-Grained%2520Zero-Shot%2520Composed%2520Image%2520Retrieval%2520method%2520with%2520Complementary%2520Visual-Semantic%2520Integration%2520%2528CVSI%2529.%2520Specifically%252C%2520CVSI%2520leverages%2520three%2520key%2520components%253A%2520%25281%2529%2520Visual%2520Information%2520Extraction%252C%2520which%2520not%2520only%2520extracts%2520global%2520image%2520features%2520but%2520also%2520uses%2520a%2520pre-trained%2520mapping%2520network%2520to%2520convert%2520the%2520image%2520into%2520a%2520pseudo%2520token%252C%2520combining%2520it%2520with%2520the%2520modification%2520text%2520and%2520the%2520objects%2520most%2520likely%2520to%2520be%2520added.%2520%25282%2529%2520Semantic%2520Information%2520Extraction%252C%2520which%2520involves%2520using%2520a%2520pre-trained%2520captioning%2520model%2520to%2520generate%2520multiple%2520captions%2520for%2520the%2520reference%2520image%252C%2520followed%2520by%2520leveraging%2520an%2520LLM%2520to%2520generate%2520the%2520modified%2520captions%2520and%2520the%2520objects%2520most%2520likely%2520to%2520be%2520added.%2520%25283%2529%2520Complementary%2520Information%2520Retrieval%252C%2520which%2520integrates%2520information%2520extracted%2520from%2520both%2520the%2520query%2520and%2520database%2520images%2520to%2520retrieve%2520the%2520target%2520image%252C%2520enabling%2520the%2520system%2520to%2520efficiently%2520handle%2520retrieval%2520queries%2520in%2520a%2520variety%2520of%2520situations.%2520Extensive%2520experiments%2520on%2520three%2520public%2520datasets%2520%2528e.g.%252C%2520CIRR%252C%2520CIRCO%252C%2520and%2520FashionIQ%2529%2520demonstrate%2520that%2520CVSI%2520significantly%2520outperforms%2520existing%2520state-of-the-art%2520methods.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/yyc6631/CVSI.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14060v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fine-Grained%20Zero-Shot%20Composed%20Image%20Retrieval%20with%20Complementary%20Visual-Semantic%20Integration&entry.906535625=Yongcong%20Ye%20and%20Kai%20Zhang%20and%20Yanghai%20Zhang%20and%20Enhong%20Chen%20and%20Longfei%20Li%20and%20Jun%20Zhou&entry.1292438233=Zero-shot%20composed%20image%20retrieval%20%28ZS-CIR%29%20is%20a%20rapidly%20growing%20area%20with%20significant%20practical%20applications%2C%20allowing%20users%20to%20retrieve%20a%20target%20image%20by%20providing%20a%20reference%20image%20and%20a%20relative%20caption%20describing%20the%20desired%20modifications.%20Existing%20ZS-CIR%20methods%20often%20struggle%20to%20capture%20fine-grained%20changes%20and%20integrate%20visual%20and%20semantic%20information%20effectively.%20They%20primarily%20rely%20on%20either%20transforming%20the%20multimodal%20query%20into%20a%20single%20text%20using%20image-to-text%20models%20or%20employing%20large%20language%20models%20for%20target%20image%20description%20generation%2C%20approaches%20that%20often%20fail%20to%20capture%20complementary%20visual%20information%20and%20complete%20semantic%20context.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20Fine-Grained%20Zero-Shot%20Composed%20Image%20Retrieval%20method%20with%20Complementary%20Visual-Semantic%20Integration%20%28CVSI%29.%20Specifically%2C%20CVSI%20leverages%20three%20key%20components%3A%20%281%29%20Visual%20Information%20Extraction%2C%20which%20not%20only%20extracts%20global%20image%20features%20but%20also%20uses%20a%20pre-trained%20mapping%20network%20to%20convert%20the%20image%20into%20a%20pseudo%20token%2C%20combining%20it%20with%20the%20modification%20text%20and%20the%20objects%20most%20likely%20to%20be%20added.%20%282%29%20Semantic%20Information%20Extraction%2C%20which%20involves%20using%20a%20pre-trained%20captioning%20model%20to%20generate%20multiple%20captions%20for%20the%20reference%20image%2C%20followed%20by%20leveraging%20an%20LLM%20to%20generate%20the%20modified%20captions%20and%20the%20objects%20most%20likely%20to%20be%20added.%20%283%29%20Complementary%20Information%20Retrieval%2C%20which%20integrates%20information%20extracted%20from%20both%20the%20query%20and%20database%20images%20to%20retrieve%20the%20target%20image%2C%20enabling%20the%20system%20to%20efficiently%20handle%20retrieval%20queries%20in%20a%20variety%20of%20situations.%20Extensive%20experiments%20on%20three%20public%20datasets%20%28e.g.%2C%20CIRR%2C%20CIRCO%2C%20and%20FashionIQ%29%20demonstrate%20that%20CVSI%20significantly%20outperforms%20existing%20state-of-the-art%20methods.%20Our%20code%20is%20available%20at%20https%3A//github.com/yyc6631/CVSI.&entry.1838667208=http%3A//arxiv.org/abs/2601.14060v1&entry.124074799=Read"},
{"title": "On the Role of Rotation Equivariance in Monocular 3D Human Pose Estimation", "author": "Pavlo Melnyk and Cuong Le and Urs Waldmann and Per-Erik Forss\u00e9n and Bastian Wandt", "abstract": "Estimating 3D from 2D is one of the central tasks in computer vision. In this work, we consider the monocular setting, i.e. single-view input, for 3D human pose estimation (HPE). Here, the task is to predict a 3D point set of human skeletal joints from a single 2D input image. While by definition this is an ill-posed problem, recent work has presented methods that solve it with up to several-centimetre error. Typically, these methods employ a two-step approach, where the first step is to detect the 2D skeletal joints in the input image, followed by the step of 2D-to-3D lifting. We find that common lifting models fail when encountering a rotated input. We argue that learning a single human pose along with its in-plane rotations is considerably easier and more geometrically grounded than directly learning a point-to-point mapping. Furthermore, our intuition is that endowing the model with the notion of rotation equivariance without explicitly constraining its parameter space should lead to a more straightforward learning process than one with equivariance by design. Utilising the common HPE benchmarks, we confirm that the 2D rotation equivariance per se improves the model performance on human poses akin to rotations in the image plane, and can be efficiently and straightforwardly learned by augmentation, outperforming state-of-the-art equivariant-by-design methods.", "link": "http://arxiv.org/abs/2601.13913v1", "date": "2026-01-20", "relevancy": 2.7913, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5803}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5612}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Role%20of%20Rotation%20Equivariance%20in%20Monocular%203D%20Human%20Pose%20Estimation&body=Title%3A%20On%20the%20Role%20of%20Rotation%20Equivariance%20in%20Monocular%203D%20Human%20Pose%20Estimation%0AAuthor%3A%20Pavlo%20Melnyk%20and%20Cuong%20Le%20and%20Urs%20Waldmann%20and%20Per-Erik%20Forss%C3%A9n%20and%20Bastian%20Wandt%0AAbstract%3A%20Estimating%203D%20from%202D%20is%20one%20of%20the%20central%20tasks%20in%20computer%20vision.%20In%20this%20work%2C%20we%20consider%20the%20monocular%20setting%2C%20i.e.%20single-view%20input%2C%20for%203D%20human%20pose%20estimation%20%28HPE%29.%20Here%2C%20the%20task%20is%20to%20predict%20a%203D%20point%20set%20of%20human%20skeletal%20joints%20from%20a%20single%202D%20input%20image.%20While%20by%20definition%20this%20is%20an%20ill-posed%20problem%2C%20recent%20work%20has%20presented%20methods%20that%20solve%20it%20with%20up%20to%20several-centimetre%20error.%20Typically%2C%20these%20methods%20employ%20a%20two-step%20approach%2C%20where%20the%20first%20step%20is%20to%20detect%20the%202D%20skeletal%20joints%20in%20the%20input%20image%2C%20followed%20by%20the%20step%20of%202D-to-3D%20lifting.%20We%20find%20that%20common%20lifting%20models%20fail%20when%20encountering%20a%20rotated%20input.%20We%20argue%20that%20learning%20a%20single%20human%20pose%20along%20with%20its%20in-plane%20rotations%20is%20considerably%20easier%20and%20more%20geometrically%20grounded%20than%20directly%20learning%20a%20point-to-point%20mapping.%20Furthermore%2C%20our%20intuition%20is%20that%20endowing%20the%20model%20with%20the%20notion%20of%20rotation%20equivariance%20without%20explicitly%20constraining%20its%20parameter%20space%20should%20lead%20to%20a%20more%20straightforward%20learning%20process%20than%20one%20with%20equivariance%20by%20design.%20Utilising%20the%20common%20HPE%20benchmarks%2C%20we%20confirm%20that%20the%202D%20rotation%20equivariance%20per%20se%20improves%20the%20model%20performance%20on%20human%20poses%20akin%20to%20rotations%20in%20the%20image%20plane%2C%20and%20can%20be%20efficiently%20and%20straightforwardly%20learned%20by%20augmentation%2C%20outperforming%20state-of-the-art%20equivariant-by-design%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13913v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Role%2520of%2520Rotation%2520Equivariance%2520in%2520Monocular%25203D%2520Human%2520Pose%2520Estimation%26entry.906535625%3DPavlo%2520Melnyk%2520and%2520Cuong%2520Le%2520and%2520Urs%2520Waldmann%2520and%2520Per-Erik%2520Forss%25C3%25A9n%2520and%2520Bastian%2520Wandt%26entry.1292438233%3DEstimating%25203D%2520from%25202D%2520is%2520one%2520of%2520the%2520central%2520tasks%2520in%2520computer%2520vision.%2520In%2520this%2520work%252C%2520we%2520consider%2520the%2520monocular%2520setting%252C%2520i.e.%2520single-view%2520input%252C%2520for%25203D%2520human%2520pose%2520estimation%2520%2528HPE%2529.%2520Here%252C%2520the%2520task%2520is%2520to%2520predict%2520a%25203D%2520point%2520set%2520of%2520human%2520skeletal%2520joints%2520from%2520a%2520single%25202D%2520input%2520image.%2520While%2520by%2520definition%2520this%2520is%2520an%2520ill-posed%2520problem%252C%2520recent%2520work%2520has%2520presented%2520methods%2520that%2520solve%2520it%2520with%2520up%2520to%2520several-centimetre%2520error.%2520Typically%252C%2520these%2520methods%2520employ%2520a%2520two-step%2520approach%252C%2520where%2520the%2520first%2520step%2520is%2520to%2520detect%2520the%25202D%2520skeletal%2520joints%2520in%2520the%2520input%2520image%252C%2520followed%2520by%2520the%2520step%2520of%25202D-to-3D%2520lifting.%2520We%2520find%2520that%2520common%2520lifting%2520models%2520fail%2520when%2520encountering%2520a%2520rotated%2520input.%2520We%2520argue%2520that%2520learning%2520a%2520single%2520human%2520pose%2520along%2520with%2520its%2520in-plane%2520rotations%2520is%2520considerably%2520easier%2520and%2520more%2520geometrically%2520grounded%2520than%2520directly%2520learning%2520a%2520point-to-point%2520mapping.%2520Furthermore%252C%2520our%2520intuition%2520is%2520that%2520endowing%2520the%2520model%2520with%2520the%2520notion%2520of%2520rotation%2520equivariance%2520without%2520explicitly%2520constraining%2520its%2520parameter%2520space%2520should%2520lead%2520to%2520a%2520more%2520straightforward%2520learning%2520process%2520than%2520one%2520with%2520equivariance%2520by%2520design.%2520Utilising%2520the%2520common%2520HPE%2520benchmarks%252C%2520we%2520confirm%2520that%2520the%25202D%2520rotation%2520equivariance%2520per%2520se%2520improves%2520the%2520model%2520performance%2520on%2520human%2520poses%2520akin%2520to%2520rotations%2520in%2520the%2520image%2520plane%252C%2520and%2520can%2520be%2520efficiently%2520and%2520straightforwardly%2520learned%2520by%2520augmentation%252C%2520outperforming%2520state-of-the-art%2520equivariant-by-design%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13913v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Role%20of%20Rotation%20Equivariance%20in%20Monocular%203D%20Human%20Pose%20Estimation&entry.906535625=Pavlo%20Melnyk%20and%20Cuong%20Le%20and%20Urs%20Waldmann%20and%20Per-Erik%20Forss%C3%A9n%20and%20Bastian%20Wandt&entry.1292438233=Estimating%203D%20from%202D%20is%20one%20of%20the%20central%20tasks%20in%20computer%20vision.%20In%20this%20work%2C%20we%20consider%20the%20monocular%20setting%2C%20i.e.%20single-view%20input%2C%20for%203D%20human%20pose%20estimation%20%28HPE%29.%20Here%2C%20the%20task%20is%20to%20predict%20a%203D%20point%20set%20of%20human%20skeletal%20joints%20from%20a%20single%202D%20input%20image.%20While%20by%20definition%20this%20is%20an%20ill-posed%20problem%2C%20recent%20work%20has%20presented%20methods%20that%20solve%20it%20with%20up%20to%20several-centimetre%20error.%20Typically%2C%20these%20methods%20employ%20a%20two-step%20approach%2C%20where%20the%20first%20step%20is%20to%20detect%20the%202D%20skeletal%20joints%20in%20the%20input%20image%2C%20followed%20by%20the%20step%20of%202D-to-3D%20lifting.%20We%20find%20that%20common%20lifting%20models%20fail%20when%20encountering%20a%20rotated%20input.%20We%20argue%20that%20learning%20a%20single%20human%20pose%20along%20with%20its%20in-plane%20rotations%20is%20considerably%20easier%20and%20more%20geometrically%20grounded%20than%20directly%20learning%20a%20point-to-point%20mapping.%20Furthermore%2C%20our%20intuition%20is%20that%20endowing%20the%20model%20with%20the%20notion%20of%20rotation%20equivariance%20without%20explicitly%20constraining%20its%20parameter%20space%20should%20lead%20to%20a%20more%20straightforward%20learning%20process%20than%20one%20with%20equivariance%20by%20design.%20Utilising%20the%20common%20HPE%20benchmarks%2C%20we%20confirm%20that%20the%202D%20rotation%20equivariance%20per%20se%20improves%20the%20model%20performance%20on%20human%20poses%20akin%20to%20rotations%20in%20the%20image%20plane%2C%20and%20can%20be%20efficiently%20and%20straightforwardly%20learned%20by%20augmentation%2C%20outperforming%20state-of-the-art%20equivariant-by-design%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.13913v1&entry.124074799=Read"},
{"title": "FlyPose: Towards Robust Human Pose Estimation From Aerial Views", "author": "Hassaan Farooq and Marvin Brenner and Peter St\u00fctz", "abstract": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ~20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose.", "link": "http://arxiv.org/abs/2601.05747v2", "date": "2026-01-20", "relevancy": 2.7861, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5664}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5623}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.543}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FlyPose%3A%20Towards%20Robust%20Human%20Pose%20Estimation%20From%20Aerial%20Views&body=Title%3A%20FlyPose%3A%20Towards%20Robust%20Human%20Pose%20Estimation%20From%20Aerial%20Views%0AAuthor%3A%20Hassaan%20Farooq%20and%20Marvin%20Brenner%20and%20Peter%20St%C3%BCtz%0AAbstract%3A%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20are%20increasingly%20deployed%20in%20close%20proximity%20to%20humans%20for%20applications%20such%20as%20parcel%20delivery%2C%20traffic%20monitoring%2C%20disaster%20response%20and%20infrastructure%20inspections.%20Ensuring%20safe%20and%20reliable%20operation%20in%20these%20human-populated%20environments%20demands%20accurate%20perception%20of%20human%20poses%20and%20actions%20from%20an%20aerial%20viewpoint.%20This%20perspective%20challenges%20existing%20methods%20with%20low%20resolution%2C%20steep%20viewing%20angles%20and%20%28self-%29occlusion%2C%20especially%20if%20the%20application%20demands%20realtime%20feasibile%20models.%20We%20train%20and%20deploy%20FlyPose%2C%20a%20lightweight%20top-down%20human%20pose%20estimation%20pipeline%20for%20aerial%20imagery.%20Through%20multi-dataset%20training%2C%20we%20achieve%20an%20average%20improvement%20of%206.8%20mAP%20in%20person%20detection%20across%20the%20test-sets%20of%20Manipal-UAV%2C%20VisDrone%2C%20HIT-UAV%20as%20well%20as%20our%20custom%20dataset.%20For%202D%20human%20pose%20estimation%20we%20report%20an%20improvement%20of%2016.3%20mAP%20on%20the%20challenging%20UAV-Human%20dataset.%20FlyPose%20runs%20with%20an%20inference%20latency%20of%20~20%20milliseconds%20including%20preprocessing%20on%20a%20Jetson%20Orin%20AGX%20Developer%20Kit%20and%20is%20deployed%20onboard%20a%20quadrotor%20UAV%20during%20flight%20experiments.%20We%20also%20publish%20FlyPose-104%2C%20a%20small%20but%20challenging%20aerial%20human%20pose%20estimation%20dataset%2C%20that%20includes%20manual%20annotations%20from%20difficult%20aerial%20perspectives%3A%20https%3A//github.com/farooqhassaan/FlyPose.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05747v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFlyPose%253A%2520Towards%2520Robust%2520Human%2520Pose%2520Estimation%2520From%2520Aerial%2520Views%26entry.906535625%3DHassaan%2520Farooq%2520and%2520Marvin%2520Brenner%2520and%2520Peter%2520St%25C3%25BCtz%26entry.1292438233%3DUnmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520are%2520increasingly%2520deployed%2520in%2520close%2520proximity%2520to%2520humans%2520for%2520applications%2520such%2520as%2520parcel%2520delivery%252C%2520traffic%2520monitoring%252C%2520disaster%2520response%2520and%2520infrastructure%2520inspections.%2520Ensuring%2520safe%2520and%2520reliable%2520operation%2520in%2520these%2520human-populated%2520environments%2520demands%2520accurate%2520perception%2520of%2520human%2520poses%2520and%2520actions%2520from%2520an%2520aerial%2520viewpoint.%2520This%2520perspective%2520challenges%2520existing%2520methods%2520with%2520low%2520resolution%252C%2520steep%2520viewing%2520angles%2520and%2520%2528self-%2529occlusion%252C%2520especially%2520if%2520the%2520application%2520demands%2520realtime%2520feasibile%2520models.%2520We%2520train%2520and%2520deploy%2520FlyPose%252C%2520a%2520lightweight%2520top-down%2520human%2520pose%2520estimation%2520pipeline%2520for%2520aerial%2520imagery.%2520Through%2520multi-dataset%2520training%252C%2520we%2520achieve%2520an%2520average%2520improvement%2520of%25206.8%2520mAP%2520in%2520person%2520detection%2520across%2520the%2520test-sets%2520of%2520Manipal-UAV%252C%2520VisDrone%252C%2520HIT-UAV%2520as%2520well%2520as%2520our%2520custom%2520dataset.%2520For%25202D%2520human%2520pose%2520estimation%2520we%2520report%2520an%2520improvement%2520of%252016.3%2520mAP%2520on%2520the%2520challenging%2520UAV-Human%2520dataset.%2520FlyPose%2520runs%2520with%2520an%2520inference%2520latency%2520of%2520~20%2520milliseconds%2520including%2520preprocessing%2520on%2520a%2520Jetson%2520Orin%2520AGX%2520Developer%2520Kit%2520and%2520is%2520deployed%2520onboard%2520a%2520quadrotor%2520UAV%2520during%2520flight%2520experiments.%2520We%2520also%2520publish%2520FlyPose-104%252C%2520a%2520small%2520but%2520challenging%2520aerial%2520human%2520pose%2520estimation%2520dataset%252C%2520that%2520includes%2520manual%2520annotations%2520from%2520difficult%2520aerial%2520perspectives%253A%2520https%253A//github.com/farooqhassaan/FlyPose.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05747v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FlyPose%3A%20Towards%20Robust%20Human%20Pose%20Estimation%20From%20Aerial%20Views&entry.906535625=Hassaan%20Farooq%20and%20Marvin%20Brenner%20and%20Peter%20St%C3%BCtz&entry.1292438233=Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20are%20increasingly%20deployed%20in%20close%20proximity%20to%20humans%20for%20applications%20such%20as%20parcel%20delivery%2C%20traffic%20monitoring%2C%20disaster%20response%20and%20infrastructure%20inspections.%20Ensuring%20safe%20and%20reliable%20operation%20in%20these%20human-populated%20environments%20demands%20accurate%20perception%20of%20human%20poses%20and%20actions%20from%20an%20aerial%20viewpoint.%20This%20perspective%20challenges%20existing%20methods%20with%20low%20resolution%2C%20steep%20viewing%20angles%20and%20%28self-%29occlusion%2C%20especially%20if%20the%20application%20demands%20realtime%20feasibile%20models.%20We%20train%20and%20deploy%20FlyPose%2C%20a%20lightweight%20top-down%20human%20pose%20estimation%20pipeline%20for%20aerial%20imagery.%20Through%20multi-dataset%20training%2C%20we%20achieve%20an%20average%20improvement%20of%206.8%20mAP%20in%20person%20detection%20across%20the%20test-sets%20of%20Manipal-UAV%2C%20VisDrone%2C%20HIT-UAV%20as%20well%20as%20our%20custom%20dataset.%20For%202D%20human%20pose%20estimation%20we%20report%20an%20improvement%20of%2016.3%20mAP%20on%20the%20challenging%20UAV-Human%20dataset.%20FlyPose%20runs%20with%20an%20inference%20latency%20of%20~20%20milliseconds%20including%20preprocessing%20on%20a%20Jetson%20Orin%20AGX%20Developer%20Kit%20and%20is%20deployed%20onboard%20a%20quadrotor%20UAV%20during%20flight%20experiments.%20We%20also%20publish%20FlyPose-104%2C%20a%20small%20but%20challenging%20aerial%20human%20pose%20estimation%20dataset%2C%20that%20includes%20manual%20annotations%20from%20difficult%20aerial%20perspectives%3A%20https%3A//github.com/farooqhassaan/FlyPose.&entry.1838667208=http%3A//arxiv.org/abs/2601.05747v2&entry.124074799=Read"},
{"title": "Glance-or-Gaze: Incentivizing LMMs to Adaptively Focus Search via Reinforcement Learning", "author": "Hongbo Bai and Yujin Zhou and Yile Wu and Chi-Min Chan and Pengcheng Wen and Kunhao Pan and Sirui Han and Yike Guo", "abstract": "Large Multimodal Models (LMMs) have achieved remarkable success in visual understanding, yet they struggle with knowledge-intensive queries involving long-tail entities or evolving information due to static parametric knowledge. Recent search-augmented approaches attempt to address this limitation, but existing methods rely on indiscriminate whole-image retrieval that introduces substantial visual redundancy and noise, and lack deep iterative reflection, limiting their effectiveness on complex visual queries. To overcome these challenges, we propose Glance-or-Gaze (GoG), a fully autonomous framework that shifts from passive perception to active visual planning. GoG introduces a Selective Gaze mechanism that dynamically chooses whether to glance at global context or gaze into high-value regions, filtering irrelevant information before retrieval. We design a dual-stage training strategy: Reflective GoG Behavior Alignment via supervised fine-tuning instills the fundamental GoG paradigm, while Complexity-Adaptive Reinforcement Learning further enhances the model's capability to handle complex queries through iterative reasoning. Experiments across six benchmarks demonstrate state-of-the-art performance. Ablation studies confirm that both Selective Gaze and complexity-adaptive RL are essential for effective visual search. We will release our data and models for further exploration soon.", "link": "http://arxiv.org/abs/2601.13942v1", "date": "2026-01-20", "relevancy": 2.7662, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5542}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5528}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Glance-or-Gaze%3A%20Incentivizing%20LMMs%20to%20Adaptively%20Focus%20Search%20via%20Reinforcement%20Learning&body=Title%3A%20Glance-or-Gaze%3A%20Incentivizing%20LMMs%20to%20Adaptively%20Focus%20Search%20via%20Reinforcement%20Learning%0AAuthor%3A%20Hongbo%20Bai%20and%20Yujin%20Zhou%20and%20Yile%20Wu%20and%20Chi-Min%20Chan%20and%20Pengcheng%20Wen%20and%20Kunhao%20Pan%20and%20Sirui%20Han%20and%20Yike%20Guo%0AAbstract%3A%20Large%20Multimodal%20Models%20%28LMMs%29%20have%20achieved%20remarkable%20success%20in%20visual%20understanding%2C%20yet%20they%20struggle%20with%20knowledge-intensive%20queries%20involving%20long-tail%20entities%20or%20evolving%20information%20due%20to%20static%20parametric%20knowledge.%20Recent%20search-augmented%20approaches%20attempt%20to%20address%20this%20limitation%2C%20but%20existing%20methods%20rely%20on%20indiscriminate%20whole-image%20retrieval%20that%20introduces%20substantial%20visual%20redundancy%20and%20noise%2C%20and%20lack%20deep%20iterative%20reflection%2C%20limiting%20their%20effectiveness%20on%20complex%20visual%20queries.%20To%20overcome%20these%20challenges%2C%20we%20propose%20Glance-or-Gaze%20%28GoG%29%2C%20a%20fully%20autonomous%20framework%20that%20shifts%20from%20passive%20perception%20to%20active%20visual%20planning.%20GoG%20introduces%20a%20Selective%20Gaze%20mechanism%20that%20dynamically%20chooses%20whether%20to%20glance%20at%20global%20context%20or%20gaze%20into%20high-value%20regions%2C%20filtering%20irrelevant%20information%20before%20retrieval.%20We%20design%20a%20dual-stage%20training%20strategy%3A%20Reflective%20GoG%20Behavior%20Alignment%20via%20supervised%20fine-tuning%20instills%20the%20fundamental%20GoG%20paradigm%2C%20while%20Complexity-Adaptive%20Reinforcement%20Learning%20further%20enhances%20the%20model%27s%20capability%20to%20handle%20complex%20queries%20through%20iterative%20reasoning.%20Experiments%20across%20six%20benchmarks%20demonstrate%20state-of-the-art%20performance.%20Ablation%20studies%20confirm%20that%20both%20Selective%20Gaze%20and%20complexity-adaptive%20RL%20are%20essential%20for%20effective%20visual%20search.%20We%20will%20release%20our%20data%20and%20models%20for%20further%20exploration%20soon.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13942v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlance-or-Gaze%253A%2520Incentivizing%2520LMMs%2520to%2520Adaptively%2520Focus%2520Search%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DHongbo%2520Bai%2520and%2520Yujin%2520Zhou%2520and%2520Yile%2520Wu%2520and%2520Chi-Min%2520Chan%2520and%2520Pengcheng%2520Wen%2520and%2520Kunhao%2520Pan%2520and%2520Sirui%2520Han%2520and%2520Yike%2520Guo%26entry.1292438233%3DLarge%2520Multimodal%2520Models%2520%2528LMMs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%2520visual%2520understanding%252C%2520yet%2520they%2520struggle%2520with%2520knowledge-intensive%2520queries%2520involving%2520long-tail%2520entities%2520or%2520evolving%2520information%2520due%2520to%2520static%2520parametric%2520knowledge.%2520Recent%2520search-augmented%2520approaches%2520attempt%2520to%2520address%2520this%2520limitation%252C%2520but%2520existing%2520methods%2520rely%2520on%2520indiscriminate%2520whole-image%2520retrieval%2520that%2520introduces%2520substantial%2520visual%2520redundancy%2520and%2520noise%252C%2520and%2520lack%2520deep%2520iterative%2520reflection%252C%2520limiting%2520their%2520effectiveness%2520on%2520complex%2520visual%2520queries.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520propose%2520Glance-or-Gaze%2520%2528GoG%2529%252C%2520a%2520fully%2520autonomous%2520framework%2520that%2520shifts%2520from%2520passive%2520perception%2520to%2520active%2520visual%2520planning.%2520GoG%2520introduces%2520a%2520Selective%2520Gaze%2520mechanism%2520that%2520dynamically%2520chooses%2520whether%2520to%2520glance%2520at%2520global%2520context%2520or%2520gaze%2520into%2520high-value%2520regions%252C%2520filtering%2520irrelevant%2520information%2520before%2520retrieval.%2520We%2520design%2520a%2520dual-stage%2520training%2520strategy%253A%2520Reflective%2520GoG%2520Behavior%2520Alignment%2520via%2520supervised%2520fine-tuning%2520instills%2520the%2520fundamental%2520GoG%2520paradigm%252C%2520while%2520Complexity-Adaptive%2520Reinforcement%2520Learning%2520further%2520enhances%2520the%2520model%2527s%2520capability%2520to%2520handle%2520complex%2520queries%2520through%2520iterative%2520reasoning.%2520Experiments%2520across%2520six%2520benchmarks%2520demonstrate%2520state-of-the-art%2520performance.%2520Ablation%2520studies%2520confirm%2520that%2520both%2520Selective%2520Gaze%2520and%2520complexity-adaptive%2520RL%2520are%2520essential%2520for%2520effective%2520visual%2520search.%2520We%2520will%2520release%2520our%2520data%2520and%2520models%2520for%2520further%2520exploration%2520soon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13942v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Glance-or-Gaze%3A%20Incentivizing%20LMMs%20to%20Adaptively%20Focus%20Search%20via%20Reinforcement%20Learning&entry.906535625=Hongbo%20Bai%20and%20Yujin%20Zhou%20and%20Yile%20Wu%20and%20Chi-Min%20Chan%20and%20Pengcheng%20Wen%20and%20Kunhao%20Pan%20and%20Sirui%20Han%20and%20Yike%20Guo&entry.1292438233=Large%20Multimodal%20Models%20%28LMMs%29%20have%20achieved%20remarkable%20success%20in%20visual%20understanding%2C%20yet%20they%20struggle%20with%20knowledge-intensive%20queries%20involving%20long-tail%20entities%20or%20evolving%20information%20due%20to%20static%20parametric%20knowledge.%20Recent%20search-augmented%20approaches%20attempt%20to%20address%20this%20limitation%2C%20but%20existing%20methods%20rely%20on%20indiscriminate%20whole-image%20retrieval%20that%20introduces%20substantial%20visual%20redundancy%20and%20noise%2C%20and%20lack%20deep%20iterative%20reflection%2C%20limiting%20their%20effectiveness%20on%20complex%20visual%20queries.%20To%20overcome%20these%20challenges%2C%20we%20propose%20Glance-or-Gaze%20%28GoG%29%2C%20a%20fully%20autonomous%20framework%20that%20shifts%20from%20passive%20perception%20to%20active%20visual%20planning.%20GoG%20introduces%20a%20Selective%20Gaze%20mechanism%20that%20dynamically%20chooses%20whether%20to%20glance%20at%20global%20context%20or%20gaze%20into%20high-value%20regions%2C%20filtering%20irrelevant%20information%20before%20retrieval.%20We%20design%20a%20dual-stage%20training%20strategy%3A%20Reflective%20GoG%20Behavior%20Alignment%20via%20supervised%20fine-tuning%20instills%20the%20fundamental%20GoG%20paradigm%2C%20while%20Complexity-Adaptive%20Reinforcement%20Learning%20further%20enhances%20the%20model%27s%20capability%20to%20handle%20complex%20queries%20through%20iterative%20reasoning.%20Experiments%20across%20six%20benchmarks%20demonstrate%20state-of-the-art%20performance.%20Ablation%20studies%20confirm%20that%20both%20Selective%20Gaze%20and%20complexity-adaptive%20RL%20are%20essential%20for%20effective%20visual%20search.%20We%20will%20release%20our%20data%20and%20models%20for%20further%20exploration%20soon.&entry.1838667208=http%3A//arxiv.org/abs/2601.13942v1&entry.124074799=Read"},
{"title": "Decoder-Free Supervoxel GNN for Accurate Brain-Tumor Localization in Multi-Modal MRI", "author": "Andrea Protani and Marc Molina Van Den Bosch and Lorenzo Giusti and Heloisa Barbosa Da Silva and Paolo Cacace and Albert Sund Aillet and Miguel Angel Gonzalez Ballester and Friedhelm Hummel and Luigi Serio", "abstract": "Modern vision backbones for 3D medical imaging typically process dense voxel grids through parameter-heavy encoder-decoder structures, a design that allocates a significant portion of its parameters to spatial reconstruction rather than feature learning. Our approach introduces SVGFormer, a decoder-free pipeline built upon a content-aware grouping stage that partitions the volume into a semantic graph of supervoxels. Its hierarchical encoder learns rich node representations by combining a patch-level Transformer with a supervoxel-level Graph Attention Network, jointly modeling fine-grained intra-region features and broader inter-regional dependencies. This design concentrates all learnable capacity on feature encoding and provides inherent, dual-scale explainability from the patch to the region level. To validate the framework's flexibility, we trained two specialized models on the BraTS dataset: one for node-level classification and one for tumor proportion regression. Both models achieved strong performance, with the classification model achieving a F1-score of 0.875 and the regression model a MAE of 0.028, confirming the encoder's ability to learn discriminative and localized features. Our results establish that a graph-based, encoder-only paradigm offers an accurate and inherently interpretable alternative for 3D medical image representation.", "link": "http://arxiv.org/abs/2601.14055v1", "date": "2026-01-20", "relevancy": 2.7632, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5544}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5517}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5517}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoder-Free%20Supervoxel%20GNN%20for%20Accurate%20Brain-Tumor%20Localization%20in%20Multi-Modal%20MRI&body=Title%3A%20Decoder-Free%20Supervoxel%20GNN%20for%20Accurate%20Brain-Tumor%20Localization%20in%20Multi-Modal%20MRI%0AAuthor%3A%20Andrea%20Protani%20and%20Marc%20Molina%20Van%20Den%20Bosch%20and%20Lorenzo%20Giusti%20and%20Heloisa%20Barbosa%20Da%20Silva%20and%20Paolo%20Cacace%20and%20Albert%20Sund%20Aillet%20and%20Miguel%20Angel%20Gonzalez%20Ballester%20and%20Friedhelm%20Hummel%20and%20Luigi%20Serio%0AAbstract%3A%20Modern%20vision%20backbones%20for%203D%20medical%20imaging%20typically%20process%20dense%20voxel%20grids%20through%20parameter-heavy%20encoder-decoder%20structures%2C%20a%20design%20that%20allocates%20a%20significant%20portion%20of%20its%20parameters%20to%20spatial%20reconstruction%20rather%20than%20feature%20learning.%20Our%20approach%20introduces%20SVGFormer%2C%20a%20decoder-free%20pipeline%20built%20upon%20a%20content-aware%20grouping%20stage%20that%20partitions%20the%20volume%20into%20a%20semantic%20graph%20of%20supervoxels.%20Its%20hierarchical%20encoder%20learns%20rich%20node%20representations%20by%20combining%20a%20patch-level%20Transformer%20with%20a%20supervoxel-level%20Graph%20Attention%20Network%2C%20jointly%20modeling%20fine-grained%20intra-region%20features%20and%20broader%20inter-regional%20dependencies.%20This%20design%20concentrates%20all%20learnable%20capacity%20on%20feature%20encoding%20and%20provides%20inherent%2C%20dual-scale%20explainability%20from%20the%20patch%20to%20the%20region%20level.%20To%20validate%20the%20framework%27s%20flexibility%2C%20we%20trained%20two%20specialized%20models%20on%20the%20BraTS%20dataset%3A%20one%20for%20node-level%20classification%20and%20one%20for%20tumor%20proportion%20regression.%20Both%20models%20achieved%20strong%20performance%2C%20with%20the%20classification%20model%20achieving%20a%20F1-score%20of%200.875%20and%20the%20regression%20model%20a%20MAE%20of%200.028%2C%20confirming%20the%20encoder%27s%20ability%20to%20learn%20discriminative%20and%20localized%20features.%20Our%20results%20establish%20that%20a%20graph-based%2C%20encoder-only%20paradigm%20offers%20an%20accurate%20and%20inherently%20interpretable%20alternative%20for%203D%20medical%20image%20representation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14055v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoder-Free%2520Supervoxel%2520GNN%2520for%2520Accurate%2520Brain-Tumor%2520Localization%2520in%2520Multi-Modal%2520MRI%26entry.906535625%3DAndrea%2520Protani%2520and%2520Marc%2520Molina%2520Van%2520Den%2520Bosch%2520and%2520Lorenzo%2520Giusti%2520and%2520Heloisa%2520Barbosa%2520Da%2520Silva%2520and%2520Paolo%2520Cacace%2520and%2520Albert%2520Sund%2520Aillet%2520and%2520Miguel%2520Angel%2520Gonzalez%2520Ballester%2520and%2520Friedhelm%2520Hummel%2520and%2520Luigi%2520Serio%26entry.1292438233%3DModern%2520vision%2520backbones%2520for%25203D%2520medical%2520imaging%2520typically%2520process%2520dense%2520voxel%2520grids%2520through%2520parameter-heavy%2520encoder-decoder%2520structures%252C%2520a%2520design%2520that%2520allocates%2520a%2520significant%2520portion%2520of%2520its%2520parameters%2520to%2520spatial%2520reconstruction%2520rather%2520than%2520feature%2520learning.%2520Our%2520approach%2520introduces%2520SVGFormer%252C%2520a%2520decoder-free%2520pipeline%2520built%2520upon%2520a%2520content-aware%2520grouping%2520stage%2520that%2520partitions%2520the%2520volume%2520into%2520a%2520semantic%2520graph%2520of%2520supervoxels.%2520Its%2520hierarchical%2520encoder%2520learns%2520rich%2520node%2520representations%2520by%2520combining%2520a%2520patch-level%2520Transformer%2520with%2520a%2520supervoxel-level%2520Graph%2520Attention%2520Network%252C%2520jointly%2520modeling%2520fine-grained%2520intra-region%2520features%2520and%2520broader%2520inter-regional%2520dependencies.%2520This%2520design%2520concentrates%2520all%2520learnable%2520capacity%2520on%2520feature%2520encoding%2520and%2520provides%2520inherent%252C%2520dual-scale%2520explainability%2520from%2520the%2520patch%2520to%2520the%2520region%2520level.%2520To%2520validate%2520the%2520framework%2527s%2520flexibility%252C%2520we%2520trained%2520two%2520specialized%2520models%2520on%2520the%2520BraTS%2520dataset%253A%2520one%2520for%2520node-level%2520classification%2520and%2520one%2520for%2520tumor%2520proportion%2520regression.%2520Both%2520models%2520achieved%2520strong%2520performance%252C%2520with%2520the%2520classification%2520model%2520achieving%2520a%2520F1-score%2520of%25200.875%2520and%2520the%2520regression%2520model%2520a%2520MAE%2520of%25200.028%252C%2520confirming%2520the%2520encoder%2527s%2520ability%2520to%2520learn%2520discriminative%2520and%2520localized%2520features.%2520Our%2520results%2520establish%2520that%2520a%2520graph-based%252C%2520encoder-only%2520paradigm%2520offers%2520an%2520accurate%2520and%2520inherently%2520interpretable%2520alternative%2520for%25203D%2520medical%2520image%2520representation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14055v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoder-Free%20Supervoxel%20GNN%20for%20Accurate%20Brain-Tumor%20Localization%20in%20Multi-Modal%20MRI&entry.906535625=Andrea%20Protani%20and%20Marc%20Molina%20Van%20Den%20Bosch%20and%20Lorenzo%20Giusti%20and%20Heloisa%20Barbosa%20Da%20Silva%20and%20Paolo%20Cacace%20and%20Albert%20Sund%20Aillet%20and%20Miguel%20Angel%20Gonzalez%20Ballester%20and%20Friedhelm%20Hummel%20and%20Luigi%20Serio&entry.1292438233=Modern%20vision%20backbones%20for%203D%20medical%20imaging%20typically%20process%20dense%20voxel%20grids%20through%20parameter-heavy%20encoder-decoder%20structures%2C%20a%20design%20that%20allocates%20a%20significant%20portion%20of%20its%20parameters%20to%20spatial%20reconstruction%20rather%20than%20feature%20learning.%20Our%20approach%20introduces%20SVGFormer%2C%20a%20decoder-free%20pipeline%20built%20upon%20a%20content-aware%20grouping%20stage%20that%20partitions%20the%20volume%20into%20a%20semantic%20graph%20of%20supervoxels.%20Its%20hierarchical%20encoder%20learns%20rich%20node%20representations%20by%20combining%20a%20patch-level%20Transformer%20with%20a%20supervoxel-level%20Graph%20Attention%20Network%2C%20jointly%20modeling%20fine-grained%20intra-region%20features%20and%20broader%20inter-regional%20dependencies.%20This%20design%20concentrates%20all%20learnable%20capacity%20on%20feature%20encoding%20and%20provides%20inherent%2C%20dual-scale%20explainability%20from%20the%20patch%20to%20the%20region%20level.%20To%20validate%20the%20framework%27s%20flexibility%2C%20we%20trained%20two%20specialized%20models%20on%20the%20BraTS%20dataset%3A%20one%20for%20node-level%20classification%20and%20one%20for%20tumor%20proportion%20regression.%20Both%20models%20achieved%20strong%20performance%2C%20with%20the%20classification%20model%20achieving%20a%20F1-score%20of%200.875%20and%20the%20regression%20model%20a%20MAE%20of%200.028%2C%20confirming%20the%20encoder%27s%20ability%20to%20learn%20discriminative%20and%20localized%20features.%20Our%20results%20establish%20that%20a%20graph-based%2C%20encoder-only%20paradigm%20offers%20an%20accurate%20and%20inherently%20interpretable%20alternative%20for%203D%20medical%20image%20representation.&entry.1838667208=http%3A//arxiv.org/abs/2601.14055v1&entry.124074799=Read"},
{"title": "Chain-of-Thought Compression Should Not Be Blind: V-Skip for Efficient Multimodal Reasoning via Dual-Path Anchoring", "author": "Dongxu Zhang and Yiding Sun and Cheng Tan and Wenbiao Yan and Ning Yang and Jihua Zhu and Hiajun Zhang", "abstract": "While Chain-of-Thought (CoT) reasoning significantly enhances the performance of Multimodal Large Language Models (MLLMs), its autoregressive nature incurs prohibitive latency constraints. Current efforts to mitigate this via token compression often fail by blindly applying text-centric metrics to multimodal contexts. We identify a critical failure mode termed Visual Amnesia, where linguistically redundant tokens are erroneously pruned, leading to hallucinations. To address this, we introduce V-Skip that reformulates token pruning as a Visual-Anchored Information Bottleneck (VA-IB) optimization problem. V-Skip employs a dual-path gating mechanism that weighs token importance through both linguistic surprisal and cross-modal attention flow, effectively rescuing visually salient anchors. Extensive experiments on Qwen2-VL and Llama-3.2 families demonstrate that V-Skip achieves a $2.9\\times$ speedup with negligible accuracy loss. Specifically, it preserves fine-grained visual details, outperforming other baselines over 30\\% on the DocVQA.", "link": "http://arxiv.org/abs/2601.13879v1", "date": "2026-01-20", "relevancy": 2.723, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5464}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5464}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chain-of-Thought%20Compression%20Should%20Not%20Be%20Blind%3A%20V-Skip%20for%20Efficient%20Multimodal%20Reasoning%20via%20Dual-Path%20Anchoring&body=Title%3A%20Chain-of-Thought%20Compression%20Should%20Not%20Be%20Blind%3A%20V-Skip%20for%20Efficient%20Multimodal%20Reasoning%20via%20Dual-Path%20Anchoring%0AAuthor%3A%20Dongxu%20Zhang%20and%20Yiding%20Sun%20and%20Cheng%20Tan%20and%20Wenbiao%20Yan%20and%20Ning%20Yang%20and%20Jihua%20Zhu%20and%20Hiajun%20Zhang%0AAbstract%3A%20While%20Chain-of-Thought%20%28CoT%29%20reasoning%20significantly%20enhances%20the%20performance%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20its%20autoregressive%20nature%20incurs%20prohibitive%20latency%20constraints.%20Current%20efforts%20to%20mitigate%20this%20via%20token%20compression%20often%20fail%20by%20blindly%20applying%20text-centric%20metrics%20to%20multimodal%20contexts.%20We%20identify%20a%20critical%20failure%20mode%20termed%20Visual%20Amnesia%2C%20where%20linguistically%20redundant%20tokens%20are%20erroneously%20pruned%2C%20leading%20to%20hallucinations.%20To%20address%20this%2C%20we%20introduce%20V-Skip%20that%20reformulates%20token%20pruning%20as%20a%20Visual-Anchored%20Information%20Bottleneck%20%28VA-IB%29%20optimization%20problem.%20V-Skip%20employs%20a%20dual-path%20gating%20mechanism%20that%20weighs%20token%20importance%20through%20both%20linguistic%20surprisal%20and%20cross-modal%20attention%20flow%2C%20effectively%20rescuing%20visually%20salient%20anchors.%20Extensive%20experiments%20on%20Qwen2-VL%20and%20Llama-3.2%20families%20demonstrate%20that%20V-Skip%20achieves%20a%20%242.9%5Ctimes%24%20speedup%20with%20negligible%20accuracy%20loss.%20Specifically%2C%20it%20preserves%20fine-grained%20visual%20details%2C%20outperforming%20other%20baselines%20over%2030%5C%25%20on%20the%20DocVQA.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChain-of-Thought%2520Compression%2520Should%2520Not%2520Be%2520Blind%253A%2520V-Skip%2520for%2520Efficient%2520Multimodal%2520Reasoning%2520via%2520Dual-Path%2520Anchoring%26entry.906535625%3DDongxu%2520Zhang%2520and%2520Yiding%2520Sun%2520and%2520Cheng%2520Tan%2520and%2520Wenbiao%2520Yan%2520and%2520Ning%2520Yang%2520and%2520Jihua%2520Zhu%2520and%2520Hiajun%2520Zhang%26entry.1292438233%3DWhile%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520significantly%2520enhances%2520the%2520performance%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520its%2520autoregressive%2520nature%2520incurs%2520prohibitive%2520latency%2520constraints.%2520Current%2520efforts%2520to%2520mitigate%2520this%2520via%2520token%2520compression%2520often%2520fail%2520by%2520blindly%2520applying%2520text-centric%2520metrics%2520to%2520multimodal%2520contexts.%2520We%2520identify%2520a%2520critical%2520failure%2520mode%2520termed%2520Visual%2520Amnesia%252C%2520where%2520linguistically%2520redundant%2520tokens%2520are%2520erroneously%2520pruned%252C%2520leading%2520to%2520hallucinations.%2520To%2520address%2520this%252C%2520we%2520introduce%2520V-Skip%2520that%2520reformulates%2520token%2520pruning%2520as%2520a%2520Visual-Anchored%2520Information%2520Bottleneck%2520%2528VA-IB%2529%2520optimization%2520problem.%2520V-Skip%2520employs%2520a%2520dual-path%2520gating%2520mechanism%2520that%2520weighs%2520token%2520importance%2520through%2520both%2520linguistic%2520surprisal%2520and%2520cross-modal%2520attention%2520flow%252C%2520effectively%2520rescuing%2520visually%2520salient%2520anchors.%2520Extensive%2520experiments%2520on%2520Qwen2-VL%2520and%2520Llama-3.2%2520families%2520demonstrate%2520that%2520V-Skip%2520achieves%2520a%2520%25242.9%255Ctimes%2524%2520speedup%2520with%2520negligible%2520accuracy%2520loss.%2520Specifically%252C%2520it%2520preserves%2520fine-grained%2520visual%2520details%252C%2520outperforming%2520other%2520baselines%2520over%252030%255C%2525%2520on%2520the%2520DocVQA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chain-of-Thought%20Compression%20Should%20Not%20Be%20Blind%3A%20V-Skip%20for%20Efficient%20Multimodal%20Reasoning%20via%20Dual-Path%20Anchoring&entry.906535625=Dongxu%20Zhang%20and%20Yiding%20Sun%20and%20Cheng%20Tan%20and%20Wenbiao%20Yan%20and%20Ning%20Yang%20and%20Jihua%20Zhu%20and%20Hiajun%20Zhang&entry.1292438233=While%20Chain-of-Thought%20%28CoT%29%20reasoning%20significantly%20enhances%20the%20performance%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20its%20autoregressive%20nature%20incurs%20prohibitive%20latency%20constraints.%20Current%20efforts%20to%20mitigate%20this%20via%20token%20compression%20often%20fail%20by%20blindly%20applying%20text-centric%20metrics%20to%20multimodal%20contexts.%20We%20identify%20a%20critical%20failure%20mode%20termed%20Visual%20Amnesia%2C%20where%20linguistically%20redundant%20tokens%20are%20erroneously%20pruned%2C%20leading%20to%20hallucinations.%20To%20address%20this%2C%20we%20introduce%20V-Skip%20that%20reformulates%20token%20pruning%20as%20a%20Visual-Anchored%20Information%20Bottleneck%20%28VA-IB%29%20optimization%20problem.%20V-Skip%20employs%20a%20dual-path%20gating%20mechanism%20that%20weighs%20token%20importance%20through%20both%20linguistic%20surprisal%20and%20cross-modal%20attention%20flow%2C%20effectively%20rescuing%20visually%20salient%20anchors.%20Extensive%20experiments%20on%20Qwen2-VL%20and%20Llama-3.2%20families%20demonstrate%20that%20V-Skip%20achieves%20a%20%242.9%5Ctimes%24%20speedup%20with%20negligible%20accuracy%20loss.%20Specifically%2C%20it%20preserves%20fine-grained%20visual%20details%2C%20outperforming%20other%20baselines%20over%2030%5C%25%20on%20the%20DocVQA.&entry.1838667208=http%3A//arxiv.org/abs/2601.13879v1&entry.124074799=Read"},
{"title": "Active Cross-Modal Visuo-Tactile Perception of Deformable Linear Objects", "author": "Raffaele Mazza and Ciro Natale and Pietro Falco", "abstract": "This paper presents a novel cross-modal visuo-tactile perception framework for the 3D shape reconstruction of deformable linear objects (DLOs), with a specific focus on cables subject to severe visual occlusions. Unlike existing methods relying predominantly on vision, whose performance degrades under varying illumination, background clutter, or partial visibility, the proposed approach integrates foundation-model-based visual perception with adaptive tactile exploration. The visual pipeline exploits SAM for instance segmentation and Florence for semantic refinement, followed by skeletonization, endpoint detection, and point-cloud extraction. Occluded cable segments are autonomously identified and explored with a tactile sensor, which provides local point clouds that are merged with the visual data through Euclidean clustering and topology-preserving fusion. A B-spline interpolation driven by endpoint-guided point sorting yields a smooth and complete reconstruction of the cable shape. Experimental validation using a robotic manipulator equipped with an RGB-D camera and a tactile pad demonstrates that the proposed framework accurately reconstructs both simple and highly curved single or multiple cable configurations, even when large portions are occluded. These results highlight the potential of foundation-model-enhanced cross-modal perception for advancing robotic manipulation of deformable objects.", "link": "http://arxiv.org/abs/2601.13979v1", "date": "2026-01-20", "relevancy": 2.7193, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5481}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5417}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5417}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Active%20Cross-Modal%20Visuo-Tactile%20Perception%20of%20Deformable%20Linear%20Objects&body=Title%3A%20Active%20Cross-Modal%20Visuo-Tactile%20Perception%20of%20Deformable%20Linear%20Objects%0AAuthor%3A%20Raffaele%20Mazza%20and%20Ciro%20Natale%20and%20Pietro%20Falco%0AAbstract%3A%20This%20paper%20presents%20a%20novel%20cross-modal%20visuo-tactile%20perception%20framework%20for%20the%203D%20shape%20reconstruction%20of%20deformable%20linear%20objects%20%28DLOs%29%2C%20with%20a%20specific%20focus%20on%20cables%20subject%20to%20severe%20visual%20occlusions.%20Unlike%20existing%20methods%20relying%20predominantly%20on%20vision%2C%20whose%20performance%20degrades%20under%20varying%20illumination%2C%20background%20clutter%2C%20or%20partial%20visibility%2C%20the%20proposed%20approach%20integrates%20foundation-model-based%20visual%20perception%20with%20adaptive%20tactile%20exploration.%20The%20visual%20pipeline%20exploits%20SAM%20for%20instance%20segmentation%20and%20Florence%20for%20semantic%20refinement%2C%20followed%20by%20skeletonization%2C%20endpoint%20detection%2C%20and%20point-cloud%20extraction.%20Occluded%20cable%20segments%20are%20autonomously%20identified%20and%20explored%20with%20a%20tactile%20sensor%2C%20which%20provides%20local%20point%20clouds%20that%20are%20merged%20with%20the%20visual%20data%20through%20Euclidean%20clustering%20and%20topology-preserving%20fusion.%20A%20B-spline%20interpolation%20driven%20by%20endpoint-guided%20point%20sorting%20yields%20a%20smooth%20and%20complete%20reconstruction%20of%20the%20cable%20shape.%20Experimental%20validation%20using%20a%20robotic%20manipulator%20equipped%20with%20an%20RGB-D%20camera%20and%20a%20tactile%20pad%20demonstrates%20that%20the%20proposed%20framework%20accurately%20reconstructs%20both%20simple%20and%20highly%20curved%20single%20or%20multiple%20cable%20configurations%2C%20even%20when%20large%20portions%20are%20occluded.%20These%20results%20highlight%20the%20potential%20of%20foundation-model-enhanced%20cross-modal%20perception%20for%20advancing%20robotic%20manipulation%20of%20deformable%20objects.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13979v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DActive%2520Cross-Modal%2520Visuo-Tactile%2520Perception%2520of%2520Deformable%2520Linear%2520Objects%26entry.906535625%3DRaffaele%2520Mazza%2520and%2520Ciro%2520Natale%2520and%2520Pietro%2520Falco%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520novel%2520cross-modal%2520visuo-tactile%2520perception%2520framework%2520for%2520the%25203D%2520shape%2520reconstruction%2520of%2520deformable%2520linear%2520objects%2520%2528DLOs%2529%252C%2520with%2520a%2520specific%2520focus%2520on%2520cables%2520subject%2520to%2520severe%2520visual%2520occlusions.%2520Unlike%2520existing%2520methods%2520relying%2520predominantly%2520on%2520vision%252C%2520whose%2520performance%2520degrades%2520under%2520varying%2520illumination%252C%2520background%2520clutter%252C%2520or%2520partial%2520visibility%252C%2520the%2520proposed%2520approach%2520integrates%2520foundation-model-based%2520visual%2520perception%2520with%2520adaptive%2520tactile%2520exploration.%2520The%2520visual%2520pipeline%2520exploits%2520SAM%2520for%2520instance%2520segmentation%2520and%2520Florence%2520for%2520semantic%2520refinement%252C%2520followed%2520by%2520skeletonization%252C%2520endpoint%2520detection%252C%2520and%2520point-cloud%2520extraction.%2520Occluded%2520cable%2520segments%2520are%2520autonomously%2520identified%2520and%2520explored%2520with%2520a%2520tactile%2520sensor%252C%2520which%2520provides%2520local%2520point%2520clouds%2520that%2520are%2520merged%2520with%2520the%2520visual%2520data%2520through%2520Euclidean%2520clustering%2520and%2520topology-preserving%2520fusion.%2520A%2520B-spline%2520interpolation%2520driven%2520by%2520endpoint-guided%2520point%2520sorting%2520yields%2520a%2520smooth%2520and%2520complete%2520reconstruction%2520of%2520the%2520cable%2520shape.%2520Experimental%2520validation%2520using%2520a%2520robotic%2520manipulator%2520equipped%2520with%2520an%2520RGB-D%2520camera%2520and%2520a%2520tactile%2520pad%2520demonstrates%2520that%2520the%2520proposed%2520framework%2520accurately%2520reconstructs%2520both%2520simple%2520and%2520highly%2520curved%2520single%2520or%2520multiple%2520cable%2520configurations%252C%2520even%2520when%2520large%2520portions%2520are%2520occluded.%2520These%2520results%2520highlight%2520the%2520potential%2520of%2520foundation-model-enhanced%2520cross-modal%2520perception%2520for%2520advancing%2520robotic%2520manipulation%2520of%2520deformable%2520objects.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13979v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Active%20Cross-Modal%20Visuo-Tactile%20Perception%20of%20Deformable%20Linear%20Objects&entry.906535625=Raffaele%20Mazza%20and%20Ciro%20Natale%20and%20Pietro%20Falco&entry.1292438233=This%20paper%20presents%20a%20novel%20cross-modal%20visuo-tactile%20perception%20framework%20for%20the%203D%20shape%20reconstruction%20of%20deformable%20linear%20objects%20%28DLOs%29%2C%20with%20a%20specific%20focus%20on%20cables%20subject%20to%20severe%20visual%20occlusions.%20Unlike%20existing%20methods%20relying%20predominantly%20on%20vision%2C%20whose%20performance%20degrades%20under%20varying%20illumination%2C%20background%20clutter%2C%20or%20partial%20visibility%2C%20the%20proposed%20approach%20integrates%20foundation-model-based%20visual%20perception%20with%20adaptive%20tactile%20exploration.%20The%20visual%20pipeline%20exploits%20SAM%20for%20instance%20segmentation%20and%20Florence%20for%20semantic%20refinement%2C%20followed%20by%20skeletonization%2C%20endpoint%20detection%2C%20and%20point-cloud%20extraction.%20Occluded%20cable%20segments%20are%20autonomously%20identified%20and%20explored%20with%20a%20tactile%20sensor%2C%20which%20provides%20local%20point%20clouds%20that%20are%20merged%20with%20the%20visual%20data%20through%20Euclidean%20clustering%20and%20topology-preserving%20fusion.%20A%20B-spline%20interpolation%20driven%20by%20endpoint-guided%20point%20sorting%20yields%20a%20smooth%20and%20complete%20reconstruction%20of%20the%20cable%20shape.%20Experimental%20validation%20using%20a%20robotic%20manipulator%20equipped%20with%20an%20RGB-D%20camera%20and%20a%20tactile%20pad%20demonstrates%20that%20the%20proposed%20framework%20accurately%20reconstructs%20both%20simple%20and%20highly%20curved%20single%20or%20multiple%20cable%20configurations%2C%20even%20when%20large%20portions%20are%20occluded.%20These%20results%20highlight%20the%20potential%20of%20foundation-model-enhanced%20cross-modal%20perception%20for%20advancing%20robotic%20manipulation%20of%20deformable%20objects.&entry.1838667208=http%3A//arxiv.org/abs/2601.13979v1&entry.124074799=Read"},
{"title": "Generative Language Models on Nucleotide Sequences of Human Genes", "author": "Musa Nuri Ihtiyar and Arzucan Ozgur", "abstract": "Language models, especially transformer-based ones, have achieved colossal success in NLP. To be precise, studies like BERT for NLU and works like GPT-3 for NLG are very important. If we consider DNA sequences as a text written with an alphabet of four letters representing the nucleotides, they are similar in structure to natural languages. This similarity has led to the development of discriminative language models such as DNABert in the field of DNA-related bioinformatics. To our knowledge, however, the generative side of the coin is still largely unexplored. Therefore, we have focused on the development of an autoregressive generative language model such as GPT-3 for DNA sequences. Since working with whole DNA sequences is challenging without extensive computational resources, we decided to conduct our study on a smaller scale and focus on nucleotide sequences of human genes rather than the whole DNA. This decision has not changed the structure of the problem, as both DNA and genes can be considered as 1D sequences consisting of four different nucleotides without losing much information and without oversimplification. Firstly, we systematically studied an almost entirely unexplored problem and observed that RNNs perform best, while simple techniques such as N-grams are also promising. Another beneficial point was learning how to work with generative models on languages we do not understand, unlike natural languages. The importance of using real-world tasks beyond classical metrics such as perplexity was noted. In addition, we examined whether the data-hungry nature of these models can be altered by selecting a language with minimal vocabulary size, four due to four different types of nucleotides. The reason for reviewing this was that choosing such a language might make the problem easier. However, in this study, we found that this did not change the amount of data required very much.", "link": "http://arxiv.org/abs/2307.10634v3", "date": "2026-01-20", "relevancy": 2.6775, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5475}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5401}, {"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.5189}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generative%20Language%20Models%20on%20Nucleotide%20Sequences%20of%20Human%20Genes&body=Title%3A%20Generative%20Language%20Models%20on%20Nucleotide%20Sequences%20of%20Human%20Genes%0AAuthor%3A%20Musa%20Nuri%20Ihtiyar%20and%20Arzucan%20Ozgur%0AAbstract%3A%20Language%20models%2C%20especially%20transformer-based%20ones%2C%20have%20achieved%20colossal%20success%20in%20NLP.%20To%20be%20precise%2C%20studies%20like%20BERT%20for%20NLU%20and%20works%20like%20GPT-3%20for%20NLG%20are%20very%20important.%20If%20we%20consider%20DNA%20sequences%20as%20a%20text%20written%20with%20an%20alphabet%20of%20four%20letters%20representing%20the%20nucleotides%2C%20they%20are%20similar%20in%20structure%20to%20natural%20languages.%20This%20similarity%20has%20led%20to%20the%20development%20of%20discriminative%20language%20models%20such%20as%20DNABert%20in%20the%20field%20of%20DNA-related%20bioinformatics.%20To%20our%20knowledge%2C%20however%2C%20the%20generative%20side%20of%20the%20coin%20is%20still%20largely%20unexplored.%20Therefore%2C%20we%20have%20focused%20on%20the%20development%20of%20an%20autoregressive%20generative%20language%20model%20such%20as%20GPT-3%20for%20DNA%20sequences.%20Since%20working%20with%20whole%20DNA%20sequences%20is%20challenging%20without%20extensive%20computational%20resources%2C%20we%20decided%20to%20conduct%20our%20study%20on%20a%20smaller%20scale%20and%20focus%20on%20nucleotide%20sequences%20of%20human%20genes%20rather%20than%20the%20whole%20DNA.%20This%20decision%20has%20not%20changed%20the%20structure%20of%20the%20problem%2C%20as%20both%20DNA%20and%20genes%20can%20be%20considered%20as%201D%20sequences%20consisting%20of%20four%20different%20nucleotides%20without%20losing%20much%20information%20and%20without%20oversimplification.%20Firstly%2C%20we%20systematically%20studied%20an%20almost%20entirely%20unexplored%20problem%20and%20observed%20that%20RNNs%20perform%20best%2C%20while%20simple%20techniques%20such%20as%20N-grams%20are%20also%20promising.%20Another%20beneficial%20point%20was%20learning%20how%20to%20work%20with%20generative%20models%20on%20languages%20we%20do%20not%20understand%2C%20unlike%20natural%20languages.%20The%20importance%20of%20using%20real-world%20tasks%20beyond%20classical%20metrics%20such%20as%20perplexity%20was%20noted.%20In%20addition%2C%20we%20examined%20whether%20the%20data-hungry%20nature%20of%20these%20models%20can%20be%20altered%20by%20selecting%20a%20language%20with%20minimal%20vocabulary%20size%2C%20four%20due%20to%20four%20different%20types%20of%20nucleotides.%20The%20reason%20for%20reviewing%20this%20was%20that%20choosing%20such%20a%20language%20might%20make%20the%20problem%20easier.%20However%2C%20in%20this%20study%2C%20we%20found%20that%20this%20did%20not%20change%20the%20amount%20of%20data%20required%20very%20much.%0ALink%3A%20http%3A//arxiv.org/abs/2307.10634v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenerative%2520Language%2520Models%2520on%2520Nucleotide%2520Sequences%2520of%2520Human%2520Genes%26entry.906535625%3DMusa%2520Nuri%2520Ihtiyar%2520and%2520Arzucan%2520Ozgur%26entry.1292438233%3DLanguage%2520models%252C%2520especially%2520transformer-based%2520ones%252C%2520have%2520achieved%2520colossal%2520success%2520in%2520NLP.%2520To%2520be%2520precise%252C%2520studies%2520like%2520BERT%2520for%2520NLU%2520and%2520works%2520like%2520GPT-3%2520for%2520NLG%2520are%2520very%2520important.%2520If%2520we%2520consider%2520DNA%2520sequences%2520as%2520a%2520text%2520written%2520with%2520an%2520alphabet%2520of%2520four%2520letters%2520representing%2520the%2520nucleotides%252C%2520they%2520are%2520similar%2520in%2520structure%2520to%2520natural%2520languages.%2520This%2520similarity%2520has%2520led%2520to%2520the%2520development%2520of%2520discriminative%2520language%2520models%2520such%2520as%2520DNABert%2520in%2520the%2520field%2520of%2520DNA-related%2520bioinformatics.%2520To%2520our%2520knowledge%252C%2520however%252C%2520the%2520generative%2520side%2520of%2520the%2520coin%2520is%2520still%2520largely%2520unexplored.%2520Therefore%252C%2520we%2520have%2520focused%2520on%2520the%2520development%2520of%2520an%2520autoregressive%2520generative%2520language%2520model%2520such%2520as%2520GPT-3%2520for%2520DNA%2520sequences.%2520Since%2520working%2520with%2520whole%2520DNA%2520sequences%2520is%2520challenging%2520without%2520extensive%2520computational%2520resources%252C%2520we%2520decided%2520to%2520conduct%2520our%2520study%2520on%2520a%2520smaller%2520scale%2520and%2520focus%2520on%2520nucleotide%2520sequences%2520of%2520human%2520genes%2520rather%2520than%2520the%2520whole%2520DNA.%2520This%2520decision%2520has%2520not%2520changed%2520the%2520structure%2520of%2520the%2520problem%252C%2520as%2520both%2520DNA%2520and%2520genes%2520can%2520be%2520considered%2520as%25201D%2520sequences%2520consisting%2520of%2520four%2520different%2520nucleotides%2520without%2520losing%2520much%2520information%2520and%2520without%2520oversimplification.%2520Firstly%252C%2520we%2520systematically%2520studied%2520an%2520almost%2520entirely%2520unexplored%2520problem%2520and%2520observed%2520that%2520RNNs%2520perform%2520best%252C%2520while%2520simple%2520techniques%2520such%2520as%2520N-grams%2520are%2520also%2520promising.%2520Another%2520beneficial%2520point%2520was%2520learning%2520how%2520to%2520work%2520with%2520generative%2520models%2520on%2520languages%2520we%2520do%2520not%2520understand%252C%2520unlike%2520natural%2520languages.%2520The%2520importance%2520of%2520using%2520real-world%2520tasks%2520beyond%2520classical%2520metrics%2520such%2520as%2520perplexity%2520was%2520noted.%2520In%2520addition%252C%2520we%2520examined%2520whether%2520the%2520data-hungry%2520nature%2520of%2520these%2520models%2520can%2520be%2520altered%2520by%2520selecting%2520a%2520language%2520with%2520minimal%2520vocabulary%2520size%252C%2520four%2520due%2520to%2520four%2520different%2520types%2520of%2520nucleotides.%2520The%2520reason%2520for%2520reviewing%2520this%2520was%2520that%2520choosing%2520such%2520a%2520language%2520might%2520make%2520the%2520problem%2520easier.%2520However%252C%2520in%2520this%2520study%252C%2520we%2520found%2520that%2520this%2520did%2520not%2520change%2520the%2520amount%2520of%2520data%2520required%2520very%2520much.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2307.10634v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generative%20Language%20Models%20on%20Nucleotide%20Sequences%20of%20Human%20Genes&entry.906535625=Musa%20Nuri%20Ihtiyar%20and%20Arzucan%20Ozgur&entry.1292438233=Language%20models%2C%20especially%20transformer-based%20ones%2C%20have%20achieved%20colossal%20success%20in%20NLP.%20To%20be%20precise%2C%20studies%20like%20BERT%20for%20NLU%20and%20works%20like%20GPT-3%20for%20NLG%20are%20very%20important.%20If%20we%20consider%20DNA%20sequences%20as%20a%20text%20written%20with%20an%20alphabet%20of%20four%20letters%20representing%20the%20nucleotides%2C%20they%20are%20similar%20in%20structure%20to%20natural%20languages.%20This%20similarity%20has%20led%20to%20the%20development%20of%20discriminative%20language%20models%20such%20as%20DNABert%20in%20the%20field%20of%20DNA-related%20bioinformatics.%20To%20our%20knowledge%2C%20however%2C%20the%20generative%20side%20of%20the%20coin%20is%20still%20largely%20unexplored.%20Therefore%2C%20we%20have%20focused%20on%20the%20development%20of%20an%20autoregressive%20generative%20language%20model%20such%20as%20GPT-3%20for%20DNA%20sequences.%20Since%20working%20with%20whole%20DNA%20sequences%20is%20challenging%20without%20extensive%20computational%20resources%2C%20we%20decided%20to%20conduct%20our%20study%20on%20a%20smaller%20scale%20and%20focus%20on%20nucleotide%20sequences%20of%20human%20genes%20rather%20than%20the%20whole%20DNA.%20This%20decision%20has%20not%20changed%20the%20structure%20of%20the%20problem%2C%20as%20both%20DNA%20and%20genes%20can%20be%20considered%20as%201D%20sequences%20consisting%20of%20four%20different%20nucleotides%20without%20losing%20much%20information%20and%20without%20oversimplification.%20Firstly%2C%20we%20systematically%20studied%20an%20almost%20entirely%20unexplored%20problem%20and%20observed%20that%20RNNs%20perform%20best%2C%20while%20simple%20techniques%20such%20as%20N-grams%20are%20also%20promising.%20Another%20beneficial%20point%20was%20learning%20how%20to%20work%20with%20generative%20models%20on%20languages%20we%20do%20not%20understand%2C%20unlike%20natural%20languages.%20The%20importance%20of%20using%20real-world%20tasks%20beyond%20classical%20metrics%20such%20as%20perplexity%20was%20noted.%20In%20addition%2C%20we%20examined%20whether%20the%20data-hungry%20nature%20of%20these%20models%20can%20be%20altered%20by%20selecting%20a%20language%20with%20minimal%20vocabulary%20size%2C%20four%20due%20to%20four%20different%20types%20of%20nucleotides.%20The%20reason%20for%20reviewing%20this%20was%20that%20choosing%20such%20a%20language%20might%20make%20the%20problem%20easier.%20However%2C%20in%20this%20study%2C%20we%20found%20that%20this%20did%20not%20change%20the%20amount%20of%20data%20required%20very%20much.&entry.1838667208=http%3A//arxiv.org/abs/2307.10634v3&entry.124074799=Read"},
{"title": "Virtual Urbanism: An AI-Driven Framework for Quantifying Urban Identity. A Tokyo-Based Pilot Study Using Diffusion-Generated Synthetic Environments", "author": "Glinskaya Maria", "abstract": "This paper introduces Virtual Urbanism (VU), a multimodal AI-driven analytical framework for quantifying urban identity through the medium of synthetic urban replicas. The framework aims to advance computationally tractable urban identity metrics. To demonstrate feasibility, the pilot study Virtual Urbanism and Tokyo Microcosms is presented. A pipeline integrating Stable Diffusion and LoRA models was used to produce synthetic replicas of nine Tokyo areas rendered as dynamic synthetic urban sequences, excluding existing orientation markers to elicit core identity-forming elements. Human-evaluation experiments (I) assessed perceptual legitimacy of replicas; (II) quantified area-level identity; (III) derived core identity-forming elements. Results showed a mean identification accuracy of ~81%, confirming the validity of the replicas. Urban Identity Level (UIL) metric enabled assessment of identity levels across areas, while semantic analysis revealed culturally embedded typologies as core identity-forming elements, positioning VU as a viable framework for AI-augmented urban analysis, outlining a path toward automated, multi-parameter identity metrics.", "link": "http://arxiv.org/abs/2601.13846v1", "date": "2026-01-20", "relevancy": 2.6769, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5572}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5497}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4992}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Virtual%20Urbanism%3A%20An%20AI-Driven%20Framework%20for%20Quantifying%20Urban%20Identity.%20A%20Tokyo-Based%20Pilot%20Study%20Using%20Diffusion-Generated%20Synthetic%20Environments&body=Title%3A%20Virtual%20Urbanism%3A%20An%20AI-Driven%20Framework%20for%20Quantifying%20Urban%20Identity.%20A%20Tokyo-Based%20Pilot%20Study%20Using%20Diffusion-Generated%20Synthetic%20Environments%0AAuthor%3A%20Glinskaya%20Maria%0AAbstract%3A%20This%20paper%20introduces%20Virtual%20Urbanism%20%28VU%29%2C%20a%20multimodal%20AI-driven%20analytical%20framework%20for%20quantifying%20urban%20identity%20through%20the%20medium%20of%20synthetic%20urban%20replicas.%20The%20framework%20aims%20to%20advance%20computationally%20tractable%20urban%20identity%20metrics.%20To%20demonstrate%20feasibility%2C%20the%20pilot%20study%20Virtual%20Urbanism%20and%20Tokyo%20Microcosms%20is%20presented.%20A%20pipeline%20integrating%20Stable%20Diffusion%20and%20LoRA%20models%20was%20used%20to%20produce%20synthetic%20replicas%20of%20nine%20Tokyo%20areas%20rendered%20as%20dynamic%20synthetic%20urban%20sequences%2C%20excluding%20existing%20orientation%20markers%20to%20elicit%20core%20identity-forming%20elements.%20Human-evaluation%20experiments%20%28I%29%20assessed%20perceptual%20legitimacy%20of%20replicas%3B%20%28II%29%20quantified%20area-level%20identity%3B%20%28III%29%20derived%20core%20identity-forming%20elements.%20Results%20showed%20a%20mean%20identification%20accuracy%20of%20~81%25%2C%20confirming%20the%20validity%20of%20the%20replicas.%20Urban%20Identity%20Level%20%28UIL%29%20metric%20enabled%20assessment%20of%20identity%20levels%20across%20areas%2C%20while%20semantic%20analysis%20revealed%20culturally%20embedded%20typologies%20as%20core%20identity-forming%20elements%2C%20positioning%20VU%20as%20a%20viable%20framework%20for%20AI-augmented%20urban%20analysis%2C%20outlining%20a%20path%20toward%20automated%2C%20multi-parameter%20identity%20metrics.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13846v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVirtual%2520Urbanism%253A%2520An%2520AI-Driven%2520Framework%2520for%2520Quantifying%2520Urban%2520Identity.%2520A%2520Tokyo-Based%2520Pilot%2520Study%2520Using%2520Diffusion-Generated%2520Synthetic%2520Environments%26entry.906535625%3DGlinskaya%2520Maria%26entry.1292438233%3DThis%2520paper%2520introduces%2520Virtual%2520Urbanism%2520%2528VU%2529%252C%2520a%2520multimodal%2520AI-driven%2520analytical%2520framework%2520for%2520quantifying%2520urban%2520identity%2520through%2520the%2520medium%2520of%2520synthetic%2520urban%2520replicas.%2520The%2520framework%2520aims%2520to%2520advance%2520computationally%2520tractable%2520urban%2520identity%2520metrics.%2520To%2520demonstrate%2520feasibility%252C%2520the%2520pilot%2520study%2520Virtual%2520Urbanism%2520and%2520Tokyo%2520Microcosms%2520is%2520presented.%2520A%2520pipeline%2520integrating%2520Stable%2520Diffusion%2520and%2520LoRA%2520models%2520was%2520used%2520to%2520produce%2520synthetic%2520replicas%2520of%2520nine%2520Tokyo%2520areas%2520rendered%2520as%2520dynamic%2520synthetic%2520urban%2520sequences%252C%2520excluding%2520existing%2520orientation%2520markers%2520to%2520elicit%2520core%2520identity-forming%2520elements.%2520Human-evaluation%2520experiments%2520%2528I%2529%2520assessed%2520perceptual%2520legitimacy%2520of%2520replicas%253B%2520%2528II%2529%2520quantified%2520area-level%2520identity%253B%2520%2528III%2529%2520derived%2520core%2520identity-forming%2520elements.%2520Results%2520showed%2520a%2520mean%2520identification%2520accuracy%2520of%2520~81%2525%252C%2520confirming%2520the%2520validity%2520of%2520the%2520replicas.%2520Urban%2520Identity%2520Level%2520%2528UIL%2529%2520metric%2520enabled%2520assessment%2520of%2520identity%2520levels%2520across%2520areas%252C%2520while%2520semantic%2520analysis%2520revealed%2520culturally%2520embedded%2520typologies%2520as%2520core%2520identity-forming%2520elements%252C%2520positioning%2520VU%2520as%2520a%2520viable%2520framework%2520for%2520AI-augmented%2520urban%2520analysis%252C%2520outlining%2520a%2520path%2520toward%2520automated%252C%2520multi-parameter%2520identity%2520metrics.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13846v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Virtual%20Urbanism%3A%20An%20AI-Driven%20Framework%20for%20Quantifying%20Urban%20Identity.%20A%20Tokyo-Based%20Pilot%20Study%20Using%20Diffusion-Generated%20Synthetic%20Environments&entry.906535625=Glinskaya%20Maria&entry.1292438233=This%20paper%20introduces%20Virtual%20Urbanism%20%28VU%29%2C%20a%20multimodal%20AI-driven%20analytical%20framework%20for%20quantifying%20urban%20identity%20through%20the%20medium%20of%20synthetic%20urban%20replicas.%20The%20framework%20aims%20to%20advance%20computationally%20tractable%20urban%20identity%20metrics.%20To%20demonstrate%20feasibility%2C%20the%20pilot%20study%20Virtual%20Urbanism%20and%20Tokyo%20Microcosms%20is%20presented.%20A%20pipeline%20integrating%20Stable%20Diffusion%20and%20LoRA%20models%20was%20used%20to%20produce%20synthetic%20replicas%20of%20nine%20Tokyo%20areas%20rendered%20as%20dynamic%20synthetic%20urban%20sequences%2C%20excluding%20existing%20orientation%20markers%20to%20elicit%20core%20identity-forming%20elements.%20Human-evaluation%20experiments%20%28I%29%20assessed%20perceptual%20legitimacy%20of%20replicas%3B%20%28II%29%20quantified%20area-level%20identity%3B%20%28III%29%20derived%20core%20identity-forming%20elements.%20Results%20showed%20a%20mean%20identification%20accuracy%20of%20~81%25%2C%20confirming%20the%20validity%20of%20the%20replicas.%20Urban%20Identity%20Level%20%28UIL%29%20metric%20enabled%20assessment%20of%20identity%20levels%20across%20areas%2C%20while%20semantic%20analysis%20revealed%20culturally%20embedded%20typologies%20as%20core%20identity-forming%20elements%2C%20positioning%20VU%20as%20a%20viable%20framework%20for%20AI-augmented%20urban%20analysis%2C%20outlining%20a%20path%20toward%20automated%2C%20multi-parameter%20identity%20metrics.&entry.1838667208=http%3A//arxiv.org/abs/2601.13846v1&entry.124074799=Read"},
{"title": "OmniOVCD: Streamlining Open-Vocabulary Change Detection with SAM 3", "author": "Xu Zhang and Danyang Li and Yingjie Xia and Xiaohang Dong and Hualong Yu and Jianye Wang and Qicheng Li", "abstract": "Change Detection (CD) is a fundamental task in remote sensing. It monitors the evolution of land cover over time. Based on this, Open-Vocabulary Change Detection (OVCD) introduces a new requirement. It aims to reduce the reliance on predefined categories. Existing training-free OVCD methods mostly use CLIP to identify categories. These methods also need extra models like DINO to extract features. However, combining different models often causes problems in matching features and makes the system unstable. Recently, the Segment Anything Model 3 (SAM 3) is introduced. It integrates segmentation and identification capabilities within one promptable model, which offers new possibilities for the OVCD task. In this paper, we propose OmniOVCD, a standalone framework designed for OVCD. By leveraging the decoupled output heads of SAM 3, we propose a Synergistic Fusion to Instance Decoupling (SFID) strategy. SFID first fuses the semantic, instance, and presence outputs of SAM 3 to construct land-cover masks, and then decomposes them into individual instance masks for change comparison. This design preserves high accuracy in category recognition and maintains instance-level consistency across images. As a result, the model can generate accurate change masks. Experiments on four public benchmarks (LEVIR-CD, WHU-CD, S2Looking, and SECOND) demonstrate SOTA performance, achieving IoU scores of 67.2, 66.5, 24.5, and 27.1 (class-average), respectively, surpassing all previous methods.", "link": "http://arxiv.org/abs/2601.13895v1", "date": "2026-01-20", "relevancy": 2.6735, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5479}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5479}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5084}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OmniOVCD%3A%20Streamlining%20Open-Vocabulary%20Change%20Detection%20with%20SAM%203&body=Title%3A%20OmniOVCD%3A%20Streamlining%20Open-Vocabulary%20Change%20Detection%20with%20SAM%203%0AAuthor%3A%20Xu%20Zhang%20and%20Danyang%20Li%20and%20Yingjie%20Xia%20and%20Xiaohang%20Dong%20and%20Hualong%20Yu%20and%20Jianye%20Wang%20and%20Qicheng%20Li%0AAbstract%3A%20Change%20Detection%20%28CD%29%20is%20a%20fundamental%20task%20in%20remote%20sensing.%20It%20monitors%20the%20evolution%20of%20land%20cover%20over%20time.%20Based%20on%20this%2C%20Open-Vocabulary%20Change%20Detection%20%28OVCD%29%20introduces%20a%20new%20requirement.%20It%20aims%20to%20reduce%20the%20reliance%20on%20predefined%20categories.%20Existing%20training-free%20OVCD%20methods%20mostly%20use%20CLIP%20to%20identify%20categories.%20These%20methods%20also%20need%20extra%20models%20like%20DINO%20to%20extract%20features.%20However%2C%20combining%20different%20models%20often%20causes%20problems%20in%20matching%20features%20and%20makes%20the%20system%20unstable.%20Recently%2C%20the%20Segment%20Anything%20Model%203%20%28SAM%203%29%20is%20introduced.%20It%20integrates%20segmentation%20and%20identification%20capabilities%20within%20one%20promptable%20model%2C%20which%20offers%20new%20possibilities%20for%20the%20OVCD%20task.%20In%20this%20paper%2C%20we%20propose%20OmniOVCD%2C%20a%20standalone%20framework%20designed%20for%20OVCD.%20By%20leveraging%20the%20decoupled%20output%20heads%20of%20SAM%203%2C%20we%20propose%20a%20Synergistic%20Fusion%20to%20Instance%20Decoupling%20%28SFID%29%20strategy.%20SFID%20first%20fuses%20the%20semantic%2C%20instance%2C%20and%20presence%20outputs%20of%20SAM%203%20to%20construct%20land-cover%20masks%2C%20and%20then%20decomposes%20them%20into%20individual%20instance%20masks%20for%20change%20comparison.%20This%20design%20preserves%20high%20accuracy%20in%20category%20recognition%20and%20maintains%20instance-level%20consistency%20across%20images.%20As%20a%20result%2C%20the%20model%20can%20generate%20accurate%20change%20masks.%20Experiments%20on%20four%20public%20benchmarks%20%28LEVIR-CD%2C%20WHU-CD%2C%20S2Looking%2C%20and%20SECOND%29%20demonstrate%20SOTA%20performance%2C%20achieving%20IoU%20scores%20of%2067.2%2C%2066.5%2C%2024.5%2C%20and%2027.1%20%28class-average%29%2C%20respectively%2C%20surpassing%20all%20previous%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13895v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOmniOVCD%253A%2520Streamlining%2520Open-Vocabulary%2520Change%2520Detection%2520with%2520SAM%25203%26entry.906535625%3DXu%2520Zhang%2520and%2520Danyang%2520Li%2520and%2520Yingjie%2520Xia%2520and%2520Xiaohang%2520Dong%2520and%2520Hualong%2520Yu%2520and%2520Jianye%2520Wang%2520and%2520Qicheng%2520Li%26entry.1292438233%3DChange%2520Detection%2520%2528CD%2529%2520is%2520a%2520fundamental%2520task%2520in%2520remote%2520sensing.%2520It%2520monitors%2520the%2520evolution%2520of%2520land%2520cover%2520over%2520time.%2520Based%2520on%2520this%252C%2520Open-Vocabulary%2520Change%2520Detection%2520%2528OVCD%2529%2520introduces%2520a%2520new%2520requirement.%2520It%2520aims%2520to%2520reduce%2520the%2520reliance%2520on%2520predefined%2520categories.%2520Existing%2520training-free%2520OVCD%2520methods%2520mostly%2520use%2520CLIP%2520to%2520identify%2520categories.%2520These%2520methods%2520also%2520need%2520extra%2520models%2520like%2520DINO%2520to%2520extract%2520features.%2520However%252C%2520combining%2520different%2520models%2520often%2520causes%2520problems%2520in%2520matching%2520features%2520and%2520makes%2520the%2520system%2520unstable.%2520Recently%252C%2520the%2520Segment%2520Anything%2520Model%25203%2520%2528SAM%25203%2529%2520is%2520introduced.%2520It%2520integrates%2520segmentation%2520and%2520identification%2520capabilities%2520within%2520one%2520promptable%2520model%252C%2520which%2520offers%2520new%2520possibilities%2520for%2520the%2520OVCD%2520task.%2520In%2520this%2520paper%252C%2520we%2520propose%2520OmniOVCD%252C%2520a%2520standalone%2520framework%2520designed%2520for%2520OVCD.%2520By%2520leveraging%2520the%2520decoupled%2520output%2520heads%2520of%2520SAM%25203%252C%2520we%2520propose%2520a%2520Synergistic%2520Fusion%2520to%2520Instance%2520Decoupling%2520%2528SFID%2529%2520strategy.%2520SFID%2520first%2520fuses%2520the%2520semantic%252C%2520instance%252C%2520and%2520presence%2520outputs%2520of%2520SAM%25203%2520to%2520construct%2520land-cover%2520masks%252C%2520and%2520then%2520decomposes%2520them%2520into%2520individual%2520instance%2520masks%2520for%2520change%2520comparison.%2520This%2520design%2520preserves%2520high%2520accuracy%2520in%2520category%2520recognition%2520and%2520maintains%2520instance-level%2520consistency%2520across%2520images.%2520As%2520a%2520result%252C%2520the%2520model%2520can%2520generate%2520accurate%2520change%2520masks.%2520Experiments%2520on%2520four%2520public%2520benchmarks%2520%2528LEVIR-CD%252C%2520WHU-CD%252C%2520S2Looking%252C%2520and%2520SECOND%2529%2520demonstrate%2520SOTA%2520performance%252C%2520achieving%2520IoU%2520scores%2520of%252067.2%252C%252066.5%252C%252024.5%252C%2520and%252027.1%2520%2528class-average%2529%252C%2520respectively%252C%2520surpassing%2520all%2520previous%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13895v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OmniOVCD%3A%20Streamlining%20Open-Vocabulary%20Change%20Detection%20with%20SAM%203&entry.906535625=Xu%20Zhang%20and%20Danyang%20Li%20and%20Yingjie%20Xia%20and%20Xiaohang%20Dong%20and%20Hualong%20Yu%20and%20Jianye%20Wang%20and%20Qicheng%20Li&entry.1292438233=Change%20Detection%20%28CD%29%20is%20a%20fundamental%20task%20in%20remote%20sensing.%20It%20monitors%20the%20evolution%20of%20land%20cover%20over%20time.%20Based%20on%20this%2C%20Open-Vocabulary%20Change%20Detection%20%28OVCD%29%20introduces%20a%20new%20requirement.%20It%20aims%20to%20reduce%20the%20reliance%20on%20predefined%20categories.%20Existing%20training-free%20OVCD%20methods%20mostly%20use%20CLIP%20to%20identify%20categories.%20These%20methods%20also%20need%20extra%20models%20like%20DINO%20to%20extract%20features.%20However%2C%20combining%20different%20models%20often%20causes%20problems%20in%20matching%20features%20and%20makes%20the%20system%20unstable.%20Recently%2C%20the%20Segment%20Anything%20Model%203%20%28SAM%203%29%20is%20introduced.%20It%20integrates%20segmentation%20and%20identification%20capabilities%20within%20one%20promptable%20model%2C%20which%20offers%20new%20possibilities%20for%20the%20OVCD%20task.%20In%20this%20paper%2C%20we%20propose%20OmniOVCD%2C%20a%20standalone%20framework%20designed%20for%20OVCD.%20By%20leveraging%20the%20decoupled%20output%20heads%20of%20SAM%203%2C%20we%20propose%20a%20Synergistic%20Fusion%20to%20Instance%20Decoupling%20%28SFID%29%20strategy.%20SFID%20first%20fuses%20the%20semantic%2C%20instance%2C%20and%20presence%20outputs%20of%20SAM%203%20to%20construct%20land-cover%20masks%2C%20and%20then%20decomposes%20them%20into%20individual%20instance%20masks%20for%20change%20comparison.%20This%20design%20preserves%20high%20accuracy%20in%20category%20recognition%20and%20maintains%20instance-level%20consistency%20across%20images.%20As%20a%20result%2C%20the%20model%20can%20generate%20accurate%20change%20masks.%20Experiments%20on%20four%20public%20benchmarks%20%28LEVIR-CD%2C%20WHU-CD%2C%20S2Looking%2C%20and%20SECOND%29%20demonstrate%20SOTA%20performance%2C%20achieving%20IoU%20scores%20of%2067.2%2C%2066.5%2C%2024.5%2C%20and%2027.1%20%28class-average%29%2C%20respectively%2C%20surpassing%20all%20previous%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.13895v1&entry.124074799=Read"},
{"title": "Adaptive Riemannian Graph Neural Networks", "author": "Xudong Wang and Chris Ding and Tongxin Li and Jicong Fan", "abstract": "Graph data often exhibits complex geometric heterogeneity, where structures with varying local curvature, such as tree-like hierarchies and dense communities, coexist within a single network. Existing geometric GNNs, which embed graphs into single fixed-curvature manifolds or discrete product spaces, struggle to capture this diversity. We introduce Adaptive Riemannian Graph Neural Networks (ARGNN), a novel framework that learns a continuous and anisotropic Riemannian metric tensor field over the graph. It allows each node to determine its optimal local geometry, enabling the model to fluidly adapt to the graph's structural landscape. Our core innovation is an efficient parameterization of the node-wise metric tensor, specializing to a learnable diagonal form that captures directional geometric information while maintaining computational tractability. To ensure geometric regularity and stable training, we integrate a Ricci flow-inspired regularization that smooths the learned manifold. Theoretically, we establish the rigorous geometric evolution convergence guarantee for ARGNN and provide a continuous generalization that unifies prior fixed or mixed-curvature GNNs. Empirically, our method demonstrates superior performance on both homophilic and heterophilic benchmark datasets with the ability to capture diverse structures adaptively. Moreover, the learned geometries both offer interpretable insights into the underlying graph structure and empirically corroborate our theoretical analysis.", "link": "http://arxiv.org/abs/2508.02600v2", "date": "2026-01-20", "relevancy": 2.6397, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5554}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5211}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5073}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Riemannian%20Graph%20Neural%20Networks&body=Title%3A%20Adaptive%20Riemannian%20Graph%20Neural%20Networks%0AAuthor%3A%20Xudong%20Wang%20and%20Chris%20Ding%20and%20Tongxin%20Li%20and%20Jicong%20Fan%0AAbstract%3A%20Graph%20data%20often%20exhibits%20complex%20geometric%20heterogeneity%2C%20where%20structures%20with%20varying%20local%20curvature%2C%20such%20as%20tree-like%20hierarchies%20and%20dense%20communities%2C%20coexist%20within%20a%20single%20network.%20Existing%20geometric%20GNNs%2C%20which%20embed%20graphs%20into%20single%20fixed-curvature%20manifolds%20or%20discrete%20product%20spaces%2C%20struggle%20to%20capture%20this%20diversity.%20We%20introduce%20Adaptive%20Riemannian%20Graph%20Neural%20Networks%20%28ARGNN%29%2C%20a%20novel%20framework%20that%20learns%20a%20continuous%20and%20anisotropic%20Riemannian%20metric%20tensor%20field%20over%20the%20graph.%20It%20allows%20each%20node%20to%20determine%20its%20optimal%20local%20geometry%2C%20enabling%20the%20model%20to%20fluidly%20adapt%20to%20the%20graph%27s%20structural%20landscape.%20Our%20core%20innovation%20is%20an%20efficient%20parameterization%20of%20the%20node-wise%20metric%20tensor%2C%20specializing%20to%20a%20learnable%20diagonal%20form%20that%20captures%20directional%20geometric%20information%20while%20maintaining%20computational%20tractability.%20To%20ensure%20geometric%20regularity%20and%20stable%20training%2C%20we%20integrate%20a%20Ricci%20flow-inspired%20regularization%20that%20smooths%20the%20learned%20manifold.%20Theoretically%2C%20we%20establish%20the%20rigorous%20geometric%20evolution%20convergence%20guarantee%20for%20ARGNN%20and%20provide%20a%20continuous%20generalization%20that%20unifies%20prior%20fixed%20or%20mixed-curvature%20GNNs.%20Empirically%2C%20our%20method%20demonstrates%20superior%20performance%20on%20both%20homophilic%20and%20heterophilic%20benchmark%20datasets%20with%20the%20ability%20to%20capture%20diverse%20structures%20adaptively.%20Moreover%2C%20the%20learned%20geometries%20both%20offer%20interpretable%20insights%20into%20the%20underlying%20graph%20structure%20and%20empirically%20corroborate%20our%20theoretical%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2508.02600v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Riemannian%2520Graph%2520Neural%2520Networks%26entry.906535625%3DXudong%2520Wang%2520and%2520Chris%2520Ding%2520and%2520Tongxin%2520Li%2520and%2520Jicong%2520Fan%26entry.1292438233%3DGraph%2520data%2520often%2520exhibits%2520complex%2520geometric%2520heterogeneity%252C%2520where%2520structures%2520with%2520varying%2520local%2520curvature%252C%2520such%2520as%2520tree-like%2520hierarchies%2520and%2520dense%2520communities%252C%2520coexist%2520within%2520a%2520single%2520network.%2520Existing%2520geometric%2520GNNs%252C%2520which%2520embed%2520graphs%2520into%2520single%2520fixed-curvature%2520manifolds%2520or%2520discrete%2520product%2520spaces%252C%2520struggle%2520to%2520capture%2520this%2520diversity.%2520We%2520introduce%2520Adaptive%2520Riemannian%2520Graph%2520Neural%2520Networks%2520%2528ARGNN%2529%252C%2520a%2520novel%2520framework%2520that%2520learns%2520a%2520continuous%2520and%2520anisotropic%2520Riemannian%2520metric%2520tensor%2520field%2520over%2520the%2520graph.%2520It%2520allows%2520each%2520node%2520to%2520determine%2520its%2520optimal%2520local%2520geometry%252C%2520enabling%2520the%2520model%2520to%2520fluidly%2520adapt%2520to%2520the%2520graph%2527s%2520structural%2520landscape.%2520Our%2520core%2520innovation%2520is%2520an%2520efficient%2520parameterization%2520of%2520the%2520node-wise%2520metric%2520tensor%252C%2520specializing%2520to%2520a%2520learnable%2520diagonal%2520form%2520that%2520captures%2520directional%2520geometric%2520information%2520while%2520maintaining%2520computational%2520tractability.%2520To%2520ensure%2520geometric%2520regularity%2520and%2520stable%2520training%252C%2520we%2520integrate%2520a%2520Ricci%2520flow-inspired%2520regularization%2520that%2520smooths%2520the%2520learned%2520manifold.%2520Theoretically%252C%2520we%2520establish%2520the%2520rigorous%2520geometric%2520evolution%2520convergence%2520guarantee%2520for%2520ARGNN%2520and%2520provide%2520a%2520continuous%2520generalization%2520that%2520unifies%2520prior%2520fixed%2520or%2520mixed-curvature%2520GNNs.%2520Empirically%252C%2520our%2520method%2520demonstrates%2520superior%2520performance%2520on%2520both%2520homophilic%2520and%2520heterophilic%2520benchmark%2520datasets%2520with%2520the%2520ability%2520to%2520capture%2520diverse%2520structures%2520adaptively.%2520Moreover%252C%2520the%2520learned%2520geometries%2520both%2520offer%2520interpretable%2520insights%2520into%2520the%2520underlying%2520graph%2520structure%2520and%2520empirically%2520corroborate%2520our%2520theoretical%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.02600v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Riemannian%20Graph%20Neural%20Networks&entry.906535625=Xudong%20Wang%20and%20Chris%20Ding%20and%20Tongxin%20Li%20and%20Jicong%20Fan&entry.1292438233=Graph%20data%20often%20exhibits%20complex%20geometric%20heterogeneity%2C%20where%20structures%20with%20varying%20local%20curvature%2C%20such%20as%20tree-like%20hierarchies%20and%20dense%20communities%2C%20coexist%20within%20a%20single%20network.%20Existing%20geometric%20GNNs%2C%20which%20embed%20graphs%20into%20single%20fixed-curvature%20manifolds%20or%20discrete%20product%20spaces%2C%20struggle%20to%20capture%20this%20diversity.%20We%20introduce%20Adaptive%20Riemannian%20Graph%20Neural%20Networks%20%28ARGNN%29%2C%20a%20novel%20framework%20that%20learns%20a%20continuous%20and%20anisotropic%20Riemannian%20metric%20tensor%20field%20over%20the%20graph.%20It%20allows%20each%20node%20to%20determine%20its%20optimal%20local%20geometry%2C%20enabling%20the%20model%20to%20fluidly%20adapt%20to%20the%20graph%27s%20structural%20landscape.%20Our%20core%20innovation%20is%20an%20efficient%20parameterization%20of%20the%20node-wise%20metric%20tensor%2C%20specializing%20to%20a%20learnable%20diagonal%20form%20that%20captures%20directional%20geometric%20information%20while%20maintaining%20computational%20tractability.%20To%20ensure%20geometric%20regularity%20and%20stable%20training%2C%20we%20integrate%20a%20Ricci%20flow-inspired%20regularization%20that%20smooths%20the%20learned%20manifold.%20Theoretically%2C%20we%20establish%20the%20rigorous%20geometric%20evolution%20convergence%20guarantee%20for%20ARGNN%20and%20provide%20a%20continuous%20generalization%20that%20unifies%20prior%20fixed%20or%20mixed-curvature%20GNNs.%20Empirically%2C%20our%20method%20demonstrates%20superior%20performance%20on%20both%20homophilic%20and%20heterophilic%20benchmark%20datasets%20with%20the%20ability%20to%20capture%20diverse%20structures%20adaptively.%20Moreover%2C%20the%20learned%20geometries%20both%20offer%20interpretable%20insights%20into%20the%20underlying%20graph%20structure%20and%20empirically%20corroborate%20our%20theoretical%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2508.02600v2&entry.124074799=Read"},
{"title": "DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes", "author": "Aisha Al-Mohannadi and Ayisha Firoz and Yin Yang and Muhammad Imran and Ferda Ofli", "abstract": "Social media imagery provides a low-latency source of situational information during natural and human-induced disasters, enabling rapid damage assessment and response. While Visual Question Answering (VQA) has shown strong performance in general-purpose domains, its suitability for the complex and safety-critical reasoning required in disaster response remains unclear. We introduce DisasterVQA, a benchmark dataset designed for perception and reasoning in crisis contexts. DisasterVQA consists of 1,395 real-world images and 4,405 expert-curated question-answer pairs spanning diverse events such as floods, wildfires, and earthquakes. Grounded in humanitarian frameworks including FEMA ESF and OCHA MIRA, the dataset includes binary, multiple-choice, and open-ended questions covering situational awareness and operational decision-making tasks. We benchmark seven state-of-the-art vision-language models and find performance variability across question types, disaster categories, regions, and humanitarian tasks. Although models achieve high accuracy on binary questions, they struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios. DisasterVQA provides a challenging and practical benchmark to guide the development of more robust and operationally meaningful vision-language models for disaster response. The dataset is publicly available at https://zenodo.org/records/18267770.", "link": "http://arxiv.org/abs/2601.13839v1", "date": "2026-01-20", "relevancy": 2.6309, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5333}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5333}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DisasterVQA%3A%20A%20Visual%20Question%20Answering%20Benchmark%20Dataset%20for%20Disaster%20Scenes&body=Title%3A%20DisasterVQA%3A%20A%20Visual%20Question%20Answering%20Benchmark%20Dataset%20for%20Disaster%20Scenes%0AAuthor%3A%20Aisha%20Al-Mohannadi%20and%20Ayisha%20Firoz%20and%20Yin%20Yang%20and%20Muhammad%20Imran%20and%20Ferda%20Ofli%0AAbstract%3A%20Social%20media%20imagery%20provides%20a%20low-latency%20source%20of%20situational%20information%20during%20natural%20and%20human-induced%20disasters%2C%20enabling%20rapid%20damage%20assessment%20and%20response.%20While%20Visual%20Question%20Answering%20%28VQA%29%20has%20shown%20strong%20performance%20in%20general-purpose%20domains%2C%20its%20suitability%20for%20the%20complex%20and%20safety-critical%20reasoning%20required%20in%20disaster%20response%20remains%20unclear.%20We%20introduce%20DisasterVQA%2C%20a%20benchmark%20dataset%20designed%20for%20perception%20and%20reasoning%20in%20crisis%20contexts.%20DisasterVQA%20consists%20of%201%2C395%20real-world%20images%20and%204%2C405%20expert-curated%20question-answer%20pairs%20spanning%20diverse%20events%20such%20as%20floods%2C%20wildfires%2C%20and%20earthquakes.%20Grounded%20in%20humanitarian%20frameworks%20including%20FEMA%20ESF%20and%20OCHA%20MIRA%2C%20the%20dataset%20includes%20binary%2C%20multiple-choice%2C%20and%20open-ended%20questions%20covering%20situational%20awareness%20and%20operational%20decision-making%20tasks.%20We%20benchmark%20seven%20state-of-the-art%20vision-language%20models%20and%20find%20performance%20variability%20across%20question%20types%2C%20disaster%20categories%2C%20regions%2C%20and%20humanitarian%20tasks.%20Although%20models%20achieve%20high%20accuracy%20on%20binary%20questions%2C%20they%20struggle%20with%20fine-grained%20quantitative%20reasoning%2C%20object%20counting%2C%20and%20context-sensitive%20interpretation%2C%20particularly%20for%20underrepresented%20disaster%20scenarios.%20DisasterVQA%20provides%20a%20challenging%20and%20practical%20benchmark%20to%20guide%20the%20development%20of%20more%20robust%20and%20operationally%20meaningful%20vision-language%20models%20for%20disaster%20response.%20The%20dataset%20is%20publicly%20available%20at%20https%3A//zenodo.org/records/18267770.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13839v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisasterVQA%253A%2520A%2520Visual%2520Question%2520Answering%2520Benchmark%2520Dataset%2520for%2520Disaster%2520Scenes%26entry.906535625%3DAisha%2520Al-Mohannadi%2520and%2520Ayisha%2520Firoz%2520and%2520Yin%2520Yang%2520and%2520Muhammad%2520Imran%2520and%2520Ferda%2520Ofli%26entry.1292438233%3DSocial%2520media%2520imagery%2520provides%2520a%2520low-latency%2520source%2520of%2520situational%2520information%2520during%2520natural%2520and%2520human-induced%2520disasters%252C%2520enabling%2520rapid%2520damage%2520assessment%2520and%2520response.%2520While%2520Visual%2520Question%2520Answering%2520%2528VQA%2529%2520has%2520shown%2520strong%2520performance%2520in%2520general-purpose%2520domains%252C%2520its%2520suitability%2520for%2520the%2520complex%2520and%2520safety-critical%2520reasoning%2520required%2520in%2520disaster%2520response%2520remains%2520unclear.%2520We%2520introduce%2520DisasterVQA%252C%2520a%2520benchmark%2520dataset%2520designed%2520for%2520perception%2520and%2520reasoning%2520in%2520crisis%2520contexts.%2520DisasterVQA%2520consists%2520of%25201%252C395%2520real-world%2520images%2520and%25204%252C405%2520expert-curated%2520question-answer%2520pairs%2520spanning%2520diverse%2520events%2520such%2520as%2520floods%252C%2520wildfires%252C%2520and%2520earthquakes.%2520Grounded%2520in%2520humanitarian%2520frameworks%2520including%2520FEMA%2520ESF%2520and%2520OCHA%2520MIRA%252C%2520the%2520dataset%2520includes%2520binary%252C%2520multiple-choice%252C%2520and%2520open-ended%2520questions%2520covering%2520situational%2520awareness%2520and%2520operational%2520decision-making%2520tasks.%2520We%2520benchmark%2520seven%2520state-of-the-art%2520vision-language%2520models%2520and%2520find%2520performance%2520variability%2520across%2520question%2520types%252C%2520disaster%2520categories%252C%2520regions%252C%2520and%2520humanitarian%2520tasks.%2520Although%2520models%2520achieve%2520high%2520accuracy%2520on%2520binary%2520questions%252C%2520they%2520struggle%2520with%2520fine-grained%2520quantitative%2520reasoning%252C%2520object%2520counting%252C%2520and%2520context-sensitive%2520interpretation%252C%2520particularly%2520for%2520underrepresented%2520disaster%2520scenarios.%2520DisasterVQA%2520provides%2520a%2520challenging%2520and%2520practical%2520benchmark%2520to%2520guide%2520the%2520development%2520of%2520more%2520robust%2520and%2520operationally%2520meaningful%2520vision-language%2520models%2520for%2520disaster%2520response.%2520The%2520dataset%2520is%2520publicly%2520available%2520at%2520https%253A//zenodo.org/records/18267770.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13839v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DisasterVQA%3A%20A%20Visual%20Question%20Answering%20Benchmark%20Dataset%20for%20Disaster%20Scenes&entry.906535625=Aisha%20Al-Mohannadi%20and%20Ayisha%20Firoz%20and%20Yin%20Yang%20and%20Muhammad%20Imran%20and%20Ferda%20Ofli&entry.1292438233=Social%20media%20imagery%20provides%20a%20low-latency%20source%20of%20situational%20information%20during%20natural%20and%20human-induced%20disasters%2C%20enabling%20rapid%20damage%20assessment%20and%20response.%20While%20Visual%20Question%20Answering%20%28VQA%29%20has%20shown%20strong%20performance%20in%20general-purpose%20domains%2C%20its%20suitability%20for%20the%20complex%20and%20safety-critical%20reasoning%20required%20in%20disaster%20response%20remains%20unclear.%20We%20introduce%20DisasterVQA%2C%20a%20benchmark%20dataset%20designed%20for%20perception%20and%20reasoning%20in%20crisis%20contexts.%20DisasterVQA%20consists%20of%201%2C395%20real-world%20images%20and%204%2C405%20expert-curated%20question-answer%20pairs%20spanning%20diverse%20events%20such%20as%20floods%2C%20wildfires%2C%20and%20earthquakes.%20Grounded%20in%20humanitarian%20frameworks%20including%20FEMA%20ESF%20and%20OCHA%20MIRA%2C%20the%20dataset%20includes%20binary%2C%20multiple-choice%2C%20and%20open-ended%20questions%20covering%20situational%20awareness%20and%20operational%20decision-making%20tasks.%20We%20benchmark%20seven%20state-of-the-art%20vision-language%20models%20and%20find%20performance%20variability%20across%20question%20types%2C%20disaster%20categories%2C%20regions%2C%20and%20humanitarian%20tasks.%20Although%20models%20achieve%20high%20accuracy%20on%20binary%20questions%2C%20they%20struggle%20with%20fine-grained%20quantitative%20reasoning%2C%20object%20counting%2C%20and%20context-sensitive%20interpretation%2C%20particularly%20for%20underrepresented%20disaster%20scenarios.%20DisasterVQA%20provides%20a%20challenging%20and%20practical%20benchmark%20to%20guide%20the%20development%20of%20more%20robust%20and%20operationally%20meaningful%20vision-language%20models%20for%20disaster%20response.%20The%20dataset%20is%20publicly%20available%20at%20https%3A//zenodo.org/records/18267770.&entry.1838667208=http%3A//arxiv.org/abs/2601.13839v1&entry.124074799=Read"},
{"title": "IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models", "author": "Liang Shi and Wei Li and Kevin M Beussman and Lin Chen and Yun Fu", "abstract": "Instance-level recognition (ILR) concerns distinguishing individual instances from one another, with person re-identification as a prominent example. Despite the impressive visual perception capabilities of modern VLMs, we find their performance on ILR unsatisfactory, often dramatically underperforming domain-specific ILR models. This limitation hinders many practical application of VLMs, e.g. where recognizing familiar people and objects is crucial for effective visual understanding. Existing solutions typically learn to recognize instances one at a time using instance-specific datasets, which not only incur substantial data collection and training costs but also struggle with fine-grained discrimination. In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition. We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner. Further, IIR-VLM leverages this knowledge for instance-aware visual understanding. We validate IIR-VLM's efficacy on existing instance personalization benchmarks. Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.", "link": "http://arxiv.org/abs/2601.14188v1", "date": "2026-01-20", "relevancy": 2.5793, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5238}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5119}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20IIR-VLM%3A%20In-Context%20Instance-level%20Recognition%20for%20Large%20Vision-Language%20Models&body=Title%3A%20IIR-VLM%3A%20In-Context%20Instance-level%20Recognition%20for%20Large%20Vision-Language%20Models%0AAuthor%3A%20Liang%20Shi%20and%20Wei%20Li%20and%20Kevin%20M%20Beussman%20and%20Lin%20Chen%20and%20Yun%20Fu%0AAbstract%3A%20Instance-level%20recognition%20%28ILR%29%20concerns%20distinguishing%20individual%20instances%20from%20one%20another%2C%20with%20person%20re-identification%20as%20a%20prominent%20example.%20Despite%20the%20impressive%20visual%20perception%20capabilities%20of%20modern%20VLMs%2C%20we%20find%20their%20performance%20on%20ILR%20unsatisfactory%2C%20often%20dramatically%20underperforming%20domain-specific%20ILR%20models.%20This%20limitation%20hinders%20many%20practical%20application%20of%20VLMs%2C%20e.g.%20where%20recognizing%20familiar%20people%20and%20objects%20is%20crucial%20for%20effective%20visual%20understanding.%20Existing%20solutions%20typically%20learn%20to%20recognize%20instances%20one%20at%20a%20time%20using%20instance-specific%20datasets%2C%20which%20not%20only%20incur%20substantial%20data%20collection%20and%20training%20costs%20but%20also%20struggle%20with%20fine-grained%20discrimination.%20In%20this%20work%2C%20we%20propose%20IIR-VLM%2C%20a%20VLM%20enhanced%20for%20In-context%20Instance-level%20Recognition.%20We%20integrate%20pre-trained%20ILR%20expert%20models%20as%20auxiliary%20visual%20encoders%20to%20provide%20specialized%20features%20for%20learning%20diverse%20instances%2C%20which%20enables%20VLMs%20to%20learn%20new%20instances%20in-context%20in%20a%20one-shot%20manner.%20Further%2C%20IIR-VLM%20leverages%20this%20knowledge%20for%20instance-aware%20visual%20understanding.%20We%20validate%20IIR-VLM%27s%20efficacy%20on%20existing%20instance%20personalization%20benchmarks.%20Finally%2C%20we%20demonstrate%20its%20superior%20ILR%20performance%20on%20a%20challenging%20new%20benchmark%2C%20which%20assesses%20ILR%20capabilities%20across%20varying%20difficulty%20and%20diverse%20categories%2C%20with%20person%2C%20face%2C%20pet%20and%20general%20objects%20as%20the%20instances%20at%20task.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14188v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIIR-VLM%253A%2520In-Context%2520Instance-level%2520Recognition%2520for%2520Large%2520Vision-Language%2520Models%26entry.906535625%3DLiang%2520Shi%2520and%2520Wei%2520Li%2520and%2520Kevin%2520M%2520Beussman%2520and%2520Lin%2520Chen%2520and%2520Yun%2520Fu%26entry.1292438233%3DInstance-level%2520recognition%2520%2528ILR%2529%2520concerns%2520distinguishing%2520individual%2520instances%2520from%2520one%2520another%252C%2520with%2520person%2520re-identification%2520as%2520a%2520prominent%2520example.%2520Despite%2520the%2520impressive%2520visual%2520perception%2520capabilities%2520of%2520modern%2520VLMs%252C%2520we%2520find%2520their%2520performance%2520on%2520ILR%2520unsatisfactory%252C%2520often%2520dramatically%2520underperforming%2520domain-specific%2520ILR%2520models.%2520This%2520limitation%2520hinders%2520many%2520practical%2520application%2520of%2520VLMs%252C%2520e.g.%2520where%2520recognizing%2520familiar%2520people%2520and%2520objects%2520is%2520crucial%2520for%2520effective%2520visual%2520understanding.%2520Existing%2520solutions%2520typically%2520learn%2520to%2520recognize%2520instances%2520one%2520at%2520a%2520time%2520using%2520instance-specific%2520datasets%252C%2520which%2520not%2520only%2520incur%2520substantial%2520data%2520collection%2520and%2520training%2520costs%2520but%2520also%2520struggle%2520with%2520fine-grained%2520discrimination.%2520In%2520this%2520work%252C%2520we%2520propose%2520IIR-VLM%252C%2520a%2520VLM%2520enhanced%2520for%2520In-context%2520Instance-level%2520Recognition.%2520We%2520integrate%2520pre-trained%2520ILR%2520expert%2520models%2520as%2520auxiliary%2520visual%2520encoders%2520to%2520provide%2520specialized%2520features%2520for%2520learning%2520diverse%2520instances%252C%2520which%2520enables%2520VLMs%2520to%2520learn%2520new%2520instances%2520in-context%2520in%2520a%2520one-shot%2520manner.%2520Further%252C%2520IIR-VLM%2520leverages%2520this%2520knowledge%2520for%2520instance-aware%2520visual%2520understanding.%2520We%2520validate%2520IIR-VLM%2527s%2520efficacy%2520on%2520existing%2520instance%2520personalization%2520benchmarks.%2520Finally%252C%2520we%2520demonstrate%2520its%2520superior%2520ILR%2520performance%2520on%2520a%2520challenging%2520new%2520benchmark%252C%2520which%2520assesses%2520ILR%2520capabilities%2520across%2520varying%2520difficulty%2520and%2520diverse%2520categories%252C%2520with%2520person%252C%2520face%252C%2520pet%2520and%2520general%2520objects%2520as%2520the%2520instances%2520at%2520task.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14188v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=IIR-VLM%3A%20In-Context%20Instance-level%20Recognition%20for%20Large%20Vision-Language%20Models&entry.906535625=Liang%20Shi%20and%20Wei%20Li%20and%20Kevin%20M%20Beussman%20and%20Lin%20Chen%20and%20Yun%20Fu&entry.1292438233=Instance-level%20recognition%20%28ILR%29%20concerns%20distinguishing%20individual%20instances%20from%20one%20another%2C%20with%20person%20re-identification%20as%20a%20prominent%20example.%20Despite%20the%20impressive%20visual%20perception%20capabilities%20of%20modern%20VLMs%2C%20we%20find%20their%20performance%20on%20ILR%20unsatisfactory%2C%20often%20dramatically%20underperforming%20domain-specific%20ILR%20models.%20This%20limitation%20hinders%20many%20practical%20application%20of%20VLMs%2C%20e.g.%20where%20recognizing%20familiar%20people%20and%20objects%20is%20crucial%20for%20effective%20visual%20understanding.%20Existing%20solutions%20typically%20learn%20to%20recognize%20instances%20one%20at%20a%20time%20using%20instance-specific%20datasets%2C%20which%20not%20only%20incur%20substantial%20data%20collection%20and%20training%20costs%20but%20also%20struggle%20with%20fine-grained%20discrimination.%20In%20this%20work%2C%20we%20propose%20IIR-VLM%2C%20a%20VLM%20enhanced%20for%20In-context%20Instance-level%20Recognition.%20We%20integrate%20pre-trained%20ILR%20expert%20models%20as%20auxiliary%20visual%20encoders%20to%20provide%20specialized%20features%20for%20learning%20diverse%20instances%2C%20which%20enables%20VLMs%20to%20learn%20new%20instances%20in-context%20in%20a%20one-shot%20manner.%20Further%2C%20IIR-VLM%20leverages%20this%20knowledge%20for%20instance-aware%20visual%20understanding.%20We%20validate%20IIR-VLM%27s%20efficacy%20on%20existing%20instance%20personalization%20benchmarks.%20Finally%2C%20we%20demonstrate%20its%20superior%20ILR%20performance%20on%20a%20challenging%20new%20benchmark%2C%20which%20assesses%20ILR%20capabilities%20across%20varying%20difficulty%20and%20diverse%20categories%2C%20with%20person%2C%20face%2C%20pet%20and%20general%20objects%20as%20the%20instances%20at%20task.&entry.1838667208=http%3A//arxiv.org/abs/2601.14188v1&entry.124074799=Read"},
{"title": "Federated Balanced Learning", "author": "Jiaze Li and Haoran Xu and Wanyi Wu and Changwei Wang and Shuaiguang Li and Jianzhong Ju and Zhenbo Luo and Jian Luan and Youyang Qu and Longxiang Gao and Xudong Yang and Lumin Xing", "abstract": "Federated learning is a paradigm of joint learning in which clients collaborate by sharing model parameters instead of data. However, in the non-iid setting, the global model experiences client drift, which can seriously affect the final performance of the model. Previous methods tend to correct the global model that has already deviated based on the loss function or gradient, overlooking the impact of the client samples. In this paper, we rethink the role of the client side and propose Federated Balanced Learning, i.e., FBL, to prevent this issue from the beginning through sample balance on the client side. Technically, FBL allows unbalanced data on the client side to achieve sample balance through knowledge filling and knowledge sampling using edge-side generation models, under the limitation of a fixed number of data samples on clients. Furthermore, we design a Knowledge Alignment Strategy to bridge the gap between synthetic and real data, and a Knowledge Drop Strategy to regularize our method. Meanwhile, we scale our method to real and complex scenarios, allowing different clients to adopt various methods, and extend our framework to further improve performance. Numerous experiments show that our method outperforms state-of-the-art baselines. The code is released upon acceptance.", "link": "http://arxiv.org/abs/2601.14042v1", "date": "2026-01-20", "relevancy": 2.5526, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5431}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5051}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4834}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Balanced%20Learning&body=Title%3A%20Federated%20Balanced%20Learning%0AAuthor%3A%20Jiaze%20Li%20and%20Haoran%20Xu%20and%20Wanyi%20Wu%20and%20Changwei%20Wang%20and%20Shuaiguang%20Li%20and%20Jianzhong%20Ju%20and%20Zhenbo%20Luo%20and%20Jian%20Luan%20and%20Youyang%20Qu%20and%20Longxiang%20Gao%20and%20Xudong%20Yang%20and%20Lumin%20Xing%0AAbstract%3A%20Federated%20learning%20is%20a%20paradigm%20of%20joint%20learning%20in%20which%20clients%20collaborate%20by%20sharing%20model%20parameters%20instead%20of%20data.%20However%2C%20in%20the%20non-iid%20setting%2C%20the%20global%20model%20experiences%20client%20drift%2C%20which%20can%20seriously%20affect%20the%20final%20performance%20of%20the%20model.%20Previous%20methods%20tend%20to%20correct%20the%20global%20model%20that%20has%20already%20deviated%20based%20on%20the%20loss%20function%20or%20gradient%2C%20overlooking%20the%20impact%20of%20the%20client%20samples.%20In%20this%20paper%2C%20we%20rethink%20the%20role%20of%20the%20client%20side%20and%20propose%20Federated%20Balanced%20Learning%2C%20i.e.%2C%20FBL%2C%20to%20prevent%20this%20issue%20from%20the%20beginning%20through%20sample%20balance%20on%20the%20client%20side.%20Technically%2C%20FBL%20allows%20unbalanced%20data%20on%20the%20client%20side%20to%20achieve%20sample%20balance%20through%20knowledge%20filling%20and%20knowledge%20sampling%20using%20edge-side%20generation%20models%2C%20under%20the%20limitation%20of%20a%20fixed%20number%20of%20data%20samples%20on%20clients.%20Furthermore%2C%20we%20design%20a%20Knowledge%20Alignment%20Strategy%20to%20bridge%20the%20gap%20between%20synthetic%20and%20real%20data%2C%20and%20a%20Knowledge%20Drop%20Strategy%20to%20regularize%20our%20method.%20Meanwhile%2C%20we%20scale%20our%20method%20to%20real%20and%20complex%20scenarios%2C%20allowing%20different%20clients%20to%20adopt%20various%20methods%2C%20and%20extend%20our%20framework%20to%20further%20improve%20performance.%20Numerous%20experiments%20show%20that%20our%20method%20outperforms%20state-of-the-art%20baselines.%20The%20code%20is%20released%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14042v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Balanced%2520Learning%26entry.906535625%3DJiaze%2520Li%2520and%2520Haoran%2520Xu%2520and%2520Wanyi%2520Wu%2520and%2520Changwei%2520Wang%2520and%2520Shuaiguang%2520Li%2520and%2520Jianzhong%2520Ju%2520and%2520Zhenbo%2520Luo%2520and%2520Jian%2520Luan%2520and%2520Youyang%2520Qu%2520and%2520Longxiang%2520Gao%2520and%2520Xudong%2520Yang%2520and%2520Lumin%2520Xing%26entry.1292438233%3DFederated%2520learning%2520is%2520a%2520paradigm%2520of%2520joint%2520learning%2520in%2520which%2520clients%2520collaborate%2520by%2520sharing%2520model%2520parameters%2520instead%2520of%2520data.%2520However%252C%2520in%2520the%2520non-iid%2520setting%252C%2520the%2520global%2520model%2520experiences%2520client%2520drift%252C%2520which%2520can%2520seriously%2520affect%2520the%2520final%2520performance%2520of%2520the%2520model.%2520Previous%2520methods%2520tend%2520to%2520correct%2520the%2520global%2520model%2520that%2520has%2520already%2520deviated%2520based%2520on%2520the%2520loss%2520function%2520or%2520gradient%252C%2520overlooking%2520the%2520impact%2520of%2520the%2520client%2520samples.%2520In%2520this%2520paper%252C%2520we%2520rethink%2520the%2520role%2520of%2520the%2520client%2520side%2520and%2520propose%2520Federated%2520Balanced%2520Learning%252C%2520i.e.%252C%2520FBL%252C%2520to%2520prevent%2520this%2520issue%2520from%2520the%2520beginning%2520through%2520sample%2520balance%2520on%2520the%2520client%2520side.%2520Technically%252C%2520FBL%2520allows%2520unbalanced%2520data%2520on%2520the%2520client%2520side%2520to%2520achieve%2520sample%2520balance%2520through%2520knowledge%2520filling%2520and%2520knowledge%2520sampling%2520using%2520edge-side%2520generation%2520models%252C%2520under%2520the%2520limitation%2520of%2520a%2520fixed%2520number%2520of%2520data%2520samples%2520on%2520clients.%2520Furthermore%252C%2520we%2520design%2520a%2520Knowledge%2520Alignment%2520Strategy%2520to%2520bridge%2520the%2520gap%2520between%2520synthetic%2520and%2520real%2520data%252C%2520and%2520a%2520Knowledge%2520Drop%2520Strategy%2520to%2520regularize%2520our%2520method.%2520Meanwhile%252C%2520we%2520scale%2520our%2520method%2520to%2520real%2520and%2520complex%2520scenarios%252C%2520allowing%2520different%2520clients%2520to%2520adopt%2520various%2520methods%252C%2520and%2520extend%2520our%2520framework%2520to%2520further%2520improve%2520performance.%2520Numerous%2520experiments%2520show%2520that%2520our%2520method%2520outperforms%2520state-of-the-art%2520baselines.%2520The%2520code%2520is%2520released%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14042v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Balanced%20Learning&entry.906535625=Jiaze%20Li%20and%20Haoran%20Xu%20and%20Wanyi%20Wu%20and%20Changwei%20Wang%20and%20Shuaiguang%20Li%20and%20Jianzhong%20Ju%20and%20Zhenbo%20Luo%20and%20Jian%20Luan%20and%20Youyang%20Qu%20and%20Longxiang%20Gao%20and%20Xudong%20Yang%20and%20Lumin%20Xing&entry.1292438233=Federated%20learning%20is%20a%20paradigm%20of%20joint%20learning%20in%20which%20clients%20collaborate%20by%20sharing%20model%20parameters%20instead%20of%20data.%20However%2C%20in%20the%20non-iid%20setting%2C%20the%20global%20model%20experiences%20client%20drift%2C%20which%20can%20seriously%20affect%20the%20final%20performance%20of%20the%20model.%20Previous%20methods%20tend%20to%20correct%20the%20global%20model%20that%20has%20already%20deviated%20based%20on%20the%20loss%20function%20or%20gradient%2C%20overlooking%20the%20impact%20of%20the%20client%20samples.%20In%20this%20paper%2C%20we%20rethink%20the%20role%20of%20the%20client%20side%20and%20propose%20Federated%20Balanced%20Learning%2C%20i.e.%2C%20FBL%2C%20to%20prevent%20this%20issue%20from%20the%20beginning%20through%20sample%20balance%20on%20the%20client%20side.%20Technically%2C%20FBL%20allows%20unbalanced%20data%20on%20the%20client%20side%20to%20achieve%20sample%20balance%20through%20knowledge%20filling%20and%20knowledge%20sampling%20using%20edge-side%20generation%20models%2C%20under%20the%20limitation%20of%20a%20fixed%20number%20of%20data%20samples%20on%20clients.%20Furthermore%2C%20we%20design%20a%20Knowledge%20Alignment%20Strategy%20to%20bridge%20the%20gap%20between%20synthetic%20and%20real%20data%2C%20and%20a%20Knowledge%20Drop%20Strategy%20to%20regularize%20our%20method.%20Meanwhile%2C%20we%20scale%20our%20method%20to%20real%20and%20complex%20scenarios%2C%20allowing%20different%20clients%20to%20adopt%20various%20methods%2C%20and%20extend%20our%20framework%20to%20further%20improve%20performance.%20Numerous%20experiments%20show%20that%20our%20method%20outperforms%20state-of-the-art%20baselines.%20The%20code%20is%20released%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2601.14042v1&entry.124074799=Read"},
{"title": "SHARE: A Fully Unsupervised Framework for Single Hyperspectral Image Restoration", "author": "Jiangwei Xie and Zhang Wen and Mike Davies and Dongdong Chen", "abstract": "Hyperspectral image (HSI) restoration is a fundamental challenge in computational imaging and computer vision. It involves ill-posed inverse problems, such as inpainting and super-resolution. Although deep learning methods have transformed the field through data-driven learning, their effectiveness hinges on access to meticulously curated ground-truth datasets. This fundamentally restricts their applicability in real-world scenarios where such data is unavailable. This paper presents SHARE (Single Hyperspectral Image Restoration with Equivariance), a fully unsupervised framework that unifies geometric equivariance principles with low-rank spectral modelling to eliminate the need for ground truth. SHARE's core concept is to exploit the intrinsic invariance of hyperspectral structures under differentiable geometric transformations (e.g. rotations and scaling) to derive self-supervision signals through equivariance consistency constraints. Our novel Dynamic Adaptive Spectral Attention (DASA) module further enhances this paradigm shift by explicitly encoding the global low-rank property of HSI and adaptively refining local spectral-spatial correlations through learnable attention mechanisms. Extensive experiments on HSI inpainting and super-resolution tasks demonstrate the effectiveness of SHARE. Our method outperforms many state-of-the-art unsupervised approaches and achieves performance comparable to that of supervised methods. We hope that our approach will shed new light on HSI restoration and broader scientific imaging scenarios. The code will be released at https://github.com/xuwayyy/SHARE.", "link": "http://arxiv.org/abs/2601.13987v1", "date": "2026-01-20", "relevancy": 2.5502, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5223}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.5062}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5017}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SHARE%3A%20A%20Fully%20Unsupervised%20Framework%20for%20Single%20Hyperspectral%20Image%20Restoration&body=Title%3A%20SHARE%3A%20A%20Fully%20Unsupervised%20Framework%20for%20Single%20Hyperspectral%20Image%20Restoration%0AAuthor%3A%20Jiangwei%20Xie%20and%20Zhang%20Wen%20and%20Mike%20Davies%20and%20Dongdong%20Chen%0AAbstract%3A%20Hyperspectral%20image%20%28HSI%29%20restoration%20is%20a%20fundamental%20challenge%20in%20computational%20imaging%20and%20computer%20vision.%20It%20involves%20ill-posed%20inverse%20problems%2C%20such%20as%20inpainting%20and%20super-resolution.%20Although%20deep%20learning%20methods%20have%20transformed%20the%20field%20through%20data-driven%20learning%2C%20their%20effectiveness%20hinges%20on%20access%20to%20meticulously%20curated%20ground-truth%20datasets.%20This%20fundamentally%20restricts%20their%20applicability%20in%20real-world%20scenarios%20where%20such%20data%20is%20unavailable.%20This%20paper%20presents%20SHARE%20%28Single%20Hyperspectral%20Image%20Restoration%20with%20Equivariance%29%2C%20a%20fully%20unsupervised%20framework%20that%20unifies%20geometric%20equivariance%20principles%20with%20low-rank%20spectral%20modelling%20to%20eliminate%20the%20need%20for%20ground%20truth.%20SHARE%27s%20core%20concept%20is%20to%20exploit%20the%20intrinsic%20invariance%20of%20hyperspectral%20structures%20under%20differentiable%20geometric%20transformations%20%28e.g.%20rotations%20and%20scaling%29%20to%20derive%20self-supervision%20signals%20through%20equivariance%20consistency%20constraints.%20Our%20novel%20Dynamic%20Adaptive%20Spectral%20Attention%20%28DASA%29%20module%20further%20enhances%20this%20paradigm%20shift%20by%20explicitly%20encoding%20the%20global%20low-rank%20property%20of%20HSI%20and%20adaptively%20refining%20local%20spectral-spatial%20correlations%20through%20learnable%20attention%20mechanisms.%20Extensive%20experiments%20on%20HSI%20inpainting%20and%20super-resolution%20tasks%20demonstrate%20the%20effectiveness%20of%20SHARE.%20Our%20method%20outperforms%20many%20state-of-the-art%20unsupervised%20approaches%20and%20achieves%20performance%20comparable%20to%20that%20of%20supervised%20methods.%20We%20hope%20that%20our%20approach%20will%20shed%20new%20light%20on%20HSI%20restoration%20and%20broader%20scientific%20imaging%20scenarios.%20The%20code%20will%20be%20released%20at%20https%3A//github.com/xuwayyy/SHARE.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13987v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSHARE%253A%2520A%2520Fully%2520Unsupervised%2520Framework%2520for%2520Single%2520Hyperspectral%2520Image%2520Restoration%26entry.906535625%3DJiangwei%2520Xie%2520and%2520Zhang%2520Wen%2520and%2520Mike%2520Davies%2520and%2520Dongdong%2520Chen%26entry.1292438233%3DHyperspectral%2520image%2520%2528HSI%2529%2520restoration%2520is%2520a%2520fundamental%2520challenge%2520in%2520computational%2520imaging%2520and%2520computer%2520vision.%2520It%2520involves%2520ill-posed%2520inverse%2520problems%252C%2520such%2520as%2520inpainting%2520and%2520super-resolution.%2520Although%2520deep%2520learning%2520methods%2520have%2520transformed%2520the%2520field%2520through%2520data-driven%2520learning%252C%2520their%2520effectiveness%2520hinges%2520on%2520access%2520to%2520meticulously%2520curated%2520ground-truth%2520datasets.%2520This%2520fundamentally%2520restricts%2520their%2520applicability%2520in%2520real-world%2520scenarios%2520where%2520such%2520data%2520is%2520unavailable.%2520This%2520paper%2520presents%2520SHARE%2520%2528Single%2520Hyperspectral%2520Image%2520Restoration%2520with%2520Equivariance%2529%252C%2520a%2520fully%2520unsupervised%2520framework%2520that%2520unifies%2520geometric%2520equivariance%2520principles%2520with%2520low-rank%2520spectral%2520modelling%2520to%2520eliminate%2520the%2520need%2520for%2520ground%2520truth.%2520SHARE%2527s%2520core%2520concept%2520is%2520to%2520exploit%2520the%2520intrinsic%2520invariance%2520of%2520hyperspectral%2520structures%2520under%2520differentiable%2520geometric%2520transformations%2520%2528e.g.%2520rotations%2520and%2520scaling%2529%2520to%2520derive%2520self-supervision%2520signals%2520through%2520equivariance%2520consistency%2520constraints.%2520Our%2520novel%2520Dynamic%2520Adaptive%2520Spectral%2520Attention%2520%2528DASA%2529%2520module%2520further%2520enhances%2520this%2520paradigm%2520shift%2520by%2520explicitly%2520encoding%2520the%2520global%2520low-rank%2520property%2520of%2520HSI%2520and%2520adaptively%2520refining%2520local%2520spectral-spatial%2520correlations%2520through%2520learnable%2520attention%2520mechanisms.%2520Extensive%2520experiments%2520on%2520HSI%2520inpainting%2520and%2520super-resolution%2520tasks%2520demonstrate%2520the%2520effectiveness%2520of%2520SHARE.%2520Our%2520method%2520outperforms%2520many%2520state-of-the-art%2520unsupervised%2520approaches%2520and%2520achieves%2520performance%2520comparable%2520to%2520that%2520of%2520supervised%2520methods.%2520We%2520hope%2520that%2520our%2520approach%2520will%2520shed%2520new%2520light%2520on%2520HSI%2520restoration%2520and%2520broader%2520scientific%2520imaging%2520scenarios.%2520The%2520code%2520will%2520be%2520released%2520at%2520https%253A//github.com/xuwayyy/SHARE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13987v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SHARE%3A%20A%20Fully%20Unsupervised%20Framework%20for%20Single%20Hyperspectral%20Image%20Restoration&entry.906535625=Jiangwei%20Xie%20and%20Zhang%20Wen%20and%20Mike%20Davies%20and%20Dongdong%20Chen&entry.1292438233=Hyperspectral%20image%20%28HSI%29%20restoration%20is%20a%20fundamental%20challenge%20in%20computational%20imaging%20and%20computer%20vision.%20It%20involves%20ill-posed%20inverse%20problems%2C%20such%20as%20inpainting%20and%20super-resolution.%20Although%20deep%20learning%20methods%20have%20transformed%20the%20field%20through%20data-driven%20learning%2C%20their%20effectiveness%20hinges%20on%20access%20to%20meticulously%20curated%20ground-truth%20datasets.%20This%20fundamentally%20restricts%20their%20applicability%20in%20real-world%20scenarios%20where%20such%20data%20is%20unavailable.%20This%20paper%20presents%20SHARE%20%28Single%20Hyperspectral%20Image%20Restoration%20with%20Equivariance%29%2C%20a%20fully%20unsupervised%20framework%20that%20unifies%20geometric%20equivariance%20principles%20with%20low-rank%20spectral%20modelling%20to%20eliminate%20the%20need%20for%20ground%20truth.%20SHARE%27s%20core%20concept%20is%20to%20exploit%20the%20intrinsic%20invariance%20of%20hyperspectral%20structures%20under%20differentiable%20geometric%20transformations%20%28e.g.%20rotations%20and%20scaling%29%20to%20derive%20self-supervision%20signals%20through%20equivariance%20consistency%20constraints.%20Our%20novel%20Dynamic%20Adaptive%20Spectral%20Attention%20%28DASA%29%20module%20further%20enhances%20this%20paradigm%20shift%20by%20explicitly%20encoding%20the%20global%20low-rank%20property%20of%20HSI%20and%20adaptively%20refining%20local%20spectral-spatial%20correlations%20through%20learnable%20attention%20mechanisms.%20Extensive%20experiments%20on%20HSI%20inpainting%20and%20super-resolution%20tasks%20demonstrate%20the%20effectiveness%20of%20SHARE.%20Our%20method%20outperforms%20many%20state-of-the-art%20unsupervised%20approaches%20and%20achieves%20performance%20comparable%20to%20that%20of%20supervised%20methods.%20We%20hope%20that%20our%20approach%20will%20shed%20new%20light%20on%20HSI%20restoration%20and%20broader%20scientific%20imaging%20scenarios.%20The%20code%20will%20be%20released%20at%20https%3A//github.com/xuwayyy/SHARE.&entry.1838667208=http%3A//arxiv.org/abs/2601.13987v1&entry.124074799=Read"},
{"title": "POCI-Diff: Position Objects Consistently and Interactively with 3D-Layout Guided Diffusion", "author": "Andrea Rigo and Luca Stornaiuolo and Weijie Wang and Mauro Martino and Bruno Lepri and Nicu Sebe", "abstract": "We propose a diffusion-based approach for Text-to-Image (T2I) generation with consistent and interactive 3D layout control and editing. While prior methods improve spatial adherence using 2D cues or iterative copy-warp-paste strategies, they often distort object geometry and fail to preserve consistency across edits. To address these limitations, we introduce a framework for Positioning Objects Consistently and Interactively (POCI-Diff), a novel formulation for jointly enforcing 3D geometric constraints and instance-level semantic binding within a unified diffusion process. Our method enables explicit per-object semantic control by binding individual text descriptions to specific 3D bounding boxes through Blended Latent Diffusion, allowing one-shot synthesis of complex multi-object scenes. We further propose a warping-free generative editing pipeline that supports object insertion, removal, and transformation via regeneration rather than pixel deformation. To preserve object identity and consistency across edits, we condition the diffusion process on reference images using IP-Adapter, enabling coherent object appearance throughout interactive 3D editing while maintaining global scene coherence. Experimental results demonstrate that POCI-Diff produces high-quality images consistent with the specified 3D layouts and edits, outperforming state-of-the-art methods in both visual fidelity and layout adherence while eliminating warping-induced geometric artifacts.", "link": "http://arxiv.org/abs/2601.14056v1", "date": "2026-01-20", "relevancy": 2.5405, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6389}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6372}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.6304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20POCI-Diff%3A%20Position%20Objects%20Consistently%20and%20Interactively%20with%203D-Layout%20Guided%20Diffusion&body=Title%3A%20POCI-Diff%3A%20Position%20Objects%20Consistently%20and%20Interactively%20with%203D-Layout%20Guided%20Diffusion%0AAuthor%3A%20Andrea%20Rigo%20and%20Luca%20Stornaiuolo%20and%20Weijie%20Wang%20and%20Mauro%20Martino%20and%20Bruno%20Lepri%20and%20Nicu%20Sebe%0AAbstract%3A%20We%20propose%20a%20diffusion-based%20approach%20for%20Text-to-Image%20%28T2I%29%20generation%20with%20consistent%20and%20interactive%203D%20layout%20control%20and%20editing.%20While%20prior%20methods%20improve%20spatial%20adherence%20using%202D%20cues%20or%20iterative%20copy-warp-paste%20strategies%2C%20they%20often%20distort%20object%20geometry%20and%20fail%20to%20preserve%20consistency%20across%20edits.%20To%20address%20these%20limitations%2C%20we%20introduce%20a%20framework%20for%20Positioning%20Objects%20Consistently%20and%20Interactively%20%28POCI-Diff%29%2C%20a%20novel%20formulation%20for%20jointly%20enforcing%203D%20geometric%20constraints%20and%20instance-level%20semantic%20binding%20within%20a%20unified%20diffusion%20process.%20Our%20method%20enables%20explicit%20per-object%20semantic%20control%20by%20binding%20individual%20text%20descriptions%20to%20specific%203D%20bounding%20boxes%20through%20Blended%20Latent%20Diffusion%2C%20allowing%20one-shot%20synthesis%20of%20complex%20multi-object%20scenes.%20We%20further%20propose%20a%20warping-free%20generative%20editing%20pipeline%20that%20supports%20object%20insertion%2C%20removal%2C%20and%20transformation%20via%20regeneration%20rather%20than%20pixel%20deformation.%20To%20preserve%20object%20identity%20and%20consistency%20across%20edits%2C%20we%20condition%20the%20diffusion%20process%20on%20reference%20images%20using%20IP-Adapter%2C%20enabling%20coherent%20object%20appearance%20throughout%20interactive%203D%20editing%20while%20maintaining%20global%20scene%20coherence.%20Experimental%20results%20demonstrate%20that%20POCI-Diff%20produces%20high-quality%20images%20consistent%20with%20the%20specified%203D%20layouts%20and%20edits%2C%20outperforming%20state-of-the-art%20methods%20in%20both%20visual%20fidelity%20and%20layout%20adherence%20while%20eliminating%20warping-induced%20geometric%20artifacts.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14056v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPOCI-Diff%253A%2520Position%2520Objects%2520Consistently%2520and%2520Interactively%2520with%25203D-Layout%2520Guided%2520Diffusion%26entry.906535625%3DAndrea%2520Rigo%2520and%2520Luca%2520Stornaiuolo%2520and%2520Weijie%2520Wang%2520and%2520Mauro%2520Martino%2520and%2520Bruno%2520Lepri%2520and%2520Nicu%2520Sebe%26entry.1292438233%3DWe%2520propose%2520a%2520diffusion-based%2520approach%2520for%2520Text-to-Image%2520%2528T2I%2529%2520generation%2520with%2520consistent%2520and%2520interactive%25203D%2520layout%2520control%2520and%2520editing.%2520While%2520prior%2520methods%2520improve%2520spatial%2520adherence%2520using%25202D%2520cues%2520or%2520iterative%2520copy-warp-paste%2520strategies%252C%2520they%2520often%2520distort%2520object%2520geometry%2520and%2520fail%2520to%2520preserve%2520consistency%2520across%2520edits.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520a%2520framework%2520for%2520Positioning%2520Objects%2520Consistently%2520and%2520Interactively%2520%2528POCI-Diff%2529%252C%2520a%2520novel%2520formulation%2520for%2520jointly%2520enforcing%25203D%2520geometric%2520constraints%2520and%2520instance-level%2520semantic%2520binding%2520within%2520a%2520unified%2520diffusion%2520process.%2520Our%2520method%2520enables%2520explicit%2520per-object%2520semantic%2520control%2520by%2520binding%2520individual%2520text%2520descriptions%2520to%2520specific%25203D%2520bounding%2520boxes%2520through%2520Blended%2520Latent%2520Diffusion%252C%2520allowing%2520one-shot%2520synthesis%2520of%2520complex%2520multi-object%2520scenes.%2520We%2520further%2520propose%2520a%2520warping-free%2520generative%2520editing%2520pipeline%2520that%2520supports%2520object%2520insertion%252C%2520removal%252C%2520and%2520transformation%2520via%2520regeneration%2520rather%2520than%2520pixel%2520deformation.%2520To%2520preserve%2520object%2520identity%2520and%2520consistency%2520across%2520edits%252C%2520we%2520condition%2520the%2520diffusion%2520process%2520on%2520reference%2520images%2520using%2520IP-Adapter%252C%2520enabling%2520coherent%2520object%2520appearance%2520throughout%2520interactive%25203D%2520editing%2520while%2520maintaining%2520global%2520scene%2520coherence.%2520Experimental%2520results%2520demonstrate%2520that%2520POCI-Diff%2520produces%2520high-quality%2520images%2520consistent%2520with%2520the%2520specified%25203D%2520layouts%2520and%2520edits%252C%2520outperforming%2520state-of-the-art%2520methods%2520in%2520both%2520visual%2520fidelity%2520and%2520layout%2520adherence%2520while%2520eliminating%2520warping-induced%2520geometric%2520artifacts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14056v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=POCI-Diff%3A%20Position%20Objects%20Consistently%20and%20Interactively%20with%203D-Layout%20Guided%20Diffusion&entry.906535625=Andrea%20Rigo%20and%20Luca%20Stornaiuolo%20and%20Weijie%20Wang%20and%20Mauro%20Martino%20and%20Bruno%20Lepri%20and%20Nicu%20Sebe&entry.1292438233=We%20propose%20a%20diffusion-based%20approach%20for%20Text-to-Image%20%28T2I%29%20generation%20with%20consistent%20and%20interactive%203D%20layout%20control%20and%20editing.%20While%20prior%20methods%20improve%20spatial%20adherence%20using%202D%20cues%20or%20iterative%20copy-warp-paste%20strategies%2C%20they%20often%20distort%20object%20geometry%20and%20fail%20to%20preserve%20consistency%20across%20edits.%20To%20address%20these%20limitations%2C%20we%20introduce%20a%20framework%20for%20Positioning%20Objects%20Consistently%20and%20Interactively%20%28POCI-Diff%29%2C%20a%20novel%20formulation%20for%20jointly%20enforcing%203D%20geometric%20constraints%20and%20instance-level%20semantic%20binding%20within%20a%20unified%20diffusion%20process.%20Our%20method%20enables%20explicit%20per-object%20semantic%20control%20by%20binding%20individual%20text%20descriptions%20to%20specific%203D%20bounding%20boxes%20through%20Blended%20Latent%20Diffusion%2C%20allowing%20one-shot%20synthesis%20of%20complex%20multi-object%20scenes.%20We%20further%20propose%20a%20warping-free%20generative%20editing%20pipeline%20that%20supports%20object%20insertion%2C%20removal%2C%20and%20transformation%20via%20regeneration%20rather%20than%20pixel%20deformation.%20To%20preserve%20object%20identity%20and%20consistency%20across%20edits%2C%20we%20condition%20the%20diffusion%20process%20on%20reference%20images%20using%20IP-Adapter%2C%20enabling%20coherent%20object%20appearance%20throughout%20interactive%203D%20editing%20while%20maintaining%20global%20scene%20coherence.%20Experimental%20results%20demonstrate%20that%20POCI-Diff%20produces%20high-quality%20images%20consistent%20with%20the%20specified%203D%20layouts%20and%20edits%2C%20outperforming%20state-of-the-art%20methods%20in%20both%20visual%20fidelity%20and%20layout%20adherence%20while%20eliminating%20warping-induced%20geometric%20artifacts.&entry.1838667208=http%3A//arxiv.org/abs/2601.14056v1&entry.124074799=Read"},
{"title": "A universal linearized subspace refinement framework for neural networks", "author": "Wenbo Cao and Weiwei Zhang", "abstract": "Neural networks are predominantly trained using gradient-based methods, yet in many applications their final predictions remain far from the accuracy attainable within the model's expressive capacity. We introduce Linearized Subspace Refinement (LSR), a general and architecture-agnostic framework that exploits the Jacobian-induced linear residual model at a fixed trained network state. By solving a reduced direct least-squares problem within this subspace, LSR computes a subspace-optimal solution of the linearized residual model, yielding a refined linear predictor with substantially improved accuracy over standard gradient-trained solutions, without modifying network architectures, loss formulations, or training procedures. Across supervised function approximation, data-driven operator learning, and physics-informed operator fine-tuning, we show that gradient-based training often fails to access this attainable accuracy, even when local linearization yields a convex problem. This observation indicates that loss-induced numerical ill-conditioning, rather than nonconvexity or model expressivity, can constitute a dominant practical bottleneck. In contrast, one-shot LSR systematically exposes accuracy levels not fully exploited by gradient-based training, frequently achieving order-of-magnitude error reductions. For operator-constrained problems with composite loss structures, we further introduce Iterative LSR, which alternates one-shot LSR with supervised nonlinear alignment, transforming ill-conditioned residual minimization into numerically benign fitting steps and yielding accelerated convergence and improved accuracy. By bridging nonlinear neural representations with reduced-order linear solvers at fixed linearization points, LSR provides a numerically grounded and broadly applicable refinement framework for supervised learning, operator learning, and scientific computing.", "link": "http://arxiv.org/abs/2601.13989v1", "date": "2026-01-20", "relevancy": 2.5365, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5118}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5065}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5036}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20universal%20linearized%20subspace%20refinement%20framework%20for%20neural%20networks&body=Title%3A%20A%20universal%20linearized%20subspace%20refinement%20framework%20for%20neural%20networks%0AAuthor%3A%20Wenbo%20Cao%20and%20Weiwei%20Zhang%0AAbstract%3A%20Neural%20networks%20are%20predominantly%20trained%20using%20gradient-based%20methods%2C%20yet%20in%20many%20applications%20their%20final%20predictions%20remain%20far%20from%20the%20accuracy%20attainable%20within%20the%20model%27s%20expressive%20capacity.%20We%20introduce%20Linearized%20Subspace%20Refinement%20%28LSR%29%2C%20a%20general%20and%20architecture-agnostic%20framework%20that%20exploits%20the%20Jacobian-induced%20linear%20residual%20model%20at%20a%20fixed%20trained%20network%20state.%20By%20solving%20a%20reduced%20direct%20least-squares%20problem%20within%20this%20subspace%2C%20LSR%20computes%20a%20subspace-optimal%20solution%20of%20the%20linearized%20residual%20model%2C%20yielding%20a%20refined%20linear%20predictor%20with%20substantially%20improved%20accuracy%20over%20standard%20gradient-trained%20solutions%2C%20without%20modifying%20network%20architectures%2C%20loss%20formulations%2C%20or%20training%20procedures.%20Across%20supervised%20function%20approximation%2C%20data-driven%20operator%20learning%2C%20and%20physics-informed%20operator%20fine-tuning%2C%20we%20show%20that%20gradient-based%20training%20often%20fails%20to%20access%20this%20attainable%20accuracy%2C%20even%20when%20local%20linearization%20yields%20a%20convex%20problem.%20This%20observation%20indicates%20that%20loss-induced%20numerical%20ill-conditioning%2C%20rather%20than%20nonconvexity%20or%20model%20expressivity%2C%20can%20constitute%20a%20dominant%20practical%20bottleneck.%20In%20contrast%2C%20one-shot%20LSR%20systematically%20exposes%20accuracy%20levels%20not%20fully%20exploited%20by%20gradient-based%20training%2C%20frequently%20achieving%20order-of-magnitude%20error%20reductions.%20For%20operator-constrained%20problems%20with%20composite%20loss%20structures%2C%20we%20further%20introduce%20Iterative%20LSR%2C%20which%20alternates%20one-shot%20LSR%20with%20supervised%20nonlinear%20alignment%2C%20transforming%20ill-conditioned%20residual%20minimization%20into%20numerically%20benign%20fitting%20steps%20and%20yielding%20accelerated%20convergence%20and%20improved%20accuracy.%20By%20bridging%20nonlinear%20neural%20representations%20with%20reduced-order%20linear%20solvers%20at%20fixed%20linearization%20points%2C%20LSR%20provides%20a%20numerically%20grounded%20and%20broadly%20applicable%20refinement%20framework%20for%20supervised%20learning%2C%20operator%20learning%2C%20and%20scientific%20computing.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13989v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520universal%2520linearized%2520subspace%2520refinement%2520framework%2520for%2520neural%2520networks%26entry.906535625%3DWenbo%2520Cao%2520and%2520Weiwei%2520Zhang%26entry.1292438233%3DNeural%2520networks%2520are%2520predominantly%2520trained%2520using%2520gradient-based%2520methods%252C%2520yet%2520in%2520many%2520applications%2520their%2520final%2520predictions%2520remain%2520far%2520from%2520the%2520accuracy%2520attainable%2520within%2520the%2520model%2527s%2520expressive%2520capacity.%2520We%2520introduce%2520Linearized%2520Subspace%2520Refinement%2520%2528LSR%2529%252C%2520a%2520general%2520and%2520architecture-agnostic%2520framework%2520that%2520exploits%2520the%2520Jacobian-induced%2520linear%2520residual%2520model%2520at%2520a%2520fixed%2520trained%2520network%2520state.%2520By%2520solving%2520a%2520reduced%2520direct%2520least-squares%2520problem%2520within%2520this%2520subspace%252C%2520LSR%2520computes%2520a%2520subspace-optimal%2520solution%2520of%2520the%2520linearized%2520residual%2520model%252C%2520yielding%2520a%2520refined%2520linear%2520predictor%2520with%2520substantially%2520improved%2520accuracy%2520over%2520standard%2520gradient-trained%2520solutions%252C%2520without%2520modifying%2520network%2520architectures%252C%2520loss%2520formulations%252C%2520or%2520training%2520procedures.%2520Across%2520supervised%2520function%2520approximation%252C%2520data-driven%2520operator%2520learning%252C%2520and%2520physics-informed%2520operator%2520fine-tuning%252C%2520we%2520show%2520that%2520gradient-based%2520training%2520often%2520fails%2520to%2520access%2520this%2520attainable%2520accuracy%252C%2520even%2520when%2520local%2520linearization%2520yields%2520a%2520convex%2520problem.%2520This%2520observation%2520indicates%2520that%2520loss-induced%2520numerical%2520ill-conditioning%252C%2520rather%2520than%2520nonconvexity%2520or%2520model%2520expressivity%252C%2520can%2520constitute%2520a%2520dominant%2520practical%2520bottleneck.%2520In%2520contrast%252C%2520one-shot%2520LSR%2520systematically%2520exposes%2520accuracy%2520levels%2520not%2520fully%2520exploited%2520by%2520gradient-based%2520training%252C%2520frequently%2520achieving%2520order-of-magnitude%2520error%2520reductions.%2520For%2520operator-constrained%2520problems%2520with%2520composite%2520loss%2520structures%252C%2520we%2520further%2520introduce%2520Iterative%2520LSR%252C%2520which%2520alternates%2520one-shot%2520LSR%2520with%2520supervised%2520nonlinear%2520alignment%252C%2520transforming%2520ill-conditioned%2520residual%2520minimization%2520into%2520numerically%2520benign%2520fitting%2520steps%2520and%2520yielding%2520accelerated%2520convergence%2520and%2520improved%2520accuracy.%2520By%2520bridging%2520nonlinear%2520neural%2520representations%2520with%2520reduced-order%2520linear%2520solvers%2520at%2520fixed%2520linearization%2520points%252C%2520LSR%2520provides%2520a%2520numerically%2520grounded%2520and%2520broadly%2520applicable%2520refinement%2520framework%2520for%2520supervised%2520learning%252C%2520operator%2520learning%252C%2520and%2520scientific%2520computing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13989v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20universal%20linearized%20subspace%20refinement%20framework%20for%20neural%20networks&entry.906535625=Wenbo%20Cao%20and%20Weiwei%20Zhang&entry.1292438233=Neural%20networks%20are%20predominantly%20trained%20using%20gradient-based%20methods%2C%20yet%20in%20many%20applications%20their%20final%20predictions%20remain%20far%20from%20the%20accuracy%20attainable%20within%20the%20model%27s%20expressive%20capacity.%20We%20introduce%20Linearized%20Subspace%20Refinement%20%28LSR%29%2C%20a%20general%20and%20architecture-agnostic%20framework%20that%20exploits%20the%20Jacobian-induced%20linear%20residual%20model%20at%20a%20fixed%20trained%20network%20state.%20By%20solving%20a%20reduced%20direct%20least-squares%20problem%20within%20this%20subspace%2C%20LSR%20computes%20a%20subspace-optimal%20solution%20of%20the%20linearized%20residual%20model%2C%20yielding%20a%20refined%20linear%20predictor%20with%20substantially%20improved%20accuracy%20over%20standard%20gradient-trained%20solutions%2C%20without%20modifying%20network%20architectures%2C%20loss%20formulations%2C%20or%20training%20procedures.%20Across%20supervised%20function%20approximation%2C%20data-driven%20operator%20learning%2C%20and%20physics-informed%20operator%20fine-tuning%2C%20we%20show%20that%20gradient-based%20training%20often%20fails%20to%20access%20this%20attainable%20accuracy%2C%20even%20when%20local%20linearization%20yields%20a%20convex%20problem.%20This%20observation%20indicates%20that%20loss-induced%20numerical%20ill-conditioning%2C%20rather%20than%20nonconvexity%20or%20model%20expressivity%2C%20can%20constitute%20a%20dominant%20practical%20bottleneck.%20In%20contrast%2C%20one-shot%20LSR%20systematically%20exposes%20accuracy%20levels%20not%20fully%20exploited%20by%20gradient-based%20training%2C%20frequently%20achieving%20order-of-magnitude%20error%20reductions.%20For%20operator-constrained%20problems%20with%20composite%20loss%20structures%2C%20we%20further%20introduce%20Iterative%20LSR%2C%20which%20alternates%20one-shot%20LSR%20with%20supervised%20nonlinear%20alignment%2C%20transforming%20ill-conditioned%20residual%20minimization%20into%20numerically%20benign%20fitting%20steps%20and%20yielding%20accelerated%20convergence%20and%20improved%20accuracy.%20By%20bridging%20nonlinear%20neural%20representations%20with%20reduced-order%20linear%20solvers%20at%20fixed%20linearization%20points%2C%20LSR%20provides%20a%20numerically%20grounded%20and%20broadly%20applicable%20refinement%20framework%20for%20supervised%20learning%2C%20operator%20learning%2C%20and%20scientific%20computing.&entry.1838667208=http%3A//arxiv.org/abs/2601.13989v1&entry.124074799=Read"},
{"title": "SecureSplit: Mitigating Backdoor Attacks in Split Learning", "author": "Zhihao Dou and Dongfei Cui and Weida Wang and Anjun Gao and Yueyang Quan and Mengyao Ma and Viet Vo and Guangdong Bai and Zhuqing Liu and Minghong Fang", "abstract": "Split Learning (SL) offers a framework for collaborative model training that respects data privacy by allowing participants to share the same dataset while maintaining distinct feature sets. However, SL is susceptible to backdoor attacks, in which malicious clients subtly alter their embeddings to insert hidden triggers that compromise the final trained model. To address this vulnerability, we introduce SecureSplit, a defense mechanism tailored to SL. SecureSplit applies a dimensionality transformation strategy to accentuate subtle differences between benign and poisoned embeddings, facilitating their separation. With this enhanced distinction, we develop an adaptive filtering approach that uses a majority-based voting scheme to remove contaminated embeddings while preserving clean ones. Rigorous experiments across four datasets (CIFAR-10, MNIST, CINIC-10, and ImageNette), five backdoor attack scenarios, and seven alternative defenses confirm the effectiveness of SecureSplit under various challenging conditions.", "link": "http://arxiv.org/abs/2601.14054v1", "date": "2026-01-20", "relevancy": 2.4734, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.515}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4867}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4824}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SecureSplit%3A%20Mitigating%20Backdoor%20Attacks%20in%20Split%20Learning&body=Title%3A%20SecureSplit%3A%20Mitigating%20Backdoor%20Attacks%20in%20Split%20Learning%0AAuthor%3A%20Zhihao%20Dou%20and%20Dongfei%20Cui%20and%20Weida%20Wang%20and%20Anjun%20Gao%20and%20Yueyang%20Quan%20and%20Mengyao%20Ma%20and%20Viet%20Vo%20and%20Guangdong%20Bai%20and%20Zhuqing%20Liu%20and%20Minghong%20Fang%0AAbstract%3A%20Split%20Learning%20%28SL%29%20offers%20a%20framework%20for%20collaborative%20model%20training%20that%20respects%20data%20privacy%20by%20allowing%20participants%20to%20share%20the%20same%20dataset%20while%20maintaining%20distinct%20feature%20sets.%20However%2C%20SL%20is%20susceptible%20to%20backdoor%20attacks%2C%20in%20which%20malicious%20clients%20subtly%20alter%20their%20embeddings%20to%20insert%20hidden%20triggers%20that%20compromise%20the%20final%20trained%20model.%20To%20address%20this%20vulnerability%2C%20we%20introduce%20SecureSplit%2C%20a%20defense%20mechanism%20tailored%20to%20SL.%20SecureSplit%20applies%20a%20dimensionality%20transformation%20strategy%20to%20accentuate%20subtle%20differences%20between%20benign%20and%20poisoned%20embeddings%2C%20facilitating%20their%20separation.%20With%20this%20enhanced%20distinction%2C%20we%20develop%20an%20adaptive%20filtering%20approach%20that%20uses%20a%20majority-based%20voting%20scheme%20to%20remove%20contaminated%20embeddings%20while%20preserving%20clean%20ones.%20Rigorous%20experiments%20across%20four%20datasets%20%28CIFAR-10%2C%20MNIST%2C%20CINIC-10%2C%20and%20ImageNette%29%2C%20five%20backdoor%20attack%20scenarios%2C%20and%20seven%20alternative%20defenses%20confirm%20the%20effectiveness%20of%20SecureSplit%20under%20various%20challenging%20conditions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14054v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSecureSplit%253A%2520Mitigating%2520Backdoor%2520Attacks%2520in%2520Split%2520Learning%26entry.906535625%3DZhihao%2520Dou%2520and%2520Dongfei%2520Cui%2520and%2520Weida%2520Wang%2520and%2520Anjun%2520Gao%2520and%2520Yueyang%2520Quan%2520and%2520Mengyao%2520Ma%2520and%2520Viet%2520Vo%2520and%2520Guangdong%2520Bai%2520and%2520Zhuqing%2520Liu%2520and%2520Minghong%2520Fang%26entry.1292438233%3DSplit%2520Learning%2520%2528SL%2529%2520offers%2520a%2520framework%2520for%2520collaborative%2520model%2520training%2520that%2520respects%2520data%2520privacy%2520by%2520allowing%2520participants%2520to%2520share%2520the%2520same%2520dataset%2520while%2520maintaining%2520distinct%2520feature%2520sets.%2520However%252C%2520SL%2520is%2520susceptible%2520to%2520backdoor%2520attacks%252C%2520in%2520which%2520malicious%2520clients%2520subtly%2520alter%2520their%2520embeddings%2520to%2520insert%2520hidden%2520triggers%2520that%2520compromise%2520the%2520final%2520trained%2520model.%2520To%2520address%2520this%2520vulnerability%252C%2520we%2520introduce%2520SecureSplit%252C%2520a%2520defense%2520mechanism%2520tailored%2520to%2520SL.%2520SecureSplit%2520applies%2520a%2520dimensionality%2520transformation%2520strategy%2520to%2520accentuate%2520subtle%2520differences%2520between%2520benign%2520and%2520poisoned%2520embeddings%252C%2520facilitating%2520their%2520separation.%2520With%2520this%2520enhanced%2520distinction%252C%2520we%2520develop%2520an%2520adaptive%2520filtering%2520approach%2520that%2520uses%2520a%2520majority-based%2520voting%2520scheme%2520to%2520remove%2520contaminated%2520embeddings%2520while%2520preserving%2520clean%2520ones.%2520Rigorous%2520experiments%2520across%2520four%2520datasets%2520%2528CIFAR-10%252C%2520MNIST%252C%2520CINIC-10%252C%2520and%2520ImageNette%2529%252C%2520five%2520backdoor%2520attack%2520scenarios%252C%2520and%2520seven%2520alternative%2520defenses%2520confirm%2520the%2520effectiveness%2520of%2520SecureSplit%2520under%2520various%2520challenging%2520conditions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14054v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SecureSplit%3A%20Mitigating%20Backdoor%20Attacks%20in%20Split%20Learning&entry.906535625=Zhihao%20Dou%20and%20Dongfei%20Cui%20and%20Weida%20Wang%20and%20Anjun%20Gao%20and%20Yueyang%20Quan%20and%20Mengyao%20Ma%20and%20Viet%20Vo%20and%20Guangdong%20Bai%20and%20Zhuqing%20Liu%20and%20Minghong%20Fang&entry.1292438233=Split%20Learning%20%28SL%29%20offers%20a%20framework%20for%20collaborative%20model%20training%20that%20respects%20data%20privacy%20by%20allowing%20participants%20to%20share%20the%20same%20dataset%20while%20maintaining%20distinct%20feature%20sets.%20However%2C%20SL%20is%20susceptible%20to%20backdoor%20attacks%2C%20in%20which%20malicious%20clients%20subtly%20alter%20their%20embeddings%20to%20insert%20hidden%20triggers%20that%20compromise%20the%20final%20trained%20model.%20To%20address%20this%20vulnerability%2C%20we%20introduce%20SecureSplit%2C%20a%20defense%20mechanism%20tailored%20to%20SL.%20SecureSplit%20applies%20a%20dimensionality%20transformation%20strategy%20to%20accentuate%20subtle%20differences%20between%20benign%20and%20poisoned%20embeddings%2C%20facilitating%20their%20separation.%20With%20this%20enhanced%20distinction%2C%20we%20develop%20an%20adaptive%20filtering%20approach%20that%20uses%20a%20majority-based%20voting%20scheme%20to%20remove%20contaminated%20embeddings%20while%20preserving%20clean%20ones.%20Rigorous%20experiments%20across%20four%20datasets%20%28CIFAR-10%2C%20MNIST%2C%20CINIC-10%2C%20and%20ImageNette%29%2C%20five%20backdoor%20attack%20scenarios%2C%20and%20seven%20alternative%20defenses%20confirm%20the%20effectiveness%20of%20SecureSplit%20under%20various%20challenging%20conditions.&entry.1838667208=http%3A//arxiv.org/abs/2601.14054v1&entry.124074799=Read"},
{"title": "Asymmetric regularization mechanism for GAN training with Variational Inequalities", "author": "Spyridon C. Giagtzoglou and Mark H. M. Winands and Barbara Franci", "abstract": "We formulate the training of generative adversarial networks (GANs) as a Nash equilibrium seeking problem. To stabilize the training process and find a Nash equilibrium, we propose an asymmetric regularization mechanism based on the classic Tikhonov step and on a novel zero-centered gradient penalty. Under smoothness and a local identifiability condition induced by a Gauss-Newton Gramian, we obtain explicit Lipschitz and (strong)-monotonicity constants for the regularized operator. These constants ensure last-iterate linear convergence of a single-call Extrapolation-from-the-Past (EFTP) method. Empirical simulations on an academic example show that, even when strong monotonicity cannot be achieved, the asymmetric regularization is enough to converge to an equilibrium and stabilize the trajectory.", "link": "http://arxiv.org/abs/2601.13920v1", "date": "2026-01-20", "relevancy": 2.4658, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4995}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4922}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Asymmetric%20regularization%20mechanism%20for%20GAN%20training%20with%20Variational%20Inequalities&body=Title%3A%20Asymmetric%20regularization%20mechanism%20for%20GAN%20training%20with%20Variational%20Inequalities%0AAuthor%3A%20Spyridon%20C.%20Giagtzoglou%20and%20Mark%20H.%20M.%20Winands%20and%20Barbara%20Franci%0AAbstract%3A%20We%20formulate%20the%20training%20of%20generative%20adversarial%20networks%20%28GANs%29%20as%20a%20Nash%20equilibrium%20seeking%20problem.%20To%20stabilize%20the%20training%20process%20and%20find%20a%20Nash%20equilibrium%2C%20we%20propose%20an%20asymmetric%20regularization%20mechanism%20based%20on%20the%20classic%20Tikhonov%20step%20and%20on%20a%20novel%20zero-centered%20gradient%20penalty.%20Under%20smoothness%20and%20a%20local%20identifiability%20condition%20induced%20by%20a%20Gauss-Newton%20Gramian%2C%20we%20obtain%20explicit%20Lipschitz%20and%20%28strong%29-monotonicity%20constants%20for%20the%20regularized%20operator.%20These%20constants%20ensure%20last-iterate%20linear%20convergence%20of%20a%20single-call%20Extrapolation-from-the-Past%20%28EFTP%29%20method.%20Empirical%20simulations%20on%20an%20academic%20example%20show%20that%2C%20even%20when%20strong%20monotonicity%20cannot%20be%20achieved%2C%20the%20asymmetric%20regularization%20is%20enough%20to%20converge%20to%20an%20equilibrium%20and%20stabilize%20the%20trajectory.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAsymmetric%2520regularization%2520mechanism%2520for%2520GAN%2520training%2520with%2520Variational%2520Inequalities%26entry.906535625%3DSpyridon%2520C.%2520Giagtzoglou%2520and%2520Mark%2520H.%2520M.%2520Winands%2520and%2520Barbara%2520Franci%26entry.1292438233%3DWe%2520formulate%2520the%2520training%2520of%2520generative%2520adversarial%2520networks%2520%2528GANs%2529%2520as%2520a%2520Nash%2520equilibrium%2520seeking%2520problem.%2520To%2520stabilize%2520the%2520training%2520process%2520and%2520find%2520a%2520Nash%2520equilibrium%252C%2520we%2520propose%2520an%2520asymmetric%2520regularization%2520mechanism%2520based%2520on%2520the%2520classic%2520Tikhonov%2520step%2520and%2520on%2520a%2520novel%2520zero-centered%2520gradient%2520penalty.%2520Under%2520smoothness%2520and%2520a%2520local%2520identifiability%2520condition%2520induced%2520by%2520a%2520Gauss-Newton%2520Gramian%252C%2520we%2520obtain%2520explicit%2520Lipschitz%2520and%2520%2528strong%2529-monotonicity%2520constants%2520for%2520the%2520regularized%2520operator.%2520These%2520constants%2520ensure%2520last-iterate%2520linear%2520convergence%2520of%2520a%2520single-call%2520Extrapolation-from-the-Past%2520%2528EFTP%2529%2520method.%2520Empirical%2520simulations%2520on%2520an%2520academic%2520example%2520show%2520that%252C%2520even%2520when%2520strong%2520monotonicity%2520cannot%2520be%2520achieved%252C%2520the%2520asymmetric%2520regularization%2520is%2520enough%2520to%2520converge%2520to%2520an%2520equilibrium%2520and%2520stabilize%2520the%2520trajectory.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Asymmetric%20regularization%20mechanism%20for%20GAN%20training%20with%20Variational%20Inequalities&entry.906535625=Spyridon%20C.%20Giagtzoglou%20and%20Mark%20H.%20M.%20Winands%20and%20Barbara%20Franci&entry.1292438233=We%20formulate%20the%20training%20of%20generative%20adversarial%20networks%20%28GANs%29%20as%20a%20Nash%20equilibrium%20seeking%20problem.%20To%20stabilize%20the%20training%20process%20and%20find%20a%20Nash%20equilibrium%2C%20we%20propose%20an%20asymmetric%20regularization%20mechanism%20based%20on%20the%20classic%20Tikhonov%20step%20and%20on%20a%20novel%20zero-centered%20gradient%20penalty.%20Under%20smoothness%20and%20a%20local%20identifiability%20condition%20induced%20by%20a%20Gauss-Newton%20Gramian%2C%20we%20obtain%20explicit%20Lipschitz%20and%20%28strong%29-monotonicity%20constants%20for%20the%20regularized%20operator.%20These%20constants%20ensure%20last-iterate%20linear%20convergence%20of%20a%20single-call%20Extrapolation-from-the-Past%20%28EFTP%29%20method.%20Empirical%20simulations%20on%20an%20academic%20example%20show%20that%2C%20even%20when%20strong%20monotonicity%20cannot%20be%20achieved%2C%20the%20asymmetric%20regularization%20is%20enough%20to%20converge%20to%20an%20equilibrium%20and%20stabilize%20the%20trajectory.&entry.1838667208=http%3A//arxiv.org/abs/2601.13920v1&entry.124074799=Read"},
{"title": "VideoMaMa: Mask-Guided Video Matting via Generative Prior", "author": "Sangbeom Lim and Seoung Wug Oh and Jiahui Huang and Heeji Yoon and Seungryong Kim and Joon-Young Lee", "abstract": "Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.", "link": "http://arxiv.org/abs/2601.14255v1", "date": "2026-01-20", "relevancy": 2.4633, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6291}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6128}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5903}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VideoMaMa%3A%20Mask-Guided%20Video%20Matting%20via%20Generative%20Prior&body=Title%3A%20VideoMaMa%3A%20Mask-Guided%20Video%20Matting%20via%20Generative%20Prior%0AAuthor%3A%20Sangbeom%20Lim%20and%20Seoung%20Wug%20Oh%20and%20Jiahui%20Huang%20and%20Heeji%20Yoon%20and%20Seungryong%20Kim%20and%20Joon-Young%20Lee%0AAbstract%3A%20Generalizing%20video%20matting%20models%20to%20real-world%20videos%20remains%20a%20significant%20challenge%20due%20to%20the%20scarcity%20of%20labeled%20data.%20To%20address%20this%2C%20we%20present%20Video%20Mask-to-Matte%20Model%20%28VideoMaMa%29%20that%20converts%20coarse%20segmentation%20masks%20into%20pixel%20accurate%20alpha%20mattes%2C%20by%20leveraging%20pretrained%20video%20diffusion%20models.%20VideoMaMa%20demonstrates%20strong%20zero-shot%20generalization%20to%20real-world%20footage%2C%20even%20though%20it%20is%20trained%20solely%20on%20synthetic%20data.%20Building%20on%20this%20capability%2C%20we%20develop%20a%20scalable%20pseudo-labeling%20pipeline%20for%20large-scale%20video%20matting%20and%20construct%20the%20Matting%20Anything%20in%20Video%20%28MA-V%29%20dataset%2C%20which%20offers%20high-quality%20matting%20annotations%20for%20more%20than%2050K%20real-world%20videos%20spanning%20diverse%20scenes%20and%20motions.%20To%20validate%20the%20effectiveness%20of%20this%20dataset%2C%20we%20fine-tune%20the%20SAM2%20model%20on%20MA-V%20to%20obtain%20SAM2-Matte%2C%20which%20outperforms%20the%20same%20model%20trained%20on%20existing%20matting%20datasets%20in%20terms%20of%20robustness%20on%20in-the-wild%20videos.%20These%20findings%20emphasize%20the%20importance%20of%20large-scale%20pseudo-labeled%20video%20matting%20and%20showcase%20how%20generative%20priors%20and%20accessible%20segmentation%20cues%20can%20drive%20scalable%20progress%20in%20video%20matting%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14255v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVideoMaMa%253A%2520Mask-Guided%2520Video%2520Matting%2520via%2520Generative%2520Prior%26entry.906535625%3DSangbeom%2520Lim%2520and%2520Seoung%2520Wug%2520Oh%2520and%2520Jiahui%2520Huang%2520and%2520Heeji%2520Yoon%2520and%2520Seungryong%2520Kim%2520and%2520Joon-Young%2520Lee%26entry.1292438233%3DGeneralizing%2520video%2520matting%2520models%2520to%2520real-world%2520videos%2520remains%2520a%2520significant%2520challenge%2520due%2520to%2520the%2520scarcity%2520of%2520labeled%2520data.%2520To%2520address%2520this%252C%2520we%2520present%2520Video%2520Mask-to-Matte%2520Model%2520%2528VideoMaMa%2529%2520that%2520converts%2520coarse%2520segmentation%2520masks%2520into%2520pixel%2520accurate%2520alpha%2520mattes%252C%2520by%2520leveraging%2520pretrained%2520video%2520diffusion%2520models.%2520VideoMaMa%2520demonstrates%2520strong%2520zero-shot%2520generalization%2520to%2520real-world%2520footage%252C%2520even%2520though%2520it%2520is%2520trained%2520solely%2520on%2520synthetic%2520data.%2520Building%2520on%2520this%2520capability%252C%2520we%2520develop%2520a%2520scalable%2520pseudo-labeling%2520pipeline%2520for%2520large-scale%2520video%2520matting%2520and%2520construct%2520the%2520Matting%2520Anything%2520in%2520Video%2520%2528MA-V%2529%2520dataset%252C%2520which%2520offers%2520high-quality%2520matting%2520annotations%2520for%2520more%2520than%252050K%2520real-world%2520videos%2520spanning%2520diverse%2520scenes%2520and%2520motions.%2520To%2520validate%2520the%2520effectiveness%2520of%2520this%2520dataset%252C%2520we%2520fine-tune%2520the%2520SAM2%2520model%2520on%2520MA-V%2520to%2520obtain%2520SAM2-Matte%252C%2520which%2520outperforms%2520the%2520same%2520model%2520trained%2520on%2520existing%2520matting%2520datasets%2520in%2520terms%2520of%2520robustness%2520on%2520in-the-wild%2520videos.%2520These%2520findings%2520emphasize%2520the%2520importance%2520of%2520large-scale%2520pseudo-labeled%2520video%2520matting%2520and%2520showcase%2520how%2520generative%2520priors%2520and%2520accessible%2520segmentation%2520cues%2520can%2520drive%2520scalable%2520progress%2520in%2520video%2520matting%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14255v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VideoMaMa%3A%20Mask-Guided%20Video%20Matting%20via%20Generative%20Prior&entry.906535625=Sangbeom%20Lim%20and%20Seoung%20Wug%20Oh%20and%20Jiahui%20Huang%20and%20Heeji%20Yoon%20and%20Seungryong%20Kim%20and%20Joon-Young%20Lee&entry.1292438233=Generalizing%20video%20matting%20models%20to%20real-world%20videos%20remains%20a%20significant%20challenge%20due%20to%20the%20scarcity%20of%20labeled%20data.%20To%20address%20this%2C%20we%20present%20Video%20Mask-to-Matte%20Model%20%28VideoMaMa%29%20that%20converts%20coarse%20segmentation%20masks%20into%20pixel%20accurate%20alpha%20mattes%2C%20by%20leveraging%20pretrained%20video%20diffusion%20models.%20VideoMaMa%20demonstrates%20strong%20zero-shot%20generalization%20to%20real-world%20footage%2C%20even%20though%20it%20is%20trained%20solely%20on%20synthetic%20data.%20Building%20on%20this%20capability%2C%20we%20develop%20a%20scalable%20pseudo-labeling%20pipeline%20for%20large-scale%20video%20matting%20and%20construct%20the%20Matting%20Anything%20in%20Video%20%28MA-V%29%20dataset%2C%20which%20offers%20high-quality%20matting%20annotations%20for%20more%20than%2050K%20real-world%20videos%20spanning%20diverse%20scenes%20and%20motions.%20To%20validate%20the%20effectiveness%20of%20this%20dataset%2C%20we%20fine-tune%20the%20SAM2%20model%20on%20MA-V%20to%20obtain%20SAM2-Matte%2C%20which%20outperforms%20the%20same%20model%20trained%20on%20existing%20matting%20datasets%20in%20terms%20of%20robustness%20on%20in-the-wild%20videos.%20These%20findings%20emphasize%20the%20importance%20of%20large-scale%20pseudo-labeled%20video%20matting%20and%20showcase%20how%20generative%20priors%20and%20accessible%20segmentation%20cues%20can%20drive%20scalable%20progress%20in%20video%20matting%20research.&entry.1838667208=http%3A//arxiv.org/abs/2601.14255v1&entry.124074799=Read"},
{"title": "GeoSurDepth: Harnessing Foundation Model for Spatial Geometry Consistency-Oriented Self-Supervised Surround-View Depth Estimation", "author": "Weimin Liu and Wenjun Wang and Joshua H. Meng", "abstract": "Accurate surround-view depth estimation provides a competitive alternative to laser-based sensors and is essential for 3D scene understanding in autonomous driving. While empirical studies have proposed various approaches that primarily focus on enforcing cross-view constraints at photometric level, few explicitly exploit the rich geometric structure inherent in both monocular and surround-view setting. In this work, we propose GeoSurDepth, a framework that leverages geometry consistency as the primary cue for surround-view depth estimation. Concretely, we utilize vision foundation models as pseudo geometry priors and feature representation enhancement tool to guide the network to maintain surface normal consistency in spatial 3D space and regularize object- and texture-consistent depth estimation in 2D. In addition, we introduce a novel view synthesis pipeline where 2D-3D lifting is achieved with dense depth reconstructed via spatial warping, encouraging additional photometric supervision across temporal and spatial contexts, and compensating for the limitations of target-view image reconstruction. Finally, a newly-proposed adaptive joint motion learning strategy enables the network to adaptively emphasize informative spatial geometry cues for improved motion reasoning. Extensive experiments on KITTI, DDAD and nuScenes demonstrate that GeoSurDepth achieves SoTA performance, validating the effectiveness of our approach. Our framework highlights the importance of exploiting geometry coherence and consistency for robust self-supervised depth estimation.", "link": "http://arxiv.org/abs/2601.05839v2", "date": "2026-01-20", "relevancy": 2.4597, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6186}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6127}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6121}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeoSurDepth%3A%20Harnessing%20Foundation%20Model%20for%20Spatial%20Geometry%20Consistency-Oriented%20Self-Supervised%20Surround-View%20Depth%20Estimation&body=Title%3A%20GeoSurDepth%3A%20Harnessing%20Foundation%20Model%20for%20Spatial%20Geometry%20Consistency-Oriented%20Self-Supervised%20Surround-View%20Depth%20Estimation%0AAuthor%3A%20Weimin%20Liu%20and%20Wenjun%20Wang%20and%20Joshua%20H.%20Meng%0AAbstract%3A%20Accurate%20surround-view%20depth%20estimation%20provides%20a%20competitive%20alternative%20to%20laser-based%20sensors%20and%20is%20essential%20for%203D%20scene%20understanding%20in%20autonomous%20driving.%20While%20empirical%20studies%20have%20proposed%20various%20approaches%20that%20primarily%20focus%20on%20enforcing%20cross-view%20constraints%20at%20photometric%20level%2C%20few%20explicitly%20exploit%20the%20rich%20geometric%20structure%20inherent%20in%20both%20monocular%20and%20surround-view%20setting.%20In%20this%20work%2C%20we%20propose%20GeoSurDepth%2C%20a%20framework%20that%20leverages%20geometry%20consistency%20as%20the%20primary%20cue%20for%20surround-view%20depth%20estimation.%20Concretely%2C%20we%20utilize%20vision%20foundation%20models%20as%20pseudo%20geometry%20priors%20and%20feature%20representation%20enhancement%20tool%20to%20guide%20the%20network%20to%20maintain%20surface%20normal%20consistency%20in%20spatial%203D%20space%20and%20regularize%20object-%20and%20texture-consistent%20depth%20estimation%20in%202D.%20In%20addition%2C%20we%20introduce%20a%20novel%20view%20synthesis%20pipeline%20where%202D-3D%20lifting%20is%20achieved%20with%20dense%20depth%20reconstructed%20via%20spatial%20warping%2C%20encouraging%20additional%20photometric%20supervision%20across%20temporal%20and%20spatial%20contexts%2C%20and%20compensating%20for%20the%20limitations%20of%20target-view%20image%20reconstruction.%20Finally%2C%20a%20newly-proposed%20adaptive%20joint%20motion%20learning%20strategy%20enables%20the%20network%20to%20adaptively%20emphasize%20informative%20spatial%20geometry%20cues%20for%20improved%20motion%20reasoning.%20Extensive%20experiments%20on%20KITTI%2C%20DDAD%20and%20nuScenes%20demonstrate%20that%20GeoSurDepth%20achieves%20SoTA%20performance%2C%20validating%20the%20effectiveness%20of%20our%20approach.%20Our%20framework%20highlights%20the%20importance%20of%20exploiting%20geometry%20coherence%20and%20consistency%20for%20robust%20self-supervised%20depth%20estimation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.05839v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeoSurDepth%253A%2520Harnessing%2520Foundation%2520Model%2520for%2520Spatial%2520Geometry%2520Consistency-Oriented%2520Self-Supervised%2520Surround-View%2520Depth%2520Estimation%26entry.906535625%3DWeimin%2520Liu%2520and%2520Wenjun%2520Wang%2520and%2520Joshua%2520H.%2520Meng%26entry.1292438233%3DAccurate%2520surround-view%2520depth%2520estimation%2520provides%2520a%2520competitive%2520alternative%2520to%2520laser-based%2520sensors%2520and%2520is%2520essential%2520for%25203D%2520scene%2520understanding%2520in%2520autonomous%2520driving.%2520While%2520empirical%2520studies%2520have%2520proposed%2520various%2520approaches%2520that%2520primarily%2520focus%2520on%2520enforcing%2520cross-view%2520constraints%2520at%2520photometric%2520level%252C%2520few%2520explicitly%2520exploit%2520the%2520rich%2520geometric%2520structure%2520inherent%2520in%2520both%2520monocular%2520and%2520surround-view%2520setting.%2520In%2520this%2520work%252C%2520we%2520propose%2520GeoSurDepth%252C%2520a%2520framework%2520that%2520leverages%2520geometry%2520consistency%2520as%2520the%2520primary%2520cue%2520for%2520surround-view%2520depth%2520estimation.%2520Concretely%252C%2520we%2520utilize%2520vision%2520foundation%2520models%2520as%2520pseudo%2520geometry%2520priors%2520and%2520feature%2520representation%2520enhancement%2520tool%2520to%2520guide%2520the%2520network%2520to%2520maintain%2520surface%2520normal%2520consistency%2520in%2520spatial%25203D%2520space%2520and%2520regularize%2520object-%2520and%2520texture-consistent%2520depth%2520estimation%2520in%25202D.%2520In%2520addition%252C%2520we%2520introduce%2520a%2520novel%2520view%2520synthesis%2520pipeline%2520where%25202D-3D%2520lifting%2520is%2520achieved%2520with%2520dense%2520depth%2520reconstructed%2520via%2520spatial%2520warping%252C%2520encouraging%2520additional%2520photometric%2520supervision%2520across%2520temporal%2520and%2520spatial%2520contexts%252C%2520and%2520compensating%2520for%2520the%2520limitations%2520of%2520target-view%2520image%2520reconstruction.%2520Finally%252C%2520a%2520newly-proposed%2520adaptive%2520joint%2520motion%2520learning%2520strategy%2520enables%2520the%2520network%2520to%2520adaptively%2520emphasize%2520informative%2520spatial%2520geometry%2520cues%2520for%2520improved%2520motion%2520reasoning.%2520Extensive%2520experiments%2520on%2520KITTI%252C%2520DDAD%2520and%2520nuScenes%2520demonstrate%2520that%2520GeoSurDepth%2520achieves%2520SoTA%2520performance%252C%2520validating%2520the%2520effectiveness%2520of%2520our%2520approach.%2520Our%2520framework%2520highlights%2520the%2520importance%2520of%2520exploiting%2520geometry%2520coherence%2520and%2520consistency%2520for%2520robust%2520self-supervised%2520depth%2520estimation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.05839v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeoSurDepth%3A%20Harnessing%20Foundation%20Model%20for%20Spatial%20Geometry%20Consistency-Oriented%20Self-Supervised%20Surround-View%20Depth%20Estimation&entry.906535625=Weimin%20Liu%20and%20Wenjun%20Wang%20and%20Joshua%20H.%20Meng&entry.1292438233=Accurate%20surround-view%20depth%20estimation%20provides%20a%20competitive%20alternative%20to%20laser-based%20sensors%20and%20is%20essential%20for%203D%20scene%20understanding%20in%20autonomous%20driving.%20While%20empirical%20studies%20have%20proposed%20various%20approaches%20that%20primarily%20focus%20on%20enforcing%20cross-view%20constraints%20at%20photometric%20level%2C%20few%20explicitly%20exploit%20the%20rich%20geometric%20structure%20inherent%20in%20both%20monocular%20and%20surround-view%20setting.%20In%20this%20work%2C%20we%20propose%20GeoSurDepth%2C%20a%20framework%20that%20leverages%20geometry%20consistency%20as%20the%20primary%20cue%20for%20surround-view%20depth%20estimation.%20Concretely%2C%20we%20utilize%20vision%20foundation%20models%20as%20pseudo%20geometry%20priors%20and%20feature%20representation%20enhancement%20tool%20to%20guide%20the%20network%20to%20maintain%20surface%20normal%20consistency%20in%20spatial%203D%20space%20and%20regularize%20object-%20and%20texture-consistent%20depth%20estimation%20in%202D.%20In%20addition%2C%20we%20introduce%20a%20novel%20view%20synthesis%20pipeline%20where%202D-3D%20lifting%20is%20achieved%20with%20dense%20depth%20reconstructed%20via%20spatial%20warping%2C%20encouraging%20additional%20photometric%20supervision%20across%20temporal%20and%20spatial%20contexts%2C%20and%20compensating%20for%20the%20limitations%20of%20target-view%20image%20reconstruction.%20Finally%2C%20a%20newly-proposed%20adaptive%20joint%20motion%20learning%20strategy%20enables%20the%20network%20to%20adaptively%20emphasize%20informative%20spatial%20geometry%20cues%20for%20improved%20motion%20reasoning.%20Extensive%20experiments%20on%20KITTI%2C%20DDAD%20and%20nuScenes%20demonstrate%20that%20GeoSurDepth%20achieves%20SoTA%20performance%2C%20validating%20the%20effectiveness%20of%20our%20approach.%20Our%20framework%20highlights%20the%20importance%20of%20exploiting%20geometry%20coherence%20and%20consistency%20for%20robust%20self-supervised%20depth%20estimation.&entry.1838667208=http%3A//arxiv.org/abs/2601.05839v2&entry.124074799=Read"},
{"title": "Structuring Reasoning for Complex Rules Beyond Flat Representations", "author": "Zhihao Yang and Ancheng Xu and Jingpeng Li and Liang Yan and Jiehui Zhou and Zhen Qin and Hengyu Chang and Yukun Chen and Longze Chen and Ahmadreza Argha and Hamid Alinejad-Rokny and Minghuan Tan and Yujun Cai and Min Yang", "abstract": "Large language models (LLMs) face significant challenges when processing complex rule systems, as they typically treat interdependent rules as unstructured textual data rather than as logically organized frameworks. This limitation results in reasoning divergence, where models often overlook critical rule dependencies essential for accurate interpretation. Although existing approaches such as Chain-of-Thought (CoT) reasoning have shown promise, they lack systematic methodologies for structured rule processing and are particularly susceptible to error propagation through sequential reasoning chains. To address these limitations, we propose the Dynamic Adjudication Template (DAT), a novel framework inspired by expert human reasoning processes. DAT structures the inference mechanism into three methodical stages: qualitative analysis, evidence gathering, and adjudication. During the qualitative analysis phase, the model comprehensively evaluates the contextual landscape. The subsequent evidence gathering phase involves the targeted extraction of pertinent information based on predefined template elements ([placeholder]), followed by systematic verification against applicable rules. Finally, in the adjudication phase, the model synthesizes these validated components to formulate a comprehensive judgment. Empirical results demonstrate that DAT consistently outperforms conventional CoT approaches in complex rule-based tasks. Notably, DAT enables smaller language models to match, and in some cases exceed, the performance of significantly larger LLMs, highlighting its efficiency and effectiveness in managing intricate rule systems.", "link": "http://arxiv.org/abs/2510.05134v2", "date": "2026-01-20", "relevancy": 2.4584, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.498}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.479}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Structuring%20Reasoning%20for%20Complex%20Rules%20Beyond%20Flat%20Representations&body=Title%3A%20Structuring%20Reasoning%20for%20Complex%20Rules%20Beyond%20Flat%20Representations%0AAuthor%3A%20Zhihao%20Yang%20and%20Ancheng%20Xu%20and%20Jingpeng%20Li%20and%20Liang%20Yan%20and%20Jiehui%20Zhou%20and%20Zhen%20Qin%20and%20Hengyu%20Chang%20and%20Yukun%20Chen%20and%20Longze%20Chen%20and%20Ahmadreza%20Argha%20and%20Hamid%20Alinejad-Rokny%20and%20Minghuan%20Tan%20and%20Yujun%20Cai%20and%20Min%20Yang%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20face%20significant%20challenges%20when%20processing%20complex%20rule%20systems%2C%20as%20they%20typically%20treat%20interdependent%20rules%20as%20unstructured%20textual%20data%20rather%20than%20as%20logically%20organized%20frameworks.%20This%20limitation%20results%20in%20reasoning%20divergence%2C%20where%20models%20often%20overlook%20critical%20rule%20dependencies%20essential%20for%20accurate%20interpretation.%20Although%20existing%20approaches%20such%20as%20Chain-of-Thought%20%28CoT%29%20reasoning%20have%20shown%20promise%2C%20they%20lack%20systematic%20methodologies%20for%20structured%20rule%20processing%20and%20are%20particularly%20susceptible%20to%20error%20propagation%20through%20sequential%20reasoning%20chains.%20To%20address%20these%20limitations%2C%20we%20propose%20the%20Dynamic%20Adjudication%20Template%20%28DAT%29%2C%20a%20novel%20framework%20inspired%20by%20expert%20human%20reasoning%20processes.%20DAT%20structures%20the%20inference%20mechanism%20into%20three%20methodical%20stages%3A%20qualitative%20analysis%2C%20evidence%20gathering%2C%20and%20adjudication.%20During%20the%20qualitative%20analysis%20phase%2C%20the%20model%20comprehensively%20evaluates%20the%20contextual%20landscape.%20The%20subsequent%20evidence%20gathering%20phase%20involves%20the%20targeted%20extraction%20of%20pertinent%20information%20based%20on%20predefined%20template%20elements%20%28%5Bplaceholder%5D%29%2C%20followed%20by%20systematic%20verification%20against%20applicable%20rules.%20Finally%2C%20in%20the%20adjudication%20phase%2C%20the%20model%20synthesizes%20these%20validated%20components%20to%20formulate%20a%20comprehensive%20judgment.%20Empirical%20results%20demonstrate%20that%20DAT%20consistently%20outperforms%20conventional%20CoT%20approaches%20in%20complex%20rule-based%20tasks.%20Notably%2C%20DAT%20enables%20smaller%20language%20models%20to%20match%2C%20and%20in%20some%20cases%20exceed%2C%20the%20performance%20of%20significantly%20larger%20LLMs%2C%20highlighting%20its%20efficiency%20and%20effectiveness%20in%20managing%20intricate%20rule%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2510.05134v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DStructuring%2520Reasoning%2520for%2520Complex%2520Rules%2520Beyond%2520Flat%2520Representations%26entry.906535625%3DZhihao%2520Yang%2520and%2520Ancheng%2520Xu%2520and%2520Jingpeng%2520Li%2520and%2520Liang%2520Yan%2520and%2520Jiehui%2520Zhou%2520and%2520Zhen%2520Qin%2520and%2520Hengyu%2520Chang%2520and%2520Yukun%2520Chen%2520and%2520Longze%2520Chen%2520and%2520Ahmadreza%2520Argha%2520and%2520Hamid%2520Alinejad-Rokny%2520and%2520Minghuan%2520Tan%2520and%2520Yujun%2520Cai%2520and%2520Min%2520Yang%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520face%2520significant%2520challenges%2520when%2520processing%2520complex%2520rule%2520systems%252C%2520as%2520they%2520typically%2520treat%2520interdependent%2520rules%2520as%2520unstructured%2520textual%2520data%2520rather%2520than%2520as%2520logically%2520organized%2520frameworks.%2520This%2520limitation%2520results%2520in%2520reasoning%2520divergence%252C%2520where%2520models%2520often%2520overlook%2520critical%2520rule%2520dependencies%2520essential%2520for%2520accurate%2520interpretation.%2520Although%2520existing%2520approaches%2520such%2520as%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520have%2520shown%2520promise%252C%2520they%2520lack%2520systematic%2520methodologies%2520for%2520structured%2520rule%2520processing%2520and%2520are%2520particularly%2520susceptible%2520to%2520error%2520propagation%2520through%2520sequential%2520reasoning%2520chains.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520the%2520Dynamic%2520Adjudication%2520Template%2520%2528DAT%2529%252C%2520a%2520novel%2520framework%2520inspired%2520by%2520expert%2520human%2520reasoning%2520processes.%2520DAT%2520structures%2520the%2520inference%2520mechanism%2520into%2520three%2520methodical%2520stages%253A%2520qualitative%2520analysis%252C%2520evidence%2520gathering%252C%2520and%2520adjudication.%2520During%2520the%2520qualitative%2520analysis%2520phase%252C%2520the%2520model%2520comprehensively%2520evaluates%2520the%2520contextual%2520landscape.%2520The%2520subsequent%2520evidence%2520gathering%2520phase%2520involves%2520the%2520targeted%2520extraction%2520of%2520pertinent%2520information%2520based%2520on%2520predefined%2520template%2520elements%2520%2528%255Bplaceholder%255D%2529%252C%2520followed%2520by%2520systematic%2520verification%2520against%2520applicable%2520rules.%2520Finally%252C%2520in%2520the%2520adjudication%2520phase%252C%2520the%2520model%2520synthesizes%2520these%2520validated%2520components%2520to%2520formulate%2520a%2520comprehensive%2520judgment.%2520Empirical%2520results%2520demonstrate%2520that%2520DAT%2520consistently%2520outperforms%2520conventional%2520CoT%2520approaches%2520in%2520complex%2520rule-based%2520tasks.%2520Notably%252C%2520DAT%2520enables%2520smaller%2520language%2520models%2520to%2520match%252C%2520and%2520in%2520some%2520cases%2520exceed%252C%2520the%2520performance%2520of%2520significantly%2520larger%2520LLMs%252C%2520highlighting%2520its%2520efficiency%2520and%2520effectiveness%2520in%2520managing%2520intricate%2520rule%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.05134v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Structuring%20Reasoning%20for%20Complex%20Rules%20Beyond%20Flat%20Representations&entry.906535625=Zhihao%20Yang%20and%20Ancheng%20Xu%20and%20Jingpeng%20Li%20and%20Liang%20Yan%20and%20Jiehui%20Zhou%20and%20Zhen%20Qin%20and%20Hengyu%20Chang%20and%20Yukun%20Chen%20and%20Longze%20Chen%20and%20Ahmadreza%20Argha%20and%20Hamid%20Alinejad-Rokny%20and%20Minghuan%20Tan%20and%20Yujun%20Cai%20and%20Min%20Yang&entry.1292438233=Large%20language%20models%20%28LLMs%29%20face%20significant%20challenges%20when%20processing%20complex%20rule%20systems%2C%20as%20they%20typically%20treat%20interdependent%20rules%20as%20unstructured%20textual%20data%20rather%20than%20as%20logically%20organized%20frameworks.%20This%20limitation%20results%20in%20reasoning%20divergence%2C%20where%20models%20often%20overlook%20critical%20rule%20dependencies%20essential%20for%20accurate%20interpretation.%20Although%20existing%20approaches%20such%20as%20Chain-of-Thought%20%28CoT%29%20reasoning%20have%20shown%20promise%2C%20they%20lack%20systematic%20methodologies%20for%20structured%20rule%20processing%20and%20are%20particularly%20susceptible%20to%20error%20propagation%20through%20sequential%20reasoning%20chains.%20To%20address%20these%20limitations%2C%20we%20propose%20the%20Dynamic%20Adjudication%20Template%20%28DAT%29%2C%20a%20novel%20framework%20inspired%20by%20expert%20human%20reasoning%20processes.%20DAT%20structures%20the%20inference%20mechanism%20into%20three%20methodical%20stages%3A%20qualitative%20analysis%2C%20evidence%20gathering%2C%20and%20adjudication.%20During%20the%20qualitative%20analysis%20phase%2C%20the%20model%20comprehensively%20evaluates%20the%20contextual%20landscape.%20The%20subsequent%20evidence%20gathering%20phase%20involves%20the%20targeted%20extraction%20of%20pertinent%20information%20based%20on%20predefined%20template%20elements%20%28%5Bplaceholder%5D%29%2C%20followed%20by%20systematic%20verification%20against%20applicable%20rules.%20Finally%2C%20in%20the%20adjudication%20phase%2C%20the%20model%20synthesizes%20these%20validated%20components%20to%20formulate%20a%20comprehensive%20judgment.%20Empirical%20results%20demonstrate%20that%20DAT%20consistently%20outperforms%20conventional%20CoT%20approaches%20in%20complex%20rule-based%20tasks.%20Notably%2C%20DAT%20enables%20smaller%20language%20models%20to%20match%2C%20and%20in%20some%20cases%20exceed%2C%20the%20performance%20of%20significantly%20larger%20LLMs%2C%20highlighting%20its%20efficiency%20and%20effectiveness%20in%20managing%20intricate%20rule%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2510.05134v2&entry.124074799=Read"},
{"title": "TrackletGPT: A Language-like GPT Framework for White Matter Tract Segmentation", "author": "Anoushkrit Goel and Simroop Singh and Ankita Joshi and Ranjeet Ranjan Jha and Chirag Ahuja and Aditya Nigam and Arnav Bhavsar", "abstract": "White Matter Tract Segmentation is imperative for studying brain structural connectivity, neurological disorders and neurosurgery. This task remains complex, as tracts differ among themselves, across subjects and conditions, yet have similar 3D structure across hemispheres and subjects. To address these challenges, we propose TrackletGPT, a language-like GPT framework which reintroduces sequential information in tokens using tracklets. TrackletGPT generalises seamlessly across datasets, is fully automatic, and encodes granular sub-streamline segments, Tracklets, scaling and refining GPT models in Tractography Segmentation. Based on our experiments, TrackletGPT outperforms state-of-the-art methods on average DICE, Overlap and Overreach scores on TractoInferno and HCP datasets, even on inter-dataset experiments.", "link": "http://arxiv.org/abs/2601.13935v1", "date": "2026-01-20", "relevancy": 2.4483, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4936}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4913}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TrackletGPT%3A%20A%20Language-like%20GPT%20Framework%20for%20White%20Matter%20Tract%20Segmentation&body=Title%3A%20TrackletGPT%3A%20A%20Language-like%20GPT%20Framework%20for%20White%20Matter%20Tract%20Segmentation%0AAuthor%3A%20Anoushkrit%20Goel%20and%20Simroop%20Singh%20and%20Ankita%20Joshi%20and%20Ranjeet%20Ranjan%20Jha%20and%20Chirag%20Ahuja%20and%20Aditya%20Nigam%20and%20Arnav%20Bhavsar%0AAbstract%3A%20White%20Matter%20Tract%20Segmentation%20is%20imperative%20for%20studying%20brain%20structural%20connectivity%2C%20neurological%20disorders%20and%20neurosurgery.%20This%20task%20remains%20complex%2C%20as%20tracts%20differ%20among%20themselves%2C%20across%20subjects%20and%20conditions%2C%20yet%20have%20similar%203D%20structure%20across%20hemispheres%20and%20subjects.%20To%20address%20these%20challenges%2C%20we%20propose%20TrackletGPT%2C%20a%20language-like%20GPT%20framework%20which%20reintroduces%20sequential%20information%20in%20tokens%20using%20tracklets.%20TrackletGPT%20generalises%20seamlessly%20across%20datasets%2C%20is%20fully%20automatic%2C%20and%20encodes%20granular%20sub-streamline%20segments%2C%20Tracklets%2C%20scaling%20and%20refining%20GPT%20models%20in%20Tractography%20Segmentation.%20Based%20on%20our%20experiments%2C%20TrackletGPT%20outperforms%20state-of-the-art%20methods%20on%20average%20DICE%2C%20Overlap%20and%20Overreach%20scores%20on%20TractoInferno%20and%20HCP%20datasets%2C%20even%20on%20inter-dataset%20experiments.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13935v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTrackletGPT%253A%2520A%2520Language-like%2520GPT%2520Framework%2520for%2520White%2520Matter%2520Tract%2520Segmentation%26entry.906535625%3DAnoushkrit%2520Goel%2520and%2520Simroop%2520Singh%2520and%2520Ankita%2520Joshi%2520and%2520Ranjeet%2520Ranjan%2520Jha%2520and%2520Chirag%2520Ahuja%2520and%2520Aditya%2520Nigam%2520and%2520Arnav%2520Bhavsar%26entry.1292438233%3DWhite%2520Matter%2520Tract%2520Segmentation%2520is%2520imperative%2520for%2520studying%2520brain%2520structural%2520connectivity%252C%2520neurological%2520disorders%2520and%2520neurosurgery.%2520This%2520task%2520remains%2520complex%252C%2520as%2520tracts%2520differ%2520among%2520themselves%252C%2520across%2520subjects%2520and%2520conditions%252C%2520yet%2520have%2520similar%25203D%2520structure%2520across%2520hemispheres%2520and%2520subjects.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520TrackletGPT%252C%2520a%2520language-like%2520GPT%2520framework%2520which%2520reintroduces%2520sequential%2520information%2520in%2520tokens%2520using%2520tracklets.%2520TrackletGPT%2520generalises%2520seamlessly%2520across%2520datasets%252C%2520is%2520fully%2520automatic%252C%2520and%2520encodes%2520granular%2520sub-streamline%2520segments%252C%2520Tracklets%252C%2520scaling%2520and%2520refining%2520GPT%2520models%2520in%2520Tractography%2520Segmentation.%2520Based%2520on%2520our%2520experiments%252C%2520TrackletGPT%2520outperforms%2520state-of-the-art%2520methods%2520on%2520average%2520DICE%252C%2520Overlap%2520and%2520Overreach%2520scores%2520on%2520TractoInferno%2520and%2520HCP%2520datasets%252C%2520even%2520on%2520inter-dataset%2520experiments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13935v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrackletGPT%3A%20A%20Language-like%20GPT%20Framework%20for%20White%20Matter%20Tract%20Segmentation&entry.906535625=Anoushkrit%20Goel%20and%20Simroop%20Singh%20and%20Ankita%20Joshi%20and%20Ranjeet%20Ranjan%20Jha%20and%20Chirag%20Ahuja%20and%20Aditya%20Nigam%20and%20Arnav%20Bhavsar&entry.1292438233=White%20Matter%20Tract%20Segmentation%20is%20imperative%20for%20studying%20brain%20structural%20connectivity%2C%20neurological%20disorders%20and%20neurosurgery.%20This%20task%20remains%20complex%2C%20as%20tracts%20differ%20among%20themselves%2C%20across%20subjects%20and%20conditions%2C%20yet%20have%20similar%203D%20structure%20across%20hemispheres%20and%20subjects.%20To%20address%20these%20challenges%2C%20we%20propose%20TrackletGPT%2C%20a%20language-like%20GPT%20framework%20which%20reintroduces%20sequential%20information%20in%20tokens%20using%20tracklets.%20TrackletGPT%20generalises%20seamlessly%20across%20datasets%2C%20is%20fully%20automatic%2C%20and%20encodes%20granular%20sub-streamline%20segments%2C%20Tracklets%2C%20scaling%20and%20refining%20GPT%20models%20in%20Tractography%20Segmentation.%20Based%20on%20our%20experiments%2C%20TrackletGPT%20outperforms%20state-of-the-art%20methods%20on%20average%20DICE%2C%20Overlap%20and%20Overreach%20scores%20on%20TractoInferno%20and%20HCP%20datasets%2C%20even%20on%20inter-dataset%20experiments.&entry.1838667208=http%3A//arxiv.org/abs/2601.13935v1&entry.124074799=Read"},
{"title": "DAME: Duration-Aware Matryoshka Embedding for Duration-Robust Speaker Verification", "author": "Youngmoon Jung and Joon-Young Yang and Ju-ho Kim and Jaeyoung Roh and Chang Woo Han and Hoon-Young Cho", "abstract": "Short-utterance speaker verification remains challenging due to limited speaker-discriminative cues in short speech segments. While existing methods focus on enhancing speaker encoders, the embedding learning strategy still forces a single fixed-dimensional representation reused for utterances of any length, leaving capacity misaligned with the information available at different durations. We propose Duration-Aware Matryoshka Embedding (DAME), a model-agnostic framework that builds a nested hierarchy of sub-embeddings aligned to utterance durations: lower-dimensional representations capture compact speaker traits from short utterances, while higher dimensions encode richer details from longer speech. DAME supports both training from scratch and fine-tuning, and serves as a direct alternative to conventional large-margin fine-tuning, consistently improving performance across durations. On the VoxCeleb1-O/E/H and VOiCES evaluation sets, DAME consistently reduces the equal error rate on 1-s and other short-duration trials, while maintaining full-length performance with no additional inference cost. These gains generalize across various speaker encoder architectures under both general training and fine-tuning setups.", "link": "http://arxiv.org/abs/2601.13999v1", "date": "2026-01-20", "relevancy": 2.4294, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5075}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4751}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4751}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAME%3A%20Duration-Aware%20Matryoshka%20Embedding%20for%20Duration-Robust%20Speaker%20Verification&body=Title%3A%20DAME%3A%20Duration-Aware%20Matryoshka%20Embedding%20for%20Duration-Robust%20Speaker%20Verification%0AAuthor%3A%20Youngmoon%20Jung%20and%20Joon-Young%20Yang%20and%20Ju-ho%20Kim%20and%20Jaeyoung%20Roh%20and%20Chang%20Woo%20Han%20and%20Hoon-Young%20Cho%0AAbstract%3A%20Short-utterance%20speaker%20verification%20remains%20challenging%20due%20to%20limited%20speaker-discriminative%20cues%20in%20short%20speech%20segments.%20While%20existing%20methods%20focus%20on%20enhancing%20speaker%20encoders%2C%20the%20embedding%20learning%20strategy%20still%20forces%20a%20single%20fixed-dimensional%20representation%20reused%20for%20utterances%20of%20any%20length%2C%20leaving%20capacity%20misaligned%20with%20the%20information%20available%20at%20different%20durations.%20We%20propose%20Duration-Aware%20Matryoshka%20Embedding%20%28DAME%29%2C%20a%20model-agnostic%20framework%20that%20builds%20a%20nested%20hierarchy%20of%20sub-embeddings%20aligned%20to%20utterance%20durations%3A%20lower-dimensional%20representations%20capture%20compact%20speaker%20traits%20from%20short%20utterances%2C%20while%20higher%20dimensions%20encode%20richer%20details%20from%20longer%20speech.%20DAME%20supports%20both%20training%20from%20scratch%20and%20fine-tuning%2C%20and%20serves%20as%20a%20direct%20alternative%20to%20conventional%20large-margin%20fine-tuning%2C%20consistently%20improving%20performance%20across%20durations.%20On%20the%20VoxCeleb1-O/E/H%20and%20VOiCES%20evaluation%20sets%2C%20DAME%20consistently%20reduces%20the%20equal%20error%20rate%20on%201-s%20and%20other%20short-duration%20trials%2C%20while%20maintaining%20full-length%20performance%20with%20no%20additional%20inference%20cost.%20These%20gains%20generalize%20across%20various%20speaker%20encoder%20architectures%20under%20both%20general%20training%20and%20fine-tuning%20setups.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13999v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAME%253A%2520Duration-Aware%2520Matryoshka%2520Embedding%2520for%2520Duration-Robust%2520Speaker%2520Verification%26entry.906535625%3DYoungmoon%2520Jung%2520and%2520Joon-Young%2520Yang%2520and%2520Ju-ho%2520Kim%2520and%2520Jaeyoung%2520Roh%2520and%2520Chang%2520Woo%2520Han%2520and%2520Hoon-Young%2520Cho%26entry.1292438233%3DShort-utterance%2520speaker%2520verification%2520remains%2520challenging%2520due%2520to%2520limited%2520speaker-discriminative%2520cues%2520in%2520short%2520speech%2520segments.%2520While%2520existing%2520methods%2520focus%2520on%2520enhancing%2520speaker%2520encoders%252C%2520the%2520embedding%2520learning%2520strategy%2520still%2520forces%2520a%2520single%2520fixed-dimensional%2520representation%2520reused%2520for%2520utterances%2520of%2520any%2520length%252C%2520leaving%2520capacity%2520misaligned%2520with%2520the%2520information%2520available%2520at%2520different%2520durations.%2520We%2520propose%2520Duration-Aware%2520Matryoshka%2520Embedding%2520%2528DAME%2529%252C%2520a%2520model-agnostic%2520framework%2520that%2520builds%2520a%2520nested%2520hierarchy%2520of%2520sub-embeddings%2520aligned%2520to%2520utterance%2520durations%253A%2520lower-dimensional%2520representations%2520capture%2520compact%2520speaker%2520traits%2520from%2520short%2520utterances%252C%2520while%2520higher%2520dimensions%2520encode%2520richer%2520details%2520from%2520longer%2520speech.%2520DAME%2520supports%2520both%2520training%2520from%2520scratch%2520and%2520fine-tuning%252C%2520and%2520serves%2520as%2520a%2520direct%2520alternative%2520to%2520conventional%2520large-margin%2520fine-tuning%252C%2520consistently%2520improving%2520performance%2520across%2520durations.%2520On%2520the%2520VoxCeleb1-O/E/H%2520and%2520VOiCES%2520evaluation%2520sets%252C%2520DAME%2520consistently%2520reduces%2520the%2520equal%2520error%2520rate%2520on%25201-s%2520and%2520other%2520short-duration%2520trials%252C%2520while%2520maintaining%2520full-length%2520performance%2520with%2520no%2520additional%2520inference%2520cost.%2520These%2520gains%2520generalize%2520across%2520various%2520speaker%2520encoder%2520architectures%2520under%2520both%2520general%2520training%2520and%2520fine-tuning%2520setups.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13999v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAME%3A%20Duration-Aware%20Matryoshka%20Embedding%20for%20Duration-Robust%20Speaker%20Verification&entry.906535625=Youngmoon%20Jung%20and%20Joon-Young%20Yang%20and%20Ju-ho%20Kim%20and%20Jaeyoung%20Roh%20and%20Chang%20Woo%20Han%20and%20Hoon-Young%20Cho&entry.1292438233=Short-utterance%20speaker%20verification%20remains%20challenging%20due%20to%20limited%20speaker-discriminative%20cues%20in%20short%20speech%20segments.%20While%20existing%20methods%20focus%20on%20enhancing%20speaker%20encoders%2C%20the%20embedding%20learning%20strategy%20still%20forces%20a%20single%20fixed-dimensional%20representation%20reused%20for%20utterances%20of%20any%20length%2C%20leaving%20capacity%20misaligned%20with%20the%20information%20available%20at%20different%20durations.%20We%20propose%20Duration-Aware%20Matryoshka%20Embedding%20%28DAME%29%2C%20a%20model-agnostic%20framework%20that%20builds%20a%20nested%20hierarchy%20of%20sub-embeddings%20aligned%20to%20utterance%20durations%3A%20lower-dimensional%20representations%20capture%20compact%20speaker%20traits%20from%20short%20utterances%2C%20while%20higher%20dimensions%20encode%20richer%20details%20from%20longer%20speech.%20DAME%20supports%20both%20training%20from%20scratch%20and%20fine-tuning%2C%20and%20serves%20as%20a%20direct%20alternative%20to%20conventional%20large-margin%20fine-tuning%2C%20consistently%20improving%20performance%20across%20durations.%20On%20the%20VoxCeleb1-O/E/H%20and%20VOiCES%20evaluation%20sets%2C%20DAME%20consistently%20reduces%20the%20equal%20error%20rate%20on%201-s%20and%20other%20short-duration%20trials%2C%20while%20maintaining%20full-length%20performance%20with%20no%20additional%20inference%20cost.%20These%20gains%20generalize%20across%20various%20speaker%20encoder%20architectures%20under%20both%20general%20training%20and%20fine-tuning%20setups.&entry.1838667208=http%3A//arxiv.org/abs/2601.13999v1&entry.124074799=Read"},
{"title": "\"The Whole Is Greater Than the Sum of Its Parts\": A Compatibility-Aware Multi-Teacher CoT Distillation Framework", "author": "Jin Cui and Jiaqi Guo and Jiepeng Zhou and Ruixuan Yang and Jiayi Lu and Jiajun Xu and Jiangcheng Song and Boran Zhao and Pengju Ren", "abstract": "Chain-of-Thought (CoT) reasoning empowers Large Language Models (LLMs) with remarkable capabilities but typically requires prohibitive parameter scales. CoT distillation has emerged as a promising paradigm to transfer reasoning prowess into compact Student Models (SLMs), but existing approaches often rely on a solitary teacher, capping the student's potential since individual LLMs often exhibit distinct capability biases and may suffer from catastrophic forgetting. While leveraging diverse teachers seems appealing, effectively fusing their supervisions remains challenging: teacher-student incompatibility risks amplifying hallucinations, and passive supervision fails to ensure genuine logic internalization. To address this, we introduce COMPACT, a framework that adaptively fuses supervisions from different teachers by dynamically weighting teacher gradients based on the student's real-time compatibility evaluated by a multi-dimensional metric: (1) Graph-based Consensus to filter misleading rationales by identifying mainstream reasoning paths; (2) Mutual-Information-based Adaptability to detect \"epiphany moments\" for genuinely understanding the reasoning process rather than merely imitating; and (3) Loss-based Difficulty to assess student receptivity to the teacher's guidance and prevent negative transfer. Extensive experiments and latent space analysis demonstrate that COMPACT effectively integrates diverse reasoning capabilities without damaging the model's original knowledge structure, achieving state-of-the-art performance on various benchmarks while mitigating catastrophic forgetting.", "link": "http://arxiv.org/abs/2601.13992v1", "date": "2026-01-20", "relevancy": 2.4278, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5245}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4738}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4584}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20%22The%20Whole%20Is%20Greater%20Than%20the%20Sum%20of%20Its%20Parts%22%3A%20A%20Compatibility-Aware%20Multi-Teacher%20CoT%20Distillation%20Framework&body=Title%3A%20%22The%20Whole%20Is%20Greater%20Than%20the%20Sum%20of%20Its%20Parts%22%3A%20A%20Compatibility-Aware%20Multi-Teacher%20CoT%20Distillation%20Framework%0AAuthor%3A%20Jin%20Cui%20and%20Jiaqi%20Guo%20and%20Jiepeng%20Zhou%20and%20Ruixuan%20Yang%20and%20Jiayi%20Lu%20and%20Jiajun%20Xu%20and%20Jiangcheng%20Song%20and%20Boran%20Zhao%20and%20Pengju%20Ren%0AAbstract%3A%20Chain-of-Thought%20%28CoT%29%20reasoning%20empowers%20Large%20Language%20Models%20%28LLMs%29%20with%20remarkable%20capabilities%20but%20typically%20requires%20prohibitive%20parameter%20scales.%20CoT%20distillation%20has%20emerged%20as%20a%20promising%20paradigm%20to%20transfer%20reasoning%20prowess%20into%20compact%20Student%20Models%20%28SLMs%29%2C%20but%20existing%20approaches%20often%20rely%20on%20a%20solitary%20teacher%2C%20capping%20the%20student%27s%20potential%20since%20individual%20LLMs%20often%20exhibit%20distinct%20capability%20biases%20and%20may%20suffer%20from%20catastrophic%20forgetting.%20While%20leveraging%20diverse%20teachers%20seems%20appealing%2C%20effectively%20fusing%20their%20supervisions%20remains%20challenging%3A%20teacher-student%20incompatibility%20risks%20amplifying%20hallucinations%2C%20and%20passive%20supervision%20fails%20to%20ensure%20genuine%20logic%20internalization.%20To%20address%20this%2C%20we%20introduce%20COMPACT%2C%20a%20framework%20that%20adaptively%20fuses%20supervisions%20from%20different%20teachers%20by%20dynamically%20weighting%20teacher%20gradients%20based%20on%20the%20student%27s%20real-time%20compatibility%20evaluated%20by%20a%20multi-dimensional%20metric%3A%20%281%29%20Graph-based%20Consensus%20to%20filter%20misleading%20rationales%20by%20identifying%20mainstream%20reasoning%20paths%3B%20%282%29%20Mutual-Information-based%20Adaptability%20to%20detect%20%22epiphany%20moments%22%20for%20genuinely%20understanding%20the%20reasoning%20process%20rather%20than%20merely%20imitating%3B%20and%20%283%29%20Loss-based%20Difficulty%20to%20assess%20student%20receptivity%20to%20the%20teacher%27s%20guidance%20and%20prevent%20negative%20transfer.%20Extensive%20experiments%20and%20latent%20space%20analysis%20demonstrate%20that%20COMPACT%20effectively%20integrates%20diverse%20reasoning%20capabilities%20without%20damaging%20the%20model%27s%20original%20knowledge%20structure%2C%20achieving%20state-of-the-art%20performance%20on%20various%20benchmarks%20while%20mitigating%20catastrophic%20forgetting.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13992v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D%2522The%2520Whole%2520Is%2520Greater%2520Than%2520the%2520Sum%2520of%2520Its%2520Parts%2522%253A%2520A%2520Compatibility-Aware%2520Multi-Teacher%2520CoT%2520Distillation%2520Framework%26entry.906535625%3DJin%2520Cui%2520and%2520Jiaqi%2520Guo%2520and%2520Jiepeng%2520Zhou%2520and%2520Ruixuan%2520Yang%2520and%2520Jiayi%2520Lu%2520and%2520Jiajun%2520Xu%2520and%2520Jiangcheng%2520Song%2520and%2520Boran%2520Zhao%2520and%2520Pengju%2520Ren%26entry.1292438233%3DChain-of-Thought%2520%2528CoT%2529%2520reasoning%2520empowers%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520remarkable%2520capabilities%2520but%2520typically%2520requires%2520prohibitive%2520parameter%2520scales.%2520CoT%2520distillation%2520has%2520emerged%2520as%2520a%2520promising%2520paradigm%2520to%2520transfer%2520reasoning%2520prowess%2520into%2520compact%2520Student%2520Models%2520%2528SLMs%2529%252C%2520but%2520existing%2520approaches%2520often%2520rely%2520on%2520a%2520solitary%2520teacher%252C%2520capping%2520the%2520student%2527s%2520potential%2520since%2520individual%2520LLMs%2520often%2520exhibit%2520distinct%2520capability%2520biases%2520and%2520may%2520suffer%2520from%2520catastrophic%2520forgetting.%2520While%2520leveraging%2520diverse%2520teachers%2520seems%2520appealing%252C%2520effectively%2520fusing%2520their%2520supervisions%2520remains%2520challenging%253A%2520teacher-student%2520incompatibility%2520risks%2520amplifying%2520hallucinations%252C%2520and%2520passive%2520supervision%2520fails%2520to%2520ensure%2520genuine%2520logic%2520internalization.%2520To%2520address%2520this%252C%2520we%2520introduce%2520COMPACT%252C%2520a%2520framework%2520that%2520adaptively%2520fuses%2520supervisions%2520from%2520different%2520teachers%2520by%2520dynamically%2520weighting%2520teacher%2520gradients%2520based%2520on%2520the%2520student%2527s%2520real-time%2520compatibility%2520evaluated%2520by%2520a%2520multi-dimensional%2520metric%253A%2520%25281%2529%2520Graph-based%2520Consensus%2520to%2520filter%2520misleading%2520rationales%2520by%2520identifying%2520mainstream%2520reasoning%2520paths%253B%2520%25282%2529%2520Mutual-Information-based%2520Adaptability%2520to%2520detect%2520%2522epiphany%2520moments%2522%2520for%2520genuinely%2520understanding%2520the%2520reasoning%2520process%2520rather%2520than%2520merely%2520imitating%253B%2520and%2520%25283%2529%2520Loss-based%2520Difficulty%2520to%2520assess%2520student%2520receptivity%2520to%2520the%2520teacher%2527s%2520guidance%2520and%2520prevent%2520negative%2520transfer.%2520Extensive%2520experiments%2520and%2520latent%2520space%2520analysis%2520demonstrate%2520that%2520COMPACT%2520effectively%2520integrates%2520diverse%2520reasoning%2520capabilities%2520without%2520damaging%2520the%2520model%2527s%2520original%2520knowledge%2520structure%252C%2520achieving%2520state-of-the-art%2520performance%2520on%2520various%2520benchmarks%2520while%2520mitigating%2520catastrophic%2520forgetting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13992v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=%22The%20Whole%20Is%20Greater%20Than%20the%20Sum%20of%20Its%20Parts%22%3A%20A%20Compatibility-Aware%20Multi-Teacher%20CoT%20Distillation%20Framework&entry.906535625=Jin%20Cui%20and%20Jiaqi%20Guo%20and%20Jiepeng%20Zhou%20and%20Ruixuan%20Yang%20and%20Jiayi%20Lu%20and%20Jiajun%20Xu%20and%20Jiangcheng%20Song%20and%20Boran%20Zhao%20and%20Pengju%20Ren&entry.1292438233=Chain-of-Thought%20%28CoT%29%20reasoning%20empowers%20Large%20Language%20Models%20%28LLMs%29%20with%20remarkable%20capabilities%20but%20typically%20requires%20prohibitive%20parameter%20scales.%20CoT%20distillation%20has%20emerged%20as%20a%20promising%20paradigm%20to%20transfer%20reasoning%20prowess%20into%20compact%20Student%20Models%20%28SLMs%29%2C%20but%20existing%20approaches%20often%20rely%20on%20a%20solitary%20teacher%2C%20capping%20the%20student%27s%20potential%20since%20individual%20LLMs%20often%20exhibit%20distinct%20capability%20biases%20and%20may%20suffer%20from%20catastrophic%20forgetting.%20While%20leveraging%20diverse%20teachers%20seems%20appealing%2C%20effectively%20fusing%20their%20supervisions%20remains%20challenging%3A%20teacher-student%20incompatibility%20risks%20amplifying%20hallucinations%2C%20and%20passive%20supervision%20fails%20to%20ensure%20genuine%20logic%20internalization.%20To%20address%20this%2C%20we%20introduce%20COMPACT%2C%20a%20framework%20that%20adaptively%20fuses%20supervisions%20from%20different%20teachers%20by%20dynamically%20weighting%20teacher%20gradients%20based%20on%20the%20student%27s%20real-time%20compatibility%20evaluated%20by%20a%20multi-dimensional%20metric%3A%20%281%29%20Graph-based%20Consensus%20to%20filter%20misleading%20rationales%20by%20identifying%20mainstream%20reasoning%20paths%3B%20%282%29%20Mutual-Information-based%20Adaptability%20to%20detect%20%22epiphany%20moments%22%20for%20genuinely%20understanding%20the%20reasoning%20process%20rather%20than%20merely%20imitating%3B%20and%20%283%29%20Loss-based%20Difficulty%20to%20assess%20student%20receptivity%20to%20the%20teacher%27s%20guidance%20and%20prevent%20negative%20transfer.%20Extensive%20experiments%20and%20latent%20space%20analysis%20demonstrate%20that%20COMPACT%20effectively%20integrates%20diverse%20reasoning%20capabilities%20without%20damaging%20the%20model%27s%20original%20knowledge%20structure%2C%20achieving%20state-of-the-art%20performance%20on%20various%20benchmarks%20while%20mitigating%20catastrophic%20forgetting.&entry.1838667208=http%3A//arxiv.org/abs/2601.13992v1&entry.124074799=Read"},
{"title": "The 4D Human Embryonic Brain Atlas: spatiotemporal atlas generation for rapid anatomical changes", "author": "Wietske A. P. Bastiaansen and Melek Rousian and Anton H. J. Koning and Wiro J. Niessen and Bernadette S. de Bakker and R\u00e9gine P. M. Steegers-Theunissen and Stefan Klein", "abstract": "Early brain development is crucial for lifelong neurodevelopmental health. However, current clinical practice offers limited knowledge of normal embryonic brain anatomy on ultrasound, despite the brain undergoing rapid changes within the time-span of days. To provide detailed insights into normal brain development and identify deviations, we created the 4D Human Embryonic Brain Atlas using a deep learning-based approach for groupwise registration and spatiotemporal atlas generation. Our method introduced a time-dependent initial atlas and penalized deviations from it, ensuring age-specific anatomy was maintained throughout rapid development. The atlas was generated and validated using 831 3D ultrasound images from 402 subjects in the Rotterdam Periconceptional Cohort, acquired between gestational weeks 8 and 12. We evaluated the effectiveness of our approach with an ablation study, which demonstrated that incorporating a time-dependent initial atlas and penalization produced anatomically accurate results. In contrast, omitting these adaptations led to anatomically incorrect atlas. Visual comparisons with an existing ex-vivo embryo atlas further confirmed the anatomical accuracy of our atlas. In conclusion, the proposed method successfully captures the rapid anotomical development of the embryonic brain. The resulting 4D Human Embryonic Brain Atlas provides a unique insights into this crucial early life period and holds the potential for improving the detection, prevention, and treatment of prenatal neurodevelopmental disorders.", "link": "http://arxiv.org/abs/2503.07177v2", "date": "2026-01-20", "relevancy": 2.425, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4882}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4882}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4786}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%204D%20Human%20Embryonic%20Brain%20Atlas%3A%20spatiotemporal%20atlas%20generation%20for%20rapid%20anatomical%20changes&body=Title%3A%20The%204D%20Human%20Embryonic%20Brain%20Atlas%3A%20spatiotemporal%20atlas%20generation%20for%20rapid%20anatomical%20changes%0AAuthor%3A%20Wietske%20A.%20P.%20Bastiaansen%20and%20Melek%20Rousian%20and%20Anton%20H.%20J.%20Koning%20and%20Wiro%20J.%20Niessen%20and%20Bernadette%20S.%20de%20Bakker%20and%20R%C3%A9gine%20P.%20M.%20Steegers-Theunissen%20and%20Stefan%20Klein%0AAbstract%3A%20Early%20brain%20development%20is%20crucial%20for%20lifelong%20neurodevelopmental%20health.%20However%2C%20current%20clinical%20practice%20offers%20limited%20knowledge%20of%20normal%20embryonic%20brain%20anatomy%20on%20ultrasound%2C%20despite%20the%20brain%20undergoing%20rapid%20changes%20within%20the%20time-span%20of%20days.%20To%20provide%20detailed%20insights%20into%20normal%20brain%20development%20and%20identify%20deviations%2C%20we%20created%20the%204D%20Human%20Embryonic%20Brain%20Atlas%20using%20a%20deep%20learning-based%20approach%20for%20groupwise%20registration%20and%20spatiotemporal%20atlas%20generation.%20Our%20method%20introduced%20a%20time-dependent%20initial%20atlas%20and%20penalized%20deviations%20from%20it%2C%20ensuring%20age-specific%20anatomy%20was%20maintained%20throughout%20rapid%20development.%20The%20atlas%20was%20generated%20and%20validated%20using%20831%203D%20ultrasound%20images%20from%20402%20subjects%20in%20the%20Rotterdam%20Periconceptional%20Cohort%2C%20acquired%20between%20gestational%20weeks%208%20and%2012.%20We%20evaluated%20the%20effectiveness%20of%20our%20approach%20with%20an%20ablation%20study%2C%20which%20demonstrated%20that%20incorporating%20a%20time-dependent%20initial%20atlas%20and%20penalization%20produced%20anatomically%20accurate%20results.%20In%20contrast%2C%20omitting%20these%20adaptations%20led%20to%20anatomically%20incorrect%20atlas.%20Visual%20comparisons%20with%20an%20existing%20ex-vivo%20embryo%20atlas%20further%20confirmed%20the%20anatomical%20accuracy%20of%20our%20atlas.%20In%20conclusion%2C%20the%20proposed%20method%20successfully%20captures%20the%20rapid%20anotomical%20development%20of%20the%20embryonic%20brain.%20The%20resulting%204D%20Human%20Embryonic%20Brain%20Atlas%20provides%20a%20unique%20insights%20into%20this%20crucial%20early%20life%20period%20and%20holds%20the%20potential%20for%20improving%20the%20detection%2C%20prevention%2C%20and%20treatment%20of%20prenatal%20neurodevelopmental%20disorders.%0ALink%3A%20http%3A//arxiv.org/abs/2503.07177v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%25204D%2520Human%2520Embryonic%2520Brain%2520Atlas%253A%2520spatiotemporal%2520atlas%2520generation%2520for%2520rapid%2520anatomical%2520changes%26entry.906535625%3DWietske%2520A.%2520P.%2520Bastiaansen%2520and%2520Melek%2520Rousian%2520and%2520Anton%2520H.%2520J.%2520Koning%2520and%2520Wiro%2520J.%2520Niessen%2520and%2520Bernadette%2520S.%2520de%2520Bakker%2520and%2520R%25C3%25A9gine%2520P.%2520M.%2520Steegers-Theunissen%2520and%2520Stefan%2520Klein%26entry.1292438233%3DEarly%2520brain%2520development%2520is%2520crucial%2520for%2520lifelong%2520neurodevelopmental%2520health.%2520However%252C%2520current%2520clinical%2520practice%2520offers%2520limited%2520knowledge%2520of%2520normal%2520embryonic%2520brain%2520anatomy%2520on%2520ultrasound%252C%2520despite%2520the%2520brain%2520undergoing%2520rapid%2520changes%2520within%2520the%2520time-span%2520of%2520days.%2520To%2520provide%2520detailed%2520insights%2520into%2520normal%2520brain%2520development%2520and%2520identify%2520deviations%252C%2520we%2520created%2520the%25204D%2520Human%2520Embryonic%2520Brain%2520Atlas%2520using%2520a%2520deep%2520learning-based%2520approach%2520for%2520groupwise%2520registration%2520and%2520spatiotemporal%2520atlas%2520generation.%2520Our%2520method%2520introduced%2520a%2520time-dependent%2520initial%2520atlas%2520and%2520penalized%2520deviations%2520from%2520it%252C%2520ensuring%2520age-specific%2520anatomy%2520was%2520maintained%2520throughout%2520rapid%2520development.%2520The%2520atlas%2520was%2520generated%2520and%2520validated%2520using%2520831%25203D%2520ultrasound%2520images%2520from%2520402%2520subjects%2520in%2520the%2520Rotterdam%2520Periconceptional%2520Cohort%252C%2520acquired%2520between%2520gestational%2520weeks%25208%2520and%252012.%2520We%2520evaluated%2520the%2520effectiveness%2520of%2520our%2520approach%2520with%2520an%2520ablation%2520study%252C%2520which%2520demonstrated%2520that%2520incorporating%2520a%2520time-dependent%2520initial%2520atlas%2520and%2520penalization%2520produced%2520anatomically%2520accurate%2520results.%2520In%2520contrast%252C%2520omitting%2520these%2520adaptations%2520led%2520to%2520anatomically%2520incorrect%2520atlas.%2520Visual%2520comparisons%2520with%2520an%2520existing%2520ex-vivo%2520embryo%2520atlas%2520further%2520confirmed%2520the%2520anatomical%2520accuracy%2520of%2520our%2520atlas.%2520In%2520conclusion%252C%2520the%2520proposed%2520method%2520successfully%2520captures%2520the%2520rapid%2520anotomical%2520development%2520of%2520the%2520embryonic%2520brain.%2520The%2520resulting%25204D%2520Human%2520Embryonic%2520Brain%2520Atlas%2520provides%2520a%2520unique%2520insights%2520into%2520this%2520crucial%2520early%2520life%2520period%2520and%2520holds%2520the%2520potential%2520for%2520improving%2520the%2520detection%252C%2520prevention%252C%2520and%2520treatment%2520of%2520prenatal%2520neurodevelopmental%2520disorders.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.07177v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%204D%20Human%20Embryonic%20Brain%20Atlas%3A%20spatiotemporal%20atlas%20generation%20for%20rapid%20anatomical%20changes&entry.906535625=Wietske%20A.%20P.%20Bastiaansen%20and%20Melek%20Rousian%20and%20Anton%20H.%20J.%20Koning%20and%20Wiro%20J.%20Niessen%20and%20Bernadette%20S.%20de%20Bakker%20and%20R%C3%A9gine%20P.%20M.%20Steegers-Theunissen%20and%20Stefan%20Klein&entry.1292438233=Early%20brain%20development%20is%20crucial%20for%20lifelong%20neurodevelopmental%20health.%20However%2C%20current%20clinical%20practice%20offers%20limited%20knowledge%20of%20normal%20embryonic%20brain%20anatomy%20on%20ultrasound%2C%20despite%20the%20brain%20undergoing%20rapid%20changes%20within%20the%20time-span%20of%20days.%20To%20provide%20detailed%20insights%20into%20normal%20brain%20development%20and%20identify%20deviations%2C%20we%20created%20the%204D%20Human%20Embryonic%20Brain%20Atlas%20using%20a%20deep%20learning-based%20approach%20for%20groupwise%20registration%20and%20spatiotemporal%20atlas%20generation.%20Our%20method%20introduced%20a%20time-dependent%20initial%20atlas%20and%20penalized%20deviations%20from%20it%2C%20ensuring%20age-specific%20anatomy%20was%20maintained%20throughout%20rapid%20development.%20The%20atlas%20was%20generated%20and%20validated%20using%20831%203D%20ultrasound%20images%20from%20402%20subjects%20in%20the%20Rotterdam%20Periconceptional%20Cohort%2C%20acquired%20between%20gestational%20weeks%208%20and%2012.%20We%20evaluated%20the%20effectiveness%20of%20our%20approach%20with%20an%20ablation%20study%2C%20which%20demonstrated%20that%20incorporating%20a%20time-dependent%20initial%20atlas%20and%20penalization%20produced%20anatomically%20accurate%20results.%20In%20contrast%2C%20omitting%20these%20adaptations%20led%20to%20anatomically%20incorrect%20atlas.%20Visual%20comparisons%20with%20an%20existing%20ex-vivo%20embryo%20atlas%20further%20confirmed%20the%20anatomical%20accuracy%20of%20our%20atlas.%20In%20conclusion%2C%20the%20proposed%20method%20successfully%20captures%20the%20rapid%20anotomical%20development%20of%20the%20embryonic%20brain.%20The%20resulting%204D%20Human%20Embryonic%20Brain%20Atlas%20provides%20a%20unique%20insights%20into%20this%20crucial%20early%20life%20period%20and%20holds%20the%20potential%20for%20improving%20the%20detection%2C%20prevention%2C%20and%20treatment%20of%20prenatal%20neurodevelopmental%20disorders.&entry.1838667208=http%3A//arxiv.org/abs/2503.07177v2&entry.124074799=Read"},
{"title": "Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models", "author": "Hyunjong Ok and Jaeho Lee", "abstract": "Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.", "link": "http://arxiv.org/abs/2601.14152v1", "date": "2026-01-20", "relevancy": 2.4114, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5104}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Lost%20in%20the%20Prompt%20Order%3A%20Revealing%20the%20Limitations%20of%20Causal%20Attention%20in%20Language%20Models&body=Title%3A%20Lost%20in%20the%20Prompt%20Order%3A%20Revealing%20the%20Limitations%20of%20Causal%20Attention%20in%20Language%20Models%0AAuthor%3A%20Hyunjong%20Ok%20and%20Jaeho%20Lee%0AAbstract%3A%20Large%20language%20models%20exhibit%20surprising%20sensitivity%20to%20the%20structure%20of%20the%20prompt%2C%20but%20the%20mechanisms%20underlying%20this%20sensitivity%20remain%20poorly%20understood.%20In%20this%20work%2C%20we%20conduct%20an%20in-depth%20investigation%20on%20a%20striking%20case%3A%20in%20multiple-choice%20question%20answering%2C%20placing%20context%20before%20the%20questions%20and%20options%20%28CQO%29%20outperforms%20the%20reverse%20order%20%28QOC%29%20by%20over%2014%25p%2C%20consistently%20over%20a%20wide%20range%20of%20models%20and%20datasets.%20Through%20systematic%20architectural%20analysis%2C%20we%20identify%20causal%20attention%20as%20the%20core%20mechanism%3A%20in%20QOC%20prompts%2C%20the%20causal%20mask%20prevents%20option%20tokens%20from%20attending%20to%20context%2C%20creating%20an%20information%20bottleneck%20where%20context%20becomes%20invisible%20to%20options.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14152v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLost%2520in%2520the%2520Prompt%2520Order%253A%2520Revealing%2520the%2520Limitations%2520of%2520Causal%2520Attention%2520in%2520Language%2520Models%26entry.906535625%3DHyunjong%2520Ok%2520and%2520Jaeho%2520Lee%26entry.1292438233%3DLarge%2520language%2520models%2520exhibit%2520surprising%2520sensitivity%2520to%2520the%2520structure%2520of%2520the%2520prompt%252C%2520but%2520the%2520mechanisms%2520underlying%2520this%2520sensitivity%2520remain%2520poorly%2520understood.%2520In%2520this%2520work%252C%2520we%2520conduct%2520an%2520in-depth%2520investigation%2520on%2520a%2520striking%2520case%253A%2520in%2520multiple-choice%2520question%2520answering%252C%2520placing%2520context%2520before%2520the%2520questions%2520and%2520options%2520%2528CQO%2529%2520outperforms%2520the%2520reverse%2520order%2520%2528QOC%2529%2520by%2520over%252014%2525p%252C%2520consistently%2520over%2520a%2520wide%2520range%2520of%2520models%2520and%2520datasets.%2520Through%2520systematic%2520architectural%2520analysis%252C%2520we%2520identify%2520causal%2520attention%2520as%2520the%2520core%2520mechanism%253A%2520in%2520QOC%2520prompts%252C%2520the%2520causal%2520mask%2520prevents%2520option%2520tokens%2520from%2520attending%2520to%2520context%252C%2520creating%2520an%2520information%2520bottleneck%2520where%2520context%2520becomes%2520invisible%2520to%2520options.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14152v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Lost%20in%20the%20Prompt%20Order%3A%20Revealing%20the%20Limitations%20of%20Causal%20Attention%20in%20Language%20Models&entry.906535625=Hyunjong%20Ok%20and%20Jaeho%20Lee&entry.1292438233=Large%20language%20models%20exhibit%20surprising%20sensitivity%20to%20the%20structure%20of%20the%20prompt%2C%20but%20the%20mechanisms%20underlying%20this%20sensitivity%20remain%20poorly%20understood.%20In%20this%20work%2C%20we%20conduct%20an%20in-depth%20investigation%20on%20a%20striking%20case%3A%20in%20multiple-choice%20question%20answering%2C%20placing%20context%20before%20the%20questions%20and%20options%20%28CQO%29%20outperforms%20the%20reverse%20order%20%28QOC%29%20by%20over%2014%25p%2C%20consistently%20over%20a%20wide%20range%20of%20models%20and%20datasets.%20Through%20systematic%20architectural%20analysis%2C%20we%20identify%20causal%20attention%20as%20the%20core%20mechanism%3A%20in%20QOC%20prompts%2C%20the%20causal%20mask%20prevents%20option%20tokens%20from%20attending%20to%20context%2C%20creating%20an%20information%20bottleneck%20where%20context%20becomes%20invisible%20to%20options.&entry.1838667208=http%3A//arxiv.org/abs/2601.14152v1&entry.124074799=Read"},
{"title": "Knowledge Graph-Assisted LLM Post-Training for Enhanced Legal Reasoning", "author": "Dezhao Song and Guglielmo Bonifazi and Frank Schilder and Jonathan Richard Schwarz", "abstract": "LLM post-training has primarily relied on large text corpora and human feedback, without capturing the structure of domain knowledge. This has caused models to struggle dealing with complex reasoning tasks, especially for high-stakes professional domains. In Law, reasoning requires deep understanding of the relations between various legal concepts, a key component missing in current LLM post-training. In this paper, we propose a knowledge graph (KG)-assisted approach for enhancing LLMs' reasoning capability in Legal that is generalizable to other high-stakes domains. We model key legal concepts by following the \\textbf{IRAC} (Issue, Rule, Analysis and Conclusion) framework, and construct a KG with 12K legal cases. We then produce training data using our IRAC KG, and conduct both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) with three state-of-the-art (SOTA) LLMs (30B, 49B and 70B), varying architecture and base model family. Our post-trained models obtained better average performance on 4/5 diverse legal benchmarks (14 tasks) than baselines. In particular, our 70B DPO model achieved the best score on 4/6 reasoning tasks, among baselines and a 141B SOTA legal LLM, demonstrating the effectiveness of our KG for enhancing LLMs' legal reasoning capability.", "link": "http://arxiv.org/abs/2601.13806v1", "date": "2026-01-20", "relevancy": 2.3968, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.484}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.484}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4702}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Knowledge%20Graph-Assisted%20LLM%20Post-Training%20for%20Enhanced%20Legal%20Reasoning&body=Title%3A%20Knowledge%20Graph-Assisted%20LLM%20Post-Training%20for%20Enhanced%20Legal%20Reasoning%0AAuthor%3A%20Dezhao%20Song%20and%20Guglielmo%20Bonifazi%20and%20Frank%20Schilder%20and%20Jonathan%20Richard%20Schwarz%0AAbstract%3A%20LLM%20post-training%20has%20primarily%20relied%20on%20large%20text%20corpora%20and%20human%20feedback%2C%20without%20capturing%20the%20structure%20of%20domain%20knowledge.%20This%20has%20caused%20models%20to%20struggle%20dealing%20with%20complex%20reasoning%20tasks%2C%20especially%20for%20high-stakes%20professional%20domains.%20In%20Law%2C%20reasoning%20requires%20deep%20understanding%20of%20the%20relations%20between%20various%20legal%20concepts%2C%20a%20key%20component%20missing%20in%20current%20LLM%20post-training.%20In%20this%20paper%2C%20we%20propose%20a%20knowledge%20graph%20%28KG%29-assisted%20approach%20for%20enhancing%20LLMs%27%20reasoning%20capability%20in%20Legal%20that%20is%20generalizable%20to%20other%20high-stakes%20domains.%20We%20model%20key%20legal%20concepts%20by%20following%20the%20%5Ctextbf%7BIRAC%7D%20%28Issue%2C%20Rule%2C%20Analysis%20and%20Conclusion%29%20framework%2C%20and%20construct%20a%20KG%20with%2012K%20legal%20cases.%20We%20then%20produce%20training%20data%20using%20our%20IRAC%20KG%2C%20and%20conduct%20both%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20Direct%20Preference%20Optimization%20%28DPO%29%20with%20three%20state-of-the-art%20%28SOTA%29%20LLMs%20%2830B%2C%2049B%20and%2070B%29%2C%20varying%20architecture%20and%20base%20model%20family.%20Our%20post-trained%20models%20obtained%20better%20average%20performance%20on%204/5%20diverse%20legal%20benchmarks%20%2814%20tasks%29%20than%20baselines.%20In%20particular%2C%20our%2070B%20DPO%20model%20achieved%20the%20best%20score%20on%204/6%20reasoning%20tasks%2C%20among%20baselines%20and%20a%20141B%20SOTA%20legal%20LLM%2C%20demonstrating%20the%20effectiveness%20of%20our%20KG%20for%20enhancing%20LLMs%27%20legal%20reasoning%20capability.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13806v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKnowledge%2520Graph-Assisted%2520LLM%2520Post-Training%2520for%2520Enhanced%2520Legal%2520Reasoning%26entry.906535625%3DDezhao%2520Song%2520and%2520Guglielmo%2520Bonifazi%2520and%2520Frank%2520Schilder%2520and%2520Jonathan%2520Richard%2520Schwarz%26entry.1292438233%3DLLM%2520post-training%2520has%2520primarily%2520relied%2520on%2520large%2520text%2520corpora%2520and%2520human%2520feedback%252C%2520without%2520capturing%2520the%2520structure%2520of%2520domain%2520knowledge.%2520This%2520has%2520caused%2520models%2520to%2520struggle%2520dealing%2520with%2520complex%2520reasoning%2520tasks%252C%2520especially%2520for%2520high-stakes%2520professional%2520domains.%2520In%2520Law%252C%2520reasoning%2520requires%2520deep%2520understanding%2520of%2520the%2520relations%2520between%2520various%2520legal%2520concepts%252C%2520a%2520key%2520component%2520missing%2520in%2520current%2520LLM%2520post-training.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520knowledge%2520graph%2520%2528KG%2529-assisted%2520approach%2520for%2520enhancing%2520LLMs%2527%2520reasoning%2520capability%2520in%2520Legal%2520that%2520is%2520generalizable%2520to%2520other%2520high-stakes%2520domains.%2520We%2520model%2520key%2520legal%2520concepts%2520by%2520following%2520the%2520%255Ctextbf%257BIRAC%257D%2520%2528Issue%252C%2520Rule%252C%2520Analysis%2520and%2520Conclusion%2529%2520framework%252C%2520and%2520construct%2520a%2520KG%2520with%252012K%2520legal%2520cases.%2520We%2520then%2520produce%2520training%2520data%2520using%2520our%2520IRAC%2520KG%252C%2520and%2520conduct%2520both%2520Supervised%2520Fine-Tuning%2520%2528SFT%2529%2520and%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520with%2520three%2520state-of-the-art%2520%2528SOTA%2529%2520LLMs%2520%252830B%252C%252049B%2520and%252070B%2529%252C%2520varying%2520architecture%2520and%2520base%2520model%2520family.%2520Our%2520post-trained%2520models%2520obtained%2520better%2520average%2520performance%2520on%25204/5%2520diverse%2520legal%2520benchmarks%2520%252814%2520tasks%2529%2520than%2520baselines.%2520In%2520particular%252C%2520our%252070B%2520DPO%2520model%2520achieved%2520the%2520best%2520score%2520on%25204/6%2520reasoning%2520tasks%252C%2520among%2520baselines%2520and%2520a%2520141B%2520SOTA%2520legal%2520LLM%252C%2520demonstrating%2520the%2520effectiveness%2520of%2520our%2520KG%2520for%2520enhancing%2520LLMs%2527%2520legal%2520reasoning%2520capability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13806v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Knowledge%20Graph-Assisted%20LLM%20Post-Training%20for%20Enhanced%20Legal%20Reasoning&entry.906535625=Dezhao%20Song%20and%20Guglielmo%20Bonifazi%20and%20Frank%20Schilder%20and%20Jonathan%20Richard%20Schwarz&entry.1292438233=LLM%20post-training%20has%20primarily%20relied%20on%20large%20text%20corpora%20and%20human%20feedback%2C%20without%20capturing%20the%20structure%20of%20domain%20knowledge.%20This%20has%20caused%20models%20to%20struggle%20dealing%20with%20complex%20reasoning%20tasks%2C%20especially%20for%20high-stakes%20professional%20domains.%20In%20Law%2C%20reasoning%20requires%20deep%20understanding%20of%20the%20relations%20between%20various%20legal%20concepts%2C%20a%20key%20component%20missing%20in%20current%20LLM%20post-training.%20In%20this%20paper%2C%20we%20propose%20a%20knowledge%20graph%20%28KG%29-assisted%20approach%20for%20enhancing%20LLMs%27%20reasoning%20capability%20in%20Legal%20that%20is%20generalizable%20to%20other%20high-stakes%20domains.%20We%20model%20key%20legal%20concepts%20by%20following%20the%20%5Ctextbf%7BIRAC%7D%20%28Issue%2C%20Rule%2C%20Analysis%20and%20Conclusion%29%20framework%2C%20and%20construct%20a%20KG%20with%2012K%20legal%20cases.%20We%20then%20produce%20training%20data%20using%20our%20IRAC%20KG%2C%20and%20conduct%20both%20Supervised%20Fine-Tuning%20%28SFT%29%20and%20Direct%20Preference%20Optimization%20%28DPO%29%20with%20three%20state-of-the-art%20%28SOTA%29%20LLMs%20%2830B%2C%2049B%20and%2070B%29%2C%20varying%20architecture%20and%20base%20model%20family.%20Our%20post-trained%20models%20obtained%20better%20average%20performance%20on%204/5%20diverse%20legal%20benchmarks%20%2814%20tasks%29%20than%20baselines.%20In%20particular%2C%20our%2070B%20DPO%20model%20achieved%20the%20best%20score%20on%204/6%20reasoning%20tasks%2C%20among%20baselines%20and%20a%20141B%20SOTA%20legal%20LLM%2C%20demonstrating%20the%20effectiveness%20of%20our%20KG%20for%20enhancing%20LLMs%27%20legal%20reasoning%20capability.&entry.1838667208=http%3A//arxiv.org/abs/2601.13806v1&entry.124074799=Read"},
{"title": "MATE: Matryoshka Audio-Text Embeddings for Open-Vocabulary Keyword Spotting", "author": "Youngmoon Jung and Myunghun Jung and Joon-Young Yang and Yong-Hyeok Lee and Jaeyoung Roh and Hoon-Young Cho", "abstract": "Open-vocabulary keyword spotting (KWS) with text-based enrollment has emerged as a flexible alternative to fixed-phrase triggers. Prior utterance-level matching methods, from an embedding-learning standpoint, learn embeddings at a single fixed dimensionality. We depart from this design and propose Matryoshka Audio-Text Embeddings (MATE), a dual-encoder framework that encodes multiple embedding granularities within a single vector via nested sub-embeddings (\"prefixes\"). Specifically, we introduce a PCA-guided prefix alignment: PCA-compressed versions of the full text embedding for each prefix size serve as teacher targets to align both audio and text prefixes. This alignment concentrates salient keyword cues in lower-dimensional prefixes, while higher dimensions add detail. MATE is trained with standard deep metric learning objectives for audio-text KWS, and is loss-agnostic. To our knowledge, this is the first application of matryoshka-style embeddings to KWS, achieving state-of-the-art results on WSJ and LibriPhrase without any inference overhead.", "link": "http://arxiv.org/abs/2601.14012v1", "date": "2026-01-20", "relevancy": 2.3858, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4882}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.474}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4693}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MATE%3A%20Matryoshka%20Audio-Text%20Embeddings%20for%20Open-Vocabulary%20Keyword%20Spotting&body=Title%3A%20MATE%3A%20Matryoshka%20Audio-Text%20Embeddings%20for%20Open-Vocabulary%20Keyword%20Spotting%0AAuthor%3A%20Youngmoon%20Jung%20and%20Myunghun%20Jung%20and%20Joon-Young%20Yang%20and%20Yong-Hyeok%20Lee%20and%20Jaeyoung%20Roh%20and%20Hoon-Young%20Cho%0AAbstract%3A%20Open-vocabulary%20keyword%20spotting%20%28KWS%29%20with%20text-based%20enrollment%20has%20emerged%20as%20a%20flexible%20alternative%20to%20fixed-phrase%20triggers.%20Prior%20utterance-level%20matching%20methods%2C%20from%20an%20embedding-learning%20standpoint%2C%20learn%20embeddings%20at%20a%20single%20fixed%20dimensionality.%20We%20depart%20from%20this%20design%20and%20propose%20Matryoshka%20Audio-Text%20Embeddings%20%28MATE%29%2C%20a%20dual-encoder%20framework%20that%20encodes%20multiple%20embedding%20granularities%20within%20a%20single%20vector%20via%20nested%20sub-embeddings%20%28%22prefixes%22%29.%20Specifically%2C%20we%20introduce%20a%20PCA-guided%20prefix%20alignment%3A%20PCA-compressed%20versions%20of%20the%20full%20text%20embedding%20for%20each%20prefix%20size%20serve%20as%20teacher%20targets%20to%20align%20both%20audio%20and%20text%20prefixes.%20This%20alignment%20concentrates%20salient%20keyword%20cues%20in%20lower-dimensional%20prefixes%2C%20while%20higher%20dimensions%20add%20detail.%20MATE%20is%20trained%20with%20standard%20deep%20metric%20learning%20objectives%20for%20audio-text%20KWS%2C%20and%20is%20loss-agnostic.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20application%20of%20matryoshka-style%20embeddings%20to%20KWS%2C%20achieving%20state-of-the-art%20results%20on%20WSJ%20and%20LibriPhrase%20without%20any%20inference%20overhead.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14012v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMATE%253A%2520Matryoshka%2520Audio-Text%2520Embeddings%2520for%2520Open-Vocabulary%2520Keyword%2520Spotting%26entry.906535625%3DYoungmoon%2520Jung%2520and%2520Myunghun%2520Jung%2520and%2520Joon-Young%2520Yang%2520and%2520Yong-Hyeok%2520Lee%2520and%2520Jaeyoung%2520Roh%2520and%2520Hoon-Young%2520Cho%26entry.1292438233%3DOpen-vocabulary%2520keyword%2520spotting%2520%2528KWS%2529%2520with%2520text-based%2520enrollment%2520has%2520emerged%2520as%2520a%2520flexible%2520alternative%2520to%2520fixed-phrase%2520triggers.%2520Prior%2520utterance-level%2520matching%2520methods%252C%2520from%2520an%2520embedding-learning%2520standpoint%252C%2520learn%2520embeddings%2520at%2520a%2520single%2520fixed%2520dimensionality.%2520We%2520depart%2520from%2520this%2520design%2520and%2520propose%2520Matryoshka%2520Audio-Text%2520Embeddings%2520%2528MATE%2529%252C%2520a%2520dual-encoder%2520framework%2520that%2520encodes%2520multiple%2520embedding%2520granularities%2520within%2520a%2520single%2520vector%2520via%2520nested%2520sub-embeddings%2520%2528%2522prefixes%2522%2529.%2520Specifically%252C%2520we%2520introduce%2520a%2520PCA-guided%2520prefix%2520alignment%253A%2520PCA-compressed%2520versions%2520of%2520the%2520full%2520text%2520embedding%2520for%2520each%2520prefix%2520size%2520serve%2520as%2520teacher%2520targets%2520to%2520align%2520both%2520audio%2520and%2520text%2520prefixes.%2520This%2520alignment%2520concentrates%2520salient%2520keyword%2520cues%2520in%2520lower-dimensional%2520prefixes%252C%2520while%2520higher%2520dimensions%2520add%2520detail.%2520MATE%2520is%2520trained%2520with%2520standard%2520deep%2520metric%2520learning%2520objectives%2520for%2520audio-text%2520KWS%252C%2520and%2520is%2520loss-agnostic.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520application%2520of%2520matryoshka-style%2520embeddings%2520to%2520KWS%252C%2520achieving%2520state-of-the-art%2520results%2520on%2520WSJ%2520and%2520LibriPhrase%2520without%2520any%2520inference%2520overhead.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14012v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MATE%3A%20Matryoshka%20Audio-Text%20Embeddings%20for%20Open-Vocabulary%20Keyword%20Spotting&entry.906535625=Youngmoon%20Jung%20and%20Myunghun%20Jung%20and%20Joon-Young%20Yang%20and%20Yong-Hyeok%20Lee%20and%20Jaeyoung%20Roh%20and%20Hoon-Young%20Cho&entry.1292438233=Open-vocabulary%20keyword%20spotting%20%28KWS%29%20with%20text-based%20enrollment%20has%20emerged%20as%20a%20flexible%20alternative%20to%20fixed-phrase%20triggers.%20Prior%20utterance-level%20matching%20methods%2C%20from%20an%20embedding-learning%20standpoint%2C%20learn%20embeddings%20at%20a%20single%20fixed%20dimensionality.%20We%20depart%20from%20this%20design%20and%20propose%20Matryoshka%20Audio-Text%20Embeddings%20%28MATE%29%2C%20a%20dual-encoder%20framework%20that%20encodes%20multiple%20embedding%20granularities%20within%20a%20single%20vector%20via%20nested%20sub-embeddings%20%28%22prefixes%22%29.%20Specifically%2C%20we%20introduce%20a%20PCA-guided%20prefix%20alignment%3A%20PCA-compressed%20versions%20of%20the%20full%20text%20embedding%20for%20each%20prefix%20size%20serve%20as%20teacher%20targets%20to%20align%20both%20audio%20and%20text%20prefixes.%20This%20alignment%20concentrates%20salient%20keyword%20cues%20in%20lower-dimensional%20prefixes%2C%20while%20higher%20dimensions%20add%20detail.%20MATE%20is%20trained%20with%20standard%20deep%20metric%20learning%20objectives%20for%20audio-text%20KWS%2C%20and%20is%20loss-agnostic.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20application%20of%20matryoshka-style%20embeddings%20to%20KWS%2C%20achieving%20state-of-the-art%20results%20on%20WSJ%20and%20LibriPhrase%20without%20any%20inference%20overhead.&entry.1838667208=http%3A//arxiv.org/abs/2601.14012v1&entry.124074799=Read"},
{"title": "Learning to Explain: Supervised Token Attribution from Transformer Attention Patterns", "author": "George Mihaila", "abstract": "Explainable AI (XAI) has become critical as transformer-based models are deployed in high-stakes applications including healthcare, legal systems, and financial services, where opacity hinders trust and accountability. Transformers self-attention mechanisms have proven valuable for model interpretability, with attention weights successfully used to understand model focus and behavior (Xu et al., 2015); (Wiegreffe and Pinter, 2019). However, existing attention-based explanation methods rely on manually defined aggregation strategies and fixed attribution rules (Abnar and Zuidema, 2020a); (Chefer et al., 2021), while model-agnostic approaches (LIME, SHAP) treat the model as a black box and incur significant computational costs through input perturbation. We introduce Explanation Network (ExpNet), a lightweight neural network that learns an explicit mapping from transformer attention patterns to token-level importance scores. Unlike prior methods, ExpNet discovers optimal attention feature combinations automatically rather than relying on predetermined rules. We evaluate ExpNet in a challenging cross-task setting and benchmark it against a broad spectrum of model-agnostic methods and attention-based techniques spanning four methodological families.", "link": "http://arxiv.org/abs/2601.14112v1", "date": "2026-01-20", "relevancy": 2.3789, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4837}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4732}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4704}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Explain%3A%20Supervised%20Token%20Attribution%20from%20Transformer%20Attention%20Patterns&body=Title%3A%20Learning%20to%20Explain%3A%20Supervised%20Token%20Attribution%20from%20Transformer%20Attention%20Patterns%0AAuthor%3A%20George%20Mihaila%0AAbstract%3A%20Explainable%20AI%20%28XAI%29%20has%20become%20critical%20as%20transformer-based%20models%20are%20deployed%20in%20high-stakes%20applications%20including%20healthcare%2C%20legal%20systems%2C%20and%20financial%20services%2C%20where%20opacity%20hinders%20trust%20and%20accountability.%20Transformers%20self-attention%20mechanisms%20have%20proven%20valuable%20for%20model%20interpretability%2C%20with%20attention%20weights%20successfully%20used%20to%20understand%20model%20focus%20and%20behavior%20%28Xu%20et%20al.%2C%202015%29%3B%20%28Wiegreffe%20and%20Pinter%2C%202019%29.%20However%2C%20existing%20attention-based%20explanation%20methods%20rely%20on%20manually%20defined%20aggregation%20strategies%20and%20fixed%20attribution%20rules%20%28Abnar%20and%20Zuidema%2C%202020a%29%3B%20%28Chefer%20et%20al.%2C%202021%29%2C%20while%20model-agnostic%20approaches%20%28LIME%2C%20SHAP%29%20treat%20the%20model%20as%20a%20black%20box%20and%20incur%20significant%20computational%20costs%20through%20input%20perturbation.%20We%20introduce%20Explanation%20Network%20%28ExpNet%29%2C%20a%20lightweight%20neural%20network%20that%20learns%20an%20explicit%20mapping%20from%20transformer%20attention%20patterns%20to%20token-level%20importance%20scores.%20Unlike%20prior%20methods%2C%20ExpNet%20discovers%20optimal%20attention%20feature%20combinations%20automatically%20rather%20than%20relying%20on%20predetermined%20rules.%20We%20evaluate%20ExpNet%20in%20a%20challenging%20cross-task%20setting%20and%20benchmark%20it%20against%20a%20broad%20spectrum%20of%20model-agnostic%20methods%20and%20attention-based%20techniques%20spanning%20four%20methodological%20families.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14112v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Explain%253A%2520Supervised%2520Token%2520Attribution%2520from%2520Transformer%2520Attention%2520Patterns%26entry.906535625%3DGeorge%2520Mihaila%26entry.1292438233%3DExplainable%2520AI%2520%2528XAI%2529%2520has%2520become%2520critical%2520as%2520transformer-based%2520models%2520are%2520deployed%2520in%2520high-stakes%2520applications%2520including%2520healthcare%252C%2520legal%2520systems%252C%2520and%2520financial%2520services%252C%2520where%2520opacity%2520hinders%2520trust%2520and%2520accountability.%2520Transformers%2520self-attention%2520mechanisms%2520have%2520proven%2520valuable%2520for%2520model%2520interpretability%252C%2520with%2520attention%2520weights%2520successfully%2520used%2520to%2520understand%2520model%2520focus%2520and%2520behavior%2520%2528Xu%2520et%2520al.%252C%25202015%2529%253B%2520%2528Wiegreffe%2520and%2520Pinter%252C%25202019%2529.%2520However%252C%2520existing%2520attention-based%2520explanation%2520methods%2520rely%2520on%2520manually%2520defined%2520aggregation%2520strategies%2520and%2520fixed%2520attribution%2520rules%2520%2528Abnar%2520and%2520Zuidema%252C%25202020a%2529%253B%2520%2528Chefer%2520et%2520al.%252C%25202021%2529%252C%2520while%2520model-agnostic%2520approaches%2520%2528LIME%252C%2520SHAP%2529%2520treat%2520the%2520model%2520as%2520a%2520black%2520box%2520and%2520incur%2520significant%2520computational%2520costs%2520through%2520input%2520perturbation.%2520We%2520introduce%2520Explanation%2520Network%2520%2528ExpNet%2529%252C%2520a%2520lightweight%2520neural%2520network%2520that%2520learns%2520an%2520explicit%2520mapping%2520from%2520transformer%2520attention%2520patterns%2520to%2520token-level%2520importance%2520scores.%2520Unlike%2520prior%2520methods%252C%2520ExpNet%2520discovers%2520optimal%2520attention%2520feature%2520combinations%2520automatically%2520rather%2520than%2520relying%2520on%2520predetermined%2520rules.%2520We%2520evaluate%2520ExpNet%2520in%2520a%2520challenging%2520cross-task%2520setting%2520and%2520benchmark%2520it%2520against%2520a%2520broad%2520spectrum%2520of%2520model-agnostic%2520methods%2520and%2520attention-based%2520techniques%2520spanning%2520four%2520methodological%2520families.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14112v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Explain%3A%20Supervised%20Token%20Attribution%20from%20Transformer%20Attention%20Patterns&entry.906535625=George%20Mihaila&entry.1292438233=Explainable%20AI%20%28XAI%29%20has%20become%20critical%20as%20transformer-based%20models%20are%20deployed%20in%20high-stakes%20applications%20including%20healthcare%2C%20legal%20systems%2C%20and%20financial%20services%2C%20where%20opacity%20hinders%20trust%20and%20accountability.%20Transformers%20self-attention%20mechanisms%20have%20proven%20valuable%20for%20model%20interpretability%2C%20with%20attention%20weights%20successfully%20used%20to%20understand%20model%20focus%20and%20behavior%20%28Xu%20et%20al.%2C%202015%29%3B%20%28Wiegreffe%20and%20Pinter%2C%202019%29.%20However%2C%20existing%20attention-based%20explanation%20methods%20rely%20on%20manually%20defined%20aggregation%20strategies%20and%20fixed%20attribution%20rules%20%28Abnar%20and%20Zuidema%2C%202020a%29%3B%20%28Chefer%20et%20al.%2C%202021%29%2C%20while%20model-agnostic%20approaches%20%28LIME%2C%20SHAP%29%20treat%20the%20model%20as%20a%20black%20box%20and%20incur%20significant%20computational%20costs%20through%20input%20perturbation.%20We%20introduce%20Explanation%20Network%20%28ExpNet%29%2C%20a%20lightweight%20neural%20network%20that%20learns%20an%20explicit%20mapping%20from%20transformer%20attention%20patterns%20to%20token-level%20importance%20scores.%20Unlike%20prior%20methods%2C%20ExpNet%20discovers%20optimal%20attention%20feature%20combinations%20automatically%20rather%20than%20relying%20on%20predetermined%20rules.%20We%20evaluate%20ExpNet%20in%20a%20challenging%20cross-task%20setting%20and%20benchmark%20it%20against%20a%20broad%20spectrum%20of%20model-agnostic%20methods%20and%20attention-based%20techniques%20spanning%20four%20methodological%20families.&entry.1838667208=http%3A//arxiv.org/abs/2601.14112v1&entry.124074799=Read"},
{"title": "UniHash: Unifying Pointwise and Pairwise Hashing Paradigms for Seen and Unseen Category Retrieval", "author": "Xiaoxu Ma and Runhao Li and Xiangbo Zhang and Zhenyu Weng", "abstract": "Effective retrieval across both seen and unseen categories is crucial for modern image retrieval systems. Retrieval on seen categories ensures precise recognition of known classes, while retrieval on unseen categories promotes generalization to novel classes with limited supervision. However, most existing deep hashing methods are confined to a single training paradigm, either pointwise or pairwise, where the former excels on seen categories and the latter generalizes better to unseen ones. To overcome this limitation, we propose Unified Hashing (UniHash), a dual-branch framework that unifies the strengths of both paradigms to achieve balanced retrieval performance across seen and unseen categories. UniHash consists of two complementary branches: a center-based branch following the pointwise paradigm and a pairwise branch following the pairwise paradigm. A novel hash code learning method is introduced to enable bidirectional knowledge transfer between branches, improving hash code discriminability and generalization. It employs a mutual learning loss to align hash representations and introduces a Split-Merge Mixture of Hash Experts (SM-MoH) module to enhance cross-branch exchange of hash representations. Theoretical analysis substantiates the effectiveness of UniHash, and extensive experiments on CIFAR-10, MSCOCO, and ImageNet demonstrate that UniHash consistently achieves state-of-the-art performance in both seen and unseen image retrieval scenarios.", "link": "http://arxiv.org/abs/2601.09828v2", "date": "2026-01-20", "relevancy": 2.3702, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4951}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4672}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniHash%3A%20Unifying%20Pointwise%20and%20Pairwise%20Hashing%20Paradigms%20for%20Seen%20and%20Unseen%20Category%20Retrieval&body=Title%3A%20UniHash%3A%20Unifying%20Pointwise%20and%20Pairwise%20Hashing%20Paradigms%20for%20Seen%20and%20Unseen%20Category%20Retrieval%0AAuthor%3A%20Xiaoxu%20Ma%20and%20Runhao%20Li%20and%20Xiangbo%20Zhang%20and%20Zhenyu%20Weng%0AAbstract%3A%20Effective%20retrieval%20across%20both%20seen%20and%20unseen%20categories%20is%20crucial%20for%20modern%20image%20retrieval%20systems.%20Retrieval%20on%20seen%20categories%20ensures%20precise%20recognition%20of%20known%20classes%2C%20while%20retrieval%20on%20unseen%20categories%20promotes%20generalization%20to%20novel%20classes%20with%20limited%20supervision.%20However%2C%20most%20existing%20deep%20hashing%20methods%20are%20confined%20to%20a%20single%20training%20paradigm%2C%20either%20pointwise%20or%20pairwise%2C%20where%20the%20former%20excels%20on%20seen%20categories%20and%20the%20latter%20generalizes%20better%20to%20unseen%20ones.%20To%20overcome%20this%20limitation%2C%20we%20propose%20Unified%20Hashing%20%28UniHash%29%2C%20a%20dual-branch%20framework%20that%20unifies%20the%20strengths%20of%20both%20paradigms%20to%20achieve%20balanced%20retrieval%20performance%20across%20seen%20and%20unseen%20categories.%20UniHash%20consists%20of%20two%20complementary%20branches%3A%20a%20center-based%20branch%20following%20the%20pointwise%20paradigm%20and%20a%20pairwise%20branch%20following%20the%20pairwise%20paradigm.%20A%20novel%20hash%20code%20learning%20method%20is%20introduced%20to%20enable%20bidirectional%20knowledge%20transfer%20between%20branches%2C%20improving%20hash%20code%20discriminability%20and%20generalization.%20It%20employs%20a%20mutual%20learning%20loss%20to%20align%20hash%20representations%20and%20introduces%20a%20Split-Merge%20Mixture%20of%20Hash%20Experts%20%28SM-MoH%29%20module%20to%20enhance%20cross-branch%20exchange%20of%20hash%20representations.%20Theoretical%20analysis%20substantiates%20the%20effectiveness%20of%20UniHash%2C%20and%20extensive%20experiments%20on%20CIFAR-10%2C%20MSCOCO%2C%20and%20ImageNet%20demonstrate%20that%20UniHash%20consistently%20achieves%20state-of-the-art%20performance%20in%20both%20seen%20and%20unseen%20image%20retrieval%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2601.09828v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniHash%253A%2520Unifying%2520Pointwise%2520and%2520Pairwise%2520Hashing%2520Paradigms%2520for%2520Seen%2520and%2520Unseen%2520Category%2520Retrieval%26entry.906535625%3DXiaoxu%2520Ma%2520and%2520Runhao%2520Li%2520and%2520Xiangbo%2520Zhang%2520and%2520Zhenyu%2520Weng%26entry.1292438233%3DEffective%2520retrieval%2520across%2520both%2520seen%2520and%2520unseen%2520categories%2520is%2520crucial%2520for%2520modern%2520image%2520retrieval%2520systems.%2520Retrieval%2520on%2520seen%2520categories%2520ensures%2520precise%2520recognition%2520of%2520known%2520classes%252C%2520while%2520retrieval%2520on%2520unseen%2520categories%2520promotes%2520generalization%2520to%2520novel%2520classes%2520with%2520limited%2520supervision.%2520However%252C%2520most%2520existing%2520deep%2520hashing%2520methods%2520are%2520confined%2520to%2520a%2520single%2520training%2520paradigm%252C%2520either%2520pointwise%2520or%2520pairwise%252C%2520where%2520the%2520former%2520excels%2520on%2520seen%2520categories%2520and%2520the%2520latter%2520generalizes%2520better%2520to%2520unseen%2520ones.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520Unified%2520Hashing%2520%2528UniHash%2529%252C%2520a%2520dual-branch%2520framework%2520that%2520unifies%2520the%2520strengths%2520of%2520both%2520paradigms%2520to%2520achieve%2520balanced%2520retrieval%2520performance%2520across%2520seen%2520and%2520unseen%2520categories.%2520UniHash%2520consists%2520of%2520two%2520complementary%2520branches%253A%2520a%2520center-based%2520branch%2520following%2520the%2520pointwise%2520paradigm%2520and%2520a%2520pairwise%2520branch%2520following%2520the%2520pairwise%2520paradigm.%2520A%2520novel%2520hash%2520code%2520learning%2520method%2520is%2520introduced%2520to%2520enable%2520bidirectional%2520knowledge%2520transfer%2520between%2520branches%252C%2520improving%2520hash%2520code%2520discriminability%2520and%2520generalization.%2520It%2520employs%2520a%2520mutual%2520learning%2520loss%2520to%2520align%2520hash%2520representations%2520and%2520introduces%2520a%2520Split-Merge%2520Mixture%2520of%2520Hash%2520Experts%2520%2528SM-MoH%2529%2520module%2520to%2520enhance%2520cross-branch%2520exchange%2520of%2520hash%2520representations.%2520Theoretical%2520analysis%2520substantiates%2520the%2520effectiveness%2520of%2520UniHash%252C%2520and%2520extensive%2520experiments%2520on%2520CIFAR-10%252C%2520MSCOCO%252C%2520and%2520ImageNet%2520demonstrate%2520that%2520UniHash%2520consistently%2520achieves%2520state-of-the-art%2520performance%2520in%2520both%2520seen%2520and%2520unseen%2520image%2520retrieval%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.09828v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniHash%3A%20Unifying%20Pointwise%20and%20Pairwise%20Hashing%20Paradigms%20for%20Seen%20and%20Unseen%20Category%20Retrieval&entry.906535625=Xiaoxu%20Ma%20and%20Runhao%20Li%20and%20Xiangbo%20Zhang%20and%20Zhenyu%20Weng&entry.1292438233=Effective%20retrieval%20across%20both%20seen%20and%20unseen%20categories%20is%20crucial%20for%20modern%20image%20retrieval%20systems.%20Retrieval%20on%20seen%20categories%20ensures%20precise%20recognition%20of%20known%20classes%2C%20while%20retrieval%20on%20unseen%20categories%20promotes%20generalization%20to%20novel%20classes%20with%20limited%20supervision.%20However%2C%20most%20existing%20deep%20hashing%20methods%20are%20confined%20to%20a%20single%20training%20paradigm%2C%20either%20pointwise%20or%20pairwise%2C%20where%20the%20former%20excels%20on%20seen%20categories%20and%20the%20latter%20generalizes%20better%20to%20unseen%20ones.%20To%20overcome%20this%20limitation%2C%20we%20propose%20Unified%20Hashing%20%28UniHash%29%2C%20a%20dual-branch%20framework%20that%20unifies%20the%20strengths%20of%20both%20paradigms%20to%20achieve%20balanced%20retrieval%20performance%20across%20seen%20and%20unseen%20categories.%20UniHash%20consists%20of%20two%20complementary%20branches%3A%20a%20center-based%20branch%20following%20the%20pointwise%20paradigm%20and%20a%20pairwise%20branch%20following%20the%20pairwise%20paradigm.%20A%20novel%20hash%20code%20learning%20method%20is%20introduced%20to%20enable%20bidirectional%20knowledge%20transfer%20between%20branches%2C%20improving%20hash%20code%20discriminability%20and%20generalization.%20It%20employs%20a%20mutual%20learning%20loss%20to%20align%20hash%20representations%20and%20introduces%20a%20Split-Merge%20Mixture%20of%20Hash%20Experts%20%28SM-MoH%29%20module%20to%20enhance%20cross-branch%20exchange%20of%20hash%20representations.%20Theoretical%20analysis%20substantiates%20the%20effectiveness%20of%20UniHash%2C%20and%20extensive%20experiments%20on%20CIFAR-10%2C%20MSCOCO%2C%20and%20ImageNet%20demonstrate%20that%20UniHash%20consistently%20achieves%20state-of-the-art%20performance%20in%20both%20seen%20and%20unseen%20image%20retrieval%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2601.09828v2&entry.124074799=Read"},
{"title": "ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models", "author": "Bruno Sienkiewicz and \u0141ukasz Neumann and Mateusz Modrzejewski", "abstract": "Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online.", "link": "http://arxiv.org/abs/2601.14157v1", "date": "2026-01-20", "relevancy": 2.3671, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4818}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4809}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ConceptCaps%20--%20a%20Distilled%20Concept%20Dataset%20for%20Interpretability%20in%20Music%20Models&body=Title%3A%20ConceptCaps%20--%20a%20Distilled%20Concept%20Dataset%20for%20Interpretability%20in%20Music%20Models%0AAuthor%3A%20Bruno%20Sienkiewicz%20and%20%C5%81ukasz%20Neumann%20and%20Mateusz%20Modrzejewski%0AAbstract%3A%20Concept-based%20interpretability%20methods%20like%20TCAV%20require%20clean%2C%20well-separated%20positive%20and%20negative%20examples%20for%20each%20concept.%20Existing%20music%20datasets%20lack%20this%20structure%3A%20tags%20are%20sparse%2C%20noisy%2C%20or%20ill-defined.%20We%20introduce%20ConceptCaps%2C%20a%20dataset%20of%2023k%20music-caption-audio%20triplets%20with%20explicit%20labels%20from%20a%20200-attribute%20taxonomy.%20Our%20pipeline%20separates%20semantic%20modeling%20from%20text%20generation%3A%20a%20VAE%20learns%20plausible%20attribute%20co-occurrence%20patterns%2C%20a%20fine-tuned%20LLM%20converts%20attribute%20lists%20into%20professional%20descriptions%2C%20and%20MusicGen%20synthesizes%20corresponding%20audio.%20This%20separation%20improves%20coherence%20and%20controllability%20over%20end-to-end%20approaches.%20We%20validate%20the%20dataset%20through%20audio-text%20alignment%20%28CLAP%29%2C%20linguistic%20quality%20metrics%20%28BERTScore%2C%20MAUVE%29%2C%20and%20TCAV%20analysis%20confirming%20that%20concept%20probes%20recover%20musically%20meaningful%20patterns.%20Dataset%20and%20code%20are%20available%20online.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14157v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DConceptCaps%2520--%2520a%2520Distilled%2520Concept%2520Dataset%2520for%2520Interpretability%2520in%2520Music%2520Models%26entry.906535625%3DBruno%2520Sienkiewicz%2520and%2520%25C5%2581ukasz%2520Neumann%2520and%2520Mateusz%2520Modrzejewski%26entry.1292438233%3DConcept-based%2520interpretability%2520methods%2520like%2520TCAV%2520require%2520clean%252C%2520well-separated%2520positive%2520and%2520negative%2520examples%2520for%2520each%2520concept.%2520Existing%2520music%2520datasets%2520lack%2520this%2520structure%253A%2520tags%2520are%2520sparse%252C%2520noisy%252C%2520or%2520ill-defined.%2520We%2520introduce%2520ConceptCaps%252C%2520a%2520dataset%2520of%252023k%2520music-caption-audio%2520triplets%2520with%2520explicit%2520labels%2520from%2520a%2520200-attribute%2520taxonomy.%2520Our%2520pipeline%2520separates%2520semantic%2520modeling%2520from%2520text%2520generation%253A%2520a%2520VAE%2520learns%2520plausible%2520attribute%2520co-occurrence%2520patterns%252C%2520a%2520fine-tuned%2520LLM%2520converts%2520attribute%2520lists%2520into%2520professional%2520descriptions%252C%2520and%2520MusicGen%2520synthesizes%2520corresponding%2520audio.%2520This%2520separation%2520improves%2520coherence%2520and%2520controllability%2520over%2520end-to-end%2520approaches.%2520We%2520validate%2520the%2520dataset%2520through%2520audio-text%2520alignment%2520%2528CLAP%2529%252C%2520linguistic%2520quality%2520metrics%2520%2528BERTScore%252C%2520MAUVE%2529%252C%2520and%2520TCAV%2520analysis%2520confirming%2520that%2520concept%2520probes%2520recover%2520musically%2520meaningful%2520patterns.%2520Dataset%2520and%2520code%2520are%2520available%2520online.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14157v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ConceptCaps%20--%20a%20Distilled%20Concept%20Dataset%20for%20Interpretability%20in%20Music%20Models&entry.906535625=Bruno%20Sienkiewicz%20and%20%C5%81ukasz%20Neumann%20and%20Mateusz%20Modrzejewski&entry.1292438233=Concept-based%20interpretability%20methods%20like%20TCAV%20require%20clean%2C%20well-separated%20positive%20and%20negative%20examples%20for%20each%20concept.%20Existing%20music%20datasets%20lack%20this%20structure%3A%20tags%20are%20sparse%2C%20noisy%2C%20or%20ill-defined.%20We%20introduce%20ConceptCaps%2C%20a%20dataset%20of%2023k%20music-caption-audio%20triplets%20with%20explicit%20labels%20from%20a%20200-attribute%20taxonomy.%20Our%20pipeline%20separates%20semantic%20modeling%20from%20text%20generation%3A%20a%20VAE%20learns%20plausible%20attribute%20co-occurrence%20patterns%2C%20a%20fine-tuned%20LLM%20converts%20attribute%20lists%20into%20professional%20descriptions%2C%20and%20MusicGen%20synthesizes%20corresponding%20audio.%20This%20separation%20improves%20coherence%20and%20controllability%20over%20end-to-end%20approaches.%20We%20validate%20the%20dataset%20through%20audio-text%20alignment%20%28CLAP%29%2C%20linguistic%20quality%20metrics%20%28BERTScore%2C%20MAUVE%29%2C%20and%20TCAV%20analysis%20confirming%20that%20concept%20probes%20recover%20musically%20meaningful%20patterns.%20Dataset%20and%20code%20are%20available%20online.&entry.1838667208=http%3A//arxiv.org/abs/2601.14157v1&entry.124074799=Read"},
{"title": "torch-sla: Differentiable Sparse Linear Algebra with Adjoint Solvers and Sparse Tensor Parallelism for PyTorch", "author": "Mingyuan Chi", "abstract": "Industrial scientific computing predominantly uses sparse matrices to represent unstructured data -- finite element meshes, graphs, point clouds. We present \\torchsla{}, an open-source PyTorch library that enables GPU-accelerated, scalable, and differentiable sparse linear algebra. The library addresses three fundamental challenges: (1) GPU acceleration for sparse linear solves, nonlinear solves (Newton, Picard, Anderson), and eigenvalue computation; (2) Multi-GPU scaling via domain decomposition with halo exchange, reaching \\textbf{400 million DOF linear solve on 3 GPUs}; and (3) Adjoint-based differentiation} achieving $\\mathcal{O}(1)$ computational graph nodes (for autograd) and $\\mathcal{O}(\\text{nnz})$ memory -- independent of solver iterations. \\torchsla{} supports multiple backends (SciPy, cuDSS, PyTorch-native) and seamlessly integrates with PyTorch autograd for end-to-end differentiable simulations. Code is available at https://github.com/walkerchi/torch-sla.", "link": "http://arxiv.org/abs/2601.13994v1", "date": "2026-01-20", "relevancy": 2.3558, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5181}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4631}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20torch-sla%3A%20Differentiable%20Sparse%20Linear%20Algebra%20with%20Adjoint%20Solvers%20and%20Sparse%20Tensor%20Parallelism%20for%20PyTorch&body=Title%3A%20torch-sla%3A%20Differentiable%20Sparse%20Linear%20Algebra%20with%20Adjoint%20Solvers%20and%20Sparse%20Tensor%20Parallelism%20for%20PyTorch%0AAuthor%3A%20Mingyuan%20Chi%0AAbstract%3A%20Industrial%20scientific%20computing%20predominantly%20uses%20sparse%20matrices%20to%20represent%20unstructured%20data%20--%20finite%20element%20meshes%2C%20graphs%2C%20point%20clouds.%20We%20present%20%5Ctorchsla%7B%7D%2C%20an%20open-source%20PyTorch%20library%20that%20enables%20GPU-accelerated%2C%20scalable%2C%20and%20differentiable%20sparse%20linear%20algebra.%20The%20library%20addresses%20three%20fundamental%20challenges%3A%20%281%29%20GPU%20acceleration%20for%20sparse%20linear%20solves%2C%20nonlinear%20solves%20%28Newton%2C%20Picard%2C%20Anderson%29%2C%20and%20eigenvalue%20computation%3B%20%282%29%20Multi-GPU%20scaling%20via%20domain%20decomposition%20with%20halo%20exchange%2C%20reaching%20%5Ctextbf%7B400%20million%20DOF%20linear%20solve%20on%203%20GPUs%7D%3B%20and%20%283%29%20Adjoint-based%20differentiation%7D%20achieving%20%24%5Cmathcal%7BO%7D%281%29%24%20computational%20graph%20nodes%20%28for%20autograd%29%20and%20%24%5Cmathcal%7BO%7D%28%5Ctext%7Bnnz%7D%29%24%20memory%20--%20independent%20of%20solver%20iterations.%20%5Ctorchsla%7B%7D%20supports%20multiple%20backends%20%28SciPy%2C%20cuDSS%2C%20PyTorch-native%29%20and%20seamlessly%20integrates%20with%20PyTorch%20autograd%20for%20end-to-end%20differentiable%20simulations.%20Code%20is%20available%20at%20https%3A//github.com/walkerchi/torch-sla.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13994v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Dtorch-sla%253A%2520Differentiable%2520Sparse%2520Linear%2520Algebra%2520with%2520Adjoint%2520Solvers%2520and%2520Sparse%2520Tensor%2520Parallelism%2520for%2520PyTorch%26entry.906535625%3DMingyuan%2520Chi%26entry.1292438233%3DIndustrial%2520scientific%2520computing%2520predominantly%2520uses%2520sparse%2520matrices%2520to%2520represent%2520unstructured%2520data%2520--%2520finite%2520element%2520meshes%252C%2520graphs%252C%2520point%2520clouds.%2520We%2520present%2520%255Ctorchsla%257B%257D%252C%2520an%2520open-source%2520PyTorch%2520library%2520that%2520enables%2520GPU-accelerated%252C%2520scalable%252C%2520and%2520differentiable%2520sparse%2520linear%2520algebra.%2520The%2520library%2520addresses%2520three%2520fundamental%2520challenges%253A%2520%25281%2529%2520GPU%2520acceleration%2520for%2520sparse%2520linear%2520solves%252C%2520nonlinear%2520solves%2520%2528Newton%252C%2520Picard%252C%2520Anderson%2529%252C%2520and%2520eigenvalue%2520computation%253B%2520%25282%2529%2520Multi-GPU%2520scaling%2520via%2520domain%2520decomposition%2520with%2520halo%2520exchange%252C%2520reaching%2520%255Ctextbf%257B400%2520million%2520DOF%2520linear%2520solve%2520on%25203%2520GPUs%257D%253B%2520and%2520%25283%2529%2520Adjoint-based%2520differentiation%257D%2520achieving%2520%2524%255Cmathcal%257BO%257D%25281%2529%2524%2520computational%2520graph%2520nodes%2520%2528for%2520autograd%2529%2520and%2520%2524%255Cmathcal%257BO%257D%2528%255Ctext%257Bnnz%257D%2529%2524%2520memory%2520--%2520independent%2520of%2520solver%2520iterations.%2520%255Ctorchsla%257B%257D%2520supports%2520multiple%2520backends%2520%2528SciPy%252C%2520cuDSS%252C%2520PyTorch-native%2529%2520and%2520seamlessly%2520integrates%2520with%2520PyTorch%2520autograd%2520for%2520end-to-end%2520differentiable%2520simulations.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/walkerchi/torch-sla.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13994v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=torch-sla%3A%20Differentiable%20Sparse%20Linear%20Algebra%20with%20Adjoint%20Solvers%20and%20Sparse%20Tensor%20Parallelism%20for%20PyTorch&entry.906535625=Mingyuan%20Chi&entry.1292438233=Industrial%20scientific%20computing%20predominantly%20uses%20sparse%20matrices%20to%20represent%20unstructured%20data%20--%20finite%20element%20meshes%2C%20graphs%2C%20point%20clouds.%20We%20present%20%5Ctorchsla%7B%7D%2C%20an%20open-source%20PyTorch%20library%20that%20enables%20GPU-accelerated%2C%20scalable%2C%20and%20differentiable%20sparse%20linear%20algebra.%20The%20library%20addresses%20three%20fundamental%20challenges%3A%20%281%29%20GPU%20acceleration%20for%20sparse%20linear%20solves%2C%20nonlinear%20solves%20%28Newton%2C%20Picard%2C%20Anderson%29%2C%20and%20eigenvalue%20computation%3B%20%282%29%20Multi-GPU%20scaling%20via%20domain%20decomposition%20with%20halo%20exchange%2C%20reaching%20%5Ctextbf%7B400%20million%20DOF%20linear%20solve%20on%203%20GPUs%7D%3B%20and%20%283%29%20Adjoint-based%20differentiation%7D%20achieving%20%24%5Cmathcal%7BO%7D%281%29%24%20computational%20graph%20nodes%20%28for%20autograd%29%20and%20%24%5Cmathcal%7BO%7D%28%5Ctext%7Bnnz%7D%29%24%20memory%20--%20independent%20of%20solver%20iterations.%20%5Ctorchsla%7B%7D%20supports%20multiple%20backends%20%28SciPy%2C%20cuDSS%2C%20PyTorch-native%29%20and%20seamlessly%20integrates%20with%20PyTorch%20autograd%20for%20end-to-end%20differentiable%20simulations.%20Code%20is%20available%20at%20https%3A//github.com/walkerchi/torch-sla.&entry.1838667208=http%3A//arxiv.org/abs/2601.13994v1&entry.124074799=Read"},
{"title": "AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning", "author": "Ran Gong and Xiaohan Zhang and Jinghuan Shang and Maria Vittoria Minniti and Jigarkumar Patel and Valerio Pepe and Riedana Yan and Ahmet Gundogdu and Ivan Kapelyukh and Ali Abbas and Xiaoqiang Yan and Harsh Patel and Laura Herlant and Karl Schmeckpeper", "abstract": "Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .", "link": "http://arxiv.org/abs/2512.17853v2", "date": "2026-01-20", "relevancy": 2.3499, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.6158}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5875}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5761}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AnyTask%3A%20an%20Automated%20Task%20and%20Data%20Generation%20Framework%20for%20Advancing%20Sim-to-Real%20Policy%20Learning&body=Title%3A%20AnyTask%3A%20an%20Automated%20Task%20and%20Data%20Generation%20Framework%20for%20Advancing%20Sim-to-Real%20Policy%20Learning%0AAuthor%3A%20Ran%20Gong%20and%20Xiaohan%20Zhang%20and%20Jinghuan%20Shang%20and%20Maria%20Vittoria%20Minniti%20and%20Jigarkumar%20Patel%20and%20Valerio%20Pepe%20and%20Riedana%20Yan%20and%20Ahmet%20Gundogdu%20and%20Ivan%20Kapelyukh%20and%20Ali%20Abbas%20and%20Xiaoqiang%20Yan%20and%20Harsh%20Patel%20and%20Laura%20Herlant%20and%20Karl%20Schmeckpeper%0AAbstract%3A%20Generalist%20robot%20learning%20remains%20constrained%20by%20data%3A%20large-scale%2C%20diverse%2C%20and%20high-quality%20interaction%20data%20are%20expensive%20to%20collect%20in%20the%20real%20world.%20While%20simulation%20has%20become%20a%20promising%20way%20for%20scaling%20up%20data%20collection%2C%20the%20related%20tasks%2C%20including%20simulation%20task%20design%2C%20task-aware%20scene%20generation%2C%20expert%20demonstration%20synthesis%2C%20and%20sim-to-real%20transfer%2C%20still%20demand%20substantial%20human%20effort.%20We%20present%20AnyTask%2C%20an%20automated%20framework%20that%20pairs%20massively%20parallel%20GPU%20simulation%20with%20foundation%20models%20to%20design%20diverse%20manipulation%20tasks%20and%20synthesize%20robot%20data.%20We%20introduce%20three%20AnyTask%20agents%20for%20generating%20expert%20demonstrations%20aiming%20to%20solve%20as%20many%20tasks%20as%20possible%3A%201%29%20ViPR%2C%20a%20novel%20task%20and%20motion%20planning%20agent%20with%20VLM-in-the-loop%20Parallel%20Refinement%3B%202%29%20ViPR-Eureka%2C%20a%20reinforcement%20learning%20agent%20with%20generated%20dense%20rewards%20and%20LLM-guided%20contact%20sampling%3B%203%29%20ViPR-RL%2C%20a%20hybrid%20planning%20and%20learning%20approach%20that%20jointly%20produces%20high-quality%20demonstrations%20with%20only%20sparse%20rewards.%20We%20train%20behavior%20cloning%20policies%20on%20generated%20data%2C%20validate%20them%20in%20simulation%2C%20and%20deploy%20them%20directly%20on%20real%20robot%20hardware.%20The%20policies%20generalize%20to%20novel%20object%20poses%2C%20achieving%2044%25%20average%20success%20across%20a%20suite%20of%20real-world%20pick-and-place%2C%20drawer%20opening%2C%20contact-rich%20pushing%2C%20and%20long-horizon%20manipulation%20tasks.%20Our%20project%20website%20is%20at%20https%3A//anytask.rai-inst.com%20.%0ALink%3A%20http%3A//arxiv.org/abs/2512.17853v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAnyTask%253A%2520an%2520Automated%2520Task%2520and%2520Data%2520Generation%2520Framework%2520for%2520Advancing%2520Sim-to-Real%2520Policy%2520Learning%26entry.906535625%3DRan%2520Gong%2520and%2520Xiaohan%2520Zhang%2520and%2520Jinghuan%2520Shang%2520and%2520Maria%2520Vittoria%2520Minniti%2520and%2520Jigarkumar%2520Patel%2520and%2520Valerio%2520Pepe%2520and%2520Riedana%2520Yan%2520and%2520Ahmet%2520Gundogdu%2520and%2520Ivan%2520Kapelyukh%2520and%2520Ali%2520Abbas%2520and%2520Xiaoqiang%2520Yan%2520and%2520Harsh%2520Patel%2520and%2520Laura%2520Herlant%2520and%2520Karl%2520Schmeckpeper%26entry.1292438233%3DGeneralist%2520robot%2520learning%2520remains%2520constrained%2520by%2520data%253A%2520large-scale%252C%2520diverse%252C%2520and%2520high-quality%2520interaction%2520data%2520are%2520expensive%2520to%2520collect%2520in%2520the%2520real%2520world.%2520While%2520simulation%2520has%2520become%2520a%2520promising%2520way%2520for%2520scaling%2520up%2520data%2520collection%252C%2520the%2520related%2520tasks%252C%2520including%2520simulation%2520task%2520design%252C%2520task-aware%2520scene%2520generation%252C%2520expert%2520demonstration%2520synthesis%252C%2520and%2520sim-to-real%2520transfer%252C%2520still%2520demand%2520substantial%2520human%2520effort.%2520We%2520present%2520AnyTask%252C%2520an%2520automated%2520framework%2520that%2520pairs%2520massively%2520parallel%2520GPU%2520simulation%2520with%2520foundation%2520models%2520to%2520design%2520diverse%2520manipulation%2520tasks%2520and%2520synthesize%2520robot%2520data.%2520We%2520introduce%2520three%2520AnyTask%2520agents%2520for%2520generating%2520expert%2520demonstrations%2520aiming%2520to%2520solve%2520as%2520many%2520tasks%2520as%2520possible%253A%25201%2529%2520ViPR%252C%2520a%2520novel%2520task%2520and%2520motion%2520planning%2520agent%2520with%2520VLM-in-the-loop%2520Parallel%2520Refinement%253B%25202%2529%2520ViPR-Eureka%252C%2520a%2520reinforcement%2520learning%2520agent%2520with%2520generated%2520dense%2520rewards%2520and%2520LLM-guided%2520contact%2520sampling%253B%25203%2529%2520ViPR-RL%252C%2520a%2520hybrid%2520planning%2520and%2520learning%2520approach%2520that%2520jointly%2520produces%2520high-quality%2520demonstrations%2520with%2520only%2520sparse%2520rewards.%2520We%2520train%2520behavior%2520cloning%2520policies%2520on%2520generated%2520data%252C%2520validate%2520them%2520in%2520simulation%252C%2520and%2520deploy%2520them%2520directly%2520on%2520real%2520robot%2520hardware.%2520The%2520policies%2520generalize%2520to%2520novel%2520object%2520poses%252C%2520achieving%252044%2525%2520average%2520success%2520across%2520a%2520suite%2520of%2520real-world%2520pick-and-place%252C%2520drawer%2520opening%252C%2520contact-rich%2520pushing%252C%2520and%2520long-horizon%2520manipulation%2520tasks.%2520Our%2520project%2520website%2520is%2520at%2520https%253A//anytask.rai-inst.com%2520.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.17853v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AnyTask%3A%20an%20Automated%20Task%20and%20Data%20Generation%20Framework%20for%20Advancing%20Sim-to-Real%20Policy%20Learning&entry.906535625=Ran%20Gong%20and%20Xiaohan%20Zhang%20and%20Jinghuan%20Shang%20and%20Maria%20Vittoria%20Minniti%20and%20Jigarkumar%20Patel%20and%20Valerio%20Pepe%20and%20Riedana%20Yan%20and%20Ahmet%20Gundogdu%20and%20Ivan%20Kapelyukh%20and%20Ali%20Abbas%20and%20Xiaoqiang%20Yan%20and%20Harsh%20Patel%20and%20Laura%20Herlant%20and%20Karl%20Schmeckpeper&entry.1292438233=Generalist%20robot%20learning%20remains%20constrained%20by%20data%3A%20large-scale%2C%20diverse%2C%20and%20high-quality%20interaction%20data%20are%20expensive%20to%20collect%20in%20the%20real%20world.%20While%20simulation%20has%20become%20a%20promising%20way%20for%20scaling%20up%20data%20collection%2C%20the%20related%20tasks%2C%20including%20simulation%20task%20design%2C%20task-aware%20scene%20generation%2C%20expert%20demonstration%20synthesis%2C%20and%20sim-to-real%20transfer%2C%20still%20demand%20substantial%20human%20effort.%20We%20present%20AnyTask%2C%20an%20automated%20framework%20that%20pairs%20massively%20parallel%20GPU%20simulation%20with%20foundation%20models%20to%20design%20diverse%20manipulation%20tasks%20and%20synthesize%20robot%20data.%20We%20introduce%20three%20AnyTask%20agents%20for%20generating%20expert%20demonstrations%20aiming%20to%20solve%20as%20many%20tasks%20as%20possible%3A%201%29%20ViPR%2C%20a%20novel%20task%20and%20motion%20planning%20agent%20with%20VLM-in-the-loop%20Parallel%20Refinement%3B%202%29%20ViPR-Eureka%2C%20a%20reinforcement%20learning%20agent%20with%20generated%20dense%20rewards%20and%20LLM-guided%20contact%20sampling%3B%203%29%20ViPR-RL%2C%20a%20hybrid%20planning%20and%20learning%20approach%20that%20jointly%20produces%20high-quality%20demonstrations%20with%20only%20sparse%20rewards.%20We%20train%20behavior%20cloning%20policies%20on%20generated%20data%2C%20validate%20them%20in%20simulation%2C%20and%20deploy%20them%20directly%20on%20real%20robot%20hardware.%20The%20policies%20generalize%20to%20novel%20object%20poses%2C%20achieving%2044%25%20average%20success%20across%20a%20suite%20of%20real-world%20pick-and-place%2C%20drawer%20opening%2C%20contact-rich%20pushing%2C%20and%20long-horizon%20manipulation%20tasks.%20Our%20project%20website%20is%20at%20https%3A//anytask.rai-inst.com%20.&entry.1838667208=http%3A//arxiv.org/abs/2512.17853v2&entry.124074799=Read"},
{"title": "Joint Discriminative-Generative Modeling via Dual Adversarial Training", "author": "Xuwang Yin and Claire Zhang and Julie Steele and Nir Shavit and Tony T. Wang", "abstract": "Simultaneously achieving robust classification and high-fidelity generative modeling within a single framework presents a significant challenge. Hybrid approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as EBMs but are often limited by the instability and poor sample quality inherent in Stochastic Gradient Langevin Dynamics (SGLD)-based training. We address these limitations by proposing a novel training framework that integrates adversarial training (AT) principles for both discriminative robustness and stable generative learning. The proposed method introduces three key innovations: (1) the replacement of SGLD-based JEM learning with a stable, AT-based approach that optimizes the energy function by discriminating between real data and Projected Gradient Descent (PGD)-generated contrastive samples using the BCE loss; (2) synergistic adversarial training for the discriminative component that enhances classification robustness while eliminating the need for explicit gradient penalties; and (3) a two-stage training strategy that addresses normalization-related instabilities and enables leveraging pretrained robust classifiers, generalizing effectively across diverse architectures. Experiments on CIFAR-10/100 and ImageNet demonstrate that our approach: (1) is the first EBM-based hybrid to scale to high-resolution datasets with high training stability, simultaneously achieving state-of-the-art discriminative and generative performance on ImageNet 256$\\times$256; (2) uniquely combines generative quality with adversarial robustness, enabling critical applications like robust counterfactual explanations; and (3) functions as a competitive standalone generative model, matching the generative quality of autoregressive methods (VAR-d16) and surpassing diffusion models while offering unique versatility.", "link": "http://arxiv.org/abs/2510.13872v3", "date": "2026-01-20", "relevancy": 2.3349, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5974}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5756}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5733}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Joint%20Discriminative-Generative%20Modeling%20via%20Dual%20Adversarial%20Training&body=Title%3A%20Joint%20Discriminative-Generative%20Modeling%20via%20Dual%20Adversarial%20Training%0AAuthor%3A%20Xuwang%20Yin%20and%20Claire%20Zhang%20and%20Julie%20Steele%20and%20Nir%20Shavit%20and%20Tony%20T.%20Wang%0AAbstract%3A%20Simultaneously%20achieving%20robust%20classification%20and%20high-fidelity%20generative%20modeling%20within%20a%20single%20framework%20presents%20a%20significant%20challenge.%20Hybrid%20approaches%2C%20such%20as%20Joint%20Energy-Based%20Models%20%28JEM%29%2C%20interpret%20classifiers%20as%20EBMs%20but%20are%20often%20limited%20by%20the%20instability%20and%20poor%20sample%20quality%20inherent%20in%20Stochastic%20Gradient%20Langevin%20Dynamics%20%28SGLD%29-based%20training.%20We%20address%20these%20limitations%20by%20proposing%20a%20novel%20training%20framework%20that%20integrates%20adversarial%20training%20%28AT%29%20principles%20for%20both%20discriminative%20robustness%20and%20stable%20generative%20learning.%20The%20proposed%20method%20introduces%20three%20key%20innovations%3A%20%281%29%20the%20replacement%20of%20SGLD-based%20JEM%20learning%20with%20a%20stable%2C%20AT-based%20approach%20that%20optimizes%20the%20energy%20function%20by%20discriminating%20between%20real%20data%20and%20Projected%20Gradient%20Descent%20%28PGD%29-generated%20contrastive%20samples%20using%20the%20BCE%20loss%3B%20%282%29%20synergistic%20adversarial%20training%20for%20the%20discriminative%20component%20that%20enhances%20classification%20robustness%20while%20eliminating%20the%20need%20for%20explicit%20gradient%20penalties%3B%20and%20%283%29%20a%20two-stage%20training%20strategy%20that%20addresses%20normalization-related%20instabilities%20and%20enables%20leveraging%20pretrained%20robust%20classifiers%2C%20generalizing%20effectively%20across%20diverse%20architectures.%20Experiments%20on%20CIFAR-10/100%20and%20ImageNet%20demonstrate%20that%20our%20approach%3A%20%281%29%20is%20the%20first%20EBM-based%20hybrid%20to%20scale%20to%20high-resolution%20datasets%20with%20high%20training%20stability%2C%20simultaneously%20achieving%20state-of-the-art%20discriminative%20and%20generative%20performance%20on%20ImageNet%20256%24%5Ctimes%24256%3B%20%282%29%20uniquely%20combines%20generative%20quality%20with%20adversarial%20robustness%2C%20enabling%20critical%20applications%20like%20robust%20counterfactual%20explanations%3B%20and%20%283%29%20functions%20as%20a%20competitive%20standalone%20generative%20model%2C%20matching%20the%20generative%20quality%20of%20autoregressive%20methods%20%28VAR-d16%29%20and%20surpassing%20diffusion%20models%20while%20offering%20unique%20versatility.%0ALink%3A%20http%3A//arxiv.org/abs/2510.13872v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DJoint%2520Discriminative-Generative%2520Modeling%2520via%2520Dual%2520Adversarial%2520Training%26entry.906535625%3DXuwang%2520Yin%2520and%2520Claire%2520Zhang%2520and%2520Julie%2520Steele%2520and%2520Nir%2520Shavit%2520and%2520Tony%2520T.%2520Wang%26entry.1292438233%3DSimultaneously%2520achieving%2520robust%2520classification%2520and%2520high-fidelity%2520generative%2520modeling%2520within%2520a%2520single%2520framework%2520presents%2520a%2520significant%2520challenge.%2520Hybrid%2520approaches%252C%2520such%2520as%2520Joint%2520Energy-Based%2520Models%2520%2528JEM%2529%252C%2520interpret%2520classifiers%2520as%2520EBMs%2520but%2520are%2520often%2520limited%2520by%2520the%2520instability%2520and%2520poor%2520sample%2520quality%2520inherent%2520in%2520Stochastic%2520Gradient%2520Langevin%2520Dynamics%2520%2528SGLD%2529-based%2520training.%2520We%2520address%2520these%2520limitations%2520by%2520proposing%2520a%2520novel%2520training%2520framework%2520that%2520integrates%2520adversarial%2520training%2520%2528AT%2529%2520principles%2520for%2520both%2520discriminative%2520robustness%2520and%2520stable%2520generative%2520learning.%2520The%2520proposed%2520method%2520introduces%2520three%2520key%2520innovations%253A%2520%25281%2529%2520the%2520replacement%2520of%2520SGLD-based%2520JEM%2520learning%2520with%2520a%2520stable%252C%2520AT-based%2520approach%2520that%2520optimizes%2520the%2520energy%2520function%2520by%2520discriminating%2520between%2520real%2520data%2520and%2520Projected%2520Gradient%2520Descent%2520%2528PGD%2529-generated%2520contrastive%2520samples%2520using%2520the%2520BCE%2520loss%253B%2520%25282%2529%2520synergistic%2520adversarial%2520training%2520for%2520the%2520discriminative%2520component%2520that%2520enhances%2520classification%2520robustness%2520while%2520eliminating%2520the%2520need%2520for%2520explicit%2520gradient%2520penalties%253B%2520and%2520%25283%2529%2520a%2520two-stage%2520training%2520strategy%2520that%2520addresses%2520normalization-related%2520instabilities%2520and%2520enables%2520leveraging%2520pretrained%2520robust%2520classifiers%252C%2520generalizing%2520effectively%2520across%2520diverse%2520architectures.%2520Experiments%2520on%2520CIFAR-10/100%2520and%2520ImageNet%2520demonstrate%2520that%2520our%2520approach%253A%2520%25281%2529%2520is%2520the%2520first%2520EBM-based%2520hybrid%2520to%2520scale%2520to%2520high-resolution%2520datasets%2520with%2520high%2520training%2520stability%252C%2520simultaneously%2520achieving%2520state-of-the-art%2520discriminative%2520and%2520generative%2520performance%2520on%2520ImageNet%2520256%2524%255Ctimes%2524256%253B%2520%25282%2529%2520uniquely%2520combines%2520generative%2520quality%2520with%2520adversarial%2520robustness%252C%2520enabling%2520critical%2520applications%2520like%2520robust%2520counterfactual%2520explanations%253B%2520and%2520%25283%2529%2520functions%2520as%2520a%2520competitive%2520standalone%2520generative%2520model%252C%2520matching%2520the%2520generative%2520quality%2520of%2520autoregressive%2520methods%2520%2528VAR-d16%2529%2520and%2520surpassing%2520diffusion%2520models%2520while%2520offering%2520unique%2520versatility.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.13872v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Joint%20Discriminative-Generative%20Modeling%20via%20Dual%20Adversarial%20Training&entry.906535625=Xuwang%20Yin%20and%20Claire%20Zhang%20and%20Julie%20Steele%20and%20Nir%20Shavit%20and%20Tony%20T.%20Wang&entry.1292438233=Simultaneously%20achieving%20robust%20classification%20and%20high-fidelity%20generative%20modeling%20within%20a%20single%20framework%20presents%20a%20significant%20challenge.%20Hybrid%20approaches%2C%20such%20as%20Joint%20Energy-Based%20Models%20%28JEM%29%2C%20interpret%20classifiers%20as%20EBMs%20but%20are%20often%20limited%20by%20the%20instability%20and%20poor%20sample%20quality%20inherent%20in%20Stochastic%20Gradient%20Langevin%20Dynamics%20%28SGLD%29-based%20training.%20We%20address%20these%20limitations%20by%20proposing%20a%20novel%20training%20framework%20that%20integrates%20adversarial%20training%20%28AT%29%20principles%20for%20both%20discriminative%20robustness%20and%20stable%20generative%20learning.%20The%20proposed%20method%20introduces%20three%20key%20innovations%3A%20%281%29%20the%20replacement%20of%20SGLD-based%20JEM%20learning%20with%20a%20stable%2C%20AT-based%20approach%20that%20optimizes%20the%20energy%20function%20by%20discriminating%20between%20real%20data%20and%20Projected%20Gradient%20Descent%20%28PGD%29-generated%20contrastive%20samples%20using%20the%20BCE%20loss%3B%20%282%29%20synergistic%20adversarial%20training%20for%20the%20discriminative%20component%20that%20enhances%20classification%20robustness%20while%20eliminating%20the%20need%20for%20explicit%20gradient%20penalties%3B%20and%20%283%29%20a%20two-stage%20training%20strategy%20that%20addresses%20normalization-related%20instabilities%20and%20enables%20leveraging%20pretrained%20robust%20classifiers%2C%20generalizing%20effectively%20across%20diverse%20architectures.%20Experiments%20on%20CIFAR-10/100%20and%20ImageNet%20demonstrate%20that%20our%20approach%3A%20%281%29%20is%20the%20first%20EBM-based%20hybrid%20to%20scale%20to%20high-resolution%20datasets%20with%20high%20training%20stability%2C%20simultaneously%20achieving%20state-of-the-art%20discriminative%20and%20generative%20performance%20on%20ImageNet%20256%24%5Ctimes%24256%3B%20%282%29%20uniquely%20combines%20generative%20quality%20with%20adversarial%20robustness%2C%20enabling%20critical%20applications%20like%20robust%20counterfactual%20explanations%3B%20and%20%283%29%20functions%20as%20a%20competitive%20standalone%20generative%20model%2C%20matching%20the%20generative%20quality%20of%20autoregressive%20methods%20%28VAR-d16%29%20and%20surpassing%20diffusion%20models%20while%20offering%20unique%20versatility.&entry.1838667208=http%3A//arxiv.org/abs/2510.13872v3&entry.124074799=Read"},
{"title": "GeLoc3r: Enhancing Relative Camera Pose Regression with Geometric Consistency Regularization", "author": "Jingxing Li and Yongjae Lee and Deliang Fan", "abstract": "Prior ReLoc3R achieves breakthrough performance with fast 25ms inference and state-of-the-art regression accuracy, yet our analysis reveals subtle geometric inconsistencies in its internal representations that prevent reaching the precision ceiling of correspondence-based methods like MASt3R (which require 300ms per pair). In this work, we present GeLoc3r, a novel approach to relative camera pose estimation that enhances pose regression methods through Geometric Consistency Regularization (GCR). GeLoc3r overcomes the speed-accuracy dilemma by training regression networks to produce geometrically consistent poses without inference-time geometric computation. During training, GeLoc3r leverages ground-truth depth to generate dense 3D-2D correspondences, weights them using a FusionTransformer that learns correspondence importance, and computes geometrically-consistent poses via weighted RANSAC. This creates a consistency loss that transfers geometric knowledge into the regression network. Unlike FAR method which requires both regression and geometric solving at inference, GeLoc3r only uses the enhanced regression head at test time, maintaining ReLoc3R's fast speed and approaching MASt3R's high accuracy. On challenging benchmarks, GeLoc3r consistently outperforms ReLoc3R, achieving significant improvements including 40.45% vs. 34.85% AUC@5\u00b0 on the CO3Dv2 dataset (16% relative improvement), 68.66% vs. 66.70% AUC@5\u00b0 on RealEstate10K, and 50.45% vs. 49.60% on MegaDepth1500. By teaching geometric consistency during training rather than enforcing it at inference, GeLoc3r represents a paradigm shift in how neural networks learn camera geometry, achieving both the speed of regression and the geometric understanding of correspondence methods.", "link": "http://arxiv.org/abs/2509.23038v2", "date": "2026-01-20", "relevancy": 2.3302, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6026}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5789}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GeLoc3r%3A%20Enhancing%20Relative%20Camera%20Pose%20Regression%20with%20Geometric%20Consistency%20Regularization&body=Title%3A%20GeLoc3r%3A%20Enhancing%20Relative%20Camera%20Pose%20Regression%20with%20Geometric%20Consistency%20Regularization%0AAuthor%3A%20Jingxing%20Li%20and%20Yongjae%20Lee%20and%20Deliang%20Fan%0AAbstract%3A%20Prior%20ReLoc3R%20achieves%20breakthrough%20performance%20with%20fast%2025ms%20inference%20and%20state-of-the-art%20regression%20accuracy%2C%20yet%20our%20analysis%20reveals%20subtle%20geometric%20inconsistencies%20in%20its%20internal%20representations%20that%20prevent%20reaching%20the%20precision%20ceiling%20of%20correspondence-based%20methods%20like%20MASt3R%20%28which%20require%20300ms%20per%20pair%29.%20In%20this%20work%2C%20we%20present%20GeLoc3r%2C%20a%20novel%20approach%20to%20relative%20camera%20pose%20estimation%20that%20enhances%20pose%20regression%20methods%20through%20Geometric%20Consistency%20Regularization%20%28GCR%29.%20GeLoc3r%20overcomes%20the%20speed-accuracy%20dilemma%20by%20training%20regression%20networks%20to%20produce%20geometrically%20consistent%20poses%20without%20inference-time%20geometric%20computation.%20During%20training%2C%20GeLoc3r%20leverages%20ground-truth%20depth%20to%20generate%20dense%203D-2D%20correspondences%2C%20weights%20them%20using%20a%20FusionTransformer%20that%20learns%20correspondence%20importance%2C%20and%20computes%20geometrically-consistent%20poses%20via%20weighted%20RANSAC.%20This%20creates%20a%20consistency%20loss%20that%20transfers%20geometric%20knowledge%20into%20the%20regression%20network.%20Unlike%20FAR%20method%20which%20requires%20both%20regression%20and%20geometric%20solving%20at%20inference%2C%20GeLoc3r%20only%20uses%20the%20enhanced%20regression%20head%20at%20test%20time%2C%20maintaining%20ReLoc3R%27s%20fast%20speed%20and%20approaching%20MASt3R%27s%20high%20accuracy.%20On%20challenging%20benchmarks%2C%20GeLoc3r%20consistently%20outperforms%20ReLoc3R%2C%20achieving%20significant%20improvements%20including%2040.45%25%20vs.%2034.85%25%20AUC%405%C2%B0%20on%20the%20CO3Dv2%20dataset%20%2816%25%20relative%20improvement%29%2C%2068.66%25%20vs.%2066.70%25%20AUC%405%C2%B0%20on%20RealEstate10K%2C%20and%2050.45%25%20vs.%2049.60%25%20on%20MegaDepth1500.%20By%20teaching%20geometric%20consistency%20during%20training%20rather%20than%20enforcing%20it%20at%20inference%2C%20GeLoc3r%20represents%20a%20paradigm%20shift%20in%20how%20neural%20networks%20learn%20camera%20geometry%2C%20achieving%20both%20the%20speed%20of%20regression%20and%20the%20geometric%20understanding%20of%20correspondence%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2509.23038v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeLoc3r%253A%2520Enhancing%2520Relative%2520Camera%2520Pose%2520Regression%2520with%2520Geometric%2520Consistency%2520Regularization%26entry.906535625%3DJingxing%2520Li%2520and%2520Yongjae%2520Lee%2520and%2520Deliang%2520Fan%26entry.1292438233%3DPrior%2520ReLoc3R%2520achieves%2520breakthrough%2520performance%2520with%2520fast%252025ms%2520inference%2520and%2520state-of-the-art%2520regression%2520accuracy%252C%2520yet%2520our%2520analysis%2520reveals%2520subtle%2520geometric%2520inconsistencies%2520in%2520its%2520internal%2520representations%2520that%2520prevent%2520reaching%2520the%2520precision%2520ceiling%2520of%2520correspondence-based%2520methods%2520like%2520MASt3R%2520%2528which%2520require%2520300ms%2520per%2520pair%2529.%2520In%2520this%2520work%252C%2520we%2520present%2520GeLoc3r%252C%2520a%2520novel%2520approach%2520to%2520relative%2520camera%2520pose%2520estimation%2520that%2520enhances%2520pose%2520regression%2520methods%2520through%2520Geometric%2520Consistency%2520Regularization%2520%2528GCR%2529.%2520GeLoc3r%2520overcomes%2520the%2520speed-accuracy%2520dilemma%2520by%2520training%2520regression%2520networks%2520to%2520produce%2520geometrically%2520consistent%2520poses%2520without%2520inference-time%2520geometric%2520computation.%2520During%2520training%252C%2520GeLoc3r%2520leverages%2520ground-truth%2520depth%2520to%2520generate%2520dense%25203D-2D%2520correspondences%252C%2520weights%2520them%2520using%2520a%2520FusionTransformer%2520that%2520learns%2520correspondence%2520importance%252C%2520and%2520computes%2520geometrically-consistent%2520poses%2520via%2520weighted%2520RANSAC.%2520This%2520creates%2520a%2520consistency%2520loss%2520that%2520transfers%2520geometric%2520knowledge%2520into%2520the%2520regression%2520network.%2520Unlike%2520FAR%2520method%2520which%2520requires%2520both%2520regression%2520and%2520geometric%2520solving%2520at%2520inference%252C%2520GeLoc3r%2520only%2520uses%2520the%2520enhanced%2520regression%2520head%2520at%2520test%2520time%252C%2520maintaining%2520ReLoc3R%2527s%2520fast%2520speed%2520and%2520approaching%2520MASt3R%2527s%2520high%2520accuracy.%2520On%2520challenging%2520benchmarks%252C%2520GeLoc3r%2520consistently%2520outperforms%2520ReLoc3R%252C%2520achieving%2520significant%2520improvements%2520including%252040.45%2525%2520vs.%252034.85%2525%2520AUC%25405%25C2%25B0%2520on%2520the%2520CO3Dv2%2520dataset%2520%252816%2525%2520relative%2520improvement%2529%252C%252068.66%2525%2520vs.%252066.70%2525%2520AUC%25405%25C2%25B0%2520on%2520RealEstate10K%252C%2520and%252050.45%2525%2520vs.%252049.60%2525%2520on%2520MegaDepth1500.%2520By%2520teaching%2520geometric%2520consistency%2520during%2520training%2520rather%2520than%2520enforcing%2520it%2520at%2520inference%252C%2520GeLoc3r%2520represents%2520a%2520paradigm%2520shift%2520in%2520how%2520neural%2520networks%2520learn%2520camera%2520geometry%252C%2520achieving%2520both%2520the%2520speed%2520of%2520regression%2520and%2520the%2520geometric%2520understanding%2520of%2520correspondence%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.23038v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GeLoc3r%3A%20Enhancing%20Relative%20Camera%20Pose%20Regression%20with%20Geometric%20Consistency%20Regularization&entry.906535625=Jingxing%20Li%20and%20Yongjae%20Lee%20and%20Deliang%20Fan&entry.1292438233=Prior%20ReLoc3R%20achieves%20breakthrough%20performance%20with%20fast%2025ms%20inference%20and%20state-of-the-art%20regression%20accuracy%2C%20yet%20our%20analysis%20reveals%20subtle%20geometric%20inconsistencies%20in%20its%20internal%20representations%20that%20prevent%20reaching%20the%20precision%20ceiling%20of%20correspondence-based%20methods%20like%20MASt3R%20%28which%20require%20300ms%20per%20pair%29.%20In%20this%20work%2C%20we%20present%20GeLoc3r%2C%20a%20novel%20approach%20to%20relative%20camera%20pose%20estimation%20that%20enhances%20pose%20regression%20methods%20through%20Geometric%20Consistency%20Regularization%20%28GCR%29.%20GeLoc3r%20overcomes%20the%20speed-accuracy%20dilemma%20by%20training%20regression%20networks%20to%20produce%20geometrically%20consistent%20poses%20without%20inference-time%20geometric%20computation.%20During%20training%2C%20GeLoc3r%20leverages%20ground-truth%20depth%20to%20generate%20dense%203D-2D%20correspondences%2C%20weights%20them%20using%20a%20FusionTransformer%20that%20learns%20correspondence%20importance%2C%20and%20computes%20geometrically-consistent%20poses%20via%20weighted%20RANSAC.%20This%20creates%20a%20consistency%20loss%20that%20transfers%20geometric%20knowledge%20into%20the%20regression%20network.%20Unlike%20FAR%20method%20which%20requires%20both%20regression%20and%20geometric%20solving%20at%20inference%2C%20GeLoc3r%20only%20uses%20the%20enhanced%20regression%20head%20at%20test%20time%2C%20maintaining%20ReLoc3R%27s%20fast%20speed%20and%20approaching%20MASt3R%27s%20high%20accuracy.%20On%20challenging%20benchmarks%2C%20GeLoc3r%20consistently%20outperforms%20ReLoc3R%2C%20achieving%20significant%20improvements%20including%2040.45%25%20vs.%2034.85%25%20AUC%405%C2%B0%20on%20the%20CO3Dv2%20dataset%20%2816%25%20relative%20improvement%29%2C%2068.66%25%20vs.%2066.70%25%20AUC%405%C2%B0%20on%20RealEstate10K%2C%20and%2050.45%25%20vs.%2049.60%25%20on%20MegaDepth1500.%20By%20teaching%20geometric%20consistency%20during%20training%20rather%20than%20enforcing%20it%20at%20inference%2C%20GeLoc3r%20represents%20a%20paradigm%20shift%20in%20how%20neural%20networks%20learn%20camera%20geometry%2C%20achieving%20both%20the%20speed%20of%20regression%20and%20the%20geometric%20understanding%20of%20correspondence%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2509.23038v2&entry.124074799=Read"},
{"title": "DocReward: A Document Reward Model for Structuring and Stylizing", "author": "Junpeng Liu and Yuzhong Zhao and Bowen Cao and Jiayu Ding and Yilin Jia and Tengchao Lv and Yupan Huang and Shaohan Huang and Nan Yang and Li Dong and Lei Cui and Tao Ge and Xun Wang and Huitian Jiao and Sun Mao and FNU Kartik and Si-Qing Chen and Wai Lam and Furu Wei", "abstract": "Recent advances in agentic workflows have enabled the automation of tasks such as professional document generation. However, they primarily focus on textual quality, neglecting visual structure and style, which are crucial for readability and engagement. This gap stems mainly from a lack of effective reward models capable of guiding agents toward producing documents with high structural and stylistic professionalism. To address this, we propose DocReward, a document reward model that evaluates documents based on their structure and style. The model is trained under a textual-quality-agnostic framework to assess professionalism without being influenced by textual quality. To achieve this, we construct a multi-domain dataset DocPair of 117K paired documents, covering 32 domains and 267 document types, each comprising a high- and low-professionalism document with identical content but different structure and style. This setup enables the model to evaluate professionalism comprehensively and independently of textual quality. DocReward is trained using the Bradley-Terry loss to score documents, penalizing predictions that contradict the annotated ranking. On a manually annotated benchmark, DocReward outperforms GPT-5 by 14.6 percentage points in accuracy. Extrinsic RL experiments further validate its effectiveness in guiding professional document generation.", "link": "http://arxiv.org/abs/2510.11391v2", "date": "2026-01-20", "relevancy": 2.3215, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4765}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4656}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DocReward%3A%20A%20Document%20Reward%20Model%20for%20Structuring%20and%20Stylizing&body=Title%3A%20DocReward%3A%20A%20Document%20Reward%20Model%20for%20Structuring%20and%20Stylizing%0AAuthor%3A%20Junpeng%20Liu%20and%20Yuzhong%20Zhao%20and%20Bowen%20Cao%20and%20Jiayu%20Ding%20and%20Yilin%20Jia%20and%20Tengchao%20Lv%20and%20Yupan%20Huang%20and%20Shaohan%20Huang%20and%20Nan%20Yang%20and%20Li%20Dong%20and%20Lei%20Cui%20and%20Tao%20Ge%20and%20Xun%20Wang%20and%20Huitian%20Jiao%20and%20Sun%20Mao%20and%20FNU%20Kartik%20and%20Si-Qing%20Chen%20and%20Wai%20Lam%20and%20Furu%20Wei%0AAbstract%3A%20Recent%20advances%20in%20agentic%20workflows%20have%20enabled%20the%20automation%20of%20tasks%20such%20as%20professional%20document%20generation.%20However%2C%20they%20primarily%20focus%20on%20textual%20quality%2C%20neglecting%20visual%20structure%20and%20style%2C%20which%20are%20crucial%20for%20readability%20and%20engagement.%20This%20gap%20stems%20mainly%20from%20a%20lack%20of%20effective%20reward%20models%20capable%20of%20guiding%20agents%20toward%20producing%20documents%20with%20high%20structural%20and%20stylistic%20professionalism.%20To%20address%20this%2C%20we%20propose%20DocReward%2C%20a%20document%20reward%20model%20that%20evaluates%20documents%20based%20on%20their%20structure%20and%20style.%20The%20model%20is%20trained%20under%20a%20textual-quality-agnostic%20framework%20to%20assess%20professionalism%20without%20being%20influenced%20by%20textual%20quality.%20To%20achieve%20this%2C%20we%20construct%20a%20multi-domain%20dataset%20DocPair%20of%20117K%20paired%20documents%2C%20covering%2032%20domains%20and%20267%20document%20types%2C%20each%20comprising%20a%20high-%20and%20low-professionalism%20document%20with%20identical%20content%20but%20different%20structure%20and%20style.%20This%20setup%20enables%20the%20model%20to%20evaluate%20professionalism%20comprehensively%20and%20independently%20of%20textual%20quality.%20DocReward%20is%20trained%20using%20the%20Bradley-Terry%20loss%20to%20score%20documents%2C%20penalizing%20predictions%20that%20contradict%20the%20annotated%20ranking.%20On%20a%20manually%20annotated%20benchmark%2C%20DocReward%20outperforms%20GPT-5%20by%2014.6%20percentage%20points%20in%20accuracy.%20Extrinsic%20RL%20experiments%20further%20validate%20its%20effectiveness%20in%20guiding%20professional%20document%20generation.%0ALink%3A%20http%3A//arxiv.org/abs/2510.11391v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDocReward%253A%2520A%2520Document%2520Reward%2520Model%2520for%2520Structuring%2520and%2520Stylizing%26entry.906535625%3DJunpeng%2520Liu%2520and%2520Yuzhong%2520Zhao%2520and%2520Bowen%2520Cao%2520and%2520Jiayu%2520Ding%2520and%2520Yilin%2520Jia%2520and%2520Tengchao%2520Lv%2520and%2520Yupan%2520Huang%2520and%2520Shaohan%2520Huang%2520and%2520Nan%2520Yang%2520and%2520Li%2520Dong%2520and%2520Lei%2520Cui%2520and%2520Tao%2520Ge%2520and%2520Xun%2520Wang%2520and%2520Huitian%2520Jiao%2520and%2520Sun%2520Mao%2520and%2520FNU%2520Kartik%2520and%2520Si-Qing%2520Chen%2520and%2520Wai%2520Lam%2520and%2520Furu%2520Wei%26entry.1292438233%3DRecent%2520advances%2520in%2520agentic%2520workflows%2520have%2520enabled%2520the%2520automation%2520of%2520tasks%2520such%2520as%2520professional%2520document%2520generation.%2520However%252C%2520they%2520primarily%2520focus%2520on%2520textual%2520quality%252C%2520neglecting%2520visual%2520structure%2520and%2520style%252C%2520which%2520are%2520crucial%2520for%2520readability%2520and%2520engagement.%2520This%2520gap%2520stems%2520mainly%2520from%2520a%2520lack%2520of%2520effective%2520reward%2520models%2520capable%2520of%2520guiding%2520agents%2520toward%2520producing%2520documents%2520with%2520high%2520structural%2520and%2520stylistic%2520professionalism.%2520To%2520address%2520this%252C%2520we%2520propose%2520DocReward%252C%2520a%2520document%2520reward%2520model%2520that%2520evaluates%2520documents%2520based%2520on%2520their%2520structure%2520and%2520style.%2520The%2520model%2520is%2520trained%2520under%2520a%2520textual-quality-agnostic%2520framework%2520to%2520assess%2520professionalism%2520without%2520being%2520influenced%2520by%2520textual%2520quality.%2520To%2520achieve%2520this%252C%2520we%2520construct%2520a%2520multi-domain%2520dataset%2520DocPair%2520of%2520117K%2520paired%2520documents%252C%2520covering%252032%2520domains%2520and%2520267%2520document%2520types%252C%2520each%2520comprising%2520a%2520high-%2520and%2520low-professionalism%2520document%2520with%2520identical%2520content%2520but%2520different%2520structure%2520and%2520style.%2520This%2520setup%2520enables%2520the%2520model%2520to%2520evaluate%2520professionalism%2520comprehensively%2520and%2520independently%2520of%2520textual%2520quality.%2520DocReward%2520is%2520trained%2520using%2520the%2520Bradley-Terry%2520loss%2520to%2520score%2520documents%252C%2520penalizing%2520predictions%2520that%2520contradict%2520the%2520annotated%2520ranking.%2520On%2520a%2520manually%2520annotated%2520benchmark%252C%2520DocReward%2520outperforms%2520GPT-5%2520by%252014.6%2520percentage%2520points%2520in%2520accuracy.%2520Extrinsic%2520RL%2520experiments%2520further%2520validate%2520its%2520effectiveness%2520in%2520guiding%2520professional%2520document%2520generation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11391v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DocReward%3A%20A%20Document%20Reward%20Model%20for%20Structuring%20and%20Stylizing&entry.906535625=Junpeng%20Liu%20and%20Yuzhong%20Zhao%20and%20Bowen%20Cao%20and%20Jiayu%20Ding%20and%20Yilin%20Jia%20and%20Tengchao%20Lv%20and%20Yupan%20Huang%20and%20Shaohan%20Huang%20and%20Nan%20Yang%20and%20Li%20Dong%20and%20Lei%20Cui%20and%20Tao%20Ge%20and%20Xun%20Wang%20and%20Huitian%20Jiao%20and%20Sun%20Mao%20and%20FNU%20Kartik%20and%20Si-Qing%20Chen%20and%20Wai%20Lam%20and%20Furu%20Wei&entry.1292438233=Recent%20advances%20in%20agentic%20workflows%20have%20enabled%20the%20automation%20of%20tasks%20such%20as%20professional%20document%20generation.%20However%2C%20they%20primarily%20focus%20on%20textual%20quality%2C%20neglecting%20visual%20structure%20and%20style%2C%20which%20are%20crucial%20for%20readability%20and%20engagement.%20This%20gap%20stems%20mainly%20from%20a%20lack%20of%20effective%20reward%20models%20capable%20of%20guiding%20agents%20toward%20producing%20documents%20with%20high%20structural%20and%20stylistic%20professionalism.%20To%20address%20this%2C%20we%20propose%20DocReward%2C%20a%20document%20reward%20model%20that%20evaluates%20documents%20based%20on%20their%20structure%20and%20style.%20The%20model%20is%20trained%20under%20a%20textual-quality-agnostic%20framework%20to%20assess%20professionalism%20without%20being%20influenced%20by%20textual%20quality.%20To%20achieve%20this%2C%20we%20construct%20a%20multi-domain%20dataset%20DocPair%20of%20117K%20paired%20documents%2C%20covering%2032%20domains%20and%20267%20document%20types%2C%20each%20comprising%20a%20high-%20and%20low-professionalism%20document%20with%20identical%20content%20but%20different%20structure%20and%20style.%20This%20setup%20enables%20the%20model%20to%20evaluate%20professionalism%20comprehensively%20and%20independently%20of%20textual%20quality.%20DocReward%20is%20trained%20using%20the%20Bradley-Terry%20loss%20to%20score%20documents%2C%20penalizing%20predictions%20that%20contradict%20the%20annotated%20ranking.%20On%20a%20manually%20annotated%20benchmark%2C%20DocReward%20outperforms%20GPT-5%20by%2014.6%20percentage%20points%20in%20accuracy.%20Extrinsic%20RL%20experiments%20further%20validate%20its%20effectiveness%20in%20guiding%20professional%20document%20generation.&entry.1838667208=http%3A//arxiv.org/abs/2510.11391v2&entry.124074799=Read"},
{"title": "Exact Constraint Enforcement in Physics-Informed Extreme Learning Machines using Null-Space Projection Framework", "author": "Rishi Mishra and  Smriti and Balaji Srinivasan and Sundararajan Natarajan and Ganapathy Krishnamurthi", "abstract": "Physics-informed extreme learning machines (PIELMs) typically impose boundary and initial conditions through penalty terms, yielding only approximate satisfaction that is sensitive to user-specified weights and can propagate errors into the interior solution. This work introduces Null-Space Projected PIELM (NP-PIELM), achieving exact constraint enforcement through algebraic projection in coefficient space. The method exploits the geometric structure of the admissible coefficient manifold, recognizing that it admits a decomposition through the null space of the boundary operator. By characterizing this manifold via a translation-invariant representation and projecting onto the kernel component, optimization is restricted to constraint-preserving directions, transforming the constrained problem into unconstrained least-squares where boundary conditions are satisfied exactly at discrete collocation points. This eliminates penalty coefficients, dual variables, and problem-specific constructions while preserving single-shot training efficiency. Numerical experiments on elliptic and parabolic problems including complex geometries and mixed boundary conditions validate the framework.", "link": "http://arxiv.org/abs/2601.10999v2", "date": "2026-01-20", "relevancy": 2.3115, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4656}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4619}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4595}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exact%20Constraint%20Enforcement%20in%20Physics-Informed%20Extreme%20Learning%20Machines%20using%20Null-Space%20Projection%20Framework&body=Title%3A%20Exact%20Constraint%20Enforcement%20in%20Physics-Informed%20Extreme%20Learning%20Machines%20using%20Null-Space%20Projection%20Framework%0AAuthor%3A%20Rishi%20Mishra%20and%20%20Smriti%20and%20Balaji%20Srinivasan%20and%20Sundararajan%20Natarajan%20and%20Ganapathy%20Krishnamurthi%0AAbstract%3A%20Physics-informed%20extreme%20learning%20machines%20%28PIELMs%29%20typically%20impose%20boundary%20and%20initial%20conditions%20through%20penalty%20terms%2C%20yielding%20only%20approximate%20satisfaction%20that%20is%20sensitive%20to%20user-specified%20weights%20and%20can%20propagate%20errors%20into%20the%20interior%20solution.%20This%20work%20introduces%20Null-Space%20Projected%20PIELM%20%28NP-PIELM%29%2C%20achieving%20exact%20constraint%20enforcement%20through%20algebraic%20projection%20in%20coefficient%20space.%20The%20method%20exploits%20the%20geometric%20structure%20of%20the%20admissible%20coefficient%20manifold%2C%20recognizing%20that%20it%20admits%20a%20decomposition%20through%20the%20null%20space%20of%20the%20boundary%20operator.%20By%20characterizing%20this%20manifold%20via%20a%20translation-invariant%20representation%20and%20projecting%20onto%20the%20kernel%20component%2C%20optimization%20is%20restricted%20to%20constraint-preserving%20directions%2C%20transforming%20the%20constrained%20problem%20into%20unconstrained%20least-squares%20where%20boundary%20conditions%20are%20satisfied%20exactly%20at%20discrete%20collocation%20points.%20This%20eliminates%20penalty%20coefficients%2C%20dual%20variables%2C%20and%20problem-specific%20constructions%20while%20preserving%20single-shot%20training%20efficiency.%20Numerical%20experiments%20on%20elliptic%20and%20parabolic%20problems%20including%20complex%20geometries%20and%20mixed%20boundary%20conditions%20validate%20the%20framework.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10999v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExact%2520Constraint%2520Enforcement%2520in%2520Physics-Informed%2520Extreme%2520Learning%2520Machines%2520using%2520Null-Space%2520Projection%2520Framework%26entry.906535625%3DRishi%2520Mishra%2520and%2520%2520Smriti%2520and%2520Balaji%2520Srinivasan%2520and%2520Sundararajan%2520Natarajan%2520and%2520Ganapathy%2520Krishnamurthi%26entry.1292438233%3DPhysics-informed%2520extreme%2520learning%2520machines%2520%2528PIELMs%2529%2520typically%2520impose%2520boundary%2520and%2520initial%2520conditions%2520through%2520penalty%2520terms%252C%2520yielding%2520only%2520approximate%2520satisfaction%2520that%2520is%2520sensitive%2520to%2520user-specified%2520weights%2520and%2520can%2520propagate%2520errors%2520into%2520the%2520interior%2520solution.%2520This%2520work%2520introduces%2520Null-Space%2520Projected%2520PIELM%2520%2528NP-PIELM%2529%252C%2520achieving%2520exact%2520constraint%2520enforcement%2520through%2520algebraic%2520projection%2520in%2520coefficient%2520space.%2520The%2520method%2520exploits%2520the%2520geometric%2520structure%2520of%2520the%2520admissible%2520coefficient%2520manifold%252C%2520recognizing%2520that%2520it%2520admits%2520a%2520decomposition%2520through%2520the%2520null%2520space%2520of%2520the%2520boundary%2520operator.%2520By%2520characterizing%2520this%2520manifold%2520via%2520a%2520translation-invariant%2520representation%2520and%2520projecting%2520onto%2520the%2520kernel%2520component%252C%2520optimization%2520is%2520restricted%2520to%2520constraint-preserving%2520directions%252C%2520transforming%2520the%2520constrained%2520problem%2520into%2520unconstrained%2520least-squares%2520where%2520boundary%2520conditions%2520are%2520satisfied%2520exactly%2520at%2520discrete%2520collocation%2520points.%2520This%2520eliminates%2520penalty%2520coefficients%252C%2520dual%2520variables%252C%2520and%2520problem-specific%2520constructions%2520while%2520preserving%2520single-shot%2520training%2520efficiency.%2520Numerical%2520experiments%2520on%2520elliptic%2520and%2520parabolic%2520problems%2520including%2520complex%2520geometries%2520and%2520mixed%2520boundary%2520conditions%2520validate%2520the%2520framework.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10999v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exact%20Constraint%20Enforcement%20in%20Physics-Informed%20Extreme%20Learning%20Machines%20using%20Null-Space%20Projection%20Framework&entry.906535625=Rishi%20Mishra%20and%20%20Smriti%20and%20Balaji%20Srinivasan%20and%20Sundararajan%20Natarajan%20and%20Ganapathy%20Krishnamurthi&entry.1292438233=Physics-informed%20extreme%20learning%20machines%20%28PIELMs%29%20typically%20impose%20boundary%20and%20initial%20conditions%20through%20penalty%20terms%2C%20yielding%20only%20approximate%20satisfaction%20that%20is%20sensitive%20to%20user-specified%20weights%20and%20can%20propagate%20errors%20into%20the%20interior%20solution.%20This%20work%20introduces%20Null-Space%20Projected%20PIELM%20%28NP-PIELM%29%2C%20achieving%20exact%20constraint%20enforcement%20through%20algebraic%20projection%20in%20coefficient%20space.%20The%20method%20exploits%20the%20geometric%20structure%20of%20the%20admissible%20coefficient%20manifold%2C%20recognizing%20that%20it%20admits%20a%20decomposition%20through%20the%20null%20space%20of%20the%20boundary%20operator.%20By%20characterizing%20this%20manifold%20via%20a%20translation-invariant%20representation%20and%20projecting%20onto%20the%20kernel%20component%2C%20optimization%20is%20restricted%20to%20constraint-preserving%20directions%2C%20transforming%20the%20constrained%20problem%20into%20unconstrained%20least-squares%20where%20boundary%20conditions%20are%20satisfied%20exactly%20at%20discrete%20collocation%20points.%20This%20eliminates%20penalty%20coefficients%2C%20dual%20variables%2C%20and%20problem-specific%20constructions%20while%20preserving%20single-shot%20training%20efficiency.%20Numerical%20experiments%20on%20elliptic%20and%20parabolic%20problems%20including%20complex%20geometries%20and%20mixed%20boundary%20conditions%20validate%20the%20framework.&entry.1838667208=http%3A//arxiv.org/abs/2601.10999v2&entry.124074799=Read"},
{"title": "TalkingHeadBench: A Multi-Modal Benchmark & Analysis of Talking-Head DeepFake Detection", "author": "Xinqi Xiong and Prakrut Patel and Qingyuan Fan and Amisha Wadhwa and Sarathy Selvam and Xiao Guo and Luchao Qi and Xiaoming Liu and Roni Sengupta", "abstract": "The rapid advancement of talking-head deepfake generation fueled by advanced generative models has elevated the realism of synthetic videos to a level that poses substantial risks in domains such as media, politics, and finance. However, current benchmarks for deepfake talking-head detection fail to reflect this progress, relying on outdated generators and offering limited insight into model robustness and generalization. We introduce TalkingHeadBench, a comprehensive multi-model multi-generator benchmark and curated dataset designed to evaluate the performance of state-of-the-art detectors on the most advanced generators. Our dataset includes deepfakes synthesized by leading academic and commercial models and features carefully constructed protocols to assess generalization under distribution shifts in identity and generator characteristics. We benchmark a diverse set of existing detection methods, including CNNs, vision transformers, and temporal models, and analyze their robustness and generalization capabilities. In addition, we provide error analysis using Grad-CAM visualizations to expose common failure modes and detector biases. TalkingHeadBench is hosted on https://huggingface.co/datasets/luchaoqi/TalkingHeadBench with open access to all data splits and protocols. Our benchmark aims to accelerate research towards more robust and generalizable detection models in the face of rapidly evolving generative techniques.", "link": "http://arxiv.org/abs/2505.24866v3", "date": "2026-01-20", "relevancy": 2.3074, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.6087}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5573}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5528}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TalkingHeadBench%3A%20A%20Multi-Modal%20Benchmark%20%26%20Analysis%20of%20Talking-Head%20DeepFake%20Detection&body=Title%3A%20TalkingHeadBench%3A%20A%20Multi-Modal%20Benchmark%20%26%20Analysis%20of%20Talking-Head%20DeepFake%20Detection%0AAuthor%3A%20Xinqi%20Xiong%20and%20Prakrut%20Patel%20and%20Qingyuan%20Fan%20and%20Amisha%20Wadhwa%20and%20Sarathy%20Selvam%20and%20Xiao%20Guo%20and%20Luchao%20Qi%20and%20Xiaoming%20Liu%20and%20Roni%20Sengupta%0AAbstract%3A%20The%20rapid%20advancement%20of%20talking-head%20deepfake%20generation%20fueled%20by%20advanced%20generative%20models%20has%20elevated%20the%20realism%20of%20synthetic%20videos%20to%20a%20level%20that%20poses%20substantial%20risks%20in%20domains%20such%20as%20media%2C%20politics%2C%20and%20finance.%20However%2C%20current%20benchmarks%20for%20deepfake%20talking-head%20detection%20fail%20to%20reflect%20this%20progress%2C%20relying%20on%20outdated%20generators%20and%20offering%20limited%20insight%20into%20model%20robustness%20and%20generalization.%20We%20introduce%20TalkingHeadBench%2C%20a%20comprehensive%20multi-model%20multi-generator%20benchmark%20and%20curated%20dataset%20designed%20to%20evaluate%20the%20performance%20of%20state-of-the-art%20detectors%20on%20the%20most%20advanced%20generators.%20Our%20dataset%20includes%20deepfakes%20synthesized%20by%20leading%20academic%20and%20commercial%20models%20and%20features%20carefully%20constructed%20protocols%20to%20assess%20generalization%20under%20distribution%20shifts%20in%20identity%20and%20generator%20characteristics.%20We%20benchmark%20a%20diverse%20set%20of%20existing%20detection%20methods%2C%20including%20CNNs%2C%20vision%20transformers%2C%20and%20temporal%20models%2C%20and%20analyze%20their%20robustness%20and%20generalization%20capabilities.%20In%20addition%2C%20we%20provide%20error%20analysis%20using%20Grad-CAM%20visualizations%20to%20expose%20common%20failure%20modes%20and%20detector%20biases.%20TalkingHeadBench%20is%20hosted%20on%20https%3A//huggingface.co/datasets/luchaoqi/TalkingHeadBench%20with%20open%20access%20to%20all%20data%20splits%20and%20protocols.%20Our%20benchmark%20aims%20to%20accelerate%20research%20towards%20more%20robust%20and%20generalizable%20detection%20models%20in%20the%20face%20of%20rapidly%20evolving%20generative%20techniques.%0ALink%3A%20http%3A//arxiv.org/abs/2505.24866v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTalkingHeadBench%253A%2520A%2520Multi-Modal%2520Benchmark%2520%2526%2520Analysis%2520of%2520Talking-Head%2520DeepFake%2520Detection%26entry.906535625%3DXinqi%2520Xiong%2520and%2520Prakrut%2520Patel%2520and%2520Qingyuan%2520Fan%2520and%2520Amisha%2520Wadhwa%2520and%2520Sarathy%2520Selvam%2520and%2520Xiao%2520Guo%2520and%2520Luchao%2520Qi%2520and%2520Xiaoming%2520Liu%2520and%2520Roni%2520Sengupta%26entry.1292438233%3DThe%2520rapid%2520advancement%2520of%2520talking-head%2520deepfake%2520generation%2520fueled%2520by%2520advanced%2520generative%2520models%2520has%2520elevated%2520the%2520realism%2520of%2520synthetic%2520videos%2520to%2520a%2520level%2520that%2520poses%2520substantial%2520risks%2520in%2520domains%2520such%2520as%2520media%252C%2520politics%252C%2520and%2520finance.%2520However%252C%2520current%2520benchmarks%2520for%2520deepfake%2520talking-head%2520detection%2520fail%2520to%2520reflect%2520this%2520progress%252C%2520relying%2520on%2520outdated%2520generators%2520and%2520offering%2520limited%2520insight%2520into%2520model%2520robustness%2520and%2520generalization.%2520We%2520introduce%2520TalkingHeadBench%252C%2520a%2520comprehensive%2520multi-model%2520multi-generator%2520benchmark%2520and%2520curated%2520dataset%2520designed%2520to%2520evaluate%2520the%2520performance%2520of%2520state-of-the-art%2520detectors%2520on%2520the%2520most%2520advanced%2520generators.%2520Our%2520dataset%2520includes%2520deepfakes%2520synthesized%2520by%2520leading%2520academic%2520and%2520commercial%2520models%2520and%2520features%2520carefully%2520constructed%2520protocols%2520to%2520assess%2520generalization%2520under%2520distribution%2520shifts%2520in%2520identity%2520and%2520generator%2520characteristics.%2520We%2520benchmark%2520a%2520diverse%2520set%2520of%2520existing%2520detection%2520methods%252C%2520including%2520CNNs%252C%2520vision%2520transformers%252C%2520and%2520temporal%2520models%252C%2520and%2520analyze%2520their%2520robustness%2520and%2520generalization%2520capabilities.%2520In%2520addition%252C%2520we%2520provide%2520error%2520analysis%2520using%2520Grad-CAM%2520visualizations%2520to%2520expose%2520common%2520failure%2520modes%2520and%2520detector%2520biases.%2520TalkingHeadBench%2520is%2520hosted%2520on%2520https%253A//huggingface.co/datasets/luchaoqi/TalkingHeadBench%2520with%2520open%2520access%2520to%2520all%2520data%2520splits%2520and%2520protocols.%2520Our%2520benchmark%2520aims%2520to%2520accelerate%2520research%2520towards%2520more%2520robust%2520and%2520generalizable%2520detection%2520models%2520in%2520the%2520face%2520of%2520rapidly%2520evolving%2520generative%2520techniques.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.24866v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TalkingHeadBench%3A%20A%20Multi-Modal%20Benchmark%20%26%20Analysis%20of%20Talking-Head%20DeepFake%20Detection&entry.906535625=Xinqi%20Xiong%20and%20Prakrut%20Patel%20and%20Qingyuan%20Fan%20and%20Amisha%20Wadhwa%20and%20Sarathy%20Selvam%20and%20Xiao%20Guo%20and%20Luchao%20Qi%20and%20Xiaoming%20Liu%20and%20Roni%20Sengupta&entry.1292438233=The%20rapid%20advancement%20of%20talking-head%20deepfake%20generation%20fueled%20by%20advanced%20generative%20models%20has%20elevated%20the%20realism%20of%20synthetic%20videos%20to%20a%20level%20that%20poses%20substantial%20risks%20in%20domains%20such%20as%20media%2C%20politics%2C%20and%20finance.%20However%2C%20current%20benchmarks%20for%20deepfake%20talking-head%20detection%20fail%20to%20reflect%20this%20progress%2C%20relying%20on%20outdated%20generators%20and%20offering%20limited%20insight%20into%20model%20robustness%20and%20generalization.%20We%20introduce%20TalkingHeadBench%2C%20a%20comprehensive%20multi-model%20multi-generator%20benchmark%20and%20curated%20dataset%20designed%20to%20evaluate%20the%20performance%20of%20state-of-the-art%20detectors%20on%20the%20most%20advanced%20generators.%20Our%20dataset%20includes%20deepfakes%20synthesized%20by%20leading%20academic%20and%20commercial%20models%20and%20features%20carefully%20constructed%20protocols%20to%20assess%20generalization%20under%20distribution%20shifts%20in%20identity%20and%20generator%20characteristics.%20We%20benchmark%20a%20diverse%20set%20of%20existing%20detection%20methods%2C%20including%20CNNs%2C%20vision%20transformers%2C%20and%20temporal%20models%2C%20and%20analyze%20their%20robustness%20and%20generalization%20capabilities.%20In%20addition%2C%20we%20provide%20error%20analysis%20using%20Grad-CAM%20visualizations%20to%20expose%20common%20failure%20modes%20and%20detector%20biases.%20TalkingHeadBench%20is%20hosted%20on%20https%3A//huggingface.co/datasets/luchaoqi/TalkingHeadBench%20with%20open%20access%20to%20all%20data%20splits%20and%20protocols.%20Our%20benchmark%20aims%20to%20accelerate%20research%20towards%20more%20robust%20and%20generalizable%20detection%20models%20in%20the%20face%20of%20rapidly%20evolving%20generative%20techniques.&entry.1838667208=http%3A//arxiv.org/abs/2505.24866v3&entry.124074799=Read"},
{"title": "Soft Tail-dropping for Adaptive Visual Tokenization", "author": "Zeyuan Chen and Kai Zhang and Zhuowen Tu and Yuanjun Xiong", "abstract": "We present Soft Tail-dropping Adaptive Tokenizer (STAT), a 1D discrete visual tokenizer that adaptively chooses the number of output tokens per image according to its structural complexity and level of detail. STAT encodes an image into a sequence of discrete codes together with per-token keep probabilities. Beyond standard autoencoder objectives, we regularize these keep probabilities to be monotonically decreasing along the sequence and explicitly align their distribution with an image-level complexity measure. As a result, STAT produces length-adaptive 1D visual tokens that are naturally compatible with causal 1D autoregressive (AR) visual generative models. On ImageNet-1k, equipping vanilla causal AR models with STAT yields competitive or superior visual generation quality compared to other probabilistic model families, while also exhibiting favorable scaling behavior that has been elusive in prior vanilla AR visual generation attempts.", "link": "http://arxiv.org/abs/2601.14246v1", "date": "2026-01-20", "relevancy": 2.2549, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6231}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5214}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5213}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Soft%20Tail-dropping%20for%20Adaptive%20Visual%20Tokenization&body=Title%3A%20Soft%20Tail-dropping%20for%20Adaptive%20Visual%20Tokenization%0AAuthor%3A%20Zeyuan%20Chen%20and%20Kai%20Zhang%20and%20Zhuowen%20Tu%20and%20Yuanjun%20Xiong%0AAbstract%3A%20We%20present%20Soft%20Tail-dropping%20Adaptive%20Tokenizer%20%28STAT%29%2C%20a%201D%20discrete%20visual%20tokenizer%20that%20adaptively%20chooses%20the%20number%20of%20output%20tokens%20per%20image%20according%20to%20its%20structural%20complexity%20and%20level%20of%20detail.%20STAT%20encodes%20an%20image%20into%20a%20sequence%20of%20discrete%20codes%20together%20with%20per-token%20keep%20probabilities.%20Beyond%20standard%20autoencoder%20objectives%2C%20we%20regularize%20these%20keep%20probabilities%20to%20be%20monotonically%20decreasing%20along%20the%20sequence%20and%20explicitly%20align%20their%20distribution%20with%20an%20image-level%20complexity%20measure.%20As%20a%20result%2C%20STAT%20produces%20length-adaptive%201D%20visual%20tokens%20that%20are%20naturally%20compatible%20with%20causal%201D%20autoregressive%20%28AR%29%20visual%20generative%20models.%20On%20ImageNet-1k%2C%20equipping%20vanilla%20causal%20AR%20models%20with%20STAT%20yields%20competitive%20or%20superior%20visual%20generation%20quality%20compared%20to%20other%20probabilistic%20model%20families%2C%20while%20also%20exhibiting%20favorable%20scaling%20behavior%20that%20has%20been%20elusive%20in%20prior%20vanilla%20AR%20visual%20generation%20attempts.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14246v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoft%2520Tail-dropping%2520for%2520Adaptive%2520Visual%2520Tokenization%26entry.906535625%3DZeyuan%2520Chen%2520and%2520Kai%2520Zhang%2520and%2520Zhuowen%2520Tu%2520and%2520Yuanjun%2520Xiong%26entry.1292438233%3DWe%2520present%2520Soft%2520Tail-dropping%2520Adaptive%2520Tokenizer%2520%2528STAT%2529%252C%2520a%25201D%2520discrete%2520visual%2520tokenizer%2520that%2520adaptively%2520chooses%2520the%2520number%2520of%2520output%2520tokens%2520per%2520image%2520according%2520to%2520its%2520structural%2520complexity%2520and%2520level%2520of%2520detail.%2520STAT%2520encodes%2520an%2520image%2520into%2520a%2520sequence%2520of%2520discrete%2520codes%2520together%2520with%2520per-token%2520keep%2520probabilities.%2520Beyond%2520standard%2520autoencoder%2520objectives%252C%2520we%2520regularize%2520these%2520keep%2520probabilities%2520to%2520be%2520monotonically%2520decreasing%2520along%2520the%2520sequence%2520and%2520explicitly%2520align%2520their%2520distribution%2520with%2520an%2520image-level%2520complexity%2520measure.%2520As%2520a%2520result%252C%2520STAT%2520produces%2520length-adaptive%25201D%2520visual%2520tokens%2520that%2520are%2520naturally%2520compatible%2520with%2520causal%25201D%2520autoregressive%2520%2528AR%2529%2520visual%2520generative%2520models.%2520On%2520ImageNet-1k%252C%2520equipping%2520vanilla%2520causal%2520AR%2520models%2520with%2520STAT%2520yields%2520competitive%2520or%2520superior%2520visual%2520generation%2520quality%2520compared%2520to%2520other%2520probabilistic%2520model%2520families%252C%2520while%2520also%2520exhibiting%2520favorable%2520scaling%2520behavior%2520that%2520has%2520been%2520elusive%2520in%2520prior%2520vanilla%2520AR%2520visual%2520generation%2520attempts.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14246v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Soft%20Tail-dropping%20for%20Adaptive%20Visual%20Tokenization&entry.906535625=Zeyuan%20Chen%20and%20Kai%20Zhang%20and%20Zhuowen%20Tu%20and%20Yuanjun%20Xiong&entry.1292438233=We%20present%20Soft%20Tail-dropping%20Adaptive%20Tokenizer%20%28STAT%29%2C%20a%201D%20discrete%20visual%20tokenizer%20that%20adaptively%20chooses%20the%20number%20of%20output%20tokens%20per%20image%20according%20to%20its%20structural%20complexity%20and%20level%20of%20detail.%20STAT%20encodes%20an%20image%20into%20a%20sequence%20of%20discrete%20codes%20together%20with%20per-token%20keep%20probabilities.%20Beyond%20standard%20autoencoder%20objectives%2C%20we%20regularize%20these%20keep%20probabilities%20to%20be%20monotonically%20decreasing%20along%20the%20sequence%20and%20explicitly%20align%20their%20distribution%20with%20an%20image-level%20complexity%20measure.%20As%20a%20result%2C%20STAT%20produces%20length-adaptive%201D%20visual%20tokens%20that%20are%20naturally%20compatible%20with%20causal%201D%20autoregressive%20%28AR%29%20visual%20generative%20models.%20On%20ImageNet-1k%2C%20equipping%20vanilla%20causal%20AR%20models%20with%20STAT%20yields%20competitive%20or%20superior%20visual%20generation%20quality%20compared%20to%20other%20probabilistic%20model%20families%2C%20while%20also%20exhibiting%20favorable%20scaling%20behavior%20that%20has%20been%20elusive%20in%20prior%20vanilla%20AR%20visual%20generation%20attempts.&entry.1838667208=http%3A//arxiv.org/abs/2601.14246v1&entry.124074799=Read"},
{"title": "Correcting and Quantifying Systematic Errors in 3D Box Annotations for Autonomous Driving", "author": "Alexandre Justo Miro and Ludvig af Klinteberg and Bogdan Timus and Aron Asefaw and Ajinkya Khoche and Thomas Gustafsson and Sina Sharif Mansouri and Masoud Daneshtalab", "abstract": "Accurate ground truth annotations are critical to supervised learning and evaluating the performance of autonomous vehicle systems. These vehicles are typically equipped with active sensors, such as LiDAR, which scan the environment in predefined patterns. 3D box annotation based on data from such sensors is challenging in dynamic scenarios, where objects are observed at different timestamps, hence different positions. Without proper handling of this phenomenon, systematic errors are prone to being introduced in the box annotations. Our work is the first to discover such annotation errors in widely used, publicly available datasets. Through our novel offline estimation method, we correct the annotations so that they follow physically feasible trajectories and achieve spatial and temporal consistency with the sensor data. For the first time, we define metrics for this problem; and we evaluate our method on the Argoverse 2, MAN TruckScenes, and our proprietary datasets. Our approach increases the quality of box annotations by more than 17% in these datasets. Furthermore, we quantify the annotation errors in them and find that the original annotations are misplaced by up to 2.5 m, with highly dynamic objects being the most affected. Finally, we test the impact of the errors in benchmarking and find that the impact is larger than the improvements that state-of-the-art methods typically achieve with respect to the previous state-of-the-art methods; showing that accurate annotations are essential for correct interpretation of performance. Our code is available at https://github.com/alexandre-justo-miro/annotation-correction-3D-boxes.", "link": "http://arxiv.org/abs/2601.14038v1", "date": "2026-01-20", "relevancy": 2.2489, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6046}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5642}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Correcting%20and%20Quantifying%20Systematic%20Errors%20in%203D%20Box%20Annotations%20for%20Autonomous%20Driving&body=Title%3A%20Correcting%20and%20Quantifying%20Systematic%20Errors%20in%203D%20Box%20Annotations%20for%20Autonomous%20Driving%0AAuthor%3A%20Alexandre%20Justo%20Miro%20and%20Ludvig%20af%20Klinteberg%20and%20Bogdan%20Timus%20and%20Aron%20Asefaw%20and%20Ajinkya%20Khoche%20and%20Thomas%20Gustafsson%20and%20Sina%20Sharif%20Mansouri%20and%20Masoud%20Daneshtalab%0AAbstract%3A%20Accurate%20ground%20truth%20annotations%20are%20critical%20to%20supervised%20learning%20and%20evaluating%20the%20performance%20of%20autonomous%20vehicle%20systems.%20These%20vehicles%20are%20typically%20equipped%20with%20active%20sensors%2C%20such%20as%20LiDAR%2C%20which%20scan%20the%20environment%20in%20predefined%20patterns.%203D%20box%20annotation%20based%20on%20data%20from%20such%20sensors%20is%20challenging%20in%20dynamic%20scenarios%2C%20where%20objects%20are%20observed%20at%20different%20timestamps%2C%20hence%20different%20positions.%20Without%20proper%20handling%20of%20this%20phenomenon%2C%20systematic%20errors%20are%20prone%20to%20being%20introduced%20in%20the%20box%20annotations.%20Our%20work%20is%20the%20first%20to%20discover%20such%20annotation%20errors%20in%20widely%20used%2C%20publicly%20available%20datasets.%20Through%20our%20novel%20offline%20estimation%20method%2C%20we%20correct%20the%20annotations%20so%20that%20they%20follow%20physically%20feasible%20trajectories%20and%20achieve%20spatial%20and%20temporal%20consistency%20with%20the%20sensor%20data.%20For%20the%20first%20time%2C%20we%20define%20metrics%20for%20this%20problem%3B%20and%20we%20evaluate%20our%20method%20on%20the%20Argoverse%202%2C%20MAN%20TruckScenes%2C%20and%20our%20proprietary%20datasets.%20Our%20approach%20increases%20the%20quality%20of%20box%20annotations%20by%20more%20than%2017%25%20in%20these%20datasets.%20Furthermore%2C%20we%20quantify%20the%20annotation%20errors%20in%20them%20and%20find%20that%20the%20original%20annotations%20are%20misplaced%20by%20up%20to%202.5%20m%2C%20with%20highly%20dynamic%20objects%20being%20the%20most%20affected.%20Finally%2C%20we%20test%20the%20impact%20of%20the%20errors%20in%20benchmarking%20and%20find%20that%20the%20impact%20is%20larger%20than%20the%20improvements%20that%20state-of-the-art%20methods%20typically%20achieve%20with%20respect%20to%20the%20previous%20state-of-the-art%20methods%3B%20showing%20that%20accurate%20annotations%20are%20essential%20for%20correct%20interpretation%20of%20performance.%20Our%20code%20is%20available%20at%20https%3A//github.com/alexandre-justo-miro/annotation-correction-3D-boxes.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14038v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCorrecting%2520and%2520Quantifying%2520Systematic%2520Errors%2520in%25203D%2520Box%2520Annotations%2520for%2520Autonomous%2520Driving%26entry.906535625%3DAlexandre%2520Justo%2520Miro%2520and%2520Ludvig%2520af%2520Klinteberg%2520and%2520Bogdan%2520Timus%2520and%2520Aron%2520Asefaw%2520and%2520Ajinkya%2520Khoche%2520and%2520Thomas%2520Gustafsson%2520and%2520Sina%2520Sharif%2520Mansouri%2520and%2520Masoud%2520Daneshtalab%26entry.1292438233%3DAccurate%2520ground%2520truth%2520annotations%2520are%2520critical%2520to%2520supervised%2520learning%2520and%2520evaluating%2520the%2520performance%2520of%2520autonomous%2520vehicle%2520systems.%2520These%2520vehicles%2520are%2520typically%2520equipped%2520with%2520active%2520sensors%252C%2520such%2520as%2520LiDAR%252C%2520which%2520scan%2520the%2520environment%2520in%2520predefined%2520patterns.%25203D%2520box%2520annotation%2520based%2520on%2520data%2520from%2520such%2520sensors%2520is%2520challenging%2520in%2520dynamic%2520scenarios%252C%2520where%2520objects%2520are%2520observed%2520at%2520different%2520timestamps%252C%2520hence%2520different%2520positions.%2520Without%2520proper%2520handling%2520of%2520this%2520phenomenon%252C%2520systematic%2520errors%2520are%2520prone%2520to%2520being%2520introduced%2520in%2520the%2520box%2520annotations.%2520Our%2520work%2520is%2520the%2520first%2520to%2520discover%2520such%2520annotation%2520errors%2520in%2520widely%2520used%252C%2520publicly%2520available%2520datasets.%2520Through%2520our%2520novel%2520offline%2520estimation%2520method%252C%2520we%2520correct%2520the%2520annotations%2520so%2520that%2520they%2520follow%2520physically%2520feasible%2520trajectories%2520and%2520achieve%2520spatial%2520and%2520temporal%2520consistency%2520with%2520the%2520sensor%2520data.%2520For%2520the%2520first%2520time%252C%2520we%2520define%2520metrics%2520for%2520this%2520problem%253B%2520and%2520we%2520evaluate%2520our%2520method%2520on%2520the%2520Argoverse%25202%252C%2520MAN%2520TruckScenes%252C%2520and%2520our%2520proprietary%2520datasets.%2520Our%2520approach%2520increases%2520the%2520quality%2520of%2520box%2520annotations%2520by%2520more%2520than%252017%2525%2520in%2520these%2520datasets.%2520Furthermore%252C%2520we%2520quantify%2520the%2520annotation%2520errors%2520in%2520them%2520and%2520find%2520that%2520the%2520original%2520annotations%2520are%2520misplaced%2520by%2520up%2520to%25202.5%2520m%252C%2520with%2520highly%2520dynamic%2520objects%2520being%2520the%2520most%2520affected.%2520Finally%252C%2520we%2520test%2520the%2520impact%2520of%2520the%2520errors%2520in%2520benchmarking%2520and%2520find%2520that%2520the%2520impact%2520is%2520larger%2520than%2520the%2520improvements%2520that%2520state-of-the-art%2520methods%2520typically%2520achieve%2520with%2520respect%2520to%2520the%2520previous%2520state-of-the-art%2520methods%253B%2520showing%2520that%2520accurate%2520annotations%2520are%2520essential%2520for%2520correct%2520interpretation%2520of%2520performance.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/alexandre-justo-miro/annotation-correction-3D-boxes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14038v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Correcting%20and%20Quantifying%20Systematic%20Errors%20in%203D%20Box%20Annotations%20for%20Autonomous%20Driving&entry.906535625=Alexandre%20Justo%20Miro%20and%20Ludvig%20af%20Klinteberg%20and%20Bogdan%20Timus%20and%20Aron%20Asefaw%20and%20Ajinkya%20Khoche%20and%20Thomas%20Gustafsson%20and%20Sina%20Sharif%20Mansouri%20and%20Masoud%20Daneshtalab&entry.1292438233=Accurate%20ground%20truth%20annotations%20are%20critical%20to%20supervised%20learning%20and%20evaluating%20the%20performance%20of%20autonomous%20vehicle%20systems.%20These%20vehicles%20are%20typically%20equipped%20with%20active%20sensors%2C%20such%20as%20LiDAR%2C%20which%20scan%20the%20environment%20in%20predefined%20patterns.%203D%20box%20annotation%20based%20on%20data%20from%20such%20sensors%20is%20challenging%20in%20dynamic%20scenarios%2C%20where%20objects%20are%20observed%20at%20different%20timestamps%2C%20hence%20different%20positions.%20Without%20proper%20handling%20of%20this%20phenomenon%2C%20systematic%20errors%20are%20prone%20to%20being%20introduced%20in%20the%20box%20annotations.%20Our%20work%20is%20the%20first%20to%20discover%20such%20annotation%20errors%20in%20widely%20used%2C%20publicly%20available%20datasets.%20Through%20our%20novel%20offline%20estimation%20method%2C%20we%20correct%20the%20annotations%20so%20that%20they%20follow%20physically%20feasible%20trajectories%20and%20achieve%20spatial%20and%20temporal%20consistency%20with%20the%20sensor%20data.%20For%20the%20first%20time%2C%20we%20define%20metrics%20for%20this%20problem%3B%20and%20we%20evaluate%20our%20method%20on%20the%20Argoverse%202%2C%20MAN%20TruckScenes%2C%20and%20our%20proprietary%20datasets.%20Our%20approach%20increases%20the%20quality%20of%20box%20annotations%20by%20more%20than%2017%25%20in%20these%20datasets.%20Furthermore%2C%20we%20quantify%20the%20annotation%20errors%20in%20them%20and%20find%20that%20the%20original%20annotations%20are%20misplaced%20by%20up%20to%202.5%20m%2C%20with%20highly%20dynamic%20objects%20being%20the%20most%20affected.%20Finally%2C%20we%20test%20the%20impact%20of%20the%20errors%20in%20benchmarking%20and%20find%20that%20the%20impact%20is%20larger%20than%20the%20improvements%20that%20state-of-the-art%20methods%20typically%20achieve%20with%20respect%20to%20the%20previous%20state-of-the-art%20methods%3B%20showing%20that%20accurate%20annotations%20are%20essential%20for%20correct%20interpretation%20of%20performance.%20Our%20code%20is%20available%20at%20https%3A//github.com/alexandre-justo-miro/annotation-correction-3D-boxes.&entry.1838667208=http%3A//arxiv.org/abs/2601.14038v1&entry.124074799=Read"},
{"title": "Learning Domain-Invariant Representations for Cross-Domain Image Registration via Scene-Appearance Disentanglement", "author": "Jiahao Qin and Yiwen Wang", "abstract": "Image registration under domain shift remains a fundamental challenge in computer vision and medical imaging: when source and target images exhibit systematic intensity differences, the brightness constancy assumption underlying conventional registration methods is violated, rendering correspondence\n  estimation ill-posed. We propose SAR-Net, a unified framework that addresses this challenge through principled scene-appearance disentanglement. Our key insight is that observed images can be decomposed into domain-invariant scene representations and domain-specific appearance codes, enabling registration\n  via re-rendering rather than direct intensity matching. We establish theoretical conditions under which this decomposition enables consistent cross-domain alignment (Proposition 1) and prove that our scene consistency loss provides a sufficient condition for geometric correspondence in the shared latent\n  space (Proposition 2). Empirically, we validate SAR-Net on the ANHIR (Automatic Non-rigid Histological Image Registration) challenge benchmark, where multi-stain histopathology images exhibit coupled domain shift from different staining protocols and geometric distortion from tissue preparation. Our method\n  achieves a median relative Target Registration Error (rTRE) of 0.25%, outperforming the state-of-the-art MEVIS method (0.27% rTRE) by 7.4%, with robustness of 99.1%. Code is available at https://github.com/D-ST-Sword/SAR-NET", "link": "http://arxiv.org/abs/2601.08875v2", "date": "2026-01-20", "relevancy": 2.2489, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5769}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5603}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5304}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Domain-Invariant%20Representations%20for%20Cross-Domain%20Image%20Registration%20via%20Scene-Appearance%20Disentanglement&body=Title%3A%20Learning%20Domain-Invariant%20Representations%20for%20Cross-Domain%20Image%20Registration%20via%20Scene-Appearance%20Disentanglement%0AAuthor%3A%20Jiahao%20Qin%20and%20Yiwen%20Wang%0AAbstract%3A%20Image%20registration%20under%20domain%20shift%20remains%20a%20fundamental%20challenge%20in%20computer%20vision%20and%20medical%20imaging%3A%20when%20source%20and%20target%20images%20exhibit%20systematic%20intensity%20differences%2C%20the%20brightness%20constancy%20assumption%20underlying%20conventional%20registration%20methods%20is%20violated%2C%20rendering%20correspondence%0A%20%20estimation%20ill-posed.%20We%20propose%20SAR-Net%2C%20a%20unified%20framework%20that%20addresses%20this%20challenge%20through%20principled%20scene-appearance%20disentanglement.%20Our%20key%20insight%20is%20that%20observed%20images%20can%20be%20decomposed%20into%20domain-invariant%20scene%20representations%20and%20domain-specific%20appearance%20codes%2C%20enabling%20registration%0A%20%20via%20re-rendering%20rather%20than%20direct%20intensity%20matching.%20We%20establish%20theoretical%20conditions%20under%20which%20this%20decomposition%20enables%20consistent%20cross-domain%20alignment%20%28Proposition%201%29%20and%20prove%20that%20our%20scene%20consistency%20loss%20provides%20a%20sufficient%20condition%20for%20geometric%20correspondence%20in%20the%20shared%20latent%0A%20%20space%20%28Proposition%202%29.%20Empirically%2C%20we%20validate%20SAR-Net%20on%20the%20ANHIR%20%28Automatic%20Non-rigid%20Histological%20Image%20Registration%29%20challenge%20benchmark%2C%20where%20multi-stain%20histopathology%20images%20exhibit%20coupled%20domain%20shift%20from%20different%20staining%20protocols%20and%20geometric%20distortion%20from%20tissue%20preparation.%20Our%20method%0A%20%20achieves%20a%20median%20relative%20Target%20Registration%20Error%20%28rTRE%29%20of%200.25%25%2C%20outperforming%20the%20state-of-the-art%20MEVIS%20method%20%280.27%25%20rTRE%29%20by%207.4%25%2C%20with%20robustness%20of%2099.1%25.%20Code%20is%20available%20at%20https%3A//github.com/D-ST-Sword/SAR-NET%0ALink%3A%20http%3A//arxiv.org/abs/2601.08875v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Domain-Invariant%2520Representations%2520for%2520Cross-Domain%2520Image%2520Registration%2520via%2520Scene-Appearance%2520Disentanglement%26entry.906535625%3DJiahao%2520Qin%2520and%2520Yiwen%2520Wang%26entry.1292438233%3DImage%2520registration%2520under%2520domain%2520shift%2520remains%2520a%2520fundamental%2520challenge%2520in%2520computer%2520vision%2520and%2520medical%2520imaging%253A%2520when%2520source%2520and%2520target%2520images%2520exhibit%2520systematic%2520intensity%2520differences%252C%2520the%2520brightness%2520constancy%2520assumption%2520underlying%2520conventional%2520registration%2520methods%2520is%2520violated%252C%2520rendering%2520correspondence%250A%2520%2520estimation%2520ill-posed.%2520We%2520propose%2520SAR-Net%252C%2520a%2520unified%2520framework%2520that%2520addresses%2520this%2520challenge%2520through%2520principled%2520scene-appearance%2520disentanglement.%2520Our%2520key%2520insight%2520is%2520that%2520observed%2520images%2520can%2520be%2520decomposed%2520into%2520domain-invariant%2520scene%2520representations%2520and%2520domain-specific%2520appearance%2520codes%252C%2520enabling%2520registration%250A%2520%2520via%2520re-rendering%2520rather%2520than%2520direct%2520intensity%2520matching.%2520We%2520establish%2520theoretical%2520conditions%2520under%2520which%2520this%2520decomposition%2520enables%2520consistent%2520cross-domain%2520alignment%2520%2528Proposition%25201%2529%2520and%2520prove%2520that%2520our%2520scene%2520consistency%2520loss%2520provides%2520a%2520sufficient%2520condition%2520for%2520geometric%2520correspondence%2520in%2520the%2520shared%2520latent%250A%2520%2520space%2520%2528Proposition%25202%2529.%2520Empirically%252C%2520we%2520validate%2520SAR-Net%2520on%2520the%2520ANHIR%2520%2528Automatic%2520Non-rigid%2520Histological%2520Image%2520Registration%2529%2520challenge%2520benchmark%252C%2520where%2520multi-stain%2520histopathology%2520images%2520exhibit%2520coupled%2520domain%2520shift%2520from%2520different%2520staining%2520protocols%2520and%2520geometric%2520distortion%2520from%2520tissue%2520preparation.%2520Our%2520method%250A%2520%2520achieves%2520a%2520median%2520relative%2520Target%2520Registration%2520Error%2520%2528rTRE%2529%2520of%25200.25%2525%252C%2520outperforming%2520the%2520state-of-the-art%2520MEVIS%2520method%2520%25280.27%2525%2520rTRE%2529%2520by%25207.4%2525%252C%2520with%2520robustness%2520of%252099.1%2525.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/D-ST-Sword/SAR-NET%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.08875v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Domain-Invariant%20Representations%20for%20Cross-Domain%20Image%20Registration%20via%20Scene-Appearance%20Disentanglement&entry.906535625=Jiahao%20Qin%20and%20Yiwen%20Wang&entry.1292438233=Image%20registration%20under%20domain%20shift%20remains%20a%20fundamental%20challenge%20in%20computer%20vision%20and%20medical%20imaging%3A%20when%20source%20and%20target%20images%20exhibit%20systematic%20intensity%20differences%2C%20the%20brightness%20constancy%20assumption%20underlying%20conventional%20registration%20methods%20is%20violated%2C%20rendering%20correspondence%0A%20%20estimation%20ill-posed.%20We%20propose%20SAR-Net%2C%20a%20unified%20framework%20that%20addresses%20this%20challenge%20through%20principled%20scene-appearance%20disentanglement.%20Our%20key%20insight%20is%20that%20observed%20images%20can%20be%20decomposed%20into%20domain-invariant%20scene%20representations%20and%20domain-specific%20appearance%20codes%2C%20enabling%20registration%0A%20%20via%20re-rendering%20rather%20than%20direct%20intensity%20matching.%20We%20establish%20theoretical%20conditions%20under%20which%20this%20decomposition%20enables%20consistent%20cross-domain%20alignment%20%28Proposition%201%29%20and%20prove%20that%20our%20scene%20consistency%20loss%20provides%20a%20sufficient%20condition%20for%20geometric%20correspondence%20in%20the%20shared%20latent%0A%20%20space%20%28Proposition%202%29.%20Empirically%2C%20we%20validate%20SAR-Net%20on%20the%20ANHIR%20%28Automatic%20Non-rigid%20Histological%20Image%20Registration%29%20challenge%20benchmark%2C%20where%20multi-stain%20histopathology%20images%20exhibit%20coupled%20domain%20shift%20from%20different%20staining%20protocols%20and%20geometric%20distortion%20from%20tissue%20preparation.%20Our%20method%0A%20%20achieves%20a%20median%20relative%20Target%20Registration%20Error%20%28rTRE%29%20of%200.25%25%2C%20outperforming%20the%20state-of-the-art%20MEVIS%20method%20%280.27%25%20rTRE%29%20by%207.4%25%2C%20with%20robustness%20of%2099.1%25.%20Code%20is%20available%20at%20https%3A//github.com/D-ST-Sword/SAR-NET&entry.1838667208=http%3A//arxiv.org/abs/2601.08875v2&entry.124074799=Read"},
{"title": "DExTeR: Weakly Semi-Supervised Object Detection with Class and Instance Experts for Medical Imaging", "author": "Adrien Meyer and Didier Mutter and Nicolas Padoy", "abstract": "Detecting anatomical landmarks in medical imaging is essential for diagnosis and intervention guidance. However, object detection models rely on costly bounding box annotations, limiting scalability. Weakly Semi-Supervised Object Detection (WSSOD) with point annotations proposes annotating each instance with a single point, minimizing annotation time while preserving localization signals. A Point-to-Box teacher model, trained on a small box-labeled subset, converts these point annotations into pseudo-box labels to train a student detector. Yet, medical imagery presents unique challenges, including overlapping anatomy, variable object sizes, and elusive structures, which hinder accurate bounding box inference. To overcome these challenges, we introduce DExTeR (DETR with Experts), a transformer-based Point-to-Box regressor tailored for medical imaging. Built upon Point-DETR, DExTeR encodes single-point annotations as object queries, refining feature extraction with the proposed class-guided deformable attention, which guides attention sampling using point coordinates and class labels to capture class-specific characteristics. To improve discrimination in complex structures, it introduces CLICK-MoE (CLass, Instance, and Common Knowledge Mixture of Experts), decoupling class and instance representations to reduce confusion among adjacent or overlapping instances. Finally, we implement a multi-point training strategy which promotes prediction consistency across different point placements, improving robustness to annotation variability. DExTeR achieves state-of-the-art performance across three datasets spanning different medical domains (endoscopy, chest X-rays, and endoscopic ultrasound) highlighting its potential to reduce annotation costs while maintaining high detection accuracy.", "link": "http://arxiv.org/abs/2601.13954v1", "date": "2026-01-20", "relevancy": 2.2268, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5775}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5713}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DExTeR%3A%20Weakly%20Semi-Supervised%20Object%20Detection%20with%20Class%20and%20Instance%20Experts%20for%20Medical%20Imaging&body=Title%3A%20DExTeR%3A%20Weakly%20Semi-Supervised%20Object%20Detection%20with%20Class%20and%20Instance%20Experts%20for%20Medical%20Imaging%0AAuthor%3A%20Adrien%20Meyer%20and%20Didier%20Mutter%20and%20Nicolas%20Padoy%0AAbstract%3A%20Detecting%20anatomical%20landmarks%20in%20medical%20imaging%20is%20essential%20for%20diagnosis%20and%20intervention%20guidance.%20However%2C%20object%20detection%20models%20rely%20on%20costly%20bounding%20box%20annotations%2C%20limiting%20scalability.%20Weakly%20Semi-Supervised%20Object%20Detection%20%28WSSOD%29%20with%20point%20annotations%20proposes%20annotating%20each%20instance%20with%20a%20single%20point%2C%20minimizing%20annotation%20time%20while%20preserving%20localization%20signals.%20A%20Point-to-Box%20teacher%20model%2C%20trained%20on%20a%20small%20box-labeled%20subset%2C%20converts%20these%20point%20annotations%20into%20pseudo-box%20labels%20to%20train%20a%20student%20detector.%20Yet%2C%20medical%20imagery%20presents%20unique%20challenges%2C%20including%20overlapping%20anatomy%2C%20variable%20object%20sizes%2C%20and%20elusive%20structures%2C%20which%20hinder%20accurate%20bounding%20box%20inference.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20DExTeR%20%28DETR%20with%20Experts%29%2C%20a%20transformer-based%20Point-to-Box%20regressor%20tailored%20for%20medical%20imaging.%20Built%20upon%20Point-DETR%2C%20DExTeR%20encodes%20single-point%20annotations%20as%20object%20queries%2C%20refining%20feature%20extraction%20with%20the%20proposed%20class-guided%20deformable%20attention%2C%20which%20guides%20attention%20sampling%20using%20point%20coordinates%20and%20class%20labels%20to%20capture%20class-specific%20characteristics.%20To%20improve%20discrimination%20in%20complex%20structures%2C%20it%20introduces%20CLICK-MoE%20%28CLass%2C%20Instance%2C%20and%20Common%20Knowledge%20Mixture%20of%20Experts%29%2C%20decoupling%20class%20and%20instance%20representations%20to%20reduce%20confusion%20among%20adjacent%20or%20overlapping%20instances.%20Finally%2C%20we%20implement%20a%20multi-point%20training%20strategy%20which%20promotes%20prediction%20consistency%20across%20different%20point%20placements%2C%20improving%20robustness%20to%20annotation%20variability.%20DExTeR%20achieves%20state-of-the-art%20performance%20across%20three%20datasets%20spanning%20different%20medical%20domains%20%28endoscopy%2C%20chest%20X-rays%2C%20and%20endoscopic%20ultrasound%29%20highlighting%20its%20potential%20to%20reduce%20annotation%20costs%20while%20maintaining%20high%20detection%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13954v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDExTeR%253A%2520Weakly%2520Semi-Supervised%2520Object%2520Detection%2520with%2520Class%2520and%2520Instance%2520Experts%2520for%2520Medical%2520Imaging%26entry.906535625%3DAdrien%2520Meyer%2520and%2520Didier%2520Mutter%2520and%2520Nicolas%2520Padoy%26entry.1292438233%3DDetecting%2520anatomical%2520landmarks%2520in%2520medical%2520imaging%2520is%2520essential%2520for%2520diagnosis%2520and%2520intervention%2520guidance.%2520However%252C%2520object%2520detection%2520models%2520rely%2520on%2520costly%2520bounding%2520box%2520annotations%252C%2520limiting%2520scalability.%2520Weakly%2520Semi-Supervised%2520Object%2520Detection%2520%2528WSSOD%2529%2520with%2520point%2520annotations%2520proposes%2520annotating%2520each%2520instance%2520with%2520a%2520single%2520point%252C%2520minimizing%2520annotation%2520time%2520while%2520preserving%2520localization%2520signals.%2520A%2520Point-to-Box%2520teacher%2520model%252C%2520trained%2520on%2520a%2520small%2520box-labeled%2520subset%252C%2520converts%2520these%2520point%2520annotations%2520into%2520pseudo-box%2520labels%2520to%2520train%2520a%2520student%2520detector.%2520Yet%252C%2520medical%2520imagery%2520presents%2520unique%2520challenges%252C%2520including%2520overlapping%2520anatomy%252C%2520variable%2520object%2520sizes%252C%2520and%2520elusive%2520structures%252C%2520which%2520hinder%2520accurate%2520bounding%2520box%2520inference.%2520To%2520overcome%2520these%2520challenges%252C%2520we%2520introduce%2520DExTeR%2520%2528DETR%2520with%2520Experts%2529%252C%2520a%2520transformer-based%2520Point-to-Box%2520regressor%2520tailored%2520for%2520medical%2520imaging.%2520Built%2520upon%2520Point-DETR%252C%2520DExTeR%2520encodes%2520single-point%2520annotations%2520as%2520object%2520queries%252C%2520refining%2520feature%2520extraction%2520with%2520the%2520proposed%2520class-guided%2520deformable%2520attention%252C%2520which%2520guides%2520attention%2520sampling%2520using%2520point%2520coordinates%2520and%2520class%2520labels%2520to%2520capture%2520class-specific%2520characteristics.%2520To%2520improve%2520discrimination%2520in%2520complex%2520structures%252C%2520it%2520introduces%2520CLICK-MoE%2520%2528CLass%252C%2520Instance%252C%2520and%2520Common%2520Knowledge%2520Mixture%2520of%2520Experts%2529%252C%2520decoupling%2520class%2520and%2520instance%2520representations%2520to%2520reduce%2520confusion%2520among%2520adjacent%2520or%2520overlapping%2520instances.%2520Finally%252C%2520we%2520implement%2520a%2520multi-point%2520training%2520strategy%2520which%2520promotes%2520prediction%2520consistency%2520across%2520different%2520point%2520placements%252C%2520improving%2520robustness%2520to%2520annotation%2520variability.%2520DExTeR%2520achieves%2520state-of-the-art%2520performance%2520across%2520three%2520datasets%2520spanning%2520different%2520medical%2520domains%2520%2528endoscopy%252C%2520chest%2520X-rays%252C%2520and%2520endoscopic%2520ultrasound%2529%2520highlighting%2520its%2520potential%2520to%2520reduce%2520annotation%2520costs%2520while%2520maintaining%2520high%2520detection%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13954v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DExTeR%3A%20Weakly%20Semi-Supervised%20Object%20Detection%20with%20Class%20and%20Instance%20Experts%20for%20Medical%20Imaging&entry.906535625=Adrien%20Meyer%20and%20Didier%20Mutter%20and%20Nicolas%20Padoy&entry.1292438233=Detecting%20anatomical%20landmarks%20in%20medical%20imaging%20is%20essential%20for%20diagnosis%20and%20intervention%20guidance.%20However%2C%20object%20detection%20models%20rely%20on%20costly%20bounding%20box%20annotations%2C%20limiting%20scalability.%20Weakly%20Semi-Supervised%20Object%20Detection%20%28WSSOD%29%20with%20point%20annotations%20proposes%20annotating%20each%20instance%20with%20a%20single%20point%2C%20minimizing%20annotation%20time%20while%20preserving%20localization%20signals.%20A%20Point-to-Box%20teacher%20model%2C%20trained%20on%20a%20small%20box-labeled%20subset%2C%20converts%20these%20point%20annotations%20into%20pseudo-box%20labels%20to%20train%20a%20student%20detector.%20Yet%2C%20medical%20imagery%20presents%20unique%20challenges%2C%20including%20overlapping%20anatomy%2C%20variable%20object%20sizes%2C%20and%20elusive%20structures%2C%20which%20hinder%20accurate%20bounding%20box%20inference.%20To%20overcome%20these%20challenges%2C%20we%20introduce%20DExTeR%20%28DETR%20with%20Experts%29%2C%20a%20transformer-based%20Point-to-Box%20regressor%20tailored%20for%20medical%20imaging.%20Built%20upon%20Point-DETR%2C%20DExTeR%20encodes%20single-point%20annotations%20as%20object%20queries%2C%20refining%20feature%20extraction%20with%20the%20proposed%20class-guided%20deformable%20attention%2C%20which%20guides%20attention%20sampling%20using%20point%20coordinates%20and%20class%20labels%20to%20capture%20class-specific%20characteristics.%20To%20improve%20discrimination%20in%20complex%20structures%2C%20it%20introduces%20CLICK-MoE%20%28CLass%2C%20Instance%2C%20and%20Common%20Knowledge%20Mixture%20of%20Experts%29%2C%20decoupling%20class%20and%20instance%20representations%20to%20reduce%20confusion%20among%20adjacent%20or%20overlapping%20instances.%20Finally%2C%20we%20implement%20a%20multi-point%20training%20strategy%20which%20promotes%20prediction%20consistency%20across%20different%20point%20placements%2C%20improving%20robustness%20to%20annotation%20variability.%20DExTeR%20achieves%20state-of-the-art%20performance%20across%20three%20datasets%20spanning%20different%20medical%20domains%20%28endoscopy%2C%20chest%20X-rays%2C%20and%20endoscopic%20ultrasound%29%20highlighting%20its%20potential%20to%20reduce%20annotation%20costs%20while%20maintaining%20high%20detection%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2601.13954v1&entry.124074799=Read"},
{"title": "GuideTouch: An Obstacle Avoidance Device for Visually Impaired", "author": "Timofei Kozlov and Artem Trandofilov and Georgii Gazaryan and Issatay Tokmurziyev and Miguel Altamirano Cabrera and Dzmitry Tsetserukou", "abstract": "Safe navigation for the visually impaired individuals remains a critical challenge, especially concerning head-level obstacles, which traditional mobility aids often fail to detect. We introduce GuideTouch, a compact, affordable, standalone wearable device designed for autonomous obstacle avoidance. The system integrates two vertically aligned Time-of-Flight (ToF) sensors, enabling three-dimensional environmental perception, and four vibrotactile actuators that provide directional haptic feedback. Proximity and direction information is communicated via an intuitive 4-point vibrotactile feedback system located across the user's shoulders and upper chest. For real-world robustness, the device includes a unique centrifugal self-cleaning optical cover mechanism and a sound alarm system for location if the device is dropped. We evaluated the haptic perception accuracy across 22 participants (17 male and 5 female, aged 21-48, mean 25.7, sd 6.1). Statistical analysis confirmed a significant difference between the perception accuracy of different patterns. The system demonstrated high recognition accuracy, achieving an average of 92.9% for single and double motor (primary directional) patterns. Furthermore, preliminary experiments with 14 visually impaired users validated this interface, showing a recognition accuracy of 93.75% for primary directional cues. The results demonstrate that GuideTouch enables intuitive spatial perception and could significantly improve the safety, confidence, and autonomy of users with visual impairments during independent navigation.", "link": "http://arxiv.org/abs/2601.13813v1", "date": "2026-01-20", "relevancy": 2.2267, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.6593}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5203}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4686}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GuideTouch%3A%20An%20Obstacle%20Avoidance%20Device%20for%20Visually%20Impaired&body=Title%3A%20GuideTouch%3A%20An%20Obstacle%20Avoidance%20Device%20for%20Visually%20Impaired%0AAuthor%3A%20Timofei%20Kozlov%20and%20Artem%20Trandofilov%20and%20Georgii%20Gazaryan%20and%20Issatay%20Tokmurziyev%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20Safe%20navigation%20for%20the%20visually%20impaired%20individuals%20remains%20a%20critical%20challenge%2C%20especially%20concerning%20head-level%20obstacles%2C%20which%20traditional%20mobility%20aids%20often%20fail%20to%20detect.%20We%20introduce%20GuideTouch%2C%20a%20compact%2C%20affordable%2C%20standalone%20wearable%20device%20designed%20for%20autonomous%20obstacle%20avoidance.%20The%20system%20integrates%20two%20vertically%20aligned%20Time-of-Flight%20%28ToF%29%20sensors%2C%20enabling%20three-dimensional%20environmental%20perception%2C%20and%20four%20vibrotactile%20actuators%20that%20provide%20directional%20haptic%20feedback.%20Proximity%20and%20direction%20information%20is%20communicated%20via%20an%20intuitive%204-point%20vibrotactile%20feedback%20system%20located%20across%20the%20user%27s%20shoulders%20and%20upper%20chest.%20For%20real-world%20robustness%2C%20the%20device%20includes%20a%20unique%20centrifugal%20self-cleaning%20optical%20cover%20mechanism%20and%20a%20sound%20alarm%20system%20for%20location%20if%20the%20device%20is%20dropped.%20We%20evaluated%20the%20haptic%20perception%20accuracy%20across%2022%20participants%20%2817%20male%20and%205%20female%2C%20aged%2021-48%2C%20mean%2025.7%2C%20sd%206.1%29.%20Statistical%20analysis%20confirmed%20a%20significant%20difference%20between%20the%20perception%20accuracy%20of%20different%20patterns.%20The%20system%20demonstrated%20high%20recognition%20accuracy%2C%20achieving%20an%20average%20of%2092.9%25%20for%20single%20and%20double%20motor%20%28primary%20directional%29%20patterns.%20Furthermore%2C%20preliminary%20experiments%20with%2014%20visually%20impaired%20users%20validated%20this%20interface%2C%20showing%20a%20recognition%20accuracy%20of%2093.75%25%20for%20primary%20directional%20cues.%20The%20results%20demonstrate%20that%20GuideTouch%20enables%20intuitive%20spatial%20perception%20and%20could%20significantly%20improve%20the%20safety%2C%20confidence%2C%20and%20autonomy%20of%20users%20with%20visual%20impairments%20during%20independent%20navigation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13813v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGuideTouch%253A%2520An%2520Obstacle%2520Avoidance%2520Device%2520for%2520Visually%2520Impaired%26entry.906535625%3DTimofei%2520Kozlov%2520and%2520Artem%2520Trandofilov%2520and%2520Georgii%2520Gazaryan%2520and%2520Issatay%2520Tokmurziyev%2520and%2520Miguel%2520Altamirano%2520Cabrera%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3DSafe%2520navigation%2520for%2520the%2520visually%2520impaired%2520individuals%2520remains%2520a%2520critical%2520challenge%252C%2520especially%2520concerning%2520head-level%2520obstacles%252C%2520which%2520traditional%2520mobility%2520aids%2520often%2520fail%2520to%2520detect.%2520We%2520introduce%2520GuideTouch%252C%2520a%2520compact%252C%2520affordable%252C%2520standalone%2520wearable%2520device%2520designed%2520for%2520autonomous%2520obstacle%2520avoidance.%2520The%2520system%2520integrates%2520two%2520vertically%2520aligned%2520Time-of-Flight%2520%2528ToF%2529%2520sensors%252C%2520enabling%2520three-dimensional%2520environmental%2520perception%252C%2520and%2520four%2520vibrotactile%2520actuators%2520that%2520provide%2520directional%2520haptic%2520feedback.%2520Proximity%2520and%2520direction%2520information%2520is%2520communicated%2520via%2520an%2520intuitive%25204-point%2520vibrotactile%2520feedback%2520system%2520located%2520across%2520the%2520user%2527s%2520shoulders%2520and%2520upper%2520chest.%2520For%2520real-world%2520robustness%252C%2520the%2520device%2520includes%2520a%2520unique%2520centrifugal%2520self-cleaning%2520optical%2520cover%2520mechanism%2520and%2520a%2520sound%2520alarm%2520system%2520for%2520location%2520if%2520the%2520device%2520is%2520dropped.%2520We%2520evaluated%2520the%2520haptic%2520perception%2520accuracy%2520across%252022%2520participants%2520%252817%2520male%2520and%25205%2520female%252C%2520aged%252021-48%252C%2520mean%252025.7%252C%2520sd%25206.1%2529.%2520Statistical%2520analysis%2520confirmed%2520a%2520significant%2520difference%2520between%2520the%2520perception%2520accuracy%2520of%2520different%2520patterns.%2520The%2520system%2520demonstrated%2520high%2520recognition%2520accuracy%252C%2520achieving%2520an%2520average%2520of%252092.9%2525%2520for%2520single%2520and%2520double%2520motor%2520%2528primary%2520directional%2529%2520patterns.%2520Furthermore%252C%2520preliminary%2520experiments%2520with%252014%2520visually%2520impaired%2520users%2520validated%2520this%2520interface%252C%2520showing%2520a%2520recognition%2520accuracy%2520of%252093.75%2525%2520for%2520primary%2520directional%2520cues.%2520The%2520results%2520demonstrate%2520that%2520GuideTouch%2520enables%2520intuitive%2520spatial%2520perception%2520and%2520could%2520significantly%2520improve%2520the%2520safety%252C%2520confidence%252C%2520and%2520autonomy%2520of%2520users%2520with%2520visual%2520impairments%2520during%2520independent%2520navigation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13813v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GuideTouch%3A%20An%20Obstacle%20Avoidance%20Device%20for%20Visually%20Impaired&entry.906535625=Timofei%20Kozlov%20and%20Artem%20Trandofilov%20and%20Georgii%20Gazaryan%20and%20Issatay%20Tokmurziyev%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou&entry.1292438233=Safe%20navigation%20for%20the%20visually%20impaired%20individuals%20remains%20a%20critical%20challenge%2C%20especially%20concerning%20head-level%20obstacles%2C%20which%20traditional%20mobility%20aids%20often%20fail%20to%20detect.%20We%20introduce%20GuideTouch%2C%20a%20compact%2C%20affordable%2C%20standalone%20wearable%20device%20designed%20for%20autonomous%20obstacle%20avoidance.%20The%20system%20integrates%20two%20vertically%20aligned%20Time-of-Flight%20%28ToF%29%20sensors%2C%20enabling%20three-dimensional%20environmental%20perception%2C%20and%20four%20vibrotactile%20actuators%20that%20provide%20directional%20haptic%20feedback.%20Proximity%20and%20direction%20information%20is%20communicated%20via%20an%20intuitive%204-point%20vibrotactile%20feedback%20system%20located%20across%20the%20user%27s%20shoulders%20and%20upper%20chest.%20For%20real-world%20robustness%2C%20the%20device%20includes%20a%20unique%20centrifugal%20self-cleaning%20optical%20cover%20mechanism%20and%20a%20sound%20alarm%20system%20for%20location%20if%20the%20device%20is%20dropped.%20We%20evaluated%20the%20haptic%20perception%20accuracy%20across%2022%20participants%20%2817%20male%20and%205%20female%2C%20aged%2021-48%2C%20mean%2025.7%2C%20sd%206.1%29.%20Statistical%20analysis%20confirmed%20a%20significant%20difference%20between%20the%20perception%20accuracy%20of%20different%20patterns.%20The%20system%20demonstrated%20high%20recognition%20accuracy%2C%20achieving%20an%20average%20of%2092.9%25%20for%20single%20and%20double%20motor%20%28primary%20directional%29%20patterns.%20Furthermore%2C%20preliminary%20experiments%20with%2014%20visually%20impaired%20users%20validated%20this%20interface%2C%20showing%20a%20recognition%20accuracy%20of%2093.75%25%20for%20primary%20directional%20cues.%20The%20results%20demonstrate%20that%20GuideTouch%20enables%20intuitive%20spatial%20perception%20and%20could%20significantly%20improve%20the%20safety%2C%20confidence%2C%20and%20autonomy%20of%20users%20with%20visual%20impairments%20during%20independent%20navigation.&entry.1838667208=http%3A//arxiv.org/abs/2601.13813v1&entry.124074799=Read"},
{"title": "FutureOmni: Evaluating Future Forecasting from Omni-Modal Context for Multimodal LLMs", "author": "Qian Chen and Jinlan Fu and Changsong Li and See-Kiong Ng and Xipeng Qiu", "abstract": "Although Multimodal Large Language Models (MLLMs) demonstrate strong omni-modal perception, their ability to forecast future events from audio-visual cues remains largely unexplored, as existing benchmarks focus mainly on retrospective understanding. To bridge this gap, we introduce FutureOmni, the first benchmark designed to evaluate omni-modal future forecasting from audio-visual environments. The evaluated models are required to perform cross-modal causal and temporal reasoning, as well as effectively leverage internal knowledge to predict future events. FutureOmni is constructed via a scalable LLM-assisted, human-in-the-loop pipeline and contains 919 videos and 1,034 multiple-choice QA pairs across 8 primary domains. Evaluations on 13 omni-modal and 7 video-only models show that current systems struggle with audio-visual future prediction, particularly in speech-heavy scenarios, with the best accuracy of 64.8% achieved by Gemini 3 Flash. To mitigate this limitation, we curate a 7K-sample instruction-tuning dataset and propose an Omni-Modal Future Forecasting (OFF) training strategy. Evaluations on FutureOmni and popular audio-visual and video-only benchmarks demonstrate that OFF enhances future forecasting and generalization. We publicly release all code (https://github.com/OpenMOSS/FutureOmni) and datasets (https://huggingface.co/datasets/OpenMOSS-Team/FutureOmni).", "link": "http://arxiv.org/abs/2601.13836v1", "date": "2026-01-20", "relevancy": 2.22, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5582}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5582}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5389}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FutureOmni%3A%20Evaluating%20Future%20Forecasting%20from%20Omni-Modal%20Context%20for%20Multimodal%20LLMs&body=Title%3A%20FutureOmni%3A%20Evaluating%20Future%20Forecasting%20from%20Omni-Modal%20Context%20for%20Multimodal%20LLMs%0AAuthor%3A%20Qian%20Chen%20and%20Jinlan%20Fu%20and%20Changsong%20Li%20and%20See-Kiong%20Ng%20and%20Xipeng%20Qiu%0AAbstract%3A%20Although%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20demonstrate%20strong%20omni-modal%20perception%2C%20their%20ability%20to%20forecast%20future%20events%20from%20audio-visual%20cues%20remains%20largely%20unexplored%2C%20as%20existing%20benchmarks%20focus%20mainly%20on%20retrospective%20understanding.%20To%20bridge%20this%20gap%2C%20we%20introduce%20FutureOmni%2C%20the%20first%20benchmark%20designed%20to%20evaluate%20omni-modal%20future%20forecasting%20from%20audio-visual%20environments.%20The%20evaluated%20models%20are%20required%20to%20perform%20cross-modal%20causal%20and%20temporal%20reasoning%2C%20as%20well%20as%20effectively%20leverage%20internal%20knowledge%20to%20predict%20future%20events.%20FutureOmni%20is%20constructed%20via%20a%20scalable%20LLM-assisted%2C%20human-in-the-loop%20pipeline%20and%20contains%20919%20videos%20and%201%2C034%20multiple-choice%20QA%20pairs%20across%208%20primary%20domains.%20Evaluations%20on%2013%20omni-modal%20and%207%20video-only%20models%20show%20that%20current%20systems%20struggle%20with%20audio-visual%20future%20prediction%2C%20particularly%20in%20speech-heavy%20scenarios%2C%20with%20the%20best%20accuracy%20of%2064.8%25%20achieved%20by%20Gemini%203%20Flash.%20To%20mitigate%20this%20limitation%2C%20we%20curate%20a%207K-sample%20instruction-tuning%20dataset%20and%20propose%20an%20Omni-Modal%20Future%20Forecasting%20%28OFF%29%20training%20strategy.%20Evaluations%20on%20FutureOmni%20and%20popular%20audio-visual%20and%20video-only%20benchmarks%20demonstrate%20that%20OFF%20enhances%20future%20forecasting%20and%20generalization.%20We%20publicly%20release%20all%20code%20%28https%3A//github.com/OpenMOSS/FutureOmni%29%20and%20datasets%20%28https%3A//huggingface.co/datasets/OpenMOSS-Team/FutureOmni%29.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13836v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFutureOmni%253A%2520Evaluating%2520Future%2520Forecasting%2520from%2520Omni-Modal%2520Context%2520for%2520Multimodal%2520LLMs%26entry.906535625%3DQian%2520Chen%2520and%2520Jinlan%2520Fu%2520and%2520Changsong%2520Li%2520and%2520See-Kiong%2520Ng%2520and%2520Xipeng%2520Qiu%26entry.1292438233%3DAlthough%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520demonstrate%2520strong%2520omni-modal%2520perception%252C%2520their%2520ability%2520to%2520forecast%2520future%2520events%2520from%2520audio-visual%2520cues%2520remains%2520largely%2520unexplored%252C%2520as%2520existing%2520benchmarks%2520focus%2520mainly%2520on%2520retrospective%2520understanding.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520FutureOmni%252C%2520the%2520first%2520benchmark%2520designed%2520to%2520evaluate%2520omni-modal%2520future%2520forecasting%2520from%2520audio-visual%2520environments.%2520The%2520evaluated%2520models%2520are%2520required%2520to%2520perform%2520cross-modal%2520causal%2520and%2520temporal%2520reasoning%252C%2520as%2520well%2520as%2520effectively%2520leverage%2520internal%2520knowledge%2520to%2520predict%2520future%2520events.%2520FutureOmni%2520is%2520constructed%2520via%2520a%2520scalable%2520LLM-assisted%252C%2520human-in-the-loop%2520pipeline%2520and%2520contains%2520919%2520videos%2520and%25201%252C034%2520multiple-choice%2520QA%2520pairs%2520across%25208%2520primary%2520domains.%2520Evaluations%2520on%252013%2520omni-modal%2520and%25207%2520video-only%2520models%2520show%2520that%2520current%2520systems%2520struggle%2520with%2520audio-visual%2520future%2520prediction%252C%2520particularly%2520in%2520speech-heavy%2520scenarios%252C%2520with%2520the%2520best%2520accuracy%2520of%252064.8%2525%2520achieved%2520by%2520Gemini%25203%2520Flash.%2520To%2520mitigate%2520this%2520limitation%252C%2520we%2520curate%2520a%25207K-sample%2520instruction-tuning%2520dataset%2520and%2520propose%2520an%2520Omni-Modal%2520Future%2520Forecasting%2520%2528OFF%2529%2520training%2520strategy.%2520Evaluations%2520on%2520FutureOmni%2520and%2520popular%2520audio-visual%2520and%2520video-only%2520benchmarks%2520demonstrate%2520that%2520OFF%2520enhances%2520future%2520forecasting%2520and%2520generalization.%2520We%2520publicly%2520release%2520all%2520code%2520%2528https%253A//github.com/OpenMOSS/FutureOmni%2529%2520and%2520datasets%2520%2528https%253A//huggingface.co/datasets/OpenMOSS-Team/FutureOmni%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13836v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FutureOmni%3A%20Evaluating%20Future%20Forecasting%20from%20Omni-Modal%20Context%20for%20Multimodal%20LLMs&entry.906535625=Qian%20Chen%20and%20Jinlan%20Fu%20and%20Changsong%20Li%20and%20See-Kiong%20Ng%20and%20Xipeng%20Qiu&entry.1292438233=Although%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20demonstrate%20strong%20omni-modal%20perception%2C%20their%20ability%20to%20forecast%20future%20events%20from%20audio-visual%20cues%20remains%20largely%20unexplored%2C%20as%20existing%20benchmarks%20focus%20mainly%20on%20retrospective%20understanding.%20To%20bridge%20this%20gap%2C%20we%20introduce%20FutureOmni%2C%20the%20first%20benchmark%20designed%20to%20evaluate%20omni-modal%20future%20forecasting%20from%20audio-visual%20environments.%20The%20evaluated%20models%20are%20required%20to%20perform%20cross-modal%20causal%20and%20temporal%20reasoning%2C%20as%20well%20as%20effectively%20leverage%20internal%20knowledge%20to%20predict%20future%20events.%20FutureOmni%20is%20constructed%20via%20a%20scalable%20LLM-assisted%2C%20human-in-the-loop%20pipeline%20and%20contains%20919%20videos%20and%201%2C034%20multiple-choice%20QA%20pairs%20across%208%20primary%20domains.%20Evaluations%20on%2013%20omni-modal%20and%207%20video-only%20models%20show%20that%20current%20systems%20struggle%20with%20audio-visual%20future%20prediction%2C%20particularly%20in%20speech-heavy%20scenarios%2C%20with%20the%20best%20accuracy%20of%2064.8%25%20achieved%20by%20Gemini%203%20Flash.%20To%20mitigate%20this%20limitation%2C%20we%20curate%20a%207K-sample%20instruction-tuning%20dataset%20and%20propose%20an%20Omni-Modal%20Future%20Forecasting%20%28OFF%29%20training%20strategy.%20Evaluations%20on%20FutureOmni%20and%20popular%20audio-visual%20and%20video-only%20benchmarks%20demonstrate%20that%20OFF%20enhances%20future%20forecasting%20and%20generalization.%20We%20publicly%20release%20all%20code%20%28https%3A//github.com/OpenMOSS/FutureOmni%29%20and%20datasets%20%28https%3A//huggingface.co/datasets/OpenMOSS-Team/FutureOmni%29.&entry.1838667208=http%3A//arxiv.org/abs/2601.13836v1&entry.124074799=Read"},
{"title": "The Case for \"Thick Evaluations\" of Cultural Representation in AI", "author": "Rida Qadri and Mark Diaz and Ding Wang and Michael Madaio", "abstract": "Generative AI model outputs have been increasingly evaluated for their (in)ability to represent non-Western cultures. We argue that these evaluations often operate through reductive ideals of representation, abstracted from how people define their own representation and neglecting the inherently interpretive and contextual nature of cultural representation. In contrast to these 'thin' evaluations, we introduce the idea of 'thick evaluations:' a more granular, situated, and discursive measurement framework for evaluating representations of social worlds in AI outputs, steeped in communities' own understandings of representation. We develop this evaluation framework through workshops in South Asia, by studying the 'thick' ways in which people interpret and assign meaning to AI-generated images of their own cultures. We introduce practices for thicker evaluations of representation that expand the understanding of representation underpinning AI evaluations and by co-constructing metrics with communities, bringing measurement in line with the experiences of communities on the ground.", "link": "http://arxiv.org/abs/2503.19075v2", "date": "2026-01-20", "relevancy": 2.2171, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4509}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4418}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4376}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Case%20for%20%22Thick%20Evaluations%22%20of%20Cultural%20Representation%20in%20AI&body=Title%3A%20The%20Case%20for%20%22Thick%20Evaluations%22%20of%20Cultural%20Representation%20in%20AI%0AAuthor%3A%20Rida%20Qadri%20and%20Mark%20Diaz%20and%20Ding%20Wang%20and%20Michael%20Madaio%0AAbstract%3A%20Generative%20AI%20model%20outputs%20have%20been%20increasingly%20evaluated%20for%20their%20%28in%29ability%20to%20represent%20non-Western%20cultures.%20We%20argue%20that%20these%20evaluations%20often%20operate%20through%20reductive%20ideals%20of%20representation%2C%20abstracted%20from%20how%20people%20define%20their%20own%20representation%20and%20neglecting%20the%20inherently%20interpretive%20and%20contextual%20nature%20of%20cultural%20representation.%20In%20contrast%20to%20these%20%27thin%27%20evaluations%2C%20we%20introduce%20the%20idea%20of%20%27thick%20evaluations%3A%27%20a%20more%20granular%2C%20situated%2C%20and%20discursive%20measurement%20framework%20for%20evaluating%20representations%20of%20social%20worlds%20in%20AI%20outputs%2C%20steeped%20in%20communities%27%20own%20understandings%20of%20representation.%20We%20develop%20this%20evaluation%20framework%20through%20workshops%20in%20South%20Asia%2C%20by%20studying%20the%20%27thick%27%20ways%20in%20which%20people%20interpret%20and%20assign%20meaning%20to%20AI-generated%20images%20of%20their%20own%20cultures.%20We%20introduce%20practices%20for%20thicker%20evaluations%20of%20representation%20that%20expand%20the%20understanding%20of%20representation%20underpinning%20AI%20evaluations%20and%20by%20co-constructing%20metrics%20with%20communities%2C%20bringing%20measurement%20in%20line%20with%20the%20experiences%20of%20communities%20on%20the%20ground.%0ALink%3A%20http%3A//arxiv.org/abs/2503.19075v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Case%2520for%2520%2522Thick%2520Evaluations%2522%2520of%2520Cultural%2520Representation%2520in%2520AI%26entry.906535625%3DRida%2520Qadri%2520and%2520Mark%2520Diaz%2520and%2520Ding%2520Wang%2520and%2520Michael%2520Madaio%26entry.1292438233%3DGenerative%2520AI%2520model%2520outputs%2520have%2520been%2520increasingly%2520evaluated%2520for%2520their%2520%2528in%2529ability%2520to%2520represent%2520non-Western%2520cultures.%2520We%2520argue%2520that%2520these%2520evaluations%2520often%2520operate%2520through%2520reductive%2520ideals%2520of%2520representation%252C%2520abstracted%2520from%2520how%2520people%2520define%2520their%2520own%2520representation%2520and%2520neglecting%2520the%2520inherently%2520interpretive%2520and%2520contextual%2520nature%2520of%2520cultural%2520representation.%2520In%2520contrast%2520to%2520these%2520%2527thin%2527%2520evaluations%252C%2520we%2520introduce%2520the%2520idea%2520of%2520%2527thick%2520evaluations%253A%2527%2520a%2520more%2520granular%252C%2520situated%252C%2520and%2520discursive%2520measurement%2520framework%2520for%2520evaluating%2520representations%2520of%2520social%2520worlds%2520in%2520AI%2520outputs%252C%2520steeped%2520in%2520communities%2527%2520own%2520understandings%2520of%2520representation.%2520We%2520develop%2520this%2520evaluation%2520framework%2520through%2520workshops%2520in%2520South%2520Asia%252C%2520by%2520studying%2520the%2520%2527thick%2527%2520ways%2520in%2520which%2520people%2520interpret%2520and%2520assign%2520meaning%2520to%2520AI-generated%2520images%2520of%2520their%2520own%2520cultures.%2520We%2520introduce%2520practices%2520for%2520thicker%2520evaluations%2520of%2520representation%2520that%2520expand%2520the%2520understanding%2520of%2520representation%2520underpinning%2520AI%2520evaluations%2520and%2520by%2520co-constructing%2520metrics%2520with%2520communities%252C%2520bringing%2520measurement%2520in%2520line%2520with%2520the%2520experiences%2520of%2520communities%2520on%2520the%2520ground.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.19075v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Case%20for%20%22Thick%20Evaluations%22%20of%20Cultural%20Representation%20in%20AI&entry.906535625=Rida%20Qadri%20and%20Mark%20Diaz%20and%20Ding%20Wang%20and%20Michael%20Madaio&entry.1292438233=Generative%20AI%20model%20outputs%20have%20been%20increasingly%20evaluated%20for%20their%20%28in%29ability%20to%20represent%20non-Western%20cultures.%20We%20argue%20that%20these%20evaluations%20often%20operate%20through%20reductive%20ideals%20of%20representation%2C%20abstracted%20from%20how%20people%20define%20their%20own%20representation%20and%20neglecting%20the%20inherently%20interpretive%20and%20contextual%20nature%20of%20cultural%20representation.%20In%20contrast%20to%20these%20%27thin%27%20evaluations%2C%20we%20introduce%20the%20idea%20of%20%27thick%20evaluations%3A%27%20a%20more%20granular%2C%20situated%2C%20and%20discursive%20measurement%20framework%20for%20evaluating%20representations%20of%20social%20worlds%20in%20AI%20outputs%2C%20steeped%20in%20communities%27%20own%20understandings%20of%20representation.%20We%20develop%20this%20evaluation%20framework%20through%20workshops%20in%20South%20Asia%2C%20by%20studying%20the%20%27thick%27%20ways%20in%20which%20people%20interpret%20and%20assign%20meaning%20to%20AI-generated%20images%20of%20their%20own%20cultures.%20We%20introduce%20practices%20for%20thicker%20evaluations%20of%20representation%20that%20expand%20the%20understanding%20of%20representation%20underpinning%20AI%20evaluations%20and%20by%20co-constructing%20metrics%20with%20communities%2C%20bringing%20measurement%20in%20line%20with%20the%20experiences%20of%20communities%20on%20the%20ground.&entry.1838667208=http%3A//arxiv.org/abs/2503.19075v2&entry.124074799=Read"},
{"title": "SoK: On the Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems", "author": "Quentin Le Roux and Yannick Teglia and Teddy Furon and Philippe Loubet-Moundi and Eric Bourbao", "abstract": "The widespread deployment of Deep Learning-based Face Recognition Systems raises many security concerns. While prior research has identified backdoor vulnerabilities on isolated components, Backdoor Attacks on real-world, unconstrained pipelines remain underexplored. This SoK paper presents the first comprehensive system-level analysis and measurement of the impact of Backdoor Attacks on fully-fledged Face Recognition Systems. We combine the existing Supervised Learning backdoor literature targeting face detectors, face antispoofing, and face feature extractors to demonstrate a system-level vulnerability. By analyzing 20 pipeline configurations and 15 attack scenarios in a holistic manner, we reveal that an attacker only needs a single backdoored model to compromise an entire Face Recognition System. Finally, we discuss the impact of such attacks and propose best practices and countermeasures for stakeholders.", "link": "http://arxiv.org/abs/2507.01607v5", "date": "2026-01-20", "relevancy": 2.2166, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4597}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4372}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4331}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SoK%3A%20On%20the%20Survivability%20of%20Backdoor%20Attacks%20on%20Unconstrained%20Face%20Recognition%20Systems&body=Title%3A%20SoK%3A%20On%20the%20Survivability%20of%20Backdoor%20Attacks%20on%20Unconstrained%20Face%20Recognition%20Systems%0AAuthor%3A%20Quentin%20Le%20Roux%20and%20Yannick%20Teglia%20and%20Teddy%20Furon%20and%20Philippe%20Loubet-Moundi%20and%20Eric%20Bourbao%0AAbstract%3A%20The%20widespread%20deployment%20of%20Deep%20Learning-based%20Face%20Recognition%20Systems%20raises%20many%20security%20concerns.%20While%20prior%20research%20has%20identified%20backdoor%20vulnerabilities%20on%20isolated%20components%2C%20Backdoor%20Attacks%20on%20real-world%2C%20unconstrained%20pipelines%20remain%20underexplored.%20This%20SoK%20paper%20presents%20the%20first%20comprehensive%20system-level%20analysis%20and%20measurement%20of%20the%20impact%20of%20Backdoor%20Attacks%20on%20fully-fledged%20Face%20Recognition%20Systems.%20We%20combine%20the%20existing%20Supervised%20Learning%20backdoor%20literature%20targeting%20face%20detectors%2C%20face%20antispoofing%2C%20and%20face%20feature%20extractors%20to%20demonstrate%20a%20system-level%20vulnerability.%20By%20analyzing%2020%20pipeline%20configurations%20and%2015%20attack%20scenarios%20in%20a%20holistic%20manner%2C%20we%20reveal%20that%20an%20attacker%20only%20needs%20a%20single%20backdoored%20model%20to%20compromise%20an%20entire%20Face%20Recognition%20System.%20Finally%2C%20we%20discuss%20the%20impact%20of%20such%20attacks%20and%20propose%20best%20practices%20and%20countermeasures%20for%20stakeholders.%0ALink%3A%20http%3A//arxiv.org/abs/2507.01607v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSoK%253A%2520On%2520the%2520Survivability%2520of%2520Backdoor%2520Attacks%2520on%2520Unconstrained%2520Face%2520Recognition%2520Systems%26entry.906535625%3DQuentin%2520Le%2520Roux%2520and%2520Yannick%2520Teglia%2520and%2520Teddy%2520Furon%2520and%2520Philippe%2520Loubet-Moundi%2520and%2520Eric%2520Bourbao%26entry.1292438233%3DThe%2520widespread%2520deployment%2520of%2520Deep%2520Learning-based%2520Face%2520Recognition%2520Systems%2520raises%2520many%2520security%2520concerns.%2520While%2520prior%2520research%2520has%2520identified%2520backdoor%2520vulnerabilities%2520on%2520isolated%2520components%252C%2520Backdoor%2520Attacks%2520on%2520real-world%252C%2520unconstrained%2520pipelines%2520remain%2520underexplored.%2520This%2520SoK%2520paper%2520presents%2520the%2520first%2520comprehensive%2520system-level%2520analysis%2520and%2520measurement%2520of%2520the%2520impact%2520of%2520Backdoor%2520Attacks%2520on%2520fully-fledged%2520Face%2520Recognition%2520Systems.%2520We%2520combine%2520the%2520existing%2520Supervised%2520Learning%2520backdoor%2520literature%2520targeting%2520face%2520detectors%252C%2520face%2520antispoofing%252C%2520and%2520face%2520feature%2520extractors%2520to%2520demonstrate%2520a%2520system-level%2520vulnerability.%2520By%2520analyzing%252020%2520pipeline%2520configurations%2520and%252015%2520attack%2520scenarios%2520in%2520a%2520holistic%2520manner%252C%2520we%2520reveal%2520that%2520an%2520attacker%2520only%2520needs%2520a%2520single%2520backdoored%2520model%2520to%2520compromise%2520an%2520entire%2520Face%2520Recognition%2520System.%2520Finally%252C%2520we%2520discuss%2520the%2520impact%2520of%2520such%2520attacks%2520and%2520propose%2520best%2520practices%2520and%2520countermeasures%2520for%2520stakeholders.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.01607v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SoK%3A%20On%20the%20Survivability%20of%20Backdoor%20Attacks%20on%20Unconstrained%20Face%20Recognition%20Systems&entry.906535625=Quentin%20Le%20Roux%20and%20Yannick%20Teglia%20and%20Teddy%20Furon%20and%20Philippe%20Loubet-Moundi%20and%20Eric%20Bourbao&entry.1292438233=The%20widespread%20deployment%20of%20Deep%20Learning-based%20Face%20Recognition%20Systems%20raises%20many%20security%20concerns.%20While%20prior%20research%20has%20identified%20backdoor%20vulnerabilities%20on%20isolated%20components%2C%20Backdoor%20Attacks%20on%20real-world%2C%20unconstrained%20pipelines%20remain%20underexplored.%20This%20SoK%20paper%20presents%20the%20first%20comprehensive%20system-level%20analysis%20and%20measurement%20of%20the%20impact%20of%20Backdoor%20Attacks%20on%20fully-fledged%20Face%20Recognition%20Systems.%20We%20combine%20the%20existing%20Supervised%20Learning%20backdoor%20literature%20targeting%20face%20detectors%2C%20face%20antispoofing%2C%20and%20face%20feature%20extractors%20to%20demonstrate%20a%20system-level%20vulnerability.%20By%20analyzing%2020%20pipeline%20configurations%20and%2015%20attack%20scenarios%20in%20a%20holistic%20manner%2C%20we%20reveal%20that%20an%20attacker%20only%20needs%20a%20single%20backdoored%20model%20to%20compromise%20an%20entire%20Face%20Recognition%20System.%20Finally%2C%20we%20discuss%20the%20impact%20of%20such%20attacks%20and%20propose%20best%20practices%20and%20countermeasures%20for%20stakeholders.&entry.1838667208=http%3A//arxiv.org/abs/2507.01607v5&entry.124074799=Read"},
{"title": "VENI: Variational Encoder for Natural Illumination", "author": "Paul Walker and James A. D. Gardner and Andreea Ardelean and William A. P. Smith and Bernhard Egger", "abstract": "Inverse rendering is an ill-posed problem, but priors like illumination priors, can simplify it. Existing work either disregards the spherical and rotation-equivariant nature of illumination environments or does not provide a well-behaved latent space. We propose a rotation-equivariant variational autoencoder that models natural illumination on the sphere without relying on 2D projections. To preserve the SO(2)-equivariance of environment maps, we use a novel Vector Neuron Vision Transformer (VN-ViT) as encoder and a rotation-equivariant conditional neural field as decoder. In the encoder, we reduce the equivariance from SO(3) to SO(2) using a novel SO(2)-equivariant fully connected layer, an extension of Vector Neurons. We show that our SO(2)-equivariant fully connected layer outperforms standard Vector Neurons when used in our SO(2)-equivariant model. Compared to previous methods, our variational autoencoder enables smoother interpolation in latent space and offers a more well-behaved latent space.", "link": "http://arxiv.org/abs/2601.14079v1", "date": "2026-01-20", "relevancy": 2.2122, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5699}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5421}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5405}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VENI%3A%20Variational%20Encoder%20for%20Natural%20Illumination&body=Title%3A%20VENI%3A%20Variational%20Encoder%20for%20Natural%20Illumination%0AAuthor%3A%20Paul%20Walker%20and%20James%20A.%20D.%20Gardner%20and%20Andreea%20Ardelean%20and%20William%20A.%20P.%20Smith%20and%20Bernhard%20Egger%0AAbstract%3A%20Inverse%20rendering%20is%20an%20ill-posed%20problem%2C%20but%20priors%20like%20illumination%20priors%2C%20can%20simplify%20it.%20Existing%20work%20either%20disregards%20the%20spherical%20and%20rotation-equivariant%20nature%20of%20illumination%20environments%20or%20does%20not%20provide%20a%20well-behaved%20latent%20space.%20We%20propose%20a%20rotation-equivariant%20variational%20autoencoder%20that%20models%20natural%20illumination%20on%20the%20sphere%20without%20relying%20on%202D%20projections.%20To%20preserve%20the%20SO%282%29-equivariance%20of%20environment%20maps%2C%20we%20use%20a%20novel%20Vector%20Neuron%20Vision%20Transformer%20%28VN-ViT%29%20as%20encoder%20and%20a%20rotation-equivariant%20conditional%20neural%20field%20as%20decoder.%20In%20the%20encoder%2C%20we%20reduce%20the%20equivariance%20from%20SO%283%29%20to%20SO%282%29%20using%20a%20novel%20SO%282%29-equivariant%20fully%20connected%20layer%2C%20an%20extension%20of%20Vector%20Neurons.%20We%20show%20that%20our%20SO%282%29-equivariant%20fully%20connected%20layer%20outperforms%20standard%20Vector%20Neurons%20when%20used%20in%20our%20SO%282%29-equivariant%20model.%20Compared%20to%20previous%20methods%2C%20our%20variational%20autoencoder%20enables%20smoother%20interpolation%20in%20latent%20space%20and%20offers%20a%20more%20well-behaved%20latent%20space.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14079v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVENI%253A%2520Variational%2520Encoder%2520for%2520Natural%2520Illumination%26entry.906535625%3DPaul%2520Walker%2520and%2520James%2520A.%2520D.%2520Gardner%2520and%2520Andreea%2520Ardelean%2520and%2520William%2520A.%2520P.%2520Smith%2520and%2520Bernhard%2520Egger%26entry.1292438233%3DInverse%2520rendering%2520is%2520an%2520ill-posed%2520problem%252C%2520but%2520priors%2520like%2520illumination%2520priors%252C%2520can%2520simplify%2520it.%2520Existing%2520work%2520either%2520disregards%2520the%2520spherical%2520and%2520rotation-equivariant%2520nature%2520of%2520illumination%2520environments%2520or%2520does%2520not%2520provide%2520a%2520well-behaved%2520latent%2520space.%2520We%2520propose%2520a%2520rotation-equivariant%2520variational%2520autoencoder%2520that%2520models%2520natural%2520illumination%2520on%2520the%2520sphere%2520without%2520relying%2520on%25202D%2520projections.%2520To%2520preserve%2520the%2520SO%25282%2529-equivariance%2520of%2520environment%2520maps%252C%2520we%2520use%2520a%2520novel%2520Vector%2520Neuron%2520Vision%2520Transformer%2520%2528VN-ViT%2529%2520as%2520encoder%2520and%2520a%2520rotation-equivariant%2520conditional%2520neural%2520field%2520as%2520decoder.%2520In%2520the%2520encoder%252C%2520we%2520reduce%2520the%2520equivariance%2520from%2520SO%25283%2529%2520to%2520SO%25282%2529%2520using%2520a%2520novel%2520SO%25282%2529-equivariant%2520fully%2520connected%2520layer%252C%2520an%2520extension%2520of%2520Vector%2520Neurons.%2520We%2520show%2520that%2520our%2520SO%25282%2529-equivariant%2520fully%2520connected%2520layer%2520outperforms%2520standard%2520Vector%2520Neurons%2520when%2520used%2520in%2520our%2520SO%25282%2529-equivariant%2520model.%2520Compared%2520to%2520previous%2520methods%252C%2520our%2520variational%2520autoencoder%2520enables%2520smoother%2520interpolation%2520in%2520latent%2520space%2520and%2520offers%2520a%2520more%2520well-behaved%2520latent%2520space.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14079v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VENI%3A%20Variational%20Encoder%20for%20Natural%20Illumination&entry.906535625=Paul%20Walker%20and%20James%20A.%20D.%20Gardner%20and%20Andreea%20Ardelean%20and%20William%20A.%20P.%20Smith%20and%20Bernhard%20Egger&entry.1292438233=Inverse%20rendering%20is%20an%20ill-posed%20problem%2C%20but%20priors%20like%20illumination%20priors%2C%20can%20simplify%20it.%20Existing%20work%20either%20disregards%20the%20spherical%20and%20rotation-equivariant%20nature%20of%20illumination%20environments%20or%20does%20not%20provide%20a%20well-behaved%20latent%20space.%20We%20propose%20a%20rotation-equivariant%20variational%20autoencoder%20that%20models%20natural%20illumination%20on%20the%20sphere%20without%20relying%20on%202D%20projections.%20To%20preserve%20the%20SO%282%29-equivariance%20of%20environment%20maps%2C%20we%20use%20a%20novel%20Vector%20Neuron%20Vision%20Transformer%20%28VN-ViT%29%20as%20encoder%20and%20a%20rotation-equivariant%20conditional%20neural%20field%20as%20decoder.%20In%20the%20encoder%2C%20we%20reduce%20the%20equivariance%20from%20SO%283%29%20to%20SO%282%29%20using%20a%20novel%20SO%282%29-equivariant%20fully%20connected%20layer%2C%20an%20extension%20of%20Vector%20Neurons.%20We%20show%20that%20our%20SO%282%29-equivariant%20fully%20connected%20layer%20outperforms%20standard%20Vector%20Neurons%20when%20used%20in%20our%20SO%282%29-equivariant%20model.%20Compared%20to%20previous%20methods%2C%20our%20variational%20autoencoder%20enables%20smoother%20interpolation%20in%20latent%20space%20and%20offers%20a%20more%20well-behaved%20latent%20space.&entry.1838667208=http%3A//arxiv.org/abs/2601.14079v1&entry.124074799=Read"},
{"title": "PGOT: A Physics-Geometry Operator Transformer for Complex PDEs", "author": "Zhuo Zhang and Xi Yang and Ying Miao and Xiaobin Hu and Yifu Gao and Yuan Zhao and Yong Yang and Canqun Yang and Boocheong Khoo", "abstract": "While Transformers have demonstrated remarkable potential in modeling Partial Differential Equations (PDEs), modeling large-scale unstructured meshes with complex geometries remains a significant challenge. Existing efficient architectures often employ feature dimensionality reduction strategies, which inadvertently induces Geometric Aliasing, resulting in the loss of critical physical boundary information. To address this, we propose the Physics-Geometry Operator Transformer (PGOT), designed to reconstruct physical feature learning through explicit geometry awareness. Specifically, we propose Spectrum-Preserving Geometric Attention (SpecGeo-Attention). Utilizing a ``physics slicing-geometry injection\" mechanism, this module incorporates multi-scale geometric encodings to explicitly preserve multi-scale geometric features while maintaining linear computational complexity $O(N)$. Furthermore, PGOT dynamically routes computations to low-order linear paths for smooth regions and high-order non-linear paths for shock waves and discontinuities based on spatial coordinates, enabling spatially adaptive and high-precision physical field modeling. PGOT achieves consistent state-of-the-art performance across four standard benchmarks and excels in large-scale industrial tasks including airfoil and car designs.", "link": "http://arxiv.org/abs/2512.23192v2", "date": "2026-01-20", "relevancy": 2.1973, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.57}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5526}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5273}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PGOT%3A%20A%20Physics-Geometry%20Operator%20Transformer%20for%20Complex%20PDEs&body=Title%3A%20PGOT%3A%20A%20Physics-Geometry%20Operator%20Transformer%20for%20Complex%20PDEs%0AAuthor%3A%20Zhuo%20Zhang%20and%20Xi%20Yang%20and%20Ying%20Miao%20and%20Xiaobin%20Hu%20and%20Yifu%20Gao%20and%20Yuan%20Zhao%20and%20Yong%20Yang%20and%20Canqun%20Yang%20and%20Boocheong%20Khoo%0AAbstract%3A%20While%20Transformers%20have%20demonstrated%20remarkable%20potential%20in%20modeling%20Partial%20Differential%20Equations%20%28PDEs%29%2C%20modeling%20large-scale%20unstructured%20meshes%20with%20complex%20geometries%20remains%20a%20significant%20challenge.%20Existing%20efficient%20architectures%20often%20employ%20feature%20dimensionality%20reduction%20strategies%2C%20which%20inadvertently%20induces%20Geometric%20Aliasing%2C%20resulting%20in%20the%20loss%20of%20critical%20physical%20boundary%20information.%20To%20address%20this%2C%20we%20propose%20the%20Physics-Geometry%20Operator%20Transformer%20%28PGOT%29%2C%20designed%20to%20reconstruct%20physical%20feature%20learning%20through%20explicit%20geometry%20awareness.%20Specifically%2C%20we%20propose%20Spectrum-Preserving%20Geometric%20Attention%20%28SpecGeo-Attention%29.%20Utilizing%20a%20%60%60physics%20slicing-geometry%20injection%22%20mechanism%2C%20this%20module%20incorporates%20multi-scale%20geometric%20encodings%20to%20explicitly%20preserve%20multi-scale%20geometric%20features%20while%20maintaining%20linear%20computational%20complexity%20%24O%28N%29%24.%20Furthermore%2C%20PGOT%20dynamically%20routes%20computations%20to%20low-order%20linear%20paths%20for%20smooth%20regions%20and%20high-order%20non-linear%20paths%20for%20shock%20waves%20and%20discontinuities%20based%20on%20spatial%20coordinates%2C%20enabling%20spatially%20adaptive%20and%20high-precision%20physical%20field%20modeling.%20PGOT%20achieves%20consistent%20state-of-the-art%20performance%20across%20four%20standard%20benchmarks%20and%20excels%20in%20large-scale%20industrial%20tasks%20including%20airfoil%20and%20car%20designs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.23192v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPGOT%253A%2520A%2520Physics-Geometry%2520Operator%2520Transformer%2520for%2520Complex%2520PDEs%26entry.906535625%3DZhuo%2520Zhang%2520and%2520Xi%2520Yang%2520and%2520Ying%2520Miao%2520and%2520Xiaobin%2520Hu%2520and%2520Yifu%2520Gao%2520and%2520Yuan%2520Zhao%2520and%2520Yong%2520Yang%2520and%2520Canqun%2520Yang%2520and%2520Boocheong%2520Khoo%26entry.1292438233%3DWhile%2520Transformers%2520have%2520demonstrated%2520remarkable%2520potential%2520in%2520modeling%2520Partial%2520Differential%2520Equations%2520%2528PDEs%2529%252C%2520modeling%2520large-scale%2520unstructured%2520meshes%2520with%2520complex%2520geometries%2520remains%2520a%2520significant%2520challenge.%2520Existing%2520efficient%2520architectures%2520often%2520employ%2520feature%2520dimensionality%2520reduction%2520strategies%252C%2520which%2520inadvertently%2520induces%2520Geometric%2520Aliasing%252C%2520resulting%2520in%2520the%2520loss%2520of%2520critical%2520physical%2520boundary%2520information.%2520To%2520address%2520this%252C%2520we%2520propose%2520the%2520Physics-Geometry%2520Operator%2520Transformer%2520%2528PGOT%2529%252C%2520designed%2520to%2520reconstruct%2520physical%2520feature%2520learning%2520through%2520explicit%2520geometry%2520awareness.%2520Specifically%252C%2520we%2520propose%2520Spectrum-Preserving%2520Geometric%2520Attention%2520%2528SpecGeo-Attention%2529.%2520Utilizing%2520a%2520%2560%2560physics%2520slicing-geometry%2520injection%2522%2520mechanism%252C%2520this%2520module%2520incorporates%2520multi-scale%2520geometric%2520encodings%2520to%2520explicitly%2520preserve%2520multi-scale%2520geometric%2520features%2520while%2520maintaining%2520linear%2520computational%2520complexity%2520%2524O%2528N%2529%2524.%2520Furthermore%252C%2520PGOT%2520dynamically%2520routes%2520computations%2520to%2520low-order%2520linear%2520paths%2520for%2520smooth%2520regions%2520and%2520high-order%2520non-linear%2520paths%2520for%2520shock%2520waves%2520and%2520discontinuities%2520based%2520on%2520spatial%2520coordinates%252C%2520enabling%2520spatially%2520adaptive%2520and%2520high-precision%2520physical%2520field%2520modeling.%2520PGOT%2520achieves%2520consistent%2520state-of-the-art%2520performance%2520across%2520four%2520standard%2520benchmarks%2520and%2520excels%2520in%2520large-scale%2520industrial%2520tasks%2520including%2520airfoil%2520and%2520car%2520designs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.23192v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PGOT%3A%20A%20Physics-Geometry%20Operator%20Transformer%20for%20Complex%20PDEs&entry.906535625=Zhuo%20Zhang%20and%20Xi%20Yang%20and%20Ying%20Miao%20and%20Xiaobin%20Hu%20and%20Yifu%20Gao%20and%20Yuan%20Zhao%20and%20Yong%20Yang%20and%20Canqun%20Yang%20and%20Boocheong%20Khoo&entry.1292438233=While%20Transformers%20have%20demonstrated%20remarkable%20potential%20in%20modeling%20Partial%20Differential%20Equations%20%28PDEs%29%2C%20modeling%20large-scale%20unstructured%20meshes%20with%20complex%20geometries%20remains%20a%20significant%20challenge.%20Existing%20efficient%20architectures%20often%20employ%20feature%20dimensionality%20reduction%20strategies%2C%20which%20inadvertently%20induces%20Geometric%20Aliasing%2C%20resulting%20in%20the%20loss%20of%20critical%20physical%20boundary%20information.%20To%20address%20this%2C%20we%20propose%20the%20Physics-Geometry%20Operator%20Transformer%20%28PGOT%29%2C%20designed%20to%20reconstruct%20physical%20feature%20learning%20through%20explicit%20geometry%20awareness.%20Specifically%2C%20we%20propose%20Spectrum-Preserving%20Geometric%20Attention%20%28SpecGeo-Attention%29.%20Utilizing%20a%20%60%60physics%20slicing-geometry%20injection%22%20mechanism%2C%20this%20module%20incorporates%20multi-scale%20geometric%20encodings%20to%20explicitly%20preserve%20multi-scale%20geometric%20features%20while%20maintaining%20linear%20computational%20complexity%20%24O%28N%29%24.%20Furthermore%2C%20PGOT%20dynamically%20routes%20computations%20to%20low-order%20linear%20paths%20for%20smooth%20regions%20and%20high-order%20non-linear%20paths%20for%20shock%20waves%20and%20discontinuities%20based%20on%20spatial%20coordinates%2C%20enabling%20spatially%20adaptive%20and%20high-precision%20physical%20field%20modeling.%20PGOT%20achieves%20consistent%20state-of-the-art%20performance%20across%20four%20standard%20benchmarks%20and%20excels%20in%20large-scale%20industrial%20tasks%20including%20airfoil%20and%20car%20designs.&entry.1838667208=http%3A//arxiv.org/abs/2512.23192v2&entry.124074799=Read"},
{"title": "The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts", "author": "Sangmitra Madhusudan and Kaige Chen and Ali Emami", "abstract": "When language models correctly parse \"The cat that the dog chased meowed,\" are they analyzing syntax or simply familiar with dogs chasing cats? Despite extensive benchmarking, we lack methods to distinguish structural understanding from semantic pattern matching. We introduce CenterBench, a dataset of 9,720 comprehension questions on center-embedded sentences (like \"The cat [that the dog chased] meowed\") where relative clauses nest recursively, creating processing demands from simple to deeply nested structures. Each sentence has a syntactically identical but semantically implausible counterpart (e.g., mailmen prescribe medicine, doctors deliver mail) and six comprehension questions testing surface understanding, syntactic dependencies, and causal reasoning. Testing six models reveals that performance gaps between plausible and implausible sentences widen systematically with complexity, with models showing median gaps up to 26.8 percentage points, quantifying when they abandon structural analysis for semantic associations. Notably, semantic plausibility harms performance on questions about resulting actions, where following causal relationships matters more than semantic coherence. Reasoning models improve accuracy but their traces show semantic shortcuts, overthinking, and answer refusal. Unlike models whose plausibility advantage systematically widens with complexity, humans shows variable semantic effects. CenterBench provides the first framework to identify when models shift from structural analysis to pattern matching.", "link": "http://arxiv.org/abs/2510.20543v2", "date": "2026-01-20", "relevancy": 2.1945, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5676}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5676}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4539}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Dog%20the%20Cat%20Chased%20Stumped%20the%20Model%3A%20Measuring%20When%20Language%20Models%20Abandon%20Structure%20for%20Shortcuts&body=Title%3A%20The%20Dog%20the%20Cat%20Chased%20Stumped%20the%20Model%3A%20Measuring%20When%20Language%20Models%20Abandon%20Structure%20for%20Shortcuts%0AAuthor%3A%20Sangmitra%20Madhusudan%20and%20Kaige%20Chen%20and%20Ali%20Emami%0AAbstract%3A%20When%20language%20models%20correctly%20parse%20%22The%20cat%20that%20the%20dog%20chased%20meowed%2C%22%20are%20they%20analyzing%20syntax%20or%20simply%20familiar%20with%20dogs%20chasing%20cats%3F%20Despite%20extensive%20benchmarking%2C%20we%20lack%20methods%20to%20distinguish%20structural%20understanding%20from%20semantic%20pattern%20matching.%20We%20introduce%20CenterBench%2C%20a%20dataset%20of%209%2C720%20comprehension%20questions%20on%20center-embedded%20sentences%20%28like%20%22The%20cat%20%5Bthat%20the%20dog%20chased%5D%20meowed%22%29%20where%20relative%20clauses%20nest%20recursively%2C%20creating%20processing%20demands%20from%20simple%20to%20deeply%20nested%20structures.%20Each%20sentence%20has%20a%20syntactically%20identical%20but%20semantically%20implausible%20counterpart%20%28e.g.%2C%20mailmen%20prescribe%20medicine%2C%20doctors%20deliver%20mail%29%20and%20six%20comprehension%20questions%20testing%20surface%20understanding%2C%20syntactic%20dependencies%2C%20and%20causal%20reasoning.%20Testing%20six%20models%20reveals%20that%20performance%20gaps%20between%20plausible%20and%20implausible%20sentences%20widen%20systematically%20with%20complexity%2C%20with%20models%20showing%20median%20gaps%20up%20to%2026.8%20percentage%20points%2C%20quantifying%20when%20they%20abandon%20structural%20analysis%20for%20semantic%20associations.%20Notably%2C%20semantic%20plausibility%20harms%20performance%20on%20questions%20about%20resulting%20actions%2C%20where%20following%20causal%20relationships%20matters%20more%20than%20semantic%20coherence.%20Reasoning%20models%20improve%20accuracy%20but%20their%20traces%20show%20semantic%20shortcuts%2C%20overthinking%2C%20and%20answer%20refusal.%20Unlike%20models%20whose%20plausibility%20advantage%20systematically%20widens%20with%20complexity%2C%20humans%20shows%20variable%20semantic%20effects.%20CenterBench%20provides%20the%20first%20framework%20to%20identify%20when%20models%20shift%20from%20structural%20analysis%20to%20pattern%20matching.%0ALink%3A%20http%3A//arxiv.org/abs/2510.20543v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Dog%2520the%2520Cat%2520Chased%2520Stumped%2520the%2520Model%253A%2520Measuring%2520When%2520Language%2520Models%2520Abandon%2520Structure%2520for%2520Shortcuts%26entry.906535625%3DSangmitra%2520Madhusudan%2520and%2520Kaige%2520Chen%2520and%2520Ali%2520Emami%26entry.1292438233%3DWhen%2520language%2520models%2520correctly%2520parse%2520%2522The%2520cat%2520that%2520the%2520dog%2520chased%2520meowed%252C%2522%2520are%2520they%2520analyzing%2520syntax%2520or%2520simply%2520familiar%2520with%2520dogs%2520chasing%2520cats%253F%2520Despite%2520extensive%2520benchmarking%252C%2520we%2520lack%2520methods%2520to%2520distinguish%2520structural%2520understanding%2520from%2520semantic%2520pattern%2520matching.%2520We%2520introduce%2520CenterBench%252C%2520a%2520dataset%2520of%25209%252C720%2520comprehension%2520questions%2520on%2520center-embedded%2520sentences%2520%2528like%2520%2522The%2520cat%2520%255Bthat%2520the%2520dog%2520chased%255D%2520meowed%2522%2529%2520where%2520relative%2520clauses%2520nest%2520recursively%252C%2520creating%2520processing%2520demands%2520from%2520simple%2520to%2520deeply%2520nested%2520structures.%2520Each%2520sentence%2520has%2520a%2520syntactically%2520identical%2520but%2520semantically%2520implausible%2520counterpart%2520%2528e.g.%252C%2520mailmen%2520prescribe%2520medicine%252C%2520doctors%2520deliver%2520mail%2529%2520and%2520six%2520comprehension%2520questions%2520testing%2520surface%2520understanding%252C%2520syntactic%2520dependencies%252C%2520and%2520causal%2520reasoning.%2520Testing%2520six%2520models%2520reveals%2520that%2520performance%2520gaps%2520between%2520plausible%2520and%2520implausible%2520sentences%2520widen%2520systematically%2520with%2520complexity%252C%2520with%2520models%2520showing%2520median%2520gaps%2520up%2520to%252026.8%2520percentage%2520points%252C%2520quantifying%2520when%2520they%2520abandon%2520structural%2520analysis%2520for%2520semantic%2520associations.%2520Notably%252C%2520semantic%2520plausibility%2520harms%2520performance%2520on%2520questions%2520about%2520resulting%2520actions%252C%2520where%2520following%2520causal%2520relationships%2520matters%2520more%2520than%2520semantic%2520coherence.%2520Reasoning%2520models%2520improve%2520accuracy%2520but%2520their%2520traces%2520show%2520semantic%2520shortcuts%252C%2520overthinking%252C%2520and%2520answer%2520refusal.%2520Unlike%2520models%2520whose%2520plausibility%2520advantage%2520systematically%2520widens%2520with%2520complexity%252C%2520humans%2520shows%2520variable%2520semantic%2520effects.%2520CenterBench%2520provides%2520the%2520first%2520framework%2520to%2520identify%2520when%2520models%2520shift%2520from%2520structural%2520analysis%2520to%2520pattern%2520matching.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.20543v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Dog%20the%20Cat%20Chased%20Stumped%20the%20Model%3A%20Measuring%20When%20Language%20Models%20Abandon%20Structure%20for%20Shortcuts&entry.906535625=Sangmitra%20Madhusudan%20and%20Kaige%20Chen%20and%20Ali%20Emami&entry.1292438233=When%20language%20models%20correctly%20parse%20%22The%20cat%20that%20the%20dog%20chased%20meowed%2C%22%20are%20they%20analyzing%20syntax%20or%20simply%20familiar%20with%20dogs%20chasing%20cats%3F%20Despite%20extensive%20benchmarking%2C%20we%20lack%20methods%20to%20distinguish%20structural%20understanding%20from%20semantic%20pattern%20matching.%20We%20introduce%20CenterBench%2C%20a%20dataset%20of%209%2C720%20comprehension%20questions%20on%20center-embedded%20sentences%20%28like%20%22The%20cat%20%5Bthat%20the%20dog%20chased%5D%20meowed%22%29%20where%20relative%20clauses%20nest%20recursively%2C%20creating%20processing%20demands%20from%20simple%20to%20deeply%20nested%20structures.%20Each%20sentence%20has%20a%20syntactically%20identical%20but%20semantically%20implausible%20counterpart%20%28e.g.%2C%20mailmen%20prescribe%20medicine%2C%20doctors%20deliver%20mail%29%20and%20six%20comprehension%20questions%20testing%20surface%20understanding%2C%20syntactic%20dependencies%2C%20and%20causal%20reasoning.%20Testing%20six%20models%20reveals%20that%20performance%20gaps%20between%20plausible%20and%20implausible%20sentences%20widen%20systematically%20with%20complexity%2C%20with%20models%20showing%20median%20gaps%20up%20to%2026.8%20percentage%20points%2C%20quantifying%20when%20they%20abandon%20structural%20analysis%20for%20semantic%20associations.%20Notably%2C%20semantic%20plausibility%20harms%20performance%20on%20questions%20about%20resulting%20actions%2C%20where%20following%20causal%20relationships%20matters%20more%20than%20semantic%20coherence.%20Reasoning%20models%20improve%20accuracy%20but%20their%20traces%20show%20semantic%20shortcuts%2C%20overthinking%2C%20and%20answer%20refusal.%20Unlike%20models%20whose%20plausibility%20advantage%20systematically%20widens%20with%20complexity%2C%20humans%20shows%20variable%20semantic%20effects.%20CenterBench%20provides%20the%20first%20framework%20to%20identify%20when%20models%20shift%20from%20structural%20analysis%20to%20pattern%20matching.&entry.1838667208=http%3A//arxiv.org/abs/2510.20543v2&entry.124074799=Read"},
{"title": "GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning", "author": "Shutong Ding and Ke Hu and Shan Zhong and Haoyang Luo and Weinan Zhang and Jingya Wang and Jun Wang and Ye Shi", "abstract": "Recent advances in reinforcement learning (RL) have demonstrated the powerful exploration capabilities and multimodality of generative diffusion-based policies. While substantial progress has been made in offline RL and off-policy RL settings, integrating diffusion policies into on-policy frameworks like PPO remains underexplored. This gap is particularly significant given the widespread use of large-scale parallel GPU-accelerated simulators, such as IsaacLab, which are optimized for on-policy RL algorithms and enable rapid training of complex robotic tasks. A key challenge lies in computing state-action log-likelihoods under diffusion policies, which is straightforward for Gaussian policies but intractable for flow-based models due to irreversible forward-reverse processes and discretization errors (e.g., Euler-Maruyama approximations). To bridge this gap, we propose GenPO, a generative policy optimization framework that leverages exact diffusion inversion to construct invertible action mappings. GenPO introduces a novel doubled dummy action mechanism that enables invertibility via alternating updates, resolving log-likelihood computation barriers. Furthermore, we also use the action log-likelihood for unbiased entropy and KL divergence estimation, enabling KL-adaptive learning rates and entropy regularization in on-policy updates. Extensive experiments on eight IsaacLab benchmarks, including legged locomotion (Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate GenPO's superiority over existing RL baselines. Notably, GenPO is the first method to successfully integrate diffusion policies into on-policy RL, unlocking their potential for large-scale parallelized training and real-world robotic deployment.", "link": "http://arxiv.org/abs/2505.18763v3", "date": "2026-01-20", "relevancy": 2.1914, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5531}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5476}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GenPO%3A%20Generative%20Diffusion%20Models%20Meet%20On-Policy%20Reinforcement%20Learning&body=Title%3A%20GenPO%3A%20Generative%20Diffusion%20Models%20Meet%20On-Policy%20Reinforcement%20Learning%0AAuthor%3A%20Shutong%20Ding%20and%20Ke%20Hu%20and%20Shan%20Zhong%20and%20Haoyang%20Luo%20and%20Weinan%20Zhang%20and%20Jingya%20Wang%20and%20Jun%20Wang%20and%20Ye%20Shi%0AAbstract%3A%20Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20have%20demonstrated%20the%20powerful%20exploration%20capabilities%20and%20multimodality%20of%20generative%20diffusion-based%20policies.%20While%20substantial%20progress%20has%20been%20made%20in%20offline%20RL%20and%20off-policy%20RL%20settings%2C%20integrating%20diffusion%20policies%20into%20on-policy%20frameworks%20like%20PPO%20remains%20underexplored.%20This%20gap%20is%20particularly%20significant%20given%20the%20widespread%20use%20of%20large-scale%20parallel%20GPU-accelerated%20simulators%2C%20such%20as%20IsaacLab%2C%20which%20are%20optimized%20for%20on-policy%20RL%20algorithms%20and%20enable%20rapid%20training%20of%20complex%20robotic%20tasks.%20A%20key%20challenge%20lies%20in%20computing%20state-action%20log-likelihoods%20under%20diffusion%20policies%2C%20which%20is%20straightforward%20for%20Gaussian%20policies%20but%20intractable%20for%20flow-based%20models%20due%20to%20irreversible%20forward-reverse%20processes%20and%20discretization%20errors%20%28e.g.%2C%20Euler-Maruyama%20approximations%29.%20To%20bridge%20this%20gap%2C%20we%20propose%20GenPO%2C%20a%20generative%20policy%20optimization%20framework%20that%20leverages%20exact%20diffusion%20inversion%20to%20construct%20invertible%20action%20mappings.%20GenPO%20introduces%20a%20novel%20doubled%20dummy%20action%20mechanism%20that%20enables%20invertibility%20via%20alternating%20updates%2C%20resolving%20log-likelihood%20computation%20barriers.%20Furthermore%2C%20we%20also%20use%20the%20action%20log-likelihood%20for%20unbiased%20entropy%20and%20KL%20divergence%20estimation%2C%20enabling%20KL-adaptive%20learning%20rates%20and%20entropy%20regularization%20in%20on-policy%20updates.%20Extensive%20experiments%20on%20eight%20IsaacLab%20benchmarks%2C%20including%20legged%20locomotion%20%28Ant%2C%20Humanoid%2C%20Anymal-D%2C%20Unitree%20H1%2C%20Go2%29%2C%20dexterous%20manipulation%20%28Shadow%20Hand%29%2C%20aerial%20control%20%28Quadcopter%29%2C%20and%20robotic%20arm%20tasks%20%28Franka%29%2C%20demonstrate%20GenPO%27s%20superiority%20over%20existing%20RL%20baselines.%20Notably%2C%20GenPO%20is%20the%20first%20method%20to%20successfully%20integrate%20diffusion%20policies%20into%20on-policy%20RL%2C%20unlocking%20their%20potential%20for%20large-scale%20parallelized%20training%20and%20real-world%20robotic%20deployment.%0ALink%3A%20http%3A//arxiv.org/abs/2505.18763v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGenPO%253A%2520Generative%2520Diffusion%2520Models%2520Meet%2520On-Policy%2520Reinforcement%2520Learning%26entry.906535625%3DShutong%2520Ding%2520and%2520Ke%2520Hu%2520and%2520Shan%2520Zhong%2520and%2520Haoyang%2520Luo%2520and%2520Weinan%2520Zhang%2520and%2520Jingya%2520Wang%2520and%2520Jun%2520Wang%2520and%2520Ye%2520Shi%26entry.1292438233%3DRecent%2520advances%2520in%2520reinforcement%2520learning%2520%2528RL%2529%2520have%2520demonstrated%2520the%2520powerful%2520exploration%2520capabilities%2520and%2520multimodality%2520of%2520generative%2520diffusion-based%2520policies.%2520While%2520substantial%2520progress%2520has%2520been%2520made%2520in%2520offline%2520RL%2520and%2520off-policy%2520RL%2520settings%252C%2520integrating%2520diffusion%2520policies%2520into%2520on-policy%2520frameworks%2520like%2520PPO%2520remains%2520underexplored.%2520This%2520gap%2520is%2520particularly%2520significant%2520given%2520the%2520widespread%2520use%2520of%2520large-scale%2520parallel%2520GPU-accelerated%2520simulators%252C%2520such%2520as%2520IsaacLab%252C%2520which%2520are%2520optimized%2520for%2520on-policy%2520RL%2520algorithms%2520and%2520enable%2520rapid%2520training%2520of%2520complex%2520robotic%2520tasks.%2520A%2520key%2520challenge%2520lies%2520in%2520computing%2520state-action%2520log-likelihoods%2520under%2520diffusion%2520policies%252C%2520which%2520is%2520straightforward%2520for%2520Gaussian%2520policies%2520but%2520intractable%2520for%2520flow-based%2520models%2520due%2520to%2520irreversible%2520forward-reverse%2520processes%2520and%2520discretization%2520errors%2520%2528e.g.%252C%2520Euler-Maruyama%2520approximations%2529.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520propose%2520GenPO%252C%2520a%2520generative%2520policy%2520optimization%2520framework%2520that%2520leverages%2520exact%2520diffusion%2520inversion%2520to%2520construct%2520invertible%2520action%2520mappings.%2520GenPO%2520introduces%2520a%2520novel%2520doubled%2520dummy%2520action%2520mechanism%2520that%2520enables%2520invertibility%2520via%2520alternating%2520updates%252C%2520resolving%2520log-likelihood%2520computation%2520barriers.%2520Furthermore%252C%2520we%2520also%2520use%2520the%2520action%2520log-likelihood%2520for%2520unbiased%2520entropy%2520and%2520KL%2520divergence%2520estimation%252C%2520enabling%2520KL-adaptive%2520learning%2520rates%2520and%2520entropy%2520regularization%2520in%2520on-policy%2520updates.%2520Extensive%2520experiments%2520on%2520eight%2520IsaacLab%2520benchmarks%252C%2520including%2520legged%2520locomotion%2520%2528Ant%252C%2520Humanoid%252C%2520Anymal-D%252C%2520Unitree%2520H1%252C%2520Go2%2529%252C%2520dexterous%2520manipulation%2520%2528Shadow%2520Hand%2529%252C%2520aerial%2520control%2520%2528Quadcopter%2529%252C%2520and%2520robotic%2520arm%2520tasks%2520%2528Franka%2529%252C%2520demonstrate%2520GenPO%2527s%2520superiority%2520over%2520existing%2520RL%2520baselines.%2520Notably%252C%2520GenPO%2520is%2520the%2520first%2520method%2520to%2520successfully%2520integrate%2520diffusion%2520policies%2520into%2520on-policy%2520RL%252C%2520unlocking%2520their%2520potential%2520for%2520large-scale%2520parallelized%2520training%2520and%2520real-world%2520robotic%2520deployment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.18763v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GenPO%3A%20Generative%20Diffusion%20Models%20Meet%20On-Policy%20Reinforcement%20Learning&entry.906535625=Shutong%20Ding%20and%20Ke%20Hu%20and%20Shan%20Zhong%20and%20Haoyang%20Luo%20and%20Weinan%20Zhang%20and%20Jingya%20Wang%20and%20Jun%20Wang%20and%20Ye%20Shi&entry.1292438233=Recent%20advances%20in%20reinforcement%20learning%20%28RL%29%20have%20demonstrated%20the%20powerful%20exploration%20capabilities%20and%20multimodality%20of%20generative%20diffusion-based%20policies.%20While%20substantial%20progress%20has%20been%20made%20in%20offline%20RL%20and%20off-policy%20RL%20settings%2C%20integrating%20diffusion%20policies%20into%20on-policy%20frameworks%20like%20PPO%20remains%20underexplored.%20This%20gap%20is%20particularly%20significant%20given%20the%20widespread%20use%20of%20large-scale%20parallel%20GPU-accelerated%20simulators%2C%20such%20as%20IsaacLab%2C%20which%20are%20optimized%20for%20on-policy%20RL%20algorithms%20and%20enable%20rapid%20training%20of%20complex%20robotic%20tasks.%20A%20key%20challenge%20lies%20in%20computing%20state-action%20log-likelihoods%20under%20diffusion%20policies%2C%20which%20is%20straightforward%20for%20Gaussian%20policies%20but%20intractable%20for%20flow-based%20models%20due%20to%20irreversible%20forward-reverse%20processes%20and%20discretization%20errors%20%28e.g.%2C%20Euler-Maruyama%20approximations%29.%20To%20bridge%20this%20gap%2C%20we%20propose%20GenPO%2C%20a%20generative%20policy%20optimization%20framework%20that%20leverages%20exact%20diffusion%20inversion%20to%20construct%20invertible%20action%20mappings.%20GenPO%20introduces%20a%20novel%20doubled%20dummy%20action%20mechanism%20that%20enables%20invertibility%20via%20alternating%20updates%2C%20resolving%20log-likelihood%20computation%20barriers.%20Furthermore%2C%20we%20also%20use%20the%20action%20log-likelihood%20for%20unbiased%20entropy%20and%20KL%20divergence%20estimation%2C%20enabling%20KL-adaptive%20learning%20rates%20and%20entropy%20regularization%20in%20on-policy%20updates.%20Extensive%20experiments%20on%20eight%20IsaacLab%20benchmarks%2C%20including%20legged%20locomotion%20%28Ant%2C%20Humanoid%2C%20Anymal-D%2C%20Unitree%20H1%2C%20Go2%29%2C%20dexterous%20manipulation%20%28Shadow%20Hand%29%2C%20aerial%20control%20%28Quadcopter%29%2C%20and%20robotic%20arm%20tasks%20%28Franka%29%2C%20demonstrate%20GenPO%27s%20superiority%20over%20existing%20RL%20baselines.%20Notably%2C%20GenPO%20is%20the%20first%20method%20to%20successfully%20integrate%20diffusion%20policies%20into%20on-policy%20RL%2C%20unlocking%20their%20potential%20for%20large-scale%20parallelized%20training%20and%20real-world%20robotic%20deployment.&entry.1838667208=http%3A//arxiv.org/abs/2505.18763v3&entry.124074799=Read"},
{"title": "Curriculum-Based Strategies for Efficient Cross-Domain Action Recognition", "author": "Emily Kim and Allen Wu and Jessica Hodgins", "abstract": "Despite significant progress in human action recognition, generalizing to diverse viewpoints remains a challenge. Most existing datasets are captured from ground-level perspectives, and models trained on them often struggle to transfer to drastically different domains such as aerial views. This paper examines how curriculum-based training strategies can improve generalization to unseen real aerial-view data without using any real aerial data during training.\n  We explore curriculum learning for cross-view action recognition using two out-of-domain sources: synthetic aerial-view data and real ground-view data. Our results on the evaluation on order of training (fine-tuning on synthetic aerial data vs. real ground data) shows that fine-tuning on real ground data but differ in how they transition from synthetic to real. The first uses a two-stage curriculum with direct fine-tuning, while the second applies a progressive curriculum that expands the dataset in multiple stages before fine-tuning. We evaluate both methods on the REMAG dataset using SlowFast (CNN-based) and MViTv2 (Transformer-based) architectures.\n  Results show that combining the two out-of-domain datasets clearly outperforms training on a single domain, whether real ground-view or synthetic aerial-view. Both curriculum strategies match the top-1 accuracy of simple dataset combination while offering efficiency gains. With the two-step fine-tuning method, SlowFast achieves up to a 37% reduction in iterations and MViTv2 up to a 30% reduction compared to simple combination. The multi-step progressive approach further reduces iterations, by up to 9% for SlowFast and 30% for MViTv2, relative to the two-step method. These findings demonstrate that curriculum-based training can maintain comparable performance (top-1 accuracy within 3% range) while improving training efficiency in cross-view action recognition.", "link": "http://arxiv.org/abs/2601.14101v1", "date": "2026-01-20", "relevancy": 2.1818, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.5549}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5539}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curriculum-Based%20Strategies%20for%20Efficient%20Cross-Domain%20Action%20Recognition&body=Title%3A%20Curriculum-Based%20Strategies%20for%20Efficient%20Cross-Domain%20Action%20Recognition%0AAuthor%3A%20Emily%20Kim%20and%20Allen%20Wu%20and%20Jessica%20Hodgins%0AAbstract%3A%20Despite%20significant%20progress%20in%20human%20action%20recognition%2C%20generalizing%20to%20diverse%20viewpoints%20remains%20a%20challenge.%20Most%20existing%20datasets%20are%20captured%20from%20ground-level%20perspectives%2C%20and%20models%20trained%20on%20them%20often%20struggle%20to%20transfer%20to%20drastically%20different%20domains%20such%20as%20aerial%20views.%20This%20paper%20examines%20how%20curriculum-based%20training%20strategies%20can%20improve%20generalization%20to%20unseen%20real%20aerial-view%20data%20without%20using%20any%20real%20aerial%20data%20during%20training.%0A%20%20We%20explore%20curriculum%20learning%20for%20cross-view%20action%20recognition%20using%20two%20out-of-domain%20sources%3A%20synthetic%20aerial-view%20data%20and%20real%20ground-view%20data.%20Our%20results%20on%20the%20evaluation%20on%20order%20of%20training%20%28fine-tuning%20on%20synthetic%20aerial%20data%20vs.%20real%20ground%20data%29%20shows%20that%20fine-tuning%20on%20real%20ground%20data%20but%20differ%20in%20how%20they%20transition%20from%20synthetic%20to%20real.%20The%20first%20uses%20a%20two-stage%20curriculum%20with%20direct%20fine-tuning%2C%20while%20the%20second%20applies%20a%20progressive%20curriculum%20that%20expands%20the%20dataset%20in%20multiple%20stages%20before%20fine-tuning.%20We%20evaluate%20both%20methods%20on%20the%20REMAG%20dataset%20using%20SlowFast%20%28CNN-based%29%20and%20MViTv2%20%28Transformer-based%29%20architectures.%0A%20%20Results%20show%20that%20combining%20the%20two%20out-of-domain%20datasets%20clearly%20outperforms%20training%20on%20a%20single%20domain%2C%20whether%20real%20ground-view%20or%20synthetic%20aerial-view.%20Both%20curriculum%20strategies%20match%20the%20top-1%20accuracy%20of%20simple%20dataset%20combination%20while%20offering%20efficiency%20gains.%20With%20the%20two-step%20fine-tuning%20method%2C%20SlowFast%20achieves%20up%20to%20a%2037%25%20reduction%20in%20iterations%20and%20MViTv2%20up%20to%20a%2030%25%20reduction%20compared%20to%20simple%20combination.%20The%20multi-step%20progressive%20approach%20further%20reduces%20iterations%2C%20by%20up%20to%209%25%20for%20SlowFast%20and%2030%25%20for%20MViTv2%2C%20relative%20to%20the%20two-step%20method.%20These%20findings%20demonstrate%20that%20curriculum-based%20training%20can%20maintain%20comparable%20performance%20%28top-1%20accuracy%20within%203%25%20range%29%20while%20improving%20training%20efficiency%20in%20cross-view%20action%20recognition.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14101v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurriculum-Based%2520Strategies%2520for%2520Efficient%2520Cross-Domain%2520Action%2520Recognition%26entry.906535625%3DEmily%2520Kim%2520and%2520Allen%2520Wu%2520and%2520Jessica%2520Hodgins%26entry.1292438233%3DDespite%2520significant%2520progress%2520in%2520human%2520action%2520recognition%252C%2520generalizing%2520to%2520diverse%2520viewpoints%2520remains%2520a%2520challenge.%2520Most%2520existing%2520datasets%2520are%2520captured%2520from%2520ground-level%2520perspectives%252C%2520and%2520models%2520trained%2520on%2520them%2520often%2520struggle%2520to%2520transfer%2520to%2520drastically%2520different%2520domains%2520such%2520as%2520aerial%2520views.%2520This%2520paper%2520examines%2520how%2520curriculum-based%2520training%2520strategies%2520can%2520improve%2520generalization%2520to%2520unseen%2520real%2520aerial-view%2520data%2520without%2520using%2520any%2520real%2520aerial%2520data%2520during%2520training.%250A%2520%2520We%2520explore%2520curriculum%2520learning%2520for%2520cross-view%2520action%2520recognition%2520using%2520two%2520out-of-domain%2520sources%253A%2520synthetic%2520aerial-view%2520data%2520and%2520real%2520ground-view%2520data.%2520Our%2520results%2520on%2520the%2520evaluation%2520on%2520order%2520of%2520training%2520%2528fine-tuning%2520on%2520synthetic%2520aerial%2520data%2520vs.%2520real%2520ground%2520data%2529%2520shows%2520that%2520fine-tuning%2520on%2520real%2520ground%2520data%2520but%2520differ%2520in%2520how%2520they%2520transition%2520from%2520synthetic%2520to%2520real.%2520The%2520first%2520uses%2520a%2520two-stage%2520curriculum%2520with%2520direct%2520fine-tuning%252C%2520while%2520the%2520second%2520applies%2520a%2520progressive%2520curriculum%2520that%2520expands%2520the%2520dataset%2520in%2520multiple%2520stages%2520before%2520fine-tuning.%2520We%2520evaluate%2520both%2520methods%2520on%2520the%2520REMAG%2520dataset%2520using%2520SlowFast%2520%2528CNN-based%2529%2520and%2520MViTv2%2520%2528Transformer-based%2529%2520architectures.%250A%2520%2520Results%2520show%2520that%2520combining%2520the%2520two%2520out-of-domain%2520datasets%2520clearly%2520outperforms%2520training%2520on%2520a%2520single%2520domain%252C%2520whether%2520real%2520ground-view%2520or%2520synthetic%2520aerial-view.%2520Both%2520curriculum%2520strategies%2520match%2520the%2520top-1%2520accuracy%2520of%2520simple%2520dataset%2520combination%2520while%2520offering%2520efficiency%2520gains.%2520With%2520the%2520two-step%2520fine-tuning%2520method%252C%2520SlowFast%2520achieves%2520up%2520to%2520a%252037%2525%2520reduction%2520in%2520iterations%2520and%2520MViTv2%2520up%2520to%2520a%252030%2525%2520reduction%2520compared%2520to%2520simple%2520combination.%2520The%2520multi-step%2520progressive%2520approach%2520further%2520reduces%2520iterations%252C%2520by%2520up%2520to%25209%2525%2520for%2520SlowFast%2520and%252030%2525%2520for%2520MViTv2%252C%2520relative%2520to%2520the%2520two-step%2520method.%2520These%2520findings%2520demonstrate%2520that%2520curriculum-based%2520training%2520can%2520maintain%2520comparable%2520performance%2520%2528top-1%2520accuracy%2520within%25203%2525%2520range%2529%2520while%2520improving%2520training%2520efficiency%2520in%2520cross-view%2520action%2520recognition.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14101v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curriculum-Based%20Strategies%20for%20Efficient%20Cross-Domain%20Action%20Recognition&entry.906535625=Emily%20Kim%20and%20Allen%20Wu%20and%20Jessica%20Hodgins&entry.1292438233=Despite%20significant%20progress%20in%20human%20action%20recognition%2C%20generalizing%20to%20diverse%20viewpoints%20remains%20a%20challenge.%20Most%20existing%20datasets%20are%20captured%20from%20ground-level%20perspectives%2C%20and%20models%20trained%20on%20them%20often%20struggle%20to%20transfer%20to%20drastically%20different%20domains%20such%20as%20aerial%20views.%20This%20paper%20examines%20how%20curriculum-based%20training%20strategies%20can%20improve%20generalization%20to%20unseen%20real%20aerial-view%20data%20without%20using%20any%20real%20aerial%20data%20during%20training.%0A%20%20We%20explore%20curriculum%20learning%20for%20cross-view%20action%20recognition%20using%20two%20out-of-domain%20sources%3A%20synthetic%20aerial-view%20data%20and%20real%20ground-view%20data.%20Our%20results%20on%20the%20evaluation%20on%20order%20of%20training%20%28fine-tuning%20on%20synthetic%20aerial%20data%20vs.%20real%20ground%20data%29%20shows%20that%20fine-tuning%20on%20real%20ground%20data%20but%20differ%20in%20how%20they%20transition%20from%20synthetic%20to%20real.%20The%20first%20uses%20a%20two-stage%20curriculum%20with%20direct%20fine-tuning%2C%20while%20the%20second%20applies%20a%20progressive%20curriculum%20that%20expands%20the%20dataset%20in%20multiple%20stages%20before%20fine-tuning.%20We%20evaluate%20both%20methods%20on%20the%20REMAG%20dataset%20using%20SlowFast%20%28CNN-based%29%20and%20MViTv2%20%28Transformer-based%29%20architectures.%0A%20%20Results%20show%20that%20combining%20the%20two%20out-of-domain%20datasets%20clearly%20outperforms%20training%20on%20a%20single%20domain%2C%20whether%20real%20ground-view%20or%20synthetic%20aerial-view.%20Both%20curriculum%20strategies%20match%20the%20top-1%20accuracy%20of%20simple%20dataset%20combination%20while%20offering%20efficiency%20gains.%20With%20the%20two-step%20fine-tuning%20method%2C%20SlowFast%20achieves%20up%20to%20a%2037%25%20reduction%20in%20iterations%20and%20MViTv2%20up%20to%20a%2030%25%20reduction%20compared%20to%20simple%20combination.%20The%20multi-step%20progressive%20approach%20further%20reduces%20iterations%2C%20by%20up%20to%209%25%20for%20SlowFast%20and%2030%25%20for%20MViTv2%2C%20relative%20to%20the%20two-step%20method.%20These%20findings%20demonstrate%20that%20curriculum-based%20training%20can%20maintain%20comparable%20performance%20%28top-1%20accuracy%20within%203%25%20range%29%20while%20improving%20training%20efficiency%20in%20cross-view%20action%20recognition.&entry.1838667208=http%3A//arxiv.org/abs/2601.14101v1&entry.124074799=Read"},
{"title": "FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation", "author": "Jing Zuo and Lingzhou Mu and Fan Jiang and Chengcheng Ma and Mu Xu and Yonggang Qi", "abstract": "Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.", "link": "http://arxiv.org/abs/2601.13976v1", "date": "2026-01-20", "relevancy": 2.1817, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5453}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FantasyVLN%3A%20Unified%20Multimodal%20Chain-of-Thought%20Reasoning%20for%20Vision-Language%20Navigation&body=Title%3A%20FantasyVLN%3A%20Unified%20Multimodal%20Chain-of-Thought%20Reasoning%20for%20Vision-Language%20Navigation%0AAuthor%3A%20Jing%20Zuo%20and%20Lingzhou%20Mu%20and%20Fan%20Jiang%20and%20Chengcheng%20Ma%20and%20Mu%20Xu%20and%20Yonggang%20Qi%0AAbstract%3A%20Achieving%20human-level%20performance%20in%20Vision-and-Language%20Navigation%20%28VLN%29%20requires%20an%20embodied%20agent%20to%20jointly%20understand%20multimodal%20instructions%20and%20visual-spatial%20context%20while%20reasoning%20over%20long%20action%20sequences.%20Recent%20works%2C%20such%20as%20NavCoT%20and%20NavGPT-2%2C%20demonstrate%20the%20potential%20of%20Chain-of-Thought%20%28CoT%29%20reasoning%20for%20improving%20interpretability%20and%20long-horizon%20planning.%20Moreover%2C%20multimodal%20extensions%20like%20OctoNav-R1%20and%20CoT-VLA%20further%20validate%20CoT%20as%20a%20promising%20pathway%20toward%20human-like%20navigation%20reasoning.%20However%2C%20existing%20approaches%20face%20critical%20drawbacks%3A%20purely%20textual%20CoTs%20lack%20spatial%20grounding%20and%20easily%20overfit%20to%20sparse%20annotated%20reasoning%20steps%2C%20while%20multimodal%20CoTs%20incur%20severe%20token%20inflation%20by%20generating%20imagined%20visual%20observations%2C%20making%20real-time%20navigation%20impractical.%20In%20this%20work%2C%20we%20propose%20FantasyVLN%2C%20a%20unified%20implicit%20reasoning%20framework%20that%20preserves%20the%20benefits%20of%20CoT%20reasoning%20without%20explicit%20token%20overhead.%20Specifically%2C%20imagined%20visual%20tokens%20are%20encoded%20into%20a%20compact%20latent%20space%20using%20a%20pretrained%20Visual%20AutoRegressor%20%28VAR%29%20during%20CoT%20reasoning%20training%2C%20and%20the%20model%20jointly%20learns%20from%20textual%2C%20visual%2C%20and%20multimodal%20CoT%20modes%20under%20a%20unified%20multi-CoT%20strategy.%20At%20inference%2C%20our%20model%20performs%20direct%20instruction-to-action%20mapping%20while%20still%20enjoying%20reasoning-aware%20representations.%20Extensive%20experiments%20on%20LH-VLN%20show%20that%20our%20approach%20achieves%20reasoning-aware%20yet%20real-time%20navigation%2C%20improving%20success%20rates%20and%20efficiency%20while%20reducing%20inference%20latency%20by%20an%20order%20of%20magnitude%20compared%20to%20explicit%20CoT%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13976v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFantasyVLN%253A%2520Unified%2520Multimodal%2520Chain-of-Thought%2520Reasoning%2520for%2520Vision-Language%2520Navigation%26entry.906535625%3DJing%2520Zuo%2520and%2520Lingzhou%2520Mu%2520and%2520Fan%2520Jiang%2520and%2520Chengcheng%2520Ma%2520and%2520Mu%2520Xu%2520and%2520Yonggang%2520Qi%26entry.1292438233%3DAchieving%2520human-level%2520performance%2520in%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%2520requires%2520an%2520embodied%2520agent%2520to%2520jointly%2520understand%2520multimodal%2520instructions%2520and%2520visual-spatial%2520context%2520while%2520reasoning%2520over%2520long%2520action%2520sequences.%2520Recent%2520works%252C%2520such%2520as%2520NavCoT%2520and%2520NavGPT-2%252C%2520demonstrate%2520the%2520potential%2520of%2520Chain-of-Thought%2520%2528CoT%2529%2520reasoning%2520for%2520improving%2520interpretability%2520and%2520long-horizon%2520planning.%2520Moreover%252C%2520multimodal%2520extensions%2520like%2520OctoNav-R1%2520and%2520CoT-VLA%2520further%2520validate%2520CoT%2520as%2520a%2520promising%2520pathway%2520toward%2520human-like%2520navigation%2520reasoning.%2520However%252C%2520existing%2520approaches%2520face%2520critical%2520drawbacks%253A%2520purely%2520textual%2520CoTs%2520lack%2520spatial%2520grounding%2520and%2520easily%2520overfit%2520to%2520sparse%2520annotated%2520reasoning%2520steps%252C%2520while%2520multimodal%2520CoTs%2520incur%2520severe%2520token%2520inflation%2520by%2520generating%2520imagined%2520visual%2520observations%252C%2520making%2520real-time%2520navigation%2520impractical.%2520In%2520this%2520work%252C%2520we%2520propose%2520FantasyVLN%252C%2520a%2520unified%2520implicit%2520reasoning%2520framework%2520that%2520preserves%2520the%2520benefits%2520of%2520CoT%2520reasoning%2520without%2520explicit%2520token%2520overhead.%2520Specifically%252C%2520imagined%2520visual%2520tokens%2520are%2520encoded%2520into%2520a%2520compact%2520latent%2520space%2520using%2520a%2520pretrained%2520Visual%2520AutoRegressor%2520%2528VAR%2529%2520during%2520CoT%2520reasoning%2520training%252C%2520and%2520the%2520model%2520jointly%2520learns%2520from%2520textual%252C%2520visual%252C%2520and%2520multimodal%2520CoT%2520modes%2520under%2520a%2520unified%2520multi-CoT%2520strategy.%2520At%2520inference%252C%2520our%2520model%2520performs%2520direct%2520instruction-to-action%2520mapping%2520while%2520still%2520enjoying%2520reasoning-aware%2520representations.%2520Extensive%2520experiments%2520on%2520LH-VLN%2520show%2520that%2520our%2520approach%2520achieves%2520reasoning-aware%2520yet%2520real-time%2520navigation%252C%2520improving%2520success%2520rates%2520and%2520efficiency%2520while%2520reducing%2520inference%2520latency%2520by%2520an%2520order%2520of%2520magnitude%2520compared%2520to%2520explicit%2520CoT%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13976v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FantasyVLN%3A%20Unified%20Multimodal%20Chain-of-Thought%20Reasoning%20for%20Vision-Language%20Navigation&entry.906535625=Jing%20Zuo%20and%20Lingzhou%20Mu%20and%20Fan%20Jiang%20and%20Chengcheng%20Ma%20and%20Mu%20Xu%20and%20Yonggang%20Qi&entry.1292438233=Achieving%20human-level%20performance%20in%20Vision-and-Language%20Navigation%20%28VLN%29%20requires%20an%20embodied%20agent%20to%20jointly%20understand%20multimodal%20instructions%20and%20visual-spatial%20context%20while%20reasoning%20over%20long%20action%20sequences.%20Recent%20works%2C%20such%20as%20NavCoT%20and%20NavGPT-2%2C%20demonstrate%20the%20potential%20of%20Chain-of-Thought%20%28CoT%29%20reasoning%20for%20improving%20interpretability%20and%20long-horizon%20planning.%20Moreover%2C%20multimodal%20extensions%20like%20OctoNav-R1%20and%20CoT-VLA%20further%20validate%20CoT%20as%20a%20promising%20pathway%20toward%20human-like%20navigation%20reasoning.%20However%2C%20existing%20approaches%20face%20critical%20drawbacks%3A%20purely%20textual%20CoTs%20lack%20spatial%20grounding%20and%20easily%20overfit%20to%20sparse%20annotated%20reasoning%20steps%2C%20while%20multimodal%20CoTs%20incur%20severe%20token%20inflation%20by%20generating%20imagined%20visual%20observations%2C%20making%20real-time%20navigation%20impractical.%20In%20this%20work%2C%20we%20propose%20FantasyVLN%2C%20a%20unified%20implicit%20reasoning%20framework%20that%20preserves%20the%20benefits%20of%20CoT%20reasoning%20without%20explicit%20token%20overhead.%20Specifically%2C%20imagined%20visual%20tokens%20are%20encoded%20into%20a%20compact%20latent%20space%20using%20a%20pretrained%20Visual%20AutoRegressor%20%28VAR%29%20during%20CoT%20reasoning%20training%2C%20and%20the%20model%20jointly%20learns%20from%20textual%2C%20visual%2C%20and%20multimodal%20CoT%20modes%20under%20a%20unified%20multi-CoT%20strategy.%20At%20inference%2C%20our%20model%20performs%20direct%20instruction-to-action%20mapping%20while%20still%20enjoying%20reasoning-aware%20representations.%20Extensive%20experiments%20on%20LH-VLN%20show%20that%20our%20approach%20achieves%20reasoning-aware%20yet%20real-time%20navigation%2C%20improving%20success%20rates%20and%20efficiency%20while%20reducing%20inference%20latency%20by%20an%20order%20of%20magnitude%20compared%20to%20explicit%20CoT%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.13976v1&entry.124074799=Read"},
{"title": "Deferred Commitment Decoding for Diffusion Language Models", "author": "Yingte Shu and Yuchuan Tian and Chao Xu and Yunhe Wang and Hanting Chen", "abstract": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding certainty and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a certainty-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.73% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 16.5%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.", "link": "http://arxiv.org/abs/2601.02076v2", "date": "2026-01-20", "relevancy": 2.1802, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6284}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5307}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5261}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deferred%20Commitment%20Decoding%20for%20Diffusion%20Language%20Models&body=Title%3A%20Deferred%20Commitment%20Decoding%20for%20Diffusion%20Language%20Models%0AAuthor%3A%20Yingte%20Shu%20and%20Yuchuan%20Tian%20and%20Chao%20Xu%20and%20Yunhe%20Wang%20and%20Hanting%20Chen%0AAbstract%3A%20Diffusion%20language%20models%20%28DLMs%29%20have%20recently%20emerged%20as%20a%20strong%20alternative%20to%20autoregressive%20models%20by%20enabling%20parallel%20text%20generation.%20To%20improve%20inference%20efficiency%20and%20KV-cache%20compatibility%2C%20prior%20work%20commonly%20adopts%20block-based%20diffusion%2C%20decoding%20tokens%20block%20by%20block.%20However%2C%20this%20paradigm%20suffers%20from%20a%20structural%20limitation%20that%20we%20term%20Boundary-Induced%20Context%20Truncation%20%28BICT%29%3A%20undecoded%20tokens%20near%20block%20boundaries%20are%20forced%20to%20commit%20without%20access%20to%20nearby%20future%20context%2C%20even%20when%20such%20context%20could%20substantially%20reduce%20uncertainty.%20This%20limitation%20degrades%20decoding%20certainty%20and%20generation%20quality%2C%20especially%20for%20tasks%20requiring%20precise%20reasoning%2C%20such%20as%20mathematical%20problem%20solving%20and%20code%20generation.%20We%20propose%20Deferred%20Commitment%20Decoding%20%28DCD%29%2C%20a%20novel%2C%20training-free%20decoding%20strategy%20that%20mitigates%20this%20issue.%20DCD%20maintains%20a%20certainty-aware%20sliding%20window%20over%20masked%20tokens%2C%20resolving%20low-uncertainty%20tokens%20early%20while%20deferring%20high-uncertainty%20tokens%20until%20sufficient%20contextual%20evidence%20becomes%20available.%20Extensive%20experiments%20across%20multiple%20diffusion%20language%20models%2C%20benchmarks%2C%20and%20caching%20configurations%20show%20that%20DCD%20improves%20generation%20accuracy%20by%201.73%25%20with%20comparable%20time%20on%20average%20compared%20to%20fixed%20block-based%20diffusion%20methods%2C%20with%20the%20most%20significant%20improvement%20reaching%2016.5%25.%20These%20results%20demonstrate%20that%20deferring%20token%20commitment%20based%20on%20uncertainty%20is%20a%20simple%20yet%20effective%20principle%20for%20improving%20both%20the%20quality%20and%20efficiency%20of%20diffusion%20language%20model%20decoding.%0ALink%3A%20http%3A//arxiv.org/abs/2601.02076v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeferred%2520Commitment%2520Decoding%2520for%2520Diffusion%2520Language%2520Models%26entry.906535625%3DYingte%2520Shu%2520and%2520Yuchuan%2520Tian%2520and%2520Chao%2520Xu%2520and%2520Yunhe%2520Wang%2520and%2520Hanting%2520Chen%26entry.1292438233%3DDiffusion%2520language%2520models%2520%2528DLMs%2529%2520have%2520recently%2520emerged%2520as%2520a%2520strong%2520alternative%2520to%2520autoregressive%2520models%2520by%2520enabling%2520parallel%2520text%2520generation.%2520To%2520improve%2520inference%2520efficiency%2520and%2520KV-cache%2520compatibility%252C%2520prior%2520work%2520commonly%2520adopts%2520block-based%2520diffusion%252C%2520decoding%2520tokens%2520block%2520by%2520block.%2520However%252C%2520this%2520paradigm%2520suffers%2520from%2520a%2520structural%2520limitation%2520that%2520we%2520term%2520Boundary-Induced%2520Context%2520Truncation%2520%2528BICT%2529%253A%2520undecoded%2520tokens%2520near%2520block%2520boundaries%2520are%2520forced%2520to%2520commit%2520without%2520access%2520to%2520nearby%2520future%2520context%252C%2520even%2520when%2520such%2520context%2520could%2520substantially%2520reduce%2520uncertainty.%2520This%2520limitation%2520degrades%2520decoding%2520certainty%2520and%2520generation%2520quality%252C%2520especially%2520for%2520tasks%2520requiring%2520precise%2520reasoning%252C%2520such%2520as%2520mathematical%2520problem%2520solving%2520and%2520code%2520generation.%2520We%2520propose%2520Deferred%2520Commitment%2520Decoding%2520%2528DCD%2529%252C%2520a%2520novel%252C%2520training-free%2520decoding%2520strategy%2520that%2520mitigates%2520this%2520issue.%2520DCD%2520maintains%2520a%2520certainty-aware%2520sliding%2520window%2520over%2520masked%2520tokens%252C%2520resolving%2520low-uncertainty%2520tokens%2520early%2520while%2520deferring%2520high-uncertainty%2520tokens%2520until%2520sufficient%2520contextual%2520evidence%2520becomes%2520available.%2520Extensive%2520experiments%2520across%2520multiple%2520diffusion%2520language%2520models%252C%2520benchmarks%252C%2520and%2520caching%2520configurations%2520show%2520that%2520DCD%2520improves%2520generation%2520accuracy%2520by%25201.73%2525%2520with%2520comparable%2520time%2520on%2520average%2520compared%2520to%2520fixed%2520block-based%2520diffusion%2520methods%252C%2520with%2520the%2520most%2520significant%2520improvement%2520reaching%252016.5%2525.%2520These%2520results%2520demonstrate%2520that%2520deferring%2520token%2520commitment%2520based%2520on%2520uncertainty%2520is%2520a%2520simple%2520yet%2520effective%2520principle%2520for%2520improving%2520both%2520the%2520quality%2520and%2520efficiency%2520of%2520diffusion%2520language%2520model%2520decoding.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.02076v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deferred%20Commitment%20Decoding%20for%20Diffusion%20Language%20Models&entry.906535625=Yingte%20Shu%20and%20Yuchuan%20Tian%20and%20Chao%20Xu%20and%20Yunhe%20Wang%20and%20Hanting%20Chen&entry.1292438233=Diffusion%20language%20models%20%28DLMs%29%20have%20recently%20emerged%20as%20a%20strong%20alternative%20to%20autoregressive%20models%20by%20enabling%20parallel%20text%20generation.%20To%20improve%20inference%20efficiency%20and%20KV-cache%20compatibility%2C%20prior%20work%20commonly%20adopts%20block-based%20diffusion%2C%20decoding%20tokens%20block%20by%20block.%20However%2C%20this%20paradigm%20suffers%20from%20a%20structural%20limitation%20that%20we%20term%20Boundary-Induced%20Context%20Truncation%20%28BICT%29%3A%20undecoded%20tokens%20near%20block%20boundaries%20are%20forced%20to%20commit%20without%20access%20to%20nearby%20future%20context%2C%20even%20when%20such%20context%20could%20substantially%20reduce%20uncertainty.%20This%20limitation%20degrades%20decoding%20certainty%20and%20generation%20quality%2C%20especially%20for%20tasks%20requiring%20precise%20reasoning%2C%20such%20as%20mathematical%20problem%20solving%20and%20code%20generation.%20We%20propose%20Deferred%20Commitment%20Decoding%20%28DCD%29%2C%20a%20novel%2C%20training-free%20decoding%20strategy%20that%20mitigates%20this%20issue.%20DCD%20maintains%20a%20certainty-aware%20sliding%20window%20over%20masked%20tokens%2C%20resolving%20low-uncertainty%20tokens%20early%20while%20deferring%20high-uncertainty%20tokens%20until%20sufficient%20contextual%20evidence%20becomes%20available.%20Extensive%20experiments%20across%20multiple%20diffusion%20language%20models%2C%20benchmarks%2C%20and%20caching%20configurations%20show%20that%20DCD%20improves%20generation%20accuracy%20by%201.73%25%20with%20comparable%20time%20on%20average%20compared%20to%20fixed%20block-based%20diffusion%20methods%2C%20with%20the%20most%20significant%20improvement%20reaching%2016.5%25.%20These%20results%20demonstrate%20that%20deferring%20token%20commitment%20based%20on%20uncertainty%20is%20a%20simple%20yet%20effective%20principle%20for%20improving%20both%20the%20quality%20and%20efficiency%20of%20diffusion%20language%20model%20decoding.&entry.1838667208=http%3A//arxiv.org/abs/2601.02076v2&entry.124074799=Read"},
{"title": "Co-Initialization of Control Filter and Secondary Path via Meta-Learning for Active Noise Control", "author": "Ziyi Yang and Li Rao and Zhengding Luo and Dongyuan Shi and Qirui Huang and Woon-Seng Gan", "abstract": "Active noise control (ANC) must adapt quickly when the acoustic environment changes, yet early performance is largely dictated by initialization. We address this with a Model-Agnostic Meta-Learning (MAML) co-initialization that jointly sets the control filter and the secondary-path model for FxLMS-based ANC while keeping the runtime algorithm unchanged. The initializer is pre-trained on a small set of measured paths using short two-phase inner loops that mimic identification followed by residual-noise reduction, and is applied by simply setting the learned initial coefficients. In an online secondary path modeling FxLMS testbed, it yields lower early-stage error, shorter time-to-target, reduced auxiliary-noise energy, and faster recovery after path changes than a baseline without re-initialization. The method provides a simple fast start for feedforward ANC under environment changes, requiring a small set of paths to pre-train.", "link": "http://arxiv.org/abs/2601.13849v1", "date": "2026-01-20", "relevancy": 2.1795, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4811}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4141}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4124}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Co-Initialization%20of%20Control%20Filter%20and%20Secondary%20Path%20via%20Meta-Learning%20for%20Active%20Noise%20Control&body=Title%3A%20Co-Initialization%20of%20Control%20Filter%20and%20Secondary%20Path%20via%20Meta-Learning%20for%20Active%20Noise%20Control%0AAuthor%3A%20Ziyi%20Yang%20and%20Li%20Rao%20and%20Zhengding%20Luo%20and%20Dongyuan%20Shi%20and%20Qirui%20Huang%20and%20Woon-Seng%20Gan%0AAbstract%3A%20Active%20noise%20control%20%28ANC%29%20must%20adapt%20quickly%20when%20the%20acoustic%20environment%20changes%2C%20yet%20early%20performance%20is%20largely%20dictated%20by%20initialization.%20We%20address%20this%20with%20a%20Model-Agnostic%20Meta-Learning%20%28MAML%29%20co-initialization%20that%20jointly%20sets%20the%20control%20filter%20and%20the%20secondary-path%20model%20for%20FxLMS-based%20ANC%20while%20keeping%20the%20runtime%20algorithm%20unchanged.%20The%20initializer%20is%20pre-trained%20on%20a%20small%20set%20of%20measured%20paths%20using%20short%20two-phase%20inner%20loops%20that%20mimic%20identification%20followed%20by%20residual-noise%20reduction%2C%20and%20is%20applied%20by%20simply%20setting%20the%20learned%20initial%20coefficients.%20In%20an%20online%20secondary%20path%20modeling%20FxLMS%20testbed%2C%20it%20yields%20lower%20early-stage%20error%2C%20shorter%20time-to-target%2C%20reduced%20auxiliary-noise%20energy%2C%20and%20faster%20recovery%20after%20path%20changes%20than%20a%20baseline%20without%20re-initialization.%20The%20method%20provides%20a%20simple%20fast%20start%20for%20feedforward%20ANC%20under%20environment%20changes%2C%20requiring%20a%20small%20set%20of%20paths%20to%20pre-train.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13849v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCo-Initialization%2520of%2520Control%2520Filter%2520and%2520Secondary%2520Path%2520via%2520Meta-Learning%2520for%2520Active%2520Noise%2520Control%26entry.906535625%3DZiyi%2520Yang%2520and%2520Li%2520Rao%2520and%2520Zhengding%2520Luo%2520and%2520Dongyuan%2520Shi%2520and%2520Qirui%2520Huang%2520and%2520Woon-Seng%2520Gan%26entry.1292438233%3DActive%2520noise%2520control%2520%2528ANC%2529%2520must%2520adapt%2520quickly%2520when%2520the%2520acoustic%2520environment%2520changes%252C%2520yet%2520early%2520performance%2520is%2520largely%2520dictated%2520by%2520initialization.%2520We%2520address%2520this%2520with%2520a%2520Model-Agnostic%2520Meta-Learning%2520%2528MAML%2529%2520co-initialization%2520that%2520jointly%2520sets%2520the%2520control%2520filter%2520and%2520the%2520secondary-path%2520model%2520for%2520FxLMS-based%2520ANC%2520while%2520keeping%2520the%2520runtime%2520algorithm%2520unchanged.%2520The%2520initializer%2520is%2520pre-trained%2520on%2520a%2520small%2520set%2520of%2520measured%2520paths%2520using%2520short%2520two-phase%2520inner%2520loops%2520that%2520mimic%2520identification%2520followed%2520by%2520residual-noise%2520reduction%252C%2520and%2520is%2520applied%2520by%2520simply%2520setting%2520the%2520learned%2520initial%2520coefficients.%2520In%2520an%2520online%2520secondary%2520path%2520modeling%2520FxLMS%2520testbed%252C%2520it%2520yields%2520lower%2520early-stage%2520error%252C%2520shorter%2520time-to-target%252C%2520reduced%2520auxiliary-noise%2520energy%252C%2520and%2520faster%2520recovery%2520after%2520path%2520changes%2520than%2520a%2520baseline%2520without%2520re-initialization.%2520The%2520method%2520provides%2520a%2520simple%2520fast%2520start%2520for%2520feedforward%2520ANC%2520under%2520environment%2520changes%252C%2520requiring%2520a%2520small%2520set%2520of%2520paths%2520to%2520pre-train.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13849v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-Initialization%20of%20Control%20Filter%20and%20Secondary%20Path%20via%20Meta-Learning%20for%20Active%20Noise%20Control&entry.906535625=Ziyi%20Yang%20and%20Li%20Rao%20and%20Zhengding%20Luo%20and%20Dongyuan%20Shi%20and%20Qirui%20Huang%20and%20Woon-Seng%20Gan&entry.1292438233=Active%20noise%20control%20%28ANC%29%20must%20adapt%20quickly%20when%20the%20acoustic%20environment%20changes%2C%20yet%20early%20performance%20is%20largely%20dictated%20by%20initialization.%20We%20address%20this%20with%20a%20Model-Agnostic%20Meta-Learning%20%28MAML%29%20co-initialization%20that%20jointly%20sets%20the%20control%20filter%20and%20the%20secondary-path%20model%20for%20FxLMS-based%20ANC%20while%20keeping%20the%20runtime%20algorithm%20unchanged.%20The%20initializer%20is%20pre-trained%20on%20a%20small%20set%20of%20measured%20paths%20using%20short%20two-phase%20inner%20loops%20that%20mimic%20identification%20followed%20by%20residual-noise%20reduction%2C%20and%20is%20applied%20by%20simply%20setting%20the%20learned%20initial%20coefficients.%20In%20an%20online%20secondary%20path%20modeling%20FxLMS%20testbed%2C%20it%20yields%20lower%20early-stage%20error%2C%20shorter%20time-to-target%2C%20reduced%20auxiliary-noise%20energy%2C%20and%20faster%20recovery%20after%20path%20changes%20than%20a%20baseline%20without%20re-initialization.%20The%20method%20provides%20a%20simple%20fast%20start%20for%20feedforward%20ANC%20under%20environment%20changes%2C%20requiring%20a%20small%20set%20of%20paths%20to%20pre-train.&entry.1838667208=http%3A//arxiv.org/abs/2601.13849v1&entry.124074799=Read"},
{"title": "Multimodal Emotion Recognition using Audio-Video Transformer Fusion with Cross Attention", "author": "Joe Dhanith P R and Shravan Venkatraman and Vigya Sharma and Santhosh Malarvannan", "abstract": "Multimodal emotion recognition (MER) aims to infer human affect by jointly modeling audio and visual cues; however, existing approaches often struggle with temporal misalignment, weakly discriminative feature representations, and suboptimal fusion of heterogeneous modalities. To address these challenges, we propose AVT-CA, an Audio-Video Transformer architecture with cross attention for robust emotion recognition. The proposed model introduces a hierarchical video feature representation that combines channel attention, spatial attention, and local feature extraction to emphasize emotionally salient regions while suppressing irrelevant information. These refined visual features are integrated with audio representations through an intermediate transformer-based fusion mechanism that captures interlinked temporal dependencies across modalities. Furthermore, a cross-attention module selectively reinforces mutually consistent audio-visual cues, enabling effective feature selection and noise-aware fusion. Extensive experiments on three benchmark datasets, CMU-MOSEI, RAVDESS, and CREMA-D, demonstrate that AVT-CA consistently outperforms state-of-the-art baselines, achieving significant improvements in both accuracy and F1-score. Our source code is publicly available at https://github.com/shravan-18/AVTCA.", "link": "http://arxiv.org/abs/2407.18552v4", "date": "2026-01-20", "relevancy": 2.1722, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5676}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5377}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5206}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multimodal%20Emotion%20Recognition%20using%20Audio-Video%20Transformer%20Fusion%20with%20Cross%20Attention&body=Title%3A%20Multimodal%20Emotion%20Recognition%20using%20Audio-Video%20Transformer%20Fusion%20with%20Cross%20Attention%0AAuthor%3A%20Joe%20Dhanith%20P%20R%20and%20Shravan%20Venkatraman%20and%20Vigya%20Sharma%20and%20Santhosh%20Malarvannan%0AAbstract%3A%20Multimodal%20emotion%20recognition%20%28MER%29%20aims%20to%20infer%20human%20affect%20by%20jointly%20modeling%20audio%20and%20visual%20cues%3B%20however%2C%20existing%20approaches%20often%20struggle%20with%20temporal%20misalignment%2C%20weakly%20discriminative%20feature%20representations%2C%20and%20suboptimal%20fusion%20of%20heterogeneous%20modalities.%20To%20address%20these%20challenges%2C%20we%20propose%20AVT-CA%2C%20an%20Audio-Video%20Transformer%20architecture%20with%20cross%20attention%20for%20robust%20emotion%20recognition.%20The%20proposed%20model%20introduces%20a%20hierarchical%20video%20feature%20representation%20that%20combines%20channel%20attention%2C%20spatial%20attention%2C%20and%20local%20feature%20extraction%20to%20emphasize%20emotionally%20salient%20regions%20while%20suppressing%20irrelevant%20information.%20These%20refined%20visual%20features%20are%20integrated%20with%20audio%20representations%20through%20an%20intermediate%20transformer-based%20fusion%20mechanism%20that%20captures%20interlinked%20temporal%20dependencies%20across%20modalities.%20Furthermore%2C%20a%20cross-attention%20module%20selectively%20reinforces%20mutually%20consistent%20audio-visual%20cues%2C%20enabling%20effective%20feature%20selection%20and%20noise-aware%20fusion.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%2C%20CMU-MOSEI%2C%20RAVDESS%2C%20and%20CREMA-D%2C%20demonstrate%20that%20AVT-CA%20consistently%20outperforms%20state-of-the-art%20baselines%2C%20achieving%20significant%20improvements%20in%20both%20accuracy%20and%20F1-score.%20Our%20source%20code%20is%20publicly%20available%20at%20https%3A//github.com/shravan-18/AVTCA.%0ALink%3A%20http%3A//arxiv.org/abs/2407.18552v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMultimodal%2520Emotion%2520Recognition%2520using%2520Audio-Video%2520Transformer%2520Fusion%2520with%2520Cross%2520Attention%26entry.906535625%3DJoe%2520Dhanith%2520P%2520R%2520and%2520Shravan%2520Venkatraman%2520and%2520Vigya%2520Sharma%2520and%2520Santhosh%2520Malarvannan%26entry.1292438233%3DMultimodal%2520emotion%2520recognition%2520%2528MER%2529%2520aims%2520to%2520infer%2520human%2520affect%2520by%2520jointly%2520modeling%2520audio%2520and%2520visual%2520cues%253B%2520however%252C%2520existing%2520approaches%2520often%2520struggle%2520with%2520temporal%2520misalignment%252C%2520weakly%2520discriminative%2520feature%2520representations%252C%2520and%2520suboptimal%2520fusion%2520of%2520heterogeneous%2520modalities.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520AVT-CA%252C%2520an%2520Audio-Video%2520Transformer%2520architecture%2520with%2520cross%2520attention%2520for%2520robust%2520emotion%2520recognition.%2520The%2520proposed%2520model%2520introduces%2520a%2520hierarchical%2520video%2520feature%2520representation%2520that%2520combines%2520channel%2520attention%252C%2520spatial%2520attention%252C%2520and%2520local%2520feature%2520extraction%2520to%2520emphasize%2520emotionally%2520salient%2520regions%2520while%2520suppressing%2520irrelevant%2520information.%2520These%2520refined%2520visual%2520features%2520are%2520integrated%2520with%2520audio%2520representations%2520through%2520an%2520intermediate%2520transformer-based%2520fusion%2520mechanism%2520that%2520captures%2520interlinked%2520temporal%2520dependencies%2520across%2520modalities.%2520Furthermore%252C%2520a%2520cross-attention%2520module%2520selectively%2520reinforces%2520mutually%2520consistent%2520audio-visual%2520cues%252C%2520enabling%2520effective%2520feature%2520selection%2520and%2520noise-aware%2520fusion.%2520Extensive%2520experiments%2520on%2520three%2520benchmark%2520datasets%252C%2520CMU-MOSEI%252C%2520RAVDESS%252C%2520and%2520CREMA-D%252C%2520demonstrate%2520that%2520AVT-CA%2520consistently%2520outperforms%2520state-of-the-art%2520baselines%252C%2520achieving%2520significant%2520improvements%2520in%2520both%2520accuracy%2520and%2520F1-score.%2520Our%2520source%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/shravan-18/AVTCA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.18552v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multimodal%20Emotion%20Recognition%20using%20Audio-Video%20Transformer%20Fusion%20with%20Cross%20Attention&entry.906535625=Joe%20Dhanith%20P%20R%20and%20Shravan%20Venkatraman%20and%20Vigya%20Sharma%20and%20Santhosh%20Malarvannan&entry.1292438233=Multimodal%20emotion%20recognition%20%28MER%29%20aims%20to%20infer%20human%20affect%20by%20jointly%20modeling%20audio%20and%20visual%20cues%3B%20however%2C%20existing%20approaches%20often%20struggle%20with%20temporal%20misalignment%2C%20weakly%20discriminative%20feature%20representations%2C%20and%20suboptimal%20fusion%20of%20heterogeneous%20modalities.%20To%20address%20these%20challenges%2C%20we%20propose%20AVT-CA%2C%20an%20Audio-Video%20Transformer%20architecture%20with%20cross%20attention%20for%20robust%20emotion%20recognition.%20The%20proposed%20model%20introduces%20a%20hierarchical%20video%20feature%20representation%20that%20combines%20channel%20attention%2C%20spatial%20attention%2C%20and%20local%20feature%20extraction%20to%20emphasize%20emotionally%20salient%20regions%20while%20suppressing%20irrelevant%20information.%20These%20refined%20visual%20features%20are%20integrated%20with%20audio%20representations%20through%20an%20intermediate%20transformer-based%20fusion%20mechanism%20that%20captures%20interlinked%20temporal%20dependencies%20across%20modalities.%20Furthermore%2C%20a%20cross-attention%20module%20selectively%20reinforces%20mutually%20consistent%20audio-visual%20cues%2C%20enabling%20effective%20feature%20selection%20and%20noise-aware%20fusion.%20Extensive%20experiments%20on%20three%20benchmark%20datasets%2C%20CMU-MOSEI%2C%20RAVDESS%2C%20and%20CREMA-D%2C%20demonstrate%20that%20AVT-CA%20consistently%20outperforms%20state-of-the-art%20baselines%2C%20achieving%20significant%20improvements%20in%20both%20accuracy%20and%20F1-score.%20Our%20source%20code%20is%20publicly%20available%20at%20https%3A//github.com/shravan-18/AVTCA.&entry.1838667208=http%3A//arxiv.org/abs/2407.18552v4&entry.124074799=Read"},
{"title": "Optimal L2 Regularization in High-dimensional Continual Linear Regression", "author": "Gilad Karpel and Edward Moroshko and Ran Levinstein and Ron Meir and Daniel Soudry and Itay Evron", "abstract": "We study generalization in an overparameterized continual linear regression setting, where a model is trained with L2 (isotropic) regularization across a sequence of tasks. We derive a closed-form expression for the expected generalization loss in the high-dimensional regime that holds for arbitrary linear teachers. We demonstrate that isotropic regularization mitigates label noise under both single-teacher and multiple i.i.d. teacher settings, whereas prior work accommodating multiple teachers either did not employ regularization or used memory-demanding methods. Furthermore, we prove that the optimal fixed regularization strength scales nearly linearly with the number of tasks $T$, specifically as $T/\\ln T$. To our knowledge, this is the first such result in theoretical continual learning. Finally, we validate our theoretical findings through experiments on linear regression and neural networks, illustrating how this scaling law affects generalization and offering a practical recipe for the design of continual learning systems.", "link": "http://arxiv.org/abs/2601.13844v1", "date": "2026-01-20", "relevancy": 2.172, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4361}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4348}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4323}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20L2%20Regularization%20in%20High-dimensional%20Continual%20Linear%20Regression&body=Title%3A%20Optimal%20L2%20Regularization%20in%20High-dimensional%20Continual%20Linear%20Regression%0AAuthor%3A%20Gilad%20Karpel%20and%20Edward%20Moroshko%20and%20Ran%20Levinstein%20and%20Ron%20Meir%20and%20Daniel%20Soudry%20and%20Itay%20Evron%0AAbstract%3A%20We%20study%20generalization%20in%20an%20overparameterized%20continual%20linear%20regression%20setting%2C%20where%20a%20model%20is%20trained%20with%20L2%20%28isotropic%29%20regularization%20across%20a%20sequence%20of%20tasks.%20We%20derive%20a%20closed-form%20expression%20for%20the%20expected%20generalization%20loss%20in%20the%20high-dimensional%20regime%20that%20holds%20for%20arbitrary%20linear%20teachers.%20We%20demonstrate%20that%20isotropic%20regularization%20mitigates%20label%20noise%20under%20both%20single-teacher%20and%20multiple%20i.i.d.%20teacher%20settings%2C%20whereas%20prior%20work%20accommodating%20multiple%20teachers%20either%20did%20not%20employ%20regularization%20or%20used%20memory-demanding%20methods.%20Furthermore%2C%20we%20prove%20that%20the%20optimal%20fixed%20regularization%20strength%20scales%20nearly%20linearly%20with%20the%20number%20of%20tasks%20%24T%24%2C%20specifically%20as%20%24T/%5Cln%20T%24.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20such%20result%20in%20theoretical%20continual%20learning.%20Finally%2C%20we%20validate%20our%20theoretical%20findings%20through%20experiments%20on%20linear%20regression%20and%20neural%20networks%2C%20illustrating%20how%20this%20scaling%20law%20affects%20generalization%20and%20offering%20a%20practical%20recipe%20for%20the%20design%20of%20continual%20learning%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13844v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520L2%2520Regularization%2520in%2520High-dimensional%2520Continual%2520Linear%2520Regression%26entry.906535625%3DGilad%2520Karpel%2520and%2520Edward%2520Moroshko%2520and%2520Ran%2520Levinstein%2520and%2520Ron%2520Meir%2520and%2520Daniel%2520Soudry%2520and%2520Itay%2520Evron%26entry.1292438233%3DWe%2520study%2520generalization%2520in%2520an%2520overparameterized%2520continual%2520linear%2520regression%2520setting%252C%2520where%2520a%2520model%2520is%2520trained%2520with%2520L2%2520%2528isotropic%2529%2520regularization%2520across%2520a%2520sequence%2520of%2520tasks.%2520We%2520derive%2520a%2520closed-form%2520expression%2520for%2520the%2520expected%2520generalization%2520loss%2520in%2520the%2520high-dimensional%2520regime%2520that%2520holds%2520for%2520arbitrary%2520linear%2520teachers.%2520We%2520demonstrate%2520that%2520isotropic%2520regularization%2520mitigates%2520label%2520noise%2520under%2520both%2520single-teacher%2520and%2520multiple%2520i.i.d.%2520teacher%2520settings%252C%2520whereas%2520prior%2520work%2520accommodating%2520multiple%2520teachers%2520either%2520did%2520not%2520employ%2520regularization%2520or%2520used%2520memory-demanding%2520methods.%2520Furthermore%252C%2520we%2520prove%2520that%2520the%2520optimal%2520fixed%2520regularization%2520strength%2520scales%2520nearly%2520linearly%2520with%2520the%2520number%2520of%2520tasks%2520%2524T%2524%252C%2520specifically%2520as%2520%2524T/%255Cln%2520T%2524.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520such%2520result%2520in%2520theoretical%2520continual%2520learning.%2520Finally%252C%2520we%2520validate%2520our%2520theoretical%2520findings%2520through%2520experiments%2520on%2520linear%2520regression%2520and%2520neural%2520networks%252C%2520illustrating%2520how%2520this%2520scaling%2520law%2520affects%2520generalization%2520and%2520offering%2520a%2520practical%2520recipe%2520for%2520the%2520design%2520of%2520continual%2520learning%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13844v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20L2%20Regularization%20in%20High-dimensional%20Continual%20Linear%20Regression&entry.906535625=Gilad%20Karpel%20and%20Edward%20Moroshko%20and%20Ran%20Levinstein%20and%20Ron%20Meir%20and%20Daniel%20Soudry%20and%20Itay%20Evron&entry.1292438233=We%20study%20generalization%20in%20an%20overparameterized%20continual%20linear%20regression%20setting%2C%20where%20a%20model%20is%20trained%20with%20L2%20%28isotropic%29%20regularization%20across%20a%20sequence%20of%20tasks.%20We%20derive%20a%20closed-form%20expression%20for%20the%20expected%20generalization%20loss%20in%20the%20high-dimensional%20regime%20that%20holds%20for%20arbitrary%20linear%20teachers.%20We%20demonstrate%20that%20isotropic%20regularization%20mitigates%20label%20noise%20under%20both%20single-teacher%20and%20multiple%20i.i.d.%20teacher%20settings%2C%20whereas%20prior%20work%20accommodating%20multiple%20teachers%20either%20did%20not%20employ%20regularization%20or%20used%20memory-demanding%20methods.%20Furthermore%2C%20we%20prove%20that%20the%20optimal%20fixed%20regularization%20strength%20scales%20nearly%20linearly%20with%20the%20number%20of%20tasks%20%24T%24%2C%20specifically%20as%20%24T/%5Cln%20T%24.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20such%20result%20in%20theoretical%20continual%20learning.%20Finally%2C%20we%20validate%20our%20theoretical%20findings%20through%20experiments%20on%20linear%20regression%20and%20neural%20networks%2C%20illustrating%20how%20this%20scaling%20law%20affects%20generalization%20and%20offering%20a%20practical%20recipe%20for%20the%20design%20of%20continual%20learning%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.13844v1&entry.124074799=Read"},
{"title": "LLMOrbit: A Circular Taxonomy of Large Language Models -From Scaling Walls to Agentic AI Systems", "author": "Badri N. Patro and Vijay S. Agneeswaran", "abstract": "The field of artificial intelligence has undergone a revolution from foundational Transformer architectures to reasoning-capable systems approaching human-level performance. We present LLMOrbit, a comprehensive circular taxonomy navigating the landscape of large language models spanning 2019-2025. This survey examines over 50 models across 15 organizations through eight interconnected orbital dimensions, documenting architectural innovations, training methodologies, and efficiency patterns defining modern LLMs, generative AI, and agentic systems. We identify three critical crises: (1) data scarcity (9-27T tokens depleted by 2026-2028), (2) exponential cost growth ($3M to $300M+ in 5 years), and (3) unsustainable energy consumption (22x increase), establishing the scaling wall limiting brute-force approaches. Our analysis reveals six paradigms breaking this wall: (1) test-time compute (o1, DeepSeek-R1 achieve GPT-4 performance with 10x inference compute), (2) quantization (4-8x compression), (3) distributed edge computing (10x cost reduction), (4) model merging, (5) efficient training (ORPO reduces memory 50%), and (6) small specialized models (Phi-4 14B matches larger models). Three paradigm shifts emerge: (1) post-training gains (RLHF, GRPO, pure RL contribute substantially, DeepSeek-R1 achieving 79.8% MATH), (2) efficiency revolution (MoE routing 18x efficiency, Multi-head Latent Attention 8x KV cache compression enables GPT-4-level performance at <$0.30/M tokens), and (3) democratization (open-source Llama 3 88.6% MMLU surpasses GPT-4 86.4%). We provide insights into techniques (RLHF, PPO, DPO, GRPO, ORPO), trace evolution from passive generation to tool-using agents (ReAct, RAG, multi-agent systems), and analyze post-training innovations.", "link": "http://arxiv.org/abs/2601.14053v1", "date": "2026-01-20", "relevancy": 2.171, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5469}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5419}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5419}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMOrbit%3A%20A%20Circular%20Taxonomy%20of%20Large%20Language%20Models%20-From%20Scaling%20Walls%20to%20Agentic%20AI%20Systems&body=Title%3A%20LLMOrbit%3A%20A%20Circular%20Taxonomy%20of%20Large%20Language%20Models%20-From%20Scaling%20Walls%20to%20Agentic%20AI%20Systems%0AAuthor%3A%20Badri%20N.%20Patro%20and%20Vijay%20S.%20Agneeswaran%0AAbstract%3A%20The%20field%20of%20artificial%20intelligence%20has%20undergone%20a%20revolution%20from%20foundational%20Transformer%20architectures%20to%20reasoning-capable%20systems%20approaching%20human-level%20performance.%20We%20present%20LLMOrbit%2C%20a%20comprehensive%20circular%20taxonomy%20navigating%20the%20landscape%20of%20large%20language%20models%20spanning%202019-2025.%20This%20survey%20examines%20over%2050%20models%20across%2015%20organizations%20through%20eight%20interconnected%20orbital%20dimensions%2C%20documenting%20architectural%20innovations%2C%20training%20methodologies%2C%20and%20efficiency%20patterns%20defining%20modern%20LLMs%2C%20generative%20AI%2C%20and%20agentic%20systems.%20We%20identify%20three%20critical%20crises%3A%20%281%29%20data%20scarcity%20%289-27T%20tokens%20depleted%20by%202026-2028%29%2C%20%282%29%20exponential%20cost%20growth%20%28%243M%20to%20%24300M%2B%20in%205%20years%29%2C%20and%20%283%29%20unsustainable%20energy%20consumption%20%2822x%20increase%29%2C%20establishing%20the%20scaling%20wall%20limiting%20brute-force%20approaches.%20Our%20analysis%20reveals%20six%20paradigms%20breaking%20this%20wall%3A%20%281%29%20test-time%20compute%20%28o1%2C%20DeepSeek-R1%20achieve%20GPT-4%20performance%20with%2010x%20inference%20compute%29%2C%20%282%29%20quantization%20%284-8x%20compression%29%2C%20%283%29%20distributed%20edge%20computing%20%2810x%20cost%20reduction%29%2C%20%284%29%20model%20merging%2C%20%285%29%20efficient%20training%20%28ORPO%20reduces%20memory%2050%25%29%2C%20and%20%286%29%20small%20specialized%20models%20%28Phi-4%2014B%20matches%20larger%20models%29.%20Three%20paradigm%20shifts%20emerge%3A%20%281%29%20post-training%20gains%20%28RLHF%2C%20GRPO%2C%20pure%20RL%20contribute%20substantially%2C%20DeepSeek-R1%20achieving%2079.8%25%20MATH%29%2C%20%282%29%20efficiency%20revolution%20%28MoE%20routing%2018x%20efficiency%2C%20Multi-head%20Latent%20Attention%208x%20KV%20cache%20compression%20enables%20GPT-4-level%20performance%20at%20%3C%240.30/M%20tokens%29%2C%20and%20%283%29%20democratization%20%28open-source%20Llama%203%2088.6%25%20MMLU%20surpasses%20GPT-4%2086.4%25%29.%20We%20provide%20insights%20into%20techniques%20%28RLHF%2C%20PPO%2C%20DPO%2C%20GRPO%2C%20ORPO%29%2C%20trace%20evolution%20from%20passive%20generation%20to%20tool-using%20agents%20%28ReAct%2C%20RAG%2C%20multi-agent%20systems%29%2C%20and%20analyze%20post-training%20innovations.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14053v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMOrbit%253A%2520A%2520Circular%2520Taxonomy%2520of%2520Large%2520Language%2520Models%2520-From%2520Scaling%2520Walls%2520to%2520Agentic%2520AI%2520Systems%26entry.906535625%3DBadri%2520N.%2520Patro%2520and%2520Vijay%2520S.%2520Agneeswaran%26entry.1292438233%3DThe%2520field%2520of%2520artificial%2520intelligence%2520has%2520undergone%2520a%2520revolution%2520from%2520foundational%2520Transformer%2520architectures%2520to%2520reasoning-capable%2520systems%2520approaching%2520human-level%2520performance.%2520We%2520present%2520LLMOrbit%252C%2520a%2520comprehensive%2520circular%2520taxonomy%2520navigating%2520the%2520landscape%2520of%2520large%2520language%2520models%2520spanning%25202019-2025.%2520This%2520survey%2520examines%2520over%252050%2520models%2520across%252015%2520organizations%2520through%2520eight%2520interconnected%2520orbital%2520dimensions%252C%2520documenting%2520architectural%2520innovations%252C%2520training%2520methodologies%252C%2520and%2520efficiency%2520patterns%2520defining%2520modern%2520LLMs%252C%2520generative%2520AI%252C%2520and%2520agentic%2520systems.%2520We%2520identify%2520three%2520critical%2520crises%253A%2520%25281%2529%2520data%2520scarcity%2520%25289-27T%2520tokens%2520depleted%2520by%25202026-2028%2529%252C%2520%25282%2529%2520exponential%2520cost%2520growth%2520%2528%25243M%2520to%2520%2524300M%252B%2520in%25205%2520years%2529%252C%2520and%2520%25283%2529%2520unsustainable%2520energy%2520consumption%2520%252822x%2520increase%2529%252C%2520establishing%2520the%2520scaling%2520wall%2520limiting%2520brute-force%2520approaches.%2520Our%2520analysis%2520reveals%2520six%2520paradigms%2520breaking%2520this%2520wall%253A%2520%25281%2529%2520test-time%2520compute%2520%2528o1%252C%2520DeepSeek-R1%2520achieve%2520GPT-4%2520performance%2520with%252010x%2520inference%2520compute%2529%252C%2520%25282%2529%2520quantization%2520%25284-8x%2520compression%2529%252C%2520%25283%2529%2520distributed%2520edge%2520computing%2520%252810x%2520cost%2520reduction%2529%252C%2520%25284%2529%2520model%2520merging%252C%2520%25285%2529%2520efficient%2520training%2520%2528ORPO%2520reduces%2520memory%252050%2525%2529%252C%2520and%2520%25286%2529%2520small%2520specialized%2520models%2520%2528Phi-4%252014B%2520matches%2520larger%2520models%2529.%2520Three%2520paradigm%2520shifts%2520emerge%253A%2520%25281%2529%2520post-training%2520gains%2520%2528RLHF%252C%2520GRPO%252C%2520pure%2520RL%2520contribute%2520substantially%252C%2520DeepSeek-R1%2520achieving%252079.8%2525%2520MATH%2529%252C%2520%25282%2529%2520efficiency%2520revolution%2520%2528MoE%2520routing%252018x%2520efficiency%252C%2520Multi-head%2520Latent%2520Attention%25208x%2520KV%2520cache%2520compression%2520enables%2520GPT-4-level%2520performance%2520at%2520%253C%25240.30/M%2520tokens%2529%252C%2520and%2520%25283%2529%2520democratization%2520%2528open-source%2520Llama%25203%252088.6%2525%2520MMLU%2520surpasses%2520GPT-4%252086.4%2525%2529.%2520We%2520provide%2520insights%2520into%2520techniques%2520%2528RLHF%252C%2520PPO%252C%2520DPO%252C%2520GRPO%252C%2520ORPO%2529%252C%2520trace%2520evolution%2520from%2520passive%2520generation%2520to%2520tool-using%2520agents%2520%2528ReAct%252C%2520RAG%252C%2520multi-agent%2520systems%2529%252C%2520and%2520analyze%2520post-training%2520innovations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14053v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMOrbit%3A%20A%20Circular%20Taxonomy%20of%20Large%20Language%20Models%20-From%20Scaling%20Walls%20to%20Agentic%20AI%20Systems&entry.906535625=Badri%20N.%20Patro%20and%20Vijay%20S.%20Agneeswaran&entry.1292438233=The%20field%20of%20artificial%20intelligence%20has%20undergone%20a%20revolution%20from%20foundational%20Transformer%20architectures%20to%20reasoning-capable%20systems%20approaching%20human-level%20performance.%20We%20present%20LLMOrbit%2C%20a%20comprehensive%20circular%20taxonomy%20navigating%20the%20landscape%20of%20large%20language%20models%20spanning%202019-2025.%20This%20survey%20examines%20over%2050%20models%20across%2015%20organizations%20through%20eight%20interconnected%20orbital%20dimensions%2C%20documenting%20architectural%20innovations%2C%20training%20methodologies%2C%20and%20efficiency%20patterns%20defining%20modern%20LLMs%2C%20generative%20AI%2C%20and%20agentic%20systems.%20We%20identify%20three%20critical%20crises%3A%20%281%29%20data%20scarcity%20%289-27T%20tokens%20depleted%20by%202026-2028%29%2C%20%282%29%20exponential%20cost%20growth%20%28%243M%20to%20%24300M%2B%20in%205%20years%29%2C%20and%20%283%29%20unsustainable%20energy%20consumption%20%2822x%20increase%29%2C%20establishing%20the%20scaling%20wall%20limiting%20brute-force%20approaches.%20Our%20analysis%20reveals%20six%20paradigms%20breaking%20this%20wall%3A%20%281%29%20test-time%20compute%20%28o1%2C%20DeepSeek-R1%20achieve%20GPT-4%20performance%20with%2010x%20inference%20compute%29%2C%20%282%29%20quantization%20%284-8x%20compression%29%2C%20%283%29%20distributed%20edge%20computing%20%2810x%20cost%20reduction%29%2C%20%284%29%20model%20merging%2C%20%285%29%20efficient%20training%20%28ORPO%20reduces%20memory%2050%25%29%2C%20and%20%286%29%20small%20specialized%20models%20%28Phi-4%2014B%20matches%20larger%20models%29.%20Three%20paradigm%20shifts%20emerge%3A%20%281%29%20post-training%20gains%20%28RLHF%2C%20GRPO%2C%20pure%20RL%20contribute%20substantially%2C%20DeepSeek-R1%20achieving%2079.8%25%20MATH%29%2C%20%282%29%20efficiency%20revolution%20%28MoE%20routing%2018x%20efficiency%2C%20Multi-head%20Latent%20Attention%208x%20KV%20cache%20compression%20enables%20GPT-4-level%20performance%20at%20%3C%240.30/M%20tokens%29%2C%20and%20%283%29%20democratization%20%28open-source%20Llama%203%2088.6%25%20MMLU%20surpasses%20GPT-4%2086.4%25%29.%20We%20provide%20insights%20into%20techniques%20%28RLHF%2C%20PPO%2C%20DPO%2C%20GRPO%2C%20ORPO%29%2C%20trace%20evolution%20from%20passive%20generation%20to%20tool-using%20agents%20%28ReAct%2C%20RAG%2C%20multi-agent%20systems%29%2C%20and%20analyze%20post-training%20innovations.&entry.1838667208=http%3A//arxiv.org/abs/2601.14053v1&entry.124074799=Read"},
{"title": "Tube-Based Robust Control Strategy for Vision-Guided Autonomous Vehicles", "author": "Der-Hau Lee", "abstract": "A robust control strategy for autonomous vehicles can improve system stability, enhance riding comfort, and prevent driving accidents. This paper presents a novel interpolation-tube-based constrained iterative linear quadratic regulator (itube-CILQR) algorithm for autonomous computer-vision-based vehicle lane-keeping. The goal of the algorithm is to enhance robustness during high-speed cornering on tight turns. Compared with standard tube-based approaches, the proposed itube-CILQR algorithm reduces system conservatism and exhibits higher computational speed. Numerical simulations and vision-based experiments were conducted to examine the feasibility of using the proposed algorithm for controlling autonomous vehicles. The results indicated that the proposed algorithm achieved superior vehicle lane-keeping performance to variational CILQR-based methods and model predictive control (MPC) approaches involving the use of a classical interior-point optimizer. Specifically, itube-CILQR required an average runtime of 3.45 ms to generate a control signal for guiding a self-driving vehicle. By comparison, itube-MPC typically required a 4.32 times longer computation time to complete the same task. Moreover, the influence of conservatism on system behavior was investigated by exploring the variations in the interpolation variables derived using the proposed itube-CILQR algorithm during lane-keeping maneuvers.", "link": "http://arxiv.org/abs/2503.18752v2", "date": "2026-01-20", "relevancy": 2.1559, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5862}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5134}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5019}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tube-Based%20Robust%20Control%20Strategy%20for%20Vision-Guided%20Autonomous%20Vehicles&body=Title%3A%20Tube-Based%20Robust%20Control%20Strategy%20for%20Vision-Guided%20Autonomous%20Vehicles%0AAuthor%3A%20Der-Hau%20Lee%0AAbstract%3A%20A%20robust%20control%20strategy%20for%20autonomous%20vehicles%20can%20improve%20system%20stability%2C%20enhance%20riding%20comfort%2C%20and%20prevent%20driving%20accidents.%20This%20paper%20presents%20a%20novel%20interpolation-tube-based%20constrained%20iterative%20linear%20quadratic%20regulator%20%28itube-CILQR%29%20algorithm%20for%20autonomous%20computer-vision-based%20vehicle%20lane-keeping.%20The%20goal%20of%20the%20algorithm%20is%20to%20enhance%20robustness%20during%20high-speed%20cornering%20on%20tight%20turns.%20Compared%20with%20standard%20tube-based%20approaches%2C%20the%20proposed%20itube-CILQR%20algorithm%20reduces%20system%20conservatism%20and%20exhibits%20higher%20computational%20speed.%20Numerical%20simulations%20and%20vision-based%20experiments%20were%20conducted%20to%20examine%20the%20feasibility%20of%20using%20the%20proposed%20algorithm%20for%20controlling%20autonomous%20vehicles.%20The%20results%20indicated%20that%20the%20proposed%20algorithm%20achieved%20superior%20vehicle%20lane-keeping%20performance%20to%20variational%20CILQR-based%20methods%20and%20model%20predictive%20control%20%28MPC%29%20approaches%20involving%20the%20use%20of%20a%20classical%20interior-point%20optimizer.%20Specifically%2C%20itube-CILQR%20required%20an%20average%20runtime%20of%203.45%20ms%20to%20generate%20a%20control%20signal%20for%20guiding%20a%20self-driving%20vehicle.%20By%20comparison%2C%20itube-MPC%20typically%20required%20a%204.32%20times%20longer%20computation%20time%20to%20complete%20the%20same%20task.%20Moreover%2C%20the%20influence%20of%20conservatism%20on%20system%20behavior%20was%20investigated%20by%20exploring%20the%20variations%20in%20the%20interpolation%20variables%20derived%20using%20the%20proposed%20itube-CILQR%20algorithm%20during%20lane-keeping%20maneuvers.%0ALink%3A%20http%3A//arxiv.org/abs/2503.18752v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTube-Based%2520Robust%2520Control%2520Strategy%2520for%2520Vision-Guided%2520Autonomous%2520Vehicles%26entry.906535625%3DDer-Hau%2520Lee%26entry.1292438233%3DA%2520robust%2520control%2520strategy%2520for%2520autonomous%2520vehicles%2520can%2520improve%2520system%2520stability%252C%2520enhance%2520riding%2520comfort%252C%2520and%2520prevent%2520driving%2520accidents.%2520This%2520paper%2520presents%2520a%2520novel%2520interpolation-tube-based%2520constrained%2520iterative%2520linear%2520quadratic%2520regulator%2520%2528itube-CILQR%2529%2520algorithm%2520for%2520autonomous%2520computer-vision-based%2520vehicle%2520lane-keeping.%2520The%2520goal%2520of%2520the%2520algorithm%2520is%2520to%2520enhance%2520robustness%2520during%2520high-speed%2520cornering%2520on%2520tight%2520turns.%2520Compared%2520with%2520standard%2520tube-based%2520approaches%252C%2520the%2520proposed%2520itube-CILQR%2520algorithm%2520reduces%2520system%2520conservatism%2520and%2520exhibits%2520higher%2520computational%2520speed.%2520Numerical%2520simulations%2520and%2520vision-based%2520experiments%2520were%2520conducted%2520to%2520examine%2520the%2520feasibility%2520of%2520using%2520the%2520proposed%2520algorithm%2520for%2520controlling%2520autonomous%2520vehicles.%2520The%2520results%2520indicated%2520that%2520the%2520proposed%2520algorithm%2520achieved%2520superior%2520vehicle%2520lane-keeping%2520performance%2520to%2520variational%2520CILQR-based%2520methods%2520and%2520model%2520predictive%2520control%2520%2528MPC%2529%2520approaches%2520involving%2520the%2520use%2520of%2520a%2520classical%2520interior-point%2520optimizer.%2520Specifically%252C%2520itube-CILQR%2520required%2520an%2520average%2520runtime%2520of%25203.45%2520ms%2520to%2520generate%2520a%2520control%2520signal%2520for%2520guiding%2520a%2520self-driving%2520vehicle.%2520By%2520comparison%252C%2520itube-MPC%2520typically%2520required%2520a%25204.32%2520times%2520longer%2520computation%2520time%2520to%2520complete%2520the%2520same%2520task.%2520Moreover%252C%2520the%2520influence%2520of%2520conservatism%2520on%2520system%2520behavior%2520was%2520investigated%2520by%2520exploring%2520the%2520variations%2520in%2520the%2520interpolation%2520variables%2520derived%2520using%2520the%2520proposed%2520itube-CILQR%2520algorithm%2520during%2520lane-keeping%2520maneuvers.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2503.18752v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tube-Based%20Robust%20Control%20Strategy%20for%20Vision-Guided%20Autonomous%20Vehicles&entry.906535625=Der-Hau%20Lee&entry.1292438233=A%20robust%20control%20strategy%20for%20autonomous%20vehicles%20can%20improve%20system%20stability%2C%20enhance%20riding%20comfort%2C%20and%20prevent%20driving%20accidents.%20This%20paper%20presents%20a%20novel%20interpolation-tube-based%20constrained%20iterative%20linear%20quadratic%20regulator%20%28itube-CILQR%29%20algorithm%20for%20autonomous%20computer-vision-based%20vehicle%20lane-keeping.%20The%20goal%20of%20the%20algorithm%20is%20to%20enhance%20robustness%20during%20high-speed%20cornering%20on%20tight%20turns.%20Compared%20with%20standard%20tube-based%20approaches%2C%20the%20proposed%20itube-CILQR%20algorithm%20reduces%20system%20conservatism%20and%20exhibits%20higher%20computational%20speed.%20Numerical%20simulations%20and%20vision-based%20experiments%20were%20conducted%20to%20examine%20the%20feasibility%20of%20using%20the%20proposed%20algorithm%20for%20controlling%20autonomous%20vehicles.%20The%20results%20indicated%20that%20the%20proposed%20algorithm%20achieved%20superior%20vehicle%20lane-keeping%20performance%20to%20variational%20CILQR-based%20methods%20and%20model%20predictive%20control%20%28MPC%29%20approaches%20involving%20the%20use%20of%20a%20classical%20interior-point%20optimizer.%20Specifically%2C%20itube-CILQR%20required%20an%20average%20runtime%20of%203.45%20ms%20to%20generate%20a%20control%20signal%20for%20guiding%20a%20self-driving%20vehicle.%20By%20comparison%2C%20itube-MPC%20typically%20required%20a%204.32%20times%20longer%20computation%20time%20to%20complete%20the%20same%20task.%20Moreover%2C%20the%20influence%20of%20conservatism%20on%20system%20behavior%20was%20investigated%20by%20exploring%20the%20variations%20in%20the%20interpolation%20variables%20derived%20using%20the%20proposed%20itube-CILQR%20algorithm%20during%20lane-keeping%20maneuvers.&entry.1838667208=http%3A//arxiv.org/abs/2503.18752v2&entry.124074799=Read"},
{"title": "HoverAI: An Embodied Aerial Agent for Natural Human-Drone Interaction", "author": "Yuhua Jin and Nikita Kuzmin and Georgii Demianchuk and Mariya Lezina and Fawad Mehboob and Issatay Tokmurziyev and Miguel Altamirano Cabrera and Muhammad Ahsan Mustafa and Dzmitry Tsetserukou", "abstract": "Drones operating in human-occupied spaces suffer from insufficient communication mechanisms that create uncertainty about their intentions. We present HoverAI, an embodied aerial agent that integrates drone mobility, infrastructure-independent visual projection, and real-time conversational AI into a unified platform. Equipped with a MEMS laser projector, onboard semi-rigid screen, and RGB camera, HoverAI perceives users through vision and voice, responding via lip-synced avatars that adapt appearance to user demographics. The system employs a multimodal pipeline combining VAD, ASR (Whisper), LLM-based intent classification, RAG for dialogue, face analysis for personalization, and voice synthesis (XTTS v2). Evaluation demonstrates high accuracy in command recognition (F1: 0.90), demographic estimation (gender F1: 0.89, age MAE: 5.14 years), and speech transcription (WER: 0.181). By uniting aerial robotics with adaptive conversational AI and self-contained visual output, HoverAI introduces a new class of spatially-aware, socially responsive embodied agents for applications in guidance, assistance, and human-centered interaction.", "link": "http://arxiv.org/abs/2601.13801v1", "date": "2026-01-20", "relevancy": 2.1545, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5611}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5245}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HoverAI%3A%20An%20Embodied%20Aerial%20Agent%20for%20Natural%20Human-Drone%20Interaction&body=Title%3A%20HoverAI%3A%20An%20Embodied%20Aerial%20Agent%20for%20Natural%20Human-Drone%20Interaction%0AAuthor%3A%20Yuhua%20Jin%20and%20Nikita%20Kuzmin%20and%20Georgii%20Demianchuk%20and%20Mariya%20Lezina%20and%20Fawad%20Mehboob%20and%20Issatay%20Tokmurziyev%20and%20Miguel%20Altamirano%20Cabrera%20and%20Muhammad%20Ahsan%20Mustafa%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20Drones%20operating%20in%20human-occupied%20spaces%20suffer%20from%20insufficient%20communication%20mechanisms%20that%20create%20uncertainty%20about%20their%20intentions.%20We%20present%20HoverAI%2C%20an%20embodied%20aerial%20agent%20that%20integrates%20drone%20mobility%2C%20infrastructure-independent%20visual%20projection%2C%20and%20real-time%20conversational%20AI%20into%20a%20unified%20platform.%20Equipped%20with%20a%20MEMS%20laser%20projector%2C%20onboard%20semi-rigid%20screen%2C%20and%20RGB%20camera%2C%20HoverAI%20perceives%20users%20through%20vision%20and%20voice%2C%20responding%20via%20lip-synced%20avatars%20that%20adapt%20appearance%20to%20user%20demographics.%20The%20system%20employs%20a%20multimodal%20pipeline%20combining%20VAD%2C%20ASR%20%28Whisper%29%2C%20LLM-based%20intent%20classification%2C%20RAG%20for%20dialogue%2C%20face%20analysis%20for%20personalization%2C%20and%20voice%20synthesis%20%28XTTS%20v2%29.%20Evaluation%20demonstrates%20high%20accuracy%20in%20command%20recognition%20%28F1%3A%200.90%29%2C%20demographic%20estimation%20%28gender%20F1%3A%200.89%2C%20age%20MAE%3A%205.14%20years%29%2C%20and%20speech%20transcription%20%28WER%3A%200.181%29.%20By%20uniting%20aerial%20robotics%20with%20adaptive%20conversational%20AI%20and%20self-contained%20visual%20output%2C%20HoverAI%20introduces%20a%20new%20class%20of%20spatially-aware%2C%20socially%20responsive%20embodied%20agents%20for%20applications%20in%20guidance%2C%20assistance%2C%20and%20human-centered%20interaction.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13801v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHoverAI%253A%2520An%2520Embodied%2520Aerial%2520Agent%2520for%2520Natural%2520Human-Drone%2520Interaction%26entry.906535625%3DYuhua%2520Jin%2520and%2520Nikita%2520Kuzmin%2520and%2520Georgii%2520Demianchuk%2520and%2520Mariya%2520Lezina%2520and%2520Fawad%2520Mehboob%2520and%2520Issatay%2520Tokmurziyev%2520and%2520Miguel%2520Altamirano%2520Cabrera%2520and%2520Muhammad%2520Ahsan%2520Mustafa%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3DDrones%2520operating%2520in%2520human-occupied%2520spaces%2520suffer%2520from%2520insufficient%2520communication%2520mechanisms%2520that%2520create%2520uncertainty%2520about%2520their%2520intentions.%2520We%2520present%2520HoverAI%252C%2520an%2520embodied%2520aerial%2520agent%2520that%2520integrates%2520drone%2520mobility%252C%2520infrastructure-independent%2520visual%2520projection%252C%2520and%2520real-time%2520conversational%2520AI%2520into%2520a%2520unified%2520platform.%2520Equipped%2520with%2520a%2520MEMS%2520laser%2520projector%252C%2520onboard%2520semi-rigid%2520screen%252C%2520and%2520RGB%2520camera%252C%2520HoverAI%2520perceives%2520users%2520through%2520vision%2520and%2520voice%252C%2520responding%2520via%2520lip-synced%2520avatars%2520that%2520adapt%2520appearance%2520to%2520user%2520demographics.%2520The%2520system%2520employs%2520a%2520multimodal%2520pipeline%2520combining%2520VAD%252C%2520ASR%2520%2528Whisper%2529%252C%2520LLM-based%2520intent%2520classification%252C%2520RAG%2520for%2520dialogue%252C%2520face%2520analysis%2520for%2520personalization%252C%2520and%2520voice%2520synthesis%2520%2528XTTS%2520v2%2529.%2520Evaluation%2520demonstrates%2520high%2520accuracy%2520in%2520command%2520recognition%2520%2528F1%253A%25200.90%2529%252C%2520demographic%2520estimation%2520%2528gender%2520F1%253A%25200.89%252C%2520age%2520MAE%253A%25205.14%2520years%2529%252C%2520and%2520speech%2520transcription%2520%2528WER%253A%25200.181%2529.%2520By%2520uniting%2520aerial%2520robotics%2520with%2520adaptive%2520conversational%2520AI%2520and%2520self-contained%2520visual%2520output%252C%2520HoverAI%2520introduces%2520a%2520new%2520class%2520of%2520spatially-aware%252C%2520socially%2520responsive%2520embodied%2520agents%2520for%2520applications%2520in%2520guidance%252C%2520assistance%252C%2520and%2520human-centered%2520interaction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13801v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HoverAI%3A%20An%20Embodied%20Aerial%20Agent%20for%20Natural%20Human-Drone%20Interaction&entry.906535625=Yuhua%20Jin%20and%20Nikita%20Kuzmin%20and%20Georgii%20Demianchuk%20and%20Mariya%20Lezina%20and%20Fawad%20Mehboob%20and%20Issatay%20Tokmurziyev%20and%20Miguel%20Altamirano%20Cabrera%20and%20Muhammad%20Ahsan%20Mustafa%20and%20Dzmitry%20Tsetserukou&entry.1292438233=Drones%20operating%20in%20human-occupied%20spaces%20suffer%20from%20insufficient%20communication%20mechanisms%20that%20create%20uncertainty%20about%20their%20intentions.%20We%20present%20HoverAI%2C%20an%20embodied%20aerial%20agent%20that%20integrates%20drone%20mobility%2C%20infrastructure-independent%20visual%20projection%2C%20and%20real-time%20conversational%20AI%20into%20a%20unified%20platform.%20Equipped%20with%20a%20MEMS%20laser%20projector%2C%20onboard%20semi-rigid%20screen%2C%20and%20RGB%20camera%2C%20HoverAI%20perceives%20users%20through%20vision%20and%20voice%2C%20responding%20via%20lip-synced%20avatars%20that%20adapt%20appearance%20to%20user%20demographics.%20The%20system%20employs%20a%20multimodal%20pipeline%20combining%20VAD%2C%20ASR%20%28Whisper%29%2C%20LLM-based%20intent%20classification%2C%20RAG%20for%20dialogue%2C%20face%20analysis%20for%20personalization%2C%20and%20voice%20synthesis%20%28XTTS%20v2%29.%20Evaluation%20demonstrates%20high%20accuracy%20in%20command%20recognition%20%28F1%3A%200.90%29%2C%20demographic%20estimation%20%28gender%20F1%3A%200.89%2C%20age%20MAE%3A%205.14%20years%29%2C%20and%20speech%20transcription%20%28WER%3A%200.181%29.%20By%20uniting%20aerial%20robotics%20with%20adaptive%20conversational%20AI%20and%20self-contained%20visual%20output%2C%20HoverAI%20introduces%20a%20new%20class%20of%20spatially-aware%2C%20socially%20responsive%20embodied%20agents%20for%20applications%20in%20guidance%2C%20assistance%2C%20and%20human-centered%20interaction.&entry.1838667208=http%3A//arxiv.org/abs/2601.13801v1&entry.124074799=Read"},
{"title": "ASBA: A-line State Space Model and B-line Attention for Sparse Optical Doppler Tomography Reconstruction", "author": "Zhenghong Li and Wensheng Cheng and Congwu Du and Yingtian Pan and Zhaozheng Yin and Haibin Ling", "abstract": "Optical Doppler Tomography (ODT) is an emerging blood flow analysis technique. A 2D ODT image (B-scan) is generated by sequentially acquiring 1D depth-resolved raw A-scans (A-line) along the lateral axis (B-line), followed by Doppler phase-subtraction analysis. To ensure high-fidelity B-scan images, current practices rely on dense sampling, which prolongs scanning time, increases storage demands, and limits the capture of rapid blood flow dynamics. Recent studies have explored sparse sampling of raw A-scans to alleviate these limitations, but their effectiveness is hindered by the conservative sampling rates and the uniform modeling of flow and background signals. In this study, we introduce a novel blood flow-aware network, named ASBA (A-line ROI State space model and B-line phase Attention), to reconstruct ODT images from highly sparsely sampled raw A-scans. Specifically, we propose an A-line ROI state space model to extract sparsely distributed flow features along the A-line, and a B-line phase attention to capture long-range flow signals along each B-line based on phase difference. Moreover, we introduce a flow-aware weighted loss function that encourages the network to prioritize the accurate reconstruction of flow signals. Extensive experiments on real animal data demonstrate that the proposed approach clearly outperforms existing state-of-the-art reconstruction methods.", "link": "http://arxiv.org/abs/2601.14165v1", "date": "2026-01-20", "relevancy": 2.1468, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5443}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5412}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5291}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ASBA%3A%20A-line%20State%20Space%20Model%20and%20B-line%20Attention%20for%20Sparse%20Optical%20Doppler%20Tomography%20Reconstruction&body=Title%3A%20ASBA%3A%20A-line%20State%20Space%20Model%20and%20B-line%20Attention%20for%20Sparse%20Optical%20Doppler%20Tomography%20Reconstruction%0AAuthor%3A%20Zhenghong%20Li%20and%20Wensheng%20Cheng%20and%20Congwu%20Du%20and%20Yingtian%20Pan%20and%20Zhaozheng%20Yin%20and%20Haibin%20Ling%0AAbstract%3A%20Optical%20Doppler%20Tomography%20%28ODT%29%20is%20an%20emerging%20blood%20flow%20analysis%20technique.%20A%202D%20ODT%20image%20%28B-scan%29%20is%20generated%20by%20sequentially%20acquiring%201D%20depth-resolved%20raw%20A-scans%20%28A-line%29%20along%20the%20lateral%20axis%20%28B-line%29%2C%20followed%20by%20Doppler%20phase-subtraction%20analysis.%20To%20ensure%20high-fidelity%20B-scan%20images%2C%20current%20practices%20rely%20on%20dense%20sampling%2C%20which%20prolongs%20scanning%20time%2C%20increases%20storage%20demands%2C%20and%20limits%20the%20capture%20of%20rapid%20blood%20flow%20dynamics.%20Recent%20studies%20have%20explored%20sparse%20sampling%20of%20raw%20A-scans%20to%20alleviate%20these%20limitations%2C%20but%20their%20effectiveness%20is%20hindered%20by%20the%20conservative%20sampling%20rates%20and%20the%20uniform%20modeling%20of%20flow%20and%20background%20signals.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20blood%20flow-aware%20network%2C%20named%20ASBA%20%28A-line%20ROI%20State%20space%20model%20and%20B-line%20phase%20Attention%29%2C%20to%20reconstruct%20ODT%20images%20from%20highly%20sparsely%20sampled%20raw%20A-scans.%20Specifically%2C%20we%20propose%20an%20A-line%20ROI%20state%20space%20model%20to%20extract%20sparsely%20distributed%20flow%20features%20along%20the%20A-line%2C%20and%20a%20B-line%20phase%20attention%20to%20capture%20long-range%20flow%20signals%20along%20each%20B-line%20based%20on%20phase%20difference.%20Moreover%2C%20we%20introduce%20a%20flow-aware%20weighted%20loss%20function%20that%20encourages%20the%20network%20to%20prioritize%20the%20accurate%20reconstruction%20of%20flow%20signals.%20Extensive%20experiments%20on%20real%20animal%20data%20demonstrate%20that%20the%20proposed%20approach%20clearly%20outperforms%20existing%20state-of-the-art%20reconstruction%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14165v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DASBA%253A%2520A-line%2520State%2520Space%2520Model%2520and%2520B-line%2520Attention%2520for%2520Sparse%2520Optical%2520Doppler%2520Tomography%2520Reconstruction%26entry.906535625%3DZhenghong%2520Li%2520and%2520Wensheng%2520Cheng%2520and%2520Congwu%2520Du%2520and%2520Yingtian%2520Pan%2520and%2520Zhaozheng%2520Yin%2520and%2520Haibin%2520Ling%26entry.1292438233%3DOptical%2520Doppler%2520Tomography%2520%2528ODT%2529%2520is%2520an%2520emerging%2520blood%2520flow%2520analysis%2520technique.%2520A%25202D%2520ODT%2520image%2520%2528B-scan%2529%2520is%2520generated%2520by%2520sequentially%2520acquiring%25201D%2520depth-resolved%2520raw%2520A-scans%2520%2528A-line%2529%2520along%2520the%2520lateral%2520axis%2520%2528B-line%2529%252C%2520followed%2520by%2520Doppler%2520phase-subtraction%2520analysis.%2520To%2520ensure%2520high-fidelity%2520B-scan%2520images%252C%2520current%2520practices%2520rely%2520on%2520dense%2520sampling%252C%2520which%2520prolongs%2520scanning%2520time%252C%2520increases%2520storage%2520demands%252C%2520and%2520limits%2520the%2520capture%2520of%2520rapid%2520blood%2520flow%2520dynamics.%2520Recent%2520studies%2520have%2520explored%2520sparse%2520sampling%2520of%2520raw%2520A-scans%2520to%2520alleviate%2520these%2520limitations%252C%2520but%2520their%2520effectiveness%2520is%2520hindered%2520by%2520the%2520conservative%2520sampling%2520rates%2520and%2520the%2520uniform%2520modeling%2520of%2520flow%2520and%2520background%2520signals.%2520In%2520this%2520study%252C%2520we%2520introduce%2520a%2520novel%2520blood%2520flow-aware%2520network%252C%2520named%2520ASBA%2520%2528A-line%2520ROI%2520State%2520space%2520model%2520and%2520B-line%2520phase%2520Attention%2529%252C%2520to%2520reconstruct%2520ODT%2520images%2520from%2520highly%2520sparsely%2520sampled%2520raw%2520A-scans.%2520Specifically%252C%2520we%2520propose%2520an%2520A-line%2520ROI%2520state%2520space%2520model%2520to%2520extract%2520sparsely%2520distributed%2520flow%2520features%2520along%2520the%2520A-line%252C%2520and%2520a%2520B-line%2520phase%2520attention%2520to%2520capture%2520long-range%2520flow%2520signals%2520along%2520each%2520B-line%2520based%2520on%2520phase%2520difference.%2520Moreover%252C%2520we%2520introduce%2520a%2520flow-aware%2520weighted%2520loss%2520function%2520that%2520encourages%2520the%2520network%2520to%2520prioritize%2520the%2520accurate%2520reconstruction%2520of%2520flow%2520signals.%2520Extensive%2520experiments%2520on%2520real%2520animal%2520data%2520demonstrate%2520that%2520the%2520proposed%2520approach%2520clearly%2520outperforms%2520existing%2520state-of-the-art%2520reconstruction%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14165v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ASBA%3A%20A-line%20State%20Space%20Model%20and%20B-line%20Attention%20for%20Sparse%20Optical%20Doppler%20Tomography%20Reconstruction&entry.906535625=Zhenghong%20Li%20and%20Wensheng%20Cheng%20and%20Congwu%20Du%20and%20Yingtian%20Pan%20and%20Zhaozheng%20Yin%20and%20Haibin%20Ling&entry.1292438233=Optical%20Doppler%20Tomography%20%28ODT%29%20is%20an%20emerging%20blood%20flow%20analysis%20technique.%20A%202D%20ODT%20image%20%28B-scan%29%20is%20generated%20by%20sequentially%20acquiring%201D%20depth-resolved%20raw%20A-scans%20%28A-line%29%20along%20the%20lateral%20axis%20%28B-line%29%2C%20followed%20by%20Doppler%20phase-subtraction%20analysis.%20To%20ensure%20high-fidelity%20B-scan%20images%2C%20current%20practices%20rely%20on%20dense%20sampling%2C%20which%20prolongs%20scanning%20time%2C%20increases%20storage%20demands%2C%20and%20limits%20the%20capture%20of%20rapid%20blood%20flow%20dynamics.%20Recent%20studies%20have%20explored%20sparse%20sampling%20of%20raw%20A-scans%20to%20alleviate%20these%20limitations%2C%20but%20their%20effectiveness%20is%20hindered%20by%20the%20conservative%20sampling%20rates%20and%20the%20uniform%20modeling%20of%20flow%20and%20background%20signals.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20blood%20flow-aware%20network%2C%20named%20ASBA%20%28A-line%20ROI%20State%20space%20model%20and%20B-line%20phase%20Attention%29%2C%20to%20reconstruct%20ODT%20images%20from%20highly%20sparsely%20sampled%20raw%20A-scans.%20Specifically%2C%20we%20propose%20an%20A-line%20ROI%20state%20space%20model%20to%20extract%20sparsely%20distributed%20flow%20features%20along%20the%20A-line%2C%20and%20a%20B-line%20phase%20attention%20to%20capture%20long-range%20flow%20signals%20along%20each%20B-line%20based%20on%20phase%20difference.%20Moreover%2C%20we%20introduce%20a%20flow-aware%20weighted%20loss%20function%20that%20encourages%20the%20network%20to%20prioritize%20the%20accurate%20reconstruction%20of%20flow%20signals.%20Extensive%20experiments%20on%20real%20animal%20data%20demonstrate%20that%20the%20proposed%20approach%20clearly%20outperforms%20existing%20state-of-the-art%20reconstruction%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.14165v1&entry.124074799=Read"},
{"title": "Riemannian Liquid Spatio-Temporal Graph Network", "author": "Liangsi Lu and Jingchao Wang and Zhaorong Dai and Hanqian Liu and Yang Shi", "abstract": "Liquid Time-Constant networks (LTCs), a type of continuous-time graph neural network, excel at modeling irregularly-sampled dynamics but are fundamentally confined to Euclidean space. This limitation introduces significant geometric distortion when representing real-world graphs with inherent non-Euclidean structures (e.g., hierarchies and cycles), degrading representation quality. To overcome this limitation, we introduce the Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), a framework that unifies continuous-time liquid dynamics with the geometric inductive biases of Riemannian manifolds. RLSTG models graph evolution through an Ordinary Differential Equation (ODE) formulated directly on a curved manifold, enabling it to faithfully capture the intrinsic geometry of both structurally static and dynamic spatio-temporal graphs. Moreover, we provide rigorous theoretical guarantees for RLSTG, extending stability theorems of LTCs to the Riemannian domain and quantifying its expressive power via state trajectory analysis. Extensive experiments on real-world benchmarks demonstrate that, by combining advanced temporal dynamics with a Riemannian spatial representation, RLSTG achieves superior performance on graphs with complex structures. Project Page: https://rlstg.github.io", "link": "http://arxiv.org/abs/2601.14115v1", "date": "2026-01-20", "relevancy": 2.1361, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5437}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5374}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5013}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Riemannian%20Liquid%20Spatio-Temporal%20Graph%20Network&body=Title%3A%20Riemannian%20Liquid%20Spatio-Temporal%20Graph%20Network%0AAuthor%3A%20Liangsi%20Lu%20and%20Jingchao%20Wang%20and%20Zhaorong%20Dai%20and%20Hanqian%20Liu%20and%20Yang%20Shi%0AAbstract%3A%20Liquid%20Time-Constant%20networks%20%28LTCs%29%2C%20a%20type%20of%20continuous-time%20graph%20neural%20network%2C%20excel%20at%20modeling%20irregularly-sampled%20dynamics%20but%20are%20fundamentally%20confined%20to%20Euclidean%20space.%20This%20limitation%20introduces%20significant%20geometric%20distortion%20when%20representing%20real-world%20graphs%20with%20inherent%20non-Euclidean%20structures%20%28e.g.%2C%20hierarchies%20and%20cycles%29%2C%20degrading%20representation%20quality.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20the%20Riemannian%20Liquid%20Spatio-Temporal%20Graph%20Network%20%28RLSTG%29%2C%20a%20framework%20that%20unifies%20continuous-time%20liquid%20dynamics%20with%20the%20geometric%20inductive%20biases%20of%20Riemannian%20manifolds.%20RLSTG%20models%20graph%20evolution%20through%20an%20Ordinary%20Differential%20Equation%20%28ODE%29%20formulated%20directly%20on%20a%20curved%20manifold%2C%20enabling%20it%20to%20faithfully%20capture%20the%20intrinsic%20geometry%20of%20both%20structurally%20static%20and%20dynamic%20spatio-temporal%20graphs.%20Moreover%2C%20we%20provide%20rigorous%20theoretical%20guarantees%20for%20RLSTG%2C%20extending%20stability%20theorems%20of%20LTCs%20to%20the%20Riemannian%20domain%20and%20quantifying%20its%20expressive%20power%20via%20state%20trajectory%20analysis.%20Extensive%20experiments%20on%20real-world%20benchmarks%20demonstrate%20that%2C%20by%20combining%20advanced%20temporal%20dynamics%20with%20a%20Riemannian%20spatial%20representation%2C%20RLSTG%20achieves%20superior%20performance%20on%20graphs%20with%20complex%20structures.%20Project%20Page%3A%20https%3A//rlstg.github.io%0ALink%3A%20http%3A//arxiv.org/abs/2601.14115v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRiemannian%2520Liquid%2520Spatio-Temporal%2520Graph%2520Network%26entry.906535625%3DLiangsi%2520Lu%2520and%2520Jingchao%2520Wang%2520and%2520Zhaorong%2520Dai%2520and%2520Hanqian%2520Liu%2520and%2520Yang%2520Shi%26entry.1292438233%3DLiquid%2520Time-Constant%2520networks%2520%2528LTCs%2529%252C%2520a%2520type%2520of%2520continuous-time%2520graph%2520neural%2520network%252C%2520excel%2520at%2520modeling%2520irregularly-sampled%2520dynamics%2520but%2520are%2520fundamentally%2520confined%2520to%2520Euclidean%2520space.%2520This%2520limitation%2520introduces%2520significant%2520geometric%2520distortion%2520when%2520representing%2520real-world%2520graphs%2520with%2520inherent%2520non-Euclidean%2520structures%2520%2528e.g.%252C%2520hierarchies%2520and%2520cycles%2529%252C%2520degrading%2520representation%2520quality.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520introduce%2520the%2520Riemannian%2520Liquid%2520Spatio-Temporal%2520Graph%2520Network%2520%2528RLSTG%2529%252C%2520a%2520framework%2520that%2520unifies%2520continuous-time%2520liquid%2520dynamics%2520with%2520the%2520geometric%2520inductive%2520biases%2520of%2520Riemannian%2520manifolds.%2520RLSTG%2520models%2520graph%2520evolution%2520through%2520an%2520Ordinary%2520Differential%2520Equation%2520%2528ODE%2529%2520formulated%2520directly%2520on%2520a%2520curved%2520manifold%252C%2520enabling%2520it%2520to%2520faithfully%2520capture%2520the%2520intrinsic%2520geometry%2520of%2520both%2520structurally%2520static%2520and%2520dynamic%2520spatio-temporal%2520graphs.%2520Moreover%252C%2520we%2520provide%2520rigorous%2520theoretical%2520guarantees%2520for%2520RLSTG%252C%2520extending%2520stability%2520theorems%2520of%2520LTCs%2520to%2520the%2520Riemannian%2520domain%2520and%2520quantifying%2520its%2520expressive%2520power%2520via%2520state%2520trajectory%2520analysis.%2520Extensive%2520experiments%2520on%2520real-world%2520benchmarks%2520demonstrate%2520that%252C%2520by%2520combining%2520advanced%2520temporal%2520dynamics%2520with%2520a%2520Riemannian%2520spatial%2520representation%252C%2520RLSTG%2520achieves%2520superior%2520performance%2520on%2520graphs%2520with%2520complex%2520structures.%2520Project%2520Page%253A%2520https%253A//rlstg.github.io%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14115v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Riemannian%20Liquid%20Spatio-Temporal%20Graph%20Network&entry.906535625=Liangsi%20Lu%20and%20Jingchao%20Wang%20and%20Zhaorong%20Dai%20and%20Hanqian%20Liu%20and%20Yang%20Shi&entry.1292438233=Liquid%20Time-Constant%20networks%20%28LTCs%29%2C%20a%20type%20of%20continuous-time%20graph%20neural%20network%2C%20excel%20at%20modeling%20irregularly-sampled%20dynamics%20but%20are%20fundamentally%20confined%20to%20Euclidean%20space.%20This%20limitation%20introduces%20significant%20geometric%20distortion%20when%20representing%20real-world%20graphs%20with%20inherent%20non-Euclidean%20structures%20%28e.g.%2C%20hierarchies%20and%20cycles%29%2C%20degrading%20representation%20quality.%20To%20overcome%20this%20limitation%2C%20we%20introduce%20the%20Riemannian%20Liquid%20Spatio-Temporal%20Graph%20Network%20%28RLSTG%29%2C%20a%20framework%20that%20unifies%20continuous-time%20liquid%20dynamics%20with%20the%20geometric%20inductive%20biases%20of%20Riemannian%20manifolds.%20RLSTG%20models%20graph%20evolution%20through%20an%20Ordinary%20Differential%20Equation%20%28ODE%29%20formulated%20directly%20on%20a%20curved%20manifold%2C%20enabling%20it%20to%20faithfully%20capture%20the%20intrinsic%20geometry%20of%20both%20structurally%20static%20and%20dynamic%20spatio-temporal%20graphs.%20Moreover%2C%20we%20provide%20rigorous%20theoretical%20guarantees%20for%20RLSTG%2C%20extending%20stability%20theorems%20of%20LTCs%20to%20the%20Riemannian%20domain%20and%20quantifying%20its%20expressive%20power%20via%20state%20trajectory%20analysis.%20Extensive%20experiments%20on%20real-world%20benchmarks%20demonstrate%20that%2C%20by%20combining%20advanced%20temporal%20dynamics%20with%20a%20Riemannian%20spatial%20representation%2C%20RLSTG%20achieves%20superior%20performance%20on%20graphs%20with%20complex%20structures.%20Project%20Page%3A%20https%3A//rlstg.github.io&entry.1838667208=http%3A//arxiv.org/abs/2601.14115v1&entry.124074799=Read"},
{"title": "Towards Fast Coarse-graining and Equation Discovery with Foundation Inference Models", "author": "Manuel Hinz and Maximilian Mauel and Patrick Seifner and David Berghaus and Kostadin Cvejoski and Ramses J. Sanchez", "abstract": "High-dimensional recordings of dynamical processes are often characterized by a much smaller set of effective variables, evolving on low-dimensional manifolds. Identifying these latent dynamics requires solving two intertwined problems: discovering appropriate coarse-grained variables and simultaneously fitting the governing equations. Most machine learning approaches tackle these tasks jointly by training autoencoders together with models that enforce dynamical consistency. We propose to decouple the two problems by leveraging the recently introduced Foundation Inference Models (FIMs). FIMs are pretrained models that estimate the infinitesimal generators of dynamical systems (e.g., the drift and diffusion of a stochastic differential equation) in zero-shot mode. By amortizing the inference of the dynamics through a FIM with frozen weights, and training only the encoder-decoder map, we define a simple, simulation-consistent loss that stabilizes representation learning. A proof of concept on a stochastic double-well system with semicircle diffusion, embedded into synthetic video data, illustrates the potential of this approach for fast and reusable coarse-graining pipelines.", "link": "http://arxiv.org/abs/2510.12618v2", "date": "2026-01-20", "relevancy": 2.1355, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5434}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5273}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5269}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Fast%20Coarse-graining%20and%20Equation%20Discovery%20with%20Foundation%20Inference%20Models&body=Title%3A%20Towards%20Fast%20Coarse-graining%20and%20Equation%20Discovery%20with%20Foundation%20Inference%20Models%0AAuthor%3A%20Manuel%20Hinz%20and%20Maximilian%20Mauel%20and%20Patrick%20Seifner%20and%20David%20Berghaus%20and%20Kostadin%20Cvejoski%20and%20Ramses%20J.%20Sanchez%0AAbstract%3A%20High-dimensional%20recordings%20of%20dynamical%20processes%20are%20often%20characterized%20by%20a%20much%20smaller%20set%20of%20effective%20variables%2C%20evolving%20on%20low-dimensional%20manifolds.%20Identifying%20these%20latent%20dynamics%20requires%20solving%20two%20intertwined%20problems%3A%20discovering%20appropriate%20coarse-grained%20variables%20and%20simultaneously%20fitting%20the%20governing%20equations.%20Most%20machine%20learning%20approaches%20tackle%20these%20tasks%20jointly%20by%20training%20autoencoders%20together%20with%20models%20that%20enforce%20dynamical%20consistency.%20We%20propose%20to%20decouple%20the%20two%20problems%20by%20leveraging%20the%20recently%20introduced%20Foundation%20Inference%20Models%20%28FIMs%29.%20FIMs%20are%20pretrained%20models%20that%20estimate%20the%20infinitesimal%20generators%20of%20dynamical%20systems%20%28e.g.%2C%20the%20drift%20and%20diffusion%20of%20a%20stochastic%20differential%20equation%29%20in%20zero-shot%20mode.%20By%20amortizing%20the%20inference%20of%20the%20dynamics%20through%20a%20FIM%20with%20frozen%20weights%2C%20and%20training%20only%20the%20encoder-decoder%20map%2C%20we%20define%20a%20simple%2C%20simulation-consistent%20loss%20that%20stabilizes%20representation%20learning.%20A%20proof%20of%20concept%20on%20a%20stochastic%20double-well%20system%20with%20semicircle%20diffusion%2C%20embedded%20into%20synthetic%20video%20data%2C%20illustrates%20the%20potential%20of%20this%20approach%20for%20fast%20and%20reusable%20coarse-graining%20pipelines.%0ALink%3A%20http%3A//arxiv.org/abs/2510.12618v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Fast%2520Coarse-graining%2520and%2520Equation%2520Discovery%2520with%2520Foundation%2520Inference%2520Models%26entry.906535625%3DManuel%2520Hinz%2520and%2520Maximilian%2520Mauel%2520and%2520Patrick%2520Seifner%2520and%2520David%2520Berghaus%2520and%2520Kostadin%2520Cvejoski%2520and%2520Ramses%2520J.%2520Sanchez%26entry.1292438233%3DHigh-dimensional%2520recordings%2520of%2520dynamical%2520processes%2520are%2520often%2520characterized%2520by%2520a%2520much%2520smaller%2520set%2520of%2520effective%2520variables%252C%2520evolving%2520on%2520low-dimensional%2520manifolds.%2520Identifying%2520these%2520latent%2520dynamics%2520requires%2520solving%2520two%2520intertwined%2520problems%253A%2520discovering%2520appropriate%2520coarse-grained%2520variables%2520and%2520simultaneously%2520fitting%2520the%2520governing%2520equations.%2520Most%2520machine%2520learning%2520approaches%2520tackle%2520these%2520tasks%2520jointly%2520by%2520training%2520autoencoders%2520together%2520with%2520models%2520that%2520enforce%2520dynamical%2520consistency.%2520We%2520propose%2520to%2520decouple%2520the%2520two%2520problems%2520by%2520leveraging%2520the%2520recently%2520introduced%2520Foundation%2520Inference%2520Models%2520%2528FIMs%2529.%2520FIMs%2520are%2520pretrained%2520models%2520that%2520estimate%2520the%2520infinitesimal%2520generators%2520of%2520dynamical%2520systems%2520%2528e.g.%252C%2520the%2520drift%2520and%2520diffusion%2520of%2520a%2520stochastic%2520differential%2520equation%2529%2520in%2520zero-shot%2520mode.%2520By%2520amortizing%2520the%2520inference%2520of%2520the%2520dynamics%2520through%2520a%2520FIM%2520with%2520frozen%2520weights%252C%2520and%2520training%2520only%2520the%2520encoder-decoder%2520map%252C%2520we%2520define%2520a%2520simple%252C%2520simulation-consistent%2520loss%2520that%2520stabilizes%2520representation%2520learning.%2520A%2520proof%2520of%2520concept%2520on%2520a%2520stochastic%2520double-well%2520system%2520with%2520semicircle%2520diffusion%252C%2520embedded%2520into%2520synthetic%2520video%2520data%252C%2520illustrates%2520the%2520potential%2520of%2520this%2520approach%2520for%2520fast%2520and%2520reusable%2520coarse-graining%2520pipelines.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.12618v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Fast%20Coarse-graining%20and%20Equation%20Discovery%20with%20Foundation%20Inference%20Models&entry.906535625=Manuel%20Hinz%20and%20Maximilian%20Mauel%20and%20Patrick%20Seifner%20and%20David%20Berghaus%20and%20Kostadin%20Cvejoski%20and%20Ramses%20J.%20Sanchez&entry.1292438233=High-dimensional%20recordings%20of%20dynamical%20processes%20are%20often%20characterized%20by%20a%20much%20smaller%20set%20of%20effective%20variables%2C%20evolving%20on%20low-dimensional%20manifolds.%20Identifying%20these%20latent%20dynamics%20requires%20solving%20two%20intertwined%20problems%3A%20discovering%20appropriate%20coarse-grained%20variables%20and%20simultaneously%20fitting%20the%20governing%20equations.%20Most%20machine%20learning%20approaches%20tackle%20these%20tasks%20jointly%20by%20training%20autoencoders%20together%20with%20models%20that%20enforce%20dynamical%20consistency.%20We%20propose%20to%20decouple%20the%20two%20problems%20by%20leveraging%20the%20recently%20introduced%20Foundation%20Inference%20Models%20%28FIMs%29.%20FIMs%20are%20pretrained%20models%20that%20estimate%20the%20infinitesimal%20generators%20of%20dynamical%20systems%20%28e.g.%2C%20the%20drift%20and%20diffusion%20of%20a%20stochastic%20differential%20equation%29%20in%20zero-shot%20mode.%20By%20amortizing%20the%20inference%20of%20the%20dynamics%20through%20a%20FIM%20with%20frozen%20weights%2C%20and%20training%20only%20the%20encoder-decoder%20map%2C%20we%20define%20a%20simple%2C%20simulation-consistent%20loss%20that%20stabilizes%20representation%20learning.%20A%20proof%20of%20concept%20on%20a%20stochastic%20double-well%20system%20with%20semicircle%20diffusion%2C%20embedded%20into%20synthetic%20video%20data%2C%20illustrates%20the%20potential%20of%20this%20approach%20for%20fast%20and%20reusable%20coarse-graining%20pipelines.&entry.1838667208=http%3A//arxiv.org/abs/2510.12618v2&entry.124074799=Read"},
{"title": "RL-BioAug: Label-Efficient Reinforcement Learning for Self-Supervised EEG Representation Learning", "author": "Cheol-Hui Lee and Hwa-Yeon Lee and Dong-Joo Kim", "abstract": "The quality of data augmentation serves as a critical determinant for the performance of contrastive learning in EEG tasks. Although this paradigm is promising for utilizing unlabeled data, static or random augmentation strategies often fail to preserve intrinsic information due to the non-stationarity of EEG signals where statistical properties change over time. To address this, we propose RL-BioAug, a framework that leverages a label-efficient reinforcement learning (RL) agent to autonomously determine optimal augmentation policies. While utilizing only a minimal fraction (10\\%) of labeled data to guide the agent's policy, our method enables the encoder to learn robust representations in a strictly self-supervised manner. Experimental results demonstrate that RL-BioAug significantly outperforms the random selection strategy, achieving substantial improvements of 9.69\\% and 8.80\\% in Macro-F1 score on the Sleep-EDFX and CHB-MIT datasets, respectively. Notably, this agent mainly chose optimal strategies for each task -- for example, Time Masking with a 62\\% probability for sleep stage classification and Crop \\& Resize with a 77\\% probability for seizure detection. Our framework suggests its potential to replace conventional heuristic-based augmentations and establish a new autonomous paradigm for data augmentation. The source code is available at \\href{https://github.com/dlcjfgmlnasa/RL-BioAug}{https://github.com/dlcjfgmlnasa/RL-BioAug}.", "link": "http://arxiv.org/abs/2601.13964v1", "date": "2026-01-20", "relevancy": 2.117, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5381}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5344}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5183}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RL-BioAug%3A%20Label-Efficient%20Reinforcement%20Learning%20for%20Self-Supervised%20EEG%20Representation%20Learning&body=Title%3A%20RL-BioAug%3A%20Label-Efficient%20Reinforcement%20Learning%20for%20Self-Supervised%20EEG%20Representation%20Learning%0AAuthor%3A%20Cheol-Hui%20Lee%20and%20Hwa-Yeon%20Lee%20and%20Dong-Joo%20Kim%0AAbstract%3A%20The%20quality%20of%20data%20augmentation%20serves%20as%20a%20critical%20determinant%20for%20the%20performance%20of%20contrastive%20learning%20in%20EEG%20tasks.%20Although%20this%20paradigm%20is%20promising%20for%20utilizing%20unlabeled%20data%2C%20static%20or%20random%20augmentation%20strategies%20often%20fail%20to%20preserve%20intrinsic%20information%20due%20to%20the%20non-stationarity%20of%20EEG%20signals%20where%20statistical%20properties%20change%20over%20time.%20To%20address%20this%2C%20we%20propose%20RL-BioAug%2C%20a%20framework%20that%20leverages%20a%20label-efficient%20reinforcement%20learning%20%28RL%29%20agent%20to%20autonomously%20determine%20optimal%20augmentation%20policies.%20While%20utilizing%20only%20a%20minimal%20fraction%20%2810%5C%25%29%20of%20labeled%20data%20to%20guide%20the%20agent%27s%20policy%2C%20our%20method%20enables%20the%20encoder%20to%20learn%20robust%20representations%20in%20a%20strictly%20self-supervised%20manner.%20Experimental%20results%20demonstrate%20that%20RL-BioAug%20significantly%20outperforms%20the%20random%20selection%20strategy%2C%20achieving%20substantial%20improvements%20of%209.69%5C%25%20and%208.80%5C%25%20in%20Macro-F1%20score%20on%20the%20Sleep-EDFX%20and%20CHB-MIT%20datasets%2C%20respectively.%20Notably%2C%20this%20agent%20mainly%20chose%20optimal%20strategies%20for%20each%20task%20--%20for%20example%2C%20Time%20Masking%20with%20a%2062%5C%25%20probability%20for%20sleep%20stage%20classification%20and%20Crop%20%5C%26%20Resize%20with%20a%2077%5C%25%20probability%20for%20seizure%20detection.%20Our%20framework%20suggests%20its%20potential%20to%20replace%20conventional%20heuristic-based%20augmentations%20and%20establish%20a%20new%20autonomous%20paradigm%20for%20data%20augmentation.%20The%20source%20code%20is%20available%20at%20%5Chref%7Bhttps%3A//github.com/dlcjfgmlnasa/RL-BioAug%7D%7Bhttps%3A//github.com/dlcjfgmlnasa/RL-BioAug%7D.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13964v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRL-BioAug%253A%2520Label-Efficient%2520Reinforcement%2520Learning%2520for%2520Self-Supervised%2520EEG%2520Representation%2520Learning%26entry.906535625%3DCheol-Hui%2520Lee%2520and%2520Hwa-Yeon%2520Lee%2520and%2520Dong-Joo%2520Kim%26entry.1292438233%3DThe%2520quality%2520of%2520data%2520augmentation%2520serves%2520as%2520a%2520critical%2520determinant%2520for%2520the%2520performance%2520of%2520contrastive%2520learning%2520in%2520EEG%2520tasks.%2520Although%2520this%2520paradigm%2520is%2520promising%2520for%2520utilizing%2520unlabeled%2520data%252C%2520static%2520or%2520random%2520augmentation%2520strategies%2520often%2520fail%2520to%2520preserve%2520intrinsic%2520information%2520due%2520to%2520the%2520non-stationarity%2520of%2520EEG%2520signals%2520where%2520statistical%2520properties%2520change%2520over%2520time.%2520To%2520address%2520this%252C%2520we%2520propose%2520RL-BioAug%252C%2520a%2520framework%2520that%2520leverages%2520a%2520label-efficient%2520reinforcement%2520learning%2520%2528RL%2529%2520agent%2520to%2520autonomously%2520determine%2520optimal%2520augmentation%2520policies.%2520While%2520utilizing%2520only%2520a%2520minimal%2520fraction%2520%252810%255C%2525%2529%2520of%2520labeled%2520data%2520to%2520guide%2520the%2520agent%2527s%2520policy%252C%2520our%2520method%2520enables%2520the%2520encoder%2520to%2520learn%2520robust%2520representations%2520in%2520a%2520strictly%2520self-supervised%2520manner.%2520Experimental%2520results%2520demonstrate%2520that%2520RL-BioAug%2520significantly%2520outperforms%2520the%2520random%2520selection%2520strategy%252C%2520achieving%2520substantial%2520improvements%2520of%25209.69%255C%2525%2520and%25208.80%255C%2525%2520in%2520Macro-F1%2520score%2520on%2520the%2520Sleep-EDFX%2520and%2520CHB-MIT%2520datasets%252C%2520respectively.%2520Notably%252C%2520this%2520agent%2520mainly%2520chose%2520optimal%2520strategies%2520for%2520each%2520task%2520--%2520for%2520example%252C%2520Time%2520Masking%2520with%2520a%252062%255C%2525%2520probability%2520for%2520sleep%2520stage%2520classification%2520and%2520Crop%2520%255C%2526%2520Resize%2520with%2520a%252077%255C%2525%2520probability%2520for%2520seizure%2520detection.%2520Our%2520framework%2520suggests%2520its%2520potential%2520to%2520replace%2520conventional%2520heuristic-based%2520augmentations%2520and%2520establish%2520a%2520new%2520autonomous%2520paradigm%2520for%2520data%2520augmentation.%2520The%2520source%2520code%2520is%2520available%2520at%2520%255Chref%257Bhttps%253A//github.com/dlcjfgmlnasa/RL-BioAug%257D%257Bhttps%253A//github.com/dlcjfgmlnasa/RL-BioAug%257D.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13964v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RL-BioAug%3A%20Label-Efficient%20Reinforcement%20Learning%20for%20Self-Supervised%20EEG%20Representation%20Learning&entry.906535625=Cheol-Hui%20Lee%20and%20Hwa-Yeon%20Lee%20and%20Dong-Joo%20Kim&entry.1292438233=The%20quality%20of%20data%20augmentation%20serves%20as%20a%20critical%20determinant%20for%20the%20performance%20of%20contrastive%20learning%20in%20EEG%20tasks.%20Although%20this%20paradigm%20is%20promising%20for%20utilizing%20unlabeled%20data%2C%20static%20or%20random%20augmentation%20strategies%20often%20fail%20to%20preserve%20intrinsic%20information%20due%20to%20the%20non-stationarity%20of%20EEG%20signals%20where%20statistical%20properties%20change%20over%20time.%20To%20address%20this%2C%20we%20propose%20RL-BioAug%2C%20a%20framework%20that%20leverages%20a%20label-efficient%20reinforcement%20learning%20%28RL%29%20agent%20to%20autonomously%20determine%20optimal%20augmentation%20policies.%20While%20utilizing%20only%20a%20minimal%20fraction%20%2810%5C%25%29%20of%20labeled%20data%20to%20guide%20the%20agent%27s%20policy%2C%20our%20method%20enables%20the%20encoder%20to%20learn%20robust%20representations%20in%20a%20strictly%20self-supervised%20manner.%20Experimental%20results%20demonstrate%20that%20RL-BioAug%20significantly%20outperforms%20the%20random%20selection%20strategy%2C%20achieving%20substantial%20improvements%20of%209.69%5C%25%20and%208.80%5C%25%20in%20Macro-F1%20score%20on%20the%20Sleep-EDFX%20and%20CHB-MIT%20datasets%2C%20respectively.%20Notably%2C%20this%20agent%20mainly%20chose%20optimal%20strategies%20for%20each%20task%20--%20for%20example%2C%20Time%20Masking%20with%20a%2062%5C%25%20probability%20for%20sleep%20stage%20classification%20and%20Crop%20%5C%26%20Resize%20with%20a%2077%5C%25%20probability%20for%20seizure%20detection.%20Our%20framework%20suggests%20its%20potential%20to%20replace%20conventional%20heuristic-based%20augmentations%20and%20establish%20a%20new%20autonomous%20paradigm%20for%20data%20augmentation.%20The%20source%20code%20is%20available%20at%20%5Chref%7Bhttps%3A//github.com/dlcjfgmlnasa/RL-BioAug%7D%7Bhttps%3A//github.com/dlcjfgmlnasa/RL-BioAug%7D.&entry.1838667208=http%3A//arxiv.org/abs/2601.13964v1&entry.124074799=Read"},
{"title": "Progressive self-supervised blind-spot denoising method for LDCT denoising", "author": "Yichao Liu and Yueyang Teng and Junwen Guo", "abstract": "Self-supervised learning is increasingly investigated for low-dose computed tomography (LDCT) image denoising, as it alleviates the dependence on paired normal-dose CT (NDCT) data, which are often difficult to acquire in clinical practice. In this paper, we propose a novel self-supervised training strategy that relies exclusively on LDCT images. We introduce a step-wise blind-spot denoising mechanism that enforces conditional independence in a progressive manner, enabling more fine-grained denoising learning. In addition, we add Gaussian noise to LDCT images, which acts as a regularization and mitigates overfitting. Extensive experiments on the Mayo LDCT dataset demonstrate that the proposed method consistently outperforms existing self-supervised approaches and achieves performance comparable to, or better than, several representative supervised denoising methods.", "link": "http://arxiv.org/abs/2601.14180v1", "date": "2026-01-20", "relevancy": 2.1072, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5362}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5204}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20self-supervised%20blind-spot%20denoising%20method%20for%20LDCT%20denoising&body=Title%3A%20Progressive%20self-supervised%20blind-spot%20denoising%20method%20for%20LDCT%20denoising%0AAuthor%3A%20Yichao%20Liu%20and%20Yueyang%20Teng%20and%20Junwen%20Guo%0AAbstract%3A%20Self-supervised%20learning%20is%20increasingly%20investigated%20for%20low-dose%20computed%20tomography%20%28LDCT%29%20image%20denoising%2C%20as%20it%20alleviates%20the%20dependence%20on%20paired%20normal-dose%20CT%20%28NDCT%29%20data%2C%20which%20are%20often%20difficult%20to%20acquire%20in%20clinical%20practice.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20self-supervised%20training%20strategy%20that%20relies%20exclusively%20on%20LDCT%20images.%20We%20introduce%20a%20step-wise%20blind-spot%20denoising%20mechanism%20that%20enforces%20conditional%20independence%20in%20a%20progressive%20manner%2C%20enabling%20more%20fine-grained%20denoising%20learning.%20In%20addition%2C%20we%20add%20Gaussian%20noise%20to%20LDCT%20images%2C%20which%20acts%20as%20a%20regularization%20and%20mitigates%20overfitting.%20Extensive%20experiments%20on%20the%20Mayo%20LDCT%20dataset%20demonstrate%20that%20the%20proposed%20method%20consistently%20outperforms%20existing%20self-supervised%20approaches%20and%20achieves%20performance%20comparable%20to%2C%20or%20better%20than%2C%20several%20representative%20supervised%20denoising%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14180v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520self-supervised%2520blind-spot%2520denoising%2520method%2520for%2520LDCT%2520denoising%26entry.906535625%3DYichao%2520Liu%2520and%2520Yueyang%2520Teng%2520and%2520Junwen%2520Guo%26entry.1292438233%3DSelf-supervised%2520learning%2520is%2520increasingly%2520investigated%2520for%2520low-dose%2520computed%2520tomography%2520%2528LDCT%2529%2520image%2520denoising%252C%2520as%2520it%2520alleviates%2520the%2520dependence%2520on%2520paired%2520normal-dose%2520CT%2520%2528NDCT%2529%2520data%252C%2520which%2520are%2520often%2520difficult%2520to%2520acquire%2520in%2520clinical%2520practice.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520self-supervised%2520training%2520strategy%2520that%2520relies%2520exclusively%2520on%2520LDCT%2520images.%2520We%2520introduce%2520a%2520step-wise%2520blind-spot%2520denoising%2520mechanism%2520that%2520enforces%2520conditional%2520independence%2520in%2520a%2520progressive%2520manner%252C%2520enabling%2520more%2520fine-grained%2520denoising%2520learning.%2520In%2520addition%252C%2520we%2520add%2520Gaussian%2520noise%2520to%2520LDCT%2520images%252C%2520which%2520acts%2520as%2520a%2520regularization%2520and%2520mitigates%2520overfitting.%2520Extensive%2520experiments%2520on%2520the%2520Mayo%2520LDCT%2520dataset%2520demonstrate%2520that%2520the%2520proposed%2520method%2520consistently%2520outperforms%2520existing%2520self-supervised%2520approaches%2520and%2520achieves%2520performance%2520comparable%2520to%252C%2520or%2520better%2520than%252C%2520several%2520representative%2520supervised%2520denoising%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14180v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20self-supervised%20blind-spot%20denoising%20method%20for%20LDCT%20denoising&entry.906535625=Yichao%20Liu%20and%20Yueyang%20Teng%20and%20Junwen%20Guo&entry.1292438233=Self-supervised%20learning%20is%20increasingly%20investigated%20for%20low-dose%20computed%20tomography%20%28LDCT%29%20image%20denoising%2C%20as%20it%20alleviates%20the%20dependence%20on%20paired%20normal-dose%20CT%20%28NDCT%29%20data%2C%20which%20are%20often%20difficult%20to%20acquire%20in%20clinical%20practice.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20self-supervised%20training%20strategy%20that%20relies%20exclusively%20on%20LDCT%20images.%20We%20introduce%20a%20step-wise%20blind-spot%20denoising%20mechanism%20that%20enforces%20conditional%20independence%20in%20a%20progressive%20manner%2C%20enabling%20more%20fine-grained%20denoising%20learning.%20In%20addition%2C%20we%20add%20Gaussian%20noise%20to%20LDCT%20images%2C%20which%20acts%20as%20a%20regularization%20and%20mitigates%20overfitting.%20Extensive%20experiments%20on%20the%20Mayo%20LDCT%20dataset%20demonstrate%20that%20the%20proposed%20method%20consistently%20outperforms%20existing%20self-supervised%20approaches%20and%20achieves%20performance%20comparable%20to%2C%20or%20better%20than%2C%20several%20representative%20supervised%20denoising%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2601.14180v1&entry.124074799=Read"},
{"title": "LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR", "author": "Said Taghadouini and Adrien Cavaill\u00e8s and Baptiste Aubertin", "abstract": "We present \\textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \\textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.", "link": "http://arxiv.org/abs/2601.14251v1", "date": "2026-01-20", "relevancy": 2.1066, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5323}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4984}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LightOnOCR%3A%20A%201B%20End-to-End%20Multilingual%20Vision-Language%20Model%20for%20State-of-the-Art%20OCR&body=Title%3A%20LightOnOCR%3A%20A%201B%20End-to-End%20Multilingual%20Vision-Language%20Model%20for%20State-of-the-Art%20OCR%0AAuthor%3A%20Said%20Taghadouini%20and%20Adrien%20Cavaill%C3%A8s%20and%20Baptiste%20Aubertin%0AAbstract%3A%20We%20present%20%5Ctextbf%7BLightOnOCR-2-1B%7D%2C%20a%201B-parameter%20end-to-end%20multilingual%20vision--language%20model%20that%20converts%20document%20images%20%28e.g.%2C%20PDFs%29%20into%20clean%2C%20naturally%20ordered%20text%20without%20brittle%20OCR%20pipelines.%20Trained%20on%20a%20large-scale%2C%20high-quality%20distillation%20mix%20with%20strong%20coverage%20of%20scans%2C%20French%20documents%2C%20and%20scientific%20PDFs%2C%20LightOnOCR-2%20achieves%20state-of-the-art%20results%20on%20OlmOCR-Bench%20while%20being%209%24%5Ctimes%24%20smaller%20and%20substantially%20faster%20than%20prior%20best-performing%20models.%20We%20further%20extend%20the%20output%20format%20to%20predict%20normalized%20bounding%20boxes%20for%20embedded%20images%2C%20introducing%20localization%20during%20pretraining%20via%20a%20resume%20strategy%20and%20refining%20it%20with%20RLVR%20using%20IoU-based%20rewards.%20Finally%2C%20we%20improve%20robustness%20with%20checkpoint%20averaging%20and%20task-arithmetic%20merging.%20We%20release%20model%20checkpoints%20under%20Apache%202.0%2C%20and%20publicly%20release%20the%20dataset%20and%20%5Ctextbf%7BLightOnOCR-bbox-bench%7D%20evaluation%20under%20their%20respective%20licenses.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14251v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLightOnOCR%253A%2520A%25201B%2520End-to-End%2520Multilingual%2520Vision-Language%2520Model%2520for%2520State-of-the-Art%2520OCR%26entry.906535625%3DSaid%2520Taghadouini%2520and%2520Adrien%2520Cavaill%25C3%25A8s%2520and%2520Baptiste%2520Aubertin%26entry.1292438233%3DWe%2520present%2520%255Ctextbf%257BLightOnOCR-2-1B%257D%252C%2520a%25201B-parameter%2520end-to-end%2520multilingual%2520vision--language%2520model%2520that%2520converts%2520document%2520images%2520%2528e.g.%252C%2520PDFs%2529%2520into%2520clean%252C%2520naturally%2520ordered%2520text%2520without%2520brittle%2520OCR%2520pipelines.%2520Trained%2520on%2520a%2520large-scale%252C%2520high-quality%2520distillation%2520mix%2520with%2520strong%2520coverage%2520of%2520scans%252C%2520French%2520documents%252C%2520and%2520scientific%2520PDFs%252C%2520LightOnOCR-2%2520achieves%2520state-of-the-art%2520results%2520on%2520OlmOCR-Bench%2520while%2520being%25209%2524%255Ctimes%2524%2520smaller%2520and%2520substantially%2520faster%2520than%2520prior%2520best-performing%2520models.%2520We%2520further%2520extend%2520the%2520output%2520format%2520to%2520predict%2520normalized%2520bounding%2520boxes%2520for%2520embedded%2520images%252C%2520introducing%2520localization%2520during%2520pretraining%2520via%2520a%2520resume%2520strategy%2520and%2520refining%2520it%2520with%2520RLVR%2520using%2520IoU-based%2520rewards.%2520Finally%252C%2520we%2520improve%2520robustness%2520with%2520checkpoint%2520averaging%2520and%2520task-arithmetic%2520merging.%2520We%2520release%2520model%2520checkpoints%2520under%2520Apache%25202.0%252C%2520and%2520publicly%2520release%2520the%2520dataset%2520and%2520%255Ctextbf%257BLightOnOCR-bbox-bench%257D%2520evaluation%2520under%2520their%2520respective%2520licenses.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14251v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LightOnOCR%3A%20A%201B%20End-to-End%20Multilingual%20Vision-Language%20Model%20for%20State-of-the-Art%20OCR&entry.906535625=Said%20Taghadouini%20and%20Adrien%20Cavaill%C3%A8s%20and%20Baptiste%20Aubertin&entry.1292438233=We%20present%20%5Ctextbf%7BLightOnOCR-2-1B%7D%2C%20a%201B-parameter%20end-to-end%20multilingual%20vision--language%20model%20that%20converts%20document%20images%20%28e.g.%2C%20PDFs%29%20into%20clean%2C%20naturally%20ordered%20text%20without%20brittle%20OCR%20pipelines.%20Trained%20on%20a%20large-scale%2C%20high-quality%20distillation%20mix%20with%20strong%20coverage%20of%20scans%2C%20French%20documents%2C%20and%20scientific%20PDFs%2C%20LightOnOCR-2%20achieves%20state-of-the-art%20results%20on%20OlmOCR-Bench%20while%20being%209%24%5Ctimes%24%20smaller%20and%20substantially%20faster%20than%20prior%20best-performing%20models.%20We%20further%20extend%20the%20output%20format%20to%20predict%20normalized%20bounding%20boxes%20for%20embedded%20images%2C%20introducing%20localization%20during%20pretraining%20via%20a%20resume%20strategy%20and%20refining%20it%20with%20RLVR%20using%20IoU-based%20rewards.%20Finally%2C%20we%20improve%20robustness%20with%20checkpoint%20averaging%20and%20task-arithmetic%20merging.%20We%20release%20model%20checkpoints%20under%20Apache%202.0%2C%20and%20publicly%20release%20the%20dataset%20and%20%5Ctextbf%7BLightOnOCR-bbox-bench%7D%20evaluation%20under%20their%20respective%20licenses.&entry.1838667208=http%3A//arxiv.org/abs/2601.14251v1&entry.124074799=Read"},
{"title": "BikeActions: An Open Platform and Benchmark for Cyclist-Centric VRU Action Recognition", "author": "Max A. Buettner and Kanak Mazumder and Luca Koecher and Mario Finkbeiner and Sebastian Niebler and Fabian B. Flohr", "abstract": "Anticipating the intentions of Vulnerable Road Users (VRUs) is a critical challenge for safe autonomous driving (AD) and mobile robotics. While current research predominantly focuses on pedestrian crossing behaviors from a vehicle's perspective, interactions within dense shared spaces remain underexplored. To bridge this gap, we introduce FUSE-Bike, the first fully open perception platform of its kind. Equipped with two LiDARs, a camera, and GNSS, it facilitates high-fidelity, close-range data capture directly from a cyclist's viewpoint. Leveraging this platform, we present BikeActions, a novel multi-modal dataset comprising 852 annotated samples across 5 distinct action classes, specifically tailored to improve VRU behavior modeling. We establish a rigorous benchmark by evaluating state-of-the-art graph convolution and transformer-based models on our publicly released data splits, establishing the first performance baselines for this challenging task. We release the full dataset together with data curation tools, the open hardware design, and the benchmark code to foster future research in VRU action understanding under https://iv.ee.hm.edu/bikeactions/.", "link": "http://arxiv.org/abs/2601.10521v2", "date": "2026-01-20", "relevancy": 2.1008, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5383}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.536}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5092}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BikeActions%3A%20An%20Open%20Platform%20and%20Benchmark%20for%20Cyclist-Centric%20VRU%20Action%20Recognition&body=Title%3A%20BikeActions%3A%20An%20Open%20Platform%20and%20Benchmark%20for%20Cyclist-Centric%20VRU%20Action%20Recognition%0AAuthor%3A%20Max%20A.%20Buettner%20and%20Kanak%20Mazumder%20and%20Luca%20Koecher%20and%20Mario%20Finkbeiner%20and%20Sebastian%20Niebler%20and%20Fabian%20B.%20Flohr%0AAbstract%3A%20Anticipating%20the%20intentions%20of%20Vulnerable%20Road%20Users%20%28VRUs%29%20is%20a%20critical%20challenge%20for%20safe%20autonomous%20driving%20%28AD%29%20and%20mobile%20robotics.%20While%20current%20research%20predominantly%20focuses%20on%20pedestrian%20crossing%20behaviors%20from%20a%20vehicle%27s%20perspective%2C%20interactions%20within%20dense%20shared%20spaces%20remain%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20introduce%20FUSE-Bike%2C%20the%20first%20fully%20open%20perception%20platform%20of%20its%20kind.%20Equipped%20with%20two%20LiDARs%2C%20a%20camera%2C%20and%20GNSS%2C%20it%20facilitates%20high-fidelity%2C%20close-range%20data%20capture%20directly%20from%20a%20cyclist%27s%20viewpoint.%20Leveraging%20this%20platform%2C%20we%20present%20BikeActions%2C%20a%20novel%20multi-modal%20dataset%20comprising%20852%20annotated%20samples%20across%205%20distinct%20action%20classes%2C%20specifically%20tailored%20to%20improve%20VRU%20behavior%20modeling.%20We%20establish%20a%20rigorous%20benchmark%20by%20evaluating%20state-of-the-art%20graph%20convolution%20and%20transformer-based%20models%20on%20our%20publicly%20released%20data%20splits%2C%20establishing%20the%20first%20performance%20baselines%20for%20this%20challenging%20task.%20We%20release%20the%20full%20dataset%20together%20with%20data%20curation%20tools%2C%20the%20open%20hardware%20design%2C%20and%20the%20benchmark%20code%20to%20foster%20future%20research%20in%20VRU%20action%20understanding%20under%20https%3A//iv.ee.hm.edu/bikeactions/.%0ALink%3A%20http%3A//arxiv.org/abs/2601.10521v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBikeActions%253A%2520An%2520Open%2520Platform%2520and%2520Benchmark%2520for%2520Cyclist-Centric%2520VRU%2520Action%2520Recognition%26entry.906535625%3DMax%2520A.%2520Buettner%2520and%2520Kanak%2520Mazumder%2520and%2520Luca%2520Koecher%2520and%2520Mario%2520Finkbeiner%2520and%2520Sebastian%2520Niebler%2520and%2520Fabian%2520B.%2520Flohr%26entry.1292438233%3DAnticipating%2520the%2520intentions%2520of%2520Vulnerable%2520Road%2520Users%2520%2528VRUs%2529%2520is%2520a%2520critical%2520challenge%2520for%2520safe%2520autonomous%2520driving%2520%2528AD%2529%2520and%2520mobile%2520robotics.%2520While%2520current%2520research%2520predominantly%2520focuses%2520on%2520pedestrian%2520crossing%2520behaviors%2520from%2520a%2520vehicle%2527s%2520perspective%252C%2520interactions%2520within%2520dense%2520shared%2520spaces%2520remain%2520underexplored.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520FUSE-Bike%252C%2520the%2520first%2520fully%2520open%2520perception%2520platform%2520of%2520its%2520kind.%2520Equipped%2520with%2520two%2520LiDARs%252C%2520a%2520camera%252C%2520and%2520GNSS%252C%2520it%2520facilitates%2520high-fidelity%252C%2520close-range%2520data%2520capture%2520directly%2520from%2520a%2520cyclist%2527s%2520viewpoint.%2520Leveraging%2520this%2520platform%252C%2520we%2520present%2520BikeActions%252C%2520a%2520novel%2520multi-modal%2520dataset%2520comprising%2520852%2520annotated%2520samples%2520across%25205%2520distinct%2520action%2520classes%252C%2520specifically%2520tailored%2520to%2520improve%2520VRU%2520behavior%2520modeling.%2520We%2520establish%2520a%2520rigorous%2520benchmark%2520by%2520evaluating%2520state-of-the-art%2520graph%2520convolution%2520and%2520transformer-based%2520models%2520on%2520our%2520publicly%2520released%2520data%2520splits%252C%2520establishing%2520the%2520first%2520performance%2520baselines%2520for%2520this%2520challenging%2520task.%2520We%2520release%2520the%2520full%2520dataset%2520together%2520with%2520data%2520curation%2520tools%252C%2520the%2520open%2520hardware%2520design%252C%2520and%2520the%2520benchmark%2520code%2520to%2520foster%2520future%2520research%2520in%2520VRU%2520action%2520understanding%2520under%2520https%253A//iv.ee.hm.edu/bikeactions/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.10521v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BikeActions%3A%20An%20Open%20Platform%20and%20Benchmark%20for%20Cyclist-Centric%20VRU%20Action%20Recognition&entry.906535625=Max%20A.%20Buettner%20and%20Kanak%20Mazumder%20and%20Luca%20Koecher%20and%20Mario%20Finkbeiner%20and%20Sebastian%20Niebler%20and%20Fabian%20B.%20Flohr&entry.1292438233=Anticipating%20the%20intentions%20of%20Vulnerable%20Road%20Users%20%28VRUs%29%20is%20a%20critical%20challenge%20for%20safe%20autonomous%20driving%20%28AD%29%20and%20mobile%20robotics.%20While%20current%20research%20predominantly%20focuses%20on%20pedestrian%20crossing%20behaviors%20from%20a%20vehicle%27s%20perspective%2C%20interactions%20within%20dense%20shared%20spaces%20remain%20underexplored.%20To%20bridge%20this%20gap%2C%20we%20introduce%20FUSE-Bike%2C%20the%20first%20fully%20open%20perception%20platform%20of%20its%20kind.%20Equipped%20with%20two%20LiDARs%2C%20a%20camera%2C%20and%20GNSS%2C%20it%20facilitates%20high-fidelity%2C%20close-range%20data%20capture%20directly%20from%20a%20cyclist%27s%20viewpoint.%20Leveraging%20this%20platform%2C%20we%20present%20BikeActions%2C%20a%20novel%20multi-modal%20dataset%20comprising%20852%20annotated%20samples%20across%205%20distinct%20action%20classes%2C%20specifically%20tailored%20to%20improve%20VRU%20behavior%20modeling.%20We%20establish%20a%20rigorous%20benchmark%20by%20evaluating%20state-of-the-art%20graph%20convolution%20and%20transformer-based%20models%20on%20our%20publicly%20released%20data%20splits%2C%20establishing%20the%20first%20performance%20baselines%20for%20this%20challenging%20task.%20We%20release%20the%20full%20dataset%20together%20with%20data%20curation%20tools%2C%20the%20open%20hardware%20design%2C%20and%20the%20benchmark%20code%20to%20foster%20future%20research%20in%20VRU%20action%20understanding%20under%20https%3A//iv.ee.hm.edu/bikeactions/.&entry.1838667208=http%3A//arxiv.org/abs/2601.10521v2&entry.124074799=Read"},
{"title": "Multi-Scenario Highway Lane-Change Intention Prediction: A Temporal Physics-Informed Multi-Modal Framework", "author": "Jiazhao Shi and Ziyu Wang and Yichen Lin and Shoufeng Lu", "abstract": "Lane-change intention prediction is safety-critical for autonomous driving and ADAS, but remains difficult in naturalistic traffic due to noisy kinematics, severe class imbalance, and limited generalization across heterogeneous highway scenarios. We propose Temporal Physics-Informed AI (TPI-AI), a hybrid framework that fuses deep temporal representations with physics-inspired interaction cues. A two-layer bidirectional LSTM (Bi-LSTM) encoder learns compact embeddings from multi-step trajectory histories; we concatenate these embeddings with kinematics-, safety-, and interaction-aware features (e.g., headway, TTC, and safe-gap indicators) and train a LightGBM classifier for three-class intention recognition (No-LC, Left-LC, Right-LC). To improve minority-class reliability, we apply imbalance-aware optimization including resampling/weighting and fold-wise threshold calibration. Experiments on two large-scale drone-based datasets, highD (straight highways) and exiD (ramp-rich environments), use location-based splits and evaluate prediction horizons T = 1, 2, 3 s. TPI-AI outperforms standalone LightGBM and Bi-LSTM baselines, achieving macro-F1 of 0.9562, 0.9124, 0.8345 on highD and 0.9247, 0.8197, 0.7605 on exiD at T = 1, 2, 3 s, respectively. These results show that combining physics-informed interaction features with learned temporal embeddings yields robust multi-scenario lane-change intention prediction.", "link": "http://arxiv.org/abs/2512.24075v2", "date": "2026-01-20", "relevancy": 2.1007, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6011}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5174}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5025}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-Scenario%20Highway%20Lane-Change%20Intention%20Prediction%3A%20A%20Temporal%20Physics-Informed%20Multi-Modal%20Framework&body=Title%3A%20Multi-Scenario%20Highway%20Lane-Change%20Intention%20Prediction%3A%20A%20Temporal%20Physics-Informed%20Multi-Modal%20Framework%0AAuthor%3A%20Jiazhao%20Shi%20and%20Ziyu%20Wang%20and%20Yichen%20Lin%20and%20Shoufeng%20Lu%0AAbstract%3A%20Lane-change%20intention%20prediction%20is%20safety-critical%20for%20autonomous%20driving%20and%20ADAS%2C%20but%20remains%20difficult%20in%20naturalistic%20traffic%20due%20to%20noisy%20kinematics%2C%20severe%20class%20imbalance%2C%20and%20limited%20generalization%20across%20heterogeneous%20highway%20scenarios.%20We%20propose%20Temporal%20Physics-Informed%20AI%20%28TPI-AI%29%2C%20a%20hybrid%20framework%20that%20fuses%20deep%20temporal%20representations%20with%20physics-inspired%20interaction%20cues.%20A%20two-layer%20bidirectional%20LSTM%20%28Bi-LSTM%29%20encoder%20learns%20compact%20embeddings%20from%20multi-step%20trajectory%20histories%3B%20we%20concatenate%20these%20embeddings%20with%20kinematics-%2C%20safety-%2C%20and%20interaction-aware%20features%20%28e.g.%2C%20headway%2C%20TTC%2C%20and%20safe-gap%20indicators%29%20and%20train%20a%20LightGBM%20classifier%20for%20three-class%20intention%20recognition%20%28No-LC%2C%20Left-LC%2C%20Right-LC%29.%20To%20improve%20minority-class%20reliability%2C%20we%20apply%20imbalance-aware%20optimization%20including%20resampling/weighting%20and%20fold-wise%20threshold%20calibration.%20Experiments%20on%20two%20large-scale%20drone-based%20datasets%2C%20highD%20%28straight%20highways%29%20and%20exiD%20%28ramp-rich%20environments%29%2C%20use%20location-based%20splits%20and%20evaluate%20prediction%20horizons%20T%20%3D%201%2C%202%2C%203%20s.%20TPI-AI%20outperforms%20standalone%20LightGBM%20and%20Bi-LSTM%20baselines%2C%20achieving%20macro-F1%20of%200.9562%2C%200.9124%2C%200.8345%20on%20highD%20and%200.9247%2C%200.8197%2C%200.7605%20on%20exiD%20at%20T%20%3D%201%2C%202%2C%203%20s%2C%20respectively.%20These%20results%20show%20that%20combining%20physics-informed%20interaction%20features%20with%20learned%20temporal%20embeddings%20yields%20robust%20multi-scenario%20lane-change%20intention%20prediction.%0ALink%3A%20http%3A//arxiv.org/abs/2512.24075v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-Scenario%2520Highway%2520Lane-Change%2520Intention%2520Prediction%253A%2520A%2520Temporal%2520Physics-Informed%2520Multi-Modal%2520Framework%26entry.906535625%3DJiazhao%2520Shi%2520and%2520Ziyu%2520Wang%2520and%2520Yichen%2520Lin%2520and%2520Shoufeng%2520Lu%26entry.1292438233%3DLane-change%2520intention%2520prediction%2520is%2520safety-critical%2520for%2520autonomous%2520driving%2520and%2520ADAS%252C%2520but%2520remains%2520difficult%2520in%2520naturalistic%2520traffic%2520due%2520to%2520noisy%2520kinematics%252C%2520severe%2520class%2520imbalance%252C%2520and%2520limited%2520generalization%2520across%2520heterogeneous%2520highway%2520scenarios.%2520We%2520propose%2520Temporal%2520Physics-Informed%2520AI%2520%2528TPI-AI%2529%252C%2520a%2520hybrid%2520framework%2520that%2520fuses%2520deep%2520temporal%2520representations%2520with%2520physics-inspired%2520interaction%2520cues.%2520A%2520two-layer%2520bidirectional%2520LSTM%2520%2528Bi-LSTM%2529%2520encoder%2520learns%2520compact%2520embeddings%2520from%2520multi-step%2520trajectory%2520histories%253B%2520we%2520concatenate%2520these%2520embeddings%2520with%2520kinematics-%252C%2520safety-%252C%2520and%2520interaction-aware%2520features%2520%2528e.g.%252C%2520headway%252C%2520TTC%252C%2520and%2520safe-gap%2520indicators%2529%2520and%2520train%2520a%2520LightGBM%2520classifier%2520for%2520three-class%2520intention%2520recognition%2520%2528No-LC%252C%2520Left-LC%252C%2520Right-LC%2529.%2520To%2520improve%2520minority-class%2520reliability%252C%2520we%2520apply%2520imbalance-aware%2520optimization%2520including%2520resampling/weighting%2520and%2520fold-wise%2520threshold%2520calibration.%2520Experiments%2520on%2520two%2520large-scale%2520drone-based%2520datasets%252C%2520highD%2520%2528straight%2520highways%2529%2520and%2520exiD%2520%2528ramp-rich%2520environments%2529%252C%2520use%2520location-based%2520splits%2520and%2520evaluate%2520prediction%2520horizons%2520T%2520%253D%25201%252C%25202%252C%25203%2520s.%2520TPI-AI%2520outperforms%2520standalone%2520LightGBM%2520and%2520Bi-LSTM%2520baselines%252C%2520achieving%2520macro-F1%2520of%25200.9562%252C%25200.9124%252C%25200.8345%2520on%2520highD%2520and%25200.9247%252C%25200.8197%252C%25200.7605%2520on%2520exiD%2520at%2520T%2520%253D%25201%252C%25202%252C%25203%2520s%252C%2520respectively.%2520These%2520results%2520show%2520that%2520combining%2520physics-informed%2520interaction%2520features%2520with%2520learned%2520temporal%2520embeddings%2520yields%2520robust%2520multi-scenario%2520lane-change%2520intention%2520prediction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.24075v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-Scenario%20Highway%20Lane-Change%20Intention%20Prediction%3A%20A%20Temporal%20Physics-Informed%20Multi-Modal%20Framework&entry.906535625=Jiazhao%20Shi%20and%20Ziyu%20Wang%20and%20Yichen%20Lin%20and%20Shoufeng%20Lu&entry.1292438233=Lane-change%20intention%20prediction%20is%20safety-critical%20for%20autonomous%20driving%20and%20ADAS%2C%20but%20remains%20difficult%20in%20naturalistic%20traffic%20due%20to%20noisy%20kinematics%2C%20severe%20class%20imbalance%2C%20and%20limited%20generalization%20across%20heterogeneous%20highway%20scenarios.%20We%20propose%20Temporal%20Physics-Informed%20AI%20%28TPI-AI%29%2C%20a%20hybrid%20framework%20that%20fuses%20deep%20temporal%20representations%20with%20physics-inspired%20interaction%20cues.%20A%20two-layer%20bidirectional%20LSTM%20%28Bi-LSTM%29%20encoder%20learns%20compact%20embeddings%20from%20multi-step%20trajectory%20histories%3B%20we%20concatenate%20these%20embeddings%20with%20kinematics-%2C%20safety-%2C%20and%20interaction-aware%20features%20%28e.g.%2C%20headway%2C%20TTC%2C%20and%20safe-gap%20indicators%29%20and%20train%20a%20LightGBM%20classifier%20for%20three-class%20intention%20recognition%20%28No-LC%2C%20Left-LC%2C%20Right-LC%29.%20To%20improve%20minority-class%20reliability%2C%20we%20apply%20imbalance-aware%20optimization%20including%20resampling/weighting%20and%20fold-wise%20threshold%20calibration.%20Experiments%20on%20two%20large-scale%20drone-based%20datasets%2C%20highD%20%28straight%20highways%29%20and%20exiD%20%28ramp-rich%20environments%29%2C%20use%20location-based%20splits%20and%20evaluate%20prediction%20horizons%20T%20%3D%201%2C%202%2C%203%20s.%20TPI-AI%20outperforms%20standalone%20LightGBM%20and%20Bi-LSTM%20baselines%2C%20achieving%20macro-F1%20of%200.9562%2C%200.9124%2C%200.8345%20on%20highD%20and%200.9247%2C%200.8197%2C%200.7605%20on%20exiD%20at%20T%20%3D%201%2C%202%2C%203%20s%2C%20respectively.%20These%20results%20show%20that%20combining%20physics-informed%20interaction%20features%20with%20learned%20temporal%20embeddings%20yields%20robust%20multi-scenario%20lane-change%20intention%20prediction.&entry.1838667208=http%3A//arxiv.org/abs/2512.24075v2&entry.124074799=Read"},
{"title": "DroneVLA: VLA based Aerial Manipulation", "author": "Fawad Mehboob and Monijesu James and Amir Habel and Jeffrin Sam and Miguel Altamirano Cabrera and Dzmitry Tsetserukou", "abstract": "As aerial platforms evolve from passive observers to active manipulators, the challenge shifts toward designing intuitive interfaces that allow non-expert users to command these systems naturally. This work introduces a novel concept of autonomous aerial manipulation system capable of interpreting high-level natural language commands to retrieve objects and deliver them to a human user. The system is intended to integrate a MediaPipe based on Grounding DINO and a Vision-Language-Action (VLA) model with a custom-built drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. VLA performs semantic reasoning to interpret the intent of a user prompt and generates a prioritized task queue for grasping of relevant objects in the scene. Grounding DINO and dynamic A* planning algorithm are used to navigate and safely relocate the object. To ensure safe and natural interaction during the handover phase, the system employs a human-centric controller driven by MediaPipe. This module provides real-time human pose estimation, allowing the drone to employ visual servoing to maintain a stable, distinct position directly in front of the user, facilitating a comfortable handover. We demonstrate the system's efficacy through real-world experiments for localization and navigation, which resulted in a 0.164m, 0.070m, and 0.084m of max, mean euclidean, and root-mean squared errors, respectively, highlighting the feasibility of VLA for aerial manipulation operations.", "link": "http://arxiv.org/abs/2601.13809v1", "date": "2026-01-20", "relevancy": 2.0963, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5324}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5246}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5155}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DroneVLA%3A%20VLA%20based%20Aerial%20Manipulation&body=Title%3A%20DroneVLA%3A%20VLA%20based%20Aerial%20Manipulation%0AAuthor%3A%20Fawad%20Mehboob%20and%20Monijesu%20James%20and%20Amir%20Habel%20and%20Jeffrin%20Sam%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou%0AAbstract%3A%20As%20aerial%20platforms%20evolve%20from%20passive%20observers%20to%20active%20manipulators%2C%20the%20challenge%20shifts%20toward%20designing%20intuitive%20interfaces%20that%20allow%20non-expert%20users%20to%20command%20these%20systems%20naturally.%20This%20work%20introduces%20a%20novel%20concept%20of%20autonomous%20aerial%20manipulation%20system%20capable%20of%20interpreting%20high-level%20natural%20language%20commands%20to%20retrieve%20objects%20and%20deliver%20them%20to%20a%20human%20user.%20The%20system%20is%20intended%20to%20integrate%20a%20MediaPipe%20based%20on%20Grounding%20DINO%20and%20a%20Vision-Language-Action%20%28VLA%29%20model%20with%20a%20custom-built%20drone%20equipped%20with%20a%201-DOF%20gripper%20and%20an%20Intel%20RealSense%20RGB-D%20camera.%20VLA%20performs%20semantic%20reasoning%20to%20interpret%20the%20intent%20of%20a%20user%20prompt%20and%20generates%20a%20prioritized%20task%20queue%20for%20grasping%20of%20relevant%20objects%20in%20the%20scene.%20Grounding%20DINO%20and%20dynamic%20A%2A%20planning%20algorithm%20are%20used%20to%20navigate%20and%20safely%20relocate%20the%20object.%20To%20ensure%20safe%20and%20natural%20interaction%20during%20the%20handover%20phase%2C%20the%20system%20employs%20a%20human-centric%20controller%20driven%20by%20MediaPipe.%20This%20module%20provides%20real-time%20human%20pose%20estimation%2C%20allowing%20the%20drone%20to%20employ%20visual%20servoing%20to%20maintain%20a%20stable%2C%20distinct%20position%20directly%20in%20front%20of%20the%20user%2C%20facilitating%20a%20comfortable%20handover.%20We%20demonstrate%20the%20system%27s%20efficacy%20through%20real-world%20experiments%20for%20localization%20and%20navigation%2C%20which%20resulted%20in%20a%200.164m%2C%200.070m%2C%20and%200.084m%20of%20max%2C%20mean%20euclidean%2C%20and%20root-mean%20squared%20errors%2C%20respectively%2C%20highlighting%20the%20feasibility%20of%20VLA%20for%20aerial%20manipulation%20operations.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13809v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDroneVLA%253A%2520VLA%2520based%2520Aerial%2520Manipulation%26entry.906535625%3DFawad%2520Mehboob%2520and%2520Monijesu%2520James%2520and%2520Amir%2520Habel%2520and%2520Jeffrin%2520Sam%2520and%2520Miguel%2520Altamirano%2520Cabrera%2520and%2520Dzmitry%2520Tsetserukou%26entry.1292438233%3DAs%2520aerial%2520platforms%2520evolve%2520from%2520passive%2520observers%2520to%2520active%2520manipulators%252C%2520the%2520challenge%2520shifts%2520toward%2520designing%2520intuitive%2520interfaces%2520that%2520allow%2520non-expert%2520users%2520to%2520command%2520these%2520systems%2520naturally.%2520This%2520work%2520introduces%2520a%2520novel%2520concept%2520of%2520autonomous%2520aerial%2520manipulation%2520system%2520capable%2520of%2520interpreting%2520high-level%2520natural%2520language%2520commands%2520to%2520retrieve%2520objects%2520and%2520deliver%2520them%2520to%2520a%2520human%2520user.%2520The%2520system%2520is%2520intended%2520to%2520integrate%2520a%2520MediaPipe%2520based%2520on%2520Grounding%2520DINO%2520and%2520a%2520Vision-Language-Action%2520%2528VLA%2529%2520model%2520with%2520a%2520custom-built%2520drone%2520equipped%2520with%2520a%25201-DOF%2520gripper%2520and%2520an%2520Intel%2520RealSense%2520RGB-D%2520camera.%2520VLA%2520performs%2520semantic%2520reasoning%2520to%2520interpret%2520the%2520intent%2520of%2520a%2520user%2520prompt%2520and%2520generates%2520a%2520prioritized%2520task%2520queue%2520for%2520grasping%2520of%2520relevant%2520objects%2520in%2520the%2520scene.%2520Grounding%2520DINO%2520and%2520dynamic%2520A%252A%2520planning%2520algorithm%2520are%2520used%2520to%2520navigate%2520and%2520safely%2520relocate%2520the%2520object.%2520To%2520ensure%2520safe%2520and%2520natural%2520interaction%2520during%2520the%2520handover%2520phase%252C%2520the%2520system%2520employs%2520a%2520human-centric%2520controller%2520driven%2520by%2520MediaPipe.%2520This%2520module%2520provides%2520real-time%2520human%2520pose%2520estimation%252C%2520allowing%2520the%2520drone%2520to%2520employ%2520visual%2520servoing%2520to%2520maintain%2520a%2520stable%252C%2520distinct%2520position%2520directly%2520in%2520front%2520of%2520the%2520user%252C%2520facilitating%2520a%2520comfortable%2520handover.%2520We%2520demonstrate%2520the%2520system%2527s%2520efficacy%2520through%2520real-world%2520experiments%2520for%2520localization%2520and%2520navigation%252C%2520which%2520resulted%2520in%2520a%25200.164m%252C%25200.070m%252C%2520and%25200.084m%2520of%2520max%252C%2520mean%2520euclidean%252C%2520and%2520root-mean%2520squared%2520errors%252C%2520respectively%252C%2520highlighting%2520the%2520feasibility%2520of%2520VLA%2520for%2520aerial%2520manipulation%2520operations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13809v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DroneVLA%3A%20VLA%20based%20Aerial%20Manipulation&entry.906535625=Fawad%20Mehboob%20and%20Monijesu%20James%20and%20Amir%20Habel%20and%20Jeffrin%20Sam%20and%20Miguel%20Altamirano%20Cabrera%20and%20Dzmitry%20Tsetserukou&entry.1292438233=As%20aerial%20platforms%20evolve%20from%20passive%20observers%20to%20active%20manipulators%2C%20the%20challenge%20shifts%20toward%20designing%20intuitive%20interfaces%20that%20allow%20non-expert%20users%20to%20command%20these%20systems%20naturally.%20This%20work%20introduces%20a%20novel%20concept%20of%20autonomous%20aerial%20manipulation%20system%20capable%20of%20interpreting%20high-level%20natural%20language%20commands%20to%20retrieve%20objects%20and%20deliver%20them%20to%20a%20human%20user.%20The%20system%20is%20intended%20to%20integrate%20a%20MediaPipe%20based%20on%20Grounding%20DINO%20and%20a%20Vision-Language-Action%20%28VLA%29%20model%20with%20a%20custom-built%20drone%20equipped%20with%20a%201-DOF%20gripper%20and%20an%20Intel%20RealSense%20RGB-D%20camera.%20VLA%20performs%20semantic%20reasoning%20to%20interpret%20the%20intent%20of%20a%20user%20prompt%20and%20generates%20a%20prioritized%20task%20queue%20for%20grasping%20of%20relevant%20objects%20in%20the%20scene.%20Grounding%20DINO%20and%20dynamic%20A%2A%20planning%20algorithm%20are%20used%20to%20navigate%20and%20safely%20relocate%20the%20object.%20To%20ensure%20safe%20and%20natural%20interaction%20during%20the%20handover%20phase%2C%20the%20system%20employs%20a%20human-centric%20controller%20driven%20by%20MediaPipe.%20This%20module%20provides%20real-time%20human%20pose%20estimation%2C%20allowing%20the%20drone%20to%20employ%20visual%20servoing%20to%20maintain%20a%20stable%2C%20distinct%20position%20directly%20in%20front%20of%20the%20user%2C%20facilitating%20a%20comfortable%20handover.%20We%20demonstrate%20the%20system%27s%20efficacy%20through%20real-world%20experiments%20for%20localization%20and%20navigation%2C%20which%20resulted%20in%20a%200.164m%2C%200.070m%2C%20and%200.084m%20of%20max%2C%20mean%20euclidean%2C%20and%20root-mean%20squared%20errors%2C%20respectively%2C%20highlighting%20the%20feasibility%20of%20VLA%20for%20aerial%20manipulation%20operations.&entry.1838667208=http%3A//arxiv.org/abs/2601.13809v1&entry.124074799=Read"},
{"title": "Optimizing Energy and Data Collection in UAV-aided IoT Networks using Attention-based Multi-Objective Reinforcement Learning", "author": "Babacar Toure and Dimitrios Tsilimantos and Omid Esrafilian and Marios Kountouris", "abstract": "Due to their adaptability and mobility, Unmanned Aerial Vehicles (UAVs) are becoming increasingly essential for wireless network services, particularly for data harvesting tasks. In this context, Artificial Intelligence (AI)-based approaches have gained significant attention for addressing UAV path planning tasks in large and complex environments, bridging the gap with real-world deployments. However, many existing algorithms suffer from limited training data, which hampers their performance in highly dynamic environments. Moreover, they often overlook the inherently multi-objective nature of the task, treating it in an overly simplistic manner. To address these limitations, we propose an attention-based Multi-Objective Reinforcement Learning (MORL) architecture that explicitly handles the trade-off between data collection and energy consumption in urban environments, even without prior knowledge of wireless channel conditions. Our method develops a single model capable of adapting to varying trade-off preferences and dynamic scenario parameters without the need for fine-tuning or retraining. Extensive simulations show that our approach achieves substantial improvements in performance, model compactness, sample efficiency, and most importantly, generalization to previously unseen scenarios, outperforming existing RL solutions.", "link": "http://arxiv.org/abs/2601.14092v1", "date": "2026-01-20", "relevancy": 2.0924, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5452}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5398}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4976}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimizing%20Energy%20and%20Data%20Collection%20in%20UAV-aided%20IoT%20Networks%20using%20Attention-based%20Multi-Objective%20Reinforcement%20Learning&body=Title%3A%20Optimizing%20Energy%20and%20Data%20Collection%20in%20UAV-aided%20IoT%20Networks%20using%20Attention-based%20Multi-Objective%20Reinforcement%20Learning%0AAuthor%3A%20Babacar%20Toure%20and%20Dimitrios%20Tsilimantos%20and%20Omid%20Esrafilian%20and%20Marios%20Kountouris%0AAbstract%3A%20Due%20to%20their%20adaptability%20and%20mobility%2C%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20are%20becoming%20increasingly%20essential%20for%20wireless%20network%20services%2C%20particularly%20for%20data%20harvesting%20tasks.%20In%20this%20context%2C%20Artificial%20Intelligence%20%28AI%29-based%20approaches%20have%20gained%20significant%20attention%20for%20addressing%20UAV%20path%20planning%20tasks%20in%20large%20and%20complex%20environments%2C%20bridging%20the%20gap%20with%20real-world%20deployments.%20However%2C%20many%20existing%20algorithms%20suffer%20from%20limited%20training%20data%2C%20which%20hampers%20their%20performance%20in%20highly%20dynamic%20environments.%20Moreover%2C%20they%20often%20overlook%20the%20inherently%20multi-objective%20nature%20of%20the%20task%2C%20treating%20it%20in%20an%20overly%20simplistic%20manner.%20To%20address%20these%20limitations%2C%20we%20propose%20an%20attention-based%20Multi-Objective%20Reinforcement%20Learning%20%28MORL%29%20architecture%20that%20explicitly%20handles%20the%20trade-off%20between%20data%20collection%20and%20energy%20consumption%20in%20urban%20environments%2C%20even%20without%20prior%20knowledge%20of%20wireless%20channel%20conditions.%20Our%20method%20develops%20a%20single%20model%20capable%20of%20adapting%20to%20varying%20trade-off%20preferences%20and%20dynamic%20scenario%20parameters%20without%20the%20need%20for%20fine-tuning%20or%20retraining.%20Extensive%20simulations%20show%20that%20our%20approach%20achieves%20substantial%20improvements%20in%20performance%2C%20model%20compactness%2C%20sample%20efficiency%2C%20and%20most%20importantly%2C%20generalization%20to%20previously%20unseen%20scenarios%2C%20outperforming%20existing%20RL%20solutions.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14092v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimizing%2520Energy%2520and%2520Data%2520Collection%2520in%2520UAV-aided%2520IoT%2520Networks%2520using%2520Attention-based%2520Multi-Objective%2520Reinforcement%2520Learning%26entry.906535625%3DBabacar%2520Toure%2520and%2520Dimitrios%2520Tsilimantos%2520and%2520Omid%2520Esrafilian%2520and%2520Marios%2520Kountouris%26entry.1292438233%3DDue%2520to%2520their%2520adaptability%2520and%2520mobility%252C%2520Unmanned%2520Aerial%2520Vehicles%2520%2528UAVs%2529%2520are%2520becoming%2520increasingly%2520essential%2520for%2520wireless%2520network%2520services%252C%2520particularly%2520for%2520data%2520harvesting%2520tasks.%2520In%2520this%2520context%252C%2520Artificial%2520Intelligence%2520%2528AI%2529-based%2520approaches%2520have%2520gained%2520significant%2520attention%2520for%2520addressing%2520UAV%2520path%2520planning%2520tasks%2520in%2520large%2520and%2520complex%2520environments%252C%2520bridging%2520the%2520gap%2520with%2520real-world%2520deployments.%2520However%252C%2520many%2520existing%2520algorithms%2520suffer%2520from%2520limited%2520training%2520data%252C%2520which%2520hampers%2520their%2520performance%2520in%2520highly%2520dynamic%2520environments.%2520Moreover%252C%2520they%2520often%2520overlook%2520the%2520inherently%2520multi-objective%2520nature%2520of%2520the%2520task%252C%2520treating%2520it%2520in%2520an%2520overly%2520simplistic%2520manner.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520an%2520attention-based%2520Multi-Objective%2520Reinforcement%2520Learning%2520%2528MORL%2529%2520architecture%2520that%2520explicitly%2520handles%2520the%2520trade-off%2520between%2520data%2520collection%2520and%2520energy%2520consumption%2520in%2520urban%2520environments%252C%2520even%2520without%2520prior%2520knowledge%2520of%2520wireless%2520channel%2520conditions.%2520Our%2520method%2520develops%2520a%2520single%2520model%2520capable%2520of%2520adapting%2520to%2520varying%2520trade-off%2520preferences%2520and%2520dynamic%2520scenario%2520parameters%2520without%2520the%2520need%2520for%2520fine-tuning%2520or%2520retraining.%2520Extensive%2520simulations%2520show%2520that%2520our%2520approach%2520achieves%2520substantial%2520improvements%2520in%2520performance%252C%2520model%2520compactness%252C%2520sample%2520efficiency%252C%2520and%2520most%2520importantly%252C%2520generalization%2520to%2520previously%2520unseen%2520scenarios%252C%2520outperforming%2520existing%2520RL%2520solutions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14092v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimizing%20Energy%20and%20Data%20Collection%20in%20UAV-aided%20IoT%20Networks%20using%20Attention-based%20Multi-Objective%20Reinforcement%20Learning&entry.906535625=Babacar%20Toure%20and%20Dimitrios%20Tsilimantos%20and%20Omid%20Esrafilian%20and%20Marios%20Kountouris&entry.1292438233=Due%20to%20their%20adaptability%20and%20mobility%2C%20Unmanned%20Aerial%20Vehicles%20%28UAVs%29%20are%20becoming%20increasingly%20essential%20for%20wireless%20network%20services%2C%20particularly%20for%20data%20harvesting%20tasks.%20In%20this%20context%2C%20Artificial%20Intelligence%20%28AI%29-based%20approaches%20have%20gained%20significant%20attention%20for%20addressing%20UAV%20path%20planning%20tasks%20in%20large%20and%20complex%20environments%2C%20bridging%20the%20gap%20with%20real-world%20deployments.%20However%2C%20many%20existing%20algorithms%20suffer%20from%20limited%20training%20data%2C%20which%20hampers%20their%20performance%20in%20highly%20dynamic%20environments.%20Moreover%2C%20they%20often%20overlook%20the%20inherently%20multi-objective%20nature%20of%20the%20task%2C%20treating%20it%20in%20an%20overly%20simplistic%20manner.%20To%20address%20these%20limitations%2C%20we%20propose%20an%20attention-based%20Multi-Objective%20Reinforcement%20Learning%20%28MORL%29%20architecture%20that%20explicitly%20handles%20the%20trade-off%20between%20data%20collection%20and%20energy%20consumption%20in%20urban%20environments%2C%20even%20without%20prior%20knowledge%20of%20wireless%20channel%20conditions.%20Our%20method%20develops%20a%20single%20model%20capable%20of%20adapting%20to%20varying%20trade-off%20preferences%20and%20dynamic%20scenario%20parameters%20without%20the%20need%20for%20fine-tuning%20or%20retraining.%20Extensive%20simulations%20show%20that%20our%20approach%20achieves%20substantial%20improvements%20in%20performance%2C%20model%20compactness%2C%20sample%20efficiency%2C%20and%20most%20importantly%2C%20generalization%20to%20previously%20unseen%20scenarios%2C%20outperforming%20existing%20RL%20solutions.&entry.1838667208=http%3A//arxiv.org/abs/2601.14092v1&entry.124074799=Read"},
{"title": "Discriminant Learning-based Colorspace for Blade Segmentation", "author": "Ra\u00fcl P\u00e9rez-Gonzalo and Andreas Espersen and Antonio Agudo", "abstract": "Suboptimal color representation often hinders accurate image segmentation, yet many modern algorithms neglect this critical preprocessing step. This work presents a novel multidimensional nonlinear discriminant analysis algorithm, Colorspace Discriminant Analysis (CSDA), for improved segmentation. Extending Linear Discriminant Analysis into a deep learning context, CSDA customizes color representation by maximizing multidimensional signed inter-class separability while minimizing intra-class variability through a generalized discriminative loss. To ensure stable training, we introduce three alternative losses that enable end-to-end optimization of both the discriminative colorspace and segmentation process. Experiments on wind turbine blade data demonstrate significant accuracy gains, emphasizing the importance of tailored preprocessing in domain-specific segmentation.", "link": "http://arxiv.org/abs/2601.13816v1", "date": "2026-01-20", "relevancy": 2.0897, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5308}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5177}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.516}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discriminant%20Learning-based%20Colorspace%20for%20Blade%20Segmentation&body=Title%3A%20Discriminant%20Learning-based%20Colorspace%20for%20Blade%20Segmentation%0AAuthor%3A%20Ra%C3%BCl%20P%C3%A9rez-Gonzalo%20and%20Andreas%20Espersen%20and%20Antonio%20Agudo%0AAbstract%3A%20Suboptimal%20color%20representation%20often%20hinders%20accurate%20image%20segmentation%2C%20yet%20many%20modern%20algorithms%20neglect%20this%20critical%20preprocessing%20step.%20This%20work%20presents%20a%20novel%20multidimensional%20nonlinear%20discriminant%20analysis%20algorithm%2C%20Colorspace%20Discriminant%20Analysis%20%28CSDA%29%2C%20for%20improved%20segmentation.%20Extending%20Linear%20Discriminant%20Analysis%20into%20a%20deep%20learning%20context%2C%20CSDA%20customizes%20color%20representation%20by%20maximizing%20multidimensional%20signed%20inter-class%20separability%20while%20minimizing%20intra-class%20variability%20through%20a%20generalized%20discriminative%20loss.%20To%20ensure%20stable%20training%2C%20we%20introduce%20three%20alternative%20losses%20that%20enable%20end-to-end%20optimization%20of%20both%20the%20discriminative%20colorspace%20and%20segmentation%20process.%20Experiments%20on%20wind%20turbine%20blade%20data%20demonstrate%20significant%20accuracy%20gains%2C%20emphasizing%20the%20importance%20of%20tailored%20preprocessing%20in%20domain-specific%20segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13816v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscriminant%2520Learning-based%2520Colorspace%2520for%2520Blade%2520Segmentation%26entry.906535625%3DRa%25C3%25BCl%2520P%25C3%25A9rez-Gonzalo%2520and%2520Andreas%2520Espersen%2520and%2520Antonio%2520Agudo%26entry.1292438233%3DSuboptimal%2520color%2520representation%2520often%2520hinders%2520accurate%2520image%2520segmentation%252C%2520yet%2520many%2520modern%2520algorithms%2520neglect%2520this%2520critical%2520preprocessing%2520step.%2520This%2520work%2520presents%2520a%2520novel%2520multidimensional%2520nonlinear%2520discriminant%2520analysis%2520algorithm%252C%2520Colorspace%2520Discriminant%2520Analysis%2520%2528CSDA%2529%252C%2520for%2520improved%2520segmentation.%2520Extending%2520Linear%2520Discriminant%2520Analysis%2520into%2520a%2520deep%2520learning%2520context%252C%2520CSDA%2520customizes%2520color%2520representation%2520by%2520maximizing%2520multidimensional%2520signed%2520inter-class%2520separability%2520while%2520minimizing%2520intra-class%2520variability%2520through%2520a%2520generalized%2520discriminative%2520loss.%2520To%2520ensure%2520stable%2520training%252C%2520we%2520introduce%2520three%2520alternative%2520losses%2520that%2520enable%2520end-to-end%2520optimization%2520of%2520both%2520the%2520discriminative%2520colorspace%2520and%2520segmentation%2520process.%2520Experiments%2520on%2520wind%2520turbine%2520blade%2520data%2520demonstrate%2520significant%2520accuracy%2520gains%252C%2520emphasizing%2520the%2520importance%2520of%2520tailored%2520preprocessing%2520in%2520domain-specific%2520segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13816v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discriminant%20Learning-based%20Colorspace%20for%20Blade%20Segmentation&entry.906535625=Ra%C3%BCl%20P%C3%A9rez-Gonzalo%20and%20Andreas%20Espersen%20and%20Antonio%20Agudo&entry.1292438233=Suboptimal%20color%20representation%20often%20hinders%20accurate%20image%20segmentation%2C%20yet%20many%20modern%20algorithms%20neglect%20this%20critical%20preprocessing%20step.%20This%20work%20presents%20a%20novel%20multidimensional%20nonlinear%20discriminant%20analysis%20algorithm%2C%20Colorspace%20Discriminant%20Analysis%20%28CSDA%29%2C%20for%20improved%20segmentation.%20Extending%20Linear%20Discriminant%20Analysis%20into%20a%20deep%20learning%20context%2C%20CSDA%20customizes%20color%20representation%20by%20maximizing%20multidimensional%20signed%20inter-class%20separability%20while%20minimizing%20intra-class%20variability%20through%20a%20generalized%20discriminative%20loss.%20To%20ensure%20stable%20training%2C%20we%20introduce%20three%20alternative%20losses%20that%20enable%20end-to-end%20optimization%20of%20both%20the%20discriminative%20colorspace%20and%20segmentation%20process.%20Experiments%20on%20wind%20turbine%20blade%20data%20demonstrate%20significant%20accuracy%20gains%2C%20emphasizing%20the%20importance%20of%20tailored%20preprocessing%20in%20domain-specific%20segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2601.13816v1&entry.124074799=Read"},
{"title": "KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning", "author": "Egor Cherepanov and Daniil Zelezetsky and Alexey K. Kovalev and Aleksandr I. Panov", "abstract": "Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.", "link": "http://arxiv.org/abs/2601.14232v1", "date": "2026-01-20", "relevancy": 2.0895, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5274}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5217}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5177}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20KAGE-Bench%3A%20Fast%20Known-Axis%20Visual%20Generalization%20Evaluation%20for%20Reinforcement%20Learning&body=Title%3A%20KAGE-Bench%3A%20Fast%20Known-Axis%20Visual%20Generalization%20Evaluation%20for%20Reinforcement%20Learning%0AAuthor%3A%20Egor%20Cherepanov%20and%20Daniil%20Zelezetsky%20and%20Alexey%20K.%20Kovalev%20and%20Aleksandr%20I.%20Panov%0AAbstract%3A%20Pixel-based%20reinforcement%20learning%20agents%20often%20fail%20under%20purely%20visual%20distribution%20shift%20even%20when%20latent%20dynamics%20and%20rewards%20are%20unchanged%2C%20but%20existing%20benchmarks%20entangle%20multiple%20sources%20of%20shift%20and%20hinder%20systematic%20analysis.%20We%20introduce%20KAGE-Env%2C%20a%20JAX-native%202D%20platformer%20that%20factorizes%20the%20observation%20process%20into%20independently%20controllable%20visual%20axes%20while%20keeping%20the%20underlying%20control%20problem%20fixed.%20By%20construction%2C%20varying%20a%20visual%20axis%20affects%20performance%20only%20through%20the%20induced%20state-conditional%20action%20distribution%20of%20a%20pixel%20policy%2C%20providing%20a%20clean%20abstraction%20for%20visual%20generalization.%20Building%20on%20this%20environment%2C%20we%20define%20KAGE-Bench%2C%20a%20benchmark%20of%20six%20known-axis%20suites%20comprising%2034%20train-evaluation%20configuration%20pairs%20that%20isolate%20individual%20visual%20shifts.%20Using%20a%20standard%20PPO-CNN%20baseline%2C%20we%20observe%20strong%20axis-dependent%20failures%2C%20with%20background%20and%20photometric%20shifts%20often%20collapsing%20success%2C%20while%20agent-appearance%20shifts%20are%20comparatively%20benign.%20Several%20shifts%20preserve%20forward%20motion%20while%20breaking%20task%20completion%2C%20showing%20that%20return%20alone%20can%20obscure%20generalization%20failures.%20Finally%2C%20the%20fully%20vectorized%20JAX%20implementation%20enables%20up%20to%2033M%20environment%20steps%20per%20second%20on%20a%20single%20GPU%2C%20enabling%20fast%20and%20reproducible%20sweeps%20over%20visual%20factors.%20Code%3A%20https%3A//avanturist322.github.io/KAGEBench/.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14232v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DKAGE-Bench%253A%2520Fast%2520Known-Axis%2520Visual%2520Generalization%2520Evaluation%2520for%2520Reinforcement%2520Learning%26entry.906535625%3DEgor%2520Cherepanov%2520and%2520Daniil%2520Zelezetsky%2520and%2520Alexey%2520K.%2520Kovalev%2520and%2520Aleksandr%2520I.%2520Panov%26entry.1292438233%3DPixel-based%2520reinforcement%2520learning%2520agents%2520often%2520fail%2520under%2520purely%2520visual%2520distribution%2520shift%2520even%2520when%2520latent%2520dynamics%2520and%2520rewards%2520are%2520unchanged%252C%2520but%2520existing%2520benchmarks%2520entangle%2520multiple%2520sources%2520of%2520shift%2520and%2520hinder%2520systematic%2520analysis.%2520We%2520introduce%2520KAGE-Env%252C%2520a%2520JAX-native%25202D%2520platformer%2520that%2520factorizes%2520the%2520observation%2520process%2520into%2520independently%2520controllable%2520visual%2520axes%2520while%2520keeping%2520the%2520underlying%2520control%2520problem%2520fixed.%2520By%2520construction%252C%2520varying%2520a%2520visual%2520axis%2520affects%2520performance%2520only%2520through%2520the%2520induced%2520state-conditional%2520action%2520distribution%2520of%2520a%2520pixel%2520policy%252C%2520providing%2520a%2520clean%2520abstraction%2520for%2520visual%2520generalization.%2520Building%2520on%2520this%2520environment%252C%2520we%2520define%2520KAGE-Bench%252C%2520a%2520benchmark%2520of%2520six%2520known-axis%2520suites%2520comprising%252034%2520train-evaluation%2520configuration%2520pairs%2520that%2520isolate%2520individual%2520visual%2520shifts.%2520Using%2520a%2520standard%2520PPO-CNN%2520baseline%252C%2520we%2520observe%2520strong%2520axis-dependent%2520failures%252C%2520with%2520background%2520and%2520photometric%2520shifts%2520often%2520collapsing%2520success%252C%2520while%2520agent-appearance%2520shifts%2520are%2520comparatively%2520benign.%2520Several%2520shifts%2520preserve%2520forward%2520motion%2520while%2520breaking%2520task%2520completion%252C%2520showing%2520that%2520return%2520alone%2520can%2520obscure%2520generalization%2520failures.%2520Finally%252C%2520the%2520fully%2520vectorized%2520JAX%2520implementation%2520enables%2520up%2520to%252033M%2520environment%2520steps%2520per%2520second%2520on%2520a%2520single%2520GPU%252C%2520enabling%2520fast%2520and%2520reproducible%2520sweeps%2520over%2520visual%2520factors.%2520Code%253A%2520https%253A//avanturist322.github.io/KAGEBench/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14232v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=KAGE-Bench%3A%20Fast%20Known-Axis%20Visual%20Generalization%20Evaluation%20for%20Reinforcement%20Learning&entry.906535625=Egor%20Cherepanov%20and%20Daniil%20Zelezetsky%20and%20Alexey%20K.%20Kovalev%20and%20Aleksandr%20I.%20Panov&entry.1292438233=Pixel-based%20reinforcement%20learning%20agents%20often%20fail%20under%20purely%20visual%20distribution%20shift%20even%20when%20latent%20dynamics%20and%20rewards%20are%20unchanged%2C%20but%20existing%20benchmarks%20entangle%20multiple%20sources%20of%20shift%20and%20hinder%20systematic%20analysis.%20We%20introduce%20KAGE-Env%2C%20a%20JAX-native%202D%20platformer%20that%20factorizes%20the%20observation%20process%20into%20independently%20controllable%20visual%20axes%20while%20keeping%20the%20underlying%20control%20problem%20fixed.%20By%20construction%2C%20varying%20a%20visual%20axis%20affects%20performance%20only%20through%20the%20induced%20state-conditional%20action%20distribution%20of%20a%20pixel%20policy%2C%20providing%20a%20clean%20abstraction%20for%20visual%20generalization.%20Building%20on%20this%20environment%2C%20we%20define%20KAGE-Bench%2C%20a%20benchmark%20of%20six%20known-axis%20suites%20comprising%2034%20train-evaluation%20configuration%20pairs%20that%20isolate%20individual%20visual%20shifts.%20Using%20a%20standard%20PPO-CNN%20baseline%2C%20we%20observe%20strong%20axis-dependent%20failures%2C%20with%20background%20and%20photometric%20shifts%20often%20collapsing%20success%2C%20while%20agent-appearance%20shifts%20are%20comparatively%20benign.%20Several%20shifts%20preserve%20forward%20motion%20while%20breaking%20task%20completion%2C%20showing%20that%20return%20alone%20can%20obscure%20generalization%20failures.%20Finally%2C%20the%20fully%20vectorized%20JAX%20implementation%20enables%20up%20to%2033M%20environment%20steps%20per%20second%20on%20a%20single%20GPU%2C%20enabling%20fast%20and%20reproducible%20sweeps%20over%20visual%20factors.%20Code%3A%20https%3A//avanturist322.github.io/KAGEBench/.&entry.1838667208=http%3A//arxiv.org/abs/2601.14232v1&entry.124074799=Read"},
{"title": "Generalizing Abstention for Noise-Robust Learning in Medical Image Segmentation", "author": "Wesam Moustafa and Hossam Elsafty and Helen Schneider and Lorenz Sparrenberg and Rafet Sifa", "abstract": "Label noise is a critical problem in medical image segmentation, often arising from the inherent difficulty of manual annotation. Models trained on noisy data are prone to overfitting, which degrades their generalization performance. While a number of methods and strategies have been proposed to mitigate noisy labels in the segmentation domain, this area remains largely under-explored. The abstention mechanism has proven effective in classification tasks by enhancing the capabilities of Cross Entropy, yet its potential in segmentation remains unverified. In this paper, we address this gap by introducing a universal and modular abstention framework capable of enhancing the noise-robustness of a diverse range of loss functions. Our framework improves upon prior work with two key components: an informed regularization term to guide abstention behaviour, and a more flexible power-law-based auto-tuning algorithm for the abstention penalty. We demonstrate the framework's versatility by systematically integrating it with three distinct loss functions to create three novel, noise-robust variants: GAC, SAC, and ADS. Experiments on the CaDIS and DSAD medical datasets show our methods consistently and significantly outperform their non-abstaining baselines, especially under high noise levels. This work establishes that enabling models to selectively ignore corrupted samples is a powerful and generalizable strategy for building more reliable segmentation models. Our code is publicly available at https://github.com/wemous/abstention-for-segmentation.", "link": "http://arxiv.org/abs/2601.14039v1", "date": "2026-01-20", "relevancy": 2.0819, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5367}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5181}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5164}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Generalizing%20Abstention%20for%20Noise-Robust%20Learning%20in%20Medical%20Image%20Segmentation&body=Title%3A%20Generalizing%20Abstention%20for%20Noise-Robust%20Learning%20in%20Medical%20Image%20Segmentation%0AAuthor%3A%20Wesam%20Moustafa%20and%20Hossam%20Elsafty%20and%20Helen%20Schneider%20and%20Lorenz%20Sparrenberg%20and%20Rafet%20Sifa%0AAbstract%3A%20Label%20noise%20is%20a%20critical%20problem%20in%20medical%20image%20segmentation%2C%20often%20arising%20from%20the%20inherent%20difficulty%20of%20manual%20annotation.%20Models%20trained%20on%20noisy%20data%20are%20prone%20to%20overfitting%2C%20which%20degrades%20their%20generalization%20performance.%20While%20a%20number%20of%20methods%20and%20strategies%20have%20been%20proposed%20to%20mitigate%20noisy%20labels%20in%20the%20segmentation%20domain%2C%20this%20area%20remains%20largely%20under-explored.%20The%20abstention%20mechanism%20has%20proven%20effective%20in%20classification%20tasks%20by%20enhancing%20the%20capabilities%20of%20Cross%20Entropy%2C%20yet%20its%20potential%20in%20segmentation%20remains%20unverified.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%20introducing%20a%20universal%20and%20modular%20abstention%20framework%20capable%20of%20enhancing%20the%20noise-robustness%20of%20a%20diverse%20range%20of%20loss%20functions.%20Our%20framework%20improves%20upon%20prior%20work%20with%20two%20key%20components%3A%20an%20informed%20regularization%20term%20to%20guide%20abstention%20behaviour%2C%20and%20a%20more%20flexible%20power-law-based%20auto-tuning%20algorithm%20for%20the%20abstention%20penalty.%20We%20demonstrate%20the%20framework%27s%20versatility%20by%20systematically%20integrating%20it%20with%20three%20distinct%20loss%20functions%20to%20create%20three%20novel%2C%20noise-robust%20variants%3A%20GAC%2C%20SAC%2C%20and%20ADS.%20Experiments%20on%20the%20CaDIS%20and%20DSAD%20medical%20datasets%20show%20our%20methods%20consistently%20and%20significantly%20outperform%20their%20non-abstaining%20baselines%2C%20especially%20under%20high%20noise%20levels.%20This%20work%20establishes%20that%20enabling%20models%20to%20selectively%20ignore%20corrupted%20samples%20is%20a%20powerful%20and%20generalizable%20strategy%20for%20building%20more%20reliable%20segmentation%20models.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/wemous/abstention-for-segmentation.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14039v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGeneralizing%2520Abstention%2520for%2520Noise-Robust%2520Learning%2520in%2520Medical%2520Image%2520Segmentation%26entry.906535625%3DWesam%2520Moustafa%2520and%2520Hossam%2520Elsafty%2520and%2520Helen%2520Schneider%2520and%2520Lorenz%2520Sparrenberg%2520and%2520Rafet%2520Sifa%26entry.1292438233%3DLabel%2520noise%2520is%2520a%2520critical%2520problem%2520in%2520medical%2520image%2520segmentation%252C%2520often%2520arising%2520from%2520the%2520inherent%2520difficulty%2520of%2520manual%2520annotation.%2520Models%2520trained%2520on%2520noisy%2520data%2520are%2520prone%2520to%2520overfitting%252C%2520which%2520degrades%2520their%2520generalization%2520performance.%2520While%2520a%2520number%2520of%2520methods%2520and%2520strategies%2520have%2520been%2520proposed%2520to%2520mitigate%2520noisy%2520labels%2520in%2520the%2520segmentation%2520domain%252C%2520this%2520area%2520remains%2520largely%2520under-explored.%2520The%2520abstention%2520mechanism%2520has%2520proven%2520effective%2520in%2520classification%2520tasks%2520by%2520enhancing%2520the%2520capabilities%2520of%2520Cross%2520Entropy%252C%2520yet%2520its%2520potential%2520in%2520segmentation%2520remains%2520unverified.%2520In%2520this%2520paper%252C%2520we%2520address%2520this%2520gap%2520by%2520introducing%2520a%2520universal%2520and%2520modular%2520abstention%2520framework%2520capable%2520of%2520enhancing%2520the%2520noise-robustness%2520of%2520a%2520diverse%2520range%2520of%2520loss%2520functions.%2520Our%2520framework%2520improves%2520upon%2520prior%2520work%2520with%2520two%2520key%2520components%253A%2520an%2520informed%2520regularization%2520term%2520to%2520guide%2520abstention%2520behaviour%252C%2520and%2520a%2520more%2520flexible%2520power-law-based%2520auto-tuning%2520algorithm%2520for%2520the%2520abstention%2520penalty.%2520We%2520demonstrate%2520the%2520framework%2527s%2520versatility%2520by%2520systematically%2520integrating%2520it%2520with%2520three%2520distinct%2520loss%2520functions%2520to%2520create%2520three%2520novel%252C%2520noise-robust%2520variants%253A%2520GAC%252C%2520SAC%252C%2520and%2520ADS.%2520Experiments%2520on%2520the%2520CaDIS%2520and%2520DSAD%2520medical%2520datasets%2520show%2520our%2520methods%2520consistently%2520and%2520significantly%2520outperform%2520their%2520non-abstaining%2520baselines%252C%2520especially%2520under%2520high%2520noise%2520levels.%2520This%2520work%2520establishes%2520that%2520enabling%2520models%2520to%2520selectively%2520ignore%2520corrupted%2520samples%2520is%2520a%2520powerful%2520and%2520generalizable%2520strategy%2520for%2520building%2520more%2520reliable%2520segmentation%2520models.%2520Our%2520code%2520is%2520publicly%2520available%2520at%2520https%253A//github.com/wemous/abstention-for-segmentation.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14039v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalizing%20Abstention%20for%20Noise-Robust%20Learning%20in%20Medical%20Image%20Segmentation&entry.906535625=Wesam%20Moustafa%20and%20Hossam%20Elsafty%20and%20Helen%20Schneider%20and%20Lorenz%20Sparrenberg%20and%20Rafet%20Sifa&entry.1292438233=Label%20noise%20is%20a%20critical%20problem%20in%20medical%20image%20segmentation%2C%20often%20arising%20from%20the%20inherent%20difficulty%20of%20manual%20annotation.%20Models%20trained%20on%20noisy%20data%20are%20prone%20to%20overfitting%2C%20which%20degrades%20their%20generalization%20performance.%20While%20a%20number%20of%20methods%20and%20strategies%20have%20been%20proposed%20to%20mitigate%20noisy%20labels%20in%20the%20segmentation%20domain%2C%20this%20area%20remains%20largely%20under-explored.%20The%20abstention%20mechanism%20has%20proven%20effective%20in%20classification%20tasks%20by%20enhancing%20the%20capabilities%20of%20Cross%20Entropy%2C%20yet%20its%20potential%20in%20segmentation%20remains%20unverified.%20In%20this%20paper%2C%20we%20address%20this%20gap%20by%20introducing%20a%20universal%20and%20modular%20abstention%20framework%20capable%20of%20enhancing%20the%20noise-robustness%20of%20a%20diverse%20range%20of%20loss%20functions.%20Our%20framework%20improves%20upon%20prior%20work%20with%20two%20key%20components%3A%20an%20informed%20regularization%20term%20to%20guide%20abstention%20behaviour%2C%20and%20a%20more%20flexible%20power-law-based%20auto-tuning%20algorithm%20for%20the%20abstention%20penalty.%20We%20demonstrate%20the%20framework%27s%20versatility%20by%20systematically%20integrating%20it%20with%20three%20distinct%20loss%20functions%20to%20create%20three%20novel%2C%20noise-robust%20variants%3A%20GAC%2C%20SAC%2C%20and%20ADS.%20Experiments%20on%20the%20CaDIS%20and%20DSAD%20medical%20datasets%20show%20our%20methods%20consistently%20and%20significantly%20outperform%20their%20non-abstaining%20baselines%2C%20especially%20under%20high%20noise%20levels.%20This%20work%20establishes%20that%20enabling%20models%20to%20selectively%20ignore%20corrupted%20samples%20is%20a%20powerful%20and%20generalizable%20strategy%20for%20building%20more%20reliable%20segmentation%20models.%20Our%20code%20is%20publicly%20available%20at%20https%3A//github.com/wemous/abstention-for-segmentation.&entry.1838667208=http%3A//arxiv.org/abs/2601.14039v1&entry.124074799=Read"},
{"title": "DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning", "author": "Abdurrahim Yilmaz and Ozan Erdem and Ece Gokyayla and Ayda Acar and Burc Bugra Dagtas and Dilara Ilhan Erdil and Gulsum Gencoglan and Burak Temelkuran", "abstract": "Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.", "link": "http://arxiv.org/abs/2601.14084v1", "date": "2026-01-20", "relevancy": 2.0782, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.526}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.526}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4871}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DermaBench%3A%20A%20Clinician-Annotated%20Benchmark%20Dataset%20for%20Dermatology%20Visual%20Question%20Answering%20and%20Reasoning&body=Title%3A%20DermaBench%3A%20A%20Clinician-Annotated%20Benchmark%20Dataset%20for%20Dermatology%20Visual%20Question%20Answering%20and%20Reasoning%0AAuthor%3A%20Abdurrahim%20Yilmaz%20and%20Ozan%20Erdem%20and%20Ece%20Gokyayla%20and%20Ayda%20Acar%20and%20Burc%20Bugra%20Dagtas%20and%20Dilara%20Ilhan%20Erdil%20and%20Gulsum%20Gencoglan%20and%20Burak%20Temelkuran%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%20are%20increasingly%20important%20in%20medical%20applications%3B%20however%2C%20their%20evaluation%20in%20dermatology%20remains%20limited%20by%20datasets%20that%20focus%20primarily%20on%20image-level%20classification%20tasks%20such%20as%20lesion%20recognition.%20While%20valuable%20for%20recognition%2C%20such%20datasets%20cannot%20assess%20the%20full%20visual%20understanding%2C%20language%20grounding%2C%20and%20clinical%20reasoning%20capabilities%20of%20multimodal%20models.%20Visual%20question%20answering%20%28VQA%29%20benchmarks%20are%20required%20to%20evaluate%20how%20models%20interpret%20dermatological%20images%2C%20reason%20over%20fine-grained%20morphology%2C%20and%20generate%20clinically%20meaningful%20descriptions.%20We%20introduce%20DermaBench%2C%20a%20clinician-annotated%20dermatology%20VQA%20benchmark%20built%20on%20the%20Diverse%20Dermatology%20Images%20%28DDI%29%20dataset.%20DermaBench%20comprises%20656%20clinical%20images%20from%20570%20unique%20patients%20spanning%20Fitzpatrick%20skin%20types%20I-VI.%20Using%20a%20hierarchical%20annotation%20schema%20with%2022%20main%20questions%20%28single-choice%2C%20multi-choice%2C%20and%20open-ended%29%2C%20expert%20dermatologists%20annotated%20each%20image%20for%20diagnosis%2C%20anatomic%20site%2C%20lesion%20morphology%2C%20distribution%2C%20surface%20features%2C%20color%2C%20and%20image%20quality%2C%20together%20with%20open-ended%20narrative%20descriptions%20and%20summaries%2C%20yielding%20approximately%2014.474%20VQA-style%20annotations.%20DermaBench%20is%20released%20as%20a%20metadata-only%20dataset%20to%20respect%20upstream%20licensing%20and%20is%20publicly%20available%20at%20Harvard%20Dataverse.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14084v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDermaBench%253A%2520A%2520Clinician-Annotated%2520Benchmark%2520Dataset%2520for%2520Dermatology%2520Visual%2520Question%2520Answering%2520and%2520Reasoning%26entry.906535625%3DAbdurrahim%2520Yilmaz%2520and%2520Ozan%2520Erdem%2520and%2520Ece%2520Gokyayla%2520and%2520Ayda%2520Acar%2520and%2520Burc%2520Bugra%2520Dagtas%2520and%2520Dilara%2520Ilhan%2520Erdil%2520and%2520Gulsum%2520Gencoglan%2520and%2520Burak%2520Temelkuran%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%2520are%2520increasingly%2520important%2520in%2520medical%2520applications%253B%2520however%252C%2520their%2520evaluation%2520in%2520dermatology%2520remains%2520limited%2520by%2520datasets%2520that%2520focus%2520primarily%2520on%2520image-level%2520classification%2520tasks%2520such%2520as%2520lesion%2520recognition.%2520While%2520valuable%2520for%2520recognition%252C%2520such%2520datasets%2520cannot%2520assess%2520the%2520full%2520visual%2520understanding%252C%2520language%2520grounding%252C%2520and%2520clinical%2520reasoning%2520capabilities%2520of%2520multimodal%2520models.%2520Visual%2520question%2520answering%2520%2528VQA%2529%2520benchmarks%2520are%2520required%2520to%2520evaluate%2520how%2520models%2520interpret%2520dermatological%2520images%252C%2520reason%2520over%2520fine-grained%2520morphology%252C%2520and%2520generate%2520clinically%2520meaningful%2520descriptions.%2520We%2520introduce%2520DermaBench%252C%2520a%2520clinician-annotated%2520dermatology%2520VQA%2520benchmark%2520built%2520on%2520the%2520Diverse%2520Dermatology%2520Images%2520%2528DDI%2529%2520dataset.%2520DermaBench%2520comprises%2520656%2520clinical%2520images%2520from%2520570%2520unique%2520patients%2520spanning%2520Fitzpatrick%2520skin%2520types%2520I-VI.%2520Using%2520a%2520hierarchical%2520annotation%2520schema%2520with%252022%2520main%2520questions%2520%2528single-choice%252C%2520multi-choice%252C%2520and%2520open-ended%2529%252C%2520expert%2520dermatologists%2520annotated%2520each%2520image%2520for%2520diagnosis%252C%2520anatomic%2520site%252C%2520lesion%2520morphology%252C%2520distribution%252C%2520surface%2520features%252C%2520color%252C%2520and%2520image%2520quality%252C%2520together%2520with%2520open-ended%2520narrative%2520descriptions%2520and%2520summaries%252C%2520yielding%2520approximately%252014.474%2520VQA-style%2520annotations.%2520DermaBench%2520is%2520released%2520as%2520a%2520metadata-only%2520dataset%2520to%2520respect%2520upstream%2520licensing%2520and%2520is%2520publicly%2520available%2520at%2520Harvard%2520Dataverse.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14084v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DermaBench%3A%20A%20Clinician-Annotated%20Benchmark%20Dataset%20for%20Dermatology%20Visual%20Question%20Answering%20and%20Reasoning&entry.906535625=Abdurrahim%20Yilmaz%20and%20Ozan%20Erdem%20and%20Ece%20Gokyayla%20and%20Ayda%20Acar%20and%20Burc%20Bugra%20Dagtas%20and%20Dilara%20Ilhan%20Erdil%20and%20Gulsum%20Gencoglan%20and%20Burak%20Temelkuran&entry.1292438233=Vision-language%20models%20%28VLMs%29%20are%20increasingly%20important%20in%20medical%20applications%3B%20however%2C%20their%20evaluation%20in%20dermatology%20remains%20limited%20by%20datasets%20that%20focus%20primarily%20on%20image-level%20classification%20tasks%20such%20as%20lesion%20recognition.%20While%20valuable%20for%20recognition%2C%20such%20datasets%20cannot%20assess%20the%20full%20visual%20understanding%2C%20language%20grounding%2C%20and%20clinical%20reasoning%20capabilities%20of%20multimodal%20models.%20Visual%20question%20answering%20%28VQA%29%20benchmarks%20are%20required%20to%20evaluate%20how%20models%20interpret%20dermatological%20images%2C%20reason%20over%20fine-grained%20morphology%2C%20and%20generate%20clinically%20meaningful%20descriptions.%20We%20introduce%20DermaBench%2C%20a%20clinician-annotated%20dermatology%20VQA%20benchmark%20built%20on%20the%20Diverse%20Dermatology%20Images%20%28DDI%29%20dataset.%20DermaBench%20comprises%20656%20clinical%20images%20from%20570%20unique%20patients%20spanning%20Fitzpatrick%20skin%20types%20I-VI.%20Using%20a%20hierarchical%20annotation%20schema%20with%2022%20main%20questions%20%28single-choice%2C%20multi-choice%2C%20and%20open-ended%29%2C%20expert%20dermatologists%20annotated%20each%20image%20for%20diagnosis%2C%20anatomic%20site%2C%20lesion%20morphology%2C%20distribution%2C%20surface%20features%2C%20color%2C%20and%20image%20quality%2C%20together%20with%20open-ended%20narrative%20descriptions%20and%20summaries%2C%20yielding%20approximately%2014.474%20VQA-style%20annotations.%20DermaBench%20is%20released%20as%20a%20metadata-only%20dataset%20to%20respect%20upstream%20licensing%20and%20is%20publicly%20available%20at%20Harvard%20Dataverse.&entry.1838667208=http%3A//arxiv.org/abs/2601.14084v1&entry.124074799=Read"},
{"title": "Shuttling Compiler for Trapped-Ion Quantum Computers Based on Large Language Models", "author": "Fabian Kreppel and Reza Salkhordeh and Ferdinand Schmidt-Kaler and Andr\u00e9 Brinkmann", "abstract": "Trapped-ion quantum computers based on segmented traps rely on shuttling operations to establish long-range connectivity between sub-registers. Qubit routing dynamically reconfigures qubit positions so that all qubits involved in a gate operation are co-located within the same segment, a task whose complexity increases with system size. To address this challenge, we propose a layout-independent compilation strategy based on large language models (LLMs). Specifically, we fine-tune pretrained LLMs to generate the required shuttling operations. We evaluate this approach on linear and branched one-dimensional architectures using quantum circuits of up to $16$ qubits. Our results show that the fine-tuned LLMs generate valid shuttling schedules and, in some cases, outperform previous shuttling compilers by requiring approximately $15\\,\\%$ less shuttle overhead. However, results degrade as the algorithms increase in width and depth. In future, we plan to improve LLM-based shuttle compilation by enhancing our training pipeline using Direct Preference Optimization (DPO) and Gradient Regularized Policy Optimization (GRPO).", "link": "http://arxiv.org/abs/2512.18021v2", "date": "2026-01-20", "relevancy": 2.0742, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4151}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4151}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4143}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Shuttling%20Compiler%20for%20Trapped-Ion%20Quantum%20Computers%20Based%20on%20Large%20Language%20Models&body=Title%3A%20Shuttling%20Compiler%20for%20Trapped-Ion%20Quantum%20Computers%20Based%20on%20Large%20Language%20Models%0AAuthor%3A%20Fabian%20Kreppel%20and%20Reza%20Salkhordeh%20and%20Ferdinand%20Schmidt-Kaler%20and%20Andr%C3%A9%20Brinkmann%0AAbstract%3A%20Trapped-ion%20quantum%20computers%20based%20on%20segmented%20traps%20rely%20on%20shuttling%20operations%20to%20establish%20long-range%20connectivity%20between%20sub-registers.%20Qubit%20routing%20dynamically%20reconfigures%20qubit%20positions%20so%20that%20all%20qubits%20involved%20in%20a%20gate%20operation%20are%20co-located%20within%20the%20same%20segment%2C%20a%20task%20whose%20complexity%20increases%20with%20system%20size.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20layout-independent%20compilation%20strategy%20based%20on%20large%20language%20models%20%28LLMs%29.%20Specifically%2C%20we%20fine-tune%20pretrained%20LLMs%20to%20generate%20the%20required%20shuttling%20operations.%20We%20evaluate%20this%20approach%20on%20linear%20and%20branched%20one-dimensional%20architectures%20using%20quantum%20circuits%20of%20up%20to%20%2416%24%20qubits.%20Our%20results%20show%20that%20the%20fine-tuned%20LLMs%20generate%20valid%20shuttling%20schedules%20and%2C%20in%20some%20cases%2C%20outperform%20previous%20shuttling%20compilers%20by%20requiring%20approximately%20%2415%5C%2C%5C%25%24%20less%20shuttle%20overhead.%20However%2C%20results%20degrade%20as%20the%20algorithms%20increase%20in%20width%20and%20depth.%20In%20future%2C%20we%20plan%20to%20improve%20LLM-based%20shuttle%20compilation%20by%20enhancing%20our%20training%20pipeline%20using%20Direct%20Preference%20Optimization%20%28DPO%29%20and%20Gradient%20Regularized%20Policy%20Optimization%20%28GRPO%29.%0ALink%3A%20http%3A//arxiv.org/abs/2512.18021v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DShuttling%2520Compiler%2520for%2520Trapped-Ion%2520Quantum%2520Computers%2520Based%2520on%2520Large%2520Language%2520Models%26entry.906535625%3DFabian%2520Kreppel%2520and%2520Reza%2520Salkhordeh%2520and%2520Ferdinand%2520Schmidt-Kaler%2520and%2520Andr%25C3%25A9%2520Brinkmann%26entry.1292438233%3DTrapped-ion%2520quantum%2520computers%2520based%2520on%2520segmented%2520traps%2520rely%2520on%2520shuttling%2520operations%2520to%2520establish%2520long-range%2520connectivity%2520between%2520sub-registers.%2520Qubit%2520routing%2520dynamically%2520reconfigures%2520qubit%2520positions%2520so%2520that%2520all%2520qubits%2520involved%2520in%2520a%2520gate%2520operation%2520are%2520co-located%2520within%2520the%2520same%2520segment%252C%2520a%2520task%2520whose%2520complexity%2520increases%2520with%2520system%2520size.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520a%2520layout-independent%2520compilation%2520strategy%2520based%2520on%2520large%2520language%2520models%2520%2528LLMs%2529.%2520Specifically%252C%2520we%2520fine-tune%2520pretrained%2520LLMs%2520to%2520generate%2520the%2520required%2520shuttling%2520operations.%2520We%2520evaluate%2520this%2520approach%2520on%2520linear%2520and%2520branched%2520one-dimensional%2520architectures%2520using%2520quantum%2520circuits%2520of%2520up%2520to%2520%252416%2524%2520qubits.%2520Our%2520results%2520show%2520that%2520the%2520fine-tuned%2520LLMs%2520generate%2520valid%2520shuttling%2520schedules%2520and%252C%2520in%2520some%2520cases%252C%2520outperform%2520previous%2520shuttling%2520compilers%2520by%2520requiring%2520approximately%2520%252415%255C%252C%255C%2525%2524%2520less%2520shuttle%2520overhead.%2520However%252C%2520results%2520degrade%2520as%2520the%2520algorithms%2520increase%2520in%2520width%2520and%2520depth.%2520In%2520future%252C%2520we%2520plan%2520to%2520improve%2520LLM-based%2520shuttle%2520compilation%2520by%2520enhancing%2520our%2520training%2520pipeline%2520using%2520Direct%2520Preference%2520Optimization%2520%2528DPO%2529%2520and%2520Gradient%2520Regularized%2520Policy%2520Optimization%2520%2528GRPO%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.18021v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Shuttling%20Compiler%20for%20Trapped-Ion%20Quantum%20Computers%20Based%20on%20Large%20Language%20Models&entry.906535625=Fabian%20Kreppel%20and%20Reza%20Salkhordeh%20and%20Ferdinand%20Schmidt-Kaler%20and%20Andr%C3%A9%20Brinkmann&entry.1292438233=Trapped-ion%20quantum%20computers%20based%20on%20segmented%20traps%20rely%20on%20shuttling%20operations%20to%20establish%20long-range%20connectivity%20between%20sub-registers.%20Qubit%20routing%20dynamically%20reconfigures%20qubit%20positions%20so%20that%20all%20qubits%20involved%20in%20a%20gate%20operation%20are%20co-located%20within%20the%20same%20segment%2C%20a%20task%20whose%20complexity%20increases%20with%20system%20size.%20To%20address%20this%20challenge%2C%20we%20propose%20a%20layout-independent%20compilation%20strategy%20based%20on%20large%20language%20models%20%28LLMs%29.%20Specifically%2C%20we%20fine-tune%20pretrained%20LLMs%20to%20generate%20the%20required%20shuttling%20operations.%20We%20evaluate%20this%20approach%20on%20linear%20and%20branched%20one-dimensional%20architectures%20using%20quantum%20circuits%20of%20up%20to%20%2416%24%20qubits.%20Our%20results%20show%20that%20the%20fine-tuned%20LLMs%20generate%20valid%20shuttling%20schedules%20and%2C%20in%20some%20cases%2C%20outperform%20previous%20shuttling%20compilers%20by%20requiring%20approximately%20%2415%5C%2C%5C%25%24%20less%20shuttle%20overhead.%20However%2C%20results%20degrade%20as%20the%20algorithms%20increase%20in%20width%20and%20depth.%20In%20future%2C%20we%20plan%20to%20improve%20LLM-based%20shuttle%20compilation%20by%20enhancing%20our%20training%20pipeline%20using%20Direct%20Preference%20Optimization%20%28DPO%29%20and%20Gradient%20Regularized%20Policy%20Optimization%20%28GRPO%29.&entry.1838667208=http%3A//arxiv.org/abs/2512.18021v2&entry.124074799=Read"},
{"title": "Toward Efficient Agents: Memory, Tool learning, and Planning", "author": "Xiaofang Yang and Lijun Li and Heng Zhou and Tong Zhu and Xiaoye Qu and Yuchen Fan and Qianshan Wei and Rui Ye and Li Kang and Yiran Qin and Zhiqiang Kou and Daizong Liu and Qi Li and Ning Ding and Siheng Chen and Jing Shao", "abstract": "Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.", "link": "http://arxiv.org/abs/2601.14192v1", "date": "2026-01-20", "relevancy": 2.0611, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5608}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5118}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4712}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Efficient%20Agents%3A%20Memory%2C%20Tool%20learning%2C%20and%20Planning&body=Title%3A%20Toward%20Efficient%20Agents%3A%20Memory%2C%20Tool%20learning%2C%20and%20Planning%0AAuthor%3A%20Xiaofang%20Yang%20and%20Lijun%20Li%20and%20Heng%20Zhou%20and%20Tong%20Zhu%20and%20Xiaoye%20Qu%20and%20Yuchen%20Fan%20and%20Qianshan%20Wei%20and%20Rui%20Ye%20and%20Li%20Kang%20and%20Yiran%20Qin%20and%20Zhiqiang%20Kou%20and%20Daizong%20Liu%20and%20Qi%20Li%20and%20Ning%20Ding%20and%20Siheng%20Chen%20and%20Jing%20Shao%0AAbstract%3A%20Recent%20years%20have%20witnessed%20increasing%20interest%20in%20extending%20large%20language%20models%20into%20agentic%20systems.%20While%20the%20effectiveness%20of%20agents%20has%20continued%20to%20improve%2C%20efficiency%2C%20which%20is%20crucial%20for%20real-world%20deployment%2C%20has%20often%20been%20overlooked.%20This%20paper%20therefore%20investigates%20efficiency%20from%20three%20core%20components%20of%20agents%3A%20memory%2C%20tool%20learning%2C%20and%20planning%2C%20considering%20costs%20such%20as%20latency%2C%20tokens%2C%20steps%2C%20etc.%20Aimed%20at%20conducting%20comprehensive%20research%20addressing%20the%20efficiency%20of%20the%20agentic%20system%20itself%2C%20we%20review%20a%20broad%20range%20of%20recent%20approaches%20that%20differ%20in%20implementation%20yet%20frequently%20converge%20on%20shared%20high-level%20principles%20including%20but%20not%20limited%20to%20bounding%20context%20via%20compression%20and%20management%2C%20designing%20reinforcement%20learning%20rewards%20to%20minimize%20tool%20invocation%2C%20and%20employing%20controlled%20search%20mechanisms%20to%20enhance%20efficiency%2C%20which%20we%20discuss%20in%20detail.%20Accordingly%2C%20we%20characterize%20efficiency%20in%20two%20complementary%20ways%3A%20comparing%20effectiveness%20under%20a%20fixed%20cost%20budget%2C%20and%20comparing%20cost%20at%20a%20comparable%20level%20of%20effectiveness.%20This%20trade-off%20can%20also%20be%20viewed%20through%20the%20Pareto%20frontier%20between%20effectiveness%20and%20cost.%20From%20this%20perspective%2C%20we%20also%20examine%20efficiency%20oriented%20benchmarks%20by%20summarizing%20evaluation%20protocols%20for%20these%20components%20and%20consolidating%20commonly%20reported%20efficiency%20metrics%20from%20both%20benchmark%20and%20methodological%20studies.%20Moreover%2C%20we%20discuss%20the%20key%20challenges%20and%20future%20directions%2C%20with%20the%20goal%20of%20providing%20promising%20insights.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14192v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Efficient%2520Agents%253A%2520Memory%252C%2520Tool%2520learning%252C%2520and%2520Planning%26entry.906535625%3DXiaofang%2520Yang%2520and%2520Lijun%2520Li%2520and%2520Heng%2520Zhou%2520and%2520Tong%2520Zhu%2520and%2520Xiaoye%2520Qu%2520and%2520Yuchen%2520Fan%2520and%2520Qianshan%2520Wei%2520and%2520Rui%2520Ye%2520and%2520Li%2520Kang%2520and%2520Yiran%2520Qin%2520and%2520Zhiqiang%2520Kou%2520and%2520Daizong%2520Liu%2520and%2520Qi%2520Li%2520and%2520Ning%2520Ding%2520and%2520Siheng%2520Chen%2520and%2520Jing%2520Shao%26entry.1292438233%3DRecent%2520years%2520have%2520witnessed%2520increasing%2520interest%2520in%2520extending%2520large%2520language%2520models%2520into%2520agentic%2520systems.%2520While%2520the%2520effectiveness%2520of%2520agents%2520has%2520continued%2520to%2520improve%252C%2520efficiency%252C%2520which%2520is%2520crucial%2520for%2520real-world%2520deployment%252C%2520has%2520often%2520been%2520overlooked.%2520This%2520paper%2520therefore%2520investigates%2520efficiency%2520from%2520three%2520core%2520components%2520of%2520agents%253A%2520memory%252C%2520tool%2520learning%252C%2520and%2520planning%252C%2520considering%2520costs%2520such%2520as%2520latency%252C%2520tokens%252C%2520steps%252C%2520etc.%2520Aimed%2520at%2520conducting%2520comprehensive%2520research%2520addressing%2520the%2520efficiency%2520of%2520the%2520agentic%2520system%2520itself%252C%2520we%2520review%2520a%2520broad%2520range%2520of%2520recent%2520approaches%2520that%2520differ%2520in%2520implementation%2520yet%2520frequently%2520converge%2520on%2520shared%2520high-level%2520principles%2520including%2520but%2520not%2520limited%2520to%2520bounding%2520context%2520via%2520compression%2520and%2520management%252C%2520designing%2520reinforcement%2520learning%2520rewards%2520to%2520minimize%2520tool%2520invocation%252C%2520and%2520employing%2520controlled%2520search%2520mechanisms%2520to%2520enhance%2520efficiency%252C%2520which%2520we%2520discuss%2520in%2520detail.%2520Accordingly%252C%2520we%2520characterize%2520efficiency%2520in%2520two%2520complementary%2520ways%253A%2520comparing%2520effectiveness%2520under%2520a%2520fixed%2520cost%2520budget%252C%2520and%2520comparing%2520cost%2520at%2520a%2520comparable%2520level%2520of%2520effectiveness.%2520This%2520trade-off%2520can%2520also%2520be%2520viewed%2520through%2520the%2520Pareto%2520frontier%2520between%2520effectiveness%2520and%2520cost.%2520From%2520this%2520perspective%252C%2520we%2520also%2520examine%2520efficiency%2520oriented%2520benchmarks%2520by%2520summarizing%2520evaluation%2520protocols%2520for%2520these%2520components%2520and%2520consolidating%2520commonly%2520reported%2520efficiency%2520metrics%2520from%2520both%2520benchmark%2520and%2520methodological%2520studies.%2520Moreover%252C%2520we%2520discuss%2520the%2520key%2520challenges%2520and%2520future%2520directions%252C%2520with%2520the%2520goal%2520of%2520providing%2520promising%2520insights.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14192v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Efficient%20Agents%3A%20Memory%2C%20Tool%20learning%2C%20and%20Planning&entry.906535625=Xiaofang%20Yang%20and%20Lijun%20Li%20and%20Heng%20Zhou%20and%20Tong%20Zhu%20and%20Xiaoye%20Qu%20and%20Yuchen%20Fan%20and%20Qianshan%20Wei%20and%20Rui%20Ye%20and%20Li%20Kang%20and%20Yiran%20Qin%20and%20Zhiqiang%20Kou%20and%20Daizong%20Liu%20and%20Qi%20Li%20and%20Ning%20Ding%20and%20Siheng%20Chen%20and%20Jing%20Shao&entry.1292438233=Recent%20years%20have%20witnessed%20increasing%20interest%20in%20extending%20large%20language%20models%20into%20agentic%20systems.%20While%20the%20effectiveness%20of%20agents%20has%20continued%20to%20improve%2C%20efficiency%2C%20which%20is%20crucial%20for%20real-world%20deployment%2C%20has%20often%20been%20overlooked.%20This%20paper%20therefore%20investigates%20efficiency%20from%20three%20core%20components%20of%20agents%3A%20memory%2C%20tool%20learning%2C%20and%20planning%2C%20considering%20costs%20such%20as%20latency%2C%20tokens%2C%20steps%2C%20etc.%20Aimed%20at%20conducting%20comprehensive%20research%20addressing%20the%20efficiency%20of%20the%20agentic%20system%20itself%2C%20we%20review%20a%20broad%20range%20of%20recent%20approaches%20that%20differ%20in%20implementation%20yet%20frequently%20converge%20on%20shared%20high-level%20principles%20including%20but%20not%20limited%20to%20bounding%20context%20via%20compression%20and%20management%2C%20designing%20reinforcement%20learning%20rewards%20to%20minimize%20tool%20invocation%2C%20and%20employing%20controlled%20search%20mechanisms%20to%20enhance%20efficiency%2C%20which%20we%20discuss%20in%20detail.%20Accordingly%2C%20we%20characterize%20efficiency%20in%20two%20complementary%20ways%3A%20comparing%20effectiveness%20under%20a%20fixed%20cost%20budget%2C%20and%20comparing%20cost%20at%20a%20comparable%20level%20of%20effectiveness.%20This%20trade-off%20can%20also%20be%20viewed%20through%20the%20Pareto%20frontier%20between%20effectiveness%20and%20cost.%20From%20this%20perspective%2C%20we%20also%20examine%20efficiency%20oriented%20benchmarks%20by%20summarizing%20evaluation%20protocols%20for%20these%20components%20and%20consolidating%20commonly%20reported%20efficiency%20metrics%20from%20both%20benchmark%20and%20methodological%20studies.%20Moreover%2C%20we%20discuss%20the%20key%20challenges%20and%20future%20directions%2C%20with%20the%20goal%20of%20providing%20promising%20insights.&entry.1838667208=http%3A//arxiv.org/abs/2601.14192v1&entry.124074799=Read"},
{"title": "Unsupervised Video Class-Incremental Learning via Deep Embedded Clustering Management", "author": "Nattapong Kurpukdee and Adrian G. Bors", "abstract": "Unsupervised video class incremental learning (uVCIL) represents an important learning paradigm for learning video information without forgetting, and without considering any data labels. Prior approaches have focused on supervised class-incremental learning, relying on using the knowledge of labels and task boundaries, which is costly, requires human annotation, or is simply not a realistic option. In this paper, we propose a simple yet effective approach to address the uVCIL. We first consider a deep feature extractor network, providing a set of representative video features during each task without assuming any class or task information. We then progressively build a series of deep clusters from the extracted features. During the successive task learning, the model updated from the previous task is used as an initial state in order to transfer knowledge to the current learning task. We perform in-depth evaluations on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, by ignoring the labels from the supervised setting. Our approach significantly outperforms other baselines on all datasets.", "link": "http://arxiv.org/abs/2601.14069v1", "date": "2026-01-20", "relevancy": 2.0395, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5197}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5177}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4981}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Video%20Class-Incremental%20Learning%20via%20Deep%20Embedded%20Clustering%20Management&body=Title%3A%20Unsupervised%20Video%20Class-Incremental%20Learning%20via%20Deep%20Embedded%20Clustering%20Management%0AAuthor%3A%20Nattapong%20Kurpukdee%20and%20Adrian%20G.%20Bors%0AAbstract%3A%20Unsupervised%20video%20class%20incremental%20learning%20%28uVCIL%29%20represents%20an%20important%20learning%20paradigm%20for%20learning%20video%20information%20without%20forgetting%2C%20and%20without%20considering%20any%20data%20labels.%20Prior%20approaches%20have%20focused%20on%20supervised%20class-incremental%20learning%2C%20relying%20on%20using%20the%20knowledge%20of%20labels%20and%20task%20boundaries%2C%20which%20is%20costly%2C%20requires%20human%20annotation%2C%20or%20is%20simply%20not%20a%20realistic%20option.%20In%20this%20paper%2C%20we%20propose%20a%20simple%20yet%20effective%20approach%20to%20address%20the%20uVCIL.%20We%20first%20consider%20a%20deep%20feature%20extractor%20network%2C%20providing%20a%20set%20of%20representative%20video%20features%20during%20each%20task%20without%20assuming%20any%20class%20or%20task%20information.%20We%20then%20progressively%20build%20a%20series%20of%20deep%20clusters%20from%20the%20extracted%20features.%20During%20the%20successive%20task%20learning%2C%20the%20model%20updated%20from%20the%20previous%20task%20is%20used%20as%20an%20initial%20state%20in%20order%20to%20transfer%20knowledge%20to%20the%20current%20learning%20task.%20We%20perform%20in-depth%20evaluations%20on%20three%20standard%20video%20action%20recognition%20datasets%2C%20including%20UCF101%2C%20HMDB51%2C%20and%20Something-to-Something%20V2%2C%20by%20ignoring%20the%20labels%20from%20the%20supervised%20setting.%20Our%20approach%20significantly%20outperforms%20other%20baselines%20on%20all%20datasets.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14069v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Video%2520Class-Incremental%2520Learning%2520via%2520Deep%2520Embedded%2520Clustering%2520Management%26entry.906535625%3DNattapong%2520Kurpukdee%2520and%2520Adrian%2520G.%2520Bors%26entry.1292438233%3DUnsupervised%2520video%2520class%2520incremental%2520learning%2520%2528uVCIL%2529%2520represents%2520an%2520important%2520learning%2520paradigm%2520for%2520learning%2520video%2520information%2520without%2520forgetting%252C%2520and%2520without%2520considering%2520any%2520data%2520labels.%2520Prior%2520approaches%2520have%2520focused%2520on%2520supervised%2520class-incremental%2520learning%252C%2520relying%2520on%2520using%2520the%2520knowledge%2520of%2520labels%2520and%2520task%2520boundaries%252C%2520which%2520is%2520costly%252C%2520requires%2520human%2520annotation%252C%2520or%2520is%2520simply%2520not%2520a%2520realistic%2520option.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520simple%2520yet%2520effective%2520approach%2520to%2520address%2520the%2520uVCIL.%2520We%2520first%2520consider%2520a%2520deep%2520feature%2520extractor%2520network%252C%2520providing%2520a%2520set%2520of%2520representative%2520video%2520features%2520during%2520each%2520task%2520without%2520assuming%2520any%2520class%2520or%2520task%2520information.%2520We%2520then%2520progressively%2520build%2520a%2520series%2520of%2520deep%2520clusters%2520from%2520the%2520extracted%2520features.%2520During%2520the%2520successive%2520task%2520learning%252C%2520the%2520model%2520updated%2520from%2520the%2520previous%2520task%2520is%2520used%2520as%2520an%2520initial%2520state%2520in%2520order%2520to%2520transfer%2520knowledge%2520to%2520the%2520current%2520learning%2520task.%2520We%2520perform%2520in-depth%2520evaluations%2520on%2520three%2520standard%2520video%2520action%2520recognition%2520datasets%252C%2520including%2520UCF101%252C%2520HMDB51%252C%2520and%2520Something-to-Something%2520V2%252C%2520by%2520ignoring%2520the%2520labels%2520from%2520the%2520supervised%2520setting.%2520Our%2520approach%2520significantly%2520outperforms%2520other%2520baselines%2520on%2520all%2520datasets.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14069v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Video%20Class-Incremental%20Learning%20via%20Deep%20Embedded%20Clustering%20Management&entry.906535625=Nattapong%20Kurpukdee%20and%20Adrian%20G.%20Bors&entry.1292438233=Unsupervised%20video%20class%20incremental%20learning%20%28uVCIL%29%20represents%20an%20important%20learning%20paradigm%20for%20learning%20video%20information%20without%20forgetting%2C%20and%20without%20considering%20any%20data%20labels.%20Prior%20approaches%20have%20focused%20on%20supervised%20class-incremental%20learning%2C%20relying%20on%20using%20the%20knowledge%20of%20labels%20and%20task%20boundaries%2C%20which%20is%20costly%2C%20requires%20human%20annotation%2C%20or%20is%20simply%20not%20a%20realistic%20option.%20In%20this%20paper%2C%20we%20propose%20a%20simple%20yet%20effective%20approach%20to%20address%20the%20uVCIL.%20We%20first%20consider%20a%20deep%20feature%20extractor%20network%2C%20providing%20a%20set%20of%20representative%20video%20features%20during%20each%20task%20without%20assuming%20any%20class%20or%20task%20information.%20We%20then%20progressively%20build%20a%20series%20of%20deep%20clusters%20from%20the%20extracted%20features.%20During%20the%20successive%20task%20learning%2C%20the%20model%20updated%20from%20the%20previous%20task%20is%20used%20as%20an%20initial%20state%20in%20order%20to%20transfer%20knowledge%20to%20the%20current%20learning%20task.%20We%20perform%20in-depth%20evaluations%20on%20three%20standard%20video%20action%20recognition%20datasets%2C%20including%20UCF101%2C%20HMDB51%2C%20and%20Something-to-Something%20V2%2C%20by%20ignoring%20the%20labels%20from%20the%20supervised%20setting.%20Our%20approach%20significantly%20outperforms%20other%20baselines%20on%20all%20datasets.&entry.1838667208=http%3A//arxiv.org/abs/2601.14069v1&entry.124074799=Read"},
{"title": "Federated Unsupervised Semantic Segmentation", "author": "Evangelos Charalampakis and Vasileios Mygdalis and Ioannis Pitas", "abstract": "This work explores the application of Federated Learning (FL) to Unsupervised Semantic image Segmentation (USS). Recent USS methods extract pixel-level features using frozen visual foundation models and refine them through self-supervised objectives that encourage semantic grouping. These features are then grouped to semantic clusters to produce segmentation masks. Extending these ideas to federated settings requires feature representation and cluster centroid alignment across distributed clients, an inherently difficult task under heterogeneous data distributions in the absence of supervision. To address this, we propose FUSS (Federated Unsupervised image Semantic Segmentation) which is, to our knowledge, the first framework to enable fully decentralized, label-free semantic segmentation training. FUSS introduces novel federation strategies that promote global consistency in feature and prototype space, jointly optimizing local segmentation heads and shared semantic centroids. Experiments on both benchmark and real-world datasets, including binary and multi-class segmentation tasks, show that FUSS consistently outperforms local-only client trainings as well as extensions of classical FL algorithms under varying client data distributions. To fully support reproducibility, the source code, data partitioning scripts, and implementation details are publicly available at: https://github.com/evanchar/FUSS", "link": "http://arxiv.org/abs/2505.23292v2", "date": "2026-01-20", "relevancy": 2.0369, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5353}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5016}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4863}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Federated%20Unsupervised%20Semantic%20Segmentation&body=Title%3A%20Federated%20Unsupervised%20Semantic%20Segmentation%0AAuthor%3A%20Evangelos%20Charalampakis%20and%20Vasileios%20Mygdalis%20and%20Ioannis%20Pitas%0AAbstract%3A%20This%20work%20explores%20the%20application%20of%20Federated%20Learning%20%28FL%29%20to%20Unsupervised%20Semantic%20image%20Segmentation%20%28USS%29.%20Recent%20USS%20methods%20extract%20pixel-level%20features%20using%20frozen%20visual%20foundation%20models%20and%20refine%20them%20through%20self-supervised%20objectives%20that%20encourage%20semantic%20grouping.%20These%20features%20are%20then%20grouped%20to%20semantic%20clusters%20to%20produce%20segmentation%20masks.%20Extending%20these%20ideas%20to%20federated%20settings%20requires%20feature%20representation%20and%20cluster%20centroid%20alignment%20across%20distributed%20clients%2C%20an%20inherently%20difficult%20task%20under%20heterogeneous%20data%20distributions%20in%20the%20absence%20of%20supervision.%20To%20address%20this%2C%20we%20propose%20FUSS%20%28Federated%20Unsupervised%20image%20Semantic%20Segmentation%29%20which%20is%2C%20to%20our%20knowledge%2C%20the%20first%20framework%20to%20enable%20fully%20decentralized%2C%20label-free%20semantic%20segmentation%20training.%20FUSS%20introduces%20novel%20federation%20strategies%20that%20promote%20global%20consistency%20in%20feature%20and%20prototype%20space%2C%20jointly%20optimizing%20local%20segmentation%20heads%20and%20shared%20semantic%20centroids.%20Experiments%20on%20both%20benchmark%20and%20real-world%20datasets%2C%20including%20binary%20and%20multi-class%20segmentation%20tasks%2C%20show%20that%20FUSS%20consistently%20outperforms%20local-only%20client%20trainings%20as%20well%20as%20extensions%20of%20classical%20FL%20algorithms%20under%20varying%20client%20data%20distributions.%20To%20fully%20support%20reproducibility%2C%20the%20source%20code%2C%20data%20partitioning%20scripts%2C%20and%20implementation%20details%20are%20publicly%20available%20at%3A%20https%3A//github.com/evanchar/FUSS%0ALink%3A%20http%3A//arxiv.org/abs/2505.23292v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFederated%2520Unsupervised%2520Semantic%2520Segmentation%26entry.906535625%3DEvangelos%2520Charalampakis%2520and%2520Vasileios%2520Mygdalis%2520and%2520Ioannis%2520Pitas%26entry.1292438233%3DThis%2520work%2520explores%2520the%2520application%2520of%2520Federated%2520Learning%2520%2528FL%2529%2520to%2520Unsupervised%2520Semantic%2520image%2520Segmentation%2520%2528USS%2529.%2520Recent%2520USS%2520methods%2520extract%2520pixel-level%2520features%2520using%2520frozen%2520visual%2520foundation%2520models%2520and%2520refine%2520them%2520through%2520self-supervised%2520objectives%2520that%2520encourage%2520semantic%2520grouping.%2520These%2520features%2520are%2520then%2520grouped%2520to%2520semantic%2520clusters%2520to%2520produce%2520segmentation%2520masks.%2520Extending%2520these%2520ideas%2520to%2520federated%2520settings%2520requires%2520feature%2520representation%2520and%2520cluster%2520centroid%2520alignment%2520across%2520distributed%2520clients%252C%2520an%2520inherently%2520difficult%2520task%2520under%2520heterogeneous%2520data%2520distributions%2520in%2520the%2520absence%2520of%2520supervision.%2520To%2520address%2520this%252C%2520we%2520propose%2520FUSS%2520%2528Federated%2520Unsupervised%2520image%2520Semantic%2520Segmentation%2529%2520which%2520is%252C%2520to%2520our%2520knowledge%252C%2520the%2520first%2520framework%2520to%2520enable%2520fully%2520decentralized%252C%2520label-free%2520semantic%2520segmentation%2520training.%2520FUSS%2520introduces%2520novel%2520federation%2520strategies%2520that%2520promote%2520global%2520consistency%2520in%2520feature%2520and%2520prototype%2520space%252C%2520jointly%2520optimizing%2520local%2520segmentation%2520heads%2520and%2520shared%2520semantic%2520centroids.%2520Experiments%2520on%2520both%2520benchmark%2520and%2520real-world%2520datasets%252C%2520including%2520binary%2520and%2520multi-class%2520segmentation%2520tasks%252C%2520show%2520that%2520FUSS%2520consistently%2520outperforms%2520local-only%2520client%2520trainings%2520as%2520well%2520as%2520extensions%2520of%2520classical%2520FL%2520algorithms%2520under%2520varying%2520client%2520data%2520distributions.%2520To%2520fully%2520support%2520reproducibility%252C%2520the%2520source%2520code%252C%2520data%2520partitioning%2520scripts%252C%2520and%2520implementation%2520details%2520are%2520publicly%2520available%2520at%253A%2520https%253A//github.com/evanchar/FUSS%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.23292v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Federated%20Unsupervised%20Semantic%20Segmentation&entry.906535625=Evangelos%20Charalampakis%20and%20Vasileios%20Mygdalis%20and%20Ioannis%20Pitas&entry.1292438233=This%20work%20explores%20the%20application%20of%20Federated%20Learning%20%28FL%29%20to%20Unsupervised%20Semantic%20image%20Segmentation%20%28USS%29.%20Recent%20USS%20methods%20extract%20pixel-level%20features%20using%20frozen%20visual%20foundation%20models%20and%20refine%20them%20through%20self-supervised%20objectives%20that%20encourage%20semantic%20grouping.%20These%20features%20are%20then%20grouped%20to%20semantic%20clusters%20to%20produce%20segmentation%20masks.%20Extending%20these%20ideas%20to%20federated%20settings%20requires%20feature%20representation%20and%20cluster%20centroid%20alignment%20across%20distributed%20clients%2C%20an%20inherently%20difficult%20task%20under%20heterogeneous%20data%20distributions%20in%20the%20absence%20of%20supervision.%20To%20address%20this%2C%20we%20propose%20FUSS%20%28Federated%20Unsupervised%20image%20Semantic%20Segmentation%29%20which%20is%2C%20to%20our%20knowledge%2C%20the%20first%20framework%20to%20enable%20fully%20decentralized%2C%20label-free%20semantic%20segmentation%20training.%20FUSS%20introduces%20novel%20federation%20strategies%20that%20promote%20global%20consistency%20in%20feature%20and%20prototype%20space%2C%20jointly%20optimizing%20local%20segmentation%20heads%20and%20shared%20semantic%20centroids.%20Experiments%20on%20both%20benchmark%20and%20real-world%20datasets%2C%20including%20binary%20and%20multi-class%20segmentation%20tasks%2C%20show%20that%20FUSS%20consistently%20outperforms%20local-only%20client%20trainings%20as%20well%20as%20extensions%20of%20classical%20FL%20algorithms%20under%20varying%20client%20data%20distributions.%20To%20fully%20support%20reproducibility%2C%20the%20source%20code%2C%20data%20partitioning%20scripts%2C%20and%20implementation%20details%20are%20publicly%20available%20at%3A%20https%3A//github.com/evanchar/FUSS&entry.1838667208=http%3A//arxiv.org/abs/2505.23292v2&entry.124074799=Read"},
{"title": "Task-Aware Mixture-of-Experts for Time Series Analysis", "author": "Xingjian Wu and Zhengyu Li and Hanyin Cheng and Xiangfei Qiu and Jilin Hu and Chenjuan Guo and Bin Yang", "abstract": "Time Series Analysis is widely used in various real-world applications such as weather forecasting, financial fraud detection, imputation for missing data in IoT systems, and classification for action recognization. Mixture-of-Experts (MoE), as a powerful architecture, though demonstrating effectiveness in NLP, still falls short in adapting to versatile tasks in time series analytics due to its task-agnostic router and the lack of capability in modeling channel correlations. In this study, we propose a novel, general MoE-based time series framework called PatchMoE to support the intricate ``knowledge'' utilization for distinct tasks, thus task-aware. Based on the observation that hierarchical representations often vary across tasks, e.g., forecasting vs. classification, we propose a Recurrent Noisy Gating to utilize the hierarchical information in routing, thus obtaining task-sepcific capability. And the routing strategy is operated on time series tokens in both temporal and channel dimensions, and encouraged by a meticulously designed Temporal \\& Channel Load Balancing Loss to model the intricate temporal and channel correlations. Comprehensive experiments on five downstream tasks demonstrate the state-of-the-art performance of PatchMoE.", "link": "http://arxiv.org/abs/2509.22279v3", "date": "2026-01-20", "relevancy": 1.8329, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4604}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4588}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Task-Aware%20Mixture-of-Experts%20for%20Time%20Series%20Analysis&body=Title%3A%20Task-Aware%20Mixture-of-Experts%20for%20Time%20Series%20Analysis%0AAuthor%3A%20Xingjian%20Wu%20and%20Zhengyu%20Li%20and%20Hanyin%20Cheng%20and%20Xiangfei%20Qiu%20and%20Jilin%20Hu%20and%20Chenjuan%20Guo%20and%20Bin%20Yang%0AAbstract%3A%20Time%20Series%20Analysis%20is%20widely%20used%20in%20various%20real-world%20applications%20such%20as%20weather%20forecasting%2C%20financial%20fraud%20detection%2C%20imputation%20for%20missing%20data%20in%20IoT%20systems%2C%20and%20classification%20for%20action%20recognization.%20Mixture-of-Experts%20%28MoE%29%2C%20as%20a%20powerful%20architecture%2C%20though%20demonstrating%20effectiveness%20in%20NLP%2C%20still%20falls%20short%20in%20adapting%20to%20versatile%20tasks%20in%20time%20series%20analytics%20due%20to%20its%20task-agnostic%20router%20and%20the%20lack%20of%20capability%20in%20modeling%20channel%20correlations.%20In%20this%20study%2C%20we%20propose%20a%20novel%2C%20general%20MoE-based%20time%20series%20framework%20called%20PatchMoE%20to%20support%20the%20intricate%20%60%60knowledge%27%27%20utilization%20for%20distinct%20tasks%2C%20thus%20task-aware.%20Based%20on%20the%20observation%20that%20hierarchical%20representations%20often%20vary%20across%20tasks%2C%20e.g.%2C%20forecasting%20vs.%20classification%2C%20we%20propose%20a%20Recurrent%20Noisy%20Gating%20to%20utilize%20the%20hierarchical%20information%20in%20routing%2C%20thus%20obtaining%20task-sepcific%20capability.%20And%20the%20routing%20strategy%20is%20operated%20on%20time%20series%20tokens%20in%20both%20temporal%20and%20channel%20dimensions%2C%20and%20encouraged%20by%20a%20meticulously%20designed%20Temporal%20%5C%26%20Channel%20Load%20Balancing%20Loss%20to%20model%20the%20intricate%20temporal%20and%20channel%20correlations.%20Comprehensive%20experiments%20on%20five%20downstream%20tasks%20demonstrate%20the%20state-of-the-art%20performance%20of%20PatchMoE.%0ALink%3A%20http%3A//arxiv.org/abs/2509.22279v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTask-Aware%2520Mixture-of-Experts%2520for%2520Time%2520Series%2520Analysis%26entry.906535625%3DXingjian%2520Wu%2520and%2520Zhengyu%2520Li%2520and%2520Hanyin%2520Cheng%2520and%2520Xiangfei%2520Qiu%2520and%2520Jilin%2520Hu%2520and%2520Chenjuan%2520Guo%2520and%2520Bin%2520Yang%26entry.1292438233%3DTime%2520Series%2520Analysis%2520is%2520widely%2520used%2520in%2520various%2520real-world%2520applications%2520such%2520as%2520weather%2520forecasting%252C%2520financial%2520fraud%2520detection%252C%2520imputation%2520for%2520missing%2520data%2520in%2520IoT%2520systems%252C%2520and%2520classification%2520for%2520action%2520recognization.%2520Mixture-of-Experts%2520%2528MoE%2529%252C%2520as%2520a%2520powerful%2520architecture%252C%2520though%2520demonstrating%2520effectiveness%2520in%2520NLP%252C%2520still%2520falls%2520short%2520in%2520adapting%2520to%2520versatile%2520tasks%2520in%2520time%2520series%2520analytics%2520due%2520to%2520its%2520task-agnostic%2520router%2520and%2520the%2520lack%2520of%2520capability%2520in%2520modeling%2520channel%2520correlations.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520novel%252C%2520general%2520MoE-based%2520time%2520series%2520framework%2520called%2520PatchMoE%2520to%2520support%2520the%2520intricate%2520%2560%2560knowledge%2527%2527%2520utilization%2520for%2520distinct%2520tasks%252C%2520thus%2520task-aware.%2520Based%2520on%2520the%2520observation%2520that%2520hierarchical%2520representations%2520often%2520vary%2520across%2520tasks%252C%2520e.g.%252C%2520forecasting%2520vs.%2520classification%252C%2520we%2520propose%2520a%2520Recurrent%2520Noisy%2520Gating%2520to%2520utilize%2520the%2520hierarchical%2520information%2520in%2520routing%252C%2520thus%2520obtaining%2520task-sepcific%2520capability.%2520And%2520the%2520routing%2520strategy%2520is%2520operated%2520on%2520time%2520series%2520tokens%2520in%2520both%2520temporal%2520and%2520channel%2520dimensions%252C%2520and%2520encouraged%2520by%2520a%2520meticulously%2520designed%2520Temporal%2520%255C%2526%2520Channel%2520Load%2520Balancing%2520Loss%2520to%2520model%2520the%2520intricate%2520temporal%2520and%2520channel%2520correlations.%2520Comprehensive%2520experiments%2520on%2520five%2520downstream%2520tasks%2520demonstrate%2520the%2520state-of-the-art%2520performance%2520of%2520PatchMoE.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.22279v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Task-Aware%20Mixture-of-Experts%20for%20Time%20Series%20Analysis&entry.906535625=Xingjian%20Wu%20and%20Zhengyu%20Li%20and%20Hanyin%20Cheng%20and%20Xiangfei%20Qiu%20and%20Jilin%20Hu%20and%20Chenjuan%20Guo%20and%20Bin%20Yang&entry.1292438233=Time%20Series%20Analysis%20is%20widely%20used%20in%20various%20real-world%20applications%20such%20as%20weather%20forecasting%2C%20financial%20fraud%20detection%2C%20imputation%20for%20missing%20data%20in%20IoT%20systems%2C%20and%20classification%20for%20action%20recognization.%20Mixture-of-Experts%20%28MoE%29%2C%20as%20a%20powerful%20architecture%2C%20though%20demonstrating%20effectiveness%20in%20NLP%2C%20still%20falls%20short%20in%20adapting%20to%20versatile%20tasks%20in%20time%20series%20analytics%20due%20to%20its%20task-agnostic%20router%20and%20the%20lack%20of%20capability%20in%20modeling%20channel%20correlations.%20In%20this%20study%2C%20we%20propose%20a%20novel%2C%20general%20MoE-based%20time%20series%20framework%20called%20PatchMoE%20to%20support%20the%20intricate%20%60%60knowledge%27%27%20utilization%20for%20distinct%20tasks%2C%20thus%20task-aware.%20Based%20on%20the%20observation%20that%20hierarchical%20representations%20often%20vary%20across%20tasks%2C%20e.g.%2C%20forecasting%20vs.%20classification%2C%20we%20propose%20a%20Recurrent%20Noisy%20Gating%20to%20utilize%20the%20hierarchical%20information%20in%20routing%2C%20thus%20obtaining%20task-sepcific%20capability.%20And%20the%20routing%20strategy%20is%20operated%20on%20time%20series%20tokens%20in%20both%20temporal%20and%20channel%20dimensions%2C%20and%20encouraged%20by%20a%20meticulously%20designed%20Temporal%20%5C%26%20Channel%20Load%20Balancing%20Loss%20to%20model%20the%20intricate%20temporal%20and%20channel%20correlations.%20Comprehensive%20experiments%20on%20five%20downstream%20tasks%20demonstrate%20the%20state-of-the-art%20performance%20of%20PatchMoE.&entry.1838667208=http%3A//arxiv.org/abs/2509.22279v3&entry.124074799=Read"},
{"title": "Revitalizing Black-Box Interpretability: Actionable Interpretability for LLMs via Proxy Models", "author": "Junhao Liu and Haonan Yu and Zhenyu Yan and Xin Zhang", "abstract": "Post-hoc explanations provide transparency and are essential for guiding model optimization, such as prompt engineering and data sanitation. However, applying model-agnostic techniques to Large Language Models (LLMs) is hindered by prohibitive computational costs, rendering these tools dormant for real-world applications. To revitalize model-agnostic interpretability, we propose a budget-friendly proxy framework that leverages efficient models to approximate the decision boundaries of expensive LLMs. We introduce a screen-and-apply mechanism to statistically verify local alignment before deployment. Our empirical evaluation confirms that proxy explanations achieve over 90% fidelity with only 11% of the oracle's cost. Building on this foundation, we demonstrate the actionable utility of our framework in prompt compression and poisoned example removal. Results show that reliable proxy explanations effectively guide optimization, transforming interpretability from a passive observation tool into a scalable primitive for LLM development. Additionally, we open-source code and datasets to facilitate future research.", "link": "http://arxiv.org/abs/2505.12509v2", "date": "2026-01-20", "relevancy": 1.978, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4996}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4996}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4692}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Revitalizing%20Black-Box%20Interpretability%3A%20Actionable%20Interpretability%20for%20LLMs%20via%20Proxy%20Models&body=Title%3A%20Revitalizing%20Black-Box%20Interpretability%3A%20Actionable%20Interpretability%20for%20LLMs%20via%20Proxy%20Models%0AAuthor%3A%20Junhao%20Liu%20and%20Haonan%20Yu%20and%20Zhenyu%20Yan%20and%20Xin%20Zhang%0AAbstract%3A%20Post-hoc%20explanations%20provide%20transparency%20and%20are%20essential%20for%20guiding%20model%20optimization%2C%20such%20as%20prompt%20engineering%20and%20data%20sanitation.%20However%2C%20applying%20model-agnostic%20techniques%20to%20Large%20Language%20Models%20%28LLMs%29%20is%20hindered%20by%20prohibitive%20computational%20costs%2C%20rendering%20these%20tools%20dormant%20for%20real-world%20applications.%20To%20revitalize%20model-agnostic%20interpretability%2C%20we%20propose%20a%20budget-friendly%20proxy%20framework%20that%20leverages%20efficient%20models%20to%20approximate%20the%20decision%20boundaries%20of%20expensive%20LLMs.%20We%20introduce%20a%20screen-and-apply%20mechanism%20to%20statistically%20verify%20local%20alignment%20before%20deployment.%20Our%20empirical%20evaluation%20confirms%20that%20proxy%20explanations%20achieve%20over%2090%25%20fidelity%20with%20only%2011%25%20of%20the%20oracle%27s%20cost.%20Building%20on%20this%20foundation%2C%20we%20demonstrate%20the%20actionable%20utility%20of%20our%20framework%20in%20prompt%20compression%20and%20poisoned%20example%20removal.%20Results%20show%20that%20reliable%20proxy%20explanations%20effectively%20guide%20optimization%2C%20transforming%20interpretability%20from%20a%20passive%20observation%20tool%20into%20a%20scalable%20primitive%20for%20LLM%20development.%20Additionally%2C%20we%20open-source%20code%20and%20datasets%20to%20facilitate%20future%20research.%0ALink%3A%20http%3A//arxiv.org/abs/2505.12509v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRevitalizing%2520Black-Box%2520Interpretability%253A%2520Actionable%2520Interpretability%2520for%2520LLMs%2520via%2520Proxy%2520Models%26entry.906535625%3DJunhao%2520Liu%2520and%2520Haonan%2520Yu%2520and%2520Zhenyu%2520Yan%2520and%2520Xin%2520Zhang%26entry.1292438233%3DPost-hoc%2520explanations%2520provide%2520transparency%2520and%2520are%2520essential%2520for%2520guiding%2520model%2520optimization%252C%2520such%2520as%2520prompt%2520engineering%2520and%2520data%2520sanitation.%2520However%252C%2520applying%2520model-agnostic%2520techniques%2520to%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520is%2520hindered%2520by%2520prohibitive%2520computational%2520costs%252C%2520rendering%2520these%2520tools%2520dormant%2520for%2520real-world%2520applications.%2520To%2520revitalize%2520model-agnostic%2520interpretability%252C%2520we%2520propose%2520a%2520budget-friendly%2520proxy%2520framework%2520that%2520leverages%2520efficient%2520models%2520to%2520approximate%2520the%2520decision%2520boundaries%2520of%2520expensive%2520LLMs.%2520We%2520introduce%2520a%2520screen-and-apply%2520mechanism%2520to%2520statistically%2520verify%2520local%2520alignment%2520before%2520deployment.%2520Our%2520empirical%2520evaluation%2520confirms%2520that%2520proxy%2520explanations%2520achieve%2520over%252090%2525%2520fidelity%2520with%2520only%252011%2525%2520of%2520the%2520oracle%2527s%2520cost.%2520Building%2520on%2520this%2520foundation%252C%2520we%2520demonstrate%2520the%2520actionable%2520utility%2520of%2520our%2520framework%2520in%2520prompt%2520compression%2520and%2520poisoned%2520example%2520removal.%2520Results%2520show%2520that%2520reliable%2520proxy%2520explanations%2520effectively%2520guide%2520optimization%252C%2520transforming%2520interpretability%2520from%2520a%2520passive%2520observation%2520tool%2520into%2520a%2520scalable%2520primitive%2520for%2520LLM%2520development.%2520Additionally%252C%2520we%2520open-source%2520code%2520and%2520datasets%2520to%2520facilitate%2520future%2520research.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12509v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Revitalizing%20Black-Box%20Interpretability%3A%20Actionable%20Interpretability%20for%20LLMs%20via%20Proxy%20Models&entry.906535625=Junhao%20Liu%20and%20Haonan%20Yu%20and%20Zhenyu%20Yan%20and%20Xin%20Zhang&entry.1292438233=Post-hoc%20explanations%20provide%20transparency%20and%20are%20essential%20for%20guiding%20model%20optimization%2C%20such%20as%20prompt%20engineering%20and%20data%20sanitation.%20However%2C%20applying%20model-agnostic%20techniques%20to%20Large%20Language%20Models%20%28LLMs%29%20is%20hindered%20by%20prohibitive%20computational%20costs%2C%20rendering%20these%20tools%20dormant%20for%20real-world%20applications.%20To%20revitalize%20model-agnostic%20interpretability%2C%20we%20propose%20a%20budget-friendly%20proxy%20framework%20that%20leverages%20efficient%20models%20to%20approximate%20the%20decision%20boundaries%20of%20expensive%20LLMs.%20We%20introduce%20a%20screen-and-apply%20mechanism%20to%20statistically%20verify%20local%20alignment%20before%20deployment.%20Our%20empirical%20evaluation%20confirms%20that%20proxy%20explanations%20achieve%20over%2090%25%20fidelity%20with%20only%2011%25%20of%20the%20oracle%27s%20cost.%20Building%20on%20this%20foundation%2C%20we%20demonstrate%20the%20actionable%20utility%20of%20our%20framework%20in%20prompt%20compression%20and%20poisoned%20example%20removal.%20Results%20show%20that%20reliable%20proxy%20explanations%20effectively%20guide%20optimization%2C%20transforming%20interpretability%20from%20a%20passive%20observation%20tool%20into%20a%20scalable%20primitive%20for%20LLM%20development.%20Additionally%2C%20we%20open-source%20code%20and%20datasets%20to%20facilitate%20future%20research.&entry.1838667208=http%3A//arxiv.org/abs/2505.12509v2&entry.124074799=Read"},
{"title": "PREFAB: PREFerence-based Affective Modeling for Low-Budget Self-Annotation", "author": "Jaeyoung Moon and Youjin Choi and Yucheon Park and David Melhart and Georgios N. Yannakakis and Kyung-Joong Kim", "abstract": "Self-annotation is the gold standard for collecting affective state labels in affective computing. Existing methods typically rely on full annotation, requiring users to continuously label affective states across entire sessions. While this process yields fine-grained data, it is time-consuming, cognitively demanding, and prone to fatigue and errors. To address these issues, we present PREFAB, a low-budget retrospective self-annotation method that targets affective inflection regions rather than full annotation. Grounded in the peak-end rule and ordinal representations of emotion, PREFAB employs a preference-learning model to detect relative affective changes, directing annotators to label only selected segments while interpolating the remainder of the stimulus. We further introduce a preview mechanism that provides brief contextual cues to assist annotation. We evaluate PREFAB through a technical performance study and a 25-participant user study. Results show that PREFAB outperforms baselines in modeling affective inflections while mitigating workload (and conditionally mitigating temporal burden). Importantly PREFAB improves annotator confidence without degrading annotation quality.", "link": "http://arxiv.org/abs/2601.13904v1", "date": "2026-01-20", "relevancy": 1.5059, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5087}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4975}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4894}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PREFAB%3A%20PREFerence-based%20Affective%20Modeling%20for%20Low-Budget%20Self-Annotation&body=Title%3A%20PREFAB%3A%20PREFerence-based%20Affective%20Modeling%20for%20Low-Budget%20Self-Annotation%0AAuthor%3A%20Jaeyoung%20Moon%20and%20Youjin%20Choi%20and%20Yucheon%20Park%20and%20David%20Melhart%20and%20Georgios%20N.%20Yannakakis%20and%20Kyung-Joong%20Kim%0AAbstract%3A%20Self-annotation%20is%20the%20gold%20standard%20for%20collecting%20affective%20state%20labels%20in%20affective%20computing.%20Existing%20methods%20typically%20rely%20on%20full%20annotation%2C%20requiring%20users%20to%20continuously%20label%20affective%20states%20across%20entire%20sessions.%20While%20this%20process%20yields%20fine-grained%20data%2C%20it%20is%20time-consuming%2C%20cognitively%20demanding%2C%20and%20prone%20to%20fatigue%20and%20errors.%20To%20address%20these%20issues%2C%20we%20present%20PREFAB%2C%20a%20low-budget%20retrospective%20self-annotation%20method%20that%20targets%20affective%20inflection%20regions%20rather%20than%20full%20annotation.%20Grounded%20in%20the%20peak-end%20rule%20and%20ordinal%20representations%20of%20emotion%2C%20PREFAB%20employs%20a%20preference-learning%20model%20to%20detect%20relative%20affective%20changes%2C%20directing%20annotators%20to%20label%20only%20selected%20segments%20while%20interpolating%20the%20remainder%20of%20the%20stimulus.%20We%20further%20introduce%20a%20preview%20mechanism%20that%20provides%20brief%20contextual%20cues%20to%20assist%20annotation.%20We%20evaluate%20PREFAB%20through%20a%20technical%20performance%20study%20and%20a%2025-participant%20user%20study.%20Results%20show%20that%20PREFAB%20outperforms%20baselines%20in%20modeling%20affective%20inflections%20while%20mitigating%20workload%20%28and%20conditionally%20mitigating%20temporal%20burden%29.%20Importantly%20PREFAB%20improves%20annotator%20confidence%20without%20degrading%20annotation%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13904v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPREFAB%253A%2520PREFerence-based%2520Affective%2520Modeling%2520for%2520Low-Budget%2520Self-Annotation%26entry.906535625%3DJaeyoung%2520Moon%2520and%2520Youjin%2520Choi%2520and%2520Yucheon%2520Park%2520and%2520David%2520Melhart%2520and%2520Georgios%2520N.%2520Yannakakis%2520and%2520Kyung-Joong%2520Kim%26entry.1292438233%3DSelf-annotation%2520is%2520the%2520gold%2520standard%2520for%2520collecting%2520affective%2520state%2520labels%2520in%2520affective%2520computing.%2520Existing%2520methods%2520typically%2520rely%2520on%2520full%2520annotation%252C%2520requiring%2520users%2520to%2520continuously%2520label%2520affective%2520states%2520across%2520entire%2520sessions.%2520While%2520this%2520process%2520yields%2520fine-grained%2520data%252C%2520it%2520is%2520time-consuming%252C%2520cognitively%2520demanding%252C%2520and%2520prone%2520to%2520fatigue%2520and%2520errors.%2520To%2520address%2520these%2520issues%252C%2520we%2520present%2520PREFAB%252C%2520a%2520low-budget%2520retrospective%2520self-annotation%2520method%2520that%2520targets%2520affective%2520inflection%2520regions%2520rather%2520than%2520full%2520annotation.%2520Grounded%2520in%2520the%2520peak-end%2520rule%2520and%2520ordinal%2520representations%2520of%2520emotion%252C%2520PREFAB%2520employs%2520a%2520preference-learning%2520model%2520to%2520detect%2520relative%2520affective%2520changes%252C%2520directing%2520annotators%2520to%2520label%2520only%2520selected%2520segments%2520while%2520interpolating%2520the%2520remainder%2520of%2520the%2520stimulus.%2520We%2520further%2520introduce%2520a%2520preview%2520mechanism%2520that%2520provides%2520brief%2520contextual%2520cues%2520to%2520assist%2520annotation.%2520We%2520evaluate%2520PREFAB%2520through%2520a%2520technical%2520performance%2520study%2520and%2520a%252025-participant%2520user%2520study.%2520Results%2520show%2520that%2520PREFAB%2520outperforms%2520baselines%2520in%2520modeling%2520affective%2520inflections%2520while%2520mitigating%2520workload%2520%2528and%2520conditionally%2520mitigating%2520temporal%2520burden%2529.%2520Importantly%2520PREFAB%2520improves%2520annotator%2520confidence%2520without%2520degrading%2520annotation%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13904v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PREFAB%3A%20PREFerence-based%20Affective%20Modeling%20for%20Low-Budget%20Self-Annotation&entry.906535625=Jaeyoung%20Moon%20and%20Youjin%20Choi%20and%20Yucheon%20Park%20and%20David%20Melhart%20and%20Georgios%20N.%20Yannakakis%20and%20Kyung-Joong%20Kim&entry.1292438233=Self-annotation%20is%20the%20gold%20standard%20for%20collecting%20affective%20state%20labels%20in%20affective%20computing.%20Existing%20methods%20typically%20rely%20on%20full%20annotation%2C%20requiring%20users%20to%20continuously%20label%20affective%20states%20across%20entire%20sessions.%20While%20this%20process%20yields%20fine-grained%20data%2C%20it%20is%20time-consuming%2C%20cognitively%20demanding%2C%20and%20prone%20to%20fatigue%20and%20errors.%20To%20address%20these%20issues%2C%20we%20present%20PREFAB%2C%20a%20low-budget%20retrospective%20self-annotation%20method%20that%20targets%20affective%20inflection%20regions%20rather%20than%20full%20annotation.%20Grounded%20in%20the%20peak-end%20rule%20and%20ordinal%20representations%20of%20emotion%2C%20PREFAB%20employs%20a%20preference-learning%20model%20to%20detect%20relative%20affective%20changes%2C%20directing%20annotators%20to%20label%20only%20selected%20segments%20while%20interpolating%20the%20remainder%20of%20the%20stimulus.%20We%20further%20introduce%20a%20preview%20mechanism%20that%20provides%20brief%20contextual%20cues%20to%20assist%20annotation.%20We%20evaluate%20PREFAB%20through%20a%20technical%20performance%20study%20and%20a%2025-participant%20user%20study.%20Results%20show%20that%20PREFAB%20outperforms%20baselines%20in%20modeling%20affective%20inflections%20while%20mitigating%20workload%20%28and%20conditionally%20mitigating%20temporal%20burden%29.%20Importantly%20PREFAB%20improves%20annotator%20confidence%20without%20degrading%20annotation%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2601.13904v1&entry.124074799=Read"},
{"title": "Tensorization of neural networks for improved privacy and interpretability", "author": "Jos\u00e9 Ram\u00f3n Pareja Monturiol and Alejandro Pozas-Kerstjens and David P\u00e9rez-Garc\u00eda", "abstract": "We present a tensorization algorithm for constructing tensor train/matrix product state (MPS) representations of functions, drawing on sketching and cross interpolation ideas. The method only requires black-box access to the target function and a small set of sample points defining the domain of interest. Thus, it is particularly well-suited for machine learning models, where the domain of interest is naturally defined by the training dataset. We show that this approach can be used to enhance the privacy and interpretability of neural network models. Specifically, we apply our decomposition to (i) obfuscate neural networks whose parameters encode patterns tied to the training data distribution, and (ii) estimate topological phases of matter that are easily accessible from the MPS representation. Additionally, we show that this tensorization can serve as an efficient initialization method for optimizing MPS in general settings, and that, for model compression, our algorithm achieves a superior trade-off between memory and time complexity compared to conventional tensorization methods of neural networks.", "link": "http://arxiv.org/abs/2501.06300v3", "date": "2026-01-20", "relevancy": 1.3623, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4687}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.4524}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4438}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tensorization%20of%20neural%20networks%20for%20improved%20privacy%20and%20interpretability&body=Title%3A%20Tensorization%20of%20neural%20networks%20for%20improved%20privacy%20and%20interpretability%0AAuthor%3A%20Jos%C3%A9%20Ram%C3%B3n%20Pareja%20Monturiol%20and%20Alejandro%20Pozas-Kerstjens%20and%20David%20P%C3%A9rez-Garc%C3%ADa%0AAbstract%3A%20We%20present%20a%20tensorization%20algorithm%20for%20constructing%20tensor%20train/matrix%20product%20state%20%28MPS%29%20representations%20of%20functions%2C%20drawing%20on%20sketching%20and%20cross%20interpolation%20ideas.%20The%20method%20only%20requires%20black-box%20access%20to%20the%20target%20function%20and%20a%20small%20set%20of%20sample%20points%20defining%20the%20domain%20of%20interest.%20Thus%2C%20it%20is%20particularly%20well-suited%20for%20machine%20learning%20models%2C%20where%20the%20domain%20of%20interest%20is%20naturally%20defined%20by%20the%20training%20dataset.%20We%20show%20that%20this%20approach%20can%20be%20used%20to%20enhance%20the%20privacy%20and%20interpretability%20of%20neural%20network%20models.%20Specifically%2C%20we%20apply%20our%20decomposition%20to%20%28i%29%20obfuscate%20neural%20networks%20whose%20parameters%20encode%20patterns%20tied%20to%20the%20training%20data%20distribution%2C%20and%20%28ii%29%20estimate%20topological%20phases%20of%20matter%20that%20are%20easily%20accessible%20from%20the%20MPS%20representation.%20Additionally%2C%20we%20show%20that%20this%20tensorization%20can%20serve%20as%20an%20efficient%20initialization%20method%20for%20optimizing%20MPS%20in%20general%20settings%2C%20and%20that%2C%20for%20model%20compression%2C%20our%20algorithm%20achieves%20a%20superior%20trade-off%20between%20memory%20and%20time%20complexity%20compared%20to%20conventional%20tensorization%20methods%20of%20neural%20networks.%0ALink%3A%20http%3A//arxiv.org/abs/2501.06300v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTensorization%2520of%2520neural%2520networks%2520for%2520improved%2520privacy%2520and%2520interpretability%26entry.906535625%3DJos%25C3%25A9%2520Ram%25C3%25B3n%2520Pareja%2520Monturiol%2520and%2520Alejandro%2520Pozas-Kerstjens%2520and%2520David%2520P%25C3%25A9rez-Garc%25C3%25ADa%26entry.1292438233%3DWe%2520present%2520a%2520tensorization%2520algorithm%2520for%2520constructing%2520tensor%2520train/matrix%2520product%2520state%2520%2528MPS%2529%2520representations%2520of%2520functions%252C%2520drawing%2520on%2520sketching%2520and%2520cross%2520interpolation%2520ideas.%2520The%2520method%2520only%2520requires%2520black-box%2520access%2520to%2520the%2520target%2520function%2520and%2520a%2520small%2520set%2520of%2520sample%2520points%2520defining%2520the%2520domain%2520of%2520interest.%2520Thus%252C%2520it%2520is%2520particularly%2520well-suited%2520for%2520machine%2520learning%2520models%252C%2520where%2520the%2520domain%2520of%2520interest%2520is%2520naturally%2520defined%2520by%2520the%2520training%2520dataset.%2520We%2520show%2520that%2520this%2520approach%2520can%2520be%2520used%2520to%2520enhance%2520the%2520privacy%2520and%2520interpretability%2520of%2520neural%2520network%2520models.%2520Specifically%252C%2520we%2520apply%2520our%2520decomposition%2520to%2520%2528i%2529%2520obfuscate%2520neural%2520networks%2520whose%2520parameters%2520encode%2520patterns%2520tied%2520to%2520the%2520training%2520data%2520distribution%252C%2520and%2520%2528ii%2529%2520estimate%2520topological%2520phases%2520of%2520matter%2520that%2520are%2520easily%2520accessible%2520from%2520the%2520MPS%2520representation.%2520Additionally%252C%2520we%2520show%2520that%2520this%2520tensorization%2520can%2520serve%2520as%2520an%2520efficient%2520initialization%2520method%2520for%2520optimizing%2520MPS%2520in%2520general%2520settings%252C%2520and%2520that%252C%2520for%2520model%2520compression%252C%2520our%2520algorithm%2520achieves%2520a%2520superior%2520trade-off%2520between%2520memory%2520and%2520time%2520complexity%2520compared%2520to%2520conventional%2520tensorization%2520methods%2520of%2520neural%2520networks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06300v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tensorization%20of%20neural%20networks%20for%20improved%20privacy%20and%20interpretability&entry.906535625=Jos%C3%A9%20Ram%C3%B3n%20Pareja%20Monturiol%20and%20Alejandro%20Pozas-Kerstjens%20and%20David%20P%C3%A9rez-Garc%C3%ADa&entry.1292438233=We%20present%20a%20tensorization%20algorithm%20for%20constructing%20tensor%20train/matrix%20product%20state%20%28MPS%29%20representations%20of%20functions%2C%20drawing%20on%20sketching%20and%20cross%20interpolation%20ideas.%20The%20method%20only%20requires%20black-box%20access%20to%20the%20target%20function%20and%20a%20small%20set%20of%20sample%20points%20defining%20the%20domain%20of%20interest.%20Thus%2C%20it%20is%20particularly%20well-suited%20for%20machine%20learning%20models%2C%20where%20the%20domain%20of%20interest%20is%20naturally%20defined%20by%20the%20training%20dataset.%20We%20show%20that%20this%20approach%20can%20be%20used%20to%20enhance%20the%20privacy%20and%20interpretability%20of%20neural%20network%20models.%20Specifically%2C%20we%20apply%20our%20decomposition%20to%20%28i%29%20obfuscate%20neural%20networks%20whose%20parameters%20encode%20patterns%20tied%20to%20the%20training%20data%20distribution%2C%20and%20%28ii%29%20estimate%20topological%20phases%20of%20matter%20that%20are%20easily%20accessible%20from%20the%20MPS%20representation.%20Additionally%2C%20we%20show%20that%20this%20tensorization%20can%20serve%20as%20an%20efficient%20initialization%20method%20for%20optimizing%20MPS%20in%20general%20settings%2C%20and%20that%2C%20for%20model%20compression%2C%20our%20algorithm%20achieves%20a%20superior%20trade-off%20between%20memory%20and%20time%20complexity%20compared%20to%20conventional%20tensorization%20methods%20of%20neural%20networks.&entry.1838667208=http%3A//arxiv.org/abs/2501.06300v3&entry.124074799=Read"},
{"title": "MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems", "author": "Yiyang Wang and Yiqiao Jin and Alex Cabral and Josiah Hester", "abstract": "Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.", "link": "http://arxiv.org/abs/2601.14230v1", "date": "2026-01-20", "relevancy": 1.5106, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5258}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5201}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.488}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MASCOT%3A%20Towards%20Multi-Agent%20Socio-Collaborative%20Companion%20Systems&body=Title%3A%20MASCOT%3A%20Towards%20Multi-Agent%20Socio-Collaborative%20Companion%20Systems%0AAuthor%3A%20Yiyang%20Wang%20and%20Yiqiao%20Jin%20and%20Alex%20Cabral%20and%20Josiah%20Hester%0AAbstract%3A%20Multi-agent%20systems%20%28MAS%29%20have%20recently%20emerged%20as%20promising%20socio-collaborative%20companions%20for%20emotional%20and%20cognitive%20support.%20However%2C%20these%20systems%20frequently%20suffer%20from%20persona%20collapse--where%20agents%20revert%20to%20generic%2C%20homogenized%20assistant%20behaviors--and%20social%20sycophancy%2C%20which%20produces%20redundant%2C%20non-constructive%20dialogue.%20We%20propose%20MASCOT%2C%20a%20generalizable%20framework%20for%20multi-perspective%20socio-collaborative%20companions.%20MASCOT%20introduces%20a%20novel%20bi-level%20optimization%20strategy%20to%20harmonize%20individual%20and%20collective%20behaviors%3A%201%29%20Persona-Aware%20Behavioral%20Alignment%2C%20an%20RLAIF-driven%20pipeline%20that%20finetunes%20individual%20agents%20for%20strict%20persona%20fidelity%20to%20prevent%20identity%20loss%3B%20and%202%29%20Collaborative%20Dialogue%20Optimization%2C%20a%20meta-policy%20guided%20by%20group-level%20rewards%20to%20ensure%20diverse%20and%20productive%20discourse.%20Extensive%20evaluations%20across%20psychological%20support%20and%20workplace%20domains%20demonstrate%20that%20MASCOT%20significantly%20outperforms%20state-of-the-art%20baselines%2C%20achieving%20improvements%20of%20up%20to%20%2B14.1%20in%20Persona%20Consistency%20and%20%2B10.6%20in%20Social%20Contribution.%20Our%20framework%20provides%20a%20practical%20roadmap%20for%20engineering%20the%20next%20generation%20of%20socially%20intelligent%20multi-agent%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2601.14230v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMASCOT%253A%2520Towards%2520Multi-Agent%2520Socio-Collaborative%2520Companion%2520Systems%26entry.906535625%3DYiyang%2520Wang%2520and%2520Yiqiao%2520Jin%2520and%2520Alex%2520Cabral%2520and%2520Josiah%2520Hester%26entry.1292438233%3DMulti-agent%2520systems%2520%2528MAS%2529%2520have%2520recently%2520emerged%2520as%2520promising%2520socio-collaborative%2520companions%2520for%2520emotional%2520and%2520cognitive%2520support.%2520However%252C%2520these%2520systems%2520frequently%2520suffer%2520from%2520persona%2520collapse--where%2520agents%2520revert%2520to%2520generic%252C%2520homogenized%2520assistant%2520behaviors--and%2520social%2520sycophancy%252C%2520which%2520produces%2520redundant%252C%2520non-constructive%2520dialogue.%2520We%2520propose%2520MASCOT%252C%2520a%2520generalizable%2520framework%2520for%2520multi-perspective%2520socio-collaborative%2520companions.%2520MASCOT%2520introduces%2520a%2520novel%2520bi-level%2520optimization%2520strategy%2520to%2520harmonize%2520individual%2520and%2520collective%2520behaviors%253A%25201%2529%2520Persona-Aware%2520Behavioral%2520Alignment%252C%2520an%2520RLAIF-driven%2520pipeline%2520that%2520finetunes%2520individual%2520agents%2520for%2520strict%2520persona%2520fidelity%2520to%2520prevent%2520identity%2520loss%253B%2520and%25202%2529%2520Collaborative%2520Dialogue%2520Optimization%252C%2520a%2520meta-policy%2520guided%2520by%2520group-level%2520rewards%2520to%2520ensure%2520diverse%2520and%2520productive%2520discourse.%2520Extensive%2520evaluations%2520across%2520psychological%2520support%2520and%2520workplace%2520domains%2520demonstrate%2520that%2520MASCOT%2520significantly%2520outperforms%2520state-of-the-art%2520baselines%252C%2520achieving%2520improvements%2520of%2520up%2520to%2520%252B14.1%2520in%2520Persona%2520Consistency%2520and%2520%252B10.6%2520in%2520Social%2520Contribution.%2520Our%2520framework%2520provides%2520a%2520practical%2520roadmap%2520for%2520engineering%2520the%2520next%2520generation%2520of%2520socially%2520intelligent%2520multi-agent%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.14230v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MASCOT%3A%20Towards%20Multi-Agent%20Socio-Collaborative%20Companion%20Systems&entry.906535625=Yiyang%20Wang%20and%20Yiqiao%20Jin%20and%20Alex%20Cabral%20and%20Josiah%20Hester&entry.1292438233=Multi-agent%20systems%20%28MAS%29%20have%20recently%20emerged%20as%20promising%20socio-collaborative%20companions%20for%20emotional%20and%20cognitive%20support.%20However%2C%20these%20systems%20frequently%20suffer%20from%20persona%20collapse--where%20agents%20revert%20to%20generic%2C%20homogenized%20assistant%20behaviors--and%20social%20sycophancy%2C%20which%20produces%20redundant%2C%20non-constructive%20dialogue.%20We%20propose%20MASCOT%2C%20a%20generalizable%20framework%20for%20multi-perspective%20socio-collaborative%20companions.%20MASCOT%20introduces%20a%20novel%20bi-level%20optimization%20strategy%20to%20harmonize%20individual%20and%20collective%20behaviors%3A%201%29%20Persona-Aware%20Behavioral%20Alignment%2C%20an%20RLAIF-driven%20pipeline%20that%20finetunes%20individual%20agents%20for%20strict%20persona%20fidelity%20to%20prevent%20identity%20loss%3B%20and%202%29%20Collaborative%20Dialogue%20Optimization%2C%20a%20meta-policy%20guided%20by%20group-level%20rewards%20to%20ensure%20diverse%20and%20productive%20discourse.%20Extensive%20evaluations%20across%20psychological%20support%20and%20workplace%20domains%20demonstrate%20that%20MASCOT%20significantly%20outperforms%20state-of-the-art%20baselines%2C%20achieving%20improvements%20of%20up%20to%20%2B14.1%20in%20Persona%20Consistency%20and%20%2B10.6%20in%20Social%20Contribution.%20Our%20framework%20provides%20a%20practical%20roadmap%20for%20engineering%20the%20next%20generation%20of%20socially%20intelligent%20multi-agent%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2601.14230v1&entry.124074799=Read"},
{"title": "Harmonizing the Deep: A Unified Information Pipeline for Robust Marine Biodiversity Assessment Across Heterogeneous Domains", "author": "Marco Piccolo and Qiwei Han and Astrid van Toor and Joachim Vanneste", "abstract": "Marine biodiversity monitoring requires scalability and reliability across complex underwater environments to support conservation and invasive-species management. Yet existing detection solutions often exhibit a pronounced deployment gap, with performance degrading sharply when transferred to new sites. This work establishes the foundational detection layer for a multi-year invasive species monitoring initiative targeting Arctic and Atlantic marine ecosystems. We address this challenge by developing a Unified Information Pipeline that standardises heterogeneous datasets into a comparable information flow and evaluates a fixed, deployment-relevant detector under controlled cross-domain protocols. Across multiple domains, we find that structural factors, such as scene composition, object density, and contextual redundancy, explain cross-domain performance loss more strongly than visual degradation such as turbidity, with sparse scenes inducing a characteristic \"Context Collapse\" failure mode. We further validate operational feasibility by benchmarking inference on low-cost edge hardware, showing that runtime optimisation enables practical sampling rates for remote monitoring. The results shift emphasis from image enhancement toward structure-aware reliability, providing a democratised tool for consistent marine ecosystem assessment.", "link": "http://arxiv.org/abs/2601.13975v1", "date": "2026-01-20", "relevancy": 1.9878, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5058}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4912}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Harmonizing%20the%20Deep%3A%20A%20Unified%20Information%20Pipeline%20for%20Robust%20Marine%20Biodiversity%20Assessment%20Across%20Heterogeneous%20Domains&body=Title%3A%20Harmonizing%20the%20Deep%3A%20A%20Unified%20Information%20Pipeline%20for%20Robust%20Marine%20Biodiversity%20Assessment%20Across%20Heterogeneous%20Domains%0AAuthor%3A%20Marco%20Piccolo%20and%20Qiwei%20Han%20and%20Astrid%20van%20Toor%20and%20Joachim%20Vanneste%0AAbstract%3A%20Marine%20biodiversity%20monitoring%20requires%20scalability%20and%20reliability%20across%20complex%20underwater%20environments%20to%20support%20conservation%20and%20invasive-species%20management.%20Yet%20existing%20detection%20solutions%20often%20exhibit%20a%20pronounced%20deployment%20gap%2C%20with%20performance%20degrading%20sharply%20when%20transferred%20to%20new%20sites.%20This%20work%20establishes%20the%20foundational%20detection%20layer%20for%20a%20multi-year%20invasive%20species%20monitoring%20initiative%20targeting%20Arctic%20and%20Atlantic%20marine%20ecosystems.%20We%20address%20this%20challenge%20by%20developing%20a%20Unified%20Information%20Pipeline%20that%20standardises%20heterogeneous%20datasets%20into%20a%20comparable%20information%20flow%20and%20evaluates%20a%20fixed%2C%20deployment-relevant%20detector%20under%20controlled%20cross-domain%20protocols.%20Across%20multiple%20domains%2C%20we%20find%20that%20structural%20factors%2C%20such%20as%20scene%20composition%2C%20object%20density%2C%20and%20contextual%20redundancy%2C%20explain%20cross-domain%20performance%20loss%20more%20strongly%20than%20visual%20degradation%20such%20as%20turbidity%2C%20with%20sparse%20scenes%20inducing%20a%20characteristic%20%22Context%20Collapse%22%20failure%20mode.%20We%20further%20validate%20operational%20feasibility%20by%20benchmarking%20inference%20on%20low-cost%20edge%20hardware%2C%20showing%20that%20runtime%20optimisation%20enables%20practical%20sampling%20rates%20for%20remote%20monitoring.%20The%20results%20shift%20emphasis%20from%20image%20enhancement%20toward%20structure-aware%20reliability%2C%20providing%20a%20democratised%20tool%20for%20consistent%20marine%20ecosystem%20assessment.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13975v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHarmonizing%2520the%2520Deep%253A%2520A%2520Unified%2520Information%2520Pipeline%2520for%2520Robust%2520Marine%2520Biodiversity%2520Assessment%2520Across%2520Heterogeneous%2520Domains%26entry.906535625%3DMarco%2520Piccolo%2520and%2520Qiwei%2520Han%2520and%2520Astrid%2520van%2520Toor%2520and%2520Joachim%2520Vanneste%26entry.1292438233%3DMarine%2520biodiversity%2520monitoring%2520requires%2520scalability%2520and%2520reliability%2520across%2520complex%2520underwater%2520environments%2520to%2520support%2520conservation%2520and%2520invasive-species%2520management.%2520Yet%2520existing%2520detection%2520solutions%2520often%2520exhibit%2520a%2520pronounced%2520deployment%2520gap%252C%2520with%2520performance%2520degrading%2520sharply%2520when%2520transferred%2520to%2520new%2520sites.%2520This%2520work%2520establishes%2520the%2520foundational%2520detection%2520layer%2520for%2520a%2520multi-year%2520invasive%2520species%2520monitoring%2520initiative%2520targeting%2520Arctic%2520and%2520Atlantic%2520marine%2520ecosystems.%2520We%2520address%2520this%2520challenge%2520by%2520developing%2520a%2520Unified%2520Information%2520Pipeline%2520that%2520standardises%2520heterogeneous%2520datasets%2520into%2520a%2520comparable%2520information%2520flow%2520and%2520evaluates%2520a%2520fixed%252C%2520deployment-relevant%2520detector%2520under%2520controlled%2520cross-domain%2520protocols.%2520Across%2520multiple%2520domains%252C%2520we%2520find%2520that%2520structural%2520factors%252C%2520such%2520as%2520scene%2520composition%252C%2520object%2520density%252C%2520and%2520contextual%2520redundancy%252C%2520explain%2520cross-domain%2520performance%2520loss%2520more%2520strongly%2520than%2520visual%2520degradation%2520such%2520as%2520turbidity%252C%2520with%2520sparse%2520scenes%2520inducing%2520a%2520characteristic%2520%2522Context%2520Collapse%2522%2520failure%2520mode.%2520We%2520further%2520validate%2520operational%2520feasibility%2520by%2520benchmarking%2520inference%2520on%2520low-cost%2520edge%2520hardware%252C%2520showing%2520that%2520runtime%2520optimisation%2520enables%2520practical%2520sampling%2520rates%2520for%2520remote%2520monitoring.%2520The%2520results%2520shift%2520emphasis%2520from%2520image%2520enhancement%2520toward%2520structure-aware%2520reliability%252C%2520providing%2520a%2520democratised%2520tool%2520for%2520consistent%2520marine%2520ecosystem%2520assessment.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13975v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Harmonizing%20the%20Deep%3A%20A%20Unified%20Information%20Pipeline%20for%20Robust%20Marine%20Biodiversity%20Assessment%20Across%20Heterogeneous%20Domains&entry.906535625=Marco%20Piccolo%20and%20Qiwei%20Han%20and%20Astrid%20van%20Toor%20and%20Joachim%20Vanneste&entry.1292438233=Marine%20biodiversity%20monitoring%20requires%20scalability%20and%20reliability%20across%20complex%20underwater%20environments%20to%20support%20conservation%20and%20invasive-species%20management.%20Yet%20existing%20detection%20solutions%20often%20exhibit%20a%20pronounced%20deployment%20gap%2C%20with%20performance%20degrading%20sharply%20when%20transferred%20to%20new%20sites.%20This%20work%20establishes%20the%20foundational%20detection%20layer%20for%20a%20multi-year%20invasive%20species%20monitoring%20initiative%20targeting%20Arctic%20and%20Atlantic%20marine%20ecosystems.%20We%20address%20this%20challenge%20by%20developing%20a%20Unified%20Information%20Pipeline%20that%20standardises%20heterogeneous%20datasets%20into%20a%20comparable%20information%20flow%20and%20evaluates%20a%20fixed%2C%20deployment-relevant%20detector%20under%20controlled%20cross-domain%20protocols.%20Across%20multiple%20domains%2C%20we%20find%20that%20structural%20factors%2C%20such%20as%20scene%20composition%2C%20object%20density%2C%20and%20contextual%20redundancy%2C%20explain%20cross-domain%20performance%20loss%20more%20strongly%20than%20visual%20degradation%20such%20as%20turbidity%2C%20with%20sparse%20scenes%20inducing%20a%20characteristic%20%22Context%20Collapse%22%20failure%20mode.%20We%20further%20validate%20operational%20feasibility%20by%20benchmarking%20inference%20on%20low-cost%20edge%20hardware%2C%20showing%20that%20runtime%20optimisation%20enables%20practical%20sampling%20rates%20for%20remote%20monitoring.%20The%20results%20shift%20emphasis%20from%20image%20enhancement%20toward%20structure-aware%20reliability%2C%20providing%20a%20democratised%20tool%20for%20consistent%20marine%20ecosystem%20assessment.&entry.1838667208=http%3A//arxiv.org/abs/2601.13975v1&entry.124074799=Read"},
{"title": "Safety on the Fly: Constructing Robust Safety Filters via Policy Control Barrier Functions at Runtime", "author": "Luzia Knoedler and Oswin So and Ji Yin and Mitchell Black and Zachary Serlin and Panagiotis Tsiotras and Javier Alonso-Mora and Chuchu Fan", "abstract": "Control Barrier Functions (CBFs) have proven to be an effective tool for performing safe control synthesis for nonlinear systems. However, guaranteeing safety in the presence of disturbances and input constraints for high relative degree systems is a difficult problem. In this work, we propose the Robust Policy CBF (RPCBF), a practical approach for constructing robust CBF approximations online via the estimation of a value function. We establish conditions under which the approximation qualifies as a valid CBF and demonstrate the effectiveness of the RPCBF-safety filter in simulation on a variety of high relative degree input-constrained systems. Finally, we demonstrate the benefits of our method in compensating for model errors on a hardware quadcopter platform by treating the model errors as disturbances. Website including code: www.oswinso.xyz/rpcbf/", "link": "http://arxiv.org/abs/2410.11157v3", "date": "2026-01-20", "relevancy": 1.8742, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5067}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4806}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Safety%20on%20the%20Fly%3A%20Constructing%20Robust%20Safety%20Filters%20via%20Policy%20Control%20Barrier%20Functions%20at%20Runtime&body=Title%3A%20Safety%20on%20the%20Fly%3A%20Constructing%20Robust%20Safety%20Filters%20via%20Policy%20Control%20Barrier%20Functions%20at%20Runtime%0AAuthor%3A%20Luzia%20Knoedler%20and%20Oswin%20So%20and%20Ji%20Yin%20and%20Mitchell%20Black%20and%20Zachary%20Serlin%20and%20Panagiotis%20Tsiotras%20and%20Javier%20Alonso-Mora%20and%20Chuchu%20Fan%0AAbstract%3A%20Control%20Barrier%20Functions%20%28CBFs%29%20have%20proven%20to%20be%20an%20effective%20tool%20for%20performing%20safe%20control%20synthesis%20for%20nonlinear%20systems.%20However%2C%20guaranteeing%20safety%20in%20the%20presence%20of%20disturbances%20and%20input%20constraints%20for%20high%20relative%20degree%20systems%20is%20a%20difficult%20problem.%20In%20this%20work%2C%20we%20propose%20the%20Robust%20Policy%20CBF%20%28RPCBF%29%2C%20a%20practical%20approach%20for%20constructing%20robust%20CBF%20approximations%20online%20via%20the%20estimation%20of%20a%20value%20function.%20We%20establish%20conditions%20under%20which%20the%20approximation%20qualifies%20as%20a%20valid%20CBF%20and%20demonstrate%20the%20effectiveness%20of%20the%20RPCBF-safety%20filter%20in%20simulation%20on%20a%20variety%20of%20high%20relative%20degree%20input-constrained%20systems.%20Finally%2C%20we%20demonstrate%20the%20benefits%20of%20our%20method%20in%20compensating%20for%20model%20errors%20on%20a%20hardware%20quadcopter%20platform%20by%20treating%20the%20model%20errors%20as%20disturbances.%20Website%20including%20code%3A%20www.oswinso.xyz/rpcbf/%0ALink%3A%20http%3A//arxiv.org/abs/2410.11157v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSafety%2520on%2520the%2520Fly%253A%2520Constructing%2520Robust%2520Safety%2520Filters%2520via%2520Policy%2520Control%2520Barrier%2520Functions%2520at%2520Runtime%26entry.906535625%3DLuzia%2520Knoedler%2520and%2520Oswin%2520So%2520and%2520Ji%2520Yin%2520and%2520Mitchell%2520Black%2520and%2520Zachary%2520Serlin%2520and%2520Panagiotis%2520Tsiotras%2520and%2520Javier%2520Alonso-Mora%2520and%2520Chuchu%2520Fan%26entry.1292438233%3DControl%2520Barrier%2520Functions%2520%2528CBFs%2529%2520have%2520proven%2520to%2520be%2520an%2520effective%2520tool%2520for%2520performing%2520safe%2520control%2520synthesis%2520for%2520nonlinear%2520systems.%2520However%252C%2520guaranteeing%2520safety%2520in%2520the%2520presence%2520of%2520disturbances%2520and%2520input%2520constraints%2520for%2520high%2520relative%2520degree%2520systems%2520is%2520a%2520difficult%2520problem.%2520In%2520this%2520work%252C%2520we%2520propose%2520the%2520Robust%2520Policy%2520CBF%2520%2528RPCBF%2529%252C%2520a%2520practical%2520approach%2520for%2520constructing%2520robust%2520CBF%2520approximations%2520online%2520via%2520the%2520estimation%2520of%2520a%2520value%2520function.%2520We%2520establish%2520conditions%2520under%2520which%2520the%2520approximation%2520qualifies%2520as%2520a%2520valid%2520CBF%2520and%2520demonstrate%2520the%2520effectiveness%2520of%2520the%2520RPCBF-safety%2520filter%2520in%2520simulation%2520on%2520a%2520variety%2520of%2520high%2520relative%2520degree%2520input-constrained%2520systems.%2520Finally%252C%2520we%2520demonstrate%2520the%2520benefits%2520of%2520our%2520method%2520in%2520compensating%2520for%2520model%2520errors%2520on%2520a%2520hardware%2520quadcopter%2520platform%2520by%2520treating%2520the%2520model%2520errors%2520as%2520disturbances.%2520Website%2520including%2520code%253A%2520www.oswinso.xyz/rpcbf/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.11157v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Safety%20on%20the%20Fly%3A%20Constructing%20Robust%20Safety%20Filters%20via%20Policy%20Control%20Barrier%20Functions%20at%20Runtime&entry.906535625=Luzia%20Knoedler%20and%20Oswin%20So%20and%20Ji%20Yin%20and%20Mitchell%20Black%20and%20Zachary%20Serlin%20and%20Panagiotis%20Tsiotras%20and%20Javier%20Alonso-Mora%20and%20Chuchu%20Fan&entry.1292438233=Control%20Barrier%20Functions%20%28CBFs%29%20have%20proven%20to%20be%20an%20effective%20tool%20for%20performing%20safe%20control%20synthesis%20for%20nonlinear%20systems.%20However%2C%20guaranteeing%20safety%20in%20the%20presence%20of%20disturbances%20and%20input%20constraints%20for%20high%20relative%20degree%20systems%20is%20a%20difficult%20problem.%20In%20this%20work%2C%20we%20propose%20the%20Robust%20Policy%20CBF%20%28RPCBF%29%2C%20a%20practical%20approach%20for%20constructing%20robust%20CBF%20approximations%20online%20via%20the%20estimation%20of%20a%20value%20function.%20We%20establish%20conditions%20under%20which%20the%20approximation%20qualifies%20as%20a%20valid%20CBF%20and%20demonstrate%20the%20effectiveness%20of%20the%20RPCBF-safety%20filter%20in%20simulation%20on%20a%20variety%20of%20high%20relative%20degree%20input-constrained%20systems.%20Finally%2C%20we%20demonstrate%20the%20benefits%20of%20our%20method%20in%20compensating%20for%20model%20errors%20on%20a%20hardware%20quadcopter%20platform%20by%20treating%20the%20model%20errors%20as%20disturbances.%20Website%20including%20code%3A%20www.oswinso.xyz/rpcbf/&entry.1838667208=http%3A//arxiv.org/abs/2410.11157v3&entry.124074799=Read"},
{"title": "HyperWalker: Dynamic Hypergraph-Based Deep Diagnosis for Multi-Hop Clinical Modeling across EHR and X-Ray in Medical VLMs", "author": "Yuezhe Yang and Hao Wang and Yige Peng and Jinman Kim and Lei Bi", "abstract": "Automated clinical diagnosis remains a core challenge in medical AI, which usually requires models to integrate multi-modal data and reason across complex, case-specific contexts. Although recent methods have advanced medical report generation (MRG) and visual question answering (VQA) with medical vision-language models (VLMs), these methods, however, predominantly operate under a sample-isolated inference paradigm, as such processing cases independently without access to longitudinal electronic health records (EHRs) or structurally related patient examples. This paradigm limits reasoning to image-derived information alone, which ignores external complementary medical evidence for potentially more accurate diagnosis. To overcome this limitation, we propose \\textbf{HyperWalker}, a \\textit{Deep Diagnosis} framework that reformulates clinical reasoning via dynamic hypergraphs and test-time training. First, we construct a dynamic hypergraph, termed \\textbf{iBrochure}, to model the structural heterogeneity of EHR data and implicit high-order associations among multimodal clinical information. Within this hypergraph, a reinforcement learning agent, \\textbf{Walker}, navigates to and identifies optimal diagnostic paths. To ensure comprehensive coverage of diverse clinical characteristics in test samples, we incorporate a \\textit{linger mechanism}, a multi-hop orthogonal retrieval strategy that iteratively selects clinically complementary neighborhood cases reflecting distinct clinical attributes. Experiments on MRG with MIMIC and medical VQA on EHRXQA demonstrate that HyperWalker achieves state-of-the-art performance. Code is available at: https://github.com/Bean-Young/HyperWalker", "link": "http://arxiv.org/abs/2601.13919v1", "date": "2026-01-20", "relevancy": 1.5355, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.518}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5101}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.498}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HyperWalker%3A%20Dynamic%20Hypergraph-Based%20Deep%20Diagnosis%20for%20Multi-Hop%20Clinical%20Modeling%20across%20EHR%20and%20X-Ray%20in%20Medical%20VLMs&body=Title%3A%20HyperWalker%3A%20Dynamic%20Hypergraph-Based%20Deep%20Diagnosis%20for%20Multi-Hop%20Clinical%20Modeling%20across%20EHR%20and%20X-Ray%20in%20Medical%20VLMs%0AAuthor%3A%20Yuezhe%20Yang%20and%20Hao%20Wang%20and%20Yige%20Peng%20and%20Jinman%20Kim%20and%20Lei%20Bi%0AAbstract%3A%20Automated%20clinical%20diagnosis%20remains%20a%20core%20challenge%20in%20medical%20AI%2C%20which%20usually%20requires%20models%20to%20integrate%20multi-modal%20data%20and%20reason%20across%20complex%2C%20case-specific%20contexts.%20Although%20recent%20methods%20have%20advanced%20medical%20report%20generation%20%28MRG%29%20and%20visual%20question%20answering%20%28VQA%29%20with%20medical%20vision-language%20models%20%28VLMs%29%2C%20these%20methods%2C%20however%2C%20predominantly%20operate%20under%20a%20sample-isolated%20inference%20paradigm%2C%20as%20such%20processing%20cases%20independently%20without%20access%20to%20longitudinal%20electronic%20health%20records%20%28EHRs%29%20or%20structurally%20related%20patient%20examples.%20This%20paradigm%20limits%20reasoning%20to%20image-derived%20information%20alone%2C%20which%20ignores%20external%20complementary%20medical%20evidence%20for%20potentially%20more%20accurate%20diagnosis.%20To%20overcome%20this%20limitation%2C%20we%20propose%20%5Ctextbf%7BHyperWalker%7D%2C%20a%20%5Ctextit%7BDeep%20Diagnosis%7D%20framework%20that%20reformulates%20clinical%20reasoning%20via%20dynamic%20hypergraphs%20and%20test-time%20training.%20First%2C%20we%20construct%20a%20dynamic%20hypergraph%2C%20termed%20%5Ctextbf%7BiBrochure%7D%2C%20to%20model%20the%20structural%20heterogeneity%20of%20EHR%20data%20and%20implicit%20high-order%20associations%20among%20multimodal%20clinical%20information.%20Within%20this%20hypergraph%2C%20a%20reinforcement%20learning%20agent%2C%20%5Ctextbf%7BWalker%7D%2C%20navigates%20to%20and%20identifies%20optimal%20diagnostic%20paths.%20To%20ensure%20comprehensive%20coverage%20of%20diverse%20clinical%20characteristics%20in%20test%20samples%2C%20we%20incorporate%20a%20%5Ctextit%7Blinger%20mechanism%7D%2C%20a%20multi-hop%20orthogonal%20retrieval%20strategy%20that%20iteratively%20selects%20clinically%20complementary%20neighborhood%20cases%20reflecting%20distinct%20clinical%20attributes.%20Experiments%20on%20MRG%20with%20MIMIC%20and%20medical%20VQA%20on%20EHRXQA%20demonstrate%20that%20HyperWalker%20achieves%20state-of-the-art%20performance.%20Code%20is%20available%20at%3A%20https%3A//github.com/Bean-Young/HyperWalker%0ALink%3A%20http%3A//arxiv.org/abs/2601.13919v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHyperWalker%253A%2520Dynamic%2520Hypergraph-Based%2520Deep%2520Diagnosis%2520for%2520Multi-Hop%2520Clinical%2520Modeling%2520across%2520EHR%2520and%2520X-Ray%2520in%2520Medical%2520VLMs%26entry.906535625%3DYuezhe%2520Yang%2520and%2520Hao%2520Wang%2520and%2520Yige%2520Peng%2520and%2520Jinman%2520Kim%2520and%2520Lei%2520Bi%26entry.1292438233%3DAutomated%2520clinical%2520diagnosis%2520remains%2520a%2520core%2520challenge%2520in%2520medical%2520AI%252C%2520which%2520usually%2520requires%2520models%2520to%2520integrate%2520multi-modal%2520data%2520and%2520reason%2520across%2520complex%252C%2520case-specific%2520contexts.%2520Although%2520recent%2520methods%2520have%2520advanced%2520medical%2520report%2520generation%2520%2528MRG%2529%2520and%2520visual%2520question%2520answering%2520%2528VQA%2529%2520with%2520medical%2520vision-language%2520models%2520%2528VLMs%2529%252C%2520these%2520methods%252C%2520however%252C%2520predominantly%2520operate%2520under%2520a%2520sample-isolated%2520inference%2520paradigm%252C%2520as%2520such%2520processing%2520cases%2520independently%2520without%2520access%2520to%2520longitudinal%2520electronic%2520health%2520records%2520%2528EHRs%2529%2520or%2520structurally%2520related%2520patient%2520examples.%2520This%2520paradigm%2520limits%2520reasoning%2520to%2520image-derived%2520information%2520alone%252C%2520which%2520ignores%2520external%2520complementary%2520medical%2520evidence%2520for%2520potentially%2520more%2520accurate%2520diagnosis.%2520To%2520overcome%2520this%2520limitation%252C%2520we%2520propose%2520%255Ctextbf%257BHyperWalker%257D%252C%2520a%2520%255Ctextit%257BDeep%2520Diagnosis%257D%2520framework%2520that%2520reformulates%2520clinical%2520reasoning%2520via%2520dynamic%2520hypergraphs%2520and%2520test-time%2520training.%2520First%252C%2520we%2520construct%2520a%2520dynamic%2520hypergraph%252C%2520termed%2520%255Ctextbf%257BiBrochure%257D%252C%2520to%2520model%2520the%2520structural%2520heterogeneity%2520of%2520EHR%2520data%2520and%2520implicit%2520high-order%2520associations%2520among%2520multimodal%2520clinical%2520information.%2520Within%2520this%2520hypergraph%252C%2520a%2520reinforcement%2520learning%2520agent%252C%2520%255Ctextbf%257BWalker%257D%252C%2520navigates%2520to%2520and%2520identifies%2520optimal%2520diagnostic%2520paths.%2520To%2520ensure%2520comprehensive%2520coverage%2520of%2520diverse%2520clinical%2520characteristics%2520in%2520test%2520samples%252C%2520we%2520incorporate%2520a%2520%255Ctextit%257Blinger%2520mechanism%257D%252C%2520a%2520multi-hop%2520orthogonal%2520retrieval%2520strategy%2520that%2520iteratively%2520selects%2520clinically%2520complementary%2520neighborhood%2520cases%2520reflecting%2520distinct%2520clinical%2520attributes.%2520Experiments%2520on%2520MRG%2520with%2520MIMIC%2520and%2520medical%2520VQA%2520on%2520EHRXQA%2520demonstrate%2520that%2520HyperWalker%2520achieves%2520state-of-the-art%2520performance.%2520Code%2520is%2520available%2520at%253A%2520https%253A//github.com/Bean-Young/HyperWalker%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13919v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HyperWalker%3A%20Dynamic%20Hypergraph-Based%20Deep%20Diagnosis%20for%20Multi-Hop%20Clinical%20Modeling%20across%20EHR%20and%20X-Ray%20in%20Medical%20VLMs&entry.906535625=Yuezhe%20Yang%20and%20Hao%20Wang%20and%20Yige%20Peng%20and%20Jinman%20Kim%20and%20Lei%20Bi&entry.1292438233=Automated%20clinical%20diagnosis%20remains%20a%20core%20challenge%20in%20medical%20AI%2C%20which%20usually%20requires%20models%20to%20integrate%20multi-modal%20data%20and%20reason%20across%20complex%2C%20case-specific%20contexts.%20Although%20recent%20methods%20have%20advanced%20medical%20report%20generation%20%28MRG%29%20and%20visual%20question%20answering%20%28VQA%29%20with%20medical%20vision-language%20models%20%28VLMs%29%2C%20these%20methods%2C%20however%2C%20predominantly%20operate%20under%20a%20sample-isolated%20inference%20paradigm%2C%20as%20such%20processing%20cases%20independently%20without%20access%20to%20longitudinal%20electronic%20health%20records%20%28EHRs%29%20or%20structurally%20related%20patient%20examples.%20This%20paradigm%20limits%20reasoning%20to%20image-derived%20information%20alone%2C%20which%20ignores%20external%20complementary%20medical%20evidence%20for%20potentially%20more%20accurate%20diagnosis.%20To%20overcome%20this%20limitation%2C%20we%20propose%20%5Ctextbf%7BHyperWalker%7D%2C%20a%20%5Ctextit%7BDeep%20Diagnosis%7D%20framework%20that%20reformulates%20clinical%20reasoning%20via%20dynamic%20hypergraphs%20and%20test-time%20training.%20First%2C%20we%20construct%20a%20dynamic%20hypergraph%2C%20termed%20%5Ctextbf%7BiBrochure%7D%2C%20to%20model%20the%20structural%20heterogeneity%20of%20EHR%20data%20and%20implicit%20high-order%20associations%20among%20multimodal%20clinical%20information.%20Within%20this%20hypergraph%2C%20a%20reinforcement%20learning%20agent%2C%20%5Ctextbf%7BWalker%7D%2C%20navigates%20to%20and%20identifies%20optimal%20diagnostic%20paths.%20To%20ensure%20comprehensive%20coverage%20of%20diverse%20clinical%20characteristics%20in%20test%20samples%2C%20we%20incorporate%20a%20%5Ctextit%7Blinger%20mechanism%7D%2C%20a%20multi-hop%20orthogonal%20retrieval%20strategy%20that%20iteratively%20selects%20clinically%20complementary%20neighborhood%20cases%20reflecting%20distinct%20clinical%20attributes.%20Experiments%20on%20MRG%20with%20MIMIC%20and%20medical%20VQA%20on%20EHRXQA%20demonstrate%20that%20HyperWalker%20achieves%20state-of-the-art%20performance.%20Code%20is%20available%20at%3A%20https%3A//github.com/Bean-Young/HyperWalker&entry.1838667208=http%3A//arxiv.org/abs/2601.13919v1&entry.124074799=Read"},
{"title": "Learned Hallucination Detection in Black-Box LLMs using Token-level Entropy Production Rate", "author": "Charles Moslonka and Hicham Randrianarivo and Arthur Garnier and Emmanuel Malherbe", "abstract": "Hallucinations in Large Language Model (LLM) outputs for Question Answering (QA) tasks can critically undermine their real-world reliability. This paper introduces a methodology for robust, one-shot hallucination detection, specifically designed for scenarios with limited data access, such as interacting with black-box LLM APIs that typically expose only a few top candidate log-probabilities per token. Our approach derives uncertainty indicators directly from these readily available log-probabilities generated during non-greedy decoding. We first derive an Entropy Production Rate (EPR) that offers baseline performance, later augmented with supervised learning. Our learned model leverages the entropic contributions of the accessible top-ranked tokens within a single generated sequence, without multiple re-runs per query. Evaluated across diverse QA datasets and multiple LLMs, this estimator significantly improves token-level hallucination detection over state-of-the-art methods. Crucially, high performance is demonstrated using only the typically small set of available log-probabilities (e.g., top-10 per token), confirming its practical efficiency and suitability for API-constrained deployments. This work provides a lightweight technique to enhance the trustworthiness of LLM responses, at the token level, after a single generation pass, for QA and Retrieval-Augmented Generation (RAG) systems. Our experiments confirmed the performance of our method against existing approaches on public dataset as well as for a financial framework analyzing annual company reports.", "link": "http://arxiv.org/abs/2509.04492v2", "date": "2026-01-20", "relevancy": 1.9635, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5423}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4935}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4677}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learned%20Hallucination%20Detection%20in%20Black-Box%20LLMs%20using%20Token-level%20Entropy%20Production%20Rate&body=Title%3A%20Learned%20Hallucination%20Detection%20in%20Black-Box%20LLMs%20using%20Token-level%20Entropy%20Production%20Rate%0AAuthor%3A%20Charles%20Moslonka%20and%20Hicham%20Randrianarivo%20and%20Arthur%20Garnier%20and%20Emmanuel%20Malherbe%0AAbstract%3A%20Hallucinations%20in%20Large%20Language%20Model%20%28LLM%29%20outputs%20for%20Question%20Answering%20%28QA%29%20tasks%20can%20critically%20undermine%20their%20real-world%20reliability.%20This%20paper%20introduces%20a%20methodology%20for%20robust%2C%20one-shot%20hallucination%20detection%2C%20specifically%20designed%20for%20scenarios%20with%20limited%20data%20access%2C%20such%20as%20interacting%20with%20black-box%20LLM%20APIs%20that%20typically%20expose%20only%20a%20few%20top%20candidate%20log-probabilities%20per%20token.%20Our%20approach%20derives%20uncertainty%20indicators%20directly%20from%20these%20readily%20available%20log-probabilities%20generated%20during%20non-greedy%20decoding.%20We%20first%20derive%20an%20Entropy%20Production%20Rate%20%28EPR%29%20that%20offers%20baseline%20performance%2C%20later%20augmented%20with%20supervised%20learning.%20Our%20learned%20model%20leverages%20the%20entropic%20contributions%20of%20the%20accessible%20top-ranked%20tokens%20within%20a%20single%20generated%20sequence%2C%20without%20multiple%20re-runs%20per%20query.%20Evaluated%20across%20diverse%20QA%20datasets%20and%20multiple%20LLMs%2C%20this%20estimator%20significantly%20improves%20token-level%20hallucination%20detection%20over%20state-of-the-art%20methods.%20Crucially%2C%20high%20performance%20is%20demonstrated%20using%20only%20the%20typically%20small%20set%20of%20available%20log-probabilities%20%28e.g.%2C%20top-10%20per%20token%29%2C%20confirming%20its%20practical%20efficiency%20and%20suitability%20for%20API-constrained%20deployments.%20This%20work%20provides%20a%20lightweight%20technique%20to%20enhance%20the%20trustworthiness%20of%20LLM%20responses%2C%20at%20the%20token%20level%2C%20after%20a%20single%20generation%20pass%2C%20for%20QA%20and%20Retrieval-Augmented%20Generation%20%28RAG%29%20systems.%20Our%20experiments%20confirmed%20the%20performance%20of%20our%20method%20against%20existing%20approaches%20on%20public%20dataset%20as%20well%20as%20for%20a%20financial%20framework%20analyzing%20annual%20company%20reports.%0ALink%3A%20http%3A//arxiv.org/abs/2509.04492v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearned%2520Hallucination%2520Detection%2520in%2520Black-Box%2520LLMs%2520using%2520Token-level%2520Entropy%2520Production%2520Rate%26entry.906535625%3DCharles%2520Moslonka%2520and%2520Hicham%2520Randrianarivo%2520and%2520Arthur%2520Garnier%2520and%2520Emmanuel%2520Malherbe%26entry.1292438233%3DHallucinations%2520in%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520outputs%2520for%2520Question%2520Answering%2520%2528QA%2529%2520tasks%2520can%2520critically%2520undermine%2520their%2520real-world%2520reliability.%2520This%2520paper%2520introduces%2520a%2520methodology%2520for%2520robust%252C%2520one-shot%2520hallucination%2520detection%252C%2520specifically%2520designed%2520for%2520scenarios%2520with%2520limited%2520data%2520access%252C%2520such%2520as%2520interacting%2520with%2520black-box%2520LLM%2520APIs%2520that%2520typically%2520expose%2520only%2520a%2520few%2520top%2520candidate%2520log-probabilities%2520per%2520token.%2520Our%2520approach%2520derives%2520uncertainty%2520indicators%2520directly%2520from%2520these%2520readily%2520available%2520log-probabilities%2520generated%2520during%2520non-greedy%2520decoding.%2520We%2520first%2520derive%2520an%2520Entropy%2520Production%2520Rate%2520%2528EPR%2529%2520that%2520offers%2520baseline%2520performance%252C%2520later%2520augmented%2520with%2520supervised%2520learning.%2520Our%2520learned%2520model%2520leverages%2520the%2520entropic%2520contributions%2520of%2520the%2520accessible%2520top-ranked%2520tokens%2520within%2520a%2520single%2520generated%2520sequence%252C%2520without%2520multiple%2520re-runs%2520per%2520query.%2520Evaluated%2520across%2520diverse%2520QA%2520datasets%2520and%2520multiple%2520LLMs%252C%2520this%2520estimator%2520significantly%2520improves%2520token-level%2520hallucination%2520detection%2520over%2520state-of-the-art%2520methods.%2520Crucially%252C%2520high%2520performance%2520is%2520demonstrated%2520using%2520only%2520the%2520typically%2520small%2520set%2520of%2520available%2520log-probabilities%2520%2528e.g.%252C%2520top-10%2520per%2520token%2529%252C%2520confirming%2520its%2520practical%2520efficiency%2520and%2520suitability%2520for%2520API-constrained%2520deployments.%2520This%2520work%2520provides%2520a%2520lightweight%2520technique%2520to%2520enhance%2520the%2520trustworthiness%2520of%2520LLM%2520responses%252C%2520at%2520the%2520token%2520level%252C%2520after%2520a%2520single%2520generation%2520pass%252C%2520for%2520QA%2520and%2520Retrieval-Augmented%2520Generation%2520%2528RAG%2529%2520systems.%2520Our%2520experiments%2520confirmed%2520the%2520performance%2520of%2520our%2520method%2520against%2520existing%2520approaches%2520on%2520public%2520dataset%2520as%2520well%2520as%2520for%2520a%2520financial%2520framework%2520analyzing%2520annual%2520company%2520reports.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.04492v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learned%20Hallucination%20Detection%20in%20Black-Box%20LLMs%20using%20Token-level%20Entropy%20Production%20Rate&entry.906535625=Charles%20Moslonka%20and%20Hicham%20Randrianarivo%20and%20Arthur%20Garnier%20and%20Emmanuel%20Malherbe&entry.1292438233=Hallucinations%20in%20Large%20Language%20Model%20%28LLM%29%20outputs%20for%20Question%20Answering%20%28QA%29%20tasks%20can%20critically%20undermine%20their%20real-world%20reliability.%20This%20paper%20introduces%20a%20methodology%20for%20robust%2C%20one-shot%20hallucination%20detection%2C%20specifically%20designed%20for%20scenarios%20with%20limited%20data%20access%2C%20such%20as%20interacting%20with%20black-box%20LLM%20APIs%20that%20typically%20expose%20only%20a%20few%20top%20candidate%20log-probabilities%20per%20token.%20Our%20approach%20derives%20uncertainty%20indicators%20directly%20from%20these%20readily%20available%20log-probabilities%20generated%20during%20non-greedy%20decoding.%20We%20first%20derive%20an%20Entropy%20Production%20Rate%20%28EPR%29%20that%20offers%20baseline%20performance%2C%20later%20augmented%20with%20supervised%20learning.%20Our%20learned%20model%20leverages%20the%20entropic%20contributions%20of%20the%20accessible%20top-ranked%20tokens%20within%20a%20single%20generated%20sequence%2C%20without%20multiple%20re-runs%20per%20query.%20Evaluated%20across%20diverse%20QA%20datasets%20and%20multiple%20LLMs%2C%20this%20estimator%20significantly%20improves%20token-level%20hallucination%20detection%20over%20state-of-the-art%20methods.%20Crucially%2C%20high%20performance%20is%20demonstrated%20using%20only%20the%20typically%20small%20set%20of%20available%20log-probabilities%20%28e.g.%2C%20top-10%20per%20token%29%2C%20confirming%20its%20practical%20efficiency%20and%20suitability%20for%20API-constrained%20deployments.%20This%20work%20provides%20a%20lightweight%20technique%20to%20enhance%20the%20trustworthiness%20of%20LLM%20responses%2C%20at%20the%20token%20level%2C%20after%20a%20single%20generation%20pass%2C%20for%20QA%20and%20Retrieval-Augmented%20Generation%20%28RAG%29%20systems.%20Our%20experiments%20confirmed%20the%20performance%20of%20our%20method%20against%20existing%20approaches%20on%20public%20dataset%20as%20well%20as%20for%20a%20financial%20framework%20analyzing%20annual%20company%20reports.&entry.1838667208=http%3A//arxiv.org/abs/2509.04492v2&entry.124074799=Read"},
{"title": "HardSecBench: Benchmarking the Security Awareness of LLMs for Hardware Code Generation", "author": "Qirui Chen and Jingxian Shuai and Shuangwu Chen and Shenghao Ye and Zijian Wen and Xufei Su and Jie Jin and Jiangming Li and Jun Chen and Xiaobin Tan and Jian Yang", "abstract": "Large language models (LLMs) are being increasingly integrated into practical hardware and firmware development pipelines for code generation. Existing studies have primarily focused on evaluating the functional correctness of LLM-generated code, yet paid limited attention to its security issues. However, LLM-generated code that appears functionally sound may embed security flaws which could induce catastrophic damages after deployment. This critical research gap motivates us to design a benchmark for assessing security awareness under realistic specifications. In this work, we introduce HardSecBench, a benchmark with 924 tasks spanning Verilog Register Transfer Level (RTL) and firmware-level C, covering 76 hardware-relevant Common Weakness Enumeration (CWE) entries. Each task includes a structured specification, a secure reference implementation, and executable tests. To automate artifact synthesis, we propose a multi-agent pipeline that decouples synthesis from verification and grounds evaluation in execution evidence, enabling reliable evaluation. Using HardSecBench, we evaluate a range of LLMs on hardware and firmware code generation and find that models often satisfy functional requirements while still leaving security risks. We also find that security results vary with prompting. These findings highlight pressing challenges and offer actionable insights for future advancements in LLM-assisted hardware design. Our data and code will be released soon.", "link": "http://arxiv.org/abs/2601.13864v1", "date": "2026-01-20", "relevancy": 1.6012, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4046}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3995}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.3995}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20HardSecBench%3A%20Benchmarking%20the%20Security%20Awareness%20of%20LLMs%20for%20Hardware%20Code%20Generation&body=Title%3A%20HardSecBench%3A%20Benchmarking%20the%20Security%20Awareness%20of%20LLMs%20for%20Hardware%20Code%20Generation%0AAuthor%3A%20Qirui%20Chen%20and%20Jingxian%20Shuai%20and%20Shuangwu%20Chen%20and%20Shenghao%20Ye%20and%20Zijian%20Wen%20and%20Xufei%20Su%20and%20Jie%20Jin%20and%20Jiangming%20Li%20and%20Jun%20Chen%20and%20Xiaobin%20Tan%20and%20Jian%20Yang%0AAbstract%3A%20Large%20language%20models%20%28LLMs%29%20are%20being%20increasingly%20integrated%20into%20practical%20hardware%20and%20firmware%20development%20pipelines%20for%20code%20generation.%20Existing%20studies%20have%20primarily%20focused%20on%20evaluating%20the%20functional%20correctness%20of%20LLM-generated%20code%2C%20yet%20paid%20limited%20attention%20to%20its%20security%20issues.%20However%2C%20LLM-generated%20code%20that%20appears%20functionally%20sound%20may%20embed%20security%20flaws%20which%20could%20induce%20catastrophic%20damages%20after%20deployment.%20This%20critical%20research%20gap%20motivates%20us%20to%20design%20a%20benchmark%20for%20assessing%20security%20awareness%20under%20realistic%20specifications.%20In%20this%20work%2C%20we%20introduce%20HardSecBench%2C%20a%20benchmark%20with%20924%20tasks%20spanning%20Verilog%20Register%20Transfer%20Level%20%28RTL%29%20and%20firmware-level%20C%2C%20covering%2076%20hardware-relevant%20Common%20Weakness%20Enumeration%20%28CWE%29%20entries.%20Each%20task%20includes%20a%20structured%20specification%2C%20a%20secure%20reference%20implementation%2C%20and%20executable%20tests.%20To%20automate%20artifact%20synthesis%2C%20we%20propose%20a%20multi-agent%20pipeline%20that%20decouples%20synthesis%20from%20verification%20and%20grounds%20evaluation%20in%20execution%20evidence%2C%20enabling%20reliable%20evaluation.%20Using%20HardSecBench%2C%20we%20evaluate%20a%20range%20of%20LLMs%20on%20hardware%20and%20firmware%20code%20generation%20and%20find%20that%20models%20often%20satisfy%20functional%20requirements%20while%20still%20leaving%20security%20risks.%20We%20also%20find%20that%20security%20results%20vary%20with%20prompting.%20These%20findings%20highlight%20pressing%20challenges%20and%20offer%20actionable%20insights%20for%20future%20advancements%20in%20LLM-assisted%20hardware%20design.%20Our%20data%20and%20code%20will%20be%20released%20soon.%0ALink%3A%20http%3A//arxiv.org/abs/2601.13864v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHardSecBench%253A%2520Benchmarking%2520the%2520Security%2520Awareness%2520of%2520LLMs%2520for%2520Hardware%2520Code%2520Generation%26entry.906535625%3DQirui%2520Chen%2520and%2520Jingxian%2520Shuai%2520and%2520Shuangwu%2520Chen%2520and%2520Shenghao%2520Ye%2520and%2520Zijian%2520Wen%2520and%2520Xufei%2520Su%2520and%2520Jie%2520Jin%2520and%2520Jiangming%2520Li%2520and%2520Jun%2520Chen%2520and%2520Xiaobin%2520Tan%2520and%2520Jian%2520Yang%26entry.1292438233%3DLarge%2520language%2520models%2520%2528LLMs%2529%2520are%2520being%2520increasingly%2520integrated%2520into%2520practical%2520hardware%2520and%2520firmware%2520development%2520pipelines%2520for%2520code%2520generation.%2520Existing%2520studies%2520have%2520primarily%2520focused%2520on%2520evaluating%2520the%2520functional%2520correctness%2520of%2520LLM-generated%2520code%252C%2520yet%2520paid%2520limited%2520attention%2520to%2520its%2520security%2520issues.%2520However%252C%2520LLM-generated%2520code%2520that%2520appears%2520functionally%2520sound%2520may%2520embed%2520security%2520flaws%2520which%2520could%2520induce%2520catastrophic%2520damages%2520after%2520deployment.%2520This%2520critical%2520research%2520gap%2520motivates%2520us%2520to%2520design%2520a%2520benchmark%2520for%2520assessing%2520security%2520awareness%2520under%2520realistic%2520specifications.%2520In%2520this%2520work%252C%2520we%2520introduce%2520HardSecBench%252C%2520a%2520benchmark%2520with%2520924%2520tasks%2520spanning%2520Verilog%2520Register%2520Transfer%2520Level%2520%2528RTL%2529%2520and%2520firmware-level%2520C%252C%2520covering%252076%2520hardware-relevant%2520Common%2520Weakness%2520Enumeration%2520%2528CWE%2529%2520entries.%2520Each%2520task%2520includes%2520a%2520structured%2520specification%252C%2520a%2520secure%2520reference%2520implementation%252C%2520and%2520executable%2520tests.%2520To%2520automate%2520artifact%2520synthesis%252C%2520we%2520propose%2520a%2520multi-agent%2520pipeline%2520that%2520decouples%2520synthesis%2520from%2520verification%2520and%2520grounds%2520evaluation%2520in%2520execution%2520evidence%252C%2520enabling%2520reliable%2520evaluation.%2520Using%2520HardSecBench%252C%2520we%2520evaluate%2520a%2520range%2520of%2520LLMs%2520on%2520hardware%2520and%2520firmware%2520code%2520generation%2520and%2520find%2520that%2520models%2520often%2520satisfy%2520functional%2520requirements%2520while%2520still%2520leaving%2520security%2520risks.%2520We%2520also%2520find%2520that%2520security%2520results%2520vary%2520with%2520prompting.%2520These%2520findings%2520highlight%2520pressing%2520challenges%2520and%2520offer%2520actionable%2520insights%2520for%2520future%2520advancements%2520in%2520LLM-assisted%2520hardware%2520design.%2520Our%2520data%2520and%2520code%2520will%2520be%2520released%2520soon.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2601.13864v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HardSecBench%3A%20Benchmarking%20the%20Security%20Awareness%20of%20LLMs%20for%20Hardware%20Code%20Generation&entry.906535625=Qirui%20Chen%20and%20Jingxian%20Shuai%20and%20Shuangwu%20Chen%20and%20Shenghao%20Ye%20and%20Zijian%20Wen%20and%20Xufei%20Su%20and%20Jie%20Jin%20and%20Jiangming%20Li%20and%20Jun%20Chen%20and%20Xiaobin%20Tan%20and%20Jian%20Yang&entry.1292438233=Large%20language%20models%20%28LLMs%29%20are%20being%20increasingly%20integrated%20into%20practical%20hardware%20and%20firmware%20development%20pipelines%20for%20code%20generation.%20Existing%20studies%20have%20primarily%20focused%20on%20evaluating%20the%20functional%20correctness%20of%20LLM-generated%20code%2C%20yet%20paid%20limited%20attention%20to%20its%20security%20issues.%20However%2C%20LLM-generated%20code%20that%20appears%20functionally%20sound%20may%20embed%20security%20flaws%20which%20could%20induce%20catastrophic%20damages%20after%20deployment.%20This%20critical%20research%20gap%20motivates%20us%20to%20design%20a%20benchmark%20for%20assessing%20security%20awareness%20under%20realistic%20specifications.%20In%20this%20work%2C%20we%20introduce%20HardSecBench%2C%20a%20benchmark%20with%20924%20tasks%20spanning%20Verilog%20Register%20Transfer%20Level%20%28RTL%29%20and%20firmware-level%20C%2C%20covering%2076%20hardware-relevant%20Common%20Weakness%20Enumeration%20%28CWE%29%20entries.%20Each%20task%20includes%20a%20structured%20specification%2C%20a%20secure%20reference%20implementation%2C%20and%20executable%20tests.%20To%20automate%20artifact%20synthesis%2C%20we%20propose%20a%20multi-agent%20pipeline%20that%20decouples%20synthesis%20from%20verification%20and%20grounds%20evaluation%20in%20execution%20evidence%2C%20enabling%20reliable%20evaluation.%20Using%20HardSecBench%2C%20we%20evaluate%20a%20range%20of%20LLMs%20on%20hardware%20and%20firmware%20code%20generation%20and%20find%20that%20models%20often%20satisfy%20functional%20requirements%20while%20still%20leaving%20security%20risks.%20We%20also%20find%20that%20security%20results%20vary%20with%20prompting.%20These%20findings%20highlight%20pressing%20challenges%20and%20offer%20actionable%20insights%20for%20future%20advancements%20in%20LLM-assisted%20hardware%20design.%20Our%20data%20and%20code%20will%20be%20released%20soon.&entry.1838667208=http%3A//arxiv.org/abs/2601.13864v1&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


