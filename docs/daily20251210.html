<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20251209.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs", "author": "Yijia Guo and Tong Hu and Zhiwei Li and Liwen Hu and Keming Qian and Xitong Lin and Shengbo Chen and Tiejun Huang and Lei Ma", "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled efficient free-viewpoint rendering and photorealistic scene reconstruction. While on-the-fly extensions of 3DGS have shown promise for real-time reconstruction from monocular RGB streams, they often fail to achieve complete 3D coverage due to the limited field of view (FOV). Employing a multi-camera rig fundamentally addresses this limitation. In this paper, we present the first on-the-fly 3D reconstruction framework for multi-camera rigs. Our method incrementally fuses dense RGB streams from multiple overlapping cameras into a unified Gaussian representation, achieving drift-free trajectory estimation and efficient online reconstruction. We propose a hierarchical camera initialization scheme that enables coarse inter-camera alignment without calibration, followed by a lightweight multi-camera bundle adjustment that stabilizes trajectories while maintaining real-time performance. Furthermore, we introduce a redundancy-free Gaussian sampling strategy and a frequency-aware optimization scheduler to reduce the number of Gaussian primitives and the required optimization iterations, thereby maintaining both efficiency and reconstruction fidelity. Our method reconstructs hundreds of meters of 3D scenes within just 2 minutes using only raw multi-camera video streams, demonstrating unprecedented speed, robustness, and Fidelity for on-the-fly 3D scene reconstruction.", "link": "http://arxiv.org/abs/2512.08498v1", "date": "2025-12-09", "relevancy": 3.5711, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.7757}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.7126}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6544}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On-the-fly%20Large-scale%203D%20Reconstruction%20from%20Multi-Camera%20Rigs&body=Title%3A%20On-the-fly%20Large-scale%203D%20Reconstruction%20from%20Multi-Camera%20Rigs%0AAuthor%3A%20Yijia%20Guo%20and%20Tong%20Hu%20and%20Zhiwei%20Li%20and%20Liwen%20Hu%20and%20Keming%20Qian%20and%20Xitong%20Lin%20and%20Shengbo%20Chen%20and%20Tiejun%20Huang%20and%20Lei%20Ma%0AAbstract%3A%20Recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20enabled%20efficient%20free-viewpoint%20rendering%20and%20photorealistic%20scene%20reconstruction.%20While%20on-the-fly%20extensions%20of%203DGS%20have%20shown%20promise%20for%20real-time%20reconstruction%20from%20monocular%20RGB%20streams%2C%20they%20often%20fail%20to%20achieve%20complete%203D%20coverage%20due%20to%20the%20limited%20field%20of%20view%20%28FOV%29.%20Employing%20a%20multi-camera%20rig%20fundamentally%20addresses%20this%20limitation.%20In%20this%20paper%2C%20we%20present%20the%20first%20on-the-fly%203D%20reconstruction%20framework%20for%20multi-camera%20rigs.%20Our%20method%20incrementally%20fuses%20dense%20RGB%20streams%20from%20multiple%20overlapping%20cameras%20into%20a%20unified%20Gaussian%20representation%2C%20achieving%20drift-free%20trajectory%20estimation%20and%20efficient%20online%20reconstruction.%20We%20propose%20a%20hierarchical%20camera%20initialization%20scheme%20that%20enables%20coarse%20inter-camera%20alignment%20without%20calibration%2C%20followed%20by%20a%20lightweight%20multi-camera%20bundle%20adjustment%20that%20stabilizes%20trajectories%20while%20maintaining%20real-time%20performance.%20Furthermore%2C%20we%20introduce%20a%20redundancy-free%20Gaussian%20sampling%20strategy%20and%20a%20frequency-aware%20optimization%20scheduler%20to%20reduce%20the%20number%20of%20Gaussian%20primitives%20and%20the%20required%20optimization%20iterations%2C%20thereby%20maintaining%20both%20efficiency%20and%20reconstruction%20fidelity.%20Our%20method%20reconstructs%20hundreds%20of%20meters%20of%203D%20scenes%20within%20just%202%20minutes%20using%20only%20raw%20multi-camera%20video%20streams%2C%20demonstrating%20unprecedented%20speed%2C%20robustness%2C%20and%20Fidelity%20for%20on-the-fly%203D%20scene%20reconstruction.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08498v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn-the-fly%2520Large-scale%25203D%2520Reconstruction%2520from%2520Multi-Camera%2520Rigs%26entry.906535625%3DYijia%2520Guo%2520and%2520Tong%2520Hu%2520and%2520Zhiwei%2520Li%2520and%2520Liwen%2520Hu%2520and%2520Keming%2520Qian%2520and%2520Xitong%2520Lin%2520and%2520Shengbo%2520Chen%2520and%2520Tiejun%2520Huang%2520and%2520Lei%2520Ma%26entry.1292438233%3DRecent%2520advances%2520in%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520have%2520enabled%2520efficient%2520free-viewpoint%2520rendering%2520and%2520photorealistic%2520scene%2520reconstruction.%2520While%2520on-the-fly%2520extensions%2520of%25203DGS%2520have%2520shown%2520promise%2520for%2520real-time%2520reconstruction%2520from%2520monocular%2520RGB%2520streams%252C%2520they%2520often%2520fail%2520to%2520achieve%2520complete%25203D%2520coverage%2520due%2520to%2520the%2520limited%2520field%2520of%2520view%2520%2528FOV%2529.%2520Employing%2520a%2520multi-camera%2520rig%2520fundamentally%2520addresses%2520this%2520limitation.%2520In%2520this%2520paper%252C%2520we%2520present%2520the%2520first%2520on-the-fly%25203D%2520reconstruction%2520framework%2520for%2520multi-camera%2520rigs.%2520Our%2520method%2520incrementally%2520fuses%2520dense%2520RGB%2520streams%2520from%2520multiple%2520overlapping%2520cameras%2520into%2520a%2520unified%2520Gaussian%2520representation%252C%2520achieving%2520drift-free%2520trajectory%2520estimation%2520and%2520efficient%2520online%2520reconstruction.%2520We%2520propose%2520a%2520hierarchical%2520camera%2520initialization%2520scheme%2520that%2520enables%2520coarse%2520inter-camera%2520alignment%2520without%2520calibration%252C%2520followed%2520by%2520a%2520lightweight%2520multi-camera%2520bundle%2520adjustment%2520that%2520stabilizes%2520trajectories%2520while%2520maintaining%2520real-time%2520performance.%2520Furthermore%252C%2520we%2520introduce%2520a%2520redundancy-free%2520Gaussian%2520sampling%2520strategy%2520and%2520a%2520frequency-aware%2520optimization%2520scheduler%2520to%2520reduce%2520the%2520number%2520of%2520Gaussian%2520primitives%2520and%2520the%2520required%2520optimization%2520iterations%252C%2520thereby%2520maintaining%2520both%2520efficiency%2520and%2520reconstruction%2520fidelity.%2520Our%2520method%2520reconstructs%2520hundreds%2520of%2520meters%2520of%25203D%2520scenes%2520within%2520just%25202%2520minutes%2520using%2520only%2520raw%2520multi-camera%2520video%2520streams%252C%2520demonstrating%2520unprecedented%2520speed%252C%2520robustness%252C%2520and%2520Fidelity%2520for%2520on-the-fly%25203D%2520scene%2520reconstruction.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08498v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On-the-fly%20Large-scale%203D%20Reconstruction%20from%20Multi-Camera%20Rigs&entry.906535625=Yijia%20Guo%20and%20Tong%20Hu%20and%20Zhiwei%20Li%20and%20Liwen%20Hu%20and%20Keming%20Qian%20and%20Xitong%20Lin%20and%20Shengbo%20Chen%20and%20Tiejun%20Huang%20and%20Lei%20Ma&entry.1292438233=Recent%20advances%20in%203D%20Gaussian%20Splatting%20%283DGS%29%20have%20enabled%20efficient%20free-viewpoint%20rendering%20and%20photorealistic%20scene%20reconstruction.%20While%20on-the-fly%20extensions%20of%203DGS%20have%20shown%20promise%20for%20real-time%20reconstruction%20from%20monocular%20RGB%20streams%2C%20they%20often%20fail%20to%20achieve%20complete%203D%20coverage%20due%20to%20the%20limited%20field%20of%20view%20%28FOV%29.%20Employing%20a%20multi-camera%20rig%20fundamentally%20addresses%20this%20limitation.%20In%20this%20paper%2C%20we%20present%20the%20first%20on-the-fly%203D%20reconstruction%20framework%20for%20multi-camera%20rigs.%20Our%20method%20incrementally%20fuses%20dense%20RGB%20streams%20from%20multiple%20overlapping%20cameras%20into%20a%20unified%20Gaussian%20representation%2C%20achieving%20drift-free%20trajectory%20estimation%20and%20efficient%20online%20reconstruction.%20We%20propose%20a%20hierarchical%20camera%20initialization%20scheme%20that%20enables%20coarse%20inter-camera%20alignment%20without%20calibration%2C%20followed%20by%20a%20lightweight%20multi-camera%20bundle%20adjustment%20that%20stabilizes%20trajectories%20while%20maintaining%20real-time%20performance.%20Furthermore%2C%20we%20introduce%20a%20redundancy-free%20Gaussian%20sampling%20strategy%20and%20a%20frequency-aware%20optimization%20scheduler%20to%20reduce%20the%20number%20of%20Gaussian%20primitives%20and%20the%20required%20optimization%20iterations%2C%20thereby%20maintaining%20both%20efficiency%20and%20reconstruction%20fidelity.%20Our%20method%20reconstructs%20hundreds%20of%20meters%20of%203D%20scenes%20within%20just%202%20minutes%20using%20only%20raw%20multi-camera%20video%20streams%2C%20demonstrating%20unprecedented%20speed%2C%20robustness%2C%20and%20Fidelity%20for%20on-the-fly%203D%20scene%20reconstruction.&entry.1838667208=http%3A//arxiv.org/abs/2512.08498v1&entry.124074799=Read"},
{"title": "Learning to Control Physically-simulated 3D Characters via Generating and Mimicking 2D Motions", "author": "Jianan Li and Xiao Chen and Tao Huang and Tien-Tsin Wong", "abstract": "Video data is more cost-effective than motion capture data for learning 3D character motion controllers, yet synthesizing realistic and diverse behaviors directly from videos remains challenging. Previous approaches typically rely on off-the-shelf motion reconstruction techniques to obtain 3D trajectories for physics-based imitation. These reconstruction methods struggle with generalizability, as they either require 3D training data (potentially scarce) or fail to produce physically plausible poses, hindering their application to challenging scenarios like human-object interaction (HOI) or non-human characters. We tackle this challenge by introducing Mimic2DM, a novel motion imitation framework that learns the control policy directly and solely from widely available 2D keypoint trajectories extracted from videos. By minimizing the reprojection error, we train a general single-view 2D motion tracking policy capable of following arbitrary 2D reference motions in physics simulation, using only 2D motion data. The policy, when trained on diverse 2D motions captured from different or slightly different viewpoints, can further acquire 3D motion tracking capabilities by aggregating multiple views. Moreover, we develop a transformer-based autoregressive 2D motion generator and integrate it into a hierarchical control framework, where the generator produces high-quality 2D reference trajectories to guide the tracking policy. We show that the proposed approach is versatile and can effectively learn to synthesize physically plausible and diverse motions across a range of domains, including dancing, soccer dribbling, and animal movements, without any reliance on explicit 3D motion data. Project Website: https://jiann-li.github.io/mimic2dm/", "link": "http://arxiv.org/abs/2512.08500v1", "date": "2025-12-09", "relevancy": 3.2692, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.7445}, {"title": "PhysGen: Rigid-Body Physics-Grounded Image-to-Video Generation", "link": "http://arxiv.org/abs/2409.18964v1", "similarity": 0.6124}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.6046}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20to%20Control%20Physically-simulated%203D%20Characters%20via%20Generating%20and%20Mimicking%202D%20Motions&body=Title%3A%20Learning%20to%20Control%20Physically-simulated%203D%20Characters%20via%20Generating%20and%20Mimicking%202D%20Motions%0AAuthor%3A%20Jianan%20Li%20and%20Xiao%20Chen%20and%20Tao%20Huang%20and%20Tien-Tsin%20Wong%0AAbstract%3A%20Video%20data%20is%20more%20cost-effective%20than%20motion%20capture%20data%20for%20learning%203D%20character%20motion%20controllers%2C%20yet%20synthesizing%20realistic%20and%20diverse%20behaviors%20directly%20from%20videos%20remains%20challenging.%20Previous%20approaches%20typically%20rely%20on%20off-the-shelf%20motion%20reconstruction%20techniques%20to%20obtain%203D%20trajectories%20for%20physics-based%20imitation.%20These%20reconstruction%20methods%20struggle%20with%20generalizability%2C%20as%20they%20either%20require%203D%20training%20data%20%28potentially%20scarce%29%20or%20fail%20to%20produce%20physically%20plausible%20poses%2C%20hindering%20their%20application%20to%20challenging%20scenarios%20like%20human-object%20interaction%20%28HOI%29%20or%20non-human%20characters.%20We%20tackle%20this%20challenge%20by%20introducing%20Mimic2DM%2C%20a%20novel%20motion%20imitation%20framework%20that%20learns%20the%20control%20policy%20directly%20and%20solely%20from%20widely%20available%202D%20keypoint%20trajectories%20extracted%20from%20videos.%20By%20minimizing%20the%20reprojection%20error%2C%20we%20train%20a%20general%20single-view%202D%20motion%20tracking%20policy%20capable%20of%20following%20arbitrary%202D%20reference%20motions%20in%20physics%20simulation%2C%20using%20only%202D%20motion%20data.%20The%20policy%2C%20when%20trained%20on%20diverse%202D%20motions%20captured%20from%20different%20or%20slightly%20different%20viewpoints%2C%20can%20further%20acquire%203D%20motion%20tracking%20capabilities%20by%20aggregating%20multiple%20views.%20Moreover%2C%20we%20develop%20a%20transformer-based%20autoregressive%202D%20motion%20generator%20and%20integrate%20it%20into%20a%20hierarchical%20control%20framework%2C%20where%20the%20generator%20produces%20high-quality%202D%20reference%20trajectories%20to%20guide%20the%20tracking%20policy.%20We%20show%20that%20the%20proposed%20approach%20is%20versatile%20and%20can%20effectively%20learn%20to%20synthesize%20physically%20plausible%20and%20diverse%20motions%20across%20a%20range%20of%20domains%2C%20including%20dancing%2C%20soccer%20dribbling%2C%20and%20animal%20movements%2C%20without%20any%20reliance%20on%20explicit%203D%20motion%20data.%20Project%20Website%3A%20https%3A//jiann-li.github.io/mimic2dm/%0ALink%3A%20http%3A//arxiv.org/abs/2512.08500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520to%2520Control%2520Physically-simulated%25203D%2520Characters%2520via%2520Generating%2520and%2520Mimicking%25202D%2520Motions%26entry.906535625%3DJianan%2520Li%2520and%2520Xiao%2520Chen%2520and%2520Tao%2520Huang%2520and%2520Tien-Tsin%2520Wong%26entry.1292438233%3DVideo%2520data%2520is%2520more%2520cost-effective%2520than%2520motion%2520capture%2520data%2520for%2520learning%25203D%2520character%2520motion%2520controllers%252C%2520yet%2520synthesizing%2520realistic%2520and%2520diverse%2520behaviors%2520directly%2520from%2520videos%2520remains%2520challenging.%2520Previous%2520approaches%2520typically%2520rely%2520on%2520off-the-shelf%2520motion%2520reconstruction%2520techniques%2520to%2520obtain%25203D%2520trajectories%2520for%2520physics-based%2520imitation.%2520These%2520reconstruction%2520methods%2520struggle%2520with%2520generalizability%252C%2520as%2520they%2520either%2520require%25203D%2520training%2520data%2520%2528potentially%2520scarce%2529%2520or%2520fail%2520to%2520produce%2520physically%2520plausible%2520poses%252C%2520hindering%2520their%2520application%2520to%2520challenging%2520scenarios%2520like%2520human-object%2520interaction%2520%2528HOI%2529%2520or%2520non-human%2520characters.%2520We%2520tackle%2520this%2520challenge%2520by%2520introducing%2520Mimic2DM%252C%2520a%2520novel%2520motion%2520imitation%2520framework%2520that%2520learns%2520the%2520control%2520policy%2520directly%2520and%2520solely%2520from%2520widely%2520available%25202D%2520keypoint%2520trajectories%2520extracted%2520from%2520videos.%2520By%2520minimizing%2520the%2520reprojection%2520error%252C%2520we%2520train%2520a%2520general%2520single-view%25202D%2520motion%2520tracking%2520policy%2520capable%2520of%2520following%2520arbitrary%25202D%2520reference%2520motions%2520in%2520physics%2520simulation%252C%2520using%2520only%25202D%2520motion%2520data.%2520The%2520policy%252C%2520when%2520trained%2520on%2520diverse%25202D%2520motions%2520captured%2520from%2520different%2520or%2520slightly%2520different%2520viewpoints%252C%2520can%2520further%2520acquire%25203D%2520motion%2520tracking%2520capabilities%2520by%2520aggregating%2520multiple%2520views.%2520Moreover%252C%2520we%2520develop%2520a%2520transformer-based%2520autoregressive%25202D%2520motion%2520generator%2520and%2520integrate%2520it%2520into%2520a%2520hierarchical%2520control%2520framework%252C%2520where%2520the%2520generator%2520produces%2520high-quality%25202D%2520reference%2520trajectories%2520to%2520guide%2520the%2520tracking%2520policy.%2520We%2520show%2520that%2520the%2520proposed%2520approach%2520is%2520versatile%2520and%2520can%2520effectively%2520learn%2520to%2520synthesize%2520physically%2520plausible%2520and%2520diverse%2520motions%2520across%2520a%2520range%2520of%2520domains%252C%2520including%2520dancing%252C%2520soccer%2520dribbling%252C%2520and%2520animal%2520movements%252C%2520without%2520any%2520reliance%2520on%2520explicit%25203D%2520motion%2520data.%2520Project%2520Website%253A%2520https%253A//jiann-li.github.io/mimic2dm/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20to%20Control%20Physically-simulated%203D%20Characters%20via%20Generating%20and%20Mimicking%202D%20Motions&entry.906535625=Jianan%20Li%20and%20Xiao%20Chen%20and%20Tao%20Huang%20and%20Tien-Tsin%20Wong&entry.1292438233=Video%20data%20is%20more%20cost-effective%20than%20motion%20capture%20data%20for%20learning%203D%20character%20motion%20controllers%2C%20yet%20synthesizing%20realistic%20and%20diverse%20behaviors%20directly%20from%20videos%20remains%20challenging.%20Previous%20approaches%20typically%20rely%20on%20off-the-shelf%20motion%20reconstruction%20techniques%20to%20obtain%203D%20trajectories%20for%20physics-based%20imitation.%20These%20reconstruction%20methods%20struggle%20with%20generalizability%2C%20as%20they%20either%20require%203D%20training%20data%20%28potentially%20scarce%29%20or%20fail%20to%20produce%20physically%20plausible%20poses%2C%20hindering%20their%20application%20to%20challenging%20scenarios%20like%20human-object%20interaction%20%28HOI%29%20or%20non-human%20characters.%20We%20tackle%20this%20challenge%20by%20introducing%20Mimic2DM%2C%20a%20novel%20motion%20imitation%20framework%20that%20learns%20the%20control%20policy%20directly%20and%20solely%20from%20widely%20available%202D%20keypoint%20trajectories%20extracted%20from%20videos.%20By%20minimizing%20the%20reprojection%20error%2C%20we%20train%20a%20general%20single-view%202D%20motion%20tracking%20policy%20capable%20of%20following%20arbitrary%202D%20reference%20motions%20in%20physics%20simulation%2C%20using%20only%202D%20motion%20data.%20The%20policy%2C%20when%20trained%20on%20diverse%202D%20motions%20captured%20from%20different%20or%20slightly%20different%20viewpoints%2C%20can%20further%20acquire%203D%20motion%20tracking%20capabilities%20by%20aggregating%20multiple%20views.%20Moreover%2C%20we%20develop%20a%20transformer-based%20autoregressive%202D%20motion%20generator%20and%20integrate%20it%20into%20a%20hierarchical%20control%20framework%2C%20where%20the%20generator%20produces%20high-quality%202D%20reference%20trajectories%20to%20guide%20the%20tracking%20policy.%20We%20show%20that%20the%20proposed%20approach%20is%20versatile%20and%20can%20effectively%20learn%20to%20synthesize%20physically%20plausible%20and%20diverse%20motions%20across%20a%20range%20of%20domains%2C%20including%20dancing%2C%20soccer%20dribbling%2C%20and%20animal%20movements%2C%20without%20any%20reliance%20on%20explicit%203D%20motion%20data.%20Project%20Website%3A%20https%3A//jiann-li.github.io/mimic2dm/&entry.1838667208=http%3A//arxiv.org/abs/2512.08500v1&entry.124074799=Read"},
{"title": "OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics", "author": "Jisang Yoo and Gyeongjin Kang and Hyun-kyu Ko and Hyeonwoo Yu and Eunbyung Park", "abstract": "Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.", "link": "http://arxiv.org/abs/2512.08625v1", "date": "2025-12-09", "relevancy": 3.1907, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.699}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6089}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6065}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OpenMonoGS-SLAM%3A%20Monocular%20Gaussian%20Splatting%20SLAM%20with%20Open-set%20Semantics&body=Title%3A%20OpenMonoGS-SLAM%3A%20Monocular%20Gaussian%20Splatting%20SLAM%20with%20Open-set%20Semantics%0AAuthor%3A%20Jisang%20Yoo%20and%20Gyeongjin%20Kang%20and%20Hyun-kyu%20Ko%20and%20Hyeonwoo%20Yu%20and%20Eunbyung%20Park%0AAbstract%3A%20Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20is%20a%20foundational%20component%20in%20robotics%2C%20AR/VR%2C%20and%20autonomous%20systems.%20With%20the%20rising%20focus%20on%20spatial%20AI%20in%20recent%20years%2C%20combining%20SLAM%20with%20semantic%20understanding%20has%20become%20increasingly%20important%20for%20enabling%20intelligent%20perception%20and%20interaction.%20Recent%20efforts%20have%20explored%20this%20integration%2C%20but%20they%20often%20rely%20on%20depth%20sensors%20or%20closed-set%20semantic%20models%2C%20limiting%20their%20scalability%20and%20adaptability%20in%20open-world%20environments.%20In%20this%20work%2C%20we%20present%20OpenMonoGS-SLAM%2C%20the%20first%20monocular%20SLAM%20framework%20that%20unifies%203D%20Gaussian%20Splatting%20%283DGS%29%20with%20open-set%20semantic%20understanding.%20To%20achieve%20our%20goal%2C%20we%20leverage%20recent%20advances%20in%20Visual%20Foundation%20Models%20%28VFMs%29%2C%20including%20MASt3R%20for%20visual%20geometry%20and%20SAM%20and%20CLIP%20for%20open-vocabulary%20semantics.%20These%20models%20provide%20robust%20generalization%20across%20diverse%20tasks%2C%20enabling%20accurate%20monocular%20camera%20tracking%20and%20mapping%2C%20as%20well%20as%20a%20rich%20understanding%20of%20semantics%20in%20open-world%20environments.%20Our%20method%20operates%20without%20any%20depth%20input%20or%203D%20semantic%20ground%20truth%2C%20relying%20solely%20on%20self-supervised%20learning%20objectives.%20Furthermore%2C%20we%20propose%20a%20memory%20mechanism%20specifically%20designed%20to%20manage%20high-dimensional%20semantic%20features%2C%20which%20effectively%20constructs%20Gaussian%20semantic%20feature%20maps%2C%20leading%20to%20strong%20overall%20performance.%20Experimental%20results%20demonstrate%20that%20our%20approach%20achieves%20performance%20comparable%20to%20or%20surpassing%20existing%20baselines%20in%20both%20closed-set%20and%20open-set%20segmentation%20tasks%2C%20all%20without%20relying%20on%20supplementary%20sensors%20such%20as%20depth%20maps%20or%20semantic%20annotations.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08625v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOpenMonoGS-SLAM%253A%2520Monocular%2520Gaussian%2520Splatting%2520SLAM%2520with%2520Open-set%2520Semantics%26entry.906535625%3DJisang%2520Yoo%2520and%2520Gyeongjin%2520Kang%2520and%2520Hyun-kyu%2520Ko%2520and%2520Hyeonwoo%2520Yu%2520and%2520Eunbyung%2520Park%26entry.1292438233%3DSimultaneous%2520Localization%2520and%2520Mapping%2520%2528SLAM%2529%2520is%2520a%2520foundational%2520component%2520in%2520robotics%252C%2520AR/VR%252C%2520and%2520autonomous%2520systems.%2520With%2520the%2520rising%2520focus%2520on%2520spatial%2520AI%2520in%2520recent%2520years%252C%2520combining%2520SLAM%2520with%2520semantic%2520understanding%2520has%2520become%2520increasingly%2520important%2520for%2520enabling%2520intelligent%2520perception%2520and%2520interaction.%2520Recent%2520efforts%2520have%2520explored%2520this%2520integration%252C%2520but%2520they%2520often%2520rely%2520on%2520depth%2520sensors%2520or%2520closed-set%2520semantic%2520models%252C%2520limiting%2520their%2520scalability%2520and%2520adaptability%2520in%2520open-world%2520environments.%2520In%2520this%2520work%252C%2520we%2520present%2520OpenMonoGS-SLAM%252C%2520the%2520first%2520monocular%2520SLAM%2520framework%2520that%2520unifies%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%2520with%2520open-set%2520semantic%2520understanding.%2520To%2520achieve%2520our%2520goal%252C%2520we%2520leverage%2520recent%2520advances%2520in%2520Visual%2520Foundation%2520Models%2520%2528VFMs%2529%252C%2520including%2520MASt3R%2520for%2520visual%2520geometry%2520and%2520SAM%2520and%2520CLIP%2520for%2520open-vocabulary%2520semantics.%2520These%2520models%2520provide%2520robust%2520generalization%2520across%2520diverse%2520tasks%252C%2520enabling%2520accurate%2520monocular%2520camera%2520tracking%2520and%2520mapping%252C%2520as%2520well%2520as%2520a%2520rich%2520understanding%2520of%2520semantics%2520in%2520open-world%2520environments.%2520Our%2520method%2520operates%2520without%2520any%2520depth%2520input%2520or%25203D%2520semantic%2520ground%2520truth%252C%2520relying%2520solely%2520on%2520self-supervised%2520learning%2520objectives.%2520Furthermore%252C%2520we%2520propose%2520a%2520memory%2520mechanism%2520specifically%2520designed%2520to%2520manage%2520high-dimensional%2520semantic%2520features%252C%2520which%2520effectively%2520constructs%2520Gaussian%2520semantic%2520feature%2520maps%252C%2520leading%2520to%2520strong%2520overall%2520performance.%2520Experimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520achieves%2520performance%2520comparable%2520to%2520or%2520surpassing%2520existing%2520baselines%2520in%2520both%2520closed-set%2520and%2520open-set%2520segmentation%2520tasks%252C%2520all%2520without%2520relying%2520on%2520supplementary%2520sensors%2520such%2520as%2520depth%2520maps%2520or%2520semantic%2520annotations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08625v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OpenMonoGS-SLAM%3A%20Monocular%20Gaussian%20Splatting%20SLAM%20with%20Open-set%20Semantics&entry.906535625=Jisang%20Yoo%20and%20Gyeongjin%20Kang%20and%20Hyun-kyu%20Ko%20and%20Hyeonwoo%20Yu%20and%20Eunbyung%20Park&entry.1292438233=Simultaneous%20Localization%20and%20Mapping%20%28SLAM%29%20is%20a%20foundational%20component%20in%20robotics%2C%20AR/VR%2C%20and%20autonomous%20systems.%20With%20the%20rising%20focus%20on%20spatial%20AI%20in%20recent%20years%2C%20combining%20SLAM%20with%20semantic%20understanding%20has%20become%20increasingly%20important%20for%20enabling%20intelligent%20perception%20and%20interaction.%20Recent%20efforts%20have%20explored%20this%20integration%2C%20but%20they%20often%20rely%20on%20depth%20sensors%20or%20closed-set%20semantic%20models%2C%20limiting%20their%20scalability%20and%20adaptability%20in%20open-world%20environments.%20In%20this%20work%2C%20we%20present%20OpenMonoGS-SLAM%2C%20the%20first%20monocular%20SLAM%20framework%20that%20unifies%203D%20Gaussian%20Splatting%20%283DGS%29%20with%20open-set%20semantic%20understanding.%20To%20achieve%20our%20goal%2C%20we%20leverage%20recent%20advances%20in%20Visual%20Foundation%20Models%20%28VFMs%29%2C%20including%20MASt3R%20for%20visual%20geometry%20and%20SAM%20and%20CLIP%20for%20open-vocabulary%20semantics.%20These%20models%20provide%20robust%20generalization%20across%20diverse%20tasks%2C%20enabling%20accurate%20monocular%20camera%20tracking%20and%20mapping%2C%20as%20well%20as%20a%20rich%20understanding%20of%20semantics%20in%20open-world%20environments.%20Our%20method%20operates%20without%20any%20depth%20input%20or%203D%20semantic%20ground%20truth%2C%20relying%20solely%20on%20self-supervised%20learning%20objectives.%20Furthermore%2C%20we%20propose%20a%20memory%20mechanism%20specifically%20designed%20to%20manage%20high-dimensional%20semantic%20features%2C%20which%20effectively%20constructs%20Gaussian%20semantic%20feature%20maps%2C%20leading%20to%20strong%20overall%20performance.%20Experimental%20results%20demonstrate%20that%20our%20approach%20achieves%20performance%20comparable%20to%20or%20surpassing%20existing%20baselines%20in%20both%20closed-set%20and%20open-set%20segmentation%20tasks%2C%20all%20without%20relying%20on%20supplementary%20sensors%20such%20as%20depth%20maps%20or%20semantic%20annotations.&entry.1838667208=http%3A//arxiv.org/abs/2512.08625v1&entry.124074799=Read"},
{"title": "Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning", "author": "Yi Zhang and Chun-Wun Cheng and Junyi He and Ke Yu and Yushun Tang and Carola-Bibiane Sch\u00f6nlieb and Zhihai He and Angelica I. Aviles-Rivero", "abstract": "Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \\textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the Poincar\u00e9 ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.", "link": "http://arxiv.org/abs/2512.08820v1", "date": "2025-12-09", "relevancy": 3.1461, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.687}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6003}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Training-Free%20Dual%20Hyperbolic%20Adapters%20for%20Better%20Cross-Modal%20Reasoning&body=Title%3A%20Training-Free%20Dual%20Hyperbolic%20Adapters%20for%20Better%20Cross-Modal%20Reasoning%0AAuthor%3A%20Yi%20Zhang%20and%20Chun-Wun%20Cheng%20and%20Junyi%20He%20and%20Ke%20Yu%20and%20Yushun%20Tang%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Zhihai%20He%20and%20Angelica%20I.%20Aviles-Rivero%0AAbstract%3A%20Recent%20research%20in%20Vision-Language%20Models%20%28VLMs%29%20has%20significantly%20advanced%20our%20capabilities%20in%20cross-modal%20reasoning.%20However%2C%20existing%20methods%20suffer%20from%20performance%20degradation%20with%20domain%20changes%20or%20require%20substantial%20computational%20resources%20for%20fine-tuning%20in%20new%20domains.%20To%20address%20this%20issue%2C%20we%20develop%20a%20new%20adaptation%20method%20for%20large%20vision-language%20models%2C%20called%20%5Ctextit%7BTraining-free%20Dual%20Hyperbolic%20Adapters%7D%20%28T-DHA%29.%20We%20characterize%20the%20vision-language%20relationship%20between%20semantic%20concepts%2C%20which%20typically%20has%20a%20hierarchical%20tree%20structure%2C%20in%20the%20hyperbolic%20space%20instead%20of%20the%20traditional%20Euclidean%20space.%20Hyperbolic%20spaces%20exhibit%20exponential%20volume%20growth%20with%20radius%2C%20unlike%20the%20polynomial%20growth%20in%20Euclidean%20space.%20We%20find%20that%20this%20unique%20property%20is%20particularly%20effective%20for%20embedding%20hierarchical%20data%20structures%20using%20the%20Poincar%C3%A9%20ball%20model%2C%20achieving%20significantly%20improved%20representation%20and%20discrimination%20power.%20Coupled%20with%20negative%20learning%2C%20it%20provides%20more%20accurate%20and%20robust%20classifications%20with%20fewer%20feature%20dimensions.%20Our%20extensive%20experimental%20results%20on%20various%20datasets%20demonstrate%20that%20the%20T-DHA%20method%20significantly%20outperforms%20existing%20state-of-the-art%20methods%20in%20few-shot%20image%20recognition%20and%20domain%20generalization%20tasks.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08820v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTraining-Free%2520Dual%2520Hyperbolic%2520Adapters%2520for%2520Better%2520Cross-Modal%2520Reasoning%26entry.906535625%3DYi%2520Zhang%2520and%2520Chun-Wun%2520Cheng%2520and%2520Junyi%2520He%2520and%2520Ke%2520Yu%2520and%2520Yushun%2520Tang%2520and%2520Carola-Bibiane%2520Sch%25C3%25B6nlieb%2520and%2520Zhihai%2520He%2520and%2520Angelica%2520I.%2520Aviles-Rivero%26entry.1292438233%3DRecent%2520research%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520has%2520significantly%2520advanced%2520our%2520capabilities%2520in%2520cross-modal%2520reasoning.%2520However%252C%2520existing%2520methods%2520suffer%2520from%2520performance%2520degradation%2520with%2520domain%2520changes%2520or%2520require%2520substantial%2520computational%2520resources%2520for%2520fine-tuning%2520in%2520new%2520domains.%2520To%2520address%2520this%2520issue%252C%2520we%2520develop%2520a%2520new%2520adaptation%2520method%2520for%2520large%2520vision-language%2520models%252C%2520called%2520%255Ctextit%257BTraining-free%2520Dual%2520Hyperbolic%2520Adapters%257D%2520%2528T-DHA%2529.%2520We%2520characterize%2520the%2520vision-language%2520relationship%2520between%2520semantic%2520concepts%252C%2520which%2520typically%2520has%2520a%2520hierarchical%2520tree%2520structure%252C%2520in%2520the%2520hyperbolic%2520space%2520instead%2520of%2520the%2520traditional%2520Euclidean%2520space.%2520Hyperbolic%2520spaces%2520exhibit%2520exponential%2520volume%2520growth%2520with%2520radius%252C%2520unlike%2520the%2520polynomial%2520growth%2520in%2520Euclidean%2520space.%2520We%2520find%2520that%2520this%2520unique%2520property%2520is%2520particularly%2520effective%2520for%2520embedding%2520hierarchical%2520data%2520structures%2520using%2520the%2520Poincar%25C3%25A9%2520ball%2520model%252C%2520achieving%2520significantly%2520improved%2520representation%2520and%2520discrimination%2520power.%2520Coupled%2520with%2520negative%2520learning%252C%2520it%2520provides%2520more%2520accurate%2520and%2520robust%2520classifications%2520with%2520fewer%2520feature%2520dimensions.%2520Our%2520extensive%2520experimental%2520results%2520on%2520various%2520datasets%2520demonstrate%2520that%2520the%2520T-DHA%2520method%2520significantly%2520outperforms%2520existing%2520state-of-the-art%2520methods%2520in%2520few-shot%2520image%2520recognition%2520and%2520domain%2520generalization%2520tasks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08820v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Training-Free%20Dual%20Hyperbolic%20Adapters%20for%20Better%20Cross-Modal%20Reasoning&entry.906535625=Yi%20Zhang%20and%20Chun-Wun%20Cheng%20and%20Junyi%20He%20and%20Ke%20Yu%20and%20Yushun%20Tang%20and%20Carola-Bibiane%20Sch%C3%B6nlieb%20and%20Zhihai%20He%20and%20Angelica%20I.%20Aviles-Rivero&entry.1292438233=Recent%20research%20in%20Vision-Language%20Models%20%28VLMs%29%20has%20significantly%20advanced%20our%20capabilities%20in%20cross-modal%20reasoning.%20However%2C%20existing%20methods%20suffer%20from%20performance%20degradation%20with%20domain%20changes%20or%20require%20substantial%20computational%20resources%20for%20fine-tuning%20in%20new%20domains.%20To%20address%20this%20issue%2C%20we%20develop%20a%20new%20adaptation%20method%20for%20large%20vision-language%20models%2C%20called%20%5Ctextit%7BTraining-free%20Dual%20Hyperbolic%20Adapters%7D%20%28T-DHA%29.%20We%20characterize%20the%20vision-language%20relationship%20between%20semantic%20concepts%2C%20which%20typically%20has%20a%20hierarchical%20tree%20structure%2C%20in%20the%20hyperbolic%20space%20instead%20of%20the%20traditional%20Euclidean%20space.%20Hyperbolic%20spaces%20exhibit%20exponential%20volume%20growth%20with%20radius%2C%20unlike%20the%20polynomial%20growth%20in%20Euclidean%20space.%20We%20find%20that%20this%20unique%20property%20is%20particularly%20effective%20for%20embedding%20hierarchical%20data%20structures%20using%20the%20Poincar%C3%A9%20ball%20model%2C%20achieving%20significantly%20improved%20representation%20and%20discrimination%20power.%20Coupled%20with%20negative%20learning%2C%20it%20provides%20more%20accurate%20and%20robust%20classifications%20with%20fewer%20feature%20dimensions.%20Our%20extensive%20experimental%20results%20on%20various%20datasets%20demonstrate%20that%20the%20T-DHA%20method%20significantly%20outperforms%20existing%20state-of-the-art%20methods%20in%20few-shot%20image%20recognition%20and%20domain%20generalization%20tasks.&entry.1838667208=http%3A//arxiv.org/abs/2512.08820v1&entry.124074799=Read"},
{"title": "Visionary: The World Model Carrier Built on WebGPU-Powered Gaussian Splatting Platform", "author": "Yuning Gong and Yifei Liu and Yifan Zhan and Muyao Niu and Xueying Li and Yuanjun Liao and Jiaming Chen and Yuanyuan Gao and Jiaqi Chen and Minming Chen and Li Zhou and Yuning Zhang and Wei Wang and Xiaoqing Hou and Huaxi Huang and Shixiang Tang and Le Ma and Dingwen Zhang and Xue Yang and Junchi Yan and Yanchi Zhang and Yinqiang Zheng and Xiao Sun and Zhihang Zhong", "abstract": "Neural rendering, particularly 3D Gaussian Splatting (3DGS), has evolved rapidly and become a key component for building world models. However, existing viewer solutions remain fragmented, heavy, or constrained by legacy pipelines, resulting in high deployment friction and limited support for dynamic content and generative models. In this work, we present Visionary, an open, web-native platform for real-time various Gaussian Splatting and meshes rendering. Built on an efficient WebGPU renderer with per-frame ONNX inference, Visionary enables dynamic neural processing while maintaining a lightweight, \"click-to-run\" browser experience. It introduces a standardized Gaussian Generator contract, which not only supports standard 3DGS rendering but also allows plug-and-play algorithms to generate or update Gaussians each frame. Such inference also enables us to apply feedforward generative post-processing. The platform further offers a plug in three.js library with a concise TypeScript API for seamless integration into existing web applications. Experiments show that, under identical 3DGS assets, Visionary achieves superior rendering efficiency compared to current Web viewers due to GPU-based primitive sorting. It already supports multiple variants, including MLP-based 3DGS, 4DGS, neural avatars, and style transformation or enhancement networks. By unifying inference and rendering directly in the browser, Visionary significantly lowers the barrier to reproduction, comparison, and deployment of 3DGS-family methods, serving as a unified World Model Carrier for both reconstructive and generative paradigms.", "link": "http://arxiv.org/abs/2512.08478v1", "date": "2025-12-09", "relevancy": 3.1198, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6361}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.6277}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6081}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Visionary%3A%20The%20World%20Model%20Carrier%20Built%20on%20WebGPU-Powered%20Gaussian%20Splatting%20Platform&body=Title%3A%20Visionary%3A%20The%20World%20Model%20Carrier%20Built%20on%20WebGPU-Powered%20Gaussian%20Splatting%20Platform%0AAuthor%3A%20Yuning%20Gong%20and%20Yifei%20Liu%20and%20Yifan%20Zhan%20and%20Muyao%20Niu%20and%20Xueying%20Li%20and%20Yuanjun%20Liao%20and%20Jiaming%20Chen%20and%20Yuanyuan%20Gao%20and%20Jiaqi%20Chen%20and%20Minming%20Chen%20and%20Li%20Zhou%20and%20Yuning%20Zhang%20and%20Wei%20Wang%20and%20Xiaoqing%20Hou%20and%20Huaxi%20Huang%20and%20Shixiang%20Tang%20and%20Le%20Ma%20and%20Dingwen%20Zhang%20and%20Xue%20Yang%20and%20Junchi%20Yan%20and%20Yanchi%20Zhang%20and%20Yinqiang%20Zheng%20and%20Xiao%20Sun%20and%20Zhihang%20Zhong%0AAbstract%3A%20Neural%20rendering%2C%20particularly%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20has%20evolved%20rapidly%20and%20become%20a%20key%20component%20for%20building%20world%20models.%20However%2C%20existing%20viewer%20solutions%20remain%20fragmented%2C%20heavy%2C%20or%20constrained%20by%20legacy%20pipelines%2C%20resulting%20in%20high%20deployment%20friction%20and%20limited%20support%20for%20dynamic%20content%20and%20generative%20models.%20In%20this%20work%2C%20we%20present%20Visionary%2C%20an%20open%2C%20web-native%20platform%20for%20real-time%20various%20Gaussian%20Splatting%20and%20meshes%20rendering.%20Built%20on%20an%20efficient%20WebGPU%20renderer%20with%20per-frame%20ONNX%20inference%2C%20Visionary%20enables%20dynamic%20neural%20processing%20while%20maintaining%20a%20lightweight%2C%20%22click-to-run%22%20browser%20experience.%20It%20introduces%20a%20standardized%20Gaussian%20Generator%20contract%2C%20which%20not%20only%20supports%20standard%203DGS%20rendering%20but%20also%20allows%20plug-and-play%20algorithms%20to%20generate%20or%20update%20Gaussians%20each%20frame.%20Such%20inference%20also%20enables%20us%20to%20apply%20feedforward%20generative%20post-processing.%20The%20platform%20further%20offers%20a%20plug%20in%20three.js%20library%20with%20a%20concise%20TypeScript%20API%20for%20seamless%20integration%20into%20existing%20web%20applications.%20Experiments%20show%20that%2C%20under%20identical%203DGS%20assets%2C%20Visionary%20achieves%20superior%20rendering%20efficiency%20compared%20to%20current%20Web%20viewers%20due%20to%20GPU-based%20primitive%20sorting.%20It%20already%20supports%20multiple%20variants%2C%20including%20MLP-based%203DGS%2C%204DGS%2C%20neural%20avatars%2C%20and%20style%20transformation%20or%20enhancement%20networks.%20By%20unifying%20inference%20and%20rendering%20directly%20in%20the%20browser%2C%20Visionary%20significantly%20lowers%20the%20barrier%20to%20reproduction%2C%20comparison%2C%20and%20deployment%20of%203DGS-family%20methods%2C%20serving%20as%20a%20unified%20World%20Model%20Carrier%20for%20both%20reconstructive%20and%20generative%20paradigms.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08478v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVisionary%253A%2520The%2520World%2520Model%2520Carrier%2520Built%2520on%2520WebGPU-Powered%2520Gaussian%2520Splatting%2520Platform%26entry.906535625%3DYuning%2520Gong%2520and%2520Yifei%2520Liu%2520and%2520Yifan%2520Zhan%2520and%2520Muyao%2520Niu%2520and%2520Xueying%2520Li%2520and%2520Yuanjun%2520Liao%2520and%2520Jiaming%2520Chen%2520and%2520Yuanyuan%2520Gao%2520and%2520Jiaqi%2520Chen%2520and%2520Minming%2520Chen%2520and%2520Li%2520Zhou%2520and%2520Yuning%2520Zhang%2520and%2520Wei%2520Wang%2520and%2520Xiaoqing%2520Hou%2520and%2520Huaxi%2520Huang%2520and%2520Shixiang%2520Tang%2520and%2520Le%2520Ma%2520and%2520Dingwen%2520Zhang%2520and%2520Xue%2520Yang%2520and%2520Junchi%2520Yan%2520and%2520Yanchi%2520Zhang%2520and%2520Yinqiang%2520Zheng%2520and%2520Xiao%2520Sun%2520and%2520Zhihang%2520Zhong%26entry.1292438233%3DNeural%2520rendering%252C%2520particularly%25203D%2520Gaussian%2520Splatting%2520%25283DGS%2529%252C%2520has%2520evolved%2520rapidly%2520and%2520become%2520a%2520key%2520component%2520for%2520building%2520world%2520models.%2520However%252C%2520existing%2520viewer%2520solutions%2520remain%2520fragmented%252C%2520heavy%252C%2520or%2520constrained%2520by%2520legacy%2520pipelines%252C%2520resulting%2520in%2520high%2520deployment%2520friction%2520and%2520limited%2520support%2520for%2520dynamic%2520content%2520and%2520generative%2520models.%2520In%2520this%2520work%252C%2520we%2520present%2520Visionary%252C%2520an%2520open%252C%2520web-native%2520platform%2520for%2520real-time%2520various%2520Gaussian%2520Splatting%2520and%2520meshes%2520rendering.%2520Built%2520on%2520an%2520efficient%2520WebGPU%2520renderer%2520with%2520per-frame%2520ONNX%2520inference%252C%2520Visionary%2520enables%2520dynamic%2520neural%2520processing%2520while%2520maintaining%2520a%2520lightweight%252C%2520%2522click-to-run%2522%2520browser%2520experience.%2520It%2520introduces%2520a%2520standardized%2520Gaussian%2520Generator%2520contract%252C%2520which%2520not%2520only%2520supports%2520standard%25203DGS%2520rendering%2520but%2520also%2520allows%2520plug-and-play%2520algorithms%2520to%2520generate%2520or%2520update%2520Gaussians%2520each%2520frame.%2520Such%2520inference%2520also%2520enables%2520us%2520to%2520apply%2520feedforward%2520generative%2520post-processing.%2520The%2520platform%2520further%2520offers%2520a%2520plug%2520in%2520three.js%2520library%2520with%2520a%2520concise%2520TypeScript%2520API%2520for%2520seamless%2520integration%2520into%2520existing%2520web%2520applications.%2520Experiments%2520show%2520that%252C%2520under%2520identical%25203DGS%2520assets%252C%2520Visionary%2520achieves%2520superior%2520rendering%2520efficiency%2520compared%2520to%2520current%2520Web%2520viewers%2520due%2520to%2520GPU-based%2520primitive%2520sorting.%2520It%2520already%2520supports%2520multiple%2520variants%252C%2520including%2520MLP-based%25203DGS%252C%25204DGS%252C%2520neural%2520avatars%252C%2520and%2520style%2520transformation%2520or%2520enhancement%2520networks.%2520By%2520unifying%2520inference%2520and%2520rendering%2520directly%2520in%2520the%2520browser%252C%2520Visionary%2520significantly%2520lowers%2520the%2520barrier%2520to%2520reproduction%252C%2520comparison%252C%2520and%2520deployment%2520of%25203DGS-family%2520methods%252C%2520serving%2520as%2520a%2520unified%2520World%2520Model%2520Carrier%2520for%2520both%2520reconstructive%2520and%2520generative%2520paradigms.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08478v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Visionary%3A%20The%20World%20Model%20Carrier%20Built%20on%20WebGPU-Powered%20Gaussian%20Splatting%20Platform&entry.906535625=Yuning%20Gong%20and%20Yifei%20Liu%20and%20Yifan%20Zhan%20and%20Muyao%20Niu%20and%20Xueying%20Li%20and%20Yuanjun%20Liao%20and%20Jiaming%20Chen%20and%20Yuanyuan%20Gao%20and%20Jiaqi%20Chen%20and%20Minming%20Chen%20and%20Li%20Zhou%20and%20Yuning%20Zhang%20and%20Wei%20Wang%20and%20Xiaoqing%20Hou%20and%20Huaxi%20Huang%20and%20Shixiang%20Tang%20and%20Le%20Ma%20and%20Dingwen%20Zhang%20and%20Xue%20Yang%20and%20Junchi%20Yan%20and%20Yanchi%20Zhang%20and%20Yinqiang%20Zheng%20and%20Xiao%20Sun%20and%20Zhihang%20Zhong&entry.1292438233=Neural%20rendering%2C%20particularly%203D%20Gaussian%20Splatting%20%283DGS%29%2C%20has%20evolved%20rapidly%20and%20become%20a%20key%20component%20for%20building%20world%20models.%20However%2C%20existing%20viewer%20solutions%20remain%20fragmented%2C%20heavy%2C%20or%20constrained%20by%20legacy%20pipelines%2C%20resulting%20in%20high%20deployment%20friction%20and%20limited%20support%20for%20dynamic%20content%20and%20generative%20models.%20In%20this%20work%2C%20we%20present%20Visionary%2C%20an%20open%2C%20web-native%20platform%20for%20real-time%20various%20Gaussian%20Splatting%20and%20meshes%20rendering.%20Built%20on%20an%20efficient%20WebGPU%20renderer%20with%20per-frame%20ONNX%20inference%2C%20Visionary%20enables%20dynamic%20neural%20processing%20while%20maintaining%20a%20lightweight%2C%20%22click-to-run%22%20browser%20experience.%20It%20introduces%20a%20standardized%20Gaussian%20Generator%20contract%2C%20which%20not%20only%20supports%20standard%203DGS%20rendering%20but%20also%20allows%20plug-and-play%20algorithms%20to%20generate%20or%20update%20Gaussians%20each%20frame.%20Such%20inference%20also%20enables%20us%20to%20apply%20feedforward%20generative%20post-processing.%20The%20platform%20further%20offers%20a%20plug%20in%20three.js%20library%20with%20a%20concise%20TypeScript%20API%20for%20seamless%20integration%20into%20existing%20web%20applications.%20Experiments%20show%20that%2C%20under%20identical%203DGS%20assets%2C%20Visionary%20achieves%20superior%20rendering%20efficiency%20compared%20to%20current%20Web%20viewers%20due%20to%20GPU-based%20primitive%20sorting.%20It%20already%20supports%20multiple%20variants%2C%20including%20MLP-based%203DGS%2C%204DGS%2C%20neural%20avatars%2C%20and%20style%20transformation%20or%20enhancement%20networks.%20By%20unifying%20inference%20and%20rendering%20directly%20in%20the%20browser%2C%20Visionary%20significantly%20lowers%20the%20barrier%20to%20reproduction%2C%20comparison%2C%20and%20deployment%20of%203DGS-family%20methods%2C%20serving%20as%20a%20unified%20World%20Model%20Carrier%20for%20both%20reconstructive%20and%20generative%20paradigms.&entry.1838667208=http%3A//arxiv.org/abs/2512.08478v1&entry.124074799=Read"},
{"title": "RGS-DR: Deferred Reflections and Residual Shading in 2D Gaussian Splatting", "author": "Georgios Kouros and Minye Wu and Tinne Tuytelaars", "abstract": "In this work, we address specular appearance in inverse rendering using 2D Gaussian splatting with deferred shading and argue for a refinement stage to improve specular detail, thereby bridging the gap with reconstruction-only methods. Our pipeline estimates editable material properties and environment illumination while employing a directional residual pass that captures leftover view-dependent effects for further refining novel view synthesis. In contrast to per-Gaussian shading with shortest-axis normals and normal residuals, which tends to result in more noisy geometry and specular appearance, a pixel-deferred surfel formulation with specular residuals yields sharper highlights, cleaner materials, and improved editability. We evaluate our approach on rendering and reconstruction quality on three popular datasets featuring glossy objects, and also demonstrate high-quality relighting and material editing.", "link": "http://arxiv.org/abs/2504.18468v6", "date": "2025-12-09", "relevancy": 3.1163, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6629}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.63}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5769}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RGS-DR%3A%20Deferred%20Reflections%20and%20Residual%20Shading%20in%202D%20Gaussian%20Splatting&body=Title%3A%20RGS-DR%3A%20Deferred%20Reflections%20and%20Residual%20Shading%20in%202D%20Gaussian%20Splatting%0AAuthor%3A%20Georgios%20Kouros%20and%20Minye%20Wu%20and%20Tinne%20Tuytelaars%0AAbstract%3A%20In%20this%20work%2C%20we%20address%20specular%20appearance%20in%20inverse%20rendering%20using%202D%20Gaussian%20splatting%20with%20deferred%20shading%20and%20argue%20for%20a%20refinement%20stage%20to%20improve%20specular%20detail%2C%20thereby%20bridging%20the%20gap%20with%20reconstruction-only%20methods.%20Our%20pipeline%20estimates%20editable%20material%20properties%20and%20environment%20illumination%20while%20employing%20a%20directional%20residual%20pass%20that%20captures%20leftover%20view-dependent%20effects%20for%20further%20refining%20novel%20view%20synthesis.%20In%20contrast%20to%20per-Gaussian%20shading%20with%20shortest-axis%20normals%20and%20normal%20residuals%2C%20which%20tends%20to%20result%20in%20more%20noisy%20geometry%20and%20specular%20appearance%2C%20a%20pixel-deferred%20surfel%20formulation%20with%20specular%20residuals%20yields%20sharper%20highlights%2C%20cleaner%20materials%2C%20and%20improved%20editability.%20We%20evaluate%20our%20approach%20on%20rendering%20and%20reconstruction%20quality%20on%20three%20popular%20datasets%20featuring%20glossy%20objects%2C%20and%20also%20demonstrate%20high-quality%20relighting%20and%20material%20editing.%0ALink%3A%20http%3A//arxiv.org/abs/2504.18468v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRGS-DR%253A%2520Deferred%2520Reflections%2520and%2520Residual%2520Shading%2520in%25202D%2520Gaussian%2520Splatting%26entry.906535625%3DGeorgios%2520Kouros%2520and%2520Minye%2520Wu%2520and%2520Tinne%2520Tuytelaars%26entry.1292438233%3DIn%2520this%2520work%252C%2520we%2520address%2520specular%2520appearance%2520in%2520inverse%2520rendering%2520using%25202D%2520Gaussian%2520splatting%2520with%2520deferred%2520shading%2520and%2520argue%2520for%2520a%2520refinement%2520stage%2520to%2520improve%2520specular%2520detail%252C%2520thereby%2520bridging%2520the%2520gap%2520with%2520reconstruction-only%2520methods.%2520Our%2520pipeline%2520estimates%2520editable%2520material%2520properties%2520and%2520environment%2520illumination%2520while%2520employing%2520a%2520directional%2520residual%2520pass%2520that%2520captures%2520leftover%2520view-dependent%2520effects%2520for%2520further%2520refining%2520novel%2520view%2520synthesis.%2520In%2520contrast%2520to%2520per-Gaussian%2520shading%2520with%2520shortest-axis%2520normals%2520and%2520normal%2520residuals%252C%2520which%2520tends%2520to%2520result%2520in%2520more%2520noisy%2520geometry%2520and%2520specular%2520appearance%252C%2520a%2520pixel-deferred%2520surfel%2520formulation%2520with%2520specular%2520residuals%2520yields%2520sharper%2520highlights%252C%2520cleaner%2520materials%252C%2520and%2520improved%2520editability.%2520We%2520evaluate%2520our%2520approach%2520on%2520rendering%2520and%2520reconstruction%2520quality%2520on%2520three%2520popular%2520datasets%2520featuring%2520glossy%2520objects%252C%2520and%2520also%2520demonstrate%2520high-quality%2520relighting%2520and%2520material%2520editing.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2504.18468v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RGS-DR%3A%20Deferred%20Reflections%20and%20Residual%20Shading%20in%202D%20Gaussian%20Splatting&entry.906535625=Georgios%20Kouros%20and%20Minye%20Wu%20and%20Tinne%20Tuytelaars&entry.1292438233=In%20this%20work%2C%20we%20address%20specular%20appearance%20in%20inverse%20rendering%20using%202D%20Gaussian%20splatting%20with%20deferred%20shading%20and%20argue%20for%20a%20refinement%20stage%20to%20improve%20specular%20detail%2C%20thereby%20bridging%20the%20gap%20with%20reconstruction-only%20methods.%20Our%20pipeline%20estimates%20editable%20material%20properties%20and%20environment%20illumination%20while%20employing%20a%20directional%20residual%20pass%20that%20captures%20leftover%20view-dependent%20effects%20for%20further%20refining%20novel%20view%20synthesis.%20In%20contrast%20to%20per-Gaussian%20shading%20with%20shortest-axis%20normals%20and%20normal%20residuals%2C%20which%20tends%20to%20result%20in%20more%20noisy%20geometry%20and%20specular%20appearance%2C%20a%20pixel-deferred%20surfel%20formulation%20with%20specular%20residuals%20yields%20sharper%20highlights%2C%20cleaner%20materials%2C%20and%20improved%20editability.%20We%20evaluate%20our%20approach%20on%20rendering%20and%20reconstruction%20quality%20on%20three%20popular%20datasets%20featuring%20glossy%20objects%2C%20and%20also%20demonstrate%20high-quality%20relighting%20and%20material%20editing.&entry.1838667208=http%3A//arxiv.org/abs/2504.18468v6&entry.124074799=Read"},
{"title": "Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning", "author": "Huilin Xu and Zhuoyang Liu and Yixiang Luomei and Feng Xu", "abstract": "Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.", "link": "http://arxiv.org/abs/2512.08639v1", "date": "2025-12-09", "relevancy": 3.0858, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6193}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6161}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6161}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Aerial%20Vision-Language%20Navigation%20with%20a%20Unified%20Framework%20for%20Spatial%2C%20Temporal%20and%20Embodied%20Reasoning&body=Title%3A%20Aerial%20Vision-Language%20Navigation%20with%20a%20Unified%20Framework%20for%20Spatial%2C%20Temporal%20and%20Embodied%20Reasoning%0AAuthor%3A%20Huilin%20Xu%20and%20Zhuoyang%20Liu%20and%20Yixiang%20Luomei%20and%20Feng%20Xu%0AAbstract%3A%20Aerial%20Vision-and-Language%20Navigation%20%28VLN%29%20aims%20to%20enable%20unmanned%20aerial%20vehicles%20%28UAVs%29%20to%20interpret%20natural%20language%20instructions%20and%20navigate%20complex%20urban%20environments%20using%20onboard%20visual%20observation.%20This%20task%20holds%20promise%20for%20real-world%20applications%20such%20as%20low-altitude%20inspection%2C%20search-and-rescue%2C%20and%20autonomous%20aerial%20delivery.%20Existing%20methods%20often%20rely%20on%20panoramic%20images%2C%20depth%20inputs%2C%20or%20odometry%20to%20support%20spatial%20reasoning%20and%20action%20planning.%20These%20requirements%20increase%20system%20cost%20and%20integration%20complexity%2C%20thus%20hindering%20practical%20deployment%20for%20lightweight%20UAVs.%20We%20present%20a%20unified%20aerial%20VLN%20framework%20that%20operates%20solely%20on%20egocentric%20monocular%20RGB%20observations%20and%20natural%20language%20instructions.%20The%20model%20formulates%20navigation%20as%20a%20next-token%20prediction%20problem%2C%20jointly%20optimizing%20spatial%20perception%2C%20trajectory%20reasoning%2C%20and%20action%20prediction%20through%20prompt-guided%20multi-task%20learning.%20Moreover%2C%20we%20propose%20a%20keyframe%20selection%20strategy%20to%20reduce%20visual%20redundancy%20by%20retaining%20semantically%20informative%20frames%2C%20along%20with%20an%20action%20merging%20and%20label%20reweighting%20mechanism%20that%20mitigates%20long-tailed%20supervision%20imbalance%20and%20facilitates%20stable%20multi-task%20co-training.%20Extensive%20experiments%20on%20the%20Aerial%20VLN%20benchmark%20validate%20the%20effectiveness%20of%20our%20method.%20Under%20the%20challenging%20monocular%20RGB-only%20setting%2C%20our%20model%20achieves%20strong%20results%20across%20both%20seen%20and%20unseen%20environments.%20It%20significantly%20outperforms%20existing%20RGB-only%20baselines%20and%20narrows%20the%20performance%20gap%20with%20state-of-the-art%20panoramic%20RGB-D%20counterparts.%20Comprehensive%20ablation%20studies%20further%20demonstrate%20the%20contribution%20of%20our%20task%20design%20and%20architectural%20choices.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08639v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAerial%2520Vision-Language%2520Navigation%2520with%2520a%2520Unified%2520Framework%2520for%2520Spatial%252C%2520Temporal%2520and%2520Embodied%2520Reasoning%26entry.906535625%3DHuilin%2520Xu%2520and%2520Zhuoyang%2520Liu%2520and%2520Yixiang%2520Luomei%2520and%2520Feng%2520Xu%26entry.1292438233%3DAerial%2520Vision-and-Language%2520Navigation%2520%2528VLN%2529%2520aims%2520to%2520enable%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%2520to%2520interpret%2520natural%2520language%2520instructions%2520and%2520navigate%2520complex%2520urban%2520environments%2520using%2520onboard%2520visual%2520observation.%2520This%2520task%2520holds%2520promise%2520for%2520real-world%2520applications%2520such%2520as%2520low-altitude%2520inspection%252C%2520search-and-rescue%252C%2520and%2520autonomous%2520aerial%2520delivery.%2520Existing%2520methods%2520often%2520rely%2520on%2520panoramic%2520images%252C%2520depth%2520inputs%252C%2520or%2520odometry%2520to%2520support%2520spatial%2520reasoning%2520and%2520action%2520planning.%2520These%2520requirements%2520increase%2520system%2520cost%2520and%2520integration%2520complexity%252C%2520thus%2520hindering%2520practical%2520deployment%2520for%2520lightweight%2520UAVs.%2520We%2520present%2520a%2520unified%2520aerial%2520VLN%2520framework%2520that%2520operates%2520solely%2520on%2520egocentric%2520monocular%2520RGB%2520observations%2520and%2520natural%2520language%2520instructions.%2520The%2520model%2520formulates%2520navigation%2520as%2520a%2520next-token%2520prediction%2520problem%252C%2520jointly%2520optimizing%2520spatial%2520perception%252C%2520trajectory%2520reasoning%252C%2520and%2520action%2520prediction%2520through%2520prompt-guided%2520multi-task%2520learning.%2520Moreover%252C%2520we%2520propose%2520a%2520keyframe%2520selection%2520strategy%2520to%2520reduce%2520visual%2520redundancy%2520by%2520retaining%2520semantically%2520informative%2520frames%252C%2520along%2520with%2520an%2520action%2520merging%2520and%2520label%2520reweighting%2520mechanism%2520that%2520mitigates%2520long-tailed%2520supervision%2520imbalance%2520and%2520facilitates%2520stable%2520multi-task%2520co-training.%2520Extensive%2520experiments%2520on%2520the%2520Aerial%2520VLN%2520benchmark%2520validate%2520the%2520effectiveness%2520of%2520our%2520method.%2520Under%2520the%2520challenging%2520monocular%2520RGB-only%2520setting%252C%2520our%2520model%2520achieves%2520strong%2520results%2520across%2520both%2520seen%2520and%2520unseen%2520environments.%2520It%2520significantly%2520outperforms%2520existing%2520RGB-only%2520baselines%2520and%2520narrows%2520the%2520performance%2520gap%2520with%2520state-of-the-art%2520panoramic%2520RGB-D%2520counterparts.%2520Comprehensive%2520ablation%2520studies%2520further%2520demonstrate%2520the%2520contribution%2520of%2520our%2520task%2520design%2520and%2520architectural%2520choices.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08639v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Aerial%20Vision-Language%20Navigation%20with%20a%20Unified%20Framework%20for%20Spatial%2C%20Temporal%20and%20Embodied%20Reasoning&entry.906535625=Huilin%20Xu%20and%20Zhuoyang%20Liu%20and%20Yixiang%20Luomei%20and%20Feng%20Xu&entry.1292438233=Aerial%20Vision-and-Language%20Navigation%20%28VLN%29%20aims%20to%20enable%20unmanned%20aerial%20vehicles%20%28UAVs%29%20to%20interpret%20natural%20language%20instructions%20and%20navigate%20complex%20urban%20environments%20using%20onboard%20visual%20observation.%20This%20task%20holds%20promise%20for%20real-world%20applications%20such%20as%20low-altitude%20inspection%2C%20search-and-rescue%2C%20and%20autonomous%20aerial%20delivery.%20Existing%20methods%20often%20rely%20on%20panoramic%20images%2C%20depth%20inputs%2C%20or%20odometry%20to%20support%20spatial%20reasoning%20and%20action%20planning.%20These%20requirements%20increase%20system%20cost%20and%20integration%20complexity%2C%20thus%20hindering%20practical%20deployment%20for%20lightweight%20UAVs.%20We%20present%20a%20unified%20aerial%20VLN%20framework%20that%20operates%20solely%20on%20egocentric%20monocular%20RGB%20observations%20and%20natural%20language%20instructions.%20The%20model%20formulates%20navigation%20as%20a%20next-token%20prediction%20problem%2C%20jointly%20optimizing%20spatial%20perception%2C%20trajectory%20reasoning%2C%20and%20action%20prediction%20through%20prompt-guided%20multi-task%20learning.%20Moreover%2C%20we%20propose%20a%20keyframe%20selection%20strategy%20to%20reduce%20visual%20redundancy%20by%20retaining%20semantically%20informative%20frames%2C%20along%20with%20an%20action%20merging%20and%20label%20reweighting%20mechanism%20that%20mitigates%20long-tailed%20supervision%20imbalance%20and%20facilitates%20stable%20multi-task%20co-training.%20Extensive%20experiments%20on%20the%20Aerial%20VLN%20benchmark%20validate%20the%20effectiveness%20of%20our%20method.%20Under%20the%20challenging%20monocular%20RGB-only%20setting%2C%20our%20model%20achieves%20strong%20results%20across%20both%20seen%20and%20unseen%20environments.%20It%20significantly%20outperforms%20existing%20RGB-only%20baselines%20and%20narrows%20the%20performance%20gap%20with%20state-of-the-art%20panoramic%20RGB-D%20counterparts.%20Comprehensive%20ablation%20studies%20further%20demonstrate%20the%20contribution%20of%20our%20task%20design%20and%20architectural%20choices.&entry.1838667208=http%3A//arxiv.org/abs/2512.08639v1&entry.124074799=Read"},
{"title": "SDT-6D: Fully Sparse Depth-Transformer for Staged End-to-End 6D Pose Estimation in Industrial Multi-View Bin Picking", "author": "Nico Leuze and Maximilian Hoh and Samed Do\u011fan and Nicolas R. -Pe\u00f1a and Alfred Schoettl", "abstract": "Accurately recovering 6D poses in densely packed industrial bin-picking environments remain a serious challenge, owing to occlusions, reflections, and textureless parts. We introduce a holistic depth-only 6D pose estimation approach that fuses multi-view depth maps into either a fine-grained 3D point cloud in its vanilla version, or a sparse Truncated Signed Distance Field (TSDF). At the core of our framework lies a staged heatmap mechanism that yields scene-adaptive attention priors across different resolutions, steering computation toward foreground regions, thus keeping memory requirements at high resolutions feasible. Along, we propose a density-aware sparse transformer block that dynamically attends to (self-) occlusions and the non-uniform distribution of 3D data. While sparse 3D approaches has proven effective for long-range perception, its potential in close-range robotic applications remains underexplored. Our framework operates fully sparse, enabling high-resolution volumetric representations to capture fine geometric details crucial for accurate pose estimation in clutter. Our method processes the entire scene integrally, predicting the 6D pose via a novel per-voxel voting strategy, allowing simultaneous pose predictions for an arbitrary number of target objects. We validate our method on the recently published IPD and MV-YCB multi-view datasets, demonstrating competitive performance in heavily cluttered industrial and household bin picking scenarios.", "link": "http://arxiv.org/abs/2512.08430v1", "date": "2025-12-09", "relevancy": 3.0572, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6208}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6131}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6003}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SDT-6D%3A%20Fully%20Sparse%20Depth-Transformer%20for%20Staged%20End-to-End%206D%20Pose%20Estimation%20in%20Industrial%20Multi-View%20Bin%20Picking&body=Title%3A%20SDT-6D%3A%20Fully%20Sparse%20Depth-Transformer%20for%20Staged%20End-to-End%206D%20Pose%20Estimation%20in%20Industrial%20Multi-View%20Bin%20Picking%0AAuthor%3A%20Nico%20Leuze%20and%20Maximilian%20Hoh%20and%20Samed%20Do%C4%9Fan%20and%20Nicolas%20R.%20-Pe%C3%B1a%20and%20Alfred%20Schoettl%0AAbstract%3A%20Accurately%20recovering%206D%20poses%20in%20densely%20packed%20industrial%20bin-picking%20environments%20remain%20a%20serious%20challenge%2C%20owing%20to%20occlusions%2C%20reflections%2C%20and%20textureless%20parts.%20We%20introduce%20a%20holistic%20depth-only%206D%20pose%20estimation%20approach%20that%20fuses%20multi-view%20depth%20maps%20into%20either%20a%20fine-grained%203D%20point%20cloud%20in%20its%20vanilla%20version%2C%20or%20a%20sparse%20Truncated%20Signed%20Distance%20Field%20%28TSDF%29.%20At%20the%20core%20of%20our%20framework%20lies%20a%20staged%20heatmap%20mechanism%20that%20yields%20scene-adaptive%20attention%20priors%20across%20different%20resolutions%2C%20steering%20computation%20toward%20foreground%20regions%2C%20thus%20keeping%20memory%20requirements%20at%20high%20resolutions%20feasible.%20Along%2C%20we%20propose%20a%20density-aware%20sparse%20transformer%20block%20that%20dynamically%20attends%20to%20%28self-%29%20occlusions%20and%20the%20non-uniform%20distribution%20of%203D%20data.%20While%20sparse%203D%20approaches%20has%20proven%20effective%20for%20long-range%20perception%2C%20its%20potential%20in%20close-range%20robotic%20applications%20remains%20underexplored.%20Our%20framework%20operates%20fully%20sparse%2C%20enabling%20high-resolution%20volumetric%20representations%20to%20capture%20fine%20geometric%20details%20crucial%20for%20accurate%20pose%20estimation%20in%20clutter.%20Our%20method%20processes%20the%20entire%20scene%20integrally%2C%20predicting%20the%206D%20pose%20via%20a%20novel%20per-voxel%20voting%20strategy%2C%20allowing%20simultaneous%20pose%20predictions%20for%20an%20arbitrary%20number%20of%20target%20objects.%20We%20validate%20our%20method%20on%20the%20recently%20published%20IPD%20and%20MV-YCB%20multi-view%20datasets%2C%20demonstrating%20competitive%20performance%20in%20heavily%20cluttered%20industrial%20and%20household%20bin%20picking%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSDT-6D%253A%2520Fully%2520Sparse%2520Depth-Transformer%2520for%2520Staged%2520End-to-End%25206D%2520Pose%2520Estimation%2520in%2520Industrial%2520Multi-View%2520Bin%2520Picking%26entry.906535625%3DNico%2520Leuze%2520and%2520Maximilian%2520Hoh%2520and%2520Samed%2520Do%25C4%259Fan%2520and%2520Nicolas%2520R.%2520-Pe%25C3%25B1a%2520and%2520Alfred%2520Schoettl%26entry.1292438233%3DAccurately%2520recovering%25206D%2520poses%2520in%2520densely%2520packed%2520industrial%2520bin-picking%2520environments%2520remain%2520a%2520serious%2520challenge%252C%2520owing%2520to%2520occlusions%252C%2520reflections%252C%2520and%2520textureless%2520parts.%2520We%2520introduce%2520a%2520holistic%2520depth-only%25206D%2520pose%2520estimation%2520approach%2520that%2520fuses%2520multi-view%2520depth%2520maps%2520into%2520either%2520a%2520fine-grained%25203D%2520point%2520cloud%2520in%2520its%2520vanilla%2520version%252C%2520or%2520a%2520sparse%2520Truncated%2520Signed%2520Distance%2520Field%2520%2528TSDF%2529.%2520At%2520the%2520core%2520of%2520our%2520framework%2520lies%2520a%2520staged%2520heatmap%2520mechanism%2520that%2520yields%2520scene-adaptive%2520attention%2520priors%2520across%2520different%2520resolutions%252C%2520steering%2520computation%2520toward%2520foreground%2520regions%252C%2520thus%2520keeping%2520memory%2520requirements%2520at%2520high%2520resolutions%2520feasible.%2520Along%252C%2520we%2520propose%2520a%2520density-aware%2520sparse%2520transformer%2520block%2520that%2520dynamically%2520attends%2520to%2520%2528self-%2529%2520occlusions%2520and%2520the%2520non-uniform%2520distribution%2520of%25203D%2520data.%2520While%2520sparse%25203D%2520approaches%2520has%2520proven%2520effective%2520for%2520long-range%2520perception%252C%2520its%2520potential%2520in%2520close-range%2520robotic%2520applications%2520remains%2520underexplored.%2520Our%2520framework%2520operates%2520fully%2520sparse%252C%2520enabling%2520high-resolution%2520volumetric%2520representations%2520to%2520capture%2520fine%2520geometric%2520details%2520crucial%2520for%2520accurate%2520pose%2520estimation%2520in%2520clutter.%2520Our%2520method%2520processes%2520the%2520entire%2520scene%2520integrally%252C%2520predicting%2520the%25206D%2520pose%2520via%2520a%2520novel%2520per-voxel%2520voting%2520strategy%252C%2520allowing%2520simultaneous%2520pose%2520predictions%2520for%2520an%2520arbitrary%2520number%2520of%2520target%2520objects.%2520We%2520validate%2520our%2520method%2520on%2520the%2520recently%2520published%2520IPD%2520and%2520MV-YCB%2520multi-view%2520datasets%252C%2520demonstrating%2520competitive%2520performance%2520in%2520heavily%2520cluttered%2520industrial%2520and%2520household%2520bin%2520picking%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SDT-6D%3A%20Fully%20Sparse%20Depth-Transformer%20for%20Staged%20End-to-End%206D%20Pose%20Estimation%20in%20Industrial%20Multi-View%20Bin%20Picking&entry.906535625=Nico%20Leuze%20and%20Maximilian%20Hoh%20and%20Samed%20Do%C4%9Fan%20and%20Nicolas%20R.%20-Pe%C3%B1a%20and%20Alfred%20Schoettl&entry.1292438233=Accurately%20recovering%206D%20poses%20in%20densely%20packed%20industrial%20bin-picking%20environments%20remain%20a%20serious%20challenge%2C%20owing%20to%20occlusions%2C%20reflections%2C%20and%20textureless%20parts.%20We%20introduce%20a%20holistic%20depth-only%206D%20pose%20estimation%20approach%20that%20fuses%20multi-view%20depth%20maps%20into%20either%20a%20fine-grained%203D%20point%20cloud%20in%20its%20vanilla%20version%2C%20or%20a%20sparse%20Truncated%20Signed%20Distance%20Field%20%28TSDF%29.%20At%20the%20core%20of%20our%20framework%20lies%20a%20staged%20heatmap%20mechanism%20that%20yields%20scene-adaptive%20attention%20priors%20across%20different%20resolutions%2C%20steering%20computation%20toward%20foreground%20regions%2C%20thus%20keeping%20memory%20requirements%20at%20high%20resolutions%20feasible.%20Along%2C%20we%20propose%20a%20density-aware%20sparse%20transformer%20block%20that%20dynamically%20attends%20to%20%28self-%29%20occlusions%20and%20the%20non-uniform%20distribution%20of%203D%20data.%20While%20sparse%203D%20approaches%20has%20proven%20effective%20for%20long-range%20perception%2C%20its%20potential%20in%20close-range%20robotic%20applications%20remains%20underexplored.%20Our%20framework%20operates%20fully%20sparse%2C%20enabling%20high-resolution%20volumetric%20representations%20to%20capture%20fine%20geometric%20details%20crucial%20for%20accurate%20pose%20estimation%20in%20clutter.%20Our%20method%20processes%20the%20entire%20scene%20integrally%2C%20predicting%20the%206D%20pose%20via%20a%20novel%20per-voxel%20voting%20strategy%2C%20allowing%20simultaneous%20pose%20predictions%20for%20an%20arbitrary%20number%20of%20target%20objects.%20We%20validate%20our%20method%20on%20the%20recently%20published%20IPD%20and%20MV-YCB%20multi-view%20datasets%2C%20demonstrating%20competitive%20performance%20in%20heavily%20cluttered%20industrial%20and%20household%20bin%20picking%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.08430v1&entry.124074799=Read"},
{"title": "Dual-Branch Center-Surrounding Contrast: Rethinking Contrastive Learning for 3D Point Clouds", "author": "Shaofeng Zhang and Xuanqi Chen and Xiangdong Zhang and Sitong Wu and Junchi Yan", "abstract": "Most existing self-supervised learning (SSL) approaches for 3D point clouds are dominated by generative methods based on Masked Autoencoders (MAE). However, these generative methods have been proven to struggle to capture high-level discriminative features effectively, leading to poor performance on linear probing and other downstream tasks. In contrast, contrastive methods excel in discriminative feature representation and generalization ability on image data. Despite this, contrastive learning (CL) in 3D data remains scarce. Besides, simply applying CL methods designed for 2D data to 3D fails to effectively learn 3D local details. To address these challenges, we propose a novel Dual-Branch \\textbf{C}enter-\\textbf{S}urrounding \\textbf{Con}trast (CSCon) framework. Specifically, we apply masking to the center and surrounding parts separately, constructing dual-branch inputs with center-biased and surrounding-biased representations to better capture rich geometric information. Meanwhile, we introduce a patch-level contrastive loss to further enhance both high-level information and local sensitivity. Under the FULL and ALL protocols, CSCon achieves performance comparable to generative methods; under the MLP-LINEAR, MLP-3, and ONLY-NEW protocols, our method attains state-of-the-art results, even surpassing cross-modal approaches. In particular, under the MLP-LINEAR protocol, our method outperforms the baseline (Point-MAE) by \\textbf{7.9\\%}, \\textbf{6.7\\%}, and \\textbf{10.3\\%} on the three variants of ScanObjectNN, respectively. The code will be made publicly available.", "link": "http://arxiv.org/abs/2512.08673v1", "date": "2025-12-09", "relevancy": 3.0543, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6336}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.6016}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5974}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Dual-Branch%20Center-Surrounding%20Contrast%3A%20Rethinking%20Contrastive%20Learning%20for%203D%20Point%20Clouds&body=Title%3A%20Dual-Branch%20Center-Surrounding%20Contrast%3A%20Rethinking%20Contrastive%20Learning%20for%203D%20Point%20Clouds%0AAuthor%3A%20Shaofeng%20Zhang%20and%20Xuanqi%20Chen%20and%20Xiangdong%20Zhang%20and%20Sitong%20Wu%20and%20Junchi%20Yan%0AAbstract%3A%20Most%20existing%20self-supervised%20learning%20%28SSL%29%20approaches%20for%203D%20point%20clouds%20are%20dominated%20by%20generative%20methods%20based%20on%20Masked%20Autoencoders%20%28MAE%29.%20However%2C%20these%20generative%20methods%20have%20been%20proven%20to%20struggle%20to%20capture%20high-level%20discriminative%20features%20effectively%2C%20leading%20to%20poor%20performance%20on%20linear%20probing%20and%20other%20downstream%20tasks.%20In%20contrast%2C%20contrastive%20methods%20excel%20in%20discriminative%20feature%20representation%20and%20generalization%20ability%20on%20image%20data.%20Despite%20this%2C%20contrastive%20learning%20%28CL%29%20in%203D%20data%20remains%20scarce.%20Besides%2C%20simply%20applying%20CL%20methods%20designed%20for%202D%20data%20to%203D%20fails%20to%20effectively%20learn%203D%20local%20details.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20Dual-Branch%20%5Ctextbf%7BC%7Denter-%5Ctextbf%7BS%7Durrounding%20%5Ctextbf%7BCon%7Dtrast%20%28CSCon%29%20framework.%20Specifically%2C%20we%20apply%20masking%20to%20the%20center%20and%20surrounding%20parts%20separately%2C%20constructing%20dual-branch%20inputs%20with%20center-biased%20and%20surrounding-biased%20representations%20to%20better%20capture%20rich%20geometric%20information.%20Meanwhile%2C%20we%20introduce%20a%20patch-level%20contrastive%20loss%20to%20further%20enhance%20both%20high-level%20information%20and%20local%20sensitivity.%20Under%20the%20FULL%20and%20ALL%20protocols%2C%20CSCon%20achieves%20performance%20comparable%20to%20generative%20methods%3B%20under%20the%20MLP-LINEAR%2C%20MLP-3%2C%20and%20ONLY-NEW%20protocols%2C%20our%20method%20attains%20state-of-the-art%20results%2C%20even%20surpassing%20cross-modal%20approaches.%20In%20particular%2C%20under%20the%20MLP-LINEAR%20protocol%2C%20our%20method%20outperforms%20the%20baseline%20%28Point-MAE%29%20by%20%5Ctextbf%7B7.9%5C%25%7D%2C%20%5Ctextbf%7B6.7%5C%25%7D%2C%20and%20%5Ctextbf%7B10.3%5C%25%7D%20on%20the%20three%20variants%20of%20ScanObjectNN%2C%20respectively.%20The%20code%20will%20be%20made%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08673v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDual-Branch%2520Center-Surrounding%2520Contrast%253A%2520Rethinking%2520Contrastive%2520Learning%2520for%25203D%2520Point%2520Clouds%26entry.906535625%3DShaofeng%2520Zhang%2520and%2520Xuanqi%2520Chen%2520and%2520Xiangdong%2520Zhang%2520and%2520Sitong%2520Wu%2520and%2520Junchi%2520Yan%26entry.1292438233%3DMost%2520existing%2520self-supervised%2520learning%2520%2528SSL%2529%2520approaches%2520for%25203D%2520point%2520clouds%2520are%2520dominated%2520by%2520generative%2520methods%2520based%2520on%2520Masked%2520Autoencoders%2520%2528MAE%2529.%2520However%252C%2520these%2520generative%2520methods%2520have%2520been%2520proven%2520to%2520struggle%2520to%2520capture%2520high-level%2520discriminative%2520features%2520effectively%252C%2520leading%2520to%2520poor%2520performance%2520on%2520linear%2520probing%2520and%2520other%2520downstream%2520tasks.%2520In%2520contrast%252C%2520contrastive%2520methods%2520excel%2520in%2520discriminative%2520feature%2520representation%2520and%2520generalization%2520ability%2520on%2520image%2520data.%2520Despite%2520this%252C%2520contrastive%2520learning%2520%2528CL%2529%2520in%25203D%2520data%2520remains%2520scarce.%2520Besides%252C%2520simply%2520applying%2520CL%2520methods%2520designed%2520for%25202D%2520data%2520to%25203D%2520fails%2520to%2520effectively%2520learn%25203D%2520local%2520details.%2520To%2520address%2520these%2520challenges%252C%2520we%2520propose%2520a%2520novel%2520Dual-Branch%2520%255Ctextbf%257BC%257Denter-%255Ctextbf%257BS%257Durrounding%2520%255Ctextbf%257BCon%257Dtrast%2520%2528CSCon%2529%2520framework.%2520Specifically%252C%2520we%2520apply%2520masking%2520to%2520the%2520center%2520and%2520surrounding%2520parts%2520separately%252C%2520constructing%2520dual-branch%2520inputs%2520with%2520center-biased%2520and%2520surrounding-biased%2520representations%2520to%2520better%2520capture%2520rich%2520geometric%2520information.%2520Meanwhile%252C%2520we%2520introduce%2520a%2520patch-level%2520contrastive%2520loss%2520to%2520further%2520enhance%2520both%2520high-level%2520information%2520and%2520local%2520sensitivity.%2520Under%2520the%2520FULL%2520and%2520ALL%2520protocols%252C%2520CSCon%2520achieves%2520performance%2520comparable%2520to%2520generative%2520methods%253B%2520under%2520the%2520MLP-LINEAR%252C%2520MLP-3%252C%2520and%2520ONLY-NEW%2520protocols%252C%2520our%2520method%2520attains%2520state-of-the-art%2520results%252C%2520even%2520surpassing%2520cross-modal%2520approaches.%2520In%2520particular%252C%2520under%2520the%2520MLP-LINEAR%2520protocol%252C%2520our%2520method%2520outperforms%2520the%2520baseline%2520%2528Point-MAE%2529%2520by%2520%255Ctextbf%257B7.9%255C%2525%257D%252C%2520%255Ctextbf%257B6.7%255C%2525%257D%252C%2520and%2520%255Ctextbf%257B10.3%255C%2525%257D%2520on%2520the%2520three%2520variants%2520of%2520ScanObjectNN%252C%2520respectively.%2520The%2520code%2520will%2520be%2520made%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08673v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Branch%20Center-Surrounding%20Contrast%3A%20Rethinking%20Contrastive%20Learning%20for%203D%20Point%20Clouds&entry.906535625=Shaofeng%20Zhang%20and%20Xuanqi%20Chen%20and%20Xiangdong%20Zhang%20and%20Sitong%20Wu%20and%20Junchi%20Yan&entry.1292438233=Most%20existing%20self-supervised%20learning%20%28SSL%29%20approaches%20for%203D%20point%20clouds%20are%20dominated%20by%20generative%20methods%20based%20on%20Masked%20Autoencoders%20%28MAE%29.%20However%2C%20these%20generative%20methods%20have%20been%20proven%20to%20struggle%20to%20capture%20high-level%20discriminative%20features%20effectively%2C%20leading%20to%20poor%20performance%20on%20linear%20probing%20and%20other%20downstream%20tasks.%20In%20contrast%2C%20contrastive%20methods%20excel%20in%20discriminative%20feature%20representation%20and%20generalization%20ability%20on%20image%20data.%20Despite%20this%2C%20contrastive%20learning%20%28CL%29%20in%203D%20data%20remains%20scarce.%20Besides%2C%20simply%20applying%20CL%20methods%20designed%20for%202D%20data%20to%203D%20fails%20to%20effectively%20learn%203D%20local%20details.%20To%20address%20these%20challenges%2C%20we%20propose%20a%20novel%20Dual-Branch%20%5Ctextbf%7BC%7Denter-%5Ctextbf%7BS%7Durrounding%20%5Ctextbf%7BCon%7Dtrast%20%28CSCon%29%20framework.%20Specifically%2C%20we%20apply%20masking%20to%20the%20center%20and%20surrounding%20parts%20separately%2C%20constructing%20dual-branch%20inputs%20with%20center-biased%20and%20surrounding-biased%20representations%20to%20better%20capture%20rich%20geometric%20information.%20Meanwhile%2C%20we%20introduce%20a%20patch-level%20contrastive%20loss%20to%20further%20enhance%20both%20high-level%20information%20and%20local%20sensitivity.%20Under%20the%20FULL%20and%20ALL%20protocols%2C%20CSCon%20achieves%20performance%20comparable%20to%20generative%20methods%3B%20under%20the%20MLP-LINEAR%2C%20MLP-3%2C%20and%20ONLY-NEW%20protocols%2C%20our%20method%20attains%20state-of-the-art%20results%2C%20even%20surpassing%20cross-modal%20approaches.%20In%20particular%2C%20under%20the%20MLP-LINEAR%20protocol%2C%20our%20method%20outperforms%20the%20baseline%20%28Point-MAE%29%20by%20%5Ctextbf%7B7.9%5C%25%7D%2C%20%5Ctextbf%7B6.7%5C%25%7D%2C%20and%20%5Ctextbf%7B10.3%5C%25%7D%20on%20the%20three%20variants%20of%20ScanObjectNN%2C%20respectively.%20The%20code%20will%20be%20made%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2512.08673v1&entry.124074799=Read"},
{"title": "Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment", "author": "Youming Deng and Songyou Peng and Junyi Zhang and Kathryn Heal and Tiancheng Sun and John Flynn and Steve Marschner and Lucy Chai", "abstract": "Novel View Synthesis (NVS) has traditionally relied on models with explicit 3D inductive biases combined with known camera parameters from Structure-from-Motion (SfM) beforehand. Recent vision foundation models like VGGT take an orthogonal approach -- 3D knowledge is gained implicitly through training data and loss objectives, enabling feed-forward prediction of both camera parameters and 3D representations directly from a set of uncalibrated images. While flexible, VGGT features lack explicit multi-view geometric consistency, and we find that improving such 3D feature consistency benefits both NVS and pose estimation tasks. We introduce Selfi, a self-improving 3D reconstruction pipeline via feature alignment, transforming a VGGT backbone into a high-fidelity 3D reconstruction engine by leveraging its own outputs as pseudo-ground-truth. Specifically, we train a lightweight feature adapter using a reprojection-based consistency loss, which distills VGGT outputs into a new geometrically-aligned feature space that captures spatial proximity in 3D. This enables state-of-the-art performance in both NVS and camera pose estimation, demonstrating that feature alignment is a highly beneficial step for downstream 3D reasoning.", "link": "http://arxiv.org/abs/2512.08930v1", "date": "2025-12-09", "relevancy": 3.0182, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.616}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.6003}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5946}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Selfi%3A%20Self%20Improving%20Reconstruction%20Engine%20via%203D%20Geometric%20Feature%20Alignment&body=Title%3A%20Selfi%3A%20Self%20Improving%20Reconstruction%20Engine%20via%203D%20Geometric%20Feature%20Alignment%0AAuthor%3A%20Youming%20Deng%20and%20Songyou%20Peng%20and%20Junyi%20Zhang%20and%20Kathryn%20Heal%20and%20Tiancheng%20Sun%20and%20John%20Flynn%20and%20Steve%20Marschner%20and%20Lucy%20Chai%0AAbstract%3A%20Novel%20View%20Synthesis%20%28NVS%29%20has%20traditionally%20relied%20on%20models%20with%20explicit%203D%20inductive%20biases%20combined%20with%20known%20camera%20parameters%20from%20Structure-from-Motion%20%28SfM%29%20beforehand.%20Recent%20vision%20foundation%20models%20like%20VGGT%20take%20an%20orthogonal%20approach%20--%203D%20knowledge%20is%20gained%20implicitly%20through%20training%20data%20and%20loss%20objectives%2C%20enabling%20feed-forward%20prediction%20of%20both%20camera%20parameters%20and%203D%20representations%20directly%20from%20a%20set%20of%20uncalibrated%20images.%20While%20flexible%2C%20VGGT%20features%20lack%20explicit%20multi-view%20geometric%20consistency%2C%20and%20we%20find%20that%20improving%20such%203D%20feature%20consistency%20benefits%20both%20NVS%20and%20pose%20estimation%20tasks.%20We%20introduce%20Selfi%2C%20a%20self-improving%203D%20reconstruction%20pipeline%20via%20feature%20alignment%2C%20transforming%20a%20VGGT%20backbone%20into%20a%20high-fidelity%203D%20reconstruction%20engine%20by%20leveraging%20its%20own%20outputs%20as%20pseudo-ground-truth.%20Specifically%2C%20we%20train%20a%20lightweight%20feature%20adapter%20using%20a%20reprojection-based%20consistency%20loss%2C%20which%20distills%20VGGT%20outputs%20into%20a%20new%20geometrically-aligned%20feature%20space%20that%20captures%20spatial%20proximity%20in%203D.%20This%20enables%20state-of-the-art%20performance%20in%20both%20NVS%20and%20camera%20pose%20estimation%2C%20demonstrating%20that%20feature%20alignment%20is%20a%20highly%20beneficial%20step%20for%20downstream%203D%20reasoning.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08930v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelfi%253A%2520Self%2520Improving%2520Reconstruction%2520Engine%2520via%25203D%2520Geometric%2520Feature%2520Alignment%26entry.906535625%3DYouming%2520Deng%2520and%2520Songyou%2520Peng%2520and%2520Junyi%2520Zhang%2520and%2520Kathryn%2520Heal%2520and%2520Tiancheng%2520Sun%2520and%2520John%2520Flynn%2520and%2520Steve%2520Marschner%2520and%2520Lucy%2520Chai%26entry.1292438233%3DNovel%2520View%2520Synthesis%2520%2528NVS%2529%2520has%2520traditionally%2520relied%2520on%2520models%2520with%2520explicit%25203D%2520inductive%2520biases%2520combined%2520with%2520known%2520camera%2520parameters%2520from%2520Structure-from-Motion%2520%2528SfM%2529%2520beforehand.%2520Recent%2520vision%2520foundation%2520models%2520like%2520VGGT%2520take%2520an%2520orthogonal%2520approach%2520--%25203D%2520knowledge%2520is%2520gained%2520implicitly%2520through%2520training%2520data%2520and%2520loss%2520objectives%252C%2520enabling%2520feed-forward%2520prediction%2520of%2520both%2520camera%2520parameters%2520and%25203D%2520representations%2520directly%2520from%2520a%2520set%2520of%2520uncalibrated%2520images.%2520While%2520flexible%252C%2520VGGT%2520features%2520lack%2520explicit%2520multi-view%2520geometric%2520consistency%252C%2520and%2520we%2520find%2520that%2520improving%2520such%25203D%2520feature%2520consistency%2520benefits%2520both%2520NVS%2520and%2520pose%2520estimation%2520tasks.%2520We%2520introduce%2520Selfi%252C%2520a%2520self-improving%25203D%2520reconstruction%2520pipeline%2520via%2520feature%2520alignment%252C%2520transforming%2520a%2520VGGT%2520backbone%2520into%2520a%2520high-fidelity%25203D%2520reconstruction%2520engine%2520by%2520leveraging%2520its%2520own%2520outputs%2520as%2520pseudo-ground-truth.%2520Specifically%252C%2520we%2520train%2520a%2520lightweight%2520feature%2520adapter%2520using%2520a%2520reprojection-based%2520consistency%2520loss%252C%2520which%2520distills%2520VGGT%2520outputs%2520into%2520a%2520new%2520geometrically-aligned%2520feature%2520space%2520that%2520captures%2520spatial%2520proximity%2520in%25203D.%2520This%2520enables%2520state-of-the-art%2520performance%2520in%2520both%2520NVS%2520and%2520camera%2520pose%2520estimation%252C%2520demonstrating%2520that%2520feature%2520alignment%2520is%2520a%2520highly%2520beneficial%2520step%2520for%2520downstream%25203D%2520reasoning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08930v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Selfi%3A%20Self%20Improving%20Reconstruction%20Engine%20via%203D%20Geometric%20Feature%20Alignment&entry.906535625=Youming%20Deng%20and%20Songyou%20Peng%20and%20Junyi%20Zhang%20and%20Kathryn%20Heal%20and%20Tiancheng%20Sun%20and%20John%20Flynn%20and%20Steve%20Marschner%20and%20Lucy%20Chai&entry.1292438233=Novel%20View%20Synthesis%20%28NVS%29%20has%20traditionally%20relied%20on%20models%20with%20explicit%203D%20inductive%20biases%20combined%20with%20known%20camera%20parameters%20from%20Structure-from-Motion%20%28SfM%29%20beforehand.%20Recent%20vision%20foundation%20models%20like%20VGGT%20take%20an%20orthogonal%20approach%20--%203D%20knowledge%20is%20gained%20implicitly%20through%20training%20data%20and%20loss%20objectives%2C%20enabling%20feed-forward%20prediction%20of%20both%20camera%20parameters%20and%203D%20representations%20directly%20from%20a%20set%20of%20uncalibrated%20images.%20While%20flexible%2C%20VGGT%20features%20lack%20explicit%20multi-view%20geometric%20consistency%2C%20and%20we%20find%20that%20improving%20such%203D%20feature%20consistency%20benefits%20both%20NVS%20and%20pose%20estimation%20tasks.%20We%20introduce%20Selfi%2C%20a%20self-improving%203D%20reconstruction%20pipeline%20via%20feature%20alignment%2C%20transforming%20a%20VGGT%20backbone%20into%20a%20high-fidelity%203D%20reconstruction%20engine%20by%20leveraging%20its%20own%20outputs%20as%20pseudo-ground-truth.%20Specifically%2C%20we%20train%20a%20lightweight%20feature%20adapter%20using%20a%20reprojection-based%20consistency%20loss%2C%20which%20distills%20VGGT%20outputs%20into%20a%20new%20geometrically-aligned%20feature%20space%20that%20captures%20spatial%20proximity%20in%203D.%20This%20enables%20state-of-the-art%20performance%20in%20both%20NVS%20and%20camera%20pose%20estimation%2C%20demonstrating%20that%20feature%20alignment%20is%20a%20highly%20beneficial%20step%20for%20downstream%203D%20reasoning.&entry.1838667208=http%3A//arxiv.org/abs/2512.08930v1&entry.124074799=Read"},
{"title": "SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds", "author": "Alexander Dow and Manduhu Manduhu and Matheus Santos and Ben Bartlett and Gerard Dooly and James Riordan", "abstract": "This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.", "link": "http://arxiv.org/abs/2512.08557v1", "date": "2025-12-09", "relevancy": 2.9777, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6275}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5795}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5795}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SSCATeR%3A%20Sparse%20Scatter-Based%20Convolution%20Algorithm%20with%20Temporal%20Data%20Recycling%20for%20Real-Time%203D%20Object%20Detection%20in%20LiDAR%20Point%20Clouds&body=Title%3A%20SSCATeR%3A%20Sparse%20Scatter-Based%20Convolution%20Algorithm%20with%20Temporal%20Data%20Recycling%20for%20Real-Time%203D%20Object%20Detection%20in%20LiDAR%20Point%20Clouds%0AAuthor%3A%20Alexander%20Dow%20and%20Manduhu%20Manduhu%20and%20Matheus%20Santos%20and%20Ben%20Bartlett%20and%20Gerard%20Dooly%20and%20James%20Riordan%0AAbstract%3A%20This%20work%20leverages%20the%20continuous%20sweeping%20motion%20of%20LiDAR%20scanning%20to%20concentrate%20object%20detection%20efforts%20on%20specific%20regions%20that%20receive%20a%20change%20in%20point%20data%20from%20one%20frame%20to%20another.%20We%20achieve%20this%20by%20using%20a%20sliding%20time%20window%20with%20short%20strides%20and%20consider%20the%20temporal%20dimension%20by%20storing%20convolution%20results%20between%20passes.%20This%20allows%20us%20to%20ignore%20unchanged%20regions%2C%20significantly%20reducing%20the%20number%20of%20convolution%20operations%20per%20forward%20pass%20without%20sacrificing%20accuracy.%20This%20data%20reuse%20scheme%20introduces%20extreme%20sparsity%20to%20detection%20data.%20To%20exploit%20this%20sparsity%2C%20we%20extend%20our%20previous%20work%20on%20scatter-based%20convolutions%20to%20allow%20for%20data%20reuse%2C%20and%20as%20such%20propose%20Sparse%20Scatter-Based%20Convolution%20Algorithm%20with%20Temporal%20Data%20Recycling%20%28SSCATeR%29.%20This%20operation%20treats%20incoming%20LiDAR%20data%20as%20a%20continuous%20stream%20and%20acts%20only%20on%20the%20changing%20parts%20of%20the%20point%20cloud.%20By%20doing%20so%2C%20we%20achieve%20the%20same%20results%20with%20as%20much%20as%20a%206.61-fold%20reduction%20in%20processing%20time.%20Our%20test%20results%20show%20that%20the%20feature%20maps%20output%20by%20our%20method%20are%20identical%20to%20those%20produced%20by%20traditional%20sparse%20convolution%20techniques%2C%20whilst%20greatly%20increasing%20the%20computational%20efficiency%20of%20the%20network.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08557v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSSCATeR%253A%2520Sparse%2520Scatter-Based%2520Convolution%2520Algorithm%2520with%2520Temporal%2520Data%2520Recycling%2520for%2520Real-Time%25203D%2520Object%2520Detection%2520in%2520LiDAR%2520Point%2520Clouds%26entry.906535625%3DAlexander%2520Dow%2520and%2520Manduhu%2520Manduhu%2520and%2520Matheus%2520Santos%2520and%2520Ben%2520Bartlett%2520and%2520Gerard%2520Dooly%2520and%2520James%2520Riordan%26entry.1292438233%3DThis%2520work%2520leverages%2520the%2520continuous%2520sweeping%2520motion%2520of%2520LiDAR%2520scanning%2520to%2520concentrate%2520object%2520detection%2520efforts%2520on%2520specific%2520regions%2520that%2520receive%2520a%2520change%2520in%2520point%2520data%2520from%2520one%2520frame%2520to%2520another.%2520We%2520achieve%2520this%2520by%2520using%2520a%2520sliding%2520time%2520window%2520with%2520short%2520strides%2520and%2520consider%2520the%2520temporal%2520dimension%2520by%2520storing%2520convolution%2520results%2520between%2520passes.%2520This%2520allows%2520us%2520to%2520ignore%2520unchanged%2520regions%252C%2520significantly%2520reducing%2520the%2520number%2520of%2520convolution%2520operations%2520per%2520forward%2520pass%2520without%2520sacrificing%2520accuracy.%2520This%2520data%2520reuse%2520scheme%2520introduces%2520extreme%2520sparsity%2520to%2520detection%2520data.%2520To%2520exploit%2520this%2520sparsity%252C%2520we%2520extend%2520our%2520previous%2520work%2520on%2520scatter-based%2520convolutions%2520to%2520allow%2520for%2520data%2520reuse%252C%2520and%2520as%2520such%2520propose%2520Sparse%2520Scatter-Based%2520Convolution%2520Algorithm%2520with%2520Temporal%2520Data%2520Recycling%2520%2528SSCATeR%2529.%2520This%2520operation%2520treats%2520incoming%2520LiDAR%2520data%2520as%2520a%2520continuous%2520stream%2520and%2520acts%2520only%2520on%2520the%2520changing%2520parts%2520of%2520the%2520point%2520cloud.%2520By%2520doing%2520so%252C%2520we%2520achieve%2520the%2520same%2520results%2520with%2520as%2520much%2520as%2520a%25206.61-fold%2520reduction%2520in%2520processing%2520time.%2520Our%2520test%2520results%2520show%2520that%2520the%2520feature%2520maps%2520output%2520by%2520our%2520method%2520are%2520identical%2520to%2520those%2520produced%2520by%2520traditional%2520sparse%2520convolution%2520techniques%252C%2520whilst%2520greatly%2520increasing%2520the%2520computational%2520efficiency%2520of%2520the%2520network.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08557v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SSCATeR%3A%20Sparse%20Scatter-Based%20Convolution%20Algorithm%20with%20Temporal%20Data%20Recycling%20for%20Real-Time%203D%20Object%20Detection%20in%20LiDAR%20Point%20Clouds&entry.906535625=Alexander%20Dow%20and%20Manduhu%20Manduhu%20and%20Matheus%20Santos%20and%20Ben%20Bartlett%20and%20Gerard%20Dooly%20and%20James%20Riordan&entry.1292438233=This%20work%20leverages%20the%20continuous%20sweeping%20motion%20of%20LiDAR%20scanning%20to%20concentrate%20object%20detection%20efforts%20on%20specific%20regions%20that%20receive%20a%20change%20in%20point%20data%20from%20one%20frame%20to%20another.%20We%20achieve%20this%20by%20using%20a%20sliding%20time%20window%20with%20short%20strides%20and%20consider%20the%20temporal%20dimension%20by%20storing%20convolution%20results%20between%20passes.%20This%20allows%20us%20to%20ignore%20unchanged%20regions%2C%20significantly%20reducing%20the%20number%20of%20convolution%20operations%20per%20forward%20pass%20without%20sacrificing%20accuracy.%20This%20data%20reuse%20scheme%20introduces%20extreme%20sparsity%20to%20detection%20data.%20To%20exploit%20this%20sparsity%2C%20we%20extend%20our%20previous%20work%20on%20scatter-based%20convolutions%20to%20allow%20for%20data%20reuse%2C%20and%20as%20such%20propose%20Sparse%20Scatter-Based%20Convolution%20Algorithm%20with%20Temporal%20Data%20Recycling%20%28SSCATeR%29.%20This%20operation%20treats%20incoming%20LiDAR%20data%20as%20a%20continuous%20stream%20and%20acts%20only%20on%20the%20changing%20parts%20of%20the%20point%20cloud.%20By%20doing%20so%2C%20we%20achieve%20the%20same%20results%20with%20as%20much%20as%20a%206.61-fold%20reduction%20in%20processing%20time.%20Our%20test%20results%20show%20that%20the%20feature%20maps%20output%20by%20our%20method%20are%20identical%20to%20those%20produced%20by%20traditional%20sparse%20convolution%20techniques%2C%20whilst%20greatly%20increasing%20the%20computational%20efficiency%20of%20the%20network.&entry.1838667208=http%3A//arxiv.org/abs/2512.08557v1&entry.124074799=Read"},
{"title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens", "author": "Kaizhi Zheng and Xuehai He and Xin Eric Wang", "abstract": "The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a profound capability in multimodal understanding. However, the simultaneous generation of images with coherent texts is still underdeveloped. Addressing this, we introduce a novel interleaved vision-and-language generation method, centered around the concept of ``generative vokens\". These vokens serve as pivotal elements contributing to coherent image-text outputs. Our method is marked by a unique two-stage training strategy for description-free multimodal generation, which does not necessitate extensive descriptions of images. We integrate classifier-free guidance to enhance the alignment of generated images and texts, ensuring more seamless and contextually relevant multimodal interactions. Our model, MiniGPT-5, exhibits substantial improvement over the baseline models on multimodal generation datasets, including MMDialog and VIST. The human evaluation shows MiniGPT-5 is better than the baseline model on more than 56\\% cases for multimodal generation, highlighting its efficacy across diverse benchmarks.", "link": "http://arxiv.org/abs/2310.02239v4", "date": "2025-12-09", "relevancy": 2.9374, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5967}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5845}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5812}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MiniGPT-5%3A%20Interleaved%20Vision-and-Language%20Generation%20via%20Generative%20Vokens&body=Title%3A%20MiniGPT-5%3A%20Interleaved%20Vision-and-Language%20Generation%20via%20Generative%20Vokens%0AAuthor%3A%20Kaizhi%20Zheng%20and%20Xuehai%20He%20and%20Xin%20Eric%20Wang%0AAbstract%3A%20The%20effectiveness%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20demonstrates%20a%20profound%20capability%20in%20multimodal%20understanding.%20However%2C%20the%20simultaneous%20generation%20of%20images%20with%20coherent%20texts%20is%20still%20underdeveloped.%20Addressing%20this%2C%20we%20introduce%20a%20novel%20interleaved%20vision-and-language%20generation%20method%2C%20centered%20around%20the%20concept%20of%20%60%60generative%20vokens%22.%20These%20vokens%20serve%20as%20pivotal%20elements%20contributing%20to%20coherent%20image-text%20outputs.%20Our%20method%20is%20marked%20by%20a%20unique%20two-stage%20training%20strategy%20for%20description-free%20multimodal%20generation%2C%20which%20does%20not%20necessitate%20extensive%20descriptions%20of%20images.%20We%20integrate%20classifier-free%20guidance%20to%20enhance%20the%20alignment%20of%20generated%20images%20and%20texts%2C%20ensuring%20more%20seamless%20and%20contextually%20relevant%20multimodal%20interactions.%20Our%20model%2C%20MiniGPT-5%2C%20exhibits%20substantial%20improvement%20over%20the%20baseline%20models%20on%20multimodal%20generation%20datasets%2C%20including%20MMDialog%20and%20VIST.%20The%20human%20evaluation%20shows%20MiniGPT-5%20is%20better%20than%20the%20baseline%20model%20on%20more%20than%2056%5C%25%20cases%20for%20multimodal%20generation%2C%20highlighting%20its%20efficacy%20across%20diverse%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2310.02239v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMiniGPT-5%253A%2520Interleaved%2520Vision-and-Language%2520Generation%2520via%2520Generative%2520Vokens%26entry.906535625%3DKaizhi%2520Zheng%2520and%2520Xuehai%2520He%2520and%2520Xin%2520Eric%2520Wang%26entry.1292438233%3DThe%2520effectiveness%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%2520demonstrates%2520a%2520profound%2520capability%2520in%2520multimodal%2520understanding.%2520However%252C%2520the%2520simultaneous%2520generation%2520of%2520images%2520with%2520coherent%2520texts%2520is%2520still%2520underdeveloped.%2520Addressing%2520this%252C%2520we%2520introduce%2520a%2520novel%2520interleaved%2520vision-and-language%2520generation%2520method%252C%2520centered%2520around%2520the%2520concept%2520of%2520%2560%2560generative%2520vokens%2522.%2520These%2520vokens%2520serve%2520as%2520pivotal%2520elements%2520contributing%2520to%2520coherent%2520image-text%2520outputs.%2520Our%2520method%2520is%2520marked%2520by%2520a%2520unique%2520two-stage%2520training%2520strategy%2520for%2520description-free%2520multimodal%2520generation%252C%2520which%2520does%2520not%2520necessitate%2520extensive%2520descriptions%2520of%2520images.%2520We%2520integrate%2520classifier-free%2520guidance%2520to%2520enhance%2520the%2520alignment%2520of%2520generated%2520images%2520and%2520texts%252C%2520ensuring%2520more%2520seamless%2520and%2520contextually%2520relevant%2520multimodal%2520interactions.%2520Our%2520model%252C%2520MiniGPT-5%252C%2520exhibits%2520substantial%2520improvement%2520over%2520the%2520baseline%2520models%2520on%2520multimodal%2520generation%2520datasets%252C%2520including%2520MMDialog%2520and%2520VIST.%2520The%2520human%2520evaluation%2520shows%2520MiniGPT-5%2520is%2520better%2520than%2520the%2520baseline%2520model%2520on%2520more%2520than%252056%255C%2525%2520cases%2520for%2520multimodal%2520generation%252C%2520highlighting%2520its%2520efficacy%2520across%2520diverse%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.02239v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MiniGPT-5%3A%20Interleaved%20Vision-and-Language%20Generation%20via%20Generative%20Vokens&entry.906535625=Kaizhi%20Zheng%20and%20Xuehai%20He%20and%20Xin%20Eric%20Wang&entry.1292438233=The%20effectiveness%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%20demonstrates%20a%20profound%20capability%20in%20multimodal%20understanding.%20However%2C%20the%20simultaneous%20generation%20of%20images%20with%20coherent%20texts%20is%20still%20underdeveloped.%20Addressing%20this%2C%20we%20introduce%20a%20novel%20interleaved%20vision-and-language%20generation%20method%2C%20centered%20around%20the%20concept%20of%20%60%60generative%20vokens%22.%20These%20vokens%20serve%20as%20pivotal%20elements%20contributing%20to%20coherent%20image-text%20outputs.%20Our%20method%20is%20marked%20by%20a%20unique%20two-stage%20training%20strategy%20for%20description-free%20multimodal%20generation%2C%20which%20does%20not%20necessitate%20extensive%20descriptions%20of%20images.%20We%20integrate%20classifier-free%20guidance%20to%20enhance%20the%20alignment%20of%20generated%20images%20and%20texts%2C%20ensuring%20more%20seamless%20and%20contextually%20relevant%20multimodal%20interactions.%20Our%20model%2C%20MiniGPT-5%2C%20exhibits%20substantial%20improvement%20over%20the%20baseline%20models%20on%20multimodal%20generation%20datasets%2C%20including%20MMDialog%20and%20VIST.%20The%20human%20evaluation%20shows%20MiniGPT-5%20is%20better%20than%20the%20baseline%20model%20on%20more%20than%2056%5C%25%20cases%20for%20multimodal%20generation%2C%20highlighting%20its%20efficacy%20across%20diverse%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2310.02239v4&entry.124074799=Read"},
{"title": "Small Drafts, Big Verdict: Information-Intensive Visual Reasoning via Speculation", "author": "Yuhan Liu and Lianhui Qin and Shengjie Wang", "abstract": "Large Vision-Language Models (VLMs) have achieved remarkable progress in multimodal understanding, yet they struggle when reasoning over information-intensive images that densely interleave textual annotations with fine-grained graphical elements. The main challenges lie in precisely localizing critical cues in dense layouts and multi-hop reasoning to integrate dispersed evidence. We propose Speculative Verdict (SV), a training-free framework inspired by speculative decoding that combines multiple lightweight draft experts with a large verdict model. In the draft stage, small VLMs act as draft experts to generate reasoning paths that provide diverse localization candidates; in the verdict stage, a strong VLM synthesizes these paths to produce the final answer, minimizing computational cost while recovering correct answers. To further improve efficiency and accuracy, SV introduces a consensus expert selection mechanism that forwards only high-agreement reasoning paths to the verdict. Empirically, SV achieves consistent gains on challenging information-intensive and high-resolution visual question answering benchmarks, including InfographicVQA, ChartMuseum, ChartQAPro, and HR-Bench 4K. By synthesizing correct insights from multiple partially accurate reasoning paths, SV achieves both error correction and cost-efficiency compared to large proprietary models or training pipelines. Code is available at https://github.com/Tinaliu0123/speculative-verdict.", "link": "http://arxiv.org/abs/2510.20812v3", "date": "2025-12-09", "relevancy": 2.9194, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5856}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.583}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.583}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Small%20Drafts%2C%20Big%20Verdict%3A%20Information-Intensive%20Visual%20Reasoning%20via%20Speculation&body=Title%3A%20Small%20Drafts%2C%20Big%20Verdict%3A%20Information-Intensive%20Visual%20Reasoning%20via%20Speculation%0AAuthor%3A%20Yuhan%20Liu%20and%20Lianhui%20Qin%20and%20Shengjie%20Wang%0AAbstract%3A%20Large%20Vision-Language%20Models%20%28VLMs%29%20have%20achieved%20remarkable%20progress%20in%20multimodal%20understanding%2C%20yet%20they%20struggle%20when%20reasoning%20over%20information-intensive%20images%20that%20densely%20interleave%20textual%20annotations%20with%20fine-grained%20graphical%20elements.%20The%20main%20challenges%20lie%20in%20precisely%20localizing%20critical%20cues%20in%20dense%20layouts%20and%20multi-hop%20reasoning%20to%20integrate%20dispersed%20evidence.%20We%20propose%20Speculative%20Verdict%20%28SV%29%2C%20a%20training-free%20framework%20inspired%20by%20speculative%20decoding%20that%20combines%20multiple%20lightweight%20draft%20experts%20with%20a%20large%20verdict%20model.%20In%20the%20draft%20stage%2C%20small%20VLMs%20act%20as%20draft%20experts%20to%20generate%20reasoning%20paths%20that%20provide%20diverse%20localization%20candidates%3B%20in%20the%20verdict%20stage%2C%20a%20strong%20VLM%20synthesizes%20these%20paths%20to%20produce%20the%20final%20answer%2C%20minimizing%20computational%20cost%20while%20recovering%20correct%20answers.%20To%20further%20improve%20efficiency%20and%20accuracy%2C%20SV%20introduces%20a%20consensus%20expert%20selection%20mechanism%20that%20forwards%20only%20high-agreement%20reasoning%20paths%20to%20the%20verdict.%20Empirically%2C%20SV%20achieves%20consistent%20gains%20on%20challenging%20information-intensive%20and%20high-resolution%20visual%20question%20answering%20benchmarks%2C%20including%20InfographicVQA%2C%20ChartMuseum%2C%20ChartQAPro%2C%20and%20HR-Bench%204K.%20By%20synthesizing%20correct%20insights%20from%20multiple%20partially%20accurate%20reasoning%20paths%2C%20SV%20achieves%20both%20error%20correction%20and%20cost-efficiency%20compared%20to%20large%20proprietary%20models%20or%20training%20pipelines.%20Code%20is%20available%20at%20https%3A//github.com/Tinaliu0123/speculative-verdict.%0ALink%3A%20http%3A//arxiv.org/abs/2510.20812v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSmall%2520Drafts%252C%2520Big%2520Verdict%253A%2520Information-Intensive%2520Visual%2520Reasoning%2520via%2520Speculation%26entry.906535625%3DYuhan%2520Liu%2520and%2520Lianhui%2520Qin%2520and%2520Shengjie%2520Wang%26entry.1292438233%3DLarge%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520have%2520achieved%2520remarkable%2520progress%2520in%2520multimodal%2520understanding%252C%2520yet%2520they%2520struggle%2520when%2520reasoning%2520over%2520information-intensive%2520images%2520that%2520densely%2520interleave%2520textual%2520annotations%2520with%2520fine-grained%2520graphical%2520elements.%2520The%2520main%2520challenges%2520lie%2520in%2520precisely%2520localizing%2520critical%2520cues%2520in%2520dense%2520layouts%2520and%2520multi-hop%2520reasoning%2520to%2520integrate%2520dispersed%2520evidence.%2520We%2520propose%2520Speculative%2520Verdict%2520%2528SV%2529%252C%2520a%2520training-free%2520framework%2520inspired%2520by%2520speculative%2520decoding%2520that%2520combines%2520multiple%2520lightweight%2520draft%2520experts%2520with%2520a%2520large%2520verdict%2520model.%2520In%2520the%2520draft%2520stage%252C%2520small%2520VLMs%2520act%2520as%2520draft%2520experts%2520to%2520generate%2520reasoning%2520paths%2520that%2520provide%2520diverse%2520localization%2520candidates%253B%2520in%2520the%2520verdict%2520stage%252C%2520a%2520strong%2520VLM%2520synthesizes%2520these%2520paths%2520to%2520produce%2520the%2520final%2520answer%252C%2520minimizing%2520computational%2520cost%2520while%2520recovering%2520correct%2520answers.%2520To%2520further%2520improve%2520efficiency%2520and%2520accuracy%252C%2520SV%2520introduces%2520a%2520consensus%2520expert%2520selection%2520mechanism%2520that%2520forwards%2520only%2520high-agreement%2520reasoning%2520paths%2520to%2520the%2520verdict.%2520Empirically%252C%2520SV%2520achieves%2520consistent%2520gains%2520on%2520challenging%2520information-intensive%2520and%2520high-resolution%2520visual%2520question%2520answering%2520benchmarks%252C%2520including%2520InfographicVQA%252C%2520ChartMuseum%252C%2520ChartQAPro%252C%2520and%2520HR-Bench%25204K.%2520By%2520synthesizing%2520correct%2520insights%2520from%2520multiple%2520partially%2520accurate%2520reasoning%2520paths%252C%2520SV%2520achieves%2520both%2520error%2520correction%2520and%2520cost-efficiency%2520compared%2520to%2520large%2520proprietary%2520models%2520or%2520training%2520pipelines.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/Tinaliu0123/speculative-verdict.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.20812v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Small%20Drafts%2C%20Big%20Verdict%3A%20Information-Intensive%20Visual%20Reasoning%20via%20Speculation&entry.906535625=Yuhan%20Liu%20and%20Lianhui%20Qin%20and%20Shengjie%20Wang&entry.1292438233=Large%20Vision-Language%20Models%20%28VLMs%29%20have%20achieved%20remarkable%20progress%20in%20multimodal%20understanding%2C%20yet%20they%20struggle%20when%20reasoning%20over%20information-intensive%20images%20that%20densely%20interleave%20textual%20annotations%20with%20fine-grained%20graphical%20elements.%20The%20main%20challenges%20lie%20in%20precisely%20localizing%20critical%20cues%20in%20dense%20layouts%20and%20multi-hop%20reasoning%20to%20integrate%20dispersed%20evidence.%20We%20propose%20Speculative%20Verdict%20%28SV%29%2C%20a%20training-free%20framework%20inspired%20by%20speculative%20decoding%20that%20combines%20multiple%20lightweight%20draft%20experts%20with%20a%20large%20verdict%20model.%20In%20the%20draft%20stage%2C%20small%20VLMs%20act%20as%20draft%20experts%20to%20generate%20reasoning%20paths%20that%20provide%20diverse%20localization%20candidates%3B%20in%20the%20verdict%20stage%2C%20a%20strong%20VLM%20synthesizes%20these%20paths%20to%20produce%20the%20final%20answer%2C%20minimizing%20computational%20cost%20while%20recovering%20correct%20answers.%20To%20further%20improve%20efficiency%20and%20accuracy%2C%20SV%20introduces%20a%20consensus%20expert%20selection%20mechanism%20that%20forwards%20only%20high-agreement%20reasoning%20paths%20to%20the%20verdict.%20Empirically%2C%20SV%20achieves%20consistent%20gains%20on%20challenging%20information-intensive%20and%20high-resolution%20visual%20question%20answering%20benchmarks%2C%20including%20InfographicVQA%2C%20ChartMuseum%2C%20ChartQAPro%2C%20and%20HR-Bench%204K.%20By%20synthesizing%20correct%20insights%20from%20multiple%20partially%20accurate%20reasoning%20paths%2C%20SV%20achieves%20both%20error%20correction%20and%20cost-efficiency%20compared%20to%20large%20proprietary%20models%20or%20training%20pipelines.%20Code%20is%20available%20at%20https%3A//github.com/Tinaliu0123/speculative-verdict.&entry.1838667208=http%3A//arxiv.org/abs/2510.20812v3&entry.124074799=Read"},
{"title": "Learning Geodesics of Geometric Shape Deformations From Images", "author": "Nian Wu and Miaomiao Zhang", "abstract": "This paper presents a novel method, named geodesic deformable networks (GDN), that for the first time enables the learning of geodesic flows of deformation fields derived from images. In particular, the capability of our proposed GDN being able to predict geodesics is important for quantifying and comparing deformable shape presented in images. The geodesic deformations, also known as optimal transformations that align pairwise images, are often parameterized by a time sequence of smooth vector fields governed by nonlinear differential equations. A bountiful literature has been focusing on learning the initial conditions (e.g., initial velocity fields) based on registration networks. However, the definition of geodesics central to deformation-based shape analysis is blind to the networks. To address this problem, we carefully develop an efficient neural operator to treat the geodesics as unknown mapping functions learned from the latent deformation spaces. A composition of integral operators and smooth activation functions is then formulated to effectively approximate such mappings. In contrast to previous works, our GDN jointly optimizes a newly defined geodesic loss, which adds additional benefits to promote the network regularizability and generalizability. We demonstrate the effectiveness of GDN on both 2D synthetic data and 3D real brain magnetic resonance imaging (MRI).", "link": "http://arxiv.org/abs/2410.18797v3", "date": "2025-12-09", "relevancy": 2.901, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5916}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5779}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Geodesics%20of%20Geometric%20Shape%20Deformations%20From%20Images&body=Title%3A%20Learning%20Geodesics%20of%20Geometric%20Shape%20Deformations%20From%20Images%0AAuthor%3A%20Nian%20Wu%20and%20Miaomiao%20Zhang%0AAbstract%3A%20This%20paper%20presents%20a%20novel%20method%2C%20named%20geodesic%20deformable%20networks%20%28GDN%29%2C%20that%20for%20the%20first%20time%20enables%20the%20learning%20of%20geodesic%20flows%20of%20deformation%20fields%20derived%20from%20images.%20In%20particular%2C%20the%20capability%20of%20our%20proposed%20GDN%20being%20able%20to%20predict%20geodesics%20is%20important%20for%20quantifying%20and%20comparing%20deformable%20shape%20presented%20in%20images.%20The%20geodesic%20deformations%2C%20also%20known%20as%20optimal%20transformations%20that%20align%20pairwise%20images%2C%20are%20often%20parameterized%20by%20a%20time%20sequence%20of%20smooth%20vector%20fields%20governed%20by%20nonlinear%20differential%20equations.%20A%20bountiful%20literature%20has%20been%20focusing%20on%20learning%20the%20initial%20conditions%20%28e.g.%2C%20initial%20velocity%20fields%29%20based%20on%20registration%20networks.%20However%2C%20the%20definition%20of%20geodesics%20central%20to%20deformation-based%20shape%20analysis%20is%20blind%20to%20the%20networks.%20To%20address%20this%20problem%2C%20we%20carefully%20develop%20an%20efficient%20neural%20operator%20to%20treat%20the%20geodesics%20as%20unknown%20mapping%20functions%20learned%20from%20the%20latent%20deformation%20spaces.%20A%20composition%20of%20integral%20operators%20and%20smooth%20activation%20functions%20is%20then%20formulated%20to%20effectively%20approximate%20such%20mappings.%20In%20contrast%20to%20previous%20works%2C%20our%20GDN%20jointly%20optimizes%20a%20newly%20defined%20geodesic%20loss%2C%20which%20adds%20additional%20benefits%20to%20promote%20the%20network%20regularizability%20and%20generalizability.%20We%20demonstrate%20the%20effectiveness%20of%20GDN%20on%20both%202D%20synthetic%20data%20and%203D%20real%20brain%20magnetic%20resonance%20imaging%20%28MRI%29.%0ALink%3A%20http%3A//arxiv.org/abs/2410.18797v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Geodesics%2520of%2520Geometric%2520Shape%2520Deformations%2520From%2520Images%26entry.906535625%3DNian%2520Wu%2520and%2520Miaomiao%2520Zhang%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520novel%2520method%252C%2520named%2520geodesic%2520deformable%2520networks%2520%2528GDN%2529%252C%2520that%2520for%2520the%2520first%2520time%2520enables%2520the%2520learning%2520of%2520geodesic%2520flows%2520of%2520deformation%2520fields%2520derived%2520from%2520images.%2520In%2520particular%252C%2520the%2520capability%2520of%2520our%2520proposed%2520GDN%2520being%2520able%2520to%2520predict%2520geodesics%2520is%2520important%2520for%2520quantifying%2520and%2520comparing%2520deformable%2520shape%2520presented%2520in%2520images.%2520The%2520geodesic%2520deformations%252C%2520also%2520known%2520as%2520optimal%2520transformations%2520that%2520align%2520pairwise%2520images%252C%2520are%2520often%2520parameterized%2520by%2520a%2520time%2520sequence%2520of%2520smooth%2520vector%2520fields%2520governed%2520by%2520nonlinear%2520differential%2520equations.%2520A%2520bountiful%2520literature%2520has%2520been%2520focusing%2520on%2520learning%2520the%2520initial%2520conditions%2520%2528e.g.%252C%2520initial%2520velocity%2520fields%2529%2520based%2520on%2520registration%2520networks.%2520However%252C%2520the%2520definition%2520of%2520geodesics%2520central%2520to%2520deformation-based%2520shape%2520analysis%2520is%2520blind%2520to%2520the%2520networks.%2520To%2520address%2520this%2520problem%252C%2520we%2520carefully%2520develop%2520an%2520efficient%2520neural%2520operator%2520to%2520treat%2520the%2520geodesics%2520as%2520unknown%2520mapping%2520functions%2520learned%2520from%2520the%2520latent%2520deformation%2520spaces.%2520A%2520composition%2520of%2520integral%2520operators%2520and%2520smooth%2520activation%2520functions%2520is%2520then%2520formulated%2520to%2520effectively%2520approximate%2520such%2520mappings.%2520In%2520contrast%2520to%2520previous%2520works%252C%2520our%2520GDN%2520jointly%2520optimizes%2520a%2520newly%2520defined%2520geodesic%2520loss%252C%2520which%2520adds%2520additional%2520benefits%2520to%2520promote%2520the%2520network%2520regularizability%2520and%2520generalizability.%2520We%2520demonstrate%2520the%2520effectiveness%2520of%2520GDN%2520on%2520both%25202D%2520synthetic%2520data%2520and%25203D%2520real%2520brain%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2410.18797v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Geodesics%20of%20Geometric%20Shape%20Deformations%20From%20Images&entry.906535625=Nian%20Wu%20and%20Miaomiao%20Zhang&entry.1292438233=This%20paper%20presents%20a%20novel%20method%2C%20named%20geodesic%20deformable%20networks%20%28GDN%29%2C%20that%20for%20the%20first%20time%20enables%20the%20learning%20of%20geodesic%20flows%20of%20deformation%20fields%20derived%20from%20images.%20In%20particular%2C%20the%20capability%20of%20our%20proposed%20GDN%20being%20able%20to%20predict%20geodesics%20is%20important%20for%20quantifying%20and%20comparing%20deformable%20shape%20presented%20in%20images.%20The%20geodesic%20deformations%2C%20also%20known%20as%20optimal%20transformations%20that%20align%20pairwise%20images%2C%20are%20often%20parameterized%20by%20a%20time%20sequence%20of%20smooth%20vector%20fields%20governed%20by%20nonlinear%20differential%20equations.%20A%20bountiful%20literature%20has%20been%20focusing%20on%20learning%20the%20initial%20conditions%20%28e.g.%2C%20initial%20velocity%20fields%29%20based%20on%20registration%20networks.%20However%2C%20the%20definition%20of%20geodesics%20central%20to%20deformation-based%20shape%20analysis%20is%20blind%20to%20the%20networks.%20To%20address%20this%20problem%2C%20we%20carefully%20develop%20an%20efficient%20neural%20operator%20to%20treat%20the%20geodesics%20as%20unknown%20mapping%20functions%20learned%20from%20the%20latent%20deformation%20spaces.%20A%20composition%20of%20integral%20operators%20and%20smooth%20activation%20functions%20is%20then%20formulated%20to%20effectively%20approximate%20such%20mappings.%20In%20contrast%20to%20previous%20works%2C%20our%20GDN%20jointly%20optimizes%20a%20newly%20defined%20geodesic%20loss%2C%20which%20adds%20additional%20benefits%20to%20promote%20the%20network%20regularizability%20and%20generalizability.%20We%20demonstrate%20the%20effectiveness%20of%20GDN%20on%20both%202D%20synthetic%20data%20and%203D%20real%20brain%20magnetic%20resonance%20imaging%20%28MRI%29.&entry.1838667208=http%3A//arxiv.org/abs/2410.18797v3&entry.124074799=Read"},
{"title": "Scale-invariant and View-relational Representation Learning for Full Surround Monocular Depth", "author": "Kyumin Hwang and Wonhyeok Choi and Kiljoon Han and Wonjoon Choi and Minwoo Choi and Yongcheon Na and Minwoo Park and Sunghoon Im", "abstract": "Recent foundation models demonstrate strong generalization capabilities in monocular depth estimation. However, directly applying these models to Full Surround Monocular Depth Estimation (FSMDE) presents two major challenges: (1) high computational cost, which limits real-time performance, and (2) difficulty in estimating metric-scale depth, as these models are typically trained to predict only relative depth. To address these limitations, we propose a novel knowledge distillation strategy that transfers robust depth knowledge from a foundation model to a lightweight FSMDE network. Our approach leverages a hybrid regression framework combining the knowledge distillation scheme--traditionally used in classification--with a depth binning module to enhance scale consistency. Specifically, we introduce a cross-interaction knowledge distillation scheme that distills the scale-invariant depth bin probabilities of a foundation model into the student network while guiding it to infer metric-scale depth bin centers from ground-truth depth. Furthermore, we propose view-relational knowledge distillation, which encodes structural relationships among adjacent camera views and transfers them to enhance cross-view depth consistency. Experiments on DDAD and nuScenes demonstrate the effectiveness of our method compared to conventional supervised methods and existing knowledge distillation approaches. Moreover, our method achieves a favorable trade-off between performance and efficiency, meeting real-time requirements.", "link": "http://arxiv.org/abs/2512.08700v1", "date": "2025-12-09", "relevancy": 2.9009, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5804}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5804}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5797}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Scale-invariant%20and%20View-relational%20Representation%20Learning%20for%20Full%20Surround%20Monocular%20Depth&body=Title%3A%20Scale-invariant%20and%20View-relational%20Representation%20Learning%20for%20Full%20Surround%20Monocular%20Depth%0AAuthor%3A%20Kyumin%20Hwang%20and%20Wonhyeok%20Choi%20and%20Kiljoon%20Han%20and%20Wonjoon%20Choi%20and%20Minwoo%20Choi%20and%20Yongcheon%20Na%20and%20Minwoo%20Park%20and%20Sunghoon%20Im%0AAbstract%3A%20Recent%20foundation%20models%20demonstrate%20strong%20generalization%20capabilities%20in%20monocular%20depth%20estimation.%20However%2C%20directly%20applying%20these%20models%20to%20Full%20Surround%20Monocular%20Depth%20Estimation%20%28FSMDE%29%20presents%20two%20major%20challenges%3A%20%281%29%20high%20computational%20cost%2C%20which%20limits%20real-time%20performance%2C%20and%20%282%29%20difficulty%20in%20estimating%20metric-scale%20depth%2C%20as%20these%20models%20are%20typically%20trained%20to%20predict%20only%20relative%20depth.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20knowledge%20distillation%20strategy%20that%20transfers%20robust%20depth%20knowledge%20from%20a%20foundation%20model%20to%20a%20lightweight%20FSMDE%20network.%20Our%20approach%20leverages%20a%20hybrid%20regression%20framework%20combining%20the%20knowledge%20distillation%20scheme--traditionally%20used%20in%20classification--with%20a%20depth%20binning%20module%20to%20enhance%20scale%20consistency.%20Specifically%2C%20we%20introduce%20a%20cross-interaction%20knowledge%20distillation%20scheme%20that%20distills%20the%20scale-invariant%20depth%20bin%20probabilities%20of%20a%20foundation%20model%20into%20the%20student%20network%20while%20guiding%20it%20to%20infer%20metric-scale%20depth%20bin%20centers%20from%20ground-truth%20depth.%20Furthermore%2C%20we%20propose%20view-relational%20knowledge%20distillation%2C%20which%20encodes%20structural%20relationships%20among%20adjacent%20camera%20views%20and%20transfers%20them%20to%20enhance%20cross-view%20depth%20consistency.%20Experiments%20on%20DDAD%20and%20nuScenes%20demonstrate%20the%20effectiveness%20of%20our%20method%20compared%20to%20conventional%20supervised%20methods%20and%20existing%20knowledge%20distillation%20approaches.%20Moreover%2C%20our%20method%20achieves%20a%20favorable%20trade-off%20between%20performance%20and%20efficiency%2C%20meeting%20real-time%20requirements.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08700v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DScale-invariant%2520and%2520View-relational%2520Representation%2520Learning%2520for%2520Full%2520Surround%2520Monocular%2520Depth%26entry.906535625%3DKyumin%2520Hwang%2520and%2520Wonhyeok%2520Choi%2520and%2520Kiljoon%2520Han%2520and%2520Wonjoon%2520Choi%2520and%2520Minwoo%2520Choi%2520and%2520Yongcheon%2520Na%2520and%2520Minwoo%2520Park%2520and%2520Sunghoon%2520Im%26entry.1292438233%3DRecent%2520foundation%2520models%2520demonstrate%2520strong%2520generalization%2520capabilities%2520in%2520monocular%2520depth%2520estimation.%2520However%252C%2520directly%2520applying%2520these%2520models%2520to%2520Full%2520Surround%2520Monocular%2520Depth%2520Estimation%2520%2528FSMDE%2529%2520presents%2520two%2520major%2520challenges%253A%2520%25281%2529%2520high%2520computational%2520cost%252C%2520which%2520limits%2520real-time%2520performance%252C%2520and%2520%25282%2529%2520difficulty%2520in%2520estimating%2520metric-scale%2520depth%252C%2520as%2520these%2520models%2520are%2520typically%2520trained%2520to%2520predict%2520only%2520relative%2520depth.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520knowledge%2520distillation%2520strategy%2520that%2520transfers%2520robust%2520depth%2520knowledge%2520from%2520a%2520foundation%2520model%2520to%2520a%2520lightweight%2520FSMDE%2520network.%2520Our%2520approach%2520leverages%2520a%2520hybrid%2520regression%2520framework%2520combining%2520the%2520knowledge%2520distillation%2520scheme--traditionally%2520used%2520in%2520classification--with%2520a%2520depth%2520binning%2520module%2520to%2520enhance%2520scale%2520consistency.%2520Specifically%252C%2520we%2520introduce%2520a%2520cross-interaction%2520knowledge%2520distillation%2520scheme%2520that%2520distills%2520the%2520scale-invariant%2520depth%2520bin%2520probabilities%2520of%2520a%2520foundation%2520model%2520into%2520the%2520student%2520network%2520while%2520guiding%2520it%2520to%2520infer%2520metric-scale%2520depth%2520bin%2520centers%2520from%2520ground-truth%2520depth.%2520Furthermore%252C%2520we%2520propose%2520view-relational%2520knowledge%2520distillation%252C%2520which%2520encodes%2520structural%2520relationships%2520among%2520adjacent%2520camera%2520views%2520and%2520transfers%2520them%2520to%2520enhance%2520cross-view%2520depth%2520consistency.%2520Experiments%2520on%2520DDAD%2520and%2520nuScenes%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520method%2520compared%2520to%2520conventional%2520supervised%2520methods%2520and%2520existing%2520knowledge%2520distillation%2520approaches.%2520Moreover%252C%2520our%2520method%2520achieves%2520a%2520favorable%2520trade-off%2520between%2520performance%2520and%2520efficiency%252C%2520meeting%2520real-time%2520requirements.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08700v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Scale-invariant%20and%20View-relational%20Representation%20Learning%20for%20Full%20Surround%20Monocular%20Depth&entry.906535625=Kyumin%20Hwang%20and%20Wonhyeok%20Choi%20and%20Kiljoon%20Han%20and%20Wonjoon%20Choi%20and%20Minwoo%20Choi%20and%20Yongcheon%20Na%20and%20Minwoo%20Park%20and%20Sunghoon%20Im&entry.1292438233=Recent%20foundation%20models%20demonstrate%20strong%20generalization%20capabilities%20in%20monocular%20depth%20estimation.%20However%2C%20directly%20applying%20these%20models%20to%20Full%20Surround%20Monocular%20Depth%20Estimation%20%28FSMDE%29%20presents%20two%20major%20challenges%3A%20%281%29%20high%20computational%20cost%2C%20which%20limits%20real-time%20performance%2C%20and%20%282%29%20difficulty%20in%20estimating%20metric-scale%20depth%2C%20as%20these%20models%20are%20typically%20trained%20to%20predict%20only%20relative%20depth.%20To%20address%20these%20limitations%2C%20we%20propose%20a%20novel%20knowledge%20distillation%20strategy%20that%20transfers%20robust%20depth%20knowledge%20from%20a%20foundation%20model%20to%20a%20lightweight%20FSMDE%20network.%20Our%20approach%20leverages%20a%20hybrid%20regression%20framework%20combining%20the%20knowledge%20distillation%20scheme--traditionally%20used%20in%20classification--with%20a%20depth%20binning%20module%20to%20enhance%20scale%20consistency.%20Specifically%2C%20we%20introduce%20a%20cross-interaction%20knowledge%20distillation%20scheme%20that%20distills%20the%20scale-invariant%20depth%20bin%20probabilities%20of%20a%20foundation%20model%20into%20the%20student%20network%20while%20guiding%20it%20to%20infer%20metric-scale%20depth%20bin%20centers%20from%20ground-truth%20depth.%20Furthermore%2C%20we%20propose%20view-relational%20knowledge%20distillation%2C%20which%20encodes%20structural%20relationships%20among%20adjacent%20camera%20views%20and%20transfers%20them%20to%20enhance%20cross-view%20depth%20consistency.%20Experiments%20on%20DDAD%20and%20nuScenes%20demonstrate%20the%20effectiveness%20of%20our%20method%20compared%20to%20conventional%20supervised%20methods%20and%20existing%20knowledge%20distillation%20approaches.%20Moreover%2C%20our%20method%20achieves%20a%20favorable%20trade-off%20between%20performance%20and%20efficiency%2C%20meeting%20real-time%20requirements.&entry.1838667208=http%3A//arxiv.org/abs/2512.08700v1&entry.124074799=Read"},
{"title": "InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models", "author": "Hongyuan Tao and Bencheng Liao and Shaoyu Chen and Haoran Yin and Qian Zhang and Wenyu Liu and Xinggang Wang", "abstract": "Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.", "link": "http://arxiv.org/abs/2512.08829v1", "date": "2025-12-09", "relevancy": 2.8824, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5803}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5803}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5689}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InfiniteVL%3A%20Synergizing%20Linear%20and%20Sparse%20Attention%20for%20Highly-Efficient%2C%20Unlimited-Input%20Vision-Language%20Models&body=Title%3A%20InfiniteVL%3A%20Synergizing%20Linear%20and%20Sparse%20Attention%20for%20Highly-Efficient%2C%20Unlimited-Input%20Vision-Language%20Models%0AAuthor%3A%20Hongyuan%20Tao%20and%20Bencheng%20Liao%20and%20Shaoyu%20Chen%20and%20Haoran%20Yin%20and%20Qian%20Zhang%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang%0AAbstract%3A%20Window%20attention%20and%20linear%20attention%20represent%20two%20principal%20strategies%20for%20mitigating%20the%20quadratic%20complexity%20and%20ever-growing%20KV%20cache%20in%20Vision-Language%20Models%20%28VLMs%29.%20However%2C%20we%20observe%20that%20window-based%20VLMs%20suffer%20performance%20degradation%20when%20sequence%20length%20exceeds%20the%20window%20size%2C%20while%20linear%20attention%20underperforms%20on%20information-intensive%20tasks%20such%20as%20OCR%20and%20document%20understanding.%20To%20overcome%20these%20limitations%2C%20we%20propose%20InfiniteVL%2C%20a%20linear-complexity%20VLM%20architecture%20that%20synergizes%20sliding%20window%20attention%20%28SWA%29%20with%20Gated%20DeltaNet.%20For%20achieving%20competitive%20multimodal%20performance%20under%20constrained%20resources%2C%20we%20design%20a%20three-stage%20training%20strategy%20comprising%20distillation%20pretraining%2C%20instruction%20tuning%2C%20and%20long-sequence%20SFT.%20Remarkably%2C%20using%20less%20than%202%5C%25%20of%20the%20training%20data%20required%20by%20leading%20VLMs%2C%20InfiniteVL%20not%20only%20substantially%20outperforms%20previous%20linear-complexity%20VLMs%20but%20also%20matches%20the%20performance%20of%20leading%20Transformer-based%20VLMs%2C%20while%20demonstrating%20effective%20long-term%20memory%20retention.%20Compared%20to%20similar-sized%20Transformer-based%20VLMs%20accelerated%20by%20FlashAttention-2%2C%20InfiniteVL%20achieves%20over%203.6%5Ctimes%20inference%20speedup%20while%20maintaining%20constant%20latency%20and%20memory%20footprint.%20In%20streaming%20video%20understanding%20scenarios%2C%20it%20sustains%20a%20stable%2024%20FPS%20real-time%20prefill%20speed%20while%20preserving%20long-term%20memory%20cache.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/hustvl/InfiniteVL.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08829v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfiniteVL%253A%2520Synergizing%2520Linear%2520and%2520Sparse%2520Attention%2520for%2520Highly-Efficient%252C%2520Unlimited-Input%2520Vision-Language%2520Models%26entry.906535625%3DHongyuan%2520Tao%2520and%2520Bencheng%2520Liao%2520and%2520Shaoyu%2520Chen%2520and%2520Haoran%2520Yin%2520and%2520Qian%2520Zhang%2520and%2520Wenyu%2520Liu%2520and%2520Xinggang%2520Wang%26entry.1292438233%3DWindow%2520attention%2520and%2520linear%2520attention%2520represent%2520two%2520principal%2520strategies%2520for%2520mitigating%2520the%2520quadratic%2520complexity%2520and%2520ever-growing%2520KV%2520cache%2520in%2520Vision-Language%2520Models%2520%2528VLMs%2529.%2520However%252C%2520we%2520observe%2520that%2520window-based%2520VLMs%2520suffer%2520performance%2520degradation%2520when%2520sequence%2520length%2520exceeds%2520the%2520window%2520size%252C%2520while%2520linear%2520attention%2520underperforms%2520on%2520information-intensive%2520tasks%2520such%2520as%2520OCR%2520and%2520document%2520understanding.%2520To%2520overcome%2520these%2520limitations%252C%2520we%2520propose%2520InfiniteVL%252C%2520a%2520linear-complexity%2520VLM%2520architecture%2520that%2520synergizes%2520sliding%2520window%2520attention%2520%2528SWA%2529%2520with%2520Gated%2520DeltaNet.%2520For%2520achieving%2520competitive%2520multimodal%2520performance%2520under%2520constrained%2520resources%252C%2520we%2520design%2520a%2520three-stage%2520training%2520strategy%2520comprising%2520distillation%2520pretraining%252C%2520instruction%2520tuning%252C%2520and%2520long-sequence%2520SFT.%2520Remarkably%252C%2520using%2520less%2520than%25202%255C%2525%2520of%2520the%2520training%2520data%2520required%2520by%2520leading%2520VLMs%252C%2520InfiniteVL%2520not%2520only%2520substantially%2520outperforms%2520previous%2520linear-complexity%2520VLMs%2520but%2520also%2520matches%2520the%2520performance%2520of%2520leading%2520Transformer-based%2520VLMs%252C%2520while%2520demonstrating%2520effective%2520long-term%2520memory%2520retention.%2520Compared%2520to%2520similar-sized%2520Transformer-based%2520VLMs%2520accelerated%2520by%2520FlashAttention-2%252C%2520InfiniteVL%2520achieves%2520over%25203.6%255Ctimes%2520inference%2520speedup%2520while%2520maintaining%2520constant%2520latency%2520and%2520memory%2520footprint.%2520In%2520streaming%2520video%2520understanding%2520scenarios%252C%2520it%2520sustains%2520a%2520stable%252024%2520FPS%2520real-time%2520prefill%2520speed%2520while%2520preserving%2520long-term%2520memory%2520cache.%2520Code%2520and%2520models%2520are%2520available%2520at%2520https%253A//github.com/hustvl/InfiniteVL.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08829v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InfiniteVL%3A%20Synergizing%20Linear%20and%20Sparse%20Attention%20for%20Highly-Efficient%2C%20Unlimited-Input%20Vision-Language%20Models&entry.906535625=Hongyuan%20Tao%20and%20Bencheng%20Liao%20and%20Shaoyu%20Chen%20and%20Haoran%20Yin%20and%20Qian%20Zhang%20and%20Wenyu%20Liu%20and%20Xinggang%20Wang&entry.1292438233=Window%20attention%20and%20linear%20attention%20represent%20two%20principal%20strategies%20for%20mitigating%20the%20quadratic%20complexity%20and%20ever-growing%20KV%20cache%20in%20Vision-Language%20Models%20%28VLMs%29.%20However%2C%20we%20observe%20that%20window-based%20VLMs%20suffer%20performance%20degradation%20when%20sequence%20length%20exceeds%20the%20window%20size%2C%20while%20linear%20attention%20underperforms%20on%20information-intensive%20tasks%20such%20as%20OCR%20and%20document%20understanding.%20To%20overcome%20these%20limitations%2C%20we%20propose%20InfiniteVL%2C%20a%20linear-complexity%20VLM%20architecture%20that%20synergizes%20sliding%20window%20attention%20%28SWA%29%20with%20Gated%20DeltaNet.%20For%20achieving%20competitive%20multimodal%20performance%20under%20constrained%20resources%2C%20we%20design%20a%20three-stage%20training%20strategy%20comprising%20distillation%20pretraining%2C%20instruction%20tuning%2C%20and%20long-sequence%20SFT.%20Remarkably%2C%20using%20less%20than%202%5C%25%20of%20the%20training%20data%20required%20by%20leading%20VLMs%2C%20InfiniteVL%20not%20only%20substantially%20outperforms%20previous%20linear-complexity%20VLMs%20but%20also%20matches%20the%20performance%20of%20leading%20Transformer-based%20VLMs%2C%20while%20demonstrating%20effective%20long-term%20memory%20retention.%20Compared%20to%20similar-sized%20Transformer-based%20VLMs%20accelerated%20by%20FlashAttention-2%2C%20InfiniteVL%20achieves%20over%203.6%5Ctimes%20inference%20speedup%20while%20maintaining%20constant%20latency%20and%20memory%20footprint.%20In%20streaming%20video%20understanding%20scenarios%2C%20it%20sustains%20a%20stable%2024%20FPS%20real-time%20prefill%20speed%20while%20preserving%20long-term%20memory%20cache.%20Code%20and%20models%20are%20available%20at%20https%3A//github.com/hustvl/InfiniteVL.&entry.1838667208=http%3A//arxiv.org/abs/2512.08829v1&entry.124074799=Read"},
{"title": "SATGround: A Spatially-Aware Approach for Visual Grounding in Remote Sensing", "author": "Aysim Toker and Andreea-Maria Oncescu and Roy Miles and Ismail Elezi and Jiankang Deng", "abstract": "Vision-language models (VLMs) are emerging as powerful generalist tools for remote sensing, capable of integrating information across diverse tasks and enabling flexible, instruction-based interactions via a chat interface. In this work, we enhance VLM-based visual grounding in satellite imagery by proposing a novel structured localization mechanism. Our approach involves finetuning a pretrained VLM on a diverse set of instruction-following tasks, while interfacing a dedicated grounding module through specialized control tokens for localization. This method facilitates joint reasoning over both language and spatial information, significantly enhancing the model's ability to precisely localize objects in complex satellite scenes. We evaluate our framework on several remote sensing benchmarks, consistently improving the state-of-the-art, including a 24.8% relative improvement over previous methods on visual grounding. Our results highlight the benefits of integrating structured spatial reasoning into VLMs, paving the way for more reliable real-world satellite data analysis.", "link": "http://arxiv.org/abs/2512.08881v1", "date": "2025-12-09", "relevancy": 2.8585, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5748}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5701}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5701}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SATGround%3A%20A%20Spatially-Aware%20Approach%20for%20Visual%20Grounding%20in%20Remote%20Sensing&body=Title%3A%20SATGround%3A%20A%20Spatially-Aware%20Approach%20for%20Visual%20Grounding%20in%20Remote%20Sensing%0AAuthor%3A%20Aysim%20Toker%20and%20Andreea-Maria%20Oncescu%20and%20Roy%20Miles%20and%20Ismail%20Elezi%20and%20Jiankang%20Deng%0AAbstract%3A%20Vision-language%20models%20%28VLMs%29%20are%20emerging%20as%20powerful%20generalist%20tools%20for%20remote%20sensing%2C%20capable%20of%20integrating%20information%20across%20diverse%20tasks%20and%20enabling%20flexible%2C%20instruction-based%20interactions%20via%20a%20chat%20interface.%20In%20this%20work%2C%20we%20enhance%20VLM-based%20visual%20grounding%20in%20satellite%20imagery%20by%20proposing%20a%20novel%20structured%20localization%20mechanism.%20Our%20approach%20involves%20finetuning%20a%20pretrained%20VLM%20on%20a%20diverse%20set%20of%20instruction-following%20tasks%2C%20while%20interfacing%20a%20dedicated%20grounding%20module%20through%20specialized%20control%20tokens%20for%20localization.%20This%20method%20facilitates%20joint%20reasoning%20over%20both%20language%20and%20spatial%20information%2C%20significantly%20enhancing%20the%20model%27s%20ability%20to%20precisely%20localize%20objects%20in%20complex%20satellite%20scenes.%20We%20evaluate%20our%20framework%20on%20several%20remote%20sensing%20benchmarks%2C%20consistently%20improving%20the%20state-of-the-art%2C%20including%20a%2024.8%25%20relative%20improvement%20over%20previous%20methods%20on%20visual%20grounding.%20Our%20results%20highlight%20the%20benefits%20of%20integrating%20structured%20spatial%20reasoning%20into%20VLMs%2C%20paving%20the%20way%20for%20more%20reliable%20real-world%20satellite%20data%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08881v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSATGround%253A%2520A%2520Spatially-Aware%2520Approach%2520for%2520Visual%2520Grounding%2520in%2520Remote%2520Sensing%26entry.906535625%3DAysim%2520Toker%2520and%2520Andreea-Maria%2520Oncescu%2520and%2520Roy%2520Miles%2520and%2520Ismail%2520Elezi%2520and%2520Jiankang%2520Deng%26entry.1292438233%3DVision-language%2520models%2520%2528VLMs%2529%2520are%2520emerging%2520as%2520powerful%2520generalist%2520tools%2520for%2520remote%2520sensing%252C%2520capable%2520of%2520integrating%2520information%2520across%2520diverse%2520tasks%2520and%2520enabling%2520flexible%252C%2520instruction-based%2520interactions%2520via%2520a%2520chat%2520interface.%2520In%2520this%2520work%252C%2520we%2520enhance%2520VLM-based%2520visual%2520grounding%2520in%2520satellite%2520imagery%2520by%2520proposing%2520a%2520novel%2520structured%2520localization%2520mechanism.%2520Our%2520approach%2520involves%2520finetuning%2520a%2520pretrained%2520VLM%2520on%2520a%2520diverse%2520set%2520of%2520instruction-following%2520tasks%252C%2520while%2520interfacing%2520a%2520dedicated%2520grounding%2520module%2520through%2520specialized%2520control%2520tokens%2520for%2520localization.%2520This%2520method%2520facilitates%2520joint%2520reasoning%2520over%2520both%2520language%2520and%2520spatial%2520information%252C%2520significantly%2520enhancing%2520the%2520model%2527s%2520ability%2520to%2520precisely%2520localize%2520objects%2520in%2520complex%2520satellite%2520scenes.%2520We%2520evaluate%2520our%2520framework%2520on%2520several%2520remote%2520sensing%2520benchmarks%252C%2520consistently%2520improving%2520the%2520state-of-the-art%252C%2520including%2520a%252024.8%2525%2520relative%2520improvement%2520over%2520previous%2520methods%2520on%2520visual%2520grounding.%2520Our%2520results%2520highlight%2520the%2520benefits%2520of%2520integrating%2520structured%2520spatial%2520reasoning%2520into%2520VLMs%252C%2520paving%2520the%2520way%2520for%2520more%2520reliable%2520real-world%2520satellite%2520data%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08881v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SATGround%3A%20A%20Spatially-Aware%20Approach%20for%20Visual%20Grounding%20in%20Remote%20Sensing&entry.906535625=Aysim%20Toker%20and%20Andreea-Maria%20Oncescu%20and%20Roy%20Miles%20and%20Ismail%20Elezi%20and%20Jiankang%20Deng&entry.1292438233=Vision-language%20models%20%28VLMs%29%20are%20emerging%20as%20powerful%20generalist%20tools%20for%20remote%20sensing%2C%20capable%20of%20integrating%20information%20across%20diverse%20tasks%20and%20enabling%20flexible%2C%20instruction-based%20interactions%20via%20a%20chat%20interface.%20In%20this%20work%2C%20we%20enhance%20VLM-based%20visual%20grounding%20in%20satellite%20imagery%20by%20proposing%20a%20novel%20structured%20localization%20mechanism.%20Our%20approach%20involves%20finetuning%20a%20pretrained%20VLM%20on%20a%20diverse%20set%20of%20instruction-following%20tasks%2C%20while%20interfacing%20a%20dedicated%20grounding%20module%20through%20specialized%20control%20tokens%20for%20localization.%20This%20method%20facilitates%20joint%20reasoning%20over%20both%20language%20and%20spatial%20information%2C%20significantly%20enhancing%20the%20model%27s%20ability%20to%20precisely%20localize%20objects%20in%20complex%20satellite%20scenes.%20We%20evaluate%20our%20framework%20on%20several%20remote%20sensing%20benchmarks%2C%20consistently%20improving%20the%20state-of-the-art%2C%20including%20a%2024.8%25%20relative%20improvement%20over%20previous%20methods%20on%20visual%20grounding.%20Our%20results%20highlight%20the%20benefits%20of%20integrating%20structured%20spatial%20reasoning%20into%20VLMs%2C%20paving%20the%20way%20for%20more%20reliable%20real-world%20satellite%20data%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2512.08881v1&entry.124074799=Read"},
{"title": "The Missing Point in Vision Transformers for Universal Image Segmentation", "author": "Sajjad Shahabodini and Mobina Mansoori and Farnoush Bayatmakou and Jamshid Abouei and Konstantinos N. Plataniotis and Arash Mohammadi", "abstract": "Image segmentation remains a challenging task in computer vision, demanding robust mask generation and precise classification. Recent mask-based approaches yield high-quality masks by capturing global context. However, accurately classifying these masks, especially in the presence of ambiguous boundaries and imbalanced class distributions, remains an open challenge. In this work, we introduce ViT-P, a novel two-stage segmentation framework that decouples mask generation from classification. The first stage employs a proposal generator to produce class-agnostic mask proposals, while the second stage utilizes a point-based classification model built on the Vision Transformer (ViT) to refine predictions by focusing on mask central points. ViT-P serves as a pre-training-free adapter, allowing the integration of various pre-trained vision transformers without modifying their architecture, ensuring adaptability to dense prediction tasks. Furthermore, we demonstrate that coarse and bounding box annotations can effectively enhance classification without requiring additional training on fine annotation datasets, reducing annotation costs while maintaining strong performance. Extensive experiments across COCO, ADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving state-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4 mIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic segmentation. The code and pretrained models are available at: https://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.", "link": "http://arxiv.org/abs/2505.19795v2", "date": "2025-12-09", "relevancy": 2.8557, "topK": [{"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.582}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5708}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5606}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20The%20Missing%20Point%20in%20Vision%20Transformers%20for%20Universal%20Image%20Segmentation&body=Title%3A%20The%20Missing%20Point%20in%20Vision%20Transformers%20for%20Universal%20Image%20Segmentation%0AAuthor%3A%20Sajjad%20Shahabodini%20and%20Mobina%20Mansoori%20and%20Farnoush%20Bayatmakou%20and%20Jamshid%20Abouei%20and%20Konstantinos%20N.%20Plataniotis%20and%20Arash%20Mohammadi%0AAbstract%3A%20Image%20segmentation%20remains%20a%20challenging%20task%20in%20computer%20vision%2C%20demanding%20robust%20mask%20generation%20and%20precise%20classification.%20Recent%20mask-based%20approaches%20yield%20high-quality%20masks%20by%20capturing%20global%20context.%20However%2C%20accurately%20classifying%20these%20masks%2C%20especially%20in%20the%20presence%20of%20ambiguous%20boundaries%20and%20imbalanced%20class%20distributions%2C%20remains%20an%20open%20challenge.%20In%20this%20work%2C%20we%20introduce%20ViT-P%2C%20a%20novel%20two-stage%20segmentation%20framework%20that%20decouples%20mask%20generation%20from%20classification.%20The%20first%20stage%20employs%20a%20proposal%20generator%20to%20produce%20class-agnostic%20mask%20proposals%2C%20while%20the%20second%20stage%20utilizes%20a%20point-based%20classification%20model%20built%20on%20the%20Vision%20Transformer%20%28ViT%29%20to%20refine%20predictions%20by%20focusing%20on%20mask%20central%20points.%20ViT-P%20serves%20as%20a%20pre-training-free%20adapter%2C%20allowing%20the%20integration%20of%20various%20pre-trained%20vision%20transformers%20without%20modifying%20their%20architecture%2C%20ensuring%20adaptability%20to%20dense%20prediction%20tasks.%20Furthermore%2C%20we%20demonstrate%20that%20coarse%20and%20bounding%20box%20annotations%20can%20effectively%20enhance%20classification%20without%20requiring%20additional%20training%20on%20fine%20annotation%20datasets%2C%20reducing%20annotation%20costs%20while%20maintaining%20strong%20performance.%20Extensive%20experiments%20across%20COCO%2C%20ADE20K%2C%20and%20Cityscapes%20datasets%20validate%20the%20effectiveness%20of%20ViT-P%2C%20achieving%20state-of-the-art%20results%20with%2054.0%20PQ%20on%20ADE20K%20panoptic%20segmentation%2C%2087.4%20mIoU%20on%20Cityscapes%20semantic%20segmentation%2C%20and%2063.6%20mIoU%20on%20ADE20K%20semantic%20segmentation.%20The%20code%20and%20pretrained%20models%20are%20available%20at%3A%20https%3A//github.com/sajjad-sh33/ViT-P%7D%7Bhttps%3A//github.com/sajjad-sh33/ViT-P.%0ALink%3A%20http%3A//arxiv.org/abs/2505.19795v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DThe%2520Missing%2520Point%2520in%2520Vision%2520Transformers%2520for%2520Universal%2520Image%2520Segmentation%26entry.906535625%3DSajjad%2520Shahabodini%2520and%2520Mobina%2520Mansoori%2520and%2520Farnoush%2520Bayatmakou%2520and%2520Jamshid%2520Abouei%2520and%2520Konstantinos%2520N.%2520Plataniotis%2520and%2520Arash%2520Mohammadi%26entry.1292438233%3DImage%2520segmentation%2520remains%2520a%2520challenging%2520task%2520in%2520computer%2520vision%252C%2520demanding%2520robust%2520mask%2520generation%2520and%2520precise%2520classification.%2520Recent%2520mask-based%2520approaches%2520yield%2520high-quality%2520masks%2520by%2520capturing%2520global%2520context.%2520However%252C%2520accurately%2520classifying%2520these%2520masks%252C%2520especially%2520in%2520the%2520presence%2520of%2520ambiguous%2520boundaries%2520and%2520imbalanced%2520class%2520distributions%252C%2520remains%2520an%2520open%2520challenge.%2520In%2520this%2520work%252C%2520we%2520introduce%2520ViT-P%252C%2520a%2520novel%2520two-stage%2520segmentation%2520framework%2520that%2520decouples%2520mask%2520generation%2520from%2520classification.%2520The%2520first%2520stage%2520employs%2520a%2520proposal%2520generator%2520to%2520produce%2520class-agnostic%2520mask%2520proposals%252C%2520while%2520the%2520second%2520stage%2520utilizes%2520a%2520point-based%2520classification%2520model%2520built%2520on%2520the%2520Vision%2520Transformer%2520%2528ViT%2529%2520to%2520refine%2520predictions%2520by%2520focusing%2520on%2520mask%2520central%2520points.%2520ViT-P%2520serves%2520as%2520a%2520pre-training-free%2520adapter%252C%2520allowing%2520the%2520integration%2520of%2520various%2520pre-trained%2520vision%2520transformers%2520without%2520modifying%2520their%2520architecture%252C%2520ensuring%2520adaptability%2520to%2520dense%2520prediction%2520tasks.%2520Furthermore%252C%2520we%2520demonstrate%2520that%2520coarse%2520and%2520bounding%2520box%2520annotations%2520can%2520effectively%2520enhance%2520classification%2520without%2520requiring%2520additional%2520training%2520on%2520fine%2520annotation%2520datasets%252C%2520reducing%2520annotation%2520costs%2520while%2520maintaining%2520strong%2520performance.%2520Extensive%2520experiments%2520across%2520COCO%252C%2520ADE20K%252C%2520and%2520Cityscapes%2520datasets%2520validate%2520the%2520effectiveness%2520of%2520ViT-P%252C%2520achieving%2520state-of-the-art%2520results%2520with%252054.0%2520PQ%2520on%2520ADE20K%2520panoptic%2520segmentation%252C%252087.4%2520mIoU%2520on%2520Cityscapes%2520semantic%2520segmentation%252C%2520and%252063.6%2520mIoU%2520on%2520ADE20K%2520semantic%2520segmentation.%2520The%2520code%2520and%2520pretrained%2520models%2520are%2520available%2520at%253A%2520https%253A//github.com/sajjad-sh33/ViT-P%257D%257Bhttps%253A//github.com/sajjad-sh33/ViT-P.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.19795v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Missing%20Point%20in%20Vision%20Transformers%20for%20Universal%20Image%20Segmentation&entry.906535625=Sajjad%20Shahabodini%20and%20Mobina%20Mansoori%20and%20Farnoush%20Bayatmakou%20and%20Jamshid%20Abouei%20and%20Konstantinos%20N.%20Plataniotis%20and%20Arash%20Mohammadi&entry.1292438233=Image%20segmentation%20remains%20a%20challenging%20task%20in%20computer%20vision%2C%20demanding%20robust%20mask%20generation%20and%20precise%20classification.%20Recent%20mask-based%20approaches%20yield%20high-quality%20masks%20by%20capturing%20global%20context.%20However%2C%20accurately%20classifying%20these%20masks%2C%20especially%20in%20the%20presence%20of%20ambiguous%20boundaries%20and%20imbalanced%20class%20distributions%2C%20remains%20an%20open%20challenge.%20In%20this%20work%2C%20we%20introduce%20ViT-P%2C%20a%20novel%20two-stage%20segmentation%20framework%20that%20decouples%20mask%20generation%20from%20classification.%20The%20first%20stage%20employs%20a%20proposal%20generator%20to%20produce%20class-agnostic%20mask%20proposals%2C%20while%20the%20second%20stage%20utilizes%20a%20point-based%20classification%20model%20built%20on%20the%20Vision%20Transformer%20%28ViT%29%20to%20refine%20predictions%20by%20focusing%20on%20mask%20central%20points.%20ViT-P%20serves%20as%20a%20pre-training-free%20adapter%2C%20allowing%20the%20integration%20of%20various%20pre-trained%20vision%20transformers%20without%20modifying%20their%20architecture%2C%20ensuring%20adaptability%20to%20dense%20prediction%20tasks.%20Furthermore%2C%20we%20demonstrate%20that%20coarse%20and%20bounding%20box%20annotations%20can%20effectively%20enhance%20classification%20without%20requiring%20additional%20training%20on%20fine%20annotation%20datasets%2C%20reducing%20annotation%20costs%20while%20maintaining%20strong%20performance.%20Extensive%20experiments%20across%20COCO%2C%20ADE20K%2C%20and%20Cityscapes%20datasets%20validate%20the%20effectiveness%20of%20ViT-P%2C%20achieving%20state-of-the-art%20results%20with%2054.0%20PQ%20on%20ADE20K%20panoptic%20segmentation%2C%2087.4%20mIoU%20on%20Cityscapes%20semantic%20segmentation%2C%20and%2063.6%20mIoU%20on%20ADE20K%20semantic%20segmentation.%20The%20code%20and%20pretrained%20models%20are%20available%20at%3A%20https%3A//github.com/sajjad-sh33/ViT-P%7D%7Bhttps%3A//github.com/sajjad-sh33/ViT-P.&entry.1838667208=http%3A//arxiv.org/abs/2505.19795v2&entry.124074799=Read"},
{"title": "Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects", "author": "Georgios Kouros and Minye Wu and Tinne Tuytelaars", "abstract": "Accurate reconstruction and relighting of glossy objects remains a longstanding challenge, as object shape, material properties, and illumination are inherently difficult to disentangle. Existing neural rendering approaches often rely on simplified BRDF models or parameterizations that couple diffuse and specular components, which restrict faithful material recovery and limit relighting fidelity. We propose a relightable framework that integrates a microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian Splatting with deferred shading. This formulation enables more physically consistent material decomposition, while diffusion-based priors for surface normals and diffuse color guide early-stage optimization and mitigate ambiguity. A coarse-to-fine environment map optimization accelerates convergence, and negative-only environment map clipping preserves high-dynamic-range specular reflections. Extensive experiments on complex, glossy scenes demonstrate that our method achieves high-quality geometry and material reconstruction, delivering substantially more realistic and consistent relighting under novel illumination compared to existing Gaussian splatting methods.", "link": "http://arxiv.org/abs/2510.02069v2", "date": "2025-12-09", "relevancy": 2.8502, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6076}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5571}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5454}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spec-Gloss%20Surfels%20and%20Normal-Diffuse%20Priors%20for%20Relightable%20Glossy%20Objects&body=Title%3A%20Spec-Gloss%20Surfels%20and%20Normal-Diffuse%20Priors%20for%20Relightable%20Glossy%20Objects%0AAuthor%3A%20Georgios%20Kouros%20and%20Minye%20Wu%20and%20Tinne%20Tuytelaars%0AAbstract%3A%20Accurate%20reconstruction%20and%20relighting%20of%20glossy%20objects%20remains%20a%20longstanding%20challenge%2C%20as%20object%20shape%2C%20material%20properties%2C%20and%20illumination%20are%20inherently%20difficult%20to%20disentangle.%20Existing%20neural%20rendering%20approaches%20often%20rely%20on%20simplified%20BRDF%20models%20or%20parameterizations%20that%20couple%20diffuse%20and%20specular%20components%2C%20which%20restrict%20faithful%20material%20recovery%20and%20limit%20relighting%20fidelity.%20We%20propose%20a%20relightable%20framework%20that%20integrates%20a%20microfacet%20BRDF%20with%20the%20specular-glossiness%20parameterization%20into%202D%20Gaussian%20Splatting%20with%20deferred%20shading.%20This%20formulation%20enables%20more%20physically%20consistent%20material%20decomposition%2C%20while%20diffusion-based%20priors%20for%20surface%20normals%20and%20diffuse%20color%20guide%20early-stage%20optimization%20and%20mitigate%20ambiguity.%20A%20coarse-to-fine%20environment%20map%20optimization%20accelerates%20convergence%2C%20and%20negative-only%20environment%20map%20clipping%20preserves%20high-dynamic-range%20specular%20reflections.%20Extensive%20experiments%20on%20complex%2C%20glossy%20scenes%20demonstrate%20that%20our%20method%20achieves%20high-quality%20geometry%20and%20material%20reconstruction%2C%20delivering%20substantially%20more%20realistic%20and%20consistent%20relighting%20under%20novel%20illumination%20compared%20to%20existing%20Gaussian%20splatting%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2510.02069v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpec-Gloss%2520Surfels%2520and%2520Normal-Diffuse%2520Priors%2520for%2520Relightable%2520Glossy%2520Objects%26entry.906535625%3DGeorgios%2520Kouros%2520and%2520Minye%2520Wu%2520and%2520Tinne%2520Tuytelaars%26entry.1292438233%3DAccurate%2520reconstruction%2520and%2520relighting%2520of%2520glossy%2520objects%2520remains%2520a%2520longstanding%2520challenge%252C%2520as%2520object%2520shape%252C%2520material%2520properties%252C%2520and%2520illumination%2520are%2520inherently%2520difficult%2520to%2520disentangle.%2520Existing%2520neural%2520rendering%2520approaches%2520often%2520rely%2520on%2520simplified%2520BRDF%2520models%2520or%2520parameterizations%2520that%2520couple%2520diffuse%2520and%2520specular%2520components%252C%2520which%2520restrict%2520faithful%2520material%2520recovery%2520and%2520limit%2520relighting%2520fidelity.%2520We%2520propose%2520a%2520relightable%2520framework%2520that%2520integrates%2520a%2520microfacet%2520BRDF%2520with%2520the%2520specular-glossiness%2520parameterization%2520into%25202D%2520Gaussian%2520Splatting%2520with%2520deferred%2520shading.%2520This%2520formulation%2520enables%2520more%2520physically%2520consistent%2520material%2520decomposition%252C%2520while%2520diffusion-based%2520priors%2520for%2520surface%2520normals%2520and%2520diffuse%2520color%2520guide%2520early-stage%2520optimization%2520and%2520mitigate%2520ambiguity.%2520A%2520coarse-to-fine%2520environment%2520map%2520optimization%2520accelerates%2520convergence%252C%2520and%2520negative-only%2520environment%2520map%2520clipping%2520preserves%2520high-dynamic-range%2520specular%2520reflections.%2520Extensive%2520experiments%2520on%2520complex%252C%2520glossy%2520scenes%2520demonstrate%2520that%2520our%2520method%2520achieves%2520high-quality%2520geometry%2520and%2520material%2520reconstruction%252C%2520delivering%2520substantially%2520more%2520realistic%2520and%2520consistent%2520relighting%2520under%2520novel%2520illumination%2520compared%2520to%2520existing%2520Gaussian%2520splatting%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.02069v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spec-Gloss%20Surfels%20and%20Normal-Diffuse%20Priors%20for%20Relightable%20Glossy%20Objects&entry.906535625=Georgios%20Kouros%20and%20Minye%20Wu%20and%20Tinne%20Tuytelaars&entry.1292438233=Accurate%20reconstruction%20and%20relighting%20of%20glossy%20objects%20remains%20a%20longstanding%20challenge%2C%20as%20object%20shape%2C%20material%20properties%2C%20and%20illumination%20are%20inherently%20difficult%20to%20disentangle.%20Existing%20neural%20rendering%20approaches%20often%20rely%20on%20simplified%20BRDF%20models%20or%20parameterizations%20that%20couple%20diffuse%20and%20specular%20components%2C%20which%20restrict%20faithful%20material%20recovery%20and%20limit%20relighting%20fidelity.%20We%20propose%20a%20relightable%20framework%20that%20integrates%20a%20microfacet%20BRDF%20with%20the%20specular-glossiness%20parameterization%20into%202D%20Gaussian%20Splatting%20with%20deferred%20shading.%20This%20formulation%20enables%20more%20physically%20consistent%20material%20decomposition%2C%20while%20diffusion-based%20priors%20for%20surface%20normals%20and%20diffuse%20color%20guide%20early-stage%20optimization%20and%20mitigate%20ambiguity.%20A%20coarse-to-fine%20environment%20map%20optimization%20accelerates%20convergence%2C%20and%20negative-only%20environment%20map%20clipping%20preserves%20high-dynamic-range%20specular%20reflections.%20Extensive%20experiments%20on%20complex%2C%20glossy%20scenes%20demonstrate%20that%20our%20method%20achieves%20high-quality%20geometry%20and%20material%20reconstruction%2C%20delivering%20substantially%20more%20realistic%20and%20consistent%20relighting%20under%20novel%20illumination%20compared%20to%20existing%20Gaussian%20splatting%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2510.02069v2&entry.124074799=Read"},
{"title": "EMMA: Efficient Multimodal Understanding, Generation, and Editing with a Unified Architecture", "author": "Xin He and Longhui Wei and Jianbo Ouyang and Minghui Liao and Lingxi Xie and Qi Tian", "abstract": "We propose EMMA, an efficient and unified architecture for multimodal understanding, generation and editing. Specifically, EMMA primarily consists of 1) An efficient autoencoder with a 32x compression ratio, which significantly reduces the number of tokens required for generation. This also ensures the training balance between understanding and generation tasks by applying the same compression ratio to images. 2) Channel-wise concatenation instead of token-wise concatenation among visual understanding and generation tokens, which further reduces the visual tokens in unified architectures. 3) A shared-and-decoupled network that enables mutual improvements across tasks while meeting the task-specific modeling requirements. 4) A mixture-of-experts mechanism adopted for visual understanding encoder, which substantially improves perceptual capabilities with a few parameters increase. Extensive experiments have shown that EMMA-4B can significantly outperform state-of-the-art unified multimodal approaches (e.g., BAGEL-7B) in both efficiency and performance, while also achieving competitive results compared to recent multimodal understanding and generation experts (e.g., Qwen3-VL and Qwen-Image). We believe that EMMA lays a solid foundation for the future development of unified multimodal architectures.", "link": "http://arxiv.org/abs/2512.04810v4", "date": "2025-12-09", "relevancy": 2.8288, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5774}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5599}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5599}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20EMMA%3A%20Efficient%20Multimodal%20Understanding%2C%20Generation%2C%20and%20Editing%20with%20a%20Unified%20Architecture&body=Title%3A%20EMMA%3A%20Efficient%20Multimodal%20Understanding%2C%20Generation%2C%20and%20Editing%20with%20a%20Unified%20Architecture%0AAuthor%3A%20Xin%20He%20and%20Longhui%20Wei%20and%20Jianbo%20Ouyang%20and%20Minghui%20Liao%20and%20Lingxi%20Xie%20and%20Qi%20Tian%0AAbstract%3A%20We%20propose%20EMMA%2C%20an%20efficient%20and%20unified%20architecture%20for%20multimodal%20understanding%2C%20generation%20and%20editing.%20Specifically%2C%20EMMA%20primarily%20consists%20of%201%29%20An%20efficient%20autoencoder%20with%20a%2032x%20compression%20ratio%2C%20which%20significantly%20reduces%20the%20number%20of%20tokens%20required%20for%20generation.%20This%20also%20ensures%20the%20training%20balance%20between%20understanding%20and%20generation%20tasks%20by%20applying%20the%20same%20compression%20ratio%20to%20images.%202%29%20Channel-wise%20concatenation%20instead%20of%20token-wise%20concatenation%20among%20visual%20understanding%20and%20generation%20tokens%2C%20which%20further%20reduces%20the%20visual%20tokens%20in%20unified%20architectures.%203%29%20A%20shared-and-decoupled%20network%20that%20enables%20mutual%20improvements%20across%20tasks%20while%20meeting%20the%20task-specific%20modeling%20requirements.%204%29%20A%20mixture-of-experts%20mechanism%20adopted%20for%20visual%20understanding%20encoder%2C%20which%20substantially%20improves%20perceptual%20capabilities%20with%20a%20few%20parameters%20increase.%20Extensive%20experiments%20have%20shown%20that%20EMMA-4B%20can%20significantly%20outperform%20state-of-the-art%20unified%20multimodal%20approaches%20%28e.g.%2C%20BAGEL-7B%29%20in%20both%20efficiency%20and%20performance%2C%20while%20also%20achieving%20competitive%20results%20compared%20to%20recent%20multimodal%20understanding%20and%20generation%20experts%20%28e.g.%2C%20Qwen3-VL%20and%20Qwen-Image%29.%20We%20believe%20that%20EMMA%20lays%20a%20solid%20foundation%20for%20the%20future%20development%20of%20unified%20multimodal%20architectures.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04810v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEMMA%253A%2520Efficient%2520Multimodal%2520Understanding%252C%2520Generation%252C%2520and%2520Editing%2520with%2520a%2520Unified%2520Architecture%26entry.906535625%3DXin%2520He%2520and%2520Longhui%2520Wei%2520and%2520Jianbo%2520Ouyang%2520and%2520Minghui%2520Liao%2520and%2520Lingxi%2520Xie%2520and%2520Qi%2520Tian%26entry.1292438233%3DWe%2520propose%2520EMMA%252C%2520an%2520efficient%2520and%2520unified%2520architecture%2520for%2520multimodal%2520understanding%252C%2520generation%2520and%2520editing.%2520Specifically%252C%2520EMMA%2520primarily%2520consists%2520of%25201%2529%2520An%2520efficient%2520autoencoder%2520with%2520a%252032x%2520compression%2520ratio%252C%2520which%2520significantly%2520reduces%2520the%2520number%2520of%2520tokens%2520required%2520for%2520generation.%2520This%2520also%2520ensures%2520the%2520training%2520balance%2520between%2520understanding%2520and%2520generation%2520tasks%2520by%2520applying%2520the%2520same%2520compression%2520ratio%2520to%2520images.%25202%2529%2520Channel-wise%2520concatenation%2520instead%2520of%2520token-wise%2520concatenation%2520among%2520visual%2520understanding%2520and%2520generation%2520tokens%252C%2520which%2520further%2520reduces%2520the%2520visual%2520tokens%2520in%2520unified%2520architectures.%25203%2529%2520A%2520shared-and-decoupled%2520network%2520that%2520enables%2520mutual%2520improvements%2520across%2520tasks%2520while%2520meeting%2520the%2520task-specific%2520modeling%2520requirements.%25204%2529%2520A%2520mixture-of-experts%2520mechanism%2520adopted%2520for%2520visual%2520understanding%2520encoder%252C%2520which%2520substantially%2520improves%2520perceptual%2520capabilities%2520with%2520a%2520few%2520parameters%2520increase.%2520Extensive%2520experiments%2520have%2520shown%2520that%2520EMMA-4B%2520can%2520significantly%2520outperform%2520state-of-the-art%2520unified%2520multimodal%2520approaches%2520%2528e.g.%252C%2520BAGEL-7B%2529%2520in%2520both%2520efficiency%2520and%2520performance%252C%2520while%2520also%2520achieving%2520competitive%2520results%2520compared%2520to%2520recent%2520multimodal%2520understanding%2520and%2520generation%2520experts%2520%2528e.g.%252C%2520Qwen3-VL%2520and%2520Qwen-Image%2529.%2520We%2520believe%2520that%2520EMMA%2520lays%2520a%2520solid%2520foundation%2520for%2520the%2520future%2520development%2520of%2520unified%2520multimodal%2520architectures.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04810v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=EMMA%3A%20Efficient%20Multimodal%20Understanding%2C%20Generation%2C%20and%20Editing%20with%20a%20Unified%20Architecture&entry.906535625=Xin%20He%20and%20Longhui%20Wei%20and%20Jianbo%20Ouyang%20and%20Minghui%20Liao%20and%20Lingxi%20Xie%20and%20Qi%20Tian&entry.1292438233=We%20propose%20EMMA%2C%20an%20efficient%20and%20unified%20architecture%20for%20multimodal%20understanding%2C%20generation%20and%20editing.%20Specifically%2C%20EMMA%20primarily%20consists%20of%201%29%20An%20efficient%20autoencoder%20with%20a%2032x%20compression%20ratio%2C%20which%20significantly%20reduces%20the%20number%20of%20tokens%20required%20for%20generation.%20This%20also%20ensures%20the%20training%20balance%20between%20understanding%20and%20generation%20tasks%20by%20applying%20the%20same%20compression%20ratio%20to%20images.%202%29%20Channel-wise%20concatenation%20instead%20of%20token-wise%20concatenation%20among%20visual%20understanding%20and%20generation%20tokens%2C%20which%20further%20reduces%20the%20visual%20tokens%20in%20unified%20architectures.%203%29%20A%20shared-and-decoupled%20network%20that%20enables%20mutual%20improvements%20across%20tasks%20while%20meeting%20the%20task-specific%20modeling%20requirements.%204%29%20A%20mixture-of-experts%20mechanism%20adopted%20for%20visual%20understanding%20encoder%2C%20which%20substantially%20improves%20perceptual%20capabilities%20with%20a%20few%20parameters%20increase.%20Extensive%20experiments%20have%20shown%20that%20EMMA-4B%20can%20significantly%20outperform%20state-of-the-art%20unified%20multimodal%20approaches%20%28e.g.%2C%20BAGEL-7B%29%20in%20both%20efficiency%20and%20performance%2C%20while%20also%20achieving%20competitive%20results%20compared%20to%20recent%20multimodal%20understanding%20and%20generation%20experts%20%28e.g.%2C%20Qwen3-VL%20and%20Qwen-Image%29.%20We%20believe%20that%20EMMA%20lays%20a%20solid%20foundation%20for%20the%20future%20development%20of%20unified%20multimodal%20architectures.&entry.1838667208=http%3A//arxiv.org/abs/2512.04810v4&entry.124074799=Read"},
{"title": "SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images", "author": "Kaiyu Li and Shengqi Zhang and Yupeng Deng and Zhi Wang and Deyu Meng and Xiangyong Cao", "abstract": "Most existing methods for training-free Open-Vocabulary Semantic Segmentation (OVSS) are based on CLIP. While these approaches have made progress, they often face challenges in precise localization or require complex pipelines to combine separate modules, especially in remote sensing scenarios where numerous dense and small targets are present. Recently, Segment Anything Model 3 (SAM 3) was proposed, unifying segmentation and recognition in a promptable framework. In this paper, we present a preliminary exploration of applying SAM 3 to the remote sensing OVSS task without any training. First, we implement a mask fusion strategy that combines the outputs from SAM 3's semantic segmentation head and the Transformer decoder (instance head). This allows us to leverage the strengths of both heads for better land coverage. Second, we utilize the presence score from the presence head to filter out categories that do not exist in the scene, reducing false positives caused by the vast vocabulary sizes and patch-level processing in geospatial scenes. We evaluate our method on extensive remote sensing datasets. Experiments show that this simple adaptation achieves promising performance, demonstrating the potential of SAM 3 for remote sensing OVSS. Our code is released at https://github.com/earth-insights/SegEarth-OV-3.", "link": "http://arxiv.org/abs/2512.08730v1", "date": "2025-12-09", "relevancy": 2.8067, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5641}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5641}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5559}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SegEarth-OV3%3A%20Exploring%20SAM%203%20for%20Open-Vocabulary%20Semantic%20Segmentation%20in%20Remote%20Sensing%20Images&body=Title%3A%20SegEarth-OV3%3A%20Exploring%20SAM%203%20for%20Open-Vocabulary%20Semantic%20Segmentation%20in%20Remote%20Sensing%20Images%0AAuthor%3A%20Kaiyu%20Li%20and%20Shengqi%20Zhang%20and%20Yupeng%20Deng%20and%20Zhi%20Wang%20and%20Deyu%20Meng%20and%20Xiangyong%20Cao%0AAbstract%3A%20Most%20existing%20methods%20for%20training-free%20Open-Vocabulary%20Semantic%20Segmentation%20%28OVSS%29%20are%20based%20on%20CLIP.%20While%20these%20approaches%20have%20made%20progress%2C%20they%20often%20face%20challenges%20in%20precise%20localization%20or%20require%20complex%20pipelines%20to%20combine%20separate%20modules%2C%20especially%20in%20remote%20sensing%20scenarios%20where%20numerous%20dense%20and%20small%20targets%20are%20present.%20Recently%2C%20Segment%20Anything%20Model%203%20%28SAM%203%29%20was%20proposed%2C%20unifying%20segmentation%20and%20recognition%20in%20a%20promptable%20framework.%20In%20this%20paper%2C%20we%20present%20a%20preliminary%20exploration%20of%20applying%20SAM%203%20to%20the%20remote%20sensing%20OVSS%20task%20without%20any%20training.%20First%2C%20we%20implement%20a%20mask%20fusion%20strategy%20that%20combines%20the%20outputs%20from%20SAM%203%27s%20semantic%20segmentation%20head%20and%20the%20Transformer%20decoder%20%28instance%20head%29.%20This%20allows%20us%20to%20leverage%20the%20strengths%20of%20both%20heads%20for%20better%20land%20coverage.%20Second%2C%20we%20utilize%20the%20presence%20score%20from%20the%20presence%20head%20to%20filter%20out%20categories%20that%20do%20not%20exist%20in%20the%20scene%2C%20reducing%20false%20positives%20caused%20by%20the%20vast%20vocabulary%20sizes%20and%20patch-level%20processing%20in%20geospatial%20scenes.%20We%20evaluate%20our%20method%20on%20extensive%20remote%20sensing%20datasets.%20Experiments%20show%20that%20this%20simple%20adaptation%20achieves%20promising%20performance%2C%20demonstrating%20the%20potential%20of%20SAM%203%20for%20remote%20sensing%20OVSS.%20Our%20code%20is%20released%20at%20https%3A//github.com/earth-insights/SegEarth-OV-3.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08730v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSegEarth-OV3%253A%2520Exploring%2520SAM%25203%2520for%2520Open-Vocabulary%2520Semantic%2520Segmentation%2520in%2520Remote%2520Sensing%2520Images%26entry.906535625%3DKaiyu%2520Li%2520and%2520Shengqi%2520Zhang%2520and%2520Yupeng%2520Deng%2520and%2520Zhi%2520Wang%2520and%2520Deyu%2520Meng%2520and%2520Xiangyong%2520Cao%26entry.1292438233%3DMost%2520existing%2520methods%2520for%2520training-free%2520Open-Vocabulary%2520Semantic%2520Segmentation%2520%2528OVSS%2529%2520are%2520based%2520on%2520CLIP.%2520While%2520these%2520approaches%2520have%2520made%2520progress%252C%2520they%2520often%2520face%2520challenges%2520in%2520precise%2520localization%2520or%2520require%2520complex%2520pipelines%2520to%2520combine%2520separate%2520modules%252C%2520especially%2520in%2520remote%2520sensing%2520scenarios%2520where%2520numerous%2520dense%2520and%2520small%2520targets%2520are%2520present.%2520Recently%252C%2520Segment%2520Anything%2520Model%25203%2520%2528SAM%25203%2529%2520was%2520proposed%252C%2520unifying%2520segmentation%2520and%2520recognition%2520in%2520a%2520promptable%2520framework.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520preliminary%2520exploration%2520of%2520applying%2520SAM%25203%2520to%2520the%2520remote%2520sensing%2520OVSS%2520task%2520without%2520any%2520training.%2520First%252C%2520we%2520implement%2520a%2520mask%2520fusion%2520strategy%2520that%2520combines%2520the%2520outputs%2520from%2520SAM%25203%2527s%2520semantic%2520segmentation%2520head%2520and%2520the%2520Transformer%2520decoder%2520%2528instance%2520head%2529.%2520This%2520allows%2520us%2520to%2520leverage%2520the%2520strengths%2520of%2520both%2520heads%2520for%2520better%2520land%2520coverage.%2520Second%252C%2520we%2520utilize%2520the%2520presence%2520score%2520from%2520the%2520presence%2520head%2520to%2520filter%2520out%2520categories%2520that%2520do%2520not%2520exist%2520in%2520the%2520scene%252C%2520reducing%2520false%2520positives%2520caused%2520by%2520the%2520vast%2520vocabulary%2520sizes%2520and%2520patch-level%2520processing%2520in%2520geospatial%2520scenes.%2520We%2520evaluate%2520our%2520method%2520on%2520extensive%2520remote%2520sensing%2520datasets.%2520Experiments%2520show%2520that%2520this%2520simple%2520adaptation%2520achieves%2520promising%2520performance%252C%2520demonstrating%2520the%2520potential%2520of%2520SAM%25203%2520for%2520remote%2520sensing%2520OVSS.%2520Our%2520code%2520is%2520released%2520at%2520https%253A//github.com/earth-insights/SegEarth-OV-3.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08730v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SegEarth-OV3%3A%20Exploring%20SAM%203%20for%20Open-Vocabulary%20Semantic%20Segmentation%20in%20Remote%20Sensing%20Images&entry.906535625=Kaiyu%20Li%20and%20Shengqi%20Zhang%20and%20Yupeng%20Deng%20and%20Zhi%20Wang%20and%20Deyu%20Meng%20and%20Xiangyong%20Cao&entry.1292438233=Most%20existing%20methods%20for%20training-free%20Open-Vocabulary%20Semantic%20Segmentation%20%28OVSS%29%20are%20based%20on%20CLIP.%20While%20these%20approaches%20have%20made%20progress%2C%20they%20often%20face%20challenges%20in%20precise%20localization%20or%20require%20complex%20pipelines%20to%20combine%20separate%20modules%2C%20especially%20in%20remote%20sensing%20scenarios%20where%20numerous%20dense%20and%20small%20targets%20are%20present.%20Recently%2C%20Segment%20Anything%20Model%203%20%28SAM%203%29%20was%20proposed%2C%20unifying%20segmentation%20and%20recognition%20in%20a%20promptable%20framework.%20In%20this%20paper%2C%20we%20present%20a%20preliminary%20exploration%20of%20applying%20SAM%203%20to%20the%20remote%20sensing%20OVSS%20task%20without%20any%20training.%20First%2C%20we%20implement%20a%20mask%20fusion%20strategy%20that%20combines%20the%20outputs%20from%20SAM%203%27s%20semantic%20segmentation%20head%20and%20the%20Transformer%20decoder%20%28instance%20head%29.%20This%20allows%20us%20to%20leverage%20the%20strengths%20of%20both%20heads%20for%20better%20land%20coverage.%20Second%2C%20we%20utilize%20the%20presence%20score%20from%20the%20presence%20head%20to%20filter%20out%20categories%20that%20do%20not%20exist%20in%20the%20scene%2C%20reducing%20false%20positives%20caused%20by%20the%20vast%20vocabulary%20sizes%20and%20patch-level%20processing%20in%20geospatial%20scenes.%20We%20evaluate%20our%20method%20on%20extensive%20remote%20sensing%20datasets.%20Experiments%20show%20that%20this%20simple%20adaptation%20achieves%20promising%20performance%2C%20demonstrating%20the%20potential%20of%20SAM%203%20for%20remote%20sensing%20OVSS.%20Our%20code%20is%20released%20at%20https%3A//github.com/earth-insights/SegEarth-OV-3.&entry.1838667208=http%3A//arxiv.org/abs/2512.08730v1&entry.124074799=Read"},
{"title": "Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference", "author": "Amit Bendkhale", "abstract": "Verifiable geometric reasoning is a critical component for trustworthy and controllable agentic AI. Despite impressive capabilities, Vision-Language Models (VLMs) often fail under realistic scene changes. We present Tri-Bench, a compact benchmark of planar triangle problems that isolates relative geometric reasoning while stressing two deployment-critical factors: camera pose (planar vs. tilted) and scene context via object interference (10 everyday objects). To test verifiability and control, we evaluate four recent VLMs using a single, fixed prompt whose guardrail explicitly describes a surrounding square border, enabling correct answers via homography. We evaluate six simple tasks over binary and continuous targets, and observe that the overall accuracy with respect to 3D ground truth is modest, ~69% on average (best ~75%, worst ~64%). The same responses align even more closely with 2D projections in the image plane, where mean accuracy is ~72%. All four VLMs consistently fail, with accuracy falling to ~0%, on recognizing minority shape classes (equilateral, isosceles, right-angled triangles). Additionally, overall VLM accuracy degrades by ~4.1% under camera tilt. This demonstrates that models fail to correctly utilize the explicit frame-of-reference hint provided in the prompt and default to 2D image plane cues. Finally, we find that object interference has no significant effect on VLM accuracy.", "link": "http://arxiv.org/abs/2512.08860v1", "date": "2025-12-09", "relevancy": 2.7888, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5692}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5692}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5348}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Tri-Bench%3A%20Stress-Testing%20VLM%20Reliability%20on%20Spatial%20Reasoning%20under%20Camera%20Tilt%20and%20Object%20Interference&body=Title%3A%20Tri-Bench%3A%20Stress-Testing%20VLM%20Reliability%20on%20Spatial%20Reasoning%20under%20Camera%20Tilt%20and%20Object%20Interference%0AAuthor%3A%20Amit%20Bendkhale%0AAbstract%3A%20Verifiable%20geometric%20reasoning%20is%20a%20critical%20component%20for%20trustworthy%20and%20controllable%20agentic%20AI.%20Despite%20impressive%20capabilities%2C%20Vision-Language%20Models%20%28VLMs%29%20often%20fail%20under%20realistic%20scene%20changes.%20We%20present%20Tri-Bench%2C%20a%20compact%20benchmark%20of%20planar%20triangle%20problems%20that%20isolates%20relative%20geometric%20reasoning%20while%20stressing%20two%20deployment-critical%20factors%3A%20camera%20pose%20%28planar%20vs.%20tilted%29%20and%20scene%20context%20via%20object%20interference%20%2810%20everyday%20objects%29.%20To%20test%20verifiability%20and%20control%2C%20we%20evaluate%20four%20recent%20VLMs%20using%20a%20single%2C%20fixed%20prompt%20whose%20guardrail%20explicitly%20describes%20a%20surrounding%20square%20border%2C%20enabling%20correct%20answers%20via%20homography.%20We%20evaluate%20six%20simple%20tasks%20over%20binary%20and%20continuous%20targets%2C%20and%20observe%20that%20the%20overall%20accuracy%20with%20respect%20to%203D%20ground%20truth%20is%20modest%2C%20~69%25%20on%20average%20%28best%20~75%25%2C%20worst%20~64%25%29.%20The%20same%20responses%20align%20even%20more%20closely%20with%202D%20projections%20in%20the%20image%20plane%2C%20where%20mean%20accuracy%20is%20~72%25.%20All%20four%20VLMs%20consistently%20fail%2C%20with%20accuracy%20falling%20to%20~0%25%2C%20on%20recognizing%20minority%20shape%20classes%20%28equilateral%2C%20isosceles%2C%20right-angled%20triangles%29.%20Additionally%2C%20overall%20VLM%20accuracy%20degrades%20by%20~4.1%25%20under%20camera%20tilt.%20This%20demonstrates%20that%20models%20fail%20to%20correctly%20utilize%20the%20explicit%20frame-of-reference%20hint%20provided%20in%20the%20prompt%20and%20default%20to%202D%20image%20plane%20cues.%20Finally%2C%20we%20find%20that%20object%20interference%20has%20no%20significant%20effect%20on%20VLM%20accuracy.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08860v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTri-Bench%253A%2520Stress-Testing%2520VLM%2520Reliability%2520on%2520Spatial%2520Reasoning%2520under%2520Camera%2520Tilt%2520and%2520Object%2520Interference%26entry.906535625%3DAmit%2520Bendkhale%26entry.1292438233%3DVerifiable%2520geometric%2520reasoning%2520is%2520a%2520critical%2520component%2520for%2520trustworthy%2520and%2520controllable%2520agentic%2520AI.%2520Despite%2520impressive%2520capabilities%252C%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520often%2520fail%2520under%2520realistic%2520scene%2520changes.%2520We%2520present%2520Tri-Bench%252C%2520a%2520compact%2520benchmark%2520of%2520planar%2520triangle%2520problems%2520that%2520isolates%2520relative%2520geometric%2520reasoning%2520while%2520stressing%2520two%2520deployment-critical%2520factors%253A%2520camera%2520pose%2520%2528planar%2520vs.%2520tilted%2529%2520and%2520scene%2520context%2520via%2520object%2520interference%2520%252810%2520everyday%2520objects%2529.%2520To%2520test%2520verifiability%2520and%2520control%252C%2520we%2520evaluate%2520four%2520recent%2520VLMs%2520using%2520a%2520single%252C%2520fixed%2520prompt%2520whose%2520guardrail%2520explicitly%2520describes%2520a%2520surrounding%2520square%2520border%252C%2520enabling%2520correct%2520answers%2520via%2520homography.%2520We%2520evaluate%2520six%2520simple%2520tasks%2520over%2520binary%2520and%2520continuous%2520targets%252C%2520and%2520observe%2520that%2520the%2520overall%2520accuracy%2520with%2520respect%2520to%25203D%2520ground%2520truth%2520is%2520modest%252C%2520~69%2525%2520on%2520average%2520%2528best%2520~75%2525%252C%2520worst%2520~64%2525%2529.%2520The%2520same%2520responses%2520align%2520even%2520more%2520closely%2520with%25202D%2520projections%2520in%2520the%2520image%2520plane%252C%2520where%2520mean%2520accuracy%2520is%2520~72%2525.%2520All%2520four%2520VLMs%2520consistently%2520fail%252C%2520with%2520accuracy%2520falling%2520to%2520~0%2525%252C%2520on%2520recognizing%2520minority%2520shape%2520classes%2520%2528equilateral%252C%2520isosceles%252C%2520right-angled%2520triangles%2529.%2520Additionally%252C%2520overall%2520VLM%2520accuracy%2520degrades%2520by%2520~4.1%2525%2520under%2520camera%2520tilt.%2520This%2520demonstrates%2520that%2520models%2520fail%2520to%2520correctly%2520utilize%2520the%2520explicit%2520frame-of-reference%2520hint%2520provided%2520in%2520the%2520prompt%2520and%2520default%2520to%25202D%2520image%2520plane%2520cues.%2520Finally%252C%2520we%2520find%2520that%2520object%2520interference%2520has%2520no%2520significant%2520effect%2520on%2520VLM%2520accuracy.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08860v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Tri-Bench%3A%20Stress-Testing%20VLM%20Reliability%20on%20Spatial%20Reasoning%20under%20Camera%20Tilt%20and%20Object%20Interference&entry.906535625=Amit%20Bendkhale&entry.1292438233=Verifiable%20geometric%20reasoning%20is%20a%20critical%20component%20for%20trustworthy%20and%20controllable%20agentic%20AI.%20Despite%20impressive%20capabilities%2C%20Vision-Language%20Models%20%28VLMs%29%20often%20fail%20under%20realistic%20scene%20changes.%20We%20present%20Tri-Bench%2C%20a%20compact%20benchmark%20of%20planar%20triangle%20problems%20that%20isolates%20relative%20geometric%20reasoning%20while%20stressing%20two%20deployment-critical%20factors%3A%20camera%20pose%20%28planar%20vs.%20tilted%29%20and%20scene%20context%20via%20object%20interference%20%2810%20everyday%20objects%29.%20To%20test%20verifiability%20and%20control%2C%20we%20evaluate%20four%20recent%20VLMs%20using%20a%20single%2C%20fixed%20prompt%20whose%20guardrail%20explicitly%20describes%20a%20surrounding%20square%20border%2C%20enabling%20correct%20answers%20via%20homography.%20We%20evaluate%20six%20simple%20tasks%20over%20binary%20and%20continuous%20targets%2C%20and%20observe%20that%20the%20overall%20accuracy%20with%20respect%20to%203D%20ground%20truth%20is%20modest%2C%20~69%25%20on%20average%20%28best%20~75%25%2C%20worst%20~64%25%29.%20The%20same%20responses%20align%20even%20more%20closely%20with%202D%20projections%20in%20the%20image%20plane%2C%20where%20mean%20accuracy%20is%20~72%25.%20All%20four%20VLMs%20consistently%20fail%2C%20with%20accuracy%20falling%20to%20~0%25%2C%20on%20recognizing%20minority%20shape%20classes%20%28equilateral%2C%20isosceles%2C%20right-angled%20triangles%29.%20Additionally%2C%20overall%20VLM%20accuracy%20degrades%20by%20~4.1%25%20under%20camera%20tilt.%20This%20demonstrates%20that%20models%20fail%20to%20correctly%20utilize%20the%20explicit%20frame-of-reference%20hint%20provided%20in%20the%20prompt%20and%20default%20to%202D%20image%20plane%20cues.%20Finally%2C%20we%20find%20that%20object%20interference%20has%20no%20significant%20effect%20on%20VLM%20accuracy.&entry.1838667208=http%3A//arxiv.org/abs/2512.08860v1&entry.124074799=Read"},
{"title": "You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction", "author": "Logan Lawrence and Oindrila Saha and Megan Wei and Chen Sun and Subhransu Maji and Grant Van Horn", "abstract": "Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or don't consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language.", "link": "http://arxiv.org/abs/2510.14885v2", "date": "2025-12-09", "relevancy": 2.7439, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5591}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5436}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20You%20May%20Speak%20Freely%3A%20Improving%20the%20Fine-Grained%20Visual%20Recognition%20Capabilities%20of%20Multimodal%20Large%20Language%20Models%20with%20Answer%20Extraction&body=Title%3A%20You%20May%20Speak%20Freely%3A%20Improving%20the%20Fine-Grained%20Visual%20Recognition%20Capabilities%20of%20Multimodal%20Large%20Language%20Models%20with%20Answer%20Extraction%0AAuthor%3A%20Logan%20Lawrence%20and%20Oindrila%20Saha%20and%20Megan%20Wei%20and%20Chen%20Sun%20and%20Subhransu%20Maji%20and%20Grant%20Van%20Horn%0AAbstract%3A%20Despite%20the%20renewed%20interest%20in%20zero-shot%20visual%20classification%20due%20to%20the%20rise%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20the%20problem%20of%20evaluating%20free-form%20responses%20of%20auto-regressive%20models%20remains%20a%20persistent%20challenge.%20Most%20existing%20works%20focus%20on%20language-only%20tasks%20or%20don%27t%20consider%20Multiple%20Choice%20Questions%20%28MCQs%29%20beyond%205-way%20options%2C%20both%20of%20which%20are%20critical%20capabilities%20to%20solve%20tasks%20in%20Fine-Grained%20Visual%20Classification%20%28FGVC%29%20where%20choice%20counts%20are%20in%20the%20hundreds%20to%20thousands%20and%20the%20choices%20are%20highly%20related.%20Furthermore%2C%20in%20this%20highly%20multi-way%20MCQ%20setting%20it%20is%20not%20clear%20how%20to%20extend%20LLM%20choice%20extraction%20to%20retrieval-based%20problems%2C%20where%20computing%20probabilities%20over%20the%20choice%20set%20is%20computationally%20costly.%20In%20this%20work%20we%20investigate%20nlg2choice%2C%20a%20simple%20two-stage%20method%20which%20first%20asks%20the%20MLLM%20an%20open-ended%20question%20for%20the%20task%20with%20minimal%20constraints%2C%20then%20uses%20text-only%20constrained%20decoding%20to%20predict%20the%20most%20likely%20choice.%20In%20retrieval%20settings%2C%20we%20compute%20the%20probability%20of%20the%20constrained%20response%20taking%20that%20choice%20with%20an%20early%20stopping%20method%20to%20significantly%20improve%20throughput.%20Our%20results%20show%20improvement%20over%20a%20suite%20of%20seven%20fine-grained%20visual%20datasets%20when%20evaluating%20in%20terms%20of%20classification%20and%20retrieval%2C%20and%20show%20that%20this%20performance%20holds%20over%20the%20various%20ways%20that%20users%20of%20LLMs%20can%20implement%20tasks%20in%20natural%20language.%0ALink%3A%20http%3A//arxiv.org/abs/2510.14885v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DYou%2520May%2520Speak%2520Freely%253A%2520Improving%2520the%2520Fine-Grained%2520Visual%2520Recognition%2520Capabilities%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520with%2520Answer%2520Extraction%26entry.906535625%3DLogan%2520Lawrence%2520and%2520Oindrila%2520Saha%2520and%2520Megan%2520Wei%2520and%2520Chen%2520Sun%2520and%2520Subhransu%2520Maji%2520and%2520Grant%2520Van%2520Horn%26entry.1292438233%3DDespite%2520the%2520renewed%2520interest%2520in%2520zero-shot%2520visual%2520classification%2520due%2520to%2520the%2520rise%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520the%2520problem%2520of%2520evaluating%2520free-form%2520responses%2520of%2520auto-regressive%2520models%2520remains%2520a%2520persistent%2520challenge.%2520Most%2520existing%2520works%2520focus%2520on%2520language-only%2520tasks%2520or%2520don%2527t%2520consider%2520Multiple%2520Choice%2520Questions%2520%2528MCQs%2529%2520beyond%25205-way%2520options%252C%2520both%2520of%2520which%2520are%2520critical%2520capabilities%2520to%2520solve%2520tasks%2520in%2520Fine-Grained%2520Visual%2520Classification%2520%2528FGVC%2529%2520where%2520choice%2520counts%2520are%2520in%2520the%2520hundreds%2520to%2520thousands%2520and%2520the%2520choices%2520are%2520highly%2520related.%2520Furthermore%252C%2520in%2520this%2520highly%2520multi-way%2520MCQ%2520setting%2520it%2520is%2520not%2520clear%2520how%2520to%2520extend%2520LLM%2520choice%2520extraction%2520to%2520retrieval-based%2520problems%252C%2520where%2520computing%2520probabilities%2520over%2520the%2520choice%2520set%2520is%2520computationally%2520costly.%2520In%2520this%2520work%2520we%2520investigate%2520nlg2choice%252C%2520a%2520simple%2520two-stage%2520method%2520which%2520first%2520asks%2520the%2520MLLM%2520an%2520open-ended%2520question%2520for%2520the%2520task%2520with%2520minimal%2520constraints%252C%2520then%2520uses%2520text-only%2520constrained%2520decoding%2520to%2520predict%2520the%2520most%2520likely%2520choice.%2520In%2520retrieval%2520settings%252C%2520we%2520compute%2520the%2520probability%2520of%2520the%2520constrained%2520response%2520taking%2520that%2520choice%2520with%2520an%2520early%2520stopping%2520method%2520to%2520significantly%2520improve%2520throughput.%2520Our%2520results%2520show%2520improvement%2520over%2520a%2520suite%2520of%2520seven%2520fine-grained%2520visual%2520datasets%2520when%2520evaluating%2520in%2520terms%2520of%2520classification%2520and%2520retrieval%252C%2520and%2520show%2520that%2520this%2520performance%2520holds%2520over%2520the%2520various%2520ways%2520that%2520users%2520of%2520LLMs%2520can%2520implement%2520tasks%2520in%2520natural%2520language.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.14885v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=You%20May%20Speak%20Freely%3A%20Improving%20the%20Fine-Grained%20Visual%20Recognition%20Capabilities%20of%20Multimodal%20Large%20Language%20Models%20with%20Answer%20Extraction&entry.906535625=Logan%20Lawrence%20and%20Oindrila%20Saha%20and%20Megan%20Wei%20and%20Chen%20Sun%20and%20Subhransu%20Maji%20and%20Grant%20Van%20Horn&entry.1292438233=Despite%20the%20renewed%20interest%20in%20zero-shot%20visual%20classification%20due%20to%20the%20rise%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20the%20problem%20of%20evaluating%20free-form%20responses%20of%20auto-regressive%20models%20remains%20a%20persistent%20challenge.%20Most%20existing%20works%20focus%20on%20language-only%20tasks%20or%20don%27t%20consider%20Multiple%20Choice%20Questions%20%28MCQs%29%20beyond%205-way%20options%2C%20both%20of%20which%20are%20critical%20capabilities%20to%20solve%20tasks%20in%20Fine-Grained%20Visual%20Classification%20%28FGVC%29%20where%20choice%20counts%20are%20in%20the%20hundreds%20to%20thousands%20and%20the%20choices%20are%20highly%20related.%20Furthermore%2C%20in%20this%20highly%20multi-way%20MCQ%20setting%20it%20is%20not%20clear%20how%20to%20extend%20LLM%20choice%20extraction%20to%20retrieval-based%20problems%2C%20where%20computing%20probabilities%20over%20the%20choice%20set%20is%20computationally%20costly.%20In%20this%20work%20we%20investigate%20nlg2choice%2C%20a%20simple%20two-stage%20method%20which%20first%20asks%20the%20MLLM%20an%20open-ended%20question%20for%20the%20task%20with%20minimal%20constraints%2C%20then%20uses%20text-only%20constrained%20decoding%20to%20predict%20the%20most%20likely%20choice.%20In%20retrieval%20settings%2C%20we%20compute%20the%20probability%20of%20the%20constrained%20response%20taking%20that%20choice%20with%20an%20early%20stopping%20method%20to%20significantly%20improve%20throughput.%20Our%20results%20show%20improvement%20over%20a%20suite%20of%20seven%20fine-grained%20visual%20datasets%20when%20evaluating%20in%20terms%20of%20classification%20and%20retrieval%2C%20and%20show%20that%20this%20performance%20holds%20over%20the%20various%20ways%20that%20users%20of%20LLMs%20can%20implement%20tasks%20in%20natural%20language.&entry.1838667208=http%3A//arxiv.org/abs/2510.14885v2&entry.124074799=Read"},
{"title": "LapFM: A Laparoscopic Segmentation Foundation Model via Hierarchical Concept Evolving Pre-training", "author": "Qing Xu and Kun Yuan and Yuxiang Luo and Yuhao Zhai and Wenting Duan and Nassir Navab and Zhen Chen", "abstract": "Surgical segmentation is pivotal for scene understanding yet remains hindered by annotation scarcity and semantic inconsistency across diverse procedures. Existing approaches typically fine-tune natural foundation models (e.g., SAM) with limited supervision, functioning merely as domain adapters rather than surgical foundation models. Consequently, they struggle to generalize across the vast variability of surgical targets. To bridge this gap, we present LapFM, a foundation model designed to evolve robust segmentation capabilities from massive unlabeled surgical images. Distinct from medical foundation models relying on inefficient self-supervised proxy tasks, LapFM leverages a Hierarchical Concept Evolving Pre-training paradigm. First, we establish a Laparoscopic Concept Hierarchy (LCH) via a hierarchical mask decoder with parent-child query embeddings, unifying diverse entities (i.e., Anatomy, Tissue, and Instrument) into a scalable knowledge structure with cross-granularity semantic consistency. Second, we propose a Confidence-driven Evolving Labeling that iteratively generates and filters pseudo-labels based on hierarchical consistency, progressively incorporating reliable samples from unlabeled images into training. This process yields LapBench-114K, a large-scale benchmark comprising 114K image-mask pairs. Extensive experiments demonstrate that LapFM significantly outperforms state-of-the-art methods, establishing new standards for granularity-adaptive generalization in universal laparoscopic segmentation. The source code is available at https://github.com/xq141839/LapFM.", "link": "http://arxiv.org/abs/2512.08439v1", "date": "2025-12-09", "relevancy": 2.6972, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5475}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5354}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5354}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LapFM%3A%20A%20Laparoscopic%20Segmentation%20Foundation%20Model%20via%20Hierarchical%20Concept%20Evolving%20Pre-training&body=Title%3A%20LapFM%3A%20A%20Laparoscopic%20Segmentation%20Foundation%20Model%20via%20Hierarchical%20Concept%20Evolving%20Pre-training%0AAuthor%3A%20Qing%20Xu%20and%20Kun%20Yuan%20and%20Yuxiang%20Luo%20and%20Yuhao%20Zhai%20and%20Wenting%20Duan%20and%20Nassir%20Navab%20and%20Zhen%20Chen%0AAbstract%3A%20Surgical%20segmentation%20is%20pivotal%20for%20scene%20understanding%20yet%20remains%20hindered%20by%20annotation%20scarcity%20and%20semantic%20inconsistency%20across%20diverse%20procedures.%20Existing%20approaches%20typically%20fine-tune%20natural%20foundation%20models%20%28e.g.%2C%20SAM%29%20with%20limited%20supervision%2C%20functioning%20merely%20as%20domain%20adapters%20rather%20than%20surgical%20foundation%20models.%20Consequently%2C%20they%20struggle%20to%20generalize%20across%20the%20vast%20variability%20of%20surgical%20targets.%20To%20bridge%20this%20gap%2C%20we%20present%20LapFM%2C%20a%20foundation%20model%20designed%20to%20evolve%20robust%20segmentation%20capabilities%20from%20massive%20unlabeled%20surgical%20images.%20Distinct%20from%20medical%20foundation%20models%20relying%20on%20inefficient%20self-supervised%20proxy%20tasks%2C%20LapFM%20leverages%20a%20Hierarchical%20Concept%20Evolving%20Pre-training%20paradigm.%20First%2C%20we%20establish%20a%20Laparoscopic%20Concept%20Hierarchy%20%28LCH%29%20via%20a%20hierarchical%20mask%20decoder%20with%20parent-child%20query%20embeddings%2C%20unifying%20diverse%20entities%20%28i.e.%2C%20Anatomy%2C%20Tissue%2C%20and%20Instrument%29%20into%20a%20scalable%20knowledge%20structure%20with%20cross-granularity%20semantic%20consistency.%20Second%2C%20we%20propose%20a%20Confidence-driven%20Evolving%20Labeling%20that%20iteratively%20generates%20and%20filters%20pseudo-labels%20based%20on%20hierarchical%20consistency%2C%20progressively%20incorporating%20reliable%20samples%20from%20unlabeled%20images%20into%20training.%20This%20process%20yields%20LapBench-114K%2C%20a%20large-scale%20benchmark%20comprising%20114K%20image-mask%20pairs.%20Extensive%20experiments%20demonstrate%20that%20LapFM%20significantly%20outperforms%20state-of-the-art%20methods%2C%20establishing%20new%20standards%20for%20granularity-adaptive%20generalization%20in%20universal%20laparoscopic%20segmentation.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/xq141839/LapFM.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08439v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLapFM%253A%2520A%2520Laparoscopic%2520Segmentation%2520Foundation%2520Model%2520via%2520Hierarchical%2520Concept%2520Evolving%2520Pre-training%26entry.906535625%3DQing%2520Xu%2520and%2520Kun%2520Yuan%2520and%2520Yuxiang%2520Luo%2520and%2520Yuhao%2520Zhai%2520and%2520Wenting%2520Duan%2520and%2520Nassir%2520Navab%2520and%2520Zhen%2520Chen%26entry.1292438233%3DSurgical%2520segmentation%2520is%2520pivotal%2520for%2520scene%2520understanding%2520yet%2520remains%2520hindered%2520by%2520annotation%2520scarcity%2520and%2520semantic%2520inconsistency%2520across%2520diverse%2520procedures.%2520Existing%2520approaches%2520typically%2520fine-tune%2520natural%2520foundation%2520models%2520%2528e.g.%252C%2520SAM%2529%2520with%2520limited%2520supervision%252C%2520functioning%2520merely%2520as%2520domain%2520adapters%2520rather%2520than%2520surgical%2520foundation%2520models.%2520Consequently%252C%2520they%2520struggle%2520to%2520generalize%2520across%2520the%2520vast%2520variability%2520of%2520surgical%2520targets.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520present%2520LapFM%252C%2520a%2520foundation%2520model%2520designed%2520to%2520evolve%2520robust%2520segmentation%2520capabilities%2520from%2520massive%2520unlabeled%2520surgical%2520images.%2520Distinct%2520from%2520medical%2520foundation%2520models%2520relying%2520on%2520inefficient%2520self-supervised%2520proxy%2520tasks%252C%2520LapFM%2520leverages%2520a%2520Hierarchical%2520Concept%2520Evolving%2520Pre-training%2520paradigm.%2520First%252C%2520we%2520establish%2520a%2520Laparoscopic%2520Concept%2520Hierarchy%2520%2528LCH%2529%2520via%2520a%2520hierarchical%2520mask%2520decoder%2520with%2520parent-child%2520query%2520embeddings%252C%2520unifying%2520diverse%2520entities%2520%2528i.e.%252C%2520Anatomy%252C%2520Tissue%252C%2520and%2520Instrument%2529%2520into%2520a%2520scalable%2520knowledge%2520structure%2520with%2520cross-granularity%2520semantic%2520consistency.%2520Second%252C%2520we%2520propose%2520a%2520Confidence-driven%2520Evolving%2520Labeling%2520that%2520iteratively%2520generates%2520and%2520filters%2520pseudo-labels%2520based%2520on%2520hierarchical%2520consistency%252C%2520progressively%2520incorporating%2520reliable%2520samples%2520from%2520unlabeled%2520images%2520into%2520training.%2520This%2520process%2520yields%2520LapBench-114K%252C%2520a%2520large-scale%2520benchmark%2520comprising%2520114K%2520image-mask%2520pairs.%2520Extensive%2520experiments%2520demonstrate%2520that%2520LapFM%2520significantly%2520outperforms%2520state-of-the-art%2520methods%252C%2520establishing%2520new%2520standards%2520for%2520granularity-adaptive%2520generalization%2520in%2520universal%2520laparoscopic%2520segmentation.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/xq141839/LapFM.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08439v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LapFM%3A%20A%20Laparoscopic%20Segmentation%20Foundation%20Model%20via%20Hierarchical%20Concept%20Evolving%20Pre-training&entry.906535625=Qing%20Xu%20and%20Kun%20Yuan%20and%20Yuxiang%20Luo%20and%20Yuhao%20Zhai%20and%20Wenting%20Duan%20and%20Nassir%20Navab%20and%20Zhen%20Chen&entry.1292438233=Surgical%20segmentation%20is%20pivotal%20for%20scene%20understanding%20yet%20remains%20hindered%20by%20annotation%20scarcity%20and%20semantic%20inconsistency%20across%20diverse%20procedures.%20Existing%20approaches%20typically%20fine-tune%20natural%20foundation%20models%20%28e.g.%2C%20SAM%29%20with%20limited%20supervision%2C%20functioning%20merely%20as%20domain%20adapters%20rather%20than%20surgical%20foundation%20models.%20Consequently%2C%20they%20struggle%20to%20generalize%20across%20the%20vast%20variability%20of%20surgical%20targets.%20To%20bridge%20this%20gap%2C%20we%20present%20LapFM%2C%20a%20foundation%20model%20designed%20to%20evolve%20robust%20segmentation%20capabilities%20from%20massive%20unlabeled%20surgical%20images.%20Distinct%20from%20medical%20foundation%20models%20relying%20on%20inefficient%20self-supervised%20proxy%20tasks%2C%20LapFM%20leverages%20a%20Hierarchical%20Concept%20Evolving%20Pre-training%20paradigm.%20First%2C%20we%20establish%20a%20Laparoscopic%20Concept%20Hierarchy%20%28LCH%29%20via%20a%20hierarchical%20mask%20decoder%20with%20parent-child%20query%20embeddings%2C%20unifying%20diverse%20entities%20%28i.e.%2C%20Anatomy%2C%20Tissue%2C%20and%20Instrument%29%20into%20a%20scalable%20knowledge%20structure%20with%20cross-granularity%20semantic%20consistency.%20Second%2C%20we%20propose%20a%20Confidence-driven%20Evolving%20Labeling%20that%20iteratively%20generates%20and%20filters%20pseudo-labels%20based%20on%20hierarchical%20consistency%2C%20progressively%20incorporating%20reliable%20samples%20from%20unlabeled%20images%20into%20training.%20This%20process%20yields%20LapBench-114K%2C%20a%20large-scale%20benchmark%20comprising%20114K%20image-mask%20pairs.%20Extensive%20experiments%20demonstrate%20that%20LapFM%20significantly%20outperforms%20state-of-the-art%20methods%2C%20establishing%20new%20standards%20for%20granularity-adaptive%20generalization%20in%20universal%20laparoscopic%20segmentation.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/xq141839/LapFM.&entry.1838667208=http%3A//arxiv.org/abs/2512.08439v1&entry.124074799=Read"},
{"title": "TRACE: A Framework for Analyzing and Enhancing Stepwise Reasoning in Vision-Language Models", "author": "Shima Imani and Seungwhan Moon and Lambert Mathias and Lu Zhang and Babak Damavandi", "abstract": "Reliable mathematical and scientific reasoning remains an open challenge for large vision-language models. Standard final-answer evaluation often masks reasoning errors, allowing silent failures to persist. To address this gap, we introduce TRACE, a framework for Transparent Reasoning And Consistency Evaluation that diagnoses reasoning trajectories rather than only end results. At its core, TRACE leverages Auxiliary Reasoning Sets, compact sub question answer pairs that decompose complex problems, evaluate intermediate steps through consistency-based metrics, and expose failures overlooked by standard evaluation. Our experiments show that consistency across ARS correlates with final-answer correctness and helps pinpoint the reasoning steps where failures arise, offering actionable signals for model improvement. Furthermore, TRACE defines confidence regions that distinguish reliable from unreliable reasoning paths, supporting effective filtering, debugging, and model refinement.", "link": "http://arxiv.org/abs/2512.05943v2", "date": "2025-12-09", "relevancy": 2.6895, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5592}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4953}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20TRACE%3A%20A%20Framework%20for%20Analyzing%20and%20Enhancing%20Stepwise%20Reasoning%20in%20Vision-Language%20Models&body=Title%3A%20TRACE%3A%20A%20Framework%20for%20Analyzing%20and%20Enhancing%20Stepwise%20Reasoning%20in%20Vision-Language%20Models%0AAuthor%3A%20Shima%20Imani%20and%20Seungwhan%20Moon%20and%20Lambert%20Mathias%20and%20Lu%20Zhang%20and%20Babak%20Damavandi%0AAbstract%3A%20Reliable%20mathematical%20and%20scientific%20reasoning%20remains%20an%20open%20challenge%20for%20large%20vision-language%20models.%20Standard%20final-answer%20evaluation%20often%20masks%20reasoning%20errors%2C%20allowing%20silent%20failures%20to%20persist.%20To%20address%20this%20gap%2C%20we%20introduce%20TRACE%2C%20a%20framework%20for%20Transparent%20Reasoning%20And%20Consistency%20Evaluation%20that%20diagnoses%20reasoning%20trajectories%20rather%20than%20only%20end%20results.%20At%20its%20core%2C%20TRACE%20leverages%20Auxiliary%20Reasoning%20Sets%2C%20compact%20sub%20question%20answer%20pairs%20that%20decompose%20complex%20problems%2C%20evaluate%20intermediate%20steps%20through%20consistency-based%20metrics%2C%20and%20expose%20failures%20overlooked%20by%20standard%20evaluation.%20Our%20experiments%20show%20that%20consistency%20across%20ARS%20correlates%20with%20final-answer%20correctness%20and%20helps%20pinpoint%20the%20reasoning%20steps%20where%20failures%20arise%2C%20offering%20actionable%20signals%20for%20model%20improvement.%20Furthermore%2C%20TRACE%20defines%20confidence%20regions%20that%20distinguish%20reliable%20from%20unreliable%20reasoning%20paths%2C%20supporting%20effective%20filtering%2C%20debugging%2C%20and%20model%20refinement.%0ALink%3A%20http%3A//arxiv.org/abs/2512.05943v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTRACE%253A%2520A%2520Framework%2520for%2520Analyzing%2520and%2520Enhancing%2520Stepwise%2520Reasoning%2520in%2520Vision-Language%2520Models%26entry.906535625%3DShima%2520Imani%2520and%2520Seungwhan%2520Moon%2520and%2520Lambert%2520Mathias%2520and%2520Lu%2520Zhang%2520and%2520Babak%2520Damavandi%26entry.1292438233%3DReliable%2520mathematical%2520and%2520scientific%2520reasoning%2520remains%2520an%2520open%2520challenge%2520for%2520large%2520vision-language%2520models.%2520Standard%2520final-answer%2520evaluation%2520often%2520masks%2520reasoning%2520errors%252C%2520allowing%2520silent%2520failures%2520to%2520persist.%2520To%2520address%2520this%2520gap%252C%2520we%2520introduce%2520TRACE%252C%2520a%2520framework%2520for%2520Transparent%2520Reasoning%2520And%2520Consistency%2520Evaluation%2520that%2520diagnoses%2520reasoning%2520trajectories%2520rather%2520than%2520only%2520end%2520results.%2520At%2520its%2520core%252C%2520TRACE%2520leverages%2520Auxiliary%2520Reasoning%2520Sets%252C%2520compact%2520sub%2520question%2520answer%2520pairs%2520that%2520decompose%2520complex%2520problems%252C%2520evaluate%2520intermediate%2520steps%2520through%2520consistency-based%2520metrics%252C%2520and%2520expose%2520failures%2520overlooked%2520by%2520standard%2520evaluation.%2520Our%2520experiments%2520show%2520that%2520consistency%2520across%2520ARS%2520correlates%2520with%2520final-answer%2520correctness%2520and%2520helps%2520pinpoint%2520the%2520reasoning%2520steps%2520where%2520failures%2520arise%252C%2520offering%2520actionable%2520signals%2520for%2520model%2520improvement.%2520Furthermore%252C%2520TRACE%2520defines%2520confidence%2520regions%2520that%2520distinguish%2520reliable%2520from%2520unreliable%2520reasoning%2520paths%252C%2520supporting%2520effective%2520filtering%252C%2520debugging%252C%2520and%2520model%2520refinement.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.05943v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TRACE%3A%20A%20Framework%20for%20Analyzing%20and%20Enhancing%20Stepwise%20Reasoning%20in%20Vision-Language%20Models&entry.906535625=Shima%20Imani%20and%20Seungwhan%20Moon%20and%20Lambert%20Mathias%20and%20Lu%20Zhang%20and%20Babak%20Damavandi&entry.1292438233=Reliable%20mathematical%20and%20scientific%20reasoning%20remains%20an%20open%20challenge%20for%20large%20vision-language%20models.%20Standard%20final-answer%20evaluation%20often%20masks%20reasoning%20errors%2C%20allowing%20silent%20failures%20to%20persist.%20To%20address%20this%20gap%2C%20we%20introduce%20TRACE%2C%20a%20framework%20for%20Transparent%20Reasoning%20And%20Consistency%20Evaluation%20that%20diagnoses%20reasoning%20trajectories%20rather%20than%20only%20end%20results.%20At%20its%20core%2C%20TRACE%20leverages%20Auxiliary%20Reasoning%20Sets%2C%20compact%20sub%20question%20answer%20pairs%20that%20decompose%20complex%20problems%2C%20evaluate%20intermediate%20steps%20through%20consistency-based%20metrics%2C%20and%20expose%20failures%20overlooked%20by%20standard%20evaluation.%20Our%20experiments%20show%20that%20consistency%20across%20ARS%20correlates%20with%20final-answer%20correctness%20and%20helps%20pinpoint%20the%20reasoning%20steps%20where%20failures%20arise%2C%20offering%20actionable%20signals%20for%20model%20improvement.%20Furthermore%2C%20TRACE%20defines%20confidence%20regions%20that%20distinguish%20reliable%20from%20unreliable%20reasoning%20paths%2C%20supporting%20effective%20filtering%2C%20debugging%2C%20and%20model%20refinement.&entry.1838667208=http%3A//arxiv.org/abs/2512.05943v2&entry.124074799=Read"},
{"title": "Co-Seg++: Mutual Prompt-Guided Collaborative Learning for Versatile Medical Segmentation", "author": "Qing Xu and Yuxiang Luo and Wenting Duan and Zhen Chen", "abstract": "Medical image analysis is critical yet challenged by the need of jointly segmenting organs or tissues, and numerous instances for anatomical structures and tumor microenvironment analysis. Existing studies typically formulated different segmentation tasks in isolation, which overlooks the fundamental interdependencies between these tasks, leading to suboptimal segmentation performance and insufficient medical image understanding. To address this issue, we propose a Co-Seg++ framework for versatile medical segmentation. Specifically, we introduce a novel co-segmentation paradigm, allowing semantic and instance segmentation tasks to mutually enhance each other. We first devise a spatio-sequential prompt encoder (SSP-Encoder) to capture long-range spatial and sequential relationships between segmentation regions and image embeddings as prior spatial constraints. Moreover, we devise a multi-task collaborative decoder (MTC-Decoder) that leverages cross-guidance to strengthen the contextual consistency of both tasks, jointly computing semantic and instance segmentation masks. Extensive experiments on diverse CT and histopathology datasets demonstrate that the proposed Co-Seg++ outperforms state-of-the-arts in the semantic, instance, and panoptic segmentation of dental anatomical structures, histopathology tissues, and nuclei instances. The source code is available at https://github.com/xq141839/Co-Seg-Plus.", "link": "http://arxiv.org/abs/2506.17159v2", "date": "2025-12-09", "relevancy": 2.6777, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5442}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5312}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Co-Seg%2B%2B%3A%20Mutual%20Prompt-Guided%20Collaborative%20Learning%20for%20Versatile%20Medical%20Segmentation&body=Title%3A%20Co-Seg%2B%2B%3A%20Mutual%20Prompt-Guided%20Collaborative%20Learning%20for%20Versatile%20Medical%20Segmentation%0AAuthor%3A%20Qing%20Xu%20and%20Yuxiang%20Luo%20and%20Wenting%20Duan%20and%20Zhen%20Chen%0AAbstract%3A%20Medical%20image%20analysis%20is%20critical%20yet%20challenged%20by%20the%20need%20of%20jointly%20segmenting%20organs%20or%20tissues%2C%20and%20numerous%20instances%20for%20anatomical%20structures%20and%20tumor%20microenvironment%20analysis.%20Existing%20studies%20typically%20formulated%20different%20segmentation%20tasks%20in%20isolation%2C%20which%20overlooks%20the%20fundamental%20interdependencies%20between%20these%20tasks%2C%20leading%20to%20suboptimal%20segmentation%20performance%20and%20insufficient%20medical%20image%20understanding.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Co-Seg%2B%2B%20framework%20for%20versatile%20medical%20segmentation.%20Specifically%2C%20we%20introduce%20a%20novel%20co-segmentation%20paradigm%2C%20allowing%20semantic%20and%20instance%20segmentation%20tasks%20to%20mutually%20enhance%20each%20other.%20We%20first%20devise%20a%20spatio-sequential%20prompt%20encoder%20%28SSP-Encoder%29%20to%20capture%20long-range%20spatial%20and%20sequential%20relationships%20between%20segmentation%20regions%20and%20image%20embeddings%20as%20prior%20spatial%20constraints.%20Moreover%2C%20we%20devise%20a%20multi-task%20collaborative%20decoder%20%28MTC-Decoder%29%20that%20leverages%20cross-guidance%20to%20strengthen%20the%20contextual%20consistency%20of%20both%20tasks%2C%20jointly%20computing%20semantic%20and%20instance%20segmentation%20masks.%20Extensive%20experiments%20on%20diverse%20CT%20and%20histopathology%20datasets%20demonstrate%20that%20the%20proposed%20Co-Seg%2B%2B%20outperforms%20state-of-the-arts%20in%20the%20semantic%2C%20instance%2C%20and%20panoptic%20segmentation%20of%20dental%20anatomical%20structures%2C%20histopathology%20tissues%2C%20and%20nuclei%20instances.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/xq141839/Co-Seg-Plus.%0ALink%3A%20http%3A//arxiv.org/abs/2506.17159v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCo-Seg%252B%252B%253A%2520Mutual%2520Prompt-Guided%2520Collaborative%2520Learning%2520for%2520Versatile%2520Medical%2520Segmentation%26entry.906535625%3DQing%2520Xu%2520and%2520Yuxiang%2520Luo%2520and%2520Wenting%2520Duan%2520and%2520Zhen%2520Chen%26entry.1292438233%3DMedical%2520image%2520analysis%2520is%2520critical%2520yet%2520challenged%2520by%2520the%2520need%2520of%2520jointly%2520segmenting%2520organs%2520or%2520tissues%252C%2520and%2520numerous%2520instances%2520for%2520anatomical%2520structures%2520and%2520tumor%2520microenvironment%2520analysis.%2520Existing%2520studies%2520typically%2520formulated%2520different%2520segmentation%2520tasks%2520in%2520isolation%252C%2520which%2520overlooks%2520the%2520fundamental%2520interdependencies%2520between%2520these%2520tasks%252C%2520leading%2520to%2520suboptimal%2520segmentation%2520performance%2520and%2520insufficient%2520medical%2520image%2520understanding.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520a%2520Co-Seg%252B%252B%2520framework%2520for%2520versatile%2520medical%2520segmentation.%2520Specifically%252C%2520we%2520introduce%2520a%2520novel%2520co-segmentation%2520paradigm%252C%2520allowing%2520semantic%2520and%2520instance%2520segmentation%2520tasks%2520to%2520mutually%2520enhance%2520each%2520other.%2520We%2520first%2520devise%2520a%2520spatio-sequential%2520prompt%2520encoder%2520%2528SSP-Encoder%2529%2520to%2520capture%2520long-range%2520spatial%2520and%2520sequential%2520relationships%2520between%2520segmentation%2520regions%2520and%2520image%2520embeddings%2520as%2520prior%2520spatial%2520constraints.%2520Moreover%252C%2520we%2520devise%2520a%2520multi-task%2520collaborative%2520decoder%2520%2528MTC-Decoder%2529%2520that%2520leverages%2520cross-guidance%2520to%2520strengthen%2520the%2520contextual%2520consistency%2520of%2520both%2520tasks%252C%2520jointly%2520computing%2520semantic%2520and%2520instance%2520segmentation%2520masks.%2520Extensive%2520experiments%2520on%2520diverse%2520CT%2520and%2520histopathology%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520Co-Seg%252B%252B%2520outperforms%2520state-of-the-arts%2520in%2520the%2520semantic%252C%2520instance%252C%2520and%2520panoptic%2520segmentation%2520of%2520dental%2520anatomical%2520structures%252C%2520histopathology%2520tissues%252C%2520and%2520nuclei%2520instances.%2520The%2520source%2520code%2520is%2520available%2520at%2520https%253A//github.com/xq141839/Co-Seg-Plus.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.17159v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Co-Seg%2B%2B%3A%20Mutual%20Prompt-Guided%20Collaborative%20Learning%20for%20Versatile%20Medical%20Segmentation&entry.906535625=Qing%20Xu%20and%20Yuxiang%20Luo%20and%20Wenting%20Duan%20and%20Zhen%20Chen&entry.1292438233=Medical%20image%20analysis%20is%20critical%20yet%20challenged%20by%20the%20need%20of%20jointly%20segmenting%20organs%20or%20tissues%2C%20and%20numerous%20instances%20for%20anatomical%20structures%20and%20tumor%20microenvironment%20analysis.%20Existing%20studies%20typically%20formulated%20different%20segmentation%20tasks%20in%20isolation%2C%20which%20overlooks%20the%20fundamental%20interdependencies%20between%20these%20tasks%2C%20leading%20to%20suboptimal%20segmentation%20performance%20and%20insufficient%20medical%20image%20understanding.%20To%20address%20this%20issue%2C%20we%20propose%20a%20Co-Seg%2B%2B%20framework%20for%20versatile%20medical%20segmentation.%20Specifically%2C%20we%20introduce%20a%20novel%20co-segmentation%20paradigm%2C%20allowing%20semantic%20and%20instance%20segmentation%20tasks%20to%20mutually%20enhance%20each%20other.%20We%20first%20devise%20a%20spatio-sequential%20prompt%20encoder%20%28SSP-Encoder%29%20to%20capture%20long-range%20spatial%20and%20sequential%20relationships%20between%20segmentation%20regions%20and%20image%20embeddings%20as%20prior%20spatial%20constraints.%20Moreover%2C%20we%20devise%20a%20multi-task%20collaborative%20decoder%20%28MTC-Decoder%29%20that%20leverages%20cross-guidance%20to%20strengthen%20the%20contextual%20consistency%20of%20both%20tasks%2C%20jointly%20computing%20semantic%20and%20instance%20segmentation%20masks.%20Extensive%20experiments%20on%20diverse%20CT%20and%20histopathology%20datasets%20demonstrate%20that%20the%20proposed%20Co-Seg%2B%2B%20outperforms%20state-of-the-arts%20in%20the%20semantic%2C%20instance%2C%20and%20panoptic%20segmentation%20of%20dental%20anatomical%20structures%2C%20histopathology%20tissues%2C%20and%20nuclei%20instances.%20The%20source%20code%20is%20available%20at%20https%3A//github.com/xq141839/Co-Seg-Plus.&entry.1838667208=http%3A//arxiv.org/abs/2506.17159v2&entry.124074799=Read"},
{"title": "What really matters for person re-identification? A Mixture-of-Experts Framework for Semantic Attribute Importance", "author": "Athena Psalta and Vasileios Tsironis and Konstantinos Karantzalos", "abstract": "State-of-the-art person re-identification methods achieve impressive accuracy but remain largely opaque, leaving open the question: which high-level semantic attributes do these models actually rely on? We propose MoSAIC-ReID, a Mixture-of-Experts framework that systematically quantifies the importance of pedestrian attributes for re-identification. Our approach uses LoRA-based experts, each linked to a single attribute, and an oracle router that enables controlled attribution analysis. While MoSAIC-ReID achieves competitive performance on Market-1501 and DukeMTMC under the assumption that attribute annotations are available at test time, its primary value lies in providing a large-scale, quantitative study of attribute importance across intrinsic and extrinsic cues. Using generalized linear models, statistical tests, and feature-importance analyses, we reveal which attributes, such as clothing colors and intrinsic characteristics, contribute most strongly, while infrequent cues (e.g. accessories) have limited effect. This work offers a principled framework for interpretable ReID and highlights the requirements for integrating explicit semantic knowledge in practice. Code is available at https://github.com/psaltaath/MoSAIC-ReID", "link": "http://arxiv.org/abs/2512.08697v1", "date": "2025-12-09", "relevancy": 2.6365, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5343}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5262}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5214}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20What%20really%20matters%20for%20person%20re-identification%3F%20A%20Mixture-of-Experts%20Framework%20for%20Semantic%20Attribute%20Importance&body=Title%3A%20What%20really%20matters%20for%20person%20re-identification%3F%20A%20Mixture-of-Experts%20Framework%20for%20Semantic%20Attribute%20Importance%0AAuthor%3A%20Athena%20Psalta%20and%20Vasileios%20Tsironis%20and%20Konstantinos%20Karantzalos%0AAbstract%3A%20State-of-the-art%20person%20re-identification%20methods%20achieve%20impressive%20accuracy%20but%20remain%20largely%20opaque%2C%20leaving%20open%20the%20question%3A%20which%20high-level%20semantic%20attributes%20do%20these%20models%20actually%20rely%20on%3F%20We%20propose%20MoSAIC-ReID%2C%20a%20Mixture-of-Experts%20framework%20that%20systematically%20quantifies%20the%20importance%20of%20pedestrian%20attributes%20for%20re-identification.%20Our%20approach%20uses%20LoRA-based%20experts%2C%20each%20linked%20to%20a%20single%20attribute%2C%20and%20an%20oracle%20router%20that%20enables%20controlled%20attribution%20analysis.%20While%20MoSAIC-ReID%20achieves%20competitive%20performance%20on%20Market-1501%20and%20DukeMTMC%20under%20the%20assumption%20that%20attribute%20annotations%20are%20available%20at%20test%20time%2C%20its%20primary%20value%20lies%20in%20providing%20a%20large-scale%2C%20quantitative%20study%20of%20attribute%20importance%20across%20intrinsic%20and%20extrinsic%20cues.%20Using%20generalized%20linear%20models%2C%20statistical%20tests%2C%20and%20feature-importance%20analyses%2C%20we%20reveal%20which%20attributes%2C%20such%20as%20clothing%20colors%20and%20intrinsic%20characteristics%2C%20contribute%20most%20strongly%2C%20while%20infrequent%20cues%20%28e.g.%20accessories%29%20have%20limited%20effect.%20This%20work%20offers%20a%20principled%20framework%20for%20interpretable%20ReID%20and%20highlights%20the%20requirements%20for%20integrating%20explicit%20semantic%20knowledge%20in%20practice.%20Code%20is%20available%20at%20https%3A//github.com/psaltaath/MoSAIC-ReID%0ALink%3A%20http%3A//arxiv.org/abs/2512.08697v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhat%2520really%2520matters%2520for%2520person%2520re-identification%253F%2520A%2520Mixture-of-Experts%2520Framework%2520for%2520Semantic%2520Attribute%2520Importance%26entry.906535625%3DAthena%2520Psalta%2520and%2520Vasileios%2520Tsironis%2520and%2520Konstantinos%2520Karantzalos%26entry.1292438233%3DState-of-the-art%2520person%2520re-identification%2520methods%2520achieve%2520impressive%2520accuracy%2520but%2520remain%2520largely%2520opaque%252C%2520leaving%2520open%2520the%2520question%253A%2520which%2520high-level%2520semantic%2520attributes%2520do%2520these%2520models%2520actually%2520rely%2520on%253F%2520We%2520propose%2520MoSAIC-ReID%252C%2520a%2520Mixture-of-Experts%2520framework%2520that%2520systematically%2520quantifies%2520the%2520importance%2520of%2520pedestrian%2520attributes%2520for%2520re-identification.%2520Our%2520approach%2520uses%2520LoRA-based%2520experts%252C%2520each%2520linked%2520to%2520a%2520single%2520attribute%252C%2520and%2520an%2520oracle%2520router%2520that%2520enables%2520controlled%2520attribution%2520analysis.%2520While%2520MoSAIC-ReID%2520achieves%2520competitive%2520performance%2520on%2520Market-1501%2520and%2520DukeMTMC%2520under%2520the%2520assumption%2520that%2520attribute%2520annotations%2520are%2520available%2520at%2520test%2520time%252C%2520its%2520primary%2520value%2520lies%2520in%2520providing%2520a%2520large-scale%252C%2520quantitative%2520study%2520of%2520attribute%2520importance%2520across%2520intrinsic%2520and%2520extrinsic%2520cues.%2520Using%2520generalized%2520linear%2520models%252C%2520statistical%2520tests%252C%2520and%2520feature-importance%2520analyses%252C%2520we%2520reveal%2520which%2520attributes%252C%2520such%2520as%2520clothing%2520colors%2520and%2520intrinsic%2520characteristics%252C%2520contribute%2520most%2520strongly%252C%2520while%2520infrequent%2520cues%2520%2528e.g.%2520accessories%2529%2520have%2520limited%2520effect.%2520This%2520work%2520offers%2520a%2520principled%2520framework%2520for%2520interpretable%2520ReID%2520and%2520highlights%2520the%2520requirements%2520for%2520integrating%2520explicit%2520semantic%2520knowledge%2520in%2520practice.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/psaltaath/MoSAIC-ReID%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08697v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=What%20really%20matters%20for%20person%20re-identification%3F%20A%20Mixture-of-Experts%20Framework%20for%20Semantic%20Attribute%20Importance&entry.906535625=Athena%20Psalta%20and%20Vasileios%20Tsironis%20and%20Konstantinos%20Karantzalos&entry.1292438233=State-of-the-art%20person%20re-identification%20methods%20achieve%20impressive%20accuracy%20but%20remain%20largely%20opaque%2C%20leaving%20open%20the%20question%3A%20which%20high-level%20semantic%20attributes%20do%20these%20models%20actually%20rely%20on%3F%20We%20propose%20MoSAIC-ReID%2C%20a%20Mixture-of-Experts%20framework%20that%20systematically%20quantifies%20the%20importance%20of%20pedestrian%20attributes%20for%20re-identification.%20Our%20approach%20uses%20LoRA-based%20experts%2C%20each%20linked%20to%20a%20single%20attribute%2C%20and%20an%20oracle%20router%20that%20enables%20controlled%20attribution%20analysis.%20While%20MoSAIC-ReID%20achieves%20competitive%20performance%20on%20Market-1501%20and%20DukeMTMC%20under%20the%20assumption%20that%20attribute%20annotations%20are%20available%20at%20test%20time%2C%20its%20primary%20value%20lies%20in%20providing%20a%20large-scale%2C%20quantitative%20study%20of%20attribute%20importance%20across%20intrinsic%20and%20extrinsic%20cues.%20Using%20generalized%20linear%20models%2C%20statistical%20tests%2C%20and%20feature-importance%20analyses%2C%20we%20reveal%20which%20attributes%2C%20such%20as%20clothing%20colors%20and%20intrinsic%20characteristics%2C%20contribute%20most%20strongly%2C%20while%20infrequent%20cues%20%28e.g.%20accessories%29%20have%20limited%20effect.%20This%20work%20offers%20a%20principled%20framework%20for%20interpretable%20ReID%20and%20highlights%20the%20requirements%20for%20integrating%20explicit%20semantic%20knowledge%20in%20practice.%20Code%20is%20available%20at%20https%3A//github.com/psaltaath/MoSAIC-ReID&entry.1838667208=http%3A//arxiv.org/abs/2512.08697v1&entry.124074799=Read"},
{"title": "Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs", "author": "Angela van Sprang and Laurens Samson and Ana Lucic and Erman Acar and Sennay Ghebreab and Yuki M. Asano", "abstract": "We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.", "link": "http://arxiv.org/abs/2512.08923v1", "date": "2025-12-09", "relevancy": 2.6319, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5291}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.525}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Same%20Content%2C%20Different%20Answers%3A%20Cross-Modal%20Inconsistency%20in%20MLLMs&body=Title%3A%20Same%20Content%2C%20Different%20Answers%3A%20Cross-Modal%20Inconsistency%20in%20MLLMs%0AAuthor%3A%20Angela%20van%20Sprang%20and%20Laurens%20Samson%20and%20Ana%20Lucic%20and%20Erman%20Acar%20and%20Sennay%20Ghebreab%20and%20Yuki%20M.%20Asano%0AAbstract%3A%20We%20introduce%20two%20new%20benchmarks%20REST%20and%20REST%2B%28Render-Equivalence%20Stress%20Tests%29%20to%20enable%20systematic%20evaluation%20of%20cross-modal%20inconsistency%20in%20multimodal%20large%20language%20models%20%28MLLMs%29.%20MLLMs%20are%20trained%20to%20represent%20vision%20and%20language%20in%20the%20same%20embedding%20space%2C%20yet%20they%20cannot%20perform%20the%20same%20tasks%20in%20both%20modalities.%20Our%20benchmarks%20contain%20samples%20with%20the%20same%20semantic%20information%20in%20three%20modalities%20%28image%2C%20text%2C%20mixed%29%20and%20we%20show%20that%20state-of-the-art%20MLLMs%20cannot%20consistently%20reason%20over%20these%20different%20modalities.%20We%20evaluate%2015%20MLLMs%20and%20find%20that%20the%20degree%20of%20modality%20inconsistency%20varies%20substantially%2C%20even%20when%20accounting%20for%20problems%20with%20text%20recognition%20%28OCR%29.%20Neither%20rendering%20text%20as%20image%20nor%20rendering%20an%20image%20as%20text%20solves%20the%20inconsistency.%20Even%20if%20OCR%20is%20correct%2C%20we%20find%20that%20visual%20characteristics%20%28text%20colour%20and%20resolution%2C%20but%20not%20font%29%20and%20the%20number%20of%20vision%20tokens%20have%20an%20impact%20on%20model%20performance.%20Finally%2C%20we%20find%20that%20our%20consistency%20score%20correlates%20with%20the%20modality%20gap%20between%20text%20and%20images%2C%20highlighting%20a%20mechanistic%20interpretation%20of%20cross-modal%20inconsistent%20MLLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08923v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSame%2520Content%252C%2520Different%2520Answers%253A%2520Cross-Modal%2520Inconsistency%2520in%2520MLLMs%26entry.906535625%3DAngela%2520van%2520Sprang%2520and%2520Laurens%2520Samson%2520and%2520Ana%2520Lucic%2520and%2520Erman%2520Acar%2520and%2520Sennay%2520Ghebreab%2520and%2520Yuki%2520M.%2520Asano%26entry.1292438233%3DWe%2520introduce%2520two%2520new%2520benchmarks%2520REST%2520and%2520REST%252B%2528Render-Equivalence%2520Stress%2520Tests%2529%2520to%2520enable%2520systematic%2520evaluation%2520of%2520cross-modal%2520inconsistency%2520in%2520multimodal%2520large%2520language%2520models%2520%2528MLLMs%2529.%2520MLLMs%2520are%2520trained%2520to%2520represent%2520vision%2520and%2520language%2520in%2520the%2520same%2520embedding%2520space%252C%2520yet%2520they%2520cannot%2520perform%2520the%2520same%2520tasks%2520in%2520both%2520modalities.%2520Our%2520benchmarks%2520contain%2520samples%2520with%2520the%2520same%2520semantic%2520information%2520in%2520three%2520modalities%2520%2528image%252C%2520text%252C%2520mixed%2529%2520and%2520we%2520show%2520that%2520state-of-the-art%2520MLLMs%2520cannot%2520consistently%2520reason%2520over%2520these%2520different%2520modalities.%2520We%2520evaluate%252015%2520MLLMs%2520and%2520find%2520that%2520the%2520degree%2520of%2520modality%2520inconsistency%2520varies%2520substantially%252C%2520even%2520when%2520accounting%2520for%2520problems%2520with%2520text%2520recognition%2520%2528OCR%2529.%2520Neither%2520rendering%2520text%2520as%2520image%2520nor%2520rendering%2520an%2520image%2520as%2520text%2520solves%2520the%2520inconsistency.%2520Even%2520if%2520OCR%2520is%2520correct%252C%2520we%2520find%2520that%2520visual%2520characteristics%2520%2528text%2520colour%2520and%2520resolution%252C%2520but%2520not%2520font%2529%2520and%2520the%2520number%2520of%2520vision%2520tokens%2520have%2520an%2520impact%2520on%2520model%2520performance.%2520Finally%252C%2520we%2520find%2520that%2520our%2520consistency%2520score%2520correlates%2520with%2520the%2520modality%2520gap%2520between%2520text%2520and%2520images%252C%2520highlighting%2520a%2520mechanistic%2520interpretation%2520of%2520cross-modal%2520inconsistent%2520MLLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08923v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Same%20Content%2C%20Different%20Answers%3A%20Cross-Modal%20Inconsistency%20in%20MLLMs&entry.906535625=Angela%20van%20Sprang%20and%20Laurens%20Samson%20and%20Ana%20Lucic%20and%20Erman%20Acar%20and%20Sennay%20Ghebreab%20and%20Yuki%20M.%20Asano&entry.1292438233=We%20introduce%20two%20new%20benchmarks%20REST%20and%20REST%2B%28Render-Equivalence%20Stress%20Tests%29%20to%20enable%20systematic%20evaluation%20of%20cross-modal%20inconsistency%20in%20multimodal%20large%20language%20models%20%28MLLMs%29.%20MLLMs%20are%20trained%20to%20represent%20vision%20and%20language%20in%20the%20same%20embedding%20space%2C%20yet%20they%20cannot%20perform%20the%20same%20tasks%20in%20both%20modalities.%20Our%20benchmarks%20contain%20samples%20with%20the%20same%20semantic%20information%20in%20three%20modalities%20%28image%2C%20text%2C%20mixed%29%20and%20we%20show%20that%20state-of-the-art%20MLLMs%20cannot%20consistently%20reason%20over%20these%20different%20modalities.%20We%20evaluate%2015%20MLLMs%20and%20find%20that%20the%20degree%20of%20modality%20inconsistency%20varies%20substantially%2C%20even%20when%20accounting%20for%20problems%20with%20text%20recognition%20%28OCR%29.%20Neither%20rendering%20text%20as%20image%20nor%20rendering%20an%20image%20as%20text%20solves%20the%20inconsistency.%20Even%20if%20OCR%20is%20correct%2C%20we%20find%20that%20visual%20characteristics%20%28text%20colour%20and%20resolution%2C%20but%20not%20font%29%20and%20the%20number%20of%20vision%20tokens%20have%20an%20impact%20on%20model%20performance.%20Finally%2C%20we%20find%20that%20our%20consistency%20score%20correlates%20with%20the%20modality%20gap%20between%20text%20and%20images%2C%20highlighting%20a%20mechanistic%20interpretation%20of%20cross-modal%20inconsistent%20MLLMs.&entry.1838667208=http%3A//arxiv.org/abs/2512.08923v1&entry.124074799=Read"},
{"title": "Pose-Based Sign Language Spotting via an End-to-End Encoder Architecture", "author": "Samuel Ebimobowei Johnny and Blessed Guda and Emmanuel Enejo Aaron and Assane Gueye", "abstract": "Automatic Sign Language Recognition (ASLR) has emerged as a vital field for bridging the gap between deaf and hearing communities. However, the problem of sign-to-sign retrieval or detecting a specific sign within a sequence of continuous signs remains largely unexplored. We define this novel task as Sign Language Spotting. In this paper, we present a first step toward sign language retrieval by addressing the challenge of detecting the presence or absence of a query sign video within a sentence-level gloss or sign video. Unlike conventional approaches that rely on intermediate gloss recognition or text-based matching, we propose an end-to-end model that directly operates on pose keypoints extracted from sign videos. Our architecture employs an encoder-only backbone with a binary classification head to determine whether the query sign appears within the target sequence. By focusing on pose representations instead of raw RGB frames, our method significantly reduces computational cost and mitigates visual noise. We evaluate our approach on the Word Presence Prediction dataset from the WSLP 2025 shared task, achieving 61.88\\% accuracy and 60.00\\% F1-score. These results demonstrate the effectiveness of our pose-based framework for Sign Language Spotting, establishing a strong foundation for future research in automatic sign language retrieval and verification. Code is available at https://github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting", "link": "http://arxiv.org/abs/2512.08738v1", "date": "2025-12-09", "relevancy": 2.63, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5355}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5355}, {"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.5069}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pose-Based%20Sign%20Language%20Spotting%20via%20an%20End-to-End%20Encoder%20Architecture&body=Title%3A%20Pose-Based%20Sign%20Language%20Spotting%20via%20an%20End-to-End%20Encoder%20Architecture%0AAuthor%3A%20Samuel%20Ebimobowei%20Johnny%20and%20Blessed%20Guda%20and%20Emmanuel%20Enejo%20Aaron%20and%20Assane%20Gueye%0AAbstract%3A%20Automatic%20Sign%20Language%20Recognition%20%28ASLR%29%20has%20emerged%20as%20a%20vital%20field%20for%20bridging%20the%20gap%20between%20deaf%20and%20hearing%20communities.%20However%2C%20the%20problem%20of%20sign-to-sign%20retrieval%20or%20detecting%20a%20specific%20sign%20within%20a%20sequence%20of%20continuous%20signs%20remains%20largely%20unexplored.%20We%20define%20this%20novel%20task%20as%20Sign%20Language%20Spotting.%20In%20this%20paper%2C%20we%20present%20a%20first%20step%20toward%20sign%20language%20retrieval%20by%20addressing%20the%20challenge%20of%20detecting%20the%20presence%20or%20absence%20of%20a%20query%20sign%20video%20within%20a%20sentence-level%20gloss%20or%20sign%20video.%20Unlike%20conventional%20approaches%20that%20rely%20on%20intermediate%20gloss%20recognition%20or%20text-based%20matching%2C%20we%20propose%20an%20end-to-end%20model%20that%20directly%20operates%20on%20pose%20keypoints%20extracted%20from%20sign%20videos.%20Our%20architecture%20employs%20an%20encoder-only%20backbone%20with%20a%20binary%20classification%20head%20to%20determine%20whether%20the%20query%20sign%20appears%20within%20the%20target%20sequence.%20By%20focusing%20on%20pose%20representations%20instead%20of%20raw%20RGB%20frames%2C%20our%20method%20significantly%20reduces%20computational%20cost%20and%20mitigates%20visual%20noise.%20We%20evaluate%20our%20approach%20on%20the%20Word%20Presence%20Prediction%20dataset%20from%20the%20WSLP%202025%20shared%20task%2C%20achieving%2061.88%5C%25%20accuracy%20and%2060.00%5C%25%20F1-score.%20These%20results%20demonstrate%20the%20effectiveness%20of%20our%20pose-based%20framework%20for%20Sign%20Language%20Spotting%2C%20establishing%20a%20strong%20foundation%20for%20future%20research%20in%20automatic%20sign%20language%20retrieval%20and%20verification.%20Code%20is%20available%20at%20https%3A//github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting%0ALink%3A%20http%3A//arxiv.org/abs/2512.08738v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPose-Based%2520Sign%2520Language%2520Spotting%2520via%2520an%2520End-to-End%2520Encoder%2520Architecture%26entry.906535625%3DSamuel%2520Ebimobowei%2520Johnny%2520and%2520Blessed%2520Guda%2520and%2520Emmanuel%2520Enejo%2520Aaron%2520and%2520Assane%2520Gueye%26entry.1292438233%3DAutomatic%2520Sign%2520Language%2520Recognition%2520%2528ASLR%2529%2520has%2520emerged%2520as%2520a%2520vital%2520field%2520for%2520bridging%2520the%2520gap%2520between%2520deaf%2520and%2520hearing%2520communities.%2520However%252C%2520the%2520problem%2520of%2520sign-to-sign%2520retrieval%2520or%2520detecting%2520a%2520specific%2520sign%2520within%2520a%2520sequence%2520of%2520continuous%2520signs%2520remains%2520largely%2520unexplored.%2520We%2520define%2520this%2520novel%2520task%2520as%2520Sign%2520Language%2520Spotting.%2520In%2520this%2520paper%252C%2520we%2520present%2520a%2520first%2520step%2520toward%2520sign%2520language%2520retrieval%2520by%2520addressing%2520the%2520challenge%2520of%2520detecting%2520the%2520presence%2520or%2520absence%2520of%2520a%2520query%2520sign%2520video%2520within%2520a%2520sentence-level%2520gloss%2520or%2520sign%2520video.%2520Unlike%2520conventional%2520approaches%2520that%2520rely%2520on%2520intermediate%2520gloss%2520recognition%2520or%2520text-based%2520matching%252C%2520we%2520propose%2520an%2520end-to-end%2520model%2520that%2520directly%2520operates%2520on%2520pose%2520keypoints%2520extracted%2520from%2520sign%2520videos.%2520Our%2520architecture%2520employs%2520an%2520encoder-only%2520backbone%2520with%2520a%2520binary%2520classification%2520head%2520to%2520determine%2520whether%2520the%2520query%2520sign%2520appears%2520within%2520the%2520target%2520sequence.%2520By%2520focusing%2520on%2520pose%2520representations%2520instead%2520of%2520raw%2520RGB%2520frames%252C%2520our%2520method%2520significantly%2520reduces%2520computational%2520cost%2520and%2520mitigates%2520visual%2520noise.%2520We%2520evaluate%2520our%2520approach%2520on%2520the%2520Word%2520Presence%2520Prediction%2520dataset%2520from%2520the%2520WSLP%25202025%2520shared%2520task%252C%2520achieving%252061.88%255C%2525%2520accuracy%2520and%252060.00%255C%2525%2520F1-score.%2520These%2520results%2520demonstrate%2520the%2520effectiveness%2520of%2520our%2520pose-based%2520framework%2520for%2520Sign%2520Language%2520Spotting%252C%2520establishing%2520a%2520strong%2520foundation%2520for%2520future%2520research%2520in%2520automatic%2520sign%2520language%2520retrieval%2520and%2520verification.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08738v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pose-Based%20Sign%20Language%20Spotting%20via%20an%20End-to-End%20Encoder%20Architecture&entry.906535625=Samuel%20Ebimobowei%20Johnny%20and%20Blessed%20Guda%20and%20Emmanuel%20Enejo%20Aaron%20and%20Assane%20Gueye&entry.1292438233=Automatic%20Sign%20Language%20Recognition%20%28ASLR%29%20has%20emerged%20as%20a%20vital%20field%20for%20bridging%20the%20gap%20between%20deaf%20and%20hearing%20communities.%20However%2C%20the%20problem%20of%20sign-to-sign%20retrieval%20or%20detecting%20a%20specific%20sign%20within%20a%20sequence%20of%20continuous%20signs%20remains%20largely%20unexplored.%20We%20define%20this%20novel%20task%20as%20Sign%20Language%20Spotting.%20In%20this%20paper%2C%20we%20present%20a%20first%20step%20toward%20sign%20language%20retrieval%20by%20addressing%20the%20challenge%20of%20detecting%20the%20presence%20or%20absence%20of%20a%20query%20sign%20video%20within%20a%20sentence-level%20gloss%20or%20sign%20video.%20Unlike%20conventional%20approaches%20that%20rely%20on%20intermediate%20gloss%20recognition%20or%20text-based%20matching%2C%20we%20propose%20an%20end-to-end%20model%20that%20directly%20operates%20on%20pose%20keypoints%20extracted%20from%20sign%20videos.%20Our%20architecture%20employs%20an%20encoder-only%20backbone%20with%20a%20binary%20classification%20head%20to%20determine%20whether%20the%20query%20sign%20appears%20within%20the%20target%20sequence.%20By%20focusing%20on%20pose%20representations%20instead%20of%20raw%20RGB%20frames%2C%20our%20method%20significantly%20reduces%20computational%20cost%20and%20mitigates%20visual%20noise.%20We%20evaluate%20our%20approach%20on%20the%20Word%20Presence%20Prediction%20dataset%20from%20the%20WSLP%202025%20shared%20task%2C%20achieving%2061.88%5C%25%20accuracy%20and%2060.00%5C%25%20F1-score.%20These%20results%20demonstrate%20the%20effectiveness%20of%20our%20pose-based%20framework%20for%20Sign%20Language%20Spotting%2C%20establishing%20a%20strong%20foundation%20for%20future%20research%20in%20automatic%20sign%20language%20retrieval%20and%20verification.%20Code%20is%20available%20at%20https%3A//github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting&entry.1838667208=http%3A//arxiv.org/abs/2512.08738v1&entry.124074799=Read"},
{"title": "SceneGen: Single-Image 3D Scene Generation in One Feedforward Pass", "author": "Yanxu Meng and Haoning Wu and Ya Zhang and Weidi Xie", "abstract": "3D content generation has recently attracted significant research interest, driven by its critical applications in VR/AR and embodied AI. In this work, we tackle the challenging task of synthesizing multiple 3D assets within a single scene image. Concretely, our contributions are fourfold: (i) we present SceneGen, a novel framework that takes a scene image and corresponding object masks as input, simultaneously producing multiple 3D assets with geometry and texture. Notably, SceneGen operates with no need for extra optimization or asset retrieval; (ii) we introduce a novel feature aggregation module that integrates local and global scene information from visual and geometric encoders within the feature extraction module. Coupled with a position head, this enables the generation of 3D assets and their relative spatial positions in a single feedforward pass; (iii) we demonstrate SceneGen's direct extensibility to multi-image input scenarios. Despite being trained solely on single-image inputs, our architecture yields improved generation performance when multiple images are provided; and (iv) extensive quantitative and qualitative evaluations confirm the efficiency and robustness of our approach. We believe this paradigm offers a novel solution for high-quality 3D content generation, potentially advancing its practical applications in downstream tasks. The code and model will be publicly available at: https://mengmouxu.github.io/SceneGen.", "link": "http://arxiv.org/abs/2508.15769v2", "date": "2025-12-09", "relevancy": 2.6282, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6638}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6557}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6557}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SceneGen%3A%20Single-Image%203D%20Scene%20Generation%20in%20One%20Feedforward%20Pass&body=Title%3A%20SceneGen%3A%20Single-Image%203D%20Scene%20Generation%20in%20One%20Feedforward%20Pass%0AAuthor%3A%20Yanxu%20Meng%20and%20Haoning%20Wu%20and%20Ya%20Zhang%20and%20Weidi%20Xie%0AAbstract%3A%203D%20content%20generation%20has%20recently%20attracted%20significant%20research%20interest%2C%20driven%20by%20its%20critical%20applications%20in%20VR/AR%20and%20embodied%20AI.%20In%20this%20work%2C%20we%20tackle%20the%20challenging%20task%20of%20synthesizing%20multiple%203D%20assets%20within%20a%20single%20scene%20image.%20Concretely%2C%20our%20contributions%20are%20fourfold%3A%20%28i%29%20we%20present%20SceneGen%2C%20a%20novel%20framework%20that%20takes%20a%20scene%20image%20and%20corresponding%20object%20masks%20as%20input%2C%20simultaneously%20producing%20multiple%203D%20assets%20with%20geometry%20and%20texture.%20Notably%2C%20SceneGen%20operates%20with%20no%20need%20for%20extra%20optimization%20or%20asset%20retrieval%3B%20%28ii%29%20we%20introduce%20a%20novel%20feature%20aggregation%20module%20that%20integrates%20local%20and%20global%20scene%20information%20from%20visual%20and%20geometric%20encoders%20within%20the%20feature%20extraction%20module.%20Coupled%20with%20a%20position%20head%2C%20this%20enables%20the%20generation%20of%203D%20assets%20and%20their%20relative%20spatial%20positions%20in%20a%20single%20feedforward%20pass%3B%20%28iii%29%20we%20demonstrate%20SceneGen%27s%20direct%20extensibility%20to%20multi-image%20input%20scenarios.%20Despite%20being%20trained%20solely%20on%20single-image%20inputs%2C%20our%20architecture%20yields%20improved%20generation%20performance%20when%20multiple%20images%20are%20provided%3B%20and%20%28iv%29%20extensive%20quantitative%20and%20qualitative%20evaluations%20confirm%20the%20efficiency%20and%20robustness%20of%20our%20approach.%20We%20believe%20this%20paradigm%20offers%20a%20novel%20solution%20for%20high-quality%203D%20content%20generation%2C%20potentially%20advancing%20its%20practical%20applications%20in%20downstream%20tasks.%20The%20code%20and%20model%20will%20be%20publicly%20available%20at%3A%20https%3A//mengmouxu.github.io/SceneGen.%0ALink%3A%20http%3A//arxiv.org/abs/2508.15769v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSceneGen%253A%2520Single-Image%25203D%2520Scene%2520Generation%2520in%2520One%2520Feedforward%2520Pass%26entry.906535625%3DYanxu%2520Meng%2520and%2520Haoning%2520Wu%2520and%2520Ya%2520Zhang%2520and%2520Weidi%2520Xie%26entry.1292438233%3D3D%2520content%2520generation%2520has%2520recently%2520attracted%2520significant%2520research%2520interest%252C%2520driven%2520by%2520its%2520critical%2520applications%2520in%2520VR/AR%2520and%2520embodied%2520AI.%2520In%2520this%2520work%252C%2520we%2520tackle%2520the%2520challenging%2520task%2520of%2520synthesizing%2520multiple%25203D%2520assets%2520within%2520a%2520single%2520scene%2520image.%2520Concretely%252C%2520our%2520contributions%2520are%2520fourfold%253A%2520%2528i%2529%2520we%2520present%2520SceneGen%252C%2520a%2520novel%2520framework%2520that%2520takes%2520a%2520scene%2520image%2520and%2520corresponding%2520object%2520masks%2520as%2520input%252C%2520simultaneously%2520producing%2520multiple%25203D%2520assets%2520with%2520geometry%2520and%2520texture.%2520Notably%252C%2520SceneGen%2520operates%2520with%2520no%2520need%2520for%2520extra%2520optimization%2520or%2520asset%2520retrieval%253B%2520%2528ii%2529%2520we%2520introduce%2520a%2520novel%2520feature%2520aggregation%2520module%2520that%2520integrates%2520local%2520and%2520global%2520scene%2520information%2520from%2520visual%2520and%2520geometric%2520encoders%2520within%2520the%2520feature%2520extraction%2520module.%2520Coupled%2520with%2520a%2520position%2520head%252C%2520this%2520enables%2520the%2520generation%2520of%25203D%2520assets%2520and%2520their%2520relative%2520spatial%2520positions%2520in%2520a%2520single%2520feedforward%2520pass%253B%2520%2528iii%2529%2520we%2520demonstrate%2520SceneGen%2527s%2520direct%2520extensibility%2520to%2520multi-image%2520input%2520scenarios.%2520Despite%2520being%2520trained%2520solely%2520on%2520single-image%2520inputs%252C%2520our%2520architecture%2520yields%2520improved%2520generation%2520performance%2520when%2520multiple%2520images%2520are%2520provided%253B%2520and%2520%2528iv%2529%2520extensive%2520quantitative%2520and%2520qualitative%2520evaluations%2520confirm%2520the%2520efficiency%2520and%2520robustness%2520of%2520our%2520approach.%2520We%2520believe%2520this%2520paradigm%2520offers%2520a%2520novel%2520solution%2520for%2520high-quality%25203D%2520content%2520generation%252C%2520potentially%2520advancing%2520its%2520practical%2520applications%2520in%2520downstream%2520tasks.%2520The%2520code%2520and%2520model%2520will%2520be%2520publicly%2520available%2520at%253A%2520https%253A//mengmouxu.github.io/SceneGen.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.15769v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SceneGen%3A%20Single-Image%203D%20Scene%20Generation%20in%20One%20Feedforward%20Pass&entry.906535625=Yanxu%20Meng%20and%20Haoning%20Wu%20and%20Ya%20Zhang%20and%20Weidi%20Xie&entry.1292438233=3D%20content%20generation%20has%20recently%20attracted%20significant%20research%20interest%2C%20driven%20by%20its%20critical%20applications%20in%20VR/AR%20and%20embodied%20AI.%20In%20this%20work%2C%20we%20tackle%20the%20challenging%20task%20of%20synthesizing%20multiple%203D%20assets%20within%20a%20single%20scene%20image.%20Concretely%2C%20our%20contributions%20are%20fourfold%3A%20%28i%29%20we%20present%20SceneGen%2C%20a%20novel%20framework%20that%20takes%20a%20scene%20image%20and%20corresponding%20object%20masks%20as%20input%2C%20simultaneously%20producing%20multiple%203D%20assets%20with%20geometry%20and%20texture.%20Notably%2C%20SceneGen%20operates%20with%20no%20need%20for%20extra%20optimization%20or%20asset%20retrieval%3B%20%28ii%29%20we%20introduce%20a%20novel%20feature%20aggregation%20module%20that%20integrates%20local%20and%20global%20scene%20information%20from%20visual%20and%20geometric%20encoders%20within%20the%20feature%20extraction%20module.%20Coupled%20with%20a%20position%20head%2C%20this%20enables%20the%20generation%20of%203D%20assets%20and%20their%20relative%20spatial%20positions%20in%20a%20single%20feedforward%20pass%3B%20%28iii%29%20we%20demonstrate%20SceneGen%27s%20direct%20extensibility%20to%20multi-image%20input%20scenarios.%20Despite%20being%20trained%20solely%20on%20single-image%20inputs%2C%20our%20architecture%20yields%20improved%20generation%20performance%20when%20multiple%20images%20are%20provided%3B%20and%20%28iv%29%20extensive%20quantitative%20and%20qualitative%20evaluations%20confirm%20the%20efficiency%20and%20robustness%20of%20our%20approach.%20We%20believe%20this%20paradigm%20offers%20a%20novel%20solution%20for%20high-quality%203D%20content%20generation%2C%20potentially%20advancing%20its%20practical%20applications%20in%20downstream%20tasks.%20The%20code%20and%20model%20will%20be%20publicly%20available%20at%3A%20https%3A//mengmouxu.github.io/SceneGen.&entry.1838667208=http%3A//arxiv.org/abs/2508.15769v2&entry.124074799=Read"},
{"title": "LiDAS: Lighting-driven Dynamic Active Sensing for Nighttime Perception", "author": "Simon de Moreau and Andrei Bursuc and Hafid El-Idrissi and Fabien Moutarde", "abstract": "Nighttime environments pose significant challenges for camera-based perception, as existing methods passively rely on the scene lighting. We introduce Lighting-driven Dynamic Active Sensing (LiDAS), a closed-loop active illumination system that combines off-the-shelf visual perception models with high-definition headlights. Rather than uniformly brightening the scene, LiDAS dynamically predicts an optimal illumination field that maximizes downstream perception performance, i.e., decreasing light on empty areas to reallocate it on object regions. LiDAS enables zero-shot nighttime generalization of daytime-trained models through adaptive illumination control. Trained on synthetic data and deployed zero-shot in real-world closed-loop driving scenarios, LiDAS enables +18.7% mAP50 and +5.0% mIoU over standard low-beam at equal power. It maintains performances while reducing energy use by 40%. LiDAS complements domain-generalization methods, further strengthening robustness without retraining. By turning readily available headlights into active vision actuators, LiDAS offers a cost-effective solution to robust nighttime perception.", "link": "http://arxiv.org/abs/2512.08912v1", "date": "2025-12-09", "relevancy": 2.6195, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5428}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5148}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.514}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LiDAS%3A%20Lighting-driven%20Dynamic%20Active%20Sensing%20for%20Nighttime%20Perception&body=Title%3A%20LiDAS%3A%20Lighting-driven%20Dynamic%20Active%20Sensing%20for%20Nighttime%20Perception%0AAuthor%3A%20Simon%20de%20Moreau%20and%20Andrei%20Bursuc%20and%20Hafid%20El-Idrissi%20and%20Fabien%20Moutarde%0AAbstract%3A%20Nighttime%20environments%20pose%20significant%20challenges%20for%20camera-based%20perception%2C%20as%20existing%20methods%20passively%20rely%20on%20the%20scene%20lighting.%20We%20introduce%20Lighting-driven%20Dynamic%20Active%20Sensing%20%28LiDAS%29%2C%20a%20closed-loop%20active%20illumination%20system%20that%20combines%20off-the-shelf%20visual%20perception%20models%20with%20high-definition%20headlights.%20Rather%20than%20uniformly%20brightening%20the%20scene%2C%20LiDAS%20dynamically%20predicts%20an%20optimal%20illumination%20field%20that%20maximizes%20downstream%20perception%20performance%2C%20i.e.%2C%20decreasing%20light%20on%20empty%20areas%20to%20reallocate%20it%20on%20object%20regions.%20LiDAS%20enables%20zero-shot%20nighttime%20generalization%20of%20daytime-trained%20models%20through%20adaptive%20illumination%20control.%20Trained%20on%20synthetic%20data%20and%20deployed%20zero-shot%20in%20real-world%20closed-loop%20driving%20scenarios%2C%20LiDAS%20enables%20%2B18.7%25%20mAP50%20and%20%2B5.0%25%20mIoU%20over%20standard%20low-beam%20at%20equal%20power.%20It%20maintains%20performances%20while%20reducing%20energy%20use%20by%2040%25.%20LiDAS%20complements%20domain-generalization%20methods%2C%20further%20strengthening%20robustness%20without%20retraining.%20By%20turning%20readily%20available%20headlights%20into%20active%20vision%20actuators%2C%20LiDAS%20offers%20a%20cost-effective%20solution%20to%20robust%20nighttime%20perception.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08912v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLiDAS%253A%2520Lighting-driven%2520Dynamic%2520Active%2520Sensing%2520for%2520Nighttime%2520Perception%26entry.906535625%3DSimon%2520de%2520Moreau%2520and%2520Andrei%2520Bursuc%2520and%2520Hafid%2520El-Idrissi%2520and%2520Fabien%2520Moutarde%26entry.1292438233%3DNighttime%2520environments%2520pose%2520significant%2520challenges%2520for%2520camera-based%2520perception%252C%2520as%2520existing%2520methods%2520passively%2520rely%2520on%2520the%2520scene%2520lighting.%2520We%2520introduce%2520Lighting-driven%2520Dynamic%2520Active%2520Sensing%2520%2528LiDAS%2529%252C%2520a%2520closed-loop%2520active%2520illumination%2520system%2520that%2520combines%2520off-the-shelf%2520visual%2520perception%2520models%2520with%2520high-definition%2520headlights.%2520Rather%2520than%2520uniformly%2520brightening%2520the%2520scene%252C%2520LiDAS%2520dynamically%2520predicts%2520an%2520optimal%2520illumination%2520field%2520that%2520maximizes%2520downstream%2520perception%2520performance%252C%2520i.e.%252C%2520decreasing%2520light%2520on%2520empty%2520areas%2520to%2520reallocate%2520it%2520on%2520object%2520regions.%2520LiDAS%2520enables%2520zero-shot%2520nighttime%2520generalization%2520of%2520daytime-trained%2520models%2520through%2520adaptive%2520illumination%2520control.%2520Trained%2520on%2520synthetic%2520data%2520and%2520deployed%2520zero-shot%2520in%2520real-world%2520closed-loop%2520driving%2520scenarios%252C%2520LiDAS%2520enables%2520%252B18.7%2525%2520mAP50%2520and%2520%252B5.0%2525%2520mIoU%2520over%2520standard%2520low-beam%2520at%2520equal%2520power.%2520It%2520maintains%2520performances%2520while%2520reducing%2520energy%2520use%2520by%252040%2525.%2520LiDAS%2520complements%2520domain-generalization%2520methods%252C%2520further%2520strengthening%2520robustness%2520without%2520retraining.%2520By%2520turning%2520readily%2520available%2520headlights%2520into%2520active%2520vision%2520actuators%252C%2520LiDAS%2520offers%2520a%2520cost-effective%2520solution%2520to%2520robust%2520nighttime%2520perception.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08912v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LiDAS%3A%20Lighting-driven%20Dynamic%20Active%20Sensing%20for%20Nighttime%20Perception&entry.906535625=Simon%20de%20Moreau%20and%20Andrei%20Bursuc%20and%20Hafid%20El-Idrissi%20and%20Fabien%20Moutarde&entry.1292438233=Nighttime%20environments%20pose%20significant%20challenges%20for%20camera-based%20perception%2C%20as%20existing%20methods%20passively%20rely%20on%20the%20scene%20lighting.%20We%20introduce%20Lighting-driven%20Dynamic%20Active%20Sensing%20%28LiDAS%29%2C%20a%20closed-loop%20active%20illumination%20system%20that%20combines%20off-the-shelf%20visual%20perception%20models%20with%20high-definition%20headlights.%20Rather%20than%20uniformly%20brightening%20the%20scene%2C%20LiDAS%20dynamically%20predicts%20an%20optimal%20illumination%20field%20that%20maximizes%20downstream%20perception%20performance%2C%20i.e.%2C%20decreasing%20light%20on%20empty%20areas%20to%20reallocate%20it%20on%20object%20regions.%20LiDAS%20enables%20zero-shot%20nighttime%20generalization%20of%20daytime-trained%20models%20through%20adaptive%20illumination%20control.%20Trained%20on%20synthetic%20data%20and%20deployed%20zero-shot%20in%20real-world%20closed-loop%20driving%20scenarios%2C%20LiDAS%20enables%20%2B18.7%25%20mAP50%20and%20%2B5.0%25%20mIoU%20over%20standard%20low-beam%20at%20equal%20power.%20It%20maintains%20performances%20while%20reducing%20energy%20use%20by%2040%25.%20LiDAS%20complements%20domain-generalization%20methods%2C%20further%20strengthening%20robustness%20without%20retraining.%20By%20turning%20readily%20available%20headlights%20into%20active%20vision%20actuators%2C%20LiDAS%20offers%20a%20cost-effective%20solution%20to%20robust%20nighttime%20perception.&entry.1838667208=http%3A//arxiv.org/abs/2512.08912v1&entry.124074799=Read"},
{"title": "Self-Evolving 3D Scene Generation from a Single Image", "author": "Kaizhi Zheng and Yue Fan and Jing Gu and Zishuo Xu and Xuehai He and Xin Eric Wang", "abstract": "Generating high-quality, textured 3D scenes from a single image remains a fundamental challenge in vision and graphics. Recent image-to-3D generators recover reasonable geometry from single views, but their object-centric training limits generalization to complex, large-scale scenes with faithful structure and texture. We present EvoScene, a self-evolving, training-free framework that progressively reconstructs complete 3D scenes from single images. The key idea is combining the complementary strengths of existing models: geometric reasoning from 3D generation models and visual knowledge from video generation models. Through three iterative stages--Spatial Prior Initialization, Visual-guided 3D Scene Mesh Generation, and Spatial-guided Novel View Generation--EvoScene alternates between 2D and 3D domains, gradually improving both structure and appearance. Experiments on diverse scenes demonstrate that EvoScene achieves superior geometric stability, view-consistent textures, and unseen-region completion compared to strong baselines, producing ready-to-use 3D meshes for practical applications.", "link": "http://arxiv.org/abs/2512.08905v1", "date": "2025-12-09", "relevancy": 2.6051, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.6858}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6444}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Self-Evolving%203D%20Scene%20Generation%20from%20a%20Single%20Image&body=Title%3A%20Self-Evolving%203D%20Scene%20Generation%20from%20a%20Single%20Image%0AAuthor%3A%20Kaizhi%20Zheng%20and%20Yue%20Fan%20and%20Jing%20Gu%20and%20Zishuo%20Xu%20and%20Xuehai%20He%20and%20Xin%20Eric%20Wang%0AAbstract%3A%20Generating%20high-quality%2C%20textured%203D%20scenes%20from%20a%20single%20image%20remains%20a%20fundamental%20challenge%20in%20vision%20and%20graphics.%20Recent%20image-to-3D%20generators%20recover%20reasonable%20geometry%20from%20single%20views%2C%20but%20their%20object-centric%20training%20limits%20generalization%20to%20complex%2C%20large-scale%20scenes%20with%20faithful%20structure%20and%20texture.%20We%20present%20EvoScene%2C%20a%20self-evolving%2C%20training-free%20framework%20that%20progressively%20reconstructs%20complete%203D%20scenes%20from%20single%20images.%20The%20key%20idea%20is%20combining%20the%20complementary%20strengths%20of%20existing%20models%3A%20geometric%20reasoning%20from%203D%20generation%20models%20and%20visual%20knowledge%20from%20video%20generation%20models.%20Through%20three%20iterative%20stages--Spatial%20Prior%20Initialization%2C%20Visual-guided%203D%20Scene%20Mesh%20Generation%2C%20and%20Spatial-guided%20Novel%20View%20Generation--EvoScene%20alternates%20between%202D%20and%203D%20domains%2C%20gradually%20improving%20both%20structure%20and%20appearance.%20Experiments%20on%20diverse%20scenes%20demonstrate%20that%20EvoScene%20achieves%20superior%20geometric%20stability%2C%20view-consistent%20textures%2C%20and%20unseen-region%20completion%20compared%20to%20strong%20baselines%2C%20producing%20ready-to-use%203D%20meshes%20for%20practical%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08905v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSelf-Evolving%25203D%2520Scene%2520Generation%2520from%2520a%2520Single%2520Image%26entry.906535625%3DKaizhi%2520Zheng%2520and%2520Yue%2520Fan%2520and%2520Jing%2520Gu%2520and%2520Zishuo%2520Xu%2520and%2520Xuehai%2520He%2520and%2520Xin%2520Eric%2520Wang%26entry.1292438233%3DGenerating%2520high-quality%252C%2520textured%25203D%2520scenes%2520from%2520a%2520single%2520image%2520remains%2520a%2520fundamental%2520challenge%2520in%2520vision%2520and%2520graphics.%2520Recent%2520image-to-3D%2520generators%2520recover%2520reasonable%2520geometry%2520from%2520single%2520views%252C%2520but%2520their%2520object-centric%2520training%2520limits%2520generalization%2520to%2520complex%252C%2520large-scale%2520scenes%2520with%2520faithful%2520structure%2520and%2520texture.%2520We%2520present%2520EvoScene%252C%2520a%2520self-evolving%252C%2520training-free%2520framework%2520that%2520progressively%2520reconstructs%2520complete%25203D%2520scenes%2520from%2520single%2520images.%2520The%2520key%2520idea%2520is%2520combining%2520the%2520complementary%2520strengths%2520of%2520existing%2520models%253A%2520geometric%2520reasoning%2520from%25203D%2520generation%2520models%2520and%2520visual%2520knowledge%2520from%2520video%2520generation%2520models.%2520Through%2520three%2520iterative%2520stages--Spatial%2520Prior%2520Initialization%252C%2520Visual-guided%25203D%2520Scene%2520Mesh%2520Generation%252C%2520and%2520Spatial-guided%2520Novel%2520View%2520Generation--EvoScene%2520alternates%2520between%25202D%2520and%25203D%2520domains%252C%2520gradually%2520improving%2520both%2520structure%2520and%2520appearance.%2520Experiments%2520on%2520diverse%2520scenes%2520demonstrate%2520that%2520EvoScene%2520achieves%2520superior%2520geometric%2520stability%252C%2520view-consistent%2520textures%252C%2520and%2520unseen-region%2520completion%2520compared%2520to%2520strong%2520baselines%252C%2520producing%2520ready-to-use%25203D%2520meshes%2520for%2520practical%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08905v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Self-Evolving%203D%20Scene%20Generation%20from%20a%20Single%20Image&entry.906535625=Kaizhi%20Zheng%20and%20Yue%20Fan%20and%20Jing%20Gu%20and%20Zishuo%20Xu%20and%20Xuehai%20He%20and%20Xin%20Eric%20Wang&entry.1292438233=Generating%20high-quality%2C%20textured%203D%20scenes%20from%20a%20single%20image%20remains%20a%20fundamental%20challenge%20in%20vision%20and%20graphics.%20Recent%20image-to-3D%20generators%20recover%20reasonable%20geometry%20from%20single%20views%2C%20but%20their%20object-centric%20training%20limits%20generalization%20to%20complex%2C%20large-scale%20scenes%20with%20faithful%20structure%20and%20texture.%20We%20present%20EvoScene%2C%20a%20self-evolving%2C%20training-free%20framework%20that%20progressively%20reconstructs%20complete%203D%20scenes%20from%20single%20images.%20The%20key%20idea%20is%20combining%20the%20complementary%20strengths%20of%20existing%20models%3A%20geometric%20reasoning%20from%203D%20generation%20models%20and%20visual%20knowledge%20from%20video%20generation%20models.%20Through%20three%20iterative%20stages--Spatial%20Prior%20Initialization%2C%20Visual-guided%203D%20Scene%20Mesh%20Generation%2C%20and%20Spatial-guided%20Novel%20View%20Generation--EvoScene%20alternates%20between%202D%20and%203D%20domains%2C%20gradually%20improving%20both%20structure%20and%20appearance.%20Experiments%20on%20diverse%20scenes%20demonstrate%20that%20EvoScene%20achieves%20superior%20geometric%20stability%2C%20view-consistent%20textures%2C%20and%20unseen-region%20completion%20compared%20to%20strong%20baselines%2C%20producing%20ready-to-use%203D%20meshes%20for%20practical%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2512.08905v1&entry.124074799=Read"},
{"title": "Do Natural Language Descriptions of Model Activations Convey Privileged Information?", "author": "Millicent Li and Alberto Mario Ceballos Arroyo and Giordano Rogers and Naomi Saphra and Byron C. Wallace", "abstract": "Recent interpretability methods have proposed to translate LLM internal representations into natural language descriptions using a second verbalizer LLM. This is intended to illuminate how the target model represents and operates on inputs. But do such activation verbalization approaches actually provide privileged knowledge about the internal workings of the target model, or do they merely convey information about its inputs? We critically evaluate popular verbalization methods across datasets used in prior work and find that they can succeed at benchmarks without any access to target model internals, suggesting that these datasets may not be ideal for evaluating verbalization methods. We then run controlled experiments which reveal that verbalizations often reflect the parametric knowledge of the verbalizer LLM which generated them, rather than the knowledge of the target LLM whose activations are decoded. Taken together, our results indicate a need for targeted benchmarks and experimental controls to rigorously assess whether verbalization methods provide meaningful insights into the operations of LLMs.", "link": "http://arxiv.org/abs/2509.13316v3", "date": "2025-12-09", "relevancy": 2.5854, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5455}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4603}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Do%20Natural%20Language%20Descriptions%20of%20Model%20Activations%20Convey%20Privileged%20Information%3F&body=Title%3A%20Do%20Natural%20Language%20Descriptions%20of%20Model%20Activations%20Convey%20Privileged%20Information%3F%0AAuthor%3A%20Millicent%20Li%20and%20Alberto%20Mario%20Ceballos%20Arroyo%20and%20Giordano%20Rogers%20and%20Naomi%20Saphra%20and%20Byron%20C.%20Wallace%0AAbstract%3A%20Recent%20interpretability%20methods%20have%20proposed%20to%20translate%20LLM%20internal%20representations%20into%20natural%20language%20descriptions%20using%20a%20second%20verbalizer%20LLM.%20This%20is%20intended%20to%20illuminate%20how%20the%20target%20model%20represents%20and%20operates%20on%20inputs.%20But%20do%20such%20activation%20verbalization%20approaches%20actually%20provide%20privileged%20knowledge%20about%20the%20internal%20workings%20of%20the%20target%20model%2C%20or%20do%20they%20merely%20convey%20information%20about%20its%20inputs%3F%20We%20critically%20evaluate%20popular%20verbalization%20methods%20across%20datasets%20used%20in%20prior%20work%20and%20find%20that%20they%20can%20succeed%20at%20benchmarks%20without%20any%20access%20to%20target%20model%20internals%2C%20suggesting%20that%20these%20datasets%20may%20not%20be%20ideal%20for%20evaluating%20verbalization%20methods.%20We%20then%20run%20controlled%20experiments%20which%20reveal%20that%20verbalizations%20often%20reflect%20the%20parametric%20knowledge%20of%20the%20verbalizer%20LLM%20which%20generated%20them%2C%20rather%20than%20the%20knowledge%20of%20the%20target%20LLM%20whose%20activations%20are%20decoded.%20Taken%20together%2C%20our%20results%20indicate%20a%20need%20for%20targeted%20benchmarks%20and%20experimental%20controls%20to%20rigorously%20assess%20whether%20verbalization%20methods%20provide%20meaningful%20insights%20into%20the%20operations%20of%20LLMs.%0ALink%3A%20http%3A//arxiv.org/abs/2509.13316v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDo%2520Natural%2520Language%2520Descriptions%2520of%2520Model%2520Activations%2520Convey%2520Privileged%2520Information%253F%26entry.906535625%3DMillicent%2520Li%2520and%2520Alberto%2520Mario%2520Ceballos%2520Arroyo%2520and%2520Giordano%2520Rogers%2520and%2520Naomi%2520Saphra%2520and%2520Byron%2520C.%2520Wallace%26entry.1292438233%3DRecent%2520interpretability%2520methods%2520have%2520proposed%2520to%2520translate%2520LLM%2520internal%2520representations%2520into%2520natural%2520language%2520descriptions%2520using%2520a%2520second%2520verbalizer%2520LLM.%2520This%2520is%2520intended%2520to%2520illuminate%2520how%2520the%2520target%2520model%2520represents%2520and%2520operates%2520on%2520inputs.%2520But%2520do%2520such%2520activation%2520verbalization%2520approaches%2520actually%2520provide%2520privileged%2520knowledge%2520about%2520the%2520internal%2520workings%2520of%2520the%2520target%2520model%252C%2520or%2520do%2520they%2520merely%2520convey%2520information%2520about%2520its%2520inputs%253F%2520We%2520critically%2520evaluate%2520popular%2520verbalization%2520methods%2520across%2520datasets%2520used%2520in%2520prior%2520work%2520and%2520find%2520that%2520they%2520can%2520succeed%2520at%2520benchmarks%2520without%2520any%2520access%2520to%2520target%2520model%2520internals%252C%2520suggesting%2520that%2520these%2520datasets%2520may%2520not%2520be%2520ideal%2520for%2520evaluating%2520verbalization%2520methods.%2520We%2520then%2520run%2520controlled%2520experiments%2520which%2520reveal%2520that%2520verbalizations%2520often%2520reflect%2520the%2520parametric%2520knowledge%2520of%2520the%2520verbalizer%2520LLM%2520which%2520generated%2520them%252C%2520rather%2520than%2520the%2520knowledge%2520of%2520the%2520target%2520LLM%2520whose%2520activations%2520are%2520decoded.%2520Taken%2520together%252C%2520our%2520results%2520indicate%2520a%2520need%2520for%2520targeted%2520benchmarks%2520and%2520experimental%2520controls%2520to%2520rigorously%2520assess%2520whether%2520verbalization%2520methods%2520provide%2520meaningful%2520insights%2520into%2520the%2520operations%2520of%2520LLMs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.13316v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Do%20Natural%20Language%20Descriptions%20of%20Model%20Activations%20Convey%20Privileged%20Information%3F&entry.906535625=Millicent%20Li%20and%20Alberto%20Mario%20Ceballos%20Arroyo%20and%20Giordano%20Rogers%20and%20Naomi%20Saphra%20and%20Byron%20C.%20Wallace&entry.1292438233=Recent%20interpretability%20methods%20have%20proposed%20to%20translate%20LLM%20internal%20representations%20into%20natural%20language%20descriptions%20using%20a%20second%20verbalizer%20LLM.%20This%20is%20intended%20to%20illuminate%20how%20the%20target%20model%20represents%20and%20operates%20on%20inputs.%20But%20do%20such%20activation%20verbalization%20approaches%20actually%20provide%20privileged%20knowledge%20about%20the%20internal%20workings%20of%20the%20target%20model%2C%20or%20do%20they%20merely%20convey%20information%20about%20its%20inputs%3F%20We%20critically%20evaluate%20popular%20verbalization%20methods%20across%20datasets%20used%20in%20prior%20work%20and%20find%20that%20they%20can%20succeed%20at%20benchmarks%20without%20any%20access%20to%20target%20model%20internals%2C%20suggesting%20that%20these%20datasets%20may%20not%20be%20ideal%20for%20evaluating%20verbalization%20methods.%20We%20then%20run%20controlled%20experiments%20which%20reveal%20that%20verbalizations%20often%20reflect%20the%20parametric%20knowledge%20of%20the%20verbalizer%20LLM%20which%20generated%20them%2C%20rather%20than%20the%20knowledge%20of%20the%20target%20LLM%20whose%20activations%20are%20decoded.%20Taken%20together%2C%20our%20results%20indicate%20a%20need%20for%20targeted%20benchmarks%20and%20experimental%20controls%20to%20rigorously%20assess%20whether%20verbalization%20methods%20provide%20meaningful%20insights%20into%20the%20operations%20of%20LLMs.&entry.1838667208=http%3A//arxiv.org/abs/2509.13316v3&entry.124074799=Read"},
{"title": "UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs", "author": "Yizhuo Ding and Wanying Qu and Jiawei Geng and Wenqi Shao and Yanwei Fu", "abstract": "Large Language Models (LLMs) achieve strong performance across diverse tasks but face prohibitive computational and memory costs. Pruning offers a promising path by inducing sparsity while preserving architectural flexibility. However, existing methods struggle to balance efficiency and robustness: local metric approaches prune layer by layer but often collapse under high sparsity, whereas global feedback methods enforce consistency at the cost of expensive weight updates or restrictive semi-structured formats. We present UniPruning, a unified post-training pruning framework that combines the speed of local saliency metrics with the stability of global coordination, enabled by a mirror descent based optimization, all without updating model weights. UniPruning leverages fast layer-wise scoring and a lightweight global controller to allocate a single sparsity budget, supporting both unstructured and semi-structured N :M pruning within one framework. After a brief calibration, it can generate pruning masks for arbitrary sparsity levels in one shot, and adapts seamlessly to hardware-aware constraints. Extensive experiments on multiple pretrained LLM families and standard benchmarks show that UniPruning consistently delivers competitive or superior perplexity and zero-shot accuracy. Ablation studies further highlight the importance of mirror descent and local saliency anchoring. Overall, UniPruning provides an efficient, principled, and scalable solution for sparsifying large-scale LLMs. Our code is available at: https://github.com/RainbowQTT/UniPruning.", "link": "http://arxiv.org/abs/2510.03291v2", "date": "2025-12-09", "relevancy": 2.5503, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5175}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5116}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5011}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UniPruning%3A%20Unifying%20Local%20Metric%20and%20Global%20Feedback%20for%20Scalable%20Sparse%20LLMs&body=Title%3A%20UniPruning%3A%20Unifying%20Local%20Metric%20and%20Global%20Feedback%20for%20Scalable%20Sparse%20LLMs%0AAuthor%3A%20Yizhuo%20Ding%20and%20Wanying%20Qu%20and%20Jiawei%20Geng%20and%20Wenqi%20Shao%20and%20Yanwei%20Fu%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20achieve%20strong%20performance%20across%20diverse%20tasks%20but%20face%20prohibitive%20computational%20and%20memory%20costs.%20Pruning%20offers%20a%20promising%20path%20by%20inducing%20sparsity%20while%20preserving%20architectural%20flexibility.%20However%2C%20existing%20methods%20struggle%20to%20balance%20efficiency%20and%20robustness%3A%20local%20metric%20approaches%20prune%20layer%20by%20layer%20but%20often%20collapse%20under%20high%20sparsity%2C%20whereas%20global%20feedback%20methods%20enforce%20consistency%20at%20the%20cost%20of%20expensive%20weight%20updates%20or%20restrictive%20semi-structured%20formats.%20We%20present%20UniPruning%2C%20a%20unified%20post-training%20pruning%20framework%20that%20combines%20the%20speed%20of%20local%20saliency%20metrics%20with%20the%20stability%20of%20global%20coordination%2C%20enabled%20by%20a%20mirror%20descent%20based%20optimization%2C%20all%20without%20updating%20model%20weights.%20UniPruning%20leverages%20fast%20layer-wise%20scoring%20and%20a%20lightweight%20global%20controller%20to%20allocate%20a%20single%20sparsity%20budget%2C%20supporting%20both%20unstructured%20and%20semi-structured%20N%20%3AM%20pruning%20within%20one%20framework.%20After%20a%20brief%20calibration%2C%20it%20can%20generate%20pruning%20masks%20for%20arbitrary%20sparsity%20levels%20in%20one%20shot%2C%20and%20adapts%20seamlessly%20to%20hardware-aware%20constraints.%20Extensive%20experiments%20on%20multiple%20pretrained%20LLM%20families%20and%20standard%20benchmarks%20show%20that%20UniPruning%20consistently%20delivers%20competitive%20or%20superior%20perplexity%20and%20zero-shot%20accuracy.%20Ablation%20studies%20further%20highlight%20the%20importance%20of%20mirror%20descent%20and%20local%20saliency%20anchoring.%20Overall%2C%20UniPruning%20provides%20an%20efficient%2C%20principled%2C%20and%20scalable%20solution%20for%20sparsifying%20large-scale%20LLMs.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/RainbowQTT/UniPruning.%0ALink%3A%20http%3A//arxiv.org/abs/2510.03291v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUniPruning%253A%2520Unifying%2520Local%2520Metric%2520and%2520Global%2520Feedback%2520for%2520Scalable%2520Sparse%2520LLMs%26entry.906535625%3DYizhuo%2520Ding%2520and%2520Wanying%2520Qu%2520and%2520Jiawei%2520Geng%2520and%2520Wenqi%2520Shao%2520and%2520Yanwei%2520Fu%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520achieve%2520strong%2520performance%2520across%2520diverse%2520tasks%2520but%2520face%2520prohibitive%2520computational%2520and%2520memory%2520costs.%2520Pruning%2520offers%2520a%2520promising%2520path%2520by%2520inducing%2520sparsity%2520while%2520preserving%2520architectural%2520flexibility.%2520However%252C%2520existing%2520methods%2520struggle%2520to%2520balance%2520efficiency%2520and%2520robustness%253A%2520local%2520metric%2520approaches%2520prune%2520layer%2520by%2520layer%2520but%2520often%2520collapse%2520under%2520high%2520sparsity%252C%2520whereas%2520global%2520feedback%2520methods%2520enforce%2520consistency%2520at%2520the%2520cost%2520of%2520expensive%2520weight%2520updates%2520or%2520restrictive%2520semi-structured%2520formats.%2520We%2520present%2520UniPruning%252C%2520a%2520unified%2520post-training%2520pruning%2520framework%2520that%2520combines%2520the%2520speed%2520of%2520local%2520saliency%2520metrics%2520with%2520the%2520stability%2520of%2520global%2520coordination%252C%2520enabled%2520by%2520a%2520mirror%2520descent%2520based%2520optimization%252C%2520all%2520without%2520updating%2520model%2520weights.%2520UniPruning%2520leverages%2520fast%2520layer-wise%2520scoring%2520and%2520a%2520lightweight%2520global%2520controller%2520to%2520allocate%2520a%2520single%2520sparsity%2520budget%252C%2520supporting%2520both%2520unstructured%2520and%2520semi-structured%2520N%2520%253AM%2520pruning%2520within%2520one%2520framework.%2520After%2520a%2520brief%2520calibration%252C%2520it%2520can%2520generate%2520pruning%2520masks%2520for%2520arbitrary%2520sparsity%2520levels%2520in%2520one%2520shot%252C%2520and%2520adapts%2520seamlessly%2520to%2520hardware-aware%2520constraints.%2520Extensive%2520experiments%2520on%2520multiple%2520pretrained%2520LLM%2520families%2520and%2520standard%2520benchmarks%2520show%2520that%2520UniPruning%2520consistently%2520delivers%2520competitive%2520or%2520superior%2520perplexity%2520and%2520zero-shot%2520accuracy.%2520Ablation%2520studies%2520further%2520highlight%2520the%2520importance%2520of%2520mirror%2520descent%2520and%2520local%2520saliency%2520anchoring.%2520Overall%252C%2520UniPruning%2520provides%2520an%2520efficient%252C%2520principled%252C%2520and%2520scalable%2520solution%2520for%2520sparsifying%2520large-scale%2520LLMs.%2520Our%2520code%2520is%2520available%2520at%253A%2520https%253A//github.com/RainbowQTT/UniPruning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.03291v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UniPruning%3A%20Unifying%20Local%20Metric%20and%20Global%20Feedback%20for%20Scalable%20Sparse%20LLMs&entry.906535625=Yizhuo%20Ding%20and%20Wanying%20Qu%20and%20Jiawei%20Geng%20and%20Wenqi%20Shao%20and%20Yanwei%20Fu&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20achieve%20strong%20performance%20across%20diverse%20tasks%20but%20face%20prohibitive%20computational%20and%20memory%20costs.%20Pruning%20offers%20a%20promising%20path%20by%20inducing%20sparsity%20while%20preserving%20architectural%20flexibility.%20However%2C%20existing%20methods%20struggle%20to%20balance%20efficiency%20and%20robustness%3A%20local%20metric%20approaches%20prune%20layer%20by%20layer%20but%20often%20collapse%20under%20high%20sparsity%2C%20whereas%20global%20feedback%20methods%20enforce%20consistency%20at%20the%20cost%20of%20expensive%20weight%20updates%20or%20restrictive%20semi-structured%20formats.%20We%20present%20UniPruning%2C%20a%20unified%20post-training%20pruning%20framework%20that%20combines%20the%20speed%20of%20local%20saliency%20metrics%20with%20the%20stability%20of%20global%20coordination%2C%20enabled%20by%20a%20mirror%20descent%20based%20optimization%2C%20all%20without%20updating%20model%20weights.%20UniPruning%20leverages%20fast%20layer-wise%20scoring%20and%20a%20lightweight%20global%20controller%20to%20allocate%20a%20single%20sparsity%20budget%2C%20supporting%20both%20unstructured%20and%20semi-structured%20N%20%3AM%20pruning%20within%20one%20framework.%20After%20a%20brief%20calibration%2C%20it%20can%20generate%20pruning%20masks%20for%20arbitrary%20sparsity%20levels%20in%20one%20shot%2C%20and%20adapts%20seamlessly%20to%20hardware-aware%20constraints.%20Extensive%20experiments%20on%20multiple%20pretrained%20LLM%20families%20and%20standard%20benchmarks%20show%20that%20UniPruning%20consistently%20delivers%20competitive%20or%20superior%20perplexity%20and%20zero-shot%20accuracy.%20Ablation%20studies%20further%20highlight%20the%20importance%20of%20mirror%20descent%20and%20local%20saliency%20anchoring.%20Overall%2C%20UniPruning%20provides%20an%20efficient%2C%20principled%2C%20and%20scalable%20solution%20for%20sparsifying%20large-scale%20LLMs.%20Our%20code%20is%20available%20at%3A%20https%3A//github.com/RainbowQTT/UniPruning.&entry.1838667208=http%3A//arxiv.org/abs/2510.03291v2&entry.124074799=Read"},
{"title": "From Fibers to Cells: Fourier-Based Registration Enables Virtual Cresyl Violet Staining From 3D Polarized Light Imaging", "author": "Alexander Oberstrass and Esteban Vaca and Eric Upschulte and Meiqi Niu and Nicola Palomero-Gallagher and David Graessel and Christian Schiffer and Markus Axer and Katrin Amunts and Timo Dickscheid", "abstract": "Comprehensive assessment of the various aspects of the brain's microstructure requires the use of complementary imaging techniques. This includes measuring the spatial distribution of cell bodies (cytoarchitecture) and nerve fibers (myeloarchitecture). The gold standard for cytoarchitectonic analysis is light microscopic imaging of cell-body stained tissue sections. To reveal the 3D orientations of nerve fibers, 3D Polarized Light Imaging (3D-PLI) has been introduced, a method that is label-free and allows subsequent staining of sections after 3D-PLI measurement. By post-staining for cell bodies, a direct link between fiber- and cytoarchitecture can potentially be established in the same section. However, inevitable distortions introduced during the staining process make a costly nonlinear and cross-modal registration necessary in order to study the detailed relationships between cells and fibers in the images. In addition, the complexity of processing histological sections for post-staining only allows for a limited number of such samples. In this work, we take advantage of deep learning methods for image-to-image translation to generate a virtual staining of 3D-PLI that is spatially aligned at the cellular level. We use a supervised setting, building on a unique dataset of brain sections, to which Cresyl violet staining has been applied after 3D-PLI measurement. To ensure high correspondence between both modalities, we address the misalignment of training data using Fourier-based registration. In this way, registration can be efficiently calculated during training for local image patches of target and predicted staining. We demonstrate that the proposed method can predict a Cresyl violet staining from 3D-PLI, resulting in a virtual staining that exhibits plausible patterns of cell organization in gray matter, with larger cell bodies being localized at their expected positions.", "link": "http://arxiv.org/abs/2505.11394v2", "date": "2025-12-09", "relevancy": 2.5482, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5175}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5057}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Fibers%20to%20Cells%3A%20Fourier-Based%20Registration%20Enables%20Virtual%20Cresyl%20Violet%20Staining%20From%203D%20Polarized%20Light%20Imaging&body=Title%3A%20From%20Fibers%20to%20Cells%3A%20Fourier-Based%20Registration%20Enables%20Virtual%20Cresyl%20Violet%20Staining%20From%203D%20Polarized%20Light%20Imaging%0AAuthor%3A%20Alexander%20Oberstrass%20and%20Esteban%20Vaca%20and%20Eric%20Upschulte%20and%20Meiqi%20Niu%20and%20Nicola%20Palomero-Gallagher%20and%20David%20Graessel%20and%20Christian%20Schiffer%20and%20Markus%20Axer%20and%20Katrin%20Amunts%20and%20Timo%20Dickscheid%0AAbstract%3A%20Comprehensive%20assessment%20of%20the%20various%20aspects%20of%20the%20brain%27s%20microstructure%20requires%20the%20use%20of%20complementary%20imaging%20techniques.%20This%20includes%20measuring%20the%20spatial%20distribution%20of%20cell%20bodies%20%28cytoarchitecture%29%20and%20nerve%20fibers%20%28myeloarchitecture%29.%20The%20gold%20standard%20for%20cytoarchitectonic%20analysis%20is%20light%20microscopic%20imaging%20of%20cell-body%20stained%20tissue%20sections.%20To%20reveal%20the%203D%20orientations%20of%20nerve%20fibers%2C%203D%20Polarized%20Light%20Imaging%20%283D-PLI%29%20has%20been%20introduced%2C%20a%20method%20that%20is%20label-free%20and%20allows%20subsequent%20staining%20of%20sections%20after%203D-PLI%20measurement.%20By%20post-staining%20for%20cell%20bodies%2C%20a%20direct%20link%20between%20fiber-%20and%20cytoarchitecture%20can%20potentially%20be%20established%20in%20the%20same%20section.%20However%2C%20inevitable%20distortions%20introduced%20during%20the%20staining%20process%20make%20a%20costly%20nonlinear%20and%20cross-modal%20registration%20necessary%20in%20order%20to%20study%20the%20detailed%20relationships%20between%20cells%20and%20fibers%20in%20the%20images.%20In%20addition%2C%20the%20complexity%20of%20processing%20histological%20sections%20for%20post-staining%20only%20allows%20for%20a%20limited%20number%20of%20such%20samples.%20In%20this%20work%2C%20we%20take%20advantage%20of%20deep%20learning%20methods%20for%20image-to-image%20translation%20to%20generate%20a%20virtual%20staining%20of%203D-PLI%20that%20is%20spatially%20aligned%20at%20the%20cellular%20level.%20We%20use%20a%20supervised%20setting%2C%20building%20on%20a%20unique%20dataset%20of%20brain%20sections%2C%20to%20which%20Cresyl%20violet%20staining%20has%20been%20applied%20after%203D-PLI%20measurement.%20To%20ensure%20high%20correspondence%20between%20both%20modalities%2C%20we%20address%20the%20misalignment%20of%20training%20data%20using%20Fourier-based%20registration.%20In%20this%20way%2C%20registration%20can%20be%20efficiently%20calculated%20during%20training%20for%20local%20image%20patches%20of%20target%20and%20predicted%20staining.%20We%20demonstrate%20that%20the%20proposed%20method%20can%20predict%20a%20Cresyl%20violet%20staining%20from%203D-PLI%2C%20resulting%20in%20a%20virtual%20staining%20that%20exhibits%20plausible%20patterns%20of%20cell%20organization%20in%20gray%20matter%2C%20with%20larger%20cell%20bodies%20being%20localized%20at%20their%20expected%20positions.%0ALink%3A%20http%3A//arxiv.org/abs/2505.11394v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Fibers%2520to%2520Cells%253A%2520Fourier-Based%2520Registration%2520Enables%2520Virtual%2520Cresyl%2520Violet%2520Staining%2520From%25203D%2520Polarized%2520Light%2520Imaging%26entry.906535625%3DAlexander%2520Oberstrass%2520and%2520Esteban%2520Vaca%2520and%2520Eric%2520Upschulte%2520and%2520Meiqi%2520Niu%2520and%2520Nicola%2520Palomero-Gallagher%2520and%2520David%2520Graessel%2520and%2520Christian%2520Schiffer%2520and%2520Markus%2520Axer%2520and%2520Katrin%2520Amunts%2520and%2520Timo%2520Dickscheid%26entry.1292438233%3DComprehensive%2520assessment%2520of%2520the%2520various%2520aspects%2520of%2520the%2520brain%2527s%2520microstructure%2520requires%2520the%2520use%2520of%2520complementary%2520imaging%2520techniques.%2520This%2520includes%2520measuring%2520the%2520spatial%2520distribution%2520of%2520cell%2520bodies%2520%2528cytoarchitecture%2529%2520and%2520nerve%2520fibers%2520%2528myeloarchitecture%2529.%2520The%2520gold%2520standard%2520for%2520cytoarchitectonic%2520analysis%2520is%2520light%2520microscopic%2520imaging%2520of%2520cell-body%2520stained%2520tissue%2520sections.%2520To%2520reveal%2520the%25203D%2520orientations%2520of%2520nerve%2520fibers%252C%25203D%2520Polarized%2520Light%2520Imaging%2520%25283D-PLI%2529%2520has%2520been%2520introduced%252C%2520a%2520method%2520that%2520is%2520label-free%2520and%2520allows%2520subsequent%2520staining%2520of%2520sections%2520after%25203D-PLI%2520measurement.%2520By%2520post-staining%2520for%2520cell%2520bodies%252C%2520a%2520direct%2520link%2520between%2520fiber-%2520and%2520cytoarchitecture%2520can%2520potentially%2520be%2520established%2520in%2520the%2520same%2520section.%2520However%252C%2520inevitable%2520distortions%2520introduced%2520during%2520the%2520staining%2520process%2520make%2520a%2520costly%2520nonlinear%2520and%2520cross-modal%2520registration%2520necessary%2520in%2520order%2520to%2520study%2520the%2520detailed%2520relationships%2520between%2520cells%2520and%2520fibers%2520in%2520the%2520images.%2520In%2520addition%252C%2520the%2520complexity%2520of%2520processing%2520histological%2520sections%2520for%2520post-staining%2520only%2520allows%2520for%2520a%2520limited%2520number%2520of%2520such%2520samples.%2520In%2520this%2520work%252C%2520we%2520take%2520advantage%2520of%2520deep%2520learning%2520methods%2520for%2520image-to-image%2520translation%2520to%2520generate%2520a%2520virtual%2520staining%2520of%25203D-PLI%2520that%2520is%2520spatially%2520aligned%2520at%2520the%2520cellular%2520level.%2520We%2520use%2520a%2520supervised%2520setting%252C%2520building%2520on%2520a%2520unique%2520dataset%2520of%2520brain%2520sections%252C%2520to%2520which%2520Cresyl%2520violet%2520staining%2520has%2520been%2520applied%2520after%25203D-PLI%2520measurement.%2520To%2520ensure%2520high%2520correspondence%2520between%2520both%2520modalities%252C%2520we%2520address%2520the%2520misalignment%2520of%2520training%2520data%2520using%2520Fourier-based%2520registration.%2520In%2520this%2520way%252C%2520registration%2520can%2520be%2520efficiently%2520calculated%2520during%2520training%2520for%2520local%2520image%2520patches%2520of%2520target%2520and%2520predicted%2520staining.%2520We%2520demonstrate%2520that%2520the%2520proposed%2520method%2520can%2520predict%2520a%2520Cresyl%2520violet%2520staining%2520from%25203D-PLI%252C%2520resulting%2520in%2520a%2520virtual%2520staining%2520that%2520exhibits%2520plausible%2520patterns%2520of%2520cell%2520organization%2520in%2520gray%2520matter%252C%2520with%2520larger%2520cell%2520bodies%2520being%2520localized%2520at%2520their%2520expected%2520positions.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.11394v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Fibers%20to%20Cells%3A%20Fourier-Based%20Registration%20Enables%20Virtual%20Cresyl%20Violet%20Staining%20From%203D%20Polarized%20Light%20Imaging&entry.906535625=Alexander%20Oberstrass%20and%20Esteban%20Vaca%20and%20Eric%20Upschulte%20and%20Meiqi%20Niu%20and%20Nicola%20Palomero-Gallagher%20and%20David%20Graessel%20and%20Christian%20Schiffer%20and%20Markus%20Axer%20and%20Katrin%20Amunts%20and%20Timo%20Dickscheid&entry.1292438233=Comprehensive%20assessment%20of%20the%20various%20aspects%20of%20the%20brain%27s%20microstructure%20requires%20the%20use%20of%20complementary%20imaging%20techniques.%20This%20includes%20measuring%20the%20spatial%20distribution%20of%20cell%20bodies%20%28cytoarchitecture%29%20and%20nerve%20fibers%20%28myeloarchitecture%29.%20The%20gold%20standard%20for%20cytoarchitectonic%20analysis%20is%20light%20microscopic%20imaging%20of%20cell-body%20stained%20tissue%20sections.%20To%20reveal%20the%203D%20orientations%20of%20nerve%20fibers%2C%203D%20Polarized%20Light%20Imaging%20%283D-PLI%29%20has%20been%20introduced%2C%20a%20method%20that%20is%20label-free%20and%20allows%20subsequent%20staining%20of%20sections%20after%203D-PLI%20measurement.%20By%20post-staining%20for%20cell%20bodies%2C%20a%20direct%20link%20between%20fiber-%20and%20cytoarchitecture%20can%20potentially%20be%20established%20in%20the%20same%20section.%20However%2C%20inevitable%20distortions%20introduced%20during%20the%20staining%20process%20make%20a%20costly%20nonlinear%20and%20cross-modal%20registration%20necessary%20in%20order%20to%20study%20the%20detailed%20relationships%20between%20cells%20and%20fibers%20in%20the%20images.%20In%20addition%2C%20the%20complexity%20of%20processing%20histological%20sections%20for%20post-staining%20only%20allows%20for%20a%20limited%20number%20of%20such%20samples.%20In%20this%20work%2C%20we%20take%20advantage%20of%20deep%20learning%20methods%20for%20image-to-image%20translation%20to%20generate%20a%20virtual%20staining%20of%203D-PLI%20that%20is%20spatially%20aligned%20at%20the%20cellular%20level.%20We%20use%20a%20supervised%20setting%2C%20building%20on%20a%20unique%20dataset%20of%20brain%20sections%2C%20to%20which%20Cresyl%20violet%20staining%20has%20been%20applied%20after%203D-PLI%20measurement.%20To%20ensure%20high%20correspondence%20between%20both%20modalities%2C%20we%20address%20the%20misalignment%20of%20training%20data%20using%20Fourier-based%20registration.%20In%20this%20way%2C%20registration%20can%20be%20efficiently%20calculated%20during%20training%20for%20local%20image%20patches%20of%20target%20and%20predicted%20staining.%20We%20demonstrate%20that%20the%20proposed%20method%20can%20predict%20a%20Cresyl%20violet%20staining%20from%203D-PLI%2C%20resulting%20in%20a%20virtual%20staining%20that%20exhibits%20plausible%20patterns%20of%20cell%20organization%20in%20gray%20matter%2C%20with%20larger%20cell%20bodies%20being%20localized%20at%20their%20expected%20positions.&entry.1838667208=http%3A//arxiv.org/abs/2505.11394v2&entry.124074799=Read"},
{"title": "Towards Foundation Models with Native Multi-Agent Intelligence", "author": "Shuyue Hu and Haoyang Yan and Yiqun Zhang and Yang Chen and Dongzhan Zhou and Lei Bai", "abstract": "Foundation models (FMs) are increasingly assuming the role of the \"brain\" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.", "link": "http://arxiv.org/abs/2512.08743v1", "date": "2025-12-09", "relevancy": 2.5314, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5075}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5057}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Foundation%20Models%20with%20Native%20Multi-Agent%20Intelligence&body=Title%3A%20Towards%20Foundation%20Models%20with%20Native%20Multi-Agent%20Intelligence%0AAuthor%3A%20Shuyue%20Hu%20and%20Haoyang%20Yan%20and%20Yiqun%20Zhang%20and%20Yang%20Chen%20and%20Dongzhan%20Zhou%20and%20Lei%20Bai%0AAbstract%3A%20Foundation%20models%20%28FMs%29%20are%20increasingly%20assuming%20the%20role%20of%20the%20%22brain%22%20of%20AI%20agents.%20While%20recent%20efforts%20have%20begun%20to%20equip%20FMs%20with%20native%20single-agent%20abilities%20--%20such%20as%20GUI%20interaction%20or%20integrated%20tool%20use%20--%20we%20argue%20that%20the%20next%20frontier%20is%20endowing%20FMs%20with%20native%20multi-agent%20intelligence.%20We%20identify%20four%20core%20capabilities%20of%20FMs%20in%20multi-agent%20contexts%3A%20understanding%2C%20planning%2C%20efficient%20communication%2C%20and%20adaptation.%20Contrary%20to%20assumptions%20about%20the%20spontaneous%20emergence%20of%20such%20abilities%2C%20we%20provide%20extensive%20empirical%20evidence%20across%2041%20large%20language%20models%20showing%20that%20strong%20single-agent%20performance%20alone%20does%20not%20automatically%20yield%20robust%20multi-agent%20intelligence.%20To%20address%20this%20gap%2C%20we%20outline%20key%20research%20directions%20--%20spanning%20dataset%20construction%2C%20evaluation%2C%20training%20paradigms%2C%20and%20safety%20considerations%20--%20for%20building%20FMs%20with%20native%20multi-agent%20intelligence.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08743v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Foundation%2520Models%2520with%2520Native%2520Multi-Agent%2520Intelligence%26entry.906535625%3DShuyue%2520Hu%2520and%2520Haoyang%2520Yan%2520and%2520Yiqun%2520Zhang%2520and%2520Yang%2520Chen%2520and%2520Dongzhan%2520Zhou%2520and%2520Lei%2520Bai%26entry.1292438233%3DFoundation%2520models%2520%2528FMs%2529%2520are%2520increasingly%2520assuming%2520the%2520role%2520of%2520the%2520%2522brain%2522%2520of%2520AI%2520agents.%2520While%2520recent%2520efforts%2520have%2520begun%2520to%2520equip%2520FMs%2520with%2520native%2520single-agent%2520abilities%2520--%2520such%2520as%2520GUI%2520interaction%2520or%2520integrated%2520tool%2520use%2520--%2520we%2520argue%2520that%2520the%2520next%2520frontier%2520is%2520endowing%2520FMs%2520with%2520native%2520multi-agent%2520intelligence.%2520We%2520identify%2520four%2520core%2520capabilities%2520of%2520FMs%2520in%2520multi-agent%2520contexts%253A%2520understanding%252C%2520planning%252C%2520efficient%2520communication%252C%2520and%2520adaptation.%2520Contrary%2520to%2520assumptions%2520about%2520the%2520spontaneous%2520emergence%2520of%2520such%2520abilities%252C%2520we%2520provide%2520extensive%2520empirical%2520evidence%2520across%252041%2520large%2520language%2520models%2520showing%2520that%2520strong%2520single-agent%2520performance%2520alone%2520does%2520not%2520automatically%2520yield%2520robust%2520multi-agent%2520intelligence.%2520To%2520address%2520this%2520gap%252C%2520we%2520outline%2520key%2520research%2520directions%2520--%2520spanning%2520dataset%2520construction%252C%2520evaluation%252C%2520training%2520paradigms%252C%2520and%2520safety%2520considerations%2520--%2520for%2520building%2520FMs%2520with%2520native%2520multi-agent%2520intelligence.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08743v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Foundation%20Models%20with%20Native%20Multi-Agent%20Intelligence&entry.906535625=Shuyue%20Hu%20and%20Haoyang%20Yan%20and%20Yiqun%20Zhang%20and%20Yang%20Chen%20and%20Dongzhan%20Zhou%20and%20Lei%20Bai&entry.1292438233=Foundation%20models%20%28FMs%29%20are%20increasingly%20assuming%20the%20role%20of%20the%20%22brain%22%20of%20AI%20agents.%20While%20recent%20efforts%20have%20begun%20to%20equip%20FMs%20with%20native%20single-agent%20abilities%20--%20such%20as%20GUI%20interaction%20or%20integrated%20tool%20use%20--%20we%20argue%20that%20the%20next%20frontier%20is%20endowing%20FMs%20with%20native%20multi-agent%20intelligence.%20We%20identify%20four%20core%20capabilities%20of%20FMs%20in%20multi-agent%20contexts%3A%20understanding%2C%20planning%2C%20efficient%20communication%2C%20and%20adaptation.%20Contrary%20to%20assumptions%20about%20the%20spontaneous%20emergence%20of%20such%20abilities%2C%20we%20provide%20extensive%20empirical%20evidence%20across%2041%20large%20language%20models%20showing%20that%20strong%20single-agent%20performance%20alone%20does%20not%20automatically%20yield%20robust%20multi-agent%20intelligence.%20To%20address%20this%20gap%2C%20we%20outline%20key%20research%20directions%20--%20spanning%20dataset%20construction%2C%20evaluation%2C%20training%20paradigms%2C%20and%20safety%20considerations%20--%20for%20building%20FMs%20with%20native%20multi-agent%20intelligence.&entry.1838667208=http%3A//arxiv.org/abs/2512.08743v1&entry.124074799=Read"},
{"title": "Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement", "author": "Xinyue Liang and Zhinyuan Ma and Lingchen Sun and Yanjun Guo and Lei Zhang", "abstract": "Although recent 3D-native generators have made great progress in synthesizing reliable geometry, they still fall short in achieving realistic appearances. A key obstacle lies in the lack of diverse and high-quality real-world 3D assets with rich texture details, since capturing such data is intrinsically difficult due to the diverse scales of scenes, non-rigid motions of objects, and the limited precision of 3D scanners. We introduce Photo3D, a framework for advancing photorealistic 3D generation, which is driven by the image data generated by the GPT-4o-Image model. Considering that the generated images can distort 3D structures due to their lack of multi-view consistency, we design a structure-aligned multi-view synthesis pipeline and construct a detail-enhanced multi-view dataset paired with 3D geometry. Building on it, we present a realistic detail enhancement scheme that leverages perceptual feature adaptation and semantic structure matching to enforce appearance consistency with realistic details while preserving the structural consistency with the 3D-native geometry. Our scheme is general to different 3D-native generators, and we present dedicated training strategies to facilitate the optimization of geometry-texture coupled and decoupled 3D-native generation paradigms. Experiments demonstrate that Photo3D generalizes well across diverse 3D-native generation paradigms and achieves state-of-the-art photorealistic 3D generation performance.", "link": "http://arxiv.org/abs/2512.08535v1", "date": "2025-12-09", "relevancy": 2.5261, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6354}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6354}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.612}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Photo3D%3A%20Advancing%20Photorealistic%203D%20Generation%20through%20Structure-Aligned%20Detail%20Enhancement&body=Title%3A%20Photo3D%3A%20Advancing%20Photorealistic%203D%20Generation%20through%20Structure-Aligned%20Detail%20Enhancement%0AAuthor%3A%20Xinyue%20Liang%20and%20Zhinyuan%20Ma%20and%20Lingchen%20Sun%20and%20Yanjun%20Guo%20and%20Lei%20Zhang%0AAbstract%3A%20Although%20recent%203D-native%20generators%20have%20made%20great%20progress%20in%20synthesizing%20reliable%20geometry%2C%20they%20still%20fall%20short%20in%20achieving%20realistic%20appearances.%20A%20key%20obstacle%20lies%20in%20the%20lack%20of%20diverse%20and%20high-quality%20real-world%203D%20assets%20with%20rich%20texture%20details%2C%20since%20capturing%20such%20data%20is%20intrinsically%20difficult%20due%20to%20the%20diverse%20scales%20of%20scenes%2C%20non-rigid%20motions%20of%20objects%2C%20and%20the%20limited%20precision%20of%203D%20scanners.%20We%20introduce%20Photo3D%2C%20a%20framework%20for%20advancing%20photorealistic%203D%20generation%2C%20which%20is%20driven%20by%20the%20image%20data%20generated%20by%20the%20GPT-4o-Image%20model.%20Considering%20that%20the%20generated%20images%20can%20distort%203D%20structures%20due%20to%20their%20lack%20of%20multi-view%20consistency%2C%20we%20design%20a%20structure-aligned%20multi-view%20synthesis%20pipeline%20and%20construct%20a%20detail-enhanced%20multi-view%20dataset%20paired%20with%203D%20geometry.%20Building%20on%20it%2C%20we%20present%20a%20realistic%20detail%20enhancement%20scheme%20that%20leverages%20perceptual%20feature%20adaptation%20and%20semantic%20structure%20matching%20to%20enforce%20appearance%20consistency%20with%20realistic%20details%20while%20preserving%20the%20structural%20consistency%20with%20the%203D-native%20geometry.%20Our%20scheme%20is%20general%20to%20different%203D-native%20generators%2C%20and%20we%20present%20dedicated%20training%20strategies%20to%20facilitate%20the%20optimization%20of%20geometry-texture%20coupled%20and%20decoupled%203D-native%20generation%20paradigms.%20Experiments%20demonstrate%20that%20Photo3D%20generalizes%20well%20across%20diverse%203D-native%20generation%20paradigms%20and%20achieves%20state-of-the-art%20photorealistic%203D%20generation%20performance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08535v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPhoto3D%253A%2520Advancing%2520Photorealistic%25203D%2520Generation%2520through%2520Structure-Aligned%2520Detail%2520Enhancement%26entry.906535625%3DXinyue%2520Liang%2520and%2520Zhinyuan%2520Ma%2520and%2520Lingchen%2520Sun%2520and%2520Yanjun%2520Guo%2520and%2520Lei%2520Zhang%26entry.1292438233%3DAlthough%2520recent%25203D-native%2520generators%2520have%2520made%2520great%2520progress%2520in%2520synthesizing%2520reliable%2520geometry%252C%2520they%2520still%2520fall%2520short%2520in%2520achieving%2520realistic%2520appearances.%2520A%2520key%2520obstacle%2520lies%2520in%2520the%2520lack%2520of%2520diverse%2520and%2520high-quality%2520real-world%25203D%2520assets%2520with%2520rich%2520texture%2520details%252C%2520since%2520capturing%2520such%2520data%2520is%2520intrinsically%2520difficult%2520due%2520to%2520the%2520diverse%2520scales%2520of%2520scenes%252C%2520non-rigid%2520motions%2520of%2520objects%252C%2520and%2520the%2520limited%2520precision%2520of%25203D%2520scanners.%2520We%2520introduce%2520Photo3D%252C%2520a%2520framework%2520for%2520advancing%2520photorealistic%25203D%2520generation%252C%2520which%2520is%2520driven%2520by%2520the%2520image%2520data%2520generated%2520by%2520the%2520GPT-4o-Image%2520model.%2520Considering%2520that%2520the%2520generated%2520images%2520can%2520distort%25203D%2520structures%2520due%2520to%2520their%2520lack%2520of%2520multi-view%2520consistency%252C%2520we%2520design%2520a%2520structure-aligned%2520multi-view%2520synthesis%2520pipeline%2520and%2520construct%2520a%2520detail-enhanced%2520multi-view%2520dataset%2520paired%2520with%25203D%2520geometry.%2520Building%2520on%2520it%252C%2520we%2520present%2520a%2520realistic%2520detail%2520enhancement%2520scheme%2520that%2520leverages%2520perceptual%2520feature%2520adaptation%2520and%2520semantic%2520structure%2520matching%2520to%2520enforce%2520appearance%2520consistency%2520with%2520realistic%2520details%2520while%2520preserving%2520the%2520structural%2520consistency%2520with%2520the%25203D-native%2520geometry.%2520Our%2520scheme%2520is%2520general%2520to%2520different%25203D-native%2520generators%252C%2520and%2520we%2520present%2520dedicated%2520training%2520strategies%2520to%2520facilitate%2520the%2520optimization%2520of%2520geometry-texture%2520coupled%2520and%2520decoupled%25203D-native%2520generation%2520paradigms.%2520Experiments%2520demonstrate%2520that%2520Photo3D%2520generalizes%2520well%2520across%2520diverse%25203D-native%2520generation%2520paradigms%2520and%2520achieves%2520state-of-the-art%2520photorealistic%25203D%2520generation%2520performance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08535v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Photo3D%3A%20Advancing%20Photorealistic%203D%20Generation%20through%20Structure-Aligned%20Detail%20Enhancement&entry.906535625=Xinyue%20Liang%20and%20Zhinyuan%20Ma%20and%20Lingchen%20Sun%20and%20Yanjun%20Guo%20and%20Lei%20Zhang&entry.1292438233=Although%20recent%203D-native%20generators%20have%20made%20great%20progress%20in%20synthesizing%20reliable%20geometry%2C%20they%20still%20fall%20short%20in%20achieving%20realistic%20appearances.%20A%20key%20obstacle%20lies%20in%20the%20lack%20of%20diverse%20and%20high-quality%20real-world%203D%20assets%20with%20rich%20texture%20details%2C%20since%20capturing%20such%20data%20is%20intrinsically%20difficult%20due%20to%20the%20diverse%20scales%20of%20scenes%2C%20non-rigid%20motions%20of%20objects%2C%20and%20the%20limited%20precision%20of%203D%20scanners.%20We%20introduce%20Photo3D%2C%20a%20framework%20for%20advancing%20photorealistic%203D%20generation%2C%20which%20is%20driven%20by%20the%20image%20data%20generated%20by%20the%20GPT-4o-Image%20model.%20Considering%20that%20the%20generated%20images%20can%20distort%203D%20structures%20due%20to%20their%20lack%20of%20multi-view%20consistency%2C%20we%20design%20a%20structure-aligned%20multi-view%20synthesis%20pipeline%20and%20construct%20a%20detail-enhanced%20multi-view%20dataset%20paired%20with%203D%20geometry.%20Building%20on%20it%2C%20we%20present%20a%20realistic%20detail%20enhancement%20scheme%20that%20leverages%20perceptual%20feature%20adaptation%20and%20semantic%20structure%20matching%20to%20enforce%20appearance%20consistency%20with%20realistic%20details%20while%20preserving%20the%20structural%20consistency%20with%20the%203D-native%20geometry.%20Our%20scheme%20is%20general%20to%20different%203D-native%20generators%2C%20and%20we%20present%20dedicated%20training%20strategies%20to%20facilitate%20the%20optimization%20of%20geometry-texture%20coupled%20and%20decoupled%203D-native%20generation%20paradigms.%20Experiments%20demonstrate%20that%20Photo3D%20generalizes%20well%20across%20diverse%203D-native%20generation%20paradigms%20and%20achieves%20state-of-the-art%20photorealistic%203D%20generation%20performance.&entry.1838667208=http%3A//arxiv.org/abs/2512.08535v1&entry.124074799=Read"},
{"title": "CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale", "author": "Shahar Sarfaty and Adi Haviv and Uri Hacohen and Niva Elkin-Koren and Roi Livni and Amit H. Bermano", "abstract": "The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.", "link": "http://arxiv.org/abs/2512.08826v1", "date": "2025-12-09", "relevancy": 2.507, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5139}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4952}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4952}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CARLoS%3A%20Retrieval%20via%20Concise%20Assessment%20Representation%20of%20LoRAs%20at%20Scale&body=Title%3A%20CARLoS%3A%20Retrieval%20via%20Concise%20Assessment%20Representation%20of%20LoRAs%20at%20Scale%0AAuthor%3A%20Shahar%20Sarfaty%20and%20Adi%20Haviv%20and%20Uri%20Hacohen%20and%20Niva%20Elkin-Koren%20and%20Roi%20Livni%20and%20Amit%20H.%20Bermano%0AAbstract%3A%20The%20rapid%20proliferation%20of%20generative%20components%2C%20such%20as%20LoRAs%2C%20has%20created%20a%20vast%20but%20unstructured%20ecosystem.%20Existing%20discovery%20methods%20depend%20on%20unreliable%20user%20descriptions%20or%20biased%20popularity%20metrics%2C%20hindering%20usability.%20We%20present%20CARLoS%2C%20a%20large-scale%20framework%20for%20characterizing%20LoRAs%20without%20requiring%20additional%20metadata.%20Analyzing%20over%20650%20LoRAs%2C%20we%20employ%20them%20in%20image%20generation%20over%20a%20variety%20of%20prompts%20and%20seeds%2C%20as%20a%20credible%20way%20to%20assess%20their%20behavior.%20Using%20CLIP%20embeddings%20and%20their%20difference%20to%20a%20base-model%20generation%2C%20we%20concisely%20define%20a%20three-part%20representation%3A%20Directions%2C%20defining%20semantic%20shift%3B%20Strength%2C%20quantifying%20the%20significance%20of%20the%20effect%3B%20and%20Consistency%2C%20quantifying%20how%20stable%20the%20effect%20is.%20Using%20these%20representations%2C%20we%20develop%20an%20efficient%20retrieval%20framework%20that%20semantically%20matches%20textual%20queries%20to%20relevant%20LoRAs%20while%20filtering%20overly%20strong%20or%20unstable%20ones%2C%20outperforming%20textual%20baselines%20in%20automated%20and%20human%20evaluations.%20While%20retrieval%20is%20our%20primary%20focus%2C%20the%20same%20representation%20also%20supports%20analyses%20linking%20Strength%20and%20Consistency%20to%20legal%20notions%20of%20substantiality%20and%20volition%2C%20key%20considerations%20in%20copyright%2C%20positioning%20CARLoS%20as%20a%20practical%20system%20with%20broader%20relevance%20for%20LoRA%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08826v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCARLoS%253A%2520Retrieval%2520via%2520Concise%2520Assessment%2520Representation%2520of%2520LoRAs%2520at%2520Scale%26entry.906535625%3DShahar%2520Sarfaty%2520and%2520Adi%2520Haviv%2520and%2520Uri%2520Hacohen%2520and%2520Niva%2520Elkin-Koren%2520and%2520Roi%2520Livni%2520and%2520Amit%2520H.%2520Bermano%26entry.1292438233%3DThe%2520rapid%2520proliferation%2520of%2520generative%2520components%252C%2520such%2520as%2520LoRAs%252C%2520has%2520created%2520a%2520vast%2520but%2520unstructured%2520ecosystem.%2520Existing%2520discovery%2520methods%2520depend%2520on%2520unreliable%2520user%2520descriptions%2520or%2520biased%2520popularity%2520metrics%252C%2520hindering%2520usability.%2520We%2520present%2520CARLoS%252C%2520a%2520large-scale%2520framework%2520for%2520characterizing%2520LoRAs%2520without%2520requiring%2520additional%2520metadata.%2520Analyzing%2520over%2520650%2520LoRAs%252C%2520we%2520employ%2520them%2520in%2520image%2520generation%2520over%2520a%2520variety%2520of%2520prompts%2520and%2520seeds%252C%2520as%2520a%2520credible%2520way%2520to%2520assess%2520their%2520behavior.%2520Using%2520CLIP%2520embeddings%2520and%2520their%2520difference%2520to%2520a%2520base-model%2520generation%252C%2520we%2520concisely%2520define%2520a%2520three-part%2520representation%253A%2520Directions%252C%2520defining%2520semantic%2520shift%253B%2520Strength%252C%2520quantifying%2520the%2520significance%2520of%2520the%2520effect%253B%2520and%2520Consistency%252C%2520quantifying%2520how%2520stable%2520the%2520effect%2520is.%2520Using%2520these%2520representations%252C%2520we%2520develop%2520an%2520efficient%2520retrieval%2520framework%2520that%2520semantically%2520matches%2520textual%2520queries%2520to%2520relevant%2520LoRAs%2520while%2520filtering%2520overly%2520strong%2520or%2520unstable%2520ones%252C%2520outperforming%2520textual%2520baselines%2520in%2520automated%2520and%2520human%2520evaluations.%2520While%2520retrieval%2520is%2520our%2520primary%2520focus%252C%2520the%2520same%2520representation%2520also%2520supports%2520analyses%2520linking%2520Strength%2520and%2520Consistency%2520to%2520legal%2520notions%2520of%2520substantiality%2520and%2520volition%252C%2520key%2520considerations%2520in%2520copyright%252C%2520positioning%2520CARLoS%2520as%2520a%2520practical%2520system%2520with%2520broader%2520relevance%2520for%2520LoRA%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08826v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CARLoS%3A%20Retrieval%20via%20Concise%20Assessment%20Representation%20of%20LoRAs%20at%20Scale&entry.906535625=Shahar%20Sarfaty%20and%20Adi%20Haviv%20and%20Uri%20Hacohen%20and%20Niva%20Elkin-Koren%20and%20Roi%20Livni%20and%20Amit%20H.%20Bermano&entry.1292438233=The%20rapid%20proliferation%20of%20generative%20components%2C%20such%20as%20LoRAs%2C%20has%20created%20a%20vast%20but%20unstructured%20ecosystem.%20Existing%20discovery%20methods%20depend%20on%20unreliable%20user%20descriptions%20or%20biased%20popularity%20metrics%2C%20hindering%20usability.%20We%20present%20CARLoS%2C%20a%20large-scale%20framework%20for%20characterizing%20LoRAs%20without%20requiring%20additional%20metadata.%20Analyzing%20over%20650%20LoRAs%2C%20we%20employ%20them%20in%20image%20generation%20over%20a%20variety%20of%20prompts%20and%20seeds%2C%20as%20a%20credible%20way%20to%20assess%20their%20behavior.%20Using%20CLIP%20embeddings%20and%20their%20difference%20to%20a%20base-model%20generation%2C%20we%20concisely%20define%20a%20three-part%20representation%3A%20Directions%2C%20defining%20semantic%20shift%3B%20Strength%2C%20quantifying%20the%20significance%20of%20the%20effect%3B%20and%20Consistency%2C%20quantifying%20how%20stable%20the%20effect%20is.%20Using%20these%20representations%2C%20we%20develop%20an%20efficient%20retrieval%20framework%20that%20semantically%20matches%20textual%20queries%20to%20relevant%20LoRAs%20while%20filtering%20overly%20strong%20or%20unstable%20ones%2C%20outperforming%20textual%20baselines%20in%20automated%20and%20human%20evaluations.%20While%20retrieval%20is%20our%20primary%20focus%2C%20the%20same%20representation%20also%20supports%20analyses%20linking%20Strength%20and%20Consistency%20to%20legal%20notions%20of%20substantiality%20and%20volition%2C%20key%20considerations%20in%20copyright%2C%20positioning%20CARLoS%20as%20a%20practical%20system%20with%20broader%20relevance%20for%20LoRA%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2512.08826v1&entry.124074799=Read"},
{"title": "AI-powered virtual tissues from spatial proteomics for clinical diagnostics and biomedical discovery", "author": "Johann Wenckstern and Eeshaan Jain and Yexiang Cheng and Benedikt von Querfurth and Kiril Vasilev and Matteo Pariset and Phil F. Cheng and Petros Liakopoulos and Olivier Michielin and Andreas Wicki and Gabriele Gut and Charlotte Bunne", "abstract": "Spatial proteomics technologies have transformed our understanding of complex tissue architecture in cancer but present unique challenges for computational analysis. Each study uses a different marker panel and protocol, and most methods are tailored to single cohorts, which limits knowledge transfer and robust biomarker discovery. Here we present Virtual Tissues (VirTues), a general-purpose foundation model for spatial proteomics that learns marker-aware, multi-scale representations of proteins, cells, niches and tissues directly from multiplex imaging data. From a single pretrained backbone, VirTues supports marker reconstruction, cell typing and niche annotation, spatial biomarker discovery, and patient stratification, including zero-shot annotation across heterogeneous panels and datasets. In triple-negative breast cancer, VirTues-derived biomarkers predict anti-PD-L1 chemo-immunotherapy response and stratify disease-free survival in an independent cohort, outperforming state-of-the-art biomarkers derived from the same datasets and current clinical stratification schemes.", "link": "http://arxiv.org/abs/2501.06039v2", "date": "2025-12-09", "relevancy": 2.5064, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5075}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5075}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4888}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AI-powered%20virtual%20tissues%20from%20spatial%20proteomics%20for%20clinical%20diagnostics%20and%20biomedical%20discovery&body=Title%3A%20AI-powered%20virtual%20tissues%20from%20spatial%20proteomics%20for%20clinical%20diagnostics%20and%20biomedical%20discovery%0AAuthor%3A%20Johann%20Wenckstern%20and%20Eeshaan%20Jain%20and%20Yexiang%20Cheng%20and%20Benedikt%20von%20Querfurth%20and%20Kiril%20Vasilev%20and%20Matteo%20Pariset%20and%20Phil%20F.%20Cheng%20and%20Petros%20Liakopoulos%20and%20Olivier%20Michielin%20and%20Andreas%20Wicki%20and%20Gabriele%20Gut%20and%20Charlotte%20Bunne%0AAbstract%3A%20Spatial%20proteomics%20technologies%20have%20transformed%20our%20understanding%20of%20complex%20tissue%20architecture%20in%20cancer%20but%20present%20unique%20challenges%20for%20computational%20analysis.%20Each%20study%20uses%20a%20different%20marker%20panel%20and%20protocol%2C%20and%20most%20methods%20are%20tailored%20to%20single%20cohorts%2C%20which%20limits%20knowledge%20transfer%20and%20robust%20biomarker%20discovery.%20Here%20we%20present%20Virtual%20Tissues%20%28VirTues%29%2C%20a%20general-purpose%20foundation%20model%20for%20spatial%20proteomics%20that%20learns%20marker-aware%2C%20multi-scale%20representations%20of%20proteins%2C%20cells%2C%20niches%20and%20tissues%20directly%20from%20multiplex%20imaging%20data.%20From%20a%20single%20pretrained%20backbone%2C%20VirTues%20supports%20marker%20reconstruction%2C%20cell%20typing%20and%20niche%20annotation%2C%20spatial%20biomarker%20discovery%2C%20and%20patient%20stratification%2C%20including%20zero-shot%20annotation%20across%20heterogeneous%20panels%20and%20datasets.%20In%20triple-negative%20breast%20cancer%2C%20VirTues-derived%20biomarkers%20predict%20anti-PD-L1%20chemo-immunotherapy%20response%20and%20stratify%20disease-free%20survival%20in%20an%20independent%20cohort%2C%20outperforming%20state-of-the-art%20biomarkers%20derived%20from%20the%20same%20datasets%20and%20current%20clinical%20stratification%20schemes.%0ALink%3A%20http%3A//arxiv.org/abs/2501.06039v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAI-powered%2520virtual%2520tissues%2520from%2520spatial%2520proteomics%2520for%2520clinical%2520diagnostics%2520and%2520biomedical%2520discovery%26entry.906535625%3DJohann%2520Wenckstern%2520and%2520Eeshaan%2520Jain%2520and%2520Yexiang%2520Cheng%2520and%2520Benedikt%2520von%2520Querfurth%2520and%2520Kiril%2520Vasilev%2520and%2520Matteo%2520Pariset%2520and%2520Phil%2520F.%2520Cheng%2520and%2520Petros%2520Liakopoulos%2520and%2520Olivier%2520Michielin%2520and%2520Andreas%2520Wicki%2520and%2520Gabriele%2520Gut%2520and%2520Charlotte%2520Bunne%26entry.1292438233%3DSpatial%2520proteomics%2520technologies%2520have%2520transformed%2520our%2520understanding%2520of%2520complex%2520tissue%2520architecture%2520in%2520cancer%2520but%2520present%2520unique%2520challenges%2520for%2520computational%2520analysis.%2520Each%2520study%2520uses%2520a%2520different%2520marker%2520panel%2520and%2520protocol%252C%2520and%2520most%2520methods%2520are%2520tailored%2520to%2520single%2520cohorts%252C%2520which%2520limits%2520knowledge%2520transfer%2520and%2520robust%2520biomarker%2520discovery.%2520Here%2520we%2520present%2520Virtual%2520Tissues%2520%2528VirTues%2529%252C%2520a%2520general-purpose%2520foundation%2520model%2520for%2520spatial%2520proteomics%2520that%2520learns%2520marker-aware%252C%2520multi-scale%2520representations%2520of%2520proteins%252C%2520cells%252C%2520niches%2520and%2520tissues%2520directly%2520from%2520multiplex%2520imaging%2520data.%2520From%2520a%2520single%2520pretrained%2520backbone%252C%2520VirTues%2520supports%2520marker%2520reconstruction%252C%2520cell%2520typing%2520and%2520niche%2520annotation%252C%2520spatial%2520biomarker%2520discovery%252C%2520and%2520patient%2520stratification%252C%2520including%2520zero-shot%2520annotation%2520across%2520heterogeneous%2520panels%2520and%2520datasets.%2520In%2520triple-negative%2520breast%2520cancer%252C%2520VirTues-derived%2520biomarkers%2520predict%2520anti-PD-L1%2520chemo-immunotherapy%2520response%2520and%2520stratify%2520disease-free%2520survival%2520in%2520an%2520independent%2520cohort%252C%2520outperforming%2520state-of-the-art%2520biomarkers%2520derived%2520from%2520the%2520same%2520datasets%2520and%2520current%2520clinical%2520stratification%2520schemes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2501.06039v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AI-powered%20virtual%20tissues%20from%20spatial%20proteomics%20for%20clinical%20diagnostics%20and%20biomedical%20discovery&entry.906535625=Johann%20Wenckstern%20and%20Eeshaan%20Jain%20and%20Yexiang%20Cheng%20and%20Benedikt%20von%20Querfurth%20and%20Kiril%20Vasilev%20and%20Matteo%20Pariset%20and%20Phil%20F.%20Cheng%20and%20Petros%20Liakopoulos%20and%20Olivier%20Michielin%20and%20Andreas%20Wicki%20and%20Gabriele%20Gut%20and%20Charlotte%20Bunne&entry.1292438233=Spatial%20proteomics%20technologies%20have%20transformed%20our%20understanding%20of%20complex%20tissue%20architecture%20in%20cancer%20but%20present%20unique%20challenges%20for%20computational%20analysis.%20Each%20study%20uses%20a%20different%20marker%20panel%20and%20protocol%2C%20and%20most%20methods%20are%20tailored%20to%20single%20cohorts%2C%20which%20limits%20knowledge%20transfer%20and%20robust%20biomarker%20discovery.%20Here%20we%20present%20Virtual%20Tissues%20%28VirTues%29%2C%20a%20general-purpose%20foundation%20model%20for%20spatial%20proteomics%20that%20learns%20marker-aware%2C%20multi-scale%20representations%20of%20proteins%2C%20cells%2C%20niches%20and%20tissues%20directly%20from%20multiplex%20imaging%20data.%20From%20a%20single%20pretrained%20backbone%2C%20VirTues%20supports%20marker%20reconstruction%2C%20cell%20typing%20and%20niche%20annotation%2C%20spatial%20biomarker%20discovery%2C%20and%20patient%20stratification%2C%20including%20zero-shot%20annotation%20across%20heterogeneous%20panels%20and%20datasets.%20In%20triple-negative%20breast%20cancer%2C%20VirTues-derived%20biomarkers%20predict%20anti-PD-L1%20chemo-immunotherapy%20response%20and%20stratify%20disease-free%20survival%20in%20an%20independent%20cohort%2C%20outperforming%20state-of-the-art%20biomarkers%20derived%20from%20the%20same%20datasets%20and%20current%20clinical%20stratification%20schemes.&entry.1838667208=http%3A//arxiv.org/abs/2501.06039v2&entry.124074799=Read"},
{"title": "Efficiently Reconstructing Dynamic Scenes One D4RT at a Time", "author": "Chuhan Zhang and Guillaume Le Moing and Skanda Koppula and Ignacio Rocco and Liliane Momeni and Junyu Xie and Shuyang Sun and Rahul Sukthankar and Jo\u00eblle K Barral and Raia Hadsell and Zoubin Ghahramani and Andrew Zisserman and Junlin Zhang and Mehdi SM Sajjadi", "abstract": "Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.", "link": "http://arxiv.org/abs/2512.08924v1", "date": "2025-12-09", "relevancy": 2.4996, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6405}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6218}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6218}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Efficiently%20Reconstructing%20Dynamic%20Scenes%20One%20D4RT%20at%20a%20Time&body=Title%3A%20Efficiently%20Reconstructing%20Dynamic%20Scenes%20One%20D4RT%20at%20a%20Time%0AAuthor%3A%20Chuhan%20Zhang%20and%20Guillaume%20Le%20Moing%20and%20Skanda%20Koppula%20and%20Ignacio%20Rocco%20and%20Liliane%20Momeni%20and%20Junyu%20Xie%20and%20Shuyang%20Sun%20and%20Rahul%20Sukthankar%20and%20Jo%C3%ABlle%20K%20Barral%20and%20Raia%20Hadsell%20and%20Zoubin%20Ghahramani%20and%20Andrew%20Zisserman%20and%20Junlin%20Zhang%20and%20Mehdi%20SM%20Sajjadi%0AAbstract%3A%20Understanding%20and%20reconstructing%20the%20complex%20geometry%20and%20motion%20of%20dynamic%20scenes%20from%20video%20remains%20a%20formidable%20challenge%20in%20computer%20vision.%20This%20paper%20introduces%20D4RT%2C%20a%20simple%20yet%20powerful%20feedforward%20model%20designed%20to%20efficiently%20solve%20this%20task.%20D4RT%20utilizes%20a%20unified%20transformer%20architecture%20to%20jointly%20infer%20depth%2C%20spatio-temporal%20correspondence%2C%20and%20full%20camera%20parameters%20from%20a%20single%20video.%20Its%20core%20innovation%20is%20a%20novel%20querying%20mechanism%20that%20sidesteps%20the%20heavy%20computation%20of%20dense%2C%20per-frame%20decoding%20and%20the%20complexity%20of%20managing%20multiple%2C%20task-specific%20decoders.%20Our%20decoding%20interface%20allows%20the%20model%20to%20independently%20and%20flexibly%20probe%20the%203D%20position%20of%20any%20point%20in%20space%20and%20time.%20The%20result%20is%20a%20lightweight%20and%20highly%20scalable%20method%20that%20enables%20remarkably%20efficient%20training%20and%20inference.%20We%20demonstrate%20that%20our%20approach%20sets%20a%20new%20state%20of%20the%20art%2C%20outperforming%20previous%20methods%20across%20a%20wide%20spectrum%20of%204D%20reconstruction%20tasks.%20We%20refer%20to%20the%20project%20webpage%20for%20animated%20results%3A%20https%3A//d4rt-paper.github.io/.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08924v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEfficiently%2520Reconstructing%2520Dynamic%2520Scenes%2520One%2520D4RT%2520at%2520a%2520Time%26entry.906535625%3DChuhan%2520Zhang%2520and%2520Guillaume%2520Le%2520Moing%2520and%2520Skanda%2520Koppula%2520and%2520Ignacio%2520Rocco%2520and%2520Liliane%2520Momeni%2520and%2520Junyu%2520Xie%2520and%2520Shuyang%2520Sun%2520and%2520Rahul%2520Sukthankar%2520and%2520Jo%25C3%25ABlle%2520K%2520Barral%2520and%2520Raia%2520Hadsell%2520and%2520Zoubin%2520Ghahramani%2520and%2520Andrew%2520Zisserman%2520and%2520Junlin%2520Zhang%2520and%2520Mehdi%2520SM%2520Sajjadi%26entry.1292438233%3DUnderstanding%2520and%2520reconstructing%2520the%2520complex%2520geometry%2520and%2520motion%2520of%2520dynamic%2520scenes%2520from%2520video%2520remains%2520a%2520formidable%2520challenge%2520in%2520computer%2520vision.%2520This%2520paper%2520introduces%2520D4RT%252C%2520a%2520simple%2520yet%2520powerful%2520feedforward%2520model%2520designed%2520to%2520efficiently%2520solve%2520this%2520task.%2520D4RT%2520utilizes%2520a%2520unified%2520transformer%2520architecture%2520to%2520jointly%2520infer%2520depth%252C%2520spatio-temporal%2520correspondence%252C%2520and%2520full%2520camera%2520parameters%2520from%2520a%2520single%2520video.%2520Its%2520core%2520innovation%2520is%2520a%2520novel%2520querying%2520mechanism%2520that%2520sidesteps%2520the%2520heavy%2520computation%2520of%2520dense%252C%2520per-frame%2520decoding%2520and%2520the%2520complexity%2520of%2520managing%2520multiple%252C%2520task-specific%2520decoders.%2520Our%2520decoding%2520interface%2520allows%2520the%2520model%2520to%2520independently%2520and%2520flexibly%2520probe%2520the%25203D%2520position%2520of%2520any%2520point%2520in%2520space%2520and%2520time.%2520The%2520result%2520is%2520a%2520lightweight%2520and%2520highly%2520scalable%2520method%2520that%2520enables%2520remarkably%2520efficient%2520training%2520and%2520inference.%2520We%2520demonstrate%2520that%2520our%2520approach%2520sets%2520a%2520new%2520state%2520of%2520the%2520art%252C%2520outperforming%2520previous%2520methods%2520across%2520a%2520wide%2520spectrum%2520of%25204D%2520reconstruction%2520tasks.%2520We%2520refer%2520to%2520the%2520project%2520webpage%2520for%2520animated%2520results%253A%2520https%253A//d4rt-paper.github.io/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08924v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Efficiently%20Reconstructing%20Dynamic%20Scenes%20One%20D4RT%20at%20a%20Time&entry.906535625=Chuhan%20Zhang%20and%20Guillaume%20Le%20Moing%20and%20Skanda%20Koppula%20and%20Ignacio%20Rocco%20and%20Liliane%20Momeni%20and%20Junyu%20Xie%20and%20Shuyang%20Sun%20and%20Rahul%20Sukthankar%20and%20Jo%C3%ABlle%20K%20Barral%20and%20Raia%20Hadsell%20and%20Zoubin%20Ghahramani%20and%20Andrew%20Zisserman%20and%20Junlin%20Zhang%20and%20Mehdi%20SM%20Sajjadi&entry.1292438233=Understanding%20and%20reconstructing%20the%20complex%20geometry%20and%20motion%20of%20dynamic%20scenes%20from%20video%20remains%20a%20formidable%20challenge%20in%20computer%20vision.%20This%20paper%20introduces%20D4RT%2C%20a%20simple%20yet%20powerful%20feedforward%20model%20designed%20to%20efficiently%20solve%20this%20task.%20D4RT%20utilizes%20a%20unified%20transformer%20architecture%20to%20jointly%20infer%20depth%2C%20spatio-temporal%20correspondence%2C%20and%20full%20camera%20parameters%20from%20a%20single%20video.%20Its%20core%20innovation%20is%20a%20novel%20querying%20mechanism%20that%20sidesteps%20the%20heavy%20computation%20of%20dense%2C%20per-frame%20decoding%20and%20the%20complexity%20of%20managing%20multiple%2C%20task-specific%20decoders.%20Our%20decoding%20interface%20allows%20the%20model%20to%20independently%20and%20flexibly%20probe%20the%203D%20position%20of%20any%20point%20in%20space%20and%20time.%20The%20result%20is%20a%20lightweight%20and%20highly%20scalable%20method%20that%20enables%20remarkably%20efficient%20training%20and%20inference.%20We%20demonstrate%20that%20our%20approach%20sets%20a%20new%20state%20of%20the%20art%2C%20outperforming%20previous%20methods%20across%20a%20wide%20spectrum%20of%204D%20reconstruction%20tasks.%20We%20refer%20to%20the%20project%20webpage%20for%20animated%20results%3A%20https%3A//d4rt-paper.github.io/.&entry.1838667208=http%3A//arxiv.org/abs/2512.08924v1&entry.124074799=Read"},
{"title": "When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation", "author": "Amirkia Rafiei Oskooei and Kaan Baturalp Cosdan and Husamettin Isiktas and Mehmet S. Aktas", "abstract": "Large Language Models (LLMs) with vast context windows offer new avenues for in-context learning (ICL), where providing many examples (\"many-shot\" prompting) is often assumed to enhance performance. We investigate this assumption for the complex task of code translation. Through a large-scale empirical study of over 90,000 translations, we systematically evaluate the impact of scaling in-context examples from zero-shot to many-shot configurations of up to 625 examples, with prompts spanning from approximately 100,000 to 800,000 tokens. Our findings reveal a \"many-shot paradox\": while static similarity metrics may modestly improve with more examples, functional correctness consistently peaks with few-shot prompting (5-25 examples). Providing substantially more examples often degrades this crucial functional performance. This study highlights that for code translation, the quality of a few well-chosen examples outweighs sheer quantity, challenging the universal efficacy of \"more is better\" for ICL and underscoring the task-dependent nature of optimal prompting strategies. Our results have significant implications for effectively leveraging LLMs in software engineering.", "link": "http://arxiv.org/abs/2510.16809v2", "date": "2025-12-09", "relevancy": 2.487, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.516}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4602}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20When%20Many-Shot%20Prompting%20Fails%3A%20An%20Empirical%20Study%20of%20LLM%20Code%20Translation&body=Title%3A%20When%20Many-Shot%20Prompting%20Fails%3A%20An%20Empirical%20Study%20of%20LLM%20Code%20Translation%0AAuthor%3A%20Amirkia%20Rafiei%20Oskooei%20and%20Kaan%20Baturalp%20Cosdan%20and%20Husamettin%20Isiktas%20and%20Mehmet%20S.%20Aktas%0AAbstract%3A%20Large%20Language%20Models%20%28LLMs%29%20with%20vast%20context%20windows%20offer%20new%20avenues%20for%20in-context%20learning%20%28ICL%29%2C%20where%20providing%20many%20examples%20%28%22many-shot%22%20prompting%29%20is%20often%20assumed%20to%20enhance%20performance.%20We%20investigate%20this%20assumption%20for%20the%20complex%20task%20of%20code%20translation.%20Through%20a%20large-scale%20empirical%20study%20of%20over%2090%2C000%20translations%2C%20we%20systematically%20evaluate%20the%20impact%20of%20scaling%20in-context%20examples%20from%20zero-shot%20to%20many-shot%20configurations%20of%20up%20to%20625%20examples%2C%20with%20prompts%20spanning%20from%20approximately%20100%2C000%20to%20800%2C000%20tokens.%20Our%20findings%20reveal%20a%20%22many-shot%20paradox%22%3A%20while%20static%20similarity%20metrics%20may%20modestly%20improve%20with%20more%20examples%2C%20functional%20correctness%20consistently%20peaks%20with%20few-shot%20prompting%20%285-25%20examples%29.%20Providing%20substantially%20more%20examples%20often%20degrades%20this%20crucial%20functional%20performance.%20This%20study%20highlights%20that%20for%20code%20translation%2C%20the%20quality%20of%20a%20few%20well-chosen%20examples%20outweighs%20sheer%20quantity%2C%20challenging%20the%20universal%20efficacy%20of%20%22more%20is%20better%22%20for%20ICL%20and%20underscoring%20the%20task-dependent%20nature%20of%20optimal%20prompting%20strategies.%20Our%20results%20have%20significant%20implications%20for%20effectively%20leveraging%20LLMs%20in%20software%20engineering.%0ALink%3A%20http%3A//arxiv.org/abs/2510.16809v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhen%2520Many-Shot%2520Prompting%2520Fails%253A%2520An%2520Empirical%2520Study%2520of%2520LLM%2520Code%2520Translation%26entry.906535625%3DAmirkia%2520Rafiei%2520Oskooei%2520and%2520Kaan%2520Baturalp%2520Cosdan%2520and%2520Husamettin%2520Isiktas%2520and%2520Mehmet%2520S.%2520Aktas%26entry.1292438233%3DLarge%2520Language%2520Models%2520%2528LLMs%2529%2520with%2520vast%2520context%2520windows%2520offer%2520new%2520avenues%2520for%2520in-context%2520learning%2520%2528ICL%2529%252C%2520where%2520providing%2520many%2520examples%2520%2528%2522many-shot%2522%2520prompting%2529%2520is%2520often%2520assumed%2520to%2520enhance%2520performance.%2520We%2520investigate%2520this%2520assumption%2520for%2520the%2520complex%2520task%2520of%2520code%2520translation.%2520Through%2520a%2520large-scale%2520empirical%2520study%2520of%2520over%252090%252C000%2520translations%252C%2520we%2520systematically%2520evaluate%2520the%2520impact%2520of%2520scaling%2520in-context%2520examples%2520from%2520zero-shot%2520to%2520many-shot%2520configurations%2520of%2520up%2520to%2520625%2520examples%252C%2520with%2520prompts%2520spanning%2520from%2520approximately%2520100%252C000%2520to%2520800%252C000%2520tokens.%2520Our%2520findings%2520reveal%2520a%2520%2522many-shot%2520paradox%2522%253A%2520while%2520static%2520similarity%2520metrics%2520may%2520modestly%2520improve%2520with%2520more%2520examples%252C%2520functional%2520correctness%2520consistently%2520peaks%2520with%2520few-shot%2520prompting%2520%25285-25%2520examples%2529.%2520Providing%2520substantially%2520more%2520examples%2520often%2520degrades%2520this%2520crucial%2520functional%2520performance.%2520This%2520study%2520highlights%2520that%2520for%2520code%2520translation%252C%2520the%2520quality%2520of%2520a%2520few%2520well-chosen%2520examples%2520outweighs%2520sheer%2520quantity%252C%2520challenging%2520the%2520universal%2520efficacy%2520of%2520%2522more%2520is%2520better%2522%2520for%2520ICL%2520and%2520underscoring%2520the%2520task-dependent%2520nature%2520of%2520optimal%2520prompting%2520strategies.%2520Our%2520results%2520have%2520significant%2520implications%2520for%2520effectively%2520leveraging%2520LLMs%2520in%2520software%2520engineering.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.16809v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=When%20Many-Shot%20Prompting%20Fails%3A%20An%20Empirical%20Study%20of%20LLM%20Code%20Translation&entry.906535625=Amirkia%20Rafiei%20Oskooei%20and%20Kaan%20Baturalp%20Cosdan%20and%20Husamettin%20Isiktas%20and%20Mehmet%20S.%20Aktas&entry.1292438233=Large%20Language%20Models%20%28LLMs%29%20with%20vast%20context%20windows%20offer%20new%20avenues%20for%20in-context%20learning%20%28ICL%29%2C%20where%20providing%20many%20examples%20%28%22many-shot%22%20prompting%29%20is%20often%20assumed%20to%20enhance%20performance.%20We%20investigate%20this%20assumption%20for%20the%20complex%20task%20of%20code%20translation.%20Through%20a%20large-scale%20empirical%20study%20of%20over%2090%2C000%20translations%2C%20we%20systematically%20evaluate%20the%20impact%20of%20scaling%20in-context%20examples%20from%20zero-shot%20to%20many-shot%20configurations%20of%20up%20to%20625%20examples%2C%20with%20prompts%20spanning%20from%20approximately%20100%2C000%20to%20800%2C000%20tokens.%20Our%20findings%20reveal%20a%20%22many-shot%20paradox%22%3A%20while%20static%20similarity%20metrics%20may%20modestly%20improve%20with%20more%20examples%2C%20functional%20correctness%20consistently%20peaks%20with%20few-shot%20prompting%20%285-25%20examples%29.%20Providing%20substantially%20more%20examples%20often%20degrades%20this%20crucial%20functional%20performance.%20This%20study%20highlights%20that%20for%20code%20translation%2C%20the%20quality%20of%20a%20few%20well-chosen%20examples%20outweighs%20sheer%20quantity%2C%20challenging%20the%20universal%20efficacy%20of%20%22more%20is%20better%22%20for%20ICL%20and%20underscoring%20the%20task-dependent%20nature%20of%20optimal%20prompting%20strategies.%20Our%20results%20have%20significant%20implications%20for%20effectively%20leveraging%20LLMs%20in%20software%20engineering.&entry.1838667208=http%3A//arxiv.org/abs/2510.16809v2&entry.124074799=Read"},
{"title": "Discovering Influential Factors in Variational Autoencoders", "author": "Shiqi Liu and Jingxin Liu and Qian Zhao and Xiangyong Cao and Huibin Li and Deyu Meng and Hongying Meng and Sheng Liu", "abstract": "In the field of machine learning, it is still a critical issue to identify and supervise the learned representation without manually intervening or intuition assistance to extract useful knowledge or serve for the downstream tasks. In this work, we focus on supervising the influential factors extracted by the variational autoencoder(VAE). The VAE is proposed to learn independent low dimension representation while facing the problem that sometimes pre-set factors are ignored. We argue that the mutual information of the input and each learned factor of the representation plays a necessary indicator of discovering the influential factors. We find the VAE objective inclines to induce mutual information sparsity in factor dimension over the data intrinsic dimension and therefore result in some non-influential factors whose function on data reconstruction could be ignored. We show mutual information also influences the lower bound of the VAE's reconstruction error and downstream classification task. To make such indicator applicable, we design an algorithm for calculating the mutual information for the VAE and prove its consistency. Experimental results on MNIST, CelebA and DEAP datasets show that mutual information can help determine influential factors, of which some are interpretable and can be used to further generation and classification tasks, and help discover the variant that connects with emotion on DEAP dataset.", "link": "http://arxiv.org/abs/1809.01804v3", "date": "2025-12-09", "relevancy": 2.4663, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5338}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4749}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4711}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Discovering%20Influential%20Factors%20in%20Variational%20Autoencoders&body=Title%3A%20Discovering%20Influential%20Factors%20in%20Variational%20Autoencoders%0AAuthor%3A%20Shiqi%20Liu%20and%20Jingxin%20Liu%20and%20Qian%20Zhao%20and%20Xiangyong%20Cao%20and%20Huibin%20Li%20and%20Deyu%20Meng%20and%20Hongying%20Meng%20and%20Sheng%20Liu%0AAbstract%3A%20In%20the%20field%20of%20machine%20learning%2C%20it%20is%20still%20a%20critical%20issue%20to%20identify%20and%20supervise%20the%20learned%20representation%20without%20manually%20intervening%20or%20intuition%20assistance%20to%20extract%20useful%20knowledge%20or%20serve%20for%20the%20downstream%20tasks.%20In%20this%20work%2C%20we%20focus%20on%20supervising%20the%20influential%20factors%20extracted%20by%20the%20variational%20autoencoder%28VAE%29.%20The%20VAE%20is%20proposed%20to%20learn%20independent%20low%20dimension%20representation%20while%20facing%20the%20problem%20that%20sometimes%20pre-set%20factors%20are%20ignored.%20We%20argue%20that%20the%20mutual%20information%20of%20the%20input%20and%20each%20learned%20factor%20of%20the%20representation%20plays%20a%20necessary%20indicator%20of%20discovering%20the%20influential%20factors.%20We%20find%20the%20VAE%20objective%20inclines%20to%20induce%20mutual%20information%20sparsity%20in%20factor%20dimension%20over%20the%20data%20intrinsic%20dimension%20and%20therefore%20result%20in%20some%20non-influential%20factors%20whose%20function%20on%20data%20reconstruction%20could%20be%20ignored.%20We%20show%20mutual%20information%20also%20influences%20the%20lower%20bound%20of%20the%20VAE%27s%20reconstruction%20error%20and%20downstream%20classification%20task.%20To%20make%20such%20indicator%20applicable%2C%20we%20design%20an%20algorithm%20for%20calculating%20the%20mutual%20information%20for%20the%20VAE%20and%20prove%20its%20consistency.%20Experimental%20results%20on%20MNIST%2C%20CelebA%20and%20DEAP%20datasets%20show%20that%20mutual%20information%20can%20help%20determine%20influential%20factors%2C%20of%20which%20some%20are%20interpretable%20and%20can%20be%20used%20to%20further%20generation%20and%20classification%20tasks%2C%20and%20help%20discover%20the%20variant%20that%20connects%20with%20emotion%20on%20DEAP%20dataset.%0ALink%3A%20http%3A//arxiv.org/abs/1809.01804v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDiscovering%2520Influential%2520Factors%2520in%2520Variational%2520Autoencoders%26entry.906535625%3DShiqi%2520Liu%2520and%2520Jingxin%2520Liu%2520and%2520Qian%2520Zhao%2520and%2520Xiangyong%2520Cao%2520and%2520Huibin%2520Li%2520and%2520Deyu%2520Meng%2520and%2520Hongying%2520Meng%2520and%2520Sheng%2520Liu%26entry.1292438233%3DIn%2520the%2520field%2520of%2520machine%2520learning%252C%2520it%2520is%2520still%2520a%2520critical%2520issue%2520to%2520identify%2520and%2520supervise%2520the%2520learned%2520representation%2520without%2520manually%2520intervening%2520or%2520intuition%2520assistance%2520to%2520extract%2520useful%2520knowledge%2520or%2520serve%2520for%2520the%2520downstream%2520tasks.%2520In%2520this%2520work%252C%2520we%2520focus%2520on%2520supervising%2520the%2520influential%2520factors%2520extracted%2520by%2520the%2520variational%2520autoencoder%2528VAE%2529.%2520The%2520VAE%2520is%2520proposed%2520to%2520learn%2520independent%2520low%2520dimension%2520representation%2520while%2520facing%2520the%2520problem%2520that%2520sometimes%2520pre-set%2520factors%2520are%2520ignored.%2520We%2520argue%2520that%2520the%2520mutual%2520information%2520of%2520the%2520input%2520and%2520each%2520learned%2520factor%2520of%2520the%2520representation%2520plays%2520a%2520necessary%2520indicator%2520of%2520discovering%2520the%2520influential%2520factors.%2520We%2520find%2520the%2520VAE%2520objective%2520inclines%2520to%2520induce%2520mutual%2520information%2520sparsity%2520in%2520factor%2520dimension%2520over%2520the%2520data%2520intrinsic%2520dimension%2520and%2520therefore%2520result%2520in%2520some%2520non-influential%2520factors%2520whose%2520function%2520on%2520data%2520reconstruction%2520could%2520be%2520ignored.%2520We%2520show%2520mutual%2520information%2520also%2520influences%2520the%2520lower%2520bound%2520of%2520the%2520VAE%2527s%2520reconstruction%2520error%2520and%2520downstream%2520classification%2520task.%2520To%2520make%2520such%2520indicator%2520applicable%252C%2520we%2520design%2520an%2520algorithm%2520for%2520calculating%2520the%2520mutual%2520information%2520for%2520the%2520VAE%2520and%2520prove%2520its%2520consistency.%2520Experimental%2520results%2520on%2520MNIST%252C%2520CelebA%2520and%2520DEAP%2520datasets%2520show%2520that%2520mutual%2520information%2520can%2520help%2520determine%2520influential%2520factors%252C%2520of%2520which%2520some%2520are%2520interpretable%2520and%2520can%2520be%2520used%2520to%2520further%2520generation%2520and%2520classification%2520tasks%252C%2520and%2520help%2520discover%2520the%2520variant%2520that%2520connects%2520with%2520emotion%2520on%2520DEAP%2520dataset.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/1809.01804v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Discovering%20Influential%20Factors%20in%20Variational%20Autoencoders&entry.906535625=Shiqi%20Liu%20and%20Jingxin%20Liu%20and%20Qian%20Zhao%20and%20Xiangyong%20Cao%20and%20Huibin%20Li%20and%20Deyu%20Meng%20and%20Hongying%20Meng%20and%20Sheng%20Liu&entry.1292438233=In%20the%20field%20of%20machine%20learning%2C%20it%20is%20still%20a%20critical%20issue%20to%20identify%20and%20supervise%20the%20learned%20representation%20without%20manually%20intervening%20or%20intuition%20assistance%20to%20extract%20useful%20knowledge%20or%20serve%20for%20the%20downstream%20tasks.%20In%20this%20work%2C%20we%20focus%20on%20supervising%20the%20influential%20factors%20extracted%20by%20the%20variational%20autoencoder%28VAE%29.%20The%20VAE%20is%20proposed%20to%20learn%20independent%20low%20dimension%20representation%20while%20facing%20the%20problem%20that%20sometimes%20pre-set%20factors%20are%20ignored.%20We%20argue%20that%20the%20mutual%20information%20of%20the%20input%20and%20each%20learned%20factor%20of%20the%20representation%20plays%20a%20necessary%20indicator%20of%20discovering%20the%20influential%20factors.%20We%20find%20the%20VAE%20objective%20inclines%20to%20induce%20mutual%20information%20sparsity%20in%20factor%20dimension%20over%20the%20data%20intrinsic%20dimension%20and%20therefore%20result%20in%20some%20non-influential%20factors%20whose%20function%20on%20data%20reconstruction%20could%20be%20ignored.%20We%20show%20mutual%20information%20also%20influences%20the%20lower%20bound%20of%20the%20VAE%27s%20reconstruction%20error%20and%20downstream%20classification%20task.%20To%20make%20such%20indicator%20applicable%2C%20we%20design%20an%20algorithm%20for%20calculating%20the%20mutual%20information%20for%20the%20VAE%20and%20prove%20its%20consistency.%20Experimental%20results%20on%20MNIST%2C%20CelebA%20and%20DEAP%20datasets%20show%20that%20mutual%20information%20can%20help%20determine%20influential%20factors%2C%20of%20which%20some%20are%20interpretable%20and%20can%20be%20used%20to%20further%20generation%20and%20classification%20tasks%2C%20and%20help%20discover%20the%20variant%20that%20connects%20with%20emotion%20on%20DEAP%20dataset.&entry.1838667208=http%3A//arxiv.org/abs/1809.01804v3&entry.124074799=Read"},
{"title": "DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process", "author": "Mohammad Abu-Shaira and Ajita Rattani and Weishi Shi", "abstract": "Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.", "link": "http://arxiv.org/abs/2512.08879v1", "date": "2025-12-09", "relevancy": 2.4619, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4986}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4902}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4883}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DAO-GP%20Drift%20Aware%20Online%20Non-Linear%20Regression%20Gaussian-Process&body=Title%3A%20DAO-GP%20Drift%20Aware%20Online%20Non-Linear%20Regression%20Gaussian-Process%0AAuthor%3A%20Mohammad%20Abu-Shaira%20and%20Ajita%20Rattani%20and%20Weishi%20Shi%0AAbstract%3A%20Real-world%20datasets%20often%20exhibit%20temporal%20dynamics%20characterized%20by%20evolving%20data%20distributions.%20Disregarding%20this%20phenomenon%2C%20commonly%20referred%20to%20as%20concept%20drift%2C%20can%20significantly%20diminish%20a%20model%27s%20predictive%20accuracy.%20Furthermore%2C%20the%20presence%20of%20hyperparameters%20in%20online%20models%20exacerbates%20this%20issue.%20These%20parameters%20are%20typically%20fixed%20and%20cannot%20be%20dynamically%20adjusted%20by%20the%20user%20in%20response%20to%20the%20evolving%20data%20distribution.%20Gaussian%20Process%20%28GP%29%20models%20offer%20powerful%20non-parametric%20regression%20capabilities%20with%20uncertainty%20quantification%2C%20making%20them%20ideal%20for%20modeling%20complex%20data%20relationships%20in%20an%20online%20setting.%20However%2C%20conventional%20online%20GP%20methods%20face%20several%20critical%20limitations%2C%20including%20a%20lack%20of%20drift-awareness%2C%20reliance%20on%20fixed%20hyperparameters%2C%20vulnerability%20to%20data%20snooping%2C%20absence%20of%20a%20principled%20decay%20mechanism%2C%20and%20memory%20inefficiencies.%20In%20response%2C%20we%20propose%20DAO-GP%20%28Drift-Aware%20Online%20Gaussian%20Process%29%2C%20a%20novel%2C%20fully%20adaptive%2C%20hyperparameter-free%2C%20decayed%2C%20and%20sparse%20non-linear%20regression%20model.%20DAO-GP%20features%20a%20built-in%20drift%20detection%20and%20adaptation%20mechanism%20that%20dynamically%20adjusts%20model%20behavior%20based%20on%20the%20severity%20of%20drift.%20Extensive%20empirical%20evaluations%20confirm%20DAO-GP%27s%20robustness%20across%20stationary%20conditions%2C%20diverse%20drift%20types%20%28abrupt%2C%20incremental%2C%20gradual%29%2C%20and%20varied%20data%20characteristics.%20Analyses%20demonstrate%20its%20dynamic%20adaptation%2C%20efficient%20in-memory%20and%20decay-based%20management%2C%20and%20evolving%20inducing%20points.%20Compared%20with%20state-of-the-art%20parametric%20and%20non-parametric%20models%2C%20DAO-GP%20consistently%20achieves%20superior%20or%20competitive%20performance%2C%20establishing%20it%20as%20a%20drift-resilient%20solution%20for%20online%20non-linear%20regression.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08879v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDAO-GP%2520Drift%2520Aware%2520Online%2520Non-Linear%2520Regression%2520Gaussian-Process%26entry.906535625%3DMohammad%2520Abu-Shaira%2520and%2520Ajita%2520Rattani%2520and%2520Weishi%2520Shi%26entry.1292438233%3DReal-world%2520datasets%2520often%2520exhibit%2520temporal%2520dynamics%2520characterized%2520by%2520evolving%2520data%2520distributions.%2520Disregarding%2520this%2520phenomenon%252C%2520commonly%2520referred%2520to%2520as%2520concept%2520drift%252C%2520can%2520significantly%2520diminish%2520a%2520model%2527s%2520predictive%2520accuracy.%2520Furthermore%252C%2520the%2520presence%2520of%2520hyperparameters%2520in%2520online%2520models%2520exacerbates%2520this%2520issue.%2520These%2520parameters%2520are%2520typically%2520fixed%2520and%2520cannot%2520be%2520dynamically%2520adjusted%2520by%2520the%2520user%2520in%2520response%2520to%2520the%2520evolving%2520data%2520distribution.%2520Gaussian%2520Process%2520%2528GP%2529%2520models%2520offer%2520powerful%2520non-parametric%2520regression%2520capabilities%2520with%2520uncertainty%2520quantification%252C%2520making%2520them%2520ideal%2520for%2520modeling%2520complex%2520data%2520relationships%2520in%2520an%2520online%2520setting.%2520However%252C%2520conventional%2520online%2520GP%2520methods%2520face%2520several%2520critical%2520limitations%252C%2520including%2520a%2520lack%2520of%2520drift-awareness%252C%2520reliance%2520on%2520fixed%2520hyperparameters%252C%2520vulnerability%2520to%2520data%2520snooping%252C%2520absence%2520of%2520a%2520principled%2520decay%2520mechanism%252C%2520and%2520memory%2520inefficiencies.%2520In%2520response%252C%2520we%2520propose%2520DAO-GP%2520%2528Drift-Aware%2520Online%2520Gaussian%2520Process%2529%252C%2520a%2520novel%252C%2520fully%2520adaptive%252C%2520hyperparameter-free%252C%2520decayed%252C%2520and%2520sparse%2520non-linear%2520regression%2520model.%2520DAO-GP%2520features%2520a%2520built-in%2520drift%2520detection%2520and%2520adaptation%2520mechanism%2520that%2520dynamically%2520adjusts%2520model%2520behavior%2520based%2520on%2520the%2520severity%2520of%2520drift.%2520Extensive%2520empirical%2520evaluations%2520confirm%2520DAO-GP%2527s%2520robustness%2520across%2520stationary%2520conditions%252C%2520diverse%2520drift%2520types%2520%2528abrupt%252C%2520incremental%252C%2520gradual%2529%252C%2520and%2520varied%2520data%2520characteristics.%2520Analyses%2520demonstrate%2520its%2520dynamic%2520adaptation%252C%2520efficient%2520in-memory%2520and%2520decay-based%2520management%252C%2520and%2520evolving%2520inducing%2520points.%2520Compared%2520with%2520state-of-the-art%2520parametric%2520and%2520non-parametric%2520models%252C%2520DAO-GP%2520consistently%2520achieves%2520superior%2520or%2520competitive%2520performance%252C%2520establishing%2520it%2520as%2520a%2520drift-resilient%2520solution%2520for%2520online%2520non-linear%2520regression.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08879v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DAO-GP%20Drift%20Aware%20Online%20Non-Linear%20Regression%20Gaussian-Process&entry.906535625=Mohammad%20Abu-Shaira%20and%20Ajita%20Rattani%20and%20Weishi%20Shi&entry.1292438233=Real-world%20datasets%20often%20exhibit%20temporal%20dynamics%20characterized%20by%20evolving%20data%20distributions.%20Disregarding%20this%20phenomenon%2C%20commonly%20referred%20to%20as%20concept%20drift%2C%20can%20significantly%20diminish%20a%20model%27s%20predictive%20accuracy.%20Furthermore%2C%20the%20presence%20of%20hyperparameters%20in%20online%20models%20exacerbates%20this%20issue.%20These%20parameters%20are%20typically%20fixed%20and%20cannot%20be%20dynamically%20adjusted%20by%20the%20user%20in%20response%20to%20the%20evolving%20data%20distribution.%20Gaussian%20Process%20%28GP%29%20models%20offer%20powerful%20non-parametric%20regression%20capabilities%20with%20uncertainty%20quantification%2C%20making%20them%20ideal%20for%20modeling%20complex%20data%20relationships%20in%20an%20online%20setting.%20However%2C%20conventional%20online%20GP%20methods%20face%20several%20critical%20limitations%2C%20including%20a%20lack%20of%20drift-awareness%2C%20reliance%20on%20fixed%20hyperparameters%2C%20vulnerability%20to%20data%20snooping%2C%20absence%20of%20a%20principled%20decay%20mechanism%2C%20and%20memory%20inefficiencies.%20In%20response%2C%20we%20propose%20DAO-GP%20%28Drift-Aware%20Online%20Gaussian%20Process%29%2C%20a%20novel%2C%20fully%20adaptive%2C%20hyperparameter-free%2C%20decayed%2C%20and%20sparse%20non-linear%20regression%20model.%20DAO-GP%20features%20a%20built-in%20drift%20detection%20and%20adaptation%20mechanism%20that%20dynamically%20adjusts%20model%20behavior%20based%20on%20the%20severity%20of%20drift.%20Extensive%20empirical%20evaluations%20confirm%20DAO-GP%27s%20robustness%20across%20stationary%20conditions%2C%20diverse%20drift%20types%20%28abrupt%2C%20incremental%2C%20gradual%29%2C%20and%20varied%20data%20characteristics.%20Analyses%20demonstrate%20its%20dynamic%20adaptation%2C%20efficient%20in-memory%20and%20decay-based%20management%2C%20and%20evolving%20inducing%20points.%20Compared%20with%20state-of-the-art%20parametric%20and%20non-parametric%20models%2C%20DAO-GP%20consistently%20achieves%20superior%20or%20competitive%20performance%2C%20establishing%20it%20as%20a%20drift-resilient%20solution%20for%20online%20non-linear%20regression.&entry.1838667208=http%3A//arxiv.org/abs/2512.08879v1&entry.124074799=Read"},
{"title": "Solving Over-Smoothing in GNNs via Nonlocal Message Passing: Algebraic Smoothing and Depth Scalability", "author": "Weiqi Guan and Junlin He", "abstract": "The relationship between Layer Normalization (LN) placement and the over-smoothing phenomenon remains underexplored. We identify a critical dilemma: Pre-LN architectures avoid over-smoothing but suffer from the curse of depth, while Post-LN architectures bypass the curse of depth but experience over-smoothing.\n  To resolve this, we propose a new method based on Post-LN that induces algebraic smoothing, preventing over-smoothing without the curse of depth. Empirical results across five benchmarks demonstrate that our approach supports deeper networks (up to 256 layers) and improves performance, requiring no additional parameters.\n  Key contributions:\n  Theoretical Characterization: Analysis of LN dynamics and their impact on over-smoothing and the curse of depth.\n  A Principled Solution: A parameter-efficient method that induces algebraic smoothing and avoids over-smoothing and the curse of depth.\n  Empirical Validation: Extensive experiments showing the effectiveness of the method in deeper GNNs.", "link": "http://arxiv.org/abs/2512.08475v1", "date": "2025-12-09", "relevancy": 2.4536, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5039}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4945}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Solving%20Over-Smoothing%20in%20GNNs%20via%20Nonlocal%20Message%20Passing%3A%20Algebraic%20Smoothing%20and%20Depth%20Scalability&body=Title%3A%20Solving%20Over-Smoothing%20in%20GNNs%20via%20Nonlocal%20Message%20Passing%3A%20Algebraic%20Smoothing%20and%20Depth%20Scalability%0AAuthor%3A%20Weiqi%20Guan%20and%20Junlin%20He%0AAbstract%3A%20The%20relationship%20between%20Layer%20Normalization%20%28LN%29%20placement%20and%20the%20over-smoothing%20phenomenon%20remains%20underexplored.%20We%20identify%20a%20critical%20dilemma%3A%20Pre-LN%20architectures%20avoid%20over-smoothing%20but%20suffer%20from%20the%20curse%20of%20depth%2C%20while%20Post-LN%20architectures%20bypass%20the%20curse%20of%20depth%20but%20experience%20over-smoothing.%0A%20%20To%20resolve%20this%2C%20we%20propose%20a%20new%20method%20based%20on%20Post-LN%20that%20induces%20algebraic%20smoothing%2C%20preventing%20over-smoothing%20without%20the%20curse%20of%20depth.%20Empirical%20results%20across%20five%20benchmarks%20demonstrate%20that%20our%20approach%20supports%20deeper%20networks%20%28up%20to%20256%20layers%29%20and%20improves%20performance%2C%20requiring%20no%20additional%20parameters.%0A%20%20Key%20contributions%3A%0A%20%20Theoretical%20Characterization%3A%20Analysis%20of%20LN%20dynamics%20and%20their%20impact%20on%20over-smoothing%20and%20the%20curse%20of%20depth.%0A%20%20A%20Principled%20Solution%3A%20A%20parameter-efficient%20method%20that%20induces%20algebraic%20smoothing%20and%20avoids%20over-smoothing%20and%20the%20curse%20of%20depth.%0A%20%20Empirical%20Validation%3A%20Extensive%20experiments%20showing%20the%20effectiveness%20of%20the%20method%20in%20deeper%20GNNs.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08475v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSolving%2520Over-Smoothing%2520in%2520GNNs%2520via%2520Nonlocal%2520Message%2520Passing%253A%2520Algebraic%2520Smoothing%2520and%2520Depth%2520Scalability%26entry.906535625%3DWeiqi%2520Guan%2520and%2520Junlin%2520He%26entry.1292438233%3DThe%2520relationship%2520between%2520Layer%2520Normalization%2520%2528LN%2529%2520placement%2520and%2520the%2520over-smoothing%2520phenomenon%2520remains%2520underexplored.%2520We%2520identify%2520a%2520critical%2520dilemma%253A%2520Pre-LN%2520architectures%2520avoid%2520over-smoothing%2520but%2520suffer%2520from%2520the%2520curse%2520of%2520depth%252C%2520while%2520Post-LN%2520architectures%2520bypass%2520the%2520curse%2520of%2520depth%2520but%2520experience%2520over-smoothing.%250A%2520%2520To%2520resolve%2520this%252C%2520we%2520propose%2520a%2520new%2520method%2520based%2520on%2520Post-LN%2520that%2520induces%2520algebraic%2520smoothing%252C%2520preventing%2520over-smoothing%2520without%2520the%2520curse%2520of%2520depth.%2520Empirical%2520results%2520across%2520five%2520benchmarks%2520demonstrate%2520that%2520our%2520approach%2520supports%2520deeper%2520networks%2520%2528up%2520to%2520256%2520layers%2529%2520and%2520improves%2520performance%252C%2520requiring%2520no%2520additional%2520parameters.%250A%2520%2520Key%2520contributions%253A%250A%2520%2520Theoretical%2520Characterization%253A%2520Analysis%2520of%2520LN%2520dynamics%2520and%2520their%2520impact%2520on%2520over-smoothing%2520and%2520the%2520curse%2520of%2520depth.%250A%2520%2520A%2520Principled%2520Solution%253A%2520A%2520parameter-efficient%2520method%2520that%2520induces%2520algebraic%2520smoothing%2520and%2520avoids%2520over-smoothing%2520and%2520the%2520curse%2520of%2520depth.%250A%2520%2520Empirical%2520Validation%253A%2520Extensive%2520experiments%2520showing%2520the%2520effectiveness%2520of%2520the%2520method%2520in%2520deeper%2520GNNs.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08475v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Solving%20Over-Smoothing%20in%20GNNs%20via%20Nonlocal%20Message%20Passing%3A%20Algebraic%20Smoothing%20and%20Depth%20Scalability&entry.906535625=Weiqi%20Guan%20and%20Junlin%20He&entry.1292438233=The%20relationship%20between%20Layer%20Normalization%20%28LN%29%20placement%20and%20the%20over-smoothing%20phenomenon%20remains%20underexplored.%20We%20identify%20a%20critical%20dilemma%3A%20Pre-LN%20architectures%20avoid%20over-smoothing%20but%20suffer%20from%20the%20curse%20of%20depth%2C%20while%20Post-LN%20architectures%20bypass%20the%20curse%20of%20depth%20but%20experience%20over-smoothing.%0A%20%20To%20resolve%20this%2C%20we%20propose%20a%20new%20method%20based%20on%20Post-LN%20that%20induces%20algebraic%20smoothing%2C%20preventing%20over-smoothing%20without%20the%20curse%20of%20depth.%20Empirical%20results%20across%20five%20benchmarks%20demonstrate%20that%20our%20approach%20supports%20deeper%20networks%20%28up%20to%20256%20layers%29%20and%20improves%20performance%2C%20requiring%20no%20additional%20parameters.%0A%20%20Key%20contributions%3A%0A%20%20Theoretical%20Characterization%3A%20Analysis%20of%20LN%20dynamics%20and%20their%20impact%20on%20over-smoothing%20and%20the%20curse%20of%20depth.%0A%20%20A%20Principled%20Solution%3A%20A%20parameter-efficient%20method%20that%20induces%20algebraic%20smoothing%20and%20avoids%20over-smoothing%20and%20the%20curse%20of%20depth.%0A%20%20Empirical%20Validation%3A%20Extensive%20experiments%20showing%20the%20effectiveness%20of%20the%20method%20in%20deeper%20GNNs.&entry.1838667208=http%3A//arxiv.org/abs/2512.08475v1&entry.124074799=Read"},
{"title": "On the Temporality for Sketch Representation Learning", "author": "Marcelo Isaias de Moraes Junior and Moacir Antonelli Ponti", "abstract": "Sketches are simple human hand-drawn abstractions of complex scenes and real-world objects. Although the field of sketch representation learning has advanced significantly, there is still a gap in understanding the true relevance of the temporal aspect to the quality of these representations. This work investigates whether it is indeed justifiable to treat sketches as sequences, as well as which internal orders play a more relevant role. The results indicate that, although the use of traditional positional encodings is valid for modeling sketches as sequences, absolute coordinates consistently outperform relative ones. Furthermore, non-autoregressive decoders outperform their autoregressive counterparts. Finally, the importance of temporality was shown to depend on both the order considered and the task evaluated.", "link": "http://arxiv.org/abs/2512.04007v2", "date": "2025-12-09", "relevancy": 2.4519, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5177}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4794}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.474}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Temporality%20for%20Sketch%20Representation%20Learning&body=Title%3A%20On%20the%20Temporality%20for%20Sketch%20Representation%20Learning%0AAuthor%3A%20Marcelo%20Isaias%20de%20Moraes%20Junior%20and%20Moacir%20Antonelli%20Ponti%0AAbstract%3A%20Sketches%20are%20simple%20human%20hand-drawn%20abstractions%20of%20complex%20scenes%20and%20real-world%20objects.%20Although%20the%20field%20of%20sketch%20representation%20learning%20has%20advanced%20significantly%2C%20there%20is%20still%20a%20gap%20in%20understanding%20the%20true%20relevance%20of%20the%20temporal%20aspect%20to%20the%20quality%20of%20these%20representations.%20This%20work%20investigates%20whether%20it%20is%20indeed%20justifiable%20to%20treat%20sketches%20as%20sequences%2C%20as%20well%20as%20which%20internal%20orders%20play%20a%20more%20relevant%20role.%20The%20results%20indicate%20that%2C%20although%20the%20use%20of%20traditional%20positional%20encodings%20is%20valid%20for%20modeling%20sketches%20as%20sequences%2C%20absolute%20coordinates%20consistently%20outperform%20relative%20ones.%20Furthermore%2C%20non-autoregressive%20decoders%20outperform%20their%20autoregressive%20counterparts.%20Finally%2C%20the%20importance%20of%20temporality%20was%20shown%20to%20depend%20on%20both%20the%20order%20considered%20and%20the%20task%20evaluated.%0ALink%3A%20http%3A//arxiv.org/abs/2512.04007v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Temporality%2520for%2520Sketch%2520Representation%2520Learning%26entry.906535625%3DMarcelo%2520Isaias%2520de%2520Moraes%2520Junior%2520and%2520Moacir%2520Antonelli%2520Ponti%26entry.1292438233%3DSketches%2520are%2520simple%2520human%2520hand-drawn%2520abstractions%2520of%2520complex%2520scenes%2520and%2520real-world%2520objects.%2520Although%2520the%2520field%2520of%2520sketch%2520representation%2520learning%2520has%2520advanced%2520significantly%252C%2520there%2520is%2520still%2520a%2520gap%2520in%2520understanding%2520the%2520true%2520relevance%2520of%2520the%2520temporal%2520aspect%2520to%2520the%2520quality%2520of%2520these%2520representations.%2520This%2520work%2520investigates%2520whether%2520it%2520is%2520indeed%2520justifiable%2520to%2520treat%2520sketches%2520as%2520sequences%252C%2520as%2520well%2520as%2520which%2520internal%2520orders%2520play%2520a%2520more%2520relevant%2520role.%2520The%2520results%2520indicate%2520that%252C%2520although%2520the%2520use%2520of%2520traditional%2520positional%2520encodings%2520is%2520valid%2520for%2520modeling%2520sketches%2520as%2520sequences%252C%2520absolute%2520coordinates%2520consistently%2520outperform%2520relative%2520ones.%2520Furthermore%252C%2520non-autoregressive%2520decoders%2520outperform%2520their%2520autoregressive%2520counterparts.%2520Finally%252C%2520the%2520importance%2520of%2520temporality%2520was%2520shown%2520to%2520depend%2520on%2520both%2520the%2520order%2520considered%2520and%2520the%2520task%2520evaluated.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.04007v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Temporality%20for%20Sketch%20Representation%20Learning&entry.906535625=Marcelo%20Isaias%20de%20Moraes%20Junior%20and%20Moacir%20Antonelli%20Ponti&entry.1292438233=Sketches%20are%20simple%20human%20hand-drawn%20abstractions%20of%20complex%20scenes%20and%20real-world%20objects.%20Although%20the%20field%20of%20sketch%20representation%20learning%20has%20advanced%20significantly%2C%20there%20is%20still%20a%20gap%20in%20understanding%20the%20true%20relevance%20of%20the%20temporal%20aspect%20to%20the%20quality%20of%20these%20representations.%20This%20work%20investigates%20whether%20it%20is%20indeed%20justifiable%20to%20treat%20sketches%20as%20sequences%2C%20as%20well%20as%20which%20internal%20orders%20play%20a%20more%20relevant%20role.%20The%20results%20indicate%20that%2C%20although%20the%20use%20of%20traditional%20positional%20encodings%20is%20valid%20for%20modeling%20sketches%20as%20sequences%2C%20absolute%20coordinates%20consistently%20outperform%20relative%20ones.%20Furthermore%2C%20non-autoregressive%20decoders%20outperform%20their%20autoregressive%20counterparts.%20Finally%2C%20the%20importance%20of%20temporality%20was%20shown%20to%20depend%20on%20both%20the%20order%20considered%20and%20the%20task%20evaluated.&entry.1838667208=http%3A//arxiv.org/abs/2512.04007v2&entry.124074799=Read"},
{"title": "Spike-EVPR: Deep Spiking Residual Networks with SNN-Tailored Representations for Event-Based Visual Place Recognition", "author": "Zuntao Liu and Yaohui Li and Chenming Hu and Delei Kong and Junjie Jiang and Zheng Fang", "abstract": "Event cameras are ideal for visual place recognition (VPR) in challenging environments due to their high temporal resolution and high dynamic range. However, existing methods convert sparse events into dense frame-like representations for Artificial Neural Networks (ANNs), ignoring event sparsity and incurring high computational cost. Spiking Neural Networks (SNNs) complement event data through discrete spike signals to enable energy-efficient VPR, but their application is hindered by the lack of effective spike-compatible representations and deep architectures capable of learning discriminative global descriptors. To address these limitations, we propose Spike-EVPR, a directly trained, end-to-end SNN framework tailored for event-based VPR. First, we introduce two complementary event representations, MCS-Tensor and TSS-Tensor, designed to reduce temporal redundancy while preserving essential spatio-temporal cues. Furthermore, we propose a deep spiking residual architecture that effectively aggregates these features to generate robust place descriptors. Extensive experiments on the Brisbane-Event-VPR and DDD20 datasets demonstrate that Spike-EVPR achieves state-of-the-art performance, improving Recall@1 by 7.61% and 13.20%, respectively, while significantly reducing energy consumption.", "link": "http://arxiv.org/abs/2402.10476v2", "date": "2025-12-09", "relevancy": 2.4334, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5009}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4977}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4614}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Spike-EVPR%3A%20Deep%20Spiking%20Residual%20Networks%20with%20SNN-Tailored%20Representations%20for%20Event-Based%20Visual%20Place%20Recognition&body=Title%3A%20Spike-EVPR%3A%20Deep%20Spiking%20Residual%20Networks%20with%20SNN-Tailored%20Representations%20for%20Event-Based%20Visual%20Place%20Recognition%0AAuthor%3A%20Zuntao%20Liu%20and%20Yaohui%20Li%20and%20Chenming%20Hu%20and%20Delei%20Kong%20and%20Junjie%20Jiang%20and%20Zheng%20Fang%0AAbstract%3A%20Event%20cameras%20are%20ideal%20for%20visual%20place%20recognition%20%28VPR%29%20in%20challenging%20environments%20due%20to%20their%20high%20temporal%20resolution%20and%20high%20dynamic%20range.%20However%2C%20existing%20methods%20convert%20sparse%20events%20into%20dense%20frame-like%20representations%20for%20Artificial%20Neural%20Networks%20%28ANNs%29%2C%20ignoring%20event%20sparsity%20and%20incurring%20high%20computational%20cost.%20Spiking%20Neural%20Networks%20%28SNNs%29%20complement%20event%20data%20through%20discrete%20spike%20signals%20to%20enable%20energy-efficient%20VPR%2C%20but%20their%20application%20is%20hindered%20by%20the%20lack%20of%20effective%20spike-compatible%20representations%20and%20deep%20architectures%20capable%20of%20learning%20discriminative%20global%20descriptors.%20To%20address%20these%20limitations%2C%20we%20propose%20Spike-EVPR%2C%20a%20directly%20trained%2C%20end-to-end%20SNN%20framework%20tailored%20for%20event-based%20VPR.%20First%2C%20we%20introduce%20two%20complementary%20event%20representations%2C%20MCS-Tensor%20and%20TSS-Tensor%2C%20designed%20to%20reduce%20temporal%20redundancy%20while%20preserving%20essential%20spatio-temporal%20cues.%20Furthermore%2C%20we%20propose%20a%20deep%20spiking%20residual%20architecture%20that%20effectively%20aggregates%20these%20features%20to%20generate%20robust%20place%20descriptors.%20Extensive%20experiments%20on%20the%20Brisbane-Event-VPR%20and%20DDD20%20datasets%20demonstrate%20that%20Spike-EVPR%20achieves%20state-of-the-art%20performance%2C%20improving%20Recall%401%20by%207.61%25%20and%2013.20%25%2C%20respectively%2C%20while%20significantly%20reducing%20energy%20consumption.%0ALink%3A%20http%3A//arxiv.org/abs/2402.10476v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSpike-EVPR%253A%2520Deep%2520Spiking%2520Residual%2520Networks%2520with%2520SNN-Tailored%2520Representations%2520for%2520Event-Based%2520Visual%2520Place%2520Recognition%26entry.906535625%3DZuntao%2520Liu%2520and%2520Yaohui%2520Li%2520and%2520Chenming%2520Hu%2520and%2520Delei%2520Kong%2520and%2520Junjie%2520Jiang%2520and%2520Zheng%2520Fang%26entry.1292438233%3DEvent%2520cameras%2520are%2520ideal%2520for%2520visual%2520place%2520recognition%2520%2528VPR%2529%2520in%2520challenging%2520environments%2520due%2520to%2520their%2520high%2520temporal%2520resolution%2520and%2520high%2520dynamic%2520range.%2520However%252C%2520existing%2520methods%2520convert%2520sparse%2520events%2520into%2520dense%2520frame-like%2520representations%2520for%2520Artificial%2520Neural%2520Networks%2520%2528ANNs%2529%252C%2520ignoring%2520event%2520sparsity%2520and%2520incurring%2520high%2520computational%2520cost.%2520Spiking%2520Neural%2520Networks%2520%2528SNNs%2529%2520complement%2520event%2520data%2520through%2520discrete%2520spike%2520signals%2520to%2520enable%2520energy-efficient%2520VPR%252C%2520but%2520their%2520application%2520is%2520hindered%2520by%2520the%2520lack%2520of%2520effective%2520spike-compatible%2520representations%2520and%2520deep%2520architectures%2520capable%2520of%2520learning%2520discriminative%2520global%2520descriptors.%2520To%2520address%2520these%2520limitations%252C%2520we%2520propose%2520Spike-EVPR%252C%2520a%2520directly%2520trained%252C%2520end-to-end%2520SNN%2520framework%2520tailored%2520for%2520event-based%2520VPR.%2520First%252C%2520we%2520introduce%2520two%2520complementary%2520event%2520representations%252C%2520MCS-Tensor%2520and%2520TSS-Tensor%252C%2520designed%2520to%2520reduce%2520temporal%2520redundancy%2520while%2520preserving%2520essential%2520spatio-temporal%2520cues.%2520Furthermore%252C%2520we%2520propose%2520a%2520deep%2520spiking%2520residual%2520architecture%2520that%2520effectively%2520aggregates%2520these%2520features%2520to%2520generate%2520robust%2520place%2520descriptors.%2520Extensive%2520experiments%2520on%2520the%2520Brisbane-Event-VPR%2520and%2520DDD20%2520datasets%2520demonstrate%2520that%2520Spike-EVPR%2520achieves%2520state-of-the-art%2520performance%252C%2520improving%2520Recall%25401%2520by%25207.61%2525%2520and%252013.20%2525%252C%2520respectively%252C%2520while%2520significantly%2520reducing%2520energy%2520consumption.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.10476v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Spike-EVPR%3A%20Deep%20Spiking%20Residual%20Networks%20with%20SNN-Tailored%20Representations%20for%20Event-Based%20Visual%20Place%20Recognition&entry.906535625=Zuntao%20Liu%20and%20Yaohui%20Li%20and%20Chenming%20Hu%20and%20Delei%20Kong%20and%20Junjie%20Jiang%20and%20Zheng%20Fang&entry.1292438233=Event%20cameras%20are%20ideal%20for%20visual%20place%20recognition%20%28VPR%29%20in%20challenging%20environments%20due%20to%20their%20high%20temporal%20resolution%20and%20high%20dynamic%20range.%20However%2C%20existing%20methods%20convert%20sparse%20events%20into%20dense%20frame-like%20representations%20for%20Artificial%20Neural%20Networks%20%28ANNs%29%2C%20ignoring%20event%20sparsity%20and%20incurring%20high%20computational%20cost.%20Spiking%20Neural%20Networks%20%28SNNs%29%20complement%20event%20data%20through%20discrete%20spike%20signals%20to%20enable%20energy-efficient%20VPR%2C%20but%20their%20application%20is%20hindered%20by%20the%20lack%20of%20effective%20spike-compatible%20representations%20and%20deep%20architectures%20capable%20of%20learning%20discriminative%20global%20descriptors.%20To%20address%20these%20limitations%2C%20we%20propose%20Spike-EVPR%2C%20a%20directly%20trained%2C%20end-to-end%20SNN%20framework%20tailored%20for%20event-based%20VPR.%20First%2C%20we%20introduce%20two%20complementary%20event%20representations%2C%20MCS-Tensor%20and%20TSS-Tensor%2C%20designed%20to%20reduce%20temporal%20redundancy%20while%20preserving%20essential%20spatio-temporal%20cues.%20Furthermore%2C%20we%20propose%20a%20deep%20spiking%20residual%20architecture%20that%20effectively%20aggregates%20these%20features%20to%20generate%20robust%20place%20descriptors.%20Extensive%20experiments%20on%20the%20Brisbane-Event-VPR%20and%20DDD20%20datasets%20demonstrate%20that%20Spike-EVPR%20achieves%20state-of-the-art%20performance%2C%20improving%20Recall%401%20by%207.61%25%20and%2013.20%25%2C%20respectively%2C%20while%20significantly%20reducing%20energy%20consumption.&entry.1838667208=http%3A//arxiv.org/abs/2402.10476v2&entry.124074799=Read"},
{"title": "Pay Less Attention to Function Words for Free Robustness of Vision-Language Models", "author": "Qiwei Tian and Chenhao Lin and Zhengyu Zhao and Chao Shen", "abstract": "To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.", "link": "http://arxiv.org/abs/2512.07222v2", "date": "2025-12-09", "relevancy": 2.4252, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4967}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4792}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4792}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Pay%20Less%20Attention%20to%20Function%20Words%20for%20Free%20Robustness%20of%20Vision-Language%20Models&body=Title%3A%20Pay%20Less%20Attention%20to%20Function%20Words%20for%20Free%20Robustness%20of%20Vision-Language%20Models%0AAuthor%3A%20Qiwei%20Tian%20and%20Chenhao%20Lin%20and%20Zhengyu%20Zhao%20and%20Chao%20Shen%0AAbstract%3A%20To%20address%20the%20trade-off%20between%20robustness%20and%20performance%20for%20robust%20VLM%2C%20we%20observe%20that%20function%20words%20could%20incur%20vulnerability%20of%20VLMs%20against%20cross-modal%20adversarial%20attacks%2C%20and%20propose%20Function-word%20De-Attention%20%28FDA%29%20accordingly%20to%20mitigate%20the%20impact%20of%20function%20words.%20Similar%20to%20differential%20amplifiers%2C%20our%20FDA%20calculates%20the%20original%20and%20the%20function-word%20cross-attention%20within%20attention%20heads%2C%20and%20differentially%20subtracts%20the%20latter%20from%20the%20former%20for%20more%20aligned%20and%20robust%20VLMs.%20Comprehensive%20experiments%20include%202%20SOTA%20baselines%20under%206%20different%20attacks%20on%202%20downstream%20tasks%2C%203%20datasets%2C%20and%203%20models.%20Overall%2C%20our%20FDA%20yields%20an%20average%2018/13/53%25%20ASR%20drop%20with%20only%200.2/0.3/0.6%25%20performance%20drops%20on%20the%203%20tested%20models%20on%20retrieval%2C%20and%20a%2090%25%20ASR%20drop%20with%20a%200.3%25%20performance%20gain%20on%20visual%20grounding.%20We%20demonstrate%20the%20scalability%2C%20generalization%2C%20and%20zero-shot%20performance%20of%20FDA%20experimentally%2C%20as%20well%20as%20in-depth%20ablation%20studies%20and%20analysis.%20Code%20will%20be%20made%20publicly%20at%20https%3A//github.com/michaeltian108/FDA.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07222v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPay%2520Less%2520Attention%2520to%2520Function%2520Words%2520for%2520Free%2520Robustness%2520of%2520Vision-Language%2520Models%26entry.906535625%3DQiwei%2520Tian%2520and%2520Chenhao%2520Lin%2520and%2520Zhengyu%2520Zhao%2520and%2520Chao%2520Shen%26entry.1292438233%3DTo%2520address%2520the%2520trade-off%2520between%2520robustness%2520and%2520performance%2520for%2520robust%2520VLM%252C%2520we%2520observe%2520that%2520function%2520words%2520could%2520incur%2520vulnerability%2520of%2520VLMs%2520against%2520cross-modal%2520adversarial%2520attacks%252C%2520and%2520propose%2520Function-word%2520De-Attention%2520%2528FDA%2529%2520accordingly%2520to%2520mitigate%2520the%2520impact%2520of%2520function%2520words.%2520Similar%2520to%2520differential%2520amplifiers%252C%2520our%2520FDA%2520calculates%2520the%2520original%2520and%2520the%2520function-word%2520cross-attention%2520within%2520attention%2520heads%252C%2520and%2520differentially%2520subtracts%2520the%2520latter%2520from%2520the%2520former%2520for%2520more%2520aligned%2520and%2520robust%2520VLMs.%2520Comprehensive%2520experiments%2520include%25202%2520SOTA%2520baselines%2520under%25206%2520different%2520attacks%2520on%25202%2520downstream%2520tasks%252C%25203%2520datasets%252C%2520and%25203%2520models.%2520Overall%252C%2520our%2520FDA%2520yields%2520an%2520average%252018/13/53%2525%2520ASR%2520drop%2520with%2520only%25200.2/0.3/0.6%2525%2520performance%2520drops%2520on%2520the%25203%2520tested%2520models%2520on%2520retrieval%252C%2520and%2520a%252090%2525%2520ASR%2520drop%2520with%2520a%25200.3%2525%2520performance%2520gain%2520on%2520visual%2520grounding.%2520We%2520demonstrate%2520the%2520scalability%252C%2520generalization%252C%2520and%2520zero-shot%2520performance%2520of%2520FDA%2520experimentally%252C%2520as%2520well%2520as%2520in-depth%2520ablation%2520studies%2520and%2520analysis.%2520Code%2520will%2520be%2520made%2520publicly%2520at%2520https%253A//github.com/michaeltian108/FDA.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07222v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Pay%20Less%20Attention%20to%20Function%20Words%20for%20Free%20Robustness%20of%20Vision-Language%20Models&entry.906535625=Qiwei%20Tian%20and%20Chenhao%20Lin%20and%20Zhengyu%20Zhao%20and%20Chao%20Shen&entry.1292438233=To%20address%20the%20trade-off%20between%20robustness%20and%20performance%20for%20robust%20VLM%2C%20we%20observe%20that%20function%20words%20could%20incur%20vulnerability%20of%20VLMs%20against%20cross-modal%20adversarial%20attacks%2C%20and%20propose%20Function-word%20De-Attention%20%28FDA%29%20accordingly%20to%20mitigate%20the%20impact%20of%20function%20words.%20Similar%20to%20differential%20amplifiers%2C%20our%20FDA%20calculates%20the%20original%20and%20the%20function-word%20cross-attention%20within%20attention%20heads%2C%20and%20differentially%20subtracts%20the%20latter%20from%20the%20former%20for%20more%20aligned%20and%20robust%20VLMs.%20Comprehensive%20experiments%20include%202%20SOTA%20baselines%20under%206%20different%20attacks%20on%202%20downstream%20tasks%2C%203%20datasets%2C%20and%203%20models.%20Overall%2C%20our%20FDA%20yields%20an%20average%2018/13/53%25%20ASR%20drop%20with%20only%200.2/0.3/0.6%25%20performance%20drops%20on%20the%203%20tested%20models%20on%20retrieval%2C%20and%20a%2090%25%20ASR%20drop%20with%20a%200.3%25%20performance%20gain%20on%20visual%20grounding.%20We%20demonstrate%20the%20scalability%2C%20generalization%2C%20and%20zero-shot%20performance%20of%20FDA%20experimentally%2C%20as%20well%20as%20in-depth%20ablation%20studies%20and%20analysis.%20Code%20will%20be%20made%20publicly%20at%20https%3A//github.com/michaeltian108/FDA.&entry.1838667208=http%3A//arxiv.org/abs/2512.07222v2&entry.124074799=Read"},
{"title": "Mathematical Proof as a Litmus Test: Revealing Failure Modes of Advanced Large Reasoning Models", "author": "Dadi Guo and Jiayu Liu and Zhiyuan Fan and Zhitao He and Haoran Li and Yuxin Li and Yumeng Wang and Yi R. Fung", "abstract": "Large reasoning models (e.g., R1, o3) have demonstrated remarkable mathematical problem-solving abilities. However, the high reported accuracy of these advanced models on popular datasets, reliance on purely numerical evaluation and potential benchmark leakage, often masks their true reasoning shortcomings. To address this, we propose leveraging the inherent rigor and methodological complexity of mathematical proofs as a diagnostic tool to expose these hidden failures. Specifically, we introduce the RFMDataset (Reveal Failure Modes), a collection of 200 diverse mathematical proof problems, and thoroughly evaluate advanced models' performance on it. Our in-depth analysis of their failures uncovers 10 fine-grained error types, which shows fundamental limitations in current large reasoning models: 1) large reasoning models grapple profoundly with mathematical proofs, with some generating entirely correct proofs for less than 20% of problems and failing even on basic ones; 2) models exhibit a diverse spectrum of reasoning failures, prominently demonstrating the lack of guarantees for the correctness and rigor of single-step reasoning; and 3) models show hallucination and incompleteness during the reasoning process. Our findings reveal that models' self-reflection is insufficient to resolve the current logical dilemmas, necessitating formalized and fine-grained logical training.", "link": "http://arxiv.org/abs/2506.17114v4", "date": "2025-12-09", "relevancy": 2.4018, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4877}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4877}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4657}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mathematical%20Proof%20as%20a%20Litmus%20Test%3A%20Revealing%20Failure%20Modes%20of%20Advanced%20Large%20Reasoning%20Models&body=Title%3A%20Mathematical%20Proof%20as%20a%20Litmus%20Test%3A%20Revealing%20Failure%20Modes%20of%20Advanced%20Large%20Reasoning%20Models%0AAuthor%3A%20Dadi%20Guo%20and%20Jiayu%20Liu%20and%20Zhiyuan%20Fan%20and%20Zhitao%20He%20and%20Haoran%20Li%20and%20Yuxin%20Li%20and%20Yumeng%20Wang%20and%20Yi%20R.%20Fung%0AAbstract%3A%20Large%20reasoning%20models%20%28e.g.%2C%20R1%2C%20o3%29%20have%20demonstrated%20remarkable%20mathematical%20problem-solving%20abilities.%20However%2C%20the%20high%20reported%20accuracy%20of%20these%20advanced%20models%20on%20popular%20datasets%2C%20reliance%20on%20purely%20numerical%20evaluation%20and%20potential%20benchmark%20leakage%2C%20often%20masks%20their%20true%20reasoning%20shortcomings.%20To%20address%20this%2C%20we%20propose%20leveraging%20the%20inherent%20rigor%20and%20methodological%20complexity%20of%20mathematical%20proofs%20as%20a%20diagnostic%20tool%20to%20expose%20these%20hidden%20failures.%20Specifically%2C%20we%20introduce%20the%20RFMDataset%20%28Reveal%20Failure%20Modes%29%2C%20a%20collection%20of%20200%20diverse%20mathematical%20proof%20problems%2C%20and%20thoroughly%20evaluate%20advanced%20models%27%20performance%20on%20it.%20Our%20in-depth%20analysis%20of%20their%20failures%20uncovers%2010%20fine-grained%20error%20types%2C%20which%20shows%20fundamental%20limitations%20in%20current%20large%20reasoning%20models%3A%201%29%20large%20reasoning%20models%20grapple%20profoundly%20with%20mathematical%20proofs%2C%20with%20some%20generating%20entirely%20correct%20proofs%20for%20less%20than%2020%25%20of%20problems%20and%20failing%20even%20on%20basic%20ones%3B%202%29%20models%20exhibit%20a%20diverse%20spectrum%20of%20reasoning%20failures%2C%20prominently%20demonstrating%20the%20lack%20of%20guarantees%20for%20the%20correctness%20and%20rigor%20of%20single-step%20reasoning%3B%20and%203%29%20models%20show%20hallucination%20and%20incompleteness%20during%20the%20reasoning%20process.%20Our%20findings%20reveal%20that%20models%27%20self-reflection%20is%20insufficient%20to%20resolve%20the%20current%20logical%20dilemmas%2C%20necessitating%20formalized%20and%20fine-grained%20logical%20training.%0ALink%3A%20http%3A//arxiv.org/abs/2506.17114v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMathematical%2520Proof%2520as%2520a%2520Litmus%2520Test%253A%2520Revealing%2520Failure%2520Modes%2520of%2520Advanced%2520Large%2520Reasoning%2520Models%26entry.906535625%3DDadi%2520Guo%2520and%2520Jiayu%2520Liu%2520and%2520Zhiyuan%2520Fan%2520and%2520Zhitao%2520He%2520and%2520Haoran%2520Li%2520and%2520Yuxin%2520Li%2520and%2520Yumeng%2520Wang%2520and%2520Yi%2520R.%2520Fung%26entry.1292438233%3DLarge%2520reasoning%2520models%2520%2528e.g.%252C%2520R1%252C%2520o3%2529%2520have%2520demonstrated%2520remarkable%2520mathematical%2520problem-solving%2520abilities.%2520However%252C%2520the%2520high%2520reported%2520accuracy%2520of%2520these%2520advanced%2520models%2520on%2520popular%2520datasets%252C%2520reliance%2520on%2520purely%2520numerical%2520evaluation%2520and%2520potential%2520benchmark%2520leakage%252C%2520often%2520masks%2520their%2520true%2520reasoning%2520shortcomings.%2520To%2520address%2520this%252C%2520we%2520propose%2520leveraging%2520the%2520inherent%2520rigor%2520and%2520methodological%2520complexity%2520of%2520mathematical%2520proofs%2520as%2520a%2520diagnostic%2520tool%2520to%2520expose%2520these%2520hidden%2520failures.%2520Specifically%252C%2520we%2520introduce%2520the%2520RFMDataset%2520%2528Reveal%2520Failure%2520Modes%2529%252C%2520a%2520collection%2520of%2520200%2520diverse%2520mathematical%2520proof%2520problems%252C%2520and%2520thoroughly%2520evaluate%2520advanced%2520models%2527%2520performance%2520on%2520it.%2520Our%2520in-depth%2520analysis%2520of%2520their%2520failures%2520uncovers%252010%2520fine-grained%2520error%2520types%252C%2520which%2520shows%2520fundamental%2520limitations%2520in%2520current%2520large%2520reasoning%2520models%253A%25201%2529%2520large%2520reasoning%2520models%2520grapple%2520profoundly%2520with%2520mathematical%2520proofs%252C%2520with%2520some%2520generating%2520entirely%2520correct%2520proofs%2520for%2520less%2520than%252020%2525%2520of%2520problems%2520and%2520failing%2520even%2520on%2520basic%2520ones%253B%25202%2529%2520models%2520exhibit%2520a%2520diverse%2520spectrum%2520of%2520reasoning%2520failures%252C%2520prominently%2520demonstrating%2520the%2520lack%2520of%2520guarantees%2520for%2520the%2520correctness%2520and%2520rigor%2520of%2520single-step%2520reasoning%253B%2520and%25203%2529%2520models%2520show%2520hallucination%2520and%2520incompleteness%2520during%2520the%2520reasoning%2520process.%2520Our%2520findings%2520reveal%2520that%2520models%2527%2520self-reflection%2520is%2520insufficient%2520to%2520resolve%2520the%2520current%2520logical%2520dilemmas%252C%2520necessitating%2520formalized%2520and%2520fine-grained%2520logical%2520training.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.17114v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mathematical%20Proof%20as%20a%20Litmus%20Test%3A%20Revealing%20Failure%20Modes%20of%20Advanced%20Large%20Reasoning%20Models&entry.906535625=Dadi%20Guo%20and%20Jiayu%20Liu%20and%20Zhiyuan%20Fan%20and%20Zhitao%20He%20and%20Haoran%20Li%20and%20Yuxin%20Li%20and%20Yumeng%20Wang%20and%20Yi%20R.%20Fung&entry.1292438233=Large%20reasoning%20models%20%28e.g.%2C%20R1%2C%20o3%29%20have%20demonstrated%20remarkable%20mathematical%20problem-solving%20abilities.%20However%2C%20the%20high%20reported%20accuracy%20of%20these%20advanced%20models%20on%20popular%20datasets%2C%20reliance%20on%20purely%20numerical%20evaluation%20and%20potential%20benchmark%20leakage%2C%20often%20masks%20their%20true%20reasoning%20shortcomings.%20To%20address%20this%2C%20we%20propose%20leveraging%20the%20inherent%20rigor%20and%20methodological%20complexity%20of%20mathematical%20proofs%20as%20a%20diagnostic%20tool%20to%20expose%20these%20hidden%20failures.%20Specifically%2C%20we%20introduce%20the%20RFMDataset%20%28Reveal%20Failure%20Modes%29%2C%20a%20collection%20of%20200%20diverse%20mathematical%20proof%20problems%2C%20and%20thoroughly%20evaluate%20advanced%20models%27%20performance%20on%20it.%20Our%20in-depth%20analysis%20of%20their%20failures%20uncovers%2010%20fine-grained%20error%20types%2C%20which%20shows%20fundamental%20limitations%20in%20current%20large%20reasoning%20models%3A%201%29%20large%20reasoning%20models%20grapple%20profoundly%20with%20mathematical%20proofs%2C%20with%20some%20generating%20entirely%20correct%20proofs%20for%20less%20than%2020%25%20of%20problems%20and%20failing%20even%20on%20basic%20ones%3B%202%29%20models%20exhibit%20a%20diverse%20spectrum%20of%20reasoning%20failures%2C%20prominently%20demonstrating%20the%20lack%20of%20guarantees%20for%20the%20correctness%20and%20rigor%20of%20single-step%20reasoning%3B%20and%203%29%20models%20show%20hallucination%20and%20incompleteness%20during%20the%20reasoning%20process.%20Our%20findings%20reveal%20that%20models%27%20self-reflection%20is%20insufficient%20to%20resolve%20the%20current%20logical%20dilemmas%2C%20necessitating%20formalized%20and%20fine-grained%20logical%20training.&entry.1838667208=http%3A//arxiv.org/abs/2506.17114v4&entry.124074799=Read"},
{"title": "A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation", "author": "Art\u00far I. K\u00e1roly and P\u00e9ter Galambos", "abstract": "Industrial mushroom cultivation increasingly relies on computer vision for monitoring and automated harvesting. However, developing accurate detection and segmentation models requires large, precisely annotated datasets that are costly to produce. Synthetic data provides a scalable alternative, yet often lacks sufficient realism to generalize to real-world scenarios. This paper presents a novel workflow that integrates 3D rendering in Blender with a constrained diffusion model to automatically generate high-quality annotated, photorealistic synthetic images of Agaricus Bisporus mushrooms. This approach preserves full control over 3D scene configuration and annotations while achieving photorealism without the need for specialized computer graphics expertise. We release two synthetic datasets (each containing 6,000 images depicting over 250k mushroom instances) and evaluate Mask R-CNN models trained on them in a zero-shot setting. When tested on two independent real-world datasets (including a newly collected benchmark), our method achieves state-of-the-art segmentation performance (F1 = 0.859 on M18K), despite using only synthetic training data. Although the approach is demonstrated on Agaricus Bisporus mushrooms, the proposed pipeline can be readily adapted to other mushroom species or to other agricultural domains, such as fruit and leaf detection.", "link": "http://arxiv.org/abs/2512.08747v1", "date": "2025-12-09", "relevancy": 2.3914, "topK": [{"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6103}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5977}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5854}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Scalable%20Pipeline%20Combining%20Procedural%203D%20Graphics%20and%20Guided%20Diffusion%20for%20Photorealistic%20Synthetic%20Training%20Data%20Generation%20in%20White%20Button%20Mushroom%20Segmentation&body=Title%3A%20A%20Scalable%20Pipeline%20Combining%20Procedural%203D%20Graphics%20and%20Guided%20Diffusion%20for%20Photorealistic%20Synthetic%20Training%20Data%20Generation%20in%20White%20Button%20Mushroom%20Segmentation%0AAuthor%3A%20Art%C3%BAr%20I.%20K%C3%A1roly%20and%20P%C3%A9ter%20Galambos%0AAbstract%3A%20Industrial%20mushroom%20cultivation%20increasingly%20relies%20on%20computer%20vision%20for%20monitoring%20and%20automated%20harvesting.%20However%2C%20developing%20accurate%20detection%20and%20segmentation%20models%20requires%20large%2C%20precisely%20annotated%20datasets%20that%20are%20costly%20to%20produce.%20Synthetic%20data%20provides%20a%20scalable%20alternative%2C%20yet%20often%20lacks%20sufficient%20realism%20to%20generalize%20to%20real-world%20scenarios.%20This%20paper%20presents%20a%20novel%20workflow%20that%20integrates%203D%20rendering%20in%20Blender%20with%20a%20constrained%20diffusion%20model%20to%20automatically%20generate%20high-quality%20annotated%2C%20photorealistic%20synthetic%20images%20of%20Agaricus%20Bisporus%20mushrooms.%20This%20approach%20preserves%20full%20control%20over%203D%20scene%20configuration%20and%20annotations%20while%20achieving%20photorealism%20without%20the%20need%20for%20specialized%20computer%20graphics%20expertise.%20We%20release%20two%20synthetic%20datasets%20%28each%20containing%206%2C000%20images%20depicting%20over%20250k%20mushroom%20instances%29%20and%20evaluate%20Mask%20R-CNN%20models%20trained%20on%20them%20in%20a%20zero-shot%20setting.%20When%20tested%20on%20two%20independent%20real-world%20datasets%20%28including%20a%20newly%20collected%20benchmark%29%2C%20our%20method%20achieves%20state-of-the-art%20segmentation%20performance%20%28F1%20%3D%200.859%20on%20M18K%29%2C%20despite%20using%20only%20synthetic%20training%20data.%20Although%20the%20approach%20is%20demonstrated%20on%20Agaricus%20Bisporus%20mushrooms%2C%20the%20proposed%20pipeline%20can%20be%20readily%20adapted%20to%20other%20mushroom%20species%20or%20to%20other%20agricultural%20domains%2C%20such%20as%20fruit%20and%20leaf%20detection.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08747v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Scalable%2520Pipeline%2520Combining%2520Procedural%25203D%2520Graphics%2520and%2520Guided%2520Diffusion%2520for%2520Photorealistic%2520Synthetic%2520Training%2520Data%2520Generation%2520in%2520White%2520Button%2520Mushroom%2520Segmentation%26entry.906535625%3DArt%25C3%25BAr%2520I.%2520K%25C3%25A1roly%2520and%2520P%25C3%25A9ter%2520Galambos%26entry.1292438233%3DIndustrial%2520mushroom%2520cultivation%2520increasingly%2520relies%2520on%2520computer%2520vision%2520for%2520monitoring%2520and%2520automated%2520harvesting.%2520However%252C%2520developing%2520accurate%2520detection%2520and%2520segmentation%2520models%2520requires%2520large%252C%2520precisely%2520annotated%2520datasets%2520that%2520are%2520costly%2520to%2520produce.%2520Synthetic%2520data%2520provides%2520a%2520scalable%2520alternative%252C%2520yet%2520often%2520lacks%2520sufficient%2520realism%2520to%2520generalize%2520to%2520real-world%2520scenarios.%2520This%2520paper%2520presents%2520a%2520novel%2520workflow%2520that%2520integrates%25203D%2520rendering%2520in%2520Blender%2520with%2520a%2520constrained%2520diffusion%2520model%2520to%2520automatically%2520generate%2520high-quality%2520annotated%252C%2520photorealistic%2520synthetic%2520images%2520of%2520Agaricus%2520Bisporus%2520mushrooms.%2520This%2520approach%2520preserves%2520full%2520control%2520over%25203D%2520scene%2520configuration%2520and%2520annotations%2520while%2520achieving%2520photorealism%2520without%2520the%2520need%2520for%2520specialized%2520computer%2520graphics%2520expertise.%2520We%2520release%2520two%2520synthetic%2520datasets%2520%2528each%2520containing%25206%252C000%2520images%2520depicting%2520over%2520250k%2520mushroom%2520instances%2529%2520and%2520evaluate%2520Mask%2520R-CNN%2520models%2520trained%2520on%2520them%2520in%2520a%2520zero-shot%2520setting.%2520When%2520tested%2520on%2520two%2520independent%2520real-world%2520datasets%2520%2528including%2520a%2520newly%2520collected%2520benchmark%2529%252C%2520our%2520method%2520achieves%2520state-of-the-art%2520segmentation%2520performance%2520%2528F1%2520%253D%25200.859%2520on%2520M18K%2529%252C%2520despite%2520using%2520only%2520synthetic%2520training%2520data.%2520Although%2520the%2520approach%2520is%2520demonstrated%2520on%2520Agaricus%2520Bisporus%2520mushrooms%252C%2520the%2520proposed%2520pipeline%2520can%2520be%2520readily%2520adapted%2520to%2520other%2520mushroom%2520species%2520or%2520to%2520other%2520agricultural%2520domains%252C%2520such%2520as%2520fruit%2520and%2520leaf%2520detection.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08747v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Scalable%20Pipeline%20Combining%20Procedural%203D%20Graphics%20and%20Guided%20Diffusion%20for%20Photorealistic%20Synthetic%20Training%20Data%20Generation%20in%20White%20Button%20Mushroom%20Segmentation&entry.906535625=Art%C3%BAr%20I.%20K%C3%A1roly%20and%20P%C3%A9ter%20Galambos&entry.1292438233=Industrial%20mushroom%20cultivation%20increasingly%20relies%20on%20computer%20vision%20for%20monitoring%20and%20automated%20harvesting.%20However%2C%20developing%20accurate%20detection%20and%20segmentation%20models%20requires%20large%2C%20precisely%20annotated%20datasets%20that%20are%20costly%20to%20produce.%20Synthetic%20data%20provides%20a%20scalable%20alternative%2C%20yet%20often%20lacks%20sufficient%20realism%20to%20generalize%20to%20real-world%20scenarios.%20This%20paper%20presents%20a%20novel%20workflow%20that%20integrates%203D%20rendering%20in%20Blender%20with%20a%20constrained%20diffusion%20model%20to%20automatically%20generate%20high-quality%20annotated%2C%20photorealistic%20synthetic%20images%20of%20Agaricus%20Bisporus%20mushrooms.%20This%20approach%20preserves%20full%20control%20over%203D%20scene%20configuration%20and%20annotations%20while%20achieving%20photorealism%20without%20the%20need%20for%20specialized%20computer%20graphics%20expertise.%20We%20release%20two%20synthetic%20datasets%20%28each%20containing%206%2C000%20images%20depicting%20over%20250k%20mushroom%20instances%29%20and%20evaluate%20Mask%20R-CNN%20models%20trained%20on%20them%20in%20a%20zero-shot%20setting.%20When%20tested%20on%20two%20independent%20real-world%20datasets%20%28including%20a%20newly%20collected%20benchmark%29%2C%20our%20method%20achieves%20state-of-the-art%20segmentation%20performance%20%28F1%20%3D%200.859%20on%20M18K%29%2C%20despite%20using%20only%20synthetic%20training%20data.%20Although%20the%20approach%20is%20demonstrated%20on%20Agaricus%20Bisporus%20mushrooms%2C%20the%20proposed%20pipeline%20can%20be%20readily%20adapted%20to%20other%20mushroom%20species%20or%20to%20other%20agricultural%20domains%2C%20such%20as%20fruit%20and%20leaf%20detection.&entry.1838667208=http%3A//arxiv.org/abs/2512.08747v1&entry.124074799=Read"},
{"title": "Learning and Editing Universal Graph Prompt Tuning via Reinforcement Learning", "author": "Jinfeng Xu and Zheyu Chen and Shuo Yang and Jinze Li and Hewei Wang and Yijie Li and Edith C. H. Ngai", "abstract": "Early graph prompt tuning approaches relied on task-specific designs for Graph Neural Networks (GNNs), limiting their adaptability across diverse pre-training strategies. In contrast, another promising line of research has investigated universal graph prompt tuning, which operates directly in the input graph's feature space and builds a theoretical foundation that universal graph prompt tuning can theoretically achieve an equivalent effect of any prompting function, eliminating dependence on specific pre-training strategies. Recent works propose selective node-based graph prompt tuning to pursue more ideal prompts. However, we argue that selective node-based graph prompt tuning inevitably compromises the theoretical foundation of universal graph prompt tuning. In this paper, we strengthen the theoretical foundation of universal graph prompt tuning by introducing stricter constraints, demonstrating that adding prompts to all nodes is a necessary condition for achieving the universality of graph prompts. To this end, we propose a novel model and paradigm, Learning and Editing Universal GrAph Prompt Tuning (LEAP), which preserves the theoretical foundation of universal graph prompt tuning while pursuing more ideal prompts. Specifically, we first build the basic universal graph prompts to preserve the theoretical foundation and then employ actor-critic reinforcement learning to select nodes and edit prompts. Extensive experiments on graph- and node-level tasks across various pre-training strategies in both full-shot and few-shot scenarios show that LEAP consistently outperforms fine-tuning and other prompt-based approaches.", "link": "http://arxiv.org/abs/2512.08763v1", "date": "2025-12-09", "relevancy": 2.3828, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4824}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4801}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20and%20Editing%20Universal%20Graph%20Prompt%20Tuning%20via%20Reinforcement%20Learning&body=Title%3A%20Learning%20and%20Editing%20Universal%20Graph%20Prompt%20Tuning%20via%20Reinforcement%20Learning%0AAuthor%3A%20Jinfeng%20Xu%20and%20Zheyu%20Chen%20and%20Shuo%20Yang%20and%20Jinze%20Li%20and%20Hewei%20Wang%20and%20Yijie%20Li%20and%20Edith%20C.%20H.%20Ngai%0AAbstract%3A%20Early%20graph%20prompt%20tuning%20approaches%20relied%20on%20task-specific%20designs%20for%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20limiting%20their%20adaptability%20across%20diverse%20pre-training%20strategies.%20In%20contrast%2C%20another%20promising%20line%20of%20research%20has%20investigated%20universal%20graph%20prompt%20tuning%2C%20which%20operates%20directly%20in%20the%20input%20graph%27s%20feature%20space%20and%20builds%20a%20theoretical%20foundation%20that%20universal%20graph%20prompt%20tuning%20can%20theoretically%20achieve%20an%20equivalent%20effect%20of%20any%20prompting%20function%2C%20eliminating%20dependence%20on%20specific%20pre-training%20strategies.%20Recent%20works%20propose%20selective%20node-based%20graph%20prompt%20tuning%20to%20pursue%20more%20ideal%20prompts.%20However%2C%20we%20argue%20that%20selective%20node-based%20graph%20prompt%20tuning%20inevitably%20compromises%20the%20theoretical%20foundation%20of%20universal%20graph%20prompt%20tuning.%20In%20this%20paper%2C%20we%20strengthen%20the%20theoretical%20foundation%20of%20universal%20graph%20prompt%20tuning%20by%20introducing%20stricter%20constraints%2C%20demonstrating%20that%20adding%20prompts%20to%20all%20nodes%20is%20a%20necessary%20condition%20for%20achieving%20the%20universality%20of%20graph%20prompts.%20To%20this%20end%2C%20we%20propose%20a%20novel%20model%20and%20paradigm%2C%20Learning%20and%20Editing%20Universal%20GrAph%20Prompt%20Tuning%20%28LEAP%29%2C%20which%20preserves%20the%20theoretical%20foundation%20of%20universal%20graph%20prompt%20tuning%20while%20pursuing%20more%20ideal%20prompts.%20Specifically%2C%20we%20first%20build%20the%20basic%20universal%20graph%20prompts%20to%20preserve%20the%20theoretical%20foundation%20and%20then%20employ%20actor-critic%20reinforcement%20learning%20to%20select%20nodes%20and%20edit%20prompts.%20Extensive%20experiments%20on%20graph-%20and%20node-level%20tasks%20across%20various%20pre-training%20strategies%20in%20both%20full-shot%20and%20few-shot%20scenarios%20show%20that%20LEAP%20consistently%20outperforms%20fine-tuning%20and%20other%20prompt-based%20approaches.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08763v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520and%2520Editing%2520Universal%2520Graph%2520Prompt%2520Tuning%2520via%2520Reinforcement%2520Learning%26entry.906535625%3DJinfeng%2520Xu%2520and%2520Zheyu%2520Chen%2520and%2520Shuo%2520Yang%2520and%2520Jinze%2520Li%2520and%2520Hewei%2520Wang%2520and%2520Yijie%2520Li%2520and%2520Edith%2520C.%2520H.%2520Ngai%26entry.1292438233%3DEarly%2520graph%2520prompt%2520tuning%2520approaches%2520relied%2520on%2520task-specific%2520designs%2520for%2520Graph%2520Neural%2520Networks%2520%2528GNNs%2529%252C%2520limiting%2520their%2520adaptability%2520across%2520diverse%2520pre-training%2520strategies.%2520In%2520contrast%252C%2520another%2520promising%2520line%2520of%2520research%2520has%2520investigated%2520universal%2520graph%2520prompt%2520tuning%252C%2520which%2520operates%2520directly%2520in%2520the%2520input%2520graph%2527s%2520feature%2520space%2520and%2520builds%2520a%2520theoretical%2520foundation%2520that%2520universal%2520graph%2520prompt%2520tuning%2520can%2520theoretically%2520achieve%2520an%2520equivalent%2520effect%2520of%2520any%2520prompting%2520function%252C%2520eliminating%2520dependence%2520on%2520specific%2520pre-training%2520strategies.%2520Recent%2520works%2520propose%2520selective%2520node-based%2520graph%2520prompt%2520tuning%2520to%2520pursue%2520more%2520ideal%2520prompts.%2520However%252C%2520we%2520argue%2520that%2520selective%2520node-based%2520graph%2520prompt%2520tuning%2520inevitably%2520compromises%2520the%2520theoretical%2520foundation%2520of%2520universal%2520graph%2520prompt%2520tuning.%2520In%2520this%2520paper%252C%2520we%2520strengthen%2520the%2520theoretical%2520foundation%2520of%2520universal%2520graph%2520prompt%2520tuning%2520by%2520introducing%2520stricter%2520constraints%252C%2520demonstrating%2520that%2520adding%2520prompts%2520to%2520all%2520nodes%2520is%2520a%2520necessary%2520condition%2520for%2520achieving%2520the%2520universality%2520of%2520graph%2520prompts.%2520To%2520this%2520end%252C%2520we%2520propose%2520a%2520novel%2520model%2520and%2520paradigm%252C%2520Learning%2520and%2520Editing%2520Universal%2520GrAph%2520Prompt%2520Tuning%2520%2528LEAP%2529%252C%2520which%2520preserves%2520the%2520theoretical%2520foundation%2520of%2520universal%2520graph%2520prompt%2520tuning%2520while%2520pursuing%2520more%2520ideal%2520prompts.%2520Specifically%252C%2520we%2520first%2520build%2520the%2520basic%2520universal%2520graph%2520prompts%2520to%2520preserve%2520the%2520theoretical%2520foundation%2520and%2520then%2520employ%2520actor-critic%2520reinforcement%2520learning%2520to%2520select%2520nodes%2520and%2520edit%2520prompts.%2520Extensive%2520experiments%2520on%2520graph-%2520and%2520node-level%2520tasks%2520across%2520various%2520pre-training%2520strategies%2520in%2520both%2520full-shot%2520and%2520few-shot%2520scenarios%2520show%2520that%2520LEAP%2520consistently%2520outperforms%2520fine-tuning%2520and%2520other%2520prompt-based%2520approaches.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08763v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20and%20Editing%20Universal%20Graph%20Prompt%20Tuning%20via%20Reinforcement%20Learning&entry.906535625=Jinfeng%20Xu%20and%20Zheyu%20Chen%20and%20Shuo%20Yang%20and%20Jinze%20Li%20and%20Hewei%20Wang%20and%20Yijie%20Li%20and%20Edith%20C.%20H.%20Ngai&entry.1292438233=Early%20graph%20prompt%20tuning%20approaches%20relied%20on%20task-specific%20designs%20for%20Graph%20Neural%20Networks%20%28GNNs%29%2C%20limiting%20their%20adaptability%20across%20diverse%20pre-training%20strategies.%20In%20contrast%2C%20another%20promising%20line%20of%20research%20has%20investigated%20universal%20graph%20prompt%20tuning%2C%20which%20operates%20directly%20in%20the%20input%20graph%27s%20feature%20space%20and%20builds%20a%20theoretical%20foundation%20that%20universal%20graph%20prompt%20tuning%20can%20theoretically%20achieve%20an%20equivalent%20effect%20of%20any%20prompting%20function%2C%20eliminating%20dependence%20on%20specific%20pre-training%20strategies.%20Recent%20works%20propose%20selective%20node-based%20graph%20prompt%20tuning%20to%20pursue%20more%20ideal%20prompts.%20However%2C%20we%20argue%20that%20selective%20node-based%20graph%20prompt%20tuning%20inevitably%20compromises%20the%20theoretical%20foundation%20of%20universal%20graph%20prompt%20tuning.%20In%20this%20paper%2C%20we%20strengthen%20the%20theoretical%20foundation%20of%20universal%20graph%20prompt%20tuning%20by%20introducing%20stricter%20constraints%2C%20demonstrating%20that%20adding%20prompts%20to%20all%20nodes%20is%20a%20necessary%20condition%20for%20achieving%20the%20universality%20of%20graph%20prompts.%20To%20this%20end%2C%20we%20propose%20a%20novel%20model%20and%20paradigm%2C%20Learning%20and%20Editing%20Universal%20GrAph%20Prompt%20Tuning%20%28LEAP%29%2C%20which%20preserves%20the%20theoretical%20foundation%20of%20universal%20graph%20prompt%20tuning%20while%20pursuing%20more%20ideal%20prompts.%20Specifically%2C%20we%20first%20build%20the%20basic%20universal%20graph%20prompts%20to%20preserve%20the%20theoretical%20foundation%20and%20then%20employ%20actor-critic%20reinforcement%20learning%20to%20select%20nodes%20and%20edit%20prompts.%20Extensive%20experiments%20on%20graph-%20and%20node-level%20tasks%20across%20various%20pre-training%20strategies%20in%20both%20full-shot%20and%20few-shot%20scenarios%20show%20that%20LEAP%20consistently%20outperforms%20fine-tuning%20and%20other%20prompt-based%20approaches.&entry.1838667208=http%3A//arxiv.org/abs/2512.08763v1&entry.124074799=Read"},
{"title": "B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability", "author": "Yifan Wang and Sukrut Rao and Ji-Ung Lee and Mayank Jobanputra and Vera Demberg", "abstract": "Post-hoc explanation methods for black-box models often struggle with faithfulness and human interpretability due to the lack of explainability in current neural architectures. Meanwhile, B-cos networks have been introduced to improve model explainability by proposing an architecture that removes bias terms and promotes input-weight alignment. Although B-cos networks have shown success in building explainable systems, their application has so far been limited to computer vision models and their associated training pipelines. In this work, we introduce B-cos LMs, i.e., B-cos Language Models (LMs) empowered for natural language processing (NLP) tasks. Our approach directly transforms pre-trained language models into B-cos LMs by combining B-cos conversion and task fine-tuning, improving efficiency compared to previous methods. Automatic and human evaluation results demonstrate that B-cos LMs produce more faithful and human interpretable explanations than post-hoc methods, while maintaining task performance comparable to conventional fine-tuning. Our in-depth analysis explores how B-cos LMs differ from conventionally fine-tuned models in their learning processes and explanation patterns. Finally, we present a first exploration of transforming decoder-only models to B-cos LMs for generation tasks. Our code is available at https://github.com/Ewanwong/bcos_lm.", "link": "http://arxiv.org/abs/2502.12992v4", "date": "2025-12-09", "relevancy": 2.3818, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4852}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4587}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20B-cos%20LM%3A%20Efficiently%20Transforming%20Pre-trained%20Language%20Models%20for%20Improved%20Explainability&body=Title%3A%20B-cos%20LM%3A%20Efficiently%20Transforming%20Pre-trained%20Language%20Models%20for%20Improved%20Explainability%0AAuthor%3A%20Yifan%20Wang%20and%20Sukrut%20Rao%20and%20Ji-Ung%20Lee%20and%20Mayank%20Jobanputra%20and%20Vera%20Demberg%0AAbstract%3A%20Post-hoc%20explanation%20methods%20for%20black-box%20models%20often%20struggle%20with%20faithfulness%20and%20human%20interpretability%20due%20to%20the%20lack%20of%20explainability%20in%20current%20neural%20architectures.%20Meanwhile%2C%20B-cos%20networks%20have%20been%20introduced%20to%20improve%20model%20explainability%20by%20proposing%20an%20architecture%20that%20removes%20bias%20terms%20and%20promotes%20input-weight%20alignment.%20Although%20B-cos%20networks%20have%20shown%20success%20in%20building%20explainable%20systems%2C%20their%20application%20has%20so%20far%20been%20limited%20to%20computer%20vision%20models%20and%20their%20associated%20training%20pipelines.%20In%20this%20work%2C%20we%20introduce%20B-cos%20LMs%2C%20i.e.%2C%20B-cos%20Language%20Models%20%28LMs%29%20empowered%20for%20natural%20language%20processing%20%28NLP%29%20tasks.%20Our%20approach%20directly%20transforms%20pre-trained%20language%20models%20into%20B-cos%20LMs%20by%20combining%20B-cos%20conversion%20and%20task%20fine-tuning%2C%20improving%20efficiency%20compared%20to%20previous%20methods.%20Automatic%20and%20human%20evaluation%20results%20demonstrate%20that%20B-cos%20LMs%20produce%20more%20faithful%20and%20human%20interpretable%20explanations%20than%20post-hoc%20methods%2C%20while%20maintaining%20task%20performance%20comparable%20to%20conventional%20fine-tuning.%20Our%20in-depth%20analysis%20explores%20how%20B-cos%20LMs%20differ%20from%20conventionally%20fine-tuned%20models%20in%20their%20learning%20processes%20and%20explanation%20patterns.%20Finally%2C%20we%20present%20a%20first%20exploration%20of%20transforming%20decoder-only%20models%20to%20B-cos%20LMs%20for%20generation%20tasks.%20Our%20code%20is%20available%20at%20https%3A//github.com/Ewanwong/bcos_lm.%0ALink%3A%20http%3A//arxiv.org/abs/2502.12992v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DB-cos%2520LM%253A%2520Efficiently%2520Transforming%2520Pre-trained%2520Language%2520Models%2520for%2520Improved%2520Explainability%26entry.906535625%3DYifan%2520Wang%2520and%2520Sukrut%2520Rao%2520and%2520Ji-Ung%2520Lee%2520and%2520Mayank%2520Jobanputra%2520and%2520Vera%2520Demberg%26entry.1292438233%3DPost-hoc%2520explanation%2520methods%2520for%2520black-box%2520models%2520often%2520struggle%2520with%2520faithfulness%2520and%2520human%2520interpretability%2520due%2520to%2520the%2520lack%2520of%2520explainability%2520in%2520current%2520neural%2520architectures.%2520Meanwhile%252C%2520B-cos%2520networks%2520have%2520been%2520introduced%2520to%2520improve%2520model%2520explainability%2520by%2520proposing%2520an%2520architecture%2520that%2520removes%2520bias%2520terms%2520and%2520promotes%2520input-weight%2520alignment.%2520Although%2520B-cos%2520networks%2520have%2520shown%2520success%2520in%2520building%2520explainable%2520systems%252C%2520their%2520application%2520has%2520so%2520far%2520been%2520limited%2520to%2520computer%2520vision%2520models%2520and%2520their%2520associated%2520training%2520pipelines.%2520In%2520this%2520work%252C%2520we%2520introduce%2520B-cos%2520LMs%252C%2520i.e.%252C%2520B-cos%2520Language%2520Models%2520%2528LMs%2529%2520empowered%2520for%2520natural%2520language%2520processing%2520%2528NLP%2529%2520tasks.%2520Our%2520approach%2520directly%2520transforms%2520pre-trained%2520language%2520models%2520into%2520B-cos%2520LMs%2520by%2520combining%2520B-cos%2520conversion%2520and%2520task%2520fine-tuning%252C%2520improving%2520efficiency%2520compared%2520to%2520previous%2520methods.%2520Automatic%2520and%2520human%2520evaluation%2520results%2520demonstrate%2520that%2520B-cos%2520LMs%2520produce%2520more%2520faithful%2520and%2520human%2520interpretable%2520explanations%2520than%2520post-hoc%2520methods%252C%2520while%2520maintaining%2520task%2520performance%2520comparable%2520to%2520conventional%2520fine-tuning.%2520Our%2520in-depth%2520analysis%2520explores%2520how%2520B-cos%2520LMs%2520differ%2520from%2520conventionally%2520fine-tuned%2520models%2520in%2520their%2520learning%2520processes%2520and%2520explanation%2520patterns.%2520Finally%252C%2520we%2520present%2520a%2520first%2520exploration%2520of%2520transforming%2520decoder-only%2520models%2520to%2520B-cos%2520LMs%2520for%2520generation%2520tasks.%2520Our%2520code%2520is%2520available%2520at%2520https%253A//github.com/Ewanwong/bcos_lm.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2502.12992v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=B-cos%20LM%3A%20Efficiently%20Transforming%20Pre-trained%20Language%20Models%20for%20Improved%20Explainability&entry.906535625=Yifan%20Wang%20and%20Sukrut%20Rao%20and%20Ji-Ung%20Lee%20and%20Mayank%20Jobanputra%20and%20Vera%20Demberg&entry.1292438233=Post-hoc%20explanation%20methods%20for%20black-box%20models%20often%20struggle%20with%20faithfulness%20and%20human%20interpretability%20due%20to%20the%20lack%20of%20explainability%20in%20current%20neural%20architectures.%20Meanwhile%2C%20B-cos%20networks%20have%20been%20introduced%20to%20improve%20model%20explainability%20by%20proposing%20an%20architecture%20that%20removes%20bias%20terms%20and%20promotes%20input-weight%20alignment.%20Although%20B-cos%20networks%20have%20shown%20success%20in%20building%20explainable%20systems%2C%20their%20application%20has%20so%20far%20been%20limited%20to%20computer%20vision%20models%20and%20their%20associated%20training%20pipelines.%20In%20this%20work%2C%20we%20introduce%20B-cos%20LMs%2C%20i.e.%2C%20B-cos%20Language%20Models%20%28LMs%29%20empowered%20for%20natural%20language%20processing%20%28NLP%29%20tasks.%20Our%20approach%20directly%20transforms%20pre-trained%20language%20models%20into%20B-cos%20LMs%20by%20combining%20B-cos%20conversion%20and%20task%20fine-tuning%2C%20improving%20efficiency%20compared%20to%20previous%20methods.%20Automatic%20and%20human%20evaluation%20results%20demonstrate%20that%20B-cos%20LMs%20produce%20more%20faithful%20and%20human%20interpretable%20explanations%20than%20post-hoc%20methods%2C%20while%20maintaining%20task%20performance%20comparable%20to%20conventional%20fine-tuning.%20Our%20in-depth%20analysis%20explores%20how%20B-cos%20LMs%20differ%20from%20conventionally%20fine-tuned%20models%20in%20their%20learning%20processes%20and%20explanation%20patterns.%20Finally%2C%20we%20present%20a%20first%20exploration%20of%20transforming%20decoder-only%20models%20to%20B-cos%20LMs%20for%20generation%20tasks.%20Our%20code%20is%20available%20at%20https%3A//github.com/Ewanwong/bcos_lm.&entry.1838667208=http%3A//arxiv.org/abs/2502.12992v4&entry.124074799=Read"},
{"title": "No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers", "author": "Damiano Marsili and Georgia Gkioxari", "abstract": "Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/", "link": "http://arxiv.org/abs/2512.08889v1", "date": "2025-12-09", "relevancy": 2.3817, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6012}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.6012}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5666}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No%20Labels%2C%20No%20Problem%3A%20Training%20Visual%20Reasoners%20with%20Multimodal%20Verifiers&body=Title%3A%20No%20Labels%2C%20No%20Problem%3A%20Training%20Visual%20Reasoners%20with%20Multimodal%20Verifiers%0AAuthor%3A%20Damiano%20Marsili%20and%20Georgia%20Gkioxari%0AAbstract%3A%20Visual%20reasoning%20is%20challenging%2C%20requiring%20both%20precise%20object%20grounding%20and%20understanding%20complex%20spatial%20relationships.%20Existing%20methods%20fall%20into%20two%20camps%3A%20language-only%20chain-of-thought%20approaches%2C%20which%20demand%20large-scale%20%28image%2C%20query%2C%20answer%29%20supervision%2C%20and%20program-synthesis%20approaches%20which%20use%20pre-trained%20models%20and%20avoid%20training%2C%20but%20suffer%20from%20flawed%20logic%20and%20erroneous%20grounding.%20We%20propose%20an%20annotation-free%20training%20framework%20that%20improves%20both%20reasoning%20and%20grounding.%20Our%20framework%20uses%20AI-powered%20verifiers%3A%20an%20LLM%20verifier%20refines%20LLM%20reasoning%20via%20reinforcement%20learning%2C%20while%20a%20VLM%20verifier%20strengthens%20visual%20grounding%20through%20automated%20hard-negative%20mining%2C%20eliminating%20the%20need%20for%20ground%20truth%20labels.%20This%20design%20combines%20the%20strengths%20of%20modern%20AI%20systems%3A%20advanced%20language-only%20reasoning%20models%20for%20decomposing%20spatial%20queries%20into%20simpler%20subtasks%2C%20and%20strong%20vision%20specialist%20models%20improved%20via%20performant%20VLM%20critics.%20We%20evaluate%20our%20approach%20across%20diverse%20spatial%20reasoning%20tasks%2C%20and%20show%20that%20our%20method%20improves%20visual%20reasoning%20and%20surpasses%20open-source%20and%20proprietary%20models%2C%20while%20with%20our%20improved%20visual%20grounding%20model%20we%20further%20outperform%20recent%20text-only%20visual%20reasoning%20methods.%20Project%20webpage%3A%20https%3A//glab-caltech.github.io/valor/%0ALink%3A%20http%3A//arxiv.org/abs/2512.08889v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo%2520Labels%252C%2520No%2520Problem%253A%2520Training%2520Visual%2520Reasoners%2520with%2520Multimodal%2520Verifiers%26entry.906535625%3DDamiano%2520Marsili%2520and%2520Georgia%2520Gkioxari%26entry.1292438233%3DVisual%2520reasoning%2520is%2520challenging%252C%2520requiring%2520both%2520precise%2520object%2520grounding%2520and%2520understanding%2520complex%2520spatial%2520relationships.%2520Existing%2520methods%2520fall%2520into%2520two%2520camps%253A%2520language-only%2520chain-of-thought%2520approaches%252C%2520which%2520demand%2520large-scale%2520%2528image%252C%2520query%252C%2520answer%2529%2520supervision%252C%2520and%2520program-synthesis%2520approaches%2520which%2520use%2520pre-trained%2520models%2520and%2520avoid%2520training%252C%2520but%2520suffer%2520from%2520flawed%2520logic%2520and%2520erroneous%2520grounding.%2520We%2520propose%2520an%2520annotation-free%2520training%2520framework%2520that%2520improves%2520both%2520reasoning%2520and%2520grounding.%2520Our%2520framework%2520uses%2520AI-powered%2520verifiers%253A%2520an%2520LLM%2520verifier%2520refines%2520LLM%2520reasoning%2520via%2520reinforcement%2520learning%252C%2520while%2520a%2520VLM%2520verifier%2520strengthens%2520visual%2520grounding%2520through%2520automated%2520hard-negative%2520mining%252C%2520eliminating%2520the%2520need%2520for%2520ground%2520truth%2520labels.%2520This%2520design%2520combines%2520the%2520strengths%2520of%2520modern%2520AI%2520systems%253A%2520advanced%2520language-only%2520reasoning%2520models%2520for%2520decomposing%2520spatial%2520queries%2520into%2520simpler%2520subtasks%252C%2520and%2520strong%2520vision%2520specialist%2520models%2520improved%2520via%2520performant%2520VLM%2520critics.%2520We%2520evaluate%2520our%2520approach%2520across%2520diverse%2520spatial%2520reasoning%2520tasks%252C%2520and%2520show%2520that%2520our%2520method%2520improves%2520visual%2520reasoning%2520and%2520surpasses%2520open-source%2520and%2520proprietary%2520models%252C%2520while%2520with%2520our%2520improved%2520visual%2520grounding%2520model%2520we%2520further%2520outperform%2520recent%2520text-only%2520visual%2520reasoning%2520methods.%2520Project%2520webpage%253A%2520https%253A//glab-caltech.github.io/valor/%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08889v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No%20Labels%2C%20No%20Problem%3A%20Training%20Visual%20Reasoners%20with%20Multimodal%20Verifiers&entry.906535625=Damiano%20Marsili%20and%20Georgia%20Gkioxari&entry.1292438233=Visual%20reasoning%20is%20challenging%2C%20requiring%20both%20precise%20object%20grounding%20and%20understanding%20complex%20spatial%20relationships.%20Existing%20methods%20fall%20into%20two%20camps%3A%20language-only%20chain-of-thought%20approaches%2C%20which%20demand%20large-scale%20%28image%2C%20query%2C%20answer%29%20supervision%2C%20and%20program-synthesis%20approaches%20which%20use%20pre-trained%20models%20and%20avoid%20training%2C%20but%20suffer%20from%20flawed%20logic%20and%20erroneous%20grounding.%20We%20propose%20an%20annotation-free%20training%20framework%20that%20improves%20both%20reasoning%20and%20grounding.%20Our%20framework%20uses%20AI-powered%20verifiers%3A%20an%20LLM%20verifier%20refines%20LLM%20reasoning%20via%20reinforcement%20learning%2C%20while%20a%20VLM%20verifier%20strengthens%20visual%20grounding%20through%20automated%20hard-negative%20mining%2C%20eliminating%20the%20need%20for%20ground%20truth%20labels.%20This%20design%20combines%20the%20strengths%20of%20modern%20AI%20systems%3A%20advanced%20language-only%20reasoning%20models%20for%20decomposing%20spatial%20queries%20into%20simpler%20subtasks%2C%20and%20strong%20vision%20specialist%20models%20improved%20via%20performant%20VLM%20critics.%20We%20evaluate%20our%20approach%20across%20diverse%20spatial%20reasoning%20tasks%2C%20and%20show%20that%20our%20method%20improves%20visual%20reasoning%20and%20surpasses%20open-source%20and%20proprietary%20models%2C%20while%20with%20our%20improved%20visual%20grounding%20model%20we%20further%20outperform%20recent%20text-only%20visual%20reasoning%20methods.%20Project%20webpage%3A%20https%3A//glab-caltech.github.io/valor/&entry.1838667208=http%3A//arxiv.org/abs/2512.08889v1&entry.124074799=Read"},
{"title": "CAPE: A CLIP-Aware Pointing Ensemble of Complementary Heatmap Cues for Embodied Reference Understanding", "author": "Fevziye Irem Eyiokur and Dogucan Yaman and Haz\u0131m Kemal Ekenel and Alexander Waibel", "abstract": "We address Embodied Reference Understanding, the task of predicting the object a person in the scene refers to through pointing gesture and language. This requires multimodal reasoning over text, visual pointing cues, and scene context, yet existing methods often fail to fully exploit visual disambiguation signals. We also observe that while the referent often aligns with the head-to-fingertip direction, in many cases it aligns more closely with the wrist-to-fingertip direction, making a single-line assumption overly limiting. To address this, we propose a dual-model framework, where one model learns from the head-to-fingertip direction and the other from the wrist-to-fingertip direction. We introduce a Gaussian ray heatmap representation of these lines and use them as input to provide a strong supervisory signal that encourages the model to better attend to pointing cues. To fuse their complementary strengths, we present the CLIP-Aware Pointing Ensemble module, which performs a hybrid ensemble guided by CLIP features. We further incorporate an auxiliary object center prediction head to enhance referent localization. We validate our approach on YouRefIt, achieving 75.0 mAP at 0.25 IoU, alongside state-of-the-art CLIP and C_D scores, and demonstrate its generality on unseen CAESAR and ISL Pointing, showing robust performance across benchmarks.", "link": "http://arxiv.org/abs/2507.21888v3", "date": "2025-12-09", "relevancy": 2.3718, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6143}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5797}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5728}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CAPE%3A%20A%20CLIP-Aware%20Pointing%20Ensemble%20of%20Complementary%20Heatmap%20Cues%20for%20Embodied%20Reference%20Understanding&body=Title%3A%20CAPE%3A%20A%20CLIP-Aware%20Pointing%20Ensemble%20of%20Complementary%20Heatmap%20Cues%20for%20Embodied%20Reference%20Understanding%0AAuthor%3A%20Fevziye%20Irem%20Eyiokur%20and%20Dogucan%20Yaman%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Alexander%20Waibel%0AAbstract%3A%20We%20address%20Embodied%20Reference%20Understanding%2C%20the%20task%20of%20predicting%20the%20object%20a%20person%20in%20the%20scene%20refers%20to%20through%20pointing%20gesture%20and%20language.%20This%20requires%20multimodal%20reasoning%20over%20text%2C%20visual%20pointing%20cues%2C%20and%20scene%20context%2C%20yet%20existing%20methods%20often%20fail%20to%20fully%20exploit%20visual%20disambiguation%20signals.%20We%20also%20observe%20that%20while%20the%20referent%20often%20aligns%20with%20the%20head-to-fingertip%20direction%2C%20in%20many%20cases%20it%20aligns%20more%20closely%20with%20the%20wrist-to-fingertip%20direction%2C%20making%20a%20single-line%20assumption%20overly%20limiting.%20To%20address%20this%2C%20we%20propose%20a%20dual-model%20framework%2C%20where%20one%20model%20learns%20from%20the%20head-to-fingertip%20direction%20and%20the%20other%20from%20the%20wrist-to-fingertip%20direction.%20We%20introduce%20a%20Gaussian%20ray%20heatmap%20representation%20of%20these%20lines%20and%20use%20them%20as%20input%20to%20provide%20a%20strong%20supervisory%20signal%20that%20encourages%20the%20model%20to%20better%20attend%20to%20pointing%20cues.%20To%20fuse%20their%20complementary%20strengths%2C%20we%20present%20the%20CLIP-Aware%20Pointing%20Ensemble%20module%2C%20which%20performs%20a%20hybrid%20ensemble%20guided%20by%20CLIP%20features.%20We%20further%20incorporate%20an%20auxiliary%20object%20center%20prediction%20head%20to%20enhance%20referent%20localization.%20We%20validate%20our%20approach%20on%20YouRefIt%2C%20achieving%2075.0%20mAP%20at%200.25%20IoU%2C%20alongside%20state-of-the-art%20CLIP%20and%20C_D%20scores%2C%20and%20demonstrate%20its%20generality%20on%20unseen%20CAESAR%20and%20ISL%20Pointing%2C%20showing%20robust%20performance%20across%20benchmarks.%0ALink%3A%20http%3A//arxiv.org/abs/2507.21888v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCAPE%253A%2520A%2520CLIP-Aware%2520Pointing%2520Ensemble%2520of%2520Complementary%2520Heatmap%2520Cues%2520for%2520Embodied%2520Reference%2520Understanding%26entry.906535625%3DFevziye%2520Irem%2520Eyiokur%2520and%2520Dogucan%2520Yaman%2520and%2520Haz%25C4%25B1m%2520Kemal%2520Ekenel%2520and%2520Alexander%2520Waibel%26entry.1292438233%3DWe%2520address%2520Embodied%2520Reference%2520Understanding%252C%2520the%2520task%2520of%2520predicting%2520the%2520object%2520a%2520person%2520in%2520the%2520scene%2520refers%2520to%2520through%2520pointing%2520gesture%2520and%2520language.%2520This%2520requires%2520multimodal%2520reasoning%2520over%2520text%252C%2520visual%2520pointing%2520cues%252C%2520and%2520scene%2520context%252C%2520yet%2520existing%2520methods%2520often%2520fail%2520to%2520fully%2520exploit%2520visual%2520disambiguation%2520signals.%2520We%2520also%2520observe%2520that%2520while%2520the%2520referent%2520often%2520aligns%2520with%2520the%2520head-to-fingertip%2520direction%252C%2520in%2520many%2520cases%2520it%2520aligns%2520more%2520closely%2520with%2520the%2520wrist-to-fingertip%2520direction%252C%2520making%2520a%2520single-line%2520assumption%2520overly%2520limiting.%2520To%2520address%2520this%252C%2520we%2520propose%2520a%2520dual-model%2520framework%252C%2520where%2520one%2520model%2520learns%2520from%2520the%2520head-to-fingertip%2520direction%2520and%2520the%2520other%2520from%2520the%2520wrist-to-fingertip%2520direction.%2520We%2520introduce%2520a%2520Gaussian%2520ray%2520heatmap%2520representation%2520of%2520these%2520lines%2520and%2520use%2520them%2520as%2520input%2520to%2520provide%2520a%2520strong%2520supervisory%2520signal%2520that%2520encourages%2520the%2520model%2520to%2520better%2520attend%2520to%2520pointing%2520cues.%2520To%2520fuse%2520their%2520complementary%2520strengths%252C%2520we%2520present%2520the%2520CLIP-Aware%2520Pointing%2520Ensemble%2520module%252C%2520which%2520performs%2520a%2520hybrid%2520ensemble%2520guided%2520by%2520CLIP%2520features.%2520We%2520further%2520incorporate%2520an%2520auxiliary%2520object%2520center%2520prediction%2520head%2520to%2520enhance%2520referent%2520localization.%2520We%2520validate%2520our%2520approach%2520on%2520YouRefIt%252C%2520achieving%252075.0%2520mAP%2520at%25200.25%2520IoU%252C%2520alongside%2520state-of-the-art%2520CLIP%2520and%2520C_D%2520scores%252C%2520and%2520demonstrate%2520its%2520generality%2520on%2520unseen%2520CAESAR%2520and%2520ISL%2520Pointing%252C%2520showing%2520robust%2520performance%2520across%2520benchmarks.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.21888v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CAPE%3A%20A%20CLIP-Aware%20Pointing%20Ensemble%20of%20Complementary%20Heatmap%20Cues%20for%20Embodied%20Reference%20Understanding&entry.906535625=Fevziye%20Irem%20Eyiokur%20and%20Dogucan%20Yaman%20and%20Haz%C4%B1m%20Kemal%20Ekenel%20and%20Alexander%20Waibel&entry.1292438233=We%20address%20Embodied%20Reference%20Understanding%2C%20the%20task%20of%20predicting%20the%20object%20a%20person%20in%20the%20scene%20refers%20to%20through%20pointing%20gesture%20and%20language.%20This%20requires%20multimodal%20reasoning%20over%20text%2C%20visual%20pointing%20cues%2C%20and%20scene%20context%2C%20yet%20existing%20methods%20often%20fail%20to%20fully%20exploit%20visual%20disambiguation%20signals.%20We%20also%20observe%20that%20while%20the%20referent%20often%20aligns%20with%20the%20head-to-fingertip%20direction%2C%20in%20many%20cases%20it%20aligns%20more%20closely%20with%20the%20wrist-to-fingertip%20direction%2C%20making%20a%20single-line%20assumption%20overly%20limiting.%20To%20address%20this%2C%20we%20propose%20a%20dual-model%20framework%2C%20where%20one%20model%20learns%20from%20the%20head-to-fingertip%20direction%20and%20the%20other%20from%20the%20wrist-to-fingertip%20direction.%20We%20introduce%20a%20Gaussian%20ray%20heatmap%20representation%20of%20these%20lines%20and%20use%20them%20as%20input%20to%20provide%20a%20strong%20supervisory%20signal%20that%20encourages%20the%20model%20to%20better%20attend%20to%20pointing%20cues.%20To%20fuse%20their%20complementary%20strengths%2C%20we%20present%20the%20CLIP-Aware%20Pointing%20Ensemble%20module%2C%20which%20performs%20a%20hybrid%20ensemble%20guided%20by%20CLIP%20features.%20We%20further%20incorporate%20an%20auxiliary%20object%20center%20prediction%20head%20to%20enhance%20referent%20localization.%20We%20validate%20our%20approach%20on%20YouRefIt%2C%20achieving%2075.0%20mAP%20at%200.25%20IoU%2C%20alongside%20state-of-the-art%20CLIP%20and%20C_D%20scores%2C%20and%20demonstrate%20its%20generality%20on%20unseen%20CAESAR%20and%20ISL%20Pointing%2C%20showing%20robust%20performance%20across%20benchmarks.&entry.1838667208=http%3A//arxiv.org/abs/2507.21888v3&entry.124074799=Read"},
{"title": "ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation", "author": "Ruihang Xu and Dewei Zhou and Fan Ma and Yi Yang", "abstract": "Multi-instance image generation (MIG) remains a significant challenge for modern diffusion models due to key limitations in achieving precise control over object layout and preserving the identity of multiple distinct subjects. To address these limitations, we introduce ContextGen, a novel Diffusion Transformer framework for multi-instance generation that is guided by both layout and reference images. Our approach integrates two key technical contributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates the composite layout image into the generation context to robustly anchor the objects in their desired positions, and Identity Consistency Attention (ICA), an innovative attention mechanism that leverages contextual reference images to ensure the identity consistency of multiple instances. Recognizing the lack of large-scale, hierarchically-structured datasets for this task, we introduce IMIG-100K, the first dataset with detailed layout and identity annotations. Extensive experiments demonstrate that ContextGen sets a new state-of-the-art, outperforming existing methods in control precision, identity fidelity, and overall visual quality.", "link": "http://arxiv.org/abs/2510.11000v2", "date": "2025-12-09", "relevancy": 2.3514, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6112}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5933}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5624}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20ContextGen%3A%20Contextual%20Layout%20Anchoring%20for%20Identity-Consistent%20Multi-Instance%20Generation&body=Title%3A%20ContextGen%3A%20Contextual%20Layout%20Anchoring%20for%20Identity-Consistent%20Multi-Instance%20Generation%0AAuthor%3A%20Ruihang%20Xu%20and%20Dewei%20Zhou%20and%20Fan%20Ma%20and%20Yi%20Yang%0AAbstract%3A%20Multi-instance%20image%20generation%20%28MIG%29%20remains%20a%20significant%20challenge%20for%20modern%20diffusion%20models%20due%20to%20key%20limitations%20in%20achieving%20precise%20control%20over%20object%20layout%20and%20preserving%20the%20identity%20of%20multiple%20distinct%20subjects.%20To%20address%20these%20limitations%2C%20we%20introduce%20ContextGen%2C%20a%20novel%20Diffusion%20Transformer%20framework%20for%20multi-instance%20generation%20that%20is%20guided%20by%20both%20layout%20and%20reference%20images.%20Our%20approach%20integrates%20two%20key%20technical%20contributions%3A%20a%20Contextual%20Layout%20Anchoring%20%28CLA%29%20mechanism%20that%20incorporates%20the%20composite%20layout%20image%20into%20the%20generation%20context%20to%20robustly%20anchor%20the%20objects%20in%20their%20desired%20positions%2C%20and%20Identity%20Consistency%20Attention%20%28ICA%29%2C%20an%20innovative%20attention%20mechanism%20that%20leverages%20contextual%20reference%20images%20to%20ensure%20the%20identity%20consistency%20of%20multiple%20instances.%20Recognizing%20the%20lack%20of%20large-scale%2C%20hierarchically-structured%20datasets%20for%20this%20task%2C%20we%20introduce%20IMIG-100K%2C%20the%20first%20dataset%20with%20detailed%20layout%20and%20identity%20annotations.%20Extensive%20experiments%20demonstrate%20that%20ContextGen%20sets%20a%20new%20state-of-the-art%2C%20outperforming%20existing%20methods%20in%20control%20precision%2C%20identity%20fidelity%2C%20and%20overall%20visual%20quality.%0ALink%3A%20http%3A//arxiv.org/abs/2510.11000v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DContextGen%253A%2520Contextual%2520Layout%2520Anchoring%2520for%2520Identity-Consistent%2520Multi-Instance%2520Generation%26entry.906535625%3DRuihang%2520Xu%2520and%2520Dewei%2520Zhou%2520and%2520Fan%2520Ma%2520and%2520Yi%2520Yang%26entry.1292438233%3DMulti-instance%2520image%2520generation%2520%2528MIG%2529%2520remains%2520a%2520significant%2520challenge%2520for%2520modern%2520diffusion%2520models%2520due%2520to%2520key%2520limitations%2520in%2520achieving%2520precise%2520control%2520over%2520object%2520layout%2520and%2520preserving%2520the%2520identity%2520of%2520multiple%2520distinct%2520subjects.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520ContextGen%252C%2520a%2520novel%2520Diffusion%2520Transformer%2520framework%2520for%2520multi-instance%2520generation%2520that%2520is%2520guided%2520by%2520both%2520layout%2520and%2520reference%2520images.%2520Our%2520approach%2520integrates%2520two%2520key%2520technical%2520contributions%253A%2520a%2520Contextual%2520Layout%2520Anchoring%2520%2528CLA%2529%2520mechanism%2520that%2520incorporates%2520the%2520composite%2520layout%2520image%2520into%2520the%2520generation%2520context%2520to%2520robustly%2520anchor%2520the%2520objects%2520in%2520their%2520desired%2520positions%252C%2520and%2520Identity%2520Consistency%2520Attention%2520%2528ICA%2529%252C%2520an%2520innovative%2520attention%2520mechanism%2520that%2520leverages%2520contextual%2520reference%2520images%2520to%2520ensure%2520the%2520identity%2520consistency%2520of%2520multiple%2520instances.%2520Recognizing%2520the%2520lack%2520of%2520large-scale%252C%2520hierarchically-structured%2520datasets%2520for%2520this%2520task%252C%2520we%2520introduce%2520IMIG-100K%252C%2520the%2520first%2520dataset%2520with%2520detailed%2520layout%2520and%2520identity%2520annotations.%2520Extensive%2520experiments%2520demonstrate%2520that%2520ContextGen%2520sets%2520a%2520new%2520state-of-the-art%252C%2520outperforming%2520existing%2520methods%2520in%2520control%2520precision%252C%2520identity%2520fidelity%252C%2520and%2520overall%2520visual%2520quality.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2510.11000v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ContextGen%3A%20Contextual%20Layout%20Anchoring%20for%20Identity-Consistent%20Multi-Instance%20Generation&entry.906535625=Ruihang%20Xu%20and%20Dewei%20Zhou%20and%20Fan%20Ma%20and%20Yi%20Yang&entry.1292438233=Multi-instance%20image%20generation%20%28MIG%29%20remains%20a%20significant%20challenge%20for%20modern%20diffusion%20models%20due%20to%20key%20limitations%20in%20achieving%20precise%20control%20over%20object%20layout%20and%20preserving%20the%20identity%20of%20multiple%20distinct%20subjects.%20To%20address%20these%20limitations%2C%20we%20introduce%20ContextGen%2C%20a%20novel%20Diffusion%20Transformer%20framework%20for%20multi-instance%20generation%20that%20is%20guided%20by%20both%20layout%20and%20reference%20images.%20Our%20approach%20integrates%20two%20key%20technical%20contributions%3A%20a%20Contextual%20Layout%20Anchoring%20%28CLA%29%20mechanism%20that%20incorporates%20the%20composite%20layout%20image%20into%20the%20generation%20context%20to%20robustly%20anchor%20the%20objects%20in%20their%20desired%20positions%2C%20and%20Identity%20Consistency%20Attention%20%28ICA%29%2C%20an%20innovative%20attention%20mechanism%20that%20leverages%20contextual%20reference%20images%20to%20ensure%20the%20identity%20consistency%20of%20multiple%20instances.%20Recognizing%20the%20lack%20of%20large-scale%2C%20hierarchically-structured%20datasets%20for%20this%20task%2C%20we%20introduce%20IMIG-100K%2C%20the%20first%20dataset%20with%20detailed%20layout%20and%20identity%20annotations.%20Extensive%20experiments%20demonstrate%20that%20ContextGen%20sets%20a%20new%20state-of-the-art%2C%20outperforming%20existing%20methods%20in%20control%20precision%2C%20identity%20fidelity%2C%20and%20overall%20visual%20quality.&entry.1838667208=http%3A//arxiv.org/abs/2510.11000v2&entry.124074799=Read"},
{"title": "OCCDiff: Occupancy Diffusion Model for High-Fidelity 3D Building Reconstruction from Noisy Point Clouds", "author": "Jialu Sui and Rui Liu and Hongsheng Zhang", "abstract": "A major challenge in reconstructing buildings from LiDAR point clouds lies in accurately capturing building surfaces under varying point densities and noise interference. To flexibly gather high-quality 3D profiles of the building in diverse resolution, we propose OCCDiff applying latent diffusion in the occupancy function space. Our OCCDiff combines a latent diffusion process with a function autoencoder architecture to generate continuous occupancy functions evaluable at arbitrary locations. Moreover, a point encoder is proposed to provide condition features to diffusion learning, constraint the final occupancy prediction for occupancy decoder, and insert multi-modal features for latent generation to latent encoder. To further enhance the model performance, a multi-task training strategy is employed, ensuring that the point encoder learns diverse and robust feature representations. Empirical results show that our method generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy data.", "link": "http://arxiv.org/abs/2512.08506v1", "date": "2025-12-09", "relevancy": 2.3514, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5914}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5914}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5699}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OCCDiff%3A%20Occupancy%20Diffusion%20Model%20for%20High-Fidelity%203D%20Building%20Reconstruction%20from%20Noisy%20Point%20Clouds&body=Title%3A%20OCCDiff%3A%20Occupancy%20Diffusion%20Model%20for%20High-Fidelity%203D%20Building%20Reconstruction%20from%20Noisy%20Point%20Clouds%0AAuthor%3A%20Jialu%20Sui%20and%20Rui%20Liu%20and%20Hongsheng%20Zhang%0AAbstract%3A%20A%20major%20challenge%20in%20reconstructing%20buildings%20from%20LiDAR%20point%20clouds%20lies%20in%20accurately%20capturing%20building%20surfaces%20under%20varying%20point%20densities%20and%20noise%20interference.%20To%20flexibly%20gather%20high-quality%203D%20profiles%20of%20the%20building%20in%20diverse%20resolution%2C%20we%20propose%20OCCDiff%20applying%20latent%20diffusion%20in%20the%20occupancy%20function%20space.%20Our%20OCCDiff%20combines%20a%20latent%20diffusion%20process%20with%20a%20function%20autoencoder%20architecture%20to%20generate%20continuous%20occupancy%20functions%20evaluable%20at%20arbitrary%20locations.%20Moreover%2C%20a%20point%20encoder%20is%20proposed%20to%20provide%20condition%20features%20to%20diffusion%20learning%2C%20constraint%20the%20final%20occupancy%20prediction%20for%20occupancy%20decoder%2C%20and%20insert%20multi-modal%20features%20for%20latent%20generation%20to%20latent%20encoder.%20To%20further%20enhance%20the%20model%20performance%2C%20a%20multi-task%20training%20strategy%20is%20employed%2C%20ensuring%20that%20the%20point%20encoder%20learns%20diverse%20and%20robust%20feature%20representations.%20Empirical%20results%20show%20that%20our%20method%20generates%20physically%20consistent%20samples%20with%20high%20fidelity%20to%20the%20target%20distribution%20and%20exhibits%20robustness%20to%20noisy%20data.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08506v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOCCDiff%253A%2520Occupancy%2520Diffusion%2520Model%2520for%2520High-Fidelity%25203D%2520Building%2520Reconstruction%2520from%2520Noisy%2520Point%2520Clouds%26entry.906535625%3DJialu%2520Sui%2520and%2520Rui%2520Liu%2520and%2520Hongsheng%2520Zhang%26entry.1292438233%3DA%2520major%2520challenge%2520in%2520reconstructing%2520buildings%2520from%2520LiDAR%2520point%2520clouds%2520lies%2520in%2520accurately%2520capturing%2520building%2520surfaces%2520under%2520varying%2520point%2520densities%2520and%2520noise%2520interference.%2520To%2520flexibly%2520gather%2520high-quality%25203D%2520profiles%2520of%2520the%2520building%2520in%2520diverse%2520resolution%252C%2520we%2520propose%2520OCCDiff%2520applying%2520latent%2520diffusion%2520in%2520the%2520occupancy%2520function%2520space.%2520Our%2520OCCDiff%2520combines%2520a%2520latent%2520diffusion%2520process%2520with%2520a%2520function%2520autoencoder%2520architecture%2520to%2520generate%2520continuous%2520occupancy%2520functions%2520evaluable%2520at%2520arbitrary%2520locations.%2520Moreover%252C%2520a%2520point%2520encoder%2520is%2520proposed%2520to%2520provide%2520condition%2520features%2520to%2520diffusion%2520learning%252C%2520constraint%2520the%2520final%2520occupancy%2520prediction%2520for%2520occupancy%2520decoder%252C%2520and%2520insert%2520multi-modal%2520features%2520for%2520latent%2520generation%2520to%2520latent%2520encoder.%2520To%2520further%2520enhance%2520the%2520model%2520performance%252C%2520a%2520multi-task%2520training%2520strategy%2520is%2520employed%252C%2520ensuring%2520that%2520the%2520point%2520encoder%2520learns%2520diverse%2520and%2520robust%2520feature%2520representations.%2520Empirical%2520results%2520show%2520that%2520our%2520method%2520generates%2520physically%2520consistent%2520samples%2520with%2520high%2520fidelity%2520to%2520the%2520target%2520distribution%2520and%2520exhibits%2520robustness%2520to%2520noisy%2520data.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08506v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OCCDiff%3A%20Occupancy%20Diffusion%20Model%20for%20High-Fidelity%203D%20Building%20Reconstruction%20from%20Noisy%20Point%20Clouds&entry.906535625=Jialu%20Sui%20and%20Rui%20Liu%20and%20Hongsheng%20Zhang&entry.1292438233=A%20major%20challenge%20in%20reconstructing%20buildings%20from%20LiDAR%20point%20clouds%20lies%20in%20accurately%20capturing%20building%20surfaces%20under%20varying%20point%20densities%20and%20noise%20interference.%20To%20flexibly%20gather%20high-quality%203D%20profiles%20of%20the%20building%20in%20diverse%20resolution%2C%20we%20propose%20OCCDiff%20applying%20latent%20diffusion%20in%20the%20occupancy%20function%20space.%20Our%20OCCDiff%20combines%20a%20latent%20diffusion%20process%20with%20a%20function%20autoencoder%20architecture%20to%20generate%20continuous%20occupancy%20functions%20evaluable%20at%20arbitrary%20locations.%20Moreover%2C%20a%20point%20encoder%20is%20proposed%20to%20provide%20condition%20features%20to%20diffusion%20learning%2C%20constraint%20the%20final%20occupancy%20prediction%20for%20occupancy%20decoder%2C%20and%20insert%20multi-modal%20features%20for%20latent%20generation%20to%20latent%20encoder.%20To%20further%20enhance%20the%20model%20performance%2C%20a%20multi-task%20training%20strategy%20is%20employed%2C%20ensuring%20that%20the%20point%20encoder%20learns%20diverse%20and%20robust%20feature%20representations.%20Empirical%20results%20show%20that%20our%20method%20generates%20physically%20consistent%20samples%20with%20high%20fidelity%20to%20the%20target%20distribution%20and%20exhibits%20robustness%20to%20noisy%20data.&entry.1838667208=http%3A//arxiv.org/abs/2512.08506v1&entry.124074799=Read"},
{"title": "BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain", "author": "Navve Wasserman and Matias Cosarinsky and Yuval Golbari and Aude Oliva and Antonio Torralba and Tamar Rott Shaham and Michal Irani", "abstract": "Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.", "link": "http://arxiv.org/abs/2512.08560v1", "date": "2025-12-09", "relevancy": 2.3513, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5963}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5963}, {"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5457}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BrainExplore%3A%20Large-Scale%20Discovery%20of%20Interpretable%20Visual%20Representations%20in%20the%20Human%20Brain&body=Title%3A%20BrainExplore%3A%20Large-Scale%20Discovery%20of%20Interpretable%20Visual%20Representations%20in%20the%20Human%20Brain%0AAuthor%3A%20Navve%20Wasserman%20and%20Matias%20Cosarinsky%20and%20Yuval%20Golbari%20and%20Aude%20Oliva%20and%20Antonio%20Torralba%20and%20Tamar%20Rott%20Shaham%20and%20Michal%20Irani%0AAbstract%3A%20Understanding%20how%20the%20human%20brain%20represents%20visual%20concepts%2C%20and%20in%20which%20brain%20regions%20these%20representations%20are%20encoded%2C%20remains%20a%20long-standing%20challenge.%20Decades%20of%20work%20have%20advanced%20our%20understanding%20of%20visual%20representations%2C%20yet%20brain%20signals%20remain%20large%20and%20complex%2C%20and%20the%20space%20of%20possible%20visual%20concepts%20is%20vast.%20As%20a%20result%2C%20most%20studies%20remain%20small-scale%2C%20rely%20on%20manual%20inspection%2C%20focus%20on%20specific%20regions%20and%20properties%2C%20and%20rarely%20include%20systematic%20validation.%20We%20present%20a%20large-scale%2C%20automated%20framework%20for%20discovering%20and%20explaining%20visual%20representations%20across%20the%20human%20cortex.%20Our%20method%20comprises%20two%20main%20stages.%20First%2C%20we%20discover%20candidate%20interpretable%20patterns%20in%20fMRI%20activity%20through%20unsupervised%2C%20data-driven%20decomposition%20methods.%20Next%2C%20we%20explain%20each%20pattern%20by%20identifying%20the%20set%20of%20natural%20images%20that%20most%20strongly%20elicit%20it%20and%20generating%20a%20natural-language%20description%20of%20their%20shared%20visual%20meaning.%20To%20scale%20this%20process%2C%20we%20introduce%20an%20automated%20pipeline%20that%20tests%20multiple%20candidate%20explanations%2C%20assigns%20quantitative%20reliability%20scores%2C%20and%20selects%20the%20most%20consistent%20description%20for%20each%20voxel%20pattern.%20Our%20framework%20reveals%20thousands%20of%20interpretable%20patterns%20spanning%20many%20distinct%20visual%20concepts%2C%20including%20fine-grained%20representations%20previously%20unreported.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08560v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBrainExplore%253A%2520Large-Scale%2520Discovery%2520of%2520Interpretable%2520Visual%2520Representations%2520in%2520the%2520Human%2520Brain%26entry.906535625%3DNavve%2520Wasserman%2520and%2520Matias%2520Cosarinsky%2520and%2520Yuval%2520Golbari%2520and%2520Aude%2520Oliva%2520and%2520Antonio%2520Torralba%2520and%2520Tamar%2520Rott%2520Shaham%2520and%2520Michal%2520Irani%26entry.1292438233%3DUnderstanding%2520how%2520the%2520human%2520brain%2520represents%2520visual%2520concepts%252C%2520and%2520in%2520which%2520brain%2520regions%2520these%2520representations%2520are%2520encoded%252C%2520remains%2520a%2520long-standing%2520challenge.%2520Decades%2520of%2520work%2520have%2520advanced%2520our%2520understanding%2520of%2520visual%2520representations%252C%2520yet%2520brain%2520signals%2520remain%2520large%2520and%2520complex%252C%2520and%2520the%2520space%2520of%2520possible%2520visual%2520concepts%2520is%2520vast.%2520As%2520a%2520result%252C%2520most%2520studies%2520remain%2520small-scale%252C%2520rely%2520on%2520manual%2520inspection%252C%2520focus%2520on%2520specific%2520regions%2520and%2520properties%252C%2520and%2520rarely%2520include%2520systematic%2520validation.%2520We%2520present%2520a%2520large-scale%252C%2520automated%2520framework%2520for%2520discovering%2520and%2520explaining%2520visual%2520representations%2520across%2520the%2520human%2520cortex.%2520Our%2520method%2520comprises%2520two%2520main%2520stages.%2520First%252C%2520we%2520discover%2520candidate%2520interpretable%2520patterns%2520in%2520fMRI%2520activity%2520through%2520unsupervised%252C%2520data-driven%2520decomposition%2520methods.%2520Next%252C%2520we%2520explain%2520each%2520pattern%2520by%2520identifying%2520the%2520set%2520of%2520natural%2520images%2520that%2520most%2520strongly%2520elicit%2520it%2520and%2520generating%2520a%2520natural-language%2520description%2520of%2520their%2520shared%2520visual%2520meaning.%2520To%2520scale%2520this%2520process%252C%2520we%2520introduce%2520an%2520automated%2520pipeline%2520that%2520tests%2520multiple%2520candidate%2520explanations%252C%2520assigns%2520quantitative%2520reliability%2520scores%252C%2520and%2520selects%2520the%2520most%2520consistent%2520description%2520for%2520each%2520voxel%2520pattern.%2520Our%2520framework%2520reveals%2520thousands%2520of%2520interpretable%2520patterns%2520spanning%2520many%2520distinct%2520visual%2520concepts%252C%2520including%2520fine-grained%2520representations%2520previously%2520unreported.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08560v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BrainExplore%3A%20Large-Scale%20Discovery%20of%20Interpretable%20Visual%20Representations%20in%20the%20Human%20Brain&entry.906535625=Navve%20Wasserman%20and%20Matias%20Cosarinsky%20and%20Yuval%20Golbari%20and%20Aude%20Oliva%20and%20Antonio%20Torralba%20and%20Tamar%20Rott%20Shaham%20and%20Michal%20Irani&entry.1292438233=Understanding%20how%20the%20human%20brain%20represents%20visual%20concepts%2C%20and%20in%20which%20brain%20regions%20these%20representations%20are%20encoded%2C%20remains%20a%20long-standing%20challenge.%20Decades%20of%20work%20have%20advanced%20our%20understanding%20of%20visual%20representations%2C%20yet%20brain%20signals%20remain%20large%20and%20complex%2C%20and%20the%20space%20of%20possible%20visual%20concepts%20is%20vast.%20As%20a%20result%2C%20most%20studies%20remain%20small-scale%2C%20rely%20on%20manual%20inspection%2C%20focus%20on%20specific%20regions%20and%20properties%2C%20and%20rarely%20include%20systematic%20validation.%20We%20present%20a%20large-scale%2C%20automated%20framework%20for%20discovering%20and%20explaining%20visual%20representations%20across%20the%20human%20cortex.%20Our%20method%20comprises%20two%20main%20stages.%20First%2C%20we%20discover%20candidate%20interpretable%20patterns%20in%20fMRI%20activity%20through%20unsupervised%2C%20data-driven%20decomposition%20methods.%20Next%2C%20we%20explain%20each%20pattern%20by%20identifying%20the%20set%20of%20natural%20images%20that%20most%20strongly%20elicit%20it%20and%20generating%20a%20natural-language%20description%20of%20their%20shared%20visual%20meaning.%20To%20scale%20this%20process%2C%20we%20introduce%20an%20automated%20pipeline%20that%20tests%20multiple%20candidate%20explanations%2C%20assigns%20quantitative%20reliability%20scores%2C%20and%20selects%20the%20most%20consistent%20description%20for%20each%20voxel%20pattern.%20Our%20framework%20reveals%20thousands%20of%20interpretable%20patterns%20spanning%20many%20distinct%20visual%20concepts%2C%20including%20fine-grained%20representations%20previously%20unreported.&entry.1838667208=http%3A//arxiv.org/abs/2512.08560v1&entry.124074799=Read"},
{"title": "Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation", "author": "Young Kyung Kim and Oded Schlesinger and Yuzhou Zhao and J. Matias Di Martino and Guillermo Sapiro", "abstract": "While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a \"black box.\" This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.", "link": "http://arxiv.org/abs/2512.08645v1", "date": "2025-12-09", "relevancy": 2.3371, "topK": [{"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.6056}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5823}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5777}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Chain-of-Image%20Generation%3A%20Toward%20Monitorable%20and%20Controllable%20Image%20Generation&body=Title%3A%20Chain-of-Image%20Generation%3A%20Toward%20Monitorable%20and%20Controllable%20Image%20Generation%0AAuthor%3A%20Young%20Kyung%20Kim%20and%20Oded%20Schlesinger%20and%20Yuzhou%20Zhao%20and%20J.%20Matias%20Di%20Martino%20and%20Guillermo%20Sapiro%0AAbstract%3A%20While%20state-of-the-art%20image%20generation%20models%20achieve%20remarkable%20visual%20quality%2C%20their%20internal%20generative%20processes%20remain%20a%20%22black%20box.%22%20This%20opacity%20limits%20human%20observation%20and%20intervention%2C%20and%20poses%20a%20barrier%20to%20ensuring%20model%20reliability%2C%20safety%2C%20and%20control.%20Furthermore%2C%20their%20non-human-like%20workflows%20make%20them%20difficult%20for%20human%20observers%20to%20interpret.%20To%20address%20this%2C%20we%20introduce%20the%20Chain-of-Image%20Generation%20%28CoIG%29%20framework%2C%20which%20reframes%20image%20generation%20as%20a%20sequential%2C%20semantic%20process%20analogous%20to%20how%20humans%20create%20art.%20Similar%20to%20the%20advantages%20in%20monitorability%20and%20performance%20that%20Chain-of-Thought%20%28CoT%29%20brought%20to%20large%20language%20models%20%28LLMs%29%2C%20CoIG%20can%20produce%20equivalent%20benefits%20in%20text-to-image%20generation.%20CoIG%20utilizes%20an%20LLM%20to%20decompose%20a%20complex%20prompt%20into%20a%20sequence%20of%20simple%2C%20step-by-step%20instructions.%20The%20image%20generation%20model%20then%20executes%20this%20plan%20by%20progressively%20generating%20and%20editing%20the%20image.%20Each%20step%20focuses%20on%20a%20single%20semantic%20entity%2C%20enabling%20direct%20monitoring.%20We%20formally%20assess%20this%20property%20using%20two%20novel%20metrics%3A%20CoIG%20Readability%2C%20which%20evaluates%20the%20clarity%20of%20each%20intermediate%20step%20via%20its%20corresponding%20output%3B%20and%20Causal%20Relevance%2C%20which%20quantifies%20the%20impact%20of%20each%20procedural%20step%20on%20the%20final%20generated%20image.%20We%20further%20show%20that%20our%20framework%20mitigates%20entity%20collapse%20by%20decomposing%20the%20complex%20generation%20task%20into%20simple%20subproblems%2C%20analogous%20to%20the%20procedural%20reasoning%20employed%20by%20CoT.%20Our%20experimental%20results%20indicate%20that%20CoIG%20substantially%20enhances%20quantitative%20monitorability%20while%20achieving%20competitive%20compositional%20robustness%20compared%20to%20established%20baseline%20models.%20The%20framework%20is%20model-agnostic%20and%20can%20be%20integrated%20with%20any%20image%20generation%20model.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08645v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DChain-of-Image%2520Generation%253A%2520Toward%2520Monitorable%2520and%2520Controllable%2520Image%2520Generation%26entry.906535625%3DYoung%2520Kyung%2520Kim%2520and%2520Oded%2520Schlesinger%2520and%2520Yuzhou%2520Zhao%2520and%2520J.%2520Matias%2520Di%2520Martino%2520and%2520Guillermo%2520Sapiro%26entry.1292438233%3DWhile%2520state-of-the-art%2520image%2520generation%2520models%2520achieve%2520remarkable%2520visual%2520quality%252C%2520their%2520internal%2520generative%2520processes%2520remain%2520a%2520%2522black%2520box.%2522%2520This%2520opacity%2520limits%2520human%2520observation%2520and%2520intervention%252C%2520and%2520poses%2520a%2520barrier%2520to%2520ensuring%2520model%2520reliability%252C%2520safety%252C%2520and%2520control.%2520Furthermore%252C%2520their%2520non-human-like%2520workflows%2520make%2520them%2520difficult%2520for%2520human%2520observers%2520to%2520interpret.%2520To%2520address%2520this%252C%2520we%2520introduce%2520the%2520Chain-of-Image%2520Generation%2520%2528CoIG%2529%2520framework%252C%2520which%2520reframes%2520image%2520generation%2520as%2520a%2520sequential%252C%2520semantic%2520process%2520analogous%2520to%2520how%2520humans%2520create%2520art.%2520Similar%2520to%2520the%2520advantages%2520in%2520monitorability%2520and%2520performance%2520that%2520Chain-of-Thought%2520%2528CoT%2529%2520brought%2520to%2520large%2520language%2520models%2520%2528LLMs%2529%252C%2520CoIG%2520can%2520produce%2520equivalent%2520benefits%2520in%2520text-to-image%2520generation.%2520CoIG%2520utilizes%2520an%2520LLM%2520to%2520decompose%2520a%2520complex%2520prompt%2520into%2520a%2520sequence%2520of%2520simple%252C%2520step-by-step%2520instructions.%2520The%2520image%2520generation%2520model%2520then%2520executes%2520this%2520plan%2520by%2520progressively%2520generating%2520and%2520editing%2520the%2520image.%2520Each%2520step%2520focuses%2520on%2520a%2520single%2520semantic%2520entity%252C%2520enabling%2520direct%2520monitoring.%2520We%2520formally%2520assess%2520this%2520property%2520using%2520two%2520novel%2520metrics%253A%2520CoIG%2520Readability%252C%2520which%2520evaluates%2520the%2520clarity%2520of%2520each%2520intermediate%2520step%2520via%2520its%2520corresponding%2520output%253B%2520and%2520Causal%2520Relevance%252C%2520which%2520quantifies%2520the%2520impact%2520of%2520each%2520procedural%2520step%2520on%2520the%2520final%2520generated%2520image.%2520We%2520further%2520show%2520that%2520our%2520framework%2520mitigates%2520entity%2520collapse%2520by%2520decomposing%2520the%2520complex%2520generation%2520task%2520into%2520simple%2520subproblems%252C%2520analogous%2520to%2520the%2520procedural%2520reasoning%2520employed%2520by%2520CoT.%2520Our%2520experimental%2520results%2520indicate%2520that%2520CoIG%2520substantially%2520enhances%2520quantitative%2520monitorability%2520while%2520achieving%2520competitive%2520compositional%2520robustness%2520compared%2520to%2520established%2520baseline%2520models.%2520The%2520framework%2520is%2520model-agnostic%2520and%2520can%2520be%2520integrated%2520with%2520any%2520image%2520generation%2520model.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08645v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Chain-of-Image%20Generation%3A%20Toward%20Monitorable%20and%20Controllable%20Image%20Generation&entry.906535625=Young%20Kyung%20Kim%20and%20Oded%20Schlesinger%20and%20Yuzhou%20Zhao%20and%20J.%20Matias%20Di%20Martino%20and%20Guillermo%20Sapiro&entry.1292438233=While%20state-of-the-art%20image%20generation%20models%20achieve%20remarkable%20visual%20quality%2C%20their%20internal%20generative%20processes%20remain%20a%20%22black%20box.%22%20This%20opacity%20limits%20human%20observation%20and%20intervention%2C%20and%20poses%20a%20barrier%20to%20ensuring%20model%20reliability%2C%20safety%2C%20and%20control.%20Furthermore%2C%20their%20non-human-like%20workflows%20make%20them%20difficult%20for%20human%20observers%20to%20interpret.%20To%20address%20this%2C%20we%20introduce%20the%20Chain-of-Image%20Generation%20%28CoIG%29%20framework%2C%20which%20reframes%20image%20generation%20as%20a%20sequential%2C%20semantic%20process%20analogous%20to%20how%20humans%20create%20art.%20Similar%20to%20the%20advantages%20in%20monitorability%20and%20performance%20that%20Chain-of-Thought%20%28CoT%29%20brought%20to%20large%20language%20models%20%28LLMs%29%2C%20CoIG%20can%20produce%20equivalent%20benefits%20in%20text-to-image%20generation.%20CoIG%20utilizes%20an%20LLM%20to%20decompose%20a%20complex%20prompt%20into%20a%20sequence%20of%20simple%2C%20step-by-step%20instructions.%20The%20image%20generation%20model%20then%20executes%20this%20plan%20by%20progressively%20generating%20and%20editing%20the%20image.%20Each%20step%20focuses%20on%20a%20single%20semantic%20entity%2C%20enabling%20direct%20monitoring.%20We%20formally%20assess%20this%20property%20using%20two%20novel%20metrics%3A%20CoIG%20Readability%2C%20which%20evaluates%20the%20clarity%20of%20each%20intermediate%20step%20via%20its%20corresponding%20output%3B%20and%20Causal%20Relevance%2C%20which%20quantifies%20the%20impact%20of%20each%20procedural%20step%20on%20the%20final%20generated%20image.%20We%20further%20show%20that%20our%20framework%20mitigates%20entity%20collapse%20by%20decomposing%20the%20complex%20generation%20task%20into%20simple%20subproblems%2C%20analogous%20to%20the%20procedural%20reasoning%20employed%20by%20CoT.%20Our%20experimental%20results%20indicate%20that%20CoIG%20substantially%20enhances%20quantitative%20monitorability%20while%20achieving%20competitive%20compositional%20robustness%20compared%20to%20established%20baseline%20models.%20The%20framework%20is%20model-agnostic%20and%20can%20be%20integrated%20with%20any%20image%20generation%20model.&entry.1838667208=http%3A//arxiv.org/abs/2512.08645v1&entry.124074799=Read"},
{"title": "Humanoid Whole-Body Badminton via Multi-Stage Reinforcement Learning", "author": "Chenhao Liu and Leyun Jiang and Yibo Wang and Kairan Yao and Jinchen Fu and Xiaoyu Ren", "abstract": "Humanoid robots have demonstrated strong capabilities for interacting with static scenes across locomotion, manipulation, and more challenging loco-manipulation tasks. Yet the real world is dynamic, and quasi-static interactions are insufficient to cope with diverse environmental conditions. As a step toward more dynamic interaction scenarios, we present a reinforcement-learning-based training pipeline that produces a unified whole-body controller for humanoid badminton, enabling coordinated lower-body footwork and upper-body striking without motion priors or expert demonstrations. Training follows a three-stage curriculum: first footwork acquisition, then precision-guided racket swing generation, and finally task-focused refinement, yielding motions in which both legs and arms serve the hitting objective. For deployment, we incorporate an Extended Kalman Filter (EKF) to estimate and predict shuttlecock trajectories for target striking. We also introduce a prediction-free variant that dispenses with EKF and explicit trajectory prediction. To validate the framework, we conduct five sets of experiments in both simulation and the real world. In simulation, two robots sustain a rally of 21 consecutive hits. Moreover, the prediction-free variant achieves successful hits with comparable performance relative to the target-known policy. In real-world tests, both prediction and controller modules exhibit high accuracy, and on-court hitting achieves an outgoing shuttle speed up to 19.1 m/s with a mean return landing distance of 4 m. These experimental results show that our proposed training scheme can deliver highly dynamic while precise goal striking in badminton, and can be adapted to more dynamics-critical domains.", "link": "http://arxiv.org/abs/2511.11218v2", "date": "2025-12-09", "relevancy": 2.3291, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6024}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5827}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Humanoid%20Whole-Body%20Badminton%20via%20Multi-Stage%20Reinforcement%20Learning&body=Title%3A%20Humanoid%20Whole-Body%20Badminton%20via%20Multi-Stage%20Reinforcement%20Learning%0AAuthor%3A%20Chenhao%20Liu%20and%20Leyun%20Jiang%20and%20Yibo%20Wang%20and%20Kairan%20Yao%20and%20Jinchen%20Fu%20and%20Xiaoyu%20Ren%0AAbstract%3A%20Humanoid%20robots%20have%20demonstrated%20strong%20capabilities%20for%20interacting%20with%20static%20scenes%20across%20locomotion%2C%20manipulation%2C%20and%20more%20challenging%20loco-manipulation%20tasks.%20Yet%20the%20real%20world%20is%20dynamic%2C%20and%20quasi-static%20interactions%20are%20insufficient%20to%20cope%20with%20diverse%20environmental%20conditions.%20As%20a%20step%20toward%20more%20dynamic%20interaction%20scenarios%2C%20we%20present%20a%20reinforcement-learning-based%20training%20pipeline%20that%20produces%20a%20unified%20whole-body%20controller%20for%20humanoid%20badminton%2C%20enabling%20coordinated%20lower-body%20footwork%20and%20upper-body%20striking%20without%20motion%20priors%20or%20expert%20demonstrations.%20Training%20follows%20a%20three-stage%20curriculum%3A%20first%20footwork%20acquisition%2C%20then%20precision-guided%20racket%20swing%20generation%2C%20and%20finally%20task-focused%20refinement%2C%20yielding%20motions%20in%20which%20both%20legs%20and%20arms%20serve%20the%20hitting%20objective.%20For%20deployment%2C%20we%20incorporate%20an%20Extended%20Kalman%20Filter%20%28EKF%29%20to%20estimate%20and%20predict%20shuttlecock%20trajectories%20for%20target%20striking.%20We%20also%20introduce%20a%20prediction-free%20variant%20that%20dispenses%20with%20EKF%20and%20explicit%20trajectory%20prediction.%20To%20validate%20the%20framework%2C%20we%20conduct%20five%20sets%20of%20experiments%20in%20both%20simulation%20and%20the%20real%20world.%20In%20simulation%2C%20two%20robots%20sustain%20a%20rally%20of%2021%20consecutive%20hits.%20Moreover%2C%20the%20prediction-free%20variant%20achieves%20successful%20hits%20with%20comparable%20performance%20relative%20to%20the%20target-known%20policy.%20In%20real-world%20tests%2C%20both%20prediction%20and%20controller%20modules%20exhibit%20high%20accuracy%2C%20and%20on-court%20hitting%20achieves%20an%20outgoing%20shuttle%20speed%20up%20to%2019.1%20m/s%20with%20a%20mean%20return%20landing%20distance%20of%204%20m.%20These%20experimental%20results%20show%20that%20our%20proposed%20training%20scheme%20can%20deliver%20highly%20dynamic%20while%20precise%20goal%20striking%20in%20badminton%2C%20and%20can%20be%20adapted%20to%20more%20dynamics-critical%20domains.%0ALink%3A%20http%3A//arxiv.org/abs/2511.11218v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHumanoid%2520Whole-Body%2520Badminton%2520via%2520Multi-Stage%2520Reinforcement%2520Learning%26entry.906535625%3DChenhao%2520Liu%2520and%2520Leyun%2520Jiang%2520and%2520Yibo%2520Wang%2520and%2520Kairan%2520Yao%2520and%2520Jinchen%2520Fu%2520and%2520Xiaoyu%2520Ren%26entry.1292438233%3DHumanoid%2520robots%2520have%2520demonstrated%2520strong%2520capabilities%2520for%2520interacting%2520with%2520static%2520scenes%2520across%2520locomotion%252C%2520manipulation%252C%2520and%2520more%2520challenging%2520loco-manipulation%2520tasks.%2520Yet%2520the%2520real%2520world%2520is%2520dynamic%252C%2520and%2520quasi-static%2520interactions%2520are%2520insufficient%2520to%2520cope%2520with%2520diverse%2520environmental%2520conditions.%2520As%2520a%2520step%2520toward%2520more%2520dynamic%2520interaction%2520scenarios%252C%2520we%2520present%2520a%2520reinforcement-learning-based%2520training%2520pipeline%2520that%2520produces%2520a%2520unified%2520whole-body%2520controller%2520for%2520humanoid%2520badminton%252C%2520enabling%2520coordinated%2520lower-body%2520footwork%2520and%2520upper-body%2520striking%2520without%2520motion%2520priors%2520or%2520expert%2520demonstrations.%2520Training%2520follows%2520a%2520three-stage%2520curriculum%253A%2520first%2520footwork%2520acquisition%252C%2520then%2520precision-guided%2520racket%2520swing%2520generation%252C%2520and%2520finally%2520task-focused%2520refinement%252C%2520yielding%2520motions%2520in%2520which%2520both%2520legs%2520and%2520arms%2520serve%2520the%2520hitting%2520objective.%2520For%2520deployment%252C%2520we%2520incorporate%2520an%2520Extended%2520Kalman%2520Filter%2520%2528EKF%2529%2520to%2520estimate%2520and%2520predict%2520shuttlecock%2520trajectories%2520for%2520target%2520striking.%2520We%2520also%2520introduce%2520a%2520prediction-free%2520variant%2520that%2520dispenses%2520with%2520EKF%2520and%2520explicit%2520trajectory%2520prediction.%2520To%2520validate%2520the%2520framework%252C%2520we%2520conduct%2520five%2520sets%2520of%2520experiments%2520in%2520both%2520simulation%2520and%2520the%2520real%2520world.%2520In%2520simulation%252C%2520two%2520robots%2520sustain%2520a%2520rally%2520of%252021%2520consecutive%2520hits.%2520Moreover%252C%2520the%2520prediction-free%2520variant%2520achieves%2520successful%2520hits%2520with%2520comparable%2520performance%2520relative%2520to%2520the%2520target-known%2520policy.%2520In%2520real-world%2520tests%252C%2520both%2520prediction%2520and%2520controller%2520modules%2520exhibit%2520high%2520accuracy%252C%2520and%2520on-court%2520hitting%2520achieves%2520an%2520outgoing%2520shuttle%2520speed%2520up%2520to%252019.1%2520m/s%2520with%2520a%2520mean%2520return%2520landing%2520distance%2520of%25204%2520m.%2520These%2520experimental%2520results%2520show%2520that%2520our%2520proposed%2520training%2520scheme%2520can%2520deliver%2520highly%2520dynamic%2520while%2520precise%2520goal%2520striking%2520in%2520badminton%252C%2520and%2520can%2520be%2520adapted%2520to%2520more%2520dynamics-critical%2520domains.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.11218v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Humanoid%20Whole-Body%20Badminton%20via%20Multi-Stage%20Reinforcement%20Learning&entry.906535625=Chenhao%20Liu%20and%20Leyun%20Jiang%20and%20Yibo%20Wang%20and%20Kairan%20Yao%20and%20Jinchen%20Fu%20and%20Xiaoyu%20Ren&entry.1292438233=Humanoid%20robots%20have%20demonstrated%20strong%20capabilities%20for%20interacting%20with%20static%20scenes%20across%20locomotion%2C%20manipulation%2C%20and%20more%20challenging%20loco-manipulation%20tasks.%20Yet%20the%20real%20world%20is%20dynamic%2C%20and%20quasi-static%20interactions%20are%20insufficient%20to%20cope%20with%20diverse%20environmental%20conditions.%20As%20a%20step%20toward%20more%20dynamic%20interaction%20scenarios%2C%20we%20present%20a%20reinforcement-learning-based%20training%20pipeline%20that%20produces%20a%20unified%20whole-body%20controller%20for%20humanoid%20badminton%2C%20enabling%20coordinated%20lower-body%20footwork%20and%20upper-body%20striking%20without%20motion%20priors%20or%20expert%20demonstrations.%20Training%20follows%20a%20three-stage%20curriculum%3A%20first%20footwork%20acquisition%2C%20then%20precision-guided%20racket%20swing%20generation%2C%20and%20finally%20task-focused%20refinement%2C%20yielding%20motions%20in%20which%20both%20legs%20and%20arms%20serve%20the%20hitting%20objective.%20For%20deployment%2C%20we%20incorporate%20an%20Extended%20Kalman%20Filter%20%28EKF%29%20to%20estimate%20and%20predict%20shuttlecock%20trajectories%20for%20target%20striking.%20We%20also%20introduce%20a%20prediction-free%20variant%20that%20dispenses%20with%20EKF%20and%20explicit%20trajectory%20prediction.%20To%20validate%20the%20framework%2C%20we%20conduct%20five%20sets%20of%20experiments%20in%20both%20simulation%20and%20the%20real%20world.%20In%20simulation%2C%20two%20robots%20sustain%20a%20rally%20of%2021%20consecutive%20hits.%20Moreover%2C%20the%20prediction-free%20variant%20achieves%20successful%20hits%20with%20comparable%20performance%20relative%20to%20the%20target-known%20policy.%20In%20real-world%20tests%2C%20both%20prediction%20and%20controller%20modules%20exhibit%20high%20accuracy%2C%20and%20on-court%20hitting%20achieves%20an%20outgoing%20shuttle%20speed%20up%20to%2019.1%20m/s%20with%20a%20mean%20return%20landing%20distance%20of%204%20m.%20These%20experimental%20results%20show%20that%20our%20proposed%20training%20scheme%20can%20deliver%20highly%20dynamic%20while%20precise%20goal%20striking%20in%20badminton%2C%20and%20can%20be%20adapted%20to%20more%20dynamics-critical%20domains.&entry.1838667208=http%3A//arxiv.org/abs/2511.11218v2&entry.124074799=Read"},
{"title": "High-Throughput Unsupervised Profiling of the Morphology of 316L Powder Particles for Use in Additive Manufacturing", "author": "Emmanuel Akeweje and Conall Kirk and Chi-Wai Chan and Denis Dowling and Mimi Zhang", "abstract": "Selective Laser Melting (SLM) is a powder-bed additive manufacturing technique whose part quality depends critically on feedstock morphology. However, conventional powder characterization methods are low-throughput and qualitative, failing to capture the heterogeneity of industrial-scale batches. We present an automated, machine learning framework that couples high-throughput imaging with shape extraction and clustering to profile metallic powder morphology at scale. We develop and evaluate three clustering pipelines: an autoencoder pipeline, a shape-descriptor pipeline, and a functional-data pipeline. Across a dataset of approximately 126,000 powder images (0.5-102 micrometer diameter), internal validity metrics identify the Fourier-descriptor + k-means pipeline as the most effective, achieving the lowest Davies-Bouldin index and highest Calinski-Harabasz score while maintaining sub-millisecond runtime per particle on a standard desktop workstation. Although the present work focuses on establishing the morphological-clustering framework, the resulting shape groups form a basis for future studies examining their relationship to flowability, packing density, and SLM part quality. Overall, this unsupervised learning framework enables rapid, automated assessment of powder morphology and supports tracking of shape evolution across reuse cycles, offering a path toward real-time feedstock monitoring in SLM workflows.", "link": "http://arxiv.org/abs/2512.06012v2", "date": "2025-12-09", "relevancy": 2.3209, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4972}, {"title": "GST: Precise 3D Human Body from a Single Image with Gaussian Splatting\n  Transformers", "link": "http://arxiv.org/abs/2409.04196v1", "similarity": 0.4489}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20High-Throughput%20Unsupervised%20Profiling%20of%20the%20Morphology%20of%20316L%20Powder%20Particles%20for%20Use%20in%20Additive%20Manufacturing&body=Title%3A%20High-Throughput%20Unsupervised%20Profiling%20of%20the%20Morphology%20of%20316L%20Powder%20Particles%20for%20Use%20in%20Additive%20Manufacturing%0AAuthor%3A%20Emmanuel%20Akeweje%20and%20Conall%20Kirk%20and%20Chi-Wai%20Chan%20and%20Denis%20Dowling%20and%20Mimi%20Zhang%0AAbstract%3A%20Selective%20Laser%20Melting%20%28SLM%29%20is%20a%20powder-bed%20additive%20manufacturing%20technique%20whose%20part%20quality%20depends%20critically%20on%20feedstock%20morphology.%20However%2C%20conventional%20powder%20characterization%20methods%20are%20low-throughput%20and%20qualitative%2C%20failing%20to%20capture%20the%20heterogeneity%20of%20industrial-scale%20batches.%20We%20present%20an%20automated%2C%20machine%20learning%20framework%20that%20couples%20high-throughput%20imaging%20with%20shape%20extraction%20and%20clustering%20to%20profile%20metallic%20powder%20morphology%20at%20scale.%20We%20develop%20and%20evaluate%20three%20clustering%20pipelines%3A%20an%20autoencoder%20pipeline%2C%20a%20shape-descriptor%20pipeline%2C%20and%20a%20functional-data%20pipeline.%20Across%20a%20dataset%20of%20approximately%20126%2C000%20powder%20images%20%280.5-102%20micrometer%20diameter%29%2C%20internal%20validity%20metrics%20identify%20the%20Fourier-descriptor%20%2B%20k-means%20pipeline%20as%20the%20most%20effective%2C%20achieving%20the%20lowest%20Davies-Bouldin%20index%20and%20highest%20Calinski-Harabasz%20score%20while%20maintaining%20sub-millisecond%20runtime%20per%20particle%20on%20a%20standard%20desktop%20workstation.%20Although%20the%20present%20work%20focuses%20on%20establishing%20the%20morphological-clustering%20framework%2C%20the%20resulting%20shape%20groups%20form%20a%20basis%20for%20future%20studies%20examining%20their%20relationship%20to%20flowability%2C%20packing%20density%2C%20and%20SLM%20part%20quality.%20Overall%2C%20this%20unsupervised%20learning%20framework%20enables%20rapid%2C%20automated%20assessment%20of%20powder%20morphology%20and%20supports%20tracking%20of%20shape%20evolution%20across%20reuse%20cycles%2C%20offering%20a%20path%20toward%20real-time%20feedstock%20monitoring%20in%20SLM%20workflows.%0ALink%3A%20http%3A//arxiv.org/abs/2512.06012v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DHigh-Throughput%2520Unsupervised%2520Profiling%2520of%2520the%2520Morphology%2520of%2520316L%2520Powder%2520Particles%2520for%2520Use%2520in%2520Additive%2520Manufacturing%26entry.906535625%3DEmmanuel%2520Akeweje%2520and%2520Conall%2520Kirk%2520and%2520Chi-Wai%2520Chan%2520and%2520Denis%2520Dowling%2520and%2520Mimi%2520Zhang%26entry.1292438233%3DSelective%2520Laser%2520Melting%2520%2528SLM%2529%2520is%2520a%2520powder-bed%2520additive%2520manufacturing%2520technique%2520whose%2520part%2520quality%2520depends%2520critically%2520on%2520feedstock%2520morphology.%2520However%252C%2520conventional%2520powder%2520characterization%2520methods%2520are%2520low-throughput%2520and%2520qualitative%252C%2520failing%2520to%2520capture%2520the%2520heterogeneity%2520of%2520industrial-scale%2520batches.%2520We%2520present%2520an%2520automated%252C%2520machine%2520learning%2520framework%2520that%2520couples%2520high-throughput%2520imaging%2520with%2520shape%2520extraction%2520and%2520clustering%2520to%2520profile%2520metallic%2520powder%2520morphology%2520at%2520scale.%2520We%2520develop%2520and%2520evaluate%2520three%2520clustering%2520pipelines%253A%2520an%2520autoencoder%2520pipeline%252C%2520a%2520shape-descriptor%2520pipeline%252C%2520and%2520a%2520functional-data%2520pipeline.%2520Across%2520a%2520dataset%2520of%2520approximately%2520126%252C000%2520powder%2520images%2520%25280.5-102%2520micrometer%2520diameter%2529%252C%2520internal%2520validity%2520metrics%2520identify%2520the%2520Fourier-descriptor%2520%252B%2520k-means%2520pipeline%2520as%2520the%2520most%2520effective%252C%2520achieving%2520the%2520lowest%2520Davies-Bouldin%2520index%2520and%2520highest%2520Calinski-Harabasz%2520score%2520while%2520maintaining%2520sub-millisecond%2520runtime%2520per%2520particle%2520on%2520a%2520standard%2520desktop%2520workstation.%2520Although%2520the%2520present%2520work%2520focuses%2520on%2520establishing%2520the%2520morphological-clustering%2520framework%252C%2520the%2520resulting%2520shape%2520groups%2520form%2520a%2520basis%2520for%2520future%2520studies%2520examining%2520their%2520relationship%2520to%2520flowability%252C%2520packing%2520density%252C%2520and%2520SLM%2520part%2520quality.%2520Overall%252C%2520this%2520unsupervised%2520learning%2520framework%2520enables%2520rapid%252C%2520automated%2520assessment%2520of%2520powder%2520morphology%2520and%2520supports%2520tracking%2520of%2520shape%2520evolution%2520across%2520reuse%2520cycles%252C%2520offering%2520a%2520path%2520toward%2520real-time%2520feedstock%2520monitoring%2520in%2520SLM%2520workflows.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.06012v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=High-Throughput%20Unsupervised%20Profiling%20of%20the%20Morphology%20of%20316L%20Powder%20Particles%20for%20Use%20in%20Additive%20Manufacturing&entry.906535625=Emmanuel%20Akeweje%20and%20Conall%20Kirk%20and%20Chi-Wai%20Chan%20and%20Denis%20Dowling%20and%20Mimi%20Zhang&entry.1292438233=Selective%20Laser%20Melting%20%28SLM%29%20is%20a%20powder-bed%20additive%20manufacturing%20technique%20whose%20part%20quality%20depends%20critically%20on%20feedstock%20morphology.%20However%2C%20conventional%20powder%20characterization%20methods%20are%20low-throughput%20and%20qualitative%2C%20failing%20to%20capture%20the%20heterogeneity%20of%20industrial-scale%20batches.%20We%20present%20an%20automated%2C%20machine%20learning%20framework%20that%20couples%20high-throughput%20imaging%20with%20shape%20extraction%20and%20clustering%20to%20profile%20metallic%20powder%20morphology%20at%20scale.%20We%20develop%20and%20evaluate%20three%20clustering%20pipelines%3A%20an%20autoencoder%20pipeline%2C%20a%20shape-descriptor%20pipeline%2C%20and%20a%20functional-data%20pipeline.%20Across%20a%20dataset%20of%20approximately%20126%2C000%20powder%20images%20%280.5-102%20micrometer%20diameter%29%2C%20internal%20validity%20metrics%20identify%20the%20Fourier-descriptor%20%2B%20k-means%20pipeline%20as%20the%20most%20effective%2C%20achieving%20the%20lowest%20Davies-Bouldin%20index%20and%20highest%20Calinski-Harabasz%20score%20while%20maintaining%20sub-millisecond%20runtime%20per%20particle%20on%20a%20standard%20desktop%20workstation.%20Although%20the%20present%20work%20focuses%20on%20establishing%20the%20morphological-clustering%20framework%2C%20the%20resulting%20shape%20groups%20form%20a%20basis%20for%20future%20studies%20examining%20their%20relationship%20to%20flowability%2C%20packing%20density%2C%20and%20SLM%20part%20quality.%20Overall%2C%20this%20unsupervised%20learning%20framework%20enables%20rapid%2C%20automated%20assessment%20of%20powder%20morphology%20and%20supports%20tracking%20of%20shape%20evolution%20across%20reuse%20cycles%2C%20offering%20a%20path%20toward%20real-time%20feedstock%20monitoring%20in%20SLM%20workflows.&entry.1838667208=http%3A//arxiv.org/abs/2512.06012v2&entry.124074799=Read"},
{"title": "No-Regret Strategy Solving in Imperfect-Information Games via Pre-Trained Embedding", "author": "Yanchang Fu and Shengda Liu and Pei Xu and Kaiqi Huang", "abstract": "High-quality information set abstraction remains a core challenge in solving large-scale imperfect-information extensive-form games (IIEFGs)--such as no-limit Texas Hold'em--where the finite nature of spatial resources hinders solving strategies for the full game. State-of-the-art AI methods rely on pre-trained discrete clustering for abstraction, yet their hard classification irreversibly discards critical information: specifically, the quantifiable subtle differences between information sets--vital for strategy solving--thus compromising the quality of such solving. Inspired by the word embedding paradigm in natural language processing, this paper proposes the Embedding CFR algorithm, a novel approach for solving strategies in IIEFGs within an embedding space. The algorithm pre-trains and embeds the features of individual information sets into an interconnected low-dimensional continuous space, where the resulting vectors more precisely capture both the distinctions and connections between information sets. Embedding CFR introduces a strategy-solving process driven by regret accumulation and strategy updates in this embedding space, with supporting theoretical analysis verifying its ability to reduce cumulative regret. Experiments on poker show that with the same spatial overhead, Embedding CFR achieves significantly faster exploitability convergence compared to cluster-based abstraction algorithms, confirming its effectiveness. Furthermore, to our knowledge, it is the first algorithm in poker AI that pre-trains information set abstractions via low-dimensional embedding for strategy solving.", "link": "http://arxiv.org/abs/2511.12083v2", "date": "2025-12-09", "relevancy": 2.3168, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4684}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4608}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20No-Regret%20Strategy%20Solving%20in%20Imperfect-Information%20Games%20via%20Pre-Trained%20Embedding&body=Title%3A%20No-Regret%20Strategy%20Solving%20in%20Imperfect-Information%20Games%20via%20Pre-Trained%20Embedding%0AAuthor%3A%20Yanchang%20Fu%20and%20Shengda%20Liu%20and%20Pei%20Xu%20and%20Kaiqi%20Huang%0AAbstract%3A%20High-quality%20information%20set%20abstraction%20remains%20a%20core%20challenge%20in%20solving%20large-scale%20imperfect-information%20extensive-form%20games%20%28IIEFGs%29--such%20as%20no-limit%20Texas%20Hold%27em--where%20the%20finite%20nature%20of%20spatial%20resources%20hinders%20solving%20strategies%20for%20the%20full%20game.%20State-of-the-art%20AI%20methods%20rely%20on%20pre-trained%20discrete%20clustering%20for%20abstraction%2C%20yet%20their%20hard%20classification%20irreversibly%20discards%20critical%20information%3A%20specifically%2C%20the%20quantifiable%20subtle%20differences%20between%20information%20sets--vital%20for%20strategy%20solving--thus%20compromising%20the%20quality%20of%20such%20solving.%20Inspired%20by%20the%20word%20embedding%20paradigm%20in%20natural%20language%20processing%2C%20this%20paper%20proposes%20the%20Embedding%20CFR%20algorithm%2C%20a%20novel%20approach%20for%20solving%20strategies%20in%20IIEFGs%20within%20an%20embedding%20space.%20The%20algorithm%20pre-trains%20and%20embeds%20the%20features%20of%20individual%20information%20sets%20into%20an%20interconnected%20low-dimensional%20continuous%20space%2C%20where%20the%20resulting%20vectors%20more%20precisely%20capture%20both%20the%20distinctions%20and%20connections%20between%20information%20sets.%20Embedding%20CFR%20introduces%20a%20strategy-solving%20process%20driven%20by%20regret%20accumulation%20and%20strategy%20updates%20in%20this%20embedding%20space%2C%20with%20supporting%20theoretical%20analysis%20verifying%20its%20ability%20to%20reduce%20cumulative%20regret.%20Experiments%20on%20poker%20show%20that%20with%20the%20same%20spatial%20overhead%2C%20Embedding%20CFR%20achieves%20significantly%20faster%20exploitability%20convergence%20compared%20to%20cluster-based%20abstraction%20algorithms%2C%20confirming%20its%20effectiveness.%20Furthermore%2C%20to%20our%20knowledge%2C%20it%20is%20the%20first%20algorithm%20in%20poker%20AI%20that%20pre-trains%20information%20set%20abstractions%20via%20low-dimensional%20embedding%20for%20strategy%20solving.%0ALink%3A%20http%3A//arxiv.org/abs/2511.12083v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNo-Regret%2520Strategy%2520Solving%2520in%2520Imperfect-Information%2520Games%2520via%2520Pre-Trained%2520Embedding%26entry.906535625%3DYanchang%2520Fu%2520and%2520Shengda%2520Liu%2520and%2520Pei%2520Xu%2520and%2520Kaiqi%2520Huang%26entry.1292438233%3DHigh-quality%2520information%2520set%2520abstraction%2520remains%2520a%2520core%2520challenge%2520in%2520solving%2520large-scale%2520imperfect-information%2520extensive-form%2520games%2520%2528IIEFGs%2529--such%2520as%2520no-limit%2520Texas%2520Hold%2527em--where%2520the%2520finite%2520nature%2520of%2520spatial%2520resources%2520hinders%2520solving%2520strategies%2520for%2520the%2520full%2520game.%2520State-of-the-art%2520AI%2520methods%2520rely%2520on%2520pre-trained%2520discrete%2520clustering%2520for%2520abstraction%252C%2520yet%2520their%2520hard%2520classification%2520irreversibly%2520discards%2520critical%2520information%253A%2520specifically%252C%2520the%2520quantifiable%2520subtle%2520differences%2520between%2520information%2520sets--vital%2520for%2520strategy%2520solving--thus%2520compromising%2520the%2520quality%2520of%2520such%2520solving.%2520Inspired%2520by%2520the%2520word%2520embedding%2520paradigm%2520in%2520natural%2520language%2520processing%252C%2520this%2520paper%2520proposes%2520the%2520Embedding%2520CFR%2520algorithm%252C%2520a%2520novel%2520approach%2520for%2520solving%2520strategies%2520in%2520IIEFGs%2520within%2520an%2520embedding%2520space.%2520The%2520algorithm%2520pre-trains%2520and%2520embeds%2520the%2520features%2520of%2520individual%2520information%2520sets%2520into%2520an%2520interconnected%2520low-dimensional%2520continuous%2520space%252C%2520where%2520the%2520resulting%2520vectors%2520more%2520precisely%2520capture%2520both%2520the%2520distinctions%2520and%2520connections%2520between%2520information%2520sets.%2520Embedding%2520CFR%2520introduces%2520a%2520strategy-solving%2520process%2520driven%2520by%2520regret%2520accumulation%2520and%2520strategy%2520updates%2520in%2520this%2520embedding%2520space%252C%2520with%2520supporting%2520theoretical%2520analysis%2520verifying%2520its%2520ability%2520to%2520reduce%2520cumulative%2520regret.%2520Experiments%2520on%2520poker%2520show%2520that%2520with%2520the%2520same%2520spatial%2520overhead%252C%2520Embedding%2520CFR%2520achieves%2520significantly%2520faster%2520exploitability%2520convergence%2520compared%2520to%2520cluster-based%2520abstraction%2520algorithms%252C%2520confirming%2520its%2520effectiveness.%2520Furthermore%252C%2520to%2520our%2520knowledge%252C%2520it%2520is%2520the%2520first%2520algorithm%2520in%2520poker%2520AI%2520that%2520pre-trains%2520information%2520set%2520abstractions%2520via%2520low-dimensional%2520embedding%2520for%2520strategy%2520solving.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.12083v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=No-Regret%20Strategy%20Solving%20in%20Imperfect-Information%20Games%20via%20Pre-Trained%20Embedding&entry.906535625=Yanchang%20Fu%20and%20Shengda%20Liu%20and%20Pei%20Xu%20and%20Kaiqi%20Huang&entry.1292438233=High-quality%20information%20set%20abstraction%20remains%20a%20core%20challenge%20in%20solving%20large-scale%20imperfect-information%20extensive-form%20games%20%28IIEFGs%29--such%20as%20no-limit%20Texas%20Hold%27em--where%20the%20finite%20nature%20of%20spatial%20resources%20hinders%20solving%20strategies%20for%20the%20full%20game.%20State-of-the-art%20AI%20methods%20rely%20on%20pre-trained%20discrete%20clustering%20for%20abstraction%2C%20yet%20their%20hard%20classification%20irreversibly%20discards%20critical%20information%3A%20specifically%2C%20the%20quantifiable%20subtle%20differences%20between%20information%20sets--vital%20for%20strategy%20solving--thus%20compromising%20the%20quality%20of%20such%20solving.%20Inspired%20by%20the%20word%20embedding%20paradigm%20in%20natural%20language%20processing%2C%20this%20paper%20proposes%20the%20Embedding%20CFR%20algorithm%2C%20a%20novel%20approach%20for%20solving%20strategies%20in%20IIEFGs%20within%20an%20embedding%20space.%20The%20algorithm%20pre-trains%20and%20embeds%20the%20features%20of%20individual%20information%20sets%20into%20an%20interconnected%20low-dimensional%20continuous%20space%2C%20where%20the%20resulting%20vectors%20more%20precisely%20capture%20both%20the%20distinctions%20and%20connections%20between%20information%20sets.%20Embedding%20CFR%20introduces%20a%20strategy-solving%20process%20driven%20by%20regret%20accumulation%20and%20strategy%20updates%20in%20this%20embedding%20space%2C%20with%20supporting%20theoretical%20analysis%20verifying%20its%20ability%20to%20reduce%20cumulative%20regret.%20Experiments%20on%20poker%20show%20that%20with%20the%20same%20spatial%20overhead%2C%20Embedding%20CFR%20achieves%20significantly%20faster%20exploitability%20convergence%20compared%20to%20cluster-based%20abstraction%20algorithms%2C%20confirming%20its%20effectiveness.%20Furthermore%2C%20to%20our%20knowledge%2C%20it%20is%20the%20first%20algorithm%20in%20poker%20AI%20that%20pre-trains%20information%20set%20abstractions%20via%20low-dimensional%20embedding%20for%20strategy%20solving.&entry.1838667208=http%3A//arxiv.org/abs/2511.12083v2&entry.124074799=Read"},
{"title": "Automatic Essay Scoring and Feedback Generation in Basque Language Learning", "author": "Ekhi Azurmendi and Xabier Arregi and Oier Lopez de Lacalle", "abstract": "This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.", "link": "http://arxiv.org/abs/2512.08713v1", "date": "2025-12-09", "relevancy": 2.2982, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4967}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.443}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4392}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Automatic%20Essay%20Scoring%20and%20Feedback%20Generation%20in%20Basque%20Language%20Learning&body=Title%3A%20Automatic%20Essay%20Scoring%20and%20Feedback%20Generation%20in%20Basque%20Language%20Learning%0AAuthor%3A%20Ekhi%20Azurmendi%20and%20Xabier%20Arregi%20and%20Oier%20Lopez%20de%20Lacalle%0AAbstract%3A%20This%20paper%20introduces%20the%20first%20publicly%20available%20dataset%20for%20Automatic%20Essay%20Scoring%20%28AES%29%20and%20feedback%20generation%20in%20Basque%2C%20targeting%20the%20CEFR%20C1%20proficiency%20level.%20The%20dataset%20comprises%203%2C200%20essays%20from%20HABE%2C%20each%20annotated%20by%20expert%20evaluators%20with%20criterion%20specific%20scores%20covering%20correctness%2C%20richness%2C%20coherence%2C%20cohesion%2C%20and%20task%20alignment%20enriched%20with%20detailed%20feedback%20and%20error%20examples.%20We%20fine-tune%20open-source%20models%2C%20including%20RoBERTa-EusCrawl%20and%20Latxa%208B/70B%2C%20for%20both%20scoring%20and%20explanation%20generation.%20Our%20experiments%20show%20that%20encoder%20models%20remain%20highly%20reliable%20for%20AES%2C%20while%20supervised%20fine-tuning%20%28SFT%29%20of%20Latxa%20significantly%20enhances%20performance%2C%20surpassing%20state-of-the-art%20%28SoTA%29%20closed-source%20systems%20such%20as%20GPT-5%20and%20Claude%20Sonnet%204.5%20in%20scoring%20consistency%20and%20feedback%20quality.%20We%20also%20propose%20a%20novel%20evaluation%20methodology%20for%20assessing%20feedback%20generation%2C%20combining%20automatic%20consistency%20metrics%20with%20expert-based%20validation%20of%20extracted%20learner%20errors.%20Results%20demonstrate%20that%20the%20fine-tuned%20Latxa%20model%20produces%20criterion-aligned%2C%20pedagogically%20meaningful%20feedback%20and%20identifies%20a%20wider%20range%20of%20error%20types%20than%20proprietary%20models.%20This%20resource%20and%20benchmark%20establish%20a%20foundation%20for%20transparent%2C%20reproducible%2C%20and%20educationally%20grounded%20NLP%20research%20in%20low-resource%20languages%20such%20as%20Basque.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08713v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAutomatic%2520Essay%2520Scoring%2520and%2520Feedback%2520Generation%2520in%2520Basque%2520Language%2520Learning%26entry.906535625%3DEkhi%2520Azurmendi%2520and%2520Xabier%2520Arregi%2520and%2520Oier%2520Lopez%2520de%2520Lacalle%26entry.1292438233%3DThis%2520paper%2520introduces%2520the%2520first%2520publicly%2520available%2520dataset%2520for%2520Automatic%2520Essay%2520Scoring%2520%2528AES%2529%2520and%2520feedback%2520generation%2520in%2520Basque%252C%2520targeting%2520the%2520CEFR%2520C1%2520proficiency%2520level.%2520The%2520dataset%2520comprises%25203%252C200%2520essays%2520from%2520HABE%252C%2520each%2520annotated%2520by%2520expert%2520evaluators%2520with%2520criterion%2520specific%2520scores%2520covering%2520correctness%252C%2520richness%252C%2520coherence%252C%2520cohesion%252C%2520and%2520task%2520alignment%2520enriched%2520with%2520detailed%2520feedback%2520and%2520error%2520examples.%2520We%2520fine-tune%2520open-source%2520models%252C%2520including%2520RoBERTa-EusCrawl%2520and%2520Latxa%25208B/70B%252C%2520for%2520both%2520scoring%2520and%2520explanation%2520generation.%2520Our%2520experiments%2520show%2520that%2520encoder%2520models%2520remain%2520highly%2520reliable%2520for%2520AES%252C%2520while%2520supervised%2520fine-tuning%2520%2528SFT%2529%2520of%2520Latxa%2520significantly%2520enhances%2520performance%252C%2520surpassing%2520state-of-the-art%2520%2528SoTA%2529%2520closed-source%2520systems%2520such%2520as%2520GPT-5%2520and%2520Claude%2520Sonnet%25204.5%2520in%2520scoring%2520consistency%2520and%2520feedback%2520quality.%2520We%2520also%2520propose%2520a%2520novel%2520evaluation%2520methodology%2520for%2520assessing%2520feedback%2520generation%252C%2520combining%2520automatic%2520consistency%2520metrics%2520with%2520expert-based%2520validation%2520of%2520extracted%2520learner%2520errors.%2520Results%2520demonstrate%2520that%2520the%2520fine-tuned%2520Latxa%2520model%2520produces%2520criterion-aligned%252C%2520pedagogically%2520meaningful%2520feedback%2520and%2520identifies%2520a%2520wider%2520range%2520of%2520error%2520types%2520than%2520proprietary%2520models.%2520This%2520resource%2520and%2520benchmark%2520establish%2520a%2520foundation%2520for%2520transparent%252C%2520reproducible%252C%2520and%2520educationally%2520grounded%2520NLP%2520research%2520in%2520low-resource%2520languages%2520such%2520as%2520Basque.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08713v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Automatic%20Essay%20Scoring%20and%20Feedback%20Generation%20in%20Basque%20Language%20Learning&entry.906535625=Ekhi%20Azurmendi%20and%20Xabier%20Arregi%20and%20Oier%20Lopez%20de%20Lacalle&entry.1292438233=This%20paper%20introduces%20the%20first%20publicly%20available%20dataset%20for%20Automatic%20Essay%20Scoring%20%28AES%29%20and%20feedback%20generation%20in%20Basque%2C%20targeting%20the%20CEFR%20C1%20proficiency%20level.%20The%20dataset%20comprises%203%2C200%20essays%20from%20HABE%2C%20each%20annotated%20by%20expert%20evaluators%20with%20criterion%20specific%20scores%20covering%20correctness%2C%20richness%2C%20coherence%2C%20cohesion%2C%20and%20task%20alignment%20enriched%20with%20detailed%20feedback%20and%20error%20examples.%20We%20fine-tune%20open-source%20models%2C%20including%20RoBERTa-EusCrawl%20and%20Latxa%208B/70B%2C%20for%20both%20scoring%20and%20explanation%20generation.%20Our%20experiments%20show%20that%20encoder%20models%20remain%20highly%20reliable%20for%20AES%2C%20while%20supervised%20fine-tuning%20%28SFT%29%20of%20Latxa%20significantly%20enhances%20performance%2C%20surpassing%20state-of-the-art%20%28SoTA%29%20closed-source%20systems%20such%20as%20GPT-5%20and%20Claude%20Sonnet%204.5%20in%20scoring%20consistency%20and%20feedback%20quality.%20We%20also%20propose%20a%20novel%20evaluation%20methodology%20for%20assessing%20feedback%20generation%2C%20combining%20automatic%20consistency%20metrics%20with%20expert-based%20validation%20of%20extracted%20learner%20errors.%20Results%20demonstrate%20that%20the%20fine-tuned%20Latxa%20model%20produces%20criterion-aligned%2C%20pedagogically%20meaningful%20feedback%20and%20identifies%20a%20wider%20range%20of%20error%20types%20than%20proprietary%20models.%20This%20resource%20and%20benchmark%20establish%20a%20foundation%20for%20transparent%2C%20reproducible%2C%20and%20educationally%20grounded%20NLP%20research%20in%20low-resource%20languages%20such%20as%20Basque.&entry.1838667208=http%3A//arxiv.org/abs/2512.08713v1&entry.124074799=Read"},
{"title": "End-to-End Fine-Tuning of 3D Texture Generation using Differentiable Rewards", "author": "AmirHossein Zamani and Tianhao Xie and Amir G. Aghdam and Tiberiu Popa and Eugene Belilovsky", "abstract": "While recent 3D generative models can produce high-quality texture images, they often fail to capture human preferences or meet task-specific requirements. Moreover, a core challenge in the 3D texture generation domain is that most existing approaches rely on repeated calls to 2D text-to-image generative models, which lack an inherent understanding of the 3D structure of the input 3D mesh object. To alleviate these issues, we propose an end-to-end differentiable, reinforcement-learning-free framework that embeds human feedback, expressed as differentiable reward functions, directly into the 3D texture synthesis pipeline. By back-propagating preference signals through both geometric and appearance modules of the proposed framework, our method generates textures that respect the 3D geometry structure and align with desired criteria. To demonstrate its versatility, we introduce three novel geometry-aware reward functions, which offer a more controllable and interpretable pathway for creating high-quality 3D content from natural language. By conducting qualitative, quantitative, and user-preference evaluations against state-of-the-art methods, we demonstrate that our proposed strategy consistently outperforms existing approaches. Our implementation code is publicly available at: https://github.com/AHHHZ975/Differentiable-Texture-Learning", "link": "http://arxiv.org/abs/2506.18331v4", "date": "2025-12-09", "relevancy": 2.2932, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5763}, {"title": "FabricDiffusion: High-Fidelity Texture Transfer for 3D Garments\n  Generation from In-The-Wild Clothing Images", "link": "http://arxiv.org/abs/2410.01801v1", "similarity": 0.5759}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5695}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-End%20Fine-Tuning%20of%203D%20Texture%20Generation%20using%20Differentiable%20Rewards&body=Title%3A%20End-to-End%20Fine-Tuning%20of%203D%20Texture%20Generation%20using%20Differentiable%20Rewards%0AAuthor%3A%20AmirHossein%20Zamani%20and%20Tianhao%20Xie%20and%20Amir%20G.%20Aghdam%20and%20Tiberiu%20Popa%20and%20Eugene%20Belilovsky%0AAbstract%3A%20While%20recent%203D%20generative%20models%20can%20produce%20high-quality%20texture%20images%2C%20they%20often%20fail%20to%20capture%20human%20preferences%20or%20meet%20task-specific%20requirements.%20Moreover%2C%20a%20core%20challenge%20in%20the%203D%20texture%20generation%20domain%20is%20that%20most%20existing%20approaches%20rely%20on%20repeated%20calls%20to%202D%20text-to-image%20generative%20models%2C%20which%20lack%20an%20inherent%20understanding%20of%20the%203D%20structure%20of%20the%20input%203D%20mesh%20object.%20To%20alleviate%20these%20issues%2C%20we%20propose%20an%20end-to-end%20differentiable%2C%20reinforcement-learning-free%20framework%20that%20embeds%20human%20feedback%2C%20expressed%20as%20differentiable%20reward%20functions%2C%20directly%20into%20the%203D%20texture%20synthesis%20pipeline.%20By%20back-propagating%20preference%20signals%20through%20both%20geometric%20and%20appearance%20modules%20of%20the%20proposed%20framework%2C%20our%20method%20generates%20textures%20that%20respect%20the%203D%20geometry%20structure%20and%20align%20with%20desired%20criteria.%20To%20demonstrate%20its%20versatility%2C%20we%20introduce%20three%20novel%20geometry-aware%20reward%20functions%2C%20which%20offer%20a%20more%20controllable%20and%20interpretable%20pathway%20for%20creating%20high-quality%203D%20content%20from%20natural%20language.%20By%20conducting%20qualitative%2C%20quantitative%2C%20and%20user-preference%20evaluations%20against%20state-of-the-art%20methods%2C%20we%20demonstrate%20that%20our%20proposed%20strategy%20consistently%20outperforms%20existing%20approaches.%20Our%20implementation%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/AHHHZ975/Differentiable-Texture-Learning%0ALink%3A%20http%3A//arxiv.org/abs/2506.18331v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-End%2520Fine-Tuning%2520of%25203D%2520Texture%2520Generation%2520using%2520Differentiable%2520Rewards%26entry.906535625%3DAmirHossein%2520Zamani%2520and%2520Tianhao%2520Xie%2520and%2520Amir%2520G.%2520Aghdam%2520and%2520Tiberiu%2520Popa%2520and%2520Eugene%2520Belilovsky%26entry.1292438233%3DWhile%2520recent%25203D%2520generative%2520models%2520can%2520produce%2520high-quality%2520texture%2520images%252C%2520they%2520often%2520fail%2520to%2520capture%2520human%2520preferences%2520or%2520meet%2520task-specific%2520requirements.%2520Moreover%252C%2520a%2520core%2520challenge%2520in%2520the%25203D%2520texture%2520generation%2520domain%2520is%2520that%2520most%2520existing%2520approaches%2520rely%2520on%2520repeated%2520calls%2520to%25202D%2520text-to-image%2520generative%2520models%252C%2520which%2520lack%2520an%2520inherent%2520understanding%2520of%2520the%25203D%2520structure%2520of%2520the%2520input%25203D%2520mesh%2520object.%2520To%2520alleviate%2520these%2520issues%252C%2520we%2520propose%2520an%2520end-to-end%2520differentiable%252C%2520reinforcement-learning-free%2520framework%2520that%2520embeds%2520human%2520feedback%252C%2520expressed%2520as%2520differentiable%2520reward%2520functions%252C%2520directly%2520into%2520the%25203D%2520texture%2520synthesis%2520pipeline.%2520By%2520back-propagating%2520preference%2520signals%2520through%2520both%2520geometric%2520and%2520appearance%2520modules%2520of%2520the%2520proposed%2520framework%252C%2520our%2520method%2520generates%2520textures%2520that%2520respect%2520the%25203D%2520geometry%2520structure%2520and%2520align%2520with%2520desired%2520criteria.%2520To%2520demonstrate%2520its%2520versatility%252C%2520we%2520introduce%2520three%2520novel%2520geometry-aware%2520reward%2520functions%252C%2520which%2520offer%2520a%2520more%2520controllable%2520and%2520interpretable%2520pathway%2520for%2520creating%2520high-quality%25203D%2520content%2520from%2520natural%2520language.%2520By%2520conducting%2520qualitative%252C%2520quantitative%252C%2520and%2520user-preference%2520evaluations%2520against%2520state-of-the-art%2520methods%252C%2520we%2520demonstrate%2520that%2520our%2520proposed%2520strategy%2520consistently%2520outperforms%2520existing%2520approaches.%2520Our%2520implementation%2520code%2520is%2520publicly%2520available%2520at%253A%2520https%253A//github.com/AHHHZ975/Differentiable-Texture-Learning%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.18331v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-End%20Fine-Tuning%20of%203D%20Texture%20Generation%20using%20Differentiable%20Rewards&entry.906535625=AmirHossein%20Zamani%20and%20Tianhao%20Xie%20and%20Amir%20G.%20Aghdam%20and%20Tiberiu%20Popa%20and%20Eugene%20Belilovsky&entry.1292438233=While%20recent%203D%20generative%20models%20can%20produce%20high-quality%20texture%20images%2C%20they%20often%20fail%20to%20capture%20human%20preferences%20or%20meet%20task-specific%20requirements.%20Moreover%2C%20a%20core%20challenge%20in%20the%203D%20texture%20generation%20domain%20is%20that%20most%20existing%20approaches%20rely%20on%20repeated%20calls%20to%202D%20text-to-image%20generative%20models%2C%20which%20lack%20an%20inherent%20understanding%20of%20the%203D%20structure%20of%20the%20input%203D%20mesh%20object.%20To%20alleviate%20these%20issues%2C%20we%20propose%20an%20end-to-end%20differentiable%2C%20reinforcement-learning-free%20framework%20that%20embeds%20human%20feedback%2C%20expressed%20as%20differentiable%20reward%20functions%2C%20directly%20into%20the%203D%20texture%20synthesis%20pipeline.%20By%20back-propagating%20preference%20signals%20through%20both%20geometric%20and%20appearance%20modules%20of%20the%20proposed%20framework%2C%20our%20method%20generates%20textures%20that%20respect%20the%203D%20geometry%20structure%20and%20align%20with%20desired%20criteria.%20To%20demonstrate%20its%20versatility%2C%20we%20introduce%20three%20novel%20geometry-aware%20reward%20functions%2C%20which%20offer%20a%20more%20controllable%20and%20interpretable%20pathway%20for%20creating%20high-quality%203D%20content%20from%20natural%20language.%20By%20conducting%20qualitative%2C%20quantitative%2C%20and%20user-preference%20evaluations%20against%20state-of-the-art%20methods%2C%20we%20demonstrate%20that%20our%20proposed%20strategy%20consistently%20outperforms%20existing%20approaches.%20Our%20implementation%20code%20is%20publicly%20available%20at%3A%20https%3A//github.com/AHHHZ975/Differentiable-Texture-Learning&entry.1838667208=http%3A//arxiv.org/abs/2506.18331v4&entry.124074799=Read"},
{"title": "Wukong's 72 Transformations: High-fidelity Textured 3D Morphing via Flow Models", "author": "Minghao Yin and Yukang Cao and Kai Han", "abstract": "We present WUKONG, a novel training-free framework for high-fidelity textured 3D morphing that takes a pair of source and target prompts (image or text) as input. Unlike conventional methods -- which rely on manual correspondence matching and deformation trajectory estimation (limiting generalization and requiring costly preprocessing) -- WUKONG leverages the generative prior of flow-based transformers to produce high-fidelity 3D transitions with rich texture details. To ensure smooth shape transitions, we exploit the inherent continuity of flow-based generative processes and formulate morphing as an optimal transport barycenter problem. We further introduce a sequential initialization strategy to prevent abrupt geometric distortions and preserve identity coherence. For faithful texture preservation, we propose a similarity-guided semantic consistency mechanism that selectively retains high-frequency details and enables precise control over blending dynamics. This avoids common artifacts like oversmoothing while maintaining semantic fidelity. Extensive quantitative and qualitative evaluations demonstrate that WUKONG significantly outperforms state-of-the-art methods, achieving superior results across diverse geometry and texture variations.", "link": "http://arxiv.org/abs/2511.22425v2", "date": "2025-12-09", "relevancy": 2.29, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.6312}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.581}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5406}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wukong%27s%2072%20Transformations%3A%20High-fidelity%20Textured%203D%20Morphing%20via%20Flow%20Models&body=Title%3A%20Wukong%27s%2072%20Transformations%3A%20High-fidelity%20Textured%203D%20Morphing%20via%20Flow%20Models%0AAuthor%3A%20Minghao%20Yin%20and%20Yukang%20Cao%20and%20Kai%20Han%0AAbstract%3A%20We%20present%20WUKONG%2C%20a%20novel%20training-free%20framework%20for%20high-fidelity%20textured%203D%20morphing%20that%20takes%20a%20pair%20of%20source%20and%20target%20prompts%20%28image%20or%20text%29%20as%20input.%20Unlike%20conventional%20methods%20--%20which%20rely%20on%20manual%20correspondence%20matching%20and%20deformation%20trajectory%20estimation%20%28limiting%20generalization%20and%20requiring%20costly%20preprocessing%29%20--%20WUKONG%20leverages%20the%20generative%20prior%20of%20flow-based%20transformers%20to%20produce%20high-fidelity%203D%20transitions%20with%20rich%20texture%20details.%20To%20ensure%20smooth%20shape%20transitions%2C%20we%20exploit%20the%20inherent%20continuity%20of%20flow-based%20generative%20processes%20and%20formulate%20morphing%20as%20an%20optimal%20transport%20barycenter%20problem.%20We%20further%20introduce%20a%20sequential%20initialization%20strategy%20to%20prevent%20abrupt%20geometric%20distortions%20and%20preserve%20identity%20coherence.%20For%20faithful%20texture%20preservation%2C%20we%20propose%20a%20similarity-guided%20semantic%20consistency%20mechanism%20that%20selectively%20retains%20high-frequency%20details%20and%20enables%20precise%20control%20over%20blending%20dynamics.%20This%20avoids%20common%20artifacts%20like%20oversmoothing%20while%20maintaining%20semantic%20fidelity.%20Extensive%20quantitative%20and%20qualitative%20evaluations%20demonstrate%20that%20WUKONG%20significantly%20outperforms%20state-of-the-art%20methods%2C%20achieving%20superior%20results%20across%20diverse%20geometry%20and%20texture%20variations.%0ALink%3A%20http%3A//arxiv.org/abs/2511.22425v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWukong%2527s%252072%2520Transformations%253A%2520High-fidelity%2520Textured%25203D%2520Morphing%2520via%2520Flow%2520Models%26entry.906535625%3DMinghao%2520Yin%2520and%2520Yukang%2520Cao%2520and%2520Kai%2520Han%26entry.1292438233%3DWe%2520present%2520WUKONG%252C%2520a%2520novel%2520training-free%2520framework%2520for%2520high-fidelity%2520textured%25203D%2520morphing%2520that%2520takes%2520a%2520pair%2520of%2520source%2520and%2520target%2520prompts%2520%2528image%2520or%2520text%2529%2520as%2520input.%2520Unlike%2520conventional%2520methods%2520--%2520which%2520rely%2520on%2520manual%2520correspondence%2520matching%2520and%2520deformation%2520trajectory%2520estimation%2520%2528limiting%2520generalization%2520and%2520requiring%2520costly%2520preprocessing%2529%2520--%2520WUKONG%2520leverages%2520the%2520generative%2520prior%2520of%2520flow-based%2520transformers%2520to%2520produce%2520high-fidelity%25203D%2520transitions%2520with%2520rich%2520texture%2520details.%2520To%2520ensure%2520smooth%2520shape%2520transitions%252C%2520we%2520exploit%2520the%2520inherent%2520continuity%2520of%2520flow-based%2520generative%2520processes%2520and%2520formulate%2520morphing%2520as%2520an%2520optimal%2520transport%2520barycenter%2520problem.%2520We%2520further%2520introduce%2520a%2520sequential%2520initialization%2520strategy%2520to%2520prevent%2520abrupt%2520geometric%2520distortions%2520and%2520preserve%2520identity%2520coherence.%2520For%2520faithful%2520texture%2520preservation%252C%2520we%2520propose%2520a%2520similarity-guided%2520semantic%2520consistency%2520mechanism%2520that%2520selectively%2520retains%2520high-frequency%2520details%2520and%2520enables%2520precise%2520control%2520over%2520blending%2520dynamics.%2520This%2520avoids%2520common%2520artifacts%2520like%2520oversmoothing%2520while%2520maintaining%2520semantic%2520fidelity.%2520Extensive%2520quantitative%2520and%2520qualitative%2520evaluations%2520demonstrate%2520that%2520WUKONG%2520significantly%2520outperforms%2520state-of-the-art%2520methods%252C%2520achieving%2520superior%2520results%2520across%2520diverse%2520geometry%2520and%2520texture%2520variations.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.22425v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wukong%27s%2072%20Transformations%3A%20High-fidelity%20Textured%203D%20Morphing%20via%20Flow%20Models&entry.906535625=Minghao%20Yin%20and%20Yukang%20Cao%20and%20Kai%20Han&entry.1292438233=We%20present%20WUKONG%2C%20a%20novel%20training-free%20framework%20for%20high-fidelity%20textured%203D%20morphing%20that%20takes%20a%20pair%20of%20source%20and%20target%20prompts%20%28image%20or%20text%29%20as%20input.%20Unlike%20conventional%20methods%20--%20which%20rely%20on%20manual%20correspondence%20matching%20and%20deformation%20trajectory%20estimation%20%28limiting%20generalization%20and%20requiring%20costly%20preprocessing%29%20--%20WUKONG%20leverages%20the%20generative%20prior%20of%20flow-based%20transformers%20to%20produce%20high-fidelity%203D%20transitions%20with%20rich%20texture%20details.%20To%20ensure%20smooth%20shape%20transitions%2C%20we%20exploit%20the%20inherent%20continuity%20of%20flow-based%20generative%20processes%20and%20formulate%20morphing%20as%20an%20optimal%20transport%20barycenter%20problem.%20We%20further%20introduce%20a%20sequential%20initialization%20strategy%20to%20prevent%20abrupt%20geometric%20distortions%20and%20preserve%20identity%20coherence.%20For%20faithful%20texture%20preservation%2C%20we%20propose%20a%20similarity-guided%20semantic%20consistency%20mechanism%20that%20selectively%20retains%20high-frequency%20details%20and%20enables%20precise%20control%20over%20blending%20dynamics.%20This%20avoids%20common%20artifacts%20like%20oversmoothing%20while%20maintaining%20semantic%20fidelity.%20Extensive%20quantitative%20and%20qualitative%20evaluations%20demonstrate%20that%20WUKONG%20significantly%20outperforms%20state-of-the-art%20methods%2C%20achieving%20superior%20results%20across%20diverse%20geometry%20and%20texture%20variations.&entry.1838667208=http%3A//arxiv.org/abs/2511.22425v2&entry.124074799=Read"},
{"title": "Data-Driven Dynamic Parameter Learning of manipulator robots", "author": "Mohammed Elseiagy and Tsige Tadesse Alemayoh and Ranulfo Bezerra and Shotaro Kojima and Kazunori Ohno", "abstract": "Bridging the sim-to-real gap remains a fundamental challenge in robotics, as accurate dynamic parameter estimation is essential for reliable model-based control, realistic simulation, and safe deployment of manipulators. Traditional analytical approaches often fall short when faced with complex robot structures and interactions. Data-driven methods offer a promising alternative, yet conventional neural networks such as recurrent models struggle to capture long-range dependencies critical for accurate estimation. In this study, we propose a Transformer-based approach for dynamic parameter estimation, supported by an automated pipeline that generates diverse robot models and enriched trajectory data using Jacobian-derived features. The dataset consists of 8,192 robots with varied inertial and frictional properties. Leveraging attention mechanisms, our model effectively captures both temporal and spatial dependencies. Experimental results highlight the influence of sequence length, sampling rate, and architecture, with the best configuration (sequence length 64, 64 Hz, four layers, 32 heads) achieving a validation R2 of 0.8633. Mass and inertia are estimated with near-perfect accuracy, Coulomb friction with moderate-to-high accuracy, while viscous friction and distal link center-of-mass remain more challenging. These results demonstrate that combining Transformers with automated dataset generation and kinematic enrichment enables scalable, accurate dynamic parameter estimation, contributing to improved sim-to-real transfer in robotic systems", "link": "http://arxiv.org/abs/2512.08767v1", "date": "2025-12-09", "relevancy": 2.2781, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6302}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5676}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5472}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data-Driven%20Dynamic%20Parameter%20Learning%20of%20manipulator%20robots&body=Title%3A%20Data-Driven%20Dynamic%20Parameter%20Learning%20of%20manipulator%20robots%0AAuthor%3A%20Mohammed%20Elseiagy%20and%20Tsige%20Tadesse%20Alemayoh%20and%20Ranulfo%20Bezerra%20and%20Shotaro%20Kojima%20and%20Kazunori%20Ohno%0AAbstract%3A%20Bridging%20the%20sim-to-real%20gap%20remains%20a%20fundamental%20challenge%20in%20robotics%2C%20as%20accurate%20dynamic%20parameter%20estimation%20is%20essential%20for%20reliable%20model-based%20control%2C%20realistic%20simulation%2C%20and%20safe%20deployment%20of%20manipulators.%20Traditional%20analytical%20approaches%20often%20fall%20short%20when%20faced%20with%20complex%20robot%20structures%20and%20interactions.%20Data-driven%20methods%20offer%20a%20promising%20alternative%2C%20yet%20conventional%20neural%20networks%20such%20as%20recurrent%20models%20struggle%20to%20capture%20long-range%20dependencies%20critical%20for%20accurate%20estimation.%20In%20this%20study%2C%20we%20propose%20a%20Transformer-based%20approach%20for%20dynamic%20parameter%20estimation%2C%20supported%20by%20an%20automated%20pipeline%20that%20generates%20diverse%20robot%20models%20and%20enriched%20trajectory%20data%20using%20Jacobian-derived%20features.%20The%20dataset%20consists%20of%208%2C192%20robots%20with%20varied%20inertial%20and%20frictional%20properties.%20Leveraging%20attention%20mechanisms%2C%20our%20model%20effectively%20captures%20both%20temporal%20and%20spatial%20dependencies.%20Experimental%20results%20highlight%20the%20influence%20of%20sequence%20length%2C%20sampling%20rate%2C%20and%20architecture%2C%20with%20the%20best%20configuration%20%28sequence%20length%2064%2C%2064%20Hz%2C%20four%20layers%2C%2032%20heads%29%20achieving%20a%20validation%20R2%20of%200.8633.%20Mass%20and%20inertia%20are%20estimated%20with%20near-perfect%20accuracy%2C%20Coulomb%20friction%20with%20moderate-to-high%20accuracy%2C%20while%20viscous%20friction%20and%20distal%20link%20center-of-mass%20remain%20more%20challenging.%20These%20results%20demonstrate%20that%20combining%20Transformers%20with%20automated%20dataset%20generation%20and%20kinematic%20enrichment%20enables%20scalable%2C%20accurate%20dynamic%20parameter%20estimation%2C%20contributing%20to%20improved%20sim-to-real%20transfer%20in%20robotic%20systems%0ALink%3A%20http%3A//arxiv.org/abs/2512.08767v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData-Driven%2520Dynamic%2520Parameter%2520Learning%2520of%2520manipulator%2520robots%26entry.906535625%3DMohammed%2520Elseiagy%2520and%2520Tsige%2520Tadesse%2520Alemayoh%2520and%2520Ranulfo%2520Bezerra%2520and%2520Shotaro%2520Kojima%2520and%2520Kazunori%2520Ohno%26entry.1292438233%3DBridging%2520the%2520sim-to-real%2520gap%2520remains%2520a%2520fundamental%2520challenge%2520in%2520robotics%252C%2520as%2520accurate%2520dynamic%2520parameter%2520estimation%2520is%2520essential%2520for%2520reliable%2520model-based%2520control%252C%2520realistic%2520simulation%252C%2520and%2520safe%2520deployment%2520of%2520manipulators.%2520Traditional%2520analytical%2520approaches%2520often%2520fall%2520short%2520when%2520faced%2520with%2520complex%2520robot%2520structures%2520and%2520interactions.%2520Data-driven%2520methods%2520offer%2520a%2520promising%2520alternative%252C%2520yet%2520conventional%2520neural%2520networks%2520such%2520as%2520recurrent%2520models%2520struggle%2520to%2520capture%2520long-range%2520dependencies%2520critical%2520for%2520accurate%2520estimation.%2520In%2520this%2520study%252C%2520we%2520propose%2520a%2520Transformer-based%2520approach%2520for%2520dynamic%2520parameter%2520estimation%252C%2520supported%2520by%2520an%2520automated%2520pipeline%2520that%2520generates%2520diverse%2520robot%2520models%2520and%2520enriched%2520trajectory%2520data%2520using%2520Jacobian-derived%2520features.%2520The%2520dataset%2520consists%2520of%25208%252C192%2520robots%2520with%2520varied%2520inertial%2520and%2520frictional%2520properties.%2520Leveraging%2520attention%2520mechanisms%252C%2520our%2520model%2520effectively%2520captures%2520both%2520temporal%2520and%2520spatial%2520dependencies.%2520Experimental%2520results%2520highlight%2520the%2520influence%2520of%2520sequence%2520length%252C%2520sampling%2520rate%252C%2520and%2520architecture%252C%2520with%2520the%2520best%2520configuration%2520%2528sequence%2520length%252064%252C%252064%2520Hz%252C%2520four%2520layers%252C%252032%2520heads%2529%2520achieving%2520a%2520validation%2520R2%2520of%25200.8633.%2520Mass%2520and%2520inertia%2520are%2520estimated%2520with%2520near-perfect%2520accuracy%252C%2520Coulomb%2520friction%2520with%2520moderate-to-high%2520accuracy%252C%2520while%2520viscous%2520friction%2520and%2520distal%2520link%2520center-of-mass%2520remain%2520more%2520challenging.%2520These%2520results%2520demonstrate%2520that%2520combining%2520Transformers%2520with%2520automated%2520dataset%2520generation%2520and%2520kinematic%2520enrichment%2520enables%2520scalable%252C%2520accurate%2520dynamic%2520parameter%2520estimation%252C%2520contributing%2520to%2520improved%2520sim-to-real%2520transfer%2520in%2520robotic%2520systems%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08767v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data-Driven%20Dynamic%20Parameter%20Learning%20of%20manipulator%20robots&entry.906535625=Mohammed%20Elseiagy%20and%20Tsige%20Tadesse%20Alemayoh%20and%20Ranulfo%20Bezerra%20and%20Shotaro%20Kojima%20and%20Kazunori%20Ohno&entry.1292438233=Bridging%20the%20sim-to-real%20gap%20remains%20a%20fundamental%20challenge%20in%20robotics%2C%20as%20accurate%20dynamic%20parameter%20estimation%20is%20essential%20for%20reliable%20model-based%20control%2C%20realistic%20simulation%2C%20and%20safe%20deployment%20of%20manipulators.%20Traditional%20analytical%20approaches%20often%20fall%20short%20when%20faced%20with%20complex%20robot%20structures%20and%20interactions.%20Data-driven%20methods%20offer%20a%20promising%20alternative%2C%20yet%20conventional%20neural%20networks%20such%20as%20recurrent%20models%20struggle%20to%20capture%20long-range%20dependencies%20critical%20for%20accurate%20estimation.%20In%20this%20study%2C%20we%20propose%20a%20Transformer-based%20approach%20for%20dynamic%20parameter%20estimation%2C%20supported%20by%20an%20automated%20pipeline%20that%20generates%20diverse%20robot%20models%20and%20enriched%20trajectory%20data%20using%20Jacobian-derived%20features.%20The%20dataset%20consists%20of%208%2C192%20robots%20with%20varied%20inertial%20and%20frictional%20properties.%20Leveraging%20attention%20mechanisms%2C%20our%20model%20effectively%20captures%20both%20temporal%20and%20spatial%20dependencies.%20Experimental%20results%20highlight%20the%20influence%20of%20sequence%20length%2C%20sampling%20rate%2C%20and%20architecture%2C%20with%20the%20best%20configuration%20%28sequence%20length%2064%2C%2064%20Hz%2C%20four%20layers%2C%2032%20heads%29%20achieving%20a%20validation%20R2%20of%200.8633.%20Mass%20and%20inertia%20are%20estimated%20with%20near-perfect%20accuracy%2C%20Coulomb%20friction%20with%20moderate-to-high%20accuracy%2C%20while%20viscous%20friction%20and%20distal%20link%20center-of-mass%20remain%20more%20challenging.%20These%20results%20demonstrate%20that%20combining%20Transformers%20with%20automated%20dataset%20generation%20and%20kinematic%20enrichment%20enables%20scalable%2C%20accurate%20dynamic%20parameter%20estimation%2C%20contributing%20to%20improved%20sim-to-real%20transfer%20in%20robotic%20systems&entry.1838667208=http%3A//arxiv.org/abs/2512.08767v1&entry.124074799=Read"},
{"title": "PET Image Reconstruction Using Deep Diffusion Image Prior", "author": "Fumio Hashimoto and Kuang Gong", "abstract": "Diffusion models have shown great promise in medical image denoising and reconstruction, but their application to Positron Emission Tomography (PET) imaging remains limited by tracer-specific contrast variability and high computational demands. In this work, we proposed an anatomical prior-guided PET image reconstruction method based on diffusion models, inspired by the deep diffusion image prior (DDIP) framework. The proposed method alternated between diffusion sampling and model fine-tuning guided by the PET sinogram, enabling the reconstruction of high-quality images from various PET tracers using a score function pretrained on a dataset of another tracer. To improve computational efficiency, the half-quadratic splitting (HQS) algorithm was adopted to decouple network optimization from iterative PET reconstruction. The proposed method was evaluated using one simulation and two clinical datasets. For the simulation study, a model pretrained on [$^{18}$F]FDG data was tested on [$^{18}$F]FDG data and amyloid-negative PET data to assess out-of-distribution (OOD) performance. For the clinical-data validation, ten low-dose [$^{18}$F]FDG datasets and one [$^{18}$F]Florbetapir dataset were tested on a model pretrained on data from another tracer. Experiment results show that the proposed PET reconstruction method can generalize robustly across tracer distributions and scanner types, providing an efficient and versatile reconstruction framework for low-dose PET imaging.", "link": "http://arxiv.org/abs/2507.15078v2", "date": "2025-12-09", "relevancy": 2.2781, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5867}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5661}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PET%20Image%20Reconstruction%20Using%20Deep%20Diffusion%20Image%20Prior&body=Title%3A%20PET%20Image%20Reconstruction%20Using%20Deep%20Diffusion%20Image%20Prior%0AAuthor%3A%20Fumio%20Hashimoto%20and%20Kuang%20Gong%0AAbstract%3A%20Diffusion%20models%20have%20shown%20great%20promise%20in%20medical%20image%20denoising%20and%20reconstruction%2C%20but%20their%20application%20to%20Positron%20Emission%20Tomography%20%28PET%29%20imaging%20remains%20limited%20by%20tracer-specific%20contrast%20variability%20and%20high%20computational%20demands.%20In%20this%20work%2C%20we%20proposed%20an%20anatomical%20prior-guided%20PET%20image%20reconstruction%20method%20based%20on%20diffusion%20models%2C%20inspired%20by%20the%20deep%20diffusion%20image%20prior%20%28DDIP%29%20framework.%20The%20proposed%20method%20alternated%20between%20diffusion%20sampling%20and%20model%20fine-tuning%20guided%20by%20the%20PET%20sinogram%2C%20enabling%20the%20reconstruction%20of%20high-quality%20images%20from%20various%20PET%20tracers%20using%20a%20score%20function%20pretrained%20on%20a%20dataset%20of%20another%20tracer.%20To%20improve%20computational%20efficiency%2C%20the%20half-quadratic%20splitting%20%28HQS%29%20algorithm%20was%20adopted%20to%20decouple%20network%20optimization%20from%20iterative%20PET%20reconstruction.%20The%20proposed%20method%20was%20evaluated%20using%20one%20simulation%20and%20two%20clinical%20datasets.%20For%20the%20simulation%20study%2C%20a%20model%20pretrained%20on%20%5B%24%5E%7B18%7D%24F%5DFDG%20data%20was%20tested%20on%20%5B%24%5E%7B18%7D%24F%5DFDG%20data%20and%20amyloid-negative%20PET%20data%20to%20assess%20out-of-distribution%20%28OOD%29%20performance.%20For%20the%20clinical-data%20validation%2C%20ten%20low-dose%20%5B%24%5E%7B18%7D%24F%5DFDG%20datasets%20and%20one%20%5B%24%5E%7B18%7D%24F%5DFlorbetapir%20dataset%20were%20tested%20on%20a%20model%20pretrained%20on%20data%20from%20another%20tracer.%20Experiment%20results%20show%20that%20the%20proposed%20PET%20reconstruction%20method%20can%20generalize%20robustly%20across%20tracer%20distributions%20and%20scanner%20types%2C%20providing%20an%20efficient%20and%20versatile%20reconstruction%20framework%20for%20low-dose%20PET%20imaging.%0ALink%3A%20http%3A//arxiv.org/abs/2507.15078v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPET%2520Image%2520Reconstruction%2520Using%2520Deep%2520Diffusion%2520Image%2520Prior%26entry.906535625%3DFumio%2520Hashimoto%2520and%2520Kuang%2520Gong%26entry.1292438233%3DDiffusion%2520models%2520have%2520shown%2520great%2520promise%2520in%2520medical%2520image%2520denoising%2520and%2520reconstruction%252C%2520but%2520their%2520application%2520to%2520Positron%2520Emission%2520Tomography%2520%2528PET%2529%2520imaging%2520remains%2520limited%2520by%2520tracer-specific%2520contrast%2520variability%2520and%2520high%2520computational%2520demands.%2520In%2520this%2520work%252C%2520we%2520proposed%2520an%2520anatomical%2520prior-guided%2520PET%2520image%2520reconstruction%2520method%2520based%2520on%2520diffusion%2520models%252C%2520inspired%2520by%2520the%2520deep%2520diffusion%2520image%2520prior%2520%2528DDIP%2529%2520framework.%2520The%2520proposed%2520method%2520alternated%2520between%2520diffusion%2520sampling%2520and%2520model%2520fine-tuning%2520guided%2520by%2520the%2520PET%2520sinogram%252C%2520enabling%2520the%2520reconstruction%2520of%2520high-quality%2520images%2520from%2520various%2520PET%2520tracers%2520using%2520a%2520score%2520function%2520pretrained%2520on%2520a%2520dataset%2520of%2520another%2520tracer.%2520To%2520improve%2520computational%2520efficiency%252C%2520the%2520half-quadratic%2520splitting%2520%2528HQS%2529%2520algorithm%2520was%2520adopted%2520to%2520decouple%2520network%2520optimization%2520from%2520iterative%2520PET%2520reconstruction.%2520The%2520proposed%2520method%2520was%2520evaluated%2520using%2520one%2520simulation%2520and%2520two%2520clinical%2520datasets.%2520For%2520the%2520simulation%2520study%252C%2520a%2520model%2520pretrained%2520on%2520%255B%2524%255E%257B18%257D%2524F%255DFDG%2520data%2520was%2520tested%2520on%2520%255B%2524%255E%257B18%257D%2524F%255DFDG%2520data%2520and%2520amyloid-negative%2520PET%2520data%2520to%2520assess%2520out-of-distribution%2520%2528OOD%2529%2520performance.%2520For%2520the%2520clinical-data%2520validation%252C%2520ten%2520low-dose%2520%255B%2524%255E%257B18%257D%2524F%255DFDG%2520datasets%2520and%2520one%2520%255B%2524%255E%257B18%257D%2524F%255DFlorbetapir%2520dataset%2520were%2520tested%2520on%2520a%2520model%2520pretrained%2520on%2520data%2520from%2520another%2520tracer.%2520Experiment%2520results%2520show%2520that%2520the%2520proposed%2520PET%2520reconstruction%2520method%2520can%2520generalize%2520robustly%2520across%2520tracer%2520distributions%2520and%2520scanner%2520types%252C%2520providing%2520an%2520efficient%2520and%2520versatile%2520reconstruction%2520framework%2520for%2520low-dose%2520PET%2520imaging.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.15078v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PET%20Image%20Reconstruction%20Using%20Deep%20Diffusion%20Image%20Prior&entry.906535625=Fumio%20Hashimoto%20and%20Kuang%20Gong&entry.1292438233=Diffusion%20models%20have%20shown%20great%20promise%20in%20medical%20image%20denoising%20and%20reconstruction%2C%20but%20their%20application%20to%20Positron%20Emission%20Tomography%20%28PET%29%20imaging%20remains%20limited%20by%20tracer-specific%20contrast%20variability%20and%20high%20computational%20demands.%20In%20this%20work%2C%20we%20proposed%20an%20anatomical%20prior-guided%20PET%20image%20reconstruction%20method%20based%20on%20diffusion%20models%2C%20inspired%20by%20the%20deep%20diffusion%20image%20prior%20%28DDIP%29%20framework.%20The%20proposed%20method%20alternated%20between%20diffusion%20sampling%20and%20model%20fine-tuning%20guided%20by%20the%20PET%20sinogram%2C%20enabling%20the%20reconstruction%20of%20high-quality%20images%20from%20various%20PET%20tracers%20using%20a%20score%20function%20pretrained%20on%20a%20dataset%20of%20another%20tracer.%20To%20improve%20computational%20efficiency%2C%20the%20half-quadratic%20splitting%20%28HQS%29%20algorithm%20was%20adopted%20to%20decouple%20network%20optimization%20from%20iterative%20PET%20reconstruction.%20The%20proposed%20method%20was%20evaluated%20using%20one%20simulation%20and%20two%20clinical%20datasets.%20For%20the%20simulation%20study%2C%20a%20model%20pretrained%20on%20%5B%24%5E%7B18%7D%24F%5DFDG%20data%20was%20tested%20on%20%5B%24%5E%7B18%7D%24F%5DFDG%20data%20and%20amyloid-negative%20PET%20data%20to%20assess%20out-of-distribution%20%28OOD%29%20performance.%20For%20the%20clinical-data%20validation%2C%20ten%20low-dose%20%5B%24%5E%7B18%7D%24F%5DFDG%20datasets%20and%20one%20%5B%24%5E%7B18%7D%24F%5DFlorbetapir%20dataset%20were%20tested%20on%20a%20model%20pretrained%20on%20data%20from%20another%20tracer.%20Experiment%20results%20show%20that%20the%20proposed%20PET%20reconstruction%20method%20can%20generalize%20robustly%20across%20tracer%20distributions%20and%20scanner%20types%2C%20providing%20an%20efficient%20and%20versatile%20reconstruction%20framework%20for%20low-dose%20PET%20imaging.&entry.1838667208=http%3A//arxiv.org/abs/2507.15078v2&entry.124074799=Read"},
{"title": "A Sensor-Aware Phenomenological Framework for Lidar Degradation Simulation and SLAM Robustness Evaluation", "author": "Doumegna Mawuto Koudjo Felix and Xianjia Yu and Zhuo Zou and Tomi Westerlund", "abstract": "Lidar-based SLAM systems are highly sensitive to adverse conditions such as occlusion, noise, and field-of-view (FoV) degradation, yet existing robustness evaluation methods either lack physical grounding or do not capture sensor-specific behavior. This paper presents a sensor-aware, phenomenological framework for simulating interpretable lidar degradations directly on real point clouds, enabling controlled and reproducible SLAM stress testing. Unlike image-derived corruption benchmarks (e.g., SemanticKITTI-C) or simulation-only approaches (e.g., lidarsim), the proposed system preserves per-point geometry, intensity, and temporal structure while applying structured dropout, FoV reduction, Gaussian noise, occlusion masking, sparsification, and motion distortion. The framework features autonomous topic and sensor detection, modular configuration with four severity tiers (light--extreme), and real-time performance (less than 20 ms per frame) compatible with ROS workflows. Experimental validation across three lidar architectures and five state-of-the-art SLAM systems reveals distinct robustness patterns shaped by sensor design and environmental context. The open-source implementation provides a practical foundation for benchmarking lidar-based SLAM under physically meaningful degradation scenarios.", "link": "http://arxiv.org/abs/2512.08653v1", "date": "2025-12-09", "relevancy": 2.2665, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5906}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5832}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.536}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Sensor-Aware%20Phenomenological%20Framework%20for%20Lidar%20Degradation%20Simulation%20and%20SLAM%20Robustness%20Evaluation&body=Title%3A%20A%20Sensor-Aware%20Phenomenological%20Framework%20for%20Lidar%20Degradation%20Simulation%20and%20SLAM%20Robustness%20Evaluation%0AAuthor%3A%20Doumegna%20Mawuto%20Koudjo%20Felix%20and%20Xianjia%20Yu%20and%20Zhuo%20Zou%20and%20Tomi%20Westerlund%0AAbstract%3A%20Lidar-based%20SLAM%20systems%20are%20highly%20sensitive%20to%20adverse%20conditions%20such%20as%20occlusion%2C%20noise%2C%20and%20field-of-view%20%28FoV%29%20degradation%2C%20yet%20existing%20robustness%20evaluation%20methods%20either%20lack%20physical%20grounding%20or%20do%20not%20capture%20sensor-specific%20behavior.%20This%20paper%20presents%20a%20sensor-aware%2C%20phenomenological%20framework%20for%20simulating%20interpretable%20lidar%20degradations%20directly%20on%20real%20point%20clouds%2C%20enabling%20controlled%20and%20reproducible%20SLAM%20stress%20testing.%20Unlike%20image-derived%20corruption%20benchmarks%20%28e.g.%2C%20SemanticKITTI-C%29%20or%20simulation-only%20approaches%20%28e.g.%2C%20lidarsim%29%2C%20the%20proposed%20system%20preserves%20per-point%20geometry%2C%20intensity%2C%20and%20temporal%20structure%20while%20applying%20structured%20dropout%2C%20FoV%20reduction%2C%20Gaussian%20noise%2C%20occlusion%20masking%2C%20sparsification%2C%20and%20motion%20distortion.%20The%20framework%20features%20autonomous%20topic%20and%20sensor%20detection%2C%20modular%20configuration%20with%20four%20severity%20tiers%20%28light--extreme%29%2C%20and%20real-time%20performance%20%28less%20than%2020%20ms%20per%20frame%29%20compatible%20with%20ROS%20workflows.%20Experimental%20validation%20across%20three%20lidar%20architectures%20and%20five%20state-of-the-art%20SLAM%20systems%20reveals%20distinct%20robustness%20patterns%20shaped%20by%20sensor%20design%20and%20environmental%20context.%20The%20open-source%20implementation%20provides%20a%20practical%20foundation%20for%20benchmarking%20lidar-based%20SLAM%20under%20physically%20meaningful%20degradation%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08653v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Sensor-Aware%2520Phenomenological%2520Framework%2520for%2520Lidar%2520Degradation%2520Simulation%2520and%2520SLAM%2520Robustness%2520Evaluation%26entry.906535625%3DDoumegna%2520Mawuto%2520Koudjo%2520Felix%2520and%2520Xianjia%2520Yu%2520and%2520Zhuo%2520Zou%2520and%2520Tomi%2520Westerlund%26entry.1292438233%3DLidar-based%2520SLAM%2520systems%2520are%2520highly%2520sensitive%2520to%2520adverse%2520conditions%2520such%2520as%2520occlusion%252C%2520noise%252C%2520and%2520field-of-view%2520%2528FoV%2529%2520degradation%252C%2520yet%2520existing%2520robustness%2520evaluation%2520methods%2520either%2520lack%2520physical%2520grounding%2520or%2520do%2520not%2520capture%2520sensor-specific%2520behavior.%2520This%2520paper%2520presents%2520a%2520sensor-aware%252C%2520phenomenological%2520framework%2520for%2520simulating%2520interpretable%2520lidar%2520degradations%2520directly%2520on%2520real%2520point%2520clouds%252C%2520enabling%2520controlled%2520and%2520reproducible%2520SLAM%2520stress%2520testing.%2520Unlike%2520image-derived%2520corruption%2520benchmarks%2520%2528e.g.%252C%2520SemanticKITTI-C%2529%2520or%2520simulation-only%2520approaches%2520%2528e.g.%252C%2520lidarsim%2529%252C%2520the%2520proposed%2520system%2520preserves%2520per-point%2520geometry%252C%2520intensity%252C%2520and%2520temporal%2520structure%2520while%2520applying%2520structured%2520dropout%252C%2520FoV%2520reduction%252C%2520Gaussian%2520noise%252C%2520occlusion%2520masking%252C%2520sparsification%252C%2520and%2520motion%2520distortion.%2520The%2520framework%2520features%2520autonomous%2520topic%2520and%2520sensor%2520detection%252C%2520modular%2520configuration%2520with%2520four%2520severity%2520tiers%2520%2528light--extreme%2529%252C%2520and%2520real-time%2520performance%2520%2528less%2520than%252020%2520ms%2520per%2520frame%2529%2520compatible%2520with%2520ROS%2520workflows.%2520Experimental%2520validation%2520across%2520three%2520lidar%2520architectures%2520and%2520five%2520state-of-the-art%2520SLAM%2520systems%2520reveals%2520distinct%2520robustness%2520patterns%2520shaped%2520by%2520sensor%2520design%2520and%2520environmental%2520context.%2520The%2520open-source%2520implementation%2520provides%2520a%2520practical%2520foundation%2520for%2520benchmarking%2520lidar-based%2520SLAM%2520under%2520physically%2520meaningful%2520degradation%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08653v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Sensor-Aware%20Phenomenological%20Framework%20for%20Lidar%20Degradation%20Simulation%20and%20SLAM%20Robustness%20Evaluation&entry.906535625=Doumegna%20Mawuto%20Koudjo%20Felix%20and%20Xianjia%20Yu%20and%20Zhuo%20Zou%20and%20Tomi%20Westerlund&entry.1292438233=Lidar-based%20SLAM%20systems%20are%20highly%20sensitive%20to%20adverse%20conditions%20such%20as%20occlusion%2C%20noise%2C%20and%20field-of-view%20%28FoV%29%20degradation%2C%20yet%20existing%20robustness%20evaluation%20methods%20either%20lack%20physical%20grounding%20or%20do%20not%20capture%20sensor-specific%20behavior.%20This%20paper%20presents%20a%20sensor-aware%2C%20phenomenological%20framework%20for%20simulating%20interpretable%20lidar%20degradations%20directly%20on%20real%20point%20clouds%2C%20enabling%20controlled%20and%20reproducible%20SLAM%20stress%20testing.%20Unlike%20image-derived%20corruption%20benchmarks%20%28e.g.%2C%20SemanticKITTI-C%29%20or%20simulation-only%20approaches%20%28e.g.%2C%20lidarsim%29%2C%20the%20proposed%20system%20preserves%20per-point%20geometry%2C%20intensity%2C%20and%20temporal%20structure%20while%20applying%20structured%20dropout%2C%20FoV%20reduction%2C%20Gaussian%20noise%2C%20occlusion%20masking%2C%20sparsification%2C%20and%20motion%20distortion.%20The%20framework%20features%20autonomous%20topic%20and%20sensor%20detection%2C%20modular%20configuration%20with%20four%20severity%20tiers%20%28light--extreme%29%2C%20and%20real-time%20performance%20%28less%20than%2020%20ms%20per%20frame%29%20compatible%20with%20ROS%20workflows.%20Experimental%20validation%20across%20three%20lidar%20architectures%20and%20five%20state-of-the-art%20SLAM%20systems%20reveals%20distinct%20robustness%20patterns%20shaped%20by%20sensor%20design%20and%20environmental%20context.%20The%20open-source%20implementation%20provides%20a%20practical%20foundation%20for%20benchmarking%20lidar-based%20SLAM%20under%20physically%20meaningful%20degradation%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.08653v1&entry.124074799=Read"},
{"title": "Uncertainty-Aware Subset Selection for Robust Visual Explainability under Distribution Shifts", "author": "Madhav Gupta and Vishak Prasad C and Ganesh Ramakrishnan", "abstract": "Subset selection-based methods are widely used to explain deep vision models: they attribute predictions by highlighting the most influential image regions and support object-level explanations. While these methods perform well in in-distribution (ID) settings, their behavior under out-of-distribution (OOD) conditions remains poorly understood. Through extensive experiments across multiple ID-OOD sets, we find that reliability of the existing subset based methods degrades markedly, yielding redundant, unstable, and uncertainty-sensitive explanations. To address these shortcomings, we introduce a framework that combines submodular subset selection with layer-wise, gradient-based uncertainty estimation to improve robustness and fidelity without requiring additional training or auxiliary models. Our approach estimates uncertainty via adaptive weight perturbations and uses these estimates to guide submodular optimization, ensuring diverse and informative subset selection. Empirical evaluations show that, beyond mitigating the weaknesses of existing methods under OOD scenarios, our framework also yields improvements in ID settings. These findings highlight limitations of current subset-based approaches and demonstrate how uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.", "link": "http://arxiv.org/abs/2512.08445v1", "date": "2025-12-09", "relevancy": 2.2547, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.6207}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.578}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Uncertainty-Aware%20Subset%20Selection%20for%20Robust%20Visual%20Explainability%20under%20Distribution%20Shifts&body=Title%3A%20Uncertainty-Aware%20Subset%20Selection%20for%20Robust%20Visual%20Explainability%20under%20Distribution%20Shifts%0AAuthor%3A%20Madhav%20Gupta%20and%20Vishak%20Prasad%20C%20and%20Ganesh%20Ramakrishnan%0AAbstract%3A%20Subset%20selection-based%20methods%20are%20widely%20used%20to%20explain%20deep%20vision%20models%3A%20they%20attribute%20predictions%20by%20highlighting%20the%20most%20influential%20image%20regions%20and%20support%20object-level%20explanations.%20While%20these%20methods%20perform%20well%20in%20in-distribution%20%28ID%29%20settings%2C%20their%20behavior%20under%20out-of-distribution%20%28OOD%29%20conditions%20remains%20poorly%20understood.%20Through%20extensive%20experiments%20across%20multiple%20ID-OOD%20sets%2C%20we%20find%20that%20reliability%20of%20the%20existing%20subset%20based%20methods%20degrades%20markedly%2C%20yielding%20redundant%2C%20unstable%2C%20and%20uncertainty-sensitive%20explanations.%20To%20address%20these%20shortcomings%2C%20we%20introduce%20a%20framework%20that%20combines%20submodular%20subset%20selection%20with%20layer-wise%2C%20gradient-based%20uncertainty%20estimation%20to%20improve%20robustness%20and%20fidelity%20without%20requiring%20additional%20training%20or%20auxiliary%20models.%20Our%20approach%20estimates%20uncertainty%20via%20adaptive%20weight%20perturbations%20and%20uses%20these%20estimates%20to%20guide%20submodular%20optimization%2C%20ensuring%20diverse%20and%20informative%20subset%20selection.%20Empirical%20evaluations%20show%20that%2C%20beyond%20mitigating%20the%20weaknesses%20of%20existing%20methods%20under%20OOD%20scenarios%2C%20our%20framework%20also%20yields%20improvements%20in%20ID%20settings.%20These%20findings%20highlight%20limitations%20of%20current%20subset-based%20approaches%20and%20demonstrate%20how%20uncertainty-driven%20optimization%20can%20enhance%20attribution%20and%20object-level%20interpretability%2C%20paving%20the%20way%20for%20more%20transparent%20and%20trustworthy%20AI%20in%20real-world%20vision%20applications.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08445v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUncertainty-Aware%2520Subset%2520Selection%2520for%2520Robust%2520Visual%2520Explainability%2520under%2520Distribution%2520Shifts%26entry.906535625%3DMadhav%2520Gupta%2520and%2520Vishak%2520Prasad%2520C%2520and%2520Ganesh%2520Ramakrishnan%26entry.1292438233%3DSubset%2520selection-based%2520methods%2520are%2520widely%2520used%2520to%2520explain%2520deep%2520vision%2520models%253A%2520they%2520attribute%2520predictions%2520by%2520highlighting%2520the%2520most%2520influential%2520image%2520regions%2520and%2520support%2520object-level%2520explanations.%2520While%2520these%2520methods%2520perform%2520well%2520in%2520in-distribution%2520%2528ID%2529%2520settings%252C%2520their%2520behavior%2520under%2520out-of-distribution%2520%2528OOD%2529%2520conditions%2520remains%2520poorly%2520understood.%2520Through%2520extensive%2520experiments%2520across%2520multiple%2520ID-OOD%2520sets%252C%2520we%2520find%2520that%2520reliability%2520of%2520the%2520existing%2520subset%2520based%2520methods%2520degrades%2520markedly%252C%2520yielding%2520redundant%252C%2520unstable%252C%2520and%2520uncertainty-sensitive%2520explanations.%2520To%2520address%2520these%2520shortcomings%252C%2520we%2520introduce%2520a%2520framework%2520that%2520combines%2520submodular%2520subset%2520selection%2520with%2520layer-wise%252C%2520gradient-based%2520uncertainty%2520estimation%2520to%2520improve%2520robustness%2520and%2520fidelity%2520without%2520requiring%2520additional%2520training%2520or%2520auxiliary%2520models.%2520Our%2520approach%2520estimates%2520uncertainty%2520via%2520adaptive%2520weight%2520perturbations%2520and%2520uses%2520these%2520estimates%2520to%2520guide%2520submodular%2520optimization%252C%2520ensuring%2520diverse%2520and%2520informative%2520subset%2520selection.%2520Empirical%2520evaluations%2520show%2520that%252C%2520beyond%2520mitigating%2520the%2520weaknesses%2520of%2520existing%2520methods%2520under%2520OOD%2520scenarios%252C%2520our%2520framework%2520also%2520yields%2520improvements%2520in%2520ID%2520settings.%2520These%2520findings%2520highlight%2520limitations%2520of%2520current%2520subset-based%2520approaches%2520and%2520demonstrate%2520how%2520uncertainty-driven%2520optimization%2520can%2520enhance%2520attribution%2520and%2520object-level%2520interpretability%252C%2520paving%2520the%2520way%2520for%2520more%2520transparent%2520and%2520trustworthy%2520AI%2520in%2520real-world%2520vision%2520applications.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08445v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Uncertainty-Aware%20Subset%20Selection%20for%20Robust%20Visual%20Explainability%20under%20Distribution%20Shifts&entry.906535625=Madhav%20Gupta%20and%20Vishak%20Prasad%20C%20and%20Ganesh%20Ramakrishnan&entry.1292438233=Subset%20selection-based%20methods%20are%20widely%20used%20to%20explain%20deep%20vision%20models%3A%20they%20attribute%20predictions%20by%20highlighting%20the%20most%20influential%20image%20regions%20and%20support%20object-level%20explanations.%20While%20these%20methods%20perform%20well%20in%20in-distribution%20%28ID%29%20settings%2C%20their%20behavior%20under%20out-of-distribution%20%28OOD%29%20conditions%20remains%20poorly%20understood.%20Through%20extensive%20experiments%20across%20multiple%20ID-OOD%20sets%2C%20we%20find%20that%20reliability%20of%20the%20existing%20subset%20based%20methods%20degrades%20markedly%2C%20yielding%20redundant%2C%20unstable%2C%20and%20uncertainty-sensitive%20explanations.%20To%20address%20these%20shortcomings%2C%20we%20introduce%20a%20framework%20that%20combines%20submodular%20subset%20selection%20with%20layer-wise%2C%20gradient-based%20uncertainty%20estimation%20to%20improve%20robustness%20and%20fidelity%20without%20requiring%20additional%20training%20or%20auxiliary%20models.%20Our%20approach%20estimates%20uncertainty%20via%20adaptive%20weight%20perturbations%20and%20uses%20these%20estimates%20to%20guide%20submodular%20optimization%2C%20ensuring%20diverse%20and%20informative%20subset%20selection.%20Empirical%20evaluations%20show%20that%2C%20beyond%20mitigating%20the%20weaknesses%20of%20existing%20methods%20under%20OOD%20scenarios%2C%20our%20framework%20also%20yields%20improvements%20in%20ID%20settings.%20These%20findings%20highlight%20limitations%20of%20current%20subset-based%20approaches%20and%20demonstrate%20how%20uncertainty-driven%20optimization%20can%20enhance%20attribution%20and%20object-level%20interpretability%2C%20paving%20the%20way%20for%20more%20transparent%20and%20trustworthy%20AI%20in%20real-world%20vision%20applications.&entry.1838667208=http%3A//arxiv.org/abs/2512.08445v1&entry.124074799=Read"},
{"title": "A Lightweight Transfer Learning-Based State-of-Health Monitoring with Application to Lithium-ion Batteries in Unmanned Air Vehicles", "author": "Jiang Liu and Yan Qin and Wei Dai and Chau Yuen", "abstract": "Accurate and rapid state-of-health (SOH) monitoring plays an important role in indicating energy information for lithium-ion battery-powered portable mobile devices. To confront their variable working conditions, transfer learning (TL) emerges as a promising technique for leveraging knowledge from data-rich source working conditions, significantly reducing the training data required for SOH monitoring from target working conditions. However, traditional TL-based SOH monitoring is infeasible when applied in portable mobile devices since substantial computational resources are consumed during the TL stage and unexpectedly reduce the working endurance. To address these challenges, this paper proposes a lightweight TL-based SOH monitoring approach with constructive incremental transfer learning (CITL). First, taking advantage of the unlabeled data in the target domain, a semi-supervised TL mechanism is proposed to minimize the monitoring residual in a constructive way, through iteratively adding network nodes in the CITL. Second, the cross-domain learning ability of node parameters for CITL is comprehensively guaranteed through structural risk minimization, transfer mismatching minimization, and manifold consistency maximization. Moreover, the convergence analysis of the CITL is given, theoretically guaranteeing the efficacy of TL performance and network compactness. Finally, the proposed approach is verified through extensive experiments with a realistic unmanned air vehicles (UAV) battery dataset collected from dozens of flight missions. Specifically, the CITL outperforms SS-TCA, MMD-LSTM-DA, DDAN, BO-CNN-TL, and AS$^3$LSTM, in SOH estimation by 83.73%, 61.15%, 28.24%, 87.70%, and 57.34%, respectively, as evaluated using the index root mean square error.", "link": "http://arxiv.org/abs/2512.08512v1", "date": "2025-12-09", "relevancy": 2.2446, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4539}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4479}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4449}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Lightweight%20Transfer%20Learning-Based%20State-of-Health%20Monitoring%20with%20Application%20to%20Lithium-ion%20Batteries%20in%20Unmanned%20Air%20Vehicles&body=Title%3A%20A%20Lightweight%20Transfer%20Learning-Based%20State-of-Health%20Monitoring%20with%20Application%20to%20Lithium-ion%20Batteries%20in%20Unmanned%20Air%20Vehicles%0AAuthor%3A%20Jiang%20Liu%20and%20Yan%20Qin%20and%20Wei%20Dai%20and%20Chau%20Yuen%0AAbstract%3A%20Accurate%20and%20rapid%20state-of-health%20%28SOH%29%20monitoring%20plays%20an%20important%20role%20in%20indicating%20energy%20information%20for%20lithium-ion%20battery-powered%20portable%20mobile%20devices.%20To%20confront%20their%20variable%20working%20conditions%2C%20transfer%20learning%20%28TL%29%20emerges%20as%20a%20promising%20technique%20for%20leveraging%20knowledge%20from%20data-rich%20source%20working%20conditions%2C%20significantly%20reducing%20the%20training%20data%20required%20for%20SOH%20monitoring%20from%20target%20working%20conditions.%20However%2C%20traditional%20TL-based%20SOH%20monitoring%20is%20infeasible%20when%20applied%20in%20portable%20mobile%20devices%20since%20substantial%20computational%20resources%20are%20consumed%20during%20the%20TL%20stage%20and%20unexpectedly%20reduce%20the%20working%20endurance.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20lightweight%20TL-based%20SOH%20monitoring%20approach%20with%20constructive%20incremental%20transfer%20learning%20%28CITL%29.%20First%2C%20taking%20advantage%20of%20the%20unlabeled%20data%20in%20the%20target%20domain%2C%20a%20semi-supervised%20TL%20mechanism%20is%20proposed%20to%20minimize%20the%20monitoring%20residual%20in%20a%20constructive%20way%2C%20through%20iteratively%20adding%20network%20nodes%20in%20the%20CITL.%20Second%2C%20the%20cross-domain%20learning%20ability%20of%20node%20parameters%20for%20CITL%20is%20comprehensively%20guaranteed%20through%20structural%20risk%20minimization%2C%20transfer%20mismatching%20minimization%2C%20and%20manifold%20consistency%20maximization.%20Moreover%2C%20the%20convergence%20analysis%20of%20the%20CITL%20is%20given%2C%20theoretically%20guaranteeing%20the%20efficacy%20of%20TL%20performance%20and%20network%20compactness.%20Finally%2C%20the%20proposed%20approach%20is%20verified%20through%20extensive%20experiments%20with%20a%20realistic%20unmanned%20air%20vehicles%20%28UAV%29%20battery%20dataset%20collected%20from%20dozens%20of%20flight%20missions.%20Specifically%2C%20the%20CITL%20outperforms%20SS-TCA%2C%20MMD-LSTM-DA%2C%20DDAN%2C%20BO-CNN-TL%2C%20and%20AS%24%5E3%24LSTM%2C%20in%20SOH%20estimation%20by%2083.73%25%2C%2061.15%25%2C%2028.24%25%2C%2087.70%25%2C%20and%2057.34%25%2C%20respectively%2C%20as%20evaluated%20using%20the%20index%20root%20mean%20square%20error.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08512v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Lightweight%2520Transfer%2520Learning-Based%2520State-of-Health%2520Monitoring%2520with%2520Application%2520to%2520Lithium-ion%2520Batteries%2520in%2520Unmanned%2520Air%2520Vehicles%26entry.906535625%3DJiang%2520Liu%2520and%2520Yan%2520Qin%2520and%2520Wei%2520Dai%2520and%2520Chau%2520Yuen%26entry.1292438233%3DAccurate%2520and%2520rapid%2520state-of-health%2520%2528SOH%2529%2520monitoring%2520plays%2520an%2520important%2520role%2520in%2520indicating%2520energy%2520information%2520for%2520lithium-ion%2520battery-powered%2520portable%2520mobile%2520devices.%2520To%2520confront%2520their%2520variable%2520working%2520conditions%252C%2520transfer%2520learning%2520%2528TL%2529%2520emerges%2520as%2520a%2520promising%2520technique%2520for%2520leveraging%2520knowledge%2520from%2520data-rich%2520source%2520working%2520conditions%252C%2520significantly%2520reducing%2520the%2520training%2520data%2520required%2520for%2520SOH%2520monitoring%2520from%2520target%2520working%2520conditions.%2520However%252C%2520traditional%2520TL-based%2520SOH%2520monitoring%2520is%2520infeasible%2520when%2520applied%2520in%2520portable%2520mobile%2520devices%2520since%2520substantial%2520computational%2520resources%2520are%2520consumed%2520during%2520the%2520TL%2520stage%2520and%2520unexpectedly%2520reduce%2520the%2520working%2520endurance.%2520To%2520address%2520these%2520challenges%252C%2520this%2520paper%2520proposes%2520a%2520lightweight%2520TL-based%2520SOH%2520monitoring%2520approach%2520with%2520constructive%2520incremental%2520transfer%2520learning%2520%2528CITL%2529.%2520First%252C%2520taking%2520advantage%2520of%2520the%2520unlabeled%2520data%2520in%2520the%2520target%2520domain%252C%2520a%2520semi-supervised%2520TL%2520mechanism%2520is%2520proposed%2520to%2520minimize%2520the%2520monitoring%2520residual%2520in%2520a%2520constructive%2520way%252C%2520through%2520iteratively%2520adding%2520network%2520nodes%2520in%2520the%2520CITL.%2520Second%252C%2520the%2520cross-domain%2520learning%2520ability%2520of%2520node%2520parameters%2520for%2520CITL%2520is%2520comprehensively%2520guaranteed%2520through%2520structural%2520risk%2520minimization%252C%2520transfer%2520mismatching%2520minimization%252C%2520and%2520manifold%2520consistency%2520maximization.%2520Moreover%252C%2520the%2520convergence%2520analysis%2520of%2520the%2520CITL%2520is%2520given%252C%2520theoretically%2520guaranteeing%2520the%2520efficacy%2520of%2520TL%2520performance%2520and%2520network%2520compactness.%2520Finally%252C%2520the%2520proposed%2520approach%2520is%2520verified%2520through%2520extensive%2520experiments%2520with%2520a%2520realistic%2520unmanned%2520air%2520vehicles%2520%2528UAV%2529%2520battery%2520dataset%2520collected%2520from%2520dozens%2520of%2520flight%2520missions.%2520Specifically%252C%2520the%2520CITL%2520outperforms%2520SS-TCA%252C%2520MMD-LSTM-DA%252C%2520DDAN%252C%2520BO-CNN-TL%252C%2520and%2520AS%2524%255E3%2524LSTM%252C%2520in%2520SOH%2520estimation%2520by%252083.73%2525%252C%252061.15%2525%252C%252028.24%2525%252C%252087.70%2525%252C%2520and%252057.34%2525%252C%2520respectively%252C%2520as%2520evaluated%2520using%2520the%2520index%2520root%2520mean%2520square%2520error.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08512v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Lightweight%20Transfer%20Learning-Based%20State-of-Health%20Monitoring%20with%20Application%20to%20Lithium-ion%20Batteries%20in%20Unmanned%20Air%20Vehicles&entry.906535625=Jiang%20Liu%20and%20Yan%20Qin%20and%20Wei%20Dai%20and%20Chau%20Yuen&entry.1292438233=Accurate%20and%20rapid%20state-of-health%20%28SOH%29%20monitoring%20plays%20an%20important%20role%20in%20indicating%20energy%20information%20for%20lithium-ion%20battery-powered%20portable%20mobile%20devices.%20To%20confront%20their%20variable%20working%20conditions%2C%20transfer%20learning%20%28TL%29%20emerges%20as%20a%20promising%20technique%20for%20leveraging%20knowledge%20from%20data-rich%20source%20working%20conditions%2C%20significantly%20reducing%20the%20training%20data%20required%20for%20SOH%20monitoring%20from%20target%20working%20conditions.%20However%2C%20traditional%20TL-based%20SOH%20monitoring%20is%20infeasible%20when%20applied%20in%20portable%20mobile%20devices%20since%20substantial%20computational%20resources%20are%20consumed%20during%20the%20TL%20stage%20and%20unexpectedly%20reduce%20the%20working%20endurance.%20To%20address%20these%20challenges%2C%20this%20paper%20proposes%20a%20lightweight%20TL-based%20SOH%20monitoring%20approach%20with%20constructive%20incremental%20transfer%20learning%20%28CITL%29.%20First%2C%20taking%20advantage%20of%20the%20unlabeled%20data%20in%20the%20target%20domain%2C%20a%20semi-supervised%20TL%20mechanism%20is%20proposed%20to%20minimize%20the%20monitoring%20residual%20in%20a%20constructive%20way%2C%20through%20iteratively%20adding%20network%20nodes%20in%20the%20CITL.%20Second%2C%20the%20cross-domain%20learning%20ability%20of%20node%20parameters%20for%20CITL%20is%20comprehensively%20guaranteed%20through%20structural%20risk%20minimization%2C%20transfer%20mismatching%20minimization%2C%20and%20manifold%20consistency%20maximization.%20Moreover%2C%20the%20convergence%20analysis%20of%20the%20CITL%20is%20given%2C%20theoretically%20guaranteeing%20the%20efficacy%20of%20TL%20performance%20and%20network%20compactness.%20Finally%2C%20the%20proposed%20approach%20is%20verified%20through%20extensive%20experiments%20with%20a%20realistic%20unmanned%20air%20vehicles%20%28UAV%29%20battery%20dataset%20collected%20from%20dozens%20of%20flight%20missions.%20Specifically%2C%20the%20CITL%20outperforms%20SS-TCA%2C%20MMD-LSTM-DA%2C%20DDAN%2C%20BO-CNN-TL%2C%20and%20AS%24%5E3%24LSTM%2C%20in%20SOH%20estimation%20by%2083.73%25%2C%2061.15%25%2C%2028.24%25%2C%2087.70%25%2C%20and%2057.34%25%2C%20respectively%2C%20as%20evaluated%20using%20the%20index%20root%20mean%20square%20error.&entry.1838667208=http%3A//arxiv.org/abs/2512.08512v1&entry.124074799=Read"},
{"title": "Mixture of Contexts for Long Video Generation", "author": "Shengqu Cai and Ceyuan Yang and Lvmin Zhang and Yuwei Guo and Junfei Xiao and Ziyan Yang and Yinghao Xu and Zhenheng Yang and Alan Yuille and Leonidas Guibas and Maneesh Agrawala and Lu Jiang and Gordon Wetzstein", "abstract": "Long video generation is fundamentally a long context memory problem: models must retain and retrieve salient events across a long range without collapsing or drifting. However, scaling diffusion transformers to generate long-context videos is fundamentally limited by the quadratic cost of self-attention, which makes memory and computation intractable and difficult to optimize for long sequences. We recast long-context video generation as an internal information retrieval task and propose a simple, learnable sparse attention routing module, Mixture of Contexts (MoC), as an effective long-term memory retrieval engine. In MoC, each query dynamically selects a few informative chunks plus mandatory anchors (caption, local windows) to attend to, with causal routing that prevents loop closures. As we scale the data and gradually sparsify the routing, the model allocates compute to salient history, preserving identities, actions, and scenes over minutes of content. Efficiency follows as a byproduct of retrieval (near-linear scaling), which enables practical training and synthesis, and the emergence of memory and consistency at the scale of minutes.", "link": "http://arxiv.org/abs/2508.21058v3", "date": "2025-12-09", "relevancy": 2.2441, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5808}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5794}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Mixture%20of%20Contexts%20for%20Long%20Video%20Generation&body=Title%3A%20Mixture%20of%20Contexts%20for%20Long%20Video%20Generation%0AAuthor%3A%20Shengqu%20Cai%20and%20Ceyuan%20Yang%20and%20Lvmin%20Zhang%20and%20Yuwei%20Guo%20and%20Junfei%20Xiao%20and%20Ziyan%20Yang%20and%20Yinghao%20Xu%20and%20Zhenheng%20Yang%20and%20Alan%20Yuille%20and%20Leonidas%20Guibas%20and%20Maneesh%20Agrawala%20and%20Lu%20Jiang%20and%20Gordon%20Wetzstein%0AAbstract%3A%20Long%20video%20generation%20is%20fundamentally%20a%20long%20context%20memory%20problem%3A%20models%20must%20retain%20and%20retrieve%20salient%20events%20across%20a%20long%20range%20without%20collapsing%20or%20drifting.%20However%2C%20scaling%20diffusion%20transformers%20to%20generate%20long-context%20videos%20is%20fundamentally%20limited%20by%20the%20quadratic%20cost%20of%20self-attention%2C%20which%20makes%20memory%20and%20computation%20intractable%20and%20difficult%20to%20optimize%20for%20long%20sequences.%20We%20recast%20long-context%20video%20generation%20as%20an%20internal%20information%20retrieval%20task%20and%20propose%20a%20simple%2C%20learnable%20sparse%20attention%20routing%20module%2C%20Mixture%20of%20Contexts%20%28MoC%29%2C%20as%20an%20effective%20long-term%20memory%20retrieval%20engine.%20In%20MoC%2C%20each%20query%20dynamically%20selects%20a%20few%20informative%20chunks%20plus%20mandatory%20anchors%20%28caption%2C%20local%20windows%29%20to%20attend%20to%2C%20with%20causal%20routing%20that%20prevents%20loop%20closures.%20As%20we%20scale%20the%20data%20and%20gradually%20sparsify%20the%20routing%2C%20the%20model%20allocates%20compute%20to%20salient%20history%2C%20preserving%20identities%2C%20actions%2C%20and%20scenes%20over%20minutes%20of%20content.%20Efficiency%20follows%20as%20a%20byproduct%20of%20retrieval%20%28near-linear%20scaling%29%2C%20which%20enables%20practical%20training%20and%20synthesis%2C%20and%20the%20emergence%20of%20memory%20and%20consistency%20at%20the%20scale%20of%20minutes.%0ALink%3A%20http%3A//arxiv.org/abs/2508.21058v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMixture%2520of%2520Contexts%2520for%2520Long%2520Video%2520Generation%26entry.906535625%3DShengqu%2520Cai%2520and%2520Ceyuan%2520Yang%2520and%2520Lvmin%2520Zhang%2520and%2520Yuwei%2520Guo%2520and%2520Junfei%2520Xiao%2520and%2520Ziyan%2520Yang%2520and%2520Yinghao%2520Xu%2520and%2520Zhenheng%2520Yang%2520and%2520Alan%2520Yuille%2520and%2520Leonidas%2520Guibas%2520and%2520Maneesh%2520Agrawala%2520and%2520Lu%2520Jiang%2520and%2520Gordon%2520Wetzstein%26entry.1292438233%3DLong%2520video%2520generation%2520is%2520fundamentally%2520a%2520long%2520context%2520memory%2520problem%253A%2520models%2520must%2520retain%2520and%2520retrieve%2520salient%2520events%2520across%2520a%2520long%2520range%2520without%2520collapsing%2520or%2520drifting.%2520However%252C%2520scaling%2520diffusion%2520transformers%2520to%2520generate%2520long-context%2520videos%2520is%2520fundamentally%2520limited%2520by%2520the%2520quadratic%2520cost%2520of%2520self-attention%252C%2520which%2520makes%2520memory%2520and%2520computation%2520intractable%2520and%2520difficult%2520to%2520optimize%2520for%2520long%2520sequences.%2520We%2520recast%2520long-context%2520video%2520generation%2520as%2520an%2520internal%2520information%2520retrieval%2520task%2520and%2520propose%2520a%2520simple%252C%2520learnable%2520sparse%2520attention%2520routing%2520module%252C%2520Mixture%2520of%2520Contexts%2520%2528MoC%2529%252C%2520as%2520an%2520effective%2520long-term%2520memory%2520retrieval%2520engine.%2520In%2520MoC%252C%2520each%2520query%2520dynamically%2520selects%2520a%2520few%2520informative%2520chunks%2520plus%2520mandatory%2520anchors%2520%2528caption%252C%2520local%2520windows%2529%2520to%2520attend%2520to%252C%2520with%2520causal%2520routing%2520that%2520prevents%2520loop%2520closures.%2520As%2520we%2520scale%2520the%2520data%2520and%2520gradually%2520sparsify%2520the%2520routing%252C%2520the%2520model%2520allocates%2520compute%2520to%2520salient%2520history%252C%2520preserving%2520identities%252C%2520actions%252C%2520and%2520scenes%2520over%2520minutes%2520of%2520content.%2520Efficiency%2520follows%2520as%2520a%2520byproduct%2520of%2520retrieval%2520%2528near-linear%2520scaling%2529%252C%2520which%2520enables%2520practical%2520training%2520and%2520synthesis%252C%2520and%2520the%2520emergence%2520of%2520memory%2520and%2520consistency%2520at%2520the%2520scale%2520of%2520minutes.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.21058v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Mixture%20of%20Contexts%20for%20Long%20Video%20Generation&entry.906535625=Shengqu%20Cai%20and%20Ceyuan%20Yang%20and%20Lvmin%20Zhang%20and%20Yuwei%20Guo%20and%20Junfei%20Xiao%20and%20Ziyan%20Yang%20and%20Yinghao%20Xu%20and%20Zhenheng%20Yang%20and%20Alan%20Yuille%20and%20Leonidas%20Guibas%20and%20Maneesh%20Agrawala%20and%20Lu%20Jiang%20and%20Gordon%20Wetzstein&entry.1292438233=Long%20video%20generation%20is%20fundamentally%20a%20long%20context%20memory%20problem%3A%20models%20must%20retain%20and%20retrieve%20salient%20events%20across%20a%20long%20range%20without%20collapsing%20or%20drifting.%20However%2C%20scaling%20diffusion%20transformers%20to%20generate%20long-context%20videos%20is%20fundamentally%20limited%20by%20the%20quadratic%20cost%20of%20self-attention%2C%20which%20makes%20memory%20and%20computation%20intractable%20and%20difficult%20to%20optimize%20for%20long%20sequences.%20We%20recast%20long-context%20video%20generation%20as%20an%20internal%20information%20retrieval%20task%20and%20propose%20a%20simple%2C%20learnable%20sparse%20attention%20routing%20module%2C%20Mixture%20of%20Contexts%20%28MoC%29%2C%20as%20an%20effective%20long-term%20memory%20retrieval%20engine.%20In%20MoC%2C%20each%20query%20dynamically%20selects%20a%20few%20informative%20chunks%20plus%20mandatory%20anchors%20%28caption%2C%20local%20windows%29%20to%20attend%20to%2C%20with%20causal%20routing%20that%20prevents%20loop%20closures.%20As%20we%20scale%20the%20data%20and%20gradually%20sparsify%20the%20routing%2C%20the%20model%20allocates%20compute%20to%20salient%20history%2C%20preserving%20identities%2C%20actions%2C%20and%20scenes%20over%20minutes%20of%20content.%20Efficiency%20follows%20as%20a%20byproduct%20of%20retrieval%20%28near-linear%20scaling%29%2C%20which%20enables%20practical%20training%20and%20synthesis%2C%20and%20the%20emergence%20of%20memory%20and%20consistency%20at%20the%20scale%20of%20minutes.&entry.1838667208=http%3A//arxiv.org/abs/2508.21058v3&entry.124074799=Read"},
{"title": "Fused Gromov-Wasserstein Contrastive Learning for Effective Enzyme-Reaction Screening", "author": "Gengmo Zhou and Feng Yu and Wenda Wang and Zhifeng Gao and Guolin Ke and Zhewei Wei and Zhen Wang", "abstract": "Enzymes are crucial catalysts that enable a wide range of biochemical reactions. Efficiently identifying specific enzymes from vast protein libraries is essential for advancing biocatalysis. Traditional computational methods for enzyme screening and retrieval are time-consuming and resource-intensive. Recently, deep learning approaches have shown promise. However, these methods focus solely on the interaction between enzymes and reactions, overlooking the inherent hierarchical relationships within each domain. To address these limitations, we introduce FGW-CLIP, a novel contrastive learning framework based on optimizing the fused Gromov-Wasserstein distance. FGW-CLIP incorporates multiple alignments, including inter-domain alignment between reactions and enzymes and intra-domain alignment within enzymes and reactions. By introducing a tailored regularization term, our method minimizes the Gromov-Wasserstein distance between enzyme and reaction spaces, which enhances information integration across these domains. Extensive evaluations demonstrate the superiority of FGW-CLIP in challenging enzyme-reaction tasks. On the widely-used EnzymeMap benchmark, FGW-CLIP achieves state-of-the-art performance in enzyme virtual screening, as measured by BEDROC and EF metrics. Moreover, FGW-CLIP consistently outperforms across all three splits of ReactZyme, the largest enzyme-reaction benchmark, demonstrating robust generalization to novel enzymes and reactions. These results position FGW-CLIP as a promising framework for enzyme discovery in complex biochemical settings, with strong adaptability across diverse screening scenarios.", "link": "http://arxiv.org/abs/2512.08508v1", "date": "2025-12-09", "relevancy": 2.2423, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4597}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4428}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4428}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fused%20Gromov-Wasserstein%20Contrastive%20Learning%20for%20Effective%20Enzyme-Reaction%20Screening&body=Title%3A%20Fused%20Gromov-Wasserstein%20Contrastive%20Learning%20for%20Effective%20Enzyme-Reaction%20Screening%0AAuthor%3A%20Gengmo%20Zhou%20and%20Feng%20Yu%20and%20Wenda%20Wang%20and%20Zhifeng%20Gao%20and%20Guolin%20Ke%20and%20Zhewei%20Wei%20and%20Zhen%20Wang%0AAbstract%3A%20Enzymes%20are%20crucial%20catalysts%20that%20enable%20a%20wide%20range%20of%20biochemical%20reactions.%20Efficiently%20identifying%20specific%20enzymes%20from%20vast%20protein%20libraries%20is%20essential%20for%20advancing%20biocatalysis.%20Traditional%20computational%20methods%20for%20enzyme%20screening%20and%20retrieval%20are%20time-consuming%20and%20resource-intensive.%20Recently%2C%20deep%20learning%20approaches%20have%20shown%20promise.%20However%2C%20these%20methods%20focus%20solely%20on%20the%20interaction%20between%20enzymes%20and%20reactions%2C%20overlooking%20the%20inherent%20hierarchical%20relationships%20within%20each%20domain.%20To%20address%20these%20limitations%2C%20we%20introduce%20FGW-CLIP%2C%20a%20novel%20contrastive%20learning%20framework%20based%20on%20optimizing%20the%20fused%20Gromov-Wasserstein%20distance.%20FGW-CLIP%20incorporates%20multiple%20alignments%2C%20including%20inter-domain%20alignment%20between%20reactions%20and%20enzymes%20and%20intra-domain%20alignment%20within%20enzymes%20and%20reactions.%20By%20introducing%20a%20tailored%20regularization%20term%2C%20our%20method%20minimizes%20the%20Gromov-Wasserstein%20distance%20between%20enzyme%20and%20reaction%20spaces%2C%20which%20enhances%20information%20integration%20across%20these%20domains.%20Extensive%20evaluations%20demonstrate%20the%20superiority%20of%20FGW-CLIP%20in%20challenging%20enzyme-reaction%20tasks.%20On%20the%20widely-used%20EnzymeMap%20benchmark%2C%20FGW-CLIP%20achieves%20state-of-the-art%20performance%20in%20enzyme%20virtual%20screening%2C%20as%20measured%20by%20BEDROC%20and%20EF%20metrics.%20Moreover%2C%20FGW-CLIP%20consistently%20outperforms%20across%20all%20three%20splits%20of%20ReactZyme%2C%20the%20largest%20enzyme-reaction%20benchmark%2C%20demonstrating%20robust%20generalization%20to%20novel%20enzymes%20and%20reactions.%20These%20results%20position%20FGW-CLIP%20as%20a%20promising%20framework%20for%20enzyme%20discovery%20in%20complex%20biochemical%20settings%2C%20with%20strong%20adaptability%20across%20diverse%20screening%20scenarios.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08508v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFused%2520Gromov-Wasserstein%2520Contrastive%2520Learning%2520for%2520Effective%2520Enzyme-Reaction%2520Screening%26entry.906535625%3DGengmo%2520Zhou%2520and%2520Feng%2520Yu%2520and%2520Wenda%2520Wang%2520and%2520Zhifeng%2520Gao%2520and%2520Guolin%2520Ke%2520and%2520Zhewei%2520Wei%2520and%2520Zhen%2520Wang%26entry.1292438233%3DEnzymes%2520are%2520crucial%2520catalysts%2520that%2520enable%2520a%2520wide%2520range%2520of%2520biochemical%2520reactions.%2520Efficiently%2520identifying%2520specific%2520enzymes%2520from%2520vast%2520protein%2520libraries%2520is%2520essential%2520for%2520advancing%2520biocatalysis.%2520Traditional%2520computational%2520methods%2520for%2520enzyme%2520screening%2520and%2520retrieval%2520are%2520time-consuming%2520and%2520resource-intensive.%2520Recently%252C%2520deep%2520learning%2520approaches%2520have%2520shown%2520promise.%2520However%252C%2520these%2520methods%2520focus%2520solely%2520on%2520the%2520interaction%2520between%2520enzymes%2520and%2520reactions%252C%2520overlooking%2520the%2520inherent%2520hierarchical%2520relationships%2520within%2520each%2520domain.%2520To%2520address%2520these%2520limitations%252C%2520we%2520introduce%2520FGW-CLIP%252C%2520a%2520novel%2520contrastive%2520learning%2520framework%2520based%2520on%2520optimizing%2520the%2520fused%2520Gromov-Wasserstein%2520distance.%2520FGW-CLIP%2520incorporates%2520multiple%2520alignments%252C%2520including%2520inter-domain%2520alignment%2520between%2520reactions%2520and%2520enzymes%2520and%2520intra-domain%2520alignment%2520within%2520enzymes%2520and%2520reactions.%2520By%2520introducing%2520a%2520tailored%2520regularization%2520term%252C%2520our%2520method%2520minimizes%2520the%2520Gromov-Wasserstein%2520distance%2520between%2520enzyme%2520and%2520reaction%2520spaces%252C%2520which%2520enhances%2520information%2520integration%2520across%2520these%2520domains.%2520Extensive%2520evaluations%2520demonstrate%2520the%2520superiority%2520of%2520FGW-CLIP%2520in%2520challenging%2520enzyme-reaction%2520tasks.%2520On%2520the%2520widely-used%2520EnzymeMap%2520benchmark%252C%2520FGW-CLIP%2520achieves%2520state-of-the-art%2520performance%2520in%2520enzyme%2520virtual%2520screening%252C%2520as%2520measured%2520by%2520BEDROC%2520and%2520EF%2520metrics.%2520Moreover%252C%2520FGW-CLIP%2520consistently%2520outperforms%2520across%2520all%2520three%2520splits%2520of%2520ReactZyme%252C%2520the%2520largest%2520enzyme-reaction%2520benchmark%252C%2520demonstrating%2520robust%2520generalization%2520to%2520novel%2520enzymes%2520and%2520reactions.%2520These%2520results%2520position%2520FGW-CLIP%2520as%2520a%2520promising%2520framework%2520for%2520enzyme%2520discovery%2520in%2520complex%2520biochemical%2520settings%252C%2520with%2520strong%2520adaptability%2520across%2520diverse%2520screening%2520scenarios.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08508v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fused%20Gromov-Wasserstein%20Contrastive%20Learning%20for%20Effective%20Enzyme-Reaction%20Screening&entry.906535625=Gengmo%20Zhou%20and%20Feng%20Yu%20and%20Wenda%20Wang%20and%20Zhifeng%20Gao%20and%20Guolin%20Ke%20and%20Zhewei%20Wei%20and%20Zhen%20Wang&entry.1292438233=Enzymes%20are%20crucial%20catalysts%20that%20enable%20a%20wide%20range%20of%20biochemical%20reactions.%20Efficiently%20identifying%20specific%20enzymes%20from%20vast%20protein%20libraries%20is%20essential%20for%20advancing%20biocatalysis.%20Traditional%20computational%20methods%20for%20enzyme%20screening%20and%20retrieval%20are%20time-consuming%20and%20resource-intensive.%20Recently%2C%20deep%20learning%20approaches%20have%20shown%20promise.%20However%2C%20these%20methods%20focus%20solely%20on%20the%20interaction%20between%20enzymes%20and%20reactions%2C%20overlooking%20the%20inherent%20hierarchical%20relationships%20within%20each%20domain.%20To%20address%20these%20limitations%2C%20we%20introduce%20FGW-CLIP%2C%20a%20novel%20contrastive%20learning%20framework%20based%20on%20optimizing%20the%20fused%20Gromov-Wasserstein%20distance.%20FGW-CLIP%20incorporates%20multiple%20alignments%2C%20including%20inter-domain%20alignment%20between%20reactions%20and%20enzymes%20and%20intra-domain%20alignment%20within%20enzymes%20and%20reactions.%20By%20introducing%20a%20tailored%20regularization%20term%2C%20our%20method%20minimizes%20the%20Gromov-Wasserstein%20distance%20between%20enzyme%20and%20reaction%20spaces%2C%20which%20enhances%20information%20integration%20across%20these%20domains.%20Extensive%20evaluations%20demonstrate%20the%20superiority%20of%20FGW-CLIP%20in%20challenging%20enzyme-reaction%20tasks.%20On%20the%20widely-used%20EnzymeMap%20benchmark%2C%20FGW-CLIP%20achieves%20state-of-the-art%20performance%20in%20enzyme%20virtual%20screening%2C%20as%20measured%20by%20BEDROC%20and%20EF%20metrics.%20Moreover%2C%20FGW-CLIP%20consistently%20outperforms%20across%20all%20three%20splits%20of%20ReactZyme%2C%20the%20largest%20enzyme-reaction%20benchmark%2C%20demonstrating%20robust%20generalization%20to%20novel%20enzymes%20and%20reactions.%20These%20results%20position%20FGW-CLIP%20as%20a%20promising%20framework%20for%20enzyme%20discovery%20in%20complex%20biochemical%20settings%2C%20with%20strong%20adaptability%20across%20diverse%20screening%20scenarios.&entry.1838667208=http%3A//arxiv.org/abs/2512.08508v1&entry.124074799=Read"},
{"title": "Graph Coloring for Multi-Task Learning", "author": "Santosh Patapati", "abstract": "When different objectives conflict with each other in multi-task learning, gradients begin to interfere and slow convergence, thereby potentially reducing the final model's performance. To address this, we introduce SON-GOKU, a scheduler that computes gradient interference, constructs an interference graph, and then applies greedy graph-coloring to partition tasks into groups that align well with each other. At each training step, only one group (color class) of tasks are activated, and the grouping partition is constantly recomputed as task relationships evolve throughout training. By ensuring that each mini-batch contains only tasks that pull the model in the same direction, our method improves the effectiveness of any underlying multi-task learning optimizer without additional tuning. Since tasks within these groups will update in compatible directions, multi-task learning will improve model performance rather than impede it. Empirical results on six different datasets show that this interference-aware graph-coloring approach consistently outperforms baselines and state-of-the-art multi-task optimizers. We provide extensive theory showing why grouping and sequential updates improve multi-task learning, with guarantees on descent, convergence, and accurately identifying what tasks conflict or align.", "link": "http://arxiv.org/abs/2509.16959v4", "date": "2025-12-09", "relevancy": 2.2396, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4547}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.445}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.444}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Coloring%20for%20Multi-Task%20Learning&body=Title%3A%20Graph%20Coloring%20for%20Multi-Task%20Learning%0AAuthor%3A%20Santosh%20Patapati%0AAbstract%3A%20When%20different%20objectives%20conflict%20with%20each%20other%20in%20multi-task%20learning%2C%20gradients%20begin%20to%20interfere%20and%20slow%20convergence%2C%20thereby%20potentially%20reducing%20the%20final%20model%27s%20performance.%20To%20address%20this%2C%20we%20introduce%20SON-GOKU%2C%20a%20scheduler%20that%20computes%20gradient%20interference%2C%20constructs%20an%20interference%20graph%2C%20and%20then%20applies%20greedy%20graph-coloring%20to%20partition%20tasks%20into%20groups%20that%20align%20well%20with%20each%20other.%20At%20each%20training%20step%2C%20only%20one%20group%20%28color%20class%29%20of%20tasks%20are%20activated%2C%20and%20the%20grouping%20partition%20is%20constantly%20recomputed%20as%20task%20relationships%20evolve%20throughout%20training.%20By%20ensuring%20that%20each%20mini-batch%20contains%20only%20tasks%20that%20pull%20the%20model%20in%20the%20same%20direction%2C%20our%20method%20improves%20the%20effectiveness%20of%20any%20underlying%20multi-task%20learning%20optimizer%20without%20additional%20tuning.%20Since%20tasks%20within%20these%20groups%20will%20update%20in%20compatible%20directions%2C%20multi-task%20learning%20will%20improve%20model%20performance%20rather%20than%20impede%20it.%20Empirical%20results%20on%20six%20different%20datasets%20show%20that%20this%20interference-aware%20graph-coloring%20approach%20consistently%20outperforms%20baselines%20and%20state-of-the-art%20multi-task%20optimizers.%20We%20provide%20extensive%20theory%20showing%20why%20grouping%20and%20sequential%20updates%20improve%20multi-task%20learning%2C%20with%20guarantees%20on%20descent%2C%20convergence%2C%20and%20accurately%20identifying%20what%20tasks%20conflict%20or%20align.%0ALink%3A%20http%3A//arxiv.org/abs/2509.16959v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Coloring%2520for%2520Multi-Task%2520Learning%26entry.906535625%3DSantosh%2520Patapati%26entry.1292438233%3DWhen%2520different%2520objectives%2520conflict%2520with%2520each%2520other%2520in%2520multi-task%2520learning%252C%2520gradients%2520begin%2520to%2520interfere%2520and%2520slow%2520convergence%252C%2520thereby%2520potentially%2520reducing%2520the%2520final%2520model%2527s%2520performance.%2520To%2520address%2520this%252C%2520we%2520introduce%2520SON-GOKU%252C%2520a%2520scheduler%2520that%2520computes%2520gradient%2520interference%252C%2520constructs%2520an%2520interference%2520graph%252C%2520and%2520then%2520applies%2520greedy%2520graph-coloring%2520to%2520partition%2520tasks%2520into%2520groups%2520that%2520align%2520well%2520with%2520each%2520other.%2520At%2520each%2520training%2520step%252C%2520only%2520one%2520group%2520%2528color%2520class%2529%2520of%2520tasks%2520are%2520activated%252C%2520and%2520the%2520grouping%2520partition%2520is%2520constantly%2520recomputed%2520as%2520task%2520relationships%2520evolve%2520throughout%2520training.%2520By%2520ensuring%2520that%2520each%2520mini-batch%2520contains%2520only%2520tasks%2520that%2520pull%2520the%2520model%2520in%2520the%2520same%2520direction%252C%2520our%2520method%2520improves%2520the%2520effectiveness%2520of%2520any%2520underlying%2520multi-task%2520learning%2520optimizer%2520without%2520additional%2520tuning.%2520Since%2520tasks%2520within%2520these%2520groups%2520will%2520update%2520in%2520compatible%2520directions%252C%2520multi-task%2520learning%2520will%2520improve%2520model%2520performance%2520rather%2520than%2520impede%2520it.%2520Empirical%2520results%2520on%2520six%2520different%2520datasets%2520show%2520that%2520this%2520interference-aware%2520graph-coloring%2520approach%2520consistently%2520outperforms%2520baselines%2520and%2520state-of-the-art%2520multi-task%2520optimizers.%2520We%2520provide%2520extensive%2520theory%2520showing%2520why%2520grouping%2520and%2520sequential%2520updates%2520improve%2520multi-task%2520learning%252C%2520with%2520guarantees%2520on%2520descent%252C%2520convergence%252C%2520and%2520accurately%2520identifying%2520what%2520tasks%2520conflict%2520or%2520align.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2509.16959v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Coloring%20for%20Multi-Task%20Learning&entry.906535625=Santosh%20Patapati&entry.1292438233=When%20different%20objectives%20conflict%20with%20each%20other%20in%20multi-task%20learning%2C%20gradients%20begin%20to%20interfere%20and%20slow%20convergence%2C%20thereby%20potentially%20reducing%20the%20final%20model%27s%20performance.%20To%20address%20this%2C%20we%20introduce%20SON-GOKU%2C%20a%20scheduler%20that%20computes%20gradient%20interference%2C%20constructs%20an%20interference%20graph%2C%20and%20then%20applies%20greedy%20graph-coloring%20to%20partition%20tasks%20into%20groups%20that%20align%20well%20with%20each%20other.%20At%20each%20training%20step%2C%20only%20one%20group%20%28color%20class%29%20of%20tasks%20are%20activated%2C%20and%20the%20grouping%20partition%20is%20constantly%20recomputed%20as%20task%20relationships%20evolve%20throughout%20training.%20By%20ensuring%20that%20each%20mini-batch%20contains%20only%20tasks%20that%20pull%20the%20model%20in%20the%20same%20direction%2C%20our%20method%20improves%20the%20effectiveness%20of%20any%20underlying%20multi-task%20learning%20optimizer%20without%20additional%20tuning.%20Since%20tasks%20within%20these%20groups%20will%20update%20in%20compatible%20directions%2C%20multi-task%20learning%20will%20improve%20model%20performance%20rather%20than%20impede%20it.%20Empirical%20results%20on%20six%20different%20datasets%20show%20that%20this%20interference-aware%20graph-coloring%20approach%20consistently%20outperforms%20baselines%20and%20state-of-the-art%20multi-task%20optimizers.%20We%20provide%20extensive%20theory%20showing%20why%20grouping%20and%20sequential%20updates%20improve%20multi-task%20learning%2C%20with%20guarantees%20on%20descent%2C%20convergence%2C%20and%20accurately%20identifying%20what%20tasks%20conflict%20or%20align.&entry.1838667208=http%3A//arxiv.org/abs/2509.16959v4&entry.124074799=Read"},
{"title": "Detecting value-expressive text posts in Russian social media", "author": "Maria Milkova and Maksim Rudnev and Lidia Okolskaya", "abstract": "Basic values are concepts or beliefs which pertain to desirable end-states and transcend specific situations. Studying personal values in social media can illuminate how and why societal values evolve especially when the stimuli-based methods, such as surveys, are inefficient, for instance, in hard-to-reach populations. On the other hand, user-generated content is driven by the massive use of stereotyped, culturally defined speech constructions rather than authentic expressions of personal values. We aimed to find a model that can accurately detect value-expressive posts in Russian social media VKontakte. A training dataset of 5,035 posts was annotated by three experts, 304 crowd-workers and ChatGPT. Crowd-workers and experts showed only moderate agreement in categorizing posts. ChatGPT was more consistent but struggled with spam detection. We applied an ensemble of human- and AI-assisted annotation involving active learning approach, subsequently trained several classification models using embeddings from various pre-trained transformer-based language models. The best performance was achieved with embeddings from a fine-tuned rubert-tiny2 model, yielding high value detection quality (F1 = 0.77, F1-macro = 0.83). This model provides a crucial step to a study of values within and between Russian social media users.", "link": "http://arxiv.org/abs/2312.08968v3", "date": "2025-12-09", "relevancy": 2.2393, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4652}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4423}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.436}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20value-expressive%20text%20posts%20in%20Russian%20social%20media&body=Title%3A%20Detecting%20value-expressive%20text%20posts%20in%20Russian%20social%20media%0AAuthor%3A%20Maria%20Milkova%20and%20Maksim%20Rudnev%20and%20Lidia%20Okolskaya%0AAbstract%3A%20Basic%20values%20are%20concepts%20or%20beliefs%20which%20pertain%20to%20desirable%20end-states%20and%20transcend%20specific%20situations.%20Studying%20personal%20values%20in%20social%20media%20can%20illuminate%20how%20and%20why%20societal%20values%20evolve%20especially%20when%20the%20stimuli-based%20methods%2C%20such%20as%20surveys%2C%20are%20inefficient%2C%20for%20instance%2C%20in%20hard-to-reach%20populations.%20On%20the%20other%20hand%2C%20user-generated%20content%20is%20driven%20by%20the%20massive%20use%20of%20stereotyped%2C%20culturally%20defined%20speech%20constructions%20rather%20than%20authentic%20expressions%20of%20personal%20values.%20We%20aimed%20to%20find%20a%20model%20that%20can%20accurately%20detect%20value-expressive%20posts%20in%20Russian%20social%20media%20VKontakte.%20A%20training%20dataset%20of%205%2C035%20posts%20was%20annotated%20by%20three%20experts%2C%20304%20crowd-workers%20and%20ChatGPT.%20Crowd-workers%20and%20experts%20showed%20only%20moderate%20agreement%20in%20categorizing%20posts.%20ChatGPT%20was%20more%20consistent%20but%20struggled%20with%20spam%20detection.%20We%20applied%20an%20ensemble%20of%20human-%20and%20AI-assisted%20annotation%20involving%20active%20learning%20approach%2C%20subsequently%20trained%20several%20classification%20models%20using%20embeddings%20from%20various%20pre-trained%20transformer-based%20language%20models.%20The%20best%20performance%20was%20achieved%20with%20embeddings%20from%20a%20fine-tuned%20rubert-tiny2%20model%2C%20yielding%20high%20value%20detection%20quality%20%28F1%20%3D%200.77%2C%20F1-macro%20%3D%200.83%29.%20This%20model%20provides%20a%20crucial%20step%20to%20a%20study%20of%20values%20within%20and%20between%20Russian%20social%20media%20users.%0ALink%3A%20http%3A//arxiv.org/abs/2312.08968v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520value-expressive%2520text%2520posts%2520in%2520Russian%2520social%2520media%26entry.906535625%3DMaria%2520Milkova%2520and%2520Maksim%2520Rudnev%2520and%2520Lidia%2520Okolskaya%26entry.1292438233%3DBasic%2520values%2520are%2520concepts%2520or%2520beliefs%2520which%2520pertain%2520to%2520desirable%2520end-states%2520and%2520transcend%2520specific%2520situations.%2520Studying%2520personal%2520values%2520in%2520social%2520media%2520can%2520illuminate%2520how%2520and%2520why%2520societal%2520values%2520evolve%2520especially%2520when%2520the%2520stimuli-based%2520methods%252C%2520such%2520as%2520surveys%252C%2520are%2520inefficient%252C%2520for%2520instance%252C%2520in%2520hard-to-reach%2520populations.%2520On%2520the%2520other%2520hand%252C%2520user-generated%2520content%2520is%2520driven%2520by%2520the%2520massive%2520use%2520of%2520stereotyped%252C%2520culturally%2520defined%2520speech%2520constructions%2520rather%2520than%2520authentic%2520expressions%2520of%2520personal%2520values.%2520We%2520aimed%2520to%2520find%2520a%2520model%2520that%2520can%2520accurately%2520detect%2520value-expressive%2520posts%2520in%2520Russian%2520social%2520media%2520VKontakte.%2520A%2520training%2520dataset%2520of%25205%252C035%2520posts%2520was%2520annotated%2520by%2520three%2520experts%252C%2520304%2520crowd-workers%2520and%2520ChatGPT.%2520Crowd-workers%2520and%2520experts%2520showed%2520only%2520moderate%2520agreement%2520in%2520categorizing%2520posts.%2520ChatGPT%2520was%2520more%2520consistent%2520but%2520struggled%2520with%2520spam%2520detection.%2520We%2520applied%2520an%2520ensemble%2520of%2520human-%2520and%2520AI-assisted%2520annotation%2520involving%2520active%2520learning%2520approach%252C%2520subsequently%2520trained%2520several%2520classification%2520models%2520using%2520embeddings%2520from%2520various%2520pre-trained%2520transformer-based%2520language%2520models.%2520The%2520best%2520performance%2520was%2520achieved%2520with%2520embeddings%2520from%2520a%2520fine-tuned%2520rubert-tiny2%2520model%252C%2520yielding%2520high%2520value%2520detection%2520quality%2520%2528F1%2520%253D%25200.77%252C%2520F1-macro%2520%253D%25200.83%2529.%2520This%2520model%2520provides%2520a%2520crucial%2520step%2520to%2520a%2520study%2520of%2520values%2520within%2520and%2520between%2520Russian%2520social%2520media%2520users.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.08968v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20value-expressive%20text%20posts%20in%20Russian%20social%20media&entry.906535625=Maria%20Milkova%20and%20Maksim%20Rudnev%20and%20Lidia%20Okolskaya&entry.1292438233=Basic%20values%20are%20concepts%20or%20beliefs%20which%20pertain%20to%20desirable%20end-states%20and%20transcend%20specific%20situations.%20Studying%20personal%20values%20in%20social%20media%20can%20illuminate%20how%20and%20why%20societal%20values%20evolve%20especially%20when%20the%20stimuli-based%20methods%2C%20such%20as%20surveys%2C%20are%20inefficient%2C%20for%20instance%2C%20in%20hard-to-reach%20populations.%20On%20the%20other%20hand%2C%20user-generated%20content%20is%20driven%20by%20the%20massive%20use%20of%20stereotyped%2C%20culturally%20defined%20speech%20constructions%20rather%20than%20authentic%20expressions%20of%20personal%20values.%20We%20aimed%20to%20find%20a%20model%20that%20can%20accurately%20detect%20value-expressive%20posts%20in%20Russian%20social%20media%20VKontakte.%20A%20training%20dataset%20of%205%2C035%20posts%20was%20annotated%20by%20three%20experts%2C%20304%20crowd-workers%20and%20ChatGPT.%20Crowd-workers%20and%20experts%20showed%20only%20moderate%20agreement%20in%20categorizing%20posts.%20ChatGPT%20was%20more%20consistent%20but%20struggled%20with%20spam%20detection.%20We%20applied%20an%20ensemble%20of%20human-%20and%20AI-assisted%20annotation%20involving%20active%20learning%20approach%2C%20subsequently%20trained%20several%20classification%20models%20using%20embeddings%20from%20various%20pre-trained%20transformer-based%20language%20models.%20The%20best%20performance%20was%20achieved%20with%20embeddings%20from%20a%20fine-tuned%20rubert-tiny2%20model%2C%20yielding%20high%20value%20detection%20quality%20%28F1%20%3D%200.77%2C%20F1-macro%20%3D%200.83%29.%20This%20model%20provides%20a%20crucial%20step%20to%20a%20study%20of%20values%20within%20and%20between%20Russian%20social%20media%20users.&entry.1838667208=http%3A//arxiv.org/abs/2312.08968v3&entry.124074799=Read"},
{"title": "OSMO: Open-Source Tactile Glove for Human-to-Robot Skill Transfer", "author": "Jessica Yin and Haozhi Qi and Youngsun Wi and Sayantan Kundu and Mike Lambeta and William Yang and Changhao Wang and Tingfan Wu and Jitendra Malik and Tess Hellebrekers", "abstract": "Human video demonstrations provide abundant training data for learning robot policies, but video alone cannot capture the rich contact signals critical for mastering manipulation. We introduce OSMO, an open-source wearable tactile glove designed for human-to-robot skill transfer. The glove features 12 three-axis tactile sensors across the fingertips and palm and is designed to be compatible with state-of-the-art hand-tracking methods for in-the-wild data collection. We demonstrate that a robot policy trained exclusively on human demonstrations collected with OSMO, without any real robot data, is capable of executing a challenging contact-rich manipulation task. By equipping both the human and the robot with the same glove, OSMO minimizes the visual and tactile embodiment gap, enabling the transfer of continuous shear and normal force feedback while avoiding the need for image inpainting or other vision-based force inference. On a real-world wiping task requiring sustained contact pressure, our tactile-aware policy achieves a 72% success rate, outperforming vision-only baselines by eliminating contact-related failure modes. We release complete hardware designs, firmware, and assembly instructions to support community adoption.", "link": "http://arxiv.org/abs/2512.08920v1", "date": "2025-12-09", "relevancy": 2.2173, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5603}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5514}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OSMO%3A%20Open-Source%20Tactile%20Glove%20for%20Human-to-Robot%20Skill%20Transfer&body=Title%3A%20OSMO%3A%20Open-Source%20Tactile%20Glove%20for%20Human-to-Robot%20Skill%20Transfer%0AAuthor%3A%20Jessica%20Yin%20and%20Haozhi%20Qi%20and%20Youngsun%20Wi%20and%20Sayantan%20Kundu%20and%20Mike%20Lambeta%20and%20William%20Yang%20and%20Changhao%20Wang%20and%20Tingfan%20Wu%20and%20Jitendra%20Malik%20and%20Tess%20Hellebrekers%0AAbstract%3A%20Human%20video%20demonstrations%20provide%20abundant%20training%20data%20for%20learning%20robot%20policies%2C%20but%20video%20alone%20cannot%20capture%20the%20rich%20contact%20signals%20critical%20for%20mastering%20manipulation.%20We%20introduce%20OSMO%2C%20an%20open-source%20wearable%20tactile%20glove%20designed%20for%20human-to-robot%20skill%20transfer.%20The%20glove%20features%2012%20three-axis%20tactile%20sensors%20across%20the%20fingertips%20and%20palm%20and%20is%20designed%20to%20be%20compatible%20with%20state-of-the-art%20hand-tracking%20methods%20for%20in-the-wild%20data%20collection.%20We%20demonstrate%20that%20a%20robot%20policy%20trained%20exclusively%20on%20human%20demonstrations%20collected%20with%20OSMO%2C%20without%20any%20real%20robot%20data%2C%20is%20capable%20of%20executing%20a%20challenging%20contact-rich%20manipulation%20task.%20By%20equipping%20both%20the%20human%20and%20the%20robot%20with%20the%20same%20glove%2C%20OSMO%20minimizes%20the%20visual%20and%20tactile%20embodiment%20gap%2C%20enabling%20the%20transfer%20of%20continuous%20shear%20and%20normal%20force%20feedback%20while%20avoiding%20the%20need%20for%20image%20inpainting%20or%20other%20vision-based%20force%20inference.%20On%20a%20real-world%20wiping%20task%20requiring%20sustained%20contact%20pressure%2C%20our%20tactile-aware%20policy%20achieves%20a%2072%25%20success%20rate%2C%20outperforming%20vision-only%20baselines%20by%20eliminating%20contact-related%20failure%20modes.%20We%20release%20complete%20hardware%20designs%2C%20firmware%2C%20and%20assembly%20instructions%20to%20support%20community%20adoption.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08920v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOSMO%253A%2520Open-Source%2520Tactile%2520Glove%2520for%2520Human-to-Robot%2520Skill%2520Transfer%26entry.906535625%3DJessica%2520Yin%2520and%2520Haozhi%2520Qi%2520and%2520Youngsun%2520Wi%2520and%2520Sayantan%2520Kundu%2520and%2520Mike%2520Lambeta%2520and%2520William%2520Yang%2520and%2520Changhao%2520Wang%2520and%2520Tingfan%2520Wu%2520and%2520Jitendra%2520Malik%2520and%2520Tess%2520Hellebrekers%26entry.1292438233%3DHuman%2520video%2520demonstrations%2520provide%2520abundant%2520training%2520data%2520for%2520learning%2520robot%2520policies%252C%2520but%2520video%2520alone%2520cannot%2520capture%2520the%2520rich%2520contact%2520signals%2520critical%2520for%2520mastering%2520manipulation.%2520We%2520introduce%2520OSMO%252C%2520an%2520open-source%2520wearable%2520tactile%2520glove%2520designed%2520for%2520human-to-robot%2520skill%2520transfer.%2520The%2520glove%2520features%252012%2520three-axis%2520tactile%2520sensors%2520across%2520the%2520fingertips%2520and%2520palm%2520and%2520is%2520designed%2520to%2520be%2520compatible%2520with%2520state-of-the-art%2520hand-tracking%2520methods%2520for%2520in-the-wild%2520data%2520collection.%2520We%2520demonstrate%2520that%2520a%2520robot%2520policy%2520trained%2520exclusively%2520on%2520human%2520demonstrations%2520collected%2520with%2520OSMO%252C%2520without%2520any%2520real%2520robot%2520data%252C%2520is%2520capable%2520of%2520executing%2520a%2520challenging%2520contact-rich%2520manipulation%2520task.%2520By%2520equipping%2520both%2520the%2520human%2520and%2520the%2520robot%2520with%2520the%2520same%2520glove%252C%2520OSMO%2520minimizes%2520the%2520visual%2520and%2520tactile%2520embodiment%2520gap%252C%2520enabling%2520the%2520transfer%2520of%2520continuous%2520shear%2520and%2520normal%2520force%2520feedback%2520while%2520avoiding%2520the%2520need%2520for%2520image%2520inpainting%2520or%2520other%2520vision-based%2520force%2520inference.%2520On%2520a%2520real-world%2520wiping%2520task%2520requiring%2520sustained%2520contact%2520pressure%252C%2520our%2520tactile-aware%2520policy%2520achieves%2520a%252072%2525%2520success%2520rate%252C%2520outperforming%2520vision-only%2520baselines%2520by%2520eliminating%2520contact-related%2520failure%2520modes.%2520We%2520release%2520complete%2520hardware%2520designs%252C%2520firmware%252C%2520and%2520assembly%2520instructions%2520to%2520support%2520community%2520adoption.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08920v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OSMO%3A%20Open-Source%20Tactile%20Glove%20for%20Human-to-Robot%20Skill%20Transfer&entry.906535625=Jessica%20Yin%20and%20Haozhi%20Qi%20and%20Youngsun%20Wi%20and%20Sayantan%20Kundu%20and%20Mike%20Lambeta%20and%20William%20Yang%20and%20Changhao%20Wang%20and%20Tingfan%20Wu%20and%20Jitendra%20Malik%20and%20Tess%20Hellebrekers&entry.1292438233=Human%20video%20demonstrations%20provide%20abundant%20training%20data%20for%20learning%20robot%20policies%2C%20but%20video%20alone%20cannot%20capture%20the%20rich%20contact%20signals%20critical%20for%20mastering%20manipulation.%20We%20introduce%20OSMO%2C%20an%20open-source%20wearable%20tactile%20glove%20designed%20for%20human-to-robot%20skill%20transfer.%20The%20glove%20features%2012%20three-axis%20tactile%20sensors%20across%20the%20fingertips%20and%20palm%20and%20is%20designed%20to%20be%20compatible%20with%20state-of-the-art%20hand-tracking%20methods%20for%20in-the-wild%20data%20collection.%20We%20demonstrate%20that%20a%20robot%20policy%20trained%20exclusively%20on%20human%20demonstrations%20collected%20with%20OSMO%2C%20without%20any%20real%20robot%20data%2C%20is%20capable%20of%20executing%20a%20challenging%20contact-rich%20manipulation%20task.%20By%20equipping%20both%20the%20human%20and%20the%20robot%20with%20the%20same%20glove%2C%20OSMO%20minimizes%20the%20visual%20and%20tactile%20embodiment%20gap%2C%20enabling%20the%20transfer%20of%20continuous%20shear%20and%20normal%20force%20feedback%20while%20avoiding%20the%20need%20for%20image%20inpainting%20or%20other%20vision-based%20force%20inference.%20On%20a%20real-world%20wiping%20task%20requiring%20sustained%20contact%20pressure%2C%20our%20tactile-aware%20policy%20achieves%20a%2072%25%20success%20rate%2C%20outperforming%20vision-only%20baselines%20by%20eliminating%20contact-related%20failure%20modes.%20We%20release%20complete%20hardware%20designs%2C%20firmware%2C%20and%20assembly%20instructions%20to%20support%20community%20adoption.&entry.1838667208=http%3A//arxiv.org/abs/2512.08920v1&entry.124074799=Read"},
{"title": "AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models", "author": "Mohammad Zbeeb and Hasan Abed Al Kader Hammoud and Sina Mukalled and Nadine Rizk and Fatima Karnib and Issam Lakkis and Ammar Mohanna and Bernard Ghanem", "abstract": "We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.", "link": "http://arxiv.org/abs/2511.14295v2", "date": "2025-12-09", "relevancy": 2.2056, "topK": [{"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4445}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4445}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4344}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20AraLingBench%20A%20Human-Annotated%20Benchmark%20for%20Evaluating%20Arabic%20Linguistic%20Capabilities%20of%20Large%20Language%20Models&body=Title%3A%20AraLingBench%20A%20Human-Annotated%20Benchmark%20for%20Evaluating%20Arabic%20Linguistic%20Capabilities%20of%20Large%20Language%20Models%0AAuthor%3A%20Mohammad%20Zbeeb%20and%20Hasan%20Abed%20Al%20Kader%20Hammoud%20and%20Sina%20Mukalled%20and%20Nadine%20Rizk%20and%20Fatima%20Karnib%20and%20Issam%20Lakkis%20and%20Ammar%20Mohanna%20and%20Bernard%20Ghanem%0AAbstract%3A%20We%20present%20AraLingBench%3A%20a%20fully%20human%20annotated%20benchmark%20for%20evaluating%20the%20Arabic%20linguistic%20competence%20of%20large%20language%20models%20%28LLMs%29.%20The%20benchmark%20spans%20five%20core%20categories%3A%20grammar%2C%20morphology%2C%20spelling%2C%20reading%20comprehension%2C%20and%20syntax%2C%20through%20150%20expert-designed%20multiple%20choice%20questions%20that%20directly%20assess%20structural%20language%20understanding.%20Evaluating%2035%20Arabic%20and%20bilingual%20LLMs%20reveals%20that%20current%20models%20demonstrate%20strong%20surface%20level%20proficiency%20but%20struggle%20with%20deeper%20grammatical%20and%20syntactic%20reasoning.%20AraLingBench%20highlights%20a%20persistent%20gap%20between%20high%20scores%20on%20knowledge-based%20benchmarks%20and%20true%20linguistic%20mastery%2C%20showing%20that%20many%20models%20succeed%20through%20memorization%20or%20pattern%20recognition%20rather%20than%20authentic%20comprehension.%20By%20isolating%20and%20measuring%20fundamental%20linguistic%20skills%2C%20AraLingBench%20provides%20a%20diagnostic%20framework%20for%20developing%20Arabic%20LLMs.%20The%20full%20evaluation%20code%20is%20publicly%20available%20on%20GitHub.%0ALink%3A%20http%3A//arxiv.org/abs/2511.14295v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAraLingBench%2520A%2520Human-Annotated%2520Benchmark%2520for%2520Evaluating%2520Arabic%2520Linguistic%2520Capabilities%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DMohammad%2520Zbeeb%2520and%2520Hasan%2520Abed%2520Al%2520Kader%2520Hammoud%2520and%2520Sina%2520Mukalled%2520and%2520Nadine%2520Rizk%2520and%2520Fatima%2520Karnib%2520and%2520Issam%2520Lakkis%2520and%2520Ammar%2520Mohanna%2520and%2520Bernard%2520Ghanem%26entry.1292438233%3DWe%2520present%2520AraLingBench%253A%2520a%2520fully%2520human%2520annotated%2520benchmark%2520for%2520evaluating%2520the%2520Arabic%2520linguistic%2520competence%2520of%2520large%2520language%2520models%2520%2528LLMs%2529.%2520The%2520benchmark%2520spans%2520five%2520core%2520categories%253A%2520grammar%252C%2520morphology%252C%2520spelling%252C%2520reading%2520comprehension%252C%2520and%2520syntax%252C%2520through%2520150%2520expert-designed%2520multiple%2520choice%2520questions%2520that%2520directly%2520assess%2520structural%2520language%2520understanding.%2520Evaluating%252035%2520Arabic%2520and%2520bilingual%2520LLMs%2520reveals%2520that%2520current%2520models%2520demonstrate%2520strong%2520surface%2520level%2520proficiency%2520but%2520struggle%2520with%2520deeper%2520grammatical%2520and%2520syntactic%2520reasoning.%2520AraLingBench%2520highlights%2520a%2520persistent%2520gap%2520between%2520high%2520scores%2520on%2520knowledge-based%2520benchmarks%2520and%2520true%2520linguistic%2520mastery%252C%2520showing%2520that%2520many%2520models%2520succeed%2520through%2520memorization%2520or%2520pattern%2520recognition%2520rather%2520than%2520authentic%2520comprehension.%2520By%2520isolating%2520and%2520measuring%2520fundamental%2520linguistic%2520skills%252C%2520AraLingBench%2520provides%2520a%2520diagnostic%2520framework%2520for%2520developing%2520Arabic%2520LLMs.%2520The%2520full%2520evaluation%2520code%2520is%2520publicly%2520available%2520on%2520GitHub.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2511.14295v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=AraLingBench%20A%20Human-Annotated%20Benchmark%20for%20Evaluating%20Arabic%20Linguistic%20Capabilities%20of%20Large%20Language%20Models&entry.906535625=Mohammad%20Zbeeb%20and%20Hasan%20Abed%20Al%20Kader%20Hammoud%20and%20Sina%20Mukalled%20and%20Nadine%20Rizk%20and%20Fatima%20Karnib%20and%20Issam%20Lakkis%20and%20Ammar%20Mohanna%20and%20Bernard%20Ghanem&entry.1292438233=We%20present%20AraLingBench%3A%20a%20fully%20human%20annotated%20benchmark%20for%20evaluating%20the%20Arabic%20linguistic%20competence%20of%20large%20language%20models%20%28LLMs%29.%20The%20benchmark%20spans%20five%20core%20categories%3A%20grammar%2C%20morphology%2C%20spelling%2C%20reading%20comprehension%2C%20and%20syntax%2C%20through%20150%20expert-designed%20multiple%20choice%20questions%20that%20directly%20assess%20structural%20language%20understanding.%20Evaluating%2035%20Arabic%20and%20bilingual%20LLMs%20reveals%20that%20current%20models%20demonstrate%20strong%20surface%20level%20proficiency%20but%20struggle%20with%20deeper%20grammatical%20and%20syntactic%20reasoning.%20AraLingBench%20highlights%20a%20persistent%20gap%20between%20high%20scores%20on%20knowledge-based%20benchmarks%20and%20true%20linguistic%20mastery%2C%20showing%20that%20many%20models%20succeed%20through%20memorization%20or%20pattern%20recognition%20rather%20than%20authentic%20comprehension.%20By%20isolating%20and%20measuring%20fundamental%20linguistic%20skills%2C%20AraLingBench%20provides%20a%20diagnostic%20framework%20for%20developing%20Arabic%20LLMs.%20The%20full%20evaluation%20code%20is%20publicly%20available%20on%20GitHub.&entry.1838667208=http%3A//arxiv.org/abs/2511.14295v2&entry.124074799=Read"},
{"title": "SimSUM: Simulated Benchmark with Structured and Unstructured Medical Records", "author": "Paloma Rabaey and Stefan Heytens and Thomas Demeester", "abstract": "Clinical information extraction, which involves structuring clinical concepts from unstructured medical text, remains a challenging problem that could benefit from the inclusion of tabular background information available in electronic health records. Existing open-source datasets lack explicit links between structured features and clinical concepts in the text, motivating the need for a new research dataset. We introduce SimSUM, a benchmark dataset of 10,000 simulated patient records that link unstructured clinical notes with structured background variables. Each record simulates a patient encounter in the domain of respiratory diseases and includes tabular data (e.g., symptoms, diagnoses, underlying conditions) generated from a Bayesian network whose structure and parameters are defined by domain experts. A large language model (GPT-4o) is prompted to generate a clinical note describing the encounter, including symptoms and relevant context. These notes are annotated with span-level symptom mentions. We conduct an expert evaluation to assess note quality and run baseline predictive models on both the tabular and textual data. The SimSUM dataset is primarily designed to support research on clinical information extraction in the presence of tabular background variables, which can be linked through domain knowledge to concepts of interest to be extracted from the text -- namely, symptoms in the case of SimSUM. Secondary uses include research on the automation of clinical reasoning over both tabular data and text, causal effect estimation in the presence of tabular and/or textual confounders, and multi-modal synthetic data generation. SimSUM is not intended for training clinical decision support systems or production-grade models, but rather to facilitate reproducible research in a simplified and controlled setting.", "link": "http://arxiv.org/abs/2409.08936v4", "date": "2025-12-09", "relevancy": 2.1929, "topK": [{"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.4427}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4365}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.4365}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SimSUM%3A%20Simulated%20Benchmark%20with%20Structured%20and%20Unstructured%20Medical%20Records&body=Title%3A%20SimSUM%3A%20Simulated%20Benchmark%20with%20Structured%20and%20Unstructured%20Medical%20Records%0AAuthor%3A%20Paloma%20Rabaey%20and%20Stefan%20Heytens%20and%20Thomas%20Demeester%0AAbstract%3A%20Clinical%20information%20extraction%2C%20which%20involves%20structuring%20clinical%20concepts%20from%20unstructured%20medical%20text%2C%20remains%20a%20challenging%20problem%20that%20could%20benefit%20from%20the%20inclusion%20of%20tabular%20background%20information%20available%20in%20electronic%20health%20records.%20Existing%20open-source%20datasets%20lack%20explicit%20links%20between%20structured%20features%20and%20clinical%20concepts%20in%20the%20text%2C%20motivating%20the%20need%20for%20a%20new%20research%20dataset.%20We%20introduce%20SimSUM%2C%20a%20benchmark%20dataset%20of%2010%2C000%20simulated%20patient%20records%20that%20link%20unstructured%20clinical%20notes%20with%20structured%20background%20variables.%20Each%20record%20simulates%20a%20patient%20encounter%20in%20the%20domain%20of%20respiratory%20diseases%20and%20includes%20tabular%20data%20%28e.g.%2C%20symptoms%2C%20diagnoses%2C%20underlying%20conditions%29%20generated%20from%20a%20Bayesian%20network%20whose%20structure%20and%20parameters%20are%20defined%20by%20domain%20experts.%20A%20large%20language%20model%20%28GPT-4o%29%20is%20prompted%20to%20generate%20a%20clinical%20note%20describing%20the%20encounter%2C%20including%20symptoms%20and%20relevant%20context.%20These%20notes%20are%20annotated%20with%20span-level%20symptom%20mentions.%20We%20conduct%20an%20expert%20evaluation%20to%20assess%20note%20quality%20and%20run%20baseline%20predictive%20models%20on%20both%20the%20tabular%20and%20textual%20data.%20The%20SimSUM%20dataset%20is%20primarily%20designed%20to%20support%20research%20on%20clinical%20information%20extraction%20in%20the%20presence%20of%20tabular%20background%20variables%2C%20which%20can%20be%20linked%20through%20domain%20knowledge%20to%20concepts%20of%20interest%20to%20be%20extracted%20from%20the%20text%20--%20namely%2C%20symptoms%20in%20the%20case%20of%20SimSUM.%20Secondary%20uses%20include%20research%20on%20the%20automation%20of%20clinical%20reasoning%20over%20both%20tabular%20data%20and%20text%2C%20causal%20effect%20estimation%20in%20the%20presence%20of%20tabular%20and/or%20textual%20confounders%2C%20and%20multi-modal%20synthetic%20data%20generation.%20SimSUM%20is%20not%20intended%20for%20training%20clinical%20decision%20support%20systems%20or%20production-grade%20models%2C%20but%20rather%20to%20facilitate%20reproducible%20research%20in%20a%20simplified%20and%20controlled%20setting.%0ALink%3A%20http%3A//arxiv.org/abs/2409.08936v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSimSUM%253A%2520Simulated%2520Benchmark%2520with%2520Structured%2520and%2520Unstructured%2520Medical%2520Records%26entry.906535625%3DPaloma%2520Rabaey%2520and%2520Stefan%2520Heytens%2520and%2520Thomas%2520Demeester%26entry.1292438233%3DClinical%2520information%2520extraction%252C%2520which%2520involves%2520structuring%2520clinical%2520concepts%2520from%2520unstructured%2520medical%2520text%252C%2520remains%2520a%2520challenging%2520problem%2520that%2520could%2520benefit%2520from%2520the%2520inclusion%2520of%2520tabular%2520background%2520information%2520available%2520in%2520electronic%2520health%2520records.%2520Existing%2520open-source%2520datasets%2520lack%2520explicit%2520links%2520between%2520structured%2520features%2520and%2520clinical%2520concepts%2520in%2520the%2520text%252C%2520motivating%2520the%2520need%2520for%2520a%2520new%2520research%2520dataset.%2520We%2520introduce%2520SimSUM%252C%2520a%2520benchmark%2520dataset%2520of%252010%252C000%2520simulated%2520patient%2520records%2520that%2520link%2520unstructured%2520clinical%2520notes%2520with%2520structured%2520background%2520variables.%2520Each%2520record%2520simulates%2520a%2520patient%2520encounter%2520in%2520the%2520domain%2520of%2520respiratory%2520diseases%2520and%2520includes%2520tabular%2520data%2520%2528e.g.%252C%2520symptoms%252C%2520diagnoses%252C%2520underlying%2520conditions%2529%2520generated%2520from%2520a%2520Bayesian%2520network%2520whose%2520structure%2520and%2520parameters%2520are%2520defined%2520by%2520domain%2520experts.%2520A%2520large%2520language%2520model%2520%2528GPT-4o%2529%2520is%2520prompted%2520to%2520generate%2520a%2520clinical%2520note%2520describing%2520the%2520encounter%252C%2520including%2520symptoms%2520and%2520relevant%2520context.%2520These%2520notes%2520are%2520annotated%2520with%2520span-level%2520symptom%2520mentions.%2520We%2520conduct%2520an%2520expert%2520evaluation%2520to%2520assess%2520note%2520quality%2520and%2520run%2520baseline%2520predictive%2520models%2520on%2520both%2520the%2520tabular%2520and%2520textual%2520data.%2520The%2520SimSUM%2520dataset%2520is%2520primarily%2520designed%2520to%2520support%2520research%2520on%2520clinical%2520information%2520extraction%2520in%2520the%2520presence%2520of%2520tabular%2520background%2520variables%252C%2520which%2520can%2520be%2520linked%2520through%2520domain%2520knowledge%2520to%2520concepts%2520of%2520interest%2520to%2520be%2520extracted%2520from%2520the%2520text%2520--%2520namely%252C%2520symptoms%2520in%2520the%2520case%2520of%2520SimSUM.%2520Secondary%2520uses%2520include%2520research%2520on%2520the%2520automation%2520of%2520clinical%2520reasoning%2520over%2520both%2520tabular%2520data%2520and%2520text%252C%2520causal%2520effect%2520estimation%2520in%2520the%2520presence%2520of%2520tabular%2520and/or%2520textual%2520confounders%252C%2520and%2520multi-modal%2520synthetic%2520data%2520generation.%2520SimSUM%2520is%2520not%2520intended%2520for%2520training%2520clinical%2520decision%2520support%2520systems%2520or%2520production-grade%2520models%252C%2520but%2520rather%2520to%2520facilitate%2520reproducible%2520research%2520in%2520a%2520simplified%2520and%2520controlled%2520setting.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2409.08936v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SimSUM%3A%20Simulated%20Benchmark%20with%20Structured%20and%20Unstructured%20Medical%20Records&entry.906535625=Paloma%20Rabaey%20and%20Stefan%20Heytens%20and%20Thomas%20Demeester&entry.1292438233=Clinical%20information%20extraction%2C%20which%20involves%20structuring%20clinical%20concepts%20from%20unstructured%20medical%20text%2C%20remains%20a%20challenging%20problem%20that%20could%20benefit%20from%20the%20inclusion%20of%20tabular%20background%20information%20available%20in%20electronic%20health%20records.%20Existing%20open-source%20datasets%20lack%20explicit%20links%20between%20structured%20features%20and%20clinical%20concepts%20in%20the%20text%2C%20motivating%20the%20need%20for%20a%20new%20research%20dataset.%20We%20introduce%20SimSUM%2C%20a%20benchmark%20dataset%20of%2010%2C000%20simulated%20patient%20records%20that%20link%20unstructured%20clinical%20notes%20with%20structured%20background%20variables.%20Each%20record%20simulates%20a%20patient%20encounter%20in%20the%20domain%20of%20respiratory%20diseases%20and%20includes%20tabular%20data%20%28e.g.%2C%20symptoms%2C%20diagnoses%2C%20underlying%20conditions%29%20generated%20from%20a%20Bayesian%20network%20whose%20structure%20and%20parameters%20are%20defined%20by%20domain%20experts.%20A%20large%20language%20model%20%28GPT-4o%29%20is%20prompted%20to%20generate%20a%20clinical%20note%20describing%20the%20encounter%2C%20including%20symptoms%20and%20relevant%20context.%20These%20notes%20are%20annotated%20with%20span-level%20symptom%20mentions.%20We%20conduct%20an%20expert%20evaluation%20to%20assess%20note%20quality%20and%20run%20baseline%20predictive%20models%20on%20both%20the%20tabular%20and%20textual%20data.%20The%20SimSUM%20dataset%20is%20primarily%20designed%20to%20support%20research%20on%20clinical%20information%20extraction%20in%20the%20presence%20of%20tabular%20background%20variables%2C%20which%20can%20be%20linked%20through%20domain%20knowledge%20to%20concepts%20of%20interest%20to%20be%20extracted%20from%20the%20text%20--%20namely%2C%20symptoms%20in%20the%20case%20of%20SimSUM.%20Secondary%20uses%20include%20research%20on%20the%20automation%20of%20clinical%20reasoning%20over%20both%20tabular%20data%20and%20text%2C%20causal%20effect%20estimation%20in%20the%20presence%20of%20tabular%20and/or%20textual%20confounders%2C%20and%20multi-modal%20synthetic%20data%20generation.%20SimSUM%20is%20not%20intended%20for%20training%20clinical%20decision%20support%20systems%20or%20production-grade%20models%2C%20but%20rather%20to%20facilitate%20reproducible%20research%20in%20a%20simplified%20and%20controlled%20setting.&entry.1838667208=http%3A//arxiv.org/abs/2409.08936v4&entry.124074799=Read"},
{"title": "DIVER: Reinforced Diffusion Breaks Imitation Bottlenecks in End-to-End Autonomous Driving", "author": "Ziying Song and Lin Liu and Hongyu Pan and Bencheng Liao and Mingzhe Guo and Lei Yang and Yongchang Zhang and Shaoqing Xu and Caiyan Jia and Yadan Luo", "abstract": "Most end-to-end autonomous driving methods rely on imitation learning from single expert demonstrations, often leading to conservative and homogeneous behaviors that limit generalization in complex real-world scenarios. In this work, we propose DIVER, an end-to-end driving framework that integrates reinforcement learning with diffusion-based generation to produce diverse and feasible trajectories. At the core of DIVER lies a reinforced diffusion-based generation mechanism. First, the model conditions on map elements and surrounding agents to generate multiple reference trajectories from a single ground-truth trajectory, alleviating the limitations of imitation learning that arise from relying solely on single expert demonstrations. Second, reinforcement learning is employed to guide the diffusion process, where reward-based supervision enforces safety and diversity constraints on the generated trajectories, thereby enhancing their practicality and generalization capability. Furthermore, to address the limitations of L2-based open-loop metrics in capturing trajectory diversity, we propose a novel Diversity metric to evaluate the diversity of multi-mode predictions.Extensive experiments on the closed-loop NAVSIM and Bench2Drive benchmarks, as well as the open-loop nuScenes dataset, demonstrate that DIVER significantly improves trajectory diversity, effectively addressing the mode collapse problem inherent in imitation learning.", "link": "http://arxiv.org/abs/2507.04049v3", "date": "2025-12-09", "relevancy": 2.1848, "topK": [{"title": "WorldExplorer: Towards Generating Fully Navigable 3D Scenes", "link": "http://arxiv.org/abs/2506.01799v2", "similarity": 0.5725}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5426}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.5393}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIVER%3A%20Reinforced%20Diffusion%20Breaks%20Imitation%20Bottlenecks%20in%20End-to-End%20Autonomous%20Driving&body=Title%3A%20DIVER%3A%20Reinforced%20Diffusion%20Breaks%20Imitation%20Bottlenecks%20in%20End-to-End%20Autonomous%20Driving%0AAuthor%3A%20Ziying%20Song%20and%20Lin%20Liu%20and%20Hongyu%20Pan%20and%20Bencheng%20Liao%20and%20Mingzhe%20Guo%20and%20Lei%20Yang%20and%20Yongchang%20Zhang%20and%20Shaoqing%20Xu%20and%20Caiyan%20Jia%20and%20Yadan%20Luo%0AAbstract%3A%20Most%20end-to-end%20autonomous%20driving%20methods%20rely%20on%20imitation%20learning%20from%20single%20expert%20demonstrations%2C%20often%20leading%20to%20conservative%20and%20homogeneous%20behaviors%20that%20limit%20generalization%20in%20complex%20real-world%20scenarios.%20In%20this%20work%2C%20we%20propose%20DIVER%2C%20an%20end-to-end%20driving%20framework%20that%20integrates%20reinforcement%20learning%20with%20diffusion-based%20generation%20to%20produce%20diverse%20and%20feasible%20trajectories.%20At%20the%20core%20of%20DIVER%20lies%20a%20reinforced%20diffusion-based%20generation%20mechanism.%20First%2C%20the%20model%20conditions%20on%20map%20elements%20and%20surrounding%20agents%20to%20generate%20multiple%20reference%20trajectories%20from%20a%20single%20ground-truth%20trajectory%2C%20alleviating%20the%20limitations%20of%20imitation%20learning%20that%20arise%20from%20relying%20solely%20on%20single%20expert%20demonstrations.%20Second%2C%20reinforcement%20learning%20is%20employed%20to%20guide%20the%20diffusion%20process%2C%20where%20reward-based%20supervision%20enforces%20safety%20and%20diversity%20constraints%20on%20the%20generated%20trajectories%2C%20thereby%20enhancing%20their%20practicality%20and%20generalization%20capability.%20Furthermore%2C%20to%20address%20the%20limitations%20of%20L2-based%20open-loop%20metrics%20in%20capturing%20trajectory%20diversity%2C%20we%20propose%20a%20novel%20Diversity%20metric%20to%20evaluate%20the%20diversity%20of%20multi-mode%20predictions.Extensive%20experiments%20on%20the%20closed-loop%20NAVSIM%20and%20Bench2Drive%20benchmarks%2C%20as%20well%20as%20the%20open-loop%20nuScenes%20dataset%2C%20demonstrate%20that%20DIVER%20significantly%20improves%20trajectory%20diversity%2C%20effectively%20addressing%20the%20mode%20collapse%20problem%20inherent%20in%20imitation%20learning.%0ALink%3A%20http%3A//arxiv.org/abs/2507.04049v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIVER%253A%2520Reinforced%2520Diffusion%2520Breaks%2520Imitation%2520Bottlenecks%2520in%2520End-to-End%2520Autonomous%2520Driving%26entry.906535625%3DZiying%2520Song%2520and%2520Lin%2520Liu%2520and%2520Hongyu%2520Pan%2520and%2520Bencheng%2520Liao%2520and%2520Mingzhe%2520Guo%2520and%2520Lei%2520Yang%2520and%2520Yongchang%2520Zhang%2520and%2520Shaoqing%2520Xu%2520and%2520Caiyan%2520Jia%2520and%2520Yadan%2520Luo%26entry.1292438233%3DMost%2520end-to-end%2520autonomous%2520driving%2520methods%2520rely%2520on%2520imitation%2520learning%2520from%2520single%2520expert%2520demonstrations%252C%2520often%2520leading%2520to%2520conservative%2520and%2520homogeneous%2520behaviors%2520that%2520limit%2520generalization%2520in%2520complex%2520real-world%2520scenarios.%2520In%2520this%2520work%252C%2520we%2520propose%2520DIVER%252C%2520an%2520end-to-end%2520driving%2520framework%2520that%2520integrates%2520reinforcement%2520learning%2520with%2520diffusion-based%2520generation%2520to%2520produce%2520diverse%2520and%2520feasible%2520trajectories.%2520At%2520the%2520core%2520of%2520DIVER%2520lies%2520a%2520reinforced%2520diffusion-based%2520generation%2520mechanism.%2520First%252C%2520the%2520model%2520conditions%2520on%2520map%2520elements%2520and%2520surrounding%2520agents%2520to%2520generate%2520multiple%2520reference%2520trajectories%2520from%2520a%2520single%2520ground-truth%2520trajectory%252C%2520alleviating%2520the%2520limitations%2520of%2520imitation%2520learning%2520that%2520arise%2520from%2520relying%2520solely%2520on%2520single%2520expert%2520demonstrations.%2520Second%252C%2520reinforcement%2520learning%2520is%2520employed%2520to%2520guide%2520the%2520diffusion%2520process%252C%2520where%2520reward-based%2520supervision%2520enforces%2520safety%2520and%2520diversity%2520constraints%2520on%2520the%2520generated%2520trajectories%252C%2520thereby%2520enhancing%2520their%2520practicality%2520and%2520generalization%2520capability.%2520Furthermore%252C%2520to%2520address%2520the%2520limitations%2520of%2520L2-based%2520open-loop%2520metrics%2520in%2520capturing%2520trajectory%2520diversity%252C%2520we%2520propose%2520a%2520novel%2520Diversity%2520metric%2520to%2520evaluate%2520the%2520diversity%2520of%2520multi-mode%2520predictions.Extensive%2520experiments%2520on%2520the%2520closed-loop%2520NAVSIM%2520and%2520Bench2Drive%2520benchmarks%252C%2520as%2520well%2520as%2520the%2520open-loop%2520nuScenes%2520dataset%252C%2520demonstrate%2520that%2520DIVER%2520significantly%2520improves%2520trajectory%2520diversity%252C%2520effectively%2520addressing%2520the%2520mode%2520collapse%2520problem%2520inherent%2520in%2520imitation%2520learning.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2507.04049v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIVER%3A%20Reinforced%20Diffusion%20Breaks%20Imitation%20Bottlenecks%20in%20End-to-End%20Autonomous%20Driving&entry.906535625=Ziying%20Song%20and%20Lin%20Liu%20and%20Hongyu%20Pan%20and%20Bencheng%20Liao%20and%20Mingzhe%20Guo%20and%20Lei%20Yang%20and%20Yongchang%20Zhang%20and%20Shaoqing%20Xu%20and%20Caiyan%20Jia%20and%20Yadan%20Luo&entry.1292438233=Most%20end-to-end%20autonomous%20driving%20methods%20rely%20on%20imitation%20learning%20from%20single%20expert%20demonstrations%2C%20often%20leading%20to%20conservative%20and%20homogeneous%20behaviors%20that%20limit%20generalization%20in%20complex%20real-world%20scenarios.%20In%20this%20work%2C%20we%20propose%20DIVER%2C%20an%20end-to-end%20driving%20framework%20that%20integrates%20reinforcement%20learning%20with%20diffusion-based%20generation%20to%20produce%20diverse%20and%20feasible%20trajectories.%20At%20the%20core%20of%20DIVER%20lies%20a%20reinforced%20diffusion-based%20generation%20mechanism.%20First%2C%20the%20model%20conditions%20on%20map%20elements%20and%20surrounding%20agents%20to%20generate%20multiple%20reference%20trajectories%20from%20a%20single%20ground-truth%20trajectory%2C%20alleviating%20the%20limitations%20of%20imitation%20learning%20that%20arise%20from%20relying%20solely%20on%20single%20expert%20demonstrations.%20Second%2C%20reinforcement%20learning%20is%20employed%20to%20guide%20the%20diffusion%20process%2C%20where%20reward-based%20supervision%20enforces%20safety%20and%20diversity%20constraints%20on%20the%20generated%20trajectories%2C%20thereby%20enhancing%20their%20practicality%20and%20generalization%20capability.%20Furthermore%2C%20to%20address%20the%20limitations%20of%20L2-based%20open-loop%20metrics%20in%20capturing%20trajectory%20diversity%2C%20we%20propose%20a%20novel%20Diversity%20metric%20to%20evaluate%20the%20diversity%20of%20multi-mode%20predictions.Extensive%20experiments%20on%20the%20closed-loop%20NAVSIM%20and%20Bench2Drive%20benchmarks%2C%20as%20well%20as%20the%20open-loop%20nuScenes%20dataset%2C%20demonstrate%20that%20DIVER%20significantly%20improves%20trajectory%20diversity%2C%20effectively%20addressing%20the%20mode%20collapse%20problem%20inherent%20in%20imitation%20learning.&entry.1838667208=http%3A//arxiv.org/abs/2507.04049v3&entry.124074799=Read"},
{"title": "Performance Comparison of Aerial RIS and STAR-RIS in 3D Wireless Environments", "author": "Dongdong Yang and Bin Li and Jiguang He", "abstract": "Reconfigurable intelligent surface (RIS) and simultaneously transmitting and reflecting RIS (STAR-RIS) have emerged as key enablers for enhancing wireless coverage and capacity in next-generation networks. When mounted on unmanned aerial vehicles (UAVs), they benefit from flexible deployment and improved line-of-sight conditions. Despite their promising potential, a comprehensive performance comparison between aerial RIS and STAR-RIS architectures has not been thoroughly investigated. This letter presents a detailed performance comparison between aerial RIS and STAR-RIS in three-dimensional wireless environments. Accurate channel models incorporating directional radiation patterns are established, and the influence of deployment altitude and orientation is thoroughly examined. To optimize the system sum-rate, we formulate joint optimization problems for both architectures and propose an efficient solution based on the weighted minimum mean square error and block coordinate descent algorithms. Simulation results reveal that STAR-RIS outperforms RIS in low-altitude scenarios due to its full-space coverage capability, whereas RIS delivers better performance near the base station at higher altitudes. The findings provide practical insights for the deployment of aerial intelligent surfaces in future 6G communication systems.", "link": "http://arxiv.org/abs/2512.08755v1", "date": "2025-12-09", "relevancy": 2.1784, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.477}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.417}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.413}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Performance%20Comparison%20of%20Aerial%20RIS%20and%20STAR-RIS%20in%203D%20Wireless%20Environments&body=Title%3A%20Performance%20Comparison%20of%20Aerial%20RIS%20and%20STAR-RIS%20in%203D%20Wireless%20Environments%0AAuthor%3A%20Dongdong%20Yang%20and%20Bin%20Li%20and%20Jiguang%20He%0AAbstract%3A%20Reconfigurable%20intelligent%20surface%20%28RIS%29%20and%20simultaneously%20transmitting%20and%20reflecting%20RIS%20%28STAR-RIS%29%20have%20emerged%20as%20key%20enablers%20for%20enhancing%20wireless%20coverage%20and%20capacity%20in%20next-generation%20networks.%20When%20mounted%20on%20unmanned%20aerial%20vehicles%20%28UAVs%29%2C%20they%20benefit%20from%20flexible%20deployment%20and%20improved%20line-of-sight%20conditions.%20Despite%20their%20promising%20potential%2C%20a%20comprehensive%20performance%20comparison%20between%20aerial%20RIS%20and%20STAR-RIS%20architectures%20has%20not%20been%20thoroughly%20investigated.%20This%20letter%20presents%20a%20detailed%20performance%20comparison%20between%20aerial%20RIS%20and%20STAR-RIS%20in%20three-dimensional%20wireless%20environments.%20Accurate%20channel%20models%20incorporating%20directional%20radiation%20patterns%20are%20established%2C%20and%20the%20influence%20of%20deployment%20altitude%20and%20orientation%20is%20thoroughly%20examined.%20To%20optimize%20the%20system%20sum-rate%2C%20we%20formulate%20joint%20optimization%20problems%20for%20both%20architectures%20and%20propose%20an%20efficient%20solution%20based%20on%20the%20weighted%20minimum%20mean%20square%20error%20and%20block%20coordinate%20descent%20algorithms.%20Simulation%20results%20reveal%20that%20STAR-RIS%20outperforms%20RIS%20in%20low-altitude%20scenarios%20due%20to%20its%20full-space%20coverage%20capability%2C%20whereas%20RIS%20delivers%20better%20performance%20near%20the%20base%20station%20at%20higher%20altitudes.%20The%20findings%20provide%20practical%20insights%20for%20the%20deployment%20of%20aerial%20intelligent%20surfaces%20in%20future%206G%20communication%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08755v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPerformance%2520Comparison%2520of%2520Aerial%2520RIS%2520and%2520STAR-RIS%2520in%25203D%2520Wireless%2520Environments%26entry.906535625%3DDongdong%2520Yang%2520and%2520Bin%2520Li%2520and%2520Jiguang%2520He%26entry.1292438233%3DReconfigurable%2520intelligent%2520surface%2520%2528RIS%2529%2520and%2520simultaneously%2520transmitting%2520and%2520reflecting%2520RIS%2520%2528STAR-RIS%2529%2520have%2520emerged%2520as%2520key%2520enablers%2520for%2520enhancing%2520wireless%2520coverage%2520and%2520capacity%2520in%2520next-generation%2520networks.%2520When%2520mounted%2520on%2520unmanned%2520aerial%2520vehicles%2520%2528UAVs%2529%252C%2520they%2520benefit%2520from%2520flexible%2520deployment%2520and%2520improved%2520line-of-sight%2520conditions.%2520Despite%2520their%2520promising%2520potential%252C%2520a%2520comprehensive%2520performance%2520comparison%2520between%2520aerial%2520RIS%2520and%2520STAR-RIS%2520architectures%2520has%2520not%2520been%2520thoroughly%2520investigated.%2520This%2520letter%2520presents%2520a%2520detailed%2520performance%2520comparison%2520between%2520aerial%2520RIS%2520and%2520STAR-RIS%2520in%2520three-dimensional%2520wireless%2520environments.%2520Accurate%2520channel%2520models%2520incorporating%2520directional%2520radiation%2520patterns%2520are%2520established%252C%2520and%2520the%2520influence%2520of%2520deployment%2520altitude%2520and%2520orientation%2520is%2520thoroughly%2520examined.%2520To%2520optimize%2520the%2520system%2520sum-rate%252C%2520we%2520formulate%2520joint%2520optimization%2520problems%2520for%2520both%2520architectures%2520and%2520propose%2520an%2520efficient%2520solution%2520based%2520on%2520the%2520weighted%2520minimum%2520mean%2520square%2520error%2520and%2520block%2520coordinate%2520descent%2520algorithms.%2520Simulation%2520results%2520reveal%2520that%2520STAR-RIS%2520outperforms%2520RIS%2520in%2520low-altitude%2520scenarios%2520due%2520to%2520its%2520full-space%2520coverage%2520capability%252C%2520whereas%2520RIS%2520delivers%2520better%2520performance%2520near%2520the%2520base%2520station%2520at%2520higher%2520altitudes.%2520The%2520findings%2520provide%2520practical%2520insights%2520for%2520the%2520deployment%2520of%2520aerial%2520intelligent%2520surfaces%2520in%2520future%25206G%2520communication%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08755v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Performance%20Comparison%20of%20Aerial%20RIS%20and%20STAR-RIS%20in%203D%20Wireless%20Environments&entry.906535625=Dongdong%20Yang%20and%20Bin%20Li%20and%20Jiguang%20He&entry.1292438233=Reconfigurable%20intelligent%20surface%20%28RIS%29%20and%20simultaneously%20transmitting%20and%20reflecting%20RIS%20%28STAR-RIS%29%20have%20emerged%20as%20key%20enablers%20for%20enhancing%20wireless%20coverage%20and%20capacity%20in%20next-generation%20networks.%20When%20mounted%20on%20unmanned%20aerial%20vehicles%20%28UAVs%29%2C%20they%20benefit%20from%20flexible%20deployment%20and%20improved%20line-of-sight%20conditions.%20Despite%20their%20promising%20potential%2C%20a%20comprehensive%20performance%20comparison%20between%20aerial%20RIS%20and%20STAR-RIS%20architectures%20has%20not%20been%20thoroughly%20investigated.%20This%20letter%20presents%20a%20detailed%20performance%20comparison%20between%20aerial%20RIS%20and%20STAR-RIS%20in%20three-dimensional%20wireless%20environments.%20Accurate%20channel%20models%20incorporating%20directional%20radiation%20patterns%20are%20established%2C%20and%20the%20influence%20of%20deployment%20altitude%20and%20orientation%20is%20thoroughly%20examined.%20To%20optimize%20the%20system%20sum-rate%2C%20we%20formulate%20joint%20optimization%20problems%20for%20both%20architectures%20and%20propose%20an%20efficient%20solution%20based%20on%20the%20weighted%20minimum%20mean%20square%20error%20and%20block%20coordinate%20descent%20algorithms.%20Simulation%20results%20reveal%20that%20STAR-RIS%20outperforms%20RIS%20in%20low-altitude%20scenarios%20due%20to%20its%20full-space%20coverage%20capability%2C%20whereas%20RIS%20delivers%20better%20performance%20near%20the%20base%20station%20at%20higher%20altitudes.%20The%20findings%20provide%20practical%20insights%20for%20the%20deployment%20of%20aerial%20intelligent%20surfaces%20in%20future%206G%20communication%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.08755v1&entry.124074799=Read"},
{"title": "From Cells to Survival: Hierarchical Analysis of Cell Inter-Relations in Multiplex Microscopy for Lung Cancer Prognosis", "author": "Olle Edgren Sch\u00fcllerqvist and Jens Baumann and Joakim Lindblad and Love Nordling and Artur Mezheyeuski and Patrick Micke and Nata\u0161a Sladoje", "abstract": "The tumor microenvironment (TME) has emerged as a promising source of prognostic biomarkers. To fully leverage its potential, analysis methods must capture complex interactions between different cell types. We propose HiGINE -- a hierarchical graph-based approach to predict patient survival (short vs. long) from TME characterization in multiplex immunofluorescence (mIF) images and enhance risk stratification in lung cancer. Our model encodes both local and global inter-relations in cell neighborhoods, incorporating information about cell types and morphology. Multimodal fusion, aggregating cancer stage with mIF-derived features, further boosts performance. We validate HiGINE on two public datasets, demonstrating improved risk stratification, robustness, and generalizability.", "link": "http://arxiv.org/abs/2512.08572v1", "date": "2025-12-09", "relevancy": 2.1752, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4372}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4367}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20From%20Cells%20to%20Survival%3A%20Hierarchical%20Analysis%20of%20Cell%20Inter-Relations%20in%20Multiplex%20Microscopy%20for%20Lung%20Cancer%20Prognosis&body=Title%3A%20From%20Cells%20to%20Survival%3A%20Hierarchical%20Analysis%20of%20Cell%20Inter-Relations%20in%20Multiplex%20Microscopy%20for%20Lung%20Cancer%20Prognosis%0AAuthor%3A%20Olle%20Edgren%20Sch%C3%BCllerqvist%20and%20Jens%20Baumann%20and%20Joakim%20Lindblad%20and%20Love%20Nordling%20and%20Artur%20Mezheyeuski%20and%20Patrick%20Micke%20and%20Nata%C5%A1a%20Sladoje%0AAbstract%3A%20The%20tumor%20microenvironment%20%28TME%29%20has%20emerged%20as%20a%20promising%20source%20of%20prognostic%20biomarkers.%20To%20fully%20leverage%20its%20potential%2C%20analysis%20methods%20must%20capture%20complex%20interactions%20between%20different%20cell%20types.%20We%20propose%20HiGINE%20--%20a%20hierarchical%20graph-based%20approach%20to%20predict%20patient%20survival%20%28short%20vs.%20long%29%20from%20TME%20characterization%20in%20multiplex%20immunofluorescence%20%28mIF%29%20images%20and%20enhance%20risk%20stratification%20in%20lung%20cancer.%20Our%20model%20encodes%20both%20local%20and%20global%20inter-relations%20in%20cell%20neighborhoods%2C%20incorporating%20information%20about%20cell%20types%20and%20morphology.%20Multimodal%20fusion%2C%20aggregating%20cancer%20stage%20with%20mIF-derived%20features%2C%20further%20boosts%20performance.%20We%20validate%20HiGINE%20on%20two%20public%20datasets%2C%20demonstrating%20improved%20risk%20stratification%2C%20robustness%2C%20and%20generalizability.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08572v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFrom%2520Cells%2520to%2520Survival%253A%2520Hierarchical%2520Analysis%2520of%2520Cell%2520Inter-Relations%2520in%2520Multiplex%2520Microscopy%2520for%2520Lung%2520Cancer%2520Prognosis%26entry.906535625%3DOlle%2520Edgren%2520Sch%25C3%25BCllerqvist%2520and%2520Jens%2520Baumann%2520and%2520Joakim%2520Lindblad%2520and%2520Love%2520Nordling%2520and%2520Artur%2520Mezheyeuski%2520and%2520Patrick%2520Micke%2520and%2520Nata%25C5%25A1a%2520Sladoje%26entry.1292438233%3DThe%2520tumor%2520microenvironment%2520%2528TME%2529%2520has%2520emerged%2520as%2520a%2520promising%2520source%2520of%2520prognostic%2520biomarkers.%2520To%2520fully%2520leverage%2520its%2520potential%252C%2520analysis%2520methods%2520must%2520capture%2520complex%2520interactions%2520between%2520different%2520cell%2520types.%2520We%2520propose%2520HiGINE%2520--%2520a%2520hierarchical%2520graph-based%2520approach%2520to%2520predict%2520patient%2520survival%2520%2528short%2520vs.%2520long%2529%2520from%2520TME%2520characterization%2520in%2520multiplex%2520immunofluorescence%2520%2528mIF%2529%2520images%2520and%2520enhance%2520risk%2520stratification%2520in%2520lung%2520cancer.%2520Our%2520model%2520encodes%2520both%2520local%2520and%2520global%2520inter-relations%2520in%2520cell%2520neighborhoods%252C%2520incorporating%2520information%2520about%2520cell%2520types%2520and%2520morphology.%2520Multimodal%2520fusion%252C%2520aggregating%2520cancer%2520stage%2520with%2520mIF-derived%2520features%252C%2520further%2520boosts%2520performance.%2520We%2520validate%2520HiGINE%2520on%2520two%2520public%2520datasets%252C%2520demonstrating%2520improved%2520risk%2520stratification%252C%2520robustness%252C%2520and%2520generalizability.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08572v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=From%20Cells%20to%20Survival%3A%20Hierarchical%20Analysis%20of%20Cell%20Inter-Relations%20in%20Multiplex%20Microscopy%20for%20Lung%20Cancer%20Prognosis&entry.906535625=Olle%20Edgren%20Sch%C3%BCllerqvist%20and%20Jens%20Baumann%20and%20Joakim%20Lindblad%20and%20Love%20Nordling%20and%20Artur%20Mezheyeuski%20and%20Patrick%20Micke%20and%20Nata%C5%A1a%20Sladoje&entry.1292438233=The%20tumor%20microenvironment%20%28TME%29%20has%20emerged%20as%20a%20promising%20source%20of%20prognostic%20biomarkers.%20To%20fully%20leverage%20its%20potential%2C%20analysis%20methods%20must%20capture%20complex%20interactions%20between%20different%20cell%20types.%20We%20propose%20HiGINE%20--%20a%20hierarchical%20graph-based%20approach%20to%20predict%20patient%20survival%20%28short%20vs.%20long%29%20from%20TME%20characterization%20in%20multiplex%20immunofluorescence%20%28mIF%29%20images%20and%20enhance%20risk%20stratification%20in%20lung%20cancer.%20Our%20model%20encodes%20both%20local%20and%20global%20inter-relations%20in%20cell%20neighborhoods%2C%20incorporating%20information%20about%20cell%20types%20and%20morphology.%20Multimodal%20fusion%2C%20aggregating%20cancer%20stage%20with%20mIF-derived%20features%2C%20further%20boosts%20performance.%20We%20validate%20HiGINE%20on%20two%20public%20datasets%2C%20demonstrating%20improved%20risk%20stratification%2C%20robustness%2C%20and%20generalizability.&entry.1838667208=http%3A//arxiv.org/abs/2512.08572v1&entry.124074799=Read"},
{"title": "Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning", "author": "Junnan Qiu and Jie Li", "abstract": "Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.", "link": "http://arxiv.org/abs/2512.08485v1", "date": "2025-12-09", "relevancy": 2.1741, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.451}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4335}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.42}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Perturbation%20Budget%20Allocation%20for%20Data%20Poisoning%20in%20Offline%20Reinforcement%20Learning&body=Title%3A%20Optimal%20Perturbation%20Budget%20Allocation%20for%20Data%20Poisoning%20in%20Offline%20Reinforcement%20Learning%0AAuthor%3A%20Junnan%20Qiu%20and%20Jie%20Li%0AAbstract%3A%20Offline%20Reinforcement%20Learning%20%28RL%29%20enables%20policy%20optimization%20from%20static%20datasets%20but%20is%20inherently%20vulnerable%20to%20data%20poisoning%20attacks.%20Existing%20attack%20strategies%20typically%20rely%20on%20locally%20uniform%20perturbations%2C%20which%20treat%20all%20samples%20indiscriminately.%20This%20approach%20is%20inefficient%2C%20as%20it%20wastes%20the%20perturbation%20budget%20on%20low-impact%20samples%2C%20and%20lacks%20stealthiness%20due%20to%20significant%20statistical%20deviations.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Global%20Budget%20Allocation%20attack%20strategy.%20Leveraging%20the%20theoretical%20insight%20that%20a%20sample%27s%20influence%20on%20value%20function%20convergence%20is%20proportional%20to%20its%20Temporal%20Difference%20%28TD%29%20error%2C%20we%20formulate%20the%20attack%20as%20a%20global%20resource%20allocation%20problem.%20We%20derive%20a%20closed-form%20solution%20where%20perturbation%20magnitudes%20are%20assigned%20proportional%20to%20the%20TD-error%20sensitivity%20under%20a%20global%20L2%20constraint.%20Empirical%20results%20on%20D4RL%20benchmarks%20demonstrate%20that%20our%20method%20significantly%20outperforms%20baseline%20strategies%2C%20achieving%20up%20to%2080%25%20performance%20degradation%20with%20minimal%20perturbations%20that%20evade%20detection%20by%20state-of-the-art%20statistical%20and%20spectral%20defenses.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08485v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Perturbation%2520Budget%2520Allocation%2520for%2520Data%2520Poisoning%2520in%2520Offline%2520Reinforcement%2520Learning%26entry.906535625%3DJunnan%2520Qiu%2520and%2520Jie%2520Li%26entry.1292438233%3DOffline%2520Reinforcement%2520Learning%2520%2528RL%2529%2520enables%2520policy%2520optimization%2520from%2520static%2520datasets%2520but%2520is%2520inherently%2520vulnerable%2520to%2520data%2520poisoning%2520attacks.%2520Existing%2520attack%2520strategies%2520typically%2520rely%2520on%2520locally%2520uniform%2520perturbations%252C%2520which%2520treat%2520all%2520samples%2520indiscriminately.%2520This%2520approach%2520is%2520inefficient%252C%2520as%2520it%2520wastes%2520the%2520perturbation%2520budget%2520on%2520low-impact%2520samples%252C%2520and%2520lacks%2520stealthiness%2520due%2520to%2520significant%2520statistical%2520deviations.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%2520novel%2520Global%2520Budget%2520Allocation%2520attack%2520strategy.%2520Leveraging%2520the%2520theoretical%2520insight%2520that%2520a%2520sample%2527s%2520influence%2520on%2520value%2520function%2520convergence%2520is%2520proportional%2520to%2520its%2520Temporal%2520Difference%2520%2528TD%2529%2520error%252C%2520we%2520formulate%2520the%2520attack%2520as%2520a%2520global%2520resource%2520allocation%2520problem.%2520We%2520derive%2520a%2520closed-form%2520solution%2520where%2520perturbation%2520magnitudes%2520are%2520assigned%2520proportional%2520to%2520the%2520TD-error%2520sensitivity%2520under%2520a%2520global%2520L2%2520constraint.%2520Empirical%2520results%2520on%2520D4RL%2520benchmarks%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520baseline%2520strategies%252C%2520achieving%2520up%2520to%252080%2525%2520performance%2520degradation%2520with%2520minimal%2520perturbations%2520that%2520evade%2520detection%2520by%2520state-of-the-art%2520statistical%2520and%2520spectral%2520defenses.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08485v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Perturbation%20Budget%20Allocation%20for%20Data%20Poisoning%20in%20Offline%20Reinforcement%20Learning&entry.906535625=Junnan%20Qiu%20and%20Jie%20Li&entry.1292438233=Offline%20Reinforcement%20Learning%20%28RL%29%20enables%20policy%20optimization%20from%20static%20datasets%20but%20is%20inherently%20vulnerable%20to%20data%20poisoning%20attacks.%20Existing%20attack%20strategies%20typically%20rely%20on%20locally%20uniform%20perturbations%2C%20which%20treat%20all%20samples%20indiscriminately.%20This%20approach%20is%20inefficient%2C%20as%20it%20wastes%20the%20perturbation%20budget%20on%20low-impact%20samples%2C%20and%20lacks%20stealthiness%20due%20to%20significant%20statistical%20deviations.%20In%20this%20paper%2C%20we%20propose%20a%20novel%20Global%20Budget%20Allocation%20attack%20strategy.%20Leveraging%20the%20theoretical%20insight%20that%20a%20sample%27s%20influence%20on%20value%20function%20convergence%20is%20proportional%20to%20its%20Temporal%20Difference%20%28TD%29%20error%2C%20we%20formulate%20the%20attack%20as%20a%20global%20resource%20allocation%20problem.%20We%20derive%20a%20closed-form%20solution%20where%20perturbation%20magnitudes%20are%20assigned%20proportional%20to%20the%20TD-error%20sensitivity%20under%20a%20global%20L2%20constraint.%20Empirical%20results%20on%20D4RL%20benchmarks%20demonstrate%20that%20our%20method%20significantly%20outperforms%20baseline%20strategies%2C%20achieving%20up%20to%2080%25%20performance%20degradation%20with%20minimal%20perturbations%20that%20evade%20detection%20by%20state-of-the-art%20statistical%20and%20spectral%20defenses.&entry.1838667208=http%3A//arxiv.org/abs/2512.08485v1&entry.124074799=Read"},
{"title": "Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata", "author": "Danial Jafarzadeh Jazi and Maryam Hajiesmaeili", "abstract": "Decoding brain states from functional magnetic resonance imaging (fMRI) data is vital for advancing neuroscience and clinical applications. While traditional machine learning and deep learning approaches have made strides in leveraging the high-dimensional and complex nature of fMRI data, they often fail to utilize the contextual richness provided by Digital Imaging and Communications in Medicine (DICOM) metadata. This paper presents a novel framework integrating transformer-based architectures with multimodal inputs, including fMRI data and DICOM metadata. By employing attention mechanisms, the proposed method captures intricate spatial-temporal patterns and contextual relationships, enhancing model accuracy, interpretability, and robustness. The potential of this framework spans applications in clinical diagnostics, cognitive neuroscience, and personalized medicine. Limitations, such as metadata variability and computational demands, are addressed, and future directions for optimizing scalability and generalizability are discussed.", "link": "http://arxiv.org/abs/2512.08462v1", "date": "2025-12-09", "relevancy": 2.1649, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5495}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5389}, {"title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene\n  Understanding", "link": "http://arxiv.org/abs/2409.03757v1", "similarity": 0.5339}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Transformers%20for%20Multimodal%20Brain%20State%20Decoding%3A%20Integrating%20Functional%20Magnetic%20Resonance%20Imaging%20Data%20and%20Medical%20Metadata&body=Title%3A%20Transformers%20for%20Multimodal%20Brain%20State%20Decoding%3A%20Integrating%20Functional%20Magnetic%20Resonance%20Imaging%20Data%20and%20Medical%20Metadata%0AAuthor%3A%20Danial%20Jafarzadeh%20Jazi%20and%20Maryam%20Hajiesmaeili%0AAbstract%3A%20Decoding%20brain%20states%20from%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29%20data%20is%20vital%20for%20advancing%20neuroscience%20and%20clinical%20applications.%20While%20traditional%20machine%20learning%20and%20deep%20learning%20approaches%20have%20made%20strides%20in%20leveraging%20the%20high-dimensional%20and%20complex%20nature%20of%20fMRI%20data%2C%20they%20often%20fail%20to%20utilize%20the%20contextual%20richness%20provided%20by%20Digital%20Imaging%20and%20Communications%20in%20Medicine%20%28DICOM%29%20metadata.%20This%20paper%20presents%20a%20novel%20framework%20integrating%20transformer-based%20architectures%20with%20multimodal%20inputs%2C%20including%20fMRI%20data%20and%20DICOM%20metadata.%20By%20employing%20attention%20mechanisms%2C%20the%20proposed%20method%20captures%20intricate%20spatial-temporal%20patterns%20and%20contextual%20relationships%2C%20enhancing%20model%20accuracy%2C%20interpretability%2C%20and%20robustness.%20The%20potential%20of%20this%20framework%20spans%20applications%20in%20clinical%20diagnostics%2C%20cognitive%20neuroscience%2C%20and%20personalized%20medicine.%20Limitations%2C%20such%20as%20metadata%20variability%20and%20computational%20demands%2C%20are%20addressed%2C%20and%20future%20directions%20for%20optimizing%20scalability%20and%20generalizability%20are%20discussed.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08462v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTransformers%2520for%2520Multimodal%2520Brain%2520State%2520Decoding%253A%2520Integrating%2520Functional%2520Magnetic%2520Resonance%2520Imaging%2520Data%2520and%2520Medical%2520Metadata%26entry.906535625%3DDanial%2520Jafarzadeh%2520Jazi%2520and%2520Maryam%2520Hajiesmaeili%26entry.1292438233%3DDecoding%2520brain%2520states%2520from%2520functional%2520magnetic%2520resonance%2520imaging%2520%2528fMRI%2529%2520data%2520is%2520vital%2520for%2520advancing%2520neuroscience%2520and%2520clinical%2520applications.%2520While%2520traditional%2520machine%2520learning%2520and%2520deep%2520learning%2520approaches%2520have%2520made%2520strides%2520in%2520leveraging%2520the%2520high-dimensional%2520and%2520complex%2520nature%2520of%2520fMRI%2520data%252C%2520they%2520often%2520fail%2520to%2520utilize%2520the%2520contextual%2520richness%2520provided%2520by%2520Digital%2520Imaging%2520and%2520Communications%2520in%2520Medicine%2520%2528DICOM%2529%2520metadata.%2520This%2520paper%2520presents%2520a%2520novel%2520framework%2520integrating%2520transformer-based%2520architectures%2520with%2520multimodal%2520inputs%252C%2520including%2520fMRI%2520data%2520and%2520DICOM%2520metadata.%2520By%2520employing%2520attention%2520mechanisms%252C%2520the%2520proposed%2520method%2520captures%2520intricate%2520spatial-temporal%2520patterns%2520and%2520contextual%2520relationships%252C%2520enhancing%2520model%2520accuracy%252C%2520interpretability%252C%2520and%2520robustness.%2520The%2520potential%2520of%2520this%2520framework%2520spans%2520applications%2520in%2520clinical%2520diagnostics%252C%2520cognitive%2520neuroscience%252C%2520and%2520personalized%2520medicine.%2520Limitations%252C%2520such%2520as%2520metadata%2520variability%2520and%2520computational%2520demands%252C%2520are%2520addressed%252C%2520and%2520future%2520directions%2520for%2520optimizing%2520scalability%2520and%2520generalizability%2520are%2520discussed.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08462v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Transformers%20for%20Multimodal%20Brain%20State%20Decoding%3A%20Integrating%20Functional%20Magnetic%20Resonance%20Imaging%20Data%20and%20Medical%20Metadata&entry.906535625=Danial%20Jafarzadeh%20Jazi%20and%20Maryam%20Hajiesmaeili&entry.1292438233=Decoding%20brain%20states%20from%20functional%20magnetic%20resonance%20imaging%20%28fMRI%29%20data%20is%20vital%20for%20advancing%20neuroscience%20and%20clinical%20applications.%20While%20traditional%20machine%20learning%20and%20deep%20learning%20approaches%20have%20made%20strides%20in%20leveraging%20the%20high-dimensional%20and%20complex%20nature%20of%20fMRI%20data%2C%20they%20often%20fail%20to%20utilize%20the%20contextual%20richness%20provided%20by%20Digital%20Imaging%20and%20Communications%20in%20Medicine%20%28DICOM%29%20metadata.%20This%20paper%20presents%20a%20novel%20framework%20integrating%20transformer-based%20architectures%20with%20multimodal%20inputs%2C%20including%20fMRI%20data%20and%20DICOM%20metadata.%20By%20employing%20attention%20mechanisms%2C%20the%20proposed%20method%20captures%20intricate%20spatial-temporal%20patterns%20and%20contextual%20relationships%2C%20enhancing%20model%20accuracy%2C%20interpretability%2C%20and%20robustness.%20The%20potential%20of%20this%20framework%20spans%20applications%20in%20clinical%20diagnostics%2C%20cognitive%20neuroscience%2C%20and%20personalized%20medicine.%20Limitations%2C%20such%20as%20metadata%20variability%20and%20computational%20demands%2C%20are%20addressed%2C%20and%20future%20directions%20for%20optimizing%20scalability%20and%20generalizability%20are%20discussed.&entry.1838667208=http%3A//arxiv.org/abs/2512.08462v1&entry.124074799=Read"},
{"title": "Evaluating and Preserving High-level Fidelity in Super-Resolution", "author": "Josep M. Rocafort and Shaolin Su and Alexandra Gomez-Villa and Javier Vazquez-Corral", "abstract": "Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.", "link": "http://arxiv.org/abs/2512.07037v2", "date": "2025-12-09", "relevancy": 2.1623, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5649}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5312}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.52}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evaluating%20and%20Preserving%20High-level%20Fidelity%20in%20Super-Resolution&body=Title%3A%20Evaluating%20and%20Preserving%20High-level%20Fidelity%20in%20Super-Resolution%0AAuthor%3A%20Josep%20M.%20Rocafort%20and%20Shaolin%20Su%20and%20Alexandra%20Gomez-Villa%20and%20Javier%20Vazquez-Corral%0AAbstract%3A%20Recent%20image%20Super-Resolution%20%28SR%29%20models%20are%20achieving%20impressive%20effects%20in%20reconstructing%20details%20and%20delivering%20visually%20pleasant%20outputs.%20However%2C%20the%20overpowering%20generative%20ability%20can%20sometimes%20hallucinate%20and%20thus%20change%20the%20image%20content%20despite%20gaining%20high%20visual%20quality.%20This%20type%20of%20high-level%20change%20can%20be%20easily%20identified%20by%20humans%20yet%20not%20well-studied%20in%20existing%20low-level%20image%20quality%20metrics.%20In%20this%20paper%2C%20we%20establish%20the%20importance%20of%20measuring%20high-level%20fidelity%20for%20SR%20models%20as%20a%20complementary%20criterion%20to%20reveal%20the%20reliability%20of%20generative%20SR%20models.%20We%20construct%20the%20first%20annotated%20dataset%20with%20fidelity%20scores%20from%20different%20SR%20models%2C%20and%20evaluate%20how%20state-of-the-art%20%28SOTA%29%20SR%20models%20actually%20perform%20in%20preserving%20high-level%20fidelity.%20Based%20on%20the%20dataset%2C%20we%20then%20analyze%20how%20existing%20image%20quality%20metrics%20correlate%20with%20fidelity%20measurement%2C%20and%20further%20show%20that%20this%20high-level%20task%20can%20be%20better%20addressed%20by%20foundation%20models.%20Finally%2C%20by%20fine-tuning%20SR%20models%20based%20on%20our%20fidelity%20feedback%2C%20we%20show%20that%20both%20semantic%20fidelity%20and%20perceptual%20quality%20can%20be%20improved%2C%20demonstrating%20the%20potential%20value%20of%20our%20proposed%20criteria%2C%20both%20in%20model%20evaluation%20and%20optimization.%20We%20will%20release%20the%20dataset%2C%20code%2C%20and%20models%20upon%20acceptance.%0ALink%3A%20http%3A//arxiv.org/abs/2512.07037v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvaluating%2520and%2520Preserving%2520High-level%2520Fidelity%2520in%2520Super-Resolution%26entry.906535625%3DJosep%2520M.%2520Rocafort%2520and%2520Shaolin%2520Su%2520and%2520Alexandra%2520Gomez-Villa%2520and%2520Javier%2520Vazquez-Corral%26entry.1292438233%3DRecent%2520image%2520Super-Resolution%2520%2528SR%2529%2520models%2520are%2520achieving%2520impressive%2520effects%2520in%2520reconstructing%2520details%2520and%2520delivering%2520visually%2520pleasant%2520outputs.%2520However%252C%2520the%2520overpowering%2520generative%2520ability%2520can%2520sometimes%2520hallucinate%2520and%2520thus%2520change%2520the%2520image%2520content%2520despite%2520gaining%2520high%2520visual%2520quality.%2520This%2520type%2520of%2520high-level%2520change%2520can%2520be%2520easily%2520identified%2520by%2520humans%2520yet%2520not%2520well-studied%2520in%2520existing%2520low-level%2520image%2520quality%2520metrics.%2520In%2520this%2520paper%252C%2520we%2520establish%2520the%2520importance%2520of%2520measuring%2520high-level%2520fidelity%2520for%2520SR%2520models%2520as%2520a%2520complementary%2520criterion%2520to%2520reveal%2520the%2520reliability%2520of%2520generative%2520SR%2520models.%2520We%2520construct%2520the%2520first%2520annotated%2520dataset%2520with%2520fidelity%2520scores%2520from%2520different%2520SR%2520models%252C%2520and%2520evaluate%2520how%2520state-of-the-art%2520%2528SOTA%2529%2520SR%2520models%2520actually%2520perform%2520in%2520preserving%2520high-level%2520fidelity.%2520Based%2520on%2520the%2520dataset%252C%2520we%2520then%2520analyze%2520how%2520existing%2520image%2520quality%2520metrics%2520correlate%2520with%2520fidelity%2520measurement%252C%2520and%2520further%2520show%2520that%2520this%2520high-level%2520task%2520can%2520be%2520better%2520addressed%2520by%2520foundation%2520models.%2520Finally%252C%2520by%2520fine-tuning%2520SR%2520models%2520based%2520on%2520our%2520fidelity%2520feedback%252C%2520we%2520show%2520that%2520both%2520semantic%2520fidelity%2520and%2520perceptual%2520quality%2520can%2520be%2520improved%252C%2520demonstrating%2520the%2520potential%2520value%2520of%2520our%2520proposed%2520criteria%252C%2520both%2520in%2520model%2520evaluation%2520and%2520optimization.%2520We%2520will%2520release%2520the%2520dataset%252C%2520code%252C%2520and%2520models%2520upon%2520acceptance.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.07037v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evaluating%20and%20Preserving%20High-level%20Fidelity%20in%20Super-Resolution&entry.906535625=Josep%20M.%20Rocafort%20and%20Shaolin%20Su%20and%20Alexandra%20Gomez-Villa%20and%20Javier%20Vazquez-Corral&entry.1292438233=Recent%20image%20Super-Resolution%20%28SR%29%20models%20are%20achieving%20impressive%20effects%20in%20reconstructing%20details%20and%20delivering%20visually%20pleasant%20outputs.%20However%2C%20the%20overpowering%20generative%20ability%20can%20sometimes%20hallucinate%20and%20thus%20change%20the%20image%20content%20despite%20gaining%20high%20visual%20quality.%20This%20type%20of%20high-level%20change%20can%20be%20easily%20identified%20by%20humans%20yet%20not%20well-studied%20in%20existing%20low-level%20image%20quality%20metrics.%20In%20this%20paper%2C%20we%20establish%20the%20importance%20of%20measuring%20high-level%20fidelity%20for%20SR%20models%20as%20a%20complementary%20criterion%20to%20reveal%20the%20reliability%20of%20generative%20SR%20models.%20We%20construct%20the%20first%20annotated%20dataset%20with%20fidelity%20scores%20from%20different%20SR%20models%2C%20and%20evaluate%20how%20state-of-the-art%20%28SOTA%29%20SR%20models%20actually%20perform%20in%20preserving%20high-level%20fidelity.%20Based%20on%20the%20dataset%2C%20we%20then%20analyze%20how%20existing%20image%20quality%20metrics%20correlate%20with%20fidelity%20measurement%2C%20and%20further%20show%20that%20this%20high-level%20task%20can%20be%20better%20addressed%20by%20foundation%20models.%20Finally%2C%20by%20fine-tuning%20SR%20models%20based%20on%20our%20fidelity%20feedback%2C%20we%20show%20that%20both%20semantic%20fidelity%20and%20perceptual%20quality%20can%20be%20improved%2C%20demonstrating%20the%20potential%20value%20of%20our%20proposed%20criteria%2C%20both%20in%20model%20evaluation%20and%20optimization.%20We%20will%20release%20the%20dataset%2C%20code%2C%20and%20models%20upon%20acceptance.&entry.1838667208=http%3A//arxiv.org/abs/2512.07037v2&entry.124074799=Read"},
{"title": "MatteViT: High-Frequency-Aware Document Shadow Removal with Shadow Matte Guidance", "author": "Chaewon Kim and Seoyeon Lee and Jonghyuk Park", "abstract": "Document shadow removal is essential for enhancing the clarity of digitized documents. Preserving high-frequency details (e.g., text edges and lines) is critical in this process because shadows often obscure or distort fine structures. This paper proposes a matte vision transformer (MatteViT), a novel shadow removal framework that applies spatial and frequency-domain information to eliminate shadows while preserving fine-grained structural details. To effectively retain these details, we employ two preservation strategies. First, our method introduces a lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components. Second, we present a continuous luminance-based shadow matte, generated using a custom-built matte dataset and shadow matte generator, which provides precise spatial guidance from the earliest processing stage. These strategies enable the model to accurately identify fine-grained regions and restore them with high fidelity. Extensive experiments on public benchmarks (RDD and Kligler) demonstrate that MatteViT achieves state-of-the-art performance, providing a robust and practical solution for real-world document shadow removal. Furthermore, the proposed method better preserves text-level details in downstream tasks, such as optical character recognition, improving recognition performance over prior methods.", "link": "http://arxiv.org/abs/2512.08789v1", "date": "2025-12-09", "relevancy": 2.1603, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5582}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5359}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5052}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MatteViT%3A%20High-Frequency-Aware%20Document%20Shadow%20Removal%20with%20Shadow%20Matte%20Guidance&body=Title%3A%20MatteViT%3A%20High-Frequency-Aware%20Document%20Shadow%20Removal%20with%20Shadow%20Matte%20Guidance%0AAuthor%3A%20Chaewon%20Kim%20and%20Seoyeon%20Lee%20and%20Jonghyuk%20Park%0AAbstract%3A%20Document%20shadow%20removal%20is%20essential%20for%20enhancing%20the%20clarity%20of%20digitized%20documents.%20Preserving%20high-frequency%20details%20%28e.g.%2C%20text%20edges%20and%20lines%29%20is%20critical%20in%20this%20process%20because%20shadows%20often%20obscure%20or%20distort%20fine%20structures.%20This%20paper%20proposes%20a%20matte%20vision%20transformer%20%28MatteViT%29%2C%20a%20novel%20shadow%20removal%20framework%20that%20applies%20spatial%20and%20frequency-domain%20information%20to%20eliminate%20shadows%20while%20preserving%20fine-grained%20structural%20details.%20To%20effectively%20retain%20these%20details%2C%20we%20employ%20two%20preservation%20strategies.%20First%2C%20our%20method%20introduces%20a%20lightweight%20high-frequency%20amplification%20module%20%28HFAM%29%20that%20decomposes%20and%20adaptively%20amplifies%20high-frequency%20components.%20Second%2C%20we%20present%20a%20continuous%20luminance-based%20shadow%20matte%2C%20generated%20using%20a%20custom-built%20matte%20dataset%20and%20shadow%20matte%20generator%2C%20which%20provides%20precise%20spatial%20guidance%20from%20the%20earliest%20processing%20stage.%20These%20strategies%20enable%20the%20model%20to%20accurately%20identify%20fine-grained%20regions%20and%20restore%20them%20with%20high%20fidelity.%20Extensive%20experiments%20on%20public%20benchmarks%20%28RDD%20and%20Kligler%29%20demonstrate%20that%20MatteViT%20achieves%20state-of-the-art%20performance%2C%20providing%20a%20robust%20and%20practical%20solution%20for%20real-world%20document%20shadow%20removal.%20Furthermore%2C%20the%20proposed%20method%20better%20preserves%20text-level%20details%20in%20downstream%20tasks%2C%20such%20as%20optical%20character%20recognition%2C%20improving%20recognition%20performance%20over%20prior%20methods.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08789v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMatteViT%253A%2520High-Frequency-Aware%2520Document%2520Shadow%2520Removal%2520with%2520Shadow%2520Matte%2520Guidance%26entry.906535625%3DChaewon%2520Kim%2520and%2520Seoyeon%2520Lee%2520and%2520Jonghyuk%2520Park%26entry.1292438233%3DDocument%2520shadow%2520removal%2520is%2520essential%2520for%2520enhancing%2520the%2520clarity%2520of%2520digitized%2520documents.%2520Preserving%2520high-frequency%2520details%2520%2528e.g.%252C%2520text%2520edges%2520and%2520lines%2529%2520is%2520critical%2520in%2520this%2520process%2520because%2520shadows%2520often%2520obscure%2520or%2520distort%2520fine%2520structures.%2520This%2520paper%2520proposes%2520a%2520matte%2520vision%2520transformer%2520%2528MatteViT%2529%252C%2520a%2520novel%2520shadow%2520removal%2520framework%2520that%2520applies%2520spatial%2520and%2520frequency-domain%2520information%2520to%2520eliminate%2520shadows%2520while%2520preserving%2520fine-grained%2520structural%2520details.%2520To%2520effectively%2520retain%2520these%2520details%252C%2520we%2520employ%2520two%2520preservation%2520strategies.%2520First%252C%2520our%2520method%2520introduces%2520a%2520lightweight%2520high-frequency%2520amplification%2520module%2520%2528HFAM%2529%2520that%2520decomposes%2520and%2520adaptively%2520amplifies%2520high-frequency%2520components.%2520Second%252C%2520we%2520present%2520a%2520continuous%2520luminance-based%2520shadow%2520matte%252C%2520generated%2520using%2520a%2520custom-built%2520matte%2520dataset%2520and%2520shadow%2520matte%2520generator%252C%2520which%2520provides%2520precise%2520spatial%2520guidance%2520from%2520the%2520earliest%2520processing%2520stage.%2520These%2520strategies%2520enable%2520the%2520model%2520to%2520accurately%2520identify%2520fine-grained%2520regions%2520and%2520restore%2520them%2520with%2520high%2520fidelity.%2520Extensive%2520experiments%2520on%2520public%2520benchmarks%2520%2528RDD%2520and%2520Kligler%2529%2520demonstrate%2520that%2520MatteViT%2520achieves%2520state-of-the-art%2520performance%252C%2520providing%2520a%2520robust%2520and%2520practical%2520solution%2520for%2520real-world%2520document%2520shadow%2520removal.%2520Furthermore%252C%2520the%2520proposed%2520method%2520better%2520preserves%2520text-level%2520details%2520in%2520downstream%2520tasks%252C%2520such%2520as%2520optical%2520character%2520recognition%252C%2520improving%2520recognition%2520performance%2520over%2520prior%2520methods.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08789v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MatteViT%3A%20High-Frequency-Aware%20Document%20Shadow%20Removal%20with%20Shadow%20Matte%20Guidance&entry.906535625=Chaewon%20Kim%20and%20Seoyeon%20Lee%20and%20Jonghyuk%20Park&entry.1292438233=Document%20shadow%20removal%20is%20essential%20for%20enhancing%20the%20clarity%20of%20digitized%20documents.%20Preserving%20high-frequency%20details%20%28e.g.%2C%20text%20edges%20and%20lines%29%20is%20critical%20in%20this%20process%20because%20shadows%20often%20obscure%20or%20distort%20fine%20structures.%20This%20paper%20proposes%20a%20matte%20vision%20transformer%20%28MatteViT%29%2C%20a%20novel%20shadow%20removal%20framework%20that%20applies%20spatial%20and%20frequency-domain%20information%20to%20eliminate%20shadows%20while%20preserving%20fine-grained%20structural%20details.%20To%20effectively%20retain%20these%20details%2C%20we%20employ%20two%20preservation%20strategies.%20First%2C%20our%20method%20introduces%20a%20lightweight%20high-frequency%20amplification%20module%20%28HFAM%29%20that%20decomposes%20and%20adaptively%20amplifies%20high-frequency%20components.%20Second%2C%20we%20present%20a%20continuous%20luminance-based%20shadow%20matte%2C%20generated%20using%20a%20custom-built%20matte%20dataset%20and%20shadow%20matte%20generator%2C%20which%20provides%20precise%20spatial%20guidance%20from%20the%20earliest%20processing%20stage.%20These%20strategies%20enable%20the%20model%20to%20accurately%20identify%20fine-grained%20regions%20and%20restore%20them%20with%20high%20fidelity.%20Extensive%20experiments%20on%20public%20benchmarks%20%28RDD%20and%20Kligler%29%20demonstrate%20that%20MatteViT%20achieves%20state-of-the-art%20performance%2C%20providing%20a%20robust%20and%20practical%20solution%20for%20real-world%20document%20shadow%20removal.%20Furthermore%2C%20the%20proposed%20method%20better%20preserves%20text-level%20details%20in%20downstream%20tasks%2C%20such%20as%20optical%20character%20recognition%2C%20improving%20recognition%20performance%20over%20prior%20methods.&entry.1838667208=http%3A//arxiv.org/abs/2512.08789v1&entry.124074799=Read"},
{"title": "MVP: Multiple View Prediction Improves GUI Grounding", "author": "Yunzhu Zhang and Zeyu Pan and Zhengwen Zeng and Shuheng Shen and Changhua Meng and Linchao Zhu", "abstract": "GUI grounding, which translates natural language instructions into precise pixel coordinates, is essential for developing practical GUI agents. However, we observe that existing grounding models exhibit significant coordinate prediction instability, minor visual perturbations (e.g. cropping a few pixels) can drastically alter predictions, flipping results between correct and incorrect. This instability severely undermines model performance, especially for samples with high-resolution and small UI elements. To address this issue, we propose Multi-View Prediction (MVP), a training-free framework that enhances grounding performance through multi-view inference. Our key insight is that while single-view predictions may be unstable, aggregating predictions from multiple carefully cropped views can effectively distinguish correct coordinates from outliers. MVP comprises two components: (1) Attention-Guided View Proposal, which derives diverse views guided by instruction-to-image attention scores, and (2) Multi-Coordinates Clustering, which ensembles predictions by selecting the centroid of the densest spatial cluster. Extensive experiments demonstrate MVP's effectiveness across various models and benchmarks. Notably, on ScreenSpot-Pro, MVP boosts UI-TARS-1.5-7B to 56.1%, GTA1-7B to 61.7%, Qwen3VL-8B-Instruct to 65.3%, and Qwen3VL-32B-Instruct to 74.0%. The code is available at https://github.com/ZJUSCL/MVP.", "link": "http://arxiv.org/abs/2512.08529v1", "date": "2025-12-09", "relevancy": 2.1472, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.571}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5469}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.513}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MVP%3A%20Multiple%20View%20Prediction%20Improves%20GUI%20Grounding&body=Title%3A%20MVP%3A%20Multiple%20View%20Prediction%20Improves%20GUI%20Grounding%0AAuthor%3A%20Yunzhu%20Zhang%20and%20Zeyu%20Pan%20and%20Zhengwen%20Zeng%20and%20Shuheng%20Shen%20and%20Changhua%20Meng%20and%20Linchao%20Zhu%0AAbstract%3A%20GUI%20grounding%2C%20which%20translates%20natural%20language%20instructions%20into%20precise%20pixel%20coordinates%2C%20is%20essential%20for%20developing%20practical%20GUI%20agents.%20However%2C%20we%20observe%20that%20existing%20grounding%20models%20exhibit%20significant%20coordinate%20prediction%20instability%2C%20minor%20visual%20perturbations%20%28e.g.%20cropping%20a%20few%20pixels%29%20can%20drastically%20alter%20predictions%2C%20flipping%20results%20between%20correct%20and%20incorrect.%20This%20instability%20severely%20undermines%20model%20performance%2C%20especially%20for%20samples%20with%20high-resolution%20and%20small%20UI%20elements.%20To%20address%20this%20issue%2C%20we%20propose%20Multi-View%20Prediction%20%28MVP%29%2C%20a%20training-free%20framework%20that%20enhances%20grounding%20performance%20through%20multi-view%20inference.%20Our%20key%20insight%20is%20that%20while%20single-view%20predictions%20may%20be%20unstable%2C%20aggregating%20predictions%20from%20multiple%20carefully%20cropped%20views%20can%20effectively%20distinguish%20correct%20coordinates%20from%20outliers.%20MVP%20comprises%20two%20components%3A%20%281%29%20Attention-Guided%20View%20Proposal%2C%20which%20derives%20diverse%20views%20guided%20by%20instruction-to-image%20attention%20scores%2C%20and%20%282%29%20Multi-Coordinates%20Clustering%2C%20which%20ensembles%20predictions%20by%20selecting%20the%20centroid%20of%20the%20densest%20spatial%20cluster.%20Extensive%20experiments%20demonstrate%20MVP%27s%20effectiveness%20across%20various%20models%20and%20benchmarks.%20Notably%2C%20on%20ScreenSpot-Pro%2C%20MVP%20boosts%20UI-TARS-1.5-7B%20to%2056.1%25%2C%20GTA1-7B%20to%2061.7%25%2C%20Qwen3VL-8B-Instruct%20to%2065.3%25%2C%20and%20Qwen3VL-32B-Instruct%20to%2074.0%25.%20The%20code%20is%20available%20at%20https%3A//github.com/ZJUSCL/MVP.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08529v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMVP%253A%2520Multiple%2520View%2520Prediction%2520Improves%2520GUI%2520Grounding%26entry.906535625%3DYunzhu%2520Zhang%2520and%2520Zeyu%2520Pan%2520and%2520Zhengwen%2520Zeng%2520and%2520Shuheng%2520Shen%2520and%2520Changhua%2520Meng%2520and%2520Linchao%2520Zhu%26entry.1292438233%3DGUI%2520grounding%252C%2520which%2520translates%2520natural%2520language%2520instructions%2520into%2520precise%2520pixel%2520coordinates%252C%2520is%2520essential%2520for%2520developing%2520practical%2520GUI%2520agents.%2520However%252C%2520we%2520observe%2520that%2520existing%2520grounding%2520models%2520exhibit%2520significant%2520coordinate%2520prediction%2520instability%252C%2520minor%2520visual%2520perturbations%2520%2528e.g.%2520cropping%2520a%2520few%2520pixels%2529%2520can%2520drastically%2520alter%2520predictions%252C%2520flipping%2520results%2520between%2520correct%2520and%2520incorrect.%2520This%2520instability%2520severely%2520undermines%2520model%2520performance%252C%2520especially%2520for%2520samples%2520with%2520high-resolution%2520and%2520small%2520UI%2520elements.%2520To%2520address%2520this%2520issue%252C%2520we%2520propose%2520Multi-View%2520Prediction%2520%2528MVP%2529%252C%2520a%2520training-free%2520framework%2520that%2520enhances%2520grounding%2520performance%2520through%2520multi-view%2520inference.%2520Our%2520key%2520insight%2520is%2520that%2520while%2520single-view%2520predictions%2520may%2520be%2520unstable%252C%2520aggregating%2520predictions%2520from%2520multiple%2520carefully%2520cropped%2520views%2520can%2520effectively%2520distinguish%2520correct%2520coordinates%2520from%2520outliers.%2520MVP%2520comprises%2520two%2520components%253A%2520%25281%2529%2520Attention-Guided%2520View%2520Proposal%252C%2520which%2520derives%2520diverse%2520views%2520guided%2520by%2520instruction-to-image%2520attention%2520scores%252C%2520and%2520%25282%2529%2520Multi-Coordinates%2520Clustering%252C%2520which%2520ensembles%2520predictions%2520by%2520selecting%2520the%2520centroid%2520of%2520the%2520densest%2520spatial%2520cluster.%2520Extensive%2520experiments%2520demonstrate%2520MVP%2527s%2520effectiveness%2520across%2520various%2520models%2520and%2520benchmarks.%2520Notably%252C%2520on%2520ScreenSpot-Pro%252C%2520MVP%2520boosts%2520UI-TARS-1.5-7B%2520to%252056.1%2525%252C%2520GTA1-7B%2520to%252061.7%2525%252C%2520Qwen3VL-8B-Instruct%2520to%252065.3%2525%252C%2520and%2520Qwen3VL-32B-Instruct%2520to%252074.0%2525.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/ZJUSCL/MVP.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08529v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MVP%3A%20Multiple%20View%20Prediction%20Improves%20GUI%20Grounding&entry.906535625=Yunzhu%20Zhang%20and%20Zeyu%20Pan%20and%20Zhengwen%20Zeng%20and%20Shuheng%20Shen%20and%20Changhua%20Meng%20and%20Linchao%20Zhu&entry.1292438233=GUI%20grounding%2C%20which%20translates%20natural%20language%20instructions%20into%20precise%20pixel%20coordinates%2C%20is%20essential%20for%20developing%20practical%20GUI%20agents.%20However%2C%20we%20observe%20that%20existing%20grounding%20models%20exhibit%20significant%20coordinate%20prediction%20instability%2C%20minor%20visual%20perturbations%20%28e.g.%20cropping%20a%20few%20pixels%29%20can%20drastically%20alter%20predictions%2C%20flipping%20results%20between%20correct%20and%20incorrect.%20This%20instability%20severely%20undermines%20model%20performance%2C%20especially%20for%20samples%20with%20high-resolution%20and%20small%20UI%20elements.%20To%20address%20this%20issue%2C%20we%20propose%20Multi-View%20Prediction%20%28MVP%29%2C%20a%20training-free%20framework%20that%20enhances%20grounding%20performance%20through%20multi-view%20inference.%20Our%20key%20insight%20is%20that%20while%20single-view%20predictions%20may%20be%20unstable%2C%20aggregating%20predictions%20from%20multiple%20carefully%20cropped%20views%20can%20effectively%20distinguish%20correct%20coordinates%20from%20outliers.%20MVP%20comprises%20two%20components%3A%20%281%29%20Attention-Guided%20View%20Proposal%2C%20which%20derives%20diverse%20views%20guided%20by%20instruction-to-image%20attention%20scores%2C%20and%20%282%29%20Multi-Coordinates%20Clustering%2C%20which%20ensembles%20predictions%20by%20selecting%20the%20centroid%20of%20the%20densest%20spatial%20cluster.%20Extensive%20experiments%20demonstrate%20MVP%27s%20effectiveness%20across%20various%20models%20and%20benchmarks.%20Notably%2C%20on%20ScreenSpot-Pro%2C%20MVP%20boosts%20UI-TARS-1.5-7B%20to%2056.1%25%2C%20GTA1-7B%20to%2061.7%25%2C%20Qwen3VL-8B-Instruct%20to%2065.3%25%2C%20and%20Qwen3VL-32B-Instruct%20to%2074.0%25.%20The%20code%20is%20available%20at%20https%3A//github.com/ZJUSCL/MVP.&entry.1838667208=http%3A//arxiv.org/abs/2512.08529v1&entry.124074799=Read"},
{"title": "Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders", "author": "Guangzhi Xiong and Zhenghao He and Bohan Liu and Sanchit Sinha and Aidong Zhang", "abstract": "Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.", "link": "http://arxiv.org/abs/2512.08892v1", "date": "2025-12-09", "relevancy": 2.1465, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.55}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5456}, {"title": "GeneVA: A Dataset of Human Annotations for Generative Text to Video\n  Artifacts", "link": "http://arxiv.org/abs/2509.08818v1", "similarity": 0.5223}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Toward%20Faithful%20Retrieval-Augmented%20Generation%20with%20Sparse%20Autoencoders&body=Title%3A%20Toward%20Faithful%20Retrieval-Augmented%20Generation%20with%20Sparse%20Autoencoders%0AAuthor%3A%20Guangzhi%20Xiong%20and%20Zhenghao%20He%20and%20Bohan%20Liu%20and%20Sanchit%20Sinha%20and%20Aidong%20Zhang%0AAbstract%3A%20Retrieval-Augmented%20Generation%20%28RAG%29%20improves%20the%20factuality%20of%20large%20language%20models%20%28LLMs%29%20by%20grounding%20outputs%20in%20retrieved%20evidence%2C%20but%20faithfulness%20failures%2C%20where%20generations%20contradict%20or%20extend%20beyond%20the%20provided%20sources%2C%20remain%20a%20critical%20challenge.%20Existing%20hallucination%20detection%20methods%20for%20RAG%20often%20rely%20either%20on%20large-scale%20detector%20training%2C%20which%20requires%20substantial%20annotated%20data%2C%20or%20on%20querying%20external%20LLM%20judges%2C%20which%20leads%20to%20high%20inference%20costs.%20Although%20some%20approaches%20attempt%20to%20leverage%20internal%20representations%20of%20LLMs%20for%20hallucination%20detection%2C%20their%20accuracy%20remains%20limited.%20Motivated%20by%20recent%20advances%20in%20mechanistic%20interpretability%2C%20we%20employ%20sparse%20autoencoders%20%28SAEs%29%20to%20disentangle%20internal%20activations%2C%20successfully%20identifying%20features%20that%20are%20specifically%20triggered%20during%20RAG%20hallucinations.%20Building%20on%20a%20systematic%20pipeline%20of%20information-based%20feature%20selection%20and%20additive%20feature%20modeling%2C%20we%20introduce%20RAGLens%2C%20a%20lightweight%20hallucination%20detector%20that%20accurately%20flags%20unfaithful%20RAG%20outputs%20using%20LLM%20internal%20representations.%20RAGLens%20not%20only%20achieves%20superior%20detection%20performance%20compared%20to%20existing%20methods%2C%20but%20also%20provides%20interpretable%20rationales%20for%20its%20decisions%2C%20enabling%20effective%20post-hoc%20mitigation%20of%20unfaithful%20RAG.%20Finally%2C%20we%20justify%20our%20design%20choices%20and%20reveal%20new%20insights%20into%20the%20distribution%20of%20hallucination-related%20signals%20within%20LLMs.%20The%20code%20is%20available%20at%20https%3A//github.com/Teddy-XiongGZ/RAGLens.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08892v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DToward%2520Faithful%2520Retrieval-Augmented%2520Generation%2520with%2520Sparse%2520Autoencoders%26entry.906535625%3DGuangzhi%2520Xiong%2520and%2520Zhenghao%2520He%2520and%2520Bohan%2520Liu%2520and%2520Sanchit%2520Sinha%2520and%2520Aidong%2520Zhang%26entry.1292438233%3DRetrieval-Augmented%2520Generation%2520%2528RAG%2529%2520improves%2520the%2520factuality%2520of%2520large%2520language%2520models%2520%2528LLMs%2529%2520by%2520grounding%2520outputs%2520in%2520retrieved%2520evidence%252C%2520but%2520faithfulness%2520failures%252C%2520where%2520generations%2520contradict%2520or%2520extend%2520beyond%2520the%2520provided%2520sources%252C%2520remain%2520a%2520critical%2520challenge.%2520Existing%2520hallucination%2520detection%2520methods%2520for%2520RAG%2520often%2520rely%2520either%2520on%2520large-scale%2520detector%2520training%252C%2520which%2520requires%2520substantial%2520annotated%2520data%252C%2520or%2520on%2520querying%2520external%2520LLM%2520judges%252C%2520which%2520leads%2520to%2520high%2520inference%2520costs.%2520Although%2520some%2520approaches%2520attempt%2520to%2520leverage%2520internal%2520representations%2520of%2520LLMs%2520for%2520hallucination%2520detection%252C%2520their%2520accuracy%2520remains%2520limited.%2520Motivated%2520by%2520recent%2520advances%2520in%2520mechanistic%2520interpretability%252C%2520we%2520employ%2520sparse%2520autoencoders%2520%2528SAEs%2529%2520to%2520disentangle%2520internal%2520activations%252C%2520successfully%2520identifying%2520features%2520that%2520are%2520specifically%2520triggered%2520during%2520RAG%2520hallucinations.%2520Building%2520on%2520a%2520systematic%2520pipeline%2520of%2520information-based%2520feature%2520selection%2520and%2520additive%2520feature%2520modeling%252C%2520we%2520introduce%2520RAGLens%252C%2520a%2520lightweight%2520hallucination%2520detector%2520that%2520accurately%2520flags%2520unfaithful%2520RAG%2520outputs%2520using%2520LLM%2520internal%2520representations.%2520RAGLens%2520not%2520only%2520achieves%2520superior%2520detection%2520performance%2520compared%2520to%2520existing%2520methods%252C%2520but%2520also%2520provides%2520interpretable%2520rationales%2520for%2520its%2520decisions%252C%2520enabling%2520effective%2520post-hoc%2520mitigation%2520of%2520unfaithful%2520RAG.%2520Finally%252C%2520we%2520justify%2520our%2520design%2520choices%2520and%2520reveal%2520new%2520insights%2520into%2520the%2520distribution%2520of%2520hallucination-related%2520signals%2520within%2520LLMs.%2520The%2520code%2520is%2520available%2520at%2520https%253A//github.com/Teddy-XiongGZ/RAGLens.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08892v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Toward%20Faithful%20Retrieval-Augmented%20Generation%20with%20Sparse%20Autoencoders&entry.906535625=Guangzhi%20Xiong%20and%20Zhenghao%20He%20and%20Bohan%20Liu%20and%20Sanchit%20Sinha%20and%20Aidong%20Zhang&entry.1292438233=Retrieval-Augmented%20Generation%20%28RAG%29%20improves%20the%20factuality%20of%20large%20language%20models%20%28LLMs%29%20by%20grounding%20outputs%20in%20retrieved%20evidence%2C%20but%20faithfulness%20failures%2C%20where%20generations%20contradict%20or%20extend%20beyond%20the%20provided%20sources%2C%20remain%20a%20critical%20challenge.%20Existing%20hallucination%20detection%20methods%20for%20RAG%20often%20rely%20either%20on%20large-scale%20detector%20training%2C%20which%20requires%20substantial%20annotated%20data%2C%20or%20on%20querying%20external%20LLM%20judges%2C%20which%20leads%20to%20high%20inference%20costs.%20Although%20some%20approaches%20attempt%20to%20leverage%20internal%20representations%20of%20LLMs%20for%20hallucination%20detection%2C%20their%20accuracy%20remains%20limited.%20Motivated%20by%20recent%20advances%20in%20mechanistic%20interpretability%2C%20we%20employ%20sparse%20autoencoders%20%28SAEs%29%20to%20disentangle%20internal%20activations%2C%20successfully%20identifying%20features%20that%20are%20specifically%20triggered%20during%20RAG%20hallucinations.%20Building%20on%20a%20systematic%20pipeline%20of%20information-based%20feature%20selection%20and%20additive%20feature%20modeling%2C%20we%20introduce%20RAGLens%2C%20a%20lightweight%20hallucination%20detector%20that%20accurately%20flags%20unfaithful%20RAG%20outputs%20using%20LLM%20internal%20representations.%20RAGLens%20not%20only%20achieves%20superior%20detection%20performance%20compared%20to%20existing%20methods%2C%20but%20also%20provides%20interpretable%20rationales%20for%20its%20decisions%2C%20enabling%20effective%20post-hoc%20mitigation%20of%20unfaithful%20RAG.%20Finally%2C%20we%20justify%20our%20design%20choices%20and%20reveal%20new%20insights%20into%20the%20distribution%20of%20hallucination-related%20signals%20within%20LLMs.%20The%20code%20is%20available%20at%20https%3A//github.com/Teddy-XiongGZ/RAGLens.&entry.1838667208=http%3A//arxiv.org/abs/2512.08892v1&entry.124074799=Read"},
{"title": "Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?", "author": "Jeongwhan Choi and Woosung Kang and Minseo Kim and Jongwoo Kim and Noseong Park", "abstract": "Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.", "link": "http://arxiv.org/abs/2512.08798v1", "date": "2025-12-09", "relevancy": 2.1456, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4472}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4202}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4199}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Can%20TabPFN%20Compete%20with%20GNNs%20for%20Node%20Classification%20via%20Graph%20Tabularization%3F&body=Title%3A%20Can%20TabPFN%20Compete%20with%20GNNs%20for%20Node%20Classification%20via%20Graph%20Tabularization%3F%0AAuthor%3A%20Jeongwhan%20Choi%20and%20Woosung%20Kang%20and%20Minseo%20Kim%20and%20Jongwoo%20Kim%20and%20Noseong%20Park%0AAbstract%3A%20Foundation%20models%20pretrained%20on%20large%20data%20have%20demonstrated%20remarkable%20zero-shot%20generalization%20capabilities%20across%20domains.%20Building%20on%20the%20success%20of%20TabPFN%20for%20tabular%20data%20and%20its%20recent%20extension%20to%20time%20series%2C%20we%20investigate%20whether%20graph%20node%20classification%20can%20be%20effectively%20reformulated%20as%20a%20tabular%20learning%20problem.%20We%20introduce%20TabPFN-GN%2C%20which%20transforms%20graph%20data%20into%20tabular%20features%20by%20extracting%20node%20attributes%2C%20structural%20properties%2C%20positional%20encodings%2C%20and%20optionally%20smoothed%20neighborhood%20features.%20This%20enables%20TabPFN%20to%20perform%20direct%20node%20classification%20without%20any%20graph-specific%20training%20or%20language%20model%20dependencies.%20Our%20experiments%20on%2012%20benchmark%20datasets%20reveal%20that%20TabPFN-GN%20achieves%20competitive%20performance%20with%20GNNs%20on%20homophilous%20graphs%20and%20consistently%20outperforms%20them%20on%20heterophilous%20graphs.%20These%20results%20demonstrate%20that%20principled%20feature%20engineering%20can%20bridge%20the%20gap%20between%20tabular%20and%20graph%20domains%2C%20providing%20a%20practical%20alternative%20to%20task-specific%20GNN%20training%20and%20LLM-dependent%20graph%20foundation%20models.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08798v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCan%2520TabPFN%2520Compete%2520with%2520GNNs%2520for%2520Node%2520Classification%2520via%2520Graph%2520Tabularization%253F%26entry.906535625%3DJeongwhan%2520Choi%2520and%2520Woosung%2520Kang%2520and%2520Minseo%2520Kim%2520and%2520Jongwoo%2520Kim%2520and%2520Noseong%2520Park%26entry.1292438233%3DFoundation%2520models%2520pretrained%2520on%2520large%2520data%2520have%2520demonstrated%2520remarkable%2520zero-shot%2520generalization%2520capabilities%2520across%2520domains.%2520Building%2520on%2520the%2520success%2520of%2520TabPFN%2520for%2520tabular%2520data%2520and%2520its%2520recent%2520extension%2520to%2520time%2520series%252C%2520we%2520investigate%2520whether%2520graph%2520node%2520classification%2520can%2520be%2520effectively%2520reformulated%2520as%2520a%2520tabular%2520learning%2520problem.%2520We%2520introduce%2520TabPFN-GN%252C%2520which%2520transforms%2520graph%2520data%2520into%2520tabular%2520features%2520by%2520extracting%2520node%2520attributes%252C%2520structural%2520properties%252C%2520positional%2520encodings%252C%2520and%2520optionally%2520smoothed%2520neighborhood%2520features.%2520This%2520enables%2520TabPFN%2520to%2520perform%2520direct%2520node%2520classification%2520without%2520any%2520graph-specific%2520training%2520or%2520language%2520model%2520dependencies.%2520Our%2520experiments%2520on%252012%2520benchmark%2520datasets%2520reveal%2520that%2520TabPFN-GN%2520achieves%2520competitive%2520performance%2520with%2520GNNs%2520on%2520homophilous%2520graphs%2520and%2520consistently%2520outperforms%2520them%2520on%2520heterophilous%2520graphs.%2520These%2520results%2520demonstrate%2520that%2520principled%2520feature%2520engineering%2520can%2520bridge%2520the%2520gap%2520between%2520tabular%2520and%2520graph%2520domains%252C%2520providing%2520a%2520practical%2520alternative%2520to%2520task-specific%2520GNN%2520training%2520and%2520LLM-dependent%2520graph%2520foundation%2520models.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08798v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Can%20TabPFN%20Compete%20with%20GNNs%20for%20Node%20Classification%20via%20Graph%20Tabularization%3F&entry.906535625=Jeongwhan%20Choi%20and%20Woosung%20Kang%20and%20Minseo%20Kim%20and%20Jongwoo%20Kim%20and%20Noseong%20Park&entry.1292438233=Foundation%20models%20pretrained%20on%20large%20data%20have%20demonstrated%20remarkable%20zero-shot%20generalization%20capabilities%20across%20domains.%20Building%20on%20the%20success%20of%20TabPFN%20for%20tabular%20data%20and%20its%20recent%20extension%20to%20time%20series%2C%20we%20investigate%20whether%20graph%20node%20classification%20can%20be%20effectively%20reformulated%20as%20a%20tabular%20learning%20problem.%20We%20introduce%20TabPFN-GN%2C%20which%20transforms%20graph%20data%20into%20tabular%20features%20by%20extracting%20node%20attributes%2C%20structural%20properties%2C%20positional%20encodings%2C%20and%20optionally%20smoothed%20neighborhood%20features.%20This%20enables%20TabPFN%20to%20perform%20direct%20node%20classification%20without%20any%20graph-specific%20training%20or%20language%20model%20dependencies.%20Our%20experiments%20on%2012%20benchmark%20datasets%20reveal%20that%20TabPFN-GN%20achieves%20competitive%20performance%20with%20GNNs%20on%20homophilous%20graphs%20and%20consistently%20outperforms%20them%20on%20heterophilous%20graphs.%20These%20results%20demonstrate%20that%20principled%20feature%20engineering%20can%20bridge%20the%20gap%20between%20tabular%20and%20graph%20domains%2C%20providing%20a%20practical%20alternative%20to%20task-specific%20GNN%20training%20and%20LLM-dependent%20graph%20foundation%20models.&entry.1838667208=http%3A//arxiv.org/abs/2512.08798v1&entry.124074799=Read"},
{"title": "Modular Neural Image Signal Processing", "author": "Mahmoud Afifi and Zhongling Wang and Ran Zhang and Michael S. Brown", "abstract": "This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM", "link": "http://arxiv.org/abs/2512.08564v1", "date": "2025-12-09", "relevancy": 2.1441, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5563}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.5467}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5172}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Modular%20Neural%20Image%20Signal%20Processing&body=Title%3A%20Modular%20Neural%20Image%20Signal%20Processing%0AAuthor%3A%20Mahmoud%20Afifi%20and%20Zhongling%20Wang%20and%20Ran%20Zhang%20and%20Michael%20S.%20Brown%0AAbstract%3A%20This%20paper%20presents%20a%20modular%20neural%20image%20signal%20processing%20%28ISP%29%20framework%20that%20processes%20raw%20inputs%20and%20renders%20high-quality%20display-referred%20images.%20Unlike%20prior%20neural%20ISP%20designs%2C%20our%20method%20introduces%20a%20high%20degree%20of%20modularity%2C%20providing%20full%20control%20over%20multiple%20intermediate%20stages%20of%20the%20rendering%20process.~This%20modular%20design%20not%20only%20achieves%20high%20rendering%20accuracy%20but%20also%20improves%20scalability%2C%20debuggability%2C%20generalization%20to%20unseen%20cameras%2C%20and%20flexibility%20to%20match%20different%20user-preference%20styles.%20To%20demonstrate%20the%20advantages%20of%20this%20design%2C%20we%20built%20a%20user-interactive%20photo-editing%20tool%20that%20leverages%20our%20neural%20ISP%20to%20support%20diverse%20editing%20operations%20and%20picture%20styles.%20The%20tool%20is%20carefully%20engineered%20to%20take%20advantage%20of%20the%20high-quality%20rendering%20of%20our%20neural%20ISP%20and%20to%20enable%20unlimited%20post-editable%20re-rendering.%20Our%20method%20is%20a%20fully%20learning-based%20framework%20with%20variants%20of%20different%20capacities%2C%20all%20of%20moderate%20size%20%28ranging%20from%20~0.5%20M%20to%20~3.9%20M%20parameters%20for%20the%20entire%20pipeline%29%2C%20and%20consistently%20delivers%20competitive%20qualitative%20and%20quantitative%20results%20across%20multiple%20test%20sets.%20Watch%20the%20supplemental%20video%20at%3A%20https%3A//youtu.be/ByhQjQSjxVM%0ALink%3A%20http%3A//arxiv.org/abs/2512.08564v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModular%2520Neural%2520Image%2520Signal%2520Processing%26entry.906535625%3DMahmoud%2520Afifi%2520and%2520Zhongling%2520Wang%2520and%2520Ran%2520Zhang%2520and%2520Michael%2520S.%2520Brown%26entry.1292438233%3DThis%2520paper%2520presents%2520a%2520modular%2520neural%2520image%2520signal%2520processing%2520%2528ISP%2529%2520framework%2520that%2520processes%2520raw%2520inputs%2520and%2520renders%2520high-quality%2520display-referred%2520images.%2520Unlike%2520prior%2520neural%2520ISP%2520designs%252C%2520our%2520method%2520introduces%2520a%2520high%2520degree%2520of%2520modularity%252C%2520providing%2520full%2520control%2520over%2520multiple%2520intermediate%2520stages%2520of%2520the%2520rendering%2520process.~This%2520modular%2520design%2520not%2520only%2520achieves%2520high%2520rendering%2520accuracy%2520but%2520also%2520improves%2520scalability%252C%2520debuggability%252C%2520generalization%2520to%2520unseen%2520cameras%252C%2520and%2520flexibility%2520to%2520match%2520different%2520user-preference%2520styles.%2520To%2520demonstrate%2520the%2520advantages%2520of%2520this%2520design%252C%2520we%2520built%2520a%2520user-interactive%2520photo-editing%2520tool%2520that%2520leverages%2520our%2520neural%2520ISP%2520to%2520support%2520diverse%2520editing%2520operations%2520and%2520picture%2520styles.%2520The%2520tool%2520is%2520carefully%2520engineered%2520to%2520take%2520advantage%2520of%2520the%2520high-quality%2520rendering%2520of%2520our%2520neural%2520ISP%2520and%2520to%2520enable%2520unlimited%2520post-editable%2520re-rendering.%2520Our%2520method%2520is%2520a%2520fully%2520learning-based%2520framework%2520with%2520variants%2520of%2520different%2520capacities%252C%2520all%2520of%2520moderate%2520size%2520%2528ranging%2520from%2520~0.5%2520M%2520to%2520~3.9%2520M%2520parameters%2520for%2520the%2520entire%2520pipeline%2529%252C%2520and%2520consistently%2520delivers%2520competitive%2520qualitative%2520and%2520quantitative%2520results%2520across%2520multiple%2520test%2520sets.%2520Watch%2520the%2520supplemental%2520video%2520at%253A%2520https%253A//youtu.be/ByhQjQSjxVM%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08564v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Modular%20Neural%20Image%20Signal%20Processing&entry.906535625=Mahmoud%20Afifi%20and%20Zhongling%20Wang%20and%20Ran%20Zhang%20and%20Michael%20S.%20Brown&entry.1292438233=This%20paper%20presents%20a%20modular%20neural%20image%20signal%20processing%20%28ISP%29%20framework%20that%20processes%20raw%20inputs%20and%20renders%20high-quality%20display-referred%20images.%20Unlike%20prior%20neural%20ISP%20designs%2C%20our%20method%20introduces%20a%20high%20degree%20of%20modularity%2C%20providing%20full%20control%20over%20multiple%20intermediate%20stages%20of%20the%20rendering%20process.~This%20modular%20design%20not%20only%20achieves%20high%20rendering%20accuracy%20but%20also%20improves%20scalability%2C%20debuggability%2C%20generalization%20to%20unseen%20cameras%2C%20and%20flexibility%20to%20match%20different%20user-preference%20styles.%20To%20demonstrate%20the%20advantages%20of%20this%20design%2C%20we%20built%20a%20user-interactive%20photo-editing%20tool%20that%20leverages%20our%20neural%20ISP%20to%20support%20diverse%20editing%20operations%20and%20picture%20styles.%20The%20tool%20is%20carefully%20engineered%20to%20take%20advantage%20of%20the%20high-quality%20rendering%20of%20our%20neural%20ISP%20and%20to%20enable%20unlimited%20post-editable%20re-rendering.%20Our%20method%20is%20a%20fully%20learning-based%20framework%20with%20variants%20of%20different%20capacities%2C%20all%20of%20moderate%20size%20%28ranging%20from%20~0.5%20M%20to%20~3.9%20M%20parameters%20for%20the%20entire%20pipeline%29%2C%20and%20consistently%20delivers%20competitive%20qualitative%20and%20quantitative%20results%20across%20multiple%20test%20sets.%20Watch%20the%20supplemental%20video%20at%3A%20https%3A//youtu.be/ByhQjQSjxVM&entry.1838667208=http%3A//arxiv.org/abs/2512.08564v1&entry.124074799=Read"},
{"title": "Gaussian Approximation for Two-Timescale Linear Stochastic Approximation", "author": "Bogdan Butyrin and Artemy Rubtsov and Alexey Naumov and Vladimir Ulyanov and Sergey Samsonov", "abstract": "In this paper, we establish non-asymptotic bounds for accuracy of normal approximation for linear two-timescale stochastic approximation (TTSA) algorithms driven by martingale difference or Markov noise. Focusing on both the last iterate and Polyak-Ruppert averaging regimes, we derive bounds for normal approximation in terms of the convex distance between probability distributions. Our analysis reveals a non-trivial interaction between the fast and slow timescales: the normal approximation rate for the last iterate improves as the timescale separation increases, while it decreases in the Polyak-Ruppert averaged setting. We also provide the high-order moment bounds for the error of linear TTSA algorithm, which may be of independent interest.", "link": "http://arxiv.org/abs/2508.07928v2", "date": "2025-12-09", "relevancy": 2.1346, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4549}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4217}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4042}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Gaussian%20Approximation%20for%20Two-Timescale%20Linear%20Stochastic%20Approximation&body=Title%3A%20Gaussian%20Approximation%20for%20Two-Timescale%20Linear%20Stochastic%20Approximation%0AAuthor%3A%20Bogdan%20Butyrin%20and%20Artemy%20Rubtsov%20and%20Alexey%20Naumov%20and%20Vladimir%20Ulyanov%20and%20Sergey%20Samsonov%0AAbstract%3A%20In%20this%20paper%2C%20we%20establish%20non-asymptotic%20bounds%20for%20accuracy%20of%20normal%20approximation%20for%20linear%20two-timescale%20stochastic%20approximation%20%28TTSA%29%20algorithms%20driven%20by%20martingale%20difference%20or%20Markov%20noise.%20Focusing%20on%20both%20the%20last%20iterate%20and%20Polyak-Ruppert%20averaging%20regimes%2C%20we%20derive%20bounds%20for%20normal%20approximation%20in%20terms%20of%20the%20convex%20distance%20between%20probability%20distributions.%20Our%20analysis%20reveals%20a%20non-trivial%20interaction%20between%20the%20fast%20and%20slow%20timescales%3A%20the%20normal%20approximation%20rate%20for%20the%20last%20iterate%20improves%20as%20the%20timescale%20separation%20increases%2C%20while%20it%20decreases%20in%20the%20Polyak-Ruppert%20averaged%20setting.%20We%20also%20provide%20the%20high-order%20moment%20bounds%20for%20the%20error%20of%20linear%20TTSA%20algorithm%2C%20which%20may%20be%20of%20independent%20interest.%0ALink%3A%20http%3A//arxiv.org/abs/2508.07928v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGaussian%2520Approximation%2520for%2520Two-Timescale%2520Linear%2520Stochastic%2520Approximation%26entry.906535625%3DBogdan%2520Butyrin%2520and%2520Artemy%2520Rubtsov%2520and%2520Alexey%2520Naumov%2520and%2520Vladimir%2520Ulyanov%2520and%2520Sergey%2520Samsonov%26entry.1292438233%3DIn%2520this%2520paper%252C%2520we%2520establish%2520non-asymptotic%2520bounds%2520for%2520accuracy%2520of%2520normal%2520approximation%2520for%2520linear%2520two-timescale%2520stochastic%2520approximation%2520%2528TTSA%2529%2520algorithms%2520driven%2520by%2520martingale%2520difference%2520or%2520Markov%2520noise.%2520Focusing%2520on%2520both%2520the%2520last%2520iterate%2520and%2520Polyak-Ruppert%2520averaging%2520regimes%252C%2520we%2520derive%2520bounds%2520for%2520normal%2520approximation%2520in%2520terms%2520of%2520the%2520convex%2520distance%2520between%2520probability%2520distributions.%2520Our%2520analysis%2520reveals%2520a%2520non-trivial%2520interaction%2520between%2520the%2520fast%2520and%2520slow%2520timescales%253A%2520the%2520normal%2520approximation%2520rate%2520for%2520the%2520last%2520iterate%2520improves%2520as%2520the%2520timescale%2520separation%2520increases%252C%2520while%2520it%2520decreases%2520in%2520the%2520Polyak-Ruppert%2520averaged%2520setting.%2520We%2520also%2520provide%2520the%2520high-order%2520moment%2520bounds%2520for%2520the%2520error%2520of%2520linear%2520TTSA%2520algorithm%252C%2520which%2520may%2520be%2520of%2520independent%2520interest.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2508.07928v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gaussian%20Approximation%20for%20Two-Timescale%20Linear%20Stochastic%20Approximation&entry.906535625=Bogdan%20Butyrin%20and%20Artemy%20Rubtsov%20and%20Alexey%20Naumov%20and%20Vladimir%20Ulyanov%20and%20Sergey%20Samsonov&entry.1292438233=In%20this%20paper%2C%20we%20establish%20non-asymptotic%20bounds%20for%20accuracy%20of%20normal%20approximation%20for%20linear%20two-timescale%20stochastic%20approximation%20%28TTSA%29%20algorithms%20driven%20by%20martingale%20difference%20or%20Markov%20noise.%20Focusing%20on%20both%20the%20last%20iterate%20and%20Polyak-Ruppert%20averaging%20regimes%2C%20we%20derive%20bounds%20for%20normal%20approximation%20in%20terms%20of%20the%20convex%20distance%20between%20probability%20distributions.%20Our%20analysis%20reveals%20a%20non-trivial%20interaction%20between%20the%20fast%20and%20slow%20timescales%3A%20the%20normal%20approximation%20rate%20for%20the%20last%20iterate%20improves%20as%20the%20timescale%20separation%20increases%2C%20while%20it%20decreases%20in%20the%20Polyak-Ruppert%20averaged%20setting.%20We%20also%20provide%20the%20high-order%20moment%20bounds%20for%20the%20error%20of%20linear%20TTSA%20algorithm%2C%20which%20may%20be%20of%20independent%20interest.&entry.1838667208=http%3A//arxiv.org/abs/2508.07928v2&entry.124074799=Read"},
{"title": "Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning", "author": "Zhenyu Zhang and Guangyao Chen and Yixiong Zou and Zhimeng Huang and Yuhua Li", "abstract": "The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of \"emptiness\" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.", "link": "http://arxiv.org/abs/2512.08606v1", "date": "2025-12-09", "relevancy": 2.1232, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5723}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5078}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4985}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupling%20Template%20Bias%20in%20CLIP%3A%20Harnessing%20Empty%20Prompts%20for%20Enhanced%20Few-Shot%20Learning&body=Title%3A%20Decoupling%20Template%20Bias%20in%20CLIP%3A%20Harnessing%20Empty%20Prompts%20for%20Enhanced%20Few-Shot%20Learning%0AAuthor%3A%20Zhenyu%20Zhang%20and%20Guangyao%20Chen%20and%20Yixiong%20Zou%20and%20Zhimeng%20Huang%20and%20Yuhua%20Li%0AAbstract%3A%20The%20Contrastive%20Language-Image%20Pre-Training%20%28CLIP%29%20model%20excels%20in%20few-shot%20learning%20by%20aligning%20visual%20and%20textual%20representations.%20Our%20study%20shows%20that%20template-sample%20similarity%20%28TSS%29%2C%20defined%20as%20the%20resemblance%20between%20a%20text%20template%20and%20an%20image%20sample%2C%20introduces%20bias.%20This%20bias%20leads%20the%20model%20to%20rely%20on%20template%20proximity%20rather%20than%20true%20sample-to-category%20alignment%2C%20reducing%20both%20accuracy%20and%20robustness%20in%20classification.%20We%20present%20a%20framework%20that%20uses%20empty%20prompts%2C%20textual%20inputs%20that%20convey%20the%20idea%20of%20%22emptiness%22%20without%20category%20information.%20These%20prompts%20capture%20unbiased%20template%20features%20and%20offset%20TSS%20bias.%20The%20framework%20employs%20two%20stages.%20During%20pre-training%2C%20empty%20prompts%20reveal%20and%20reduce%20template-induced%20bias%20within%20the%20CLIP%20encoder.%20During%20few-shot%20fine-tuning%2C%20a%20bias%20calibration%20loss%20enforces%20correct%20alignment%20between%20images%20and%20their%20categories%2C%20ensuring%20the%20model%20focuses%20on%20relevant%20visual%20cues.%20Experiments%20across%20multiple%20benchmarks%20demonstrate%20that%20our%20template%20correction%20method%20significantly%20reduces%20performance%20fluctuations%20caused%20by%20TSS%2C%20yielding%20higher%20classification%20accuracy%20and%20stronger%20robustness.%20The%20repository%20of%20this%20project%20is%20available%20at%20https%3A//github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08606v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupling%2520Template%2520Bias%2520in%2520CLIP%253A%2520Harnessing%2520Empty%2520Prompts%2520for%2520Enhanced%2520Few-Shot%2520Learning%26entry.906535625%3DZhenyu%2520Zhang%2520and%2520Guangyao%2520Chen%2520and%2520Yixiong%2520Zou%2520and%2520Zhimeng%2520Huang%2520and%2520Yuhua%2520Li%26entry.1292438233%3DThe%2520Contrastive%2520Language-Image%2520Pre-Training%2520%2528CLIP%2529%2520model%2520excels%2520in%2520few-shot%2520learning%2520by%2520aligning%2520visual%2520and%2520textual%2520representations.%2520Our%2520study%2520shows%2520that%2520template-sample%2520similarity%2520%2528TSS%2529%252C%2520defined%2520as%2520the%2520resemblance%2520between%2520a%2520text%2520template%2520and%2520an%2520image%2520sample%252C%2520introduces%2520bias.%2520This%2520bias%2520leads%2520the%2520model%2520to%2520rely%2520on%2520template%2520proximity%2520rather%2520than%2520true%2520sample-to-category%2520alignment%252C%2520reducing%2520both%2520accuracy%2520and%2520robustness%2520in%2520classification.%2520We%2520present%2520a%2520framework%2520that%2520uses%2520empty%2520prompts%252C%2520textual%2520inputs%2520that%2520convey%2520the%2520idea%2520of%2520%2522emptiness%2522%2520without%2520category%2520information.%2520These%2520prompts%2520capture%2520unbiased%2520template%2520features%2520and%2520offset%2520TSS%2520bias.%2520The%2520framework%2520employs%2520two%2520stages.%2520During%2520pre-training%252C%2520empty%2520prompts%2520reveal%2520and%2520reduce%2520template-induced%2520bias%2520within%2520the%2520CLIP%2520encoder.%2520During%2520few-shot%2520fine-tuning%252C%2520a%2520bias%2520calibration%2520loss%2520enforces%2520correct%2520alignment%2520between%2520images%2520and%2520their%2520categories%252C%2520ensuring%2520the%2520model%2520focuses%2520on%2520relevant%2520visual%2520cues.%2520Experiments%2520across%2520multiple%2520benchmarks%2520demonstrate%2520that%2520our%2520template%2520correction%2520method%2520significantly%2520reduces%2520performance%2520fluctuations%2520caused%2520by%2520TSS%252C%2520yielding%2520higher%2520classification%2520accuracy%2520and%2520stronger%2520robustness.%2520The%2520repository%2520of%2520this%2520project%2520is%2520available%2520at%2520https%253A//github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08606v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupling%20Template%20Bias%20in%20CLIP%3A%20Harnessing%20Empty%20Prompts%20for%20Enhanced%20Few-Shot%20Learning&entry.906535625=Zhenyu%20Zhang%20and%20Guangyao%20Chen%20and%20Yixiong%20Zou%20and%20Zhimeng%20Huang%20and%20Yuhua%20Li&entry.1292438233=The%20Contrastive%20Language-Image%20Pre-Training%20%28CLIP%29%20model%20excels%20in%20few-shot%20learning%20by%20aligning%20visual%20and%20textual%20representations.%20Our%20study%20shows%20that%20template-sample%20similarity%20%28TSS%29%2C%20defined%20as%20the%20resemblance%20between%20a%20text%20template%20and%20an%20image%20sample%2C%20introduces%20bias.%20This%20bias%20leads%20the%20model%20to%20rely%20on%20template%20proximity%20rather%20than%20true%20sample-to-category%20alignment%2C%20reducing%20both%20accuracy%20and%20robustness%20in%20classification.%20We%20present%20a%20framework%20that%20uses%20empty%20prompts%2C%20textual%20inputs%20that%20convey%20the%20idea%20of%20%22emptiness%22%20without%20category%20information.%20These%20prompts%20capture%20unbiased%20template%20features%20and%20offset%20TSS%20bias.%20The%20framework%20employs%20two%20stages.%20During%20pre-training%2C%20empty%20prompts%20reveal%20and%20reduce%20template-induced%20bias%20within%20the%20CLIP%20encoder.%20During%20few-shot%20fine-tuning%2C%20a%20bias%20calibration%20loss%20enforces%20correct%20alignment%20between%20images%20and%20their%20categories%2C%20ensuring%20the%20model%20focuses%20on%20relevant%20visual%20cues.%20Experiments%20across%20multiple%20benchmarks%20demonstrate%20that%20our%20template%20correction%20method%20significantly%20reduces%20performance%20fluctuations%20caused%20by%20TSS%2C%20yielding%20higher%20classification%20accuracy%20and%20stronger%20robustness.%20The%20repository%20of%20this%20project%20is%20available%20at%20https%3A//github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.&entry.1838667208=http%3A//arxiv.org/abs/2512.08606v1&entry.124074799=Read"},
{"title": "Forecasting Fails: Unveiling Evasion Attacks in Weather Prediction Models", "author": "Huzaifa Arif and Pin-Yu Chen and Alex Gittens and James Diffenderfer and Bhavya Kailkhura", "abstract": "With the increasing reliance on AI models for weather forecasting, it is imperative to evaluate their vulnerability to adversarial perturbations. This work introduces Weather Adaptive Adversarial Perturbation Optimization (WAAPO), a novel framework for generating targeted adversarial perturbations that are both effective in manipulating forecasts and stealthy to avoid detection. WAAPO achieves this by incorporating constraints for channel sparsity, spatial localization, and smoothness, ensuring that perturbations remain physically realistic and imperceptible. Using the ERA5 dataset and FourCastNet (Pathak et al. 2022), we demonstrate WAAPO's ability to generate adversarial trajectories that align closely with predefined targets, even under constrained conditions. Our experiments highlight critical vulnerabilities in AI-driven forecasting models, where small perturbations to initial conditions can result in significant deviations in predicted weather patterns. These findings underscore the need for robust safeguards to protect against adversarial exploitation in operational forecasting systems.", "link": "http://arxiv.org/abs/2512.08832v1", "date": "2025-12-09", "relevancy": 2.1101, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4289}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4197}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Forecasting%20Fails%3A%20Unveiling%20Evasion%20Attacks%20in%20Weather%20Prediction%20Models&body=Title%3A%20Forecasting%20Fails%3A%20Unveiling%20Evasion%20Attacks%20in%20Weather%20Prediction%20Models%0AAuthor%3A%20Huzaifa%20Arif%20and%20Pin-Yu%20Chen%20and%20Alex%20Gittens%20and%20James%20Diffenderfer%20and%20Bhavya%20Kailkhura%0AAbstract%3A%20With%20the%20increasing%20reliance%20on%20AI%20models%20for%20weather%20forecasting%2C%20it%20is%20imperative%20to%20evaluate%20their%20vulnerability%20to%20adversarial%20perturbations.%20This%20work%20introduces%20Weather%20Adaptive%20Adversarial%20Perturbation%20Optimization%20%28WAAPO%29%2C%20a%20novel%20framework%20for%20generating%20targeted%20adversarial%20perturbations%20that%20are%20both%20effective%20in%20manipulating%20forecasts%20and%20stealthy%20to%20avoid%20detection.%20WAAPO%20achieves%20this%20by%20incorporating%20constraints%20for%20channel%20sparsity%2C%20spatial%20localization%2C%20and%20smoothness%2C%20ensuring%20that%20perturbations%20remain%20physically%20realistic%20and%20imperceptible.%20Using%20the%20ERA5%20dataset%20and%20FourCastNet%20%28Pathak%20et%20al.%202022%29%2C%20we%20demonstrate%20WAAPO%27s%20ability%20to%20generate%20adversarial%20trajectories%20that%20align%20closely%20with%20predefined%20targets%2C%20even%20under%20constrained%20conditions.%20Our%20experiments%20highlight%20critical%20vulnerabilities%20in%20AI-driven%20forecasting%20models%2C%20where%20small%20perturbations%20to%20initial%20conditions%20can%20result%20in%20significant%20deviations%20in%20predicted%20weather%20patterns.%20These%20findings%20underscore%20the%20need%20for%20robust%20safeguards%20to%20protect%20against%20adversarial%20exploitation%20in%20operational%20forecasting%20systems.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08832v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DForecasting%2520Fails%253A%2520Unveiling%2520Evasion%2520Attacks%2520in%2520Weather%2520Prediction%2520Models%26entry.906535625%3DHuzaifa%2520Arif%2520and%2520Pin-Yu%2520Chen%2520and%2520Alex%2520Gittens%2520and%2520James%2520Diffenderfer%2520and%2520Bhavya%2520Kailkhura%26entry.1292438233%3DWith%2520the%2520increasing%2520reliance%2520on%2520AI%2520models%2520for%2520weather%2520forecasting%252C%2520it%2520is%2520imperative%2520to%2520evaluate%2520their%2520vulnerability%2520to%2520adversarial%2520perturbations.%2520This%2520work%2520introduces%2520Weather%2520Adaptive%2520Adversarial%2520Perturbation%2520Optimization%2520%2528WAAPO%2529%252C%2520a%2520novel%2520framework%2520for%2520generating%2520targeted%2520adversarial%2520perturbations%2520that%2520are%2520both%2520effective%2520in%2520manipulating%2520forecasts%2520and%2520stealthy%2520to%2520avoid%2520detection.%2520WAAPO%2520achieves%2520this%2520by%2520incorporating%2520constraints%2520for%2520channel%2520sparsity%252C%2520spatial%2520localization%252C%2520and%2520smoothness%252C%2520ensuring%2520that%2520perturbations%2520remain%2520physically%2520realistic%2520and%2520imperceptible.%2520Using%2520the%2520ERA5%2520dataset%2520and%2520FourCastNet%2520%2528Pathak%2520et%2520al.%25202022%2529%252C%2520we%2520demonstrate%2520WAAPO%2527s%2520ability%2520to%2520generate%2520adversarial%2520trajectories%2520that%2520align%2520closely%2520with%2520predefined%2520targets%252C%2520even%2520under%2520constrained%2520conditions.%2520Our%2520experiments%2520highlight%2520critical%2520vulnerabilities%2520in%2520AI-driven%2520forecasting%2520models%252C%2520where%2520small%2520perturbations%2520to%2520initial%2520conditions%2520can%2520result%2520in%2520significant%2520deviations%2520in%2520predicted%2520weather%2520patterns.%2520These%2520findings%2520underscore%2520the%2520need%2520for%2520robust%2520safeguards%2520to%2520protect%2520against%2520adversarial%2520exploitation%2520in%2520operational%2520forecasting%2520systems.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08832v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Forecasting%20Fails%3A%20Unveiling%20Evasion%20Attacks%20in%20Weather%20Prediction%20Models&entry.906535625=Huzaifa%20Arif%20and%20Pin-Yu%20Chen%20and%20Alex%20Gittens%20and%20James%20Diffenderfer%20and%20Bhavya%20Kailkhura&entry.1292438233=With%20the%20increasing%20reliance%20on%20AI%20models%20for%20weather%20forecasting%2C%20it%20is%20imperative%20to%20evaluate%20their%20vulnerability%20to%20adversarial%20perturbations.%20This%20work%20introduces%20Weather%20Adaptive%20Adversarial%20Perturbation%20Optimization%20%28WAAPO%29%2C%20a%20novel%20framework%20for%20generating%20targeted%20adversarial%20perturbations%20that%20are%20both%20effective%20in%20manipulating%20forecasts%20and%20stealthy%20to%20avoid%20detection.%20WAAPO%20achieves%20this%20by%20incorporating%20constraints%20for%20channel%20sparsity%2C%20spatial%20localization%2C%20and%20smoothness%2C%20ensuring%20that%20perturbations%20remain%20physically%20realistic%20and%20imperceptible.%20Using%20the%20ERA5%20dataset%20and%20FourCastNet%20%28Pathak%20et%20al.%202022%29%2C%20we%20demonstrate%20WAAPO%27s%20ability%20to%20generate%20adversarial%20trajectories%20that%20align%20closely%20with%20predefined%20targets%2C%20even%20under%20constrained%20conditions.%20Our%20experiments%20highlight%20critical%20vulnerabilities%20in%20AI-driven%20forecasting%20models%2C%20where%20small%20perturbations%20to%20initial%20conditions%20can%20result%20in%20significant%20deviations%20in%20predicted%20weather%20patterns.%20These%20findings%20underscore%20the%20need%20for%20robust%20safeguards%20to%20protect%20against%20adversarial%20exploitation%20in%20operational%20forecasting%20systems.&entry.1838667208=http%3A//arxiv.org/abs/2512.08832v1&entry.124074799=Read"},
{"title": "C-DIRA: Computationally Efficient Dynamic ROI Routing and Domain-Invariant Adversarial Learning for Lightweight Driver Behavior Recognition", "author": "Keito Inoshita", "abstract": "Driver distraction behavior recognition using in-vehicle cameras demands real-time inference on edge devices. However, lightweight models often fail to capture fine-grained behavioral cues, resulting in reduced performance on unseen drivers or under varying conditions. ROI-based methods also increase computational cost, making it difficult to balance efficiency and accuracy. This work addresses the need for a lightweight architecture that overcomes these constraints. We propose Computationally efficient Dynamic region of Interest Routing and domain-invariant Adversarial learning for lightweight driver behavior recognition (C-DIRA). The framework combines saliency-driven Top-K ROI pooling and fused classification for local feature extraction and integration. Dynamic ROI routing enables selective computation by applying ROI inference only to high difficulty data samples. Moreover, pseudo-domain labeling and adversarial learning are used to learn domain-invariant features robust to driver and background variation. Experiments on the State Farm Distracted Driver Detection Dataset show that C-DIRA maintains high accuracy with significantly fewer FLOPs and lower latency than prior lightweight models. It also demonstrates robustness under visual degradation such as blur and low-light, and stable performance across unseen domains. These results confirm C-DIRA's effectiveness in achieving compactness, efficiency, and generalization.", "link": "http://arxiv.org/abs/2512.08647v1", "date": "2025-12-09", "relevancy": 2.1093, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5412}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5289}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5202}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20C-DIRA%3A%20Computationally%20Efficient%20Dynamic%20ROI%20Routing%20and%20Domain-Invariant%20Adversarial%20Learning%20for%20Lightweight%20Driver%20Behavior%20Recognition&body=Title%3A%20C-DIRA%3A%20Computationally%20Efficient%20Dynamic%20ROI%20Routing%20and%20Domain-Invariant%20Adversarial%20Learning%20for%20Lightweight%20Driver%20Behavior%20Recognition%0AAuthor%3A%20Keito%20Inoshita%0AAbstract%3A%20Driver%20distraction%20behavior%20recognition%20using%20in-vehicle%20cameras%20demands%20real-time%20inference%20on%20edge%20devices.%20However%2C%20lightweight%20models%20often%20fail%20to%20capture%20fine-grained%20behavioral%20cues%2C%20resulting%20in%20reduced%20performance%20on%20unseen%20drivers%20or%20under%20varying%20conditions.%20ROI-based%20methods%20also%20increase%20computational%20cost%2C%20making%20it%20difficult%20to%20balance%20efficiency%20and%20accuracy.%20This%20work%20addresses%20the%20need%20for%20a%20lightweight%20architecture%20that%20overcomes%20these%20constraints.%20We%20propose%20Computationally%20efficient%20Dynamic%20region%20of%20Interest%20Routing%20and%20domain-invariant%20Adversarial%20learning%20for%20lightweight%20driver%20behavior%20recognition%20%28C-DIRA%29.%20The%20framework%20combines%20saliency-driven%20Top-K%20ROI%20pooling%20and%20fused%20classification%20for%20local%20feature%20extraction%20and%20integration.%20Dynamic%20ROI%20routing%20enables%20selective%20computation%20by%20applying%20ROI%20inference%20only%20to%20high%20difficulty%20data%20samples.%20Moreover%2C%20pseudo-domain%20labeling%20and%20adversarial%20learning%20are%20used%20to%20learn%20domain-invariant%20features%20robust%20to%20driver%20and%20background%20variation.%20Experiments%20on%20the%20State%20Farm%20Distracted%20Driver%20Detection%20Dataset%20show%20that%20C-DIRA%20maintains%20high%20accuracy%20with%20significantly%20fewer%20FLOPs%20and%20lower%20latency%20than%20prior%20lightweight%20models.%20It%20also%20demonstrates%20robustness%20under%20visual%20degradation%20such%20as%20blur%20and%20low-light%2C%20and%20stable%20performance%20across%20unseen%20domains.%20These%20results%20confirm%20C-DIRA%27s%20effectiveness%20in%20achieving%20compactness%2C%20efficiency%2C%20and%20generalization.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08647v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DC-DIRA%253A%2520Computationally%2520Efficient%2520Dynamic%2520ROI%2520Routing%2520and%2520Domain-Invariant%2520Adversarial%2520Learning%2520for%2520Lightweight%2520Driver%2520Behavior%2520Recognition%26entry.906535625%3DKeito%2520Inoshita%26entry.1292438233%3DDriver%2520distraction%2520behavior%2520recognition%2520using%2520in-vehicle%2520cameras%2520demands%2520real-time%2520inference%2520on%2520edge%2520devices.%2520However%252C%2520lightweight%2520models%2520often%2520fail%2520to%2520capture%2520fine-grained%2520behavioral%2520cues%252C%2520resulting%2520in%2520reduced%2520performance%2520on%2520unseen%2520drivers%2520or%2520under%2520varying%2520conditions.%2520ROI-based%2520methods%2520also%2520increase%2520computational%2520cost%252C%2520making%2520it%2520difficult%2520to%2520balance%2520efficiency%2520and%2520accuracy.%2520This%2520work%2520addresses%2520the%2520need%2520for%2520a%2520lightweight%2520architecture%2520that%2520overcomes%2520these%2520constraints.%2520We%2520propose%2520Computationally%2520efficient%2520Dynamic%2520region%2520of%2520Interest%2520Routing%2520and%2520domain-invariant%2520Adversarial%2520learning%2520for%2520lightweight%2520driver%2520behavior%2520recognition%2520%2528C-DIRA%2529.%2520The%2520framework%2520combines%2520saliency-driven%2520Top-K%2520ROI%2520pooling%2520and%2520fused%2520classification%2520for%2520local%2520feature%2520extraction%2520and%2520integration.%2520Dynamic%2520ROI%2520routing%2520enables%2520selective%2520computation%2520by%2520applying%2520ROI%2520inference%2520only%2520to%2520high%2520difficulty%2520data%2520samples.%2520Moreover%252C%2520pseudo-domain%2520labeling%2520and%2520adversarial%2520learning%2520are%2520used%2520to%2520learn%2520domain-invariant%2520features%2520robust%2520to%2520driver%2520and%2520background%2520variation.%2520Experiments%2520on%2520the%2520State%2520Farm%2520Distracted%2520Driver%2520Detection%2520Dataset%2520show%2520that%2520C-DIRA%2520maintains%2520high%2520accuracy%2520with%2520significantly%2520fewer%2520FLOPs%2520and%2520lower%2520latency%2520than%2520prior%2520lightweight%2520models.%2520It%2520also%2520demonstrates%2520robustness%2520under%2520visual%2520degradation%2520such%2520as%2520blur%2520and%2520low-light%252C%2520and%2520stable%2520performance%2520across%2520unseen%2520domains.%2520These%2520results%2520confirm%2520C-DIRA%2527s%2520effectiveness%2520in%2520achieving%2520compactness%252C%2520efficiency%252C%2520and%2520generalization.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08647v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=C-DIRA%3A%20Computationally%20Efficient%20Dynamic%20ROI%20Routing%20and%20Domain-Invariant%20Adversarial%20Learning%20for%20Lightweight%20Driver%20Behavior%20Recognition&entry.906535625=Keito%20Inoshita&entry.1292438233=Driver%20distraction%20behavior%20recognition%20using%20in-vehicle%20cameras%20demands%20real-time%20inference%20on%20edge%20devices.%20However%2C%20lightweight%20models%20often%20fail%20to%20capture%20fine-grained%20behavioral%20cues%2C%20resulting%20in%20reduced%20performance%20on%20unseen%20drivers%20or%20under%20varying%20conditions.%20ROI-based%20methods%20also%20increase%20computational%20cost%2C%20making%20it%20difficult%20to%20balance%20efficiency%20and%20accuracy.%20This%20work%20addresses%20the%20need%20for%20a%20lightweight%20architecture%20that%20overcomes%20these%20constraints.%20We%20propose%20Computationally%20efficient%20Dynamic%20region%20of%20Interest%20Routing%20and%20domain-invariant%20Adversarial%20learning%20for%20lightweight%20driver%20behavior%20recognition%20%28C-DIRA%29.%20The%20framework%20combines%20saliency-driven%20Top-K%20ROI%20pooling%20and%20fused%20classification%20for%20local%20feature%20extraction%20and%20integration.%20Dynamic%20ROI%20routing%20enables%20selective%20computation%20by%20applying%20ROI%20inference%20only%20to%20high%20difficulty%20data%20samples.%20Moreover%2C%20pseudo-domain%20labeling%20and%20adversarial%20learning%20are%20used%20to%20learn%20domain-invariant%20features%20robust%20to%20driver%20and%20background%20variation.%20Experiments%20on%20the%20State%20Farm%20Distracted%20Driver%20Detection%20Dataset%20show%20that%20C-DIRA%20maintains%20high%20accuracy%20with%20significantly%20fewer%20FLOPs%20and%20lower%20latency%20than%20prior%20lightweight%20models.%20It%20also%20demonstrates%20robustness%20under%20visual%20degradation%20such%20as%20blur%20and%20low-light%2C%20and%20stable%20performance%20across%20unseen%20domains.%20These%20results%20confirm%20C-DIRA%27s%20effectiveness%20in%20achieving%20compactness%2C%20efficiency%2C%20and%20generalization.&entry.1838667208=http%3A//arxiv.org/abs/2512.08647v1&entry.124074799=Read"},
{"title": "Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery", "author": "Yuna Kato and Shohei Mori and Hideo Saito and Yoshifumi Takatsume and Hiroki Kajita and Mariko Isogawa", "abstract": "Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.", "link": "http://arxiv.org/abs/2512.08577v1", "date": "2025-12-09", "relevancy": 1.6337, "topK": [{"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.57}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5379}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.5356}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disturbance-Free%20Surgical%20Video%20Generation%20from%20Multi-Camera%20Shadowless%20Lamps%20for%20Open%20Surgery&body=Title%3A%20Disturbance-Free%20Surgical%20Video%20Generation%20from%20Multi-Camera%20Shadowless%20Lamps%20for%20Open%20Surgery%0AAuthor%3A%20Yuna%20Kato%20and%20Shohei%20Mori%20and%20Hideo%20Saito%20and%20Yoshifumi%20Takatsume%20and%20Hiroki%20Kajita%20and%20Mariko%20Isogawa%0AAbstract%3A%20Video%20recordings%20of%20open%20surgeries%20are%20greatly%20required%20for%20education%20and%20research%20purposes.%20However%2C%20capturing%20unobstructed%20videos%20is%20challenging%20since%20surgeons%20frequently%20block%20the%20camera%20field%20of%20view.%20To%20avoid%20occlusion%2C%20the%20positions%20and%20angles%20of%20the%20camera%20must%20be%20frequently%20adjusted%2C%20which%20is%20highly%20labor-intensive.%20Prior%20work%20has%20addressed%20this%20issue%20by%20installing%20multiple%20cameras%20on%20a%20shadowless%20lamp%20and%20arranging%20them%20to%20fully%20surround%20the%20surgical%20area.%20This%20setup%20increases%20the%20chances%20of%20some%20cameras%20capturing%20an%20unobstructed%20view.%20However%2C%20manual%20image%20alignment%20is%20needed%20in%20post-processing%20since%20camera%20configurations%20change%20every%20time%20surgeons%20move%20the%20lamp%20for%20optimal%20lighting.%20This%20paper%20aims%20to%20fully%20automate%20this%20alignment%20task.%20The%20proposed%20method%20identifies%20frames%20in%20which%20the%20lighting%20system%20moves%2C%20realigns%20them%2C%20and%20selects%20the%20camera%20with%20the%20least%20occlusion%20to%20generate%20a%20video%20that%20consistently%20presents%20the%20surgical%20field%20from%20a%20fixed%20perspective.%20A%20user%20study%20involving%20surgeons%20demonstrated%20that%20videos%20generated%20by%20our%20method%20were%20superior%20to%20those%20produced%20by%20conventional%20methods%20in%20terms%20of%20the%20ease%20of%20confirming%20the%20surgical%20area%20and%20the%20comfort%20during%20video%20viewing.%20Additionally%2C%20our%20approach%20showed%20improvements%20in%20video%20quality%20over%20existing%20techniques.%20Furthermore%2C%20we%20implemented%20several%20synthesis%20options%20for%20the%20proposed%20view-synthesis%20method%20and%20conducted%20a%20user%20study%20to%20assess%20surgeons%27%20preferences%20for%20each%20option.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08577v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisturbance-Free%2520Surgical%2520Video%2520Generation%2520from%2520Multi-Camera%2520Shadowless%2520Lamps%2520for%2520Open%2520Surgery%26entry.906535625%3DYuna%2520Kato%2520and%2520Shohei%2520Mori%2520and%2520Hideo%2520Saito%2520and%2520Yoshifumi%2520Takatsume%2520and%2520Hiroki%2520Kajita%2520and%2520Mariko%2520Isogawa%26entry.1292438233%3DVideo%2520recordings%2520of%2520open%2520surgeries%2520are%2520greatly%2520required%2520for%2520education%2520and%2520research%2520purposes.%2520However%252C%2520capturing%2520unobstructed%2520videos%2520is%2520challenging%2520since%2520surgeons%2520frequently%2520block%2520the%2520camera%2520field%2520of%2520view.%2520To%2520avoid%2520occlusion%252C%2520the%2520positions%2520and%2520angles%2520of%2520the%2520camera%2520must%2520be%2520frequently%2520adjusted%252C%2520which%2520is%2520highly%2520labor-intensive.%2520Prior%2520work%2520has%2520addressed%2520this%2520issue%2520by%2520installing%2520multiple%2520cameras%2520on%2520a%2520shadowless%2520lamp%2520and%2520arranging%2520them%2520to%2520fully%2520surround%2520the%2520surgical%2520area.%2520This%2520setup%2520increases%2520the%2520chances%2520of%2520some%2520cameras%2520capturing%2520an%2520unobstructed%2520view.%2520However%252C%2520manual%2520image%2520alignment%2520is%2520needed%2520in%2520post-processing%2520since%2520camera%2520configurations%2520change%2520every%2520time%2520surgeons%2520move%2520the%2520lamp%2520for%2520optimal%2520lighting.%2520This%2520paper%2520aims%2520to%2520fully%2520automate%2520this%2520alignment%2520task.%2520The%2520proposed%2520method%2520identifies%2520frames%2520in%2520which%2520the%2520lighting%2520system%2520moves%252C%2520realigns%2520them%252C%2520and%2520selects%2520the%2520camera%2520with%2520the%2520least%2520occlusion%2520to%2520generate%2520a%2520video%2520that%2520consistently%2520presents%2520the%2520surgical%2520field%2520from%2520a%2520fixed%2520perspective.%2520A%2520user%2520study%2520involving%2520surgeons%2520demonstrated%2520that%2520videos%2520generated%2520by%2520our%2520method%2520were%2520superior%2520to%2520those%2520produced%2520by%2520conventional%2520methods%2520in%2520terms%2520of%2520the%2520ease%2520of%2520confirming%2520the%2520surgical%2520area%2520and%2520the%2520comfort%2520during%2520video%2520viewing.%2520Additionally%252C%2520our%2520approach%2520showed%2520improvements%2520in%2520video%2520quality%2520over%2520existing%2520techniques.%2520Furthermore%252C%2520we%2520implemented%2520several%2520synthesis%2520options%2520for%2520the%2520proposed%2520view-synthesis%2520method%2520and%2520conducted%2520a%2520user%2520study%2520to%2520assess%2520surgeons%2527%2520preferences%2520for%2520each%2520option.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08577v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disturbance-Free%20Surgical%20Video%20Generation%20from%20Multi-Camera%20Shadowless%20Lamps%20for%20Open%20Surgery&entry.906535625=Yuna%20Kato%20and%20Shohei%20Mori%20and%20Hideo%20Saito%20and%20Yoshifumi%20Takatsume%20and%20Hiroki%20Kajita%20and%20Mariko%20Isogawa&entry.1292438233=Video%20recordings%20of%20open%20surgeries%20are%20greatly%20required%20for%20education%20and%20research%20purposes.%20However%2C%20capturing%20unobstructed%20videos%20is%20challenging%20since%20surgeons%20frequently%20block%20the%20camera%20field%20of%20view.%20To%20avoid%20occlusion%2C%20the%20positions%20and%20angles%20of%20the%20camera%20must%20be%20frequently%20adjusted%2C%20which%20is%20highly%20labor-intensive.%20Prior%20work%20has%20addressed%20this%20issue%20by%20installing%20multiple%20cameras%20on%20a%20shadowless%20lamp%20and%20arranging%20them%20to%20fully%20surround%20the%20surgical%20area.%20This%20setup%20increases%20the%20chances%20of%20some%20cameras%20capturing%20an%20unobstructed%20view.%20However%2C%20manual%20image%20alignment%20is%20needed%20in%20post-processing%20since%20camera%20configurations%20change%20every%20time%20surgeons%20move%20the%20lamp%20for%20optimal%20lighting.%20This%20paper%20aims%20to%20fully%20automate%20this%20alignment%20task.%20The%20proposed%20method%20identifies%20frames%20in%20which%20the%20lighting%20system%20moves%2C%20realigns%20them%2C%20and%20selects%20the%20camera%20with%20the%20least%20occlusion%20to%20generate%20a%20video%20that%20consistently%20presents%20the%20surgical%20field%20from%20a%20fixed%20perspective.%20A%20user%20study%20involving%20surgeons%20demonstrated%20that%20videos%20generated%20by%20our%20method%20were%20superior%20to%20those%20produced%20by%20conventional%20methods%20in%20terms%20of%20the%20ease%20of%20confirming%20the%20surgical%20area%20and%20the%20comfort%20during%20video%20viewing.%20Additionally%2C%20our%20approach%20showed%20improvements%20in%20video%20quality%20over%20existing%20techniques.%20Furthermore%2C%20we%20implemented%20several%20synthesis%20options%20for%20the%20proposed%20view-synthesis%20method%20and%20conducted%20a%20user%20study%20to%20assess%20surgeons%27%20preferences%20for%20each%20option.&entry.1838667208=http%3A//arxiv.org/abs/2512.08577v1&entry.124074799=Read"},
{"title": "Deep generative modelling of canonical ensemble with differentiable thermal properties", "author": "Shuo-Hui Li and Yao-Wen Zhang and Ding Pan", "abstract": "It is a long-standing challenge to accurately and efficiently compute thermodynamic quantities of many-body systems at thermal equilibrium. The conventional methods, e.g., Markov chain Monte Carlo, require many steps to equilibrate. The recently developed deep learning methods can perform direct sampling, but only work at a single trained temperature point and risk biased sampling. Here, we propose a variational method for canonical ensembles with differentiable temperature, which gives thermodynamic quantities as continuous functions of temperature akin to an analytical solution. The proposed method is a general framework that works with any tractable density generative model. At optimal, the model is theoretically guaranteed to be the unbiased Boltzmann distribution. We validated our method by calculating phase transitions in the Ising and XY models, demonstrating that our direct-sampling simulations are as accurate as Markov chain Monte Carlo, but more efficient. Moreover, our differentiable free energy aligns closely with the exact one to the second-order derivative, indicating that the variational model captures the subtle thermal transitions at the phase transitions. This functional dependence on external parameters is a fundamental advancement in combining the exceptional fitting ability of deep learning with rigorous physical analysis.", "link": "http://arxiv.org/abs/2404.18404v2", "date": "2025-12-09", "relevancy": 1.4033, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4834}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4665}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4553}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Deep%20generative%20modelling%20of%20canonical%20ensemble%20with%20differentiable%20thermal%20properties&body=Title%3A%20Deep%20generative%20modelling%20of%20canonical%20ensemble%20with%20differentiable%20thermal%20properties%0AAuthor%3A%20Shuo-Hui%20Li%20and%20Yao-Wen%20Zhang%20and%20Ding%20Pan%0AAbstract%3A%20It%20is%20a%20long-standing%20challenge%20to%20accurately%20and%20efficiently%20compute%20thermodynamic%20quantities%20of%20many-body%20systems%20at%20thermal%20equilibrium.%20The%20conventional%20methods%2C%20e.g.%2C%20Markov%20chain%20Monte%20Carlo%2C%20require%20many%20steps%20to%20equilibrate.%20The%20recently%20developed%20deep%20learning%20methods%20can%20perform%20direct%20sampling%2C%20but%20only%20work%20at%20a%20single%20trained%20temperature%20point%20and%20risk%20biased%20sampling.%20Here%2C%20we%20propose%20a%20variational%20method%20for%20canonical%20ensembles%20with%20differentiable%20temperature%2C%20which%20gives%20thermodynamic%20quantities%20as%20continuous%20functions%20of%20temperature%20akin%20to%20an%20analytical%20solution.%20The%20proposed%20method%20is%20a%20general%20framework%20that%20works%20with%20any%20tractable%20density%20generative%20model.%20At%20optimal%2C%20the%20model%20is%20theoretically%20guaranteed%20to%20be%20the%20unbiased%20Boltzmann%20distribution.%20We%20validated%20our%20method%20by%20calculating%20phase%20transitions%20in%20the%20Ising%20and%20XY%20models%2C%20demonstrating%20that%20our%20direct-sampling%20simulations%20are%20as%20accurate%20as%20Markov%20chain%20Monte%20Carlo%2C%20but%20more%20efficient.%20Moreover%2C%20our%20differentiable%20free%20energy%20aligns%20closely%20with%20the%20exact%20one%20to%20the%20second-order%20derivative%2C%20indicating%20that%20the%20variational%20model%20captures%20the%20subtle%20thermal%20transitions%20at%20the%20phase%20transitions.%20This%20functional%20dependence%20on%20external%20parameters%20is%20a%20fundamental%20advancement%20in%20combining%20the%20exceptional%20fitting%20ability%20of%20deep%20learning%20with%20rigorous%20physical%20analysis.%0ALink%3A%20http%3A//arxiv.org/abs/2404.18404v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeep%2520generative%2520modelling%2520of%2520canonical%2520ensemble%2520with%2520differentiable%2520thermal%2520properties%26entry.906535625%3DShuo-Hui%2520Li%2520and%2520Yao-Wen%2520Zhang%2520and%2520Ding%2520Pan%26entry.1292438233%3DIt%2520is%2520a%2520long-standing%2520challenge%2520to%2520accurately%2520and%2520efficiently%2520compute%2520thermodynamic%2520quantities%2520of%2520many-body%2520systems%2520at%2520thermal%2520equilibrium.%2520The%2520conventional%2520methods%252C%2520e.g.%252C%2520Markov%2520chain%2520Monte%2520Carlo%252C%2520require%2520many%2520steps%2520to%2520equilibrate.%2520The%2520recently%2520developed%2520deep%2520learning%2520methods%2520can%2520perform%2520direct%2520sampling%252C%2520but%2520only%2520work%2520at%2520a%2520single%2520trained%2520temperature%2520point%2520and%2520risk%2520biased%2520sampling.%2520Here%252C%2520we%2520propose%2520a%2520variational%2520method%2520for%2520canonical%2520ensembles%2520with%2520differentiable%2520temperature%252C%2520which%2520gives%2520thermodynamic%2520quantities%2520as%2520continuous%2520functions%2520of%2520temperature%2520akin%2520to%2520an%2520analytical%2520solution.%2520The%2520proposed%2520method%2520is%2520a%2520general%2520framework%2520that%2520works%2520with%2520any%2520tractable%2520density%2520generative%2520model.%2520At%2520optimal%252C%2520the%2520model%2520is%2520theoretically%2520guaranteed%2520to%2520be%2520the%2520unbiased%2520Boltzmann%2520distribution.%2520We%2520validated%2520our%2520method%2520by%2520calculating%2520phase%2520transitions%2520in%2520the%2520Ising%2520and%2520XY%2520models%252C%2520demonstrating%2520that%2520our%2520direct-sampling%2520simulations%2520are%2520as%2520accurate%2520as%2520Markov%2520chain%2520Monte%2520Carlo%252C%2520but%2520more%2520efficient.%2520Moreover%252C%2520our%2520differentiable%2520free%2520energy%2520aligns%2520closely%2520with%2520the%2520exact%2520one%2520to%2520the%2520second-order%2520derivative%252C%2520indicating%2520that%2520the%2520variational%2520model%2520captures%2520the%2520subtle%2520thermal%2520transitions%2520at%2520the%2520phase%2520transitions.%2520This%2520functional%2520dependence%2520on%2520external%2520parameters%2520is%2520a%2520fundamental%2520advancement%2520in%2520combining%2520the%2520exceptional%2520fitting%2520ability%2520of%2520deep%2520learning%2520with%2520rigorous%2520physical%2520analysis.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.18404v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Deep%20generative%20modelling%20of%20canonical%20ensemble%20with%20differentiable%20thermal%20properties&entry.906535625=Shuo-Hui%20Li%20and%20Yao-Wen%20Zhang%20and%20Ding%20Pan&entry.1292438233=It%20is%20a%20long-standing%20challenge%20to%20accurately%20and%20efficiently%20compute%20thermodynamic%20quantities%20of%20many-body%20systems%20at%20thermal%20equilibrium.%20The%20conventional%20methods%2C%20e.g.%2C%20Markov%20chain%20Monte%20Carlo%2C%20require%20many%20steps%20to%20equilibrate.%20The%20recently%20developed%20deep%20learning%20methods%20can%20perform%20direct%20sampling%2C%20but%20only%20work%20at%20a%20single%20trained%20temperature%20point%20and%20risk%20biased%20sampling.%20Here%2C%20we%20propose%20a%20variational%20method%20for%20canonical%20ensembles%20with%20differentiable%20temperature%2C%20which%20gives%20thermodynamic%20quantities%20as%20continuous%20functions%20of%20temperature%20akin%20to%20an%20analytical%20solution.%20The%20proposed%20method%20is%20a%20general%20framework%20that%20works%20with%20any%20tractable%20density%20generative%20model.%20At%20optimal%2C%20the%20model%20is%20theoretically%20guaranteed%20to%20be%20the%20unbiased%20Boltzmann%20distribution.%20We%20validated%20our%20method%20by%20calculating%20phase%20transitions%20in%20the%20Ising%20and%20XY%20models%2C%20demonstrating%20that%20our%20direct-sampling%20simulations%20are%20as%20accurate%20as%20Markov%20chain%20Monte%20Carlo%2C%20but%20more%20efficient.%20Moreover%2C%20our%20differentiable%20free%20energy%20aligns%20closely%20with%20the%20exact%20one%20to%20the%20second-order%20derivative%2C%20indicating%20that%20the%20variational%20model%20captures%20the%20subtle%20thermal%20transitions%20at%20the%20phase%20transitions.%20This%20functional%20dependence%20on%20external%20parameters%20is%20a%20fundamental%20advancement%20in%20combining%20the%20exceptional%20fitting%20ability%20of%20deep%20learning%20with%20rigorous%20physical%20analysis.&entry.1838667208=http%3A//arxiv.org/abs/2404.18404v2&entry.124074799=Read"},
{"title": "Decoupled Design of Time-Varying Control Barrier Functions via Equivariances", "author": "Adrian Wiltz and Dimos V. Dimarogonas", "abstract": "This article presents a systematic method for designing time-varying Control Barrier Functions (CBF) composed of a time-invariant component and multiple time-dependent components, leveraging structural properties of the system dynamics. The method involves the construction of a specific class of time-invariant CBFs that encode the system's dynamic capabilities with respect to a given constraint, and augments them subsequently with appropriately designed time-dependent transformations. While transformations uniformly varying the time-invariant CBF can be applied to arbitrary systems, transformations exploiting structural properties in the dynamics - equivariances in particular - enable the handling of a broader and more expressive class of time-varying constraints. The article shows how to leverage such properties in the design of time-varying CBFs. The proposed method decouples the design of time variations from the computationally expensive construction of the underlying CBFs, thereby providing a computationally attractive method to the design of time-varying CBFs. The method accounts for input constraints and under-actuation, and requires only qualitative knowledge on the time-variation of the constraints making it suitable to the application in uncertain environments.", "link": "http://arxiv.org/abs/2512.08607v1", "date": "2025-12-09", "relevancy": 1.1944, "topK": [{"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4003}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3967}, {"title": "Thinking Outside the BBox: Unconstrained Generative Object Compositing", "link": "http://arxiv.org/abs/2409.04559v2", "similarity": 0.3943}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoupled%20Design%20of%20Time-Varying%20Control%20Barrier%20Functions%20via%20Equivariances&body=Title%3A%20Decoupled%20Design%20of%20Time-Varying%20Control%20Barrier%20Functions%20via%20Equivariances%0AAuthor%3A%20Adrian%20Wiltz%20and%20Dimos%20V.%20Dimarogonas%0AAbstract%3A%20This%20article%20presents%20a%20systematic%20method%20for%20designing%20time-varying%20Control%20Barrier%20Functions%20%28CBF%29%20composed%20of%20a%20time-invariant%20component%20and%20multiple%20time-dependent%20components%2C%20leveraging%20structural%20properties%20of%20the%20system%20dynamics.%20The%20method%20involves%20the%20construction%20of%20a%20specific%20class%20of%20time-invariant%20CBFs%20that%20encode%20the%20system%27s%20dynamic%20capabilities%20with%20respect%20to%20a%20given%20constraint%2C%20and%20augments%20them%20subsequently%20with%20appropriately%20designed%20time-dependent%20transformations.%20While%20transformations%20uniformly%20varying%20the%20time-invariant%20CBF%20can%20be%20applied%20to%20arbitrary%20systems%2C%20transformations%20exploiting%20structural%20properties%20in%20the%20dynamics%20-%20equivariances%20in%20particular%20-%20enable%20the%20handling%20of%20a%20broader%20and%20more%20expressive%20class%20of%20time-varying%20constraints.%20The%20article%20shows%20how%20to%20leverage%20such%20properties%20in%20the%20design%20of%20time-varying%20CBFs.%20The%20proposed%20method%20decouples%20the%20design%20of%20time%20variations%20from%20the%20computationally%20expensive%20construction%20of%20the%20underlying%20CBFs%2C%20thereby%20providing%20a%20computationally%20attractive%20method%20to%20the%20design%20of%20time-varying%20CBFs.%20The%20method%20accounts%20for%20input%20constraints%20and%20under-actuation%2C%20and%20requires%20only%20qualitative%20knowledge%20on%20the%20time-variation%20of%20the%20constraints%20making%20it%20suitable%20to%20the%20application%20in%20uncertain%20environments.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08607v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoupled%2520Design%2520of%2520Time-Varying%2520Control%2520Barrier%2520Functions%2520via%2520Equivariances%26entry.906535625%3DAdrian%2520Wiltz%2520and%2520Dimos%2520V.%2520Dimarogonas%26entry.1292438233%3DThis%2520article%2520presents%2520a%2520systematic%2520method%2520for%2520designing%2520time-varying%2520Control%2520Barrier%2520Functions%2520%2528CBF%2529%2520composed%2520of%2520a%2520time-invariant%2520component%2520and%2520multiple%2520time-dependent%2520components%252C%2520leveraging%2520structural%2520properties%2520of%2520the%2520system%2520dynamics.%2520The%2520method%2520involves%2520the%2520construction%2520of%2520a%2520specific%2520class%2520of%2520time-invariant%2520CBFs%2520that%2520encode%2520the%2520system%2527s%2520dynamic%2520capabilities%2520with%2520respect%2520to%2520a%2520given%2520constraint%252C%2520and%2520augments%2520them%2520subsequently%2520with%2520appropriately%2520designed%2520time-dependent%2520transformations.%2520While%2520transformations%2520uniformly%2520varying%2520the%2520time-invariant%2520CBF%2520can%2520be%2520applied%2520to%2520arbitrary%2520systems%252C%2520transformations%2520exploiting%2520structural%2520properties%2520in%2520the%2520dynamics%2520-%2520equivariances%2520in%2520particular%2520-%2520enable%2520the%2520handling%2520of%2520a%2520broader%2520and%2520more%2520expressive%2520class%2520of%2520time-varying%2520constraints.%2520The%2520article%2520shows%2520how%2520to%2520leverage%2520such%2520properties%2520in%2520the%2520design%2520of%2520time-varying%2520CBFs.%2520The%2520proposed%2520method%2520decouples%2520the%2520design%2520of%2520time%2520variations%2520from%2520the%2520computationally%2520expensive%2520construction%2520of%2520the%2520underlying%2520CBFs%252C%2520thereby%2520providing%2520a%2520computationally%2520attractive%2520method%2520to%2520the%2520design%2520of%2520time-varying%2520CBFs.%2520The%2520method%2520accounts%2520for%2520input%2520constraints%2520and%2520under-actuation%252C%2520and%2520requires%2520only%2520qualitative%2520knowledge%2520on%2520the%2520time-variation%2520of%2520the%2520constraints%2520making%2520it%2520suitable%2520to%2520the%2520application%2520in%2520uncertain%2520environments.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08607v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoupled%20Design%20of%20Time-Varying%20Control%20Barrier%20Functions%20via%20Equivariances&entry.906535625=Adrian%20Wiltz%20and%20Dimos%20V.%20Dimarogonas&entry.1292438233=This%20article%20presents%20a%20systematic%20method%20for%20designing%20time-varying%20Control%20Barrier%20Functions%20%28CBF%29%20composed%20of%20a%20time-invariant%20component%20and%20multiple%20time-dependent%20components%2C%20leveraging%20structural%20properties%20of%20the%20system%20dynamics.%20The%20method%20involves%20the%20construction%20of%20a%20specific%20class%20of%20time-invariant%20CBFs%20that%20encode%20the%20system%27s%20dynamic%20capabilities%20with%20respect%20to%20a%20given%20constraint%2C%20and%20augments%20them%20subsequently%20with%20appropriately%20designed%20time-dependent%20transformations.%20While%20transformations%20uniformly%20varying%20the%20time-invariant%20CBF%20can%20be%20applied%20to%20arbitrary%20systems%2C%20transformations%20exploiting%20structural%20properties%20in%20the%20dynamics%20-%20equivariances%20in%20particular%20-%20enable%20the%20handling%20of%20a%20broader%20and%20more%20expressive%20class%20of%20time-varying%20constraints.%20The%20article%20shows%20how%20to%20leverage%20such%20properties%20in%20the%20design%20of%20time-varying%20CBFs.%20The%20proposed%20method%20decouples%20the%20design%20of%20time%20variations%20from%20the%20computationally%20expensive%20construction%20of%20the%20underlying%20CBFs%2C%20thereby%20providing%20a%20computationally%20attractive%20method%20to%20the%20design%20of%20time-varying%20CBFs.%20The%20method%20accounts%20for%20input%20constraints%20and%20under-actuation%2C%20and%20requires%20only%20qualitative%20knowledge%20on%20the%20time-variation%20of%20the%20constraints%20making%20it%20suitable%20to%20the%20application%20in%20uncertain%20environments.&entry.1838667208=http%3A//arxiv.org/abs/2512.08607v1&entry.124074799=Read"},
{"title": "Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans", "author": "Tammy Zhong and Yang Song and Maurice Pagnucco", "abstract": "Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.", "link": "http://arxiv.org/abs/2512.08536v1", "date": "2025-12-09", "relevancy": 1.3917, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5144}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4498}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4487}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Principles2Plan%3A%20LLM-Guided%20System%20for%20Operationalising%20Ethical%20Principles%20into%20Plans&body=Title%3A%20Principles2Plan%3A%20LLM-Guided%20System%20for%20Operationalising%20Ethical%20Principles%20into%20Plans%0AAuthor%3A%20Tammy%20Zhong%20and%20Yang%20Song%20and%20Maurice%20Pagnucco%0AAbstract%3A%20Ethical%20awareness%20is%20critical%20for%20robots%20operating%20in%20human%20environments%2C%20yet%20existing%20automated%20planning%20tools%20provide%20little%20support.%20Manually%20specifying%20ethical%20rules%20is%20labour-intensive%20and%20highly%20context-specific.%20We%20present%20Principles2Plan%2C%20an%20interactive%20research%20prototype%20demonstrating%20how%20a%20human%20and%20a%20Large%20Language%20Model%20%28LLM%29%20can%20collaborate%20to%20produce%20context-sensitive%20ethical%20rules%20and%20guide%20automated%20planning.%20A%20domain%20expert%20provides%20the%20planning%20domain%2C%20problem%20details%2C%20and%20relevant%20high-level%20principles%20such%20as%20beneficence%20and%20privacy.%20The%20system%20generates%20operationalisable%20ethical%20rules%20consistent%20with%20these%20principles%2C%20which%20the%20user%20can%20review%2C%20prioritise%2C%20and%20supply%20to%20a%20planner%20to%20produce%20ethically-informed%20plans.%20To%20our%20knowledge%2C%20no%20prior%20system%20supports%20users%20in%20generating%20principle-grounded%20rules%20for%20classical%20planning%20contexts.%20Principles2Plan%20showcases%20the%20potential%20of%20human-LLM%20collaboration%20for%20making%20ethical%20automated%20planning%20more%20practical%20and%20feasible.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08536v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPrinciples2Plan%253A%2520LLM-Guided%2520System%2520for%2520Operationalising%2520Ethical%2520Principles%2520into%2520Plans%26entry.906535625%3DTammy%2520Zhong%2520and%2520Yang%2520Song%2520and%2520Maurice%2520Pagnucco%26entry.1292438233%3DEthical%2520awareness%2520is%2520critical%2520for%2520robots%2520operating%2520in%2520human%2520environments%252C%2520yet%2520existing%2520automated%2520planning%2520tools%2520provide%2520little%2520support.%2520Manually%2520specifying%2520ethical%2520rules%2520is%2520labour-intensive%2520and%2520highly%2520context-specific.%2520We%2520present%2520Principles2Plan%252C%2520an%2520interactive%2520research%2520prototype%2520demonstrating%2520how%2520a%2520human%2520and%2520a%2520Large%2520Language%2520Model%2520%2528LLM%2529%2520can%2520collaborate%2520to%2520produce%2520context-sensitive%2520ethical%2520rules%2520and%2520guide%2520automated%2520planning.%2520A%2520domain%2520expert%2520provides%2520the%2520planning%2520domain%252C%2520problem%2520details%252C%2520and%2520relevant%2520high-level%2520principles%2520such%2520as%2520beneficence%2520and%2520privacy.%2520The%2520system%2520generates%2520operationalisable%2520ethical%2520rules%2520consistent%2520with%2520these%2520principles%252C%2520which%2520the%2520user%2520can%2520review%252C%2520prioritise%252C%2520and%2520supply%2520to%2520a%2520planner%2520to%2520produce%2520ethically-informed%2520plans.%2520To%2520our%2520knowledge%252C%2520no%2520prior%2520system%2520supports%2520users%2520in%2520generating%2520principle-grounded%2520rules%2520for%2520classical%2520planning%2520contexts.%2520Principles2Plan%2520showcases%2520the%2520potential%2520of%2520human-LLM%2520collaboration%2520for%2520making%2520ethical%2520automated%2520planning%2520more%2520practical%2520and%2520feasible.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08536v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Principles2Plan%3A%20LLM-Guided%20System%20for%20Operationalising%20Ethical%20Principles%20into%20Plans&entry.906535625=Tammy%20Zhong%20and%20Yang%20Song%20and%20Maurice%20Pagnucco&entry.1292438233=Ethical%20awareness%20is%20critical%20for%20robots%20operating%20in%20human%20environments%2C%20yet%20existing%20automated%20planning%20tools%20provide%20little%20support.%20Manually%20specifying%20ethical%20rules%20is%20labour-intensive%20and%20highly%20context-specific.%20We%20present%20Principles2Plan%2C%20an%20interactive%20research%20prototype%20demonstrating%20how%20a%20human%20and%20a%20Large%20Language%20Model%20%28LLM%29%20can%20collaborate%20to%20produce%20context-sensitive%20ethical%20rules%20and%20guide%20automated%20planning.%20A%20domain%20expert%20provides%20the%20planning%20domain%2C%20problem%20details%2C%20and%20relevant%20high-level%20principles%20such%20as%20beneficence%20and%20privacy.%20The%20system%20generates%20operationalisable%20ethical%20rules%20consistent%20with%20these%20principles%2C%20which%20the%20user%20can%20review%2C%20prioritise%2C%20and%20supply%20to%20a%20planner%20to%20produce%20ethically-informed%20plans.%20To%20our%20knowledge%2C%20no%20prior%20system%20supports%20users%20in%20generating%20principle-grounded%20rules%20for%20classical%20planning%20contexts.%20Principles2Plan%20showcases%20the%20potential%20of%20human-LLM%20collaboration%20for%20making%20ethical%20automated%20planning%20more%20practical%20and%20feasible.&entry.1838667208=http%3A//arxiv.org/abs/2512.08536v1&entry.124074799=Read"},
{"title": "Curse of Slicing: Why Sliced Mutual Information is a Deceptive Measure of Statistical Dependence", "author": "Alexander Semenenko and Ivan Butakov and Alexey Frolov and Ivan Oseledets", "abstract": "Sliced Mutual Information (SMI) is widely used as a scalable alternative to mutual information for measuring non-linear statistical dependence. Despite its advantages, such as faster convergence, robustness to high dimensionality, and nullification only under statistical independence, we demonstrate that SMI is highly susceptible to data manipulation and exhibits counterintuitive behavior. Through extensive benchmarking and theoretical analysis, we show that SMI saturates easily, fails to detect increases in statistical dependence, prioritizes redundancy over informative content, and in some cases, performs worse than correlation coefficient.", "link": "http://arxiv.org/abs/2506.04053v3", "date": "2025-12-09", "relevancy": 1.4928, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.3785}, {"title": "MiraGe: Editable 2D Images using Gaussian Splatting", "link": "http://arxiv.org/abs/2410.01521v1", "similarity": 0.3707}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Curse%20of%20Slicing%3A%20Why%20Sliced%20Mutual%20Information%20is%20a%20Deceptive%20Measure%20of%20Statistical%20Dependence&body=Title%3A%20Curse%20of%20Slicing%3A%20Why%20Sliced%20Mutual%20Information%20is%20a%20Deceptive%20Measure%20of%20Statistical%20Dependence%0AAuthor%3A%20Alexander%20Semenenko%20and%20Ivan%20Butakov%20and%20Alexey%20Frolov%20and%20Ivan%20Oseledets%0AAbstract%3A%20Sliced%20Mutual%20Information%20%28SMI%29%20is%20widely%20used%20as%20a%20scalable%20alternative%20to%20mutual%20information%20for%20measuring%20non-linear%20statistical%20dependence.%20Despite%20its%20advantages%2C%20such%20as%20faster%20convergence%2C%20robustness%20to%20high%20dimensionality%2C%20and%20nullification%20only%20under%20statistical%20independence%2C%20we%20demonstrate%20that%20SMI%20is%20highly%20susceptible%20to%20data%20manipulation%20and%20exhibits%20counterintuitive%20behavior.%20Through%20extensive%20benchmarking%20and%20theoretical%20analysis%2C%20we%20show%20that%20SMI%20saturates%20easily%2C%20fails%20to%20detect%20increases%20in%20statistical%20dependence%2C%20prioritizes%20redundancy%20over%20informative%20content%2C%20and%20in%20some%20cases%2C%20performs%20worse%20than%20correlation%20coefficient.%0ALink%3A%20http%3A//arxiv.org/abs/2506.04053v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCurse%2520of%2520Slicing%253A%2520Why%2520Sliced%2520Mutual%2520Information%2520is%2520a%2520Deceptive%2520Measure%2520of%2520Statistical%2520Dependence%26entry.906535625%3DAlexander%2520Semenenko%2520and%2520Ivan%2520Butakov%2520and%2520Alexey%2520Frolov%2520and%2520Ivan%2520Oseledets%26entry.1292438233%3DSliced%2520Mutual%2520Information%2520%2528SMI%2529%2520is%2520widely%2520used%2520as%2520a%2520scalable%2520alternative%2520to%2520mutual%2520information%2520for%2520measuring%2520non-linear%2520statistical%2520dependence.%2520Despite%2520its%2520advantages%252C%2520such%2520as%2520faster%2520convergence%252C%2520robustness%2520to%2520high%2520dimensionality%252C%2520and%2520nullification%2520only%2520under%2520statistical%2520independence%252C%2520we%2520demonstrate%2520that%2520SMI%2520is%2520highly%2520susceptible%2520to%2520data%2520manipulation%2520and%2520exhibits%2520counterintuitive%2520behavior.%2520Through%2520extensive%2520benchmarking%2520and%2520theoretical%2520analysis%252C%2520we%2520show%2520that%2520SMI%2520saturates%2520easily%252C%2520fails%2520to%2520detect%2520increases%2520in%2520statistical%2520dependence%252C%2520prioritizes%2520redundancy%2520over%2520informative%2520content%252C%2520and%2520in%2520some%2520cases%252C%2520performs%2520worse%2520than%2520correlation%2520coefficient.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2506.04053v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Curse%20of%20Slicing%3A%20Why%20Sliced%20Mutual%20Information%20is%20a%20Deceptive%20Measure%20of%20Statistical%20Dependence&entry.906535625=Alexander%20Semenenko%20and%20Ivan%20Butakov%20and%20Alexey%20Frolov%20and%20Ivan%20Oseledets&entry.1292438233=Sliced%20Mutual%20Information%20%28SMI%29%20is%20widely%20used%20as%20a%20scalable%20alternative%20to%20mutual%20information%20for%20measuring%20non-linear%20statistical%20dependence.%20Despite%20its%20advantages%2C%20such%20as%20faster%20convergence%2C%20robustness%20to%20high%20dimensionality%2C%20and%20nullification%20only%20under%20statistical%20independence%2C%20we%20demonstrate%20that%20SMI%20is%20highly%20susceptible%20to%20data%20manipulation%20and%20exhibits%20counterintuitive%20behavior.%20Through%20extensive%20benchmarking%20and%20theoretical%20analysis%2C%20we%20show%20that%20SMI%20saturates%20easily%2C%20fails%20to%20detect%20increases%20in%20statistical%20dependence%2C%20prioritizes%20redundancy%20over%20informative%20content%2C%20and%20in%20some%20cases%2C%20performs%20worse%20than%20correlation%20coefficient.&entry.1838667208=http%3A//arxiv.org/abs/2506.04053v3&entry.124074799=Read"},
{"title": "Multi-domain performance analysis with scores tailored to user preferences", "author": "S\u00e9bastien Pi\u00e9rard and Adrien Deli\u00e8ge and Marc Van Droogenbroeck", "abstract": "The performance of algorithms, methods, and models tends to depend heavily on the distribution of cases on which they are applied, this distribution being specific to the applicative domain. After performing an evaluation in several domains, it is highly informative to compute a (weighted) mean performance and, as shown in this paper, to scrutinize what happens during this averaging. To achieve this goal, we adopt a probabilistic framework and consider a performance as a probability measure (e.g., a normalized confusion matrix for a classification task). It appears that the corresponding weighted mean is known to be the summarization, and that only some remarkable scores assign to the summarized performance a value equal to a weighted arithmetic mean of the values assigned to the domain-specific performances. These scores include the family of ranking scores, a continuum parameterized by user preferences, and that the weights to consider in the arithmetic mean depend on the user preferences. Based on this, we rigorously define four domains, named easiest, most difficult, preponderant, and bottleneck domains, as functions of user preferences. After establishing the theory in a general setting, regardless of the task, we develop new visual tools for two-class classification.", "link": "http://arxiv.org/abs/2512.08715v1", "date": "2025-12-09", "relevancy": 1.8313, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.497}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4568}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4432}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-domain%20performance%20analysis%20with%20scores%20tailored%20to%20user%20preferences&body=Title%3A%20Multi-domain%20performance%20analysis%20with%20scores%20tailored%20to%20user%20preferences%0AAuthor%3A%20S%C3%A9bastien%20Pi%C3%A9rard%20and%20Adrien%20Deli%C3%A8ge%20and%20Marc%20Van%20Droogenbroeck%0AAbstract%3A%20The%20performance%20of%20algorithms%2C%20methods%2C%20and%20models%20tends%20to%20depend%20heavily%20on%20the%20distribution%20of%20cases%20on%20which%20they%20are%20applied%2C%20this%20distribution%20being%20specific%20to%20the%20applicative%20domain.%20After%20performing%20an%20evaluation%20in%20several%20domains%2C%20it%20is%20highly%20informative%20to%20compute%20a%20%28weighted%29%20mean%20performance%20and%2C%20as%20shown%20in%20this%20paper%2C%20to%20scrutinize%20what%20happens%20during%20this%20averaging.%20To%20achieve%20this%20goal%2C%20we%20adopt%20a%20probabilistic%20framework%20and%20consider%20a%20performance%20as%20a%20probability%20measure%20%28e.g.%2C%20a%20normalized%20confusion%20matrix%20for%20a%20classification%20task%29.%20It%20appears%20that%20the%20corresponding%20weighted%20mean%20is%20known%20to%20be%20the%20summarization%2C%20and%20that%20only%20some%20remarkable%20scores%20assign%20to%20the%20summarized%20performance%20a%20value%20equal%20to%20a%20weighted%20arithmetic%20mean%20of%20the%20values%20assigned%20to%20the%20domain-specific%20performances.%20These%20scores%20include%20the%20family%20of%20ranking%20scores%2C%20a%20continuum%20parameterized%20by%20user%20preferences%2C%20and%20that%20the%20weights%20to%20consider%20in%20the%20arithmetic%20mean%20depend%20on%20the%20user%20preferences.%20Based%20on%20this%2C%20we%20rigorously%20define%20four%20domains%2C%20named%20easiest%2C%20most%20difficult%2C%20preponderant%2C%20and%20bottleneck%20domains%2C%20as%20functions%20of%20user%20preferences.%20After%20establishing%20the%20theory%20in%20a%20general%20setting%2C%20regardless%20of%20the%20task%2C%20we%20develop%20new%20visual%20tools%20for%20two-class%20classification.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08715v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-domain%2520performance%2520analysis%2520with%2520scores%2520tailored%2520to%2520user%2520preferences%26entry.906535625%3DS%25C3%25A9bastien%2520Pi%25C3%25A9rard%2520and%2520Adrien%2520Deli%25C3%25A8ge%2520and%2520Marc%2520Van%2520Droogenbroeck%26entry.1292438233%3DThe%2520performance%2520of%2520algorithms%252C%2520methods%252C%2520and%2520models%2520tends%2520to%2520depend%2520heavily%2520on%2520the%2520distribution%2520of%2520cases%2520on%2520which%2520they%2520are%2520applied%252C%2520this%2520distribution%2520being%2520specific%2520to%2520the%2520applicative%2520domain.%2520After%2520performing%2520an%2520evaluation%2520in%2520several%2520domains%252C%2520it%2520is%2520highly%2520informative%2520to%2520compute%2520a%2520%2528weighted%2529%2520mean%2520performance%2520and%252C%2520as%2520shown%2520in%2520this%2520paper%252C%2520to%2520scrutinize%2520what%2520happens%2520during%2520this%2520averaging.%2520To%2520achieve%2520this%2520goal%252C%2520we%2520adopt%2520a%2520probabilistic%2520framework%2520and%2520consider%2520a%2520performance%2520as%2520a%2520probability%2520measure%2520%2528e.g.%252C%2520a%2520normalized%2520confusion%2520matrix%2520for%2520a%2520classification%2520task%2529.%2520It%2520appears%2520that%2520the%2520corresponding%2520weighted%2520mean%2520is%2520known%2520to%2520be%2520the%2520summarization%252C%2520and%2520that%2520only%2520some%2520remarkable%2520scores%2520assign%2520to%2520the%2520summarized%2520performance%2520a%2520value%2520equal%2520to%2520a%2520weighted%2520arithmetic%2520mean%2520of%2520the%2520values%2520assigned%2520to%2520the%2520domain-specific%2520performances.%2520These%2520scores%2520include%2520the%2520family%2520of%2520ranking%2520scores%252C%2520a%2520continuum%2520parameterized%2520by%2520user%2520preferences%252C%2520and%2520that%2520the%2520weights%2520to%2520consider%2520in%2520the%2520arithmetic%2520mean%2520depend%2520on%2520the%2520user%2520preferences.%2520Based%2520on%2520this%252C%2520we%2520rigorously%2520define%2520four%2520domains%252C%2520named%2520easiest%252C%2520most%2520difficult%252C%2520preponderant%252C%2520and%2520bottleneck%2520domains%252C%2520as%2520functions%2520of%2520user%2520preferences.%2520After%2520establishing%2520the%2520theory%2520in%2520a%2520general%2520setting%252C%2520regardless%2520of%2520the%2520task%252C%2520we%2520develop%2520new%2520visual%2520tools%2520for%2520two-class%2520classification.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08715v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-domain%20performance%20analysis%20with%20scores%20tailored%20to%20user%20preferences&entry.906535625=S%C3%A9bastien%20Pi%C3%A9rard%20and%20Adrien%20Deli%C3%A8ge%20and%20Marc%20Van%20Droogenbroeck&entry.1292438233=The%20performance%20of%20algorithms%2C%20methods%2C%20and%20models%20tends%20to%20depend%20heavily%20on%20the%20distribution%20of%20cases%20on%20which%20they%20are%20applied%2C%20this%20distribution%20being%20specific%20to%20the%20applicative%20domain.%20After%20performing%20an%20evaluation%20in%20several%20domains%2C%20it%20is%20highly%20informative%20to%20compute%20a%20%28weighted%29%20mean%20performance%20and%2C%20as%20shown%20in%20this%20paper%2C%20to%20scrutinize%20what%20happens%20during%20this%20averaging.%20To%20achieve%20this%20goal%2C%20we%20adopt%20a%20probabilistic%20framework%20and%20consider%20a%20performance%20as%20a%20probability%20measure%20%28e.g.%2C%20a%20normalized%20confusion%20matrix%20for%20a%20classification%20task%29.%20It%20appears%20that%20the%20corresponding%20weighted%20mean%20is%20known%20to%20be%20the%20summarization%2C%20and%20that%20only%20some%20remarkable%20scores%20assign%20to%20the%20summarized%20performance%20a%20value%20equal%20to%20a%20weighted%20arithmetic%20mean%20of%20the%20values%20assigned%20to%20the%20domain-specific%20performances.%20These%20scores%20include%20the%20family%20of%20ranking%20scores%2C%20a%20continuum%20parameterized%20by%20user%20preferences%2C%20and%20that%20the%20weights%20to%20consider%20in%20the%20arithmetic%20mean%20depend%20on%20the%20user%20preferences.%20Based%20on%20this%2C%20we%20rigorously%20define%20four%20domains%2C%20named%20easiest%2C%20most%20difficult%2C%20preponderant%2C%20and%20bottleneck%20domains%2C%20as%20functions%20of%20user%20preferences.%20After%20establishing%20the%20theory%20in%20a%20general%20setting%2C%20regardless%20of%20the%20task%2C%20we%20develop%20new%20visual%20tools%20for%20two-class%20classification.&entry.1838667208=http%3A//arxiv.org/abs/2512.08715v1&entry.124074799=Read"},
{"title": "db-LaCAM: Fast and Scalable Multi-Robot Kinodynamic Motion Planning with Discontinuity-Bounded Search and Lightweight MAPF", "author": "Akmaral Moldagalieva and Keisuke Okumura and Amanda Prorok and Wolfgang H\u00f6nig", "abstract": "State-of-the-art multi-robot kinodynamic motion planners struggle to handle more than a few robots due to high computational burden, which limits their scalability and results in slow planning time. In this work, we combine the scalability and speed of modern multi-agent path finding (MAPF) algorithms with the dynamic-awareness of kinodynamic planners to address these limitations. To this end, we propose discontinuity-Bounded LaCAM (db-LaCAM), a planner that utilizes a precomputed set of motion primitives that respect robot dynamics to generate horizon-length motion sequences, while allowing a user-defined discontinuity between successive motions. The planner db-LaCAM is resolution-complete with respect to motion primitives and supports arbitrary robot dynamics. Extensive experiments demonstrate that db-LaCAM scales efficiently to scenarios with up to 50 robots, achieving up to ten times faster runtime compared to state-of-the-art planners, while maintaining comparable solution quality. The approach is validated in both 2D and 3D environments with dynamics such as the unicycle and 3D double integrator. We demonstrate the safe execution of trajectories planned with db-LaCAM in two distinct physical experiments involving teams of flying robots and car-with-trailer robots.", "link": "http://arxiv.org/abs/2512.06796v2", "date": "2025-12-09", "relevancy": 1.6808, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.568}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5675}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5338}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20db-LaCAM%3A%20Fast%20and%20Scalable%20Multi-Robot%20Kinodynamic%20Motion%20Planning%20with%20Discontinuity-Bounded%20Search%20and%20Lightweight%20MAPF&body=Title%3A%20db-LaCAM%3A%20Fast%20and%20Scalable%20Multi-Robot%20Kinodynamic%20Motion%20Planning%20with%20Discontinuity-Bounded%20Search%20and%20Lightweight%20MAPF%0AAuthor%3A%20Akmaral%20Moldagalieva%20and%20Keisuke%20Okumura%20and%20Amanda%20Prorok%20and%20Wolfgang%20H%C3%B6nig%0AAbstract%3A%20State-of-the-art%20multi-robot%20kinodynamic%20motion%20planners%20struggle%20to%20handle%20more%20than%20a%20few%20robots%20due%20to%20high%20computational%20burden%2C%20which%20limits%20their%20scalability%20and%20results%20in%20slow%20planning%20time.%20In%20this%20work%2C%20we%20combine%20the%20scalability%20and%20speed%20of%20modern%20multi-agent%20path%20finding%20%28MAPF%29%20algorithms%20with%20the%20dynamic-awareness%20of%20kinodynamic%20planners%20to%20address%20these%20limitations.%20To%20this%20end%2C%20we%20propose%20discontinuity-Bounded%20LaCAM%20%28db-LaCAM%29%2C%20a%20planner%20that%20utilizes%20a%20precomputed%20set%20of%20motion%20primitives%20that%20respect%20robot%20dynamics%20to%20generate%20horizon-length%20motion%20sequences%2C%20while%20allowing%20a%20user-defined%20discontinuity%20between%20successive%20motions.%20The%20planner%20db-LaCAM%20is%20resolution-complete%20with%20respect%20to%20motion%20primitives%20and%20supports%20arbitrary%20robot%20dynamics.%20Extensive%20experiments%20demonstrate%20that%20db-LaCAM%20scales%20efficiently%20to%20scenarios%20with%20up%20to%2050%20robots%2C%20achieving%20up%20to%20ten%20times%20faster%20runtime%20compared%20to%20state-of-the-art%20planners%2C%20while%20maintaining%20comparable%20solution%20quality.%20The%20approach%20is%20validated%20in%20both%202D%20and%203D%20environments%20with%20dynamics%20such%20as%20the%20unicycle%20and%203D%20double%20integrator.%20We%20demonstrate%20the%20safe%20execution%20of%20trajectories%20planned%20with%20db-LaCAM%20in%20two%20distinct%20physical%20experiments%20involving%20teams%20of%20flying%20robots%20and%20car-with-trailer%20robots.%0ALink%3A%20http%3A//arxiv.org/abs/2512.06796v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3Ddb-LaCAM%253A%2520Fast%2520and%2520Scalable%2520Multi-Robot%2520Kinodynamic%2520Motion%2520Planning%2520with%2520Discontinuity-Bounded%2520Search%2520and%2520Lightweight%2520MAPF%26entry.906535625%3DAkmaral%2520Moldagalieva%2520and%2520Keisuke%2520Okumura%2520and%2520Amanda%2520Prorok%2520and%2520Wolfgang%2520H%25C3%25B6nig%26entry.1292438233%3DState-of-the-art%2520multi-robot%2520kinodynamic%2520motion%2520planners%2520struggle%2520to%2520handle%2520more%2520than%2520a%2520few%2520robots%2520due%2520to%2520high%2520computational%2520burden%252C%2520which%2520limits%2520their%2520scalability%2520and%2520results%2520in%2520slow%2520planning%2520time.%2520In%2520this%2520work%252C%2520we%2520combine%2520the%2520scalability%2520and%2520speed%2520of%2520modern%2520multi-agent%2520path%2520finding%2520%2528MAPF%2529%2520algorithms%2520with%2520the%2520dynamic-awareness%2520of%2520kinodynamic%2520planners%2520to%2520address%2520these%2520limitations.%2520To%2520this%2520end%252C%2520we%2520propose%2520discontinuity-Bounded%2520LaCAM%2520%2528db-LaCAM%2529%252C%2520a%2520planner%2520that%2520utilizes%2520a%2520precomputed%2520set%2520of%2520motion%2520primitives%2520that%2520respect%2520robot%2520dynamics%2520to%2520generate%2520horizon-length%2520motion%2520sequences%252C%2520while%2520allowing%2520a%2520user-defined%2520discontinuity%2520between%2520successive%2520motions.%2520The%2520planner%2520db-LaCAM%2520is%2520resolution-complete%2520with%2520respect%2520to%2520motion%2520primitives%2520and%2520supports%2520arbitrary%2520robot%2520dynamics.%2520Extensive%2520experiments%2520demonstrate%2520that%2520db-LaCAM%2520scales%2520efficiently%2520to%2520scenarios%2520with%2520up%2520to%252050%2520robots%252C%2520achieving%2520up%2520to%2520ten%2520times%2520faster%2520runtime%2520compared%2520to%2520state-of-the-art%2520planners%252C%2520while%2520maintaining%2520comparable%2520solution%2520quality.%2520The%2520approach%2520is%2520validated%2520in%2520both%25202D%2520and%25203D%2520environments%2520with%2520dynamics%2520such%2520as%2520the%2520unicycle%2520and%25203D%2520double%2520integrator.%2520We%2520demonstrate%2520the%2520safe%2520execution%2520of%2520trajectories%2520planned%2520with%2520db-LaCAM%2520in%2520two%2520distinct%2520physical%2520experiments%2520involving%2520teams%2520of%2520flying%2520robots%2520and%2520car-with-trailer%2520robots.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.06796v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=db-LaCAM%3A%20Fast%20and%20Scalable%20Multi-Robot%20Kinodynamic%20Motion%20Planning%20with%20Discontinuity-Bounded%20Search%20and%20Lightweight%20MAPF&entry.906535625=Akmaral%20Moldagalieva%20and%20Keisuke%20Okumura%20and%20Amanda%20Prorok%20and%20Wolfgang%20H%C3%B6nig&entry.1292438233=State-of-the-art%20multi-robot%20kinodynamic%20motion%20planners%20struggle%20to%20handle%20more%20than%20a%20few%20robots%20due%20to%20high%20computational%20burden%2C%20which%20limits%20their%20scalability%20and%20results%20in%20slow%20planning%20time.%20In%20this%20work%2C%20we%20combine%20the%20scalability%20and%20speed%20of%20modern%20multi-agent%20path%20finding%20%28MAPF%29%20algorithms%20with%20the%20dynamic-awareness%20of%20kinodynamic%20planners%20to%20address%20these%20limitations.%20To%20this%20end%2C%20we%20propose%20discontinuity-Bounded%20LaCAM%20%28db-LaCAM%29%2C%20a%20planner%20that%20utilizes%20a%20precomputed%20set%20of%20motion%20primitives%20that%20respect%20robot%20dynamics%20to%20generate%20horizon-length%20motion%20sequences%2C%20while%20allowing%20a%20user-defined%20discontinuity%20between%20successive%20motions.%20The%20planner%20db-LaCAM%20is%20resolution-complete%20with%20respect%20to%20motion%20primitives%20and%20supports%20arbitrary%20robot%20dynamics.%20Extensive%20experiments%20demonstrate%20that%20db-LaCAM%20scales%20efficiently%20to%20scenarios%20with%20up%20to%2050%20robots%2C%20achieving%20up%20to%20ten%20times%20faster%20runtime%20compared%20to%20state-of-the-art%20planners%2C%20while%20maintaining%20comparable%20solution%20quality.%20The%20approach%20is%20validated%20in%20both%202D%20and%203D%20environments%20with%20dynamics%20such%20as%20the%20unicycle%20and%203D%20double%20integrator.%20We%20demonstrate%20the%20safe%20execution%20of%20trajectories%20planned%20with%20db-LaCAM%20in%20two%20distinct%20physical%20experiments%20involving%20teams%20of%20flying%20robots%20and%20car-with-trailer%20robots.&entry.1838667208=http%3A//arxiv.org/abs/2512.06796v2&entry.124074799=Read"},
{"title": "vEDGAR -- Can CARLA Do HiL?", "author": "Nils Gehrke and David Brecht and Dominik Kulmer and Dheer Patel and Frank Diermeyer", "abstract": "Simulation offers advantages throughout the development process of automated driving functions, both in research and product development. Common open-source simulators like CARLA are extensively used in training, evaluation, and software-in-the-loop testing of new automated driving algorithms. However, the CARLA simulator lacks an evaluation where research and automated driving vehicles are simulated with their entire sensor and actuation stack in real time. The goal of this work is therefore to create a simulation framework for testing the automation software on its dedicated hardware and identifying its limits. Achieving this goal would greatly benefit the open-source development workflow of automated driving functions, designating CARLA as a consistent evaluation tool along the entire development process. To achieve this goal, in a first step, requirements are derived, and a simulation architecture is specified and implemented. Based on the formulated requirements, the proposed vEDGAR software is evaluated, resulting in a final conclusion on the applicability of CARLA for HiL testing of automated vehicles. The tool is available open source: Modified CARLA fork: https://github.com/TUMFTM/carla, vEDGAR Framework: https://github.com/TUMFTM/vEDGAR", "link": "http://arxiv.org/abs/2512.08541v1", "date": "2025-12-09", "relevancy": 1.3449, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4563}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4408}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4359}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20vEDGAR%20--%20Can%20CARLA%20Do%20HiL%3F&body=Title%3A%20vEDGAR%20--%20Can%20CARLA%20Do%20HiL%3F%0AAuthor%3A%20Nils%20Gehrke%20and%20David%20Brecht%20and%20Dominik%20Kulmer%20and%20Dheer%20Patel%20and%20Frank%20Diermeyer%0AAbstract%3A%20Simulation%20offers%20advantages%20throughout%20the%20development%20process%20of%20automated%20driving%20functions%2C%20both%20in%20research%20and%20product%20development.%20Common%20open-source%20simulators%20like%20CARLA%20are%20extensively%20used%20in%20training%2C%20evaluation%2C%20and%20software-in-the-loop%20testing%20of%20new%20automated%20driving%20algorithms.%20However%2C%20the%20CARLA%20simulator%20lacks%20an%20evaluation%20where%20research%20and%20automated%20driving%20vehicles%20are%20simulated%20with%20their%20entire%20sensor%20and%20actuation%20stack%20in%20real%20time.%20The%20goal%20of%20this%20work%20is%20therefore%20to%20create%20a%20simulation%20framework%20for%20testing%20the%20automation%20software%20on%20its%20dedicated%20hardware%20and%20identifying%20its%20limits.%20Achieving%20this%20goal%20would%20greatly%20benefit%20the%20open-source%20development%20workflow%20of%20automated%20driving%20functions%2C%20designating%20CARLA%20as%20a%20consistent%20evaluation%20tool%20along%20the%20entire%20development%20process.%20To%20achieve%20this%20goal%2C%20in%20a%20first%20step%2C%20requirements%20are%20derived%2C%20and%20a%20simulation%20architecture%20is%20specified%20and%20implemented.%20Based%20on%20the%20formulated%20requirements%2C%20the%20proposed%20vEDGAR%20software%20is%20evaluated%2C%20resulting%20in%20a%20final%20conclusion%20on%20the%20applicability%20of%20CARLA%20for%20HiL%20testing%20of%20automated%20vehicles.%20The%20tool%20is%20available%20open%20source%3A%20Modified%20CARLA%20fork%3A%20https%3A//github.com/TUMFTM/carla%2C%20vEDGAR%20Framework%3A%20https%3A//github.com/TUMFTM/vEDGAR%0ALink%3A%20http%3A//arxiv.org/abs/2512.08541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DvEDGAR%2520--%2520Can%2520CARLA%2520Do%2520HiL%253F%26entry.906535625%3DNils%2520Gehrke%2520and%2520David%2520Brecht%2520and%2520Dominik%2520Kulmer%2520and%2520Dheer%2520Patel%2520and%2520Frank%2520Diermeyer%26entry.1292438233%3DSimulation%2520offers%2520advantages%2520throughout%2520the%2520development%2520process%2520of%2520automated%2520driving%2520functions%252C%2520both%2520in%2520research%2520and%2520product%2520development.%2520Common%2520open-source%2520simulators%2520like%2520CARLA%2520are%2520extensively%2520used%2520in%2520training%252C%2520evaluation%252C%2520and%2520software-in-the-loop%2520testing%2520of%2520new%2520automated%2520driving%2520algorithms.%2520However%252C%2520the%2520CARLA%2520simulator%2520lacks%2520an%2520evaluation%2520where%2520research%2520and%2520automated%2520driving%2520vehicles%2520are%2520simulated%2520with%2520their%2520entire%2520sensor%2520and%2520actuation%2520stack%2520in%2520real%2520time.%2520The%2520goal%2520of%2520this%2520work%2520is%2520therefore%2520to%2520create%2520a%2520simulation%2520framework%2520for%2520testing%2520the%2520automation%2520software%2520on%2520its%2520dedicated%2520hardware%2520and%2520identifying%2520its%2520limits.%2520Achieving%2520this%2520goal%2520would%2520greatly%2520benefit%2520the%2520open-source%2520development%2520workflow%2520of%2520automated%2520driving%2520functions%252C%2520designating%2520CARLA%2520as%2520a%2520consistent%2520evaluation%2520tool%2520along%2520the%2520entire%2520development%2520process.%2520To%2520achieve%2520this%2520goal%252C%2520in%2520a%2520first%2520step%252C%2520requirements%2520are%2520derived%252C%2520and%2520a%2520simulation%2520architecture%2520is%2520specified%2520and%2520implemented.%2520Based%2520on%2520the%2520formulated%2520requirements%252C%2520the%2520proposed%2520vEDGAR%2520software%2520is%2520evaluated%252C%2520resulting%2520in%2520a%2520final%2520conclusion%2520on%2520the%2520applicability%2520of%2520CARLA%2520for%2520HiL%2520testing%2520of%2520automated%2520vehicles.%2520The%2520tool%2520is%2520available%2520open%2520source%253A%2520Modified%2520CARLA%2520fork%253A%2520https%253A//github.com/TUMFTM/carla%252C%2520vEDGAR%2520Framework%253A%2520https%253A//github.com/TUMFTM/vEDGAR%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=vEDGAR%20--%20Can%20CARLA%20Do%20HiL%3F&entry.906535625=Nils%20Gehrke%20and%20David%20Brecht%20and%20Dominik%20Kulmer%20and%20Dheer%20Patel%20and%20Frank%20Diermeyer&entry.1292438233=Simulation%20offers%20advantages%20throughout%20the%20development%20process%20of%20automated%20driving%20functions%2C%20both%20in%20research%20and%20product%20development.%20Common%20open-source%20simulators%20like%20CARLA%20are%20extensively%20used%20in%20training%2C%20evaluation%2C%20and%20software-in-the-loop%20testing%20of%20new%20automated%20driving%20algorithms.%20However%2C%20the%20CARLA%20simulator%20lacks%20an%20evaluation%20where%20research%20and%20automated%20driving%20vehicles%20are%20simulated%20with%20their%20entire%20sensor%20and%20actuation%20stack%20in%20real%20time.%20The%20goal%20of%20this%20work%20is%20therefore%20to%20create%20a%20simulation%20framework%20for%20testing%20the%20automation%20software%20on%20its%20dedicated%20hardware%20and%20identifying%20its%20limits.%20Achieving%20this%20goal%20would%20greatly%20benefit%20the%20open-source%20development%20workflow%20of%20automated%20driving%20functions%2C%20designating%20CARLA%20as%20a%20consistent%20evaluation%20tool%20along%20the%20entire%20development%20process.%20To%20achieve%20this%20goal%2C%20in%20a%20first%20step%2C%20requirements%20are%20derived%2C%20and%20a%20simulation%20architecture%20is%20specified%20and%20implemented.%20Based%20on%20the%20formulated%20requirements%2C%20the%20proposed%20vEDGAR%20software%20is%20evaluated%2C%20resulting%20in%20a%20final%20conclusion%20on%20the%20applicability%20of%20CARLA%20for%20HiL%20testing%20of%20automated%20vehicles.%20The%20tool%20is%20available%20open%20source%3A%20Modified%20CARLA%20fork%3A%20https%3A//github.com/TUMFTM/carla%2C%20vEDGAR%20Framework%3A%20https%3A//github.com/TUMFTM/vEDGAR&entry.1838667208=http%3A//arxiv.org/abs/2512.08541v1&entry.124074799=Read"},
{"title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance", "author": "Ruihang Chu and Yefei He and Zhekai Chen and Shiwei Zhang and Xiaogang Xu and Bin Xia and Dingdong Wang and Hongwei Yi and Xihui Liu and Hengshuang Zhao and Yu Liu and Yingya Zhang and Yujiu Yang", "abstract": "We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.", "link": "http://arxiv.org/abs/2512.08765v1", "date": "2025-12-09", "relevancy": 1.9004, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6488}, {"title": "Synthesizing Moving People with 3D Control", "link": "http://arxiv.org/abs/2401.10889v2", "similarity": 0.6294}, {"title": "CamCtrl3D: Single-Image Scene Exploration with Precise 3D Camera Control", "link": "http://arxiv.org/abs/2501.06006v1", "similarity": 0.6283}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Wan-Move%3A%20Motion-controllable%20Video%20Generation%20via%20Latent%20Trajectory%20Guidance&body=Title%3A%20Wan-Move%3A%20Motion-controllable%20Video%20Generation%20via%20Latent%20Trajectory%20Guidance%0AAuthor%3A%20Ruihang%20Chu%20and%20Yefei%20He%20and%20Zhekai%20Chen%20and%20Shiwei%20Zhang%20and%20Xiaogang%20Xu%20and%20Bin%20Xia%20and%20Dingdong%20Wang%20and%20Hongwei%20Yi%20and%20Xihui%20Liu%20and%20Hengshuang%20Zhao%20and%20Yu%20Liu%20and%20Yingya%20Zhang%20and%20Yujiu%20Yang%0AAbstract%3A%20We%20present%20Wan-Move%2C%20a%20simple%20and%20scalable%20framework%20that%20brings%20motion%20control%20to%20video%20generative%20models.%20Existing%20motion-controllable%20methods%20typically%20suffer%20from%20coarse%20control%20granularity%20and%20limited%20scalability%2C%20leaving%20their%20outputs%20insufficient%20for%20practical%20use.%20We%20narrow%20this%20gap%20by%20achieving%20precise%20and%20high-quality%20motion%20control.%20Our%20core%20idea%20is%20to%20directly%20make%20the%20original%20condition%20features%20motion-aware%20for%20guiding%20video%20synthesis.%20To%20this%20end%2C%20we%20first%20represent%20object%20motions%20with%20dense%20point%20trajectories%2C%20allowing%20fine-grained%20control%20over%20the%20scene.%20We%20then%20project%20these%20trajectories%20into%20latent%20space%20and%20propagate%20the%20first%20frame%27s%20features%20along%20each%20trajectory%2C%20producing%20an%20aligned%20spatiotemporal%20feature%20map%20that%20tells%20how%20each%20scene%20element%20should%20move.%20This%20feature%20map%20serves%20as%20the%20updated%20latent%20condition%2C%20which%20is%20naturally%20integrated%20into%20the%20off-the-shelf%20image-to-video%20model%2C%20e.g.%2C%20Wan-I2V-14B%2C%20as%20motion%20guidance%20without%20any%20architecture%20change.%20It%20removes%20the%20need%20for%20auxiliary%20motion%20encoders%20and%20makes%20fine-tuning%20base%20models%20easily%20scalable.%20Through%20scaled%20training%2C%20Wan-Move%20generates%205-second%2C%20480p%20videos%20whose%20motion%20controllability%20rivals%20Kling%201.5%20Pro%27s%20commercial%20Motion%20Brush%2C%20as%20indicated%20by%20user%20studies.%20To%20support%20comprehensive%20evaluation%2C%20we%20further%20design%20MoveBench%2C%20a%20rigorously%20curated%20benchmark%20featuring%20diverse%20content%20categories%20and%20hybrid-verified%20annotations.%20It%20is%20distinguished%20by%20larger%20data%20volume%2C%20longer%20video%20durations%2C%20and%20high-quality%20motion%20annotations.%20Extensive%20experiments%20on%20MoveBench%20and%20the%20public%20dataset%20consistently%20show%20Wan-Move%27s%20superior%20motion%20quality.%20Code%2C%20models%2C%20and%20benchmark%20data%20are%20made%20publicly%20available.%0ALink%3A%20http%3A//arxiv.org/abs/2512.08765v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWan-Move%253A%2520Motion-controllable%2520Video%2520Generation%2520via%2520Latent%2520Trajectory%2520Guidance%26entry.906535625%3DRuihang%2520Chu%2520and%2520Yefei%2520He%2520and%2520Zhekai%2520Chen%2520and%2520Shiwei%2520Zhang%2520and%2520Xiaogang%2520Xu%2520and%2520Bin%2520Xia%2520and%2520Dingdong%2520Wang%2520and%2520Hongwei%2520Yi%2520and%2520Xihui%2520Liu%2520and%2520Hengshuang%2520Zhao%2520and%2520Yu%2520Liu%2520and%2520Yingya%2520Zhang%2520and%2520Yujiu%2520Yang%26entry.1292438233%3DWe%2520present%2520Wan-Move%252C%2520a%2520simple%2520and%2520scalable%2520framework%2520that%2520brings%2520motion%2520control%2520to%2520video%2520generative%2520models.%2520Existing%2520motion-controllable%2520methods%2520typically%2520suffer%2520from%2520coarse%2520control%2520granularity%2520and%2520limited%2520scalability%252C%2520leaving%2520their%2520outputs%2520insufficient%2520for%2520practical%2520use.%2520We%2520narrow%2520this%2520gap%2520by%2520achieving%2520precise%2520and%2520high-quality%2520motion%2520control.%2520Our%2520core%2520idea%2520is%2520to%2520directly%2520make%2520the%2520original%2520condition%2520features%2520motion-aware%2520for%2520guiding%2520video%2520synthesis.%2520To%2520this%2520end%252C%2520we%2520first%2520represent%2520object%2520motions%2520with%2520dense%2520point%2520trajectories%252C%2520allowing%2520fine-grained%2520control%2520over%2520the%2520scene.%2520We%2520then%2520project%2520these%2520trajectories%2520into%2520latent%2520space%2520and%2520propagate%2520the%2520first%2520frame%2527s%2520features%2520along%2520each%2520trajectory%252C%2520producing%2520an%2520aligned%2520spatiotemporal%2520feature%2520map%2520that%2520tells%2520how%2520each%2520scene%2520element%2520should%2520move.%2520This%2520feature%2520map%2520serves%2520as%2520the%2520updated%2520latent%2520condition%252C%2520which%2520is%2520naturally%2520integrated%2520into%2520the%2520off-the-shelf%2520image-to-video%2520model%252C%2520e.g.%252C%2520Wan-I2V-14B%252C%2520as%2520motion%2520guidance%2520without%2520any%2520architecture%2520change.%2520It%2520removes%2520the%2520need%2520for%2520auxiliary%2520motion%2520encoders%2520and%2520makes%2520fine-tuning%2520base%2520models%2520easily%2520scalable.%2520Through%2520scaled%2520training%252C%2520Wan-Move%2520generates%25205-second%252C%2520480p%2520videos%2520whose%2520motion%2520controllability%2520rivals%2520Kling%25201.5%2520Pro%2527s%2520commercial%2520Motion%2520Brush%252C%2520as%2520indicated%2520by%2520user%2520studies.%2520To%2520support%2520comprehensive%2520evaluation%252C%2520we%2520further%2520design%2520MoveBench%252C%2520a%2520rigorously%2520curated%2520benchmark%2520featuring%2520diverse%2520content%2520categories%2520and%2520hybrid-verified%2520annotations.%2520It%2520is%2520distinguished%2520by%2520larger%2520data%2520volume%252C%2520longer%2520video%2520durations%252C%2520and%2520high-quality%2520motion%2520annotations.%2520Extensive%2520experiments%2520on%2520MoveBench%2520and%2520the%2520public%2520dataset%2520consistently%2520show%2520Wan-Move%2527s%2520superior%2520motion%2520quality.%2520Code%252C%2520models%252C%2520and%2520benchmark%2520data%2520are%2520made%2520publicly%2520available.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2512.08765v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Wan-Move%3A%20Motion-controllable%20Video%20Generation%20via%20Latent%20Trajectory%20Guidance&entry.906535625=Ruihang%20Chu%20and%20Yefei%20He%20and%20Zhekai%20Chen%20and%20Shiwei%20Zhang%20and%20Xiaogang%20Xu%20and%20Bin%20Xia%20and%20Dingdong%20Wang%20and%20Hongwei%20Yi%20and%20Xihui%20Liu%20and%20Hengshuang%20Zhao%20and%20Yu%20Liu%20and%20Yingya%20Zhang%20and%20Yujiu%20Yang&entry.1292438233=We%20present%20Wan-Move%2C%20a%20simple%20and%20scalable%20framework%20that%20brings%20motion%20control%20to%20video%20generative%20models.%20Existing%20motion-controllable%20methods%20typically%20suffer%20from%20coarse%20control%20granularity%20and%20limited%20scalability%2C%20leaving%20their%20outputs%20insufficient%20for%20practical%20use.%20We%20narrow%20this%20gap%20by%20achieving%20precise%20and%20high-quality%20motion%20control.%20Our%20core%20idea%20is%20to%20directly%20make%20the%20original%20condition%20features%20motion-aware%20for%20guiding%20video%20synthesis.%20To%20this%20end%2C%20we%20first%20represent%20object%20motions%20with%20dense%20point%20trajectories%2C%20allowing%20fine-grained%20control%20over%20the%20scene.%20We%20then%20project%20these%20trajectories%20into%20latent%20space%20and%20propagate%20the%20first%20frame%27s%20features%20along%20each%20trajectory%2C%20producing%20an%20aligned%20spatiotemporal%20feature%20map%20that%20tells%20how%20each%20scene%20element%20should%20move.%20This%20feature%20map%20serves%20as%20the%20updated%20latent%20condition%2C%20which%20is%20naturally%20integrated%20into%20the%20off-the-shelf%20image-to-video%20model%2C%20e.g.%2C%20Wan-I2V-14B%2C%20as%20motion%20guidance%20without%20any%20architecture%20change.%20It%20removes%20the%20need%20for%20auxiliary%20motion%20encoders%20and%20makes%20fine-tuning%20base%20models%20easily%20scalable.%20Through%20scaled%20training%2C%20Wan-Move%20generates%205-second%2C%20480p%20videos%20whose%20motion%20controllability%20rivals%20Kling%201.5%20Pro%27s%20commercial%20Motion%20Brush%2C%20as%20indicated%20by%20user%20studies.%20To%20support%20comprehensive%20evaluation%2C%20we%20further%20design%20MoveBench%2C%20a%20rigorously%20curated%20benchmark%20featuring%20diverse%20content%20categories%20and%20hybrid-verified%20annotations.%20It%20is%20distinguished%20by%20larger%20data%20volume%2C%20longer%20video%20durations%2C%20and%20high-quality%20motion%20annotations.%20Extensive%20experiments%20on%20MoveBench%20and%20the%20public%20dataset%20consistently%20show%20Wan-Move%27s%20superior%20motion%20quality.%20Code%2C%20models%2C%20and%20benchmark%20data%20are%20made%20publicly%20available.&entry.1838667208=http%3A//arxiv.org/abs/2512.08765v1&entry.124074799=Read"},
{"title": "VoiceCloak: A Multi-Dimensional Defense Framework against Unauthorized Diffusion-based Voice Cloning", "author": "Qianyue Hu and Junyan Wu and Wei Lu and Xiangyang Luo", "abstract": "Diffusion Models (DMs) have achieved remarkable success in realistic voice cloning (VC), while they also increase the risk of malicious misuse. Existing proactive defenses designed for traditional VC models aim to disrupt the forgery process, but they have been proven incompatible with DMs due to the intricate generative mechanisms of diffusion. To bridge this gap, we introduce VoiceCloak, a multi-dimensional proactive defense framework with the goal of obfuscating speaker identity and degrading perceptual quality in potential unauthorized VC. To achieve these goals, we conduct a focused analysis to identify specific vulnerabilities within DMs, allowing VoiceCloak to disrupt the cloning process by introducing adversarial perturbations into the reference audio. Specifically, to obfuscate speaker identity, VoiceCloak first targets speaker identity by distorting representation learning embeddings to maximize identity variation, which is guided by auditory perception principles. Additionally, VoiceCloak disrupts crucial conditional guidance processes, particularly attention context, thereby preventing the alignment of vocal characteristics that are essential for achieving convincing cloning. Then, to address the second objective, VoiceCloak introduces score magnitude amplification to actively steer the reverse trajectory away from the generation of high-quality speech. Noise-guided semantic corruption is further employed to disrupt structural speech semantics captured by DMs, degrading output quality. Extensive experiments highlight VoiceCloak's outstanding defense success rate against unauthorized diffusion-based voice cloning. Audio samples of VoiceCloak are available at https://voice-cloak.github.io/VoiceCloak/.", "link": "http://arxiv.org/abs/2505.12332v5", "date": "2025-12-09", "relevancy": 2.0798, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5695}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.4956}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4802}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VoiceCloak%3A%20A%20Multi-Dimensional%20Defense%20Framework%20against%20Unauthorized%20Diffusion-based%20Voice%20Cloning&body=Title%3A%20VoiceCloak%3A%20A%20Multi-Dimensional%20Defense%20Framework%20against%20Unauthorized%20Diffusion-based%20Voice%20Cloning%0AAuthor%3A%20Qianyue%20Hu%20and%20Junyan%20Wu%20and%20Wei%20Lu%20and%20Xiangyang%20Luo%0AAbstract%3A%20Diffusion%20Models%20%28DMs%29%20have%20achieved%20remarkable%20success%20in%20realistic%20voice%20cloning%20%28VC%29%2C%20while%20they%20also%20increase%20the%20risk%20of%20malicious%20misuse.%20Existing%20proactive%20defenses%20designed%20for%20traditional%20VC%20models%20aim%20to%20disrupt%20the%20forgery%20process%2C%20but%20they%20have%20been%20proven%20incompatible%20with%20DMs%20due%20to%20the%20intricate%20generative%20mechanisms%20of%20diffusion.%20To%20bridge%20this%20gap%2C%20we%20introduce%20VoiceCloak%2C%20a%20multi-dimensional%20proactive%20defense%20framework%20with%20the%20goal%20of%20obfuscating%20speaker%20identity%20and%20degrading%20perceptual%20quality%20in%20potential%20unauthorized%20VC.%20To%20achieve%20these%20goals%2C%20we%20conduct%20a%20focused%20analysis%20to%20identify%20specific%20vulnerabilities%20within%20DMs%2C%20allowing%20VoiceCloak%20to%20disrupt%20the%20cloning%20process%20by%20introducing%20adversarial%20perturbations%20into%20the%20reference%20audio.%20Specifically%2C%20to%20obfuscate%20speaker%20identity%2C%20VoiceCloak%20first%20targets%20speaker%20identity%20by%20distorting%20representation%20learning%20embeddings%20to%20maximize%20identity%20variation%2C%20which%20is%20guided%20by%20auditory%20perception%20principles.%20Additionally%2C%20VoiceCloak%20disrupts%20crucial%20conditional%20guidance%20processes%2C%20particularly%20attention%20context%2C%20thereby%20preventing%20the%20alignment%20of%20vocal%20characteristics%20that%20are%20essential%20for%20achieving%20convincing%20cloning.%20Then%2C%20to%20address%20the%20second%20objective%2C%20VoiceCloak%20introduces%20score%20magnitude%20amplification%20to%20actively%20steer%20the%20reverse%20trajectory%20away%20from%20the%20generation%20of%20high-quality%20speech.%20Noise-guided%20semantic%20corruption%20is%20further%20employed%20to%20disrupt%20structural%20speech%20semantics%20captured%20by%20DMs%2C%20degrading%20output%20quality.%20Extensive%20experiments%20highlight%20VoiceCloak%27s%20outstanding%20defense%20success%20rate%20against%20unauthorized%20diffusion-based%20voice%20cloning.%20Audio%20samples%20of%20VoiceCloak%20are%20available%20at%20https%3A//voice-cloak.github.io/VoiceCloak/.%0ALink%3A%20http%3A//arxiv.org/abs/2505.12332v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVoiceCloak%253A%2520A%2520Multi-Dimensional%2520Defense%2520Framework%2520against%2520Unauthorized%2520Diffusion-based%2520Voice%2520Cloning%26entry.906535625%3DQianyue%2520Hu%2520and%2520Junyan%2520Wu%2520and%2520Wei%2520Lu%2520and%2520Xiangyang%2520Luo%26entry.1292438233%3DDiffusion%2520Models%2520%2528DMs%2529%2520have%2520achieved%2520remarkable%2520success%2520in%2520realistic%2520voice%2520cloning%2520%2528VC%2529%252C%2520while%2520they%2520also%2520increase%2520the%2520risk%2520of%2520malicious%2520misuse.%2520Existing%2520proactive%2520defenses%2520designed%2520for%2520traditional%2520VC%2520models%2520aim%2520to%2520disrupt%2520the%2520forgery%2520process%252C%2520but%2520they%2520have%2520been%2520proven%2520incompatible%2520with%2520DMs%2520due%2520to%2520the%2520intricate%2520generative%2520mechanisms%2520of%2520diffusion.%2520To%2520bridge%2520this%2520gap%252C%2520we%2520introduce%2520VoiceCloak%252C%2520a%2520multi-dimensional%2520proactive%2520defense%2520framework%2520with%2520the%2520goal%2520of%2520obfuscating%2520speaker%2520identity%2520and%2520degrading%2520perceptual%2520quality%2520in%2520potential%2520unauthorized%2520VC.%2520To%2520achieve%2520these%2520goals%252C%2520we%2520conduct%2520a%2520focused%2520analysis%2520to%2520identify%2520specific%2520vulnerabilities%2520within%2520DMs%252C%2520allowing%2520VoiceCloak%2520to%2520disrupt%2520the%2520cloning%2520process%2520by%2520introducing%2520adversarial%2520perturbations%2520into%2520the%2520reference%2520audio.%2520Specifically%252C%2520to%2520obfuscate%2520speaker%2520identity%252C%2520VoiceCloak%2520first%2520targets%2520speaker%2520identity%2520by%2520distorting%2520representation%2520learning%2520embeddings%2520to%2520maximize%2520identity%2520variation%252C%2520which%2520is%2520guided%2520by%2520auditory%2520perception%2520principles.%2520Additionally%252C%2520VoiceCloak%2520disrupts%2520crucial%2520conditional%2520guidance%2520processes%252C%2520particularly%2520attention%2520context%252C%2520thereby%2520preventing%2520the%2520alignment%2520of%2520vocal%2520characteristics%2520that%2520are%2520essential%2520for%2520achieving%2520convincing%2520cloning.%2520Then%252C%2520to%2520address%2520the%2520second%2520objective%252C%2520VoiceCloak%2520introduces%2520score%2520magnitude%2520amplification%2520to%2520actively%2520steer%2520the%2520reverse%2520trajectory%2520away%2520from%2520the%2520generation%2520of%2520high-quality%2520speech.%2520Noise-guided%2520semantic%2520corruption%2520is%2520further%2520employed%2520to%2520disrupt%2520structural%2520speech%2520semantics%2520captured%2520by%2520DMs%252C%2520degrading%2520output%2520quality.%2520Extensive%2520experiments%2520highlight%2520VoiceCloak%2527s%2520outstanding%2520defense%2520success%2520rate%2520against%2520unauthorized%2520diffusion-based%2520voice%2520cloning.%2520Audio%2520samples%2520of%2520VoiceCloak%2520are%2520available%2520at%2520https%253A//voice-cloak.github.io/VoiceCloak/.%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2505.12332v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VoiceCloak%3A%20A%20Multi-Dimensional%20Defense%20Framework%20against%20Unauthorized%20Diffusion-based%20Voice%20Cloning&entry.906535625=Qianyue%20Hu%20and%20Junyan%20Wu%20and%20Wei%20Lu%20and%20Xiangyang%20Luo&entry.1292438233=Diffusion%20Models%20%28DMs%29%20have%20achieved%20remarkable%20success%20in%20realistic%20voice%20cloning%20%28VC%29%2C%20while%20they%20also%20increase%20the%20risk%20of%20malicious%20misuse.%20Existing%20proactive%20defenses%20designed%20for%20traditional%20VC%20models%20aim%20to%20disrupt%20the%20forgery%20process%2C%20but%20they%20have%20been%20proven%20incompatible%20with%20DMs%20due%20to%20the%20intricate%20generative%20mechanisms%20of%20diffusion.%20To%20bridge%20this%20gap%2C%20we%20introduce%20VoiceCloak%2C%20a%20multi-dimensional%20proactive%20defense%20framework%20with%20the%20goal%20of%20obfuscating%20speaker%20identity%20and%20degrading%20perceptual%20quality%20in%20potential%20unauthorized%20VC.%20To%20achieve%20these%20goals%2C%20we%20conduct%20a%20focused%20analysis%20to%20identify%20specific%20vulnerabilities%20within%20DMs%2C%20allowing%20VoiceCloak%20to%20disrupt%20the%20cloning%20process%20by%20introducing%20adversarial%20perturbations%20into%20the%20reference%20audio.%20Specifically%2C%20to%20obfuscate%20speaker%20identity%2C%20VoiceCloak%20first%20targets%20speaker%20identity%20by%20distorting%20representation%20learning%20embeddings%20to%20maximize%20identity%20variation%2C%20which%20is%20guided%20by%20auditory%20perception%20principles.%20Additionally%2C%20VoiceCloak%20disrupts%20crucial%20conditional%20guidance%20processes%2C%20particularly%20attention%20context%2C%20thereby%20preventing%20the%20alignment%20of%20vocal%20characteristics%20that%20are%20essential%20for%20achieving%20convincing%20cloning.%20Then%2C%20to%20address%20the%20second%20objective%2C%20VoiceCloak%20introduces%20score%20magnitude%20amplification%20to%20actively%20steer%20the%20reverse%20trajectory%20away%20from%20the%20generation%20of%20high-quality%20speech.%20Noise-guided%20semantic%20corruption%20is%20further%20employed%20to%20disrupt%20structural%20speech%20semantics%20captured%20by%20DMs%2C%20degrading%20output%20quality.%20Extensive%20experiments%20highlight%20VoiceCloak%27s%20outstanding%20defense%20success%20rate%20against%20unauthorized%20diffusion-based%20voice%20cloning.%20Audio%20samples%20of%20VoiceCloak%20are%20available%20at%20https%3A//voice-cloak.github.io/VoiceCloak/.&entry.1838667208=http%3A//arxiv.org/abs/2505.12332v5&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


