<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    </style>
  </head>
  <body>

    <header>
      <a id="prev-btn" href="daily20240814.html"><i class="fas fa-chevron-left"></i></a>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "3D Gaussian Editing with A Single Image", "author": "Guan Luo and Tian-Xing Xu and Ying-Tian Liu and Xiao-Xiong Fan and Fang-Lue Zhang and Song-Hai Zhang", "abstract": "  The modeling and manipulation of 3D scenes captured from the real world are\npivotal in various applications, attracting growing research interest. While\nprevious works on editing have achieved interesting results through\nmanipulating 3D meshes, they often require accurately reconstructed meshes to\nperform editing, which limits their application in 3D content generation. To\naddress this gap, we introduce a novel single-image-driven 3D scene editing\napproach based on 3D Gaussian Splatting, enabling intuitive manipulation via\ndirectly editing the content on a 2D image plane. Our method learns to optimize\nthe 3D Gaussians to align with an edited version of the image rendered from a\nuser-specified viewpoint of the original scene. To capture long-range object\ndeformation, we introduce positional loss into the optimization process of 3D\nGaussian Splatting and enable gradient propagation through reparameterization.\nTo handle occluded 3D Gaussians when rendering from the specified viewpoint, we\nbuild an anchor-based structure and employ a coarse-to-fine optimization\nstrategy capable of handling long-range deformation while maintaining\nstructural stability. Furthermore, we design a novel masking strategy to\nadaptively identify non-rigid deformation regions for fine-scale modeling.\nExtensive experiments show the effectiveness of our method in handling\ngeometric details, long-range, and non-rigid deformation, demonstrating\nsuperior editing flexibility and quality compared to previous approaches.\n", "link": "http://arxiv.org/abs/2408.07540v1", "date": "2024-08-14", "relevancy": 3.2849, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6724}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.67}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.6285}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%203D%20Gaussian%20Editing%20with%20A%20Single%20Image&body=Title%3A%203D%20Gaussian%20Editing%20with%20A%20Single%20Image%0AAuthor%3A%20Guan%20Luo%20and%20Tian-Xing%20Xu%20and%20Ying-Tian%20Liu%20and%20Xiao-Xiong%20Fan%20and%20Fang-Lue%20Zhang%20and%20Song-Hai%20Zhang%0AAbstract%3A%20%20%20The%20modeling%20and%20manipulation%20of%203D%20scenes%20captured%20from%20the%20real%20world%20are%0Apivotal%20in%20various%20applications%2C%20attracting%20growing%20research%20interest.%20While%0Aprevious%20works%20on%20editing%20have%20achieved%20interesting%20results%20through%0Amanipulating%203D%20meshes%2C%20they%20often%20require%20accurately%20reconstructed%20meshes%20to%0Aperform%20editing%2C%20which%20limits%20their%20application%20in%203D%20content%20generation.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20a%20novel%20single-image-driven%203D%20scene%20editing%0Aapproach%20based%20on%203D%20Gaussian%20Splatting%2C%20enabling%20intuitive%20manipulation%20via%0Adirectly%20editing%20the%20content%20on%20a%202D%20image%20plane.%20Our%20method%20learns%20to%20optimize%0Athe%203D%20Gaussians%20to%20align%20with%20an%20edited%20version%20of%20the%20image%20rendered%20from%20a%0Auser-specified%20viewpoint%20of%20the%20original%20scene.%20To%20capture%20long-range%20object%0Adeformation%2C%20we%20introduce%20positional%20loss%20into%20the%20optimization%20process%20of%203D%0AGaussian%20Splatting%20and%20enable%20gradient%20propagation%20through%20reparameterization.%0ATo%20handle%20occluded%203D%20Gaussians%20when%20rendering%20from%20the%20specified%20viewpoint%2C%20we%0Abuild%20an%20anchor-based%20structure%20and%20employ%20a%20coarse-to-fine%20optimization%0Astrategy%20capable%20of%20handling%20long-range%20deformation%20while%20maintaining%0Astructural%20stability.%20Furthermore%2C%20we%20design%20a%20novel%20masking%20strategy%20to%0Aadaptively%20identify%20non-rigid%20deformation%20regions%20for%20fine-scale%20modeling.%0AExtensive%20experiments%20show%20the%20effectiveness%20of%20our%20method%20in%20handling%0Ageometric%20details%2C%20long-range%2C%20and%20non-rigid%20deformation%2C%20demonstrating%0Asuperior%20editing%20flexibility%20and%20quality%20compared%20to%20previous%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07540v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3D3D%2520Gaussian%2520Editing%2520with%2520A%2520Single%2520Image%26entry.906535625%3DGuan%2520Luo%2520and%2520Tian-Xing%2520Xu%2520and%2520Ying-Tian%2520Liu%2520and%2520Xiao-Xiong%2520Fan%2520and%2520Fang-Lue%2520Zhang%2520and%2520Song-Hai%2520Zhang%26entry.1292438233%3D%2520%2520The%2520modeling%2520and%2520manipulation%2520of%25203D%2520scenes%2520captured%2520from%2520the%2520real%2520world%2520are%250Apivotal%2520in%2520various%2520applications%252C%2520attracting%2520growing%2520research%2520interest.%2520While%250Aprevious%2520works%2520on%2520editing%2520have%2520achieved%2520interesting%2520results%2520through%250Amanipulating%25203D%2520meshes%252C%2520they%2520often%2520require%2520accurately%2520reconstructed%2520meshes%2520to%250Aperform%2520editing%252C%2520which%2520limits%2520their%2520application%2520in%25203D%2520content%2520generation.%2520To%250Aaddress%2520this%2520gap%252C%2520we%2520introduce%2520a%2520novel%2520single-image-driven%25203D%2520scene%2520editing%250Aapproach%2520based%2520on%25203D%2520Gaussian%2520Splatting%252C%2520enabling%2520intuitive%2520manipulation%2520via%250Adirectly%2520editing%2520the%2520content%2520on%2520a%25202D%2520image%2520plane.%2520Our%2520method%2520learns%2520to%2520optimize%250Athe%25203D%2520Gaussians%2520to%2520align%2520with%2520an%2520edited%2520version%2520of%2520the%2520image%2520rendered%2520from%2520a%250Auser-specified%2520viewpoint%2520of%2520the%2520original%2520scene.%2520To%2520capture%2520long-range%2520object%250Adeformation%252C%2520we%2520introduce%2520positional%2520loss%2520into%2520the%2520optimization%2520process%2520of%25203D%250AGaussian%2520Splatting%2520and%2520enable%2520gradient%2520propagation%2520through%2520reparameterization.%250ATo%2520handle%2520occluded%25203D%2520Gaussians%2520when%2520rendering%2520from%2520the%2520specified%2520viewpoint%252C%2520we%250Abuild%2520an%2520anchor-based%2520structure%2520and%2520employ%2520a%2520coarse-to-fine%2520optimization%250Astrategy%2520capable%2520of%2520handling%2520long-range%2520deformation%2520while%2520maintaining%250Astructural%2520stability.%2520Furthermore%252C%2520we%2520design%2520a%2520novel%2520masking%2520strategy%2520to%250Aadaptively%2520identify%2520non-rigid%2520deformation%2520regions%2520for%2520fine-scale%2520modeling.%250AExtensive%2520experiments%2520show%2520the%2520effectiveness%2520of%2520our%2520method%2520in%2520handling%250Ageometric%2520details%252C%2520long-range%252C%2520and%2520non-rigid%2520deformation%252C%2520demonstrating%250Asuperior%2520editing%2520flexibility%2520and%2520quality%2520compared%2520to%2520previous%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07540v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=3D%20Gaussian%20Editing%20with%20A%20Single%20Image&entry.906535625=Guan%20Luo%20and%20Tian-Xing%20Xu%20and%20Ying-Tian%20Liu%20and%20Xiao-Xiong%20Fan%20and%20Fang-Lue%20Zhang%20and%20Song-Hai%20Zhang&entry.1292438233=%20%20The%20modeling%20and%20manipulation%20of%203D%20scenes%20captured%20from%20the%20real%20world%20are%0Apivotal%20in%20various%20applications%2C%20attracting%20growing%20research%20interest.%20While%0Aprevious%20works%20on%20editing%20have%20achieved%20interesting%20results%20through%0Amanipulating%203D%20meshes%2C%20they%20often%20require%20accurately%20reconstructed%20meshes%20to%0Aperform%20editing%2C%20which%20limits%20their%20application%20in%203D%20content%20generation.%20To%0Aaddress%20this%20gap%2C%20we%20introduce%20a%20novel%20single-image-driven%203D%20scene%20editing%0Aapproach%20based%20on%203D%20Gaussian%20Splatting%2C%20enabling%20intuitive%20manipulation%20via%0Adirectly%20editing%20the%20content%20on%20a%202D%20image%20plane.%20Our%20method%20learns%20to%20optimize%0Athe%203D%20Gaussians%20to%20align%20with%20an%20edited%20version%20of%20the%20image%20rendered%20from%20a%0Auser-specified%20viewpoint%20of%20the%20original%20scene.%20To%20capture%20long-range%20object%0Adeformation%2C%20we%20introduce%20positional%20loss%20into%20the%20optimization%20process%20of%203D%0AGaussian%20Splatting%20and%20enable%20gradient%20propagation%20through%20reparameterization.%0ATo%20handle%20occluded%203D%20Gaussians%20when%20rendering%20from%20the%20specified%20viewpoint%2C%20we%0Abuild%20an%20anchor-based%20structure%20and%20employ%20a%20coarse-to-fine%20optimization%0Astrategy%20capable%20of%20handling%20long-range%20deformation%20while%20maintaining%0Astructural%20stability.%20Furthermore%2C%20we%20design%20a%20novel%20masking%20strategy%20to%0Aadaptively%20identify%20non-rigid%20deformation%20regions%20for%20fine-scale%20modeling.%0AExtensive%20experiments%20show%20the%20effectiveness%20of%20our%20method%20in%20handling%0Ageometric%20details%2C%20long-range%2C%20and%20non-rigid%20deformation%2C%20demonstrating%0Asuperior%20editing%20flexibility%20and%20quality%20compared%20to%20previous%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07540v1&entry.124074799=Read"},
{"title": "GS-Pose: Generalizable Segmentation-based 6D Object Pose Estimation with\n  3D Gaussian Splatting", "author": "Dingding Cai and Janne Heikkil\u00e4 and Esa Rahtu", "abstract": "  This paper introduces GS-Pose, a unified framework for localizing and\nestimating the 6D pose of novel objects. GS-Pose begins with a set of posed RGB\nimages of a previously unseen object and builds three distinct representations\nstored in a database. At inference, GS-Pose operates sequentially by locating\nthe object in the input image, estimating its initial 6D pose using a retrieval\napproach, and refining the pose with a render-and-compare method. The key\ninsight is the application of the appropriate object representation at each\nstage of the process. In particular, for the refinement step, we leverage 3D\nGaussian splatting, a novel differentiable rendering technique that offers high\nrendering speed and relatively low optimization time. Off-the-shelf toolchains\nand commodity hardware, such as mobile phones, can be used to capture new\nobjects to be added to the database. Extensive evaluations on the LINEMOD and\nOnePose-LowTexture datasets demonstrate excellent performance, establishing the\nnew state-of-the-art. Project page: https://dingdingcai.github.io/gs-pose.\n", "link": "http://arxiv.org/abs/2403.10683v2", "date": "2024-08-14", "relevancy": 3.183, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6743}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6269}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6087}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GS-Pose%3A%20Generalizable%20Segmentation-based%206D%20Object%20Pose%20Estimation%20with%0A%20%203D%20Gaussian%20Splatting&body=Title%3A%20GS-Pose%3A%20Generalizable%20Segmentation-based%206D%20Object%20Pose%20Estimation%20with%0A%20%203D%20Gaussian%20Splatting%0AAuthor%3A%20Dingding%20Cai%20and%20Janne%20Heikkil%C3%A4%20and%20Esa%20Rahtu%0AAbstract%3A%20%20%20This%20paper%20introduces%20GS-Pose%2C%20a%20unified%20framework%20for%20localizing%20and%0Aestimating%20the%206D%20pose%20of%20novel%20objects.%20GS-Pose%20begins%20with%20a%20set%20of%20posed%20RGB%0Aimages%20of%20a%20previously%20unseen%20object%20and%20builds%20three%20distinct%20representations%0Astored%20in%20a%20database.%20At%20inference%2C%20GS-Pose%20operates%20sequentially%20by%20locating%0Athe%20object%20in%20the%20input%20image%2C%20estimating%20its%20initial%206D%20pose%20using%20a%20retrieval%0Aapproach%2C%20and%20refining%20the%20pose%20with%20a%20render-and-compare%20method.%20The%20key%0Ainsight%20is%20the%20application%20of%20the%20appropriate%20object%20representation%20at%20each%0Astage%20of%20the%20process.%20In%20particular%2C%20for%20the%20refinement%20step%2C%20we%20leverage%203D%0AGaussian%20splatting%2C%20a%20novel%20differentiable%20rendering%20technique%20that%20offers%20high%0Arendering%20speed%20and%20relatively%20low%20optimization%20time.%20Off-the-shelf%20toolchains%0Aand%20commodity%20hardware%2C%20such%20as%20mobile%20phones%2C%20can%20be%20used%20to%20capture%20new%0Aobjects%20to%20be%20added%20to%20the%20database.%20Extensive%20evaluations%20on%20the%20LINEMOD%20and%0AOnePose-LowTexture%20datasets%20demonstrate%20excellent%20performance%2C%20establishing%20the%0Anew%20state-of-the-art.%20Project%20page%3A%20https%3A//dingdingcai.github.io/gs-pose.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10683v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGS-Pose%253A%2520Generalizable%2520Segmentation-based%25206D%2520Object%2520Pose%2520Estimation%2520with%250A%2520%25203D%2520Gaussian%2520Splatting%26entry.906535625%3DDingding%2520Cai%2520and%2520Janne%2520Heikkil%25C3%25A4%2520and%2520Esa%2520Rahtu%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520GS-Pose%252C%2520a%2520unified%2520framework%2520for%2520localizing%2520and%250Aestimating%2520the%25206D%2520pose%2520of%2520novel%2520objects.%2520GS-Pose%2520begins%2520with%2520a%2520set%2520of%2520posed%2520RGB%250Aimages%2520of%2520a%2520previously%2520unseen%2520object%2520and%2520builds%2520three%2520distinct%2520representations%250Astored%2520in%2520a%2520database.%2520At%2520inference%252C%2520GS-Pose%2520operates%2520sequentially%2520by%2520locating%250Athe%2520object%2520in%2520the%2520input%2520image%252C%2520estimating%2520its%2520initial%25206D%2520pose%2520using%2520a%2520retrieval%250Aapproach%252C%2520and%2520refining%2520the%2520pose%2520with%2520a%2520render-and-compare%2520method.%2520The%2520key%250Ainsight%2520is%2520the%2520application%2520of%2520the%2520appropriate%2520object%2520representation%2520at%2520each%250Astage%2520of%2520the%2520process.%2520In%2520particular%252C%2520for%2520the%2520refinement%2520step%252C%2520we%2520leverage%25203D%250AGaussian%2520splatting%252C%2520a%2520novel%2520differentiable%2520rendering%2520technique%2520that%2520offers%2520high%250Arendering%2520speed%2520and%2520relatively%2520low%2520optimization%2520time.%2520Off-the-shelf%2520toolchains%250Aand%2520commodity%2520hardware%252C%2520such%2520as%2520mobile%2520phones%252C%2520can%2520be%2520used%2520to%2520capture%2520new%250Aobjects%2520to%2520be%2520added%2520to%2520the%2520database.%2520Extensive%2520evaluations%2520on%2520the%2520LINEMOD%2520and%250AOnePose-LowTexture%2520datasets%2520demonstrate%2520excellent%2520performance%252C%2520establishing%2520the%250Anew%2520state-of-the-art.%2520Project%2520page%253A%2520https%253A//dingdingcai.github.io/gs-pose.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.10683v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GS-Pose%3A%20Generalizable%20Segmentation-based%206D%20Object%20Pose%20Estimation%20with%0A%20%203D%20Gaussian%20Splatting&entry.906535625=Dingding%20Cai%20and%20Janne%20Heikkil%C3%A4%20and%20Esa%20Rahtu&entry.1292438233=%20%20This%20paper%20introduces%20GS-Pose%2C%20a%20unified%20framework%20for%20localizing%20and%0Aestimating%20the%206D%20pose%20of%20novel%20objects.%20GS-Pose%20begins%20with%20a%20set%20of%20posed%20RGB%0Aimages%20of%20a%20previously%20unseen%20object%20and%20builds%20three%20distinct%20representations%0Astored%20in%20a%20database.%20At%20inference%2C%20GS-Pose%20operates%20sequentially%20by%20locating%0Athe%20object%20in%20the%20input%20image%2C%20estimating%20its%20initial%206D%20pose%20using%20a%20retrieval%0Aapproach%2C%20and%20refining%20the%20pose%20with%20a%20render-and-compare%20method.%20The%20key%0Ainsight%20is%20the%20application%20of%20the%20appropriate%20object%20representation%20at%20each%0Astage%20of%20the%20process.%20In%20particular%2C%20for%20the%20refinement%20step%2C%20we%20leverage%203D%0AGaussian%20splatting%2C%20a%20novel%20differentiable%20rendering%20technique%20that%20offers%20high%0Arendering%20speed%20and%20relatively%20low%20optimization%20time.%20Off-the-shelf%20toolchains%0Aand%20commodity%20hardware%2C%20such%20as%20mobile%20phones%2C%20can%20be%20used%20to%20capture%20new%0Aobjects%20to%20be%20added%20to%20the%20database.%20Extensive%20evaluations%20on%20the%20LINEMOD%20and%0AOnePose-LowTexture%20datasets%20demonstrate%20excellent%20performance%2C%20establishing%20the%0Anew%20state-of-the-art.%20Project%20page%3A%20https%3A//dingdingcai.github.io/gs-pose.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10683v2&entry.124074799=Read"},
{"title": "R2Human: Real-Time 3D Human Appearance Rendering from a Single Image", "author": "Yuanwang Yang and Qiao Feng and Yu-Kun Lai and Kun Li", "abstract": "  Rendering 3D human appearance from a single image in real-time is crucial for\nachieving holographic communication and immersive VR/AR. Existing methods\neither rely on multi-camera setups or are constrained to offline operations. In\nthis paper, we propose R2Human, the first approach for real-time inference and\nrendering of photorealistic 3D human appearance from a single image. The core\nof our approach is to combine the strengths of implicit texture fields and\nexplicit neural rendering with our novel representation, namely Z-map. Based on\nthis, we present an end-to-end network that performs high-fidelity color\nreconstruction of visible areas and provides reliable color inference for\noccluded regions. To further enhance the 3D perception ability of our network,\nwe leverage the Fourier occupancy field as a prior for generating the texture\nfield and providing a sampling surface in the rendering stage. We also propose\na consistency loss and a spatial fusion strategy to ensure the multi-view\ncoherence. Experimental results show that our method outperforms the\nstate-of-the-art methods on both synthetic data and challenging real-world\nimages, in real-time. The project page can be found at\nhttp://cic.tju.edu.cn/faculty/likun/projects/R2Human.\n", "link": "http://arxiv.org/abs/2312.05826v4", "date": "2024-08-14", "relevancy": 3.0568, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6224}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6058}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20R2Human%3A%20Real-Time%203D%20Human%20Appearance%20Rendering%20from%20a%20Single%20Image&body=Title%3A%20R2Human%3A%20Real-Time%203D%20Human%20Appearance%20Rendering%20from%20a%20Single%20Image%0AAuthor%3A%20Yuanwang%20Yang%20and%20Qiao%20Feng%20and%20Yu-Kun%20Lai%20and%20Kun%20Li%0AAbstract%3A%20%20%20Rendering%203D%20human%20appearance%20from%20a%20single%20image%20in%20real-time%20is%20crucial%20for%0Aachieving%20holographic%20communication%20and%20immersive%20VR/AR.%20Existing%20methods%0Aeither%20rely%20on%20multi-camera%20setups%20or%20are%20constrained%20to%20offline%20operations.%20In%0Athis%20paper%2C%20we%20propose%20R2Human%2C%20the%20first%20approach%20for%20real-time%20inference%20and%0Arendering%20of%20photorealistic%203D%20human%20appearance%20from%20a%20single%20image.%20The%20core%0Aof%20our%20approach%20is%20to%20combine%20the%20strengths%20of%20implicit%20texture%20fields%20and%0Aexplicit%20neural%20rendering%20with%20our%20novel%20representation%2C%20namely%20Z-map.%20Based%20on%0Athis%2C%20we%20present%20an%20end-to-end%20network%20that%20performs%20high-fidelity%20color%0Areconstruction%20of%20visible%20areas%20and%20provides%20reliable%20color%20inference%20for%0Aoccluded%20regions.%20To%20further%20enhance%20the%203D%20perception%20ability%20of%20our%20network%2C%0Awe%20leverage%20the%20Fourier%20occupancy%20field%20as%20a%20prior%20for%20generating%20the%20texture%0Afield%20and%20providing%20a%20sampling%20surface%20in%20the%20rendering%20stage.%20We%20also%20propose%0Aa%20consistency%20loss%20and%20a%20spatial%20fusion%20strategy%20to%20ensure%20the%20multi-view%0Acoherence.%20Experimental%20results%20show%20that%20our%20method%20outperforms%20the%0Astate-of-the-art%20methods%20on%20both%20synthetic%20data%20and%20challenging%20real-world%0Aimages%2C%20in%20real-time.%20The%20project%20page%20can%20be%20found%20at%0Ahttp%3A//cic.tju.edu.cn/faculty/likun/projects/R2Human.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.05826v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DR2Human%253A%2520Real-Time%25203D%2520Human%2520Appearance%2520Rendering%2520from%2520a%2520Single%2520Image%26entry.906535625%3DYuanwang%2520Yang%2520and%2520Qiao%2520Feng%2520and%2520Yu-Kun%2520Lai%2520and%2520Kun%2520Li%26entry.1292438233%3D%2520%2520Rendering%25203D%2520human%2520appearance%2520from%2520a%2520single%2520image%2520in%2520real-time%2520is%2520crucial%2520for%250Aachieving%2520holographic%2520communication%2520and%2520immersive%2520VR/AR.%2520Existing%2520methods%250Aeither%2520rely%2520on%2520multi-camera%2520setups%2520or%2520are%2520constrained%2520to%2520offline%2520operations.%2520In%250Athis%2520paper%252C%2520we%2520propose%2520R2Human%252C%2520the%2520first%2520approach%2520for%2520real-time%2520inference%2520and%250Arendering%2520of%2520photorealistic%25203D%2520human%2520appearance%2520from%2520a%2520single%2520image.%2520The%2520core%250Aof%2520our%2520approach%2520is%2520to%2520combine%2520the%2520strengths%2520of%2520implicit%2520texture%2520fields%2520and%250Aexplicit%2520neural%2520rendering%2520with%2520our%2520novel%2520representation%252C%2520namely%2520Z-map.%2520Based%2520on%250Athis%252C%2520we%2520present%2520an%2520end-to-end%2520network%2520that%2520performs%2520high-fidelity%2520color%250Areconstruction%2520of%2520visible%2520areas%2520and%2520provides%2520reliable%2520color%2520inference%2520for%250Aoccluded%2520regions.%2520To%2520further%2520enhance%2520the%25203D%2520perception%2520ability%2520of%2520our%2520network%252C%250Awe%2520leverage%2520the%2520Fourier%2520occupancy%2520field%2520as%2520a%2520prior%2520for%2520generating%2520the%2520texture%250Afield%2520and%2520providing%2520a%2520sampling%2520surface%2520in%2520the%2520rendering%2520stage.%2520We%2520also%2520propose%250Aa%2520consistency%2520loss%2520and%2520a%2520spatial%2520fusion%2520strategy%2520to%2520ensure%2520the%2520multi-view%250Acoherence.%2520Experimental%2520results%2520show%2520that%2520our%2520method%2520outperforms%2520the%250Astate-of-the-art%2520methods%2520on%2520both%2520synthetic%2520data%2520and%2520challenging%2520real-world%250Aimages%252C%2520in%2520real-time.%2520The%2520project%2520page%2520can%2520be%2520found%2520at%250Ahttp%253A//cic.tju.edu.cn/faculty/likun/projects/R2Human.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2312.05826v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=R2Human%3A%20Real-Time%203D%20Human%20Appearance%20Rendering%20from%20a%20Single%20Image&entry.906535625=Yuanwang%20Yang%20and%20Qiao%20Feng%20and%20Yu-Kun%20Lai%20and%20Kun%20Li&entry.1292438233=%20%20Rendering%203D%20human%20appearance%20from%20a%20single%20image%20in%20real-time%20is%20crucial%20for%0Aachieving%20holographic%20communication%20and%20immersive%20VR/AR.%20Existing%20methods%0Aeither%20rely%20on%20multi-camera%20setups%20or%20are%20constrained%20to%20offline%20operations.%20In%0Athis%20paper%2C%20we%20propose%20R2Human%2C%20the%20first%20approach%20for%20real-time%20inference%20and%0Arendering%20of%20photorealistic%203D%20human%20appearance%20from%20a%20single%20image.%20The%20core%0Aof%20our%20approach%20is%20to%20combine%20the%20strengths%20of%20implicit%20texture%20fields%20and%0Aexplicit%20neural%20rendering%20with%20our%20novel%20representation%2C%20namely%20Z-map.%20Based%20on%0Athis%2C%20we%20present%20an%20end-to-end%20network%20that%20performs%20high-fidelity%20color%0Areconstruction%20of%20visible%20areas%20and%20provides%20reliable%20color%20inference%20for%0Aoccluded%20regions.%20To%20further%20enhance%20the%203D%20perception%20ability%20of%20our%20network%2C%0Awe%20leverage%20the%20Fourier%20occupancy%20field%20as%20a%20prior%20for%20generating%20the%20texture%0Afield%20and%20providing%20a%20sampling%20surface%20in%20the%20rendering%20stage.%20We%20also%20propose%0Aa%20consistency%20loss%20and%20a%20spatial%20fusion%20strategy%20to%20ensure%20the%20multi-view%0Acoherence.%20Experimental%20results%20show%20that%20our%20method%20outperforms%20the%0Astate-of-the-art%20methods%20on%20both%20synthetic%20data%20and%20challenging%20real-world%0Aimages%2C%20in%20real-time.%20The%20project%20page%20can%20be%20found%20at%0Ahttp%3A//cic.tju.edu.cn/faculty/likun/projects/R2Human.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.05826v4&entry.124074799=Read"},
{"title": "SYM3D: Learning Symmetric Triplanes for Better 3D-Awareness of GANs", "author": "Jing Yang and Kyle Fogarty and Fangcheng Zhong and Cengiz Oztireli", "abstract": "  Despite the growing success of 3D-aware GANs, which can be trained on 2D\nimages to generate high-quality 3D assets, they still rely on multi-view images\nwith camera annotations to synthesize sufficient details from all viewing\ndirections. However, the scarce availability of calibrated multi-view image\ndatasets, especially in comparison to single-view images, has limited the\npotential of 3D GANs. Moreover, while bypassing camera pose annotations with a\ncamera distribution constraint reduces dependence on exact camera parameters,\nit still struggles to generate a consistent orientation of 3D assets. To this\nend, we propose SYM3D, a novel 3D-aware GAN designed to leverage the prevalent\nreflectional symmetry structure found in natural and man-made objects,\nalongside a proposed view-aware spatial attention mechanism in learning the 3D\nrepresentation. We evaluate SYM3D on both synthetic (ShapeNet Chairs, Cars, and\nAirplanes) and real-world datasets (ABO-Chair), demonstrating its superior\nperformance in capturing detailed geometry and texture, even when trained on\nonly single-view images. Finally, we demonstrate the effectiveness of\nincorporating symmetry regularization in helping reduce artifacts in the\nmodeling of 3D assets in the text-to-3D task. Project is at\n\\url{https://jingyang2017.github.io/sym3d.github.io/}\n", "link": "http://arxiv.org/abs/2406.06432v2", "date": "2024-08-14", "relevancy": 2.9961, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6223}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5877}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5877}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20SYM3D%3A%20Learning%20Symmetric%20Triplanes%20for%20Better%203D-Awareness%20of%20GANs&body=Title%3A%20SYM3D%3A%20Learning%20Symmetric%20Triplanes%20for%20Better%203D-Awareness%20of%20GANs%0AAuthor%3A%20Jing%20Yang%20and%20Kyle%20Fogarty%20and%20Fangcheng%20Zhong%20and%20Cengiz%20Oztireli%0AAbstract%3A%20%20%20Despite%20the%20growing%20success%20of%203D-aware%20GANs%2C%20which%20can%20be%20trained%20on%202D%0Aimages%20to%20generate%20high-quality%203D%20assets%2C%20they%20still%20rely%20on%20multi-view%20images%0Awith%20camera%20annotations%20to%20synthesize%20sufficient%20details%20from%20all%20viewing%0Adirections.%20However%2C%20the%20scarce%20availability%20of%20calibrated%20multi-view%20image%0Adatasets%2C%20especially%20in%20comparison%20to%20single-view%20images%2C%20has%20limited%20the%0Apotential%20of%203D%20GANs.%20Moreover%2C%20while%20bypassing%20camera%20pose%20annotations%20with%20a%0Acamera%20distribution%20constraint%20reduces%20dependence%20on%20exact%20camera%20parameters%2C%0Ait%20still%20struggles%20to%20generate%20a%20consistent%20orientation%20of%203D%20assets.%20To%20this%0Aend%2C%20we%20propose%20SYM3D%2C%20a%20novel%203D-aware%20GAN%20designed%20to%20leverage%20the%20prevalent%0Areflectional%20symmetry%20structure%20found%20in%20natural%20and%20man-made%20objects%2C%0Aalongside%20a%20proposed%20view-aware%20spatial%20attention%20mechanism%20in%20learning%20the%203D%0Arepresentation.%20We%20evaluate%20SYM3D%20on%20both%20synthetic%20%28ShapeNet%20Chairs%2C%20Cars%2C%20and%0AAirplanes%29%20and%20real-world%20datasets%20%28ABO-Chair%29%2C%20demonstrating%20its%20superior%0Aperformance%20in%20capturing%20detailed%20geometry%20and%20texture%2C%20even%20when%20trained%20on%0Aonly%20single-view%20images.%20Finally%2C%20we%20demonstrate%20the%20effectiveness%20of%0Aincorporating%20symmetry%20regularization%20in%20helping%20reduce%20artifacts%20in%20the%0Amodeling%20of%203D%20assets%20in%20the%20text-to-3D%20task.%20Project%20is%20at%0A%5Curl%7Bhttps%3A//jingyang2017.github.io/sym3d.github.io/%7D%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.06432v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSYM3D%253A%2520Learning%2520Symmetric%2520Triplanes%2520for%2520Better%25203D-Awareness%2520of%2520GANs%26entry.906535625%3DJing%2520Yang%2520and%2520Kyle%2520Fogarty%2520and%2520Fangcheng%2520Zhong%2520and%2520Cengiz%2520Oztireli%26entry.1292438233%3D%2520%2520Despite%2520the%2520growing%2520success%2520of%25203D-aware%2520GANs%252C%2520which%2520can%2520be%2520trained%2520on%25202D%250Aimages%2520to%2520generate%2520high-quality%25203D%2520assets%252C%2520they%2520still%2520rely%2520on%2520multi-view%2520images%250Awith%2520camera%2520annotations%2520to%2520synthesize%2520sufficient%2520details%2520from%2520all%2520viewing%250Adirections.%2520However%252C%2520the%2520scarce%2520availability%2520of%2520calibrated%2520multi-view%2520image%250Adatasets%252C%2520especially%2520in%2520comparison%2520to%2520single-view%2520images%252C%2520has%2520limited%2520the%250Apotential%2520of%25203D%2520GANs.%2520Moreover%252C%2520while%2520bypassing%2520camera%2520pose%2520annotations%2520with%2520a%250Acamera%2520distribution%2520constraint%2520reduces%2520dependence%2520on%2520exact%2520camera%2520parameters%252C%250Ait%2520still%2520struggles%2520to%2520generate%2520a%2520consistent%2520orientation%2520of%25203D%2520assets.%2520To%2520this%250Aend%252C%2520we%2520propose%2520SYM3D%252C%2520a%2520novel%25203D-aware%2520GAN%2520designed%2520to%2520leverage%2520the%2520prevalent%250Areflectional%2520symmetry%2520structure%2520found%2520in%2520natural%2520and%2520man-made%2520objects%252C%250Aalongside%2520a%2520proposed%2520view-aware%2520spatial%2520attention%2520mechanism%2520in%2520learning%2520the%25203D%250Arepresentation.%2520We%2520evaluate%2520SYM3D%2520on%2520both%2520synthetic%2520%2528ShapeNet%2520Chairs%252C%2520Cars%252C%2520and%250AAirplanes%2529%2520and%2520real-world%2520datasets%2520%2528ABO-Chair%2529%252C%2520demonstrating%2520its%2520superior%250Aperformance%2520in%2520capturing%2520detailed%2520geometry%2520and%2520texture%252C%2520even%2520when%2520trained%2520on%250Aonly%2520single-view%2520images.%2520Finally%252C%2520we%2520demonstrate%2520the%2520effectiveness%2520of%250Aincorporating%2520symmetry%2520regularization%2520in%2520helping%2520reduce%2520artifacts%2520in%2520the%250Amodeling%2520of%25203D%2520assets%2520in%2520the%2520text-to-3D%2520task.%2520Project%2520is%2520at%250A%255Curl%257Bhttps%253A//jingyang2017.github.io/sym3d.github.io/%257D%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.06432v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=SYM3D%3A%20Learning%20Symmetric%20Triplanes%20for%20Better%203D-Awareness%20of%20GANs&entry.906535625=Jing%20Yang%20and%20Kyle%20Fogarty%20and%20Fangcheng%20Zhong%20and%20Cengiz%20Oztireli&entry.1292438233=%20%20Despite%20the%20growing%20success%20of%203D-aware%20GANs%2C%20which%20can%20be%20trained%20on%202D%0Aimages%20to%20generate%20high-quality%203D%20assets%2C%20they%20still%20rely%20on%20multi-view%20images%0Awith%20camera%20annotations%20to%20synthesize%20sufficient%20details%20from%20all%20viewing%0Adirections.%20However%2C%20the%20scarce%20availability%20of%20calibrated%20multi-view%20image%0Adatasets%2C%20especially%20in%20comparison%20to%20single-view%20images%2C%20has%20limited%20the%0Apotential%20of%203D%20GANs.%20Moreover%2C%20while%20bypassing%20camera%20pose%20annotations%20with%20a%0Acamera%20distribution%20constraint%20reduces%20dependence%20on%20exact%20camera%20parameters%2C%0Ait%20still%20struggles%20to%20generate%20a%20consistent%20orientation%20of%203D%20assets.%20To%20this%0Aend%2C%20we%20propose%20SYM3D%2C%20a%20novel%203D-aware%20GAN%20designed%20to%20leverage%20the%20prevalent%0Areflectional%20symmetry%20structure%20found%20in%20natural%20and%20man-made%20objects%2C%0Aalongside%20a%20proposed%20view-aware%20spatial%20attention%20mechanism%20in%20learning%20the%203D%0Arepresentation.%20We%20evaluate%20SYM3D%20on%20both%20synthetic%20%28ShapeNet%20Chairs%2C%20Cars%2C%20and%0AAirplanes%29%20and%20real-world%20datasets%20%28ABO-Chair%29%2C%20demonstrating%20its%20superior%0Aperformance%20in%20capturing%20detailed%20geometry%20and%20texture%2C%20even%20when%20trained%20on%0Aonly%20single-view%20images.%20Finally%2C%20we%20demonstrate%20the%20effectiveness%20of%0Aincorporating%20symmetry%20regularization%20in%20helping%20reduce%20artifacts%20in%20the%0Amodeling%20of%203D%20assets%20in%20the%20text-to-3D%20task.%20Project%20is%20at%0A%5Curl%7Bhttps%3A//jingyang2017.github.io/sym3d.github.io/%7D%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.06432v2&entry.124074799=Read"},
{"title": "DynaMoN: Motion-Aware Fast and Robust Camera Localization for Dynamic\n  Neural Radiance Fields", "author": "Nicolas Schischka and Hannah Schieber and Mert Asim Karaoglu and Melih G\u00f6rg\u00fcl\u00fc and Florian Gr\u00f6tzner and Alexander Ladikos and Daniel Roth and Nassir Navab and Benjamin Busam", "abstract": "  The accurate reconstruction of dynamic scenes with neural radiance fields is\nsignificantly dependent on the estimation of camera poses. Widely used\nstructure-from-motion pipelines encounter difficulties in accurately tracking\nthe camera trajectory when faced with separate dynamics of the scene content\nand the camera movement. To address this challenge, we propose Dynamic\nMotion-Aware Fast and Robust Camera Localization for Dynamic Neural Radiance\nFields (DynaMoN). DynaMoN utilizes semantic segmentation and generic motion\nmasks to handle dynamic content for initial camera pose estimation and\nstatics-focused ray sampling for fast and accurate novel-view synthesis. Our\nnovel iterative learning scheme switches between training the NeRF and updating\nthe pose parameters for an improved reconstruction and trajectory estimation\nquality. The proposed pipeline shows significant acceleration of the training\nprocess. We extensively evaluate our approach on two real-world dynamic\ndatasets, the TUM RGB-D dataset and the BONN RGB-D Dynamic dataset. DynaMoN\nimproves over the state-of-the-art both in terms of reconstruction quality and\ntrajectory accuracy. We plan to make our code public to enhance research in\nthis area.\n", "link": "http://arxiv.org/abs/2309.08927v3", "date": "2024-08-14", "relevancy": 2.9911, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.6131}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5946}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5869}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DynaMoN%3A%20Motion-Aware%20Fast%20and%20Robust%20Camera%20Localization%20for%20Dynamic%0A%20%20Neural%20Radiance%20Fields&body=Title%3A%20DynaMoN%3A%20Motion-Aware%20Fast%20and%20Robust%20Camera%20Localization%20for%20Dynamic%0A%20%20Neural%20Radiance%20Fields%0AAuthor%3A%20Nicolas%20Schischka%20and%20Hannah%20Schieber%20and%20Mert%20Asim%20Karaoglu%20and%20Melih%20G%C3%B6rg%C3%BCl%C3%BC%20and%20Florian%20Gr%C3%B6tzner%20and%20Alexander%20Ladikos%20and%20Daniel%20Roth%20and%20Nassir%20Navab%20and%20Benjamin%20Busam%0AAbstract%3A%20%20%20The%20accurate%20reconstruction%20of%20dynamic%20scenes%20with%20neural%20radiance%20fields%20is%0Asignificantly%20dependent%20on%20the%20estimation%20of%20camera%20poses.%20Widely%20used%0Astructure-from-motion%20pipelines%20encounter%20difficulties%20in%20accurately%20tracking%0Athe%20camera%20trajectory%20when%20faced%20with%20separate%20dynamics%20of%20the%20scene%20content%0Aand%20the%20camera%20movement.%20To%20address%20this%20challenge%2C%20we%20propose%20Dynamic%0AMotion-Aware%20Fast%20and%20Robust%20Camera%20Localization%20for%20Dynamic%20Neural%20Radiance%0AFields%20%28DynaMoN%29.%20DynaMoN%20utilizes%20semantic%20segmentation%20and%20generic%20motion%0Amasks%20to%20handle%20dynamic%20content%20for%20initial%20camera%20pose%20estimation%20and%0Astatics-focused%20ray%20sampling%20for%20fast%20and%20accurate%20novel-view%20synthesis.%20Our%0Anovel%20iterative%20learning%20scheme%20switches%20between%20training%20the%20NeRF%20and%20updating%0Athe%20pose%20parameters%20for%20an%20improved%20reconstruction%20and%20trajectory%20estimation%0Aquality.%20The%20proposed%20pipeline%20shows%20significant%20acceleration%20of%20the%20training%0Aprocess.%20We%20extensively%20evaluate%20our%20approach%20on%20two%20real-world%20dynamic%0Adatasets%2C%20the%20TUM%20RGB-D%20dataset%20and%20the%20BONN%20RGB-D%20Dynamic%20dataset.%20DynaMoN%0Aimproves%20over%20the%20state-of-the-art%20both%20in%20terms%20of%20reconstruction%20quality%20and%0Atrajectory%20accuracy.%20We%20plan%20to%20make%20our%20code%20public%20to%20enhance%20research%20in%0Athis%20area.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2309.08927v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDynaMoN%253A%2520Motion-Aware%2520Fast%2520and%2520Robust%2520Camera%2520Localization%2520for%2520Dynamic%250A%2520%2520Neural%2520Radiance%2520Fields%26entry.906535625%3DNicolas%2520Schischka%2520and%2520Hannah%2520Schieber%2520and%2520Mert%2520Asim%2520Karaoglu%2520and%2520Melih%2520G%25C3%25B6rg%25C3%25BCl%25C3%25BC%2520and%2520Florian%2520Gr%25C3%25B6tzner%2520and%2520Alexander%2520Ladikos%2520and%2520Daniel%2520Roth%2520and%2520Nassir%2520Navab%2520and%2520Benjamin%2520Busam%26entry.1292438233%3D%2520%2520The%2520accurate%2520reconstruction%2520of%2520dynamic%2520scenes%2520with%2520neural%2520radiance%2520fields%2520is%250Asignificantly%2520dependent%2520on%2520the%2520estimation%2520of%2520camera%2520poses.%2520Widely%2520used%250Astructure-from-motion%2520pipelines%2520encounter%2520difficulties%2520in%2520accurately%2520tracking%250Athe%2520camera%2520trajectory%2520when%2520faced%2520with%2520separate%2520dynamics%2520of%2520the%2520scene%2520content%250Aand%2520the%2520camera%2520movement.%2520To%2520address%2520this%2520challenge%252C%2520we%2520propose%2520Dynamic%250AMotion-Aware%2520Fast%2520and%2520Robust%2520Camera%2520Localization%2520for%2520Dynamic%2520Neural%2520Radiance%250AFields%2520%2528DynaMoN%2529.%2520DynaMoN%2520utilizes%2520semantic%2520segmentation%2520and%2520generic%2520motion%250Amasks%2520to%2520handle%2520dynamic%2520content%2520for%2520initial%2520camera%2520pose%2520estimation%2520and%250Astatics-focused%2520ray%2520sampling%2520for%2520fast%2520and%2520accurate%2520novel-view%2520synthesis.%2520Our%250Anovel%2520iterative%2520learning%2520scheme%2520switches%2520between%2520training%2520the%2520NeRF%2520and%2520updating%250Athe%2520pose%2520parameters%2520for%2520an%2520improved%2520reconstruction%2520and%2520trajectory%2520estimation%250Aquality.%2520The%2520proposed%2520pipeline%2520shows%2520significant%2520acceleration%2520of%2520the%2520training%250Aprocess.%2520We%2520extensively%2520evaluate%2520our%2520approach%2520on%2520two%2520real-world%2520dynamic%250Adatasets%252C%2520the%2520TUM%2520RGB-D%2520dataset%2520and%2520the%2520BONN%2520RGB-D%2520Dynamic%2520dataset.%2520DynaMoN%250Aimproves%2520over%2520the%2520state-of-the-art%2520both%2520in%2520terms%2520of%2520reconstruction%2520quality%2520and%250Atrajectory%2520accuracy.%2520We%2520plan%2520to%2520make%2520our%2520code%2520public%2520to%2520enhance%2520research%2520in%250Athis%2520area.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2309.08927v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DynaMoN%3A%20Motion-Aware%20Fast%20and%20Robust%20Camera%20Localization%20for%20Dynamic%0A%20%20Neural%20Radiance%20Fields&entry.906535625=Nicolas%20Schischka%20and%20Hannah%20Schieber%20and%20Mert%20Asim%20Karaoglu%20and%20Melih%20G%C3%B6rg%C3%BCl%C3%BC%20and%20Florian%20Gr%C3%B6tzner%20and%20Alexander%20Ladikos%20and%20Daniel%20Roth%20and%20Nassir%20Navab%20and%20Benjamin%20Busam&entry.1292438233=%20%20The%20accurate%20reconstruction%20of%20dynamic%20scenes%20with%20neural%20radiance%20fields%20is%0Asignificantly%20dependent%20on%20the%20estimation%20of%20camera%20poses.%20Widely%20used%0Astructure-from-motion%20pipelines%20encounter%20difficulties%20in%20accurately%20tracking%0Athe%20camera%20trajectory%20when%20faced%20with%20separate%20dynamics%20of%20the%20scene%20content%0Aand%20the%20camera%20movement.%20To%20address%20this%20challenge%2C%20we%20propose%20Dynamic%0AMotion-Aware%20Fast%20and%20Robust%20Camera%20Localization%20for%20Dynamic%20Neural%20Radiance%0AFields%20%28DynaMoN%29.%20DynaMoN%20utilizes%20semantic%20segmentation%20and%20generic%20motion%0Amasks%20to%20handle%20dynamic%20content%20for%20initial%20camera%20pose%20estimation%20and%0Astatics-focused%20ray%20sampling%20for%20fast%20and%20accurate%20novel-view%20synthesis.%20Our%0Anovel%20iterative%20learning%20scheme%20switches%20between%20training%20the%20NeRF%20and%20updating%0Athe%20pose%20parameters%20for%20an%20improved%20reconstruction%20and%20trajectory%20estimation%0Aquality.%20The%20proposed%20pipeline%20shows%20significant%20acceleration%20of%20the%20training%0Aprocess.%20We%20extensively%20evaluate%20our%20approach%20on%20two%20real-world%20dynamic%0Adatasets%2C%20the%20TUM%20RGB-D%20dataset%20and%20the%20BONN%20RGB-D%20Dynamic%20dataset.%20DynaMoN%0Aimproves%20over%20the%20state-of-the-art%20both%20in%20terms%20of%20reconstruction%20quality%20and%0Atrajectory%20accuracy.%20We%20plan%20to%20make%20our%20code%20public%20to%20enhance%20research%20in%0Athis%20area.%0A&entry.1838667208=http%3A//arxiv.org/abs/2309.08927v3&entry.124074799=Read"},
{"title": "Rethinking Open-Vocabulary Segmentation of Radiance Fields in 3D Space", "author": "Hyunjee Lee and Youngsik Yun and Jeongmin Bae and Seoha Kim and Youngjung Uh", "abstract": "  Understanding the 3D semantics of a scene is a fundamental problem for\nvarious scenarios such as embodied agents. While NeRFs and 3DGS excel at\nnovel-view synthesis, previous methods for understanding their semantics have\nbeen limited to incomplete 3D understanding: their segmentation results are 2D\nmasks and their supervision is anchored at 2D pixels. This paper revisits the\nproblem set to pursue a better 3D understanding of a scene modeled by NeRFs and\n3DGS as follows. 1) We directly supervise the 3D points to train the language\nembedding field. It achieves state-of-the-art accuracy without relying on\nmulti-scale language embeddings. 2) We transfer the pre-trained language field\nto 3DGS, achieving the first real-time rendering speed without sacrificing\ntraining time or accuracy. 3) We introduce a 3D querying and evaluation\nprotocol for assessing the reconstructed geometry and semantics together. Code,\ncheckpoints, and annotations will be available online. Project page:\nhttps://hyunji12.github.io/Open3DRF\n", "link": "http://arxiv.org/abs/2408.07416v1", "date": "2024-08-14", "relevancy": 2.9322, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6072}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.576}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.576}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20Open-Vocabulary%20Segmentation%20of%20Radiance%20Fields%20in%203D%20Space&body=Title%3A%20Rethinking%20Open-Vocabulary%20Segmentation%20of%20Radiance%20Fields%20in%203D%20Space%0AAuthor%3A%20Hyunjee%20Lee%20and%20Youngsik%20Yun%20and%20Jeongmin%20Bae%20and%20Seoha%20Kim%20and%20Youngjung%20Uh%0AAbstract%3A%20%20%20Understanding%20the%203D%20semantics%20of%20a%20scene%20is%20a%20fundamental%20problem%20for%0Avarious%20scenarios%20such%20as%20embodied%20agents.%20While%20NeRFs%20and%203DGS%20excel%20at%0Anovel-view%20synthesis%2C%20previous%20methods%20for%20understanding%20their%20semantics%20have%0Abeen%20limited%20to%20incomplete%203D%20understanding%3A%20their%20segmentation%20results%20are%202D%0Amasks%20and%20their%20supervision%20is%20anchored%20at%202D%20pixels.%20This%20paper%20revisits%20the%0Aproblem%20set%20to%20pursue%20a%20better%203D%20understanding%20of%20a%20scene%20modeled%20by%20NeRFs%20and%0A3DGS%20as%20follows.%201%29%20We%20directly%20supervise%20the%203D%20points%20to%20train%20the%20language%0Aembedding%20field.%20It%20achieves%20state-of-the-art%20accuracy%20without%20relying%20on%0Amulti-scale%20language%20embeddings.%202%29%20We%20transfer%20the%20pre-trained%20language%20field%0Ato%203DGS%2C%20achieving%20the%20first%20real-time%20rendering%20speed%20without%20sacrificing%0Atraining%20time%20or%20accuracy.%203%29%20We%20introduce%20a%203D%20querying%20and%20evaluation%0Aprotocol%20for%20assessing%20the%20reconstructed%20geometry%20and%20semantics%20together.%20Code%2C%0Acheckpoints%2C%20and%20annotations%20will%20be%20available%20online.%20Project%20page%3A%0Ahttps%3A//hyunji12.github.io/Open3DRF%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07416v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520Open-Vocabulary%2520Segmentation%2520of%2520Radiance%2520Fields%2520in%25203D%2520Space%26entry.906535625%3DHyunjee%2520Lee%2520and%2520Youngsik%2520Yun%2520and%2520Jeongmin%2520Bae%2520and%2520Seoha%2520Kim%2520and%2520Youngjung%2520Uh%26entry.1292438233%3D%2520%2520Understanding%2520the%25203D%2520semantics%2520of%2520a%2520scene%2520is%2520a%2520fundamental%2520problem%2520for%250Avarious%2520scenarios%2520such%2520as%2520embodied%2520agents.%2520While%2520NeRFs%2520and%25203DGS%2520excel%2520at%250Anovel-view%2520synthesis%252C%2520previous%2520methods%2520for%2520understanding%2520their%2520semantics%2520have%250Abeen%2520limited%2520to%2520incomplete%25203D%2520understanding%253A%2520their%2520segmentation%2520results%2520are%25202D%250Amasks%2520and%2520their%2520supervision%2520is%2520anchored%2520at%25202D%2520pixels.%2520This%2520paper%2520revisits%2520the%250Aproblem%2520set%2520to%2520pursue%2520a%2520better%25203D%2520understanding%2520of%2520a%2520scene%2520modeled%2520by%2520NeRFs%2520and%250A3DGS%2520as%2520follows.%25201%2529%2520We%2520directly%2520supervise%2520the%25203D%2520points%2520to%2520train%2520the%2520language%250Aembedding%2520field.%2520It%2520achieves%2520state-of-the-art%2520accuracy%2520without%2520relying%2520on%250Amulti-scale%2520language%2520embeddings.%25202%2529%2520We%2520transfer%2520the%2520pre-trained%2520language%2520field%250Ato%25203DGS%252C%2520achieving%2520the%2520first%2520real-time%2520rendering%2520speed%2520without%2520sacrificing%250Atraining%2520time%2520or%2520accuracy.%25203%2529%2520We%2520introduce%2520a%25203D%2520querying%2520and%2520evaluation%250Aprotocol%2520for%2520assessing%2520the%2520reconstructed%2520geometry%2520and%2520semantics%2520together.%2520Code%252C%250Acheckpoints%252C%2520and%2520annotations%2520will%2520be%2520available%2520online.%2520Project%2520page%253A%250Ahttps%253A//hyunji12.github.io/Open3DRF%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07416v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20Open-Vocabulary%20Segmentation%20of%20Radiance%20Fields%20in%203D%20Space&entry.906535625=Hyunjee%20Lee%20and%20Youngsik%20Yun%20and%20Jeongmin%20Bae%20and%20Seoha%20Kim%20and%20Youngjung%20Uh&entry.1292438233=%20%20Understanding%20the%203D%20semantics%20of%20a%20scene%20is%20a%20fundamental%20problem%20for%0Avarious%20scenarios%20such%20as%20embodied%20agents.%20While%20NeRFs%20and%203DGS%20excel%20at%0Anovel-view%20synthesis%2C%20previous%20methods%20for%20understanding%20their%20semantics%20have%0Abeen%20limited%20to%20incomplete%203D%20understanding%3A%20their%20segmentation%20results%20are%202D%0Amasks%20and%20their%20supervision%20is%20anchored%20at%202D%20pixels.%20This%20paper%20revisits%20the%0Aproblem%20set%20to%20pursue%20a%20better%203D%20understanding%20of%20a%20scene%20modeled%20by%20NeRFs%20and%0A3DGS%20as%20follows.%201%29%20We%20directly%20supervise%20the%203D%20points%20to%20train%20the%20language%0Aembedding%20field.%20It%20achieves%20state-of-the-art%20accuracy%20without%20relying%20on%0Amulti-scale%20language%20embeddings.%202%29%20We%20transfer%20the%20pre-trained%20language%20field%0Ato%203DGS%2C%20achieving%20the%20first%20real-time%20rendering%20speed%20without%20sacrificing%0Atraining%20time%20or%20accuracy.%203%29%20We%20introduce%20a%203D%20querying%20and%20evaluation%0Aprotocol%20for%20assessing%20the%20reconstructed%20geometry%20and%20semantics%20together.%20Code%2C%0Acheckpoints%2C%20and%20annotations%20will%20be%20available%20online.%20Project%20page%3A%0Ahttps%3A//hyunji12.github.io/Open3DRF%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07416v1&entry.124074799=Read"},
{"title": "Rethinking the Key Factors for the Generalization of Remote Sensing\n  Stereo Matching Networks", "author": "Liting Jiang and Feng Wang and Wenyi Zhang and Peifeng Li and Hongjian You and Yuming Xiang", "abstract": "  Stereo matching, a critical step of 3D reconstruction, has fully shifted\ntowards deep learning due to its strong feature representation of remote\nsensing images. However, ground truth for stereo matching task relies on\nexpensive airborne LiDAR data, thus making it difficult to obtain enough\nsamples for supervised learning. To improve the generalization ability of\nstereo matching networks on cross-domain data from different sensors and\nscenarios, in this paper, we dedicate to study key training factors from three\nperspectives. (1) For the selection of training dataset, it is important to\nselect data with similar regional target distribution as the test set instead\nof utilizing data from the same sensor. (2) For model structure, cascaded\nstructure that flexibly adapts to different sizes of features is preferred. (3)\nFor training manner, unsupervised methods generalize better than supervised\nmethods, and we design an unsupervised early-stop strategy to help retain the\nbest model with pre-trained weights as the basis. Extensive experiments are\nconducted to support the previous findings, on the basis of which we present an\nunsupervised stereo matching network with good generalization performance. We\nrelease the source code and the datasets at\nhttps://github.com/Elenairene/RKF_RSSM to reproduce the results and encourage\nfuture work.\n", "link": "http://arxiv.org/abs/2408.07613v1", "date": "2024-08-14", "relevancy": 2.9319, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6277}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5743}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5571}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Rethinking%20the%20Key%20Factors%20for%20the%20Generalization%20of%20Remote%20Sensing%0A%20%20Stereo%20Matching%20Networks&body=Title%3A%20Rethinking%20the%20Key%20Factors%20for%20the%20Generalization%20of%20Remote%20Sensing%0A%20%20Stereo%20Matching%20Networks%0AAuthor%3A%20Liting%20Jiang%20and%20Feng%20Wang%20and%20Wenyi%20Zhang%20and%20Peifeng%20Li%20and%20Hongjian%20You%20and%20Yuming%20Xiang%0AAbstract%3A%20%20%20Stereo%20matching%2C%20a%20critical%20step%20of%203D%20reconstruction%2C%20has%20fully%20shifted%0Atowards%20deep%20learning%20due%20to%20its%20strong%20feature%20representation%20of%20remote%0Asensing%20images.%20However%2C%20ground%20truth%20for%20stereo%20matching%20task%20relies%20on%0Aexpensive%20airborne%20LiDAR%20data%2C%20thus%20making%20it%20difficult%20to%20obtain%20enough%0Asamples%20for%20supervised%20learning.%20To%20improve%20the%20generalization%20ability%20of%0Astereo%20matching%20networks%20on%20cross-domain%20data%20from%20different%20sensors%20and%0Ascenarios%2C%20in%20this%20paper%2C%20we%20dedicate%20to%20study%20key%20training%20factors%20from%20three%0Aperspectives.%20%281%29%20For%20the%20selection%20of%20training%20dataset%2C%20it%20is%20important%20to%0Aselect%20data%20with%20similar%20regional%20target%20distribution%20as%20the%20test%20set%20instead%0Aof%20utilizing%20data%20from%20the%20same%20sensor.%20%282%29%20For%20model%20structure%2C%20cascaded%0Astructure%20that%20flexibly%20adapts%20to%20different%20sizes%20of%20features%20is%20preferred.%20%283%29%0AFor%20training%20manner%2C%20unsupervised%20methods%20generalize%20better%20than%20supervised%0Amethods%2C%20and%20we%20design%20an%20unsupervised%20early-stop%20strategy%20to%20help%20retain%20the%0Abest%20model%20with%20pre-trained%20weights%20as%20the%20basis.%20Extensive%20experiments%20are%0Aconducted%20to%20support%20the%20previous%20findings%2C%20on%20the%20basis%20of%20which%20we%20present%20an%0Aunsupervised%20stereo%20matching%20network%20with%20good%20generalization%20performance.%20We%0Arelease%20the%20source%20code%20and%20the%20datasets%20at%0Ahttps%3A//github.com/Elenairene/RKF_RSSM%20to%20reproduce%20the%20results%20and%20encourage%0Afuture%20work.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07613v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRethinking%2520the%2520Key%2520Factors%2520for%2520the%2520Generalization%2520of%2520Remote%2520Sensing%250A%2520%2520Stereo%2520Matching%2520Networks%26entry.906535625%3DLiting%2520Jiang%2520and%2520Feng%2520Wang%2520and%2520Wenyi%2520Zhang%2520and%2520Peifeng%2520Li%2520and%2520Hongjian%2520You%2520and%2520Yuming%2520Xiang%26entry.1292438233%3D%2520%2520Stereo%2520matching%252C%2520a%2520critical%2520step%2520of%25203D%2520reconstruction%252C%2520has%2520fully%2520shifted%250Atowards%2520deep%2520learning%2520due%2520to%2520its%2520strong%2520feature%2520representation%2520of%2520remote%250Asensing%2520images.%2520However%252C%2520ground%2520truth%2520for%2520stereo%2520matching%2520task%2520relies%2520on%250Aexpensive%2520airborne%2520LiDAR%2520data%252C%2520thus%2520making%2520it%2520difficult%2520to%2520obtain%2520enough%250Asamples%2520for%2520supervised%2520learning.%2520To%2520improve%2520the%2520generalization%2520ability%2520of%250Astereo%2520matching%2520networks%2520on%2520cross-domain%2520data%2520from%2520different%2520sensors%2520and%250Ascenarios%252C%2520in%2520this%2520paper%252C%2520we%2520dedicate%2520to%2520study%2520key%2520training%2520factors%2520from%2520three%250Aperspectives.%2520%25281%2529%2520For%2520the%2520selection%2520of%2520training%2520dataset%252C%2520it%2520is%2520important%2520to%250Aselect%2520data%2520with%2520similar%2520regional%2520target%2520distribution%2520as%2520the%2520test%2520set%2520instead%250Aof%2520utilizing%2520data%2520from%2520the%2520same%2520sensor.%2520%25282%2529%2520For%2520model%2520structure%252C%2520cascaded%250Astructure%2520that%2520flexibly%2520adapts%2520to%2520different%2520sizes%2520of%2520features%2520is%2520preferred.%2520%25283%2529%250AFor%2520training%2520manner%252C%2520unsupervised%2520methods%2520generalize%2520better%2520than%2520supervised%250Amethods%252C%2520and%2520we%2520design%2520an%2520unsupervised%2520early-stop%2520strategy%2520to%2520help%2520retain%2520the%250Abest%2520model%2520with%2520pre-trained%2520weights%2520as%2520the%2520basis.%2520Extensive%2520experiments%2520are%250Aconducted%2520to%2520support%2520the%2520previous%2520findings%252C%2520on%2520the%2520basis%2520of%2520which%2520we%2520present%2520an%250Aunsupervised%2520stereo%2520matching%2520network%2520with%2520good%2520generalization%2520performance.%2520We%250Arelease%2520the%2520source%2520code%2520and%2520the%2520datasets%2520at%250Ahttps%253A//github.com/Elenairene/RKF_RSSM%2520to%2520reproduce%2520the%2520results%2520and%2520encourage%250Afuture%2520work.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07613v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Rethinking%20the%20Key%20Factors%20for%20the%20Generalization%20of%20Remote%20Sensing%0A%20%20Stereo%20Matching%20Networks&entry.906535625=Liting%20Jiang%20and%20Feng%20Wang%20and%20Wenyi%20Zhang%20and%20Peifeng%20Li%20and%20Hongjian%20You%20and%20Yuming%20Xiang&entry.1292438233=%20%20Stereo%20matching%2C%20a%20critical%20step%20of%203D%20reconstruction%2C%20has%20fully%20shifted%0Atowards%20deep%20learning%20due%20to%20its%20strong%20feature%20representation%20of%20remote%0Asensing%20images.%20However%2C%20ground%20truth%20for%20stereo%20matching%20task%20relies%20on%0Aexpensive%20airborne%20LiDAR%20data%2C%20thus%20making%20it%20difficult%20to%20obtain%20enough%0Asamples%20for%20supervised%20learning.%20To%20improve%20the%20generalization%20ability%20of%0Astereo%20matching%20networks%20on%20cross-domain%20data%20from%20different%20sensors%20and%0Ascenarios%2C%20in%20this%20paper%2C%20we%20dedicate%20to%20study%20key%20training%20factors%20from%20three%0Aperspectives.%20%281%29%20For%20the%20selection%20of%20training%20dataset%2C%20it%20is%20important%20to%0Aselect%20data%20with%20similar%20regional%20target%20distribution%20as%20the%20test%20set%20instead%0Aof%20utilizing%20data%20from%20the%20same%20sensor.%20%282%29%20For%20model%20structure%2C%20cascaded%0Astructure%20that%20flexibly%20adapts%20to%20different%20sizes%20of%20features%20is%20preferred.%20%283%29%0AFor%20training%20manner%2C%20unsupervised%20methods%20generalize%20better%20than%20supervised%0Amethods%2C%20and%20we%20design%20an%20unsupervised%20early-stop%20strategy%20to%20help%20retain%20the%0Abest%20model%20with%20pre-trained%20weights%20as%20the%20basis.%20Extensive%20experiments%20are%0Aconducted%20to%20support%20the%20previous%20findings%2C%20on%20the%20basis%20of%20which%20we%20present%20an%0Aunsupervised%20stereo%20matching%20network%20with%20good%20generalization%20performance.%20We%0Arelease%20the%20source%20code%20and%20the%20datasets%20at%0Ahttps%3A//github.com/Elenairene/RKF_RSSM%20to%20reproduce%20the%20results%20and%20encourage%0Afuture%20work.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07613v1&entry.124074799=Read"},
{"title": "CLIP with Generative Latent Replay: a Strong Baseline for Incremental\n  Learning", "author": "Emanuele Frascaroli and Aniello Panariello and Pietro Buzzega and Lorenzo Bonicelli and Angelo Porrello and Simone Calderara", "abstract": "  With the emergence of Transformers and Vision-Language Models (VLMs) such as\nCLIP, fine-tuning large pre-trained models has recently become a prevalent\nstrategy in Continual Learning. This has led to the development of numerous\nprompting strategies to adapt transformer-based models without incurring\ncatastrophic forgetting. However, these strategies often compromise the\noriginal zero-shot capabilities of the pre-trained CLIP model and struggle to\nadapt to domains that significantly deviate from the pre-training data. In this\nwork, we propose Continual Generative training for Incremental prompt-Learning,\na simple and novel approach to mitigate forgetting while adapting CLIP.\nBriefly, we employ Variational Autoencoders (VAEs) to learn class-conditioned\ndistributions within the embedding space of the visual encoder. We then exploit\nthese distributions to sample new synthetic visual embeddings and train the\ncorresponding class-specific textual prompts during subsequent tasks. Through\nextensive experiments on different domains, we show that such a generative\nreplay approach can adapt to new tasks while improving zero-shot capabilities,\nevaluated using a novel metric tailored for CL scenarios. Notably, further\nanalysis reveals that our approach can bridge the gap with joint prompt tuning.\nThe codebase is available at https://github.com/aimagelab/mammoth.\n", "link": "http://arxiv.org/abs/2407.15793v3", "date": "2024-08-14", "relevancy": 2.8992, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5899}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5779}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5717}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CLIP%20with%20Generative%20Latent%20Replay%3A%20a%20Strong%20Baseline%20for%20Incremental%0A%20%20Learning&body=Title%3A%20CLIP%20with%20Generative%20Latent%20Replay%3A%20a%20Strong%20Baseline%20for%20Incremental%0A%20%20Learning%0AAuthor%3A%20Emanuele%20Frascaroli%20and%20Aniello%20Panariello%20and%20Pietro%20Buzzega%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara%0AAbstract%3A%20%20%20With%20the%20emergence%20of%20Transformers%20and%20Vision-Language%20Models%20%28VLMs%29%20such%20as%0ACLIP%2C%20fine-tuning%20large%20pre-trained%20models%20has%20recently%20become%20a%20prevalent%0Astrategy%20in%20Continual%20Learning.%20This%20has%20led%20to%20the%20development%20of%20numerous%0Aprompting%20strategies%20to%20adapt%20transformer-based%20models%20without%20incurring%0Acatastrophic%20forgetting.%20However%2C%20these%20strategies%20often%20compromise%20the%0Aoriginal%20zero-shot%20capabilities%20of%20the%20pre-trained%20CLIP%20model%20and%20struggle%20to%0Aadapt%20to%20domains%20that%20significantly%20deviate%20from%20the%20pre-training%20data.%20In%20this%0Awork%2C%20we%20propose%20Continual%20Generative%20training%20for%20Incremental%20prompt-Learning%2C%0Aa%20simple%20and%20novel%20approach%20to%20mitigate%20forgetting%20while%20adapting%20CLIP.%0ABriefly%2C%20we%20employ%20Variational%20Autoencoders%20%28VAEs%29%20to%20learn%20class-conditioned%0Adistributions%20within%20the%20embedding%20space%20of%20the%20visual%20encoder.%20We%20then%20exploit%0Athese%20distributions%20to%20sample%20new%20synthetic%20visual%20embeddings%20and%20train%20the%0Acorresponding%20class-specific%20textual%20prompts%20during%20subsequent%20tasks.%20Through%0Aextensive%20experiments%20on%20different%20domains%2C%20we%20show%20that%20such%20a%20generative%0Areplay%20approach%20can%20adapt%20to%20new%20tasks%20while%20improving%20zero-shot%20capabilities%2C%0Aevaluated%20using%20a%20novel%20metric%20tailored%20for%20CL%20scenarios.%20Notably%2C%20further%0Aanalysis%20reveals%20that%20our%20approach%20can%20bridge%20the%20gap%20with%20joint%20prompt%20tuning.%0AThe%20codebase%20is%20available%20at%20https%3A//github.com/aimagelab/mammoth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.15793v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCLIP%2520with%2520Generative%2520Latent%2520Replay%253A%2520a%2520Strong%2520Baseline%2520for%2520Incremental%250A%2520%2520Learning%26entry.906535625%3DEmanuele%2520Frascaroli%2520and%2520Aniello%2520Panariello%2520and%2520Pietro%2520Buzzega%2520and%2520Lorenzo%2520Bonicelli%2520and%2520Angelo%2520Porrello%2520and%2520Simone%2520Calderara%26entry.1292438233%3D%2520%2520With%2520the%2520emergence%2520of%2520Transformers%2520and%2520Vision-Language%2520Models%2520%2528VLMs%2529%2520such%2520as%250ACLIP%252C%2520fine-tuning%2520large%2520pre-trained%2520models%2520has%2520recently%2520become%2520a%2520prevalent%250Astrategy%2520in%2520Continual%2520Learning.%2520This%2520has%2520led%2520to%2520the%2520development%2520of%2520numerous%250Aprompting%2520strategies%2520to%2520adapt%2520transformer-based%2520models%2520without%2520incurring%250Acatastrophic%2520forgetting.%2520However%252C%2520these%2520strategies%2520often%2520compromise%2520the%250Aoriginal%2520zero-shot%2520capabilities%2520of%2520the%2520pre-trained%2520CLIP%2520model%2520and%2520struggle%2520to%250Aadapt%2520to%2520domains%2520that%2520significantly%2520deviate%2520from%2520the%2520pre-training%2520data.%2520In%2520this%250Awork%252C%2520we%2520propose%2520Continual%2520Generative%2520training%2520for%2520Incremental%2520prompt-Learning%252C%250Aa%2520simple%2520and%2520novel%2520approach%2520to%2520mitigate%2520forgetting%2520while%2520adapting%2520CLIP.%250ABriefly%252C%2520we%2520employ%2520Variational%2520Autoencoders%2520%2528VAEs%2529%2520to%2520learn%2520class-conditioned%250Adistributions%2520within%2520the%2520embedding%2520space%2520of%2520the%2520visual%2520encoder.%2520We%2520then%2520exploit%250Athese%2520distributions%2520to%2520sample%2520new%2520synthetic%2520visual%2520embeddings%2520and%2520train%2520the%250Acorresponding%2520class-specific%2520textual%2520prompts%2520during%2520subsequent%2520tasks.%2520Through%250Aextensive%2520experiments%2520on%2520different%2520domains%252C%2520we%2520show%2520that%2520such%2520a%2520generative%250Areplay%2520approach%2520can%2520adapt%2520to%2520new%2520tasks%2520while%2520improving%2520zero-shot%2520capabilities%252C%250Aevaluated%2520using%2520a%2520novel%2520metric%2520tailored%2520for%2520CL%2520scenarios.%2520Notably%252C%2520further%250Aanalysis%2520reveals%2520that%2520our%2520approach%2520can%2520bridge%2520the%2520gap%2520with%2520joint%2520prompt%2520tuning.%250AThe%2520codebase%2520is%2520available%2520at%2520https%253A//github.com/aimagelab/mammoth.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.15793v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CLIP%20with%20Generative%20Latent%20Replay%3A%20a%20Strong%20Baseline%20for%20Incremental%0A%20%20Learning&entry.906535625=Emanuele%20Frascaroli%20and%20Aniello%20Panariello%20and%20Pietro%20Buzzega%20and%20Lorenzo%20Bonicelli%20and%20Angelo%20Porrello%20and%20Simone%20Calderara&entry.1292438233=%20%20With%20the%20emergence%20of%20Transformers%20and%20Vision-Language%20Models%20%28VLMs%29%20such%20as%0ACLIP%2C%20fine-tuning%20large%20pre-trained%20models%20has%20recently%20become%20a%20prevalent%0Astrategy%20in%20Continual%20Learning.%20This%20has%20led%20to%20the%20development%20of%20numerous%0Aprompting%20strategies%20to%20adapt%20transformer-based%20models%20without%20incurring%0Acatastrophic%20forgetting.%20However%2C%20these%20strategies%20often%20compromise%20the%0Aoriginal%20zero-shot%20capabilities%20of%20the%20pre-trained%20CLIP%20model%20and%20struggle%20to%0Aadapt%20to%20domains%20that%20significantly%20deviate%20from%20the%20pre-training%20data.%20In%20this%0Awork%2C%20we%20propose%20Continual%20Generative%20training%20for%20Incremental%20prompt-Learning%2C%0Aa%20simple%20and%20novel%20approach%20to%20mitigate%20forgetting%20while%20adapting%20CLIP.%0ABriefly%2C%20we%20employ%20Variational%20Autoencoders%20%28VAEs%29%20to%20learn%20class-conditioned%0Adistributions%20within%20the%20embedding%20space%20of%20the%20visual%20encoder.%20We%20then%20exploit%0Athese%20distributions%20to%20sample%20new%20synthetic%20visual%20embeddings%20and%20train%20the%0Acorresponding%20class-specific%20textual%20prompts%20during%20subsequent%20tasks.%20Through%0Aextensive%20experiments%20on%20different%20domains%2C%20we%20show%20that%20such%20a%20generative%0Areplay%20approach%20can%20adapt%20to%20new%20tasks%20while%20improving%20zero-shot%20capabilities%2C%0Aevaluated%20using%20a%20novel%20metric%20tailored%20for%20CL%20scenarios.%20Notably%2C%20further%0Aanalysis%20reveals%20that%20our%20approach%20can%20bridge%20the%20gap%20with%20joint%20prompt%20tuning.%0AThe%20codebase%20is%20available%20at%20https%3A//github.com/aimagelab/mammoth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.15793v3&entry.124074799=Read"},
{"title": "Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic\n  Image Design and Generation", "author": "Zhengyuan Yang and Jianfeng Wang and Linjie Li and Kevin Lin and Chung-Ching Lin and Zicheng Liu and Lijuan Wang", "abstract": "  We introduce ``Idea to Image,'' a system that enables multimodal iterative\nself-refinement with GPT-4V(ision) for automatic image design and generation.\nHumans can quickly identify the characteristics of different text-to-image\n(T2I) models via iterative explorations. This enables them to efficiently\nconvert their high-level generation ideas into effective T2I prompts that can\nproduce good images. We investigate if systems based on large multimodal models\n(LMMs) can develop analogous multimodal self-refinement abilities that enable\nexploring unknown models or environments via self-refining tries. Idea2Img\ncyclically generates revised T2I prompts to synthesize draft images, and\nprovides directional feedback for prompt revision, both conditioned on its\nmemory of the probed T2I model's characteristics. The iterative self-refinement\nbrings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Img\ncan process input ideas with interleaved image-text sequences, follow ideas\nwith design instructions, and generate images of better semantic and visual\nqualities. The user preference study validates the efficacy of multimodal\niterative self-refinement on automatic image design and generation.\n", "link": "http://arxiv.org/abs/2310.08541v2", "date": "2024-08-14", "relevancy": 2.8681, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.595}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5791}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5467}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Idea2Img%3A%20Iterative%20Self-Refinement%20with%20GPT-4V%28ision%29%20for%20Automatic%0A%20%20Image%20Design%20and%20Generation&body=Title%3A%20Idea2Img%3A%20Iterative%20Self-Refinement%20with%20GPT-4V%28ision%29%20for%20Automatic%0A%20%20Image%20Design%20and%20Generation%0AAuthor%3A%20Zhengyuan%20Yang%20and%20Jianfeng%20Wang%20and%20Linjie%20Li%20and%20Kevin%20Lin%20and%20Chung-Ching%20Lin%20and%20Zicheng%20Liu%20and%20Lijuan%20Wang%0AAbstract%3A%20%20%20We%20introduce%20%60%60Idea%20to%20Image%2C%27%27%20a%20system%20that%20enables%20multimodal%20iterative%0Aself-refinement%20with%20GPT-4V%28ision%29%20for%20automatic%20image%20design%20and%20generation.%0AHumans%20can%20quickly%20identify%20the%20characteristics%20of%20different%20text-to-image%0A%28T2I%29%20models%20via%20iterative%20explorations.%20This%20enables%20them%20to%20efficiently%0Aconvert%20their%20high-level%20generation%20ideas%20into%20effective%20T2I%20prompts%20that%20can%0Aproduce%20good%20images.%20We%20investigate%20if%20systems%20based%20on%20large%20multimodal%20models%0A%28LMMs%29%20can%20develop%20analogous%20multimodal%20self-refinement%20abilities%20that%20enable%0Aexploring%20unknown%20models%20or%20environments%20via%20self-refining%20tries.%20Idea2Img%0Acyclically%20generates%20revised%20T2I%20prompts%20to%20synthesize%20draft%20images%2C%20and%0Aprovides%20directional%20feedback%20for%20prompt%20revision%2C%20both%20conditioned%20on%20its%0Amemory%20of%20the%20probed%20T2I%20model%27s%20characteristics.%20The%20iterative%20self-refinement%0Abrings%20Idea2Img%20various%20advantages%20over%20vanilla%20T2I%20models.%20Notably%2C%20Idea2Img%0Acan%20process%20input%20ideas%20with%20interleaved%20image-text%20sequences%2C%20follow%20ideas%0Awith%20design%20instructions%2C%20and%20generate%20images%20of%20better%20semantic%20and%20visual%0Aqualities.%20The%20user%20preference%20study%20validates%20the%20efficacy%20of%20multimodal%0Aiterative%20self-refinement%20on%20automatic%20image%20design%20and%20generation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.08541v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DIdea2Img%253A%2520Iterative%2520Self-Refinement%2520with%2520GPT-4V%2528ision%2529%2520for%2520Automatic%250A%2520%2520Image%2520Design%2520and%2520Generation%26entry.906535625%3DZhengyuan%2520Yang%2520and%2520Jianfeng%2520Wang%2520and%2520Linjie%2520Li%2520and%2520Kevin%2520Lin%2520and%2520Chung-Ching%2520Lin%2520and%2520Zicheng%2520Liu%2520and%2520Lijuan%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520%2560%2560Idea%2520to%2520Image%252C%2527%2527%2520a%2520system%2520that%2520enables%2520multimodal%2520iterative%250Aself-refinement%2520with%2520GPT-4V%2528ision%2529%2520for%2520automatic%2520image%2520design%2520and%2520generation.%250AHumans%2520can%2520quickly%2520identify%2520the%2520characteristics%2520of%2520different%2520text-to-image%250A%2528T2I%2529%2520models%2520via%2520iterative%2520explorations.%2520This%2520enables%2520them%2520to%2520efficiently%250Aconvert%2520their%2520high-level%2520generation%2520ideas%2520into%2520effective%2520T2I%2520prompts%2520that%2520can%250Aproduce%2520good%2520images.%2520We%2520investigate%2520if%2520systems%2520based%2520on%2520large%2520multimodal%2520models%250A%2528LMMs%2529%2520can%2520develop%2520analogous%2520multimodal%2520self-refinement%2520abilities%2520that%2520enable%250Aexploring%2520unknown%2520models%2520or%2520environments%2520via%2520self-refining%2520tries.%2520Idea2Img%250Acyclically%2520generates%2520revised%2520T2I%2520prompts%2520to%2520synthesize%2520draft%2520images%252C%2520and%250Aprovides%2520directional%2520feedback%2520for%2520prompt%2520revision%252C%2520both%2520conditioned%2520on%2520its%250Amemory%2520of%2520the%2520probed%2520T2I%2520model%2527s%2520characteristics.%2520The%2520iterative%2520self-refinement%250Abrings%2520Idea2Img%2520various%2520advantages%2520over%2520vanilla%2520T2I%2520models.%2520Notably%252C%2520Idea2Img%250Acan%2520process%2520input%2520ideas%2520with%2520interleaved%2520image-text%2520sequences%252C%2520follow%2520ideas%250Awith%2520design%2520instructions%252C%2520and%2520generate%2520images%2520of%2520better%2520semantic%2520and%2520visual%250Aqualities.%2520The%2520user%2520preference%2520study%2520validates%2520the%2520efficacy%2520of%2520multimodal%250Aiterative%2520self-refinement%2520on%2520automatic%2520image%2520design%2520and%2520generation.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.08541v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Idea2Img%3A%20Iterative%20Self-Refinement%20with%20GPT-4V%28ision%29%20for%20Automatic%0A%20%20Image%20Design%20and%20Generation&entry.906535625=Zhengyuan%20Yang%20and%20Jianfeng%20Wang%20and%20Linjie%20Li%20and%20Kevin%20Lin%20and%20Chung-Ching%20Lin%20and%20Zicheng%20Liu%20and%20Lijuan%20Wang&entry.1292438233=%20%20We%20introduce%20%60%60Idea%20to%20Image%2C%27%27%20a%20system%20that%20enables%20multimodal%20iterative%0Aself-refinement%20with%20GPT-4V%28ision%29%20for%20automatic%20image%20design%20and%20generation.%0AHumans%20can%20quickly%20identify%20the%20characteristics%20of%20different%20text-to-image%0A%28T2I%29%20models%20via%20iterative%20explorations.%20This%20enables%20them%20to%20efficiently%0Aconvert%20their%20high-level%20generation%20ideas%20into%20effective%20T2I%20prompts%20that%20can%0Aproduce%20good%20images.%20We%20investigate%20if%20systems%20based%20on%20large%20multimodal%20models%0A%28LMMs%29%20can%20develop%20analogous%20multimodal%20self-refinement%20abilities%20that%20enable%0Aexploring%20unknown%20models%20or%20environments%20via%20self-refining%20tries.%20Idea2Img%0Acyclically%20generates%20revised%20T2I%20prompts%20to%20synthesize%20draft%20images%2C%20and%0Aprovides%20directional%20feedback%20for%20prompt%20revision%2C%20both%20conditioned%20on%20its%0Amemory%20of%20the%20probed%20T2I%20model%27s%20characteristics.%20The%20iterative%20self-refinement%0Abrings%20Idea2Img%20various%20advantages%20over%20vanilla%20T2I%20models.%20Notably%2C%20Idea2Img%0Acan%20process%20input%20ideas%20with%20interleaved%20image-text%20sequences%2C%20follow%20ideas%0Awith%20design%20instructions%2C%20and%20generate%20images%20of%20better%20semantic%20and%20visual%0Aqualities.%20The%20user%20preference%20study%20validates%20the%20efficacy%20of%20multimodal%0Aiterative%20self-refinement%20on%20automatic%20image%20design%20and%20generation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.08541v2&entry.124074799=Read"},
{"title": "Panacea+: Panoramic and Controllable Video Generation for Autonomous\n  Driving", "author": "Yuqing Wen and Yucheng Zhao and Yingfei Liu and Binyuan Huang and Fan Jia and Yanhui Wang and Chi Zhang and Tiancai Wang and Xiaoyan Sun and Xiangyu Zhang", "abstract": "  The field of autonomous driving increasingly demands high-quality annotated\nvideo training data. In this paper, we propose Panacea+, a powerful and\nuniversally applicable framework for generating video data in driving scenes.\nBuilt upon the foundation of our previous work, Panacea, Panacea+ adopts a\nmulti-view appearance noise prior mechanism and a super-resolution module for\nenhanced consistency and increased resolution. Extensive experiments show that\nthe generated video samples from Panacea+ greatly benefit a wide range of tasks\non different datasets, including 3D object tracking, 3D object detection, and\nlane detection tasks on the nuScenes and Argoverse 2 dataset. These results\nstrongly prove Panacea+ to be a valuable data generation framework for\nautonomous driving.\n", "link": "http://arxiv.org/abs/2408.07605v1", "date": "2024-08-14", "relevancy": 2.8548, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6082}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5574}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Panacea%2B%3A%20Panoramic%20and%20Controllable%20Video%20Generation%20for%20Autonomous%0A%20%20Driving&body=Title%3A%20Panacea%2B%3A%20Panoramic%20and%20Controllable%20Video%20Generation%20for%20Autonomous%0A%20%20Driving%0AAuthor%3A%20Yuqing%20Wen%20and%20Yucheng%20Zhao%20and%20Yingfei%20Liu%20and%20Binyuan%20Huang%20and%20Fan%20Jia%20and%20Yanhui%20Wang%20and%20Chi%20Zhang%20and%20Tiancai%20Wang%20and%20Xiaoyan%20Sun%20and%20Xiangyu%20Zhang%0AAbstract%3A%20%20%20The%20field%20of%20autonomous%20driving%20increasingly%20demands%20high-quality%20annotated%0Avideo%20training%20data.%20In%20this%20paper%2C%20we%20propose%20Panacea%2B%2C%20a%20powerful%20and%0Auniversally%20applicable%20framework%20for%20generating%20video%20data%20in%20driving%20scenes.%0ABuilt%20upon%20the%20foundation%20of%20our%20previous%20work%2C%20Panacea%2C%20Panacea%2B%20adopts%20a%0Amulti-view%20appearance%20noise%20prior%20mechanism%20and%20a%20super-resolution%20module%20for%0Aenhanced%20consistency%20and%20increased%20resolution.%20Extensive%20experiments%20show%20that%0Athe%20generated%20video%20samples%20from%20Panacea%2B%20greatly%20benefit%20a%20wide%20range%20of%20tasks%0Aon%20different%20datasets%2C%20including%203D%20object%20tracking%2C%203D%20object%20detection%2C%20and%0Alane%20detection%20tasks%20on%20the%20nuScenes%20and%20Argoverse%202%20dataset.%20These%20results%0Astrongly%20prove%20Panacea%2B%20to%20be%20a%20valuable%20data%20generation%20framework%20for%0Aautonomous%20driving.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07605v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPanacea%252B%253A%2520Panoramic%2520and%2520Controllable%2520Video%2520Generation%2520for%2520Autonomous%250A%2520%2520Driving%26entry.906535625%3DYuqing%2520Wen%2520and%2520Yucheng%2520Zhao%2520and%2520Yingfei%2520Liu%2520and%2520Binyuan%2520Huang%2520and%2520Fan%2520Jia%2520and%2520Yanhui%2520Wang%2520and%2520Chi%2520Zhang%2520and%2520Tiancai%2520Wang%2520and%2520Xiaoyan%2520Sun%2520and%2520Xiangyu%2520Zhang%26entry.1292438233%3D%2520%2520The%2520field%2520of%2520autonomous%2520driving%2520increasingly%2520demands%2520high-quality%2520annotated%250Avideo%2520training%2520data.%2520In%2520this%2520paper%252C%2520we%2520propose%2520Panacea%252B%252C%2520a%2520powerful%2520and%250Auniversally%2520applicable%2520framework%2520for%2520generating%2520video%2520data%2520in%2520driving%2520scenes.%250ABuilt%2520upon%2520the%2520foundation%2520of%2520our%2520previous%2520work%252C%2520Panacea%252C%2520Panacea%252B%2520adopts%2520a%250Amulti-view%2520appearance%2520noise%2520prior%2520mechanism%2520and%2520a%2520super-resolution%2520module%2520for%250Aenhanced%2520consistency%2520and%2520increased%2520resolution.%2520Extensive%2520experiments%2520show%2520that%250Athe%2520generated%2520video%2520samples%2520from%2520Panacea%252B%2520greatly%2520benefit%2520a%2520wide%2520range%2520of%2520tasks%250Aon%2520different%2520datasets%252C%2520including%25203D%2520object%2520tracking%252C%25203D%2520object%2520detection%252C%2520and%250Alane%2520detection%2520tasks%2520on%2520the%2520nuScenes%2520and%2520Argoverse%25202%2520dataset.%2520These%2520results%250Astrongly%2520prove%2520Panacea%252B%2520to%2520be%2520a%2520valuable%2520data%2520generation%2520framework%2520for%250Aautonomous%2520driving.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07605v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Panacea%2B%3A%20Panoramic%20and%20Controllable%20Video%20Generation%20for%20Autonomous%0A%20%20Driving&entry.906535625=Yuqing%20Wen%20and%20Yucheng%20Zhao%20and%20Yingfei%20Liu%20and%20Binyuan%20Huang%20and%20Fan%20Jia%20and%20Yanhui%20Wang%20and%20Chi%20Zhang%20and%20Tiancai%20Wang%20and%20Xiaoyan%20Sun%20and%20Xiangyu%20Zhang&entry.1292438233=%20%20The%20field%20of%20autonomous%20driving%20increasingly%20demands%20high-quality%20annotated%0Avideo%20training%20data.%20In%20this%20paper%2C%20we%20propose%20Panacea%2B%2C%20a%20powerful%20and%0Auniversally%20applicable%20framework%20for%20generating%20video%20data%20in%20driving%20scenes.%0ABuilt%20upon%20the%20foundation%20of%20our%20previous%20work%2C%20Panacea%2C%20Panacea%2B%20adopts%20a%0Amulti-view%20appearance%20noise%20prior%20mechanism%20and%20a%20super-resolution%20module%20for%0Aenhanced%20consistency%20and%20increased%20resolution.%20Extensive%20experiments%20show%20that%0Athe%20generated%20video%20samples%20from%20Panacea%2B%20greatly%20benefit%20a%20wide%20range%20of%20tasks%0Aon%20different%20datasets%2C%20including%203D%20object%20tracking%2C%203D%20object%20detection%2C%20and%0Alane%20detection%20tasks%20on%20the%20nuScenes%20and%20Argoverse%202%20dataset.%20These%20results%0Astrongly%20prove%20Panacea%2B%20to%20be%20a%20valuable%20data%20generation%20framework%20for%0Aautonomous%20driving.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07605v1&entry.124074799=Read"},
{"title": "On the Utility of 3D Hand Poses for Action Recognition", "author": "Md Salman Shamil and Dibyadip Chatterjee and Fadime Sener and Shugao Ma and Angela Yao", "abstract": "  3D hand pose is an underexplored modality for action recognition. Poses are\ncompact yet informative and can greatly benefit applications with limited\ncompute budgets. However, poses alone offer an incomplete understanding of\nactions, as they cannot fully capture objects and environments with which\nhumans interact. We propose HandFormer, a novel multimodal transformer, to\nefficiently model hand-object interactions. HandFormer combines 3D hand poses\nat a high temporal resolution for fine-grained motion modeling with sparsely\nsampled RGB frames for encoding scene semantics. Observing the unique\ncharacteristics of hand poses, we temporally factorize hand modeling and\nrepresent each joint by its short-term trajectories. This factorized pose\nrepresentation combined with sparse RGB samples is remarkably efficient and\nhighly accurate. Unimodal HandFormer with only hand poses outperforms existing\nskeleton-based methods at 5x fewer FLOPs. With RGB, we achieve new\nstate-of-the-art performance on Assembly101 and H2O with significant\nimprovements in egocentric action recognition.\n", "link": "http://arxiv.org/abs/2403.09805v2", "date": "2024-08-14", "relevancy": 2.8235, "topK": [{"title": "PoseAnimate: Zero-shot high fidelity pose controllable character\n  animation", "link": "http://arxiv.org/abs/2404.13680v2", "similarity": 0.573}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5656}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5555}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20On%20the%20Utility%20of%203D%20Hand%20Poses%20for%20Action%20Recognition&body=Title%3A%20On%20the%20Utility%20of%203D%20Hand%20Poses%20for%20Action%20Recognition%0AAuthor%3A%20Md%20Salman%20Shamil%20and%20Dibyadip%20Chatterjee%20and%20Fadime%20Sener%20and%20Shugao%20Ma%20and%20Angela%20Yao%0AAbstract%3A%20%20%203D%20hand%20pose%20is%20an%20underexplored%20modality%20for%20action%20recognition.%20Poses%20are%0Acompact%20yet%20informative%20and%20can%20greatly%20benefit%20applications%20with%20limited%0Acompute%20budgets.%20However%2C%20poses%20alone%20offer%20an%20incomplete%20understanding%20of%0Aactions%2C%20as%20they%20cannot%20fully%20capture%20objects%20and%20environments%20with%20which%0Ahumans%20interact.%20We%20propose%20HandFormer%2C%20a%20novel%20multimodal%20transformer%2C%20to%0Aefficiently%20model%20hand-object%20interactions.%20HandFormer%20combines%203D%20hand%20poses%0Aat%20a%20high%20temporal%20resolution%20for%20fine-grained%20motion%20modeling%20with%20sparsely%0Asampled%20RGB%20frames%20for%20encoding%20scene%20semantics.%20Observing%20the%20unique%0Acharacteristics%20of%20hand%20poses%2C%20we%20temporally%20factorize%20hand%20modeling%20and%0Arepresent%20each%20joint%20by%20its%20short-term%20trajectories.%20This%20factorized%20pose%0Arepresentation%20combined%20with%20sparse%20RGB%20samples%20is%20remarkably%20efficient%20and%0Ahighly%20accurate.%20Unimodal%20HandFormer%20with%20only%20hand%20poses%20outperforms%20existing%0Askeleton-based%20methods%20at%205x%20fewer%20FLOPs.%20With%20RGB%2C%20we%20achieve%20new%0Astate-of-the-art%20performance%20on%20Assembly101%20and%20H2O%20with%20significant%0Aimprovements%20in%20egocentric%20action%20recognition.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09805v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOn%2520the%2520Utility%2520of%25203D%2520Hand%2520Poses%2520for%2520Action%2520Recognition%26entry.906535625%3DMd%2520Salman%2520Shamil%2520and%2520Dibyadip%2520Chatterjee%2520and%2520Fadime%2520Sener%2520and%2520Shugao%2520Ma%2520and%2520Angela%2520Yao%26entry.1292438233%3D%2520%25203D%2520hand%2520pose%2520is%2520an%2520underexplored%2520modality%2520for%2520action%2520recognition.%2520Poses%2520are%250Acompact%2520yet%2520informative%2520and%2520can%2520greatly%2520benefit%2520applications%2520with%2520limited%250Acompute%2520budgets.%2520However%252C%2520poses%2520alone%2520offer%2520an%2520incomplete%2520understanding%2520of%250Aactions%252C%2520as%2520they%2520cannot%2520fully%2520capture%2520objects%2520and%2520environments%2520with%2520which%250Ahumans%2520interact.%2520We%2520propose%2520HandFormer%252C%2520a%2520novel%2520multimodal%2520transformer%252C%2520to%250Aefficiently%2520model%2520hand-object%2520interactions.%2520HandFormer%2520combines%25203D%2520hand%2520poses%250Aat%2520a%2520high%2520temporal%2520resolution%2520for%2520fine-grained%2520motion%2520modeling%2520with%2520sparsely%250Asampled%2520RGB%2520frames%2520for%2520encoding%2520scene%2520semantics.%2520Observing%2520the%2520unique%250Acharacteristics%2520of%2520hand%2520poses%252C%2520we%2520temporally%2520factorize%2520hand%2520modeling%2520and%250Arepresent%2520each%2520joint%2520by%2520its%2520short-term%2520trajectories.%2520This%2520factorized%2520pose%250Arepresentation%2520combined%2520with%2520sparse%2520RGB%2520samples%2520is%2520remarkably%2520efficient%2520and%250Ahighly%2520accurate.%2520Unimodal%2520HandFormer%2520with%2520only%2520hand%2520poses%2520outperforms%2520existing%250Askeleton-based%2520methods%2520at%25205x%2520fewer%2520FLOPs.%2520With%2520RGB%252C%2520we%2520achieve%2520new%250Astate-of-the-art%2520performance%2520on%2520Assembly101%2520and%2520H2O%2520with%2520significant%250Aimprovements%2520in%2520egocentric%2520action%2520recognition.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09805v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=On%20the%20Utility%20of%203D%20Hand%20Poses%20for%20Action%20Recognition&entry.906535625=Md%20Salman%20Shamil%20and%20Dibyadip%20Chatterjee%20and%20Fadime%20Sener%20and%20Shugao%20Ma%20and%20Angela%20Yao&entry.1292438233=%20%203D%20hand%20pose%20is%20an%20underexplored%20modality%20for%20action%20recognition.%20Poses%20are%0Acompact%20yet%20informative%20and%20can%20greatly%20benefit%20applications%20with%20limited%0Acompute%20budgets.%20However%2C%20poses%20alone%20offer%20an%20incomplete%20understanding%20of%0Aactions%2C%20as%20they%20cannot%20fully%20capture%20objects%20and%20environments%20with%20which%0Ahumans%20interact.%20We%20propose%20HandFormer%2C%20a%20novel%20multimodal%20transformer%2C%20to%0Aefficiently%20model%20hand-object%20interactions.%20HandFormer%20combines%203D%20hand%20poses%0Aat%20a%20high%20temporal%20resolution%20for%20fine-grained%20motion%20modeling%20with%20sparsely%0Asampled%20RGB%20frames%20for%20encoding%20scene%20semantics.%20Observing%20the%20unique%0Acharacteristics%20of%20hand%20poses%2C%20we%20temporally%20factorize%20hand%20modeling%20and%0Arepresent%20each%20joint%20by%20its%20short-term%20trajectories.%20This%20factorized%20pose%0Arepresentation%20combined%20with%20sparse%20RGB%20samples%20is%20remarkably%20efficient%20and%0Ahighly%20accurate.%20Unimodal%20HandFormer%20with%20only%20hand%20poses%20outperforms%20existing%0Askeleton-based%20methods%20at%205x%20fewer%20FLOPs.%20With%20RGB%2C%20we%20achieve%20new%0Astate-of-the-art%20performance%20on%20Assembly101%20and%20H2O%20with%20significant%0Aimprovements%20in%20egocentric%20action%20recognition.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09805v2&entry.124074799=Read"},
{"title": "Disentangle and denoise: Tackling context misalignment for video moment\n  retrieval", "author": "Kaijing Ma and Han Fang and Xianghao Zang and Chao Ban and Lanxiang Zhou and Zhongjiang He and Yongxiang Li and Hao Sun and Zerun Feng and Xingsong Hou", "abstract": "  Video Moment Retrieval, which aims to locate in-context video moments\naccording to a natural language query, is an essential task for cross-modal\ngrounding. Existing methods focus on enhancing the cross-modal interactions\nbetween all moments and the textual description for video understanding.\nHowever, constantly interacting with all locations is unreasonable because of\nuneven semantic distribution across the timeline and noisy visual backgrounds.\nThis paper proposes a cross-modal Context Denoising Network (CDNet) for\naccurate moment retrieval by disentangling complex correlations and denoising\nirrelevant dynamics.Specifically, we propose a query-guided semantic\ndisentanglement (QSD) to decouple video moments by estimating alignment levels\naccording to the global and fine-grained correlation. A Context-aware Dynamic\nDenoisement (CDD) is proposed to enhance understanding of aligned\nspatial-temporal details by learning a group of query-relevant offsets.\nExtensive experiments on public benchmarks demonstrate that the proposed CDNet\nachieves state-of-the-art performances.\n", "link": "http://arxiv.org/abs/2408.07600v1", "date": "2024-08-14", "relevancy": 2.782, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5591}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5554}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5547}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Disentangle%20and%20denoise%3A%20Tackling%20context%20misalignment%20for%20video%20moment%0A%20%20retrieval&body=Title%3A%20Disentangle%20and%20denoise%3A%20Tackling%20context%20misalignment%20for%20video%20moment%0A%20%20retrieval%0AAuthor%3A%20Kaijing%20Ma%20and%20Han%20Fang%20and%20Xianghao%20Zang%20and%20Chao%20Ban%20and%20Lanxiang%20Zhou%20and%20Zhongjiang%20He%20and%20Yongxiang%20Li%20and%20Hao%20Sun%20and%20Zerun%20Feng%20and%20Xingsong%20Hou%0AAbstract%3A%20%20%20Video%20Moment%20Retrieval%2C%20which%20aims%20to%20locate%20in-context%20video%20moments%0Aaccording%20to%20a%20natural%20language%20query%2C%20is%20an%20essential%20task%20for%20cross-modal%0Agrounding.%20Existing%20methods%20focus%20on%20enhancing%20the%20cross-modal%20interactions%0Abetween%20all%20moments%20and%20the%20textual%20description%20for%20video%20understanding.%0AHowever%2C%20constantly%20interacting%20with%20all%20locations%20is%20unreasonable%20because%20of%0Auneven%20semantic%20distribution%20across%20the%20timeline%20and%20noisy%20visual%20backgrounds.%0AThis%20paper%20proposes%20a%20cross-modal%20Context%20Denoising%20Network%20%28CDNet%29%20for%0Aaccurate%20moment%20retrieval%20by%20disentangling%20complex%20correlations%20and%20denoising%0Airrelevant%20dynamics.Specifically%2C%20we%20propose%20a%20query-guided%20semantic%0Adisentanglement%20%28QSD%29%20to%20decouple%20video%20moments%20by%20estimating%20alignment%20levels%0Aaccording%20to%20the%20global%20and%20fine-grained%20correlation.%20A%20Context-aware%20Dynamic%0ADenoisement%20%28CDD%29%20is%20proposed%20to%20enhance%20understanding%20of%20aligned%0Aspatial-temporal%20details%20by%20learning%20a%20group%20of%20query-relevant%20offsets.%0AExtensive%20experiments%20on%20public%20benchmarks%20demonstrate%20that%20the%20proposed%20CDNet%0Aachieves%20state-of-the-art%20performances.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07600v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDisentangle%2520and%2520denoise%253A%2520Tackling%2520context%2520misalignment%2520for%2520video%2520moment%250A%2520%2520retrieval%26entry.906535625%3DKaijing%2520Ma%2520and%2520Han%2520Fang%2520and%2520Xianghao%2520Zang%2520and%2520Chao%2520Ban%2520and%2520Lanxiang%2520Zhou%2520and%2520Zhongjiang%2520He%2520and%2520Yongxiang%2520Li%2520and%2520Hao%2520Sun%2520and%2520Zerun%2520Feng%2520and%2520Xingsong%2520Hou%26entry.1292438233%3D%2520%2520Video%2520Moment%2520Retrieval%252C%2520which%2520aims%2520to%2520locate%2520in-context%2520video%2520moments%250Aaccording%2520to%2520a%2520natural%2520language%2520query%252C%2520is%2520an%2520essential%2520task%2520for%2520cross-modal%250Agrounding.%2520Existing%2520methods%2520focus%2520on%2520enhancing%2520the%2520cross-modal%2520interactions%250Abetween%2520all%2520moments%2520and%2520the%2520textual%2520description%2520for%2520video%2520understanding.%250AHowever%252C%2520constantly%2520interacting%2520with%2520all%2520locations%2520is%2520unreasonable%2520because%2520of%250Auneven%2520semantic%2520distribution%2520across%2520the%2520timeline%2520and%2520noisy%2520visual%2520backgrounds.%250AThis%2520paper%2520proposes%2520a%2520cross-modal%2520Context%2520Denoising%2520Network%2520%2528CDNet%2529%2520for%250Aaccurate%2520moment%2520retrieval%2520by%2520disentangling%2520complex%2520correlations%2520and%2520denoising%250Airrelevant%2520dynamics.Specifically%252C%2520we%2520propose%2520a%2520query-guided%2520semantic%250Adisentanglement%2520%2528QSD%2529%2520to%2520decouple%2520video%2520moments%2520by%2520estimating%2520alignment%2520levels%250Aaccording%2520to%2520the%2520global%2520and%2520fine-grained%2520correlation.%2520A%2520Context-aware%2520Dynamic%250ADenoisement%2520%2528CDD%2529%2520is%2520proposed%2520to%2520enhance%2520understanding%2520of%2520aligned%250Aspatial-temporal%2520details%2520by%2520learning%2520a%2520group%2520of%2520query-relevant%2520offsets.%250AExtensive%2520experiments%2520on%2520public%2520benchmarks%2520demonstrate%2520that%2520the%2520proposed%2520CDNet%250Aachieves%2520state-of-the-art%2520performances.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07600v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Disentangle%20and%20denoise%3A%20Tackling%20context%20misalignment%20for%20video%20moment%0A%20%20retrieval&entry.906535625=Kaijing%20Ma%20and%20Han%20Fang%20and%20Xianghao%20Zang%20and%20Chao%20Ban%20and%20Lanxiang%20Zhou%20and%20Zhongjiang%20He%20and%20Yongxiang%20Li%20and%20Hao%20Sun%20and%20Zerun%20Feng%20and%20Xingsong%20Hou&entry.1292438233=%20%20Video%20Moment%20Retrieval%2C%20which%20aims%20to%20locate%20in-context%20video%20moments%0Aaccording%20to%20a%20natural%20language%20query%2C%20is%20an%20essential%20task%20for%20cross-modal%0Agrounding.%20Existing%20methods%20focus%20on%20enhancing%20the%20cross-modal%20interactions%0Abetween%20all%20moments%20and%20the%20textual%20description%20for%20video%20understanding.%0AHowever%2C%20constantly%20interacting%20with%20all%20locations%20is%20unreasonable%20because%20of%0Auneven%20semantic%20distribution%20across%20the%20timeline%20and%20noisy%20visual%20backgrounds.%0AThis%20paper%20proposes%20a%20cross-modal%20Context%20Denoising%20Network%20%28CDNet%29%20for%0Aaccurate%20moment%20retrieval%20by%20disentangling%20complex%20correlations%20and%20denoising%0Airrelevant%20dynamics.Specifically%2C%20we%20propose%20a%20query-guided%20semantic%0Adisentanglement%20%28QSD%29%20to%20decouple%20video%20moments%20by%20estimating%20alignment%20levels%0Aaccording%20to%20the%20global%20and%20fine-grained%20correlation.%20A%20Context-aware%20Dynamic%0ADenoisement%20%28CDD%29%20is%20proposed%20to%20enhance%20understanding%20of%20aligned%0Aspatial-temporal%20details%20by%20learning%20a%20group%20of%20query-relevant%20offsets.%0AExtensive%20experiments%20on%20public%20benchmarks%20demonstrate%20that%20the%20proposed%20CDNet%0Aachieves%20state-of-the-art%20performances.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07600v1&entry.124074799=Read"},
{"title": "Improved 3D Whole Heart Geometry from Sparse CMR Slices", "author": "Yiyang Xu and Hao Xu and Matthew Sinclair and Esther Puyol-Ant\u00f3n and Steven A Niederer and Amedeo Chiribiri and Steven E Williams and Michelle C Williams and Alistair A Young", "abstract": "  Cardiac magnetic resonance (CMR) imaging and computed tomography (CT) are two\ncommon non-invasive imaging methods for assessing patients with cardiovascular\ndisease. CMR typically acquires multiple sparse 2D slices, with unavoidable\nrespiratory motion artefacts between slices, whereas CT acquires isotropic\ndense data but uses ionising radiation. In this study, we explore the\ncombination of Slice Shifting Algorithm (SSA), Spatial Transformer Network\n(STN), and Label Transformer Network (LTN) to: 1) correct respiratory motion\nbetween segmented slices, and 2) transform sparse segmentation data into dense\nsegmentation. All combinations were validated using synthetic motion-corrupted\nCMR slice segmentation generated from CT in 1699 cases, where the dense CT\nserves as the ground truth. In 199 testing cases, SSA-LTN achieved the best\nresults for Dice score and Huasdorff distance (94.0% and 4.7 mm respectively,\naverage over 5 labels) but gave topological errors in 8 cases. STN was\neffective as a plug-in tool for correcting all topological errors with minimal\nimpact on overall performance (93.5% and 5.0 mm respectively). SSA also proves\nto be a valuable plug-in tool, enhancing performance over both STN-based and\nLTN-based models. The code for these different combinations is available at\nhttps://github.com/XESchong/STACOM2024.\n", "link": "http://arxiv.org/abs/2408.07532v1", "date": "2024-08-14", "relevancy": 2.7717, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5677}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5476}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5476}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Improved%203D%20Whole%20Heart%20Geometry%20from%20Sparse%20CMR%20Slices&body=Title%3A%20Improved%203D%20Whole%20Heart%20Geometry%20from%20Sparse%20CMR%20Slices%0AAuthor%3A%20Yiyang%20Xu%20and%20Hao%20Xu%20and%20Matthew%20Sinclair%20and%20Esther%20Puyol-Ant%C3%B3n%20and%20Steven%20A%20Niederer%20and%20Amedeo%20Chiribiri%20and%20Steven%20E%20Williams%20and%20Michelle%20C%20Williams%20and%20Alistair%20A%20Young%0AAbstract%3A%20%20%20Cardiac%20magnetic%20resonance%20%28CMR%29%20imaging%20and%20computed%20tomography%20%28CT%29%20are%20two%0Acommon%20non-invasive%20imaging%20methods%20for%20assessing%20patients%20with%20cardiovascular%0Adisease.%20CMR%20typically%20acquires%20multiple%20sparse%202D%20slices%2C%20with%20unavoidable%0Arespiratory%20motion%20artefacts%20between%20slices%2C%20whereas%20CT%20acquires%20isotropic%0Adense%20data%20but%20uses%20ionising%20radiation.%20In%20this%20study%2C%20we%20explore%20the%0Acombination%20of%20Slice%20Shifting%20Algorithm%20%28SSA%29%2C%20Spatial%20Transformer%20Network%0A%28STN%29%2C%20and%20Label%20Transformer%20Network%20%28LTN%29%20to%3A%201%29%20correct%20respiratory%20motion%0Abetween%20segmented%20slices%2C%20and%202%29%20transform%20sparse%20segmentation%20data%20into%20dense%0Asegmentation.%20All%20combinations%20were%20validated%20using%20synthetic%20motion-corrupted%0ACMR%20slice%20segmentation%20generated%20from%20CT%20in%201699%20cases%2C%20where%20the%20dense%20CT%0Aserves%20as%20the%20ground%20truth.%20In%20199%20testing%20cases%2C%20SSA-LTN%20achieved%20the%20best%0Aresults%20for%20Dice%20score%20and%20Huasdorff%20distance%20%2894.0%25%20and%204.7%20mm%20respectively%2C%0Aaverage%20over%205%20labels%29%20but%20gave%20topological%20errors%20in%208%20cases.%20STN%20was%0Aeffective%20as%20a%20plug-in%20tool%20for%20correcting%20all%20topological%20errors%20with%20minimal%0Aimpact%20on%20overall%20performance%20%2893.5%25%20and%205.0%20mm%20respectively%29.%20SSA%20also%20proves%0Ato%20be%20a%20valuable%20plug-in%20tool%2C%20enhancing%20performance%20over%20both%20STN-based%20and%0ALTN-based%20models.%20The%20code%20for%20these%20different%20combinations%20is%20available%20at%0Ahttps%3A//github.com/XESchong/STACOM2024.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07532v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImproved%25203D%2520Whole%2520Heart%2520Geometry%2520from%2520Sparse%2520CMR%2520Slices%26entry.906535625%3DYiyang%2520Xu%2520and%2520Hao%2520Xu%2520and%2520Matthew%2520Sinclair%2520and%2520Esther%2520Puyol-Ant%25C3%25B3n%2520and%2520Steven%2520A%2520Niederer%2520and%2520Amedeo%2520Chiribiri%2520and%2520Steven%2520E%2520Williams%2520and%2520Michelle%2520C%2520Williams%2520and%2520Alistair%2520A%2520Young%26entry.1292438233%3D%2520%2520Cardiac%2520magnetic%2520resonance%2520%2528CMR%2529%2520imaging%2520and%2520computed%2520tomography%2520%2528CT%2529%2520are%2520two%250Acommon%2520non-invasive%2520imaging%2520methods%2520for%2520assessing%2520patients%2520with%2520cardiovascular%250Adisease.%2520CMR%2520typically%2520acquires%2520multiple%2520sparse%25202D%2520slices%252C%2520with%2520unavoidable%250Arespiratory%2520motion%2520artefacts%2520between%2520slices%252C%2520whereas%2520CT%2520acquires%2520isotropic%250Adense%2520data%2520but%2520uses%2520ionising%2520radiation.%2520In%2520this%2520study%252C%2520we%2520explore%2520the%250Acombination%2520of%2520Slice%2520Shifting%2520Algorithm%2520%2528SSA%2529%252C%2520Spatial%2520Transformer%2520Network%250A%2528STN%2529%252C%2520and%2520Label%2520Transformer%2520Network%2520%2528LTN%2529%2520to%253A%25201%2529%2520correct%2520respiratory%2520motion%250Abetween%2520segmented%2520slices%252C%2520and%25202%2529%2520transform%2520sparse%2520segmentation%2520data%2520into%2520dense%250Asegmentation.%2520All%2520combinations%2520were%2520validated%2520using%2520synthetic%2520motion-corrupted%250ACMR%2520slice%2520segmentation%2520generated%2520from%2520CT%2520in%25201699%2520cases%252C%2520where%2520the%2520dense%2520CT%250Aserves%2520as%2520the%2520ground%2520truth.%2520In%2520199%2520testing%2520cases%252C%2520SSA-LTN%2520achieved%2520the%2520best%250Aresults%2520for%2520Dice%2520score%2520and%2520Huasdorff%2520distance%2520%252894.0%2525%2520and%25204.7%2520mm%2520respectively%252C%250Aaverage%2520over%25205%2520labels%2529%2520but%2520gave%2520topological%2520errors%2520in%25208%2520cases.%2520STN%2520was%250Aeffective%2520as%2520a%2520plug-in%2520tool%2520for%2520correcting%2520all%2520topological%2520errors%2520with%2520minimal%250Aimpact%2520on%2520overall%2520performance%2520%252893.5%2525%2520and%25205.0%2520mm%2520respectively%2529.%2520SSA%2520also%2520proves%250Ato%2520be%2520a%2520valuable%2520plug-in%2520tool%252C%2520enhancing%2520performance%2520over%2520both%2520STN-based%2520and%250ALTN-based%2520models.%2520The%2520code%2520for%2520these%2520different%2520combinations%2520is%2520available%2520at%250Ahttps%253A//github.com/XESchong/STACOM2024.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07532v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Improved%203D%20Whole%20Heart%20Geometry%20from%20Sparse%20CMR%20Slices&entry.906535625=Yiyang%20Xu%20and%20Hao%20Xu%20and%20Matthew%20Sinclair%20and%20Esther%20Puyol-Ant%C3%B3n%20and%20Steven%20A%20Niederer%20and%20Amedeo%20Chiribiri%20and%20Steven%20E%20Williams%20and%20Michelle%20C%20Williams%20and%20Alistair%20A%20Young&entry.1292438233=%20%20Cardiac%20magnetic%20resonance%20%28CMR%29%20imaging%20and%20computed%20tomography%20%28CT%29%20are%20two%0Acommon%20non-invasive%20imaging%20methods%20for%20assessing%20patients%20with%20cardiovascular%0Adisease.%20CMR%20typically%20acquires%20multiple%20sparse%202D%20slices%2C%20with%20unavoidable%0Arespiratory%20motion%20artefacts%20between%20slices%2C%20whereas%20CT%20acquires%20isotropic%0Adense%20data%20but%20uses%20ionising%20radiation.%20In%20this%20study%2C%20we%20explore%20the%0Acombination%20of%20Slice%20Shifting%20Algorithm%20%28SSA%29%2C%20Spatial%20Transformer%20Network%0A%28STN%29%2C%20and%20Label%20Transformer%20Network%20%28LTN%29%20to%3A%201%29%20correct%20respiratory%20motion%0Abetween%20segmented%20slices%2C%20and%202%29%20transform%20sparse%20segmentation%20data%20into%20dense%0Asegmentation.%20All%20combinations%20were%20validated%20using%20synthetic%20motion-corrupted%0ACMR%20slice%20segmentation%20generated%20from%20CT%20in%201699%20cases%2C%20where%20the%20dense%20CT%0Aserves%20as%20the%20ground%20truth.%20In%20199%20testing%20cases%2C%20SSA-LTN%20achieved%20the%20best%0Aresults%20for%20Dice%20score%20and%20Huasdorff%20distance%20%2894.0%25%20and%204.7%20mm%20respectively%2C%0Aaverage%20over%205%20labels%29%20but%20gave%20topological%20errors%20in%208%20cases.%20STN%20was%0Aeffective%20as%20a%20plug-in%20tool%20for%20correcting%20all%20topological%20errors%20with%20minimal%0Aimpact%20on%20overall%20performance%20%2893.5%25%20and%205.0%20mm%20respectively%29.%20SSA%20also%20proves%0Ato%20be%20a%20valuable%20plug-in%20tool%2C%20enhancing%20performance%20over%20both%20STN-based%20and%0ALTN-based%20models.%20The%20code%20for%20these%20different%20combinations%20is%20available%20at%0Ahttps%3A//github.com/XESchong/STACOM2024.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07532v1&entry.124074799=Read"},
{"title": "Decoder ensembling for learned latent geometries", "author": "Stas Syrota and Pablo Moreno-Mu\u00f1oz and S\u00f8ren Hauberg", "abstract": "  Latent space geometry provides a rigorous and empirically valuable framework\nfor interacting with the latent variables of deep generative models. This\napproach reinterprets Euclidean latent spaces as Riemannian through a pull-back\nmetric, allowing for a standard differential geometric analysis of the latent\nspace. Unfortunately, data manifolds are generally compact and easily\ndisconnected or filled with holes, suggesting a topological mismatch to the\nEuclidean latent space. The most established solution to this mismatch is to\nlet uncertainty be a proxy for topology, but in neural network models, this is\noften realized through crude heuristics that lack principle and generally do\nnot scale to high-dimensional representations. We propose using ensembles of\ndecoders to capture model uncertainty and show how to easily compute geodesics\non the associated expected manifold. Empirically, we find this simple and\nreliable, thereby coming one step closer to easy-to-use latent geometries.\n", "link": "http://arxiv.org/abs/2408.07507v1", "date": "2024-08-14", "relevancy": 2.7231, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5552}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5425}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5361}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Decoder%20ensembling%20for%20learned%20latent%20geometries&body=Title%3A%20Decoder%20ensembling%20for%20learned%20latent%20geometries%0AAuthor%3A%20Stas%20Syrota%20and%20Pablo%20Moreno-Mu%C3%B1oz%20and%20S%C3%B8ren%20Hauberg%0AAbstract%3A%20%20%20Latent%20space%20geometry%20provides%20a%20rigorous%20and%20empirically%20valuable%20framework%0Afor%20interacting%20with%20the%20latent%20variables%20of%20deep%20generative%20models.%20This%0Aapproach%20reinterprets%20Euclidean%20latent%20spaces%20as%20Riemannian%20through%20a%20pull-back%0Ametric%2C%20allowing%20for%20a%20standard%20differential%20geometric%20analysis%20of%20the%20latent%0Aspace.%20Unfortunately%2C%20data%20manifolds%20are%20generally%20compact%20and%20easily%0Adisconnected%20or%20filled%20with%20holes%2C%20suggesting%20a%20topological%20mismatch%20to%20the%0AEuclidean%20latent%20space.%20The%20most%20established%20solution%20to%20this%20mismatch%20is%20to%0Alet%20uncertainty%20be%20a%20proxy%20for%20topology%2C%20but%20in%20neural%20network%20models%2C%20this%20is%0Aoften%20realized%20through%20crude%20heuristics%20that%20lack%20principle%20and%20generally%20do%0Anot%20scale%20to%20high-dimensional%20representations.%20We%20propose%20using%20ensembles%20of%0Adecoders%20to%20capture%20model%20uncertainty%20and%20show%20how%20to%20easily%20compute%20geodesics%0Aon%20the%20associated%20expected%20manifold.%20Empirically%2C%20we%20find%20this%20simple%20and%0Areliable%2C%20thereby%20coming%20one%20step%20closer%20to%20easy-to-use%20latent%20geometries.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07507v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDecoder%2520ensembling%2520for%2520learned%2520latent%2520geometries%26entry.906535625%3DStas%2520Syrota%2520and%2520Pablo%2520Moreno-Mu%25C3%25B1oz%2520and%2520S%25C3%25B8ren%2520Hauberg%26entry.1292438233%3D%2520%2520Latent%2520space%2520geometry%2520provides%2520a%2520rigorous%2520and%2520empirically%2520valuable%2520framework%250Afor%2520interacting%2520with%2520the%2520latent%2520variables%2520of%2520deep%2520generative%2520models.%2520This%250Aapproach%2520reinterprets%2520Euclidean%2520latent%2520spaces%2520as%2520Riemannian%2520through%2520a%2520pull-back%250Ametric%252C%2520allowing%2520for%2520a%2520standard%2520differential%2520geometric%2520analysis%2520of%2520the%2520latent%250Aspace.%2520Unfortunately%252C%2520data%2520manifolds%2520are%2520generally%2520compact%2520and%2520easily%250Adisconnected%2520or%2520filled%2520with%2520holes%252C%2520suggesting%2520a%2520topological%2520mismatch%2520to%2520the%250AEuclidean%2520latent%2520space.%2520The%2520most%2520established%2520solution%2520to%2520this%2520mismatch%2520is%2520to%250Alet%2520uncertainty%2520be%2520a%2520proxy%2520for%2520topology%252C%2520but%2520in%2520neural%2520network%2520models%252C%2520this%2520is%250Aoften%2520realized%2520through%2520crude%2520heuristics%2520that%2520lack%2520principle%2520and%2520generally%2520do%250Anot%2520scale%2520to%2520high-dimensional%2520representations.%2520We%2520propose%2520using%2520ensembles%2520of%250Adecoders%2520to%2520capture%2520model%2520uncertainty%2520and%2520show%2520how%2520to%2520easily%2520compute%2520geodesics%250Aon%2520the%2520associated%2520expected%2520manifold.%2520Empirically%252C%2520we%2520find%2520this%2520simple%2520and%250Areliable%252C%2520thereby%2520coming%2520one%2520step%2520closer%2520to%2520easy-to-use%2520latent%2520geometries.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07507v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Decoder%20ensembling%20for%20learned%20latent%20geometries&entry.906535625=Stas%20Syrota%20and%20Pablo%20Moreno-Mu%C3%B1oz%20and%20S%C3%B8ren%20Hauberg&entry.1292438233=%20%20Latent%20space%20geometry%20provides%20a%20rigorous%20and%20empirically%20valuable%20framework%0Afor%20interacting%20with%20the%20latent%20variables%20of%20deep%20generative%20models.%20This%0Aapproach%20reinterprets%20Euclidean%20latent%20spaces%20as%20Riemannian%20through%20a%20pull-back%0Ametric%2C%20allowing%20for%20a%20standard%20differential%20geometric%20analysis%20of%20the%20latent%0Aspace.%20Unfortunately%2C%20data%20manifolds%20are%20generally%20compact%20and%20easily%0Adisconnected%20or%20filled%20with%20holes%2C%20suggesting%20a%20topological%20mismatch%20to%20the%0AEuclidean%20latent%20space.%20The%20most%20established%20solution%20to%20this%20mismatch%20is%20to%0Alet%20uncertainty%20be%20a%20proxy%20for%20topology%2C%20but%20in%20neural%20network%20models%2C%20this%20is%0Aoften%20realized%20through%20crude%20heuristics%20that%20lack%20principle%20and%20generally%20do%0Anot%20scale%20to%20high-dimensional%20representations.%20We%20propose%20using%20ensembles%20of%0Adecoders%20to%20capture%20model%20uncertainty%20and%20show%20how%20to%20easily%20compute%20geodesics%0Aon%20the%20associated%20expected%20manifold.%20Empirically%2C%20we%20find%20this%20simple%20and%0Areliable%2C%20thereby%20coming%20one%20step%20closer%20to%20easy-to-use%20latent%20geometries.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07507v1&entry.124074799=Read"},
{"title": "NIGHT -- Non-Line-of-Sight Imaging from Indirect Time of Flight Data", "author": "Matteo Caligiuri and Adriano Simonetto and Pietro Zanuttigh", "abstract": "  The acquisition of objects outside the Line-of-Sight of cameras is a very\nintriguing but also extremely challenging research topic. Recent works showed\nthe feasibility of this idea exploiting transient imaging data produced by\ncustom direct Time of Flight sensors. In this paper, for the first time, we\ntackle this problem using only data from an off-the-shelf indirect Time of\nFlight sensor without any further hardware requirement. We introduced a Deep\nLearning model able to re-frame the surfaces where light bounces happen as a\nvirtual mirror. This modeling makes the task easier to handle and also\nfacilitates the construction of annotated training data. From the obtained data\nit is possible to retrieve the depth information of the hidden scene. We also\nprovide a first-in-its-kind synthetic dataset for the task and demonstrate the\nfeasibility of the proposed idea over it.\n", "link": "http://arxiv.org/abs/2403.19376v2", "date": "2024-08-14", "relevancy": 2.7131, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5452}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5452}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5375}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20NIGHT%20--%20Non-Line-of-Sight%20Imaging%20from%20Indirect%20Time%20of%20Flight%20Data&body=Title%3A%20NIGHT%20--%20Non-Line-of-Sight%20Imaging%20from%20Indirect%20Time%20of%20Flight%20Data%0AAuthor%3A%20Matteo%20Caligiuri%20and%20Adriano%20Simonetto%20and%20Pietro%20Zanuttigh%0AAbstract%3A%20%20%20The%20acquisition%20of%20objects%20outside%20the%20Line-of-Sight%20of%20cameras%20is%20a%20very%0Aintriguing%20but%20also%20extremely%20challenging%20research%20topic.%20Recent%20works%20showed%0Athe%20feasibility%20of%20this%20idea%20exploiting%20transient%20imaging%20data%20produced%20by%0Acustom%20direct%20Time%20of%20Flight%20sensors.%20In%20this%20paper%2C%20for%20the%20first%20time%2C%20we%0Atackle%20this%20problem%20using%20only%20data%20from%20an%20off-the-shelf%20indirect%20Time%20of%0AFlight%20sensor%20without%20any%20further%20hardware%20requirement.%20We%20introduced%20a%20Deep%0ALearning%20model%20able%20to%20re-frame%20the%20surfaces%20where%20light%20bounces%20happen%20as%20a%0Avirtual%20mirror.%20This%20modeling%20makes%20the%20task%20easier%20to%20handle%20and%20also%0Afacilitates%20the%20construction%20of%20annotated%20training%20data.%20From%20the%20obtained%20data%0Ait%20is%20possible%20to%20retrieve%20the%20depth%20information%20of%20the%20hidden%20scene.%20We%20also%0Aprovide%20a%20first-in-its-kind%20synthetic%20dataset%20for%20the%20task%20and%20demonstrate%20the%0Afeasibility%20of%20the%20proposed%20idea%20over%20it.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.19376v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNIGHT%2520--%2520Non-Line-of-Sight%2520Imaging%2520from%2520Indirect%2520Time%2520of%2520Flight%2520Data%26entry.906535625%3DMatteo%2520Caligiuri%2520and%2520Adriano%2520Simonetto%2520and%2520Pietro%2520Zanuttigh%26entry.1292438233%3D%2520%2520The%2520acquisition%2520of%2520objects%2520outside%2520the%2520Line-of-Sight%2520of%2520cameras%2520is%2520a%2520very%250Aintriguing%2520but%2520also%2520extremely%2520challenging%2520research%2520topic.%2520Recent%2520works%2520showed%250Athe%2520feasibility%2520of%2520this%2520idea%2520exploiting%2520transient%2520imaging%2520data%2520produced%2520by%250Acustom%2520direct%2520Time%2520of%2520Flight%2520sensors.%2520In%2520this%2520paper%252C%2520for%2520the%2520first%2520time%252C%2520we%250Atackle%2520this%2520problem%2520using%2520only%2520data%2520from%2520an%2520off-the-shelf%2520indirect%2520Time%2520of%250AFlight%2520sensor%2520without%2520any%2520further%2520hardware%2520requirement.%2520We%2520introduced%2520a%2520Deep%250ALearning%2520model%2520able%2520to%2520re-frame%2520the%2520surfaces%2520where%2520light%2520bounces%2520happen%2520as%2520a%250Avirtual%2520mirror.%2520This%2520modeling%2520makes%2520the%2520task%2520easier%2520to%2520handle%2520and%2520also%250Afacilitates%2520the%2520construction%2520of%2520annotated%2520training%2520data.%2520From%2520the%2520obtained%2520data%250Ait%2520is%2520possible%2520to%2520retrieve%2520the%2520depth%2520information%2520of%2520the%2520hidden%2520scene.%2520We%2520also%250Aprovide%2520a%2520first-in-its-kind%2520synthetic%2520dataset%2520for%2520the%2520task%2520and%2520demonstrate%2520the%250Afeasibility%2520of%2520the%2520proposed%2520idea%2520over%2520it.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.19376v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NIGHT%20--%20Non-Line-of-Sight%20Imaging%20from%20Indirect%20Time%20of%20Flight%20Data&entry.906535625=Matteo%20Caligiuri%20and%20Adriano%20Simonetto%20and%20Pietro%20Zanuttigh&entry.1292438233=%20%20The%20acquisition%20of%20objects%20outside%20the%20Line-of-Sight%20of%20cameras%20is%20a%20very%0Aintriguing%20but%20also%20extremely%20challenging%20research%20topic.%20Recent%20works%20showed%0Athe%20feasibility%20of%20this%20idea%20exploiting%20transient%20imaging%20data%20produced%20by%0Acustom%20direct%20Time%20of%20Flight%20sensors.%20In%20this%20paper%2C%20for%20the%20first%20time%2C%20we%0Atackle%20this%20problem%20using%20only%20data%20from%20an%20off-the-shelf%20indirect%20Time%20of%0AFlight%20sensor%20without%20any%20further%20hardware%20requirement.%20We%20introduced%20a%20Deep%0ALearning%20model%20able%20to%20re-frame%20the%20surfaces%20where%20light%20bounces%20happen%20as%20a%0Avirtual%20mirror.%20This%20modeling%20makes%20the%20task%20easier%20to%20handle%20and%20also%0Afacilitates%20the%20construction%20of%20annotated%20training%20data.%20From%20the%20obtained%20data%0Ait%20is%20possible%20to%20retrieve%20the%20depth%20information%20of%20the%20hidden%20scene.%20We%20also%0Aprovide%20a%20first-in-its-kind%20synthetic%20dataset%20for%20the%20task%20and%20demonstrate%20the%0Afeasibility%20of%20the%20proposed%20idea%20over%20it.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.19376v2&entry.124074799=Read"},
{"title": "Boosting Unconstrained Face Recognition with Targeted Style Adversary", "author": "Mohammad Saeed Ebrahimi Saadabadi and Sahar Rahimi Malakshan and Seyed Rasoul Hosseini and Nasser M. Nasrabadi", "abstract": "  While deep face recognition models have demonstrated remarkable performance,\nthey often struggle on the inputs from domains beyond their training data.\nRecent attempts aim to expand the training set by relying on computationally\nexpensive and inherently challenging image-space augmentation of image\ngeneration modules. In an orthogonal direction, we present a simple yet\neffective method to expand the training data by interpolating between\ninstance-level feature statistics across labeled and unlabeled sets. Our\nmethod, dubbed Targeted Style Adversary (TSA), is motivated by two\nobservations: (i) the input domain is reflected in feature statistics, and (ii)\nface recognition model performance is influenced by style information. Shifting\ntowards an unlabeled style implicitly synthesizes challenging training\ninstances. We devise a recognizability metric to constraint our framework to\npreserve the inherent identity-related information of labeled instances. The\nefficacy of our method is demonstrated through evaluations on unconstrained\nbenchmarks, outperforming or being on par with its competitors while offering\nnearly a 70\\% improvement in training speed and 40\\% less memory consumption.\n", "link": "http://arxiv.org/abs/2408.07642v1", "date": "2024-08-14", "relevancy": 2.6554, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5468}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5345}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5119}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Boosting%20Unconstrained%20Face%20Recognition%20with%20Targeted%20Style%20Adversary&body=Title%3A%20Boosting%20Unconstrained%20Face%20Recognition%20with%20Targeted%20Style%20Adversary%0AAuthor%3A%20Mohammad%20Saeed%20Ebrahimi%20Saadabadi%20and%20Sahar%20Rahimi%20Malakshan%20and%20Seyed%20Rasoul%20Hosseini%20and%20Nasser%20M.%20Nasrabadi%0AAbstract%3A%20%20%20While%20deep%20face%20recognition%20models%20have%20demonstrated%20remarkable%20performance%2C%0Athey%20often%20struggle%20on%20the%20inputs%20from%20domains%20beyond%20their%20training%20data.%0ARecent%20attempts%20aim%20to%20expand%20the%20training%20set%20by%20relying%20on%20computationally%0Aexpensive%20and%20inherently%20challenging%20image-space%20augmentation%20of%20image%0Ageneration%20modules.%20In%20an%20orthogonal%20direction%2C%20we%20present%20a%20simple%20yet%0Aeffective%20method%20to%20expand%20the%20training%20data%20by%20interpolating%20between%0Ainstance-level%20feature%20statistics%20across%20labeled%20and%20unlabeled%20sets.%20Our%0Amethod%2C%20dubbed%20Targeted%20Style%20Adversary%20%28TSA%29%2C%20is%20motivated%20by%20two%0Aobservations%3A%20%28i%29%20the%20input%20domain%20is%20reflected%20in%20feature%20statistics%2C%20and%20%28ii%29%0Aface%20recognition%20model%20performance%20is%20influenced%20by%20style%20information.%20Shifting%0Atowards%20an%20unlabeled%20style%20implicitly%20synthesizes%20challenging%20training%0Ainstances.%20We%20devise%20a%20recognizability%20metric%20to%20constraint%20our%20framework%20to%0Apreserve%20the%20inherent%20identity-related%20information%20of%20labeled%20instances.%20The%0Aefficacy%20of%20our%20method%20is%20demonstrated%20through%20evaluations%20on%20unconstrained%0Abenchmarks%2C%20outperforming%20or%20being%20on%20par%20with%20its%20competitors%20while%20offering%0Anearly%20a%2070%5C%25%20improvement%20in%20training%20speed%20and%2040%5C%25%20less%20memory%20consumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07642v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBoosting%2520Unconstrained%2520Face%2520Recognition%2520with%2520Targeted%2520Style%2520Adversary%26entry.906535625%3DMohammad%2520Saeed%2520Ebrahimi%2520Saadabadi%2520and%2520Sahar%2520Rahimi%2520Malakshan%2520and%2520Seyed%2520Rasoul%2520Hosseini%2520and%2520Nasser%2520M.%2520Nasrabadi%26entry.1292438233%3D%2520%2520While%2520deep%2520face%2520recognition%2520models%2520have%2520demonstrated%2520remarkable%2520performance%252C%250Athey%2520often%2520struggle%2520on%2520the%2520inputs%2520from%2520domains%2520beyond%2520their%2520training%2520data.%250ARecent%2520attempts%2520aim%2520to%2520expand%2520the%2520training%2520set%2520by%2520relying%2520on%2520computationally%250Aexpensive%2520and%2520inherently%2520challenging%2520image-space%2520augmentation%2520of%2520image%250Ageneration%2520modules.%2520In%2520an%2520orthogonal%2520direction%252C%2520we%2520present%2520a%2520simple%2520yet%250Aeffective%2520method%2520to%2520expand%2520the%2520training%2520data%2520by%2520interpolating%2520between%250Ainstance-level%2520feature%2520statistics%2520across%2520labeled%2520and%2520unlabeled%2520sets.%2520Our%250Amethod%252C%2520dubbed%2520Targeted%2520Style%2520Adversary%2520%2528TSA%2529%252C%2520is%2520motivated%2520by%2520two%250Aobservations%253A%2520%2528i%2529%2520the%2520input%2520domain%2520is%2520reflected%2520in%2520feature%2520statistics%252C%2520and%2520%2528ii%2529%250Aface%2520recognition%2520model%2520performance%2520is%2520influenced%2520by%2520style%2520information.%2520Shifting%250Atowards%2520an%2520unlabeled%2520style%2520implicitly%2520synthesizes%2520challenging%2520training%250Ainstances.%2520We%2520devise%2520a%2520recognizability%2520metric%2520to%2520constraint%2520our%2520framework%2520to%250Apreserve%2520the%2520inherent%2520identity-related%2520information%2520of%2520labeled%2520instances.%2520The%250Aefficacy%2520of%2520our%2520method%2520is%2520demonstrated%2520through%2520evaluations%2520on%2520unconstrained%250Abenchmarks%252C%2520outperforming%2520or%2520being%2520on%2520par%2520with%2520its%2520competitors%2520while%2520offering%250Anearly%2520a%252070%255C%2525%2520improvement%2520in%2520training%2520speed%2520and%252040%255C%2525%2520less%2520memory%2520consumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07642v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Boosting%20Unconstrained%20Face%20Recognition%20with%20Targeted%20Style%20Adversary&entry.906535625=Mohammad%20Saeed%20Ebrahimi%20Saadabadi%20and%20Sahar%20Rahimi%20Malakshan%20and%20Seyed%20Rasoul%20Hosseini%20and%20Nasser%20M.%20Nasrabadi&entry.1292438233=%20%20While%20deep%20face%20recognition%20models%20have%20demonstrated%20remarkable%20performance%2C%0Athey%20often%20struggle%20on%20the%20inputs%20from%20domains%20beyond%20their%20training%20data.%0ARecent%20attempts%20aim%20to%20expand%20the%20training%20set%20by%20relying%20on%20computationally%0Aexpensive%20and%20inherently%20challenging%20image-space%20augmentation%20of%20image%0Ageneration%20modules.%20In%20an%20orthogonal%20direction%2C%20we%20present%20a%20simple%20yet%0Aeffective%20method%20to%20expand%20the%20training%20data%20by%20interpolating%20between%0Ainstance-level%20feature%20statistics%20across%20labeled%20and%20unlabeled%20sets.%20Our%0Amethod%2C%20dubbed%20Targeted%20Style%20Adversary%20%28TSA%29%2C%20is%20motivated%20by%20two%0Aobservations%3A%20%28i%29%20the%20input%20domain%20is%20reflected%20in%20feature%20statistics%2C%20and%20%28ii%29%0Aface%20recognition%20model%20performance%20is%20influenced%20by%20style%20information.%20Shifting%0Atowards%20an%20unlabeled%20style%20implicitly%20synthesizes%20challenging%20training%0Ainstances.%20We%20devise%20a%20recognizability%20metric%20to%20constraint%20our%20framework%20to%0Apreserve%20the%20inherent%20identity-related%20information%20of%20labeled%20instances.%20The%0Aefficacy%20of%20our%20method%20is%20demonstrated%20through%20evaluations%20on%20unconstrained%0Abenchmarks%2C%20outperforming%20or%20being%20on%20par%20with%20its%20competitors%20while%20offering%0Anearly%20a%2070%5C%25%20improvement%20in%20training%20speed%20and%2040%5C%25%20less%20memory%20consumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07642v1&entry.124074799=Read"},
{"title": "See It All: Contextualized Late Aggregation for 3D Dense Captioning", "author": "Minjung Kim and Hyung Suk Lim and Seung Hwan Kim and Soonyoung Lee and Bumsoo Kim and Gunhee Kim", "abstract": "  3D dense captioning is a task to localize objects in a 3D scene and generate\ndescriptive sentences for each object. Recent approaches in 3D dense captioning\nhave adopted transformer encoder-decoder frameworks from object detection to\nbuild an end-to-end pipeline without hand-crafted components. However, these\napproaches struggle with contradicting objectives where a single query\nattention has to simultaneously view both the tightly localized object regions\nand contextual environment. To overcome this challenge, we introduce SIA\n(See-It-All), a transformer pipeline that engages in 3D dense captioning with a\nnovel paradigm called late aggregation. SIA simultaneously decodes two sets of\nqueries-context query and instance query. The instance query focuses on\nlocalization and object attribute descriptions, while the context query\nversatilely captures the region-of-interest of relationships between multiple\nobjects or with the global scene, then aggregated afterwards (i.e., late\naggregation) via simple distance-based measures. To further enhance the quality\nof contextualized caption generation, we design a novel aggregator to generate\na fully informed caption based on the surrounding context, the global\nenvironment, and object instances. Extensive experiments on two of the most\nwidely-used 3D dense captioning datasets demonstrate that our proposed method\nachieves a significant improvement over prior methods.\n", "link": "http://arxiv.org/abs/2408.07648v1", "date": "2024-08-14", "relevancy": 2.6369, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5326}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5248}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5248}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20See%20It%20All%3A%20Contextualized%20Late%20Aggregation%20for%203D%20Dense%20Captioning&body=Title%3A%20See%20It%20All%3A%20Contextualized%20Late%20Aggregation%20for%203D%20Dense%20Captioning%0AAuthor%3A%20Minjung%20Kim%20and%20Hyung%20Suk%20Lim%20and%20Seung%20Hwan%20Kim%20and%20Soonyoung%20Lee%20and%20Bumsoo%20Kim%20and%20Gunhee%20Kim%0AAbstract%3A%20%20%203D%20dense%20captioning%20is%20a%20task%20to%20localize%20objects%20in%20a%203D%20scene%20and%20generate%0Adescriptive%20sentences%20for%20each%20object.%20Recent%20approaches%20in%203D%20dense%20captioning%0Ahave%20adopted%20transformer%20encoder-decoder%20frameworks%20from%20object%20detection%20to%0Abuild%20an%20end-to-end%20pipeline%20without%20hand-crafted%20components.%20However%2C%20these%0Aapproaches%20struggle%20with%20contradicting%20objectives%20where%20a%20single%20query%0Aattention%20has%20to%20simultaneously%20view%20both%20the%20tightly%20localized%20object%20regions%0Aand%20contextual%20environment.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20SIA%0A%28See-It-All%29%2C%20a%20transformer%20pipeline%20that%20engages%20in%203D%20dense%20captioning%20with%20a%0Anovel%20paradigm%20called%20late%20aggregation.%20SIA%20simultaneously%20decodes%20two%20sets%20of%0Aqueries-context%20query%20and%20instance%20query.%20The%20instance%20query%20focuses%20on%0Alocalization%20and%20object%20attribute%20descriptions%2C%20while%20the%20context%20query%0Aversatilely%20captures%20the%20region-of-interest%20of%20relationships%20between%20multiple%0Aobjects%20or%20with%20the%20global%20scene%2C%20then%20aggregated%20afterwards%20%28i.e.%2C%20late%0Aaggregation%29%20via%20simple%20distance-based%20measures.%20To%20further%20enhance%20the%20quality%0Aof%20contextualized%20caption%20generation%2C%20we%20design%20a%20novel%20aggregator%20to%20generate%0Aa%20fully%20informed%20caption%20based%20on%20the%20surrounding%20context%2C%20the%20global%0Aenvironment%2C%20and%20object%20instances.%20Extensive%20experiments%20on%20two%20of%20the%20most%0Awidely-used%203D%20dense%20captioning%20datasets%20demonstrate%20that%20our%20proposed%20method%0Aachieves%20a%20significant%20improvement%20over%20prior%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07648v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSee%2520It%2520All%253A%2520Contextualized%2520Late%2520Aggregation%2520for%25203D%2520Dense%2520Captioning%26entry.906535625%3DMinjung%2520Kim%2520and%2520Hyung%2520Suk%2520Lim%2520and%2520Seung%2520Hwan%2520Kim%2520and%2520Soonyoung%2520Lee%2520and%2520Bumsoo%2520Kim%2520and%2520Gunhee%2520Kim%26entry.1292438233%3D%2520%25203D%2520dense%2520captioning%2520is%2520a%2520task%2520to%2520localize%2520objects%2520in%2520a%25203D%2520scene%2520and%2520generate%250Adescriptive%2520sentences%2520for%2520each%2520object.%2520Recent%2520approaches%2520in%25203D%2520dense%2520captioning%250Ahave%2520adopted%2520transformer%2520encoder-decoder%2520frameworks%2520from%2520object%2520detection%2520to%250Abuild%2520an%2520end-to-end%2520pipeline%2520without%2520hand-crafted%2520components.%2520However%252C%2520these%250Aapproaches%2520struggle%2520with%2520contradicting%2520objectives%2520where%2520a%2520single%2520query%250Aattention%2520has%2520to%2520simultaneously%2520view%2520both%2520the%2520tightly%2520localized%2520object%2520regions%250Aand%2520contextual%2520environment.%2520To%2520overcome%2520this%2520challenge%252C%2520we%2520introduce%2520SIA%250A%2528See-It-All%2529%252C%2520a%2520transformer%2520pipeline%2520that%2520engages%2520in%25203D%2520dense%2520captioning%2520with%2520a%250Anovel%2520paradigm%2520called%2520late%2520aggregation.%2520SIA%2520simultaneously%2520decodes%2520two%2520sets%2520of%250Aqueries-context%2520query%2520and%2520instance%2520query.%2520The%2520instance%2520query%2520focuses%2520on%250Alocalization%2520and%2520object%2520attribute%2520descriptions%252C%2520while%2520the%2520context%2520query%250Aversatilely%2520captures%2520the%2520region-of-interest%2520of%2520relationships%2520between%2520multiple%250Aobjects%2520or%2520with%2520the%2520global%2520scene%252C%2520then%2520aggregated%2520afterwards%2520%2528i.e.%252C%2520late%250Aaggregation%2529%2520via%2520simple%2520distance-based%2520measures.%2520To%2520further%2520enhance%2520the%2520quality%250Aof%2520contextualized%2520caption%2520generation%252C%2520we%2520design%2520a%2520novel%2520aggregator%2520to%2520generate%250Aa%2520fully%2520informed%2520caption%2520based%2520on%2520the%2520surrounding%2520context%252C%2520the%2520global%250Aenvironment%252C%2520and%2520object%2520instances.%2520Extensive%2520experiments%2520on%2520two%2520of%2520the%2520most%250Awidely-used%25203D%2520dense%2520captioning%2520datasets%2520demonstrate%2520that%2520our%2520proposed%2520method%250Aachieves%2520a%2520significant%2520improvement%2520over%2520prior%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07648v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=See%20It%20All%3A%20Contextualized%20Late%20Aggregation%20for%203D%20Dense%20Captioning&entry.906535625=Minjung%20Kim%20and%20Hyung%20Suk%20Lim%20and%20Seung%20Hwan%20Kim%20and%20Soonyoung%20Lee%20and%20Bumsoo%20Kim%20and%20Gunhee%20Kim&entry.1292438233=%20%203D%20dense%20captioning%20is%20a%20task%20to%20localize%20objects%20in%20a%203D%20scene%20and%20generate%0Adescriptive%20sentences%20for%20each%20object.%20Recent%20approaches%20in%203D%20dense%20captioning%0Ahave%20adopted%20transformer%20encoder-decoder%20frameworks%20from%20object%20detection%20to%0Abuild%20an%20end-to-end%20pipeline%20without%20hand-crafted%20components.%20However%2C%20these%0Aapproaches%20struggle%20with%20contradicting%20objectives%20where%20a%20single%20query%0Aattention%20has%20to%20simultaneously%20view%20both%20the%20tightly%20localized%20object%20regions%0Aand%20contextual%20environment.%20To%20overcome%20this%20challenge%2C%20we%20introduce%20SIA%0A%28See-It-All%29%2C%20a%20transformer%20pipeline%20that%20engages%20in%203D%20dense%20captioning%20with%20a%0Anovel%20paradigm%20called%20late%20aggregation.%20SIA%20simultaneously%20decodes%20two%20sets%20of%0Aqueries-context%20query%20and%20instance%20query.%20The%20instance%20query%20focuses%20on%0Alocalization%20and%20object%20attribute%20descriptions%2C%20while%20the%20context%20query%0Aversatilely%20captures%20the%20region-of-interest%20of%20relationships%20between%20multiple%0Aobjects%20or%20with%20the%20global%20scene%2C%20then%20aggregated%20afterwards%20%28i.e.%2C%20late%0Aaggregation%29%20via%20simple%20distance-based%20measures.%20To%20further%20enhance%20the%20quality%0Aof%20contextualized%20caption%20generation%2C%20we%20design%20a%20novel%20aggregator%20to%20generate%0Aa%20fully%20informed%20caption%20based%20on%20the%20surrounding%20context%2C%20the%20global%0Aenvironment%2C%20and%20object%20instances.%20Extensive%20experiments%20on%20two%20of%20the%20most%0Awidely-used%203D%20dense%20captioning%20datasets%20demonstrate%20that%20our%20proposed%20method%0Aachieves%20a%20significant%20improvement%20over%20prior%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07648v1&entry.124074799=Read"},
{"title": "Attention-Guided Perturbation for Unsupervised Image Anomaly Detection", "author": "Tingfeng Huang and Yuxuan Cheng and Jingbo Xia and Rui Yu and Yuxuan Cai and Jinhai Xiang and Xinwei He and Xiang Bai", "abstract": "  Reconstruction-based methods have significantly advanced modern unsupervised\nanomaly detection. However, the strong capacity of neural networks often\nviolates the underlying assumptions by reconstructing abnormal samples well. To\nalleviate this issue, we present a simple yet effective reconstruction\nframework named Attention-Guided Pertuation Network (AGPNet), which learns to\nadd perturbation noise with an attention mask, for accurate unsupervised\nanomaly detection. Specifically, it consists of two branches, \\ie, a plain\nreconstruction branch and an auxiliary attention-based perturbation branch. The\nreconstruction branch is simply a plain reconstruction network that learns to\nreconstruct normal samples, while the auxiliary branch aims to produce\nattention masks to guide the noise perturbation process for normal samples from\neasy to hard. By doing so, we are expecting to synthesize hard yet more\ninformative anomalies for training, which enable the reconstruction branch to\nlearn important inherent normal patterns both comprehensively and efficiently.\nExtensive experiments are conducted on three popular benchmarks covering\nMVTec-AD, VisA, and MVTec-3D, and show that our framework obtains leading\nanomaly detection performance under various setups including few-shot,\none-class, and multi-class setups.\n", "link": "http://arxiv.org/abs/2408.07490v1", "date": "2024-08-14", "relevancy": 2.6149, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5298}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5218}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5174}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Attention-Guided%20Perturbation%20for%20Unsupervised%20Image%20Anomaly%20Detection&body=Title%3A%20Attention-Guided%20Perturbation%20for%20Unsupervised%20Image%20Anomaly%20Detection%0AAuthor%3A%20Tingfeng%20Huang%20and%20Yuxuan%20Cheng%20and%20Jingbo%20Xia%20and%20Rui%20Yu%20and%20Yuxuan%20Cai%20and%20Jinhai%20Xiang%20and%20Xinwei%20He%20and%20Xiang%20Bai%0AAbstract%3A%20%20%20Reconstruction-based%20methods%20have%20significantly%20advanced%20modern%20unsupervised%0Aanomaly%20detection.%20However%2C%20the%20strong%20capacity%20of%20neural%20networks%20often%0Aviolates%20the%20underlying%20assumptions%20by%20reconstructing%20abnormal%20samples%20well.%20To%0Aalleviate%20this%20issue%2C%20we%20present%20a%20simple%20yet%20effective%20reconstruction%0Aframework%20named%20Attention-Guided%20Pertuation%20Network%20%28AGPNet%29%2C%20which%20learns%20to%0Aadd%20perturbation%20noise%20with%20an%20attention%20mask%2C%20for%20accurate%20unsupervised%0Aanomaly%20detection.%20Specifically%2C%20it%20consists%20of%20two%20branches%2C%20%5Cie%2C%20a%20plain%0Areconstruction%20branch%20and%20an%20auxiliary%20attention-based%20perturbation%20branch.%20The%0Areconstruction%20branch%20is%20simply%20a%20plain%20reconstruction%20network%20that%20learns%20to%0Areconstruct%20normal%20samples%2C%20while%20the%20auxiliary%20branch%20aims%20to%20produce%0Aattention%20masks%20to%20guide%20the%20noise%20perturbation%20process%20for%20normal%20samples%20from%0Aeasy%20to%20hard.%20By%20doing%20so%2C%20we%20are%20expecting%20to%20synthesize%20hard%20yet%20more%0Ainformative%20anomalies%20for%20training%2C%20which%20enable%20the%20reconstruction%20branch%20to%0Alearn%20important%20inherent%20normal%20patterns%20both%20comprehensively%20and%20efficiently.%0AExtensive%20experiments%20are%20conducted%20on%20three%20popular%20benchmarks%20covering%0AMVTec-AD%2C%20VisA%2C%20and%20MVTec-3D%2C%20and%20show%20that%20our%20framework%20obtains%20leading%0Aanomaly%20detection%20performance%20under%20various%20setups%20including%20few-shot%2C%0Aone-class%2C%20and%20multi-class%20setups.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07490v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAttention-Guided%2520Perturbation%2520for%2520Unsupervised%2520Image%2520Anomaly%2520Detection%26entry.906535625%3DTingfeng%2520Huang%2520and%2520Yuxuan%2520Cheng%2520and%2520Jingbo%2520Xia%2520and%2520Rui%2520Yu%2520and%2520Yuxuan%2520Cai%2520and%2520Jinhai%2520Xiang%2520and%2520Xinwei%2520He%2520and%2520Xiang%2520Bai%26entry.1292438233%3D%2520%2520Reconstruction-based%2520methods%2520have%2520significantly%2520advanced%2520modern%2520unsupervised%250Aanomaly%2520detection.%2520However%252C%2520the%2520strong%2520capacity%2520of%2520neural%2520networks%2520often%250Aviolates%2520the%2520underlying%2520assumptions%2520by%2520reconstructing%2520abnormal%2520samples%2520well.%2520To%250Aalleviate%2520this%2520issue%252C%2520we%2520present%2520a%2520simple%2520yet%2520effective%2520reconstruction%250Aframework%2520named%2520Attention-Guided%2520Pertuation%2520Network%2520%2528AGPNet%2529%252C%2520which%2520learns%2520to%250Aadd%2520perturbation%2520noise%2520with%2520an%2520attention%2520mask%252C%2520for%2520accurate%2520unsupervised%250Aanomaly%2520detection.%2520Specifically%252C%2520it%2520consists%2520of%2520two%2520branches%252C%2520%255Cie%252C%2520a%2520plain%250Areconstruction%2520branch%2520and%2520an%2520auxiliary%2520attention-based%2520perturbation%2520branch.%2520The%250Areconstruction%2520branch%2520is%2520simply%2520a%2520plain%2520reconstruction%2520network%2520that%2520learns%2520to%250Areconstruct%2520normal%2520samples%252C%2520while%2520the%2520auxiliary%2520branch%2520aims%2520to%2520produce%250Aattention%2520masks%2520to%2520guide%2520the%2520noise%2520perturbation%2520process%2520for%2520normal%2520samples%2520from%250Aeasy%2520to%2520hard.%2520By%2520doing%2520so%252C%2520we%2520are%2520expecting%2520to%2520synthesize%2520hard%2520yet%2520more%250Ainformative%2520anomalies%2520for%2520training%252C%2520which%2520enable%2520the%2520reconstruction%2520branch%2520to%250Alearn%2520important%2520inherent%2520normal%2520patterns%2520both%2520comprehensively%2520and%2520efficiently.%250AExtensive%2520experiments%2520are%2520conducted%2520on%2520three%2520popular%2520benchmarks%2520covering%250AMVTec-AD%252C%2520VisA%252C%2520and%2520MVTec-3D%252C%2520and%2520show%2520that%2520our%2520framework%2520obtains%2520leading%250Aanomaly%2520detection%2520performance%2520under%2520various%2520setups%2520including%2520few-shot%252C%250Aone-class%252C%2520and%2520multi-class%2520setups.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07490v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Attention-Guided%20Perturbation%20for%20Unsupervised%20Image%20Anomaly%20Detection&entry.906535625=Tingfeng%20Huang%20and%20Yuxuan%20Cheng%20and%20Jingbo%20Xia%20and%20Rui%20Yu%20and%20Yuxuan%20Cai%20and%20Jinhai%20Xiang%20and%20Xinwei%20He%20and%20Xiang%20Bai&entry.1292438233=%20%20Reconstruction-based%20methods%20have%20significantly%20advanced%20modern%20unsupervised%0Aanomaly%20detection.%20However%2C%20the%20strong%20capacity%20of%20neural%20networks%20often%0Aviolates%20the%20underlying%20assumptions%20by%20reconstructing%20abnormal%20samples%20well.%20To%0Aalleviate%20this%20issue%2C%20we%20present%20a%20simple%20yet%20effective%20reconstruction%0Aframework%20named%20Attention-Guided%20Pertuation%20Network%20%28AGPNet%29%2C%20which%20learns%20to%0Aadd%20perturbation%20noise%20with%20an%20attention%20mask%2C%20for%20accurate%20unsupervised%0Aanomaly%20detection.%20Specifically%2C%20it%20consists%20of%20two%20branches%2C%20%5Cie%2C%20a%20plain%0Areconstruction%20branch%20and%20an%20auxiliary%20attention-based%20perturbation%20branch.%20The%0Areconstruction%20branch%20is%20simply%20a%20plain%20reconstruction%20network%20that%20learns%20to%0Areconstruct%20normal%20samples%2C%20while%20the%20auxiliary%20branch%20aims%20to%20produce%0Aattention%20masks%20to%20guide%20the%20noise%20perturbation%20process%20for%20normal%20samples%20from%0Aeasy%20to%20hard.%20By%20doing%20so%2C%20we%20are%20expecting%20to%20synthesize%20hard%20yet%20more%0Ainformative%20anomalies%20for%20training%2C%20which%20enable%20the%20reconstruction%20branch%20to%0Alearn%20important%20inherent%20normal%20patterns%20both%20comprehensively%20and%20efficiently.%0AExtensive%20experiments%20are%20conducted%20on%20three%20popular%20benchmarks%20covering%0AMVTec-AD%2C%20VisA%2C%20and%20MVTec-3D%2C%20and%20show%20that%20our%20framework%20obtains%20leading%0Aanomaly%20detection%20performance%20under%20various%20setups%20including%20few-shot%2C%0Aone-class%2C%20and%20multi-class%20setups.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07490v1&entry.124074799=Read"},
{"title": "Graph Triple Attention Network: A Decoupled Perspective", "author": "Xiaotang Wang and Yun Zhu and Haizhou Shi and Yongchao Liu and Chuntao Hong", "abstract": "  Graph Transformers (GTs) have recently achieved significant success in the\ngraph domain by effectively capturing both long-range dependencies and graph\ninductive biases. However, these methods face two primary challenges: (1)\nmulti-view chaos, which results from coupling multi-view information\n(positional, structural, attribute), thereby impeding flexible usage and the\ninterpretability of the propagation process. (2) local-global chaos, which\narises from coupling local message passing with global attention, leading to\nissues of overfitting and over-globalizing. To address these challenges, we\npropose a high-level decoupled perspective of GTs, breaking them down into\nthree components and two interaction levels: positional attention, structural\nattention, and attribute attention, alongside local and global interaction.\nBased on this decoupled perspective, we design a decoupled graph triple\nattention network named DeGTA, which separately computes multi-view attentions\nand adaptively integrates multi-view local and global information. This\napproach offers three key advantages: enhanced interpretability, flexible\ndesign, and adaptive integration of local and global information. Through\nextensive experiments, DeGTA achieves state-of-the-art performance across\nvarious datasets and tasks, including node classification and graph\nclassification. Comprehensive ablation studies demonstrate that decoupling is\nessential for improving performance and enhancing interpretability. Our code is\navailable at: https://github.com/wangxiaotang0906/DeGTA\n", "link": "http://arxiv.org/abs/2408.07654v1", "date": "2024-08-14", "relevancy": 2.5855, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5232}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5201}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.508}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Graph%20Triple%20Attention%20Network%3A%20A%20Decoupled%20Perspective&body=Title%3A%20Graph%20Triple%20Attention%20Network%3A%20A%20Decoupled%20Perspective%0AAuthor%3A%20Xiaotang%20Wang%20and%20Yun%20Zhu%20and%20Haizhou%20Shi%20and%20Yongchao%20Liu%20and%20Chuntao%20Hong%0AAbstract%3A%20%20%20Graph%20Transformers%20%28GTs%29%20have%20recently%20achieved%20significant%20success%20in%20the%0Agraph%20domain%20by%20effectively%20capturing%20both%20long-range%20dependencies%20and%20graph%0Ainductive%20biases.%20However%2C%20these%20methods%20face%20two%20primary%20challenges%3A%20%281%29%0Amulti-view%20chaos%2C%20which%20results%20from%20coupling%20multi-view%20information%0A%28positional%2C%20structural%2C%20attribute%29%2C%20thereby%20impeding%20flexible%20usage%20and%20the%0Ainterpretability%20of%20the%20propagation%20process.%20%282%29%20local-global%20chaos%2C%20which%0Aarises%20from%20coupling%20local%20message%20passing%20with%20global%20attention%2C%20leading%20to%0Aissues%20of%20overfitting%20and%20over-globalizing.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20high-level%20decoupled%20perspective%20of%20GTs%2C%20breaking%20them%20down%20into%0Athree%20components%20and%20two%20interaction%20levels%3A%20positional%20attention%2C%20structural%0Aattention%2C%20and%20attribute%20attention%2C%20alongside%20local%20and%20global%20interaction.%0ABased%20on%20this%20decoupled%20perspective%2C%20we%20design%20a%20decoupled%20graph%20triple%0Aattention%20network%20named%20DeGTA%2C%20which%20separately%20computes%20multi-view%20attentions%0Aand%20adaptively%20integrates%20multi-view%20local%20and%20global%20information.%20This%0Aapproach%20offers%20three%20key%20advantages%3A%20enhanced%20interpretability%2C%20flexible%0Adesign%2C%20and%20adaptive%20integration%20of%20local%20and%20global%20information.%20Through%0Aextensive%20experiments%2C%20DeGTA%20achieves%20state-of-the-art%20performance%20across%0Avarious%20datasets%20and%20tasks%2C%20including%20node%20classification%20and%20graph%0Aclassification.%20Comprehensive%20ablation%20studies%20demonstrate%20that%20decoupling%20is%0Aessential%20for%20improving%20performance%20and%20enhancing%20interpretability.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/wangxiaotang0906/DeGTA%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07654v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGraph%2520Triple%2520Attention%2520Network%253A%2520A%2520Decoupled%2520Perspective%26entry.906535625%3DXiaotang%2520Wang%2520and%2520Yun%2520Zhu%2520and%2520Haizhou%2520Shi%2520and%2520Yongchao%2520Liu%2520and%2520Chuntao%2520Hong%26entry.1292438233%3D%2520%2520Graph%2520Transformers%2520%2528GTs%2529%2520have%2520recently%2520achieved%2520significant%2520success%2520in%2520the%250Agraph%2520domain%2520by%2520effectively%2520capturing%2520both%2520long-range%2520dependencies%2520and%2520graph%250Ainductive%2520biases.%2520However%252C%2520these%2520methods%2520face%2520two%2520primary%2520challenges%253A%2520%25281%2529%250Amulti-view%2520chaos%252C%2520which%2520results%2520from%2520coupling%2520multi-view%2520information%250A%2528positional%252C%2520structural%252C%2520attribute%2529%252C%2520thereby%2520impeding%2520flexible%2520usage%2520and%2520the%250Ainterpretability%2520of%2520the%2520propagation%2520process.%2520%25282%2529%2520local-global%2520chaos%252C%2520which%250Aarises%2520from%2520coupling%2520local%2520message%2520passing%2520with%2520global%2520attention%252C%2520leading%2520to%250Aissues%2520of%2520overfitting%2520and%2520over-globalizing.%2520To%2520address%2520these%2520challenges%252C%2520we%250Apropose%2520a%2520high-level%2520decoupled%2520perspective%2520of%2520GTs%252C%2520breaking%2520them%2520down%2520into%250Athree%2520components%2520and%2520two%2520interaction%2520levels%253A%2520positional%2520attention%252C%2520structural%250Aattention%252C%2520and%2520attribute%2520attention%252C%2520alongside%2520local%2520and%2520global%2520interaction.%250ABased%2520on%2520this%2520decoupled%2520perspective%252C%2520we%2520design%2520a%2520decoupled%2520graph%2520triple%250Aattention%2520network%2520named%2520DeGTA%252C%2520which%2520separately%2520computes%2520multi-view%2520attentions%250Aand%2520adaptively%2520integrates%2520multi-view%2520local%2520and%2520global%2520information.%2520This%250Aapproach%2520offers%2520three%2520key%2520advantages%253A%2520enhanced%2520interpretability%252C%2520flexible%250Adesign%252C%2520and%2520adaptive%2520integration%2520of%2520local%2520and%2520global%2520information.%2520Through%250Aextensive%2520experiments%252C%2520DeGTA%2520achieves%2520state-of-the-art%2520performance%2520across%250Avarious%2520datasets%2520and%2520tasks%252C%2520including%2520node%2520classification%2520and%2520graph%250Aclassification.%2520Comprehensive%2520ablation%2520studies%2520demonstrate%2520that%2520decoupling%2520is%250Aessential%2520for%2520improving%2520performance%2520and%2520enhancing%2520interpretability.%2520Our%2520code%2520is%250Aavailable%2520at%253A%2520https%253A//github.com/wangxiaotang0906/DeGTA%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07654v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Graph%20Triple%20Attention%20Network%3A%20A%20Decoupled%20Perspective&entry.906535625=Xiaotang%20Wang%20and%20Yun%20Zhu%20and%20Haizhou%20Shi%20and%20Yongchao%20Liu%20and%20Chuntao%20Hong&entry.1292438233=%20%20Graph%20Transformers%20%28GTs%29%20have%20recently%20achieved%20significant%20success%20in%20the%0Agraph%20domain%20by%20effectively%20capturing%20both%20long-range%20dependencies%20and%20graph%0Ainductive%20biases.%20However%2C%20these%20methods%20face%20two%20primary%20challenges%3A%20%281%29%0Amulti-view%20chaos%2C%20which%20results%20from%20coupling%20multi-view%20information%0A%28positional%2C%20structural%2C%20attribute%29%2C%20thereby%20impeding%20flexible%20usage%20and%20the%0Ainterpretability%20of%20the%20propagation%20process.%20%282%29%20local-global%20chaos%2C%20which%0Aarises%20from%20coupling%20local%20message%20passing%20with%20global%20attention%2C%20leading%20to%0Aissues%20of%20overfitting%20and%20over-globalizing.%20To%20address%20these%20challenges%2C%20we%0Apropose%20a%20high-level%20decoupled%20perspective%20of%20GTs%2C%20breaking%20them%20down%20into%0Athree%20components%20and%20two%20interaction%20levels%3A%20positional%20attention%2C%20structural%0Aattention%2C%20and%20attribute%20attention%2C%20alongside%20local%20and%20global%20interaction.%0ABased%20on%20this%20decoupled%20perspective%2C%20we%20design%20a%20decoupled%20graph%20triple%0Aattention%20network%20named%20DeGTA%2C%20which%20separately%20computes%20multi-view%20attentions%0Aand%20adaptively%20integrates%20multi-view%20local%20and%20global%20information.%20This%0Aapproach%20offers%20three%20key%20advantages%3A%20enhanced%20interpretability%2C%20flexible%0Adesign%2C%20and%20adaptive%20integration%20of%20local%20and%20global%20information.%20Through%0Aextensive%20experiments%2C%20DeGTA%20achieves%20state-of-the-art%20performance%20across%0Avarious%20datasets%20and%20tasks%2C%20including%20node%20classification%20and%20graph%0Aclassification.%20Comprehensive%20ablation%20studies%20demonstrate%20that%20decoupling%20is%0Aessential%20for%20improving%20performance%20and%20enhancing%20interpretability.%20Our%20code%20is%0Aavailable%20at%3A%20https%3A//github.com/wangxiaotang0906/DeGTA%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07654v1&entry.124074799=Read"},
{"title": "OMR: Occlusion-Aware Memory-Based Refinement for Video Lane Detection", "author": "Dongkwon Jin and Chang-Su Kim", "abstract": "  A novel algorithm for video lane detection is proposed in this paper. First,\nwe extract a feature map for a current frame and detect a latent mask for\nobstacles occluding lanes. Then, we enhance the feature map by developing an\nocclusion-aware memory-based refinement (OMR) module. It takes the obstacle\nmask and feature map from the current frame, previous output, and memory\ninformation as input, and processes them recursively in a video. Moreover, we\napply a novel data augmentation scheme for training the OMR module effectively.\nExperimental results show that the proposed algorithm outperforms existing\ntechniques on video lane datasets. Our codes are available at\nhttps://github.com/dongkwonjin/OMR.\n", "link": "http://arxiv.org/abs/2408.07486v1", "date": "2024-08-14", "relevancy": 2.5729, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5196}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5192}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.505}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20OMR%3A%20Occlusion-Aware%20Memory-Based%20Refinement%20for%20Video%20Lane%20Detection&body=Title%3A%20OMR%3A%20Occlusion-Aware%20Memory-Based%20Refinement%20for%20Video%20Lane%20Detection%0AAuthor%3A%20Dongkwon%20Jin%20and%20Chang-Su%20Kim%0AAbstract%3A%20%20%20A%20novel%20algorithm%20for%20video%20lane%20detection%20is%20proposed%20in%20this%20paper.%20First%2C%0Awe%20extract%20a%20feature%20map%20for%20a%20current%20frame%20and%20detect%20a%20latent%20mask%20for%0Aobstacles%20occluding%20lanes.%20Then%2C%20we%20enhance%20the%20feature%20map%20by%20developing%20an%0Aocclusion-aware%20memory-based%20refinement%20%28OMR%29%20module.%20It%20takes%20the%20obstacle%0Amask%20and%20feature%20map%20from%20the%20current%20frame%2C%20previous%20output%2C%20and%20memory%0Ainformation%20as%20input%2C%20and%20processes%20them%20recursively%20in%20a%20video.%20Moreover%2C%20we%0Aapply%20a%20novel%20data%20augmentation%20scheme%20for%20training%20the%20OMR%20module%20effectively.%0AExperimental%20results%20show%20that%20the%20proposed%20algorithm%20outperforms%20existing%0Atechniques%20on%20video%20lane%20datasets.%20Our%20codes%20are%20available%20at%0Ahttps%3A//github.com/dongkwonjin/OMR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07486v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOMR%253A%2520Occlusion-Aware%2520Memory-Based%2520Refinement%2520for%2520Video%2520Lane%2520Detection%26entry.906535625%3DDongkwon%2520Jin%2520and%2520Chang-Su%2520Kim%26entry.1292438233%3D%2520%2520A%2520novel%2520algorithm%2520for%2520video%2520lane%2520detection%2520is%2520proposed%2520in%2520this%2520paper.%2520First%252C%250Awe%2520extract%2520a%2520feature%2520map%2520for%2520a%2520current%2520frame%2520and%2520detect%2520a%2520latent%2520mask%2520for%250Aobstacles%2520occluding%2520lanes.%2520Then%252C%2520we%2520enhance%2520the%2520feature%2520map%2520by%2520developing%2520an%250Aocclusion-aware%2520memory-based%2520refinement%2520%2528OMR%2529%2520module.%2520It%2520takes%2520the%2520obstacle%250Amask%2520and%2520feature%2520map%2520from%2520the%2520current%2520frame%252C%2520previous%2520output%252C%2520and%2520memory%250Ainformation%2520as%2520input%252C%2520and%2520processes%2520them%2520recursively%2520in%2520a%2520video.%2520Moreover%252C%2520we%250Aapply%2520a%2520novel%2520data%2520augmentation%2520scheme%2520for%2520training%2520the%2520OMR%2520module%2520effectively.%250AExperimental%2520results%2520show%2520that%2520the%2520proposed%2520algorithm%2520outperforms%2520existing%250Atechniques%2520on%2520video%2520lane%2520datasets.%2520Our%2520codes%2520are%2520available%2520at%250Ahttps%253A//github.com/dongkwonjin/OMR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07486v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=OMR%3A%20Occlusion-Aware%20Memory-Based%20Refinement%20for%20Video%20Lane%20Detection&entry.906535625=Dongkwon%20Jin%20and%20Chang-Su%20Kim&entry.1292438233=%20%20A%20novel%20algorithm%20for%20video%20lane%20detection%20is%20proposed%20in%20this%20paper.%20First%2C%0Awe%20extract%20a%20feature%20map%20for%20a%20current%20frame%20and%20detect%20a%20latent%20mask%20for%0Aobstacles%20occluding%20lanes.%20Then%2C%20we%20enhance%20the%20feature%20map%20by%20developing%20an%0Aocclusion-aware%20memory-based%20refinement%20%28OMR%29%20module.%20It%20takes%20the%20obstacle%0Amask%20and%20feature%20map%20from%20the%20current%20frame%2C%20previous%20output%2C%20and%20memory%0Ainformation%20as%20input%2C%20and%20processes%20them%20recursively%20in%20a%20video.%20Moreover%2C%20we%0Aapply%20a%20novel%20data%20augmentation%20scheme%20for%20training%20the%20OMR%20module%20effectively.%0AExperimental%20results%20show%20that%20the%20proposed%20algorithm%20outperforms%20existing%0Atechniques%20on%20video%20lane%20datasets.%20Our%20codes%20are%20available%20at%0Ahttps%3A//github.com/dongkwonjin/OMR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07486v1&entry.124074799=Read"},
{"title": "CNN-JEPA: Self-Supervised Pretraining Convolutional Neural Networks\n  Using Joint Embedding Predictive Architecture", "author": "Andr\u00e1s Kalapos and B\u00e1lint Gyires-T\u00f3th", "abstract": "  Self-supervised learning (SSL) has become an important approach in\npretraining large neural networks, enabling unprecedented scaling of model and\ndataset sizes. While recent advances like I-JEPA have shown promising results\nfor Vision Transformers, adapting such methods to Convolutional Neural Networks\n(CNNs) presents unique challenges. In this paper, we introduce CNN-JEPA, a\nnovel SSL method that successfully applies the joint embedding predictive\narchitecture approach to CNNs. Our method incorporates a sparse CNN encoder to\nhandle masked inputs, a fully convolutional predictor using depthwise separable\nconvolutions, and an improved masking strategy. We demonstrate that CNN-JEPA\noutperforms I-JEPA with ViT architectures on ImageNet-100, achieving 73.3%\nlinear top-1 accuracy with a standard ResNet-50 encoder. Compared to other\nCNN-based SSL methods, CNN-JEPA requires 17-35% less training time for the same\nnumber of epochs and approaches the linear and k-NN top-1 accuracies of BYOL,\nSimCLR, and VICReg. Our approach offers a simpler, more efficient alternative\nto existing SSL methods for CNNs, requiring minimal augmentations and no\nseparate projector network.\n", "link": "http://arxiv.org/abs/2408.07514v1", "date": "2024-08-14", "relevancy": 2.5377, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5292}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.507}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4864}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20CNN-JEPA%3A%20Self-Supervised%20Pretraining%20Convolutional%20Neural%20Networks%0A%20%20Using%20Joint%20Embedding%20Predictive%20Architecture&body=Title%3A%20CNN-JEPA%3A%20Self-Supervised%20Pretraining%20Convolutional%20Neural%20Networks%0A%20%20Using%20Joint%20Embedding%20Predictive%20Architecture%0AAuthor%3A%20Andr%C3%A1s%20Kalapos%20and%20B%C3%A1lint%20Gyires-T%C3%B3th%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20become%20an%20important%20approach%20in%0Apretraining%20large%20neural%20networks%2C%20enabling%20unprecedented%20scaling%20of%20model%20and%0Adataset%20sizes.%20While%20recent%20advances%20like%20I-JEPA%20have%20shown%20promising%20results%0Afor%20Vision%20Transformers%2C%20adapting%20such%20methods%20to%20Convolutional%20Neural%20Networks%0A%28CNNs%29%20presents%20unique%20challenges.%20In%20this%20paper%2C%20we%20introduce%20CNN-JEPA%2C%20a%0Anovel%20SSL%20method%20that%20successfully%20applies%20the%20joint%20embedding%20predictive%0Aarchitecture%20approach%20to%20CNNs.%20Our%20method%20incorporates%20a%20sparse%20CNN%20encoder%20to%0Ahandle%20masked%20inputs%2C%20a%20fully%20convolutional%20predictor%20using%20depthwise%20separable%0Aconvolutions%2C%20and%20an%20improved%20masking%20strategy.%20We%20demonstrate%20that%20CNN-JEPA%0Aoutperforms%20I-JEPA%20with%20ViT%20architectures%20on%20ImageNet-100%2C%20achieving%2073.3%25%0Alinear%20top-1%20accuracy%20with%20a%20standard%20ResNet-50%20encoder.%20Compared%20to%20other%0ACNN-based%20SSL%20methods%2C%20CNN-JEPA%20requires%2017-35%25%20less%20training%20time%20for%20the%20same%0Anumber%20of%20epochs%20and%20approaches%20the%20linear%20and%20k-NN%20top-1%20accuracies%20of%20BYOL%2C%0ASimCLR%2C%20and%20VICReg.%20Our%20approach%20offers%20a%20simpler%2C%20more%20efficient%20alternative%0Ato%20existing%20SSL%20methods%20for%20CNNs%2C%20requiring%20minimal%20augmentations%20and%20no%0Aseparate%20projector%20network.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07514v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCNN-JEPA%253A%2520Self-Supervised%2520Pretraining%2520Convolutional%2520Neural%2520Networks%250A%2520%2520Using%2520Joint%2520Embedding%2520Predictive%2520Architecture%26entry.906535625%3DAndr%25C3%25A1s%2520Kalapos%2520and%2520B%25C3%25A1lint%2520Gyires-T%25C3%25B3th%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%2520become%2520an%2520important%2520approach%2520in%250Apretraining%2520large%2520neural%2520networks%252C%2520enabling%2520unprecedented%2520scaling%2520of%2520model%2520and%250Adataset%2520sizes.%2520While%2520recent%2520advances%2520like%2520I-JEPA%2520have%2520shown%2520promising%2520results%250Afor%2520Vision%2520Transformers%252C%2520adapting%2520such%2520methods%2520to%2520Convolutional%2520Neural%2520Networks%250A%2528CNNs%2529%2520presents%2520unique%2520challenges.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520CNN-JEPA%252C%2520a%250Anovel%2520SSL%2520method%2520that%2520successfully%2520applies%2520the%2520joint%2520embedding%2520predictive%250Aarchitecture%2520approach%2520to%2520CNNs.%2520Our%2520method%2520incorporates%2520a%2520sparse%2520CNN%2520encoder%2520to%250Ahandle%2520masked%2520inputs%252C%2520a%2520fully%2520convolutional%2520predictor%2520using%2520depthwise%2520separable%250Aconvolutions%252C%2520and%2520an%2520improved%2520masking%2520strategy.%2520We%2520demonstrate%2520that%2520CNN-JEPA%250Aoutperforms%2520I-JEPA%2520with%2520ViT%2520architectures%2520on%2520ImageNet-100%252C%2520achieving%252073.3%2525%250Alinear%2520top-1%2520accuracy%2520with%2520a%2520standard%2520ResNet-50%2520encoder.%2520Compared%2520to%2520other%250ACNN-based%2520SSL%2520methods%252C%2520CNN-JEPA%2520requires%252017-35%2525%2520less%2520training%2520time%2520for%2520the%2520same%250Anumber%2520of%2520epochs%2520and%2520approaches%2520the%2520linear%2520and%2520k-NN%2520top-1%2520accuracies%2520of%2520BYOL%252C%250ASimCLR%252C%2520and%2520VICReg.%2520Our%2520approach%2520offers%2520a%2520simpler%252C%2520more%2520efficient%2520alternative%250Ato%2520existing%2520SSL%2520methods%2520for%2520CNNs%252C%2520requiring%2520minimal%2520augmentations%2520and%2520no%250Aseparate%2520projector%2520network.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07514v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=CNN-JEPA%3A%20Self-Supervised%20Pretraining%20Convolutional%20Neural%20Networks%0A%20%20Using%20Joint%20Embedding%20Predictive%20Architecture&entry.906535625=Andr%C3%A1s%20Kalapos%20and%20B%C3%A1lint%20Gyires-T%C3%B3th&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20become%20an%20important%20approach%20in%0Apretraining%20large%20neural%20networks%2C%20enabling%20unprecedented%20scaling%20of%20model%20and%0Adataset%20sizes.%20While%20recent%20advances%20like%20I-JEPA%20have%20shown%20promising%20results%0Afor%20Vision%20Transformers%2C%20adapting%20such%20methods%20to%20Convolutional%20Neural%20Networks%0A%28CNNs%29%20presents%20unique%20challenges.%20In%20this%20paper%2C%20we%20introduce%20CNN-JEPA%2C%20a%0Anovel%20SSL%20method%20that%20successfully%20applies%20the%20joint%20embedding%20predictive%0Aarchitecture%20approach%20to%20CNNs.%20Our%20method%20incorporates%20a%20sparse%20CNN%20encoder%20to%0Ahandle%20masked%20inputs%2C%20a%20fully%20convolutional%20predictor%20using%20depthwise%20separable%0Aconvolutions%2C%20and%20an%20improved%20masking%20strategy.%20We%20demonstrate%20that%20CNN-JEPA%0Aoutperforms%20I-JEPA%20with%20ViT%20architectures%20on%20ImageNet-100%2C%20achieving%2073.3%25%0Alinear%20top-1%20accuracy%20with%20a%20standard%20ResNet-50%20encoder.%20Compared%20to%20other%0ACNN-based%20SSL%20methods%2C%20CNN-JEPA%20requires%2017-35%25%20less%20training%20time%20for%20the%20same%0Anumber%20of%20epochs%20and%20approaches%20the%20linear%20and%20k-NN%20top-1%20accuracies%20of%20BYOL%2C%0ASimCLR%2C%20and%20VICReg.%20Our%20approach%20offers%20a%20simpler%2C%20more%20efficient%20alternative%0Ato%20existing%20SSL%20methods%20for%20CNNs%2C%20requiring%20minimal%20augmentations%20and%20no%0Aseparate%20projector%20network.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07514v1&entry.124074799=Read"},
{"title": "Domain-invariant Representation Learning via Segment Anything Model for\n  Blood Cell Classification", "author": "Yongcheng Li and Lingcong Cai and Ying Lu and Cheng Lin and Yupeng Zhang and Jingyan Jiang and Genan Dai and Bowen Zhang and Jingzhou Cao and Xiangzhong Zhang and Xiaomao Fan", "abstract": "  Accurate classification of blood cells is of vital significance in the\ndiagnosis of hematological disorders. However, in real-world scenarios, domain\nshifts caused by the variability in laboratory procedures and settings, result\nin a rapid deterioration of the model's generalization performance. To address\nthis issue, we propose a novel framework of domain-invariant representation\nlearning (DoRL) via segment anything model (SAM) for blood cell classification.\nThe DoRL comprises two main components: a LoRA-based SAM (LoRA-SAM) and a\ncross-domain autoencoder (CAE). The advantage of DoRL is that it can extract\ndomain-invariant representations from various blood cell datasets in an\nunsupervised manner. Specifically, we first leverage the large-scale foundation\nmodel of SAM, fine-tuned with LoRA, to learn general image embeddings and\nsegment blood cells. Additionally, we introduce CAE to learn domain-invariant\nrepresentations across different-domain datasets while mitigating images'\nartifacts. To validate the effectiveness of domain-invariant representations,\nwe employ five widely used machine learning classifiers to construct blood cell\nclassification models. Experimental results on two public blood cell datasets\nand a private real dataset demonstrate that our proposed DoRL achieves a new\nstate-of-the-art cross-domain performance, surpassing existing methods by a\nsignificant margin. The source code can be available at the URL\n(https://github.com/AnoK3111/DoRL).\n", "link": "http://arxiv.org/abs/2408.07467v1", "date": "2024-08-14", "relevancy": 2.5143, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.517}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4966}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.495}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Domain-invariant%20Representation%20Learning%20via%20Segment%20Anything%20Model%20for%0A%20%20Blood%20Cell%20Classification&body=Title%3A%20Domain-invariant%20Representation%20Learning%20via%20Segment%20Anything%20Model%20for%0A%20%20Blood%20Cell%20Classification%0AAuthor%3A%20Yongcheng%20Li%20and%20Lingcong%20Cai%20and%20Ying%20Lu%20and%20Cheng%20Lin%20and%20Yupeng%20Zhang%20and%20Jingyan%20Jiang%20and%20Genan%20Dai%20and%20Bowen%20Zhang%20and%20Jingzhou%20Cao%20and%20Xiangzhong%20Zhang%20and%20Xiaomao%20Fan%0AAbstract%3A%20%20%20Accurate%20classification%20of%20blood%20cells%20is%20of%20vital%20significance%20in%20the%0Adiagnosis%20of%20hematological%20disorders.%20However%2C%20in%20real-world%20scenarios%2C%20domain%0Ashifts%20caused%20by%20the%20variability%20in%20laboratory%20procedures%20and%20settings%2C%20result%0Ain%20a%20rapid%20deterioration%20of%20the%20model%27s%20generalization%20performance.%20To%20address%0Athis%20issue%2C%20we%20propose%20a%20novel%20framework%20of%20domain-invariant%20representation%0Alearning%20%28DoRL%29%20via%20segment%20anything%20model%20%28SAM%29%20for%20blood%20cell%20classification.%0AThe%20DoRL%20comprises%20two%20main%20components%3A%20a%20LoRA-based%20SAM%20%28LoRA-SAM%29%20and%20a%0Across-domain%20autoencoder%20%28CAE%29.%20The%20advantage%20of%20DoRL%20is%20that%20it%20can%20extract%0Adomain-invariant%20representations%20from%20various%20blood%20cell%20datasets%20in%20an%0Aunsupervised%20manner.%20Specifically%2C%20we%20first%20leverage%20the%20large-scale%20foundation%0Amodel%20of%20SAM%2C%20fine-tuned%20with%20LoRA%2C%20to%20learn%20general%20image%20embeddings%20and%0Asegment%20blood%20cells.%20Additionally%2C%20we%20introduce%20CAE%20to%20learn%20domain-invariant%0Arepresentations%20across%20different-domain%20datasets%20while%20mitigating%20images%27%0Aartifacts.%20To%20validate%20the%20effectiveness%20of%20domain-invariant%20representations%2C%0Awe%20employ%20five%20widely%20used%20machine%20learning%20classifiers%20to%20construct%20blood%20cell%0Aclassification%20models.%20Experimental%20results%20on%20two%20public%20blood%20cell%20datasets%0Aand%20a%20private%20real%20dataset%20demonstrate%20that%20our%20proposed%20DoRL%20achieves%20a%20new%0Astate-of-the-art%20cross-domain%20performance%2C%20surpassing%20existing%20methods%20by%20a%0Asignificant%20margin.%20The%20source%20code%20can%20be%20available%20at%20the%20URL%0A%28https%3A//github.com/AnoK3111/DoRL%29.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07467v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDomain-invariant%2520Representation%2520Learning%2520via%2520Segment%2520Anything%2520Model%2520for%250A%2520%2520Blood%2520Cell%2520Classification%26entry.906535625%3DYongcheng%2520Li%2520and%2520Lingcong%2520Cai%2520and%2520Ying%2520Lu%2520and%2520Cheng%2520Lin%2520and%2520Yupeng%2520Zhang%2520and%2520Jingyan%2520Jiang%2520and%2520Genan%2520Dai%2520and%2520Bowen%2520Zhang%2520and%2520Jingzhou%2520Cao%2520and%2520Xiangzhong%2520Zhang%2520and%2520Xiaomao%2520Fan%26entry.1292438233%3D%2520%2520Accurate%2520classification%2520of%2520blood%2520cells%2520is%2520of%2520vital%2520significance%2520in%2520the%250Adiagnosis%2520of%2520hematological%2520disorders.%2520However%252C%2520in%2520real-world%2520scenarios%252C%2520domain%250Ashifts%2520caused%2520by%2520the%2520variability%2520in%2520laboratory%2520procedures%2520and%2520settings%252C%2520result%250Ain%2520a%2520rapid%2520deterioration%2520of%2520the%2520model%2527s%2520generalization%2520performance.%2520To%2520address%250Athis%2520issue%252C%2520we%2520propose%2520a%2520novel%2520framework%2520of%2520domain-invariant%2520representation%250Alearning%2520%2528DoRL%2529%2520via%2520segment%2520anything%2520model%2520%2528SAM%2529%2520for%2520blood%2520cell%2520classification.%250AThe%2520DoRL%2520comprises%2520two%2520main%2520components%253A%2520a%2520LoRA-based%2520SAM%2520%2528LoRA-SAM%2529%2520and%2520a%250Across-domain%2520autoencoder%2520%2528CAE%2529.%2520The%2520advantage%2520of%2520DoRL%2520is%2520that%2520it%2520can%2520extract%250Adomain-invariant%2520representations%2520from%2520various%2520blood%2520cell%2520datasets%2520in%2520an%250Aunsupervised%2520manner.%2520Specifically%252C%2520we%2520first%2520leverage%2520the%2520large-scale%2520foundation%250Amodel%2520of%2520SAM%252C%2520fine-tuned%2520with%2520LoRA%252C%2520to%2520learn%2520general%2520image%2520embeddings%2520and%250Asegment%2520blood%2520cells.%2520Additionally%252C%2520we%2520introduce%2520CAE%2520to%2520learn%2520domain-invariant%250Arepresentations%2520across%2520different-domain%2520datasets%2520while%2520mitigating%2520images%2527%250Aartifacts.%2520To%2520validate%2520the%2520effectiveness%2520of%2520domain-invariant%2520representations%252C%250Awe%2520employ%2520five%2520widely%2520used%2520machine%2520learning%2520classifiers%2520to%2520construct%2520blood%2520cell%250Aclassification%2520models.%2520Experimental%2520results%2520on%2520two%2520public%2520blood%2520cell%2520datasets%250Aand%2520a%2520private%2520real%2520dataset%2520demonstrate%2520that%2520our%2520proposed%2520DoRL%2520achieves%2520a%2520new%250Astate-of-the-art%2520cross-domain%2520performance%252C%2520surpassing%2520existing%2520methods%2520by%2520a%250Asignificant%2520margin.%2520The%2520source%2520code%2520can%2520be%2520available%2520at%2520the%2520URL%250A%2528https%253A//github.com/AnoK3111/DoRL%2529.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07467v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Domain-invariant%20Representation%20Learning%20via%20Segment%20Anything%20Model%20for%0A%20%20Blood%20Cell%20Classification&entry.906535625=Yongcheng%20Li%20and%20Lingcong%20Cai%20and%20Ying%20Lu%20and%20Cheng%20Lin%20and%20Yupeng%20Zhang%20and%20Jingyan%20Jiang%20and%20Genan%20Dai%20and%20Bowen%20Zhang%20and%20Jingzhou%20Cao%20and%20Xiangzhong%20Zhang%20and%20Xiaomao%20Fan&entry.1292438233=%20%20Accurate%20classification%20of%20blood%20cells%20is%20of%20vital%20significance%20in%20the%0Adiagnosis%20of%20hematological%20disorders.%20However%2C%20in%20real-world%20scenarios%2C%20domain%0Ashifts%20caused%20by%20the%20variability%20in%20laboratory%20procedures%20and%20settings%2C%20result%0Ain%20a%20rapid%20deterioration%20of%20the%20model%27s%20generalization%20performance.%20To%20address%0Athis%20issue%2C%20we%20propose%20a%20novel%20framework%20of%20domain-invariant%20representation%0Alearning%20%28DoRL%29%20via%20segment%20anything%20model%20%28SAM%29%20for%20blood%20cell%20classification.%0AThe%20DoRL%20comprises%20two%20main%20components%3A%20a%20LoRA-based%20SAM%20%28LoRA-SAM%29%20and%20a%0Across-domain%20autoencoder%20%28CAE%29.%20The%20advantage%20of%20DoRL%20is%20that%20it%20can%20extract%0Adomain-invariant%20representations%20from%20various%20blood%20cell%20datasets%20in%20an%0Aunsupervised%20manner.%20Specifically%2C%20we%20first%20leverage%20the%20large-scale%20foundation%0Amodel%20of%20SAM%2C%20fine-tuned%20with%20LoRA%2C%20to%20learn%20general%20image%20embeddings%20and%0Asegment%20blood%20cells.%20Additionally%2C%20we%20introduce%20CAE%20to%20learn%20domain-invariant%0Arepresentations%20across%20different-domain%20datasets%20while%20mitigating%20images%27%0Aartifacts.%20To%20validate%20the%20effectiveness%20of%20domain-invariant%20representations%2C%0Awe%20employ%20five%20widely%20used%20machine%20learning%20classifiers%20to%20construct%20blood%20cell%0Aclassification%20models.%20Experimental%20results%20on%20two%20public%20blood%20cell%20datasets%0Aand%20a%20private%20real%20dataset%20demonstrate%20that%20our%20proposed%20DoRL%20achieves%20a%20new%0Astate-of-the-art%20cross-domain%20performance%2C%20surpassing%20existing%20methods%20by%20a%0Asignificant%20margin.%20The%20source%20code%20can%20be%20available%20at%20the%20URL%0A%28https%3A//github.com/AnoK3111/DoRL%29.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07467v1&entry.124074799=Read"},
{"title": "LLMI3D: Empowering LLM with 3D Perception from a Single 2D Image", "author": "Fan Yang and Sicheng Zhao and Yanhao Zhang and Haoxiang Chen and Hui Chen and Wenbo Tang and Haonan Lu and Pengfei Xu and Zhenyu Yang and Jungong Han and Guiguang Ding", "abstract": "  Recent advancements in autonomous driving, augmented reality, robotics, and\nembodied intelligence have necessitated 3D perception algorithms. However,\ncurrent 3D perception methods, particularly small models, struggle with\nprocessing logical reasoning, question-answering, and handling open scenario\ncategories. On the other hand, generative multimodal large language models\n(MLLMs) excel in general capacity but underperform in 3D tasks, due to weak\nspatial and local object perception, poor text-based geometric numerical\noutput, and inability to handle camera focal variations. To address these\nchallenges, we propose the following solutions: Spatial-Enhanced Local Feature\nMining for better spatial feature extraction, 3D Query Token-Derived Info\nDecoding for precise geometric regression, and Geometry Projection-Based 3D\nReasoning for handling camera focal length variations. We employ\nparameter-efficient fine-tuning for a pre-trained MLLM and develop LLMI3D, a\npowerful 3D perception MLLM. Additionally, we have constructed the IG3D\ndataset, which provides fine-grained descriptions and question-answer\nannotations. Extensive experiments demonstrate that our LLMI3D achieves\nstate-of-the-art performance, significantly outperforming existing methods.\n", "link": "http://arxiv.org/abs/2408.07422v1", "date": "2024-08-14", "relevancy": 2.4882, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.6527}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6286}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6032}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20LLMI3D%3A%20Empowering%20LLM%20with%203D%20Perception%20from%20a%20Single%202D%20Image&body=Title%3A%20LLMI3D%3A%20Empowering%20LLM%20with%203D%20Perception%20from%20a%20Single%202D%20Image%0AAuthor%3A%20Fan%20Yang%20and%20Sicheng%20Zhao%20and%20Yanhao%20Zhang%20and%20Haoxiang%20Chen%20and%20Hui%20Chen%20and%20Wenbo%20Tang%20and%20Haonan%20Lu%20and%20Pengfei%20Xu%20and%20Zhenyu%20Yang%20and%20Jungong%20Han%20and%20Guiguang%20Ding%0AAbstract%3A%20%20%20Recent%20advancements%20in%20autonomous%20driving%2C%20augmented%20reality%2C%20robotics%2C%20and%0Aembodied%20intelligence%20have%20necessitated%203D%20perception%20algorithms.%20However%2C%0Acurrent%203D%20perception%20methods%2C%20particularly%20small%20models%2C%20struggle%20with%0Aprocessing%20logical%20reasoning%2C%20question-answering%2C%20and%20handling%20open%20scenario%0Acategories.%20On%20the%20other%20hand%2C%20generative%20multimodal%20large%20language%20models%0A%28MLLMs%29%20excel%20in%20general%20capacity%20but%20underperform%20in%203D%20tasks%2C%20due%20to%20weak%0Aspatial%20and%20local%20object%20perception%2C%20poor%20text-based%20geometric%20numerical%0Aoutput%2C%20and%20inability%20to%20handle%20camera%20focal%20variations.%20To%20address%20these%0Achallenges%2C%20we%20propose%20the%20following%20solutions%3A%20Spatial-Enhanced%20Local%20Feature%0AMining%20for%20better%20spatial%20feature%20extraction%2C%203D%20Query%20Token-Derived%20Info%0ADecoding%20for%20precise%20geometric%20regression%2C%20and%20Geometry%20Projection-Based%203D%0AReasoning%20for%20handling%20camera%20focal%20length%20variations.%20We%20employ%0Aparameter-efficient%20fine-tuning%20for%20a%20pre-trained%20MLLM%20and%20develop%20LLMI3D%2C%20a%0Apowerful%203D%20perception%20MLLM.%20Additionally%2C%20we%20have%20constructed%20the%20IG3D%0Adataset%2C%20which%20provides%20fine-grained%20descriptions%20and%20question-answer%0Aannotations.%20Extensive%20experiments%20demonstrate%20that%20our%20LLMI3D%20achieves%0Astate-of-the-art%20performance%2C%20significantly%20outperforming%20existing%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07422v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLLMI3D%253A%2520Empowering%2520LLM%2520with%25203D%2520Perception%2520from%2520a%2520Single%25202D%2520Image%26entry.906535625%3DFan%2520Yang%2520and%2520Sicheng%2520Zhao%2520and%2520Yanhao%2520Zhang%2520and%2520Haoxiang%2520Chen%2520and%2520Hui%2520Chen%2520and%2520Wenbo%2520Tang%2520and%2520Haonan%2520Lu%2520and%2520Pengfei%2520Xu%2520and%2520Zhenyu%2520Yang%2520and%2520Jungong%2520Han%2520and%2520Guiguang%2520Ding%26entry.1292438233%3D%2520%2520Recent%2520advancements%2520in%2520autonomous%2520driving%252C%2520augmented%2520reality%252C%2520robotics%252C%2520and%250Aembodied%2520intelligence%2520have%2520necessitated%25203D%2520perception%2520algorithms.%2520However%252C%250Acurrent%25203D%2520perception%2520methods%252C%2520particularly%2520small%2520models%252C%2520struggle%2520with%250Aprocessing%2520logical%2520reasoning%252C%2520question-answering%252C%2520and%2520handling%2520open%2520scenario%250Acategories.%2520On%2520the%2520other%2520hand%252C%2520generative%2520multimodal%2520large%2520language%2520models%250A%2528MLLMs%2529%2520excel%2520in%2520general%2520capacity%2520but%2520underperform%2520in%25203D%2520tasks%252C%2520due%2520to%2520weak%250Aspatial%2520and%2520local%2520object%2520perception%252C%2520poor%2520text-based%2520geometric%2520numerical%250Aoutput%252C%2520and%2520inability%2520to%2520handle%2520camera%2520focal%2520variations.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520the%2520following%2520solutions%253A%2520Spatial-Enhanced%2520Local%2520Feature%250AMining%2520for%2520better%2520spatial%2520feature%2520extraction%252C%25203D%2520Query%2520Token-Derived%2520Info%250ADecoding%2520for%2520precise%2520geometric%2520regression%252C%2520and%2520Geometry%2520Projection-Based%25203D%250AReasoning%2520for%2520handling%2520camera%2520focal%2520length%2520variations.%2520We%2520employ%250Aparameter-efficient%2520fine-tuning%2520for%2520a%2520pre-trained%2520MLLM%2520and%2520develop%2520LLMI3D%252C%2520a%250Apowerful%25203D%2520perception%2520MLLM.%2520Additionally%252C%2520we%2520have%2520constructed%2520the%2520IG3D%250Adataset%252C%2520which%2520provides%2520fine-grained%2520descriptions%2520and%2520question-answer%250Aannotations.%2520Extensive%2520experiments%2520demonstrate%2520that%2520our%2520LLMI3D%2520achieves%250Astate-of-the-art%2520performance%252C%2520significantly%2520outperforming%2520existing%2520methods.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07422v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLMI3D%3A%20Empowering%20LLM%20with%203D%20Perception%20from%20a%20Single%202D%20Image&entry.906535625=Fan%20Yang%20and%20Sicheng%20Zhao%20and%20Yanhao%20Zhang%20and%20Haoxiang%20Chen%20and%20Hui%20Chen%20and%20Wenbo%20Tang%20and%20Haonan%20Lu%20and%20Pengfei%20Xu%20and%20Zhenyu%20Yang%20and%20Jungong%20Han%20and%20Guiguang%20Ding&entry.1292438233=%20%20Recent%20advancements%20in%20autonomous%20driving%2C%20augmented%20reality%2C%20robotics%2C%20and%0Aembodied%20intelligence%20have%20necessitated%203D%20perception%20algorithms.%20However%2C%0Acurrent%203D%20perception%20methods%2C%20particularly%20small%20models%2C%20struggle%20with%0Aprocessing%20logical%20reasoning%2C%20question-answering%2C%20and%20handling%20open%20scenario%0Acategories.%20On%20the%20other%20hand%2C%20generative%20multimodal%20large%20language%20models%0A%28MLLMs%29%20excel%20in%20general%20capacity%20but%20underperform%20in%203D%20tasks%2C%20due%20to%20weak%0Aspatial%20and%20local%20object%20perception%2C%20poor%20text-based%20geometric%20numerical%0Aoutput%2C%20and%20inability%20to%20handle%20camera%20focal%20variations.%20To%20address%20these%0Achallenges%2C%20we%20propose%20the%20following%20solutions%3A%20Spatial-Enhanced%20Local%20Feature%0AMining%20for%20better%20spatial%20feature%20extraction%2C%203D%20Query%20Token-Derived%20Info%0ADecoding%20for%20precise%20geometric%20regression%2C%20and%20Geometry%20Projection-Based%203D%0AReasoning%20for%20handling%20camera%20focal%20length%20variations.%20We%20employ%0Aparameter-efficient%20fine-tuning%20for%20a%20pre-trained%20MLLM%20and%20develop%20LLMI3D%2C%20a%0Apowerful%203D%20perception%20MLLM.%20Additionally%2C%20we%20have%20constructed%20the%20IG3D%0Adataset%2C%20which%20provides%20fine-grained%20descriptions%20and%20question-answer%0Aannotations.%20Extensive%20experiments%20demonstrate%20that%20our%20LLMI3D%20achieves%0Astate-of-the-art%20performance%2C%20significantly%20outperforming%20existing%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07422v1&entry.124074799=Read"},
{"title": "Semi-Supervised Laplace Learning on Stiefel Manifolds", "author": "Chester Holtz and Pengwen Chen and Alexander Cloninger and Chung-Kuan Cheng and Gal Mishne", "abstract": "  Motivated by the need to address the degeneracy of canonical Laplace learning\nalgorithms in low label rates, we propose to reformulate graph-based\nsemi-supervised learning as a nonconvex generalization of a \\emph{Trust-Region\nSubproblem} (TRS). This reformulation is motivated by the well-posedness of\nLaplacian eigenvectors in the limit of infinite unlabeled data. To solve this\nproblem, we first show that a first-order condition implies the solution of a\nmanifold alignment problem and that solutions to the classical \\emph{Orthogonal\nProcrustes} problem can be used to efficiently find good classifiers that are\namenable to further refinement. To tackle refinement, we develop the framework\nof Sequential Subspace Optimization for graph-based SSL. Next, we address the\ncriticality of selecting supervised samples at low-label rates. We characterize\ninformative samples with a novel measure of centrality derived from the\nprincipal eigenvectors of a certain submatrix of the graph Laplacian. We\ndemonstrate that our framework achieves lower classification error compared to\nrecent state-of-the-art and classical semi-supervised learning methods at\nextremely low, medium, and high label rates.\n", "link": "http://arxiv.org/abs/2308.00142v2", "date": "2024-08-14", "relevancy": 2.4763, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.552}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4675}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4663}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Semi-Supervised%20Laplace%20Learning%20on%20Stiefel%20Manifolds&body=Title%3A%20Semi-Supervised%20Laplace%20Learning%20on%20Stiefel%20Manifolds%0AAuthor%3A%20Chester%20Holtz%20and%20Pengwen%20Chen%20and%20Alexander%20Cloninger%20and%20Chung-Kuan%20Cheng%20and%20Gal%20Mishne%0AAbstract%3A%20%20%20Motivated%20by%20the%20need%20to%20address%20the%20degeneracy%20of%20canonical%20Laplace%20learning%0Aalgorithms%20in%20low%20label%20rates%2C%20we%20propose%20to%20reformulate%20graph-based%0Asemi-supervised%20learning%20as%20a%20nonconvex%20generalization%20of%20a%20%5Cemph%7BTrust-Region%0ASubproblem%7D%20%28TRS%29.%20This%20reformulation%20is%20motivated%20by%20the%20well-posedness%20of%0ALaplacian%20eigenvectors%20in%20the%20limit%20of%20infinite%20unlabeled%20data.%20To%20solve%20this%0Aproblem%2C%20we%20first%20show%20that%20a%20first-order%20condition%20implies%20the%20solution%20of%20a%0Amanifold%20alignment%20problem%20and%20that%20solutions%20to%20the%20classical%20%5Cemph%7BOrthogonal%0AProcrustes%7D%20problem%20can%20be%20used%20to%20efficiently%20find%20good%20classifiers%20that%20are%0Aamenable%20to%20further%20refinement.%20To%20tackle%20refinement%2C%20we%20develop%20the%20framework%0Aof%20Sequential%20Subspace%20Optimization%20for%20graph-based%20SSL.%20Next%2C%20we%20address%20the%0Acriticality%20of%20selecting%20supervised%20samples%20at%20low-label%20rates.%20We%20characterize%0Ainformative%20samples%20with%20a%20novel%20measure%20of%20centrality%20derived%20from%20the%0Aprincipal%20eigenvectors%20of%20a%20certain%20submatrix%20of%20the%20graph%20Laplacian.%20We%0Ademonstrate%20that%20our%20framework%20achieves%20lower%20classification%20error%20compared%20to%0Arecent%20state-of-the-art%20and%20classical%20semi-supervised%20learning%20methods%20at%0Aextremely%20low%2C%20medium%2C%20and%20high%20label%20rates.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.00142v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSemi-Supervised%2520Laplace%2520Learning%2520on%2520Stiefel%2520Manifolds%26entry.906535625%3DChester%2520Holtz%2520and%2520Pengwen%2520Chen%2520and%2520Alexander%2520Cloninger%2520and%2520Chung-Kuan%2520Cheng%2520and%2520Gal%2520Mishne%26entry.1292438233%3D%2520%2520Motivated%2520by%2520the%2520need%2520to%2520address%2520the%2520degeneracy%2520of%2520canonical%2520Laplace%2520learning%250Aalgorithms%2520in%2520low%2520label%2520rates%252C%2520we%2520propose%2520to%2520reformulate%2520graph-based%250Asemi-supervised%2520learning%2520as%2520a%2520nonconvex%2520generalization%2520of%2520a%2520%255Cemph%257BTrust-Region%250ASubproblem%257D%2520%2528TRS%2529.%2520This%2520reformulation%2520is%2520motivated%2520by%2520the%2520well-posedness%2520of%250ALaplacian%2520eigenvectors%2520in%2520the%2520limit%2520of%2520infinite%2520unlabeled%2520data.%2520To%2520solve%2520this%250Aproblem%252C%2520we%2520first%2520show%2520that%2520a%2520first-order%2520condition%2520implies%2520the%2520solution%2520of%2520a%250Amanifold%2520alignment%2520problem%2520and%2520that%2520solutions%2520to%2520the%2520classical%2520%255Cemph%257BOrthogonal%250AProcrustes%257D%2520problem%2520can%2520be%2520used%2520to%2520efficiently%2520find%2520good%2520classifiers%2520that%2520are%250Aamenable%2520to%2520further%2520refinement.%2520To%2520tackle%2520refinement%252C%2520we%2520develop%2520the%2520framework%250Aof%2520Sequential%2520Subspace%2520Optimization%2520for%2520graph-based%2520SSL.%2520Next%252C%2520we%2520address%2520the%250Acriticality%2520of%2520selecting%2520supervised%2520samples%2520at%2520low-label%2520rates.%2520We%2520characterize%250Ainformative%2520samples%2520with%2520a%2520novel%2520measure%2520of%2520centrality%2520derived%2520from%2520the%250Aprincipal%2520eigenvectors%2520of%2520a%2520certain%2520submatrix%2520of%2520the%2520graph%2520Laplacian.%2520We%250Ademonstrate%2520that%2520our%2520framework%2520achieves%2520lower%2520classification%2520error%2520compared%2520to%250Arecent%2520state-of-the-art%2520and%2520classical%2520semi-supervised%2520learning%2520methods%2520at%250Aextremely%2520low%252C%2520medium%252C%2520and%2520high%2520label%2520rates.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.00142v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Semi-Supervised%20Laplace%20Learning%20on%20Stiefel%20Manifolds&entry.906535625=Chester%20Holtz%20and%20Pengwen%20Chen%20and%20Alexander%20Cloninger%20and%20Chung-Kuan%20Cheng%20and%20Gal%20Mishne&entry.1292438233=%20%20Motivated%20by%20the%20need%20to%20address%20the%20degeneracy%20of%20canonical%20Laplace%20learning%0Aalgorithms%20in%20low%20label%20rates%2C%20we%20propose%20to%20reformulate%20graph-based%0Asemi-supervised%20learning%20as%20a%20nonconvex%20generalization%20of%20a%20%5Cemph%7BTrust-Region%0ASubproblem%7D%20%28TRS%29.%20This%20reformulation%20is%20motivated%20by%20the%20well-posedness%20of%0ALaplacian%20eigenvectors%20in%20the%20limit%20of%20infinite%20unlabeled%20data.%20To%20solve%20this%0Aproblem%2C%20we%20first%20show%20that%20a%20first-order%20condition%20implies%20the%20solution%20of%20a%0Amanifold%20alignment%20problem%20and%20that%20solutions%20to%20the%20classical%20%5Cemph%7BOrthogonal%0AProcrustes%7D%20problem%20can%20be%20used%20to%20efficiently%20find%20good%20classifiers%20that%20are%0Aamenable%20to%20further%20refinement.%20To%20tackle%20refinement%2C%20we%20develop%20the%20framework%0Aof%20Sequential%20Subspace%20Optimization%20for%20graph-based%20SSL.%20Next%2C%20we%20address%20the%0Acriticality%20of%20selecting%20supervised%20samples%20at%20low-label%20rates.%20We%20characterize%0Ainformative%20samples%20with%20a%20novel%20measure%20of%20centrality%20derived%20from%20the%0Aprincipal%20eigenvectors%20of%20a%20certain%20submatrix%20of%20the%20graph%20Laplacian.%20We%0Ademonstrate%20that%20our%20framework%20achieves%20lower%20classification%20error%20compared%20to%0Arecent%20state-of-the-art%20and%20classical%20semi-supervised%20learning%20methods%20at%0Aextremely%20low%2C%20medium%2C%20and%20high%20label%20rates.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.00142v2&entry.124074799=Read"},
{"title": "PolyCL: Contrastive Learning for Polymer Representation Learning via\n  Explicit and Implicit Augmentations", "author": "Jiajun Zhou and Yijie Yang and Austin M. Mroz and Kim E. Jelfs", "abstract": "  Polymers play a crucial role in a wide array of applications due to their\ndiverse and tunable properties. Establishing the relationship between polymer\nrepresentations and their properties is crucial to the computational design and\nscreening of potential polymers via machine learning. The quality of the\nrepresentation significantly influences the effectiveness of these\ncomputational methods. Here, we present a self-supervised contrastive learning\nparadigm, PolyCL, for learning high-quality polymer representation without the\nneed for labels. Our model combines explicit and implicit augmentation\nstrategies for improved learning performance. The results demonstrate that our\nmodel achieves either better, or highly competitive, performances on transfer\nlearning tasks as a feature extractor without an overcomplicated training\nstrategy or hyperparameter optimisation. Further enhancing the efficacy of our\nmodel, we conducted extensive analyses on various augmentation combinations\nused in contrastive learning. This led to identifying the most effective\ncombination to maximise PolyCL's performance.\n", "link": "http://arxiv.org/abs/2408.07556v1", "date": "2024-08-14", "relevancy": 2.4708, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5286}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.479}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20PolyCL%3A%20Contrastive%20Learning%20for%20Polymer%20Representation%20Learning%20via%0A%20%20Explicit%20and%20Implicit%20Augmentations&body=Title%3A%20PolyCL%3A%20Contrastive%20Learning%20for%20Polymer%20Representation%20Learning%20via%0A%20%20Explicit%20and%20Implicit%20Augmentations%0AAuthor%3A%20Jiajun%20Zhou%20and%20Yijie%20Yang%20and%20Austin%20M.%20Mroz%20and%20Kim%20E.%20Jelfs%0AAbstract%3A%20%20%20Polymers%20play%20a%20crucial%20role%20in%20a%20wide%20array%20of%20applications%20due%20to%20their%0Adiverse%20and%20tunable%20properties.%20Establishing%20the%20relationship%20between%20polymer%0Arepresentations%20and%20their%20properties%20is%20crucial%20to%20the%20computational%20design%20and%0Ascreening%20of%20potential%20polymers%20via%20machine%20learning.%20The%20quality%20of%20the%0Arepresentation%20significantly%20influences%20the%20effectiveness%20of%20these%0Acomputational%20methods.%20Here%2C%20we%20present%20a%20self-supervised%20contrastive%20learning%0Aparadigm%2C%20PolyCL%2C%20for%20learning%20high-quality%20polymer%20representation%20without%20the%0Aneed%20for%20labels.%20Our%20model%20combines%20explicit%20and%20implicit%20augmentation%0Astrategies%20for%20improved%20learning%20performance.%20The%20results%20demonstrate%20that%20our%0Amodel%20achieves%20either%20better%2C%20or%20highly%20competitive%2C%20performances%20on%20transfer%0Alearning%20tasks%20as%20a%20feature%20extractor%20without%20an%20overcomplicated%20training%0Astrategy%20or%20hyperparameter%20optimisation.%20Further%20enhancing%20the%20efficacy%20of%20our%0Amodel%2C%20we%20conducted%20extensive%20analyses%20on%20various%20augmentation%20combinations%0Aused%20in%20contrastive%20learning.%20This%20led%20to%20identifying%20the%20most%20effective%0Acombination%20to%20maximise%20PolyCL%27s%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07556v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DPolyCL%253A%2520Contrastive%2520Learning%2520for%2520Polymer%2520Representation%2520Learning%2520via%250A%2520%2520Explicit%2520and%2520Implicit%2520Augmentations%26entry.906535625%3DJiajun%2520Zhou%2520and%2520Yijie%2520Yang%2520and%2520Austin%2520M.%2520Mroz%2520and%2520Kim%2520E.%2520Jelfs%26entry.1292438233%3D%2520%2520Polymers%2520play%2520a%2520crucial%2520role%2520in%2520a%2520wide%2520array%2520of%2520applications%2520due%2520to%2520their%250Adiverse%2520and%2520tunable%2520properties.%2520Establishing%2520the%2520relationship%2520between%2520polymer%250Arepresentations%2520and%2520their%2520properties%2520is%2520crucial%2520to%2520the%2520computational%2520design%2520and%250Ascreening%2520of%2520potential%2520polymers%2520via%2520machine%2520learning.%2520The%2520quality%2520of%2520the%250Arepresentation%2520significantly%2520influences%2520the%2520effectiveness%2520of%2520these%250Acomputational%2520methods.%2520Here%252C%2520we%2520present%2520a%2520self-supervised%2520contrastive%2520learning%250Aparadigm%252C%2520PolyCL%252C%2520for%2520learning%2520high-quality%2520polymer%2520representation%2520without%2520the%250Aneed%2520for%2520labels.%2520Our%2520model%2520combines%2520explicit%2520and%2520implicit%2520augmentation%250Astrategies%2520for%2520improved%2520learning%2520performance.%2520The%2520results%2520demonstrate%2520that%2520our%250Amodel%2520achieves%2520either%2520better%252C%2520or%2520highly%2520competitive%252C%2520performances%2520on%2520transfer%250Alearning%2520tasks%2520as%2520a%2520feature%2520extractor%2520without%2520an%2520overcomplicated%2520training%250Astrategy%2520or%2520hyperparameter%2520optimisation.%2520Further%2520enhancing%2520the%2520efficacy%2520of%2520our%250Amodel%252C%2520we%2520conducted%2520extensive%2520analyses%2520on%2520various%2520augmentation%2520combinations%250Aused%2520in%2520contrastive%2520learning.%2520This%2520led%2520to%2520identifying%2520the%2520most%2520effective%250Acombination%2520to%2520maximise%2520PolyCL%2527s%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07556v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=PolyCL%3A%20Contrastive%20Learning%20for%20Polymer%20Representation%20Learning%20via%0A%20%20Explicit%20and%20Implicit%20Augmentations&entry.906535625=Jiajun%20Zhou%20and%20Yijie%20Yang%20and%20Austin%20M.%20Mroz%20and%20Kim%20E.%20Jelfs&entry.1292438233=%20%20Polymers%20play%20a%20crucial%20role%20in%20a%20wide%20array%20of%20applications%20due%20to%20their%0Adiverse%20and%20tunable%20properties.%20Establishing%20the%20relationship%20between%20polymer%0Arepresentations%20and%20their%20properties%20is%20crucial%20to%20the%20computational%20design%20and%0Ascreening%20of%20potential%20polymers%20via%20machine%20learning.%20The%20quality%20of%20the%0Arepresentation%20significantly%20influences%20the%20effectiveness%20of%20these%0Acomputational%20methods.%20Here%2C%20we%20present%20a%20self-supervised%20contrastive%20learning%0Aparadigm%2C%20PolyCL%2C%20for%20learning%20high-quality%20polymer%20representation%20without%20the%0Aneed%20for%20labels.%20Our%20model%20combines%20explicit%20and%20implicit%20augmentation%0Astrategies%20for%20improved%20learning%20performance.%20The%20results%20demonstrate%20that%20our%0Amodel%20achieves%20either%20better%2C%20or%20highly%20competitive%2C%20performances%20on%20transfer%0Alearning%20tasks%20as%20a%20feature%20extractor%20without%20an%20overcomplicated%20training%0Astrategy%20or%20hyperparameter%20optimisation.%20Further%20enhancing%20the%20efficacy%20of%20our%0Amodel%2C%20we%20conducted%20extensive%20analyses%20on%20various%20augmentation%20combinations%0Aused%20in%20contrastive%20learning.%20This%20led%20to%20identifying%20the%20most%20effective%0Acombination%20to%20maximise%20PolyCL%27s%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07556v1&entry.124074799=Read"},
{"title": "Global Optimisation of Black-Box Functions with Generative Models in the\n  Wasserstein Space", "author": "Tigran Ramazyan and Mikhail Hushchyn and Denis Derkach", "abstract": "  We propose a new uncertainty estimator for gradient-free optimisation of\nblack-box simulators using deep generative surrogate models. Optimisation of\nthese simulators is especially challenging for stochastic simulators and higher\ndimensions. To address these issues, we utilise a deep generative surrogate\napproach to model the black box response for the entire parameter space. We\nthen leverage this knowledge to estimate the proposed uncertainty based on the\nWasserstein distance - the Wasserstein uncertainty. This approach is employed\nin a posterior agnostic gradient-free optimisation algorithm that minimises\nregret over the entire parameter space. A series of tests were conducted to\ndemonstrate that our method is more robust to the shape of both the black box\nfunction and the stochastic response of the black box than state-of-the-art\nmethods, such as efficient global optimisation with a deep Gaussian process\nsurrogate.\n", "link": "http://arxiv.org/abs/2407.11917v2", "date": "2024-08-14", "relevancy": 2.4655, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5058}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4904}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Global%20Optimisation%20of%20Black-Box%20Functions%20with%20Generative%20Models%20in%20the%0A%20%20Wasserstein%20Space&body=Title%3A%20Global%20Optimisation%20of%20Black-Box%20Functions%20with%20Generative%20Models%20in%20the%0A%20%20Wasserstein%20Space%0AAuthor%3A%20Tigran%20Ramazyan%20and%20Mikhail%20Hushchyn%20and%20Denis%20Derkach%0AAbstract%3A%20%20%20We%20propose%20a%20new%20uncertainty%20estimator%20for%20gradient-free%20optimisation%20of%0Ablack-box%20simulators%20using%20deep%20generative%20surrogate%20models.%20Optimisation%20of%0Athese%20simulators%20is%20especially%20challenging%20for%20stochastic%20simulators%20and%20higher%0Adimensions.%20To%20address%20these%20issues%2C%20we%20utilise%20a%20deep%20generative%20surrogate%0Aapproach%20to%20model%20the%20black%20box%20response%20for%20the%20entire%20parameter%20space.%20We%0Athen%20leverage%20this%20knowledge%20to%20estimate%20the%20proposed%20uncertainty%20based%20on%20the%0AWasserstein%20distance%20-%20the%20Wasserstein%20uncertainty.%20This%20approach%20is%20employed%0Ain%20a%20posterior%20agnostic%20gradient-free%20optimisation%20algorithm%20that%20minimises%0Aregret%20over%20the%20entire%20parameter%20space.%20A%20series%20of%20tests%20were%20conducted%20to%0Ademonstrate%20that%20our%20method%20is%20more%20robust%20to%20the%20shape%20of%20both%20the%20black%20box%0Afunction%20and%20the%20stochastic%20response%20of%20the%20black%20box%20than%20state-of-the-art%0Amethods%2C%20such%20as%20efficient%20global%20optimisation%20with%20a%20deep%20Gaussian%20process%0Asurrogate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.11917v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGlobal%2520Optimisation%2520of%2520Black-Box%2520Functions%2520with%2520Generative%2520Models%2520in%2520the%250A%2520%2520Wasserstein%2520Space%26entry.906535625%3DTigran%2520Ramazyan%2520and%2520Mikhail%2520Hushchyn%2520and%2520Denis%2520Derkach%26entry.1292438233%3D%2520%2520We%2520propose%2520a%2520new%2520uncertainty%2520estimator%2520for%2520gradient-free%2520optimisation%2520of%250Ablack-box%2520simulators%2520using%2520deep%2520generative%2520surrogate%2520models.%2520Optimisation%2520of%250Athese%2520simulators%2520is%2520especially%2520challenging%2520for%2520stochastic%2520simulators%2520and%2520higher%250Adimensions.%2520To%2520address%2520these%2520issues%252C%2520we%2520utilise%2520a%2520deep%2520generative%2520surrogate%250Aapproach%2520to%2520model%2520the%2520black%2520box%2520response%2520for%2520the%2520entire%2520parameter%2520space.%2520We%250Athen%2520leverage%2520this%2520knowledge%2520to%2520estimate%2520the%2520proposed%2520uncertainty%2520based%2520on%2520the%250AWasserstein%2520distance%2520-%2520the%2520Wasserstein%2520uncertainty.%2520This%2520approach%2520is%2520employed%250Ain%2520a%2520posterior%2520agnostic%2520gradient-free%2520optimisation%2520algorithm%2520that%2520minimises%250Aregret%2520over%2520the%2520entire%2520parameter%2520space.%2520A%2520series%2520of%2520tests%2520were%2520conducted%2520to%250Ademonstrate%2520that%2520our%2520method%2520is%2520more%2520robust%2520to%2520the%2520shape%2520of%2520both%2520the%2520black%2520box%250Afunction%2520and%2520the%2520stochastic%2520response%2520of%2520the%2520black%2520box%2520than%2520state-of-the-art%250Amethods%252C%2520such%2520as%2520efficient%2520global%2520optimisation%2520with%2520a%2520deep%2520Gaussian%2520process%250Asurrogate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.11917v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Global%20Optimisation%20of%20Black-Box%20Functions%20with%20Generative%20Models%20in%20the%0A%20%20Wasserstein%20Space&entry.906535625=Tigran%20Ramazyan%20and%20Mikhail%20Hushchyn%20and%20Denis%20Derkach&entry.1292438233=%20%20We%20propose%20a%20new%20uncertainty%20estimator%20for%20gradient-free%20optimisation%20of%0Ablack-box%20simulators%20using%20deep%20generative%20surrogate%20models.%20Optimisation%20of%0Athese%20simulators%20is%20especially%20challenging%20for%20stochastic%20simulators%20and%20higher%0Adimensions.%20To%20address%20these%20issues%2C%20we%20utilise%20a%20deep%20generative%20surrogate%0Aapproach%20to%20model%20the%20black%20box%20response%20for%20the%20entire%20parameter%20space.%20We%0Athen%20leverage%20this%20knowledge%20to%20estimate%20the%20proposed%20uncertainty%20based%20on%20the%0AWasserstein%20distance%20-%20the%20Wasserstein%20uncertainty.%20This%20approach%20is%20employed%0Ain%20a%20posterior%20agnostic%20gradient-free%20optimisation%20algorithm%20that%20minimises%0Aregret%20over%20the%20entire%20parameter%20space.%20A%20series%20of%20tests%20were%20conducted%20to%0Ademonstrate%20that%20our%20method%20is%20more%20robust%20to%20the%20shape%20of%20both%20the%20black%20box%0Afunction%20and%20the%20stochastic%20response%20of%20the%20black%20box%20than%20state-of-the-art%0Amethods%2C%20such%20as%20efficient%20global%20optimisation%20with%20a%20deep%20Gaussian%20process%0Asurrogate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.11917v2&entry.124074799=Read"},
{"title": "DifuzCam: Replacing Camera Lens with a Mask and a Diffusion Model", "author": "Erez Yosef and Raja Giryes", "abstract": "  The flat lensless camera design reduces the camera size and weight\nsignificantly. In this design, the camera lens is replaced by another optical\nelement that interferes with the incoming light. The image is recovered from\nthe raw sensor measurements using a reconstruction algorithm. Yet, the quality\nof the reconstructed images is not satisfactory. To mitigate this, we propose\nutilizing a pre-trained diffusion model with a control network and a learned\nseparable transformation for reconstruction. This allows us to build a\nprototype flat camera with high-quality imaging, presenting state-of-the-art\nresults in both terms of quality and perceptuality. We demonstrate its ability\nto leverage also textual descriptions of the captured scene to further enhance\nreconstruction. Our reconstruction method which leverages the strong\ncapabilities of a pre-trained diffusion model can be used in other imaging\nsystems for improved reconstruction results.\n", "link": "http://arxiv.org/abs/2408.07541v1", "date": "2024-08-14", "relevancy": 2.4524, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6173}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6122}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.6122}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DifuzCam%3A%20Replacing%20Camera%20Lens%20with%20a%20Mask%20and%20a%20Diffusion%20Model&body=Title%3A%20DifuzCam%3A%20Replacing%20Camera%20Lens%20with%20a%20Mask%20and%20a%20Diffusion%20Model%0AAuthor%3A%20Erez%20Yosef%20and%20Raja%20Giryes%0AAbstract%3A%20%20%20The%20flat%20lensless%20camera%20design%20reduces%20the%20camera%20size%20and%20weight%0Asignificantly.%20In%20this%20design%2C%20the%20camera%20lens%20is%20replaced%20by%20another%20optical%0Aelement%20that%20interferes%20with%20the%20incoming%20light.%20The%20image%20is%20recovered%20from%0Athe%20raw%20sensor%20measurements%20using%20a%20reconstruction%20algorithm.%20Yet%2C%20the%20quality%0Aof%20the%20reconstructed%20images%20is%20not%20satisfactory.%20To%20mitigate%20this%2C%20we%20propose%0Autilizing%20a%20pre-trained%20diffusion%20model%20with%20a%20control%20network%20and%20a%20learned%0Aseparable%20transformation%20for%20reconstruction.%20This%20allows%20us%20to%20build%20a%0Aprototype%20flat%20camera%20with%20high-quality%20imaging%2C%20presenting%20state-of-the-art%0Aresults%20in%20both%20terms%20of%20quality%20and%20perceptuality.%20We%20demonstrate%20its%20ability%0Ato%20leverage%20also%20textual%20descriptions%20of%20the%20captured%20scene%20to%20further%20enhance%0Areconstruction.%20Our%20reconstruction%20method%20which%20leverages%20the%20strong%0Acapabilities%20of%20a%20pre-trained%20diffusion%20model%20can%20be%20used%20in%20other%20imaging%0Asystems%20for%20improved%20reconstruction%20results.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07541v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDifuzCam%253A%2520Replacing%2520Camera%2520Lens%2520with%2520a%2520Mask%2520and%2520a%2520Diffusion%2520Model%26entry.906535625%3DErez%2520Yosef%2520and%2520Raja%2520Giryes%26entry.1292438233%3D%2520%2520The%2520flat%2520lensless%2520camera%2520design%2520reduces%2520the%2520camera%2520size%2520and%2520weight%250Asignificantly.%2520In%2520this%2520design%252C%2520the%2520camera%2520lens%2520is%2520replaced%2520by%2520another%2520optical%250Aelement%2520that%2520interferes%2520with%2520the%2520incoming%2520light.%2520The%2520image%2520is%2520recovered%2520from%250Athe%2520raw%2520sensor%2520measurements%2520using%2520a%2520reconstruction%2520algorithm.%2520Yet%252C%2520the%2520quality%250Aof%2520the%2520reconstructed%2520images%2520is%2520not%2520satisfactory.%2520To%2520mitigate%2520this%252C%2520we%2520propose%250Autilizing%2520a%2520pre-trained%2520diffusion%2520model%2520with%2520a%2520control%2520network%2520and%2520a%2520learned%250Aseparable%2520transformation%2520for%2520reconstruction.%2520This%2520allows%2520us%2520to%2520build%2520a%250Aprototype%2520flat%2520camera%2520with%2520high-quality%2520imaging%252C%2520presenting%2520state-of-the-art%250Aresults%2520in%2520both%2520terms%2520of%2520quality%2520and%2520perceptuality.%2520We%2520demonstrate%2520its%2520ability%250Ato%2520leverage%2520also%2520textual%2520descriptions%2520of%2520the%2520captured%2520scene%2520to%2520further%2520enhance%250Areconstruction.%2520Our%2520reconstruction%2520method%2520which%2520leverages%2520the%2520strong%250Acapabilities%2520of%2520a%2520pre-trained%2520diffusion%2520model%2520can%2520be%2520used%2520in%2520other%2520imaging%250Asystems%2520for%2520improved%2520reconstruction%2520results.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07541v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DifuzCam%3A%20Replacing%20Camera%20Lens%20with%20a%20Mask%20and%20a%20Diffusion%20Model&entry.906535625=Erez%20Yosef%20and%20Raja%20Giryes&entry.1292438233=%20%20The%20flat%20lensless%20camera%20design%20reduces%20the%20camera%20size%20and%20weight%0Asignificantly.%20In%20this%20design%2C%20the%20camera%20lens%20is%20replaced%20by%20another%20optical%0Aelement%20that%20interferes%20with%20the%20incoming%20light.%20The%20image%20is%20recovered%20from%0Athe%20raw%20sensor%20measurements%20using%20a%20reconstruction%20algorithm.%20Yet%2C%20the%20quality%0Aof%20the%20reconstructed%20images%20is%20not%20satisfactory.%20To%20mitigate%20this%2C%20we%20propose%0Autilizing%20a%20pre-trained%20diffusion%20model%20with%20a%20control%20network%20and%20a%20learned%0Aseparable%20transformation%20for%20reconstruction.%20This%20allows%20us%20to%20build%20a%0Aprototype%20flat%20camera%20with%20high-quality%20imaging%2C%20presenting%20state-of-the-art%0Aresults%20in%20both%20terms%20of%20quality%20and%20perceptuality.%20We%20demonstrate%20its%20ability%0Ato%20leverage%20also%20textual%20descriptions%20of%20the%20captured%20scene%20to%20further%20enhance%0Areconstruction.%20Our%20reconstruction%20method%20which%20leverages%20the%20strong%0Acapabilities%20of%20a%20pre-trained%20diffusion%20model%20can%20be%20used%20in%20other%20imaging%0Asystems%20for%20improved%20reconstruction%20results.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07541v1&entry.124074799=Read"},
{"title": "Costal Cartilage Segmentation with Topology Guided Deformable Mamba:\n  Method and Benchmark", "author": "Senmao Wang and Haifan Gong and Runmeng Cui and Boyao Wan and Yicheng Liu and Zhonglin Hu and Haiqing Yang and Jingyang Zhou and Bo Pan and Lin Lin and Haiyue Jiang", "abstract": "  Costal cartilage segmentation is crucial to various medical applications,\nnecessitating precise and reliable techniques due to its complex anatomy and\nthe importance of accurate diagnosis and surgical planning. We propose a novel\ndeep learning-based approach called topology-guided deformable Mamba (TGDM) for\ncostal cartilage segmentation. The TGDM is tailored to capture the intricate\nlong-range costal cartilage relationships. Our method leverages a deformable\nmodel that integrates topological priors to enhance the adaptability and\naccuracy of the segmentation process. Furthermore, we developed a comprehensive\nbenchmark that contains 165 cases for costal cartilage segmentation. This\nbenchmark sets a new standard for evaluating costal cartilage segmentation\ntechniques and provides a valuable resource for future research. Extensive\nexperiments conducted on both in-domain benchmarks and out-of domain test sets\ndemonstrate the superiority of our approach over existing methods, showing\nsignificant improvements in segmentation precision and robustness.\n", "link": "http://arxiv.org/abs/2408.07444v1", "date": "2024-08-14", "relevancy": 2.4444, "topK": [{"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5093}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4826}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4748}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Costal%20Cartilage%20Segmentation%20with%20Topology%20Guided%20Deformable%20Mamba%3A%0A%20%20Method%20and%20Benchmark&body=Title%3A%20Costal%20Cartilage%20Segmentation%20with%20Topology%20Guided%20Deformable%20Mamba%3A%0A%20%20Method%20and%20Benchmark%0AAuthor%3A%20Senmao%20Wang%20and%20Haifan%20Gong%20and%20Runmeng%20Cui%20and%20Boyao%20Wan%20and%20Yicheng%20Liu%20and%20Zhonglin%20Hu%20and%20Haiqing%20Yang%20and%20Jingyang%20Zhou%20and%20Bo%20Pan%20and%20Lin%20Lin%20and%20Haiyue%20Jiang%0AAbstract%3A%20%20%20Costal%20cartilage%20segmentation%20is%20crucial%20to%20various%20medical%20applications%2C%0Anecessitating%20precise%20and%20reliable%20techniques%20due%20to%20its%20complex%20anatomy%20and%0Athe%20importance%20of%20accurate%20diagnosis%20and%20surgical%20planning.%20We%20propose%20a%20novel%0Adeep%20learning-based%20approach%20called%20topology-guided%20deformable%20Mamba%20%28TGDM%29%20for%0Acostal%20cartilage%20segmentation.%20The%20TGDM%20is%20tailored%20to%20capture%20the%20intricate%0Along-range%20costal%20cartilage%20relationships.%20Our%20method%20leverages%20a%20deformable%0Amodel%20that%20integrates%20topological%20priors%20to%20enhance%20the%20adaptability%20and%0Aaccuracy%20of%20the%20segmentation%20process.%20Furthermore%2C%20we%20developed%20a%20comprehensive%0Abenchmark%20that%20contains%20165%20cases%20for%20costal%20cartilage%20segmentation.%20This%0Abenchmark%20sets%20a%20new%20standard%20for%20evaluating%20costal%20cartilage%20segmentation%0Atechniques%20and%20provides%20a%20valuable%20resource%20for%20future%20research.%20Extensive%0Aexperiments%20conducted%20on%20both%20in-domain%20benchmarks%20and%20out-of%20domain%20test%20sets%0Ademonstrate%20the%20superiority%20of%20our%20approach%20over%20existing%20methods%2C%20showing%0Asignificant%20improvements%20in%20segmentation%20precision%20and%20robustness.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07444v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCostal%2520Cartilage%2520Segmentation%2520with%2520Topology%2520Guided%2520Deformable%2520Mamba%253A%250A%2520%2520Method%2520and%2520Benchmark%26entry.906535625%3DSenmao%2520Wang%2520and%2520Haifan%2520Gong%2520and%2520Runmeng%2520Cui%2520and%2520Boyao%2520Wan%2520and%2520Yicheng%2520Liu%2520and%2520Zhonglin%2520Hu%2520and%2520Haiqing%2520Yang%2520and%2520Jingyang%2520Zhou%2520and%2520Bo%2520Pan%2520and%2520Lin%2520Lin%2520and%2520Haiyue%2520Jiang%26entry.1292438233%3D%2520%2520Costal%2520cartilage%2520segmentation%2520is%2520crucial%2520to%2520various%2520medical%2520applications%252C%250Anecessitating%2520precise%2520and%2520reliable%2520techniques%2520due%2520to%2520its%2520complex%2520anatomy%2520and%250Athe%2520importance%2520of%2520accurate%2520diagnosis%2520and%2520surgical%2520planning.%2520We%2520propose%2520a%2520novel%250Adeep%2520learning-based%2520approach%2520called%2520topology-guided%2520deformable%2520Mamba%2520%2528TGDM%2529%2520for%250Acostal%2520cartilage%2520segmentation.%2520The%2520TGDM%2520is%2520tailored%2520to%2520capture%2520the%2520intricate%250Along-range%2520costal%2520cartilage%2520relationships.%2520Our%2520method%2520leverages%2520a%2520deformable%250Amodel%2520that%2520integrates%2520topological%2520priors%2520to%2520enhance%2520the%2520adaptability%2520and%250Aaccuracy%2520of%2520the%2520segmentation%2520process.%2520Furthermore%252C%2520we%2520developed%2520a%2520comprehensive%250Abenchmark%2520that%2520contains%2520165%2520cases%2520for%2520costal%2520cartilage%2520segmentation.%2520This%250Abenchmark%2520sets%2520a%2520new%2520standard%2520for%2520evaluating%2520costal%2520cartilage%2520segmentation%250Atechniques%2520and%2520provides%2520a%2520valuable%2520resource%2520for%2520future%2520research.%2520Extensive%250Aexperiments%2520conducted%2520on%2520both%2520in-domain%2520benchmarks%2520and%2520out-of%2520domain%2520test%2520sets%250Ademonstrate%2520the%2520superiority%2520of%2520our%2520approach%2520over%2520existing%2520methods%252C%2520showing%250Asignificant%2520improvements%2520in%2520segmentation%2520precision%2520and%2520robustness.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07444v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Costal%20Cartilage%20Segmentation%20with%20Topology%20Guided%20Deformable%20Mamba%3A%0A%20%20Method%20and%20Benchmark&entry.906535625=Senmao%20Wang%20and%20Haifan%20Gong%20and%20Runmeng%20Cui%20and%20Boyao%20Wan%20and%20Yicheng%20Liu%20and%20Zhonglin%20Hu%20and%20Haiqing%20Yang%20and%20Jingyang%20Zhou%20and%20Bo%20Pan%20and%20Lin%20Lin%20and%20Haiyue%20Jiang&entry.1292438233=%20%20Costal%20cartilage%20segmentation%20is%20crucial%20to%20various%20medical%20applications%2C%0Anecessitating%20precise%20and%20reliable%20techniques%20due%20to%20its%20complex%20anatomy%20and%0Athe%20importance%20of%20accurate%20diagnosis%20and%20surgical%20planning.%20We%20propose%20a%20novel%0Adeep%20learning-based%20approach%20called%20topology-guided%20deformable%20Mamba%20%28TGDM%29%20for%0Acostal%20cartilage%20segmentation.%20The%20TGDM%20is%20tailored%20to%20capture%20the%20intricate%0Along-range%20costal%20cartilage%20relationships.%20Our%20method%20leverages%20a%20deformable%0Amodel%20that%20integrates%20topological%20priors%20to%20enhance%20the%20adaptability%20and%0Aaccuracy%20of%20the%20segmentation%20process.%20Furthermore%2C%20we%20developed%20a%20comprehensive%0Abenchmark%20that%20contains%20165%20cases%20for%20costal%20cartilage%20segmentation.%20This%0Abenchmark%20sets%20a%20new%20standard%20for%20evaluating%20costal%20cartilage%20segmentation%0Atechniques%20and%20provides%20a%20valuable%20resource%20for%20future%20research.%20Extensive%0Aexperiments%20conducted%20on%20both%20in-domain%20benchmarks%20and%20out-of%20domain%20test%20sets%0Ademonstrate%20the%20superiority%20of%20our%20approach%20over%20existing%20methods%2C%20showing%0Asignificant%20improvements%20in%20segmentation%20precision%20and%20robustness.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07444v1&entry.124074799=Read"},
{"title": "Agent Instructs Large Language Models to be General Zero-Shot Reasoners", "author": "Nicholas Crispino and Kyle Montgomery and Fankun Zeng and Dawn Song and Chenguang Wang", "abstract": "  We introduce a method to improve the zero-shot reasoning abilities of large\nlanguage models on general language understanding tasks. Specifically, we build\nan autonomous agent to instruct the reasoning process of large language models.\nWe show this approach further unleashes the zero-shot reasoning abilities of\nlarge language models to more tasks. We study the performance of our method on\na wide set of datasets spanning generation, classification, and reasoning. We\nshow that our method generalizes to most tasks and obtains state-of-the-art\nzero-shot performance on 20 of the 29 datasets that we evaluate. For instance,\nour method boosts the performance of state-of-the-art large language models by\na large margin, including Vicuna-13b (13.3%), Llama-2-70b-chat (23.2%), and\nGPT-3.5 Turbo (17.0%). Compared to zero-shot chain of thought, our improvement\nin reasoning is striking, with an average increase of 10.5%. With our method,\nLlama-2-70b-chat outperforms zero-shot GPT-3.5 Turbo by 10.2%.\n", "link": "http://arxiv.org/abs/2310.03710v2", "date": "2024-08-14", "relevancy": 2.4241, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5224}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4701}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4619}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Agent%20Instructs%20Large%20Language%20Models%20to%20be%20General%20Zero-Shot%20Reasoners&body=Title%3A%20Agent%20Instructs%20Large%20Language%20Models%20to%20be%20General%20Zero-Shot%20Reasoners%0AAuthor%3A%20Nicholas%20Crispino%20and%20Kyle%20Montgomery%20and%20Fankun%20Zeng%20and%20Dawn%20Song%20and%20Chenguang%20Wang%0AAbstract%3A%20%20%20We%20introduce%20a%20method%20to%20improve%20the%20zero-shot%20reasoning%20abilities%20of%20large%0Alanguage%20models%20on%20general%20language%20understanding%20tasks.%20Specifically%2C%20we%20build%0Aan%20autonomous%20agent%20to%20instruct%20the%20reasoning%20process%20of%20large%20language%20models.%0AWe%20show%20this%20approach%20further%20unleashes%20the%20zero-shot%20reasoning%20abilities%20of%0Alarge%20language%20models%20to%20more%20tasks.%20We%20study%20the%20performance%20of%20our%20method%20on%0Aa%20wide%20set%20of%20datasets%20spanning%20generation%2C%20classification%2C%20and%20reasoning.%20We%0Ashow%20that%20our%20method%20generalizes%20to%20most%20tasks%20and%20obtains%20state-of-the-art%0Azero-shot%20performance%20on%2020%20of%20the%2029%20datasets%20that%20we%20evaluate.%20For%20instance%2C%0Aour%20method%20boosts%20the%20performance%20of%20state-of-the-art%20large%20language%20models%20by%0Aa%20large%20margin%2C%20including%20Vicuna-13b%20%2813.3%25%29%2C%20Llama-2-70b-chat%20%2823.2%25%29%2C%20and%0AGPT-3.5%20Turbo%20%2817.0%25%29.%20Compared%20to%20zero-shot%20chain%20of%20thought%2C%20our%20improvement%0Ain%20reasoning%20is%20striking%2C%20with%20an%20average%20increase%20of%2010.5%25.%20With%20our%20method%2C%0ALlama-2-70b-chat%20outperforms%20zero-shot%20GPT-3.5%20Turbo%20by%2010.2%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.03710v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAgent%2520Instructs%2520Large%2520Language%2520Models%2520to%2520be%2520General%2520Zero-Shot%2520Reasoners%26entry.906535625%3DNicholas%2520Crispino%2520and%2520Kyle%2520Montgomery%2520and%2520Fankun%2520Zeng%2520and%2520Dawn%2520Song%2520and%2520Chenguang%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520a%2520method%2520to%2520improve%2520the%2520zero-shot%2520reasoning%2520abilities%2520of%2520large%250Alanguage%2520models%2520on%2520general%2520language%2520understanding%2520tasks.%2520Specifically%252C%2520we%2520build%250Aan%2520autonomous%2520agent%2520to%2520instruct%2520the%2520reasoning%2520process%2520of%2520large%2520language%2520models.%250AWe%2520show%2520this%2520approach%2520further%2520unleashes%2520the%2520zero-shot%2520reasoning%2520abilities%2520of%250Alarge%2520language%2520models%2520to%2520more%2520tasks.%2520We%2520study%2520the%2520performance%2520of%2520our%2520method%2520on%250Aa%2520wide%2520set%2520of%2520datasets%2520spanning%2520generation%252C%2520classification%252C%2520and%2520reasoning.%2520We%250Ashow%2520that%2520our%2520method%2520generalizes%2520to%2520most%2520tasks%2520and%2520obtains%2520state-of-the-art%250Azero-shot%2520performance%2520on%252020%2520of%2520the%252029%2520datasets%2520that%2520we%2520evaluate.%2520For%2520instance%252C%250Aour%2520method%2520boosts%2520the%2520performance%2520of%2520state-of-the-art%2520large%2520language%2520models%2520by%250Aa%2520large%2520margin%252C%2520including%2520Vicuna-13b%2520%252813.3%2525%2529%252C%2520Llama-2-70b-chat%2520%252823.2%2525%2529%252C%2520and%250AGPT-3.5%2520Turbo%2520%252817.0%2525%2529.%2520Compared%2520to%2520zero-shot%2520chain%2520of%2520thought%252C%2520our%2520improvement%250Ain%2520reasoning%2520is%2520striking%252C%2520with%2520an%2520average%2520increase%2520of%252010.5%2525.%2520With%2520our%2520method%252C%250ALlama-2-70b-chat%2520outperforms%2520zero-shot%2520GPT-3.5%2520Turbo%2520by%252010.2%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2310.03710v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Agent%20Instructs%20Large%20Language%20Models%20to%20be%20General%20Zero-Shot%20Reasoners&entry.906535625=Nicholas%20Crispino%20and%20Kyle%20Montgomery%20and%20Fankun%20Zeng%20and%20Dawn%20Song%20and%20Chenguang%20Wang&entry.1292438233=%20%20We%20introduce%20a%20method%20to%20improve%20the%20zero-shot%20reasoning%20abilities%20of%20large%0Alanguage%20models%20on%20general%20language%20understanding%20tasks.%20Specifically%2C%20we%20build%0Aan%20autonomous%20agent%20to%20instruct%20the%20reasoning%20process%20of%20large%20language%20models.%0AWe%20show%20this%20approach%20further%20unleashes%20the%20zero-shot%20reasoning%20abilities%20of%0Alarge%20language%20models%20to%20more%20tasks.%20We%20study%20the%20performance%20of%20our%20method%20on%0Aa%20wide%20set%20of%20datasets%20spanning%20generation%2C%20classification%2C%20and%20reasoning.%20We%0Ashow%20that%20our%20method%20generalizes%20to%20most%20tasks%20and%20obtains%20state-of-the-art%0Azero-shot%20performance%20on%2020%20of%20the%2029%20datasets%20that%20we%20evaluate.%20For%20instance%2C%0Aour%20method%20boosts%20the%20performance%20of%20state-of-the-art%20large%20language%20models%20by%0Aa%20large%20margin%2C%20including%20Vicuna-13b%20%2813.3%25%29%2C%20Llama-2-70b-chat%20%2823.2%25%29%2C%20and%0AGPT-3.5%20Turbo%20%2817.0%25%29.%20Compared%20to%20zero-shot%20chain%20of%20thought%2C%20our%20improvement%0Ain%20reasoning%20is%20striking%2C%20with%20an%20average%20increase%20of%2010.5%25.%20With%20our%20method%2C%0ALlama-2-70b-chat%20outperforms%20zero-shot%20GPT-3.5%20Turbo%20by%2010.2%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.03710v2&entry.124074799=Read"},
{"title": "G$^2$V$^2$former: Graph Guided Video Vision Transformer for Face\n  Anti-Spoofing", "author": "Jingyi Yang and Zitong Yu and Xiuming Ni and Jia He and Hui Li", "abstract": "  In videos containing spoofed faces, we may uncover the spoofing evidence\nbased on either photometric or dynamic abnormality, even a combination of both.\nPrevailing face anti-spoofing (FAS) approaches generally concentrate on the\nsingle-frame scenario, however, purely photometric-driven methods overlook the\ndynamic spoofing clues that may be exposed over time. This may lead FAS systems\nto conclude incorrect judgments, especially in cases where it is easily\ndistinguishable in terms of dynamics but challenging to discern in terms of\nphotometrics. To this end, we propose the Graph Guided Video Vision Transformer\n(G$^2$V$^2$former), which combines faces with facial landmarks for photometric\nand dynamic feature fusion. We factorize the attention into space and time, and\nfuse them via a spatiotemporal block. Specifically, we design a novel temporal\nattention called Kronecker temporal attention, which has a wider receptive\nfield, and is beneficial for capturing dynamic information. Moreover, we\nleverage the low-semantic motion of facial landmarks to guide the high-semantic\nchange of facial expressions based on the motivation that regions containing\nlandmarks may reveal more dynamic clues. Extensive experiments on nine\nbenchmark datasets demonstrate that our method achieves superior performance\nunder various scenarios. The codes will be released soon.\n", "link": "http://arxiv.org/abs/2408.07675v1", "date": "2024-08-14", "relevancy": 2.3829, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.609}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.596}, {"title": "3D Gaussian Blendshapes for Head Avatar Animation", "link": "http://arxiv.org/abs/2404.19398v2", "similarity": 0.5823}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20G%24%5E2%24V%24%5E2%24former%3A%20Graph%20Guided%20Video%20Vision%20Transformer%20for%20Face%0A%20%20Anti-Spoofing&body=Title%3A%20G%24%5E2%24V%24%5E2%24former%3A%20Graph%20Guided%20Video%20Vision%20Transformer%20for%20Face%0A%20%20Anti-Spoofing%0AAuthor%3A%20Jingyi%20Yang%20and%20Zitong%20Yu%20and%20Xiuming%20Ni%20and%20Jia%20He%20and%20Hui%20Li%0AAbstract%3A%20%20%20In%20videos%20containing%20spoofed%20faces%2C%20we%20may%20uncover%20the%20spoofing%20evidence%0Abased%20on%20either%20photometric%20or%20dynamic%20abnormality%2C%20even%20a%20combination%20of%20both.%0APrevailing%20face%20anti-spoofing%20%28FAS%29%20approaches%20generally%20concentrate%20on%20the%0Asingle-frame%20scenario%2C%20however%2C%20purely%20photometric-driven%20methods%20overlook%20the%0Adynamic%20spoofing%20clues%20that%20may%20be%20exposed%20over%20time.%20This%20may%20lead%20FAS%20systems%0Ato%20conclude%20incorrect%20judgments%2C%20especially%20in%20cases%20where%20it%20is%20easily%0Adistinguishable%20in%20terms%20of%20dynamics%20but%20challenging%20to%20discern%20in%20terms%20of%0Aphotometrics.%20To%20this%20end%2C%20we%20propose%20the%20Graph%20Guided%20Video%20Vision%20Transformer%0A%28G%24%5E2%24V%24%5E2%24former%29%2C%20which%20combines%20faces%20with%20facial%20landmarks%20for%20photometric%0Aand%20dynamic%20feature%20fusion.%20We%20factorize%20the%20attention%20into%20space%20and%20time%2C%20and%0Afuse%20them%20via%20a%20spatiotemporal%20block.%20Specifically%2C%20we%20design%20a%20novel%20temporal%0Aattention%20called%20Kronecker%20temporal%20attention%2C%20which%20has%20a%20wider%20receptive%0Afield%2C%20and%20is%20beneficial%20for%20capturing%20dynamic%20information.%20Moreover%2C%20we%0Aleverage%20the%20low-semantic%20motion%20of%20facial%20landmarks%20to%20guide%20the%20high-semantic%0Achange%20of%20facial%20expressions%20based%20on%20the%20motivation%20that%20regions%20containing%0Alandmarks%20may%20reveal%20more%20dynamic%20clues.%20Extensive%20experiments%20on%20nine%0Abenchmark%20datasets%20demonstrate%20that%20our%20method%20achieves%20superior%20performance%0Aunder%20various%20scenarios.%20The%20codes%20will%20be%20released%20soon.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07675v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DG%2524%255E2%2524V%2524%255E2%2524former%253A%2520Graph%2520Guided%2520Video%2520Vision%2520Transformer%2520for%2520Face%250A%2520%2520Anti-Spoofing%26entry.906535625%3DJingyi%2520Yang%2520and%2520Zitong%2520Yu%2520and%2520Xiuming%2520Ni%2520and%2520Jia%2520He%2520and%2520Hui%2520Li%26entry.1292438233%3D%2520%2520In%2520videos%2520containing%2520spoofed%2520faces%252C%2520we%2520may%2520uncover%2520the%2520spoofing%2520evidence%250Abased%2520on%2520either%2520photometric%2520or%2520dynamic%2520abnormality%252C%2520even%2520a%2520combination%2520of%2520both.%250APrevailing%2520face%2520anti-spoofing%2520%2528FAS%2529%2520approaches%2520generally%2520concentrate%2520on%2520the%250Asingle-frame%2520scenario%252C%2520however%252C%2520purely%2520photometric-driven%2520methods%2520overlook%2520the%250Adynamic%2520spoofing%2520clues%2520that%2520may%2520be%2520exposed%2520over%2520time.%2520This%2520may%2520lead%2520FAS%2520systems%250Ato%2520conclude%2520incorrect%2520judgments%252C%2520especially%2520in%2520cases%2520where%2520it%2520is%2520easily%250Adistinguishable%2520in%2520terms%2520of%2520dynamics%2520but%2520challenging%2520to%2520discern%2520in%2520terms%2520of%250Aphotometrics.%2520To%2520this%2520end%252C%2520we%2520propose%2520the%2520Graph%2520Guided%2520Video%2520Vision%2520Transformer%250A%2528G%2524%255E2%2524V%2524%255E2%2524former%2529%252C%2520which%2520combines%2520faces%2520with%2520facial%2520landmarks%2520for%2520photometric%250Aand%2520dynamic%2520feature%2520fusion.%2520We%2520factorize%2520the%2520attention%2520into%2520space%2520and%2520time%252C%2520and%250Afuse%2520them%2520via%2520a%2520spatiotemporal%2520block.%2520Specifically%252C%2520we%2520design%2520a%2520novel%2520temporal%250Aattention%2520called%2520Kronecker%2520temporal%2520attention%252C%2520which%2520has%2520a%2520wider%2520receptive%250Afield%252C%2520and%2520is%2520beneficial%2520for%2520capturing%2520dynamic%2520information.%2520Moreover%252C%2520we%250Aleverage%2520the%2520low-semantic%2520motion%2520of%2520facial%2520landmarks%2520to%2520guide%2520the%2520high-semantic%250Achange%2520of%2520facial%2520expressions%2520based%2520on%2520the%2520motivation%2520that%2520regions%2520containing%250Alandmarks%2520may%2520reveal%2520more%2520dynamic%2520clues.%2520Extensive%2520experiments%2520on%2520nine%250Abenchmark%2520datasets%2520demonstrate%2520that%2520our%2520method%2520achieves%2520superior%2520performance%250Aunder%2520various%2520scenarios.%2520The%2520codes%2520will%2520be%2520released%2520soon.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07675v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=G%24%5E2%24V%24%5E2%24former%3A%20Graph%20Guided%20Video%20Vision%20Transformer%20for%20Face%0A%20%20Anti-Spoofing&entry.906535625=Jingyi%20Yang%20and%20Zitong%20Yu%20and%20Xiuming%20Ni%20and%20Jia%20He%20and%20Hui%20Li&entry.1292438233=%20%20In%20videos%20containing%20spoofed%20faces%2C%20we%20may%20uncover%20the%20spoofing%20evidence%0Abased%20on%20either%20photometric%20or%20dynamic%20abnormality%2C%20even%20a%20combination%20of%20both.%0APrevailing%20face%20anti-spoofing%20%28FAS%29%20approaches%20generally%20concentrate%20on%20the%0Asingle-frame%20scenario%2C%20however%2C%20purely%20photometric-driven%20methods%20overlook%20the%0Adynamic%20spoofing%20clues%20that%20may%20be%20exposed%20over%20time.%20This%20may%20lead%20FAS%20systems%0Ato%20conclude%20incorrect%20judgments%2C%20especially%20in%20cases%20where%20it%20is%20easily%0Adistinguishable%20in%20terms%20of%20dynamics%20but%20challenging%20to%20discern%20in%20terms%20of%0Aphotometrics.%20To%20this%20end%2C%20we%20propose%20the%20Graph%20Guided%20Video%20Vision%20Transformer%0A%28G%24%5E2%24V%24%5E2%24former%29%2C%20which%20combines%20faces%20with%20facial%20landmarks%20for%20photometric%0Aand%20dynamic%20feature%20fusion.%20We%20factorize%20the%20attention%20into%20space%20and%20time%2C%20and%0Afuse%20them%20via%20a%20spatiotemporal%20block.%20Specifically%2C%20we%20design%20a%20novel%20temporal%0Aattention%20called%20Kronecker%20temporal%20attention%2C%20which%20has%20a%20wider%20receptive%0Afield%2C%20and%20is%20beneficial%20for%20capturing%20dynamic%20information.%20Moreover%2C%20we%0Aleverage%20the%20low-semantic%20motion%20of%20facial%20landmarks%20to%20guide%20the%20high-semantic%0Achange%20of%20facial%20expressions%20based%20on%20the%20motivation%20that%20regions%20containing%0Alandmarks%20may%20reveal%20more%20dynamic%20clues.%20Extensive%20experiments%20on%20nine%0Abenchmark%20datasets%20demonstrate%20that%20our%20method%20achieves%20superior%20performance%0Aunder%20various%20scenarios.%20The%20codes%20will%20be%20released%20soon.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07675v1&entry.124074799=Read"},
{"title": "A Comprehensive Survey on Synthetic Infrared Image synthesis", "author": "Avinash Upadhyay and Manoj sharma and Prerana Mukherjee and Amit Singhal and Brejesh Lall", "abstract": "  Synthetic infrared (IR) scene and target generation is an important computer\nvision problem as it allows the generation of realistic IR images and targets\nfor training and testing of various applications, such as remote sensing,\nsurveillance, and target recognition. It also helps reduce the cost and risk\nassociated with collecting real-world IR data. This survey paper aims to\nprovide a comprehensive overview of the conventional mathematical\nmodelling-based methods and deep learning-based methods used for generating\nsynthetic IR scenes and targets. The paper discusses the importance of\nsynthetic IR scene and target generation and briefly covers the mathematics of\nblackbody and grey body radiations, as well as IR image-capturing methods. The\npotential use cases of synthetic IR scenes and target generation are also\ndescribed, highlighting the significance of these techniques in various fields.\nAdditionally, the paper explores possible new ways of developing new techniques\nto enhance the efficiency and effectiveness of synthetic IR scenes and target\ngeneration while highlighting the need for further research to advance this\nfield.\n", "link": "http://arxiv.org/abs/2408.06868v2", "date": "2024-08-14", "relevancy": 2.3719, "topK": [{"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4758}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.4758}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Survey%20on%20Synthetic%20Infrared%20Image%20synthesis&body=Title%3A%20A%20Comprehensive%20Survey%20on%20Synthetic%20Infrared%20Image%20synthesis%0AAuthor%3A%20Avinash%20Upadhyay%20and%20Manoj%20sharma%20and%20Prerana%20Mukherjee%20and%20Amit%20Singhal%20and%20Brejesh%20Lall%0AAbstract%3A%20%20%20Synthetic%20infrared%20%28IR%29%20scene%20and%20target%20generation%20is%20an%20important%20computer%0Avision%20problem%20as%20it%20allows%20the%20generation%20of%20realistic%20IR%20images%20and%20targets%0Afor%20training%20and%20testing%20of%20various%20applications%2C%20such%20as%20remote%20sensing%2C%0Asurveillance%2C%20and%20target%20recognition.%20It%20also%20helps%20reduce%20the%20cost%20and%20risk%0Aassociated%20with%20collecting%20real-world%20IR%20data.%20This%20survey%20paper%20aims%20to%0Aprovide%20a%20comprehensive%20overview%20of%20the%20conventional%20mathematical%0Amodelling-based%20methods%20and%20deep%20learning-based%20methods%20used%20for%20generating%0Asynthetic%20IR%20scenes%20and%20targets.%20The%20paper%20discusses%20the%20importance%20of%0Asynthetic%20IR%20scene%20and%20target%20generation%20and%20briefly%20covers%20the%20mathematics%20of%0Ablackbody%20and%20grey%20body%20radiations%2C%20as%20well%20as%20IR%20image-capturing%20methods.%20The%0Apotential%20use%20cases%20of%20synthetic%20IR%20scenes%20and%20target%20generation%20are%20also%0Adescribed%2C%20highlighting%20the%20significance%20of%20these%20techniques%20in%20various%20fields.%0AAdditionally%2C%20the%20paper%20explores%20possible%20new%20ways%20of%20developing%20new%20techniques%0Ato%20enhance%20the%20efficiency%20and%20effectiveness%20of%20synthetic%20IR%20scenes%20and%20target%0Ageneration%20while%20highlighting%20the%20need%20for%20further%20research%20to%20advance%20this%0Afield.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06868v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Comprehensive%2520Survey%2520on%2520Synthetic%2520Infrared%2520Image%2520synthesis%26entry.906535625%3DAvinash%2520Upadhyay%2520and%2520Manoj%2520sharma%2520and%2520Prerana%2520Mukherjee%2520and%2520Amit%2520Singhal%2520and%2520Brejesh%2520Lall%26entry.1292438233%3D%2520%2520Synthetic%2520infrared%2520%2528IR%2529%2520scene%2520and%2520target%2520generation%2520is%2520an%2520important%2520computer%250Avision%2520problem%2520as%2520it%2520allows%2520the%2520generation%2520of%2520realistic%2520IR%2520images%2520and%2520targets%250Afor%2520training%2520and%2520testing%2520of%2520various%2520applications%252C%2520such%2520as%2520remote%2520sensing%252C%250Asurveillance%252C%2520and%2520target%2520recognition.%2520It%2520also%2520helps%2520reduce%2520the%2520cost%2520and%2520risk%250Aassociated%2520with%2520collecting%2520real-world%2520IR%2520data.%2520This%2520survey%2520paper%2520aims%2520to%250Aprovide%2520a%2520comprehensive%2520overview%2520of%2520the%2520conventional%2520mathematical%250Amodelling-based%2520methods%2520and%2520deep%2520learning-based%2520methods%2520used%2520for%2520generating%250Asynthetic%2520IR%2520scenes%2520and%2520targets.%2520The%2520paper%2520discusses%2520the%2520importance%2520of%250Asynthetic%2520IR%2520scene%2520and%2520target%2520generation%2520and%2520briefly%2520covers%2520the%2520mathematics%2520of%250Ablackbody%2520and%2520grey%2520body%2520radiations%252C%2520as%2520well%2520as%2520IR%2520image-capturing%2520methods.%2520The%250Apotential%2520use%2520cases%2520of%2520synthetic%2520IR%2520scenes%2520and%2520target%2520generation%2520are%2520also%250Adescribed%252C%2520highlighting%2520the%2520significance%2520of%2520these%2520techniques%2520in%2520various%2520fields.%250AAdditionally%252C%2520the%2520paper%2520explores%2520possible%2520new%2520ways%2520of%2520developing%2520new%2520techniques%250Ato%2520enhance%2520the%2520efficiency%2520and%2520effectiveness%2520of%2520synthetic%2520IR%2520scenes%2520and%2520target%250Ageneration%2520while%2520highlighting%2520the%2520need%2520for%2520further%2520research%2520to%2520advance%2520this%250Afield.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06868v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Survey%20on%20Synthetic%20Infrared%20Image%20synthesis&entry.906535625=Avinash%20Upadhyay%20and%20Manoj%20sharma%20and%20Prerana%20Mukherjee%20and%20Amit%20Singhal%20and%20Brejesh%20Lall&entry.1292438233=%20%20Synthetic%20infrared%20%28IR%29%20scene%20and%20target%20generation%20is%20an%20important%20computer%0Avision%20problem%20as%20it%20allows%20the%20generation%20of%20realistic%20IR%20images%20and%20targets%0Afor%20training%20and%20testing%20of%20various%20applications%2C%20such%20as%20remote%20sensing%2C%0Asurveillance%2C%20and%20target%20recognition.%20It%20also%20helps%20reduce%20the%20cost%20and%20risk%0Aassociated%20with%20collecting%20real-world%20IR%20data.%20This%20survey%20paper%20aims%20to%0Aprovide%20a%20comprehensive%20overview%20of%20the%20conventional%20mathematical%0Amodelling-based%20methods%20and%20deep%20learning-based%20methods%20used%20for%20generating%0Asynthetic%20IR%20scenes%20and%20targets.%20The%20paper%20discusses%20the%20importance%20of%0Asynthetic%20IR%20scene%20and%20target%20generation%20and%20briefly%20covers%20the%20mathematics%20of%0Ablackbody%20and%20grey%20body%20radiations%2C%20as%20well%20as%20IR%20image-capturing%20methods.%20The%0Apotential%20use%20cases%20of%20synthetic%20IR%20scenes%20and%20target%20generation%20are%20also%0Adescribed%2C%20highlighting%20the%20significance%20of%20these%20techniques%20in%20various%20fields.%0AAdditionally%2C%20the%20paper%20explores%20possible%20new%20ways%20of%20developing%20new%20techniques%0Ato%20enhance%20the%20efficiency%20and%20effectiveness%20of%20synthetic%20IR%20scenes%20and%20target%0Ageneration%20while%20highlighting%20the%20need%20for%20further%20research%20to%20advance%20this%0Afield.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06868v2&entry.124074799=Read"},
{"title": "RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequential\n  Recommenders", "author": "Danil Gusak and Gleb Mezentsev and Ivan Oseledets and Evgeny Frolov", "abstract": "  Scalability is a major challenge in modern recommender systems. In sequential\nrecommendations, full Cross-Entropy (CE) loss achieves state-of-the-art\nrecommendation quality but consumes excessive GPU memory with large item\ncatalogs, limiting its practicality. Using a GPU-efficient locality-sensitive\nhashing-like algorithm for approximating large tensor of logits, this paper\nintroduces a novel RECE (REduced Cross-Entropy) loss. RECE significantly\nreduces memory consumption while allowing one to enjoy the state-of-the-art\nperformance of full CE loss. Experimental results on various datasets show that\nRECE cuts training peak memory usage by up to 12 times compared to existing\nmethods while retaining or exceeding performance metrics of CE loss. The\napproach also opens up new possibilities for large-scale applications in other\ndomains.\n", "link": "http://arxiv.org/abs/2408.02354v3", "date": "2024-08-14", "relevancy": 2.3552, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4795}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4728}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4608}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RECE%3A%20Reduced%20Cross-Entropy%20Loss%20for%20Large-Catalogue%20Sequential%0A%20%20Recommenders&body=Title%3A%20RECE%3A%20Reduced%20Cross-Entropy%20Loss%20for%20Large-Catalogue%20Sequential%0A%20%20Recommenders%0AAuthor%3A%20Danil%20Gusak%20and%20Gleb%20Mezentsev%20and%20Ivan%20Oseledets%20and%20Evgeny%20Frolov%0AAbstract%3A%20%20%20Scalability%20is%20a%20major%20challenge%20in%20modern%20recommender%20systems.%20In%20sequential%0Arecommendations%2C%20full%20Cross-Entropy%20%28CE%29%20loss%20achieves%20state-of-the-art%0Arecommendation%20quality%20but%20consumes%20excessive%20GPU%20memory%20with%20large%20item%0Acatalogs%2C%20limiting%20its%20practicality.%20Using%20a%20GPU-efficient%20locality-sensitive%0Ahashing-like%20algorithm%20for%20approximating%20large%20tensor%20of%20logits%2C%20this%20paper%0Aintroduces%20a%20novel%20RECE%20%28REduced%20Cross-Entropy%29%20loss.%20RECE%20significantly%0Areduces%20memory%20consumption%20while%20allowing%20one%20to%20enjoy%20the%20state-of-the-art%0Aperformance%20of%20full%20CE%20loss.%20Experimental%20results%20on%20various%20datasets%20show%20that%0ARECE%20cuts%20training%20peak%20memory%20usage%20by%20up%20to%2012%20times%20compared%20to%20existing%0Amethods%20while%20retaining%20or%20exceeding%20performance%20metrics%20of%20CE%20loss.%20The%0Aapproach%20also%20opens%20up%20new%20possibilities%20for%20large-scale%20applications%20in%20other%0Adomains.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.02354v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRECE%253A%2520Reduced%2520Cross-Entropy%2520Loss%2520for%2520Large-Catalogue%2520Sequential%250A%2520%2520Recommenders%26entry.906535625%3DDanil%2520Gusak%2520and%2520Gleb%2520Mezentsev%2520and%2520Ivan%2520Oseledets%2520and%2520Evgeny%2520Frolov%26entry.1292438233%3D%2520%2520Scalability%2520is%2520a%2520major%2520challenge%2520in%2520modern%2520recommender%2520systems.%2520In%2520sequential%250Arecommendations%252C%2520full%2520Cross-Entropy%2520%2528CE%2529%2520loss%2520achieves%2520state-of-the-art%250Arecommendation%2520quality%2520but%2520consumes%2520excessive%2520GPU%2520memory%2520with%2520large%2520item%250Acatalogs%252C%2520limiting%2520its%2520practicality.%2520Using%2520a%2520GPU-efficient%2520locality-sensitive%250Ahashing-like%2520algorithm%2520for%2520approximating%2520large%2520tensor%2520of%2520logits%252C%2520this%2520paper%250Aintroduces%2520a%2520novel%2520RECE%2520%2528REduced%2520Cross-Entropy%2529%2520loss.%2520RECE%2520significantly%250Areduces%2520memory%2520consumption%2520while%2520allowing%2520one%2520to%2520enjoy%2520the%2520state-of-the-art%250Aperformance%2520of%2520full%2520CE%2520loss.%2520Experimental%2520results%2520on%2520various%2520datasets%2520show%2520that%250ARECE%2520cuts%2520training%2520peak%2520memory%2520usage%2520by%2520up%2520to%252012%2520times%2520compared%2520to%2520existing%250Amethods%2520while%2520retaining%2520or%2520exceeding%2520performance%2520metrics%2520of%2520CE%2520loss.%2520The%250Aapproach%2520also%2520opens%2520up%2520new%2520possibilities%2520for%2520large-scale%2520applications%2520in%2520other%250Adomains.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.02354v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RECE%3A%20Reduced%20Cross-Entropy%20Loss%20for%20Large-Catalogue%20Sequential%0A%20%20Recommenders&entry.906535625=Danil%20Gusak%20and%20Gleb%20Mezentsev%20and%20Ivan%20Oseledets%20and%20Evgeny%20Frolov&entry.1292438233=%20%20Scalability%20is%20a%20major%20challenge%20in%20modern%20recommender%20systems.%20In%20sequential%0Arecommendations%2C%20full%20Cross-Entropy%20%28CE%29%20loss%20achieves%20state-of-the-art%0Arecommendation%20quality%20but%20consumes%20excessive%20GPU%20memory%20with%20large%20item%0Acatalogs%2C%20limiting%20its%20practicality.%20Using%20a%20GPU-efficient%20locality-sensitive%0Ahashing-like%20algorithm%20for%20approximating%20large%20tensor%20of%20logits%2C%20this%20paper%0Aintroduces%20a%20novel%20RECE%20%28REduced%20Cross-Entropy%29%20loss.%20RECE%20significantly%0Areduces%20memory%20consumption%20while%20allowing%20one%20to%20enjoy%20the%20state-of-the-art%0Aperformance%20of%20full%20CE%20loss.%20Experimental%20results%20on%20various%20datasets%20show%20that%0ARECE%20cuts%20training%20peak%20memory%20usage%20by%20up%20to%2012%20times%20compared%20to%20existing%0Amethods%20while%20retaining%20or%20exceeding%20performance%20metrics%20of%20CE%20loss.%20The%0Aapproach%20also%20opens%20up%20new%20possibilities%20for%20large-scale%20applications%20in%20other%0Adomains.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.02354v3&entry.124074799=Read"},
{"title": "InternVideo2: Scaling Foundation Models for Multimodal Video\n  Understanding", "author": "Yi Wang and Kunchang Li and Xinhao Li and Jiashuo Yu and Yinan He and Chenting Wang and Guo Chen and Baoqi Pei and Ziang Yan and Rongkun Zheng and Jilan Xu and Zun Wang and Yansong Shi and Tianxiang Jiang and Songze Li and Hongjie Zhang and Yifei Huang and Yu Qiao and Yali Wang and Limin Wang", "abstract": "  We introduce InternVideo2, a new family of video foundation models (ViFM)\nthat achieve the state-of-the-art results in video recognition, video-text\ntasks, and video-centric dialogue. Our core design is a progressive training\napproach that unifies the masked video modeling, crossmodal contrastive\nlearning, and next token prediction, scaling up the video encoder size to 6B\nparameters. At the data level, we prioritize spatiotemporal consistency by\nsemantically segmenting videos and generating video-audio-speech captions. This\nimproves the alignment between video and text. Through extensive experiments,\nwe validate our designs and demonstrate superior performance on over 60 video\nand audio tasks. Notably, our model outperforms others on various video-related\ndialogue and long video understanding benchmarks, highlighting its ability to\nreason and comprehend longer contexts. Code and models are available at\nhttps://github.com/OpenGVLab/InternVideo/tree/main/InternVideo2/.\n", "link": "http://arxiv.org/abs/2403.15377v4", "date": "2024-08-14", "relevancy": 2.3548, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.602}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5928}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5738}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20InternVideo2%3A%20Scaling%20Foundation%20Models%20for%20Multimodal%20Video%0A%20%20Understanding&body=Title%3A%20InternVideo2%3A%20Scaling%20Foundation%20Models%20for%20Multimodal%20Video%0A%20%20Understanding%0AAuthor%3A%20Yi%20Wang%20and%20Kunchang%20Li%20and%20Xinhao%20Li%20and%20Jiashuo%20Yu%20and%20Yinan%20He%20and%20Chenting%20Wang%20and%20Guo%20Chen%20and%20Baoqi%20Pei%20and%20Ziang%20Yan%20and%20Rongkun%20Zheng%20and%20Jilan%20Xu%20and%20Zun%20Wang%20and%20Yansong%20Shi%20and%20Tianxiang%20Jiang%20and%20Songze%20Li%20and%20Hongjie%20Zhang%20and%20Yifei%20Huang%20and%20Yu%20Qiao%20and%20Yali%20Wang%20and%20Limin%20Wang%0AAbstract%3A%20%20%20We%20introduce%20InternVideo2%2C%20a%20new%20family%20of%20video%20foundation%20models%20%28ViFM%29%0Athat%20achieve%20the%20state-of-the-art%20results%20in%20video%20recognition%2C%20video-text%0Atasks%2C%20and%20video-centric%20dialogue.%20Our%20core%20design%20is%20a%20progressive%20training%0Aapproach%20that%20unifies%20the%20masked%20video%20modeling%2C%20crossmodal%20contrastive%0Alearning%2C%20and%20next%20token%20prediction%2C%20scaling%20up%20the%20video%20encoder%20size%20to%206B%0Aparameters.%20At%20the%20data%20level%2C%20we%20prioritize%20spatiotemporal%20consistency%20by%0Asemantically%20segmenting%20videos%20and%20generating%20video-audio-speech%20captions.%20This%0Aimproves%20the%20alignment%20between%20video%20and%20text.%20Through%20extensive%20experiments%2C%0Awe%20validate%20our%20designs%20and%20demonstrate%20superior%20performance%20on%20over%2060%20video%0Aand%20audio%20tasks.%20Notably%2C%20our%20model%20outperforms%20others%20on%20various%20video-related%0Adialogue%20and%20long%20video%20understanding%20benchmarks%2C%20highlighting%20its%20ability%20to%0Areason%20and%20comprehend%20longer%20contexts.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/OpenGVLab/InternVideo/tree/main/InternVideo2/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.15377v4%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInternVideo2%253A%2520Scaling%2520Foundation%2520Models%2520for%2520Multimodal%2520Video%250A%2520%2520Understanding%26entry.906535625%3DYi%2520Wang%2520and%2520Kunchang%2520Li%2520and%2520Xinhao%2520Li%2520and%2520Jiashuo%2520Yu%2520and%2520Yinan%2520He%2520and%2520Chenting%2520Wang%2520and%2520Guo%2520Chen%2520and%2520Baoqi%2520Pei%2520and%2520Ziang%2520Yan%2520and%2520Rongkun%2520Zheng%2520and%2520Jilan%2520Xu%2520and%2520Zun%2520Wang%2520and%2520Yansong%2520Shi%2520and%2520Tianxiang%2520Jiang%2520and%2520Songze%2520Li%2520and%2520Hongjie%2520Zhang%2520and%2520Yifei%2520Huang%2520and%2520Yu%2520Qiao%2520and%2520Yali%2520Wang%2520and%2520Limin%2520Wang%26entry.1292438233%3D%2520%2520We%2520introduce%2520InternVideo2%252C%2520a%2520new%2520family%2520of%2520video%2520foundation%2520models%2520%2528ViFM%2529%250Athat%2520achieve%2520the%2520state-of-the-art%2520results%2520in%2520video%2520recognition%252C%2520video-text%250Atasks%252C%2520and%2520video-centric%2520dialogue.%2520Our%2520core%2520design%2520is%2520a%2520progressive%2520training%250Aapproach%2520that%2520unifies%2520the%2520masked%2520video%2520modeling%252C%2520crossmodal%2520contrastive%250Alearning%252C%2520and%2520next%2520token%2520prediction%252C%2520scaling%2520up%2520the%2520video%2520encoder%2520size%2520to%25206B%250Aparameters.%2520At%2520the%2520data%2520level%252C%2520we%2520prioritize%2520spatiotemporal%2520consistency%2520by%250Asemantically%2520segmenting%2520videos%2520and%2520generating%2520video-audio-speech%2520captions.%2520This%250Aimproves%2520the%2520alignment%2520between%2520video%2520and%2520text.%2520Through%2520extensive%2520experiments%252C%250Awe%2520validate%2520our%2520designs%2520and%2520demonstrate%2520superior%2520performance%2520on%2520over%252060%2520video%250Aand%2520audio%2520tasks.%2520Notably%252C%2520our%2520model%2520outperforms%2520others%2520on%2520various%2520video-related%250Adialogue%2520and%2520long%2520video%2520understanding%2520benchmarks%252C%2520highlighting%2520its%2520ability%2520to%250Areason%2520and%2520comprehend%2520longer%2520contexts.%2520Code%2520and%2520models%2520are%2520available%2520at%250Ahttps%253A//github.com/OpenGVLab/InternVideo/tree/main/InternVideo2/.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.15377v4%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=InternVideo2%3A%20Scaling%20Foundation%20Models%20for%20Multimodal%20Video%0A%20%20Understanding&entry.906535625=Yi%20Wang%20and%20Kunchang%20Li%20and%20Xinhao%20Li%20and%20Jiashuo%20Yu%20and%20Yinan%20He%20and%20Chenting%20Wang%20and%20Guo%20Chen%20and%20Baoqi%20Pei%20and%20Ziang%20Yan%20and%20Rongkun%20Zheng%20and%20Jilan%20Xu%20and%20Zun%20Wang%20and%20Yansong%20Shi%20and%20Tianxiang%20Jiang%20and%20Songze%20Li%20and%20Hongjie%20Zhang%20and%20Yifei%20Huang%20and%20Yu%20Qiao%20and%20Yali%20Wang%20and%20Limin%20Wang&entry.1292438233=%20%20We%20introduce%20InternVideo2%2C%20a%20new%20family%20of%20video%20foundation%20models%20%28ViFM%29%0Athat%20achieve%20the%20state-of-the-art%20results%20in%20video%20recognition%2C%20video-text%0Atasks%2C%20and%20video-centric%20dialogue.%20Our%20core%20design%20is%20a%20progressive%20training%0Aapproach%20that%20unifies%20the%20masked%20video%20modeling%2C%20crossmodal%20contrastive%0Alearning%2C%20and%20next%20token%20prediction%2C%20scaling%20up%20the%20video%20encoder%20size%20to%206B%0Aparameters.%20At%20the%20data%20level%2C%20we%20prioritize%20spatiotemporal%20consistency%20by%0Asemantically%20segmenting%20videos%20and%20generating%20video-audio-speech%20captions.%20This%0Aimproves%20the%20alignment%20between%20video%20and%20text.%20Through%20extensive%20experiments%2C%0Awe%20validate%20our%20designs%20and%20demonstrate%20superior%20performance%20on%20over%2060%20video%0Aand%20audio%20tasks.%20Notably%2C%20our%20model%20outperforms%20others%20on%20various%20video-related%0Adialogue%20and%20long%20video%20understanding%20benchmarks%2C%20highlighting%20its%20ability%20to%0Areason%20and%20comprehend%20longer%20contexts.%20Code%20and%20models%20are%20available%20at%0Ahttps%3A//github.com/OpenGVLab/InternVideo/tree/main/InternVideo2/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.15377v4&entry.124074799=Read"},
{"title": "VIRUS-NeRF -- Vision, InfraRed and UltraSonic based Neural Radiance\n  Fields", "author": "Nicolaj Schmid and Cornelius von Einem and Cesar Cadena and Roland Siegwart and Lorenz Hruby and Florian Tschopp", "abstract": "  Autonomous mobile robots are an increasingly integral part of modern factory\nand warehouse operations. Obstacle detection, avoidance and path planning are\ncritical safety-relevant tasks, which are often solved using expensive LiDAR\nsensors and depth cameras. We propose to use cost-effective low-resolution\nranging sensors, such as ultrasonic and infrared time-of-flight sensors by\ndeveloping VIRUS-NeRF - Vision, InfraRed, and UltraSonic based Neural Radiance\nFields. Building upon Instant Neural Graphics Primitives with a Multiresolution\nHash Encoding (Instant-NGP), VIRUS-NeRF incorporates depth measurements from\nultrasonic and infrared sensors and utilizes them to update the occupancy grid\nused for ray marching. Experimental evaluation in 2D demonstrates that\nVIRUS-NeRF achieves comparable mapping performance to LiDAR point clouds\nregarding coverage. Notably, in small environments, its accuracy aligns with\nthat of LiDAR measurements, while in larger ones, it is bounded by the utilized\nultrasonic sensors. An in-depth ablation study reveals that adding ultrasonic\nand infrared sensors is highly effective when dealing with sparse data and low\nview variation. Further, the proposed occupancy grid of VIRUS-NeRF improves the\nmapping capabilities and increases the training speed by 46% compared to\nInstant-NGP. Overall, VIRUS-NeRF presents a promising approach for\ncost-effective local mapping in mobile robotics, with potential applications in\nsafety and navigation tasks. The code can be found at\nhttps://github.com/ethz-asl/virus nerf.\n", "link": "http://arxiv.org/abs/2403.09477v2", "date": "2024-08-14", "relevancy": 2.3227, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6061}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5656}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5613}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20VIRUS-NeRF%20--%20Vision%2C%20InfraRed%20and%20UltraSonic%20based%20Neural%20Radiance%0A%20%20Fields&body=Title%3A%20VIRUS-NeRF%20--%20Vision%2C%20InfraRed%20and%20UltraSonic%20based%20Neural%20Radiance%0A%20%20Fields%0AAuthor%3A%20Nicolaj%20Schmid%20and%20Cornelius%20von%20Einem%20and%20Cesar%20Cadena%20and%20Roland%20Siegwart%20and%20Lorenz%20Hruby%20and%20Florian%20Tschopp%0AAbstract%3A%20%20%20Autonomous%20mobile%20robots%20are%20an%20increasingly%20integral%20part%20of%20modern%20factory%0Aand%20warehouse%20operations.%20Obstacle%20detection%2C%20avoidance%20and%20path%20planning%20are%0Acritical%20safety-relevant%20tasks%2C%20which%20are%20often%20solved%20using%20expensive%20LiDAR%0Asensors%20and%20depth%20cameras.%20We%20propose%20to%20use%20cost-effective%20low-resolution%0Aranging%20sensors%2C%20such%20as%20ultrasonic%20and%20infrared%20time-of-flight%20sensors%20by%0Adeveloping%20VIRUS-NeRF%20-%20Vision%2C%20InfraRed%2C%20and%20UltraSonic%20based%20Neural%20Radiance%0AFields.%20Building%20upon%20Instant%20Neural%20Graphics%20Primitives%20with%20a%20Multiresolution%0AHash%20Encoding%20%28Instant-NGP%29%2C%20VIRUS-NeRF%20incorporates%20depth%20measurements%20from%0Aultrasonic%20and%20infrared%20sensors%20and%20utilizes%20them%20to%20update%20the%20occupancy%20grid%0Aused%20for%20ray%20marching.%20Experimental%20evaluation%20in%202D%20demonstrates%20that%0AVIRUS-NeRF%20achieves%20comparable%20mapping%20performance%20to%20LiDAR%20point%20clouds%0Aregarding%20coverage.%20Notably%2C%20in%20small%20environments%2C%20its%20accuracy%20aligns%20with%0Athat%20of%20LiDAR%20measurements%2C%20while%20in%20larger%20ones%2C%20it%20is%20bounded%20by%20the%20utilized%0Aultrasonic%20sensors.%20An%20in-depth%20ablation%20study%20reveals%20that%20adding%20ultrasonic%0Aand%20infrared%20sensors%20is%20highly%20effective%20when%20dealing%20with%20sparse%20data%20and%20low%0Aview%20variation.%20Further%2C%20the%20proposed%20occupancy%20grid%20of%20VIRUS-NeRF%20improves%20the%0Amapping%20capabilities%20and%20increases%20the%20training%20speed%20by%2046%25%20compared%20to%0AInstant-NGP.%20Overall%2C%20VIRUS-NeRF%20presents%20a%20promising%20approach%20for%0Acost-effective%20local%20mapping%20in%20mobile%20robotics%2C%20with%20potential%20applications%20in%0Asafety%20and%20navigation%20tasks.%20The%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/ethz-asl/virus%20nerf.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09477v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVIRUS-NeRF%2520--%2520Vision%252C%2520InfraRed%2520and%2520UltraSonic%2520based%2520Neural%2520Radiance%250A%2520%2520Fields%26entry.906535625%3DNicolaj%2520Schmid%2520and%2520Cornelius%2520von%2520Einem%2520and%2520Cesar%2520Cadena%2520and%2520Roland%2520Siegwart%2520and%2520Lorenz%2520Hruby%2520and%2520Florian%2520Tschopp%26entry.1292438233%3D%2520%2520Autonomous%2520mobile%2520robots%2520are%2520an%2520increasingly%2520integral%2520part%2520of%2520modern%2520factory%250Aand%2520warehouse%2520operations.%2520Obstacle%2520detection%252C%2520avoidance%2520and%2520path%2520planning%2520are%250Acritical%2520safety-relevant%2520tasks%252C%2520which%2520are%2520often%2520solved%2520using%2520expensive%2520LiDAR%250Asensors%2520and%2520depth%2520cameras.%2520We%2520propose%2520to%2520use%2520cost-effective%2520low-resolution%250Aranging%2520sensors%252C%2520such%2520as%2520ultrasonic%2520and%2520infrared%2520time-of-flight%2520sensors%2520by%250Adeveloping%2520VIRUS-NeRF%2520-%2520Vision%252C%2520InfraRed%252C%2520and%2520UltraSonic%2520based%2520Neural%2520Radiance%250AFields.%2520Building%2520upon%2520Instant%2520Neural%2520Graphics%2520Primitives%2520with%2520a%2520Multiresolution%250AHash%2520Encoding%2520%2528Instant-NGP%2529%252C%2520VIRUS-NeRF%2520incorporates%2520depth%2520measurements%2520from%250Aultrasonic%2520and%2520infrared%2520sensors%2520and%2520utilizes%2520them%2520to%2520update%2520the%2520occupancy%2520grid%250Aused%2520for%2520ray%2520marching.%2520Experimental%2520evaluation%2520in%25202D%2520demonstrates%2520that%250AVIRUS-NeRF%2520achieves%2520comparable%2520mapping%2520performance%2520to%2520LiDAR%2520point%2520clouds%250Aregarding%2520coverage.%2520Notably%252C%2520in%2520small%2520environments%252C%2520its%2520accuracy%2520aligns%2520with%250Athat%2520of%2520LiDAR%2520measurements%252C%2520while%2520in%2520larger%2520ones%252C%2520it%2520is%2520bounded%2520by%2520the%2520utilized%250Aultrasonic%2520sensors.%2520An%2520in-depth%2520ablation%2520study%2520reveals%2520that%2520adding%2520ultrasonic%250Aand%2520infrared%2520sensors%2520is%2520highly%2520effective%2520when%2520dealing%2520with%2520sparse%2520data%2520and%2520low%250Aview%2520variation.%2520Further%252C%2520the%2520proposed%2520occupancy%2520grid%2520of%2520VIRUS-NeRF%2520improves%2520the%250Amapping%2520capabilities%2520and%2520increases%2520the%2520training%2520speed%2520by%252046%2525%2520compared%2520to%250AInstant-NGP.%2520Overall%252C%2520VIRUS-NeRF%2520presents%2520a%2520promising%2520approach%2520for%250Acost-effective%2520local%2520mapping%2520in%2520mobile%2520robotics%252C%2520with%2520potential%2520applications%2520in%250Asafety%2520and%2520navigation%2520tasks.%2520The%2520code%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/ethz-asl/virus%2520nerf.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.09477v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=VIRUS-NeRF%20--%20Vision%2C%20InfraRed%20and%20UltraSonic%20based%20Neural%20Radiance%0A%20%20Fields&entry.906535625=Nicolaj%20Schmid%20and%20Cornelius%20von%20Einem%20and%20Cesar%20Cadena%20and%20Roland%20Siegwart%20and%20Lorenz%20Hruby%20and%20Florian%20Tschopp&entry.1292438233=%20%20Autonomous%20mobile%20robots%20are%20an%20increasingly%20integral%20part%20of%20modern%20factory%0Aand%20warehouse%20operations.%20Obstacle%20detection%2C%20avoidance%20and%20path%20planning%20are%0Acritical%20safety-relevant%20tasks%2C%20which%20are%20often%20solved%20using%20expensive%20LiDAR%0Asensors%20and%20depth%20cameras.%20We%20propose%20to%20use%20cost-effective%20low-resolution%0Aranging%20sensors%2C%20such%20as%20ultrasonic%20and%20infrared%20time-of-flight%20sensors%20by%0Adeveloping%20VIRUS-NeRF%20-%20Vision%2C%20InfraRed%2C%20and%20UltraSonic%20based%20Neural%20Radiance%0AFields.%20Building%20upon%20Instant%20Neural%20Graphics%20Primitives%20with%20a%20Multiresolution%0AHash%20Encoding%20%28Instant-NGP%29%2C%20VIRUS-NeRF%20incorporates%20depth%20measurements%20from%0Aultrasonic%20and%20infrared%20sensors%20and%20utilizes%20them%20to%20update%20the%20occupancy%20grid%0Aused%20for%20ray%20marching.%20Experimental%20evaluation%20in%202D%20demonstrates%20that%0AVIRUS-NeRF%20achieves%20comparable%20mapping%20performance%20to%20LiDAR%20point%20clouds%0Aregarding%20coverage.%20Notably%2C%20in%20small%20environments%2C%20its%20accuracy%20aligns%20with%0Athat%20of%20LiDAR%20measurements%2C%20while%20in%20larger%20ones%2C%20it%20is%20bounded%20by%20the%20utilized%0Aultrasonic%20sensors.%20An%20in-depth%20ablation%20study%20reveals%20that%20adding%20ultrasonic%0Aand%20infrared%20sensors%20is%20highly%20effective%20when%20dealing%20with%20sparse%20data%20and%20low%0Aview%20variation.%20Further%2C%20the%20proposed%20occupancy%20grid%20of%20VIRUS-NeRF%20improves%20the%0Amapping%20capabilities%20and%20increases%20the%20training%20speed%20by%2046%25%20compared%20to%0AInstant-NGP.%20Overall%2C%20VIRUS-NeRF%20presents%20a%20promising%20approach%20for%0Acost-effective%20local%20mapping%20in%20mobile%20robotics%2C%20with%20potential%20applications%20in%0Asafety%20and%20navigation%20tasks.%20The%20code%20can%20be%20found%20at%0Ahttps%3A//github.com/ethz-asl/virus%20nerf.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09477v2&entry.124074799=Read"},
{"title": "A Spitting Image: Modular Superpixel Tokenization in Vision Transformers", "author": "Marius Aasan and Odd Kolbj\u00f8rnsen and Anne Schistad Solberg and Ad\u00edn Ramirez Rivera", "abstract": "  Vision Transformer (ViT) architectures traditionally employ a grid-based\napproach to tokenization independent of the semantic content of an image. We\npropose a modular superpixel tokenization strategy which decouples tokenization\nand feature extraction; a shift from contemporary approaches where these are\ntreated as an undifferentiated whole. Using on-line content-aware tokenization\nand scale- and shape-invariant positional embeddings, we perform experiments\nand ablations that contrast our approach with patch-based tokenization and\nrandomized partitions as baselines. We show that our method significantly\nimproves the faithfulness of attributions, gives pixel-level granularity on\nzero-shot unsupervised dense prediction tasks, while maintaining predictive\nperformance in classification tasks. Our approach provides a modular\ntokenization framework commensurable with standard architectures, extending the\nspace of ViTs to a larger class of semantically-rich models.\n", "link": "http://arxiv.org/abs/2408.07680v1", "date": "2024-08-14", "relevancy": 2.3204, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6052}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.565}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5552}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Spitting%20Image%3A%20Modular%20Superpixel%20Tokenization%20in%20Vision%20Transformers&body=Title%3A%20A%20Spitting%20Image%3A%20Modular%20Superpixel%20Tokenization%20in%20Vision%20Transformers%0AAuthor%3A%20Marius%20Aasan%20and%20Odd%20Kolbj%C3%B8rnsen%20and%20Anne%20Schistad%20Solberg%20and%20Ad%C3%ADn%20Ramirez%20Rivera%0AAbstract%3A%20%20%20Vision%20Transformer%20%28ViT%29%20architectures%20traditionally%20employ%20a%20grid-based%0Aapproach%20to%20tokenization%20independent%20of%20the%20semantic%20content%20of%20an%20image.%20We%0Apropose%20a%20modular%20superpixel%20tokenization%20strategy%20which%20decouples%20tokenization%0Aand%20feature%20extraction%3B%20a%20shift%20from%20contemporary%20approaches%20where%20these%20are%0Atreated%20as%20an%20undifferentiated%20whole.%20Using%20on-line%20content-aware%20tokenization%0Aand%20scale-%20and%20shape-invariant%20positional%20embeddings%2C%20we%20perform%20experiments%0Aand%20ablations%20that%20contrast%20our%20approach%20with%20patch-based%20tokenization%20and%0Arandomized%20partitions%20as%20baselines.%20We%20show%20that%20our%20method%20significantly%0Aimproves%20the%20faithfulness%20of%20attributions%2C%20gives%20pixel-level%20granularity%20on%0Azero-shot%20unsupervised%20dense%20prediction%20tasks%2C%20while%20maintaining%20predictive%0Aperformance%20in%20classification%20tasks.%20Our%20approach%20provides%20a%20modular%0Atokenization%20framework%20commensurable%20with%20standard%20architectures%2C%20extending%20the%0Aspace%20of%20ViTs%20to%20a%20larger%20class%20of%20semantically-rich%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07680v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Spitting%2520Image%253A%2520Modular%2520Superpixel%2520Tokenization%2520in%2520Vision%2520Transformers%26entry.906535625%3DMarius%2520Aasan%2520and%2520Odd%2520Kolbj%25C3%25B8rnsen%2520and%2520Anne%2520Schistad%2520Solberg%2520and%2520Ad%25C3%25ADn%2520Ramirez%2520Rivera%26entry.1292438233%3D%2520%2520Vision%2520Transformer%2520%2528ViT%2529%2520architectures%2520traditionally%2520employ%2520a%2520grid-based%250Aapproach%2520to%2520tokenization%2520independent%2520of%2520the%2520semantic%2520content%2520of%2520an%2520image.%2520We%250Apropose%2520a%2520modular%2520superpixel%2520tokenization%2520strategy%2520which%2520decouples%2520tokenization%250Aand%2520feature%2520extraction%253B%2520a%2520shift%2520from%2520contemporary%2520approaches%2520where%2520these%2520are%250Atreated%2520as%2520an%2520undifferentiated%2520whole.%2520Using%2520on-line%2520content-aware%2520tokenization%250Aand%2520scale-%2520and%2520shape-invariant%2520positional%2520embeddings%252C%2520we%2520perform%2520experiments%250Aand%2520ablations%2520that%2520contrast%2520our%2520approach%2520with%2520patch-based%2520tokenization%2520and%250Arandomized%2520partitions%2520as%2520baselines.%2520We%2520show%2520that%2520our%2520method%2520significantly%250Aimproves%2520the%2520faithfulness%2520of%2520attributions%252C%2520gives%2520pixel-level%2520granularity%2520on%250Azero-shot%2520unsupervised%2520dense%2520prediction%2520tasks%252C%2520while%2520maintaining%2520predictive%250Aperformance%2520in%2520classification%2520tasks.%2520Our%2520approach%2520provides%2520a%2520modular%250Atokenization%2520framework%2520commensurable%2520with%2520standard%2520architectures%252C%2520extending%2520the%250Aspace%2520of%2520ViTs%2520to%2520a%2520larger%2520class%2520of%2520semantically-rich%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07680v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Spitting%20Image%3A%20Modular%20Superpixel%20Tokenization%20in%20Vision%20Transformers&entry.906535625=Marius%20Aasan%20and%20Odd%20Kolbj%C3%B8rnsen%20and%20Anne%20Schistad%20Solberg%20and%20Ad%C3%ADn%20Ramirez%20Rivera&entry.1292438233=%20%20Vision%20Transformer%20%28ViT%29%20architectures%20traditionally%20employ%20a%20grid-based%0Aapproach%20to%20tokenization%20independent%20of%20the%20semantic%20content%20of%20an%20image.%20We%0Apropose%20a%20modular%20superpixel%20tokenization%20strategy%20which%20decouples%20tokenization%0Aand%20feature%20extraction%3B%20a%20shift%20from%20contemporary%20approaches%20where%20these%20are%0Atreated%20as%20an%20undifferentiated%20whole.%20Using%20on-line%20content-aware%20tokenization%0Aand%20scale-%20and%20shape-invariant%20positional%20embeddings%2C%20we%20perform%20experiments%0Aand%20ablations%20that%20contrast%20our%20approach%20with%20patch-based%20tokenization%20and%0Arandomized%20partitions%20as%20baselines.%20We%20show%20that%20our%20method%20significantly%0Aimproves%20the%20faithfulness%20of%20attributions%2C%20gives%20pixel-level%20granularity%20on%0Azero-shot%20unsupervised%20dense%20prediction%20tasks%2C%20while%20maintaining%20predictive%0Aperformance%20in%20classification%20tasks.%20Our%20approach%20provides%20a%20modular%0Atokenization%20framework%20commensurable%20with%20standard%20architectures%2C%20extending%20the%0Aspace%20of%20ViTs%20to%20a%20larger%20class%20of%20semantically-rich%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07680v1&entry.124074799=Read"},
{"title": "Unsupervised Stereo Matching Network For VHR Remote Sensing Images Based\n  On Error Prediction", "author": "Liting Jiang and Yuming Xiang and Feng Wang and Hongjian You", "abstract": "  Stereo matching in remote sensing has recently garnered increased attention,\nprimarily focusing on supervised learning. However, datasets with ground truth\ngenerated by expensive airbone Lidar exhibit limited quantity and diversity,\nconstraining the effectiveness of supervised networks. In contrast,\nunsupervised learning methods can leverage the increasing availability of\nvery-high-resolution (VHR) remote sensing images, offering considerable\npotential in the realm of stereo matching. Motivated by this intuition, we\npropose a novel unsupervised stereo matching network for VHR remote sensing\nimages. A light-weight module to bridge confidence with predicted error is\nintroduced to refine the core model. Robust unsupervised losses are formulated\nto enhance network convergence. The experimental results on US3D and WHU-Stereo\ndatasets demonstrate that the proposed network achieves superior accuracy\ncompared to other unsupervised networks and exhibits better generalization\ncapabilities than supervised models. Our code will be available at\nhttps://github.com/Elenairene/CBEM.\n", "link": "http://arxiv.org/abs/2408.07419v1", "date": "2024-08-14", "relevancy": 2.3064, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5881}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5726}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5578}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Stereo%20Matching%20Network%20For%20VHR%20Remote%20Sensing%20Images%20Based%0A%20%20On%20Error%20Prediction&body=Title%3A%20Unsupervised%20Stereo%20Matching%20Network%20For%20VHR%20Remote%20Sensing%20Images%20Based%0A%20%20On%20Error%20Prediction%0AAuthor%3A%20Liting%20Jiang%20and%20Yuming%20Xiang%20and%20Feng%20Wang%20and%20Hongjian%20You%0AAbstract%3A%20%20%20Stereo%20matching%20in%20remote%20sensing%20has%20recently%20garnered%20increased%20attention%2C%0Aprimarily%20focusing%20on%20supervised%20learning.%20However%2C%20datasets%20with%20ground%20truth%0Agenerated%20by%20expensive%20airbone%20Lidar%20exhibit%20limited%20quantity%20and%20diversity%2C%0Aconstraining%20the%20effectiveness%20of%20supervised%20networks.%20In%20contrast%2C%0Aunsupervised%20learning%20methods%20can%20leverage%20the%20increasing%20availability%20of%0Avery-high-resolution%20%28VHR%29%20remote%20sensing%20images%2C%20offering%20considerable%0Apotential%20in%20the%20realm%20of%20stereo%20matching.%20Motivated%20by%20this%20intuition%2C%20we%0Apropose%20a%20novel%20unsupervised%20stereo%20matching%20network%20for%20VHR%20remote%20sensing%0Aimages.%20A%20light-weight%20module%20to%20bridge%20confidence%20with%20predicted%20error%20is%0Aintroduced%20to%20refine%20the%20core%20model.%20Robust%20unsupervised%20losses%20are%20formulated%0Ato%20enhance%20network%20convergence.%20The%20experimental%20results%20on%20US3D%20and%20WHU-Stereo%0Adatasets%20demonstrate%20that%20the%20proposed%20network%20achieves%20superior%20accuracy%0Acompared%20to%20other%20unsupervised%20networks%20and%20exhibits%20better%20generalization%0Acapabilities%20than%20supervised%20models.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/Elenairene/CBEM.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07419v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUnsupervised%2520Stereo%2520Matching%2520Network%2520For%2520VHR%2520Remote%2520Sensing%2520Images%2520Based%250A%2520%2520On%2520Error%2520Prediction%26entry.906535625%3DLiting%2520Jiang%2520and%2520Yuming%2520Xiang%2520and%2520Feng%2520Wang%2520and%2520Hongjian%2520You%26entry.1292438233%3D%2520%2520Stereo%2520matching%2520in%2520remote%2520sensing%2520has%2520recently%2520garnered%2520increased%2520attention%252C%250Aprimarily%2520focusing%2520on%2520supervised%2520learning.%2520However%252C%2520datasets%2520with%2520ground%2520truth%250Agenerated%2520by%2520expensive%2520airbone%2520Lidar%2520exhibit%2520limited%2520quantity%2520and%2520diversity%252C%250Aconstraining%2520the%2520effectiveness%2520of%2520supervised%2520networks.%2520In%2520contrast%252C%250Aunsupervised%2520learning%2520methods%2520can%2520leverage%2520the%2520increasing%2520availability%2520of%250Avery-high-resolution%2520%2528VHR%2529%2520remote%2520sensing%2520images%252C%2520offering%2520considerable%250Apotential%2520in%2520the%2520realm%2520of%2520stereo%2520matching.%2520Motivated%2520by%2520this%2520intuition%252C%2520we%250Apropose%2520a%2520novel%2520unsupervised%2520stereo%2520matching%2520network%2520for%2520VHR%2520remote%2520sensing%250Aimages.%2520A%2520light-weight%2520module%2520to%2520bridge%2520confidence%2520with%2520predicted%2520error%2520is%250Aintroduced%2520to%2520refine%2520the%2520core%2520model.%2520Robust%2520unsupervised%2520losses%2520are%2520formulated%250Ato%2520enhance%2520network%2520convergence.%2520The%2520experimental%2520results%2520on%2520US3D%2520and%2520WHU-Stereo%250Adatasets%2520demonstrate%2520that%2520the%2520proposed%2520network%2520achieves%2520superior%2520accuracy%250Acompared%2520to%2520other%2520unsupervised%2520networks%2520and%2520exhibits%2520better%2520generalization%250Acapabilities%2520than%2520supervised%2520models.%2520Our%2520code%2520will%2520be%2520available%2520at%250Ahttps%253A//github.com/Elenairene/CBEM.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07419v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Stereo%20Matching%20Network%20For%20VHR%20Remote%20Sensing%20Images%20Based%0A%20%20On%20Error%20Prediction&entry.906535625=Liting%20Jiang%20and%20Yuming%20Xiang%20and%20Feng%20Wang%20and%20Hongjian%20You&entry.1292438233=%20%20Stereo%20matching%20in%20remote%20sensing%20has%20recently%20garnered%20increased%20attention%2C%0Aprimarily%20focusing%20on%20supervised%20learning.%20However%2C%20datasets%20with%20ground%20truth%0Agenerated%20by%20expensive%20airbone%20Lidar%20exhibit%20limited%20quantity%20and%20diversity%2C%0Aconstraining%20the%20effectiveness%20of%20supervised%20networks.%20In%20contrast%2C%0Aunsupervised%20learning%20methods%20can%20leverage%20the%20increasing%20availability%20of%0Avery-high-resolution%20%28VHR%29%20remote%20sensing%20images%2C%20offering%20considerable%0Apotential%20in%20the%20realm%20of%20stereo%20matching.%20Motivated%20by%20this%20intuition%2C%20we%0Apropose%20a%20novel%20unsupervised%20stereo%20matching%20network%20for%20VHR%20remote%20sensing%0Aimages.%20A%20light-weight%20module%20to%20bridge%20confidence%20with%20predicted%20error%20is%0Aintroduced%20to%20refine%20the%20core%20model.%20Robust%20unsupervised%20losses%20are%20formulated%0Ato%20enhance%20network%20convergence.%20The%20experimental%20results%20on%20US3D%20and%20WHU-Stereo%0Adatasets%20demonstrate%20that%20the%20proposed%20network%20achieves%20superior%20accuracy%0Acompared%20to%20other%20unsupervised%20networks%20and%20exhibits%20better%20generalization%0Acapabilities%20than%20supervised%20models.%20Our%20code%20will%20be%20available%20at%0Ahttps%3A//github.com/Elenairene/CBEM.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07419v1&entry.124074799=Read"},
{"title": "Cross-aware Early Fusion with Stage-divided Vision and Language\n  Transformer Encoders for Referring Image Segmentation", "author": "Yubin Cho and Hyunwoo Yu and Suk-ju Kang", "abstract": "  Referring segmentation aims to segment a target object related to a natural\nlanguage expression. Key challenges of this task are understanding the meaning\nof complex and ambiguous language expressions and determining the relevant\nregions in the image with multiple objects by referring to the expression.\nRecent models have focused on the early fusion with the language features at\nthe intermediate stage of the vision encoder, but these approaches have a\nlimitation that the language features cannot refer to the visual information.\nTo address this issue, this paper proposes a novel architecture, Cross-aware\nearly fusion with stage-divided Vision and Language Transformer encoders\n(CrossVLT), which allows both language and vision encoders to perform the early\nfusion for improving the ability of the cross-modal context modeling. Unlike\nprevious methods, our method enables the vision and language features to refer\nto each other's information at each stage to mutually enhance the robustness of\nboth encoders. Furthermore, unlike the conventional scheme that relies solely\non the high-level features for the cross-modal alignment, we introduce a\nfeature-based alignment scheme that enables the low-level to high-level\nfeatures of the vision and language encoders to engage in the cross-modal\nalignment. By aligning the intermediate cross-modal features in all encoder\nstages, this scheme leads to effective cross-modal fusion. In this way, the\nproposed approach is simple but effective for referring image segmentation, and\nit outperforms the previous state-of-the-art methods on three public\nbenchmarks.\n", "link": "http://arxiv.org/abs/2408.07539v1", "date": "2024-08-14", "relevancy": 2.2957, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.6105}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5634}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-aware%20Early%20Fusion%20with%20Stage-divided%20Vision%20and%20Language%0A%20%20Transformer%20Encoders%20for%20Referring%20Image%20Segmentation&body=Title%3A%20Cross-aware%20Early%20Fusion%20with%20Stage-divided%20Vision%20and%20Language%0A%20%20Transformer%20Encoders%20for%20Referring%20Image%20Segmentation%0AAuthor%3A%20Yubin%20Cho%20and%20Hyunwoo%20Yu%20and%20Suk-ju%20Kang%0AAbstract%3A%20%20%20Referring%20segmentation%20aims%20to%20segment%20a%20target%20object%20related%20to%20a%20natural%0Alanguage%20expression.%20Key%20challenges%20of%20this%20task%20are%20understanding%20the%20meaning%0Aof%20complex%20and%20ambiguous%20language%20expressions%20and%20determining%20the%20relevant%0Aregions%20in%20the%20image%20with%20multiple%20objects%20by%20referring%20to%20the%20expression.%0ARecent%20models%20have%20focused%20on%20the%20early%20fusion%20with%20the%20language%20features%20at%0Athe%20intermediate%20stage%20of%20the%20vision%20encoder%2C%20but%20these%20approaches%20have%20a%0Alimitation%20that%20the%20language%20features%20cannot%20refer%20to%20the%20visual%20information.%0ATo%20address%20this%20issue%2C%20this%20paper%20proposes%20a%20novel%20architecture%2C%20Cross-aware%0Aearly%20fusion%20with%20stage-divided%20Vision%20and%20Language%20Transformer%20encoders%0A%28CrossVLT%29%2C%20which%20allows%20both%20language%20and%20vision%20encoders%20to%20perform%20the%20early%0Afusion%20for%20improving%20the%20ability%20of%20the%20cross-modal%20context%20modeling.%20Unlike%0Aprevious%20methods%2C%20our%20method%20enables%20the%20vision%20and%20language%20features%20to%20refer%0Ato%20each%20other%27s%20information%20at%20each%20stage%20to%20mutually%20enhance%20the%20robustness%20of%0Aboth%20encoders.%20Furthermore%2C%20unlike%20the%20conventional%20scheme%20that%20relies%20solely%0Aon%20the%20high-level%20features%20for%20the%20cross-modal%20alignment%2C%20we%20introduce%20a%0Afeature-based%20alignment%20scheme%20that%20enables%20the%20low-level%20to%20high-level%0Afeatures%20of%20the%20vision%20and%20language%20encoders%20to%20engage%20in%20the%20cross-modal%0Aalignment.%20By%20aligning%20the%20intermediate%20cross-modal%20features%20in%20all%20encoder%0Astages%2C%20this%20scheme%20leads%20to%20effective%20cross-modal%20fusion.%20In%20this%20way%2C%20the%0Aproposed%20approach%20is%20simple%20but%20effective%20for%20referring%20image%20segmentation%2C%20and%0Ait%20outperforms%20the%20previous%20state-of-the-art%20methods%20on%20three%20public%0Abenchmarks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07539v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-aware%2520Early%2520Fusion%2520with%2520Stage-divided%2520Vision%2520and%2520Language%250A%2520%2520Transformer%2520Encoders%2520for%2520Referring%2520Image%2520Segmentation%26entry.906535625%3DYubin%2520Cho%2520and%2520Hyunwoo%2520Yu%2520and%2520Suk-ju%2520Kang%26entry.1292438233%3D%2520%2520Referring%2520segmentation%2520aims%2520to%2520segment%2520a%2520target%2520object%2520related%2520to%2520a%2520natural%250Alanguage%2520expression.%2520Key%2520challenges%2520of%2520this%2520task%2520are%2520understanding%2520the%2520meaning%250Aof%2520complex%2520and%2520ambiguous%2520language%2520expressions%2520and%2520determining%2520the%2520relevant%250Aregions%2520in%2520the%2520image%2520with%2520multiple%2520objects%2520by%2520referring%2520to%2520the%2520expression.%250ARecent%2520models%2520have%2520focused%2520on%2520the%2520early%2520fusion%2520with%2520the%2520language%2520features%2520at%250Athe%2520intermediate%2520stage%2520of%2520the%2520vision%2520encoder%252C%2520but%2520these%2520approaches%2520have%2520a%250Alimitation%2520that%2520the%2520language%2520features%2520cannot%2520refer%2520to%2520the%2520visual%2520information.%250ATo%2520address%2520this%2520issue%252C%2520this%2520paper%2520proposes%2520a%2520novel%2520architecture%252C%2520Cross-aware%250Aearly%2520fusion%2520with%2520stage-divided%2520Vision%2520and%2520Language%2520Transformer%2520encoders%250A%2528CrossVLT%2529%252C%2520which%2520allows%2520both%2520language%2520and%2520vision%2520encoders%2520to%2520perform%2520the%2520early%250Afusion%2520for%2520improving%2520the%2520ability%2520of%2520the%2520cross-modal%2520context%2520modeling.%2520Unlike%250Aprevious%2520methods%252C%2520our%2520method%2520enables%2520the%2520vision%2520and%2520language%2520features%2520to%2520refer%250Ato%2520each%2520other%2527s%2520information%2520at%2520each%2520stage%2520to%2520mutually%2520enhance%2520the%2520robustness%2520of%250Aboth%2520encoders.%2520Furthermore%252C%2520unlike%2520the%2520conventional%2520scheme%2520that%2520relies%2520solely%250Aon%2520the%2520high-level%2520features%2520for%2520the%2520cross-modal%2520alignment%252C%2520we%2520introduce%2520a%250Afeature-based%2520alignment%2520scheme%2520that%2520enables%2520the%2520low-level%2520to%2520high-level%250Afeatures%2520of%2520the%2520vision%2520and%2520language%2520encoders%2520to%2520engage%2520in%2520the%2520cross-modal%250Aalignment.%2520By%2520aligning%2520the%2520intermediate%2520cross-modal%2520features%2520in%2520all%2520encoder%250Astages%252C%2520this%2520scheme%2520leads%2520to%2520effective%2520cross-modal%2520fusion.%2520In%2520this%2520way%252C%2520the%250Aproposed%2520approach%2520is%2520simple%2520but%2520effective%2520for%2520referring%2520image%2520segmentation%252C%2520and%250Ait%2520outperforms%2520the%2520previous%2520state-of-the-art%2520methods%2520on%2520three%2520public%250Abenchmarks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07539v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-aware%20Early%20Fusion%20with%20Stage-divided%20Vision%20and%20Language%0A%20%20Transformer%20Encoders%20for%20Referring%20Image%20Segmentation&entry.906535625=Yubin%20Cho%20and%20Hyunwoo%20Yu%20and%20Suk-ju%20Kang&entry.1292438233=%20%20Referring%20segmentation%20aims%20to%20segment%20a%20target%20object%20related%20to%20a%20natural%0Alanguage%20expression.%20Key%20challenges%20of%20this%20task%20are%20understanding%20the%20meaning%0Aof%20complex%20and%20ambiguous%20language%20expressions%20and%20determining%20the%20relevant%0Aregions%20in%20the%20image%20with%20multiple%20objects%20by%20referring%20to%20the%20expression.%0ARecent%20models%20have%20focused%20on%20the%20early%20fusion%20with%20the%20language%20features%20at%0Athe%20intermediate%20stage%20of%20the%20vision%20encoder%2C%20but%20these%20approaches%20have%20a%0Alimitation%20that%20the%20language%20features%20cannot%20refer%20to%20the%20visual%20information.%0ATo%20address%20this%20issue%2C%20this%20paper%20proposes%20a%20novel%20architecture%2C%20Cross-aware%0Aearly%20fusion%20with%20stage-divided%20Vision%20and%20Language%20Transformer%20encoders%0A%28CrossVLT%29%2C%20which%20allows%20both%20language%20and%20vision%20encoders%20to%20perform%20the%20early%0Afusion%20for%20improving%20the%20ability%20of%20the%20cross-modal%20context%20modeling.%20Unlike%0Aprevious%20methods%2C%20our%20method%20enables%20the%20vision%20and%20language%20features%20to%20refer%0Ato%20each%20other%27s%20information%20at%20each%20stage%20to%20mutually%20enhance%20the%20robustness%20of%0Aboth%20encoders.%20Furthermore%2C%20unlike%20the%20conventional%20scheme%20that%20relies%20solely%0Aon%20the%20high-level%20features%20for%20the%20cross-modal%20alignment%2C%20we%20introduce%20a%0Afeature-based%20alignment%20scheme%20that%20enables%20the%20low-level%20to%20high-level%0Afeatures%20of%20the%20vision%20and%20language%20encoders%20to%20engage%20in%20the%20cross-modal%0Aalignment.%20By%20aligning%20the%20intermediate%20cross-modal%20features%20in%20all%20encoder%0Astages%2C%20this%20scheme%20leads%20to%20effective%20cross-modal%20fusion.%20In%20this%20way%2C%20the%0Aproposed%20approach%20is%20simple%20but%20effective%20for%20referring%20image%20segmentation%2C%20and%0Ait%20outperforms%20the%20previous%20state-of-the-art%20methods%20on%20three%20public%0Abenchmarks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07539v1&entry.124074799=Read"},
{"title": "Progressive Radiance Distillation for Inverse Rendering with Gaussian\n  Splatting", "author": "Keyang Ye and Qiming Hou and Kun Zhou", "abstract": "  We propose progressive radiance distillation, an inverse rendering method\nthat combines physically-based rendering with Gaussian-based radiance field\nrendering using a distillation progress map. Taking multi-view images as input,\nour method starts from a pre-trained radiance field guidance, and distills\nphysically-based light and material parameters from the radiance field using an\nimage-fitting process. The distillation progress map is initialized to a small\nvalue, which favors radiance field rendering. During early iterations when\nfitted light and material parameters are far from convergence, the radiance\nfield fallback ensures the sanity of image loss gradients and avoids local\nminima that attracts under-fit states. As fitted parameters converge, the\nphysical model gradually takes over and the distillation progress increases\ncorrespondingly. In presence of light paths unmodeled by the physical model,\nthe distillation progress never finishes on affected pixels and the learned\nradiance field stays in the final rendering. With this designed tolerance for\nphysical model limitations, we prevent unmodeled color components from leaking\ninto light and material parameters, alleviating relighting artifacts.\nMeanwhile, the remaining radiance field compensates for the limitations of the\nphysical model, guaranteeing high-quality novel views synthesis. Experimental\nresults demonstrate that our method significantly outperforms state-of-the-art\ntechniques quality-wise in both novel view synthesis and relighting. The idea\nof progressive radiance distillation is not limited to Gaussian splatting. We\nshow that it also has positive effects for prominently specular scenes when\nadapted to a mesh-based inverse rendering method.\n", "link": "http://arxiv.org/abs/2408.07595v1", "date": "2024-08-14", "relevancy": 2.295, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.6139}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5498}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5332}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Progressive%20Radiance%20Distillation%20for%20Inverse%20Rendering%20with%20Gaussian%0A%20%20Splatting&body=Title%3A%20Progressive%20Radiance%20Distillation%20for%20Inverse%20Rendering%20with%20Gaussian%0A%20%20Splatting%0AAuthor%3A%20Keyang%20Ye%20and%20Qiming%20Hou%20and%20Kun%20Zhou%0AAbstract%3A%20%20%20We%20propose%20progressive%20radiance%20distillation%2C%20an%20inverse%20rendering%20method%0Athat%20combines%20physically-based%20rendering%20with%20Gaussian-based%20radiance%20field%0Arendering%20using%20a%20distillation%20progress%20map.%20Taking%20multi-view%20images%20as%20input%2C%0Aour%20method%20starts%20from%20a%20pre-trained%20radiance%20field%20guidance%2C%20and%20distills%0Aphysically-based%20light%20and%20material%20parameters%20from%20the%20radiance%20field%20using%20an%0Aimage-fitting%20process.%20The%20distillation%20progress%20map%20is%20initialized%20to%20a%20small%0Avalue%2C%20which%20favors%20radiance%20field%20rendering.%20During%20early%20iterations%20when%0Afitted%20light%20and%20material%20parameters%20are%20far%20from%20convergence%2C%20the%20radiance%0Afield%20fallback%20ensures%20the%20sanity%20of%20image%20loss%20gradients%20and%20avoids%20local%0Aminima%20that%20attracts%20under-fit%20states.%20As%20fitted%20parameters%20converge%2C%20the%0Aphysical%20model%20gradually%20takes%20over%20and%20the%20distillation%20progress%20increases%0Acorrespondingly.%20In%20presence%20of%20light%20paths%20unmodeled%20by%20the%20physical%20model%2C%0Athe%20distillation%20progress%20never%20finishes%20on%20affected%20pixels%20and%20the%20learned%0Aradiance%20field%20stays%20in%20the%20final%20rendering.%20With%20this%20designed%20tolerance%20for%0Aphysical%20model%20limitations%2C%20we%20prevent%20unmodeled%20color%20components%20from%20leaking%0Ainto%20light%20and%20material%20parameters%2C%20alleviating%20relighting%20artifacts.%0AMeanwhile%2C%20the%20remaining%20radiance%20field%20compensates%20for%20the%20limitations%20of%20the%0Aphysical%20model%2C%20guaranteeing%20high-quality%20novel%20views%20synthesis.%20Experimental%0Aresults%20demonstrate%20that%20our%20method%20significantly%20outperforms%20state-of-the-art%0Atechniques%20quality-wise%20in%20both%20novel%20view%20synthesis%20and%20relighting.%20The%20idea%0Aof%20progressive%20radiance%20distillation%20is%20not%20limited%20to%20Gaussian%20splatting.%20We%0Ashow%20that%20it%20also%20has%20positive%20effects%20for%20prominently%20specular%20scenes%20when%0Aadapted%20to%20a%20mesh-based%20inverse%20rendering%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07595v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProgressive%2520Radiance%2520Distillation%2520for%2520Inverse%2520Rendering%2520with%2520Gaussian%250A%2520%2520Splatting%26entry.906535625%3DKeyang%2520Ye%2520and%2520Qiming%2520Hou%2520and%2520Kun%2520Zhou%26entry.1292438233%3D%2520%2520We%2520propose%2520progressive%2520radiance%2520distillation%252C%2520an%2520inverse%2520rendering%2520method%250Athat%2520combines%2520physically-based%2520rendering%2520with%2520Gaussian-based%2520radiance%2520field%250Arendering%2520using%2520a%2520distillation%2520progress%2520map.%2520Taking%2520multi-view%2520images%2520as%2520input%252C%250Aour%2520method%2520starts%2520from%2520a%2520pre-trained%2520radiance%2520field%2520guidance%252C%2520and%2520distills%250Aphysically-based%2520light%2520and%2520material%2520parameters%2520from%2520the%2520radiance%2520field%2520using%2520an%250Aimage-fitting%2520process.%2520The%2520distillation%2520progress%2520map%2520is%2520initialized%2520to%2520a%2520small%250Avalue%252C%2520which%2520favors%2520radiance%2520field%2520rendering.%2520During%2520early%2520iterations%2520when%250Afitted%2520light%2520and%2520material%2520parameters%2520are%2520far%2520from%2520convergence%252C%2520the%2520radiance%250Afield%2520fallback%2520ensures%2520the%2520sanity%2520of%2520image%2520loss%2520gradients%2520and%2520avoids%2520local%250Aminima%2520that%2520attracts%2520under-fit%2520states.%2520As%2520fitted%2520parameters%2520converge%252C%2520the%250Aphysical%2520model%2520gradually%2520takes%2520over%2520and%2520the%2520distillation%2520progress%2520increases%250Acorrespondingly.%2520In%2520presence%2520of%2520light%2520paths%2520unmodeled%2520by%2520the%2520physical%2520model%252C%250Athe%2520distillation%2520progress%2520never%2520finishes%2520on%2520affected%2520pixels%2520and%2520the%2520learned%250Aradiance%2520field%2520stays%2520in%2520the%2520final%2520rendering.%2520With%2520this%2520designed%2520tolerance%2520for%250Aphysical%2520model%2520limitations%252C%2520we%2520prevent%2520unmodeled%2520color%2520components%2520from%2520leaking%250Ainto%2520light%2520and%2520material%2520parameters%252C%2520alleviating%2520relighting%2520artifacts.%250AMeanwhile%252C%2520the%2520remaining%2520radiance%2520field%2520compensates%2520for%2520the%2520limitations%2520of%2520the%250Aphysical%2520model%252C%2520guaranteeing%2520high-quality%2520novel%2520views%2520synthesis.%2520Experimental%250Aresults%2520demonstrate%2520that%2520our%2520method%2520significantly%2520outperforms%2520state-of-the-art%250Atechniques%2520quality-wise%2520in%2520both%2520novel%2520view%2520synthesis%2520and%2520relighting.%2520The%2520idea%250Aof%2520progressive%2520radiance%2520distillation%2520is%2520not%2520limited%2520to%2520Gaussian%2520splatting.%2520We%250Ashow%2520that%2520it%2520also%2520has%2520positive%2520effects%2520for%2520prominently%2520specular%2520scenes%2520when%250Aadapted%2520to%2520a%2520mesh-based%2520inverse%2520rendering%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07595v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Progressive%20Radiance%20Distillation%20for%20Inverse%20Rendering%20with%20Gaussian%0A%20%20Splatting&entry.906535625=Keyang%20Ye%20and%20Qiming%20Hou%20and%20Kun%20Zhou&entry.1292438233=%20%20We%20propose%20progressive%20radiance%20distillation%2C%20an%20inverse%20rendering%20method%0Athat%20combines%20physically-based%20rendering%20with%20Gaussian-based%20radiance%20field%0Arendering%20using%20a%20distillation%20progress%20map.%20Taking%20multi-view%20images%20as%20input%2C%0Aour%20method%20starts%20from%20a%20pre-trained%20radiance%20field%20guidance%2C%20and%20distills%0Aphysically-based%20light%20and%20material%20parameters%20from%20the%20radiance%20field%20using%20an%0Aimage-fitting%20process.%20The%20distillation%20progress%20map%20is%20initialized%20to%20a%20small%0Avalue%2C%20which%20favors%20radiance%20field%20rendering.%20During%20early%20iterations%20when%0Afitted%20light%20and%20material%20parameters%20are%20far%20from%20convergence%2C%20the%20radiance%0Afield%20fallback%20ensures%20the%20sanity%20of%20image%20loss%20gradients%20and%20avoids%20local%0Aminima%20that%20attracts%20under-fit%20states.%20As%20fitted%20parameters%20converge%2C%20the%0Aphysical%20model%20gradually%20takes%20over%20and%20the%20distillation%20progress%20increases%0Acorrespondingly.%20In%20presence%20of%20light%20paths%20unmodeled%20by%20the%20physical%20model%2C%0Athe%20distillation%20progress%20never%20finishes%20on%20affected%20pixels%20and%20the%20learned%0Aradiance%20field%20stays%20in%20the%20final%20rendering.%20With%20this%20designed%20tolerance%20for%0Aphysical%20model%20limitations%2C%20we%20prevent%20unmodeled%20color%20components%20from%20leaking%0Ainto%20light%20and%20material%20parameters%2C%20alleviating%20relighting%20artifacts.%0AMeanwhile%2C%20the%20remaining%20radiance%20field%20compensates%20for%20the%20limitations%20of%20the%0Aphysical%20model%2C%20guaranteeing%20high-quality%20novel%20views%20synthesis.%20Experimental%0Aresults%20demonstrate%20that%20our%20method%20significantly%20outperforms%20state-of-the-art%0Atechniques%20quality-wise%20in%20both%20novel%20view%20synthesis%20and%20relighting.%20The%20idea%0Aof%20progressive%20radiance%20distillation%20is%20not%20limited%20to%20Gaussian%20splatting.%20We%0Ashow%20that%20it%20also%20has%20positive%20effects%20for%20prominently%20specular%20scenes%20when%0Aadapted%20to%20a%20mesh-based%20inverse%20rendering%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07595v1&entry.124074799=Read"},
{"title": "DIffSteISR: Harnessing Diffusion Prior for Superior Real-world Stereo\n  Image Super-Resolution", "author": "Yuanbo Zhou and Xinlin Zhang and Wei Deng and Tao Wang and Tao Tan and Qinquan Gao and Tong Tong", "abstract": "  We introduce DiffSteISR, a pioneering framework for reconstructing real-world\nstereo images. DiffSteISR utilizes the powerful prior knowledge embedded in\npre-trained text-to-image model to efficiently recover the lost texture details\nin low-resolution stereo images. Specifically, DiffSteISR implements a\ntime-aware stereo cross attention with temperature adapter (TASCATA) to guide\nthe diffusion process, ensuring that the generated left and right views exhibit\nhigh texture consistency thereby reducing disparity error between the\nsuper-resolved images and the ground truth (GT) images. Additionally, a stereo\nomni attention control network (SOA ControlNet) is proposed to enhance the\nconsistency of super-resolved images with GT images in the pixel, perceptual,\nand distribution space. Finally, DiffSteISR incorporates a stereo semantic\nextractor (SSE) to capture unique viewpoint soft semantic information and\nshared hard tag semantic information, thereby effectively improving the\nsemantic accuracy and consistency of the generated left and right images.\nExtensive experimental results demonstrate that DiffSteISR accurately\nreconstructs natural and precise textures from low-resolution stereo images\nwhile maintaining a high consistency of semantic and texture between the left\nand right views.\n", "link": "http://arxiv.org/abs/2408.07516v1", "date": "2024-08-14", "relevancy": 2.2453, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.591}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5634}, {"title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models", "link": "http://arxiv.org/abs/2405.10314v1", "similarity": 0.5473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DIffSteISR%3A%20Harnessing%20Diffusion%20Prior%20for%20Superior%20Real-world%20Stereo%0A%20%20Image%20Super-Resolution&body=Title%3A%20DIffSteISR%3A%20Harnessing%20Diffusion%20Prior%20for%20Superior%20Real-world%20Stereo%0A%20%20Image%20Super-Resolution%0AAuthor%3A%20Yuanbo%20Zhou%20and%20Xinlin%20Zhang%20and%20Wei%20Deng%20and%20Tao%20Wang%20and%20Tao%20Tan%20and%20Qinquan%20Gao%20and%20Tong%20Tong%0AAbstract%3A%20%20%20We%20introduce%20DiffSteISR%2C%20a%20pioneering%20framework%20for%20reconstructing%20real-world%0Astereo%20images.%20DiffSteISR%20utilizes%20the%20powerful%20prior%20knowledge%20embedded%20in%0Apre-trained%20text-to-image%20model%20to%20efficiently%20recover%20the%20lost%20texture%20details%0Ain%20low-resolution%20stereo%20images.%20Specifically%2C%20DiffSteISR%20implements%20a%0Atime-aware%20stereo%20cross%20attention%20with%20temperature%20adapter%20%28TASCATA%29%20to%20guide%0Athe%20diffusion%20process%2C%20ensuring%20that%20the%20generated%20left%20and%20right%20views%20exhibit%0Ahigh%20texture%20consistency%20thereby%20reducing%20disparity%20error%20between%20the%0Asuper-resolved%20images%20and%20the%20ground%20truth%20%28GT%29%20images.%20Additionally%2C%20a%20stereo%0Aomni%20attention%20control%20network%20%28SOA%20ControlNet%29%20is%20proposed%20to%20enhance%20the%0Aconsistency%20of%20super-resolved%20images%20with%20GT%20images%20in%20the%20pixel%2C%20perceptual%2C%0Aand%20distribution%20space.%20Finally%2C%20DiffSteISR%20incorporates%20a%20stereo%20semantic%0Aextractor%20%28SSE%29%20to%20capture%20unique%20viewpoint%20soft%20semantic%20information%20and%0Ashared%20hard%20tag%20semantic%20information%2C%20thereby%20effectively%20improving%20the%0Asemantic%20accuracy%20and%20consistency%20of%20the%20generated%20left%20and%20right%20images.%0AExtensive%20experimental%20results%20demonstrate%20that%20DiffSteISR%20accurately%0Areconstructs%20natural%20and%20precise%20textures%20from%20low-resolution%20stereo%20images%0Awhile%20maintaining%20a%20high%20consistency%20of%20semantic%20and%20texture%20between%20the%20left%0Aand%20right%20views.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07516v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDIffSteISR%253A%2520Harnessing%2520Diffusion%2520Prior%2520for%2520Superior%2520Real-world%2520Stereo%250A%2520%2520Image%2520Super-Resolution%26entry.906535625%3DYuanbo%2520Zhou%2520and%2520Xinlin%2520Zhang%2520and%2520Wei%2520Deng%2520and%2520Tao%2520Wang%2520and%2520Tao%2520Tan%2520and%2520Qinquan%2520Gao%2520and%2520Tong%2520Tong%26entry.1292438233%3D%2520%2520We%2520introduce%2520DiffSteISR%252C%2520a%2520pioneering%2520framework%2520for%2520reconstructing%2520real-world%250Astereo%2520images.%2520DiffSteISR%2520utilizes%2520the%2520powerful%2520prior%2520knowledge%2520embedded%2520in%250Apre-trained%2520text-to-image%2520model%2520to%2520efficiently%2520recover%2520the%2520lost%2520texture%2520details%250Ain%2520low-resolution%2520stereo%2520images.%2520Specifically%252C%2520DiffSteISR%2520implements%2520a%250Atime-aware%2520stereo%2520cross%2520attention%2520with%2520temperature%2520adapter%2520%2528TASCATA%2529%2520to%2520guide%250Athe%2520diffusion%2520process%252C%2520ensuring%2520that%2520the%2520generated%2520left%2520and%2520right%2520views%2520exhibit%250Ahigh%2520texture%2520consistency%2520thereby%2520reducing%2520disparity%2520error%2520between%2520the%250Asuper-resolved%2520images%2520and%2520the%2520ground%2520truth%2520%2528GT%2529%2520images.%2520Additionally%252C%2520a%2520stereo%250Aomni%2520attention%2520control%2520network%2520%2528SOA%2520ControlNet%2529%2520is%2520proposed%2520to%2520enhance%2520the%250Aconsistency%2520of%2520super-resolved%2520images%2520with%2520GT%2520images%2520in%2520the%2520pixel%252C%2520perceptual%252C%250Aand%2520distribution%2520space.%2520Finally%252C%2520DiffSteISR%2520incorporates%2520a%2520stereo%2520semantic%250Aextractor%2520%2528SSE%2529%2520to%2520capture%2520unique%2520viewpoint%2520soft%2520semantic%2520information%2520and%250Ashared%2520hard%2520tag%2520semantic%2520information%252C%2520thereby%2520effectively%2520improving%2520the%250Asemantic%2520accuracy%2520and%2520consistency%2520of%2520the%2520generated%2520left%2520and%2520right%2520images.%250AExtensive%2520experimental%2520results%2520demonstrate%2520that%2520DiffSteISR%2520accurately%250Areconstructs%2520natural%2520and%2520precise%2520textures%2520from%2520low-resolution%2520stereo%2520images%250Awhile%2520maintaining%2520a%2520high%2520consistency%2520of%2520semantic%2520and%2520texture%2520between%2520the%2520left%250Aand%2520right%2520views.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07516v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DIffSteISR%3A%20Harnessing%20Diffusion%20Prior%20for%20Superior%20Real-world%20Stereo%0A%20%20Image%20Super-Resolution&entry.906535625=Yuanbo%20Zhou%20and%20Xinlin%20Zhang%20and%20Wei%20Deng%20and%20Tao%20Wang%20and%20Tao%20Tan%20and%20Qinquan%20Gao%20and%20Tong%20Tong&entry.1292438233=%20%20We%20introduce%20DiffSteISR%2C%20a%20pioneering%20framework%20for%20reconstructing%20real-world%0Astereo%20images.%20DiffSteISR%20utilizes%20the%20powerful%20prior%20knowledge%20embedded%20in%0Apre-trained%20text-to-image%20model%20to%20efficiently%20recover%20the%20lost%20texture%20details%0Ain%20low-resolution%20stereo%20images.%20Specifically%2C%20DiffSteISR%20implements%20a%0Atime-aware%20stereo%20cross%20attention%20with%20temperature%20adapter%20%28TASCATA%29%20to%20guide%0Athe%20diffusion%20process%2C%20ensuring%20that%20the%20generated%20left%20and%20right%20views%20exhibit%0Ahigh%20texture%20consistency%20thereby%20reducing%20disparity%20error%20between%20the%0Asuper-resolved%20images%20and%20the%20ground%20truth%20%28GT%29%20images.%20Additionally%2C%20a%20stereo%0Aomni%20attention%20control%20network%20%28SOA%20ControlNet%29%20is%20proposed%20to%20enhance%20the%0Aconsistency%20of%20super-resolved%20images%20with%20GT%20images%20in%20the%20pixel%2C%20perceptual%2C%0Aand%20distribution%20space.%20Finally%2C%20DiffSteISR%20incorporates%20a%20stereo%20semantic%0Aextractor%20%28SSE%29%20to%20capture%20unique%20viewpoint%20soft%20semantic%20information%20and%0Ashared%20hard%20tag%20semantic%20information%2C%20thereby%20effectively%20improving%20the%0Asemantic%20accuracy%20and%20consistency%20of%20the%20generated%20left%20and%20right%20images.%0AExtensive%20experimental%20results%20demonstrate%20that%20DiffSteISR%20accurately%0Areconstructs%20natural%20and%20precise%20textures%20from%20low-resolution%20stereo%20images%0Awhile%20maintaining%20a%20high%20consistency%20of%20semantic%20and%20texture%20between%20the%20left%0Aand%20right%20views.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07516v1&entry.124074799=Read"},
{"title": "Large Language Models Prompting With Episodic Memory", "author": "Dai Do and Quan Tran and Svetha Venkatesh and Hung Le", "abstract": "  Prompt optimization is essential for enhancing the performance of Large\nLanguage Models (LLMs) in a range of Natural Language Processing (NLP) tasks,\nparticularly in scenarios of few-shot learning where training examples are\nincorporated directly into the prompt. Despite the growing interest in\noptimizing prompts with few-shot examples, existing methods for prompt\noptimization are often resource-intensive or perform inadequately. In this\nwork, we propose PrOmpting with Episodic Memory (POEM), a novel prompt\noptimization technique that is simple, efficient, and demonstrates strong\ngeneralization capabilities. We approach prompt optimization as a Reinforcement\nLearning (RL) challenge, using episodic memory to archive combinations of input\ndata, permutations of few-shot examples, and the rewards observed during\ntraining. In the testing phase, we optimize the sequence of examples for each\ntest query by selecting the sequence that yields the highest total rewards from\nthe top-k most similar training examples in the episodic memory. Our results\nshow that POEM outperforms recent techniques like TEMPERA and RLPrompt by over\n5.3% in various text classification tasks. Furthermore, our approach adapts\nwell to broader language understanding tasks, consistently outperforming\nconventional heuristic methods for ordering examples.\n", "link": "http://arxiv.org/abs/2408.07465v1", "date": "2024-08-14", "relevancy": 2.2378, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4494}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4481}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4452}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Large%20Language%20Models%20Prompting%20With%20Episodic%20Memory&body=Title%3A%20Large%20Language%20Models%20Prompting%20With%20Episodic%20Memory%0AAuthor%3A%20Dai%20Do%20and%20Quan%20Tran%20and%20Svetha%20Venkatesh%20and%20Hung%20Le%0AAbstract%3A%20%20%20Prompt%20optimization%20is%20essential%20for%20enhancing%20the%20performance%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20in%20a%20range%20of%20Natural%20Language%20Processing%20%28NLP%29%20tasks%2C%0Aparticularly%20in%20scenarios%20of%20few-shot%20learning%20where%20training%20examples%20are%0Aincorporated%20directly%20into%20the%20prompt.%20Despite%20the%20growing%20interest%20in%0Aoptimizing%20prompts%20with%20few-shot%20examples%2C%20existing%20methods%20for%20prompt%0Aoptimization%20are%20often%20resource-intensive%20or%20perform%20inadequately.%20In%20this%0Awork%2C%20we%20propose%20PrOmpting%20with%20Episodic%20Memory%20%28POEM%29%2C%20a%20novel%20prompt%0Aoptimization%20technique%20that%20is%20simple%2C%20efficient%2C%20and%20demonstrates%20strong%0Ageneralization%20capabilities.%20We%20approach%20prompt%20optimization%20as%20a%20Reinforcement%0ALearning%20%28RL%29%20challenge%2C%20using%20episodic%20memory%20to%20archive%20combinations%20of%20input%0Adata%2C%20permutations%20of%20few-shot%20examples%2C%20and%20the%20rewards%20observed%20during%0Atraining.%20In%20the%20testing%20phase%2C%20we%20optimize%20the%20sequence%20of%20examples%20for%20each%0Atest%20query%20by%20selecting%20the%20sequence%20that%20yields%20the%20highest%20total%20rewards%20from%0Athe%20top-k%20most%20similar%20training%20examples%20in%20the%20episodic%20memory.%20Our%20results%0Ashow%20that%20POEM%20outperforms%20recent%20techniques%20like%20TEMPERA%20and%20RLPrompt%20by%20over%0A5.3%25%20in%20various%20text%20classification%20tasks.%20Furthermore%2C%20our%20approach%20adapts%0Awell%20to%20broader%20language%20understanding%20tasks%2C%20consistently%20outperforming%0Aconventional%20heuristic%20methods%20for%20ordering%20examples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07465v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLarge%2520Language%2520Models%2520Prompting%2520With%2520Episodic%2520Memory%26entry.906535625%3DDai%2520Do%2520and%2520Quan%2520Tran%2520and%2520Svetha%2520Venkatesh%2520and%2520Hung%2520Le%26entry.1292438233%3D%2520%2520Prompt%2520optimization%2520is%2520essential%2520for%2520enhancing%2520the%2520performance%2520of%2520Large%250ALanguage%2520Models%2520%2528LLMs%2529%2520in%2520a%2520range%2520of%2520Natural%2520Language%2520Processing%2520%2528NLP%2529%2520tasks%252C%250Aparticularly%2520in%2520scenarios%2520of%2520few-shot%2520learning%2520where%2520training%2520examples%2520are%250Aincorporated%2520directly%2520into%2520the%2520prompt.%2520Despite%2520the%2520growing%2520interest%2520in%250Aoptimizing%2520prompts%2520with%2520few-shot%2520examples%252C%2520existing%2520methods%2520for%2520prompt%250Aoptimization%2520are%2520often%2520resource-intensive%2520or%2520perform%2520inadequately.%2520In%2520this%250Awork%252C%2520we%2520propose%2520PrOmpting%2520with%2520Episodic%2520Memory%2520%2528POEM%2529%252C%2520a%2520novel%2520prompt%250Aoptimization%2520technique%2520that%2520is%2520simple%252C%2520efficient%252C%2520and%2520demonstrates%2520strong%250Ageneralization%2520capabilities.%2520We%2520approach%2520prompt%2520optimization%2520as%2520a%2520Reinforcement%250ALearning%2520%2528RL%2529%2520challenge%252C%2520using%2520episodic%2520memory%2520to%2520archive%2520combinations%2520of%2520input%250Adata%252C%2520permutations%2520of%2520few-shot%2520examples%252C%2520and%2520the%2520rewards%2520observed%2520during%250Atraining.%2520In%2520the%2520testing%2520phase%252C%2520we%2520optimize%2520the%2520sequence%2520of%2520examples%2520for%2520each%250Atest%2520query%2520by%2520selecting%2520the%2520sequence%2520that%2520yields%2520the%2520highest%2520total%2520rewards%2520from%250Athe%2520top-k%2520most%2520similar%2520training%2520examples%2520in%2520the%2520episodic%2520memory.%2520Our%2520results%250Ashow%2520that%2520POEM%2520outperforms%2520recent%2520techniques%2520like%2520TEMPERA%2520and%2520RLPrompt%2520by%2520over%250A5.3%2525%2520in%2520various%2520text%2520classification%2520tasks.%2520Furthermore%252C%2520our%2520approach%2520adapts%250Awell%2520to%2520broader%2520language%2520understanding%2520tasks%252C%2520consistently%2520outperforming%250Aconventional%2520heuristic%2520methods%2520for%2520ordering%2520examples.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07465v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Large%20Language%20Models%20Prompting%20With%20Episodic%20Memory&entry.906535625=Dai%20Do%20and%20Quan%20Tran%20and%20Svetha%20Venkatesh%20and%20Hung%20Le&entry.1292438233=%20%20Prompt%20optimization%20is%20essential%20for%20enhancing%20the%20performance%20of%20Large%0ALanguage%20Models%20%28LLMs%29%20in%20a%20range%20of%20Natural%20Language%20Processing%20%28NLP%29%20tasks%2C%0Aparticularly%20in%20scenarios%20of%20few-shot%20learning%20where%20training%20examples%20are%0Aincorporated%20directly%20into%20the%20prompt.%20Despite%20the%20growing%20interest%20in%0Aoptimizing%20prompts%20with%20few-shot%20examples%2C%20existing%20methods%20for%20prompt%0Aoptimization%20are%20often%20resource-intensive%20or%20perform%20inadequately.%20In%20this%0Awork%2C%20we%20propose%20PrOmpting%20with%20Episodic%20Memory%20%28POEM%29%2C%20a%20novel%20prompt%0Aoptimization%20technique%20that%20is%20simple%2C%20efficient%2C%20and%20demonstrates%20strong%0Ageneralization%20capabilities.%20We%20approach%20prompt%20optimization%20as%20a%20Reinforcement%0ALearning%20%28RL%29%20challenge%2C%20using%20episodic%20memory%20to%20archive%20combinations%20of%20input%0Adata%2C%20permutations%20of%20few-shot%20examples%2C%20and%20the%20rewards%20observed%20during%0Atraining.%20In%20the%20testing%20phase%2C%20we%20optimize%20the%20sequence%20of%20examples%20for%20each%0Atest%20query%20by%20selecting%20the%20sequence%20that%20yields%20the%20highest%20total%20rewards%20from%0Athe%20top-k%20most%20similar%20training%20examples%20in%20the%20episodic%20memory.%20Our%20results%0Ashow%20that%20POEM%20outperforms%20recent%20techniques%20like%20TEMPERA%20and%20RLPrompt%20by%20over%0A5.3%25%20in%20various%20text%20classification%20tasks.%20Furthermore%2C%20our%20approach%20adapts%0Awell%20to%20broader%20language%20understanding%20tasks%2C%20consistently%20outperforming%0Aconventional%20heuristic%20methods%20for%20ordering%20examples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07465v1&entry.124074799=Read"},
{"title": "Cross-Platform Video Person ReID: A New Benchmark Dataset and Adaptation\n  Approach", "author": "Shizhou Zhang and Wenlong Luo and De Cheng and Qingchun Yang and Lingyan Ran and Yinghui Xing and Yanning Zhang", "abstract": "  In this paper, we construct a large-scale benchmark dataset for\nGround-to-Aerial Video-based person Re-Identification, named G2A-VReID, which\ncomprises 185,907 images and 5,576 tracklets, featuring 2,788 distinct\nidentities. To our knowledge, this is the first dataset for video ReID under\nGround-to-Aerial scenarios. G2A-VReID dataset has the following\ncharacteristics: 1) Drastic view changes; 2) Large number of annotated\nidentities; 3) Rich outdoor scenarios; 4) Huge difference in resolution.\nAdditionally, we propose a new benchmark approach for cross-platform ReID by\ntransforming the cross-platform visual alignment problem into visual-semantic\nalignment through vision-language model (i.e., CLIP) and applying a\nparameter-efficient Video Set-Level-Adapter module to adapt image-based\nfoundation model to video ReID tasks, termed VSLA-CLIP. Besides, to further\nreduce the great discrepancy across the platforms, we also devise the\nplatform-bridge prompts for efficient visual feature alignment. Extensive\nexperiments demonstrate the superiority of the proposed method on all existing\nvideo ReID datasets and our proposed G2A-VReID dataset.\n", "link": "http://arxiv.org/abs/2408.07500v1", "date": "2024-08-14", "relevancy": 2.2352, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5855}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5419}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5388}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Cross-Platform%20Video%20Person%20ReID%3A%20A%20New%20Benchmark%20Dataset%20and%20Adaptation%0A%20%20Approach&body=Title%3A%20Cross-Platform%20Video%20Person%20ReID%3A%20A%20New%20Benchmark%20Dataset%20and%20Adaptation%0A%20%20Approach%0AAuthor%3A%20Shizhou%20Zhang%20and%20Wenlong%20Luo%20and%20De%20Cheng%20and%20Qingchun%20Yang%20and%20Lingyan%20Ran%20and%20Yinghui%20Xing%20and%20Yanning%20Zhang%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20construct%20a%20large-scale%20benchmark%20dataset%20for%0AGround-to-Aerial%20Video-based%20person%20Re-Identification%2C%20named%20G2A-VReID%2C%20which%0Acomprises%20185%2C907%20images%20and%205%2C576%20tracklets%2C%20featuring%202%2C788%20distinct%0Aidentities.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20dataset%20for%20video%20ReID%20under%0AGround-to-Aerial%20scenarios.%20G2A-VReID%20dataset%20has%20the%20following%0Acharacteristics%3A%201%29%20Drastic%20view%20changes%3B%202%29%20Large%20number%20of%20annotated%0Aidentities%3B%203%29%20Rich%20outdoor%20scenarios%3B%204%29%20Huge%20difference%20in%20resolution.%0AAdditionally%2C%20we%20propose%20a%20new%20benchmark%20approach%20for%20cross-platform%20ReID%20by%0Atransforming%20the%20cross-platform%20visual%20alignment%20problem%20into%20visual-semantic%0Aalignment%20through%20vision-language%20model%20%28i.e.%2C%20CLIP%29%20and%20applying%20a%0Aparameter-efficient%20Video%20Set-Level-Adapter%20module%20to%20adapt%20image-based%0Afoundation%20model%20to%20video%20ReID%20tasks%2C%20termed%20VSLA-CLIP.%20Besides%2C%20to%20further%0Areduce%20the%20great%20discrepancy%20across%20the%20platforms%2C%20we%20also%20devise%20the%0Aplatform-bridge%20prompts%20for%20efficient%20visual%20feature%20alignment.%20Extensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20the%20proposed%20method%20on%20all%20existing%0Avideo%20ReID%20datasets%20and%20our%20proposed%20G2A-VReID%20dataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07500v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DCross-Platform%2520Video%2520Person%2520ReID%253A%2520A%2520New%2520Benchmark%2520Dataset%2520and%2520Adaptation%250A%2520%2520Approach%26entry.906535625%3DShizhou%2520Zhang%2520and%2520Wenlong%2520Luo%2520and%2520De%2520Cheng%2520and%2520Qingchun%2520Yang%2520and%2520Lingyan%2520Ran%2520and%2520Yinghui%2520Xing%2520and%2520Yanning%2520Zhang%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520construct%2520a%2520large-scale%2520benchmark%2520dataset%2520for%250AGround-to-Aerial%2520Video-based%2520person%2520Re-Identification%252C%2520named%2520G2A-VReID%252C%2520which%250Acomprises%2520185%252C907%2520images%2520and%25205%252C576%2520tracklets%252C%2520featuring%25202%252C788%2520distinct%250Aidentities.%2520To%2520our%2520knowledge%252C%2520this%2520is%2520the%2520first%2520dataset%2520for%2520video%2520ReID%2520under%250AGround-to-Aerial%2520scenarios.%2520G2A-VReID%2520dataset%2520has%2520the%2520following%250Acharacteristics%253A%25201%2529%2520Drastic%2520view%2520changes%253B%25202%2529%2520Large%2520number%2520of%2520annotated%250Aidentities%253B%25203%2529%2520Rich%2520outdoor%2520scenarios%253B%25204%2529%2520Huge%2520difference%2520in%2520resolution.%250AAdditionally%252C%2520we%2520propose%2520a%2520new%2520benchmark%2520approach%2520for%2520cross-platform%2520ReID%2520by%250Atransforming%2520the%2520cross-platform%2520visual%2520alignment%2520problem%2520into%2520visual-semantic%250Aalignment%2520through%2520vision-language%2520model%2520%2528i.e.%252C%2520CLIP%2529%2520and%2520applying%2520a%250Aparameter-efficient%2520Video%2520Set-Level-Adapter%2520module%2520to%2520adapt%2520image-based%250Afoundation%2520model%2520to%2520video%2520ReID%2520tasks%252C%2520termed%2520VSLA-CLIP.%2520Besides%252C%2520to%2520further%250Areduce%2520the%2520great%2520discrepancy%2520across%2520the%2520platforms%252C%2520we%2520also%2520devise%2520the%250Aplatform-bridge%2520prompts%2520for%2520efficient%2520visual%2520feature%2520alignment.%2520Extensive%250Aexperiments%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%2520method%2520on%2520all%2520existing%250Avideo%2520ReID%2520datasets%2520and%2520our%2520proposed%2520G2A-VReID%2520dataset.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07500v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Cross-Platform%20Video%20Person%20ReID%3A%20A%20New%20Benchmark%20Dataset%20and%20Adaptation%0A%20%20Approach&entry.906535625=Shizhou%20Zhang%20and%20Wenlong%20Luo%20and%20De%20Cheng%20and%20Qingchun%20Yang%20and%20Lingyan%20Ran%20and%20Yinghui%20Xing%20and%20Yanning%20Zhang&entry.1292438233=%20%20In%20this%20paper%2C%20we%20construct%20a%20large-scale%20benchmark%20dataset%20for%0AGround-to-Aerial%20Video-based%20person%20Re-Identification%2C%20named%20G2A-VReID%2C%20which%0Acomprises%20185%2C907%20images%20and%205%2C576%20tracklets%2C%20featuring%202%2C788%20distinct%0Aidentities.%20To%20our%20knowledge%2C%20this%20is%20the%20first%20dataset%20for%20video%20ReID%20under%0AGround-to-Aerial%20scenarios.%20G2A-VReID%20dataset%20has%20the%20following%0Acharacteristics%3A%201%29%20Drastic%20view%20changes%3B%202%29%20Large%20number%20of%20annotated%0Aidentities%3B%203%29%20Rich%20outdoor%20scenarios%3B%204%29%20Huge%20difference%20in%20resolution.%0AAdditionally%2C%20we%20propose%20a%20new%20benchmark%20approach%20for%20cross-platform%20ReID%20by%0Atransforming%20the%20cross-platform%20visual%20alignment%20problem%20into%20visual-semantic%0Aalignment%20through%20vision-language%20model%20%28i.e.%2C%20CLIP%29%20and%20applying%20a%0Aparameter-efficient%20Video%20Set-Level-Adapter%20module%20to%20adapt%20image-based%0Afoundation%20model%20to%20video%20ReID%20tasks%2C%20termed%20VSLA-CLIP.%20Besides%2C%20to%20further%0Areduce%20the%20great%20discrepancy%20across%20the%20platforms%2C%20we%20also%20devise%20the%0Aplatform-bridge%20prompts%20for%20efficient%20visual%20feature%20alignment.%20Extensive%0Aexperiments%20demonstrate%20the%20superiority%20of%20the%20proposed%20method%20on%20all%20existing%0Avideo%20ReID%20datasets%20and%20our%20proposed%20G2A-VReID%20dataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07500v1&entry.124074799=Read"},
{"title": "UAHOI: Uncertainty-aware Robust Interaction Learning for HOI Detection", "author": "Mu Chen and Minghan Chen and Yi Yang", "abstract": "  This paper focuses on Human-Object Interaction (HOI) detection, addressing\nthe challenge of identifying and understanding the interactions between humans\nand objects within a given image or video frame. Spearheaded by Detection\nTransformer (DETR), recent developments lead to significant improvements by\nreplacing traditional region proposals by a set of learnable queries. However,\ndespite the powerful representation capabilities provided by Transformers,\nexisting Human-Object Interaction (HOI) detection methods still yield low\nconfidence levels when dealing with complex interactions and are prone to\noverlooking interactive actions. To address these issues, we propose a novel\napproach \\textsc{UAHOI}, Uncertainty-aware Robust Human-Object Interaction\nLearning that explicitly estimates prediction uncertainty during the training\nprocess to refine both detection and interaction predictions. Our model not\nonly predicts the HOI triplets but also quantifies the uncertainty of these\npredictions. Specifically, we model this uncertainty through the variance of\npredictions and incorporate it into the optimization objective, allowing the\nmodel to adaptively adjust its confidence threshold based on prediction\nvariance. This integration helps in mitigating the adverse effects of incorrect\nor ambiguous predictions that are common in traditional methods without any\nhand-designed components, serving as an automatic confidence threshold. Our\nmethod is flexible to existing HOI detection methods and demonstrates improved\naccuracy. We evaluate \\textsc{UAHOI} on two standard benchmarks in the field:\nV-COCO and HICO-DET, which represent challenging scenarios for HOI detection.\nThrough extensive experiments, we demonstrate that \\textsc{UAHOI} achieves\nsignificant improvements over existing state-of-the-art methods, enhancing both\nthe accuracy and robustness of HOI detection.\n", "link": "http://arxiv.org/abs/2408.07430v1", "date": "2024-08-14", "relevancy": 2.22, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.605}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.553}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5369}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20UAHOI%3A%20Uncertainty-aware%20Robust%20Interaction%20Learning%20for%20HOI%20Detection&body=Title%3A%20UAHOI%3A%20Uncertainty-aware%20Robust%20Interaction%20Learning%20for%20HOI%20Detection%0AAuthor%3A%20Mu%20Chen%20and%20Minghan%20Chen%20and%20Yi%20Yang%0AAbstract%3A%20%20%20This%20paper%20focuses%20on%20Human-Object%20Interaction%20%28HOI%29%20detection%2C%20addressing%0Athe%20challenge%20of%20identifying%20and%20understanding%20the%20interactions%20between%20humans%0Aand%20objects%20within%20a%20given%20image%20or%20video%20frame.%20Spearheaded%20by%20Detection%0ATransformer%20%28DETR%29%2C%20recent%20developments%20lead%20to%20significant%20improvements%20by%0Areplacing%20traditional%20region%20proposals%20by%20a%20set%20of%20learnable%20queries.%20However%2C%0Adespite%20the%20powerful%20representation%20capabilities%20provided%20by%20Transformers%2C%0Aexisting%20Human-Object%20Interaction%20%28HOI%29%20detection%20methods%20still%20yield%20low%0Aconfidence%20levels%20when%20dealing%20with%20complex%20interactions%20and%20are%20prone%20to%0Aoverlooking%20interactive%20actions.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%0Aapproach%20%5Ctextsc%7BUAHOI%7D%2C%20Uncertainty-aware%20Robust%20Human-Object%20Interaction%0ALearning%20that%20explicitly%20estimates%20prediction%20uncertainty%20during%20the%20training%0Aprocess%20to%20refine%20both%20detection%20and%20interaction%20predictions.%20Our%20model%20not%0Aonly%20predicts%20the%20HOI%20triplets%20but%20also%20quantifies%20the%20uncertainty%20of%20these%0Apredictions.%20Specifically%2C%20we%20model%20this%20uncertainty%20through%20the%20variance%20of%0Apredictions%20and%20incorporate%20it%20into%20the%20optimization%20objective%2C%20allowing%20the%0Amodel%20to%20adaptively%20adjust%20its%20confidence%20threshold%20based%20on%20prediction%0Avariance.%20This%20integration%20helps%20in%20mitigating%20the%20adverse%20effects%20of%20incorrect%0Aor%20ambiguous%20predictions%20that%20are%20common%20in%20traditional%20methods%20without%20any%0Ahand-designed%20components%2C%20serving%20as%20an%20automatic%20confidence%20threshold.%20Our%0Amethod%20is%20flexible%20to%20existing%20HOI%20detection%20methods%20and%20demonstrates%20improved%0Aaccuracy.%20We%20evaluate%20%5Ctextsc%7BUAHOI%7D%20on%20two%20standard%20benchmarks%20in%20the%20field%3A%0AV-COCO%20and%20HICO-DET%2C%20which%20represent%20challenging%20scenarios%20for%20HOI%20detection.%0AThrough%20extensive%20experiments%2C%20we%20demonstrate%20that%20%5Ctextsc%7BUAHOI%7D%20achieves%0Asignificant%20improvements%20over%20existing%20state-of-the-art%20methods%2C%20enhancing%20both%0Athe%20accuracy%20and%20robustness%20of%20HOI%20detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07430v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUAHOI%253A%2520Uncertainty-aware%2520Robust%2520Interaction%2520Learning%2520for%2520HOI%2520Detection%26entry.906535625%3DMu%2520Chen%2520and%2520Minghan%2520Chen%2520and%2520Yi%2520Yang%26entry.1292438233%3D%2520%2520This%2520paper%2520focuses%2520on%2520Human-Object%2520Interaction%2520%2528HOI%2529%2520detection%252C%2520addressing%250Athe%2520challenge%2520of%2520identifying%2520and%2520understanding%2520the%2520interactions%2520between%2520humans%250Aand%2520objects%2520within%2520a%2520given%2520image%2520or%2520video%2520frame.%2520Spearheaded%2520by%2520Detection%250ATransformer%2520%2528DETR%2529%252C%2520recent%2520developments%2520lead%2520to%2520significant%2520improvements%2520by%250Areplacing%2520traditional%2520region%2520proposals%2520by%2520a%2520set%2520of%2520learnable%2520queries.%2520However%252C%250Adespite%2520the%2520powerful%2520representation%2520capabilities%2520provided%2520by%2520Transformers%252C%250Aexisting%2520Human-Object%2520Interaction%2520%2528HOI%2529%2520detection%2520methods%2520still%2520yield%2520low%250Aconfidence%2520levels%2520when%2520dealing%2520with%2520complex%2520interactions%2520and%2520are%2520prone%2520to%250Aoverlooking%2520interactive%2520actions.%2520To%2520address%2520these%2520issues%252C%2520we%2520propose%2520a%2520novel%250Aapproach%2520%255Ctextsc%257BUAHOI%257D%252C%2520Uncertainty-aware%2520Robust%2520Human-Object%2520Interaction%250ALearning%2520that%2520explicitly%2520estimates%2520prediction%2520uncertainty%2520during%2520the%2520training%250Aprocess%2520to%2520refine%2520both%2520detection%2520and%2520interaction%2520predictions.%2520Our%2520model%2520not%250Aonly%2520predicts%2520the%2520HOI%2520triplets%2520but%2520also%2520quantifies%2520the%2520uncertainty%2520of%2520these%250Apredictions.%2520Specifically%252C%2520we%2520model%2520this%2520uncertainty%2520through%2520the%2520variance%2520of%250Apredictions%2520and%2520incorporate%2520it%2520into%2520the%2520optimization%2520objective%252C%2520allowing%2520the%250Amodel%2520to%2520adaptively%2520adjust%2520its%2520confidence%2520threshold%2520based%2520on%2520prediction%250Avariance.%2520This%2520integration%2520helps%2520in%2520mitigating%2520the%2520adverse%2520effects%2520of%2520incorrect%250Aor%2520ambiguous%2520predictions%2520that%2520are%2520common%2520in%2520traditional%2520methods%2520without%2520any%250Ahand-designed%2520components%252C%2520serving%2520as%2520an%2520automatic%2520confidence%2520threshold.%2520Our%250Amethod%2520is%2520flexible%2520to%2520existing%2520HOI%2520detection%2520methods%2520and%2520demonstrates%2520improved%250Aaccuracy.%2520We%2520evaluate%2520%255Ctextsc%257BUAHOI%257D%2520on%2520two%2520standard%2520benchmarks%2520in%2520the%2520field%253A%250AV-COCO%2520and%2520HICO-DET%252C%2520which%2520represent%2520challenging%2520scenarios%2520for%2520HOI%2520detection.%250AThrough%2520extensive%2520experiments%252C%2520we%2520demonstrate%2520that%2520%255Ctextsc%257BUAHOI%257D%2520achieves%250Asignificant%2520improvements%2520over%2520existing%2520state-of-the-art%2520methods%252C%2520enhancing%2520both%250Athe%2520accuracy%2520and%2520robustness%2520of%2520HOI%2520detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07430v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=UAHOI%3A%20Uncertainty-aware%20Robust%20Interaction%20Learning%20for%20HOI%20Detection&entry.906535625=Mu%20Chen%20and%20Minghan%20Chen%20and%20Yi%20Yang&entry.1292438233=%20%20This%20paper%20focuses%20on%20Human-Object%20Interaction%20%28HOI%29%20detection%2C%20addressing%0Athe%20challenge%20of%20identifying%20and%20understanding%20the%20interactions%20between%20humans%0Aand%20objects%20within%20a%20given%20image%20or%20video%20frame.%20Spearheaded%20by%20Detection%0ATransformer%20%28DETR%29%2C%20recent%20developments%20lead%20to%20significant%20improvements%20by%0Areplacing%20traditional%20region%20proposals%20by%20a%20set%20of%20learnable%20queries.%20However%2C%0Adespite%20the%20powerful%20representation%20capabilities%20provided%20by%20Transformers%2C%0Aexisting%20Human-Object%20Interaction%20%28HOI%29%20detection%20methods%20still%20yield%20low%0Aconfidence%20levels%20when%20dealing%20with%20complex%20interactions%20and%20are%20prone%20to%0Aoverlooking%20interactive%20actions.%20To%20address%20these%20issues%2C%20we%20propose%20a%20novel%0Aapproach%20%5Ctextsc%7BUAHOI%7D%2C%20Uncertainty-aware%20Robust%20Human-Object%20Interaction%0ALearning%20that%20explicitly%20estimates%20prediction%20uncertainty%20during%20the%20training%0Aprocess%20to%20refine%20both%20detection%20and%20interaction%20predictions.%20Our%20model%20not%0Aonly%20predicts%20the%20HOI%20triplets%20but%20also%20quantifies%20the%20uncertainty%20of%20these%0Apredictions.%20Specifically%2C%20we%20model%20this%20uncertainty%20through%20the%20variance%20of%0Apredictions%20and%20incorporate%20it%20into%20the%20optimization%20objective%2C%20allowing%20the%0Amodel%20to%20adaptively%20adjust%20its%20confidence%20threshold%20based%20on%20prediction%0Avariance.%20This%20integration%20helps%20in%20mitigating%20the%20adverse%20effects%20of%20incorrect%0Aor%20ambiguous%20predictions%20that%20are%20common%20in%20traditional%20methods%20without%20any%0Ahand-designed%20components%2C%20serving%20as%20an%20automatic%20confidence%20threshold.%20Our%0Amethod%20is%20flexible%20to%20existing%20HOI%20detection%20methods%20and%20demonstrates%20improved%0Aaccuracy.%20We%20evaluate%20%5Ctextsc%7BUAHOI%7D%20on%20two%20standard%20benchmarks%20in%20the%20field%3A%0AV-COCO%20and%20HICO-DET%2C%20which%20represent%20challenging%20scenarios%20for%20HOI%20detection.%0AThrough%20extensive%20experiments%2C%20we%20demonstrate%20that%20%5Ctextsc%7BUAHOI%7D%20achieves%0Asignificant%20improvements%20over%20existing%20state-of-the-art%20methods%2C%20enhancing%20both%0Athe%20accuracy%20and%20robustness%20of%20HOI%20detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07430v1&entry.124074799=Read"},
{"title": "End-to-end Semantic-centric Video-based Multimodal Affective Computing", "author": "Ronghao Lin and Ying Zeng and Sijie Mai and Haifeng Hu", "abstract": "  In the pathway toward Artificial General Intelligence (AGI), understanding\nhuman's affection is essential to enhance machine's cognition abilities. For\nachieving more sensual human-AI interaction, Multimodal Affective Computing\n(MAC) in human-spoken videos has attracted increasing attention. However,\nprevious methods are mainly devoted to designing multimodal fusion algorithms,\nsuffering from two issues: semantic imbalance caused by diverse pre-processing\noperations and semantic mismatch raised by inconsistent affection content\ncontained in different modalities comparing with the multimodal ground truth.\nBesides, the usage of manual features extractors make they fail in building\nend-to-end pipeline for multiple MAC downstream tasks. To address above\nchallenges, we propose a novel end-to-end framework named SemanticMAC to\ncompute multimodal semantic-centric affection for human-spoken videos. We\nfirstly employ pre-trained Transformer model in multimodal data pre-processing\nand design Affective Perceiver module to capture unimodal affective\ninformation. Moreover, we present a semantic-centric approach to unify\nmultimodal representation learning in three ways, including gated feature\ninteraction, multi-task pseudo label generation, and intra-/inter-sample\ncontrastive learning. Finally, SemanticMAC effectively learn specific- and\nshared-semantic representations in the guidance of semantic-centric labels.\nExtensive experimental results demonstrate that our approach surpass the\nstate-of-the-art methods on 7 public datasets in four MAC downstream tasks.\n", "link": "http://arxiv.org/abs/2408.07694v1", "date": "2024-08-14", "relevancy": 2.2171, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5608}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5529}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5483}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20End-to-end%20Semantic-centric%20Video-based%20Multimodal%20Affective%20Computing&body=Title%3A%20End-to-end%20Semantic-centric%20Video-based%20Multimodal%20Affective%20Computing%0AAuthor%3A%20Ronghao%20Lin%20and%20Ying%20Zeng%20and%20Sijie%20Mai%20and%20Haifeng%20Hu%0AAbstract%3A%20%20%20In%20the%20pathway%20toward%20Artificial%20General%20Intelligence%20%28AGI%29%2C%20understanding%0Ahuman%27s%20affection%20is%20essential%20to%20enhance%20machine%27s%20cognition%20abilities.%20For%0Aachieving%20more%20sensual%20human-AI%20interaction%2C%20Multimodal%20Affective%20Computing%0A%28MAC%29%20in%20human-spoken%20videos%20has%20attracted%20increasing%20attention.%20However%2C%0Aprevious%20methods%20are%20mainly%20devoted%20to%20designing%20multimodal%20fusion%20algorithms%2C%0Asuffering%20from%20two%20issues%3A%20semantic%20imbalance%20caused%20by%20diverse%20pre-processing%0Aoperations%20and%20semantic%20mismatch%20raised%20by%20inconsistent%20affection%20content%0Acontained%20in%20different%20modalities%20comparing%20with%20the%20multimodal%20ground%20truth.%0ABesides%2C%20the%20usage%20of%20manual%20features%20extractors%20make%20they%20fail%20in%20building%0Aend-to-end%20pipeline%20for%20multiple%20MAC%20downstream%20tasks.%20To%20address%20above%0Achallenges%2C%20we%20propose%20a%20novel%20end-to-end%20framework%20named%20SemanticMAC%20to%0Acompute%20multimodal%20semantic-centric%20affection%20for%20human-spoken%20videos.%20We%0Afirstly%20employ%20pre-trained%20Transformer%20model%20in%20multimodal%20data%20pre-processing%0Aand%20design%20Affective%20Perceiver%20module%20to%20capture%20unimodal%20affective%0Ainformation.%20Moreover%2C%20we%20present%20a%20semantic-centric%20approach%20to%20unify%0Amultimodal%20representation%20learning%20in%20three%20ways%2C%20including%20gated%20feature%0Ainteraction%2C%20multi-task%20pseudo%20label%20generation%2C%20and%20intra-/inter-sample%0Acontrastive%20learning.%20Finally%2C%20SemanticMAC%20effectively%20learn%20specific-%20and%0Ashared-semantic%20representations%20in%20the%20guidance%20of%20semantic-centric%20labels.%0AExtensive%20experimental%20results%20demonstrate%20that%20our%20approach%20surpass%20the%0Astate-of-the-art%20methods%20on%207%20public%20datasets%20in%20four%20MAC%20downstream%20tasks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07694v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEnd-to-end%2520Semantic-centric%2520Video-based%2520Multimodal%2520Affective%2520Computing%26entry.906535625%3DRonghao%2520Lin%2520and%2520Ying%2520Zeng%2520and%2520Sijie%2520Mai%2520and%2520Haifeng%2520Hu%26entry.1292438233%3D%2520%2520In%2520the%2520pathway%2520toward%2520Artificial%2520General%2520Intelligence%2520%2528AGI%2529%252C%2520understanding%250Ahuman%2527s%2520affection%2520is%2520essential%2520to%2520enhance%2520machine%2527s%2520cognition%2520abilities.%2520For%250Aachieving%2520more%2520sensual%2520human-AI%2520interaction%252C%2520Multimodal%2520Affective%2520Computing%250A%2528MAC%2529%2520in%2520human-spoken%2520videos%2520has%2520attracted%2520increasing%2520attention.%2520However%252C%250Aprevious%2520methods%2520are%2520mainly%2520devoted%2520to%2520designing%2520multimodal%2520fusion%2520algorithms%252C%250Asuffering%2520from%2520two%2520issues%253A%2520semantic%2520imbalance%2520caused%2520by%2520diverse%2520pre-processing%250Aoperations%2520and%2520semantic%2520mismatch%2520raised%2520by%2520inconsistent%2520affection%2520content%250Acontained%2520in%2520different%2520modalities%2520comparing%2520with%2520the%2520multimodal%2520ground%2520truth.%250ABesides%252C%2520the%2520usage%2520of%2520manual%2520features%2520extractors%2520make%2520they%2520fail%2520in%2520building%250Aend-to-end%2520pipeline%2520for%2520multiple%2520MAC%2520downstream%2520tasks.%2520To%2520address%2520above%250Achallenges%252C%2520we%2520propose%2520a%2520novel%2520end-to-end%2520framework%2520named%2520SemanticMAC%2520to%250Acompute%2520multimodal%2520semantic-centric%2520affection%2520for%2520human-spoken%2520videos.%2520We%250Afirstly%2520employ%2520pre-trained%2520Transformer%2520model%2520in%2520multimodal%2520data%2520pre-processing%250Aand%2520design%2520Affective%2520Perceiver%2520module%2520to%2520capture%2520unimodal%2520affective%250Ainformation.%2520Moreover%252C%2520we%2520present%2520a%2520semantic-centric%2520approach%2520to%2520unify%250Amultimodal%2520representation%2520learning%2520in%2520three%2520ways%252C%2520including%2520gated%2520feature%250Ainteraction%252C%2520multi-task%2520pseudo%2520label%2520generation%252C%2520and%2520intra-/inter-sample%250Acontrastive%2520learning.%2520Finally%252C%2520SemanticMAC%2520effectively%2520learn%2520specific-%2520and%250Ashared-semantic%2520representations%2520in%2520the%2520guidance%2520of%2520semantic-centric%2520labels.%250AExtensive%2520experimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520surpass%2520the%250Astate-of-the-art%2520methods%2520on%25207%2520public%2520datasets%2520in%2520four%2520MAC%2520downstream%2520tasks.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07694v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=End-to-end%20Semantic-centric%20Video-based%20Multimodal%20Affective%20Computing&entry.906535625=Ronghao%20Lin%20and%20Ying%20Zeng%20and%20Sijie%20Mai%20and%20Haifeng%20Hu&entry.1292438233=%20%20In%20the%20pathway%20toward%20Artificial%20General%20Intelligence%20%28AGI%29%2C%20understanding%0Ahuman%27s%20affection%20is%20essential%20to%20enhance%20machine%27s%20cognition%20abilities.%20For%0Aachieving%20more%20sensual%20human-AI%20interaction%2C%20Multimodal%20Affective%20Computing%0A%28MAC%29%20in%20human-spoken%20videos%20has%20attracted%20increasing%20attention.%20However%2C%0Aprevious%20methods%20are%20mainly%20devoted%20to%20designing%20multimodal%20fusion%20algorithms%2C%0Asuffering%20from%20two%20issues%3A%20semantic%20imbalance%20caused%20by%20diverse%20pre-processing%0Aoperations%20and%20semantic%20mismatch%20raised%20by%20inconsistent%20affection%20content%0Acontained%20in%20different%20modalities%20comparing%20with%20the%20multimodal%20ground%20truth.%0ABesides%2C%20the%20usage%20of%20manual%20features%20extractors%20make%20they%20fail%20in%20building%0Aend-to-end%20pipeline%20for%20multiple%20MAC%20downstream%20tasks.%20To%20address%20above%0Achallenges%2C%20we%20propose%20a%20novel%20end-to-end%20framework%20named%20SemanticMAC%20to%0Acompute%20multimodal%20semantic-centric%20affection%20for%20human-spoken%20videos.%20We%0Afirstly%20employ%20pre-trained%20Transformer%20model%20in%20multimodal%20data%20pre-processing%0Aand%20design%20Affective%20Perceiver%20module%20to%20capture%20unimodal%20affective%0Ainformation.%20Moreover%2C%20we%20present%20a%20semantic-centric%20approach%20to%20unify%0Amultimodal%20representation%20learning%20in%20three%20ways%2C%20including%20gated%20feature%0Ainteraction%2C%20multi-task%20pseudo%20label%20generation%2C%20and%20intra-/inter-sample%0Acontrastive%20learning.%20Finally%2C%20SemanticMAC%20effectively%20learn%20specific-%20and%0Ashared-semantic%20representations%20in%20the%20guidance%20of%20semantic-centric%20labels.%0AExtensive%20experimental%20results%20demonstrate%20that%20our%20approach%20surpass%20the%0Astate-of-the-art%20methods%20on%207%20public%20datasets%20in%20four%20MAC%20downstream%20tasks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07694v1&entry.124074799=Read"},
{"title": "Achieving Data Efficient Neural Networks with Hybrid Concept-based\n  Models", "author": "Tobias A. Opsahl and Vegard Antun", "abstract": "  Most datasets used for supervised machine learning consist of a single label\nper data point. However, in cases where more information than just the class\nlabel is available, would it be possible to train models more efficiently? We\nintroduce two novel model architectures, which we call hybrid concept-based\nmodels, that train using both class labels and additional information in the\ndataset referred to as concepts. In order to thoroughly assess their\nperformance, we introduce ConceptShapes, an open and flexible class of datasets\nwith concept labels. We show that the hybrid concept-based models outperform\nstandard computer vision models and previously proposed concept-based models\nwith respect to accuracy, especially in sparse data settings. We also introduce\nan algorithm for performing adversarial concept attacks, where an image is\nperturbed in a way that does not change a concept-based model's concept\npredictions, but changes the class prediction. The existence of such\nadversarial examples raises questions about the interpretable qualities\npromised by concept-based models.\n", "link": "http://arxiv.org/abs/2408.07438v1", "date": "2024-08-14", "relevancy": 2.2047, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5665}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5439}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5311}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Achieving%20Data%20Efficient%20Neural%20Networks%20with%20Hybrid%20Concept-based%0A%20%20Models&body=Title%3A%20Achieving%20Data%20Efficient%20Neural%20Networks%20with%20Hybrid%20Concept-based%0A%20%20Models%0AAuthor%3A%20Tobias%20A.%20Opsahl%20and%20Vegard%20Antun%0AAbstract%3A%20%20%20Most%20datasets%20used%20for%20supervised%20machine%20learning%20consist%20of%20a%20single%20label%0Aper%20data%20point.%20However%2C%20in%20cases%20where%20more%20information%20than%20just%20the%20class%0Alabel%20is%20available%2C%20would%20it%20be%20possible%20to%20train%20models%20more%20efficiently%3F%20We%0Aintroduce%20two%20novel%20model%20architectures%2C%20which%20we%20call%20hybrid%20concept-based%0Amodels%2C%20that%20train%20using%20both%20class%20labels%20and%20additional%20information%20in%20the%0Adataset%20referred%20to%20as%20concepts.%20In%20order%20to%20thoroughly%20assess%20their%0Aperformance%2C%20we%20introduce%20ConceptShapes%2C%20an%20open%20and%20flexible%20class%20of%20datasets%0Awith%20concept%20labels.%20We%20show%20that%20the%20hybrid%20concept-based%20models%20outperform%0Astandard%20computer%20vision%20models%20and%20previously%20proposed%20concept-based%20models%0Awith%20respect%20to%20accuracy%2C%20especially%20in%20sparse%20data%20settings.%20We%20also%20introduce%0Aan%20algorithm%20for%20performing%20adversarial%20concept%20attacks%2C%20where%20an%20image%20is%0Aperturbed%20in%20a%20way%20that%20does%20not%20change%20a%20concept-based%20model%27s%20concept%0Apredictions%2C%20but%20changes%20the%20class%20prediction.%20The%20existence%20of%20such%0Aadversarial%20examples%20raises%20questions%20about%20the%20interpretable%20qualities%0Apromised%20by%20concept-based%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07438v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAchieving%2520Data%2520Efficient%2520Neural%2520Networks%2520with%2520Hybrid%2520Concept-based%250A%2520%2520Models%26entry.906535625%3DTobias%2520A.%2520Opsahl%2520and%2520Vegard%2520Antun%26entry.1292438233%3D%2520%2520Most%2520datasets%2520used%2520for%2520supervised%2520machine%2520learning%2520consist%2520of%2520a%2520single%2520label%250Aper%2520data%2520point.%2520However%252C%2520in%2520cases%2520where%2520more%2520information%2520than%2520just%2520the%2520class%250Alabel%2520is%2520available%252C%2520would%2520it%2520be%2520possible%2520to%2520train%2520models%2520more%2520efficiently%253F%2520We%250Aintroduce%2520two%2520novel%2520model%2520architectures%252C%2520which%2520we%2520call%2520hybrid%2520concept-based%250Amodels%252C%2520that%2520train%2520using%2520both%2520class%2520labels%2520and%2520additional%2520information%2520in%2520the%250Adataset%2520referred%2520to%2520as%2520concepts.%2520In%2520order%2520to%2520thoroughly%2520assess%2520their%250Aperformance%252C%2520we%2520introduce%2520ConceptShapes%252C%2520an%2520open%2520and%2520flexible%2520class%2520of%2520datasets%250Awith%2520concept%2520labels.%2520We%2520show%2520that%2520the%2520hybrid%2520concept-based%2520models%2520outperform%250Astandard%2520computer%2520vision%2520models%2520and%2520previously%2520proposed%2520concept-based%2520models%250Awith%2520respect%2520to%2520accuracy%252C%2520especially%2520in%2520sparse%2520data%2520settings.%2520We%2520also%2520introduce%250Aan%2520algorithm%2520for%2520performing%2520adversarial%2520concept%2520attacks%252C%2520where%2520an%2520image%2520is%250Aperturbed%2520in%2520a%2520way%2520that%2520does%2520not%2520change%2520a%2520concept-based%2520model%2527s%2520concept%250Apredictions%252C%2520but%2520changes%2520the%2520class%2520prediction.%2520The%2520existence%2520of%2520such%250Aadversarial%2520examples%2520raises%2520questions%2520about%2520the%2520interpretable%2520qualities%250Apromised%2520by%2520concept-based%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07438v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Achieving%20Data%20Efficient%20Neural%20Networks%20with%20Hybrid%20Concept-based%0A%20%20Models&entry.906535625=Tobias%20A.%20Opsahl%20and%20Vegard%20Antun&entry.1292438233=%20%20Most%20datasets%20used%20for%20supervised%20machine%20learning%20consist%20of%20a%20single%20label%0Aper%20data%20point.%20However%2C%20in%20cases%20where%20more%20information%20than%20just%20the%20class%0Alabel%20is%20available%2C%20would%20it%20be%20possible%20to%20train%20models%20more%20efficiently%3F%20We%0Aintroduce%20two%20novel%20model%20architectures%2C%20which%20we%20call%20hybrid%20concept-based%0Amodels%2C%20that%20train%20using%20both%20class%20labels%20and%20additional%20information%20in%20the%0Adataset%20referred%20to%20as%20concepts.%20In%20order%20to%20thoroughly%20assess%20their%0Aperformance%2C%20we%20introduce%20ConceptShapes%2C%20an%20open%20and%20flexible%20class%20of%20datasets%0Awith%20concept%20labels.%20We%20show%20that%20the%20hybrid%20concept-based%20models%20outperform%0Astandard%20computer%20vision%20models%20and%20previously%20proposed%20concept-based%20models%0Awith%20respect%20to%20accuracy%2C%20especially%20in%20sparse%20data%20settings.%20We%20also%20introduce%0Aan%20algorithm%20for%20performing%20adversarial%20concept%20attacks%2C%20where%20an%20image%20is%0Aperturbed%20in%20a%20way%20that%20does%20not%20change%20a%20concept-based%20model%27s%20concept%0Apredictions%2C%20but%20changes%20the%20class%20prediction.%20The%20existence%20of%20such%0Aadversarial%20examples%20raises%20questions%20about%20the%20interpretable%20qualities%0Apromised%20by%20concept-based%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07438v1&entry.124074799=Read"},
{"title": "Object Augmentation Algorithm: Computing virtual object motion and\n  object induced interaction wrench from optical markers", "author": "Christopher Herneth and Junnan Li and Muhammad Hilman Fatoni and Amartya Ganguly and Sami Haddadin", "abstract": "  This study addresses the critical need for diverse and comprehensive data\nfocused on human arm joint torques while performing activities of daily living\n(ADL). Previous studies have often overlooked the influence of objects on joint\ntorques during ADL, resulting in limited datasets for analysis. To address this\ngap, we propose an Object Augmentation Algorithm (OAA) capable of augmenting\nexisting marker-based databases with virtual object motions and object-induced\njoint torque estimations. The OAA consists of five phases: (1) computing hand\ncoordinate systems from optical markers, (2) characterising object movements\nwith virtual markers, (3) calculating object motions through inverse kinematics\n(IK), (4) determining the wrench necessary for prescribed object motion using\ninverse dynamics (ID), and (5) computing joint torques resulting from object\nmanipulation. The algorithm's accuracy is validated through trajectory tracking\nand torque analysis on a 7+4 degree of freedom (DoF) robotic hand-arm system,\nmanipulating three unique objects. The results show that the OAA can accurately\nand precisely estimate 6 DoF object motion and object-induced joint torques.\nCorrelations between computed and measured quantities were > 0.99 for object\ntrajectories and > 0.93 for joint torques. The OAA was further shown to be\nrobust to variations in the number and placement of input markers, which are\nexpected between databases. Differences between repeated experiments were minor\nbut significant (p < 0.05). The algorithm expands the scope of available data\nand facilitates more comprehensive analyses of human-object interaction\ndynamics.\n", "link": "http://arxiv.org/abs/2408.07434v1", "date": "2024-08-14", "relevancy": 2.2029, "topK": [{"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.5604}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5483}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.542}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Object%20Augmentation%20Algorithm%3A%20Computing%20virtual%20object%20motion%20and%0A%20%20object%20induced%20interaction%20wrench%20from%20optical%20markers&body=Title%3A%20Object%20Augmentation%20Algorithm%3A%20Computing%20virtual%20object%20motion%20and%0A%20%20object%20induced%20interaction%20wrench%20from%20optical%20markers%0AAuthor%3A%20Christopher%20Herneth%20and%20Junnan%20Li%20and%20Muhammad%20Hilman%20Fatoni%20and%20Amartya%20Ganguly%20and%20Sami%20Haddadin%0AAbstract%3A%20%20%20This%20study%20addresses%20the%20critical%20need%20for%20diverse%20and%20comprehensive%20data%0Afocused%20on%20human%20arm%20joint%20torques%20while%20performing%20activities%20of%20daily%20living%0A%28ADL%29.%20Previous%20studies%20have%20often%20overlooked%20the%20influence%20of%20objects%20on%20joint%0Atorques%20during%20ADL%2C%20resulting%20in%20limited%20datasets%20for%20analysis.%20To%20address%20this%0Agap%2C%20we%20propose%20an%20Object%20Augmentation%20Algorithm%20%28OAA%29%20capable%20of%20augmenting%0Aexisting%20marker-based%20databases%20with%20virtual%20object%20motions%20and%20object-induced%0Ajoint%20torque%20estimations.%20The%20OAA%20consists%20of%20five%20phases%3A%20%281%29%20computing%20hand%0Acoordinate%20systems%20from%20optical%20markers%2C%20%282%29%20characterising%20object%20movements%0Awith%20virtual%20markers%2C%20%283%29%20calculating%20object%20motions%20through%20inverse%20kinematics%0A%28IK%29%2C%20%284%29%20determining%20the%20wrench%20necessary%20for%20prescribed%20object%20motion%20using%0Ainverse%20dynamics%20%28ID%29%2C%20and%20%285%29%20computing%20joint%20torques%20resulting%20from%20object%0Amanipulation.%20The%20algorithm%27s%20accuracy%20is%20validated%20through%20trajectory%20tracking%0Aand%20torque%20analysis%20on%20a%207%2B4%20degree%20of%20freedom%20%28DoF%29%20robotic%20hand-arm%20system%2C%0Amanipulating%20three%20unique%20objects.%20The%20results%20show%20that%20the%20OAA%20can%20accurately%0Aand%20precisely%20estimate%206%20DoF%20object%20motion%20and%20object-induced%20joint%20torques.%0ACorrelations%20between%20computed%20and%20measured%20quantities%20were%20%3E%200.99%20for%20object%0Atrajectories%20and%20%3E%200.93%20for%20joint%20torques.%20The%20OAA%20was%20further%20shown%20to%20be%0Arobust%20to%20variations%20in%20the%20number%20and%20placement%20of%20input%20markers%2C%20which%20are%0Aexpected%20between%20databases.%20Differences%20between%20repeated%20experiments%20were%20minor%0Abut%20significant%20%28p%20%3C%200.05%29.%20The%20algorithm%20expands%20the%20scope%20of%20available%20data%0Aand%20facilitates%20more%20comprehensive%20analyses%20of%20human-object%20interaction%0Adynamics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07434v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DObject%2520Augmentation%2520Algorithm%253A%2520Computing%2520virtual%2520object%2520motion%2520and%250A%2520%2520object%2520induced%2520interaction%2520wrench%2520from%2520optical%2520markers%26entry.906535625%3DChristopher%2520Herneth%2520and%2520Junnan%2520Li%2520and%2520Muhammad%2520Hilman%2520Fatoni%2520and%2520Amartya%2520Ganguly%2520and%2520Sami%2520Haddadin%26entry.1292438233%3D%2520%2520This%2520study%2520addresses%2520the%2520critical%2520need%2520for%2520diverse%2520and%2520comprehensive%2520data%250Afocused%2520on%2520human%2520arm%2520joint%2520torques%2520while%2520performing%2520activities%2520of%2520daily%2520living%250A%2528ADL%2529.%2520Previous%2520studies%2520have%2520often%2520overlooked%2520the%2520influence%2520of%2520objects%2520on%2520joint%250Atorques%2520during%2520ADL%252C%2520resulting%2520in%2520limited%2520datasets%2520for%2520analysis.%2520To%2520address%2520this%250Agap%252C%2520we%2520propose%2520an%2520Object%2520Augmentation%2520Algorithm%2520%2528OAA%2529%2520capable%2520of%2520augmenting%250Aexisting%2520marker-based%2520databases%2520with%2520virtual%2520object%2520motions%2520and%2520object-induced%250Ajoint%2520torque%2520estimations.%2520The%2520OAA%2520consists%2520of%2520five%2520phases%253A%2520%25281%2529%2520computing%2520hand%250Acoordinate%2520systems%2520from%2520optical%2520markers%252C%2520%25282%2529%2520characterising%2520object%2520movements%250Awith%2520virtual%2520markers%252C%2520%25283%2529%2520calculating%2520object%2520motions%2520through%2520inverse%2520kinematics%250A%2528IK%2529%252C%2520%25284%2529%2520determining%2520the%2520wrench%2520necessary%2520for%2520prescribed%2520object%2520motion%2520using%250Ainverse%2520dynamics%2520%2528ID%2529%252C%2520and%2520%25285%2529%2520computing%2520joint%2520torques%2520resulting%2520from%2520object%250Amanipulation.%2520The%2520algorithm%2527s%2520accuracy%2520is%2520validated%2520through%2520trajectory%2520tracking%250Aand%2520torque%2520analysis%2520on%2520a%25207%252B4%2520degree%2520of%2520freedom%2520%2528DoF%2529%2520robotic%2520hand-arm%2520system%252C%250Amanipulating%2520three%2520unique%2520objects.%2520The%2520results%2520show%2520that%2520the%2520OAA%2520can%2520accurately%250Aand%2520precisely%2520estimate%25206%2520DoF%2520object%2520motion%2520and%2520object-induced%2520joint%2520torques.%250ACorrelations%2520between%2520computed%2520and%2520measured%2520quantities%2520were%2520%253E%25200.99%2520for%2520object%250Atrajectories%2520and%2520%253E%25200.93%2520for%2520joint%2520torques.%2520The%2520OAA%2520was%2520further%2520shown%2520to%2520be%250Arobust%2520to%2520variations%2520in%2520the%2520number%2520and%2520placement%2520of%2520input%2520markers%252C%2520which%2520are%250Aexpected%2520between%2520databases.%2520Differences%2520between%2520repeated%2520experiments%2520were%2520minor%250Abut%2520significant%2520%2528p%2520%253C%25200.05%2529.%2520The%2520algorithm%2520expands%2520the%2520scope%2520of%2520available%2520data%250Aand%2520facilitates%2520more%2520comprehensive%2520analyses%2520of%2520human-object%2520interaction%250Adynamics.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07434v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object%20Augmentation%20Algorithm%3A%20Computing%20virtual%20object%20motion%20and%0A%20%20object%20induced%20interaction%20wrench%20from%20optical%20markers&entry.906535625=Christopher%20Herneth%20and%20Junnan%20Li%20and%20Muhammad%20Hilman%20Fatoni%20and%20Amartya%20Ganguly%20and%20Sami%20Haddadin&entry.1292438233=%20%20This%20study%20addresses%20the%20critical%20need%20for%20diverse%20and%20comprehensive%20data%0Afocused%20on%20human%20arm%20joint%20torques%20while%20performing%20activities%20of%20daily%20living%0A%28ADL%29.%20Previous%20studies%20have%20often%20overlooked%20the%20influence%20of%20objects%20on%20joint%0Atorques%20during%20ADL%2C%20resulting%20in%20limited%20datasets%20for%20analysis.%20To%20address%20this%0Agap%2C%20we%20propose%20an%20Object%20Augmentation%20Algorithm%20%28OAA%29%20capable%20of%20augmenting%0Aexisting%20marker-based%20databases%20with%20virtual%20object%20motions%20and%20object-induced%0Ajoint%20torque%20estimations.%20The%20OAA%20consists%20of%20five%20phases%3A%20%281%29%20computing%20hand%0Acoordinate%20systems%20from%20optical%20markers%2C%20%282%29%20characterising%20object%20movements%0Awith%20virtual%20markers%2C%20%283%29%20calculating%20object%20motions%20through%20inverse%20kinematics%0A%28IK%29%2C%20%284%29%20determining%20the%20wrench%20necessary%20for%20prescribed%20object%20motion%20using%0Ainverse%20dynamics%20%28ID%29%2C%20and%20%285%29%20computing%20joint%20torques%20resulting%20from%20object%0Amanipulation.%20The%20algorithm%27s%20accuracy%20is%20validated%20through%20trajectory%20tracking%0Aand%20torque%20analysis%20on%20a%207%2B4%20degree%20of%20freedom%20%28DoF%29%20robotic%20hand-arm%20system%2C%0Amanipulating%20three%20unique%20objects.%20The%20results%20show%20that%20the%20OAA%20can%20accurately%0Aand%20precisely%20estimate%206%20DoF%20object%20motion%20and%20object-induced%20joint%20torques.%0ACorrelations%20between%20computed%20and%20measured%20quantities%20were%20%3E%200.99%20for%20object%0Atrajectories%20and%20%3E%200.93%20for%20joint%20torques.%20The%20OAA%20was%20further%20shown%20to%20be%0Arobust%20to%20variations%20in%20the%20number%20and%20placement%20of%20input%20markers%2C%20which%20are%0Aexpected%20between%20databases.%20Differences%20between%20repeated%20experiments%20were%20minor%0Abut%20significant%20%28p%20%3C%200.05%29.%20The%20algorithm%20expands%20the%20scope%20of%20available%20data%0Aand%20facilitates%20more%20comprehensive%20analyses%20of%20human-object%20interaction%0Adynamics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07434v1&entry.124074799=Read"},
{"title": "GRFormer: Grouped Residual Self-Attention for Lightweight Single Image\n  Super-Resolution", "author": "Yuzhen Li and Zehang Deng and Yuxin Cao and Lihua Liu", "abstract": "  Previous works have shown that reducing parameter overhead and computations\nfor transformer-based single image super-resolution (SISR) models (e.g.,\nSwinIR) usually leads to a reduction of performance. In this paper, we present\nGRFormer, an efficient and lightweight method, which not only reduces the\nparameter overhead and computations, but also greatly improves performance. The\ncore of GRFormer is Grouped Residual Self-Attention (GRSA), which is\nspecifically oriented towards two fundamental components. Firstly, it\nintroduces a novel grouped residual layer (GRL) to replace the Query, Key,\nValue (QKV) linear layer in self-attention, aimed at efficiently reducing\nparameter overhead, computations, and performance loss at the same time.\nSecondly, it integrates a compact Exponential-Space Relative Position Bias\n(ES-RPB) as a substitute for the original relative position bias to improve the\nability to represent position information while further minimizing the\nparameter count. Extensive experimental results demonstrate that GRFormer\noutperforms state-of-the-art transformer-based methods for $\\times$2, $\\times$3\nand $\\times$4 SISR tasks, notably outperforming SOTA by a maximum PSNR of\n0.23dB when trained on the DIV2K dataset, while reducing the number of\nparameter and MACs by about \\textbf{60\\%} and \\textbf{49\\% } in only\nself-attention module respectively. We hope that our simple and effective\nmethod that can easily applied to SR models based on window-division\nself-attention can serve as a useful tool for further research in image\nsuper-resolution. The code is available at\n\\url{https://github.com/sisrformer/GRFormer}.\n", "link": "http://arxiv.org/abs/2408.07484v1", "date": "2024-08-14", "relevancy": 2.1996, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5655}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5608}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.53}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20GRFormer%3A%20Grouped%20Residual%20Self-Attention%20for%20Lightweight%20Single%20Image%0A%20%20Super-Resolution&body=Title%3A%20GRFormer%3A%20Grouped%20Residual%20Self-Attention%20for%20Lightweight%20Single%20Image%0A%20%20Super-Resolution%0AAuthor%3A%20Yuzhen%20Li%20and%20Zehang%20Deng%20and%20Yuxin%20Cao%20and%20Lihua%20Liu%0AAbstract%3A%20%20%20Previous%20works%20have%20shown%20that%20reducing%20parameter%20overhead%20and%20computations%0Afor%20transformer-based%20single%20image%20super-resolution%20%28SISR%29%20models%20%28e.g.%2C%0ASwinIR%29%20usually%20leads%20to%20a%20reduction%20of%20performance.%20In%20this%20paper%2C%20we%20present%0AGRFormer%2C%20an%20efficient%20and%20lightweight%20method%2C%20which%20not%20only%20reduces%20the%0Aparameter%20overhead%20and%20computations%2C%20but%20also%20greatly%20improves%20performance.%20The%0Acore%20of%20GRFormer%20is%20Grouped%20Residual%20Self-Attention%20%28GRSA%29%2C%20which%20is%0Aspecifically%20oriented%20towards%20two%20fundamental%20components.%20Firstly%2C%20it%0Aintroduces%20a%20novel%20grouped%20residual%20layer%20%28GRL%29%20to%20replace%20the%20Query%2C%20Key%2C%0AValue%20%28QKV%29%20linear%20layer%20in%20self-attention%2C%20aimed%20at%20efficiently%20reducing%0Aparameter%20overhead%2C%20computations%2C%20and%20performance%20loss%20at%20the%20same%20time.%0ASecondly%2C%20it%20integrates%20a%20compact%20Exponential-Space%20Relative%20Position%20Bias%0A%28ES-RPB%29%20as%20a%20substitute%20for%20the%20original%20relative%20position%20bias%20to%20improve%20the%0Aability%20to%20represent%20position%20information%20while%20further%20minimizing%20the%0Aparameter%20count.%20Extensive%20experimental%20results%20demonstrate%20that%20GRFormer%0Aoutperforms%20state-of-the-art%20transformer-based%20methods%20for%20%24%5Ctimes%242%2C%20%24%5Ctimes%243%0Aand%20%24%5Ctimes%244%20SISR%20tasks%2C%20notably%20outperforming%20SOTA%20by%20a%20maximum%20PSNR%20of%0A0.23dB%20when%20trained%20on%20the%20DIV2K%20dataset%2C%20while%20reducing%20the%20number%20of%0Aparameter%20and%20MACs%20by%20about%20%5Ctextbf%7B60%5C%25%7D%20and%20%5Ctextbf%7B49%5C%25%20%7D%20in%20only%0Aself-attention%20module%20respectively.%20We%20hope%20that%20our%20simple%20and%20effective%0Amethod%20that%20can%20easily%20applied%20to%20SR%20models%20based%20on%20window-division%0Aself-attention%20can%20serve%20as%20a%20useful%20tool%20for%20further%20research%20in%20image%0Asuper-resolution.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/sisrformer/GRFormer%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07484v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DGRFormer%253A%2520Grouped%2520Residual%2520Self-Attention%2520for%2520Lightweight%2520Single%2520Image%250A%2520%2520Super-Resolution%26entry.906535625%3DYuzhen%2520Li%2520and%2520Zehang%2520Deng%2520and%2520Yuxin%2520Cao%2520and%2520Lihua%2520Liu%26entry.1292438233%3D%2520%2520Previous%2520works%2520have%2520shown%2520that%2520reducing%2520parameter%2520overhead%2520and%2520computations%250Afor%2520transformer-based%2520single%2520image%2520super-resolution%2520%2528SISR%2529%2520models%2520%2528e.g.%252C%250ASwinIR%2529%2520usually%2520leads%2520to%2520a%2520reduction%2520of%2520performance.%2520In%2520this%2520paper%252C%2520we%2520present%250AGRFormer%252C%2520an%2520efficient%2520and%2520lightweight%2520method%252C%2520which%2520not%2520only%2520reduces%2520the%250Aparameter%2520overhead%2520and%2520computations%252C%2520but%2520also%2520greatly%2520improves%2520performance.%2520The%250Acore%2520of%2520GRFormer%2520is%2520Grouped%2520Residual%2520Self-Attention%2520%2528GRSA%2529%252C%2520which%2520is%250Aspecifically%2520oriented%2520towards%2520two%2520fundamental%2520components.%2520Firstly%252C%2520it%250Aintroduces%2520a%2520novel%2520grouped%2520residual%2520layer%2520%2528GRL%2529%2520to%2520replace%2520the%2520Query%252C%2520Key%252C%250AValue%2520%2528QKV%2529%2520linear%2520layer%2520in%2520self-attention%252C%2520aimed%2520at%2520efficiently%2520reducing%250Aparameter%2520overhead%252C%2520computations%252C%2520and%2520performance%2520loss%2520at%2520the%2520same%2520time.%250ASecondly%252C%2520it%2520integrates%2520a%2520compact%2520Exponential-Space%2520Relative%2520Position%2520Bias%250A%2528ES-RPB%2529%2520as%2520a%2520substitute%2520for%2520the%2520original%2520relative%2520position%2520bias%2520to%2520improve%2520the%250Aability%2520to%2520represent%2520position%2520information%2520while%2520further%2520minimizing%2520the%250Aparameter%2520count.%2520Extensive%2520experimental%2520results%2520demonstrate%2520that%2520GRFormer%250Aoutperforms%2520state-of-the-art%2520transformer-based%2520methods%2520for%2520%2524%255Ctimes%25242%252C%2520%2524%255Ctimes%25243%250Aand%2520%2524%255Ctimes%25244%2520SISR%2520tasks%252C%2520notably%2520outperforming%2520SOTA%2520by%2520a%2520maximum%2520PSNR%2520of%250A0.23dB%2520when%2520trained%2520on%2520the%2520DIV2K%2520dataset%252C%2520while%2520reducing%2520the%2520number%2520of%250Aparameter%2520and%2520MACs%2520by%2520about%2520%255Ctextbf%257B60%255C%2525%257D%2520and%2520%255Ctextbf%257B49%255C%2525%2520%257D%2520in%2520only%250Aself-attention%2520module%2520respectively.%2520We%2520hope%2520that%2520our%2520simple%2520and%2520effective%250Amethod%2520that%2520can%2520easily%2520applied%2520to%2520SR%2520models%2520based%2520on%2520window-division%250Aself-attention%2520can%2520serve%2520as%2520a%2520useful%2520tool%2520for%2520further%2520research%2520in%2520image%250Asuper-resolution.%2520The%2520code%2520is%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/sisrformer/GRFormer%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07484v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GRFormer%3A%20Grouped%20Residual%20Self-Attention%20for%20Lightweight%20Single%20Image%0A%20%20Super-Resolution&entry.906535625=Yuzhen%20Li%20and%20Zehang%20Deng%20and%20Yuxin%20Cao%20and%20Lihua%20Liu&entry.1292438233=%20%20Previous%20works%20have%20shown%20that%20reducing%20parameter%20overhead%20and%20computations%0Afor%20transformer-based%20single%20image%20super-resolution%20%28SISR%29%20models%20%28e.g.%2C%0ASwinIR%29%20usually%20leads%20to%20a%20reduction%20of%20performance.%20In%20this%20paper%2C%20we%20present%0AGRFormer%2C%20an%20efficient%20and%20lightweight%20method%2C%20which%20not%20only%20reduces%20the%0Aparameter%20overhead%20and%20computations%2C%20but%20also%20greatly%20improves%20performance.%20The%0Acore%20of%20GRFormer%20is%20Grouped%20Residual%20Self-Attention%20%28GRSA%29%2C%20which%20is%0Aspecifically%20oriented%20towards%20two%20fundamental%20components.%20Firstly%2C%20it%0Aintroduces%20a%20novel%20grouped%20residual%20layer%20%28GRL%29%20to%20replace%20the%20Query%2C%20Key%2C%0AValue%20%28QKV%29%20linear%20layer%20in%20self-attention%2C%20aimed%20at%20efficiently%20reducing%0Aparameter%20overhead%2C%20computations%2C%20and%20performance%20loss%20at%20the%20same%20time.%0ASecondly%2C%20it%20integrates%20a%20compact%20Exponential-Space%20Relative%20Position%20Bias%0A%28ES-RPB%29%20as%20a%20substitute%20for%20the%20original%20relative%20position%20bias%20to%20improve%20the%0Aability%20to%20represent%20position%20information%20while%20further%20minimizing%20the%0Aparameter%20count.%20Extensive%20experimental%20results%20demonstrate%20that%20GRFormer%0Aoutperforms%20state-of-the-art%20transformer-based%20methods%20for%20%24%5Ctimes%242%2C%20%24%5Ctimes%243%0Aand%20%24%5Ctimes%244%20SISR%20tasks%2C%20notably%20outperforming%20SOTA%20by%20a%20maximum%20PSNR%20of%0A0.23dB%20when%20trained%20on%20the%20DIV2K%20dataset%2C%20while%20reducing%20the%20number%20of%0Aparameter%20and%20MACs%20by%20about%20%5Ctextbf%7B60%5C%25%7D%20and%20%5Ctextbf%7B49%5C%25%20%7D%20in%20only%0Aself-attention%20module%20respectively.%20We%20hope%20that%20our%20simple%20and%20effective%0Amethod%20that%20can%20easily%20applied%20to%20SR%20models%20based%20on%20window-division%0Aself-attention%20can%20serve%20as%20a%20useful%20tool%20for%20further%20research%20in%20image%0Asuper-resolution.%20The%20code%20is%20available%20at%0A%5Curl%7Bhttps%3A//github.com/sisrformer/GRFormer%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07484v1&entry.124074799=Read"},
{"title": "User-customizable Shared Control for Robot Teleoperation via Virtual\n  Reality", "author": "Rui Luo and Mark Zolotas and Drake Moore and Taskin Padir", "abstract": "  Shared control can ease and enhance a human operator's ability to teleoperate\nrobots, particularly for intricate tasks demanding fine control over multiple\ndegrees of freedom. However, the arbitration process dictating how much\nautonomous assistance to administer in shared control can confuse novice\noperators and impede their understanding of the robot's behavior. To overcome\nthese adverse side-effects, we propose a novel formulation of shared control\nthat enables operators to tailor the arbitration to their unique capabilities\nand preferences. Unlike prior approaches to customizable shared control where\nusers could indirectly modify the latent parameters of the arbitration function\nby issuing a feedback command, we instead make these parameters observable and\ndirectly editable via a virtual reality (VR) interface. We present our\nuser-customizable shared control method for a teleoperation task in SE(3),\nknown as the buzz wire game. A user study is conducted with participants\nteleoperating a robotic arm in VR to complete the game. The experiment spanned\ntwo weeks per subject to investigate longitudinal trends. Our findings reveal\nthat users allowed to interactively tune the arbitration parameters across\ntrials generalize well to adaptations in the task, exhibiting improvements in\nprecision and fluency over direct teleoperation and conventional shared\ncontrol.\n", "link": "http://arxiv.org/abs/2403.13177v2", "date": "2024-08-14", "relevancy": 2.1646, "topK": [{"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5599}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5358}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5246}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20User-customizable%20Shared%20Control%20for%20Robot%20Teleoperation%20via%20Virtual%0A%20%20Reality&body=Title%3A%20User-customizable%20Shared%20Control%20for%20Robot%20Teleoperation%20via%20Virtual%0A%20%20Reality%0AAuthor%3A%20Rui%20Luo%20and%20Mark%20Zolotas%20and%20Drake%20Moore%20and%20Taskin%20Padir%0AAbstract%3A%20%20%20Shared%20control%20can%20ease%20and%20enhance%20a%20human%20operator%27s%20ability%20to%20teleoperate%0Arobots%2C%20particularly%20for%20intricate%20tasks%20demanding%20fine%20control%20over%20multiple%0Adegrees%20of%20freedom.%20However%2C%20the%20arbitration%20process%20dictating%20how%20much%0Aautonomous%20assistance%20to%20administer%20in%20shared%20control%20can%20confuse%20novice%0Aoperators%20and%20impede%20their%20understanding%20of%20the%20robot%27s%20behavior.%20To%20overcome%0Athese%20adverse%20side-effects%2C%20we%20propose%20a%20novel%20formulation%20of%20shared%20control%0Athat%20enables%20operators%20to%20tailor%20the%20arbitration%20to%20their%20unique%20capabilities%0Aand%20preferences.%20Unlike%20prior%20approaches%20to%20customizable%20shared%20control%20where%0Ausers%20could%20indirectly%20modify%20the%20latent%20parameters%20of%20the%20arbitration%20function%0Aby%20issuing%20a%20feedback%20command%2C%20we%20instead%20make%20these%20parameters%20observable%20and%0Adirectly%20editable%20via%20a%20virtual%20reality%20%28VR%29%20interface.%20We%20present%20our%0Auser-customizable%20shared%20control%20method%20for%20a%20teleoperation%20task%20in%20SE%283%29%2C%0Aknown%20as%20the%20buzz%20wire%20game.%20A%20user%20study%20is%20conducted%20with%20participants%0Ateleoperating%20a%20robotic%20arm%20in%20VR%20to%20complete%20the%20game.%20The%20experiment%20spanned%0Atwo%20weeks%20per%20subject%20to%20investigate%20longitudinal%20trends.%20Our%20findings%20reveal%0Athat%20users%20allowed%20to%20interactively%20tune%20the%20arbitration%20parameters%20across%0Atrials%20generalize%20well%20to%20adaptations%20in%20the%20task%2C%20exhibiting%20improvements%20in%0Aprecision%20and%20fluency%20over%20direct%20teleoperation%20and%20conventional%20shared%0Acontrol.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.13177v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUser-customizable%2520Shared%2520Control%2520for%2520Robot%2520Teleoperation%2520via%2520Virtual%250A%2520%2520Reality%26entry.906535625%3DRui%2520Luo%2520and%2520Mark%2520Zolotas%2520and%2520Drake%2520Moore%2520and%2520Taskin%2520Padir%26entry.1292438233%3D%2520%2520Shared%2520control%2520can%2520ease%2520and%2520enhance%2520a%2520human%2520operator%2527s%2520ability%2520to%2520teleoperate%250Arobots%252C%2520particularly%2520for%2520intricate%2520tasks%2520demanding%2520fine%2520control%2520over%2520multiple%250Adegrees%2520of%2520freedom.%2520However%252C%2520the%2520arbitration%2520process%2520dictating%2520how%2520much%250Aautonomous%2520assistance%2520to%2520administer%2520in%2520shared%2520control%2520can%2520confuse%2520novice%250Aoperators%2520and%2520impede%2520their%2520understanding%2520of%2520the%2520robot%2527s%2520behavior.%2520To%2520overcome%250Athese%2520adverse%2520side-effects%252C%2520we%2520propose%2520a%2520novel%2520formulation%2520of%2520shared%2520control%250Athat%2520enables%2520operators%2520to%2520tailor%2520the%2520arbitration%2520to%2520their%2520unique%2520capabilities%250Aand%2520preferences.%2520Unlike%2520prior%2520approaches%2520to%2520customizable%2520shared%2520control%2520where%250Ausers%2520could%2520indirectly%2520modify%2520the%2520latent%2520parameters%2520of%2520the%2520arbitration%2520function%250Aby%2520issuing%2520a%2520feedback%2520command%252C%2520we%2520instead%2520make%2520these%2520parameters%2520observable%2520and%250Adirectly%2520editable%2520via%2520a%2520virtual%2520reality%2520%2528VR%2529%2520interface.%2520We%2520present%2520our%250Auser-customizable%2520shared%2520control%2520method%2520for%2520a%2520teleoperation%2520task%2520in%2520SE%25283%2529%252C%250Aknown%2520as%2520the%2520buzz%2520wire%2520game.%2520A%2520user%2520study%2520is%2520conducted%2520with%2520participants%250Ateleoperating%2520a%2520robotic%2520arm%2520in%2520VR%2520to%2520complete%2520the%2520game.%2520The%2520experiment%2520spanned%250Atwo%2520weeks%2520per%2520subject%2520to%2520investigate%2520longitudinal%2520trends.%2520Our%2520findings%2520reveal%250Athat%2520users%2520allowed%2520to%2520interactively%2520tune%2520the%2520arbitration%2520parameters%2520across%250Atrials%2520generalize%2520well%2520to%2520adaptations%2520in%2520the%2520task%252C%2520exhibiting%2520improvements%2520in%250Aprecision%2520and%2520fluency%2520over%2520direct%2520teleoperation%2520and%2520conventional%2520shared%250Acontrol.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.13177v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=User-customizable%20Shared%20Control%20for%20Robot%20Teleoperation%20via%20Virtual%0A%20%20Reality&entry.906535625=Rui%20Luo%20and%20Mark%20Zolotas%20and%20Drake%20Moore%20and%20Taskin%20Padir&entry.1292438233=%20%20Shared%20control%20can%20ease%20and%20enhance%20a%20human%20operator%27s%20ability%20to%20teleoperate%0Arobots%2C%20particularly%20for%20intricate%20tasks%20demanding%20fine%20control%20over%20multiple%0Adegrees%20of%20freedom.%20However%2C%20the%20arbitration%20process%20dictating%20how%20much%0Aautonomous%20assistance%20to%20administer%20in%20shared%20control%20can%20confuse%20novice%0Aoperators%20and%20impede%20their%20understanding%20of%20the%20robot%27s%20behavior.%20To%20overcome%0Athese%20adverse%20side-effects%2C%20we%20propose%20a%20novel%20formulation%20of%20shared%20control%0Athat%20enables%20operators%20to%20tailor%20the%20arbitration%20to%20their%20unique%20capabilities%0Aand%20preferences.%20Unlike%20prior%20approaches%20to%20customizable%20shared%20control%20where%0Ausers%20could%20indirectly%20modify%20the%20latent%20parameters%20of%20the%20arbitration%20function%0Aby%20issuing%20a%20feedback%20command%2C%20we%20instead%20make%20these%20parameters%20observable%20and%0Adirectly%20editable%20via%20a%20virtual%20reality%20%28VR%29%20interface.%20We%20present%20our%0Auser-customizable%20shared%20control%20method%20for%20a%20teleoperation%20task%20in%20SE%283%29%2C%0Aknown%20as%20the%20buzz%20wire%20game.%20A%20user%20study%20is%20conducted%20with%20participants%0Ateleoperating%20a%20robotic%20arm%20in%20VR%20to%20complete%20the%20game.%20The%20experiment%20spanned%0Atwo%20weeks%20per%20subject%20to%20investigate%20longitudinal%20trends.%20Our%20findings%20reveal%0Athat%20users%20allowed%20to%20interactively%20tune%20the%20arbitration%20parameters%20across%0Atrials%20generalize%20well%20to%20adaptations%20in%20the%20task%2C%20exhibiting%20improvements%20in%0Aprecision%20and%20fluency%20over%20direct%20teleoperation%20and%20conventional%20shared%0Acontrol.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.13177v2&entry.124074799=Read"},
{"title": "Detecting Audio-Visual Deepfakes with Fine-Grained Inconsistencies", "author": "Marcella Astrid and Enjie Ghorbel and Djamila Aouada", "abstract": "  Existing methods on audio-visual deepfake detection mainly focus on\nhigh-level features for modeling inconsistencies between audio and visual data.\nAs a result, these approaches usually overlook finer audio-visual artifacts,\nwhich are inherent to deepfakes. Herein, we propose the introduction of\nfine-grained mechanisms for detecting subtle artifacts in both spatial and\ntemporal domains. First, we introduce a local audio-visual model capable of\ncapturing small spatial regions that are prone to inconsistencies with audio.\nFor that purpose, a fine-grained mechanism based on a spatially-local distance\ncoupled with an attention module is adopted. Second, we introduce a\ntemporally-local pseudo-fake augmentation to include samples incorporating\nsubtle temporal inconsistencies in our training set. Experiments on the DFDC\nand the FakeAVCeleb datasets demonstrate the superiority of the proposed method\nin terms of generalization as compared to the state-of-the-art under both\nin-dataset and cross-dataset settings.\n", "link": "http://arxiv.org/abs/2408.06753v2", "date": "2024-08-14", "relevancy": 2.1363, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.555}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5205}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5158}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Audio-Visual%20Deepfakes%20with%20Fine-Grained%20Inconsistencies&body=Title%3A%20Detecting%20Audio-Visual%20Deepfakes%20with%20Fine-Grained%20Inconsistencies%0AAuthor%3A%20Marcella%20Astrid%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada%0AAbstract%3A%20%20%20Existing%20methods%20on%20audio-visual%20deepfake%20detection%20mainly%20focus%20on%0Ahigh-level%20features%20for%20modeling%20inconsistencies%20between%20audio%20and%20visual%20data.%0AAs%20a%20result%2C%20these%20approaches%20usually%20overlook%20finer%20audio-visual%20artifacts%2C%0Awhich%20are%20inherent%20to%20deepfakes.%20Herein%2C%20we%20propose%20the%20introduction%20of%0Afine-grained%20mechanisms%20for%20detecting%20subtle%20artifacts%20in%20both%20spatial%20and%0Atemporal%20domains.%20First%2C%20we%20introduce%20a%20local%20audio-visual%20model%20capable%20of%0Acapturing%20small%20spatial%20regions%20that%20are%20prone%20to%20inconsistencies%20with%20audio.%0AFor%20that%20purpose%2C%20a%20fine-grained%20mechanism%20based%20on%20a%20spatially-local%20distance%0Acoupled%20with%20an%20attention%20module%20is%20adopted.%20Second%2C%20we%20introduce%20a%0Atemporally-local%20pseudo-fake%20augmentation%20to%20include%20samples%20incorporating%0Asubtle%20temporal%20inconsistencies%20in%20our%20training%20set.%20Experiments%20on%20the%20DFDC%0Aand%20the%20FakeAVCeleb%20datasets%20demonstrate%20the%20superiority%20of%20the%20proposed%20method%0Ain%20terms%20of%20generalization%20as%20compared%20to%20the%20state-of-the-art%20under%20both%0Ain-dataset%20and%20cross-dataset%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06753v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Audio-Visual%2520Deepfakes%2520with%2520Fine-Grained%2520Inconsistencies%26entry.906535625%3DMarcella%2520Astrid%2520and%2520Enjie%2520Ghorbel%2520and%2520Djamila%2520Aouada%26entry.1292438233%3D%2520%2520Existing%2520methods%2520on%2520audio-visual%2520deepfake%2520detection%2520mainly%2520focus%2520on%250Ahigh-level%2520features%2520for%2520modeling%2520inconsistencies%2520between%2520audio%2520and%2520visual%2520data.%250AAs%2520a%2520result%252C%2520these%2520approaches%2520usually%2520overlook%2520finer%2520audio-visual%2520artifacts%252C%250Awhich%2520are%2520inherent%2520to%2520deepfakes.%2520Herein%252C%2520we%2520propose%2520the%2520introduction%2520of%250Afine-grained%2520mechanisms%2520for%2520detecting%2520subtle%2520artifacts%2520in%2520both%2520spatial%2520and%250Atemporal%2520domains.%2520First%252C%2520we%2520introduce%2520a%2520local%2520audio-visual%2520model%2520capable%2520of%250Acapturing%2520small%2520spatial%2520regions%2520that%2520are%2520prone%2520to%2520inconsistencies%2520with%2520audio.%250AFor%2520that%2520purpose%252C%2520a%2520fine-grained%2520mechanism%2520based%2520on%2520a%2520spatially-local%2520distance%250Acoupled%2520with%2520an%2520attention%2520module%2520is%2520adopted.%2520Second%252C%2520we%2520introduce%2520a%250Atemporally-local%2520pseudo-fake%2520augmentation%2520to%2520include%2520samples%2520incorporating%250Asubtle%2520temporal%2520inconsistencies%2520in%2520our%2520training%2520set.%2520Experiments%2520on%2520the%2520DFDC%250Aand%2520the%2520FakeAVCeleb%2520datasets%2520demonstrate%2520the%2520superiority%2520of%2520the%2520proposed%2520method%250Ain%2520terms%2520of%2520generalization%2520as%2520compared%2520to%2520the%2520state-of-the-art%2520under%2520both%250Ain-dataset%2520and%2520cross-dataset%2520settings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06753v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Audio-Visual%20Deepfakes%20with%20Fine-Grained%20Inconsistencies&entry.906535625=Marcella%20Astrid%20and%20Enjie%20Ghorbel%20and%20Djamila%20Aouada&entry.1292438233=%20%20Existing%20methods%20on%20audio-visual%20deepfake%20detection%20mainly%20focus%20on%0Ahigh-level%20features%20for%20modeling%20inconsistencies%20between%20audio%20and%20visual%20data.%0AAs%20a%20result%2C%20these%20approaches%20usually%20overlook%20finer%20audio-visual%20artifacts%2C%0Awhich%20are%20inherent%20to%20deepfakes.%20Herein%2C%20we%20propose%20the%20introduction%20of%0Afine-grained%20mechanisms%20for%20detecting%20subtle%20artifacts%20in%20both%20spatial%20and%0Atemporal%20domains.%20First%2C%20we%20introduce%20a%20local%20audio-visual%20model%20capable%20of%0Acapturing%20small%20spatial%20regions%20that%20are%20prone%20to%20inconsistencies%20with%20audio.%0AFor%20that%20purpose%2C%20a%20fine-grained%20mechanism%20based%20on%20a%20spatially-local%20distance%0Acoupled%20with%20an%20attention%20module%20is%20adopted.%20Second%2C%20we%20introduce%20a%0Atemporally-local%20pseudo-fake%20augmentation%20to%20include%20samples%20incorporating%0Asubtle%20temporal%20inconsistencies%20in%20our%20training%20set.%20Experiments%20on%20the%20DFDC%0Aand%20the%20FakeAVCeleb%20datasets%20demonstrate%20the%20superiority%20of%20the%20proposed%20method%0Ain%20terms%20of%20generalization%20as%20compared%20to%20the%20state-of-the-art%20under%20both%0Ain-dataset%20and%20cross-dataset%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06753v2&entry.124074799=Read"},
{"title": "Towards Real-time Video Compressive Sensing on Mobile Devices", "author": "Miao Cao and Lishun Wang and Huan Wang and Guoqing Wang and Xin Yuan", "abstract": "  Video Snapshot Compressive Imaging (SCI) uses a low-speed 2D camera to\ncapture high-speed scenes as snapshot compressed measurements, followed by a\nreconstruction algorithm to retrieve the high-speed video frames. The fast\nevolving mobile devices and existing high-performance video SCI reconstruction\nalgorithms motivate us to develop mobile reconstruction methods for real-world\napplications. Yet, it is still challenging to deploy previous reconstruction\nalgorithms on mobile devices due to the complex inference process, let alone\nreal-time mobile reconstruction. To the best of our knowledge, there is no\nvideo SCI reconstruction model designed to run on the mobile devices. Towards\nthis end, in this paper, we present an effective approach for video SCI\nreconstruction, dubbed MobileSCI, which can run at real-time speed on the\nmobile devices for the first time. Specifically, we first build a U-shaped 2D\nconvolution-based architecture, which is much more efficient and\nmobile-friendly than previous state-of-the-art reconstruction methods. Besides,\nan efficient feature mixing block, based on the channel splitting and shuffling\nmechanisms, is introduced as a novel bottleneck block of our proposed MobileSCI\nto alleviate the computational burden. Finally, a customized knowledge\ndistillation strategy is utilized to further improve the reconstruction\nquality. Extensive results on both simulated and real data show that our\nproposed MobileSCI can achieve superior reconstruction quality with high\nefficiency on the mobile devices. Particularly, we can reconstruct a 256 X 256\nX 8 snapshot compressed measurement with real-time performance (about 35 FPS)\non an iPhone 15. Code is available at https://github.com/mcao92/MobileSCI.\n", "link": "http://arxiv.org/abs/2408.07530v1", "date": "2024-08-14", "relevancy": 2.0984, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5438}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5259}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.5049}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Real-time%20Video%20Compressive%20Sensing%20on%20Mobile%20Devices&body=Title%3A%20Towards%20Real-time%20Video%20Compressive%20Sensing%20on%20Mobile%20Devices%0AAuthor%3A%20Miao%20Cao%20and%20Lishun%20Wang%20and%20Huan%20Wang%20and%20Guoqing%20Wang%20and%20Xin%20Yuan%0AAbstract%3A%20%20%20Video%20Snapshot%20Compressive%20Imaging%20%28SCI%29%20uses%20a%20low-speed%202D%20camera%20to%0Acapture%20high-speed%20scenes%20as%20snapshot%20compressed%20measurements%2C%20followed%20by%20a%0Areconstruction%20algorithm%20to%20retrieve%20the%20high-speed%20video%20frames.%20The%20fast%0Aevolving%20mobile%20devices%20and%20existing%20high-performance%20video%20SCI%20reconstruction%0Aalgorithms%20motivate%20us%20to%20develop%20mobile%20reconstruction%20methods%20for%20real-world%0Aapplications.%20Yet%2C%20it%20is%20still%20challenging%20to%20deploy%20previous%20reconstruction%0Aalgorithms%20on%20mobile%20devices%20due%20to%20the%20complex%20inference%20process%2C%20let%20alone%0Areal-time%20mobile%20reconstruction.%20To%20the%20best%20of%20our%20knowledge%2C%20there%20is%20no%0Avideo%20SCI%20reconstruction%20model%20designed%20to%20run%20on%20the%20mobile%20devices.%20Towards%0Athis%20end%2C%20in%20this%20paper%2C%20we%20present%20an%20effective%20approach%20for%20video%20SCI%0Areconstruction%2C%20dubbed%20MobileSCI%2C%20which%20can%20run%20at%20real-time%20speed%20on%20the%0Amobile%20devices%20for%20the%20first%20time.%20Specifically%2C%20we%20first%20build%20a%20U-shaped%202D%0Aconvolution-based%20architecture%2C%20which%20is%20much%20more%20efficient%20and%0Amobile-friendly%20than%20previous%20state-of-the-art%20reconstruction%20methods.%20Besides%2C%0Aan%20efficient%20feature%20mixing%20block%2C%20based%20on%20the%20channel%20splitting%20and%20shuffling%0Amechanisms%2C%20is%20introduced%20as%20a%20novel%20bottleneck%20block%20of%20our%20proposed%20MobileSCI%0Ato%20alleviate%20the%20computational%20burden.%20Finally%2C%20a%20customized%20knowledge%0Adistillation%20strategy%20is%20utilized%20to%20further%20improve%20the%20reconstruction%0Aquality.%20Extensive%20results%20on%20both%20simulated%20and%20real%20data%20show%20that%20our%0Aproposed%20MobileSCI%20can%20achieve%20superior%20reconstruction%20quality%20with%20high%0Aefficiency%20on%20the%20mobile%20devices.%20Particularly%2C%20we%20can%20reconstruct%20a%20256%20X%20256%0AX%208%20snapshot%20compressed%20measurement%20with%20real-time%20performance%20%28about%2035%20FPS%29%0Aon%20an%20iPhone%2015.%20Code%20is%20available%20at%20https%3A//github.com/mcao92/MobileSCI.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07530v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Real-time%2520Video%2520Compressive%2520Sensing%2520on%2520Mobile%2520Devices%26entry.906535625%3DMiao%2520Cao%2520and%2520Lishun%2520Wang%2520and%2520Huan%2520Wang%2520and%2520Guoqing%2520Wang%2520and%2520Xin%2520Yuan%26entry.1292438233%3D%2520%2520Video%2520Snapshot%2520Compressive%2520Imaging%2520%2528SCI%2529%2520uses%2520a%2520low-speed%25202D%2520camera%2520to%250Acapture%2520high-speed%2520scenes%2520as%2520snapshot%2520compressed%2520measurements%252C%2520followed%2520by%2520a%250Areconstruction%2520algorithm%2520to%2520retrieve%2520the%2520high-speed%2520video%2520frames.%2520The%2520fast%250Aevolving%2520mobile%2520devices%2520and%2520existing%2520high-performance%2520video%2520SCI%2520reconstruction%250Aalgorithms%2520motivate%2520us%2520to%2520develop%2520mobile%2520reconstruction%2520methods%2520for%2520real-world%250Aapplications.%2520Yet%252C%2520it%2520is%2520still%2520challenging%2520to%2520deploy%2520previous%2520reconstruction%250Aalgorithms%2520on%2520mobile%2520devices%2520due%2520to%2520the%2520complex%2520inference%2520process%252C%2520let%2520alone%250Areal-time%2520mobile%2520reconstruction.%2520To%2520the%2520best%2520of%2520our%2520knowledge%252C%2520there%2520is%2520no%250Avideo%2520SCI%2520reconstruction%2520model%2520designed%2520to%2520run%2520on%2520the%2520mobile%2520devices.%2520Towards%250Athis%2520end%252C%2520in%2520this%2520paper%252C%2520we%2520present%2520an%2520effective%2520approach%2520for%2520video%2520SCI%250Areconstruction%252C%2520dubbed%2520MobileSCI%252C%2520which%2520can%2520run%2520at%2520real-time%2520speed%2520on%2520the%250Amobile%2520devices%2520for%2520the%2520first%2520time.%2520Specifically%252C%2520we%2520first%2520build%2520a%2520U-shaped%25202D%250Aconvolution-based%2520architecture%252C%2520which%2520is%2520much%2520more%2520efficient%2520and%250Amobile-friendly%2520than%2520previous%2520state-of-the-art%2520reconstruction%2520methods.%2520Besides%252C%250Aan%2520efficient%2520feature%2520mixing%2520block%252C%2520based%2520on%2520the%2520channel%2520splitting%2520and%2520shuffling%250Amechanisms%252C%2520is%2520introduced%2520as%2520a%2520novel%2520bottleneck%2520block%2520of%2520our%2520proposed%2520MobileSCI%250Ato%2520alleviate%2520the%2520computational%2520burden.%2520Finally%252C%2520a%2520customized%2520knowledge%250Adistillation%2520strategy%2520is%2520utilized%2520to%2520further%2520improve%2520the%2520reconstruction%250Aquality.%2520Extensive%2520results%2520on%2520both%2520simulated%2520and%2520real%2520data%2520show%2520that%2520our%250Aproposed%2520MobileSCI%2520can%2520achieve%2520superior%2520reconstruction%2520quality%2520with%2520high%250Aefficiency%2520on%2520the%2520mobile%2520devices.%2520Particularly%252C%2520we%2520can%2520reconstruct%2520a%2520256%2520X%2520256%250AX%25208%2520snapshot%2520compressed%2520measurement%2520with%2520real-time%2520performance%2520%2528about%252035%2520FPS%2529%250Aon%2520an%2520iPhone%252015.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/mcao92/MobileSCI.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07530v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Real-time%20Video%20Compressive%20Sensing%20on%20Mobile%20Devices&entry.906535625=Miao%20Cao%20and%20Lishun%20Wang%20and%20Huan%20Wang%20and%20Guoqing%20Wang%20and%20Xin%20Yuan&entry.1292438233=%20%20Video%20Snapshot%20Compressive%20Imaging%20%28SCI%29%20uses%20a%20low-speed%202D%20camera%20to%0Acapture%20high-speed%20scenes%20as%20snapshot%20compressed%20measurements%2C%20followed%20by%20a%0Areconstruction%20algorithm%20to%20retrieve%20the%20high-speed%20video%20frames.%20The%20fast%0Aevolving%20mobile%20devices%20and%20existing%20high-performance%20video%20SCI%20reconstruction%0Aalgorithms%20motivate%20us%20to%20develop%20mobile%20reconstruction%20methods%20for%20real-world%0Aapplications.%20Yet%2C%20it%20is%20still%20challenging%20to%20deploy%20previous%20reconstruction%0Aalgorithms%20on%20mobile%20devices%20due%20to%20the%20complex%20inference%20process%2C%20let%20alone%0Areal-time%20mobile%20reconstruction.%20To%20the%20best%20of%20our%20knowledge%2C%20there%20is%20no%0Avideo%20SCI%20reconstruction%20model%20designed%20to%20run%20on%20the%20mobile%20devices.%20Towards%0Athis%20end%2C%20in%20this%20paper%2C%20we%20present%20an%20effective%20approach%20for%20video%20SCI%0Areconstruction%2C%20dubbed%20MobileSCI%2C%20which%20can%20run%20at%20real-time%20speed%20on%20the%0Amobile%20devices%20for%20the%20first%20time.%20Specifically%2C%20we%20first%20build%20a%20U-shaped%202D%0Aconvolution-based%20architecture%2C%20which%20is%20much%20more%20efficient%20and%0Amobile-friendly%20than%20previous%20state-of-the-art%20reconstruction%20methods.%20Besides%2C%0Aan%20efficient%20feature%20mixing%20block%2C%20based%20on%20the%20channel%20splitting%20and%20shuffling%0Amechanisms%2C%20is%20introduced%20as%20a%20novel%20bottleneck%20block%20of%20our%20proposed%20MobileSCI%0Ato%20alleviate%20the%20computational%20burden.%20Finally%2C%20a%20customized%20knowledge%0Adistillation%20strategy%20is%20utilized%20to%20further%20improve%20the%20reconstruction%0Aquality.%20Extensive%20results%20on%20both%20simulated%20and%20real%20data%20show%20that%20our%0Aproposed%20MobileSCI%20can%20achieve%20superior%20reconstruction%20quality%20with%20high%0Aefficiency%20on%20the%20mobile%20devices.%20Particularly%2C%20we%20can%20reconstruct%20a%20256%20X%20256%0AX%208%20snapshot%20compressed%20measurement%20with%20real-time%20performance%20%28about%2035%20FPS%29%0Aon%20an%20iPhone%2015.%20Code%20is%20available%20at%20https%3A//github.com/mcao92/MobileSCI.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07530v1&entry.124074799=Read"},
{"title": "Switched auxiliary loss for robust training of transformer models for\n  histopathological image segmentation", "author": "Mustaffa Hussain and Saharsh Barve", "abstract": "  Functional tissue Units (FTUs) are cell population neighborhoods local to a\nparticular organ performing its main function.The FTUs provide crucial\ninformation to the pathologist in understanding the disease affecting a\nparticular organ by providing information at the cellular level.In our\nresearch, we have developed a model to segment multi-organ FTUs across 5 organs\nnamely: the kidney, large intestine, lung, prostate and spleen by utilizing the\n'HuBMAP + HPA - Hacking the Human Body' competition dataset.We propose adding\nswitched auxiliary loss for training models like the transformers to overcome\nthe diminishing gradient problem which poses a challenge towards optimal\ntraining of deep models.Overall, our model achieved a dice score of 0.793 on\nthe public dataset and 0.778 on the private dataset.The results supports the\nrobustness of the proposed training methodology.The findings also bolster the\nuse of transformers models for dense prediction tasks in the field of medical\nimage analysis.The study assists in understanding the relationships between\ncell and tissue organization thereby providing a useful medium to look at the\nimpact of cellular functions on human health.\n", "link": "http://arxiv.org/abs/2308.10994v2", "date": "2024-08-14", "relevancy": 2.0846, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.578}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5169}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5027}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Switched%20auxiliary%20loss%20for%20robust%20training%20of%20transformer%20models%20for%0A%20%20histopathological%20image%20segmentation&body=Title%3A%20Switched%20auxiliary%20loss%20for%20robust%20training%20of%20transformer%20models%20for%0A%20%20histopathological%20image%20segmentation%0AAuthor%3A%20Mustaffa%20Hussain%20and%20Saharsh%20Barve%0AAbstract%3A%20%20%20Functional%20tissue%20Units%20%28FTUs%29%20are%20cell%20population%20neighborhoods%20local%20to%20a%0Aparticular%20organ%20performing%20its%20main%20function.The%20FTUs%20provide%20crucial%0Ainformation%20to%20the%20pathologist%20in%20understanding%20the%20disease%20affecting%20a%0Aparticular%20organ%20by%20providing%20information%20at%20the%20cellular%20level.In%20our%0Aresearch%2C%20we%20have%20developed%20a%20model%20to%20segment%20multi-organ%20FTUs%20across%205%20organs%0Anamely%3A%20the%20kidney%2C%20large%20intestine%2C%20lung%2C%20prostate%20and%20spleen%20by%20utilizing%20the%0A%27HuBMAP%20%2B%20HPA%20-%20Hacking%20the%20Human%20Body%27%20competition%20dataset.We%20propose%20adding%0Aswitched%20auxiliary%20loss%20for%20training%20models%20like%20the%20transformers%20to%20overcome%0Athe%20diminishing%20gradient%20problem%20which%20poses%20a%20challenge%20towards%20optimal%0Atraining%20of%20deep%20models.Overall%2C%20our%20model%20achieved%20a%20dice%20score%20of%200.793%20on%0Athe%20public%20dataset%20and%200.778%20on%20the%20private%20dataset.The%20results%20supports%20the%0Arobustness%20of%20the%20proposed%20training%20methodology.The%20findings%20also%20bolster%20the%0Ause%20of%20transformers%20models%20for%20dense%20prediction%20tasks%20in%20the%20field%20of%20medical%0Aimage%20analysis.The%20study%20assists%20in%20understanding%20the%20relationships%20between%0Acell%20and%20tissue%20organization%20thereby%20providing%20a%20useful%20medium%20to%20look%20at%20the%0Aimpact%20of%20cellular%20functions%20on%20human%20health.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2308.10994v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSwitched%2520auxiliary%2520loss%2520for%2520robust%2520training%2520of%2520transformer%2520models%2520for%250A%2520%2520histopathological%2520image%2520segmentation%26entry.906535625%3DMustaffa%2520Hussain%2520and%2520Saharsh%2520Barve%26entry.1292438233%3D%2520%2520Functional%2520tissue%2520Units%2520%2528FTUs%2529%2520are%2520cell%2520population%2520neighborhoods%2520local%2520to%2520a%250Aparticular%2520organ%2520performing%2520its%2520main%2520function.The%2520FTUs%2520provide%2520crucial%250Ainformation%2520to%2520the%2520pathologist%2520in%2520understanding%2520the%2520disease%2520affecting%2520a%250Aparticular%2520organ%2520by%2520providing%2520information%2520at%2520the%2520cellular%2520level.In%2520our%250Aresearch%252C%2520we%2520have%2520developed%2520a%2520model%2520to%2520segment%2520multi-organ%2520FTUs%2520across%25205%2520organs%250Anamely%253A%2520the%2520kidney%252C%2520large%2520intestine%252C%2520lung%252C%2520prostate%2520and%2520spleen%2520by%2520utilizing%2520the%250A%2527HuBMAP%2520%252B%2520HPA%2520-%2520Hacking%2520the%2520Human%2520Body%2527%2520competition%2520dataset.We%2520propose%2520adding%250Aswitched%2520auxiliary%2520loss%2520for%2520training%2520models%2520like%2520the%2520transformers%2520to%2520overcome%250Athe%2520diminishing%2520gradient%2520problem%2520which%2520poses%2520a%2520challenge%2520towards%2520optimal%250Atraining%2520of%2520deep%2520models.Overall%252C%2520our%2520model%2520achieved%2520a%2520dice%2520score%2520of%25200.793%2520on%250Athe%2520public%2520dataset%2520and%25200.778%2520on%2520the%2520private%2520dataset.The%2520results%2520supports%2520the%250Arobustness%2520of%2520the%2520proposed%2520training%2520methodology.The%2520findings%2520also%2520bolster%2520the%250Ause%2520of%2520transformers%2520models%2520for%2520dense%2520prediction%2520tasks%2520in%2520the%2520field%2520of%2520medical%250Aimage%2520analysis.The%2520study%2520assists%2520in%2520understanding%2520the%2520relationships%2520between%250Acell%2520and%2520tissue%2520organization%2520thereby%2520providing%2520a%2520useful%2520medium%2520to%2520look%2520at%2520the%250Aimpact%2520of%2520cellular%2520functions%2520on%2520human%2520health.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2308.10994v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Switched%20auxiliary%20loss%20for%20robust%20training%20of%20transformer%20models%20for%0A%20%20histopathological%20image%20segmentation&entry.906535625=Mustaffa%20Hussain%20and%20Saharsh%20Barve&entry.1292438233=%20%20Functional%20tissue%20Units%20%28FTUs%29%20are%20cell%20population%20neighborhoods%20local%20to%20a%0Aparticular%20organ%20performing%20its%20main%20function.The%20FTUs%20provide%20crucial%0Ainformation%20to%20the%20pathologist%20in%20understanding%20the%20disease%20affecting%20a%0Aparticular%20organ%20by%20providing%20information%20at%20the%20cellular%20level.In%20our%0Aresearch%2C%20we%20have%20developed%20a%20model%20to%20segment%20multi-organ%20FTUs%20across%205%20organs%0Anamely%3A%20the%20kidney%2C%20large%20intestine%2C%20lung%2C%20prostate%20and%20spleen%20by%20utilizing%20the%0A%27HuBMAP%20%2B%20HPA%20-%20Hacking%20the%20Human%20Body%27%20competition%20dataset.We%20propose%20adding%0Aswitched%20auxiliary%20loss%20for%20training%20models%20like%20the%20transformers%20to%20overcome%0Athe%20diminishing%20gradient%20problem%20which%20poses%20a%20challenge%20towards%20optimal%0Atraining%20of%20deep%20models.Overall%2C%20our%20model%20achieved%20a%20dice%20score%20of%200.793%20on%0Athe%20public%20dataset%20and%200.778%20on%20the%20private%20dataset.The%20results%20supports%20the%0Arobustness%20of%20the%20proposed%20training%20methodology.The%20findings%20also%20bolster%20the%0Ause%20of%20transformers%20models%20for%20dense%20prediction%20tasks%20in%20the%20field%20of%20medical%0Aimage%20analysis.The%20study%20assists%20in%20understanding%20the%20relationships%20between%0Acell%20and%20tissue%20organization%20thereby%20providing%20a%20useful%20medium%20to%20look%20at%20the%0Aimpact%20of%20cellular%20functions%20on%20human%20health.%0A&entry.1838667208=http%3A//arxiv.org/abs/2308.10994v2&entry.124074799=Read"},
{"title": "Explicit Abnormality Extraction for Unsupervised Motion Artifact\n  Reduction in Magnetic Resonance Imaging", "author": "Yusheng Zhou and Hao Li and Jianan Liu and Zhengmin Kong and Tao Huang and Euijoon Ahn and Zhihan Lv and Jinman Kim and David Dagan Feng", "abstract": "  Motion artifacts compromise the quality of magnetic resonance imaging (MRI)\nand pose challenges to achieving diagnostic outcomes and image-guided\ntherapies. In recent years, supervised deep learning approaches have emerged as\nsuccessful solutions for motion artifact reduction (MAR). One disadvantage of\nthese methods is their dependency on acquiring paired sets of motion\nartifact-corrupted (MA-corrupted) and motion artifact-free (MA-free) MR images\nfor training purposes. Obtaining such image pairs is difficult and therefore\nlimits the application of supervised training. In this paper, we propose a\nnovel UNsupervised Abnormality Extraction Network (UNAEN) to alleviate this\nproblem. Our network is capable of working with unpaired MA-corrupted and\nMA-free images. It converts the MA-corrupted images to MA-reduced images by\nextracting abnormalities from the MA-corrupted images using a proposed artifact\nextractor, which intercepts the residual artifact maps from the MA-corrupted MR\nimages explicitly, and a reconstructor to restore the original input from the\nMA-reduced images. The performance of UNAEN was assessed by experimenting with\nvarious publicly available MRI datasets and comparing them with\nstate-of-the-art methods. The quantitative evaluation demonstrates the\nsuperiority of UNAEN over alternative MAR methods and visually exhibits fewer\nresidual artifacts. Our results substantiate the potential of UNAEN as a\npromising solution applicable in real-world clinical environments, with the\ncapability to enhance diagnostic accuracy and facilitate image-guided\ntherapies. Our codes are publicly available at\nhttps://github.com/YuSheng-Zhou/UNAEN.\n", "link": "http://arxiv.org/abs/2301.01732v6", "date": "2024-08-14", "relevancy": 2.0678, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5679}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5165}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.497}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Explicit%20Abnormality%20Extraction%20for%20Unsupervised%20Motion%20Artifact%0A%20%20Reduction%20in%20Magnetic%20Resonance%20Imaging&body=Title%3A%20Explicit%20Abnormality%20Extraction%20for%20Unsupervised%20Motion%20Artifact%0A%20%20Reduction%20in%20Magnetic%20Resonance%20Imaging%0AAuthor%3A%20Yusheng%20Zhou%20and%20Hao%20Li%20and%20Jianan%20Liu%20and%20Zhengmin%20Kong%20and%20Tao%20Huang%20and%20Euijoon%20Ahn%20and%20Zhihan%20Lv%20and%20Jinman%20Kim%20and%20David%20Dagan%20Feng%0AAbstract%3A%20%20%20Motion%20artifacts%20compromise%20the%20quality%20of%20magnetic%20resonance%20imaging%20%28MRI%29%0Aand%20pose%20challenges%20to%20achieving%20diagnostic%20outcomes%20and%20image-guided%0Atherapies.%20In%20recent%20years%2C%20supervised%20deep%20learning%20approaches%20have%20emerged%20as%0Asuccessful%20solutions%20for%20motion%20artifact%20reduction%20%28MAR%29.%20One%20disadvantage%20of%0Athese%20methods%20is%20their%20dependency%20on%20acquiring%20paired%20sets%20of%20motion%0Aartifact-corrupted%20%28MA-corrupted%29%20and%20motion%20artifact-free%20%28MA-free%29%20MR%20images%0Afor%20training%20purposes.%20Obtaining%20such%20image%20pairs%20is%20difficult%20and%20therefore%0Alimits%20the%20application%20of%20supervised%20training.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20UNsupervised%20Abnormality%20Extraction%20Network%20%28UNAEN%29%20to%20alleviate%20this%0Aproblem.%20Our%20network%20is%20capable%20of%20working%20with%20unpaired%20MA-corrupted%20and%0AMA-free%20images.%20It%20converts%20the%20MA-corrupted%20images%20to%20MA-reduced%20images%20by%0Aextracting%20abnormalities%20from%20the%20MA-corrupted%20images%20using%20a%20proposed%20artifact%0Aextractor%2C%20which%20intercepts%20the%20residual%20artifact%20maps%20from%20the%20MA-corrupted%20MR%0Aimages%20explicitly%2C%20and%20a%20reconstructor%20to%20restore%20the%20original%20input%20from%20the%0AMA-reduced%20images.%20The%20performance%20of%20UNAEN%20was%20assessed%20by%20experimenting%20with%0Avarious%20publicly%20available%20MRI%20datasets%20and%20comparing%20them%20with%0Astate-of-the-art%20methods.%20The%20quantitative%20evaluation%20demonstrates%20the%0Asuperiority%20of%20UNAEN%20over%20alternative%20MAR%20methods%20and%20visually%20exhibits%20fewer%0Aresidual%20artifacts.%20Our%20results%20substantiate%20the%20potential%20of%20UNAEN%20as%20a%0Apromising%20solution%20applicable%20in%20real-world%20clinical%20environments%2C%20with%20the%0Acapability%20to%20enhance%20diagnostic%20accuracy%20and%20facilitate%20image-guided%0Atherapies.%20Our%20codes%20are%20publicly%20available%20at%0Ahttps%3A//github.com/YuSheng-Zhou/UNAEN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2301.01732v6%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExplicit%2520Abnormality%2520Extraction%2520for%2520Unsupervised%2520Motion%2520Artifact%250A%2520%2520Reduction%2520in%2520Magnetic%2520Resonance%2520Imaging%26entry.906535625%3DYusheng%2520Zhou%2520and%2520Hao%2520Li%2520and%2520Jianan%2520Liu%2520and%2520Zhengmin%2520Kong%2520and%2520Tao%2520Huang%2520and%2520Euijoon%2520Ahn%2520and%2520Zhihan%2520Lv%2520and%2520Jinman%2520Kim%2520and%2520David%2520Dagan%2520Feng%26entry.1292438233%3D%2520%2520Motion%2520artifacts%2520compromise%2520the%2520quality%2520of%2520magnetic%2520resonance%2520imaging%2520%2528MRI%2529%250Aand%2520pose%2520challenges%2520to%2520achieving%2520diagnostic%2520outcomes%2520and%2520image-guided%250Atherapies.%2520In%2520recent%2520years%252C%2520supervised%2520deep%2520learning%2520approaches%2520have%2520emerged%2520as%250Asuccessful%2520solutions%2520for%2520motion%2520artifact%2520reduction%2520%2528MAR%2529.%2520One%2520disadvantage%2520of%250Athese%2520methods%2520is%2520their%2520dependency%2520on%2520acquiring%2520paired%2520sets%2520of%2520motion%250Aartifact-corrupted%2520%2528MA-corrupted%2529%2520and%2520motion%2520artifact-free%2520%2528MA-free%2529%2520MR%2520images%250Afor%2520training%2520purposes.%2520Obtaining%2520such%2520image%2520pairs%2520is%2520difficult%2520and%2520therefore%250Alimits%2520the%2520application%2520of%2520supervised%2520training.%2520In%2520this%2520paper%252C%2520we%2520propose%2520a%250Anovel%2520UNsupervised%2520Abnormality%2520Extraction%2520Network%2520%2528UNAEN%2529%2520to%2520alleviate%2520this%250Aproblem.%2520Our%2520network%2520is%2520capable%2520of%2520working%2520with%2520unpaired%2520MA-corrupted%2520and%250AMA-free%2520images.%2520It%2520converts%2520the%2520MA-corrupted%2520images%2520to%2520MA-reduced%2520images%2520by%250Aextracting%2520abnormalities%2520from%2520the%2520MA-corrupted%2520images%2520using%2520a%2520proposed%2520artifact%250Aextractor%252C%2520which%2520intercepts%2520the%2520residual%2520artifact%2520maps%2520from%2520the%2520MA-corrupted%2520MR%250Aimages%2520explicitly%252C%2520and%2520a%2520reconstructor%2520to%2520restore%2520the%2520original%2520input%2520from%2520the%250AMA-reduced%2520images.%2520The%2520performance%2520of%2520UNAEN%2520was%2520assessed%2520by%2520experimenting%2520with%250Avarious%2520publicly%2520available%2520MRI%2520datasets%2520and%2520comparing%2520them%2520with%250Astate-of-the-art%2520methods.%2520The%2520quantitative%2520evaluation%2520demonstrates%2520the%250Asuperiority%2520of%2520UNAEN%2520over%2520alternative%2520MAR%2520methods%2520and%2520visually%2520exhibits%2520fewer%250Aresidual%2520artifacts.%2520Our%2520results%2520substantiate%2520the%2520potential%2520of%2520UNAEN%2520as%2520a%250Apromising%2520solution%2520applicable%2520in%2520real-world%2520clinical%2520environments%252C%2520with%2520the%250Acapability%2520to%2520enhance%2520diagnostic%2520accuracy%2520and%2520facilitate%2520image-guided%250Atherapies.%2520Our%2520codes%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/YuSheng-Zhou/UNAEN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2301.01732v6%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Explicit%20Abnormality%20Extraction%20for%20Unsupervised%20Motion%20Artifact%0A%20%20Reduction%20in%20Magnetic%20Resonance%20Imaging&entry.906535625=Yusheng%20Zhou%20and%20Hao%20Li%20and%20Jianan%20Liu%20and%20Zhengmin%20Kong%20and%20Tao%20Huang%20and%20Euijoon%20Ahn%20and%20Zhihan%20Lv%20and%20Jinman%20Kim%20and%20David%20Dagan%20Feng&entry.1292438233=%20%20Motion%20artifacts%20compromise%20the%20quality%20of%20magnetic%20resonance%20imaging%20%28MRI%29%0Aand%20pose%20challenges%20to%20achieving%20diagnostic%20outcomes%20and%20image-guided%0Atherapies.%20In%20recent%20years%2C%20supervised%20deep%20learning%20approaches%20have%20emerged%20as%0Asuccessful%20solutions%20for%20motion%20artifact%20reduction%20%28MAR%29.%20One%20disadvantage%20of%0Athese%20methods%20is%20their%20dependency%20on%20acquiring%20paired%20sets%20of%20motion%0Aartifact-corrupted%20%28MA-corrupted%29%20and%20motion%20artifact-free%20%28MA-free%29%20MR%20images%0Afor%20training%20purposes.%20Obtaining%20such%20image%20pairs%20is%20difficult%20and%20therefore%0Alimits%20the%20application%20of%20supervised%20training.%20In%20this%20paper%2C%20we%20propose%20a%0Anovel%20UNsupervised%20Abnormality%20Extraction%20Network%20%28UNAEN%29%20to%20alleviate%20this%0Aproblem.%20Our%20network%20is%20capable%20of%20working%20with%20unpaired%20MA-corrupted%20and%0AMA-free%20images.%20It%20converts%20the%20MA-corrupted%20images%20to%20MA-reduced%20images%20by%0Aextracting%20abnormalities%20from%20the%20MA-corrupted%20images%20using%20a%20proposed%20artifact%0Aextractor%2C%20which%20intercepts%20the%20residual%20artifact%20maps%20from%20the%20MA-corrupted%20MR%0Aimages%20explicitly%2C%20and%20a%20reconstructor%20to%20restore%20the%20original%20input%20from%20the%0AMA-reduced%20images.%20The%20performance%20of%20UNAEN%20was%20assessed%20by%20experimenting%20with%0Avarious%20publicly%20available%20MRI%20datasets%20and%20comparing%20them%20with%0Astate-of-the-art%20methods.%20The%20quantitative%20evaluation%20demonstrates%20the%0Asuperiority%20of%20UNAEN%20over%20alternative%20MAR%20methods%20and%20visually%20exhibits%20fewer%0Aresidual%20artifacts.%20Our%20results%20substantiate%20the%20potential%20of%20UNAEN%20as%20a%0Apromising%20solution%20applicable%20in%20real-world%20clinical%20environments%2C%20with%20the%0Acapability%20to%20enhance%20diagnostic%20accuracy%20and%20facilitate%20image-guided%0Atherapies.%20Our%20codes%20are%20publicly%20available%20at%0Ahttps%3A//github.com/YuSheng-Zhou/UNAEN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2301.01732v6&entry.124074799=Read"},
{"title": "Data Science for Geographic Information Systems", "author": "Afonso Oliveira and Nuno Fachada and Jo\u00e3o P. Matos-Carvalho", "abstract": "  The integration of data science into Geographic Information Systems (GIS) has\nfacilitated the evolution of these tools into complete spatial analysis\nplatforms. The adoption of machine learning and big data techniques has\nequipped these platforms with the capacity to handle larger amounts of\nincreasingly complex data, transcending the limitations of more traditional\napproaches. This work traces the historical and technical evolution of data\nscience and GIS as fields of study, highlighting the critical points of\nconvergence between domains, and underlining the many sectors that rely on this\nintegration. A GIS application is presented as a case study in the disaster\nmanagement sector where we utilize aerial data from Tr\\'oia, Portugal, to\nemphasize the process of insight extraction from raw data. We conclude by\noutlining prospects for future research in integration of these fields in\ngeneral, and the developed application in particular.\n", "link": "http://arxiv.org/abs/2404.03754v2", "date": "2024-08-14", "relevancy": 2.0432, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4198}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4062}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Data%20Science%20for%20Geographic%20Information%20Systems&body=Title%3A%20Data%20Science%20for%20Geographic%20Information%20Systems%0AAuthor%3A%20Afonso%20Oliveira%20and%20Nuno%20Fachada%20and%20Jo%C3%A3o%20P.%20Matos-Carvalho%0AAbstract%3A%20%20%20The%20integration%20of%20data%20science%20into%20Geographic%20Information%20Systems%20%28GIS%29%20has%0Afacilitated%20the%20evolution%20of%20these%20tools%20into%20complete%20spatial%20analysis%0Aplatforms.%20The%20adoption%20of%20machine%20learning%20and%20big%20data%20techniques%20has%0Aequipped%20these%20platforms%20with%20the%20capacity%20to%20handle%20larger%20amounts%20of%0Aincreasingly%20complex%20data%2C%20transcending%20the%20limitations%20of%20more%20traditional%0Aapproaches.%20This%20work%20traces%20the%20historical%20and%20technical%20evolution%20of%20data%0Ascience%20and%20GIS%20as%20fields%20of%20study%2C%20highlighting%20the%20critical%20points%20of%0Aconvergence%20between%20domains%2C%20and%20underlining%20the%20many%20sectors%20that%20rely%20on%20this%0Aintegration.%20A%20GIS%20application%20is%20presented%20as%20a%20case%20study%20in%20the%20disaster%0Amanagement%20sector%20where%20we%20utilize%20aerial%20data%20from%20Tr%5C%27oia%2C%20Portugal%2C%20to%0Aemphasize%20the%20process%20of%20insight%20extraction%20from%20raw%20data.%20We%20conclude%20by%0Aoutlining%20prospects%20for%20future%20research%20in%20integration%20of%20these%20fields%20in%0Ageneral%2C%20and%20the%20developed%20application%20in%20particular.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.03754v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DData%2520Science%2520for%2520Geographic%2520Information%2520Systems%26entry.906535625%3DAfonso%2520Oliveira%2520and%2520Nuno%2520Fachada%2520and%2520Jo%25C3%25A3o%2520P.%2520Matos-Carvalho%26entry.1292438233%3D%2520%2520The%2520integration%2520of%2520data%2520science%2520into%2520Geographic%2520Information%2520Systems%2520%2528GIS%2529%2520has%250Afacilitated%2520the%2520evolution%2520of%2520these%2520tools%2520into%2520complete%2520spatial%2520analysis%250Aplatforms.%2520The%2520adoption%2520of%2520machine%2520learning%2520and%2520big%2520data%2520techniques%2520has%250Aequipped%2520these%2520platforms%2520with%2520the%2520capacity%2520to%2520handle%2520larger%2520amounts%2520of%250Aincreasingly%2520complex%2520data%252C%2520transcending%2520the%2520limitations%2520of%2520more%2520traditional%250Aapproaches.%2520This%2520work%2520traces%2520the%2520historical%2520and%2520technical%2520evolution%2520of%2520data%250Ascience%2520and%2520GIS%2520as%2520fields%2520of%2520study%252C%2520highlighting%2520the%2520critical%2520points%2520of%250Aconvergence%2520between%2520domains%252C%2520and%2520underlining%2520the%2520many%2520sectors%2520that%2520rely%2520on%2520this%250Aintegration.%2520A%2520GIS%2520application%2520is%2520presented%2520as%2520a%2520case%2520study%2520in%2520the%2520disaster%250Amanagement%2520sector%2520where%2520we%2520utilize%2520aerial%2520data%2520from%2520Tr%255C%2527oia%252C%2520Portugal%252C%2520to%250Aemphasize%2520the%2520process%2520of%2520insight%2520extraction%2520from%2520raw%2520data.%2520We%2520conclude%2520by%250Aoutlining%2520prospects%2520for%2520future%2520research%2520in%2520integration%2520of%2520these%2520fields%2520in%250Ageneral%252C%2520and%2520the%2520developed%2520application%2520in%2520particular.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.03754v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Data%20Science%20for%20Geographic%20Information%20Systems&entry.906535625=Afonso%20Oliveira%20and%20Nuno%20Fachada%20and%20Jo%C3%A3o%20P.%20Matos-Carvalho&entry.1292438233=%20%20The%20integration%20of%20data%20science%20into%20Geographic%20Information%20Systems%20%28GIS%29%20has%0Afacilitated%20the%20evolution%20of%20these%20tools%20into%20complete%20spatial%20analysis%0Aplatforms.%20The%20adoption%20of%20machine%20learning%20and%20big%20data%20techniques%20has%0Aequipped%20these%20platforms%20with%20the%20capacity%20to%20handle%20larger%20amounts%20of%0Aincreasingly%20complex%20data%2C%20transcending%20the%20limitations%20of%20more%20traditional%0Aapproaches.%20This%20work%20traces%20the%20historical%20and%20technical%20evolution%20of%20data%0Ascience%20and%20GIS%20as%20fields%20of%20study%2C%20highlighting%20the%20critical%20points%20of%0Aconvergence%20between%20domains%2C%20and%20underlining%20the%20many%20sectors%20that%20rely%20on%20this%0Aintegration.%20A%20GIS%20application%20is%20presented%20as%20a%20case%20study%20in%20the%20disaster%0Amanagement%20sector%20where%20we%20utilize%20aerial%20data%20from%20Tr%5C%27oia%2C%20Portugal%2C%20to%0Aemphasize%20the%20process%20of%20insight%20extraction%20from%20raw%20data.%20We%20conclude%20by%0Aoutlining%20prospects%20for%20future%20research%20in%20integration%20of%20these%20fields%20in%0Ageneral%2C%20and%20the%20developed%20application%20in%20particular.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.03754v2&entry.124074799=Read"},
{"title": "Protected Test-Time Adaptation via Online Entropy Matching: A Betting\n  Approach", "author": "Yarin Bar and Shalev Shaer and Yaniv Romano", "abstract": "  We present a novel approach for test-time adaptation via online\nself-training, consisting of two components. First, we introduce a statistical\nframework that detects distribution shifts in the classifier's entropy values\nobtained on a stream of unlabeled samples. Second, we devise an online\nadaptation mechanism that utilizes the evidence of distribution shifts captured\nby the detection tool to dynamically update the classifier's parameters. The\nresulting adaptation process drives the distribution of test entropy values\nobtained from the self-trained classifier to match those of the source domain,\nbuilding invariance to distribution shifts. This approach departs from the\nconventional self-training method, which focuses on minimizing the classifier's\nentropy. Our approach combines concepts in betting martingales and online\nlearning to form a detection tool capable of quickly reacting to distribution\nshifts. We then reveal a tight relation between our adaptation scheme and\noptimal transport, which forms the basis of our novel self-supervised loss.\nExperimental results demonstrate that our approach improves test-time accuracy\nunder distribution shifts while maintaining accuracy and calibration in their\nabsence, outperforming leading entropy minimization methods across various\nscenarios.\n", "link": "http://arxiv.org/abs/2408.07511v1", "date": "2024-08-14", "relevancy": 2.0208, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5151}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4997}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4942}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Protected%20Test-Time%20Adaptation%20via%20Online%20Entropy%20Matching%3A%20A%20Betting%0A%20%20Approach&body=Title%3A%20Protected%20Test-Time%20Adaptation%20via%20Online%20Entropy%20Matching%3A%20A%20Betting%0A%20%20Approach%0AAuthor%3A%20Yarin%20Bar%20and%20Shalev%20Shaer%20and%20Yaniv%20Romano%0AAbstract%3A%20%20%20We%20present%20a%20novel%20approach%20for%20test-time%20adaptation%20via%20online%0Aself-training%2C%20consisting%20of%20two%20components.%20First%2C%20we%20introduce%20a%20statistical%0Aframework%20that%20detects%20distribution%20shifts%20in%20the%20classifier%27s%20entropy%20values%0Aobtained%20on%20a%20stream%20of%20unlabeled%20samples.%20Second%2C%20we%20devise%20an%20online%0Aadaptation%20mechanism%20that%20utilizes%20the%20evidence%20of%20distribution%20shifts%20captured%0Aby%20the%20detection%20tool%20to%20dynamically%20update%20the%20classifier%27s%20parameters.%20The%0Aresulting%20adaptation%20process%20drives%20the%20distribution%20of%20test%20entropy%20values%0Aobtained%20from%20the%20self-trained%20classifier%20to%20match%20those%20of%20the%20source%20domain%2C%0Abuilding%20invariance%20to%20distribution%20shifts.%20This%20approach%20departs%20from%20the%0Aconventional%20self-training%20method%2C%20which%20focuses%20on%20minimizing%20the%20classifier%27s%0Aentropy.%20Our%20approach%20combines%20concepts%20in%20betting%20martingales%20and%20online%0Alearning%20to%20form%20a%20detection%20tool%20capable%20of%20quickly%20reacting%20to%20distribution%0Ashifts.%20We%20then%20reveal%20a%20tight%20relation%20between%20our%20adaptation%20scheme%20and%0Aoptimal%20transport%2C%20which%20forms%20the%20basis%20of%20our%20novel%20self-supervised%20loss.%0AExperimental%20results%20demonstrate%20that%20our%20approach%20improves%20test-time%20accuracy%0Aunder%20distribution%20shifts%20while%20maintaining%20accuracy%20and%20calibration%20in%20their%0Aabsence%2C%20outperforming%20leading%20entropy%20minimization%20methods%20across%20various%0Ascenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07511v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProtected%2520Test-Time%2520Adaptation%2520via%2520Online%2520Entropy%2520Matching%253A%2520A%2520Betting%250A%2520%2520Approach%26entry.906535625%3DYarin%2520Bar%2520and%2520Shalev%2520Shaer%2520and%2520Yaniv%2520Romano%26entry.1292438233%3D%2520%2520We%2520present%2520a%2520novel%2520approach%2520for%2520test-time%2520adaptation%2520via%2520online%250Aself-training%252C%2520consisting%2520of%2520two%2520components.%2520First%252C%2520we%2520introduce%2520a%2520statistical%250Aframework%2520that%2520detects%2520distribution%2520shifts%2520in%2520the%2520classifier%2527s%2520entropy%2520values%250Aobtained%2520on%2520a%2520stream%2520of%2520unlabeled%2520samples.%2520Second%252C%2520we%2520devise%2520an%2520online%250Aadaptation%2520mechanism%2520that%2520utilizes%2520the%2520evidence%2520of%2520distribution%2520shifts%2520captured%250Aby%2520the%2520detection%2520tool%2520to%2520dynamically%2520update%2520the%2520classifier%2527s%2520parameters.%2520The%250Aresulting%2520adaptation%2520process%2520drives%2520the%2520distribution%2520of%2520test%2520entropy%2520values%250Aobtained%2520from%2520the%2520self-trained%2520classifier%2520to%2520match%2520those%2520of%2520the%2520source%2520domain%252C%250Abuilding%2520invariance%2520to%2520distribution%2520shifts.%2520This%2520approach%2520departs%2520from%2520the%250Aconventional%2520self-training%2520method%252C%2520which%2520focuses%2520on%2520minimizing%2520the%2520classifier%2527s%250Aentropy.%2520Our%2520approach%2520combines%2520concepts%2520in%2520betting%2520martingales%2520and%2520online%250Alearning%2520to%2520form%2520a%2520detection%2520tool%2520capable%2520of%2520quickly%2520reacting%2520to%2520distribution%250Ashifts.%2520We%2520then%2520reveal%2520a%2520tight%2520relation%2520between%2520our%2520adaptation%2520scheme%2520and%250Aoptimal%2520transport%252C%2520which%2520forms%2520the%2520basis%2520of%2520our%2520novel%2520self-supervised%2520loss.%250AExperimental%2520results%2520demonstrate%2520that%2520our%2520approach%2520improves%2520test-time%2520accuracy%250Aunder%2520distribution%2520shifts%2520while%2520maintaining%2520accuracy%2520and%2520calibration%2520in%2520their%250Aabsence%252C%2520outperforming%2520leading%2520entropy%2520minimization%2520methods%2520across%2520various%250Ascenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07511v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Protected%20Test-Time%20Adaptation%20via%20Online%20Entropy%20Matching%3A%20A%20Betting%0A%20%20Approach&entry.906535625=Yarin%20Bar%20and%20Shalev%20Shaer%20and%20Yaniv%20Romano&entry.1292438233=%20%20We%20present%20a%20novel%20approach%20for%20test-time%20adaptation%20via%20online%0Aself-training%2C%20consisting%20of%20two%20components.%20First%2C%20we%20introduce%20a%20statistical%0Aframework%20that%20detects%20distribution%20shifts%20in%20the%20classifier%27s%20entropy%20values%0Aobtained%20on%20a%20stream%20of%20unlabeled%20samples.%20Second%2C%20we%20devise%20an%20online%0Aadaptation%20mechanism%20that%20utilizes%20the%20evidence%20of%20distribution%20shifts%20captured%0Aby%20the%20detection%20tool%20to%20dynamically%20update%20the%20classifier%27s%20parameters.%20The%0Aresulting%20adaptation%20process%20drives%20the%20distribution%20of%20test%20entropy%20values%0Aobtained%20from%20the%20self-trained%20classifier%20to%20match%20those%20of%20the%20source%20domain%2C%0Abuilding%20invariance%20to%20distribution%20shifts.%20This%20approach%20departs%20from%20the%0Aconventional%20self-training%20method%2C%20which%20focuses%20on%20minimizing%20the%20classifier%27s%0Aentropy.%20Our%20approach%20combines%20concepts%20in%20betting%20martingales%20and%20online%0Alearning%20to%20form%20a%20detection%20tool%20capable%20of%20quickly%20reacting%20to%20distribution%0Ashifts.%20We%20then%20reveal%20a%20tight%20relation%20between%20our%20adaptation%20scheme%20and%0Aoptimal%20transport%2C%20which%20forms%20the%20basis%20of%20our%20novel%20self-supervised%20loss.%0AExperimental%20results%20demonstrate%20that%20our%20approach%20improves%20test-time%20accuracy%0Aunder%20distribution%20shifts%20while%20maintaining%20accuracy%20and%20calibration%20in%20their%0Aabsence%2C%20outperforming%20leading%20entropy%20minimization%20methods%20across%20various%0Ascenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07511v1&entry.124074799=Read"},
{"title": "A Probabilistic Approach to Learning the Degree of Equivariance in\n  Steerable CNNs", "author": "Lars Veefkind and Gabriele Cesa", "abstract": "  Steerable convolutional neural networks (SCNNs) enhance task performance by\nmodelling geometric symmetries through equivariance constraints on weights.\nYet, unknown or varying symmetries can lead to overconstrained weights and\ndecreased performance. To address this, this paper introduces a probabilistic\nmethod to learn the degree of equivariance in SCNNs. We parameterise the degree\nof equivariance as a likelihood distribution over the transformation group\nusing Fourier coefficients, offering the option to model layer-wise and shared\nequivariance. These likelihood distributions are regularised to ensure an\ninterpretable degree of equivariance across the network. Advantages include the\napplicability to many types of equivariant networks through the flexible\nframework of SCNNs and the ability to learn equivariance with respect to any\nsubgroup of any compact group without requiring additional layers. Our\nexperiments reveal competitive performance on datasets with mixed symmetries,\nwith learnt likelihood distributions that are representative of the underlying\ndegree of equivariance.\n", "link": "http://arxiv.org/abs/2406.03946v2", "date": "2024-08-14", "relevancy": 2.0167, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5192}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5015}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5009}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Probabilistic%20Approach%20to%20Learning%20the%20Degree%20of%20Equivariance%20in%0A%20%20Steerable%20CNNs&body=Title%3A%20A%20Probabilistic%20Approach%20to%20Learning%20the%20Degree%20of%20Equivariance%20in%0A%20%20Steerable%20CNNs%0AAuthor%3A%20Lars%20Veefkind%20and%20Gabriele%20Cesa%0AAbstract%3A%20%20%20Steerable%20convolutional%20neural%20networks%20%28SCNNs%29%20enhance%20task%20performance%20by%0Amodelling%20geometric%20symmetries%20through%20equivariance%20constraints%20on%20weights.%0AYet%2C%20unknown%20or%20varying%20symmetries%20can%20lead%20to%20overconstrained%20weights%20and%0Adecreased%20performance.%20To%20address%20this%2C%20this%20paper%20introduces%20a%20probabilistic%0Amethod%20to%20learn%20the%20degree%20of%20equivariance%20in%20SCNNs.%20We%20parameterise%20the%20degree%0Aof%20equivariance%20as%20a%20likelihood%20distribution%20over%20the%20transformation%20group%0Ausing%20Fourier%20coefficients%2C%20offering%20the%20option%20to%20model%20layer-wise%20and%20shared%0Aequivariance.%20These%20likelihood%20distributions%20are%20regularised%20to%20ensure%20an%0Ainterpretable%20degree%20of%20equivariance%20across%20the%20network.%20Advantages%20include%20the%0Aapplicability%20to%20many%20types%20of%20equivariant%20networks%20through%20the%20flexible%0Aframework%20of%20SCNNs%20and%20the%20ability%20to%20learn%20equivariance%20with%20respect%20to%20any%0Asubgroup%20of%20any%20compact%20group%20without%20requiring%20additional%20layers.%20Our%0Aexperiments%20reveal%20competitive%20performance%20on%20datasets%20with%20mixed%20symmetries%2C%0Awith%20learnt%20likelihood%20distributions%20that%20are%20representative%20of%20the%20underlying%0Adegree%20of%20equivariance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.03946v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Probabilistic%2520Approach%2520to%2520Learning%2520the%2520Degree%2520of%2520Equivariance%2520in%250A%2520%2520Steerable%2520CNNs%26entry.906535625%3DLars%2520Veefkind%2520and%2520Gabriele%2520Cesa%26entry.1292438233%3D%2520%2520Steerable%2520convolutional%2520neural%2520networks%2520%2528SCNNs%2529%2520enhance%2520task%2520performance%2520by%250Amodelling%2520geometric%2520symmetries%2520through%2520equivariance%2520constraints%2520on%2520weights.%250AYet%252C%2520unknown%2520or%2520varying%2520symmetries%2520can%2520lead%2520to%2520overconstrained%2520weights%2520and%250Adecreased%2520performance.%2520To%2520address%2520this%252C%2520this%2520paper%2520introduces%2520a%2520probabilistic%250Amethod%2520to%2520learn%2520the%2520degree%2520of%2520equivariance%2520in%2520SCNNs.%2520We%2520parameterise%2520the%2520degree%250Aof%2520equivariance%2520as%2520a%2520likelihood%2520distribution%2520over%2520the%2520transformation%2520group%250Ausing%2520Fourier%2520coefficients%252C%2520offering%2520the%2520option%2520to%2520model%2520layer-wise%2520and%2520shared%250Aequivariance.%2520These%2520likelihood%2520distributions%2520are%2520regularised%2520to%2520ensure%2520an%250Ainterpretable%2520degree%2520of%2520equivariance%2520across%2520the%2520network.%2520Advantages%2520include%2520the%250Aapplicability%2520to%2520many%2520types%2520of%2520equivariant%2520networks%2520through%2520the%2520flexible%250Aframework%2520of%2520SCNNs%2520and%2520the%2520ability%2520to%2520learn%2520equivariance%2520with%2520respect%2520to%2520any%250Asubgroup%2520of%2520any%2520compact%2520group%2520without%2520requiring%2520additional%2520layers.%2520Our%250Aexperiments%2520reveal%2520competitive%2520performance%2520on%2520datasets%2520with%2520mixed%2520symmetries%252C%250Awith%2520learnt%2520likelihood%2520distributions%2520that%2520are%2520representative%2520of%2520the%2520underlying%250Adegree%2520of%2520equivariance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.03946v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Probabilistic%20Approach%20to%20Learning%20the%20Degree%20of%20Equivariance%20in%0A%20%20Steerable%20CNNs&entry.906535625=Lars%20Veefkind%20and%20Gabriele%20Cesa&entry.1292438233=%20%20Steerable%20convolutional%20neural%20networks%20%28SCNNs%29%20enhance%20task%20performance%20by%0Amodelling%20geometric%20symmetries%20through%20equivariance%20constraints%20on%20weights.%0AYet%2C%20unknown%20or%20varying%20symmetries%20can%20lead%20to%20overconstrained%20weights%20and%0Adecreased%20performance.%20To%20address%20this%2C%20this%20paper%20introduces%20a%20probabilistic%0Amethod%20to%20learn%20the%20degree%20of%20equivariance%20in%20SCNNs.%20We%20parameterise%20the%20degree%0Aof%20equivariance%20as%20a%20likelihood%20distribution%20over%20the%20transformation%20group%0Ausing%20Fourier%20coefficients%2C%20offering%20the%20option%20to%20model%20layer-wise%20and%20shared%0Aequivariance.%20These%20likelihood%20distributions%20are%20regularised%20to%20ensure%20an%0Ainterpretable%20degree%20of%20equivariance%20across%20the%20network.%20Advantages%20include%20the%0Aapplicability%20to%20many%20types%20of%20equivariant%20networks%20through%20the%20flexible%0Aframework%20of%20SCNNs%20and%20the%20ability%20to%20learn%20equivariance%20with%20respect%20to%20any%0Asubgroup%20of%20any%20compact%20group%20without%20requiring%20additional%20layers.%20Our%0Aexperiments%20reveal%20competitive%20performance%20on%20datasets%20with%20mixed%20symmetries%2C%0Awith%20learnt%20likelihood%20distributions%20that%20are%20representative%20of%20the%20underlying%0Adegree%20of%20equivariance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.03946v2&entry.124074799=Read"},
{"title": "MetaSeg: MetaFormer-based Global Contexts-aware Network for Efficient\n  Semantic Segmentation", "author": "Beoungwoo Kang and Seunghun Moon and Yubin Cho and Hyunwoo Yu and Suk-Ju Kang", "abstract": "  Beyond the Transformer, it is important to explore how to exploit the\ncapacity of the MetaFormer, an architecture that is fundamental to the\nperformance improvements of the Transformer. Previous studies have exploited it\nonly for the backbone network. Unlike previous studies, we explore the capacity\nof the Metaformer architecture more extensively in the semantic segmentation\ntask. We propose a powerful semantic segmentation network, MetaSeg, which\nleverages the Metaformer architecture from the backbone to the decoder. Our\nMetaSeg shows that the MetaFormer architecture plays a significant role in\ncapturing the useful contexts for the decoder as well as for the backbone. In\naddition, recent segmentation methods have shown that using a CNN-based\nbackbone for extracting the spatial information and a decoder for extracting\nthe global information is more effective than using a transformer-based\nbackbone with a CNN-based decoder. This motivates us to adopt the CNN-based\nbackbone using the MetaFormer block and design our MetaFormer-based decoder,\nwhich consists of a novel self-attention module to capture the global contexts.\nTo consider both the global contexts extraction and the computational\nefficiency of the self-attention for semantic segmentation, we propose a\nChannel Reduction Attention (CRA) module that reduces the channel dimension of\nthe query and key into the one dimension. In this way, our proposed MetaSeg\noutperforms the previous state-of-the-art methods with more efficient\ncomputational costs on popular semantic segmentation and a medical image\nsegmentation benchmark, including ADE20K, Cityscapes, COCO-stuff, and Synapse.\nThe code is available at \\url{https://github.com/hyunwoo137/MetaSeg}.\n", "link": "http://arxiv.org/abs/2408.07576v1", "date": "2024-08-14", "relevancy": 1.996, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5085}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4955}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4841}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MetaSeg%3A%20MetaFormer-based%20Global%20Contexts-aware%20Network%20for%20Efficient%0A%20%20Semantic%20Segmentation&body=Title%3A%20MetaSeg%3A%20MetaFormer-based%20Global%20Contexts-aware%20Network%20for%20Efficient%0A%20%20Semantic%20Segmentation%0AAuthor%3A%20Beoungwoo%20Kang%20and%20Seunghun%20Moon%20and%20Yubin%20Cho%20and%20Hyunwoo%20Yu%20and%20Suk-Ju%20Kang%0AAbstract%3A%20%20%20Beyond%20the%20Transformer%2C%20it%20is%20important%20to%20explore%20how%20to%20exploit%20the%0Acapacity%20of%20the%20MetaFormer%2C%20an%20architecture%20that%20is%20fundamental%20to%20the%0Aperformance%20improvements%20of%20the%20Transformer.%20Previous%20studies%20have%20exploited%20it%0Aonly%20for%20the%20backbone%20network.%20Unlike%20previous%20studies%2C%20we%20explore%20the%20capacity%0Aof%20the%20Metaformer%20architecture%20more%20extensively%20in%20the%20semantic%20segmentation%0Atask.%20We%20propose%20a%20powerful%20semantic%20segmentation%20network%2C%20MetaSeg%2C%20which%0Aleverages%20the%20Metaformer%20architecture%20from%20the%20backbone%20to%20the%20decoder.%20Our%0AMetaSeg%20shows%20that%20the%20MetaFormer%20architecture%20plays%20a%20significant%20role%20in%0Acapturing%20the%20useful%20contexts%20for%20the%20decoder%20as%20well%20as%20for%20the%20backbone.%20In%0Aaddition%2C%20recent%20segmentation%20methods%20have%20shown%20that%20using%20a%20CNN-based%0Abackbone%20for%20extracting%20the%20spatial%20information%20and%20a%20decoder%20for%20extracting%0Athe%20global%20information%20is%20more%20effective%20than%20using%20a%20transformer-based%0Abackbone%20with%20a%20CNN-based%20decoder.%20This%20motivates%20us%20to%20adopt%20the%20CNN-based%0Abackbone%20using%20the%20MetaFormer%20block%20and%20design%20our%20MetaFormer-based%20decoder%2C%0Awhich%20consists%20of%20a%20novel%20self-attention%20module%20to%20capture%20the%20global%20contexts.%0ATo%20consider%20both%20the%20global%20contexts%20extraction%20and%20the%20computational%0Aefficiency%20of%20the%20self-attention%20for%20semantic%20segmentation%2C%20we%20propose%20a%0AChannel%20Reduction%20Attention%20%28CRA%29%20module%20that%20reduces%20the%20channel%20dimension%20of%0Athe%20query%20and%20key%20into%20the%20one%20dimension.%20In%20this%20way%2C%20our%20proposed%20MetaSeg%0Aoutperforms%20the%20previous%20state-of-the-art%20methods%20with%20more%20efficient%0Acomputational%20costs%20on%20popular%20semantic%20segmentation%20and%20a%20medical%20image%0Asegmentation%20benchmark%2C%20including%20ADE20K%2C%20Cityscapes%2C%20COCO-stuff%2C%20and%20Synapse.%0AThe%20code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/hyunwoo137/MetaSeg%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07576v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMetaSeg%253A%2520MetaFormer-based%2520Global%2520Contexts-aware%2520Network%2520for%2520Efficient%250A%2520%2520Semantic%2520Segmentation%26entry.906535625%3DBeoungwoo%2520Kang%2520and%2520Seunghun%2520Moon%2520and%2520Yubin%2520Cho%2520and%2520Hyunwoo%2520Yu%2520and%2520Suk-Ju%2520Kang%26entry.1292438233%3D%2520%2520Beyond%2520the%2520Transformer%252C%2520it%2520is%2520important%2520to%2520explore%2520how%2520to%2520exploit%2520the%250Acapacity%2520of%2520the%2520MetaFormer%252C%2520an%2520architecture%2520that%2520is%2520fundamental%2520to%2520the%250Aperformance%2520improvements%2520of%2520the%2520Transformer.%2520Previous%2520studies%2520have%2520exploited%2520it%250Aonly%2520for%2520the%2520backbone%2520network.%2520Unlike%2520previous%2520studies%252C%2520we%2520explore%2520the%2520capacity%250Aof%2520the%2520Metaformer%2520architecture%2520more%2520extensively%2520in%2520the%2520semantic%2520segmentation%250Atask.%2520We%2520propose%2520a%2520powerful%2520semantic%2520segmentation%2520network%252C%2520MetaSeg%252C%2520which%250Aleverages%2520the%2520Metaformer%2520architecture%2520from%2520the%2520backbone%2520to%2520the%2520decoder.%2520Our%250AMetaSeg%2520shows%2520that%2520the%2520MetaFormer%2520architecture%2520plays%2520a%2520significant%2520role%2520in%250Acapturing%2520the%2520useful%2520contexts%2520for%2520the%2520decoder%2520as%2520well%2520as%2520for%2520the%2520backbone.%2520In%250Aaddition%252C%2520recent%2520segmentation%2520methods%2520have%2520shown%2520that%2520using%2520a%2520CNN-based%250Abackbone%2520for%2520extracting%2520the%2520spatial%2520information%2520and%2520a%2520decoder%2520for%2520extracting%250Athe%2520global%2520information%2520is%2520more%2520effective%2520than%2520using%2520a%2520transformer-based%250Abackbone%2520with%2520a%2520CNN-based%2520decoder.%2520This%2520motivates%2520us%2520to%2520adopt%2520the%2520CNN-based%250Abackbone%2520using%2520the%2520MetaFormer%2520block%2520and%2520design%2520our%2520MetaFormer-based%2520decoder%252C%250Awhich%2520consists%2520of%2520a%2520novel%2520self-attention%2520module%2520to%2520capture%2520the%2520global%2520contexts.%250ATo%2520consider%2520both%2520the%2520global%2520contexts%2520extraction%2520and%2520the%2520computational%250Aefficiency%2520of%2520the%2520self-attention%2520for%2520semantic%2520segmentation%252C%2520we%2520propose%2520a%250AChannel%2520Reduction%2520Attention%2520%2528CRA%2529%2520module%2520that%2520reduces%2520the%2520channel%2520dimension%2520of%250Athe%2520query%2520and%2520key%2520into%2520the%2520one%2520dimension.%2520In%2520this%2520way%252C%2520our%2520proposed%2520MetaSeg%250Aoutperforms%2520the%2520previous%2520state-of-the-art%2520methods%2520with%2520more%2520efficient%250Acomputational%2520costs%2520on%2520popular%2520semantic%2520segmentation%2520and%2520a%2520medical%2520image%250Asegmentation%2520benchmark%252C%2520including%2520ADE20K%252C%2520Cityscapes%252C%2520COCO-stuff%252C%2520and%2520Synapse.%250AThe%2520code%2520is%2520available%2520at%2520%255Curl%257Bhttps%253A//github.com/hyunwoo137/MetaSeg%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07576v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MetaSeg%3A%20MetaFormer-based%20Global%20Contexts-aware%20Network%20for%20Efficient%0A%20%20Semantic%20Segmentation&entry.906535625=Beoungwoo%20Kang%20and%20Seunghun%20Moon%20and%20Yubin%20Cho%20and%20Hyunwoo%20Yu%20and%20Suk-Ju%20Kang&entry.1292438233=%20%20Beyond%20the%20Transformer%2C%20it%20is%20important%20to%20explore%20how%20to%20exploit%20the%0Acapacity%20of%20the%20MetaFormer%2C%20an%20architecture%20that%20is%20fundamental%20to%20the%0Aperformance%20improvements%20of%20the%20Transformer.%20Previous%20studies%20have%20exploited%20it%0Aonly%20for%20the%20backbone%20network.%20Unlike%20previous%20studies%2C%20we%20explore%20the%20capacity%0Aof%20the%20Metaformer%20architecture%20more%20extensively%20in%20the%20semantic%20segmentation%0Atask.%20We%20propose%20a%20powerful%20semantic%20segmentation%20network%2C%20MetaSeg%2C%20which%0Aleverages%20the%20Metaformer%20architecture%20from%20the%20backbone%20to%20the%20decoder.%20Our%0AMetaSeg%20shows%20that%20the%20MetaFormer%20architecture%20plays%20a%20significant%20role%20in%0Acapturing%20the%20useful%20contexts%20for%20the%20decoder%20as%20well%20as%20for%20the%20backbone.%20In%0Aaddition%2C%20recent%20segmentation%20methods%20have%20shown%20that%20using%20a%20CNN-based%0Abackbone%20for%20extracting%20the%20spatial%20information%20and%20a%20decoder%20for%20extracting%0Athe%20global%20information%20is%20more%20effective%20than%20using%20a%20transformer-based%0Abackbone%20with%20a%20CNN-based%20decoder.%20This%20motivates%20us%20to%20adopt%20the%20CNN-based%0Abackbone%20using%20the%20MetaFormer%20block%20and%20design%20our%20MetaFormer-based%20decoder%2C%0Awhich%20consists%20of%20a%20novel%20self-attention%20module%20to%20capture%20the%20global%20contexts.%0ATo%20consider%20both%20the%20global%20contexts%20extraction%20and%20the%20computational%0Aefficiency%20of%20the%20self-attention%20for%20semantic%20segmentation%2C%20we%20propose%20a%0AChannel%20Reduction%20Attention%20%28CRA%29%20module%20that%20reduces%20the%20channel%20dimension%20of%0Athe%20query%20and%20key%20into%20the%20one%20dimension.%20In%20this%20way%2C%20our%20proposed%20MetaSeg%0Aoutperforms%20the%20previous%20state-of-the-art%20methods%20with%20more%20efficient%0Acomputational%20costs%20on%20popular%20semantic%20segmentation%20and%20a%20medical%20image%0Asegmentation%20benchmark%2C%20including%20ADE20K%2C%20Cityscapes%2C%20COCO-stuff%2C%20and%20Synapse.%0AThe%20code%20is%20available%20at%20%5Curl%7Bhttps%3A//github.com/hyunwoo137/MetaSeg%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07576v1&entry.124074799=Read"},
{"title": "Alignment-Enhanced Decoding:Defending via Token-Level Adaptive Refining\n  of Probability Distributions", "author": "Quan Liu and Zhenhong Zhou and Longzhu He and Yi Liu and Wei Zhang and Sen Su", "abstract": "  Large language models are susceptible to jailbreak attacks, which can result\nin the generation of harmful content. While prior defenses mitigate these risks\nby perturbing or inspecting inputs, they ignore competing objectives, the\nunderlying cause of alignment failures. In this paper, we propose\nAlignment-Enhanced Decoding (AED), a novel defense that employs adaptive\ndecoding to address the root causes of jailbreak issues. We first define the\nCompetitive Index to quantify alignment failures and utilize feedback from\nself-evaluation to compute post-alignment logits. Then, AED adaptively combines\nAED and post-alignment logits with the original logits to obtain harmless and\nhelpful distributions. Consequently, our method enhances safety alignment while\nmaintaining helpfulness. We conduct experiments across five models and four\ncommon jailbreaks, with the results validating the effectiveness of our\napproach. Code is available at https://github.com/GIGABaozi/AED.git.\n", "link": "http://arxiv.org/abs/2408.07663v1", "date": "2024-08-14", "relevancy": 1.9865, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.498}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4959}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4951}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Alignment-Enhanced%20Decoding%3ADefending%20via%20Token-Level%20Adaptive%20Refining%0A%20%20of%20Probability%20Distributions&body=Title%3A%20Alignment-Enhanced%20Decoding%3ADefending%20via%20Token-Level%20Adaptive%20Refining%0A%20%20of%20Probability%20Distributions%0AAuthor%3A%20Quan%20Liu%20and%20Zhenhong%20Zhou%20and%20Longzhu%20He%20and%20Yi%20Liu%20and%20Wei%20Zhang%20and%20Sen%20Su%0AAbstract%3A%20%20%20Large%20language%20models%20are%20susceptible%20to%20jailbreak%20attacks%2C%20which%20can%20result%0Ain%20the%20generation%20of%20harmful%20content.%20While%20prior%20defenses%20mitigate%20these%20risks%0Aby%20perturbing%20or%20inspecting%20inputs%2C%20they%20ignore%20competing%20objectives%2C%20the%0Aunderlying%20cause%20of%20alignment%20failures.%20In%20this%20paper%2C%20we%20propose%0AAlignment-Enhanced%20Decoding%20%28AED%29%2C%20a%20novel%20defense%20that%20employs%20adaptive%0Adecoding%20to%20address%20the%20root%20causes%20of%20jailbreak%20issues.%20We%20first%20define%20the%0ACompetitive%20Index%20to%20quantify%20alignment%20failures%20and%20utilize%20feedback%20from%0Aself-evaluation%20to%20compute%20post-alignment%20logits.%20Then%2C%20AED%20adaptively%20combines%0AAED%20and%20post-alignment%20logits%20with%20the%20original%20logits%20to%20obtain%20harmless%20and%0Ahelpful%20distributions.%20Consequently%2C%20our%20method%20enhances%20safety%20alignment%20while%0Amaintaining%20helpfulness.%20We%20conduct%20experiments%20across%20five%20models%20and%20four%0Acommon%20jailbreaks%2C%20with%20the%20results%20validating%20the%20effectiveness%20of%20our%0Aapproach.%20Code%20is%20available%20at%20https%3A//github.com/GIGABaozi/AED.git.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07663v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAlignment-Enhanced%2520Decoding%253ADefending%2520via%2520Token-Level%2520Adaptive%2520Refining%250A%2520%2520of%2520Probability%2520Distributions%26entry.906535625%3DQuan%2520Liu%2520and%2520Zhenhong%2520Zhou%2520and%2520Longzhu%2520He%2520and%2520Yi%2520Liu%2520and%2520Wei%2520Zhang%2520and%2520Sen%2520Su%26entry.1292438233%3D%2520%2520Large%2520language%2520models%2520are%2520susceptible%2520to%2520jailbreak%2520attacks%252C%2520which%2520can%2520result%250Ain%2520the%2520generation%2520of%2520harmful%2520content.%2520While%2520prior%2520defenses%2520mitigate%2520these%2520risks%250Aby%2520perturbing%2520or%2520inspecting%2520inputs%252C%2520they%2520ignore%2520competing%2520objectives%252C%2520the%250Aunderlying%2520cause%2520of%2520alignment%2520failures.%2520In%2520this%2520paper%252C%2520we%2520propose%250AAlignment-Enhanced%2520Decoding%2520%2528AED%2529%252C%2520a%2520novel%2520defense%2520that%2520employs%2520adaptive%250Adecoding%2520to%2520address%2520the%2520root%2520causes%2520of%2520jailbreak%2520issues.%2520We%2520first%2520define%2520the%250ACompetitive%2520Index%2520to%2520quantify%2520alignment%2520failures%2520and%2520utilize%2520feedback%2520from%250Aself-evaluation%2520to%2520compute%2520post-alignment%2520logits.%2520Then%252C%2520AED%2520adaptively%2520combines%250AAED%2520and%2520post-alignment%2520logits%2520with%2520the%2520original%2520logits%2520to%2520obtain%2520harmless%2520and%250Ahelpful%2520distributions.%2520Consequently%252C%2520our%2520method%2520enhances%2520safety%2520alignment%2520while%250Amaintaining%2520helpfulness.%2520We%2520conduct%2520experiments%2520across%2520five%2520models%2520and%2520four%250Acommon%2520jailbreaks%252C%2520with%2520the%2520results%2520validating%2520the%2520effectiveness%2520of%2520our%250Aapproach.%2520Code%2520is%2520available%2520at%2520https%253A//github.com/GIGABaozi/AED.git.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07663v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Alignment-Enhanced%20Decoding%3ADefending%20via%20Token-Level%20Adaptive%20Refining%0A%20%20of%20Probability%20Distributions&entry.906535625=Quan%20Liu%20and%20Zhenhong%20Zhou%20and%20Longzhu%20He%20and%20Yi%20Liu%20and%20Wei%20Zhang%20and%20Sen%20Su&entry.1292438233=%20%20Large%20language%20models%20are%20susceptible%20to%20jailbreak%20attacks%2C%20which%20can%20result%0Ain%20the%20generation%20of%20harmful%20content.%20While%20prior%20defenses%20mitigate%20these%20risks%0Aby%20perturbing%20or%20inspecting%20inputs%2C%20they%20ignore%20competing%20objectives%2C%20the%0Aunderlying%20cause%20of%20alignment%20failures.%20In%20this%20paper%2C%20we%20propose%0AAlignment-Enhanced%20Decoding%20%28AED%29%2C%20a%20novel%20defense%20that%20employs%20adaptive%0Adecoding%20to%20address%20the%20root%20causes%20of%20jailbreak%20issues.%20We%20first%20define%20the%0ACompetitive%20Index%20to%20quantify%20alignment%20failures%20and%20utilize%20feedback%20from%0Aself-evaluation%20to%20compute%20post-alignment%20logits.%20Then%2C%20AED%20adaptively%20combines%0AAED%20and%20post-alignment%20logits%20with%20the%20original%20logits%20to%20obtain%20harmless%20and%0Ahelpful%20distributions.%20Consequently%2C%20our%20method%20enhances%20safety%20alignment%20while%0Amaintaining%20helpfulness.%20We%20conduct%20experiments%20across%20five%20models%20and%20four%0Acommon%20jailbreaks%2C%20with%20the%20results%20validating%20the%20effectiveness%20of%20our%0Aapproach.%20Code%20is%20available%20at%20https%3A//github.com/GIGABaozi/AED.git.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07663v1&entry.124074799=Read"},
{"title": "Whitening Consistently Improves Self-Supervised Learning", "author": "Andr\u00e1s Kalapos and B\u00e1lint Gyires-T\u00f3th", "abstract": "  Self-supervised learning (SSL) has been shown to be a powerful approach for\nlearning visual representations. In this study, we propose incorporating ZCA\nwhitening as the final layer of the encoder in self-supervised learning to\nenhance the quality of learned features by normalizing and decorrelating them.\nAlthough whitening has been utilized in SSL in previous works, its potential to\nuniversally improve any SSL model has not been explored. We demonstrate that\nadding whitening as the last layer of SSL pretrained encoders is independent of\nthe self-supervised learning method and encoder architecture, thus it improves\nperformance for a wide range of SSL methods across multiple encoder\narchitectures and datasets. Our experiments show that whitening is capable of\nimproving linear and k-NN probing accuracy by 1-5%. Additionally, we propose\nmetrics that allow for a comprehensive analysis of the learned features,\nprovide insights into the quality of the representations and help identify\ncollapse patterns.\n", "link": "http://arxiv.org/abs/2408.07519v1", "date": "2024-08-14", "relevancy": 1.9812, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.517}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4874}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4607}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Whitening%20Consistently%20Improves%20Self-Supervised%20Learning&body=Title%3A%20Whitening%20Consistently%20Improves%20Self-Supervised%20Learning%0AAuthor%3A%20Andr%C3%A1s%20Kalapos%20and%20B%C3%A1lint%20Gyires-T%C3%B3th%0AAbstract%3A%20%20%20Self-supervised%20learning%20%28SSL%29%20has%20been%20shown%20to%20be%20a%20powerful%20approach%20for%0Alearning%20visual%20representations.%20In%20this%20study%2C%20we%20propose%20incorporating%20ZCA%0Awhitening%20as%20the%20final%20layer%20of%20the%20encoder%20in%20self-supervised%20learning%20to%0Aenhance%20the%20quality%20of%20learned%20features%20by%20normalizing%20and%20decorrelating%20them.%0AAlthough%20whitening%20has%20been%20utilized%20in%20SSL%20in%20previous%20works%2C%20its%20potential%20to%0Auniversally%20improve%20any%20SSL%20model%20has%20not%20been%20explored.%20We%20demonstrate%20that%0Aadding%20whitening%20as%20the%20last%20layer%20of%20SSL%20pretrained%20encoders%20is%20independent%20of%0Athe%20self-supervised%20learning%20method%20and%20encoder%20architecture%2C%20thus%20it%20improves%0Aperformance%20for%20a%20wide%20range%20of%20SSL%20methods%20across%20multiple%20encoder%0Aarchitectures%20and%20datasets.%20Our%20experiments%20show%20that%20whitening%20is%20capable%20of%0Aimproving%20linear%20and%20k-NN%20probing%20accuracy%20by%201-5%25.%20Additionally%2C%20we%20propose%0Ametrics%20that%20allow%20for%20a%20comprehensive%20analysis%20of%20the%20learned%20features%2C%0Aprovide%20insights%20into%20the%20quality%20of%20the%20representations%20and%20help%20identify%0Acollapse%20patterns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07519v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWhitening%2520Consistently%2520Improves%2520Self-Supervised%2520Learning%26entry.906535625%3DAndr%25C3%25A1s%2520Kalapos%2520and%2520B%25C3%25A1lint%2520Gyires-T%25C3%25B3th%26entry.1292438233%3D%2520%2520Self-supervised%2520learning%2520%2528SSL%2529%2520has%2520been%2520shown%2520to%2520be%2520a%2520powerful%2520approach%2520for%250Alearning%2520visual%2520representations.%2520In%2520this%2520study%252C%2520we%2520propose%2520incorporating%2520ZCA%250Awhitening%2520as%2520the%2520final%2520layer%2520of%2520the%2520encoder%2520in%2520self-supervised%2520learning%2520to%250Aenhance%2520the%2520quality%2520of%2520learned%2520features%2520by%2520normalizing%2520and%2520decorrelating%2520them.%250AAlthough%2520whitening%2520has%2520been%2520utilized%2520in%2520SSL%2520in%2520previous%2520works%252C%2520its%2520potential%2520to%250Auniversally%2520improve%2520any%2520SSL%2520model%2520has%2520not%2520been%2520explored.%2520We%2520demonstrate%2520that%250Aadding%2520whitening%2520as%2520the%2520last%2520layer%2520of%2520SSL%2520pretrained%2520encoders%2520is%2520independent%2520of%250Athe%2520self-supervised%2520learning%2520method%2520and%2520encoder%2520architecture%252C%2520thus%2520it%2520improves%250Aperformance%2520for%2520a%2520wide%2520range%2520of%2520SSL%2520methods%2520across%2520multiple%2520encoder%250Aarchitectures%2520and%2520datasets.%2520Our%2520experiments%2520show%2520that%2520whitening%2520is%2520capable%2520of%250Aimproving%2520linear%2520and%2520k-NN%2520probing%2520accuracy%2520by%25201-5%2525.%2520Additionally%252C%2520we%2520propose%250Ametrics%2520that%2520allow%2520for%2520a%2520comprehensive%2520analysis%2520of%2520the%2520learned%2520features%252C%250Aprovide%2520insights%2520into%2520the%2520quality%2520of%2520the%2520representations%2520and%2520help%2520identify%250Acollapse%2520patterns.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07519v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Whitening%20Consistently%20Improves%20Self-Supervised%20Learning&entry.906535625=Andr%C3%A1s%20Kalapos%20and%20B%C3%A1lint%20Gyires-T%C3%B3th&entry.1292438233=%20%20Self-supervised%20learning%20%28SSL%29%20has%20been%20shown%20to%20be%20a%20powerful%20approach%20for%0Alearning%20visual%20representations.%20In%20this%20study%2C%20we%20propose%20incorporating%20ZCA%0Awhitening%20as%20the%20final%20layer%20of%20the%20encoder%20in%20self-supervised%20learning%20to%0Aenhance%20the%20quality%20of%20learned%20features%20by%20normalizing%20and%20decorrelating%20them.%0AAlthough%20whitening%20has%20been%20utilized%20in%20SSL%20in%20previous%20works%2C%20its%20potential%20to%0Auniversally%20improve%20any%20SSL%20model%20has%20not%20been%20explored.%20We%20demonstrate%20that%0Aadding%20whitening%20as%20the%20last%20layer%20of%20SSL%20pretrained%20encoders%20is%20independent%20of%0Athe%20self-supervised%20learning%20method%20and%20encoder%20architecture%2C%20thus%20it%20improves%0Aperformance%20for%20a%20wide%20range%20of%20SSL%20methods%20across%20multiple%20encoder%0Aarchitectures%20and%20datasets.%20Our%20experiments%20show%20that%20whitening%20is%20capable%20of%0Aimproving%20linear%20and%20k-NN%20probing%20accuracy%20by%201-5%25.%20Additionally%2C%20we%20propose%0Ametrics%20that%20allow%20for%20a%20comprehensive%20analysis%20of%20the%20learned%20features%2C%0Aprovide%20insights%20into%20the%20quality%20of%20the%20representations%20and%20help%20identify%0Acollapse%20patterns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07519v1&entry.124074799=Read"},
{"title": "Amuro & Char: Analyzing the Relationship between Pre-Training and\n  Fine-Tuning of Large Language Models", "author": "Kaiser Sun and Mark Dredze", "abstract": "  The development of large language models leads to the formation of a\npre-train-then-align paradigm, in which the model is typically pre-trained on a\nlarge text corpus and undergoes a tuning stage to align the model with human\npreference or downstream tasks. In this work, we investigate the relationship\nbetween pre-training and fine-tuning by fine-tuning multiple intermediate\npre-trained model checkpoints. Our results on 18 datasets suggest that i)\ncontinual pre-training improves the model in a latent way that unveils after\nfine-tuning; ii) with extra fine-tuning, the datasets that the model does not\ndemonstrate capability gain much more than those that the model performs well\nduring the pre-training stage; iii) although model benefits significantly\nthrough supervised fine-tuning, it may forget previously known domain knowledge\nand the tasks that are not seen during fine-tuning; iv) the model resembles\nhigh sensitivity to evaluation prompts after supervised fine-tuning, but this\nsensitivity can be alleviated by more pre-training.\n", "link": "http://arxiv.org/abs/2408.06663v2", "date": "2024-08-14", "relevancy": 1.9693, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5044}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4946}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4565}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Amuro%20%26%20Char%3A%20Analyzing%20the%20Relationship%20between%20Pre-Training%20and%0A%20%20Fine-Tuning%20of%20Large%20Language%20Models&body=Title%3A%20Amuro%20%26%20Char%3A%20Analyzing%20the%20Relationship%20between%20Pre-Training%20and%0A%20%20Fine-Tuning%20of%20Large%20Language%20Models%0AAuthor%3A%20Kaiser%20Sun%20and%20Mark%20Dredze%0AAbstract%3A%20%20%20The%20development%20of%20large%20language%20models%20leads%20to%20the%20formation%20of%20a%0Apre-train-then-align%20paradigm%2C%20in%20which%20the%20model%20is%20typically%20pre-trained%20on%20a%0Alarge%20text%20corpus%20and%20undergoes%20a%20tuning%20stage%20to%20align%20the%20model%20with%20human%0Apreference%20or%20downstream%20tasks.%20In%20this%20work%2C%20we%20investigate%20the%20relationship%0Abetween%20pre-training%20and%20fine-tuning%20by%20fine-tuning%20multiple%20intermediate%0Apre-trained%20model%20checkpoints.%20Our%20results%20on%2018%20datasets%20suggest%20that%20i%29%0Acontinual%20pre-training%20improves%20the%20model%20in%20a%20latent%20way%20that%20unveils%20after%0Afine-tuning%3B%20ii%29%20with%20extra%20fine-tuning%2C%20the%20datasets%20that%20the%20model%20does%20not%0Ademonstrate%20capability%20gain%20much%20more%20than%20those%20that%20the%20model%20performs%20well%0Aduring%20the%20pre-training%20stage%3B%20iii%29%20although%20model%20benefits%20significantly%0Athrough%20supervised%20fine-tuning%2C%20it%20may%20forget%20previously%20known%20domain%20knowledge%0Aand%20the%20tasks%20that%20are%20not%20seen%20during%20fine-tuning%3B%20iv%29%20the%20model%20resembles%0Ahigh%20sensitivity%20to%20evaluation%20prompts%20after%20supervised%20fine-tuning%2C%20but%20this%0Asensitivity%20can%20be%20alleviated%20by%20more%20pre-training.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06663v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAmuro%2520%2526%2520Char%253A%2520Analyzing%2520the%2520Relationship%2520between%2520Pre-Training%2520and%250A%2520%2520Fine-Tuning%2520of%2520Large%2520Language%2520Models%26entry.906535625%3DKaiser%2520Sun%2520and%2520Mark%2520Dredze%26entry.1292438233%3D%2520%2520The%2520development%2520of%2520large%2520language%2520models%2520leads%2520to%2520the%2520formation%2520of%2520a%250Apre-train-then-align%2520paradigm%252C%2520in%2520which%2520the%2520model%2520is%2520typically%2520pre-trained%2520on%2520a%250Alarge%2520text%2520corpus%2520and%2520undergoes%2520a%2520tuning%2520stage%2520to%2520align%2520the%2520model%2520with%2520human%250Apreference%2520or%2520downstream%2520tasks.%2520In%2520this%2520work%252C%2520we%2520investigate%2520the%2520relationship%250Abetween%2520pre-training%2520and%2520fine-tuning%2520by%2520fine-tuning%2520multiple%2520intermediate%250Apre-trained%2520model%2520checkpoints.%2520Our%2520results%2520on%252018%2520datasets%2520suggest%2520that%2520i%2529%250Acontinual%2520pre-training%2520improves%2520the%2520model%2520in%2520a%2520latent%2520way%2520that%2520unveils%2520after%250Afine-tuning%253B%2520ii%2529%2520with%2520extra%2520fine-tuning%252C%2520the%2520datasets%2520that%2520the%2520model%2520does%2520not%250Ademonstrate%2520capability%2520gain%2520much%2520more%2520than%2520those%2520that%2520the%2520model%2520performs%2520well%250Aduring%2520the%2520pre-training%2520stage%253B%2520iii%2529%2520although%2520model%2520benefits%2520significantly%250Athrough%2520supervised%2520fine-tuning%252C%2520it%2520may%2520forget%2520previously%2520known%2520domain%2520knowledge%250Aand%2520the%2520tasks%2520that%2520are%2520not%2520seen%2520during%2520fine-tuning%253B%2520iv%2529%2520the%2520model%2520resembles%250Ahigh%2520sensitivity%2520to%2520evaluation%2520prompts%2520after%2520supervised%2520fine-tuning%252C%2520but%2520this%250Asensitivity%2520can%2520be%2520alleviated%2520by%2520more%2520pre-training.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06663v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Amuro%20%26%20Char%3A%20Analyzing%20the%20Relationship%20between%20Pre-Training%20and%0A%20%20Fine-Tuning%20of%20Large%20Language%20Models&entry.906535625=Kaiser%20Sun%20and%20Mark%20Dredze&entry.1292438233=%20%20The%20development%20of%20large%20language%20models%20leads%20to%20the%20formation%20of%20a%0Apre-train-then-align%20paradigm%2C%20in%20which%20the%20model%20is%20typically%20pre-trained%20on%20a%0Alarge%20text%20corpus%20and%20undergoes%20a%20tuning%20stage%20to%20align%20the%20model%20with%20human%0Apreference%20or%20downstream%20tasks.%20In%20this%20work%2C%20we%20investigate%20the%20relationship%0Abetween%20pre-training%20and%20fine-tuning%20by%20fine-tuning%20multiple%20intermediate%0Apre-trained%20model%20checkpoints.%20Our%20results%20on%2018%20datasets%20suggest%20that%20i%29%0Acontinual%20pre-training%20improves%20the%20model%20in%20a%20latent%20way%20that%20unveils%20after%0Afine-tuning%3B%20ii%29%20with%20extra%20fine-tuning%2C%20the%20datasets%20that%20the%20model%20does%20not%0Ademonstrate%20capability%20gain%20much%20more%20than%20those%20that%20the%20model%20performs%20well%0Aduring%20the%20pre-training%20stage%3B%20iii%29%20although%20model%20benefits%20significantly%0Athrough%20supervised%20fine-tuning%2C%20it%20may%20forget%20previously%20known%20domain%20knowledge%0Aand%20the%20tasks%20that%20are%20not%20seen%20during%20fine-tuning%3B%20iv%29%20the%20model%20resembles%0Ahigh%20sensitivity%20to%20evaluation%20prompts%20after%20supervised%20fine-tuning%2C%20but%20this%0Asensitivity%20can%20be%20alleviated%20by%20more%20pre-training.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06663v2&entry.124074799=Read"},
{"title": "RSD-DOG : A New Image Descriptor based on Second Order Derivatives", "author": "Darshan Venkatrayappa and Philippe Montesinos and Daniel Diep and Baptiste Magnier", "abstract": "  This paper introduces the new and powerful image patch descriptor based on\nsecond order image statistics/derivatives. Here, the image patch is treated as\na 3D surface with intensity being the 3rd dimension. The considered 3D surface\nhas a rich set of second order features/statistics such as ridges, valleys,\ncliffs and so on, that can be easily captured by using the difference of\nrotating semi Gaussian filters. The originality of this method is based on\nsuccessfully combining the response of the directional filters with that of the\nDifference of Gaussian (DOG) approach. The obtained descriptor shows a good\ndiscriminative power when dealing with the variations in illumination, scale,\nrotation, blur, viewpoint and compression. The experiments on image matching,\ndemonstrates the advantage of the obtained descriptor when compared to its\nfirst order counterparts such as SIFT, DAISY, GLOH, GIST and LIDRIC.\n", "link": "http://arxiv.org/abs/2408.07687v1", "date": "2024-08-14", "relevancy": 1.96, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5192}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4903}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20RSD-DOG%20%3A%20A%20New%20Image%20Descriptor%20based%20on%20Second%20Order%20Derivatives&body=Title%3A%20RSD-DOG%20%3A%20A%20New%20Image%20Descriptor%20based%20on%20Second%20Order%20Derivatives%0AAuthor%3A%20Darshan%20Venkatrayappa%20and%20Philippe%20Montesinos%20and%20Daniel%20Diep%20and%20Baptiste%20Magnier%0AAbstract%3A%20%20%20This%20paper%20introduces%20the%20new%20and%20powerful%20image%20patch%20descriptor%20based%20on%0Asecond%20order%20image%20statistics/derivatives.%20Here%2C%20the%20image%20patch%20is%20treated%20as%0Aa%203D%20surface%20with%20intensity%20being%20the%203rd%20dimension.%20The%20considered%203D%20surface%0Ahas%20a%20rich%20set%20of%20second%20order%20features/statistics%20such%20as%20ridges%2C%20valleys%2C%0Acliffs%20and%20so%20on%2C%20that%20can%20be%20easily%20captured%20by%20using%20the%20difference%20of%0Arotating%20semi%20Gaussian%20filters.%20The%20originality%20of%20this%20method%20is%20based%20on%0Asuccessfully%20combining%20the%20response%20of%20the%20directional%20filters%20with%20that%20of%20the%0ADifference%20of%20Gaussian%20%28DOG%29%20approach.%20The%20obtained%20descriptor%20shows%20a%20good%0Adiscriminative%20power%20when%20dealing%20with%20the%20variations%20in%20illumination%2C%20scale%2C%0Arotation%2C%20blur%2C%20viewpoint%20and%20compression.%20The%20experiments%20on%20image%20matching%2C%0Ademonstrates%20the%20advantage%20of%20the%20obtained%20descriptor%20when%20compared%20to%20its%0Afirst%20order%20counterparts%20such%20as%20SIFT%2C%20DAISY%2C%20GLOH%2C%20GIST%20and%20LIDRIC.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07687v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRSD-DOG%2520%253A%2520A%2520New%2520Image%2520Descriptor%2520based%2520on%2520Second%2520Order%2520Derivatives%26entry.906535625%3DDarshan%2520Venkatrayappa%2520and%2520Philippe%2520Montesinos%2520and%2520Daniel%2520Diep%2520and%2520Baptiste%2520Magnier%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520the%2520new%2520and%2520powerful%2520image%2520patch%2520descriptor%2520based%2520on%250Asecond%2520order%2520image%2520statistics/derivatives.%2520Here%252C%2520the%2520image%2520patch%2520is%2520treated%2520as%250Aa%25203D%2520surface%2520with%2520intensity%2520being%2520the%25203rd%2520dimension.%2520The%2520considered%25203D%2520surface%250Ahas%2520a%2520rich%2520set%2520of%2520second%2520order%2520features/statistics%2520such%2520as%2520ridges%252C%2520valleys%252C%250Acliffs%2520and%2520so%2520on%252C%2520that%2520can%2520be%2520easily%2520captured%2520by%2520using%2520the%2520difference%2520of%250Arotating%2520semi%2520Gaussian%2520filters.%2520The%2520originality%2520of%2520this%2520method%2520is%2520based%2520on%250Asuccessfully%2520combining%2520the%2520response%2520of%2520the%2520directional%2520filters%2520with%2520that%2520of%2520the%250ADifference%2520of%2520Gaussian%2520%2528DOG%2529%2520approach.%2520The%2520obtained%2520descriptor%2520shows%2520a%2520good%250Adiscriminative%2520power%2520when%2520dealing%2520with%2520the%2520variations%2520in%2520illumination%252C%2520scale%252C%250Arotation%252C%2520blur%252C%2520viewpoint%2520and%2520compression.%2520The%2520experiments%2520on%2520image%2520matching%252C%250Ademonstrates%2520the%2520advantage%2520of%2520the%2520obtained%2520descriptor%2520when%2520compared%2520to%2520its%250Afirst%2520order%2520counterparts%2520such%2520as%2520SIFT%252C%2520DAISY%252C%2520GLOH%252C%2520GIST%2520and%2520LIDRIC.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07687v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=RSD-DOG%20%3A%20A%20New%20Image%20Descriptor%20based%20on%20Second%20Order%20Derivatives&entry.906535625=Darshan%20Venkatrayappa%20and%20Philippe%20Montesinos%20and%20Daniel%20Diep%20and%20Baptiste%20Magnier&entry.1292438233=%20%20This%20paper%20introduces%20the%20new%20and%20powerful%20image%20patch%20descriptor%20based%20on%0Asecond%20order%20image%20statistics/derivatives.%20Here%2C%20the%20image%20patch%20is%20treated%20as%0Aa%203D%20surface%20with%20intensity%20being%20the%203rd%20dimension.%20The%20considered%203D%20surface%0Ahas%20a%20rich%20set%20of%20second%20order%20features/statistics%20such%20as%20ridges%2C%20valleys%2C%0Acliffs%20and%20so%20on%2C%20that%20can%20be%20easily%20captured%20by%20using%20the%20difference%20of%0Arotating%20semi%20Gaussian%20filters.%20The%20originality%20of%20this%20method%20is%20based%20on%0Asuccessfully%20combining%20the%20response%20of%20the%20directional%20filters%20with%20that%20of%20the%0ADifference%20of%20Gaussian%20%28DOG%29%20approach.%20The%20obtained%20descriptor%20shows%20a%20good%0Adiscriminative%20power%20when%20dealing%20with%20the%20variations%20in%20illumination%2C%20scale%2C%0Arotation%2C%20blur%2C%20viewpoint%20and%20compression.%20The%20experiments%20on%20image%20matching%2C%0Ademonstrates%20the%20advantage%20of%20the%20obtained%20descriptor%20when%20compared%20to%20its%0Afirst%20order%20counterparts%20such%20as%20SIFT%2C%20DAISY%2C%20GLOH%2C%20GIST%20and%20LIDRIC.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07687v1&entry.124074799=Read"},
{"title": "Detecting Near-Duplicate Face Images", "author": "Sudipta Banerjee and Arun Ross", "abstract": "  Near-duplicate images are often generated when applying repeated photometric\nand geometric transformations that produce imperceptible variants of the\noriginal image. Consequently, a deluge of near-duplicates can be circulated\nonline posing copyright infringement concerns. The concerns are more severe\nwhen biometric data is altered through such nuanced transformations. In this\nwork, we address the challenge of near-duplicate detection in face images by,\nfirstly, identifying the original image from a set of near-duplicates and,\nsecondly, deducing the relationship between the original image and the\nnear-duplicates. We construct a tree-like structure, called an Image Phylogeny\nTree (IPT) using a graph-theoretic approach to estimate the relationship, i.e.,\ndetermine the sequence in which they have been generated. We further extend our\nmethod to create an ensemble of IPTs known as Image Phylogeny Forests (IPFs).\nWe rigorously evaluate our method to demonstrate robustness across other\nmodalities, unseen transformations by latest generative models and IPT\nconfigurations, thereby significantly advancing the state-of-the-art\nperformance by 42% on IPF reconstruction accuracy.\n", "link": "http://arxiv.org/abs/2408.07689v1", "date": "2024-08-14", "relevancy": 1.9519, "topK": [{"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.4962}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4883}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4843}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Detecting%20Near-Duplicate%20Face%20Images&body=Title%3A%20Detecting%20Near-Duplicate%20Face%20Images%0AAuthor%3A%20Sudipta%20Banerjee%20and%20Arun%20Ross%0AAbstract%3A%20%20%20Near-duplicate%20images%20are%20often%20generated%20when%20applying%20repeated%20photometric%0Aand%20geometric%20transformations%20that%20produce%20imperceptible%20variants%20of%20the%0Aoriginal%20image.%20Consequently%2C%20a%20deluge%20of%20near-duplicates%20can%20be%20circulated%0Aonline%20posing%20copyright%20infringement%20concerns.%20The%20concerns%20are%20more%20severe%0Awhen%20biometric%20data%20is%20altered%20through%20such%20nuanced%20transformations.%20In%20this%0Awork%2C%20we%20address%20the%20challenge%20of%20near-duplicate%20detection%20in%20face%20images%20by%2C%0Afirstly%2C%20identifying%20the%20original%20image%20from%20a%20set%20of%20near-duplicates%20and%2C%0Asecondly%2C%20deducing%20the%20relationship%20between%20the%20original%20image%20and%20the%0Anear-duplicates.%20We%20construct%20a%20tree-like%20structure%2C%20called%20an%20Image%20Phylogeny%0ATree%20%28IPT%29%20using%20a%20graph-theoretic%20approach%20to%20estimate%20the%20relationship%2C%20i.e.%2C%0Adetermine%20the%20sequence%20in%20which%20they%20have%20been%20generated.%20We%20further%20extend%20our%0Amethod%20to%20create%20an%20ensemble%20of%20IPTs%20known%20as%20Image%20Phylogeny%20Forests%20%28IPFs%29.%0AWe%20rigorously%20evaluate%20our%20method%20to%20demonstrate%20robustness%20across%20other%0Amodalities%2C%20unseen%20transformations%20by%20latest%20generative%20models%20and%20IPT%0Aconfigurations%2C%20thereby%20significantly%20advancing%20the%20state-of-the-art%0Aperformance%20by%2042%25%20on%20IPF%20reconstruction%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07689v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDetecting%2520Near-Duplicate%2520Face%2520Images%26entry.906535625%3DSudipta%2520Banerjee%2520and%2520Arun%2520Ross%26entry.1292438233%3D%2520%2520Near-duplicate%2520images%2520are%2520often%2520generated%2520when%2520applying%2520repeated%2520photometric%250Aand%2520geometric%2520transformations%2520that%2520produce%2520imperceptible%2520variants%2520of%2520the%250Aoriginal%2520image.%2520Consequently%252C%2520a%2520deluge%2520of%2520near-duplicates%2520can%2520be%2520circulated%250Aonline%2520posing%2520copyright%2520infringement%2520concerns.%2520The%2520concerns%2520are%2520more%2520severe%250Awhen%2520biometric%2520data%2520is%2520altered%2520through%2520such%2520nuanced%2520transformations.%2520In%2520this%250Awork%252C%2520we%2520address%2520the%2520challenge%2520of%2520near-duplicate%2520detection%2520in%2520face%2520images%2520by%252C%250Afirstly%252C%2520identifying%2520the%2520original%2520image%2520from%2520a%2520set%2520of%2520near-duplicates%2520and%252C%250Asecondly%252C%2520deducing%2520the%2520relationship%2520between%2520the%2520original%2520image%2520and%2520the%250Anear-duplicates.%2520We%2520construct%2520a%2520tree-like%2520structure%252C%2520called%2520an%2520Image%2520Phylogeny%250ATree%2520%2528IPT%2529%2520using%2520a%2520graph-theoretic%2520approach%2520to%2520estimate%2520the%2520relationship%252C%2520i.e.%252C%250Adetermine%2520the%2520sequence%2520in%2520which%2520they%2520have%2520been%2520generated.%2520We%2520further%2520extend%2520our%250Amethod%2520to%2520create%2520an%2520ensemble%2520of%2520IPTs%2520known%2520as%2520Image%2520Phylogeny%2520Forests%2520%2528IPFs%2529.%250AWe%2520rigorously%2520evaluate%2520our%2520method%2520to%2520demonstrate%2520robustness%2520across%2520other%250Amodalities%252C%2520unseen%2520transformations%2520by%2520latest%2520generative%2520models%2520and%2520IPT%250Aconfigurations%252C%2520thereby%2520significantly%2520advancing%2520the%2520state-of-the-art%250Aperformance%2520by%252042%2525%2520on%2520IPF%2520reconstruction%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07689v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Detecting%20Near-Duplicate%20Face%20Images&entry.906535625=Sudipta%20Banerjee%20and%20Arun%20Ross&entry.1292438233=%20%20Near-duplicate%20images%20are%20often%20generated%20when%20applying%20repeated%20photometric%0Aand%20geometric%20transformations%20that%20produce%20imperceptible%20variants%20of%20the%0Aoriginal%20image.%20Consequently%2C%20a%20deluge%20of%20near-duplicates%20can%20be%20circulated%0Aonline%20posing%20copyright%20infringement%20concerns.%20The%20concerns%20are%20more%20severe%0Awhen%20biometric%20data%20is%20altered%20through%20such%20nuanced%20transformations.%20In%20this%0Awork%2C%20we%20address%20the%20challenge%20of%20near-duplicate%20detection%20in%20face%20images%20by%2C%0Afirstly%2C%20identifying%20the%20original%20image%20from%20a%20set%20of%20near-duplicates%20and%2C%0Asecondly%2C%20deducing%20the%20relationship%20between%20the%20original%20image%20and%20the%0Anear-duplicates.%20We%20construct%20a%20tree-like%20structure%2C%20called%20an%20Image%20Phylogeny%0ATree%20%28IPT%29%20using%20a%20graph-theoretic%20approach%20to%20estimate%20the%20relationship%2C%20i.e.%2C%0Adetermine%20the%20sequence%20in%20which%20they%20have%20been%20generated.%20We%20further%20extend%20our%0Amethod%20to%20create%20an%20ensemble%20of%20IPTs%20known%20as%20Image%20Phylogeny%20Forests%20%28IPFs%29.%0AWe%20rigorously%20evaluate%20our%20method%20to%20demonstrate%20robustness%20across%20other%0Amodalities%2C%20unseen%20transformations%20by%20latest%20generative%20models%20and%20IPT%0Aconfigurations%2C%20thereby%20significantly%20advancing%20the%20state-of-the-art%0Aperformance%20by%2042%25%20on%20IPF%20reconstruction%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07689v1&entry.124074799=Read"},
{"title": "Latent Anomaly Detection Through Density Matrices", "author": "Joseph Gallego-Mejia and Oscar Bustos-Brinez and Fabio A. Gonz\u00e1lez", "abstract": "  This paper introduces a novel anomaly detection framework that combines the\nrobust statistical principles of density-estimation-based anomaly detection\nmethods with the representation-learning capabilities of deep learning models.\nThe method originated from this framework is presented in two different\nversions: a shallow approach employing a density-estimation model based on\nadaptive Fourier features and density matrices, and a deep approach that\nintegrates an autoencoder to learn a low-dimensional representation of the\ndata. By estimating the density of new samples, both methods are able to find\nnormality scores. The methods can be seamlessly integrated into an end-to-end\narchitecture and optimized using gradient-based optimization techniques. To\nevaluate their performance, extensive experiments were conducted on various\nbenchmark datasets. The results demonstrate that both versions of the method\ncan achieve comparable or superior performance when compared to other\nstate-of-the-art methods. Notably, the shallow approach performs better on\ndatasets with fewer dimensions, while the autoencoder-based approach shows\nimproved performance on datasets with higher dimensions.\n", "link": "http://arxiv.org/abs/2408.07623v1", "date": "2024-08-14", "relevancy": 1.9516, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4932}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4871}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4865}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Latent%20Anomaly%20Detection%20Through%20Density%20Matrices&body=Title%3A%20Latent%20Anomaly%20Detection%20Through%20Density%20Matrices%0AAuthor%3A%20Joseph%20Gallego-Mejia%20and%20Oscar%20Bustos-Brinez%20and%20Fabio%20A.%20Gonz%C3%A1lez%0AAbstract%3A%20%20%20This%20paper%20introduces%20a%20novel%20anomaly%20detection%20framework%20that%20combines%20the%0Arobust%20statistical%20principles%20of%20density-estimation-based%20anomaly%20detection%0Amethods%20with%20the%20representation-learning%20capabilities%20of%20deep%20learning%20models.%0AThe%20method%20originated%20from%20this%20framework%20is%20presented%20in%20two%20different%0Aversions%3A%20a%20shallow%20approach%20employing%20a%20density-estimation%20model%20based%20on%0Aadaptive%20Fourier%20features%20and%20density%20matrices%2C%20and%20a%20deep%20approach%20that%0Aintegrates%20an%20autoencoder%20to%20learn%20a%20low-dimensional%20representation%20of%20the%0Adata.%20By%20estimating%20the%20density%20of%20new%20samples%2C%20both%20methods%20are%20able%20to%20find%0Anormality%20scores.%20The%20methods%20can%20be%20seamlessly%20integrated%20into%20an%20end-to-end%0Aarchitecture%20and%20optimized%20using%20gradient-based%20optimization%20techniques.%20To%0Aevaluate%20their%20performance%2C%20extensive%20experiments%20were%20conducted%20on%20various%0Abenchmark%20datasets.%20The%20results%20demonstrate%20that%20both%20versions%20of%20the%20method%0Acan%20achieve%20comparable%20or%20superior%20performance%20when%20compared%20to%20other%0Astate-of-the-art%20methods.%20Notably%2C%20the%20shallow%20approach%20performs%20better%20on%0Adatasets%20with%20fewer%20dimensions%2C%20while%20the%20autoencoder-based%20approach%20shows%0Aimproved%20performance%20on%20datasets%20with%20higher%20dimensions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07623v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLatent%2520Anomaly%2520Detection%2520Through%2520Density%2520Matrices%26entry.906535625%3DJoseph%2520Gallego-Mejia%2520and%2520Oscar%2520Bustos-Brinez%2520and%2520Fabio%2520A.%2520Gonz%25C3%25A1lez%26entry.1292438233%3D%2520%2520This%2520paper%2520introduces%2520a%2520novel%2520anomaly%2520detection%2520framework%2520that%2520combines%2520the%250Arobust%2520statistical%2520principles%2520of%2520density-estimation-based%2520anomaly%2520detection%250Amethods%2520with%2520the%2520representation-learning%2520capabilities%2520of%2520deep%2520learning%2520models.%250AThe%2520method%2520originated%2520from%2520this%2520framework%2520is%2520presented%2520in%2520two%2520different%250Aversions%253A%2520a%2520shallow%2520approach%2520employing%2520a%2520density-estimation%2520model%2520based%2520on%250Aadaptive%2520Fourier%2520features%2520and%2520density%2520matrices%252C%2520and%2520a%2520deep%2520approach%2520that%250Aintegrates%2520an%2520autoencoder%2520to%2520learn%2520a%2520low-dimensional%2520representation%2520of%2520the%250Adata.%2520By%2520estimating%2520the%2520density%2520of%2520new%2520samples%252C%2520both%2520methods%2520are%2520able%2520to%2520find%250Anormality%2520scores.%2520The%2520methods%2520can%2520be%2520seamlessly%2520integrated%2520into%2520an%2520end-to-end%250Aarchitecture%2520and%2520optimized%2520using%2520gradient-based%2520optimization%2520techniques.%2520To%250Aevaluate%2520their%2520performance%252C%2520extensive%2520experiments%2520were%2520conducted%2520on%2520various%250Abenchmark%2520datasets.%2520The%2520results%2520demonstrate%2520that%2520both%2520versions%2520of%2520the%2520method%250Acan%2520achieve%2520comparable%2520or%2520superior%2520performance%2520when%2520compared%2520to%2520other%250Astate-of-the-art%2520methods.%2520Notably%252C%2520the%2520shallow%2520approach%2520performs%2520better%2520on%250Adatasets%2520with%2520fewer%2520dimensions%252C%2520while%2520the%2520autoencoder-based%2520approach%2520shows%250Aimproved%2520performance%2520on%2520datasets%2520with%2520higher%2520dimensions.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07623v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Latent%20Anomaly%20Detection%20Through%20Density%20Matrices&entry.906535625=Joseph%20Gallego-Mejia%20and%20Oscar%20Bustos-Brinez%20and%20Fabio%20A.%20Gonz%C3%A1lez&entry.1292438233=%20%20This%20paper%20introduces%20a%20novel%20anomaly%20detection%20framework%20that%20combines%20the%0Arobust%20statistical%20principles%20of%20density-estimation-based%20anomaly%20detection%0Amethods%20with%20the%20representation-learning%20capabilities%20of%20deep%20learning%20models.%0AThe%20method%20originated%20from%20this%20framework%20is%20presented%20in%20two%20different%0Aversions%3A%20a%20shallow%20approach%20employing%20a%20density-estimation%20model%20based%20on%0Aadaptive%20Fourier%20features%20and%20density%20matrices%2C%20and%20a%20deep%20approach%20that%0Aintegrates%20an%20autoencoder%20to%20learn%20a%20low-dimensional%20representation%20of%20the%0Adata.%20By%20estimating%20the%20density%20of%20new%20samples%2C%20both%20methods%20are%20able%20to%20find%0Anormality%20scores.%20The%20methods%20can%20be%20seamlessly%20integrated%20into%20an%20end-to-end%0Aarchitecture%20and%20optimized%20using%20gradient-based%20optimization%20techniques.%20To%0Aevaluate%20their%20performance%2C%20extensive%20experiments%20were%20conducted%20on%20various%0Abenchmark%20datasets.%20The%20results%20demonstrate%20that%20both%20versions%20of%20the%20method%0Acan%20achieve%20comparable%20or%20superior%20performance%20when%20compared%20to%20other%0Astate-of-the-art%20methods.%20Notably%2C%20the%20shallow%20approach%20performs%20better%20on%0Adatasets%20with%20fewer%20dimensions%2C%20while%20the%20autoencoder-based%20approach%20shows%0Aimproved%20performance%20on%20datasets%20with%20higher%20dimensions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07623v1&entry.124074799=Read"},
{"title": "WavLLM: Towards Robust and Adaptive Speech Large Language Model", "author": "Shujie Hu and Long Zhou and Shujie Liu and Sanyuan Chen and Lingwei Meng and Hongkun Hao and Jing Pan and Xunying Liu and Jinyu Li and Sunit Sivasankaran and Linquan Liu and Furu Wei", "abstract": "  The recent advancements in large language models (LLMs) have revolutionized\nthe field of natural language processing, progressively broadening their scope\nto multimodal perception and generation. However, effectively integrating\nlistening capabilities into LLMs poses significant challenges, particularly\nwith respect to generalizing across varied contexts and executing complex\nauditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech\nlarge language model with dual encoders, and a prompt-aware LoRA weight\nadapter, optimized by a two-stage curriculum learning approach. Leveraging dual\nencoders, we decouple different types of speech information, utilizing a\nWhisper encoder to process the semantic content of speech, and a WavLM encoder\nto capture the unique characteristics of the speaker's identity. Within the\ncurriculum learning framework, WavLLM first builds its foundational\ncapabilities by optimizing on mixed elementary single tasks, followed by\nadvanced multi-task training on more complex tasks such as combinations of the\nelementary tasks. To enhance the flexibility and adherence to different tasks\nand instructions, a prompt-aware LoRA weight adapter is introduced in the\nsecond advanced multi-task training stage. We validate the proposed model on\nuniversal speech benchmarks including tasks such as ASR, ST, SV, ER, and also\napply it to specialized datasets like Gaokao English listening comprehension\nset for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments\ndemonstrate that the proposed model achieves state-of-the-art performance\nacross a range of speech tasks on the same model size, exhibiting robust\ngeneralization capabilities in executing complex tasks using CoT approach.\nFurthermore, our model successfully completes Gaokao tasks without specialized\ntraining. The codes, models, audio, and Gaokao evaluation set can be accessed\nat \\url{aka.ms/wavllm}.\n", "link": "http://arxiv.org/abs/2404.00656v2", "date": "2024-08-14", "relevancy": 1.9513, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5009}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4787}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.478}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20WavLLM%3A%20Towards%20Robust%20and%20Adaptive%20Speech%20Large%20Language%20Model&body=Title%3A%20WavLLM%3A%20Towards%20Robust%20and%20Adaptive%20Speech%20Large%20Language%20Model%0AAuthor%3A%20Shujie%20Hu%20and%20Long%20Zhou%20and%20Shujie%20Liu%20and%20Sanyuan%20Chen%20and%20Lingwei%20Meng%20and%20Hongkun%20Hao%20and%20Jing%20Pan%20and%20Xunying%20Liu%20and%20Jinyu%20Li%20and%20Sunit%20Sivasankaran%20and%20Linquan%20Liu%20and%20Furu%20Wei%0AAbstract%3A%20%20%20The%20recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20revolutionized%0Athe%20field%20of%20natural%20language%20processing%2C%20progressively%20broadening%20their%20scope%0Ato%20multimodal%20perception%20and%20generation.%20However%2C%20effectively%20integrating%0Alistening%20capabilities%20into%20LLMs%20poses%20significant%20challenges%2C%20particularly%0Awith%20respect%20to%20generalizing%20across%20varied%20contexts%20and%20executing%20complex%0Aauditory%20tasks.%20In%20this%20work%2C%20we%20introduce%20WavLLM%2C%20a%20robust%20and%20adaptive%20speech%0Alarge%20language%20model%20with%20dual%20encoders%2C%20and%20a%20prompt-aware%20LoRA%20weight%0Aadapter%2C%20optimized%20by%20a%20two-stage%20curriculum%20learning%20approach.%20Leveraging%20dual%0Aencoders%2C%20we%20decouple%20different%20types%20of%20speech%20information%2C%20utilizing%20a%0AWhisper%20encoder%20to%20process%20the%20semantic%20content%20of%20speech%2C%20and%20a%20WavLM%20encoder%0Ato%20capture%20the%20unique%20characteristics%20of%20the%20speaker%27s%20identity.%20Within%20the%0Acurriculum%20learning%20framework%2C%20WavLLM%20first%20builds%20its%20foundational%0Acapabilities%20by%20optimizing%20on%20mixed%20elementary%20single%20tasks%2C%20followed%20by%0Aadvanced%20multi-task%20training%20on%20more%20complex%20tasks%20such%20as%20combinations%20of%20the%0Aelementary%20tasks.%20To%20enhance%20the%20flexibility%20and%20adherence%20to%20different%20tasks%0Aand%20instructions%2C%20a%20prompt-aware%20LoRA%20weight%20adapter%20is%20introduced%20in%20the%0Asecond%20advanced%20multi-task%20training%20stage.%20We%20validate%20the%20proposed%20model%20on%0Auniversal%20speech%20benchmarks%20including%20tasks%20such%20as%20ASR%2C%20ST%2C%20SV%2C%20ER%2C%20and%20also%0Aapply%20it%20to%20specialized%20datasets%20like%20Gaokao%20English%20listening%20comprehension%0Aset%20for%20SQA%2C%20and%20speech%20Chain-of-Thought%20%28CoT%29%20evaluation%20set.%20Experiments%0Ademonstrate%20that%20the%20proposed%20model%20achieves%20state-of-the-art%20performance%0Aacross%20a%20range%20of%20speech%20tasks%20on%20the%20same%20model%20size%2C%20exhibiting%20robust%0Ageneralization%20capabilities%20in%20executing%20complex%20tasks%20using%20CoT%20approach.%0AFurthermore%2C%20our%20model%20successfully%20completes%20Gaokao%20tasks%20without%20specialized%0Atraining.%20The%20codes%2C%20models%2C%20audio%2C%20and%20Gaokao%20evaluation%20set%20can%20be%20accessed%0Aat%20%5Curl%7Baka.ms/wavllm%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2404.00656v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DWavLLM%253A%2520Towards%2520Robust%2520and%2520Adaptive%2520Speech%2520Large%2520Language%2520Model%26entry.906535625%3DShujie%2520Hu%2520and%2520Long%2520Zhou%2520and%2520Shujie%2520Liu%2520and%2520Sanyuan%2520Chen%2520and%2520Lingwei%2520Meng%2520and%2520Hongkun%2520Hao%2520and%2520Jing%2520Pan%2520and%2520Xunying%2520Liu%2520and%2520Jinyu%2520Li%2520and%2520Sunit%2520Sivasankaran%2520and%2520Linquan%2520Liu%2520and%2520Furu%2520Wei%26entry.1292438233%3D%2520%2520The%2520recent%2520advancements%2520in%2520large%2520language%2520models%2520%2528LLMs%2529%2520have%2520revolutionized%250Athe%2520field%2520of%2520natural%2520language%2520processing%252C%2520progressively%2520broadening%2520their%2520scope%250Ato%2520multimodal%2520perception%2520and%2520generation.%2520However%252C%2520effectively%2520integrating%250Alistening%2520capabilities%2520into%2520LLMs%2520poses%2520significant%2520challenges%252C%2520particularly%250Awith%2520respect%2520to%2520generalizing%2520across%2520varied%2520contexts%2520and%2520executing%2520complex%250Aauditory%2520tasks.%2520In%2520this%2520work%252C%2520we%2520introduce%2520WavLLM%252C%2520a%2520robust%2520and%2520adaptive%2520speech%250Alarge%2520language%2520model%2520with%2520dual%2520encoders%252C%2520and%2520a%2520prompt-aware%2520LoRA%2520weight%250Aadapter%252C%2520optimized%2520by%2520a%2520two-stage%2520curriculum%2520learning%2520approach.%2520Leveraging%2520dual%250Aencoders%252C%2520we%2520decouple%2520different%2520types%2520of%2520speech%2520information%252C%2520utilizing%2520a%250AWhisper%2520encoder%2520to%2520process%2520the%2520semantic%2520content%2520of%2520speech%252C%2520and%2520a%2520WavLM%2520encoder%250Ato%2520capture%2520the%2520unique%2520characteristics%2520of%2520the%2520speaker%2527s%2520identity.%2520Within%2520the%250Acurriculum%2520learning%2520framework%252C%2520WavLLM%2520first%2520builds%2520its%2520foundational%250Acapabilities%2520by%2520optimizing%2520on%2520mixed%2520elementary%2520single%2520tasks%252C%2520followed%2520by%250Aadvanced%2520multi-task%2520training%2520on%2520more%2520complex%2520tasks%2520such%2520as%2520combinations%2520of%2520the%250Aelementary%2520tasks.%2520To%2520enhance%2520the%2520flexibility%2520and%2520adherence%2520to%2520different%2520tasks%250Aand%2520instructions%252C%2520a%2520prompt-aware%2520LoRA%2520weight%2520adapter%2520is%2520introduced%2520in%2520the%250Asecond%2520advanced%2520multi-task%2520training%2520stage.%2520We%2520validate%2520the%2520proposed%2520model%2520on%250Auniversal%2520speech%2520benchmarks%2520including%2520tasks%2520such%2520as%2520ASR%252C%2520ST%252C%2520SV%252C%2520ER%252C%2520and%2520also%250Aapply%2520it%2520to%2520specialized%2520datasets%2520like%2520Gaokao%2520English%2520listening%2520comprehension%250Aset%2520for%2520SQA%252C%2520and%2520speech%2520Chain-of-Thought%2520%2528CoT%2529%2520evaluation%2520set.%2520Experiments%250Ademonstrate%2520that%2520the%2520proposed%2520model%2520achieves%2520state-of-the-art%2520performance%250Aacross%2520a%2520range%2520of%2520speech%2520tasks%2520on%2520the%2520same%2520model%2520size%252C%2520exhibiting%2520robust%250Ageneralization%2520capabilities%2520in%2520executing%2520complex%2520tasks%2520using%2520CoT%2520approach.%250AFurthermore%252C%2520our%2520model%2520successfully%2520completes%2520Gaokao%2520tasks%2520without%2520specialized%250Atraining.%2520The%2520codes%252C%2520models%252C%2520audio%252C%2520and%2520Gaokao%2520evaluation%2520set%2520can%2520be%2520accessed%250Aat%2520%255Curl%257Baka.ms/wavllm%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2404.00656v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WavLLM%3A%20Towards%20Robust%20and%20Adaptive%20Speech%20Large%20Language%20Model&entry.906535625=Shujie%20Hu%20and%20Long%20Zhou%20and%20Shujie%20Liu%20and%20Sanyuan%20Chen%20and%20Lingwei%20Meng%20and%20Hongkun%20Hao%20and%20Jing%20Pan%20and%20Xunying%20Liu%20and%20Jinyu%20Li%20and%20Sunit%20Sivasankaran%20and%20Linquan%20Liu%20and%20Furu%20Wei&entry.1292438233=%20%20The%20recent%20advancements%20in%20large%20language%20models%20%28LLMs%29%20have%20revolutionized%0Athe%20field%20of%20natural%20language%20processing%2C%20progressively%20broadening%20their%20scope%0Ato%20multimodal%20perception%20and%20generation.%20However%2C%20effectively%20integrating%0Alistening%20capabilities%20into%20LLMs%20poses%20significant%20challenges%2C%20particularly%0Awith%20respect%20to%20generalizing%20across%20varied%20contexts%20and%20executing%20complex%0Aauditory%20tasks.%20In%20this%20work%2C%20we%20introduce%20WavLLM%2C%20a%20robust%20and%20adaptive%20speech%0Alarge%20language%20model%20with%20dual%20encoders%2C%20and%20a%20prompt-aware%20LoRA%20weight%0Aadapter%2C%20optimized%20by%20a%20two-stage%20curriculum%20learning%20approach.%20Leveraging%20dual%0Aencoders%2C%20we%20decouple%20different%20types%20of%20speech%20information%2C%20utilizing%20a%0AWhisper%20encoder%20to%20process%20the%20semantic%20content%20of%20speech%2C%20and%20a%20WavLM%20encoder%0Ato%20capture%20the%20unique%20characteristics%20of%20the%20speaker%27s%20identity.%20Within%20the%0Acurriculum%20learning%20framework%2C%20WavLLM%20first%20builds%20its%20foundational%0Acapabilities%20by%20optimizing%20on%20mixed%20elementary%20single%20tasks%2C%20followed%20by%0Aadvanced%20multi-task%20training%20on%20more%20complex%20tasks%20such%20as%20combinations%20of%20the%0Aelementary%20tasks.%20To%20enhance%20the%20flexibility%20and%20adherence%20to%20different%20tasks%0Aand%20instructions%2C%20a%20prompt-aware%20LoRA%20weight%20adapter%20is%20introduced%20in%20the%0Asecond%20advanced%20multi-task%20training%20stage.%20We%20validate%20the%20proposed%20model%20on%0Auniversal%20speech%20benchmarks%20including%20tasks%20such%20as%20ASR%2C%20ST%2C%20SV%2C%20ER%2C%20and%20also%0Aapply%20it%20to%20specialized%20datasets%20like%20Gaokao%20English%20listening%20comprehension%0Aset%20for%20SQA%2C%20and%20speech%20Chain-of-Thought%20%28CoT%29%20evaluation%20set.%20Experiments%0Ademonstrate%20that%20the%20proposed%20model%20achieves%20state-of-the-art%20performance%0Aacross%20a%20range%20of%20speech%20tasks%20on%20the%20same%20model%20size%2C%20exhibiting%20robust%0Ageneralization%20capabilities%20in%20executing%20complex%20tasks%20using%20CoT%20approach.%0AFurthermore%2C%20our%20model%20successfully%20completes%20Gaokao%20tasks%20without%20specialized%0Atraining.%20The%20codes%2C%20models%2C%20audio%2C%20and%20Gaokao%20evaluation%20set%20can%20be%20accessed%0Aat%20%5Curl%7Baka.ms/wavllm%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2404.00656v2&entry.124074799=Read"},
{"title": "Robust online reconstruction of continuous-time signals from a lean\n  spike train ensemble code", "author": "Anik Chattopadhyay and Arunava Banerjee", "abstract": "  Sensory stimuli in animals are encoded into spike trains by neurons, offering\nadvantages such as sparsity, energy efficiency, and high temporal resolution.\nThis paper presents a signal processing framework that deterministically\nencodes continuous-time signals into biologically feasible spike trains, and\naddresses the questions about representable signal classes and reconstruction\nbounds. The framework considers encoding of a signal through spike trains\ngenerated by an ensemble of neurons using a convolve-then-threshold mechanism\nwith various convolution kernels. A closed-form solution to the inverse\nproblem, from spike trains to signal reconstruction, is derived in the Hilbert\nspace of shifted kernel functions, ensuring sparse representation of a\ngeneralized Finite Rate of Innovation (FRI) class of signals. Additionally,\ninspired by real-time processing in biological systems, an efficient iterative\nversion of the optimal reconstruction is formulated that considers only a\nfinite window of past spikes, ensuring robustness of the technique to\nill-conditioned encoding; convergence guarantees of the windowed reconstruction\nto the optimal solution are then provided. Experiments on a large audio dataset\ndemonstrate excellent reconstruction accuracy at spike rates as low as\none-fifth of the Nyquist rate, while showing clear competitive advantage in\ncomparison to state-of-the-art sparse coding techniques in the low spike rate\nregime.\n", "link": "http://arxiv.org/abs/2408.05950v2", "date": "2024-08-14", "relevancy": 1.947, "topK": [{"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.5007}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4811}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4661}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20online%20reconstruction%20of%20continuous-time%20signals%20from%20a%20lean%0A%20%20spike%20train%20ensemble%20code&body=Title%3A%20Robust%20online%20reconstruction%20of%20continuous-time%20signals%20from%20a%20lean%0A%20%20spike%20train%20ensemble%20code%0AAuthor%3A%20Anik%20Chattopadhyay%20and%20Arunava%20Banerjee%0AAbstract%3A%20%20%20Sensory%20stimuli%20in%20animals%20are%20encoded%20into%20spike%20trains%20by%20neurons%2C%20offering%0Aadvantages%20such%20as%20sparsity%2C%20energy%20efficiency%2C%20and%20high%20temporal%20resolution.%0AThis%20paper%20presents%20a%20signal%20processing%20framework%20that%20deterministically%0Aencodes%20continuous-time%20signals%20into%20biologically%20feasible%20spike%20trains%2C%20and%0Aaddresses%20the%20questions%20about%20representable%20signal%20classes%20and%20reconstruction%0Abounds.%20The%20framework%20considers%20encoding%20of%20a%20signal%20through%20spike%20trains%0Agenerated%20by%20an%20ensemble%20of%20neurons%20using%20a%20convolve-then-threshold%20mechanism%0Awith%20various%20convolution%20kernels.%20A%20closed-form%20solution%20to%20the%20inverse%0Aproblem%2C%20from%20spike%20trains%20to%20signal%20reconstruction%2C%20is%20derived%20in%20the%20Hilbert%0Aspace%20of%20shifted%20kernel%20functions%2C%20ensuring%20sparse%20representation%20of%20a%0Ageneralized%20Finite%20Rate%20of%20Innovation%20%28FRI%29%20class%20of%20signals.%20Additionally%2C%0Ainspired%20by%20real-time%20processing%20in%20biological%20systems%2C%20an%20efficient%20iterative%0Aversion%20of%20the%20optimal%20reconstruction%20is%20formulated%20that%20considers%20only%20a%0Afinite%20window%20of%20past%20spikes%2C%20ensuring%20robustness%20of%20the%20technique%20to%0Aill-conditioned%20encoding%3B%20convergence%20guarantees%20of%20the%20windowed%20reconstruction%0Ato%20the%20optimal%20solution%20are%20then%20provided.%20Experiments%20on%20a%20large%20audio%20dataset%0Ademonstrate%20excellent%20reconstruction%20accuracy%20at%20spike%20rates%20as%20low%20as%0Aone-fifth%20of%20the%20Nyquist%20rate%2C%20while%20showing%20clear%20competitive%20advantage%20in%0Acomparison%20to%20state-of-the-art%20sparse%20coding%20techniques%20in%20the%20low%20spike%20rate%0Aregime.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.05950v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520online%2520reconstruction%2520of%2520continuous-time%2520signals%2520from%2520a%2520lean%250A%2520%2520spike%2520train%2520ensemble%2520code%26entry.906535625%3DAnik%2520Chattopadhyay%2520and%2520Arunava%2520Banerjee%26entry.1292438233%3D%2520%2520Sensory%2520stimuli%2520in%2520animals%2520are%2520encoded%2520into%2520spike%2520trains%2520by%2520neurons%252C%2520offering%250Aadvantages%2520such%2520as%2520sparsity%252C%2520energy%2520efficiency%252C%2520and%2520high%2520temporal%2520resolution.%250AThis%2520paper%2520presents%2520a%2520signal%2520processing%2520framework%2520that%2520deterministically%250Aencodes%2520continuous-time%2520signals%2520into%2520biologically%2520feasible%2520spike%2520trains%252C%2520and%250Aaddresses%2520the%2520questions%2520about%2520representable%2520signal%2520classes%2520and%2520reconstruction%250Abounds.%2520The%2520framework%2520considers%2520encoding%2520of%2520a%2520signal%2520through%2520spike%2520trains%250Agenerated%2520by%2520an%2520ensemble%2520of%2520neurons%2520using%2520a%2520convolve-then-threshold%2520mechanism%250Awith%2520various%2520convolution%2520kernels.%2520A%2520closed-form%2520solution%2520to%2520the%2520inverse%250Aproblem%252C%2520from%2520spike%2520trains%2520to%2520signal%2520reconstruction%252C%2520is%2520derived%2520in%2520the%2520Hilbert%250Aspace%2520of%2520shifted%2520kernel%2520functions%252C%2520ensuring%2520sparse%2520representation%2520of%2520a%250Ageneralized%2520Finite%2520Rate%2520of%2520Innovation%2520%2528FRI%2529%2520class%2520of%2520signals.%2520Additionally%252C%250Ainspired%2520by%2520real-time%2520processing%2520in%2520biological%2520systems%252C%2520an%2520efficient%2520iterative%250Aversion%2520of%2520the%2520optimal%2520reconstruction%2520is%2520formulated%2520that%2520considers%2520only%2520a%250Afinite%2520window%2520of%2520past%2520spikes%252C%2520ensuring%2520robustness%2520of%2520the%2520technique%2520to%250Aill-conditioned%2520encoding%253B%2520convergence%2520guarantees%2520of%2520the%2520windowed%2520reconstruction%250Ato%2520the%2520optimal%2520solution%2520are%2520then%2520provided.%2520Experiments%2520on%2520a%2520large%2520audio%2520dataset%250Ademonstrate%2520excellent%2520reconstruction%2520accuracy%2520at%2520spike%2520rates%2520as%2520low%2520as%250Aone-fifth%2520of%2520the%2520Nyquist%2520rate%252C%2520while%2520showing%2520clear%2520competitive%2520advantage%2520in%250Acomparison%2520to%2520state-of-the-art%2520sparse%2520coding%2520techniques%2520in%2520the%2520low%2520spike%2520rate%250Aregime.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.05950v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20online%20reconstruction%20of%20continuous-time%20signals%20from%20a%20lean%0A%20%20spike%20train%20ensemble%20code&entry.906535625=Anik%20Chattopadhyay%20and%20Arunava%20Banerjee&entry.1292438233=%20%20Sensory%20stimuli%20in%20animals%20are%20encoded%20into%20spike%20trains%20by%20neurons%2C%20offering%0Aadvantages%20such%20as%20sparsity%2C%20energy%20efficiency%2C%20and%20high%20temporal%20resolution.%0AThis%20paper%20presents%20a%20signal%20processing%20framework%20that%20deterministically%0Aencodes%20continuous-time%20signals%20into%20biologically%20feasible%20spike%20trains%2C%20and%0Aaddresses%20the%20questions%20about%20representable%20signal%20classes%20and%20reconstruction%0Abounds.%20The%20framework%20considers%20encoding%20of%20a%20signal%20through%20spike%20trains%0Agenerated%20by%20an%20ensemble%20of%20neurons%20using%20a%20convolve-then-threshold%20mechanism%0Awith%20various%20convolution%20kernels.%20A%20closed-form%20solution%20to%20the%20inverse%0Aproblem%2C%20from%20spike%20trains%20to%20signal%20reconstruction%2C%20is%20derived%20in%20the%20Hilbert%0Aspace%20of%20shifted%20kernel%20functions%2C%20ensuring%20sparse%20representation%20of%20a%0Ageneralized%20Finite%20Rate%20of%20Innovation%20%28FRI%29%20class%20of%20signals.%20Additionally%2C%0Ainspired%20by%20real-time%20processing%20in%20biological%20systems%2C%20an%20efficient%20iterative%0Aversion%20of%20the%20optimal%20reconstruction%20is%20formulated%20that%20considers%20only%20a%0Afinite%20window%20of%20past%20spikes%2C%20ensuring%20robustness%20of%20the%20technique%20to%0Aill-conditioned%20encoding%3B%20convergence%20guarantees%20of%20the%20windowed%20reconstruction%0Ato%20the%20optimal%20solution%20are%20then%20provided.%20Experiments%20on%20a%20large%20audio%20dataset%0Ademonstrate%20excellent%20reconstruction%20accuracy%20at%20spike%20rates%20as%20low%20as%0Aone-fifth%20of%20the%20Nyquist%20rate%2C%20while%20showing%20clear%20competitive%20advantage%20in%0Acomparison%20to%20state-of-the-art%20sparse%20coding%20techniques%20in%20the%20low%20spike%20rate%0Aregime.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.05950v2&entry.124074799=Read"},
{"title": "Problem Solving Through Human-AI Preference-Based Cooperation", "author": "Subhabrata Dutta and Timo Kaufmann and Goran Glava\u0161 and Ivan Habernal and Kristian Kersting and Frauke Kreuter and Mira Mezini and Iryna Gurevych and Eyke H\u00fcllermeier and Hinrich Schuetze", "abstract": "  While there is a widespread belief that artificial general intelligence (AGI)\n-- or even superhuman AI -- is imminent, complex problems in expert domains are\nfar from being solved. We argue that such problems require human-AI cooperation\nand that the current state of the art in generative AI is unable to play the\nrole of a reliable partner due to a multitude of shortcomings, including\ninability to keep track of a complex solution artifact (e.g., a software\nprogram), limited support for versatile human preference expression and lack of\nadapting to human preference in an interactive setting. To address these\nchallenges, we propose HAI-Co2, a novel human-AI co-construction framework. We\nformalize HAI-Co2 and discuss the difficult open research problems that it\nfaces. Finally, we present a case study of HAI-Co2 and demonstrate its efficacy\ncompared to monolithic generative AI models.\n", "link": "http://arxiv.org/abs/2408.07461v1", "date": "2024-08-14", "relevancy": 1.9439, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4998}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4949}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4716}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Problem%20Solving%20Through%20Human-AI%20Preference-Based%20Cooperation&body=Title%3A%20Problem%20Solving%20Through%20Human-AI%20Preference-Based%20Cooperation%0AAuthor%3A%20Subhabrata%20Dutta%20and%20Timo%20Kaufmann%20and%20Goran%20Glava%C5%A1%20and%20Ivan%20Habernal%20and%20Kristian%20Kersting%20and%20Frauke%20Kreuter%20and%20Mira%20Mezini%20and%20Iryna%20Gurevych%20and%20Eyke%20H%C3%BCllermeier%20and%20Hinrich%20Schuetze%0AAbstract%3A%20%20%20While%20there%20is%20a%20widespread%20belief%20that%20artificial%20general%20intelligence%20%28AGI%29%0A--%20or%20even%20superhuman%20AI%20--%20is%20imminent%2C%20complex%20problems%20in%20expert%20domains%20are%0Afar%20from%20being%20solved.%20We%20argue%20that%20such%20problems%20require%20human-AI%20cooperation%0Aand%20that%20the%20current%20state%20of%20the%20art%20in%20generative%20AI%20is%20unable%20to%20play%20the%0Arole%20of%20a%20reliable%20partner%20due%20to%20a%20multitude%20of%20shortcomings%2C%20including%0Ainability%20to%20keep%20track%20of%20a%20complex%20solution%20artifact%20%28e.g.%2C%20a%20software%0Aprogram%29%2C%20limited%20support%20for%20versatile%20human%20preference%20expression%20and%20lack%20of%0Aadapting%20to%20human%20preference%20in%20an%20interactive%20setting.%20To%20address%20these%0Achallenges%2C%20we%20propose%20HAI-Co2%2C%20a%20novel%20human-AI%20co-construction%20framework.%20We%0Aformalize%20HAI-Co2%20and%20discuss%20the%20difficult%20open%20research%20problems%20that%20it%0Afaces.%20Finally%2C%20we%20present%20a%20case%20study%20of%20HAI-Co2%20and%20demonstrate%20its%20efficacy%0Acompared%20to%20monolithic%20generative%20AI%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07461v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DProblem%2520Solving%2520Through%2520Human-AI%2520Preference-Based%2520Cooperation%26entry.906535625%3DSubhabrata%2520Dutta%2520and%2520Timo%2520Kaufmann%2520and%2520Goran%2520Glava%25C5%25A1%2520and%2520Ivan%2520Habernal%2520and%2520Kristian%2520Kersting%2520and%2520Frauke%2520Kreuter%2520and%2520Mira%2520Mezini%2520and%2520Iryna%2520Gurevych%2520and%2520Eyke%2520H%25C3%25BCllermeier%2520and%2520Hinrich%2520Schuetze%26entry.1292438233%3D%2520%2520While%2520there%2520is%2520a%2520widespread%2520belief%2520that%2520artificial%2520general%2520intelligence%2520%2528AGI%2529%250A--%2520or%2520even%2520superhuman%2520AI%2520--%2520is%2520imminent%252C%2520complex%2520problems%2520in%2520expert%2520domains%2520are%250Afar%2520from%2520being%2520solved.%2520We%2520argue%2520that%2520such%2520problems%2520require%2520human-AI%2520cooperation%250Aand%2520that%2520the%2520current%2520state%2520of%2520the%2520art%2520in%2520generative%2520AI%2520is%2520unable%2520to%2520play%2520the%250Arole%2520of%2520a%2520reliable%2520partner%2520due%2520to%2520a%2520multitude%2520of%2520shortcomings%252C%2520including%250Ainability%2520to%2520keep%2520track%2520of%2520a%2520complex%2520solution%2520artifact%2520%2528e.g.%252C%2520a%2520software%250Aprogram%2529%252C%2520limited%2520support%2520for%2520versatile%2520human%2520preference%2520expression%2520and%2520lack%2520of%250Aadapting%2520to%2520human%2520preference%2520in%2520an%2520interactive%2520setting.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520propose%2520HAI-Co2%252C%2520a%2520novel%2520human-AI%2520co-construction%2520framework.%2520We%250Aformalize%2520HAI-Co2%2520and%2520discuss%2520the%2520difficult%2520open%2520research%2520problems%2520that%2520it%250Afaces.%2520Finally%252C%2520we%2520present%2520a%2520case%2520study%2520of%2520HAI-Co2%2520and%2520demonstrate%2520its%2520efficacy%250Acompared%2520to%2520monolithic%2520generative%2520AI%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07461v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Problem%20Solving%20Through%20Human-AI%20Preference-Based%20Cooperation&entry.906535625=Subhabrata%20Dutta%20and%20Timo%20Kaufmann%20and%20Goran%20Glava%C5%A1%20and%20Ivan%20Habernal%20and%20Kristian%20Kersting%20and%20Frauke%20Kreuter%20and%20Mira%20Mezini%20and%20Iryna%20Gurevych%20and%20Eyke%20H%C3%BCllermeier%20and%20Hinrich%20Schuetze&entry.1292438233=%20%20While%20there%20is%20a%20widespread%20belief%20that%20artificial%20general%20intelligence%20%28AGI%29%0A--%20or%20even%20superhuman%20AI%20--%20is%20imminent%2C%20complex%20problems%20in%20expert%20domains%20are%0Afar%20from%20being%20solved.%20We%20argue%20that%20such%20problems%20require%20human-AI%20cooperation%0Aand%20that%20the%20current%20state%20of%20the%20art%20in%20generative%20AI%20is%20unable%20to%20play%20the%0Arole%20of%20a%20reliable%20partner%20due%20to%20a%20multitude%20of%20shortcomings%2C%20including%0Ainability%20to%20keep%20track%20of%20a%20complex%20solution%20artifact%20%28e.g.%2C%20a%20software%0Aprogram%29%2C%20limited%20support%20for%20versatile%20human%20preference%20expression%20and%20lack%20of%0Aadapting%20to%20human%20preference%20in%20an%20interactive%20setting.%20To%20address%20these%0Achallenges%2C%20we%20propose%20HAI-Co2%2C%20a%20novel%20human-AI%20co-construction%20framework.%20We%0Aformalize%20HAI-Co2%20and%20discuss%20the%20difficult%20open%20research%20problems%20that%20it%0Afaces.%20Finally%2C%20we%20present%20a%20case%20study%20of%20HAI-Co2%20and%20demonstrate%20its%20efficacy%0Acompared%20to%20monolithic%20generative%20AI%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07461v1&entry.124074799=Read"},
{"title": "A Nested Graph Reinforcement Learning-based Decision-making Strategy for\n  Eco-platooning", "author": "Xin Gao and Xueyuan Li and Hao Liu and Ao Li and Zhaoyang Ma and Zirui Li", "abstract": "  Platooning technology is renowned for its precise vehicle control, traffic\nflow optimization, and energy efficiency enhancement. However, in large-scale\nmixed platoons, vehicle heterogeneity and unpredictable traffic conditions lead\nto virtual bottlenecks. These bottlenecks result in reduced traffic throughput\nand increased energy consumption within the platoon. To address these\nchallenges, we introduce a decision-making strategy based on nested graph\nreinforcement learning. This strategy improves collaborative decision-making,\nensuring energy efficiency and alleviating congestion. We propose a theory of\nnested traffic graph representation that maps dynamic interactions between\nvehicles and platoons in non-Euclidean spaces. By incorporating spatio-temporal\nweighted graph into a multi-head attention mechanism, we further enhance the\nmodel's capacity to process both local and global data. Additionally, we have\ndeveloped a nested graph reinforcement learning framework to enhance the\nself-iterative learning capabilities of platooning. Using the I-24 dataset, we\ndesigned and conducted comparative algorithm experiments, generalizability\ntesting, and permeability ablation experiments, thereby validating the proposed\nstrategy's effectiveness. Compared to the baseline, our strategy increases\nthroughput by 10% and decreases energy use by 9%. Specifically, increasing the\npenetration rate of CAVs significantly enhances traffic throughput, though it\nalso increases energy consumption.\n", "link": "http://arxiv.org/abs/2408.07578v1", "date": "2024-08-14", "relevancy": 1.9434, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4921}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.483}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4774}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Nested%20Graph%20Reinforcement%20Learning-based%20Decision-making%20Strategy%20for%0A%20%20Eco-platooning&body=Title%3A%20A%20Nested%20Graph%20Reinforcement%20Learning-based%20Decision-making%20Strategy%20for%0A%20%20Eco-platooning%0AAuthor%3A%20Xin%20Gao%20and%20Xueyuan%20Li%20and%20Hao%20Liu%20and%20Ao%20Li%20and%20Zhaoyang%20Ma%20and%20Zirui%20Li%0AAbstract%3A%20%20%20Platooning%20technology%20is%20renowned%20for%20its%20precise%20vehicle%20control%2C%20traffic%0Aflow%20optimization%2C%20and%20energy%20efficiency%20enhancement.%20However%2C%20in%20large-scale%0Amixed%20platoons%2C%20vehicle%20heterogeneity%20and%20unpredictable%20traffic%20conditions%20lead%0Ato%20virtual%20bottlenecks.%20These%20bottlenecks%20result%20in%20reduced%20traffic%20throughput%0Aand%20increased%20energy%20consumption%20within%20the%20platoon.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20a%20decision-making%20strategy%20based%20on%20nested%20graph%0Areinforcement%20learning.%20This%20strategy%20improves%20collaborative%20decision-making%2C%0Aensuring%20energy%20efficiency%20and%20alleviating%20congestion.%20We%20propose%20a%20theory%20of%0Anested%20traffic%20graph%20representation%20that%20maps%20dynamic%20interactions%20between%0Avehicles%20and%20platoons%20in%20non-Euclidean%20spaces.%20By%20incorporating%20spatio-temporal%0Aweighted%20graph%20into%20a%20multi-head%20attention%20mechanism%2C%20we%20further%20enhance%20the%0Amodel%27s%20capacity%20to%20process%20both%20local%20and%20global%20data.%20Additionally%2C%20we%20have%0Adeveloped%20a%20nested%20graph%20reinforcement%20learning%20framework%20to%20enhance%20the%0Aself-iterative%20learning%20capabilities%20of%20platooning.%20Using%20the%20I-24%20dataset%2C%20we%0Adesigned%20and%20conducted%20comparative%20algorithm%20experiments%2C%20generalizability%0Atesting%2C%20and%20permeability%20ablation%20experiments%2C%20thereby%20validating%20the%20proposed%0Astrategy%27s%20effectiveness.%20Compared%20to%20the%20baseline%2C%20our%20strategy%20increases%0Athroughput%20by%2010%25%20and%20decreases%20energy%20use%20by%209%25.%20Specifically%2C%20increasing%20the%0Apenetration%20rate%20of%20CAVs%20significantly%20enhances%20traffic%20throughput%2C%20though%20it%0Aalso%20increases%20energy%20consumption.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07578v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Nested%2520Graph%2520Reinforcement%2520Learning-based%2520Decision-making%2520Strategy%2520for%250A%2520%2520Eco-platooning%26entry.906535625%3DXin%2520Gao%2520and%2520Xueyuan%2520Li%2520and%2520Hao%2520Liu%2520and%2520Ao%2520Li%2520and%2520Zhaoyang%2520Ma%2520and%2520Zirui%2520Li%26entry.1292438233%3D%2520%2520Platooning%2520technology%2520is%2520renowned%2520for%2520its%2520precise%2520vehicle%2520control%252C%2520traffic%250Aflow%2520optimization%252C%2520and%2520energy%2520efficiency%2520enhancement.%2520However%252C%2520in%2520large-scale%250Amixed%2520platoons%252C%2520vehicle%2520heterogeneity%2520and%2520unpredictable%2520traffic%2520conditions%2520lead%250Ato%2520virtual%2520bottlenecks.%2520These%2520bottlenecks%2520result%2520in%2520reduced%2520traffic%2520throughput%250Aand%2520increased%2520energy%2520consumption%2520within%2520the%2520platoon.%2520To%2520address%2520these%250Achallenges%252C%2520we%2520introduce%2520a%2520decision-making%2520strategy%2520based%2520on%2520nested%2520graph%250Areinforcement%2520learning.%2520This%2520strategy%2520improves%2520collaborative%2520decision-making%252C%250Aensuring%2520energy%2520efficiency%2520and%2520alleviating%2520congestion.%2520We%2520propose%2520a%2520theory%2520of%250Anested%2520traffic%2520graph%2520representation%2520that%2520maps%2520dynamic%2520interactions%2520between%250Avehicles%2520and%2520platoons%2520in%2520non-Euclidean%2520spaces.%2520By%2520incorporating%2520spatio-temporal%250Aweighted%2520graph%2520into%2520a%2520multi-head%2520attention%2520mechanism%252C%2520we%2520further%2520enhance%2520the%250Amodel%2527s%2520capacity%2520to%2520process%2520both%2520local%2520and%2520global%2520data.%2520Additionally%252C%2520we%2520have%250Adeveloped%2520a%2520nested%2520graph%2520reinforcement%2520learning%2520framework%2520to%2520enhance%2520the%250Aself-iterative%2520learning%2520capabilities%2520of%2520platooning.%2520Using%2520the%2520I-24%2520dataset%252C%2520we%250Adesigned%2520and%2520conducted%2520comparative%2520algorithm%2520experiments%252C%2520generalizability%250Atesting%252C%2520and%2520permeability%2520ablation%2520experiments%252C%2520thereby%2520validating%2520the%2520proposed%250Astrategy%2527s%2520effectiveness.%2520Compared%2520to%2520the%2520baseline%252C%2520our%2520strategy%2520increases%250Athroughput%2520by%252010%2525%2520and%2520decreases%2520energy%2520use%2520by%25209%2525.%2520Specifically%252C%2520increasing%2520the%250Apenetration%2520rate%2520of%2520CAVs%2520significantly%2520enhances%2520traffic%2520throughput%252C%2520though%2520it%250Aalso%2520increases%2520energy%2520consumption.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07578v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Nested%20Graph%20Reinforcement%20Learning-based%20Decision-making%20Strategy%20for%0A%20%20Eco-platooning&entry.906535625=Xin%20Gao%20and%20Xueyuan%20Li%20and%20Hao%20Liu%20and%20Ao%20Li%20and%20Zhaoyang%20Ma%20and%20Zirui%20Li&entry.1292438233=%20%20Platooning%20technology%20is%20renowned%20for%20its%20precise%20vehicle%20control%2C%20traffic%0Aflow%20optimization%2C%20and%20energy%20efficiency%20enhancement.%20However%2C%20in%20large-scale%0Amixed%20platoons%2C%20vehicle%20heterogeneity%20and%20unpredictable%20traffic%20conditions%20lead%0Ato%20virtual%20bottlenecks.%20These%20bottlenecks%20result%20in%20reduced%20traffic%20throughput%0Aand%20increased%20energy%20consumption%20within%20the%20platoon.%20To%20address%20these%0Achallenges%2C%20we%20introduce%20a%20decision-making%20strategy%20based%20on%20nested%20graph%0Areinforcement%20learning.%20This%20strategy%20improves%20collaborative%20decision-making%2C%0Aensuring%20energy%20efficiency%20and%20alleviating%20congestion.%20We%20propose%20a%20theory%20of%0Anested%20traffic%20graph%20representation%20that%20maps%20dynamic%20interactions%20between%0Avehicles%20and%20platoons%20in%20non-Euclidean%20spaces.%20By%20incorporating%20spatio-temporal%0Aweighted%20graph%20into%20a%20multi-head%20attention%20mechanism%2C%20we%20further%20enhance%20the%0Amodel%27s%20capacity%20to%20process%20both%20local%20and%20global%20data.%20Additionally%2C%20we%20have%0Adeveloped%20a%20nested%20graph%20reinforcement%20learning%20framework%20to%20enhance%20the%0Aself-iterative%20learning%20capabilities%20of%20platooning.%20Using%20the%20I-24%20dataset%2C%20we%0Adesigned%20and%20conducted%20comparative%20algorithm%20experiments%2C%20generalizability%0Atesting%2C%20and%20permeability%20ablation%20experiments%2C%20thereby%20validating%20the%20proposed%0Astrategy%27s%20effectiveness.%20Compared%20to%20the%20baseline%2C%20our%20strategy%20increases%0Athroughput%20by%2010%25%20and%20decreases%20energy%20use%20by%209%25.%20Specifically%2C%20increasing%20the%0Apenetration%20rate%20of%20CAVs%20significantly%20enhances%20traffic%20throughput%2C%20though%20it%0Aalso%20increases%20energy%20consumption.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07578v1&entry.124074799=Read"},
{"title": "Evidential Graph Contrastive Alignment for Source-Free Blending-Target\n  Domain Adaptation", "author": "Juepeng Zheng and Yibin Wen and Jinxiao Zhang and Runmin Dong and Haohuan Fu", "abstract": "  In this paper, we firstly tackle a more realistic Domain Adaptation (DA)\nsetting: Source-Free Blending-Target Domain Adaptation (SF-BTDA), where we can\nnot access to source domain data while facing mixed multiple target domains\nwithout any domain labels in prior. Compared to existing DA scenarios, SF-BTDA\ngenerally faces the co-existence of different label shifts in different\ntargets, along with noisy target pseudo labels generated from the source model.\nIn this paper, we propose a new method called Evidential Contrastive Alignment\n(ECA) to decouple the blending target domain and alleviate the effect from\nnoisy target pseudo labels. First, to improve the quality of pseudo target\nlabels, we propose a calibrated evidential learning module to iteratively\nimprove both the accuracy and certainty of the resulting model and adaptively\ngenerate high-quality pseudo target labels. Second, we design a graph\ncontrastive learning with the domain distance matrix and confidence-uncertainty\ncriterion, to minimize the distribution gap of samples of a same class in the\nblended target domains, which alleviates the co-existence of different label\nshifts in blended targets. We conduct a new benchmark based on three standard\nDA datasets and ECA outperforms other methods with considerable gains and\nachieves comparable results compared with those that have domain labels or\nsource data in prior.\n", "link": "http://arxiv.org/abs/2408.07527v1", "date": "2024-08-14", "relevancy": 1.9297, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.507}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4785}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4765}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Evidential%20Graph%20Contrastive%20Alignment%20for%20Source-Free%20Blending-Target%0A%20%20Domain%20Adaptation&body=Title%3A%20Evidential%20Graph%20Contrastive%20Alignment%20for%20Source-Free%20Blending-Target%0A%20%20Domain%20Adaptation%0AAuthor%3A%20Juepeng%20Zheng%20and%20Yibin%20Wen%20and%20Jinxiao%20Zhang%20and%20Runmin%20Dong%20and%20Haohuan%20Fu%0AAbstract%3A%20%20%20In%20this%20paper%2C%20we%20firstly%20tackle%20a%20more%20realistic%20Domain%20Adaptation%20%28DA%29%0Asetting%3A%20Source-Free%20Blending-Target%20Domain%20Adaptation%20%28SF-BTDA%29%2C%20where%20we%20can%0Anot%20access%20to%20source%20domain%20data%20while%20facing%20mixed%20multiple%20target%20domains%0Awithout%20any%20domain%20labels%20in%20prior.%20Compared%20to%20existing%20DA%20scenarios%2C%20SF-BTDA%0Agenerally%20faces%20the%20co-existence%20of%20different%20label%20shifts%20in%20different%0Atargets%2C%20along%20with%20noisy%20target%20pseudo%20labels%20generated%20from%20the%20source%20model.%0AIn%20this%20paper%2C%20we%20propose%20a%20new%20method%20called%20Evidential%20Contrastive%20Alignment%0A%28ECA%29%20to%20decouple%20the%20blending%20target%20domain%20and%20alleviate%20the%20effect%20from%0Anoisy%20target%20pseudo%20labels.%20First%2C%20to%20improve%20the%20quality%20of%20pseudo%20target%0Alabels%2C%20we%20propose%20a%20calibrated%20evidential%20learning%20module%20to%20iteratively%0Aimprove%20both%20the%20accuracy%20and%20certainty%20of%20the%20resulting%20model%20and%20adaptively%0Agenerate%20high-quality%20pseudo%20target%20labels.%20Second%2C%20we%20design%20a%20graph%0Acontrastive%20learning%20with%20the%20domain%20distance%20matrix%20and%20confidence-uncertainty%0Acriterion%2C%20to%20minimize%20the%20distribution%20gap%20of%20samples%20of%20a%20same%20class%20in%20the%0Ablended%20target%20domains%2C%20which%20alleviates%20the%20co-existence%20of%20different%20label%0Ashifts%20in%20blended%20targets.%20We%20conduct%20a%20new%20benchmark%20based%20on%20three%20standard%0ADA%20datasets%20and%20ECA%20outperforms%20other%20methods%20with%20considerable%20gains%20and%0Aachieves%20comparable%20results%20compared%20with%20those%20that%20have%20domain%20labels%20or%0Asource%20data%20in%20prior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07527v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DEvidential%2520Graph%2520Contrastive%2520Alignment%2520for%2520Source-Free%2520Blending-Target%250A%2520%2520Domain%2520Adaptation%26entry.906535625%3DJuepeng%2520Zheng%2520and%2520Yibin%2520Wen%2520and%2520Jinxiao%2520Zhang%2520and%2520Runmin%2520Dong%2520and%2520Haohuan%2520Fu%26entry.1292438233%3D%2520%2520In%2520this%2520paper%252C%2520we%2520firstly%2520tackle%2520a%2520more%2520realistic%2520Domain%2520Adaptation%2520%2528DA%2529%250Asetting%253A%2520Source-Free%2520Blending-Target%2520Domain%2520Adaptation%2520%2528SF-BTDA%2529%252C%2520where%2520we%2520can%250Anot%2520access%2520to%2520source%2520domain%2520data%2520while%2520facing%2520mixed%2520multiple%2520target%2520domains%250Awithout%2520any%2520domain%2520labels%2520in%2520prior.%2520Compared%2520to%2520existing%2520DA%2520scenarios%252C%2520SF-BTDA%250Agenerally%2520faces%2520the%2520co-existence%2520of%2520different%2520label%2520shifts%2520in%2520different%250Atargets%252C%2520along%2520with%2520noisy%2520target%2520pseudo%2520labels%2520generated%2520from%2520the%2520source%2520model.%250AIn%2520this%2520paper%252C%2520we%2520propose%2520a%2520new%2520method%2520called%2520Evidential%2520Contrastive%2520Alignment%250A%2528ECA%2529%2520to%2520decouple%2520the%2520blending%2520target%2520domain%2520and%2520alleviate%2520the%2520effect%2520from%250Anoisy%2520target%2520pseudo%2520labels.%2520First%252C%2520to%2520improve%2520the%2520quality%2520of%2520pseudo%2520target%250Alabels%252C%2520we%2520propose%2520a%2520calibrated%2520evidential%2520learning%2520module%2520to%2520iteratively%250Aimprove%2520both%2520the%2520accuracy%2520and%2520certainty%2520of%2520the%2520resulting%2520model%2520and%2520adaptively%250Agenerate%2520high-quality%2520pseudo%2520target%2520labels.%2520Second%252C%2520we%2520design%2520a%2520graph%250Acontrastive%2520learning%2520with%2520the%2520domain%2520distance%2520matrix%2520and%2520confidence-uncertainty%250Acriterion%252C%2520to%2520minimize%2520the%2520distribution%2520gap%2520of%2520samples%2520of%2520a%2520same%2520class%2520in%2520the%250Ablended%2520target%2520domains%252C%2520which%2520alleviates%2520the%2520co-existence%2520of%2520different%2520label%250Ashifts%2520in%2520blended%2520targets.%2520We%2520conduct%2520a%2520new%2520benchmark%2520based%2520on%2520three%2520standard%250ADA%2520datasets%2520and%2520ECA%2520outperforms%2520other%2520methods%2520with%2520considerable%2520gains%2520and%250Aachieves%2520comparable%2520results%2520compared%2520with%2520those%2520that%2520have%2520domain%2520labels%2520or%250Asource%2520data%2520in%2520prior.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07527v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Evidential%20Graph%20Contrastive%20Alignment%20for%20Source-Free%20Blending-Target%0A%20%20Domain%20Adaptation&entry.906535625=Juepeng%20Zheng%20and%20Yibin%20Wen%20and%20Jinxiao%20Zhang%20and%20Runmin%20Dong%20and%20Haohuan%20Fu&entry.1292438233=%20%20In%20this%20paper%2C%20we%20firstly%20tackle%20a%20more%20realistic%20Domain%20Adaptation%20%28DA%29%0Asetting%3A%20Source-Free%20Blending-Target%20Domain%20Adaptation%20%28SF-BTDA%29%2C%20where%20we%20can%0Anot%20access%20to%20source%20domain%20data%20while%20facing%20mixed%20multiple%20target%20domains%0Awithout%20any%20domain%20labels%20in%20prior.%20Compared%20to%20existing%20DA%20scenarios%2C%20SF-BTDA%0Agenerally%20faces%20the%20co-existence%20of%20different%20label%20shifts%20in%20different%0Atargets%2C%20along%20with%20noisy%20target%20pseudo%20labels%20generated%20from%20the%20source%20model.%0AIn%20this%20paper%2C%20we%20propose%20a%20new%20method%20called%20Evidential%20Contrastive%20Alignment%0A%28ECA%29%20to%20decouple%20the%20blending%20target%20domain%20and%20alleviate%20the%20effect%20from%0Anoisy%20target%20pseudo%20labels.%20First%2C%20to%20improve%20the%20quality%20of%20pseudo%20target%0Alabels%2C%20we%20propose%20a%20calibrated%20evidential%20learning%20module%20to%20iteratively%0Aimprove%20both%20the%20accuracy%20and%20certainty%20of%20the%20resulting%20model%20and%20adaptively%0Agenerate%20high-quality%20pseudo%20target%20labels.%20Second%2C%20we%20design%20a%20graph%0Acontrastive%20learning%20with%20the%20domain%20distance%20matrix%20and%20confidence-uncertainty%0Acriterion%2C%20to%20minimize%20the%20distribution%20gap%20of%20samples%20of%20a%20same%20class%20in%20the%0Ablended%20target%20domains%2C%20which%20alleviates%20the%20co-existence%20of%20different%20label%0Ashifts%20in%20blended%20targets.%20We%20conduct%20a%20new%20benchmark%20based%20on%20three%20standard%0ADA%20datasets%20and%20ECA%20outperforms%20other%20methods%20with%20considerable%20gains%20and%0Aachieves%20comparable%20results%20compared%20with%20those%20that%20have%20domain%20labels%20or%0Asource%20data%20in%20prior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07527v1&entry.124074799=Read"},
{"title": "Battery GraphNets : Relational Learning for Lithium-ion Batteries(LiBs)\n  Life Estimation", "author": "Sakhinana Sagar Srinivas and Rajat Kumar Sarkar and Venkataramana Runkana", "abstract": "  Battery life estimation is critical for optimizing battery performance and\nguaranteeing minimal degradation for better efficiency and reliability of\nbattery-powered systems. The existing methods to predict the Remaining Useful\nLife(RUL) of Lithium-ion Batteries (LiBs) neglect the relational dependencies\nof the battery parameters to model the nonlinear degradation trajectories. We\npresent the Battery GraphNets framework that jointly learns to incorporate a\ndiscrete dependency graph structure between battery parameters to capture the\ncomplex interactions and the graph-learning algorithm to model the intrinsic\nbattery degradation for RUL prognosis. The proposed method outperforms several\npopular methods by a significant margin on publicly available battery datasets\nand achieves SOTA performance. We report the ablation studies to support the\nefficacy of our approach.\n", "link": "http://arxiv.org/abs/2408.07624v1", "date": "2024-08-14", "relevancy": 1.9289, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4935}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4788}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4627}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Battery%20GraphNets%20%3A%20Relational%20Learning%20for%20Lithium-ion%20Batteries%28LiBs%29%0A%20%20Life%20Estimation&body=Title%3A%20Battery%20GraphNets%20%3A%20Relational%20Learning%20for%20Lithium-ion%20Batteries%28LiBs%29%0A%20%20Life%20Estimation%0AAuthor%3A%20Sakhinana%20Sagar%20Srinivas%20and%20Rajat%20Kumar%20Sarkar%20and%20Venkataramana%20Runkana%0AAbstract%3A%20%20%20Battery%20life%20estimation%20is%20critical%20for%20optimizing%20battery%20performance%20and%0Aguaranteeing%20minimal%20degradation%20for%20better%20efficiency%20and%20reliability%20of%0Abattery-powered%20systems.%20The%20existing%20methods%20to%20predict%20the%20Remaining%20Useful%0ALife%28RUL%29%20of%20Lithium-ion%20Batteries%20%28LiBs%29%20neglect%20the%20relational%20dependencies%0Aof%20the%20battery%20parameters%20to%20model%20the%20nonlinear%20degradation%20trajectories.%20We%0Apresent%20the%20Battery%20GraphNets%20framework%20that%20jointly%20learns%20to%20incorporate%20a%0Adiscrete%20dependency%20graph%20structure%20between%20battery%20parameters%20to%20capture%20the%0Acomplex%20interactions%20and%20the%20graph-learning%20algorithm%20to%20model%20the%20intrinsic%0Abattery%20degradation%20for%20RUL%20prognosis.%20The%20proposed%20method%20outperforms%20several%0Apopular%20methods%20by%20a%20significant%20margin%20on%20publicly%20available%20battery%20datasets%0Aand%20achieves%20SOTA%20performance.%20We%20report%20the%20ablation%20studies%20to%20support%20the%0Aefficacy%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07624v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBattery%2520GraphNets%2520%253A%2520Relational%2520Learning%2520for%2520Lithium-ion%2520Batteries%2528LiBs%2529%250A%2520%2520Life%2520Estimation%26entry.906535625%3DSakhinana%2520Sagar%2520Srinivas%2520and%2520Rajat%2520Kumar%2520Sarkar%2520and%2520Venkataramana%2520Runkana%26entry.1292438233%3D%2520%2520Battery%2520life%2520estimation%2520is%2520critical%2520for%2520optimizing%2520battery%2520performance%2520and%250Aguaranteeing%2520minimal%2520degradation%2520for%2520better%2520efficiency%2520and%2520reliability%2520of%250Abattery-powered%2520systems.%2520The%2520existing%2520methods%2520to%2520predict%2520the%2520Remaining%2520Useful%250ALife%2528RUL%2529%2520of%2520Lithium-ion%2520Batteries%2520%2528LiBs%2529%2520neglect%2520the%2520relational%2520dependencies%250Aof%2520the%2520battery%2520parameters%2520to%2520model%2520the%2520nonlinear%2520degradation%2520trajectories.%2520We%250Apresent%2520the%2520Battery%2520GraphNets%2520framework%2520that%2520jointly%2520learns%2520to%2520incorporate%2520a%250Adiscrete%2520dependency%2520graph%2520structure%2520between%2520battery%2520parameters%2520to%2520capture%2520the%250Acomplex%2520interactions%2520and%2520the%2520graph-learning%2520algorithm%2520to%2520model%2520the%2520intrinsic%250Abattery%2520degradation%2520for%2520RUL%2520prognosis.%2520The%2520proposed%2520method%2520outperforms%2520several%250Apopular%2520methods%2520by%2520a%2520significant%2520margin%2520on%2520publicly%2520available%2520battery%2520datasets%250Aand%2520achieves%2520SOTA%2520performance.%2520We%2520report%2520the%2520ablation%2520studies%2520to%2520support%2520the%250Aefficacy%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07624v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Battery%20GraphNets%20%3A%20Relational%20Learning%20for%20Lithium-ion%20Batteries%28LiBs%29%0A%20%20Life%20Estimation&entry.906535625=Sakhinana%20Sagar%20Srinivas%20and%20Rajat%20Kumar%20Sarkar%20and%20Venkataramana%20Runkana&entry.1292438233=%20%20Battery%20life%20estimation%20is%20critical%20for%20optimizing%20battery%20performance%20and%0Aguaranteeing%20minimal%20degradation%20for%20better%20efficiency%20and%20reliability%20of%0Abattery-powered%20systems.%20The%20existing%20methods%20to%20predict%20the%20Remaining%20Useful%0ALife%28RUL%29%20of%20Lithium-ion%20Batteries%20%28LiBs%29%20neglect%20the%20relational%20dependencies%0Aof%20the%20battery%20parameters%20to%20model%20the%20nonlinear%20degradation%20trajectories.%20We%0Apresent%20the%20Battery%20GraphNets%20framework%20that%20jointly%20learns%20to%20incorporate%20a%0Adiscrete%20dependency%20graph%20structure%20between%20battery%20parameters%20to%20capture%20the%0Acomplex%20interactions%20and%20the%20graph-learning%20algorithm%20to%20model%20the%20intrinsic%0Abattery%20degradation%20for%20RUL%20prognosis.%20The%20proposed%20method%20outperforms%20several%0Apopular%20methods%20by%20a%20significant%20margin%20on%20publicly%20available%20battery%20datasets%0Aand%20achieves%20SOTA%20performance.%20We%20report%20the%20ablation%20studies%20to%20support%20the%0Aefficacy%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07624v1&entry.124074799=Read"},
{"title": "Bayesian Learning in a Nonlinear Multiscale State-Space Model", "author": "Nayely V\u00e9lez-Cruz and Manfred D. Laubichler", "abstract": "  The ubiquity of multiscale interactions in complex systems is\nwell-recognized, with development and heredity serving as a prime example of\nhow processes at different temporal scales influence one another. This work\nintroduces a novel multiscale state-space model to explore the dynamic\ninterplay between systems interacting across different time scales, with\nfeedback between each scale. We propose a Bayesian learning framework to\nestimate unknown states by learning the unknown process noise covariances\nwithin this multiscale model. We develop a Particle Gibbs with Ancestor\nSampling (PGAS) algorithm for inference and demonstrate through simulations the\nefficacy of our approach.\n", "link": "http://arxiv.org/abs/2408.06425v2", "date": "2024-08-14", "relevancy": 1.92, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5959}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4602}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4535}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Bayesian%20Learning%20in%20a%20Nonlinear%20Multiscale%20State-Space%20Model&body=Title%3A%20Bayesian%20Learning%20in%20a%20Nonlinear%20Multiscale%20State-Space%20Model%0AAuthor%3A%20Nayely%20V%C3%A9lez-Cruz%20and%20Manfred%20D.%20Laubichler%0AAbstract%3A%20%20%20The%20ubiquity%20of%20multiscale%20interactions%20in%20complex%20systems%20is%0Awell-recognized%2C%20with%20development%20and%20heredity%20serving%20as%20a%20prime%20example%20of%0Ahow%20processes%20at%20different%20temporal%20scales%20influence%20one%20another.%20This%20work%0Aintroduces%20a%20novel%20multiscale%20state-space%20model%20to%20explore%20the%20dynamic%0Ainterplay%20between%20systems%20interacting%20across%20different%20time%20scales%2C%20with%0Afeedback%20between%20each%20scale.%20We%20propose%20a%20Bayesian%20learning%20framework%20to%0Aestimate%20unknown%20states%20by%20learning%20the%20unknown%20process%20noise%20covariances%0Awithin%20this%20multiscale%20model.%20We%20develop%20a%20Particle%20Gibbs%20with%20Ancestor%0ASampling%20%28PGAS%29%20algorithm%20for%20inference%20and%20demonstrate%20through%20simulations%20the%0Aefficacy%20of%20our%20approach.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.06425v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBayesian%2520Learning%2520in%2520a%2520Nonlinear%2520Multiscale%2520State-Space%2520Model%26entry.906535625%3DNayely%2520V%25C3%25A9lez-Cruz%2520and%2520Manfred%2520D.%2520Laubichler%26entry.1292438233%3D%2520%2520The%2520ubiquity%2520of%2520multiscale%2520interactions%2520in%2520complex%2520systems%2520is%250Awell-recognized%252C%2520with%2520development%2520and%2520heredity%2520serving%2520as%2520a%2520prime%2520example%2520of%250Ahow%2520processes%2520at%2520different%2520temporal%2520scales%2520influence%2520one%2520another.%2520This%2520work%250Aintroduces%2520a%2520novel%2520multiscale%2520state-space%2520model%2520to%2520explore%2520the%2520dynamic%250Ainterplay%2520between%2520systems%2520interacting%2520across%2520different%2520time%2520scales%252C%2520with%250Afeedback%2520between%2520each%2520scale.%2520We%2520propose%2520a%2520Bayesian%2520learning%2520framework%2520to%250Aestimate%2520unknown%2520states%2520by%2520learning%2520the%2520unknown%2520process%2520noise%2520covariances%250Awithin%2520this%2520multiscale%2520model.%2520We%2520develop%2520a%2520Particle%2520Gibbs%2520with%2520Ancestor%250ASampling%2520%2528PGAS%2529%2520algorithm%2520for%2520inference%2520and%2520demonstrate%2520through%2520simulations%2520the%250Aefficacy%2520of%2520our%2520approach.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.06425v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Bayesian%20Learning%20in%20a%20Nonlinear%20Multiscale%20State-Space%20Model&entry.906535625=Nayely%20V%C3%A9lez-Cruz%20and%20Manfred%20D.%20Laubichler&entry.1292438233=%20%20The%20ubiquity%20of%20multiscale%20interactions%20in%20complex%20systems%20is%0Awell-recognized%2C%20with%20development%20and%20heredity%20serving%20as%20a%20prime%20example%20of%0Ahow%20processes%20at%20different%20temporal%20scales%20influence%20one%20another.%20This%20work%0Aintroduces%20a%20novel%20multiscale%20state-space%20model%20to%20explore%20the%20dynamic%0Ainterplay%20between%20systems%20interacting%20across%20different%20time%20scales%2C%20with%0Afeedback%20between%20each%20scale.%20We%20propose%20a%20Bayesian%20learning%20framework%20to%0Aestimate%20unknown%20states%20by%20learning%20the%20unknown%20process%20noise%20covariances%0Awithin%20this%20multiscale%20model.%20We%20develop%20a%20Particle%20Gibbs%20with%20Ancestor%0ASampling%20%28PGAS%29%20algorithm%20for%20inference%20and%20demonstrate%20through%20simulations%20the%0Aefficacy%20of%20our%20approach.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.06425v2&entry.124074799=Read"},
{"title": "Interpretable Graph Neural Networks for Heterogeneous Tabular Data", "author": "Amr Alkhatib and Henrik Bostr\u00f6m", "abstract": "  Many machine learning algorithms for tabular data produce black-box models,\nwhich prevent users from understanding the rationale behind the model\npredictions. In their unconstrained form, graph neural networks fall into this\ncategory, and they have further limited abilities to handle heterogeneous data.\nTo overcome these limitations, an approach is proposed, called IGNH\n(Interpretable Graph Neural Network for Heterogeneous tabular data), which\nhandles both categorical and numerical features, while constraining the\nlearning process to generate exact feature attributions together with the\npredictions. A large-scale empirical investigation is presented, showing that\nthe feature attributions provided by IGNH align with Shapley values that are\ncomputed post hoc. Furthermore, the results show that IGNH outperforms two\npowerful machine learning algorithms for tabular data, Random Forests and\nTabNet, while reaching a similar level of performance as XGBoost.\n", "link": "http://arxiv.org/abs/2408.07661v1", "date": "2024-08-14", "relevancy": 1.9178, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4965}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.469}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4665}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Interpretable%20Graph%20Neural%20Networks%20for%20Heterogeneous%20Tabular%20Data&body=Title%3A%20Interpretable%20Graph%20Neural%20Networks%20for%20Heterogeneous%20Tabular%20Data%0AAuthor%3A%20Amr%20Alkhatib%20and%20Henrik%20Bostr%C3%B6m%0AAbstract%3A%20%20%20Many%20machine%20learning%20algorithms%20for%20tabular%20data%20produce%20black-box%20models%2C%0Awhich%20prevent%20users%20from%20understanding%20the%20rationale%20behind%20the%20model%0Apredictions.%20In%20their%20unconstrained%20form%2C%20graph%20neural%20networks%20fall%20into%20this%0Acategory%2C%20and%20they%20have%20further%20limited%20abilities%20to%20handle%20heterogeneous%20data.%0ATo%20overcome%20these%20limitations%2C%20an%20approach%20is%20proposed%2C%20called%20IGNH%0A%28Interpretable%20Graph%20Neural%20Network%20for%20Heterogeneous%20tabular%20data%29%2C%20which%0Ahandles%20both%20categorical%20and%20numerical%20features%2C%20while%20constraining%20the%0Alearning%20process%20to%20generate%20exact%20feature%20attributions%20together%20with%20the%0Apredictions.%20A%20large-scale%20empirical%20investigation%20is%20presented%2C%20showing%20that%0Athe%20feature%20attributions%20provided%20by%20IGNH%20align%20with%20Shapley%20values%20that%20are%0Acomputed%20post%20hoc.%20Furthermore%2C%20the%20results%20show%20that%20IGNH%20outperforms%20two%0Apowerful%20machine%20learning%20algorithms%20for%20tabular%20data%2C%20Random%20Forests%20and%0ATabNet%2C%20while%20reaching%20a%20similar%20level%20of%20performance%20as%20XGBoost.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07661v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInterpretable%2520Graph%2520Neural%2520Networks%2520for%2520Heterogeneous%2520Tabular%2520Data%26entry.906535625%3DAmr%2520Alkhatib%2520and%2520Henrik%2520Bostr%25C3%25B6m%26entry.1292438233%3D%2520%2520Many%2520machine%2520learning%2520algorithms%2520for%2520tabular%2520data%2520produce%2520black-box%2520models%252C%250Awhich%2520prevent%2520users%2520from%2520understanding%2520the%2520rationale%2520behind%2520the%2520model%250Apredictions.%2520In%2520their%2520unconstrained%2520form%252C%2520graph%2520neural%2520networks%2520fall%2520into%2520this%250Acategory%252C%2520and%2520they%2520have%2520further%2520limited%2520abilities%2520to%2520handle%2520heterogeneous%2520data.%250ATo%2520overcome%2520these%2520limitations%252C%2520an%2520approach%2520is%2520proposed%252C%2520called%2520IGNH%250A%2528Interpretable%2520Graph%2520Neural%2520Network%2520for%2520Heterogeneous%2520tabular%2520data%2529%252C%2520which%250Ahandles%2520both%2520categorical%2520and%2520numerical%2520features%252C%2520while%2520constraining%2520the%250Alearning%2520process%2520to%2520generate%2520exact%2520feature%2520attributions%2520together%2520with%2520the%250Apredictions.%2520A%2520large-scale%2520empirical%2520investigation%2520is%2520presented%252C%2520showing%2520that%250Athe%2520feature%2520attributions%2520provided%2520by%2520IGNH%2520align%2520with%2520Shapley%2520values%2520that%2520are%250Acomputed%2520post%2520hoc.%2520Furthermore%252C%2520the%2520results%2520show%2520that%2520IGNH%2520outperforms%2520two%250Apowerful%2520machine%2520learning%2520algorithms%2520for%2520tabular%2520data%252C%2520Random%2520Forests%2520and%250ATabNet%252C%2520while%2520reaching%2520a%2520similar%2520level%2520of%2520performance%2520as%2520XGBoost.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07661v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Interpretable%20Graph%20Neural%20Networks%20for%20Heterogeneous%20Tabular%20Data&entry.906535625=Amr%20Alkhatib%20and%20Henrik%20Bostr%C3%B6m&entry.1292438233=%20%20Many%20machine%20learning%20algorithms%20for%20tabular%20data%20produce%20black-box%20models%2C%0Awhich%20prevent%20users%20from%20understanding%20the%20rationale%20behind%20the%20model%0Apredictions.%20In%20their%20unconstrained%20form%2C%20graph%20neural%20networks%20fall%20into%20this%0Acategory%2C%20and%20they%20have%20further%20limited%20abilities%20to%20handle%20heterogeneous%20data.%0ATo%20overcome%20these%20limitations%2C%20an%20approach%20is%20proposed%2C%20called%20IGNH%0A%28Interpretable%20Graph%20Neural%20Network%20for%20Heterogeneous%20tabular%20data%29%2C%20which%0Ahandles%20both%20categorical%20and%20numerical%20features%2C%20while%20constraining%20the%0Alearning%20process%20to%20generate%20exact%20feature%20attributions%20together%20with%20the%0Apredictions.%20A%20large-scale%20empirical%20investigation%20is%20presented%2C%20showing%20that%0Athe%20feature%20attributions%20provided%20by%20IGNH%20align%20with%20Shapley%20values%20that%20are%0Acomputed%20post%20hoc.%20Furthermore%2C%20the%20results%20show%20that%20IGNH%20outperforms%20two%0Apowerful%20machine%20learning%20algorithms%20for%20tabular%20data%2C%20Random%20Forests%20and%0ATabNet%2C%20while%20reaching%20a%20similar%20level%20of%20performance%20as%20XGBoost.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07661v1&entry.124074799=Read"},
{"title": "Multi-task Heterogeneous Graph Learning on Electronic Health Records", "author": "Tsai Hor Chan and Guosheng Yin and Kyongtae Bae and Lequan Yu", "abstract": "  Learning electronic health records (EHRs) has received emerging attention\nbecause of its capability to facilitate accurate medical diagnosis. Since the\nEHRs contain enriched information specifying complex interactions between\nentities, modeling EHRs with graphs is shown to be effective in practice. The\nEHRs, however, present a great degree of heterogeneity, sparsity, and\ncomplexity, which hamper the performance of most of the models applied to them.\nMoreover, existing approaches modeling EHRs often focus on learning the\nrepresentations for a single task, overlooking the multi-task nature of EHR\nanalysis problems and resulting in limited generalizability across different\ntasks. In view of these limitations, we propose a novel framework for EHR\nmodeling, namely MulT-EHR (Multi-Task EHR), which leverages a heterogeneous\ngraph to mine the complex relations and model the heterogeneity in the EHRs. To\nmitigate the large degree of noise, we introduce a denoising module based on\nthe causal inference framework to adjust for severe confounding effects and\nreduce noise in the EHR data. Additionally, since our model adopts a single\ngraph neural network for simultaneous multi-task prediction, we design a\nmulti-task learning module to leverage the inter-task knowledge to regularize\nthe training process. Extensive empirical studies on MIMIC-III and MIMIC-IV\ndatasets validate that the proposed method consistently outperforms the\nstate-of-the-art designs in four popular EHR analysis tasks -- drug\nrecommendation, and predictions of the length of stay, mortality, and\nreadmission. Thorough ablation studies demonstrate the robustness of our method\nupon variations to key components and hyperparameters.\n", "link": "http://arxiv.org/abs/2408.07569v1", "date": "2024-08-14", "relevancy": 1.8882, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4893}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4699}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4673}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Multi-task%20Heterogeneous%20Graph%20Learning%20on%20Electronic%20Health%20Records&body=Title%3A%20Multi-task%20Heterogeneous%20Graph%20Learning%20on%20Electronic%20Health%20Records%0AAuthor%3A%20Tsai%20Hor%20Chan%20and%20Guosheng%20Yin%20and%20Kyongtae%20Bae%20and%20Lequan%20Yu%0AAbstract%3A%20%20%20Learning%20electronic%20health%20records%20%28EHRs%29%20has%20received%20emerging%20attention%0Abecause%20of%20its%20capability%20to%20facilitate%20accurate%20medical%20diagnosis.%20Since%20the%0AEHRs%20contain%20enriched%20information%20specifying%20complex%20interactions%20between%0Aentities%2C%20modeling%20EHRs%20with%20graphs%20is%20shown%20to%20be%20effective%20in%20practice.%20The%0AEHRs%2C%20however%2C%20present%20a%20great%20degree%20of%20heterogeneity%2C%20sparsity%2C%20and%0Acomplexity%2C%20which%20hamper%20the%20performance%20of%20most%20of%20the%20models%20applied%20to%20them.%0AMoreover%2C%20existing%20approaches%20modeling%20EHRs%20often%20focus%20on%20learning%20the%0Arepresentations%20for%20a%20single%20task%2C%20overlooking%20the%20multi-task%20nature%20of%20EHR%0Aanalysis%20problems%20and%20resulting%20in%20limited%20generalizability%20across%20different%0Atasks.%20In%20view%20of%20these%20limitations%2C%20we%20propose%20a%20novel%20framework%20for%20EHR%0Amodeling%2C%20namely%20MulT-EHR%20%28Multi-Task%20EHR%29%2C%20which%20leverages%20a%20heterogeneous%0Agraph%20to%20mine%20the%20complex%20relations%20and%20model%20the%20heterogeneity%20in%20the%20EHRs.%20To%0Amitigate%20the%20large%20degree%20of%20noise%2C%20we%20introduce%20a%20denoising%20module%20based%20on%0Athe%20causal%20inference%20framework%20to%20adjust%20for%20severe%20confounding%20effects%20and%0Areduce%20noise%20in%20the%20EHR%20data.%20Additionally%2C%20since%20our%20model%20adopts%20a%20single%0Agraph%20neural%20network%20for%20simultaneous%20multi-task%20prediction%2C%20we%20design%20a%0Amulti-task%20learning%20module%20to%20leverage%20the%20inter-task%20knowledge%20to%20regularize%0Athe%20training%20process.%20Extensive%20empirical%20studies%20on%20MIMIC-III%20and%20MIMIC-IV%0Adatasets%20validate%20that%20the%20proposed%20method%20consistently%20outperforms%20the%0Astate-of-the-art%20designs%20in%20four%20popular%20EHR%20analysis%20tasks%20--%20drug%0Arecommendation%2C%20and%20predictions%20of%20the%20length%20of%20stay%2C%20mortality%2C%20and%0Areadmission.%20Thorough%20ablation%20studies%20demonstrate%20the%20robustness%20of%20our%20method%0Aupon%20variations%20to%20key%20components%20and%20hyperparameters.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07569v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMulti-task%2520Heterogeneous%2520Graph%2520Learning%2520on%2520Electronic%2520Health%2520Records%26entry.906535625%3DTsai%2520Hor%2520Chan%2520and%2520Guosheng%2520Yin%2520and%2520Kyongtae%2520Bae%2520and%2520Lequan%2520Yu%26entry.1292438233%3D%2520%2520Learning%2520electronic%2520health%2520records%2520%2528EHRs%2529%2520has%2520received%2520emerging%2520attention%250Abecause%2520of%2520its%2520capability%2520to%2520facilitate%2520accurate%2520medical%2520diagnosis.%2520Since%2520the%250AEHRs%2520contain%2520enriched%2520information%2520specifying%2520complex%2520interactions%2520between%250Aentities%252C%2520modeling%2520EHRs%2520with%2520graphs%2520is%2520shown%2520to%2520be%2520effective%2520in%2520practice.%2520The%250AEHRs%252C%2520however%252C%2520present%2520a%2520great%2520degree%2520of%2520heterogeneity%252C%2520sparsity%252C%2520and%250Acomplexity%252C%2520which%2520hamper%2520the%2520performance%2520of%2520most%2520of%2520the%2520models%2520applied%2520to%2520them.%250AMoreover%252C%2520existing%2520approaches%2520modeling%2520EHRs%2520often%2520focus%2520on%2520learning%2520the%250Arepresentations%2520for%2520a%2520single%2520task%252C%2520overlooking%2520the%2520multi-task%2520nature%2520of%2520EHR%250Aanalysis%2520problems%2520and%2520resulting%2520in%2520limited%2520generalizability%2520across%2520different%250Atasks.%2520In%2520view%2520of%2520these%2520limitations%252C%2520we%2520propose%2520a%2520novel%2520framework%2520for%2520EHR%250Amodeling%252C%2520namely%2520MulT-EHR%2520%2528Multi-Task%2520EHR%2529%252C%2520which%2520leverages%2520a%2520heterogeneous%250Agraph%2520to%2520mine%2520the%2520complex%2520relations%2520and%2520model%2520the%2520heterogeneity%2520in%2520the%2520EHRs.%2520To%250Amitigate%2520the%2520large%2520degree%2520of%2520noise%252C%2520we%2520introduce%2520a%2520denoising%2520module%2520based%2520on%250Athe%2520causal%2520inference%2520framework%2520to%2520adjust%2520for%2520severe%2520confounding%2520effects%2520and%250Areduce%2520noise%2520in%2520the%2520EHR%2520data.%2520Additionally%252C%2520since%2520our%2520model%2520adopts%2520a%2520single%250Agraph%2520neural%2520network%2520for%2520simultaneous%2520multi-task%2520prediction%252C%2520we%2520design%2520a%250Amulti-task%2520learning%2520module%2520to%2520leverage%2520the%2520inter-task%2520knowledge%2520to%2520regularize%250Athe%2520training%2520process.%2520Extensive%2520empirical%2520studies%2520on%2520MIMIC-III%2520and%2520MIMIC-IV%250Adatasets%2520validate%2520that%2520the%2520proposed%2520method%2520consistently%2520outperforms%2520the%250Astate-of-the-art%2520designs%2520in%2520four%2520popular%2520EHR%2520analysis%2520tasks%2520--%2520drug%250Arecommendation%252C%2520and%2520predictions%2520of%2520the%2520length%2520of%2520stay%252C%2520mortality%252C%2520and%250Areadmission.%2520Thorough%2520ablation%2520studies%2520demonstrate%2520the%2520robustness%2520of%2520our%2520method%250Aupon%2520variations%2520to%2520key%2520components%2520and%2520hyperparameters.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07569v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Multi-task%20Heterogeneous%20Graph%20Learning%20on%20Electronic%20Health%20Records&entry.906535625=Tsai%20Hor%20Chan%20and%20Guosheng%20Yin%20and%20Kyongtae%20Bae%20and%20Lequan%20Yu&entry.1292438233=%20%20Learning%20electronic%20health%20records%20%28EHRs%29%20has%20received%20emerging%20attention%0Abecause%20of%20its%20capability%20to%20facilitate%20accurate%20medical%20diagnosis.%20Since%20the%0AEHRs%20contain%20enriched%20information%20specifying%20complex%20interactions%20between%0Aentities%2C%20modeling%20EHRs%20with%20graphs%20is%20shown%20to%20be%20effective%20in%20practice.%20The%0AEHRs%2C%20however%2C%20present%20a%20great%20degree%20of%20heterogeneity%2C%20sparsity%2C%20and%0Acomplexity%2C%20which%20hamper%20the%20performance%20of%20most%20of%20the%20models%20applied%20to%20them.%0AMoreover%2C%20existing%20approaches%20modeling%20EHRs%20often%20focus%20on%20learning%20the%0Arepresentations%20for%20a%20single%20task%2C%20overlooking%20the%20multi-task%20nature%20of%20EHR%0Aanalysis%20problems%20and%20resulting%20in%20limited%20generalizability%20across%20different%0Atasks.%20In%20view%20of%20these%20limitations%2C%20we%20propose%20a%20novel%20framework%20for%20EHR%0Amodeling%2C%20namely%20MulT-EHR%20%28Multi-Task%20EHR%29%2C%20which%20leverages%20a%20heterogeneous%0Agraph%20to%20mine%20the%20complex%20relations%20and%20model%20the%20heterogeneity%20in%20the%20EHRs.%20To%0Amitigate%20the%20large%20degree%20of%20noise%2C%20we%20introduce%20a%20denoising%20module%20based%20on%0Athe%20causal%20inference%20framework%20to%20adjust%20for%20severe%20confounding%20effects%20and%0Areduce%20noise%20in%20the%20EHR%20data.%20Additionally%2C%20since%20our%20model%20adopts%20a%20single%0Agraph%20neural%20network%20for%20simultaneous%20multi-task%20prediction%2C%20we%20design%20a%0Amulti-task%20learning%20module%20to%20leverage%20the%20inter-task%20knowledge%20to%20regularize%0Athe%20training%20process.%20Extensive%20empirical%20studies%20on%20MIMIC-III%20and%20MIMIC-IV%0Adatasets%20validate%20that%20the%20proposed%20method%20consistently%20outperforms%20the%0Astate-of-the-art%20designs%20in%20four%20popular%20EHR%20analysis%20tasks%20--%20drug%0Arecommendation%2C%20and%20predictions%20of%20the%20length%20of%20stay%2C%20mortality%2C%20and%0Areadmission.%20Thorough%20ablation%20studies%20demonstrate%20the%20robustness%20of%20our%20method%0Aupon%20variations%20to%20key%20components%20and%20hyperparameters.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07569v1&entry.124074799=Read"},
{"title": "DeCo: Decoupled Human-Centered Diffusion Video Editing with Motion\n  Consistency", "author": "Xiaojing Zhong and Xinyi Huang and Xiaofeng Yang and Guosheng Lin and Qingyao Wu", "abstract": "  Diffusion models usher a new era of video editing, flexibly manipulating the\nvideo contents with text prompts. Despite the widespread application demand in\nediting human-centered videos, these models face significant challenges in\nhandling complex objects like humans. In this paper, we introduce DeCo, a novel\nvideo editing framework specifically designed to treat humans and the\nbackground as separate editable targets, ensuring global spatial-temporal\nconsistency by maintaining the coherence of each individual component.\nSpecifically, we propose a decoupled dynamic human representation that utilizes\na parametric human body prior to generate tailored humans while preserving the\nconsistent motions as the original video. In addition, we consider the\nbackground as a layered atlas to apply text-guided image editing approaches on\nit. To further enhance the geometry and texture of humans during the\noptimization, we extend the calculation of score distillation sampling into\nnormal space and image space. Moreover, we tackle inconsistent lighting between\nthe edited targets by leveraging a lighting-aware video harmonizer, a problem\npreviously overlooked in decompose-edit-combine approaches. Extensive\nqualitative and numerical experiments demonstrate that DeCo outperforms prior\nvideo editing methods in human-centered videos, especially in longer videos.\n", "link": "http://arxiv.org/abs/2408.07481v1", "date": "2024-08-14", "relevancy": 1.8821, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6729}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6239}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.5905}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20DeCo%3A%20Decoupled%20Human-Centered%20Diffusion%20Video%20Editing%20with%20Motion%0A%20%20Consistency&body=Title%3A%20DeCo%3A%20Decoupled%20Human-Centered%20Diffusion%20Video%20Editing%20with%20Motion%0A%20%20Consistency%0AAuthor%3A%20Xiaojing%20Zhong%20and%20Xinyi%20Huang%20and%20Xiaofeng%20Yang%20and%20Guosheng%20Lin%20and%20Qingyao%20Wu%0AAbstract%3A%20%20%20Diffusion%20models%20usher%20a%20new%20era%20of%20video%20editing%2C%20flexibly%20manipulating%20the%0Avideo%20contents%20with%20text%20prompts.%20Despite%20the%20widespread%20application%20demand%20in%0Aediting%20human-centered%20videos%2C%20these%20models%20face%20significant%20challenges%20in%0Ahandling%20complex%20objects%20like%20humans.%20In%20this%20paper%2C%20we%20introduce%20DeCo%2C%20a%20novel%0Avideo%20editing%20framework%20specifically%20designed%20to%20treat%20humans%20and%20the%0Abackground%20as%20separate%20editable%20targets%2C%20ensuring%20global%20spatial-temporal%0Aconsistency%20by%20maintaining%20the%20coherence%20of%20each%20individual%20component.%0ASpecifically%2C%20we%20propose%20a%20decoupled%20dynamic%20human%20representation%20that%20utilizes%0Aa%20parametric%20human%20body%20prior%20to%20generate%20tailored%20humans%20while%20preserving%20the%0Aconsistent%20motions%20as%20the%20original%20video.%20In%20addition%2C%20we%20consider%20the%0Abackground%20as%20a%20layered%20atlas%20to%20apply%20text-guided%20image%20editing%20approaches%20on%0Ait.%20To%20further%20enhance%20the%20geometry%20and%20texture%20of%20humans%20during%20the%0Aoptimization%2C%20we%20extend%20the%20calculation%20of%20score%20distillation%20sampling%20into%0Anormal%20space%20and%20image%20space.%20Moreover%2C%20we%20tackle%20inconsistent%20lighting%20between%0Athe%20edited%20targets%20by%20leveraging%20a%20lighting-aware%20video%20harmonizer%2C%20a%20problem%0Apreviously%20overlooked%20in%20decompose-edit-combine%20approaches.%20Extensive%0Aqualitative%20and%20numerical%20experiments%20demonstrate%20that%20DeCo%20outperforms%20prior%0Avideo%20editing%20methods%20in%20human-centered%20videos%2C%20especially%20in%20longer%20videos.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07481v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDeCo%253A%2520Decoupled%2520Human-Centered%2520Diffusion%2520Video%2520Editing%2520with%2520Motion%250A%2520%2520Consistency%26entry.906535625%3DXiaojing%2520Zhong%2520and%2520Xinyi%2520Huang%2520and%2520Xiaofeng%2520Yang%2520and%2520Guosheng%2520Lin%2520and%2520Qingyao%2520Wu%26entry.1292438233%3D%2520%2520Diffusion%2520models%2520usher%2520a%2520new%2520era%2520of%2520video%2520editing%252C%2520flexibly%2520manipulating%2520the%250Avideo%2520contents%2520with%2520text%2520prompts.%2520Despite%2520the%2520widespread%2520application%2520demand%2520in%250Aediting%2520human-centered%2520videos%252C%2520these%2520models%2520face%2520significant%2520challenges%2520in%250Ahandling%2520complex%2520objects%2520like%2520humans.%2520In%2520this%2520paper%252C%2520we%2520introduce%2520DeCo%252C%2520a%2520novel%250Avideo%2520editing%2520framework%2520specifically%2520designed%2520to%2520treat%2520humans%2520and%2520the%250Abackground%2520as%2520separate%2520editable%2520targets%252C%2520ensuring%2520global%2520spatial-temporal%250Aconsistency%2520by%2520maintaining%2520the%2520coherence%2520of%2520each%2520individual%2520component.%250ASpecifically%252C%2520we%2520propose%2520a%2520decoupled%2520dynamic%2520human%2520representation%2520that%2520utilizes%250Aa%2520parametric%2520human%2520body%2520prior%2520to%2520generate%2520tailored%2520humans%2520while%2520preserving%2520the%250Aconsistent%2520motions%2520as%2520the%2520original%2520video.%2520In%2520addition%252C%2520we%2520consider%2520the%250Abackground%2520as%2520a%2520layered%2520atlas%2520to%2520apply%2520text-guided%2520image%2520editing%2520approaches%2520on%250Ait.%2520To%2520further%2520enhance%2520the%2520geometry%2520and%2520texture%2520of%2520humans%2520during%2520the%250Aoptimization%252C%2520we%2520extend%2520the%2520calculation%2520of%2520score%2520distillation%2520sampling%2520into%250Anormal%2520space%2520and%2520image%2520space.%2520Moreover%252C%2520we%2520tackle%2520inconsistent%2520lighting%2520between%250Athe%2520edited%2520targets%2520by%2520leveraging%2520a%2520lighting-aware%2520video%2520harmonizer%252C%2520a%2520problem%250Apreviously%2520overlooked%2520in%2520decompose-edit-combine%2520approaches.%2520Extensive%250Aqualitative%2520and%2520numerical%2520experiments%2520demonstrate%2520that%2520DeCo%2520outperforms%2520prior%250Avideo%2520editing%2520methods%2520in%2520human-centered%2520videos%252C%2520especially%2520in%2520longer%2520videos.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07481v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=DeCo%3A%20Decoupled%20Human-Centered%20Diffusion%20Video%20Editing%20with%20Motion%0A%20%20Consistency&entry.906535625=Xiaojing%20Zhong%20and%20Xinyi%20Huang%20and%20Xiaofeng%20Yang%20and%20Guosheng%20Lin%20and%20Qingyao%20Wu&entry.1292438233=%20%20Diffusion%20models%20usher%20a%20new%20era%20of%20video%20editing%2C%20flexibly%20manipulating%20the%0Avideo%20contents%20with%20text%20prompts.%20Despite%20the%20widespread%20application%20demand%20in%0Aediting%20human-centered%20videos%2C%20these%20models%20face%20significant%20challenges%20in%0Ahandling%20complex%20objects%20like%20humans.%20In%20this%20paper%2C%20we%20introduce%20DeCo%2C%20a%20novel%0Avideo%20editing%20framework%20specifically%20designed%20to%20treat%20humans%20and%20the%0Abackground%20as%20separate%20editable%20targets%2C%20ensuring%20global%20spatial-temporal%0Aconsistency%20by%20maintaining%20the%20coherence%20of%20each%20individual%20component.%0ASpecifically%2C%20we%20propose%20a%20decoupled%20dynamic%20human%20representation%20that%20utilizes%0Aa%20parametric%20human%20body%20prior%20to%20generate%20tailored%20humans%20while%20preserving%20the%0Aconsistent%20motions%20as%20the%20original%20video.%20In%20addition%2C%20we%20consider%20the%0Abackground%20as%20a%20layered%20atlas%20to%20apply%20text-guided%20image%20editing%20approaches%20on%0Ait.%20To%20further%20enhance%20the%20geometry%20and%20texture%20of%20humans%20during%20the%0Aoptimization%2C%20we%20extend%20the%20calculation%20of%20score%20distillation%20sampling%20into%0Anormal%20space%20and%20image%20space.%20Moreover%2C%20we%20tackle%20inconsistent%20lighting%20between%0Athe%20edited%20targets%20by%20leveraging%20a%20lighting-aware%20video%20harmonizer%2C%20a%20problem%0Apreviously%20overlooked%20in%20decompose-edit-combine%20approaches.%20Extensive%0Aqualitative%20and%20numerical%20experiments%20demonstrate%20that%20DeCo%20outperforms%20prior%0Avideo%20editing%20methods%20in%20human-centered%20videos%2C%20especially%20in%20longer%20videos.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07481v1&entry.124074799=Read"},
{"title": "MagicFace: Training-free Universal-Style Human Image Customized\n  Synthesis", "author": "Yibin Wang and Weizhong Zhang and Cheng Jin", "abstract": "  Existing human image personalized generation methods often require tedious\ntraining: either fine-tuning with a few images or retraining on large-scale\ndatasets. In such cases, these methods are prone to overfitting and encounter\ndifficulties when personalizing individuals of diverse styles. Moreover, these\ntraining-based approaches also struggle with multi-concept human image\ncustomizing. To this end, we propose MagicFace, the first method for\nuniversal-style human image personalized synthesis that enables\nsingle/multi-concept customization for humans of any style in a training-free\nmanner. MagicFace introduces a coarse-to-fine generation pipeline, involving\ntwo sequential stages: semantic scene construction and concept feature\ninjection. This is achieved by our Reference-aware Self-Attention (RSA) and\nRegion-grouped Blend Attention (RBA) mechanisms. Specifically, in the first\nstage, RSA enables the latent image to query features from reference concepts\nsimultaneously, extracting the coarse-grained overall semantic understanding to\nfacilitate the initial semantic layout establishment. In the second stage, we\nemploy an attention-based semantic segmentation method to pinpoint the\ngenerated regions of all concepts in the latent image at each step. Following\nthis, RBA divides the pixels of the latent image into semantic groups, with\neach group querying fine-grained features from its reference concept, which\nensures precise attribute alignment and feature injection. Throughout the\ntwo-stage process, a weight mask strategy is employed to ensure the model\nfocuses more on the reference concepts. Extensive experiments demonstrate our\nsuperiority in both human-centric subject-to-image synthesis and multi-concept\nhuman image customization. Our approach also can be applied to texture\ntransformation, further enhancing its versatility and applicability.\n", "link": "http://arxiv.org/abs/2408.07433v1", "date": "2024-08-14", "relevancy": 1.8821, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.6616}, {"title": "Total Selfie: Generating Full-Body Selfies", "link": "http://arxiv.org/abs/2308.14740v2", "similarity": 0.5905}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5788}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MagicFace%3A%20Training-free%20Universal-Style%20Human%20Image%20Customized%0A%20%20Synthesis&body=Title%3A%20MagicFace%3A%20Training-free%20Universal-Style%20Human%20Image%20Customized%0A%20%20Synthesis%0AAuthor%3A%20Yibin%20Wang%20and%20Weizhong%20Zhang%20and%20Cheng%20Jin%0AAbstract%3A%20%20%20Existing%20human%20image%20personalized%20generation%20methods%20often%20require%20tedious%0Atraining%3A%20either%20fine-tuning%20with%20a%20few%20images%20or%20retraining%20on%20large-scale%0Adatasets.%20In%20such%20cases%2C%20these%20methods%20are%20prone%20to%20overfitting%20and%20encounter%0Adifficulties%20when%20personalizing%20individuals%20of%20diverse%20styles.%20Moreover%2C%20these%0Atraining-based%20approaches%20also%20struggle%20with%20multi-concept%20human%20image%0Acustomizing.%20To%20this%20end%2C%20we%20propose%20MagicFace%2C%20the%20first%20method%20for%0Auniversal-style%20human%20image%20personalized%20synthesis%20that%20enables%0Asingle/multi-concept%20customization%20for%20humans%20of%20any%20style%20in%20a%20training-free%0Amanner.%20MagicFace%20introduces%20a%20coarse-to-fine%20generation%20pipeline%2C%20involving%0Atwo%20sequential%20stages%3A%20semantic%20scene%20construction%20and%20concept%20feature%0Ainjection.%20This%20is%20achieved%20by%20our%20Reference-aware%20Self-Attention%20%28RSA%29%20and%0ARegion-grouped%20Blend%20Attention%20%28RBA%29%20mechanisms.%20Specifically%2C%20in%20the%20first%0Astage%2C%20RSA%20enables%20the%20latent%20image%20to%20query%20features%20from%20reference%20concepts%0Asimultaneously%2C%20extracting%20the%20coarse-grained%20overall%20semantic%20understanding%20to%0Afacilitate%20the%20initial%20semantic%20layout%20establishment.%20In%20the%20second%20stage%2C%20we%0Aemploy%20an%20attention-based%20semantic%20segmentation%20method%20to%20pinpoint%20the%0Agenerated%20regions%20of%20all%20concepts%20in%20the%20latent%20image%20at%20each%20step.%20Following%0Athis%2C%20RBA%20divides%20the%20pixels%20of%20the%20latent%20image%20into%20semantic%20groups%2C%20with%0Aeach%20group%20querying%20fine-grained%20features%20from%20its%20reference%20concept%2C%20which%0Aensures%20precise%20attribute%20alignment%20and%20feature%20injection.%20Throughout%20the%0Atwo-stage%20process%2C%20a%20weight%20mask%20strategy%20is%20employed%20to%20ensure%20the%20model%0Afocuses%20more%20on%20the%20reference%20concepts.%20Extensive%20experiments%20demonstrate%20our%0Asuperiority%20in%20both%20human-centric%20subject-to-image%20synthesis%20and%20multi-concept%0Ahuman%20image%20customization.%20Our%20approach%20also%20can%20be%20applied%20to%20texture%0Atransformation%2C%20further%20enhancing%20its%20versatility%20and%20applicability.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07433v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMagicFace%253A%2520Training-free%2520Universal-Style%2520Human%2520Image%2520Customized%250A%2520%2520Synthesis%26entry.906535625%3DYibin%2520Wang%2520and%2520Weizhong%2520Zhang%2520and%2520Cheng%2520Jin%26entry.1292438233%3D%2520%2520Existing%2520human%2520image%2520personalized%2520generation%2520methods%2520often%2520require%2520tedious%250Atraining%253A%2520either%2520fine-tuning%2520with%2520a%2520few%2520images%2520or%2520retraining%2520on%2520large-scale%250Adatasets.%2520In%2520such%2520cases%252C%2520these%2520methods%2520are%2520prone%2520to%2520overfitting%2520and%2520encounter%250Adifficulties%2520when%2520personalizing%2520individuals%2520of%2520diverse%2520styles.%2520Moreover%252C%2520these%250Atraining-based%2520approaches%2520also%2520struggle%2520with%2520multi-concept%2520human%2520image%250Acustomizing.%2520To%2520this%2520end%252C%2520we%2520propose%2520MagicFace%252C%2520the%2520first%2520method%2520for%250Auniversal-style%2520human%2520image%2520personalized%2520synthesis%2520that%2520enables%250Asingle/multi-concept%2520customization%2520for%2520humans%2520of%2520any%2520style%2520in%2520a%2520training-free%250Amanner.%2520MagicFace%2520introduces%2520a%2520coarse-to-fine%2520generation%2520pipeline%252C%2520involving%250Atwo%2520sequential%2520stages%253A%2520semantic%2520scene%2520construction%2520and%2520concept%2520feature%250Ainjection.%2520This%2520is%2520achieved%2520by%2520our%2520Reference-aware%2520Self-Attention%2520%2528RSA%2529%2520and%250ARegion-grouped%2520Blend%2520Attention%2520%2528RBA%2529%2520mechanisms.%2520Specifically%252C%2520in%2520the%2520first%250Astage%252C%2520RSA%2520enables%2520the%2520latent%2520image%2520to%2520query%2520features%2520from%2520reference%2520concepts%250Asimultaneously%252C%2520extracting%2520the%2520coarse-grained%2520overall%2520semantic%2520understanding%2520to%250Afacilitate%2520the%2520initial%2520semantic%2520layout%2520establishment.%2520In%2520the%2520second%2520stage%252C%2520we%250Aemploy%2520an%2520attention-based%2520semantic%2520segmentation%2520method%2520to%2520pinpoint%2520the%250Agenerated%2520regions%2520of%2520all%2520concepts%2520in%2520the%2520latent%2520image%2520at%2520each%2520step.%2520Following%250Athis%252C%2520RBA%2520divides%2520the%2520pixels%2520of%2520the%2520latent%2520image%2520into%2520semantic%2520groups%252C%2520with%250Aeach%2520group%2520querying%2520fine-grained%2520features%2520from%2520its%2520reference%2520concept%252C%2520which%250Aensures%2520precise%2520attribute%2520alignment%2520and%2520feature%2520injection.%2520Throughout%2520the%250Atwo-stage%2520process%252C%2520a%2520weight%2520mask%2520strategy%2520is%2520employed%2520to%2520ensure%2520the%2520model%250Afocuses%2520more%2520on%2520the%2520reference%2520concepts.%2520Extensive%2520experiments%2520demonstrate%2520our%250Asuperiority%2520in%2520both%2520human-centric%2520subject-to-image%2520synthesis%2520and%2520multi-concept%250Ahuman%2520image%2520customization.%2520Our%2520approach%2520also%2520can%2520be%2520applied%2520to%2520texture%250Atransformation%252C%2520further%2520enhancing%2520its%2520versatility%2520and%2520applicability.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07433v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MagicFace%3A%20Training-free%20Universal-Style%20Human%20Image%20Customized%0A%20%20Synthesis&entry.906535625=Yibin%20Wang%20and%20Weizhong%20Zhang%20and%20Cheng%20Jin&entry.1292438233=%20%20Existing%20human%20image%20personalized%20generation%20methods%20often%20require%20tedious%0Atraining%3A%20either%20fine-tuning%20with%20a%20few%20images%20or%20retraining%20on%20large-scale%0Adatasets.%20In%20such%20cases%2C%20these%20methods%20are%20prone%20to%20overfitting%20and%20encounter%0Adifficulties%20when%20personalizing%20individuals%20of%20diverse%20styles.%20Moreover%2C%20these%0Atraining-based%20approaches%20also%20struggle%20with%20multi-concept%20human%20image%0Acustomizing.%20To%20this%20end%2C%20we%20propose%20MagicFace%2C%20the%20first%20method%20for%0Auniversal-style%20human%20image%20personalized%20synthesis%20that%20enables%0Asingle/multi-concept%20customization%20for%20humans%20of%20any%20style%20in%20a%20training-free%0Amanner.%20MagicFace%20introduces%20a%20coarse-to-fine%20generation%20pipeline%2C%20involving%0Atwo%20sequential%20stages%3A%20semantic%20scene%20construction%20and%20concept%20feature%0Ainjection.%20This%20is%20achieved%20by%20our%20Reference-aware%20Self-Attention%20%28RSA%29%20and%0ARegion-grouped%20Blend%20Attention%20%28RBA%29%20mechanisms.%20Specifically%2C%20in%20the%20first%0Astage%2C%20RSA%20enables%20the%20latent%20image%20to%20query%20features%20from%20reference%20concepts%0Asimultaneously%2C%20extracting%20the%20coarse-grained%20overall%20semantic%20understanding%20to%0Afacilitate%20the%20initial%20semantic%20layout%20establishment.%20In%20the%20second%20stage%2C%20we%0Aemploy%20an%20attention-based%20semantic%20segmentation%20method%20to%20pinpoint%20the%0Agenerated%20regions%20of%20all%20concepts%20in%20the%20latent%20image%20at%20each%20step.%20Following%0Athis%2C%20RBA%20divides%20the%20pixels%20of%20the%20latent%20image%20into%20semantic%20groups%2C%20with%0Aeach%20group%20querying%20fine-grained%20features%20from%20its%20reference%20concept%2C%20which%0Aensures%20precise%20attribute%20alignment%20and%20feature%20injection.%20Throughout%20the%0Atwo-stage%20process%2C%20a%20weight%20mask%20strategy%20is%20employed%20to%20ensure%20the%20model%0Afocuses%20more%20on%20the%20reference%20concepts.%20Extensive%20experiments%20demonstrate%20our%0Asuperiority%20in%20both%20human-centric%20subject-to-image%20synthesis%20and%20multi-concept%0Ahuman%20image%20customization.%20Our%20approach%20also%20can%20be%20applied%20to%20texture%0Atransformation%2C%20further%20enhancing%20its%20versatility%20and%20applicability.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07433v1&entry.124074799=Read"},
{"title": "Implicit Causal Representation Learning via Switchable Mechanisms", "author": "Shayan Shirahmad Gale Bagi and Zahra Gharaee and Oliver Schulte and Mark Crowley", "abstract": "  Learning causal representations from observational and interventional data in\nthe absence of known ground-truth graph structures necessitates implicit latent\ncausal representation learning. Implicit learning of causal mechanisms\ntypically involves two categories of interventional data: hard and soft\ninterventions. In real-world scenarios, soft interventions are often more\nrealistic than hard interventions, as the latter require fully controlled\nenvironments. Unlike hard interventions, which directly force changes in a\ncausal variable, soft interventions exert influence indirectly by affecting the\ncausal mechanism. However, the subtlety of soft interventions impose several\nchallenges for learning causal models. One challenge is that soft\nintervention's effects are ambiguous, since parental relations remain intact.\nIn this paper, we tackle the challenges of learning causal models using soft\ninterventions while retaining implicit modelling. We propose ICLR-SM, which\nmodels the effects of soft interventions by employing a causal mechanism switch\nvariable designed to toggle between different causal mechanisms. In our\nexperiments, we consistently observe improved learning of identifiable, causal\nrepresentations, compared to baseline approaches.\n", "link": "http://arxiv.org/abs/2402.11124v3", "date": "2024-08-14", "relevancy": 1.8782, "topK": [{"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4854}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4728}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4525}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Implicit%20Causal%20Representation%20Learning%20via%20Switchable%20Mechanisms&body=Title%3A%20Implicit%20Causal%20Representation%20Learning%20via%20Switchable%20Mechanisms%0AAuthor%3A%20Shayan%20Shirahmad%20Gale%20Bagi%20and%20Zahra%20Gharaee%20and%20Oliver%20Schulte%20and%20Mark%20Crowley%0AAbstract%3A%20%20%20Learning%20causal%20representations%20from%20observational%20and%20interventional%20data%20in%0Athe%20absence%20of%20known%20ground-truth%20graph%20structures%20necessitates%20implicit%20latent%0Acausal%20representation%20learning.%20Implicit%20learning%20of%20causal%20mechanisms%0Atypically%20involves%20two%20categories%20of%20interventional%20data%3A%20hard%20and%20soft%0Ainterventions.%20In%20real-world%20scenarios%2C%20soft%20interventions%20are%20often%20more%0Arealistic%20than%20hard%20interventions%2C%20as%20the%20latter%20require%20fully%20controlled%0Aenvironments.%20Unlike%20hard%20interventions%2C%20which%20directly%20force%20changes%20in%20a%0Acausal%20variable%2C%20soft%20interventions%20exert%20influence%20indirectly%20by%20affecting%20the%0Acausal%20mechanism.%20However%2C%20the%20subtlety%20of%20soft%20interventions%20impose%20several%0Achallenges%20for%20learning%20causal%20models.%20One%20challenge%20is%20that%20soft%0Aintervention%27s%20effects%20are%20ambiguous%2C%20since%20parental%20relations%20remain%20intact.%0AIn%20this%20paper%2C%20we%20tackle%20the%20challenges%20of%20learning%20causal%20models%20using%20soft%0Ainterventions%20while%20retaining%20implicit%20modelling.%20We%20propose%20ICLR-SM%2C%20which%0Amodels%20the%20effects%20of%20soft%20interventions%20by%20employing%20a%20causal%20mechanism%20switch%0Avariable%20designed%20to%20toggle%20between%20different%20causal%20mechanisms.%20In%20our%0Aexperiments%2C%20we%20consistently%20observe%20improved%20learning%20of%20identifiable%2C%20causal%0Arepresentations%2C%20compared%20to%20baseline%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.11124v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DImplicit%2520Causal%2520Representation%2520Learning%2520via%2520Switchable%2520Mechanisms%26entry.906535625%3DShayan%2520Shirahmad%2520Gale%2520Bagi%2520and%2520Zahra%2520Gharaee%2520and%2520Oliver%2520Schulte%2520and%2520Mark%2520Crowley%26entry.1292438233%3D%2520%2520Learning%2520causal%2520representations%2520from%2520observational%2520and%2520interventional%2520data%2520in%250Athe%2520absence%2520of%2520known%2520ground-truth%2520graph%2520structures%2520necessitates%2520implicit%2520latent%250Acausal%2520representation%2520learning.%2520Implicit%2520learning%2520of%2520causal%2520mechanisms%250Atypically%2520involves%2520two%2520categories%2520of%2520interventional%2520data%253A%2520hard%2520and%2520soft%250Ainterventions.%2520In%2520real-world%2520scenarios%252C%2520soft%2520interventions%2520are%2520often%2520more%250Arealistic%2520than%2520hard%2520interventions%252C%2520as%2520the%2520latter%2520require%2520fully%2520controlled%250Aenvironments.%2520Unlike%2520hard%2520interventions%252C%2520which%2520directly%2520force%2520changes%2520in%2520a%250Acausal%2520variable%252C%2520soft%2520interventions%2520exert%2520influence%2520indirectly%2520by%2520affecting%2520the%250Acausal%2520mechanism.%2520However%252C%2520the%2520subtlety%2520of%2520soft%2520interventions%2520impose%2520several%250Achallenges%2520for%2520learning%2520causal%2520models.%2520One%2520challenge%2520is%2520that%2520soft%250Aintervention%2527s%2520effects%2520are%2520ambiguous%252C%2520since%2520parental%2520relations%2520remain%2520intact.%250AIn%2520this%2520paper%252C%2520we%2520tackle%2520the%2520challenges%2520of%2520learning%2520causal%2520models%2520using%2520soft%250Ainterventions%2520while%2520retaining%2520implicit%2520modelling.%2520We%2520propose%2520ICLR-SM%252C%2520which%250Amodels%2520the%2520effects%2520of%2520soft%2520interventions%2520by%2520employing%2520a%2520causal%2520mechanism%2520switch%250Avariable%2520designed%2520to%2520toggle%2520between%2520different%2520causal%2520mechanisms.%2520In%2520our%250Aexperiments%252C%2520we%2520consistently%2520observe%2520improved%2520learning%2520of%2520identifiable%252C%2520causal%250Arepresentations%252C%2520compared%2520to%2520baseline%2520approaches.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.11124v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Causal%20Representation%20Learning%20via%20Switchable%20Mechanisms&entry.906535625=Shayan%20Shirahmad%20Gale%20Bagi%20and%20Zahra%20Gharaee%20and%20Oliver%20Schulte%20and%20Mark%20Crowley&entry.1292438233=%20%20Learning%20causal%20representations%20from%20observational%20and%20interventional%20data%20in%0Athe%20absence%20of%20known%20ground-truth%20graph%20structures%20necessitates%20implicit%20latent%0Acausal%20representation%20learning.%20Implicit%20learning%20of%20causal%20mechanisms%0Atypically%20involves%20two%20categories%20of%20interventional%20data%3A%20hard%20and%20soft%0Ainterventions.%20In%20real-world%20scenarios%2C%20soft%20interventions%20are%20often%20more%0Arealistic%20than%20hard%20interventions%2C%20as%20the%20latter%20require%20fully%20controlled%0Aenvironments.%20Unlike%20hard%20interventions%2C%20which%20directly%20force%20changes%20in%20a%0Acausal%20variable%2C%20soft%20interventions%20exert%20influence%20indirectly%20by%20affecting%20the%0Acausal%20mechanism.%20However%2C%20the%20subtlety%20of%20soft%20interventions%20impose%20several%0Achallenges%20for%20learning%20causal%20models.%20One%20challenge%20is%20that%20soft%0Aintervention%27s%20effects%20are%20ambiguous%2C%20since%20parental%20relations%20remain%20intact.%0AIn%20this%20paper%2C%20we%20tackle%20the%20challenges%20of%20learning%20causal%20models%20using%20soft%0Ainterventions%20while%20retaining%20implicit%20modelling.%20We%20propose%20ICLR-SM%2C%20which%0Amodels%20the%20effects%20of%20soft%20interventions%20by%20employing%20a%20causal%20mechanism%20switch%0Avariable%20designed%20to%20toggle%20between%20different%20causal%20mechanisms.%20In%20our%0Aexperiments%2C%20we%20consistently%20observe%20improved%20learning%20of%20identifiable%2C%20causal%0Arepresentations%2C%20compared%20to%20baseline%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.11124v3&entry.124074799=Read"},
{"title": "Using Explainable AI for EEG-based Reduced Montage Neonatal Seizure\n  Detection", "author": "Dinuka Sandun Udayantha and Kavindu Weerasinghe and Nima Wickramasinghe and Akila Abeyratne and Kithmin Wickremasinghe and Jithangi Wanigasinghe and Anjula De Silva and Chamira U. S. Edussooriya", "abstract": "  The neonatal period is the most vulnerable time for the development of\nseizures. Seizures in the immature brain lead to detrimental consequences,\ntherefore require early diagnosis. The gold-standard for neonatal seizure\ndetection currently relies on continuous video-EEG monitoring; which involves\nrecording multi-channel electroencephalogram (EEG) alongside real-time video\nmonitoring within a neonatal intensive care unit (NICU). However, video-EEG\nmonitoring technology requires clinical expertise and is often limited to\ntechnologically advanced and resourceful settings. Cost-effective new\ntechniques could help the medical fraternity make an accurate diagnosis and\nadvocate treatment without delay. In this work, a novel explainable deep\nlearning model to automate the neonatal seizure detection process with a\nreduced EEG montage is proposed, which employs convolutional nets, graph\nattention layers, and fully connected layers. Beyond its ability to detect\nseizures in real-time with a reduced montage, this model offers the unique\nadvantage of real-time interpretability. By evaluating the performance on the\nZenodo dataset with 10-fold cross-validation, the presented model achieves an\nabsolute improvement of 8.31% and 42.86% in area under curve (AUC) and recall,\nrespectively.\n", "link": "http://arxiv.org/abs/2406.16908v3", "date": "2024-08-14", "relevancy": 1.866, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4817}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4707}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4562}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Using%20Explainable%20AI%20for%20EEG-based%20Reduced%20Montage%20Neonatal%20Seizure%0A%20%20Detection&body=Title%3A%20Using%20Explainable%20AI%20for%20EEG-based%20Reduced%20Montage%20Neonatal%20Seizure%0A%20%20Detection%0AAuthor%3A%20Dinuka%20Sandun%20Udayantha%20and%20Kavindu%20Weerasinghe%20and%20Nima%20Wickramasinghe%20and%20Akila%20Abeyratne%20and%20Kithmin%20Wickremasinghe%20and%20Jithangi%20Wanigasinghe%20and%20Anjula%20De%20Silva%20and%20Chamira%20U.%20S.%20Edussooriya%0AAbstract%3A%20%20%20The%20neonatal%20period%20is%20the%20most%20vulnerable%20time%20for%20the%20development%20of%0Aseizures.%20Seizures%20in%20the%20immature%20brain%20lead%20to%20detrimental%20consequences%2C%0Atherefore%20require%20early%20diagnosis.%20The%20gold-standard%20for%20neonatal%20seizure%0Adetection%20currently%20relies%20on%20continuous%20video-EEG%20monitoring%3B%20which%20involves%0Arecording%20multi-channel%20electroencephalogram%20%28EEG%29%20alongside%20real-time%20video%0Amonitoring%20within%20a%20neonatal%20intensive%20care%20unit%20%28NICU%29.%20However%2C%20video-EEG%0Amonitoring%20technology%20requires%20clinical%20expertise%20and%20is%20often%20limited%20to%0Atechnologically%20advanced%20and%20resourceful%20settings.%20Cost-effective%20new%0Atechniques%20could%20help%20the%20medical%20fraternity%20make%20an%20accurate%20diagnosis%20and%0Aadvocate%20treatment%20without%20delay.%20In%20this%20work%2C%20a%20novel%20explainable%20deep%0Alearning%20model%20to%20automate%20the%20neonatal%20seizure%20detection%20process%20with%20a%0Areduced%20EEG%20montage%20is%20proposed%2C%20which%20employs%20convolutional%20nets%2C%20graph%0Aattention%20layers%2C%20and%20fully%20connected%20layers.%20Beyond%20its%20ability%20to%20detect%0Aseizures%20in%20real-time%20with%20a%20reduced%20montage%2C%20this%20model%20offers%20the%20unique%0Aadvantage%20of%20real-time%20interpretability.%20By%20evaluating%20the%20performance%20on%20the%0AZenodo%20dataset%20with%2010-fold%20cross-validation%2C%20the%20presented%20model%20achieves%20an%0Aabsolute%20improvement%20of%208.31%25%20and%2042.86%25%20in%20area%20under%20curve%20%28AUC%29%20and%20recall%2C%0Arespectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2406.16908v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DUsing%2520Explainable%2520AI%2520for%2520EEG-based%2520Reduced%2520Montage%2520Neonatal%2520Seizure%250A%2520%2520Detection%26entry.906535625%3DDinuka%2520Sandun%2520Udayantha%2520and%2520Kavindu%2520Weerasinghe%2520and%2520Nima%2520Wickramasinghe%2520and%2520Akila%2520Abeyratne%2520and%2520Kithmin%2520Wickremasinghe%2520and%2520Jithangi%2520Wanigasinghe%2520and%2520Anjula%2520De%2520Silva%2520and%2520Chamira%2520U.%2520S.%2520Edussooriya%26entry.1292438233%3D%2520%2520The%2520neonatal%2520period%2520is%2520the%2520most%2520vulnerable%2520time%2520for%2520the%2520development%2520of%250Aseizures.%2520Seizures%2520in%2520the%2520immature%2520brain%2520lead%2520to%2520detrimental%2520consequences%252C%250Atherefore%2520require%2520early%2520diagnosis.%2520The%2520gold-standard%2520for%2520neonatal%2520seizure%250Adetection%2520currently%2520relies%2520on%2520continuous%2520video-EEG%2520monitoring%253B%2520which%2520involves%250Arecording%2520multi-channel%2520electroencephalogram%2520%2528EEG%2529%2520alongside%2520real-time%2520video%250Amonitoring%2520within%2520a%2520neonatal%2520intensive%2520care%2520unit%2520%2528NICU%2529.%2520However%252C%2520video-EEG%250Amonitoring%2520technology%2520requires%2520clinical%2520expertise%2520and%2520is%2520often%2520limited%2520to%250Atechnologically%2520advanced%2520and%2520resourceful%2520settings.%2520Cost-effective%2520new%250Atechniques%2520could%2520help%2520the%2520medical%2520fraternity%2520make%2520an%2520accurate%2520diagnosis%2520and%250Aadvocate%2520treatment%2520without%2520delay.%2520In%2520this%2520work%252C%2520a%2520novel%2520explainable%2520deep%250Alearning%2520model%2520to%2520automate%2520the%2520neonatal%2520seizure%2520detection%2520process%2520with%2520a%250Areduced%2520EEG%2520montage%2520is%2520proposed%252C%2520which%2520employs%2520convolutional%2520nets%252C%2520graph%250Aattention%2520layers%252C%2520and%2520fully%2520connected%2520layers.%2520Beyond%2520its%2520ability%2520to%2520detect%250Aseizures%2520in%2520real-time%2520with%2520a%2520reduced%2520montage%252C%2520this%2520model%2520offers%2520the%2520unique%250Aadvantage%2520of%2520real-time%2520interpretability.%2520By%2520evaluating%2520the%2520performance%2520on%2520the%250AZenodo%2520dataset%2520with%252010-fold%2520cross-validation%252C%2520the%2520presented%2520model%2520achieves%2520an%250Aabsolute%2520improvement%2520of%25208.31%2525%2520and%252042.86%2525%2520in%2520area%2520under%2520curve%2520%2528AUC%2529%2520and%2520recall%252C%250Arespectively.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2406.16908v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Using%20Explainable%20AI%20for%20EEG-based%20Reduced%20Montage%20Neonatal%20Seizure%0A%20%20Detection&entry.906535625=Dinuka%20Sandun%20Udayantha%20and%20Kavindu%20Weerasinghe%20and%20Nima%20Wickramasinghe%20and%20Akila%20Abeyratne%20and%20Kithmin%20Wickremasinghe%20and%20Jithangi%20Wanigasinghe%20and%20Anjula%20De%20Silva%20and%20Chamira%20U.%20S.%20Edussooriya&entry.1292438233=%20%20The%20neonatal%20period%20is%20the%20most%20vulnerable%20time%20for%20the%20development%20of%0Aseizures.%20Seizures%20in%20the%20immature%20brain%20lead%20to%20detrimental%20consequences%2C%0Atherefore%20require%20early%20diagnosis.%20The%20gold-standard%20for%20neonatal%20seizure%0Adetection%20currently%20relies%20on%20continuous%20video-EEG%20monitoring%3B%20which%20involves%0Arecording%20multi-channel%20electroencephalogram%20%28EEG%29%20alongside%20real-time%20video%0Amonitoring%20within%20a%20neonatal%20intensive%20care%20unit%20%28NICU%29.%20However%2C%20video-EEG%0Amonitoring%20technology%20requires%20clinical%20expertise%20and%20is%20often%20limited%20to%0Atechnologically%20advanced%20and%20resourceful%20settings.%20Cost-effective%20new%0Atechniques%20could%20help%20the%20medical%20fraternity%20make%20an%20accurate%20diagnosis%20and%0Aadvocate%20treatment%20without%20delay.%20In%20this%20work%2C%20a%20novel%20explainable%20deep%0Alearning%20model%20to%20automate%20the%20neonatal%20seizure%20detection%20process%20with%20a%0Areduced%20EEG%20montage%20is%20proposed%2C%20which%20employs%20convolutional%20nets%2C%20graph%0Aattention%20layers%2C%20and%20fully%20connected%20layers.%20Beyond%20its%20ability%20to%20detect%0Aseizures%20in%20real-time%20with%20a%20reduced%20montage%2C%20this%20model%20offers%20the%20unique%0Aadvantage%20of%20real-time%20interpretability.%20By%20evaluating%20the%20performance%20on%20the%0AZenodo%20dataset%20with%2010-fold%20cross-validation%2C%20the%20presented%20model%20achieves%20an%0Aabsolute%20improvement%20of%208.31%25%20and%2042.86%25%20in%20area%20under%20curve%20%28AUC%29%20and%20recall%2C%0Arespectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2406.16908v3&entry.124074799=Read"},
{"title": "Natural Language to Verilog: Design of a Recurrent Spiking Neural\n  Network using Large Language Models and ChatGPT", "author": "Paola Vitolo and George Psaltakis and Michael Tomlinson and Gian Domenico Licciardo and Andreas G. Andreou", "abstract": "  This paper investigates the use of Large Language Models (LLMs) for\nautomating the generation of hardware description code, aiming to explore their\npotential in supporting and enhancing the development of efficient neuromorphic\ncomputing architectures. Building on our prior work, we employ OpenAI's\nChatGPT4 and natural language prompts to synthesize a RTL Verilog module of a\nprogrammable recurrent spiking neural network, while also generating test\nbenches to assess the system's correctness. The resultant design was validated\nin three case studies, the exclusive OR,the IRIS flower classification and the\nMNIST hand-written digit classification, achieving accuracies of up to 96.6%.\nTo verify its synthesizability and implementability, the design was prototyped\non a field-programmable gate array and implemented on SkyWater 130 nm\ntechnology by using an open-source electronic design automation flow.\nAdditionally, we have submitted it to Tiny Tapeout 6 chip fabrication program\nto further evaluate the system on-chip performance in the future.\n", "link": "http://arxiv.org/abs/2405.01419v2", "date": "2024-08-14", "relevancy": 1.8627, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4738}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4658}, {"title": "RTG-SLAM: Real-time 3D Reconstruction at Scale using Gaussian Splatting", "link": "http://arxiv.org/abs/2404.19706v1", "similarity": 0.4575}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Natural%20Language%20to%20Verilog%3A%20Design%20of%20a%20Recurrent%20Spiking%20Neural%0A%20%20Network%20using%20Large%20Language%20Models%20and%20ChatGPT&body=Title%3A%20Natural%20Language%20to%20Verilog%3A%20Design%20of%20a%20Recurrent%20Spiking%20Neural%0A%20%20Network%20using%20Large%20Language%20Models%20and%20ChatGPT%0AAuthor%3A%20Paola%20Vitolo%20and%20George%20Psaltakis%20and%20Michael%20Tomlinson%20and%20Gian%20Domenico%20Licciardo%20and%20Andreas%20G.%20Andreou%0AAbstract%3A%20%20%20This%20paper%20investigates%20the%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20for%0Aautomating%20the%20generation%20of%20hardware%20description%20code%2C%20aiming%20to%20explore%20their%0Apotential%20in%20supporting%20and%20enhancing%20the%20development%20of%20efficient%20neuromorphic%0Acomputing%20architectures.%20Building%20on%20our%20prior%20work%2C%20we%20employ%20OpenAI%27s%0AChatGPT4%20and%20natural%20language%20prompts%20to%20synthesize%20a%20RTL%20Verilog%20module%20of%20a%0Aprogrammable%20recurrent%20spiking%20neural%20network%2C%20while%20also%20generating%20test%0Abenches%20to%20assess%20the%20system%27s%20correctness.%20The%20resultant%20design%20was%20validated%0Ain%20three%20case%20studies%2C%20the%20exclusive%20OR%2Cthe%20IRIS%20flower%20classification%20and%20the%0AMNIST%20hand-written%20digit%20classification%2C%20achieving%20accuracies%20of%20up%20to%2096.6%25.%0ATo%20verify%20its%20synthesizability%20and%20implementability%2C%20the%20design%20was%20prototyped%0Aon%20a%20field-programmable%20gate%20array%20and%20implemented%20on%20SkyWater%20130%20nm%0Atechnology%20by%20using%20an%20open-source%20electronic%20design%20automation%20flow.%0AAdditionally%2C%20we%20have%20submitted%20it%20to%20Tiny%20Tapeout%206%20chip%20fabrication%20program%0Ato%20further%20evaluate%20the%20system%20on-chip%20performance%20in%20the%20future.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.01419v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNatural%2520Language%2520to%2520Verilog%253A%2520Design%2520of%2520a%2520Recurrent%2520Spiking%2520Neural%250A%2520%2520Network%2520using%2520Large%2520Language%2520Models%2520and%2520ChatGPT%26entry.906535625%3DPaola%2520Vitolo%2520and%2520George%2520Psaltakis%2520and%2520Michael%2520Tomlinson%2520and%2520Gian%2520Domenico%2520Licciardo%2520and%2520Andreas%2520G.%2520Andreou%26entry.1292438233%3D%2520%2520This%2520paper%2520investigates%2520the%2520use%2520of%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520for%250Aautomating%2520the%2520generation%2520of%2520hardware%2520description%2520code%252C%2520aiming%2520to%2520explore%2520their%250Apotential%2520in%2520supporting%2520and%2520enhancing%2520the%2520development%2520of%2520efficient%2520neuromorphic%250Acomputing%2520architectures.%2520Building%2520on%2520our%2520prior%2520work%252C%2520we%2520employ%2520OpenAI%2527s%250AChatGPT4%2520and%2520natural%2520language%2520prompts%2520to%2520synthesize%2520a%2520RTL%2520Verilog%2520module%2520of%2520a%250Aprogrammable%2520recurrent%2520spiking%2520neural%2520network%252C%2520while%2520also%2520generating%2520test%250Abenches%2520to%2520assess%2520the%2520system%2527s%2520correctness.%2520The%2520resultant%2520design%2520was%2520validated%250Ain%2520three%2520case%2520studies%252C%2520the%2520exclusive%2520OR%252Cthe%2520IRIS%2520flower%2520classification%2520and%2520the%250AMNIST%2520hand-written%2520digit%2520classification%252C%2520achieving%2520accuracies%2520of%2520up%2520to%252096.6%2525.%250ATo%2520verify%2520its%2520synthesizability%2520and%2520implementability%252C%2520the%2520design%2520was%2520prototyped%250Aon%2520a%2520field-programmable%2520gate%2520array%2520and%2520implemented%2520on%2520SkyWater%2520130%2520nm%250Atechnology%2520by%2520using%2520an%2520open-source%2520electronic%2520design%2520automation%2520flow.%250AAdditionally%252C%2520we%2520have%2520submitted%2520it%2520to%2520Tiny%2520Tapeout%25206%2520chip%2520fabrication%2520program%250Ato%2520further%2520evaluate%2520the%2520system%2520on-chip%2520performance%2520in%2520the%2520future.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.01419v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Natural%20Language%20to%20Verilog%3A%20Design%20of%20a%20Recurrent%20Spiking%20Neural%0A%20%20Network%20using%20Large%20Language%20Models%20and%20ChatGPT&entry.906535625=Paola%20Vitolo%20and%20George%20Psaltakis%20and%20Michael%20Tomlinson%20and%20Gian%20Domenico%20Licciardo%20and%20Andreas%20G.%20Andreou&entry.1292438233=%20%20This%20paper%20investigates%20the%20use%20of%20Large%20Language%20Models%20%28LLMs%29%20for%0Aautomating%20the%20generation%20of%20hardware%20description%20code%2C%20aiming%20to%20explore%20their%0Apotential%20in%20supporting%20and%20enhancing%20the%20development%20of%20efficient%20neuromorphic%0Acomputing%20architectures.%20Building%20on%20our%20prior%20work%2C%20we%20employ%20OpenAI%27s%0AChatGPT4%20and%20natural%20language%20prompts%20to%20synthesize%20a%20RTL%20Verilog%20module%20of%20a%0Aprogrammable%20recurrent%20spiking%20neural%20network%2C%20while%20also%20generating%20test%0Abenches%20to%20assess%20the%20system%27s%20correctness.%20The%20resultant%20design%20was%20validated%0Ain%20three%20case%20studies%2C%20the%20exclusive%20OR%2Cthe%20IRIS%20flower%20classification%20and%20the%0AMNIST%20hand-written%20digit%20classification%2C%20achieving%20accuracies%20of%20up%20to%2096.6%25.%0ATo%20verify%20its%20synthesizability%20and%20implementability%2C%20the%20design%20was%20prototyped%0Aon%20a%20field-programmable%20gate%20array%20and%20implemented%20on%20SkyWater%20130%20nm%0Atechnology%20by%20using%20an%20open-source%20electronic%20design%20automation%20flow.%0AAdditionally%2C%20we%20have%20submitted%20it%20to%20Tiny%20Tapeout%206%20chip%20fabrication%20program%0Ato%20further%20evaluate%20the%20system%20on-chip%20performance%20in%20the%20future.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.01419v2&entry.124074799=Read"},
{"title": "One Step Diffusion-based Super-Resolution with Time-Aware Distillation", "author": "Xiao He and Huaao Tang and Zhijun Tu and Junchao Zhang and Kun Cheng and Hanting Chen and Yong Guo and Mingrui Zhu and Nannan Wang and Xinbo Gao and Jie Hu", "abstract": "  Diffusion-based image super-resolution (SR) methods have shown promise in\nreconstructing high-resolution images with fine details from low-resolution\ncounterparts. However, these approaches typically require tens or even hundreds\nof iterative samplings, resulting in significant latency. Recently, techniques\nhave been devised to enhance the sampling efficiency of diffusion-based SR\nmodels via knowledge distillation. Nonetheless, when aligning the knowledge of\nstudent and teacher models, these solutions either solely rely on pixel-level\nloss constraints or neglect the fact that diffusion models prioritize varying\nlevels of information at different time steps. To accomplish effective and\nefficient image super-resolution, we propose a time-aware diffusion\ndistillation method, named TAD-SR. Specifically, we introduce a novel score\ndistillation strategy to align the data distribution between the outputs of the\nstudent and teacher models after minor noise perturbation. This distillation\nstrategy enables the student network to concentrate more on the high-frequency\ndetails. Furthermore, to mitigate performance limitations stemming from\ndistillation, we integrate a latent adversarial loss and devise a time-aware\ndiscriminator that leverages diffusion priors to effectively distinguish\nbetween real images and generated images. Extensive experiments conducted on\nsynthetic and real-world datasets demonstrate that the proposed method achieves\ncomparable or even superior performance compared to both previous\nstate-of-the-art (SOTA) methods and the teacher model in just one sampling\nstep. Codes are available at https://github.com/LearningHx/TAD-SR.\n", "link": "http://arxiv.org/abs/2408.07476v1", "date": "2024-08-14", "relevancy": 1.8575, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.6822}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.6116}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5749}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20One%20Step%20Diffusion-based%20Super-Resolution%20with%20Time-Aware%20Distillation&body=Title%3A%20One%20Step%20Diffusion-based%20Super-Resolution%20with%20Time-Aware%20Distillation%0AAuthor%3A%20Xiao%20He%20and%20Huaao%20Tang%20and%20Zhijun%20Tu%20and%20Junchao%20Zhang%20and%20Kun%20Cheng%20and%20Hanting%20Chen%20and%20Yong%20Guo%20and%20Mingrui%20Zhu%20and%20Nannan%20Wang%20and%20Xinbo%20Gao%20and%20Jie%20Hu%0AAbstract%3A%20%20%20Diffusion-based%20image%20super-resolution%20%28SR%29%20methods%20have%20shown%20promise%20in%0Areconstructing%20high-resolution%20images%20with%20fine%20details%20from%20low-resolution%0Acounterparts.%20However%2C%20these%20approaches%20typically%20require%20tens%20or%20even%20hundreds%0Aof%20iterative%20samplings%2C%20resulting%20in%20significant%20latency.%20Recently%2C%20techniques%0Ahave%20been%20devised%20to%20enhance%20the%20sampling%20efficiency%20of%20diffusion-based%20SR%0Amodels%20via%20knowledge%20distillation.%20Nonetheless%2C%20when%20aligning%20the%20knowledge%20of%0Astudent%20and%20teacher%20models%2C%20these%20solutions%20either%20solely%20rely%20on%20pixel-level%0Aloss%20constraints%20or%20neglect%20the%20fact%20that%20diffusion%20models%20prioritize%20varying%0Alevels%20of%20information%20at%20different%20time%20steps.%20To%20accomplish%20effective%20and%0Aefficient%20image%20super-resolution%2C%20we%20propose%20a%20time-aware%20diffusion%0Adistillation%20method%2C%20named%20TAD-SR.%20Specifically%2C%20we%20introduce%20a%20novel%20score%0Adistillation%20strategy%20to%20align%20the%20data%20distribution%20between%20the%20outputs%20of%20the%0Astudent%20and%20teacher%20models%20after%20minor%20noise%20perturbation.%20This%20distillation%0Astrategy%20enables%20the%20student%20network%20to%20concentrate%20more%20on%20the%20high-frequency%0Adetails.%20Furthermore%2C%20to%20mitigate%20performance%20limitations%20stemming%20from%0Adistillation%2C%20we%20integrate%20a%20latent%20adversarial%20loss%20and%20devise%20a%20time-aware%0Adiscriminator%20that%20leverages%20diffusion%20priors%20to%20effectively%20distinguish%0Abetween%20real%20images%20and%20generated%20images.%20Extensive%20experiments%20conducted%20on%0Asynthetic%20and%20real-world%20datasets%20demonstrate%20that%20the%20proposed%20method%20achieves%0Acomparable%20or%20even%20superior%20performance%20compared%20to%20both%20previous%0Astate-of-the-art%20%28SOTA%29%20methods%20and%20the%20teacher%20model%20in%20just%20one%20sampling%0Astep.%20Codes%20are%20available%20at%20https%3A//github.com/LearningHx/TAD-SR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07476v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOne%2520Step%2520Diffusion-based%2520Super-Resolution%2520with%2520Time-Aware%2520Distillation%26entry.906535625%3DXiao%2520He%2520and%2520Huaao%2520Tang%2520and%2520Zhijun%2520Tu%2520and%2520Junchao%2520Zhang%2520and%2520Kun%2520Cheng%2520and%2520Hanting%2520Chen%2520and%2520Yong%2520Guo%2520and%2520Mingrui%2520Zhu%2520and%2520Nannan%2520Wang%2520and%2520Xinbo%2520Gao%2520and%2520Jie%2520Hu%26entry.1292438233%3D%2520%2520Diffusion-based%2520image%2520super-resolution%2520%2528SR%2529%2520methods%2520have%2520shown%2520promise%2520in%250Areconstructing%2520high-resolution%2520images%2520with%2520fine%2520details%2520from%2520low-resolution%250Acounterparts.%2520However%252C%2520these%2520approaches%2520typically%2520require%2520tens%2520or%2520even%2520hundreds%250Aof%2520iterative%2520samplings%252C%2520resulting%2520in%2520significant%2520latency.%2520Recently%252C%2520techniques%250Ahave%2520been%2520devised%2520to%2520enhance%2520the%2520sampling%2520efficiency%2520of%2520diffusion-based%2520SR%250Amodels%2520via%2520knowledge%2520distillation.%2520Nonetheless%252C%2520when%2520aligning%2520the%2520knowledge%2520of%250Astudent%2520and%2520teacher%2520models%252C%2520these%2520solutions%2520either%2520solely%2520rely%2520on%2520pixel-level%250Aloss%2520constraints%2520or%2520neglect%2520the%2520fact%2520that%2520diffusion%2520models%2520prioritize%2520varying%250Alevels%2520of%2520information%2520at%2520different%2520time%2520steps.%2520To%2520accomplish%2520effective%2520and%250Aefficient%2520image%2520super-resolution%252C%2520we%2520propose%2520a%2520time-aware%2520diffusion%250Adistillation%2520method%252C%2520named%2520TAD-SR.%2520Specifically%252C%2520we%2520introduce%2520a%2520novel%2520score%250Adistillation%2520strategy%2520to%2520align%2520the%2520data%2520distribution%2520between%2520the%2520outputs%2520of%2520the%250Astudent%2520and%2520teacher%2520models%2520after%2520minor%2520noise%2520perturbation.%2520This%2520distillation%250Astrategy%2520enables%2520the%2520student%2520network%2520to%2520concentrate%2520more%2520on%2520the%2520high-frequency%250Adetails.%2520Furthermore%252C%2520to%2520mitigate%2520performance%2520limitations%2520stemming%2520from%250Adistillation%252C%2520we%2520integrate%2520a%2520latent%2520adversarial%2520loss%2520and%2520devise%2520a%2520time-aware%250Adiscriminator%2520that%2520leverages%2520diffusion%2520priors%2520to%2520effectively%2520distinguish%250Abetween%2520real%2520images%2520and%2520generated%2520images.%2520Extensive%2520experiments%2520conducted%2520on%250Asynthetic%2520and%2520real-world%2520datasets%2520demonstrate%2520that%2520the%2520proposed%2520method%2520achieves%250Acomparable%2520or%2520even%2520superior%2520performance%2520compared%2520to%2520both%2520previous%250Astate-of-the-art%2520%2528SOTA%2529%2520methods%2520and%2520the%2520teacher%2520model%2520in%2520just%2520one%2520sampling%250Astep.%2520Codes%2520are%2520available%2520at%2520https%253A//github.com/LearningHx/TAD-SR.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07476v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=One%20Step%20Diffusion-based%20Super-Resolution%20with%20Time-Aware%20Distillation&entry.906535625=Xiao%20He%20and%20Huaao%20Tang%20and%20Zhijun%20Tu%20and%20Junchao%20Zhang%20and%20Kun%20Cheng%20and%20Hanting%20Chen%20and%20Yong%20Guo%20and%20Mingrui%20Zhu%20and%20Nannan%20Wang%20and%20Xinbo%20Gao%20and%20Jie%20Hu&entry.1292438233=%20%20Diffusion-based%20image%20super-resolution%20%28SR%29%20methods%20have%20shown%20promise%20in%0Areconstructing%20high-resolution%20images%20with%20fine%20details%20from%20low-resolution%0Acounterparts.%20However%2C%20these%20approaches%20typically%20require%20tens%20or%20even%20hundreds%0Aof%20iterative%20samplings%2C%20resulting%20in%20significant%20latency.%20Recently%2C%20techniques%0Ahave%20been%20devised%20to%20enhance%20the%20sampling%20efficiency%20of%20diffusion-based%20SR%0Amodels%20via%20knowledge%20distillation.%20Nonetheless%2C%20when%20aligning%20the%20knowledge%20of%0Astudent%20and%20teacher%20models%2C%20these%20solutions%20either%20solely%20rely%20on%20pixel-level%0Aloss%20constraints%20or%20neglect%20the%20fact%20that%20diffusion%20models%20prioritize%20varying%0Alevels%20of%20information%20at%20different%20time%20steps.%20To%20accomplish%20effective%20and%0Aefficient%20image%20super-resolution%2C%20we%20propose%20a%20time-aware%20diffusion%0Adistillation%20method%2C%20named%20TAD-SR.%20Specifically%2C%20we%20introduce%20a%20novel%20score%0Adistillation%20strategy%20to%20align%20the%20data%20distribution%20between%20the%20outputs%20of%20the%0Astudent%20and%20teacher%20models%20after%20minor%20noise%20perturbation.%20This%20distillation%0Astrategy%20enables%20the%20student%20network%20to%20concentrate%20more%20on%20the%20high-frequency%0Adetails.%20Furthermore%2C%20to%20mitigate%20performance%20limitations%20stemming%20from%0Adistillation%2C%20we%20integrate%20a%20latent%20adversarial%20loss%20and%20devise%20a%20time-aware%0Adiscriminator%20that%20leverages%20diffusion%20priors%20to%20effectively%20distinguish%0Abetween%20real%20images%20and%20generated%20images.%20Extensive%20experiments%20conducted%20on%0Asynthetic%20and%20real-world%20datasets%20demonstrate%20that%20the%20proposed%20method%20achieves%0Acomparable%20or%20even%20superior%20performance%20compared%20to%20both%20previous%0Astate-of-the-art%20%28SOTA%29%20methods%20and%20the%20teacher%20model%20in%20just%20one%20sampling%0Astep.%20Codes%20are%20available%20at%20https%3A//github.com/LearningHx/TAD-SR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07476v1&entry.124074799=Read"},
{"title": "FedQUIT: On-Device Federated Unlearning via a Quasi-Competent Virtual\n  Teacher", "author": "Alessio Mora and Lorenzo Valerio and Paolo Bellavista and Andrea Passarella", "abstract": "  Federated Learning (FL) promises better privacy guarantees for individuals'\ndata when machine learning models are collaboratively trained. When an FL\nparticipant exercises its right to be forgotten, i.e., to detach from the FL\nframework it has participated and to remove its past contributions to the\nglobal model, the FL solution should perform all the necessary steps to make it\npossible without sacrificing the overall performance of the global model, which\nis not supported in state-of-the-art related solutions nowadays. In this paper,\nwe propose FedQUIT, a novel algorithm that uses knowledge distillation to scrub\nthe contribution of the forgetting data from an FL global model while\npreserving its generalization ability. FedQUIT directly works on clients'\ndevices and does not require sharing additional information if compared with a\nregular FL process, nor does it assume the availability of publicly available\nproxy data. Our solution is efficient, effective, and applicable in both\ncentralized and federated settings. Our experimental results show that, on\naverage, FedQUIT requires less than 2.5% additional communication rounds to\nrecover generalization performances after unlearning, obtaining a sanitized\nglobal model whose predictions are comparable to those of a global model that\nhas never seen the data to be forgotten.\n", "link": "http://arxiv.org/abs/2408.07587v1", "date": "2024-08-14", "relevancy": 1.8532, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4851}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4611}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4568}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20FedQUIT%3A%20On-Device%20Federated%20Unlearning%20via%20a%20Quasi-Competent%20Virtual%0A%20%20Teacher&body=Title%3A%20FedQUIT%3A%20On-Device%20Federated%20Unlearning%20via%20a%20Quasi-Competent%20Virtual%0A%20%20Teacher%0AAuthor%3A%20Alessio%20Mora%20and%20Lorenzo%20Valerio%20and%20Paolo%20Bellavista%20and%20Andrea%20Passarella%0AAbstract%3A%20%20%20Federated%20Learning%20%28FL%29%20promises%20better%20privacy%20guarantees%20for%20individuals%27%0Adata%20when%20machine%20learning%20models%20are%20collaboratively%20trained.%20When%20an%20FL%0Aparticipant%20exercises%20its%20right%20to%20be%20forgotten%2C%20i.e.%2C%20to%20detach%20from%20the%20FL%0Aframework%20it%20has%20participated%20and%20to%20remove%20its%20past%20contributions%20to%20the%0Aglobal%20model%2C%20the%20FL%20solution%20should%20perform%20all%20the%20necessary%20steps%20to%20make%20it%0Apossible%20without%20sacrificing%20the%20overall%20performance%20of%20the%20global%20model%2C%20which%0Ais%20not%20supported%20in%20state-of-the-art%20related%20solutions%20nowadays.%20In%20this%20paper%2C%0Awe%20propose%20FedQUIT%2C%20a%20novel%20algorithm%20that%20uses%20knowledge%20distillation%20to%20scrub%0Athe%20contribution%20of%20the%20forgetting%20data%20from%20an%20FL%20global%20model%20while%0Apreserving%20its%20generalization%20ability.%20FedQUIT%20directly%20works%20on%20clients%27%0Adevices%20and%20does%20not%20require%20sharing%20additional%20information%20if%20compared%20with%20a%0Aregular%20FL%20process%2C%20nor%20does%20it%20assume%20the%20availability%20of%20publicly%20available%0Aproxy%20data.%20Our%20solution%20is%20efficient%2C%20effective%2C%20and%20applicable%20in%20both%0Acentralized%20and%20federated%20settings.%20Our%20experimental%20results%20show%20that%2C%20on%0Aaverage%2C%20FedQUIT%20requires%20less%20than%202.5%25%20additional%20communication%20rounds%20to%0Arecover%20generalization%20performances%20after%20unlearning%2C%20obtaining%20a%20sanitized%0Aglobal%20model%20whose%20predictions%20are%20comparable%20to%20those%20of%20a%20global%20model%20that%0Ahas%20never%20seen%20the%20data%20to%20be%20forgotten.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07587v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFedQUIT%253A%2520On-Device%2520Federated%2520Unlearning%2520via%2520a%2520Quasi-Competent%2520Virtual%250A%2520%2520Teacher%26entry.906535625%3DAlessio%2520Mora%2520and%2520Lorenzo%2520Valerio%2520and%2520Paolo%2520Bellavista%2520and%2520Andrea%2520Passarella%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520%2528FL%2529%2520promises%2520better%2520privacy%2520guarantees%2520for%2520individuals%2527%250Adata%2520when%2520machine%2520learning%2520models%2520are%2520collaboratively%2520trained.%2520When%2520an%2520FL%250Aparticipant%2520exercises%2520its%2520right%2520to%2520be%2520forgotten%252C%2520i.e.%252C%2520to%2520detach%2520from%2520the%2520FL%250Aframework%2520it%2520has%2520participated%2520and%2520to%2520remove%2520its%2520past%2520contributions%2520to%2520the%250Aglobal%2520model%252C%2520the%2520FL%2520solution%2520should%2520perform%2520all%2520the%2520necessary%2520steps%2520to%2520make%2520it%250Apossible%2520without%2520sacrificing%2520the%2520overall%2520performance%2520of%2520the%2520global%2520model%252C%2520which%250Ais%2520not%2520supported%2520in%2520state-of-the-art%2520related%2520solutions%2520nowadays.%2520In%2520this%2520paper%252C%250Awe%2520propose%2520FedQUIT%252C%2520a%2520novel%2520algorithm%2520that%2520uses%2520knowledge%2520distillation%2520to%2520scrub%250Athe%2520contribution%2520of%2520the%2520forgetting%2520data%2520from%2520an%2520FL%2520global%2520model%2520while%250Apreserving%2520its%2520generalization%2520ability.%2520FedQUIT%2520directly%2520works%2520on%2520clients%2527%250Adevices%2520and%2520does%2520not%2520require%2520sharing%2520additional%2520information%2520if%2520compared%2520with%2520a%250Aregular%2520FL%2520process%252C%2520nor%2520does%2520it%2520assume%2520the%2520availability%2520of%2520publicly%2520available%250Aproxy%2520data.%2520Our%2520solution%2520is%2520efficient%252C%2520effective%252C%2520and%2520applicable%2520in%2520both%250Acentralized%2520and%2520federated%2520settings.%2520Our%2520experimental%2520results%2520show%2520that%252C%2520on%250Aaverage%252C%2520FedQUIT%2520requires%2520less%2520than%25202.5%2525%2520additional%2520communication%2520rounds%2520to%250Arecover%2520generalization%2520performances%2520after%2520unlearning%252C%2520obtaining%2520a%2520sanitized%250Aglobal%2520model%2520whose%2520predictions%2520are%2520comparable%2520to%2520those%2520of%2520a%2520global%2520model%2520that%250Ahas%2520never%2520seen%2520the%2520data%2520to%2520be%2520forgotten.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07587v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FedQUIT%3A%20On-Device%20Federated%20Unlearning%20via%20a%20Quasi-Competent%20Virtual%0A%20%20Teacher&entry.906535625=Alessio%20Mora%20and%20Lorenzo%20Valerio%20and%20Paolo%20Bellavista%20and%20Andrea%20Passarella&entry.1292438233=%20%20Federated%20Learning%20%28FL%29%20promises%20better%20privacy%20guarantees%20for%20individuals%27%0Adata%20when%20machine%20learning%20models%20are%20collaboratively%20trained.%20When%20an%20FL%0Aparticipant%20exercises%20its%20right%20to%20be%20forgotten%2C%20i.e.%2C%20to%20detach%20from%20the%20FL%0Aframework%20it%20has%20participated%20and%20to%20remove%20its%20past%20contributions%20to%20the%0Aglobal%20model%2C%20the%20FL%20solution%20should%20perform%20all%20the%20necessary%20steps%20to%20make%20it%0Apossible%20without%20sacrificing%20the%20overall%20performance%20of%20the%20global%20model%2C%20which%0Ais%20not%20supported%20in%20state-of-the-art%20related%20solutions%20nowadays.%20In%20this%20paper%2C%0Awe%20propose%20FedQUIT%2C%20a%20novel%20algorithm%20that%20uses%20knowledge%20distillation%20to%20scrub%0Athe%20contribution%20of%20the%20forgetting%20data%20from%20an%20FL%20global%20model%20while%0Apreserving%20its%20generalization%20ability.%20FedQUIT%20directly%20works%20on%20clients%27%0Adevices%20and%20does%20not%20require%20sharing%20additional%20information%20if%20compared%20with%20a%0Aregular%20FL%20process%2C%20nor%20does%20it%20assume%20the%20availability%20of%20publicly%20available%0Aproxy%20data.%20Our%20solution%20is%20efficient%2C%20effective%2C%20and%20applicable%20in%20both%0Acentralized%20and%20federated%20settings.%20Our%20experimental%20results%20show%20that%2C%20on%0Aaverage%2C%20FedQUIT%20requires%20less%20than%202.5%25%20additional%20communication%20rounds%20to%0Arecover%20generalization%20performances%20after%20unlearning%2C%20obtaining%20a%20sanitized%0Aglobal%20model%20whose%20predictions%20are%20comparable%20to%20those%20of%20a%20global%20model%20that%0Ahas%20never%20seen%20the%20data%20to%20be%20forgotten.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07587v1&entry.124074799=Read"},
{"title": "New Curriculum, New Chance -- Retrieval Augmented Generation for Lesson\n  Planning in Ugandan Secondary Schools. Prototype Quality Evaluation", "author": "Simon Kloker and Herbertson Bukoli and Twaha Kateete", "abstract": "  Introduction: Poor educational quality in Secondary Schools is still regarded\nas one of the major struggles in 21st century Uganda - especially in rural\nareas. Research identifies several problems, including low quality or absent\nteacher lesson planning. As the government pushes towards the implementation of\na new curriculum, exiting lesson plans become obsolete and the problem is\nworsened. Using a Retrieval Augmented Generation approach, we developed a\nprototype that generates customized lesson plans based on the\ngovernment-accredited textbooks. This helps teachers create lesson plans more\nefficiently and with better quality, ensuring they are fully aligned the new\ncurriculum and the competence-based learning approach.\n  Methods: The prototype was created using Cohere LLM and Sentence Embeddings,\nand LangChain Framework - and thereafter made available on a public website.\nVector stores were trained for three new curriculum textbooks (ICT,\nMathematics, History), all at Secondary 1 Level. Twenty-four lessons plans were\ngenerated following a pseudo-random generation protocol, based on the suggested\nperiods in the textbooks. The lesson plans were analyzed regarding their\ntechnical quality by three independent raters following the Lesson Plan\nAnalysis Protocol (LPAP) by Ndihokubwayo et al. (2022) that is specifically\ndesigned for East Africa and competence-based curriculums.\n  Results: Evaluation of 24 lesson plans using the LPAP resulted in an average\nquality of between 75 and 80%, corresponding to \"very good lesson plan\". None\nof the lesson plans scored below 65%, although one lesson plan could be argued\nto have been missing the topic. In conclusion, the quality of the generated\nlesson plans is at least comparable, if not better, than those created by\nhumans, as demonstrated in a study in Rwanda, whereby no lesson plan even\nreached the benchmark of 50%.\n", "link": "http://arxiv.org/abs/2408.07542v1", "date": "2024-08-14", "relevancy": 1.8476, "topK": [{"title": "DressCode: Autoregressively Sewing and Generating Garments from Text\n  Guidance", "link": "http://arxiv.org/abs/2401.16465v3", "similarity": 0.4733}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4615}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4343}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20New%20Curriculum%2C%20New%20Chance%20--%20Retrieval%20Augmented%20Generation%20for%20Lesson%0A%20%20Planning%20in%20Ugandan%20Secondary%20Schools.%20Prototype%20Quality%20Evaluation&body=Title%3A%20New%20Curriculum%2C%20New%20Chance%20--%20Retrieval%20Augmented%20Generation%20for%20Lesson%0A%20%20Planning%20in%20Ugandan%20Secondary%20Schools.%20Prototype%20Quality%20Evaluation%0AAuthor%3A%20Simon%20Kloker%20and%20Herbertson%20Bukoli%20and%20Twaha%20Kateete%0AAbstract%3A%20%20%20Introduction%3A%20Poor%20educational%20quality%20in%20Secondary%20Schools%20is%20still%20regarded%0Aas%20one%20of%20the%20major%20struggles%20in%2021st%20century%20Uganda%20-%20especially%20in%20rural%0Aareas.%20Research%20identifies%20several%20problems%2C%20including%20low%20quality%20or%20absent%0Ateacher%20lesson%20planning.%20As%20the%20government%20pushes%20towards%20the%20implementation%20of%0Aa%20new%20curriculum%2C%20exiting%20lesson%20plans%20become%20obsolete%20and%20the%20problem%20is%0Aworsened.%20Using%20a%20Retrieval%20Augmented%20Generation%20approach%2C%20we%20developed%20a%0Aprototype%20that%20generates%20customized%20lesson%20plans%20based%20on%20the%0Agovernment-accredited%20textbooks.%20This%20helps%20teachers%20create%20lesson%20plans%20more%0Aefficiently%20and%20with%20better%20quality%2C%20ensuring%20they%20are%20fully%20aligned%20the%20new%0Acurriculum%20and%20the%20competence-based%20learning%20approach.%0A%20%20Methods%3A%20The%20prototype%20was%20created%20using%20Cohere%20LLM%20and%20Sentence%20Embeddings%2C%0Aand%20LangChain%20Framework%20-%20and%20thereafter%20made%20available%20on%20a%20public%20website.%0AVector%20stores%20were%20trained%20for%20three%20new%20curriculum%20textbooks%20%28ICT%2C%0AMathematics%2C%20History%29%2C%20all%20at%20Secondary%201%20Level.%20Twenty-four%20lessons%20plans%20were%0Agenerated%20following%20a%20pseudo-random%20generation%20protocol%2C%20based%20on%20the%20suggested%0Aperiods%20in%20the%20textbooks.%20The%20lesson%20plans%20were%20analyzed%20regarding%20their%0Atechnical%20quality%20by%20three%20independent%20raters%20following%20the%20Lesson%20Plan%0AAnalysis%20Protocol%20%28LPAP%29%20by%20Ndihokubwayo%20et%20al.%20%282022%29%20that%20is%20specifically%0Adesigned%20for%20East%20Africa%20and%20competence-based%20curriculums.%0A%20%20Results%3A%20Evaluation%20of%2024%20lesson%20plans%20using%20the%20LPAP%20resulted%20in%20an%20average%0Aquality%20of%20between%2075%20and%2080%25%2C%20corresponding%20to%20%22very%20good%20lesson%20plan%22.%20None%0Aof%20the%20lesson%20plans%20scored%20below%2065%25%2C%20although%20one%20lesson%20plan%20could%20be%20argued%0Ato%20have%20been%20missing%20the%20topic.%20In%20conclusion%2C%20the%20quality%20of%20the%20generated%0Alesson%20plans%20is%20at%20least%20comparable%2C%20if%20not%20better%2C%20than%20those%20created%20by%0Ahumans%2C%20as%20demonstrated%20in%20a%20study%20in%20Rwanda%2C%20whereby%20no%20lesson%20plan%20even%0Areached%20the%20benchmark%20of%2050%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07542v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DNew%2520Curriculum%252C%2520New%2520Chance%2520--%2520Retrieval%2520Augmented%2520Generation%2520for%2520Lesson%250A%2520%2520Planning%2520in%2520Ugandan%2520Secondary%2520Schools.%2520Prototype%2520Quality%2520Evaluation%26entry.906535625%3DSimon%2520Kloker%2520and%2520Herbertson%2520Bukoli%2520and%2520Twaha%2520Kateete%26entry.1292438233%3D%2520%2520Introduction%253A%2520Poor%2520educational%2520quality%2520in%2520Secondary%2520Schools%2520is%2520still%2520regarded%250Aas%2520one%2520of%2520the%2520major%2520struggles%2520in%252021st%2520century%2520Uganda%2520-%2520especially%2520in%2520rural%250Aareas.%2520Research%2520identifies%2520several%2520problems%252C%2520including%2520low%2520quality%2520or%2520absent%250Ateacher%2520lesson%2520planning.%2520As%2520the%2520government%2520pushes%2520towards%2520the%2520implementation%2520of%250Aa%2520new%2520curriculum%252C%2520exiting%2520lesson%2520plans%2520become%2520obsolete%2520and%2520the%2520problem%2520is%250Aworsened.%2520Using%2520a%2520Retrieval%2520Augmented%2520Generation%2520approach%252C%2520we%2520developed%2520a%250Aprototype%2520that%2520generates%2520customized%2520lesson%2520plans%2520based%2520on%2520the%250Agovernment-accredited%2520textbooks.%2520This%2520helps%2520teachers%2520create%2520lesson%2520plans%2520more%250Aefficiently%2520and%2520with%2520better%2520quality%252C%2520ensuring%2520they%2520are%2520fully%2520aligned%2520the%2520new%250Acurriculum%2520and%2520the%2520competence-based%2520learning%2520approach.%250A%2520%2520Methods%253A%2520The%2520prototype%2520was%2520created%2520using%2520Cohere%2520LLM%2520and%2520Sentence%2520Embeddings%252C%250Aand%2520LangChain%2520Framework%2520-%2520and%2520thereafter%2520made%2520available%2520on%2520a%2520public%2520website.%250AVector%2520stores%2520were%2520trained%2520for%2520three%2520new%2520curriculum%2520textbooks%2520%2528ICT%252C%250AMathematics%252C%2520History%2529%252C%2520all%2520at%2520Secondary%25201%2520Level.%2520Twenty-four%2520lessons%2520plans%2520were%250Agenerated%2520following%2520a%2520pseudo-random%2520generation%2520protocol%252C%2520based%2520on%2520the%2520suggested%250Aperiods%2520in%2520the%2520textbooks.%2520The%2520lesson%2520plans%2520were%2520analyzed%2520regarding%2520their%250Atechnical%2520quality%2520by%2520three%2520independent%2520raters%2520following%2520the%2520Lesson%2520Plan%250AAnalysis%2520Protocol%2520%2528LPAP%2529%2520by%2520Ndihokubwayo%2520et%2520al.%2520%25282022%2529%2520that%2520is%2520specifically%250Adesigned%2520for%2520East%2520Africa%2520and%2520competence-based%2520curriculums.%250A%2520%2520Results%253A%2520Evaluation%2520of%252024%2520lesson%2520plans%2520using%2520the%2520LPAP%2520resulted%2520in%2520an%2520average%250Aquality%2520of%2520between%252075%2520and%252080%2525%252C%2520corresponding%2520to%2520%2522very%2520good%2520lesson%2520plan%2522.%2520None%250Aof%2520the%2520lesson%2520plans%2520scored%2520below%252065%2525%252C%2520although%2520one%2520lesson%2520plan%2520could%2520be%2520argued%250Ato%2520have%2520been%2520missing%2520the%2520topic.%2520In%2520conclusion%252C%2520the%2520quality%2520of%2520the%2520generated%250Alesson%2520plans%2520is%2520at%2520least%2520comparable%252C%2520if%2520not%2520better%252C%2520than%2520those%2520created%2520by%250Ahumans%252C%2520as%2520demonstrated%2520in%2520a%2520study%2520in%2520Rwanda%252C%2520whereby%2520no%2520lesson%2520plan%2520even%250Areached%2520the%2520benchmark%2520of%252050%2525.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07542v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=New%20Curriculum%2C%20New%20Chance%20--%20Retrieval%20Augmented%20Generation%20for%20Lesson%0A%20%20Planning%20in%20Ugandan%20Secondary%20Schools.%20Prototype%20Quality%20Evaluation&entry.906535625=Simon%20Kloker%20and%20Herbertson%20Bukoli%20and%20Twaha%20Kateete&entry.1292438233=%20%20Introduction%3A%20Poor%20educational%20quality%20in%20Secondary%20Schools%20is%20still%20regarded%0Aas%20one%20of%20the%20major%20struggles%20in%2021st%20century%20Uganda%20-%20especially%20in%20rural%0Aareas.%20Research%20identifies%20several%20problems%2C%20including%20low%20quality%20or%20absent%0Ateacher%20lesson%20planning.%20As%20the%20government%20pushes%20towards%20the%20implementation%20of%0Aa%20new%20curriculum%2C%20exiting%20lesson%20plans%20become%20obsolete%20and%20the%20problem%20is%0Aworsened.%20Using%20a%20Retrieval%20Augmented%20Generation%20approach%2C%20we%20developed%20a%0Aprototype%20that%20generates%20customized%20lesson%20plans%20based%20on%20the%0Agovernment-accredited%20textbooks.%20This%20helps%20teachers%20create%20lesson%20plans%20more%0Aefficiently%20and%20with%20better%20quality%2C%20ensuring%20they%20are%20fully%20aligned%20the%20new%0Acurriculum%20and%20the%20competence-based%20learning%20approach.%0A%20%20Methods%3A%20The%20prototype%20was%20created%20using%20Cohere%20LLM%20and%20Sentence%20Embeddings%2C%0Aand%20LangChain%20Framework%20-%20and%20thereafter%20made%20available%20on%20a%20public%20website.%0AVector%20stores%20were%20trained%20for%20three%20new%20curriculum%20textbooks%20%28ICT%2C%0AMathematics%2C%20History%29%2C%20all%20at%20Secondary%201%20Level.%20Twenty-four%20lessons%20plans%20were%0Agenerated%20following%20a%20pseudo-random%20generation%20protocol%2C%20based%20on%20the%20suggested%0Aperiods%20in%20the%20textbooks.%20The%20lesson%20plans%20were%20analyzed%20regarding%20their%0Atechnical%20quality%20by%20three%20independent%20raters%20following%20the%20Lesson%20Plan%0AAnalysis%20Protocol%20%28LPAP%29%20by%20Ndihokubwayo%20et%20al.%20%282022%29%20that%20is%20specifically%0Adesigned%20for%20East%20Africa%20and%20competence-based%20curriculums.%0A%20%20Results%3A%20Evaluation%20of%2024%20lesson%20plans%20using%20the%20LPAP%20resulted%20in%20an%20average%0Aquality%20of%20between%2075%20and%2080%25%2C%20corresponding%20to%20%22very%20good%20lesson%20plan%22.%20None%0Aof%20the%20lesson%20plans%20scored%20below%2065%25%2C%20although%20one%20lesson%20plan%20could%20be%20argued%0Ato%20have%20been%20missing%20the%20topic.%20In%20conclusion%2C%20the%20quality%20of%20the%20generated%0Alesson%20plans%20is%20at%20least%20comparable%2C%20if%20not%20better%2C%20than%20those%20created%20by%0Ahumans%2C%20as%20demonstrated%20in%20a%20study%20in%20Rwanda%2C%20whereby%20no%20lesson%20plan%20even%0Areached%20the%20benchmark%20of%2050%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07542v1&entry.124074799=Read"},
{"title": "Adaptive Basis Function Selection for Computationally Efficient\n  Predictions", "author": "Anton Kullberg and Frida Viset and Isaac Skog and Gustaf Hendeby", "abstract": "  Basis Function (BF) expansions are a cornerstone of any engineer's toolbox\nfor computational function approximation which shares connections with both\nneural networks and Gaussian processes. Even though BF expansions are an\nintuitive and straightforward model to use, they suffer from quadratic\ncomputational complexity in the number of BFs if the predictive variance is to\nbe computed. We develop a method to automatically select the most important BFs\nfor prediction in a sub-domain of the model domain. This significantly reduces\nthe computational complexity of computing predictions while maintaining\npredictive accuracy. The proposed method is demonstrated using two numerical\nexamples, where reductions up to 50-75% are possible without significantly\nreducing the predictive accuracy.\n", "link": "http://arxiv.org/abs/2408.07480v1", "date": "2024-08-14", "relevancy": 1.8368, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5011}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4543}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4473}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Adaptive%20Basis%20Function%20Selection%20for%20Computationally%20Efficient%0A%20%20Predictions&body=Title%3A%20Adaptive%20Basis%20Function%20Selection%20for%20Computationally%20Efficient%0A%20%20Predictions%0AAuthor%3A%20Anton%20Kullberg%20and%20Frida%20Viset%20and%20Isaac%20Skog%20and%20Gustaf%20Hendeby%0AAbstract%3A%20%20%20Basis%20Function%20%28BF%29%20expansions%20are%20a%20cornerstone%20of%20any%20engineer%27s%20toolbox%0Afor%20computational%20function%20approximation%20which%20shares%20connections%20with%20both%0Aneural%20networks%20and%20Gaussian%20processes.%20Even%20though%20BF%20expansions%20are%20an%0Aintuitive%20and%20straightforward%20model%20to%20use%2C%20they%20suffer%20from%20quadratic%0Acomputational%20complexity%20in%20the%20number%20of%20BFs%20if%20the%20predictive%20variance%20is%20to%0Abe%20computed.%20We%20develop%20a%20method%20to%20automatically%20select%20the%20most%20important%20BFs%0Afor%20prediction%20in%20a%20sub-domain%20of%20the%20model%20domain.%20This%20significantly%20reduces%0Athe%20computational%20complexity%20of%20computing%20predictions%20while%20maintaining%0Apredictive%20accuracy.%20The%20proposed%20method%20is%20demonstrated%20using%20two%20numerical%0Aexamples%2C%20where%20reductions%20up%20to%2050-75%25%20are%20possible%20without%20significantly%0Areducing%20the%20predictive%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07480v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DAdaptive%2520Basis%2520Function%2520Selection%2520for%2520Computationally%2520Efficient%250A%2520%2520Predictions%26entry.906535625%3DAnton%2520Kullberg%2520and%2520Frida%2520Viset%2520and%2520Isaac%2520Skog%2520and%2520Gustaf%2520Hendeby%26entry.1292438233%3D%2520%2520Basis%2520Function%2520%2528BF%2529%2520expansions%2520are%2520a%2520cornerstone%2520of%2520any%2520engineer%2527s%2520toolbox%250Afor%2520computational%2520function%2520approximation%2520which%2520shares%2520connections%2520with%2520both%250Aneural%2520networks%2520and%2520Gaussian%2520processes.%2520Even%2520though%2520BF%2520expansions%2520are%2520an%250Aintuitive%2520and%2520straightforward%2520model%2520to%2520use%252C%2520they%2520suffer%2520from%2520quadratic%250Acomputational%2520complexity%2520in%2520the%2520number%2520of%2520BFs%2520if%2520the%2520predictive%2520variance%2520is%2520to%250Abe%2520computed.%2520We%2520develop%2520a%2520method%2520to%2520automatically%2520select%2520the%2520most%2520important%2520BFs%250Afor%2520prediction%2520in%2520a%2520sub-domain%2520of%2520the%2520model%2520domain.%2520This%2520significantly%2520reduces%250Athe%2520computational%2520complexity%2520of%2520computing%2520predictions%2520while%2520maintaining%250Apredictive%2520accuracy.%2520The%2520proposed%2520method%2520is%2520demonstrated%2520using%2520two%2520numerical%250Aexamples%252C%2520where%2520reductions%2520up%2520to%252050-75%2525%2520are%2520possible%2520without%2520significantly%250Areducing%2520the%2520predictive%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07480v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Adaptive%20Basis%20Function%20Selection%20for%20Computationally%20Efficient%0A%20%20Predictions&entry.906535625=Anton%20Kullberg%20and%20Frida%20Viset%20and%20Isaac%20Skog%20and%20Gustaf%20Hendeby&entry.1292438233=%20%20Basis%20Function%20%28BF%29%20expansions%20are%20a%20cornerstone%20of%20any%20engineer%27s%20toolbox%0Afor%20computational%20function%20approximation%20which%20shares%20connections%20with%20both%0Aneural%20networks%20and%20Gaussian%20processes.%20Even%20though%20BF%20expansions%20are%20an%0Aintuitive%20and%20straightforward%20model%20to%20use%2C%20they%20suffer%20from%20quadratic%0Acomputational%20complexity%20in%20the%20number%20of%20BFs%20if%20the%20predictive%20variance%20is%20to%0Abe%20computed.%20We%20develop%20a%20method%20to%20automatically%20select%20the%20most%20important%20BFs%0Afor%20prediction%20in%20a%20sub-domain%20of%20the%20model%20domain.%20This%20significantly%20reduces%0Athe%20computational%20complexity%20of%20computing%20predictions%20while%20maintaining%0Apredictive%20accuracy.%20The%20proposed%20method%20is%20demonstrated%20using%20two%20numerical%0Aexamples%2C%20where%20reductions%20up%20to%2050-75%25%20are%20possible%20without%20significantly%0Areducing%20the%20predictive%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07480v1&entry.124074799=Read"},
{"title": "Distilling the Knowledge in Data Pruning", "author": "Emanuel Ben-Baruch and Adam Botach and Igor Kviatkovsky and Manoj Aggarwal and G\u00e9rard Medioni", "abstract": "  With the increasing size of datasets used for training neural networks, data\npruning becomes an attractive field of research. However, most current data\npruning algorithms are limited in their ability to preserve accuracy compared\nto models trained on the full data, especially in high pruning regimes. In this\npaper we explore the application of data pruning while incorporating knowledge\ndistillation (KD) when training on a pruned subset. That is, rather than\nrelying solely on ground-truth labels, we also use the soft predictions from a\nteacher network pre-trained on the complete data. By integrating KD into\ntraining, we demonstrate significant improvement across datasets, pruning\nmethods, and on all pruning fractions. We first establish a theoretical\nmotivation for employing self-distillation to improve training on pruned data.\nThen, we empirically make a compelling and highly practical observation: using\nKD, simple random pruning is comparable or superior to sophisticated pruning\nmethods across all pruning regimes. On ImageNet for example, we achieve\nsuperior accuracy despite training on a random subset of only 50% of the data.\nAdditionally, we demonstrate a crucial connection between the pruning factor\nand the optimal knowledge distillation weight. This helps mitigate the impact\nof samples with noisy labels and low-quality images retained by typical pruning\nalgorithms. Finally, we make an intriguing observation: when using lower\npruning fractions, larger teachers lead to accuracy degradation, while\nsurprisingly, employing teachers with a smaller capacity than the student's may\nimprove results. Our code will be made available.\n", "link": "http://arxiv.org/abs/2403.07854v2", "date": "2024-08-14", "relevancy": 1.8326, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4776}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4511}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4415}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Distilling%20the%20Knowledge%20in%20Data%20Pruning&body=Title%3A%20Distilling%20the%20Knowledge%20in%20Data%20Pruning%0AAuthor%3A%20Emanuel%20Ben-Baruch%20and%20Adam%20Botach%20and%20Igor%20Kviatkovsky%20and%20Manoj%20Aggarwal%20and%20G%C3%A9rard%20Medioni%0AAbstract%3A%20%20%20With%20the%20increasing%20size%20of%20datasets%20used%20for%20training%20neural%20networks%2C%20data%0Apruning%20becomes%20an%20attractive%20field%20of%20research.%20However%2C%20most%20current%20data%0Apruning%20algorithms%20are%20limited%20in%20their%20ability%20to%20preserve%20accuracy%20compared%0Ato%20models%20trained%20on%20the%20full%20data%2C%20especially%20in%20high%20pruning%20regimes.%20In%20this%0Apaper%20we%20explore%20the%20application%20of%20data%20pruning%20while%20incorporating%20knowledge%0Adistillation%20%28KD%29%20when%20training%20on%20a%20pruned%20subset.%20That%20is%2C%20rather%20than%0Arelying%20solely%20on%20ground-truth%20labels%2C%20we%20also%20use%20the%20soft%20predictions%20from%20a%0Ateacher%20network%20pre-trained%20on%20the%20complete%20data.%20By%20integrating%20KD%20into%0Atraining%2C%20we%20demonstrate%20significant%20improvement%20across%20datasets%2C%20pruning%0Amethods%2C%20and%20on%20all%20pruning%20fractions.%20We%20first%20establish%20a%20theoretical%0Amotivation%20for%20employing%20self-distillation%20to%20improve%20training%20on%20pruned%20data.%0AThen%2C%20we%20empirically%20make%20a%20compelling%20and%20highly%20practical%20observation%3A%20using%0AKD%2C%20simple%20random%20pruning%20is%20comparable%20or%20superior%20to%20sophisticated%20pruning%0Amethods%20across%20all%20pruning%20regimes.%20On%20ImageNet%20for%20example%2C%20we%20achieve%0Asuperior%20accuracy%20despite%20training%20on%20a%20random%20subset%20of%20only%2050%25%20of%20the%20data.%0AAdditionally%2C%20we%20demonstrate%20a%20crucial%20connection%20between%20the%20pruning%20factor%0Aand%20the%20optimal%20knowledge%20distillation%20weight.%20This%20helps%20mitigate%20the%20impact%0Aof%20samples%20with%20noisy%20labels%20and%20low-quality%20images%20retained%20by%20typical%20pruning%0Aalgorithms.%20Finally%2C%20we%20make%20an%20intriguing%20observation%3A%20when%20using%20lower%0Apruning%20fractions%2C%20larger%20teachers%20lead%20to%20accuracy%20degradation%2C%20while%0Asurprisingly%2C%20employing%20teachers%20with%20a%20smaller%20capacity%20than%20the%20student%27s%20may%0Aimprove%20results.%20Our%20code%20will%20be%20made%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07854v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DDistilling%2520the%2520Knowledge%2520in%2520Data%2520Pruning%26entry.906535625%3DEmanuel%2520Ben-Baruch%2520and%2520Adam%2520Botach%2520and%2520Igor%2520Kviatkovsky%2520and%2520Manoj%2520Aggarwal%2520and%2520G%25C3%25A9rard%2520Medioni%26entry.1292438233%3D%2520%2520With%2520the%2520increasing%2520size%2520of%2520datasets%2520used%2520for%2520training%2520neural%2520networks%252C%2520data%250Apruning%2520becomes%2520an%2520attractive%2520field%2520of%2520research.%2520However%252C%2520most%2520current%2520data%250Apruning%2520algorithms%2520are%2520limited%2520in%2520their%2520ability%2520to%2520preserve%2520accuracy%2520compared%250Ato%2520models%2520trained%2520on%2520the%2520full%2520data%252C%2520especially%2520in%2520high%2520pruning%2520regimes.%2520In%2520this%250Apaper%2520we%2520explore%2520the%2520application%2520of%2520data%2520pruning%2520while%2520incorporating%2520knowledge%250Adistillation%2520%2528KD%2529%2520when%2520training%2520on%2520a%2520pruned%2520subset.%2520That%2520is%252C%2520rather%2520than%250Arelying%2520solely%2520on%2520ground-truth%2520labels%252C%2520we%2520also%2520use%2520the%2520soft%2520predictions%2520from%2520a%250Ateacher%2520network%2520pre-trained%2520on%2520the%2520complete%2520data.%2520By%2520integrating%2520KD%2520into%250Atraining%252C%2520we%2520demonstrate%2520significant%2520improvement%2520across%2520datasets%252C%2520pruning%250Amethods%252C%2520and%2520on%2520all%2520pruning%2520fractions.%2520We%2520first%2520establish%2520a%2520theoretical%250Amotivation%2520for%2520employing%2520self-distillation%2520to%2520improve%2520training%2520on%2520pruned%2520data.%250AThen%252C%2520we%2520empirically%2520make%2520a%2520compelling%2520and%2520highly%2520practical%2520observation%253A%2520using%250AKD%252C%2520simple%2520random%2520pruning%2520is%2520comparable%2520or%2520superior%2520to%2520sophisticated%2520pruning%250Amethods%2520across%2520all%2520pruning%2520regimes.%2520On%2520ImageNet%2520for%2520example%252C%2520we%2520achieve%250Asuperior%2520accuracy%2520despite%2520training%2520on%2520a%2520random%2520subset%2520of%2520only%252050%2525%2520of%2520the%2520data.%250AAdditionally%252C%2520we%2520demonstrate%2520a%2520crucial%2520connection%2520between%2520the%2520pruning%2520factor%250Aand%2520the%2520optimal%2520knowledge%2520distillation%2520weight.%2520This%2520helps%2520mitigate%2520the%2520impact%250Aof%2520samples%2520with%2520noisy%2520labels%2520and%2520low-quality%2520images%2520retained%2520by%2520typical%2520pruning%250Aalgorithms.%2520Finally%252C%2520we%2520make%2520an%2520intriguing%2520observation%253A%2520when%2520using%2520lower%250Apruning%2520fractions%252C%2520larger%2520teachers%2520lead%2520to%2520accuracy%2520degradation%252C%2520while%250Asurprisingly%252C%2520employing%2520teachers%2520with%2520a%2520smaller%2520capacity%2520than%2520the%2520student%2527s%2520may%250Aimprove%2520results.%2520Our%2520code%2520will%2520be%2520made%2520available.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2403.07854v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Distilling%20the%20Knowledge%20in%20Data%20Pruning&entry.906535625=Emanuel%20Ben-Baruch%20and%20Adam%20Botach%20and%20Igor%20Kviatkovsky%20and%20Manoj%20Aggarwal%20and%20G%C3%A9rard%20Medioni&entry.1292438233=%20%20With%20the%20increasing%20size%20of%20datasets%20used%20for%20training%20neural%20networks%2C%20data%0Apruning%20becomes%20an%20attractive%20field%20of%20research.%20However%2C%20most%20current%20data%0Apruning%20algorithms%20are%20limited%20in%20their%20ability%20to%20preserve%20accuracy%20compared%0Ato%20models%20trained%20on%20the%20full%20data%2C%20especially%20in%20high%20pruning%20regimes.%20In%20this%0Apaper%20we%20explore%20the%20application%20of%20data%20pruning%20while%20incorporating%20knowledge%0Adistillation%20%28KD%29%20when%20training%20on%20a%20pruned%20subset.%20That%20is%2C%20rather%20than%0Arelying%20solely%20on%20ground-truth%20labels%2C%20we%20also%20use%20the%20soft%20predictions%20from%20a%0Ateacher%20network%20pre-trained%20on%20the%20complete%20data.%20By%20integrating%20KD%20into%0Atraining%2C%20we%20demonstrate%20significant%20improvement%20across%20datasets%2C%20pruning%0Amethods%2C%20and%20on%20all%20pruning%20fractions.%20We%20first%20establish%20a%20theoretical%0Amotivation%20for%20employing%20self-distillation%20to%20improve%20training%20on%20pruned%20data.%0AThen%2C%20we%20empirically%20make%20a%20compelling%20and%20highly%20practical%20observation%3A%20using%0AKD%2C%20simple%20random%20pruning%20is%20comparable%20or%20superior%20to%20sophisticated%20pruning%0Amethods%20across%20all%20pruning%20regimes.%20On%20ImageNet%20for%20example%2C%20we%20achieve%0Asuperior%20accuracy%20despite%20training%20on%20a%20random%20subset%20of%20only%2050%25%20of%20the%20data.%0AAdditionally%2C%20we%20demonstrate%20a%20crucial%20connection%20between%20the%20pruning%20factor%0Aand%20the%20optimal%20knowledge%20distillation%20weight.%20This%20helps%20mitigate%20the%20impact%0Aof%20samples%20with%20noisy%20labels%20and%20low-quality%20images%20retained%20by%20typical%20pruning%0Aalgorithms.%20Finally%2C%20we%20make%20an%20intriguing%20observation%3A%20when%20using%20lower%0Apruning%20fractions%2C%20larger%20teachers%20lead%20to%20accuracy%20degradation%2C%20while%0Asurprisingly%2C%20employing%20teachers%20with%20a%20smaller%20capacity%20than%20the%20student%27s%20may%0Aimprove%20results.%20Our%20code%20will%20be%20made%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07854v2&entry.124074799=Read"},
{"title": "Exploring Retrieval Augmented Generation in Arabic", "author": "Samhaa R. El-Beltagy and Mohamed A. Abdallah", "abstract": "  Recently, Retrieval Augmented Generation (RAG) has emerged as a powerful\ntechnique in natural language processing, combining the strengths of\nretrieval-based and generation-based models to enhance text generation tasks.\nHowever, the application of RAG in Arabic, a language with unique\ncharacteristics and resource constraints, remains underexplored. This paper\npresents a comprehensive case study on the implementation and evaluation of RAG\nfor Arabic text. The work focuses on exploring various semantic embedding\nmodels in the retrieval stage and several LLMs in the generation stage, in\norder to investigate what works and what doesn't in the context of Arabic. The\nwork also touches upon the issue of variations between document dialect and\nquery dialect in the retrieval stage. Results show that existing semantic\nembedding models and LLMs can be effectively employed to build Arabic RAG\npipelines.\n", "link": "http://arxiv.org/abs/2408.07425v1", "date": "2024-08-14", "relevancy": 1.8326, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4638}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4619}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.451}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Exploring%20Retrieval%20Augmented%20Generation%20in%20Arabic&body=Title%3A%20Exploring%20Retrieval%20Augmented%20Generation%20in%20Arabic%0AAuthor%3A%20Samhaa%20R.%20El-Beltagy%20and%20Mohamed%20A.%20Abdallah%0AAbstract%3A%20%20%20Recently%2C%20Retrieval%20Augmented%20Generation%20%28RAG%29%20has%20emerged%20as%20a%20powerful%0Atechnique%20in%20natural%20language%20processing%2C%20combining%20the%20strengths%20of%0Aretrieval-based%20and%20generation-based%20models%20to%20enhance%20text%20generation%20tasks.%0AHowever%2C%20the%20application%20of%20RAG%20in%20Arabic%2C%20a%20language%20with%20unique%0Acharacteristics%20and%20resource%20constraints%2C%20remains%20underexplored.%20This%20paper%0Apresents%20a%20comprehensive%20case%20study%20on%20the%20implementation%20and%20evaluation%20of%20RAG%0Afor%20Arabic%20text.%20The%20work%20focuses%20on%20exploring%20various%20semantic%20embedding%0Amodels%20in%20the%20retrieval%20stage%20and%20several%20LLMs%20in%20the%20generation%20stage%2C%20in%0Aorder%20to%20investigate%20what%20works%20and%20what%20doesn%27t%20in%20the%20context%20of%20Arabic.%20The%0Awork%20also%20touches%20upon%20the%20issue%20of%20variations%20between%20document%20dialect%20and%0Aquery%20dialect%20in%20the%20retrieval%20stage.%20Results%20show%20that%20existing%20semantic%0Aembedding%20models%20and%20LLMs%20can%20be%20effectively%20employed%20to%20build%20Arabic%20RAG%0Apipelines.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07425v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DExploring%2520Retrieval%2520Augmented%2520Generation%2520in%2520Arabic%26entry.906535625%3DSamhaa%2520R.%2520El-Beltagy%2520and%2520Mohamed%2520A.%2520Abdallah%26entry.1292438233%3D%2520%2520Recently%252C%2520Retrieval%2520Augmented%2520Generation%2520%2528RAG%2529%2520has%2520emerged%2520as%2520a%2520powerful%250Atechnique%2520in%2520natural%2520language%2520processing%252C%2520combining%2520the%2520strengths%2520of%250Aretrieval-based%2520and%2520generation-based%2520models%2520to%2520enhance%2520text%2520generation%2520tasks.%250AHowever%252C%2520the%2520application%2520of%2520RAG%2520in%2520Arabic%252C%2520a%2520language%2520with%2520unique%250Acharacteristics%2520and%2520resource%2520constraints%252C%2520remains%2520underexplored.%2520This%2520paper%250Apresents%2520a%2520comprehensive%2520case%2520study%2520on%2520the%2520implementation%2520and%2520evaluation%2520of%2520RAG%250Afor%2520Arabic%2520text.%2520The%2520work%2520focuses%2520on%2520exploring%2520various%2520semantic%2520embedding%250Amodels%2520in%2520the%2520retrieval%2520stage%2520and%2520several%2520LLMs%2520in%2520the%2520generation%2520stage%252C%2520in%250Aorder%2520to%2520investigate%2520what%2520works%2520and%2520what%2520doesn%2527t%2520in%2520the%2520context%2520of%2520Arabic.%2520The%250Awork%2520also%2520touches%2520upon%2520the%2520issue%2520of%2520variations%2520between%2520document%2520dialect%2520and%250Aquery%2520dialect%2520in%2520the%2520retrieval%2520stage.%2520Results%2520show%2520that%2520existing%2520semantic%250Aembedding%2520models%2520and%2520LLMs%2520can%2520be%2520effectively%2520employed%2520to%2520build%2520Arabic%2520RAG%250Apipelines.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07425v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%20Retrieval%20Augmented%20Generation%20in%20Arabic&entry.906535625=Samhaa%20R.%20El-Beltagy%20and%20Mohamed%20A.%20Abdallah&entry.1292438233=%20%20Recently%2C%20Retrieval%20Augmented%20Generation%20%28RAG%29%20has%20emerged%20as%20a%20powerful%0Atechnique%20in%20natural%20language%20processing%2C%20combining%20the%20strengths%20of%0Aretrieval-based%20and%20generation-based%20models%20to%20enhance%20text%20generation%20tasks.%0AHowever%2C%20the%20application%20of%20RAG%20in%20Arabic%2C%20a%20language%20with%20unique%0Acharacteristics%20and%20resource%20constraints%2C%20remains%20underexplored.%20This%20paper%0Apresents%20a%20comprehensive%20case%20study%20on%20the%20implementation%20and%20evaluation%20of%20RAG%0Afor%20Arabic%20text.%20The%20work%20focuses%20on%20exploring%20various%20semantic%20embedding%0Amodels%20in%20the%20retrieval%20stage%20and%20several%20LLMs%20in%20the%20generation%20stage%2C%20in%0Aorder%20to%20investigate%20what%20works%20and%20what%20doesn%27t%20in%20the%20context%20of%20Arabic.%20The%0Awork%20also%20touches%20upon%20the%20issue%20of%20variations%20between%20document%20dialect%20and%0Aquery%20dialect%20in%20the%20retrieval%20stage.%20Results%20show%20that%20existing%20semantic%0Aembedding%20models%20and%20LLMs%20can%20be%20effectively%20employed%20to%20build%20Arabic%20RAG%0Apipelines.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07425v1&entry.124074799=Read"},
{"title": "MEEG and AT-DGNN: Improving EEG Emotion Recognition with Music\n  Introducing and Graph-based Learning", "author": "Minghao Xiao and Zhengxi Zhu and Bin Jiang and Meixia Qu and Wenyu Wang", "abstract": "  We present the MEEG dataset, a multi-modal collection of music-induced\nelectroencephalogram (EEG) recordings designed to capture emotional responses\nto various musical stimuli across different valence and arousal levels. This\npublic dataset facilitates an in-depth examination of brainwave patterns within\nmusical contexts, providing a robust foundation for studying brain network\ntopology during emotional processing. Leveraging the MEEG dataset, we introduce\nthe Attention-based Temporal Learner with Dynamic Graph Neural Network\n(AT-DGNN), a novel framework for EEG-based emotion recognition. This model\ncombines an attention mechanism with a dynamic graph neural network (DGNN) to\ncapture intricate EEG dynamics. The AT-DGNN achieves state-of-the-art (SOTA)\nperformance with an accuracy of 83.74% in arousal recognition and 86.01% in\nvalence recognition, outperforming existing SOTA methods. Comparative analysis\nwith traditional datasets, such as DEAP, further validates the model's\neffectiveness and underscores the potency of music as an emotional stimulus.\nThis study advances graph-based learning methodology in brain-computer\ninterfaces (BCI), significantly improving the accuracy of EEG-based emotion\nrecognition. The MEEG dataset and source code are publicly available at\nhttps://github.com/xmh1011/AT-DGNN.\n", "link": "http://arxiv.org/abs/2407.05550v3", "date": "2024-08-14", "relevancy": 1.8284, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4794}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4445}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.433}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MEEG%20and%20AT-DGNN%3A%20Improving%20EEG%20Emotion%20Recognition%20with%20Music%0A%20%20Introducing%20and%20Graph-based%20Learning&body=Title%3A%20MEEG%20and%20AT-DGNN%3A%20Improving%20EEG%20Emotion%20Recognition%20with%20Music%0A%20%20Introducing%20and%20Graph-based%20Learning%0AAuthor%3A%20Minghao%20Xiao%20and%20Zhengxi%20Zhu%20and%20Bin%20Jiang%20and%20Meixia%20Qu%20and%20Wenyu%20Wang%0AAbstract%3A%20%20%20We%20present%20the%20MEEG%20dataset%2C%20a%20multi-modal%20collection%20of%20music-induced%0Aelectroencephalogram%20%28EEG%29%20recordings%20designed%20to%20capture%20emotional%20responses%0Ato%20various%20musical%20stimuli%20across%20different%20valence%20and%20arousal%20levels.%20This%0Apublic%20dataset%20facilitates%20an%20in-depth%20examination%20of%20brainwave%20patterns%20within%0Amusical%20contexts%2C%20providing%20a%20robust%20foundation%20for%20studying%20brain%20network%0Atopology%20during%20emotional%20processing.%20Leveraging%20the%20MEEG%20dataset%2C%20we%20introduce%0Athe%20Attention-based%20Temporal%20Learner%20with%20Dynamic%20Graph%20Neural%20Network%0A%28AT-DGNN%29%2C%20a%20novel%20framework%20for%20EEG-based%20emotion%20recognition.%20This%20model%0Acombines%20an%20attention%20mechanism%20with%20a%20dynamic%20graph%20neural%20network%20%28DGNN%29%20to%0Acapture%20intricate%20EEG%20dynamics.%20The%20AT-DGNN%20achieves%20state-of-the-art%20%28SOTA%29%0Aperformance%20with%20an%20accuracy%20of%2083.74%25%20in%20arousal%20recognition%20and%2086.01%25%20in%0Avalence%20recognition%2C%20outperforming%20existing%20SOTA%20methods.%20Comparative%20analysis%0Awith%20traditional%20datasets%2C%20such%20as%20DEAP%2C%20further%20validates%20the%20model%27s%0Aeffectiveness%20and%20underscores%20the%20potency%20of%20music%20as%20an%20emotional%20stimulus.%0AThis%20study%20advances%20graph-based%20learning%20methodology%20in%20brain-computer%0Ainterfaces%20%28BCI%29%2C%20significantly%20improving%20the%20accuracy%20of%20EEG-based%20emotion%0Arecognition.%20The%20MEEG%20dataset%20and%20source%20code%20are%20publicly%20available%20at%0Ahttps%3A//github.com/xmh1011/AT-DGNN.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.05550v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMEEG%2520and%2520AT-DGNN%253A%2520Improving%2520EEG%2520Emotion%2520Recognition%2520with%2520Music%250A%2520%2520Introducing%2520and%2520Graph-based%2520Learning%26entry.906535625%3DMinghao%2520Xiao%2520and%2520Zhengxi%2520Zhu%2520and%2520Bin%2520Jiang%2520and%2520Meixia%2520Qu%2520and%2520Wenyu%2520Wang%26entry.1292438233%3D%2520%2520We%2520present%2520the%2520MEEG%2520dataset%252C%2520a%2520multi-modal%2520collection%2520of%2520music-induced%250Aelectroencephalogram%2520%2528EEG%2529%2520recordings%2520designed%2520to%2520capture%2520emotional%2520responses%250Ato%2520various%2520musical%2520stimuli%2520across%2520different%2520valence%2520and%2520arousal%2520levels.%2520This%250Apublic%2520dataset%2520facilitates%2520an%2520in-depth%2520examination%2520of%2520brainwave%2520patterns%2520within%250Amusical%2520contexts%252C%2520providing%2520a%2520robust%2520foundation%2520for%2520studying%2520brain%2520network%250Atopology%2520during%2520emotional%2520processing.%2520Leveraging%2520the%2520MEEG%2520dataset%252C%2520we%2520introduce%250Athe%2520Attention-based%2520Temporal%2520Learner%2520with%2520Dynamic%2520Graph%2520Neural%2520Network%250A%2528AT-DGNN%2529%252C%2520a%2520novel%2520framework%2520for%2520EEG-based%2520emotion%2520recognition.%2520This%2520model%250Acombines%2520an%2520attention%2520mechanism%2520with%2520a%2520dynamic%2520graph%2520neural%2520network%2520%2528DGNN%2529%2520to%250Acapture%2520intricate%2520EEG%2520dynamics.%2520The%2520AT-DGNN%2520achieves%2520state-of-the-art%2520%2528SOTA%2529%250Aperformance%2520with%2520an%2520accuracy%2520of%252083.74%2525%2520in%2520arousal%2520recognition%2520and%252086.01%2525%2520in%250Avalence%2520recognition%252C%2520outperforming%2520existing%2520SOTA%2520methods.%2520Comparative%2520analysis%250Awith%2520traditional%2520datasets%252C%2520such%2520as%2520DEAP%252C%2520further%2520validates%2520the%2520model%2527s%250Aeffectiveness%2520and%2520underscores%2520the%2520potency%2520of%2520music%2520as%2520an%2520emotional%2520stimulus.%250AThis%2520study%2520advances%2520graph-based%2520learning%2520methodology%2520in%2520brain-computer%250Ainterfaces%2520%2528BCI%2529%252C%2520significantly%2520improving%2520the%2520accuracy%2520of%2520EEG-based%2520emotion%250Arecognition.%2520The%2520MEEG%2520dataset%2520and%2520source%2520code%2520are%2520publicly%2520available%2520at%250Ahttps%253A//github.com/xmh1011/AT-DGNN.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.05550v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MEEG%20and%20AT-DGNN%3A%20Improving%20EEG%20Emotion%20Recognition%20with%20Music%0A%20%20Introducing%20and%20Graph-based%20Learning&entry.906535625=Minghao%20Xiao%20and%20Zhengxi%20Zhu%20and%20Bin%20Jiang%20and%20Meixia%20Qu%20and%20Wenyu%20Wang&entry.1292438233=%20%20We%20present%20the%20MEEG%20dataset%2C%20a%20multi-modal%20collection%20of%20music-induced%0Aelectroencephalogram%20%28EEG%29%20recordings%20designed%20to%20capture%20emotional%20responses%0Ato%20various%20musical%20stimuli%20across%20different%20valence%20and%20arousal%20levels.%20This%0Apublic%20dataset%20facilitates%20an%20in-depth%20examination%20of%20brainwave%20patterns%20within%0Amusical%20contexts%2C%20providing%20a%20robust%20foundation%20for%20studying%20brain%20network%0Atopology%20during%20emotional%20processing.%20Leveraging%20the%20MEEG%20dataset%2C%20we%20introduce%0Athe%20Attention-based%20Temporal%20Learner%20with%20Dynamic%20Graph%20Neural%20Network%0A%28AT-DGNN%29%2C%20a%20novel%20framework%20for%20EEG-based%20emotion%20recognition.%20This%20model%0Acombines%20an%20attention%20mechanism%20with%20a%20dynamic%20graph%20neural%20network%20%28DGNN%29%20to%0Acapture%20intricate%20EEG%20dynamics.%20The%20AT-DGNN%20achieves%20state-of-the-art%20%28SOTA%29%0Aperformance%20with%20an%20accuracy%20of%2083.74%25%20in%20arousal%20recognition%20and%2086.01%25%20in%0Avalence%20recognition%2C%20outperforming%20existing%20SOTA%20methods.%20Comparative%20analysis%0Awith%20traditional%20datasets%2C%20such%20as%20DEAP%2C%20further%20validates%20the%20model%27s%0Aeffectiveness%20and%20underscores%20the%20potency%20of%20music%20as%20an%20emotional%20stimulus.%0AThis%20study%20advances%20graph-based%20learning%20methodology%20in%20brain-computer%0Ainterfaces%20%28BCI%29%2C%20significantly%20improving%20the%20accuracy%20of%20EEG-based%20emotion%0Arecognition.%20The%20MEEG%20dataset%20and%20source%20code%20are%20publicly%20available%20at%0Ahttps%3A//github.com/xmh1011/AT-DGNN.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.05550v3&entry.124074799=Read"},
{"title": "BlockFUL: Enabling Unlearning in Blockchained Federated Learning", "author": "Xiao Liu and Mingyuan Li and Xu Wang and Guangsheng Yu and Wei Ni and Lixiang Li and Haipeng Peng and Renping Liu", "abstract": "  Unlearning in Federated Learning (FL) presents significant challenges, as\nmodels grow and evolve with complex inheritance relationships. This complexity\nis amplified when blockchain is employed to ensure the integrity and\ntraceability of FL, where the need to edit multiple interlinked blockchain\nrecords and update all inherited models complicates the process.In this paper,\nwe introduce Blockchained Federated Unlearning (BlockFUL), a novel framework\nwith a dual-chain structure comprising a live chain and an archive chain for\nenabling unlearning capabilities within Blockchained FL. BlockFUL introduces\ntwo new unlearning paradigms, i.e., parallel and sequential paradigms, which\ncan be effectively implemented through gradient-ascent-based and\nre-training-based unlearning methods. These methods enhance the unlearning\nprocess across multiple inherited models by enabling efficient consensus\noperations and reducing computational costs. Our extensive experiments validate\nthat these methods effectively reduce data dependency and operational overhead,\nthereby boosting the overall performance of unlearning inherited models within\nBlockFUL on CIFAR-10 and Fashion-MNIST datasets using AlexNet, ResNet18, and\nMobileNetV2 models.\n", "link": "http://arxiv.org/abs/2402.16294v2", "date": "2024-08-14", "relevancy": 1.8263, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4662}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4508}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4469}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20BlockFUL%3A%20Enabling%20Unlearning%20in%20Blockchained%20Federated%20Learning&body=Title%3A%20BlockFUL%3A%20Enabling%20Unlearning%20in%20Blockchained%20Federated%20Learning%0AAuthor%3A%20Xiao%20Liu%20and%20Mingyuan%20Li%20and%20Xu%20Wang%20and%20Guangsheng%20Yu%20and%20Wei%20Ni%20and%20Lixiang%20Li%20and%20Haipeng%20Peng%20and%20Renping%20Liu%0AAbstract%3A%20%20%20Unlearning%20in%20Federated%20Learning%20%28FL%29%20presents%20significant%20challenges%2C%20as%0Amodels%20grow%20and%20evolve%20with%20complex%20inheritance%20relationships.%20This%20complexity%0Ais%20amplified%20when%20blockchain%20is%20employed%20to%20ensure%20the%20integrity%20and%0Atraceability%20of%20FL%2C%20where%20the%20need%20to%20edit%20multiple%20interlinked%20blockchain%0Arecords%20and%20update%20all%20inherited%20models%20complicates%20the%20process.In%20this%20paper%2C%0Awe%20introduce%20Blockchained%20Federated%20Unlearning%20%28BlockFUL%29%2C%20a%20novel%20framework%0Awith%20a%20dual-chain%20structure%20comprising%20a%20live%20chain%20and%20an%20archive%20chain%20for%0Aenabling%20unlearning%20capabilities%20within%20Blockchained%20FL.%20BlockFUL%20introduces%0Atwo%20new%20unlearning%20paradigms%2C%20i.e.%2C%20parallel%20and%20sequential%20paradigms%2C%20which%0Acan%20be%20effectively%20implemented%20through%20gradient-ascent-based%20and%0Are-training-based%20unlearning%20methods.%20These%20methods%20enhance%20the%20unlearning%0Aprocess%20across%20multiple%20inherited%20models%20by%20enabling%20efficient%20consensus%0Aoperations%20and%20reducing%20computational%20costs.%20Our%20extensive%20experiments%20validate%0Athat%20these%20methods%20effectively%20reduce%20data%20dependency%20and%20operational%20overhead%2C%0Athereby%20boosting%20the%20overall%20performance%20of%20unlearning%20inherited%20models%20within%0ABlockFUL%20on%20CIFAR-10%20and%20Fashion-MNIST%20datasets%20using%20AlexNet%2C%20ResNet18%2C%20and%0AMobileNetV2%20models.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.16294v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DBlockFUL%253A%2520Enabling%2520Unlearning%2520in%2520Blockchained%2520Federated%2520Learning%26entry.906535625%3DXiao%2520Liu%2520and%2520Mingyuan%2520Li%2520and%2520Xu%2520Wang%2520and%2520Guangsheng%2520Yu%2520and%2520Wei%2520Ni%2520and%2520Lixiang%2520Li%2520and%2520Haipeng%2520Peng%2520and%2520Renping%2520Liu%26entry.1292438233%3D%2520%2520Unlearning%2520in%2520Federated%2520Learning%2520%2528FL%2529%2520presents%2520significant%2520challenges%252C%2520as%250Amodels%2520grow%2520and%2520evolve%2520with%2520complex%2520inheritance%2520relationships.%2520This%2520complexity%250Ais%2520amplified%2520when%2520blockchain%2520is%2520employed%2520to%2520ensure%2520the%2520integrity%2520and%250Atraceability%2520of%2520FL%252C%2520where%2520the%2520need%2520to%2520edit%2520multiple%2520interlinked%2520blockchain%250Arecords%2520and%2520update%2520all%2520inherited%2520models%2520complicates%2520the%2520process.In%2520this%2520paper%252C%250Awe%2520introduce%2520Blockchained%2520Federated%2520Unlearning%2520%2528BlockFUL%2529%252C%2520a%2520novel%2520framework%250Awith%2520a%2520dual-chain%2520structure%2520comprising%2520a%2520live%2520chain%2520and%2520an%2520archive%2520chain%2520for%250Aenabling%2520unlearning%2520capabilities%2520within%2520Blockchained%2520FL.%2520BlockFUL%2520introduces%250Atwo%2520new%2520unlearning%2520paradigms%252C%2520i.e.%252C%2520parallel%2520and%2520sequential%2520paradigms%252C%2520which%250Acan%2520be%2520effectively%2520implemented%2520through%2520gradient-ascent-based%2520and%250Are-training-based%2520unlearning%2520methods.%2520These%2520methods%2520enhance%2520the%2520unlearning%250Aprocess%2520across%2520multiple%2520inherited%2520models%2520by%2520enabling%2520efficient%2520consensus%250Aoperations%2520and%2520reducing%2520computational%2520costs.%2520Our%2520extensive%2520experiments%2520validate%250Athat%2520these%2520methods%2520effectively%2520reduce%2520data%2520dependency%2520and%2520operational%2520overhead%252C%250Athereby%2520boosting%2520the%2520overall%2520performance%2520of%2520unlearning%2520inherited%2520models%2520within%250ABlockFUL%2520on%2520CIFAR-10%2520and%2520Fashion-MNIST%2520datasets%2520using%2520AlexNet%252C%2520ResNet18%252C%2520and%250AMobileNetV2%2520models.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.16294v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=BlockFUL%3A%20Enabling%20Unlearning%20in%20Blockchained%20Federated%20Learning&entry.906535625=Xiao%20Liu%20and%20Mingyuan%20Li%20and%20Xu%20Wang%20and%20Guangsheng%20Yu%20and%20Wei%20Ni%20and%20Lixiang%20Li%20and%20Haipeng%20Peng%20and%20Renping%20Liu&entry.1292438233=%20%20Unlearning%20in%20Federated%20Learning%20%28FL%29%20presents%20significant%20challenges%2C%20as%0Amodels%20grow%20and%20evolve%20with%20complex%20inheritance%20relationships.%20This%20complexity%0Ais%20amplified%20when%20blockchain%20is%20employed%20to%20ensure%20the%20integrity%20and%0Atraceability%20of%20FL%2C%20where%20the%20need%20to%20edit%20multiple%20interlinked%20blockchain%0Arecords%20and%20update%20all%20inherited%20models%20complicates%20the%20process.In%20this%20paper%2C%0Awe%20introduce%20Blockchained%20Federated%20Unlearning%20%28BlockFUL%29%2C%20a%20novel%20framework%0Awith%20a%20dual-chain%20structure%20comprising%20a%20live%20chain%20and%20an%20archive%20chain%20for%0Aenabling%20unlearning%20capabilities%20within%20Blockchained%20FL.%20BlockFUL%20introduces%0Atwo%20new%20unlearning%20paradigms%2C%20i.e.%2C%20parallel%20and%20sequential%20paradigms%2C%20which%0Acan%20be%20effectively%20implemented%20through%20gradient-ascent-based%20and%0Are-training-based%20unlearning%20methods.%20These%20methods%20enhance%20the%20unlearning%0Aprocess%20across%20multiple%20inherited%20models%20by%20enabling%20efficient%20consensus%0Aoperations%20and%20reducing%20computational%20costs.%20Our%20extensive%20experiments%20validate%0Athat%20these%20methods%20effectively%20reduce%20data%20dependency%20and%20operational%20overhead%2C%0Athereby%20boosting%20the%20overall%20performance%20of%20unlearning%20inherited%20models%20within%0ABlockFUL%20on%20CIFAR-10%20and%20Fashion-MNIST%20datasets%20using%20AlexNet%2C%20ResNet18%2C%20and%0AMobileNetV2%20models.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.16294v2&entry.124074799=Read"},
{"title": "A Data-Driven Defense against Edge-case Model Poisoning Attacks on\n  Federated Learning", "author": "Kiran Purohit and Soumi Das and Sourangshu Bhattacharya and Santu Rana", "abstract": "  Federated Learning systems are increasingly subjected to a multitude of model\npoisoning attacks from clients. Among these, edge-case attacks that target a\nsmall fraction of the input space are nearly impossible to detect using\nexisting defenses, leading to a high attack success rate. We propose an\neffective defense using an external defense dataset, which provides information\nabout the attack target. The defense dataset contains a mix of poisoned and\nclean examples, with only a few known to be clean. The proposed method,\nDataDefense, uses this dataset to learn a poisoned data detector model which\nmarks each example in the defense dataset as poisoned or clean. It also learns\na client importance model that estimates the probability of a client update\nbeing malicious. The global model is then updated as a weighted average of the\nclient models' updates. The poisoned data detector and the client importance\nmodel parameters are updated using an alternating minimization strategy over\nthe Federated Learning rounds. Extensive experiments on standard attack\nscenarios demonstrate that DataDefense can defend against model poisoning\nattacks where other state-of-the-art defenses fail. In particular, DataDefense\nis able to reduce the attack success rate by at least ~ 40% on standard attack\nsetups and by more than 80% on some setups. Furthermore, DataDefense requires\nvery few defense examples (as few as five) to achieve a near-optimal reduction\nin attack success rate.\n", "link": "http://arxiv.org/abs/2305.02022v2", "date": "2024-08-14", "relevancy": 1.8248, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4667}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4558}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4524}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Data-Driven%20Defense%20against%20Edge-case%20Model%20Poisoning%20Attacks%20on%0A%20%20Federated%20Learning&body=Title%3A%20A%20Data-Driven%20Defense%20against%20Edge-case%20Model%20Poisoning%20Attacks%20on%0A%20%20Federated%20Learning%0AAuthor%3A%20Kiran%20Purohit%20and%20Soumi%20Das%20and%20Sourangshu%20Bhattacharya%20and%20Santu%20Rana%0AAbstract%3A%20%20%20Federated%20Learning%20systems%20are%20increasingly%20subjected%20to%20a%20multitude%20of%20model%0Apoisoning%20attacks%20from%20clients.%20Among%20these%2C%20edge-case%20attacks%20that%20target%20a%0Asmall%20fraction%20of%20the%20input%20space%20are%20nearly%20impossible%20to%20detect%20using%0Aexisting%20defenses%2C%20leading%20to%20a%20high%20attack%20success%20rate.%20We%20propose%20an%0Aeffective%20defense%20using%20an%20external%20defense%20dataset%2C%20which%20provides%20information%0Aabout%20the%20attack%20target.%20The%20defense%20dataset%20contains%20a%20mix%20of%20poisoned%20and%0Aclean%20examples%2C%20with%20only%20a%20few%20known%20to%20be%20clean.%20The%20proposed%20method%2C%0ADataDefense%2C%20uses%20this%20dataset%20to%20learn%20a%20poisoned%20data%20detector%20model%20which%0Amarks%20each%20example%20in%20the%20defense%20dataset%20as%20poisoned%20or%20clean.%20It%20also%20learns%0Aa%20client%20importance%20model%20that%20estimates%20the%20probability%20of%20a%20client%20update%0Abeing%20malicious.%20The%20global%20model%20is%20then%20updated%20as%20a%20weighted%20average%20of%20the%0Aclient%20models%27%20updates.%20The%20poisoned%20data%20detector%20and%20the%20client%20importance%0Amodel%20parameters%20are%20updated%20using%20an%20alternating%20minimization%20strategy%20over%0Athe%20Federated%20Learning%20rounds.%20Extensive%20experiments%20on%20standard%20attack%0Ascenarios%20demonstrate%20that%20DataDefense%20can%20defend%20against%20model%20poisoning%0Aattacks%20where%20other%20state-of-the-art%20defenses%20fail.%20In%20particular%2C%20DataDefense%0Ais%20able%20to%20reduce%20the%20attack%20success%20rate%20by%20at%20least%20~%2040%25%20on%20standard%20attack%0Asetups%20and%20by%20more%20than%2080%25%20on%20some%20setups.%20Furthermore%2C%20DataDefense%20requires%0Avery%20few%20defense%20examples%20%28as%20few%20as%20five%29%20to%20achieve%20a%20near-optimal%20reduction%0Ain%20attack%20success%20rate.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2305.02022v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Data-Driven%2520Defense%2520against%2520Edge-case%2520Model%2520Poisoning%2520Attacks%2520on%250A%2520%2520Federated%2520Learning%26entry.906535625%3DKiran%2520Purohit%2520and%2520Soumi%2520Das%2520and%2520Sourangshu%2520Bhattacharya%2520and%2520Santu%2520Rana%26entry.1292438233%3D%2520%2520Federated%2520Learning%2520systems%2520are%2520increasingly%2520subjected%2520to%2520a%2520multitude%2520of%2520model%250Apoisoning%2520attacks%2520from%2520clients.%2520Among%2520these%252C%2520edge-case%2520attacks%2520that%2520target%2520a%250Asmall%2520fraction%2520of%2520the%2520input%2520space%2520are%2520nearly%2520impossible%2520to%2520detect%2520using%250Aexisting%2520defenses%252C%2520leading%2520to%2520a%2520high%2520attack%2520success%2520rate.%2520We%2520propose%2520an%250Aeffective%2520defense%2520using%2520an%2520external%2520defense%2520dataset%252C%2520which%2520provides%2520information%250Aabout%2520the%2520attack%2520target.%2520The%2520defense%2520dataset%2520contains%2520a%2520mix%2520of%2520poisoned%2520and%250Aclean%2520examples%252C%2520with%2520only%2520a%2520few%2520known%2520to%2520be%2520clean.%2520The%2520proposed%2520method%252C%250ADataDefense%252C%2520uses%2520this%2520dataset%2520to%2520learn%2520a%2520poisoned%2520data%2520detector%2520model%2520which%250Amarks%2520each%2520example%2520in%2520the%2520defense%2520dataset%2520as%2520poisoned%2520or%2520clean.%2520It%2520also%2520learns%250Aa%2520client%2520importance%2520model%2520that%2520estimates%2520the%2520probability%2520of%2520a%2520client%2520update%250Abeing%2520malicious.%2520The%2520global%2520model%2520is%2520then%2520updated%2520as%2520a%2520weighted%2520average%2520of%2520the%250Aclient%2520models%2527%2520updates.%2520The%2520poisoned%2520data%2520detector%2520and%2520the%2520client%2520importance%250Amodel%2520parameters%2520are%2520updated%2520using%2520an%2520alternating%2520minimization%2520strategy%2520over%250Athe%2520Federated%2520Learning%2520rounds.%2520Extensive%2520experiments%2520on%2520standard%2520attack%250Ascenarios%2520demonstrate%2520that%2520DataDefense%2520can%2520defend%2520against%2520model%2520poisoning%250Aattacks%2520where%2520other%2520state-of-the-art%2520defenses%2520fail.%2520In%2520particular%252C%2520DataDefense%250Ais%2520able%2520to%2520reduce%2520the%2520attack%2520success%2520rate%2520by%2520at%2520least%2520~%252040%2525%2520on%2520standard%2520attack%250Asetups%2520and%2520by%2520more%2520than%252080%2525%2520on%2520some%2520setups.%2520Furthermore%252C%2520DataDefense%2520requires%250Avery%2520few%2520defense%2520examples%2520%2528as%2520few%2520as%2520five%2529%2520to%2520achieve%2520a%2520near-optimal%2520reduction%250Ain%2520attack%2520success%2520rate.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2305.02022v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Data-Driven%20Defense%20against%20Edge-case%20Model%20Poisoning%20Attacks%20on%0A%20%20Federated%20Learning&entry.906535625=Kiran%20Purohit%20and%20Soumi%20Das%20and%20Sourangshu%20Bhattacharya%20and%20Santu%20Rana&entry.1292438233=%20%20Federated%20Learning%20systems%20are%20increasingly%20subjected%20to%20a%20multitude%20of%20model%0Apoisoning%20attacks%20from%20clients.%20Among%20these%2C%20edge-case%20attacks%20that%20target%20a%0Asmall%20fraction%20of%20the%20input%20space%20are%20nearly%20impossible%20to%20detect%20using%0Aexisting%20defenses%2C%20leading%20to%20a%20high%20attack%20success%20rate.%20We%20propose%20an%0Aeffective%20defense%20using%20an%20external%20defense%20dataset%2C%20which%20provides%20information%0Aabout%20the%20attack%20target.%20The%20defense%20dataset%20contains%20a%20mix%20of%20poisoned%20and%0Aclean%20examples%2C%20with%20only%20a%20few%20known%20to%20be%20clean.%20The%20proposed%20method%2C%0ADataDefense%2C%20uses%20this%20dataset%20to%20learn%20a%20poisoned%20data%20detector%20model%20which%0Amarks%20each%20example%20in%20the%20defense%20dataset%20as%20poisoned%20or%20clean.%20It%20also%20learns%0Aa%20client%20importance%20model%20that%20estimates%20the%20probability%20of%20a%20client%20update%0Abeing%20malicious.%20The%20global%20model%20is%20then%20updated%20as%20a%20weighted%20average%20of%20the%0Aclient%20models%27%20updates.%20The%20poisoned%20data%20detector%20and%20the%20client%20importance%0Amodel%20parameters%20are%20updated%20using%20an%20alternating%20minimization%20strategy%20over%0Athe%20Federated%20Learning%20rounds.%20Extensive%20experiments%20on%20standard%20attack%0Ascenarios%20demonstrate%20that%20DataDefense%20can%20defend%20against%20model%20poisoning%0Aattacks%20where%20other%20state-of-the-art%20defenses%20fail.%20In%20particular%2C%20DataDefense%0Ais%20able%20to%20reduce%20the%20attack%20success%20rate%20by%20at%20least%20~%2040%25%20on%20standard%20attack%0Asetups%20and%20by%20more%20than%2080%25%20on%20some%20setups.%20Furthermore%2C%20DataDefense%20requires%0Avery%20few%20defense%20examples%20%28as%20few%20as%20five%29%20to%20achieve%20a%20near-optimal%20reduction%0Ain%20attack%20success%20rate.%0A&entry.1838667208=http%3A//arxiv.org/abs/2305.02022v2&entry.124074799=Read"},
{"title": "Optimal Baseline Corrections for Off-Policy Contextual Bandits", "author": "Shashank Gupta and Olivier Jeunen and Harrie Oosterhuis and Maarten de Rijke", "abstract": "  The off-policy learning paradigm allows for recommender systems and general\nranking applications to be framed as decision-making problems, where we aim to\nlearn decision policies that optimize an unbiased offline estimate of an online\nreward metric. With unbiasedness comes potentially high variance, and prevalent\nmethods exist to reduce estimation variance. These methods typically make use\nof control variates, either additive (i.e., baseline corrections or doubly\nrobust methods) or multiplicative (i.e., self-normalisation). Our work unifies\nthese approaches by proposing a single framework built on their equivalence in\nlearning scenarios. The foundation of our framework is the derivation of an\nequivalent baseline correction for all of the existing control variates.\nConsequently, our framework enables us to characterize the variance-optimal\nunbiased estimator and provide a closed-form solution for it. This optimal\nestimator brings significantly improved performance in both evaluation and\nlearning, and minimizes data requirements. Empirical observations corroborate\nour theoretical findings.\n", "link": "http://arxiv.org/abs/2405.05736v2", "date": "2024-08-14", "relevancy": 1.8247, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4926}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4569}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4409}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Optimal%20Baseline%20Corrections%20for%20Off-Policy%20Contextual%20Bandits&body=Title%3A%20Optimal%20Baseline%20Corrections%20for%20Off-Policy%20Contextual%20Bandits%0AAuthor%3A%20Shashank%20Gupta%20and%20Olivier%20Jeunen%20and%20Harrie%20Oosterhuis%20and%20Maarten%20de%20Rijke%0AAbstract%3A%20%20%20The%20off-policy%20learning%20paradigm%20allows%20for%20recommender%20systems%20and%20general%0Aranking%20applications%20to%20be%20framed%20as%20decision-making%20problems%2C%20where%20we%20aim%20to%0Alearn%20decision%20policies%20that%20optimize%20an%20unbiased%20offline%20estimate%20of%20an%20online%0Areward%20metric.%20With%20unbiasedness%20comes%20potentially%20high%20variance%2C%20and%20prevalent%0Amethods%20exist%20to%20reduce%20estimation%20variance.%20These%20methods%20typically%20make%20use%0Aof%20control%20variates%2C%20either%20additive%20%28i.e.%2C%20baseline%20corrections%20or%20doubly%0Arobust%20methods%29%20or%20multiplicative%20%28i.e.%2C%20self-normalisation%29.%20Our%20work%20unifies%0Athese%20approaches%20by%20proposing%20a%20single%20framework%20built%20on%20their%20equivalence%20in%0Alearning%20scenarios.%20The%20foundation%20of%20our%20framework%20is%20the%20derivation%20of%20an%0Aequivalent%20baseline%20correction%20for%20all%20of%20the%20existing%20control%20variates.%0AConsequently%2C%20our%20framework%20enables%20us%20to%20characterize%20the%20variance-optimal%0Aunbiased%20estimator%20and%20provide%20a%20closed-form%20solution%20for%20it.%20This%20optimal%0Aestimator%20brings%20significantly%20improved%20performance%20in%20both%20evaluation%20and%0Alearning%2C%20and%20minimizes%20data%20requirements.%20Empirical%20observations%20corroborate%0Aour%20theoretical%20findings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2405.05736v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DOptimal%2520Baseline%2520Corrections%2520for%2520Off-Policy%2520Contextual%2520Bandits%26entry.906535625%3DShashank%2520Gupta%2520and%2520Olivier%2520Jeunen%2520and%2520Harrie%2520Oosterhuis%2520and%2520Maarten%2520de%2520Rijke%26entry.1292438233%3D%2520%2520The%2520off-policy%2520learning%2520paradigm%2520allows%2520for%2520recommender%2520systems%2520and%2520general%250Aranking%2520applications%2520to%2520be%2520framed%2520as%2520decision-making%2520problems%252C%2520where%2520we%2520aim%2520to%250Alearn%2520decision%2520policies%2520that%2520optimize%2520an%2520unbiased%2520offline%2520estimate%2520of%2520an%2520online%250Areward%2520metric.%2520With%2520unbiasedness%2520comes%2520potentially%2520high%2520variance%252C%2520and%2520prevalent%250Amethods%2520exist%2520to%2520reduce%2520estimation%2520variance.%2520These%2520methods%2520typically%2520make%2520use%250Aof%2520control%2520variates%252C%2520either%2520additive%2520%2528i.e.%252C%2520baseline%2520corrections%2520or%2520doubly%250Arobust%2520methods%2529%2520or%2520multiplicative%2520%2528i.e.%252C%2520self-normalisation%2529.%2520Our%2520work%2520unifies%250Athese%2520approaches%2520by%2520proposing%2520a%2520single%2520framework%2520built%2520on%2520their%2520equivalence%2520in%250Alearning%2520scenarios.%2520The%2520foundation%2520of%2520our%2520framework%2520is%2520the%2520derivation%2520of%2520an%250Aequivalent%2520baseline%2520correction%2520for%2520all%2520of%2520the%2520existing%2520control%2520variates.%250AConsequently%252C%2520our%2520framework%2520enables%2520us%2520to%2520characterize%2520the%2520variance-optimal%250Aunbiased%2520estimator%2520and%2520provide%2520a%2520closed-form%2520solution%2520for%2520it.%2520This%2520optimal%250Aestimator%2520brings%2520significantly%2520improved%2520performance%2520in%2520both%2520evaluation%2520and%250Alearning%252C%2520and%2520minimizes%2520data%2520requirements.%2520Empirical%2520observations%2520corroborate%250Aour%2520theoretical%2520findings.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2405.05736v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Optimal%20Baseline%20Corrections%20for%20Off-Policy%20Contextual%20Bandits&entry.906535625=Shashank%20Gupta%20and%20Olivier%20Jeunen%20and%20Harrie%20Oosterhuis%20and%20Maarten%20de%20Rijke&entry.1292438233=%20%20The%20off-policy%20learning%20paradigm%20allows%20for%20recommender%20systems%20and%20general%0Aranking%20applications%20to%20be%20framed%20as%20decision-making%20problems%2C%20where%20we%20aim%20to%0Alearn%20decision%20policies%20that%20optimize%20an%20unbiased%20offline%20estimate%20of%20an%20online%0Areward%20metric.%20With%20unbiasedness%20comes%20potentially%20high%20variance%2C%20and%20prevalent%0Amethods%20exist%20to%20reduce%20estimation%20variance.%20These%20methods%20typically%20make%20use%0Aof%20control%20variates%2C%20either%20additive%20%28i.e.%2C%20baseline%20corrections%20or%20doubly%0Arobust%20methods%29%20or%20multiplicative%20%28i.e.%2C%20self-normalisation%29.%20Our%20work%20unifies%0Athese%20approaches%20by%20proposing%20a%20single%20framework%20built%20on%20their%20equivalence%20in%0Alearning%20scenarios.%20The%20foundation%20of%20our%20framework%20is%20the%20derivation%20of%20an%0Aequivalent%20baseline%20correction%20for%20all%20of%20the%20existing%20control%20variates.%0AConsequently%2C%20our%20framework%20enables%20us%20to%20characterize%20the%20variance-optimal%0Aunbiased%20estimator%20and%20provide%20a%20closed-form%20solution%20for%20it.%20This%20optimal%0Aestimator%20brings%20significantly%20improved%20performance%20in%20both%20evaluation%20and%0Alearning%2C%20and%20minimizes%20data%20requirements.%20Empirical%20observations%20corroborate%0Aour%20theoretical%20findings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2405.05736v2&entry.124074799=Read"},
{"title": "Massive Activations in Large Language Models", "author": "Mingjie Sun and Xinlei Chen and J. Zico Kolter and Zhuang Liu", "abstract": "  We observe an empirical phenomenon in Large Language Models (LLMs) -- very\nfew activations exhibit significantly larger values than others (e.g., 100,000\ntimes larger). We call them massive activations. First, we demonstrate the\nwidespread existence of massive activations across various LLMs and\ncharacterize their locations. Second, we find their values largely stay\nconstant regardless of the input, and they function as indispensable bias terms\nin LLMs. Third, these massive activations lead to the concentration of\nattention probabilities to their corresponding tokens, and further, implicit\nbias terms in the self-attention output. Last, we also study massive\nactivations in Vision Transformers. Code is available at\nhttps://github.com/locuslab/massive-activations.\n", "link": "http://arxiv.org/abs/2402.17762v2", "date": "2024-08-14", "relevancy": 1.8151, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.474}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4503}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4492}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Massive%20Activations%20in%20Large%20Language%20Models&body=Title%3A%20Massive%20Activations%20in%20Large%20Language%20Models%0AAuthor%3A%20Mingjie%20Sun%20and%20Xinlei%20Chen%20and%20J.%20Zico%20Kolter%20and%20Zhuang%20Liu%0AAbstract%3A%20%20%20We%20observe%20an%20empirical%20phenomenon%20in%20Large%20Language%20Models%20%28LLMs%29%20--%20very%0Afew%20activations%20exhibit%20significantly%20larger%20values%20than%20others%20%28e.g.%2C%20100%2C000%0Atimes%20larger%29.%20We%20call%20them%20massive%20activations.%20First%2C%20we%20demonstrate%20the%0Awidespread%20existence%20of%20massive%20activations%20across%20various%20LLMs%20and%0Acharacterize%20their%20locations.%20Second%2C%20we%20find%20their%20values%20largely%20stay%0Aconstant%20regardless%20of%20the%20input%2C%20and%20they%20function%20as%20indispensable%20bias%20terms%0Ain%20LLMs.%20Third%2C%20these%20massive%20activations%20lead%20to%20the%20concentration%20of%0Aattention%20probabilities%20to%20their%20corresponding%20tokens%2C%20and%20further%2C%20implicit%0Abias%20terms%20in%20the%20self-attention%20output.%20Last%2C%20we%20also%20study%20massive%0Aactivations%20in%20Vision%20Transformers.%20Code%20is%20available%20at%0Ahttps%3A//github.com/locuslab/massive-activations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.17762v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMassive%2520Activations%2520in%2520Large%2520Language%2520Models%26entry.906535625%3DMingjie%2520Sun%2520and%2520Xinlei%2520Chen%2520and%2520J.%2520Zico%2520Kolter%2520and%2520Zhuang%2520Liu%26entry.1292438233%3D%2520%2520We%2520observe%2520an%2520empirical%2520phenomenon%2520in%2520Large%2520Language%2520Models%2520%2528LLMs%2529%2520--%2520very%250Afew%2520activations%2520exhibit%2520significantly%2520larger%2520values%2520than%2520others%2520%2528e.g.%252C%2520100%252C000%250Atimes%2520larger%2529.%2520We%2520call%2520them%2520massive%2520activations.%2520First%252C%2520we%2520demonstrate%2520the%250Awidespread%2520existence%2520of%2520massive%2520activations%2520across%2520various%2520LLMs%2520and%250Acharacterize%2520their%2520locations.%2520Second%252C%2520we%2520find%2520their%2520values%2520largely%2520stay%250Aconstant%2520regardless%2520of%2520the%2520input%252C%2520and%2520they%2520function%2520as%2520indispensable%2520bias%2520terms%250Ain%2520LLMs.%2520Third%252C%2520these%2520massive%2520activations%2520lead%2520to%2520the%2520concentration%2520of%250Aattention%2520probabilities%2520to%2520their%2520corresponding%2520tokens%252C%2520and%2520further%252C%2520implicit%250Abias%2520terms%2520in%2520the%2520self-attention%2520output.%2520Last%252C%2520we%2520also%2520study%2520massive%250Aactivations%2520in%2520Vision%2520Transformers.%2520Code%2520is%2520available%2520at%250Ahttps%253A//github.com/locuslab/massive-activations.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2402.17762v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Massive%20Activations%20in%20Large%20Language%20Models&entry.906535625=Mingjie%20Sun%20and%20Xinlei%20Chen%20and%20J.%20Zico%20Kolter%20and%20Zhuang%20Liu&entry.1292438233=%20%20We%20observe%20an%20empirical%20phenomenon%20in%20Large%20Language%20Models%20%28LLMs%29%20--%20very%0Afew%20activations%20exhibit%20significantly%20larger%20values%20than%20others%20%28e.g.%2C%20100%2C000%0Atimes%20larger%29.%20We%20call%20them%20massive%20activations.%20First%2C%20we%20demonstrate%20the%0Awidespread%20existence%20of%20massive%20activations%20across%20various%20LLMs%20and%0Acharacterize%20their%20locations.%20Second%2C%20we%20find%20their%20values%20largely%20stay%0Aconstant%20regardless%20of%20the%20input%2C%20and%20they%20function%20as%20indispensable%20bias%20terms%0Ain%20LLMs.%20Third%2C%20these%20massive%20activations%20lead%20to%20the%20concentration%20of%0Aattention%20probabilities%20to%20their%20corresponding%20tokens%2C%20and%20further%2C%20implicit%0Abias%20terms%20in%20the%20self-attention%20output.%20Last%2C%20we%20also%20study%20massive%0Aactivations%20in%20Vision%20Transformers.%20Code%20is%20available%20at%0Ahttps%3A//github.com/locuslab/massive-activations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.17762v2&entry.124074799=Read"},
{"title": "Volley Revolver: A Novel Matrix-Encoding Method for Privacy-Preserving\n  Neural Networks (Inference)", "author": "John Chiang", "abstract": "  In this work, we present a novel matrix-encoding method that is particularly\nconvenient for neural networks to make predictions in a privacy-preserving\nmanner using homomorphic encryption. Based on this encoding method, we\nimplement a convolutional neural network for handwritten image classification\nover encryption. For two matrices $A$ and $B$ to perform homomorphic\nmultiplication, the main idea behind it, in a simple version, is to encrypt\nmatrix $A$ and the transpose of matrix $B$ into two ciphertexts respectively.\nWith additional operations, the homomorphic matrix multiplication can be\ncalculated over encrypted matrices efficiently. For the convolution operation,\nwe in advance span each convolution kernel to a matrix space of the same size\nas the input image so as to generate several ciphertexts, each of which is\nlater used together with the ciphertext encrypting input images for calculating\nsome of the final convolution results. We accumulate all these intermediate\nresults and thus complete the convolution operation.\n  In a public cloud with 40 vCPUs, our convolutional neural network\nimplementation on the MNIST testing dataset takes $\\sim$ 287 seconds to compute\nten likelihoods of 32 encrypted images of size $28 \\times 28$ simultaneously.\nThe data owner only needs to upload one ciphertext ($\\sim 19.8$ MB) encrypting\nthese 32 images to the public cloud.\n", "link": "http://arxiv.org/abs/2201.12577v5", "date": "2024-08-14", "relevancy": 1.8045, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4559}, {"title": "Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Monocular\n  Videos", "link": "http://arxiv.org/abs/2404.12379v2", "similarity": 0.4482}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4464}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Volley%20Revolver%3A%20A%20Novel%20Matrix-Encoding%20Method%20for%20Privacy-Preserving%0A%20%20Neural%20Networks%20%28Inference%29&body=Title%3A%20Volley%20Revolver%3A%20A%20Novel%20Matrix-Encoding%20Method%20for%20Privacy-Preserving%0A%20%20Neural%20Networks%20%28Inference%29%0AAuthor%3A%20John%20Chiang%0AAbstract%3A%20%20%20In%20this%20work%2C%20we%20present%20a%20novel%20matrix-encoding%20method%20that%20is%20particularly%0Aconvenient%20for%20neural%20networks%20to%20make%20predictions%20in%20a%20privacy-preserving%0Amanner%20using%20homomorphic%20encryption.%20Based%20on%20this%20encoding%20method%2C%20we%0Aimplement%20a%20convolutional%20neural%20network%20for%20handwritten%20image%20classification%0Aover%20encryption.%20For%20two%20matrices%20%24A%24%20and%20%24B%24%20to%20perform%20homomorphic%0Amultiplication%2C%20the%20main%20idea%20behind%20it%2C%20in%20a%20simple%20version%2C%20is%20to%20encrypt%0Amatrix%20%24A%24%20and%20the%20transpose%20of%20matrix%20%24B%24%20into%20two%20ciphertexts%20respectively.%0AWith%20additional%20operations%2C%20the%20homomorphic%20matrix%20multiplication%20can%20be%0Acalculated%20over%20encrypted%20matrices%20efficiently.%20For%20the%20convolution%20operation%2C%0Awe%20in%20advance%20span%20each%20convolution%20kernel%20to%20a%20matrix%20space%20of%20the%20same%20size%0Aas%20the%20input%20image%20so%20as%20to%20generate%20several%20ciphertexts%2C%20each%20of%20which%20is%0Alater%20used%20together%20with%20the%20ciphertext%20encrypting%20input%20images%20for%20calculating%0Asome%20of%20the%20final%20convolution%20results.%20We%20accumulate%20all%20these%20intermediate%0Aresults%20and%20thus%20complete%20the%20convolution%20operation.%0A%20%20In%20a%20public%20cloud%20with%2040%20vCPUs%2C%20our%20convolutional%20neural%20network%0Aimplementation%20on%20the%20MNIST%20testing%20dataset%20takes%20%24%5Csim%24%20287%20seconds%20to%20compute%0Aten%20likelihoods%20of%2032%20encrypted%20images%20of%20size%20%2428%20%5Ctimes%2028%24%20simultaneously.%0AThe%20data%20owner%20only%20needs%20to%20upload%20one%20ciphertext%20%28%24%5Csim%2019.8%24%20MB%29%20encrypting%0Athese%2032%20images%20to%20the%20public%20cloud.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2201.12577v5%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DVolley%2520Revolver%253A%2520A%2520Novel%2520Matrix-Encoding%2520Method%2520for%2520Privacy-Preserving%250A%2520%2520Neural%2520Networks%2520%2528Inference%2529%26entry.906535625%3DJohn%2520Chiang%26entry.1292438233%3D%2520%2520In%2520this%2520work%252C%2520we%2520present%2520a%2520novel%2520matrix-encoding%2520method%2520that%2520is%2520particularly%250Aconvenient%2520for%2520neural%2520networks%2520to%2520make%2520predictions%2520in%2520a%2520privacy-preserving%250Amanner%2520using%2520homomorphic%2520encryption.%2520Based%2520on%2520this%2520encoding%2520method%252C%2520we%250Aimplement%2520a%2520convolutional%2520neural%2520network%2520for%2520handwritten%2520image%2520classification%250Aover%2520encryption.%2520For%2520two%2520matrices%2520%2524A%2524%2520and%2520%2524B%2524%2520to%2520perform%2520homomorphic%250Amultiplication%252C%2520the%2520main%2520idea%2520behind%2520it%252C%2520in%2520a%2520simple%2520version%252C%2520is%2520to%2520encrypt%250Amatrix%2520%2524A%2524%2520and%2520the%2520transpose%2520of%2520matrix%2520%2524B%2524%2520into%2520two%2520ciphertexts%2520respectively.%250AWith%2520additional%2520operations%252C%2520the%2520homomorphic%2520matrix%2520multiplication%2520can%2520be%250Acalculated%2520over%2520encrypted%2520matrices%2520efficiently.%2520For%2520the%2520convolution%2520operation%252C%250Awe%2520in%2520advance%2520span%2520each%2520convolution%2520kernel%2520to%2520a%2520matrix%2520space%2520of%2520the%2520same%2520size%250Aas%2520the%2520input%2520image%2520so%2520as%2520to%2520generate%2520several%2520ciphertexts%252C%2520each%2520of%2520which%2520is%250Alater%2520used%2520together%2520with%2520the%2520ciphertext%2520encrypting%2520input%2520images%2520for%2520calculating%250Asome%2520of%2520the%2520final%2520convolution%2520results.%2520We%2520accumulate%2520all%2520these%2520intermediate%250Aresults%2520and%2520thus%2520complete%2520the%2520convolution%2520operation.%250A%2520%2520In%2520a%2520public%2520cloud%2520with%252040%2520vCPUs%252C%2520our%2520convolutional%2520neural%2520network%250Aimplementation%2520on%2520the%2520MNIST%2520testing%2520dataset%2520takes%2520%2524%255Csim%2524%2520287%2520seconds%2520to%2520compute%250Aten%2520likelihoods%2520of%252032%2520encrypted%2520images%2520of%2520size%2520%252428%2520%255Ctimes%252028%2524%2520simultaneously.%250AThe%2520data%2520owner%2520only%2520needs%2520to%2520upload%2520one%2520ciphertext%2520%2528%2524%255Csim%252019.8%2524%2520MB%2529%2520encrypting%250Athese%252032%2520images%2520to%2520the%2520public%2520cloud.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2201.12577v5%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Volley%20Revolver%3A%20A%20Novel%20Matrix-Encoding%20Method%20for%20Privacy-Preserving%0A%20%20Neural%20Networks%20%28Inference%29&entry.906535625=John%20Chiang&entry.1292438233=%20%20In%20this%20work%2C%20we%20present%20a%20novel%20matrix-encoding%20method%20that%20is%20particularly%0Aconvenient%20for%20neural%20networks%20to%20make%20predictions%20in%20a%20privacy-preserving%0Amanner%20using%20homomorphic%20encryption.%20Based%20on%20this%20encoding%20method%2C%20we%0Aimplement%20a%20convolutional%20neural%20network%20for%20handwritten%20image%20classification%0Aover%20encryption.%20For%20two%20matrices%20%24A%24%20and%20%24B%24%20to%20perform%20homomorphic%0Amultiplication%2C%20the%20main%20idea%20behind%20it%2C%20in%20a%20simple%20version%2C%20is%20to%20encrypt%0Amatrix%20%24A%24%20and%20the%20transpose%20of%20matrix%20%24B%24%20into%20two%20ciphertexts%20respectively.%0AWith%20additional%20operations%2C%20the%20homomorphic%20matrix%20multiplication%20can%20be%0Acalculated%20over%20encrypted%20matrices%20efficiently.%20For%20the%20convolution%20operation%2C%0Awe%20in%20advance%20span%20each%20convolution%20kernel%20to%20a%20matrix%20space%20of%20the%20same%20size%0Aas%20the%20input%20image%20so%20as%20to%20generate%20several%20ciphertexts%2C%20each%20of%20which%20is%0Alater%20used%20together%20with%20the%20ciphertext%20encrypting%20input%20images%20for%20calculating%0Asome%20of%20the%20final%20convolution%20results.%20We%20accumulate%20all%20these%20intermediate%0Aresults%20and%20thus%20complete%20the%20convolution%20operation.%0A%20%20In%20a%20public%20cloud%20with%2040%20vCPUs%2C%20our%20convolutional%20neural%20network%0Aimplementation%20on%20the%20MNIST%20testing%20dataset%20takes%20%24%5Csim%24%20287%20seconds%20to%20compute%0Aten%20likelihoods%20of%2032%20encrypted%20images%20of%20size%20%2428%20%5Ctimes%2028%24%20simultaneously.%0AThe%20data%20owner%20only%20needs%20to%20upload%20one%20ciphertext%20%28%24%5Csim%2019.8%24%20MB%29%20encrypting%0Athese%2032%20images%20to%20the%20public%20cloud.%0A&entry.1838667208=http%3A//arxiv.org/abs/2201.12577v5&entry.124074799=Read"},
{"title": "Ramsey Theorems for Trees and a General 'Private Learning Implies Online\n  Learning' Theorem", "author": "Simone Fioravanti and Steve Hanneke and Shay Moran and Hilla Schefler and Iska Tsubari", "abstract": "  This work continues to investigate the link between differentially private\n(DP) and online learning. Alon, Livni, Malliaris, and Moran (2019) showed that\nfor binary concept classes, DP learnability of a given class implies that it\nhas a finite Littlestone dimension (equivalently, that it is online learnable).\nTheir proof relies on a model-theoretic result by Hodges (1997), which\ndemonstrates that any binary concept class with a large Littlestone dimension\ncontains a large subclass of thresholds. In a follow-up work, Jung, Kim, and\nTewari (2020) extended this proof to multiclass PAC learning with a bounded\nnumber of labels. Unfortunately, Hodges's result does not apply in other\nnatural settings such as multiclass PAC learning with an unbounded label space,\nand PAC learning of partial concept classes.\n  This naturally raises the question of whether DP learnability continues to\nimply online learnability in more general scenarios: indeed, Alon, Hanneke,\nHolzman, and Moran (2021) explicitly leave it as an open question in the\ncontext of partial concept classes, and the same question is open in the\ngeneral multiclass setting. In this work, we give a positive answer to these\nquestions showing that for general classification tasks, DP learnability\nimplies online learnability. Our proof reasons directly about Littlestone\ntrees, without relying on thresholds. We achieve this by establishing several\nRamsey-type theorems for trees, which might be of independent interest.\n", "link": "http://arxiv.org/abs/2407.07765v2", "date": "2024-08-14", "relevancy": 1.7751, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4805}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.447}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4259}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Ramsey%20Theorems%20for%20Trees%20and%20a%20General%20%27Private%20Learning%20Implies%20Online%0A%20%20Learning%27%20Theorem&body=Title%3A%20Ramsey%20Theorems%20for%20Trees%20and%20a%20General%20%27Private%20Learning%20Implies%20Online%0A%20%20Learning%27%20Theorem%0AAuthor%3A%20Simone%20Fioravanti%20and%20Steve%20Hanneke%20and%20Shay%20Moran%20and%20Hilla%20Schefler%20and%20Iska%20Tsubari%0AAbstract%3A%20%20%20This%20work%20continues%20to%20investigate%20the%20link%20between%20differentially%20private%0A%28DP%29%20and%20online%20learning.%20Alon%2C%20Livni%2C%20Malliaris%2C%20and%20Moran%20%282019%29%20showed%20that%0Afor%20binary%20concept%20classes%2C%20DP%20learnability%20of%20a%20given%20class%20implies%20that%20it%0Ahas%20a%20finite%20Littlestone%20dimension%20%28equivalently%2C%20that%20it%20is%20online%20learnable%29.%0ATheir%20proof%20relies%20on%20a%20model-theoretic%20result%20by%20Hodges%20%281997%29%2C%20which%0Ademonstrates%20that%20any%20binary%20concept%20class%20with%20a%20large%20Littlestone%20dimension%0Acontains%20a%20large%20subclass%20of%20thresholds.%20In%20a%20follow-up%20work%2C%20Jung%2C%20Kim%2C%20and%0ATewari%20%282020%29%20extended%20this%20proof%20to%20multiclass%20PAC%20learning%20with%20a%20bounded%0Anumber%20of%20labels.%20Unfortunately%2C%20Hodges%27s%20result%20does%20not%20apply%20in%20other%0Anatural%20settings%20such%20as%20multiclass%20PAC%20learning%20with%20an%20unbounded%20label%20space%2C%0Aand%20PAC%20learning%20of%20partial%20concept%20classes.%0A%20%20This%20naturally%20raises%20the%20question%20of%20whether%20DP%20learnability%20continues%20to%0Aimply%20online%20learnability%20in%20more%20general%20scenarios%3A%20indeed%2C%20Alon%2C%20Hanneke%2C%0AHolzman%2C%20and%20Moran%20%282021%29%20explicitly%20leave%20it%20as%20an%20open%20question%20in%20the%0Acontext%20of%20partial%20concept%20classes%2C%20and%20the%20same%20question%20is%20open%20in%20the%0Ageneral%20multiclass%20setting.%20In%20this%20work%2C%20we%20give%20a%20positive%20answer%20to%20these%0Aquestions%20showing%20that%20for%20general%20classification%20tasks%2C%20DP%20learnability%0Aimplies%20online%20learnability.%20Our%20proof%20reasons%20directly%20about%20Littlestone%0Atrees%2C%20without%20relying%20on%20thresholds.%20We%20achieve%20this%20by%20establishing%20several%0ARamsey-type%20theorems%20for%20trees%2C%20which%20might%20be%20of%20independent%20interest.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.07765v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRamsey%2520Theorems%2520for%2520Trees%2520and%2520a%2520General%2520%2527Private%2520Learning%2520Implies%2520Online%250A%2520%2520Learning%2527%2520Theorem%26entry.906535625%3DSimone%2520Fioravanti%2520and%2520Steve%2520Hanneke%2520and%2520Shay%2520Moran%2520and%2520Hilla%2520Schefler%2520and%2520Iska%2520Tsubari%26entry.1292438233%3D%2520%2520This%2520work%2520continues%2520to%2520investigate%2520the%2520link%2520between%2520differentially%2520private%250A%2528DP%2529%2520and%2520online%2520learning.%2520Alon%252C%2520Livni%252C%2520Malliaris%252C%2520and%2520Moran%2520%25282019%2529%2520showed%2520that%250Afor%2520binary%2520concept%2520classes%252C%2520DP%2520learnability%2520of%2520a%2520given%2520class%2520implies%2520that%2520it%250Ahas%2520a%2520finite%2520Littlestone%2520dimension%2520%2528equivalently%252C%2520that%2520it%2520is%2520online%2520learnable%2529.%250ATheir%2520proof%2520relies%2520on%2520a%2520model-theoretic%2520result%2520by%2520Hodges%2520%25281997%2529%252C%2520which%250Ademonstrates%2520that%2520any%2520binary%2520concept%2520class%2520with%2520a%2520large%2520Littlestone%2520dimension%250Acontains%2520a%2520large%2520subclass%2520of%2520thresholds.%2520In%2520a%2520follow-up%2520work%252C%2520Jung%252C%2520Kim%252C%2520and%250ATewari%2520%25282020%2529%2520extended%2520this%2520proof%2520to%2520multiclass%2520PAC%2520learning%2520with%2520a%2520bounded%250Anumber%2520of%2520labels.%2520Unfortunately%252C%2520Hodges%2527s%2520result%2520does%2520not%2520apply%2520in%2520other%250Anatural%2520settings%2520such%2520as%2520multiclass%2520PAC%2520learning%2520with%2520an%2520unbounded%2520label%2520space%252C%250Aand%2520PAC%2520learning%2520of%2520partial%2520concept%2520classes.%250A%2520%2520This%2520naturally%2520raises%2520the%2520question%2520of%2520whether%2520DP%2520learnability%2520continues%2520to%250Aimply%2520online%2520learnability%2520in%2520more%2520general%2520scenarios%253A%2520indeed%252C%2520Alon%252C%2520Hanneke%252C%250AHolzman%252C%2520and%2520Moran%2520%25282021%2529%2520explicitly%2520leave%2520it%2520as%2520an%2520open%2520question%2520in%2520the%250Acontext%2520of%2520partial%2520concept%2520classes%252C%2520and%2520the%2520same%2520question%2520is%2520open%2520in%2520the%250Ageneral%2520multiclass%2520setting.%2520In%2520this%2520work%252C%2520we%2520give%2520a%2520positive%2520answer%2520to%2520these%250Aquestions%2520showing%2520that%2520for%2520general%2520classification%2520tasks%252C%2520DP%2520learnability%250Aimplies%2520online%2520learnability.%2520Our%2520proof%2520reasons%2520directly%2520about%2520Littlestone%250Atrees%252C%2520without%2520relying%2520on%2520thresholds.%2520We%2520achieve%2520this%2520by%2520establishing%2520several%250ARamsey-type%2520theorems%2520for%2520trees%252C%2520which%2520might%2520be%2520of%2520independent%2520interest.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.07765v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Ramsey%20Theorems%20for%20Trees%20and%20a%20General%20%27Private%20Learning%20Implies%20Online%0A%20%20Learning%27%20Theorem&entry.906535625=Simone%20Fioravanti%20and%20Steve%20Hanneke%20and%20Shay%20Moran%20and%20Hilla%20Schefler%20and%20Iska%20Tsubari&entry.1292438233=%20%20This%20work%20continues%20to%20investigate%20the%20link%20between%20differentially%20private%0A%28DP%29%20and%20online%20learning.%20Alon%2C%20Livni%2C%20Malliaris%2C%20and%20Moran%20%282019%29%20showed%20that%0Afor%20binary%20concept%20classes%2C%20DP%20learnability%20of%20a%20given%20class%20implies%20that%20it%0Ahas%20a%20finite%20Littlestone%20dimension%20%28equivalently%2C%20that%20it%20is%20online%20learnable%29.%0ATheir%20proof%20relies%20on%20a%20model-theoretic%20result%20by%20Hodges%20%281997%29%2C%20which%0Ademonstrates%20that%20any%20binary%20concept%20class%20with%20a%20large%20Littlestone%20dimension%0Acontains%20a%20large%20subclass%20of%20thresholds.%20In%20a%20follow-up%20work%2C%20Jung%2C%20Kim%2C%20and%0ATewari%20%282020%29%20extended%20this%20proof%20to%20multiclass%20PAC%20learning%20with%20a%20bounded%0Anumber%20of%20labels.%20Unfortunately%2C%20Hodges%27s%20result%20does%20not%20apply%20in%20other%0Anatural%20settings%20such%20as%20multiclass%20PAC%20learning%20with%20an%20unbounded%20label%20space%2C%0Aand%20PAC%20learning%20of%20partial%20concept%20classes.%0A%20%20This%20naturally%20raises%20the%20question%20of%20whether%20DP%20learnability%20continues%20to%0Aimply%20online%20learnability%20in%20more%20general%20scenarios%3A%20indeed%2C%20Alon%2C%20Hanneke%2C%0AHolzman%2C%20and%20Moran%20%282021%29%20explicitly%20leave%20it%20as%20an%20open%20question%20in%20the%0Acontext%20of%20partial%20concept%20classes%2C%20and%20the%20same%20question%20is%20open%20in%20the%0Ageneral%20multiclass%20setting.%20In%20this%20work%2C%20we%20give%20a%20positive%20answer%20to%20these%0Aquestions%20showing%20that%20for%20general%20classification%20tasks%2C%20DP%20learnability%0Aimplies%20online%20learnability.%20Our%20proof%20reasons%20directly%20about%20Littlestone%0Atrees%2C%20without%20relying%20on%20thresholds.%20We%20achieve%20this%20by%20establishing%20several%0ARamsey-type%20theorems%20for%20trees%2C%20which%20might%20be%20of%20independent%20interest.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.07765v2&entry.124074799=Read"},
{"title": "Towards Fair and Rigorous Evaluations: Hyperparameter Optimization for\n  Top-N Recommendation Task with Implicit Feedback", "author": "Hui Fang and Xu Feng and Lu Qin and Zhu Sun", "abstract": "  The widespread use of the internet has led to an overwhelming amount of data,\nwhich has resulted in the problem of information overload. Recommender systems\nhave emerged as a solution to this problem by providing personalized\nrecommendations to users based on their preferences and historical data.\nHowever, as recommendation models become increasingly complex, finding the best\nhyperparameter combination for different models has become a challenge. The\nhigh-dimensional hyperparameter search space poses numerous challenges for\nresearchers, and failure to disclose hyperparameter settings may impede the\nreproducibility of research results. In this paper, we investigate the Top-N\nimplicit recommendation problem and focus on optimizing the benchmark\nrecommendation algorithm commonly used in comparative experiments using\nhyperparameter optimization algorithms. We propose a research methodology that\nfollows the principles of a fair comparison, employing seven types of\nhyperparameter search algorithms to fine-tune six common recommendation\nalgorithms on three datasets. We have identified the most suitable\nhyperparameter search algorithms for various recommendation algorithms on\ndifferent types of datasets as a reference for later study. This study\ncontributes to algorithmic research in recommender systems based on\nhyperparameter optimization, providing a fair basis for comparison.\n", "link": "http://arxiv.org/abs/2408.07630v1", "date": "2024-08-14", "relevancy": 1.7722, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4484}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4433}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.4377}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Towards%20Fair%20and%20Rigorous%20Evaluations%3A%20Hyperparameter%20Optimization%20for%0A%20%20Top-N%20Recommendation%20Task%20with%20Implicit%20Feedback&body=Title%3A%20Towards%20Fair%20and%20Rigorous%20Evaluations%3A%20Hyperparameter%20Optimization%20for%0A%20%20Top-N%20Recommendation%20Task%20with%20Implicit%20Feedback%0AAuthor%3A%20Hui%20Fang%20and%20Xu%20Feng%20and%20Lu%20Qin%20and%20Zhu%20Sun%0AAbstract%3A%20%20%20The%20widespread%20use%20of%20the%20internet%20has%20led%20to%20an%20overwhelming%20amount%20of%20data%2C%0Awhich%20has%20resulted%20in%20the%20problem%20of%20information%20overload.%20Recommender%20systems%0Ahave%20emerged%20as%20a%20solution%20to%20this%20problem%20by%20providing%20personalized%0Arecommendations%20to%20users%20based%20on%20their%20preferences%20and%20historical%20data.%0AHowever%2C%20as%20recommendation%20models%20become%20increasingly%20complex%2C%20finding%20the%20best%0Ahyperparameter%20combination%20for%20different%20models%20has%20become%20a%20challenge.%20The%0Ahigh-dimensional%20hyperparameter%20search%20space%20poses%20numerous%20challenges%20for%0Aresearchers%2C%20and%20failure%20to%20disclose%20hyperparameter%20settings%20may%20impede%20the%0Areproducibility%20of%20research%20results.%20In%20this%20paper%2C%20we%20investigate%20the%20Top-N%0Aimplicit%20recommendation%20problem%20and%20focus%20on%20optimizing%20the%20benchmark%0Arecommendation%20algorithm%20commonly%20used%20in%20comparative%20experiments%20using%0Ahyperparameter%20optimization%20algorithms.%20We%20propose%20a%20research%20methodology%20that%0Afollows%20the%20principles%20of%20a%20fair%20comparison%2C%20employing%20seven%20types%20of%0Ahyperparameter%20search%20algorithms%20to%20fine-tune%20six%20common%20recommendation%0Aalgorithms%20on%20three%20datasets.%20We%20have%20identified%20the%20most%20suitable%0Ahyperparameter%20search%20algorithms%20for%20various%20recommendation%20algorithms%20on%0Adifferent%20types%20of%20datasets%20as%20a%20reference%20for%20later%20study.%20This%20study%0Acontributes%20to%20algorithmic%20research%20in%20recommender%20systems%20based%20on%0Ahyperparameter%20optimization%2C%20providing%20a%20fair%20basis%20for%20comparison.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07630v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DTowards%2520Fair%2520and%2520Rigorous%2520Evaluations%253A%2520Hyperparameter%2520Optimization%2520for%250A%2520%2520Top-N%2520Recommendation%2520Task%2520with%2520Implicit%2520Feedback%26entry.906535625%3DHui%2520Fang%2520and%2520Xu%2520Feng%2520and%2520Lu%2520Qin%2520and%2520Zhu%2520Sun%26entry.1292438233%3D%2520%2520The%2520widespread%2520use%2520of%2520the%2520internet%2520has%2520led%2520to%2520an%2520overwhelming%2520amount%2520of%2520data%252C%250Awhich%2520has%2520resulted%2520in%2520the%2520problem%2520of%2520information%2520overload.%2520Recommender%2520systems%250Ahave%2520emerged%2520as%2520a%2520solution%2520to%2520this%2520problem%2520by%2520providing%2520personalized%250Arecommendations%2520to%2520users%2520based%2520on%2520their%2520preferences%2520and%2520historical%2520data.%250AHowever%252C%2520as%2520recommendation%2520models%2520become%2520increasingly%2520complex%252C%2520finding%2520the%2520best%250Ahyperparameter%2520combination%2520for%2520different%2520models%2520has%2520become%2520a%2520challenge.%2520The%250Ahigh-dimensional%2520hyperparameter%2520search%2520space%2520poses%2520numerous%2520challenges%2520for%250Aresearchers%252C%2520and%2520failure%2520to%2520disclose%2520hyperparameter%2520settings%2520may%2520impede%2520the%250Areproducibility%2520of%2520research%2520results.%2520In%2520this%2520paper%252C%2520we%2520investigate%2520the%2520Top-N%250Aimplicit%2520recommendation%2520problem%2520and%2520focus%2520on%2520optimizing%2520the%2520benchmark%250Arecommendation%2520algorithm%2520commonly%2520used%2520in%2520comparative%2520experiments%2520using%250Ahyperparameter%2520optimization%2520algorithms.%2520We%2520propose%2520a%2520research%2520methodology%2520that%250Afollows%2520the%2520principles%2520of%2520a%2520fair%2520comparison%252C%2520employing%2520seven%2520types%2520of%250Ahyperparameter%2520search%2520algorithms%2520to%2520fine-tune%2520six%2520common%2520recommendation%250Aalgorithms%2520on%2520three%2520datasets.%2520We%2520have%2520identified%2520the%2520most%2520suitable%250Ahyperparameter%2520search%2520algorithms%2520for%2520various%2520recommendation%2520algorithms%2520on%250Adifferent%2520types%2520of%2520datasets%2520as%2520a%2520reference%2520for%2520later%2520study.%2520This%2520study%250Acontributes%2520to%2520algorithmic%2520research%2520in%2520recommender%2520systems%2520based%2520on%250Ahyperparameter%2520optimization%252C%2520providing%2520a%2520fair%2520basis%2520for%2520comparison.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07630v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Fair%20and%20Rigorous%20Evaluations%3A%20Hyperparameter%20Optimization%20for%0A%20%20Top-N%20Recommendation%20Task%20with%20Implicit%20Feedback&entry.906535625=Hui%20Fang%20and%20Xu%20Feng%20and%20Lu%20Qin%20and%20Zhu%20Sun&entry.1292438233=%20%20The%20widespread%20use%20of%20the%20internet%20has%20led%20to%20an%20overwhelming%20amount%20of%20data%2C%0Awhich%20has%20resulted%20in%20the%20problem%20of%20information%20overload.%20Recommender%20systems%0Ahave%20emerged%20as%20a%20solution%20to%20this%20problem%20by%20providing%20personalized%0Arecommendations%20to%20users%20based%20on%20their%20preferences%20and%20historical%20data.%0AHowever%2C%20as%20recommendation%20models%20become%20increasingly%20complex%2C%20finding%20the%20best%0Ahyperparameter%20combination%20for%20different%20models%20has%20become%20a%20challenge.%20The%0Ahigh-dimensional%20hyperparameter%20search%20space%20poses%20numerous%20challenges%20for%0Aresearchers%2C%20and%20failure%20to%20disclose%20hyperparameter%20settings%20may%20impede%20the%0Areproducibility%20of%20research%20results.%20In%20this%20paper%2C%20we%20investigate%20the%20Top-N%0Aimplicit%20recommendation%20problem%20and%20focus%20on%20optimizing%20the%20benchmark%0Arecommendation%20algorithm%20commonly%20used%20in%20comparative%20experiments%20using%0Ahyperparameter%20optimization%20algorithms.%20We%20propose%20a%20research%20methodology%20that%0Afollows%20the%20principles%20of%20a%20fair%20comparison%2C%20employing%20seven%20types%20of%0Ahyperparameter%20search%20algorithms%20to%20fine-tune%20six%20common%20recommendation%0Aalgorithms%20on%20three%20datasets.%20We%20have%20identified%20the%20most%20suitable%0Ahyperparameter%20search%20algorithms%20for%20various%20recommendation%20algorithms%20on%0Adifferent%20types%20of%20datasets%20as%20a%20reference%20for%20later%20study.%20This%20study%0Acontributes%20to%20algorithmic%20research%20in%20recommender%20systems%20based%20on%0Ahyperparameter%20optimization%2C%20providing%20a%20fair%20basis%20for%20comparison.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07630v1&entry.124074799=Read"},
{"title": "Learning Optimal Signal Temporal Logic Decision Trees for\n  Classification: A Max-Flow MILP Formulation", "author": "Kaier Liang and Gustavo A. Cardona and Disha Kamale and Cristian-Ioan Vasile", "abstract": "  This paper presents a novel framework for inferring timed temporal logic\nproperties from data. The dataset comprises pairs of finite-time system traces\nand corresponding labels, denoting whether the traces demonstrate specific\ndesired behaviors, e.g. whether the ship follows a safe route or not. Our\nproposed approach leverages decision-tree-based methods to infer Signal\nTemporal Logic classifiers using primitive formulae. We formulate the inference\nprocess as a mixed integer linear programming optimization problem, recursively\ngenerating constraints to determine both data classification and tree\nstructure. Applying a max-flow algorithm on the resultant tree transforms the\nproblem into a global optimization challenge, leading to improved\nclassification rates compared to prior methodologies. Moreover, we introduce a\ntechnique to reduce the number of constraints by exploiting the symmetry\ninherent in STL primitives, which enhances the algorithm's time performance and\ninterpretability. To assess our algorithm's effectiveness and classification\nperformance, we conduct three case studies involving two-class, multi-class,\nand complex formula classification scenarios.\n", "link": "http://arxiv.org/abs/2407.21090v2", "date": "2024-08-14", "relevancy": 1.77, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4727}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4452}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.4277}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Learning%20Optimal%20Signal%20Temporal%20Logic%20Decision%20Trees%20for%0A%20%20Classification%3A%20A%20Max-Flow%20MILP%20Formulation&body=Title%3A%20Learning%20Optimal%20Signal%20Temporal%20Logic%20Decision%20Trees%20for%0A%20%20Classification%3A%20A%20Max-Flow%20MILP%20Formulation%0AAuthor%3A%20Kaier%20Liang%20and%20Gustavo%20A.%20Cardona%20and%20Disha%20Kamale%20and%20Cristian-Ioan%20Vasile%0AAbstract%3A%20%20%20This%20paper%20presents%20a%20novel%20framework%20for%20inferring%20timed%20temporal%20logic%0Aproperties%20from%20data.%20The%20dataset%20comprises%20pairs%20of%20finite-time%20system%20traces%0Aand%20corresponding%20labels%2C%20denoting%20whether%20the%20traces%20demonstrate%20specific%0Adesired%20behaviors%2C%20e.g.%20whether%20the%20ship%20follows%20a%20safe%20route%20or%20not.%20Our%0Aproposed%20approach%20leverages%20decision-tree-based%20methods%20to%20infer%20Signal%0ATemporal%20Logic%20classifiers%20using%20primitive%20formulae.%20We%20formulate%20the%20inference%0Aprocess%20as%20a%20mixed%20integer%20linear%20programming%20optimization%20problem%2C%20recursively%0Agenerating%20constraints%20to%20determine%20both%20data%20classification%20and%20tree%0Astructure.%20Applying%20a%20max-flow%20algorithm%20on%20the%20resultant%20tree%20transforms%20the%0Aproblem%20into%20a%20global%20optimization%20challenge%2C%20leading%20to%20improved%0Aclassification%20rates%20compared%20to%20prior%20methodologies.%20Moreover%2C%20we%20introduce%20a%0Atechnique%20to%20reduce%20the%20number%20of%20constraints%20by%20exploiting%20the%20symmetry%0Ainherent%20in%20STL%20primitives%2C%20which%20enhances%20the%20algorithm%27s%20time%20performance%20and%0Ainterpretability.%20To%20assess%20our%20algorithm%27s%20effectiveness%20and%20classification%0Aperformance%2C%20we%20conduct%20three%20case%20studies%20involving%20two-class%2C%20multi-class%2C%0Aand%20complex%20formula%20classification%20scenarios.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2407.21090v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DLearning%2520Optimal%2520Signal%2520Temporal%2520Logic%2520Decision%2520Trees%2520for%250A%2520%2520Classification%253A%2520A%2520Max-Flow%2520MILP%2520Formulation%26entry.906535625%3DKaier%2520Liang%2520and%2520Gustavo%2520A.%2520Cardona%2520and%2520Disha%2520Kamale%2520and%2520Cristian-Ioan%2520Vasile%26entry.1292438233%3D%2520%2520This%2520paper%2520presents%2520a%2520novel%2520framework%2520for%2520inferring%2520timed%2520temporal%2520logic%250Aproperties%2520from%2520data.%2520The%2520dataset%2520comprises%2520pairs%2520of%2520finite-time%2520system%2520traces%250Aand%2520corresponding%2520labels%252C%2520denoting%2520whether%2520the%2520traces%2520demonstrate%2520specific%250Adesired%2520behaviors%252C%2520e.g.%2520whether%2520the%2520ship%2520follows%2520a%2520safe%2520route%2520or%2520not.%2520Our%250Aproposed%2520approach%2520leverages%2520decision-tree-based%2520methods%2520to%2520infer%2520Signal%250ATemporal%2520Logic%2520classifiers%2520using%2520primitive%2520formulae.%2520We%2520formulate%2520the%2520inference%250Aprocess%2520as%2520a%2520mixed%2520integer%2520linear%2520programming%2520optimization%2520problem%252C%2520recursively%250Agenerating%2520constraints%2520to%2520determine%2520both%2520data%2520classification%2520and%2520tree%250Astructure.%2520Applying%2520a%2520max-flow%2520algorithm%2520on%2520the%2520resultant%2520tree%2520transforms%2520the%250Aproblem%2520into%2520a%2520global%2520optimization%2520challenge%252C%2520leading%2520to%2520improved%250Aclassification%2520rates%2520compared%2520to%2520prior%2520methodologies.%2520Moreover%252C%2520we%2520introduce%2520a%250Atechnique%2520to%2520reduce%2520the%2520number%2520of%2520constraints%2520by%2520exploiting%2520the%2520symmetry%250Ainherent%2520in%2520STL%2520primitives%252C%2520which%2520enhances%2520the%2520algorithm%2527s%2520time%2520performance%2520and%250Ainterpretability.%2520To%2520assess%2520our%2520algorithm%2527s%2520effectiveness%2520and%2520classification%250Aperformance%252C%2520we%2520conduct%2520three%2520case%2520studies%2520involving%2520two-class%252C%2520multi-class%252C%250Aand%2520complex%2520formula%2520classification%2520scenarios.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2407.21090v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Learning%20Optimal%20Signal%20Temporal%20Logic%20Decision%20Trees%20for%0A%20%20Classification%3A%20A%20Max-Flow%20MILP%20Formulation&entry.906535625=Kaier%20Liang%20and%20Gustavo%20A.%20Cardona%20and%20Disha%20Kamale%20and%20Cristian-Ioan%20Vasile&entry.1292438233=%20%20This%20paper%20presents%20a%20novel%20framework%20for%20inferring%20timed%20temporal%20logic%0Aproperties%20from%20data.%20The%20dataset%20comprises%20pairs%20of%20finite-time%20system%20traces%0Aand%20corresponding%20labels%2C%20denoting%20whether%20the%20traces%20demonstrate%20specific%0Adesired%20behaviors%2C%20e.g.%20whether%20the%20ship%20follows%20a%20safe%20route%20or%20not.%20Our%0Aproposed%20approach%20leverages%20decision-tree-based%20methods%20to%20infer%20Signal%0ATemporal%20Logic%20classifiers%20using%20primitive%20formulae.%20We%20formulate%20the%20inference%0Aprocess%20as%20a%20mixed%20integer%20linear%20programming%20optimization%20problem%2C%20recursively%0Agenerating%20constraints%20to%20determine%20both%20data%20classification%20and%20tree%0Astructure.%20Applying%20a%20max-flow%20algorithm%20on%20the%20resultant%20tree%20transforms%20the%0Aproblem%20into%20a%20global%20optimization%20challenge%2C%20leading%20to%20improved%0Aclassification%20rates%20compared%20to%20prior%20methodologies.%20Moreover%2C%20we%20introduce%20a%0Atechnique%20to%20reduce%20the%20number%20of%20constraints%20by%20exploiting%20the%20symmetry%0Ainherent%20in%20STL%20primitives%2C%20which%20enhances%20the%20algorithm%27s%20time%20performance%20and%0Ainterpretability.%20To%20assess%20our%20algorithm%27s%20effectiveness%20and%20classification%0Aperformance%2C%20we%20conduct%20three%20case%20studies%20involving%20two-class%2C%20multi-class%2C%0Aand%20complex%20formula%20classification%20scenarios.%0A&entry.1838667208=http%3A//arxiv.org/abs/2407.21090v2&entry.124074799=Read"},
{"title": "MathScape: Evaluating MLLMs in multimodal Math Scenarios through a\n  Hierarchical Benchmark", "author": "Minxuan Zhou and Hao Liang and Tianpeng Li and Zhiyu Wu and Mingan Lin and Linzhuang Sun and Yaqi Zhou and Yan Zhang and Xiaoqin Huang and Yicong Chen and Yujing Qiao and Weipeng Chen and Bin Cui and Wentao Zhang and Zenan Zhou", "abstract": "  With the development of Multimodal Large Language Models (MLLMs), the\nevaluation of multimodal models in the context of mathematical problems has\nbecome a valuable research field. Multimodal visual-textual mathematical\nreasoning serves as a critical indicator for evaluating the comprehension and\ncomplex multi-step quantitative reasoning abilities of MLLMs. However, previous\nmultimodal math benchmarks have not sufficiently integrated visual and textual\ninformation. To address this gap, we proposed MathScape, a new benchmark that\nemphasizes the understanding and application of combined visual and textual\ninformation. MathScape is designed to evaluate photo-based math problem\nscenarios, assessing the theoretical understanding and application ability of\nMLLMs through a categorical hierarchical approach. We conduct a\nmulti-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark\nis challenging even for the most sophisticated models. By analyzing the\nevaluation results, we identify the limitations of MLLMs, offering valuable\ninsights for enhancing model performance.\n", "link": "http://arxiv.org/abs/2408.07543v1", "date": "2024-08-14", "relevancy": 1.5539, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.535}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5057}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4878}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20MathScape%3A%20Evaluating%20MLLMs%20in%20multimodal%20Math%20Scenarios%20through%20a%0A%20%20Hierarchical%20Benchmark&body=Title%3A%20MathScape%3A%20Evaluating%20MLLMs%20in%20multimodal%20Math%20Scenarios%20through%20a%0A%20%20Hierarchical%20Benchmark%0AAuthor%3A%20Minxuan%20Zhou%20and%20Hao%20Liang%20and%20Tianpeng%20Li%20and%20Zhiyu%20Wu%20and%20Mingan%20Lin%20and%20Linzhuang%20Sun%20and%20Yaqi%20Zhou%20and%20Yan%20Zhang%20and%20Xiaoqin%20Huang%20and%20Yicong%20Chen%20and%20Yujing%20Qiao%20and%20Weipeng%20Chen%20and%20Bin%20Cui%20and%20Wentao%20Zhang%20and%20Zenan%20Zhou%0AAbstract%3A%20%20%20With%20the%20development%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20the%0Aevaluation%20of%20multimodal%20models%20in%20the%20context%20of%20mathematical%20problems%20has%0Abecome%20a%20valuable%20research%20field.%20Multimodal%20visual-textual%20mathematical%0Areasoning%20serves%20as%20a%20critical%20indicator%20for%20evaluating%20the%20comprehension%20and%0Acomplex%20multi-step%20quantitative%20reasoning%20abilities%20of%20MLLMs.%20However%2C%20previous%0Amultimodal%20math%20benchmarks%20have%20not%20sufficiently%20integrated%20visual%20and%20textual%0Ainformation.%20To%20address%20this%20gap%2C%20we%20proposed%20MathScape%2C%20a%20new%20benchmark%20that%0Aemphasizes%20the%20understanding%20and%20application%20of%20combined%20visual%20and%20textual%0Ainformation.%20MathScape%20is%20designed%20to%20evaluate%20photo-based%20math%20problem%0Ascenarios%2C%20assessing%20the%20theoretical%20understanding%20and%20application%20ability%20of%0AMLLMs%20through%20a%20categorical%20hierarchical%20approach.%20We%20conduct%20a%0Amulti-dimensional%20evaluation%20on%2011%20advanced%20MLLMs%2C%20revealing%20that%20our%20benchmark%0Ais%20challenging%20even%20for%20the%20most%20sophisticated%20models.%20By%20analyzing%20the%0Aevaluation%20results%2C%20we%20identify%20the%20limitations%20of%20MLLMs%2C%20offering%20valuable%0Ainsights%20for%20enhancing%20model%20performance.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07543v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DMathScape%253A%2520Evaluating%2520MLLMs%2520in%2520multimodal%2520Math%2520Scenarios%2520through%2520a%250A%2520%2520Hierarchical%2520Benchmark%26entry.906535625%3DMinxuan%2520Zhou%2520and%2520Hao%2520Liang%2520and%2520Tianpeng%2520Li%2520and%2520Zhiyu%2520Wu%2520and%2520Mingan%2520Lin%2520and%2520Linzhuang%2520Sun%2520and%2520Yaqi%2520Zhou%2520and%2520Yan%2520Zhang%2520and%2520Xiaoqin%2520Huang%2520and%2520Yicong%2520Chen%2520and%2520Yujing%2520Qiao%2520and%2520Weipeng%2520Chen%2520and%2520Bin%2520Cui%2520and%2520Wentao%2520Zhang%2520and%2520Zenan%2520Zhou%26entry.1292438233%3D%2520%2520With%2520the%2520development%2520of%2520Multimodal%2520Large%2520Language%2520Models%2520%2528MLLMs%2529%252C%2520the%250Aevaluation%2520of%2520multimodal%2520models%2520in%2520the%2520context%2520of%2520mathematical%2520problems%2520has%250Abecome%2520a%2520valuable%2520research%2520field.%2520Multimodal%2520visual-textual%2520mathematical%250Areasoning%2520serves%2520as%2520a%2520critical%2520indicator%2520for%2520evaluating%2520the%2520comprehension%2520and%250Acomplex%2520multi-step%2520quantitative%2520reasoning%2520abilities%2520of%2520MLLMs.%2520However%252C%2520previous%250Amultimodal%2520math%2520benchmarks%2520have%2520not%2520sufficiently%2520integrated%2520visual%2520and%2520textual%250Ainformation.%2520To%2520address%2520this%2520gap%252C%2520we%2520proposed%2520MathScape%252C%2520a%2520new%2520benchmark%2520that%250Aemphasizes%2520the%2520understanding%2520and%2520application%2520of%2520combined%2520visual%2520and%2520textual%250Ainformation.%2520MathScape%2520is%2520designed%2520to%2520evaluate%2520photo-based%2520math%2520problem%250Ascenarios%252C%2520assessing%2520the%2520theoretical%2520understanding%2520and%2520application%2520ability%2520of%250AMLLMs%2520through%2520a%2520categorical%2520hierarchical%2520approach.%2520We%2520conduct%2520a%250Amulti-dimensional%2520evaluation%2520on%252011%2520advanced%2520MLLMs%252C%2520revealing%2520that%2520our%2520benchmark%250Ais%2520challenging%2520even%2520for%2520the%2520most%2520sophisticated%2520models.%2520By%2520analyzing%2520the%250Aevaluation%2520results%252C%2520we%2520identify%2520the%2520limitations%2520of%2520MLLMs%252C%2520offering%2520valuable%250Ainsights%2520for%2520enhancing%2520model%2520performance.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07543v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MathScape%3A%20Evaluating%20MLLMs%20in%20multimodal%20Math%20Scenarios%20through%20a%0A%20%20Hierarchical%20Benchmark&entry.906535625=Minxuan%20Zhou%20and%20Hao%20Liang%20and%20Tianpeng%20Li%20and%20Zhiyu%20Wu%20and%20Mingan%20Lin%20and%20Linzhuang%20Sun%20and%20Yaqi%20Zhou%20and%20Yan%20Zhang%20and%20Xiaoqin%20Huang%20and%20Yicong%20Chen%20and%20Yujing%20Qiao%20and%20Weipeng%20Chen%20and%20Bin%20Cui%20and%20Wentao%20Zhang%20and%20Zenan%20Zhou&entry.1292438233=%20%20With%20the%20development%20of%20Multimodal%20Large%20Language%20Models%20%28MLLMs%29%2C%20the%0Aevaluation%20of%20multimodal%20models%20in%20the%20context%20of%20mathematical%20problems%20has%0Abecome%20a%20valuable%20research%20field.%20Multimodal%20visual-textual%20mathematical%0Areasoning%20serves%20as%20a%20critical%20indicator%20for%20evaluating%20the%20comprehension%20and%0Acomplex%20multi-step%20quantitative%20reasoning%20abilities%20of%20MLLMs.%20However%2C%20previous%0Amultimodal%20math%20benchmarks%20have%20not%20sufficiently%20integrated%20visual%20and%20textual%0Ainformation.%20To%20address%20this%20gap%2C%20we%20proposed%20MathScape%2C%20a%20new%20benchmark%20that%0Aemphasizes%20the%20understanding%20and%20application%20of%20combined%20visual%20and%20textual%0Ainformation.%20MathScape%20is%20designed%20to%20evaluate%20photo-based%20math%20problem%0Ascenarios%2C%20assessing%20the%20theoretical%20understanding%20and%20application%20ability%20of%0AMLLMs%20through%20a%20categorical%20hierarchical%20approach.%20We%20conduct%20a%0Amulti-dimensional%20evaluation%20on%2011%20advanced%20MLLMs%2C%20revealing%20that%20our%20benchmark%0Ais%20challenging%20even%20for%20the%20most%20sophisticated%20models.%20By%20analyzing%20the%0Aevaluation%20results%2C%20we%20identify%20the%20limitations%20of%20MLLMs%2C%20offering%20valuable%0Ainsights%20for%20enhancing%20model%20performance.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07543v1&entry.124074799=Read"},
{"title": "A Survey of Open Source User Activity Traces with Applications to User\n  Mobility Characterization and Modeling", "author": "Sinjoni Mukhopadhyay King and Faisal Nawab and Katia Obraczka", "abstract": "  The current state-of-the-art in user mobility research has extensively relied\non open-source mobility traces captured from pedestrian and vehicular activity\nthrough a variety of communication technologies as users engage in a wide-range\nof applications, including connected healthcare, localization, social media,\ne-commerce, etc. Most of these traces are feature-rich and diverse, not only in\nthe information they provide, but also in how they can be used and leveraged.\nThis diversity poses two main challenges for researchers and practitioners who\nwish to make use of available mobility datasets. First, it is quite difficult\nto get a bird's eye view of the available traces without spending considerable\ntime looking them up. Second, once they have found the traces, they still need\nto figure out whether the traces are adequate to their needs.\n  The purpose of this survey is three-fold. It proposes a taxonomy to classify\nopen-source mobility traces including their mobility mode, data source and\ncollection technology. It then uses the proposed taxonomy to classify existing\nopen-source mobility traces and finally, highlights three case studies using\npopular publicly available datasets to showcase how our taxonomy can tease out\nfeature sets in traces to help determine their applicability to specific\nuse-cases.\n", "link": "http://arxiv.org/abs/2110.06382v3", "date": "2024-08-14", "relevancy": 1.7498, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4601}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4346}, {"title": "VirtualModel: Generating Object-ID-retentive Human-object Interaction\n  Image by Diffusion Model for E-commerce Marketing", "link": "http://arxiv.org/abs/2405.09985v1", "similarity": 0.4312}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20A%20Survey%20of%20Open%20Source%20User%20Activity%20Traces%20with%20Applications%20to%20User%0A%20%20Mobility%20Characterization%20and%20Modeling&body=Title%3A%20A%20Survey%20of%20Open%20Source%20User%20Activity%20Traces%20with%20Applications%20to%20User%0A%20%20Mobility%20Characterization%20and%20Modeling%0AAuthor%3A%20Sinjoni%20Mukhopadhyay%20King%20and%20Faisal%20Nawab%20and%20Katia%20Obraczka%0AAbstract%3A%20%20%20The%20current%20state-of-the-art%20in%20user%20mobility%20research%20has%20extensively%20relied%0Aon%20open-source%20mobility%20traces%20captured%20from%20pedestrian%20and%20vehicular%20activity%0Athrough%20a%20variety%20of%20communication%20technologies%20as%20users%20engage%20in%20a%20wide-range%0Aof%20applications%2C%20including%20connected%20healthcare%2C%20localization%2C%20social%20media%2C%0Ae-commerce%2C%20etc.%20Most%20of%20these%20traces%20are%20feature-rich%20and%20diverse%2C%20not%20only%20in%0Athe%20information%20they%20provide%2C%20but%20also%20in%20how%20they%20can%20be%20used%20and%20leveraged.%0AThis%20diversity%20poses%20two%20main%20challenges%20for%20researchers%20and%20practitioners%20who%0Awish%20to%20make%20use%20of%20available%20mobility%20datasets.%20First%2C%20it%20is%20quite%20difficult%0Ato%20get%20a%20bird%27s%20eye%20view%20of%20the%20available%20traces%20without%20spending%20considerable%0Atime%20looking%20them%20up.%20Second%2C%20once%20they%20have%20found%20the%20traces%2C%20they%20still%20need%0Ato%20figure%20out%20whether%20the%20traces%20are%20adequate%20to%20their%20needs.%0A%20%20The%20purpose%20of%20this%20survey%20is%20three-fold.%20It%20proposes%20a%20taxonomy%20to%20classify%0Aopen-source%20mobility%20traces%20including%20their%20mobility%20mode%2C%20data%20source%20and%0Acollection%20technology.%20It%20then%20uses%20the%20proposed%20taxonomy%20to%20classify%20existing%0Aopen-source%20mobility%20traces%20and%20finally%2C%20highlights%20three%20case%20studies%20using%0Apopular%20publicly%20available%20datasets%20to%20showcase%20how%20our%20taxonomy%20can%20tease%20out%0Afeature%20sets%20in%20traces%20to%20help%20determine%20their%20applicability%20to%20specific%0Ause-cases.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2110.06382v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DA%2520Survey%2520of%2520Open%2520Source%2520User%2520Activity%2520Traces%2520with%2520Applications%2520to%2520User%250A%2520%2520Mobility%2520Characterization%2520and%2520Modeling%26entry.906535625%3DSinjoni%2520Mukhopadhyay%2520King%2520and%2520Faisal%2520Nawab%2520and%2520Katia%2520Obraczka%26entry.1292438233%3D%2520%2520The%2520current%2520state-of-the-art%2520in%2520user%2520mobility%2520research%2520has%2520extensively%2520relied%250Aon%2520open-source%2520mobility%2520traces%2520captured%2520from%2520pedestrian%2520and%2520vehicular%2520activity%250Athrough%2520a%2520variety%2520of%2520communication%2520technologies%2520as%2520users%2520engage%2520in%2520a%2520wide-range%250Aof%2520applications%252C%2520including%2520connected%2520healthcare%252C%2520localization%252C%2520social%2520media%252C%250Ae-commerce%252C%2520etc.%2520Most%2520of%2520these%2520traces%2520are%2520feature-rich%2520and%2520diverse%252C%2520not%2520only%2520in%250Athe%2520information%2520they%2520provide%252C%2520but%2520also%2520in%2520how%2520they%2520can%2520be%2520used%2520and%2520leveraged.%250AThis%2520diversity%2520poses%2520two%2520main%2520challenges%2520for%2520researchers%2520and%2520practitioners%2520who%250Awish%2520to%2520make%2520use%2520of%2520available%2520mobility%2520datasets.%2520First%252C%2520it%2520is%2520quite%2520difficult%250Ato%2520get%2520a%2520bird%2527s%2520eye%2520view%2520of%2520the%2520available%2520traces%2520without%2520spending%2520considerable%250Atime%2520looking%2520them%2520up.%2520Second%252C%2520once%2520they%2520have%2520found%2520the%2520traces%252C%2520they%2520still%2520need%250Ato%2520figure%2520out%2520whether%2520the%2520traces%2520are%2520adequate%2520to%2520their%2520needs.%250A%2520%2520The%2520purpose%2520of%2520this%2520survey%2520is%2520three-fold.%2520It%2520proposes%2520a%2520taxonomy%2520to%2520classify%250Aopen-source%2520mobility%2520traces%2520including%2520their%2520mobility%2520mode%252C%2520data%2520source%2520and%250Acollection%2520technology.%2520It%2520then%2520uses%2520the%2520proposed%2520taxonomy%2520to%2520classify%2520existing%250Aopen-source%2520mobility%2520traces%2520and%2520finally%252C%2520highlights%2520three%2520case%2520studies%2520using%250Apopular%2520publicly%2520available%2520datasets%2520to%2520showcase%2520how%2520our%2520taxonomy%2520can%2520tease%2520out%250Afeature%2520sets%2520in%2520traces%2520to%2520help%2520determine%2520their%2520applicability%2520to%2520specific%250Ause-cases.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2110.06382v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Survey%20of%20Open%20Source%20User%20Activity%20Traces%20with%20Applications%20to%20User%0A%20%20Mobility%20Characterization%20and%20Modeling&entry.906535625=Sinjoni%20Mukhopadhyay%20King%20and%20Faisal%20Nawab%20and%20Katia%20Obraczka&entry.1292438233=%20%20The%20current%20state-of-the-art%20in%20user%20mobility%20research%20has%20extensively%20relied%0Aon%20open-source%20mobility%20traces%20captured%20from%20pedestrian%20and%20vehicular%20activity%0Athrough%20a%20variety%20of%20communication%20technologies%20as%20users%20engage%20in%20a%20wide-range%0Aof%20applications%2C%20including%20connected%20healthcare%2C%20localization%2C%20social%20media%2C%0Ae-commerce%2C%20etc.%20Most%20of%20these%20traces%20are%20feature-rich%20and%20diverse%2C%20not%20only%20in%0Athe%20information%20they%20provide%2C%20but%20also%20in%20how%20they%20can%20be%20used%20and%20leveraged.%0AThis%20diversity%20poses%20two%20main%20challenges%20for%20researchers%20and%20practitioners%20who%0Awish%20to%20make%20use%20of%20available%20mobility%20datasets.%20First%2C%20it%20is%20quite%20difficult%0Ato%20get%20a%20bird%27s%20eye%20view%20of%20the%20available%20traces%20without%20spending%20considerable%0Atime%20looking%20them%20up.%20Second%2C%20once%20they%20have%20found%20the%20traces%2C%20they%20still%20need%0Ato%20figure%20out%20whether%20the%20traces%20are%20adequate%20to%20their%20needs.%0A%20%20The%20purpose%20of%20this%20survey%20is%20three-fold.%20It%20proposes%20a%20taxonomy%20to%20classify%0Aopen-source%20mobility%20traces%20including%20their%20mobility%20mode%2C%20data%20source%20and%0Acollection%20technology.%20It%20then%20uses%20the%20proposed%20taxonomy%20to%20classify%20existing%0Aopen-source%20mobility%20traces%20and%20finally%2C%20highlights%20three%20case%20studies%20using%0Apopular%20publicly%20available%20datasets%20to%20showcase%20how%20our%20taxonomy%20can%20tease%20out%0Afeature%20sets%20in%20traces%20to%20help%20determine%20their%20applicability%20to%20specific%0Ause-cases.%0A&entry.1838667208=http%3A//arxiv.org/abs/2110.06382v3&entry.124074799=Read"},
{"title": "Fair Enough? A map of the current limitations of the requirements to\n  have fair algorithms", "author": "Daniele Regoli and Alessandro Castelnovo and Nicole Inverardi and Gabriele Nanino and Ilaria Penco", "abstract": "  In recent years, the increase in the usage and efficiency of Artificial\nIntelligence and, more in general, of Automated Decision-Making systems has\nbrought with it an increasing and welcome awareness of the risks associated\nwith such systems. One of such risks is that of perpetuating or even amplifying\nbias and unjust disparities present in the data from which many of these\nsystems learn to adjust and optimise their decisions. This awareness has on the\none hand encouraged several scientific communities to come up with more and\nmore appropriate ways and methods to assess, quantify, and possibly mitigate\nsuch biases and disparities. On the other hand, it has prompted more and more\nlayers of society, including policy makers, to call for fair algorithms. We\nbelieve that while many excellent and multidisciplinary research is currently\nbeing conducted, what is still fundamentally missing is the awareness that\nhaving fair algorithms is per se a nearly meaningless requirement that needs to\nbe complemented with many additional social choices to become actionable.\nNamely, there is a hiatus between what the society is demanding from Automated\nDecision-Making systems, and what this demand actually means in real-world\nscenarios. In this work, we outline the key features of such a hiatus and\npinpoint a set of crucial open points that we as a society must address in\norder to give a concrete meaning to the increasing demand of fairness in\nAutomated Decision-Making systems.\n", "link": "http://arxiv.org/abs/2311.12435v3", "date": "2024-08-14", "relevancy": 1.7492, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4476}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4469}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4236}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fair%20Enough%3F%20A%20map%20of%20the%20current%20limitations%20of%20the%20requirements%20to%0A%20%20have%20fair%20algorithms&body=Title%3A%20Fair%20Enough%3F%20A%20map%20of%20the%20current%20limitations%20of%20the%20requirements%20to%0A%20%20have%20fair%20algorithms%0AAuthor%3A%20Daniele%20Regoli%20and%20Alessandro%20Castelnovo%20and%20Nicole%20Inverardi%20and%20Gabriele%20Nanino%20and%20Ilaria%20Penco%0AAbstract%3A%20%20%20In%20recent%20years%2C%20the%20increase%20in%20the%20usage%20and%20efficiency%20of%20Artificial%0AIntelligence%20and%2C%20more%20in%20general%2C%20of%20Automated%20Decision-Making%20systems%20has%0Abrought%20with%20it%20an%20increasing%20and%20welcome%20awareness%20of%20the%20risks%20associated%0Awith%20such%20systems.%20One%20of%20such%20risks%20is%20that%20of%20perpetuating%20or%20even%20amplifying%0Abias%20and%20unjust%20disparities%20present%20in%20the%20data%20from%20which%20many%20of%20these%0Asystems%20learn%20to%20adjust%20and%20optimise%20their%20decisions.%20This%20awareness%20has%20on%20the%0Aone%20hand%20encouraged%20several%20scientific%20communities%20to%20come%20up%20with%20more%20and%0Amore%20appropriate%20ways%20and%20methods%20to%20assess%2C%20quantify%2C%20and%20possibly%20mitigate%0Asuch%20biases%20and%20disparities.%20On%20the%20other%20hand%2C%20it%20has%20prompted%20more%20and%20more%0Alayers%20of%20society%2C%20including%20policy%20makers%2C%20to%20call%20for%20fair%20algorithms.%20We%0Abelieve%20that%20while%20many%20excellent%20and%20multidisciplinary%20research%20is%20currently%0Abeing%20conducted%2C%20what%20is%20still%20fundamentally%20missing%20is%20the%20awareness%20that%0Ahaving%20fair%20algorithms%20is%20per%20se%20a%20nearly%20meaningless%20requirement%20that%20needs%20to%0Abe%20complemented%20with%20many%20additional%20social%20choices%20to%20become%20actionable.%0ANamely%2C%20there%20is%20a%20hiatus%20between%20what%20the%20society%20is%20demanding%20from%20Automated%0ADecision-Making%20systems%2C%20and%20what%20this%20demand%20actually%20means%20in%20real-world%0Ascenarios.%20In%20this%20work%2C%20we%20outline%20the%20key%20features%20of%20such%20a%20hiatus%20and%0Apinpoint%20a%20set%20of%20crucial%20open%20points%20that%20we%20as%20a%20society%20must%20address%20in%0Aorder%20to%20give%20a%20concrete%20meaning%20to%20the%20increasing%20demand%20of%20fairness%20in%0AAutomated%20Decision-Making%20systems.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2311.12435v3%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFair%2520Enough%253F%2520A%2520map%2520of%2520the%2520current%2520limitations%2520of%2520the%2520requirements%2520to%250A%2520%2520have%2520fair%2520algorithms%26entry.906535625%3DDaniele%2520Regoli%2520and%2520Alessandro%2520Castelnovo%2520and%2520Nicole%2520Inverardi%2520and%2520Gabriele%2520Nanino%2520and%2520Ilaria%2520Penco%26entry.1292438233%3D%2520%2520In%2520recent%2520years%252C%2520the%2520increase%2520in%2520the%2520usage%2520and%2520efficiency%2520of%2520Artificial%250AIntelligence%2520and%252C%2520more%2520in%2520general%252C%2520of%2520Automated%2520Decision-Making%2520systems%2520has%250Abrought%2520with%2520it%2520an%2520increasing%2520and%2520welcome%2520awareness%2520of%2520the%2520risks%2520associated%250Awith%2520such%2520systems.%2520One%2520of%2520such%2520risks%2520is%2520that%2520of%2520perpetuating%2520or%2520even%2520amplifying%250Abias%2520and%2520unjust%2520disparities%2520present%2520in%2520the%2520data%2520from%2520which%2520many%2520of%2520these%250Asystems%2520learn%2520to%2520adjust%2520and%2520optimise%2520their%2520decisions.%2520This%2520awareness%2520has%2520on%2520the%250Aone%2520hand%2520encouraged%2520several%2520scientific%2520communities%2520to%2520come%2520up%2520with%2520more%2520and%250Amore%2520appropriate%2520ways%2520and%2520methods%2520to%2520assess%252C%2520quantify%252C%2520and%2520possibly%2520mitigate%250Asuch%2520biases%2520and%2520disparities.%2520On%2520the%2520other%2520hand%252C%2520it%2520has%2520prompted%2520more%2520and%2520more%250Alayers%2520of%2520society%252C%2520including%2520policy%2520makers%252C%2520to%2520call%2520for%2520fair%2520algorithms.%2520We%250Abelieve%2520that%2520while%2520many%2520excellent%2520and%2520multidisciplinary%2520research%2520is%2520currently%250Abeing%2520conducted%252C%2520what%2520is%2520still%2520fundamentally%2520missing%2520is%2520the%2520awareness%2520that%250Ahaving%2520fair%2520algorithms%2520is%2520per%2520se%2520a%2520nearly%2520meaningless%2520requirement%2520that%2520needs%2520to%250Abe%2520complemented%2520with%2520many%2520additional%2520social%2520choices%2520to%2520become%2520actionable.%250ANamely%252C%2520there%2520is%2520a%2520hiatus%2520between%2520what%2520the%2520society%2520is%2520demanding%2520from%2520Automated%250ADecision-Making%2520systems%252C%2520and%2520what%2520this%2520demand%2520actually%2520means%2520in%2520real-world%250Ascenarios.%2520In%2520this%2520work%252C%2520we%2520outline%2520the%2520key%2520features%2520of%2520such%2520a%2520hiatus%2520and%250Apinpoint%2520a%2520set%2520of%2520crucial%2520open%2520points%2520that%2520we%2520as%2520a%2520society%2520must%2520address%2520in%250Aorder%2520to%2520give%2520a%2520concrete%2520meaning%2520to%2520the%2520increasing%2520demand%2520of%2520fairness%2520in%250AAutomated%2520Decision-Making%2520systems.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2311.12435v3%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fair%20Enough%3F%20A%20map%20of%20the%20current%20limitations%20of%20the%20requirements%20to%0A%20%20have%20fair%20algorithms&entry.906535625=Daniele%20Regoli%20and%20Alessandro%20Castelnovo%20and%20Nicole%20Inverardi%20and%20Gabriele%20Nanino%20and%20Ilaria%20Penco&entry.1292438233=%20%20In%20recent%20years%2C%20the%20increase%20in%20the%20usage%20and%20efficiency%20of%20Artificial%0AIntelligence%20and%2C%20more%20in%20general%2C%20of%20Automated%20Decision-Making%20systems%20has%0Abrought%20with%20it%20an%20increasing%20and%20welcome%20awareness%20of%20the%20risks%20associated%0Awith%20such%20systems.%20One%20of%20such%20risks%20is%20that%20of%20perpetuating%20or%20even%20amplifying%0Abias%20and%20unjust%20disparities%20present%20in%20the%20data%20from%20which%20many%20of%20these%0Asystems%20learn%20to%20adjust%20and%20optimise%20their%20decisions.%20This%20awareness%20has%20on%20the%0Aone%20hand%20encouraged%20several%20scientific%20communities%20to%20come%20up%20with%20more%20and%0Amore%20appropriate%20ways%20and%20methods%20to%20assess%2C%20quantify%2C%20and%20possibly%20mitigate%0Asuch%20biases%20and%20disparities.%20On%20the%20other%20hand%2C%20it%20has%20prompted%20more%20and%20more%0Alayers%20of%20society%2C%20including%20policy%20makers%2C%20to%20call%20for%20fair%20algorithms.%20We%0Abelieve%20that%20while%20many%20excellent%20and%20multidisciplinary%20research%20is%20currently%0Abeing%20conducted%2C%20what%20is%20still%20fundamentally%20missing%20is%20the%20awareness%20that%0Ahaving%20fair%20algorithms%20is%20per%20se%20a%20nearly%20meaningless%20requirement%20that%20needs%20to%0Abe%20complemented%20with%20many%20additional%20social%20choices%20to%20become%20actionable.%0ANamely%2C%20there%20is%20a%20hiatus%20between%20what%20the%20society%20is%20demanding%20from%20Automated%0ADecision-Making%20systems%2C%20and%20what%20this%20demand%20actually%20means%20in%20real-world%0Ascenarios.%20In%20this%20work%2C%20we%20outline%20the%20key%20features%20of%20such%20a%20hiatus%20and%0Apinpoint%20a%20set%20of%20crucial%20open%20points%20that%20we%20as%20a%20society%20must%20address%20in%0Aorder%20to%20give%20a%20concrete%20meaning%20to%20the%20increasing%20demand%20of%20fairness%20in%0AAutomated%20Decision-Making%20systems.%0A&entry.1838667208=http%3A//arxiv.org/abs/2311.12435v3&entry.124074799=Read"},
{"title": "Infra-YOLO: Efficient Neural Network Structure with Model Compression\n  for Real-Time Infrared Small Object Detection", "author": "Zhonglin Chen and Anyu Geng and Jianan Jiang and Jiwu Lu and Di Wu", "abstract": "  Although convolutional neural networks have made outstanding achievements in\nvisible light target detection, there are still many challenges in infrared\nsmall object detection because of the low signal-to-noise ratio, incomplete\nobject structure, and a lack of reliable infrared small object dataset. To\nresolve limitations of the infrared small object dataset, a new dataset named\nInfraTiny was constructed, and more than 85% bounding box is less than 32x32\npixels (3218 images and a total of 20,893 bounding boxes). A multi-scale\nattention mechanism module (MSAM) and a Feature Fusion Augmentation Pyramid\nModule (FFAFPM) were proposed and deployed onto embedded devices. The MSAM\nenables the network to obtain scale perception information by acquiring\ndifferent receptive fields, while the background noise information is\nsuppressed to enhance feature extraction ability. The proposed FFAFPM can\nenrich semantic information, and enhance the fusion of shallow feature and deep\nfeature, thus false positive results have been significantly reduced. By\nintegrating the proposed methods into the YOLO model, which is named\nInfra-YOLO, infrared small object detection performance has been improved.\nCompared to yolov3, mAP@0.5 has been improved by 2.7%; and compared to yolov4,\nthat by 2.5% on the InfraTiny dataset. The proposed Infra-YOLO was also\ntransferred onto the embedded device in the unmanned aerial vehicle (UAV) for\nreal application scenarios, where the channel pruning method is adopted to\nreduce FLOPs and to achieve a tradeoff between speed and accuracy. Even if the\nparameters of Infra-YOLO are reduced by 88% with the pruning method, a gain of\n0.7% is still achieved on mAP@0.5 compared to yolov3, and a gain of 0.5%\ncompared to yolov4. Experimental results show that the proposed MSAM and FFAFPM\nmethod can improve infrared small object detection performance compared with\nthe previous benchmark method.\n", "link": "http://arxiv.org/abs/2408.07455v1", "date": "2024-08-14", "relevancy": 1.5875, "topK": [{"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.5455}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5297}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5115}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Infra-YOLO%3A%20Efficient%20Neural%20Network%20Structure%20with%20Model%20Compression%0A%20%20for%20Real-Time%20Infrared%20Small%20Object%20Detection&body=Title%3A%20Infra-YOLO%3A%20Efficient%20Neural%20Network%20Structure%20with%20Model%20Compression%0A%20%20for%20Real-Time%20Infrared%20Small%20Object%20Detection%0AAuthor%3A%20Zhonglin%20Chen%20and%20Anyu%20Geng%20and%20Jianan%20Jiang%20and%20Jiwu%20Lu%20and%20Di%20Wu%0AAbstract%3A%20%20%20Although%20convolutional%20neural%20networks%20have%20made%20outstanding%20achievements%20in%0Avisible%20light%20target%20detection%2C%20there%20are%20still%20many%20challenges%20in%20infrared%0Asmall%20object%20detection%20because%20of%20the%20low%20signal-to-noise%20ratio%2C%20incomplete%0Aobject%20structure%2C%20and%20a%20lack%20of%20reliable%20infrared%20small%20object%20dataset.%20To%0Aresolve%20limitations%20of%20the%20infrared%20small%20object%20dataset%2C%20a%20new%20dataset%20named%0AInfraTiny%20was%20constructed%2C%20and%20more%20than%2085%25%20bounding%20box%20is%20less%20than%2032x32%0Apixels%20%283218%20images%20and%20a%20total%20of%2020%2C893%20bounding%20boxes%29.%20A%20multi-scale%0Aattention%20mechanism%20module%20%28MSAM%29%20and%20a%20Feature%20Fusion%20Augmentation%20Pyramid%0AModule%20%28FFAFPM%29%20were%20proposed%20and%20deployed%20onto%20embedded%20devices.%20The%20MSAM%0Aenables%20the%20network%20to%20obtain%20scale%20perception%20information%20by%20acquiring%0Adifferent%20receptive%20fields%2C%20while%20the%20background%20noise%20information%20is%0Asuppressed%20to%20enhance%20feature%20extraction%20ability.%20The%20proposed%20FFAFPM%20can%0Aenrich%20semantic%20information%2C%20and%20enhance%20the%20fusion%20of%20shallow%20feature%20and%20deep%0Afeature%2C%20thus%20false%20positive%20results%20have%20been%20significantly%20reduced.%20By%0Aintegrating%20the%20proposed%20methods%20into%20the%20YOLO%20model%2C%20which%20is%20named%0AInfra-YOLO%2C%20infrared%20small%20object%20detection%20performance%20has%20been%20improved.%0ACompared%20to%20yolov3%2C%20mAP%400.5%20has%20been%20improved%20by%202.7%25%3B%20and%20compared%20to%20yolov4%2C%0Athat%20by%202.5%25%20on%20the%20InfraTiny%20dataset.%20The%20proposed%20Infra-YOLO%20was%20also%0Atransferred%20onto%20the%20embedded%20device%20in%20the%20unmanned%20aerial%20vehicle%20%28UAV%29%20for%0Areal%20application%20scenarios%2C%20where%20the%20channel%20pruning%20method%20is%20adopted%20to%0Areduce%20FLOPs%20and%20to%20achieve%20a%20tradeoff%20between%20speed%20and%20accuracy.%20Even%20if%20the%0Aparameters%20of%20Infra-YOLO%20are%20reduced%20by%2088%25%20with%20the%20pruning%20method%2C%20a%20gain%20of%0A0.7%25%20is%20still%20achieved%20on%20mAP%400.5%20compared%20to%20yolov3%2C%20and%20a%20gain%20of%200.5%25%0Acompared%20to%20yolov4.%20Experimental%20results%20show%20that%20the%20proposed%20MSAM%20and%20FFAFPM%0Amethod%20can%20improve%20infrared%20small%20object%20detection%20performance%20compared%20with%0Athe%20previous%20benchmark%20method.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07455v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DInfra-YOLO%253A%2520Efficient%2520Neural%2520Network%2520Structure%2520with%2520Model%2520Compression%250A%2520%2520for%2520Real-Time%2520Infrared%2520Small%2520Object%2520Detection%26entry.906535625%3DZhonglin%2520Chen%2520and%2520Anyu%2520Geng%2520and%2520Jianan%2520Jiang%2520and%2520Jiwu%2520Lu%2520and%2520Di%2520Wu%26entry.1292438233%3D%2520%2520Although%2520convolutional%2520neural%2520networks%2520have%2520made%2520outstanding%2520achievements%2520in%250Avisible%2520light%2520target%2520detection%252C%2520there%2520are%2520still%2520many%2520challenges%2520in%2520infrared%250Asmall%2520object%2520detection%2520because%2520of%2520the%2520low%2520signal-to-noise%2520ratio%252C%2520incomplete%250Aobject%2520structure%252C%2520and%2520a%2520lack%2520of%2520reliable%2520infrared%2520small%2520object%2520dataset.%2520To%250Aresolve%2520limitations%2520of%2520the%2520infrared%2520small%2520object%2520dataset%252C%2520a%2520new%2520dataset%2520named%250AInfraTiny%2520was%2520constructed%252C%2520and%2520more%2520than%252085%2525%2520bounding%2520box%2520is%2520less%2520than%252032x32%250Apixels%2520%25283218%2520images%2520and%2520a%2520total%2520of%252020%252C893%2520bounding%2520boxes%2529.%2520A%2520multi-scale%250Aattention%2520mechanism%2520module%2520%2528MSAM%2529%2520and%2520a%2520Feature%2520Fusion%2520Augmentation%2520Pyramid%250AModule%2520%2528FFAFPM%2529%2520were%2520proposed%2520and%2520deployed%2520onto%2520embedded%2520devices.%2520The%2520MSAM%250Aenables%2520the%2520network%2520to%2520obtain%2520scale%2520perception%2520information%2520by%2520acquiring%250Adifferent%2520receptive%2520fields%252C%2520while%2520the%2520background%2520noise%2520information%2520is%250Asuppressed%2520to%2520enhance%2520feature%2520extraction%2520ability.%2520The%2520proposed%2520FFAFPM%2520can%250Aenrich%2520semantic%2520information%252C%2520and%2520enhance%2520the%2520fusion%2520of%2520shallow%2520feature%2520and%2520deep%250Afeature%252C%2520thus%2520false%2520positive%2520results%2520have%2520been%2520significantly%2520reduced.%2520By%250Aintegrating%2520the%2520proposed%2520methods%2520into%2520the%2520YOLO%2520model%252C%2520which%2520is%2520named%250AInfra-YOLO%252C%2520infrared%2520small%2520object%2520detection%2520performance%2520has%2520been%2520improved.%250ACompared%2520to%2520yolov3%252C%2520mAP%25400.5%2520has%2520been%2520improved%2520by%25202.7%2525%253B%2520and%2520compared%2520to%2520yolov4%252C%250Athat%2520by%25202.5%2525%2520on%2520the%2520InfraTiny%2520dataset.%2520The%2520proposed%2520Infra-YOLO%2520was%2520also%250Atransferred%2520onto%2520the%2520embedded%2520device%2520in%2520the%2520unmanned%2520aerial%2520vehicle%2520%2528UAV%2529%2520for%250Areal%2520application%2520scenarios%252C%2520where%2520the%2520channel%2520pruning%2520method%2520is%2520adopted%2520to%250Areduce%2520FLOPs%2520and%2520to%2520achieve%2520a%2520tradeoff%2520between%2520speed%2520and%2520accuracy.%2520Even%2520if%2520the%250Aparameters%2520of%2520Infra-YOLO%2520are%2520reduced%2520by%252088%2525%2520with%2520the%2520pruning%2520method%252C%2520a%2520gain%2520of%250A0.7%2525%2520is%2520still%2520achieved%2520on%2520mAP%25400.5%2520compared%2520to%2520yolov3%252C%2520and%2520a%2520gain%2520of%25200.5%2525%250Acompared%2520to%2520yolov4.%2520Experimental%2520results%2520show%2520that%2520the%2520proposed%2520MSAM%2520and%2520FFAFPM%250Amethod%2520can%2520improve%2520infrared%2520small%2520object%2520detection%2520performance%2520compared%2520with%250Athe%2520previous%2520benchmark%2520method.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07455v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Infra-YOLO%3A%20Efficient%20Neural%20Network%20Structure%20with%20Model%20Compression%0A%20%20for%20Real-Time%20Infrared%20Small%20Object%20Detection&entry.906535625=Zhonglin%20Chen%20and%20Anyu%20Geng%20and%20Jianan%20Jiang%20and%20Jiwu%20Lu%20and%20Di%20Wu&entry.1292438233=%20%20Although%20convolutional%20neural%20networks%20have%20made%20outstanding%20achievements%20in%0Avisible%20light%20target%20detection%2C%20there%20are%20still%20many%20challenges%20in%20infrared%0Asmall%20object%20detection%20because%20of%20the%20low%20signal-to-noise%20ratio%2C%20incomplete%0Aobject%20structure%2C%20and%20a%20lack%20of%20reliable%20infrared%20small%20object%20dataset.%20To%0Aresolve%20limitations%20of%20the%20infrared%20small%20object%20dataset%2C%20a%20new%20dataset%20named%0AInfraTiny%20was%20constructed%2C%20and%20more%20than%2085%25%20bounding%20box%20is%20less%20than%2032x32%0Apixels%20%283218%20images%20and%20a%20total%20of%2020%2C893%20bounding%20boxes%29.%20A%20multi-scale%0Aattention%20mechanism%20module%20%28MSAM%29%20and%20a%20Feature%20Fusion%20Augmentation%20Pyramid%0AModule%20%28FFAFPM%29%20were%20proposed%20and%20deployed%20onto%20embedded%20devices.%20The%20MSAM%0Aenables%20the%20network%20to%20obtain%20scale%20perception%20information%20by%20acquiring%0Adifferent%20receptive%20fields%2C%20while%20the%20background%20noise%20information%20is%0Asuppressed%20to%20enhance%20feature%20extraction%20ability.%20The%20proposed%20FFAFPM%20can%0Aenrich%20semantic%20information%2C%20and%20enhance%20the%20fusion%20of%20shallow%20feature%20and%20deep%0Afeature%2C%20thus%20false%20positive%20results%20have%20been%20significantly%20reduced.%20By%0Aintegrating%20the%20proposed%20methods%20into%20the%20YOLO%20model%2C%20which%20is%20named%0AInfra-YOLO%2C%20infrared%20small%20object%20detection%20performance%20has%20been%20improved.%0ACompared%20to%20yolov3%2C%20mAP%400.5%20has%20been%20improved%20by%202.7%25%3B%20and%20compared%20to%20yolov4%2C%0Athat%20by%202.5%25%20on%20the%20InfraTiny%20dataset.%20The%20proposed%20Infra-YOLO%20was%20also%0Atransferred%20onto%20the%20embedded%20device%20in%20the%20unmanned%20aerial%20vehicle%20%28UAV%29%20for%0Areal%20application%20scenarios%2C%20where%20the%20channel%20pruning%20method%20is%20adopted%20to%0Areduce%20FLOPs%20and%20to%20achieve%20a%20tradeoff%20between%20speed%20and%20accuracy.%20Even%20if%20the%0Aparameters%20of%20Infra-YOLO%20are%20reduced%20by%2088%25%20with%20the%20pruning%20method%2C%20a%20gain%20of%0A0.7%25%20is%20still%20achieved%20on%20mAP%400.5%20compared%20to%20yolov3%2C%20and%20a%20gain%20of%200.5%25%0Acompared%20to%20yolov4.%20Experimental%20results%20show%20that%20the%20proposed%20MSAM%20and%20FFAFPM%0Amethod%20can%20improve%20infrared%20small%20object%20detection%20performance%20compared%20with%0Athe%20previous%20benchmark%20method.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07455v1&entry.124074799=Read"},
{"title": "Fact or Fiction? Improving Fact Verification with Knowledge Graphs\n  through Simplified Subgraph Retrievals", "author": "Tobias A. Opsahl", "abstract": "  Despite recent success in natural language processing (NLP), fact\nverification still remains a difficult task. Due to misinformation spreading\nincreasingly fast, attention has been directed towards automatically verifying\nthe correctness of claims. In the domain of NLP, this is usually done by\ntraining supervised machine learning models to verify claims by utilizing\nevidence from trustworthy corpora. We present efficient methods for verifying\nclaims on a dataset where the evidence is in the form of structured knowledge\ngraphs. We use the FactKG dataset, which is constructed from the DBpedia\nknowledge graph extracted from Wikipedia. By simplifying the evidence retrieval\nprocess, from fine-tuned language models to simple logical retrievals, we are\nable to construct models that both require less computational resources and\nachieve better test-set accuracy.\n", "link": "http://arxiv.org/abs/2408.07453v1", "date": "2024-08-14", "relevancy": 1.2421, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4214}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4123}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4109}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Fact%20or%20Fiction%3F%20Improving%20Fact%20Verification%20with%20Knowledge%20Graphs%0A%20%20through%20Simplified%20Subgraph%20Retrievals&body=Title%3A%20Fact%20or%20Fiction%3F%20Improving%20Fact%20Verification%20with%20Knowledge%20Graphs%0A%20%20through%20Simplified%20Subgraph%20Retrievals%0AAuthor%3A%20Tobias%20A.%20Opsahl%0AAbstract%3A%20%20%20Despite%20recent%20success%20in%20natural%20language%20processing%20%28NLP%29%2C%20fact%0Averification%20still%20remains%20a%20difficult%20task.%20Due%20to%20misinformation%20spreading%0Aincreasingly%20fast%2C%20attention%20has%20been%20directed%20towards%20automatically%20verifying%0Athe%20correctness%20of%20claims.%20In%20the%20domain%20of%20NLP%2C%20this%20is%20usually%20done%20by%0Atraining%20supervised%20machine%20learning%20models%20to%20verify%20claims%20by%20utilizing%0Aevidence%20from%20trustworthy%20corpora.%20We%20present%20efficient%20methods%20for%20verifying%0Aclaims%20on%20a%20dataset%20where%20the%20evidence%20is%20in%20the%20form%20of%20structured%20knowledge%0Agraphs.%20We%20use%20the%20FactKG%20dataset%2C%20which%20is%20constructed%20from%20the%20DBpedia%0Aknowledge%20graph%20extracted%20from%20Wikipedia.%20By%20simplifying%20the%20evidence%20retrieval%0Aprocess%2C%20from%20fine-tuned%20language%20models%20to%20simple%20logical%20retrievals%2C%20we%20are%0Aable%20to%20construct%20models%20that%20both%20require%20less%20computational%20resources%20and%0Aachieve%20better%20test-set%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07453v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFact%2520or%2520Fiction%253F%2520Improving%2520Fact%2520Verification%2520with%2520Knowledge%2520Graphs%250A%2520%2520through%2520Simplified%2520Subgraph%2520Retrievals%26entry.906535625%3DTobias%2520A.%2520Opsahl%26entry.1292438233%3D%2520%2520Despite%2520recent%2520success%2520in%2520natural%2520language%2520processing%2520%2528NLP%2529%252C%2520fact%250Averification%2520still%2520remains%2520a%2520difficult%2520task.%2520Due%2520to%2520misinformation%2520spreading%250Aincreasingly%2520fast%252C%2520attention%2520has%2520been%2520directed%2520towards%2520automatically%2520verifying%250Athe%2520correctness%2520of%2520claims.%2520In%2520the%2520domain%2520of%2520NLP%252C%2520this%2520is%2520usually%2520done%2520by%250Atraining%2520supervised%2520machine%2520learning%2520models%2520to%2520verify%2520claims%2520by%2520utilizing%250Aevidence%2520from%2520trustworthy%2520corpora.%2520We%2520present%2520efficient%2520methods%2520for%2520verifying%250Aclaims%2520on%2520a%2520dataset%2520where%2520the%2520evidence%2520is%2520in%2520the%2520form%2520of%2520structured%2520knowledge%250Agraphs.%2520We%2520use%2520the%2520FactKG%2520dataset%252C%2520which%2520is%2520constructed%2520from%2520the%2520DBpedia%250Aknowledge%2520graph%2520extracted%2520from%2520Wikipedia.%2520By%2520simplifying%2520the%2520evidence%2520retrieval%250Aprocess%252C%2520from%2520fine-tuned%2520language%2520models%2520to%2520simple%2520logical%2520retrievals%252C%2520we%2520are%250Aable%2520to%2520construct%2520models%2520that%2520both%2520require%2520less%2520computational%2520resources%2520and%250Aachieve%2520better%2520test-set%2520accuracy.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07453v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Fact%20or%20Fiction%3F%20Improving%20Fact%20Verification%20with%20Knowledge%20Graphs%0A%20%20through%20Simplified%20Subgraph%20Retrievals&entry.906535625=Tobias%20A.%20Opsahl&entry.1292438233=%20%20Despite%20recent%20success%20in%20natural%20language%20processing%20%28NLP%29%2C%20fact%0Averification%20still%20remains%20a%20difficult%20task.%20Due%20to%20misinformation%20spreading%0Aincreasingly%20fast%2C%20attention%20has%20been%20directed%20towards%20automatically%20verifying%0Athe%20correctness%20of%20claims.%20In%20the%20domain%20of%20NLP%2C%20this%20is%20usually%20done%20by%0Atraining%20supervised%20machine%20learning%20models%20to%20verify%20claims%20by%20utilizing%0Aevidence%20from%20trustworthy%20corpora.%20We%20present%20efficient%20methods%20for%20verifying%0Aclaims%20on%20a%20dataset%20where%20the%20evidence%20is%20in%20the%20form%20of%20structured%20knowledge%0Agraphs.%20We%20use%20the%20FactKG%20dataset%2C%20which%20is%20constructed%20from%20the%20DBpedia%0Aknowledge%20graph%20extracted%20from%20Wikipedia.%20By%20simplifying%20the%20evidence%20retrieval%0Aprocess%2C%20from%20fine-tuned%20language%20models%20to%20simple%20logical%20retrievals%2C%20we%20are%0Aable%20to%20construct%20models%20that%20both%20require%20less%20computational%20resources%20and%0Aachieve%20better%20test-set%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07453v1&entry.124074799=Read"},
{"title": "Faster Stochastic Optimization with Arbitrary Delays via Asynchronous\n  Mini-Batching", "author": "Amit Attia and Ofir Gaash and Tomer Koren", "abstract": "  We consider the problem of asynchronous stochastic optimization, where an\noptimization algorithm makes updates based on stale stochastic gradients of the\nobjective that are subject to an arbitrary (possibly adversarial) sequence of\ndelays. We present a procedure which, for any given $q \\in (0,1]$, transforms\nany standard stochastic first-order method to an asynchronous method with\nconvergence guarantee depending on the $q$-quantile delay of the sequence. This\napproach leads to convergence rates of the form $O(\\tau_q/qT+\\sigma/\\sqrt{qT})$\nfor non-convex and $O(\\tau_q^2/(q T)^2+\\sigma/\\sqrt{qT})$ for convex smooth\nproblems, where $\\tau_q$ is the $q$-quantile delay, generalizing and improving\non existing results that depend on the average delay. We further show a method\nthat automatically adapts to all quantiles simultaneously, without any prior\nknowledge of the delays, achieving convergence rates of the form $O(\\inf_{q}\n\\tau_q/qT+\\sigma/\\sqrt{qT})$ for non-convex and $O(\\inf_{q} \\tau_q^2/(q\nT)^2+\\sigma/\\sqrt{qT})$ for convex smooth problems. Our technique is based on\nasynchronous mini-batching with a careful batch-size selection and filtering of\nstale gradients.\n", "link": "http://arxiv.org/abs/2408.07503v1", "date": "2024-08-14", "relevancy": 1.7453, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.465}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.4409}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4058}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Faster%20Stochastic%20Optimization%20with%20Arbitrary%20Delays%20via%20Asynchronous%0A%20%20Mini-Batching&body=Title%3A%20Faster%20Stochastic%20Optimization%20with%20Arbitrary%20Delays%20via%20Asynchronous%0A%20%20Mini-Batching%0AAuthor%3A%20Amit%20Attia%20and%20Ofir%20Gaash%20and%20Tomer%20Koren%0AAbstract%3A%20%20%20We%20consider%20the%20problem%20of%20asynchronous%20stochastic%20optimization%2C%20where%20an%0Aoptimization%20algorithm%20makes%20updates%20based%20on%20stale%20stochastic%20gradients%20of%20the%0Aobjective%20that%20are%20subject%20to%20an%20arbitrary%20%28possibly%20adversarial%29%20sequence%20of%0Adelays.%20We%20present%20a%20procedure%20which%2C%20for%20any%20given%20%24q%20%5Cin%20%280%2C1%5D%24%2C%20transforms%0Aany%20standard%20stochastic%20first-order%20method%20to%20an%20asynchronous%20method%20with%0Aconvergence%20guarantee%20depending%20on%20the%20%24q%24-quantile%20delay%20of%20the%20sequence.%20This%0Aapproach%20leads%20to%20convergence%20rates%20of%20the%20form%20%24O%28%5Ctau_q/qT%2B%5Csigma/%5Csqrt%7BqT%7D%29%24%0Afor%20non-convex%20and%20%24O%28%5Ctau_q%5E2/%28q%20T%29%5E2%2B%5Csigma/%5Csqrt%7BqT%7D%29%24%20for%20convex%20smooth%0Aproblems%2C%20where%20%24%5Ctau_q%24%20is%20the%20%24q%24-quantile%20delay%2C%20generalizing%20and%20improving%0Aon%20existing%20results%20that%20depend%20on%20the%20average%20delay.%20We%20further%20show%20a%20method%0Athat%20automatically%20adapts%20to%20all%20quantiles%20simultaneously%2C%20without%20any%20prior%0Aknowledge%20of%20the%20delays%2C%20achieving%20convergence%20rates%20of%20the%20form%20%24O%28%5Cinf_%7Bq%7D%0A%5Ctau_q/qT%2B%5Csigma/%5Csqrt%7BqT%7D%29%24%20for%20non-convex%20and%20%24O%28%5Cinf_%7Bq%7D%20%5Ctau_q%5E2/%28q%0AT%29%5E2%2B%5Csigma/%5Csqrt%7BqT%7D%29%24%20for%20convex%20smooth%20problems.%20Our%20technique%20is%20based%20on%0Aasynchronous%20mini-batching%20with%20a%20careful%20batch-size%20selection%20and%20filtering%20of%0Astale%20gradients.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07503v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DFaster%2520Stochastic%2520Optimization%2520with%2520Arbitrary%2520Delays%2520via%2520Asynchronous%250A%2520%2520Mini-Batching%26entry.906535625%3DAmit%2520Attia%2520and%2520Ofir%2520Gaash%2520and%2520Tomer%2520Koren%26entry.1292438233%3D%2520%2520We%2520consider%2520the%2520problem%2520of%2520asynchronous%2520stochastic%2520optimization%252C%2520where%2520an%250Aoptimization%2520algorithm%2520makes%2520updates%2520based%2520on%2520stale%2520stochastic%2520gradients%2520of%2520the%250Aobjective%2520that%2520are%2520subject%2520to%2520an%2520arbitrary%2520%2528possibly%2520adversarial%2529%2520sequence%2520of%250Adelays.%2520We%2520present%2520a%2520procedure%2520which%252C%2520for%2520any%2520given%2520%2524q%2520%255Cin%2520%25280%252C1%255D%2524%252C%2520transforms%250Aany%2520standard%2520stochastic%2520first-order%2520method%2520to%2520an%2520asynchronous%2520method%2520with%250Aconvergence%2520guarantee%2520depending%2520on%2520the%2520%2524q%2524-quantile%2520delay%2520of%2520the%2520sequence.%2520This%250Aapproach%2520leads%2520to%2520convergence%2520rates%2520of%2520the%2520form%2520%2524O%2528%255Ctau_q/qT%252B%255Csigma/%255Csqrt%257BqT%257D%2529%2524%250Afor%2520non-convex%2520and%2520%2524O%2528%255Ctau_q%255E2/%2528q%2520T%2529%255E2%252B%255Csigma/%255Csqrt%257BqT%257D%2529%2524%2520for%2520convex%2520smooth%250Aproblems%252C%2520where%2520%2524%255Ctau_q%2524%2520is%2520the%2520%2524q%2524-quantile%2520delay%252C%2520generalizing%2520and%2520improving%250Aon%2520existing%2520results%2520that%2520depend%2520on%2520the%2520average%2520delay.%2520We%2520further%2520show%2520a%2520method%250Athat%2520automatically%2520adapts%2520to%2520all%2520quantiles%2520simultaneously%252C%2520without%2520any%2520prior%250Aknowledge%2520of%2520the%2520delays%252C%2520achieving%2520convergence%2520rates%2520of%2520the%2520form%2520%2524O%2528%255Cinf_%257Bq%257D%250A%255Ctau_q/qT%252B%255Csigma/%255Csqrt%257BqT%257D%2529%2524%2520for%2520non-convex%2520and%2520%2524O%2528%255Cinf_%257Bq%257D%2520%255Ctau_q%255E2/%2528q%250AT%2529%255E2%252B%255Csigma/%255Csqrt%257BqT%257D%2529%2524%2520for%2520convex%2520smooth%2520problems.%2520Our%2520technique%2520is%2520based%2520on%250Aasynchronous%2520mini-batching%2520with%2520a%2520careful%2520batch-size%2520selection%2520and%2520filtering%2520of%250Astale%2520gradients.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07503v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Faster%20Stochastic%20Optimization%20with%20Arbitrary%20Delays%20via%20Asynchronous%0A%20%20Mini-Batching&entry.906535625=Amit%20Attia%20and%20Ofir%20Gaash%20and%20Tomer%20Koren&entry.1292438233=%20%20We%20consider%20the%20problem%20of%20asynchronous%20stochastic%20optimization%2C%20where%20an%0Aoptimization%20algorithm%20makes%20updates%20based%20on%20stale%20stochastic%20gradients%20of%20the%0Aobjective%20that%20are%20subject%20to%20an%20arbitrary%20%28possibly%20adversarial%29%20sequence%20of%0Adelays.%20We%20present%20a%20procedure%20which%2C%20for%20any%20given%20%24q%20%5Cin%20%280%2C1%5D%24%2C%20transforms%0Aany%20standard%20stochastic%20first-order%20method%20to%20an%20asynchronous%20method%20with%0Aconvergence%20guarantee%20depending%20on%20the%20%24q%24-quantile%20delay%20of%20the%20sequence.%20This%0Aapproach%20leads%20to%20convergence%20rates%20of%20the%20form%20%24O%28%5Ctau_q/qT%2B%5Csigma/%5Csqrt%7BqT%7D%29%24%0Afor%20non-convex%20and%20%24O%28%5Ctau_q%5E2/%28q%20T%29%5E2%2B%5Csigma/%5Csqrt%7BqT%7D%29%24%20for%20convex%20smooth%0Aproblems%2C%20where%20%24%5Ctau_q%24%20is%20the%20%24q%24-quantile%20delay%2C%20generalizing%20and%20improving%0Aon%20existing%20results%20that%20depend%20on%20the%20average%20delay.%20We%20further%20show%20a%20method%0Athat%20automatically%20adapts%20to%20all%20quantiles%20simultaneously%2C%20without%20any%20prior%0Aknowledge%20of%20the%20delays%2C%20achieving%20convergence%20rates%20of%20the%20form%20%24O%28%5Cinf_%7Bq%7D%0A%5Ctau_q/qT%2B%5Csigma/%5Csqrt%7BqT%7D%29%24%20for%20non-convex%20and%20%24O%28%5Cinf_%7Bq%7D%20%5Ctau_q%5E2/%28q%0AT%29%5E2%2B%5Csigma/%5Csqrt%7BqT%7D%29%24%20for%20convex%20smooth%20problems.%20Our%20technique%20is%20based%20on%0Aasynchronous%20mini-batching%20with%20a%20careful%20batch-size%20selection%20and%20filtering%20of%0Astale%20gradients.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07503v1&entry.124074799=Read"},
{"title": "QirK: Question Answering via Intermediate Representation on Knowledge\n  Graphs", "author": "Jan Luca Scheerer and Anton Lykov and Moe Kayali and Ilias Fountalis and Dan Olteanu and Nikolaos Vasiloglou and Dan Suciu", "abstract": "  We demonstrate QirK, a system for answering natural language questions on\nKnowledge Graphs (KG). QirK can answer structurally complex questions that are\nstill beyond the reach of emerging Large Language Models (LLMs). It does so\nusing a unique combination of database technology, LLMs, and semantic search\nover vector embeddings. The glue for these components is an intermediate\nrepresentation (IR). The input question is mapped to IR using LLMs, which is\nthen repaired into a valid relational database query with the aid of a semantic\nsearch on vector embeddings. This allows a practical synthesis of LLM\ncapabilities and KG reliability.\n  A short video demonstrating QirK is available at\nhttps://youtu.be/6c81BLmOZ0U.\n", "link": "http://arxiv.org/abs/2408.07494v1", "date": "2024-08-14", "relevancy": 1.297, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4646}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.4298}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4064}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20QirK%3A%20Question%20Answering%20via%20Intermediate%20Representation%20on%20Knowledge%0A%20%20Graphs&body=Title%3A%20QirK%3A%20Question%20Answering%20via%20Intermediate%20Representation%20on%20Knowledge%0A%20%20Graphs%0AAuthor%3A%20Jan%20Luca%20Scheerer%20and%20Anton%20Lykov%20and%20Moe%20Kayali%20and%20Ilias%20Fountalis%20and%20Dan%20Olteanu%20and%20Nikolaos%20Vasiloglou%20and%20Dan%20Suciu%0AAbstract%3A%20%20%20We%20demonstrate%20QirK%2C%20a%20system%20for%20answering%20natural%20language%20questions%20on%0AKnowledge%20Graphs%20%28KG%29.%20QirK%20can%20answer%20structurally%20complex%20questions%20that%20are%0Astill%20beyond%20the%20reach%20of%20emerging%20Large%20Language%20Models%20%28LLMs%29.%20It%20does%20so%0Ausing%20a%20unique%20combination%20of%20database%20technology%2C%20LLMs%2C%20and%20semantic%20search%0Aover%20vector%20embeddings.%20The%20glue%20for%20these%20components%20is%20an%20intermediate%0Arepresentation%20%28IR%29.%20The%20input%20question%20is%20mapped%20to%20IR%20using%20LLMs%2C%20which%20is%0Athen%20repaired%20into%20a%20valid%20relational%20database%20query%20with%20the%20aid%20of%20a%20semantic%0Asearch%20on%20vector%20embeddings.%20This%20allows%20a%20practical%20synthesis%20of%20LLM%0Acapabilities%20and%20KG%20reliability.%0A%20%20A%20short%20video%20demonstrating%20QirK%20is%20available%20at%0Ahttps%3A//youtu.be/6c81BLmOZ0U.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07494v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DQirK%253A%2520Question%2520Answering%2520via%2520Intermediate%2520Representation%2520on%2520Knowledge%250A%2520%2520Graphs%26entry.906535625%3DJan%2520Luca%2520Scheerer%2520and%2520Anton%2520Lykov%2520and%2520Moe%2520Kayali%2520and%2520Ilias%2520Fountalis%2520and%2520Dan%2520Olteanu%2520and%2520Nikolaos%2520Vasiloglou%2520and%2520Dan%2520Suciu%26entry.1292438233%3D%2520%2520We%2520demonstrate%2520QirK%252C%2520a%2520system%2520for%2520answering%2520natural%2520language%2520questions%2520on%250AKnowledge%2520Graphs%2520%2528KG%2529.%2520QirK%2520can%2520answer%2520structurally%2520complex%2520questions%2520that%2520are%250Astill%2520beyond%2520the%2520reach%2520of%2520emerging%2520Large%2520Language%2520Models%2520%2528LLMs%2529.%2520It%2520does%2520so%250Ausing%2520a%2520unique%2520combination%2520of%2520database%2520technology%252C%2520LLMs%252C%2520and%2520semantic%2520search%250Aover%2520vector%2520embeddings.%2520The%2520glue%2520for%2520these%2520components%2520is%2520an%2520intermediate%250Arepresentation%2520%2528IR%2529.%2520The%2520input%2520question%2520is%2520mapped%2520to%2520IR%2520using%2520LLMs%252C%2520which%2520is%250Athen%2520repaired%2520into%2520a%2520valid%2520relational%2520database%2520query%2520with%2520the%2520aid%2520of%2520a%2520semantic%250Asearch%2520on%2520vector%2520embeddings.%2520This%2520allows%2520a%2520practical%2520synthesis%2520of%2520LLM%250Acapabilities%2520and%2520KG%2520reliability.%250A%2520%2520A%2520short%2520video%2520demonstrating%2520QirK%2520is%2520available%2520at%250Ahttps%253A//youtu.be/6c81BLmOZ0U.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07494v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QirK%3A%20Question%20Answering%20via%20Intermediate%20Representation%20on%20Knowledge%0A%20%20Graphs&entry.906535625=Jan%20Luca%20Scheerer%20and%20Anton%20Lykov%20and%20Moe%20Kayali%20and%20Ilias%20Fountalis%20and%20Dan%20Olteanu%20and%20Nikolaos%20Vasiloglou%20and%20Dan%20Suciu&entry.1292438233=%20%20We%20demonstrate%20QirK%2C%20a%20system%20for%20answering%20natural%20language%20questions%20on%0AKnowledge%20Graphs%20%28KG%29.%20QirK%20can%20answer%20structurally%20complex%20questions%20that%20are%0Astill%20beyond%20the%20reach%20of%20emerging%20Large%20Language%20Models%20%28LLMs%29.%20It%20does%20so%0Ausing%20a%20unique%20combination%20of%20database%20technology%2C%20LLMs%2C%20and%20semantic%20search%0Aover%20vector%20embeddings.%20The%20glue%20for%20these%20components%20is%20an%20intermediate%0Arepresentation%20%28IR%29.%20The%20input%20question%20is%20mapped%20to%20IR%20using%20LLMs%2C%20which%20is%0Athen%20repaired%20into%20a%20valid%20relational%20database%20query%20with%20the%20aid%20of%20a%20semantic%0Asearch%20on%20vector%20embeddings.%20This%20allows%20a%20practical%20synthesis%20of%20LLM%0Acapabilities%20and%20KG%20reliability.%0A%20%20A%20short%20video%20demonstrating%20QirK%20is%20available%20at%0Ahttps%3A//youtu.be/6c81BLmOZ0U.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07494v1&entry.124074799=Read"},
{"title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories,\n  Applications and Opportunities", "author": "Enneng Yang and Li Shen and Guibing Guo and Xingwei Wang and Xiaochun Cao and Jie Zhang and Dacheng Tao", "abstract": "  Model merging is an efficient empowerment technique in the machine learning\ncommunity that does not require the collection of raw training data and does\nnot require expensive computation. As model merging becomes increasingly\nprevalent across various fields, it is crucial to understand the available\nmodel merging techniques comprehensively. However, there is a significant gap\nin the literature regarding a systematic and thorough review of these\ntechniques. This survey provides a comprehensive overview of model merging\nmethods and theories, their applications in various domains and settings, and\nfuture research directions. Specifically, we first propose a new taxonomic\napproach that exhaustively discusses existing model merging methods. Secondly,\nwe discuss the application of model merging techniques in large language\nmodels, multimodal large language models, and 10+ machine learning subfields,\nincluding continual learning, multi-task learning, few-shot learning, etc.\nFinally, we highlight the remaining challenges of model merging and discuss\nfuture research directions. A comprehensive list of papers about model merging\nis available at\n\\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.\n", "link": "http://arxiv.org/abs/2408.07666v1", "date": "2024-08-14", "relevancy": 1.4522, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5011}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4717}, {"title": "Multimodal-Conditioned Latent Diffusion Models for Fashion Image Editing", "link": "http://arxiv.org/abs/2403.14828v2", "similarity": 0.4541}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Model%20Merging%20in%20LLMs%2C%20MLLMs%2C%20and%20Beyond%3A%20Methods%2C%20Theories%2C%0A%20%20Applications%20and%20Opportunities&body=Title%3A%20Model%20Merging%20in%20LLMs%2C%20MLLMs%2C%20and%20Beyond%3A%20Methods%2C%20Theories%2C%0A%20%20Applications%20and%20Opportunities%0AAuthor%3A%20Enneng%20Yang%20and%20Li%20Shen%20and%20Guibing%20Guo%20and%20Xingwei%20Wang%20and%20Xiaochun%20Cao%20and%20Jie%20Zhang%20and%20Dacheng%20Tao%0AAbstract%3A%20%20%20Model%20merging%20is%20an%20efficient%20empowerment%20technique%20in%20the%20machine%20learning%0Acommunity%20that%20does%20not%20require%20the%20collection%20of%20raw%20training%20data%20and%20does%0Anot%20require%20expensive%20computation.%20As%20model%20merging%20becomes%20increasingly%0Aprevalent%20across%20various%20fields%2C%20it%20is%20crucial%20to%20understand%20the%20available%0Amodel%20merging%20techniques%20comprehensively.%20However%2C%20there%20is%20a%20significant%20gap%0Ain%20the%20literature%20regarding%20a%20systematic%20and%20thorough%20review%20of%20these%0Atechniques.%20This%20survey%20provides%20a%20comprehensive%20overview%20of%20model%20merging%0Amethods%20and%20theories%2C%20their%20applications%20in%20various%20domains%20and%20settings%2C%20and%0Afuture%20research%20directions.%20Specifically%2C%20we%20first%20propose%20a%20new%20taxonomic%0Aapproach%20that%20exhaustively%20discusses%20existing%20model%20merging%20methods.%20Secondly%2C%0Awe%20discuss%20the%20application%20of%20model%20merging%20techniques%20in%20large%20language%0Amodels%2C%20multimodal%20large%20language%20models%2C%20and%2010%2B%20machine%20learning%20subfields%2C%0Aincluding%20continual%20learning%2C%20multi-task%20learning%2C%20few-shot%20learning%2C%20etc.%0AFinally%2C%20we%20highlight%20the%20remaining%20challenges%20of%20model%20merging%20and%20discuss%0Afuture%20research%20directions.%20A%20comprehensive%20list%20of%20papers%20about%20model%20merging%0Ais%20available%20at%0A%5Curl%7Bhttps%3A//github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications%7D.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07666v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DModel%2520Merging%2520in%2520LLMs%252C%2520MLLMs%252C%2520and%2520Beyond%253A%2520Methods%252C%2520Theories%252C%250A%2520%2520Applications%2520and%2520Opportunities%26entry.906535625%3DEnneng%2520Yang%2520and%2520Li%2520Shen%2520and%2520Guibing%2520Guo%2520and%2520Xingwei%2520Wang%2520and%2520Xiaochun%2520Cao%2520and%2520Jie%2520Zhang%2520and%2520Dacheng%2520Tao%26entry.1292438233%3D%2520%2520Model%2520merging%2520is%2520an%2520efficient%2520empowerment%2520technique%2520in%2520the%2520machine%2520learning%250Acommunity%2520that%2520does%2520not%2520require%2520the%2520collection%2520of%2520raw%2520training%2520data%2520and%2520does%250Anot%2520require%2520expensive%2520computation.%2520As%2520model%2520merging%2520becomes%2520increasingly%250Aprevalent%2520across%2520various%2520fields%252C%2520it%2520is%2520crucial%2520to%2520understand%2520the%2520available%250Amodel%2520merging%2520techniques%2520comprehensively.%2520However%252C%2520there%2520is%2520a%2520significant%2520gap%250Ain%2520the%2520literature%2520regarding%2520a%2520systematic%2520and%2520thorough%2520review%2520of%2520these%250Atechniques.%2520This%2520survey%2520provides%2520a%2520comprehensive%2520overview%2520of%2520model%2520merging%250Amethods%2520and%2520theories%252C%2520their%2520applications%2520in%2520various%2520domains%2520and%2520settings%252C%2520and%250Afuture%2520research%2520directions.%2520Specifically%252C%2520we%2520first%2520propose%2520a%2520new%2520taxonomic%250Aapproach%2520that%2520exhaustively%2520discusses%2520existing%2520model%2520merging%2520methods.%2520Secondly%252C%250Awe%2520discuss%2520the%2520application%2520of%2520model%2520merging%2520techniques%2520in%2520large%2520language%250Amodels%252C%2520multimodal%2520large%2520language%2520models%252C%2520and%252010%252B%2520machine%2520learning%2520subfields%252C%250Aincluding%2520continual%2520learning%252C%2520multi-task%2520learning%252C%2520few-shot%2520learning%252C%2520etc.%250AFinally%252C%2520we%2520highlight%2520the%2520remaining%2520challenges%2520of%2520model%2520merging%2520and%2520discuss%250Afuture%2520research%2520directions.%2520A%2520comprehensive%2520list%2520of%2520papers%2520about%2520model%2520merging%250Ais%2520available%2520at%250A%255Curl%257Bhttps%253A//github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications%257D.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07666v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Model%20Merging%20in%20LLMs%2C%20MLLMs%2C%20and%20Beyond%3A%20Methods%2C%20Theories%2C%0A%20%20Applications%20and%20Opportunities&entry.906535625=Enneng%20Yang%20and%20Li%20Shen%20and%20Guibing%20Guo%20and%20Xingwei%20Wang%20and%20Xiaochun%20Cao%20and%20Jie%20Zhang%20and%20Dacheng%20Tao&entry.1292438233=%20%20Model%20merging%20is%20an%20efficient%20empowerment%20technique%20in%20the%20machine%20learning%0Acommunity%20that%20does%20not%20require%20the%20collection%20of%20raw%20training%20data%20and%20does%0Anot%20require%20expensive%20computation.%20As%20model%20merging%20becomes%20increasingly%0Aprevalent%20across%20various%20fields%2C%20it%20is%20crucial%20to%20understand%20the%20available%0Amodel%20merging%20techniques%20comprehensively.%20However%2C%20there%20is%20a%20significant%20gap%0Ain%20the%20literature%20regarding%20a%20systematic%20and%20thorough%20review%20of%20these%0Atechniques.%20This%20survey%20provides%20a%20comprehensive%20overview%20of%20model%20merging%0Amethods%20and%20theories%2C%20their%20applications%20in%20various%20domains%20and%20settings%2C%20and%0Afuture%20research%20directions.%20Specifically%2C%20we%20first%20propose%20a%20new%20taxonomic%0Aapproach%20that%20exhaustively%20discusses%20existing%20model%20merging%20methods.%20Secondly%2C%0Awe%20discuss%20the%20application%20of%20model%20merging%20techniques%20in%20large%20language%0Amodels%2C%20multimodal%20large%20language%20models%2C%20and%2010%2B%20machine%20learning%20subfields%2C%0Aincluding%20continual%20learning%2C%20multi-task%20learning%2C%20few-shot%20learning%2C%20etc.%0AFinally%2C%20we%20highlight%20the%20remaining%20challenges%20of%20model%20merging%20and%20discuss%0Afuture%20research%20directions.%20A%20comprehensive%20list%20of%20papers%20about%20model%20merging%0Ais%20available%20at%0A%5Curl%7Bhttps%3A//github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications%7D.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07666v1&entry.124074799=Read"},
{"title": "Sonic: Fast and Transferable Data Poisoning on Clustering Algorithms", "author": "Francesco Villani and Dario Lazzaro and Antonio Emanuele Cin\u00e0 and Matteo Dell'Amico and Battista Biggio and Fabio Roli", "abstract": "  Data poisoning attacks on clustering algorithms have received limited\nattention, with existing methods struggling to scale efficiently as dataset\nsizes and feature counts increase. These attacks typically require\nre-clustering the entire dataset multiple times to generate predictions and\nassess the attacker's objectives, significantly hindering their scalability.\nThis paper addresses these limitations by proposing Sonic, a novel genetic data\npoisoning attack that leverages incremental and scalable clustering algorithms,\ne.g., FISHDBC, as surrogates to accelerate poisoning attacks against\ngraph-based and density-based clustering methods, such as HDBSCAN. We\nempirically demonstrate the effectiveness and efficiency of Sonic in poisoning\nthe target clustering algorithms. We then conduct a comprehensive analysis of\nthe factors affecting the scalability and transferability of poisoning attacks\nagainst clustering algorithms, and we conclude by examining the robustness of\nhyperparameters in our attack strategy Sonic.\n", "link": "http://arxiv.org/abs/2408.07558v1", "date": "2024-08-14", "relevancy": 1.6323, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4206}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4023}, {"title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions", "link": "http://arxiv.org/abs/2403.16627v1", "similarity": 0.3911}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Sonic%3A%20Fast%20and%20Transferable%20Data%20Poisoning%20on%20Clustering%20Algorithms&body=Title%3A%20Sonic%3A%20Fast%20and%20Transferable%20Data%20Poisoning%20on%20Clustering%20Algorithms%0AAuthor%3A%20Francesco%20Villani%20and%20Dario%20Lazzaro%20and%20Antonio%20Emanuele%20Cin%C3%A0%20and%20Matteo%20Dell%27Amico%20and%20Battista%20Biggio%20and%20Fabio%20Roli%0AAbstract%3A%20%20%20Data%20poisoning%20attacks%20on%20clustering%20algorithms%20have%20received%20limited%0Aattention%2C%20with%20existing%20methods%20struggling%20to%20scale%20efficiently%20as%20dataset%0Asizes%20and%20feature%20counts%20increase.%20These%20attacks%20typically%20require%0Are-clustering%20the%20entire%20dataset%20multiple%20times%20to%20generate%20predictions%20and%0Aassess%20the%20attacker%27s%20objectives%2C%20significantly%20hindering%20their%20scalability.%0AThis%20paper%20addresses%20these%20limitations%20by%20proposing%20Sonic%2C%20a%20novel%20genetic%20data%0Apoisoning%20attack%20that%20leverages%20incremental%20and%20scalable%20clustering%20algorithms%2C%0Ae.g.%2C%20FISHDBC%2C%20as%20surrogates%20to%20accelerate%20poisoning%20attacks%20against%0Agraph-based%20and%20density-based%20clustering%20methods%2C%20such%20as%20HDBSCAN.%20We%0Aempirically%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20Sonic%20in%20poisoning%0Athe%20target%20clustering%20algorithms.%20We%20then%20conduct%20a%20comprehensive%20analysis%20of%0Athe%20factors%20affecting%20the%20scalability%20and%20transferability%20of%20poisoning%20attacks%0Aagainst%20clustering%20algorithms%2C%20and%20we%20conclude%20by%20examining%20the%20robustness%20of%0Ahyperparameters%20in%20our%20attack%20strategy%20Sonic.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.07558v1%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DSonic%253A%2520Fast%2520and%2520Transferable%2520Data%2520Poisoning%2520on%2520Clustering%2520Algorithms%26entry.906535625%3DFrancesco%2520Villani%2520and%2520Dario%2520Lazzaro%2520and%2520Antonio%2520Emanuele%2520Cin%25C3%25A0%2520and%2520Matteo%2520Dell%2527Amico%2520and%2520Battista%2520Biggio%2520and%2520Fabio%2520Roli%26entry.1292438233%3D%2520%2520Data%2520poisoning%2520attacks%2520on%2520clustering%2520algorithms%2520have%2520received%2520limited%250Aattention%252C%2520with%2520existing%2520methods%2520struggling%2520to%2520scale%2520efficiently%2520as%2520dataset%250Asizes%2520and%2520feature%2520counts%2520increase.%2520These%2520attacks%2520typically%2520require%250Are-clustering%2520the%2520entire%2520dataset%2520multiple%2520times%2520to%2520generate%2520predictions%2520and%250Aassess%2520the%2520attacker%2527s%2520objectives%252C%2520significantly%2520hindering%2520their%2520scalability.%250AThis%2520paper%2520addresses%2520these%2520limitations%2520by%2520proposing%2520Sonic%252C%2520a%2520novel%2520genetic%2520data%250Apoisoning%2520attack%2520that%2520leverages%2520incremental%2520and%2520scalable%2520clustering%2520algorithms%252C%250Ae.g.%252C%2520FISHDBC%252C%2520as%2520surrogates%2520to%2520accelerate%2520poisoning%2520attacks%2520against%250Agraph-based%2520and%2520density-based%2520clustering%2520methods%252C%2520such%2520as%2520HDBSCAN.%2520We%250Aempirically%2520demonstrate%2520the%2520effectiveness%2520and%2520efficiency%2520of%2520Sonic%2520in%2520poisoning%250Athe%2520target%2520clustering%2520algorithms.%2520We%2520then%2520conduct%2520a%2520comprehensive%2520analysis%2520of%250Athe%2520factors%2520affecting%2520the%2520scalability%2520and%2520transferability%2520of%2520poisoning%2520attacks%250Aagainst%2520clustering%2520algorithms%252C%2520and%2520we%2520conclude%2520by%2520examining%2520the%2520robustness%2520of%250Ahyperparameters%2520in%2520our%2520attack%2520strategy%2520Sonic.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.07558v1%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Sonic%3A%20Fast%20and%20Transferable%20Data%20Poisoning%20on%20Clustering%20Algorithms&entry.906535625=Francesco%20Villani%20and%20Dario%20Lazzaro%20and%20Antonio%20Emanuele%20Cin%C3%A0%20and%20Matteo%20Dell%27Amico%20and%20Battista%20Biggio%20and%20Fabio%20Roli&entry.1292438233=%20%20Data%20poisoning%20attacks%20on%20clustering%20algorithms%20have%20received%20limited%0Aattention%2C%20with%20existing%20methods%20struggling%20to%20scale%20efficiently%20as%20dataset%0Asizes%20and%20feature%20counts%20increase.%20These%20attacks%20typically%20require%0Are-clustering%20the%20entire%20dataset%20multiple%20times%20to%20generate%20predictions%20and%0Aassess%20the%20attacker%27s%20objectives%2C%20significantly%20hindering%20their%20scalability.%0AThis%20paper%20addresses%20these%20limitations%20by%20proposing%20Sonic%2C%20a%20novel%20genetic%20data%0Apoisoning%20attack%20that%20leverages%20incremental%20and%20scalable%20clustering%20algorithms%2C%0Ae.g.%2C%20FISHDBC%2C%20as%20surrogates%20to%20accelerate%20poisoning%20attacks%20against%0Agraph-based%20and%20density-based%20clustering%20methods%2C%20such%20as%20HDBSCAN.%20We%0Aempirically%20demonstrate%20the%20effectiveness%20and%20efficiency%20of%20Sonic%20in%20poisoning%0Athe%20target%20clustering%20algorithms.%20We%20then%20conduct%20a%20comprehensive%20analysis%20of%0Athe%20factors%20affecting%20the%20scalability%20and%20transferability%20of%20poisoning%20attacks%0Aagainst%20clustering%20algorithms%2C%20and%20we%20conclude%20by%20examining%20the%20robustness%20of%0Ahyperparameters%20in%20our%20attack%20strategy%20Sonic.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.07558v1&entry.124074799=Read"},
{"title": "Robust Curve Detection in Volumetric Medical Imaging via Attraction\n  Field", "author": "Farukh Yaushev and Daria Nogina and Valentin Samokhin and Mariya Dugova and Ekaterina Petrash and Dmitry Sevryukov and Mikhail Belyaev and Maxim Pisov", "abstract": "  Understanding body part geometry is crucial for precise medical diagnostics.\nCurves effectively describe anatomical structures and are widely used in\nmedical imaging applications related to cardiovascular, respiratory, and\nskeletal diseases. Traditional curve detection methods are often task-specific,\nrelying heavily on domain-specific features, limiting their broader\napplicability. This paper introduces a novel approach for detecting\nnon-branching curves, which does not require prior knowledge of the object's\norientation, shape, or position. Our method uses neural networks to predict (1)\nan attraction field, which offers subpixel accuracy, and (2) a closeness map,\nwhich limits the region of interest and essentially eliminates outliers far\nfrom the desired curve. We tested our curve detector on several clinically\nrelevant tasks with diverse morphologies and achieved impressive subpixel-level\naccuracy results that surpass existing methods, highlighting its versatility\nand robustness. Additionally, to support further advancements in this field, we\nprovide our private annotations of aortic centerlines and masks, which can\nserve as a benchmark for future research. The dataset can be found at\nhttps://github.com/neuro-ml/curve-detection.\n", "link": "http://arxiv.org/abs/2408.01159v2", "date": "2024-08-14", "relevancy": 1.4516, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4917}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4851}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4729}], "mailto": "mailto:haoxiang.li.2024@gmail.com?subject=%5BarXrec%5D%20Robust%20Curve%20Detection%20in%20Volumetric%20Medical%20Imaging%20via%20Attraction%0A%20%20Field&body=Title%3A%20Robust%20Curve%20Detection%20in%20Volumetric%20Medical%20Imaging%20via%20Attraction%0A%20%20Field%0AAuthor%3A%20Farukh%20Yaushev%20and%20Daria%20Nogina%20and%20Valentin%20Samokhin%20and%20Mariya%20Dugova%20and%20Ekaterina%20Petrash%20and%20Dmitry%20Sevryukov%20and%20Mikhail%20Belyaev%20and%20Maxim%20Pisov%0AAbstract%3A%20%20%20Understanding%20body%20part%20geometry%20is%20crucial%20for%20precise%20medical%20diagnostics.%0ACurves%20effectively%20describe%20anatomical%20structures%20and%20are%20widely%20used%20in%0Amedical%20imaging%20applications%20related%20to%20cardiovascular%2C%20respiratory%2C%20and%0Askeletal%20diseases.%20Traditional%20curve%20detection%20methods%20are%20often%20task-specific%2C%0Arelying%20heavily%20on%20domain-specific%20features%2C%20limiting%20their%20broader%0Aapplicability.%20This%20paper%20introduces%20a%20novel%20approach%20for%20detecting%0Anon-branching%20curves%2C%20which%20does%20not%20require%20prior%20knowledge%20of%20the%20object%27s%0Aorientation%2C%20shape%2C%20or%20position.%20Our%20method%20uses%20neural%20networks%20to%20predict%20%281%29%0Aan%20attraction%20field%2C%20which%20offers%20subpixel%20accuracy%2C%20and%20%282%29%20a%20closeness%20map%2C%0Awhich%20limits%20the%20region%20of%20interest%20and%20essentially%20eliminates%20outliers%20far%0Afrom%20the%20desired%20curve.%20We%20tested%20our%20curve%20detector%20on%20several%20clinically%0Arelevant%20tasks%20with%20diverse%20morphologies%20and%20achieved%20impressive%20subpixel-level%0Aaccuracy%20results%20that%20surpass%20existing%20methods%2C%20highlighting%20its%20versatility%0Aand%20robustness.%20Additionally%2C%20to%20support%20further%20advancements%20in%20this%20field%2C%20we%0Aprovide%20our%20private%20annotations%20of%20aortic%20centerlines%20and%20masks%2C%20which%20can%0Aserve%20as%20a%20benchmark%20for%20future%20research.%20The%20dataset%20can%20be%20found%20at%0Ahttps%3A//github.com/neuro-ml/curve-detection.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2408.01159v2%0AForm%3A%20https%3A//docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform%3Fentry.1380929848%3DRobust%2520Curve%2520Detection%2520in%2520Volumetric%2520Medical%2520Imaging%2520via%2520Attraction%250A%2520%2520Field%26entry.906535625%3DFarukh%2520Yaushev%2520and%2520Daria%2520Nogina%2520and%2520Valentin%2520Samokhin%2520and%2520Mariya%2520Dugova%2520and%2520Ekaterina%2520Petrash%2520and%2520Dmitry%2520Sevryukov%2520and%2520Mikhail%2520Belyaev%2520and%2520Maxim%2520Pisov%26entry.1292438233%3D%2520%2520Understanding%2520body%2520part%2520geometry%2520is%2520crucial%2520for%2520precise%2520medical%2520diagnostics.%250ACurves%2520effectively%2520describe%2520anatomical%2520structures%2520and%2520are%2520widely%2520used%2520in%250Amedical%2520imaging%2520applications%2520related%2520to%2520cardiovascular%252C%2520respiratory%252C%2520and%250Askeletal%2520diseases.%2520Traditional%2520curve%2520detection%2520methods%2520are%2520often%2520task-specific%252C%250Arelying%2520heavily%2520on%2520domain-specific%2520features%252C%2520limiting%2520their%2520broader%250Aapplicability.%2520This%2520paper%2520introduces%2520a%2520novel%2520approach%2520for%2520detecting%250Anon-branching%2520curves%252C%2520which%2520does%2520not%2520require%2520prior%2520knowledge%2520of%2520the%2520object%2527s%250Aorientation%252C%2520shape%252C%2520or%2520position.%2520Our%2520method%2520uses%2520neural%2520networks%2520to%2520predict%2520%25281%2529%250Aan%2520attraction%2520field%252C%2520which%2520offers%2520subpixel%2520accuracy%252C%2520and%2520%25282%2529%2520a%2520closeness%2520map%252C%250Awhich%2520limits%2520the%2520region%2520of%2520interest%2520and%2520essentially%2520eliminates%2520outliers%2520far%250Afrom%2520the%2520desired%2520curve.%2520We%2520tested%2520our%2520curve%2520detector%2520on%2520several%2520clinically%250Arelevant%2520tasks%2520with%2520diverse%2520morphologies%2520and%2520achieved%2520impressive%2520subpixel-level%250Aaccuracy%2520results%2520that%2520surpass%2520existing%2520methods%252C%2520highlighting%2520its%2520versatility%250Aand%2520robustness.%2520Additionally%252C%2520to%2520support%2520further%2520advancements%2520in%2520this%2520field%252C%2520we%250Aprovide%2520our%2520private%2520annotations%2520of%2520aortic%2520centerlines%2520and%2520masks%252C%2520which%2520can%250Aserve%2520as%2520a%2520benchmark%2520for%2520future%2520research.%2520The%2520dataset%2520can%2520be%2520found%2520at%250Ahttps%253A//github.com/neuro-ml/curve-detection.%250A%26entry.1838667208%3Dhttp%253A//arxiv.org/abs/2408.01159v2%26entry.124074799%3DRead", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Robust%20Curve%20Detection%20in%20Volumetric%20Medical%20Imaging%20via%20Attraction%0A%20%20Field&entry.906535625=Farukh%20Yaushev%20and%20Daria%20Nogina%20and%20Valentin%20Samokhin%20and%20Mariya%20Dugova%20and%20Ekaterina%20Petrash%20and%20Dmitry%20Sevryukov%20and%20Mikhail%20Belyaev%20and%20Maxim%20Pisov&entry.1292438233=%20%20Understanding%20body%20part%20geometry%20is%20crucial%20for%20precise%20medical%20diagnostics.%0ACurves%20effectively%20describe%20anatomical%20structures%20and%20are%20widely%20used%20in%0Amedical%20imaging%20applications%20related%20to%20cardiovascular%2C%20respiratory%2C%20and%0Askeletal%20diseases.%20Traditional%20curve%20detection%20methods%20are%20often%20task-specific%2C%0Arelying%20heavily%20on%20domain-specific%20features%2C%20limiting%20their%20broader%0Aapplicability.%20This%20paper%20introduces%20a%20novel%20approach%20for%20detecting%0Anon-branching%20curves%2C%20which%20does%20not%20require%20prior%20knowledge%20of%20the%20object%27s%0Aorientation%2C%20shape%2C%20or%20position.%20Our%20method%20uses%20neural%20networks%20to%20predict%20%281%29%0Aan%20attraction%20field%2C%20which%20offers%20subpixel%20accuracy%2C%20and%20%282%29%20a%20closeness%20map%2C%0Awhich%20limits%20the%20region%20of%20interest%20and%20essentially%20eliminates%20outliers%20far%0Afrom%20the%20desired%20curve.%20We%20tested%20our%20curve%20detector%20on%20several%20clinically%0Arelevant%20tasks%20with%20diverse%20morphologies%20and%20achieved%20impressive%20subpixel-level%0Aaccuracy%20results%20that%20surpass%20existing%20methods%2C%20highlighting%20its%20versatility%0Aand%20robustness.%20Additionally%2C%20to%20support%20further%20advancements%20in%20this%20field%2C%20we%0Aprovide%20our%20private%20annotations%20of%20aortic%20centerlines%20and%20masks%2C%20which%20can%0Aserve%20as%20a%20benchmark%20for%20future%20research.%20The%20dataset%20can%20be%20found%20at%0Ahttps%3A//github.com/neuro-ml/curve-detection.%0A&entry.1838667208=http%3A//arxiv.org/abs/2408.01159v2&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


