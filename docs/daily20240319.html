<!DOCTYPE html>
<html lang="en">
  <head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-V34CNNDP8V"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-V34CNNDP8V');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arxiv Paper Selection</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css" crossorigin="anonymous" referrerpolicy="no-referrer" />
    <style>
    body {
      font-family: Arial, sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
    }
    header {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      background-color: #ffffff;
      color: black;
      padding: 10px;
      text-align: center;
      z-index: 1000;
      border-bottom: 1px solid #ddd;
    }
    header div {
      display: block;
      margin: 10px auto;
    }

    #home-icon {
      display: block;
      float: left;
      margin: 5px;
      text-decoration: none;
      color: black;
    }

    main {
      margin-top: 60px; /* Adjusted margin to account for fixed header */
      padding: 20px;
    }

    .post {
      background-color: white;
      border: 1px solid #ddd;
      border-radius: 5px;
      margin-bottom: 10px;
      padding: 10px 20px;
      max-height: 2000px;
      overflow: scroll;
    }
    .post img {
      display: block;
      margin-top: 5px;
      max-width: auto;
      max-height: 100px;
    }
    .post .clear {
      clear: both;
      display: block;
    }
    .post a {
      text-decoration: none;
    }
    .post a:hover {
      color: #0056b3;
    }
    .post a:visited {
      color: #0056b3;
    }
    .post div.comment {
      text-align: right;
    }
    .post div.comment a {
      margin: 1em;
    }
    .post .text {
      margin: 1em 0em;
      padding: 0;
    }
    .post .text .title {
    }
    .post .text .author {
    }
    .post .text .abstract {
    }
    .post .topK {
      display: block;
      margin: 0.5em;
    }
    .post .date {
      margin: 0;
      padding: 0;
      text-size: small; 
      color: gray;
    }
    .post .link {
      margin: 0;
      padding: 0;
    }
    @media screen and (max-width: 600px) {
      body {
        max-width: 100%; 
      }
      #home-icon {
        float: none;
        display: block;
        text-align: center;
        margin-bottom: 10px;
      }
    }
    footer {
      width: 100%;
      background-color: #ddd;
      text-align: center;
      z-index: 1000;
      padding: 20px 0px;
      margin-bottom: 20px;
      left: 0;
    }

    #next-btn,
    #prev-btn {
      background-color: #4CAF50;
      color: white;
      padding: 8px 16px;
      margin: 0 50px;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }

    .links {
      padding: 20px;
    }
    .links a {
      text-decoration: none;
    }
    .links a:hover {
      color: #0056b3;
    }
    .links a:visited {
      color: #0056b3;
    }

    #page-index {
      font-size: small;
    }
    .ads {
      width: 100%;
    }

    </style>
  </head>
  <body>

    <header>
      <a href="https://haoxiang.org/">About</a>
    </header>

    <main id="content">
      <!-- Posts will be dynamically added here using JavaScript -->
    </main>

    <script>
      // Dummy data for posts
      const posts = [
{"title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images", "author": "Ruyi Xu and Yuan Yao and Zonghao Guo and Junbo Cui and Zanlin Ni and Chunjiang Ge and Tat-Seng Chua and Zhiyuan Liu and Maosong Sun and Gao Huang", "abstract": "  Visual encoding constitutes the basis of large multimodal models (LMMs) in\nunderstanding the visual world. Conventional LMMs process images in fixed sizes\nand limited resolutions, while recent explorations in this direction are\nlimited in adaptivity, efficiency, and even correctness. In this work, we first\ntake GPT-4V and LLaVA-1.5 as representative examples and expose systematic\nflaws rooted in their visual encoding strategy. To address the challenges, we\npresent LLaVA-UHD, a large multimodal model that can efficiently perceive\nimages in any aspect ratio and high resolution. LLaVA-UHD includes three key\ncomponents: (1) An image modularization strategy that divides native-resolution\nimages into smaller variable-sized slices for efficient and extensible\nencoding, (2) a compression module that further condenses image tokens from\nvisual encoders, and (3) a spatial schema to organize slice tokens for LLMs.\nComprehensive experiments show that LLaVA-UHD outperforms established LMMs\ntrained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our\nmodel built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)\nresolution images using only 94% inference computation, and achieves 6.4\naccuracy improvement on TextVQA. Moreover, the model can be efficiently trained\nin academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of\nLLaVA-1.5). We make the data and code publicly available at\nhttps://github.com/thunlp/LLaVA-UHD.\n", "link": "http://arxiv.org/abs/2403.11703v1", "date": "2024-03-18", "relevancy": 2.8189, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5767}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5593}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5554}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LLaVA-UHD%3A%20an%20LMM%20Perceiving%20Any%20Aspect%20Ratio%20and%20High-Resolution%20Images&body=Title%3A%20LLaVA-UHD%3A%20an%20LMM%20Perceiving%20Any%20Aspect%20Ratio%20and%20High-Resolution%20Images%0AAuthor%3A%20Ruyi%20Xu%20and%20Yuan%20Yao%20and%20Zonghao%20Guo%20and%20Junbo%20Cui%20and%20Zanlin%20Ni%20and%20Chunjiang%20Ge%20and%20Tat-Seng%20Chua%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun%20and%20Gao%20Huang%0AAbstract%3A%20%20%20Visual%20encoding%20constitutes%20the%20basis%20of%20large%20multimodal%20models%20%28LMMs%29%20in%0Aunderstanding%20the%20visual%20world.%20Conventional%20LMMs%20process%20images%20in%20fixed%20sizes%0Aand%20limited%20resolutions%2C%20while%20recent%20explorations%20in%20this%20direction%20are%0Alimited%20in%20adaptivity%2C%20efficiency%2C%20and%20even%20correctness.%20In%20this%20work%2C%20we%20first%0Atake%20GPT-4V%20and%20LLaVA-1.5%20as%20representative%20examples%20and%20expose%20systematic%0Aflaws%20rooted%20in%20their%20visual%20encoding%20strategy.%20To%20address%20the%20challenges%2C%20we%0Apresent%20LLaVA-UHD%2C%20a%20large%20multimodal%20model%20that%20can%20efficiently%20perceive%0Aimages%20in%20any%20aspect%20ratio%20and%20high%20resolution.%20LLaVA-UHD%20includes%20three%20key%0Acomponents%3A%20%281%29%20An%20image%20modularization%20strategy%20that%20divides%20native-resolution%0Aimages%20into%20smaller%20variable-sized%20slices%20for%20efficient%20and%20extensible%0Aencoding%2C%20%282%29%20a%20compression%20module%20that%20further%20condenses%20image%20tokens%20from%0Avisual%20encoders%2C%20and%20%283%29%20a%20spatial%20schema%20to%20organize%20slice%20tokens%20for%20LLMs.%0AComprehensive%20experiments%20show%20that%20LLaVA-UHD%20outperforms%20established%20LMMs%0Atrained%20with%202-3%20orders%20of%20magnitude%20more%20data%20on%209%20benchmarks.%20Notably%2C%20our%0Amodel%20built%20on%20LLaVA-1.5%20336x336%20supports%206%20times%20larger%20%28i.e.%2C%20672x1088%29%0Aresolution%20images%20using%20only%2094%25%20inference%20computation%2C%20and%20achieves%206.4%0Aaccuracy%20improvement%20on%20TextVQA.%20Moreover%2C%20the%20model%20can%20be%20efficiently%20trained%0Ain%20academic%20settings%2C%20within%2023%20hours%20on%208%20A100%20GPUs%20%28vs.%2026%20hours%20of%0ALLaVA-1.5%29.%20We%20make%20the%20data%20and%20code%20publicly%20available%20at%0Ahttps%3A//github.com/thunlp/LLaVA-UHD.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11703v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LLaVA-UHD%3A%20an%20LMM%20Perceiving%20Any%20Aspect%20Ratio%20and%20High-Resolution%20Images&entry.906535625=Ruyi%20Xu%20and%20Yuan%20Yao%20and%20Zonghao%20Guo%20and%20Junbo%20Cui%20and%20Zanlin%20Ni%20and%20Chunjiang%20Ge%20and%20Tat-Seng%20Chua%20and%20Zhiyuan%20Liu%20and%20Maosong%20Sun%20and%20Gao%20Huang&entry.1292438233=%20%20Visual%20encoding%20constitutes%20the%20basis%20of%20large%20multimodal%20models%20%28LMMs%29%20in%0Aunderstanding%20the%20visual%20world.%20Conventional%20LMMs%20process%20images%20in%20fixed%20sizes%0Aand%20limited%20resolutions%2C%20while%20recent%20explorations%20in%20this%20direction%20are%0Alimited%20in%20adaptivity%2C%20efficiency%2C%20and%20even%20correctness.%20In%20this%20work%2C%20we%20first%0Atake%20GPT-4V%20and%20LLaVA-1.5%20as%20representative%20examples%20and%20expose%20systematic%0Aflaws%20rooted%20in%20their%20visual%20encoding%20strategy.%20To%20address%20the%20challenges%2C%20we%0Apresent%20LLaVA-UHD%2C%20a%20large%20multimodal%20model%20that%20can%20efficiently%20perceive%0Aimages%20in%20any%20aspect%20ratio%20and%20high%20resolution.%20LLaVA-UHD%20includes%20three%20key%0Acomponents%3A%20%281%29%20An%20image%20modularization%20strategy%20that%20divides%20native-resolution%0Aimages%20into%20smaller%20variable-sized%20slices%20for%20efficient%20and%20extensible%0Aencoding%2C%20%282%29%20a%20compression%20module%20that%20further%20condenses%20image%20tokens%20from%0Avisual%20encoders%2C%20and%20%283%29%20a%20spatial%20schema%20to%20organize%20slice%20tokens%20for%20LLMs.%0AComprehensive%20experiments%20show%20that%20LLaVA-UHD%20outperforms%20established%20LMMs%0Atrained%20with%202-3%20orders%20of%20magnitude%20more%20data%20on%209%20benchmarks.%20Notably%2C%20our%0Amodel%20built%20on%20LLaVA-1.5%20336x336%20supports%206%20times%20larger%20%28i.e.%2C%20672x1088%29%0Aresolution%20images%20using%20only%2094%25%20inference%20computation%2C%20and%20achieves%206.4%0Aaccuracy%20improvement%20on%20TextVQA.%20Moreover%2C%20the%20model%20can%20be%20efficiently%20trained%0Ain%20academic%20settings%2C%20within%2023%20hours%20on%208%20A100%20GPUs%20%28vs.%2026%20hours%20of%0ALLaVA-1.5%29.%20We%20make%20the%20data%20and%20code%20publicly%20available%20at%0Ahttps%3A//github.com/thunlp/LLaVA-UHD.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11703v1&entry.124074799=Read"},
{"title": "Implicit Discriminative Knowledge Learning for Visible-Infrared Person\n  Re-Identification", "author": "Kaijie Ren and Lei Zhang", "abstract": "  Visible-Infrared Person Re-identification (VI-ReID) is a challenging\ncross-modal pedestrian retrieval task, due to significant intra-class\nvariations and cross-modal discrepancies among different cameras. Existing\nworks mainly focus on embedding images of different modalities into a unified\nspace to mine modality-shared features. They only seek distinctive information\nwithin these shared features, while ignoring the identity-aware useful\ninformation that is implicit in the modality-specific features. To address this\nissue, we propose a novel Implicit Discriminative Knowledge Learning (IDKL)\nnetwork to uncover and leverage the implicit discriminative information\ncontained within the modality-specific. First, we extract modality-specific and\nmodality-shared features using a novel dual-stream network. Then, the\nmodality-specific features undergo purification to reduce their modality style\ndiscrepancies while preserving identity-aware discriminative knowledge.\nSubsequently, this kind of implicit knowledge is distilled into the\nmodality-shared feature to enhance its distinctiveness. Finally, an alignment\nloss is proposed to minimize modality discrepancy on enhanced modality-shared\nfeatures. Extensive experiments on multiple public datasets demonstrate the\nsuperiority of IDKL network over the state-of-the-art methods. Code is\navailable at https://github.com/1KK077/IDKL.\n", "link": "http://arxiv.org/abs/2403.11708v1", "date": "2024-03-18", "relevancy": 2.7909, "topK": [{"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5822}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5673}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.525}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Implicit%20Discriminative%20Knowledge%20Learning%20for%20Visible-Infrared%20Person%0A%20%20Re-Identification&body=Title%3A%20Implicit%20Discriminative%20Knowledge%20Learning%20for%20Visible-Infrared%20Person%0A%20%20Re-Identification%0AAuthor%3A%20Kaijie%20Ren%20and%20Lei%20Zhang%0AAbstract%3A%20%20%20Visible-Infrared%20Person%20Re-identification%20%28VI-ReID%29%20is%20a%20challenging%0Across-modal%20pedestrian%20retrieval%20task%2C%20due%20to%20significant%20intra-class%0Avariations%20and%20cross-modal%20discrepancies%20among%20different%20cameras.%20Existing%0Aworks%20mainly%20focus%20on%20embedding%20images%20of%20different%20modalities%20into%20a%20unified%0Aspace%20to%20mine%20modality-shared%20features.%20They%20only%20seek%20distinctive%20information%0Awithin%20these%20shared%20features%2C%20while%20ignoring%20the%20identity-aware%20useful%0Ainformation%20that%20is%20implicit%20in%20the%20modality-specific%20features.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20novel%20Implicit%20Discriminative%20Knowledge%20Learning%20%28IDKL%29%0Anetwork%20to%20uncover%20and%20leverage%20the%20implicit%20discriminative%20information%0Acontained%20within%20the%20modality-specific.%20First%2C%20we%20extract%20modality-specific%20and%0Amodality-shared%20features%20using%20a%20novel%20dual-stream%20network.%20Then%2C%20the%0Amodality-specific%20features%20undergo%20purification%20to%20reduce%20their%20modality%20style%0Adiscrepancies%20while%20preserving%20identity-aware%20discriminative%20knowledge.%0ASubsequently%2C%20this%20kind%20of%20implicit%20knowledge%20is%20distilled%20into%20the%0Amodality-shared%20feature%20to%20enhance%20its%20distinctiveness.%20Finally%2C%20an%20alignment%0Aloss%20is%20proposed%20to%20minimize%20modality%20discrepancy%20on%20enhanced%20modality-shared%0Afeatures.%20Extensive%20experiments%20on%20multiple%20public%20datasets%20demonstrate%20the%0Asuperiority%20of%20IDKL%20network%20over%20the%20state-of-the-art%20methods.%20Code%20is%0Aavailable%20at%20https%3A//github.com/1KK077/IDKL.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11708v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Implicit%20Discriminative%20Knowledge%20Learning%20for%20Visible-Infrared%20Person%0A%20%20Re-Identification&entry.906535625=Kaijie%20Ren%20and%20Lei%20Zhang&entry.1292438233=%20%20Visible-Infrared%20Person%20Re-identification%20%28VI-ReID%29%20is%20a%20challenging%0Across-modal%20pedestrian%20retrieval%20task%2C%20due%20to%20significant%20intra-class%0Avariations%20and%20cross-modal%20discrepancies%20among%20different%20cameras.%20Existing%0Aworks%20mainly%20focus%20on%20embedding%20images%20of%20different%20modalities%20into%20a%20unified%0Aspace%20to%20mine%20modality-shared%20features.%20They%20only%20seek%20distinctive%20information%0Awithin%20these%20shared%20features%2C%20while%20ignoring%20the%20identity-aware%20useful%0Ainformation%20that%20is%20implicit%20in%20the%20modality-specific%20features.%20To%20address%20this%0Aissue%2C%20we%20propose%20a%20novel%20Implicit%20Discriminative%20Knowledge%20Learning%20%28IDKL%29%0Anetwork%20to%20uncover%20and%20leverage%20the%20implicit%20discriminative%20information%0Acontained%20within%20the%20modality-specific.%20First%2C%20we%20extract%20modality-specific%20and%0Amodality-shared%20features%20using%20a%20novel%20dual-stream%20network.%20Then%2C%20the%0Amodality-specific%20features%20undergo%20purification%20to%20reduce%20their%20modality%20style%0Adiscrepancies%20while%20preserving%20identity-aware%20discriminative%20knowledge.%0ASubsequently%2C%20this%20kind%20of%20implicit%20knowledge%20is%20distilled%20into%20the%0Amodality-shared%20feature%20to%20enhance%20its%20distinctiveness.%20Finally%2C%20an%20alignment%0Aloss%20is%20proposed%20to%20minimize%20modality%20discrepancy%20on%20enhanced%20modality-shared%0Afeatures.%20Extensive%20experiments%20on%20multiple%20public%20datasets%20demonstrate%20the%0Asuperiority%20of%20IDKL%20network%20over%20the%20state-of-the-art%20methods.%20Code%20is%0Aavailable%20at%20https%3A//github.com/1KK077/IDKL.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11708v1&entry.124074799=Read"},
{"title": "An Accurate and Real-time Relative Pose Estimation from Triple\n  Point-line Images by Decoupling Rotation and Translation", "author": "Zewen Xu and Yijia He and Hao Wei and Bo Xu and BinJian Xie and Yihong Wu", "abstract": "  Line features are valid complements for point features in man-made\nenvironments. 3D-2D constraints provided by line features have been widely used\nin Visual Odometry (VO) and Structure-from-Motion (SfM) systems. However, how\nto accurately solve three-view relative motion only with 2D observations of\npoints and lines in real time has not been fully explored. In this paper, we\npropose a novel three-view pose solver based on rotation-translation decoupled\nestimation. First, a high-precision rotation estimation method based on normal\nvector coplanarity constraints that consider the uncertainty of observations is\nproposed, which can be solved by Levenberg-Marquardt (LM) algorithm\nefficiently. Second, a robust linear translation constraint that minimizes the\ndegree of the rotation components and feature observation components in\nequations is elaborately designed for estimating translations accurately.\nExperiments on synthetic data and real-world data show that the proposed\napproach improves both rotation and translation accuracy compared to the\nclassical trifocal-tensor-based method and the state-of-the-art two-view\nalgorithm in outdoor and indoor environments.\n", "link": "http://arxiv.org/abs/2403.11639v1", "date": "2024-03-18", "relevancy": 2.7606, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5921}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5377}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5265}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20Accurate%20and%20Real-time%20Relative%20Pose%20Estimation%20from%20Triple%0A%20%20Point-line%20Images%20by%20Decoupling%20Rotation%20and%20Translation&body=Title%3A%20An%20Accurate%20and%20Real-time%20Relative%20Pose%20Estimation%20from%20Triple%0A%20%20Point-line%20Images%20by%20Decoupling%20Rotation%20and%20Translation%0AAuthor%3A%20Zewen%20Xu%20and%20Yijia%20He%20and%20Hao%20Wei%20and%20Bo%20Xu%20and%20BinJian%20Xie%20and%20Yihong%20Wu%0AAbstract%3A%20%20%20Line%20features%20are%20valid%20complements%20for%20point%20features%20in%20man-made%0Aenvironments.%203D-2D%20constraints%20provided%20by%20line%20features%20have%20been%20widely%20used%0Ain%20Visual%20Odometry%20%28VO%29%20and%20Structure-from-Motion%20%28SfM%29%20systems.%20However%2C%20how%0Ato%20accurately%20solve%20three-view%20relative%20motion%20only%20with%202D%20observations%20of%0Apoints%20and%20lines%20in%20real%20time%20has%20not%20been%20fully%20explored.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20three-view%20pose%20solver%20based%20on%20rotation-translation%20decoupled%0Aestimation.%20First%2C%20a%20high-precision%20rotation%20estimation%20method%20based%20on%20normal%0Avector%20coplanarity%20constraints%20that%20consider%20the%20uncertainty%20of%20observations%20is%0Aproposed%2C%20which%20can%20be%20solved%20by%20Levenberg-Marquardt%20%28LM%29%20algorithm%0Aefficiently.%20Second%2C%20a%20robust%20linear%20translation%20constraint%20that%20minimizes%20the%0Adegree%20of%20the%20rotation%20components%20and%20feature%20observation%20components%20in%0Aequations%20is%20elaborately%20designed%20for%20estimating%20translations%20accurately.%0AExperiments%20on%20synthetic%20data%20and%20real-world%20data%20show%20that%20the%20proposed%0Aapproach%20improves%20both%20rotation%20and%20translation%20accuracy%20compared%20to%20the%0Aclassical%20trifocal-tensor-based%20method%20and%20the%20state-of-the-art%20two-view%0Aalgorithm%20in%20outdoor%20and%20indoor%20environments.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11639v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Accurate%20and%20Real-time%20Relative%20Pose%20Estimation%20from%20Triple%0A%20%20Point-line%20Images%20by%20Decoupling%20Rotation%20and%20Translation&entry.906535625=Zewen%20Xu%20and%20Yijia%20He%20and%20Hao%20Wei%20and%20Bo%20Xu%20and%20BinJian%20Xie%20and%20Yihong%20Wu&entry.1292438233=%20%20Line%20features%20are%20valid%20complements%20for%20point%20features%20in%20man-made%0Aenvironments.%203D-2D%20constraints%20provided%20by%20line%20features%20have%20been%20widely%20used%0Ain%20Visual%20Odometry%20%28VO%29%20and%20Structure-from-Motion%20%28SfM%29%20systems.%20However%2C%20how%0Ato%20accurately%20solve%20three-view%20relative%20motion%20only%20with%202D%20observations%20of%0Apoints%20and%20lines%20in%20real%20time%20has%20not%20been%20fully%20explored.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20three-view%20pose%20solver%20based%20on%20rotation-translation%20decoupled%0Aestimation.%20First%2C%20a%20high-precision%20rotation%20estimation%20method%20based%20on%20normal%0Avector%20coplanarity%20constraints%20that%20consider%20the%20uncertainty%20of%20observations%20is%0Aproposed%2C%20which%20can%20be%20solved%20by%20Levenberg-Marquardt%20%28LM%29%20algorithm%0Aefficiently.%20Second%2C%20a%20robust%20linear%20translation%20constraint%20that%20minimizes%20the%0Adegree%20of%20the%20rotation%20components%20and%20feature%20observation%20components%20in%0Aequations%20is%20elaborately%20designed%20for%20estimating%20translations%20accurately.%0AExperiments%20on%20synthetic%20data%20and%20real-world%20data%20show%20that%20the%20proposed%0Aapproach%20improves%20both%20rotation%20and%20translation%20accuracy%20compared%20to%20the%0Aclassical%20trifocal-tensor-based%20method%20and%20the%20state-of-the-art%20two-view%0Aalgorithm%20in%20outdoor%20and%20indoor%20environments.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11639v1&entry.124074799=Read"},
{"title": "FE-DeTr: Keypoint Detection and Tracking in Low-quality Image Frames\n  with Events", "author": "Xiangyuan Wang and Kuangyi Chen and Wen Yang and Lei Yu and Yannan Xing and Huai Yu", "abstract": "  Keypoint detection and tracking in traditional image frames are often\ncompromised by image quality issues such as motion blur and extreme lighting\nconditions. Event cameras offer potential solutions to these challenges by\nvirtue of their high temporal resolution and high dynamic range. However, they\nhave limited performance in practical applications due to their inherent noise\nin event data. This paper advocates fusing the complementary information from\nimage frames and event streams to achieve more robust keypoint detection and\ntracking. Specifically, we propose a novel keypoint detection network that\nfuses the textural and structural information from image frames with the\nhigh-temporal-resolution motion information from event streams, namely FE-DeTr.\nThe network leverages a temporal response consistency for supervision, ensuring\nstable and efficient keypoint detection. Moreover, we use a spatio-temporal\nnearest-neighbor search strategy for robust keypoint tracking. Extensive\nexperiments are conducted on a new dataset featuring both image frames and\nevent data captured under extreme conditions. The experimental results confirm\nthe superior performance of our method over both existing frame-based and\nevent-based methods.\n", "link": "http://arxiv.org/abs/2403.11662v1", "date": "2024-03-18", "relevancy": 2.7498, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5771}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5379}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5349}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20FE-DeTr%3A%20Keypoint%20Detection%20and%20Tracking%20in%20Low-quality%20Image%20Frames%0A%20%20with%20Events&body=Title%3A%20FE-DeTr%3A%20Keypoint%20Detection%20and%20Tracking%20in%20Low-quality%20Image%20Frames%0A%20%20with%20Events%0AAuthor%3A%20Xiangyuan%20Wang%20and%20Kuangyi%20Chen%20and%20Wen%20Yang%20and%20Lei%20Yu%20and%20Yannan%20Xing%20and%20Huai%20Yu%0AAbstract%3A%20%20%20Keypoint%20detection%20and%20tracking%20in%20traditional%20image%20frames%20are%20often%0Acompromised%20by%20image%20quality%20issues%20such%20as%20motion%20blur%20and%20extreme%20lighting%0Aconditions.%20Event%20cameras%20offer%20potential%20solutions%20to%20these%20challenges%20by%0Avirtue%20of%20their%20high%20temporal%20resolution%20and%20high%20dynamic%20range.%20However%2C%20they%0Ahave%20limited%20performance%20in%20practical%20applications%20due%20to%20their%20inherent%20noise%0Ain%20event%20data.%20This%20paper%20advocates%20fusing%20the%20complementary%20information%20from%0Aimage%20frames%20and%20event%20streams%20to%20achieve%20more%20robust%20keypoint%20detection%20and%0Atracking.%20Specifically%2C%20we%20propose%20a%20novel%20keypoint%20detection%20network%20that%0Afuses%20the%20textural%20and%20structural%20information%20from%20image%20frames%20with%20the%0Ahigh-temporal-resolution%20motion%20information%20from%20event%20streams%2C%20namely%20FE-DeTr.%0AThe%20network%20leverages%20a%20temporal%20response%20consistency%20for%20supervision%2C%20ensuring%0Astable%20and%20efficient%20keypoint%20detection.%20Moreover%2C%20we%20use%20a%20spatio-temporal%0Anearest-neighbor%20search%20strategy%20for%20robust%20keypoint%20tracking.%20Extensive%0Aexperiments%20are%20conducted%20on%20a%20new%20dataset%20featuring%20both%20image%20frames%20and%0Aevent%20data%20captured%20under%20extreme%20conditions.%20The%20experimental%20results%20confirm%0Athe%20superior%20performance%20of%20our%20method%20over%20both%20existing%20frame-based%20and%0Aevent-based%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11662v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=FE-DeTr%3A%20Keypoint%20Detection%20and%20Tracking%20in%20Low-quality%20Image%20Frames%0A%20%20with%20Events&entry.906535625=Xiangyuan%20Wang%20and%20Kuangyi%20Chen%20and%20Wen%20Yang%20and%20Lei%20Yu%20and%20Yannan%20Xing%20and%20Huai%20Yu&entry.1292438233=%20%20Keypoint%20detection%20and%20tracking%20in%20traditional%20image%20frames%20are%20often%0Acompromised%20by%20image%20quality%20issues%20such%20as%20motion%20blur%20and%20extreme%20lighting%0Aconditions.%20Event%20cameras%20offer%20potential%20solutions%20to%20these%20challenges%20by%0Avirtue%20of%20their%20high%20temporal%20resolution%20and%20high%20dynamic%20range.%20However%2C%20they%0Ahave%20limited%20performance%20in%20practical%20applications%20due%20to%20their%20inherent%20noise%0Ain%20event%20data.%20This%20paper%20advocates%20fusing%20the%20complementary%20information%20from%0Aimage%20frames%20and%20event%20streams%20to%20achieve%20more%20robust%20keypoint%20detection%20and%0Atracking.%20Specifically%2C%20we%20propose%20a%20novel%20keypoint%20detection%20network%20that%0Afuses%20the%20textural%20and%20structural%20information%20from%20image%20frames%20with%20the%0Ahigh-temporal-resolution%20motion%20information%20from%20event%20streams%2C%20namely%20FE-DeTr.%0AThe%20network%20leverages%20a%20temporal%20response%20consistency%20for%20supervision%2C%20ensuring%0Astable%20and%20efficient%20keypoint%20detection.%20Moreover%2C%20we%20use%20a%20spatio-temporal%0Anearest-neighbor%20search%20strategy%20for%20robust%20keypoint%20tracking.%20Extensive%0Aexperiments%20are%20conducted%20on%20a%20new%20dataset%20featuring%20both%20image%20frames%20and%0Aevent%20data%20captured%20under%20extreme%20conditions.%20The%20experimental%20results%20confirm%0Athe%20superior%20performance%20of%20our%20method%20over%20both%20existing%20frame-based%20and%0Aevent-based%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11662v1&entry.124074799=Read"},
{"title": "A Spatial-Temporal Progressive Fusion Network for Breast Lesion\n  Segmentation in Ultrasound Videos", "author": "Zhengzheng Tu and Zigang Zhu and Yayang Duan and Bo Jiang and Qishun Wang and Chaoxue Zhang", "abstract": "  Ultrasound video-based breast lesion segmentation provides a valuable\nassistance in early breast lesion detection and treatment. However, existing\nworks mainly focus on lesion segmentation based on ultrasound breast images\nwhich usually can not be adapted well to obtain desirable results on ultrasound\nvideos. The main challenge for ultrasound video-based breast lesion\nsegmentation is how to exploit the lesion cues of both intra-frame and\ninter-frame simultaneously. To address this problem, we propose a novel\nSpatial-Temporal Progressive Fusion Network (STPFNet) for video based breast\nlesion segmentation problem. The main aspects of the proposed STPFNet are\nthreefold. First, we propose to adopt a unified network architecture to capture\nboth spatial dependences within each ultrasound frame and temporal correlations\nbetween different frames together for ultrasound data representation. Second,\nwe propose a new fusion module, termed Multi-Scale Feature Fusion (MSFF), to\nfuse spatial and temporal cues together for lesion detection. MSFF can help to\ndetermine the boundary contour of lesion region to overcome the issue of lesion\nboundary blurring. Third, we propose to exploit the segmentation result of\nprevious frame as the prior knowledge to suppress the noisy background and\nlearn more robust representation. In particular, we introduce a new publicly\navailable ultrasound video breast lesion segmentation dataset, termed UVBLS200,\nwhich is specifically dedicated to breast lesion segmentation. It contains 200\nvideos, including 80 videos of benign lesions and 120 videos of malignant\nlesions. Experiments on the proposed dataset demonstrate that the proposed\nSTPFNet achieves better breast lesion detection performance than\nstate-of-the-art methods.\n", "link": "http://arxiv.org/abs/2403.11699v1", "date": "2024-03-18", "relevancy": 2.5582, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5247}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5093}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5009}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Spatial-Temporal%20Progressive%20Fusion%20Network%20for%20Breast%20Lesion%0A%20%20Segmentation%20in%20Ultrasound%20Videos&body=Title%3A%20A%20Spatial-Temporal%20Progressive%20Fusion%20Network%20for%20Breast%20Lesion%0A%20%20Segmentation%20in%20Ultrasound%20Videos%0AAuthor%3A%20Zhengzheng%20Tu%20and%20Zigang%20Zhu%20and%20Yayang%20Duan%20and%20Bo%20Jiang%20and%20Qishun%20Wang%20and%20Chaoxue%20Zhang%0AAbstract%3A%20%20%20Ultrasound%20video-based%20breast%20lesion%20segmentation%20provides%20a%20valuable%0Aassistance%20in%20early%20breast%20lesion%20detection%20and%20treatment.%20However%2C%20existing%0Aworks%20mainly%20focus%20on%20lesion%20segmentation%20based%20on%20ultrasound%20breast%20images%0Awhich%20usually%20can%20not%20be%20adapted%20well%20to%20obtain%20desirable%20results%20on%20ultrasound%0Avideos.%20The%20main%20challenge%20for%20ultrasound%20video-based%20breast%20lesion%0Asegmentation%20is%20how%20to%20exploit%20the%20lesion%20cues%20of%20both%20intra-frame%20and%0Ainter-frame%20simultaneously.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%0ASpatial-Temporal%20Progressive%20Fusion%20Network%20%28STPFNet%29%20for%20video%20based%20breast%0Alesion%20segmentation%20problem.%20The%20main%20aspects%20of%20the%20proposed%20STPFNet%20are%0Athreefold.%20First%2C%20we%20propose%20to%20adopt%20a%20unified%20network%20architecture%20to%20capture%0Aboth%20spatial%20dependences%20within%20each%20ultrasound%20frame%20and%20temporal%20correlations%0Abetween%20different%20frames%20together%20for%20ultrasound%20data%20representation.%20Second%2C%0Awe%20propose%20a%20new%20fusion%20module%2C%20termed%20Multi-Scale%20Feature%20Fusion%20%28MSFF%29%2C%20to%0Afuse%20spatial%20and%20temporal%20cues%20together%20for%20lesion%20detection.%20MSFF%20can%20help%20to%0Adetermine%20the%20boundary%20contour%20of%20lesion%20region%20to%20overcome%20the%20issue%20of%20lesion%0Aboundary%20blurring.%20Third%2C%20we%20propose%20to%20exploit%20the%20segmentation%20result%20of%0Aprevious%20frame%20as%20the%20prior%20knowledge%20to%20suppress%20the%20noisy%20background%20and%0Alearn%20more%20robust%20representation.%20In%20particular%2C%20we%20introduce%20a%20new%20publicly%0Aavailable%20ultrasound%20video%20breast%20lesion%20segmentation%20dataset%2C%20termed%20UVBLS200%2C%0Awhich%20is%20specifically%20dedicated%20to%20breast%20lesion%20segmentation.%20It%20contains%20200%0Avideos%2C%20including%2080%20videos%20of%20benign%20lesions%20and%20120%20videos%20of%20malignant%0Alesions.%20Experiments%20on%20the%20proposed%20dataset%20demonstrate%20that%20the%20proposed%0ASTPFNet%20achieves%20better%20breast%20lesion%20detection%20performance%20than%0Astate-of-the-art%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11699v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Spatial-Temporal%20Progressive%20Fusion%20Network%20for%20Breast%20Lesion%0A%20%20Segmentation%20in%20Ultrasound%20Videos&entry.906535625=Zhengzheng%20Tu%20and%20Zigang%20Zhu%20and%20Yayang%20Duan%20and%20Bo%20Jiang%20and%20Qishun%20Wang%20and%20Chaoxue%20Zhang&entry.1292438233=%20%20Ultrasound%20video-based%20breast%20lesion%20segmentation%20provides%20a%20valuable%0Aassistance%20in%20early%20breast%20lesion%20detection%20and%20treatment.%20However%2C%20existing%0Aworks%20mainly%20focus%20on%20lesion%20segmentation%20based%20on%20ultrasound%20breast%20images%0Awhich%20usually%20can%20not%20be%20adapted%20well%20to%20obtain%20desirable%20results%20on%20ultrasound%0Avideos.%20The%20main%20challenge%20for%20ultrasound%20video-based%20breast%20lesion%0Asegmentation%20is%20how%20to%20exploit%20the%20lesion%20cues%20of%20both%20intra-frame%20and%0Ainter-frame%20simultaneously.%20To%20address%20this%20problem%2C%20we%20propose%20a%20novel%0ASpatial-Temporal%20Progressive%20Fusion%20Network%20%28STPFNet%29%20for%20video%20based%20breast%0Alesion%20segmentation%20problem.%20The%20main%20aspects%20of%20the%20proposed%20STPFNet%20are%0Athreefold.%20First%2C%20we%20propose%20to%20adopt%20a%20unified%20network%20architecture%20to%20capture%0Aboth%20spatial%20dependences%20within%20each%20ultrasound%20frame%20and%20temporal%20correlations%0Abetween%20different%20frames%20together%20for%20ultrasound%20data%20representation.%20Second%2C%0Awe%20propose%20a%20new%20fusion%20module%2C%20termed%20Multi-Scale%20Feature%20Fusion%20%28MSFF%29%2C%20to%0Afuse%20spatial%20and%20temporal%20cues%20together%20for%20lesion%20detection.%20MSFF%20can%20help%20to%0Adetermine%20the%20boundary%20contour%20of%20lesion%20region%20to%20overcome%20the%20issue%20of%20lesion%0Aboundary%20blurring.%20Third%2C%20we%20propose%20to%20exploit%20the%20segmentation%20result%20of%0Aprevious%20frame%20as%20the%20prior%20knowledge%20to%20suppress%20the%20noisy%20background%20and%0Alearn%20more%20robust%20representation.%20In%20particular%2C%20we%20introduce%20a%20new%20publicly%0Aavailable%20ultrasound%20video%20breast%20lesion%20segmentation%20dataset%2C%20termed%20UVBLS200%2C%0Awhich%20is%20specifically%20dedicated%20to%20breast%20lesion%20segmentation.%20It%20contains%20200%0Avideos%2C%20including%2080%20videos%20of%20benign%20lesions%20and%20120%20videos%20of%20malignant%0Alesions.%20Experiments%20on%20the%20proposed%20dataset%20demonstrate%20that%20the%20proposed%0ASTPFNet%20achieves%20better%20breast%20lesion%20detection%20performance%20than%0Astate-of-the-art%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11699v1&entry.124074799=Read"},
{"title": "Unconstrained Stochastic CCA: Unifying Multiview and Self-Supervised\n  Learning", "author": "James Chapman and Lennie Wells and Ana Lawry Aguila", "abstract": "  The Canonical Correlation Analysis (CCA) family of methods is foundational in\nmultiview learning. Regularised linear CCA methods can be seen to generalise\nPartial Least Squares (PLS) and be unified with a Generalized Eigenvalue\nProblem (GEP) framework. However, classical algorithms for these linear methods\nare computationally infeasible for large-scale data. Extensions to Deep CCA\nshow great promise, but current training procedures are slow and complicated.\nFirst we propose a novel unconstrained objective that characterizes the top\nsubspace of GEPs. Our core contribution is a family of fast algorithms for\nstochastic PLS, stochastic CCA, and Deep CCA, simply obtained by applying\nstochastic gradient descent (SGD) to the corresponding CCA objectives. Our\nalgorithms show far faster convergence and recover higher correlations than the\nprevious state-of-the-art on all standard CCA and Deep CCA benchmarks. These\nimprovements allow us to perform a first-of-its-kind PLS analysis of an\nextremely large biomedical dataset from the UK Biobank, with over 33,000\nindividuals and 500,000 features. Finally, we apply our algorithms to match the\nperformance of `CCA-family' Self-Supervised Learning (SSL) methods on CIFAR-10\nand CIFAR-100 with minimal hyper-parameter tuning, and also present theory to\nclarify the links between these methods and classical CCA, laying the\ngroundwork for future insights.\n", "link": "http://arxiv.org/abs/2310.01012v3", "date": "2024-03-18", "relevancy": 2.5516, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5358}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.5003}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4949}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unconstrained%20Stochastic%20CCA%3A%20Unifying%20Multiview%20and%20Self-Supervised%0A%20%20Learning&body=Title%3A%20Unconstrained%20Stochastic%20CCA%3A%20Unifying%20Multiview%20and%20Self-Supervised%0A%20%20Learning%0AAuthor%3A%20James%20Chapman%20and%20Lennie%20Wells%20and%20Ana%20Lawry%20Aguila%0AAbstract%3A%20%20%20The%20Canonical%20Correlation%20Analysis%20%28CCA%29%20family%20of%20methods%20is%20foundational%20in%0Amultiview%20learning.%20Regularised%20linear%20CCA%20methods%20can%20be%20seen%20to%20generalise%0APartial%20Least%20Squares%20%28PLS%29%20and%20be%20unified%20with%20a%20Generalized%20Eigenvalue%0AProblem%20%28GEP%29%20framework.%20However%2C%20classical%20algorithms%20for%20these%20linear%20methods%0Aare%20computationally%20infeasible%20for%20large-scale%20data.%20Extensions%20to%20Deep%20CCA%0Ashow%20great%20promise%2C%20but%20current%20training%20procedures%20are%20slow%20and%20complicated.%0AFirst%20we%20propose%20a%20novel%20unconstrained%20objective%20that%20characterizes%20the%20top%0Asubspace%20of%20GEPs.%20Our%20core%20contribution%20is%20a%20family%20of%20fast%20algorithms%20for%0Astochastic%20PLS%2C%20stochastic%20CCA%2C%20and%20Deep%20CCA%2C%20simply%20obtained%20by%20applying%0Astochastic%20gradient%20descent%20%28SGD%29%20to%20the%20corresponding%20CCA%20objectives.%20Our%0Aalgorithms%20show%20far%20faster%20convergence%20and%20recover%20higher%20correlations%20than%20the%0Aprevious%20state-of-the-art%20on%20all%20standard%20CCA%20and%20Deep%20CCA%20benchmarks.%20These%0Aimprovements%20allow%20us%20to%20perform%20a%20first-of-its-kind%20PLS%20analysis%20of%20an%0Aextremely%20large%20biomedical%20dataset%20from%20the%20UK%20Biobank%2C%20with%20over%2033%2C000%0Aindividuals%20and%20500%2C000%20features.%20Finally%2C%20we%20apply%20our%20algorithms%20to%20match%20the%0Aperformance%20of%20%60CCA-family%27%20Self-Supervised%20Learning%20%28SSL%29%20methods%20on%20CIFAR-10%0Aand%20CIFAR-100%20with%20minimal%20hyper-parameter%20tuning%2C%20and%20also%20present%20theory%20to%0Aclarify%20the%20links%20between%20these%20methods%20and%20classical%20CCA%2C%20laying%20the%0Agroundwork%20for%20future%20insights.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2310.01012v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unconstrained%20Stochastic%20CCA%3A%20Unifying%20Multiview%20and%20Self-Supervised%0A%20%20Learning&entry.906535625=James%20Chapman%20and%20Lennie%20Wells%20and%20Ana%20Lawry%20Aguila&entry.1292438233=%20%20The%20Canonical%20Correlation%20Analysis%20%28CCA%29%20family%20of%20methods%20is%20foundational%20in%0Amultiview%20learning.%20Regularised%20linear%20CCA%20methods%20can%20be%20seen%20to%20generalise%0APartial%20Least%20Squares%20%28PLS%29%20and%20be%20unified%20with%20a%20Generalized%20Eigenvalue%0AProblem%20%28GEP%29%20framework.%20However%2C%20classical%20algorithms%20for%20these%20linear%20methods%0Aare%20computationally%20infeasible%20for%20large-scale%20data.%20Extensions%20to%20Deep%20CCA%0Ashow%20great%20promise%2C%20but%20current%20training%20procedures%20are%20slow%20and%20complicated.%0AFirst%20we%20propose%20a%20novel%20unconstrained%20objective%20that%20characterizes%20the%20top%0Asubspace%20of%20GEPs.%20Our%20core%20contribution%20is%20a%20family%20of%20fast%20algorithms%20for%0Astochastic%20PLS%2C%20stochastic%20CCA%2C%20and%20Deep%20CCA%2C%20simply%20obtained%20by%20applying%0Astochastic%20gradient%20descent%20%28SGD%29%20to%20the%20corresponding%20CCA%20objectives.%20Our%0Aalgorithms%20show%20far%20faster%20convergence%20and%20recover%20higher%20correlations%20than%20the%0Aprevious%20state-of-the-art%20on%20all%20standard%20CCA%20and%20Deep%20CCA%20benchmarks.%20These%0Aimprovements%20allow%20us%20to%20perform%20a%20first-of-its-kind%20PLS%20analysis%20of%20an%0Aextremely%20large%20biomedical%20dataset%20from%20the%20UK%20Biobank%2C%20with%20over%2033%2C000%0Aindividuals%20and%20500%2C000%20features.%20Finally%2C%20we%20apply%20our%20algorithms%20to%20match%20the%0Aperformance%20of%20%60CCA-family%27%20Self-Supervised%20Learning%20%28SSL%29%20methods%20on%20CIFAR-10%0Aand%20CIFAR-100%20with%20minimal%20hyper-parameter%20tuning%2C%20and%20also%20present%20theory%20to%0Aclarify%20the%20links%20between%20these%20methods%20and%20classical%20CCA%2C%20laying%20the%0Agroundwork%20for%20future%20insights.%0A&entry.1838667208=http%3A//arxiv.org/abs/2310.01012v3&entry.124074799=Read"},
{"title": "QEAN: Quaternion-Enhanced Attention Network for Visual Dance Generation", "author": "Zhizhen Zhou and Yejing Huo and Guoheng Huang and An Zeng and Xuhang Chen and Lian Huang and Zinuo Li", "abstract": "  The study of music-generated dance is a novel and challenging Image\ngeneration task. It aims to input a piece of music and seed motions, then\ngenerate natural dance movements for the subsequent music. Transformer-based\nmethods face challenges in time series prediction tasks related to human\nmovements and music due to their struggle in capturing the nonlinear\nrelationship and temporal aspects. This can lead to issues like joint\ndeformation, role deviation, floating, and inconsistencies in dance movements\ngenerated in response to the music. In this paper, we propose a\nQuaternion-Enhanced Attention Network (QEAN) for visual dance synthesis from a\nquaternion perspective, which consists of a Spin Position Embedding (SPE)\nmodule and a Quaternion Rotary Attention (QRA) module. First, SPE embeds\nposition information into self-attention in a rotational manner, leading to\nbetter learning of features of movement sequences and audio sequences, and\nimproved understanding of the connection between music and dance. Second, QRA\nrepresents and fuses 3D motion features and audio features in the form of a\nseries of quaternions, enabling the model to better learn the temporal\ncoordination of music and dance under the complex temporal cycle conditions of\ndance generation. Finally, we conducted experiments on the dataset AIST++, and\nthe results show that our approach achieves better and more robust performance\nin generating accurate, high-quality dance movements. Our source code and\ndataset can be available from https://github.com/MarasyZZ/QEAN and\nhttps://google.github.io/aistplusplus_dataset respectively.\n", "link": "http://arxiv.org/abs/2403.11626v1", "date": "2024-03-18", "relevancy": 2.5383, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5134}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.5123}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4973}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20QEAN%3A%20Quaternion-Enhanced%20Attention%20Network%20for%20Visual%20Dance%20Generation&body=Title%3A%20QEAN%3A%20Quaternion-Enhanced%20Attention%20Network%20for%20Visual%20Dance%20Generation%0AAuthor%3A%20Zhizhen%20Zhou%20and%20Yejing%20Huo%20and%20Guoheng%20Huang%20and%20An%20Zeng%20and%20Xuhang%20Chen%20and%20Lian%20Huang%20and%20Zinuo%20Li%0AAbstract%3A%20%20%20The%20study%20of%20music-generated%20dance%20is%20a%20novel%20and%20challenging%20Image%0Ageneration%20task.%20It%20aims%20to%20input%20a%20piece%20of%20music%20and%20seed%20motions%2C%20then%0Agenerate%20natural%20dance%20movements%20for%20the%20subsequent%20music.%20Transformer-based%0Amethods%20face%20challenges%20in%20time%20series%20prediction%20tasks%20related%20to%20human%0Amovements%20and%20music%20due%20to%20their%20struggle%20in%20capturing%20the%20nonlinear%0Arelationship%20and%20temporal%20aspects.%20This%20can%20lead%20to%20issues%20like%20joint%0Adeformation%2C%20role%20deviation%2C%20floating%2C%20and%20inconsistencies%20in%20dance%20movements%0Agenerated%20in%20response%20to%20the%20music.%20In%20this%20paper%2C%20we%20propose%20a%0AQuaternion-Enhanced%20Attention%20Network%20%28QEAN%29%20for%20visual%20dance%20synthesis%20from%20a%0Aquaternion%20perspective%2C%20which%20consists%20of%20a%20Spin%20Position%20Embedding%20%28SPE%29%0Amodule%20and%20a%20Quaternion%20Rotary%20Attention%20%28QRA%29%20module.%20First%2C%20SPE%20embeds%0Aposition%20information%20into%20self-attention%20in%20a%20rotational%20manner%2C%20leading%20to%0Abetter%20learning%20of%20features%20of%20movement%20sequences%20and%20audio%20sequences%2C%20and%0Aimproved%20understanding%20of%20the%20connection%20between%20music%20and%20dance.%20Second%2C%20QRA%0Arepresents%20and%20fuses%203D%20motion%20features%20and%20audio%20features%20in%20the%20form%20of%20a%0Aseries%20of%20quaternions%2C%20enabling%20the%20model%20to%20better%20learn%20the%20temporal%0Acoordination%20of%20music%20and%20dance%20under%20the%20complex%20temporal%20cycle%20conditions%20of%0Adance%20generation.%20Finally%2C%20we%20conducted%20experiments%20on%20the%20dataset%20AIST%2B%2B%2C%20and%0Athe%20results%20show%20that%20our%20approach%20achieves%20better%20and%20more%20robust%20performance%0Ain%20generating%20accurate%2C%20high-quality%20dance%20movements.%20Our%20source%20code%20and%0Adataset%20can%20be%20available%20from%20https%3A//github.com/MarasyZZ/QEAN%20and%0Ahttps%3A//google.github.io/aistplusplus_dataset%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11626v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=QEAN%3A%20Quaternion-Enhanced%20Attention%20Network%20for%20Visual%20Dance%20Generation&entry.906535625=Zhizhen%20Zhou%20and%20Yejing%20Huo%20and%20Guoheng%20Huang%20and%20An%20Zeng%20and%20Xuhang%20Chen%20and%20Lian%20Huang%20and%20Zinuo%20Li&entry.1292438233=%20%20The%20study%20of%20music-generated%20dance%20is%20a%20novel%20and%20challenging%20Image%0Ageneration%20task.%20It%20aims%20to%20input%20a%20piece%20of%20music%20and%20seed%20motions%2C%20then%0Agenerate%20natural%20dance%20movements%20for%20the%20subsequent%20music.%20Transformer-based%0Amethods%20face%20challenges%20in%20time%20series%20prediction%20tasks%20related%20to%20human%0Amovements%20and%20music%20due%20to%20their%20struggle%20in%20capturing%20the%20nonlinear%0Arelationship%20and%20temporal%20aspects.%20This%20can%20lead%20to%20issues%20like%20joint%0Adeformation%2C%20role%20deviation%2C%20floating%2C%20and%20inconsistencies%20in%20dance%20movements%0Agenerated%20in%20response%20to%20the%20music.%20In%20this%20paper%2C%20we%20propose%20a%0AQuaternion-Enhanced%20Attention%20Network%20%28QEAN%29%20for%20visual%20dance%20synthesis%20from%20a%0Aquaternion%20perspective%2C%20which%20consists%20of%20a%20Spin%20Position%20Embedding%20%28SPE%29%0Amodule%20and%20a%20Quaternion%20Rotary%20Attention%20%28QRA%29%20module.%20First%2C%20SPE%20embeds%0Aposition%20information%20into%20self-attention%20in%20a%20rotational%20manner%2C%20leading%20to%0Abetter%20learning%20of%20features%20of%20movement%20sequences%20and%20audio%20sequences%2C%20and%0Aimproved%20understanding%20of%20the%20connection%20between%20music%20and%20dance.%20Second%2C%20QRA%0Arepresents%20and%20fuses%203D%20motion%20features%20and%20audio%20features%20in%20the%20form%20of%20a%0Aseries%20of%20quaternions%2C%20enabling%20the%20model%20to%20better%20learn%20the%20temporal%0Acoordination%20of%20music%20and%20dance%20under%20the%20complex%20temporal%20cycle%20conditions%20of%0Adance%20generation.%20Finally%2C%20we%20conducted%20experiments%20on%20the%20dataset%20AIST%2B%2B%2C%20and%0Athe%20results%20show%20that%20our%20approach%20achieves%20better%20and%20more%20robust%20performance%0Ain%20generating%20accurate%2C%20high-quality%20dance%20movements.%20Our%20source%20code%20and%0Adataset%20can%20be%20available%20from%20https%3A//github.com/MarasyZZ/QEAN%20and%0Ahttps%3A//google.github.io/aistplusplus_dataset%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11626v1&entry.124074799=Read"},
{"title": "Compositional Kronecker Context Optimization for Vision-Language Models", "author": "Kun Ding and Xiaohui Li and Qiang Yu and Ying Wang and Haojian Zhang and Shiming Xiang", "abstract": "  Context Optimization (CoOp) has emerged as a simple yet effective technique\nfor adapting CLIP-like vision-language models to downstream image recognition\ntasks. Nevertheless, learning compact context with satisfactory base-to-new,\ndomain and cross-task generalization ability while adapting to new tasks is\nstill a challenge. To tackle such a challenge, we propose a lightweight yet\ngeneralizable approach termed Compositional Kronecker Context Optimization\n(CK-CoOp). Technically, the prompt's context words in CK-CoOp are learnable\nvectors, which are crafted by linearly combining base vectors sourced from a\ndictionary. These base vectors consist of a non-learnable component obtained by\nquantizing the weights in the token embedding layer, and a learnable component\nconstructed by applying Kronecker product on several learnable tiny matrices.\nIntuitively, the compositional structure mitigates the risk of overfitting on\ntraining data by remembering more pre-trained knowledge. Meantime, the\nKronecker product breaks the non-learnable restrictions of the dictionary,\nthereby enhancing representation ability with minimal additional parameters.\nExtensive experiments confirm that CK-CoOp achieves state-of-the-art\nperformance under base-to-new, domain and cross-task generalization evaluation,\nbut also has the metrics of fewer learnable parameters and efficient training\nand inference speed.\n", "link": "http://arxiv.org/abs/2403.11631v1", "date": "2024-03-18", "relevancy": 2.4922, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5323}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4987}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4644}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Compositional%20Kronecker%20Context%20Optimization%20for%20Vision-Language%20Models&body=Title%3A%20Compositional%20Kronecker%20Context%20Optimization%20for%20Vision-Language%20Models%0AAuthor%3A%20Kun%20Ding%20and%20Xiaohui%20Li%20and%20Qiang%20Yu%20and%20Ying%20Wang%20and%20Haojian%20Zhang%20and%20Shiming%20Xiang%0AAbstract%3A%20%20%20Context%20Optimization%20%28CoOp%29%20has%20emerged%20as%20a%20simple%20yet%20effective%20technique%0Afor%20adapting%20CLIP-like%20vision-language%20models%20to%20downstream%20image%20recognition%0Atasks.%20Nevertheless%2C%20learning%20compact%20context%20with%20satisfactory%20base-to-new%2C%0Adomain%20and%20cross-task%20generalization%20ability%20while%20adapting%20to%20new%20tasks%20is%0Astill%20a%20challenge.%20To%20tackle%20such%20a%20challenge%2C%20we%20propose%20a%20lightweight%20yet%0Ageneralizable%20approach%20termed%20Compositional%20Kronecker%20Context%20Optimization%0A%28CK-CoOp%29.%20Technically%2C%20the%20prompt%27s%20context%20words%20in%20CK-CoOp%20are%20learnable%0Avectors%2C%20which%20are%20crafted%20by%20linearly%20combining%20base%20vectors%20sourced%20from%20a%0Adictionary.%20These%20base%20vectors%20consist%20of%20a%20non-learnable%20component%20obtained%20by%0Aquantizing%20the%20weights%20in%20the%20token%20embedding%20layer%2C%20and%20a%20learnable%20component%0Aconstructed%20by%20applying%20Kronecker%20product%20on%20several%20learnable%20tiny%20matrices.%0AIntuitively%2C%20the%20compositional%20structure%20mitigates%20the%20risk%20of%20overfitting%20on%0Atraining%20data%20by%20remembering%20more%20pre-trained%20knowledge.%20Meantime%2C%20the%0AKronecker%20product%20breaks%20the%20non-learnable%20restrictions%20of%20the%20dictionary%2C%0Athereby%20enhancing%20representation%20ability%20with%20minimal%20additional%20parameters.%0AExtensive%20experiments%20confirm%20that%20CK-CoOp%20achieves%20state-of-the-art%0Aperformance%20under%20base-to-new%2C%20domain%20and%20cross-task%20generalization%20evaluation%2C%0Abut%20also%20has%20the%20metrics%20of%20fewer%20learnable%20parameters%20and%20efficient%20training%0Aand%20inference%20speed.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11631v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Compositional%20Kronecker%20Context%20Optimization%20for%20Vision-Language%20Models&entry.906535625=Kun%20Ding%20and%20Xiaohui%20Li%20and%20Qiang%20Yu%20and%20Ying%20Wang%20and%20Haojian%20Zhang%20and%20Shiming%20Xiang&entry.1292438233=%20%20Context%20Optimization%20%28CoOp%29%20has%20emerged%20as%20a%20simple%20yet%20effective%20technique%0Afor%20adapting%20CLIP-like%20vision-language%20models%20to%20downstream%20image%20recognition%0Atasks.%20Nevertheless%2C%20learning%20compact%20context%20with%20satisfactory%20base-to-new%2C%0Adomain%20and%20cross-task%20generalization%20ability%20while%20adapting%20to%20new%20tasks%20is%0Astill%20a%20challenge.%20To%20tackle%20such%20a%20challenge%2C%20we%20propose%20a%20lightweight%20yet%0Ageneralizable%20approach%20termed%20Compositional%20Kronecker%20Context%20Optimization%0A%28CK-CoOp%29.%20Technically%2C%20the%20prompt%27s%20context%20words%20in%20CK-CoOp%20are%20learnable%0Avectors%2C%20which%20are%20crafted%20by%20linearly%20combining%20base%20vectors%20sourced%20from%20a%0Adictionary.%20These%20base%20vectors%20consist%20of%20a%20non-learnable%20component%20obtained%20by%0Aquantizing%20the%20weights%20in%20the%20token%20embedding%20layer%2C%20and%20a%20learnable%20component%0Aconstructed%20by%20applying%20Kronecker%20product%20on%20several%20learnable%20tiny%20matrices.%0AIntuitively%2C%20the%20compositional%20structure%20mitigates%20the%20risk%20of%20overfitting%20on%0Atraining%20data%20by%20remembering%20more%20pre-trained%20knowledge.%20Meantime%2C%20the%0AKronecker%20product%20breaks%20the%20non-learnable%20restrictions%20of%20the%20dictionary%2C%0Athereby%20enhancing%20representation%20ability%20with%20minimal%20additional%20parameters.%0AExtensive%20experiments%20confirm%20that%20CK-CoOp%20achieves%20state-of-the-art%0Aperformance%20under%20base-to-new%2C%20domain%20and%20cross-task%20generalization%20evaluation%2C%0Abut%20also%20has%20the%20metrics%20of%20fewer%20learnable%20parameters%20and%20efficient%20training%0Aand%20inference%20speed.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11631v1&entry.124074799=Read"},
{"title": "Normalized Validity Scores for DNNs in Regression based Eye Feature\n  Extraction", "author": "Wolfgang Fuhl", "abstract": "  We propose an improvement to the landmark validity loss. Landmark detection\nis widely used in head pose estimation, eyelid shape extraction, as well as\npupil and iris segmentation. There are numerous additional applications where\nlandmark detection is used to estimate the shape of complex objects. One part\nof this process is the accurate and fine-grained detection of the shape. The\nother part is the validity or inaccuracy per landmark, which can be used to\ndetect unreliable areas, where the shape possibly does not fit, and to improve\nthe accuracy of the entire shape extraction by excluding inaccurate landmarks.\nWe propose a normalization in the loss formulation, which improves the accuracy\nof the entire approach due to the numerical balance of the normalized\ninaccuracy. In addition, we propose a margin for the inaccuracy to reduce the\nimpact of gradients, which are produced by negligible errors close to the\nground truth.\n", "link": "http://arxiv.org/abs/2403.11665v1", "date": "2024-03-18", "relevancy": 2.4422, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5273}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4708}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4672}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Normalized%20Validity%20Scores%20for%20DNNs%20in%20Regression%20based%20Eye%20Feature%0A%20%20Extraction&body=Title%3A%20Normalized%20Validity%20Scores%20for%20DNNs%20in%20Regression%20based%20Eye%20Feature%0A%20%20Extraction%0AAuthor%3A%20Wolfgang%20Fuhl%0AAbstract%3A%20%20%20We%20propose%20an%20improvement%20to%20the%20landmark%20validity%20loss.%20Landmark%20detection%0Ais%20widely%20used%20in%20head%20pose%20estimation%2C%20eyelid%20shape%20extraction%2C%20as%20well%20as%0Apupil%20and%20iris%20segmentation.%20There%20are%20numerous%20additional%20applications%20where%0Alandmark%20detection%20is%20used%20to%20estimate%20the%20shape%20of%20complex%20objects.%20One%20part%0Aof%20this%20process%20is%20the%20accurate%20and%20fine-grained%20detection%20of%20the%20shape.%20The%0Aother%20part%20is%20the%20validity%20or%20inaccuracy%20per%20landmark%2C%20which%20can%20be%20used%20to%0Adetect%20unreliable%20areas%2C%20where%20the%20shape%20possibly%20does%20not%20fit%2C%20and%20to%20improve%0Athe%20accuracy%20of%20the%20entire%20shape%20extraction%20by%20excluding%20inaccurate%20landmarks.%0AWe%20propose%20a%20normalization%20in%20the%20loss%20formulation%2C%20which%20improves%20the%20accuracy%0Aof%20the%20entire%20approach%20due%20to%20the%20numerical%20balance%20of%20the%20normalized%0Ainaccuracy.%20In%20addition%2C%20we%20propose%20a%20margin%20for%20the%20inaccuracy%20to%20reduce%20the%0Aimpact%20of%20gradients%2C%20which%20are%20produced%20by%20negligible%20errors%20close%20to%20the%0Aground%20truth.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11665v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Normalized%20Validity%20Scores%20for%20DNNs%20in%20Regression%20based%20Eye%20Feature%0A%20%20Extraction&entry.906535625=Wolfgang%20Fuhl&entry.1292438233=%20%20We%20propose%20an%20improvement%20to%20the%20landmark%20validity%20loss.%20Landmark%20detection%0Ais%20widely%20used%20in%20head%20pose%20estimation%2C%20eyelid%20shape%20extraction%2C%20as%20well%20as%0Apupil%20and%20iris%20segmentation.%20There%20are%20numerous%20additional%20applications%20where%0Alandmark%20detection%20is%20used%20to%20estimate%20the%20shape%20of%20complex%20objects.%20One%20part%0Aof%20this%20process%20is%20the%20accurate%20and%20fine-grained%20detection%20of%20the%20shape.%20The%0Aother%20part%20is%20the%20validity%20or%20inaccuracy%20per%20landmark%2C%20which%20can%20be%20used%20to%0Adetect%20unreliable%20areas%2C%20where%20the%20shape%20possibly%20does%20not%20fit%2C%20and%20to%20improve%0Athe%20accuracy%20of%20the%20entire%20shape%20extraction%20by%20excluding%20inaccurate%20landmarks.%0AWe%20propose%20a%20normalization%20in%20the%20loss%20formulation%2C%20which%20improves%20the%20accuracy%0Aof%20the%20entire%20approach%20due%20to%20the%20numerical%20balance%20of%20the%20normalized%0Ainaccuracy.%20In%20addition%2C%20we%20propose%20a%20margin%20for%20the%20inaccuracy%20to%20reduce%20the%0Aimpact%20of%20gradients%2C%20which%20are%20produced%20by%20negligible%20errors%20close%20to%20the%0Aground%20truth.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11665v1&entry.124074799=Read"},
{"title": "Combining Local and Global Perception for Autonomous Navigation on\n  Nano-UAVs", "author": "Lorenzo Lamberti and Georg Rutishauser and Francesco Conti and Luca Benini", "abstract": "  A critical challenge in deploying unmanned aerial vehicles (UAVs) for\nautonomous tasks is their ability to navigate in an unknown environment. This\npaper introduces a novel vision-depth fusion approach for autonomous navigation\non nano-UAVs. We combine the visual-based PULP-Dronet convolutional neural\nnetwork for semantic information extraction, i.e., serving as the global\nperception, with 8x8px depth maps for close-proximity maneuvers, i.e., the\nlocal perception. When tested in-field, our integration strategy highlights the\ncomplementary strengths of both visual and depth sensory information. We\nachieve a 100% success rate over 15 flights in a complex navigation scenario,\nencompassing straight pathways, static obstacle avoidance, and 90{\\deg} turns.\n", "link": "http://arxiv.org/abs/2403.11661v1", "date": "2024-03-18", "relevancy": 2.3812, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.6645}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5494}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5371}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Combining%20Local%20and%20Global%20Perception%20for%20Autonomous%20Navigation%20on%0A%20%20Nano-UAVs&body=Title%3A%20Combining%20Local%20and%20Global%20Perception%20for%20Autonomous%20Navigation%20on%0A%20%20Nano-UAVs%0AAuthor%3A%20Lorenzo%20Lamberti%20and%20Georg%20Rutishauser%20and%20Francesco%20Conti%20and%20Luca%20Benini%0AAbstract%3A%20%20%20A%20critical%20challenge%20in%20deploying%20unmanned%20aerial%20vehicles%20%28UAVs%29%20for%0Aautonomous%20tasks%20is%20their%20ability%20to%20navigate%20in%20an%20unknown%20environment.%20This%0Apaper%20introduces%20a%20novel%20vision-depth%20fusion%20approach%20for%20autonomous%20navigation%0Aon%20nano-UAVs.%20We%20combine%20the%20visual-based%20PULP-Dronet%20convolutional%20neural%0Anetwork%20for%20semantic%20information%20extraction%2C%20i.e.%2C%20serving%20as%20the%20global%0Aperception%2C%20with%208x8px%20depth%20maps%20for%20close-proximity%20maneuvers%2C%20i.e.%2C%20the%0Alocal%20perception.%20When%20tested%20in-field%2C%20our%20integration%20strategy%20highlights%20the%0Acomplementary%20strengths%20of%20both%20visual%20and%20depth%20sensory%20information.%20We%0Aachieve%20a%20100%25%20success%20rate%20over%2015%20flights%20in%20a%20complex%20navigation%20scenario%2C%0Aencompassing%20straight%20pathways%2C%20static%20obstacle%20avoidance%2C%20and%2090%7B%5Cdeg%7D%20turns.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11661v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Combining%20Local%20and%20Global%20Perception%20for%20Autonomous%20Navigation%20on%0A%20%20Nano-UAVs&entry.906535625=Lorenzo%20Lamberti%20and%20Georg%20Rutishauser%20and%20Francesco%20Conti%20and%20Luca%20Benini&entry.1292438233=%20%20A%20critical%20challenge%20in%20deploying%20unmanned%20aerial%20vehicles%20%28UAVs%29%20for%0Aautonomous%20tasks%20is%20their%20ability%20to%20navigate%20in%20an%20unknown%20environment.%20This%0Apaper%20introduces%20a%20novel%20vision-depth%20fusion%20approach%20for%20autonomous%20navigation%0Aon%20nano-UAVs.%20We%20combine%20the%20visual-based%20PULP-Dronet%20convolutional%20neural%0Anetwork%20for%20semantic%20information%20extraction%2C%20i.e.%2C%20serving%20as%20the%20global%0Aperception%2C%20with%208x8px%20depth%20maps%20for%20close-proximity%20maneuvers%2C%20i.e.%2C%20the%0Alocal%20perception.%20When%20tested%20in-field%2C%20our%20integration%20strategy%20highlights%20the%0Acomplementary%20strengths%20of%20both%20visual%20and%20depth%20sensory%20information.%20We%0Aachieve%20a%20100%25%20success%20rate%20over%2015%20flights%20in%20a%20complex%20navigation%20scenario%2C%0Aencompassing%20straight%20pathways%2C%20static%20obstacle%20avoidance%2C%20and%2090%7B%5Cdeg%7D%20turns.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11661v1&entry.124074799=Read"},
{"title": "Identifying Policy Gradient Subspaces", "author": "Jan Schneider and Pierre Schumacher and Simon Guist and Le Chen and Daniel H\u00e4ufle and Bernhard Sch\u00f6lkopf and Dieter B\u00fcchler", "abstract": "  Policy gradient methods hold great potential for solving complex continuous\ncontrol tasks. Still, their training efficiency can be improved by exploiting\nstructure within the optimization problem. Recent work indicates that\nsupervised learning can be accelerated by leveraging the fact that gradients\nlie in a low-dimensional and slowly-changing subspace. In this paper, we\nconduct a thorough evaluation of this phenomenon for two popular deep policy\ngradient methods on various simulated benchmark tasks. Our results demonstrate\nthe existence of such gradient subspaces despite the continuously changing data\ndistribution inherent to reinforcement learning. These findings reveal\npromising directions for future work on more efficient reinforcement learning,\ne.g., through improving parameter-space exploration or enabling second-order\noptimization.\n", "link": "http://arxiv.org/abs/2401.06604v3", "date": "2024-03-18", "relevancy": 2.3735, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4848}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4729}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4664}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Identifying%20Policy%20Gradient%20Subspaces&body=Title%3A%20Identifying%20Policy%20Gradient%20Subspaces%0AAuthor%3A%20Jan%20Schneider%20and%20Pierre%20Schumacher%20and%20Simon%20Guist%20and%20Le%20Chen%20and%20Daniel%20H%C3%A4ufle%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Dieter%20B%C3%BCchler%0AAbstract%3A%20%20%20Policy%20gradient%20methods%20hold%20great%20potential%20for%20solving%20complex%20continuous%0Acontrol%20tasks.%20Still%2C%20their%20training%20efficiency%20can%20be%20improved%20by%20exploiting%0Astructure%20within%20the%20optimization%20problem.%20Recent%20work%20indicates%20that%0Asupervised%20learning%20can%20be%20accelerated%20by%20leveraging%20the%20fact%20that%20gradients%0Alie%20in%20a%20low-dimensional%20and%20slowly-changing%20subspace.%20In%20this%20paper%2C%20we%0Aconduct%20a%20thorough%20evaluation%20of%20this%20phenomenon%20for%20two%20popular%20deep%20policy%0Agradient%20methods%20on%20various%20simulated%20benchmark%20tasks.%20Our%20results%20demonstrate%0Athe%20existence%20of%20such%20gradient%20subspaces%20despite%20the%20continuously%20changing%20data%0Adistribution%20inherent%20to%20reinforcement%20learning.%20These%20findings%20reveal%0Apromising%20directions%20for%20future%20work%20on%20more%20efficient%20reinforcement%20learning%2C%0Ae.g.%2C%20through%20improving%20parameter-space%20exploration%20or%20enabling%20second-order%0Aoptimization.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.06604v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Policy%20Gradient%20Subspaces&entry.906535625=Jan%20Schneider%20and%20Pierre%20Schumacher%20and%20Simon%20Guist%20and%20Le%20Chen%20and%20Daniel%20H%C3%A4ufle%20and%20Bernhard%20Sch%C3%B6lkopf%20and%20Dieter%20B%C3%BCchler&entry.1292438233=%20%20Policy%20gradient%20methods%20hold%20great%20potential%20for%20solving%20complex%20continuous%0Acontrol%20tasks.%20Still%2C%20their%20training%20efficiency%20can%20be%20improved%20by%20exploiting%0Astructure%20within%20the%20optimization%20problem.%20Recent%20work%20indicates%20that%0Asupervised%20learning%20can%20be%20accelerated%20by%20leveraging%20the%20fact%20that%20gradients%0Alie%20in%20a%20low-dimensional%20and%20slowly-changing%20subspace.%20In%20this%20paper%2C%20we%0Aconduct%20a%20thorough%20evaluation%20of%20this%20phenomenon%20for%20two%20popular%20deep%20policy%0Agradient%20methods%20on%20various%20simulated%20benchmark%20tasks.%20Our%20results%20demonstrate%0Athe%20existence%20of%20such%20gradient%20subspaces%20despite%20the%20continuously%20changing%20data%0Adistribution%20inherent%20to%20reinforcement%20learning.%20These%20findings%20reveal%0Apromising%20directions%20for%20future%20work%20on%20more%20efficient%20reinforcement%20learning%2C%0Ae.g.%2C%20through%20improving%20parameter-space%20exploration%20or%20enabling%20second-order%0Aoptimization.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.06604v3&entry.124074799=Read"},
{"title": "NEDS-SLAM: A Novel Neural Explicit Dense Semantic SLAM Framework using\n  3D Gaussian Splatting", "author": "Yiming Ji and Yang Liu and Guanghu Xie and Boyu Ma and Zongwu Xie", "abstract": "  We propose NEDS-SLAM, an Explicit Dense semantic SLAM system based on 3D\nGaussian representation, that enables robust 3D semantic mapping, accurate\ncamera tracking, and high-quality rendering in real-time. In the system, we\npropose a Spatially Consistent Feature Fusion model to reduce the effect of\nerroneous estimates from pre-trained segmentation head on semantic\nreconstruction, achieving robust 3D semantic Gaussian mapping. Additionally, we\nemploy a lightweight encoder-decoder to compress the high-dimensional semantic\nfeatures into a compact 3D Gaussian representation, mitigating the burden of\nexcessive memory consumption. Furthermore, we leverage the advantage of 3D\nGaussian splatting, which enables efficient and differentiable novel view\nrendering, and propose a Virtual Camera View Pruning method to eliminate\noutlier GS points, thereby effectively enhancing the quality of scene\nrepresentations. Our NEDS-SLAM method demonstrates competitive performance over\nexisting dense semantic SLAM methods in terms of mapping and tracking accuracy\non Replica and ScanNet datasets, while also showing excellent capabilities in\n3D dense semantic mapping.\n", "link": "http://arxiv.org/abs/2403.11679v1", "date": "2024-03-18", "relevancy": 2.3469, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.6031}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5862}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5471}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20NEDS-SLAM%3A%20A%20Novel%20Neural%20Explicit%20Dense%20Semantic%20SLAM%20Framework%20using%0A%20%203D%20Gaussian%20Splatting&body=Title%3A%20NEDS-SLAM%3A%20A%20Novel%20Neural%20Explicit%20Dense%20Semantic%20SLAM%20Framework%20using%0A%20%203D%20Gaussian%20Splatting%0AAuthor%3A%20Yiming%20Ji%20and%20Yang%20Liu%20and%20Guanghu%20Xie%20and%20Boyu%20Ma%20and%20Zongwu%20Xie%0AAbstract%3A%20%20%20We%20propose%20NEDS-SLAM%2C%20an%20Explicit%20Dense%20semantic%20SLAM%20system%20based%20on%203D%0AGaussian%20representation%2C%20that%20enables%20robust%203D%20semantic%20mapping%2C%20accurate%0Acamera%20tracking%2C%20and%20high-quality%20rendering%20in%20real-time.%20In%20the%20system%2C%20we%0Apropose%20a%20Spatially%20Consistent%20Feature%20Fusion%20model%20to%20reduce%20the%20effect%20of%0Aerroneous%20estimates%20from%20pre-trained%20segmentation%20head%20on%20semantic%0Areconstruction%2C%20achieving%20robust%203D%20semantic%20Gaussian%20mapping.%20Additionally%2C%20we%0Aemploy%20a%20lightweight%20encoder-decoder%20to%20compress%20the%20high-dimensional%20semantic%0Afeatures%20into%20a%20compact%203D%20Gaussian%20representation%2C%20mitigating%20the%20burden%20of%0Aexcessive%20memory%20consumption.%20Furthermore%2C%20we%20leverage%20the%20advantage%20of%203D%0AGaussian%20splatting%2C%20which%20enables%20efficient%20and%20differentiable%20novel%20view%0Arendering%2C%20and%20propose%20a%20Virtual%20Camera%20View%20Pruning%20method%20to%20eliminate%0Aoutlier%20GS%20points%2C%20thereby%20effectively%20enhancing%20the%20quality%20of%20scene%0Arepresentations.%20Our%20NEDS-SLAM%20method%20demonstrates%20competitive%20performance%20over%0Aexisting%20dense%20semantic%20SLAM%20methods%20in%20terms%20of%20mapping%20and%20tracking%20accuracy%0Aon%20Replica%20and%20ScanNet%20datasets%2C%20while%20also%20showing%20excellent%20capabilities%20in%0A3D%20dense%20semantic%20mapping.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11679v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=NEDS-SLAM%3A%20A%20Novel%20Neural%20Explicit%20Dense%20Semantic%20SLAM%20Framework%20using%0A%20%203D%20Gaussian%20Splatting&entry.906535625=Yiming%20Ji%20and%20Yang%20Liu%20and%20Guanghu%20Xie%20and%20Boyu%20Ma%20and%20Zongwu%20Xie&entry.1292438233=%20%20We%20propose%20NEDS-SLAM%2C%20an%20Explicit%20Dense%20semantic%20SLAM%20system%20based%20on%203D%0AGaussian%20representation%2C%20that%20enables%20robust%203D%20semantic%20mapping%2C%20accurate%0Acamera%20tracking%2C%20and%20high-quality%20rendering%20in%20real-time.%20In%20the%20system%2C%20we%0Apropose%20a%20Spatially%20Consistent%20Feature%20Fusion%20model%20to%20reduce%20the%20effect%20of%0Aerroneous%20estimates%20from%20pre-trained%20segmentation%20head%20on%20semantic%0Areconstruction%2C%20achieving%20robust%203D%20semantic%20Gaussian%20mapping.%20Additionally%2C%20we%0Aemploy%20a%20lightweight%20encoder-decoder%20to%20compress%20the%20high-dimensional%20semantic%0Afeatures%20into%20a%20compact%203D%20Gaussian%20representation%2C%20mitigating%20the%20burden%20of%0Aexcessive%20memory%20consumption.%20Furthermore%2C%20we%20leverage%20the%20advantage%20of%203D%0AGaussian%20splatting%2C%20which%20enables%20efficient%20and%20differentiable%20novel%20view%0Arendering%2C%20and%20propose%20a%20Virtual%20Camera%20View%20Pruning%20method%20to%20eliminate%0Aoutlier%20GS%20points%2C%20thereby%20effectively%20enhancing%20the%20quality%20of%20scene%0Arepresentations.%20Our%20NEDS-SLAM%20method%20demonstrates%20competitive%20performance%20over%0Aexisting%20dense%20semantic%20SLAM%20methods%20in%20terms%20of%20mapping%20and%20tracking%20accuracy%0Aon%20Replica%20and%20ScanNet%20datasets%2C%20while%20also%20showing%20excellent%20capabilities%20in%0A3D%20dense%20semantic%20mapping.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11679v1&entry.124074799=Read"},
{"title": "TTT-KD: Test-Time Training for 3D Semantic Segmentation through\n  Knowledge Distillation from Foundation Models", "author": "Lisa Weijler and Muhammad Jehanzeb Mirza and Leon Sick and Can Ekkazan and Pedro Hermosilla", "abstract": "  Test-Time Training (TTT) proposes to adapt a pre-trained network to changing\ndata distributions on-the-fly. In this work, we propose the first TTT method\nfor 3D semantic segmentation, TTT-KD, which models Knowledge Distillation (KD)\nfrom foundation models (e.g. DINOv2) as a self-supervised objective for\nadaptation to distribution shifts at test-time. Given access to paired\nimage-pointcloud (2D-3D) data, we first optimize a 3D segmentation backbone for\nthe main task of semantic segmentation using the pointclouds and the task of 2D\n$\\to$ 3D KD by using an off-the-shelf 2D pre-trained foundation model. At\ntest-time, our TTT-KD updates the 3D segmentation backbone for each test\nsample, by using the self-supervised task of knowledge distillation, before\nperforming the final prediction. Extensive evaluations on multiple indoor and\noutdoor 3D segmentation benchmarks show the utility of TTT-KD, as it improves\nperformance for both in-distribution (ID) and out-of-distribution (ODO) test\ndatasets. We achieve a gain of up to 13% mIoU (7% on average) when the train\nand test distributions are similar and up to 45% (20% on average) when adapting\nto OOD test samples.\n", "link": "http://arxiv.org/abs/2403.11691v1", "date": "2024-03-18", "relevancy": 2.3214, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5964}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5718}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5677}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TTT-KD%3A%20Test-Time%20Training%20for%203D%20Semantic%20Segmentation%20through%0A%20%20Knowledge%20Distillation%20from%20Foundation%20Models&body=Title%3A%20TTT-KD%3A%20Test-Time%20Training%20for%203D%20Semantic%20Segmentation%20through%0A%20%20Knowledge%20Distillation%20from%20Foundation%20Models%0AAuthor%3A%20Lisa%20Weijler%20and%20Muhammad%20Jehanzeb%20Mirza%20and%20Leon%20Sick%20and%20Can%20Ekkazan%20and%20Pedro%20Hermosilla%0AAbstract%3A%20%20%20Test-Time%20Training%20%28TTT%29%20proposes%20to%20adapt%20a%20pre-trained%20network%20to%20changing%0Adata%20distributions%20on-the-fly.%20In%20this%20work%2C%20we%20propose%20the%20first%20TTT%20method%0Afor%203D%20semantic%20segmentation%2C%20TTT-KD%2C%20which%20models%20Knowledge%20Distillation%20%28KD%29%0Afrom%20foundation%20models%20%28e.g.%20DINOv2%29%20as%20a%20self-supervised%20objective%20for%0Aadaptation%20to%20distribution%20shifts%20at%20test-time.%20Given%20access%20to%20paired%0Aimage-pointcloud%20%282D-3D%29%20data%2C%20we%20first%20optimize%20a%203D%20segmentation%20backbone%20for%0Athe%20main%20task%20of%20semantic%20segmentation%20using%20the%20pointclouds%20and%20the%20task%20of%202D%0A%24%5Cto%24%203D%20KD%20by%20using%20an%20off-the-shelf%202D%20pre-trained%20foundation%20model.%20At%0Atest-time%2C%20our%20TTT-KD%20updates%20the%203D%20segmentation%20backbone%20for%20each%20test%0Asample%2C%20by%20using%20the%20self-supervised%20task%20of%20knowledge%20distillation%2C%20before%0Aperforming%20the%20final%20prediction.%20Extensive%20evaluations%20on%20multiple%20indoor%20and%0Aoutdoor%203D%20segmentation%20benchmarks%20show%20the%20utility%20of%20TTT-KD%2C%20as%20it%20improves%0Aperformance%20for%20both%20in-distribution%20%28ID%29%20and%20out-of-distribution%20%28ODO%29%20test%0Adatasets.%20We%20achieve%20a%20gain%20of%20up%20to%2013%25%20mIoU%20%287%25%20on%20average%29%20when%20the%20train%0Aand%20test%20distributions%20are%20similar%20and%20up%20to%2045%25%20%2820%25%20on%20average%29%20when%20adapting%0Ato%20OOD%20test%20samples.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11691v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TTT-KD%3A%20Test-Time%20Training%20for%203D%20Semantic%20Segmentation%20through%0A%20%20Knowledge%20Distillation%20from%20Foundation%20Models&entry.906535625=Lisa%20Weijler%20and%20Muhammad%20Jehanzeb%20Mirza%20and%20Leon%20Sick%20and%20Can%20Ekkazan%20and%20Pedro%20Hermosilla&entry.1292438233=%20%20Test-Time%20Training%20%28TTT%29%20proposes%20to%20adapt%20a%20pre-trained%20network%20to%20changing%0Adata%20distributions%20on-the-fly.%20In%20this%20work%2C%20we%20propose%20the%20first%20TTT%20method%0Afor%203D%20semantic%20segmentation%2C%20TTT-KD%2C%20which%20models%20Knowledge%20Distillation%20%28KD%29%0Afrom%20foundation%20models%20%28e.g.%20DINOv2%29%20as%20a%20self-supervised%20objective%20for%0Aadaptation%20to%20distribution%20shifts%20at%20test-time.%20Given%20access%20to%20paired%0Aimage-pointcloud%20%282D-3D%29%20data%2C%20we%20first%20optimize%20a%203D%20segmentation%20backbone%20for%0Athe%20main%20task%20of%20semantic%20segmentation%20using%20the%20pointclouds%20and%20the%20task%20of%202D%0A%24%5Cto%24%203D%20KD%20by%20using%20an%20off-the-shelf%202D%20pre-trained%20foundation%20model.%20At%0Atest-time%2C%20our%20TTT-KD%20updates%20the%203D%20segmentation%20backbone%20for%20each%20test%0Asample%2C%20by%20using%20the%20self-supervised%20task%20of%20knowledge%20distillation%2C%20before%0Aperforming%20the%20final%20prediction.%20Extensive%20evaluations%20on%20multiple%20indoor%20and%0Aoutdoor%203D%20segmentation%20benchmarks%20show%20the%20utility%20of%20TTT-KD%2C%20as%20it%20improves%0Aperformance%20for%20both%20in-distribution%20%28ID%29%20and%20out-of-distribution%20%28ODO%29%20test%0Adatasets.%20We%20achieve%20a%20gain%20of%20up%20to%2013%25%20mIoU%20%287%25%20on%20average%29%20when%20the%20train%0Aand%20test%20distributions%20are%20similar%20and%20up%20to%2045%25%20%2820%25%20on%20average%29%20when%20adapting%0Ato%20OOD%20test%20samples.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11691v1&entry.124074799=Read"},
{"title": "Personalized 3D Human Pose and Shape Refinement", "author": "Tom Wehrbein and Bodo Rosenhahn and Iain Matthews and Carsten Stoll", "abstract": "  Recently, regression-based methods have dominated the field of 3D human pose\nand shape estimation. Despite their promising results, a common issue is the\nmisalignment between predictions and image observations, often caused by minor\njoint rotation errors that accumulate along the kinematic chain. To address\nthis issue, we propose to construct dense correspondences between initial human\nmodel estimates and the corresponding images that can be used to refine the\ninitial predictions. To this end, we utilize renderings of the 3D models to\npredict per-pixel 2D displacements between the synthetic renderings and the RGB\nimages. This allows us to effectively integrate and exploit appearance\ninformation of the persons. Our per-pixel displacements can be efficiently\ntransformed to per-visible-vertex displacements and then used for 3D model\nrefinement by minimizing a reprojection loss. To demonstrate the effectiveness\nof our approach, we refine the initial 3D human mesh predictions of multiple\nmodels using different refinement procedures on 3DPW and RICH. We show that our\napproach not only consistently leads to better image-model alignment, but also\nto improved 3D accuracy.\n", "link": "http://arxiv.org/abs/2403.11634v1", "date": "2024-03-18", "relevancy": 2.3188, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5947}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5773}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5481}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Personalized%203D%20Human%20Pose%20and%20Shape%20Refinement&body=Title%3A%20Personalized%203D%20Human%20Pose%20and%20Shape%20Refinement%0AAuthor%3A%20Tom%20Wehrbein%20and%20Bodo%20Rosenhahn%20and%20Iain%20Matthews%20and%20Carsten%20Stoll%0AAbstract%3A%20%20%20Recently%2C%20regression-based%20methods%20have%20dominated%20the%20field%20of%203D%20human%20pose%0Aand%20shape%20estimation.%20Despite%20their%20promising%20results%2C%20a%20common%20issue%20is%20the%0Amisalignment%20between%20predictions%20and%20image%20observations%2C%20often%20caused%20by%20minor%0Ajoint%20rotation%20errors%20that%20accumulate%20along%20the%20kinematic%20chain.%20To%20address%0Athis%20issue%2C%20we%20propose%20to%20construct%20dense%20correspondences%20between%20initial%20human%0Amodel%20estimates%20and%20the%20corresponding%20images%20that%20can%20be%20used%20to%20refine%20the%0Ainitial%20predictions.%20To%20this%20end%2C%20we%20utilize%20renderings%20of%20the%203D%20models%20to%0Apredict%20per-pixel%202D%20displacements%20between%20the%20synthetic%20renderings%20and%20the%20RGB%0Aimages.%20This%20allows%20us%20to%20effectively%20integrate%20and%20exploit%20appearance%0Ainformation%20of%20the%20persons.%20Our%20per-pixel%20displacements%20can%20be%20efficiently%0Atransformed%20to%20per-visible-vertex%20displacements%20and%20then%20used%20for%203D%20model%0Arefinement%20by%20minimizing%20a%20reprojection%20loss.%20To%20demonstrate%20the%20effectiveness%0Aof%20our%20approach%2C%20we%20refine%20the%20initial%203D%20human%20mesh%20predictions%20of%20multiple%0Amodels%20using%20different%20refinement%20procedures%20on%203DPW%20and%20RICH.%20We%20show%20that%20our%0Aapproach%20not%20only%20consistently%20leads%20to%20better%20image-model%20alignment%2C%20but%20also%0Ato%20improved%203D%20accuracy.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11634v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Personalized%203D%20Human%20Pose%20and%20Shape%20Refinement&entry.906535625=Tom%20Wehrbein%20and%20Bodo%20Rosenhahn%20and%20Iain%20Matthews%20and%20Carsten%20Stoll&entry.1292438233=%20%20Recently%2C%20regression-based%20methods%20have%20dominated%20the%20field%20of%203D%20human%20pose%0Aand%20shape%20estimation.%20Despite%20their%20promising%20results%2C%20a%20common%20issue%20is%20the%0Amisalignment%20between%20predictions%20and%20image%20observations%2C%20often%20caused%20by%20minor%0Ajoint%20rotation%20errors%20that%20accumulate%20along%20the%20kinematic%20chain.%20To%20address%0Athis%20issue%2C%20we%20propose%20to%20construct%20dense%20correspondences%20between%20initial%20human%0Amodel%20estimates%20and%20the%20corresponding%20images%20that%20can%20be%20used%20to%20refine%20the%0Ainitial%20predictions.%20To%20this%20end%2C%20we%20utilize%20renderings%20of%20the%203D%20models%20to%0Apredict%20per-pixel%202D%20displacements%20between%20the%20synthetic%20renderings%20and%20the%20RGB%0Aimages.%20This%20allows%20us%20to%20effectively%20integrate%20and%20exploit%20appearance%0Ainformation%20of%20the%20persons.%20Our%20per-pixel%20displacements%20can%20be%20efficiently%0Atransformed%20to%20per-visible-vertex%20displacements%20and%20then%20used%20for%203D%20model%0Arefinement%20by%20minimizing%20a%20reprojection%20loss.%20To%20demonstrate%20the%20effectiveness%0Aof%20our%20approach%2C%20we%20refine%20the%20initial%203D%20human%20mesh%20predictions%20of%20multiple%0Amodels%20using%20different%20refinement%20procedures%20on%203DPW%20and%20RICH.%20We%20show%20that%20our%0Aapproach%20not%20only%20consistently%20leads%20to%20better%20image-model%20alignment%2C%20but%20also%0Ato%20improved%203D%20accuracy.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11634v1&entry.124074799=Read"},
{"title": "Unsupervised Modality-Transferable Video Highlight Detection with\n  Representation Activation Sequence Learning", "author": "Tingtian Li and Zixun Sun and Xinyu Xiao", "abstract": "  Identifying highlight moments of raw video materials is crucial for improving\nthe efficiency of editing videos that are pervasive on internet platforms.\nHowever, the extensive work of manually labeling footage has created obstacles\nto applying supervised methods to videos of unseen categories. The absence of\nan audio modality that contains valuable cues for highlight detection in many\nvideos also makes it difficult to use multimodal strategies. In this paper, we\npropose a novel model with cross-modal perception for unsupervised highlight\ndetection. The proposed model learns representations with visual-audio level\nsemantics from image-audio pair data via a self-reconstruction task. To achieve\nunsupervised highlight detection, we investigate the latent representations of\nthe network and propose the representation activation sequence learning (RASL)\nmodule with k-point contrastive learning to learn significant representation\nactivations. To connect the visual modality with the audio modality, we use the\nsymmetric contrastive learning (SCL) module to learn the paired visual and\naudio representations. Furthermore, an auxiliary task of masked feature vector\nsequence (FVS) reconstruction is simultaneously conducted during pretraining\nfor representation enhancement. During inference, the cross-modal pretrained\nmodel can generate representations with paired visual-audio semantics given\nonly the visual modality. The RASL module is used to output the highlight\nscores. The experimental results show that the proposed framework achieves\nsuperior performance compared to other state-of-the-art approaches.\n", "link": "http://arxiv.org/abs/2403.09401v2", "date": "2024-03-18", "relevancy": 2.224, "topK": [{"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.5684}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5504}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.539}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Unsupervised%20Modality-Transferable%20Video%20Highlight%20Detection%20with%0A%20%20Representation%20Activation%20Sequence%20Learning&body=Title%3A%20Unsupervised%20Modality-Transferable%20Video%20Highlight%20Detection%20with%0A%20%20Representation%20Activation%20Sequence%20Learning%0AAuthor%3A%20Tingtian%20Li%20and%20Zixun%20Sun%20and%20Xinyu%20Xiao%0AAbstract%3A%20%20%20Identifying%20highlight%20moments%20of%20raw%20video%20materials%20is%20crucial%20for%20improving%0Athe%20efficiency%20of%20editing%20videos%20that%20are%20pervasive%20on%20internet%20platforms.%0AHowever%2C%20the%20extensive%20work%20of%20manually%20labeling%20footage%20has%20created%20obstacles%0Ato%20applying%20supervised%20methods%20to%20videos%20of%20unseen%20categories.%20The%20absence%20of%0Aan%20audio%20modality%20that%20contains%20valuable%20cues%20for%20highlight%20detection%20in%20many%0Avideos%20also%20makes%20it%20difficult%20to%20use%20multimodal%20strategies.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20model%20with%20cross-modal%20perception%20for%20unsupervised%20highlight%0Adetection.%20The%20proposed%20model%20learns%20representations%20with%20visual-audio%20level%0Asemantics%20from%20image-audio%20pair%20data%20via%20a%20self-reconstruction%20task.%20To%20achieve%0Aunsupervised%20highlight%20detection%2C%20we%20investigate%20the%20latent%20representations%20of%0Athe%20network%20and%20propose%20the%20representation%20activation%20sequence%20learning%20%28RASL%29%0Amodule%20with%20k-point%20contrastive%20learning%20to%20learn%20significant%20representation%0Aactivations.%20To%20connect%20the%20visual%20modality%20with%20the%20audio%20modality%2C%20we%20use%20the%0Asymmetric%20contrastive%20learning%20%28SCL%29%20module%20to%20learn%20the%20paired%20visual%20and%0Aaudio%20representations.%20Furthermore%2C%20an%20auxiliary%20task%20of%20masked%20feature%20vector%0Asequence%20%28FVS%29%20reconstruction%20is%20simultaneously%20conducted%20during%20pretraining%0Afor%20representation%20enhancement.%20During%20inference%2C%20the%20cross-modal%20pretrained%0Amodel%20can%20generate%20representations%20with%20paired%20visual-audio%20semantics%20given%0Aonly%20the%20visual%20modality.%20The%20RASL%20module%20is%20used%20to%20output%20the%20highlight%0Ascores.%20The%20experimental%20results%20show%20that%20the%20proposed%20framework%20achieves%0Asuperior%20performance%20compared%20to%20other%20state-of-the-art%20approaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.09401v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Unsupervised%20Modality-Transferable%20Video%20Highlight%20Detection%20with%0A%20%20Representation%20Activation%20Sequence%20Learning&entry.906535625=Tingtian%20Li%20and%20Zixun%20Sun%20and%20Xinyu%20Xiao&entry.1292438233=%20%20Identifying%20highlight%20moments%20of%20raw%20video%20materials%20is%20crucial%20for%20improving%0Athe%20efficiency%20of%20editing%20videos%20that%20are%20pervasive%20on%20internet%20platforms.%0AHowever%2C%20the%20extensive%20work%20of%20manually%20labeling%20footage%20has%20created%20obstacles%0Ato%20applying%20supervised%20methods%20to%20videos%20of%20unseen%20categories.%20The%20absence%20of%0Aan%20audio%20modality%20that%20contains%20valuable%20cues%20for%20highlight%20detection%20in%20many%0Avideos%20also%20makes%20it%20difficult%20to%20use%20multimodal%20strategies.%20In%20this%20paper%2C%20we%0Apropose%20a%20novel%20model%20with%20cross-modal%20perception%20for%20unsupervised%20highlight%0Adetection.%20The%20proposed%20model%20learns%20representations%20with%20visual-audio%20level%0Asemantics%20from%20image-audio%20pair%20data%20via%20a%20self-reconstruction%20task.%20To%20achieve%0Aunsupervised%20highlight%20detection%2C%20we%20investigate%20the%20latent%20representations%20of%0Athe%20network%20and%20propose%20the%20representation%20activation%20sequence%20learning%20%28RASL%29%0Amodule%20with%20k-point%20contrastive%20learning%20to%20learn%20significant%20representation%0Aactivations.%20To%20connect%20the%20visual%20modality%20with%20the%20audio%20modality%2C%20we%20use%20the%0Asymmetric%20contrastive%20learning%20%28SCL%29%20module%20to%20learn%20the%20paired%20visual%20and%0Aaudio%20representations.%20Furthermore%2C%20an%20auxiliary%20task%20of%20masked%20feature%20vector%0Asequence%20%28FVS%29%20reconstruction%20is%20simultaneously%20conducted%20during%20pretraining%0Afor%20representation%20enhancement.%20During%20inference%2C%20the%20cross-modal%20pretrained%0Amodel%20can%20generate%20representations%20with%20paired%20visual-audio%20semantics%20given%0Aonly%20the%20visual%20modality.%20The%20RASL%20module%20is%20used%20to%20output%20the%20highlight%0Ascores.%20The%20experimental%20results%20show%20that%20the%20proposed%20framework%20achieves%0Asuperior%20performance%20compared%20to%20other%20state-of-the-art%20approaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.09401v2&entry.124074799=Read"},
{"title": "Binary Noise for Binary Tasks: Masked Bernoulli Diffusion for\n  Unsupervised Anomaly Detection", "author": "Julia Wolleb and Florentin Bieder and Paul Friedrich and Peter Zhang and Alicia Durrer and Philippe C. Cattin", "abstract": "  The high performance of denoising diffusion models for image generation has\npaved the way for their application in unsupervised medical anomaly detection.\nAs diffusion-based methods require a lot of GPU memory and have long sampling\ntimes, we present a novel and fast unsupervised anomaly detection approach\nbased on latent Bernoulli diffusion models. We first apply an autoencoder to\ncompress the input images into a binary latent representation. Next, a\ndiffusion model that follows a Bernoulli noise schedule is employed to this\nlatent space and trained to restore binary latent representations from\nperturbed ones. The binary nature of this diffusion model allows us to identify\nentries in the latent space that have a high probability of flipping their\nbinary code during the denoising process, which indicates out-of-distribution\ndata. We propose a masking algorithm based on these probabilities, which\nimproves the anomaly detection scores. We achieve state-of-the-art performance\ncompared to other diffusion-based unsupervised anomaly detection algorithms\nwhile significantly reducing sampling time and memory consumption. The code is\navailable at https://github.com/JuliaWolleb/Anomaly_berdiff.\n", "link": "http://arxiv.org/abs/2403.11667v1", "date": "2024-03-18", "relevancy": 2.2238, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5688}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5624}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5076}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Binary%20Noise%20for%20Binary%20Tasks%3A%20Masked%20Bernoulli%20Diffusion%20for%0A%20%20Unsupervised%20Anomaly%20Detection&body=Title%3A%20Binary%20Noise%20for%20Binary%20Tasks%3A%20Masked%20Bernoulli%20Diffusion%20for%0A%20%20Unsupervised%20Anomaly%20Detection%0AAuthor%3A%20Julia%20Wolleb%20and%20Florentin%20Bieder%20and%20Paul%20Friedrich%20and%20Peter%20Zhang%20and%20Alicia%20Durrer%20and%20Philippe%20C.%20Cattin%0AAbstract%3A%20%20%20The%20high%20performance%20of%20denoising%20diffusion%20models%20for%20image%20generation%20has%0Apaved%20the%20way%20for%20their%20application%20in%20unsupervised%20medical%20anomaly%20detection.%0AAs%20diffusion-based%20methods%20require%20a%20lot%20of%20GPU%20memory%20and%20have%20long%20sampling%0Atimes%2C%20we%20present%20a%20novel%20and%20fast%20unsupervised%20anomaly%20detection%20approach%0Abased%20on%20latent%20Bernoulli%20diffusion%20models.%20We%20first%20apply%20an%20autoencoder%20to%0Acompress%20the%20input%20images%20into%20a%20binary%20latent%20representation.%20Next%2C%20a%0Adiffusion%20model%20that%20follows%20a%20Bernoulli%20noise%20schedule%20is%20employed%20to%20this%0Alatent%20space%20and%20trained%20to%20restore%20binary%20latent%20representations%20from%0Aperturbed%20ones.%20The%20binary%20nature%20of%20this%20diffusion%20model%20allows%20us%20to%20identify%0Aentries%20in%20the%20latent%20space%20that%20have%20a%20high%20probability%20of%20flipping%20their%0Abinary%20code%20during%20the%20denoising%20process%2C%20which%20indicates%20out-of-distribution%0Adata.%20We%20propose%20a%20masking%20algorithm%20based%20on%20these%20probabilities%2C%20which%0Aimproves%20the%20anomaly%20detection%20scores.%20We%20achieve%20state-of-the-art%20performance%0Acompared%20to%20other%20diffusion-based%20unsupervised%20anomaly%20detection%20algorithms%0Awhile%20significantly%20reducing%20sampling%20time%20and%20memory%20consumption.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/JuliaWolleb/Anomaly_berdiff.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11667v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Binary%20Noise%20for%20Binary%20Tasks%3A%20Masked%20Bernoulli%20Diffusion%20for%0A%20%20Unsupervised%20Anomaly%20Detection&entry.906535625=Julia%20Wolleb%20and%20Florentin%20Bieder%20and%20Paul%20Friedrich%20and%20Peter%20Zhang%20and%20Alicia%20Durrer%20and%20Philippe%20C.%20Cattin&entry.1292438233=%20%20The%20high%20performance%20of%20denoising%20diffusion%20models%20for%20image%20generation%20has%0Apaved%20the%20way%20for%20their%20application%20in%20unsupervised%20medical%20anomaly%20detection.%0AAs%20diffusion-based%20methods%20require%20a%20lot%20of%20GPU%20memory%20and%20have%20long%20sampling%0Atimes%2C%20we%20present%20a%20novel%20and%20fast%20unsupervised%20anomaly%20detection%20approach%0Abased%20on%20latent%20Bernoulli%20diffusion%20models.%20We%20first%20apply%20an%20autoencoder%20to%0Acompress%20the%20input%20images%20into%20a%20binary%20latent%20representation.%20Next%2C%20a%0Adiffusion%20model%20that%20follows%20a%20Bernoulli%20noise%20schedule%20is%20employed%20to%20this%0Alatent%20space%20and%20trained%20to%20restore%20binary%20latent%20representations%20from%0Aperturbed%20ones.%20The%20binary%20nature%20of%20this%20diffusion%20model%20allows%20us%20to%20identify%0Aentries%20in%20the%20latent%20space%20that%20have%20a%20high%20probability%20of%20flipping%20their%0Abinary%20code%20during%20the%20denoising%20process%2C%20which%20indicates%20out-of-distribution%0Adata.%20We%20propose%20a%20masking%20algorithm%20based%20on%20these%20probabilities%2C%20which%0Aimproves%20the%20anomaly%20detection%20scores.%20We%20achieve%20state-of-the-art%20performance%0Acompared%20to%20other%20diffusion-based%20unsupervised%20anomaly%20detection%20algorithms%0Awhile%20significantly%20reducing%20sampling%20time%20and%20memory%20consumption.%20The%20code%20is%0Aavailable%20at%20https%3A//github.com/JuliaWolleb/Anomaly_berdiff.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11667v1&entry.124074799=Read"},
{"title": "Exploring 3D-aware Latent Spaces for Efficiently Learning Numerous\n  Scenes", "author": "Antoine Schnepf and Karim Kassab and Jean-Yves Franceschi and Laurent Caraffa and Flavian Vasile and Jeremie Mary and Andrew Comport and Val\u00e9rie Gouet-Brunet", "abstract": "  We present a method enabling the scaling of NeRFs to learn a large number of\nsemantically-similar scenes. We combine two techniques to improve the required\ntraining time and memory cost per scene. First, we learn a 3D-aware latent\nspace in which we train Tri-Plane scene representations, hence reducing the\nresolution at which scenes are learned. Moreover, we present a way to share\ncommon information across scenes, hence allowing for a reduction of model\ncomplexity to learn a particular scene. Our method reduces effective per-scene\nmemory costs by 44% and per-scene time costs by 86% when training 1000 scenes.\nOur project page can be found at https://3da-ae.github.io .\n", "link": "http://arxiv.org/abs/2403.11678v1", "date": "2024-03-18", "relevancy": 2.2225, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5772}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5692}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5286}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Exploring%203D-aware%20Latent%20Spaces%20for%20Efficiently%20Learning%20Numerous%0A%20%20Scenes&body=Title%3A%20Exploring%203D-aware%20Latent%20Spaces%20for%20Efficiently%20Learning%20Numerous%0A%20%20Scenes%0AAuthor%3A%20Antoine%20Schnepf%20and%20Karim%20Kassab%20and%20Jean-Yves%20Franceschi%20and%20Laurent%20Caraffa%20and%20Flavian%20Vasile%20and%20Jeremie%20Mary%20and%20Andrew%20Comport%20and%20Val%C3%A9rie%20Gouet-Brunet%0AAbstract%3A%20%20%20We%20present%20a%20method%20enabling%20the%20scaling%20of%20NeRFs%20to%20learn%20a%20large%20number%20of%0Asemantically-similar%20scenes.%20We%20combine%20two%20techniques%20to%20improve%20the%20required%0Atraining%20time%20and%20memory%20cost%20per%20scene.%20First%2C%20we%20learn%20a%203D-aware%20latent%0Aspace%20in%20which%20we%20train%20Tri-Plane%20scene%20representations%2C%20hence%20reducing%20the%0Aresolution%20at%20which%20scenes%20are%20learned.%20Moreover%2C%20we%20present%20a%20way%20to%20share%0Acommon%20information%20across%20scenes%2C%20hence%20allowing%20for%20a%20reduction%20of%20model%0Acomplexity%20to%20learn%20a%20particular%20scene.%20Our%20method%20reduces%20effective%20per-scene%0Amemory%20costs%20by%2044%25%20and%20per-scene%20time%20costs%20by%2086%25%20when%20training%201000%20scenes.%0AOur%20project%20page%20can%20be%20found%20at%20https%3A//3da-ae.github.io%20.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11678v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Exploring%203D-aware%20Latent%20Spaces%20for%20Efficiently%20Learning%20Numerous%0A%20%20Scenes&entry.906535625=Antoine%20Schnepf%20and%20Karim%20Kassab%20and%20Jean-Yves%20Franceschi%20and%20Laurent%20Caraffa%20and%20Flavian%20Vasile%20and%20Jeremie%20Mary%20and%20Andrew%20Comport%20and%20Val%C3%A9rie%20Gouet-Brunet&entry.1292438233=%20%20We%20present%20a%20method%20enabling%20the%20scaling%20of%20NeRFs%20to%20learn%20a%20large%20number%20of%0Asemantically-similar%20scenes.%20We%20combine%20two%20techniques%20to%20improve%20the%20required%0Atraining%20time%20and%20memory%20cost%20per%20scene.%20First%2C%20we%20learn%20a%203D-aware%20latent%0Aspace%20in%20which%20we%20train%20Tri-Plane%20scene%20representations%2C%20hence%20reducing%20the%0Aresolution%20at%20which%20scenes%20are%20learned.%20Moreover%2C%20we%20present%20a%20way%20to%20share%0Acommon%20information%20across%20scenes%2C%20hence%20allowing%20for%20a%20reduction%20of%20model%0Acomplexity%20to%20learn%20a%20particular%20scene.%20Our%20method%20reduces%20effective%20per-scene%0Amemory%20costs%20by%2044%25%20and%20per-scene%20time%20costs%20by%2086%25%20when%20training%201000%20scenes.%0AOur%20project%20page%20can%20be%20found%20at%20https%3A//3da-ae.github.io%20.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11678v1&entry.124074799=Read"},
{"title": "LoRA-Composer: Leveraging Low-Rank Adaptation for Multi-Concept\n  Customization in Training-Free Diffusion Models", "author": "Yang Yang and Wen Wang and Liang Peng and Chaotian Song and Yao Chen and Hengjia Li and Xiaolong Yang and Qinglin Lu and Deng Cai and Boxi Wu and Wei Liu", "abstract": "  Customization generation techniques have significantly advanced the synthesis\nof specific concepts across varied contexts. Multi-concept customization\nemerges as the challenging task within this domain. Existing approaches often\nrely on training a Low-Rank Adaptations (LoRA) fusion matrix of multiple LoRA\nto merge various concepts into a single image. However, we identify this\nstraightforward method faces two major challenges: 1) concept confusion, which\noccurs when the model cannot preserve distinct individual characteristics, and\n2) concept vanishing, where the model fails to generate the intended subjects.\nTo address these issues, we introduce LoRA-Composer, a training-free framework\ndesigned for seamlessly integrating multiple LoRAs, thereby enhancing the\nharmony among different concepts within generated images. LoRA-Composer\naddresses concept vanishing through Concept Injection Constraints, enhancing\nconcept visibility via an expanded cross-attention mechanism. To combat concept\nconfusion, Concept Isolation Constraints are introduced, refining the\nself-attention computation. Furthermore, Latent Re-initialization is proposed\nto effectively stimulate concept-specific latent within designated regions. Our\nextensive testing showcases a notable enhancement in LoRA-Composer's\nperformance compared to standard baselines, especially when eliminating the\nimage-based conditions like canny edge or pose estimations. Code is released at\nhttps://github.com/Young98CN/LoRA\\_Composer.\n", "link": "http://arxiv.org/abs/2403.11627v1", "date": "2024-03-18", "relevancy": 2.2215, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.6141}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5483}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5389}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LoRA-Composer%3A%20Leveraging%20Low-Rank%20Adaptation%20for%20Multi-Concept%0A%20%20Customization%20in%20Training-Free%20Diffusion%20Models&body=Title%3A%20LoRA-Composer%3A%20Leveraging%20Low-Rank%20Adaptation%20for%20Multi-Concept%0A%20%20Customization%20in%20Training-Free%20Diffusion%20Models%0AAuthor%3A%20Yang%20Yang%20and%20Wen%20Wang%20and%20Liang%20Peng%20and%20Chaotian%20Song%20and%20Yao%20Chen%20and%20Hengjia%20Li%20and%20Xiaolong%20Yang%20and%20Qinglin%20Lu%20and%20Deng%20Cai%20and%20Boxi%20Wu%20and%20Wei%20Liu%0AAbstract%3A%20%20%20Customization%20generation%20techniques%20have%20significantly%20advanced%20the%20synthesis%0Aof%20specific%20concepts%20across%20varied%20contexts.%20Multi-concept%20customization%0Aemerges%20as%20the%20challenging%20task%20within%20this%20domain.%20Existing%20approaches%20often%0Arely%20on%20training%20a%20Low-Rank%20Adaptations%20%28LoRA%29%20fusion%20matrix%20of%20multiple%20LoRA%0Ato%20merge%20various%20concepts%20into%20a%20single%20image.%20However%2C%20we%20identify%20this%0Astraightforward%20method%20faces%20two%20major%20challenges%3A%201%29%20concept%20confusion%2C%20which%0Aoccurs%20when%20the%20model%20cannot%20preserve%20distinct%20individual%20characteristics%2C%20and%0A2%29%20concept%20vanishing%2C%20where%20the%20model%20fails%20to%20generate%20the%20intended%20subjects.%0ATo%20address%20these%20issues%2C%20we%20introduce%20LoRA-Composer%2C%20a%20training-free%20framework%0Adesigned%20for%20seamlessly%20integrating%20multiple%20LoRAs%2C%20thereby%20enhancing%20the%0Aharmony%20among%20different%20concepts%20within%20generated%20images.%20LoRA-Composer%0Aaddresses%20concept%20vanishing%20through%20Concept%20Injection%20Constraints%2C%20enhancing%0Aconcept%20visibility%20via%20an%20expanded%20cross-attention%20mechanism.%20To%20combat%20concept%0Aconfusion%2C%20Concept%20Isolation%20Constraints%20are%20introduced%2C%20refining%20the%0Aself-attention%20computation.%20Furthermore%2C%20Latent%20Re-initialization%20is%20proposed%0Ato%20effectively%20stimulate%20concept-specific%20latent%20within%20designated%20regions.%20Our%0Aextensive%20testing%20showcases%20a%20notable%20enhancement%20in%20LoRA-Composer%27s%0Aperformance%20compared%20to%20standard%20baselines%2C%20especially%20when%20eliminating%20the%0Aimage-based%20conditions%20like%20canny%20edge%20or%20pose%20estimations.%20Code%20is%20released%20at%0Ahttps%3A//github.com/Young98CN/LoRA%5C_Composer.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11627v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LoRA-Composer%3A%20Leveraging%20Low-Rank%20Adaptation%20for%20Multi-Concept%0A%20%20Customization%20in%20Training-Free%20Diffusion%20Models&entry.906535625=Yang%20Yang%20and%20Wen%20Wang%20and%20Liang%20Peng%20and%20Chaotian%20Song%20and%20Yao%20Chen%20and%20Hengjia%20Li%20and%20Xiaolong%20Yang%20and%20Qinglin%20Lu%20and%20Deng%20Cai%20and%20Boxi%20Wu%20and%20Wei%20Liu&entry.1292438233=%20%20Customization%20generation%20techniques%20have%20significantly%20advanced%20the%20synthesis%0Aof%20specific%20concepts%20across%20varied%20contexts.%20Multi-concept%20customization%0Aemerges%20as%20the%20challenging%20task%20within%20this%20domain.%20Existing%20approaches%20often%0Arely%20on%20training%20a%20Low-Rank%20Adaptations%20%28LoRA%29%20fusion%20matrix%20of%20multiple%20LoRA%0Ato%20merge%20various%20concepts%20into%20a%20single%20image.%20However%2C%20we%20identify%20this%0Astraightforward%20method%20faces%20two%20major%20challenges%3A%201%29%20concept%20confusion%2C%20which%0Aoccurs%20when%20the%20model%20cannot%20preserve%20distinct%20individual%20characteristics%2C%20and%0A2%29%20concept%20vanishing%2C%20where%20the%20model%20fails%20to%20generate%20the%20intended%20subjects.%0ATo%20address%20these%20issues%2C%20we%20introduce%20LoRA-Composer%2C%20a%20training-free%20framework%0Adesigned%20for%20seamlessly%20integrating%20multiple%20LoRAs%2C%20thereby%20enhancing%20the%0Aharmony%20among%20different%20concepts%20within%20generated%20images.%20LoRA-Composer%0Aaddresses%20concept%20vanishing%20through%20Concept%20Injection%20Constraints%2C%20enhancing%0Aconcept%20visibility%20via%20an%20expanded%20cross-attention%20mechanism.%20To%20combat%20concept%0Aconfusion%2C%20Concept%20Isolation%20Constraints%20are%20introduced%2C%20refining%20the%0Aself-attention%20computation.%20Furthermore%2C%20Latent%20Re-initialization%20is%20proposed%0Ato%20effectively%20stimulate%20concept-specific%20latent%20within%20designated%20regions.%20Our%0Aextensive%20testing%20showcases%20a%20notable%20enhancement%20in%20LoRA-Composer%27s%0Aperformance%20compared%20to%20standard%20baselines%2C%20especially%20when%20eliminating%20the%0Aimage-based%20conditions%20like%20canny%20edge%20or%20pose%20estimations.%20Code%20is%20released%20at%0Ahttps%3A//github.com/Young98CN/LoRA%5C_Composer.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11627v1&entry.124074799=Read"},
{"title": "Better (pseudo-)labels for semi-supervised instance segmentation", "author": "Fran\u00e7ois Porcher and Camille Couprie and Marc Szafraniec and Jakob Verbeek", "abstract": "  Despite the availability of large datasets for tasks like image\nclassification and image-text alignment, labeled data for more complex\nrecognition tasks, such as detection and segmentation, is less abundant. In\nparticular, for instance segmentation annotations are time-consuming to\nproduce, and the distribution of instances is often highly skewed across\nclasses. While semi-supervised teacher-student distillation methods show\npromise in leveraging vast amounts of unlabeled data, they suffer from\nmiscalibration, resulting in overconfidence in frequently represented classes\nand underconfidence in rarer ones. Additionally, these methods encounter\ndifficulties in efficiently learning from a limited set of examples. We\nintroduce a dual-strategy to enhance the teacher model's training process,\nsubstantially improving the performance on few-shot learning. Secondly, we\npropose a calibration correction mechanism that that enables the student model\nto correct the teacher's calibration errors. Using our approach, we observed\nmarked improvements over a state-of-the-art supervised baseline performance on\nthe LVIS dataset, with an increase of 2.8% in average precision (AP) and 10.3%\ngain in AP for rare classes.\n", "link": "http://arxiv.org/abs/2403.11675v1", "date": "2024-03-18", "relevancy": 2.2113, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5616}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5585}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.5436}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Better%20%28pseudo-%29labels%20for%20semi-supervised%20instance%20segmentation&body=Title%3A%20Better%20%28pseudo-%29labels%20for%20semi-supervised%20instance%20segmentation%0AAuthor%3A%20Fran%C3%A7ois%20Porcher%20and%20Camille%20Couprie%20and%20Marc%20Szafraniec%20and%20Jakob%20Verbeek%0AAbstract%3A%20%20%20Despite%20the%20availability%20of%20large%20datasets%20for%20tasks%20like%20image%0Aclassification%20and%20image-text%20alignment%2C%20labeled%20data%20for%20more%20complex%0Arecognition%20tasks%2C%20such%20as%20detection%20and%20segmentation%2C%20is%20less%20abundant.%20In%0Aparticular%2C%20for%20instance%20segmentation%20annotations%20are%20time-consuming%20to%0Aproduce%2C%20and%20the%20distribution%20of%20instances%20is%20often%20highly%20skewed%20across%0Aclasses.%20While%20semi-supervised%20teacher-student%20distillation%20methods%20show%0Apromise%20in%20leveraging%20vast%20amounts%20of%20unlabeled%20data%2C%20they%20suffer%20from%0Amiscalibration%2C%20resulting%20in%20overconfidence%20in%20frequently%20represented%20classes%0Aand%20underconfidence%20in%20rarer%20ones.%20Additionally%2C%20these%20methods%20encounter%0Adifficulties%20in%20efficiently%20learning%20from%20a%20limited%20set%20of%20examples.%20We%0Aintroduce%20a%20dual-strategy%20to%20enhance%20the%20teacher%20model%27s%20training%20process%2C%0Asubstantially%20improving%20the%20performance%20on%20few-shot%20learning.%20Secondly%2C%20we%0Apropose%20a%20calibration%20correction%20mechanism%20that%20that%20enables%20the%20student%20model%0Ato%20correct%20the%20teacher%27s%20calibration%20errors.%20Using%20our%20approach%2C%20we%20observed%0Amarked%20improvements%20over%20a%20state-of-the-art%20supervised%20baseline%20performance%20on%0Athe%20LVIS%20dataset%2C%20with%20an%20increase%20of%202.8%25%20in%20average%20precision%20%28AP%29%20and%2010.3%25%0Again%20in%20AP%20for%20rare%20classes.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11675v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Better%20%28pseudo-%29labels%20for%20semi-supervised%20instance%20segmentation&entry.906535625=Fran%C3%A7ois%20Porcher%20and%20Camille%20Couprie%20and%20Marc%20Szafraniec%20and%20Jakob%20Verbeek&entry.1292438233=%20%20Despite%20the%20availability%20of%20large%20datasets%20for%20tasks%20like%20image%0Aclassification%20and%20image-text%20alignment%2C%20labeled%20data%20for%20more%20complex%0Arecognition%20tasks%2C%20such%20as%20detection%20and%20segmentation%2C%20is%20less%20abundant.%20In%0Aparticular%2C%20for%20instance%20segmentation%20annotations%20are%20time-consuming%20to%0Aproduce%2C%20and%20the%20distribution%20of%20instances%20is%20often%20highly%20skewed%20across%0Aclasses.%20While%20semi-supervised%20teacher-student%20distillation%20methods%20show%0Apromise%20in%20leveraging%20vast%20amounts%20of%20unlabeled%20data%2C%20they%20suffer%20from%0Amiscalibration%2C%20resulting%20in%20overconfidence%20in%20frequently%20represented%20classes%0Aand%20underconfidence%20in%20rarer%20ones.%20Additionally%2C%20these%20methods%20encounter%0Adifficulties%20in%20efficiently%20learning%20from%20a%20limited%20set%20of%20examples.%20We%0Aintroduce%20a%20dual-strategy%20to%20enhance%20the%20teacher%20model%27s%20training%20process%2C%0Asubstantially%20improving%20the%20performance%20on%20few-shot%20learning.%20Secondly%2C%20we%0Apropose%20a%20calibration%20correction%20mechanism%20that%20that%20enables%20the%20student%20model%0Ato%20correct%20the%20teacher%27s%20calibration%20errors.%20Using%20our%20approach%2C%20we%20observed%0Amarked%20improvements%20over%20a%20state-of-the-art%20supervised%20baseline%20performance%20on%0Athe%20LVIS%20dataset%2C%20with%20an%20increase%20of%202.8%25%20in%20average%20precision%20%28AP%29%20and%2010.3%25%0Again%20in%20AP%20for%20rare%20classes.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11675v1&entry.124074799=Read"},
{"title": "LocalStyleFool: Regional Video Style Transfer Attack Using Segment\n  Anything Model", "author": "Yuxin Cao and Jinghao Li and Xi Xiao and Derui Wang and Minhui Xue and Hao Ge and Wei Liu and Guangwu Hu", "abstract": "  Previous work has shown that well-crafted adversarial perturbations can\nthreaten the security of video recognition systems. Attackers can invade such\nmodels with a low query budget when the perturbations are semantic-invariant,\nsuch as StyleFool. Despite the query efficiency, the naturalness of the minutia\nareas still requires amelioration, since StyleFool leverages style transfer to\nall pixels in each frame. To close the gap, we propose LocalStyleFool, an\nimproved black-box video adversarial attack that superimposes regional\nstyle-transfer-based perturbations on videos. Benefiting from the popularity\nand scalably usability of Segment Anything Model (SAM), we first extract\ndifferent regions according to semantic information and then track them through\nthe video stream to maintain the temporal consistency. Then, we add\nstyle-transfer-based perturbations to several regions selected based on the\nassociative criterion of transfer-based gradient information and regional area.\nPerturbation fine adjustment is followed to make stylized videos adversarial.\nWe demonstrate that LocalStyleFool can improve both intra-frame and inter-frame\nnaturalness through a human-assessed survey, while maintaining competitive\nfooling rate and query efficiency. Successful experiments on the\nhigh-resolution dataset also showcase that scrupulous segmentation of SAM helps\nto improve the scalability of adversarial attacks under high-resolution data.\n", "link": "http://arxiv.org/abs/2403.11656v1", "date": "2024-03-18", "relevancy": 2.1419, "topK": [{"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5892}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5475}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.502}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20LocalStyleFool%3A%20Regional%20Video%20Style%20Transfer%20Attack%20Using%20Segment%0A%20%20Anything%20Model&body=Title%3A%20LocalStyleFool%3A%20Regional%20Video%20Style%20Transfer%20Attack%20Using%20Segment%0A%20%20Anything%20Model%0AAuthor%3A%20Yuxin%20Cao%20and%20Jinghao%20Li%20and%20Xi%20Xiao%20and%20Derui%20Wang%20and%20Minhui%20Xue%20and%20Hao%20Ge%20and%20Wei%20Liu%20and%20Guangwu%20Hu%0AAbstract%3A%20%20%20Previous%20work%20has%20shown%20that%20well-crafted%20adversarial%20perturbations%20can%0Athreaten%20the%20security%20of%20video%20recognition%20systems.%20Attackers%20can%20invade%20such%0Amodels%20with%20a%20low%20query%20budget%20when%20the%20perturbations%20are%20semantic-invariant%2C%0Asuch%20as%20StyleFool.%20Despite%20the%20query%20efficiency%2C%20the%20naturalness%20of%20the%20minutia%0Aareas%20still%20requires%20amelioration%2C%20since%20StyleFool%20leverages%20style%20transfer%20to%0Aall%20pixels%20in%20each%20frame.%20To%20close%20the%20gap%2C%20we%20propose%20LocalStyleFool%2C%20an%0Aimproved%20black-box%20video%20adversarial%20attack%20that%20superimposes%20regional%0Astyle-transfer-based%20perturbations%20on%20videos.%20Benefiting%20from%20the%20popularity%0Aand%20scalably%20usability%20of%20Segment%20Anything%20Model%20%28SAM%29%2C%20we%20first%20extract%0Adifferent%20regions%20according%20to%20semantic%20information%20and%20then%20track%20them%20through%0Athe%20video%20stream%20to%20maintain%20the%20temporal%20consistency.%20Then%2C%20we%20add%0Astyle-transfer-based%20perturbations%20to%20several%20regions%20selected%20based%20on%20the%0Aassociative%20criterion%20of%20transfer-based%20gradient%20information%20and%20regional%20area.%0APerturbation%20fine%20adjustment%20is%20followed%20to%20make%20stylized%20videos%20adversarial.%0AWe%20demonstrate%20that%20LocalStyleFool%20can%20improve%20both%20intra-frame%20and%20inter-frame%0Anaturalness%20through%20a%20human-assessed%20survey%2C%20while%20maintaining%20competitive%0Afooling%20rate%20and%20query%20efficiency.%20Successful%20experiments%20on%20the%0Ahigh-resolution%20dataset%20also%20showcase%20that%20scrupulous%20segmentation%20of%20SAM%20helps%0Ato%20improve%20the%20scalability%20of%20adversarial%20attacks%20under%20high-resolution%20data.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11656v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=LocalStyleFool%3A%20Regional%20Video%20Style%20Transfer%20Attack%20Using%20Segment%0A%20%20Anything%20Model&entry.906535625=Yuxin%20Cao%20and%20Jinghao%20Li%20and%20Xi%20Xiao%20and%20Derui%20Wang%20and%20Minhui%20Xue%20and%20Hao%20Ge%20and%20Wei%20Liu%20and%20Guangwu%20Hu&entry.1292438233=%20%20Previous%20work%20has%20shown%20that%20well-crafted%20adversarial%20perturbations%20can%0Athreaten%20the%20security%20of%20video%20recognition%20systems.%20Attackers%20can%20invade%20such%0Amodels%20with%20a%20low%20query%20budget%20when%20the%20perturbations%20are%20semantic-invariant%2C%0Asuch%20as%20StyleFool.%20Despite%20the%20query%20efficiency%2C%20the%20naturalness%20of%20the%20minutia%0Aareas%20still%20requires%20amelioration%2C%20since%20StyleFool%20leverages%20style%20transfer%20to%0Aall%20pixels%20in%20each%20frame.%20To%20close%20the%20gap%2C%20we%20propose%20LocalStyleFool%2C%20an%0Aimproved%20black-box%20video%20adversarial%20attack%20that%20superimposes%20regional%0Astyle-transfer-based%20perturbations%20on%20videos.%20Benefiting%20from%20the%20popularity%0Aand%20scalably%20usability%20of%20Segment%20Anything%20Model%20%28SAM%29%2C%20we%20first%20extract%0Adifferent%20regions%20according%20to%20semantic%20information%20and%20then%20track%20them%20through%0Athe%20video%20stream%20to%20maintain%20the%20temporal%20consistency.%20Then%2C%20we%20add%0Astyle-transfer-based%20perturbations%20to%20several%20regions%20selected%20based%20on%20the%0Aassociative%20criterion%20of%20transfer-based%20gradient%20information%20and%20regional%20area.%0APerturbation%20fine%20adjustment%20is%20followed%20to%20make%20stylized%20videos%20adversarial.%0AWe%20demonstrate%20that%20LocalStyleFool%20can%20improve%20both%20intra-frame%20and%20inter-frame%0Anaturalness%20through%20a%20human-assessed%20survey%2C%20while%20maintaining%20competitive%0Afooling%20rate%20and%20query%20efficiency.%20Successful%20experiments%20on%20the%0Ahigh-resolution%20dataset%20also%20showcase%20that%20scrupulous%20segmentation%20of%20SAM%20helps%0Ato%20improve%20the%20scalability%20of%20adversarial%20attacks%20under%20high-resolution%20data.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11656v1&entry.124074799=Read"},
{"title": "GaussNav: Gaussian Splatting for Visual Navigation", "author": "Xiaohan Lei and Min Wang and Wengang Zhou and Houqiang Li", "abstract": "  In embodied vision, Instance ImageGoal Navigation (IIN) requires an agent to\nlocate a specific object depicted in a goal image within an unexplored\nenvironment. The primary difficulty of IIN stems from the necessity of\nrecognizing the target object across varying viewpoints and rejecting potential\ndistractors.\n  Existing map-based navigation methods largely adopt the representation form\nof Bird's Eye View (BEV) maps, which, however, lack the representation of\ndetailed textures in a scene.\n  To address the above issues, we propose a new Gaussian Splatting Navigation\n(abbreviated as GaussNav) framework for IIN task, which constructs a novel map\nrepresentation based on 3D Gaussian Splatting (3DGS).\n  The proposed framework enables the agent to not only memorize the geometry\nand semantic information of the scene, but also retain the textural features of\nobjects.\n  Our GaussNav framework demonstrates a significant leap in performance,\nevidenced by an increase in Success weighted by Path Length (SPL) from 0.252 to\n0.578 on the challenging Habitat-Matterport 3D (HM3D) dataset.\n  Our code will be made publicly available.\n", "link": "http://arxiv.org/abs/2403.11625v1", "date": "2024-03-18", "relevancy": 2.1311, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5556}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.5204}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5068}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20GaussNav%3A%20Gaussian%20Splatting%20for%20Visual%20Navigation&body=Title%3A%20GaussNav%3A%20Gaussian%20Splatting%20for%20Visual%20Navigation%0AAuthor%3A%20Xiaohan%20Lei%20and%20Min%20Wang%20and%20Wengang%20Zhou%20and%20Houqiang%20Li%0AAbstract%3A%20%20%20In%20embodied%20vision%2C%20Instance%20ImageGoal%20Navigation%20%28IIN%29%20requires%20an%20agent%20to%0Alocate%20a%20specific%20object%20depicted%20in%20a%20goal%20image%20within%20an%20unexplored%0Aenvironment.%20The%20primary%20difficulty%20of%20IIN%20stems%20from%20the%20necessity%20of%0Arecognizing%20the%20target%20object%20across%20varying%20viewpoints%20and%20rejecting%20potential%0Adistractors.%0A%20%20Existing%20map-based%20navigation%20methods%20largely%20adopt%20the%20representation%20form%0Aof%20Bird%27s%20Eye%20View%20%28BEV%29%20maps%2C%20which%2C%20however%2C%20lack%20the%20representation%20of%0Adetailed%20textures%20in%20a%20scene.%0A%20%20To%20address%20the%20above%20issues%2C%20we%20propose%20a%20new%20Gaussian%20Splatting%20Navigation%0A%28abbreviated%20as%20GaussNav%29%20framework%20for%20IIN%20task%2C%20which%20constructs%20a%20novel%20map%0Arepresentation%20based%20on%203D%20Gaussian%20Splatting%20%283DGS%29.%0A%20%20The%20proposed%20framework%20enables%20the%20agent%20to%20not%20only%20memorize%20the%20geometry%0Aand%20semantic%20information%20of%20the%20scene%2C%20but%20also%20retain%20the%20textural%20features%20of%0Aobjects.%0A%20%20Our%20GaussNav%20framework%20demonstrates%20a%20significant%20leap%20in%20performance%2C%0Aevidenced%20by%20an%20increase%20in%20Success%20weighted%20by%20Path%20Length%20%28SPL%29%20from%200.252%20to%0A0.578%20on%20the%20challenging%20Habitat-Matterport%203D%20%28HM3D%29%20dataset.%0A%20%20Our%20code%20will%20be%20made%20publicly%20available.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11625v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=GaussNav%3A%20Gaussian%20Splatting%20for%20Visual%20Navigation&entry.906535625=Xiaohan%20Lei%20and%20Min%20Wang%20and%20Wengang%20Zhou%20and%20Houqiang%20Li&entry.1292438233=%20%20In%20embodied%20vision%2C%20Instance%20ImageGoal%20Navigation%20%28IIN%29%20requires%20an%20agent%20to%0Alocate%20a%20specific%20object%20depicted%20in%20a%20goal%20image%20within%20an%20unexplored%0Aenvironment.%20The%20primary%20difficulty%20of%20IIN%20stems%20from%20the%20necessity%20of%0Arecognizing%20the%20target%20object%20across%20varying%20viewpoints%20and%20rejecting%20potential%0Adistractors.%0A%20%20Existing%20map-based%20navigation%20methods%20largely%20adopt%20the%20representation%20form%0Aof%20Bird%27s%20Eye%20View%20%28BEV%29%20maps%2C%20which%2C%20however%2C%20lack%20the%20representation%20of%0Adetailed%20textures%20in%20a%20scene.%0A%20%20To%20address%20the%20above%20issues%2C%20we%20propose%20a%20new%20Gaussian%20Splatting%20Navigation%0A%28abbreviated%20as%20GaussNav%29%20framework%20for%20IIN%20task%2C%20which%20constructs%20a%20novel%20map%0Arepresentation%20based%20on%203D%20Gaussian%20Splatting%20%283DGS%29.%0A%20%20The%20proposed%20framework%20enables%20the%20agent%20to%20not%20only%20memorize%20the%20geometry%0Aand%20semantic%20information%20of%20the%20scene%2C%20but%20also%20retain%20the%20textural%20features%20of%0Aobjects.%0A%20%20Our%20GaussNav%20framework%20demonstrates%20a%20significant%20leap%20in%20performance%2C%0Aevidenced%20by%20an%20increase%20in%20Success%20weighted%20by%20Path%20Length%20%28SPL%29%20from%200.252%20to%0A0.578%20on%20the%20challenging%20Habitat-Matterport%203D%20%28HM3D%29%20dataset.%0A%20%20Our%20code%20will%20be%20made%20publicly%20available.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11625v1&entry.124074799=Read"},
{"title": "ECAMP: Entity-centered Context-aware Medical Vision Language\n  Pre-training", "author": "Rongsheng Wang and Qingsong Yao and Haoran Lai and Zhiyang He and Xiaodong Tao and Zihang Jiang and S. Kevin Zhou", "abstract": "  Despite significant advancements in medical vision-language pre-training,\nexisting methods have largely overlooked the inherent entity-specific context\nwithin radiology reports and the complex cross-modality contextual\nrelationships between text and images. To close this gap, we propose a novel\nEntity-centered Context-aware Medical Vision-language Pre-training (ECAMP)\nframework, which is designed to enable a more entity-centered and\ncontext-sensitive interpretation of medical data. Utilizing the recent powerful\nlarge language model, we distill entity-centered context from medical reports,\nwhich enables ECAMP to gain more effective supervision from the text modality.\nBy further pre-training our model with carefully designed entity-aware,\ncontext-enhanced masked language modeling and context-guided super-resolution\ntasks, ECAMP significantly refines the interplay between text and image\nmodalities, leading to an enhanced ability to extract entity-centered\ncontextual features. Besides, our proposed multi-scale context fusion design\nalso improves the semantic integration of both coarse and fine-level image\nrepresentations, prompting better performance for multi-scale downstream\napplications. Combining these components leads to significant performance leaps\nover current state-of-the-art methods and establishes a new standard for\ncross-modality learning in medical imaging, whose effectiveness is demonstrated\nby our extensive experiments on various tasks including classification,\nsegmentation, and detection across several public datasets. Code and models are\navailable at https://github.com/ToniChopp/ECAMP.\n", "link": "http://arxiv.org/abs/2312.13316v2", "date": "2024-03-18", "relevancy": 2.1163, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.571}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5059}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4964}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20ECAMP%3A%20Entity-centered%20Context-aware%20Medical%20Vision%20Language%0A%20%20Pre-training&body=Title%3A%20ECAMP%3A%20Entity-centered%20Context-aware%20Medical%20Vision%20Language%0A%20%20Pre-training%0AAuthor%3A%20Rongsheng%20Wang%20and%20Qingsong%20Yao%20and%20Haoran%20Lai%20and%20Zhiyang%20He%20and%20Xiaodong%20Tao%20and%20Zihang%20Jiang%20and%20S.%20Kevin%20Zhou%0AAbstract%3A%20%20%20Despite%20significant%20advancements%20in%20medical%20vision-language%20pre-training%2C%0Aexisting%20methods%20have%20largely%20overlooked%20the%20inherent%20entity-specific%20context%0Awithin%20radiology%20reports%20and%20the%20complex%20cross-modality%20contextual%0Arelationships%20between%20text%20and%20images.%20To%20close%20this%20gap%2C%20we%20propose%20a%20novel%0AEntity-centered%20Context-aware%20Medical%20Vision-language%20Pre-training%20%28ECAMP%29%0Aframework%2C%20which%20is%20designed%20to%20enable%20a%20more%20entity-centered%20and%0Acontext-sensitive%20interpretation%20of%20medical%20data.%20Utilizing%20the%20recent%20powerful%0Alarge%20language%20model%2C%20we%20distill%20entity-centered%20context%20from%20medical%20reports%2C%0Awhich%20enables%20ECAMP%20to%20gain%20more%20effective%20supervision%20from%20the%20text%20modality.%0ABy%20further%20pre-training%20our%20model%20with%20carefully%20designed%20entity-aware%2C%0Acontext-enhanced%20masked%20language%20modeling%20and%20context-guided%20super-resolution%0Atasks%2C%20ECAMP%20significantly%20refines%20the%20interplay%20between%20text%20and%20image%0Amodalities%2C%20leading%20to%20an%20enhanced%20ability%20to%20extract%20entity-centered%0Acontextual%20features.%20Besides%2C%20our%20proposed%20multi-scale%20context%20fusion%20design%0Aalso%20improves%20the%20semantic%20integration%20of%20both%20coarse%20and%20fine-level%20image%0Arepresentations%2C%20prompting%20better%20performance%20for%20multi-scale%20downstream%0Aapplications.%20Combining%20these%20components%20leads%20to%20significant%20performance%20leaps%0Aover%20current%20state-of-the-art%20methods%20and%20establishes%20a%20new%20standard%20for%0Across-modality%20learning%20in%20medical%20imaging%2C%20whose%20effectiveness%20is%20demonstrated%0Aby%20our%20extensive%20experiments%20on%20various%20tasks%20including%20classification%2C%0Asegmentation%2C%20and%20detection%20across%20several%20public%20datasets.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/ToniChopp/ECAMP.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.13316v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=ECAMP%3A%20Entity-centered%20Context-aware%20Medical%20Vision%20Language%0A%20%20Pre-training&entry.906535625=Rongsheng%20Wang%20and%20Qingsong%20Yao%20and%20Haoran%20Lai%20and%20Zhiyang%20He%20and%20Xiaodong%20Tao%20and%20Zihang%20Jiang%20and%20S.%20Kevin%20Zhou&entry.1292438233=%20%20Despite%20significant%20advancements%20in%20medical%20vision-language%20pre-training%2C%0Aexisting%20methods%20have%20largely%20overlooked%20the%20inherent%20entity-specific%20context%0Awithin%20radiology%20reports%20and%20the%20complex%20cross-modality%20contextual%0Arelationships%20between%20text%20and%20images.%20To%20close%20this%20gap%2C%20we%20propose%20a%20novel%0AEntity-centered%20Context-aware%20Medical%20Vision-language%20Pre-training%20%28ECAMP%29%0Aframework%2C%20which%20is%20designed%20to%20enable%20a%20more%20entity-centered%20and%0Acontext-sensitive%20interpretation%20of%20medical%20data.%20Utilizing%20the%20recent%20powerful%0Alarge%20language%20model%2C%20we%20distill%20entity-centered%20context%20from%20medical%20reports%2C%0Awhich%20enables%20ECAMP%20to%20gain%20more%20effective%20supervision%20from%20the%20text%20modality.%0ABy%20further%20pre-training%20our%20model%20with%20carefully%20designed%20entity-aware%2C%0Acontext-enhanced%20masked%20language%20modeling%20and%20context-guided%20super-resolution%0Atasks%2C%20ECAMP%20significantly%20refines%20the%20interplay%20between%20text%20and%20image%0Amodalities%2C%20leading%20to%20an%20enhanced%20ability%20to%20extract%20entity-centered%0Acontextual%20features.%20Besides%2C%20our%20proposed%20multi-scale%20context%20fusion%20design%0Aalso%20improves%20the%20semantic%20integration%20of%20both%20coarse%20and%20fine-level%20image%0Arepresentations%2C%20prompting%20better%20performance%20for%20multi-scale%20downstream%0Aapplications.%20Combining%20these%20components%20leads%20to%20significant%20performance%20leaps%0Aover%20current%20state-of-the-art%20methods%20and%20establishes%20a%20new%20standard%20for%0Across-modality%20learning%20in%20medical%20imaging%2C%20whose%20effectiveness%20is%20demonstrated%0Aby%20our%20extensive%20experiments%20on%20various%20tasks%20including%20classification%2C%0Asegmentation%2C%20and%20detection%20across%20several%20public%20datasets.%20Code%20and%20models%20are%0Aavailable%20at%20https%3A//github.com/ToniChopp/ECAMP.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.13316v2&entry.124074799=Read"},
{"title": "Arc2Face: A Foundation Model of Human Faces", "author": "Foivos Paraperas Papantoniou and Alexandros Lattas and Stylianos Moschoglou and Jiankang Deng and Bernhard Kainz and Stefanos Zafeiriou", "abstract": "  This paper presents Arc2Face, an identity-conditioned face foundation model,\nwhich, given the ArcFace embedding of a person, can generate diverse\nphoto-realistic images with an unparalleled degree of face similarity than\nexisting models. Despite previous attempts to decode face recognition features\ninto detailed images, we find that common high-resolution datasets (e.g. FFHQ)\nlack sufficient identities to reconstruct any subject. To that end, we\nmeticulously upsample a significant portion of the WebFace42M database, the\nlargest public dataset for face recognition (FR). Arc2Face builds upon a\npretrained Stable Diffusion model, yet adapts it to the task of ID-to-face\ngeneration, conditioned solely on ID vectors. Deviating from recent works that\ncombine ID with text embeddings for zero-shot personalization of text-to-image\nmodels, we emphasize on the compactness of FR features, which can fully capture\nthe essence of the human face, as opposed to hand-crafted prompts. Crucially,\ntext-augmented models struggle to decouple identity and text, usually\nnecessitating some description of the given face to achieve satisfactory\nsimilarity. Arc2Face, however, only needs the discriminative features of\nArcFace to guide the generation, offering a robust prior for a plethora of\ntasks where ID consistency is of paramount importance. As an example, we train\na FR model on synthetic images from our model and achieve superior performance\nto existing synthetic datasets.\n", "link": "http://arxiv.org/abs/2403.11641v1", "date": "2024-03-18", "relevancy": 2.106, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5743}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5049}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.4874}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Arc2Face%3A%20A%20Foundation%20Model%20of%20Human%20Faces&body=Title%3A%20Arc2Face%3A%20A%20Foundation%20Model%20of%20Human%20Faces%0AAuthor%3A%20Foivos%20Paraperas%20Papantoniou%20and%20Alexandros%20Lattas%20and%20Stylianos%20Moschoglou%20and%20Jiankang%20Deng%20and%20Bernhard%20Kainz%20and%20Stefanos%20Zafeiriou%0AAbstract%3A%20%20%20This%20paper%20presents%20Arc2Face%2C%20an%20identity-conditioned%20face%20foundation%20model%2C%0Awhich%2C%20given%20the%20ArcFace%20embedding%20of%20a%20person%2C%20can%20generate%20diverse%0Aphoto-realistic%20images%20with%20an%20unparalleled%20degree%20of%20face%20similarity%20than%0Aexisting%20models.%20Despite%20previous%20attempts%20to%20decode%20face%20recognition%20features%0Ainto%20detailed%20images%2C%20we%20find%20that%20common%20high-resolution%20datasets%20%28e.g.%20FFHQ%29%0Alack%20sufficient%20identities%20to%20reconstruct%20any%20subject.%20To%20that%20end%2C%20we%0Ameticulously%20upsample%20a%20significant%20portion%20of%20the%20WebFace42M%20database%2C%20the%0Alargest%20public%20dataset%20for%20face%20recognition%20%28FR%29.%20Arc2Face%20builds%20upon%20a%0Apretrained%20Stable%20Diffusion%20model%2C%20yet%20adapts%20it%20to%20the%20task%20of%20ID-to-face%0Ageneration%2C%20conditioned%20solely%20on%20ID%20vectors.%20Deviating%20from%20recent%20works%20that%0Acombine%20ID%20with%20text%20embeddings%20for%20zero-shot%20personalization%20of%20text-to-image%0Amodels%2C%20we%20emphasize%20on%20the%20compactness%20of%20FR%20features%2C%20which%20can%20fully%20capture%0Athe%20essence%20of%20the%20human%20face%2C%20as%20opposed%20to%20hand-crafted%20prompts.%20Crucially%2C%0Atext-augmented%20models%20struggle%20to%20decouple%20identity%20and%20text%2C%20usually%0Anecessitating%20some%20description%20of%20the%20given%20face%20to%20achieve%20satisfactory%0Asimilarity.%20Arc2Face%2C%20however%2C%20only%20needs%20the%20discriminative%20features%20of%0AArcFace%20to%20guide%20the%20generation%2C%20offering%20a%20robust%20prior%20for%20a%20plethora%20of%0Atasks%20where%20ID%20consistency%20is%20of%20paramount%20importance.%20As%20an%20example%2C%20we%20train%0Aa%20FR%20model%20on%20synthetic%20images%20from%20our%20model%20and%20achieve%20superior%20performance%0Ato%20existing%20synthetic%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11641v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Arc2Face%3A%20A%20Foundation%20Model%20of%20Human%20Faces&entry.906535625=Foivos%20Paraperas%20Papantoniou%20and%20Alexandros%20Lattas%20and%20Stylianos%20Moschoglou%20and%20Jiankang%20Deng%20and%20Bernhard%20Kainz%20and%20Stefanos%20Zafeiriou&entry.1292438233=%20%20This%20paper%20presents%20Arc2Face%2C%20an%20identity-conditioned%20face%20foundation%20model%2C%0Awhich%2C%20given%20the%20ArcFace%20embedding%20of%20a%20person%2C%20can%20generate%20diverse%0Aphoto-realistic%20images%20with%20an%20unparalleled%20degree%20of%20face%20similarity%20than%0Aexisting%20models.%20Despite%20previous%20attempts%20to%20decode%20face%20recognition%20features%0Ainto%20detailed%20images%2C%20we%20find%20that%20common%20high-resolution%20datasets%20%28e.g.%20FFHQ%29%0Alack%20sufficient%20identities%20to%20reconstruct%20any%20subject.%20To%20that%20end%2C%20we%0Ameticulously%20upsample%20a%20significant%20portion%20of%20the%20WebFace42M%20database%2C%20the%0Alargest%20public%20dataset%20for%20face%20recognition%20%28FR%29.%20Arc2Face%20builds%20upon%20a%0Apretrained%20Stable%20Diffusion%20model%2C%20yet%20adapts%20it%20to%20the%20task%20of%20ID-to-face%0Ageneration%2C%20conditioned%20solely%20on%20ID%20vectors.%20Deviating%20from%20recent%20works%20that%0Acombine%20ID%20with%20text%20embeddings%20for%20zero-shot%20personalization%20of%20text-to-image%0Amodels%2C%20we%20emphasize%20on%20the%20compactness%20of%20FR%20features%2C%20which%20can%20fully%20capture%0Athe%20essence%20of%20the%20human%20face%2C%20as%20opposed%20to%20hand-crafted%20prompts.%20Crucially%2C%0Atext-augmented%20models%20struggle%20to%20decouple%20identity%20and%20text%2C%20usually%0Anecessitating%20some%20description%20of%20the%20given%20face%20to%20achieve%20satisfactory%0Asimilarity.%20Arc2Face%2C%20however%2C%20only%20needs%20the%20discriminative%20features%20of%0AArcFace%20to%20guide%20the%20generation%2C%20offering%20a%20robust%20prior%20for%20a%20plethora%20of%0Atasks%20where%20ID%20consistency%20is%20of%20paramount%20importance.%20As%20an%20example%2C%20we%20train%0Aa%20FR%20model%20on%20synthetic%20images%20from%20our%20model%20and%20achieve%20superior%20performance%0Ato%20existing%20synthetic%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11641v1&entry.124074799=Read"},
{"title": "WIA-LD2ND: Wavelet-based Image Alignment for Self-supervised Low-Dose CT\n  Denoising", "author": "Haoyu Zhao and Guyu Liang and Zhou Zhao and Bo Du and Yongchao Xu and Rui Yu", "abstract": "  In clinical examinations and diagnoses, low-dose computed tomography (LDCT)\nis crucial for minimizing health risks compared with normal-dose computed\ntomography (NDCT). However, reducing the radiation dose compromises the\nsignal-to-noise ratio, leading to degraded quality of CT images. To address\nthis, we analyze LDCT denoising task based on experimental results from the\nfrequency perspective, and then introduce a novel self-supervised CT image\ndenoising method called WIA-LD2ND, only using NDCT data. The proposed WIA-LD2ND\ncomprises two modules: Wavelet-based Image Alignment (WIA) and Frequency-Aware\nMulti-scale Loss (FAM). First, WIA is introduced to align NDCT with LDCT by\nmainly adding noise to the high-frequency components, which is the main\ndifference between LDCT and NDCT. Second, to better capture high-frequency\ncomponents and detailed information, Frequency-Aware Multi-scale Loss (FAM) is\nproposed by effectively utilizing multi-scale feature space. Extensive\nexperiments on two public LDCT denoising datasets demonstrate that our\nWIA-LD2ND, only uses NDCT, outperforms existing several state-of-the-art\nweakly-supervised and self-supervised methods.\n", "link": "http://arxiv.org/abs/2403.11672v1", "date": "2024-03-18", "relevancy": 2.0938, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5539}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.5058}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4914}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20WIA-LD2ND%3A%20Wavelet-based%20Image%20Alignment%20for%20Self-supervised%20Low-Dose%20CT%0A%20%20Denoising&body=Title%3A%20WIA-LD2ND%3A%20Wavelet-based%20Image%20Alignment%20for%20Self-supervised%20Low-Dose%20CT%0A%20%20Denoising%0AAuthor%3A%20Haoyu%20Zhao%20and%20Guyu%20Liang%20and%20Zhou%20Zhao%20and%20Bo%20Du%20and%20Yongchao%20Xu%20and%20Rui%20Yu%0AAbstract%3A%20%20%20In%20clinical%20examinations%20and%20diagnoses%2C%20low-dose%20computed%20tomography%20%28LDCT%29%0Ais%20crucial%20for%20minimizing%20health%20risks%20compared%20with%20normal-dose%20computed%0Atomography%20%28NDCT%29.%20However%2C%20reducing%20the%20radiation%20dose%20compromises%20the%0Asignal-to-noise%20ratio%2C%20leading%20to%20degraded%20quality%20of%20CT%20images.%20To%20address%0Athis%2C%20we%20analyze%20LDCT%20denoising%20task%20based%20on%20experimental%20results%20from%20the%0Afrequency%20perspective%2C%20and%20then%20introduce%20a%20novel%20self-supervised%20CT%20image%0Adenoising%20method%20called%20WIA-LD2ND%2C%20only%20using%20NDCT%20data.%20The%20proposed%20WIA-LD2ND%0Acomprises%20two%20modules%3A%20Wavelet-based%20Image%20Alignment%20%28WIA%29%20and%20Frequency-Aware%0AMulti-scale%20Loss%20%28FAM%29.%20First%2C%20WIA%20is%20introduced%20to%20align%20NDCT%20with%20LDCT%20by%0Amainly%20adding%20noise%20to%20the%20high-frequency%20components%2C%20which%20is%20the%20main%0Adifference%20between%20LDCT%20and%20NDCT.%20Second%2C%20to%20better%20capture%20high-frequency%0Acomponents%20and%20detailed%20information%2C%20Frequency-Aware%20Multi-scale%20Loss%20%28FAM%29%20is%0Aproposed%20by%20effectively%20utilizing%20multi-scale%20feature%20space.%20Extensive%0Aexperiments%20on%20two%20public%20LDCT%20denoising%20datasets%20demonstrate%20that%20our%0AWIA-LD2ND%2C%20only%20uses%20NDCT%2C%20outperforms%20existing%20several%20state-of-the-art%0Aweakly-supervised%20and%20self-supervised%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11672v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=WIA-LD2ND%3A%20Wavelet-based%20Image%20Alignment%20for%20Self-supervised%20Low-Dose%20CT%0A%20%20Denoising&entry.906535625=Haoyu%20Zhao%20and%20Guyu%20Liang%20and%20Zhou%20Zhao%20and%20Bo%20Du%20and%20Yongchao%20Xu%20and%20Rui%20Yu&entry.1292438233=%20%20In%20clinical%20examinations%20and%20diagnoses%2C%20low-dose%20computed%20tomography%20%28LDCT%29%0Ais%20crucial%20for%20minimizing%20health%20risks%20compared%20with%20normal-dose%20computed%0Atomography%20%28NDCT%29.%20However%2C%20reducing%20the%20radiation%20dose%20compromises%20the%0Asignal-to-noise%20ratio%2C%20leading%20to%20degraded%20quality%20of%20CT%20images.%20To%20address%0Athis%2C%20we%20analyze%20LDCT%20denoising%20task%20based%20on%20experimental%20results%20from%20the%0Afrequency%20perspective%2C%20and%20then%20introduce%20a%20novel%20self-supervised%20CT%20image%0Adenoising%20method%20called%20WIA-LD2ND%2C%20only%20using%20NDCT%20data.%20The%20proposed%20WIA-LD2ND%0Acomprises%20two%20modules%3A%20Wavelet-based%20Image%20Alignment%20%28WIA%29%20and%20Frequency-Aware%0AMulti-scale%20Loss%20%28FAM%29.%20First%2C%20WIA%20is%20introduced%20to%20align%20NDCT%20with%20LDCT%20by%0Amainly%20adding%20noise%20to%20the%20high-frequency%20components%2C%20which%20is%20the%20main%0Adifference%20between%20LDCT%20and%20NDCT.%20Second%2C%20to%20better%20capture%20high-frequency%0Acomponents%20and%20detailed%20information%2C%20Frequency-Aware%20Multi-scale%20Loss%20%28FAM%29%20is%0Aproposed%20by%20effectively%20utilizing%20multi-scale%20feature%20space.%20Extensive%0Aexperiments%20on%20two%20public%20LDCT%20denoising%20datasets%20demonstrate%20that%20our%0AWIA-LD2ND%2C%20only%20uses%20NDCT%2C%20outperforms%20existing%20several%20state-of-the-art%0Aweakly-supervised%20and%20self-supervised%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11672v1&entry.124074799=Read"},
{"title": "Collaborative Aquatic Positioning System Utilising Multi-beam Sonar and\n  Depth Sensors", "author": "{Xueliang Cheng and Barry Lennox and Keir Groves", "abstract": "  Accurate positioning of remotely operated underwater vehicles (ROVs) in\nconfined environments is crucial for inspection and mapping tasks and is also a\nprerequisite for autonomous operations. Presently, there are no positioning\nsystems available that are suited for real-world use in confined underwater\nenvironments, unconstrained by environmental lighting and water turbidity\nlevels and have sufficient accuracy for long-term, reliable and repeatable\nnavigation. This shortage presents a significant barrier to enhancing the\ncapabilities of ROVs in such scenarios. This paper introduces an innovative\npositioning system for ROVs operating in confined, cluttered underwater\nsettings, achieved through the collaboration of an omnidirectional surface\nvehicle and an ROV. A formulation is proposed and evaluated in the simulation\nagainst ground truth. The experimental results from the simulation form a proof\nof principle of the proposed system and also demonstrate its deployability.\nUnlike many previous approaches, the system does not rely on fixed\ninfrastructure or tracking of features in the environment and can cover large\nenclosed areas without additional equipment.\n", "link": "http://arxiv.org/abs/2403.10397v2", "date": "2024-03-18", "relevancy": 2.0853, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5401}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5202}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.503}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Collaborative%20Aquatic%20Positioning%20System%20Utilising%20Multi-beam%20Sonar%20and%0A%20%20Depth%20Sensors&body=Title%3A%20Collaborative%20Aquatic%20Positioning%20System%20Utilising%20Multi-beam%20Sonar%20and%0A%20%20Depth%20Sensors%0AAuthor%3A%20%7BXueliang%20Cheng%20and%20Barry%20Lennox%20and%20Keir%20Groves%0AAbstract%3A%20%20%20Accurate%20positioning%20of%20remotely%20operated%20underwater%20vehicles%20%28ROVs%29%20in%0Aconfined%20environments%20is%20crucial%20for%20inspection%20and%20mapping%20tasks%20and%20is%20also%20a%0Aprerequisite%20for%20autonomous%20operations.%20Presently%2C%20there%20are%20no%20positioning%0Asystems%20available%20that%20are%20suited%20for%20real-world%20use%20in%20confined%20underwater%0Aenvironments%2C%20unconstrained%20by%20environmental%20lighting%20and%20water%20turbidity%0Alevels%20and%20have%20sufficient%20accuracy%20for%20long-term%2C%20reliable%20and%20repeatable%0Anavigation.%20This%20shortage%20presents%20a%20significant%20barrier%20to%20enhancing%20the%0Acapabilities%20of%20ROVs%20in%20such%20scenarios.%20This%20paper%20introduces%20an%20innovative%0Apositioning%20system%20for%20ROVs%20operating%20in%20confined%2C%20cluttered%20underwater%0Asettings%2C%20achieved%20through%20the%20collaboration%20of%20an%20omnidirectional%20surface%0Avehicle%20and%20an%20ROV.%20A%20formulation%20is%20proposed%20and%20evaluated%20in%20the%20simulation%0Aagainst%20ground%20truth.%20The%20experimental%20results%20from%20the%20simulation%20form%20a%20proof%0Aof%20principle%20of%20the%20proposed%20system%20and%20also%20demonstrate%20its%20deployability.%0AUnlike%20many%20previous%20approaches%2C%20the%20system%20does%20not%20rely%20on%20fixed%0Ainfrastructure%20or%20tracking%20of%20features%20in%20the%20environment%20and%20can%20cover%20large%0Aenclosed%20areas%20without%20additional%20equipment.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10397v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Collaborative%20Aquatic%20Positioning%20System%20Utilising%20Multi-beam%20Sonar%20and%0A%20%20Depth%20Sensors&entry.906535625=%7BXueliang%20Cheng%20and%20Barry%20Lennox%20and%20Keir%20Groves&entry.1292438233=%20%20Accurate%20positioning%20of%20remotely%20operated%20underwater%20vehicles%20%28ROVs%29%20in%0Aconfined%20environments%20is%20crucial%20for%20inspection%20and%20mapping%20tasks%20and%20is%20also%20a%0Aprerequisite%20for%20autonomous%20operations.%20Presently%2C%20there%20are%20no%20positioning%0Asystems%20available%20that%20are%20suited%20for%20real-world%20use%20in%20confined%20underwater%0Aenvironments%2C%20unconstrained%20by%20environmental%20lighting%20and%20water%20turbidity%0Alevels%20and%20have%20sufficient%20accuracy%20for%20long-term%2C%20reliable%20and%20repeatable%0Anavigation.%20This%20shortage%20presents%20a%20significant%20barrier%20to%20enhancing%20the%0Acapabilities%20of%20ROVs%20in%20such%20scenarios.%20This%20paper%20introduces%20an%20innovative%0Apositioning%20system%20for%20ROVs%20operating%20in%20confined%2C%20cluttered%20underwater%0Asettings%2C%20achieved%20through%20the%20collaboration%20of%20an%20omnidirectional%20surface%0Avehicle%20and%20an%20ROV.%20A%20formulation%20is%20proposed%20and%20evaluated%20in%20the%20simulation%0Aagainst%20ground%20truth.%20The%20experimental%20results%20from%20the%20simulation%20form%20a%20proof%0Aof%20principle%20of%20the%20proposed%20system%20and%20also%20demonstrate%20its%20deployability.%0AUnlike%20many%20previous%20approaches%2C%20the%20system%20does%20not%20rely%20on%20fixed%0Ainfrastructure%20or%20tracking%20of%20features%20in%20the%20environment%20and%20can%20cover%20large%0Aenclosed%20areas%20without%20additional%20equipment.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10397v2&entry.124074799=Read"},
{"title": "TrajectoryNAS: A Neural Architecture Search for Trajectory Prediction", "author": "Ali Asghar Sharifi and Ali Zoljodi and Masoud Daneshtalab", "abstract": "  Autonomous driving systems are a rapidly evolving technology that enables\ndriverless car production. Trajectory prediction is a critical component of\nautonomous driving systems, enabling cars to anticipate the movements of\nsurrounding objects for safe navigation. Trajectory prediction using Lidar\npoint-cloud data performs better than 2D images due to providing 3D\ninformation. However, processing point-cloud data is more complicated and\ntime-consuming than 2D images. Hence, state-of-the-art 3D trajectory\npredictions using point-cloud data suffer from slow and erroneous predictions.\nThis paper introduces TrajectoryNAS, a pioneering method that focuses on\nutilizing point cloud data for trajectory prediction. By leveraging Neural\nArchitecture Search (NAS), TrajectoryNAS automates the design of trajectory\nprediction models, encompassing object detection, tracking, and forecasting in\na cohesive manner. This approach not only addresses the complex\ninterdependencies among these tasks but also emphasizes the importance of\naccuracy and efficiency in trajectory modeling. Through empirical studies,\nTrajectoryNAS demonstrates its effectiveness in enhancing the performance of\nautonomous driving systems, marking a significant advancement in the\nfield.Experimental results reveal that TrajcetoryNAS yield a minimum of 4.8\nhigger accuracy and 1.1* lower latency over competing methods on the NuScenes\ndataset.\n", "link": "http://arxiv.org/abs/2403.11695v1", "date": "2024-03-18", "relevancy": 2.075, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5325}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5286}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5034}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20TrajectoryNAS%3A%20A%20Neural%20Architecture%20Search%20for%20Trajectory%20Prediction&body=Title%3A%20TrajectoryNAS%3A%20A%20Neural%20Architecture%20Search%20for%20Trajectory%20Prediction%0AAuthor%3A%20Ali%20Asghar%20Sharifi%20and%20Ali%20Zoljodi%20and%20Masoud%20Daneshtalab%0AAbstract%3A%20%20%20Autonomous%20driving%20systems%20are%20a%20rapidly%20evolving%20technology%20that%20enables%0Adriverless%20car%20production.%20Trajectory%20prediction%20is%20a%20critical%20component%20of%0Aautonomous%20driving%20systems%2C%20enabling%20cars%20to%20anticipate%20the%20movements%20of%0Asurrounding%20objects%20for%20safe%20navigation.%20Trajectory%20prediction%20using%20Lidar%0Apoint-cloud%20data%20performs%20better%20than%202D%20images%20due%20to%20providing%203D%0Ainformation.%20However%2C%20processing%20point-cloud%20data%20is%20more%20complicated%20and%0Atime-consuming%20than%202D%20images.%20Hence%2C%20state-of-the-art%203D%20trajectory%0Apredictions%20using%20point-cloud%20data%20suffer%20from%20slow%20and%20erroneous%20predictions.%0AThis%20paper%20introduces%20TrajectoryNAS%2C%20a%20pioneering%20method%20that%20focuses%20on%0Autilizing%20point%20cloud%20data%20for%20trajectory%20prediction.%20By%20leveraging%20Neural%0AArchitecture%20Search%20%28NAS%29%2C%20TrajectoryNAS%20automates%20the%20design%20of%20trajectory%0Aprediction%20models%2C%20encompassing%20object%20detection%2C%20tracking%2C%20and%20forecasting%20in%0Aa%20cohesive%20manner.%20This%20approach%20not%20only%20addresses%20the%20complex%0Ainterdependencies%20among%20these%20tasks%20but%20also%20emphasizes%20the%20importance%20of%0Aaccuracy%20and%20efficiency%20in%20trajectory%20modeling.%20Through%20empirical%20studies%2C%0ATrajectoryNAS%20demonstrates%20its%20effectiveness%20in%20enhancing%20the%20performance%20of%0Aautonomous%20driving%20systems%2C%20marking%20a%20significant%20advancement%20in%20the%0Afield.Experimental%20results%20reveal%20that%20TrajcetoryNAS%20yield%20a%20minimum%20of%204.8%0Ahigger%20accuracy%20and%201.1%2A%20lower%20latency%20over%20competing%20methods%20on%20the%20NuScenes%0Adataset.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11695v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=TrajectoryNAS%3A%20A%20Neural%20Architecture%20Search%20for%20Trajectory%20Prediction&entry.906535625=Ali%20Asghar%20Sharifi%20and%20Ali%20Zoljodi%20and%20Masoud%20Daneshtalab&entry.1292438233=%20%20Autonomous%20driving%20systems%20are%20a%20rapidly%20evolving%20technology%20that%20enables%0Adriverless%20car%20production.%20Trajectory%20prediction%20is%20a%20critical%20component%20of%0Aautonomous%20driving%20systems%2C%20enabling%20cars%20to%20anticipate%20the%20movements%20of%0Asurrounding%20objects%20for%20safe%20navigation.%20Trajectory%20prediction%20using%20Lidar%0Apoint-cloud%20data%20performs%20better%20than%202D%20images%20due%20to%20providing%203D%0Ainformation.%20However%2C%20processing%20point-cloud%20data%20is%20more%20complicated%20and%0Atime-consuming%20than%202D%20images.%20Hence%2C%20state-of-the-art%203D%20trajectory%0Apredictions%20using%20point-cloud%20data%20suffer%20from%20slow%20and%20erroneous%20predictions.%0AThis%20paper%20introduces%20TrajectoryNAS%2C%20a%20pioneering%20method%20that%20focuses%20on%0Autilizing%20point%20cloud%20data%20for%20trajectory%20prediction.%20By%20leveraging%20Neural%0AArchitecture%20Search%20%28NAS%29%2C%20TrajectoryNAS%20automates%20the%20design%20of%20trajectory%0Aprediction%20models%2C%20encompassing%20object%20detection%2C%20tracking%2C%20and%20forecasting%20in%0Aa%20cohesive%20manner.%20This%20approach%20not%20only%20addresses%20the%20complex%0Ainterdependencies%20among%20these%20tasks%20but%20also%20emphasizes%20the%20importance%20of%0Aaccuracy%20and%20efficiency%20in%20trajectory%20modeling.%20Through%20empirical%20studies%2C%0ATrajectoryNAS%20demonstrates%20its%20effectiveness%20in%20enhancing%20the%20performance%20of%0Aautonomous%20driving%20systems%2C%20marking%20a%20significant%20advancement%20in%20the%0Afield.Experimental%20results%20reveal%20that%20TrajcetoryNAS%20yield%20a%20minimum%20of%204.8%0Ahigger%20accuracy%20and%201.1%2A%20lower%20latency%20over%20competing%20methods%20on%20the%20NuScenes%0Adataset.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11695v1&entry.124074799=Read"},
{"title": "Stop Reasoning! When Multimodal LLMs with Chain-of-Thought Reasoning\n  Meets Adversarial Images", "author": "Zefeng Wang and Zhen Han and Shuo Chen and Fan Xue and Zifeng Ding and Xun Xiao and Volker Tresp and Philip Torr and Jindong Gu", "abstract": "  Recently, Multimodal LLMs (MLLMs) have shown a great ability to understand\nimages. However, like traditional vision models, they are still vulnerable to\nadversarial images. Meanwhile, Chain-of-Thought (CoT) reasoning has been widely\nexplored on MLLMs, which not only improves model's performance, but also\nenhances model's explainability by giving intermediate reasoning steps.\nNevertheless, there is still a lack of study regarding MLLMs' adversarial\nrobustness with CoT and an understanding of what the rationale looks like when\nMLLMs infer wrong answers with adversarial images. Our research evaluates the\nadversarial robustness of MLLMs when employing CoT reasoning, finding that CoT\nmarginally improves adversarial robustness against existing attack methods.\nMoreover, we introduce a novel stop-reasoning attack technique that effectively\nbypasses the CoT-induced robustness enhancements. Finally, we demonstrate the\nalterations in CoT reasoning when MLLMs confront adversarial images, shedding\nlight on their reasoning process under adversarial attacks.\n", "link": "http://arxiv.org/abs/2402.14899v2", "date": "2024-03-18", "relevancy": 2.0705, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5418}, {"title": "Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance", "link": "http://openaccess.thecvf.com/content/ICCV2023/html/Fan_Flexible_Visual_Recognition_by_Evidential_Modeling_of_Confusion_and_Ignorance_ICCV_2023_paper.html", "similarity": 0.5041}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.491}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Stop%20Reasoning%21%20When%20Multimodal%20LLMs%20with%20Chain-of-Thought%20Reasoning%0A%20%20Meets%20Adversarial%20Images&body=Title%3A%20Stop%20Reasoning%21%20When%20Multimodal%20LLMs%20with%20Chain-of-Thought%20Reasoning%0A%20%20Meets%20Adversarial%20Images%0AAuthor%3A%20Zefeng%20Wang%20and%20Zhen%20Han%20and%20Shuo%20Chen%20and%20Fan%20Xue%20and%20Zifeng%20Ding%20and%20Xun%20Xiao%20and%20Volker%20Tresp%20and%20Philip%20Torr%20and%20Jindong%20Gu%0AAbstract%3A%20%20%20Recently%2C%20Multimodal%20LLMs%20%28MLLMs%29%20have%20shown%20a%20great%20ability%20to%20understand%0Aimages.%20However%2C%20like%20traditional%20vision%20models%2C%20they%20are%20still%20vulnerable%20to%0Aadversarial%20images.%20Meanwhile%2C%20Chain-of-Thought%20%28CoT%29%20reasoning%20has%20been%20widely%0Aexplored%20on%20MLLMs%2C%20which%20not%20only%20improves%20model%27s%20performance%2C%20but%20also%0Aenhances%20model%27s%20explainability%20by%20giving%20intermediate%20reasoning%20steps.%0ANevertheless%2C%20there%20is%20still%20a%20lack%20of%20study%20regarding%20MLLMs%27%20adversarial%0Arobustness%20with%20CoT%20and%20an%20understanding%20of%20what%20the%20rationale%20looks%20like%20when%0AMLLMs%20infer%20wrong%20answers%20with%20adversarial%20images.%20Our%20research%20evaluates%20the%0Aadversarial%20robustness%20of%20MLLMs%20when%20employing%20CoT%20reasoning%2C%20finding%20that%20CoT%0Amarginally%20improves%20adversarial%20robustness%20against%20existing%20attack%20methods.%0AMoreover%2C%20we%20introduce%20a%20novel%20stop-reasoning%20attack%20technique%20that%20effectively%0Abypasses%20the%20CoT-induced%20robustness%20enhancements.%20Finally%2C%20we%20demonstrate%20the%0Aalterations%20in%20CoT%20reasoning%20when%20MLLMs%20confront%20adversarial%20images%2C%20shedding%0Alight%20on%20their%20reasoning%20process%20under%20adversarial%20attacks.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14899v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Stop%20Reasoning%21%20When%20Multimodal%20LLMs%20with%20Chain-of-Thought%20Reasoning%0A%20%20Meets%20Adversarial%20Images&entry.906535625=Zefeng%20Wang%20and%20Zhen%20Han%20and%20Shuo%20Chen%20and%20Fan%20Xue%20and%20Zifeng%20Ding%20and%20Xun%20Xiao%20and%20Volker%20Tresp%20and%20Philip%20Torr%20and%20Jindong%20Gu&entry.1292438233=%20%20Recently%2C%20Multimodal%20LLMs%20%28MLLMs%29%20have%20shown%20a%20great%20ability%20to%20understand%0Aimages.%20However%2C%20like%20traditional%20vision%20models%2C%20they%20are%20still%20vulnerable%20to%0Aadversarial%20images.%20Meanwhile%2C%20Chain-of-Thought%20%28CoT%29%20reasoning%20has%20been%20widely%0Aexplored%20on%20MLLMs%2C%20which%20not%20only%20improves%20model%27s%20performance%2C%20but%20also%0Aenhances%20model%27s%20explainability%20by%20giving%20intermediate%20reasoning%20steps.%0ANevertheless%2C%20there%20is%20still%20a%20lack%20of%20study%20regarding%20MLLMs%27%20adversarial%0Arobustness%20with%20CoT%20and%20an%20understanding%20of%20what%20the%20rationale%20looks%20like%20when%0AMLLMs%20infer%20wrong%20answers%20with%20adversarial%20images.%20Our%20research%20evaluates%20the%0Aadversarial%20robustness%20of%20MLLMs%20when%20employing%20CoT%20reasoning%2C%20finding%20that%20CoT%0Amarginally%20improves%20adversarial%20robustness%20against%20existing%20attack%20methods.%0AMoreover%2C%20we%20introduce%20a%20novel%20stop-reasoning%20attack%20technique%20that%20effectively%0Abypasses%20the%20CoT-induced%20robustness%20enhancements.%20Finally%2C%20we%20demonstrate%20the%0Aalterations%20in%20CoT%20reasoning%20when%20MLLMs%20confront%20adversarial%20images%2C%20shedding%0Alight%20on%20their%20reasoning%20process%20under%20adversarial%20attacks.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14899v2&entry.124074799=Read"},
{"title": "MoreStyle: Relax Low-frequency Constraint of Fourier-based Image\n  Reconstruction in Generalizable Medical Image Segmentation", "author": "Haoyu Zhao and Wenhui Dong and Rui Yu and Zhou Zhao and Du Bo and Yongchao Xu", "abstract": "  The task of single-source domain generalization (SDG) in medical image\nsegmentation is crucial due to frequent domain shifts in clinical image\ndatasets. To address the challenge of poor generalization across different\ndomains, we introduce a Plug-and-Play module for data augmentation called\nMoreStyle. MoreStyle diversifies image styles by relaxing low-frequency\nconstraints in Fourier space, guiding the image reconstruction network. With\nthe help of adversarial learning, MoreStyle further expands the style range and\npinpoints the most intricate style combinations within latent features. To\nhandle significant style variations, we introduce an uncertainty-weighted loss.\nThis loss emphasizes hard-to-classify pixels resulting only from style shifts\nwhile mitigating true hard-to-classify pixels in both MoreStyle-generated and\noriginal images. Extensive experiments on two widely used benchmarks\ndemonstrate that the proposed MoreStyle effectively helps to achieve good\ndomain generalization ability, and has the potential to further boost the\nperformance of some state-of-the-art SDG methods.\n", "link": "http://arxiv.org/abs/2403.11689v1", "date": "2024-03-18", "relevancy": 2.0659, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.5231}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5172}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.5096}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MoreStyle%3A%20Relax%20Low-frequency%20Constraint%20of%20Fourier-based%20Image%0A%20%20Reconstruction%20in%20Generalizable%20Medical%20Image%20Segmentation&body=Title%3A%20MoreStyle%3A%20Relax%20Low-frequency%20Constraint%20of%20Fourier-based%20Image%0A%20%20Reconstruction%20in%20Generalizable%20Medical%20Image%20Segmentation%0AAuthor%3A%20Haoyu%20Zhao%20and%20Wenhui%20Dong%20and%20Rui%20Yu%20and%20Zhou%20Zhao%20and%20Du%20Bo%20and%20Yongchao%20Xu%0AAbstract%3A%20%20%20The%20task%20of%20single-source%20domain%20generalization%20%28SDG%29%20in%20medical%20image%0Asegmentation%20is%20crucial%20due%20to%20frequent%20domain%20shifts%20in%20clinical%20image%0Adatasets.%20To%20address%20the%20challenge%20of%20poor%20generalization%20across%20different%0Adomains%2C%20we%20introduce%20a%20Plug-and-Play%20module%20for%20data%20augmentation%20called%0AMoreStyle.%20MoreStyle%20diversifies%20image%20styles%20by%20relaxing%20low-frequency%0Aconstraints%20in%20Fourier%20space%2C%20guiding%20the%20image%20reconstruction%20network.%20With%0Athe%20help%20of%20adversarial%20learning%2C%20MoreStyle%20further%20expands%20the%20style%20range%20and%0Apinpoints%20the%20most%20intricate%20style%20combinations%20within%20latent%20features.%20To%0Ahandle%20significant%20style%20variations%2C%20we%20introduce%20an%20uncertainty-weighted%20loss.%0AThis%20loss%20emphasizes%20hard-to-classify%20pixels%20resulting%20only%20from%20style%20shifts%0Awhile%20mitigating%20true%20hard-to-classify%20pixels%20in%20both%20MoreStyle-generated%20and%0Aoriginal%20images.%20Extensive%20experiments%20on%20two%20widely%20used%20benchmarks%0Ademonstrate%20that%20the%20proposed%20MoreStyle%20effectively%20helps%20to%20achieve%20good%0Adomain%20generalization%20ability%2C%20and%20has%20the%20potential%20to%20further%20boost%20the%0Aperformance%20of%20some%20state-of-the-art%20SDG%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11689v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MoreStyle%3A%20Relax%20Low-frequency%20Constraint%20of%20Fourier-based%20Image%0A%20%20Reconstruction%20in%20Generalizable%20Medical%20Image%20Segmentation&entry.906535625=Haoyu%20Zhao%20and%20Wenhui%20Dong%20and%20Rui%20Yu%20and%20Zhou%20Zhao%20and%20Du%20Bo%20and%20Yongchao%20Xu&entry.1292438233=%20%20The%20task%20of%20single-source%20domain%20generalization%20%28SDG%29%20in%20medical%20image%0Asegmentation%20is%20crucial%20due%20to%20frequent%20domain%20shifts%20in%20clinical%20image%0Adatasets.%20To%20address%20the%20challenge%20of%20poor%20generalization%20across%20different%0Adomains%2C%20we%20introduce%20a%20Plug-and-Play%20module%20for%20data%20augmentation%20called%0AMoreStyle.%20MoreStyle%20diversifies%20image%20styles%20by%20relaxing%20low-frequency%0Aconstraints%20in%20Fourier%20space%2C%20guiding%20the%20image%20reconstruction%20network.%20With%0Athe%20help%20of%20adversarial%20learning%2C%20MoreStyle%20further%20expands%20the%20style%20range%20and%0Apinpoints%20the%20most%20intricate%20style%20combinations%20within%20latent%20features.%20To%0Ahandle%20significant%20style%20variations%2C%20we%20introduce%20an%20uncertainty-weighted%20loss.%0AThis%20loss%20emphasizes%20hard-to-classify%20pixels%20resulting%20only%20from%20style%20shifts%0Awhile%20mitigating%20true%20hard-to-classify%20pixels%20in%20both%20MoreStyle-generated%20and%0Aoriginal%20images.%20Extensive%20experiments%20on%20two%20widely%20used%20benchmarks%0Ademonstrate%20that%20the%20proposed%20MoreStyle%20effectively%20helps%20to%20achieve%20good%0Adomain%20generalization%20ability%2C%20and%20has%20the%20potential%20to%20further%20boost%20the%0Aperformance%20of%20some%20state-of-the-art%20SDG%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11689v1&entry.124074799=Read"},
{"title": "Prioritized Semantic Learning for Zero-shot Instance Navigation", "author": "Xander Sun and Louis Lau and Hoyard Zhi and Ronghe Qiu and Junwei Liang", "abstract": "  We study zero-shot instance navigation, in which the agent navigates to a\nspecific object without using object annotations for training. Previous object\nnavigation approaches apply the image-goal navigation (ImageNav) task (go to\nthe location of an image) for pretraining, and transfer the agent to achieve\nobject goals using a vision-language model. However, these approaches lead to\nissues of semantic neglect, where the model fails to learn meaningful semantic\nalignments. In this paper, we propose a Prioritized Semantic Learning (PSL)\nmethod to improve the semantic understanding ability of navigation agents.\nSpecifically, a semantic-enhanced PSL agent is proposed and a prioritized\nsemantic training strategy is introduced to select goal images that exhibit\nclear semantic supervision and relax the reward function from strict exact view\nmatching. At inference time, a semantic expansion inference scheme is designed\nto preserve the same granularity level of the goal-semantic as training.\nFurthermore, for the popular HM3D environment, we present an Instance\nNavigation (InstanceNav) task that requires going to a specific object instance\nwith detailed descriptions, as opposed to the Object Navigation (ObjectNav)\ntask where the goal is defined merely by the object category. Our PSL agent\noutperforms the previous state-of-the-art by 66% on zero-shot ObjectNav in\nterms of success rate and is also superior on the new InstanceNav task. Code\nwill be released at https://anonymous.4open. science/r/PSL/.\n", "link": "http://arxiv.org/abs/2403.11650v1", "date": "2024-03-18", "relevancy": 2.0501, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.53}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.5156}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5024}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Prioritized%20Semantic%20Learning%20for%20Zero-shot%20Instance%20Navigation&body=Title%3A%20Prioritized%20Semantic%20Learning%20for%20Zero-shot%20Instance%20Navigation%0AAuthor%3A%20Xander%20Sun%20and%20Louis%20Lau%20and%20Hoyard%20Zhi%20and%20Ronghe%20Qiu%20and%20Junwei%20Liang%0AAbstract%3A%20%20%20We%20study%20zero-shot%20instance%20navigation%2C%20in%20which%20the%20agent%20navigates%20to%20a%0Aspecific%20object%20without%20using%20object%20annotations%20for%20training.%20Previous%20object%0Anavigation%20approaches%20apply%20the%20image-goal%20navigation%20%28ImageNav%29%20task%20%28go%20to%0Athe%20location%20of%20an%20image%29%20for%20pretraining%2C%20and%20transfer%20the%20agent%20to%20achieve%0Aobject%20goals%20using%20a%20vision-language%20model.%20However%2C%20these%20approaches%20lead%20to%0Aissues%20of%20semantic%20neglect%2C%20where%20the%20model%20fails%20to%20learn%20meaningful%20semantic%0Aalignments.%20In%20this%20paper%2C%20we%20propose%20a%20Prioritized%20Semantic%20Learning%20%28PSL%29%0Amethod%20to%20improve%20the%20semantic%20understanding%20ability%20of%20navigation%20agents.%0ASpecifically%2C%20a%20semantic-enhanced%20PSL%20agent%20is%20proposed%20and%20a%20prioritized%0Asemantic%20training%20strategy%20is%20introduced%20to%20select%20goal%20images%20that%20exhibit%0Aclear%20semantic%20supervision%20and%20relax%20the%20reward%20function%20from%20strict%20exact%20view%0Amatching.%20At%20inference%20time%2C%20a%20semantic%20expansion%20inference%20scheme%20is%20designed%0Ato%20preserve%20the%20same%20granularity%20level%20of%20the%20goal-semantic%20as%20training.%0AFurthermore%2C%20for%20the%20popular%20HM3D%20environment%2C%20we%20present%20an%20Instance%0ANavigation%20%28InstanceNav%29%20task%20that%20requires%20going%20to%20a%20specific%20object%20instance%0Awith%20detailed%20descriptions%2C%20as%20opposed%20to%20the%20Object%20Navigation%20%28ObjectNav%29%0Atask%20where%20the%20goal%20is%20defined%20merely%20by%20the%20object%20category.%20Our%20PSL%20agent%0Aoutperforms%20the%20previous%20state-of-the-art%20by%2066%25%20on%20zero-shot%20ObjectNav%20in%0Aterms%20of%20success%20rate%20and%20is%20also%20superior%20on%20the%20new%20InstanceNav%20task.%20Code%0Awill%20be%20released%20at%20https%3A//anonymous.4open.%20science/r/PSL/.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11650v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Prioritized%20Semantic%20Learning%20for%20Zero-shot%20Instance%20Navigation&entry.906535625=Xander%20Sun%20and%20Louis%20Lau%20and%20Hoyard%20Zhi%20and%20Ronghe%20Qiu%20and%20Junwei%20Liang&entry.1292438233=%20%20We%20study%20zero-shot%20instance%20navigation%2C%20in%20which%20the%20agent%20navigates%20to%20a%0Aspecific%20object%20without%20using%20object%20annotations%20for%20training.%20Previous%20object%0Anavigation%20approaches%20apply%20the%20image-goal%20navigation%20%28ImageNav%29%20task%20%28go%20to%0Athe%20location%20of%20an%20image%29%20for%20pretraining%2C%20and%20transfer%20the%20agent%20to%20achieve%0Aobject%20goals%20using%20a%20vision-language%20model.%20However%2C%20these%20approaches%20lead%20to%0Aissues%20of%20semantic%20neglect%2C%20where%20the%20model%20fails%20to%20learn%20meaningful%20semantic%0Aalignments.%20In%20this%20paper%2C%20we%20propose%20a%20Prioritized%20Semantic%20Learning%20%28PSL%29%0Amethod%20to%20improve%20the%20semantic%20understanding%20ability%20of%20navigation%20agents.%0ASpecifically%2C%20a%20semantic-enhanced%20PSL%20agent%20is%20proposed%20and%20a%20prioritized%0Asemantic%20training%20strategy%20is%20introduced%20to%20select%20goal%20images%20that%20exhibit%0Aclear%20semantic%20supervision%20and%20relax%20the%20reward%20function%20from%20strict%20exact%20view%0Amatching.%20At%20inference%20time%2C%20a%20semantic%20expansion%20inference%20scheme%20is%20designed%0Ato%20preserve%20the%20same%20granularity%20level%20of%20the%20goal-semantic%20as%20training.%0AFurthermore%2C%20for%20the%20popular%20HM3D%20environment%2C%20we%20present%20an%20Instance%0ANavigation%20%28InstanceNav%29%20task%20that%20requires%20going%20to%20a%20specific%20object%20instance%0Awith%20detailed%20descriptions%2C%20as%20opposed%20to%20the%20Object%20Navigation%20%28ObjectNav%29%0Atask%20where%20the%20goal%20is%20defined%20merely%20by%20the%20object%20category.%20Our%20PSL%20agent%0Aoutperforms%20the%20previous%20state-of-the-art%20by%2066%25%20on%20zero-shot%20ObjectNav%20in%0Aterms%20of%20success%20rate%20and%20is%20also%20superior%20on%20the%20new%20InstanceNav%20task.%20Code%0Awill%20be%20released%20at%20https%3A//anonymous.4open.%20science/r/PSL/.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11650v1&entry.124074799=Read"},
{"title": "MedMerge: Merging Models for Effective Transfer Learning to Medical\n  Imaging Tasks", "author": "Ibrahim Almakky and Santosh Sanjeev and Anees Ur Rehman Hashmi and Mohammad Areeb Qazi and Mohammad Yaqub", "abstract": "  Transfer learning has become a powerful tool to initialize deep learning\nmodels to achieve faster convergence and higher performance. This is especially\nuseful in the medical imaging analysis domain, where data scarcity limits\npossible performance gains for deep learning models. Some advancements have\nbeen made in boosting the transfer learning performance gain by merging models\nstarting from the same initialization. However, in the medical imaging analysis\ndomain, there is an opportunity in merging models starting from different\ninitialisations, thus combining the features learnt from different tasks. In\nthis work, we propose MedMerge, a method whereby the weights of different\nmodels can be merged, and their features can be effectively utilized to boost\nperformance on a new task. With MedMerge, we learn kernel-level weights that\ncan later be used to merge the models into a single model, even when starting\nfrom different initializations. Testing on various medical imaging analysis\ntasks, we show that our merged model can achieve significant performance gains,\nwith up to 3% improvement on the F1 score. The code implementation of this work\nwill be available at www.github.com/BioMedIA-MBZUAI/MedMerge.\n", "link": "http://arxiv.org/abs/2403.11646v1", "date": "2024-03-18", "relevancy": 2.0474, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.5476}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5215}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4878}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MedMerge%3A%20Merging%20Models%20for%20Effective%20Transfer%20Learning%20to%20Medical%0A%20%20Imaging%20Tasks&body=Title%3A%20MedMerge%3A%20Merging%20Models%20for%20Effective%20Transfer%20Learning%20to%20Medical%0A%20%20Imaging%20Tasks%0AAuthor%3A%20Ibrahim%20Almakky%20and%20Santosh%20Sanjeev%20and%20Anees%20Ur%20Rehman%20Hashmi%20and%20Mohammad%20Areeb%20Qazi%20and%20Mohammad%20Yaqub%0AAbstract%3A%20%20%20Transfer%20learning%20has%20become%20a%20powerful%20tool%20to%20initialize%20deep%20learning%0Amodels%20to%20achieve%20faster%20convergence%20and%20higher%20performance.%20This%20is%20especially%0Auseful%20in%20the%20medical%20imaging%20analysis%20domain%2C%20where%20data%20scarcity%20limits%0Apossible%20performance%20gains%20for%20deep%20learning%20models.%20Some%20advancements%20have%0Abeen%20made%20in%20boosting%20the%20transfer%20learning%20performance%20gain%20by%20merging%20models%0Astarting%20from%20the%20same%20initialization.%20However%2C%20in%20the%20medical%20imaging%20analysis%0Adomain%2C%20there%20is%20an%20opportunity%20in%20merging%20models%20starting%20from%20different%0Ainitialisations%2C%20thus%20combining%20the%20features%20learnt%20from%20different%20tasks.%20In%0Athis%20work%2C%20we%20propose%20MedMerge%2C%20a%20method%20whereby%20the%20weights%20of%20different%0Amodels%20can%20be%20merged%2C%20and%20their%20features%20can%20be%20effectively%20utilized%20to%20boost%0Aperformance%20on%20a%20new%20task.%20With%20MedMerge%2C%20we%20learn%20kernel-level%20weights%20that%0Acan%20later%20be%20used%20to%20merge%20the%20models%20into%20a%20single%20model%2C%20even%20when%20starting%0Afrom%20different%20initializations.%20Testing%20on%20various%20medical%20imaging%20analysis%0Atasks%2C%20we%20show%20that%20our%20merged%20model%20can%20achieve%20significant%20performance%20gains%2C%0Awith%20up%20to%203%25%20improvement%20on%20the%20F1%20score.%20The%20code%20implementation%20of%20this%20work%0Awill%20be%20available%20at%20www.github.com/BioMedIA-MBZUAI/MedMerge.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11646v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MedMerge%3A%20Merging%20Models%20for%20Effective%20Transfer%20Learning%20to%20Medical%0A%20%20Imaging%20Tasks&entry.906535625=Ibrahim%20Almakky%20and%20Santosh%20Sanjeev%20and%20Anees%20Ur%20Rehman%20Hashmi%20and%20Mohammad%20Areeb%20Qazi%20and%20Mohammad%20Yaqub&entry.1292438233=%20%20Transfer%20learning%20has%20become%20a%20powerful%20tool%20to%20initialize%20deep%20learning%0Amodels%20to%20achieve%20faster%20convergence%20and%20higher%20performance.%20This%20is%20especially%0Auseful%20in%20the%20medical%20imaging%20analysis%20domain%2C%20where%20data%20scarcity%20limits%0Apossible%20performance%20gains%20for%20deep%20learning%20models.%20Some%20advancements%20have%0Abeen%20made%20in%20boosting%20the%20transfer%20learning%20performance%20gain%20by%20merging%20models%0Astarting%20from%20the%20same%20initialization.%20However%2C%20in%20the%20medical%20imaging%20analysis%0Adomain%2C%20there%20is%20an%20opportunity%20in%20merging%20models%20starting%20from%20different%0Ainitialisations%2C%20thus%20combining%20the%20features%20learnt%20from%20different%20tasks.%20In%0Athis%20work%2C%20we%20propose%20MedMerge%2C%20a%20method%20whereby%20the%20weights%20of%20different%0Amodels%20can%20be%20merged%2C%20and%20their%20features%20can%20be%20effectively%20utilized%20to%20boost%0Aperformance%20on%20a%20new%20task.%20With%20MedMerge%2C%20we%20learn%20kernel-level%20weights%20that%0Acan%20later%20be%20used%20to%20merge%20the%20models%20into%20a%20single%20model%2C%20even%20when%20starting%0Afrom%20different%20initializations.%20Testing%20on%20various%20medical%20imaging%20analysis%0Atasks%2C%20we%20show%20that%20our%20merged%20model%20can%20achieve%20significant%20performance%20gains%2C%0Awith%20up%20to%203%25%20improvement%20on%20the%20F1%20score.%20The%20code%20implementation%20of%20this%20work%0Awill%20be%20available%20at%20www.github.com/BioMedIA-MBZUAI/MedMerge.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11646v1&entry.124074799=Read"},
{"title": "Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A\n  Brain-Inspired Method for Parameter-Efficient Fine-Tuning", "author": "Yao Liang and Yuwei Wang and Yang Li and Yi Zeng", "abstract": "  Fine-tuning techniques based on Large Pretrained Language Models (LPLMs) have\nbeen proven to significantly enhance model performance on a variety of\ndownstream tasks and effectively control the output behaviors of LPLMs. Recent\nstudies have proposed numerous methods for fine-tuning a small number of\nparameters based on open-source LPLMs, reducing the demand for computational\nand storage resources. Among these, reparameterization fine-tuning methods\nrepresented by LoRA (Low-Rank Adaptation) have gained popularity. We find that\nalthough these methods perform well in many aspects, there is still\nconsiderable room for improvement in terms of complex task adaptability,\nperformance, stability, and algorithm complexity. In response to this, inspired\nby the idea that the functions of the brain are shaped by its geometric\nstructure, this paper integrates this idea into LoRA technology and proposes a\nnew matrix transformation-based reparameterization method for efficient\nfine-tuning, named Matrix-Transformation based Low-Rank Adaptation (MTLoRA).\nMTLoRA aims to dynamically alter its spatial geometric structure by applying a\ntransformation-matrix T to perform linear transformations, such as rotation,\nscaling, and translation, on the task-specific parameter matrix, generating new\nmatrix feature patterns (eigenvectors) to mimic the fundamental influence of\ncomplex geometric structure feature patterns in the brain on functions, thereby\nenhancing the model's performance in downstream tasks. In Natural Language\nUnderstanding (NLU) tasks, it is evaluated using the GLUE benchmark test, and\nthe results reveal that MTLoRA achieves an overall performance increase of\nabout 1.0% across eight tasks; in Natural Language Generation (NLG) tasks,\nMTLoRA improves performance by an average of 0.95% and 0.56% in the DART and\nWebNLG tasks, respectively.\n", "link": "http://arxiv.org/abs/2403.07440v2", "date": "2024-03-18", "relevancy": 2.0299, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5264}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4984}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4921}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Matrix-Transformation%20Based%20Low-Rank%20Adaptation%20%28MTLoRA%29%3A%20A%0A%20%20Brain-Inspired%20Method%20for%20Parameter-Efficient%20Fine-Tuning&body=Title%3A%20Matrix-Transformation%20Based%20Low-Rank%20Adaptation%20%28MTLoRA%29%3A%20A%0A%20%20Brain-Inspired%20Method%20for%20Parameter-Efficient%20Fine-Tuning%0AAuthor%3A%20Yao%20Liang%20and%20Yuwei%20Wang%20and%20Yang%20Li%20and%20Yi%20Zeng%0AAbstract%3A%20%20%20Fine-tuning%20techniques%20based%20on%20Large%20Pretrained%20Language%20Models%20%28LPLMs%29%20have%0Abeen%20proven%20to%20significantly%20enhance%20model%20performance%20on%20a%20variety%20of%0Adownstream%20tasks%20and%20effectively%20control%20the%20output%20behaviors%20of%20LPLMs.%20Recent%0Astudies%20have%20proposed%20numerous%20methods%20for%20fine-tuning%20a%20small%20number%20of%0Aparameters%20based%20on%20open-source%20LPLMs%2C%20reducing%20the%20demand%20for%20computational%0Aand%20storage%20resources.%20Among%20these%2C%20reparameterization%20fine-tuning%20methods%0Arepresented%20by%20LoRA%20%28Low-Rank%20Adaptation%29%20have%20gained%20popularity.%20We%20find%20that%0Aalthough%20these%20methods%20perform%20well%20in%20many%20aspects%2C%20there%20is%20still%0Aconsiderable%20room%20for%20improvement%20in%20terms%20of%20complex%20task%20adaptability%2C%0Aperformance%2C%20stability%2C%20and%20algorithm%20complexity.%20In%20response%20to%20this%2C%20inspired%0Aby%20the%20idea%20that%20the%20functions%20of%20the%20brain%20are%20shaped%20by%20its%20geometric%0Astructure%2C%20this%20paper%20integrates%20this%20idea%20into%20LoRA%20technology%20and%20proposes%20a%0Anew%20matrix%20transformation-based%20reparameterization%20method%20for%20efficient%0Afine-tuning%2C%20named%20Matrix-Transformation%20based%20Low-Rank%20Adaptation%20%28MTLoRA%29.%0AMTLoRA%20aims%20to%20dynamically%20alter%20its%20spatial%20geometric%20structure%20by%20applying%20a%0Atransformation-matrix%20T%20to%20perform%20linear%20transformations%2C%20such%20as%20rotation%2C%0Ascaling%2C%20and%20translation%2C%20on%20the%20task-specific%20parameter%20matrix%2C%20generating%20new%0Amatrix%20feature%20patterns%20%28eigenvectors%29%20to%20mimic%20the%20fundamental%20influence%20of%0Acomplex%20geometric%20structure%20feature%20patterns%20in%20the%20brain%20on%20functions%2C%20thereby%0Aenhancing%20the%20model%27s%20performance%20in%20downstream%20tasks.%20In%20Natural%20Language%0AUnderstanding%20%28NLU%29%20tasks%2C%20it%20is%20evaluated%20using%20the%20GLUE%20benchmark%20test%2C%20and%0Athe%20results%20reveal%20that%20MTLoRA%20achieves%20an%20overall%20performance%20increase%20of%0Aabout%201.0%25%20across%20eight%20tasks%3B%20in%20Natural%20Language%20Generation%20%28NLG%29%20tasks%2C%0AMTLoRA%20improves%20performance%20by%20an%20average%20of%200.95%25%20and%200.56%25%20in%20the%20DART%20and%0AWebNLG%20tasks%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.07440v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matrix-Transformation%20Based%20Low-Rank%20Adaptation%20%28MTLoRA%29%3A%20A%0A%20%20Brain-Inspired%20Method%20for%20Parameter-Efficient%20Fine-Tuning&entry.906535625=Yao%20Liang%20and%20Yuwei%20Wang%20and%20Yang%20Li%20and%20Yi%20Zeng&entry.1292438233=%20%20Fine-tuning%20techniques%20based%20on%20Large%20Pretrained%20Language%20Models%20%28LPLMs%29%20have%0Abeen%20proven%20to%20significantly%20enhance%20model%20performance%20on%20a%20variety%20of%0Adownstream%20tasks%20and%20effectively%20control%20the%20output%20behaviors%20of%20LPLMs.%20Recent%0Astudies%20have%20proposed%20numerous%20methods%20for%20fine-tuning%20a%20small%20number%20of%0Aparameters%20based%20on%20open-source%20LPLMs%2C%20reducing%20the%20demand%20for%20computational%0Aand%20storage%20resources.%20Among%20these%2C%20reparameterization%20fine-tuning%20methods%0Arepresented%20by%20LoRA%20%28Low-Rank%20Adaptation%29%20have%20gained%20popularity.%20We%20find%20that%0Aalthough%20these%20methods%20perform%20well%20in%20many%20aspects%2C%20there%20is%20still%0Aconsiderable%20room%20for%20improvement%20in%20terms%20of%20complex%20task%20adaptability%2C%0Aperformance%2C%20stability%2C%20and%20algorithm%20complexity.%20In%20response%20to%20this%2C%20inspired%0Aby%20the%20idea%20that%20the%20functions%20of%20the%20brain%20are%20shaped%20by%20its%20geometric%0Astructure%2C%20this%20paper%20integrates%20this%20idea%20into%20LoRA%20technology%20and%20proposes%20a%0Anew%20matrix%20transformation-based%20reparameterization%20method%20for%20efficient%0Afine-tuning%2C%20named%20Matrix-Transformation%20based%20Low-Rank%20Adaptation%20%28MTLoRA%29.%0AMTLoRA%20aims%20to%20dynamically%20alter%20its%20spatial%20geometric%20structure%20by%20applying%20a%0Atransformation-matrix%20T%20to%20perform%20linear%20transformations%2C%20such%20as%20rotation%2C%0Ascaling%2C%20and%20translation%2C%20on%20the%20task-specific%20parameter%20matrix%2C%20generating%20new%0Amatrix%20feature%20patterns%20%28eigenvectors%29%20to%20mimic%20the%20fundamental%20influence%20of%0Acomplex%20geometric%20structure%20feature%20patterns%20in%20the%20brain%20on%20functions%2C%20thereby%0Aenhancing%20the%20model%27s%20performance%20in%20downstream%20tasks.%20In%20Natural%20Language%0AUnderstanding%20%28NLU%29%20tasks%2C%20it%20is%20evaluated%20using%20the%20GLUE%20benchmark%20test%2C%20and%0Athe%20results%20reveal%20that%20MTLoRA%20achieves%20an%20overall%20performance%20increase%20of%0Aabout%201.0%25%20across%20eight%20tasks%3B%20in%20Natural%20Language%20Generation%20%28NLG%29%20tasks%2C%0AMTLoRA%20improves%20performance%20by%20an%20average%20of%200.95%25%20and%200.56%25%20in%20the%20DART%20and%0AWebNLG%20tasks%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.07440v2&entry.124074799=Read"},
{"title": "Dual-Channel Multiplex Graph Neural Networks for Recommendation", "author": "Xiang Li and Chaofan Fu and Zhongying Zhao and Guanjie Zheng and Chao Huang and Junyu Dong and Yanwei Yu", "abstract": "  Efficient recommender systems play a crucial role in accurately capturing\nuser and item attributes that mirror individual preferences. Some existing\nrecommendation techniques have started to shift their focus towards modeling\nvarious types of interaction relations between users and items in real-world\nrecommendation scenarios, such as clicks, marking favorites, and purchases on\nonline shopping platforms. Nevertheless, these approaches still grapple with\ntwo significant shortcomings: (1) Insufficient modeling and exploitation of the\nimpact of various behavior patterns formed by multiplex relations between users\nand items on representation learning, and (2) ignoring the effect of different\nrelations in the behavior patterns on the target relation in recommender system\nscenarios. In this study, we introduce a novel recommendation framework,\nDual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the\naforementioned challenges. It incorporates an explicit behavior pattern\nrepresentation learner to capture the behavior patterns composed of multiplex\nuser-item interaction relations, and includes a relation chain representation\nlearning and a relation chain-aware encoder to discover the impact of various\nauxiliary relations on the target relation, the dependencies between different\nrelations, and mine the appropriate order of relations in a behavior pattern.\nExtensive experiments on three real-world datasets demonstrate that our \\model\nsurpasses various state-of-the-art recommendation methods. It outperforms the\nbest baselines by 10.06\\% and 12.15\\% on average across all datasets in terms\nof R@10 and N@10 respectively.\n", "link": "http://arxiv.org/abs/2403.11624v1", "date": "2024-03-18", "relevancy": 2.0159, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.5363}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.4813}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4797}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Dual-Channel%20Multiplex%20Graph%20Neural%20Networks%20for%20Recommendation&body=Title%3A%20Dual-Channel%20Multiplex%20Graph%20Neural%20Networks%20for%20Recommendation%0AAuthor%3A%20Xiang%20Li%20and%20Chaofan%20Fu%20and%20Zhongying%20Zhao%20and%20Guanjie%20Zheng%20and%20Chao%20Huang%20and%20Junyu%20Dong%20and%20Yanwei%20Yu%0AAbstract%3A%20%20%20Efficient%20recommender%20systems%20play%20a%20crucial%20role%20in%20accurately%20capturing%0Auser%20and%20item%20attributes%20that%20mirror%20individual%20preferences.%20Some%20existing%0Arecommendation%20techniques%20have%20started%20to%20shift%20their%20focus%20towards%20modeling%0Avarious%20types%20of%20interaction%20relations%20between%20users%20and%20items%20in%20real-world%0Arecommendation%20scenarios%2C%20such%20as%20clicks%2C%20marking%20favorites%2C%20and%20purchases%20on%0Aonline%20shopping%20platforms.%20Nevertheless%2C%20these%20approaches%20still%20grapple%20with%0Atwo%20significant%20shortcomings%3A%20%281%29%20Insufficient%20modeling%20and%20exploitation%20of%20the%0Aimpact%20of%20various%20behavior%20patterns%20formed%20by%20multiplex%20relations%20between%20users%0Aand%20items%20on%20representation%20learning%2C%20and%20%282%29%20ignoring%20the%20effect%20of%20different%0Arelations%20in%20the%20behavior%20patterns%20on%20the%20target%20relation%20in%20recommender%20system%0Ascenarios.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20recommendation%20framework%2C%0ADual-Channel%20Multiplex%20Graph%20Neural%20Network%20%28DCMGNN%29%2C%20which%20addresses%20the%0Aaforementioned%20challenges.%20It%20incorporates%20an%20explicit%20behavior%20pattern%0Arepresentation%20learner%20to%20capture%20the%20behavior%20patterns%20composed%20of%20multiplex%0Auser-item%20interaction%20relations%2C%20and%20includes%20a%20relation%20chain%20representation%0Alearning%20and%20a%20relation%20chain-aware%20encoder%20to%20discover%20the%20impact%20of%20various%0Aauxiliary%20relations%20on%20the%20target%20relation%2C%20the%20dependencies%20between%20different%0Arelations%2C%20and%20mine%20the%20appropriate%20order%20of%20relations%20in%20a%20behavior%20pattern.%0AExtensive%20experiments%20on%20three%20real-world%20datasets%20demonstrate%20that%20our%20%5Cmodel%0Asurpasses%20various%20state-of-the-art%20recommendation%20methods.%20It%20outperforms%20the%0Abest%20baselines%20by%2010.06%5C%25%20and%2012.15%5C%25%20on%20average%20across%20all%20datasets%20in%20terms%0Aof%20R%4010%20and%20N%4010%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11624v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Dual-Channel%20Multiplex%20Graph%20Neural%20Networks%20for%20Recommendation&entry.906535625=Xiang%20Li%20and%20Chaofan%20Fu%20and%20Zhongying%20Zhao%20and%20Guanjie%20Zheng%20and%20Chao%20Huang%20and%20Junyu%20Dong%20and%20Yanwei%20Yu&entry.1292438233=%20%20Efficient%20recommender%20systems%20play%20a%20crucial%20role%20in%20accurately%20capturing%0Auser%20and%20item%20attributes%20that%20mirror%20individual%20preferences.%20Some%20existing%0Arecommendation%20techniques%20have%20started%20to%20shift%20their%20focus%20towards%20modeling%0Avarious%20types%20of%20interaction%20relations%20between%20users%20and%20items%20in%20real-world%0Arecommendation%20scenarios%2C%20such%20as%20clicks%2C%20marking%20favorites%2C%20and%20purchases%20on%0Aonline%20shopping%20platforms.%20Nevertheless%2C%20these%20approaches%20still%20grapple%20with%0Atwo%20significant%20shortcomings%3A%20%281%29%20Insufficient%20modeling%20and%20exploitation%20of%20the%0Aimpact%20of%20various%20behavior%20patterns%20formed%20by%20multiplex%20relations%20between%20users%0Aand%20items%20on%20representation%20learning%2C%20and%20%282%29%20ignoring%20the%20effect%20of%20different%0Arelations%20in%20the%20behavior%20patterns%20on%20the%20target%20relation%20in%20recommender%20system%0Ascenarios.%20In%20this%20study%2C%20we%20introduce%20a%20novel%20recommendation%20framework%2C%0ADual-Channel%20Multiplex%20Graph%20Neural%20Network%20%28DCMGNN%29%2C%20which%20addresses%20the%0Aaforementioned%20challenges.%20It%20incorporates%20an%20explicit%20behavior%20pattern%0Arepresentation%20learner%20to%20capture%20the%20behavior%20patterns%20composed%20of%20multiplex%0Auser-item%20interaction%20relations%2C%20and%20includes%20a%20relation%20chain%20representation%0Alearning%20and%20a%20relation%20chain-aware%20encoder%20to%20discover%20the%20impact%20of%20various%0Aauxiliary%20relations%20on%20the%20target%20relation%2C%20the%20dependencies%20between%20different%0Arelations%2C%20and%20mine%20the%20appropriate%20order%20of%20relations%20in%20a%20behavior%20pattern.%0AExtensive%20experiments%20on%20three%20real-world%20datasets%20demonstrate%20that%20our%20%5Cmodel%0Asurpasses%20various%20state-of-the-art%20recommendation%20methods.%20It%20outperforms%20the%0Abest%20baselines%20by%2010.06%5C%25%20and%2012.15%5C%25%20on%20average%20across%20all%20datasets%20in%20terms%0Aof%20R%4010%20and%20N%4010%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11624v1&entry.124074799=Read"},
{"title": "Guiding the generation of counterfactual explanations through temporal\n  background knowledge for Predictive Process Monitoring", "author": "Andrei Buliga and Chiara Di Francescomarino and Chiara Ghidini and Ivan Donadello and Fabrizio Maria Maggi", "abstract": "  Counterfactual explanations suggest what should be different in the input\ninstance to change the outcome of an AI system. When dealing with\ncounterfactual explanations in the field of Predictive Process Monitoring,\nhowever, control flow relationships among events have to be carefully\nconsidered. A counterfactual, indeed, should not violate control flow\nrelationships among activities (temporal background knowledege). Within the\nfield of Explainability in Predictive Process Monitoring, there have been a\nseries of works regarding counterfactual explanations for outcome-based\npredictions. However, none of them consider the inclusion of temporal\nbackground knowledge when generating these counterfactuals. In this work, we\nadapt state-of-the-art techniques for counterfactual generation in the domain\nof XAI that are based on genetic algorithms to consider a series of temporal\nconstraints at runtime. We assume that this temporal background knowledge is\ngiven, and we adapt the fitness function, as well as the crossover and mutation\noperators, to maintain the satisfaction of the constraints. The proposed\nmethods are evaluated with respect to state-of-the-art genetic algorithms for\ncounterfactual generation and the results are presented. We showcase that the\ninclusion of temporal background knowledge allows the generation of\ncounterfactuals more conformant to the temporal background knowledge, without\nhowever losing in terms of the counterfactual traditional quality metrics.\n", "link": "http://arxiv.org/abs/2403.11642v1", "date": "2024-03-18", "relevancy": 2.0105, "topK": [{"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.5099}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5031}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4833}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Guiding%20the%20generation%20of%20counterfactual%20explanations%20through%20temporal%0A%20%20background%20knowledge%20for%20Predictive%20Process%20Monitoring&body=Title%3A%20Guiding%20the%20generation%20of%20counterfactual%20explanations%20through%20temporal%0A%20%20background%20knowledge%20for%20Predictive%20Process%20Monitoring%0AAuthor%3A%20Andrei%20Buliga%20and%20Chiara%20Di%20Francescomarino%20and%20Chiara%20Ghidini%20and%20Ivan%20Donadello%20and%20Fabrizio%20Maria%20Maggi%0AAbstract%3A%20%20%20Counterfactual%20explanations%20suggest%20what%20should%20be%20different%20in%20the%20input%0Ainstance%20to%20change%20the%20outcome%20of%20an%20AI%20system.%20When%20dealing%20with%0Acounterfactual%20explanations%20in%20the%20field%20of%20Predictive%20Process%20Monitoring%2C%0Ahowever%2C%20control%20flow%20relationships%20among%20events%20have%20to%20be%20carefully%0Aconsidered.%20A%20counterfactual%2C%20indeed%2C%20should%20not%20violate%20control%20flow%0Arelationships%20among%20activities%20%28temporal%20background%20knowledege%29.%20Within%20the%0Afield%20of%20Explainability%20in%20Predictive%20Process%20Monitoring%2C%20there%20have%20been%20a%0Aseries%20of%20works%20regarding%20counterfactual%20explanations%20for%20outcome-based%0Apredictions.%20However%2C%20none%20of%20them%20consider%20the%20inclusion%20of%20temporal%0Abackground%20knowledge%20when%20generating%20these%20counterfactuals.%20In%20this%20work%2C%20we%0Aadapt%20state-of-the-art%20techniques%20for%20counterfactual%20generation%20in%20the%20domain%0Aof%20XAI%20that%20are%20based%20on%20genetic%20algorithms%20to%20consider%20a%20series%20of%20temporal%0Aconstraints%20at%20runtime.%20We%20assume%20that%20this%20temporal%20background%20knowledge%20is%0Agiven%2C%20and%20we%20adapt%20the%20fitness%20function%2C%20as%20well%20as%20the%20crossover%20and%20mutation%0Aoperators%2C%20to%20maintain%20the%20satisfaction%20of%20the%20constraints.%20The%20proposed%0Amethods%20are%20evaluated%20with%20respect%20to%20state-of-the-art%20genetic%20algorithms%20for%0Acounterfactual%20generation%20and%20the%20results%20are%20presented.%20We%20showcase%20that%20the%0Ainclusion%20of%20temporal%20background%20knowledge%20allows%20the%20generation%20of%0Acounterfactuals%20more%20conformant%20to%20the%20temporal%20background%20knowledge%2C%20without%0Ahowever%20losing%20in%20terms%20of%20the%20counterfactual%20traditional%20quality%20metrics.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11642v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Guiding%20the%20generation%20of%20counterfactual%20explanations%20through%20temporal%0A%20%20background%20knowledge%20for%20Predictive%20Process%20Monitoring&entry.906535625=Andrei%20Buliga%20and%20Chiara%20Di%20Francescomarino%20and%20Chiara%20Ghidini%20and%20Ivan%20Donadello%20and%20Fabrizio%20Maria%20Maggi&entry.1292438233=%20%20Counterfactual%20explanations%20suggest%20what%20should%20be%20different%20in%20the%20input%0Ainstance%20to%20change%20the%20outcome%20of%20an%20AI%20system.%20When%20dealing%20with%0Acounterfactual%20explanations%20in%20the%20field%20of%20Predictive%20Process%20Monitoring%2C%0Ahowever%2C%20control%20flow%20relationships%20among%20events%20have%20to%20be%20carefully%0Aconsidered.%20A%20counterfactual%2C%20indeed%2C%20should%20not%20violate%20control%20flow%0Arelationships%20among%20activities%20%28temporal%20background%20knowledege%29.%20Within%20the%0Afield%20of%20Explainability%20in%20Predictive%20Process%20Monitoring%2C%20there%20have%20been%20a%0Aseries%20of%20works%20regarding%20counterfactual%20explanations%20for%20outcome-based%0Apredictions.%20However%2C%20none%20of%20them%20consider%20the%20inclusion%20of%20temporal%0Abackground%20knowledge%20when%20generating%20these%20counterfactuals.%20In%20this%20work%2C%20we%0Aadapt%20state-of-the-art%20techniques%20for%20counterfactual%20generation%20in%20the%20domain%0Aof%20XAI%20that%20are%20based%20on%20genetic%20algorithms%20to%20consider%20a%20series%20of%20temporal%0Aconstraints%20at%20runtime.%20We%20assume%20that%20this%20temporal%20background%20knowledge%20is%0Agiven%2C%20and%20we%20adapt%20the%20fitness%20function%2C%20as%20well%20as%20the%20crossover%20and%20mutation%0Aoperators%2C%20to%20maintain%20the%20satisfaction%20of%20the%20constraints.%20The%20proposed%0Amethods%20are%20evaluated%20with%20respect%20to%20state-of-the-art%20genetic%20algorithms%20for%0Acounterfactual%20generation%20and%20the%20results%20are%20presented.%20We%20showcase%20that%20the%0Ainclusion%20of%20temporal%20background%20knowledge%20allows%20the%20generation%20of%0Acounterfactuals%20more%20conformant%20to%20the%20temporal%20background%20knowledge%2C%20without%0Ahowever%20losing%20in%20terms%20of%20the%20counterfactual%20traditional%20quality%20metrics.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11642v1&entry.124074799=Read"},
{"title": "Towards Generalizing to Unseen Domains with Few Labels", "author": "Chamuditha Jayanga Galappaththige and Sanoojan Baliah and Malitha Gunawardhana and Muhammad Haris Khan", "abstract": "  We approach the challenge of addressing semi-supervised domain generalization\n(SSDG). Specifically, our aim is to obtain a model that learns\ndomain-generalizable features by leveraging a limited subset of labelled data\nalongside a substantially larger pool of unlabeled data. Existing domain\ngeneralization (DG) methods which are unable to exploit unlabeled data perform\npoorly compared to semi-supervised learning (SSL) methods under SSDG setting.\nNevertheless, SSL methods have considerable room for performance improvement\nwhen compared to fully-supervised DG training. To tackle this underexplored,\nyet highly practical problem of SSDG, we make the following core contributions.\nFirst, we propose a feature-based conformity technique that matches the\nposterior distributions from the feature space with the pseudo-label from the\nmodel's output space. Second, we develop a semantics alignment loss to learn\nsemantically-compatible representations by regularizing the semantic structure\nin the feature space. Our method is plug-and-play and can be readily integrated\nwith different SSL-based SSDG baselines without introducing any additional\nparameters. Extensive experimental results across five challenging DG\nbenchmarks with four strong SSL baselines suggest that our method provides\nconsistent and notable gains in two different SSDG settings.\n", "link": "http://arxiv.org/abs/2403.11674v1", "date": "2024-03-18", "relevancy": 1.9906, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.5245}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4897}, {"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.474}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Towards%20Generalizing%20to%20Unseen%20Domains%20with%20Few%20Labels&body=Title%3A%20Towards%20Generalizing%20to%20Unseen%20Domains%20with%20Few%20Labels%0AAuthor%3A%20Chamuditha%20Jayanga%20Galappaththige%20and%20Sanoojan%20Baliah%20and%20Malitha%20Gunawardhana%20and%20Muhammad%20Haris%20Khan%0AAbstract%3A%20%20%20We%20approach%20the%20challenge%20of%20addressing%20semi-supervised%20domain%20generalization%0A%28SSDG%29.%20Specifically%2C%20our%20aim%20is%20to%20obtain%20a%20model%20that%20learns%0Adomain-generalizable%20features%20by%20leveraging%20a%20limited%20subset%20of%20labelled%20data%0Aalongside%20a%20substantially%20larger%20pool%20of%20unlabeled%20data.%20Existing%20domain%0Ageneralization%20%28DG%29%20methods%20which%20are%20unable%20to%20exploit%20unlabeled%20data%20perform%0Apoorly%20compared%20to%20semi-supervised%20learning%20%28SSL%29%20methods%20under%20SSDG%20setting.%0ANevertheless%2C%20SSL%20methods%20have%20considerable%20room%20for%20performance%20improvement%0Awhen%20compared%20to%20fully-supervised%20DG%20training.%20To%20tackle%20this%20underexplored%2C%0Ayet%20highly%20practical%20problem%20of%20SSDG%2C%20we%20make%20the%20following%20core%20contributions.%0AFirst%2C%20we%20propose%20a%20feature-based%20conformity%20technique%20that%20matches%20the%0Aposterior%20distributions%20from%20the%20feature%20space%20with%20the%20pseudo-label%20from%20the%0Amodel%27s%20output%20space.%20Second%2C%20we%20develop%20a%20semantics%20alignment%20loss%20to%20learn%0Asemantically-compatible%20representations%20by%20regularizing%20the%20semantic%20structure%0Ain%20the%20feature%20space.%20Our%20method%20is%20plug-and-play%20and%20can%20be%20readily%20integrated%0Awith%20different%20SSL-based%20SSDG%20baselines%20without%20introducing%20any%20additional%0Aparameters.%20Extensive%20experimental%20results%20across%20five%20challenging%20DG%0Abenchmarks%20with%20four%20strong%20SSL%20baselines%20suggest%20that%20our%20method%20provides%0Aconsistent%20and%20notable%20gains%20in%20two%20different%20SSDG%20settings.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11674v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Towards%20Generalizing%20to%20Unseen%20Domains%20with%20Few%20Labels&entry.906535625=Chamuditha%20Jayanga%20Galappaththige%20and%20Sanoojan%20Baliah%20and%20Malitha%20Gunawardhana%20and%20Muhammad%20Haris%20Khan&entry.1292438233=%20%20We%20approach%20the%20challenge%20of%20addressing%20semi-supervised%20domain%20generalization%0A%28SSDG%29.%20Specifically%2C%20our%20aim%20is%20to%20obtain%20a%20model%20that%20learns%0Adomain-generalizable%20features%20by%20leveraging%20a%20limited%20subset%20of%20labelled%20data%0Aalongside%20a%20substantially%20larger%20pool%20of%20unlabeled%20data.%20Existing%20domain%0Ageneralization%20%28DG%29%20methods%20which%20are%20unable%20to%20exploit%20unlabeled%20data%20perform%0Apoorly%20compared%20to%20semi-supervised%20learning%20%28SSL%29%20methods%20under%20SSDG%20setting.%0ANevertheless%2C%20SSL%20methods%20have%20considerable%20room%20for%20performance%20improvement%0Awhen%20compared%20to%20fully-supervised%20DG%20training.%20To%20tackle%20this%20underexplored%2C%0Ayet%20highly%20practical%20problem%20of%20SSDG%2C%20we%20make%20the%20following%20core%20contributions.%0AFirst%2C%20we%20propose%20a%20feature-based%20conformity%20technique%20that%20matches%20the%0Aposterior%20distributions%20from%20the%20feature%20space%20with%20the%20pseudo-label%20from%20the%0Amodel%27s%20output%20space.%20Second%2C%20we%20develop%20a%20semantics%20alignment%20loss%20to%20learn%0Asemantically-compatible%20representations%20by%20regularizing%20the%20semantic%20structure%0Ain%20the%20feature%20space.%20Our%20method%20is%20plug-and-play%20and%20can%20be%20readily%20integrated%0Awith%20different%20SSL-based%20SSDG%20baselines%20without%20introducing%20any%20additional%0Aparameters.%20Extensive%20experimental%20results%20across%20five%20challenging%20DG%0Abenchmarks%20with%20four%20strong%20SSL%20baselines%20suggest%20that%20our%20method%20provides%0Aconsistent%20and%20notable%20gains%20in%20two%20different%20SSDG%20settings.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11674v1&entry.124074799=Read"},
{"title": "Convergence of SGD for Training Neural Networks with Sliced Wasserstein\n  Losses", "author": "Eloi Tanguy", "abstract": "  Optimal Transport has sparked vivid interest in recent years, in particular\nthanks to the Wasserstein distance, which provides a geometrically sensible and\nintuitive way of comparing probability measures. For computational reasons, the\nSliced Wasserstein (SW) distance was introduced as an alternative to the\nWasserstein distance, and has seen uses for training generative Neural Networks\n(NNs). While convergence of Stochastic Gradient Descent (SGD) has been observed\npractically in such a setting, there is to our knowledge no theoretical\nguarantee for this observation. Leveraging recent works on convergence of SGD\non non-smooth and non-convex functions by Bianchi et al. (2022), we aim to\nbridge that knowledge gap, and provide a realistic context under which\nfixed-step SGD trajectories for the SW loss on NN parameters converge. More\nprecisely, we show that the trajectories approach the set of (sub)-gradient\nflow equations as the step decreases. Under stricter assumptions, we show a\nmuch stronger convergence result for noised and projected SGD schemes, namely\nthat the long-run limits of the trajectories approach a set of generalised\ncritical points of the loss function.\n", "link": "http://arxiv.org/abs/2307.11714v3", "date": "2024-03-18", "relevancy": 1.955, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5157}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4852}, {"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4815}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Convergence%20of%20SGD%20for%20Training%20Neural%20Networks%20with%20Sliced%20Wasserstein%0A%20%20Losses&body=Title%3A%20Convergence%20of%20SGD%20for%20Training%20Neural%20Networks%20with%20Sliced%20Wasserstein%0A%20%20Losses%0AAuthor%3A%20Eloi%20Tanguy%0AAbstract%3A%20%20%20Optimal%20Transport%20has%20sparked%20vivid%20interest%20in%20recent%20years%2C%20in%20particular%0Athanks%20to%20the%20Wasserstein%20distance%2C%20which%20provides%20a%20geometrically%20sensible%20and%0Aintuitive%20way%20of%20comparing%20probability%20measures.%20For%20computational%20reasons%2C%20the%0ASliced%20Wasserstein%20%28SW%29%20distance%20was%20introduced%20as%20an%20alternative%20to%20the%0AWasserstein%20distance%2C%20and%20has%20seen%20uses%20for%20training%20generative%20Neural%20Networks%0A%28NNs%29.%20While%20convergence%20of%20Stochastic%20Gradient%20Descent%20%28SGD%29%20has%20been%20observed%0Apractically%20in%20such%20a%20setting%2C%20there%20is%20to%20our%20knowledge%20no%20theoretical%0Aguarantee%20for%20this%20observation.%20Leveraging%20recent%20works%20on%20convergence%20of%20SGD%0Aon%20non-smooth%20and%20non-convex%20functions%20by%20Bianchi%20et%20al.%20%282022%29%2C%20we%20aim%20to%0Abridge%20that%20knowledge%20gap%2C%20and%20provide%20a%20realistic%20context%20under%20which%0Afixed-step%20SGD%20trajectories%20for%20the%20SW%20loss%20on%20NN%20parameters%20converge.%20More%0Aprecisely%2C%20we%20show%20that%20the%20trajectories%20approach%20the%20set%20of%20%28sub%29-gradient%0Aflow%20equations%20as%20the%20step%20decreases.%20Under%20stricter%20assumptions%2C%20we%20show%20a%0Amuch%20stronger%20convergence%20result%20for%20noised%20and%20projected%20SGD%20schemes%2C%20namely%0Athat%20the%20long-run%20limits%20of%20the%20trajectories%20approach%20a%20set%20of%20generalised%0Acritical%20points%20of%20the%20loss%20function.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.11714v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Convergence%20of%20SGD%20for%20Training%20Neural%20Networks%20with%20Sliced%20Wasserstein%0A%20%20Losses&entry.906535625=Eloi%20Tanguy&entry.1292438233=%20%20Optimal%20Transport%20has%20sparked%20vivid%20interest%20in%20recent%20years%2C%20in%20particular%0Athanks%20to%20the%20Wasserstein%20distance%2C%20which%20provides%20a%20geometrically%20sensible%20and%0Aintuitive%20way%20of%20comparing%20probability%20measures.%20For%20computational%20reasons%2C%20the%0ASliced%20Wasserstein%20%28SW%29%20distance%20was%20introduced%20as%20an%20alternative%20to%20the%0AWasserstein%20distance%2C%20and%20has%20seen%20uses%20for%20training%20generative%20Neural%20Networks%0A%28NNs%29.%20While%20convergence%20of%20Stochastic%20Gradient%20Descent%20%28SGD%29%20has%20been%20observed%0Apractically%20in%20such%20a%20setting%2C%20there%20is%20to%20our%20knowledge%20no%20theoretical%0Aguarantee%20for%20this%20observation.%20Leveraging%20recent%20works%20on%20convergence%20of%20SGD%0Aon%20non-smooth%20and%20non-convex%20functions%20by%20Bianchi%20et%20al.%20%282022%29%2C%20we%20aim%20to%0Abridge%20that%20knowledge%20gap%2C%20and%20provide%20a%20realistic%20context%20under%20which%0Afixed-step%20SGD%20trajectories%20for%20the%20SW%20loss%20on%20NN%20parameters%20converge.%20More%0Aprecisely%2C%20we%20show%20that%20the%20trajectories%20approach%20the%20set%20of%20%28sub%29-gradient%0Aflow%20equations%20as%20the%20step%20decreases.%20Under%20stricter%20assumptions%2C%20we%20show%20a%0Amuch%20stronger%20convergence%20result%20for%20noised%20and%20projected%20SGD%20schemes%2C%20namely%0Athat%20the%20long-run%20limits%20of%20the%20trajectories%20approach%20a%20set%20of%20generalised%0Acritical%20points%20of%20the%20loss%20function.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.11714v3&entry.124074799=Read"},
{"title": "A Comprehensive Evaluation of Augmentations for Robust OOD\n  Self-Supervised Contrastive Phonocardiogram Representation Learning", "author": "Aristotelis Ballas and Vasileios Papapanagiotou and Christos Diou", "abstract": "  Despite the recent increase in research activity, deep-learning models have\nnot yet been widely accepted in several real-world settings, such as medicine.\nThe shortage of high-quality annotated data often hinders the development of\nrobust and generalizable models, which do not suffer from degraded\neffectiveness when presented with newly-collected, out-of-distribution (OOD)\ndatasets. Contrastive Self-Supervised Learning (SSL) offers a potential\nsolution to labeled data scarcity, as it takes advantage of unlabeled data to\nincrease model effectiveness and robustness. In this research, we propose\napplying contrastive SSL for detecting abnormalities in 1D phonocardiogram\n(PCG) samples by learning a generalized representation of the signal.\nSpecifically, we perform an extensive comparative evaluation of a wide range of\naudio-based augmentations, evaluate trained classifiers on multiple datasets\nacross different downstream tasks, and finally report on the impact of each\naugmentation in model training. We experimentally demonstrate that, depending\non its training distribution, the effectiveness of a fully-supervised model can\ndegrade up to 32% when evaluated on unseen data, while SSL models only lose up\nto 10% or even improve in some cases. We argue and experimentally demonstrate\nthat, contrastive SSL pretraining can assist in providing robust classifiers\nwhich can generalize to unseen, OOD data, without relying on time- and\nlabor-intensive annotation processes by medical experts. Furthermore, the\nproposed extensive evaluation protocol sheds light on the most promising and\nappropriate augmentations for robust PCG signal processing, by calculating\ntheir effect size on model training. Finally, we provide researchers and\npractitioners with a roadmap towards producing robust models for PCG\nclassification, in addition to an open-source codebase for developing novel\napproaches.\n", "link": "http://arxiv.org/abs/2312.00502v2", "date": "2024-03-18", "relevancy": 1.943, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4934}, {"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.4904}, {"title": "Weakly-guided self-supervised pretraining for temporal activity detection", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/25189", "similarity": 0.478}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Comprehensive%20Evaluation%20of%20Augmentations%20for%20Robust%20OOD%0A%20%20Self-Supervised%20Contrastive%20Phonocardiogram%20Representation%20Learning&body=Title%3A%20A%20Comprehensive%20Evaluation%20of%20Augmentations%20for%20Robust%20OOD%0A%20%20Self-Supervised%20Contrastive%20Phonocardiogram%20Representation%20Learning%0AAuthor%3A%20Aristotelis%20Ballas%20and%20Vasileios%20Papapanagiotou%20and%20Christos%20Diou%0AAbstract%3A%20%20%20Despite%20the%20recent%20increase%20in%20research%20activity%2C%20deep-learning%20models%20have%0Anot%20yet%20been%20widely%20accepted%20in%20several%20real-world%20settings%2C%20such%20as%20medicine.%0AThe%20shortage%20of%20high-quality%20annotated%20data%20often%20hinders%20the%20development%20of%0Arobust%20and%20generalizable%20models%2C%20which%20do%20not%20suffer%20from%20degraded%0Aeffectiveness%20when%20presented%20with%20newly-collected%2C%20out-of-distribution%20%28OOD%29%0Adatasets.%20Contrastive%20Self-Supervised%20Learning%20%28SSL%29%20offers%20a%20potential%0Asolution%20to%20labeled%20data%20scarcity%2C%20as%20it%20takes%20advantage%20of%20unlabeled%20data%20to%0Aincrease%20model%20effectiveness%20and%20robustness.%20In%20this%20research%2C%20we%20propose%0Aapplying%20contrastive%20SSL%20for%20detecting%20abnormalities%20in%201D%20phonocardiogram%0A%28PCG%29%20samples%20by%20learning%20a%20generalized%20representation%20of%20the%20signal.%0ASpecifically%2C%20we%20perform%20an%20extensive%20comparative%20evaluation%20of%20a%20wide%20range%20of%0Aaudio-based%20augmentations%2C%20evaluate%20trained%20classifiers%20on%20multiple%20datasets%0Aacross%20different%20downstream%20tasks%2C%20and%20finally%20report%20on%20the%20impact%20of%20each%0Aaugmentation%20in%20model%20training.%20We%20experimentally%20demonstrate%20that%2C%20depending%0Aon%20its%20training%20distribution%2C%20the%20effectiveness%20of%20a%20fully-supervised%20model%20can%0Adegrade%20up%20to%2032%25%20when%20evaluated%20on%20unseen%20data%2C%20while%20SSL%20models%20only%20lose%20up%0Ato%2010%25%20or%20even%20improve%20in%20some%20cases.%20We%20argue%20and%20experimentally%20demonstrate%0Athat%2C%20contrastive%20SSL%20pretraining%20can%20assist%20in%20providing%20robust%20classifiers%0Awhich%20can%20generalize%20to%20unseen%2C%20OOD%20data%2C%20without%20relying%20on%20time-%20and%0Alabor-intensive%20annotation%20processes%20by%20medical%20experts.%20Furthermore%2C%20the%0Aproposed%20extensive%20evaluation%20protocol%20sheds%20light%20on%20the%20most%20promising%20and%0Aappropriate%20augmentations%20for%20robust%20PCG%20signal%20processing%2C%20by%20calculating%0Atheir%20effect%20size%20on%20model%20training.%20Finally%2C%20we%20provide%20researchers%20and%0Apractitioners%20with%20a%20roadmap%20towards%20producing%20robust%20models%20for%20PCG%0Aclassification%2C%20in%20addition%20to%20an%20open-source%20codebase%20for%20developing%20novel%0Aapproaches.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2312.00502v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Comprehensive%20Evaluation%20of%20Augmentations%20for%20Robust%20OOD%0A%20%20Self-Supervised%20Contrastive%20Phonocardiogram%20Representation%20Learning&entry.906535625=Aristotelis%20Ballas%20and%20Vasileios%20Papapanagiotou%20and%20Christos%20Diou&entry.1292438233=%20%20Despite%20the%20recent%20increase%20in%20research%20activity%2C%20deep-learning%20models%20have%0Anot%20yet%20been%20widely%20accepted%20in%20several%20real-world%20settings%2C%20such%20as%20medicine.%0AThe%20shortage%20of%20high-quality%20annotated%20data%20often%20hinders%20the%20development%20of%0Arobust%20and%20generalizable%20models%2C%20which%20do%20not%20suffer%20from%20degraded%0Aeffectiveness%20when%20presented%20with%20newly-collected%2C%20out-of-distribution%20%28OOD%29%0Adatasets.%20Contrastive%20Self-Supervised%20Learning%20%28SSL%29%20offers%20a%20potential%0Asolution%20to%20labeled%20data%20scarcity%2C%20as%20it%20takes%20advantage%20of%20unlabeled%20data%20to%0Aincrease%20model%20effectiveness%20and%20robustness.%20In%20this%20research%2C%20we%20propose%0Aapplying%20contrastive%20SSL%20for%20detecting%20abnormalities%20in%201D%20phonocardiogram%0A%28PCG%29%20samples%20by%20learning%20a%20generalized%20representation%20of%20the%20signal.%0ASpecifically%2C%20we%20perform%20an%20extensive%20comparative%20evaluation%20of%20a%20wide%20range%20of%0Aaudio-based%20augmentations%2C%20evaluate%20trained%20classifiers%20on%20multiple%20datasets%0Aacross%20different%20downstream%20tasks%2C%20and%20finally%20report%20on%20the%20impact%20of%20each%0Aaugmentation%20in%20model%20training.%20We%20experimentally%20demonstrate%20that%2C%20depending%0Aon%20its%20training%20distribution%2C%20the%20effectiveness%20of%20a%20fully-supervised%20model%20can%0Adegrade%20up%20to%2032%25%20when%20evaluated%20on%20unseen%20data%2C%20while%20SSL%20models%20only%20lose%20up%0Ato%2010%25%20or%20even%20improve%20in%20some%20cases.%20We%20argue%20and%20experimentally%20demonstrate%0Athat%2C%20contrastive%20SSL%20pretraining%20can%20assist%20in%20providing%20robust%20classifiers%0Awhich%20can%20generalize%20to%20unseen%2C%20OOD%20data%2C%20without%20relying%20on%20time-%20and%0Alabor-intensive%20annotation%20processes%20by%20medical%20experts.%20Furthermore%2C%20the%0Aproposed%20extensive%20evaluation%20protocol%20sheds%20light%20on%20the%20most%20promising%20and%0Aappropriate%20augmentations%20for%20robust%20PCG%20signal%20processing%2C%20by%20calculating%0Atheir%20effect%20size%20on%20model%20training.%20Finally%2C%20we%20provide%20researchers%20and%0Apractitioners%20with%20a%20roadmap%20towards%20producing%20robust%20models%20for%20PCG%0Aclassification%2C%20in%20addition%20to%20an%20open-source%20codebase%20for%20developing%20novel%0Aapproaches.%0A&entry.1838667208=http%3A//arxiv.org/abs/2312.00502v2&entry.124074799=Read"},
{"title": "Histo-Genomic Knowledge Distillation For Cancer Prognosis From\n  Histopathology Whole Slide Images", "author": "Zhikang Wang and Yumeng Zhang and Yingxue Xu and Seiya Imoto and Hao Chen and Jiangning Song", "abstract": "  Histo-genomic multi-modal methods have recently emerged as a powerful\nparadigm, demonstrating significant potential for improving cancer prognosis.\nHowever, genome sequencing, unlike histopathology imaging, is still not widely\naccessible in underdeveloped regions, limiting the application of these\nmulti-modal approaches in clinical settings. To address this, we propose a\nnovel Genome-informed Hyper-Attention Network, termed G-HANet, which is capable\nof effectively distilling the histo-genomic knowledge during training to\nelevate uni-modal whole slide image (WSI)-based inference for the first time.\nCompared with traditional knowledge distillation methods (i.e., teacher-student\narchitecture) in other tasks, our end-to-end model is superior in terms of\ntraining efficiency and learning cross-modal interactions. Specifically, the\nnetwork comprises the cross-modal associating branch (CAB) and hyper-attention\nsurvival branch (HSB). Through the genomic data reconstruction from WSIs, CAB\neffectively distills the associations between functional genotypes and\nmorphological phenotypes and offers insights into the gene expression profiles\nin the feature space. Subsequently, HSB leverages the distilled histo-genomic\nassociations as well as the generated morphology-based weights to achieve the\nhyper-attention modeling of the patients from both histopathology and genomic\nperspectives to improve cancer prognosis. Extensive experiments are conducted\non five TCGA benchmarking datasets and the results demonstrate that G-HANet\nsignificantly outperforms the state-of-the-art WSI-based methods and achieves\ncompetitive performance with genome-based and multi-modal methods. G-HANet is\nexpected to be explored as a useful tool by the research community to address\nthe current bottleneck of insufficient histo-genomic data pairing in the\ncontext of cancer prognosis and precision oncology.\n", "link": "http://arxiv.org/abs/2403.10040v2", "date": "2024-03-18", "relevancy": 1.8878, "topK": [{"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4743}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4714}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4675}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Histo-Genomic%20Knowledge%20Distillation%20For%20Cancer%20Prognosis%20From%0A%20%20Histopathology%20Whole%20Slide%20Images&body=Title%3A%20Histo-Genomic%20Knowledge%20Distillation%20For%20Cancer%20Prognosis%20From%0A%20%20Histopathology%20Whole%20Slide%20Images%0AAuthor%3A%20Zhikang%20Wang%20and%20Yumeng%20Zhang%20and%20Yingxue%20Xu%20and%20Seiya%20Imoto%20and%20Hao%20Chen%20and%20Jiangning%20Song%0AAbstract%3A%20%20%20Histo-genomic%20multi-modal%20methods%20have%20recently%20emerged%20as%20a%20powerful%0Aparadigm%2C%20demonstrating%20significant%20potential%20for%20improving%20cancer%20prognosis.%0AHowever%2C%20genome%20sequencing%2C%20unlike%20histopathology%20imaging%2C%20is%20still%20not%20widely%0Aaccessible%20in%20underdeveloped%20regions%2C%20limiting%20the%20application%20of%20these%0Amulti-modal%20approaches%20in%20clinical%20settings.%20To%20address%20this%2C%20we%20propose%20a%0Anovel%20Genome-informed%20Hyper-Attention%20Network%2C%20termed%20G-HANet%2C%20which%20is%20capable%0Aof%20effectively%20distilling%20the%20histo-genomic%20knowledge%20during%20training%20to%0Aelevate%20uni-modal%20whole%20slide%20image%20%28WSI%29-based%20inference%20for%20the%20first%20time.%0ACompared%20with%20traditional%20knowledge%20distillation%20methods%20%28i.e.%2C%20teacher-student%0Aarchitecture%29%20in%20other%20tasks%2C%20our%20end-to-end%20model%20is%20superior%20in%20terms%20of%0Atraining%20efficiency%20and%20learning%20cross-modal%20interactions.%20Specifically%2C%20the%0Anetwork%20comprises%20the%20cross-modal%20associating%20branch%20%28CAB%29%20and%20hyper-attention%0Asurvival%20branch%20%28HSB%29.%20Through%20the%20genomic%20data%20reconstruction%20from%20WSIs%2C%20CAB%0Aeffectively%20distills%20the%20associations%20between%20functional%20genotypes%20and%0Amorphological%20phenotypes%20and%20offers%20insights%20into%20the%20gene%20expression%20profiles%0Ain%20the%20feature%20space.%20Subsequently%2C%20HSB%20leverages%20the%20distilled%20histo-genomic%0Aassociations%20as%20well%20as%20the%20generated%20morphology-based%20weights%20to%20achieve%20the%0Ahyper-attention%20modeling%20of%20the%20patients%20from%20both%20histopathology%20and%20genomic%0Aperspectives%20to%20improve%20cancer%20prognosis.%20Extensive%20experiments%20are%20conducted%0Aon%20five%20TCGA%20benchmarking%20datasets%20and%20the%20results%20demonstrate%20that%20G-HANet%0Asignificantly%20outperforms%20the%20state-of-the-art%20WSI-based%20methods%20and%20achieves%0Acompetitive%20performance%20with%20genome-based%20and%20multi-modal%20methods.%20G-HANet%20is%0Aexpected%20to%20be%20explored%20as%20a%20useful%20tool%20by%20the%20research%20community%20to%20address%0Athe%20current%20bottleneck%20of%20insufficient%20histo-genomic%20data%20pairing%20in%20the%0Acontext%20of%20cancer%20prognosis%20and%20precision%20oncology.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.10040v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Histo-Genomic%20Knowledge%20Distillation%20For%20Cancer%20Prognosis%20From%0A%20%20Histopathology%20Whole%20Slide%20Images&entry.906535625=Zhikang%20Wang%20and%20Yumeng%20Zhang%20and%20Yingxue%20Xu%20and%20Seiya%20Imoto%20and%20Hao%20Chen%20and%20Jiangning%20Song&entry.1292438233=%20%20Histo-genomic%20multi-modal%20methods%20have%20recently%20emerged%20as%20a%20powerful%0Aparadigm%2C%20demonstrating%20significant%20potential%20for%20improving%20cancer%20prognosis.%0AHowever%2C%20genome%20sequencing%2C%20unlike%20histopathology%20imaging%2C%20is%20still%20not%20widely%0Aaccessible%20in%20underdeveloped%20regions%2C%20limiting%20the%20application%20of%20these%0Amulti-modal%20approaches%20in%20clinical%20settings.%20To%20address%20this%2C%20we%20propose%20a%0Anovel%20Genome-informed%20Hyper-Attention%20Network%2C%20termed%20G-HANet%2C%20which%20is%20capable%0Aof%20effectively%20distilling%20the%20histo-genomic%20knowledge%20during%20training%20to%0Aelevate%20uni-modal%20whole%20slide%20image%20%28WSI%29-based%20inference%20for%20the%20first%20time.%0ACompared%20with%20traditional%20knowledge%20distillation%20methods%20%28i.e.%2C%20teacher-student%0Aarchitecture%29%20in%20other%20tasks%2C%20our%20end-to-end%20model%20is%20superior%20in%20terms%20of%0Atraining%20efficiency%20and%20learning%20cross-modal%20interactions.%20Specifically%2C%20the%0Anetwork%20comprises%20the%20cross-modal%20associating%20branch%20%28CAB%29%20and%20hyper-attention%0Asurvival%20branch%20%28HSB%29.%20Through%20the%20genomic%20data%20reconstruction%20from%20WSIs%2C%20CAB%0Aeffectively%20distills%20the%20associations%20between%20functional%20genotypes%20and%0Amorphological%20phenotypes%20and%20offers%20insights%20into%20the%20gene%20expression%20profiles%0Ain%20the%20feature%20space.%20Subsequently%2C%20HSB%20leverages%20the%20distilled%20histo-genomic%0Aassociations%20as%20well%20as%20the%20generated%20morphology-based%20weights%20to%20achieve%20the%0Ahyper-attention%20modeling%20of%20the%20patients%20from%20both%20histopathology%20and%20genomic%0Aperspectives%20to%20improve%20cancer%20prognosis.%20Extensive%20experiments%20are%20conducted%0Aon%20five%20TCGA%20benchmarking%20datasets%20and%20the%20results%20demonstrate%20that%20G-HANet%0Asignificantly%20outperforms%20the%20state-of-the-art%20WSI-based%20methods%20and%20achieves%0Acompetitive%20performance%20with%20genome-based%20and%20multi-modal%20methods.%20G-HANet%20is%0Aexpected%20to%20be%20explored%20as%20a%20useful%20tool%20by%20the%20research%20community%20to%20address%0Athe%20current%20bottleneck%20of%20insufficient%20histo-genomic%20data%20pairing%20in%20the%0Acontext%20of%20cancer%20prognosis%20and%20precision%20oncology.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.10040v2&entry.124074799=Read"},
{"title": "Matching Non-Identical Objects", "author": "Yusuke Marumo and Kazuhiko Kawamoto and Hiroshi Kera", "abstract": "  Not identical but similar objects are everywhere in the world. Examples\ninclude four-legged animals such as dogs and cats, cars of different models,\nakin flowers in various colors, and countless others. In this study, we address\na novel task of matching such non-identical objects. We propose a simple\nweighting scheme of descriptors that enhances various sparse image matching\nmethods, which were originally designed for matching identical objects captured\nfrom different perspectives, and achieve semantically robust matching. The\nexperiments show successful matching between non-identical objects in various\ncases including domain shift. Further, we present a first evaluation of the\nrobustness of the image matching methods under common corruptions, which is a\nsort of domain shift, and the proposed method improves the matching in this\ncase as well.\n", "link": "http://arxiv.org/abs/2403.08227v2", "date": "2024-03-18", "relevancy": 1.8779, "topK": [{"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.4861}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.458}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4574}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Matching%20Non-Identical%20Objects&body=Title%3A%20Matching%20Non-Identical%20Objects%0AAuthor%3A%20Yusuke%20Marumo%20and%20Kazuhiko%20Kawamoto%20and%20Hiroshi%20Kera%0AAbstract%3A%20%20%20Not%20identical%20but%20similar%20objects%20are%20everywhere%20in%20the%20world.%20Examples%0Ainclude%20four-legged%20animals%20such%20as%20dogs%20and%20cats%2C%20cars%20of%20different%20models%2C%0Aakin%20flowers%20in%20various%20colors%2C%20and%20countless%20others.%20In%20this%20study%2C%20we%20address%0Aa%20novel%20task%20of%20matching%20such%20non-identical%20objects.%20We%20propose%20a%20simple%0Aweighting%20scheme%20of%20descriptors%20that%20enhances%20various%20sparse%20image%20matching%0Amethods%2C%20which%20were%20originally%20designed%20for%20matching%20identical%20objects%20captured%0Afrom%20different%20perspectives%2C%20and%20achieve%20semantically%20robust%20matching.%20The%0Aexperiments%20show%20successful%20matching%20between%20non-identical%20objects%20in%20various%0Acases%20including%20domain%20shift.%20Further%2C%20we%20present%20a%20first%20evaluation%20of%20the%0Arobustness%20of%20the%20image%20matching%20methods%20under%20common%20corruptions%2C%20which%20is%20a%0Asort%20of%20domain%20shift%2C%20and%20the%20proposed%20method%20improves%20the%20matching%20in%20this%0Acase%20as%20well.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.08227v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Matching%20Non-Identical%20Objects&entry.906535625=Yusuke%20Marumo%20and%20Kazuhiko%20Kawamoto%20and%20Hiroshi%20Kera&entry.1292438233=%20%20Not%20identical%20but%20similar%20objects%20are%20everywhere%20in%20the%20world.%20Examples%0Ainclude%20four-legged%20animals%20such%20as%20dogs%20and%20cats%2C%20cars%20of%20different%20models%2C%0Aakin%20flowers%20in%20various%20colors%2C%20and%20countless%20others.%20In%20this%20study%2C%20we%20address%0Aa%20novel%20task%20of%20matching%20such%20non-identical%20objects.%20We%20propose%20a%20simple%0Aweighting%20scheme%20of%20descriptors%20that%20enhances%20various%20sparse%20image%20matching%0Amethods%2C%20which%20were%20originally%20designed%20for%20matching%20identical%20objects%20captured%0Afrom%20different%20perspectives%2C%20and%20achieve%20semantically%20robust%20matching.%20The%0Aexperiments%20show%20successful%20matching%20between%20non-identical%20objects%20in%20various%0Acases%20including%20domain%20shift.%20Further%2C%20we%20present%20a%20first%20evaluation%20of%20the%0Arobustness%20of%20the%20image%20matching%20methods%20under%20common%20corruptions%2C%20which%20is%20a%0Asort%20of%20domain%20shift%2C%20and%20the%20proposed%20method%20improves%20the%20matching%20in%20this%0Acase%20as%20well.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.08227v2&entry.124074799=Read"},
{"title": "Synthesizing multi-log grasp poses", "author": "Arvid F\u00e4lldin and Erik Wallin and Tommy L\u00f6fstedt and Martin Servin", "abstract": "  Multi-object grasping is a challenging task. It is important for energy and\ncost-efficient operation of industrial crane manipulators, such as those used\nto collect tree logs off the forest floor and onto forest machines. In this\nwork, we used synthetic data from physics simulations to explore how\ndata-driven modeling can be used to infer multi-object grasp poses from images.\nWe showed that convolutional neural networks can be trained specifically for\nsynthesizing multi-object grasps. Using RGB-Depth images and instance\nsegmentation masks as input, a U-Net model outputs grasp maps with\ncorresponding grapple orientation and opening width. Given an observation of a\npile of logs, the model can be used to synthesize and rate the possible grasp\nposes and select the most suitable one, with the possibility to respect\nchanging operational constraints such as lift capacity and reach. When tested\non previously unseen data, the proposed model found successful grasp poses with\nan accuracy of 95%.\n", "link": "http://arxiv.org/abs/2403.11623v1", "date": "2024-03-18", "relevancy": 1.8777, "topK": [{"title": "UGG: Unified Generative Grasping", "link": "https://arxiv.org/abs/2311.16917", "similarity": 0.6734}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5842}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5487}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Synthesizing%20multi-log%20grasp%20poses&body=Title%3A%20Synthesizing%20multi-log%20grasp%20poses%0AAuthor%3A%20Arvid%20F%C3%A4lldin%20and%20Erik%20Wallin%20and%20Tommy%20L%C3%B6fstedt%20and%20Martin%20Servin%0AAbstract%3A%20%20%20Multi-object%20grasping%20is%20a%20challenging%20task.%20It%20is%20important%20for%20energy%20and%0Acost-efficient%20operation%20of%20industrial%20crane%20manipulators%2C%20such%20as%20those%20used%0Ato%20collect%20tree%20logs%20off%20the%20forest%20floor%20and%20onto%20forest%20machines.%20In%20this%0Awork%2C%20we%20used%20synthetic%20data%20from%20physics%20simulations%20to%20explore%20how%0Adata-driven%20modeling%20can%20be%20used%20to%20infer%20multi-object%20grasp%20poses%20from%20images.%0AWe%20showed%20that%20convolutional%20neural%20networks%20can%20be%20trained%20specifically%20for%0Asynthesizing%20multi-object%20grasps.%20Using%20RGB-Depth%20images%20and%20instance%0Asegmentation%20masks%20as%20input%2C%20a%20U-Net%20model%20outputs%20grasp%20maps%20with%0Acorresponding%20grapple%20orientation%20and%20opening%20width.%20Given%20an%20observation%20of%20a%0Apile%20of%20logs%2C%20the%20model%20can%20be%20used%20to%20synthesize%20and%20rate%20the%20possible%20grasp%0Aposes%20and%20select%20the%20most%20suitable%20one%2C%20with%20the%20possibility%20to%20respect%0Achanging%20operational%20constraints%20such%20as%20lift%20capacity%20and%20reach.%20When%20tested%0Aon%20previously%20unseen%20data%2C%20the%20proposed%20model%20found%20successful%20grasp%20poses%20with%0Aan%20accuracy%20of%2095%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11623v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Synthesizing%20multi-log%20grasp%20poses&entry.906535625=Arvid%20F%C3%A4lldin%20and%20Erik%20Wallin%20and%20Tommy%20L%C3%B6fstedt%20and%20Martin%20Servin&entry.1292438233=%20%20Multi-object%20grasping%20is%20a%20challenging%20task.%20It%20is%20important%20for%20energy%20and%0Acost-efficient%20operation%20of%20industrial%20crane%20manipulators%2C%20such%20as%20those%20used%0Ato%20collect%20tree%20logs%20off%20the%20forest%20floor%20and%20onto%20forest%20machines.%20In%20this%0Awork%2C%20we%20used%20synthetic%20data%20from%20physics%20simulations%20to%20explore%20how%0Adata-driven%20modeling%20can%20be%20used%20to%20infer%20multi-object%20grasp%20poses%20from%20images.%0AWe%20showed%20that%20convolutional%20neural%20networks%20can%20be%20trained%20specifically%20for%0Asynthesizing%20multi-object%20grasps.%20Using%20RGB-Depth%20images%20and%20instance%0Asegmentation%20masks%20as%20input%2C%20a%20U-Net%20model%20outputs%20grasp%20maps%20with%0Acorresponding%20grapple%20orientation%20and%20opening%20width.%20Given%20an%20observation%20of%20a%0Apile%20of%20logs%2C%20the%20model%20can%20be%20used%20to%20synthesize%20and%20rate%20the%20possible%20grasp%0Aposes%20and%20select%20the%20most%20suitable%20one%2C%20with%20the%20possibility%20to%20respect%0Achanging%20operational%20constraints%20such%20as%20lift%20capacity%20and%20reach.%20When%20tested%0Aon%20previously%20unseen%20data%2C%20the%20proposed%20model%20found%20successful%20grasp%20poses%20with%0Aan%20accuracy%20of%2095%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11623v1&entry.124074799=Read"},
{"title": "Gridless 2D Recovery of Lines using the Sliding Frank-Wolfe Algorithm", "author": "K\u00e9vin Polisano and Basile Dubois-Bonnaire and Sylvain Meignen", "abstract": "  We present a new approach leveraging the Sliding Frank--Wolfe algorithm to\naddress the challenge of line recovery in degraded images. Building upon\nadvances in conditional gradient methods for sparse inverse problems with\ndifferentiable measurement models, we propose two distinct models tailored for\nline detection tasks within the realm of blurred line deconvolution and ridge\ndetection of linear chirps in spectrogram images.\n", "link": "http://arxiv.org/abs/2403.11649v1", "date": "2024-03-18", "relevancy": 1.8735, "topK": [{"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4796}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4622}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4596}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Gridless%202D%20Recovery%20of%20Lines%20using%20the%20Sliding%20Frank-Wolfe%20Algorithm&body=Title%3A%20Gridless%202D%20Recovery%20of%20Lines%20using%20the%20Sliding%20Frank-Wolfe%20Algorithm%0AAuthor%3A%20K%C3%A9vin%20Polisano%20and%20Basile%20Dubois-Bonnaire%20and%20Sylvain%20Meignen%0AAbstract%3A%20%20%20We%20present%20a%20new%20approach%20leveraging%20the%20Sliding%20Frank--Wolfe%20algorithm%20to%0Aaddress%20the%20challenge%20of%20line%20recovery%20in%20degraded%20images.%20Building%20upon%0Aadvances%20in%20conditional%20gradient%20methods%20for%20sparse%20inverse%20problems%20with%0Adifferentiable%20measurement%20models%2C%20we%20propose%20two%20distinct%20models%20tailored%20for%0Aline%20detection%20tasks%20within%20the%20realm%20of%20blurred%20line%20deconvolution%20and%20ridge%0Adetection%20of%20linear%20chirps%20in%20spectrogram%20images.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11649v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Gridless%202D%20Recovery%20of%20Lines%20using%20the%20Sliding%20Frank-Wolfe%20Algorithm&entry.906535625=K%C3%A9vin%20Polisano%20and%20Basile%20Dubois-Bonnaire%20and%20Sylvain%20Meignen&entry.1292438233=%20%20We%20present%20a%20new%20approach%20leveraging%20the%20Sliding%20Frank--Wolfe%20algorithm%20to%0Aaddress%20the%20challenge%20of%20line%20recovery%20in%20degraded%20images.%20Building%20upon%0Aadvances%20in%20conditional%20gradient%20methods%20for%20sparse%20inverse%20problems%20with%0Adifferentiable%20measurement%20models%2C%20we%20propose%20two%20distinct%20models%20tailored%20for%0Aline%20detection%20tasks%20within%20the%20realm%20of%20blurred%20line%20deconvolution%20and%20ridge%0Adetection%20of%20linear%20chirps%20in%20spectrogram%20images.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11649v1&entry.124074799=Read"},
{"title": "Nonsmooth Implicit Differentiation: Deterministic and Stochastic\n  Convergence Rates", "author": "Riccardo Grazzi and Massimiliano Pontil and Saverio Salzo", "abstract": "  We study the problem of efficiently computing the derivative of the\nfixed-point of a parametric non-differentiable contraction map. This problem\nhas wide applications in machine learning, including hyperparameter\noptimization, meta-learning and data poisoning attacks. We analyze two popular\napproaches: iterative differentiation (ITD) and approximate implicit\ndifferentiation (AID). A key challenge behind the nonsmooth setting is that the\nchain rule does not hold anymore. Building upon the recent work by Bolte et al.\n(2022), who proved the linear convergence of non-differentiable ITD, we provide\nrefined linear convergence rates for both ITD and AID in the deterministic\ncase. We further introduce NSID, a new method to compute the implicit\nderivative when the fixed point is defined as the composition of an outer map\nand an inner map which is accessible only through a stochastic unbiased\nestimator. We establish rates for the convergence of NSID to the true\nderivative, encompassing the best available rates in the smooth setting. We\npresent illustrative experiments confirming our analysis.\n", "link": "http://arxiv.org/abs/2403.11687v1", "date": "2024-03-18", "relevancy": 1.8542, "topK": [{"title": "Boosted dynamic neural networks", "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26302", "similarity": 0.4645}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4644}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.459}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Nonsmooth%20Implicit%20Differentiation%3A%20Deterministic%20and%20Stochastic%0A%20%20Convergence%20Rates&body=Title%3A%20Nonsmooth%20Implicit%20Differentiation%3A%20Deterministic%20and%20Stochastic%0A%20%20Convergence%20Rates%0AAuthor%3A%20Riccardo%20Grazzi%20and%20Massimiliano%20Pontil%20and%20Saverio%20Salzo%0AAbstract%3A%20%20%20We%20study%20the%20problem%20of%20efficiently%20computing%20the%20derivative%20of%20the%0Afixed-point%20of%20a%20parametric%20non-differentiable%20contraction%20map.%20This%20problem%0Ahas%20wide%20applications%20in%20machine%20learning%2C%20including%20hyperparameter%0Aoptimization%2C%20meta-learning%20and%20data%20poisoning%20attacks.%20We%20analyze%20two%20popular%0Aapproaches%3A%20iterative%20differentiation%20%28ITD%29%20and%20approximate%20implicit%0Adifferentiation%20%28AID%29.%20A%20key%20challenge%20behind%20the%20nonsmooth%20setting%20is%20that%20the%0Achain%20rule%20does%20not%20hold%20anymore.%20Building%20upon%20the%20recent%20work%20by%20Bolte%20et%20al.%0A%282022%29%2C%20who%20proved%20the%20linear%20convergence%20of%20non-differentiable%20ITD%2C%20we%20provide%0Arefined%20linear%20convergence%20rates%20for%20both%20ITD%20and%20AID%20in%20the%20deterministic%0Acase.%20We%20further%20introduce%20NSID%2C%20a%20new%20method%20to%20compute%20the%20implicit%0Aderivative%20when%20the%20fixed%20point%20is%20defined%20as%20the%20composition%20of%20an%20outer%20map%0Aand%20an%20inner%20map%20which%20is%20accessible%20only%20through%20a%20stochastic%20unbiased%0Aestimator.%20We%20establish%20rates%20for%20the%20convergence%20of%20NSID%20to%20the%20true%0Aderivative%2C%20encompassing%20the%20best%20available%20rates%20in%20the%20smooth%20setting.%20We%0Apresent%20illustrative%20experiments%20confirming%20our%20analysis.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11687v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Nonsmooth%20Implicit%20Differentiation%3A%20Deterministic%20and%20Stochastic%0A%20%20Convergence%20Rates&entry.906535625=Riccardo%20Grazzi%20and%20Massimiliano%20Pontil%20and%20Saverio%20Salzo&entry.1292438233=%20%20We%20study%20the%20problem%20of%20efficiently%20computing%20the%20derivative%20of%20the%0Afixed-point%20of%20a%20parametric%20non-differentiable%20contraction%20map.%20This%20problem%0Ahas%20wide%20applications%20in%20machine%20learning%2C%20including%20hyperparameter%0Aoptimization%2C%20meta-learning%20and%20data%20poisoning%20attacks.%20We%20analyze%20two%20popular%0Aapproaches%3A%20iterative%20differentiation%20%28ITD%29%20and%20approximate%20implicit%0Adifferentiation%20%28AID%29.%20A%20key%20challenge%20behind%20the%20nonsmooth%20setting%20is%20that%20the%0Achain%20rule%20does%20not%20hold%20anymore.%20Building%20upon%20the%20recent%20work%20by%20Bolte%20et%20al.%0A%282022%29%2C%20who%20proved%20the%20linear%20convergence%20of%20non-differentiable%20ITD%2C%20we%20provide%0Arefined%20linear%20convergence%20rates%20for%20both%20ITD%20and%20AID%20in%20the%20deterministic%0Acase.%20We%20further%20introduce%20NSID%2C%20a%20new%20method%20to%20compute%20the%20implicit%0Aderivative%20when%20the%20fixed%20point%20is%20defined%20as%20the%20composition%20of%20an%20outer%20map%0Aand%20an%20inner%20map%20which%20is%20accessible%20only%20through%20a%20stochastic%20unbiased%0Aestimator.%20We%20establish%20rates%20for%20the%20convergence%20of%20NSID%20to%20the%20true%0Aderivative%2C%20encompassing%20the%20best%20available%20rates%20in%20the%20smooth%20setting.%20We%0Apresent%20illustrative%20experiments%20confirming%20our%20analysis.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11687v1&entry.124074799=Read"},
{"title": "Identifying Three-Dimensional Radiative Patterns Associated with Early\n  Tropical Cyclone Intensification", "author": "Frederick Iat-Hin Tam and Tom Beucler and James H. Ruppert Jr", "abstract": "  Cloud radiative feedback impacts early tropical cyclone (TC) intensification,\nbut limitations in existing diagnostic frameworks make them unsuitable for\nstudying asymmetric or transient radiative heating. We propose a linear\nVariational Encoder-Decoder (VED) to learn the hidden relationship between\nradiation and the surface intensification of realistic simulated TCs. Limiting\nVED model inputs enables using its uncertainty to identify periods when\nradiation has more importance for intensification. A close examination of the\nextracted 3D radiative structures suggests that longwave radiative forcing from\ninner core deep convection and shallow clouds both contribute to\nintensification, with the deep convection having the most impact overall. We\nfind that deep convection downwind of the shallow clouds is critical to the\nintensification of Haiyan. Our work demonstrates that machine learning can\ndiscover thermodynamic-kinematic relationships without relying on axisymmetric\nor deterministic assumptions, paving the way towards the objective discovery of\nprocesses leading to TC intensification in realistic conditions.\n", "link": "http://arxiv.org/abs/2401.09493v3", "date": "2024-03-18", "relevancy": 1.7474, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4422}, {"title": "IAE: Implicit Autoencoder for Point Cloud Self-supervised Representation Learning", "link": "https://arxiv.org/abs/2201.00785", "similarity": 0.436}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4356}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Identifying%20Three-Dimensional%20Radiative%20Patterns%20Associated%20with%20Early%0A%20%20Tropical%20Cyclone%20Intensification&body=Title%3A%20Identifying%20Three-Dimensional%20Radiative%20Patterns%20Associated%20with%20Early%0A%20%20Tropical%20Cyclone%20Intensification%0AAuthor%3A%20Frederick%20Iat-Hin%20Tam%20and%20Tom%20Beucler%20and%20James%20H.%20Ruppert%20Jr%0AAbstract%3A%20%20%20Cloud%20radiative%20feedback%20impacts%20early%20tropical%20cyclone%20%28TC%29%20intensification%2C%0Abut%20limitations%20in%20existing%20diagnostic%20frameworks%20make%20them%20unsuitable%20for%0Astudying%20asymmetric%20or%20transient%20radiative%20heating.%20We%20propose%20a%20linear%0AVariational%20Encoder-Decoder%20%28VED%29%20to%20learn%20the%20hidden%20relationship%20between%0Aradiation%20and%20the%20surface%20intensification%20of%20realistic%20simulated%20TCs.%20Limiting%0AVED%20model%20inputs%20enables%20using%20its%20uncertainty%20to%20identify%20periods%20when%0Aradiation%20has%20more%20importance%20for%20intensification.%20A%20close%20examination%20of%20the%0Aextracted%203D%20radiative%20structures%20suggests%20that%20longwave%20radiative%20forcing%20from%0Ainner%20core%20deep%20convection%20and%20shallow%20clouds%20both%20contribute%20to%0Aintensification%2C%20with%20the%20deep%20convection%20having%20the%20most%20impact%20overall.%20We%0Afind%20that%20deep%20convection%20downwind%20of%20the%20shallow%20clouds%20is%20critical%20to%20the%0Aintensification%20of%20Haiyan.%20Our%20work%20demonstrates%20that%20machine%20learning%20can%0Adiscover%20thermodynamic-kinematic%20relationships%20without%20relying%20on%20axisymmetric%0Aor%20deterministic%20assumptions%2C%20paving%20the%20way%20towards%20the%20objective%20discovery%20of%0Aprocesses%20leading%20to%20TC%20intensification%20in%20realistic%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2401.09493v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Identifying%20Three-Dimensional%20Radiative%20Patterns%20Associated%20with%20Early%0A%20%20Tropical%20Cyclone%20Intensification&entry.906535625=Frederick%20Iat-Hin%20Tam%20and%20Tom%20Beucler%20and%20James%20H.%20Ruppert%20Jr&entry.1292438233=%20%20Cloud%20radiative%20feedback%20impacts%20early%20tropical%20cyclone%20%28TC%29%20intensification%2C%0Abut%20limitations%20in%20existing%20diagnostic%20frameworks%20make%20them%20unsuitable%20for%0Astudying%20asymmetric%20or%20transient%20radiative%20heating.%20We%20propose%20a%20linear%0AVariational%20Encoder-Decoder%20%28VED%29%20to%20learn%20the%20hidden%20relationship%20between%0Aradiation%20and%20the%20surface%20intensification%20of%20realistic%20simulated%20TCs.%20Limiting%0AVED%20model%20inputs%20enables%20using%20its%20uncertainty%20to%20identify%20periods%20when%0Aradiation%20has%20more%20importance%20for%20intensification.%20A%20close%20examination%20of%20the%0Aextracted%203D%20radiative%20structures%20suggests%20that%20longwave%20radiative%20forcing%20from%0Ainner%20core%20deep%20convection%20and%20shallow%20clouds%20both%20contribute%20to%0Aintensification%2C%20with%20the%20deep%20convection%20having%20the%20most%20impact%20overall.%20We%0Afind%20that%20deep%20convection%20downwind%20of%20the%20shallow%20clouds%20is%20critical%20to%20the%0Aintensification%20of%20Haiyan.%20Our%20work%20demonstrates%20that%20machine%20learning%20can%0Adiscover%20thermodynamic-kinematic%20relationships%20without%20relying%20on%20axisymmetric%0Aor%20deterministic%20assumptions%2C%20paving%20the%20way%20towards%20the%20objective%20discovery%20of%0Aprocesses%20leading%20to%20TC%20intensification%20in%20realistic%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2401.09493v3&entry.124074799=Read"},
{"title": "Measuring Meaning Composition in the Human Brain with Composition Scores\n  from Large Language Models", "author": "Changjiang Gao and Jixing Li and Jiajun Chen and Shujian Huang", "abstract": "  The process of meaning composition, wherein smaller units like morphemes or\nwords combine to form the meaning of phrases and sentences, is essential for\nhuman sentence comprehension. Despite extensive neurolinguistic research into\nthe brain regions involved in meaning composition, a computational metric to\nquantify the extent of composition is still lacking. Drawing on the key-value\nmemory interpretation of transformer feed-forward network blocks, we introduce\nthe Composition Score, a novel model-based metric designed to quantify the\ndegree of meaning composition during sentence comprehension. Experimental\nfindings show that this metric correlates with brain clusters associated with\nword frequency, structural processing, and general sensitivity to words,\nsuggesting the multifaceted nature of meaning composition during human sentence\ncomprehension.\n", "link": "http://arxiv.org/abs/2403.04325v2", "date": "2024-03-18", "relevancy": 1.7197, "topK": [{"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4437}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4209}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4181}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Measuring%20Meaning%20Composition%20in%20the%20Human%20Brain%20with%20Composition%20Scores%0A%20%20from%20Large%20Language%20Models&body=Title%3A%20Measuring%20Meaning%20Composition%20in%20the%20Human%20Brain%20with%20Composition%20Scores%0A%20%20from%20Large%20Language%20Models%0AAuthor%3A%20Changjiang%20Gao%20and%20Jixing%20Li%20and%20Jiajun%20Chen%20and%20Shujian%20Huang%0AAbstract%3A%20%20%20The%20process%20of%20meaning%20composition%2C%20wherein%20smaller%20units%20like%20morphemes%20or%0Awords%20combine%20to%20form%20the%20meaning%20of%20phrases%20and%20sentences%2C%20is%20essential%20for%0Ahuman%20sentence%20comprehension.%20Despite%20extensive%20neurolinguistic%20research%20into%0Athe%20brain%20regions%20involved%20in%20meaning%20composition%2C%20a%20computational%20metric%20to%0Aquantify%20the%20extent%20of%20composition%20is%20still%20lacking.%20Drawing%20on%20the%20key-value%0Amemory%20interpretation%20of%20transformer%20feed-forward%20network%20blocks%2C%20we%20introduce%0Athe%20Composition%20Score%2C%20a%20novel%20model-based%20metric%20designed%20to%20quantify%20the%0Adegree%20of%20meaning%20composition%20during%20sentence%20comprehension.%20Experimental%0Afindings%20show%20that%20this%20metric%20correlates%20with%20brain%20clusters%20associated%20with%0Aword%20frequency%2C%20structural%20processing%2C%20and%20general%20sensitivity%20to%20words%2C%0Asuggesting%20the%20multifaceted%20nature%20of%20meaning%20composition%20during%20human%20sentence%0Acomprehension.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.04325v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Measuring%20Meaning%20Composition%20in%20the%20Human%20Brain%20with%20Composition%20Scores%0A%20%20from%20Large%20Language%20Models&entry.906535625=Changjiang%20Gao%20and%20Jixing%20Li%20and%20Jiajun%20Chen%20and%20Shujian%20Huang&entry.1292438233=%20%20The%20process%20of%20meaning%20composition%2C%20wherein%20smaller%20units%20like%20morphemes%20or%0Awords%20combine%20to%20form%20the%20meaning%20of%20phrases%20and%20sentences%2C%20is%20essential%20for%0Ahuman%20sentence%20comprehension.%20Despite%20extensive%20neurolinguistic%20research%20into%0Athe%20brain%20regions%20involved%20in%20meaning%20composition%2C%20a%20computational%20metric%20to%0Aquantify%20the%20extent%20of%20composition%20is%20still%20lacking.%20Drawing%20on%20the%20key-value%0Amemory%20interpretation%20of%20transformer%20feed-forward%20network%20blocks%2C%20we%20introduce%0Athe%20Composition%20Score%2C%20a%20novel%20model-based%20metric%20designed%20to%20quantify%20the%0Adegree%20of%20meaning%20composition%20during%20sentence%20comprehension.%20Experimental%0Afindings%20show%20that%20this%20metric%20correlates%20with%20brain%20clusters%20associated%20with%0Aword%20frequency%2C%20structural%20processing%2C%20and%20general%20sensitivity%20to%20words%2C%0Asuggesting%20the%20multifaceted%20nature%20of%20meaning%20composition%20during%20human%20sentence%0Acomprehension.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.04325v2&entry.124074799=Read"},
{"title": "MASSTAR: A Multi-Modal and Large-Scale Scene Dataset with a Versatile\n  Toolchain for Surface Prediction and Completion", "author": "Guiyong Zheng and Jinqi Jiang and Chen Feng and Shaojie Shen and Boyu Zhou", "abstract": "  Surface prediction and completion have been widely studied in various\napplications. Recently, research in surface completion has evolved from small\nobjects to complex large-scale scenes. As a result, researchers have begun\nincreasing the volume of data and leveraging a greater variety of data\nmodalities including rendered RGB images, descriptive texts, depth images, etc,\nto enhance algorithm performance. However, existing datasets suffer from a\ndeficiency in the amounts of scene-level models along with the corresponding\nmulti-modal information. Therefore, a method to scale the datasets and generate\nmulti-modal information in them efficiently is essential. To bridge this\nresearch gap, we propose MASSTAR: a Multi-modal lArge-scale Scene dataset with\na verSatile Toolchain for surfAce pRediction and completion. We develop a\nversatile and efficient toolchain for processing the raw 3D data from the\nenvironments. It screens out a set of fine-grained scene models and generates\nthe corresponding multi-modal data. Utilizing the toolchain, we then generate\nan example dataset composed of over a thousand scene-level models with partial\nreal-world data added. We compare MASSTAR with the existing datasets, which\nvalidates its superiority: the ability to efficiently extract high-quality\nmodels from complex scenarios to expand the dataset. Additionally, several\nrepresentative surface completion algorithms are benchmarked on MASSTAR, which\nreveals that existing algorithms can hardly deal with scene-level completion.\nWe will release the source code of our toolchain and the dataset. For more\ndetails, please see our project page at https://sysu-star.github.io/MASSTAR.\n", "link": "http://arxiv.org/abs/2403.11681v1", "date": "2024-03-18", "relevancy": 1.6883, "topK": [{"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.5715}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.569}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5347}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20MASSTAR%3A%20A%20Multi-Modal%20and%20Large-Scale%20Scene%20Dataset%20with%20a%20Versatile%0A%20%20Toolchain%20for%20Surface%20Prediction%20and%20Completion&body=Title%3A%20MASSTAR%3A%20A%20Multi-Modal%20and%20Large-Scale%20Scene%20Dataset%20with%20a%20Versatile%0A%20%20Toolchain%20for%20Surface%20Prediction%20and%20Completion%0AAuthor%3A%20Guiyong%20Zheng%20and%20Jinqi%20Jiang%20and%20Chen%20Feng%20and%20Shaojie%20Shen%20and%20Boyu%20Zhou%0AAbstract%3A%20%20%20Surface%20prediction%20and%20completion%20have%20been%20widely%20studied%20in%20various%0Aapplications.%20Recently%2C%20research%20in%20surface%20completion%20has%20evolved%20from%20small%0Aobjects%20to%20complex%20large-scale%20scenes.%20As%20a%20result%2C%20researchers%20have%20begun%0Aincreasing%20the%20volume%20of%20data%20and%20leveraging%20a%20greater%20variety%20of%20data%0Amodalities%20including%20rendered%20RGB%20images%2C%20descriptive%20texts%2C%20depth%20images%2C%20etc%2C%0Ato%20enhance%20algorithm%20performance.%20However%2C%20existing%20datasets%20suffer%20from%20a%0Adeficiency%20in%20the%20amounts%20of%20scene-level%20models%20along%20with%20the%20corresponding%0Amulti-modal%20information.%20Therefore%2C%20a%20method%20to%20scale%20the%20datasets%20and%20generate%0Amulti-modal%20information%20in%20them%20efficiently%20is%20essential.%20To%20bridge%20this%0Aresearch%20gap%2C%20we%20propose%20MASSTAR%3A%20a%20Multi-modal%20lArge-scale%20Scene%20dataset%20with%0Aa%20verSatile%20Toolchain%20for%20surfAce%20pRediction%20and%20completion.%20We%20develop%20a%0Aversatile%20and%20efficient%20toolchain%20for%20processing%20the%20raw%203D%20data%20from%20the%0Aenvironments.%20It%20screens%20out%20a%20set%20of%20fine-grained%20scene%20models%20and%20generates%0Athe%20corresponding%20multi-modal%20data.%20Utilizing%20the%20toolchain%2C%20we%20then%20generate%0Aan%20example%20dataset%20composed%20of%20over%20a%20thousand%20scene-level%20models%20with%20partial%0Areal-world%20data%20added.%20We%20compare%20MASSTAR%20with%20the%20existing%20datasets%2C%20which%0Avalidates%20its%20superiority%3A%20the%20ability%20to%20efficiently%20extract%20high-quality%0Amodels%20from%20complex%20scenarios%20to%20expand%20the%20dataset.%20Additionally%2C%20several%0Arepresentative%20surface%20completion%20algorithms%20are%20benchmarked%20on%20MASSTAR%2C%20which%0Areveals%20that%20existing%20algorithms%20can%20hardly%20deal%20with%20scene-level%20completion.%0AWe%20will%20release%20the%20source%20code%20of%20our%20toolchain%20and%20the%20dataset.%20For%20more%0Adetails%2C%20please%20see%20our%20project%20page%20at%20https%3A//sysu-star.github.io/MASSTAR.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11681v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=MASSTAR%3A%20A%20Multi-Modal%20and%20Large-Scale%20Scene%20Dataset%20with%20a%20Versatile%0A%20%20Toolchain%20for%20Surface%20Prediction%20and%20Completion&entry.906535625=Guiyong%20Zheng%20and%20Jinqi%20Jiang%20and%20Chen%20Feng%20and%20Shaojie%20Shen%20and%20Boyu%20Zhou&entry.1292438233=%20%20Surface%20prediction%20and%20completion%20have%20been%20widely%20studied%20in%20various%0Aapplications.%20Recently%2C%20research%20in%20surface%20completion%20has%20evolved%20from%20small%0Aobjects%20to%20complex%20large-scale%20scenes.%20As%20a%20result%2C%20researchers%20have%20begun%0Aincreasing%20the%20volume%20of%20data%20and%20leveraging%20a%20greater%20variety%20of%20data%0Amodalities%20including%20rendered%20RGB%20images%2C%20descriptive%20texts%2C%20depth%20images%2C%20etc%2C%0Ato%20enhance%20algorithm%20performance.%20However%2C%20existing%20datasets%20suffer%20from%20a%0Adeficiency%20in%20the%20amounts%20of%20scene-level%20models%20along%20with%20the%20corresponding%0Amulti-modal%20information.%20Therefore%2C%20a%20method%20to%20scale%20the%20datasets%20and%20generate%0Amulti-modal%20information%20in%20them%20efficiently%20is%20essential.%20To%20bridge%20this%0Aresearch%20gap%2C%20we%20propose%20MASSTAR%3A%20a%20Multi-modal%20lArge-scale%20Scene%20dataset%20with%0Aa%20verSatile%20Toolchain%20for%20surfAce%20pRediction%20and%20completion.%20We%20develop%20a%0Aversatile%20and%20efficient%20toolchain%20for%20processing%20the%20raw%203D%20data%20from%20the%0Aenvironments.%20It%20screens%20out%20a%20set%20of%20fine-grained%20scene%20models%20and%20generates%0Athe%20corresponding%20multi-modal%20data.%20Utilizing%20the%20toolchain%2C%20we%20then%20generate%0Aan%20example%20dataset%20composed%20of%20over%20a%20thousand%20scene-level%20models%20with%20partial%0Areal-world%20data%20added.%20We%20compare%20MASSTAR%20with%20the%20existing%20datasets%2C%20which%0Avalidates%20its%20superiority%3A%20the%20ability%20to%20efficiently%20extract%20high-quality%0Amodels%20from%20complex%20scenarios%20to%20expand%20the%20dataset.%20Additionally%2C%20several%0Arepresentative%20surface%20completion%20algorithms%20are%20benchmarked%20on%20MASSTAR%2C%20which%0Areveals%20that%20existing%20algorithms%20can%20hardly%20deal%20with%20scene-level%20completion.%0AWe%20will%20release%20the%20source%20code%20of%20our%20toolchain%20and%20the%20dataset.%20For%20more%0Adetails%2C%20please%20see%20our%20project%20page%20at%20https%3A//sysu-star.github.io/MASSTAR.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11681v1&entry.124074799=Read"},
{"title": "An Efficient Model-Based Approach on Learning Agile Motor Skills without\n  Reinforcement", "author": "Haojie Shi and Tingguang Li and Qingxu Zhu and Jiapeng Sheng and Lei Han and Max Q. -H. Meng", "abstract": "  Learning-based methods have improved locomotion skills of quadruped robots\nthrough deep reinforcement learning. However, the sim-to-real gap and low\nsample efficiency still limit the skill transfer. To address this issue, we\npropose an efficient model-based learning framework that combines a world model\nwith a policy network. We train a differentiable world model to predict future\nstates and use it to directly supervise a Variational Autoencoder (VAE)-based\npolicy network to imitate real animal behaviors. This significantly reduces the\nneed for real interaction data and allows for rapid policy updates. We also\ndevelop a high-level network to track diverse commands and trajectories. Our\nsimulated results show a tenfold sample efficiency increase compared to\nreinforcement learning methods such as PPO. In real-world testing, our policy\nachieves proficient command-following performance with only a two-minute data\ncollection period and generalizes well to new speeds and paths.\n", "link": "http://arxiv.org/abs/2403.01962v2", "date": "2024-03-18", "relevancy": 1.6862, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.6023}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5526}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.5498}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20An%20Efficient%20Model-Based%20Approach%20on%20Learning%20Agile%20Motor%20Skills%20without%0A%20%20Reinforcement&body=Title%3A%20An%20Efficient%20Model-Based%20Approach%20on%20Learning%20Agile%20Motor%20Skills%20without%0A%20%20Reinforcement%0AAuthor%3A%20Haojie%20Shi%20and%20Tingguang%20Li%20and%20Qingxu%20Zhu%20and%20Jiapeng%20Sheng%20and%20Lei%20Han%20and%20Max%20Q.%20-H.%20Meng%0AAbstract%3A%20%20%20Learning-based%20methods%20have%20improved%20locomotion%20skills%20of%20quadruped%20robots%0Athrough%20deep%20reinforcement%20learning.%20However%2C%20the%20sim-to-real%20gap%20and%20low%0Asample%20efficiency%20still%20limit%20the%20skill%20transfer.%20To%20address%20this%20issue%2C%20we%0Apropose%20an%20efficient%20model-based%20learning%20framework%20that%20combines%20a%20world%20model%0Awith%20a%20policy%20network.%20We%20train%20a%20differentiable%20world%20model%20to%20predict%20future%0Astates%20and%20use%20it%20to%20directly%20supervise%20a%20Variational%20Autoencoder%20%28VAE%29-based%0Apolicy%20network%20to%20imitate%20real%20animal%20behaviors.%20This%20significantly%20reduces%20the%0Aneed%20for%20real%20interaction%20data%20and%20allows%20for%20rapid%20policy%20updates.%20We%20also%0Adevelop%20a%20high-level%20network%20to%20track%20diverse%20commands%20and%20trajectories.%20Our%0Asimulated%20results%20show%20a%20tenfold%20sample%20efficiency%20increase%20compared%20to%0Areinforcement%20learning%20methods%20such%20as%20PPO.%20In%20real-world%20testing%2C%20our%20policy%0Aachieves%20proficient%20command-following%20performance%20with%20only%20a%20two-minute%20data%0Acollection%20period%20and%20generalizes%20well%20to%20new%20speeds%20and%20paths.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.01962v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=An%20Efficient%20Model-Based%20Approach%20on%20Learning%20Agile%20Motor%20Skills%20without%0A%20%20Reinforcement&entry.906535625=Haojie%20Shi%20and%20Tingguang%20Li%20and%20Qingxu%20Zhu%20and%20Jiapeng%20Sheng%20and%20Lei%20Han%20and%20Max%20Q.%20-H.%20Meng&entry.1292438233=%20%20Learning-based%20methods%20have%20improved%20locomotion%20skills%20of%20quadruped%20robots%0Athrough%20deep%20reinforcement%20learning.%20However%2C%20the%20sim-to-real%20gap%20and%20low%0Asample%20efficiency%20still%20limit%20the%20skill%20transfer.%20To%20address%20this%20issue%2C%20we%0Apropose%20an%20efficient%20model-based%20learning%20framework%20that%20combines%20a%20world%20model%0Awith%20a%20policy%20network.%20We%20train%20a%20differentiable%20world%20model%20to%20predict%20future%0Astates%20and%20use%20it%20to%20directly%20supervise%20a%20Variational%20Autoencoder%20%28VAE%29-based%0Apolicy%20network%20to%20imitate%20real%20animal%20behaviors.%20This%20significantly%20reduces%20the%0Aneed%20for%20real%20interaction%20data%20and%20allows%20for%20rapid%20policy%20updates.%20We%20also%0Adevelop%20a%20high-level%20network%20to%20track%20diverse%20commands%20and%20trajectories.%20Our%0Asimulated%20results%20show%20a%20tenfold%20sample%20efficiency%20increase%20compared%20to%0Areinforcement%20learning%20methods%20such%20as%20PPO.%20In%20real-world%20testing%2C%20our%20policy%0Aachieves%20proficient%20command-following%20performance%20with%20only%20a%20two-minute%20data%0Acollection%20period%20and%20generalizes%20well%20to%20new%20speeds%20and%20paths.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.01962v2&entry.124074799=Read"},
{"title": "Generalization error of spectral algorithms", "author": "Maksim Velikanov and Maxim Panov and Dmitry Yarotsky", "abstract": "  The asymptotically precise estimation of the generalization of kernel methods\nhas recently received attention due to the parallels between neural networks\nand their associated kernels. However, prior works derive such estimates for\ntraining by kernel ridge regression (KRR), whereas neural networks are\ntypically trained with gradient descent (GD). In the present work, we consider\nthe training of kernels with a family of $\\textit{spectral algorithms}$\nspecified by profile $h(\\lambda)$, and including KRR and GD as special cases.\nThen, we derive the generalization error as a functional of learning profile\n$h(\\lambda)$ for two data models: high-dimensional Gaussian and low-dimensional\ntranslation-invariant model. Under power-law assumptions on the spectrum of the\nkernel and target, we use our framework to (i) give full loss asymptotics for\nboth noisy and noiseless observations (ii) show that the loss localizes on\ncertain spectral scales, giving a new perspective on the KRR saturation\nphenomenon (iii) conjecture, and demonstrate for the considered data models,\nthe universality of the loss w.r.t. non-spectral details of the problem, but\nonly in case of noisy observation.\n", "link": "http://arxiv.org/abs/2403.11696v1", "date": "2024-03-18", "relevancy": 1.6796, "topK": [{"title": "Breadcrumbs: Adversarial class-balanced sampling for long-tailed recognition", "link": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_37", "similarity": 0.423}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4201}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4167}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generalization%20error%20of%20spectral%20algorithms&body=Title%3A%20Generalization%20error%20of%20spectral%20algorithms%0AAuthor%3A%20Maksim%20Velikanov%20and%20Maxim%20Panov%20and%20Dmitry%20Yarotsky%0AAbstract%3A%20%20%20The%20asymptotically%20precise%20estimation%20of%20the%20generalization%20of%20kernel%20methods%0Ahas%20recently%20received%20attention%20due%20to%20the%20parallels%20between%20neural%20networks%0Aand%20their%20associated%20kernels.%20However%2C%20prior%20works%20derive%20such%20estimates%20for%0Atraining%20by%20kernel%20ridge%20regression%20%28KRR%29%2C%20whereas%20neural%20networks%20are%0Atypically%20trained%20with%20gradient%20descent%20%28GD%29.%20In%20the%20present%20work%2C%20we%20consider%0Athe%20training%20of%20kernels%20with%20a%20family%20of%20%24%5Ctextit%7Bspectral%20algorithms%7D%24%0Aspecified%20by%20profile%20%24h%28%5Clambda%29%24%2C%20and%20including%20KRR%20and%20GD%20as%20special%20cases.%0AThen%2C%20we%20derive%20the%20generalization%20error%20as%20a%20functional%20of%20learning%20profile%0A%24h%28%5Clambda%29%24%20for%20two%20data%20models%3A%20high-dimensional%20Gaussian%20and%20low-dimensional%0Atranslation-invariant%20model.%20Under%20power-law%20assumptions%20on%20the%20spectrum%20of%20the%0Akernel%20and%20target%2C%20we%20use%20our%20framework%20to%20%28i%29%20give%20full%20loss%20asymptotics%20for%0Aboth%20noisy%20and%20noiseless%20observations%20%28ii%29%20show%20that%20the%20loss%20localizes%20on%0Acertain%20spectral%20scales%2C%20giving%20a%20new%20perspective%20on%20the%20KRR%20saturation%0Aphenomenon%20%28iii%29%20conjecture%2C%20and%20demonstrate%20for%20the%20considered%20data%20models%2C%0Athe%20universality%20of%20the%20loss%20w.r.t.%20non-spectral%20details%20of%20the%20problem%2C%20but%0Aonly%20in%20case%20of%20noisy%20observation.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11696v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalization%20error%20of%20spectral%20algorithms&entry.906535625=Maksim%20Velikanov%20and%20Maxim%20Panov%20and%20Dmitry%20Yarotsky&entry.1292438233=%20%20The%20asymptotically%20precise%20estimation%20of%20the%20generalization%20of%20kernel%20methods%0Ahas%20recently%20received%20attention%20due%20to%20the%20parallels%20between%20neural%20networks%0Aand%20their%20associated%20kernels.%20However%2C%20prior%20works%20derive%20such%20estimates%20for%0Atraining%20by%20kernel%20ridge%20regression%20%28KRR%29%2C%20whereas%20neural%20networks%20are%0Atypically%20trained%20with%20gradient%20descent%20%28GD%29.%20In%20the%20present%20work%2C%20we%20consider%0Athe%20training%20of%20kernels%20with%20a%20family%20of%20%24%5Ctextit%7Bspectral%20algorithms%7D%24%0Aspecified%20by%20profile%20%24h%28%5Clambda%29%24%2C%20and%20including%20KRR%20and%20GD%20as%20special%20cases.%0AThen%2C%20we%20derive%20the%20generalization%20error%20as%20a%20functional%20of%20learning%20profile%0A%24h%28%5Clambda%29%24%20for%20two%20data%20models%3A%20high-dimensional%20Gaussian%20and%20low-dimensional%0Atranslation-invariant%20model.%20Under%20power-law%20assumptions%20on%20the%20spectrum%20of%20the%0Akernel%20and%20target%2C%20we%20use%20our%20framework%20to%20%28i%29%20give%20full%20loss%20asymptotics%20for%0Aboth%20noisy%20and%20noiseless%20observations%20%28ii%29%20show%20that%20the%20loss%20localizes%20on%0Acertain%20spectral%20scales%2C%20giving%20a%20new%20perspective%20on%20the%20KRR%20saturation%0Aphenomenon%20%28iii%29%20conjecture%2C%20and%20demonstrate%20for%20the%20considered%20data%20models%2C%0Athe%20universality%20of%20the%20loss%20w.r.t.%20non-spectral%20details%20of%20the%20problem%2C%20but%0Aonly%20in%20case%20of%20noisy%20observation.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11696v1&entry.124074799=Read"},
{"title": "HDLdebugger: Streamlining HDL debugging with Large Language Models", "author": "Xufeng Yao and Haoyang Li and Tsz Ho Chan and Wenyi Xiao and Mingxuan Yuan and Yu Huang and Lei Chen and Bei Yu", "abstract": "  In the domain of chip design, Hardware Description Languages (HDLs) play a\npivotal role. However, due to the complex syntax of HDLs and the limited\navailability of online resources, debugging HDL codes remains a difficult and\ntime-intensive task, even for seasoned engineers. Consequently, there is a\npressing need to develop automated HDL code debugging models, which can\nalleviate the burden on hardware engineers. Despite the strong capabilities of\nLarge Language Models (LLMs) in generating, completing, and debugging software\ncode, their utilization in the specialized field of HDL debugging has been\nlimited and, to date, has not yielded satisfactory results. In this paper, we\npropose an LLM-assisted HDL debugging framework, namely HDLdebugger, which\nconsists of HDL debugging data generation via a reverse engineering approach, a\nsearch engine for retrieval-augmented generation, and a retrieval-augmented LLM\nfine-tuning approach. Through the integration of these components, HDLdebugger\ncan automate and streamline HDL debugging for chip design. Our comprehensive\nexperiments, conducted on an HDL code dataset sourced from Huawei, reveal that\nHDLdebugger outperforms 13 cutting-edge LLM baselines, displaying exceptional\neffectiveness in HDL code debugging.\n", "link": "http://arxiv.org/abs/2403.11671v1", "date": "2024-03-18", "relevancy": 1.6493, "topK": [{"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4139}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.4129}, {"title": "Self-supervised Photographic Image Layout Representation Learning", "link": "http://arxiv.org/abs/2403.03740v1", "similarity": 0.4111}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20HDLdebugger%3A%20Streamlining%20HDL%20debugging%20with%20Large%20Language%20Models&body=Title%3A%20HDLdebugger%3A%20Streamlining%20HDL%20debugging%20with%20Large%20Language%20Models%0AAuthor%3A%20Xufeng%20Yao%20and%20Haoyang%20Li%20and%20Tsz%20Ho%20Chan%20and%20Wenyi%20Xiao%20and%20Mingxuan%20Yuan%20and%20Yu%20Huang%20and%20Lei%20Chen%20and%20Bei%20Yu%0AAbstract%3A%20%20%20In%20the%20domain%20of%20chip%20design%2C%20Hardware%20Description%20Languages%20%28HDLs%29%20play%20a%0Apivotal%20role.%20However%2C%20due%20to%20the%20complex%20syntax%20of%20HDLs%20and%20the%20limited%0Aavailability%20of%20online%20resources%2C%20debugging%20HDL%20codes%20remains%20a%20difficult%20and%0Atime-intensive%20task%2C%20even%20for%20seasoned%20engineers.%20Consequently%2C%20there%20is%20a%0Apressing%20need%20to%20develop%20automated%20HDL%20code%20debugging%20models%2C%20which%20can%0Aalleviate%20the%20burden%20on%20hardware%20engineers.%20Despite%20the%20strong%20capabilities%20of%0ALarge%20Language%20Models%20%28LLMs%29%20in%20generating%2C%20completing%2C%20and%20debugging%20software%0Acode%2C%20their%20utilization%20in%20the%20specialized%20field%20of%20HDL%20debugging%20has%20been%0Alimited%20and%2C%20to%20date%2C%20has%20not%20yielded%20satisfactory%20results.%20In%20this%20paper%2C%20we%0Apropose%20an%20LLM-assisted%20HDL%20debugging%20framework%2C%20namely%20HDLdebugger%2C%20which%0Aconsists%20of%20HDL%20debugging%20data%20generation%20via%20a%20reverse%20engineering%20approach%2C%20a%0Asearch%20engine%20for%20retrieval-augmented%20generation%2C%20and%20a%20retrieval-augmented%20LLM%0Afine-tuning%20approach.%20Through%20the%20integration%20of%20these%20components%2C%20HDLdebugger%0Acan%20automate%20and%20streamline%20HDL%20debugging%20for%20chip%20design.%20Our%20comprehensive%0Aexperiments%2C%20conducted%20on%20an%20HDL%20code%20dataset%20sourced%20from%20Huawei%2C%20reveal%20that%0AHDLdebugger%20outperforms%2013%20cutting-edge%20LLM%20baselines%2C%20displaying%20exceptional%0Aeffectiveness%20in%20HDL%20code%20debugging.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11671v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=HDLdebugger%3A%20Streamlining%20HDL%20debugging%20with%20Large%20Language%20Models&entry.906535625=Xufeng%20Yao%20and%20Haoyang%20Li%20and%20Tsz%20Ho%20Chan%20and%20Wenyi%20Xiao%20and%20Mingxuan%20Yuan%20and%20Yu%20Huang%20and%20Lei%20Chen%20and%20Bei%20Yu&entry.1292438233=%20%20In%20the%20domain%20of%20chip%20design%2C%20Hardware%20Description%20Languages%20%28HDLs%29%20play%20a%0Apivotal%20role.%20However%2C%20due%20to%20the%20complex%20syntax%20of%20HDLs%20and%20the%20limited%0Aavailability%20of%20online%20resources%2C%20debugging%20HDL%20codes%20remains%20a%20difficult%20and%0Atime-intensive%20task%2C%20even%20for%20seasoned%20engineers.%20Consequently%2C%20there%20is%20a%0Apressing%20need%20to%20develop%20automated%20HDL%20code%20debugging%20models%2C%20which%20can%0Aalleviate%20the%20burden%20on%20hardware%20engineers.%20Despite%20the%20strong%20capabilities%20of%0ALarge%20Language%20Models%20%28LLMs%29%20in%20generating%2C%20completing%2C%20and%20debugging%20software%0Acode%2C%20their%20utilization%20in%20the%20specialized%20field%20of%20HDL%20debugging%20has%20been%0Alimited%20and%2C%20to%20date%2C%20has%20not%20yielded%20satisfactory%20results.%20In%20this%20paper%2C%20we%0Apropose%20an%20LLM-assisted%20HDL%20debugging%20framework%2C%20namely%20HDLdebugger%2C%20which%0Aconsists%20of%20HDL%20debugging%20data%20generation%20via%20a%20reverse%20engineering%20approach%2C%20a%0Asearch%20engine%20for%20retrieval-augmented%20generation%2C%20and%20a%20retrieval-augmented%20LLM%0Afine-tuning%20approach.%20Through%20the%20integration%20of%20these%20components%2C%20HDLdebugger%0Acan%20automate%20and%20streamline%20HDL%20debugging%20for%20chip%20design.%20Our%20comprehensive%0Aexperiments%2C%20conducted%20on%20an%20HDL%20code%20dataset%20sourced%20from%20Huawei%2C%20reveal%20that%0AHDLdebugger%20outperforms%2013%20cutting-edge%20LLM%20baselines%2C%20displaying%20exceptional%0Aeffectiveness%20in%20HDL%20code%20debugging.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11671v1&entry.124074799=Read"},
{"title": "Coarsening of chiral domains in itinerant electron magnets: A machine\n  learning force field approach", "author": "Yunhao Fan and Sheng Zhang and Gia-Wei Chern", "abstract": "  Frustrated itinerant magnets often exhibit complex noncollinear or\nnoncoplanar magnetic orders which support topological electronic structures. A\ncanonical example is the anomalous quantum Hall state with a chiral spin order\nstabilized by electron-spin interactions on a triangular lattice. While a\nlong-range magnetic order cannot survive thermal fluctuations in two\ndimensions, the chiral order which results from the breaking of a discrete\nIsing symmetry persists even at finite temperatures. We present a scalable\nmachine learning (ML) framework to model the complex electron-mediated\nspin-spin interactions that stabilize the chiral magnetic domains in a\ntriangular lattice. Large-scale dynamical simulations, enabled by the ML\nforce-field models, are performed to investigate the coarsening of chiral\ndomains after a thermal quench. While the chiral phase is described by a broken\n$Z_2$ Ising-type symmetry, we find that the characteristic size of chiral\ndomains increases linearly with time, in stark contrast to the expected\nAllen-Cahn domain growth law for a non-conserved Ising order parameter field.\nThe linear growth of the chiral domains is attributed to the orientational\nanisotropy of domain boundaries. Our work also demonstrates the promising\npotential of ML models for large-scale spin dynamics of itinerant magnets.\n", "link": "http://arxiv.org/abs/2403.11705v1", "date": "2024-03-18", "relevancy": 1.6258, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4123}, {"title": "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", "link": "http://arxiv.org/abs/2402.18490v1", "similarity": 0.412}, {"title": "DDM-NET: End-to-end learning of keypoint feature Detection, Description and Matching for 3D localization", "link": "https://arxiv.org/abs/2212.04575", "similarity": 0.3986}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Coarsening%20of%20chiral%20domains%20in%20itinerant%20electron%20magnets%3A%20A%20machine%0A%20%20learning%20force%20field%20approach&body=Title%3A%20Coarsening%20of%20chiral%20domains%20in%20itinerant%20electron%20magnets%3A%20A%20machine%0A%20%20learning%20force%20field%20approach%0AAuthor%3A%20Yunhao%20Fan%20and%20Sheng%20Zhang%20and%20Gia-Wei%20Chern%0AAbstract%3A%20%20%20Frustrated%20itinerant%20magnets%20often%20exhibit%20complex%20noncollinear%20or%0Anoncoplanar%20magnetic%20orders%20which%20support%20topological%20electronic%20structures.%20A%0Acanonical%20example%20is%20the%20anomalous%20quantum%20Hall%20state%20with%20a%20chiral%20spin%20order%0Astabilized%20by%20electron-spin%20interactions%20on%20a%20triangular%20lattice.%20While%20a%0Along-range%20magnetic%20order%20cannot%20survive%20thermal%20fluctuations%20in%20two%0Adimensions%2C%20the%20chiral%20order%20which%20results%20from%20the%20breaking%20of%20a%20discrete%0AIsing%20symmetry%20persists%20even%20at%20finite%20temperatures.%20We%20present%20a%20scalable%0Amachine%20learning%20%28ML%29%20framework%20to%20model%20the%20complex%20electron-mediated%0Aspin-spin%20interactions%20that%20stabilize%20the%20chiral%20magnetic%20domains%20in%20a%0Atriangular%20lattice.%20Large-scale%20dynamical%20simulations%2C%20enabled%20by%20the%20ML%0Aforce-field%20models%2C%20are%20performed%20to%20investigate%20the%20coarsening%20of%20chiral%0Adomains%20after%20a%20thermal%20quench.%20While%20the%20chiral%20phase%20is%20described%20by%20a%20broken%0A%24Z_2%24%20Ising-type%20symmetry%2C%20we%20find%20that%20the%20characteristic%20size%20of%20chiral%0Adomains%20increases%20linearly%20with%20time%2C%20in%20stark%20contrast%20to%20the%20expected%0AAllen-Cahn%20domain%20growth%20law%20for%20a%20non-conserved%20Ising%20order%20parameter%20field.%0AThe%20linear%20growth%20of%20the%20chiral%20domains%20is%20attributed%20to%20the%20orientational%0Aanisotropy%20of%20domain%20boundaries.%20Our%20work%20also%20demonstrates%20the%20promising%0Apotential%20of%20ML%20models%20for%20large-scale%20spin%20dynamics%20of%20itinerant%20magnets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11705v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Coarsening%20of%20chiral%20domains%20in%20itinerant%20electron%20magnets%3A%20A%20machine%0A%20%20learning%20force%20field%20approach&entry.906535625=Yunhao%20Fan%20and%20Sheng%20Zhang%20and%20Gia-Wei%20Chern&entry.1292438233=%20%20Frustrated%20itinerant%20magnets%20often%20exhibit%20complex%20noncollinear%20or%0Anoncoplanar%20magnetic%20orders%20which%20support%20topological%20electronic%20structures.%20A%0Acanonical%20example%20is%20the%20anomalous%20quantum%20Hall%20state%20with%20a%20chiral%20spin%20order%0Astabilized%20by%20electron-spin%20interactions%20on%20a%20triangular%20lattice.%20While%20a%0Along-range%20magnetic%20order%20cannot%20survive%20thermal%20fluctuations%20in%20two%0Adimensions%2C%20the%20chiral%20order%20which%20results%20from%20the%20breaking%20of%20a%20discrete%0AIsing%20symmetry%20persists%20even%20at%20finite%20temperatures.%20We%20present%20a%20scalable%0Amachine%20learning%20%28ML%29%20framework%20to%20model%20the%20complex%20electron-mediated%0Aspin-spin%20interactions%20that%20stabilize%20the%20chiral%20magnetic%20domains%20in%20a%0Atriangular%20lattice.%20Large-scale%20dynamical%20simulations%2C%20enabled%20by%20the%20ML%0Aforce-field%20models%2C%20are%20performed%20to%20investigate%20the%20coarsening%20of%20chiral%0Adomains%20after%20a%20thermal%20quench.%20While%20the%20chiral%20phase%20is%20described%20by%20a%20broken%0A%24Z_2%24%20Ising-type%20symmetry%2C%20we%20find%20that%20the%20characteristic%20size%20of%20chiral%0Adomains%20increases%20linearly%20with%20time%2C%20in%20stark%20contrast%20to%20the%20expected%0AAllen-Cahn%20domain%20growth%20law%20for%20a%20non-conserved%20Ising%20order%20parameter%20field.%0AThe%20linear%20growth%20of%20the%20chiral%20domains%20is%20attributed%20to%20the%20orientational%0Aanisotropy%20of%20domain%20boundaries.%20Our%20work%20also%20demonstrates%20the%20promising%0Apotential%20of%20ML%20models%20for%20large-scale%20spin%20dynamics%20of%20itinerant%20magnets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11705v1&entry.124074799=Read"},
{"title": "Object Segmentation-Assisted Inter Prediction for Versatile Video Coding", "author": "Zhuoyuan Li and Zikun Yuan and Li Li and Dong Liu and Xiaohu Tang and Feng Wu", "abstract": "  In modern video coding standards, block-based inter prediction is widely\nadopted, which brings high compression efficiency. However, in natural videos,\nthere are usually multiple moving objects of arbitrary shapes, resulting in\ncomplex motion fields that are difficult to compactly represent. This problem\nhas been tackled by more flexible block partitioning methods in the Versatile\nVideo Coding (VVC) standard, but the more flexible partitions require more\noverhead bits to signal and still cannot be made arbitrary shaped. To address\nthis limitation, we propose an object segmentation-assisted inter prediction\nmethod (SAIP), where objects in the reference frames are segmented by some\nadvanced technologies. With a proper indication, the object segmentation mask\nis translated from the reference frame to the current frame as the\narbitrary-shaped partition of different regions without any extra signal. Using\nthe segmentation mask, motion compensation is separately performed for\ndifferent regions, achieving higher prediction accuracy. The segmentation mask\nis further used to code the motion vectors of different regions more\nefficiently. Moreover, segmentation mask is considered in the joint\nrate-distortion optimization for motion estimation and partition estimation to\nderive the motion vector of different regions and partition more accurately.\nThe proposed method is implemented into the VVC reference software, VTM version\n12.0. Experimental results show that the proposed method achieves up to 1.98%,\n1.14%, 0.79%, and on average 0.82%, 0.49%, 0.37% BD-rate reduction for common\ntest sequences, under the Low-delay P, Low-delay B, and Random Access\nconfigurations, respectively.\n", "link": "http://arxiv.org/abs/2403.11694v1", "date": "2024-03-18", "relevancy": 1.6176, "topK": [{"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.5496}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5275}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.525}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Object%20Segmentation-Assisted%20Inter%20Prediction%20for%20Versatile%20Video%20Coding&body=Title%3A%20Object%20Segmentation-Assisted%20Inter%20Prediction%20for%20Versatile%20Video%20Coding%0AAuthor%3A%20Zhuoyuan%20Li%20and%20Zikun%20Yuan%20and%20Li%20Li%20and%20Dong%20Liu%20and%20Xiaohu%20Tang%20and%20Feng%20Wu%0AAbstract%3A%20%20%20In%20modern%20video%20coding%20standards%2C%20block-based%20inter%20prediction%20is%20widely%0Aadopted%2C%20which%20brings%20high%20compression%20efficiency.%20However%2C%20in%20natural%20videos%2C%0Athere%20are%20usually%20multiple%20moving%20objects%20of%20arbitrary%20shapes%2C%20resulting%20in%0Acomplex%20motion%20fields%20that%20are%20difficult%20to%20compactly%20represent.%20This%20problem%0Ahas%20been%20tackled%20by%20more%20flexible%20block%20partitioning%20methods%20in%20the%20Versatile%0AVideo%20Coding%20%28VVC%29%20standard%2C%20but%20the%20more%20flexible%20partitions%20require%20more%0Aoverhead%20bits%20to%20signal%20and%20still%20cannot%20be%20made%20arbitrary%20shaped.%20To%20address%0Athis%20limitation%2C%20we%20propose%20an%20object%20segmentation-assisted%20inter%20prediction%0Amethod%20%28SAIP%29%2C%20where%20objects%20in%20the%20reference%20frames%20are%20segmented%20by%20some%0Aadvanced%20technologies.%20With%20a%20proper%20indication%2C%20the%20object%20segmentation%20mask%0Ais%20translated%20from%20the%20reference%20frame%20to%20the%20current%20frame%20as%20the%0Aarbitrary-shaped%20partition%20of%20different%20regions%20without%20any%20extra%20signal.%20Using%0Athe%20segmentation%20mask%2C%20motion%20compensation%20is%20separately%20performed%20for%0Adifferent%20regions%2C%20achieving%20higher%20prediction%20accuracy.%20The%20segmentation%20mask%0Ais%20further%20used%20to%20code%20the%20motion%20vectors%20of%20different%20regions%20more%0Aefficiently.%20Moreover%2C%20segmentation%20mask%20is%20considered%20in%20the%20joint%0Arate-distortion%20optimization%20for%20motion%20estimation%20and%20partition%20estimation%20to%0Aderive%20the%20motion%20vector%20of%20different%20regions%20and%20partition%20more%20accurately.%0AThe%20proposed%20method%20is%20implemented%20into%20the%20VVC%20reference%20software%2C%20VTM%20version%0A12.0.%20Experimental%20results%20show%20that%20the%20proposed%20method%20achieves%20up%20to%201.98%25%2C%0A1.14%25%2C%200.79%25%2C%20and%20on%20average%200.82%25%2C%200.49%25%2C%200.37%25%20BD-rate%20reduction%20for%20common%0Atest%20sequences%2C%20under%20the%20Low-delay%20P%2C%20Low-delay%20B%2C%20and%20Random%20Access%0Aconfigurations%2C%20respectively.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11694v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Object%20Segmentation-Assisted%20Inter%20Prediction%20for%20Versatile%20Video%20Coding&entry.906535625=Zhuoyuan%20Li%20and%20Zikun%20Yuan%20and%20Li%20Li%20and%20Dong%20Liu%20and%20Xiaohu%20Tang%20and%20Feng%20Wu&entry.1292438233=%20%20In%20modern%20video%20coding%20standards%2C%20block-based%20inter%20prediction%20is%20widely%0Aadopted%2C%20which%20brings%20high%20compression%20efficiency.%20However%2C%20in%20natural%20videos%2C%0Athere%20are%20usually%20multiple%20moving%20objects%20of%20arbitrary%20shapes%2C%20resulting%20in%0Acomplex%20motion%20fields%20that%20are%20difficult%20to%20compactly%20represent.%20This%20problem%0Ahas%20been%20tackled%20by%20more%20flexible%20block%20partitioning%20methods%20in%20the%20Versatile%0AVideo%20Coding%20%28VVC%29%20standard%2C%20but%20the%20more%20flexible%20partitions%20require%20more%0Aoverhead%20bits%20to%20signal%20and%20still%20cannot%20be%20made%20arbitrary%20shaped.%20To%20address%0Athis%20limitation%2C%20we%20propose%20an%20object%20segmentation-assisted%20inter%20prediction%0Amethod%20%28SAIP%29%2C%20where%20objects%20in%20the%20reference%20frames%20are%20segmented%20by%20some%0Aadvanced%20technologies.%20With%20a%20proper%20indication%2C%20the%20object%20segmentation%20mask%0Ais%20translated%20from%20the%20reference%20frame%20to%20the%20current%20frame%20as%20the%0Aarbitrary-shaped%20partition%20of%20different%20regions%20without%20any%20extra%20signal.%20Using%0Athe%20segmentation%20mask%2C%20motion%20compensation%20is%20separately%20performed%20for%0Adifferent%20regions%2C%20achieving%20higher%20prediction%20accuracy.%20The%20segmentation%20mask%0Ais%20further%20used%20to%20code%20the%20motion%20vectors%20of%20different%20regions%20more%0Aefficiently.%20Moreover%2C%20segmentation%20mask%20is%20considered%20in%20the%20joint%0Arate-distortion%20optimization%20for%20motion%20estimation%20and%20partition%20estimation%20to%0Aderive%20the%20motion%20vector%20of%20different%20regions%20and%20partition%20more%20accurately.%0AThe%20proposed%20method%20is%20implemented%20into%20the%20VVC%20reference%20software%2C%20VTM%20version%0A12.0.%20Experimental%20results%20show%20that%20the%20proposed%20method%20achieves%20up%20to%201.98%25%2C%0A1.14%25%2C%200.79%25%2C%20and%20on%20average%200.82%25%2C%200.49%25%2C%200.37%25%20BD-rate%20reduction%20for%20common%0Atest%20sequences%2C%20under%20the%20Low-delay%20P%2C%20Low-delay%20B%2C%20and%20Random%20Access%0Aconfigurations%2C%20respectively.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11694v1&entry.124074799=Read"},
{"title": "Urban Scene Diffusion through Semantic Occupancy Map", "author": "Junge Zhang and Qihang Zhang and Li Zhang and Ramana Rao Kompella and Gaowen Liu and Bolei Zhou", "abstract": "  Generating unbounded 3D scenes is crucial for large-scale scene understanding\nand simulation. Urban scenes, unlike natural landscapes, consist of various\ncomplex man-made objects and structures such as roads, traffic signs, vehicles,\nand buildings. To create a realistic and detailed urban scene, it is crucial to\naccurately represent the geometry and semantics of the underlying objects,\ngoing beyond their visual appearance. In this work, we propose UrbanDiffusion,\na 3D diffusion model that is conditioned on a Bird's-Eye View (BEV) map and\ngenerates an urban scene with geometry and semantics in the form of semantic\noccupancy map. Our model introduces a novel paradigm that learns the data\ndistribution of scene-level structures within a latent space and further\nenables the expansion of the synthesized scene into an arbitrary scale. After\ntraining on real-world driving datasets, our model can generate a wide range of\ndiverse urban scenes given the BEV maps from the held-out set and also\ngeneralize to the synthesized maps from a driving simulator. We further\ndemonstrate its application to scene image synthesis with a pretrained image\ngenerator as a prior.\n", "link": "http://arxiv.org/abs/2403.11697v1", "date": "2024-03-18", "relevancy": 1.6081, "topK": [{"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.54}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5349}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.5273}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Urban%20Scene%20Diffusion%20through%20Semantic%20Occupancy%20Map&body=Title%3A%20Urban%20Scene%20Diffusion%20through%20Semantic%20Occupancy%20Map%0AAuthor%3A%20Junge%20Zhang%20and%20Qihang%20Zhang%20and%20Li%20Zhang%20and%20Ramana%20Rao%20Kompella%20and%20Gaowen%20Liu%20and%20Bolei%20Zhou%0AAbstract%3A%20%20%20Generating%20unbounded%203D%20scenes%20is%20crucial%20for%20large-scale%20scene%20understanding%0Aand%20simulation.%20Urban%20scenes%2C%20unlike%20natural%20landscapes%2C%20consist%20of%20various%0Acomplex%20man-made%20objects%20and%20structures%20such%20as%20roads%2C%20traffic%20signs%2C%20vehicles%2C%0Aand%20buildings.%20To%20create%20a%20realistic%20and%20detailed%20urban%20scene%2C%20it%20is%20crucial%20to%0Aaccurately%20represent%20the%20geometry%20and%20semantics%20of%20the%20underlying%20objects%2C%0Agoing%20beyond%20their%20visual%20appearance.%20In%20this%20work%2C%20we%20propose%20UrbanDiffusion%2C%0Aa%203D%20diffusion%20model%20that%20is%20conditioned%20on%20a%20Bird%27s-Eye%20View%20%28BEV%29%20map%20and%0Agenerates%20an%20urban%20scene%20with%20geometry%20and%20semantics%20in%20the%20form%20of%20semantic%0Aoccupancy%20map.%20Our%20model%20introduces%20a%20novel%20paradigm%20that%20learns%20the%20data%0Adistribution%20of%20scene-level%20structures%20within%20a%20latent%20space%20and%20further%0Aenables%20the%20expansion%20of%20the%20synthesized%20scene%20into%20an%20arbitrary%20scale.%20After%0Atraining%20on%20real-world%20driving%20datasets%2C%20our%20model%20can%20generate%20a%20wide%20range%20of%0Adiverse%20urban%20scenes%20given%20the%20BEV%20maps%20from%20the%20held-out%20set%20and%20also%0Ageneralize%20to%20the%20synthesized%20maps%20from%20a%20driving%20simulator.%20We%20further%0Ademonstrate%20its%20application%20to%20scene%20image%20synthesis%20with%20a%20pretrained%20image%0Agenerator%20as%20a%20prior.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11697v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Urban%20Scene%20Diffusion%20through%20Semantic%20Occupancy%20Map&entry.906535625=Junge%20Zhang%20and%20Qihang%20Zhang%20and%20Li%20Zhang%20and%20Ramana%20Rao%20Kompella%20and%20Gaowen%20Liu%20and%20Bolei%20Zhou&entry.1292438233=%20%20Generating%20unbounded%203D%20scenes%20is%20crucial%20for%20large-scale%20scene%20understanding%0Aand%20simulation.%20Urban%20scenes%2C%20unlike%20natural%20landscapes%2C%20consist%20of%20various%0Acomplex%20man-made%20objects%20and%20structures%20such%20as%20roads%2C%20traffic%20signs%2C%20vehicles%2C%0Aand%20buildings.%20To%20create%20a%20realistic%20and%20detailed%20urban%20scene%2C%20it%20is%20crucial%20to%0Aaccurately%20represent%20the%20geometry%20and%20semantics%20of%20the%20underlying%20objects%2C%0Agoing%20beyond%20their%20visual%20appearance.%20In%20this%20work%2C%20we%20propose%20UrbanDiffusion%2C%0Aa%203D%20diffusion%20model%20that%20is%20conditioned%20on%20a%20Bird%27s-Eye%20View%20%28BEV%29%20map%20and%0Agenerates%20an%20urban%20scene%20with%20geometry%20and%20semantics%20in%20the%20form%20of%20semantic%0Aoccupancy%20map.%20Our%20model%20introduces%20a%20novel%20paradigm%20that%20learns%20the%20data%0Adistribution%20of%20scene-level%20structures%20within%20a%20latent%20space%20and%20further%0Aenables%20the%20expansion%20of%20the%20synthesized%20scene%20into%20an%20arbitrary%20scale.%20After%0Atraining%20on%20real-world%20driving%20datasets%2C%20our%20model%20can%20generate%20a%20wide%20range%20of%0Adiverse%20urban%20scenes%20given%20the%20BEV%20maps%20from%20the%20held-out%20set%20and%20also%0Ageneralize%20to%20the%20synthesized%20maps%20from%20a%20driving%20simulator.%20We%20further%0Ademonstrate%20its%20application%20to%20scene%20image%20synthesis%20with%20a%20pretrained%20image%0Agenerator%20as%20a%20prior.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11697v1&entry.124074799=Read"},
{"title": "Frontier-Based Exploration for Multi-Robot Rendezvous in\n  Communication-Restricted Unknown Environments", "author": "Mauro Tellaroli and Matteo Luperto and Michele Antonazzi and Nicola Basilico", "abstract": "  Multi-robot rendezvous and exploration are fundamental challenges in the\ndomain of mobile robotic systems. This paper addresses multi-robot rendezvous\nwithin an initially unknown environment where communication is only possible\nafter the rendezvous. Traditionally, exploration has been focused on rapidly\nmapping the environment, often leading to suboptimal rendezvous performance in\nlater stages. We adapt a standard frontier-based exploration technique to\nintegrate exploration and rendezvous into a unified strategy, with a mechanism\nthat allows robots to re-visit previously explored regions thus enhancing\nrendezvous opportunities. We validate our approach in 3D realistic simulations\nusing ROS, showcasing its effectiveness in achieving faster rendezvous times\ncompared to exploration strategies.\n", "link": "http://arxiv.org/abs/2403.11617v1", "date": "2024-03-18", "relevancy": 1.605, "topK": [{"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.63}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5338}, {"title": "Egocentric Computer Vision for Hands-Free Robotic Wheelchair Navigation", "link": "https://link.springer.com/article/10.1007/s10846-023-01807-4", "similarity": 0.4975}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Frontier-Based%20Exploration%20for%20Multi-Robot%20Rendezvous%20in%0A%20%20Communication-Restricted%20Unknown%20Environments&body=Title%3A%20Frontier-Based%20Exploration%20for%20Multi-Robot%20Rendezvous%20in%0A%20%20Communication-Restricted%20Unknown%20Environments%0AAuthor%3A%20Mauro%20Tellaroli%20and%20Matteo%20Luperto%20and%20Michele%20Antonazzi%20and%20Nicola%20Basilico%0AAbstract%3A%20%20%20Multi-robot%20rendezvous%20and%20exploration%20are%20fundamental%20challenges%20in%20the%0Adomain%20of%20mobile%20robotic%20systems.%20This%20paper%20addresses%20multi-robot%20rendezvous%0Awithin%20an%20initially%20unknown%20environment%20where%20communication%20is%20only%20possible%0Aafter%20the%20rendezvous.%20Traditionally%2C%20exploration%20has%20been%20focused%20on%20rapidly%0Amapping%20the%20environment%2C%20often%20leading%20to%20suboptimal%20rendezvous%20performance%20in%0Alater%20stages.%20We%20adapt%20a%20standard%20frontier-based%20exploration%20technique%20to%0Aintegrate%20exploration%20and%20rendezvous%20into%20a%20unified%20strategy%2C%20with%20a%20mechanism%0Athat%20allows%20robots%20to%20re-visit%20previously%20explored%20regions%20thus%20enhancing%0Arendezvous%20opportunities.%20We%20validate%20our%20approach%20in%203D%20realistic%20simulations%0Ausing%20ROS%2C%20showcasing%20its%20effectiveness%20in%20achieving%20faster%20rendezvous%20times%0Acompared%20to%20exploration%20strategies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11617v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Frontier-Based%20Exploration%20for%20Multi-Robot%20Rendezvous%20in%0A%20%20Communication-Restricted%20Unknown%20Environments&entry.906535625=Mauro%20Tellaroli%20and%20Matteo%20Luperto%20and%20Michele%20Antonazzi%20and%20Nicola%20Basilico&entry.1292438233=%20%20Multi-robot%20rendezvous%20and%20exploration%20are%20fundamental%20challenges%20in%20the%0Adomain%20of%20mobile%20robotic%20systems.%20This%20paper%20addresses%20multi-robot%20rendezvous%0Awithin%20an%20initially%20unknown%20environment%20where%20communication%20is%20only%20possible%0Aafter%20the%20rendezvous.%20Traditionally%2C%20exploration%20has%20been%20focused%20on%20rapidly%0Amapping%20the%20environment%2C%20often%20leading%20to%20suboptimal%20rendezvous%20performance%20in%0Alater%20stages.%20We%20adapt%20a%20standard%20frontier-based%20exploration%20technique%20to%0Aintegrate%20exploration%20and%20rendezvous%20into%20a%20unified%20strategy%2C%20with%20a%20mechanism%0Athat%20allows%20robots%20to%20re-visit%20previously%20explored%20regions%20thus%20enhancing%0Arendezvous%20opportunities.%20We%20validate%20our%20approach%20in%203D%20realistic%20simulations%0Ausing%20ROS%2C%20showcasing%20its%20effectiveness%20in%20achieving%20faster%20rendezvous%20times%0Acompared%20to%20exploration%20strategies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11617v1&entry.124074799=Read"},
{"title": "Diffusion-Based Environment-Aware Trajectory Prediction", "author": "Theodor Westny and Bj\u00f6rn Olofsson and Erik Frisk", "abstract": "  The ability to predict the future trajectories of traffic participants is\ncrucial for the safe and efficient operation of autonomous vehicles. In this\npaper, a diffusion-based generative model for multi-agent trajectory prediction\nis proposed. The model is capable of capturing the complex interactions between\ntraffic participants and the environment, accurately learning the multimodal\nnature of the data. The effectiveness of the approach is assessed on\nlarge-scale datasets of real-world traffic scenarios, showing that our model\noutperforms several well-established methods in terms of prediction accuracy.\nBy the incorporation of differential motion constraints on the model output, we\nillustrate that our model is capable of generating a diverse set of realistic\nfuture trajectories. Through the use of an interaction-aware guidance signal,\nwe further demonstrate that the model can be adapted to predict the behavior of\nless cooperative agents, emphasizing its practical applicability under\nuncertain traffic conditions.\n", "link": "http://arxiv.org/abs/2403.11643v1", "date": "2024-03-18", "relevancy": 1.5785, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.5749}, {"title": "Customize-A-Video: One-Shot Motion Customization of Text-to-Video\n  Diffusion Models", "link": "http://arxiv.org/abs/2402.14780v1", "similarity": 0.5279}, {"title": "Genie: Generative Interactive Environments", "link": "http://arxiv.org/abs/2402.15391v1", "similarity": 0.506}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Diffusion-Based%20Environment-Aware%20Trajectory%20Prediction&body=Title%3A%20Diffusion-Based%20Environment-Aware%20Trajectory%20Prediction%0AAuthor%3A%20Theodor%20Westny%20and%20Bj%C3%B6rn%20Olofsson%20and%20Erik%20Frisk%0AAbstract%3A%20%20%20The%20ability%20to%20predict%20the%20future%20trajectories%20of%20traffic%20participants%20is%0Acrucial%20for%20the%20safe%20and%20efficient%20operation%20of%20autonomous%20vehicles.%20In%20this%0Apaper%2C%20a%20diffusion-based%20generative%20model%20for%20multi-agent%20trajectory%20prediction%0Ais%20proposed.%20The%20model%20is%20capable%20of%20capturing%20the%20complex%20interactions%20between%0Atraffic%20participants%20and%20the%20environment%2C%20accurately%20learning%20the%20multimodal%0Anature%20of%20the%20data.%20The%20effectiveness%20of%20the%20approach%20is%20assessed%20on%0Alarge-scale%20datasets%20of%20real-world%20traffic%20scenarios%2C%20showing%20that%20our%20model%0Aoutperforms%20several%20well-established%20methods%20in%20terms%20of%20prediction%20accuracy.%0ABy%20the%20incorporation%20of%20differential%20motion%20constraints%20on%20the%20model%20output%2C%20we%0Aillustrate%20that%20our%20model%20is%20capable%20of%20generating%20a%20diverse%20set%20of%20realistic%0Afuture%20trajectories.%20Through%20the%20use%20of%20an%20interaction-aware%20guidance%20signal%2C%0Awe%20further%20demonstrate%20that%20the%20model%20can%20be%20adapted%20to%20predict%20the%20behavior%20of%0Aless%20cooperative%20agents%2C%20emphasizing%20its%20practical%20applicability%20under%0Auncertain%20traffic%20conditions.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11643v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Diffusion-Based%20Environment-Aware%20Trajectory%20Prediction&entry.906535625=Theodor%20Westny%20and%20Bj%C3%B6rn%20Olofsson%20and%20Erik%20Frisk&entry.1292438233=%20%20The%20ability%20to%20predict%20the%20future%20trajectories%20of%20traffic%20participants%20is%0Acrucial%20for%20the%20safe%20and%20efficient%20operation%20of%20autonomous%20vehicles.%20In%20this%0Apaper%2C%20a%20diffusion-based%20generative%20model%20for%20multi-agent%20trajectory%20prediction%0Ais%20proposed.%20The%20model%20is%20capable%20of%20capturing%20the%20complex%20interactions%20between%0Atraffic%20participants%20and%20the%20environment%2C%20accurately%20learning%20the%20multimodal%0Anature%20of%20the%20data.%20The%20effectiveness%20of%20the%20approach%20is%20assessed%20on%0Alarge-scale%20datasets%20of%20real-world%20traffic%20scenarios%2C%20showing%20that%20our%20model%0Aoutperforms%20several%20well-established%20methods%20in%20terms%20of%20prediction%20accuracy.%0ABy%20the%20incorporation%20of%20differential%20motion%20constraints%20on%20the%20model%20output%2C%20we%0Aillustrate%20that%20our%20model%20is%20capable%20of%20generating%20a%20diverse%20set%20of%20realistic%0Afuture%20trajectories.%20Through%20the%20use%20of%20an%20interaction-aware%20guidance%20signal%2C%0Awe%20further%20demonstrate%20that%20the%20model%20can%20be%20adapted%20to%20predict%20the%20behavior%20of%0Aless%20cooperative%20agents%2C%20emphasizing%20its%20practical%20applicability%20under%0Auncertain%20traffic%20conditions.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11643v1&entry.124074799=Read"},
{"title": "Generalized Multi-Source Inference for Text Conditioned Music Diffusion\n  Models", "author": "Emilian Postolache and Giorgio Mariani and Luca Cosmo and Emmanouil Benetos and Emanuele Rodol\u00e0", "abstract": "  Multi-Source Diffusion Models (MSDM) allow for compositional musical\ngeneration tasks: generating a set of coherent sources, creating\naccompaniments, and performing source separation. Despite their versatility,\nthey require estimating the joint distribution over the sources, necessitating\npre-separated musical data, which is rarely available, and fixing the number\nand type of sources at training time. This paper generalizes MSDM to arbitrary\ntime-domain diffusion models conditioned on text embeddings. These models do\nnot require separated data as they are trained on mixtures, can parameterize an\narbitrary number of sources, and allow for rich semantic control. We propose an\ninference procedure enabling the coherent generation of sources and\naccompaniments. Additionally, we adapt the Dirac separator of MSDM to perform\nsource separation. We experiment with diffusion models trained on Slakh2100 and\nMTG-Jamendo, showcasing competitive generation and separation results in a\nrelaxed data setting.\n", "link": "http://arxiv.org/abs/2403.11706v1", "date": "2024-03-18", "relevancy": 1.5072, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.5234}, {"title": "Magic-Me: Identity-Specific Video Customized Diffusion", "link": "http://arxiv.org/abs/2402.09368v1", "similarity": 0.4972}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4944}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Generalized%20Multi-Source%20Inference%20for%20Text%20Conditioned%20Music%20Diffusion%0A%20%20Models&body=Title%3A%20Generalized%20Multi-Source%20Inference%20for%20Text%20Conditioned%20Music%20Diffusion%0A%20%20Models%0AAuthor%3A%20Emilian%20Postolache%20and%20Giorgio%20Mariani%20and%20Luca%20Cosmo%20and%20Emmanouil%20Benetos%20and%20Emanuele%20Rodol%C3%A0%0AAbstract%3A%20%20%20Multi-Source%20Diffusion%20Models%20%28MSDM%29%20allow%20for%20compositional%20musical%0Ageneration%20tasks%3A%20generating%20a%20set%20of%20coherent%20sources%2C%20creating%0Aaccompaniments%2C%20and%20performing%20source%20separation.%20Despite%20their%20versatility%2C%0Athey%20require%20estimating%20the%20joint%20distribution%20over%20the%20sources%2C%20necessitating%0Apre-separated%20musical%20data%2C%20which%20is%20rarely%20available%2C%20and%20fixing%20the%20number%0Aand%20type%20of%20sources%20at%20training%20time.%20This%20paper%20generalizes%20MSDM%20to%20arbitrary%0Atime-domain%20diffusion%20models%20conditioned%20on%20text%20embeddings.%20These%20models%20do%0Anot%20require%20separated%20data%20as%20they%20are%20trained%20on%20mixtures%2C%20can%20parameterize%20an%0Aarbitrary%20number%20of%20sources%2C%20and%20allow%20for%20rich%20semantic%20control.%20We%20propose%20an%0Ainference%20procedure%20enabling%20the%20coherent%20generation%20of%20sources%20and%0Aaccompaniments.%20Additionally%2C%20we%20adapt%20the%20Dirac%20separator%20of%20MSDM%20to%20perform%0Asource%20separation.%20We%20experiment%20with%20diffusion%20models%20trained%20on%20Slakh2100%20and%0AMTG-Jamendo%2C%20showcasing%20competitive%20generation%20and%20separation%20results%20in%20a%0Arelaxed%20data%20setting.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11706v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Generalized%20Multi-Source%20Inference%20for%20Text%20Conditioned%20Music%20Diffusion%0A%20%20Models&entry.906535625=Emilian%20Postolache%20and%20Giorgio%20Mariani%20and%20Luca%20Cosmo%20and%20Emmanouil%20Benetos%20and%20Emanuele%20Rodol%C3%A0&entry.1292438233=%20%20Multi-Source%20Diffusion%20Models%20%28MSDM%29%20allow%20for%20compositional%20musical%0Ageneration%20tasks%3A%20generating%20a%20set%20of%20coherent%20sources%2C%20creating%0Aaccompaniments%2C%20and%20performing%20source%20separation.%20Despite%20their%20versatility%2C%0Athey%20require%20estimating%20the%20joint%20distribution%20over%20the%20sources%2C%20necessitating%0Apre-separated%20musical%20data%2C%20which%20is%20rarely%20available%2C%20and%20fixing%20the%20number%0Aand%20type%20of%20sources%20at%20training%20time.%20This%20paper%20generalizes%20MSDM%20to%20arbitrary%0Atime-domain%20diffusion%20models%20conditioned%20on%20text%20embeddings.%20These%20models%20do%0Anot%20require%20separated%20data%20as%20they%20are%20trained%20on%20mixtures%2C%20can%20parameterize%20an%0Aarbitrary%20number%20of%20sources%2C%20and%20allow%20for%20rich%20semantic%20control.%20We%20propose%20an%0Ainference%20procedure%20enabling%20the%20coherent%20generation%20of%20sources%20and%0Aaccompaniments.%20Additionally%2C%20we%20adapt%20the%20Dirac%20separator%20of%20MSDM%20to%20perform%0Asource%20separation.%20We%20experiment%20with%20diffusion%20models%20trained%20on%20Slakh2100%20and%0AMTG-Jamendo%2C%20showcasing%20competitive%20generation%20and%20separation%20results%20in%20a%0Arelaxed%20data%20setting.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11706v1&entry.124074799=Read"},
{"title": "Crystalformer: Infinitely Connected Attention for Periodic Structure\n  Encoding", "author": "Tatsunori Taniai and Ryo Igarashi and Yuta Suzuki and Naoya Chiba and Kotaro Saito and Yoshitaka Ushiku and Kanta Ono", "abstract": "  Predicting physical properties of materials from their crystal structures is\na fundamental problem in materials science. In peripheral areas such as the\nprediction of molecular properties, fully connected attention networks have\nbeen shown to be successful. However, unlike these finite atom arrangements,\ncrystal structures are infinitely repeating, periodic arrangements of atoms,\nwhose fully connected attention results in infinitely connected attention. In\nthis work, we show that this infinitely connected attention can lead to a\ncomputationally tractable formulation, interpreted as neural potential\nsummation, that performs infinite interatomic potential summations in a deeply\nlearned feature space. We then propose a simple yet effective Transformer-based\nencoder architecture for crystal structures called Crystalformer. Compared to\nan existing Transformer-based model, the proposed model requires only 29.4% of\nthe number of parameters, with minimal modifications to the original\nTransformer architecture. Despite the architectural simplicity, the proposed\nmethod outperforms state-of-the-art methods for various property regression\ntasks on the Materials Project and JARVIS-DFT datasets.\n", "link": "http://arxiv.org/abs/2403.11686v1", "date": "2024-03-18", "relevancy": 1.4082, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.495}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4665}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.451}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Crystalformer%3A%20Infinitely%20Connected%20Attention%20for%20Periodic%20Structure%0A%20%20Encoding&body=Title%3A%20Crystalformer%3A%20Infinitely%20Connected%20Attention%20for%20Periodic%20Structure%0A%20%20Encoding%0AAuthor%3A%20Tatsunori%20Taniai%20and%20Ryo%20Igarashi%20and%20Yuta%20Suzuki%20and%20Naoya%20Chiba%20and%20Kotaro%20Saito%20and%20Yoshitaka%20Ushiku%20and%20Kanta%20Ono%0AAbstract%3A%20%20%20Predicting%20physical%20properties%20of%20materials%20from%20their%20crystal%20structures%20is%0Aa%20fundamental%20problem%20in%20materials%20science.%20In%20peripheral%20areas%20such%20as%20the%0Aprediction%20of%20molecular%20properties%2C%20fully%20connected%20attention%20networks%20have%0Abeen%20shown%20to%20be%20successful.%20However%2C%20unlike%20these%20finite%20atom%20arrangements%2C%0Acrystal%20structures%20are%20infinitely%20repeating%2C%20periodic%20arrangements%20of%20atoms%2C%0Awhose%20fully%20connected%20attention%20results%20in%20infinitely%20connected%20attention.%20In%0Athis%20work%2C%20we%20show%20that%20this%20infinitely%20connected%20attention%20can%20lead%20to%20a%0Acomputationally%20tractable%20formulation%2C%20interpreted%20as%20neural%20potential%0Asummation%2C%20that%20performs%20infinite%20interatomic%20potential%20summations%20in%20a%20deeply%0Alearned%20feature%20space.%20We%20then%20propose%20a%20simple%20yet%20effective%20Transformer-based%0Aencoder%20architecture%20for%20crystal%20structures%20called%20Crystalformer.%20Compared%20to%0Aan%20existing%20Transformer-based%20model%2C%20the%20proposed%20model%20requires%20only%2029.4%25%20of%0Athe%20number%20of%20parameters%2C%20with%20minimal%20modifications%20to%20the%20original%0ATransformer%20architecture.%20Despite%20the%20architectural%20simplicity%2C%20the%20proposed%0Amethod%20outperforms%20state-of-the-art%20methods%20for%20various%20property%20regression%0Atasks%20on%20the%20Materials%20Project%20and%20JARVIS-DFT%20datasets.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11686v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Crystalformer%3A%20Infinitely%20Connected%20Attention%20for%20Periodic%20Structure%0A%20%20Encoding&entry.906535625=Tatsunori%20Taniai%20and%20Ryo%20Igarashi%20and%20Yuta%20Suzuki%20and%20Naoya%20Chiba%20and%20Kotaro%20Saito%20and%20Yoshitaka%20Ushiku%20and%20Kanta%20Ono&entry.1292438233=%20%20Predicting%20physical%20properties%20of%20materials%20from%20their%20crystal%20structures%20is%0Aa%20fundamental%20problem%20in%20materials%20science.%20In%20peripheral%20areas%20such%20as%20the%0Aprediction%20of%20molecular%20properties%2C%20fully%20connected%20attention%20networks%20have%0Abeen%20shown%20to%20be%20successful.%20However%2C%20unlike%20these%20finite%20atom%20arrangements%2C%0Acrystal%20structures%20are%20infinitely%20repeating%2C%20periodic%20arrangements%20of%20atoms%2C%0Awhose%20fully%20connected%20attention%20results%20in%20infinitely%20connected%20attention.%20In%0Athis%20work%2C%20we%20show%20that%20this%20infinitely%20connected%20attention%20can%20lead%20to%20a%0Acomputationally%20tractable%20formulation%2C%20interpreted%20as%20neural%20potential%0Asummation%2C%20that%20performs%20infinite%20interatomic%20potential%20summations%20in%20a%20deeply%0Alearned%20feature%20space.%20We%20then%20propose%20a%20simple%20yet%20effective%20Transformer-based%0Aencoder%20architecture%20for%20crystal%20structures%20called%20Crystalformer.%20Compared%20to%0Aan%20existing%20Transformer-based%20model%2C%20the%20proposed%20model%20requires%20only%2029.4%25%20of%0Athe%20number%20of%20parameters%2C%20with%20minimal%20modifications%20to%20the%20original%0ATransformer%20architecture.%20Despite%20the%20architectural%20simplicity%2C%20the%20proposed%0Amethod%20outperforms%20state-of-the-art%20methods%20for%20various%20property%20regression%0Atasks%20on%20the%20Materials%20Project%20and%20JARVIS-DFT%20datasets.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11686v1&entry.124074799=Read"},
{"title": "Truly No-Regret Learning in Constrained MDPs", "author": "Adrian M\u00fcller and Pragnya Alatur and Volkan Cevher and Giorgia Ramponi and Niao He", "abstract": "  Constrained Markov decision processes (CMDPs) are a common way to model\nsafety constraints in reinforcement learning. State-of-the-art methods for\nefficiently solving CMDPs are based on primal-dual algorithms. For these\nalgorithms, all currently known regret bounds allow for error cancellations --\none can compensate for a constraint violation in one round with a strict\nconstraint satisfaction in another. This makes the online learning process\nunsafe since it only guarantees safety for the final (mixture) policy but not\nduring learning. As Efroni et al. (2020) pointed out, it is an open question\nwhether primal-dual algorithms can provably achieve sublinear regret if we do\nnot allow error cancellations. In this paper, we give the first affirmative\nanswer. We first generalize a result on last-iterate convergence of regularized\nprimal-dual schemes to CMDPs with multiple constraints. Building upon this\ninsight, we propose a model-based primal-dual algorithm to learn in an unknown\nCMDP. We prove that our algorithm achieves sublinear regret without error\ncancellations.\n", "link": "http://arxiv.org/abs/2402.15776v2", "date": "2024-03-18", "relevancy": 1.3876, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4816}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4609}, {"title": "MEDL-U: Uncertainty-aware 3D Automatic Annotation based on Evidential\n  Deep Learning", "link": "http://arxiv.org/abs/2309.09599v3", "similarity": 0.4474}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Truly%20No-Regret%20Learning%20in%20Constrained%20MDPs&body=Title%3A%20Truly%20No-Regret%20Learning%20in%20Constrained%20MDPs%0AAuthor%3A%20Adrian%20M%C3%BCller%20and%20Pragnya%20Alatur%20and%20Volkan%20Cevher%20and%20Giorgia%20Ramponi%20and%20Niao%20He%0AAbstract%3A%20%20%20Constrained%20Markov%20decision%20processes%20%28CMDPs%29%20are%20a%20common%20way%20to%20model%0Asafety%20constraints%20in%20reinforcement%20learning.%20State-of-the-art%20methods%20for%0Aefficiently%20solving%20CMDPs%20are%20based%20on%20primal-dual%20algorithms.%20For%20these%0Aalgorithms%2C%20all%20currently%20known%20regret%20bounds%20allow%20for%20error%20cancellations%20--%0Aone%20can%20compensate%20for%20a%20constraint%20violation%20in%20one%20round%20with%20a%20strict%0Aconstraint%20satisfaction%20in%20another.%20This%20makes%20the%20online%20learning%20process%0Aunsafe%20since%20it%20only%20guarantees%20safety%20for%20the%20final%20%28mixture%29%20policy%20but%20not%0Aduring%20learning.%20As%20Efroni%20et%20al.%20%282020%29%20pointed%20out%2C%20it%20is%20an%20open%20question%0Awhether%20primal-dual%20algorithms%20can%20provably%20achieve%20sublinear%20regret%20if%20we%20do%0Anot%20allow%20error%20cancellations.%20In%20this%20paper%2C%20we%20give%20the%20first%20affirmative%0Aanswer.%20We%20first%20generalize%20a%20result%20on%20last-iterate%20convergence%20of%20regularized%0Aprimal-dual%20schemes%20to%20CMDPs%20with%20multiple%20constraints.%20Building%20upon%20this%0Ainsight%2C%20we%20propose%20a%20model-based%20primal-dual%20algorithm%20to%20learn%20in%20an%20unknown%0ACMDP.%20We%20prove%20that%20our%20algorithm%20achieves%20sublinear%20regret%20without%20error%0Acancellations.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.15776v2", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Truly%20No-Regret%20Learning%20in%20Constrained%20MDPs&entry.906535625=Adrian%20M%C3%BCller%20and%20Pragnya%20Alatur%20and%20Volkan%20Cevher%20and%20Giorgia%20Ramponi%20and%20Niao%20He&entry.1292438233=%20%20Constrained%20Markov%20decision%20processes%20%28CMDPs%29%20are%20a%20common%20way%20to%20model%0Asafety%20constraints%20in%20reinforcement%20learning.%20State-of-the-art%20methods%20for%0Aefficiently%20solving%20CMDPs%20are%20based%20on%20primal-dual%20algorithms.%20For%20these%0Aalgorithms%2C%20all%20currently%20known%20regret%20bounds%20allow%20for%20error%20cancellations%20--%0Aone%20can%20compensate%20for%20a%20constraint%20violation%20in%20one%20round%20with%20a%20strict%0Aconstraint%20satisfaction%20in%20another.%20This%20makes%20the%20online%20learning%20process%0Aunsafe%20since%20it%20only%20guarantees%20safety%20for%20the%20final%20%28mixture%29%20policy%20but%20not%0Aduring%20learning.%20As%20Efroni%20et%20al.%20%282020%29%20pointed%20out%2C%20it%20is%20an%20open%20question%0Awhether%20primal-dual%20algorithms%20can%20provably%20achieve%20sublinear%20regret%20if%20we%20do%0Anot%20allow%20error%20cancellations.%20In%20this%20paper%2C%20we%20give%20the%20first%20affirmative%0Aanswer.%20We%20first%20generalize%20a%20result%20on%20last-iterate%20convergence%20of%20regularized%0Aprimal-dual%20schemes%20to%20CMDPs%20with%20multiple%20constraints.%20Building%20upon%20this%0Ainsight%2C%20we%20propose%20a%20model-based%20primal-dual%20algorithm%20to%20learn%20in%20an%20unknown%0ACMDP.%20We%20prove%20that%20our%20algorithm%20achieves%20sublinear%20regret%20without%20error%0Acancellations.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.15776v2&entry.124074799=Read"},
{"title": "Time Series Compression using Quaternion Valued Neural Networks and\n  Quaternion Backpropagation", "author": "Johannes P\u00f6ppelbaum and Andreas Schwung", "abstract": "  We propose a novel quaternionic time-series compression methodology where we\ndivide a long time-series into segments of data, extract the min, max, mean and\nstandard deviation of these chunks as representative features and encapsulate\nthem in a quaternion, yielding a quaternion valued time-series. This\ntime-series is processed using quaternion valued neural network layers, where\nwe aim to preserve the relation between these features through the usage of the\nHamilton product. To train this quaternion neural network, we derive quaternion\nbackpropagation employing the GHR calculus, which is required for a valid\nproduct and chain rule in quaternion space. Furthermore, we investigate the\nconnection between the derived update rules and automatic differentiation. We\napply our proposed compression method on the Tennessee Eastman Dataset, where\nwe perform fault classification using the compressed data in two settings: a\nfully supervised one and in a semi supervised, contrastive learning setting.\nBoth times, we were able to outperform real valued counterparts as well as two\nbaseline models: one with the uncompressed time-series as the input and the\nother with a regular downsampling using the mean. Further, we could improve the\nclassification benchmark set by SimCLR-TS from 81.43% to 83.90%.\n", "link": "http://arxiv.org/abs/2403.11722v1", "date": "2024-03-18", "relevancy": 1.341, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4542}, {"title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation", "link": "http://arxiv.org/abs/2403.02827v1", "similarity": 0.4475}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4386}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Time%20Series%20Compression%20using%20Quaternion%20Valued%20Neural%20Networks%20and%0A%20%20Quaternion%20Backpropagation&body=Title%3A%20Time%20Series%20Compression%20using%20Quaternion%20Valued%20Neural%20Networks%20and%0A%20%20Quaternion%20Backpropagation%0AAuthor%3A%20Johannes%20P%C3%B6ppelbaum%20and%20Andreas%20Schwung%0AAbstract%3A%20%20%20We%20propose%20a%20novel%20quaternionic%20time-series%20compression%20methodology%20where%20we%0Adivide%20a%20long%20time-series%20into%20segments%20of%20data%2C%20extract%20the%20min%2C%20max%2C%20mean%20and%0Astandard%20deviation%20of%20these%20chunks%20as%20representative%20features%20and%20encapsulate%0Athem%20in%20a%20quaternion%2C%20yielding%20a%20quaternion%20valued%20time-series.%20This%0Atime-series%20is%20processed%20using%20quaternion%20valued%20neural%20network%20layers%2C%20where%0Awe%20aim%20to%20preserve%20the%20relation%20between%20these%20features%20through%20the%20usage%20of%20the%0AHamilton%20product.%20To%20train%20this%20quaternion%20neural%20network%2C%20we%20derive%20quaternion%0Abackpropagation%20employing%20the%20GHR%20calculus%2C%20which%20is%20required%20for%20a%20valid%0Aproduct%20and%20chain%20rule%20in%20quaternion%20space.%20Furthermore%2C%20we%20investigate%20the%0Aconnection%20between%20the%20derived%20update%20rules%20and%20automatic%20differentiation.%20We%0Aapply%20our%20proposed%20compression%20method%20on%20the%20Tennessee%20Eastman%20Dataset%2C%20where%0Awe%20perform%20fault%20classification%20using%20the%20compressed%20data%20in%20two%20settings%3A%20a%0Afully%20supervised%20one%20and%20in%20a%20semi%20supervised%2C%20contrastive%20learning%20setting.%0ABoth%20times%2C%20we%20were%20able%20to%20outperform%20real%20valued%20counterparts%20as%20well%20as%20two%0Abaseline%20models%3A%20one%20with%20the%20uncompressed%20time-series%20as%20the%20input%20and%20the%0Aother%20with%20a%20regular%20downsampling%20using%20the%20mean.%20Further%2C%20we%20could%20improve%20the%0Aclassification%20benchmark%20set%20by%20SimCLR-TS%20from%2081.43%25%20to%2083.90%25.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11722v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Time%20Series%20Compression%20using%20Quaternion%20Valued%20Neural%20Networks%20and%0A%20%20Quaternion%20Backpropagation&entry.906535625=Johannes%20P%C3%B6ppelbaum%20and%20Andreas%20Schwung&entry.1292438233=%20%20We%20propose%20a%20novel%20quaternionic%20time-series%20compression%20methodology%20where%20we%0Adivide%20a%20long%20time-series%20into%20segments%20of%20data%2C%20extract%20the%20min%2C%20max%2C%20mean%20and%0Astandard%20deviation%20of%20these%20chunks%20as%20representative%20features%20and%20encapsulate%0Athem%20in%20a%20quaternion%2C%20yielding%20a%20quaternion%20valued%20time-series.%20This%0Atime-series%20is%20processed%20using%20quaternion%20valued%20neural%20network%20layers%2C%20where%0Awe%20aim%20to%20preserve%20the%20relation%20between%20these%20features%20through%20the%20usage%20of%20the%0AHamilton%20product.%20To%20train%20this%20quaternion%20neural%20network%2C%20we%20derive%20quaternion%0Abackpropagation%20employing%20the%20GHR%20calculus%2C%20which%20is%20required%20for%20a%20valid%0Aproduct%20and%20chain%20rule%20in%20quaternion%20space.%20Furthermore%2C%20we%20investigate%20the%0Aconnection%20between%20the%20derived%20update%20rules%20and%20automatic%20differentiation.%20We%0Aapply%20our%20proposed%20compression%20method%20on%20the%20Tennessee%20Eastman%20Dataset%2C%20where%0Awe%20perform%20fault%20classification%20using%20the%20compressed%20data%20in%20two%20settings%3A%20a%0Afully%20supervised%20one%20and%20in%20a%20semi%20supervised%2C%20contrastive%20learning%20setting.%0ABoth%20times%2C%20we%20were%20able%20to%20outperform%20real%20valued%20counterparts%20as%20well%20as%20two%0Abaseline%20models%3A%20one%20with%20the%20uncompressed%20time-series%20as%20the%20input%20and%20the%0Aother%20with%20a%20regular%20downsampling%20using%20the%20mean.%20Further%2C%20we%20could%20improve%20the%0Aclassification%20benchmark%20set%20by%20SimCLR-TS%20from%2081.43%25%20to%2083.90%25.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11722v1&entry.124074799=Read"},
{"title": "The Value of Reward Lookahead in Reinforcement Learning", "author": "Nadav Merlis and Dorian Baudry and Vianney Perchet", "abstract": "  In reinforcement learning (RL), agents sequentially interact with changing\nenvironments while aiming to maximize the obtained rewards. Usually, rewards\nare observed only after acting, and so the goal is to maximize the expected\ncumulative reward. Yet, in many practical settings, reward information is\nobserved in advance -- prices are observed before performing transactions;\nnearby traffic information is partially known; and goals are oftentimes given\nto agents prior to the interaction. In this work, we aim to quantifiably\nanalyze the value of such future reward information through the lens of\ncompetitive analysis. In particular, we measure the ratio between the value of\nstandard RL agents and that of agents with partial future-reward lookahead. We\ncharacterize the worst-case reward distribution and derive exact ratios for the\nworst-case reward expectations. Surprisingly, the resulting ratios relate to\nknown quantities in offline RL and reward-free exploration. We further provide\ntight bounds for the ratio given the worst-case dynamics. Our results cover the\nfull spectrum between observing the immediate rewards before acting to\nobserving all the rewards before the interaction starts.\n", "link": "http://arxiv.org/abs/2403.11637v1", "date": "2024-03-18", "relevancy": 1.3233, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.4537}, {"title": "Safe and efficient training of a control agent", "link": "https://patents.google.com/patent/US11709462B2/en", "similarity": 0.4386}, {"title": "RoboEXP: Action-Conditioned Scene Graph via Interactive Exploration for\n  Robotic Manipulation", "link": "http://arxiv.org/abs/2402.15487v1", "similarity": 0.4347}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20The%20Value%20of%20Reward%20Lookahead%20in%20Reinforcement%20Learning&body=Title%3A%20The%20Value%20of%20Reward%20Lookahead%20in%20Reinforcement%20Learning%0AAuthor%3A%20Nadav%20Merlis%20and%20Dorian%20Baudry%20and%20Vianney%20Perchet%0AAbstract%3A%20%20%20In%20reinforcement%20learning%20%28RL%29%2C%20agents%20sequentially%20interact%20with%20changing%0Aenvironments%20while%20aiming%20to%20maximize%20the%20obtained%20rewards.%20Usually%2C%20rewards%0Aare%20observed%20only%20after%20acting%2C%20and%20so%20the%20goal%20is%20to%20maximize%20the%20expected%0Acumulative%20reward.%20Yet%2C%20in%20many%20practical%20settings%2C%20reward%20information%20is%0Aobserved%20in%20advance%20--%20prices%20are%20observed%20before%20performing%20transactions%3B%0Anearby%20traffic%20information%20is%20partially%20known%3B%20and%20goals%20are%20oftentimes%20given%0Ato%20agents%20prior%20to%20the%20interaction.%20In%20this%20work%2C%20we%20aim%20to%20quantifiably%0Aanalyze%20the%20value%20of%20such%20future%20reward%20information%20through%20the%20lens%20of%0Acompetitive%20analysis.%20In%20particular%2C%20we%20measure%20the%20ratio%20between%20the%20value%20of%0Astandard%20RL%20agents%20and%20that%20of%20agents%20with%20partial%20future-reward%20lookahead.%20We%0Acharacterize%20the%20worst-case%20reward%20distribution%20and%20derive%20exact%20ratios%20for%20the%0Aworst-case%20reward%20expectations.%20Surprisingly%2C%20the%20resulting%20ratios%20relate%20to%0Aknown%20quantities%20in%20offline%20RL%20and%20reward-free%20exploration.%20We%20further%20provide%0Atight%20bounds%20for%20the%20ratio%20given%20the%20worst-case%20dynamics.%20Our%20results%20cover%20the%0Afull%20spectrum%20between%20observing%20the%20immediate%20rewards%20before%20acting%20to%0Aobserving%20all%20the%20rewards%20before%20the%20interaction%20starts.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2403.11637v1", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=The%20Value%20of%20Reward%20Lookahead%20in%20Reinforcement%20Learning&entry.906535625=Nadav%20Merlis%20and%20Dorian%20Baudry%20and%20Vianney%20Perchet&entry.1292438233=%20%20In%20reinforcement%20learning%20%28RL%29%2C%20agents%20sequentially%20interact%20with%20changing%0Aenvironments%20while%20aiming%20to%20maximize%20the%20obtained%20rewards.%20Usually%2C%20rewards%0Aare%20observed%20only%20after%20acting%2C%20and%20so%20the%20goal%20is%20to%20maximize%20the%20expected%0Acumulative%20reward.%20Yet%2C%20in%20many%20practical%20settings%2C%20reward%20information%20is%0Aobserved%20in%20advance%20--%20prices%20are%20observed%20before%20performing%20transactions%3B%0Anearby%20traffic%20information%20is%20partially%20known%3B%20and%20goals%20are%20oftentimes%20given%0Ato%20agents%20prior%20to%20the%20interaction.%20In%20this%20work%2C%20we%20aim%20to%20quantifiably%0Aanalyze%20the%20value%20of%20such%20future%20reward%20information%20through%20the%20lens%20of%0Acompetitive%20analysis.%20In%20particular%2C%20we%20measure%20the%20ratio%20between%20the%20value%20of%0Astandard%20RL%20agents%20and%20that%20of%20agents%20with%20partial%20future-reward%20lookahead.%20We%0Acharacterize%20the%20worst-case%20reward%20distribution%20and%20derive%20exact%20ratios%20for%20the%0Aworst-case%20reward%20expectations.%20Surprisingly%2C%20the%20resulting%20ratios%20relate%20to%0Aknown%20quantities%20in%20offline%20RL%20and%20reward-free%20exploration.%20We%20further%20provide%0Atight%20bounds%20for%20the%20ratio%20given%20the%20worst-case%20dynamics.%20Our%20results%20cover%20the%0Afull%20spectrum%20between%20observing%20the%20immediate%20rewards%20before%20acting%20to%0Aobserving%20all%20the%20rewards%20before%20the%20interaction%20starts.%0A&entry.1838667208=http%3A//arxiv.org/abs/2403.11637v1&entry.124074799=Read"},
{"title": "A Temporal Bias Correction using a Machine Learning Attention model", "author": "Omer Nivron and Damon J. Wischik and Mathieu Vrac and Emily Shuckburgh", "abstract": "  Climate models are biased with respect to real world observations and usually\nneed to be calibrated prior to impact studies. The suite of statistical methods\nthat enable such calibrations is called bias correction (BC). However, current\nBC methods struggle to adjust for temporal biases, because they disregard the\ndependence between consecutive time-points. As a result, climate statistics\nwith long-range temporal properties, such as heatwave duration and frequency,\ncannot be corrected accurately, making it more difficult to produce reliable\nimpact studies on such climate statistics. In this paper, we offer a novel BC\nmethodology to correct for temporal biases. This is made possible by i)\nre-thinking BC as a probability model rather than an algorithmic procedure, and\nii) adapting state-of-the-art machine-learning (ML) probabilistic attention\nmodels to fit the BC task. With a case study of heatwave duration statistics in\nAbuja, Nigeria, and Tokyo, Japan, we show striking results compared to current\nclimate model outputs and alternative BC methods.\n", "link": "http://arxiv.org/abs/2402.14169v3", "date": "2024-03-18", "relevancy": 1.3143, "topK": [{"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.475}, {"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4284}, {"title": "uPLAM: Robust Panoptic Localization and Mapping Leveraging Perception\n  Uncertainties", "link": "http://arxiv.org/abs/2402.05840v1", "similarity": 0.4273}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20A%20Temporal%20Bias%20Correction%20using%20a%20Machine%20Learning%20Attention%20model&body=Title%3A%20A%20Temporal%20Bias%20Correction%20using%20a%20Machine%20Learning%20Attention%20model%0AAuthor%3A%20Omer%20Nivron%20and%20Damon%20J.%20Wischik%20and%20Mathieu%20Vrac%20and%20Emily%20Shuckburgh%0AAbstract%3A%20%20%20Climate%20models%20are%20biased%20with%20respect%20to%20real%20world%20observations%20and%20usually%0Aneed%20to%20be%20calibrated%20prior%20to%20impact%20studies.%20The%20suite%20of%20statistical%20methods%0Athat%20enable%20such%20calibrations%20is%20called%20bias%20correction%20%28BC%29.%20However%2C%20current%0ABC%20methods%20struggle%20to%20adjust%20for%20temporal%20biases%2C%20because%20they%20disregard%20the%0Adependence%20between%20consecutive%20time-points.%20As%20a%20result%2C%20climate%20statistics%0Awith%20long-range%20temporal%20properties%2C%20such%20as%20heatwave%20duration%20and%20frequency%2C%0Acannot%20be%20corrected%20accurately%2C%20making%20it%20more%20difficult%20to%20produce%20reliable%0Aimpact%20studies%20on%20such%20climate%20statistics.%20In%20this%20paper%2C%20we%20offer%20a%20novel%20BC%0Amethodology%20to%20correct%20for%20temporal%20biases.%20This%20is%20made%20possible%20by%20i%29%0Are-thinking%20BC%20as%20a%20probability%20model%20rather%20than%20an%20algorithmic%20procedure%2C%20and%0Aii%29%20adapting%20state-of-the-art%20machine-learning%20%28ML%29%20probabilistic%20attention%0Amodels%20to%20fit%20the%20BC%20task.%20With%20a%20case%20study%20of%20heatwave%20duration%20statistics%20in%0AAbuja%2C%20Nigeria%2C%20and%20Tokyo%2C%20Japan%2C%20we%20show%20striking%20results%20compared%20to%20current%0Aclimate%20model%20outputs%20and%20alternative%20BC%20methods.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2402.14169v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=A%20Temporal%20Bias%20Correction%20using%20a%20Machine%20Learning%20Attention%20model&entry.906535625=Omer%20Nivron%20and%20Damon%20J.%20Wischik%20and%20Mathieu%20Vrac%20and%20Emily%20Shuckburgh&entry.1292438233=%20%20Climate%20models%20are%20biased%20with%20respect%20to%20real%20world%20observations%20and%20usually%0Aneed%20to%20be%20calibrated%20prior%20to%20impact%20studies.%20The%20suite%20of%20statistical%20methods%0Athat%20enable%20such%20calibrations%20is%20called%20bias%20correction%20%28BC%29.%20However%2C%20current%0ABC%20methods%20struggle%20to%20adjust%20for%20temporal%20biases%2C%20because%20they%20disregard%20the%0Adependence%20between%20consecutive%20time-points.%20As%20a%20result%2C%20climate%20statistics%0Awith%20long-range%20temporal%20properties%2C%20such%20as%20heatwave%20duration%20and%20frequency%2C%0Acannot%20be%20corrected%20accurately%2C%20making%20it%20more%20difficult%20to%20produce%20reliable%0Aimpact%20studies%20on%20such%20climate%20statistics.%20In%20this%20paper%2C%20we%20offer%20a%20novel%20BC%0Amethodology%20to%20correct%20for%20temporal%20biases.%20This%20is%20made%20possible%20by%20i%29%0Are-thinking%20BC%20as%20a%20probability%20model%20rather%20than%20an%20algorithmic%20procedure%2C%20and%0Aii%29%20adapting%20state-of-the-art%20machine-learning%20%28ML%29%20probabilistic%20attention%0Amodels%20to%20fit%20the%20BC%20task.%20With%20a%20case%20study%20of%20heatwave%20duration%20statistics%20in%0AAbuja%2C%20Nigeria%2C%20and%20Tokyo%2C%20Japan%2C%20we%20show%20striking%20results%20compared%20to%20current%0Aclimate%20model%20outputs%20and%20alternative%20BC%20methods.%0A&entry.1838667208=http%3A//arxiv.org/abs/2402.14169v3&entry.124074799=Read"},
{"title": "Properties of Discrete Sliced Wasserstein Losses", "author": "Eloi Tanguy and R\u00e9mi Flamary and Julie Delon", "abstract": "  The Sliced Wasserstein (SW) distance has become a popular alternative to the\nWasserstein distance for comparing probability measures. Widespread\napplications include image processing, domain adaptation and generative\nmodelling, where it is common to optimise some parameters in order to minimise\nSW, which serves as a loss function between discrete probability measures\n(since measures admitting densities are numerically unattainable). All these\noptimisation problems bear the same sub-problem, which is minimising the Sliced\nWasserstein energy. In this paper we study the properties of $\\mathcal{E}: Y\n\\longmapsto \\mathrm{SW}_2^2(\\gamma_Y, \\gamma_Z)$, i.e. the SW distance between\ntwo uniform discrete measures with the same amount of points as a function of\nthe support $Y \\in \\mathbb{R}^{n \\times d}$ of one of the measures. We\ninvestigate the regularity and optimisation properties of this energy, as well\nas its Monte-Carlo approximation $\\mathcal{E}_p$ (estimating the expectation in\nSW using only $p$ samples) and show convergence results on the critical points\nof $\\mathcal{E}_p$ to those of $\\mathcal{E}$, as well as an almost-sure uniform\nconvergence. Finally, we show that in a certain sense, Stochastic Gradient\nDescent methods minimising $\\mathcal{E}$ and $\\mathcal{E}_p$ converge towards\n(Clarke) critical points of these energies.\n", "link": "http://arxiv.org/abs/2307.10352v3", "date": "2024-03-18", "relevancy": 1.2264, "topK": [{"title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis", "link": "http://arxiv.org/abs/2403.03206v1", "similarity": 0.4181}, {"title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images", "link": "http://arxiv.org/abs/2402.13573v2", "similarity": 0.4092}, {"title": "You've Got to Feel It To Believe It: Multi-Modal Bayesian Inference for\n  Semantic and Property Prediction", "link": "http://arxiv.org/abs/2402.05872v3", "similarity": 0.3984}], "mailto": "mailto:mr.pppoe@gmail.com?subject=%5BarXrec%5D%20Properties%20of%20Discrete%20Sliced%20Wasserstein%20Losses&body=Title%3A%20Properties%20of%20Discrete%20Sliced%20Wasserstein%20Losses%0AAuthor%3A%20Eloi%20Tanguy%20and%20R%C3%A9mi%20Flamary%20and%20Julie%20Delon%0AAbstract%3A%20%20%20The%20Sliced%20Wasserstein%20%28SW%29%20distance%20has%20become%20a%20popular%20alternative%20to%20the%0AWasserstein%20distance%20for%20comparing%20probability%20measures.%20Widespread%0Aapplications%20include%20image%20processing%2C%20domain%20adaptation%20and%20generative%0Amodelling%2C%20where%20it%20is%20common%20to%20optimise%20some%20parameters%20in%20order%20to%20minimise%0ASW%2C%20which%20serves%20as%20a%20loss%20function%20between%20discrete%20probability%20measures%0A%28since%20measures%20admitting%20densities%20are%20numerically%20unattainable%29.%20All%20these%0Aoptimisation%20problems%20bear%20the%20same%20sub-problem%2C%20which%20is%20minimising%20the%20Sliced%0AWasserstein%20energy.%20In%20this%20paper%20we%20study%20the%20properties%20of%20%24%5Cmathcal%7BE%7D%3A%20Y%0A%5Clongmapsto%20%5Cmathrm%7BSW%7D_2%5E2%28%5Cgamma_Y%2C%20%5Cgamma_Z%29%24%2C%20i.e.%20the%20SW%20distance%20between%0Atwo%20uniform%20discrete%20measures%20with%20the%20same%20amount%20of%20points%20as%20a%20function%20of%0Athe%20support%20%24Y%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20d%7D%24%20of%20one%20of%20the%20measures.%20We%0Ainvestigate%20the%20regularity%20and%20optimisation%20properties%20of%20this%20energy%2C%20as%20well%0Aas%20its%20Monte-Carlo%20approximation%20%24%5Cmathcal%7BE%7D_p%24%20%28estimating%20the%20expectation%20in%0ASW%20using%20only%20%24p%24%20samples%29%20and%20show%20convergence%20results%20on%20the%20critical%20points%0Aof%20%24%5Cmathcal%7BE%7D_p%24%20to%20those%20of%20%24%5Cmathcal%7BE%7D%24%2C%20as%20well%20as%20an%20almost-sure%20uniform%0Aconvergence.%20Finally%2C%20we%20show%20that%20in%20a%20certain%20sense%2C%20Stochastic%20Gradient%0ADescent%20methods%20minimising%20%24%5Cmathcal%7BE%7D%24%20and%20%24%5Cmathcal%7BE%7D_p%24%20converge%20towards%0A%28Clarke%29%20critical%20points%20of%20these%20energies.%0A%0ALink%3A%20http%3A//arxiv.org/abs/2307.10352v3", "form": "https://docs.google.com/forms/d/e/1FAIpQLSfSfFqShId9ssA7GWYmvv7m_7qsIao4K__1rDj9BurNNxUPYQ/viewform?entry.1380929848=Properties%20of%20Discrete%20Sliced%20Wasserstein%20Losses&entry.906535625=Eloi%20Tanguy%20and%20R%C3%A9mi%20Flamary%20and%20Julie%20Delon&entry.1292438233=%20%20The%20Sliced%20Wasserstein%20%28SW%29%20distance%20has%20become%20a%20popular%20alternative%20to%20the%0AWasserstein%20distance%20for%20comparing%20probability%20measures.%20Widespread%0Aapplications%20include%20image%20processing%2C%20domain%20adaptation%20and%20generative%0Amodelling%2C%20where%20it%20is%20common%20to%20optimise%20some%20parameters%20in%20order%20to%20minimise%0ASW%2C%20which%20serves%20as%20a%20loss%20function%20between%20discrete%20probability%20measures%0A%28since%20measures%20admitting%20densities%20are%20numerically%20unattainable%29.%20All%20these%0Aoptimisation%20problems%20bear%20the%20same%20sub-problem%2C%20which%20is%20minimising%20the%20Sliced%0AWasserstein%20energy.%20In%20this%20paper%20we%20study%20the%20properties%20of%20%24%5Cmathcal%7BE%7D%3A%20Y%0A%5Clongmapsto%20%5Cmathrm%7BSW%7D_2%5E2%28%5Cgamma_Y%2C%20%5Cgamma_Z%29%24%2C%20i.e.%20the%20SW%20distance%20between%0Atwo%20uniform%20discrete%20measures%20with%20the%20same%20amount%20of%20points%20as%20a%20function%20of%0Athe%20support%20%24Y%20%5Cin%20%5Cmathbb%7BR%7D%5E%7Bn%20%5Ctimes%20d%7D%24%20of%20one%20of%20the%20measures.%20We%0Ainvestigate%20the%20regularity%20and%20optimisation%20properties%20of%20this%20energy%2C%20as%20well%0Aas%20its%20Monte-Carlo%20approximation%20%24%5Cmathcal%7BE%7D_p%24%20%28estimating%20the%20expectation%20in%0ASW%20using%20only%20%24p%24%20samples%29%20and%20show%20convergence%20results%20on%20the%20critical%20points%0Aof%20%24%5Cmathcal%7BE%7D_p%24%20to%20those%20of%20%24%5Cmathcal%7BE%7D%24%2C%20as%20well%20as%20an%20almost-sure%20uniform%0Aconvergence.%20Finally%2C%20we%20show%20that%20in%20a%20certain%20sense%2C%20Stochastic%20Gradient%0ADescent%20methods%20minimising%20%24%5Cmathcal%7BE%7D%24%20and%20%24%5Cmathcal%7BE%7D_p%24%20converge%20towards%0A%28Clarke%29%20critical%20points%20of%20these%20energies.%0A&entry.1838667208=http%3A//arxiv.org/abs/2307.10352v3&entry.124074799=Read"},
      ];
      const content = document.getElementById('content');
      function createPostElement(post) {
        const postElement = document.createElement('div');
        postElement.className = 'post';
        const dateElem = document.createElement('p');
        dateElem.setAttribute("class", "date");
        dateElem.textContent = post.date;
        postElement.appendChild(dateElem);

        const textElem = document.createElement('p');
        textElem.setAttribute("class", "text");
        const titleElem = document.createElement('p');
        titleElem.setAttribute("class", "title");
        titleElem.textContent = post.title;
        textElem.appendChild(titleElem);
        const authorElem = document.createElement('p');
        authorElem.setAttribute("class", "author");
        authorElem.textContent = post.author;
        textElem.appendChild(authorElem);
        const abstractElem = document.createElement('p');
        abstractElem.setAttribute("class", "abstract");
        abstractElem.textContent = post.abstract;
        textElem.appendChild(abstractElem);

        const linkElement = document.createElement('a');
        linkElement.setAttribute("class", "link");
        linkElement.href = post.link;
        linkElement.target = "_blank";
        linkElement.textContent = post.link.length > 50 ? post.link.substring(0, 50) + '...' : post.link;
        textElem.appendChild(linkElement);
        postElement.appendChild(textElem);

        const linkElementContainer = document.createElement('div');
        linkElementContainer.setAttribute("class", "comment");
        const actionElement = document.createElement('a');
        actionElement.setAttribute("class", "comment");
        actionElement.href = post.form;
        actionElement.textContent = "Action";
        actionElement.target = "_blank";
        linkElementContainer.appendChild(actionElement);
        const emailElement = document.createElement('a');
        emailElement.setAttribute("class", "comment");
        emailElement.href = post.mailto;
        emailElement.textContent = "Email";
        emailElement.target = "_blank";
        linkElementContainer.appendChild(emailElement);
        postElement.appendChild(linkElementContainer);
        const e = document.createElement('div');
        e.setAttribute("class", "clear");
        postElement.appendChild(e);

        const relevancyContainer = document.createElement('div');
        const relevancyValElem = document.createElement('p');
        relevancyValElem.textContent = "Relevancy " + post.relevancy;
        relevancyContainer.appendChild(relevancyValElem);
        post.topK.forEach((sub) => {
          const topKElem = document.createElement('a');
          topKElem.setAttribute("class", "topK");
          topKElem.href = sub.link;
          topKElem.textContent = sub.title + " (" + sub.similarity + ")";
          topKElem.target = "_blank";
          relevancyContainer.appendChild(topKElem);
        });
        postElement.appendChild(relevancyContainer);
        return postElement;
      }
      function loadPosts() {
        // Simulate loading more posts
        posts.forEach((post) => {
          const postElement = createPostElement(post);
          content.appendChild(postElement);
        });
      }
      // Load initial posts
      loadPosts();
    </script>

  </body>
</html>


